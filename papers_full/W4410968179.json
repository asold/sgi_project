{
  "title": "On the potential of large language models to solve semantics-aware process mining tasks",
  "url": "https://openalex.org/W4410968179",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2967620842",
      "name": "Adrian Rebmann",
      "affiliations": [
        "Systems, Applications & Products in Data Processing (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2970086456",
      "name": "Fabian David Schmidt",
      "affiliations": [
        "University of Würzburg"
      ]
    },
    {
      "id": "https://openalex.org/A4207383812",
      "name": "Goran Glavaš",
      "affiliations": [
        "University of Würzburg"
      ]
    },
    {
      "id": "https://openalex.org/A2138711510",
      "name": "Han van der Aa",
      "affiliations": [
        "University of Vienna"
      ]
    },
    {
      "id": "https://openalex.org/A2967620842",
      "name": "Adrian Rebmann",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2970086456",
      "name": "Fabian David Schmidt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4207383812",
      "name": "Goran Glavaš",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2138711510",
      "name": "Han van der Aa",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4312911974",
    "https://openalex.org/W4392736374",
    "https://openalex.org/W4408985917",
    "https://openalex.org/W4390704391",
    "https://openalex.org/W2164097521",
    "https://openalex.org/W4402101044",
    "https://openalex.org/W4407208190",
    "https://openalex.org/W2899730448",
    "https://openalex.org/W4386321201",
    "https://openalex.org/W6675775337",
    "https://openalex.org/W4399194430",
    "https://openalex.org/W2584722588",
    "https://openalex.org/W4394772955",
    "https://openalex.org/W4390694561",
    "https://openalex.org/W4401009855",
    "https://openalex.org/W3210032359",
    "https://openalex.org/W4386321037",
    "https://openalex.org/W4392781032",
    "https://openalex.org/W2158891129",
    "https://openalex.org/W6600424091",
    "https://openalex.org/W3122183613",
    "https://openalex.org/W4407743882",
    "https://openalex.org/W4407554346",
    "https://openalex.org/W3199677193",
    "https://openalex.org/W4402836060",
    "https://openalex.org/W4289712091",
    "https://openalex.org/W4360971359",
    "https://openalex.org/W3170343681",
    "https://openalex.org/W4285121593",
    "https://openalex.org/W4285208245",
    "https://openalex.org/W2121717903",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W6600388300",
    "https://openalex.org/W4402092309",
    "https://openalex.org/W4399194421",
    "https://openalex.org/W3133897986",
    "https://openalex.org/W4399162617"
  ],
  "abstract": null,
  "full_text": "Open Access\n© The Author(s) 2025. Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 Inter-\nnational License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you \ngive appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified \nthe licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The \nimages or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a \ncredit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by \nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of \nthis licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.\nRESEARCH\nRebmann et al. Process Science            (2025) 2:10  \nhttps://doi.org/10.1007/s44311-025-00019-3\nProcess Science\nOn the potential of large language models \nto solve semantics-aware process mining tasks\nAdrian Rebmann1*, Fabian David Schmidt2, Goran Glavaš2 and Han van der Aa3 \nAbstract \nLarge language models (LLMs) have shown to be valuable tools for tackling process \nmining tasks. Existing studies report on their capability to support various data-driven \nprocess analyses and even, to some extent, that they are able to reason about how pro-\ncesses work. This reasoning ability suggests that there is potential for LLMs to tackle \nsemantics-aware process mining tasks, which are tasks that rely on an understand-\ning of the meaning of activities and their relationships. Examples of these include \nprocess discovery, where the meaning of activities can indicate their dependency, \nwhereas in anomaly detection the meaning can be used to recognize process behav-\nior that is abnormal. In this paper, we systematically explore the capabilities of LLMs \nfor such tasks. Unlike prior work, which largely evaluates LLMs in their default state, we \ninvestigate their utility through both in-context learning and supervised fine-tuning. \nConcretely, we define five process mining tasks requiring semantic understanding \nand provide extensive benchmarking datasets for evaluation. Our experiments reveal \nthat while LLMs struggle with challenging process mining tasks when used out of the \nbox or with minimal in-context examples, they achieve strong performance when fine-\ntuned for these tasks across a broad range of process types and industries.\nKeywords: Process mining, Large language models, Semantics-awareness\nIntroduction\nProcess mining focuses on analyzing event data from organizational processes to \nuncover actionable insights about their actual execution. Common process mining tasks \ninclude the discovery of process models, recognizing abnormal behavior, and making \npredictions about the future of ongoing cases. The majority of techniques that have been \ndeveloped to tackle these tasks are based on the analysis of frequencies, e.g., how often a \ncertain activity A is followed by a activity B in an event log. While such techniques have \nbeen proven to be successful, recent works have recognized the potential of incorporat -\ning considerations of activity semantics, i.e., the meaning of activities, when conducting \nprocess mining tasks such as discovery, anomaly detection, and prediction (van der Aa \net al. 2021; Caspary et al. 2023; Norouzifar et al. 2024).\nGiven that such semantics-aware analyses exploit the textual labels associated with \nevents, the advent of large language models (LLMs) is a particularly promising develop -\nment, due to their impressive capabilities when it comes to handling natural language. \n*Correspondence:   \nadrian.rebmann@sap.com\n1 SAP Signavio, SAP SE, \nDietmar-Hopp-Allee \n16, 69190 Walldorf, \nBaden-Württemberg, Germany\n2 Center for Artificial Intelligence \nand Data Science, University \nof Würzburg, Sanderring 2, \n97070 Würzburg, Bavaria, \nGermany\n3 Faculty of Computer Science, \nUniversity of Vienna, Währinger \nStr. 29, 1090 Vienna, Austria\nPage 2 of 29Rebmann et al. Process Science            (2025) 2:10 \nLanguage models have proven useful for semantic anomaly detection (Caspary et  al. \n2023; Busch et al. 2024) and preliminary explorations of LLMs for interacting with event \ndata show potential (Berti et al. 2023; Jessen et al. 2023; Estrada-Torres et al. 2024). How-\never, existing works either focus on evaluating one smaller language model that was fine-\ntuned on semantic anomaly detection or only use LLMs out of the box to solve general \nprocess analysis questions instead of clearly defined process mining tasks. Consequently, \na systematic and in-depth study of the application of LLMs to semantics-aware process \nmining tasks remains absent. This research gap largely follows from the lack of well-\ndefined natural language processing (NLP) tasks that effectively conceptualize the capa -\nbility to perform semantics-aware process mining tasks, along with the corresponding \nbenchmarking datasets that are necessary for structured evaluations. Addressing these \nissues is crucial for enabling robust and comparative assessments of LLMs for tackling \nprocess mining tasks.\nRecognizing this, our work makes the following contributions:\n• We define five tasks designed to evaluate the capabilities of LLMs in semantics-aware \nprocess mining. These tasks focus on (forms of ) anomaly detection, next activity pre-\ndiction, and process discovery. These tasks are motivated by the potential benefits \nthey offer in enhancing approaches to their non-semantic counterparts. Anomaly \ndetection can be improved by identifying undesired process behavior based on activ -\nity meaning, such as detecting that a delivery is created after a canceled purchase \norder. Similarly, next activity prediction benefits by narrowing options to semanti -\ncally valid choices, e.g., discarding a check request if already approved. Process dis -\ncovery can leverage activity meaning to, among others, handle event log incom -\npleteness, inferring parallel executions even if not explicitly recorded. For example, \nin order fulfillment, packaging and invoicing might occur simultaneously, but a fre -\nquency-based approach could misrepresent them as sequential.\n• We provide benchmarking datasets for each of the proposed tasks, enabling rigorous \nquantitative evaluations of the performance of LLMs for semantics-aware process \nmining. The foundation of these datasets is provided by a corpus of process behavior \nthat we derived from the largest publicly available collection of process models.\n• We conduct an experimental evaluation of several open-source LLMs on the pro -\nposed tasks and their corresponding benchmarking datasets. The evaluation includes \na comparison of LLMs in both in-context learning and fine-tuning settings, along -\nside discriminative encoder-based language models.\nIn this manner, our work provides the first systematic evaluation of (L)LMs on seman -\ntics-aware process mining tasks. Our experiments reveal that LLMs struggle with these \ntasks when used out of the box or with minimal in-context examples, but they achieve \nstrong performance when fine-tuned for these tasks.\nThis paper is an extended and revised version of our earlier work on evaluating the ability \nof LLMs to solve semantics-aware process mining tasks, published as part of the proceed-\nings of the International Conference on Process Mining 2024 (Rebmann et al. 2024a). This \nPage 3 of 29\nRebmann et al. Process Science            (2025) 2:10 \n \nworks extends the conference paper in the following ways. First, we broadened the scope of \nour work by introducing two new semantics-aware process mining tasks. These tasks fun-\ndamentally differ from the three previously introduced ones, since they focus on generation \n(of directly-follows relations and process trees), whereas the previous tasks were all classi-\nfication-based. For both new tasks, we established and publish a benchmarking dataset to \nassess and compare the performance of LLMs when solving them. We use these datasets to \nextend our experimental evaluation, shedding light on the ability of open source LLMs to \nsolve process-oriented generation tasks. Beyond these conceptual and experimental exten-\nsions, we provide more detailed statistics about the process models that our benchmark-\ning datasets are based on. Furthermore, we thoroughly revised and extended the Related \nwork section, highlighting recent advancements in evaluating LLMs for data-driven process \nanalysis tasks.\nThe remainder of this paper is structured as follows. Preliminaries section introduces \npreliminary definitions that form the basis for the semantics-aware process mining \ntasks that we define in Semantics-aware process mining tasks section.  Datasets section \ndescribes the publicly available process behavior corpus and benchmarking datasets that \nwe have established. LLM-based process mining section describes the different ways in \nwhich we use and adapt LLMs to tackle the proposed tasks. Experimental setup section \nreports on the setup that we used to conduct our evaluation experiments, of which the \nresults are presented in Results and discussion section. Finally, Related work section dis-\ncusses related work, before we conclude in Conclusion section.\nPreliminaries\nIn this section, we introduce the preliminaries that are essential for the remainder of the \npaper, covering event data, process models, directly-follows relations and graphs, foot -\nprints, process trees, and eventually-follows relations.\nEvent Data. We adopt a simple event model, focusing on the control-flow of a process. \nA trace σ is a sequence that represents the events that have been recorded for the execu-\ntion of a single instance of an organizational process. Such a trace consists of a finite \nsequence of events with each event as a record of the execution of an activity. We denote \nthis as σ =� a1 , ...,an� , with a i ∈ A , with A as the universe of possible activities that can \nbe performed in organizational processes. An event log L is a finite multi-set of traces. \nAL ⊂ A denotes the set of activities that appear in the traces of L.\nFor instance, we may have an event log L1 =[ σ1 ,σ2 ,σ3 ] , with:\nProcess Models. We define a process model M as the set of executions that are allowed \nin a process. Each execution π is represented as an activity sequence π =� a1 , ...,an� , \nwith a i ∈ A . We use AM ⊂ A , to denote the set of activities that appear in the sequences \nof M.\nFor instance, M1 ={ π1 ,π2 } with:\nσ1 =� receive order, accept order , deliver package �\nσ2 =� receive order, reject order�\nσ3 =� receive order, deliver package �.\nThis then gives A L 1 ={ receive order, accept order , reject order, deliver package }.\nPage 4 of 29Rebmann et al. Process Science            (2025) 2:10 \nThen, A M 1 ={ receive order,accept order,reject order,deliver package}.\nProcess Trees. A process tree is a hierarchical representation of a process, with a con-\ncise notation. Such a tree has a single root node and its leaves correspond to activities. \nCommonly, four operator types are used: → (sequence operator), × (exclusive choice \noperator), ∧ (parallel operator), and ⟲ (loop operator). We follow the definition of van \nder Aalst (2022a): Let O = {→, ×, ∧, ⟲} be the set of operators and τ  ∈ A be the silent \nactivity. A process tree is defined recursively:\n• if a ∈ A ∪{ τ } , then T = a is a process tree\n• if T1 ,T2 ,... ,Tn with n ≥ 1 are process trees and ⊕ ∈ {→, ×, ∧} , then \nT =⊕ (T1 ,T2 ,... ,Tn) is a process tree\n• if T1 ,T2 ,... ,Tn with n ≥ 2 are process trees and T = ⟲(T1 ,T2 ,... ,Tn).\nFor instance, → (receive order, ×(→ (accept order, deliver package ), reject order)) is a \ntree that captures the behavior defined by the execution sequences of M1 , i.e., that, after \nreceiving an order, it is either accepted and then delivered, or it is rejected.\nDirectly-Follows Relations and Graphs. The directly-follows relation > captures that \ntwo activities can immediately follow each other, either in the execution sequences of a \nprocess model or the traces of an event log. For a process model M, x > y if there exists \nan execution sequence π =� a1 , ...,an�∈M  for which there exists a i = x, a j = y with \nj = i + 1 . The same applies for a log L, where these constraints should hold for at least \none trace σ ∈ L.\nWe define a directly follows graph (DFG) D as a pair (A,  F). A is the set of possi -\nble activities in a given process and F is a set of pairs (x,  y) that represent the process’ \ndirectly-follows relations (or edges in the DFG), i.e., {(x, y)∈ A × A | x > y\nFor instance, given M1 , we obtain a DFG DM 1 = (AM 1 ,FM 1 ) , with:\nFM 1 ={ (receive order,accept order),(accept order,deliver package),(receive order, reject order)}.\nDirectly-Follows Footprints. Following the definition of van der Aalst (2022a), \nlet D = (A, F) be a directly-follows graph. D defines a (directly-follows) footprint \nfpD : (A × A) → {→, ←,||,# } such that for all (x, y) ∈ A × A:\n• fpD ((x, y)) =→ if (x, y) ∈ F and (y, x)/∈ F\n• fpD ((x, y)) =← if (x, y)/∈ F and (y, x) ∈ F\n• fpD ((x, y)) =� if (x, y) ∈ F and (y, x) ∈ F\n• fpD ((x,y)) = #  if (x, y)/∈ F and (y, x)/∈ F\nFor instance, fpDM 1 is:\nπ1 =� receive order , accept order , deliver package �\nπ2 =� receive order , reject order �\nPage 5 of 29\nRebmann et al. Process Science            (2025) 2:10 \n \nEventually-Follows Relations. The eventually-follows relation ≺ is a more relaxed \nordering relation than the directly-follows relation described above, focusing on activi -\nties that can either directly or indirectly follow each other in the activity sequences of a \nprocess model or in the traces of a log. For a process model M, x ≺ y if there exists an \nexecution sequence π =� a1 , ...,an�∈M  for which there exists a i = x, a j = y with i < j . \nThe same applies for a log L, where these constraints should hold for at least one trace \nσ ∈ L . We use EFM  to denote all eventually-follows relations of the activity sequences \nallowed by a model M.\nFor instance, EF M 1 ={ (receive order,accept order),(receive order,reject order),\n(receive order, deliver package ), (accept order, deliver package )}\nSemantics‑aware process mining tasks\nThis section describes and defines five process mining tasks that benefit from an under -\nstanding of process behavior. The tasks all focus on the control-flow perspective and are \ndesigned so that they do not involve (nor require) access to historical event data in order \nto perform them. In this manner, we are able to assess whether a language model can \nsolve the tasks based purely on its encoded knowledge of how processes generally work \nwith respect to the meaning of activities and their inter-relations. Our tasks include \ntwo forms of semantic anomaly detection, semantic next activity prediction, and two \nflavors of semantic process discovery. These tasks represent semantics-aware counter -\nparts to well-established tasks in process mining and vary considerably in terms of their \ncomplexity.\nSemantic anomaly detection\nAnomaly detection in process mining focuses on identifying outlying process behaviors \nwithin the traces of an event log (van der Aalst and de Medeiros 2005). Many approaches \nachieve this by detecting statistical outliers (Bezerra and Wainer 2013), based on the \npremise that infrequent behavior is anomalous.\nIn contrast, semantic anomaly detection (van der Aa et al. 2021) focuses on identifying \nprocess behaviors that lack logical coherence. It challenges the assumption that infre -\nquent behavior is necessarily anomalous and emphasizes that regular occurrences do \nnot always constitute proper process behavior. For example, from a semantic perspec -\ntive, creating an invoice for a rejected purchase order constitutes anomalous behavior, \nregardless of how frequently it occurs.\nDetecting anomalies based on process semantics requires a different approach com -\npared to frequency-based anomaly detection. Whereas frequency-based detection can \nfpD M 1 ((receive order, accept order)) =→\nfpD M 1 ((accept order, receive order)) =←\nfpD M 1 ((receive order, reject order)) =→\nfpD M 1 ((reject order, receive order)) =←\nfpD M 1 ((accept order, deliver package)) =→\nfpD M 1 ((deliver package, accept order)) =←\nAll other pairs in (A M 1 ×A M 1 ) = #\nPage 6 of 29Rebmann et al. Process Science            (2025) 2:10 \nbe performed by just using data in an event log (revealing statistical outliers), seman -\ntic anomaly detection requires information about how a process should (or should not) \nwork in general. By definition, such information needs to be obtained from outside of \nthe event log, e.g., from large knowledge bases or—as we do in this paper—from the \nknowledge encoded in LLMs.\nWe define two specific tasks in this context, focusing on the trace and activity-relation \nlevels.\nTrace-Level Semantic Anomaly Detection. Trace-level semantic anomaly detection \n(T-SAD) is a binary classification problem in which a trace σ ∈ L needs to be classified \nas anomalous or not, based on its semantics and the set of possible activities AL , which \nis provided as context information. For instance, for a trace σ =� register application, \napprove application, review application⟩ , the task is to classify that σ is anomalous. This \nis because an application should first be reviewed and only then approved (or rejected). \nThe challenge here is that there is no specification available of the process at hand that \ncan be used for this. Rather that anomaly needs to be inferred, requiring an understand -\ning of how processes generally work.\nActivity-Level Semantic Anomaly Detection. Activity-level semantic anomaly \ndetection (A-SAD) is more fine-granular than T-SAD, focusing on pairs of activities in \na trace rather than on an entire trace at once. In particular, A-SAD focuses on classify -\ning any eventually-follows relation a i ≺σ a j of two activities a i and a j that appear in a \ntrace σ ∈ L as anomalous or not, based on its semantics and the set of possible activities \nAL . For instance, given the trace σ =� create purchase order, reject purchase order, create \ninvoice⟩ , the eventually-follows relation reject purchase order ≺σ create invoice should be \nclassified as anomalous, whereas the other pairwise relations, i.e., create purchase order \n≺σ reject purchase order and create purchase order ≺σ create invoice should be classified \nas valid.\nSemantic next activity prediction\nNext activity prediction, also referred to as next event or next step prediction, is a fun -\ndamental task in predictive process monitoring (van der Aalst 2022b). The goal is to \npredict the next activity in an ongoing process execution (Neu et al. 2022). To address \nthis task, numerous approaches, primarily leveraging supervised deep learning methods \n(e.g., Evermann et al. (2017); Pfeiffer et al. (2021)), have been proposed.\nAs a semantics-aware counterpart for next activity prediction, we introduce the \nsemantic next activity prediction (S-NAP) task. For an incomplete trace σ , which rep -\nresents an ongoing process execution in which k activities have been performed ( k ≥ 1 ), \nthe task is to predict the next activity ak+1 in σ ∈ L based on a set of possible activities \nAL . For instance, given σ =� create purchase order, approve purchase order⟩ and AL ={\ncreate purchase order, approve purchase order, create invoice, make payment} , the task is \nto predict ak+1 as create invoice. This is because, generally, an invoice should be created \nbefore a payment is made.\nWhereas approaches for (traditional) next activity prediction train a model on histori -\ncal traces from an event log L to predict the next activity in ongoing (i.e., unseen) execu -\ntions of the process, S-NAP focuses on situations where no such historical traces are \nPage 7 of 29\nRebmann et al. Process Science            (2025) 2:10 \n \navailable. As a result, the next activity must be inferred by considering the semantics of \nthe activities involved in a process.\nSemantic process discovery\nSemantic process discovery focuses on generating a process representation that accu -\nrately captures the underlying semantics of a process, rather than reflecting observed \nbehavior in event logs. Unlike traditional approaches that generate models based on \nbehavioral relations recorded from historical process executions, semantic discovery \nincorporates domain knowledge to ensure that the resulting models represent proper \nand reasonable process behavior only based on a set of possible activities. This enables \nthe detection of not what is frequent in event data but what is meaningful and correct in \na process context. Such semantic models can form the basis for downstream tasks such \nas conformance checking, where we can compare the appropriate behavior (captured \nthrough the semantic model), to the actually observed traces.\nFor instance, a semantic discovery result should represent that a review application \nactivity must precede an approve application activity, regardless of how frequently traces \nin the event log adhere to or violate this rule. This is because the process representation \nneeds to align with logical process semantics rather than reflecting patterns in historical \nevent data. Discovering such representations requires knowledge about how processes \nshould work, which can be sourced from domain-specific knowledge bases, human \nexpertise or—as done in our work—LLMs.\nWe define two semantic discovery tasks, focusing on constructing directly-follows \ngraphs and process trees:\nSemantic Directly-Follows Graph Discovery. Semantic directly-follows graph dis -\ncovery (S-DFD) is the task of generating a directly-follows graph that represents all \nreasonable relations between activities in a process. The goal is to produce a graph \nD = (A, F) , where each edge (a, b) ∈ F reflects a valid directly-follows relation between \nactivities a and b, only based on the set of activities A. For instance, given a set of \nactivities {create purchase order, approve purchase order, reject purchase order, create \ninvoice} , the semantic DFG should include the edge (create purchase order, approve pur-\nchase order). This edge aligns with the common-sense understanding that an invoice is \ntypically created following an approved purchase order. Conversely, the DFG should not \ninclude the edge (reject purchase order, create invoice). This is because such a directly-\nfollows relation contradicts the common-sense rule that an invoice should only be cre -\nated for approved purchase orders.\nThe challenge lies in inferring the semantics of valid directly-follows relations without \nbeing able to rely on explicit process specifications and execution sequences.\nSemantic Process Tree Discovery. Semantic process tree discovery (S-PTD) is a \nmore structured task than S-DFD, focusing on constructing a hierarchical representa -\ntion of a process, namely a process tree (see Preliminaries section). The goal is to gen -\nerate a process tree whose structure reflects the behavioral constraints in a process, \nsuch as parallelism, choices, and sequential behavior, only based on a set of possible \nactivities A. For instance, given the activity set {register application, review applica -\ntion, approve application, reject application } , the semantic process tree should be →\n(register application, review application, ×(approve application, reject application)). \nPage 8 of 29Rebmann et al. Process Science            (2025) 2:10 \nThis ensures that the applications are registered before they are reviewed and, finally, \na decision is made, while only one outcome per review decision should be possible.\nS-PTD is particularly challenging as it requires an understanding of not only pair -\nwise relations but also how subsets of activities (parts of the entire process) relate to \none another as well as how process trees work. Doing this without having historical \nexecution data available, necessitates external knowledge about process semantics to \nbe able to construct trees that are both structurally and semantically sound.\nDatasets\nThis section outlines the creation and key characteristics of the corpus of process behav-\nior and benchmarking datasets used to evaluate the ability of language models to solve \nthe proposed tasks. All datasets are made publicly available (Rebmann et al. 2024b).\nA corpus of process behaviors\nLanguage models require textual input. To assess their ability to solve semantics-\naware process mining tasks, we need a collection of textual representations of process \nbehavior, referred to as a corpus. This corpus serves as the foundation for creating \ntask-specific data, which can then be used to train and evaluate language models on \nthe proposed tasks.\nCorpus creation\nAs no suitable corpus is readily available, we create one based on graphical process \nmodels (i.e., process diagrams). For this purpose, we use sap-sam (Sola et al. 2023), \nthe largest publicly available collection of process diagrams to date. In order to cre -\nate a high-quality corpus, we select only English BPMN diagrams from sap-sam that \nmeet specific requirements. These ensure that the corpus includes only unique and \nvalid process behavior. In particular, we require that a diagram can be transformed \ninto a sound block-structured workflow net and that no two diagrams have the same \nactivity set. The former requirement mitigates data quality issues in the sap-sam col -\nlection (Sola et al. 2023) and ensures that we can properly generate activity sequences \nfrom the diagram, and valid process trees from said activity sequences. The latter \nmakes sure that different models in the corpus capture different process behavior and \nensures a robust evaluation setup. Therefore, we discard a diagram, when another one \nwith the same activity set is already present in the corpus. Furthermore, we require a \ndiagram to contain at least two different activities to ensure that it actually captures \nordering relations between different activities.\nWe use the workflow net of each selected diagram to generate activity sequences, \ncapturing all executions allowed by the net. For loops, we ensure that each loop is \nexecuted at most once, so that we capture relations involving rework, yet, obtain a \nfinite set of activity sequences. For each net, this yields a process model M  according \nto the definition in Preliminaries section, i.e., a set of activity sequences capturing its \nallowed behavior. We add each such M  to the corpus.\nPage 9 of 29\nRebmann et al. Process Science            (2025) 2:10 \n \nCorpus characteristics\nWe show the characteristics of the resulting corpus in Table  1. As depicted there, the \ncomplexity of the process models varies considerably. For instance, the median number \nof unique activities is 4, whereas the maximum is 21 and the process models allow for \n10.34 activity sequences on average, whereas the maximum amount is 10,080.\nTo highlight the broad coverage of the established corpus, we categorized its process \nmodels according to their process type and the industry they relate to, using established \nframeworks. We leveraged an LLM to perform the categorization itself. The LLM was \nprompted with each process model’s name, its unique set of activities, and the target \nAPQC categories. Based on this, it assigned the most appropriate category or “Other” if \nno suitable match was found.\nFor the categorization by cross-industry process types, we used the first level of the \nAPQC Process Classification Framework 1. The results, shown in Fig.  1a, indicate that \nthe corpus spans a wide range of process types, including HR, delivery of products and \nservices, customer service management, and financial resource management. However, \n28% of the models could not be categorized using this framework.\nFor the industry-based categorization, we applied the industry categories defined by \nSAP2. The resulting distribution, illustrated in Fig. 1b, shows broad representation across \ndiverse industries, such as retail, healthcare, banking, insurance, and travel. Still, 38% of \nthe models could not be matched to any industry category.\nOverall, while some models remained uncategorized per framework, only 14% could \nnot be assigned to a category in either of the frameworks. Manual inspection of these \nreveals that these cases largely consist of generic, low-level processes-such as download -\ning documents or filling in unspecified forms-as well as daily routine processes like pre -\nparing a meal or getting ready for work.\nTask‑specific benchmarking datasets\nHaving established a text corpus of process behaviors, we are able to generate task-spe -\ncific benchmarking datasets that include task samples and a gold standard. This gold \nstandard enables objective, quantitative evaluation of language models on the tasks \nbased on established evaluation measures such as F- 1 score for classification and fitness \nscores for discovery tasks. The characteristics of the datasets are as follows:\nTable 1 Characteristics of the process behavior corpus\nCharacteristic Total Per process model\nAvg. Med. Min. Max.\n# Process models 15,857 – – – –\n# Unique activities 49,108 4.70 4 2 21\n# Unique sequences 163,484 10.34 1 1 10,080\n1 https:// www. apqc. org/ proce ss- frame works\n2 https:// www. sap. com/ indus tries. html\nPage 10 of 29Rebmann et al. Process Science            (2025) 2:10 \n• T-SAD contains a total of 291,251 samples, of which 150,301 are valid, and 140,950 \nare labeled as anomalous. Trace lengths range from 1 to 17, with an average of 6.3 \nand a median of 7.\n• A-SAD consists of 316,308 samples in total, equally divided between 158,154 valid \nsamples and 158,154 anomalous samples.\n• S-NAP comprises 1,289,081 samples in total, with prefix lengths ranging from of 1 to \ntheir full length. The mean prefix length is 4.6, and the median is 5.\n• S-DFD consists of a total of 15,857 samples, with one sample per model. The number \nof edges ranges from a minimum of 1 to a maximum of 87, with an average of 5.2 and \na median of 4.\n• S-PTD features 15,857 samples as well (as for S-DFD there is one sample per original \nmodel).\nWe next describe how we established each of these datasets.\nFig. 1 Categorization of the process models in the corpus\nPage 11 of 29\nRebmann et al. Process Science            (2025) 2:10 \n \nClassification‑task datasets\nT-SAD. To establish the T-SAD dataset, we first create an event log L for each process \nmodel M in the corpus such that each π ∈ M becomes a trace σ ∈ L . To make sure that \nthere is a minimum number of traces per log, we randomly duplicate traces in L until \na size of 100 is reached if L does not already contain at least 100 traces. Subsequently, \nfor each trace σ ∈ L , we make a decision regarding the insertion of noise, with a 50 per -\ncent probability. This noise insertion involves swapping two randomly selected activities \nwithin the trace. After swapping, we check whether the resulting sequence σ′ is indeed \nanomalous, i.e., σ ′ /∈ M . If the sequence is found to still be valid, i.e., σ′ ∈ M , we con -\ntinue iterating through potential swaps until we obtain an anomalous sequence 3. This \nensures that the dataset contains (roughly) the same amount of valid and anomalous \ntraces, which is crucial for robust model training and evaluation.\nEach of the 291,251 records of the T-SAD dataset then consists of a trace σ , the correct \nlabel of σ , i.e., Anomalous if σ/∈ M and Valid otherwise, and the set of possible activities \nin the process from which σ originates as context information.\nA-SAD. We create the A-SAD dataset based on the set of eventually-follows relations \nEFM  for each process model M in the corpus. The relations in EFM  represent all valid \nexecution orders of activities of M. Next to these, we create a set of anomalous relations \nEF̸ M  , i.e., ones that are not in EFM  . To provide a balanced dataset, we establish EF̸ M  by \nrandomly selecting relations that are not in EFM  , until we have an equal number of valid \nand anomalous relations.\nEach of the 316,308 records of the A-SAD dataset consists of an eventually-follows \nrelation r, the correct label of r, (i.e., if r is anomalous or not), and the set of activities AM \nof the process model from which r originates (as context information).\nS - N A P. For the S-NAP dataset, we first create an event log L for each process model \nM in the corpus such that each π ∈ M becomes a trace σ ∈ L . Then, we generate all pos-\nsible prefixes for each trace σ ∈ L and add them to L. This involves iteratively consider -\ning sub-traces of increasing length k from the first activity of a trace σ , up to one less \nthan the full trace length, ensuring that there is always a subsequent activity available to \nserve as a prediction label.\nEach of the 1,289,081 records of the S-NAP dataset then consists of a length-k prefix \n( σk ) of σ , the correct label of σk , i.e., the activity at position k + 1 in σ , and the set of pos-\nsible activities AL of the event log from which σ originates as context information.\nGeneration‑task datasets\nS-DFD. To establish the S-DFD dataset, we discover a DFG DM = (AM , FM ) for each \nprocess model M and its activity set AM following the definition in Preliminaries section. \nThe pairs in FM  represent the valid directly-follows relation between two activities in the \nexecution sequences of M.\nAs a result, each of the 15,857 records of the S-DFD dataset consists of the set of pos -\nsible activities AM and the true DFG DM , obtained from a process model M.\n3 We limit the number of retries to 10 per trace to guarantee termination.\nPage 12 of 29Rebmann et al. Process Science            (2025) 2:10 \nS-PTD. We establish the S-DPT dataset based on the workflow nets we obtained dur -\ning the creation of our process behavior corpus. For each workflow net that was used to \ngenerate a process model M, we translate it into a process tree TM according to the defi-\nnition in Preliminaries section.4\nEach of the resulting 15,857 records of the S-PTD dataset comprises the set of activi -\nties AM and the true process tree TM , obtained from a process model M.\nLLM‑based process mining\nThis section provides an overview of how we adapt (L)LMs for solving semantics-aware \nprocess mining tasks, using few-shot in-context learning and fine-tuning for both the \nclassification and generation tasks.\nNeural language models, based on the Transformer architecture (Vaswani et al. 2017; \nDevlin et  al. 2019), come in two main flavors: (1) bidirectional language models, also \ncommonly called encoders, which are typically (pre-)trained via masked language mod -\neling objectives in which masked tokens are predicted from both left and right context \n(Devlin et al. 2019; Liu et al. 2019), and (2) unidirectional language models, also known \nas decoders, which are trained via autoregressive language modeling objectives where \nthe next token is predicted from the preceding context (Brown et al. 2020; Touvron et al. \n2023). LLMs are large instances of the latter category (with at least a billion parameters) \nand are, following large-scale autoregressive language modeling, typically additionally \ntrained for instruction following, i.e., to provide solutions to tasks given the natural lan -\nguage description of these tasks (Wang et al. 2022; Zhang et al. 2023). Such instruction-\ntuning allows LLMs to generalize to new tasks through textual task descriptions (since \ncapturing meaning of language is what LLMs excel at) and solve them successfully even \nwhen not provided with any or only few task-specific (training) examples (in-context \nlearning).\nFew‑shot in‑context learning\nIn-context learning (ICL) aims to induce a model to perform a task by providing a small \nset of input-label examples (so-called “shots”) along with the task description; the query \nsample—the example (input) for which the label is to be generated—is provided at the \nend of the prompt (Dong et al. 2022). In-context learning allows the LLM to understand \nthe task (via the description and a few labeled examples), without supervised fine-tuning \n(i.e., without any updates to the LLM’s parameters).\nFigure 2 illustrates one-shot prompts for the S-NAP and S-DFD tasks. Each prompt \nbegins with a description of the task, followed by a single labeled instance. For the \nS-NAP task, the labeled instance includes a list of possible activities and the prefix trace \nin question, followed by the correct label. In contrast, the S-DFD task only presents the \nlist of possible activities, followed by the corresponding true directly-follows pairs. In \nboth cases, the query instance for which the LLM is expected to generate a solution \nappears at the end of the prompt.\n4 This translation is straightforward as we only retained block-structured workflow nets when creating the corpus. For \nthe details on how this translation exactly works, we refer to work on process discovery approaches, in particular, the \nInductive Miner (Leemans et al. 2013).\nPage 13 of 29\nRebmann et al. Process Science            (2025) 2:10 \n \nFine‑tuning LMs\nFine-tuning is the procedure of further training a pretrained language model, in order \nto specialize it for a specific task. The advantage compared to training a model from \nscratch is that the training data size for fine-tuning is considerably smaller, thus reducing \nresources required to train a task-specific model. We fine-tune (large) language mod -\nels for each of our semantics-aware process mining tasks. While it is possible to train \ndecoder LLMs for both classification and generation tasks we propose, the manner in \nwhich encoders are trained makes it difficult to adapt them for generation, as explained \nin the following sections.\nFine-Tuning LMs on Classification Tasks. Both encoder and decoder models can be \ntailored for classification tasks. We next describe the fine-tuning procedures for (a) dis -\ncriminative classification with encoder language models and (b) generative classification \nwith a decoder language models.\nDiscriminative Fine-Tuning of Encoder LMs. To fine-tune an encoder LM for classifica-\ntion tasks, we extend the model’s base architecture (i.e., the pretrained Transformer net-\nwork) with an additional classification layer: the parameters of the classifier are trained \nfrom scratch (i.e., randomly initialized), whereas the encoder’s parameters are updated \n(i.e., fine-tuned). Fine-tuning of an encoder LM for the T-SAD task is illustrated in Fig. 3 \nusing a record with a trace ⟨create invoice, make payment, ... ⟩ as input. The input is first \nsplit into a sequence of subword tokens.5 The actual input for encoder LMs is commonly \nsurrounded with synthetic sequence start ([CLS]) and sequence end ([SEP]) tokens. \nThe encoder (i.e., the Transformer network) outputs one vector—a transformed/contex-\ntualized representation—for each token in the input sequence, including the sequence \nstart/end tokens. Let xCLS ∈ Rd be the representation of the sequence start token CLS \n(output of the encoder) with d as the hidden size of the encoder’s Transformer network; \nFig. 2 One-shot in-context-learning prompts\n5 For more frequent words in the language, a token will commonly correspond to the whole word; less frequent words, \non the other hand, will often be broken down into more frequent subtokens (e.g., “tokenization” may be segmented into \n“token” and “ization”). The exact subword vocabulary is model dependent, i.e., each LM comes with its own tokenizer.\nPage 14 of 29Rebmann et al. Process Science            (2025) 2:10 \nthis vector xCLS can be seen as a latent semantic representation of the whole input text \nand is forwarded as input to the classifier. The classifier, in turn, is a single-layer feed-\nforward network: ˆy= softmax(W cl · xCLS + bcl) ; Wcl ∈ Rc×d  and bcl ∈ Rc are the train-\nable parameters of the classifier (c is the number of classes in the classification task) and \nsoftmax is the function commonly used to convert real-valued vectors into probability \ndistributions—the final output ˆy is thus a probability distribution over the task’s classes. \nWe train the model (jointly update the parameters of both classifier and encoder in end-\nto-end fashion) by minimizing the widely used cross-entropy loss, i.e., the negative loga -\nrithm of the probability that the model predicted for the true class of the input instance.\nT-SAD and A-SAD are binary classification tasks (i.e., c = 2 ) in which the model pre -\ndicts whether the traces and ordered activity pairs, respectively, are Valid or Anomalous. \nS-NAP is a multi-class classification task in which the set of classes is defined by the \nactivities in AM of the process model M from which the input record was created.\nGenerative Fine-Tuning of Decoder (L)LMs. Autoregressively trained decoder LLMs \ncast classification tasks as language generation tasks. Concretely, each class into which \nthe preceding text is to be classified is assigned one token from the vocabulary and the \nLLM’s language modeling head (a classifier over the LLM’s vocabulary) is supposed to \ngenerate the token of the correct class. For example, for the T-SAD task, we convert \nindividual training instances into prompts that couple (1) the set of process activities \nwith (2) the concrete trace (or activity sequence) that is to be judged as Valid or Anoma-\nlous. Then we append the prompt that asks whether the sequence is anomalous, with the \ntoken true assigned to the Anomalous sequences and token false  to the Valid sequences. \nThe whole input for the decoder LLM for a single sequence is shown below (the label \ntoken is underlined and in bold):\nActivities: {create order, approve order, reject order, create invoice, make payment}\nActivity sequence: [create order, reject order, create invoice, make payment]\nFig. 3 Illustration of discriminative classification with an encoder LM\nPage 15 of 29\nRebmann et al. Process Science            (2025) 2:10 \n \nAnomalous: true\nWe fine-tune a decoder LLM via constrained text generation: given the entire preced -\ning context (everything except the last token that indicates the class), we predict the next \ntoken, but allow the language modeling head to only predict the probabilities for the \nallowed class tokens (in the above example, only true, false), as opposed to LLM pre -\ntraining in which the next token is predicted over the entire vocabulary of the LLM. We \nillustrate constrained generative fine-tuning of a decoder LM in Fig.  4. The Transformer \nnetwork of the decoder produces the output representation by contextualizing all pre -\nceding tokens; the resulting vector is next compared against the representations of the \nallowed class tokens (in the example, true, false) to produce scores that are then con -\nverted into probabilities using softmax. We minimize the negative log likelihood of the \nprobability assigned to the correct class token: as updating all LLM parameters is com -\nputationally infeasible, we resort to parameter-efficient fine-tuning via low-rank adapta -\ntion (LoRA) (Hu et al. 2021).6\nFine-Tuning LLMs on Generation Tasks. In contrast to constraining text generation \nfor classification tasks, for the generation tasks we allow the LLM to produce output \nfreely, conditioned on the provided input context. This requires the LLM to develop a \nmore holistic understanding of the S-DFD and S-PTD tasks. While for classification \nthe outputs are limited to one of a set of predefined labels, the generation tasks involve \ncreating complete answers. The model must account for what parts of the answer have \nalready been generated and what remains to be completed.\nThe decoder LLMs we employ are pre-trained to solve tasks based on minimal con -\ntext during instruction tuning. To adapt these models for our tasks, we fine-tune them \non labeled instances comprising a textual description and a single task instance with \nthe correct solution, i.e., a true DFG or process tree. This approach leverages both the \nFig. 4 Illustration of constrained generative fine-tuning of a decoder LM\n6 For brevity, we refer the reader to the original work for details on LoRA.\nPage 16 of 29Rebmann et al. Process Science            (2025) 2:10 \nextensive knowledge gained during pre-training and the instruction-following capabili -\nties developed through post-hoc alignment. For example, for the S-PTD task, the input \nprovided to the LLM for a single sequence includes the following components (with the \nlabel underlined and highlighted in bold):\nGiven the list of activities that constitute an organizational process, determine the pro -\ncess tree of the process. A process tree is a hierarchical process model.\n[description of the process tree notation]\nMake sure that you use each activity exactly once in the tree and that you set parenthe -\nses are correctly.\nActivities: create order, approve order, reject order, create invoice\nProcess Tree: → (create order , ×(approve order , reject order), create invoice)\nAs illustrated, we provide a detailed textual description of the task (as for ICL), fol -\nlowed by the list of activities for the specific task instance, and conclude with the \nexpected output.\nImportantly, the model is not trained to predict the next token for the provided con -\ntext (the task description and the list of activities). Instead, it is fine-tuned to predict \nall the next tokens that belong to the label , conditioned on the previous tokens in the \ncorrect order. The procedure is depicted in Fig.  5. It ensures that the Transformer net -\nwork generates output tokens iteratively, while taking into account the tokens it already \nproduced as part of its answer. As for the classification tasks, we use LoRA to fine-tune \nLLMs for the generative tasks as well (cf. Fine-tuning LMs section).\nExperimental setup\nIn this section, we first explain how the datasets are split into training, validation, and \ntesting sets, followed by an introduction to the specific language models we use. We \nthen detail our ICL and fine-tuning setups. To ensure reproducibility, we make all train -\ning and evaluation scripts publicly available.7\nDataset Portions. We split all datasets based on the process models from which the \nsamples originate using 70% of instances for training, 20% for validation, and 10% for \nfinal performance evaluation. For these splits, we ensure that no activity sequence from \na model in the training set appears in any of the process models in the validation or test \nsets. This prevents any leakage of process behavior knowledge between the sets, allow -\ning for an accurate assessment of the LMs’ generalization abilities. Additionally, we use \nFig. 5 Illustration of generative fine-tuning of a decoder LM\n7 https:// github. com/a- rebma nn/ llms4 pm; this repository also includes examples of how to use LLMs to solve individual \ninstances of the five tasks.\nPage 17 of 29\nRebmann et al. Process Science            (2025) 2:10 \n \nstratified sampling based on the number of unique activities in the models to ensure a \ncomparable complexity distribution across the splits. The sizes of all splits for the five \ntasks are shown in Table 2.8\nLarge Language Models. We utilize two widely used open decoder-based LLMs that \nhave demonstrated impressive performance on NLP benchmarks: (1) Llama-3 (Llama) \nin its 8 billion parameter version 9 and (2) Mistral-2 (Mistral) in its 7 billion parameter \nversion10. We evaluate both models in few-shot ICL and fine-tuning setups.\nBaselines. We use the following baselines to put the performance of LLMs on the pro-\nposed tasks into context:\nClassification. For the classification tasks, we use three baselines: \n(1) For T-SAD and A-SAD , we compare LLMs with the state-of-the-art approach for \nrule-based semantic anomaly detection (van der Aa et al. 2021). It is important to \nnote that this baseline can only detect semantic anomalies if the respective activi -\nties refer to same business object. Consequently, it automatically classifies pairs of \nactivities with distinct business objects as valid. We evaluate our approach against \nthe best-performing configuration from the original paper, referred to as SEM4  in \ntheir experiments.\n(2) We compare LLMs in an ICL setting (i.e., without task-specific fine-tuning) against \na random classification baseline, which assigns one of the classes of the respective \ntask to each test instance with equal probability per class.\n(3) We compare generatively fine-tuned LLMs against a discriminatively fine-tuned \nencoder LM: specifically, we use RobERTa  (Liu et al. 2019) in its large version, 11 a \nstrong and widely used English-only bidirectional encoder LM. Note that fine-tun -\ning RoBERTa on A-SAD  corresponds to the state-of-the-art approach for detect -\ning anomalous eventually-follows relations (Caspary et al. 2023), with the difference \nbeing that for our experiments, we used a more powerful base model.\nGeneration. For the generation tasks, we established a baseline that randomly selects \none of the behavioral footprint relations (defined in Preliminaries section) for all pairs \nof activities in the respective set of possible activities. Note that the encoder baseline \nTable 2 Training, validation, and test split characteristics per task\nTask Total Train Validation Test\nT-SAD 291,251 227,892 43,609 19,750\nA-SAD 316,308 229,402 56,154 30,752\nS-NAP 1,289,081 1,071,529 166,811 50,741\nS-DFD 15,780 11,397 2,775 1,528\nS-PTD 15,780 11,397 2,775 1,528\n8 Note that the distributions of process types and industries in the dataset portions are similar to the corpus. For the full \ndetails, we refer to our repository.\n9 https:// huggi ngface. co/ meta- llama/ Meta- Llama-3- 8B\n10 https:// huggi ngface. co/ mistr alai/ Mistr al- 7B- Instr uct- v0.2\n11 https:// huggi ngface. co/ Faceb ookAI/ rober ta- large\nPage 18 of 29Rebmann et al. Process Science            (2025) 2:10 \ncannot be used for the generation tasks, due to the non-autoregressive pretraining and \ndiscriminative modeling objectives (cf. LLM-based process mining section).\nNote that direct comparison with statistical baseline methods (e.g., Bezerra and \nWainer (2013) for anomaly detection) is not feasible since these tackle different tasks \nthat take different inputs. In particular, statistical approaches consider an entire event \nlog, capturing data on numerous cases, in order to infer statistical patterns based on \noccurrence frequencies. Therefore, these approaches cannot be applied to the data \nthat we have, where such frequencies are not available. Any comparison to statistical \napproaches would thus not be meaningful and will have a strong self-fulfilling character.\nPerformance Measures. We use the following established performance measures to \nevaluate the classification and discovery tasks.\nClassification. We measure classification performance using the macro F 1-score, so \nthat classes equally contribute to the performance regardless of their size. Macro F 1 is \nthe simple average of per-class F 1 scores, with F 1 of class c being the harmonic mean \nbetween c’s precision and recall.\nGeneration. We measure generation quality with the well-known footprint-based fit -\nness (Carmona et  al. 2018), which can be used to compare sets of allowed execution \nsequences based on the pairwise behavioral footprint relations introduced in Prelimi -\nnaries section. In case of the S-DFD task, we establish the footprint of the gold stand -\nard DFG and the footprint of the LLM-generated DFG, according to the definition of \na footprint in Preliminaries section. Then, the footprint-based fitness is the fraction of \nequal footprint relations in both footprints. For the S-PTD task, we first play-out the \ngold-standard and LLM-generated process trees to obtain sets of allowed execution \nsequences. We use these sets to establish DFGs and then proceed as for the S-DFD task \nto obtain the fitness score.\nIn-Context Learning and Prompt Optimization. We tested multiple task formula -\ntion prompts for each task and selected the one that yielded the best performance on \nthe validation set. We evaluated 6-shot, 10-shot, and 20-shot ICL for the three classi -\nfication tasks: for the binary T-SAD and A-SAD tasks, we evenly balance positive and \nnegative instances; for S-NAP we sample one instance from randomly chosen 6, 10, or \n20 process models, respectively. For the generation tasks, we evaluated 3-shot, 5-shot, \nand 10-shot ICL, which is required due to the longer prompt lengths compared to the \nclassification tasks. Different task description prompts led to marginal performance dif -\nferences. Somewhat surprisingly, prompts with fewer shots produced better validation \nperformance: thus, we finally evaluate 6-shot ICL on our test data for the classification \nand 5-shot for generation tasks.12\nFine-Tuning. We fine-tune Llama and Mistral in batches of two instances with gradi-\nent accumulation over 16 batches, resulting in an effective batch size of 32. We fine-tune \nthe RoBERTa classification baseline in batches of 32 instances as well. All models are \ntrained using the AdamW algorithm (Loshchilov and Hutter 2018), with an initial learn -\ning rate of 1e-5. We fine-tune the LLMs for three epochs and RoBERTa  for ten epochs. \nWe run each combination of task and model three times using different random seeds, \n12 See https:// github. com/a- rebma nn/ llms4 pm for the task prompts we used.\nPage 19 of 29\nRebmann et al. Process Science            (2025) 2:10 \n \ncorresponding to different random initialization of model parameters and shuffling of \ntraining data in each run.\nResults and discussion\nWe first present the performance of LLMs on the classification tasks, followed by their \nperformance on the generation tasks. Next, we provide an in-depth analysis of the lan -\nguage models’ results, and conclude with a discussion of the training effort.\nClassification‑task results\nTable 3 shows the results of our experiments on the classification tasks, along with the \nrule-based SAD baseline, the random classification baseline, the two LLMs (Llama and \nMistral) using the ICL approach, as well as RoBERTa and the LLMs using the fine-tuning \napproach (FT). We report mean macro F1 scores as well as (±) standard deviation vari -\nance over three runs for ICL and three different random seeds for FT.\nIn‑context learning results\nFor ICL, we find that while the performance of the LLMs is consistently better than the \nrule-based baseline, it is at best marginally better (Llama) at worst (Mistral) slightly \nworse than random performance for the two semantic anomaly detection tasks, T-SAD \nand A-SAD. Specifically, Llama achieves a macro F1-score of 0.51 for T-SAD and 0.53 \nfor A-SAD, while Mistral scores 0.44 resp. 0.49. These results indicate that the LLMs \nhave not effectively learned these tasks from the few examples provided in the context. \nFor the S-NAP task, ICL with LLMs does outperform the random baseline, with Llama \nexhibiting much stronger performance (19-point gain over the random baseline) than \nMistral (only 5-point gain). The performance is nonetheless fairly poor in absolute terms \n(mere 0.32 with Llama). These results suggest that these process mining tasks drastically \ndiffer from the language processing tasks on which the LLM instruction-tuning was car -\nried out.\nFine‑tuning results\nPoor ICL performance shows that LLMs a priori know very little about how to solve \nthe proposed classification tasks and thus need to be explicitly trained for them. The \nfine-tuned encoder-LM baseline, i.e., RoBERTa , already achieves drastically better \nTable 3 Classification results (macro F1 scores)\nbest scores are highlighted in bold\nApproach Task\nT‑SAD A‑SAD S‑NAP\nRule-based (van der Aa et al. \n2021)\n0.45 ± 0.000 0.33 ± 0.000 -\nRandom 0.50 ± 0.000 0.50 ± 0.000 0.13 ± 0.000\nICL Mistral 0.49 ± 0.022 0.44 ± 0.011 0.18 ± 0.018\nICL Llama 0.51 ± 0.015 0.53 ± 0.021 0.32 ± 0.054\nFT RoBERTa 0.77 ± 0.006 0.85 ± 0.003 0.63 ± 0.048\nFT Mistral 0.79 ± 0.010 0.88 ± 0.002 0.68 ± 0.039\nFT Llama 0.79 ± 0.011 0.88 ± 0.000 0.69 ± 0.049\nPage 20 of 29Rebmann et al. Process Science            (2025) 2:10 \nperformance than ICL with LLMs: for example, it obtains the F 1-score of 0.77 on the \nT-SAD task, which is an improvement of massive 26 points over the best ICL perfor -\nmance (0.51 by Llama). Fine-tuning the LLMs yields even better performance, with \nLlama and Mistral achieving an F1-score of 0.79, a further 2-point improvement over \nRoBERTa’s performance. The same trend holds for the other two tasks: on A-SAD, both \nMistral and Llama yield very strong performance F1-scores of 0.88, outperforming \nRoBERTa by 3 points; on S-NAP , Llama and Mistral achieve F1-scores of 0.69 and 0.68, \nrespectively (6- and 5-point respective gains over RoBERTa).\nThese results show that decoder-based LLMs can effectively acquire the missing pro -\ncess knowledge through explicit task-specific fine-tuning, yielding better results than \ntheir (smaller) encoder-based counterparts such as RoBERTa . The fine-tuned LLMs \nconsistently outperform RoBERTa on all proposed classification tasks, which points to \nthe benefits of much larger-scale pretraining to which they have been comparatively \nexposed.\nTask comparison\nThe results also demonstrate considerable differences in difficulty between the classifica-\ntion tasks. For A-SAD, the LLMs achieve an impressive F1-score of 0.88, while the maxi-\nmum score for T-SAD is 0.79, and the best model scores only 0.69 for S-NAP . This aligns \nwith expectations. Solving T-SAD requires the model to identify whether process behav-\nior is valid within the context of an entire trace, whereas A-SAD only requires assessing \na single behavioral relation. The S-NAP task is by far the most challenging of the clas -\nsification tasks and simply unsolvable for many instances. For example, consider a pro -\ncess that allows for parallel execution of activities. Then, it is indeterminable—for both \nhumans and automated approaches—which activity occurs next in a trace based solely \non a prefix, as there are multiple valid options.\nGeneration‑task results\nTable 4 shows the results for the generation-based process discovery tasks, along with \nthe random generation baseline. We report mean fitness scores and (±) standard devia -\ntion variance over three runs for ICL and three different random seeds for FT.\nIn‑context learning results\nThe ICL results for the discovery tasks reveal considerable improvements over generat -\ning random behavioral relations for both the S-DFD and S-SPT variations. For example, \nTable 4 Generation Results (Fitness scores)\nbest scores are highlighted in bold\nApproach Task\nS‑DFD S‑PTD\nRandom 0.32 ± 0.008 0.32 ± 0.008\nICL Mistral 0.61 ± 0.008 0.56 ± 0.031\nICL Llama 0.60 ± 0.020 0.52 ± 0.044\nFT Mistral 0.81 ± 0.002 0.84 ± 0.004\nFT Llama 0.80 ± 0.001 0.83 ± 0.015\nPage 21 of 29\nRebmann et al. Process Science            (2025) 2:10 \n \nwhile the random baseline achieves a fitness score of 0.32, Mistral attains a score of \n0.61 for S-DFD, representing an improvement of nearly 100%. For S-PTD, the perfor -\nmance gains are slightly smaller, with Mistral achieving a 0.24 increase and Llama a 0.2 \nincrease. Interestingly, Mistral outperforms Llama on the generation tasks, whereas \nLlama performed better on classification tasks.\nOverall, these findings suggest that the process knowledge encoded in LLMs during \npretraining enhances their ability to address semantic discovery tasks compared to the \nbaseline. The weaker ICL results on the classification tasks further indicate that LLMs \ngrasp the process-related generation tasks better when provided with a handful of exam-\nples compared to the classification tasks. This difference likely stems from the closer \nresemblance of the generation tasks to the types of tasks the LLMs encountered during \npretraining. Although performance gains compared to the random baseline are substan -\ntial, a fitness value of 0.6 on average still leaves room for improvement, suggesting that \nfine-tuning is also required for semantic process discovery tasks.\nFine‑tuning results\nFine-tuning the LLMs on the generation tasks yields considerable performance gains, \nsimilar as for the classification tasks. In particular, Mistral and Llama achieve average \nfitness scores of 0.81 resp. 0.8 for S-DFD and 0.84 resp. 0.83 for S-DPT. These are mas -\nsive improvements of up to 20 points for S-DFD and even 28 points for S-PTD com -\npared to the best ICL performance. When looking at the stability of the results across \nevaluation runs, we find that on average results vary only marginally, with a standard \ndeviation of less than 0.02 for both tasks and LLMs. Given that different models may be \npossible for a given set of activities, we also assess the stability of results for the same \nactivity sets across the three evaluation runs. The results indicate that in more than 60% \nof the cases the results are stable, leading to a standard deviation of 0 across runs. How -\never, in the remaining cases, we observe a positive standard deviation between 0.007 and \n0.43, with a median of 0.12. This indicates the presence of ambiguous activity sets in \nrelation to the resulting model. When examining the results in detail, we find that devia-\ntions of more than 0.25 primarily occur for small activity sets of up to four activities. \nThese sets typically correspond to relatively sequential processes, yet the LLMs gener -\nate models that allow for numerous interleaving relationships between them. Thus, the \nambiguity with respect to the resulting model has a more significant impact on fitness in \nsmaller activity sets, which is in line with expectations.13\nThese results underscore the advantages of fine-tuning LLMs also for process-related \ngeneration tasks. The notable performance improvement in the process tree discovery \ntask demonstrates that fine-tuning is especially beneficial for tasks with multiple facets. \nIn particular, solving S-PTD requires not only a grasp of process semantics but also an \nunderstanding of a process-specific representation format. LLMs can effectively acquire \nboth capabilities through targeted task-specific fine-tuning.\n13 We provide the raw results of this analysis in our repository.\nPage 22 of 29Rebmann et al. Process Science            (2025) 2:10 \nTask comparison\nThe ICL results show that the LLMs can solve the S-DFD task better than S-PTD, \nwhich is in line with expectations, given that the discovery of DFGs is less complex \nthan the discovery of process trees. For instance Llama achieves 0.6 fitness on aver -\nage for S-DFD, yet, only 0.52 for S-PTD. This is in line with expectations, since the \nformer requires an LLM to grasp only pairwise relations, whereas the latter requires \nunderstanding how entire process parts relate to each other, while, in addition, the \nrepresentation format is not trivial.\nHowever, after fine-tuning the LLMs, this trend flips as shown by the performance \nfor the S-PTD task, which exceeds the performance on S-DFD. For example, Llama  \nachieves an average fitness score of 0.8 for S-DFD but an impressive 0.83 for S-PTD. \nThis not only shows that complex process-related generation tasks can be effectively \nlearned by LLMs, but also suggests that, by means of fine-tuning, LLMs can learn the \nrelatively few global behavioral relations in a process more effectively than the many \nlocal pair-wise relations.\nIn‑depth analysis\nTo assess whether the fine-tuned language models handle certain processes of dif -\nferent types and industries more effectively than others, we conducted an in-depth \nanalysis of their performance. Our primary focus is on S-PTD, the most intricate \nsemantics-aware process mining task, where we provide a detailed breakdown across \nthe different process types and industries. To ensure comprehensive coverage, we \nalso include an analysis of T-SAD—as a representative classification task—offering \ndetailed insights into model performance for this task category as well.\nTable 5 Process type fitness scores of FT Mistral for S-PTD and the share of the type in the test set \n(min. 0.5%)\nProcess Type Fitness Share (%)\nManage Enterprise Risk, Compliance,... 0.89 3.80\nMarket and Sell Products and Services 0.86 2.53\nManage External Relationships 0.86 6.33\nDeliver Physical Products 0.86 16.79\nDeliver Services 0.85 8.10\nManage Customer Service 0.85 8.95\nOther 0.82 27.59\nManage Financial Resources 0.82 10.55\nDevelop and Manage Business Capabilities 0.80 1.77\nAcquire, Construct, and Manage Assets 0.78 1.77\nManage Information Technology 0.78 4.05\nDevelop and Manage Human Capital 0.77 5.49\nDevelop and Manage Products and Services 0.73 1.86\nPage 23 of 29\nRebmann et al. Process Science            (2025) 2:10 \n \nIn‑depth analysis of S‑PTD\nWe aim to understand whether performance generally differs between process types \nand industries. To this end, we report on the results of FT Mistral for S-PTD, which \nalign with those for FT Llama. Table  5 shows the results per process type and Table  6 \nper industry.\nWe find that for Deliver Physical Products, with a substantial share of 17%, the model \nachieved a solid fitness score of 0.86. In contrast, Manage Financial Resources saw a \nlower score of 0.82, indicating relatively weaker model performance on more abstract \nfinancial processes compared to tangible operational ones. The model also appears to \nstruggle with more knowledge-intensive processes, such as Develop and Manage Prod -\nucts and Services, scoring below 0.75. Overall, there is no clear correlation between a \nprocess type’s share in the test set and its fitness score, though.\nAmong the industries, Retail  and Banking , with shares of 9.03% and 9.11% respec -\ntively, achieved good fitness scores of 0.88 and 0.84, indicating solid model performance \non processes of these industries. In contrast, lower scores were observed for industries \nsuch as Government (0.73), suggesting difficulties in handling sectors with potentially \nmore complex or specialized processes, which are also underrepresented in both train -\ning and test data. Interestingly, the model also struggled with knowledge-driven indus -\ntries such as Education and Research (0.80) and Professional Services (0.81).\nLooking at individual task instances, we find that in many cases, the LLM-generated \nprocess trees allow for the same execution sequences as the true process trees, which \ndemonstrates the LLMs’ ability to generate semantic process trees.\nFor instance, for the true tree →(Complete purchase request, Send to clerk, Enter into \nsystem, Fax PO , Ship Product, Receive Shipment), Mistral generated an identical tree, \nyielding a fitness of 1. But also beyond purely sequential processes in standard domains \nsuch as purchasing, the LLMs produce perfect outcomes. Examples of this include the \ngrocery checkout process →(Scan Items, Scan Rewards Card and request payment, ×\n(Process Credit Card, Accept Cash), Bag Grocery items) and the risk assessment pro -\ncess →(Risk threshold assessment, ×(Advanced risk assessment, Simple risk assessment), \nPassed assessment, ∧(Notify customer with result, Organize disbursement)). In both \ncases, only syntactical differences were present (such as inverse ordering in exclusive or \nparallel structures).\nTable 6 Industry fitness scores of FT Mistral for S-PTD and the share of the industry in the test set \n(min. 0.5%)\nIndustry Fitness Share (%) Industry Fitness Share (%)\nInsurance 0.93 7.00 Other 0.81 37.64\nMedia,... 0.90 1.01 Healthcare 0.81 9.11\nRetail 0.88 9.03 Pro. services 0.81 2.11\nRetail 0.88 9.03 Automotive 0.81 2.53\nLogistics,... 0.87 2.19 Edu., Research 0.80 5.40\nTravel 0.85 5.06 Cons. products 0.78 1.52\nHigh tech 0.85 0.68 Government 0.73 1.01\nBanking 0.84 9.11 Real estate,... 0.66 0.76\nIndust. manuf. 0.82 3.80\nPage 24 of 29Rebmann et al. Process Science            (2025) 2:10 \nEven for more challenging cases such as a prescription-fulfillment process, the LLM \nachieves high fitness scores (here 0.85). In this case the true tree, →(×(Collect walk-in \nprescription, →(Drop prescription in the appropriate box, Pick up prescriptions in the \nbox)), Enter prescription details, Validate prescription, Check insurance coverage, Fill \nprescription, Prescription pick-up request), only differs from the generated tree, →(×(Col-\nlect walk-in prescription, →(Drop prescription in the appropriate box, Pick up prescrip -\ntions in the box), →(Enter prescription details, Validate prescription, Fill prescription)), \nCheck insurance coverage, Prescription pick-up request), in that insurance coverage must \nbe checked before the prescription is filled in the true tree, whereas the inverse is the \ncase in the generated tree.\nThere are also cases where the LLM generates process trees that diverge from the \ntrue tree. For instance, while the true tree →(Process trip information, Check credit card \ninformation, Process request, Notify customer) is purely sequential, the LLM generated \na tree with a parallel construct ( ∧ ), implying that Process trip information and Check \ncredit card information can happen in any order. This is one on many instances where \nthe generated tree arguably represents a reasonable process, yet, only yields a low fitness \nscore (in this case 0.625). Other examples include a travel-reconciliation process with a \nfitness score of only 0.2. According to the true tree, ∧(Verify accounts, Verify payment \ninformation, Archive the form, Authorize the travel-advance-reconciliation form, Accept \npayment), all activities can be performed in any order, which is clearly problematic. For \ninstance, the verification of payment information should precede the acceptance of pay -\nment. In contrast, the generated tree arranges the process in a purely sequential order: \n→(Verify Accounts, Verify Payment Information, Accept Payment, Authorize the Travel-\nAdvance-Reconciliation Form, Archive the Form). Although this sequential arrangement \nis not ideal either, both this and the previous example illustrate that the process models \nin the employed collection do not always represent semantically correct models 14, and \nthat there are cases where multiple acceptable solutions exist.\nIn‑depth analysis of T‑SAD\nFor the T-SAD classification task, we find that the LLMs accurately detect anomalous \ntraces across a wide range of process types and industries as well. For this classification \ntask, we also relate the performance to encoder baseline, RoBERTa . Both the LLM and \nRoBERTa appear to be particularly effective in identifying anomalies for standard pro -\ncess types. For example, in a claim-handling process, they correctly identify anomalies \nsuch as ⟨Enter and verify claim, Handle payment, Assess claim⟩ , where the claim should \nbe assessed before sending a payment. They also correctly detect that the trace ⟨Confirm \norder, Ship product, Get shipment address, Emit Invoice, Receive Payment⟩ is anomalous \ngiven that the product is shipped before the address is determined in this order-handling \nprocess.\nFor more specialized industries such as healthcare, LLMs often outperform RoB -\nERTa. For instance, Llama correctly identifies the trace ⟨Arrival, Treatment, Triage, Dis-\ncharge, Invoicing⟩ of a hospital process as anomalous, since Triage  should occur before \n14 We aim to mitigate this issue in the future by establishing additional evaluation data based on a quality-assured model \ncollection (see also Conclusion section).\nPage 25 of 29\nRebmann et al. Process Science            (2025) 2:10 \n \nTreatment. In contrast, RoBERTa  fails to detect this anomaly. Conversely, RoBERTa  \nincorrectly flags the trace ⟨Disassemble system, Refurbish materials, Clean and paint \ncovers, Mount materials, Move to bay, Calibrate, Handover⟩ of a refurbishing process as \nanomalous, even though it is valid, whereas the fine-tuned LLM correctly classifies this \ntrace as valid.\nFinally, there are instances where both the LLMs and RoBERTa  incorrectly identify \nvalid traces as anomalous. For example, both LLM and RoBERTa  flag the trace ⟨Receive \ninvoices of partners, Handle payment of customer, Receive review, Send payment to part-\nners⟩ as anomalous, even though it is valid. According to the corresponding process \nmodel in the corpus, a review can be received at any point during an execution of the \nprocess, making this trace valid. However, this specificity might also be challenging for a \nhuman to determine without further contextual information.\nTraining effort\nFinally, we consider the effort required to train the language models on the tasks. Since \nin-context learning does not require any training effort, as the task-specific knowledge \nis provided at inference time, we focus on the training effort of fine-tuning an LLMs. \nFor the classification tasks, we can also put this effort into context by comparing against \nfine-tuning an encoder (RoBERTa).\nTable 7 shows the run times for fine-tuning the LLMs (Llama and Mistral) for the dif -\nferent tasks per epoch, i.e., pass over all training samples. As shown, the LLMs require a \nconsiderable amount of time for training across tasks. The training time varies between \nthree hours per epoch for the semantic process tree discovery task and up to 23 hours \nfor the next activity prediction task. This variance is predominantly caused by the con -\nsiderably different number of training samples per task, which ranges from roughly 16 \nthousand for the generation tasks and 1.3 million for the next activity prediction task.\nWhen comparing the training effort for Llama on the classification tasks with the \nencoder baseline, we observe substantial differences. In particular, training Llama on the \ntasks takes up to 25 times longer than training RoBERTa  per epoch on the same data. \nFor instance, while fine-tuning Llama for A-SAD takes 15 hours, RoBERTa requires only \naround 40 minutes per epoch. This difference can be attributed to the huge number of \nparameters that need to be updated for the LLM during fine-tuning, even when using \nparameter-efficient fine-tuning. However, it is worth stressing that the LLM requires \nconsiderably fewer epochs to converge in terms of validation loss across tasks. This indi -\ncates that it not only learns the tasks better (as shown in the previous subsections), but \nalso with fewer passes over the training data.\nTable 7 Average run times for fine-tuning (per epoch)\nApproach Task\nT‑SAD A‑SAD S‑NAP S‑DFD S‑PTD\nFT Mistral 9.9 h 14.3 h 21.9 h 4.6 h 3.3 h\nFT Llama 11.1 h 15.0 h 23.0 h 4.2 h 3.0 h\nFT RoBERTa 0.5 h 0.6 h 1.3 h - -\nPage 26 of 29Rebmann et al. Process Science            (2025) 2:10 \nRelated work\nThe natural language understanding capabilities of neural language models have been \napplied to various process analysis tasks, such as, extracting process information \nfrom text (Bellan et al. 2022, 2021), annotating event logs with semantic information \n(Rebmann and van der Aa 2022), detecting semantic anomalies (Caspary et al. 2023; \nBusch et  al. 2024), and generating event logs from textual records of process steps \n(Kecht et al. 2021).\nApplication of LLMs for Process Analysis. With the advent of LLMs, research -\ners have increasingly explored their potential in process analysis (Estrada-Torres et al. \n2024). Key applications include transforming textual process descriptions into for -\nmal process models (Grohs et al. 2023; Kourani et al. 2024b; Klievtsova et al. 2023; \nZiche and Apruzzese 2024; Nour Eldin et  al. 2024), generating textual descriptions \nfrom process data such as models and event logs (Berti et al. 2023), identifying bottle -\nnecks or undesired process behaviors (Berti et al. 2023), and abstracting fine-granular \nevents into higher-level ones (Fani Sani et al. 2023). However, much of this research \nhas been conducted using closed-source GPT models or proprietary software prod -\nucts like ChatGPT, which limits the ability to perform structured and reproducible \nevaluations (Estrada-Torres et al. 2024).\nEvaluation of LLMs on Process Analysis Tasks. The lack of rigorous evaluations \nof LLMs for process mining tasks has recently been highlighted within the process \nmining community (Estrada-Torres et al. 2024; Berti et al. 2024a). In response, Berti \net  al. introduced a benchmark for process mining analysis questions (Berti et  al. \n2024b). This benchmark comprises 52 prompts used to query various LLMs, with \ntheir responses evaluated by a closed-source GPT model. Although the benchmark \nprovides interesting insights, using an LLM to rate the results can yield biased out -\ncomes. Such bias arises, e.g., from the tendency of LLMs to favor their own output \n(Panickssery et al. 2024). Beyond this process-mining-specific effort, Busch and Leo -\npold (2024) present a LLM-benchmark for a set of business process management \ntasks, including the recommendation of activities during the modeling of processes \nand the identification candidates for robotic process automation. Focusing on process \nmodeling, Kourani et  al. (2024a) present a benchmark that evaluates various LLMs \nusing a set of 20 curated business processes.\nIn contrast to these works, we define process mining tasks that benefit from an \nunderstanding of process behavior. Furthermore, we evaluate LLMs using extensive \ntask-specific benchmarking datasets in both, in-context learning and fine-tuning set -\ntings. The latter has not been investigated by other works, yet, as our experiments \nshow, can be highly beneficial for challenging process-related tasks. Finally, it is \nimportant to note that our datasets provide gold standards, which allows for using \nestablished evaluation measures for classification tasks, eliminating the need for a \nproxy LLM to assess output quality.\nPage 27 of 29\nRebmann et al. Process Science            (2025) 2:10 \n \nConclusion\nIn this paper, we investigated the capabilities of LLMs to solve semantics-aware \nprocess mining tasks, i.e., tasks that benefit from an understanding of the meaning \nof process steps and their relationships. We defined five such tasks and provide an \nextensive benchmarking dataset for each of them. Our evaluation experiments that \nuse these datasets show that LLMs fail to solve the tasks in in-context learning set -\ntings. However, our results demonstrate that LLMs achieve accurate performance \nwhen fine-tuned for these tasks. Furthermore, they surpass smaller, encoder-based \nlanguage models in both scope (the types of tasks they can solve) and accuracy (the \nquality of how they solve tasks).\nIn the future, we want to investigate the integration of state-of-the-art process \nmining approaches with LLMs. Since we have shown that LLMs can solve seman -\ntics-aware process mining tasks through encoded knowledge of process semantics, \nintegrating existing process mining approaches with LLMs may yield performance \nimprovements for classical process mining tasks they address. For example, an exist -\ning next-activity prediction approach could be extended by an LLM-based semantic \ncheck that rejects predictions that do not make sense, thereby improving the over -\nall prediction performance. Furthermore, we plan to perform evaluation experiments \non additional data. In particular, we want to generate a process behavior corpus and \nbenchmarking data sets based on a large cross-domain collection of real-world refer -\nence process models, in order to investigate the generalizability of our results beyond \nthe academic process model collection that we employed in this work. Finally, we aim \nto take first steps towards process fine-tuning, i.e., creating LLMs that are specialized \nfor handling not one, but many process analysis tasks. More concretely, instead of \nfine-tuning an LLM for a specific semantics-aware process mining task that it can \nthen solve better, our goal is to fine-tune a model on task-solution pairs of many pro -\ncess analysis tasks, improving its performance on process-related tasks in general, \nsuch as, for instance, envisioned as part of a Large Process Model (Kampik et al. 2024).\nAcknowledgements\nOur research was supported by the state of Baden-Württemberg through bwHPC.\nAuthors’ contributions\nA.R. contributed to the conceptualization, methodology, implementation, data curation, analysis, and both writing the \noriginal draft and review & editing of the manuscript. F.S. contributed to implementation, analysis, and to writing the \noriginal draft. G.G. contributed to writing the original draft and reviewing & editing the manuscript. H.A. contributed to \nthe conceptualization, methodology, and both writing the original draft and reviewing & editing the manuscript.\nData availability\nOur training and evaluation scripts are accessible via the project repository linked in Experimental setup section. The \nprocess behavior corpus and benchmarking datasets are published separately at https:// zenodo. org/ recor ds/ 14273 161 \n(Rebmann et al. 2024b).\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nCompeting interests\nThe corresponding author works for a company that builds process mining software.\nReceived: 14 December 2024   Accepted: 18 May 2025\n\nPage 28 of 29Rebmann et al. Process Science            (2025) 2:10 \nReferences\nBellan P , Dragoni M, Ghidini C (2022) Extracting business process entities and relations from text using pre-trained \nlanguage models and in-context learning. In: International Conference on Enterprise Design, Operations, and \nComputing. Springer Cham, pp 182–199\nBellan P , Dragoni M, Ghidini C, Aa H, Ponzetto SP (2021) Process extraction from text: benchmarking the state of the art \nand paving the way for future challenges. arXiv preprint arXiv:2110.03754\nBerti A, Kourani H, Häfke H, Li C-Y, Schuster D (2024a) Evaluating large language models in process mining: Capabilities, \nbenchmarks, and evaluation strategies. In: BPMDS. Springer, Cham, pp 13–21\nBerti A, Kourani H, van der Aalst WM (2024b) PM-LLM-Benchmark: Evaluating large language models on process mining \ntasks. arXiv:2407.13244\nBerti A, Schuster D, van der Aalst WM (2023) Abstractions, scenarios, and prompt definitions for process mining with llms: \nA case study. In: BPM Workshops. Springer, Cham, pp 427–439\nBezerra F, Wainer J (2013) Algorithms for anomaly detection of traces in logs of process aware information systems. Inf \nSyst 38(1):33–44\nBrown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P et al (2020) Language models are few-shot learners. Adv \nNeural Inf Process Syst 33:1877–1901\nBusch K, Kampik T, Leopold H (2024) xsemad: Explainable semantic anomaly detection in event logs using sequence-to-\nsequence models. In: International Conference on Business Process Management. Springer, Cham, pp 309–327\nBusch K, Leopold H (2024) Towards a benchmark for large language models for business process management \ntasks. arXiv preprint arXiv:2410.03255\nCarmona J, van Dongen B, Solti A, Weidlich M (2018) Conformance checking: Relating Processes and Models. Springer, \nCham, vol 56\nCaspary J, Rebmann A, van der Aa H (2023) Does this make sense? Machine learning-based detection of semantic \nanomalies in business processes. In: BPM. Springer, Cham, pp 163–179\nDevlin J, Chang MW, Lee K, Toutanova K (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Under-\nstanding. In: NAACL. ACL, Kerrville, TX, pp 4171–4186\nDong Q, Li L, Dai D, Zheng C, Wu Z, Chang B et al (2022) A survey on in-context learning. arXiv:2301.00234\nEstrada-Torres B, del Río-Ortega A, Resinas M (2024) Mapping the landscape: Exploring large language model applica-\ntions in business process management. In: BPMDS, Springer, Cham, pp 22–31\nEvermann J, Rehse JR, Fettke P (2017) Predicting process behaviour using deep learning. Dec Support Syst 100:129–140\nFani Sani M, Sroka M, Burattin A (2023) Llms and process mining: Challenges in rpa: Task grouping, labelling and connec-\ntor recommendation. In: Process Mining Workshops. Springer, Cham, pp 379–391\nGrohs M, Abb L, Elsayed N, Rehse J-R (2023) Large language models can accomplish business process management \ntasks. In: BPM Workshops. Springer, Cham, pp 453–465\nHu EJ, Wallis P , Allen-Zhu Z, Li Y, Wang S, Wang L et al (2021) Lora: Low-rank adaptation of large language models. In: \nInternational Conference on Learning Representations. ICLR\nJessen U, Sroka M, Fahland D (2023) Chit-chat or deep talk: Prompt engineering for process \nmining. preprint arXiv:2307.09909\nKampik T, Warmuth C, Rebmann A, Agam R, Egger LN, Gerber A et al (2024) Large process models: A vision for business \nprocess management in the age of generative AI. KI-Künstliche Intelligenz, Springer, Cham, pp 1–15\nKecht C, Egger A, Kratsch W, Röglinger M (2021) Event log construction from customer service conversations using \nnatural language inference. In: ICPM. IEEE, pp 144–151\nKlievtsova N, Benzin JV, Kampik T, Mangler J, Rinderle-Ma S (2023) Conversational process modelling: state of the art, \napplications, and implications in practice. In: International Conference on Business Process Management. Springer, \nCham, pp 319–336\nKourani H, Berti A, Schuster D, van der Aalst WM (2024a) Evaluating large language models on business process mod-\neling: Framework, benchmark, and self-improvement analysis. arXiv preprint arXiv:2412.00023\nKourani H, Berti A, Schuster D, van der Aalst WM (2024b) Process modeling with large language models. arXiv:2403.07541\nLeemans SJ, Fahland D, Van Der Aalst WM (2013) Discovering block-structured process models from event logs-a con-\nstructive approach. In: Application and Theory of Petri Nets and Concurrency: 34th International Conference, PETRI \nNETS 2013, Springer, Cham, pp 311–329\nLiu Y, Ott M, Goyal N, Du J, Joshi M, Chen D et al (2019) Roberta: A robustly optimized bert pretraining \napproach. arXiv:1907.11692\nLoshchilov I, Hutter F (2018) Decoupled weight decay regularization. In: International Conference on Learning \nRepresentations\nNeu DA, Lahann J, Fettke P (2022) A systematic literature review on state-of-the-art deep learning methods for process \nprediction. Artif Intell Rev 55(2):801–827\nNorouzifar A, Kourani H, Dees M, van der Aalst WM (2024) Bridging domain knowledge and process discovery using large \nlanguage models. arXiv preprint arXiv:2408.17316\nNour Eldin A, Assy N, Anesini O, Dalmas B, Gaaloul W (2024) A decomposed hybrid approach to business process mod-\neling with llms. In: International Conference on Cooperative Information Systems. Springer, Cham, pp 243–260\nPanickssery A, Bowman SR, Feng S (2024) LLM evaluators recognize and favor their own \ngenerations. preprint arXiv:2404.13076\nPfeiffer P , Lahann J, Fettke P (2021) Multivariate business process representation learning utilizing gramian angular fields \nand convolutional neural networks. In: International Conference on Business Process Management. Springer, Cham, \npp 327–344\nRebmann A, Schmidt FD, Glavaš G, van der Aa H (2024a) Evaluating the ability of llms to solve semantics-aware process \nmining tasks. In: 2024 6th International Conference on Process Mining (ICPM). IEEE, pp 9–16\nRebmann A, Schmidt FD, Glavaš G, van der Aa H (2024b) Process behavior corpus and benchmarking datasets. Zenodo. \nhttps:// zenodo. org/ recor ds/ 14273 161. Accessed 23 May 2025\nPage 29 of 29\nRebmann et al. Process Science            (2025) 2:10 \n \nRebmann A, van der Aa H (2022) Enabling semantics-aware process mining through the automatic annotation of event \nlogs. Inf Syst 110:102111\nSola D, Warmuth C, Schäfer B, Badakhshan P , Rehse J-R, Kampik T (2023) SAP Signavio academic models: A large process \nmodel dataset. In: ICPM Workshops. Springer, Cham, pp 453–465\nTouvron H, Lavril T, Izacard G, Martinet X, Lachaux M-A, Lacroix T et al (2023) Llama: Open and efficient foundation \nlanguage models. arXiv:2302.13971\nvan der Aa H, Rebmann A, Leopold H (2021) Natural language-based detection of semantic execution anomalies in \nevent logs. Inf Syst 102:101824\nvan der Aalst WM (2022a) Foundations of process discovery. In: Process Mining Handbook. Springer, Cham, pp 37–75\nvan der Aalst WM (2022b) Process mining: a 360 degree overview. In: Process Mining Handbook. Springer, Cham, pp \n3–34\nvan der Aalst WMP , de Medeiros AKA (2005) Process mining and security: Detecting anomalous process executions and \nchecking process conformance. Electron Notes Theor Comput Sci 121:3–21\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN et al (2017) Attention is all you need. Adv Neural Inform \nProcess Syst 30:1–11\nWang Y, Mishra S, Alipoormolabashi P , Kordi Y, Mirzaei A, Arunkumar A et al (2022) Super-naturalinstructions: Generaliza-\ntion via declarative instructions on 1600+ nlp tasks. In: EMNLP . ICLR\nZhang S, Dong L, Li X, Zhang S, Sun X, Wang S et al (2023) Instruction tuning for large language models: A \nsurvey. arXiv:2308.10792\nZiche C, Apruzzese G (2024) Llm4pm: A case study on using large language models for process modeling in enterprise \norganizations. In: International Conference on Business Process Management. Springer, Cham, pp 472–483\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.709452748298645
    },
    {
      "name": "Process (computing)",
      "score": 0.6045847535133362
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.6033415198326111
    },
    {
      "name": "Natural language processing",
      "score": 0.4961870312690735
    },
    {
      "name": "Programming language",
      "score": 0.42382222414016724
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35687708854675293
    }
  ],
  "institutions": [],
  "cited_by": 5
}