{
  "title": "Large language models in medical education: a comparative cross-platform evaluation in answering histological questions",
  "url": "https://openalex.org/W4412367130",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A219480364",
      "name": "Volodymyr Mavrych",
      "affiliations": [
        "Alfaisal University"
      ]
    },
    {
      "id": "https://openalex.org/A3212861000",
      "name": "Einas M Yousef",
      "affiliations": [
        "Alfaisal University"
      ]
    },
    {
      "id": "https://openalex.org/A2240165648",
      "name": "Ahmed Yaqinuddin",
      "affiliations": [
        "Alfaisal University"
      ]
    },
    {
      "id": "https://openalex.org/A2045828954",
      "name": "Olena Bolgova",
      "affiliations": [
        "Alfaisal University"
      ]
    },
    {
      "id": "https://openalex.org/A219480364",
      "name": "Volodymyr Mavrych",
      "affiliations": [
        "Alfaisal University"
      ]
    },
    {
      "id": "https://openalex.org/A3212861000",
      "name": "Einas M Yousef",
      "affiliations": [
        "Alfaisal University"
      ]
    },
    {
      "id": "https://openalex.org/A2240165648",
      "name": "Ahmed Yaqinuddin",
      "affiliations": [
        "Alfaisal University"
      ]
    },
    {
      "id": "https://openalex.org/A2045828954",
      "name": "Olena Bolgova",
      "affiliations": [
        "Alfaisal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4406871476",
    "https://openalex.org/W4362467055",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4408155423",
    "https://openalex.org/W4410280049",
    "https://openalex.org/W4389992676",
    "https://openalex.org/W4402825292",
    "https://openalex.org/W4406517998",
    "https://openalex.org/W4402581085",
    "https://openalex.org/W4399774223",
    "https://openalex.org/W4399701665",
    "https://openalex.org/W4376640725",
    "https://openalex.org/W4402709868",
    "https://openalex.org/W4393049004",
    "https://openalex.org/W3189462808",
    "https://openalex.org/W4406371336",
    "https://openalex.org/W4391145008",
    "https://openalex.org/W4404621271",
    "https://openalex.org/W4391225179",
    "https://openalex.org/W4327518740",
    "https://openalex.org/W4406023834",
    "https://openalex.org/W4409345163",
    "https://openalex.org/W4403717095",
    "https://openalex.org/W4408251534",
    "https://openalex.org/W4407251580",
    "https://openalex.org/W4398759614",
    "https://openalex.org/W4391644915",
    "https://openalex.org/W4405852962"
  ],
  "abstract": "Large language models (LLMs) have shown promising capabilities across medical disciplines, yet their performance in basic medical sciences remains incompletely characterized. Medical histology, requiring factual knowledge and interpretative skills, provides a unique domain for evaluating AI capabilities in medical education. To evaluate and compare the performance of five current LLMs: GPT-4.1, Claude 3.7 Sonnet, Gemini 2.0 Flash, Copilot, and DeepSeek R1 on correctly answering medical histology multiple choice questions (MCQs). This cross-sectional comparative study used 200 USMLE-style histology MCQs across 20 topics. Each LLM completed all the questions in three separate attempts. Performance metrics included accuracy rates, test-retest reliability (ICC), and topic-specific analysis. Statistical analysis employed ANOVA with post-hoc Tukey's tests and two-way mixed ANOVA for system-topic interactions. All LLMs achieved exceptionally high accuracy (Mean 91.1%, SD 7.2). Gemini performed best (92.0%), followed by Claude (91.5%), Copilot (91.0%), GPT-4 (90.8%), and DeepSeek (90.3%), with no significant differences between systems (<i>p</i> > 0.05). Claude showed the highest reliability (ICC = 0.931), followed by GPT-4 (ICC = 0.882). Complete accuracy and reproducibility (100%) were detected in Histological Methods, Blood and Hemopoiesis, and Circulatory System, while Muscle tissue (76.0%) and Lymphoid System (84.7%) presented the greatest challenges. LLMs demonstrate exceptional accuracy and reliability in answering histological MCQs, significantly outperforming other medical disciplines. Minimal inter-system variability suggests technological maturity, though topic-specific challenges and reliability concerns indicate the continued need for human expertise. These findings reflect rapid AI advancement and identify histology as particularly suitable for AI-assisted medical education.<b>Clinical trial number</b>: The clinical trial number is not pertinent to this study as it does not involve medicinal products or therapeutic interventions.",
  "full_text": null,
  "topic": "Medical education",
  "concepts": [
    {
      "name": "Medical education",
      "score": 0.5518803596496582
    },
    {
      "name": "Computer science",
      "score": 0.4474746882915497
    },
    {
      "name": "Medicine",
      "score": 0.35899144411087036
    },
    {
      "name": "Mathematics education",
      "score": 0.3376994729042053
    },
    {
      "name": "Medical physics",
      "score": 0.33030515909194946
    },
    {
      "name": "Psychology",
      "score": 0.3062664270401001
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I134359838",
      "name": "Alfaisal University",
      "country": "SA"
    }
  ]
}