{
  "title": "Improving Retrieval Augmented Neural Machine Translation by Controlling Source and Fuzzy-Match Interactions",
  "url": "https://openalex.org/W4386566558",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2137532077",
      "name": "Cuong Hoang",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Devendra Sachan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116643131",
      "name": "Prashant Mathur",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1969842422",
      "name": "Brian Thompson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2171490998",
      "name": "MARCELLO FEDERICO",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2922349260",
    "https://openalex.org/W4226069413",
    "https://openalex.org/W3176913643",
    "https://openalex.org/W832270446",
    "https://openalex.org/W2757592053",
    "https://openalex.org/W2905342727",
    "https://openalex.org/W3175863856",
    "https://openalex.org/W4287649493",
    "https://openalex.org/W2113549939",
    "https://openalex.org/W2952317054",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W2618463334",
    "https://openalex.org/W3174160883",
    "https://openalex.org/W3034640977",
    "https://openalex.org/W4245202262",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2886342729",
    "https://openalex.org/W2788330850",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2885950361",
    "https://openalex.org/W2257408573",
    "https://openalex.org/W2945383715",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970059135",
    "https://openalex.org/W3167591218",
    "https://openalex.org/W2567571499",
    "https://openalex.org/W2891713103",
    "https://openalex.org/W3204406378",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W4293088443",
    "https://openalex.org/W2803241009",
    "https://openalex.org/W2239847623",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2963829526",
    "https://openalex.org/W3130523865"
  ],
  "abstract": "We explore zero-shot adaptation, where a general-domain model has access to customer or domain specific parallel data at inference time, but not during training. We build on the idea of Retrieval Augmented Translation (RAT) where top-k in-domain fuzzy matches are found for the source sentence, and target-language translations of those fuzzy-matched sentences are provided to the translation model at inference time. We propose a novel architecture to control interactions between a source sentence and the top-k fuzzy target-language matches, and compare it to architectures from prior work. We conduct experiments in two language pairs (En-De and En-Fr) by training models on WMT data and testing them with five and seven multi-domain datasets, respectively. Our approach consistently outperforms the alternative architectures, improving BLEU across language pair, domain, and number k of fuzzy matches.",
  "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 289–295\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nImproving Retrieval Augmented Neural Machine Translation by\nControlling Source and Fuzzy-Match Interactions\nCuong Hoang∗, Devendra Sachan∗, Prashant Mathur, Brian Thompson, Marcello Federico\nAWS AI Labs\npramathu@amazon.com\nAbstract\nWe explore zero-shot adaptation, where a\ngeneral-domain model has access to customer\nor domain specific parallel data at inference\ntime, but not during training. We build on\nthe idea of Retrieval Augmented Translation\n(RAT) where top-k in-domain fuzzy matches\nare found for the source sentence, and target-\nlanguage translations of those fuzzy-matched\nsentences are provided to the translation model\nat inference time. We propose a novel archi-\ntecture to control interactions between a source\nsentence and the top-k fuzzy target-language\nmatches, and compare it to architectures from\nprior work. We conduct experiments in two\nlanguage pairs (En-De and En-Fr) by training\nmodels on WMT data and testing them with\nfive and seven multi-domain datasets, respec-\ntively. Our approach consistently outperforms\nthe alternative architectures, improving BLEU\nacross language pair, domain, and number k\nof fuzzy matches with almost no trade-off on\ninference latency.\n1 Introduction\nDomain adaptation techniques such as fine-tuning\n(Freitag and Al-Onaizan, 2016; Luong and Man-\nning, 2015) are highly effective at increasing in-\ndomain performance of neural machine translation\n(NMT) systems, but are impractical in many re-\nalistic settings. For example, consider a single\nmachine serving translations to thousands of cus-\ntomers, each with a private Translation Memory\n(TM). In this case, adapting, storing and loading\nlarge adapted models for each customer is compu-\ntationally infeasible. In this paper we thus consider\nzero-shot adaptation instead, with a single general-\ndomain model trained from heterogeneous sources\nthat has access to the customer or domain specific\nTM only at inference time.\n∗Work done while the authors were at AWS AI Labs.\nOur work builds on Retrieval Augmented Trans-\nlation (RAT) (Li et al., 2022; Bulte and Tezcan,\n2019; Xu et al., 2020; He et al., 2021; Cai et al.,\n2021), a paradigm which combines a translation\nmodel (Vaswani et al., 2017) with an external re-\ntriever module that retrieves the top-kmost similar\nsource sentences from a TM (i.e. \"fuzzy matches\")\n(Farajian et al., 2017; Gu et al., 2017; Bulte and\nTezcan, 2019). The encoder encodes the input sen-\ntence along with the translations of the top-kfuzzy-\nmatches and passes the resulting encodings to the\ndecoder.\nPrior RAT methods for NMT have fallen into\ntwo camps: Early work (Bulte and Tezcan, 2019;\nZhang et al., 2018) concatenated the source sen-\ntence and the top-k fuzzy matches before encoding,\nrelying on the encoder’s self-attention to compare\nthe source sentence to each target sentences and\ndetermine which target phrases are relevant for the\ntranslation. More recent work (He et al., 2021; Cai\net al., 2021) has opted to encode the source sen-\ntences and the top-k fuzzy matches independently,\neffectively shifting the entire burden of determin-\ning which target phrases are relevant to the decoder.\nWe hypothesize that neither approach is ideal: In\nthe first, the encoder has access to the information\nthat we expect to be important (namely, the source\nand the fuzzy matches), but the self-attention also\nhas potentially confusing/spurious connections. In\nthe second, the encoder lacks the self-attention con-\nnections between the source and the fuzzy matches.\nTo address these issues, we propose a novel ar-\nchitecture which has self-attention connections be-\ntween the source sentence and each fuzzy-match,\nbut not between fuzzy-matches. We denote this\nmethod RAT with Selective Interactions (RAT-\nSI). Our method is illustrated in Figure 1, along\nwith two previously discussed approaches.\nExperiments in five English-German (En-De)\ndomain-specific test sets (Aharoni and Goldberg,\n2020) and seven English-French (En-Fr) domain\n289\nFigure 1: Architectures for retrieval augmented NMT. Left: Plain transformer ingesting source and retrieved fuzzy\nmatches concatenated with a separator symbol (Bulte and Tezcan, 2019), denoted herein as RAT-CAT. Center:\nTransformer with dual encoder, one for encoding the source and one for encoding each retrieved fuzzy-matches,\ninspired by He et al. (2021), denoted herein as RAT-SEP. Right: Transformer separately encoding the source and\neach source + fuzzy-match pair (this work), denoted herein as RAT-SI.\nspecific test sets (Pham et al., 2021), for k =\n{3,4,5}, demonstrate that our proposed method\noutperforms both prior approaches in 32 out of\n36 cases considered. The proposed method out-\nperforms the closest competitor by +0.82 to +1.75\nBLEU for En-De and +1.57 to +1.93 for En-Fr.\n2 Method\nTo isolate the effects of the underlying modeling\nstrategy from the various tricks and implementation\ndetails employed in prior papers, we build baseline\nmodels which distill the two primary modeling\nstrategies used in prior works:\nThe first concatenates a source sentence with\ntarget-language fuzzy matches and then encodes\nthe entire sequence, as in Bulte and Tezcan (2019)\nand Xu et al. (2020). In this approach, the cross-\nattention of the encoder must learn to find the rel-\nevant parts of target-language fuzzy-matches by\ncomparing each fuzzy-match to the source sen-\ntence, while ignoring potential spurious fuzzy-\nmatch to fuzzy-match interactions (see the left di-\nagram in Figure 1). We denote this method RAT-\nCAT.\nThe second encodes the source and each target-\nlanguage fuzzy-match separately (with two distinct\nencoders), and instead concatenates the encoded\nrepresentations, inspired by He et al. (2021) and\nCai et al. (2021). In this approach, the spurious\nconnections between the target-language fuzzy-\nmatches are eliminated, but the connections be-\ntween the source and each fuzzy-match are also\neliminated, forcing the attention in the decoder to\nfind the relevant portions in the fuzzy-match that\nare relevant to the source (see the center diagram\nin Figure 1). We denote this method RAT-SEP.\nFinally, we propose a third method which at-\ntempts to build on the strengths of each of the prior\nmethods. As in RAT-SEP, our method separately\nencodes (with the same encoder) the source and\neach target-language fuzzy-match; however, each\nfuzzy-match is jointly encoded with a copy of the\nsource, as in RAT-CAT, allowing the encoder to\nfind portions of the fuzzy-match which are relevant\nto the input. Finally, all the encoded inputs are\nconcatenated and exposed to the decoder; How-\never, the encoding of the source is only provided\nto the encoder once, to avoid potentially spurious\ninteractions between copies of the input (see the\nright diagram in Figure 1). We denote our proposed\nmethod RAT-SI.\n3 Experimental Setup\nOur experiments are in two language directions:\nEnglish-German (En-De) and English-French (En-\nFr). We train models using the public WMT\n2014 (Bojar et al., 2014) data set, with 4.5M En-\nDe sentences and 36M En-Fr sentences.\nDuring training, the model sees target-language\nfuzzy-match sentences from the same dataset it is\nbeing trained on (i.e. WMT14), but at inference,\nmodels must perform zero-shot adaptation to five\nEn-De domain-specialized TMs1 and seven En-Fr\ndomain-specialized TMs. 2 En-De data is taken\n1Medical, Law, IT, Religion and Subtitles.\n2News, Medical, Bank, Law, IT, TED and Religion.\n290\nfrom Aharoni and Goldberg (2020), which is a\nre-split version of the multi-domain data set from\nKoehn and Knowles (2017) while En-Fr data set is\ntaken from the multi-domain data set of Pham et al.\n(2021).\nTo find target-language fuzzy matches for our\nmodel from domain specific TMs, we use Okapi\nBM25 (Robertson and Zaragoza, 2009), a classical\nretrieval algorithm that performs search by comput-\ning lexical matches of the query with all sentences\nin the evidence, to obtain top-ranked sentences for\neach input. To enable fast retrieval, we leverage the\nimplementation provided by the ElasticSearch li-\nbrary.3 Specifically, we built an index using source\nsentences of each TM, and for every input source\nsentence, we collect top-ksimilar source side sen-\ntences and then use their corresponding target side\nsentences as inputs to the model.\nTo explore how each method performs (and how\nrobust they are) under different conditions, we run\na full set of experiments fork= {3,4,5}. We train\nseparate models for each language pair andkvalue,\nand then apply that model to each of the 5 (En-De)\nor 7 (En-Fr) domains.\nWe report translation quality with BLEU scores\ncomputed via Sacrebleu (Post, 2018).4 We use\ncompare-mt (Neubig et al., 2019) to perform pair-\nwise significance testing with bootstrap = 1000\nand prob_thresh = 0.05 for all pairs.\nAll models employed transformers (Vaswani\net al., 2017) with 6 encoder and 6 decoder lay-\ners. Hidden size was set to 1024 and maximum\ninput length truncated to 1024 tokens. All models\nemployed a joint source-target language subword\nvocabulary of size 32Kusing Sentencepiece algo-\nrithm (Kudo and Richardson, 2018).\nWe use the Adam optimizer (Kingma and Ba,\n2015) with β1 = 0.9, β2 = 0.98 and ϵ = 10−9;\nand (ii) increase the learning rate linearly for the\nfirst 4K training steps and decrease it thereafter;\n(iii) use batch size of 32Ksource tokens and 32K\ntarget tokens. Checkpoints are saved after every\n10K iterations during training. We train models\nwith maximum of 300Kiterations. We use dropout\nof 0.1 and label-smoothing of 0.1.\n3https://github.com/elastic/elasticsearch-py\n4SacreBleu signature: nrefs:1|case:mixed|eff:no|\ntok:13a|smooth:exp|version:2.0.0.\n4 Results\nResults for En-De are shown in Table 1 and results\nfor En-Fr are shown in Table 2.\nWe observe several trends in the results. First,\nour proposed RAT-SI method outperforms both the\nRAT-CAT and RAT-SEP methods across both lan-\nguage pairs, having the best performance in 32/36\ncases considered. In En-De, the proposed RAT-SI\nmethod has an average improvement of 1.43 BLEU\nover RAT-CAT and 2.35 BLEU over RAT-SEP,\nwhile in En-Fr we observe an average improve-\nment of 1.73 BLEU over RAT-CAT and 2.98 over\nRAT-SEP. These results support our hypothesis that\nattention connections between the source sentences\nand each fuzzy match are critical to translation qual-\nity and the connections between the fuzzy matches\nare actually harmful.\nSecond, on average, k = 5 produces the best\nresults for the RAT-SI method, but only by a small\namount. However, considering individual language\npair / domain combinations, there are many cases\nwhere k = 5 does not produce the best results,\nsometimes by several BLEU points. We hypothe-\nsize that this is due to the different domains con-\ntaining, on average, different amounts of relevant\ndata. This observation underscores the importance\nof tuning k, as well as testing new RAT methods\nunder a variety of conditions, including different k\nvalues.\nFinally, consistent with prior work, we see large\nimprovements for all online domain-adapted meth-\nods (RAT-CAT, RAT-SEP, and RAT-SI) over the\nnon-domain-adapted baseline, with improvements\nof up to+13.85 BLEU. This is not surprising, since\nthe baseline model does not take advantage of any\ndomain-specific data.\n4.1 Latency\nWhile not the focus of this work, we did a pre-\nliminary study of latency, comparing a baseline\ntransformer to RAT-CAT and RAT-SI models dur-\ning inference. We follow Domhan et al. (2020) and\nmeasure latency values as the 90thpercentile of\ninference time when translating each sentence indi-\nvidually (no batching). We run experiments on an\nEC2 p3.2xlargeinstance with a Tesla V100 GPU\nand report encoding latency results in Table 3. We\nuse a batch size of 1 and k=3 for all experiments.\nWe observe a small increase of encoding latency\nby using RAT-SI (i.e. 17.48 ms) compared to of\nRAT-CAT. We provide a breakdown of the total\n291\nModel k IT LAW REL MED SUBT Average\nBaseline n/a 27.92 35.59 11.26 30.74 19.46 24.99\nRAT-CAT\nk=3\n33.97 50.34 25.14 45.05 19.89 34.88\nRAT-SEP 32.78 49.04 22.92 44.28 20.48 33.90\nRAT-SI (this work) 33.08 52.02* 26.40* 46.16 20.83* 35.70\nRAT-CAT\nk=4\n33.67 49.59 23.40 44.87 20.27 34.36\nRAT-SEP 31.84 48.38 24.37 43.55 19.99 33.63\nRAT-SI (this work) 33.68 52.00* 28.42* 46.13 20.23 36.09\nRAT-CAT\nk=5\n33.44 49.67 24.95 44.16 20.01 34.45\nRAT-SEP 30.84 47.92 23.91 44.10 20.27 33.41\nRAT-SI (this work) 33.84 52.17* 27.53* 46.95* 20.49 36.20\nTable 1: BLEU scores for En-De experiments. The best BLEU for RAT models with a specific top-k value isbolded,\nand \"*\" indicates the best result is statistically significant compared to both the other methods. The proposed method\n(RAT-SI) produces the best results in 13/15 cases considered, with an average improvement of 1.43 BLEU over\nRAT-CAT and 2.35 BLEU over RAT-SEP.\nModel k LAW MED IT NEWS BANK REL TED Average\nBaseline n/a 52.68 31.12 32.22 35.09 41.04 14.51 35.55 34.60\nRAT-CAT\nk=3\n66.32 37.09 39.91 35.09 49.01 61.83 36.39 46.52\nRAT-SEP 64.93 37.06 38.02 35.57 49.14 53.34 37.29 45.05\nRAT-SI (this work) 66.56 41.30* 40.61 35.53 50.19* 67.55* 37.42 48.45\nRAT-CAT\nk=4\n65.71 37.31 38.71 34.60 49.43 63.55 36.10 46.49\nRAT-SEP 64.35 37.89 38.89 35.45 49.28 53.41 37.13 45.20\nRAT-SI (this work) 66.63* 39.50* 41.90* 35.71 50.04 65.20* 37.47 48.06\nRAT-CAT\nk=5\n65.60 37.35 38.74 34.46 49.33 63.21 36.08 46.40\nRAT-SEP 64.62 38.50 39.53 35.59 49.97 52.56 37.09 45.41\nRAT-SI (this work) 67.03* 39.05 41.33* 35.93* 49.82 65.49* 37.90* 48.08\nTable 2: BLEU scores for En-Fr experiments. The best BLEU for RAT models with a specific top-k value isbolded,\nand \"*\" indicates the best result is statistically significant compared to both the other methods. The proposed\nmethod (RAT-SI) produces the best results in 19/21 cases considered, with average improvements of 1.73 BLEU\nover RAT-CAT and 2.98 over RAT-SEP.\nModel Encoding Latency\nTransformer 14.80 ms\nRAT-CAT 15.23 ms\nRAT-SI 17.48 ms\nTable 3: Encoding latency in milliseconds of models\n(lower is better).\nencoding latency in Table 4 which shows encoding\nthe inputs in RAT-SI is faster than RAT-CAT but\nit requires an extra overhead for extracting the en-\ncoding of fuzzy matches from the joint encoding\nof source with fuzzy match. However, the encod-\ning time is a very small fraction of overall latency\n(see Table 5) and thus this difference appears to be\nnegligible.\nWe find that RAT-CAT and RAT-SI have nearly\nidentical latencies, and each is only slightly slower\nthan the baseline transformer (see Table 5). This\nis somewhat surprising since both methods make\nthe input to the decoder significantly longer. We\nhypothesize that we are under-utilizing the GPU in\nall cases, and thus the increased computations does\nnot increase latency. Further investigation of this is\nRAT-SI Model Encoding\nLatency\nEncoding input 14.91 ms\nExtra overheads 2.57 ms\nTotal time 17.48 ms\nTable 4: Encoding latency of RAT-SI in milliseconds\n(lower is better). Extra overheads include (1): Concate-\nnate input and k= 3input-suggestion pairs (2): Extract\nk = 3 suggestion encodings and append them to the\ninput encoding.\nleft for future work.\n5 Related Work\nBulte and Tezcan (2019) proposed augmenting the\ninput to NMT with target-language fuzzy-match\nsentences from a TM, concatenating the input and\nfuzzy-matches together. Their method was sim-\npler than prior works such as (Zhang et al., 2018),\nwhich manipulated n-gram probabilities based on\ntheir occurrence in the fuzzy-matches. Xu et al.\n(2020) proposed several enhancements using the\nsame architecture, including fine-tuning models\n292\nModel Translation Latency\nTransformer 574.02 ms\nRAT-CAT 597.28 ms\nRAT-SI 597.41 ms\nTable 5: Translation latency in milliseconds of RAT-\nCAT and our model RAT-SI (lower is better). Batch size\nwas set to one to simulate an on-demand system.\nand masking out or marking words not related to\nthe input sentence, and matching arbitrarily large\nn-grams instead of sentences.\nMore recent work has explored using separate\nencoders for input and fuzzy-match (He et al., 2021;\nCai et al., 2021). He et al. (2021) also considers the\nrealistic scenario where a TM may include noise,\nwhile Cai et al. (2021) explores finding target sen-\ntences in monolingual data instead of relying on a\nTM at inference time.\nXia et al. (2019) and Xu et al. (2020) explore\naspects of filtering fuzzy-matches by applying sim-\nilarity thresholds, leveraging word alignment in-\nformation (Zhang et al., 2018; Xu et al., 2020; He\net al., 2021) or re-ranking with additional score\n(e.g. word overlapping) (Gu et al., 2018; Zhang\net al., 2018).\nOur work is related to the use of k-nearest-\nneighbor for NMT (Khandelwal et al., 2021; Zheng\net al., 2021) but it is less expensive and does not\nrequire storage and search over a large data store\nof context representations and corresponding target\ntokens (Meng et al., 2021).\nOther works have considered online adaptation\noutside the context of RAT, including Vilar (2018),\nwho proposes Learning Hidden Unit Contributions\n(Swietojanski et al., 2016) as a compact way to\nstore many adaptations of the same general-domain\nmodel. For an overview of fuzzy-match augmenta-\ntion outside of NMT, see Li et al. (2022).\nDomain adaptation can also be performed off-\nline, typically via fine tuning (Luong and Manning,\n2015). Regularization is often applied during fine\ntuning to avoid catastrophic forgetting (Khayrallah\net al., 2018; Thompson et al., 2019a,b).\nTMs are commonly used in the localization in-\ndustry to provide suggestions to translators in order\nto boost their productivity (Federico et al., 2012).\nEnhancing translation quality of MT system by\nleveraging fuzzy-matches extracted from TMs has\nbeen explored widely for statistical MT (Koehn and\nSenellart, 2010; Mathur et al., 2013) and neural MT\nsystems (Farajian et al., 2017; Gu et al., 2017; Cao\nand Xiong, 2018; Bulte and Tezcan, 2019).\n6 Conclusion\nPrevious work in retrieval augmented translation\nhas used architectures which either have full con-\nnections between source and all fuzzy matches, or\nindependently encode the source and each fuzzy\nmatch. Based on our hypothesize that the attention\nconnections between source and each fuzzy match\nare helpful, but that the the connections between\ndifferent fuzzy matches are harmful, we propose\na new architecture (RAT-SI) with the former con-\nnections but not the latter. Experiments on several\nlanguage pairs, domains, and different numbers of\nfuzzy matches ( k) demonstrate that RAT-SI sub-\nstantially outperforms the prior architectures.\n7 Limitations\nDue to the availability of domain specific datasets,\nwe perform experiments on two high-resource lan-\nguages, both out of English. It is unclear if our\nconclusions would hold on low-resource language\npairs. Furthermore, our domains may or may not\nmatch real world use cases where an MT customer\nhas their own TM. Real TMs may be significantly\nlarger/smaller, contain multiple domains, etc.\nReferences\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised\ndomain clusters in pretrained language models. In\nProceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long\nPapers). Association for Computational Linguistics.\nOndrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve Saint-\nAmand, Radu Soricut, Lucia Specia, and Aleš Tam-\nchyna. 2014. Findings of the 2014 workshop on\nstatistical machine translation. In Proceedings of the\nNinth Workshop on Statistical Machine Translation,\npages 12–58, Baltimore, Maryland, USA. Associa-\ntion for Computational Linguistics.\nBram Bulte and Arda Tezcan. 2019. Neural fuzzy re-\npair: Integrating fuzzy matches into neural machine\ntranslation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1800–1809, Florence, Italy. Association for\nComputational Linguistics.\nDeng Cai, Yan Wang, Huayang Li, Wai Lam, and\nLemao Liu. 2021. Neural machine translation with\nmonolingual translation memory. In Proceedings\n293\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7307–7318, Online.\nAssociation for Computational Linguistics.\nQian Cao and Deyi Xiong. 2018. Encoding gated\ntranslation memory into neural machine translation.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3042–3047, Brussels, Belgium. Association for Com-\nputational Linguistics.\nTobias Domhan, Michael Denkowski, David Vilar,\nXing Niu, Felix Hieber, and Kenneth Heafield. 2020.\nThe sockeye 2 neural machine translation toolkit at\nAMTA 2020. In Proceedings of the 14th Conference\nof the Association for Machine Translation in the\nAmericas (Volume 1: Research Track), pages 110–\n115, Virtual. Association for Machine Translation in\nthe Americas.\nM. Amin Farajian, Marco Turchi, Matteo Negri, and\nMarcello Federico. 2017. Multi-domain neural ma-\nchine translation through unsupervised adaptation. In\nProceedings of the Second Conference on Machine\nTranslation, pages 127–137, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\nMarcello Federico, Alessandro Cattelan, and Marco\nTrombetti. 2012. Measuring user productivity in ma-\nchine translation enhanced computer assisted trans-\nlation. In Proceedings of the 10th Conference of the\nAssociation for Machine Translation in the Ameri-\ncas: Research Papers, San Diego, California, USA.\nAssociation for Machine Translation in the Americas.\nMarkus Freitag and Yaser Al-Onaizan. 2016. Fast\ndomain adaptation for neural machine translation.\narXiv preprint arXiv:1612.06897.\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor\nO. K. Li. 2017. Search engine guided non-parametric\nneural machine translation. CoRR, abs/1705.07267.\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor\nO. K. Li. 2018. Search engine guided neural machine\ntranslation. In Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artificial\nIntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artificial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 5133–5140. AAAI Press.\nQiuxiang He, Guoping Huang, Qu Cui, Li Li, and\nLemao Liu. 2021. Fast and accurate neural machine\ntranslation with translation memory. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3170–3180, Online.\nAssociation for Computational Linguistics.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2021. Nearest neigh-\nbor machine translation. In International Conference\non Learning Representations.\nHuda Khayrallah, Brian Thompson, Kevin Duh, and\nPhilipp Koehn. 2018. Regularized training objective\nfor continued training for domain adaptation in neural\nmachine translation. In Proceedings of the 2nd Work-\nshop on Neural Machine Translation and Generation,\npages 36–44, Melbourne, Australia. Association for\nComputational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. InProceedings\nof the First Workshop on Neural Machine Translation,\npages 28–39, Vancouver. Association for Computa-\ntional Linguistics.\nPhilipp Koehn and Jean Senellart. 2010. Convergence\nof translation memory and statistical machine transla-\ntion. In Proceedings of the Second Joint EM+/CNGL\nWorkshop: Bringing MT to the User: Research on\nIntegrating MT in the Translation Industry, pages 21–\n32, Denver, Colorado, USA. Association for Machine\nTranslation in the Americas.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2018: System Demonstrations, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 66–71. Asso-\nciation for Computational Linguistics.\nHuayang Li, Yixuan Su, Deng Cai, Yan Wang, and\nLemao Liu. 2022. A survey on retrieval-augmented\ntext generation.\nMinh-Thang Luong and Christopher Manning. 2015.\nStanford neural machine translation systems for spo-\nken language domains. In Proceedings of the 12th\nInternational Workshop on Spoken Language Trans-\nlation: Evaluation Campaign, pages 76–79, Da Nang,\nVietnam.\nPrashant Mathur, Mauro Cettolo, and Marcello Fed-\nerico. 2013. Online learning approaches in computer\nassisted translation. In Proceedings of the Eighth\nWorkshop on Statistical Machine Translation, pages\n301–308, Sofia, Bulgaria. Association for Computa-\ntional Linguistics.\nYuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xi-\naofei Sun, Tianwei Zhang, and Jiwei Li. 2021.\nFast nearest neighbor machine translation. CoRR,\nabs/2105.14528.\n294\nGraham Neubig, Zi-Yi Dou, Junjie Hu, Paul Michel,\nDanish Pruthi, and Xinyi Wang. 2019. compare-mt:\nA tool for holistic comparison of language genera-\ntion systems. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 35–41, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nMinhQuang Pham, Josep Maria Crego, and François\nYvon. 2021. Revisiting Multi-Domain Machine\nTranslation. Transactions of the Association for Com-\nputational Linguistics, 9:17–35.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nStephen Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Found. Trends Inf. Retr., 3(4):333–389.\nPawel Swietojanski, Jinyu Li, and Steve Renals. 2016.\nLearning hidden unit contributions for unsupervised\nacoustic model adaptation. IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing ,\n24(8):1450–1463.\nBrian Thompson, Jeremy Gwinnup, Huda Khayrallah,\nKevin Duh, and Philipp Koehn. 2019a. Overcoming\ncatastrophic forgetting during domain adaptation of\nneural machine translation. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2062–2068, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nBrian Thompson, Rebecca Knowles, Xuan Zhang, Huda\nKhayrallah, Kevin Duh, and Philipp Koehn. 2019b.\nHABLex: Human annotated bilingual lexicons for\nexperiments in machine translation. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1382–1387, Hong Kong,\nChina. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nDavid Vilar. 2018. Learning hidden unit contribution\nfor adapting neural machine translation models. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 500–505, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nMengzhou Xia, Guoping Huang, Lemao Liu, and Shum-\ning Shi. 2019. Graph based translation memory for\nneural machine translation. Proceedings of the AAAI\nConference on Artificial Intelligence, 33(01):7297–\n7304.\nJitao Xu, Josep Crego, and Jean Senellart. 2020. Boost-\ning neural machine translation with similar transla-\ntions. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n1580–1590, Online. Association for Computational\nLinguistics.\nJingyi Zhang, Masao Utiyama, Eiichro Sumita, Gra-\nham Neubig, and Satoshi Nakamura. 2018. Guiding\nneural machine translation with retrieved translation\npieces. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 1325–1335,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nXin Zheng, Zhirui Zhang, Junliang Guo, Shujian Huang,\nBoxing Chen, Weihua Luo, and Jiajun Chen. 2021.\nAdaptive nearest neighbor machine translation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 368–374,\nOnline. Association for Computational Linguistics.\n295",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8572169542312622
    },
    {
      "name": "Machine translation",
      "score": 0.6830527186393738
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6506161689758301
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5780779123306274
    },
    {
      "name": "Natural language processing",
      "score": 0.5556156039237976
    },
    {
      "name": "Inference",
      "score": 0.553357720375061
    },
    {
      "name": "Fuzzy logic",
      "score": 0.5507890582084656
    },
    {
      "name": "Sentence",
      "score": 0.5228815078735352
    },
    {
      "name": "Translation (biology)",
      "score": 0.4731113612651825
    },
    {
      "name": "Language model",
      "score": 0.43235546350479126
    },
    {
      "name": "Machine learning",
      "score": 0.35567283630371094
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 5
}