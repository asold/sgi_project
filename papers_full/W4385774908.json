{
  "title": "A Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization",
  "url": "https://openalex.org/W4385774908",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2122070998",
      "name": "Kang, Sungmin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4311554225",
      "name": "An, Gabin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2555373365",
      "name": "Yoo Jung Shin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3042956498",
    "https://openalex.org/W1813809747",
    "https://openalex.org/W2153418968",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W2467903332",
    "https://openalex.org/W4307412413",
    "https://openalex.org/W2958754741",
    "https://openalex.org/W2084429940",
    "https://openalex.org/W4386302809",
    "https://openalex.org/W4308643065",
    "https://openalex.org/W2962715466",
    "https://openalex.org/W1950030762",
    "https://openalex.org/W2953998535",
    "https://openalex.org/W2013655083",
    "https://openalex.org/W4284702776",
    "https://openalex.org/W3160492491",
    "https://openalex.org/W1990785546",
    "https://openalex.org/W2740264376",
    "https://openalex.org/W3105943882",
    "https://openalex.org/W2110706065",
    "https://openalex.org/W4384345748",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4309940971",
    "https://openalex.org/W4318902699",
    "https://openalex.org/W4362679230",
    "https://openalex.org/W3160700180",
    "https://openalex.org/W2156723666",
    "https://openalex.org/W4384302785",
    "https://openalex.org/W2578469907",
    "https://openalex.org/W2966980041",
    "https://openalex.org/W3162747997",
    "https://openalex.org/W2343875716",
    "https://openalex.org/W2150290224",
    "https://openalex.org/W4289976876",
    "https://openalex.org/W1843474218",
    "https://openalex.org/W2537787699",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2067436653",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4287212576",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4384304728",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4389208936",
    "https://openalex.org/W4361866031"
  ],
  "abstract": "Fault Localization (FL), in which a developer seeks to identify which part of the code is malfunctioning and needs to be fixed, is a recurring challenge in debugging. To reduce developer burden, many automated FL techniques have been proposed. However, prior work has noted that existing techniques fail to provide rationales for the suggested locations, hindering developer adoption of these techniques. With this in mind, we propose AutoFL, a Large Language Model (LLM)-based FL technique that generates an explanation of the bug along with a suggested fault location. AutoFL prompts an LLM to use function calls to navigate a repository, so that it can effectively localize faults over a large software repository and overcome the limit of the LLM context length. Extensive experiments on 798 real-world bugs in Java and Python reveal AutoFL improves method-level acc@1 by up to 233.3% over baselines. Furthermore, developers were interviewed on their impression of AutoFL-generated explanations, showing that developers generally liked the natural language explanations of AutoFL, and that they preferred reading a few, high-quality explanations instead of many.",
  "full_text": "A Quantitative and Qualitative Evaluation of LLM-Based\nExplainable Fault Localization\nSUNGMIN KANG‚àó, KAIST, South Korea\nGABIN AN‚àó, KAIST, South Korea\nSHIN YOO, KAIST, South Korea\nFault Localization (FL), in which a developer seeks to identify which part of the code is malfunctioning\nand needs to be fixed, is a recurring challenge in debugging. To reduce developer burden, many automated\nFL techniques have been proposed. However, prior work has noted that existing techniques fail to provide\nrationales for the suggested locations, hindering developer adoption of these techniques. With this in mind, we\npropose AutoFL, a Large Language Model (LLM)-based FL technique that generates an explanation of the bug\nalong with a suggested fault location. AutoFL prompts an LLM to use function calls to navigate a repository,\nso that it can effectively localize faults over a large software repository and overcome the limit of the LLM\ncontext length. Extensive experiments on 798 real-world bugs in Java and Python reveal AutoFL improves\nmethod-level acc@1 by up to 233.3% over baselines. Furthermore, developers were interviewed on their\nimpression of AutoFL-generated explanations, showing that developers generally liked the natural language\nexplanations of AutoFL, and that they preferred reading a few, high-quality explanations instead of many.\nCCS Concepts: ‚Ä¢ Software and its engineering ‚ÜíSoftware testing and debugging ; Software defect\nanalysis.\nAdditional Key Words and Phrases: language models, fault localization, debugging\nACM Reference Format:\nSungmin Kang, Gabin An, and Shin Yoo. 2024. A Quantitative and Qualitative Evaluation of LLM-Based\nExplainable Fault Localization. 1, 1 (July 2024), 23 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1 INTRODUCTION\nFault Localization (FL) is the task of identifying which part of a software system is responsible for\na bug. FL has been surveyed to take the majority of human debugging time: for example, B√∂hme\net al. report that bug diagnosis (which includes localization) took 66% of debugging time in their\nhuman study [8]. As a result, FL has been widely researched, with techniques from slicing-based\nFL [2], to others that use causality analysis [22] or model the propagation of erroneous states [52].\nDespite the promise of automated FL techniques, they have not been widely adopted by practi-\ntioners. We are unaware of the industrial adoption of FL systems, unlike the case for automated\nprogram repair (APR) [27, 43]. Looking at the various expectations that developers have of FL tools\noutlined by Kochhar et al. [20], one aspect that developers emphasize is providing rationales: almost\n90% of developers agreed that the capability of an FL technique to generate rationales is important.\nIn contrast, to the best of our knowledge, there are few FL techniques that can explain why a\n‚àóBoth authors contributed equally to this research.\nAuthors‚Äô Contact Information: Sungmin Kang, sungmin.kang@kaist.ac.kr, KAIST, Daejeon, South Korea; Gabin An, gabin.\nan@kaist.ac.kr, KAIST, Daejeon, South Korea; Shin Yoo, shin.yoo@kaist.ac.kr, KAIST, Daejeon, South Korea.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM XXXX-XXXX/2024/7-ART\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n, Vol. 1, No. 1, Article . Publication date: July 2024.\narXiv:2308.05487v3  [cs.SE]  2 Jul 2024\n2 Kang, An, and Yoo\nparticular location is the likely culprit in an accessible manner. Consequently, anexplainable FL\ntechnique, one that can explain both the bug and the reason why the suggested location is linked\nto the buggy behavior, would take FL techniques one step closer to practitioner adoption.\nIn this paper, we present AutoFL, an automated FL technique that uses Large Language Models\n(LLMs) to not only suggest potential bug locations, but also to provide explanations on how the\nbug occurred and why the suggested location is suspicious. LLMs are difficult to apply to FL in\na realistic manner, as a critical input for FL is the entire software repository, which can span\nthousands to millions of lines of code, whereas the context length of LLMs is generally limited\n- for example, the largest LLMs at the time of writing can process 32,000 tokens at once, much\nless lines. To solve this problem, we allow LLMs to navigate the source code by allowing them\nto call functions that return information on covered classes, their covered functions, and the\nimplementation and documentation for any covered function. Given a prompt asking for the LLM\nto find the answer using the available functions, the LLM invokes a series of function calls to gather\nrelevant information, and reaches a conclusion on how the bug manifested, which we treat as the\nexplanation of the bug. By construction, AutoFL has many benefits when compared to existing FL\ntechniques: it only requires a single failing test, it can deal with multiple programming languages\nseamlessly, and critically can generate explanations for developers.\nWe perform thorough experiments to evaluate both the FL capability of AutoFL and the quality\nof its explanations. Our quantitative evaluation on the FL performance of AutoFL using real-world\nbug datasets from Java and Python indicates that when using the GPT-3.5 language model, it could\nachieve comparable or superior performance to existing techniques. For example, when comparing\nagainst the Ochiai SBFL technique, AutoFL improves on its method-level acc@1 score by 19.7%\non the Defects4J benchmark and 166.7% on the BugsInPy benchmark; using GPT-4, AutoFL could\nperform even better, outperforming Ochiai by 233.3% on BugsInPy. By repeating the execution of\nAutoFL [39], we also demonstrate that we can estimate the level of confidence AutoFL has in its\nresults, providing an avenue for reducing false positives when presenting results to developers.\nMeanwhile, evaluating the quality of explanations generated by AutoFL is a difficult, yet impor-\ntant aspect when assessing AutoFL. First, manual analysis of 300 sampled explanations from 60\nbugs reveals that about 20% of generated explanations accurately describe how the bug happened,\nwhile 56.7% of all bugs had an accurate description. Critically, we presented the explanations of\nAutoFL to professional developers, and performed interviews on what they liked and disliked about\nthe explanations. Our key findings are that developers were generally supportive of explaining\nFL results and bugs, with many suggesting that the natural language descriptions of the failure\nwere helpful, but that redundant content and incorrect fix suggestions could lead them astray.\nDevelopers additionally indicated the desire to see a few explanations instead of many.\nThese results suggest that it would be helpful to automatically identify which explanations are\nhelpful. We present the results of our follow-up preliminary experiments which estimate the quality\nof explanations based on the quality of tests and patches made with an LLM conditioned on those\nexplanations. We find a positive correlation with the correctness of explanations, providing hints\nas to how explanations could be automatically assessed for developer usage.\nTo summarize, our contributions are:\n‚Ä¢We introduce AutoFL, an LLM-based FL technique that overcomes the input limitations of\nLLMs by allowing them to autonomously retrieve relevant portions of the code, and generates\nan explanation of how the bug happened before suggesting a fault location.\n‚Ä¢We rigorously evaluate the FL performance of AutoFL, and find that it can outperform FL\nbaselines over two large real-world bug benchmarks, and that it can indicate when its answer\nis likely to be correct, potentially reducing developer hassle with false positives.\n, Vol. 1, No. 1, Article . Publication date: July 2024.\nA Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization 3\nTable 1. A comparison of existing FL techniques with AutoFL. The precision of SBFL, MBFL, and IRFL\nwas recalculated based on the artifacts of Zou et al. [ 53]; for other techniques, precision comes from the\ncorresponding papers. Wu et al. [46] only evaluate statement-level FL, so their precision could not be compared.\nRequired Artifact Prec@5 on D4J Time Multilang. Rationale\nSBFL Test suite 61% [53] minutes yes no\nMBFL Test suite 54% [53] hours yes no\nIRFL Bug report 3%2 [53] seconds yes no\nCombineFL [53] All of the above 69% [53] hours no no\nDeepRL4FL [24] Test suite 79% [24] hours no no\nUniVal [22] Pass/Fail test 75% [22] minutes3 no no\nSmartFL [52] Pass/Fail test 70% [52] minutes no no\nWu et al. [46]Buggy method/class - - yes yes\nAutoFL Single test Up to 71% minutes yes yes\n‚Ä¢We evaluate the explanations generated by AutoFL, and find that accurate explanations could\nbe generated for more than half of all bugs, while developers had positive views towards the\nexplanations generated by AutoFL.\nThe remainder of the paper is organized as follows. Section 2 provides the academic background,\nand Section 3 describes how AutoFL works. The evaluation setup is described in Section 4, with\nthe results in Section 5. Section 6 describes our preliminary experiments to automatically identify\nhelpful explanations, Section 7 lays out threats to validity, and Section 8 concludes.\n2 BACKGROUND\nThis section provides the background and research context.\n2.1 LLM Tool Use\nBy integrating chain-of-thought prompting [40] with the output of tools, ReAct [51] demonstrated\nthat LLMs are capable of interacting with tools to achieve better performance on tasks. Since then,\nLLM interaction with external tools has been widely explored. For example, HuggingGPT [ 34]\nhas LLMs compose computer vision pipelines by dynamically integrating the results of various\ncomputer vision models together. LLM tool use has also been explored in software engineering,\nnotably for program repair: Xia et al. [48] integrated test feedback into the prompt, while Kang et\nal. [16] allows LLMs to invoke a debugger.\nRecent iterations of OpenAI‚Äôs LLMs have embraced this change and added a feature named\nfunction calling [31]. This capability enables users to provide function descriptions to the LLM,\nwhich can respond with JSON data requesting a function call, complete with arguments required\nfor calling the function, on the digression of the LLM. For instance, to answer a user inquiry about\nthe current weather, an LLM may call a function that retrieves the weather of a particular location,\nwhich would be processed in an automated manner and presented to the LLM so that it can provide\na coherent response.1 In this context, we aim to define a set of functions that the LLM can employ\nto gather necessary information for debugging.\n1While this is notably implemented by the OpenAI API, tools such as LangChain [ 10] allow other LLMs to incorporate\nfunctions as well; preliminary results using Llama2 [37] with AutoFL are given in our supplementary material [6].\n2While Zou et al. report this performance in their paper, their replication package did not include IRFL results.\n3This was achieved by only instrumenting the buggy class, which would not be possible in practice.\n, Vol. 1, No. 1, Article . Publication date: July 2024.\n4 Kang, An, and Yoo\nüìÅ\nCodebaseüíª get_code_snippet\nüíª get_comments\nüíª get_class_covered\nüíª get_method_covered ‚õ≥Coverage\nAutoFL\nAlgorithm\nLanguage\nModel\nStage 1 Explanation Generation FL PredictionStage 2\n1 Bug \nInformation\n2 Function \nInteraction\nup to N \ntimes\n3Bug \nExplanation\nQuery\nLocation 4 5 Answer\n‚Ä¶ ‚Ä¶ System Under Testüíæ\nFig. 1. Diagram of AutoFL. Arrows represent an interaction between components; circled numbers indicate\nthe order. Function interactions are made at most N times, where N is a predetermined parameter of AutoFL.\n2.2 Fault Localization\nFault Localization (FL) is a critical process in debugging that involves identifying specific locations\nin a program‚Äôs source code where bugs are present. Automated FL techniques help developers save\ntime, particularly in large codebases, by accurately pinpointing the code locations most likely to be\nresponsible for the target bug. We provide a comparison of existing FL work withAutoFL in Table 1.\nCommonly used FL technique families include Spectrum-based FL (SBFL), Information Retrieval-\nbased FL (IRFL), and Mutation-based FL (MBFL) [45]. While SBFL techniques are known to be the\nmost effective standalone techniques [53], they require coverage data from both passing and failing\ntests. Meeting this requirement can be challenging for large enterprise software, where coverage\nmeasurement can have high computational costs [7, 13, 18]. Additionally, most FL techniques lack\na rationale or explanation in their output, limiting their reliability and practicality in practical\ndebugging. As Kochhar et al. [20] note, rationales for FL are crucial for bug fixing, as clear rationales\nenable developers to understand why a particular location is identified as the culprit for the bug,\nhelping them incorporate their domain knowledge and make informed decisions. Meanwhile, the\nrecent work of Wu et al. [46] presents a buggy method/class and asks an LLM which location is\nlikely to be buggy and why. However, it is difficult to use as a standalone FL technique in practice,\nas it requires prerequisite knowledge of which method/class is buggy for operation.\n3 APPROACH\nIn this paper, we introduce AutoFL, a novel automated and autonomous FL technique that har-\nnesses LLMs to localize bugs in software given a single failing test. A key advantage of using\nLLMs is that they can generate natural language explanations on why a particular location seems\nlikely to be buggy, as LLMs are adept in both software engineering and natural language process-\ning tasks [9, 47]. Conceptually, we define an ‚Äòexplanation‚Äô as any text that helps the developer\ncomprehend the bug or how AutoFL came to a conclusion; to actualize this ideal and make such\nexplanations, we designed AutoFL to provide the logical flow from the root cause to the failure,\nas was identified a key component for failure explanations by Du et al. [11]. AutoFL represents a\nnew type of effort in FL research, in the sense that we can now readily generate explanations along\nwith localization results, marking a qualitative difference from prior techniques.\nAutoFL specifically focuses on method-level FL for two main reasons. First, the method granu-\nlarity is the favored granularity level among developers according to prior work [20]; furthermore,\nrecent work on LLM-based automated debugging techniques often require method-level localization\n, Vol. 1, No. 1, Article . Publication date: July 2024.\nA Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization 5\nresults as a minimum [14, 46, 47], suggesting that localization on the method level is a recurring\nproblem in LLM-based automated debugging. As mentioned earlier, dealing with large code reposi-\ntories is a challenge for LLMs, but we tackle this issue by equipping LLMs with custom-designed\nfunctions to enable code exploration and relevant information extraction.\nAn overview of AutoFL is depicted in Fig. 1. We employ a two-stage prompting process, where\nStage 1 involves inquiring about the root cause of the given failure, and Stage 2 requests output\nabout the fault location. In Stage 1, 1 AutoFL provides a prompt to the LLM containing failing test\ninformation and descriptions of available functions for debugging. 2 The LLM interacts with the\nprovided functions autonomously, to extract the information needed for the debugging of the given\nfailure. 3 Based on the gathered information, the LLM generates an explanation about the root\ncause of the observed failure. In Stage 2, 4 the LLM is queried to return the culprit location for\nthe bug, and 5 the LLM responds by providing the culprit method. In doing so, we can explicitly\nacquire both the root cause explanation and the bug location.\nListing 1. System Prompt for LLM\nYou are a debugging assistant. You will be presented with a failing test, and tools (functions) to\n‚Ü©‚Üí access the source code of the system under test (SUT). Your task is to provide a step-by-\n‚Ü©‚Üístep explanation of how the bug occurred, based on the failing test and the information\n‚Ü©‚Üíyou retrieved using tests about the SUT. You will be given N chances to interact with\n‚Ü©‚Üífunctions to gather relevant information. An example answer would look like follows.\n<HANDCRAFTED ROOT CAUSE ANALYSIS EXAMPLE>\nDuring the process, the system message 4 shown in Listing 1 is used to guide the LLM in its\nrole as a debugging assistant. As described before, we aimed for AutoFL to generate the logical\nflow from fault to failure; the prompt was designed with this in mind, as it asks for a step-by-step\nexplanation of how the bug occurred. This has the added benefit that it may also improve the\nperformance of AutoFL, as generating explanations before answering is known to help improve\nperformance [21]. Furthermore, the handcrafted example is a brief one that describes the cause of a\nbug in two sentences, nudging AutoFL towards generating concise explanations.\n3.1 Stage 1: Generating Root Cause Explanation\nAt the outset, AutoFL initiates the FL process by presenting an initial prompt about the failed test\nto the LLM (Fig. 1, 1 ). This prompt is automatically generated and includes bug-related information\nsuch as the failing test and its error stack trace. Listing 2 shows the prompt template, with the\nrelevant failure details highlighted in blue.\nFirst, the prompt specifies the name of the failing test and provides the test code snippet (Lines\n1-9). The test code snippet is enclosed within triple backticks to indicate it is a code block. We\nemploy two heuristics to minimize irrelevant content that may confuse the LLM. First, we minimize\nthe snippet to exclusively include statements placed prior to where the failure occurs, which is\nexplicitly marked with the comment \"// error occurred here\" ; as test statements after the\nfailure are not executed, they bear less relevance to the bug than the actually executed test code. In\ncase the failure is within a nested block, the entire outermost statement containing the error is\nincluded. Additionally, any preceding assertion statements are detected and removed, as we can be\nsure that these assertions passed and are thus less likely to be relevant to the bug; indeed, their\ninclusion could confuse the language model. For example, Line 384 in Listing 2 is an assertion\n4A system message refers to a message that is used to guide the behavior of the LLM by providing instructions.\n, Vol. 1, No. 1, Article . Publication date: July 2024.\n6 Kang, An, and Yoo\nListing 2. Example prompt from Defects4J Lang-48\n1 The test `...EqualsBuilderTest::testBigDecimal()` failed. The test looks like:\n2\n3 ```java\n4 381 : public void testBigDecimal() {\n5 382 : BigDecimal o1 = new BigDecimal(\"2.0\");\n6 383 : BigDecimal o2 = new BigDecimal(\"2.00\");\n7 385 : assertTrue(new EqualsBuilder().append(o1, o2).isEquals()); // error occurred here\n8 386 : }\n9 ```\n10\n11 It failed with the following error message and call stack:\n12 ```\n13 junit.framework.AssertionFailedError\n14 at ...EqualsBuilderTest::testBigDecimal(EqualsBuilderTest.java:385)\n15 ```\n16 Start by calling the `get_failing_tests_covered_classes` function.\nstatement that passed which is irrelevant to the actual bug. Hence, it was automatically removed\nand not visible in Listing 2. While AutoFL uses a single test in its prompt, note that the prompt can\nbe easily extended to handle multiple failing tests by concatenating the information from each test.\nFollowing the snippet, the prompt presents the failure symptoms consisting of an error message\nand the stack trace (Lines 11-15). Stack traces are automatically minimized by retaining only the\ninformation related to the target repository (e.g., lines related to external libraries are omitted).\nAdditionally, repeated subsequences occurring more than five times are condensed to improve\nreadability and conciseness, which helps with long stack traces, e.g., stack overflow errors.\nFinally, the prompt ends with a suggestion that the LLM call theget_covered_classes function\n(Line 16), to encourage the LLM to make use of functions when generating its answer. Preliminary\nexperiments showed the addition of this instruction improved the function call success rate by the\nLLM. Note that we append this initial function call request and its response to the message chain\nof AutoFL without requiring an actual call from the LLM.\nAlong with the prompt,AutoFL provides the four specialized functions for debugging and allows\nthe LLM to decide whether to (i) request a function call (Fig. 1, 2 ), with a limit of at mostùëÅ function\ninteractions, or (ii) to generate a user-facing bug explanation (Fig. 1, 3 ), which concludes Stage 1.\nIf the LLM requests a function call, AutoFL executes the function and provides the return value\nback to the model by adding the result to the message history. The first two functions, indicated in\nblue in the diagram, allow the LLM to obtain class-level and method-level coverage information\nrelated to the failing test. With these functions, the LLM can narrow down the methods associated\nwith the observed failure, enabling a targeted analysis of the root cause. The latter two functions,\ndenoted with red in the diagram, are general code navigation tools, which take the signature\nof a method of interest as input and return the code snippet and the relevant documentation (if\nit exists). While simple, these functions permit significant flexibility for the LLM to explore the\nrepository and access the current implementation and specification of methods. Furthermore, as\nShuster et al. [36] note, models tend to be more truthful when augmented with document-retrieving\ntools; thus, by including functions that retrieve information as a part of AutoFL, we increase the\nlikelihood that AutoFL will generate an accurate description of how the bug happened, as was our\ninitial goal. This is also indirectly reflected in our results in Section 5, which show that without\nthis information retrieval, FL performance drops. If the LLM provides invalid arguments to these\nfunctions, a guidance message aimed at improving subsequent function call requests is returned.\n, Vol. 1, No. 1, Article . Publication date: July 2024.\nA Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization 7\nFor instance, when the LLM submits incomplete method signatures that match multiple existing\nmethods, the following guidance message is returned: \"There are multiple matches to that query. Do\nyou mean any of the following: <candidates>?\" Additional details on the implementation of such\nguidance messages can be found in our artifact.\nListing 3. Prompt to Request the Fault Location\nBased on the available information, provide the signatures of the most likely culprit methods for\n‚Ü©‚Üíthe bug. Your answer will be processed automatically, so make sure to only answer with the\n‚Ü©‚Üí accurate signatures of the most likely culprit (in `ClassName.MethodName(ArgType1,\n‚Ü©‚ÜíArgType2, ...)` format), without commentary (one per line).\n3.2 Stage 2: Pinpointing The Fault Location\nAfter Stage 1 concludes, the LLM is then prompted to predict the culprit methods based on the\navailable information (Fig. 1, 4 ), based on the prompt in Listing 3. In this stage, unlike Stage 1,\nwe enforce the model to respond immediately without making function calls, assuming that the\nLLM has already (implicitly) identified the fault location(s) in Stage 2. Finally, the entire FL process\nconcludes with the LLM providing a response pinpointing the potential fault locations (Fig. 1, 5 ).\nFor instance, given the initial prompt about Lang-48 (Listing 2) in Stage 1, the LLM of AutoFL\nsequentially made four function calls: first, to obtain the class and methods covered by the failing\ntest, then to retrieve code snippets for two methods of EqualsBuilder, namely isEquals() and\nappend(Object, Object) . The LLM then identifies the root cause of the bug: an erroneous\nutilization of the equals method for BigDecimal objects, resulting in a comparison based on\nreferences rather than values in the append method. In Stage 2, the LLM specifically suggests\nEqualsBuilder.append(Object, Object) as buggy, which matches the developer patch location.\n3.3 Finalizing Fault Localization Results\nTo start, we assign scores to the methods by combining the results of ùëÖrepeated runs of AutoFL.\nSpecifically, if a final prediction contains a total ofùëõmethods, we give a score of1/ùëõto each of these\nidentified methods.5 These individual scores are then averaged over all ùëÖpredictions. Formally, the\nscore of a method ùëöis defined as:\nscore(ùëö)= 1\nùëÖ\nùëÖ‚àëÔ∏Å\nùëò=1\n( 1\n|ùëüùëò |¬∑[ùëö ‚ààùëüùëò ]) (1)\nwhere ùëüùëñ is the set of predicted methods from the ùëñ-th run, and [.]is the Iverson bracket which\nreturns 1 when the predicate inside is true, and 0 otherwise. For example, for method ùêµin Fig. 2,\nthere are four AutoFL runs that predict the method as a faulty location. Following Eq. (1), the\nscore for method B would be (0.5 +1.0 +1.0 +0.5 +0.0)/5 = 0.6. After calculating the scores for all\nmethods predicted by AutoFL, we rank them in descending order of scores. For instance, in the\ngiven example, the four predicted methods are sorted into [B, D, A, C], as their scores are [0.6,\n0.2, 0.1, 0.1], respectively. In case of a score tie, we prioritize methods that appeared in earlier\npredictions over others as a final, arbitrary tiebreaker.\nNote that if there are methods with a score of 0 (i.e., not part of the final AutoFL results) but are\ncovered by the failing tests, e.g., the method E in the given example, we append them to the end of\nthe ranked list to ensure the list includes all methods relevant to the failure. If there are multiple\n5No scores are distributed in cases where the prediction results are erroneous, or if the entireAutoFL process is interrupted\ndue to other errors.\n, Vol. 1, No. 1, Article . Publication date: July 2024.\n8 Kang, An, and Yoo\n=0.5/R\n=0.1\n1.B ‚óè‚óè‚óè‚óè  \n2.D ‚óè  \n3.A ‚óè  \n4.C ‚óè  \n5.E  ‚óè\nAutoFL Predictions (R=5)\nA,B B B B,C D\n=3.0/R\n=0.6\n=0.5/R\n=0.1\n=1.0/R\n=0.2\n=0.0/R\n=0.0\n1\nA B C D E\n.5.511.5.5\nActual Locations\nFL Ranking\nconfidence=0.6\nFig. 2. Scoring and ranking candidate methods (depicted as black rectangles) based on five AutoFL prediction\noutcomes (depicted as colored rectangles). For every prediction outcome, the scores (represented as colored\ncircles) are evenly distributed among all the methods included in that particular outcome.\nsuch methods, they are primarily sorted in descending order of the number of failing tests covering\neach method. In case of ties among them, we give priority to methods that are more frequently\nmentioned during the function interaction process of AutoFL (Fig. 1, 2 ), based on the intuition\nthat methods that are inspected by the LLM or related to inspected methods are more likely to be\nfaulty than methods that were never observed in the debugging process.\nFinally, after producing a complete ranked list of suspicious methods, e.g., [B, D, A, C, E] in\nFig. 2, our next step involves estimating the confidence in the final predictions. We gauge the level\nof confidence based on the consistency of the LLM prediction results across multiple iterations,\nmotivated by the previous work on LLM self-consistency [ 39]. The intuition is that if the LLM\nconsistently produces similar predictions, we would have more confidence in the results. Therefore,\nwe select the highest score among the methods covered by failing tests, and then use it to determine\nthe confidence score ùëÄ, as follows:\nconfidence = max\nùëö‚ààùëÄ\nùë†ùëêùëúùëüùëí(ùëö) (2)\nHaving a confidence measure helps the usability of AutoFL in two main aspects, backed by our\nresults. First, higher confidence tends to correlate with better results; thus, a practitioner may pick\na confidence threshold to automatically filter suggestions. Second, users also indicate a preference\nfor techniques that can indicate confidence for efficient use.\n4 EXPERIMENTAL SETTINGS\nThis section details the experimental setup used to evaluate AutoFL.\n4.1 Research Questions\nRQ1. How accurately does AutoFL localize faults?\n‚Ä¢Evaluation Metric: FL performance is evaluated with the acc@k metric which measures the\nnumber of bugs for which an actual buggy location was within the top ùëò suggestions of a tool,\nas previous work suggests developers are only willing to look at a few suggested locations\nwhen debugging [20, 33], and as this metric is often used by prior work [23, 52, 53]. To deal\nwith ties in the ranking, instead of using the average tiebreaker, we use the ordinal tiebreaker,\nas we believe it is closer to what a developer would experience when using an FL tool.\n‚Ä¢Baselines: We evaluate the FL capability of AutoFL using a total of 798 reproducible bugs from\nDefects4J (Java) and BugsInPy (Python); see the dataset details in Section 4.2.1). For Defects4J,\nwe compare against the best-performing standalone techniques from Zou et al. [53], namely the\nOchiai [1] and DStar [44] SBFL techniques and the Metallaxis [32] MBFL technique. To ensure\nconsistency with our experimental setup, we recalculated the method-level acc@k metrics\n, Vol. 1, No. 1, Article . Publication date: July 2024.\nA Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization 9\nfor the same set of bugs using the publicly available replication package provided by Zou et\nal. We also compare against the state-of-the-art standalone FL technique, SmartFL [ 52]. As\nSmartFL was only evaluated on a subset of Defects4J due to the complex Java features used\nin Closure [52], the comparison involves only these 222 Defects4J bugs excluding 131 bugs\nfrom Closure. Finally, we also compare against the Test-LLM baseline, which predicts the fault\nlocation using an LLM based on the failing test and error message alone (i.e., it is AutoFL\nwithout function interaction). Meanwhile, for the BugsInPy dataset, we compared against the\nSBFL results reported by Widyasari et al. [41], again recalculating method-level acc@k values\nfor the same set of bugs using their replication package.\nRQ2. How well does the confidence from AutoFL align with the actual FL performance?\nBefore a user decides whether to inspect the generated FL results, it would be beneficial if\nAutoFL could accurately predict the performance of FL. Therefore, we investigate how well the\nprediction confidence from AutoFL (defined in Eq. (2)) aligns with FL performance. We calculate\nSpearman‚Äôs rank correlation coefficients between the estimated confidence values and the following\nthree widely used ranking metrics (higher is better). These metrics are used as they are defined for\nindividual FL outcomes, whereas acc@k in RQ1 aggregates performance over the entire dataset.\n‚Ä¢Precision@1 (ùëÉ@1): The precision at the top rank, meaning it equals 1 if the highest-ranked\nmethod is found to be faulty; otherwise, it is 0.\n‚Ä¢Reciprocal Rank (ùëÖùëÖ): The reciprocal rank of the highest-ranked faulty method.\n‚Ä¢Average Precision (ùê¥ùëÉ): The average precision values for each rank of the faulty methods.\nRQ3. How good are the quality of the explanations of AutoFL?\nAs an explainable FL technique, it is important to evaluate the quality of the explanations of\nAutoFL. We anticipate that developers will receive AutoFL-generated FL rankings, with each\nmethod on the list paired with a set of explanations identifying why the method is considered faulty.\nTo this end, we manually evaluated 300 explanations from 60 bugs, which were randomly selected\nfrom the Defects4J dataset. Explanations were evaluated on the following four criteria, modeled\nafter the bug explanation evaluation criteria from Mahbub et al. [26], which are also showcased\nthrough real examples in Figure 7.\n‚Ä¢Accurate: the explanation contains a detailed description of why the bug occurs, which goes\nbeyond simply explaining the error message.\n‚Ä¢Imprecise: the explanation contains an inaccurate statement.\n‚Ä¢Concise: the explanation succinctly describes why the bug occurs, without extraneous content.\n‚Ä¢Useful: the explanation correctly describes how to fix the bug.\nAdditionally, for the evaluation of the explanations, each explanation was rated as a whole, rather\nthan in fragments. To minimize human error during the evaluation, two authors manually evaluated\neach explanation and resolved differences through discussion; these final explanation evaluation\nresults are used in our analysis. We further analyze the relationship between FL confidence and\nexplanation quality - assuming FL results are selectively shown via confidence, AutoFL would be\nmore useful if explanations tend to be of higher quality when FL results are shown.\nRQ4. How do professional developers feel about the explanations of AutoFL?\nTo evaluate the practical impact of explainable FL techniques, we invited 16 professional develop-\ners to use the explanations of AutoFL to debug bugs from the BugsInPy benchmark. In particular,\ndevelopers are asked to debug two bugs that we selected to be of moderate difficulty following\nprior work [49] for about one hour. Based on this, we present how professional developers feel\nabout the explanations of AutoFL, what still needs to be improved, and in this process identify\naspects that future explainable automated debugging techniques should place particular focus on.\n, Vol. 1, No. 1, Article . Publication date: July 2024.\n10 Kang, An, and Yoo\nTable 2. Details of the bug datasets. The terms #FTs and #PMs refer to the average number of failing tests\nand patched methods, respectively. The calculation of patched methods excludes omission bugs. Projects\nmarked with ‚Ä†originate from Defects4J [15], whereas the remaining ones are sourced from BugsInPy [42].\nProject #Bugs #FTs #PMs Project #Bugs #FTs #PMs Project #Bugs #FTs #PMs\nPySnooper 2 1.00 2.00 luigi 28 1.29 1.65 tornado 14 1.14 1.15\nansible 11 1.27 1.45 matplotlib 27 1.04 1.58 youtube-dl 42 1.00 1.39\nblack 22 1.05 2.43 pandas 165 1.25 1.53 Chart‚Ä† 26 3.54 1.52\ncookiecutter 4 1.25 2.00 sanic 3 1.00 1.00 Closure‚Ä† 131 2.63 1.25\nfastapi 16 1.94 2.42 scrapy 38 1.45 1.17 Lang‚Ä† 64 1.92 1.38\nhttpie 5 1.00 1.25 spacy 2 1.00 1.00 Math‚Ä† 106 1.66 1.32\nkeras 36 1.17 2.37 thefuck 30 1.27 1.24 Time‚Ä† 26 2.85 1.56\n4.2 Experimental Details\nThe experimental details for our study are provided.\n4.2.1 Evaluation Dataset. Our experiment is conducted using a total of 798 bugs from 21 open-\nsource projects as listed in Table 2. We use two different bug benchmarks: Defects4J v1.0 [ 15]\n(Java), chosen to allow comparison with traditional techniques, and BugsInPy [ 42] (Python), to\ndemonstrate that AutoFL can quickly be adapted to other languages as well. For Defects4J, all\nactive 353 bugs in Defects4J v1.0 are used. For BugsInPy, while the core implementation of AutoFL\ndid not need to be updated, we modified the callable function set, as the tests in BugsInPy tended\nto cover a large number of classes, while Python often includes its comments within the function\nbody. We use the improved BugsInPy dataset by Aguilar et al. [3]; nonetheless, 56 out of 501 bugs\nare excluded due to reproducibility issues, leaving 445 bugs for consideration. Further details are\nprovided in the supplementary material [6].\n4.2.2 AutoFL Default Configurations.For each bug, we runAutoFL five times, i.e.,ùëÖ = 5, using the\ngpt-3.5-turbo-0613 language model from OpenAI; this number is chosen as we found diminishing\nreturns for ùëÖ > 5 in a preliminary study we performed on a subset of the bugs from Defects4J.\nWe also set the maximum number of function interactions to ùëÅ = 10; this number was chosen as\n(i) only a small proportion of runs actually use up to 10 runs (less than 10%), and (ii) preliminary\nexperiments with ùëÅ = 20 caused the LLMs to exceed their context length limits often, dropping the\nperformance of AutoFL by half. While we treat the gpt-3.5-turbo-0613 results as our ‚Äòmain‚Äô results\nand use them in subsequent analysis, we also present the result of running AutoFL two times with\nthe gpt-4-0613 model to show performance when using an improved LLM, and to show that GPT-4\nalso benefits from our result combination mechanism (Section 3.3). While AutoFL can function\neffectively with just one failing test as input, a variety of strategies can be employed for AutoFL\nwhen dealing with multiple failing tests. In our experiment, when multiple failing tests are present,\nwe employ distinct failing test cases for each run of AutoFL. More specifically, when there are\nmultiple failing test cases (194 bugs out of 798 in total have multiple failing tests), a round-robin\napproach is adopted, selecting one failing test case for each run.\n4.2.3 Developer Feedback Details. Developers of three companies interested in LLM-based FL\nwere invited to use the explanations and FL from AutoFL to debug real bugs from open-source\nsoftware, and provide qualitative feedback. Specifically, employees of each company who had prior\nexperience with working with the authors internally recruited other employees who were available\nto participate in the experiment. The average participant had more than five years of experience\nwith software development, and more than 1.8 years of development experience with Python. We\nmade this choice as there are no clear baselines for explainable FL, and as we believed that providing\n, Vol. 1, No. 1, Article . Publication date: July 2024.\nA Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization 11\n1 2 3 4 5\nk\n100\n120\n140\n160\n180\n200\n220\n240acc@k\nFL Performance Comparison (Java)\nD4J v1.0\nAutoFL-GPT4 (x2)\nAutoFL-GPT3.5 (x5)\nTest-GPT3.5 (x5)\nDStar\nOchiai\nMetallaxis\nD4J v1.0 w/o Closure\nAutoFL-GPT3.5 (x5)\nSmartFL\n(a) FL evaluation on Defects4J\n1 2 3 4 5\nk\n50\n100\n150\n200\n250acc@k\nFL Performance Comparison (Python)\nAutoFL-GPT4 (x2)\nAutoFL-GPT3.5 (x5)\nTest-GPT3.5 (x5)\nDStar\nOchiai (b) FL evaluation on BugsInPy\nFig. 3. Performance of various FL techniques on the Defects4J and BugsInPy benchmarks.\nexplanation design guidelines from developer experience would be beneficial to future work. Prior\nto conducting our main study, a pilot study was conducted to obtain feedback and thus improve\nthe fidelity of our results, as is recommended [19], which helped us streamline the process. Overall,\nwe could recruit 16 professional developers who could participate in our study in full.\nIn the study, developers were presented with real-world bugs with failing tests, and asked\nto generate a patch that would fix the bug, similarly to B√∂hme et al. [ 8]. We used bugs from\nthe BugsInPy dataset, as Python developers were easier to recruit than C or Java developers.\nDevelopers were provided the error message, a failing test, FL results from AutoFL, and critically\n10 bug explanations fromAutoFL (5 from both GPT-3.5 and GPT-4, and translated from the original\nEnglish for participant convenience) for their task. The test execution environment was set up so\nthat participants could freely execute tests and perform print debugging. Each participant would\nfirst go through a tutorial in which they were asked to fix a simple bug and presented with the\nground-truth patch, so that they could get used to the experiment. In turn, developers would debug\ntwo bugs over the course of an hour, based solely on the error message and the FL/explanations that\nAutoFL generated. When the developer has made patches for every bug, or when they deem the\nbug too difficult to debug, we perform a semi-structured interview in which we ask the developer\nwhether FL and FL explanations would be useful in their workflow, what their view of the strengths\nand weaknesses of AutoFL-generated explanations are, and what they believe an ideal explanation\nwould be. The common themes from developer responses are analyzed and discussed between two\nauthors, and a final multi-label tagging of each developer‚Äôs response is generated. Based on this\ntagging, we present the popular themes as results, and present them with quotes from developers.\nFurther details about our developer study can be found in our supplementary material [6].\n5 RESULTS\nThis section presents the results of our experiments.\n5.1 RQ1: FL Efficacy\nIn Fig. 3, AutoFL is compared with other standalone techniques that report method-level perfor-\nmance. For the Defects4J dataset (Fig. 3a), the graph shows that AutoFL, when using GPT-3.5 as\nthe language model, outperforms SBFL and MBFL, which were the best-performing standalone\nFL techniques from Zou et al. [ 53], on the acc@1 measure. Furthermore, AutoFL outperforms\nthe state-of-the-art standalone FL technique SmartFL, which was only evaluated on a subset of\nDefects4J due to the complex Java features used in Closure [52];6 in contrast, AutoFL could be easily\n6On the same bug dataset, AutoFL still outperformed SmartFL, as shown in the square marker graphs of Fig. 3a.\n, Vol. 1, No. 1, Article . Publication date: July 2024.\n12 Kang, An, and Yoo\n1 2 3 4 5\nk\n120\n140\n160\n180\n200\n220\n240acc@k\nRerun To Performance (Java)\nGPT-4 (x2)\nGPT-4 (x1)\nGPT-3.5 (x5)\nGPT-3.5 (x4)\nGPT-3.5 (x3)\nGPT-3.5 (x2)\nGPT-3.5 (x1)\n(a) Defects4J\n1 2 3 4 5\nk\n100\n120\n140\n160\n180\n200\n220\n240\n260acc@k\nRerun To Performance (Python)\nGPT-4 (x2)\nGPT-4 (x1)\nGPT-3.5 (x5)\nGPT-3.5 (x4)\nGPT-3.5 (x3)\nGPT-3.5 (x2)\nGPT-3.5 (x1) (b) BugsInPy\nFig. 4. Performance of AutoFL as ùëÖincreases, for Defects4J and BugsInPy.\nTable 3. Execution time (seconds) per bug for AutoFL. Prep. denotes the data preparation phase, which\ninvolves gathering coverage of the failing test cases and obtaining the snippets of covered code.\nBenchmarkLLM Prep. (1) ùëÖ=1 ùëÖ=2(2) ùëÖ=5(2) Merge (3) Total (1+2+3)\nDefects4J GPT-3.5 4.87 16.40 - 82.00 0.37 87.24\nGPT-4 152.48 304.96 - 0.52 310.35\nBugsInPy GPT-3.5 23.50 34.71 - 173.55 0.28 197.33\nGPT-4 65.80 131.6 - 0.20 155.38\nexpanded to a completely different language, Python. However, the performance of AutoFL at\nacc@3 and acc@5 lags behind SBFL. This is likely because GPT-3.5 is still a limited LLM; our manual\ninspection of GPT-3.5 debugging traces reveals that it has difficulty ‚Äòdigging deep‚Äô into a repository\nto find bugs. This is further confirmed by our experiment with GPT-4: GPT-4 demonstrates stronger\nperformance on reasoning benchmarks than GPT-3.5 [30], and similarly AutoFL-GPT4 overcomes\nthe limitations of GPT-3.5 to consistently achieve better performance relative to all baselines up to\nacc@5. Comparing against the Test-GPT3.5 baseline, which prompts GPT-3.5 to predict the fault\nlocation without any function interaction, AutoFL consistently outperforms it, demonstrating that\nthe function interactions improve the performance ofAutoFL. On the Python benchmark BugsInPy,\nover which AutoFL was evaluated to demonstrate its multilingual capability, AutoFL-GPT3.5\noutperforms SBFL techniques by a substantial margin, as shown in Fig. 3b: AutoFL-GPT3.5 and\nAutoFL-GPT4 improved method-level acc@1 by 166.7% and 233.3% when compared with Ochiai.\nFor BugsInPy,AutoFL is performing on a consistent level with Defects4J, while SBFL is significantly\nworse in BugsInPy, as reported by Widyasari et al. [41]. Overall, AutoFL can perform consistently\neven while requiring fewer artifacts from the developer and it can be easily adapted to different\nlanguages, indicating the significant potential of LLM-based FL techniques.\nMeanwhile, merging more AutoFL runs to get an aggregate result helps improve performance\nby a large margin in both Defects4J and BugsInPy, as shown in Fig. 4. In addition, we observe when\nusing GPT-4, a single run can outperform five aggregated runs of AutoFL-GPT3.5, demonstrating\nthe potential of improved language models contributing to better FL performance.\nFinally, we provide additional information about the characteristics of AutoFL runs. A single\nrun (ùëÖ = 1) of AutoFL on a single bug on Defects4J using GPT-3.5 took 16.4 seconds on average,\nas shown in Table 3. Together with preparation (4.87s) and result aggregation (0.37s) for a single\nbug on average, the average runtime of AutoFL with five runs was 87.24 seconds. This is faster\nthan what Zou et al. [53] report as the time cost of SBFL (112 seconds), indicating that AutoFL\ncan operate as a lightweight FL technique. The runtime of AutoFL is dependent on the number of\n, Vol. 1, No. 1, Article . Publication date: July 2024.\nA Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization 13\n0.0 0.2 0.4 0.6 0.8 1.0\nProportion of Runs\nStep 0\nStep 1\nStep 2\nStep 3\nStep 4\nStep 5\nStep 6\nStep 7\nStep 8\nStep 9\nStep 10\nFunction Call Distribution at Each Step\nclass_cov\nmethod_cov\nsnippet\ncomments\n(a) Defects4J\n0.0 0.2 0.4 0.6 0.8 1.0\nProportion of Runs\nStep 0\nStep 1\nStep 2\nStep 3\nStep 4\nStep 5\nStep 6\nStep 7\nStep 8\nStep 9\nStep 10\nFunction Call Distribution at Each Step\npackage_cov\nclass_cov\nmethod_cov\nsnippet (b) BugsInPy\nFig. 5. Function call distribution for AutoFL-GPT3.5.\n2 4 6 8 10\nAvg. Number of Function Calls Made\n(True, True)\n47 bugs\n(True, False)\n86 bugs\n(False, True)\n34 bugs\n(False, False)\n178 bugs\n(Called in Test, Exists in Trace)\np<0.008\np<0.008\np<0.008\n(a) Defects4J\n2 4 6 8 10\nAvg. Number of Function Calls Made\n(True, True)\n55 bugs\n(False, True)\n48 bugs\n(True, False)\n58 bugs\n(False, False)\n257 bugs\n(Called in Test, Exists in Trace)\np<0.008 (b) BugsInPy\nFig. 6. Average number of function calls made by AutoFL-GPT3.5, categorized by whether at least one of the\nfaulty methods is called in the failing test case and/or exists in the stack trace.\nfunction calls it makes; in our experiments, an average of 5.37 function calls are made to determine\nthe fault location. The type of function calls made with GPT-3.57 at each step is presented in Fig. 5,\nand indicates that the length of inference chains is diverse.\nOur analysis further explores how the localization difficulty of bugs is linked to the length of\nfunction call sequences. The difficulty is heuristically measured by whether at least one of the\nactual faulty methods is directly mentioned in the failing test code or appears in the stack trace. If\nAutoFL were capable of using function calls effectively, AutoFL would perform a deeper search\nthrough the function calls for ‚Äòharder‚Äô bugs that do not directly expose the faulty method. As\nexpected, ‚Äòharder‚Äô bugs tended to result in longer chains of function calls (Figure 6). In Defects4J\nand BugsInPy, AutoFL utilizes on average 0.6 and 0.9 additional function calls respectively to\naddress the hardest bugs, i.e., (False, False), compared to the easiest bugs, i.e., (True, True). A\none-way ANOVA confirmed significant differences (ùëù < 0.05) in the mean length of function calls\nacross different bug difficulties. Subsequent pairwise t-tests with Bonferroni adjustments applied\nto the p-value threshold (adjusting it to 0.008 for six combinations) identify which differences are\nstatistically significant, as indicated as gray lines in the figure.\nAnswer to RQ1: AutoFL shows comparable or superior performance relative to prior standalone\nFL techniques with less information, can easily be applied to multiple languages, operates on a\ntimescale of minutes, and adaptively makes function calls depending on bug characteristics.\n, Vol. 1, No. 1, Article . Publication date: July 2024.\n14 Kang, An, and Yoo\nTable 4. Spearman‚Äôs rank correlation coefficients between AutoFL confidence and FL performance metrics in\neach benchmark (with ‚Äò*‚Äô denoting p < 0.0001). AutoFL is rerun 5 times using GPT-3.5.\nCorrelation withPrecision@1 Reciprocal Rank Average Precision\nDefects4J +0.57* +0.67* +0.70*\nBugsInPy +0.52* +0.50* +0.49*\n5.2 RQ2: Predicting FL Accuracy via AutoFL Confidence\nTable 4 presents Spearman‚Äôs rank correlation coefficients that illustrate the relationship between\nthe confidence values and the three FL performance metrics, namely Precision@1, Reciprocal\nRank, and Average Precision, described in Section 4.1. In both benchmark datasets, Defects4J and\nBugsInPy, we observe statistically significant positive correlations between the AutoFL confidence\nvalues and the FL performance metrics. A depiction of performance distribution by confidence\nbins is presented in the supplementary material [6]. Furthermore, although the correlation is more\npronounced within the Defects4J dataset in comparison to the BugsInPy dataset, the correlation\nwith Precision@1 remains relatively consistent across both datasets, with respective values of\n0.57 and 0.52. We conjecture that the correlation between Confidence and Precision@1 is more\nconsistent because, unlike other metrics, both are determined solely by the top-ranked prediction.\nAnswer to RQ2: Our analysis reveals statistically significant positive correlations between the\nAutoFLconfidence values and FL performance metrics in both benchmark datasets. Consequently,\nthe confidence value of AutoFL can be used to filter out potentially inaccurate results.\n5.3 RQ3: Explanation Quality\nTo evaluate the explanations generated by AutoFL, two authors independently rated 300 expla-\nnations from Defects4J and then resolved any differences through discussion. To quantify the\noverall agreement between the authors, we consolidated all labels from multiple attributes into a\nsingle list for each author and conducted the agreement analysis accordingly. The agreement of the\ninitial evaluations made was 86.5%, and the Cohen‚Äôs ùúÖcoefficient used by prior work to measure\ninter-rater agreement [38] was 0.55, a fair to good level of agreement [12].\nThe quality evaluation results are presented in Table 5. In 83.7% of individual runs,AutoFL could\nsuccessfully generate an explanation; in the other cases, either AutoFL had gone over the LLM\ntoken limit or AutoFL had exhausted the function call budget. Regarding the other measure, among\nall 300 individual explanations generated by AutoFL, 20% of them contained correct descriptions\nabout what was causing the bug (Accurate). Meanwhile, AutoFL would generate at least one\ninaccurate statement in 26.3% of all cases (Imprecise). The most common type of explanation was\n‚ÄòBland‚Äô (46.7%), in which AutoFL would generate an explanation that described the test and the\ncovered methods, but did not provide any additional analysis. When explanations are aggregated\nby bug rather than evaluated individually, at least one accurate explanation was generated for\n56.7% of all bugs, suggesting that with an appropriate selection of explanations, developers could\nbenefit from the explanations of AutoFL more often.\nIn this regard, there is a correlation between FL confidence explored in RQ2 and the average\nquality of explanations for a bug, as presented in Table 5, providing hints as to how to selectively\npresent good explanations. There is a modest improvement in all helpful qualities of explanations\nwhen filtering them by confidence, while the increase in the harmful quality (Imprecise) is negligible.\n7The results of the same analysis for GPT-4 are presented in the supplementary material [6]\n, Vol. 1, No. 1, Article . Publication date: July 2024.\nA Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization 15\nThe failing test \nSimpleRegressionTest.testSSENonNegati\nve() belongs to the \norg.apache.commons.math.stat.regressi\non package and is testing the \nSimpleRegression class.\nDefects4J Math-105 Run#2\nBland\nDefects4J Lang-47 Run#1\nAccurate, Concise, Useful\nDefects4J Math-30 Run#2\nAccurate, but also Imprecise\n(...)\nThe NullPointerException occurred at line \n1186, where str is attempted to be accessed. \nThis indicates that the obj argument passed to \nthe method is null, causing the getNullText \nmethod to be called, which returns null. \nConsequently, calling the length method on a \nnull object (str) results in a \nNullPointerException.\nTo Ô¨Åx this issue, the \nappendFixedWidthPadLeft method should \nhandle the case when the obj argument is null \nand provide a proper value for str to avoid the \nNullPointerException.\n(...) The test testBigDataSet in the class \nMannWhitneyUTest fails when asserting that the \ncalculated Mann-Whitney U statistic (result) is \nlarger than 0.1. The failure occurs in the line 113 \nof the test case. Upon investigation, the failure \nseems to be caused by incorrect calculations in \nthe methods mannWhitneyU and \ncalculateAsymptoticPValue.\nTo Ô¨Åx the issue, we suggest the following: (‚Ä¶)\n‚óè ConÔ¨Årm the correctness of the calculations \nfor EU (expected value) and VarU (variance) \nof the Mann-Whitney U statistic.\n‚óè Ensure that the computation of the standard \nscore z is correct based on the observed \nUmin, EU, and VarU. (...)\n(a) (b) (c)\nFig. 7. Example explanations from AutoFL. Explanations (b) and (c) are truncated for clarity.\nThese results show that there is both promise, and room to improve, when it comes to automatically\nidentifying helpful explanations from the human perspective, a topic we further discuss in Section 6.\nTo further clarify these results, we present three AutoFL-generated explanations in Figure 7. In\nFigure 7 (a), the explanation is true, but provides no real information about what the underlying\nbug is, so it is a ‚Äòbland‚Äô explanation that does not further the interests of the developer. Meanwhile,\nin Figure 7 (b), we see a good explanation that is simultaneously accurate, concise, and useful. The\nfirst part of the explanation (green) accurately describes how the error manifested by detailing\nwhich operations culminated in the error. In the next part of the explanation (blue) it goes on to\ndescribe what should be done to fix the issue, which corresponds with the actual developer fix. As\nall of the provided information is likely genuinely helpful and there is no extraneous content, the\nexplanation also qualifies as a concise explanation. Finally, in Figure 7 (c), we present an example\nof an explanation that is accurate but also partially imprecise. For this bug, the buggy method\nis not immediately called by the failing test, nor visible in the exception call stack. Despite this,\nthis explanation accurately pinpoints the method call chain leading to the bug (green), and thus is\naccurate as it provides information that could help the developer understand why each method\nwas suggested by AutoFL. However, the fixes suggested by the explanation are imprecise (red), as\nthey deviate from the actual developer fix. As this imprecise recommendation can be regarded as\nextraneous content, the explanation is not concise; nor is it useful, as the suggested fix is wrong.\nTo understand the circumstances in which AutoFL failed to generate good explanations, we\nanalyzed the 26 bugs for which no accurate explanations were generated. Overall, we found that\nthere were four main causes of failure. In 14 cases (53.8% of failing bugs), we found that their tests\nrelied on custom test helper functions, particularly for the bugs from the Closure project, which\nhas non-conventional test cases [28]. As the LLM of AutoFL was generally unaware of the precise\nsemantics of these test cases, it spent most of its time retrieving the helper functions, and less\ntime inspecting potentially buggy code. This suggests when using AutoFL, ideally tests should be\nself-contained; further research is required to incorporate project-specific information effectively.\nIn the second scenario, AutoFL failed for six bugs as the tests used too many classes and methods\nfor AutoFL to effectively inspect within the ùëÅ = 10 function call budget. In such cases, techniques\nsuch as test case purification may also help reduce the search space [50]. As for the remainder, in\nthree cases, the buggy methods were so long that they eventually caused a length error forAutoFL;\nmeanwhile, in three cases the LLM consistently made logical mistakes. The steady improvement\nof LLMs may help deal with these problems - indeed, GPT-4 could make correct explanations for\nthree of these six bugs (one for the context length limit and two for the logical mistakes).\n, Vol. 1, No. 1, Article . Publication date: July 2024.\n16 Kang, An, and Yoo\nTable 5. Explanation rating results of AutoFL-GPT3.5\nSubset Exists Accurate Imprecise Concise Useful ‚ÄòBland‚Äô Total\nIndividual Explanations83.7% 20.0% 26.3% 9.3% 8.0% 43.0% 300\n0.00‚â§Confidence<0.25 78.3% 10.0% 24.2% 3.3% 1.7% 46.7% 120\n0.25‚â§Confidence<0.50 87.5% 23.8% 28.8% 7.5% 11.3% 43.8% 80\n0.50‚â§Confidence<0.75 81.5% 26.2% 24.6% 16.9% 12.3% 36.9% 65\n0.75‚â§Confidence‚â§1.00 97.1% 34.3% 31.4% 20.0% 14.3% 40.0% 35\nAggregated By Bug 100% 56.7% 66.7% 31.7% 23.3% 93.3% 60\n0.00‚â§Confidence<0.25 100% 37.5% 70.8% 16.7% 8.3% 95.8% 24\n0.25‚â§Confidence<0.50 100% 62.5% 68.8% 31.3% 31.3% 93.8% 16\n0.50‚â§Confidence<0.75 100% 69.2% 53.8% 46.2% 30.8% 84.6% 13\n0.75‚â§Confidence‚â§1.00 100% 85.7% 71.4% 57.1% 42.9% 100% 7\nAnswer to RQ3: About 20% of AutoFL explanations accurately describe the root cause of the\nbug, per a manual assessment; for 56.7% of all bugs, an accurate explanation is generated at least\nonce. Interestingly, helpful explanations are more common for bugs when AutoFL is confident.\n5.4 RQ4: Developer Feedback\nFinally, we present a summary of the feedback we received from our semi-structured interviews of\ndevelopers, who debugged real-world bugs from the pandas project, sourced from BugsInPy. In\nthis section, we showcase the common answers from developers on seven key questions; further\nresponses and analysis can be found in the supplementary material [6]. In this section, we refer to\nindividual developers using their anonymous animal IDs used in our study, e.g., seal.\nIs FL Wanted?We asked if developers wanted to use FL; in our study, we had to clarify that this\nmeant being provided a list of suspicious code elements without explanations. Of the 16 developers,\n13 agreed or conditionally agreed that FL (even without explanations) would help their debugging\nefforts. Several developers remarked that the utility of FL would depend on their familiarity with\na project, noting that FL would be particularly helpful when the developer is unfamiliar with\nthe subject system, and less so when the developer has intimate knowledge. A small number of\ndevelopers disagreed that FL would help, as they were confident that they could perform FL based\non the error message. This generally positive attitude, albeit with conditions and reservations, is\nsimilar to what was reported by Kochhar et al. [20] in their survey of developer expectations on FL.\nAre FL Explanations Wanted?Critically for our work, we asked developers whether they\nwanted explanations for FL. Four developers described explanations for FL as necessary, while eight\nadditionally described explanations as useful; in total, twelve developers suggested that they would\nwant explanations when using FL. Developers commonly believed that explanations would help\nthem navigate unfamiliar code, and help them think through or fix the bug; for example, parrot\nnoted that ‚Äúwith just a location suggested, it‚Äôs vague how to fix the bug; with the cause of the bug\nexplained, it was easier to think‚Äù. Meanwhile, developers also expressed concern about explanations,\nwhich were generally conditional: one developer worried about the accuracy of explanations on\ndifficult bugs, and four thought that explanations would be unnecessary for easy code. On the\nflip side, this shows that every developer agreed that explanations would be helpful in unfamiliar\nprojects or difficult bugs, as long as the explanation is reasonably accurate.\nWhat were the strengths of explanations fromAutoFL? Based on the explanations suggested\nin our experiments, we asked developers to describe the strengths of the explanations generated by\nAutoFL. Developers appreciated that the explanations described the intention of the function(s)\nunder test, and (when fixes were provided) generally liked the fixes suggested by AutoFL. For\n, Vol. 1, No. 1, Article . Publication date: July 2024.\nA Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization 17\nexample, koala noted that ‚Äúthe error message alone didn‚Äôt provide much of a starting point; by\nfollowing and explaining the execution context of the bug, it was more convenient to solve the\ndebugging problems‚Äù. The most common theme was that developers liked a natural language\nexplanation of the error message, with five developers liking this aspect - for example, turtle\ndescribed the explanations as ‚Äúless clunky when compared to terminal messages, and more human-\nfriendly; even if it ultimately takes longer to read the message, it felt better. ‚Äù\nWhat was unnecessary in the explanations ofAutoFL? However, the same feature was\nalso seen as unnecessary as well. Two developers thought that the error message explanation\ndid not aid their understanding of the bug, suggesting customization or compartmentalization\nin explanations could help improve user experience in general; we discuss this issue in greater\ndepth in our discussion of the ideal explanation. Among developers who said that there was\nunnecessary content, most said that the overlapping content in the explanations (both between\nthe ten explanations that we provided per bug, and within each explanation) was unnecessary,\nand that it would be helpful to summarize explanations with the same content. One suggestion\nwas that clustering similar explanations together would be another way of improving developer\nconsumption of explanations. Outside of the overlapping content, most developers (10) did not find\nanything to remove from the explanations; they often noted that unnecessary content could be\nquickly skipped as well, indicating that it was not a significant problem.\nWhat was confusing in the explanations ofAutoFL? As the explanations are generally\npresented in a confident tone, there is a risk that developers could be confused by inaccurate\nexplanations. Indeed, while six developers found nothing confusing about the explanations, four\nothers thought that the suggestions of the explanations to fix tests were confusing, and two\ndevelopers noted that the patches or fault locations suggested by the explanations were insufficient\nto fix the bug, leading to confusion and wasting their time.\nWhat is the ideal explanation?We asked developers about what their ideal explanation of\na bug or FL results would be, to identify how the explanations of AutoFL could improve. Five\ndevelopers noted that having a clear template for explaining the bug and the fix would have helped;\nfor example, koala suggested a tripartite template which would clearly show ‚Äúwhere the bug\nhappened, why this is likely problematic behavior, and how to fix the bug‚Äù. Such a template would\nalso help accommodate the most commonly identified ideal features in a bug explanation - (i) the\nlogic of the failure, requested by five developers; (ii) the original intention of the code/test, requested\nby four developers; (iii) and finally a suggested patch, which was included in the ideal explanation of\n13 developers and was by far the most popular feature in an ideal bug/FL explanation. Furthermore,\nfour developers suggested that having real dynamic values incorporated in the explanation would\nhelp them understand the flow of the bug: koala noted that ‚ÄúI had to keep track of what the\nvariable values were for the bug-revealing test manually; if this information were integrated into\nthe explanation, that would be more convenient‚Äù. Finally, when asked about the ideal number of\nexplanations presented to the developer, 11 developers thought that the ten explanations presented\nin the experiment were too much, and wanted a summary. Nonetheless, they were open to seeing\nmultiple explanations - chicken noted that ‚Äúas long as the content is different, having multiple\noptions to choose from is good as well‚Äù. In this context,dragon suggested that ideally, explanations\nwould have a confidence value, which would help developers choose between multiple explanations.\nIn what order did developers read the explanations?We asked developers whether they had\nany heuristics on how to choose which explanation to read. While many answers were given, we\nfocused on the two most common answers. First, four developers answered that they had looked at\nthe FL list, and looked at explanations that supported the most likely location first. On the other\nhand, seven developers answered that they simply looked at the explanations from top to bottom,\nwithout consulting the FL list or any other information. This suggests that it would likely improve\n, Vol. 1, No. 1, Article . Publication date: July 2024.\n18 Kang, An, and Yoo\nuser experience and debugging time if the quality of explanations could be automatically assessed,\nand high-quality explanations could be presented first.\nAnswer to RQ4: Developers were generally supportive of explainable FL, and suggested that the\nexplanations of AutoFL were helpful by explaining the error message and the intention of each\nfunction. Meanwhile, developers disliked the overlap in content in the presented explanations,\nand found some explanations inaccurate. Ideally, developers wanted templated explanations that\nwould help them quickly find what they wanted, such as the bug logic or patch.\n6 FUTURE DIRECTIONS: BETTER QUALITY ESTIMATION FOR AUTOFL\nThe results of RQ4 make clear the need for selecting reliable bug explanations. The natural follow-up\nquestion is whether there are any features that can predict the quality of explanations. Explanations,\nbeing long-form text, are more difficult to automatically evaluate: they are not directly executable,\nnor is it possible to use self-consistency [39] to check which are good explanations, as explanations\ncan syntactically differ while having similar semantic content. One way of tackling this is to get\ndownstream artifacts that can be subjected to evaluation using self-consistency, such as FL predic-\ntions; as demonstrated in RQ2, there is a correlation between FL confidence and explanation quality.\nOn the other end, we may make executable downstream artifacts, and see whether explanations\nwith high-quality executable artifacts are generally of higher quality themselves. In this section,\nwe explore this question by presenting the preliminary results of such a prediction task on the\nexplanations of AutoFL, based on the following features:\n‚Ä¢Test Score: Based on a bug explanation from AutoFL, GPT-3.5 is repeatedly prompted to\ngenerate (i) a failing test that reproduces the bug and (ii) a passing test similar to the bug-\nreproducing test. The explanation is scored by the ratio of tests that behave as expected (i.e.,\nthe generated bug-reproducing test should fail). The intuition is that a good explanation of the\nbug could allow an LLM to generate bug-reproducing tests [17].\n‚Ä¢APR Score: Based on a bug explanation and the code of buggy methods suggested by AutoFL,\nGPT-3.5 is repeatedly prompted to generate a patch that would fix the bug. The explanation is\nscored by the ratio of partial fixes (e.g., the ratio of patches that make at least one previously\nfailing test pass). The intuition is that a good explanation of the bug could ease patch generation\nwhen using an LLM.\n‚Ä¢GPT Scores: GPT-3.5 is prompted to rate an explanation fromAutoFL on the same four quality\nmeasures (Accurate, Imprecise, Concise, Useful) that we used in RQ3 on a five-point Likert\nscale. The intuition is that an LLM may be capable of ‚Äòreflection‚Äô on its own answers [35], and\nbe capable of correctly identifying which explanations are accurate.\n‚Ä¢Explanation Length : An explanation is scored by its length. We include this as a simple\nbaseline to evaluate the validity of any results from the aforementioned metrics.\nThe non-parametric Spearman‚Äôs rank correlation coefficient between these features and the\nquality of explanations as evaluated in RQ3 is presented in Table 6. First, among the dynamic\nfeatures, the test score shows a statistically significant correlation with positive explanation qualities\nsuch as ‚ÄòAccurate‚Äô, while showing a low correlation for ‚ÄòWrong‚Äô. APR score shows similar correlation\ncharacteristics, but on a smaller magnitude, as partial patches were rarer (55 explanations from\nour manual examination led to a partial patch), and a slightly negative correlation with wrong\nexplanations. Overall, while dynamic features show promise in identifying truthful explanations,\ntheir correlation as of now is not strong enough to fully rely on. More detailed tables that present bug-\naggregated and bug-controlled correlation values are presented in the supplementary material [6].\n, Vol. 1, No. 1, Article . Publication date: July 2024.\nA Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization 19\nTable 6. Spearman Correlation between explanation quality predictors and actual quality. Results with\nùëù < 0.01 are marked with *, and results significant with ùëù < 0.001 are marked with **.\nname Test Score APR Score GPT useful Length\nAccurate +0.2358** +0.1946* +0.3759** +0.3009**\n‚ÄòWrong‚Äô (only imprecise)+0.0408 ‚àí0.0643 +0.3266** +0.3271**\nUseful +0.2635** +0.1942* +0.2371** +0.1585\n‚ÄòBland‚Äô ‚àí0.2364** ‚àí0.1105 ‚àí0.6026** ‚àí0.5391**\nFL Accurate +0.2737** +0.4923** +0.1437 +0.1528\n0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35\nvalue\nTest\nScore\nAPR\nScore\nFL Accurate\nFalse\nTrue\n(a) Mean of test and APR scores\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall\n0.4\n0.6\n0.8\n1.0Precision Original Confidence (AUC=0.785)\nTest Score-Boosted Confidence (AUC=0.812)\nAPR Score-Boosted Confidence  (AUC=0.827) (b) Precision-recall curve for P@1 prediction\nFig. 8. Relationship between dynamic scores and FL performance.\nAmong the GPT Scores, only GPTuseful showed interesting trends; it showed a significant cor-\nrelation (ùëù < 0.001) with every quality, including ‚ÄòWrong‚Äô, suggesting that GPT-3.5 will rate any\nexplanation that is detailed as useful. The Length feature provides a useful comparison as well:\nlike GPTuseful, Length shows a significant correlation with every quality (except ‚ÄòUseful‚Äô), and the\ncorrelation between Length and GPTuseful was also significant at 0.49. As a result, further research\nneeds to be conducted to make pure LLM-based evaluation of LLM-generated explanations viable.\nDuring the evaluation, the significant correlation between both of the dynamic scores (Test Score\nand APR score) and FL accuracy (bottom row of Table 6, Fig. 8a) piqued our interest: could dynamic\nscores be used to more accurately predict AutoFL‚Äôs performance? To explore this, we conducted\nexperiments that boosted scores of methods using the test score and APR as in Eq. (3), whereùëèùëúùëúùë†ùë°ùëñ\nis either the test score or the APR score of the ùëñ-th explanation. Intuitively, if an AutoFL run\nhas a higher dynamic score, the methods predicted in that run get a greater boost. Using APR to\nboost FL bears similarities to previous debugging work, such as MUSE [29] or ProFL [25], while\nthe combination of test generation and FL has not yet been explored to the best of our knowledge.\nùë†ùëêùëúùëüùëíùëõùëíùë§ (ùëö)= ùëöùëñùëõ(1,ùë†ùëêùëúùëüùëí (ùëö)√ó\n√ñ\n{1‚â§ùëñ‚â§ùëÖ|ùëö‚ààùëüùëñ }\n(1 +ùëèùëúùëúùë†ùë°ùëñ )) (3)\nFig. 8b presents the precision-recall curve for predicting Precision@1 based on the confidence\nthresholding on the Defects4J dataset. The new confidence values which incorporate dynamic\nscores are more accurate estimators of Precision@1 than the original confidence score, with the\narea under the curve (AUC) increased by up to 5.4% using the APR score. This shows that dynamic\nfeatures, i.e., the performance of the downstream tasks, can help refine AutoFL results as well.\nFuture Directions: Preliminary results show that dynamic evaluation of explanations has the\npotential to identify both helpful explanations and accurate FL.\n, Vol. 1, No. 1, Article . Publication date: July 2024.\n20 Kang, An, and Yoo\n7 THREATS TO VALIDITY\nInternal Validity The computation time of AutoFL varies, as it relies on OpenAI server conditions\nand the random nature of LLMs. To mitigate this, the time cost of AutoFL was averaged over\nmultiple runs. In evaluating the quality of explanations generated by AutoFL, human error in the\nvalidation process is possible. To address this, two authors independently assessed explanations\nand resolved disagreements. Data leakage is a concern with LLMs, e.g., the possibility of bug-fixing\ncommits being contained in their training data. However, the comparison results of AutoFL with\nthe Test-GPT3.5 baseline in RQ1, which shares the same model but does not interact with the\nfunctions, suggest that AutoFL‚Äôs performance is not solely due to model memorization.\nConstruct Validity To gather developer feedback, we engaged with professional developers to\nassess their experience with AutoFL. Due to security reasons, these developers tested AutoFL on\nan open-source project (pandas) rather than the projects that they are working on. Consequently,\ntheir responses and impressions may not entirely reflect the experiences that developers would\nhave during real-world debugging in their work projects.\nExternal Validity Our evaluation of AutoFL primarily focused on programs with unit tests from\nJava and Python. While our findings can be generalized within these contexts, they may not extend\nto other programming languages or different levels of testing. While we interviewed developers\nfrom three IT companies to gather insights into their experiences with AutoFL, their responses\nmay not be fully representative of all developers, given the diverse perspectives in the software\nindustry. Furthermore, the performance of AutoFL can be affected by various factors, such as the\nchoice of the language model.\n8 CONCLUSION\nThis paper presents AutoFL, an explainable LLM-based FL technique that has many useful char-\nacteristics that make it easier for practitioners to adopt, with particular strengths being its low\nrequirement of software artifacts (only a single failing test is required), reasonable runtime, and\ncritically its ability to generate explanations. Our evaluation shows the strong performance of\nAutoFL as a standalone FL tool, withAutoFL outperforming the baselines that we compare against.\nThe explanations generated by AutoFL required more nuanced evaluation: manual evaluation of\nthe explanations revealed AutoFL could generate an accurate explanation for 56.7% of all bugs.\nSurveying developers on what they wanted in bug explanations, many developers called for struc-\ntured explanations that explain the intention of tests, how the bug happened, and how to fix a\nbug; furthermore, they were willing to see only a few explanations. Motivated by this need to pick\nexplanations, we present preliminary results on automatically identifying high-quality explanations\nsuggesting that while dynamic features show promise, further research is necessary. Based on these\nresults, we hope to continue investigating how to consistently make useful explanations of bugs,\nwhich is one of the unique ways in which LLMs could benefit practitioners.\nDATA AVAILABILITY\nOur source code and data are publicly available at both GitHub [4] and Figshare [5] (archived).\nACKNOWLEDGEMENTS\nWe would also like to thank the anonymous reviewers for their thorough and helpful comments. This\nwork was supported by the National Research Foundation of Korea (NRF) funded by the Korean Gov-\nernment MSIT (RS-2023-00208998), the Engineering Research Center Program (2021R1A5A1021944),\nas well as the Institute of Information & Communications Technology Planning & Evaluation (IITP)\ngrant funded by the Korea government (MSIT) (2021-0-01001).\n, Vol. 1, No. 1, Article . Publication date: July 2024.\nA Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization 21\nREFERENCES\n[1] Rui Abreu, Peter Zoeteweij, and Arjan J.C. van Gemund. 2007. On the Accuracy of Spectrum-based Fault Localization.\nIn Testing: Academic and Industrial Conference Practice and Research Techniques - MUTATION (TAICPART-MUTATION\n2007). 89‚Äì98. https://doi.org/10.1109/TAIC.PART.2007.13\n[2] H. Agrawal, J.R. Horgan, S. London, and W.E. Wong. 1995. Fault localization using execution slices and dataflow\ntests. In Proceedings of Sixth International Symposium on Software Reliability Engineering. ISSRE‚Äô95 . 143‚Äì151. https:\n//doi.org/10.1109/ISSRE.1995.497652\n[3] Faustino Aguilar, Samuel Grayson, and Darko Marinov. 2023. Reproducing and Improving the BugsInPy Dataset. In\n2023 IEEE 23rd International Working Conference on Source Code Analysis and Manipulation (SCAM) . IEEE, 260‚Äì264.\nhttps://doi.org/10.1145/3368089.3417943\n[4] Gabin An and Sungmin Kang. 2024. GitHub Repository for the paper \"A Quantitative and Qualitative Evaluation of\nLLM-based Explainable Fault Localization\". https://github.com/coinse/autofl/\n[5] Gabin An and Sungmin Kang. 2024. Replication package of a paper \"A Quantitative and Qualitative Evaluation of\nLLM-based Explainable Fault Localization\" (FSE‚Äô24). https://doi.org/10.6084/M9.FIGSHARE.24203484\n[6] Gabin An, Sungmin Kang, and Shin Yoo. 2024. Supplementary material for the paper \"A Quantitative and Qualitative\nEvaluation of LLM-based Explainable Fault Localization\" (FSE‚Äô24). https://doi.org/10.6084/M9.FIGSHARE.25794009.V1\n[7] Thomas Bach, Artur Andrzejak, Changyun Seo, Christian Bierstedt, Christian Lemke, and Daniel Ritter et al. 2022.\nTesting Very Large Database Management Systems: The Case of SAP HANA. Datenbank-Spektrum 22, 3 (nov 2022),\n195‚Äì215. https://doi.org/10.1007/s13222-022-00426-x\n[8] Marcel B√∂hme, Ezekiel O. Soremekun, Sudipta Chattopadhyay, Emamurho Ugherughe, and Andreas Zeller. 2017.\nWhere is the Bug and How is It Fixed? An Experiment with Practitioners. In Proceedings of the 2017 11th Joint Meeting\non Foundations of Software Engineering (Paderborn, Germany) (ESEC/FSE 2017) . Association for Computing Machinery,\nNew York, NY, USA, 117‚Äì128. https://doi.org/10.1145/3106237.3106255\n[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, and Prafulla et al. Dhariwal. 2020.\nLanguage models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877‚Äì1901.\nhttps://doi.org/10.5555/3495724.3495883\n[10] Harrison Chase. 2022. LangChain. https://github.com/hwchase17/langchain\n[11] Xueying Du, Yiling Lou, Mingwei Liu, Xin Peng, and Tianyong Yang. 2023. KG4CraSolver: Recommending Crash\nSolutions via Knowledge Graph. In Proceedings of the 31st ACM Joint European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering (ESEC/FSE ‚Äô23) . ACM. https://doi.org/10.1145/3611643.3616317\n[12] Joseph L Fleiss, Bruce Levin, and Myunghee Cho Paik. 2013. Statistical methods for rates and proportions . John Wiley &\nSons. https://doi.org/10.1002/0471445428\n[13] Marko Ivankovic, Goran Petrovic, Ren√© Just, and Gordon Fraser. 2019. Code coverage at Google. In Proceedings of\nthe 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of\nSoftware Engineering . 955‚Äì963. https://doi.org/10.1145/3338906.3340459\n[14] Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. 2023. Impact of Code Language Models on Automated Program\nRepair. In Proceedings of the 45th International Conference on Software Engineering (Melbourne, Victoria, Australia)\n(ICSE ‚Äô23) . IEEE Press, 1430‚Äì1442. https://doi.org/10.1109/ICSE48619.2023.00125\n[15] Ren√© Just, Darioush Jalali, and Michael D. Ernst. 2014. Defects4J: A Database of Existing Faults to Enable Controlled\nTesting Studies for Java Programs. In Proceedings of the 2014 International Symposium on Software Testing and Analysis\n(San Jose, CA, USA) (ISSTA 2014) . Association for Computing Machinery, New York, NY, USA, 437‚Äì440. https:\n//doi.org/10.1145/2610384.2628055\n[16] Sungmin Kang, Bei Chen, Shin Yoo, and Jian-Guang Lou. 2023. Explainable Automated Debugging via Large Language\nModel-driven Scientific Debugging. https://doi.org/10.48550/arXiv.2304.02195 arXiv:2304.02195 [cs.SE]\n[17] Sungmin Kang, Juyeon Yoon, and Shin Yoo. 2023. Large Language Models are Few-shot Testers: Exploring LLM-based\nGeneral Bug Reproduction. In Proceedings of the 45th IEEE/ACM International Conference on Software Engineering (ICSE\n2023). https://doi.org/10.1109/ICSE48619.2023.00194\n[18] Yong Woo Kim. 2003. Efficient use of code coverage in large-scale software development. In Proceedings of the 2003\nconference of the Centre for Advanced Studies on Collaborative research . 145‚Äì155.\n[19] Amy J. Ko, Thomas D. LaToza, and Margaret M. Burnett. 2015. A Practical Guide to Controlled Experiments of\nSoftware Engineering Tools with Human Participants. Empirical Softw. Engg. 20, 1 (feb 2015), 110‚Äì141. https:\n//doi.org/10.1007/s10664-013-9279-3\n[20] Pavneet Singh Kochhar, Xin Xia, David Lo, and Shanping Li. 2016. Practitioners‚Äô Expectations on Automated Fault\nLocalization. In Proceedings of the 25th International Symposium on Software Testing and Analysis (Saarbr√ºcken,\nGermany) (ISSTA 2016). Association for Computing Machinery, New York, NY, USA, 165‚Äì176. https://doi.org/10.1145/\n2931037.2931051\n, Vol. 1, No. 1, Article . Publication date: July 2024.\n22 Kang, An, and Yoo\n[21] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large Language Models\nare Zero-Shot Reasoners. https://doi.org/10.48550/arXiv.2205.11916 arXiv:2205.11916 [cs.CL]\n[22] Yiƒüit K√º√ß√ºk, Tim A. D. Henderson, and Andy Podgurski. 2021. Improving Fault Localization by Integrating Value\nand Predicate Based Causal Inference Techniques. In Proceedings of the 43rd International Conference on Software\nEngineering (Madrid, Spain) (ICSE ‚Äô21) . IEEE Press, 649‚Äì660. https://doi.org/10.1109/ICSE43902.2021.00066\n[23] Xia Li, Wei Li, Yuqun Zhang, and Lingming Zhang. 2019. DeepFL: Integrating Multiple Fault Diagnosis Dimensions\nfor Deep Fault Localization. In Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing\nand Analysis (Beijing, China) (ISSTA 2019) . Association for Computing Machinery, New York, NY, USA, 169‚Äì180.\nhttps://doi.org/10.1145/3293882.3330574\n[24] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2021. Fault Localization with Code Coverage Representation Learning. In\nProceedings of the 43rd International Conference on Software Engineering (Madrid, Spain) (ICSE ‚Äô21). IEEE Press, 661‚Äì673.\nhttps://doi.org/10.1109/ICSE43902.2021.00067\n[25] Yiling Lou, Ali Ghanbari, Xia Li, Lingming Zhang, Haotian Zhang, Dan Hao, and Lu Zhang. 2020. Can automated\nprogram repair refine fault localization? a unified debugging approach. In Proceedings of the 29th ACM SIGSOFT\nInternational Symposium on Software Testing and Analysis . 75‚Äì87. https://doi.org/10.1145/3395363.3397351\n[26] Parvez Mahbub, Ohiduzzaman Shuvo, and Mohammad Masudur Rahman. 2023. Explaining Software Bugs Leveraging\nCode Structures in Neural Machine Translation. In2023 IEEE/ACM 45th International Conference on Software Engineering\n(ICSE). IEEE. https://doi.org/10.1109/icse48619.2023.00063\n[27] Alexandru Marginean, Johannes Bader, Satish Chandra, Mark Harman, Yue Jia, Ke Mao, Alexander Mols, and Andrew\nScott. 2019. SapFix: Automated End-to-End Repair at Scale. In 2019 IEEE/ACM 41st International Conference on Software\nEngineering: Software Engineering in Practice (ICSE-SEIP) . 269‚Äì278. https://doi.org/10.1109/ICSE-SEIP.2019.00039\n[28] Matias Martinez, Thomas Durieux, Romain Sommerard, Jifeng Xuan, and Martin Monperrus. 2017. Automatic repair\nof real bugs in java: A large-scale experiment on the defects4j dataset. Empirical Software Engineering 22 (2017),\n1936‚Äì1964. https://doi.org/10.1007/s10664-016-9470-4\n[29] Seokhyeon Moon, Yunho Kim, Moonzoo Kim, and Shin Yoo. 2014. Ask the Mutants: Mutating Faulty Programs for Fault\nLocalization. In 2014 IEEE Seventh International Conference on Software Testing, Verification and Validation . 153‚Äì162.\nhttps://doi.org/10.1109/ICST.2014.28\n[30] OpenAI. 2023. GPT-4 Technical Report. https://doi.org/10.48550/arXiv.2303.08774 arXiv:2303.08774 [cs.CL]\n[31] OpenAI. 2024. OpenAI documentation - Function Calling. https://platform.openai.com/docs/guides/function-calling\nLast accessed on May 13, 2024.\n[32] Mike Papadakis and Yves Le Traon. 2015. Metallaxis-FL: Mutation-Based Fault Localization. Softw. Test. Verif. Reliab.\n25, 5‚Äì7 (aug 2015), 605‚Äì628. https://doi.org/10.1002/stvr.1509\n[33] Chris Parnin and Alessandro Orso. 2011. Are Automated Debugging Techniques Actually Helping Programmers?. In\nProceedings of the 2011 International Symposium on Software Testing and Analysis (Toronto, Ontario, Canada) (ISSTA\n‚Äô11). Association for Computing Machinery, New York, NY, USA, 199‚Äì209. https://doi.org/10.1145/2001420.2001445\n[34] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. HuggingGPT:\nSolving AI Tasks with ChatGPT and its Friends in Hugging Face. https://doi.org/10.48550/arXiv.2303.17580\narXiv:2303.17580 [cs.CL]\n[35] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023.\nReflexion: Language Agents with Verbal Reinforcement Learning. https://doi.org/10.48550/arXiv.2303.11366\narXiv:2303.11366 [cs.AI]\n[36] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval Augmentation Reduces\nHallucination in Conversation. https://doi.org/10.48550/arXiv.2104.07567 arXiv:2104.07567 [cs.CL]\n[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, and Yasmine Babaei et al. 2023. Llama 2:\nOpen Foundation and Fine-Tuned Chat Models. https://doi.org/10.48550/arXiv.2307.09288 arXiv:2307.09288 [cs.CL]\n[38] Song Wang, Nishtha Shrestha, Abarna Kucheri Subburaman, Junjie Wang, Moshi Wei, and Nachiappan Nagappan.\n2021. Automatic Unit Test Generation for Machine Learning Libraries: How Far Are We?. In 2021 IEEE/ACM 43rd\nInternational Conference on Software Engineering (ICSE) . 1548‚Äì1560. https://doi.org/10.1109/ICSE43902.2021.00138\n[39] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny\nZhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. InThe Eleventh International\nConference on Learning Representations . https://doi.org/10.48550/arXiv.2203.11171\n[40] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, Quoc Le, and Denny Zhou. 2022.\nChain of Thought Prompting Elicits Reasoning in Large Language Models. ArXiv abs/2201.11903 (2022). https:\n//doi.org/10.48550/arXiv.2201.11903\n[41] Ratnadira Widyasari, Gede Artha Azriadi Prana, Stefanus Agus Haryono, Shaowei Wang, and David Lo. 2022. Real\nworld projects, real faults: evaluating spectrum based fault localization techniques on Python projects. Empirical\nSoftware Engineering 27, 6 (2022), 147. https://doi.org/10.1007/s10664-022-10189-4\n, Vol. 1, No. 1, Article . Publication date: July 2024.\nA Quantitative and Qualitative Evaluation of LLM-Based Explainable Fault Localization 23\n[42] Ratnadira Widyasari, Sheng Qin Sim, Camellia Lok, Haodi Qi, Jack Phan, Qijin Tay, Constance Tan, Fiona Wee,\nJodie Ethelda Tan, Yuheng Yieh, Brian Goh, Ferdian Thung, Hong Jin Kang, Thong Hoang, David Lo, and Eng Lieh\nOuh. 2020. BugsInPy: A Database of Existing Bugs in Python Programs to Enable Controlled Testing and Debugging\nStudies. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on\nthe Foundations of Software Engineering (Virtual Event, USA) (ESEC/FSE 2020) . Association for Computing Machinery,\nNew York, NY, USA, 1556‚Äì1560.\n[43] Emily Rowan Winter, Vesna Nowack, David Bowes, Steve Counsell, Tracy Hall, S√¶mundur Haraldsson, John Woodward,\nSerkan Kirbas, Etienne Windels, Olayori McBello, Abdurahman Atakishiyev, Kevin Kells, and Matthew Pagano.\n2022. Towards Developer-Centered Automatic Program Repair: Findings from Bloomberg. In Proceedings of the 30th\nACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering\n(Singapore, Singapore) (ESEC/FSE 2022) . Association for Computing Machinery, New York, NY, USA, 1578‚Äì1588.\nhttps://doi.org/10.1145/3540250.3558953\n[44] W. Eric Wong, Vidroha Debroy, Ruizhi Gao, and Yihao Li. 2014. The DStar Method for Effective Software Fault\nLocalization. IEEE Transactions on Reliability 63, 1 (2014), 290‚Äì308. https://doi.org/10.1109/TR.2013.2285319\n[45] W. Eric Wong, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016. A Survey on Software Fault Localization.\nIEEE Transactions on Software Engineering 42, 8 (2016), 707‚Äì740. https://doi.org/10.1109/TSE.2016.2521368\n[46] Yonghao Wu, Zheng Li, Jie M. Zhang, Mike Papadakis, Mark Harman, and Yong Liu. 2023. Large Language Models in\nFault Localisation. https://doi.org/10.48550/arXiv.2308.15276 arXiv:2308.15276 [cs.SE]\n[47] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2022. Practical Program Repair in the Era of Large Pre-trained\nLanguage Models. arXiv preprint arXiv:2210.14179 (2022). https://doi.org/10.48550/arXiv.2210.14179\n[48] Chunqiu Steven Xia and Lingming Zhang. 2023. Conversational Automated Program Repair. https://doi.org/10.48550/\narXiv.2301.13246 arXiv:2301.13246 [cs.SE]\n[49] Xin Xia, Lingfeng Bao, David Lo, and Shanping Li. 2016. ‚ÄúAutomated Debugging Considered Harmful‚Äù Considered\nHarmful: A User Study Revisiting the Usefulness of Spectra-Based Fault Localization Techniques with Professionals\nUsing Real Bugs from Large Systems. In 2016 IEEE International Conference on Software Maintenance and Evolution\n(ICSME). 267‚Äì278. https://doi.org/10.1109/ICSME.2016.67\n[50] Jifeng Xuan and Martin Monperrus. 2014. Test case purification for improving fault localization. In Proceedings of the\n22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering (Hong Kong, China) (FSE 2014) .\nAssociation for Computing Machinery, New York, NY, USA, 52‚Äì63. https://doi.org/10.1145/2635868.2635906\n[51] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing\nreasoning and acting in language models. arXiv preprint arXiv:2210.03629 (2022). https://doi.org/10.48550/arXiv.2210.\n03629\n[52] Muhan Zeng, Yiqian Wu, Zhentao Ye, Yingfei Xiong, Xin Zhang, and Lu Zhang. 2022. Fault Localization via Efficient\nProbabilistic Modeling of Program Semantics. InProceedings of the 44th International Conference on Software Engineering\n(Pittsburgh, Pennsylvania) (ICSE ‚Äô22) . Association for Computing Machinery, New York, NY, USA, 958‚Äì969. https:\n//doi.org/10.1145/3510003.3510073\n[53] Daming Zou, Jingjing Liang, Yingfei Xiong, Michael D. Ernst, and Lu Zhang. 2019. An empirical study of fault\nlocalization families and their combinations. IEEE Transactions on Software Engineering 47 (Feb. 2019), 332‚Äì347.\nhttps://doi.org/10.1109/TSE.2019.2892102\n, Vol. 1, No. 1, Article . Publication date: July 2024.",
  "topic": "Codebase",
  "concepts": [
    {
      "name": "Codebase",
      "score": 0.950956404209137
    },
    {
      "name": "Computer science",
      "score": 0.740682065486908
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6649980545043945
    },
    {
      "name": "Source code",
      "score": 0.6132460236549377
    },
    {
      "name": "Fault (geology)",
      "score": 0.5926101803779602
    },
    {
      "name": "Code (set theory)",
      "score": 0.551438570022583
    },
    {
      "name": "Process (computing)",
      "score": 0.545569896697998
    },
    {
      "name": "Software engineering",
      "score": 0.4871253967285156
    },
    {
      "name": "Software",
      "score": 0.47089675068855286
    },
    {
      "name": "Reliability engineering",
      "score": 0.4520943760871887
    },
    {
      "name": "Test case",
      "score": 0.446919709444046
    },
    {
      "name": "Function (biology)",
      "score": 0.442336767911911
    },
    {
      "name": "Computer engineering",
      "score": 0.4229617118835449
    },
    {
      "name": "Programming language",
      "score": 0.38644322752952576
    },
    {
      "name": "Machine learning",
      "score": 0.2842617630958557
    },
    {
      "name": "Engineering",
      "score": 0.15661412477493286
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Seismology",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Regression analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157485424",
      "name": "Korea Advanced Institute of Science and Technology",
      "country": "KR"
    }
  ],
  "cited_by": 8
}