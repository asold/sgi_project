{
  "title": "GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph",
  "url": "https://openalex.org/W3202105401",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2372592839",
      "name": "Yang, Junhan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2093779573",
      "name": "Liu Zheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221601469",
      "name": "Xiao, Shitao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202202832",
      "name": "Li, Chaozhuo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114070850",
      "name": "Lian, Defu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2290974901",
      "name": "Agrawal, Sanjay",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1987670803",
      "name": "Singh Amit",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A104931835",
      "name": "Sun, Guangzhong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127526945",
      "name": "Xie Xing",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2971164983",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3080997787",
    "https://openalex.org/W3021282678",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3154091824",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W2417677256",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3034828027",
    "https://openalex.org/W2962767366",
    "https://openalex.org/W3157563153",
    "https://openalex.org/W2242161203",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2970096436",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W3152893301",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2950336186",
    "https://openalex.org/W2538371562",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2964144561",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3154229486",
    "https://openalex.org/W3100848837",
    "https://openalex.org/W3113747735",
    "https://openalex.org/W2963460103"
  ],
  "abstract": "The representation learning on textual graph is to generate low-dimensional embeddings for the nodes based on the individual textual features and the neighbourhood information. Recent breakthroughs on pretrained language models and graph neural networks push forward the development of corresponding techniques. The existing works mainly rely on the cascaded model architecture: the textual features of nodes are independently encoded by language models at first; the textual embeddings are aggregated by graph neural networks afterwards. However, the above architecture is limited due to the independent modeling of textual features. In this work, we propose GraphFormers, where layerwise GNN components are nested alongside the transformer blocks of language models. With the proposed architecture, the text encoding and the graph aggregation are fused into an iterative workflow, {making} each node's semantic accurately comprehended from the global perspective. In addition, a {progressive} learning strategy is introduced, where the model is successively trained on manipulated data and original data to reinforce its capability of integrating information on graph. Extensive evaluations are conducted on three large-scale benchmark datasets, where GraphFormers outperform the SOTA baselines with comparable running efficiency.",
  "full_text": "GraphFormers: GNN-nested Transformers for\nRepresentation Learning on Textual Graph\nJunhan Yang‚ô¶‚àó, Zheng Liu ‚ô£‚Ä†, Shitao Xiao ‚ô†, Chaozhuo Li ‚ô£, Defu Lian ‚ô¶,\nSanjay Agrawal‚ô•, Amit Singh‚ô•, Guangzhong Sun‚ô¶, Xing Xie‚ô£\n‚ô¶ University of Science and Technology of China, Hefei, China\n‚ô£ Microsoft Research Asia, Beijing, China\n‚ô† Beijing University of Posts and Telecommunications, Beijing, China\n‚ô• Microsoft India Development Center, Bengaluru, India\nyangjun2@mail.ustc.edu.cn,\n{zhengliu,cli,siamit,xingx}@microsoft.com,\nstxiao@bupt.edu.cn,\n{liandefu,gzsun}@ustc.edu.cn,\nsanjayiitk0@gmail.com\nAbstract\nThe representation learning on textual graph is to generate low-dimensional embed-\ndings for the nodes based on the individual textual features and the neighbourhood\ninformation. Recent breakthroughs on pretrained language models and graph neu-\nral networks push forward the development of corresponding techniques. The\nexisting works mainly rely on the cascaded model architecture: the textual fea-\ntures of nodes are independently encoded by language models at first; the textual\nembeddings are aggregated by graph neural networks afterwards. However, the\nabove architecture is limited due to the independent modeling of textual features.\nIn this work, we propose GraphFormers, where layerwise GNN components are\nnested alongside the transformer blocks of language models. With the proposed\narchitecture, the text encoding and the graph aggregation are fused into an iterative\nworkflow, making each node‚Äôs semantic accurately comprehended from the global\nperspective. In addition, a progressive learning strategy is introduced, where the\nmodel is successively trained on manipulated data and original data to reinforce its\ncapability of integrating information on graph. Extensive evaluations are conducted\non three large-scale benchmark datasets, where GraphFormers outperform the\nSOTA baselines with comparable running efficiency. The source code is released\nat https://github.com/microsoft/GraphFormers .\n1 Introduction\nThe textual graph is a widely existed data format, where each node is annotated with its textual feature.\nThe representation learning on textual graph is to generate low-dimensional node embeddings based\non the individual textual features and the information from the neighbourhood. In recent years, the\nbreakthroughs in pretrained language models and graph neural networks contribute to the development\nof corresponding techniques. Particularly, with pretrained language models, such as BERT (Devlin\net al., 2018) and RoBERTa (Liu et al., 2019a), the underlying semantics of texts can be captured more\nprecisely; at the same time, with graph neural networks, like GraphSage (Hamilton et al., 2017a) and\nGAT (VeliÀáckovi¬¥c et al., 2018), neighbours can be effectively aggregated for more informative node\n‚àóWork was done by 2021.01 during Junhan Yang and Shitao Xiao‚Äôs internship in MSRA\n‚Ä†Corresponding author\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2105.02605v3  [cs.CL]  9 Oct 2023\n(A) Cascaded Transformers-GNN (B) GNN-nested TransformersTRMTRMTRMTRMTRMTRMTRMTRMTRMGNNGNNGNNTRMTRMTRMTRMTRMTRMTRMTRMTRMCN1 N2CN1N2CN1N2CN1N2N1N2C\nFigure 1: Model architecture comparison (a center node C is connected with two neighbours N1, N2).\n(A) Cascaded Transformers-GNN: text embeddings are independently generated by language models\nand aggregated by rear-mounted GNNs. (B) GNN-nested Transformers: the text encoding and graph\naggregation are iteratively performed with the layerwise GNNs and Transformers (TRM).\nembeddings. It is necessary to combine both techniques for better textual graph representation. As\nsuggested by GraphSage (Hamilton et al., 2017a) and PinSage (Ying et al., 2018), the textual feature\ncan be independently modeled by text encoders and further aggregated by rear-mounted GNNs for\nthe final node embeddings. Such a representation paradigm has been widely adopted by subsequent\nworks on various scenarios (Zhu et al., 2021; Li et al., 2021; Hu et al., 2020; Liu et al., 2019b; Zhou\net al., 2019), where GNNs are combined with powerful PLM-based text encoders.\nThe above way of combination is called the ‚ÄúCascaded Transformers-GNN‚Äù architecture (Figure 1 A),\nas the language models (built upon Transformers) are deployed ahead of the GNN component. With\nthe above architecture, the text encoding and the graph aggregation are performed in two consecutive\nsteps, where there is no information exchange between the nodes when text embeddings are generated.\nHowever, the above workflow is defective considering that the linked nodes are correlated, whose\nunderlying semantics can be mutually enhanced. For example, given a node ‚Äúnotes on transformers‚Äù\nand its neighbour ‚Äútutorials on machine translation‚Äù; by making reference to the whole context, the\n‚Äútransformers‚Äù here can be interpreted as a machine learning model, rather than an electric device.\nOur Work. We propose ‚ÄúGNN-nested Transformers‚Äù (GraphFormers), which are highlighted for\nthe fusion of GNNs and language models (Figure 1 B). In GraphFormers, the GNN components are\nnested alongside the transformer layers (TRM) of language models, where the text encoding and graph\naggregation are fused as an iteratively workflow. In each iteration, the linked nodes will exchange\ninformation with each other in the layerwise GNN component; thus, each node will be augmented\nby its neighbourhood information. The transformer component will work on the augmented node\nfeatures, where increasingly informative node representations can be generated for the next iteration.\nCompared with the cascaded architecture, GraphFormers achieve more sufficient utilization of the\ncross-node information on graph, which significantly benefit the representation quality. Given that the\nlayerwise GNN components merely involve simple and effective multi-head attention, GraphFormers\npreserve comparable running costs as the existing cascaded Transformers-GNN models.\nOn top of the proposed model architecture, we further improve GraphFormers‚Äô representation quality\nand practicability as follows. Firstly, the training of GraphFormers is likely to be shortcut: in\nmany cases, the center node itself can be ‚Äúsufficiently informative‚Äù, where the training tasks can be\naccomplished without leveraging the neighbourhood information. As such, GraphFormers may end\nup with insufficiently trained GNNs. Inspired by recent success of curriculum learning (Bengio et al.,\n2009), we propose to train the model progressively: the first round of training is performed with\nmanipulated data, where the nodes are randomly polluted; thus, it becomes harder to make prediction\nmerely rely on the center nodes, and the model will be forced to leverage the whole input nodes.\nThe second round of training gets back to the unpolluted data, where the model will be fit into the\ntargeted distribution. Another concern about GraphFormers is that all the linked nodes are mutually\ndependent in the representation process: once a new node is presented, all the neighbours, regardless\nof whether they have been processed before, need to be encoded from scratch. As a result, a great\ndeal of unnecessary computations will be incurred. We introduce unidirectional graph attention to\nalleviate this problem: only the center node is required to make reference to the neighbours, while the\n2\nneighbour nodes remain independently encoded. By this means, the existing neighbours‚Äô encoding\nresults can be cached and reused, which significantly saves the computation cost.\nExtensive evaluations are conducted with three million-scale textual graph datasets: DBLP, Wiki and\nProduct, where the representation quality is measured by the link prediction accuracy. According to\nour experiment results, GraphFormers significantly outperform the SOTA cascaded Transformers-\nGNN baselines with comparable running efficiency.\n2 Related Work\nThe textual graph representation is an important research topic in multiple areas, such as natural\nlanguage processing, information retrieval and graph learning (Yang et al., 2015; Wang et al., 2016b,a;\nYasunaga et al., 2017; Wang et al., 2019a; Xu et al., 2019). To learn high-quality representation for\ntextual graph, techniques on natural language understanding and graph representation need to be\njointly leveraged. In recent years, breakthroughs on pretrained language models (PLM) and graph\nneural networks (GNN) significantly advance the development of corresponding techniques.\nPLM. The PLMs are proposed to learn universal language models with neural networks trained\non large-scale corpus. The early works were based on shallow networks, e.g, word embeddings\nlearned by Skip-Gram (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). In recent years,\nthe backbone networks are being quickly scaled up: from EMLo (Peters et al., 2018), GPT (Radford\net al., 2018), to BERT (Devlin et al., 2018), XLNet (Yang et al., 2019), T5 (Raffel et al., 2019),\nGPT-3 (Brown et al., 2020). The large-scale models, which get fully trained with massive data,\ndemonstrate superior performances on general NLP tasks. One of the most critical usages of PLMs\nis text representation, where the underlying semantics of texts are captured by low-dimensional\nembeddings. Such embeddings achieve competitive results on downstream tasks, like text retrieval\nand classification (Reimers and Gurevych, 2019; Luan et al., 2020; Gao et al., 2021; Su et al., 2021).\nGNN. Graph neural networks are recognized as powerful tools of modeling graph data (Hamilton\net al., 2017b; Zhou et al., 2020). Such methods (e.g., GCN (Kipf and Welling, 2016), GAT (VeliÀáckovi¬¥c\net al., 2018), GraphSage (Hamilton et al., 2017a)) learn effective message passing mechanisms such\nthat information between the nodes can get aggregated for expressive graph representations.\nGraph neural networks may also incorporate node attributes, like texts; and it‚Äôs quite straightforward\nto leverage GNNs and PLMs for textual graph representation following the ‚Äúcascaded architecture‚Äù\nsuggested by GraphSage (Hamilton et al., 2017a): the node features are independently encoded\nat first; then, the node embeddings are aggregated via GNNs to generate the final representations.\nSuch a representation paradigm is widely adopted by subsequent works (Zhu et al., 2021; Li et al.,\n2021; Hu et al., 2020; Liu et al., 2019b; Zhou et al., 2019). However, the above approaches treat\nthe text encoding and graph aggregation as two consecutive steps, where the node-level features are\nindependently processed. Our work is different from these approaches as the text encoding and graph\naggregation are fused as an iterative workflow based on the ‚ÄúGNN-nested Transformers‚Äù.\n3 GraphFormers\nIn this work, we deal with textual graph data, where each node x is a text. The node x together with\nits neighbours Nx are denoted as Gx. Our model learns the embedding for node x based on its own\ntextual feature and the information of its neighbourhood Nx. The generated embeddings are expected\nto capture the relationship between the nodes, i.e., to accurately predict whether two nodes xq and xk\nare connected based on the embedding similarity.\n3.1 GNN-nested Transformers\nThe encoding process of GraphFormers is indicated as follows. The input nodes (the center node\nand its neighbours) are tokenized into sequences of tokens, with special tokens [CLS] padded in the\nfront, whose states are used for node representation. The input sequences are mapped into the initial\nembedding sequences {H0\ng}G based on the summation of word embeddings and position embeddings.\nThe embedding sequences are encoded by multiple layers of GNN-nested Transformers (shown as\nFigure 2), where the graph aggregation and text encoding are iteratively performed.\n3\n‚Ä¶ ‚Ä¶\nùëØùíàùüèùíçAdd & NormFeedForwardAdd & NormMulti-HeadAttentionùíÅùëÆùíçMulti-HeadAttentionùíõ‡∑úùíàùüèùíç\n ùëØùíàùíäùíçAdd & NormFeedForwardAdd & NormMulti-HeadAttentionùíõ‡∑úùíàùíäùíç\n ùëØùíàùëµùíçAdd & NormFeedForwardAdd & NormMulti-HeadAttentionùíõ‡∑úùíàùëµùíç√ó ùë≥ùíõùíàùüèùíç ùíõùíàùíäùíç ùíõùíàùëµùíçùíàùüèùíàùíä ùíàùëµùíÅ‡∑°ùëÆùíç ùëØ‡∑°ùíàùüèùíç ùëØ‡∑°ùíàùíäùíç ùëØ‡∑°ùíàùëµùíç\nFigure 2: GNN-nested Transformers (using the l-th layer for illustration). The graph aggregation\nis performed in the first place: the node-level embeddings {zl\ng}G are gathered from all the nodes\nand processed by the GNN component (the leftmost rectangle). The GNN processed node-level\nembeddings {ÀÜ zl\ng}G are dispatched to their original nodes, which forms the graph-augmented token-\nlevel embeddings. The graph-augmented token-level embeddings are further encoded by Transformer.\n‚Ä¢ Graph Aggregation in GNN. Each node is enhanced by its neighbourhood information based on\nthe layerwise graph aggregation. For each node in thel-th layer, the first token-level embedding (corre-\nsponding to [CLS]) is taken as the node-level embedding: zl\ng ‚Üê Hl\ng[0]. The node-level embeddings\nare gathered from all the nodes and passed to the layerwise GNN for graph aggregation. We leverage\nMulti-Head Attention (MHA) to encode the node-level embeddings Zl\nG ({zl\ng}G), similar as GAT\n(VeliÀáckovi¬¥c et al., 2018). For each attention head, the scaled dot-product attention is performed as:\nÀÜZl\nG = MHA(Zl\nG);\nMHA(Zl\nG) = Concat(head1, ...,headh);\nheadj = softmax(QKT\n‚àö\nd\n+ B)V;\nQ = Zl\nGWQ\nj ; K = Zl\nGWK\nj ; V = Zl\nGWV\nj ;\n(1)\nIn the above equations, WQ\nj , WK\nj , and WV\nj are the projection matrices of MHA, corresponding to\nthe j-th attention head. A learnable position bias B is added to the dot-product result; the positions\ndifferentiate the relationship between the nodes; i.e., ‚Äúcenter-to-center‚Äù (x to x), ‚Äúcenter-to-neighbour‚Äù\n(x to Nx), and ‚Äúneighbour-to-neighbour‚Äù (Nx to Nx), respectively.\nEach of the embeddings ÀÜ zl\ng (ÀÜ zl\ng ‚àà ÀÜZl\nG) is dispatched to its original node and concatenated (‚äï) with\nthe token-level embeddings, which gives rise to the graph-augmented token-level embeddings:\nbHl\ng ‚Üê Concat(ÀÜ zl\ng, Hl\ng). (2)\nIn this place, the GNN-processed node-level embeddings ÀÜZl\nG can be interpreted as ‚Äúmessagers‚Äù, with\nwhich the neighbourhood information can be introduced to each of the nodes.\n‚Ä¢ Text Encoding in Transformer. The graph-augmented token-level embeddings bHl\ng are pro-\ncessed by the transformer component (Vaswani et al., 2017), where the following computations are\nperformed:\nbHl\ng = LN(Hl\ng + MHAasy( bHl\ng));\nHl+1\ng = LN(bHl\ng + MLP(bHl\ng)).\n(3)\nIn the above equations, MLP is the Multi-Layer Projection unit, and LN is the Layer-Norm unit. We\nuse asymmetric Multi-Head Attention (MHAasy), where Q, K, V are computed as:\nQ = Hl\ngWQ\nj ; K = bHl\ngWK\nj ; V = bHl\ngWV\nj . (4)\nTherefore, the output sequence Hl+1\ng will be of the same length as the input sequence Hl\ng. The\nencoding result will be used as the input token-level embeddings for the next layer. The node-level\nembedding at the last layer zL\nx (i.e., HL\ng [0]) will be used as the final node representation.\n‚Ä¢ Workflow. We summarize GraphFormers‚Äô encoding workflow as Algorithm 1. The initial token-\nlevel embeddings {H0\ng}G are independently encoded by the first Transformer layer TRM0. For a\n4\nAlgorithm 1: GraphFormers‚Äô Workflow\nInput: The input graphs G (consist of the center node x and its neighbours).\nOutput: The embedding for the center node hx.\n1 begin\n2 for each text g ‚àà G do\n3 H1\ng ‚Üê TRM0(H0\ng); // Get the initial token-level embeddings\n4 for l = 1, ..., L‚àí 1 do\n5 Zl\nG ‚Üê {zl\ng|g‚ààG}; // Gather node-level embeddings to GNN\n6 ÀÜZl\nG ‚Üê GNN(Zl\nG); // Graph aggregation in GNN\n7 for each text g ‚àà G do\n8 bHl\ng ‚Üê Concat(ÀÜ zl\ng, Hl\ng); // Get graph-augmented token-level embeddings\n9 Hl+1\ng ‚Üê TRMl( bHl\ng); // Text encoding in Transformer\n10 Return hx ‚Üê zL\nx ;\nL-layer GraphFormers, the graph aggregation and text encoding are iteratively performed for the\nsubsequent L-1 steps (from 1 to L ‚àí1). In each step, the node-level embeddings Zl\nG are gathered and\nprocessed by the layerwise GNN component. The output node-level embeddings ÀÜZl\nG are dispatched\nto their original nodes, which generates the graph-augmented token-level embeddings bHl\ng. The graph-\naugmented token-level embedding are further processed by the Transformer component. Finally, The\nnode-level embedding (for the center node x) in the last layer zL\nx is taken as our representation result.\n‚Ä¢ Encoding Complexity. Given an input of M nodes, each one has P tokens; the time complexity of\neach layer‚Äôs encoding operation is O(M2 + MP 2): the graph aggregation takes O(M2), because M\nnode-level embeddings are gathered for multi-head attention; the text encoding takes O(MP 2), as\neach of the M node calls for the multi-head attention of P tokens. Compared with Transformers, the\nGNN‚Äôs computation cost is much smaller, mainly because of two reasons: 1)M2 ‚â™ MP 2 in general,\n2) operations like MLP are not needed in graph aggregation. Therefore, the working efficiency of\nGraphFormers is close to the cascaded GNN-Transformers as the extra computation cost of layerwise\ngraph aggregation is relatively small. Such a property is also empirically verified in our experiment.\n3.2 Model Simplification: Unidirectional Graph Aggregation\nOne concern about GraphFormers is that the input nodes are mutually dependent on each other during\nthe encoding process. As a result, to generate the embedding for a node, all the related nodes in its\nneighbourhood need to be encoded from scratch, regardless of whether they have been processed\nbefore. Such a property is unfavorable in practice as a great deal of unnecessary computation cost\nmight be incurred (i.e., a node will be repetitively encoded every time it serves as a neighbour node).\nWe leverage a simple but effective simplification, the unidirectional graph aggregation, to address\nthis problem. Particularly, only the center node x is required to make reference to the neighbourhood;\nwhile the rest of nodes Nx remain independently encoded all by their own textual features:\nHl+1\ng =\n(\nTRMl( cHlx), g= x;\nTRMl(Hl\ng), ‚àÄg ‚àà Nx. (5)\nBecause the encoding of the neighbour nodes is independent of the center node, the intermediate\nencoding results {z1...L\ng }Nx can be cached in storage3 and reused in subsequent computations when\nthey are needed. As a result, the nodes can be prevented from being encoded repetitively, which saves\na great deal of unnecessary computation cost. We empirically verify that GraphFormers maintain\nsimilar performances when the above simplification is introduced.\n3.3 Model Training: Two-Stage Progressive Learning\n‚Ä¢ Training Objective. We take advantage of link prediction as our training task. Given a pair of\nnodes q and k, the model is learned to predict whether they are connected based on their embedding\n3The encoding results can be kept in low-cost devices, whose storage capacity can be regarded as infinite.\n5\nTable 1: Specifications of the experimental datasets: the number of items, the number of neighbour\nnodes on average, and the number of training, validation, testing cases.\nProduct DBLP Wiki\n#Item 5,643,688 4,894,081 4,818,679\n#N 4.71 9.31 8.86\n#Train 22,146,934 3,009,506 7,145,834\n#Valid 30,000 60,000 66,167\n#Test 306,742 100,000 100,000\nsimilarity. Particularly, the following classification loss is minimized for a positive pair ofq and k:4\nL = ‚àílog exp(‚ü®hq, hk‚ü©)\nexp(‚ü®hq, hk‚ü©) +P\nr‚ààR exp(‚ü®hq, hr‚ü©). (6)\nIn the above equation, hq and hk are the node embeddings; ‚ü®¬∑‚ü© denotes the computation of inner\nproduct; R stands for the negative samples. In our implementation, we leverage ‚Äúin-batch negative\nsamples‚Äù (Karpukhin et al., 2020; Luan et al., 2020) for the reduction of encoding cost: a positive\nsample in one training instance will be used as a negative sample in the rest of the training instances\nwithin the same mini-batch.\n‚Ä¢ Two-stage Training. In GraphFormers, the information from the center node and neighbour\nnodes are not treated equally, which may undermine the model‚Äôs training effect. Particularly, the\ncenter node‚Äôs information can be directly utilized, while the neighbourhood information needs to be\nintroduced via three steps: 1) encoded as node-level embeddings, 2) making graph aggregation with\nthe center node, and 3) introduced to center node‚Äôs graph augmented token-level embeddings. The\nmessage passing pathway can shortcut when the center nodes are ‚Äúsufficiently informative‚Äù, i.e., two\nnodes are sufficiently similar with each other in terms of their own textual features, such that their\nconnection can be predicted without considering the neighbours. Given the existence of such cases,\nGraphFormers may end up with well-trained Transformers but insufficiently trained GNNs.\nTo alleviate the above problem, we introduce a warm-up training task, where the link prediction is\nmade based on the polluted input nodes. Particularly, for each input node g, a subset of its tokens gm\nwill be randomly masked5. As a result, the classification loss becomes:\nL‚Ä≤ = ‚àílog exp(‚ü®hÀúq, hÀúk‚ü©)\nexp(‚ü®hÀúq, hÀúk‚ü©) +P\nr‚ààR exp(‚ü®hÀúq, hÀúr‚ü©), (7)\nwhere hÀúq, hÀúk, hÀúr are the embeddings generated from the polluted nodes. The masked tokens reduce\nthe informativeness of each individual node; therefore, the model is forced to leverage the whole\ninput nodes to make the right prediction.\nFinally, the model training is organized as a two-stage progressive learning process. In the first\nstage, the model is trained to minimize L‚Ä≤ based on the polluted nodes until its convergence, which\nreinforce the model‚Äôs capability of integrating information on graph. In the second stage, the model\nis continually trained to minimize L based on the original data until the convergence, which makes\nthe model fit into the target distribution.\n4 Experimental Studies\n4.1 Data and Settings\nWe make use of the following three real-world textual graph datasets for our experimental studies.\n‚Ä¢ DBLP6, which contains the paper citation graph from DBLP up to 2020-04-09. Two papers are\nlinked if one is cited by the other one. The paper‚Äôs title is used as the textual feature.\n‚Ä¢ Wikidata5M7 (Wiki) (Wang et al., 2019b), which contains the entity graph from Wikipedia. The\nfirst sentence in each entity‚Äôs introduction is taken as its textual feature.\n4We remove the naive cases whereq and k are included by each other‚Äôs neighbour set, Nq and Nk.\n5We use the common MLM strategy, where 15% of the input tokens are masked: 80% of them are replaced\nby [MASK], the rest ones are replaced randomly or kept as the original tokens with the same probabilities.\n6https://originalstatic.aminer.cn/misc/dblp.v12.7z\n7https://deepgraphlearning.github.io/project/wikidata5m\n6\nTable 2: Overall evaluation (GraphFormers marked in bold, the best baseline underlined). Graph-\nFormers outperforms all baselines, especially the ones based on cascaded Transformers-GNN.\nProduct DBLP Wiki\nMethods P@1 NDCG MRR P@1 NDCG MRR P@1 NDCG MRR\nPLM 0.6563 0.7911 0.7344 0.5673 0.7484 0.6777 0.3466 0.5799 0.4712\nTNVE 0.4618 0.6204 0.5364 0.2978 0.5295 0.4163 0.1786 0.4274 0.2933\nIFTN 0.5233 0.6740 0.5982 0.3691 0.5798 0.4773 0.1838 0.4276 0.2945\nPLM+GAT 0.7540 0.8637 0.8232 0.6633 0.8204 0.7667 0.3006 0.5430 0.4270\nPLM+Max 0.7570 0.8678 0.8280 0.6934 0.8386 0.7900 0.3712 0.6071 0.5022\nPLM+Mean 0.7550 0.8671 0.8271 0.6896 0.8359 0.7866 0.3664 0.6037 0.4980\nPLM+Att 0.7513 0.8652 0.8246 0.6910 0.8366 0.7875 0.3709 0.6067 0.5018\nGraphFormers 0.7786 0.8793 0.8430 0.7267 0.8565 0.8133 0.3952 0.6230 0.5220\n‚Ä¢ Product Graph (Product), an even larger dataset of online products collected by a world-wide\nsearch engine. In this dataset, the users‚Äô web browsing behaviors are tracked for the targeted product\nwebpages (e.g., Amazon webpages of Nike shoes). The user‚Äôs continuously browsed webpages\nwithin a short period of time (e.g., 30 minutes) is called a ‚Äúsession‚Äù. The products within a common\nsession are connected in the graph (which is a common way of graph construction in e-commerce\nscenarios (Ying et al., 2018; Wang et al., 2018)). Each product has its unique textual description,\nwhich specifies information like the product name, brand, and saler, etc.\nThe textual features of all the datasets are in English. We make use of uncased WordPiece (Wu et al.,\n2016) to tokenize the input text. In our experiment, each text is associated with 5 uniformly sampled\nneighbours (without replacement); for texts with neighbourhood smaller than 5, all the neighbours will\nbe utilized. We summarized the specifications of all the datasets with Table 1. The experiment results\nare evaluated in terms of link prediction accuracy, i.e., to predict whether a query node and key node\nare connected given the textual features of themselves and their neighbours. In each testing instance,\none query is provided with 300 keys: 1 positive plus 299 randomly sampled negative cases. We\nleverage three common metrics to measure the prediction accuracy:Precision@1, NDCG, and MRR.\nWithout specifications, we will take the unidirectional-simplified GraphFormers trained with the\ntwo-stage progressive learning as our default model. More details about the implementations and\nthe training/testing configurations are summarized in an Appendix file. It is submitted together with\nour source code within the supplementary materials.\n4.2 Baselines\nWe focus on the comparison between GNN-nested Transformers and Cascaded Transformers-GNN.\nTo make sure the difference between both architectures can be truthfully reflected from the evaluation\nresults, GraphFormers and the Cascaded Transformers-GNN baselines are equipped with text encoders\nand graph aggregators of the same capacities. Particularly, we use the BERT-likePLM as our text\nencoder, where UniLM-base8 (Bao et al., 2020) is chosen as the network backbone for all related\nmethods; the final layer‚Äôs [CLS] token embedding is used for the text embedding.\nWe enumerate the following representative graph aggregators as used in GAT (VeliÀáckovi¬¥c et al., 2018),\nGIN (Xu et al., 2018), GraphSage (Hamilton et al., 2017a). The GAT aggregator, where the node\nembedding is generated as the weighted sum of all the text embeddings. Each text embedding‚Äôs\nrelative importance is calculated as the attention score with the center node. ThePooling-and-Concat\naggregators, where the center node‚Äôs text embedding is concatenated with the neighbours‚Äô pooling\nresult and linearly transformed for the final representation. Depending on the form of pooling function,\nwe have the following options:Max and Mean, where neighbours are aggregated by max-pooling and\nmean-pooling, respectively; Att, where the neighbours are summed up based on the attention weights\nwith the center node. By comparison, the neighbourhood information may get more emphasized with\nGAT; while the center node itself tends to be highlighted with Pooling-and-Concat.\nWe consider two more baselines which make use of simplified text encoders (such as CNN) and\nnetwork embeddings: TNVE (Wang et al., 2019a) and IFTN (Xu et al., 2019). We also include the\nPLM only baseline, which merely leverages the textual feature of the center node.\n8An enhanced BERT-like PLM showing more competitive performances than peers like RoBERTa, XLNet.\n7\nTable 3: Impact of neighbour size (#N).\nGraphFormers PLM+Max\n#N P@1 NDCG MRR P@1 NDCG MRR\n1 0.6485 0.8087 0.7522 0.6249 0.7946 0.7342\n2 0.6841 0.8308 0.7804 0.6538 0.8137 0.7583\n3 0.6980 0.8396 0.7916 0.6728 0.8256 0.7734\n4 0.7126 0.8485 0.8029 0.6823 0.8319 0.7814\n5 0.7267 0.8565 0.8133 0.6934 0.8386 0.7900\n4.3 Overall Evaluation\nThe overall evaluation results are reported in Table 2. It‚Äôs observed that GraphFormers consistently\noutperform all the baselines, especially the ones based on the cascaded Transformers-GNN, with\nnotable advantages. Particularly, it achieves 2.9%, 4.8%, 6.5% relative improvements over the\nmost competitive baselines (underlined) on each of the experimental datasets. Such an observation\nindicates that the relationship between the nodes can be captured more accurately based on the node\nembeddings generated by GraphFormers, which verifies the effectiveness of our proposed method.\nWe also observe the following underlying factors that may influence the representation quality.\nFirstly, the effective utilization of neighbourhood information is critical. With the joint consideration\nof the center node and neighbour nodes, the PLM+GNNs methods, including GraphFormers and the\ncascaded Transformers-GNN baselines, significantly outperform the PLM only baseline in most of\nthe time. We further analyze the impact of neighbourhood size as Table 3, with a fraction of neighbour\nnodes randomly sampled for each center node (using DBLP for illustration). It can be observed\nthat both GraphFormers and PLM+Max (the most competitive baseline) achieve higher prediction\naccuracy than the PLM only method (P@1:0.5673, NDCG:0.7484, MRR:0.6777, as reported in\nTable 2), even with fewer neighbour nodes included. With the increasing number of neighbour\nnodes, the advantages become gradually enlarged. However, the marginal gain is vanishing, as the\nrelative improvement becomes smaller when more neighbours are included. In all the testing cases,\nGraphFormers maintain consistent advantages over PLM+Max, which reaffirms the effectiveness of\nour proposed methods.\nSecondly, the capacity of the text encoder is crucial for textual graph representation. All the pretrained\nlanguage model based methods (GraphFormers, Cascaded Transformers-GNN baselines, PLM-only\nbaseline) significantly outperform the baselines with simplified text encoders (TNVE, IFTN).\nThirdly, the representation quality is also sensitive to the form of graph aggregator. In Product,\nthe cascaded Transformers-GNN baselines‚Äô performances are quite close to each other. In DBLP,\nPLM+(Max, Mean, Att) outperforms PLM+GAT. In Wiki, not only PLM+(Max, Mean, Att) but also\nPLM-only baseline outperform PLM+GAT. Such phenomenons could be attributed to the type of\ngraph: whether it is homogeneous or heterogeneous. Particularly, both Product and DBLP can be\nregarded as homogeneous graphs as the nodes are connected based on the same relationships; i.e.,\nco-view relationship in Product, and citation relationship in DBLP. In both homogeneous graphs, the\nconnected nodes may have quite similar semantics (the co-viewed products usually serve similar user\nintents, and the citation relationships usually indicate similar research topics); thus, the incorporated\nneighbour nodes will probably provide complementary information for the link prediction between\nthe center nodes. However, Wiki is a heterogeneous graph, where the connections between entities\nmay have highly different semantics. As a result, the incorporation of neighbour nodes may not\ncontribute to the link prediction task, especially when the incorporated neighbours and the prediction\ntarget are connected to the center nodes with totally different relationships. Considering that GAT\ntends to focus more on the neighbourhood, its performance can be vulnerable in such unfavorable\nsituations. These findings suggest that the neighbourhood information should be properly handled in\ncase that the information of the center node is wiped out.\nFinally, we may conclude different methods‚Äô utility in textual graph representation: simplified text\nencoders ‚â∫ PLMs ‚â∫ Cascaded Transformers-GNN ‚â∫ GNNs-nested Transformers. Such findings\nare consistent with our expectation that the precise modeling of individual textual feature and the\neffective integration of neighbourhood information will jointly contribute to high-quality textual\ngraph representation. GraphFormers enjoy the high expressiveness of PLMs and leverage layerwise\nnested-GNNs to facilitate graph aggregation, which contributes to both of the above perspectives.\n8\nTable 4: Ablation Studies (The top ablated methods are marked in bold; ‚Äú‚Üë‚Äù/‚Äú‚Üì‚Äù: the performance\nis increased/decreased compared with the default setting). ‚Äú-Progressive‚Äù: two-stage progressive\nlearning disabled; ‚Äú-Simplified‚Äù: unidirectional simplification disabled; ‚Äú-Shared GNNs‚Äù: GNNs\nparameters are not shared across the layers; ‚Äú-Position‚Äù: GNNs learnable position bias disabled.\nProduct DBLP Wiki\nMethods P@1 NDCG MRR P@1 NDCG MRR P@1 NDCG MRR\nGraphFormers 0.7786 0.8793 0.8430 0.7267 0.8565 0.8133 0.3952 0.6230 0.5220\nPLM+Max 0.7570 0.8678 0.8280 0.6934 0.8386 0.7900 0.3712 0.6071 0.5022\n- Progressive 0.7688 0.8751 0.8373 0.7096 0.8468 0.8007 0.3834 0.6155 0.5127\n- Simplified 0.7795 ‚Üë 0.8798 ‚Üë 0.8436 ‚Üë 0.7225 0.8542 0.8102 0.3923 0.6209 0.5195\n- Shared GNNs 0.7788 0.8795 0.8433 0.7256 0.8558 0.8123 0.3945 ‚Üì 0.6221 ‚Üì 0.5211 ‚Üì\n- Position 0.7788 0.8795 0.8434 0.7276 ‚Üë 0.8570 ‚Üë 0.8139 ‚Üë 0.3942 0.6222 0.5211\nTable 5: Time and memory costs per mini-batch for PLM+Max and GraphFormers, with neighbour\nsize increased from 3 to 200. GraphFormers achieve similar efficiency and scalability as PLM+Max.\n#N 3 5 10 20 50 100 200\nTime: PLM+Max 60.29 ms 93.41 ms 161.40 ms 295.92 ms 684.16 ms 1357.93 ms 2706.35 ms\nTime: GraphFormers 63.95 ms 97.19 ms 170.16 ms 306.12 ms 714.32 ms 1411.09 ms 2801.67 ms\nMem: PLM+Max 1.33 GiB 1.39 GiB 1.55 GiB 1.82 GiB 2.67 GiB 4.09 GiB 6.92 GiB\nMem: GraphFormers 1.33 GiB 1.39 GiB 1.55 GiB 1.83 GiB 2.70 GiB 4.28 GiB 7.33 GiB\n4.4 Ablation Studies\nThe ablation studies (as Table 4) are performed to clarify the following issues: 1) the impact of\ntwo-stage progressive learning, and 2) the impact of unidirectional-simplified GraphFormers.\nFirstly, the two-stage progressive learning substantially improves GraphFormers‚Äô representation\nquality. Without such a training strategy (\"-Progressive\": training directly on the original data), the\nmodel‚Äôs performance is decreased by 0.98%, 1.71%, and 1.18% in each of the datasets, respectively.\nSecondly, the performances between simplified and non-simplified (‚Äú-Simplified‚Äù) GraphFormers\nare comparable. In fact, the necessity of graph aggregation is not equivalent for the center node and\nthe neighbour nodes: since the center node is the one for representation, it is much more important\nto ensure that the center node may extract complementary information from its neighbours. The\nunidirectional-simplified GraphFormers maintain such a property; thus, there is little impact on the\nfinal performances. Such a finding affirms that we may safely leverage the simplified model to save\nthe cost of repetitively encoding the existing neighbours.\nWe make two additional ablation studies. ‚Äú-Shared GNNs‚Äù: the GNNs parameters sharing is disabled,\nwhere each layer maintains its own graph aggregator (by default, the layerwise GNN components in\nGraphFormers share the same set of parameters). ‚Äú-Position‚Äù: the learnable position bias ( b in Eq. 1)\nis disabled in GNNs. We find that model‚Äôs performance is little affected from the above changes.\n4.5 Efficiency Analysis\nWe compare the time efficiency between GNN-nested Transformers (GraphFormers) and Cascaded\nTransformers+GNN (using PLM+Max for comparison). The evaluation is made with a Nvidia P100\nGPU. Each mini-batch contains 32 encoding instances; each instance contains one center and #N\nneighbour nodes; the token length of each node is 16. We report the average time and memory (GPU\nRAM) costs per mini-batch as Table 5.\nFirstly, the time and memory costs of both methods grow linearly with the increment of neighbours.\n(There are overheads of time and memory costs. The time cost overhead may come from CPU\nprocessing; while the memory cost overhead is mainly due to the model parameters (Rajbhandari\net al., 2020)). We may approximately remove the overheads by deducting the time and memory costs\nwhere #N=3). Such a finding is consistent with our theoretical analysis in Section 3.1.\nSecondly, the overall time and memory costs of GraphFormers are quite close to PLM+Max. When the\nnumber of neighbour nodes is small, the differences between both methods are almost ignorable. The\ndifferences become slightly larger when more neighbour nodes are included, because the layerwise\n9\nFigure 3: Online A/B Test: the relative improvements of RPM, CY and CPC against the last version\nof production system in Bing Search (green: positive; blue: negative). In most of the time, all three\nperformance indicators are significantly improved thanks to the utilization of GraphFormers.\ngraph aggregations in GraphFormers get increasingly time consuming. However, the differences are\nstill relatively small: merely around 3.5% of the overall running costs when #N is increased to 200\n(‚Äú#N=200‚Äù is already more than enough for most of the real world scenarios).\nBased on the above observations, we may conclude that GraphFormers are more accurate, meanwhile\nequally efficient and scalable as the conventional cascaded Transformer+GNNs.\n4.6 Online A/B Test on Bing Search\nGraphFormers has been deployed as one of the major ads retrieval algorithms on Bing Search, and it\nachieves highly competitive performance against the previous production system (the combination of a\nwide spectrum of semantic representation algorithms, including large-scale PLMs and cascaded PLMs-\nGNNs). Particularly, the primary objective of Ads service is to maximize the revenue meanwhile\nincreasing the user clicks. Therefore, the following three metrics are taken as the major performance\nindicators: RPM9 (revenue per thousand impressions), CY (click yield), and CPC10 (cost per click) .\nDuring our large-scale online A/B test, GraphFormers significantly improves the overall RPM, CY ,\nCPC by 1.87%, 0.96% and 0.91%, respectively. A 11-day performance snapshot is demonstrated\nas Figure 3; it can be observed that in most of the time, all three metrics are significantly improved\nthanks to the utilization of GraphFormers (the daily performance are measured based on millions of\nimpressions, thus having strong statistic significance).\n5 Conclusion\nIn this paper, we propose a novel model architecture GraphFormers for textual graph representation.\nBy having GNNs nested alongside each transformer layer of the pretrained language model, the\nunderlying semantic of each textual node can be precisely captured and effectively integrated for\nhigh-quality textual graph representation. On top of the fundamental architecture, we introduce the\ntwo-stage progressive training strategy to further strengthen GraphFormers‚Äô representation quality; we\nalso simplify the model with the unidirectional graph aggregation, which eliminates the unnecessary\ncomputation cost. The experimental studies on three large-scale textual graph datasets verify the\neffectiveness of our proposed methods, where GraphFormers notably outperform the existing cascaded\nTransformer-GNNs methods with comparable running efficiency and scalability.\n6 Acknowledgement\nWe are grateful to anonymous reviewers for their constructive comments on this work. The work was\nsupported by grants from the National Natural Science Foundation of China (No. 62022077).\nReferences\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao,\nSonghao Piao, Ming Zhou, et al. 2020. Unilmv2: Pseudo-masked language models for unified\n9https://support.google.com/adsense/answer/190515?hl=en\n10https://support.google.com/google-ads/answer/116495?hl=en\n10\nlanguage model pre-training. In International Conference on Machine Learning, pages 642‚Äì652.\nPMLR.\nYoshua Bengio, J√©r√¥me Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning.\nIn Proceedings of the 26th annual international conference on machine learning, pages 41‚Äì48.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models\nare few-shot learners. arXiv preprint arXiv:2005.14165.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive learning of sentence\nembeddings. arXiv preprint arXiv:2104.08821.\nWill Hamilton, Zhitao Ying, and Jure Leskovec. 2017a. Inductive representation learning on large\ngraphs. In Advances in neural information processing systems, pages 1024‚Äì1034.\nWilliam L Hamilton, Rex Ying, and Jure Leskovec. 2017b. Representation learning on graphs:\nMethods and applications. arXiv preprint arXiv:1709.05584.\nZiniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. 2020. Gpt-gnn: Generative\npre-training of graph neural networks. In Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, pages 1857‚Äì1867.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanbert: Improving pre-training by representing and predicting spans. Transactions of the\nAssociation for Computational Linguistics, 8:64‚Äì77.\nVladimir Karpukhin, Barlas OÀòguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-\ntau Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint\narXiv:2004.04906.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980.\nThomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907.\nChaozhuo Li, Bochen Pang, Yuming Liu, Hao Sun, Zheng Liu, Xing Xie, Tianqi Yang, Yanling Cui,\nLiangjie Zhang, and Qi Zhang. 2021. Adsgnn: Behavior-graph augmented relevance modeling in\nsponsored search. arXiv preprint arXiv:2104.12080.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019a. Roberta: A robustly optimized bert\npretraining approach. arXiv preprint arXiv:1907.11692.\nZhenghao Liu, Chenyan Xiong, Maosong Sun, and Zhiyuan Liu. 2019b. Fine-grained fact verification\nwith kernel graph attention network. arXiv preprint arXiv:1910.09796.\nYi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2020. Sparse, dense, and\nattentional representations for text retrieval. arXiv preprint arXiv:2005.00181.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed\nrepresentations of words and phrases and their compositionality. arXiv preprint arXiv:1310.4546.\nJeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for\nword representation. In 2014 EMNLP, pages 1532‚Äì1543.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. 2018. Deep contextualized word representations. arXiv preprint\narXiv:1802.05365.\n11\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. arXiv preprint arXiv:1910.10683.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory opti-\nmizations toward training trillion parameter models. In SC20: International Conference for High\nPerformance Computing, Networking, Storage and Analysis, pages 1‚Äì16. IEEE.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-\nnetworks. arXiv preprint arXiv:1908.10084.\nJianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou. 2021. Whitening sentence representations for\nbetter semantics and faster retrieval. arXiv preprint arXiv:2103.15316.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all you need. arXiv preprint arXiv:1706.03762.\nPetar VeliÀáckovi¬¥c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\nBengio. 2018. Graph attention networks. International Conference on Learning Representations\n(ICLR).\nChenguang Wang, Yangqiu Song, Haoran Li, Ming Zhang, and Jiawei Han. 2016a. Text classification\nwith heterogeneous information network kernels. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 30.\nJizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, and Dik Lun Lee. 2018. Billion-\nscale commodity embedding for e-commerce recommendation in alibaba. In Proceedings of the\n24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages\n839‚Äì848.\nSuhang Wang, Jiliang Tang, Charu Aggarwal, and Huan Liu. 2016b. Linked document embedding\nfor classification. In Proceedings of the 25th ACM international on conference on information and\nknowledge management, pages 115‚Äì124.\nWenlin Wang, Chenyang Tao, Zhe Gan, Guoyin Wang, Liqun Chen, Xinyuan Zhang, Ruiyi Zhang,\nQian Yang, Ricardo Henao, and Lawrence Carin. 2019a. Improving textual network learning with\nvariational homophilic embeddings. arXiv preprint arXiv:1909.13456.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2019b. Kepler:\nA unified model for knowledge embedding and pre-trained language representation. arXiv preprint\narXiv:1911.06136.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google‚Äôs neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144.\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful are graph neural\nnetworks? arXiv preprint arXiv:1810.00826.\nZenan Xu, Qinliang Su, Xiaojun Quan, and Weijia Zhang. 2019. A deep neural information fusion\narchitecture for textual network embeddings. arXiv preprint arXiv:1908.11057.\nCheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. 2015. Network\nrepresentation learning with rich text information. In IJCAI, volume 2015, pages 2111‚Äì2117.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint\narXiv:1906.08237.\n12\nMichihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek, Krishnan Srinivasan, and\nDragomir Radev. 2017. Graph-based neural multi-document summarization. arXiv preprint\narXiv:1706.06681.\nRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec.\n2018. Graph convolutional neural networks for web-scale recommender systems. In Proceedings\nof the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining ,\npages 974‚Äì983.\nJie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,\nChangcheng Li, and Maosong Sun. 2020. Graph neural networks: A review of methods and\napplications. AI Open, 1:57‚Äì81.\nJie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun.\n2019. Gear: Graph-based evidence aggregating and reasoning for fact verification. arXiv preprint\narXiv:1908.01843.\nJason Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li, Markus Pelger, Liangjie Zhang, Tianqi Yan,\nRuofei Zhang, and Huasha Zhao. 2021. Textgnn: Improving text encoder via graph neural network\nin sponsored search. arXiv preprint arXiv:2101.06323.\n13\nA Implementation Details\nA.1 Masking Strategy\nWe use span masking (Joshi et al., 2020) as our masking strategy. For each iteration, we sample and\nmask a span of text, until the ratio of masked tokens has reached the threshold. We follow the settings\nin (Joshi et al., 2020). The span length l is generated from a geometric distribution l ‚àº Geo(p),\nwhere p is set to 0.2 and l is clipped at lmax = 10. As in BERT (Devlin et al., 2018), 15% of the\ninput tokens will be masked: 80% of them are replaced by [MASK], 10% are replaced by random\ntokens and 10% are kept as the original tokens.\nA.2 GraphFormers‚Äô Workflow\nAlgorithm 2 provides the pseudo-code of GraphFormers‚Äô workflow. We use original Multi-Head\nAttention in the first Transformer layer (Transformers[0]), and asymmetric Multi-Head Attention in\nthe rest Transformer layers (Transformers[1..L ‚àí 1]). In original Multi-Head Attention, Q, K, V\nare computed as:\nQ = Hl\ngWQ\nj ; K = Hl\ngWK\nj ; V = Hl\ngWV\nj . (8)\nIn asymmetric Multi-Head Attention, Q, K, V are computed as:\nQ = Hl\ngWQ\nj ; K = bHl\ngWK\nj ; V = bHl\ngWV\nj . (9)\nIn the above equations, Hl\ng are token-level embeddings, bHl\ng are graph-augmented token-level embed-\ndings, and WQ\nj , WK\nj , and WV\nj are the projection matrices of Multi-Head Attention, corresponding\nto the j-th attention head.\nIn each step, we extract the embeddings of [CLS] tokens as node-level embeddings Zl\ng. The\nnode-level embeddings Zl\ng and a learnable bias vector b are processed by the GNN component,\nwhich is a Multi-Head Attention layer. The output GNN-processed node-level embeddings ÀÜZl\ng are\nconcatenated with the original token-level embeddings Hl\ng, which generates the graph-augmented\ntoken-level embeddings bHl\ng. Then bHl\ng are processed by the Transformer component using asymmetric\nMulti-Head Attention. At last, the node-level embedding of the center node hx is returned as the\nrepresentation of the graph.\nB Training Details\nAs shown in Table 6, we present the hyperparameters used for training GraphFormers. The model is\ntrained for at most 100 epochs on all datasets. For the stability of the training process, we optimally\ntune the learning rate as 1e‚àí5 for Product, 1e‚àí6 for DBLP, and 5e‚àí6 for Wiki. We use an early\nstopping strategy on P@1 with a patience of 2 epochs and Adam (Kingma and Ba, 2014) withŒ≤1=0.9,\nŒ≤2=0.999, œµ=1e-8 for optimization. We pad the sequence length to 32 for Product and DBLP, 64\nfor Wiki, depending on different text length of each dataset. To make full use of the GPU memory,\nwe set the batch size as 240 for Product and DBLP, 160 for Wiki. Each training sample includes 12\nnodes: 1 query with its 5 neighbours, and 1 keyword with its 5 neighbours. The training is on 8√ó\nNvidia V100-16GB GPU clusters. The training of GraphFormers takes 58.8, 117.6, 151.2 hours on\naverage to converge on each of the experimental datasets (Product, DBLP, Wiki). We use Python3.6\nand PyTorch 1.6.0 for implementation. The random seeds of PyTorch and NumPy are fixed as 42.\nFor two-stage training, the training processes of the two stages share the same settings as above.\n14\nAlgorithm 2: GraphFormers‚Äô Workflow in PyTorch-Like Style\n# Input:\n# Hg[0]: initial token-level embeddings (summation of word embeddings and position\nembeddings)\n# Output:\n# hx: output embeddings\n# B: batch size\n# N: number of nodes in the graph (0th node represents the center node)\n# SL: sequence length\n# D: hidden dimension\n# L: number of GNN-nested Transformer layers\n# b: learnable bias vector for nodes\n# token-level embeddings: BxNxSLxD\nHg[1] = Transformers[0](Hg[0].view(B * N, SL, D), asymmetric = False).view(B, N, SL,\nD) # \"asymmetric = False\" means we use original Multi-Head Attention in the\nTransformer\nfor l in range(1, L):\n# node-level embeddings: BxNxD\nZg[l] = Hg[l][:, :, 0]\n# GNN-processed node-level embeddings: BxNxD\nZg_hat[l] = MultiHeadAttention(Zg[l], b)\n# graph-augmented token-level embeddings: BxNx(SL+1)xD\nHg_hat[l] = Concat([Zg_hat[l][:, :, None, :], Hg[l]], dim = 2)\n# token-level embeddings: BxNxSLxD\nHg[l + 1] = Transformers[l](Hg_hat[l].view(B * N, SL + 1, D), asymmetric = True).\nview(B, N, SL, D) # \"asymmetric = True\" means we use asymmetric Multi-Head\nAttention in the Transformer\n# graph representations: BxD\nhx = Hg[L][:, 0, 0, :]\nreturn hx\nTable 6: Hyperparameters for training GraphFormers\nOptimizer Adam\nAdam Œ≤1 0.9\nAdam Œ≤2 0.999\nAdam œµ 1e-8\nPyTorch random seed 42\nNumPy random seed 42\nProduct DBLP Wiki\nMax training epochs 100 100 100\nLearning rate 1e-5 1e-6 5e-6\nSequence length 32 32 64\nBatch size 240 240 160\n15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7971845865249634
    },
    {
      "name": "Transformer",
      "score": 0.7216416597366333
    },
    {
      "name": "Graph",
      "score": 0.5919342637062073
    },
    {
      "name": "Workflow",
      "score": 0.5573714971542358
    },
    {
      "name": "Architecture",
      "score": 0.5456883907318115
    },
    {
      "name": "Feature learning",
      "score": 0.4841764271259308
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46244651079177856
    },
    {
      "name": "Language model",
      "score": 0.451199471950531
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3905823230743408
    },
    {
      "name": "Natural language processing",
      "score": 0.3387225866317749
    },
    {
      "name": "Database",
      "score": 0.12011218070983887
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    }
  ]
}