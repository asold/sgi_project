{
  "title": "EIGEN: Event Influence GENeration using Pre-trained Language Models",
  "url": "https://openalex.org/W3094500738",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5085667632",
      "name": "Aman Madaan",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5108690891",
      "name": "Dheeraj Rajagopal",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5106542734",
      "name": "Yiming Yang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5021017923",
      "name": "Abhilasha Ravichander",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5060225743",
      "name": "Eduard Hovy",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5000188096",
      "name": "Shrimai Prabhumoye",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2952437275",
    "https://openalex.org/W2970278082",
    "https://openalex.org/W2963983586",
    "https://openalex.org/W2116492146",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W3092049183",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963436881",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W3010293452",
    "https://openalex.org/W2729046720",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3014521650",
    "https://openalex.org/W2250506749",
    "https://openalex.org/W2973230076",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2561658355",
    "https://openalex.org/W2963368301",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2798865369",
    "https://openalex.org/W3017796738",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2151295812",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2962932447",
    "https://openalex.org/W2997545008",
    "https://openalex.org/W2978170550",
    "https://openalex.org/W2913687407"
  ],
  "abstract": "Reasoning about events and tracking their influences is fundamental to understanding processes. In this paper, we present EIGEN - a method to leverage pre-trained language models to generate event influences conditioned on a context, nature of their influence, and the distance in a reasoning chain. We also derive a new dataset for research and evaluation of methods for event influence generation. EIGEN outperforms strong baselines both in terms of automated evaluation metrics (by 10 ROUGE points) and human judgments on closeness to reference and relevance of generations. Furthermore, we show that the event influences generated by EIGEN improve the performance on a \"what-if\" Question Answering (WIQA) benchmark (over 3% F1), especially for questions that require background knowledge and multi-hop reasoning.",
  "full_text": "EIGEN: Event Inﬂuence GENeration\nusing Pre-trained Language Models\nAman Madaan ∗ , Dheeraj Rajagopal ∗ , Yiming Yang,\nAbhilasha Ravichander, Eduard Hovy, Shrimai Prabhumoye\nLanguage Technologies Institute, Carnegie Mellon University\nPittsburgh, PA, USA\n{amadaan,dheeraj,yiming,aravicha,ehovy,sprabhum}@cs.cmu.edu\nAbstract\nReasoning about events and tracking their in-\nﬂuences is fundamental to understanding pro-\ncesses. In this paper, we present EIGEN - a\nmethod to leverage pre-trained language mod-\nels to generate event inﬂuences conditioned\non a context, nature of their inﬂuence, and\nthe distance in a reasoning chain. We also\nderive a new dataset for research and evalu-\nation of methods for event inﬂuence genera-\ntion. EIGEN outperforms strong baselines both\nin terms of automated evaluation metrics (by\n10 ROUGE points) and human judgments on\ncloseness to reference and relevance of genera-\ntions. Furthermore, we show that the event in-\nﬂuences generated by EIGEN improve the per-\nformance on a “what-if” Question Answering\n(WIQA ) benchmark (over 3% F1), especially\nfor questions that require background knowl-\nedge and multi-hop reasoning.\n1 Introduction\nHumans are adept at anticipating and reasoning\nabout events and their causal effects (inﬂuences)\non other events. Consider these questions - Would\nit rain more if we plant more trees?, What would\nhelp the water in boiling faster? - answering these\nquestions requires the ability to comprehend the\ncomplex processes of plant growth and water boil-\ning and the capacity to reason about how various\nevents inﬂuence each other in these processes that\nare typically implicit in text. Hence, reasoning\nabout events and inﬂuences remains a signiﬁcant\nchallenge for machines. Understanding such events\nand tracing their inﬂuence chains is essential for\nend tasks like question answering ( QA) (Tandon\net al., 2019), process tracking (Dalvi et al., 2018),\nreasoning about qualitative relationships (Tafjord\net al., 2019), and physical commonsense reason-\ning (Sap et al., 2019; Bisk et al., 2020).\n∗ authors contributed equally to this work.\nPrevious approaches have studied event under-\nstanding in the context of event extraction (Cham-\nbers and Jurafsky, 2008; Yang et al., 2019; Wang\net al., 2019), temporal event reasoning (Ning\net al., 2018; Vashishtha et al., 2019), and Question-\nAnswering (Tandon et al., 2019; Dalvi et al., 2018).\nHowever, these systems are primarily extractive\n— they reason about events already mentioned in\nthe text, limiting their ability to be integrated to\ndownstream tasks that require implicit reasoning\nabout events. The task of generating novel event\ninﬂuence in unseen contexts is still an open chal-\nlenge.\nMeanwhile, promising evidence from recent\nwork attests to the ability of pretrained language\nmodels (PLM ) to encode a wide-range of knowl-\nedge from their pretraining corpus (Bosselut et al.,\n2019; Petroni et al., 2019; Davison et al., 2019),\nenabling their successful adaptation in downstream\ntasks (Yang et al., 2020; Kumar et al., 2020; Guan\net al., 2020). Motivated by these successes, we\ninvestigate whether we can adaptPLM for the novel\ntask of event inﬂuence generation and determine\nempirically whether the generated event inﬂuences\nlead to downstream performance gains. Such an\nexploration entails two major challenges: i) lack\nof large-scale stand-alone datasets to study event\ninﬂuences, and ii) a framework to leverage PLM to\nadapt them for event inﬂuence generation.\nIn this work, we address these challenges by ﬁrst\nderiving a large corpus based on WIQA (Tandon\net al., 2019) dataset that can be used for the gen-\neration of event inﬂuences conditioned on context,\nrelationship between the events, and the distance\nbetween them in a reasoning chain. Next, we pro-\npose our framework, EIGEN , that takes a context\nand an event, and generates its inﬂuences both in\nforward and backward directions. An example use\nof our framework is shown in Figure 1. In the ﬁg-\nure, nodes represent the event inﬂuences and the\narXiv:2010.11764v1  [cs.CL]  22 Oct 2020\nFigure 1: An overview of our methodology. The procedural text describes the process of photosynthesis. For this\nexample, we generate the inﬂuence graph for the event more sunlight. The inﬂuence graph is generated for the\nrelation types - helps, hurt by, helped by and hops = {1,2}. A sample output inﬂuence graph shows the generated\nevents - bright skies, cloudy skies, plants trap sunlight, and plants grow taller\nedges represent the nature of the inﬂuence (rela-\ntion) between them. These relations can either be\npositive (when one event helps the occurrence of\nanother) or negative (when one event hurts the oc-\ncurrence of another). The distance between any\ngiven pair of nodes (in terms of number of edges\ntraversed) is denoted by hop.\nEIGEN ﬁne-tunes a PLM to generate novel event\ninﬂuences for unseen contexts using masked lan-\nguage modeling. We show empirically that our\nframework generates high quality inﬂuences for an\nevent, both in terms of automated metrics (by ∼\n10 ROUGE) and human metrics — relevance and\nproximity to the reference text. Together, the over-\nall framework can be seamlessly integrated into any\ndownstream task. In one such instance, we show\nhow the event inﬂuences generated from EIGEN\ncan be easily augmented to a downstream QA task\nand improve its performance without any need for\nmodifying the underlying model architecture. In\nsummary, our contributions are:\n1. We propose the task of event inﬂuence gener-\nation and derive a large-scale dataset for the\nsame.\n2. We propose EIGEN , a framework to generate\ntargeted inﬂuence nodes for an event. Our\nexperiments show that EIGEN outperforms\nstrong baselines in both automated and hu-\nman evaluation.\n3. We also validate our approach by augment-\ning generated inﬂuences to a downstream QA\ndataset, improving over the state of the art\nby 3% in overall accuracy, and by 8% on the\nsubset of questions that require implicit event-\ninﬂuence reasoning 1.\n2 Related Work\nEvent Inﬂuences: There has been immense in-\nterest in understanding event chains in stories and\nnews corpora in both unsupervised (Chambers and\nJurafsky, 2008) and supervised (Rudinger et al.,\n2015; Liu et al., 2018) settings. Such approaches\naim to extract the event chains that are explicitly\nmentioned in the input text and are unyielding to-\nwards implicit event reasoning. Events and their\ninﬂuences have also been studied in restricted do-\nmains such as cooking recipes (Kiddon et al., 2016;\nBosselut et al., 2018), and in general procedural\ntext (Dalvi et al., 2018) as a classiﬁcation task over\na restricted set of events. Tandon et al. (2019)\nintroduce the WIQA dataset, which relaxes this re-\nstriction by collecting event perturbations over gen-\neral procedures, where the goal is to predict the\ninﬂuence between two given events (positive, neg-\native or no-effect), while also providing explicit\nannotations for capturing the inﬂuences over mul-\ntiple reasoning hops. Albeit being resourceful, re-\nstricted task formulation limits use of these datasets\nto adapt for event inﬂuence generation task. To\n1Code and data available at https://github.com/\ndheerajrajagopal/EIGEN.\novercome this challenge, we derive a large-scale\nevent-inﬂuence dataset from WIQA (discussed in\nSection §3).\nLanguage Models for Knowledge Generation:\nThe use of large scale neural networks to gen-\nerate knowledge has been studied under various\ntask settings. Sap et al. (2019) use LSTM-based\nencoder-decoder architectures to generate general-\npurpose social commonsense knowledge. These\nmethods were then improved by replacing LSTMs\nwith large-scale pre-trained transformer language\nmodels. Bosselut et al. (2019) proposed COMET ,\nwhich ﬁne-tunes GPT (Radford et al., 2018) on\nATOMIC (Sap et al., 2019) and conceptnet (Speer\net al., 2017) for knowledge-completion task. An ex-\ntension to this work by incorporating structural and\nsemantic constraints was proposed by Malaviya\net al. (2019). Similar to Bosselut et al. (2019),\nwe leverage pre-trained language models for the\nconditional generation of events. However, un-\nlike COMET , we i) condition our generations on\na larger context and hop-information, and ii) pro-\nvide a framework for recursively generating event\ninﬂuence graphs for a given process and an event.\nAdditionally, unlike COMET , a dataset that can be\nused for our task is not readily available, and hence\nwe outline a method for adapting existing datasets\nfor our task as an additional contribution.\n3 Event Inﬂuence Generation\nEIGEN is a framework for generating ﬁne-grained\nevent inﬂuences for a given context, conditioned\non the relation and the hop information. EIGEN\nleverages a pretrained language model to learn to\ngenerate novel event inﬂuences over multiple hops.\nIn this section, we present (i) our task formula-\ntion (section §3.1), (ii) the dataset collection pro-\ncess (section §3.2) and (iii) the learning procedure\n(§3.3).\n3.1 Task\nWe formalize the event inﬂuence generation task as\nfollows: Given an input tuple (P,ns,r,h), where\n(i) P is a procedural passage P that describes the\nsteps in a process,\n(ii) ns is an event in P for which the inﬂuences are\nto be generated,\n(iii) ris an inﬂuence relation that describes the na-\nture of the inﬂuence and,\n(iv) h is the hop length (distance) between the\nevent ns and its inﬂuence in a reasoning chain,\nour task is to generate a target event nt such that\nns\nr−→nt at hop hin the context of P. We focus\non 4 broad classes of event inﬂuence relations r\nbetween events ns and nt:\n(1) helps: ns positively inﬂuences nt (ns\n+−→nt)\n(2) hurts: ns negatively inﬂuences nt (ns\n−−→nt)\n(3) helped-by: ns is positively inﬂuenced by nt\n(ns\n+←−nt), and\n(4) hurt-by: ns is negatively inﬂuenced by nt\n(ns\n−←−nt).\nWe show an example of our task in Figure 1,\nwhere we generate the inﬂuences for the event\nmore sunlight in the context of photosynthesis.\nIn this example, for the relation hurt-by, and a\nhop-length h = 1, we aim to generate the text\ncloudy skies (nt). Similarly, given h = 2 and a\nrelation helps, the system generates the target event\nplants grow taller. Note that the generation of a\nnode refers to the generation of text tokens describ-\ning the node.\n3.2 Dataset\nLack of datasets remains a challenge for study-\ning the task of event inﬂuence generation. To ad-\ndress this challenge, we adapt WIQA (Tandon et al.,\n2019) to generate a large-scale event inﬂuence gen-\neration dataset. WIQA consists of a set of proce-\ndural passages, each accompanied by a human-\ncurated inﬂuence graph. The inﬂuence graph cap-\ntures the interactions between the events and their\ninﬂuences and external perturbations in the con-\ntext of the process described by the passage. Al-\nthough these graphs can be subjective, WIQA has\nhigh inter-annotator agreement 2, motivating our\nchoice to leverage these graphs.\nWe decompose the inﬂuence graphs to create\nour generation dataset. An inﬂuence graph for a\npassage P is denoted by by G= (V,E), where V\ndenotes the set of vertices and Ethe set of edges.\nThe nodes n ∈V represent the events, and the\nedges represent the relationship (helps or hurts) be-\ntween them. Each edge ns\nr−→nt ∈Gcontributes\na sample for our training data, composed of tuples\nof the form xi = (P,ns,r,h) and yi = nt.\nFor creating multi-hop training samples for our\ntask, we exploit the transitive compositionality of\nthe inﬂuence relations. For example, if (na\n+−→\nnb) ∧(nb\n+−→nc) ≡ (na\n+−→nc). Similarly,\n(na\n+−→nb) ∧(nb\n−−→nc) ≡(na\n−−→nc). In\n20.6 Krippendorff’s alpha\nsummary, ns\n−−→nt if the path from ns to nt\nhas an odd number of hurts edges, and ns\n+−→\nnt otherwise. For example, cloudy skies −−→\nplants grow taller in Figure 1.\nWe also augment the dataset with inverse inﬂu-\nences, where our goal is to capture event inﬂuence\nin the reverse direction. For example, if na\n+−→nb,\nthen nb\n+←−na. After augmentation, our dataset\ncaptures diverse inﬂuences with respect to the re-\nlations and hops as described in section §3.1. A\ndetailed dataset statistic is shown in Table 1.\nSplit Relation Type 1-Hop 2-Hop 3-Hop Total\ntrain helps 8723 13085 5815\n119.2ktrain hurts 13081 13088 5815\ntrain is helped by 8723 13085 5815\ntrain is hurt by 13081 13088 5815\ntest helps 1382 2075 922\n18.8ktest hurts 2073 2075 922\ntest is helped by 1382 2075 922\ntest is hurt by 2073 2075 922\ndev helps 2547 3824 1697\n34.8kdev hurts 3824 3823 1697\ndev is helped by 2547 3824 1697\ndev is hurt by 3824 3823 1697\nTable 1: Breakdown of number of samples by relation\ntype, distance, and split. We maintain the same train-\ndev-test split as the WIQA dataset.\nAlthough our dataset uses relationship types\nfrom Table 1, our framework makes no relation-\nspeciﬁc assumptions and is generally applicable to\na broader range of relationships.\n3.3 Learning to Generate Inﬂuences\nAs discussed in section §3.1, the training\ndata consist of samples (xi,yi), where xi =\n(Pi,ns,ri,hi) and yi is the corresponding tar-\nget node nt. In our dataset, each procedural\npassage is used to create multiple training ex-\namples from variations in ns,ri,hi. For in-\nstance, Figure 1 shows four such training samples,\nwhere one example is as follows: xi = (Pi =\nprocedural text describing photosynthesis ,ns =\nmore sunlight ,ri = helps ,hi = 1-hop ), and\nyi = plants trap sunlight.\nEIGEN uses a language model to estimate the\nprobability of generating an end node nt for an\ninput xi. We ﬁrst transform the 4-tuple xi into\na single query sequence of tokens by concatenat-\ning its components i.e. we set xi = Pi∥ns∥ri∥hi,\nwhere ∥stands for string concatenation. Let the\nsequence of tokens representing the target event be\nyi = ⟨y1\ni,y2\ni,...,y M\ni ⟩, where N and M are the\nlengths of the query and the target event sequences.\nWe model the conditional probability pθ(yi |xi)\nas a series of conditional next token distributions\nparameterized by θ:\npθ(yi |xi) =\nM∏\nk=1\npθ(yk\ni |xi,y1\ni,..,y k−1\ni )\nEIGEN parameterizes pθ using the GPT-2 (Rad-\nford et al., 2019) pretrained language model. GPT-\n2 is based on the popular transformer architec-\nture (Vaswani et al., 2017), which consists of a\nseries of transformer blocks. Each transformer\nblock consists of two operations: a masked ver-\nsion of the multi-headed self-attention (Vaswani\net al., 2017) followed by a feed-forward network\n(FFN). Each of these operations is surrounded by a\nresidual connection (He et al., 2016), and followed\nby a layer normalization (Ba et al., 2016) operation.\nThe auto-regressive factorization of the language\nmodel pθ allows us to efﬁciently generate target\nevent inﬂuences for a given test input xj. Each to-\nken in yj is generated by sampling y1\nj ∼pθ(y|xj).\nThe next token is then drawn by sampling y2\nj ∼\npθ(u|xj,y1\nj). The process is repeated until a spec-\niﬁed end-symbol token is drawn at the Kth step.\nThe tokens ⟨y1\nj,y2\nj,...,y K−1\nj ⟩are then returned as\nthe generated target event inﬂuence.\n4 Experiments\nSetup: We use the dataset described in section\n§3.2 for training the model. The dataset statistics\nby relation type and hop information are shown\nin Table 1. EIGEN is based on the GPT-2 imple-\nmentation by Wolf et al. (2019).3, and uses nucleus\nsampling (Holtzman et al., 2019) for decoding out-\nput sequences over the ﬁne-tuned language model.\nAs discussed in ( §3.3), we concatenate the 4-tuple\nxi = (Pi,ns,ri,hi) in a single sequence of tokens.\nxi was concatenated using the template: “P what\ndoes ns ri at hi?”. All of our experiments were\ndone on a single Nvidia GeForce RTX 2080 Ti.\nThe models were ﬁne-tuned for 5 epochs for all the\nvariants.\n4.1 Baselines\nLSTM Seq-to-Seq: We train an LSTM (Hochre-\niter and Schmidhuber, 1997) based sequence to se-\nquence model (Bahdanau et al., 2015) which uses\n3Details of hyper-parameters in the Appendix A.2\nModel BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE Polarity\nGPT-2 w/o Fine-tuning* 7.66 3.05 1.56 0.91 4.79 7.85 8.25\nLSTM Seq-to-Seq 17.65 7.51 5.16 4.26 7.69 18.71 70.22\nCOMET 20.63 10.01 7.12 5.93 8.82 20.93 71.97\nEIGEN 28.97 16.23 11.69 9.74 12.85 29.65 77.24\nTable 2: Generation Quality for EIGEN and the baseline. BLEU-n refers to geometric average of BLEU scores\ncalculated upto n-grams. *- indicates that this baseline model is not ﬁne-tuned on our dataset.\nglobal attention described in (Luong et al., 2015).\nWe use pre-trained Glove (Pennington et al., 2014)\nto initialize the word embedding.4\nGPT-2 w/o Fine-tuning: The pretrained GPT-\n2 (Radford et al., 2019) without any additional\nﬁne-tuning serves as another baseline. The goal of\nthis baseline is to understand the extent to which\nGPT-2 without ﬁne-tuning could encode event in-\nﬂuence information.5\nCOMET: COMET (Bosselut et al., 2019) aims to\nperform knowledge base completion for common-\nsense knowledge bases by employing pretrained\nlanguage models. Unlike EIGEN , COMET does\nnot use any context or hop information. Although\nCOMET ’s architecture was based onGPT (Radford\net al., 2018), our implementation adapts COMET to\nuse GPT-2 for a fair comparison, and also supple-\nment each event input with hop-information. More\nconcretely, we set xi = (ns,ri,hi), with the goal\nof generating yi = nt.\n4.2 Automated Evaluation\nFor evaluating the predicted event inﬂuences, we\nuse the standard evaluation metrics BLEU (Pap-\nineni et al., 2002), METEOR (Denkowski and Lavie,\n2011), and ROUGE (Lin, 2004) 6. To complement\nthe above mentioned metrics, we also use polarity,\nwhich captures the direction of change captured by\nan inﬂuence (increasing, decreasing, neutral). For\nexample, an event inﬂuence “more sunlight” has\nthe polarity “increasing”, whereas an event “less\nrain” has the polarity “decreasing”. We calculate\nthe percentage of generated event inﬂuences that\nhave the same polarity as the reference. For exam-\nple, if both the reference event and the predicted\ntarget event are about an ‘increase,’ then we count\ntheir polarity to be the same. Otherwise, we count\n4https://github.com/OpenNMT/OpenNMT-py\n5GPT-2 implementation from Wolf et al. (2019)\n6We use Sharma et al. (2017) for calculating these metrics.\nhttps://github.com/Maluuba/nlg-eval\ntheir polarity to be different. We used a small set\nof hand-curated keywords to detect polarity 7.\nTable 2 shows that EIGEN outperforms the base-\nlines on all metrics. Furthermore, the results em-\nphasize that the pre-trained models can’t generate\nevent inﬂuences without being ﬁne-tuned on the\ntask. EIGEN outperforms COMET in all the met-\nrics by a considerable margin, (by about 8 ROUGE\npoints), reinforcing the importance of generating\nknowledge that is grounded in context.\nTable 3 breaks down the performance of EIGEN\nby relation type and the number of hop between\nthe source and the target nodes. From Table 3,\nwe observe that the best performance is obtained\non nodes generated at 1-hop with helps relation.\nThe 1-hop nodes generated with a hurts relation\nperform worse, indicating that generating negative\ninﬂuences is a harder task. Table 3 also highlights\nthat the 3-hop generations score higher than the\n2-hop relations for help and hurts relations. On\nfurther inspection, we found that this was an artifact\nfrom the human-curated inﬂuence graphs. Each\ninﬂuence graph had a maximum hop length of 3,\nand the end nodes are always of the form “more\nX” or “less X”, where X is a concept mentioned\nin the passage. Due to this templated nature of\nleaf nodes in the inﬂuence graph, the task becomes\nrelatively less challenging compared to 1-hop and\n2-hop inﬂuences.\n4.3 Human Evaluation\nIn addition to automated evaluation, we also com-\npare EIGEN with COMET for assessing the gener-\nation quality using human judgments. Three hu-\nman judges annotated 120 unique samples for rele-\nvance and reference, described next. We also com-\npared the output of the two systems for ﬂuency and\nfound that both the systems produce ﬂuent outputs.\nThis indicates that pretrained language models are\neffective in generative grammatically correct out-\n7This list of 22 words is included in the Appendix.\nRelation Hop BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE Polarity\nHelps 1-hop 33.32 20.54 15.47 13.16 14.88 34.19 82.56\nHelps 2-hop 25.99 13.77 9.20 7.44 11.78 26.71 73.88\nHelps 3-hop 32.25 17.82 14.88 13.59 14.00 33.55 80.04\nHurts 1-hop 28.67 16.79 11.71 9.46 13.04 29.37 77.23\nHurts 2-hop 25.03 13.15 8.63 6.74 11.41 26.09 74.12\nHurts 3-hop 32.72 18.08 15.03 13.65 14.46 34.19 81.02\nTable 3: Generation Quality of EIGEN by Relation Type and Node Distance\nput, even though the utility of the output may vary\ngreatly.\nTask EIGEN No Preference COMET\nRelevance 46.11 30.83 23.06\nReference 31.94 56.39 11.67\nTable 4: Results of human evaluation. The numbers\nshow the percentage(%) of times a particular option\nwas selected for each metric.\nRelevance: The annotators are provided with the\ninput of a procedural text, the source event, and\nthe relational questions. The outputs generated by\nCOMET and EIGEN are also provided in random\norder. The annotators were asked, “Which system\n(A or B) is more accurate relative to the background\ninformation given in the context?” They could also\npick option C (no preference).\nComparison with true event (reference): We\nmeasure how accurately each system-generated\nevent reﬂects the reference (true) event. Here, the\nannotators saw only the reference sentence and the\noutputs of two systems (A and B) in a randomized\norder. We asked the annotators, “Which system’s\noutput is closest in meaning to the reference?” The\nannotators could pick the options A, B, or C (no\npreference).\nFor relevance and reference comparison tasks\n(Table 4), we present the percentage of count of\nhuman judges for each of the three categories. The\ntable illustrates that EIGEN performs better than\nCOMET on both the metrics. Particularly, EIGEN\nnot only performs better thanCOMET but also much\nbetter than the “No Preference” option in the rel-\nevance metric. This means that EIGEN generates\ntarget events that logically follow the passage and\nsource events. We note that the automated metrics\nmay not capture the relevance and correctness of\nthe generated target events. The reference and rele-\nvance task scores together show that EIGEN does\nnot generate target events that are exactly similar\nto the reference target events, but they are correct\nin the context of the passage and the source event.\nThis can happen due to linguistic variation in the\ngeneration, as well as the ability of the source event\nto inﬂuence multiple target events in the context\nof the passage. We study this in more detail in the\nerror analysis presented below.\n4.4 Error Analysis\nTable 5 shows the error analysis on 100 random\nsamples from the validation set. We found that for\nabout 26% of samples, the generated event inﬂu-\nence had an exact match with the reference, and\nabout 30% of the samples had no overlap with the\nreference (category Wrong in Table 5). We found\nthat for 20% of the cases, the generated target event\nwas correct but was expressed differently compared\nto the reference text (Linguistic Variability) class\nin Table 5). Furthermore, we observed that in 17%\nof cases, the generated target event was not the\nsame as the reference target event, but it was rele-\nvant to the passage and the question, as shown in\nthe Related Event category in Table 5. In 5% of\nthe samples (Polarity), the model generates events\nwith opposite polarity compared to the reference. A\nsmall fraction (2%) of samples had incorrect gold\nannotations.\n4.5 Ablations and Discussion\nTable 6 shows the ablation results by removing\neach of paragraph, reverse edges and hop informa-\ntion from the 4-tuple xi = (Pi,ns,ri,hi) ( §3.1).\nThese ablations are performed to get an insight into\nthe contribution of each component in the input\nxi to the generation task. In line with the expecta-\ntion, the model with access to all the input compo-\nnents performs the best on almost all of our eval-\nuation metrics. We also observe that the context\nError Class Description % Question Reference Predicted\nPolarity The predicted polarity was wrong 5% What does ‘oil ﬁelds over-used’ there is not more oil\nbut event was correct help at 2-hop ? oil reﬁned is reﬁned\nLinguistic The output was a 20% What does ‘fewer rabbits will more more\nVariability linguistic variant of the reference become pregnant’ hurts at 1-hop ? rabbits babies\nRelated The output was related but 17% What does you inhale more air there will be you develop\nEvent different reference expected from the outside hurts at 1 hop ? less oxygen more blood clo-\nin your blood -ts in your veins\nWrong The output was 30% What does ‘less nutrients for more more wine\nwas completely unrelated plants’ hurt at 2-hop ? plants being produced\nErroneous The gold annotations 2% What does ‘less rabbit less more\nReference were erroneous rabbit mating’ hurt at 1-hop? rabbits babies\nTable 5: Examples of error categories. Error analysis is only shown for the incorrect outputs.\ninformation was the best indicator of model perfor-\nmance gains. Our ablation results re-emphasizes\nthat grounding event inﬂuence in the context of\nthe passage is crucial for the generation of target\nevents.\n5 Downstream QA\nIn this section, we examine the utility of EIGEN -\ngenerated graphs in a downstream question answer-\ning task on the WIQA benchmark.\n5.1 The QA Task\nWIQA (Tandon et al., 2019) is a dataset for proce-\ndural understanding, that comprises of “what-if”\nquestions to reason about the effects of one event\nperturbation on another in the context of a process.\nSpeciﬁcally, each question in WIQA consists of a\ncontext paragraph P and two input events nc and\nne. The task is to predict how nc affects ne, where\nthe result is one of: {helps, hurts, and no effect}.\n5.2 Using EIGEN to augment QA data\nWe use EIGEN to augment the event inﬂuences\nin each sample in the QA task as additional con-\ntext. Concretely, for the given context P, and\nthe event inﬂuences nc and ne, we generate for-\nward inﬂuences for nc and reverse inﬂuences for\nne using EIGEN . This scheme is intended to\ngenerate reasoning chains that connect nc to ne,\neven if ne is not an immediate consequence of\nnc. Concretely, we query EIGEN with four inputs:\n(P,nc,helps,1-hop), (P,nc,hurts,1-hop), and\n(P,ne,helped by,1-hop), (P,ne,hurt by,1-hop).\nThe generated event inﬂuences are then concate-\nnated to form a ﬂattened list of sentences xg.\nFollowing Tandon et al. (2019), we encode the\ninput sequence P∥nc∥ne using the BERT encoder\nE(Devlin et al., 2019), and use the[CLS ] token rep-\nresentation ( ˆhi) as our sequence representation. We\nthen use the same encoder Eto encode the gener-\nated inﬂuences xg∥nc∥ne, and use the [CLS ] token\nto get a representation for augmented inﬂuences\n( ˆha). Following the encoded inputs, we compute\nthe ﬁnal loss as follows:\nli = MLP1( ˆhi)\nla = MLP2( ˆha)\nL= α×Li + β×La\nwhere li,la represent the logits from ˆhi and ˆha\nrespectively, andLi and La are their corresponding\ncross-entropy losses. αand βare hyperparameters\nthat decide the contribution of the generated inﬂu-\nence graphs and the procedural text to the loss. For\nour experiments, we set α= 1and β = 0.9.\n5.3 QA Evaluation Results\nTables 7, 8, and 9 show the results from our ex-\nperiments on the WIQA QA dataset. BERT refers\nto the results from the original BERT based imple-\nmentation by Tandon et al. (2019), and BERT +\nEIGEN are the results obtained by augmenting the\nQA dataset with the inﬂuences generated by EIGEN\nas described above. Further, Tables 8 and 9 show\nthe accuracy of our method vs. the vanilla BERT\nmodel by question type and number of hops be-\ntween nc and ne. We observe from Table 8 that\naugmenting the context with generated inﬂuences\nfrom EIGEN leads to considerable gains over BERT\nbased model, with the largest improvement seen in\n3-hop questions. The strong performance on the\n3-hop question supports our hypothesis that gener-\nated inﬂuences might be able to connect two event\ninﬂuences that are farther apart in the reasoning\nPara Rev Hop BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE Polarity\n\u0017 \u0017 \u0013 20.20 9.48 6.63 5.46 8.83 20.49 73.75\n\u0017 \u0013 \u0017 19.96 9.19 6.49 5.41 8.58 20.39 72.80\n\u0017 \u0013 \u0013 20.63 10.01 7.12 5.93 8.28 20.93 71.97\n\u0013 \u0017 \u0017 26.19 14.08 10.06 8.52 11.85 26.78 74.67\n\u0013 \u0017 \u0013 27.51 15.64 11.68 10.02 12.42 27.81 76.76\n\u0013 \u0013 \u0017 26.05 14.23 10.10 8.45 11.87 27.10 75.68\n\u0013 \u0013 \u0013 28.97 16.23 11.69 9.74 12.85 29.65 77.24\nTable 6: Ablation experiments to understand the contribution of each of paragraph, reverse edges and hop informa-\ntion to the generation of the target event.\nModel Accuracy\nBERT + EIGEN 76.92\nBERT 73.80\nTable 7: QA Accuracy\nQuery Type BERT + EIGEN BERT\n1-hop 78.78 71.60\n2-hop 63.49 62.50\n3-hop 68.28 59.50\nTable 8: QA accuracy by number of hops\nchain. We also show in Table 9 that augmenting\nwith EIGEN improves performance on the difﬁcult\nexogenous category of questions, which requires\nbackground knowledge. In summary, the evalua-\ntion highlights the value of EIGEN as a framework\nfor improving performance on downstream tasks\nthat require event-based background reasoning and\nserves as an evaluation of the ability of EIGEN to\ngenerate targeted inﬂuences.\n6 Conclusion\nWe deﬁne the problem of event-inﬂuence rea-\nsoning as a generation task conditioned on con-\ntext, particularly exploring the efﬁcacy of large\nscale pre-trained language models for the task.\nWe use human-curated event inﬂuence graphs to\ntrain a model to generate targeted event inﬂuences\ngrounded in a context. Our experiments with abla-\ntions and error analysis provide insights into how\nto effectively adapt pretrained language models\nfor event inﬂuence generation and opens up ex-\nciting avenues for further research. Our method\noutperforms strong baselines on both automated\nand human evaluations. Furthermore, generated\ninﬂuences improve performance on the benchmark\nQuestion Type BERT + EIGEN BERT\nExogenous 64.04 56.13\nIn-para 73.58 79.68\nOut-of-para 90.84 89.38\nTable 9: QA accuracy by question type\nWIQA QA task without architectural changes to the\nmodel. Future work would extend the generalizabil-\nity of this method to understand more complex and\nvolatile event inﬂuences, such as events in news\narticles and stock markets.\nAcknowledgments\nThis material is based on research sponsored in\npart by the Air Force Research Laboratory under\nagreement number FA8750-19-2-0200, and in part\nby grants from National Science Foundation Se-\ncure and Trustworthy Computing program (CNS-\n1330596, CNS-15-13957, CNS-1801316, CNS-\n1914486). The U.S. Government is authorized to\nreproduce and distribute reprints for Governmen-\ntal purposes notwithstanding any copyright nota-\ntion thereon. The views and conclusions contained\nherein are those of the authors and should not be\ninterpreted as necessarily representing the ofﬁcial\npolicies or endorsements, either expressed or im-\nplied, of the Air Force Research Laboratory, the\nNSF, or the U.S. Government.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015.\nYonatan Bisk, Rowan Zellers, Ronan LeBras, Jian-\nfeng Gao, and Yejin Choi. 2020. Piqa: Reasoning\nabout physical commonsense in natural language. In\nAAAI, pages 7432–7439.\nAntoine Bosselut, Corin Ennis, Omer Levy, Ari Holtz-\nman, Dieter Fox, and Yejin Choi. 2018. Simulat-\ning action dynamics with neural process networks.\nIn International Conference on Learning Represen-\ntations.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli C ¸ elikyilmaz, and Yejin Choi.\n2019. Comet: Commonsense transformers for auto-\nmatic knowledge graph construction. In ACL.\nNathanael Chambers and Dan Jurafsky. 2008. Unsuper-\nvised learning of narrative event chains. In Proceed-\nings of ACL-08: HLT , pages 789–797, Columbus,\nOhio. Association for Computational Linguistics.\nBhavana Dalvi, Lifu Huang, Niket Tandon, Wen-tau\nYih, and Peter Clark. 2018. Tracking state changes\nin procedural text: a challenge dataset and models\nfor process paragraph comprehension. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1595–1604, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nJoe Davison, Joshua Feldman, and Alexander Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1173–1178, Hong Kong, China. As-\nsociation for Computational Linguistics.\nMichael Denkowski and Alon Lavie. 2011. Meteor 1.3:\nAutomatic metric for reliable optimization and eval-\nuation of machine translation systems. In Proceed-\nings of the sixth workshop on statistical machine\ntranslation, pages 85–91. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and\nMinlie Huang. 2020. A knowledge-enhanced pre-\ntraining model for commonsense story generation.\nTransactions of the Association for Computational\nLinguistics, 8:93–108.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751.\nChlo´e Kiddon, Luke Zettlemoyer, and Yejin Choi.\n2016. Globally coherent text generation with neural\nchecklist models. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 329–339.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2020. Data augmentation using pre-trained trans-\nformer models. arXiv preprint arXiv:2003.02245.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nFei Liu, Trevor Cohn, and Timothy Baldwin. 2018.\nNarrative modeling with memory chains and seman-\ntic supervision. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers) , pages 278–\n284, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nMinh-Thang Luong, Hieu Pham, and Christopher D\nManning. 2015. Effective approaches to attention-\nbased neural machine translation. In Proceedings of\nthe 2015 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1412–1421.\nChaitanya Malaviya, Chandra Bhagavatula, Antoine\nBosselut, and Yejin Choi. 2019. Exploiting\nstructural and semantic context for commonsense\nknowledge base completion. arXiv preprint\narXiv:1910.02915.\nQiang Ning, Zhili Feng, Hao Wu, and Dan Roth. 2018.\nJoint reasoning for temporal and causal relations. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2278–2288.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1532–1543.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nRachel Rudinger, Pushpendre Rastogi, Francis Ferraro,\nand Benjamin Van Durme. 2015. Script induction as\nlanguage modeling. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1681–1686.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A Smith, and Yejin Choi. 2019.\nAtomic: An atlas of machine commonsense for if-\nthen reasoning. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence , volume 33, pages\n3027–3035.\nShikhar Sharma, Layla El Asri, Hannes Schulz, and\nJeremie Zumer. 2017. Relevance of unsupervised\nmetrics in task-oriented dialogue for evaluating nat-\nural language generation. CoRR, abs/1706.09799.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Thirty-First AAAI Conference on\nArtiﬁcial Intelligence.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. The journal of machine learning\nresearch, 15(1):1929–1958.\nOyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau\nYih, and Ashish Sabharwal. 2019. Quarel: A dataset\nand models for answering questions about qualita-\ntive relationships. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence , volume 33, pages\n7063–7071.\nNiket Tandon, Bhavana Dalvi, Keisuke Sakaguchi, Pe-\nter Clark, and Antoine Bosselut. 2019. Wiqa: A\ndataset for “what if...” reasoning over procedural\ntext. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n6078–6087.\nSiddharth Vashishtha, Benjamin Van Durme, and\nAaron Steven White. 2019. Fine-grained\ntemporal relation extraction. arXiv preprint\narXiv:1902.01390.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nRui Wang, Deyu Zhou, and Yulan He. 2019. Open\nevent extraction from online text using a genera-\ntive adversarial network. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 282–291, Hong Kong,\nChina. Association for Computational Linguistics.\nThomas Wolf, L Debut, V Sanh, J Chaumond, C De-\nlangue, A Moi, P Cistac, T Rault, R Louf, M Fun-\ntowicz, et al. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nSen Yang, Da wei Feng, Linbo Qiao, Zhigang Kan, and\nD. Li. 2019. Exploring pre-trained language models\nfor event extraction and generation. In ACL.\nYiben Yang, Chaitanya Malaviya, Jared Fernandez,\nSwabha Swayamdipta, Ronan Le Bras, Ji-Ping\nWang, Chandra Bhagavatula, Yejin Choi, and Doug\nDowney. 2020. G-daug: Generative data augmen-\ntation for commonsense reasoning. arXiv preprint\narXiv:2004.11546.\nA Appendix\nA.1 Polarity Words\nIncreasing words helps, more, higher, increase,\nincreases, stronger, faster, greater, longer, larger,\nhelping\nDecreasing words hurts, less, lower, decrease,\ndecreases, weaker, slower, smaller, hurting, softer,\nfewer\nA.2 Hyperparameters\nSeq-to-Seq: We use 2 layers of LSTM encoder\nand decoder with a hidden size of 500, word em-\nbedding size of 300. The encoder is bidirectional.\nWe use Glove embedding of 300 dimensions.\nEIGEN : EIGEN ﬁne-tunes GPT-2 (Radford et al.,\n2019), allowing us to re-use the same hyperpa-\nrameters as with small adjustments in the rec-\nommended range. We use the medium (355M)\nvariant of GPT-2 for our experiments with 24-\nlayer, 1024-hidden, 16-heads, 345M parame-\nters ( https://huggingface.co/transformers/\npretrained_models.html). We use the weights\nreleased by Radford et al. (2019). We use\nAdam (Kingma and Ba, 2014) for optimization\nwith a learning rate of5e−05. All the dropouts (Sri-\nvastava et al., 2014) were set to 0.1 We found the\nbest hyperparameter settings by searching the space\nusing the following hyperparameters.\n1. weight decay = {0.1, 0.01, 0.05 }\n2. embedding dropout = {0.1, 0.2, 0.3 }\n3. learning rate = {1e-05, 2e-05, 5e-05, 1e-06}",
  "topic": "Leverage (statistics)",
  "concepts": [
    {
      "name": "Leverage (statistics)",
      "score": 0.7724936008453369
    },
    {
      "name": "Computer science",
      "score": 0.7240781784057617
    },
    {
      "name": "Closeness",
      "score": 0.723930835723877
    },
    {
      "name": "Event (particle physics)",
      "score": 0.5946234464645386
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5884896516799927
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.571220338344574
    },
    {
      "name": "Language model",
      "score": 0.5298891663551331
    },
    {
      "name": "Relevance (law)",
      "score": 0.5257019996643066
    },
    {
      "name": "Natural language processing",
      "score": 0.521806001663208
    },
    {
      "name": "Machine learning",
      "score": 0.4891839325428009
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4673875570297241
    },
    {
      "name": "Mathematics",
      "score": 0.1352081000804901
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ],
  "cited_by": 14
}