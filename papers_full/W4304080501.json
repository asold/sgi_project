{
  "title": "Towards Accurate Post-Training Quantization for Vision Transformer",
  "url": "https://openalex.org/W4304080501",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2745770433",
      "name": "Ding Yifu",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A4224020819",
      "name": "Qin, Haotong",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A1894957069",
      "name": "Yan Qing-hua",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2239068323",
      "name": "Chai, Zhenhua",
      "affiliations": [
        "Meizu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1996127875",
      "name": "Liu Jun-jie",
      "affiliations": [
        "Meizu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2350832834",
      "name": "Wei Xiao-lin",
      "affiliations": [
        "Meizu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2498545944",
      "name": "Liu Xiang-long",
      "affiliations": [
        "Beihang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2998218113",
    "https://openalex.org/W3180355996",
    "https://openalex.org/W3108835732",
    "https://openalex.org/W4214588794",
    "https://openalex.org/W4252345548",
    "https://openalex.org/W3162286130",
    "https://openalex.org/W3173222967",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3098085362",
    "https://openalex.org/W4214520160",
    "https://openalex.org/W2701971652",
    "https://openalex.org/W3185095134",
    "https://openalex.org/W6600339963",
    "https://openalex.org/W3010512657",
    "https://openalex.org/W3092955400",
    "https://openalex.org/W3107472389",
    "https://openalex.org/W6781533629",
    "https://openalex.org/W2946304433",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W4301076899",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3211787299",
    "https://openalex.org/W3101036738",
    "https://openalex.org/W3046835050",
    "https://openalex.org/W3216998657",
    "https://openalex.org/W3038470071",
    "https://openalex.org/W3166874749",
    "https://openalex.org/W4287330714",
    "https://openalex.org/W2962721361",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W639708223",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W3128090102",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W4391021637",
    "https://openalex.org/W2900768265",
    "https://openalex.org/W3215741372",
    "https://openalex.org/W2167090521",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3203149535",
    "https://openalex.org/W4287551327",
    "https://openalex.org/W3152698000",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3170233084",
    "https://openalex.org/W4226244084",
    "https://openalex.org/W4286910290",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W4300435436",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2962298324"
  ],
  "abstract": "Vision transformer emerges as a potential architecture for vision tasks. However, the intense computation and non-negligible delay hinder its application in the real world. As a widespread model compression technique, existing post-training quantization methods still cause severe performance drops. We find the main reasons lie in (1) the existing calibration metric is inaccurate in measuring the quantization influence for extremely low-bit representation, and (2) the existing quantization paradigm is unfriendly to the power-law distribution of Softmax. Based on these observations, we propose a novel Accurate Post-training Quantization framework for Vision Transformer, namely APQ-ViT. We first present a unified Bottom-elimination Blockwise Calibration scheme to optimize the calibration metric to perceive the overall quantization disturbance in a blockwise manner and prioritize the crucial quantization errors that influence more on the final output. Then, we design a Matthew-effect Preserving Quantization for Softmax to maintain the power-law character and keep the function of the attention mechanism. Comprehensive experiments on large-scale classification and detection datasets demonstrate that our APQ-ViT surpasses the existing post-training quantization methods by convincing margins, especially in lower bit-width settings (e.g., averagely up to 5.17% improvement for classification and 24.43% for detection on W4A4). We also highlight that APQ-ViT enjoys versatility and works well on diverse transformer variants.",
  "full_text": "Towards Accurate Post-Training Quantization\nfor Vision Transformer\nYifu Ding1,2,3, Haotong Qin1,3, Qinghua Yan1,\nZhenhua Chai2, Junjie Liu2, Xiaolin Wei2, Xianglong Liuâ€ ,1\n1 State Key Laboratory of Software Development Environment, Beihang University\n2 Meituan 3 Shen Yuan Honors College, Beihang University\n{yifuding,haotongqin,yanqh,xlliu}@buaa.edu.cn,{chaizhenhua,liujunjie10,weixiaolin02}@meituan.com\nABSTRACT\nVision transformer emerges as a potential architecture for vision\ntasks. However, the intense computation and non-negligible delay\nhinder its application in the real world. As a widespread model com-\npression technique, existing post-training quantization methods\nstill cause severe performance drops. We find the main reasons lie\nin (1) the existing calibration metric is inaccurate in measuring the\nquantization influence for extremely low-bit representation, and\n(2) the existing quantization paradigm is unfriendly to the power-\nlaw distribution of Softmax. Based on these observations, we pro-\npose a novel Accurate Post-training Quantization framework for\nVision Transformer, namely APQ-ViT. We first present a unified\nBottom-elimination Blockwise Calibration scheme to optimize the\ncalibration metric to perceive the overall quantization disturbance\nin a blockwise manner and prioritize the crucial quantization er-\nrors that influence more on the final output. Then, we design a\nMatthew-effect Preserving Quantization for Softmax to maintain the\npower-law character and keep the function of the attention mech-\nanism. Comprehensive experiments on large-scale classification\nand detection datasets demonstrate that our APQ-ViT surpasses the\nexisting post-training quantization methods by convincing margins,\nespecially in lower bit-width settings (e.g., averagely up to 5.17%\nimprovement for classification and 24.43% for detection on W4A4).\nWe also highlight that APQ-ViT enjoys versatility and works well\non diverse transformer variants.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Computer vision problems .\nKEYWORDS\npost-training quantization, vision transformer, computer vision\nACM Reference Format:\nYifu Ding1,2,3, Haotong Qin1,3, Qinghua Yan1,, Zhenhua Chai2, Junjie Liu2,\nXiaolin Wei2, Xianglong Liuâ€ ,1 . 2022. Towards Accurate Post-Training Quan-\ntization for Vision Transformer. InProceedings of the 30th ACM International\nâ€ Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nMM â€™22, October 10â€“14, 2022, Lisboa, Portugal\nÂ© 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9203-7/22/10. . . $15.00\nhttps://doi.org/10.1145/3503161.3547826\nConference on Multimedia (MM â€™22), October 10â€“14, 2022, Lisboa, Portugal.\nACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3503161.3547826\n1 INTRODUCTION\nWith the development of deep learning, the neural networks achieve\ngreat success in a various domains, such as image classification [24,\n37, 39, 42â€“44], object detection [13, 14, 26, 33, 36, 45], semantic seg-\nmentation [11, 51], etc. Recently, the Vision Transformer (ViT) [9]\nemerges as a novel and effective architecture and shows great po-\ntential for various vision tasks. However, pretrained models usually\nhave massive parameters and considerable computational over-\nheads, e.g., the ViT-L model is with 307M parameters and 190.7\nGFLOPs during inference [ 9]. The high computational complex-\nity and non-negligible latency hinder the practical applications of\nvision transformers in real-world applications especially on edge\ndevices. To address the challenge, many architectures have been\nproposed for lightweight vision transformers ([32], [15], [31]). Al-\nthough these works have achieved remarkable speedup and mem-\nory footprint reduction, they still rely on floating-point operations,\nleaving room for further compression by parameter quantization.\nAs a model compression approach, quantization compacts the\nfloating-point parameters of neural networks to lower-bit represen-\ntations, and the computation can be implemented by efficient inte-\nger operations on hardware. Thus, quantized vision transformers\nsignificantly save the storage and speed up inference. Considering\nthat re-training the transformer is time-consuming and computa-\ntionally intensive, Post-Training Quantization (PTQ) is a practical\nsolution in widespread scenarios, which just takes a small unla-\nbeled dataset to quantize (calibrate) a pre-trained network with no\nneed for training or fine-tuning. Many previous works are devoted\nto quantizing vision transformers [28, 30, 47], which shows great\npotential in both accuracy and efficiency. Applying the existing\nmethods can almost retain the original accuracy of full-precision\ntransformers under the 8-bit setting.\nHowever, when quantizing the vision transformer to ultra-low\nbit-widths (e.g.,, 4-bit weight and activation), the model suffers\nsevere accuracy drop or even crashes. Our study reveals that the\npoor performance might attribute to two issues from optimiza-\ntion and structure perspectives. From the optimization perspective,\nthe limited bit-width constraints the representation capability and\ncauses larger errors which makes the existing second-order layer-\nwise calibration metric not accurate in measuring the impact of\nquantization error on the final output. And as for the structure\nperspective, the existing quantization paradigm is unfriendly to the\nSoftmax function in the attention mechanism, which is also known\nas a normalized exponential function. It redistributes the inputs to\narXiv:2303.14341v1  [cs.CV]  25 Mar 2023\nMM â€™22, October 10â€“14, 2022, Lisboa, Portugal Yifu Ding et al.\nSignificantvalues\nQuantized\nFP\nNegligiblevalues\nMatmul\nSoftmax\nTransformerBlock\nPatchEmbed\nMatmulMatmul\nOut\nFCFC\nQ K V\nClassifier\nBottom-eliminationBlockwiseCalibration\nğœ!\nğœ!âˆ’ğœ\"!#diagğœ!âˆ’ğœ\"!\nâˆ’ğœ\"!\nTransformerBlock Matthew-effect PreservingQuantization\nOut\nMatmulAttentionstructure\nKQ V\ndiagonalmatrix\ndiagğœ•â„’ğœ•ğ‘‚$!%,â€¦, ğœ•â„’ğœ•ğ‘‚|'!|!%\nSignificantvalues\nQuantized\nFP\nNegligiblevalues\nMatmul\nSoftmax\nTransformerBlock\nPatchEmbed\nMatmulMatmul\nOut\nFCFC\nQ K V\nClassifier\nBottom-eliminationBlockwiseCalibration\nğœ!\nğœ!âˆ’ğœ\"!#diagğœ!âˆ’ğœ\"!\nâˆ’ğœ\"!\nTransformerBlock Matthew-effect PreservingQuantization\nOut\nMatmulAttentionstructure\nKQ V\ndiagonalmatrix\nFigure 1: Overview of APQ-ViT. The left is Bottom-elimination Blockwise Calibration to apply quantization in a blockwise\nmanner to perceive the quantization loss of adjacent layers, and prioritize the significant errors by eliminating the second-\norder gradient corresponding to trivial errors. The right is Matthew-effect Preserving Quantization, which is specialized for\nmaintaining the power-law distribution of the Softmax function.\nsatisfy the power-law probability, while we discover that previous\nquantization solutions are easy to damage the Matthew-effect of\nSoftmax. Therefore, specializing in the quantization strategy for\nvision transformers is a great need to improve the accuracy of the\nlow-bit quantized model.\nIn this paper, we propose an accurate post-training quantization\nmethod for vision transformers, namely APQ-ViT, which considers\nboth the optimization difficulty and the special structure for low\nbit-width (See the overview in Figure 1). First, we present a unified\nBlockwise Bottom-elimination Calibration (BBC) scheme to optimize\nthe calibration metric based on the block-stacking architecture,\nwhich can be flexibly generalized to other variants. It enables the\nmetric to perceive the quantization loss in a blockwise manner\nand prioritize the significant errors by eliding the second-order\ngradients corresponding to the inevitable trivial errors. Second, the\nMatthew-effect Preserving Quantization (MPQ) is specifically tailored\nfor the Softmax function. Instead of obeying the maximizing mutual\ninformation paradigm as many quantization methods, we hold the\nview that preserving the power-law distribution in the quantized\nSoftmax is more crucial for the attention mechanism.\nOur APQ-ViT revisits the process of post-training quantization\nfor vision transformer and presents novel insights. Comprehensive\nexperiments on the large-scale computer vision tasks (image classifi-\ncation [7] and object detection [27]) demonstrate that our APQ-ViT\nperforms remarkably well across various transformer architectures\nsuch as ViT [ 9], DeiT [41], and Swin Transformer [ 29], and sur-\npasses the existing methods by convincing margins, especially in\nlower bit-width settings (e.g., averagely up to 5.17% improvement\nfor classification and 24.43% for detection on W4A4). We highlight\nthat our APQ-ViT scheme achieves state-of-the-art accuracy per-\nformance on various bit-width settings, and enjoys versatility on\ndiverse architectures and vision tasks.\nWe summarize our main contributions as:\nâ€¢We find that for the post-training quantization of vision trans-\nformer, (1) the extremely low-bit representation makes the exist-\ning calibration metric inaccurate in measuring the quantization\nerrors; and (2) an inconsistency exists between the quantization\nparadigm and the power-law distribution of Softmax.\nâ€¢We present an accurate post-training quantization method for\nvision transformer, namely APQ-ViT, with a unified Blockwise\nBottom-elimination Calibration scheme to enable the quantiza-\ntion perception inside blocks and prioritize the crucial errors that\ninfluence the final predictions.\nâ€¢Our study reveals the power-law distribution of the Softmax func-\ntion and proposes the Matthew-effect Preserving Quantization.\nIn contrast to purely minimizing the quantization loss, it inspires\na novel perspective to preserve the character of Softmax while\nembedding quantization.\nâ€¢We evaluate the APQ-ViT on large-scale image classification and\nobject detection tasks with different model variants and bit-width,\nand obtain prevailing improvements over existing post-training\nquantization methods especially in lower bit-width.\n2 RELATED WORK\n2.1 Vision Transformer\nThe classical vision transformer [9] is constructed by pure trans-\nformer targeting to process image patches, which is stacked by\nseveral attention-based blocks that are composed of a multi-head\nself-attention module and multi-layer perceptron. DETR [ 4] fur-\nther extends to object detection, which uses ResNet as the back-\nbone and replaces the detection head with transformers. Following\nthem, there are many variants for more applications with new\ntechniques specialized for CV applications [ 10, 25, 34, 35]. Swin\nTransformer [29] emerges as a competitive backbone with excel-\nlent generalization capability for many benchmarks and remarkably\nsurpasses the state-of-the-art CNNs in most CV tasks.\nMany works are also devoted to balancing performance and ef-\nficiency. MobileViT exploits global representation capability and\nspatial order preservation of transformer by inserting it into con-\nvolution blocks. Some works simplify the attention mechanism,\nlike sparse attention [3, 48], linear approximate attention [5, 21]\nTowards Accurate Post-Training Quantization\nfor Vision Transformer MM â€™22, October 10â€“14, 2022, Lisboa, Portugal\n(a)8-bit (d)Overallcurve(b)6-bit (c)4-bit\nFigure 2: Visualization of Hessian-guided loss term of the optimal scaling factor for (a) 8-bit, (b) 6-bit and (c) 4-bit quantization.\nAnd (d) shows the curve of overall loss terms for all the candidates.\nQuadTree attention [40], or hashing-based attention [ 23]. More-\nover, general model compression techniques are also actively ap-\nplied. DeiT introduces distillation tokens to better interact with the\nteacher model through attention. [50] prunes vision transformers\nby sparsing the unnecessary feature channels by ranking the im-\nportance. [16] combines NAS and parameter sharing to search and\nreplace some self-attention modules by convolutions to improve\nthe locality extraction. The above methods focus on optimizing the\ntransformer architectures while keep the parameters full-precision,\nleaving room for further compression by quantization.\n2.2 Quantization\nModel quantization is one of the promising compression approaches,\nwhich quantizes the full-precision parameters to lower bit-width. It\ncan not only shrink the model size but reduce computational com-\nplexity by transforming floating-point calculations to fixed-point,\nwhich significantly accelerates the inference, decreases the mem-\nory footprint, and reduces energy consumption. Current quantiza-\ntion methods can be categorized as Quantization-aware Training\n(QAT) and Post-training Quantization (PTQ) according to whether\ntraining/fine-tuning or not. Considering that training vision trans-\nformers is computationally intensive and time-consuming, they\nalways have a huge demand for computation and power resources\nthat emits lots of carbon footprint. PTQ, a training-free method, is\nwell recognized as a more feasible solution, which can be broadly\ndivided into two types: 1) searching best scale factor. 2) optimizing\ncalibration strategy. To search best scale factors, [6] proposes an op-\ntimal MSE to select the scale factor that minimizes the quantization\nerror. [47] uses Twin Uniform Quantization that specially designs\ntwo scales for long-tail parameter distribution of Softmax and GeLU,\nand proposes Hessian Guided Metric to search for best scales. [12]\nalso use Piecewise Linear Quantization to make the quantized pa-\nrameters better fit the bell-shaped distribution of weight and activa-\ntion after scaling. As for the calibration strategy, AdaQuant [17, 19]\nutilizes layerwise optimization, which fixes the error induced by\nquantizing former layers by sequential calibration. EasyQuant [8]\nuses an alternative scale optimization of weight and activation,\nfixing one and optimizing the other throughout the network. [2]\nminimizes the quantization error by module-wise reconstruction\nto jointly optimize all the coupled linear layers inside each module.\n3 METHOD\nIn this section, we propose an accurate post-training quantiza-\ntion framework for vision transformer, namely APQ-ViT. We first\npresent the basic quantization pipeline and then introduce our tech-\nniques, including Blockwise Bottom-elimination Calibration (BBC)\nto tackle the optimization difficulties in low bit-width and the\nMatthew-effect Preserving Quantization (MPQ) to preserve the power-\nlaw redistribution for Softmax function.\n3.1 Preliminaries\nAs a widespread solution, the asymmetric uniform quantization is\napplied to quantize network. And in a standard quantization trans-\nformer, the input data first passes through a quantized embedding\nlayer before being fed into the quantized transformer blocks, and\neach transformer block consists of an MSA module and an MLP.\nThe computation of MSA depends on queries Q, keys K and values\nV, which are derived from hidden states H. In a specific quantized\ntransformer layer, H is first quantized to Ë†H before passing through\nlinear layers, which can be expressed as\nË†Q = Ë†wğ‘„ Ë†H, Ë†K = Ë†wğ¾ Ë†H, Ë†V = Ë†wğ‘‰ Ë†H, (1)\nwhere Ë†wğ‘„, Ë†wğ¾, Ë†wğ‘‰ represent quantized weight of three different\nlinear layers for Q, K, V respectively. The computation of self-\nattention is formulated as Eq. (2):\nAttentionğ‘(Q,K,V)= softmaxğ‘\n\u0012 Ë†Q Ã—Ë†Kğ‘‡\nâˆš\nğ‘‘\n\u0013\nÃ—Ë†V, (2)\nwhere ğ‘‘is the hidden size of the head andsoftmaxğ‘denotes the Soft-\nmax function with quantized output. The outputs of multiple heads\nare concatenated together as the output of MSA. Moreover, the\nMLP contains two quantized linear layers, and the GeLU activation\nfunction is used after the first layer.\nAmong the existing post-training quantization methods for vi-\nsion transformers, one of the representatives is PTQ4ViT [ 47],\nwhich is a typical representation of these works using the Hes-\nsian guided metric to determine the scaling factors. In classification\ntask, the task loss is L= CE(Ë†ğ‘¦,ğ‘¦), where CE is cross-entropy, Ë†ğ‘¦\nis the output of the network and ğ‘¦ is the ground truth. The ex-\npectation of loss is a function of network parameters x, which is\nE[L(x)]. The quantization brings a small perturbation ğœ–on param-\neter Ë†x = x +ğœ–. We analyze the influence of quantization to the task\nMM â€™22, October 10â€“14, 2022, Lisboa, Portugal Yifu Ding et al.\nloss by Taylor series expansion:\nE[L(Ë†x)]âˆ’ E[L(x)]â‰ˆ ğœ–ğ‘‡ Â¯ğ‘”(x)+1\n2ğœ–ğ‘‡ Â¯ğ»(x)ğœ–, (3)\nwhere Â¯ğ‘”(x)is the gradients and Â¯ğ»(x)is the Hessian matrix. The tar-\nget is to find the scaling factors to minimize the influence:minÎ”(E[L(Ë†x)]âˆ’\nE[L(x)]). Since weightâ€™s perturbation ğœ– is relatively small, we\nhave a first-order Taylor expansion that (Ë†ğ‘‚âˆ’ğ‘‚)â‰ˆ ğ‘”ğ‘‚(x)ğœ–, where\nË†ğ‘‚ = (Ë†x +ğœ–)ğ‘‡Ë†x. The second-order term in Eq. (3) could be written as\nğœ–ğ‘‡ Â¯ğ»(x)ğœ– â‰ˆ( Ë†ğ‘‚âˆ’ğ‘‚)ğ‘‡ Â¯ğ»(ğ‘‚)(Ë†ğ‘‚âˆ’ğ‘‚). (4)\nThen we follow [8, 47] to traverse the search spaces ofÎ”x by linearly\ndividing\nh\nğ›¼xmaxâˆ’xmin\n2ğ‘˜ ,ğ›½ xmaxâˆ’xmin\n2ğ‘˜\ni\nto ğ‘›candidates. ğ›¼ and ğ›½ are two\nparameters to control the search range. We alternatively search for\nthe optimal scaling factors Î”âˆ—w and Î”âˆ—a in the search space. Firstly,\nÎ”a is fixed, and we search for the optimal Î”w to minimize loss L.\nSecondly, Î”w is fixed, and we search for the optimalÎ”a to minimize\nL. Î”w and Î”a are alternately optimized for several rounds.\n3.2 Blockwise Bottom-elimination Calibration\nFrom the optimization perspective, existing typical post-training\nquantization methods for vision transformers use the second-order\nHessian-guided metric to measure the quantization loss caused\nby each candidate scaling factor and then determine the optimal\nquantizer. However, we find that for the extremely low-bit repre-\nsentation, the layerwise optimization is inaccurate since it is unable\nto perceive the quantization in a higher block scale, and the quan-\ntization error is inevitably larger while the dense Hessian matrix\nloses the attention of the significant errors.\nIdeally, we expect to determine the quantizer with the smallest\nquantization loss by a carefully designed loss term, and the loss\nshould be significantly smaller compared to other candidates which\nconverges to a local optimum. But in practice, we find that the\nHessian-guided loss terms calculated by each candidate have large\nvariance, especially at ultra-low bits (such as 4-bit). As shown in\nFigure 2, when we quantize the model to 8-bit, the quantization loss\nis steady and relatively small, the optimal loss plane in Figure 2(a)\nis also flat. However, when the bit-width is reduced to 4-bit, the\nbehavior of adjacent candidates shows a significant difference, and\nthe loss curve fluctuations heavily. Even the optimal candidate has\nlots of spikes on the loss plane (see Figure 2(c)). The phenomena\nreveal the defects of the current calibration strategy, due to the\nhigher degree of discretization in lower-bit quantization, (1) the\nloss in a single layer is larger, which has an impact on the calibra-\ntion of other layers in the layerwise calibration strategy, (2) the\nquantization loss of each element varies greatly that times larger\nthan 8-/6-bit quantization.\nTherefore, we propose aBlockwise Bottom-elimination Calibration\n(BBC) scheme for the post-training quantization. It optimizes the cal-\nibration in a blockwise manner which enables the Hessian-guided\nloss to have a perception of the quantization error of adjacent layers\nin a single block. And It uses the bottom-elimination mechanism to\nfocus on the critical errors that influence the final output instead\nof the whole loss plane.\nFirstly, we built a blockwise calibration scheme from post-training\nquantization. Taking ğ‘-th block with ğ¿layers as an example, the\n(a)Original(b)Bottom-elimination\nFigure 3: The quantization error measured by Hessian-based\nmetric. Y-axis represents the magnitude of errors. (a) is the\nwhole error distribution, and (b) visualizes the elimination\nof error in the 10th percentile.\ncomputation in the block can be represented as\nğ‘‚ğ‘ = wğ‘\nğ¿\nğ‘‡\nwğ‘\nğ¿âˆ’1\nğ‘‡\nÂ·Â·Â· wğ‘\n1\nğ‘‡\nağ‘, (5)\nwhere ağ‘ and ğ‘‚ğ‘ denote the input and output of the ğ‘-th trans-\nformer block. When the ğ‘™-th layer is calibrated, the ğ¿-th to ğ‘™-th\nlayers can be considered as a composite layer, and its weight and\nactivation is expressed as\nWğ‘\nğ‘™ = wğ‘\nğ¿\nğ‘‡\nÂ·Â·Â· wğ‘\nğ‘™+1\nğ‘‡\nwğ‘\nğ‘™\nğ‘‡\n, Ağ‘\nğ‘™ = wğ‘\nğ‘™âˆ’1\nğ‘‡\nÂ·Â·Â· wğ‘\n1\nğ‘‡\nağ‘, (6)\nTaking the weight calibration as an example, the second-order term\nof the ğ‘™-th layer in ğ‘-th transformer block can be expressed as\nğœ–ğ‘\nğ‘™\nğ‘‡ Â¯ğ»(W)ğœ–ğ‘\nğ‘™ =\n\u0010\nğ½ğ‘‚ğ‘ (Wğ‘\nğ‘™ )ğœ–ğ‘\nğ‘™\n\u0011ğ‘‡ Â¯ğ»(ğ‘‚ğ‘ )ğ½ğ‘‚ğ‘ (Wğ‘\nğ‘™ )ğœ–ğ‘\nğ‘™\nâ‰ˆE\nï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\nğœğ‘ğ‘‡\ndiag Â©Â­\nÂ«\n \nğœ•L\nğœ•ğ‘‚ğ‘\n1\n!2\n,..., Â©Â­\nÂ«\nğœ•L\nğœ•ğ‘‚ğ‘\n|ğ‘‚ğ‘ |\nÂªÂ®\nÂ¬\n2\nÂªÂ®Â®\nÂ¬\nğœğ‘\nï£¹ï£ºï£ºï£ºï£ºï£ºï£»\n,\n(7)\nwhere ğœ–ğ‘\nğ‘™ = Wğ‘\nğ‘™ âˆ’ Ë†Wğ‘\nğ‘™ , ğœğ‘ = Ë†ğ‘‚ğ‘ âˆ’ğ‘‚ğ‘, and ğ‘‚ğ‘, Ë†ğ‘‚ğ‘ are the outputs\nof the ğ‘-th block before and after quantization, respectively. ğ‘‚ğ‘\nğ‘–\ndenotes the ğ‘–-th dimension of ğ‘‚ğ‘, where ğ‘– âˆˆ[1,|ğ‘‚ğ‘|]. Therefore,\nwe optimize the calibration metric to enable the perception of the\nwhole block and reduce the impact on the final output.\nSecondly, since the quantization error is deemed as inevitable in\nrounding operation and significantly increases when the bit-width\nis smaller, the larger errors should be of major concern. Inspired\nby [1, 38, 46, 49] that pruning the gradients close to zero in the\nbackward propagation has a tiny impact on weight-updating. To\npay more attention to the large errors that perturb the final out-\nput and also strike a balance between the range and variance of\nerror, we further propose the bottom-elimination mechanism to\nobtain a sparse Hessian matrix in the optimization scheme. It con-\nsiders the Hessian-based metric as weighted second-order gradients,\nand prunes the gradients that correspond to the smallest absolute\nquantization errors. Specifically, we construct a bottom-elimination\nmatrix ğœğ‘ğ›¾ for the ğœğ‘ = Ë†ğ‘‚ğ‘ âˆ’ğ‘‚ğ‘, which aims to select the elements\nwith absolute values in ğ›¾-th percentile:\nğœğ‘\nğ›¾ = ğœğ‘[ğœğ‘]ğ›¾, [ğœğ‘]ğ›¾ =\n(\n1,where ğœğ‘ < |ğœğ‘|ğ›¾,\n0,otherwise, (8)\nwhere [Â·]denotes the Iverson bracket [18]. We apply the bottom\nelimination matrix ğœğ‘ğ›¾ to Eq. (7) so that the obtained metric reflects\nTowards Accurate Post-Training Quantization\nfor Vision Transformer MM â€™22, October 10â€“14, 2022, Lisboa, Portugal\n(a)ActivationinLinear(b)ActivationinMatmul(c)ActivationafterSoftmax\n(a)InputforLinear(b)InputforMatmul1(c)InputforMatmul2(afterSoftmax)\nFigure 4: Visualization of different input activation distribu-\ntion in the pretrained vision transformer model. (c) presents\nan extreme unbalanced distribution.\nthe critical elements in quantization that causes larger quantization\nerror, and the optimization objective function is expressed as:\nmin\nÎ”\nE\nï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\n\u0010\nğœğ‘ âˆ’ğœğ‘\nğ›¾\n\u0011ğ‘‡\ndiag Â©Â­\nÂ«\n \nğœ•L\nğœ•ğ‘‚ğ‘\n1\n!2\n,..., Â©Â­\nÂ«\nğœ•L\nğœ•ğ‘‚ğ‘\n|ğ‘‚ğ‘ |\nÂªÂ®\nÂ¬\n2\nÂªÂ®Â®\nÂ¬\n\u0010\nğœğ‘ âˆ’ğœğ‘\nğ›¾\n\u0011ï£¹ï£ºï£ºï£ºï£ºï£ºï£»\n.\n(9)\nFigure 3 visualizes the effect of the bottom-elimination mecha-\nnism for quantization errors. With the matrix ğœğ›¾, we optimize the\ncalibration metric to make it focuses on perturbations with large\nmagnitude which have nonnegligible influence on the final output\nof task predictions.\n3.3 Matthew-effect Preserving Quantization\nThe special architecture of the attention mechanism in the vision\ntransformer is also an obstacle to low-bit quantization. Especially\nthe Softmax function, also known as the normalized exponential\nfunction, is well recognized to be unfriendly to quantization. Gen-\nerally speaking, as shown in Section 3.1, each block in the vision\ntransformer usually contains three types of computation: Linear\noperation, Matmul operation, and Softmax operation, these three\noperations involve the majority of quantized representations. We\nshow a typical distribution of the activation inputs in Figure 4. We\ncan see that the input distributions of the Linear and Matmul1 are\nsimilar to the Gaussian and Laplacian distributions that common\nmethods can well quantize. However, the output of the Softmax\noperation obeys the power-law probability distribution, which is\nasymmetric and extremely unbalanced.\nAs a consensus, the ideal quantized parameters should retain\nthe information of full-precision counterparts as much as possible,\nwhich is formulated as:\narg max\nx,Ë†x\nI(x; Ë†x)= H(x)âˆ’H( Ë†x |x), (10)\nwhere H(Ë†x)is the information entropy, and H(Ë†x |x)is the condi-\ntional entropy of Ë†x given x. Since we use the deterministic quan-\ntization function, the value of Ë†x fully depends on the value of x,\ni.e., H(Ë†x |x)= 0. Thus, the objective function is equivalent to\nmaximizing the information entropy:\narg max\nË†x\nH(Ë†x)= âˆ’\nâˆ‘ï¸\nË†ğ‘¥âˆˆË†X\nğ‘Ë†x (Ë†ğ‘¥)log ğ‘Ë†x (Ë†ğ‘¥), (11)\nwhere ğ‘Ë†x denotes the probability mass function of quantized param-\neter Ë†x. The formulation suggests that a well-optimized quantizer\ntends to make the probabilities in each quantization interval equal.\n(b)Logarithmic\n(c)Segmental(d)MPQ\n(a)Full-precision\nâ„‹ğ‘=3.82\nâ„‹ğ‘=0.23â„‹ğ‘=1.20\nâ„‹ğ‘=12.24\nFigure 5: Comparison of quantizing attention scores with\ndifferent quantizers under 4-bit setting. The x-axis is the\nattention values before softmax, and the y-axis is (a) full-\nprecision or quantized to 4-bit using (b) Logarithmic quan-\ntizer, (c) Segmental quantizer [47] and (d) MPQ quantizer.\nThe left-top values are mutual information of each distri-\nbution.\nInterestingly, we observe that the quantization behavior of the\nSoftmax function breaks the widespread idea in a counter-intuitive\nway. Softmax is often used as an activation function to stable the\nnetwork since it can control the largest value computed in each\nexponent, which can be written as:\nsoftmax(x)ğ‘– = ğ‘’ğ›½ğ‘¥ğ‘–\nÎ£ğ¾\nğ‘—=1ğ‘’ğ›½ğ‘¥ğ‘—\n,for ğ‘– = 1,...,ğ¾, (12)\nwhere ğ›½ âˆˆR, and usually ğ›½ = 1 in neural networks. It creates\nextreme asymmetric and unbalanced distributions by converting\nto the exponent. Therefore, many methods are devoted to design-\ning specific quantizers for the quantization of Softmax output to\nmaximize the information, such as Segmental quantizers [12, 47],\nLogarithmic quantizers [22, 28] or apply sparsification before quan-\ntization [20]. As shown in Figure 5, the Logarithmic quantizer has\nthe largest 3.82 mutual information. It utilizes up to 11 intervals\n(about 69%) to represent the values clustered in[0,0.01](about 89%),\nwhile only spares 5 fixed-points to map the rest range(0.01,1]. And\nSegmental quantizer also utilizes more bits to cover the small values.\nHowever, consider the original function of Softmax, whenğ›½ > 0,\nthe function will create probability distributions that are more con-\ncentrated around the positions of the largest input values. To put\nit simply, Softmax makes the small values even smaller while the\nlarger values take the most probability, which is a typical phenom-\nenon of the Matthew-effect. This character helps neural networks\nto stable the activation flow. Transformer architecture also adopts\nthe Softmax as activation functions to compute the attention scores\nwhich measures the relationship of one patch in the sequence with\nMM â€™22, October 10â€“14, 2022, Lisboa, Portugal Yifu Ding et al.\nall the other patches. Unfortunately, existing quantizers prioritize\nthe overall mutual information while ignoring the Matthew-effect\nof the Softmax function. For the significant large values, the Loga-\nrithmic and Segmental quantizers only spare fewer bits, thus the\ninformation in the larger values is damaged.\nTherefore we present a Matthew-effect Preserving Quantization\n(MPQ) to quantize the Softmax output. It does not purely pursue\nthe maximization of mutual information before and after quantiza-\ntion, but maintains the Matthew-effect of Softmax output during\nthe quantization process. A typical MPQ is an asymmetric linear\nquantization, which can be expressed as:\nË†xğ‘  = clamp\n\u0012\u0016 softmax(x)\nÎ”\n\u0019\n,0,2ğ‘˜ âˆ’1\n\u0013\n, Î” = max(softmax(x))\n2ğ‘˜ âˆ’1 .\n(13)\nwhere Î” is the scaling factor, and âŒŠÂ·âŒ‰means rounding operation.\nMPQ is a straightforward method that brings no extra implemen-\ntation and inference overhead, and we find that it significantly\nimproves the performance compared with other quantizers. As\nshown in Figure 5, MPQ allocates more bits to the larger range,\nwhere the values are sparse but significant. In this way, the impor-\ntant values are quantized with finer representations, which better\npreserves the function of Softmax.\nAlgorithm 1 The optimization pipeline of APQ-ViT\nInput: Pre-trained vision transformer model and calibration set;\nOutput: Optimal scaling factors âˆ†âˆ—for all layers;\n1: for ğ‘ = 1 : #ğµğ‘™ğ‘œğ‘ğ‘˜ do\n2: Forward propagation to get full-precision ğ‘‚ğ‘ with the com-\nputation process ğ´ğµin each layer and get loss L;\n3: end for\n4: for ğ‘ = 1 : #ğµğ‘™ğ‘œğ‘ğ‘˜ do\n5: Backward propagation to get ğœ•L\nğœ•ğ‘‚ğ‘ ;\n6: end for\n7: for ğ‘ = 1 : #ğµğ‘™ğ‘œğ‘ğ‘˜ do\n8: for ğ‘™ = #ğ¿ğ‘ğ‘¦ğ‘’ğ‘Ÿ : 1 do\n9: initialize scaling factor Î”âˆ—\nğµğ‘\nğ‘™\nâ†\nğµğ‘\nğ‘™ maxâˆ’ğµğ‘\nğ‘™ min\n2ğ‘˜ for ğµğ‘\nğ‘™ ;\n10: for search_round= 1 : #ğ‘…ğ‘œğ‘¢ğ‘›ğ‘‘ do\n11: searching scaling factor Î”âˆ—\nğ´ğ‘\nğ‘™\nfor ğ´ğ‘\nğ‘™ using Eq. (9);\n12: searching scaling factor Î”âˆ—\nğµğ‘\nğ‘™\nfor ğµğ‘\nğ‘™ using Eq. (9);\n13: end for\n14: end for\n15: end for\n16: return Optimal scaling factors âˆ†âˆ—.\n3.4 Framework of APQ-ViT\nWe propose an Accurate Post-training Quantization framework\nfor vision transformer, namely APQ-ViT. The quantization process\nis shown as Algorithm 1. The APQ-ViT mainly depends on two\nnovel techniques: BBC aims to improve the second-order calibra-\ntion metric and MPQ specializes in the Softmax structure. In the\ncalibration process, APQ-ViT first obtains the output and gradi-\nent of each transformer block through a forward and backward\nTable 1: Ablation study of BBC and MPQ.\n#bit(W/A) BBC MPQ ViT-S ViT-B DeiT-B\nFull-precision 81.39 84.54 81.80\n4/4\n42.57 30.69 64.39\n! 42.86 38.40 65.57\n! 46.16 35.44 66.98\n! ! 47.95 41.41 67.48\n6/6\n78.63 81.65 80.25\n! 78.78 81.84 80.33\n! 78.95 82.20 80.38\n! ! 79.10 82.21 80.42\n8/8\n81.00 84.09 81.48\n! 81.01 84.18 81.63\n! 81.15 84.23 81.68\n! ! 81.25 84.26 81.72\nTable 2: Results of different post Softmax quantizers.\n#bit(W/A) Quantizer ViT-S DeiT-S DeiT-B\nFull-precision - 81.39 79.85 81.80\n4/4\nLog 44.72 21.59 60.91\nSegmental 37.70 22.31 60.02\nMPQ 47.95 43.55 67.48\n6/6\nLog 78.94 77.57 80.34\nSegmental 78.67 76.64 80.37\nMPQ 79.10 77.76 80.42\n8/8\nLog 81.13 79.76 81.70\nSegmental 81.00 79.47 81.70\nMPQ 81.25 79.78 81.72\npropagation and then optimizes all transformer layers in a block-\nwise manner with a bottom-elimination second-order metric. And\nMPQ is straightforwardly embedded as a quantizer after Softmax\nfunction in the Matmul operation.\nAs for computation intensity, compared with existing methods,\nthe execution process of APQ-ViT only needs to store the output\nand gradient of each transformer block instead of all the layers,\nwhich greatly reduces the storage footprint required for the entire\nprocess (reduced to about 20%). It allows the process to be performed\nentirely in GPU memory to reduce the speed penalty caused by the\ndata exchange with storage.\n4 EXPERIMENT\nIn this section, we first demonstrate the fundamental pipeline of\npost-training quantization and the experimental settings. We start\nby ablation studies to evaluate the effectiveness of each proposed\napproach. And then We compare with other methods on both image\nclassification and detection tasks with various vision transformer\narchitectures.\nTowards Accurate Post-Training Quantization\nfor Vision Transformer MM â€™22, October 10â€“14, 2022, Lisboa, Portugal\nTable 3: Comparison of different post-training quantization methods on image classification task with various vision trans-\nformer architectures and bit-widths.\nMethod #bit(W/A) ViT-T ViT-S ViT-S/32 ViT-B DeiT-T DeiT-S DeiT-B Swin-S Swin-B Swin-B/384\nFull-precision 32/32 75.47 81.39 75.99 84.54 72.21 79.85 81.80 83.23 85.27 86.44\nFQ-ViT 4/4 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10\nPTQ4ViT 4/4 17.45 42.57 35.09 30.69 36.96 34.08 64.39 76.09 74.02 78.84\nAPQ-ViT (Ours) 4/4 17.56 47.95 41.53 41.41 47.94 43.55 67.48 77.15 76.48 80.84\nFQ-ViT 8/4 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10\nPTQ4ViT 8/4 36.17 63.00 49.68 71.64 48.00 36.08 70.07 80.13 81.45 83.75\nAPQ-ViT (Ours) 8/4 38.62 67.17 64.57 72.47 56.28 41.31 71.69 80.62 82.08 83.87\nFQ-ViT 4/8 27.84 71.17 43.93 78.48 64.42 74.70 79.19 81.17 81.43 82.69\nPTQ4ViT 4/8 59.23 69.68 36.04 67.99 66.57 76.96 79.47 79.62 78.50 82.54\nAPQ-ViT (Ours) 4/8 59.42 72.30 61.81 72.63 66.71 77.14 79.55 80.56 81.94 83.42\nFQ-ViT 6/6 0.38 4.26 2.65 0.10 58.66 45.51 64.63 66.50 52.09 0.10\nPTQ4ViT 6/6 64.46 78.63 71.90 81.65 69.68 76.28 80.25 82.38 84.01 85.44\nAPQ-ViT (Ours) 6/6 69.55 79.10 72.89 82.21 70.49 77.76 80.42 82.67 84.18 85.60\nFQ-ViT 8/8 45.99 78.68 58.87 82.76 70.92 78.44 81.12 82.38 82.38 85.74\nPTQ4ViT 8/8 74.56 81.00 75.58 84.25 71.72 79.47 81.48 83.10 85.14 86.36\nAPQ-ViT (Ours) 8/8 74.79 81.25 75.64 84.26 72.02 79.78 81.72 83.16 85.16 86.40\n4.1 Settings\nPTQ scheme: We first build a post-training quantization baseline\nfor experiments, where we follow the [47] to set the search range\nof weight and activation to [0,1.2]for image classification task and\nfollow [8, 30] and set to [0.5,1.2]for detection task, and evenly\ndivide to ğ‘› = 100 intervals. he default search rounds of the alter-\nnative optimization is 3. We randomly select 32 images from the\nImageNet dataset for classification tasks and only 1 image from\nCOCO dataset for detection tasks. We empirically set ğ›¾ = 10 as\ndefault. The bit-width of the quantized model is marked as Wğ‘¤Ağ‘\nstanding for ğ‘¤-bit weight and ğ‘-bit activation. As for comparison\nmethods, we follow the official settings using the released codes.\nVision Tasks and Network Architectures: To prove the ver-\nsatility of our AFQ-ViT, we evaluate it on image classification and\ndetection tasks. We adopt the most widely-used transformer-based\nnetworks for comparison, including ViT [9], DeiT [41] and Swin\nTransformer [29] for classification task on ImageNet [7] and three\ndifferent scales of Swin Transformer for detection task on COCO\ndataset [27]. Note that we do not quantize the activation in the first\nconvolution layer and the last classification layer. We also keep the\nSoftmax, LayerNorm and GeLU functions as full-precision since\nthey cause little computational overheads, but quantizing them will\ncause a severe accuracy drop.\n4.2 Ablation Study\nWe conduct extensive ablation studies on each proposed method.\nAs Table 1 shows, we evaluate our methods on ViT-S, ViT-B and\nDeiT-B vision transformer architectures. The baseline post-training\nquantization method suffers a severe accuracy loss, especially un-\nder 4-bit setting. While our methods retain the accuracy and the\nadvantage becomes more obvious in lower bit-width. Applying the\nBBC can significantly improve the performance. For example, it\nhelps ViT-B to get 38.40% accuracy in W4A4 which is 7.71% higher\nthan the traditional method. As for the MPQ, we compare it with\nthe Logarithmic quantizer and Segmental quantizer and evaluate\nViT-S, DeiT-S and DeiT-B. As shown in Table 2, MPQ quantizer\noutperforms other quantizers by a wide margin, especially in W4A4.\nDeiT-S equipped with MPQ is 21.96% higher than the Logarithmic\nquantizer and 21.24% higher than the Segmental quantizer. We\nconjecture that it is because in the lower bit-width condition, the\nquantizer spares fewer bits to represent the large magnitude, es-\npecially for Logarithmic and Segmental quantizers, the negative\ninfluence of damaging Matthew-effect becomes manifest. Besides,\nthe phenomena are consistent in 6-/8-bit settings.\nMoreover, jointly applying the proposed methods can further\nimprove the performance, which demonstrates that the BBC opti-\nmization strategy in conjunction with MPQ can work orthogonally.\n4.3 Comparison on Classification Task\nWe first conduct extensive experiments on ImageNet classification\ntasks. We choose different transformer-based architectures, includ-\ning ViT, DeiT and Swin Transformer. The default patch size is 16Ã—16\nand the image resolution is 224Ã—224 if not specifically mentioned\n(ViT-S/32 means the patch size is 32 Ã—32, Swin-B/384 means the\nimage resolution is 384Ã—384).\nWe highlight that APQ-ViT is versatile and has prevailing im-\nprovements over different transformer variants, patch sizes, input\nresolutions and bit-widths. As Table 3 shows, our method shows\nan impressive advantage especially in lower bit-width (i.e., W4A4).\nWe observe that previous methods like FQ-ViT almost crash when\nquantizing the activations to lower than 8-bit, while our APQ-ViT\nimproves the accuracy significantly by up to 5.17% on average com-\npared to PTQ4ViT under W4A4. For some specific models, like\nViT-B, our method even outstrips PTQ4ViT by 10.72%.\nMM â€™22, October 10â€“14, 2022, Lisboa, Portugal Yifu Ding et al.\nTable 4: Comparison of different post-training quantization methods on object detection task under various bit-widths.\nMethod #bit(W/A)\nMask RCNN\nSwin-T\nAPbox APmask\nMask RCNN\nSwin-S\nAPbox APmask\nCascade Mask\nRCNN Swin-T\nAPbox APmask\nCascade Mask\nRCNN Swin-S\nAPbox APmask\nCascade Mask\nRCNN Swin-B\nAPbox APmask\nFull-precision 32/32 46.0 41.6 48.5 43.3 50.4 43.7 51.9 45.0 51.9 45.0\nBasePTQ 4/4 0.9 0.9 12.6 11.8 1.3 1.2 8.4 7.7 4.0 3.7\nPTQ4ViT 4/4 6.9 7.0 26.7 26.6 14.7 13.5 0.5 0.5 10.6 9.3\nAPQ-ViT (Ours) 4/4 23.7 22.6 44.7 40.1 27.2 24.4 47.7 41.1 47.6 41.5\nBasePTQ 8/4 2.2 2.1 23.2 21.4 3.2 3.0 16.3 14.6 7.6 6.6\nPTQ4ViT 8/4 25.5 25.0 18.3 18.0 18.4 16.7 32.7 29.0 14.4 13.1\nAPQ-ViT (Ours) 8/4 33.7 31.6 46.6 41.8 36.6 32.2 49.6 43.4 49.2 43.0\nBasePTQ 4/8 42.4 38.8 45.7 41.0 45.5 40.0 47.4 41.5 47.2 41.6\nPTQ4ViT 4/8 0.7 0.8 23.4 22.2 25.3 22.7 38.5 33.8 20.0 28.4\nAPQ-ViT (Ours) 4/8 43.1 39.3 47.3 42.2 46.2 40.7 49.4 43.0 49.0 42.9\nBasePTQ 6/6 40.1 36.8 46.7 41.8 46.3 40.7 48.9 42.7 46.3 40.7\nPTQ4ViT 6/6 5.8 6.8 6.5 6.6 14.7 13.6 12.5 10.8 14.2 12.9\nAPQ-ViT (Ours) 6/6 45.4 41.2 47.9 42.9 48.6 42.5 50.5 43.9 50.1 43.7\nFQ-ViT 8/8 45.3 41.2 48.2 42.6 49.7 43.3 51.7 44.2 51.1 44.3\nBasePTQ 8/8 45.8 41.5 48.1 42.9 48.6 42.5 50.3 43.8 49.9 43.7\nPTQ4ViT 8/8 28.0 27.1 1.5 1.4 40.3 35.6 20.8 18.7 2.0 1.9\nAPQ-ViT (Ours) 8/8 45.8 41.5 48.3 43.1 48.9 42.7 50.8 44.1 50.2 43.9\nMoreover, under W6A6 and W8A4 settings, the average accuracy\nimprovement of APQ-ViT compared with the previous method is\n1.02% and 3.87%, respectively. Under the W4A8 setting, our method\naccomplishes high accuracy. The average improvement compared\nwith FQ-ViT is 5.05%, and that with PTQ4ViT is 3.88%. It is note-\nworthy that our methods achieve almost loss-less accuracy under\nW8A8. For instance, the DeiT-B and Swin-B/384 models quantized\nby APQ-ViT have only 0.08% and 0.04% accuracy drop, within 0.10%.\nAnd the average accuracy loss of W8A8 compared with the full-\nprecision model is only about 0.23%.\n4.4 Comparison on Object Detection Task\nTo further evaluate the generalization capability of our methods, we\nextend it to object detection tasks using large-scale COCO datasets.\nWe use the Mask RCNN and Cascade Mask RCNN detectors with\nSwin Transformers (Swin-T/S/B) as backbones.\nThe results are presented in Table 4. We highlight that compared\nto classification tasks, quantizing activation to lower-bit,e.g., W4A4\nand W8A4, usually brings more challenges to model robustness and\naccuracy of detection tasks. Models calibrated by previous methods\nalmost crash, while APQ-ViT converges and recovers the accuracy.\nEspecially under the W4A4 setting, the average improvement is a\nremarkable 24.43% and 30.81% over PTQ4ViT and BasePTQ respec-\ntively. For example, our APQ-ViT achieves 44.7% and 40.1% (drops\n3.8% and 3.2%) for APbox and APmask with Cascade Mask RCNN\nSwin-S, while PTQ4ViT methods only get 26.7% and 26.6%.\nFurthermore, APQ-ViT with W6A6 and W4A8 settings also av-\neragely outstrips the BasePTQ by 2.57% and 1.20%, respectively.\nAs for W8A8, APQ-ViT gets comparable performance which only\ndrops 0.18% and 1.20% compared to full-precision counterparts with\nMask RCNN detector and Cascade Mask RCNN detector. It shows\ngreat potential for low-bit quantized detectors to meet the accuracy\nrequirements and be implemented in real-world applications.\nIn a nutshell, APQ-ViT is a versatile method that shows a great\npractical value on both image classification and object detection\ntasks over various bit-width and transformer variants.\n5 CONCLUSION\nIn this paper, we analyze the post-training quantization for vision\ntransformers from optimization and structure perspectives and\npropose a novel method, namely APQ-ViT. We first present a unified\nBottom-elimination Blockwise Calibration scheme which fixes the\noverall quantization error in a blockwise manner and prioritizes\nthe crucial errors that impact more on the final output. Moreover,\nwe design a Matthew-effect Preserving Quantization to maintain\nthe power-law distribution of Softmax and keep the function of the\nattention mechanism. Comprehensive experiments demonstrate\nthat our APQ-ViT achieves prevailing improvements, especially in\nlower bit-width settings (e.g., averagely up to 5.17% improvement\nfor classification and 24.43% for detection on W4A4). We highlight\nthat APQ-ViT is a versatile method that works well with diverse\nvision transformer variants, including DeiT and Swin Transformer.\nACKNOWLEDGEMENT\nThis work was supported by The National Key Research and Devel-\nopment Plan of China (2021ZD0110503), National Natural Science\nFoundation of China (62022009 and 61872021) and Meituan.\nTowards Accurate Post-Training Quantization\nfor Vision Transformer MM â€™22, October 10â€“14, 2022, Lisboa, Portugal\nREFERENCES\n[1] Alham Fikri Aji and Kenneth Heafield. 2017. Sparse communication for dis-\ntributed gradient descent. arXiv preprint arXiv:1704.05021 (2017).\n[2] Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, and Michael R Lyu. 2021.\nTowards efficient post-training quantization of pre-trained language models.\narXiv preprint arXiv:2109.15082 (2021).\n[3] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-\nDocument Transformer. arXiv:2004.05150 (2020).\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexan-\nder Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with\ntransformers. In European conference on computer vision . Springer, 213â€“229.\n[5] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,\nAndreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, et al. 2020. Rethinking attention with performers. arXiv preprint\narXiv:2009.14794 (2020).\n[6] Yoni Choukroun, Eli Kravchik, Fan Yang, and Pavel Kisilev. 2019. Low-bit quanti-\nzation of neural networks for efficient inference. In 2019 IEEE/CVF International\nConference on Computer Vision Workshop (ICCVW) . IEEE, 3009â€“3018.\n[7] Jia Deng, Wei Dong, Richard Socher, Li Jia Li, Kai Li, and Fei Fei Li. 2009. ImageNet:\na Large-Scale Hierarchical Image Database. In IEEE CVPR .\n[8] Wu Di, Tang Qi, Zhao Yongle, Zhang Ming, Zhang Debing, and Fu Ying. 2020.\nEasyQuant: Post-training Quantization via Scale Optimization.\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is\nWorth 16x16 Words: Transformers for Image Recognition at Scale. ICLR (2021).\n[10] Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming transformers for\nhigh-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition . 12873â€“12883.\n[11] Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and\nAndrew Zisserman. 2010. The Pascal Visual Object Classes Challenge. IJCV\n(2010).\n[12] Jun Fang, Ali Shafiee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis,\nand Joseph H Hassoun. 2020. Post-training piecewise linear quantization for deep\nneural networks. In European Conference on Computer Vision . Springer, 69â€“86.\n[13] Ross Girshick. 2015. Fast R-CNN. In IEEE ICCV .\n[14] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2014. Rich\nFeature Hierarchies for Accurate Object Detection and Semantic Segmentation.\nIn IEEE CVPR .\n[15] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand\nJoulin, HervÃ© JÃ©gou, and Matthijs Douze. 2021. LeViT: a Vision Transformer in\nConvNetâ€™s Clothing for Faster Inference. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision . 12259â€“12269.\n[16] Haoyu He, Jing Liu, Zizheng Pan, Jianfei Cai, Jing Zhang, Dacheng Tao, and\nBohan Zhuang. 2021. Pruning Self-attentions into Convolutional Layers in Single\nPath. arXiv preprint arXiv:2111.11802 (2021).\n[17] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. 2021.\nAccurate post training quantization with small calibration sets. In International\nConference on Machine Learning . PMLR, 4466â€“4475.\n[18] Kenneth E Iverson. 1962. A programming language. In Proceedings of the May\n1-3, 1962, spring joint computer conference . 345â€“351.\n[19] Divyansh Jhunjhunwala, Advait Gadhikar, Gauri Joshi, and Yonina C Eldar. 2021.\nAdaptive quantization of model updates for communication-efficient federated\nlearning. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) . IEEE, 3110â€“3114.\n[20] Tianchu Ji, Shraddhan Jain, Michael Ferdman, Peter Milder, H. Andrew Schwartz,\nand Niranjan Balasubramanian. 2021. On the Distribution, Sparsity, and Inference-\ntime Quantization of Attention Values in Transformers. In Findings of the Associ-\nation for Computational Linguistics: ACL-IJCNLP 2021 . Association for Computa-\ntional Linguistics, Online, 4147â€“4157. https://doi.org/10.18653/v1/2021.findings-\nacl.363\n[21] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret.\n2020. Transformers are rnns: Fast autoregressive transformers with linear atten-\ntion. In International Conference on Machine Learning . PMLR, 5156â€“5165.\n[22] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer.\n2021. I-bert: Integer-only bert quantization. InInternational conference on machine\nlearning. PMLR, 5506â€“5518.\n[23] Nikita Kitaev, Åukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient\ntransformer. arXiv preprint arXiv:2001.04451 (2020).\n[24] Krizhevsky, Alex, Sutskever, Ilya, Hinton, and E. Geoffrey. 2017. ImageNet\nClassification with Deep Convolutional Neural Networks.Commun. ACM (2017).\n[25] Manoj Kumar, Dirk Weissenborn, and Nal Kalchbrenner. 2021. Colorization\ntransformer. arXiv preprint arXiv:2102.04432 (2021).\n[26] Rundong Li, Yan Wang, Feng Liang, Hongwei Qin, Junjie Yan, and Rui Fan. 2019.\nFully Quantized Network for Object Detection. In IEEE CVPR .\n[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr DollÃ¡r, and C Lawrence Zitnick. 2014. Microsoft coco: Common\nobjects in context. In European conference on computer vision . Springer, 740â€“755.\n[28] Yang Lin, Tianyu Zhang, Peiqin Sun, Zheng Li, and Shuchang Zhou. 2021. FQ-\nViT: Fully Quantized Vision Transformer without Retraining. arXiv preprint\narXiv:2111.13824 (2021).\n[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. 2021. Swin Transformer: Hierarchical Vision Transformer\nusing Shifted Windows. In 2021 IEEE/CVF International Conference on Computer\nVision (ICCV) . 9992â€“10002. https://doi.org/10.1109/ICCV48922.2021.00986\n[30] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. 2021.\nPost-training quantization for vision transformer.Advances in Neural Information\nProcessing Systems 34 (2021).\n[31] Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, and\nHannaneh Hajishirzi. 2020. DeLighT: Very Deep and Light-weight Transformer.\narXiv:2008.00623 [cs.LG]\n[32] Sachin Mehta and Mohammad Rastegari. 2021. MobileViT: Light-weight, General-\npurpose, and Mobile-friendly Vision Transformer.arXiv preprint arXiv:2110.02178\n(2021).\n[33] Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng, Wanli Ouyang, and Dahua\nLin. 2019. Libra R-CNN: Towards Balanced Learning for Object Detection. In\nIEEE CVPR .\n[34] Tim Prangemeier, Christoph Reich, and Heinz Koeppl. 2020. Attention-based\ntransformers for instance segmentation of cells in microstructures. In 2020 IEEE\nInternational Conference on Bioinformatics and Biomedicine (BIBM) . IEEE, 700â€“\n707.\n[35] RenÃ© Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. 2021. Vision transformers\nfor dense prediction. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision . 12179â€“12188.\n[36] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2016. Faster R-\nCNN: Towards Real-Time Object Detection with Region Proposal Networks.\narXiv:1506.01497 [cs.CV]\n[37] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks\nfor large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).\n[38] Xu Sun, Xuancheng Ren, Shuming Ma, and Houfeng Wang. 2017. meprop: Spar-\nsified back propagation for accelerated deep learning with reduced overfitting.\nIn International Conference on Machine Learning . PMLR, 3299â€“3308.\n[39] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir\nAnguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015.\nGoing Deeper With Convolutions. In IEEE CVPR .\n[40] Shitao Tang, Jiahui Zhang, Siyu Zhu, and Ping Tan. 2022. QuadTree Attention\nfor Vision Transformers. ICLR (2022).\n[41] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre\nSablayrolles, and HervÃ© JÃ©gou. 2021. Training data-efficient image transformers\n& distillation through attention. In International Conference on Machine Learning .\nPMLR, 10347â€“10357.\n[42] Jiakai Wang, Aishan Liu, Zixin Yin, Shunchang Liu, Shiyu Tang, and Xianglong\nLiu. 2021. Dual Attention Suppression Attack: Generate Adversarial Camouflage\nin Physical World. arXiv:2103.01050 [cs.CV]\n[43] Yiru Wang, Weihao Gan, Wei Wu, and Junjie Yan. 2019. Dynamic Curriculum\nLearning for Imbalanced Data Classification. In IEEE ICCV .\n[44] Yiru Wang, Weihao Gan, Jie Yang, Wei Wu, and Junjie Yan. 2019. Dynamic\nCurriculum Learning for Imbalanced Data Classification. In ICCV.\n[45] Yanlu Wei, Renshuai Tao, Zhangjie Wu, Yuqing Ma, Libo Zhang, and Xianglong\nLiu. 2020. Occluded Prohibited Items Detection: An X-Ray Security Inspection\nBenchmark and De-Occlusion Attention Module. In Proceedings of the 28th ACM\nInternational Conference on Multimedia . 138â€“146.\n[46] Xucheng Ye, Pengcheng Dai, Junyu Luo, Xin Guo, Yingjie Qi, Jianlei Yang, and\nYiran Chen. 2020. Accelerating CNN training by pruning activation gradients. In\nEuropean Conference on Computer Vision . Springer, 322â€“338.\n[47] Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, and Guangyu Sun. 2021.\nPTQ4ViT: Post-Training Quantization Framework for Vision Transformers.arXiv\npreprint arXiv:2111.12293 (2021).\n[48] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris\nAlberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\net al. 2020. Big bird: Transformers for longer sequences. Advances in Neural\nInformation Processing Systems 33 (2020), 17283â€“17297.\n[49] Zhiyuan Zhang, Pengcheng Yang, Xuancheng Ren, Qi Su, and Xu Sun. 2020.\nMemorized sparse backpropagation. Neurocomputing 415 (2020), 397â€“407.\n[50] Mingjian Zhu, Kai Han, Yehui Tang, and Yunhe Wang. 2021. Visual transformer\npruning. arXiv e-prints (2021), arXivâ€“2104.\n[51] Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid. 2019.\nStructured Binary Neural Networks for Accurate Image Classification and Se-\nmantic Segmentation. In IEEE CVPR .",
  "topic": "Softmax function",
  "concepts": [
    {
      "name": "Softmax function",
      "score": 0.8244724273681641
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.794002115726471
    },
    {
      "name": "Computer science",
      "score": 0.6416221857070923
    },
    {
      "name": "Transformer",
      "score": 0.5605237483978271
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44227325916290283
    },
    {
      "name": "Algorithm",
      "score": 0.3278183341026306
    },
    {
      "name": "Deep learning",
      "score": 0.16541016101837158
    },
    {
      "name": "Engineering",
      "score": 0.14025193452835083
    },
    {
      "name": "Electrical engineering",
      "score": 0.08285880088806152
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}