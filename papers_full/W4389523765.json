{
  "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents",
  "url": "https://openalex.org/W4389523765",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2152021677",
      "name": "Weiwei Sun",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2971111791",
      "name": "Lingyong Yan",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2099773289",
      "name": "Xinyu Ma",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2166001617",
      "name": "Shuaiqiang Wang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2428878147",
      "name": "Pengjie Ren",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2156756276",
      "name": "Zhumin Chen",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2112665048",
      "name": "Dawei Yin",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2158267603",
      "name": "Zhaochun Ren",
      "affiliations": [
        "Leiden University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3198713493",
    "https://openalex.org/W4318719086",
    "https://openalex.org/W4368755400",
    "https://openalex.org/W2898073868",
    "https://openalex.org/W4364383092",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385565351",
    "https://openalex.org/W3196754070",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3134665270",
    "https://openalex.org/W4381686872",
    "https://openalex.org/W3100107515",
    "https://openalex.org/W2959353218",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4287018294",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W3091432621",
    "https://openalex.org/W4297899309",
    "https://openalex.org/W4389520758",
    "https://openalex.org/W4385889719",
    "https://openalex.org/W4221166196",
    "https://openalex.org/W2143331230",
    "https://openalex.org/W4225644300",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W4293111695",
    "https://openalex.org/W4385573057",
    "https://openalex.org/W4287177420",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W4300427681",
    "https://openalex.org/W4287207977",
    "https://openalex.org/W4221151914",
    "https://openalex.org/W4385571260",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4225631575",
    "https://openalex.org/W4297162632",
    "https://openalex.org/W4205103289",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3175111331"
  ],
  "abstract": "Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model's ability to rank unknown knowledge. Finally, to improve efficiency in real-world applications, we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14918–14937\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nIs ChatGPT Good at Search?\nInvestigating Large Language Models as Re-Ranking Agents\nWeiwei Sun1∗ Lingyong Yan2 Xinyu Ma2 Shuaiqiang Wang2\nPengjie Ren1 Zhumin Chen1 Dawei Yin2† Zhaochun Ren3†\n1Shandong University, Qingdao, China 2Baidu Inc., Beijing, China\n3Leiden University, Leiden, The Netherlands\n{sunnweiwei,lingyongy,xinyuma2016,shqiang.wang}@gmail.com\n{renpengjie,chenzhumin}@sdu.edu.cn yindawei@acm.org\nz.ren@liacs.leidenuniv.nl\nAbstract\nLarge Language Models (LLMs) have demon-\nstrated remarkable zero-shot generalization\nacross various language-related tasks, includ-\ning search engines. However, existing work\nutilizes the generative ability of LLMs for In-\nformation Retrieval (IR) rather than direct pas-\nsage ranking. The discrepancy between the pre-\ntraining objectives of LLMs and the ranking\nobjective poses another challenge. In this pa-\nper, we first investigate generative LLMs such\nas ChatGPT and GPT-4 for relevance ranking\nin IR. Surprisingly, our experiments reveal that\nproperly instructed LLMs can deliver compet-\nitive, even superior results to state-of-the-art\nsupervised methods on popular IR benchmarks.\nFurthermore, to address concerns about data\ncontamination of LLMs, we collect a new test\nset called NovelEval, based on the latest knowl-\nedge and aiming to verify the model’s ability\nto rank unknown knowledge. Finally, to im-\nprove efficiency in real-world applications, we\ndelve into the potential for distilling the rank-\ning capabilities of ChatGPT into small special-\nized models using a permutation distillation\nscheme. Our evaluation results turn out that\na distilled 440M model outperforms a 3B su-\npervised model on the BEIR benchmark. The\ncode to reproduce our results is available at\nwww.github.com/sunnweiwei/RankGPT.\n1 Introduction\nLarge Language Models (LLMs), such as Chat-\nGPT and GPT-4 (OpenAI, 2022, 2023), are revolu-\ntionizing natural language processing with strong\nzero-shot and few-shot generalization. By pre-\ntraining on large-scale text corpora and alignment\nfine-tuning to follow human instructions, LLMs\nhave demonstrated their superior capabilities in lan-\nguage understanding, generation, interaction, and\nreasoning (Ouyang et al., 2022).\n∗Work done during an internship at Baidu.\n†Corresponding authors.\nFigure 1: Average results of ChatGPT and GPT-4\n(zero-shot) on passage re-ranking benchmarks (TREC,\nBEIR, and Mr.TyDi), compared with BM25 and\nprevious best-supervised systems (SOTA sup., e.g.,\nmonoT5 (Nogueira et al., 2020)).\nAs one of the most successful AI applications,\nInformation Retrieval (IR) systems satisfy user re-\nquirements through several pipelined sub-modules,\nsuch as passage retrieval and re-ranking (Lin et al.,\n2020). Most previous methods heavily rely on\nmanual supervision signals, which require signifi-\ncant human effort and demonstrate weak generaliz-\nability (Campos et al., 2016; Izacard et al., 2022).\nTherefore, there is a growing interest in leveraging\nthe zero-shot language understanding and reason-\ning capabilities of LLMs in the IR area. However,\nmost existing approaches primarily focus on ex-\nploiting LLMs for content generation (e.g., query\nor passage) rather than relevance ranking for groups\nof passages (Yu et al., 2023; Microsoft, 2023).\nCompared to the common generation settings,\nthe objectives of relevance re-ranking vary signifi-\ncantly from those of LLMs: the re-ranking agents\nneed to comprehend user requirements, globally\ncompare, and rank the passages based on their rele-\nvance to queries. Therefore, leveraging the LLMs’\ncapabilities for passage re-ranking remains a chal-\nlenging and unanswered task.\nTo this end, we focus on the following questions:\n14918\n• (RQ1) How does ChatGPT perform on pas-\nsage re-ranking tasks?\n• (RQ2) How can we imitate the ranking capa-\nbilities of ChatGPT in a smaller, specialized\nmodel?\nTo answer the first question, we investigate\nprompting ChatGPT with two existing strate-\ngies (Sachan et al., 2022; Liang et al., 2022).\nHowever, we observe that they have limited per-\nformance and heavily rely on the availability of\nthe log-probability of model output. Thus, we\npropose an alternative instructional permutation\ngeneration approach, instructing the LLMs to di-\nrectly output the permutations of a group of pas-\nsages. In addition, we propose an effective sliding\nwindow strategy to address context length limita-\ntions. For a comprehensive evaluation of LLMs,\nwe employ three well-established IR benchmarks:\nTREC (Craswell et al., 2020), BEIR (Thakur et al.,\n2021), and My.TyDi (Zhang et al., 2021). Further-\nmore, to assess the LLMs on unknown knowledge\nand address concerns of data contamination, we\nsuggest collecting a continuously updated evalua-\ntion testbed and propose NovelEval, a new test set\nwith 21 novel questions.\nTo answer the second question, we introduce\na permutation distillation technique to imitate\nthe passage ranking capabilities of ChatGPT in a\nsmaller, specialized ranking model. Specifically,\nwe randomly sample 10K queries from the MS\nMARCO training set, and each query is retrieved\nby BM25 with 20 candidate passages. On this\nbasis, we distill the permutation predicted by Chat-\nGPT into a student model using a RankNet-based\ndistillation objective (Burges et al., 2005).\nOur evaluation results demonstrate that GPT-4,\nequipped with zero-shot instructional permutation\ngeneration, surpasses supervised systems across\nnearly all datasets. Figure 1 illustrates that GPT-4\noutperforms the previous state-of-the-art models\nby an average nDCG improvement of 2.7, 2.3, and\n2.7 on TREC, BEIR, and My.TyDi, respectively.\nFurthermore, GPT-4 achieves state-of-the-art per-\nformance on the new NovelEval test set. Through\nour permutation distillation experiments, we ob-\nserve that a 435M student model outperforms the\nprevious state-of-the-art monoT5 (3B) model by an\naverage nDCG improvement of 1.67 on BEIR. Ad-\nditionally, the proposed distillation method demon-\nstrates cost-efficiency benefits.\nIn summary, our contributions are tri-fold:\n• We examine instructional methods for LLMs\non passage re-ranking tasks and introduce a\nnovel permutation generation approach; See\nSection 3 for details.\n• We comprehensively evaluate ChatGPT and\nGPT-4 on various passage re-ranking bench-\nmarks, including a newly proposed NovelEval\ntest set; See Section 5 for details.\n• We propose a distillation approach for learn-\ning specialized models with the permutation\ngenerated by ChatGPT; See Section 4 for de-\ntails.\n2 Related Work\n2.1 Information Retrieval with LLMs\nRecently, large language models (LLMs) have\nfound increasing applications in information re-\ntrieval (Zhu et al., 2023). Several approaches have\nbeen proposed to utilize LLMs for passage retrieval.\nFor example, SGPT (Muennighoff, 2022) generates\ntext embeddings using GPT, generative document\nretrieval explores a differentiable search index (Tay\net al., 2022; Cao et al., 2021; Sun et al., 2023),\nand HyDE (Gao et al., 2023; Wang et al., 2023a)\ngenerates pseudo-documents using GPT-3. In ad-\ndition, LLMs have also been used for passage re-\nranking tasks. UPR (Sachan et al., 2022) and SGPT-\nCE (Muennighoff, 2022) introduce instructional\nquery generation methods, while HELM (Liang\net al., 2022) utilizes instructional relevance gener-\nation. LLMs are also employed for training data\ngeneration. InPars (Bonifacio et al., 2022) gener-\nates pseudo-queries using GPT-3, and Promptaga-\ntor (Dai et al., 2023) proposes a few-shot dense\nretrieval to leverage a few demonstrations from the\ntarget domain for pseudo-query generation. Fur-\nthermore, LLMs have been used for content gener-\nation (Yu et al., 2023) and web browsing (Nakano\net al., 2021; Izacard et al., 2023; Microsoft, 2023).\nIn this paper, we explore using ChatGPT and GPT-\n4 in passage re-ranking tasks, propose an instruc-\ntional permutation generation method, and conduct\na comprehensive evaluation of benchmarks from\nvarious domains, tasks, and languages. Recent\nwork (Ma et al., 2023) concurrently investigated\nlistwise passage re-ranking using LLMs. In com-\nparison, our study provides a more comprehensive\nevaluation, incorporating a newly annotated dataset,\nand validates the proposed permutation distillation\ntechnique.\n14919\nFigure 2: Three types of instructions for zero-shot pas-\nsage re-ranking with LLMs. The gray and yellow blocks\nindicate the inputs and outputs of the model. (a) Query\ngeneration relies on the log probability of LLMs to gen-\nerate the query based on the passage. (b) Relevance\ngeneration instructs LLMs to output relevance judg-\nments. (c) Permutation generation generates a ranked\nlist of a group of passages. See Appendix A for details.\n2.2 LLMs Specialization\nDespite their impressive capabilities, LLMs such as\nGPT-4 often come with high costs and lack open-\nsource availability. As a result, considerable re-\nsearch has explored ways to distill the capabilities\nof LLMs into specialized, custom models. For in-\nstance, Fu et al. (2023) and Magister et al. (2023)\nhave successfully distilled the reasoning ability of\nLLMs into smaller models. Self-instruct (Wang\net al., 2023b; Taori et al., 2023) propose iterative\napproaches to distill GPT-3 using their outputs. Ad-\nditionally, Sachan et al. (2023) and Shi et al. (2023)\nutilize the generation probability of LLMs to im-\nprove retrieval systems. This paper presents a per-\nmutation distillation method that leverages Chat-\nGPT as a teacher to obtain specialized re-ranking\nmodels. Our experiments demonstrate that even\nwith a small amount of ChatGPT-generated data,\nthe specialized model can outperform strong super-\nvised systems.\n3 Passage Re-Ranking with LLMs\nRanking is the core task in information retrieval ap-\nplications, such as ad-hoc search (Lin et al., 2020;\nFan et al., 2022), Web search (Zou et al., 2021),\nand open-domain question answering (Karpukhin\net al., 2020). Modern IR systems generally employ\na multi-stage pipeline where the retrieval stage fo-\ncuses on retrieving a set of candidates from a large\nFigure 3: Illustration of re-ranking 8 passages using\nsliding windows with a window size of 4 and a step size\nof 2. The blue color represents the first two windows,\nwhile the yellow color represents the last window. The\nsliding windows are applied in back-to-first order, mean-\ning that the first 2 passages in the previous window will\nparticipate in re-ranking the next window.\ncorpus, and the re-ranking stage aims to re-rank\nthis set to output a more precise list. Recent stud-\nies have explored LLMs for zero-shot re-ranking,\nsuch as instructional query generation or relevance\ngeneration (Sachan et al., 2022; Liang et al., 2022).\nHowever, existing methods have limited perfor-\nmance in re-ranking and heavily rely on the avail-\nability of the log probability of model output and\nthus cannot be applied to the latest LLMs such as\nGPT-4. Since ChatGPT and GPT-4 have a strong\ncapacity for text understanding, instruction follow-\ning, and reasoning, we introduce a novel instruc-\ntional permutation generation method with a slid-\ning window strategy to directly output a ranked\nlist given a set of candidate passages. Figure 2\nillustrates examples of three types of instructions;\nall the detailed instructions are included in Ap-\npendix A.\n3.1 Instructional Permutation Generation\nAs illustrated in Figure 2 (c), our approach involves\ninputting a group of passages into the LLMs, each\nidentified by a unique identifier (e.g., [1], [2],\netc.). We then ask the LLMs to generate the per-\nmutation of passages in descending order based\non their relevance to the query. The passages are\nranked using the identifiers, in a format such as[2]\n> [3] > [1] > etc. The proposed method ranks pas-\nsages directly without producing an intermediate\nrelevance score.\n3.2 Sliding Window Strategy\nDue to the token limitations of LLMs, we can only\nrank a limited number of passages using the per-\nmutation generation approach. To overcome this\nconstraint, we propose a sliding window strategy.\nFigure 3 illustrates an example of re-ranking 8 pas-\n14920\nsages using a sliding window. Suppose the first-\nstage retrieval model returns M passages. We re-\nrank these passages in a back-to-first order using a\nsliding window. This strategy involves two hyper-\nparameters: window size (w) and step size (s). We\nfirst use the LLMs to rank the passages from the\n(M −w)-th to the M-th. Then, we slide the win-\ndow in steps of sand re-rank the passages within\nthe range from the (M−w−s)-th to the (M−s)-\nth. This process is repeated until all passages have\nbeen re-ranked.\n4 Specialization by Permutation\nDistillation\nAlthough ChatGPT and GPT-4 are highly capable,\nthey are also too expensive to deploy in commercial\nsearch systems. Using GPT-4 to re-rank passages\nwill greatly increase the latency of the search sys-\ntem. In addition, large language models suffer from\nthe problem of unstable generation. Therefore, we\nargue that the capabilities of large language models\nare redundant for the re-ranking task. Thus, we can\ndistill the re-ranking capability of large language\nmodels into a small model by specialization.\n4.1 Permutation Distillation\nIn this paper, we present a novel permutation dis-\ntillation method that aims to distill the passage re-\nranking capability of ChatGPT into a specialized\nmodel. The key difference between our approach\nand previous distillation methods is that we directly\nuse the model-generated permutation as the tar-\nget, without introducing any inductive bias such as\nconsistency-checking or log-probability manipula-\ntion (Bonifacio et al., 2022; Sachan et al., 2023).\nTo achieve this, we sample 10,000 queries from\nMS MARCO and retrieve 20 candidate passages\nusing BM25 for each query. The objective of distil-\nlation aims to reduce the differences between the\npermutation outputs of the student and ChatGPT.\n4.2 Training Objective\nFormally, suppose we have a query qand M pas-\nsages (p1,...,p M) retrieved by BM25 (M = 20\nin our implementation). ChatGPT with instruc-\ntional permutation generation could produce the\nranking results of the M passages, denoted as\nR = ( r1,...,r M), where ri ∈[1,2,...,M ] is\nthe rank of the passage pi. For example, ri = 3\nmeans pi ranks third among the M passages. Now\nwe have a specialized model si = fθ(q,pi) with\nparameters θto calculate the relevance score si of\npaired (q,pi) using a cross-encoder. Using the per-\nmutation R generated by ChatGPT, we consider\nRankNet loss (Burges et al., 2005) to optimize the\nstudent model:\nLRankNet =\nM∑\ni=1\nM∑\nj=1\n1ri<rj log(1 + exp(si −sj))\nRankNet is a pairwise loss that measures the cor-\nrectness of relative passage orders. When using\npermutations generated by ChatGPT, we can con-\nstruct M(M −1)/2 pairs.\n4.3 Specialized Model Architecture\nRegarding the architecture of the specialized model,\nwe consider two model structures: the BERT-like\nmodel and the GPT-like model.\n4.3.1 BERT-like model.\nWe utilize a cross-encoder model (Nogueira and\nCho, 2019) based on DeBERTa-large. It concate-\nnates the query and passage with a [SEP] token\nand estimates relevance using the representation of\nthe [CLS] token.\n4.3.2 GPT-like model.\nWe utilize the LLaMA-7B (Touvron et al., 2023)\nwith a zero-shot relevance generation instruction\n(see Appendix A). It classifies the query and pas-\nsage as relevance or irrelevance by generating a\nrelevance token. The relevance score is then de-\nfined as the generation probability of the relevance\ntoken.\nFigure 5 illustrates the structure of the two types\nof specialized models.\n5 Datasets\nOur experiments are conducted on three benchmark\ndatasets and one newly collected test set NovelEval.\n5.1 Benchmark Datasets\nThe benchmark datasets include, TREC-\nDL (Craswell et al., 2020), BEIR (Thakur\net al., 2021), and Mr.TyDi (Zhang et al., 2021).\nTREC is a widely used benchmark dataset in IR\nresearch. We use the test sets of the 2019 and 2020\ncompetitions: (i) TREC-DL19 contains 43 queries,\n(ii) TREC-DL20 contains 54 queries.\nBEIR consists of diverse retrieval tasks and do-\nmains. We choose eight tasks in BEIR to evaluate\nthe models: (i) Covid: retrieves scientific articles\n14921\nfor COVID-19 related questions. (ii) NFCorpus is\na bio-medical IR data. (iii) Touche is an argument\nretrieval datasets. (iv) DBPedia retrieves entities\nfrom DBpedia corpus. (v) SciFact retrieves evi-\ndence for claims verification. (vi) Signal retrieves\nrelevant tweets for a given news title. (vii) News\nretrieves relevant news articles for news headlines.\n(viii) Robust04 evaluates poorly performing topics.\nMr.TyDiis a multilingual passages retrieval dataset\nof ten low-resource languages: Arabic, Bengali,\nFinnish, Indonesian, Japanese, Korean, Russian,\nSwahili, Telugu, and Thai. We use the first 100\nsamples in the test set of each language.\n5.2 A New Test Set – NovelEval\nThe questions in the current benchmark dataset\nare typically gathered years ago, which raises the\nissue that existing LLMs already possess knowl-\nedge of these questions (Yu et al., 2023). Further-\nmore, since many LLMs do not disclose informa-\ntion about their training data, there is a potential\nrisk of contamination of the existing benchmark\ntest set (OpenAI, 2023). However, re-ranking\nmodels are expected to possess the capability to\ncomprehend, deduce, and rank knowledge that is\ninherently unknown to them. Therefore, we sug-\ngest constructing continuously updated IR test sets\nto ensure that the questions, passages to be ranked,\nand relevance annotations have not been learned by\nthe latest LLMs for a fair evaluation.\nAs an initial effort, we built NovelEval-2306, a\nnovel test set with 21 novel questions. This test\nset is constructed by gathering questions and pas-\nsages from 4 domains that were published after the\nrelease of GPT-4. To ensure that GPT-4 did not\npossess prior knowledge of these questions, we pre-\nsented them to both gpt-4-0314 and gpt-4-0613.\nFor instance, question \"Which film was the 2023\nPalme d’Or winner?\" pertains to the Cannes Film\nFestival that took place on May 27, 2023, render-\ning its answer inaccessible to most existing LLMs.\nNext, we searched 20 candidate passages for each\nquestion using Google search. The relevance of\nthese passages was manually labeled as: 0 for not\nrelevant, 1 for partially relevant, and 2 for relevant.\nSee Appendix C for more details.\n6 Experimental Results of LLMs\n6.1 Implementation and Metrics\nIn benchmark datasets, we re-rank the top-100 pas-\nsages retrieved by BM25 using pyserini1 and use\nnDCG@{1, 5,10} as evaluation metrics. Since\nChatGPT cannot manage 100 passages at a time,\nwe use the sliding window strategy introduced in\nSection 3.2 with a window size of 20 and step\nsize of 10. In NovelEval, we randomly shuffled\nthe 20 candidate passages searched by Google and\nre-ranked them using ChatGPT and GPT-4 with\npermutation generation.\n6.2 Results on Benchmarks\nOn benchmarks, we compare ChatGPT and GPT-4\nwith state-of-the-art supervised and unsupervised\npassage re-ranking methods. The supervised base-\nlines include: monoBERT (Nogueira and Cho,\n2019), monoT5 (Nogueira et al., 2020), mmar-\ncoCE (Bonifacio et al., 2021), and Cohere Rerank2.\nThe unsupervised baselines include: UPR (Sachan\net al., 2022), InPars (Bonifacio et al., 2022), and\nPromptagator++ (Dai et al., 2023). See Appendix E\nfor more details on implementing the baseline.\nTable 1 presents the evaluation results obtained\nfrom the TREC and BEIR datasets. The follow-\ning observations can be made: (i) GPT-4, when\nequipped with the permutation generation instruc-\ntion, demonstrates superior performance on both\ndatasets. Notably, GPT-4 achieves an average im-\nprovement of 2.7 and 2.3 in nDCG@10 on TREC\nand BEIR, respectively, compared to monoT5 (3B).\n(ii) ChatGPT also exhibits impressive results on\nthe BEIR dataset, surpassing the majority of super-\nvised baselines. (iii) On BEIR, we use only GPT-4\nto re-rank the top-30 passages re-ranked by Chat-\nGPT. The method achieves good results, while the\ncost is only 1/5 of that of only using GPT-4 for\nre-ranking.\nTable 2 illustrates the results on Mr. TyDi of ten\nlow-resource languages. Overall, GPT-4 outper-\nforms the supervised system in most languages,\ndemonstrating an average improvement of 2.65\nnDCG over mmarcoCE. However, there are in-\nstances where GPT-4 performs worse than mmar-\ncoCE, particularly in low-resource languages like\nBengali, Telugu, and Thai. This may be attributed\nto the weaker language modeling ability of GPT-4\n1https://github.com/castorini/pyserini\n2https://txt.cohere.com/rerank/\n14922\nMethod DL19 DL20 Covid NFCorpus Touche DBPedia SciFact Signal News Robust04 BEIR (Avg)\nBM25 50.58 47.96 59.47 30.75 44.22 31.80 67.89 33.05 39.52 40.70 43.42\nSupervised\nmonoBERT (340M) 70.50 67.28 70.01 36.88 31.75 41.87 71.36 31.44 44.62 49.35 47.16\nmonoT5 (220M) 71.48 66.99 78.34 37.38 30.82 42.42 73.40 31.67 46.83 51.72 49.07\nmonoT5 (3B) 71.83 68.89 80.71 38.97 32.41 44.45 76.57 32.55 48.49 56.71 51.36\nCohere Rerank-v2 73.22 67.08 81.81 36.36 32.51 42.51 74.44 29.60 47.59 50.78 49.45\nUnsupervised\nUPR (FLAN-T5-XL) 53.85 56.02 68.11 35.04 19.69 30.91 72.69 31.91 43.11 42.43 42.99\nInPars (monoT5-3B) - 66.12 78.35 - - - - - - - -\nPromptagator++ (few-shot) - - 76.2 37.0 38.1 43.4 73.1 - - - -\nLLM API (Permutation generation)\ngpt-3.5-turbo 65.80 62.91 76.67 35.62 36.18 44.47 70.43 32.12 48.85 50.62 49.37\ngpt-4† 75.59 70.56 85.51 38.47 38.57 47.12 74.95 34.40 52.89 57.55 53.68\nTable 1: Results (nDCG@10) on TREC and BEIR. Best performing unsupervised and overall system(s) are\nmarked bold. All models except InPars and Promptagator++ re-rank the same BM25 top-100 passages. †On BEIR,\nwe use gpt-4 to re-rank the top-30 passages re-ranked by gpt-3.5-turbo to reduce the cost of calling gpt-4 API.\nMethod BM25 mmarcoCE gpt-3.5 +gpt-4\nArabic 39.19 68.18 71.00 72.56\nBengali 45.56 65.98 53.10 64.37\nFinnish 29.91 54.15 56.48 62.29\nIndonesian 51.79 69.94 68.45 75.47\nJapanese 27.39 49.80 50.70 58.22\nKorean 26.29 44.00 41.48 49.63\nRussian 34.04 53.16 48.75 53.45\nSwahili 45.15 60.31 62.38 67.67\nTelugu 37.05 68.92 51.69 62.22\nThai 44.62 68.36 55.57 63.41\nAvg 38.10 60.28 55.96 62.93\nTable 2: Results (nDCG@10) on Mr.TyDi.\nin these languages and the fact that text in low-\nresource languages tends to consume more tokens\nthan English text, leading to the over-cropping of\npassages. Similar trends are observed with Chat-\nGPT, which is on par with the supervised system\nin most languages, and consistently trails behind\nGPT-4 in all languages.\n6.3 Results on NovelEval\nTable 3 illustrates the evaluation results on our\nnewly collected NovelEval, a test set containing 21\nnovel questions and 420 passages that GPT-4 had\nnot learned. The results show that GPT-4 performs\nwell on these questions, significantly outperform-\ning the previous best-supervised method, monoT5\n(3B). Additionally, ChatGPT achieves a perfor-\nmance level comparable to that of monoBERT. This\noutcome implies that LLMs possess the capability\nto effectively re-rank unfamiliar information.\nMethod nDCG@1 nDCG@5 nDCG@10\nBM25 33.33 45.96 55.77\nmonoBERT (340M) 78.57 70.65 77.27\nmonoT5 (220M) 83.33 77.46 81.27\nmonoT5 (3B) 83.33 78.38 84.62\ngpt-3.5-turbo 76.19 74.15 75.71\ngpt-4 85.71 87.49 90.45\nTable 3: Results on NovelEval.\nDL19 DL20\nMethod nDCG@1/5/10 nDCG@1/5/10\ncurie-001 RG 39.53 / 40.02 / 41.53 41.98 / 34.80 / 34.91\ncurie-001 QG 50.78 / 50.77 / 49.76 50.00 / 48.36 / 48.73\ncurie-001 PG 66.67 / 56.79 / 54.21 59.57 / 55.20 / 52.17\ndavinci-003 RG 54.26 / 52.78 / 50.58 64.20 / 58.41 / 56.87\ndavinci-003 QG 37.60 / 44.73 / 45.37 51.25 / 47.46 / 45.93\ndavinci-003 PG 69.77 / 64.73 / 61.50 69.75 / 58.76 / 57.05\ngpt-3.5 PG 82.17 / 71.15 / 65.80 79.32 / 66.76 / 62.91\ngpt-4 PG 82.56 / 79.16 / 75.59 78.40 / 74.11 / 70.56\nTable 4: Compare different instruction and API end-\npoint. Best performing system(s) are marked bold. PG,\nQG, RG denote permutation generation, query genera-\ntion and relevance generation, respectively.\n6.4 Compare with Different Instructions\nWe conduct a comparison with the proposed permu-\ntation generation (PG) with previous query genera-\ntion (QG) (Sachan et al., 2022) and relevance gen-\neration (RG) (Liang et al., 2022) on TREC-DL19.\nAn example of the three types of instructions is\nin Figure 2, and the detailed implementation is in\nAppendix B. We also compare four LLMs provided\n14923\nMethod nDCG@1 nDCG@5 nDCG@10\nBM25 54.26 52.78 50.58\ngpt-3.5-turbo 82.17 71.15 65.80\nInitial passage order\n(1) Random order 26.36 25.32 25.17\n(2) Reverse order 36.43 31.79 32.77\nNumber of re-ranking\n(3) Re-rank 2 times 78.29 69.37 66.62\n(4) Re-rank 3 times 78.29 69.74 66.97\n(5) gpt-4 Rerank 80.23 76.70 73.64\nTable 5: Ablation study on TREC-DL19. We use\ngpt-3.5-turbo with permutation generation with dif-\nferent configuration.\nin the OpenAI API 3: curie-001 - GPT-3 model\nwith about 6.7 billion parameters (Brown et al.,\n2020); davinci-003 - GPT-3.5 model trained with\nRLHF and about 175 billion parameters (Ouyang\net al., 2022); gpt-3.5-turbo - The underlying\nmodel of ChatGPT (OpenAI, 2022); gpt-4 - GPT-\n4 model (OpenAI, 2023).\nThe results are listed in Table 4. From the\nresults, we can see that: (i) The proposed PG\nmethod outperforms both QG and RG methods\nin instructing LLMs to re-rank passages. We\nsuggest two explanations: First, from the result\nthat PG has significantly higher top-1 accuracy\ncompared to other methods, we infer that LLMs\ncan explicitly compare multiple passages with PG,\nallowing subtle differences between passages to\nbe discerned. Second, LLMs gain a more com-\nprehensive understanding of the query and pas-\nsages by reading multiple passages with potentially\ncomplementary information, thus improving the\nmodel’s ranking ability. (ii) With PG, ChatGPT\nperforms comparably to GPT-4 on nDCG@1, but\nlags behind it on nDCG@10. The Davinci model\n(text-davinci-003) performs poorly compared\nto ChatGPT and GPT-4. This may be because of\nthe generation stability of Davinci and ChatGPT\ntrails that of GPT-4. We delve into the stability\nanalysis of Davinci, ChatGPT, and GPT-4 in Ap-\npendix F.\n6.5 Ablation Study on TREC\nWe conducted an ablation study on TREC to gain\ninsights into the detailed configuration of permuta-\ntion generation. Table 5 illustrates the results.\n3https://platform.openai.com/docs/\napi-reference\nInitial Passage Order While our standard imple-\nmentation utilizes the ranking result of BM25 as\nthe initial order, we examined two alternative vari-\nants: random order (1) and reversed BM25 order\n(2). The results reveal that the model’s performance\nis highly sensitive to the initial passage order. This\ncould be because BM25 provides a relatively good\nstarting passage order, enabling satisfactory results\nwith only a single sliding window re-ranking.\nNumber of Re-Ranking Furthermore, we stud-\nied the influence of the number of sliding window\npasses. Models (3-4) in Table 5 show that re-\nranking more times may improve nDCG@10, but\nit somehow hurts the ranking performance on top\npassages (e.g., nDCG@1 decreased by 3.88). Re-\nranking the top 30 passages using GPT-4 showed\nnotable accuracy improvements (see the model (5)).\nThis provides an alternative method to combine\nChatGPT and GPT-4 in passage re-ranking to re-\nduce the high cost of using the GPT-4 model.\n6.6 Results of LLMs beyond ChatGPT\nWe further test the capabilities of other LLMs be-\nyond the OpenAI series on TREC DL-19. As\nshown in Table 6, we evaluate the top-20 BM25 pas-\nsage re-ranking nDCG of proprietary LLMs from\nOpenAI, Cohere, Antropic, and Google, and three\nopen-source LLMs. We see that: (i) Among the\nproprietary LLMs, GPT-4 exhibited the highest re-\nranking performance. Cohere Re-rank also showed\npromising results; however, it should be noted that\nit is a supervised specialized model. In contrast,\nthe proprietary models from Antropic and Google\nfell behind ChatGPT in terms of re-ranking effec-\ntiveness. (ii) As for the open-source LLMs, we ob-\nserved a significant performance gap compared to\nChatGPT. One possible reason for this discrepancy\ncould be the complexity involved in generating per-\nmutations of 20 passages, which seems to pose a\nchallenge for the existing open-source models.\nWe analyze the model’s unexpected behavior in\nAppendix F, and the cost of API in Appendix H.\n7 Experimental Results of Specialization\nAs mentioned in Section 4, we randomly sample\n10K queries from the MSMARCO training set and\nemploy the proposed permutation distillation to dis-\ntill ChatGPT’s predicted permutation into special-\nized re-ranking models. The specialized re-ranking\nmodels could be DeBERTa-v3-Large with a cross-\nencoder architecture or LLaMA-7B with relevance\n14924\nFigure 4: Scaling experiment. The dashed line indicates the baseline methods: GPT-4, monoT5, monoBERT, and\nChatGPT. The solid green line and solid gray line indicate the specialized Deberta models obtained by the proposed\npermutation distillation and by supervised learning on MS MARCO, respectively. This figure compares the models’\nperformance on TREC and BEIR across varying model sizes (70M to 435M) and training data sizes (500 to 10K).\nMethod ND1 ND5 ND10\nOpenAI text-davinci-003 70.54 61.90 57.24\nOpenAI gpt-3.5-turbo 75.58 66.19 60.89\nOpenAI gpt-4 79.46 71.65 65.68\nCohere rerank-english-v2.0 79.46 71.56 64.78\nAntropic claude-2 66.66 59.33 55.91\nAntropic claude-instant-1 81.01 66.71 62.23\nGoogle text-bison-001 69.77 64.46 58.67\nGoogle bard-2023.10.21 81.01 65.57 60.11\nGoogle flan-t5-xxl 52.71 51.63 50.26\nTsinghua ChatGLM-6B 54.26 52.77 50.58\nLMSYS Vicuna-13B 54.26 51.55 49.08\nTable 6: Results of different LLMs on re-ranking\ntop-20 passages on DL-19. ND{1,5,10} denote\nnDCG@{1,5,10}, respectively.\nLabel Method DL19 DL20 BEIR (Avg)\n∅ BM25 50.58 47.96 43.42\n∅ ChatGPT 65.80 62.91 49.37\nMARCO monoT5 (3B) 71.83 68.89 51.36\nMARCO DeBERTa-Large 68.89 61.38 42.64\nMARCO LLaMA-7B 69.24 58.97 47.71\nChatGPT DeBERTa-Large 70.66 67.15 53.03\nChatGPT LLaMA-7B 71.78 66.89 51.68\nTable 7: Results (nDCG@10) of specialized mod-\nels. Best performing specialized model(s) are marked\nbold. The label column denotes the relevance judge-\nments used in model training, where MARCO denotes\nuse MS MARCO annotation, ChatGPT denotes use the\noutputs of permutation generation instructed ChatGPT\nas labels. BEIR (Avg) denotes average nDCG on eight\nBEIR datasets, and the detailed results are at Table 13.\ngeneration instructions. We also implemented the\nspecialized model trained using the original MS\nMARCO labels (aka supervised learning) for com-\nparison4.\n7.1 Results on Benchmarks\nTable 7 lists the results of specialized models, and\nTable 13 includes the detailed results. Our findings\ncan be summarized as follows: (i) Permutation dis-\ntillation outperforms the supervised counterpart on\nboth TREC and BEIR datasets, potentially because\nChatGPT’s relevance judgments are more compre-\nhensive than MS MARCO labels (Arabzadeh et al.,\n2021). (ii) The specialized DeBERTa model outper-\nforms previous state-of-the-art (SOTA) baselines,\nmonoT5 (3B), on BEIR with an average nDCG of\n53.03. This result highlights the potential of dis-\ntilling LLMs for IR since it is significantly more\ncost-efficient. (iii) The distilled specialized model\nalso surpasses ChatGPT, its teacher model, on both\ndatasets. This is probably because the re-rank-\ning stability of specialized models is better than\nChatGPT. As shown in the stability analysis in Ap-\npendix F, ChatGPT is very unstable in generating\npermutations.\n7.2 Analysis on Model Size and Data Size\nIn Figure 4, we present the re-ranking performance\nof specialized DeBERTa models obtained through\npermutation distillation and supervised learning of\ndifferent model sizes (ranging from 70M to 435M)\nand training data sizes (ranging from 500 to 10K).\nOur findings indicate that the permutation-distilled\nmodels consistently outperform their supervised\ncounterparts across all settings, particularly on the\nBEIR datasets. Notably, even with only 1K training\nqueries, the permutation-distilled DeBERTa model\n4Note that all models are trained using the RankNet loss\nfor a fair comparison.\n14925\nachieves superior performance compared to the\nprevious state-of-the-art monoT5 (3B) model on\nBEIR. We also observe that increasing the number\nof model parameters yields a greater improvement\nin the ranking results than increasing the training\ndata. Finally, we find that the performance of super-\nvised models is unstable for different model sizes\nand data sizes. This may be due to the presence\nof noise in the MS MARCO labels, which leads to\noverfitting problems (Arabzadeh et al., 2021).\n8 Conclusion\nIn this paper, we conduct a comprehensive study\non passage re-ranking with LLMs. We introduce\na novel permutation generation approach to fully\nexplore the power of LLMs. Our experiments on\nthree benchmarks have demonstrated the capability\nof ChatGPT and GPT-4 in passage re-ranking. To\nfurther validate LLMs on unfamiliar knowledge,\nwe introduce a new test set called NovelEval. Ad-\nditionally, we propose a permutation distillation\nmethod, which demonstrates superior effectiveness\nand efficiency compared to existing supervised ap-\nproaches.\nLimitations\nThe limitations of this work include the main\nanalysis for OpenAI ChatGPT and GPT-4, which\nare proprietary models that are not open-source.\nAlthough we also tested on open-source models\nsuch as FLAN-T5, ChatGLM-6B, and Vicuna-13B,\nthe results still differ significantly from ChatGPT.\nHow to further exploit the open-source models\nis a question worth exploring. Additionally, this\nstudy solely focuses on examining LLMs in the\nre-ranking task. Consequently, the upper bound of\nthe ranking effect is contingent upon the recall of\nthe initial passage retrieval. Our findings also in-\ndicate that the re-ranking effect of LLMs is highly\nsensitive to the initial order of passages, which\nis usually determined by the first-stage retrieval,\nsuch as BM25. Therefore, there is a need for fur-\nther exploration into effectively utilizing LLMs to\nenhance the first-stage retrieval and improve the ro-\nbustness of LLMs in relation to the initial passage\nretrieval.\nEthics Statement\nWe acknowledge the importance of the ACM Code\nof Ethics and totally agree with it. We ensure that\nthis work is compatible with the provided code,\nin terms of publicly accessed datasets and models.\nRisks and harms of large language models include\nthe generation of harmful, offensive, or biased con-\ntent. These models are often prone to generating\nincorrect information, sometimes referred to as hal-\nlucinations. We do not expect the studied model to\nbe an exception in this regard. The LLMs used in\nthis paper were shown to suffer from bias, halluci-\nnation, and other problems. Therefore, we are not\nrecommending the use of LLMs for ranking tasks\nwith social implications, such as ranking job can-\ndidates or ranking products, because LLMs may\nexhibit racial bias, geographical bias, gender bias,\netc., in the ranking results. In addition, the use\nof LLMs in critical decision-making sessions may\npose unspecified risks. Finally, the distilled models\nare licensed under the terms of OpenAI because\nthey use ChatGPT. The distilled LLaMA models\nare further licensed under the non-commercial li-\ncense of LLaMA.\nAcknowledgements\nThis work was supported by the Natural Sci-\nence Foundation of China (62272274, 61972234,\n62072279, 62102234, 62202271), the Natu-\nral Science Foundation of Shandong Province\n(ZR2021QF129, ZR2022QF004), the Key Scien-\ntific and Technological Innovation Program of\nShandong Province (2019JZZY010129), the Fun-\ndamental Research Funds of Shandong Univer-\nsity, the China Scholarship Council under grant\nnr. 202206220085.\nReferences\nNegar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and\nCharles L. A. Clarke. 2021. Shallow pooling for\nsparse labels. Information Retrieval Journal, 25:365\n– 385.\nLuiz Henrique Bonifacio, Hugo Queiroz Abonizio,\nMarzieh Fadaee, and Rodrigo Nogueira. 2022. In-\npars: Data augmentation for information retrieval\nusing large language models. In SIGIR 2022.\nLuiz Henrique Bonifacio, Israel Campiotti, Roberto\nde Alencar Lotufo, and Rodrigo Nogueira. 2021.\nmmarco: A multilingual version of ms marco passage\nranking dataset. ArXiv, abs/2108.13897.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\n14926\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In NeurIPS\n2020.\nSebastian Bruch, Xuanhui Wang, Michael Bendersky,\nand Marc Najork. 2019. An analysis of the softmax\ncross entropy loss for learning-to-rank with binary\nrelevance. In SIGIR 2019.\nChristopher J. C. Burges, Tal Shaked, Erin Renshaw, Ari\nLazier, Matt Deeds, Nicole Hamilton, and Gregory N.\nHullender. 2005. Learning to rank using gradient\ndescent. In ICML 2005.\nDaniel Fernando Campos, Tri Nguyen, Mir Rosenberg,\nXia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\nMajumder, Li Deng, and Bhaskar Mitra. 2016. Ms\nmarco: A human generated machine reading compre-\nhension dataset. ArXiv, abs/1611.09268.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and\nFabio Petroni. 2021. Autoregressive entity retrieval.\nIn ICLR 2021.\nHyung Won Chung, Le Hou, S. Longpre, Barret Zoph,\nYi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Dasha\nValter, Sharan Narang, Gaurav Mishra, Adams Wei\nYu, Vincent Zhao, Yanping Huang, Andrew M.\nDai, Hongkun Yu, Slav Petrov, Ed Huai hsin Chi,\nJeff Dean, Jacob Devlin, Adam Roberts, Denny\nZhou, Quoc V . Le, and Jason Wei. 2022. Scal-\ning instruction-finetuned language models. ArXiv,\nabs/2210.11416.\nNick Craswell, Bhaskar Mitra, Emine Yilmaz,\nDaniel Fernando Campos, and Ellen M. V oorhees.\n2020. Overview of the trec 2020 deep learning track.\nArXiv, abs/2102.07662.\nZhuyun Dai, Vincent Zhao, Ji Ma, Yi Luan, Jianmo\nNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B.\nHall, and Ming-Wei Chang. 2023. Promptagator:\nFew-shot dense retrieval from 8 examples. In ICLR\n2023.\nYixing Fan, Xiaohui Xie, Yinqiong Cai, Jia Chen, Xinyu\nMa, Xiangsheng Li, Ruqing Zhang, and Jiafeng Guo.\n2022. Pre-training methods in information retrieval.\nFoundations and Trends in Information Retrieval ,\n16:178–317.\nYao Fu, Hao-Chun Peng, Litu Ou, Ashish Sabharwal,\nand Tushar Khot. 2023. Specializing smaller lan-\nguage models towards multi-step reasoning. In ICML\n2023.\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\n2023. Precise zero-shot dense retrieval without rele-\nvance labels. In ACL 2023.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2022. Towards unsupervised\ndense information retrieval with contrastive learning.\nTMLR.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane A. Yu,\nArmand Joulin, Sebastian Riedel, and Edouard Grave.\n2023. Few-shot learning with retrieval augmented\nlanguage models. Journal of Machine Learning Re-\nsearch, 24(251):1–43.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Yu Wu, Sergey Edunov, Danqi Chen,\nand Wen tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In EMNLP 2020.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher R’e, Diana Acosta-Navas, Drew A.\nHudson, E. Zelikman, Esin Durmus, Faisal Ladhak,\nFrieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang,\nKeshav Santhanam, Laurel J. Orr, Lucia Zheng,\nMert Yuksekgonul, Mirac Suzgun, Nathan S. Kim,\nNeel Guha, Niladri S. Chatterji, O. Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas F. Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. 2022. Holistic eval-\nuation of language models. ArXiv, abs/2211.09110.\nJimmy J. Lin, Rodrigo Nogueira, and Andrew Yates.\n2020. Pretrained transformers for text ranking: Bert\nand beyond. In WSDM 2020.\nXueguang Ma, Xinyu Crystina Zhang, Ronak Pradeep,\nand Jimmy Lin. 2023. Zero-shot listwise docu-\nment reranking with a large language model. ArXiv,\nabs/2305.02156.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub\nAdamek, Eric Malmi, and Aliaksei Severyn. 2023.\nTeaching small language models to reason. In ACL\n2023.\nMicrosoft. 2023. Confirmed: the new\nbing runs on openai’s gpt-4. https:\n//blogs.bing.com/search/march_2023/\nConfirmed-the-new-Bing-runs-on-OpenAI%\nE2%80%99s-GPT-4.\nNiklas Muennighoff. 2022. Sgpt: Gpt sentence embed-\ndings for semantic search. ArXiv, abs/2202.08904.\nReiichiro Nakano, Jacob Hilton, S. Arun Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\n14927\nChess, and John Schulman. 2021. Webgpt: Browser-\nassisted question-answering with human feedback.\nArXiv, abs/2112.09332.\nRodrigo Nogueira and Kyunghyun Cho. 2019. Passage\nre-ranking with bert. ArXiv, abs/1901.04085.\nRodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and\nJimmy Lin. 2020. Document ranking with a pre-\ntrained sequence-to-sequence model. In Findings of\nEMNLP 2020.\nOpenAI. 2022. Introducing chatgpt. https://openai.\ncom/blog/chatgpt.\nOpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke E.\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul Francis Christiano, Jan Leike, and Ryan J.\nLowe. 2022. Training language models to follow\ninstructions with human feedback. In NeurIPS 2022.\nDevendra Singh Sachan, Mike Lewis, Mandar Joshi,\nArmen Aghajanyan, Wen tau Yih, Joëlle Pineau, and\nLuke Zettlemoyer. 2022. Improving passage retrieval\nwith zero-shot question generation. In EMNLP 2022.\nDevendra Singh Sachan, Mike Lewis, Dani Yogatama,\nLuke Zettlemoyer, Joëlle Pineau, and Manzil Zaheer.\n2023. Questions are all you need to train a dense\npassage retriever. TACL, page 600–616.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\nWen tau Yih. 2023. Replug: Retrieval-augmented\nblack-box language models. ArXiv, abs/2301.12652.\nWeiwei Sun, Lingyong Yan, Zheng Chen, Shuaiqiang\nWang, Haichao Zhu, Pengjie Ren, Zhumin Chen,\nDawei Yin, M. de Rijke, and Zhaochun Ren. 2023.\nLearning to tokenize for generative retrieval. In\nNeruIPS 2023.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nYi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni,\nDara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe\nZhao, Jai Gupta, Tal Schuster, William W. Cohen,\nand Donald Metzler. 2022. Transformer memory as\na differentiable search index. In NeurIPS 2022.\nNandan Thakur, Nils Reimers, Andreas Ruckl’e, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. Beir:\nA heterogenous benchmark for zero-shot evaluation\nof information retrieval models. In NeurIPS 2021.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur’elien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. ArXiv,\nabs/2302.13971.\nLiang Wang, Nan Yang, and Furu Wei. 2023a.\nQuery2doc: Query expansion with large language\nmodels. ArXiv, abs/2303.07678.\nXuanhui Wang, Cheng Li, Nadav Golbandi, Michael\nBendersky, and Marc Najork. 2018. The lambdaloss\nframework for ranking metric optimization. In CIKM\n2018.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023b. Self-instruct: Aligning language\nmodel with self generated instructions. In ACL 2023.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. Generate\nrather than retrieve: Large language models are\nstrong context generators. In ICLR 2023.\nXinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy J.\nLin. 2021. Mr. tydi: A multi-lingual benchmark for\ndense retrieval. In MRL.\nYutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu,\nWenhan Liu, Chenlong Deng, Zhicheng Dou, and\nJi rong Wen. 2023. Large language models for infor-\nmation retrieval: A survey. ArXiv, abs/2308.07107.\nLixin Zou, Shengqiang Zhang, Hengyi Cai, Dehong Ma,\nSuqi Cheng, Daiting Shi, Shuaiqiang Wang, Zhicong\nCheng, and Dawei Yin. 2021. Pre-trained language\nmodel based ranking in baidu search. In KDD 2021.\n14928\nA Instructions\nA.1 Query Generation Instruction\nThe query generation instruction (Sachan et al., 2022) uses the log-probability of the query.\nPlease write a question based on this passage.\nPassage: {{passage}}\nQuestion: {{query}}\nA.2 Relevance Generation Instruction (few-shot)\nFollowing HELM (Liang et al., 2022), the relevance generation instruction use 4 in-context examples.\nGiven a passage and a query, predict whether the passage includes an answer to the query by\nproducing either ‘Yes‘ or ‘No‘.\nPassage: Its 25 drops per ml, you guys are all wrong. If it is water, the standard was changed 15 -\n20 years ago to make 20 drops = 1mL. The viscosity of most things is temperature dependent, so\nthis would be at room temperature. Hope this helps.\nQuery: how many eye drops per ml\nDoes the passage answer the query?\nAnswer: Yes\nPassage: RE: How many eyedrops are there in a 10 ml bottle of Cosopt? My Kaiser pharmacy\ninsists that 2 bottles should last me 100 days but I run out way before that time when I am using 4\ndrops per day.In the past other pharmacies have given me 3 10-ml bottles for 100 days.E: How\nmany eyedrops are there in a 10 ml bottle of Cosopt? My Kaiser pharmacy insists that 2 bottles\nshould last me 100 days but I run out way before that time when I am using 4 drops per day.\nQuery: how many eye drops per ml\nDoes the passage answer the query?\nAnswer: No\nPassage: : You can transfer money to your checking account from other Wells Fargo. accounts\nthrough Wells Fargo Mobile Banking with the mobile app, online, at any. Wells Fargo ATM, or at\na Wells Fargo branch. 1 Money in — deposits.\nQuery: can you open a wells fargo account online\nDoes the passage answer the query?\nAnswer: No\nPassage: You can open a Wells Fargo banking account from your home or even online. It is really\neasy to do, provided you have all of the appropriate documentation. Wells Fargo has so many bank\naccount options that you will be sure to find one that works for you. They offer free checking\naccounts with free online banking.\nQuery: can you open a wells fargo account online\nDoes the passage answer the query?\nAnswer: Yes\nPassage: {{passage}}\nQuery:{{query}}\nDoes the passage answer the query?\nAnswer:\n14929\nA.3 Relevance Generation Instruction (zero-shot)\nThis instruction is used to train LLaMA-7B specialized models.\nGiven a passage and a query, predict whether the passage includes an answer to the query by\nproducing either ‘Yes‘ or ‘No‘.\nPassage: {{passage}}\nQuery: {{query}}\nDoes the passage answer the query?\nAnswer:\nA.4 Permutation Generation Instruction (Text)\nPermutation generation (text) is used for text-davinci-003.\nThis is RankGPT, an intelligent assistant that can rank passages based on their relevancy to the\nquery.\nThe following are {{num}} passages, each indicated by number identifier []. I can rank them based\non their relevance to query: {{query}}\n[1] {{passage_1}}\n[2] {{passage_2}}\n(more passages) ...\nThe search query is: {{query}}\nI will rank the {{num}} passages above based on their relevance to the search query. The passages\nwill be listed in descending order using identifiers, and the most relevant passages should be listed\nfirst, and the output format should be [] > [] > etc, e.g., [1] > [2] > etc.\nThe ranking results of the {{num}} passages (only identifiers) is:\nA.5 Permutation Generation Instruction (Chat)\nPermutation generation instruction (chat) is used for gpt-3.5-turbo and gpt-4.\n14930\nsystem:\nYou are RankGPT, an intelligent assistant that can rank passages based on their relevancy to the\nquery.\nuser:\nI will provide you with {{num}} passages, each indicated by number identifier []. Rank them\nbased on their relevance to query: {{query}}.\nassistant:\nOkay, please provide the passages.\nuser:\n[1] {{passage_1}}\nassistant:\nReceived passage [1]\nuser:\n[2] {{passage_2}}\nassistant:\nReceived passage [2]\n(more passages) ...\nuser\nSearch Query: {{query}}.\nRank the {{num}} passages above based on their relevance to the search query. The passages\nshould be listed in descending order using identifiers, and the most relevant passages should be\nlisted first, and the output format should be [] > [], e.g., [1] > [2]. Only response the ranking results,\ndo not say any word or explain.\nB Instructional Methods on LLMs as Rernaker\nThis paper focus on re-ranking task, given M passages for a query q, the re-ranking aims to use an agent\nf(·) to output their ranking results R = (r1,...,r M), where ri ∈[1,2,...,M ] denotes the rank of pi. This\npaper studies using the LLMs as f(·).\nB.1 Instructional Query Generation\nQuery generation has been studied in Sachan et al. (2022); Muennighoff (2022), in which the relevance\nbetween a query and a passage is measured by the log-probability of the model to generate the query\nbased on the passage. Figure 2 (a) shows an example of instructional query generation.\nFormally, given query qand a passage pi, their relevance score si is calculated as:\nsi = 1\n|q|\n∑\nt\nlog p(qt|q<t,pi,Iquery) (1)\nwhere |q|denotes the number of tokens in q, qt denotes the t-th token of q, and Iquery denotes the\ninstructions, referring to Figure 2 (a). The passages are then ranked based on relevance score si.\nB.2 Instructional Relevance Generation\nRelevance generation is employed in HELM (Liang et al., 2022). Figure 2 (b) shows an example of\ninstructional relevance generation, in which LLMs are instructed to output \"Yes\" if the query and passage\n14931\nare relevant or \"No\" if they are irrelevant. The relevance score si is measured by the probability of LLMs\ngenerating the word ’Yes’ or ’No’:\nsi =\n{\n1 + p(Yes), if output is Yes\n1 −p(No), if output is No (2)\nwhere p(Yes/No) denotes the probability of LLMs generating Yes or No, and the relevance score is\nnormalized into the range [0, 2].\nThe above two methods rely on the log probability of LLM, which is often unavailable for LLM API.\nFor example, at the time of writing, OpenAI’s ChatCompletion API does not provide the log-probability\nof generation5.\nB.3 Instructional Permutation Generation\nThe proposed instructional permutation generation is a listwise approach, which directly assigns each\npassage pi a unique ranking identifier ai (e.g., [1], [2]) and places it at the beginning of pi: p′\ni =\nConcat(ai,pi). Subsequently, a generative LLM is instructed to generate a permutation of these identifiers:\nPerm = f(q,p′\n1,...,p ′\nM), where the permutation Perm indicates the rank of the identifiers ai (e.g., [1],\n[2]). We then simply map the identifiers ai to the passages pi to obtain the ranking of the passages.\nDomain Question Reference Answer\nSport What is Messi’s annual income after transferring to Miami? $50M-$60M\nSport How many goals did Haaland scored in the 2023 Champions League Final? 0\nSport Where did Benzema go after leaving Real Madrid? Saudi Arabia\nSport Where was the 2023 Premier League FA Cup Final held? Wembley Stadium\nSport Who won 2023 Laureus World Sportsman Of The Year Award? Lionel Messi\nSport Who wins NBA Finals 2023? Denver Nuggets\nTech What is the screen resolution of vision pro? 4K with one eye\nTech What is the name of the combined Deepmind and Google Brain? Google DeepMind\nTech How much video memory does the DGX GH200 have? 144TB\nTech What are the new features of PyTorch 2? faster, low memory, dynamic shapes\nTech Who will be the CEO of Twitter after Elon Musk is no longer the CEO? Linda Yaccarino\nTech What are the best papers of CVPR 2023? Visual Programming: Compositional [...]\nMovie Who sang the theme song of Transformers Rise of the Beasts? Notorious B.I.G\nMovie Who is the villain in The Flash? Eobard Thawne/Professor Zoom\nMovie How many different Spider-Men are there in Across the Spider-Verse? 280 variations\nMovie Who does Momoa play in Fast X? Dante\nMovie The Little Mermaid first week box office? $163.8 million worldwide\nMovie Which film was the 2023 Palme d’Or winner? Anatomy of a Fall\nOther Where will Blackpink’s 2023 world tour concert in France be held? the Stade de France\nOther What is the release date of song Middle Ground? May 19, 2023\nOther Where did the G7 Summit 2023 take place? Hiroshima\nTable 8: Questions and reference answers on NovelEval-2306.\nC NovelEval-2306\nTable 8 lists the collected 21 questions. These questions come from four domains and include hot topics\nfrom the past few months. For each question, we used Google search to obtain 20 passages. When using\nGoogle search, in order to avoid all pages containing the answer, we used not only the question itself as a\nsearch query, but also the entities that appear in the question as an alternative search query to obtain some\npages that are relevant but do not contain the answer. For example, for the first question \"What is Messi’s\nannual income after transferring to Miami?\", we used \"Messi\" and \"Messi transferring\" as search queries\nto get some pages that do not contain the answer. When searching, we collected the highest-ranking\nweb pages, news, and used a paragraph or paragraphs from the web pages related to the search term as\ncandidate passages. Table 9 shows the statistical information of the data. All of the LLMs (including\n5https://platform.openai.com/docs/api-reference/chat/create\n14932\ngpt-4-0314 and gpt-4-0613) we tested achieved 0% question-answering accuracy on the obtained test\nset.\nWe searched for 20 candidate passages for each question using Google search. These passages were\nmanually labeled for relevance by a group of annotators, including the authors and their highly educated\ncolleagues. To ensure consistency, the annotation process was repeated twice. Each passage was assigned\na relevance score: 0 for not relevant, 1 for partially relevant, and 2 for relevant. When evaluating the latest\nLLMs, we found that all non-retrieval-augmented models tested achieved 0% accuracy in answering the\nquestions on the test set. This test set provides a reasonable evaluation of the latest LLMs at the moment.\nSince LLMs may be continuously trained on new data, the proposed test set should be continuously\nupdated to counteract the contamination of the test set by LLMs.\nNumber of questions 21\nNumber of passages 420\nNumber of relevance annotation 420\nAverage number words of passage 149\nNumber of score 0 290\nNumber of score 1 40\nNumber of score 2 90\nTable 9: Data Statistics of NovelEval.\nD Implementation Details\nD.1 Training Configuration\nWe use DeBERTa-V3-base, which concatenates the query and passage with a[SEP] token and utilizes\nthe representation of the [CLS] token. To generate candidate passages, we randomly sample 10k queries\nand use BM25 to retrieve 20 passages for each query. We then re-rank the candidate passages using the\ngpt-3.5-turbo API with permutation generation instructions, at a cost of approximately $40. During\ntraining, we employ a batch size of 32 and utilize the AdamW optimizer with a constant learning rate of\n5 ×10−5. The model is trained for two epochs. Additionally, we implement models using the original\nMS MARCO labels for comparison.\nThe LLaMA-7B model is optimized with the AdamW optimizer, a constant learning rate of 5 ×10−5,\nand with mixed precision of bf16 and Deepspeed Zero3 strategy. All the experiments are conducted on 8\nA100-40G GPUs.\nFigure 5 illustrates the detailed model architecture of BERT-like and GPT-like specialized models.\nFigure 5: Model architecture of BERT-like and GPT-like specialized models.\n14933\nD.2 Training Objective\nUsing the permutation generated by ChatGPT, we consider the following losses to optimize the student\nmodel:\nListwise Cross-Entropy (CE) (Bruch et al., 2019) . Listwise CE is the wide-use loss for passage\nranking, which considers only one positive passage and defines the list-wise softmax cross-entropy on all\ncandidate’s passages:\nLListwise_CE = −\nM∑\ni=1\n1ri=1 log( exp(si)∑\nj exp(sj))\nwhere 1 is the indicator function.\nRankNet (Burges et al., 2005) . RankNet is a pairwise loss that measures the correctness of relative\npassage orders:\nLRankNet =\nM∑\ni=1\nM∑\nj=1\n1ri<rj log(1 + exp(si −sj))\nwhen using permutation generated by ChatGPT, we can construct M(M −1)/2 pairs.\nLambdaLoss (Wang et al., 2018). The LambdaLoss further accounts for the nDCG gains of the model\nranks. LambdaLoss uses the student model’s rank, denoted as π= (π1,...,π M), where πi is the model\npredicted rank of pi with a similar definition with ChatGPT rank R. The loss function is defined as:\nLLambda =\n∑\nri<rj\n∆NDCG log2(1 + exp(si −sj))\nin which ∆NDCG is the delta of NDCG which could be compute as∆NDCG = |Gi−Gj|| 1\nD(πi) − 1\nD(πj ) |,\nwhere D(πi) and D(πj) are the position discount functions and Gi and Gj are the gain functions used in\nNDCG (Wang et al., 2018).\nPointwise Binary Cross-Entropy (BCE). We also include the Pointwise BCE as the baseline loss for\nsupervised methods, which is calculated based on each query-document pair independently:\nLBCE = −\nM∑\ni=1\n1ri=1 log σ(si) + 1ri̸=1 log σ(1 −si)\nwhere σ(x) = 1\n1+exp(−x) is the logistic function.\nE Baselines Details\nWe include state-of-the-art supervised and unsupervised passage re-ranking methods for comparison. The\nsupervised baselines are:\n• monoBERT (Nogueira and Cho, 2019): A cross-encoder re-ranker based on BERT-large, trained on\nMS MARCO.\n• monoT5 (Nogueira et al., 2020): A sequence-to-sequence re-ranker that uses T5 to calculate the\nrelevance score6.\n• Cohere Rerank: A passage reranking API rerank-english-v2.0 developed by Cohere7. Cohere\ndoes not provide details on the structure and training method of the model.\n• mmarcoCE (Bonifacio et al., 2021): A 12-layer mMiniLM-v2 cross-encoder model 8 trained on\nmmarco, a translated version of MS MARCO. mmarcoCE serves as a baseline for Mr.TyDi.\nThe unsupervised baselines are:\n6https://huggingface.co/castorini/monot5-3b-msmarco-10k\n7https://cohere.com/rerank\n8https://huggingface.co/cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\n14934\n• UPR (Sachan et al., 2022): Unsupervised passage ranking with instructional query generation. Due\nto its superior performance, we use the FLAN-T5-XL (Chung et al., 2022) as the LLM of UPR.\n• InPars (Bonifacio et al., 2022): monoT5-3B trained on pseudo data generated by GPT-3.\n• Promptagator++ (Dai et al., 2023): A 110M cross-encoder re-ranker trained on pseudo queries\ngenerated by FALN 137B.\nMethod Repetition ↓ Missing↓ Rejection RBO ↑\ntext-davinci-003 0 280 0 72.30\ngpt-3.5-turbo 14 153 7 81.49\ngpt-4 0 1 11 82.08\nTable 10: Analysis of model stability on TREC. Repetition refers to the number of times the model generates\nduplicate passage identifiers. Missing refers to the number of missing passage identifiers in model output. Rejection\nrefers to the number of times the model rejects to perform the ranking. RBO, i.e., rank biased overlap, refers to the\nconsistency of the model in ranking the same group of passages twice.\nF Model Behavior Analysis\nIn the permutation generation method, the ranking of passages is determined by the list of model-output\npassage identifiers. However, we have observed that the models do not always produce the desired output,\nas evidenced by occasional duplicates or missing identifiers in the generated text. In Table 10, we present\nquantitative results of unexpected model behavior observed during experiments with the GPT models.\nRepetition. The repetition metric measures the occurrence of duplicate passage identifiers generated by\nthe model. The results indicate that ChatGPT produced 14 duplicate passage identifiers during re-ranking\n97 queries on two TREC datasets, whereas text-davinci-003 and GPT-4 did not exhibit any duplicates.\nMissing. We conducted a count of the number of times the model failed to include all passages in the\nre-ranked permutation output9. Our findings revealed that text-davinci-003 has the highest number\nof missing passages, totaling 280 instances. ChatGPT also misses a considerable number of passages,\noccurring 153 times. On the other hand, GPT-4 demonstrates greater stability, with only one missing\npassage in total. These results suggest that GPT-4 has higher reliability in generating permutations, which\nis critical for effective ranking.\nRejection. We have observed instances where the model refuses to re-rank passages, as evidenced by\nresponses such as \" None of the provided passages is directly relevant to the query ... \". To quantify\nthis behavior, we count the number of times this occurred and find that GPT-4 rejects ranking the most\nfrequently, followed by ChatGPT, while the Davinci model never refused to rank. This finding suggests\nthat chat LLMs tend to be more adaptable compared to completion LLMs, and may exhibit more subjective\nresponses. Note that we do not explicitly prohibit the models from rejecting ranking in the instructions, as\nwe find that it does not significantly impact the overall ranking performance.\nRBO. The sliding windows strategy involves re-ranking the top-ranked passages from the previous\nwindow in the next window. The models are expected to produce consistent rankings in two windows\nfor the same group of passages. To measure the consistency of the model’s rankings, we use RBO (rank\nbiased overlap10), which calculates the similarity between the two ranking results. The findings turn out\nthat ChatGPT and GPT-4 are more consistent in ranking passages compared to the Davinci model. GPT-4\nalso slightly outperforms ChatGPT in terms of the RBO metric.\nG Analysis on Hyperparameters of Sliding Window\nTo analyze the influence of parameters of the sliding window strategy, we adjust the window size and\nset the step size to half of the window size. The main motivation for this setup is to keep the expected\n9In our implementation, we append the missing passages in their original order at the end of the re-ranked passages.\n10https://github.com/changyaochen/rbo\n14935\nAPI Instruction Tokens Requests $USD\ntext-curie-001 Relevance generation 52,970 100 0.106\ntext-curie-001 Query generation 10,954 100 0.022\ntext-davinci-003 Query generation 11,269 100 0.225\ntext-davinci-003 Permutation generation 17,370 10 0.347\ngpt-3.5-turbo Permutation generation 19,960 10 0.040\ngpt-4 Permutation generation 19,890 10 0.596\n- rerank top-30 Permutation generation 3,271 1 0.098\nTable 11: Average token cost, number API request, and $USD per query on TREC.\nWindow size Step size nDCG@1 nDCG@5 nDCG@10\n20 10 75.58 70.50 67.05\n40 20 78.30 71.32 65.51\n60 30 75.97 69.23 65.03\n80 40 72.09 70.59 65.57\nTable 12: Analysis on Hyperparameters of Sliding Window on TREC-DL19.\noverhead of the method (number of tokens required for computation) low; i.e., most tokens in this setup\nare used for PG only twice. The experimental results are shown in Table 1211. The results show that the\neffect varies over a certain range of arrivals for different values of window size: window size=20 performs\nbest in terms of nDCG@10, while window size=40 performs best in terms of nDCG@5 and nDCG@1.\nWe speculate that a larger window size will increase the model’s ranking horizon but will also present\nchallenges in processing long contexts and large numbers of items.\nH API Cost\nIn Table 11, we provide details on the average token cost, API request times, and USD cost per query.\nIn terms of average token cost, the relevance generation method is the most expensive, as it requires 4\nin-context demonstrations. On the other hand, the permutation generation method incurs higher token\ncosts compared to the query generation method, as it involves the repeated processing of passages in\nsliding windows. Regarding the number of requests, the permutation generation method requires 10\nrequests for sliding windows, while other methods require 100 requests for re-ranking 100 passages. In\nterms of average USD cost, GPT-4 is the most expensive, with a cost of $0.596 per query. However, using\nGPT-4 for re-ranking the top-30 passages can result in significant cost savings, with a cost of $0.098\nper query for GPT-4 usage, while still achieving good results. As a result, we only utilize GPT-4 for\nre-ranking the top 30 passages of ChatGPT on BEIR and Mr.TyDi. The total cost of our experiments with\nGPT-4 amounts to $556.\nSince the experiments with ChatGPT and GPT-4 are conducted using the OpenAI API, the running\ntime is contingent on the OpenAI service, e.g., API latency. Besides, the running time can also vary\nacross different API versions and network environments. In our testing conditions, the average latency\nfor API calls for gpt-3.5-turbo and gpt-4 was around 1.1 seconds and 3.2 seconds, respectively. Our\nproposed sliding window-based permutation generation approach requires 10 API calls per query to\nre-rank 100 passages. Consequently, the average running time per query is 11 seconds for gpt-3.5-turbo\nand 32 seconds for gpt-4.\nI Results of Specialized Models\nTable 13 lists the detailed results of specialized models on TREC and BEIR.\n11Note that the results are obtained using gpt-3.5-turbo-16k API for managing long context.\n14936\nMethod DL19 DL20 Covid NFCorpus Touche DBPedia SciFact Signal News Robust04 BEIR (Avg)\nBM25 50.58 47.96 59.47 30.75 44.22 31.80 67.89 33.05 39.52 40.70 43.42\nSupervised train on MS MRACO\nmonoBERT (340M) 70.50 67.28 70.01 36.88 31.75 41.87 71.36 31.44 44.62 49.35 47.16\nmonoT5 (220M) 71.48 66.99 78.34 37.38 30.82 42.42 73.40 31.67 46.83 51.72 49.07\nmonoT5 (3B) 71.83 68.89 80.71 38.97 32.41 44.45 76.57 32.55 48.49 56.71 51.36\nCohere Rerank-v2 73.22 67.08 81.81 36.36 32.51 42.51 74.44 29.60 47.59 50.78 49.45\nUnsupervised instructional permutation generation\nChatGPT 65.80 62.91 76.67 35.62 36.18 44.47 70.43 32.12 48.85 50.62 49.37\nGPT-4 75.59 70.56 85.51 38.47 38.57 47.12 74.95 34.40 52.89 57.55 53.68\nSpecialized Models train on MARCO labels or ChatGPT predicted permutations\nMARCO Pointwise BCE 65.57 56.72 70.82 33.10 17.08 32.28 55.37 19.30 41.52 46.00 39.43\nMARCO Listwise CE 65.99 57.97 66.31 32.61 20.15 30.79 37.57 18.09 38.11 39.93 35.45\nMARCO RankNet 66.34 58.51 70.29 34.23 20.27 29.62 49.01 23.22 39.82 43.87 38.79\nMARCO LambdaLoss 64.82 56.16 72.86 34.20 19.51 32.55 51.88 26.22 42.47 45.28 40.62\nChatGPT Listwise CE 65.39 58.80 76.29 35.73 38.19 40.24 64.49 31.37 47.61 48.00 47.74\nChatGPT RankNet 65.75 59.34 81.26 36.57 39.03 42.10 68.77 31.55 52.54 52.44 50.53\nChatGPT LambdaLoss 67.17 60.56 80.63 36.74 36.73 43.75 68.21 32.58 49.00 50.51 49.77\ndeberta-v3-xsmall (70M) 64.75 55.07 78.21 35.95 35.42 41.37 67.86 30.04 47.68 49.91 48.31\ndeberta-v3-small (142M) 67.85 58.84 78.88 36.55 36.16 40.99 66.66 30.29 49.17 49.73 48.55\ndeberta-v3-base (184M) 70.28 62.52 80.81 36.15 37.25 44.06 71.70 32.45 50.84 51.33 50.57\ndeberta-v3-large (435M) 70.66 67.15 84.64 38.48 39.27 47.36 74.18 32.53 51.19 56.55 53.03\ndeberta-v3-large 5K 70.93 64.32 84.43 38.66 40.72 46.28 73.88 31.93 52.24 55.89 53.00\ndeberta-v3-large 3K 70.79 63.91 84.21 38.73 39.83 45.74 74.41 31.92 52.29 57.42 53.07\ndeberta-v3-large 1K 69.90 64.81 83.38 38.94 36.65 44.46 71.96 30.19 50.73 53.74 51.26\ndeberta-v3-large 500 69.71 62.00 83.54 37.23 33.68 44.56 70.48 28.70 45.64 42.67 48.31\ndeberta-v3-large label 10K 66.61 57.26 74.36 33.94 18.09 34.95 35.35 21.38 39.00 44.94 37.75\ndeberta-v3-large label 5K 68.98 61.38 80.73 35.68 20.48 37.34 54.63 24.25 36.94 51.13 42.64\ndeberta-v3-large label 3K 67.41 60.42 79.82 35.49 24.54 37.39 47.31 23.29 39.87 50.65 42.29\ndeberta-v3-large label 1K 65.55 60.93 77.70 33.29 23.36 36.38 31.10 21.71 34.28 38.31 37.01\ndeberta-v3-large label 500 60.59 54.45 76.20 32.93 19.66 31.54 45.66 13.99 33.48 44.49 37.24\ndeberta-v3-large monoT5-3B 73.05 68.82 84.78 38.55 34.43 43.61 75.45 30.75 49.85 56.80 51.78\ndeberta-v3-large chatgpt+label 72.42 67.30 85.96 38.75 35.06 45.43 71.81 28.52 45.91 55.57 50.88\ndeberta-v3-base label 10k 65.66 59.84 71.63 34.65 16.53 32.59 34.65 22.64 37.60 44.02 36.79\ndeberta-v3-small label 10k 63.63 52.83 68.17 30.48 18.12 31.72 33.62 18.02 34.57 36.09 33.85\ndeberta-v3-xsmall label 10k 60.89 51.15 63.58 28.67 14.87 27.12 20.60 18.97 32.61 32.67 29.89\nllama-7b 71.33 66.06 78.23 37.60 34.87 45.46 76.13 34.17 51.79 55.22 51.68\nvicuna-7b 71.80 66.89 78.32 36.87 31.81 45.40 74.23 34.28 51.13 52.91 50.62\nllama-7b 10k label 65.22 56.85 75.36 36.24 20.88 37.34 69.04 25.22 41.21 49.21 44.31\nllama-7b 5k label 69.24 58.97 80.49 37.55 28.23 39.66 71.79 26.04 44.09 53.83 47.71\nTable 13: Results (nDCG@10) on TREC and BEIR. Best performing specialized and overall system(s) are\nmarked bold. The specialized models are fine-tined on sampled queries using relevance judgements from MARCO\nor ChatGPT.\n14937",
  "topic": "Ranking (information retrieval)",
  "concepts": [
    {
      "name": "Ranking (information retrieval)",
      "score": 0.7834993600845337
    },
    {
      "name": "Computer science",
      "score": 0.6813338994979858
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6583387851715088
    },
    {
      "name": "Language model",
      "score": 0.5570820569992065
    },
    {
      "name": "Relevance (law)",
      "score": 0.5514404773712158
    },
    {
      "name": "Machine learning",
      "score": 0.5390474200248718
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4756854772567749
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4695133864879608
    },
    {
      "name": "Generative model",
      "score": 0.45453080534935
    },
    {
      "name": "Code (set theory)",
      "score": 0.4528362452983856
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.43290698528289795
    },
    {
      "name": "Generalization",
      "score": 0.420055627822876
    },
    {
      "name": "Information retrieval",
      "score": 0.3786372244358063
    },
    {
      "name": "Generative grammar",
      "score": 0.26838624477386475
    },
    {
      "name": "Mathematics",
      "score": 0.09950089454650879
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154099455",
      "name": "Shandong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I98301712",
      "name": "Baidu (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I121797337",
      "name": "Leiden University",
      "country": "NL"
    }
  ],
  "cited_by": 144
}