{
    "title": "Impact of Guidance and Interaction Strategies for LLM Use on Learner Performance and Perception",
    "url": "https://openalex.org/W4387928447",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5106407207",
            "name": "Harsh Kumar",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5022204507",
            "name": "Ilya Musabirov",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5028659802",
            "name": "Mohi Reza",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5108872910",
            "name": "Jiakai Shi",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5060253607",
            "name": "X. D. Wang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5069476228",
            "name": "Joseph Jay Williams",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5090252428",
            "name": "Anastasia Kuzminykh",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5017029944",
            "name": "Michael Liut",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4286910674",
        "https://openalex.org/W4246222016",
        "https://openalex.org/W3100002139",
        "https://openalex.org/W1570348318",
        "https://openalex.org/W2151242289",
        "https://openalex.org/W96525139",
        "https://openalex.org/W4366596891",
        "https://openalex.org/W160461043",
        "https://openalex.org/W2899516957",
        "https://openalex.org/W87887066",
        "https://openalex.org/W4385474073",
        "https://openalex.org/W4206545243",
        "https://openalex.org/W2475454661",
        "https://openalex.org/W3083762786",
        "https://openalex.org/W4384807943",
        "https://openalex.org/W3121904249",
        "https://openalex.org/W4230429854",
        "https://openalex.org/W2072014455",
        "https://openalex.org/W2061200712",
        "https://openalex.org/W2151401338",
        "https://openalex.org/W3000822833",
        "https://openalex.org/W4237066437",
        "https://openalex.org/W2754967069",
        "https://openalex.org/W4323655724",
        "https://openalex.org/W4381982883",
        "https://openalex.org/W2775325862",
        "https://openalex.org/W4384890810",
        "https://openalex.org/W4319301677",
        "https://openalex.org/W3047185145",
        "https://openalex.org/W4308244910",
        "https://openalex.org/W2067815663",
        "https://openalex.org/W2051545216",
        "https://openalex.org/W4318257512",
        "https://openalex.org/W2038385976",
        "https://openalex.org/W4244096728",
        "https://openalex.org/W2017764013",
        "https://openalex.org/W3029636270",
        "https://openalex.org/W2012126475",
        "https://openalex.org/W2084523232",
        "https://openalex.org/W3045123719",
        "https://openalex.org/W4365145607",
        "https://openalex.org/W1907496756",
        "https://openalex.org/W1906263353",
        "https://openalex.org/W652493169",
        "https://openalex.org/W2067126534",
        "https://openalex.org/W4361204578",
        "https://openalex.org/W4366548330",
        "https://openalex.org/W3177466199",
        "https://openalex.org/W4303685983",
        "https://openalex.org/W2045032268",
        "https://openalex.org/W2779206865",
        "https://openalex.org/W2804088093",
        "https://openalex.org/W2913681502",
        "https://openalex.org/W1968645376",
        "https://openalex.org/W4323526552",
        "https://openalex.org/W3099702889",
        "https://openalex.org/W4384520763",
        "https://openalex.org/W4389109516",
        "https://openalex.org/W2966150097",
        "https://openalex.org/W2037785207",
        "https://openalex.org/W2546112086",
        "https://openalex.org/W2757135061",
        "https://openalex.org/W4387606027",
        "https://openalex.org/W4381556591",
        "https://openalex.org/W4249833682",
        "https://openalex.org/W2059804788",
        "https://openalex.org/W2083599612",
        "https://openalex.org/W1556553643",
        "https://openalex.org/W4308426205",
        "https://openalex.org/W4387606440",
        "https://openalex.org/W2054286249",
        "https://openalex.org/W2010845456",
        "https://openalex.org/W4255089318",
        "https://openalex.org/W3209606054",
        "https://openalex.org/W4307905568",
        "https://openalex.org/W4367369394",
        "https://openalex.org/W2163640453",
        "https://openalex.org/W4312089323",
        "https://openalex.org/W145939632",
        "https://openalex.org/W2095184141",
        "https://openalex.org/W3159409425",
        "https://openalex.org/W4384520667",
        "https://openalex.org/W4327518740",
        "https://openalex.org/W3038228746",
        "https://openalex.org/W1983526537",
        "https://openalex.org/W2051729463",
        "https://openalex.org/W2590226783"
    ],
    "abstract": "Personalized chatbot-based teaching assistants can be crucial in addressing increasing classroom sizes, especially where direct teacher presence is limited. Large language models (LLMs) offer a promising avenue, with increasing research exploring their educational utility. However, the challenge lies not only in establishing the efficacy of LLMs but also in discerning the nuances of interaction between learners and these models, which impact learners' engagement and results. We conducted a formative study in an undergraduate computer science classroom (N=145) and a controlled experiment on Prolific (N=356) to explore the impact of four pedagogically informed guidance strategies on the learners' performance, confidence and trust in LLMs. Direct LLM answers marginally improved performance, while refining student solutions fostered trust. Structured guidance reduced random queries as well as instances of students copy-pasting assignment questions to the LLM. Our work highlights the role that teachers can play in shaping LLM-supported learning environments.",
    "full_text": "499\nImpact of Guidance and Interaction Strategies for LLM Use\non Learner Performance and Perception\nHARSH KUMAR, University of Toronto, Canada\nILYA MUSABIROV, University of Toronto, Canada\nMOHI REZA, University of Toronto, Canada\nJIAKAI SHI, University of Toronto, Canada\nXINYUAN WANG,University of Toronto, Canada\nJOSEPH JAY WILLIAMS, University of Toronto, Canada\nANASTASIA KUZMINYKH, University of Toronto, Canada\nMICHAEL LIUT, University of Toronto Mississauga, Canada\nPersonalized chatbot-based teaching assistants can be crucial in addressing increasing classroom sizes, espe-\ncially where direct teacher presence is limited. Large language models (LLMs) offer a promising avenue, with\nincreasing research exploring their educational utility. However, the challenge lies not only in establishing the\nefficacy of LLMs but also in discerning the nuances of interaction between learners and these models, which\nimpact learners‚Äô engagement and results. We conducted a formative study in an undergraduate computer\nscience classroom (N=145) and a controlled experiment on Prolific (N=356) to explore the impact of four\npedagogically informed guidance strategies on the learners‚Äô performance, confidence and trust in LLMs. Direct\nLLM answers marginally improved performance, while refining student solutions fostered trust. Structured\nguidance reduced random queries as well as instances of students copy-pasting assignment questions to the\nLLM. Our work highlights the role that teachers can play in shaping LLM-supported learning environments.\nCCS Concepts: ‚Ä¢ Human-centered computing ‚ÜíEmpirical studies in HCI ; Empirical studies in\ncollaborative and social computing ; ‚Ä¢ Applied computing ‚ÜíComputer-assisted instruction; ‚Ä¢ Social\nand professional topics ‚ÜíComputing education .\nAdditional Key Words and Phrases: large language models, tutoring systems, Human-AI collaboration, artificial\nintelligence in education, collaborative learning with AI, transparency.\nACM Reference Format:\nHarsh Kumar, Ilya Musabirov, Mohi Reza, Jiakai Shi, Xinyuan Wang, Joseph Jay Williams, Anastasia Kuzminykh,\nand Michael Liut. 2024. Impact of Guidance and Interaction Strategies for LLM Use on Learner Perfor-\nmance and Perception. Proc. ACM Hum.-Comput. Interact. 8, CSCW2, Article 499 (November 2024), 30 pages.\nhttps://doi.org/10.1145/3687038\nAuthors‚Äô addresses: Harsh Kumar, harsh@cs.toronto.edu, University of Toronto, 40 St. George Street, Toronto, Ontario,\nCanada, M5S 2E4; Ilya Musabirov, imusabirov@cs.toronto.edu, University of Toronto, Canada; Mohi Reza, mohireza@cs.\ntoronto.edu, University of Toronto, Canada; Jiakai Shi, jiakai.shi@mail.utoronto.ca, University of Toronto, Canada; Xinyuan\nWang, xyben.wang@mail.utoronto.ca, University of Toronto, Canada; Joseph Jay Williams, williams@cs.toronto.edu,\nUniversity of Toronto, Canada; Anastasia Kuzminykh, anastasia.kuzminykh@mail.utoronto.ca, University of Toronto,\nCanada; Michael Liut, michael.liut@utoronto.ca, University of Toronto Mississauga, Canada.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n2573-0142/2024/11-ART499 $15.00\nhttps://doi.org/10.1145/3687038\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\narXiv:2310.13712v3  [cs.HC]  20 Aug 2024\n499:2 Harsh Kumar, et al.\n1 INTRODUCTION\nThe value of personalized attention in enhancing student learning outcomes is well recognized in\nthe contemporary educational landscape [13, 50, 96, 124]. For instance, it is well-reflected in the 2\nSigma problem by Benjamin Bloom [14], who showed that one-to-one tutored students perform two\nstandard deviations (or ‚Äú2 Sigma‚Äù) better than students who learned via conventional instructional\nmethods. However, personalized attention in education is challenged by the continuous expansion\nof classrooms, and with a growing demand for tailored teaching approaches, there is a notable\nshortage of human teaching assistants to provide individualized attention [ 7, 105], leading to a\ncritical bottleneck in achieving optimal educational impacts.\nTo address this issue, previous research on Intelligent Tutoring Systems (ITS) and Intelligent\nTeaching Assistants (ITA) has pioneered adaptive learning experiences and personalized instruc-\ntional feedback [9, 111], increasing the interest of CSCW community to the role that conversational\nagents can play in supporting the education process [ 18, 30, 71, 91, 102, 109, 116]. The rapid\nemergence of Large Language Models (LLMs) in recent years has created even more promising\nopportunities for technological support of personalized attention needs of students [98, 125]. These\nmodels, with their ability to comprehend and generate human-like text, have the potential to\nreshape the landscape of personalized education. Consequently, researchers started exploring the\npotential advantages of the development of LLM-based agent tutors as potential surrogates for\nhuman teaching assistants [11, 34, 64, 89]. Indeed, the dialogic nature of LLM-based agents, their\nability to adapt to individualized learning patterns, and their ease of integration into diverse educa-\ntional contexts make them a potential boon for educational frameworks [41]. Although learning\nenvironments vary between subjects and educational levels, the versatility of LLMs allows their\nintegration into these specialized contexts, providing a unified platform for diverse educational\nneeds.\nHowever, although there is a considerable volume of existing research on LLM applications,\na noticeable gap remains when applied to education. For example, a large body of pedagogical\nresearch [16, 23, 58, 100, 106] suggests that the success of education and tutoring is significantly\ndefined by the communication strategies employed by an instructor. Thus, the pedagogical and\ninteraction effects are inherently interconnected, and discerning their nuanced impacts is essential\nfor the effective design and implementation of LLM tutors. However, while LLM-based systems\ncan support different interaction strategies [57, 67], it remains unclear which of these strategies\nare most beneficial for an agent to assume when interacting with students. For example, how\ncould these strategies influence the students‚Äô reception of the assistance, self-assessment, and their\nsubsequent performance? The matter is further complicated by evidence that the dynamics of social\ninteractions might differ for human-human and human-agent communications [25, 36], making\nthe direct integration of the pedagogical knowledge on learning patterns into agent behavior even\nmore challenging. Correspondingly, while there is a substantial body of literature on the impact of\nhuman-human teaching interaction strategies on learning outcomes, our understanding of how\nthese dynamics translate to an LLM-learner context remains limited. Thus, the question is not\nonly about the feasibility of these LLM-based tutors, but also about the ambiguity surrounding the\neffects of different interaction strategies on students. To be able to effectively adapt LLM-based\nagents for individual tutoring, we first need to understand what interaction strategies should be\nimplemented in these systems for optimal learning support.\nIn this work, we address this gap by exploring the following overarching research question:\nRQ What are the specific effects of common guidance strategies (providing instructions, present-\ning examples, employing metacognitive questioning, and encouraging initial problem-solving with\nsubsequent refinement) on the learning process supported by an LLM tutor?\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\nGuiding Students in Using LLMs 499:3\nIn order to answer this general question, we formulate a set of more specific RQs:\nRQ1 What differences in learners‚Äô conversation patterns with an LLM are observed under varying\nguidance given to the learner?\nRQ2 What measurable impact do these guidance strategies have on learners‚Äô performance in\nlearning tasks while using the LLM?\nRQ3 What influence do various guidance strategies have on enhancing or reducing learners‚Äô\nconfidence in their ability to use LLMs for problem solving at each stage of interaction?\nRQ4 How do guidance strategies modify learners‚Äô trust in LLM responses through the course of\ndifferent interaction stages?\nRQ5 In what specific ways do different guidance methods shape learners‚Äô self-confidence in\nproblem solving across the various stages of interaction?\nTo gain a deeper understanding of student interactions with LLMs in educational contexts,\nin this paper, we distinguish three key elements of student-agent interactions: the Guidance\nReceived, which refers to the directions or suggestions students obtain for using LLMs; the\nLearner Approach, indicating whether learners first consult the LLM or attempt problem solving\nindependently; and the LLMs‚Äô Response, emphasizing the importance of dialogue quality with\nLLM. These components are intertwined, with shifts in one potentially influencing the others. We\nthen empirically investigated these influences through two user studies. In both studies, we explored\nfour variations of the first element, using four distinct guidance strategies: list-based suggestions,\nexample-based instruction, metacognitive questioning (making learners think about their use of\nLLMs before actual use) and a problem-solving-then-refinement approach (making learners solve a\nproblem first and then refine it with LLMs). These four guidance strategies manifested intwo distinct\nstudent approaches (second element): directly consulting the LLM or relying on self-first problem\nsolving. Finally, we manipulated the third element ‚Äì LLMs‚Äô Response ‚Äì through pre-interaction\nmanipulations [19, 65].\nAlthough there is previous research on student-AI interaction, LLMs seem to provide different\nlevels of interaction experience and convincingness, even in the presence of errors. Thus, studying\nthe multidimensional picture of student perceptions and their dynamics and analyzing them in a\nformative experimental setup, combining preliminary quantitative measures as well as extensive\nqualitative feedback is needed at this initial stage. Our formative study in an undergraduate\ncomputer science classroom (ùëÅ = 145) tasked students with solving an assignment while receiving\nhelp from an LLM-based chatbot. We explored the impact of our four guidance strategies, and the\nresulting two approaches, on the student‚Äôs performance (in the assignment, as well as in the final\nexam 2 weeks after the assignment), system perception (helpfulness, confidence in LLM‚Äôs response,\ntolerance of mistakes, willingness to interact again), and their self-assessment (change in their\nself-efficacy, confidence in their answers). We found a reduction in unrelated student queries to\nLLM with structured guidance such as List of Suggestions , Metacognitive Questioning, and Solve then\nrefine with LLM approaches, while Example based instruction paradoxically increased such queries.\nFurthermore, certain types of guidance promoted deeper engagement with the material, evidenced\nby fewer instances of verbatim question copying and increased rephrasing, highlighting the impact\nof guidance on the quality and focus of student-tutor interactions in LLM environments. As we\ndiscuss further, from a statistical point of view, this setting is traditional in its inherent limitations\nof sample size by course enrollment, student dropout, and selective engagement. It allows us to\nuse it only to gather formative data for different conditions. In addition, exposing students to\nexperimental stimuli allows us to gather qualitative feedback specific to different stimuli.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\n499:4 Harsh Kumar, et al.\nInformed by findings from formative deployment, we conducted a controlled study on Prolific\n[83] in which crowdworkers (N=356) were asked to solve math problems at the elementary and\nhigh school level with the help of an LLM chatbot. This math knowledge level was selected to\nensure a standardized baseline of complexity and familiarity for a broad adult audience, allowing\nfor a more controlled and consistent assessment of the LLM chatbot‚Äôs tutoring capabilities. Besides\nmanipulating the different forms of guidance given to students, the online controlled setting\nallowed us to compare a pre-prompted LLM with an unprompted LLM. Through a 2 (LLM Chatbot :\nPrompted vs. Unprompted) x 5 (Guidance strategy : G1-List of Suggestions vs. G2-Example-based\nInstruction vs. G3-Metacognitive Questioning vs. G4-Solve then refine with LLM vs. No Instruction),\nwe explored the impact of interaction strategies on the learners‚Äô performance and their confidence\n(in LLM‚Äôs responses, ability to use LLMs, and their Math-solving ability) across the different stages\nof interaction (before receiving guidance, immediately after receiving guidance, and after task\ncompletion). We did not find statistically significant differences in Learner + AI task performance\nacross different Guidance Types and LLM Types (prompted vs. unprompted), and we observe\ninteresting patterns in confidence in the ability to use LLMs to solve math problems and trust in\nLLM responses over different stages of interaction. The guidance briefly boosts confidence in the\nability to use LLM, but both measures significantly drop after learners attempt to solve real tasks\nusing LLM. This might suggest a partially negative interaction experience in solving tasks, as well\nas the need for additional learning support for LLM use in the particular context.\nOur work informs the design of learner-LLM interactions for collaborative learning environments,\nensuring the effective and empathetic use of LLMs in pedagogical contexts. In the remainder of this\npaper, we start (Section 2) by describing the related work done in incorporating LLMs in classroom,\nand highlight the need to explore different LLM interaction strategies. In Section 3, we describe the\ndesign space of learner-LLM interactions, and propose interaction designs that are evaluated in\nSections 4 and 5. We conclude by discussing our key findings in the context of existing literature\nand operationalizing them in a set of design considerations.\n2 RELATED WORK\nIn this section, we draw from recent HCI and CSCW research on leveraging conversational agents in\neducation, as well as literature from educational psychology on guidance strategies and self-efficacy\nto highlight a pressing need to study how learners interact with LLM-based conversational agents\nin real-world settings and how to guide them using pedagogically informed strategies.\n2.1 Understanding Learner-LLM Interactions in Real-World Settings\nWith LLM-based conversational systems like ChatGPT [2], Bard [3], and BingChat [1] already out\nin the wild and used daily by hundreds of thousands of learners, educators and researchers face\nan urgent need to understand how learners interact with these systems in real-world educational\nsettings. They are both thrilled [11, 34, 89] and concerned [34, 89] about the integration of LLM-\nbased chatbots into diverse learning environments as we navigate the uncertainties regarding the\nimpact of this rapidly evolving technology on pedagogy.\nThe growing body of recent CSCW and HCI literature on the incorporation of LLM-based\nconversational agents in education has explored various ways to enhance chatbots to assist learners\nin different ways, such as using them as persuasive agents to target behavior change through\nself-reflection [69], making them more goal-aware [ 31], proactive [ 70], and credible [ 21], and\ncustomizing them to take on different roles to facilitate different discussion patterns and system\nthinking [82]. However, while such works can provide valuable preliminary insights into how we\ncan design chatbots, they often do not investigate learner interactions in real-world settings. Given\nthat learners are already interacting with widely available and powerful LLMs through tools like\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\nGuiding Students in Using LLMs 499:5\nChatGPT, it has become increasingly important to understand how learners are interacting with\nthese tools. In this work, we offer empirical insights of classroom and crowdworker interactions with\nLLMs from two field studies that contribute to this understanding, and offer design recommendations\nthat can inform the design of future LLM-based chatbot implementations.\n2.2 Leveraging Guidance Strategies to Steer Interactions Toward Learning\nIn conjunction with investigating learner-LLM interactions, we also need to understand how to\nguide students in ways that support their learning rather than impede it. Guidance plays a crucial\nrole in helping users navigate how to best interact with chatbots, but the efficacy of specific forms\nof guidance remains underexplored [116].\nThe CSCW community has begun to explore different aspects of this important space. For\nexample, Wu et al. [116] studied user preferences for two different types of guidance (Example-\nBased and Rule-Based) at four different timings of guidance (Service-Onboarding, Task-Intro, After\nFailure and Upon Request) based on data from 24 participants, and found that users preferred\nExample-Based guidance, and wanted to see the guidance as part of Task-Intro. Zhu et al. [126]\nbuilt a chatbot prototype that provided task-based instructions to novice workers, and offered\nresults from a pilot study with 7 participants that indicated that back-and-forth conversations could\nhelp novice workers follow instructions, and perceive those instructions as more actionable. Our\nwork contributes to this body of literature on how different guidance strategies impact learner\nperceptions and interactions with LLM-based chatbots. A distinguishing feature of our work\nfrom existing preliminary studies is that we deployed our experiments in two distinct learner\ngroups with comparatively larger sample sizes (145 students in an undergraduate CS classroom\nand 356 crowdworkers from Prolific). We study the specific effects of four guidance strategies\n(providing instructions, presenting examples, using metacognitive questioning, and encouraging\ninitial problem solving with subsequent refinement).\nTo inform our selection of guidance strategies, in addition to looking at the types of guidance that\nhave been explored in preliminary work within the CSCW community, we also consulted previous\npedagogical literature on various forms of instructional guidance [15], and chose to study direct\ninstruction [112], example-based instruction [115], and metacognitive strategies [ 73, 99]‚Äîsuch\nas posing reflective questions [ 8, 39], due to their documented effectiveness in prior research\n[6, 10, 17, 46, 84, 107, 123].\nTo understand why guidance is of particular relevance to LLM-based chatbot design, we turn\nto recent research on chatbot interactions of novice AI users that show prompt-writing can be\ndeceptively difficult. The response from LLM-based chatbots is primarily steered by text-based\nprompts [117, 125]. However, recent research has indicated that crafting effective prompts is\nchallenging [122], particularly for those without a deep understanding of AI [ 5, 43, 122]. The\nconversational interface of these chatbots mimics human interaction [101], potentially misleading\nstudents into thinking that prompt-writing is as straightforward as talking to humans [53]. This\ngap between perceived simplicity and the actual complexity of effective prompt-writing can breed\noverconfidence. However, when the chatbot fails to respond as expected, students‚Äô trust in the\nAI system [32] can wane, leading to frustration [ 88]. In response to these challenges, informal\ncommunities like the r/aipromptprogramming and r/ChatGPT subreddits have emerged to share\nbest practices for prompt-writing. Simultaneously, researchers are starting to establish formal\nguidelines [33, 38, 72], but given the nascent nature of this field, much remains to explore. To\nimprove student success with LLM, it is necessary to convert these guidelines into a more digestible\nformat, thereby improving LLM literacy [120, 122].\nThis paper investigates the creation of these student-centric guidelines, translating them into clear\ninstructions for LLM use. More specifically, we evaluate how various guidance strategies‚Äîincluding\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\n499:6 Harsh Kumar, et al.\ndirect instruction, example-based teaching, metacognitive questioning, and worked examples‚Äîaffect\nstudents‚Äô performance, self-confidence, trust in the chatbot, and their perceptions of its efficacy.\n2.3 Understanding Factors that Influence Learners‚Äô Help-Seeking Decisions\nTo characterize how learner-LLM interactions may be shaped by important factors that influence\nlearner help-seeking decisions more broadly, we can turn to the educational psychology literature\non self-efficacy and perceived task difficulty. The choice of when to consult external tools during\nproblem-solving isn‚Äôt exclusive to the realm of LLMs. In broader educational research, students‚Äô\ndecisions to access resources‚Äîwhether they are textbooks, online forums, peers, or tutors‚Äîoften\ncome down to an interplay between self-efficacy, perceived task difficulty, and the accessibility of\nthe tool.\nSelf-Efficacy and Task Difficulty. Research has shown that learners with high self-efficacy and\n(over-) confidence can influence learning behavior [ 81, 108]. High self-efficacy students might\nfirst attempt to tackle problems independently. However, if they perceive the task as excessively\nchallenging or outside of their competence, they are more likely to consult external sources earlier\nin their problem-solving process.\nAvailability and Perceived Utility of the Tool. The ease of accessing a tool and the learner‚Äôs belief\nin its utility can impact when and how often they use it. For instance, the popularity of online\nforums in CS courses can be attributed to the instantaneous, community-validated feedback they\nprovide [47].\nBalancing Independence and Reliance. The idea of balancing self-reliance with tool reliance fits\nwell in the framework of self-regulated learning [ 56]. In many settings, students are trained to\nstrike a balance, ensuring that they understand the basic concepts, while not hesitant to seek help\nwhen needed.\nThe aforementioned considerations have nuanced significance in the context of LLMs. The\navailability and perceived efficacy of LLMs could encourage more immediate reliance, potentially\novershadowing traditional problem-solving approaches. Our paper‚Äôs exploration of different student\napproaches offers insight into these nuanced behaviors in the specific context of LLM interactions.\nWe factored such considerations into the design of our experiments. We included questions\nthat help us quantify such factors using pre- and post-measures regarding perception of LLMs\n(confidence in LLMs‚Äô responses, helpfulness, willingness to interact again, and error tolerance)\nand self-perception (student confidence in their answers, self-confidence for the given topic of the\nassignment). To measure long-term learning, we assessed their performance on the final exam on\nthe same topic, comprising isomorphic questions to those in the assignment.\n3 DESIGN CONSIDERATIONS FOR LEARNER INTERACTION WITH LLMS\nUnderstanding how students interact with LLM-based tools becomes imperative as language models\ncontinue to find their way into educational settings. In our exploration of learner interactions with\nLLMs, we identified three critical factors influencing these engagements:\n1. Guidance Received. The directions or suggestions that students receive can shape their entire\ninteraction experience. The form and content of this guidance can determine how students approach\nand utilize LLMs to their advantage (Section 3.1).\nMotivation: Recent research highlights the challenges in prompting LLMs for specific tasks [122],\nemphasizing the need for structured guidance to support effective use [5, 43]. This is supported\nby extensive literature that indicates that appropriate guidance can help learners form the correct\nmental models of tools [59], thus facilitating their proper use. Despite the critical importance of\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\nGuiding Students in Using LLMs 499:7\nguiding learners in using LLMs, this remains a largely unexplored problem (see Section 2.2). We\naddress this gap by exploring guidance strategies for LLM use.\n2. Learner Approach. Learners‚Äô initial inclination to consult the LLM immediately or first wrestle\nwith the problem alone can have implications for their learning process. The strategies they adopt\ncan be influenced by the guidance they have received and can vary significantly in efficacy (Section\n3.2).\nMotivation: Existing research indicates that individual factors such as self-efficacy, prior expo-\nsure, and perceived utility can influence users‚Äô approach when using tools [47, 81, 108]. Investigating\nthese approaches in the context of LLM tutors is important, as forming the appropriate reliance\nand dependency on these tools is necessary for effective learning [56] (see Section 2.3 for details).\nUnderstanding the approaches students take when using LLMs and how guidance can steer these\napproaches is vital. Some approaches may not be conducive to learning, such as using LLMs to\ncheat versus using them to seek feedback [64]. Hence, we consider the learner‚Äôs approach a crucial\ndesign consideration in our study.\n3. LLMs‚Äô Response. While guidance and approach set the stage, the actual dialogue with the LLM\nremains paramount. The quality, clarity, and relevance of the LLMs‚Äô responses can make or break\nthe learning experience (Section 3.3).\nMotivation: Research suggests that the accuracy and framing of responses by conversational\nagents can significantly influence user perception and interaction with the agent [35, 118]. This is\nparticularly crucial in educational settings where how answers are framed can impact long-term\nlearning outcomes [110], even if the immediate effect seems beneficial (e.g., students using LLMs\nfor cheating) [ 64]. By exploring the steering of LLM responses, we can gain insights into the\ndifferences between publicly available LLMs, such as ChatGPT, and those configured by instructors.\nAdditionally, the nature of LLM responses can influence learners‚Äô trust in the LLM and their sense\nof self-efficacy [86, 92], as direct answers might lead to a realization of lacking knowledge [121].\nThese factors prompted us to consider the types of LLM responses as a design element in this work.\nEach of these components does not exist in isolation but intertwines and influences the others. It\nis a dynamic relationship where modifying one aspect might lead to shifts in the others. Grasping\nthese intricate relationships is key for educators, designers, and policy makers who want to harness\nthe full potential of LLMs in the learning environment.\n3.1 Guidance Strategies for LLM Engagement\nIn Section 2.2, we discussed how our guidance strategies drew insights from previous work in\nthe pedagogical literature [6, 10, 15, 17, 46, 84, 107, 123]. In this section, we revisit some of those\ninsights and describe the specifics of our design and the rationale behind each guidance strategy.\nFigure 1 shows the format of each guidance strategy used in our study.\nG1: List of Suggestions We devised a set of suggestions for the use of LLM chatbots, as depicted in\nFigure 1A. Given the limited existing research on LLM guidance for students, our suggestions\nwere based on prominent online sources [78, 80, 85]. Recognizing the growing classroom\ntrend of prescribed LLM instructions [4, 79], G1 serves as a representative model to evaluate\nits efficacy against other guidance strategies.\nG2: Example-based instruction Leveraging examples is an intuitive approach to tool introduc-\ntion [68]. However, the choice of examples plays a crucial role. For our classroom deployment,\nwe took examples of interactions from previous interview studies with students [63, 119].\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\n499:8 Harsh Kumar, et al.\nFig. 1. Different forms of guidance strategies.\nThese examples, validated by an instructor and a teaching assistant, showcased interac-\ntions where students found the LLM beneficial. Participants were presented with two such\nexemplary interactions, as illustrated in Figure 1B.\nG3: Metacognitive questioning (making learners think about their use of LLMs before actual\nuse). Framing the initial question effectively is crucial when seeking help, more so with LLM\nchat tools where the initial query shapes subsequent responses [29, 122]. To enable this skill,\nparticipants were presented with an example problem and prompted to draft their opening\nquestion to the LLM chatbot. Subsequently, they reflected on the potential benefits of their\nchosen approach (Figure 1C).\nG4: Solve then refine with LLM Worked examples, step-by-step illustrations of the process re-\nquired to complete a task, are established effective learning tools [ 103]. In this multistep\nstrategy, we first ask the students to solve a problem and provide the solution. They then\nengaged with the LLM chatbot to refine their initial answers (Figure 1D).\n3.2 Approaches to Leverage LLMs in Problem Solving\nThe manner in which students interact with LLM-based tools is often influenced by the guidance\nor instructions they receive. The sequence‚Äîwhether students consult the LLM first or try to solve\nproblems on their own‚Äîhas implications for both their learning process and the effectiveness of\nthe LLM as a tool. This builds on research in \"when\" is it better to provide hints for learning [90].\nLLM-First Approach (LFA): Students begin by seeking input from the LLM. This approach\ncan assist those who need a foundation or direction before delving into problem-solving, ensuring\nthey are on the right track from the onset. However, it may also promote dependency on the LLM,\npotentially limiting independent problem-solving capabilities.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\nGuiding Students in Using LLMs 499:9\nSelf-First Approach (SFA): Students first attempt to solve the problem independently and\nthen consult the LLM for refinements. This method promotes independent thinking and can boost\nconfidence in one‚Äôs problem-solving abilities. The LLM acts as a secondary check, reinforcing\ncorrect methodologies or suggesting improvements. Conversely, there might be a risk that students\nget stuck or frustrated if their initial attempts are misdirected.\nGuidance strategies, as detailed in Section 3.1, influence these approaches. For example, G4\nexplicitly steers students towards the SFA, while G1-3 offer more flexibility in how students choose\nto engage with the LLM.\n3.3 Response from Large Language Models\nThe response generated from proprietary models is perhaps the most difficult to control in our\nstudy, due to ever-evolving (behind-the-scene) updates to the models [22]. However, the response\nof a Prompted LLM definitely differs from an Unprompted LLM. A few-shot-prompted LLM\ncan achieve better performance in the task at hand (helping students solve problems) than an\nuninitiated one [19].\nTo explore this design space, we conducted two studies, one in an upper-year computer science\nclassroom teaching students database-related concepts and a second in an online controlled setting\nwith crowdworkers solving mathematics problems. Figures 2 and 4 show a high-level overview of\nboth study designs. They are described in detail in the following sections.\n4 STUDY-1: A FORMATIVE FIELD STUDY IN A CS CLASSROOM\nThis research study was approved by the local ethics board. The data used in this deployment was\nobtained from a third-year Introduction to Databases course (DB) at a large research-focused North\nAmerican university in Winter 2023 for Computer Science (CS) students. This course spanned 12\nweeks, employing a flipped classroom setup (students were required to complete preparation work:\nvideo modules, technical questions, and self-reflections, prior to attending lectures in person using\na learning management system). DB had one course instructor, eleven teaching assistants, and\nwas an upper-year elective course for Computer Science students. The majority of students were\nComputer Science majors, however, some are Computer Science minors. Although natural classroom\nlimitations impose constraints on achievable statistical power and, as a result, the complexity of\nquestions we can reliably answer with quantitative methods, we used this deployment as a formative\none, focusing on both acquiring statistical evidence where it is reliable and exploratory analysis\nwithout statistical conclusions to inform larger-scale deployments (both in the virtual lab (Study 2,\nsee Section 5) and in the field (future studies)).\nWe designed a randomized factorial experiment, for the guidance given to the student, with 2\n(G1-List of Suggestions : present vs. absent) x 2 (G2-Example-based instruction : present vs. absent) x\n2 (G3-Metacognitive questioning : present vs. absent) x 2 (G4-Solve then refine : present vs. absent)\nbetween subjects. The presence of G4 encouraged a Self-First Approach (SFA), while the absence\nprovided more flexibility (LLM-First Approach ‚Äì LFA) (see Section 3). At the beginning of the\nassignment, students were primed to use LLM-based support tool with the following:\n‚ÄúYou will now get some instructions related to the use and access of the dialogue-based\nsupport system. The system uses a fine-tuned large language model. Large language\nmodels (LLMs) are artificial intelligence systems that use deep learning to analyze vast\namounts of text and generate responses that are contextually relevant. ‚Äù\nAfter seeing this text, students were randomly assigned to one of the forms of instruction composed\nof factors described in Section 3. After receiving guidance and access to the LLM chatbot, students\nattempted to solve multiple-choice problems (related to databases) while receiving support from\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\n499:10 Harsh Kumar, et al.\nFig. 2. Schematic of the formative study in a CS classroom. 2 ( List of Suggestions: present vs. absent) x 2\n(Example-based Instruction: present vs. absent) x 2 (Metacognitive-questioning based Instruction: present vs.\nabsent) x 2 (Solve, then refine with LLM: present vs. absent) between subjects.\nLLMs. Besides accuracy in the assignment, we collected pre- and post-measures regarding perception\nof LLMs (confidence in LLMs‚Äô responses, helpfulness, willingness to interact again and error\ntolerance) and perception of self (confidence in their answers, self-confidence for the given topic of\nassignment). To measure long-term learning, we measured their performance on the final exam on\nthe same topic, comprising isomorphic questions to assignment problems.\n4.1 LLM-based chat support tool\nEach student was given access to a chatbot tutor, based on GPT-3 (described in Section A.1.1), to\nhelp them solve assignment problems. The model prompt was designed after careful iterations\nbetween the researchers, the instructor, and the teaching assistant. The prompt given to the model\nwas the following:\n\"The following is a conversation with a database instructor. The instructor helps the human\nsolve assignment problems related to database. The instructor never explicitly gives the\nsolution. The instructor also never writes the SQL query. The instructor would only provide\nbrainstorms to possible solutions without providing any SQL statements. This rule should be\nenforced in the entirety of the conversation. Following are the DDL files to create table given\nas part of the assignment. \"\n4.2 Participants\nThere were 218 students enrolled in DB, of which a total of 145 students (67%) completed the\nassignment. When students were asked about their familiarity with Large Language Models (LLMs),\n51.03% reported occasional use, 16.55% were regular users, 15.86% had tried them once, and 16.55%\nhad never used them before. On a scale of 1 to 7, the students‚Äô average initial self-efficacy for the\ntopic was 4.10 (SD = 1.38).\n4.3 Results\n4.3.1 Changes in interaction patterns based on guidance type. We analyzed the first query each\nstudent sent to the LLM tutor. The goal was to understand how students used LLMs to solve\nproblems and how the guidance received affected the way students used the LLM (RQ1). Of the\n145 students who completed the assignment, 138 students used LLMs. We followed an iterative\ncoding scheme to categorize each of the first queries from the students [27]. Two members of the\nresearch team went through the entire dataset and after multiple rounds of discussions, arrived\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\nGuiding Students in Using LLMs 499:11\nTable 1. Categories of first query asked to the LLM Tutor. The assignment was on the topic of serializability\nin databases. The first question in the assignment began with \"For each of the following locking protocols (a to\nd below)... \"and asked students to select the properties that were ensured for the given protocols. Majority of\nstudents asked for clarifying specific doubts related to the assignment, while many students also copy-pasted\nthe given assignment problem.\nCategory Description Example from Dataset % of\nStu-\ndents\nVerbatim First As-\nsignment Question\nThe student copies the exact question from the\nassignment verbatim as the first message to the\nLLM tutor.\n\"For each of the following locking protocols (a to\nd below), assuming that every transaction follows\nthat specific locking protocol... \"\n28%\nFirst Assignment\nQuestion Reworked\nThe student rephrases or alters the wording of the\nfirst question from the assignment, maintaining\nits original intent.\n\"Given the following, always obtain an exclusive\nlock before writing; hold exclusive locks until the\nend of transaction. No shared locks are ever obtained.\nWhich properties are ensured?\"\n14%\nUnrelated Initial In-\nquiry/Chatbot Test-\ning\nThe student‚Äôs first query is unrelated to the as-\nsignment, often to test the chatbot‚Äôs capabilities\nor driven by general inquisitiveness.\n\"Hi buddy, recite a Shakespearean sonnet... \"4%\nInitial Conceptual\nClarification Request\nThe student seeks to understand specific concepts\nor parts of the assignment more clearly with their\ninitial query.\n\"How do I read the Venn diagram for schedules?\"54%\nat four distinct categories as shown in Table 1. After this, the first coder categorized the entire\ndataset. The second coder independently labeled a random (ùëõ = 70)subsample., resulting in high\ninter-rater agreement (Cohen‚Äôs Kappa = 0.814, ùëç= 10.7, ùëù< 0.001).\nWe examined the impact of each guidance type (G1‚ÄìG4) on the nature of the first queries made\nby students to the LLM tutor. For each guidance type, students were grouped into two categories:\nthose who received the guidance and those who did not. We then analyzed the first queries made by\nthe students in each group to see how often they fell into each of the established query categories\n(Figure 3). This allowed us to compare the tendencies of the students‚Äô initial queries on the basis\nof whether they had been exposed to a particular type of guidance. We find that when G1-List of\nSuggestions, G3-Metacognitive Questioning and G4-Solve then refine with LLM are absent, students\ntend to ask unrelated questions more, compared to when it is present. On the other hand, the\npresence of G2-Example based instruction resulted in more students asking unrelated questions, or\ntrying to break the chatbot. G1, G2, and G4 show a positive effect in reducing the likelihood of the\nstudent copy-pasting the exact assignment question. However, G3 showed the opposite effect. The\nintroduction of G3-Metacognitive Questioning resulted in a higher percentage of students rephrasing\nthe first assignment question, while the absence of G4 had the same effect. For G4-Solve then refine\nwith LLM , we find that students who received this particular guidance asked clarification questions\nmore as their first query, in line with what was instructed to them as part of this strategy.\nBy aligning the types of guidance with the classified first queries, we were able to identify\ntrends in student interactions. This analysis not only contributed to a deeper understanding of\nthe immediate impact of guidance on student queries, but also offered insights into the broader\nimplications of guidance strategies in shaping student learning experiences with LLMs.\n4.3.2 Students‚Äô attitudes towards LLMs. Students rated their average initial trust in LLMs at 3.58\nout of 7 (SD = 1.54). Previous experience of LLM use is connected with the initial trust in LLM\n(ùúí2 = 18.186, ùëë ùëì = 6, ùëù < 0.01), with students having regular experience with LLMs reporting\nmore trust in them. The overall difference in trust towards LLMs (before the experiment) and on\nLLM chatbots (after the experiment) across balanced experimental conditions shows a significant\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\n499:12 Harsh Kumar, et al.\nFig. 3. Distribution of First Query Categories Based on Guidance Exposure. The bars represent the percentage\nof students whose first query falls into each category, segmented by whether they received a specific type of\nguidance (G1-G4). The percentages are calculated relative to the total number of students in each category.\nThis visualization helps identify the influence of each guidance type on the students‚Äô initial interaction with\nthe LLM. A percentage around 50% indicates that the presence or absence of the guidance type had little to\nno discernible impact on the likelihood of a student‚Äôs first query falling into that category.\ngrowth trend (from Medianùëèùëíùëì ùëúùëüùëí = 4 to Medianùëéùëì ùë°ùëíùëü = 5 on 7-point Likert scale, V (Wilcoxon) =\n1421.00, ùëù < 0.001)). This gives us insight into the learners‚Äô trust in LLMs (RQ4).\n4.3.3 Performance measures (RQ2). For Homework Score, we see a contrast between G4: Solve-\nThen-Refine (focusing on getting them to solve a problem first, then go to the LLM with questions),\nvs. G1, G2, G3 (which focused on using the LLMs while solving problems). The data suggest that G4-\nSolve-Then-Refine (encouraging self-first approach) may not have helped homework performance\nas much compared to the other forms of guidance (see Table 3).\n4.3.4 Perception of system. On average, we observe a small increase in \"confidence in the ability of\nLLM to help\" across different forms of guidance strategies and use approaches (RQ4). All conditions\nreport moderately high ratings for \"Helpfulness\" for the given topic. However, they report it to\nbe even higher on average for \"Helpfulness for other topics\", with the presence of G4-solve-then-\nrefine standing out for being particularly helpful in general. The data suggest that the presence of\nG3-Metacognitive-Questioning reduced tolerance for mistakes and willingness to interact again.\n4.3.5 Perception of self. There is negligible change in the student‚Äôs self-confidence (their confidence\non the given topic) and this is consistent across different conditions. The students‚Äô confidence in\ntheir answers is similar across different conditions, however, G4-Solve-then-refine (or encouraging\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\nGuiding Students in Using LLMs 499:13\nCategory Measure Code Range\nPerformance Homework Score HS 0 - 100\nExam Score ES 0 - 100\nScore Diff SD (-100) - 100\nPerception of System Helpfulness H 1 - 7\nHelpfulness for Other Topics HOT 1 - 7\nTolerance for Mistakes TM 1 - 7\nWillingness to Interact Again WIA 1 - 7\nChange in Confidence in\nthe LLM‚Äôs Ability to Help\nCCLAH (-14) - 14\nPerception of Self Change in Self-confidence CS (-14) - 14\nConfidence on Their Answers CTA 1 - 7\nTable 2. List of outcome measures with their scales in Study-1. We assign a code to each measure so that\nthey are easy to reference at different points in the paper.\nCode G1 G2 G3 G4\nAbsent Present Absent Present Absent Present Absent Present\nHS 58.69(¬±2.09) 59.46(¬±1.85) 57.52(¬±1.81) 60.62(¬±2.10) 57.63(¬±1.87) 60.47(¬±2.05) 61.38(¬±1.83) 56.87(¬±2.06)\nES 56.20(¬±1.52) 54.80(¬±1.70) 53.61(¬±1.74) 57.33(¬±1.45) 55.49(¬±1.79) 55.47(¬±1.44) 55.00(¬±1.43) 55.95(¬±1.78)\nSD ‚àí2.49(¬±2.34) ‚àí4.66(¬±2.23) ‚àí3.91(¬±2.14) ‚àí3.29(¬±2.42) ‚àí2.14(¬±2.19) ‚àí5.00(¬±2.37) ‚àí6.38(¬±1.97) ‚àí0.92(¬±2.51)\nH 4.51(¬±0.22) 4.82(¬±0.16) 4.56(¬±0.19) 4.78(¬±0.19) 4.70(¬±0.19) 4.64(¬±0.20) 4.54(¬±0.19) 4.80(¬±0.20)\nHOT 5.17(¬±0.21) 5.28(¬±0.16) 5.17(¬±0.18) 5.29(¬±0.19) 5.32(¬±0.18) 5.14(¬±0.19) 5.00(¬±0.19) 5.45(¬±0.18)\nTM 4.32(¬±0.22) 4.18(¬±0.20) 4.29(¬±0.21) 4.21(¬±0.22) 4.62(¬±0.20) 3.89(¬±0.22) 4.24(¬±0.20) 4.26(¬±0.22)\nWIA 5.25(¬±0.22) 5.34(¬±0.18) 5.22(¬±0.21) 5.37(¬±0.19) 5.54(¬±0.19) 5.07(¬±0.20) 5.13(¬±0.20) 5.46(¬±0.20)\nCCLAH 1.08(¬±0.2) 0.77(¬±0.22) 1.07(¬±0.22) 0.78(¬±0.20) 0.70(¬±0.21) 1.14(¬±0.20) 0.80(¬±0.19) 1.04(¬±0.23)\nCS ‚àí0.04(¬±0.16) 0.49(¬±0.20) 0.06(¬±0.17) 0.40(¬±0.19) 0.41(¬±0.17) 0.05(¬±0.19) 0.21(¬±0.18) 0.24(¬±0.18)\nCTA 3.86(¬±0.19) 3.78(¬±0.16) 3.78(¬±0.18) 3.86(¬±0.16) 3.80(¬±0.17) 3.84(¬±0.18) 4.01(¬±0.16) 3.64(¬±0.18)\nTable 3. Summary of results for Study-1‚Äôs classroom deployment. The short codes for outcome measures\nused here are defined in Table 2.\nself-first approach) stands out - the absence of G4 in the overall guidance resulted in relatively\nhigher confidence in their answers (RQ5).\nThe findings from our formative deployment established the potential for differences between\ndifferent guidance strategies (particularly between G4 and other strategies) and the resulting\napproach (self-first approach) to influence different measures related to performance and confidence.\nWe further evaluate the effect of different interaction strategies in the following section. One key\ntakeaway from the formative deployment was to introduce intermediate measures immediately\nafter giving guidance to the learner.\n5 STUDY-2: SOLVING MATH PROBLEMS IN AN ONLINE CONTROLLED SETTING\nTo further explore the design space of the LLM-learner interaction, we designed a 2 (LLM Chatbot :\nPrompted vs. Unprompted) x 5 (Guidance strategy : G1-List of Suggestions vs. G2-Example-based\nInstruction vs. G3-Metacognitive Questioning vs. G4-Solve then refine with LLM vs. No Instruction)\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\n499:14 Harsh Kumar, et al.\nFig. 4. Schematic of the experiment design for Study 2 with crowdworkers. 2 ( LLM Chatbot : Prompted\nvs. Unprompted) x 5 (Guidance strategy: G1-List of Suggestions vs. G2-Example-based Instruction vs. G3-\nMetacognitive Questioning vs. G4-Solve then refine with LLM vs. No Instruction) between subjects for the\ntype of chatbot and guidance received.\n(Figure 4) randomized controlled trial. Upon consenting to participate in the experiment, participants\nwere asked to report their confidence in their ability to solve math problems, how frequently they\nused LLMs, and their confidence in using and trusting the responses generated by LLMs. The\nparticipants were then randomized to access a chatbot, either with or without a preprompt (detailed\nin Section 5.3). They received one of four guidance strategies (explained in Section 3) or a basic\ndescription of how LLMs work (no instruction). After solving a common example problem with\nthe LLM chatbot, they rated their confidence in using and trusting LLMs. Subsequently, they\nsolved 4 math problems (randomly selected from a pool of 8 problems) with support from the\nLLM chatbot (Section 5.2). Finally, they reported their final confidence in their ability to solve\nmath problems, their confidence in using and trusting LLM responses. The experiment flow just\ndescribed is summarized and visualized in Figure 4.\n5.1 Participants\nWe recruited 356 participants, fluent in English, from Prolific [83] 1. We did not have any special\nrecruitment criteria other than fluency in English. Table 4 describes the participants.\n5.2 Task: Answering multiple choice math questions\nEach participant tackled two elementary-level math questions and two high-school-level math\nquestions. The sequence of these tasks was randomized for each individual. From the MMLU\nbenchmark dataset [51, 52], we randomly chose 4 questions each from the elementary and high\nschool math levels. The participants were then tasked with solving two questions (randomly\nsampled) from each level.\n5.3 Steering LLM Responses with Pre-Prompts: Prompted vs. Unprompted LLM Chatbot\nIn the controlled setting, we also wanted to explore the differences between an LLM that was\nprovided with vs without a system-prompt. The distinction is crucial for several reasons:\n‚Ä¢Real-world relevance: The unprompted version mirrors common LLM chatbots such as\nChatGPT, representing the typical interaction users might have with publicly available LLMs.\n‚Ä¢Educational context: A prompted LLM, on the other hand, could resemble a scenario in\nwhich instructors guide the model to provide specific types of response. This distinction was\nespecially relevant as our classroom experiment (Section 4) could not employ unprompted\nLLMs due to ethical considerations.\n1Prior research indicates that involving crowdworkers in user studies is effective [61]. It has been established that data\nobtained from crowdsourcing platforms can match the quality of traditional methods, including those involving undergrad-\nuate students and community samples [44]. Furthermore, crowdworkers have demonstrated their reliability as stand-ins for\nonline learners in various contexts [30].\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\nGuiding Students in Using LLMs 499:15\nCategory Results\nInitial self-\nconfidence in\nsolving math prob-\nlems\n1 (1.40%), 2 (3.93%), 3 (8.15%), 4 (18.26%), 5 (28.93%), 6 (22.75%), 7 (16.57%)\nwhere 1 is ‚Äúnot confident at all‚Äù and 7 is ‚Äúextremely confident‚Äù\nPrior use of LLMs Never (23.88%), Once (20.51%), Occasionally (34.55%), Regularly (21.07%)\nAge 18-25 (43.26%), 25-34 (41.57%), 35-44 (10.96%), 45-54 (2.81%), 55-64 (1.12%), 65+\n(0.28%)\nGender Male (47.75%), Female (49.44%), Non-binary / third gender (1.97%), Prefer not to\nsay (0.84%)\nEducation Some high school or less (2.25%), High School graduate (10.96%), Some college\ncredit with no degree (23.31%), Associate degree (4.49%), Bachelor‚Äôs degree\n(41.85%), Graduate or Professional degree (14.89%), Prefer not to say (2.25%)\nTotal Participants = 356\nTable 4. Demographic information of the participants recruited for Study-2 on Prolific.\nOur design probe was a GPT-3-based chat interface (see Section A.1.1), representative of other\nchat-based LLM tutors. Below, we detail the two distinct variants of LLM chatbots used in our\nstudy:\n5.3.1 GPT-3 with Pre-Prompt. This variant was introduced to explore whether guiding the model\nwith specific pre-prompts can offer advantages over generic out-of-the-box LLMs. We used the\nfollowing pre-prompt for GPT-3:\n\"You are a professional K12 math teacher helping students answer math questions.\nGive students explanations, examples, and analogies about the concept to help them under-\nstand. You should guide students in an open-ended way. Make the answer as precise and\nsuccinct as possible.\nYou should help them in a way that helps them (1) learn the concept, (2) have confidence in\ntheir understanding, and (3) have confidence in your ability to help them. Before answering,\nreflect on how your answer will help you achieve goals (1), (2), and (3). Update your answer\nbased on this reflection. \"\n5.3.2 GPT-3 without Any Pre-Prompt. This unprompted version served as a control, allowing us to\ncontrast its performance and responses with the prompted version.\n5.4 Results\n5.4.1 RQ2: Performance (Learner + AI performance in the task). The performance (average score in\nthe task) was not significantly different between different types of guidance (ùúí2\nKruskal-Wallis(4)= 0.13,\nùëõ = 356, ùëù = 1) or whether the LLM was prompted or unprompted (ùëäMann-Whitney = 15025, ùëõ = 356,\nùëù = 0.37).\nExploratory analysis of subgroups (see Figure 5) suggest that guidance strategies G2 (exam-\nples) and G3 (Metacognitive Questioning) appeared to improve performance when paired with\nunprompted LLM. This suggests that these guidance approaches may be effective in counterbal-\nancing the absence of a pre-prompt given to the LLM. Specifically, the use of examples (G2) with\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\n499:16 Harsh Kumar, et al.\nFig. 5. Performance (operationalized as accuracy in the 4 questions) across the different conditions of the\nexperiment.\nunprompted LLM achieved an average score of68.13% ¬±2.68%, compared to 58.09% ¬±4.32% obtained\nwith prompted LLM. A similar trend was observed for G3 (Metacognitive Questioning) across the\ntwo LLM conditions, calling for further investigation of the potential advantages of these guidance\nstrategies when paired with an unprompted LLM.\n5.4.2 RQ3: Confidence in ability to use LLMs to solve math problems over different stages of interaction\n(Figure 6). The confidence is measured in three stages in the study in response to the prompt ‚ÄúI\nfeel confident in my ability to use Large Language Models (LLMs) to assist me in solving math. ‚Äù . We\nmeasure this at the start of the study, immediately after receiving guidance and then after the\nproblem-solving task. General confidence is moderately high, with the median being 4 out of 7.\nOn average, across conditions, we observed significant differences in self-reported confidence in\nuse of LLMs (ùúí2\nFriedman (2)= 72.82, ùëõpairs = 356, ùëù < 0.001), with growth (ùëù < 0.001) between initial\nconfidence (Mean = 4.51, Me = 5, SD = 1.49) and immediately after getting guidance (Mean = 5.19,\nMe = 6, SD = 1.66), and a significant drop (ùëù < 0.001) after use (Mean = 4.57, Me = 5, SD = 1.81).\nThe first part of this pattern suggests that people after receiving guidance, increase their confidence\n(as opposed to judging interaction as too complex), but after acquiring more experience in solving\nreal tasks, their confidence slightly decreases. The reversion of the initial confidence level after\nthe task is interesting, as it suggests that while guidance may temporarily boost confidence, the\npractical challenges of applying knowledge during problem-solving can temper this heightened\nconfidence. Further exploratory analysis suggests that this pattern occurs for all the different\nforms of guidance, and even when there isn‚Äôt guidance being provided, for the Prompted and the\nUnprompted LLM.\nWithin the different conditions, these general trends are observed with varying degrees of\nintensity (see Figure 6). For instance, the condition where the LLM is Prompted with G4: Solve,\nthen Refine with LLM guidance shows the most pronounced retention of confidence after the\nproblem-solving task. On the other hand, the Unprompted LLM condition with G2: Example-\nbased Instruction shows the least initial boost in confidence post-guidance. These observations\nsuggest potential nuanced interplays between LLM prompting and the guidance type in influencing\nlearner confidence.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\nGuiding Students in Using LLMs 499:17\nFig. 6. This grid visualizes the participants‚Äô average confidence in their ability to use LLMs across different\nexperimental conditions. Each cell represents a combination of LLM prompting (either \"Prompted\" or \"Un-\nprompted\") and a specific guidance type. The gray dotted line in each cell represents the overall average\nconfidence across all conditions, providing a point of reference. The overarching trend showcases varying\nlevels of confidence shifts across different conditions.\n5.4.3 RQ4: Trust in LLM‚Äôs responses over different stages of interaction (Figure 7). On average, across\nconditions, we observed significant differences in reported trust in LLM responses (ùúí2\nFriedman (2)=\n80.31, ùëõpairs = 356, ùëù < 0.001).\nWhile the growth between initial trust ( Mean = 4.36, Me = 4, SD = 1.36), immediately after\ngetting guidance (Mean = 4.35, Me = 5, SD = 1.89) is not significant, there is a significant decline\nin final post-experience trust to LLM measure (Mean = 3.49, Me = 3, SD = 1.80) both compared\nto initial (ùëù < 0.001) and after instructions (ùëù < 0.001) measurements. This sharper decline might\nsuggest that participants experience untrustworthy advice from LLM.\nFurther exploratory analysis within the different conditions generally confirms this observed\ntrend (refer to Figure 7). For example, in conditions where the LLM is Unprompted with No\nInstruction guidance, the decline in confidence after the task is the highest. However, there are\nexceptions. For instance, the Unprompted LLM condition with G2: Example-based Instruction\nshows the most significant decline in confidence immediately post-guidance, suggesting that certain\nguidance types might not instill trust in the LLM‚Äôs responses.\n5.4.4 RQ5: Changes in self-confidence in solving math problems before and after the problem-solving\ntask with LLM (Figure 8). Our analysis reveals that participants‚Äô confidence in their math-solving\nabilities remains relatively stable throughout the interaction (see Figure 8). The average confidence\nbefore receiving any guidance is5.04 (SD = 1.41). After they complete the problem-solving task, this\nconfidence is 5.16 (SD = 1.62). The observed difference between these two stages is not statistically\nsignificant with ùëù = 0.079.\nWhen exploring the individual experimental conditions, this stability in confidence is largely\nconsistent. For instance, under the condition where the LLM is Unprompted and the guidance\nprovided is G3: Metacognitive Questioning , we observe a slight increase in confidence. On the\nother hand, the Unprompted LLM condition combined with G2: Example-based Instruction\nguidance shows a minor decrease in post-task confidence. However, it‚Äôs worth noting that these\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\n499:18 Harsh Kumar, et al.\nFig. 7. This grid illustrates the average confidence of participants in trusting LLM responses across different\nexperimental conditions. Each cell showcases a combination of LLM prompting and guidance type. The gray\ndotted line in each cell represents the overall average confidence across all conditions, providing a point of\nreference. The general trend indicates a decline in trust across most conditions, with a few exceptions.\nFig. 8. The grid displays the average confidence of participants in their math-solving abilities across distinct\nexperimental conditions. Each cell represents a combination of LLM prompting and a specific guidance type.\nThe gray dotted line in each cell represents the overall average confidence across all conditions, providing a\npoint of reference. The confidence levels across conditions appear to be stable with minor fluctuations.\nvariations might not necessarily imply a significant impact on the participants‚Äô self-assessment of\ntheir math-solving abilities.\n6 DISCUSSION\nThrough the two studies, we investigated the effects of different guidance strategies on the learning\nprocess of students supported by an LLM tutor. In our discussion, we first summarize the key\nempirical findings and emphasize our contribution as it relates to design of Learner-LLM systems.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\nGuiding Students in Using LLMs 499:19\nWe then provide some design considerations based on our findings, after which we conclude with\nlimitations of our work and possible future work.\n6.1 RQ1: Impact of Guidance on Initiating Dialogue with LLM Tutor\nIn our Formative Study, we observed distinct patterns in how students initiated dialogues with\nthe LLM tutor, influenced by different types of guidance (G1‚ÄìG4). Firstly, we noted a tendency\nfor students to pose unrelated questions more frequently in the absence of G1-List of Suggestions ,\nG3-Metacognitive Questioning , and G4-Solve then refine with LLM . This could be interpreted in the\nlight of existing research, which emphasizes the importance of structured guidance in collabora-\ntive environments [60, 66, 116]. The presence of structured guidance appears to channel student\ninteractions more effectively towards relevant problem-solving. Conversely, the introduction of\nG2-Example based instruction led to an increase in unrelated questions or attempts to break the\nchatbot. When students are presented with examples, they might test the system‚Äôs capabilities\nbeyond the task‚Äôs scope, reflecting a curiosity-driven exploration [48, 104].\nInterestingly, G1, G2, and G4 were effective in reducing instances of students copy-pasting\nassignment questions directly, aligning with educational theories that suggest guided learning en-\ncourages deeper processing of information [20, 77]. However, the introduction of G3-Metacognitive\nQuestioning showed an opposite effect, leading to more rephrasing of assignment questions. This\naligns with education literature that highlights the role of metacognition in encouraging students\nto engage more deeply with the material, potentially prompting them to rephrase questions in their\nown words [73, 99]. The guidance G4-Solve then refine with LLM led to an increase in clarification\nquestions as initial queries, consistent with the instruction given. This observation resonates with\nthe principle of task-oriented communication, where specific guidance can direct collaborative\nefforts towards more productive and goal-oriented dialogue [24].\n6.2 RQ2: Immediate Performance and Potential Long-term Learning with LLMs\nOur formative study highlighted a potential design tension in LLM research: the balance between\nproviding immediate assistance and ensuring long-term skill development for students. This fits\ninto the long line of work studying the effects of automated hints on learner performance and\nlearning [28, 45, 76, 93, 94]. An interesting observation in the formative study was the contrast\nbetween G4: Solve-Then-Refine (focusing on getting them to solve a problem first, then go to the\nLLM with questions), vs G1, G2, G3 (which focused on using the LLMs while solving problems).\nThere is suggestive evidence that the presence of G4-Solve-Then-Refine (encouraging self-first\napproach) may not have helped homework performance as much compared to the other forms of\nguidance (refer to Table 3). However, the performance in the final exam without any LLM support\n(considered as a measure of learning) was similar between the different forms of guidance and\nlearner approaches. A recent large-scale controlled study by Kumar et al. [64] showed that when\nlearners received explanations from GPT-4, the long-term benefits were greater for those who\nattempted problems on their own first before consulting LLM explanations.\nFor Study-2, we did not find any significant differences between the guidance strategies or\nchatbot types on average, in terms of performance on the math problem-solving task with LLM\n(see Figure 5). However, exploratory analysis looking at groups of Prompted vs. Unprompted LLM\nsuggested that G2 (examples) and G3 (metacognitive questioning) improved performance when\npaired with unprompted LLM. This suggests that the right student guidance may offset the lack of a\npre-prompt to the LLM. This finding is crucial when students independently use publicly available\nLLMs like ChatGPT. Instructors may guide this independent use by showing proper examples or\nencouraging deeper thinking about LLM use.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\n499:20 Harsh Kumar, et al.\n6.3 RQ3 & RQ4: Learner Confidence in Using LLMs and Trusting LLM Responses Across\nDifferent Stages of Problem-Solving Tasks\nFormative study suggested a small increase in the confidence of the students in the ability of\nLLM to help, after solving assignment problems with LLM. However, there were no significant\ndifferences between the different guidance strategies and approaches of LLM use. In Study-2, when\nwe looked at participants‚Äô confidence in using LLMs, we observed an interesting pattern. After\nreceiving the guidance, the confidence of the people initially increased. However, as they gained\nmore hands-on experience solving tasks with LLMs, their confidence dipped slightly. This poses\nquestions for further research: Is this confidence drop a moment of realization about the challenges\nof seeking help, be it from LLMs or others? Might this realization push them to invest more effort\nin mastering the skill? Does the drop indicate a need for better guidance and support to ensure that\nusers continue to take advantage of these tools for learning and assistance? Their trust in LLM‚Äôs\nresponses showed a similar trend. Although the growth between initial trust and immediately after\nreceiving guidance is not significant, there is a significant decline in final post-task self-reported\nconfidence in trusting LLM‚Äôs responses. There could be complementary mechanisms behind this:\nPeople are either getting incorrect or misleading responses, or the responses are just not very\nhelpful. This trend could also have been influenced by a mismatch in user expectations, often\nshaped by their familiarity with human-human interactions in problem-solving contexts. Learners\nmight have project these expectations inaccurately when interacting with AI agents, leading to a\npotential disparity in the perceived effectiveness of the assistance. Existing research has highlighted\nthat dynamics in human-agent conversations significantly differ from those in human-human\ninteractions [25, 36]. This difference could account for the observed changes in confidence and trust\nlevels, underscoring the need for aligning user expectations with the unique nature of AI-driven\nproblem-solving support. Understanding these dynamics further could be pivotal in enhancing the\neffectiveness and acceptance of LLM-based learning tools. Further studies should investigate the\nqualitative aspects of learners‚Äô experiences to answer these questions.\n6.4 RQ5: Learners‚Äô Self-Assessment During the Problem-Solving Task\nAcross both studies, we did not find significant differences between the pre-task and post-task\nmeasures of learners‚Äô confidence in their ability to solve problems for the given topic with LLM\nassistance. Our findings contrast with some existing work in education that states mastery experi-\nences (i.e., success in tasks) elevate self-confidence [12]. This could suggest that the perceived role\nof LLM in problem-solving might mediate the relationship between task success and self-confidence.\nThis holds significance for the education research - mere problem solving with advanced tools does\nnot translate to increased self-confidence. This underscores the need for complementary strategies\nin educational settings - such as reflection, discussion, or additional feedback mechanisms - to foster\nself-belief along with skill acquisition [55, 74, 95]. In Study-2, participants reported moderately\nhigh confidence in their answers. But more important than just the average confidence is the\ncalibration - whether their confidence is aligned with their actual accuracy. Approximately 23.53%\nof participants in the Unprompted LLM with G1: List of Suggestions condition reported higher than\nmedian confidence in their answers, but achieved a lower than median score in the math problems.\nThis misalignment echoes the broader educational concern of calibration in self-assessment - it is\nnot just about how confident learners feel, but how that confidence matches up to their real-world\nperformance [49, 75]. For educators and technology developers, this underscores the importance\nof creating tools that not only assist in problem-solving but also foster accurate self-reflection\nand awareness in learners. Future research should delve deeper into strategies to better calibrate\nlearners‚Äô self-perceptions when utilizing LLM-based tools.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\nGuiding Students in Using LLMs 499:21\n6.5 Subjective Measures of Helpfulness, Tolerance for Mistakes, Willingness to Interact\nAgain with the LLM Chatbot (Formative Study)\nOur formative study in the classroom allowed us to gather subjective ratings for different usability\nmeasures for LLM tutors. The presence of G3-Metacognitive-Questioning asked students to reflect\non the questions they wanted to ask before attempting the problem, analogous to what they might\nas a teacher. One could hypothesize having clearer questions to ask could improve their interaction\nwith the system as they have a clear sense of what to ask, to get useful information. However, the\ndata suggested this might not be the case, or even the opposite might happen, as the presence of\nG3-Metacognitive-Questioning reduced tolerance for mistakes and willingness to interact again.\nThis might happen by learners gaining unrealistic expectations about how LLMs could help, or\nby them having overly specific questions, whereas an LLM might be more beneficial for other\nreasons. For example, giving them space to self-explain or think through in a discussion might be\nhelpful [113, 114]. All conditions report moderately high ratings for ‚ÄúHelpfulness‚Äù for the given\ntopic. However, they report it to be even higher on average for ‚ÄúHelpfulness for other topics‚Äù, with\nthe presence of G4-solve-then-refine standing out as being particularly helpful in general.\n6.6 Methodological Design Choices for Educational LLM Interventions\nEach study helped to address specific aspects of the research questions. With Study-1, our objective\nwas to explore how undergraduate students will interact with LLMs in a natural classroom setting,\nstriving to achieve their course goals. However, this field setting constrained our design space,\nexcluding more risky exploration strategies, potentially disadvantaging some students, and limiting\nthe study sample size. We focused on rich qualitative feedback and interaction data, allowing a\ndetailed examination of how interaction strategies shape student engagement with LLMs and\ntreating quantitative assessments as formative. Consequently, Study-2, conducted with crowd-\nworkers, aimed to build on the results of Study-1 and take a more direct approach to compare the\nguidance conditions. This allowed us to evaluate individual interaction strategies and the effect of\nan unprompted LLM, which was infeasible in Study-1 due to ethical reasons.\nAlthough the classroom setting of Study-1 provided useful information on the real-world use\nof LLMs for learning, the inherent limitation of small sample sizes was a significant challenge\n[40]. To address this, we used a 2√ó2√ó2√ó2 factorial experiment design, which assessed the presence\nor absence of four distinct guidance strategies, thus improving the statistical power [ 37]. This\ncontrasted with the approach of Study-2, where the larger pool of participants allowed a broader\ncomparison of strategies and the integration of subjective measures at various intervals [61]. These\nrepeated measures could have led to high rates of attrition in the classroom setting. However, the\nvalidity of the results of Study-2 may not align with those of Study-1 [64], where students interacted\nwith LLMs as part of their coursework in a more authentic context.\n6.7 Future Work & Limitations\nThere is potential for further explorations of the design space of learner-LLM interactions. We\ncould not experiment with pre-prompts in the classroom deployment due to ethical and sample\nsize constraints, and the controlled online setting did not have a measure for long-term learning.\nFuture studies could explore the effect of changing LLM pre-prompts on the longer-term learning\noutcomes of the students. Moreover, there is rich literature on how to guide people to use tools\n(especially in the context of learning) [6, 10, 17, 46, 84, 107, 123]. Future research can build on the\nguidance strategies proposed in this paper and explore the use of other designs to guide students, as\nwell as to encourage different approaches. Furthermore, while designing learner-LLM interactions,\nit is crucial to ensure the right balance of learners‚Äô confidence in their answers while using LLMs,\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\n499:22 Harsh Kumar, et al.\nwith actual accuracy of their answers. High confidence with low accuracy could signal over-reliance\nand overconfidence. Future research could look at designing interactions that ensure an optimal\nbalance of confidence and accuracy.\nThere are limitations to the current work. Study 1 and 2 differed in various aspects, including study\ndesign, participant pool, and subject matter. Although the findings of both studies can be considered\ncomplementary, interpreting them together should be done with caution due to the distinct study\nsettings. In our formative study, the constraints of small to medium-sized classroom field studies\ninherently limit the scope of broad statistical generalizations. Additionally, observations from our\nspecific classroom setting of an introduction to database course may not be generalized to other\nclassroom settings and audiences. For example, students with non-computer science backgrounds\nmight interact with LLMs differently and differ in LLM-related trust and confidence. More studies\nare needed to see whether results from the online controlled setting generalize to learning tasks\nin other contexts and populations of online learners, who may differ from crowdworkers in their\ngoals and motivation. A larger sample confirmatory study might help to reliably evaluate multiple\ninteractions between student characteristics, experimental variables, achievement, and perception\noutcomes, which were described as exploratory in the current work. Additionally, our findings\ncould benefit further from a rigorous qualitative analysis of conversations between learners and\nLLM chatbots, which can help explain the mechanisms behind some of the effects observed in our\nstudies. Future educational interventions that use LLMs can adopt more adaptive experimental\ndesigns such as the Multiphase Optimization Strategy (MOST) or Sequential Multiple Assignment\nRandomized Trials (SMART) [26]. These designs offer the flexibility to make dynamic adjustments\nbased on evolving data, which is crucial in educational settings where interaction effects and\nstudent responses can vary widely. Using such adaptive strategies and careful design decisions,\nresearchers can better navigate the trade-offs between validity and statistical power, leading to\nmore effective and contextually appropriate LLM interventions for learning.\nDeploying guidance strategies for an LLM tutor carries several risks. Students may develop\nincorrect mental models of the LLM‚Äôs capabilities [59], particularly if the guidance (like G1-List of\ninstructions or G2-Examples) does not accurately represent the operational boundaries of the LLM.\nThe choice of examples in G2-Examples is critical; inappropriate examples might mislead students\nabout the applicability or functionality of the LLM, potentially leading to frustration or inappropriate\nreliance on the LLM tutor [62, 87]. In addition, introducing complex guidance strategies or excessive\ninformation at the beginning might overwhelm the students, leading to cognitive overload [42].\nThis is particularly a concern with strategies requiring high initial engagement or pre-existing\nknowledge. The guidance strategies in this study did not specifically account for the diverse needs\nand backgrounds of all students, such as non-native English speakers [54, 97]. Future work could\nexplore ways to make our proposed guidance strategies personalized for students from varying\nbackgrounds and more accessible to students with special needs.\n7 CONCLUSION\nThrough this work, we highlight complexities of designing learner-LLM interactions and under-\nscore the critical role of instructors, teachers, and policymakers in guiding these interactions. We\nexplore the design space of this interaction with four guidance strategies resulting in two different\napproaches of use, with a prompted vs. unprompted LLM, in a classroom and a controlled online\nsetting. Even with our findings and rich data, we merely scratch the surface on the complexities\nof collaborative learning systems involving LLM agents. LLMs demonstrate remarkable flexibility\nin supporting various strategies, but also demand oversight to ensure the application of the most\neffective strategy in each unique learning scenario. Teachers are already playing a demanding role\nin these collaborative learning environments, ensuring proper use of LLMs. This highlights an\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\nGuiding Students in Using LLMs 499:23\nemerging imperative for CSCW and HCI researchers: empirically studying diverse ways of guiding\nstudents in LLM use, thus empowering teachers relying largely on anecdotal evidence or grey\nliterature for integrating LLMs in classrooms.\nOur research has shown the promise of simple yet effective interventions, such as including\nexamples of LLM use in assignments, prompting students to reflect on their use of LLMs, and en-\ncouraging a ‚Äòsolve-first-then-consult‚Äô approach with LLMs. These insights are crucial for educators\nand educational technology developers alike, as they collaboratively shape the next generation\nof LLM tools to be intuitive and pedagogically effective for diverse learners. Ultimately, our work\npaves the way for more in-depth investigations and broadens the avenues for further research\nwithin the CSCW and HCI communities, emphasizing the dual task for researchers and designers\nin optimizing LLM implementation for genuine student understanding and engagement.\nACKNOWLEDGMENTS\nWe thank the members of the Intelligent Adaptive Interventions Lab for their feedback on the\nproject. We also thank Marko Choi and Hammad Sheikh for their help in developing the chat\ninterface, which allowed us to collect data at scale. We acknowledge the financial support of Dr. Liut\nfrom the Learning & Education Advancement Fund from the Office of the Vice-Provost, Innovations\nin Undergraduate Education, University of Toronto, Microsoft‚Äôs Accelerating Foundation Model\nResearch program, Natural Sciences and Engineering Research Council of Canada (NSERC) (#RGPIN-\n2024-04348), as well as, Dr. William‚Äôs Natural Sciences and Engineering Research Council of Canada\n(NSERC) (#RGPIN-2019-06968) and the Office of Naval Research (ONR) (#N00014-21-1-2576) grants.\nFurthermore, we acknowledge an award from DARPA for AI Tools for Adult Learning awarded to\nTutorGen for QuickTA.\nREFERENCES\n[1] [n. d.]. Bing Chat. https://www.microsoft.com/en-us/edge/features/bing-chat?form=MT00D8. Accessed: 2023-08-18.\n[2] [n. d.]. ChatGPT. https://openai.com/chatgpt. Accessed: 2023-08-18.\n[3] [n. d.]. An important next step on our AI journey. https://blog.google/technology/ai/bard-google-ai-search-updates/.\nAccessed: 2023-08-18.\n[4] 2023. Practical AI for Instructors and Students Part 1: Introduction to AI for Teachers and Students. https:\n//www.youtube.com/watch?v=t9gmyvf7JYo&amp;list=PLwRdpYzPkkn302_rL5RrXvQE8j0jLP02j&amp;index=2\n[5] Ahmad Abdellatif, Diego Costa, Khaled Badran, Rabe Abdalkareem, and Emad Shihab. 2020. Challenges in chatbot\ndevelopment: A study of stack overflow posts. In Proceedings of the 17th international conference on mining software\nrepositories. 174‚Äì185.\n[6] Gary L Adams and Siegfried Engelmann. 1996. Research on Direct Instruction: 25 years beyond DISTAR. ERIC.\n[7] Kaamil Ahmed. 2023. World needs 44m more teachers in order to educate every child, report finds. The Guardian\n(October 2023). https://www.theguardian.com/link-to-the-specific-article\n[8] Dorit Alt and Nirit Raichel. 2020. Reflective journaling and metacognitive awareness: Insights from a longitudinal\nstudy in higher education. Reflective Practice 21, 2 (2020), 145‚Äì158.\n[9] John R Anderson, Albert T Corbett, Kenneth R Koedinger, and Ray Pelletier. 1995. Cognitive tutors: Lessons learned.\nThe journal of the learning sciences 4, 2 (1995), 167‚Äì207.\n[10] Richard Anderson, Ruth Anderson, Katie M Davis, Natalie Linnell, Craig Prince, and Valentin Razmov. 2007. Supporting\nactive learning and example based instruction with classroom technology. Acm Sigcse Bulletin 39, 1 (2007), 69‚Äì73.\n[11] David Baidoo-Anu and Leticia Owusu Ansah. 2023. Education in the era of generative artificial intelligence (AI):\nUnderstanding the potential benefits of ChatGPT in promoting teaching and learning. Available at SSRN 4337484\n(2023).\n[12] Albert Bandura. 1982. Self-efficacy mechanism in human agency. American psychologist 37, 2 (1982), 122.\n[13] Matthew L Bernacki, Meghan J Greene, and Nikki G Lobczowski. 2021. A systematic review of research on personalized\nlearning: Personalized by whom, to what, how, and for what purpose (s)? Educational Psychology Review 33, 4 (2021),\n1675‚Äì1715.\n[14] Benjamin S Bloom. 1984. The 2 sigma problem: The search for methods of group instruction as effective as one-to-one\ntutoring. Educational researcher 13, 6 (1984), 4‚Äì16.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\n499:24 Harsh Kumar, et al.\n[15] Elizabeth Bonawitz, Patrick Shafto, Hyowon Gweon, Noah D Goodman, Elizabeth Spelke, and Laura Schulz. 2011.\nThe double-edged sword of pedagogy: Instruction limits spontaneous exploration and discovery. Cognition 120, 3\n(2011), 322‚Äì330.\n[16] Hilda Borko, Vicky Mayfield, Scott Marion, Roberta Flexer, and Kate Cumbo. 1997. Teachers‚Äô developing ideas and\npractices about mathematics performance assessment: Successes, stumbling blocks, and implications for professional\ndevelopment. Teaching and Teacher education 13, 3 (1997), 259‚Äì278.\n[17] Tom Bourner. 2003. Assessing reflective learning. Education+ training 45, 5 (2003), 267‚Äì272.\n[18] Thomas Breideband, Jeffrey Bush, Chelsea Chandler, Michael Chang, Rachel Dickler, Peter Foltz, Ananya Ganesh,\nRachel Lieber, William R. Penuel, Jason G. Reitman, John Weatherley, and Sidney D‚ÄôMello. 2023. The Com-\nmunity Builder (CoBi): Helping Students to Develop Better Small Group Collaborative Learning Skills. In Com-\npanion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing (Min-\nneapolis, MN, USA) (CSCW ‚Äô23 Companion) . Association for Computing Machinery, New York, NY, USA, 376‚Äì380.\nhttps://doi.org/10.1145/3584931.3607498\n[19] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877‚Äì1901.\n[20] Ryan Brydges, Heather Carnahan, Don Rose, and Adam Dubrowski. 2010. Comparing self-guided learning and\neducator-guided learning formats for simulation-based clinical training. Journal of advanced nursing 66, 8 (2010),\n1832‚Äì1844.\n[21] Heloisa Candello, Gabriel Meneguelli Soella, Cassia Sampaio Sanctos, Marcelo Carpinette Grave, and Adinan Alves\nDe Brito Filho. 2023. \" This means nothing to me\": Building credibility in conversational systems. In Proceedings of\nthe 5th International Conference on Conversational User Interfaces . 1‚Äì6.\n[22] Lingjiao Chen, Matei Zaharia, and James Zou. 2023. How is ChatGPT‚Äôs behavior changing over time? arXiv preprint\narXiv:2307.09009 (2023).\n[23] Michelene TH Chi, Stephanie A Siler, Heisawn Jeong, Takashi Yamauchi, and Robert G Hausmann. 2001. Learning\nfrom human tutoring. Cognitive science 25, 4 (2001), 471‚Äì533.\n[24] Lee S Christie, R Duncan Luce, and Josiah Macy. 1952. Communication and learning in task-oriented groups. (1952).\n[25] Leigh Clark, Nadia Pantidi, Orla Cooney, Philip Doyle, Diego Garaialde, Justin Edwards, Brendan Spillane, Emer\nGilmartin, Christine Murad, Cosmin Munteanu, et al. 2019. What makes a good conversation? Challenges in designing\ntruly conversational agents. In Proceedings of the 2019 CHI conference on human factors in computing systems . 1‚Äì12.\n[26] Linda M Collins, Susan A Murphy, and Victor Strecher. 2007. The multiphase optimization strategy (MOST) and\nthe sequential multiple assignment randomized trial (SMART): new methods for more potent eHealth interventions.\nAmerican journal of preventive medicine 32, 5 (2007), S112‚ÄìS118.\n[27] Harris Ed Cooper, Paul M Camic, Debra L Long, AT Panter, David Ed Rindskopf, and Kenneth J Sher. 2012. APA\nhandbook of research methods in psychology, Vol 2: Research designs: Quantitative, qualitative, neuropsychological, and\nbiological. American Psychological Association.\n[28] Albert T Corbett and John R Anderson. 2001. Locus of feedback control in computer-based tutoring: Impact on\nlearning rate, achievement and attitudes. In Proceedings of the SIGCHI conference on Human factors in computing\nsystems. 245‚Äì252.\n[29] Kahneman Daniel. 2017. Thinking, fast and slow .\n[30] Dan Davis, Claudia Hauff, and Geert-Jan Houben. 2018. Evaluating Crowdworkers as a Proxy for Online Learners\nin Video-Based Learning Contexts. Proc. ACM Hum.-Comput. Interact. 2, CSCW, Article 42 (nov 2018), 16 pages.\nhttps://doi.org/10.1145/3274311\n[31] Yang Deng, Wenqiang Lei, Minlie Huang, and Tat-Seng Chua. 2023. Rethinking Conversational Agents in the Era of\nLLMs: Proactivity, Non-collaborativity, and Beyond. In Proceedings of the Annual International ACM SIGIR Conference\non Research and Development in Information Retrieval in the Asia Pacific Region . 298‚Äì301.\n[32] Paul Denny, Hassan Khosravi, Arto Hellas, Juho Leinonen, and Sami Sarsa. 2023. Can We Trust AI-Generated\nEducational Content? Comparative Analysis of Human and AI-Generated Learning Resources. arXiv preprint\narXiv:2306.10509 (2023).\n[33] Paul Denny, Juho Leinonen, James Prather, Andrew Luxton-Reilly, Thezyrie Amarouche, Brett A Becker, and Brent N\nReeves. 2023. Promptly: Using Prompt Problems to Teach Learners How to Effectively Utilize AI Code Generators.\narXiv preprint arXiv:2307.16364 (2023).\n[34] Ismail Dergaa, Karim Chamari, Piotr Zmijewski, and Helmi Ben Saad. 2023. From human writing to artificial\nintelligence generated text: examining the prospects and potential threats of ChatGPT in academic writing. Biology\nof Sport 40, 2 (2023), 615‚Äì622.\n[35] Stephan Diederich, Alfred Benedikt Brendel, Stefan Morana, and Lutz Kolbe. 2022. On the design of and interaction\nwith conversational agents: An organizing and assessing review of human-computer interaction research. Journal of\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\nGuiding Students in Using LLMs 499:25\nthe Association for Information Systems 23, 1 (2022), 96‚Äì138.\n[36] Philip R Doyle, Justin Edwards, Odile Dumbleton, Leigh Clark, and Benjamin R Cowan. 2019. Mapping perceptions\nof humanness in intelligent personal assistant interaction. In Proceedings of the 21st international conference on\nhuman-computer interaction with mobile devices and services . 1‚Äì12.\n[37] John J Dziak, Inbal Nahum-Shani, and Linda M Collins. 2012. Multilevel factorial experiments for developing\nbehavioral interventions: power, sample size, and resource considerations. Psychological methods 17, 2 (2012), 153.\n[38] Sabit Ekin. 2023. Prompt Engineering For ChatGPT: A Quick Guide To Techniques, Tips, And Best Practices. (2023).\n[39] Arthur K Ellis, David W Denton, and John B Bond. 2014. An analysis of research on metacognitive teaching strategies.\nProcedia-Social and Behavioral Sciences 116 (2014), 4015‚Äì4024.\n[40] Tisha Emerson and Denise Hazlett. 2012. Classroom experiments. International handbook on teaching and learning\neconomics (2012), 90‚Äì98.\n[41] Mohammadreza Farrokhnia, Seyyed Kazem Banihashem, Omid Noroozi, and Arjen Wals. 2023. A SWOT analysis of\nChatGPT: Implications for educational practice and research. Innovations in Education and Teaching International\n(2023), 1‚Äì15.\n[42] David F Feldon. 2007. Cognitive load and classroom teaching: The double-edged sword of automaticity. Educational\npsychologist 42, 3 (2007), 123‚Äì137.\n[43] Alexander J Fiannaca, Chinmay Kulkarni, Carrie J Cai, and Michael Terry. 2023. Programming without a Programming\nLanguage: Challenges and Opportunities for Designing Developer Tools for Prompt Programming. In Extended\nAbstracts of the 2023 CHI Conference on Human Factors in Computing Systems . 1‚Äì7.\n[44] D Jake Follmer, Rayne A Sperling, and Hoi K Suen. 2017. The role of MTurk in education research: Advantages,\nissues, and future directions. Educational Researcher 46, 6 (2017), 329‚Äì334.\n[45] Davide Fossati, Barbara Di Eugenio, STELLAN Ohlsson, Christopher Brown, and Lin Chen. 2015. Data driven\nautomatic feedback generation in the iList intelligent tutoring system. Technology, Instruction, Cognition and Learning\n10, 1 (2015), 5‚Äì26.\n[46] Russell Gersten, John Woodward, and Craig Darch. 1986. Direct instruction: A research-based approach to curriculum\ndesign and teaching. Exceptional Children 53, 1 (1986), 17‚Äì31.\n[47] Elena L Glassman, Jeremy Scott, Rishabh Singh, Philip J Guo, and Robert C Miller. 2015. OverCode: Visualizing\nvariation in student solutions to programming problems at scale. ACM Transactions on Computer-Human Interaction\n(TOCHI) 22, 2 (2015), 1‚Äì35.\n[48] Marcello A G√≥mez-Maureira, Isabelle Kniestedt, Max Van Duijn, Carolien Rieffe, and Aske Plaat. 2021. Level design\npatterns that invoke curiosity-driven exploration: An empirical study across multiple conditions. Proceedings of the\nACM on Human-Computer Interaction 5, CHI PLAY (2021), 1‚Äì32.\n[49] Michael Gottlieb, Teresa M Chan, Fareen Zaver, and Rachel Ellaway. 2022. Confidence-competence alignment and\nthe role of self-confidence in medical education: A conceptual review. Medical Education 56, 1 (2022), 37‚Äì47.\n[50] Dirk Grunwald, Elizabeth Boese, Rhonda Hoenigman, Andy Sayler, and Judith Stafford. 2015. Personalized attention@\nscale: Talk isn‚Äôt cheap, but it‚Äôs effective. In Proceedings of the 46th ACM Technical Symposium on Computer Science\nEducation. 610‚Äì615.\n[51] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021.\nAligning AI With Shared Human Values. Proceedings of the International Conference on Learning Representations\n(ICLR) (2021).\n[52] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.\nMeasuring Massive Multitask Language Understanding. Proceedings of the International Conference on Learning\nRepresentations (ICLR) (2021).\n[53] Douglas R Hofstadter. 1995. Fluid concepts and creative analogies: Computer models of the fundamental mechanisms of\nthought. Basic books.\n[54] Anna Kristina Hultgren. 2019. English as the language for academic publication: On equity, disadvantage and\n‚Äònon-nativeness‚Äô as a red herring. Publications 7, 2 (2019), 31.\n[55] Jay W Jackson. 2002. Enhancing self-efficacy and learning performance. The journal of experimental education 70, 3\n(2002), 243‚Äì254.\n[56] Stuart A. Karabenick and Myron H. Dembo. 2011. Understanding and facilitating self-regulated help seeking. New\nDirections for Teaching and Learning 2011, 126 (June 2011), 33‚Äì43. https://doi.org/10.1002/tl.442\n[57] Enkelejda Kasneci, Kathrin Se√üler, Stefan K√ºchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser,\nGeorg Groh, Stephan G√ºnnemann, Eyke H√ºllermeier, et al. 2023. ChatGPT for good? On opportunities and challenges\nof large language models for education. Learning and Individual Differences 103 (2023), 102274.\n[58] Frank C Keil and Robert Andrew Wilson. 2000. Explanation and cognition . MIT press.\n[59] David E Kieras and Susan Bovair. 1984. The role of a mental model in learning to operate a device. Cognitive science\n8, 3 (1984), 255‚Äì273.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\n499:26 Harsh Kumar, et al.\n[60] Dongho Kim and Cheolil Lim. 2018. Promoting socially shared metacognitive regulation in collaborative project-based\nlearning: a framework for the design of structured guidance. Teaching in Higher Education 23, 2 (2018), 194‚Äì211.\n[61] Aniket Kittur, Ed H Chi, and Bongwon Suh. 2008. Crowdsourcing user studies with Mechanical Turk. In Proceedings\nof the SIGCHI conference on human factors in computing systems . 453‚Äì456.\n[62] Todd Kulesza, Simone Stumpf, Margaret Burnett, Sherry Yang, Irwin Kwan, and Weng-Keen Wong. 2013. Too much,\ntoo little, or just right? Ways explanations impact end users‚Äô mental models. In 2013 IEEE Symposium on visual\nlanguages and human centric computing . IEEE, 3‚Äì10.\n[63] Harsh Kumar, Ilya Musabirov, Joseph Jay Williams, and Michael Liut. 2023. QuickTA: Exploring the Design Space of\nUsing Large Language Models to Provide Support to Students. Learning Analytics and Knowledge Conference 2023\n(LAK‚Äô23).\n[64] Harsh Kumar, David M Rothschild, Daniel G Goldstein, and Jake M Hofman. 2023. Math Education with Large\nLanguage Models: Peril or Promise? Available at SSRN 4641653 (2023).\n[65] Harsh Kumar, Kunzhi Yu, Andrew Chung, Jiakai Shi, and Joseph Jay Williams. 2022. Exploring the potential of\nchatbots to provide mental well-being support for computer science students. InProceedings of the 54th ACM Technical\nSymposium on Computer Science Education V. 2 . 1339‚Äì1339.\n[66] Georgia Lazakidou and Symeon Retalis. 2010. Using computer supported collaborative learning strategies for helping\nstudents acquire self-regulated problem-solving skills in mathematics. Computers & Education 54, 1 (2010), 3‚Äì13.\n[67] Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-Ursin,\nXiang Lisa Li, Faisal Ladhak, Frieda Rong, et al. 2022. Evaluating human-language model interaction. arXiv preprint\narXiv:2212.09746 (2022).\n[68] Yo-An Lee. 2004. The work of examples in classroom instruction. Linguistics and education 15, 1-2 (2004), 99‚Äì120.\n[69] Zhuoyang Li, Minhui Liang, Hai Trung Le, Ray Lc, and Yuhan Luo. 2023. Exploring Design Opportunities for Reflective\nConversational Agents to Reduce Compulsive Smartphone Use. In Proceedings of the 5th International Conference on\nConversational User Interfaces . 1‚Äì6.\n[70] Lizi Liao, Grace Hui Yang, and Chirag Shah. 2023. Proactive conversational agents in the post-chatgpt world. In\nProceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval .\n3452‚Äì3455.\n[71] Junhua Liu, Lionell Loh, Ernest Ng, Yijia Chen, Kristin L. Wood, and Kwan Hui Lim. 2020. Self-Evolving Adaptive\nLearning for Personalized Education. In Companion Publication of the 2020 Conference on Computer Supported\nCooperative Work and Social Computing (Virtual Event, USA) (CSCW ‚Äô20 Companion) . Association for Computing\nMachinery, New York, NY, USA, 317‚Äì321. https://doi.org/10.1145/3406865.3418326\n[72] Vivian Liu and Lydia B Chilton. 2022. Design guidelines for prompt engineering text-to-image generative models. In\nProceedings of the 2022 CHI Conference on Human Factors in Computing Systems . 1‚Äì23.\n[73] Jennifer A Livingston. 2003. Metacognition: An Overview. (2003).\n[74] Howard Margolis*. 2005. Increasing struggling learners‚Äô self-efficacy: What tutors can do and say. Mentoring &\nTutoring: Partnership in Learning 13, 2 (2005), 221‚Äì238.\n[75] Theresa M Marteau, Marie Johnston, Geralyn Wynne, and Tom R Evans. 1989. Cognitive factors in the explanation\nof the mismatch between confidence and competence in performing basic life support. Psychology and Health 3, 3\n(1989), 173‚Äì182.\n[76] Samiha Marwan, Joseph Jay Williams, and Thomas Price. 2019. An evaluation of the impact of automated programming\nhints on performance and learning. In Proceedings of the 2019 ACM Conference on International Computing Education\nResearch. 61‚Äì70.\n[77] Douglas C Merrill, Brian J Reiser, Shannon K Merrill, and Shari Landes. 1995. Tutoring: Guided learning by doing.\nCognition and instruction 13, 3 (1995), 315‚Äì372.\n[78] Ricardo dos Santos Miquelino. [n. d.]. Better results with CHATGPT? ‚Äì it‚Äôs all about asking the right questions.\nhttps://www.linkedin.com/pulse/better-results-chatgpt-its-all-asking-right-questions-ricardo/\n[79] Ethan Mollick. 2023. https://www.oneusefulthing.org/p/all-my-classes-suddenly-became-ai\n[80] Ethan Mollick. 2023. Assigning AI: Seven ways of using AI in class. https://www.oneusefulthing.org/p/assigning-ai-\nseven-ways-of-using\n[81] Trevor T. Moores and Jerry Cha-Jan Chang. 2009. Self-efficacy, overconfidence, and the negative effect on subsequent\nperformance: A field study. Information & Management 46, 2 (March 2009), 69‚Äì76. https://doi.org/10.1016/j.im.2008.\n11.006\n[82] Ha Nguyen. 2023. Role design considerations of conversational agents to facilitate discussion and systems thinking.\nComputers & Education 192 (2023), 104661.\n[83] Stefan Palan and Christian Schitter. 2018. Prolific. ac‚ÄîA subject pool for online experiments. Journal of Behavioral\nand Experimental Finance 17 (2018), 22‚Äì27.\n[84] Annemarie Sullivan Palincsar. 1986. Metacognitive strategy instruction. Exceptional children 53, 2 (1986), 118‚Äì124.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\nGuiding Students in Using LLMs 499:27\n[85] Eileen Pangu. 2023. How to use large language models (LLM) in your own domains. https://towardsdatascience.\ncom/how-to-use-large-language-models-llm-in-your-own-domains-b4dff2d08464\n[86] Doeun Park, Myounglee Choo, Bohyun Jin, Un Sun Chung, Jinwoo Kim, Junghan Lee, and Yee-Jin Shin. 2023. Utilizing\na Conversational Agent to Promote Self-efficacy in Children: A Pilot Study on Low Cognitive Ability Children with\nAttention Deficit Hyperactivity Disorder. In Extended Abstracts of the 2023 CHI Conference on Human Factors in\nComputing Systems . 1‚Äì7.\n[87] Samir Passi, Shipi Dhanorkar, and Mihaela Vorvoreanu. 2024.Appropriate reliance on Generative AI: Research synthesis .\nTechnical Report MSR-TR-2024-7. Microsoft. https://www.microsoft.com/en-us/research/publication/appropriate-\nreliance-on-generative-ai-research-synthesis/\n[88] Jos√© Quiroga P√©rez, Thanasis Daradoumis, and Joan Manuel Marqu√®s Puig. 2020. Rediscovering the use of chatbots in\neducation: A systematic literature review. Computer Applications in Engineering Education 28, 6 (2020), 1549‚Äì1565.\n[89] Md Mostafizer Rahman and Yutaka Watanobe. 2023. ChatGPT for education and research: Opportunities, threats,\nand strategies. Applied Sciences 13, 9 (2023), 5783.\n[90] Leena Razzaq and Neil T Heffernan. 2010. Hints: is it better to give or wait to be asked?. InIntelligent Tutoring Systems:\n10th International Conference, ITS 2010, Pittsburgh, PA, USA, June 14-18, 2010, Proceedings, Part I 10 . Springer, 349‚Äì358.\n[91] Liam Richards Maldonado, Azza Abouzied, and Nancy W. Gleason. 2023. ReaderQuizzer: Augmenting Research\nPapers with Just-In-Time Learning Questions to Facilitate Deeper Understanding. In Companion Publication of the\n2023 Conference on Computer Supported Cooperative Work and Social Computing (Minneapolis, MN, USA) (CSCW ‚Äô23\nCompanion). Association for Computing Machinery, New York, NY, USA, 391‚Äì394. https://doi.org/10.1145/3584931.\n3607494\n[92] Lara Riefle, Carina Benz, and Tuhina Tomar. 2022. \" May I Help You?\": Exploring the Effect of Individuals‚Äô Self-Efficacy\non the Use of Conversational Agents.. In ICIS.\n[93] Kelly Rivers. 2017. Automated data-driven hint generation for learning programming . Ph. D. Dissertation. Carnegie\nMellon University.\n[94] Rohan Roy Choudhury, Hezheng Yin, and Armando Fox. 2016. Scale-driven automatic hint generation for coding style.\nIn Intelligent Tutoring Systems: 13th International Conference, ITS 2016, Zagreb, Croatia, June 7-10, 2016. Proceedings 13 .\nSpringer, 122‚Äì132.\n[95] Dale H Schunk. 1985. Self-efficacy and classroom learning. Psychology in the Schools 22, 2 (1985), 208‚Äì223.\n[96] Harriet L Schwartz and Elizabeth L Holloway. 2014. ‚ÄúI Become a Part of the Learning Process‚Äù: Mentoring Episodes\nand Individualized Attention in Graduate Education. Mentoring & Tutoring: Partnership in Learning 22, 1 (2014),\n38‚Äì55.\n[97] Jane Seale. 2013. E-learning and disability in higher education: accessibility research and practice . Routledge.\n[98] Terrence J Sejnowski. 2023. Large language models and the reverse turing test. Neural computation 35, 3 (2023),\n309‚Äì342.\n[99] H ≈ûenay ≈ûen. 2009. The relationsip between the use of metacognitive strategies and reading comprehension. Procedia-\nSocial and Behavioral Sciences 1, 1 (2009), 2301‚Äì2305.\n[100] Lorrie Shepard. 2001. The role of classroom assessment in teaching and learning. (2001).\n[101] Tuva Lunde Smestad and Frode Volden. 2019. Chatbot personalities matters: improving the user experience of chatbot\ninterfaces. In Internet Science: INSCI 2018 International Workshops, St. Petersburg, Russia, October 24‚Äì26, 2018, Revised\nSelected Papers 5 . Springer, 170‚Äì181.\n[102] Kiley Sobel, Geza Kovacs, Galen McQuillen, Andrew Cross, Nirupama Chandrasekaran, Nathalie Henry Riche, Ed\nCutrell, and Meredith Ringel Morris. 2017. EduFeed: A Social Feed to Engage Preliterate Children in Educational\nActivities. In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing\n(Portland, Oregon, USA) (CSCW ‚Äô17) . Association for Computing Machinery, New York, NY, USA, 491‚Äì504. https:\n//doi.org/10.1145/2998181.2998231\n[103] John Sweller. 2006. The worked example effect and human cognition. Learning and instruction (2006).\n[104] Alexandr Ten, Pramod Kaushik, Pierre-Yves Oudeyer, and Jacqueline Gottlieb. 2021. Humans monitor learning\nprogress in curiosity-driven exploration. Nature communications 12, 1 (2021), 5972.\n[105] UNESCO. 2022. World Teachers‚Äô Day: UNESCO sounds the alarm on the global teacher shortage crisis. UNESCO\n(October 2022). https://www.unesco.org/en/articles/world-teachers-day-unesco-sounds-alarm-global-teacher-\nshortage-crisis\n[106] Jan H Van Driel, Douwe Beijaard, and Nico Verloop. 2001. Professional development and reform in science education:\nThe role of teachers‚Äô practical knowledge. Journal of Research in Science Teaching: The Official Journal of the National\nAssociation for Research in Science Teaching 38, 2 (2001), 137‚Äì158.\n[107] Tamara Van Gog and Nikol Rummel. 2010. Example-based learning: Integrating cognitive and social-cognitive\nresearch perspectives. Educational psychology review 22 (2010), 155‚Äì174.\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\n499:28 Harsh Kumar, et al.\n[108] Jeffrey B. Vancouver, Charles M. Thompson, E. Casey Tischner, and Dan J. Putka. 2002. Two studies examining\nthe negative effect of self-efficacy on performance. Journal of Applied Psychology 87, 3 (2002), 506‚Äì516. https:\n//doi.org/10.1037/0021-9010.87.3.506\n[109] Peter Samuelson Wardrip, R. Benjamin Shapiro, Andrea Forte, Spiro Maroulis, Karen Brennan, and Ricarose Roque.\n2013. CSCW and Education: Viewing Education as a Site of Work Practice. In Proceedings of the 2013 Conference on\nComputer Supported Cooperative Work Companion (San Antonio, Texas, USA) (CSCW ‚Äô13) . Association for Computing\nMachinery, New York, NY, USA, 333‚Äì336. https://doi.org/10.1145/2441955.2442035\n[110] Florian Weber, Thiemo Wambsganss, Dominic R√ºttimann, and Matthias S√∂llner. 2021. Pedagogical Agents for\nInteractive Learning: A Taxonomy of Conversational Agents in Education.. In ICIS.\n[111] Daniel Weitekamp, Erik Harpstead, and Ken R. Koedinger. 2020. An Interaction Design for Machine Teaching to\nDevelop AI Tutors. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . Association for\nComputing Machinery, New York, NY, USA, 1‚Äì11. https://doi.org/10.1145/3313831.3376226\n[112] William AT White. 1988. A meta-analysis of the effects of direct instruction in special education. Education and\nTreatment of children (1988), 364‚Äì374.\n[113] Joseph Jay Williams, Juho Kim, Anna Rafferty, Samuel Maldonado, Krzysztof Z Gajos, Walter S Lasecki, and Neil\nHeffernan. 2016. Axis: Generating explanations at scale with learnersourcing and machine learning. In Proceedings of\nthe Third (2016) ACM Conference on Learning@ Scale . 379‚Äì388.\n[114] Joseph Jay Williams, Tania Lombrozo, Anne Hsu, Bernd Huber, and Juho Kim. 2016. Revising learner misconceptions\nwithout feedback: Prompting for reflection on anomalies. In Proceedings of the 2016 CHI conference on human factors\nin computing systems . 470‚Äì474.\n[115] J√∂rg Wittwer and Alexander Renkl. 2010. How effective are instructional explanations in example-based learning? A\nmeta-analytic review. Educational Psychology Review 22 (2010), 393‚Äì409.\n[116] Meng-Hsin Wu, Su-Fang Yeh, XiJing Chang, and Yung-Ju Chang. 2021. Exploring Users‚Äô Preferences for Chatbot‚Äôs\nGuidance Type and Timing. In Companion Publication of the 2021 Conference on Computer Supported Cooperative Work\nand Social Computing (Virtual Event, USA) (CSCW ‚Äô21 Companion) . Association for Computing Machinery, New York,\nNY, USA, 191‚Äì194. https://doi.org/10.1145/3462204.3481756\n[117] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. Ai chains: Transparent and controllable human-ai interaction\nby chaining large language model prompts. In Proceedings of the 2022 CHI conference on human factors in computing\nsystems. 1‚Äì22.\n[118] Jun Xiao, John Stasko, and Richard Catrambone. 2007. The role of choice and customization on users‚Äô interaction\nwith embodied conversational agents: effects on perception and performance. In Proceedings of the SIGCHI conference\non Human factors in computing systems . 1293‚Äì1302.\n[119] Ruiwei Xiao, Xinying Hou, Harsh Kumar, Steven Moore, John Stamper, and Michael Liut. [n. d.]. A Preliminary\nAnalysis of Students‚Äô Help Requests with an LLM-powered Chatbot when Completing CS1 Assignments. ([n. d.]).\n[120] Hong Yang. 2023. How I use ChatGPT responsibly in my teaching. Nature (2023).\n[121] Hatice Yildiz Durak. 2023. Conversational agent-based guidance: examining the effect of chatbot usage frequency and\nsatisfaction on visual design self-efficacy, engagement, satisfaction, and learner autonomy. Education and Information\nTechnologies 28, 1 (2023), 471‚Äì488.\n[122] JD Zamfirescu-Pereira, Richmond Y Wong, Bjoern Hartmann, and Qian Yang. 2023. Why Johnny can‚Äôt prompt: how\nnon-AI experts try (and fail) to design LLM prompts. In Proceedings of the 2023 CHI Conference on Human Factors in\nComputing Systems . 1‚Äì21.\n[123] Andreas Zendler and K Klein. 2018. The effect of direct instruction and web quest on learning outcome in computer\nscience education. Education and Information Technologies 23 (2018), 2765‚Äì2782.\n[124] Ling Zhang, James D Basham, and Sohyun Yang. 2020. Understanding the implementation of personalized learning:\nA research synthesis. Educational Research Review 31 (2020), 100339.\n[125] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022.\nLarge language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910 (2022).\n[126] Qingxiaoyang Zhu, Yi-Chieh Lee, and Hao-Chuan Wang. 2022. Action-a-Bot: Exploring Human-Chatbot Conversa-\ntions for Actionable Instruction Giving and Following. In Companion Publication of the 2022 Conference on Computer\nSupported Cooperative Work and Social Computing . 145‚Äì149.\nA RESEARCH METHODS\nA.1 Formative Study\nA.1.1 LLM Model Specification.\n‚Ä¢model version: text-davinci-003\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\nGuiding Students in Using LLMs 499:29\n‚Ä¢number of parameters : 175B\n‚Ä¢date of use : April 2023\nConfiguration Settings:\n‚Ä¢temperature: 0\n‚Ä¢max tokens: 300\n‚Ä¢top-p: 1\n‚Ä¢frequency penalty: 0\n‚Ä¢presence penalty: 0.6\nPrompt Design:\n‚ÄúThe following is a conversation with a database instructor. The instructor helps the human\nsolve assignment problems related to database. The instructor never explicitly gives the\nsolution. The instructor also never writes the SQL query. The instructor would only provide\nbrainstorms to possible solutions without providing any SQL statements. This rule should\nbe enforced in the entirety of the conversation. Following are the ddl files to create table\ngiven as part of the assignment. ‚Äù\nInteraction Environment:\n‚Ä¢environment: A platform with a conversation interface hosted on university servers. The\nplatform directly calls the OpenAI‚Äôs API for generated contents.\nA.2 Online Controlled Study\nA.2.1 LLM Model Specification.\n‚Ä¢model version: text-davinci-003\n‚Ä¢number of parameters : 175B\n‚Ä¢date of use : September 2023\nConfiguration Settings:\n‚Ä¢temperature: 0\n‚Ä¢max tokens: 300\n‚Ä¢top-p: 1\n‚Ä¢frequency penalty: 0\n‚Ä¢presence penalty: 0.6\nPrompt Design:\n‚Ä¢prompted LLM:\n‚ÄúYou are a professional K12 math teacher helping students answer math questions.\nGive students explanations, examples, and analogies about the concept to help them under-\nstand. You should guide students in an open-ended way. Make the answer as precise and\nsuccinct as possible.\nYou should help them in a way that helps them (1) learn the concept, (2) have confidence in\ntheir understanding, and (3) have confidence in your ability to help them. Before answering,\nreflect on how your answer will help you achieve goals (1) (2) (3). Update your answer based\non this reflection. ‚Äù\n‚Ä¢unprompted LLM:\n‚ÄúYou are a helpful assistant. ‚Äù\nInteraction Environment:\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024.\n499:30 Harsh Kumar, et al.\n‚Ä¢environment: A platform with a conversation interface hosted by Microsoft Azure service.\nThe platform directly calls the OpenAI‚Äôs API for generated contents.\nReceived January 2024; revised April 2024; accepted May 2024\nProc. ACM Hum.-Comput. Interact., Vol. 8, No. CSCW2, Article 499. Publication date: November 2024."
}