{
    "title": "Respect My Authority! HITS Without Hyperlinks, Utilizing Cluster-Based Language Models",
    "url": "https://openalex.org/W2950669208",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5041210904",
            "name": "Oren Kurland",
            "affiliations": [
                "Cornell University"
            ]
        },
        {
            "id": "https://openalex.org/A5076876084",
            "name": "Lillian Lee",
            "affiliations": [
                "Cornell University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1501307387",
        "https://openalex.org/W1803641895",
        "https://openalex.org/W2027445772",
        "https://openalex.org/W2084048649",
        "https://openalex.org/W2129971563",
        "https://openalex.org/W2028122423",
        "https://openalex.org/W2110891728",
        "https://openalex.org/W2158201212",
        "https://openalex.org/W3101913037",
        "https://openalex.org/W2133576408",
        "https://openalex.org/W2066636486",
        "https://openalex.org/W2068905009",
        "https://openalex.org/W2066867064",
        "https://openalex.org/W2136542423",
        "https://openalex.org/W2061198046",
        "https://openalex.org/W2098034778",
        "https://openalex.org/W2018557178",
        "https://openalex.org/W1975998118",
        "https://openalex.org/W1992795877",
        "https://openalex.org/W1904228841",
        "https://openalex.org/W2100958137",
        "https://openalex.org/W2114900318",
        "https://openalex.org/W2114512077",
        "https://openalex.org/W2074449313",
        "https://openalex.org/W2130395434",
        "https://openalex.org/W2138621811",
        "https://openalex.org/W2141075029",
        "https://openalex.org/W2093390569",
        "https://openalex.org/W1996764654",
        "https://openalex.org/W1972645849",
        "https://openalex.org/W1525595230"
    ],
    "abstract": "We present an approach to improving the precision of an initial document ranking wherein we utilize cluster information within a graph-based framework. The main idea is to perform re-ranking based on centrality within bipartite graphs of documents (on one side) and clusters (on the other side), on the premise that these are mutually reinforcing entities. Links between entities are created via consideration of language models induced from them. We find that our cluster-document graphs give rise to much better retrieval performance than previously proposed document-only graphs do. For example, authority-based re-ranking of documents via a HITS-style cluster-based approach outperforms a previously-proposed PageRank-inspired algorithm applied to solely-document graphs. Moreover, we also show that computing authority scores for clusters constitutes an effective method for identifying clusters containing a large percentage of relevant documents.",
    "full_text": "arXiv:0804.3599v1  [cs.IR]  22 Apr 2008\nRespect My Authority! HITS Without Hyperlinks, Utilizing\nCluster-Based Language Models\nOren Kurland and Lillian Lee\nDepartment of Computer Science\nCornell University\nIthaca, NY 14853-7501\nkurland@cs.cornell.edu, llee@cs.cornell.edu\nABSTRACT\nWe present an approach to improving the precision of an ini-\ntial document ranking wherein we utilize cluster informati on\nwithin a graph-based framework. The main idea is to per-\nform re-ranking based on centrality within bipartite graph s\nof documents (on one side) and clusters (on the other side),\non the premise that these are mutually reinforcing entities .\nLinks between entities are created via consideration of lan -\nguage models induced from them.\nWe ﬁnd that our cluster-document graphs give rise to\nmuch better retrieval performance than previously propose d\ndocument-only graphs do. For example, authority-based\nre-ranking of documents via a HITS-style cluster-based ap-\nproach outperforms a previously-proposed PageRank-inspi red\nalgorithm applied to solely-document graphs. Moreover, we\nalso show that computing authority scores for clusters con-\nstitutes an eﬀective method for identifying clusters conta in-\ning a large percentage of relevant documents.\nCategories and Subject Descriptors:H.3.3 [Information Search\nand Retrieval]: Retrieval models\nGeneral Terms: Algorithms, Experimentation\nKeywords: bipartite graph, clusters, language modeling, HITS,\nhubs, authorities, PageRank, high-accuracy retrieval, gr aph-based\nretrieval, structural re-ranking, cluster-based languag e models\n1. INTRODUCTION\nTo improve the precision of retrieval output, especially\nwithin the very few (e.g, 5 or 10) highest-ranked documents\nthat are returned, a number of researchers [36, 13, 16, 7, 22,\n34, 25, 1, 18, 9] have considered a structural re-ranking strat-\negy. The idea is to re-rank the top N documents that some\ninitial search engine produces, where the re-ordering uti-\nlizes information about inter-document relationships wit hin\nthat set. Promising results have been previously obtained\nby using document centrality within the initially retrieved\nlist to perform structural re-ranking, on the premise that\nif the quality of this list is reasonable to begin with, then\nthe documents that are most related to most of the docu-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for proﬁt or commercial advantage and that copies\nbear this notice and the full citation on the ﬁrst page. To copy otherwise, to\nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc\npermission and/or a fee.\nSIGIR’06,August 6–11, 2006, Seattle, Washington, USA.\nCopyright 2006 ACM 1-59593-369-7/06/0008 ...$5.00.\nments on the list are likely to be the most relevant ones. In\nparticular, in our prior work [18] we adapted PageRank [3]\n— which, due to the success of Google, is surely the most\nwell-established algorithm for deﬁning and computing cen-\ntrality within a directed graph — to the task of re-ranking\nnon-hyperlinked document sets.\nThe arguably most well-known alternative to PageRank\nis Kleinberg’s HITS algorithm [16]. The major conceptual\nway in which HITS diﬀers from PageRank is that it deﬁnes\ntwo diﬀerent types of central items: each node is assigned\nboth a hub and an authority score as opposed to a single\nPageRank score. In the Web setting, in which HITS was\noriginally proposed, good hubs correspond roughly to high-\nquality resource lists or collections of pointers, whereas good\nauthorities correspond to the high-quality resources them -\nselves; thus, distinguishing between two diﬀering but inte r-\ndependent types of Webpages is quite appropriate. Our pre-\nvious study [18] applied HITS to non-Web documents. We\nfound that its performance was comparable to or better than\nthat of algorithms that do not involve structural re-rankin g;\nhowever, HITS was not as eﬀective as PageRank [18].\nDo these results imply that PageRank is better than HITS\nfor structural re-ranking of non-Web documents? Not neces-\nsarily, because there may exist graph-construction method s\nthat are more suitable for HITS. Note that the only enti-\nties considered in our previous study were documents. If we\ncould introduce entities distinct from documents but enjoy -\ning a mutually reinforcing relationship with them, then we\nmight better satisfy the spirit of the hubs-versus-authori ties\ndistinction, and thus derive stronger results utilizing HI TS.\nA crucial insight of the present paper is that document\nclusters appear extremely well-suited to play this comple-\nmentary role. The intuition is that: (a) given those clus-\nters that are “most representative” of the user’s informati on\nneed, the documents within those clusters are likely to be\nrelevant; and (b) the “most representative” clusters shoul d\nbe those that contain many relevant documents. This appar-\nently circular reasoning is strongly reminiscent of the int er-\nrelated hubs and authorities concepts underlying HITS.\nAlso, clusters have long been considered a promising source\nof information. The well-known cluster hypothesis [35] en-\ncapsulates the intuition that clusters can reveal groups of\nrelevant documents; in practice, the potential utility of c lus-\ntering for this purpose has been demonstrated for both the\ncase wherein clusters were created in a query-independent\nfashion [14, 4] and the re-ranking setting [13, 22, 34].\nIn this paper, we show through an array of experiments\nthat consideration of the mutual reinforcement of clusters\nand documents in determining centrality can lead to highly\neﬀective algorithms for re-ranking an initially retrieved list.\nSpeciﬁcally, our experimental results show that the centra lity-\ninduction methods that we previously studied solely in the\ncontext of document-only graphs [18] result in much better\nre-ranking performance if implemented over bipartite grap hs\nof documents (on one side) and clusters (on the other side).\nFor example, ranking documents by their “authoritative-\nness” as computed by HITS upon these cluster-document\ngraphs yields better performance than that of a previously\nproposed PageRank implementation applied to document-\nonly graphs. Interestingly, we also ﬁnd that cluster author-\nity scores can be used to identify clusters containing a larg e\npercentage of relevant documents.\n2. ALGORITHMS FOR RE-RANKING\nSince we are focused on the structural re-ranking paradigm,\nour algorithms are applied not to the entire corpus, but to a\nsubset DN,q\ninit (henceforth Dinit), deﬁned as the top N docu-\nments retrieved in response to the query q by a given initial\nretrieval engine. Some of our algorithms also take into ac-\ncount a set C l(Dinit) of clusters of the documents in Dinit.\nWe use Sinit to refer generically to whichever set of entities\n— either Dinit or Dinit ∪ C l(Dinit) — is used by a given\nalgorithm.\nThe basic idea behind the algorithms we consider is to\ndetermine centrality within a relevance-ﬂow graph , deﬁned\nas a directed graph with non-negative weights on the edges\nin which\n• the nodes are the elements of Sinit, and\n• the weight on an edge between node u and v is based\non the strength of evidence for v’s relevance that would\nfollow from an assertion that u is relevant.\nBy construction, then, any measure of the centrality of s ∈\nSinit should measure the accumulation of evidence for its rel-\nevance according to the set of interconnections among the\nentities in Sinit. Such information can then optionally be\nsubjected to additional processing, such as integration wi th\ninformation on each item’s similarity to the query, to pro-\nduce a ﬁnal re-ranking of Dinit.\nConventions regarding graphs.The types of relevance-\nﬂow graphs we consider can all be represented as weighted\ndirected graphs of the form ( V, wt), where V is a ﬁnite non-\nempty set of nodes and wt : V × V → [0, ∞ ) is a non-\nnegative edge-weight function. Note that thus our graphs\ntechnically have edges between all ordered pairs of nodes\n(self-loops included); however, edges with zero edge-weig ht\nare conceptually equivalent to missing edges. For clarity, we\nwrite wt(u → v) instead of wt(u, v ).\n2.1 Hubs, authorities, and the HITS algorithm\nThe HITS algorithm for computing centrality can be mo-\ntivated as follows. Let G = ( V, wt) be the input graph, and\nlet v be a node in V . First, suppose we somehow knew the\nhub score hub(u) of each node u ∈ V , where “hubness” is\nthe extent to which the nodes that u points to are “good”\nin some sense. Then, v’s authority score\nauth(v) =\nX\nu∈V\nwt(u → v) ·hub(u) (1)\nwould be a natural measure of how “good” v is, since a node\nthat is “strongly” pointed to by high-quality hubs (which,\nby deﬁnition, tend to point to “good” nodes) receives a high\nscore. But where do we get the hub score for a given node u?\nA natural choice is to use the extent to which u “strongly”\npoints to highly authoritative nodes:\nhub(u) =\nX\nv∈V\nwt(u → v) ·auth(v). (2)\nClearly, Equations 1 and 2 are mutually recursive. However,\nthe iterative HITS algorithm 1 provably converges to (non-\nidentically-zero, non-negative) score functions hub∗ and auth∗\nthat satisfy the above pair of equations.\nFigure 1 depicts the “iconic” case in which the input graph\nG is one-way bipartite , that is, V can be partitioned into\nnon-empty sets VLeft and VRight such that only edges in\nVLeft × VRight can receive positive weight, and ∀u ∈ VLeft,P\nv∈VRight\nwt(u → v) > 0. It is the case that auth∗(u) = 0\nfor every u ∈ VLeft and hub∗(v) = 0 for every v ∈ VRight;\nin this sense, the left-hand nodes are “pure” hubs and the\nright-hand nodes are “pure” authorities.\nFigure 1: A one-way bipartite graph. We only show\npositive-weight edges (omitting weight values). Ac-\ncording to HITS, the left-hand nodes are (pure)\nhubs; the right-hand ones are (pure) authorities.\nNote that in the end, we need to produce a single cen-\ntrality score for each node n ∈ V . For experimental sim-\nplicity, we consider only two possibilities in this paper —\nusing auth∗(n) as the ﬁnal centrality score, or using hub∗(n)\ninstead— although combining the hub and authority scores\nis also an interesting possibility.\n2.2 Graph schemata: incorporating clusters\nRecall that the fundamental operation in our structural\nre-ranking paradigm is to compute the centrality of entitie s\n(with)in a set Sinit. One possibility is to deﬁne Sinit as\nDinit, the documents in the initially retrieved set; we refer\ngenerically to any relevance-ﬂow graph induced under this\nchoice as a document-to-document graph. But note that\nfor non-Web documents, it may not be obvious a priori what\nkinds of documents are hubs and what kinds are authorities.\nAlternatively, we can deﬁne Sinit as Dinit∪ C l(Dinit), where\nC l(Dinit) consists of clusters of the documents in Dinit. On\na purely formal level, doing so allows us to map the hubs/au-\nthorities duality discussed above onto the documents/clus ters\nduality, as follows. Recalling our discussion of the “iconi c”\ncase of one-way bipartite graphs G = (( VLeft, V Right), wt),\nwe can create document-as-authority graphs simply by\nchoosing VLeft = C l(Dinit) and VRight = Dinit, so that neces-\nsarily clusters serve the role of (pure) hubs and documents\nserve the role of (pure) authorities. Contrariwise, 2 we can\n1Strictly speaking, the algorithm and proof of convergence\nas originally presented [16] need (trivial) modiﬁcation to ap-\nply to edge-weighted graphs.\n2In practice, one can simultaneously compute the output of\ncreate document-as-hub graphs by setting VLeft = Dinit\nand VRight = C l(Dinit).\nBut the advantages of incorporating cluster-based infor-\nmation are not just formal. The well-known cluster hypoth-\nesis [35] encapsulates the intuition that clusters can reveal\ngroups of relevant documents; in practice, the potential ut il-\nity of clustering for this purpose has been demonstrated a\nnumber of times, whether the clusters were created in a\nquery-independent fashion [14, 4], or from the initially mo st-\nhighly-ranked documents for some query [13, 22, 34] (i.e.,\nin the re-ranking setting). Since central clusters are, sup -\nposedly, those that accrue the most evidence for relevance,\ndocuments that are strongly identiﬁed with such clusters\nshould themselves be judged highly relevant. 3 4 But identi-\nfying such clusters is facilitated by knowledge of which doc -\numents are most likely to be relevant — exactly the mutual\nreinforcement property that HITS was designed to leverage.\n2.3 Alternative scores: PageRank and inﬂux\nWe will compare the results of using the HITS algorithm\nagainst those derived using PageRank instead. This is a nat-\nural comparison because PageRank is the most well-known\ncentrality-induction algorithm utilized for ranking docu ments,\nand because in earlier work [18], PageRank performed quite\nwell as a tool for structural re-ranking of non-Web doc-\numents, at least when applied to document-to-document\ngraphs.\nOne can think of PageRank as a version of HITS in which\nthe hub/authority distinction has been collapsed. Thus,\nwriting “ PR” for both auth and hub, we conceptually have\nthe (single) equation\nPR(v) =\nX\nu∈V\nwt(u → v) ·PR(u). (3)\nHowever, in practice, we incorporate Brin and Page’s smooth -\ning scheme [3] together with a correction for nodes with no\npositive-weight edges emanating from them [27, 21]:\nPR(v) =\nX\nu∈V :out(u)>0\n» (1 − λ)\n|V | + λ wt(u → v)\nout(u)\n–\n·PR(u)\n+\nX\nu∈V :out(u)=0\n1\n|V | ·PR(u) (4)\nwhere out(u)\ndef\n= P\nv′∈V wt(u → v′), and λ ∈ (0, 1) is the\ndamping factor. 5\nHITS for a given document-as-authority and document-as-\nhub graph pair by “overlaying” the two into a single graph\nand suitably modifying HITS’s normalization scheme.\n3We say “are strongly identiﬁed with”, as opposed to “be-\nlong to” to allow for overlapping or probabilistic clusters .\nIndeed, the one-way bipartite graphs we construct are ill-\nsuited to the HITS algorithm if document-to-cluster links\nare based on membership in disjoint clusters.\n4This is, in some sense, a type of smoothing: a document\nmight be missing some of the query terms (perhaps due\nto synonymy), but if it lies within a sector of “document\nspace” containing many relevant documents, it could still\nbe deemed highly relevant. Recent research pursues this\nsmoothing idea at a deeper level [25, 17].\n5Under the original “random surfer” model, the sum of the\ntransition probabilities out of “no outﬂow” nodes — which\nare abundant in one-way bipartite graphs — would be (1 −\nλ), not 1. Conceptually, the role of the second summation\nin Equation 4 is to set λ = 0 for these no-outﬂow nodes.\nEquation 4 is recursive, but there are iterative algorithms\nthat provably converge to the unique positive solution PR∗\nsatisfying the sum-normalization constraint P\nv∈V PR(v) =\n1 [21]. Moreover, a (non-trivial) closed-form — and quite\neasily computed — solution exists for one-way bipartite gra phs:\nTheorem 1. If G = ( V, wt) is one-way bipartite, then\nPRbip(v)\ndef\n=\nX\nu∈V :out(u)>0\nwt(u → v)\nout(u) (5)\nis an aﬃne transformation (with respect to positive con-\nstants) of, and therefore equivalent for ranking purposes t o,\nthe unique positive sum-normalized solution to Equation 4.\n(Proof omitted due to space constraints.) Interestingly, t his\nresult shows that while one might have thought that clusters\nand documents would “compete” for PageRank score when\nplaced within the same graph, in our document-as-authority\nand document-as-hub graphs this is not the case.\nEarlier work [18] also considered scoring a node v by its\ninﬂux, P\nu∈V wt(u → v). This can be viewed as either a\nnon-recursive version of Equation 3, or as an un-normalized\nanalog of Equation 5.\n2.4 Algorithms based on centrality scores\nClearly, we can rank documents by their scores as com-\nputed by any of the functions introduced above. But when\nwe operate on document-as-authority or document-as-hub\ngraphs, centrality scores for the clusters are also produce d.\nThese can be used to derive alternative means for ranking\ndocuments. We follow Liu and Croft’s approach [25]: ﬁrst,\nrank the documents within (or most strongly associated to)\neach cluster according to the initial retrieval engine’s sc ores;\nthen, derive the ﬁnal list by concatenating the within-clus ter\nlists in order of decreasing cluster score, discarding repe ats.\nSuch an approach would be successful if cluster centrality i s\nstrongly correlated with the property of containing a large\npercentage of relevant documents.\nRanking algorithms.Since we have two possible rank-\ning paradigms, we adopt the following algorithm naming\nconventions. Names consist of a hyphen-separated preﬁx\nand suﬃx. The preﬁx (“doc” or “clust”) indicates whether\ndocuments were ranked directly by their centrality scores, or\nindirectly through the concatenation process outlined abo ve\nin which it is the clusters’ centrality scores that were em-\nployed. The suﬃx (“Auth”, “Hub”, “ PR”, or “Inﬂux”) indi-\ncates which score function ( auth∗, hub∗, PR∗ (or PRbip), or\ninﬂux) was used to measure centrality. For a given re-rank-\ning algorithm, we indicate the graph upon which it was run\nin brackets, e.g., “doc-Auth[ G]”.\n3. RELATED WORK\nThe potential merits of query-dependent clustering , that\nis, clustering the documents retrieved in response to a quer y,\nhave long been recognized [30, 36, 23, 34, 25], especially in\ninteractive retrieval settings [13, 22, 32]. However, auto mat-\nically detecting clusters that contain many relevant docu-\nments remains a very hard task [36]. Section 5.2 presents re-\nsults for detecting such clusters using centrality-based c lus-\nter ranking.\nRecently, there has been a growing body of work on graph-\nbased modeling for diﬀerent language-processing tasks whe re-\nin links are induced by inter-entity textual similarities. Ex-\namples include document (re-)ranking [7, 24, 9, 18, 39], tex t\nsummarization [11, 26], sentence retrieval [28], and docu-\nment representation [10]. In contrast to our methods, links\nconnect entities of the same type, and clusters of entities a re\nnot modeled within the graphs.\nWhile ideas similar to ours by virtue of leveraging the\nmutual reinforcement of entities of diﬀerent types, or usin g\nbipartite graphs of such entities for clustering (rather th an\nusing clusters), are abundant (e.g., [15, 8, 2]), we focus he re\non exploiting mutual reinforcement in ad hoc retrieval.\nRandom walks (with early stopping) over bipartite graphs\nof terms and documents were used for query expansion [20],\nbut in contrast to our work, no stationary solution was\nsought. A similar “short chain” approach utilizing bipar-\ntite graphs of clusters and documents for ranking an en-\ntire corpus was recently proposed [19], thereby constituti ng\nthe work most resembling ours. However, again, a station-\nary distribution was not sought. Also, query drift preven-\ntion mechanisms were required to obtain good performance;\nin our re-ranking setting, we need not employ such mecha-\nnisms.\n4. EV ALUATION FRAMEWORK\nMost aspects of the evaluation framework described be-\nlow are adopted from our previous experiments with non-\ncluster-based structural re-ranking [18] so as to facilita te\ndirect comparison. Section 4.1 of [18] provides a more de-\ntailed justiﬁcation of the experimental design. The main\nconceptual changes 6 here are: a slightly larger parameter\nsearch-space for the “out-degree” parameter δ (called the\n“ancestry” parameter α in [18]); and, of course, the incor-\nporation of clusters.\n4.1 Graph construction\nRelevance ﬂow based on language models (LMs).To\nestimate the degree to which one item, if considered rele-\nvant, can vouch for the relevance of another, we follow our\nprevious work on document-based graphs [18] and utilize\np[µ ]\nd (·), the unigram Dirichlet-smoothed language model in-\nduced from a given document d (µ is the smoothing pa-\nrameter) [38]. To adapt this estimation scheme to settings\ninvolving clusters, we derive the language model p[µ ]\nc (·) for\na cluster c by treating c as the (large) document formed by\nconcatenating7 its constituent (or most strongly associated)\ndocuments [17, 25, 19].\nThe relevance-ﬂow measure we use is essentially a directed\nsimilarity in language-model space:\nrﬂow(x, y )\ndef\n= exp\n“\n− D\n“\np [0]\nx (·)\n˛\n˛\n˛\n˛\n˛\n˛ p[µ ]\ny (·)\n””\n, (6)\nwhere D is the Kullback-Leibler divergence. The asymme-\ntry of this measure corresponds nicely to the intuition that\nrelevance ﬂow is not symmetric [18]. Moreover, this functio n\n6Some of the PageRank results appearing in our previous\npaper [18] accidentally reﬂect experiments utilizing a sub -\noptimal choice of Dinit. For citation purposes, the numbers\nreported in the current paper should be used.\n7Concatenation order is irrelevant for unigram LMs.\nis somewhat insensitive to large length diﬀerences between\nthe items in question [18], which is advantageous when both\ndocuments and clusters (which we treat as very long docu-\nments) are considered.\nPrevious work [18, 33] makes heavy use of the idea of near-\nest neighbors in language-model space. It is therefore conv e-\nnient to introduce the notation N bhd(x |m, R ), pronounced\n“neighborhood”, to denote the m items y within the “re-\nstriction set” R that have the highest values of rﬂow(x, y )\n(we break ties by item ID, assuming that these have been\nassigned to documents and clusters). Note that the neigh-\nborhood of x corresponds to what we previously termed the\n“top generators” of x [18].\nGraphs used in experiments.For a given set Dinit of ini-\ntially retrieved documents and positive integer δ (an “out-\ndegree” parameter), we consider the following three graphs .\nEach connects nodes u to the δ other nodes, drawn from\nsome speciﬁed set, that u has the highest relevance ﬂow to.\nThe document-to-document graph d↔ d has vertex set\nDinit and weight function\nwtd↔ d(u, v ) =\n(\nrﬂow(u, v ) if v ∈ N bhd(u |δ, Dinit − { u}),\n0 otherwise .\nThe document-as-authority graph c→ d has vertex set Dinit∪\nC l(Dinit) and a weight function such that positive-weight\nedges go only from clusters to documents:\nwtc→ d(u, v ) =\n8\n>\n<\n>\n:\nrﬂow(u, v ) if u ∈ C l(Dinit) and\nv ∈ N bhd(u |δ, Dinit),\n0 otherwise .\nThe document-as-hub graph d→ c has vertex set Dinit ∪\nC l(Dinit) and a weight function such that positive-weight\nedges go only from documents to clusters:\nwtd→ c(u, v ) =\n8\n>\n<\n>\n:\nrﬂow(u, v ) if u ∈ D init and\nv ∈ N bhd(u |δ, C l(Dinit)),\n0 otherwise .\nSince the latter two graphs are one-way bipartite, Theo-\nrem 1 applies to them.\nClustering Method.Clearly, our cluster-based graphs re-\nquire the construction of clusters of the documents in Dinit.\nSince this set is query-dependent, at least some of the clus-\ntering process must occur at retrieval time, mandating the\nuse of extremely eﬃcient algorithms [6, 37]. The approach\nwe adopt is to use overlapping nearest-neighbor clusters,\nwhich have formed the basis of eﬀective retrieval algorithm s\nin other work [12, 17, 19, 33]: for each document d ∈ D init,\nwe have the cluster {d} ∪ N bhd(d |k − 1, Dinit − { d}), where\nk is the cluster-size parameter.\n4.2 Experimental Setup\nWe conducted our experiments on three TREC datasets:\ncorpus # of docs queries disk(s)\nAP 242,918 51-64, 66-150 1-3\nTREC8 528,155 401-450 4-5\nWSJ 173,252 151-200 1-2\nWe applied basic tokenization and Porter stemming via the\nLemur toolkit (www.lemurproject.org), which we also used\nfor language-model induction. Topic titles served as queri es.\nAP TREC8 WSJ\nprec@5 prec@10 MRR prec@5 prec@10 MRR prec@5 prec@10 MRR\ndoc-Auth[d↔ d] .509 .486 .638 .440 .424 .648 .504 .464 .638\ndoc-PageRank[d↔ d] .519 .480 .632 .524 .446 .666 .536 .486 .699\ndoc-Auth[c→ d] .541 .501 p .669 p .544 a .452 .674 .564 a .514 a .746 a\nTable 1: Main comparison: HITS or PageRank on document-only graphs versus HITS on cluster-to-document\ngraphs. Bold: best results per column. Symbols “ p” and “ a”: doc-Auth[c → d] result diﬀers signiﬁcantly from\nthat of doc-PageRank[d ↔ d] or doc-Auth[d ↔ d], respectively.\nIn many retrieval situations of interest, ensuring that the\ntop few documents retrieved (a.k.a., “the ﬁrst page of re-\nsults”) tend to be relevant is much more important than en-\nsuring that we assign relatively high ranks to the entire set of\nrelevant documents in aggregate [31]. Hence, rather than us e\nmean average precision (MAP) as an evaluation metric, we\napply metrics more appropriate to the structural re-rankin g\ntask: precision at the top 5 and 10 documents (henceforth\nprec@5 and prec@10, respectively) and the mean reciprocal\nrank (MRR) of the ﬁrst relevant document [31]. All perfor-\nmance numbers are averaged over the set of queries for a\ngiven corpus.\nThe natural baseline for the work described here is the\nstandard language-model-based retrieval approach [29, 5] ,\nsince it is an eﬀective paradigm that makes no explicit use of\ninter-document relationships. Speciﬁcally, for a given ev al-\nuation metric e, the corresponding optimized baseline is the\nranking on documents produced by p[µ (e)]\nd (q), where µ(e) is\nthe value of the Dirichlet smoothing parameter that results\nin the best retrieval performance as measured by e.\nA ranking method might assign diﬀerent items the same\nscore; we break such ties by item ID. Alternatively, the\nscores used to determine Dinit can be utilized, if available.\nParameter selection for graph-based methods.There\nare two motivations underlying our approach to choosing\nvalues for our algorithms’ parameters [18].\nFirst, we hope to show that structural re-ranking can\nprovide better results than the optimized baselines even\nwhen initialized with a sub-optimal (yet reasonable) rank-\ning. Hence, let the initial ranking be the document ordering\ninduced on the entire corpus by p [µ 1000]\nd (q), where µ1000 is\nthe smoothing-parameter value optimizing the average non-\ninterpolated precision of the top 1000 documents. We set\nDinit to the top 50 documents in the initial ranking.\nSecond, we wish to show that good results can be achieved\nwithout a great deal of parameter tuning. Therefore, we did\nnot tune the smoothing parameter for any of the language\nmodels used to determine graph edge-weights, but rather\nsimply set µ = 2000 when smoothing was required, following\na prior suggestion [38]. Also, the other free parameters’ val-\nues were chosen so as to optimize prec@5, regardless of the\nevaluation metric under consideration. 8 As a consequence,\nour prec@10 and MRR results are presumably not as high\nas possible; but the advantage of our policy is that we can\nsee whether optimization with respect to a ﬁxed criterion\nyields good results no matter how “goodness” is measured.\n8If two diﬀerent parameter settings yield the same prec@5,\nwe choose the setting minimizing prec@10 so as to provide\na conservative estimate of expected performance. Similarl y,\nif we have ties for both prec@5 and prec@10, we choose the\nsetting minimizing MRR.\nParameter values were selected from the following sets.\nThe graph “out-degree” δ: {2, 4, 9, 19, 29, 39, 49}. The clus-\nter size k: {2, 5, 10, 20, 30}. The PageRank damping factor\nλ: {0. 05, 0. 1 . . . 0. 9, 0. 95}.\n5. EXPERIMENTAL RESULTS\nIn what follows, when we say that results or the diﬀerence\nbetween results are “signiﬁcant”, we mean according to the\ntwo-sided Wilcoxon test at a conﬁdence level of 95%.\n5.1 Re-Ranking by Document Centrality\nMain result.We ﬁrst consider our main question: can we\nsubstantially boost the eﬀectiveness of HITS by applying it\nto cluster-to-document graphs, which we have argued are\nmore suitable for it than the document-to-document graphs\nwe constructed in our previous work [18]? The answer, as\nshown in Table 1, is clearly “yes”: we see that moving to\ncluster-to-document graphs results in substantial improv e-\nment for HITS, and indeed boosts its results over those for\nPageRank on document-to-document graphs.\nFull suite of comparisons.We now turn to Figure 2,\nwhich gives the results for the re-ranking algorithms doc-\nInﬂux, doc-PageRank and doc-Auth as applied to either\nthe document-based graph d ↔ d (as in [18]) or the cluster-\ndocument graph c → d. (Discussion of doc-Hub is deferred\nto Section 5.3.)\nTo focus our discussion, it is useful to ﬁrst point out that\nin almost all of our nine evaluation settings (3 corpora × 3\nevaluation measures), all three of the re-ranking algorith ms\nperform better when applied to c → d graphs than to d ↔ d\ngraphs, as the number of dark bars in Figure 2 indicates.\nSince it is thus clearly useful to incorporate cluster-base d\ninformation, we will now mainly concentrate on c → d-based\nalgorithms.\nThe results for prec@5, the metric for which the re-ranking\nalgorithms’ parameters were optimized, show that all c→ d-\nbased algorithms outperform the prec@5-optimized baselin e\n— signiﬁcantly so for the AP corpus — even though applied\nto a sub-optimally-ranked initial set. (We hasten to point\nout that while the initial ranking is always inferior to the\ncorresponding optimized baseline, the diﬀerences are neve r\nsigniﬁcant.) In contrast, the use of d ↔ d graphs never leads\nto signiﬁcantly superior prec@5 results.\nWe also observe in Figure 2 that the doc-Auth[c → d] al-\ngorithm is always either the best of the c → d-based algo-\nrithms or clearly competitive with the best. Furthermore,\npairwise comparison of it to each of the doc-Inﬂux[c → d]\nand doc-PageRank[c → d] algorithms favors the HITS-style\ndoc-Auth[c→ d] algorithm in a majority of the evaluation\nsettings.\n 0.42\n 0.44\n 0.46\n 0.48\n 0.5\n 0.52\n 0.54\n 0.56\n 0.58\ndoc-Authdoc-PageRankdoc-Influxopt baselineinit rankingdoc-Authdoc-PageRankdoc-Influxopt baselineinit rankingdoc-Authdoc-PageRankdoc-Influxopt baselineinit ranking\nprec@5\nAP TREC8 WSJ\n 0.42\n 0.43\n 0.44\n 0.45\n 0.46\n 0.47\n 0.48\n 0.49\n 0.5\n 0.51\n 0.52\ndoc-Authdoc-PageRankdoc-Influxopt baselineinit rankingdoc-Authdoc-PageRankdoc-Influxopt baselineinit rankingdoc-Authdoc-PageRankdoc-Influxopt baselineinit ranking\nprec@10\nAP TREC8 WSJ\n 0.58\n 0.6\n 0.62\n 0.64\n 0.66\n 0.68\n 0.7\n 0.72\n 0.74\n 0.76\n 0.78\ndoc-Authdoc-PageRankdoc-Influxopt baselineinit rankingdoc-Authdoc-PageRankdoc-Influxopt baselineinit rankingdoc-Authdoc-PageRankdoc-Influxopt baselineinit ranking\nMRR\nAP TREC8 WSJ\nbaselines\nno clusters (d<->d)\nwith clusters (c->d): GAIN\nwith clusters (c->d): LOSS\nFigure 2: All re-ranking algorithms, as applied to\neither d ↔ d graphs or c → d graphs.\nWe also experimented with a few alternate graph-constructi on\nmethods, such as sum-normalizing the weights of edges out\nof nodes, and found that the doc-Auth[c → d] algorithm re-\nmained superior to doc-Inﬂux[c → d] and doc-PageRank[c → d].\nWe omit these results due to space constraints.\nAll in all, these ﬁndings lead us to believe that not only is\nit useful to incorporate information from clusters, but it c an\nbe more eﬀective to do so in a way reﬂecting the mutually-\nreinforcing nature of clusters and documents, as the HITS\nalgorithm does.\n5.2 Re-Ranking by Cluster Centrality\nWe now consider the alternative, mentioned in Section 2.4,\nof using the centrality scores for clusters as an indirect means\nof ranking documents, in the sense of identifying clusters\nthat contain a high percentage of relevant documents. Note\nthat the problem of automatically identifying such cluster s\nin the re-ranking setting has been acknowledged to be a hard\ntask for some time [36]. Nevertheless, as stated in Section\n2.4, we experimented with Liu and Croft’s general clusters-\nfor-selection approach [25]: rank the clusters, then rank t he\ndocuments within each cluster by p[µ ]\nd (q). Our baseline algo-\nrithm, clust- p[µ ]\nc (q), adopts Liu and Croft’s speciﬁc proposal\nof the CQL algorithm — except that we employ overlapping\nrather than hard clusters — wherein clusters are ranked by\nthe query likelihood p[µ ]\nc (q) instead of one of our centrality\nscores.\nTable 2 (which may appear on the next page) presents the\nperformance results. Our ﬁrst observation is that the clust -\nInﬂux[d→ c] and clust-Auth[d → c] algorithms are superior in\na majority of the relevant comparisons to the initial rank-\ning, the optimized baselines, and the clust- p[µ ]\nc (q) algorithm,\nwhere the performance diﬀerences with the latter sometimes\nachieve signiﬁcance.\nHowever, the performance of the document-centrality-base d\nalgorithm doc-Auth[c → d] is better in a majority of the eval-\nuation settings than that of any of the cluster-centrality-\nbased algorithms. On the other hand, it is possible that the\nlatter methods could be improved by a better technique for\nwithin-cluster ranking.\nTo compare the eﬀectiveness of clust-Inﬂux[d → c] and clust-\nAuth[d→ c] to that of clust- p[µ ]\nc (q) in detecting clusters with\na high percentage of relevant documents — thereby neutral-\nizing within-cluster ranking eﬀects — we present in Table 3\nthe percent of documents in the highest ranked cluster that\nare relevant. (Cluster size ( k) was ﬁxed to either 5 or 10\nand out-degree ( δ) was chosen to optimize the above per-\ncentage.) Indeed, these results clearly show that our best\ncluster-based algorithms are much better than clust- p[µ ]\nc (q)\nin detecting clusters containing a high percentage of relev ant\ndocuments, in most cases to a signiﬁcant degree.\nCluster AP TREC8 WSJ\nranking k=5 k=10 k=5 k=10 k=5 k=10\np[µ ]\nc (q) 39.2 38.8 39.6 40.6 44.0 37.0\nInﬂux[d→ c] 48.7 c 47.6 c 48.0 43.8 51.2 c 48.0 c\nAuth[d→ c] 49.5 c 47.2 c 50.8 c 46.6 53.6 c 49.0 c\nTable 3: Average relevant-document percentage\nwithin the top-ranked cluster. k: cluster size. Bold:\nbest results per column. c: result diﬀers signiﬁ-\ncantly from that of clust- p[µ ]\nc (q), used in [25].\n5.3 Further Analysis\nAuthorities versus hubs.So far, we have only considered\nutilizing the authority scores that the HITS algorithm pro-\nduces. The chart below shows the eﬀect of ranking enti-\nties by hub scores instead. Speciﬁcally, the “documents?”\ncolumn compares doc-Auth[c → d] (i.e., ranking documents\nby authoritativeness) to doc-Hub[d → c] (i.e., ranking docu-\nments by hubness); similarly, the “clusters?” column com-\npares clust-Auth[d → c] to clust-Hub[c → d]. Each entry de-\npicts, in descending order of performance (except for the\none indicated tie) as one moves left to right, those central-\nity scoring functions that lead to an improvement over the\ninitial ranking: A stands for “authority” and H for “hub”.\nCases in which the improvement is signiﬁcant are marked\nAP TREC8 WSJ\nprec@5 prec@10 MRR prec@5 prec@10 MRR prec@5 prec@10 MRR\ninit. ranking . 457 . 432 . 596 . 500 . 456 . 691 . 536 . 484 . 748\nopt. baselines . 465 . 439 . 635 . 512 . 464 . 696 . 560 . 494 .772\nclust-p[µ ]\nc (q) . 448 . 418 . 549 io . 500 . 432 .723 . 504 o . 454 io . 680\nclust-Inﬂux[d→ c] . 511 c .479 c . 619 . 524 .478 . 681 .568 c .512 c . 760\nclust-PageRank[d→ c] . 493 . 475 c . 595 . 496 . 444 . 683 . 528 . 490 c . 736\nclust-Auth[d→ c] .533 ioc . 478 c .651 c .532 . 460 . 714 . 552 . 478 . 757\nTable 2: Cluster-based re-ranking. Bold: best results per c olumn. Symbols i, o, c: results diﬀer signiﬁcantly\nfrom the initial ranking, optimized baseline, or (for the re -ranking algorithms) clust- p[µ ]\nc (q) [25], respectively.\nwith a ‘*’.\nWhen do we improve the initial ranking\nby measuring the centrality of:\ndocuments? clusters?\nprec @5 A∗H A ∗H\nAP prec @10 A∗H AH\nMRR AH A\nprec @5 AH AH\nTREC8 prec @10 HA\nMRR H H ∗A\nprec @5 AH AH (tie)\nWSJ prec @10 AH H\nMRR HA\nWe see that in many cases, hub-based re-ranking does\nyield better performance than the initial ranking. But auth ority-\nbased re-ranking appears to be an even better choice overall .\nHITS on PageRank-style graphs.Consider our compari-\nson of doc-Auth[d ↔ d] against doc-PageRank[d ↔ d]. As the\nnotation suggests, this corresponds to running HITS and\nPageRank on the same graph, d ↔ d. But an alternative in-\nterpretation [18] is that non-smoothed (or no-random-jump )\nPageRank, as expressed by Equation (3), is applied to a\ndiﬀerent version of d ↔ d wherein the original edge weights\nwt(u → v) have been smoothed as follows:\nwt[λ ](u → v)\ndef\n= 1 − λ\n|V | + λ wt(u → v)\nout(u) (7)\n(we ignore nodes with no positive-weight out-edges to sim-\nplify discussion, and omit the d ↔ d superscripts for clarity).\nHow does HITS perform on document-to-document graphs\nthat are “truly equivalent”, in the sense of employing the\nabove edge-weighting regime, to those that PageRank is\napplied to? One reason this is an interesting question is\nthat HITS assigns scores of zero to nodes that are not in\nthe graph’s largest connected component (with respect to\npositive-weight edges, considered to be bi-directional). No-\ntice that the original graph may have several connected com-\nponents, whereas utilizing wt[λ ] ensures that each node has a\npositive-weight directed edge to every other node. Additio n-\nally, the re-weighted version of HITS has provable stabilit y\nproperties [27].\nWe found that in nearly all of our evaluation settings for\ndocument-to-document graphs (three corpora × three eval-\nuation metrics), doc-Auth[d ↔ d] achieved better results us-\ning wt[λ ] edge weights. However, we cannot discount the\npossibility that the performance diﬀerences might be due\nsimply to the inclusion of the extra interpolation-paramet er\nλ. Moreover, in all but one case, the improved results were\nstill below those for doc-PageRank[d ↔ d] (and always lagged\nbehind those of doc-Auth[c → d]).\nInterestingly, the situation is qualitatively diﬀerent if we\nconsider c → d graphs instead. In brief, we applied a smooth-\ning scheme analogous to that described above, but only to\nedges leading from a left-hand node (cluster) to a right-han d\nnode (document) 9; we thus preserved the one-way bipar-\ntite structure. Only in two of the nine evaluation settings\ndid this change cause an increase in performance of doc-\nAuth[c→ d] over the results attained under the original edge-\nweighting scheme, despite the fact that the re-weighting in -\nvolves an extra free parameter. Thus, while we have al-\nready demonstrated in previous sections of this paper that\ninformation about document-cluster similarity relations hips\nis very valuable, the results just mentioned suggest that su ch\ninformation is more useful in “raw” form.\nRe-anchoring to the query.In previous work, we showed\nthat PageRank centrality scores induced over document-\nbased graphs can be used as a multiplicative weight on\ndocument query-likelihood terms, the intent being to cope\nwith cases in which centrality in Dinit and relevance are not\nstrongly correlated [18]. Indeed, employing this techniqu e\non the AP, TREC8, and WSJ corpora, prec@5 increases\nfrom . 519, . 524 and . 536, to . 531, . 56 and . 572 respectively.\nThe same modiﬁcation could be applied to the c → d-based\nalgorithms, although it is not particularly well-motivate d in\nthe HITS case. While PageRank scores correspond to a\nstationary distribution that could be loosely interpreted as\na prior [18], in which case multiplicative combination with\nquery likelihood is sensible, it is not usual to assign a prob -\nabilistic interpretation to hub or authority scores.\nNonetheless, for the sake of comparison completeness, we\napplied this idea to the doc-Auth[c → d] algorithm, yield-\ning the following performance changes: from . 541, . 544, and\n. 564 to . 537, . 572 and . 572 respectively. These results are\nstill as good as — and for two corpora better than — those\nfor PageRank as a multiplicative weight on query likelihood .\nThus, it may be the case that centrality scores induced over a\ndocument-based graph are more eﬀective as a multiplicative\nbias on query-likelihood than as direct representations of rel-\nevance in Dinit (see also [18]); but, modulo the caveat above,\nit seems that when centrality is induced over cluster-based\n9In the one-way bipartite case, the “ |V |” in Equation (7)\nmust be changed to the number of right-hand nodes.\none-way bipartite graphs, the correlation with relevance i s\nmuch stronger, and hence this kind of centrality serves as a\nbetter “bias” on query-likelihood.\n6. CONCLUSION\nWe have shown that leveraging the mutually reinforcing\nrelationship between clusters and documents to determine\ncentrality is very beneﬁcial not only for directly ﬁnding re l-\nevant documents in an initially retrieved list, but also for\nﬁnding clusters of documents from this list that contain a\nhigh number of relevant documents.\nSpeciﬁcally, we demonstrated the superiority of cluster-\ndocument bipartite graphs to document-only graphs as the\ninput to centrality-induction algorithms. Our method for\nﬁnding “authoritative” documents (or clusters) using HITS\nover these bipartite graphs results in state-of-the-art pe rfor-\nmance for document (and cluster) re-ranking.\nAcknowledgments We thank Eric Breck, Claire Cardie, Oren Et-\nzioni, Jon Kleinberg, Art Munson, Filip Radlinski, Ves Stoy anov,\nJustin Wick and the anonymous reviewers for valuable commen ts.\nThis paper is based upon work supported in part by the Nationa l\nScience Foundation under grant no. IIS-0329064 and an Alfre d P.\nSloan Research Fellowship. Any opinions, ﬁndings, and conc lusions\nor recommendations expressed are those of the authors and do not\nnecessarily reﬂect the views or oﬃcial policies, either exp ressed or\nimplied, of any sponsoring institutions, the U.S. governme nt, or any\nother entity.\n7. REFERENCES\n[1] J. Bali´ nski and C. Dani/suppress lowicz. Re-ranking method based on\ninter-document distances. Information Processing and\nManagement, 41(4):759–775, 2005.\n[2] D. Beeferman and A. L. Berger. Agglomerative clustering of a\nsearch engine query log. In Proceedings of KDD , pages\n407–416, 2000.\n[3] S. Brin and L. Page. The anatomy of a large-scale hypertex tual\nweb search engine. In Proceedings of the 7th International\nWorld Wide Web Conference , pages 107–117, 1998.\n[4] W. B. Croft. A model of cluster searching based on\nclassiﬁcation. Information Systems , 5:189–195, 1980.\n[5] W. B. Croft and J. Laﬀerty, editors. Language Modeling for\nInformation Retrieval . Number 13 in Information Retrieval\nBook Series. Kluwer, 2003.\n[6] D. R. Cutting, D. R. Karger, J. O. Pedersen, and J. W. Tukey .\nScatter/Gather: A cluster-based approach to browsing larg e\ndocument collections. In 15th Annual International SIGIR ,\npages 318–329, Denmark, June 1992.\n[7] C. Dani/suppress lowicz and J. Bali´ nski. Document ranking basedupon\nMarkov chains. Information Processing and Management ,\n41(4):759–775, 2000.\n[8] I. Dhillon. Co-clustering documents and words using bip artite\nspectral graph partitioning. In Proceedings of the Seventh\nACM SIGKDD Conference , pages 269–274, 2001.\n[9] F. Diaz. Regularizing ad hoc retrieval scores. In Proceedings of\nthe Fourteenth International Conference on Information an d\nKnowledge Managment (CIKM) , pages 672–679, 2005.\n[10] G. Erkan. Language model based document clustering usi ng\nrandom walks. In Proceedings of HLT/NAACL , 2006.\n[11] G. Erkan and D. R. Radev. LexRank: Graph-based lexical\ncentrality as salience in text summarization. Journal of\nArtiﬁcial Intelligence Research , 22:457–479, 2004.\n[12] A. Griﬃths, H. C. Luckhurst, and P. Willett. Using\ninterdocument similarity information in document retriev al\nsystems. Journal of the American Society for Information\nScience (JASIS) , 37(1):3–11, 1986. Reprinted in Karen Sparck\nJones and Peter Willett, eds., Readings in Information\nRetrieval, Morgan Kaufmann, pp. 365–373, 1997.\n[13] M. A. Hearst and J. O. Pedersen. Reexamining the cluster\nhypothesis: Scatter/Gather on retrieval results. In Proceedings\nof SIGIR , 1996.\n[14] N. Jardine and C. J. van Rijsbergen. The use of hierarchi c\nclustering in information retrieval. Information Storage and\nRetrieval, 7(5):217–240, 1971.\n[15] Y. Karov and S. Edelman. Similarity-based word sense\ndisambiguation. Computational Linguistics , 24(1):41–59, 1998.\n[16] J. Kleinberg. Authoritative sources in a hyperlinked\nenvironment. In Proceedings of the 9th ACM-SIAM\nSymposium on Discrete Algorithms (SODA) , pages 668–677,\n1998. Extended version in Journal of the ACM, 46:604–632,\n1999.\n[17] O. Kurland and L. Lee. Corpus structure, language model s,\nand ad hoc information retrieval. In Proceedings of SIGIR ,\npages 194–201, 2004.\n[18] O. Kurland and L. Lee. PageRank without hyperlinks:\nStructural re-ranking using links induced by language mode ls.\nIn Proceedings of SIGIR , pages 306–313, 2005.\n[19] O. Kurland, L. Lee, and C. Domshlak. Better than the real\nthing? Iterative pseudo-query processing using cluster-b ased\nlanguage models. In Proceedings of SIGIR , pages 19–26, 2005.\n[20] J. D. Laﬀerty and C. Zhai. Document language models, que ry\nmodels, and risk minimization for information retrieval. I n\nProceedings of SIGIR , pages 111–119, 2001.\n[21] A. N. Langville and C. D. Meyer. Deeper inside PageRank.\nInternet Mathematics , 2005.\n[22] A. Leuski. Evaluating document clustering for interac tive\ninformation retrieval. In Proceedings of the Tenth\nInternational Conference on Information and Knowledge\nManagment (CIKM) , pages 33–40, 2001.\n[23] A. Leuski and J. Allan. Evaluating a visual navigation s ystem\nfor a digital library. In Proceedings of the Second European\nconference on research and advanced technology for digital\nlibraries (ECDL) , pages 535–554, 1998.\n[24] G.-A. Levow and I. Matveeva. University of Chicago at\nCLEF2004: Cross-language text and spoken document\nretrieval. In Proceedings of CLEF , pages 170–179, 2004.\n[25] X. Liu and W. B. Croft. Cluster-based retrieval using la nguage\nmodels. In Proceedings of SIGIR , pages 186–193, 2004.\n[26] R. Mihalcea and P. Tarau. TextRank: Bringing order into\ntexts. In Proceedings of EMNLP , pages 404–411, 2004. Poster.\n[27] A. Y. Ng, A. X. Zheng, and M. I. Jordan. Stable algorithms for\nlink analysis. In Proceedings of SIGIR , pages 258–266, 2001.\n[28] J. Otterbacher, G. Erkan, and D. R. Radev. Using random\nwalks for question-focused sentence retrieval. In Proceedings of\nHuman Language Technology Conference and Conference on\nEmpirical Methods in Natural Language Processing\n(HLT/EMNLP), pages 915–922, 2005.\n[29] J. M. Ponte and W. B. Croft. A language modeling approach to\ninformation retrieval. In Proceedings of SIGIR , pages 275–281,\n1998.\n[30] S. E. Preece. Clustering as an output option. In Proceedings of\nthe American Society for Information Science , pages 189–190,\n1973.\n[31] C. Shah and W. B. Croft. Evaluating high accuracy retrie val\ntechniques. In Proceedings of SIGIR , pages 2–9, 2004.\n[32] X. Shen and C. Zhai. Active feedback in ad hoc informatio n\nretrieval. In Proceedings of SIGIR , pages 59–66, 2005.\n[33] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language model\ninformation retrieval with document expansion. In Proceedings\nof HLT/NAACL , 2006.\n[34] A. Tombros, R. Villa, and C. van Rijsbergen. The eﬀectiv eness\nof query-speciﬁc hierarchic clustering in information ret rieval.\nInformation Processing and Management , 38(4):559–582,\n2002.\n[35] C. J. van Rijsbergen. Information Retrieval . Butterworths,\nsecond edition, 1979.\n[36] P. Willett. Query speciﬁc automatic document classiﬁc ation.\nInternational Forum on Information and Documentation ,\n10(2):28–32, 1985.\n[37] O. Zamir and O. Etzioni. Web document clustering: a\nfeasibility demonstration. In Proceedings of SIGIR , pages\n46–54, 1998.\n[38] C. Zhai and J. D. Laﬀerty. A study of smoothing methods fo r\nlanguage models applied to ad hoc information retrieval. In\nProceedings of SIGIR , pages 334–342, 2001.\n[39] B. Zhang, H. Li, Y. Liu, L. Ji, W. Xi, W. Fan, Z. Chen, and\nW.-Y. Ma. Improving web search results using aﬃnity graph.\nIn Proceedings of SIGIR , pages 504–511, 2005."
}