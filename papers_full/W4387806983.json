{
    "title": "Saturn Platform: Foundation Model Operations and Generative AI for Financial Services",
    "url": "https://openalex.org/W4387806983",
    "year": 2023,
    "authors": [
        {
            "id": null,
            "name": "Antonio J. G. Busson",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5104151882",
            "name": "Rennan Gaio",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5113027900",
            "name": "Rafael H. Rocha",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2334532126",
            "name": "Francisco Evangelista",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1896604892",
            "name": "Bruno Rizzi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4280676654",
            "name": "Luan Carvalho",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5102541674",
            "name": "Rafael Miceli",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5092590550",
            "name": "Marcos Rabaioli",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2980675531",
            "name": "David Favaro",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Antonio J. G. Busson",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5104151882",
            "name": "Rennan Gaio",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5113027900",
            "name": "Rafael H. Rocha",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2334532126",
            "name": "Francisco Evangelista",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1896604892",
            "name": "Bruno Rizzi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4280676654",
            "name": "Luan Carvalho",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5102541674",
            "name": "Rafael Miceli",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5092590550",
            "name": "Marcos Rabaioli",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2980675531",
            "name": "David Favaro",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2962739339",
        "https://openalex.org/W4386114394",
        "https://openalex.org/W4386711999",
        "https://openalex.org/W4382701340",
        "https://openalex.org/W4385713994",
        "https://openalex.org/W4285077564",
        "https://openalex.org/W4366208220",
        "https://openalex.org/W4378465439",
        "https://openalex.org/W4310013385",
        "https://openalex.org/W3170989320",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4386184788",
        "https://openalex.org/W2982223350",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3194210660",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4298879432",
        "https://openalex.org/W4307073150",
        "https://openalex.org/W3169291081",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4387337835",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4389010951",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4319452276",
        "https://openalex.org/W4376643691",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4283727637",
        "https://openalex.org/W4362598371",
        "https://openalex.org/W4386385650",
        "https://openalex.org/W3093122194",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W4394659899",
        "https://openalex.org/W4321472314",
        "https://openalex.org/W4317437684",
        "https://openalex.org/W2952729433",
        "https://openalex.org/W3184144760",
        "https://openalex.org/W4379620936",
        "https://openalex.org/W2943495267",
        "https://openalex.org/W4385397000",
        "https://openalex.org/W4304195432",
        "https://openalex.org/W4330336443",
        "https://openalex.org/W4287126489",
        "https://openalex.org/W4381683870",
        "https://openalex.org/W3160590016",
        "https://openalex.org/W4386271710",
        "https://openalex.org/W4287554008",
        "https://openalex.org/W4302339848"
    ],
    "abstract": "Saturn is an innovative platform that assists Foundation Model (FM) building and its integration with IT operations (Ops). It is custom-made to meet the requirements of data scientists, enabling them to effectively create and implement FMs while enhancing collaboration within their technical domain. By offering a wide range of tools and features, Saturn streamlines and automates different stages of FM development, making it an invaluable asset for data science teams. In this white paper, we discuss the expected impacts of Saturn on the financial sector.",
    "full_text": "Saturn Platform: Foundation Model Operations and\nGenerative AI for Financial Services\nAntonio J. G. Busson\nantonio.busson@btgpactual.com\nBTG Pactual\nRennan Gaio\nrennan.gaio@btgpactual.com\nBTG Pactual\nRafael H. Rocha\nrafael-h.rocha@btgpactual.com\nBTG Pactual\nFrancisco Evangelista\nfrancisco.evangelista@btgpactual.com\nBTG Pactual\nBruno Rizzi\nbruno.rizzi@btgpactual.com\nBTG Pactual\nLuan Carvalho\nluan.carvalho@btgpactual.com\nBTG Pactual\nRafael Miceli\nrafael.miceli@btgpactual.com\nBTG Pactual\nMarcos Rabaioli\nmarcos.rabaioli@btgpactual.com\nBTG Pactual\nDavid Favaro\ndavid.favaro@btgpactual.com\nBTG Pactual\nAbstract\nSaturn is an innovative platform that assists Foundation\nModel (FM) building and its integration with IT operations\n(Ops). It is custom-made to meet the requirements of data\nscientists, enabling them to effectively create and implement\nFMs while enhancing collaboration within their technical do-\nmain. By offering a wide range of tools and features, Saturn\nstreamlines and automates different stages of FM develop-\nment, making it an invaluable asset for data science teams. In\nthis white paper, we discuss the expected impacts of Saturn\non the financial sector.\nKeywords: Foundation Model, Generative AI, FMOps, Sat-\nurn\n1 Introduction\nAn emerging Artificial Intelligence (AI) paradigm called the\nFoundation Model (FM) has shown great potential due to its\nability to learn universal representations that can be applied\nto diverse tasks [ 20]. From a technological point of view,\nfoundational models consist of deep learning models that\nare pre-trained in a self-supervised/semi-supervised manner\non a large scale and then adapted for various downstream\ntasks [2].\nThe development of FMs relies on several significant chal-\nlenges related to infrastructure, development kits, gover-\nnance, security, etc. To address these challenges, we propose\nSaturn, a platform to help the process of building, managing,\nand serving FMs. Saturn combines advanced technologies,\nintelligent automation and robust infrastructure to empower\nprofessionals to pursue accurate and reliable models. Sat-\nurn’s core was generically designed to facilitate its implan-\ntation to any application domain. However, in this work,\nIn: XXII Workshop de Ferramentas e Aplicações (WFA 2023), Ribeirão Preto,\nBrasil. Anais Estendidos do Simpósio Brasileiro de Sistemas Multimídia e\nWeb (WebMedia). Porto Alegre: Sociedade Brasileira de Computação, 2023.\n© 2023 SBC – Sociedade Brasileira de Computação.\nISSN 2596-1683\nwe will focus specifically on the impacts of its use in the\nfinancial sector.\nThe Saturn Platform is proprietary software. All rights are\nreserved and protected by the BTG Pactual.\n2 Foundation Models and Generative AI\nWe gathered requirements for the Saturno platform by ana-\nlyzing the state-of-the-art in the field of foundational models\nand generative AI.\n2.1 State-of-the-Art\nFoundational models are grounded by two techniques: (1)\ntransfer learning and (2) self-supervised learning. The idea of\ntransfer learning is to apply the knowledge that was learned\nin training from one task to another different task. On the\nother hand, in self-supervised learning, the pre-training task\nis automatically derived from unannotated data. For example,\nthe masked language modeling task used to train BERT [7]\nis to predict missing words in a sentence.\nSelf-supervised tasks are more scalable and potentially\nhelpful than models trained in a limited space by label annota-\ntion. There has been considerable progress in self-supervised\nlearning since word embedding [10], which associated each\nword with a context-independent vector, and provided the\nbasis for a wide range of NLP models.\nShortly after, self-supervised methods based on autore-\ngressive language modeling (predicting the next word given\nthe previous words) [3] became popular. This produced mod-\nels representing contextualized words such as GPT [ 15],\nELMo [14], and ULMFiT [4]. The second generation of mod-\nels based on self-supervised learning, BERT [6], GPT-2 [16],\nand RoBERTa [9] were based on the Transformer architec-\nture, incorporating deeper and more powerful sentence bidi-\nrectional encoders.\nInevitably, foundational models underwent a process of\nhomogenization of architectures since the last generation\nmodels are all Transformer derivatives [19]. While this ho-\nmogenization produces hugely high leverage (improvements\n85\nAnais Estendidos do WebMedia’2023, Ribeirão Preto, Brasil Busson et al.\nto the basic models can bring immediate benefits for most of\nthe other foundational models), all AI systems can inherit the\nsame problematic biases from some foundational models [1].\nIn addition to the NLP area, methods have been homog-\nenized among different research communities in recent years.\nFor example, similar Transformer-based modeling approaches\nhave been applied to images [12], speech [8], tabular data [5],\norganic molecules [17]. These examples point in a direction\nwhere we will have a unified set of tools to develop founda-\ntional models for a wide range of modalities [18].\nReinforcement Learning has been applied to enhance var-\nious FMs in NLP tasks. InstructGPT proposes a method\ncalled RLHF (Reinforcement Learning from Human Feed-\nback), which involves fine-tuning large models using PPO\n(Proximal Policy Optimization) based on a reward model\ntrained to align the models with human preferences [ 13].\nThis approach is also employed by ChatGPT1. The reward\nmodel is trained using comparison data generated by human\nlabelers who rank the model outputs manually. Based on\nthese rankings, the reward model or a machine labeler calcu-\nlates a reward that is then utilized to update the FM through\nPPO.\nA notable advancement in FM technology is GPT-4 [11].\nGPT-4 employs a pre-training phase where it predicts the\nnext token in a document, followed by RLHF fine-tuning.\nGPT-4 surpasses GPT-3.5 in terms of reliability, creativity,\nand ability to handle more detailed instructions as the com-\nplexity of the task increases.\n2.2 Requirements\nBased on the research in the previous subsection, the require-\nments gathered are as follows:\n1. Pre-defined self-supervised learning pipelines or frame-\nworks;\n2. Transfer learning support;\n3. Efficient data processing and training pipelines to han-\ndle large amounts of unannotated data;\n4. Framework that allows data scientists to build and\nimprove upon existing FMs easily;\n5. Mechanisms to detect and mitigate problematic biases\ninherited from FMs;\n6. Support a wide range of data modalities, including text,\nimages, speech, tabular data, etc.;\n7. Collaborative features, such as version control, model\nsharing, and experiment tracking, to facilitate collabo-\nration among data scientists;\n8. Performance optimization by utilizing parallel comput-\ning; distributed training, and hardware acceleration\n(e.g., GPUs or TPUs);\n9. Tools for monitoring model performance in real-world\nsettings and providing insights into potential drift or\ndegradation;\n1https://openai.com/blog/chatgpt\n10. Process for deploying trained FMs into production\nenvironments. This includes model-serving infrastruc-\nture, REST API support, and containerization for easy\nintegration with other applications;\n11. Support approaches like RLHF for fine-tuning FMs\nusing reward models aligned with human preferences.\n3 Saturn Platform\nSaturn is a cutting-edge platform designed to help in FM\nbuilding and seamlessly integrate it with IT operations (Ops).\nIt is specifically tailored to cater to the needs of data scien-\ntists, empowering them to efficiently develop and deploy\nfoundation models while optimizing the collaboration be-\ntween their technical expertise. The platform provides a\ncomprehensive suite of tools and features that simplify and\nautomate various aspects of FM development, making it an\nindispensable resource for data science teams.\nFigure 1 shows the architecture of the Saturn platform.\nThe platform is structured in three environments: (1) Saturn\nEnvironment; (2) Data Science (DS) Development And (3)\nAutomated FM Operations. Each environment is detailed in\nthe subsections that follows.\n3.1 Saturn Environment\nModel Zoooffers a centralized repository for FMs, allowing\ndata scientists to access and leverage pre-trained models,\narchitectures, and components. This facilitates knowledge\nsharing and reduces redundancy, enabling teams to acceler-\nate model development. Model Zooprioritizes data security\nand governance, ensuring that sensitive data and models\nare protected throughout the development and deployment\nlifecycle. It includes robust access controls, encryption mech-\nanisms, and compliance features, adhering to industry stan-\ndards and regulations.\nEmbedding Farmprovides an efficient storage solution for\nembeddings generated by FMs. It leverages advanced algo-\nrithms and optimized data structures to ensure high-speed\nretrieval (via vector database). It offers powerful manage-\nment capabilities, allowing users to organize, categorize, and\ntag embeddings. Additionally, Embedding Farmincorporates\naccess control mechanisms, ensuring that sensitive embed-\ndings are safeguarded from unauthorized access.\nSaturn can deploy FMs as API endpoints, enabling other\nsoftware to access and leverage these models’ power easily.\nWhether it is a console prompt or aLangChain System, Saturn\noffers a versatile solution that can be seamlessly integrated\ninto various financial service applications. In addition, the\nRLHF systemcollects and manages human feedback data to\ntrain reward models used to fine-tune FMs via reinforcement\nlearning.\n86\nSaturn Platform: Foundation Model Operations and Generative AI for Financial Services Anais Estendidos do WebMedia’2023, Ribeirão Preto, Brasil\nFigure 1.Architecture of the Saturn platform.\n3.2 DS Development\nThe platform provides a collaborative workspace equipped\nwith powerful development tools and libraries tailored for\nfoundation model building. Data scientists can leverage a\nrange of programming languages, frameworks, and visual-\nization tools, ensuring flexibility and compatibility with their\npreferred workflows.\nIn the Data Analysismodule, pre-trained FMs can assist in\nthe initial data exploration and preprocessing stages. They\ncan generate summaries of large datasets, identify patterns,\ngenerate data, and perform basic data-cleaning tasks. This\ncan significantly speed up the data preparation phase, allow-\ning data scientists to focus on higher-level analysis.\nOn the other hand, in the Model Analysis process, pre-\ntrained FMs allow data scientists to iterate quickly on their\nspecific task. Instead of training a model from scratch, they\ncan start with a pre-trained FM and fine-tune it, significantly\nreducing the training time and effort required. This acceler-\nated iteration process facilitates faster experimentation and\nhypothesis testing.\n3.3 Automated FM Operations (FMOps)\nThe FMOps environment allows FMs to be continuously\nupdated and deployed, ensuring they are always in sync\nwith the latest data and maintaining their performance over\ntime.\nOrchestrated processes are structured in the following\nsteps:\n• (1-3) The user sends the source code of the machine\nlearning model to a Git server, where the code will be\nstored and versioned. A continuous integration and\ncontinuous delivery (CI/CD pipeline) treadmill is set\nup to trigger a continuous training (CT) process when-\never there is a new source code.\n• (3-4) After training, the resulting model artifact is\nsaved and stored in a \"model zoo\" (model repository).\nThis centralized repository allows the storage and man-\nagement of trained models.\n• (5-6) The trained model is deployed to a production\nenvironment. This can be done through a cloud infras-\ntructure, containers, or model-specific deployment ser-\nvices. The model is configured to be accessed through\nan API endpoint, allowing external users to request\ninferences.\n• (7) A continuous monitoring (CM) system is set up to\nobserve the model’s performance in production. It can\ntrack relevant metrics. In addition, the system detects\nchanges in input data (data drift) and automatically\n87\nAnais Estendidos do WebMedia’2023, Ribeirão Preto, Brasil Busson et al.\nstarts continuous training to update the model with\nthe latest data.\n4 Application in Financial Services\nForecasting and Predictive Analytics. Financial institu-\ntions often rely on accurate forecasts and predictions for\nmaking informed decisions. FMs can be trained on histori-\ncal financial data to develop predictive models for various\nfinancial metrics, such as stock prices, market trends, and\neconomic indicators. These models can assist in generating\nforecasts, and scenario analysis, helping financial profession-\nals make more informed investment and strategic decisions.\nFinancial Report Generation. FMs can generate reports,\nsummaries, and insights based on financial data. They can\nautomatically extract relevant information from financial\nstatements, filings, or market reports and generate concise\nsummaries, reducing the time and effort required for man-\nual analysis. These generated reports can quickly overview\nkey financial metrics, trends, and investment opportunities,\nfacilitating decision-making processes.\nRisk Assessment. FMs can assist in risk assessment by\nanalyzing various data sources, including financial state-\nments, market data, credit ratings, and news articles. By\nprocessing this information, they can help identify potential\nrisks, such as credit defaults, market volatility, regulatory\nchanges, or company-specific risks. This information can\nsupport risk management and help financial professionals\nmake informed decisions regarding investment portfolios\nand risk mitigation strategies.\nFinancial Data Generation. FMs can assist in generat-\ning synthetic financial data that closely resembles real-world\ndata. This can be beneficial for various purposes, including\ntesting and validating financial models, conducting simula-\ntions, or training machine learning algorithms in a controlled\nenvironment.\n5 Final Remarks\nSaturn is a groundbreaking platform that combines founda-\ntion models and IT operations to empower data scientists in\nthe financial services industry. Saturn enables data scientists\nto accelerate model building, optimize predictive accuracy,\nand drive informed decision-making by seamlessly integrat-\ning pre-built models, intuitive development tools, and robust\ninfrastructure. With Saturn, financial institutions gain a com-\npetitive edge by leveraging cutting-edge technology while\nmaintaining the highest security and compliance standards.\nReferences\n[1] Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent\nanti-muslim bias in large language models. In Proceedings of the 2021\nAAAI/ACM Conference on AI, Ethics, and Society. 298–306.\n[2] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran\nArora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine\nBosselut, Emma Brunskill, et al. 2021. On the opportunities and risks\nof foundation models. arXiv preprint arXiv:2108.07258(2021).\n[3] Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learn-\ning. Advances in neural information processing systems28 (2015).\n[4] Jeremy Howard and Sebastian Ruder. 2018. Universal language model\nfine-tuning for text classification. arXiv preprint arXiv:1801.06146\n(2018).\n[5] Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. 2020.\nTabtransformer: Tabular data modeling using contextual embeddings.\narXiv preprint arXiv:2012.06678(2020).\n[6] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova.\n2019. BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of NAACL-HLT. 4171–4186.\n[7] JDMCK Lee and K Toutanova. 2018. Pre-training of deep bidi-\nrectional transformers for language understanding. arXiv preprint\narXiv:1810.04805 (2018).\n[8] Andy T Liu, Shu-wen Yang, Po-Han Chi, Po-chun Hsu, and Hung-yi\nLee. 2020. Mockingjay: Unsupervised speech representation learning\nwith deep bidirectional transformer encoders. In ICASSP 2020-2020\nIEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 6419–6423.\n[9] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi\nChen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n2019. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692(2019).\n[10] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Effi-\ncient estimation of word representations in vector space.arXiv preprint\narXiv:1301.3781 (2013).\n[11] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[12] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc\nSzafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco\nMassa, Alaaeldin El-Nouby, et al. 2023. Dinov2: Learning robust visual\nfeatures without supervision. arXiv preprint arXiv:2304.07193(2023).\n[13] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,\nPamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama,\nAlex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan\nLeike, and Ryan Lowe. 2022. Training language models to follow\ninstructions with human feedback. arXiv:2203.02155 [cs.CL]\n[14] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner,\nChristopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep\ncontextualized word representations. arXiv:1802.05365 [cs.CL]\n[15] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.\n2018. Improving language understanding by generative pre-training.\n(2018).\n[16] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,\nIlya Sutskever, et al. 2019. Language models are unsupervised multitask\nlearners. OpenAI blog1, 8 (2019), 9.\n[17] Daniel Rothchild, Alex Tamkin, Julie Yu, Ujval Misra, and Joseph\nGonzalez. 2021. C5t5: Controllable generation of organic molecules\nwith transformers. arXiv preprint arXiv:2108.10307(2021).\n[18] Alex Tamkin, Mike Wu, and Noah Goodman. 2020. Viewmaker net-\nworks: Learning views for unsupervised representation learning.arXiv\npreprint arXiv:2010.07432(2020).\n[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. Advances in neural information processing\nsystems 30 (2017).\n[20] Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai\nZhang, Cheng Ji, Qiben Yan, Lifang He, et al . 2023. A comprehen-\nsive survey on pretrained foundation models: A history from bert to\nchatgpt. arXiv preprint arXiv:2302.09419(2023).\n88"
}