{
    "title": "TR-MISR: Multiimage Super-Resolution Based on Feature Fusion With Transformers",
    "url": "https://openalex.org/W4206722835",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2104381469",
            "name": "Tai An",
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences",
                "Institute of Automation"
            ]
        },
        {
            "id": "https://openalex.org/A2002306827",
            "name": "Xin Zhang",
            "affiliations": [
                "Institute of Automation",
                "Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences",
                "Beijing Academy of Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2168747777",
            "name": "Chunlei Huo",
            "affiliations": [
                "Chinese Academy of Sciences",
                "Institute of Automation"
            ]
        },
        {
            "id": "https://openalex.org/A1994745107",
            "name": "Bin Xue",
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "University of Chinese Academy of Sciences",
                "Chinese Academy of Sciences",
                "Institute of Automation"
            ]
        },
        {
            "id": "https://openalex.org/A2158809908",
            "name": "Lingfeng Wang",
            "affiliations": [
                "Institute of Automation",
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2114868839",
            "name": "Chunhong Pan",
            "affiliations": [
                "Chinese Academy of Sciences",
                "Institute of Automation"
            ]
        },
        {
            "id": "https://openalex.org/A2104381469",
            "name": "Tai An",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2002306827",
            "name": "Xin Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2168747777",
            "name": "Chunlei Huo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1994745107",
            "name": "Bin Xue",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2158809908",
            "name": "Lingfeng Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2114868839",
            "name": "Chunhong Pan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1985806826",
        "https://openalex.org/W2509704168",
        "https://openalex.org/W2518224564",
        "https://openalex.org/W2320725294",
        "https://openalex.org/W2476548250",
        "https://openalex.org/W2082456873",
        "https://openalex.org/W2028790650",
        "https://openalex.org/W2810821452",
        "https://openalex.org/W3040323135",
        "https://openalex.org/W2773049485",
        "https://openalex.org/W2929208599",
        "https://openalex.org/W2927933146",
        "https://openalex.org/W2558484771",
        "https://openalex.org/W2560755969",
        "https://openalex.org/W2158253930",
        "https://openalex.org/W2122632184",
        "https://openalex.org/W1973794531",
        "https://openalex.org/W2971251319",
        "https://openalex.org/W2939570633",
        "https://openalex.org/W3003247314",
        "https://openalex.org/W2963470893",
        "https://openalex.org/W2964101377",
        "https://openalex.org/W6682137061",
        "https://openalex.org/W6771953071",
        "https://openalex.org/W3036433012",
        "https://openalex.org/W2957558419",
        "https://openalex.org/W3124133491",
        "https://openalex.org/W3088734486",
        "https://openalex.org/W3039108209",
        "https://openalex.org/W2866634454",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W2970710980",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4206706211",
        "https://openalex.org/W6788071488",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3171125843",
        "https://openalex.org/W6790749177",
        "https://openalex.org/W3214821343",
        "https://openalex.org/W6784094891",
        "https://openalex.org/W3200870516",
        "https://openalex.org/W3035022492",
        "https://openalex.org/W6796568838",
        "https://openalex.org/W2157494358",
        "https://openalex.org/W2146200771",
        "https://openalex.org/W2097074225",
        "https://openalex.org/W2121058967",
        "https://openalex.org/W1885185971",
        "https://openalex.org/W2747898905",
        "https://openalex.org/W2214802144",
        "https://openalex.org/W2414132572",
        "https://openalex.org/W2792955223",
        "https://openalex.org/W2794252992",
        "https://openalex.org/W2794276902",
        "https://openalex.org/W3046020299",
        "https://openalex.org/W3044723170",
        "https://openalex.org/W2548527721",
        "https://openalex.org/W2963210183",
        "https://openalex.org/W2604329646",
        "https://openalex.org/W2796901798",
        "https://openalex.org/W3022107227",
        "https://openalex.org/W3036200672",
        "https://openalex.org/W3154593456",
        "https://openalex.org/W3016410830",
        "https://openalex.org/W2100329651",
        "https://openalex.org/W2792111852",
        "https://openalex.org/W3129508949",
        "https://openalex.org/W2170573007",
        "https://openalex.org/W2943832862",
        "https://openalex.org/W6685670348",
        "https://openalex.org/W6757491820",
        "https://openalex.org/W3026840827",
        "https://openalex.org/W2535388113",
        "https://openalex.org/W2133665775",
        "https://openalex.org/W2081321639",
        "https://openalex.org/W2165939075",
        "https://openalex.org/W2087380704",
        "https://openalex.org/W2798664922",
        "https://openalex.org/W1485009520",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3104313739",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W3103699521",
        "https://openalex.org/W2996275425",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W2951527505",
        "https://openalex.org/W2906021645",
        "https://openalex.org/W4287330714",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3169612303",
        "https://openalex.org/W3042008420",
        "https://openalex.org/W3124718178",
        "https://openalex.org/W3117450517",
        "https://openalex.org/W2963149042"
    ],
    "abstract": "Multiimage super-resolution (MISR), as one of the most promising directions in remote sensing, has become a needy technique in the satellite market. A sequence of images collected by satellites often has plenty of views and a long time span, so integrating multiple low-resolution views into a high-resolution image with details emerges as a challenging problem. However, most MISR methods based on deep learning cannot make full use of multiple images. Their fusion modules are incapable of adapting to an image sequence with weak temporal correlations well. To cope with these problems, we propose a novel end-to-end framework called TR-MISR. It consists of three parts: An encoder based on residual blocks, a transformer-based fusion module, and a decoder based on subpixel convolution. Specifically, by rearranging multiple feature maps into vectors, the fusion module can assign dynamic attention to the same area of different satellite images simultaneously. In addition, TR-MISR adopts an additional learnable embedding vector that fuses these vectors to restore the details to the greatest extent. TR-MISR has successfully applied the transformer to MISR tasks for the first time, notably reducing the difficulty of training the transformer by ignoring the spatial relations of image patches. Extensive experiments performed on the PROBA-V Kelvin dataset demonstrate the superiority of the proposed model that provides an effective method for transformers in other low-level vision tasks.",
    "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022 1373\nTR-MISR: Multiimage Super-Resolution Based on\nFeature Fusion With Transformers\nTai An , Xin Zhang , Chunlei Huo , Member, IEEE, Bin Xue, Lingfeng Wang, Member, IEEE,\nand Chunhong Pan, Member, IEEE\nAbstract—Multiimage super-resolution (MISR), as one of the\nmost promising directions in remote sensing, has become a needy\ntechnique in the satellite market. A sequence of images collected by\nsatellites often has plenty of views and a long time span, so integrat-\ning multiple low-resolution views into a high-resolution image with\ndetails emerges as a challenging problem. However, most MISR\nmethods based on deep learning cannot make full use of multiple\nimages. Their fusion modules are incapable of adapting to an\nimage sequence with weak temporal correlations well. To cope with\nthese problems, we propose a novel end-to-end framework called\nTR-MISR. It consists of three parts: An encoder based on residual\nblocks, a transformer-based fusion module, and a decoder based on\nsubpixel convolution. Speciﬁcally, by rearranging multiple feature\nmaps into vectors, the fusion module can assign dynamic attention\nto the same area of different satellite images simultaneously. In\naddition, TR-MISR adopts an additional learnable embedding\nvector that fuses these vectors to restore the details to the greatest\nextent. TR-MISR has successfully applied the transformer to MISR\ntasks for the ﬁrst time, notably reducing the difﬁculty of training\nthe transformer by ignoring the spatial relations of image patches.\nExtensive experiments performed on the PROBA-V Kelvin dataset\ndemonstrate the superiority of the proposed model that provides\nan effective method for transformers in other low-level vision tasks.\nIndex Terms—Deep learning, end-to-end networks, feature\nextraction and fusion, multiimage super-resolution (MISR), remote\nsensing, transformers.\nI. INTRODUCTION\nI\nMAGE super-resolution, as one of the critical technologies\nin computer vision, aims to convert low-resolution images\ninto high-resolution images. High-resolution images bring rich\nhigh-frequency details and play an essential role in medical\nManuscript received October 30, 2021; revised December 2, 2021 and De-\ncember 27, 2021; accepted January 12, 2022. Date of publication January 18,\n2022; date of current version February 2, 2022. This work was supported in part\nby the National Key Research and Development Program of China under Grant\n2018AAA0100400, in part by the National Natural Science Foundation of China\nunder Grant 62071466 and Grant 61802407, and in part by the Guangxi Natu-\nral Science Foundation under Grant 2018GXNSFBA281086.(Corresponding\nauthor: Chunlei Huo.)\nTai An, Xin Zhang, and Bin Xue are with the National Laboratory of\nPattern Recognition, Institute of Automation, Chinese Academy of Sciences,\nBeijing 100190, China, and also with the School of Artiﬁcial Intelligence,\nUniversity of Chinese Academy of Sciences, Beijing 100049, China (e-mail:\nantai2018@ia.ac.cn; xin.zhang2018@nlpr.ia.ac.cn; xuebin2018@ia.ac.cn).\nChunlei Huo, Lingfeng Wang, and Chunhong Pan are with the Na-\ntional Laboratory of Pattern Recognition, Institute of Automation, Chinese\nAcademy of Sciences, Beijing 100190, China (e-mail: clhuo@nlpr.ia.ac.cn;\nlfwang@nlpr.ia.ac.cn; chpan@nlpr.ia.ac.cn).\nDigital Object Identiﬁer 10.1109/JSTARS.2022.3143532\nData is available at online at https://github.com/Suanmd/TR-MISR/\nimaging [1], face recognition [2], [3], video [4], [5], remote\nsensing [6]–[10], and other ﬁelds. Super-resolution technologies\nbased on the remote sensing data need to satisfy the actual needs,\nsuch as disaster monitoring, land cover mapping [11], [12], and\nvegetation growth [13]. Limited by the cost and transmission\nrate [14], satellites mostly cannot obtain a large number of\nhigh-resolution images. In that case, researchers have been\nactively looking for effective super-resolution methods, such\nas discrete wavelet transforms (DWTs) [15], [16], and sparse\ncoding [17]. With the rapid development of deep learning, the\nconvolutional neural network (CNN) also serves as the main tool\nin the super-resolution of satellite images [18]–[20].\nAs for image super-resolution, two subdirections are worth\nstudying: Single image super-resolution (SISR) and multiim-\nage super-resolution (MISR). MISR focuses on extracting the\ninformation of multiple low-resolution images collected from\nthe same scene and then merging them into a high-resolution\nimage. In recent years, the increasing practical needs have\nled to an expansion of the low-cost satellite market. However,\nlow-cost satellites always suffer from low-resolution images.\nIn this context, MISR has become a key technology enabling\nmore satellites to serve users better with its low cost and high\nquality. At present, deep learning methods have shown great\nsuccess on SISR [21], [22], but existing deep learning methods\nof MISR, especially the end-to-end networks, progress at a slow\npace.\nIn remote sensing, MISR tasks involve two issues that need\nconsideration. First, the time span of acquiring multiple images\nmay be so long that environmental factors become uncontrol-\nlable. Sometimes even the order of the images is unknown [23].\nTherefore, how to process multiple low-resolution images with\nweak temporal correlations becomes a signiﬁcant challenge.\nCurrently, most existing MISR methods [18], [24]–[30] ﬁx the\ninput sequence, which means they handle these multiple images\nas order frames by default, so their results are sensitive to the\nsequence order. Some methods [29], [30] randomly shufﬂe the\ninput sequence and obtain a robust result by computing the\nmean image during inference. The second issue is the insuf-\nﬁcient utilization of multiple images. Under the inﬂuence of the\nactual environment, different views of the same scene may have\ndifferent clearness (i.e., the number of clear pixels). Some ex-\nisting methods based on deep learning [26], [27], [29], [30] put\nforward a high clearness requirement for an image and discard\nunclear images, which results in a loss of useful information.\nAn ideal solution to reduce information loss is introducing\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n1374 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nthe attention mechanism to improve the utilization of multiple\nimages.\nHowever, there is no widely used attention module yet in\nMISR. As for low-level vision tasks, the attention mechanism\nhas two major categories: Channel attention [31] and spatial\nattention [32]. Because the input is a stack of multiple images,\nchannel attention, which assigns weights to different images, is\nequivalent to frame attention [33]. Spatial attention is equivalent\nto the weights given to different areas in the same image.\nMoreover, MISR requires focusing on speciﬁc areas of images\nregardless of their position, which means that the existing atten-\ntion modules [27], [30] need to be improved.\nFrom another perspective of attention mechanism, nonlocal\nattention focuses on the entire entry status space compared to\nlocal attention [34]. A typical framework is transformer [35],\nwhich bases on self-attention and can address the main problems\nfaced by MISR in remote sensing. On the one hand, compared\nwith the recurrent networks that require recursive processing of\nsequence elements, self-attention is insensitive to the sequence\norder. Transformer assigns dynamic attention to all elements\nsimultaneously, making itself more capable of capturing long-\nterm features and adapting to multiple satellite images with\nweak temporal correlations. On the other hand, unlike CNN\nthat calculates static weights, self-attention calculates weights\ndynamically and adapts to variable-length sequences, which\nimproves the utilization of multiple images.\nTwo points [36], [37] are limiting the application of transform-\ners in remote sensing. First, transformer only takes a sequence of\none-dimensional vectors as input. The image or feature should\nbe ﬂattened into a sequence of one-dimensional vectors before\nbeing fed into a transformer. In this way, transformer requires\na large number of data to learn the spatial relations between\nvectors [38]–[42]. Second, transformer is inseparable from the\npretraining of massive data compared to CNN with inductive\nbias. However, it is not easy to carry out large-scale pretraining\nfor remote sensing datasets with high speciﬁcity. Transformer\nwill not perform as well as CNN in some low-level vision\ntasks without pretraining [42], [43]. Thus, a transformer-based\nMISR framework needs to reduce the difﬁculty of training the\ntransformer in a proper way.\nTo address the problems of MISR, we propose a novel end-\nto-end network based on transformers, namely, TR-MISR. TR-\nMISR alleviates the two limitations above of the transformer. On\nthe one hand, TR-MISR does not destroy the two-dimensional\nstructure of images. On the other hand, it receives a sequence of\nfeature vectors that are fused without positional encoding [35],\n[38]. TR-MISR works as follows: First, the encoder encodes a\nset of image features. Then the transformer-based fusion module\npays attention to feature vectors of the same area from different\nimages and adopts an additional learnable embedding vector to\nfuse these features, i.e., no matter what area of a high-resolution\nimage is generated, the corresponding areas of all low-resolution\nimages can be observed simultaneously. Finally, the decoder\ngenerates the corresponding area of the high-resolution image\nby decoding the learnable embedding vector. Our method not\nonly accommodates an arbitrary number of image patches but\nalso adapts to images with a long time span. TR-MISR has\nreached the state of the art on the PROBA-V Kelvin dataset\nreleased by the European Space Agency [18] and provided an\neffective strategy for transformers that will be compatible with\nother speciﬁc low-level vision tasks in the future.\nIn short, TR-MISR has the following highlights.\n1) Our proposed method introduces transformer to MISR\ntasks in remote sensing for the ﬁrst time. TR-MISR does\nnot require pretraining, because the fusion module reduces\nthe training difﬁculty of the transformer by preventing it\nfrom additionally learning the spatial relations between\ndifferent image patches. This advantage alleviates the\nproblem of insufﬁcient MISR data in remote sensing.\n2) TR-MISR provides a new approach for addressing the\nissues faced by MISR. With our proposed feature rear-\nrangement module, TR-MISR can simultaneously focus\non all image patches and adapt to multiple images with\nweak temporal correlations. In this way, TR-MISR can\naccommodate image sequences of any length, which no-\ntably improves the utilization of multiple images.\n3) The transformer-based fusion module signiﬁcantly im-\nproves the robustness of the model to noise. Compared\nwith the existing deep learning-based MISR methods,\nTR-MISR can receive more unclear input images without\nperformance degradation. Experiments show that our pro-\nposed fusion module performs better while maintaining\nlower #FLOPs and #Params.\nThe organization of this article is as follows: Section II reviews\nthe methods of previous work. Section III presents our method\nand introduces the framework of TR-MISR. Section IV gives\nthe results of the experiment under different hyperparameters\nand makes comparisons with other methods. Finally, Section V\nconcludes the article.\nII. RELATED WORK\nSuper-resolution has two primary directions according to\nthe input data: Image super-resolution (ISR) and video super-\nresolution (VSR). The main difference between VSR and ISR\nis that the former needs to process consecutive frames and use\ninterframe information. MISR, which faces many challenging\nproblems, needs to process multiple images rather than consec-\nutive frames.\nIn this section, we describe the transformer as well as three\ndirections of super-resolution.\nA. Transformer\nWith the interdisciplinary development of deep learning, a\nnovel framework named transformer [35] stands out in nat-\nural language processing (NLP) and computer vision (CV).\nTransformer is a sequence-to-sequence framework based on\nself-attention. Essentially, self-attention is one type of nonlocal\nattention that can compute mutual attention on all elements of a\nsequence at one time. Transformer has been successfully applied\nto image classiﬁcation [38], [39], [44], object detection [40],\n[45], semantic segmentation [41], [46], and other high-level\nvision tasks, but it rarely has been applied to low-level vision\ntasks such as reference-based super-resolution [47], multitask\nAN et al.: TR-MISR: MULTIIMAGE SUPER-RESOLUTION BASED ON FEATURE FUSION WITH TRANSFORMERS 1375\nimage processing [42], image colorization [43], and video super-\nresolution [48]. Section III-A describes the structure of the\nTransformer in detail.\nB. Single Image Super-Resolution\nSISR is the technology that utilizes a single low-resolution\nimage to reconstruct a single high-resolution image. In most\ncases, the low-resolution image comes out of a degraded high-\nresolution image. The mainstream algorithms of SISR involve\nthree types. The interpolation-based methods [49] are fast and\nstraightforward but generate images with insufﬁcient details.\nThe reconstruction-based methods [50], which require complex\nprior knowledge to limit the solution space and generate a high\ncomputational cost. The learning-based methods [51], [52] focus\non learning the relationship between low-resolution images and\nhigh-resolution images, which can reduce the computational\noverhead and generate detailed results. With the development\nof deep learning, SISR has also adopted some basic frameworks\nsuch as CNN, recurrent neural network (RNN), and generative\nadversarial network [5], [21], [53]–[55] to further improve the\nperformance and generate better details.\nIn remote sensing, the frameworks based on deep learning\nstill play an essential role, such as the super-resolution of mul-\ntispectral satellite images [56], [57] and the Sentinel-2 satellite\nimagery [58]–[60].\nC. Video Super-Resolution\nVSR uses a sequence of low-resolution frames to generate a\nsequence of high-resolution frames. The pipeline of most VSR\nmethods includes a registration module, a feature extraction\nmodule, a fusion module, and a reconstruction module [61].\nThere are two types of VSR methods: Methods based on mo-\ntion estimation and motion compensation (MEMC) as well as\nlearning-based methods. Both of them emphasize the utilization\nof interframe information. Compared with traditional methods,\ndeep learning is more capable of information extraction and\nfeature fusion. It has played an essential role in estimations such\nas optical ﬂow estimation [62], [63], and kernel estimation [64].\nIn remote sensing, the super-resolution of video imagery relies\non video satellites, such as the Jilin-1 satellite [65], [66] that can\ndirectly collect videos instead of static images. At present, the\nsuper-resolution research of remote sensing video imagery is\nstill subject to public datasets.\nD. Multiimage Super-Resolution\nMISR reconstructs a high-resolution image from multiple\nlow-resolution images. Beyond single image and video pro-\ncessing, effectively combining more data means more reliable\nresults [67]–[69]. For instance, a general way to multispectral\nimage super-resolution is pan-sharpening [70]–[72], which com-\nbines prior information and fuses multiple images. When the\ncamera takes multiple images of the same scene from different\nshooting angles, views, and times, more details will be obtained,\nwhich is of great help to reconstruct high-resolution images.\nUsually, multiple images are captured within a few seconds by\nburst shooting devices, such as mobile phones and cameras with\na burst mode, or captured by satellites for several days or months.\nImages captured by burst shooting devices have a strong inter-\nframe correlation, but images captured by satellites have a weak\ncorrelation due to environmental factors, landscape changes, or\nmissing annotations [23]. MISR tasks with a strong interframe\ncorrelation are also called multiframe super-resolution (MFSR),\nbut most studies regard MISR as MFSR without considering\nthe inter-frame correlations. In this section, this article does\nnot distinguish these two concepts. In raw image scenes, the\ncamera obtains multiple images through burst shooting, such\nas the jitter camera prototype [73] applied on cameras and\nthe handheld image super-resolution [74] applied on mobile\nphones.\nAt present, most deep learning-based MISR methods employ\nthe encoder–decoder network, which includes a registration\nmodule, an encoder, a fusion module, and a decoder, to ﬁnish\nspeciﬁc tasks. The process has the following main steps: First,\nthe network ﬁxes the sequence of multiple low-resolution images\nthrough sampling and padding. After the encoder extracts image\nfeatures from multiple low-resolution images, the fusion module\nfuses these features into a fused feature. Finally, the decoder\ndecodes the fused feature and gets a high-resolution image.\nFig. 1 demonstrates this process. In remote sensing, the stack-\ning CNN layers are responsible for implementing the encoder\nand decoder, and some of the encoder designs include speciﬁc\nattention mechanism [27], [30]. Besides, the fusion module has\nmultiple designs such as ﬁxed network design [18], rule-based\ndesign [24], RNN-based design [25], and 3-D convolution-based\ndesign [26]–[30].\nFor convenience, here we denote the super-resolution scal-\ning factor by r, the low-resolution images of one scene by\n{LRi}k\ni=1 ∈ RC×W×H, and the high-resolution image (i.e., the\nground truth) byHR ∈ RC×rW ×rH , where k, C, H, and W\nrepresents the number, the channel, the height, and the width\nof images, respectively. HighRes-Net [24] is an end-to-end\nframework. It encodes{LRi}k\ni=1 and gets features{Si}k\ni=1.T h e\nfeatures are later merged in pairs by recursive design through\nt = ⌈log2 k⌉steps. Then the network decodes the fused feature\nSt to obtain the high-resolution image. MISR-GRU [25] uses\nconvGRU [75] to extract the hidden states {hi}k\ni=1 at each\ntime step and obtains the fused statehu through global average\npooling (GAP). DeepSUM [26] widely uses 3-D convolutions\nto integrate different channels, realizing the feature-wise regis-\ntration and fusion. The fusion module of DeepSUM, which is\nalso a recursive design, requiresk/2 3-D convolutional layers to\nfuse features ofkregistered images. DeepSUM++ [27] adds the\ngraph convolution in the encoder and improves the performance\nwith the help of introducing nonlocal attention.\nAnother approach learns from the mature structure of VSR.\nInspired by 3DSRNet [76] applied to VSR, Francisco Dorr\net al. [29] propose 3DWDSRNet, the core of which is to use\nthe WDSR blocks to acquire the temporal correlations be-\ntween frames. WDSR-MFSR [28] uses multiple WDSR residual\nblocks to strengthen the feature extraction further.\nFrancesco Salvettiet al.[30] realize the difference between\nMISR and MFSR and propose RAMS. RAMS tries to avoid the\n1376 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nFig. 1. Pipeline of MISR methods. The pipeline includes a pre-processing module for processing views, an encoder for extracting features, a fusion module\nfor getting a fused feature, a decoder for reconstructing a high-resolution image, and a registration module for aligning images/features. Low-resolution images\nLR1,... ,L Rk obtained by preprocessing are encoded separately to get featuresS1,... ,Sk. The fused featureSu is used to get the high-resolution imageSR.\nThe registration module can be inserted in any process of MISR, so we omit it in the ﬁgure.\neffect of the sequence order on the results in various aspects.\nSpeciﬁcally, it randomly shufﬂes the image sequence before\nimages are fed into the network. In addition, it retains features\nto a great extent in the temporal and spatial dimensions by using\nthe residual temporal attention blocks (RTABs) and the residual\nfeature attention blocks (RFABs). In testing/veriﬁcation, RAMS\nrandomly shufﬂes the image sequence several times and aver-\nages these results to obtain a reliable high-resolution image, e.g.,\nRAMS+20.\nIn summary, most existing MISR methods mainly focus on\nimage coding and use 3-D convolutions to fuse features gradu-\nally. Although the 3-D convolution can merge information across\nchannels, it is sensitive to the sequence order and noise. Also, it\nstill requires a ﬁxed number of input images, which makes most\nmethods less practical in MISR.\nIII. METHODOLOGY\nIn this section, we describe the structure of TR-MISR. First,\nwe brieﬂy introduce the transformer, especially the self-attention\nmechanism. Then we introduce three modules of TR-MISR in\ndetail and ﬁnally present the loss function.\nA. Structure of Transformer\nTransformer [35] is a sequence-to-sequence framework based\non self-attention. It consists of a set of encoders and decoders.\nThe encoder of a transformer includes a self-attention layer and\na feed-forward network. We assume that the input sequence\nof a self-attention layer isX =[ x1,...,x n] ∈ Rd×n and the\noutput sequence isY =[ y1,...,y n] ∈ Rd×n. For eachxi ∈ X,\nthe self-attention layer linearly maps xi to the query vector\nqi = Wqxi ∈ Rd′×1, the key vector ki = Wkxi ∈ Rd′×1, and\nthe value vectorvi = Wvxi ∈ Rd×1.H e r ed may not be equal\nto d′. For eachqi, the output vectoryi can be expressed by\nyi =\nn∑\nj=1\n[\nsoftmax\n( qT\ni K√\nd\n)]\nj\nvj (1)\nwhere K =[ k1,...,k n] ∈ Rd′×n represents the key matrix,d\nrepresents the length ofxi and yi. Eachyi is equivalent to the\nweighted sum of{vj}n\nj=1. The weights of{vj}n\nj=1 depend on\nthe correlations between {qi}n\ni=1 and {ki}n\ni=1. Note that, the\nattention weights are dynamically generated and not affected by\nthe position ofxi.\nThe decoder of a transformer includes three parts. The self-\nattention layer is responsible for establishing the relationship\nbetween the current decoded value and the decoded part. The\nencoder–decoder attention layer establishes the relationship be-\ntween the current decoded value and the encoded feature vectors.\nThe feed-forward network is the same as that in the transformer\nencoder.\nMoreover, by using multiheaded self-attention, transformer\nmaps the query vector qi and the key vector ki to different\nsubspaces {μ1,...,μ q} of the high-dimensional spaceμ and\ncalculate their similarity. The formation of multiple subspaces\naims to pay attention to different aspects of features [35].\nThe transformer concatenates the results {Yi}q\ni=1 produced\nby {headi}\nq\ni=1 and multiplies it by the weightWo to get the\nmultiheaded outputZ:\nZ =C o n c a t\n(\nY1,...,Y q)\n×Wo (2)\nwhere q denotes the number of multiheads.Z =[ z1,...,z n] ∈\nRd×n has the same size as the inputX.W eu s eMSA() to\nrepresent multiheaded self-attention so that the self-attention\nof the transformer can be brieﬂy represented by\n[z1,...,z n]=M S A( [x1,...,x n]) . (3)\nThe feed-forward network includes two fully connected lay-\ners that generally serve as the information integration layers.\nThese layers mainly provide nonlinear transformations for the\ntransformer and enhance its expression capability.\nB. TR-MISR Framework\nTR-MISR takes any number of low-resolution images as\ninput and outputs a high-resolution image. It consists of an\nencoder, a fusion module, and a decoder. In the encoder, the\nnetwork extracts the features through the residual blocks. Then,\nthese features are assigned dynamic attention and fused by an\nadditional learnable embedding vector in the fusion module.\nAN et al.: TR-MISR: MULTIIMAGE SUPER-RESOLUTION BASED ON FEATURE FUSION WITH TRANSFORMERS 1377\nFig. 2. Overview of TR-MISR. The encoder based on residual blocks encodes the same area of different low-resolution images. The fusion module based on\ntransformer is responsible for a featurewise fusion by an additional learnable embedding vector. The decoder based on subpixel convolution decodesall fused\nfeatures into a high-resolution image. The parameters of the encoder are shared.\nFinally, the decoder relies on sub-pixel convolution to obtain a\nhigh-resolution image. Fig. 2 shows the overview of TR-MISR.\nReferring to the feature vectors extracted from the same area of\nlow-resolution images, TR-MISR generates the corresponding\narea of the high-resolution image. In this way, nonlocal attention\nis introduced to different image patches, signiﬁcantly improving\nthe applicability of our model.\nNext, we will introduce these modules in detail.\n1) The Encoder: We represent the low-resolution images\n{LRi}k\ni=1 by a tensor of sizeB ×H ×W ×K ×Cin, whereB\nis the batch size.H, W, K, andCin represent the image height,\nthe image width, the number of images, and the channels per\nimage, respectively. The encoder refers to HighRes-Net [24]\nbased on residual blocks. MISR assumes that the information\nof multiple images is greater than that of any single image.\nHowever, the redundant information of multiple low-resolution\nimages will cause trouble for models to extract different features.\nTo mitigate this impact, we calculate a reference imageLRref\nof an image sequence by theMedian() function that highlights\ndifferences across multiple images [77]. The reference image\nLRref is used both as a shared representation and a condition\nfor implicit registration between images{LRi}k\ni=1. Each low-\nresolution imageLRi is concatenated withLRref and then fed\ninto the encoder to get the feature mapfi. The formulas are as\nfollows:\nLRref =M e d i a n(LR1,...,LR k) (4)\nGi =[ LRi,LRref ] ,i =1 ,...,k (5)\nfi =E n c o d e r\n(\nGi)\n,i =1 ,...,k (6)\nwhere k represents the number of input images. TheMedian()\nfunction aims to calculate the median image LRref from\n{LRi}k\ni=1. For convenience, we omit the batch size dimen-\nsion. The feature mapfi ∈ RH×W×Ch obtained from LRi ∈\nRH×W×Cin has expanded the number of channels (i.e.,Ch >\nCin). Herekcan be set manually. For scenes where the number of\noriginal viewsnis less thank, (k −n) padded images need to be\ngenerated; otherwise, images need sampling. We use a Boolean\nvariable αi to mark whetherLRi is a padded image or not.\n2) Fusion Module: Our transformer-based fusion module\nkeeps the spatial resolution of the input and output unchanged,\nwhile effectively capturing global context information. In self-\nattention, the query vector is a sequence that needs expressing\nby the following steps: First, the module calculates the simi-\nlarity between the query vectors and key vectors in the same\nhigh-dimensional space. Then it computes a weighted sum of\nvalue vectors to express the sequence with attention.\nTransformer can extract a set of query vectors{qi}k\ni=1,k e y\nvectors {ki}k\ni=1, and value vectors{vi}k\ni=1 from each set of\nfeature vectors {xi}k\ni=1. These vectors{qi}k\ni=1, {ki}k\ni=1, and\n{vi}k\ni=1 are calculated by (1) to obtain the output{yi}k\ni=1 with\nmutual attention. Thus, the outputy0 of a manually added vector\nx0 is the fused feature of {xi}k\ni=1. In this context, we add\na learnable embedding vector to each set of features that is\nrandomly initialized. Through multiple transformer layers, the\nlearnable embedding vector can pay attention to the same area\nof different low-resolution images.\nSpeciﬁcally, the fusion module dividesfi into H ×W one-\ndimensional vectorsxi\n(h,w) ∈ Rd0×1 along the channel dimen-\nsion. Each vector xi\n(h,w) has d0 = Ch features encoded by\nthe receptive ﬁeld that takes the coordinate(h,w) of LRi as\ncenter. As shown in Fig. 2, the module transforms{fi}k\ni=1 ∈\nRH×W×K×Ch into H ×W groups {xi\n(h,w)}k\ni=1 ∈ RK×Ch and\nadds a learnable embedding vectorx0\n(h,w) ∈ Rd0×1 at the be-\nginning of each sequence{xi\n(h,w)}k\ni=1 to obtain z0\n(h,w). Then\nz0\n(h,w) =[ x0\n(h,w),x1\n(h,w),...,x k\n(h,w)] is fed into the transformer.\nFinally, the outputzu\n(h,w) that x0\n(h,w) generates through the trans-\nformer layers fusesk vectors {xi\n(h,w)}k\ni=1. This kind of feature\n1378 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nfusion is a parallel computing process with the permutation\ninvariance property.\nFor each feature vectorxi\n(h,w) extracted from the feature map\nfi at each position(h,w), the module transforms them like above\nand uses the transformer to fuse features. We retain the entire\nstructure of the transformer encoder, including the multiheaded\nself-attention (MSA) layers, the feed-forward networks (FFN),\nlayer normalization (LN), and skip connections. For simplicity,\nwe omit all subscripts(h,w). The whole fusion processes are\nrepresented by\nz0 =\n[\nx0,x1,...,x k]\n(7)\nα =[ 1,1,..., 1] or [1,1,..., 0] (8)\nzl\na =M S A\n(\nLN(zl−1,α)\n)\n+ zl−1,l =1 ,...,M (9)\nzl =F F N\n(\nLN(zl\na)\n)\n+ zl\na,l =1 ,...,M (10)\nzu = zM |index=0 (11)\nwhere M represents the number of the transformer layers.\nzu ∈ Rd×1 is the ﬁrst element of zM .A f t e rH ×W groups\nof the fusion process, the module obtains a fused feature\nfu ∈ RH×W×Ch from {fi}k\ni=1 ∈ RH×W×K×Ch . The original\ntransformer requires a large amount of data to learn the spatial\nrelations of image patches [38]. Moreover, the fusion module\nwe proposed avoids the additional learning of spatial relations,\nsigniﬁcantly reducing the difﬁculty of training the transformer.\n3) Decoder: In super-resolution tasks, researchers mainly\nuse a decoder that increases the resolution of feature maps to\nobtain a high-resolution image. Suppose a deconvolution is used\nfor upscaling directly. In that case, the convolution will face\nzero-ﬁlled images, which introduces unnecessary calculations\nor even produces the checkerboard artifacts [78].\nSubpixel convolution [5], also known as pixel shufﬂe, is an ef-\nﬁcient and parameter-free pixel rearrangement method. We use a\nsubpixel convolution as the primary part of the decoder. It guides\nthe feature map to generate subpixels of the high-resolution\nimage. In the fusion module, the feature map is composed of\nH ×W numbers of zu\n(h,w). Speciﬁcally, zu\n(h,w) serves as the\ncommon representation of low-resolution images at the same\nposition centered on (h,w). Later it is decoded to ther×r\npatch in the high-resolution image, as illustrated in Fig. 2. The\npatch takes ((r −1)h+1 ,(r −1)h+1 )as the coordinate of\nthe upper-left corner and takes(rh,rw ) as the coordinate of the\nlower-right corner, whereris the super-resolution scaling factor.\nThe work of the decoder has two steps, involving a general\nconvolution operation and a subpixel convolution. First, the\nconvolution is to reduce the channelsCh of fu ∈ RH×W×Ch\nto Ct = r2Cin and get a response mapTR ∈ RH×W×Ct . Then,\nthe subpixel convolution is for decodingTR to a high-resolution\nimage SR ∈ RrH×rW ×Cin . In other words, starting from one\npixel at the same position in feature maps {fi}k\ni=1, the de-\ncoder generates r2 pixels in the corresponding position of the\nhigh-resolution imageSR. We choose PReLU as the activation\nfunction. The speciﬁc process is\nTR =P R e L U( C o n v(fu)) (12)\nSR(h,w,c) = TR(⌊h\nr ⌋,⌊w\nr ⌋,c·r·mod(w,r)+c·mod(h,r)) (13)\nwhere Conv() denotes a 2-D convolution,mod(·,·) returns the\nremainder of two numbers after division. Equation (13) is to\nmap the response values ofTR to the pixel values of the high-\nresolution imageSR, and the operation is parameter-free.\nThe TR-MISR framework is shown in Fig. 3.\nC. Loss Function\nWe use supervised learning to train TR-MISR in an end-to-\nend manner. TR-MISR takes multiple images{LR}k\ni=1 as input\nand outputs a single imageSR. Because low-resolution images\n{LR}k\ni=1 and the ground truthHR are in different collection\nenvironments, SRgenerates some different problems fromHR,\nsuch as horizontal/vertical offset, overall brightness deviation,\nand pixel occlusion or smear.\nAssuming that the image offset is consistent across the entire\nimage, TR-MISR requires croppingHR and SR to compensate\nfor the image offset and return an appropriate loss. The max-\nimum value of the offset required for registration isc.O nt h e\none hand, the model crops the four edges ofSR by cpixels and\nobtains SR(c,c). On the other hand, it cropsHR that takes(u,v)\nas the starting coordinate to the same size asSR(c,c) and obtains\nHR(u,v), where (u,v) ∈{ 0,..., 2c}. Then the model returns\nthe smallest loss ofSR(c,c) and HR(u,v) for eachSR–HR pair.\nConsidering the overall brightness deviation, we ﬁrst correct\nthe overall brightness ofSR(c,c) and HR(u,v) by calculating\nthe average deviation and then calculate the loss function. The\nmean absolute errorl1 and the mean square errorl2 are the most\ncommon pixel-level loss functions. In general,l1 aims to get a\nmore balanced error distribution, whilel2 focuses on penalizing\nerror pixels, aiming to reduce mispredictions.\nIn summary, we can calculate the average brightness deviation\nb(u,v) between SR(c,c) and HR(u,v) by clear pixels, then choose\neither l1 or l2 loss as the loss function\nb(u,v) = SM(u,v)∑ SM(u,v)\n(\nHR(u,v) −SR(c,c)\n)\n(14)\nl1 =m i n\nu,v\n⏐⏐(\nHR(u,v) −SR(c,c) −b(u,v)\n)\n·SM(u,v)\n⏐⏐\n∑ SM(u,v)\n(15)\nl2 =m i n\nu,v\n[(\nHR(u,v) −SR(c,c) −b(u,v)\n)\n·SM(u,v)\n]2\n∑ SM(u,v)\n(16)\nwhere the binary image SM(u,v) denotes the pixel quality\nindicator ofHR(u,v).\nIV . EXPERIMENTS\nIn this section, we conduct experiments in detail on the\nexisting MISR dataset. First, the dataset and evaluation metrics\nare described. Second, comparisons between our method and\nexisting methods are introduced. Then the effect of the hyperpa-\nrameters and the ablation study of fusion modules are presented.\nFinally, the attention from the output vector to the input sequence\nis visualized.\nAN et al.: TR-MISR: MULTIIMAGE SUPER-RESOLUTION BASED ON FEATURE FUSION WITH TRANSFORMERS 1379\nFig. 3. TR-MISR framework. k is the number of images fed into the shared encoder.N is the number of residual blocks, which determines the(2N +2 )×\n(2N +2 )size of the receptive ﬁeld.p represents the number of attention heads in the transformer, which is mainly used to pay attention to different features.\nM is the number of the Transformer layers, which can be ﬂexibly adjusted based on the size of datasets. In the feature rearrangement, multiple feature maps are\nrearranged into pairs of feature vectors along the channel dimension. It ensures that each sequence fed into the transformer belongs to the same area of different\nimages.\nA. PROBA-V Kelvin Dataset\nPROBA-V is an earth observation satellite designed to map\nland cover and vegetation growth. This dataset has been re-\nleased by the Advanced Concept Team of the European Space\nAgency (ESA) [18]. Unlike most open-source datasets for\nsuper-resolution, all the views provided by PROBA-V are real\nrecords instead of algorithm processing. For the same scene,\nthe PROBA-V satellite provides views of two resolutions that\nhave different revisit frequencies. The data collection spans 30\ndays in total, which means that some scenes may have changed.\nMore importantly, the order of the views is unknowable, so\nresearchers need to regard the views as a sequence with weak\ntime correlations.\nThe PROBA-V Kelvin dataset involves 74 manually selected\nlocations, a total of 1450 scenes that cover the red spectrum\nband RED and the near-infrared spectrum band NIR, of which\n290 scenes are for testing. Each scene includes multiple low-\nresolution images with 300-m resolution (128 ×128 grayscale\nimages, called {LRi}n\ni=1) and a high-resolution image with\n100 m resolution (384 ×384 grayscale image, calledHR). The\nsuper-resolution scaling factorris 3. These images saved in 16-\nbit png format have a bit-depth of 14. In addition, most images\nhave missing or noisy values due to the actual environment, so\neach image (LRi or HR) owns a manually labeled quality map\n(QMi or SM) indicating the concealed pixels and clear pixels.\nEvery scene has 19 images on average, to a minimum of 9 and\na maximum of 35. Speciﬁcally, all images in each scene are not\nregistered, and their brightness is often inconsistent.\nB. Evaluation Metric\nThe realism of pixel content is an essential indicator in the\nremote sensing environment. To judge the usability of the high-\nresolution imageSR, we need to consider two points in terms\nof the core evaluation indicators: 1)SR should not reconstruct\nocclusions; 2) the actual pixel value ofSR should be as close as\npossible to HR. Given these two requirements, the Advanced\nConcept Team of ESA puts forward the cPSNR metric [18] based\non the square error between pixels.\nPSNR is a critical metric for satellite image quality assess-\nment. As a modiﬁed version of PSNR, cPSNR takes the differ-\nences betweenSR and HR into account. Both sizes ofSR and\nHR are 384 ×384 with the maximum registration offsetc =3 ,\nso the starting coordinate is denoted by (u,v) ∈{ 0,..., 6}.\nFirst, we can getSRb\n(3,3) according to\nSRb\n(3,3) = SR(3,3) + b(u,v) (17)\nwhere b(u,v) has been deﬁned in (14).\nThen, the cPNSR metric is\ncPSNR(SR,HR )\n=m a x\n(u,v)∈{0,...,6}\n10 lg\n(\n2D −1\n)2\n·∑ SM(u,v)\n[(\nHR(u,v) −SRb\n(3,3)\n)\n·SM(u,v)\n]2\n(18)\nwhere Ddenotes the bit-depth ofSRand HR. SM(u,v) denotes\nthe pixel quality indicator ofHR(u,v).\nSimilarly, the cSSIM metric based on SSIM [79] can be\nrepresented by\ncSSIM(SR,HR )\n=m a x\n(u,v)∈{0,...,6}\nSSIM\n(\nHR(u,v) ·SM(u,v),SRb\n(3,3) ·SM(u,v)\n)\n(19)\nSSIM generally uses the Gaussian function to calculate the\nmean and the covariance of an image, and it is not the pri-\nmary evaluation metric of the PROBA-V Kelvin dataset. In\nthe PROBA-V challenge, the evaluation metric of MISR is the\naverage score based on cPSNR [see (18)]. To be speciﬁc, each\nscene has a baseline method, which obtains thebaseSR through\n1380 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\na merger of some bicubic upscalings of low-resolution images.\nThe scoreR of a single scene is\nR(SR,HR )= cPSNR (baseSR,HR )\ncPSNR(SR,HR ) . (20)\nGiven the average score of all test scenes, the ﬁnal score¯Ris\nrepresented by\n¯R =M e a n (R). (21)\nScore ¯Ris used as a ranking index. If the proposed method is\nbetter than the baseline overall,¯Rshould be smaller than 1. The\nsmaller the ¯R, the better the results we get.\nC. Experimental Settings\n1) Data Preparation:Before training TR-MISR, we perform\ndata augmentation by cropping and sampling. The steps are as\nfollows.\na) We crop low-resolution views to the64 ×64 LRi and\ncrop the ground-truth view to192 ×192 HR.W es e tt h e\ncropping stride to 64, i.e., crop a scene into 9 subscenes.\nb) Without rotation or ﬂipping for subscenes, we only discard\nthe unclear subscene where the average ratio of clear pixels\nis less than 85% and preserve all views in each scene.\nAt the front of the encoder,{LRi}n\ni=1 need to be processed\nin the following steps.\na) For n ≥ k, we selectk images by sampling; forn<k ,\nblank images are added by padding.\nb) We select the image with the largest number of clear pixels\nin the scene as the referenceLRc. In addition, we register\n{LRi}n\ni=1 with LRc based on subpixel image translation\nregistration [80].\nc) According to (4), we calculateLRref as the implicit coreg-\nistration information. Then each imageLRi needs to be\nencoded together withLRref .\nIndeed, the implicit coregistration informationLRref is cru-\ncial to TR-MISR. We will discuss it later in Section IV-E.\n2) Parameter Settings:The experiments are conducted by a\nserver cluster with a 64-bit Linux operating system. The hard-\nware includes Tesla V100 GPU (32 GB memory) and Intel(R)\nXeon(R) Gold 6230 CPU @ 2.10 GHz. As is shown in Fig. 3,\nwe set the number of input imagesk =2 4. In the encoder, we\nset N =2 and expand the number of channels fromCin =2 to\nCh =6 4. The kernel size of the 2-D convolution is set to3 ×3.\nThe stride and the padding are set to 1. We choose PReLU\nas the activation function. In the fusion module, we set the\nnumber of transformer multiheadsp =8 and layersM =6 .I n\nthe transformer feed-forward network, the mapping dimension\nof fully connected layers is set to 128. In the decoder, we use\na 2-D convolution to reduce the channels from Ch =6 4 to\nCr =9 . The kernel size of the 2-D convolution is set to1 ×1\nand the stride is set to 1. We choose PReLU as the activation\nfunction, and a subpixel convolutional layer with upscale factor\nr =3 is connected after the PReLU activation function.\nIn terms of network training, we set the initial learning rate of\nthe encoder and decoder to 1e-3, the learning rate of the fusion\nmodule to 5e-4. Experiments show thatl2 converges much faster\nthan l1, so we choosel2 as the loss function. We choose Adam to\ntrain our network and design a learning rate decay based on the\nevaluation metric ¯R of the validation set. If¯R is not improved\nthree times in a row, the learning rate of all modules will drop\nby 5%. We set the batch size to 4 and the training epoch to 400.\nIt takes about 60 h to train one band on a Tesla V100 GPU. After\nthis, we ﬁne-tune the model to obtain a slight improvement, i.e.,\nset the learning rate of the encoder and decoder to 5e-5 and set\nthe learning rate of the fusion module to 2.5e-5. Then it takes\nabout 20 epochs of training to obtain the best model.\nThere are three main reasons why the training time is longer\nthan that of most methods: First, transformer has a global\nattention with fully connected layers. Fully connected layers\ndo not have an advantage over convolutional layers in terms\nof time and memory. Second, the model hyperparameters are\nlarge by default. The model will achieve a faster speed if we\nslightly lower the hyperparameters, e.g., the model size and the\nnumber of images. Third, because TR-MISR is an end-to-end\nframework, some built-in operations like rearranging vectors\nalso decrease the computational speed. TR-MISR does not re-\nquire pretraining, and all weights in the network are initialized\nrandomly. In addition, all initialized seeds keep ﬁxed to ensure\nthat the experimental results are reproducible.\nD. Comparing Methods\nBecause there is no ground truth on the PROBA-V Kelvin\ntesting set, we split the training set at a ratio of 7:3. Given\nthe same training/validation set, we choose the representative\nmethods of SISR, VSR, and MISR, and compare them in terms\nof the single-band data NIR/RED and the full-band data ALL.\nThe comparison metrics include cPSNR and cSSIM. Table I and\nFigs. 4– 6 . present the results.\nHere the methods and their experimental settings to be com-\npared with TR-MISR are brieﬂy introduced.\n1) SISR Methods:\na) Bicubic: It is the baseline method. The clearest image\nLRc in each scene is selected and performed the bicubic\ninterpolation.\nb) RCAN: [31] It proposes channel attention (CA) to process\ndifferent channels. The experiment sets the residual groups\nand the residual channels to 5.\n2) VSR Method:\na) VSR-DUF: [83] It uses dynamic upsampling ﬁlters to\ngenerate corresponding ﬁlters for different inputs. The\nexperiment sets the number of input video frames to 9\nand selects a 16-layer framework.\n3) MISR Methods:\na) IBP: [82] It is one of the most classic algorithms for\nimage super-resolution that improves the resolution of\nimages through iterations. The experiment uses bicubic\ninterpolation to obtain the initial solution and uses the\nphase correlation algorithm for registration.\nb) BTV: [81] It is an image enhancement method, which\nfocuses on restoring image edges and denoising. It mini-\nmizes anl1 norm plus the bilateral regularization term in\neach iteration.\nAN et al.: TR-MISR: MULTIIMAGE SUPER-RESOLUTION BASED ON FEATURE FUSION WITH TRANSFORMERS 1381\nTABLE I\nPERFORMANCE OF DIFFERENT METHODS ON THEVALIDATION SET\nThe bold entities represent the best results for different evaluation metrics.\nFig. 4. cPSNR comparison between TR-MISR and bicubic interpolation on the validation set. Each data point represents a scene. A certain data point above the\nstraight liney = x indicates that TR-MISR can construct the corresponding scene better than the baseline.\nFig. 5. Comparison between different MISR methods on the imgset1125 scene of the NIR band.\nc) HighRes-net: [24] As the runner-up of the PROBA-V\nchallenge, it is an end-to-end framework for joint train-\ning of the encoder–decoder network and the registration\nnetwork. The experiment adopts the default framework\nand sets the number of input images to 16.\nd) MISR-GRU: [25] It uses convGRU [75] to fuse different\nfeatures and obtains the fused features by processing the\nhidden states. The experiment sets the number of input\nimages to 24.\ne) DeepSUM: [26] As the winner of the PROBA-V chal-\nlenge, it is a deep framework focusing on exploring\nthe spatio-temporal correlations between images. The\nexperiment adopts 9 images. The authors have also\nreleased DeepSUM++ [27], which brings a slight\n1382 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nFig. 6. Comparison between different MISR methods on the imgset0500 scene of the RED band.\nTABLE II\nCPSNR PERFORMANCE OF DIFFERENT ENCODER HYPERPARAMETERS ON THEVALIDATION SET\nN represents the number of residual blocks in the encoder.‘-’ denotes without the calculation of implicit co-registration. #Params(M) indicates themodel parameters.\n#FLOPs(G) indicates the computation amount of one scene during inference.\nThe bold entities represent the highest cPSNR results for different network hyperparameters.\nFig. 7. Effect of different numbers of input images k on reconstruction\noutcomes. The vertical axis represents the cPSNR performance on the validation\nset. The horizontal axis represents the training epoch.\nimprovement by introducing the graph convolution into\nthe encoder.\nf) 3DWDSRNet: [29]: It emphasizes the changing rela-\ntions of the temporal dimension between acquired frames\nTABLE III\nCPSNR PERFORMANCE OF DIFFERENT TRANSFORMER\nHYPERPARAMETERS ON THEVALIDATION SET\np denotes the humber of transformer attention heads.M denotes the depth of the\ntransformer. The deﬁnition of #FLOPs(G) is the same as that in Table II.\n(images). The experiment sets the number of input images\nto 7.\ng) RAMS: [30] Based on a large number of hand-designed\nattention blocks, it is the current state-of-the-art method\nof MISR on the PROBA-V Kelvin dataset. The experiment\nset the number of input images to 9. RAMS+20 adopts the\ntemporal self-ensemble, i.e., the input image sequence is\nshufﬂed 20 times randomly, and the mean image is taken\nunder the premise of reducing the inference speed. We\nrecord the highest results of RAMS and RAMS+20 in\nTable I.\nTable I shows that TR-MISR has achieved the highest cPSNR\nand cSSIM scores on the NIR, RED, and ALL bands, which\nAN et al.: TR-MISR: MULTIIMAGE SUPER-RESOLUTION BASED ON FEATURE FUSION WITH TRANSFORMERS 1383\nFig. 8. Effect of different input imagesk on the imgset0604 scene of the NIR band. The reconstruction outcome achieves the best whenk =3 2.\nFig. 9. Effect of different input imagesk on the imgset0503 scene of the RED band. The reconstruction outcome achieves the best whenk =3 2.\nTABLE IV\nCPSNR PERFORMANCE OF DIFFERENT NUMBERS OF INPUT\nIMAGES k ON THE VALIDATION SET\nThe deﬁnition of #FLOPs(G) is the same as that in Table II.\ndemonstrates the superiority of our method. Fig. 4 reveals that\nTR-MISR has an advantage over the baseline in 98.3% RED\nband and 97.1% NIR band.\nE. Analysis of Network Settings\nAs shown in Fig. 3, TR-MISR supports predeﬁning the model\nsize to make a tradeoff between speed and accuracy. We ﬁx the\ntransformer settings and observe the performance of the encoder\nunder different hyperparameters. There are two options for the\nencoder setting: The ﬁrst option is the number of residual blocks\nN, which determines the size of the receptive ﬁeld. The second\noption is whether to use the reference imageLRref as the implicit\ncoregistration information of{LRi}k\ni=1 during encoding.\nWe specify two fusion modules of different sizes: Transformer\nand small-transformer. The hyperparameter(p,M ) of the trans-\nformer is(8,6) and the small-transformer is(4,3).W es e tt h e\nsame random initialization and adopt 24 images as input. After\nusing the same learning strategy mentioned in Section IV-C2,\nwe get Table II. As reported in Table II, we also count ﬂoating-\npoint operations #FLOPs (unit: Giga) and parameters #Params\n(unit: Million), representing the computation amount and the\nparameter amount of the framework, respectively.\nIn summary, we come to three points: First, the transformer\nwith a large model capacity brings better results than small-\ntransformer. Second, implicit image coregistration is necessary.\nThird, a larger receptive ﬁeld is not necessarily better, i.e., the\nsemantic information of high-level layers is not always effective\nin low-level tasks such as image super-resolution.\nNext, under the condition ofLRref and N =2 , we discuss the\neffect caused by different transformer hyperparameters(p,M ).\nTable III presents the results. The Transformer with more heads\nand deeper layers results in a better feature fusion capability but\ngreater computational complexity.\n1384 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nTABLE V\nCPSNR PERFORMANCE OF THEFUSION MODULES ON DIFFERENT VALIDATION SETS\n“std” represents the standard validation set. “w/or e g” represents without image registration. “+n” indicates the number of noise images mixed into each scene.\nThe bold entities represent the highest cPSNR results for different validation sets.\nFig. 10. Comparsion between different fusion modules on the imgset0870 scene of the NIR band in the standard validation dataset.\nTABLE VI\nCOMPARISON OF#FLOPS(G) AND #PARAMS(M) FOR THE FRAMEWORKS\nBASED ON DIFFERENT FUSION MODULES\n#FLOPs(G) denotes the average computation of a single image in one\nbatch.\nF . Analysis of Numbers of Input Images\nThe core assumption of MISR is that multiple images contain\nrich details. However, whether more images have a better recon-\nstruction outcome has caused some discussion and controversy.\nIn practice, researchers often select the clearestkimages in each\nscene and feed them into the network, ignoring the remaining\nunclear images. On the one hand, more images include more\ndetails while introducing more noise. Unclear images contain\nmore noise and less information, which may have a bad effect on\nthe generated images [26]. On the other hand, more input images\nwill increase the pressure on the fusion module, because it is hard\nto extract long-distance relationships effectively. Inﬂuenced by\nthe structures such as 3-D convolution, most MISR frameworks\nbased on deep learning limit the number of input images to\nobtain a robust result. Speciﬁcally, the reconstruction outcome\nof HighRes-Net [24] achieves the best when the number of input\nimages k =1 6. The reconstruction outcome of DeepSUM [26]\nmay worsen when the number of imagesk> 9, so DeepSUM\nsets k to 9. RAMS [30] also follows this rule. However, our\nTR-MISR allows any number of images to be input at one\ntime, and more images do not cause performance degradation.\nAs illustrated in Fig. 7, we discuss the cPSNR performance of\nTR-MISR under different numbers of input imagesk.\nThe results obtained by TR-MISR under the condition of\nk =2 4are second only tok =3 2; see Figs. 7–9 and Table IV .\nIn general, the more input images, the better the reconstruction\noutcome of TR-MISR. We hold that only by using more effective\nAN et al.: TR-MISR: MULTIIMAGE SUPER-RESOLUTION BASED ON FEATURE FUSION WITH TRANSFORMERS 1385\nFig. 11. Comparsion between different fusion modules on the imgset0870 scene of the NIR band in the standard validation dataset.\nFig. 12. Attention visualization of two areas on the imgset0252 scene. The histogram indicates the intensity of attention to different image patchesduring the\nfusion process.\nimage information can the upper limit of MISR be increased.\nAs k becomes large, more unclear images are mixed into each\nscene, and the reconstruction outcome of TR-MISR tends to\nbe better. There are two main reasons for this result: 1) The\nTransformer-based fusion module performs attention to multiple\nimages dynamically, which makes itself more robust than 3-D\nconvolution with ﬁxed weights. 2) The high adaptation between\nthe decoder and the fusion module guarantees the extracted\ninformation to be directly used to restore the details of high-\nresolution images, which minimizes the reconstruction deviation\nand avoids the overlap issue [78].\nIt is worth mentioning that when we set (k,N,p,M ) to\n(32,2,8,6) and use the entire training set to train for 400 epochs\non each band, TR-MISR reaches an¯R score of 0.93001 on the\ntesting set of the PROBA-V challenge and places at the top of\nthe leaderboard.\nG. Analysis of Fusion Modules\nThe superior performance and high adaptability to image\nsequences of TR-MISR are well illustrated in Sections IV-E and\nIV-F. Since image fusion is a crucial step in MISR, it is essential\nto compare the effect of different fusion modules on generated\n1Online. [Available]: https://kelvins.esa.int/proba-v-super-resolution/leader\nboard/post-mortem-leaderboard/\nimages. In this section, we ﬁx the encoder and decoder and com-\npare the performance of the existing deep learning-based fusion\nmodules on different validation sets. The results demonstrate\nthe superiority of the transformer-based fusion module in MISR\ntasks. The fusion modules we implemented are as follows.\n1) GAP: Global average pooling (GAP) is used to average\nthe channels of feature maps.\n2) convGRU: The convGRU [75] is employed as the fusion\nmodule, as in MISR-GRU [25].\n3) convLSTM: The conﬁguration of the convLSTM [84] is\nsimilar to the convGRU mentioned above.\n4) recursion: The handcrafted iterative algorithm of\nHighRes-Net [24] is directly used.\n5) 3-D conv: A 3-layer 3-D convolution is adopted as the\nfusion module.\n6) 3-D conv + attention: Feature attention is introduced\nto the 3-D convolution by adding a FAB in front\nof each convolutional layer, which is inspired by\nRAMS [30].\n7) self-attention: A fusion module based on multiheaded self-\nattention layers of the transformer is adopted.\n8) Transformer: A transformer-based fusion module with\nmultiheaded self-attention layers and feed-forward net-\nworks is used.\nAs for the evaluation, ﬁve different validation sets are used to\nverify the robustness of different fusion modules. “std” denotes\n1386 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nthe standard validation set. “w/o reg” represents the validation\nset without registration [80]. “+1,” “+2,” and “+3” indicate that\n1, 2, or 3 noisy images are mixed into each scene, respectively.\nThese noisy images are just Gaussian noise that does not contain\nany information. The effect of noise should be considered,\nalthough images with a lot of noise have been removed in the\nPROBA-V dataset. Table V records the cPSNR results of the\neight fusion modules on the ﬁve validation sets. Examples with\ndetails are given in Figs. 10 and 11 for the NIR and RED bands,\nrespectively.\nFrom Table V, we can summarize some general conclusions:\n1) Image registration [80] improves the quality of image fu-\nsion; 2) the average pooling of feature channels can lead to\na positive result. 3) The antinoise ability of convGRU [75],\nconvLSTM [84], recursion, and 3-D convolution is low due to the\nlack of attention mechanism; 4) the transformer-based module\nhas a higher model capacity and can achieve better results than\nthe self-attention-based module. Figs. 10 and 11 reveal that\nthe high-resolution images generated by the transfomer-based\nfusion module are less noisy.\nIn addition, we show the complexity of the frameworks based\non different fusion modules in Table VI. The statistic #FLOPs(G)\ncounts the amount of calculation required for a single input\nimage. #Params(M) represents the parameter amount of the\nframework. Note that, the #Params(M) of a GAP-based frame-\nwork is 0.186 M, while the GAP operation is parameter-free.\nAs reported in Table VI, compared with other fusion modules,\nboth #FLOPs(G) and #Params(M) of the transformer are in the\nlower range. The complexity of the transformer is not high,\nbecause we only use the encoder part of the transformer to design\nthe fusion module.\nH. Attention Visualization\nIn this subsection, we show the effect of self-attention in\nthe fusion process. Because the encoder involves a total of 6\nconvolutional layers, the size of the receptive ﬁeld is13 ×13.\nAs is shown in Fig. 12, we take two sets of patches on the\nimgset0252 scene of the RED band as an example to visualize\nthe attention. First, we extract the feature vectors of selected\npatches. Then the attention of the learnable embedding vector\nto the other encoded vectors is output based on attention maps.\nThe attention here is the average value obtained under different\ntransformer heads.\nFig. 12 shows that TR-MISR assigns different attention to\ndifferent image patches and the unclear patches are assigned\nless attention. In remote sensing, pixel ﬁdelity is more mean-\ningful than visual perception. Some patches may have a good\nvisual perception, but the attention to them is at a low level\ndue to the inﬂuence of the noncontent pixels or few details.\nThe transformer-based fusion module can effectively refer to\nthe features of different patches, which is more conducive to the\nrestoration of high-resolution images.\nV. CONCLUSION\nThis article proposes TR-MISR, a novel end-to-end frame-\nwork that addresses the problems of poor adaptability and\nlow data utilization in MISR tasks. The self-attention mecha-\nnism of transformers gives us much inspiration. Our proposed\nframework mainly includes three parts: An encoder, a fusion\nmodule, and a decoder. TR-MISR places great emphasis on\nthe feature fusion capability of the fusion module. Speciﬁcally,\nthe transformer-based fusion module assigns dynamic attention\nto different image patches. It has the permutation invariance\nproperty, which is especially suitable for the MISR datasets in\nremote sensing. Our experiments discuss the inﬂuence of the\nmodel size and the number of input images on the results and\ncompare TR-MISR with other existing MISR methods. Then we\nanalyze the superiority of the transformer-based fusion module\nand show the importance of self-attention.\nTR-MISR has achieved the state of the art on the PROBA-V\nKelvin dataset. More importantly, it has alleviated the limita-\ntions of transformers in low-level computer vision tasks. The\noriginal transformer destroys the two-dimensional structure of\nimages or features and relies on massive data to surpass CNN\nthat has inherent structural advantages. Our transformer avoids\nlearning the spatial relations additionally, notably reducing the\nreliance on data. It starts with the same patches of different\nimages, giving dynamic attention and integrating them through\nan additional learnable embedding vector. After decoding, our\nTransformer obtains the image patch that is corresponding to the\nhigh-resolution image. The ablation experiment of fusion mod-\nules shows that the Transformer, especially the self-attention\nmechanism, is of great help to the improvement of image fusion\nin MISR.\nWe intend to improve this work from the following aspects: 1)\nResearch on a more effective encoder for registered/unregistered\nimages; 2) removal of redundant attention and reduction of\ntransformer complexity; and 3) further improvement of the\nresults on more public datasets in the future.\nREFERENCES\n[1] H. Greenspan, “Super-resolution in medical imaging,”Comput. J., vol. 52,\nno. 1, pp. 43–63, 2009.\n[2] J. Jiang, J. Ma, C. Chen, X. Jiang, and Z. Wang, “Noise robust face im-\nage super-resolution through smooth sparse representation,”IEEE Trans.\nCybern., vol. 47, no. 11, pp. 3991–4002, Nov. 2017.\n[3] J. Jiang, C. Chen, J. Ma, Z. Wang, Z. Wang, and R. Hu, “SRLSP:\nA face image super-resolution algorithm using smooth regression with\nlocal structure prior,”IEEE Trans. Multimedia, vol. 19, no. 1, pp. 27–40,\nJan. 2017.\n[4] A. Kappeler, S. Yoo, Q. Dai, and A. K. Katsaggelos, “Video\nsuper-resolution with convolutional neural networks,”IEEE Trans. Com-\nput. Imag., vol. 2, no. 2, pp. 109–122, Jun. 2016.\n[5] W. Shi et al., “Real-time single image and video super-resolution using\nan efﬁcient sub-pixel convolutional neural network,” inProc. IEEE Conf.\nComput. Vis. Pattern Recognit., 2016, pp. 1874–1883.\n[6] X. X. Zhu and R. Bamler, “Super-resolution power and robustness of\ncompressive sensing for spectral estimation with application to spaceborne\ntomographic SAR,” IEEE Trans. Geosci. Remote Sens., vol. 50, no. 1,\npp. 247–258, Jan. 2011.\n[7] Z. Pan et al., “Super-resolution based on compressive sensing and struc-\ntural self-similarity for remote sensing images,” IEEE Trans. Geosci.\nRemote Sens., vol. 51, no. 9, pp. 4864–4876, Sep. 2013.\n[8] J. M. Haut, R. Fernandez-Beltran, M. E. Paoletti, J. Plaza, A. Plaza, and\nF. Pla, “A new deep generative network for unsupervised remote sensing\nsingle-image super-resolution,”IEEE Trans. Geosci. Remote Sens., vol. 56,\nno. 11, pp. 6792–6810, Nov. 2018.\nAN et al.: TR-MISR: MULTIIMAGE SUPER-RESOLUTION BASED ON FEATURE FUSION WITH TRANSFORMERS 1387\n[9] P. Wang, L. Wang, H. Leung, and G. Zhang, “Super-resolution mapping\nbased on spatial-spectral correlation for spectral imagery,”IEEE Trans.\nGeosci. Remote Sens., vol. 59, no. 3, pp. 2556–2568, Mar. 2021.\n[10] F. Li, L. Xin, Y . Guo, D. Gao, X. Kong, and X. Jia, “Super-resolution\nfor GaoFen-4 remote sensing images,”IEEE Geosci. Remote Sens. Lett.,\nvol. 15, no. 1, pp. 28–32, Jan. 2017.\n[11] X. Li et al., “Spatial-temporal super-resolution land cover mapping with\na local spatial-temporal dependence model,”IEEE Trans. Geosci. Remote\nSens., vol. 57, no. 7, pp. 4951–4966, Jul. 2019.\n[12] K. Jiang, Z. Wang, P. Yi, G. Wang, T. Lu, and J. Jiang, “Edge-enhanced\nGAN for remote sensing image superresolution,”IEEE Trans. Geosci.\nRemote Sens., vol. 57, no. 8, pp. 5799–5812, Aug. 2019.\n[13] H.-j. Xu, X.-p. Wang, and T.-b. Yang, “Trend shifts in satellite-derived\nvegetation growth in Central Eurasia, 1982–2013,”Sci. Total Environ.,\nvol. 579, pp. 1658–1674, 2017.\n[14] R. Fernandez-Beltran, P. Latorre-Carmona, and F. Pla, “Single-frame\nsuper-resolution in remote sensing: A practical overview,”Int. J. Remote\nSens., vol. 38, no. 1, pp. 314–354, Jan. 2017.\n[15] H. Demirel and G. Anbarjafari, “Satellite image resolution enhancement\nusing complex wavelet transform,”IEEE Geosci. Remote Sens. Lett.,v o l .7 ,\nno. 1, pp. 123–126, Jan. 2010.\n[16] H. Demirel and G. Anbarjafari, “Discrete wavelet transform-based satel-\nlite image resolution enhancement,”IEEE Trans. Geosci. Remote Sens.,\nvol. 49, no. 6, pp. 1997–2004, Jun. 2011.\n[17] S. Yang, M. Wang, Y . Chen, and Y . Sun, “Single-image super-resolution\nreconstruction via learned geometric dictionaries and clustered sparse\ncoding,” IEEE Trans. Image Process., vol. 21, no. 9, pp. 4016–4028,\nSep. 2012.\n[18] M. Märtens, D. Izzo, A. Krzic, and D. Cox, “Super-resolution of PROBA-V\nimages using convolutional neural networks,”Astrodynamics, vol. 3, no. 4,\npp. 387–402, 2019.\n[19] X. Liu, C. Deng, J. Chanussot, D. Hong, and B. Zhao, “StfNet: A\ntwo-stream convolutional neural network for spatiotemporal image fu-\nsion,” IEEE Trans. Geosci. Remote Sens., vol. 57, no. 9, pp. 6552–6564,\nSep. 2019.\n[20] S. Mei, R. Jiang, X. Li, and Q. Du, “Spatial and spectral joint super-\nresolution using convolutional neural network,” IEEE Trans. Geosci.\nRemote Sens., vol. 58, no. 7, pp. 4590–4603, Jul. 2020.\n[21] C. Ledig et al., “Photo-realistic single image super-resolution using a\ngenerative adversarial network,” inProc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2017, pp. 4681–4690.\n[22] Y . Zhang, Y . Tian, Y . Kong, B. Zhong, and Y . Fu, “Residual dense network\nfor image super-resolution,” inProc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2018, pp. 2472–2481.\n[23] V . Mnih, N. Heess, A. Graves, and K. Kavukcuoglu, “Recurrent models\nof visual attention,” 2014,arXiv:1406.6247.\n[24] M. Deudonet al., “Highres-Net: Multi-frame super-resolution by recursive\nfusion,” 2019,arXiv:2002.06460.\n[25] M. R. Areﬁnet al., “Multi-image super-resolution for remote sensing using\ndeep recurrent networks,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. Workshops, 2020, pp. 206–207.\n[26] A. B. Molini, D. Valsesia, G. Fracastoro, and E. Magli, “DeepSum:\nDeep neural network for super-resolution of unregistered multitemporal\nimages,”IEEE Trans. Geosci. Remote Sens., vol. 58, no. 5, pp. 3644–3656,\nDec. 2019.\n[27] A. B. Molini, D. Valsesia, G. Fracastoro, and E. Magli, “DeepSum:\nNon-local deep neural network for super-resolution of unregistered mul-\ntitemporal images,” inProc. IEEE Int. Geosci. Remote Sens. Symp., 2020,\npp. 609–612.\n[28] M. Bajo, “Multi-frame super resolution of unregistered tem-\nporal images using WDSR nets,” 2020. [Online]. Available:\nhttps://doi.org/10.5281/zenodo.3733116\n[29] F. Dorr, “Satellite image multi-frame super resolution using 3D wide-\nactivation neural networks,” Remote Sens., vol. 12, no. 22, 2020,\nArt. no. 3812.\n[30] F. Salvetti, V . Mazzia, A. Khaliq, and M. Chiaberge, “Multi-image super\nresolution of remotely sensed images using residual attention deep neural\nnetworks,” Remote Sens., vol. 12, no. 14, 2020, Art. no. 2207.\n[31] Y . Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y . Fu, “Image super-\nresolution using very deep residual channel attention networks,” inProc.\nEur. Conf. Comput. Vis., 2018, pp. 286–301.\n[32] S. Woo, J. Park, J.-Y . Lee, and I. S. Kweon, “CBAM: Convolutional block\nattention module,” inProc. Eur. Conf. Comput. Vis., 2018, pp. 3–19.\n[33] D. Meng, X. Peng, K. Wang, and Y . Qiao, “Frame attention networks for\nfacial expression recognition in videos,” inProc. IEEE Int. Conf. Image\nProcess., 2019, pp. 3866–3870.\n[34] L. Weng, “Attention? attention!” lilianweng.github.io/lil-log, 2018. [On-\nline]. Available: http://lilianweng.github.io/lil-log/2018/06/24/attention-\nattention.html\n[35] A. Vaswaniet al., “Attention is all you need,” 2017,arXiv:1706.03762.\n[36] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,\n“Transformers in vision: A survey,” 2021,arXiv:2101.01169.\n[37] K. Han et al., “A survey on visual transformer,” 2020,arXiv:2012.12556.\n[38] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” 2020,arXiv:2010.11929.\n[39] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H.\nJégou, “Training data-efﬁcient image transformers & distillation through\nattention,” 2020,arXiv:2012.12877.\n[40] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S.\nZagoruyko, “End-to-end object detection with transformers,” inProc. Eur.\nConf. Comput. Vis., 2020, pp. 213–229.\n[41] S. Zheng et al., “Rethinking semantic segmentation from a sequence-to-\nsequence perspective with transformers,” 2020,arXiv:2012.15840.\n[42] H. Chen et al., “Pre-trained image processing transformer,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 12299–12310.\n[43] M. Kumar, D. Weissenborn, and N. Kalchbrenner, “Colorization trans-\nformer,” 2021,arXiv:2102.04432.\n[44] D. Hong et al., “SpectralFormer: Rethinking hyperspectral image classi-\nﬁcation with transformers,”IEEE Trans. Geosci. Remote Sens., 2021.\n[45] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable\nDETR: Deformable transformers for end-to-end object detection,” 2020,\narXiv:2010.04159.\n[46] Z. Xu, W. Zhang, T. Zhang, Z. Yang, and J. Li, “Efﬁcient transformer for\nremote sensing image segmentation,”Remote Sens., vol. 13, no. 18, 2021,\np. 3585.\n[47] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo, “Learning texture transformer\nnetwork for image super-resolution,” inProc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit., 2020, pp. 5791–5800.\n[48] J. Cao, Y . Li, K. Zhang, and L. Van Gool, “Video super-resolution trans-\nformer,” 2021,arXiv:2106.06847.\n[49] R. Keys, “Cubic convolution interpolation for digital image processing,”\nIEEE Trans. Acoust. Speech Signal Process., vol. 29, no. 6, pp. 1153–1160,\nDec. 1981.\n[50] S. Dai, M. Han, W. Xu, Y . Wu, Y . Gong, and A. K. Katsaggelos, “SoftCuts:\nA soft edge smoothness prior for color image super-resolution,”IEEE\nTrans. Image Process., vol. 18, no. 5, pp. 969–981, May 2009.\n[51] W. T. Freeman, T. R. Jones, and E. C. Pasztor, “Example-based super-\nresolution,” IEEE Comput. Graph. Appl., vol. 22, no. 2, pp. 56–65,\nMar./Apr. 2002.\n[52] J. Yang, J. Wright, T. S. Huang, and Y . Ma, “Image super-resolution\nvia sparse representation,”IEEE Trans. Image Process., vol. 19, no. 11,\npp. 2861–2873, Nov. 2010.\n[53] C. Dong, C. C. Loy, K. He, and X. Tang, “Image Super-resolution using\ndeep convolutional networks,”IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 38, no. 2, pp. 295–307, Feb. 2015.\n[54] Y . Tai, J. Yang, and X. Liu, “Image super-resolution via deep recursive\nresidual network,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit.,\n2017, pp. 3147–3155.\n[55] J. Kim, J. K. Lee, and K. M. Lee, “Deeply-recursive convolutional network\nfor image super-resolution,” inProc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2016, pp. 1637–1645.\n[56] L. Liebel and M. Körner, “Single-image super resolution for multispec-\ntral remote sensing data using convolutional neural networks,”ISPRS-\nInt. Arch. Photogrammetry, Remote Sens. Spatial Inf. Sci. , vol. 41,\npp. 883–890, 2016.\n[57] C. Tuna, G. Unal, and E. Sertel, “Single-frame super resolution of remote-\nsensing images by convolutional neural networks,”Int. J. Remote Sens.,\nvol. 39, no. 8, pp. 2463–2479, 2018.\n[58] C. Lanaras, J. Bioucas-Dias, S. Galliani, E. Baltsavias, and K. Schindler,\n“Super-resolution of sentinel-2 images: Learning a globally applicable\ndeep neural network,”Int. J. Photogrammetry Remote Sens., vol. 146,\npp. 305–319, 2018.\n[59] D. Pouliot, R. Latifovic, J. Pasher, and J. Duffe, “Landsat super-resolution\nenhancement using convolution neural networks and sentinel-2 for train-\ning,” Remote Sens., vol. 10, no. 3, 2018, p. 394.\n[60] L. S. Romero, J. Marcello, and V . Vilaplana, “Super-resolution of sentinel-\n2 imagery using generative adversarial networks,”Remote Sens., vol. 12,\nno. 15, 2020, p. 2424.\n[61] H. Liu, Z. Ruan, P. Zhao, F. Shang, L. Yang, and Y . Liu, “Video su-\nper resolution based on deep learning: A comprehensive survey,” 2020,\narXiv:2007.12928.\n1388 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\n[62] A. Ranjan and M. J. Black, “Optical ﬂow estimation using a spatial pyramid\nnetwork,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017,\npp. 4161–4170.\n[63] J. Xu, R. Ranftl, and V . Koltun, “Accurate optical ﬂow via direct cost\nvolume processing,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit.,\n2017, pp. 1289–1297.\n[64] S. Niklaus, L. Mai, and F. Liu, “Video frame interpolation via adaptive\nconvolution,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017,\npp. 670–679.\n[65] A. Xiao, Z. Wang, L. Wang, and Y . Ren, “Super-resolution for ‘Jilin-1’\nsatellite video imagery via a convolutional network,”Sensors, vol. 18,\nno. 4, 2018, p. 1194.\n[66] H. Liu, Y . Gu, T. Wang, and S. Li, “Satellite video super-resolution based\non adaptively spatiotemporal neighbors and nonlocal similarity regulariza-\ntion,” IEEE Trans. Geosci. Remote Sens., vol. 58, no. 12, pp. 8372–8383,\nDec. 2020.\n[67] L. Gao, D. Hong, J. Yao, B. Zhang, P. Gamba, and J. Chanussot,\n“Spectral superresolution of multispectral imagery with joint sparse and\nlow-rank learning,” IEEE Trans. Geosci. Remote Sens., vol. 59, no. 3,\npp. 2269–2280, Mar. 2020.\n[68] X. Cao, X. Fu, C. Xu, and D. Meng, “Deep spatial-spectral global reasoning\nnetwork for hyperspectral image denoising,”IEEE Trans. Geosci. Remote\nSens., vol. 60, 2021, Art. no. 5504714.\n[69] R. Dian, S. Li, and X. Kang, “Regularizing hyperspectral and multispectral\nimage fusion by CNN denoiser,”IEEE Trans. Neural Netw. Learn. Syst.,\nvol. 32, no. 3, pp. 1124–1135, Mar. 2020.\n[70] L. Alparone, L. Wald, J. Chanussot, C. Thomas, P. Gamba,\nand L. M. Bruce, “Comparison of pansharpening algorithms:\nOutcome of the 2006 GRS-S data-fusion contest,” IEEE Trans.\nGeosci. Remote Sens. , vol. 45, no. 10, pp. 3012–3021, Oct.\n2007.\n[71] R. Dian, S. Li, A. Guo, and L. Fang, “Deep hyperspectral image\nsharpening,” IEEE Trans. Neural Netw. Learn. Syst., vol. 29, no. 11,\npp. 5345–5355, Nov. 2018.\n[72] A. Guo, R. Dian, and S. Li, “Unsupervised blur kernel learning for\npansharpening,” in Proc. IEEE Int. Geosci. Remote Sens. Symp., 2020,\npp. 633–636.\n[73] M. Ben-Ezra, A. Zomet, and S. K. Nayar, “Video super-\nresolution using controlled subpixel detector shifts,” IEEE Trans.\nPattern Anal. Mach. Intell. , vol. 27, no. 6, pp. 977–987, Jun.\n2005.\n[74] B. Wronski et al., “Handheld multi-frame super-resolution,”ACM Trans.\nGraph., vol. 38, no. 4, pp. 1–18, 2019.\n[75] N. Ballas, L. Yao, C. Pal, and A. Courville, “Delving deeper into\nconvolutional networks for learning video representations,” 2015,\narXiv:1511.06432.\n[76] S. Y . Kim, J. Lim, T. Na, and M. Kim, “3DSRnet: Video super-resolution\nusing 3-D convolutional neural networks,” 2018,arXiv:1812.09079.\n[77] E. H. Sanchez, M. Serrurier, and M. Ortner, “Learning disen-\ntangled representations of satellite image time series,” in Proc.\nJoint Eur. Conf. Mach. Learn. Knowl. Discov. Databases , 2019,\npp. 306–321.\n[78] A. Odena, V . Dumoulin, and C. Olah, “Deconvolution and checkerboard\nartifacts,” Distill, vol. 1, no. 10, p. e3, 2016.\n[79] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Im-\nage quality assessment: From error visibility to structural similar-\nity,” IEEE Trans. Image Process., vol. 13, no. 4, pp. 600–612, Apr.\n2004.\n[80] M. Guizar-Sicairos, S. T. Thurman, and J. R. Fienup, “Efﬁcient subpixel\nimage registration algorithms,”Opt. Lett., vol. 33, no. 2, pp. 156–158,\n2008.\n[81] S. Farsiu, M. D. Robinson, M. Elad, and P. Milanfar, “Fast and robust\nmultiframe super resolution,”IEEE Trans. Image Process., vol. 13, no. 10,\npp. 1327–1344, Oct. 2004.\n[82] M. Irani and S. Peleg, “Improving resolution by image registration,”\nCVGIP: Graphical Models Image Process., vol. 53, no. 3, pp. 231–239,\n1991.\n[83] Y . Jo, S. W. Oh, J. Kang, and S. J. Kim, “Deep video super-resolution\nnetwork using dynamic upsampling ﬁlters without explicit motion com-\npensation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018,\npp. 3224–3232.\n[84] S. Xingjian, Z. Chen, H. Wang, D.-Y . Yeung, W.-K. Wong, and W.-C.\nWoo, “Convolutional LSTM network: A machine learning approach for\nprecipitation nowcasting,” inProc. Adv. Neural Inf. Process. Syst., 2015,\npp. 802–810.\nTai An received the B.S. degree in information engi-\nneering from Xi’an Jiaotong University, Xi’an, China,\nin 2018. He is currently working toward the Ph.D.\ndegree in pattern recognition and intelligent system\nwith the Institute of Automation, Chinese Academy\nof Sciences and University of Chinese Academy of\nSciences, Beijing, China.\nHis research interests include remote sensing,\npattern recognition, computer vision, especially on\nsuper-resolution.\nXin Zhang received the B.S. degree in information\nand computing science from Beijing University of\nTechnology, Beijing, China, in 2014, the M.S. de-\ngree in applied mathematics from Beijing University\nof Technology, Beijing, China, in 2018. He is cur-\nrently working toward the Ph.D. degree in pattern\nrecognition and intelligent system with the Institute\nof Automation, Chinese Academy of Sciences and\nUniversity of Chinese Academy of Sciences, Beijing,\nChina.\nHis research interests are object detection, com-\nputer vision, pattern recognition, and remote sensing.\nChunlei Huo (Member, IEEE) received the B.S.\ndegree in applied mathematics from Hebei Normal\nUniversity, Shijiazhuang, China, in 1999, the M.S.\ndegree in applied mathematics from Xidian Univer-\nsity, Xi’an, China, in 2002, and the Ph.D. degree\nin pattern recognition and intelligent system from\nthe Institute of Automation, Chinese Academy of\nSciences, Beijing, China, in 2009.\nHe is currently a Professor with the National Lab-\noratory of Pattern Recognition, Institute of Automa-\ntion, Chinese Academy of Sciences. His research\ninterests include remote sensing image processing, computer vision, pattern\nrecognition.\nBin Xue received the B.S. degree in automation from\nNorthwestern Polytechnical University, Xi’an, China,\nin 2018. He is currently working toward the Ph.D.\ndegree in pattern recognition and intelligent system\nwith the Institute of Automation, Chinese Academy\nof Sciences and University of Chinese Academy of\nSciences, Beijing, China. His research interests are\ncomputer vision and pattern recognition.\nLingfeng Wang (Member, IEEE) received the B.S.\ndegree in computer science from Wuhan University,\nWuhan, China, in 2007. He received the Ph.D. degree\nin pattern recognition and intelligent system from In-\nstitute of Automation, Chinese Academy of Sciences,\nin 2013.\nHe is currently an Associate Professor with the Na-\ntional Laboratory of Pattern Recognition of Institute\nof Automation, Chinese Academy of Sciences. His\nresearch interests include computer vision and image\nprocessing.\nChunhong Pan (Member, IEEE) received the B.S.\ndegree in automatic control from Tsinghua Univer-\nsity, Beijing, China, in 1987, the M.S. degree from\nthe Shanghai Institute of Optics and Fine Mechanics,\nChinese Academy of Sciences, Shanghai, China, in\n1990, and the Ph.D. degree in pattern recognition and\nintelligent system from the Institute of Automation,\nChinese Academy of Sciences, Beijing, in 2000.\nHe is currently a Professor with the National Labo-\nratory of Pattern Recognition of Institute of Automa-\ntion, Chinese Academy of Sciences. His research\ninterests include computer vision, image processing, computer graphics, and\nremote sensing."
}