{
  "title": "Transformers for cardiac patient mortality risk prediction from heterogeneous electronic health records",
  "url": "https://openalex.org/W4322760726",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3033127715",
      "name": "Emmi Antikainen",
      "affiliations": [
        "VTT Technical Research Centre of Finland"
      ]
    },
    {
      "id": "https://openalex.org/A2374605695",
      "name": "Joonas Linnosmaa",
      "affiliations": [
        "VTT Technical Research Centre of Finland"
      ]
    },
    {
      "id": "https://openalex.org/A2489632168",
      "name": "Adil Umer",
      "affiliations": [
        "VTT Technical Research Centre of Finland"
      ]
    },
    {
      "id": "https://openalex.org/A1711019760",
      "name": "Niku Oksala",
      "affiliations": [
        "Tampere University Hospital",
        "Tampere University",
        "Tampere University"
      ]
    },
    {
      "id": "https://openalex.org/A1952669825",
      "name": "Markku Eskola",
      "affiliations": [
        "Tampere University",
        "Tampere University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1980257032",
      "name": "Mark van Gils",
      "affiliations": [
        "Tampere University"
      ]
    },
    {
      "id": "https://openalex.org/A1550693174",
      "name": "Jussi Hernesniemi",
      "affiliations": [
        "Tampere University",
        "Tampere University",
        "Tampere University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A237246515",
      "name": "Moncef Gabbouj",
      "affiliations": [
        "Tampere University"
      ]
    },
    {
      "id": "https://openalex.org/A3033127715",
      "name": "Emmi Antikainen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2374605695",
      "name": "Joonas Linnosmaa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2489632168",
      "name": "Adil Umer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1711019760",
      "name": "Niku Oksala",
      "affiliations": [
        "Tampere University",
        "Tampere University of Applied Sciences",
        "Tampere University"
      ]
    },
    {
      "id": "https://openalex.org/A1952669825",
      "name": "Markku Eskola",
      "affiliations": [
        "Tampere University",
        "Tampere University of Applied Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1980257032",
      "name": "Mark van Gils",
      "affiliations": [
        "Tampere University"
      ]
    },
    {
      "id": "https://openalex.org/A1550693174",
      "name": "Jussi Hernesniemi",
      "affiliations": [
        "Tampere University",
        "Tampere University of Applied Sciences",
        "Tampere University"
      ]
    },
    {
      "id": "https://openalex.org/A237246515",
      "name": "Moncef Gabbouj",
      "affiliations": [
        "Tampere University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2893443580",
    "https://openalex.org/W3092301826",
    "https://openalex.org/W4292554358",
    "https://openalex.org/W2899736836",
    "https://openalex.org/W3025578374",
    "https://openalex.org/W3125603166",
    "https://openalex.org/W2790916396",
    "https://openalex.org/W3132259035",
    "https://openalex.org/W3017637887",
    "https://openalex.org/W2965570621",
    "https://openalex.org/W3160137267",
    "https://openalex.org/W3089168780",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2746230914",
    "https://openalex.org/W2806806521",
    "https://openalex.org/W2966312603",
    "https://openalex.org/W2964006392",
    "https://openalex.org/W2985962305",
    "https://openalex.org/W4297734170",
    "https://openalex.org/W3000238064",
    "https://openalex.org/W2963271116",
    "https://openalex.org/W2784499877",
    "https://openalex.org/W2607113351",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4288332959",
    "https://openalex.org/W3166996592",
    "https://openalex.org/W4287725152",
    "https://openalex.org/W2941013735",
    "https://openalex.org/W2101485098",
    "https://openalex.org/W2127841529",
    "https://openalex.org/W2944112660",
    "https://openalex.org/W2625362162",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4394662461",
    "https://openalex.org/W2883265831",
    "https://openalex.org/W3082801777",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W2271840356",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W4226250038",
    "https://openalex.org/W3098949126",
    "https://openalex.org/W3133650345"
  ],
  "abstract": "Abstract With over 17 million annual deaths, cardiovascular diseases (CVDs) dominate the cause of death statistics. CVDs can deteriorate the quality of life drastically and even cause sudden death, all the while inducing massive healthcare costs. This work studied state-of-the-art deep learning techniques to predict increased risk of death in CVD patients, building on the electronic health records (EHR) of over 23,000 cardiac patients. Taking into account the usefulness of the prediction for chronic disease patients, a prediction period of six months was selected. Two major transformer models that rely on learning bidirectional dependencies in sequential data, BERT and XLNet, were trained and compared. To our knowledge, the presented work is the first to apply XLNet on EHR data to predict mortality. The patient histories were formulated as time series consisting of varying types of clinical events, thus enabling the model to learn increasingly complex temporal dependencies. BERT and XLNet achieved an average area under the receiver operating characteristic curve (AUC) of 75.5% and 76.0%, respectively. XLNet surpassed BERT in recall by 9.8%, suggesting that it captures more positive cases than BERT, which is the main focus of recent research on EHRs and transformers.",
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2023) 13:3517  | https://doi.org/10.1038/s41598-023-30657-1\nwww.nature.com/scientificreports\nTransformers for cardiac \npatient mortality risk prediction \nfrom heterogeneous electronic \nhealth records\nEmmi Antikainen 1*, Joonas Linnosmaa 1, Adil Umer 1, Niku Oksala 2,3,4, Markku Eskola 3,5, \nMark van Gils 2, Jussi Hernesniemi 2,3,5,7 & Moncef Gabbouj 6,7\nWith over 17 million annual deaths, cardiovascular diseases (CVDs) dominate the cause of death \nstatistics. CVDs can deteriorate the quality of life drastically and even cause sudden death, all the \nwhile inducing massive healthcare costs. This work studied state-of-the-art deep learning techniques \nto predict increased risk of death in CVD patients, building on the electronic health records (EHR) of \nover 23,000 cardiac patients. Taking into account the usefulness of the prediction for chronic disease \npatients, a prediction period of six months was selected. Two major transformer models that rely on \nlearning bidirectional dependencies in sequential data, BERT and XLNet, were trained and compared. \nTo our knowledge, the presented work is the first to apply XLNet on EHR data to predict mortality. \nThe patient histories were formulated as time series consisting of varying types of clinical events, \nthus enabling the model to learn increasingly complex temporal dependencies. BERT and XLNet \nachieved an average area under the receiver operating characteristic curve (AUC) of 75.5% and 76.0%, \nrespectively. XLNet surpassed BERT in recall by 9.8%, suggesting that it captures more positive cases \nthan BERT, which is the main focus of recent research on EHRs and transformers.\nElectronic health records (EHRs) encompass evidence of patient care paths and outcomes. Different EHR models \nhave been widely adopted by healthcare facilities and continue to accumulate increasing amounts of data with \npotential to discover new medical knowledge and to support decision making to improve outcomes for new \n patients1. Although EHRs offer large volumes of longitudinal real-life data for improved machine learning (ML), \nthey still challenge the methodology with their heterogeneous, sparse, often incomplete and even erroneous  data2. \nMoreover, due to the sensitive nature of the data, privacy issues and regulations will further complicate model \ndevelopment and deployment in the  future3. Some regulations may require database anonymization to protect \ndata privacy but this may result in decreased data quality due to additional noise and gaps.\nCardiovascular diseases (CVDs) have held their ranking as the leading cause of death worldwide for years \nand continue to impose an increasing challenge to the global health. In 2017, CVDs alone caused 17.8 million \ndeaths globally, showing an alarming 21.2 % increase in the yearly CVD death count since  20074. Furthermore, \nCVDs can be a risk factor in relation to other diseases and increase the demand for hospital care. For instance, \nthey have been linked with poor prognosis in the context of COVID-19, which threatened health care capacity \nall over the  world5. The problem of CVDs has not been sufficiently addressed. While the risk could be efficiently \nreduced with lifestyle changes towards physically active lives and healthier diets, the reports of the aging popula-\ntion and overwhelming obesity rates indicate enduring prosperity for  CVDs4. Predictive models may help identify \nhigh-risk patients and patient deterioration and may be used to focus healthcare resources efficiently to improve \npatient outcomes and manage the increasing CVD counts. Data-driven approaches are expected to renew the \nclinical cardiology practice, ascertain their place in the clinician’s toolbox, and to reform our understanding of \nthe causes of  CVDs6,7.\nOPEN\n1VTT Technical Research Centre of Finland Ltd., 33101 Tampere, Finland. 2Faculty of Medicine and Health \nTechnology, Tampere University, 33720 Tampere, Finland. 3Finnish Cardiovascular Research Center Tampere, \n33520 Tampere, Finland. 4Vascular Centre, Tampere University Hospital, 33520 Tampere, Finland. 5Tays Heart \nHospital, Tampere University Hospital, 33521 Tampere, Finland. 6Faculty of Information Technology and \nCommunication Sciences, Tampere University, 33720 Tampere, Finland. 7These authors contributed equally: Jussi \nHernesniemi and Moncef Gabbouj. *email: emmi.antikainen@gmail.com\n2\nVol:.(1234567890)Scientific Reports |         (2023) 13:3517  | https://doi.org/10.1038/s41598-023-30657-1\nwww.nature.com/scientificreports/\nTransformer neural networks are the state-of-the-art machine learning methods for sequential data modelling. \nDeveloped for natural language processing, their built-in properties respond to many needs that arise when using \nEHR data. Thanks to them combining attention and positional encoding, transformers can be applied to learn \nbidirectional temporal dependencies despite the sparsity and possible errors in the large volumes of EHR data. \nTheir design to handle textual input does not exclude numerical input and may thus be useful for heterogeneous \ninput types. In the context of EHRs, they have been applied mainly to clinical notes or  diagnoses8–11. Y et, their \ncapabilities to capture more complex dependencies in heterogeneous databases have received little  attention12. \nFurthermore, prior studies have focused on one transformer variant; bidirectional encoder representations from \ntransformers (BERT)13. A newer model, XLNet, has surpassed BERT in many baseline natural language process-\ning  tasks14. This work uses an anonymous cardiac patient EHR database to compare the learning capabilities \nof BERT and XLNet in the important application of mortality risk prediction. Here, the transformer models \nare applied to multi-modal heterogeneous patient event time series, comprising both textual and numerical \nattributes.\nPrior to transformers, convolutional neural networks (CNNs) and recurrent neural networks (RNNs) achieved \nencouraging results in, e.g., arrhythmia detection from electrocardiograms (ECG), diagnostic decision sup-\nport using cardiovascular images, and diagnosis prediction from EHR  data15–18. The introduction of attention \nmechanisms provoked countless new studies reporting improved  results19–23. Importantly for clinical applica-\ntions, attention gave interpretability to the model outcomes, thus offering one solution to the primarily criticized \nshortcoming of deep learning (DL)  methods19. For example, Choi et al. presented the RETAIN model which \ncoupled attention with recurrent neural networks (RNNs) to predict heart failure from EHR data. They presented \na method for prediction interpretation while reporting an 87% area under the receiver operating characteris-\ntics curve (AUC) 21. Another relevant study was conducted by Rajkomar et al. who used an ensemble of three \nDL models, one of which was attention-based, and tested their system on EHR data from two hospitals. They \nachieved 93–95% AUC for in-patient mortality prediction at 24 h after  admission22.\nThe original Transformer relied exclusively on attention  mechanisms24. The Transformer and its variants \nsurpassed RNNs by allowing parallelized computing and by learning bidirectional dependencies. They learned \nlonger-range dependencies at improved training time, which is crucial with long input sequences like EHR \n histories24. The first studies applying transformers directly on EHRs were built on BERT, which bases its learning \nstrategy on masking the  input13. Shang et al. combined BERT with ontology embeddings from a graph neural net-\nwork creating G-BERT for medication  recommendation10. They reported a 1% increase in precision-recall AUC \nas compared to RETAIN. BEHRT by Li et al. applied BERT directly for disease prediction by using sequences \nof diagnoses available in the  EHRs9. They reported a patient-averaged AUC of 95–96% for varying prediction \nwindows extending up to 12 months. Thirdly, Rasmy et al. reported up to 2% improvement in disease predic-\ntion with their Med-BERT as compared to  RETAIN11. They evaluated Med-BERT for heart failure prediction in \ndiabetic patients and pancreatic cancer onset prediction. Some studies have additionally proposed somewhat \nmodified transformers for EHR representation  learning25–27.\nIn this study, we apply the ground-breaking transformer models on patient time series to predict 6-month \nmortality in cardiac patients. The 6 months prediction period may offer actionable predictions for many chronic \nconditions. Unlike BEHRT and Med-BERT which were trained on sequences of diagnostic codes, we incorporate \nover a dozen different event types each described by multiple attributes to capture a more complete depiction of \nthe patient history. By feeding the transformers sequences of patient events with timestamps based on age, the \nmodels may learn how the interplay between different events and their outcomes, as well as temporal dependen-\ncies, affect the patient outcome. With this approach, the patterns learned by the model may unveil unforeseen \nassociations between different events. Moreover, we study both BERT and XLNet. Unlike BERT, XLNet is an \nauto-regressive transformer variant that avoids corrupting the  input14. We exploit the anonymous EHRs of over \n23,000 cardiac patients who were treated at Tays Heart Hospital in Finland and report our findings on using \nprivacy-preserving anonymous data in model development, an increasingly common starting point for future \nEHR studies. A previous machine learning study on the same database considered a subset of 9066 consecutive \nacute coronary syndrome patients and achieved an AUC of 89% for 6-month mortality using conventional, non-\ndeep learning  methods28. This study takes up a more complex challenge of predicting mortality in all available \nCVD patients, comprising a more heterogeneous patient population.\nMethods\nStudy data. The longitudinal study data comprised three Finnish data sources: (1) the EHR by the Pirkan-\nmaa Hospital District (PHD), (2) the KARDIO registry by the Tampere Heart Hospital, and (3) the Finnish \nmortality registry by Statistics Finland. The PHD EHR extend back to the 1990’s and the date of death from \nthe mortality registry was included for the matching period. The PHD EHR data include hospital discharge \ndiagnoses, which record every diagnosis recorded for the patient in ICD-10 format (and previously in ICD-9 \nand ICD-8 formats). This data is equally reported in every hospital nationally and the validity of the registry is \nhigh for many significant cardiovascular conditions such as strokes, coronary heart disease and heart  failure29–31. \nThe KARDIO registry is the most recent of the three; its first entries date back to the early 2000’s. The original \ndatabase was automatically collected from the three registries until January 2020 as a part of a retrospective \nregistry study, MADDEC (Mass Data in Detection and Prevention of Serious Adverse Events in Cardiovascular \nDisease)32.\nThe study was approved by the Pirkanmaa Hospital District Institutional Review Board’s scientific steering \ncommittee. Informed consent is waived since the retrospective nature of the study by the Pirkanmaa Hospital \nDistrict Institutional Review Board’s scientific steering committee. The study was conducted according to the \ndeclaration of Helsinki as applicable and the study data was processed in accordance with the Finnish legislation. \n3\nVol.:(0123456789)Scientific Reports |         (2023) 13:3517  | https://doi.org/10.1038/s41598-023-30657-1\nwww.nature.com/scientificreports/\nAn anonymous version of the database was used, comprising 72,680 patients (9172 deceased patients within \nsix months of their last visit, i.e., 12.6%) all treated at the Tays Heart Hospital for different cardiac conditions.\nInput sequence formulation. The patient records were extracted from the event-oriented database, pre-\nprocessed, and finally ordered on temporal attributes to formulate a single time series of events for each patient. \nThe resulting time series were further processed into appropriate input for the transformer neural networks, as \nsummarized in Fig. 1. In the anonymous database, the temporal attributes were age-based (on a daily level, i.e., \ndays since birth) and the real dates and times were unknown.\nData pre-processing included replacing Roman numerals with Arabic numerals, filling in event start or end \ntimes when only one of the two was missing, and unifying notations. Units and measurement names, anesthesia \ntypes, and urgency classes were translated from Finnish to English. Additionally, some body-mass index (BMI) \nvalues below 0.02 were presumed to use centimeters instead of meters for height, and thus, multiplied by 104 to \nrestore correct units. For events where the ending time preceded the recorded event start time, the timestamp \norder was presumed a typographical error and the timestamps were switched. In the end, only the pre-processed \nevent start time was included as the event durations were generally error-prone.\nAny events occurring before the patient became of age were excluded. Additionally, any events with missing \nevent timing or overlap with the date of death were excluded. The latter consists of events extending to the date \nof death, e.g., resuscitation or procedures, or beyond, such as lab values or diagnoses.\nThe data sources contained 14 distinct types of events each described by a different set of attributes, as pre -\nsented in Table 1. Here, we took the liberty of excluding any attributes that were never present for an event type. \nFor angiography, percutaneous coronary intervention (PCI), coronary care unit (CCU), transcatheter valve \nimplantation (TAVI), and resuscitation events, the attributes were limited to the nine most available attributes \n(out of tens of attributes) to control input sequence length and to fit multiple events in the input sequence. Each \nevent was constructed into a sequence simply by listing the event type and the corresponding attributes in one \nsequence. Thus, each event type was represented by a specific “sentence structure” mimicking natural language. \nAny missing attribute values were filled in with ’None’ . All event representations started with the event type name, \nthe event starting time, and residence (among Finnish counties) when available. Residence was available in 67%, \n74%, and 79% of CCU, resuscitation, and hospital ward events respectively, while it was missing completely for \nTAVI and in 59–94% for other event types. Event type, start time, all operation attributes, times repeated, ward, \nsex, stenosis, imaging type, dialysis, temporary pacemaker, primary vasoactive medication, fluoroscopy time, and \nglomerular filtration rate attributes were all fully available for the relevant event types. The remaining attributes \nin labs were available in 72–75% of lab events (except textual values only in 1%). Diagnosis code and priority \nwere available in 36% and 41% of diagnosis events respectively, and anesthesia type, ASA class, and urgency in \n56%, 61%, and 93% of procedures. All measurement event attributes were available in 91–100% of measurement \nevents. The remaining attributes in angiography, PCI, CCU, and TAVI were available in 98-100% of the respec-\ntive events, whereas the other attributes for resuscitation events were available in 89% of resuscitation events.\nThe individual pre-processed events were combined in order of occurrence into one sequence per patient, \nforming the patient event timeline. Until this point, the events were linked via patient and event pseudo-iden-\ntifiers. The pseudo-identifiers were removed and the date of death was isolated and transformed into a binary \nclass: positive (1) when the date of death occurred within 182 days of the last event, and negative (0) otherwise. \nThe date of death was comprehensively obtained from the Finnish mortality registry. Importantly, in the real-life \nFigure 1.  A schematic example of how a patient’s events were formulated into a time series. The records in the \nevent-oriented database contain the event type specific attributes (yellow circles). First, the events related to the \nsame patient ID (magenta square) were combined to a sequence sorted according to their temporal attributes. \nHereafter, the patient ID was no longer necessary. Next, the class (prediction target, i.e., death within six months \nor alive) was computed using the death-related attribute (green striped circle) and the time between that and the \nprevious event. Finally, the point of prediction was randomized. If the class was positive, the time to death from \nthe final remaining event was maintained within the selected six month period.\n4\nVol:.(1234567890)Scientific Reports |         (2023) 13:3517  | https://doi.org/10.1038/s41598-023-30657-1\nwww.nature.com/scientificreports/\nclinical use-case the model could be used to produce predictions at any time of a patient timeline. Therefore, to \nproduce realistic evaluation of model performance and avoid bias due to the retrospective nature of the data, a \nrandom number of events at the end of a patient’s timeline were erased. The number of erased events was selected \nrandomly between zero and a patient-specific maximum number such that at least five events remained for the \npatient, and the death for any positive case would still occur within the selected cutoff from the final remaining \nevent.\nFinally, the input sequences were tokenized and the special tokens for class (CLS) and sentence separation \n(SEP) were added once to each patient timeline according to their expected position in BERT and  XLNet33. Any \nnumerical input was transformed into string-type integers for tokenization. The age in days was transformed \ninto full years.\nHyperparameter optimization. The model hyperparameters were optimized using Population Based \nTraining (PBT)34,35. PBT is an evolutionary algorithm, which trains several networks with varying hyperpa-\nrameters in parallel. During the training process, each network can explore hyperparameters randomly in a \npredefined space or exploit another better performing parallel model by copying its parameters and continuing \nto explore new hyperparameters with the partially trained model, without restarting the training from scratch.\nPBT was applied to optimize the learning rate, dropout fraction, and model dimensions including the num-\nber of heads and layers, as well as layer size. Due to memory limitations, only batch sizes 16 and 24 were tested. \nPBT was run for both BERT and XLNet for 30 epochs on 12 trials with the perturbation interval of ten epochs. \nSimilarly to the original transformers, Gaussian Error Linear Unit (GELU) was used for activation.\nModel evaluation. Eighty percent of the study data was used for model development, while 20% was held \nout as a test set. The development data was further split into training and validation data, comprising 80% and \n20% of the development set, respectively. Stratified splits were used to maintain a similar distribution of positive \nand negative cases in each set. The data sets were further balanced by taking a random sample of negative cases \nto match the number of positive cases (see details in Implementation). Model performance was assessed with \nAUC, precision (positive predictive value), and recall (sensitivity)36.\nPBT was performed on the development set. The top-performing BERT and XLNet models were validated \nusing stratified fivefold cross validation with the development data. Subsequently, the final BERT and XLNet \nmodels were trained with the selected hyperparameters on the full development data and evaluated on the held \nout test data set.\nTable 1.  Event specific attributes *Measurement context in text format, for example a suspected diagnosis \nor type of the visit. **Measurement context related code (e.g. an ICD-10 diagnostic code). ***Family history \n(for early coronary artery disease) was positive if at least one of the patient’s first degree relatives had suffered \na myocardial infarction or underwent coronary revascularization (PCI or coronary artery bypass surgery) at \nan early age ( < 55 and < 65 years in men and women, respectively). a International Statistical Classification of \nDiseases and Related Health Problems, the 10th revision (ICD-10). b Anatomical Therapeutic Chemical (ATC) \ncode. c Nordic Classification of Surgical Procedures (NCSP). d American Society of Anesthesiologists (ASA) \nclassification of physical status.\nEvent type Attributes\nLabs Event type, start time, residence, lab test value (num), lab test value (char), lab test name, \nlab test unit\nDiagnosis Event type, start time, residence, diagnosis codea , diagnosis priority\nMedication Event type, start time, residence, ATC codeb , daily dosage, dose unit, administration \nmethod\nOperation Event type, start time, residence, sequence number, code ID, codec\nProcedure Event type, start time, residence, anesthesia type, ASA classd , operation urgency\nMeasurement Event type, start time, residence, measurement value (num), measurement context*, \nmeasurement name, measurement unit, measurement context code**\nHospital visit Event type, start time, residence\nHospital ward Event type, start time, residence, times repeated, ward\nAngiography Event type, start time, residence, times repeated, ward, primary angiography findings, sex, \nstenosis (boolean), primary puncture places\nPercutaneous coronary intervention (PCI) Event type, start time, residence, times repeated, ward, complications, sex, indication, \nurgency\nImaging Event type, start time, residence, imaging type\nCoronary care unit (CCU) Event type, start time, residence, times repeated, ward, dialysis, sex, temporary pace-\nmaker, primary vasoactive medication\nTranscatheter aortic valve implantation (TAVI) Event type, start time, times repeated, ward, dyslipidemia, fluoroscopy time, sex, glo-\nmerular filtration rate, hypertension\nResuscitation Event type, start time, residence, times repeated, ward, family history***, sex, hyperten-\nsion, smoking\n5\nVol.:(0123456789)Scientific Reports |         (2023) 13:3517  | https://doi.org/10.1038/s41598-023-30657-1\nwww.nature.com/scientificreports/\nThe final model training was repeated five times to account for the effect of random initialization. Early \nstopping was applied when the training loss failed to improve at least by 0.0045 over 5 epochs (min_delta  \nand patience in keras EarlyStopping, selected based on the previously observed cross-validation losses). To \ninterpret what the final models had learned, the models were fed example time series from the test set and the \nattention weights were visualized using  BertViz37. Min–max normalization was applied to the attention layers \nprior to the visualization to properly highlight where attention was at its highest and lowest.\nImplementation. The data were tokenized using pretrained tokenizers (bert-base-cased, xlnet-base-cased) \navailable in the Hugging Face model  database13,14. The transformer models were implemented in Python by \nusing the Hugging Face Transformers library together with  Tensorflow33,38. The Ray Tune package (function \nAPI) was used for hyperparameter  optimization35. The data split for model evaluation was obtained using scikit-\nlearn39. The final models were trained using an Adam optimizer with an epsilon of 10−8 . The sequence length \nwas restricted to 512 tokens such that the latest information in the patient history was included. Overlength \nsequences were truncated and under-length sequences padded using the tokenizer-specific padding token.\nClass imbalance was managed by (1) down-sampling the negative examples in the training and validation \nsets and (2) using a weighted binary cross-entropy loss function. To ensure that each limited-size batch had a \nreasonable chance of including some positive cases, the negative samples were randomly down-sampled so that \n25% of the samples in both training and validation set were positive. By limiting the extent of down-sampling, \nthe related data loss was also limited. The remaining imbalance was counteracted via the loss function using \nbalanced class weights; each class was weighted by its inverse prevalence in the development set, further divided \nby the number of classes (two).\nA 32 gigabyte Tesla V100-DGXS graphics processing unit (GPU) was used in hyperparameter optimization \nand training the models.\nResults\nImplementing the exclusion criteria reduced the study data from 72,680 patients to 57,377 adult patients, includ-\ning 3771 (6.57%) positive cases. The demographic details are described in Table 2. The average age of patients was \n65 years (79 for positive cases). The sex of the patient was only available for 35.7%, most of which (61.2%) were \nmale. The notably large portion of sex information was lost upon anonymization as the national identification \nnumbers were removed.\nAfter down-sampling the development sets to counteract class imbalance (as detailed in “Methods”), the \nresulting training and validation sets contained 9640 and 2420 patients (12,060 in total with 3015 positive cases). \nThe test set comprised 11,482 patients with the number of positive cases, 756 (6.58%) corresponding approxi -\nmately to the prevalence in the full pre-processed data. Thus, the study involved 23,542 individuals (including \nall 3771 positive cases). The patient flow is summarized in Fig. 2.\nThe hyperparameters optimized using PBT are presented in Table 3. BERT performed best on learning rates \naround 5 × 10−7 to 1 ×10−6 , whereas rates an order of magnitude larger ( 5 × 10−6 to 1 × 10−5 ) worked best \nTable 2.  Pre-processed study data. The percentages depict the proportion of the (known) sex with respect to \nthe full number of patients on the same row. SD standard deviation.\nN Female Male Age range Mean years of data (SD) Mean no. of events (SD)\nPositive 3771 691 (18.3%) 1183 (31.4%) 18–102 6.5 (3.4) 1755 (2364)\nNegative 53,606 7249 (13.5%) 11,365 (21.2%) 18–105 4.2 (3.8) 553 (1091)\nTotal 57,377 7940 (13.8%) 12,548 (21.9%) 18–105 4.4 (3.9) 632 (1255)\nFigure 2.  Patient flow diagram. The total number of patients is indicated for each step and the number of \npositive cases is denoted in brackets. The final data used in model evaluation comprised 23,542 patients and is \ndepicted on a gray background.\n6\nVol:.(1234567890)Scientific Reports |         (2023) 13:3517  | https://doi.org/10.1038/s41598-023-30657-1\nwww.nature.com/scientificreports/\nfor XLNet. The selected configurations comprised 108,312,578 trainable parameters for BERT and 5,482,130 \nparameters for XLNet.\nThe models with optimized hyperparameters were cross-validated using 5-fold validation to assess their \nsensitivity to the selection of training instances. The validation results are presented in Table  4. The models \nachieved similar average AUC. BERT achieved slightly higher precision but the variance between folds was also \nhigher. However, less than half of the predicted cases were true positive cases. Finally, XLNet reached a notably \nhigher average recall, with low variance between folds. Thus, the optimized XLNet was more sensitive to detect \npositive cases than BERT.\nThe final model training was repeated five times to examine the effect of random initialization. The test results \nobtained on the held-out test set are presented in Table 5. The corresponding mean specificity scores were 78% \nand 69% for BERT and XLNet, respectively. The test set results support the observations from cross-validation. \nThe slight improvement in AUC and recall were likely due to early stopping, which stopped the training already \nbefore 50 epochs in all cases. This prevented over-fitting, which occurred remarkably early for this data and \nmodels. The drop in precision is explained by the increased class imbalance in the test set but also underlines \nthat both models produced mostly false positives, despite capturing 73–83% of the positive cases on average. \nIn comparison to BERT, the improvement in XLNet’s recall exceeds the drop in precision and, thus, the XLNet \nmodel may be more useful.\nThe final BERT and XLNet models exhibited very similar metrics (run number 5 in Table  5) and were fed \nan example time series from the test set for interpretation. The model attention for the 50 tokens nearest to the \nclassification token in an example time series are depicted in Fig.  3a,b for XLNet and BERT, respectively. The \nselected (full sequence) example was correctly labeled positive by XLNet and mislabeled negative by BERT. The \nTable 3.  Hyperparameters optimized via population based training.\nHyperparameter BERT XLNet\nHidden size 144 144\nNumber of layers 12 6\nNumber of attention heads 12 6\nFeed-forward layer hidden size 128 128\nLearning rate 1 × 10−6 5 × 10−6\nBatch size 16 16\nDropout 0.5 0.4\nTable 4.  5-fold cross-validation of optimized models. Performance metrics in the validation set, after 50 \nepochs. The best mean score for each metric (AUC, precision, recall) is in bold. \n Fold\nBERT XLNet\nAUC Precision Recall AUC Precision Recall\n1 0.7452 0.4567 0.8126 0.7438 0.4592 0.8027\n2 0.7366 0.5244 0.6783 0.7570 0.4740 0.8159\n3 0.7703 0.4711 0.8640 0.7692 0.4873 0.8292\n4 0.7432 0.5350 0.6849 0.7454 0.4496 0.8292\n5 0.7689 0.5047 0.7993 0.7557 0.4815 0.7977\nMean 0.7528 0.4984 0.7678 0.7542 0.4703 0.8149\nTable 5.  Blind test results on five different initializations. The best mean score for each metric (AUC, \nprecision, recall) is in bold.\n Run\nBERT XLNet\nAUC Precision Recall AUC Precision Recall\n1 0.7398 0.2248 0.6336 0.7556 0.1533 0.8373\n2 0.7612 0.1923 0.7421 0.7602 0.1574 0.8360\n3 0.7586 0.1919 0.7355 0.7654 0.1665 0.8201\n4 0.7547 0.1937 0.7209 0.7609 0.1601 0.8280\n5 0.7591 0.1571 0.8333 0.7586 0.1563 0.8347\nMean 0.7546 0.1919 0.7330 0.7602 0.1587 0.8312\n7\nVol.:(0123456789)Scientific Reports |         (2023) 13:3517  | https://doi.org/10.1038/s41598-023-30657-1\nwww.nature.com/scientificreports/\ncorresponding pre-processed example input before tokenization is depicted in Fig.  3c. The full patient history \ncomprised 111 events, whereas the models could only consume input from up to 38 events.\nThe 83 years old patient’s latest event was an operation encoded as H0519, which stands for a simulation \nfilm, possibly related to radiation therapy planning. Their history also showed, e.g., an angiography of the heart \nand/or coronary artery, a percutaneous transluminal coronary angioplasty, and an intraventricular stent place-\nment to enlarge the coronary artery, all within the past year. As seen in Fig. 3a at the end of the sequence, XLNet \nattends especially to the age (three instances visualized) and to the operation code. Most other layers also attend \nto age and the operation code at the <cls> classification token, while exhibiting varying attention to the other \ninputs. In contrast, BERT’s attention at the [CLS] classification token in Fig. 3b does not exhibit special attention \nto the patient’s age (not the primary focus of attention in any layer) but attends to some of the lab results. It is \nnoted that a tokenizer specialized in EHR data might not only make the interpretation easier but also improve \nattention results.\n    a               bc\n…\nlabs, 82, None, 266, None, B -Trom, E9/l\nlabs, 82, None, 1, None, fP-Kol-LDL, mmol/l\nlabs, 82, None, 344, None, E -MCHC, g/l\nlabs, 82, None, 91, None, E -MCV, fl\nlabs, 82, None, 332, None, E -MCHC, g/l\nlabs, 82, None, 3, None, B -Eryt, E12/l\noperation, 82, Päijät-Häme, 2, FN1BC\noperation, 82, Päijät-Häme, 3, FN1BT\noperation, 82, Päijät-Häme, 4, FN1YT\nangio, 82, Päijät-Häme, 1, None, 3-VD, 1, \nis stenosis, A. radialis\npci, 82, Päijät-Häme, 1, None, No complications , \n1, NSTEMI, during the same hospitalization period\nlabs, 82, None, 1, None, P -CRP, mg/l\nlabs, 82, None, 3, None, B -Eryt, E12/l\nlabs, 82, None, 3, None, P -K, mmol/l\nlabs, 82, None, 0, None, B -HKR, %\nlabs, 82, None, 129, None, B -Hb, g/l\nlabs, 82, None, 61, None, fP-Krea, umol/l\nlabs, 82, None, 0, None, B -HKR, %\nlabs, 82, None, 315, None, B -Trom, E9/l\nlabs, 82, None, 143, None, P -Na, mmol/l\nlabs, 82, None, 14, None, Pt-EKG-12, form\nlabs, 82, None, 9, None, fB-Leuk, E9/l\nlabs, 82, None, 342, None, E -MCHC, g/l\nlabs, 82, None, 29, None, E -MCH, pg\nlabs, 82, None, 88, None, E -MCV, fl\nlabs, 82, None, 88, None, E -MCV, fl\nlabs, 82, None, 5, None, fB-Leuk, E9/l\nlabs, 82, None, 14, None, Pt-EKG-12, form\nlabs, 82, None, 45, None, P -CRP, mg/l\nlabs, 82, None, 364, None, E -MCHC, g/l\nlabs, 82, None, 3, None, P -K, mmol/l\nlabs, 82, None, 30, None, E -MCH, pg\nlabs, 82, None, 66, None, fP-Krea, umol/l\nlabs, 82, None, 128, None, B -Hb, g/l\nlabs, 82, None, 4, None, B -Eryt, E12/l\nlabs, 82, None, 293, None, B -Trom, E9/l\nlabs, 82, None, 143, None, P -Na, mmol/l\noperation, 83, None, 1, H0519\nFigure 3.  Attention (a) in the fifth attention layer in XLNet at the end of an example subsequence near the \n<cls> token, and (b) in the final attention layer in BERT at the start of an example subsequence near the [CLS] \ntoken. The different colours represent the (a) six and (b) 12 attention heads; the more opaque the colour, the \nheavier the attention. The final events of the example time series are presented in a human readable format in \n(c), where the first information visible to BERT is highlighted with purple and with green for XLNet. Figures \n(a,b) were produced using  BertViz37.\n8\nVol:.(1234567890)Scientific Reports |         (2023) 13:3517  | https://doi.org/10.1038/s41598-023-30657-1\nwww.nature.com/scientificreports/\nDiscussion\nThis work explored and compared the potential of two popular transformers, BERT and XLNet, in the task of \npredicting 6-month mortality in cardiac patients at randomly chosen events recorded in their EHR. The hetero-\ngeneous electronic health record data were constructed into semi-structured multi-event time series to exploit \nthe temporal information. We achieved a higher recall with XLNet, suggesting that it captures more positive cases \nthan BERT. It has been argued that the learning strategy implemented in XLNet is better capable of capturing \nlong-term dependencies in  sequences14. To our knowledge, this is the first study exploring XLNet for mortality \nprediction from electronic health records.\nPrevious studies often set their focus on in-patient mortality within 24 h of admission, which can be especially \nbeneficial for applications at intensive care  units2. In contrast, patients with long-term conditions may profit from \nearlier predictions. The 6-month prediction period selected in this study allows time for clinicians to re-evaluate \nthe patient’s needs and make their care more effective to decrease their risk of death. It provides time for any \nadditional tests and diagnostics, as well as a realistic possibility for interventions to take effect. Six months was \nconsidered a suitable period to explore model performance in such a heterogeneous cardiac patient population.\nAs compared to a prior study using extreme gradient boosting (XGBoost) on the same database and predic-\ntion target, the presented results fall short of the previous AUC  result28. This may, however, be expected because \nthe prior study focused on a specific homogeneous patient group (with acute coronary syndrome) whereas the \ncurrent work with a larger portion of the database included a wide heterogeneous spectrum of CVD patients. \nMoreover, the more refined and smaller subset of data in the previous study allowed for features selected by expert \nclinicians, which may have further facilitated good performance but also increased manual work. Additionally, \nthis study used the anonymous database, which lead to more noise and gaps in the training data and only offered \ndates relative to a patient’s birth instead of real dates. Hence, the importance of the concurrent planning of the \nanalysis and anonymization is underscored. In this study, because the collection of study data was terminated \non a specified date without any follow-up, the data contained patients that were still in care or did not have a \nfull six months since their last event. These examples could not be filtered from the anonymous data as the real \ndates were no longer available and, thus, they may cause the model to be too optimistic about patient survival. \nThe missing real dates also prevented the analysis of time-dependent differences between patient timelines which \nmight exist due to, e.g., updates in care guidelines. Moreover, the sex of patient was largely missing although it \nis an important clinical factor affecting patient outcomes.\nEven though some transformers such as XLNet are in principle able to consume sequences of any length, \nthe models are still limited by the memory resources of the hardware used for training and visual output inter-\npretation. This poses a challenge for incorporating all different event types and their attributes from the patient \nhistory. Here, the 512 tokens representing the most recent events of the patient were used while the captured \ntime period varied. Formulating the EHR data as multi-event time series may facilitate the extraction of new \nknowledge concerning the role and relationships of different types of events. Future research may explore longer \ninput sequences with XLNet or alternative ways to incorporate multi-event information. For instance, replac-\ning code based attributes with full text descriptions may improve performance but would require longer input \nsequences to feed the model the equivalent portion of patient history. In the future, harmonization of hospital \ninformation management systems may additionally yield better grounds regarding the selection of attributes as \nthey are inherited from the hospital’s original system. Further improvement may be achieved by using tokenizers \nspecially trained on clinical data or pre-trained transformers. Here, due to the lack of such resources for XLNet, \nboth models were trained from scratch to facilitate a fair comparison. Notably, our results show that the standard \nEnglish tokenizers can produce promising learning results.\nAs demonstrated in this work, transformers provide a means to interpret individual outputs and the pre-\ndictions may therefore become a valuable part of the clinical workflow and answer to the requirements set for \nML models in CVD  predictions40. Nevertheless, intuitive and user-friendly output interpretation interfaces for \nclinicians need further development so that this capability can be properly harnessed. The resulting tools may \nbe efficiently integrated to the EHR system itself, although additional computing resources are likely required.\nConclusion\nUsing transformers to learn bi-directional dependencies in EHRs shows promise in mortality prediction, despite \nthe sparsity of the data. We compared BERT and XLNet for CVD patient mortality risk prediction from EHR \ndata. While prior research has focused on BERT for EHR applications, the results of this study suggest that future \nstudies may achieve improved results using XLNet. Similar models with actionable outputs, as presented here, \ncould improve patient outcomes with chronic diseases, such as CVDs, and be directly integrated to the EHR \nsystems for everyday clinical use.\nWe also observed that transformers may perform better in more refined patient groups. The wide spectrum \nof CVD patients in this study added complexity to the prediction problem, producing weaker performance \nas compared to conventional machine learning in a more refined patient group. Furthermore, more concise \nrepresentations have reached better learning results, whereas the multi-attribute multi-event representation \nfaces computational restrictions. Hence, in the future, improved results may be obtained via more sophisticated \nrepresentations, transfer learning from pre-trained models, or via improved computational power. As in the \npresented study, anonymous data will become an increasingly common basis for model development. In such \ncases, the performance of data-driven models may benefit from an improved anonymization process.\nData availibility\nThe anonymized data is available for scientific purpose upon reasonable request to J.H. (jussi.hernesniemi@\nsydansairaala.fi) pending the approval of the MADDEC study steering committee.\n9\nVol.:(0123456789)Scientific Reports |         (2023) 13:3517  | https://doi.org/10.1038/s41598-023-30657-1\nwww.nature.com/scientificreports/\nReceived: 7 July 2022; Accepted: 27 February 2023\nReferences\n 1. Kruse, C. S., Stein, A., Thomas, H. & Kaur, H. The use of electronic health records to support population health: A systematic \nreview of the literature. J. Med. Syst. 42, 214. https:// doi. org/ 10. 1007/ s10916- 018- 1075-6 (2018).\n 2. Si, Y . et al. Deep representation learning of patient data from electronic health records (EHR): A systematic review. J. Biomed. \nInform. 115, 103671. https:// doi. org/ 10. 1016/j. jbi. 2020. 103671 (2021).\n 3. Lähteenmäki, J., Pajula, J. & Antikainen, E. Development of medical applications based on AI models and register data-regulatory \nconsiderations. in Proceedings of the 18th Scandinavian Conference on Health Informatics. https:// doi. org/ 10. 3384/ ecp18 7024 (2022).\n 4. Roth, G. A. et al. Global, regional, and national age-sex-specific mortality for 282 causes of death in 195 countries and territories, \n1980–2017: A systematic analysis for the global burden of disease study 2017. Lancet  392, 1736–1788. https:// doi. org/ 10. 1016/ \nS0140- 6736(18) 32203-7 (2018).\n 5. Zhao, M. et al. Advances in the relationship between coronavirus infection and cardiovascular diseases. Biomed. Pharmacother.  \n127, 110230. https:// doi. org/ 10. 1016/j. biopha. 2020. 110230 (2020).\n 6. Quer, G., Arnaout, R., Henne, M. & Arnaout, R. Machine learning and the future of cardiovascular care. J. Am. Coll. Cardiol. 77, \n300–313. https:// doi. org/ 10. 1016/j. jacc. 2020. 11. 030 (2021).\n 7. Hemingway, H. et al. Big data from electronic health records for early and late translational cardiovascular research: Challenges \nand potential. Eur. Heart J. 39, 1481–1495. https:// doi. org/ 10. 1093/ eurhe artj/ ehx487 (2017).\n 8. Gao, S. et al. Limitations of transformers on clinical text classification. IEEE J. Biomed. Health Inform. 25, 3596–3607. https:// doi. \norg/ 10. 1109/ JBHI. 2021. 30623 22 (2021).\n 9. Li, Y . et al. BEHRT: Transformer for electronic health records. Sci. Rep. 10, 7155. https:// doi. org/ 10. 1038/ s41598- 020- 62922-y \n(2020).\n 10. Shang, J., Ma, T., Xiao, C. & Sun, J. Pre-training of graph augmented transformers for medication recommendation. in Proceedings \nof the 28th International Joint Conference on Artificial Intelligence, IJCAI 2019, IJCAI International Joint Conference on Artificial \nIntelligence. 5953–5959. https:// doi. org/ 10. 24963/ ijcai. 2019/ 825 (2019).\n 11. Rasmy, L., Xiang, Y ., Xie, Z., Tao, C. & Zhi, D. Med-BERT: Pretrained contextualized embeddings on large-scale structured elec-\ntronic health records for disease prediction. NPJ Digit. Med. 4, 86. https:// doi. org/ 10. 1038/ s41746- 021- 00455-y (2021).\n 12. Meng, Y ., Speier, W ., Ong, M. K. & Arnold, C. W . Bidirectional representation learning from transformers using multimodal \nelectronic health record data to predict depression. IEEE J. Biomed. Health Inform. 25, 3121–3129. https:// doi. org/ 10. 1109/ JBHI. \n2021. 30637 21 (2021).\n 13. Devlin, J., Chang, M.-W ., Lee, K. & Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understand-\ning. in Proceedings of NAACL-HLT 2019. 4171–4186. https:// doi. org/ 10. 48550/ arXiv. 1810. 04805 (2019).\n 14. Y ang, Z. et al. XLNet: Generalized autoregressive pretraining for language understanding. Adv. Neural Inf. Process. Syst.. https:// \ndoi. org/ 10. 48550/ arXiv. 1906. 08237 (2019).\n 15. Kiranyaz, S., Ince, T. & Gabbouj, M. Personalized monitoring and advance warning system for cardiac arrhythmias. Sci. Rep.  7, \n9270. https:// doi. org/ 10. 1038/ s41598- 017- 09544-z (2017).\n 16. Oh, S. L., Ng, E. Y ., Tan, R. S. & Acharya, U. R. Automated diagnosis of arrhythmia using combination of CNN and LSTM tech-\nniques with variable length heart beats. Comput. Biol. Med. 102, 278–287. https://  doi. org/ 10. 1016/j. compb iomed. 2018. 06. 002 \n(2018).\n 17. Litjens, G. et al. State-of-the-art deep learning in cardiovascular image analysis. JACC Cardiovasc. Imaging 12, 1549–1565. https:// \ndoi. org/ 10. 1016/j. jcmg. 2019. 06. 009 (2019).\n 18. Choi, E., Bahadori, M. T., Schuetz, A., Stewart, W . F . & Sun, J. Doctor AI: Predicting clinical events via recurrent neural networks. \nin Proceedings of the 1st Machine Learning for Healthcare Conference. 301–318. https:// doi. org/ 10. 48550/ arXiv. 1511. 05942 (PMLR, \nNortheastern University, 2016).\n 19. Bahdanau, D., Cho, K. & Bengio, Y . Neural machine translation by jointly learning to align and translate. in Proceedings of the 3rd \nInternational Conference on Learning Representations, ICLR. https:// doi. org/ 10. 48550/ arXiv. 1409. 0473 (2015).\n 20. Ayala Solares, J. R. et al. Deep learning for electronic health records: A comparative review of multiple deep neural architectures. \nJ. Biomed. Inform. 101, 103337. https:// doi. org/ 10. 1016/j. jbi. 2019. 103337 (2020).\n 21. Choi, E., Bahadori, M. T., Schuetz, A., Stewart, W . F . & Sun, J. RETAIN: Interpretable predictive model in healthcare using reverse \ntime attention mechanism. CoRR abs/1608.05745. https:// doi. org/ 10. 48550/ arXiv. 1608. 05745 (2016).\n 22. Rajkomar, A. et al. Scalable and accurate deep learning with electronic health records. NPJ Digit. Med.  1, 18. https:// doi. org/ 10. \n1038/ s41746- 018- 0029-1 (2018).\n 23. Pham, T., Tran, T., Phung, D. & Venkatesh, S. Predicting healthcare trajectories from medical records: A deep learning approach. \nJ. Biomed. Inform. 69, 218–229. https:// doi. org/ 10. 1016/j. jbi. 2017. 04. 001 (2017).\n 24. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. Syst. 5998–6008. https:// doi. org/ 10. 48550/ arXiv. 1706. 03762 \n(2017).\n 25. Choi, E. et al. Learning the graphical structure of electronic health records with graph convolutional transformer. in Proceedings \nof the AAAI Conference on Artificial Intelligence. Vol. 34. 606–613. https:// doi. org/ 10. 48550/ arXiv. 1906. 04716 (2020).\n 26. Ren, H., Wang, J., Zhao, W . X. & Wu, N. RAPT: Pre-Training of Time-Aware Transformer for Learning Robust Healthcare Representa-\ntion. 3503–3511. https:// doi. org/ 10. 1145/ 34475 48. 34670 69 (Association for Computing Machinery, 2021).\n 27. Kodialam, R. et al. Deep contextual clinical prediction with reverse distillation. in Proceedings of the Thirty-Fifth AAAI Conference \non Artificial Intelligence. https:// doi. org/ 10. 48550/ arXiv. 2007. 05611 (2021).\n 28. Hernesniemi, J. A. et al. Extensive phenotype data and machine learning in prediction of mortality in acute coronary syndrome—\nThe MADDEC study. Ann. Med. 51, 156–163. https:// doi. org/ 10. 1080/ 07853 890. 2019. 15963 02 (2019).\n 29. Tolonen, H. et al. The validation of the Finnish hospital discharge register and causes of death register data on stroke diagnoses. \nEur. J. Cardiovasc. Prevent. Rehabil. 14, 380–385. https:// doi. org/ 10. 1097/ 01. hjr. 00002 39466. 26132. f2 (2007).\n 30. Pajunen, P . et al. The validity of the Finnish hospital discharge register and causes of death register data on coronary heart disease. \nEur. J. Cardiovasc. Prevent. Rehabil. 12, 132–137. https:// doi. org/ 10. 1097/ 00149 831- 20050 4000- 00007 (2005).\n 31. Vuori, M. A. et al. The validity of heart failure diagnoses in the Finnish hospital discharge register. Scand. J. Public Health 48, 20–28. \nhttps:// doi. org/ 10. 1177/ 14034 94819 847051 (2020).\n 32. Hernesniemi, J. A. et al. Cohort description for MADDEC—Mass data in detection and prevention of serious adverse events in \ncardiovascular disease. in EMBEC & NBC 2017 (Eskola, H., Väisänen, O., Viik, J. & Hyttinen, J. eds.). 1113–1116. https:// doi. org/ \n10. 1007/ 978- 981- 10- 5122-7_ 278 (Springer Singapore, 2018).\n 33. Wolf, T. et al. Transformers: State-of-the-art natural language processing. in Proceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing: System Demonstrations. 38–45. https:// doi. org/ 10. 18653/ v1/ 2020. emnlp- demos.6. (Association \nfor Computational Linguistics, 2020).\n 34. Jaderberg, M. et al. Population based training of neural networks. CoRR abs/1711.09846. https:// doi. org/ 10. 48550/ arXiv. 1711. \n09846 (2017).\n10\nVol:.(1234567890)Scientific Reports |         (2023) 13:3517  | https://doi.org/10.1038/s41598-023-30657-1\nwww.nature.com/scientificreports/\n 35. Liaw, R. et al. Tune: A research platform for distributed model selection and training. arXiv preprint arXiv: 1807. 05118. https:// \ndoi. org/ 10. 48550/ arXiv. 1807. 05118 (2018).\n 36. Tohka, J. & van Gils, M. Evaluation of machine learning algorithms for health and wellness applications: A tutorial. Comput. Biol. \nMed. 132, 104324. https:// doi. org/ 10. 1016/j. compb iomed. 2021. 104324 (2021).\n 37. Vig, J. A multiscale visualization of attention in the transformer model. in Proceedings of the 57th Annual Meeting of the Association \nfor Computational Linguistics: System Demonstrations. 37–42 . https:// doi. org/ 10. 18653/ v1/ P19- 3007 (Association for Computa-\ntional Linguistics, 2019).\n 38. Abadi, M. et al. TensorFlow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv: 1603. 04467. \nSoftware available from tensorflow.org. https:// doi. org/ 10. 48550/ arXiv. 1603. 04467 (2016).\n 39. Pedregosa, F . et al. Scikit-learn: Machine learning in Python. J. Mach. Learn. Res. 12, 2825–2830 (2011).\n 40. van Smeden, M. et al. Critical appraisal of artificial intelligence-based prediction models for cardiovascular disease. Eur. Heart \nJ.https:// doi. org/ 10. 1093/ eurhe artj/ ehac2 38 (2022).\nAcknowledgements\nThis work was supported by Ministry of Social Affairs and Health, Finland, via the Data-driven identification of \nelderly individuals with future need for multi-sectoral services (MAITE) project and by Competitive State Research \nFinancing of the Expert Responsibility Area of Tampere University Hospital, and the Tampere University Hospital \nsupport association. The MADDEC project to which the research data is based on was funded by the Business \nFinland research funding (Grant 4197/31/2015).\nAuthor contributions\nE.A. and U.A. prepared the pre-acquired data for the study. E.A. and J.L. built, trained, and tested the models. \nE.A. performed model hyperparameter optimization. E.A., J.H., M.E., N.O., M.v.G., and M.G. designed the \nstudy and the data collection protocol. J.H. and M.G. contributed equally. E.A. drafted and all authors reviewed \nthe manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to E.A.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Health records",
  "concepts": [
    {
      "name": "Health records",
      "score": 0.6266343593597412
    },
    {
      "name": "Transformer",
      "score": 0.5796371698379517
    },
    {
      "name": "Recall",
      "score": 0.5593917965888977
    },
    {
      "name": "Receiver operating characteristic",
      "score": 0.5107953548431396
    },
    {
      "name": "Computer science",
      "score": 0.5021746158599854
    },
    {
      "name": "Electronic health record",
      "score": 0.48877856135368347
    },
    {
      "name": "Medicine",
      "score": 0.4580811858177185
    },
    {
      "name": "Deep learning",
      "score": 0.44085970520973206
    },
    {
      "name": "Cause of death",
      "score": 0.42726367712020874
    },
    {
      "name": "Medical emergency",
      "score": 0.42198118567466736
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4138333797454834
    },
    {
      "name": "Sudden cardiac death",
      "score": 0.4135597348213196
    },
    {
      "name": "Disease",
      "score": 0.3998764157295227
    },
    {
      "name": "Machine learning",
      "score": 0.3820507228374481
    },
    {
      "name": "Emergency medicine",
      "score": 0.3632698059082031
    },
    {
      "name": "Health care",
      "score": 0.32365185022354126
    },
    {
      "name": "Internal medicine",
      "score": 0.22798892855644226
    },
    {
      "name": "Engineering",
      "score": 0.1411493420600891
    },
    {
      "name": "Psychology",
      "score": 0.117134690284729
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Cognitive psychology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I87653560",
      "name": "VTT Technical Research Centre of Finland",
      "country": "FI"
    },
    {
      "id": "https://openalex.org/I166825849",
      "name": "Tampere University",
      "country": "FI"
    },
    {
      "id": "https://openalex.org/I4210126318",
      "name": "Tampere University Hospital",
      "country": "FI"
    }
  ]
}