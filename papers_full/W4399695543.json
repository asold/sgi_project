{
  "title": "Fredformer: Frequency Debiased Transformer for Time Series Forecasting",
  "url": "https://openalex.org/W4399695543",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Piao, Xihao",
      "affiliations": [
        "Osaka University",
        "Sanken Electric (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A2029461768",
      "name": "Chen Zheng",
      "affiliations": [
        "Osaka University",
        "Sanken Electric (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A3175047517",
      "name": "Murayama, Taichi",
      "affiliations": [
        "Sanken Electric (Japan)",
        "Osaka University"
      ]
    },
    {
      "id": "https://openalex.org/A2570557938",
      "name": "Matsubara, Yasuko",
      "affiliations": [
        "Sanken Electric (Japan)",
        "Osaka University"
      ]
    },
    {
      "id": "https://openalex.org/A2569477779",
      "name": "Sakurai, Yasushi",
      "affiliations": [
        "Osaka University",
        "Sanken Electric (Japan)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4232564811",
    "https://openalex.org/W4319777590",
    "https://openalex.org/W2090457102",
    "https://openalex.org/W3096307747",
    "https://openalex.org/W4249224151",
    "https://openalex.org/W2007221293",
    "https://openalex.org/W2604847698",
    "https://openalex.org/W4387561305",
    "https://openalex.org/W2313404644",
    "https://openalex.org/W3080157892",
    "https://openalex.org/W2914483840",
    "https://openalex.org/W4382203079",
    "https://openalex.org/W1826290430",
    "https://openalex.org/W2949582403",
    "https://openalex.org/W4306884390",
    "https://openalex.org/W3212890323",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4225871896"
  ],
  "abstract": "The Transformer model has shown leading performance in time series forecasting. Nevertheless, in some complex scenarios, it tends to learn low-frequency features in the data and overlook high-frequency features, showing a frequency bias. This bias prevents the model from accurately capturing important high-frequency data features. In this paper, we undertook empirical analyses to understand this bias and discovered that frequency bias results from the model disproportionately focusing on frequency features with higher energy. Based on our analysis, we formulate this bias and propose Fredformer, a Transformer-based framework designed to mitigate frequency bias by learning features equally across different frequency bands. This approach prevents the model from overlooking lower amplitude features important for accurate forecasting. Extensive experiments show the effectiveness of our proposed approach, which can outperform other baselines in different real-world time-series datasets. Furthermore, we introduce a lightweight variant of the Fredformer with an attention matrix approximation, which achieves comparable performance but with much fewer parameters and lower computation costs. The code is available at: https://github.com/chenzRG/Fredformer",
  "full_text": "Fredformer: Frequency Debiased Transformer for Time Series\nForecasting\nXihao Piao*\nSANKEN, Osaka University\nOsaka, Japan\npark88@sanken.osaka-u.ac.jp\nZheng Chen*\nSANKEN, Osaka University\nOsaka, Japan\nchenz@sanken.osaka-u.ac.jp\nTaichi Murayama\nSANKEN, Osaka University\nOsaka, Japan\ntaichi@sanken.osaka-u.ac.jp\nYasuko Matsubara\nSANKEN, Osaka University\nOsaka, Japan\nyasuko@sanken.osaka-u.ac.jp\nYasushi Sakurai\nSANKEN, Osaka University\nOsaka, Japan\nyasushi@sanken.osaka-u.ac.jp\nABSTRACT\nThe Transformer model has shown leading performance in time se-\nries forecasting. Nevertheless, in some complex scenarios, it tends\nto learn low-frequency features in the data and overlook high-\nfrequency features, showing a frequency bias. This bias prevents\nthe model from accurately capturing important high-frequency\ndata features. In this paper, we undertake empirical analyses to\nunderstand this bias and discover that frequency bias results from\nthe model disproportionately focusing on frequency features with\nhigher energy. Based on our analysis, we formulate this bias and\npropose Fredformer, a Transformer-based framework designed\nto mitigate frequency bias by learning features equally across dif-\nferent frequency bands. This approach prevents the model from\noverlooking lower amplitude features important for accurate fore-\ncasting. Extensive experiments show the effectiveness of our pro-\nposed approach, which can outperform other baselines in differ-\nent real-world time-series datasets. Furthermore, we introduce a\nlightweight variant of the Fredformer with an attention matrix\napproximation, which achieves comparable performance but with\nmuch fewer parameters and lower computation costs. The code is\navailable at: https://github.com/chenzRG/Fredformer\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Artificial intelligence; Neural\nnetworks.\nKEYWORDS\nTime series forecasting, Deep learning\nACM Reference Format:\nXihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Ya-\nsushi Sakurai. 2024. Fredformer: Frequency Debiased Transformer for\n* Indicates corresponding authors.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0490-1/24/08\nhttps://doi.org/10.1145/3637528.3671928\nTime Series Forecasting . In Proceedings of the 30th ACM SIGKDD Con-\nference on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“\n29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 18 pages. https:\n//doi.org/10.1145/3637528.3671928\n1 INTRODUCTION\nTime series data are ubiquitous in everyday life. Forecasting time\nseries could provide insights for decision-making support, such\nas potential traffic congestion [ 10] or changes in stock market\ntrends [34]. Accurate forecasting typically involves discerning vari-\nous informative temporal variations in historical observations, e.g.,\ntrends, seasonality, and fluctuations, which are consistent in fu-\nture time series [42]. Benefiting from the advancements in deep\nlearning, the community has seen great progress, particularly with\nTransformer-based methods [ 39, 41, 49]. Successful methods of-\nten tokenize time series with multiresolution, such as time points\n[43] or sub-series [48], and model their dependencies leveraging the\nself-attention mechanism . Several state-of-the-art (SOTA) baselines\nhave been proposed, namely PatchTST [29], Crossformer [48], and\niTransformer [24], and demonstrate impressive performance.\nDespite their success, the effectiveness with which we can cap-\nture informative temporal variations remains a concern. From a data\nperspective, a series of time observations is typically considered a\ncomplex set of signals or waves that varies over time [13, 17]. Vari-\nous temporal variations, manifested as different frequency waves,\nsuch as low-frequency long-term periodicity or high-frequency\nfluctuation, often co-occur and are intermixed in the real world\n[19, 21, 42]. While tokenizing a time series may provide fine-grained\ninformation for the model, the temporal variations in resulting\ntokens or sub-series are also entangled. This issue may complicate\nthe feature extraction and forecasting performance. Existing works\nhave proposed frequency decomposition to represent the time se-\nries and deployed Transformers on new representation to explicitly\nlearn eventful frequency features [42, 43]. Learning often incorpo-\nrates feature selection strategies in the frequency domain, such as\ntop-K or random-K [41, 50], to help Transformers better identify\nmore relevant frequencies. However, such heuristic selection may\nintroduce sub-optimal frequency correlations into the model (seen\nin Figure 1(a)), inadvertently misleading the learning process.\nFrom a model perspective, researchers have recently noticed a\nlearning bias issue that is common in the Transformer . That is, the self-\nattention mechanism often prioritizes low-frequency features at the\narXiv:2406.09009v4  [cs.LG]  3 Jul 2024\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai\n(b) PatchTST (c) Ours(a) FEDformer\nF \nF \nA \nF \nInput Ground Truth Forecasting\nFigure 1: In contrast to a frequency modeling-based work FEDformer [ 50] and a SOTA work PatchTST [ 29], our model can\naccurately capture more significant mid-to-high frequency components.\nexpense of high-frequency features [14, 30, 35, 36]. This subtle issue\nmay also appear in time series forecasting, potentially biasing model\noutcomes and leading to information losses. Figure 1(b) shows an\nelectricity case where the forecasting result successfully captures\nlow-frequency features, neglecting some consistent mid-to-high\nfrequencies. In practice, such high frequencies represent short-\nterm variations, e.g., periodicities over short durations, which serve\nas good indicators for forecasting [10, 16, 34]. However, the low-\nfrequencies typically carry a substantial portion of the energy in the\nspectrum and are dominant in time series. The amplitude of these\nlow-frequency components far exceeds that of higher frequencies\n[51], which provides the Transformer with more observations. This\nmay raise the possibility of frequency bias in time series forecasting,\nas the model might disproportionately learn from these dominant\nlow-frequency components.\nThis work explores one direction of capturing informative, com-\nplex variations by frequency domain modeling for accurate time se-\nries forecasting. We introduce Fredformer, a Frequency-debiased\nTransformer model. Fredformer follows the line of frequency\ndecomposition but further investigates how to facilitate the uses\nof Transformers in learning frequency features. To improve the\neffectiveness of our approach, we provide a comprehensive analy-\nsis of frequency bias in time series forecasting and a strategy for\ndebiasing it. Our main contributions lie in three folds.\n- Problem definition. We undertake empirical studies to investi-\ngate how this bias is introduced into time series forecasting Trans-\nformers. We observe that the main cause is the proportional dif-\nference between key frequency components. Notably, these key\ncomponents should be consistent in the historical and ground truth\nof the forecasting. We also investigate the objective and key designs\nthat affect debiasing.\n- Algorithmic design. Our Fredformer has three pivotal compo-\nnents: patching for the frequency band, sub-frequency-independent\nnormalization to mitigate proportional differences, and channel-\nwise attention within each sub-frequency band for fairness learning\nof all frequencies and attention debiasing.\n- Applicability. Fredformer undertakes NystrÃ¶m approximation\nto reduce the computational complexity of the attention maps, thus\nachieving a lightweight model with competitive performance. This\nopens new opportunities for efficient time series forecasting.\nRemark. This is the first paper to study the frequency bias issue\nin time series forecasting. Extensive experimental results on eight\ndatasets show the effectiveness of Fredformer, which achieves\nsuperior performance with 60 top-1 and 20 top-2 cases out of 80.\n2 PRELIMINARY ANALYSIS\nWe present two cases to show(i )how frequency attributes of time\nseries data introduce bias into forecasting with the Transformer\nmodel and (ii)an empirical analysis of the potential debiasing\nstrategy. This section introduces the notation and a metric for the\ncase studies in Sec. 2.1. The case analyses are detailed in Sec. 2.2.\n2.1 Preliminary\nTime Series Forecasting. Let X = {ğ’™ (ğ‘)\n1 ,..., ğ’™ (ğ‘)\nğ¿ }\nğ¶\nğ‘=1 denote\na multivariate time series consisting of ğ¶ channels, where each\nchannel records an independent ğ¿length look-back window. For\nsimplicity, we omit channel index ğ‘ in subsequent discussions. The\nforecasting task is to predict ğ» time steps in the future data Ë†X:\nË†Xğ¿+1:ğ¿+ğ» = ğ‘“(X1:ğ¿)\nwhere ğ‘“(Â·)denotes the forecasting function, which is a Transformer-\nbased model in this work. Our objective is to mitigate the learning\nbias in the Transformer and enhance the forecasting outcome Xâ€²,\nthat is, to minimize the error between Xâ€²and Ë†X.\nDiscrete Fourier Transform (DFT). We use DFT to analyze the\nfrequency content of X, Ë†X, and Xâ€². For example, given the input\nsequence {ğ’™1,..., ğ’™ğ¿}, the DFT can be formulated as\nğ’‚ğ‘˜ = 1\nğ¿\nğ¿âˆ‘ï¸\nğ‘™=1\nğ’™ğ‘™ Â·ğ‘“ğ‘˜, ğ‘˜ = 1,...,ğ¿\nwhere ğ‘“ğ‘˜ = ğ‘’âˆ’ğ‘–2ğœ‹ğ‘˜/ğ¿ denotes the ğ‘˜-th frequency component. The\nDFT coefficients A = {ğ’‚1,ğ’‚2,..., ğ’‚ğ¿}represent the amplitude in-\nformation of these frequencies. As illustrated in Figure 2 (b, left),\nfour components are observed to have higher amplitudes in the\nhistorical observations (X) and the forecasting data ( Ë†X). We refer\nFredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nRelative Error0.9 0.1Ground TruthInput\n Forecasting\n(b)\nTime domain model\n+ Non-normalization\nTime domain model\n+ Normalization\nFrequency domain \n+ Normalization\nF F F\n(a)#epoch\nk2k1\nk3 1 50 #epoch1 50\nk2k1\nk3\nF\nk2\nk1\nk3\nk2\nk1\nk3\nF\nFigure 2: Figure (a) shows the learning dynamics and results\nfor two synthetic datasets, employing line graphs to illus-\ntrate amplitudes in the frequency domain and heatmaps to\nrepresent training epoch errors. Figure (b) explores the in-\nfluence of amplitude and domain on learning by comparing\nTransformers in the time and frequency domains, both with\nand without frequency local normalization.\nto such consistent components as â€™key components â€™ (defined in Sec.\n3.1). Here, the inverse DFT (i.e., IDFT) is ğ’™ğ‘™ = Ãğ¿\nğ‘˜=1 ğ’‚ğ‘˜ Â·ğ‘“âˆ’1\nğ‘˜ , which\nreconstructs the time series data from the DFT coefficients.\nFrequency Bias Metric. Inspired by the work of [5, 46], this study\nemploys a Fourier analytic metric of relative error Î”ğ‘˜ to determine\nthe frequency bias. Given the model outputs Aâ€²and the ground\ntruth Ë†A, the mean-square error (MSE) for the ğ‘˜-th component is\ncalculated as follows: MSEğ‘˜ = |ğ’‚â€²ğ‘˜ âˆ’Ë†ğ’‚ğ‘˜|, where |Â·| denotes the L2\nnorm of a complex number. Then, the relative error is applied to\nmitigate scale differences. In other words, the error may become\nlarger as the proportion of amplitude increases.\nÎ”ğ‘˜ = |ğ’‚â€²ğ‘˜ âˆ’Ë†ğ’‚ğ‘˜|/|Ë†ğ’‚ğ‘˜|\nThis metric is used in case study analyses and the experiments\ndetailed in Section 5.2.\n2.2 Case Studies\nWe first generate single-channel time series data with a total length\nof 10000 timestamps and then employ a Transformer model [29]\nto forecast the data. The details are in Appendix A. For the first\ncase study (Case 1), we generate two datasets with three key fre-\nquency components ({ğ‘˜1,ğ‘˜2,ğ‘˜3}). Each dataset contains a different\nproportion of these three components, as illustrated in the DFT\nvisualization in Figure 2. On the left side of the figure, their ampli-\ntudes are arranged as ğ’‚ğ‘˜1 < ğ’‚ğ‘˜2 < ğ’‚ğ‘˜3, whereas on the right side,\nthe arrangement is ğ’‚ğ‘˜1 > ğ’‚ğ‘˜2 > ğ’‚ğ‘˜3. We maintain these propor-\ntions so that they are consistent between the observed A and the\nground truth Ë†A (i.e., Aâ‰ˆË†A). Then, we assess the bias for different\nğ‘˜ in the Transformer outputs Aâ€². Meanwhile, we track how Î”ğ‘˜\nchanges during the model training to show the learning bias, using\nheatmap values to represent the numerical values of Î”ğ‘˜.\nHere, we generate a dataset with four key frequency components\nfor the second case study (Case 2). This study analyzes different\nmodeling strategies to investigate their flexibility for debiasing.\n2.2.1 Investigating the Frequency Bias of Transformer (Case\n1). As shown in Figure 2(a) (left), after 50 epochs of training, the\nmodel successfully captures the amplitude of low-frequency com-\nponent ğ‘˜1 but fails to capture ğ‘˜2 and ğ‘˜3. Meanwhile, the heatmap\nvalues show that the model predominantly focuses on learning\nthe ğ‘˜1 component. In other words, the relative error decreases\nto around 0.01 (red codes) during the training. But, it lacks opti-\nmization for ğ‘˜3, resulting in a high relative error of almost 0.95.\nThese observations indicate that signals in the time domain can be\nrepresented by a series of frequency waves , typically dominated by\nlow-frequency components [19, 27, 31]. When the Transformer is\ndeployed on this mixed-frequency collection, the dominant propor-\ntion of frequencies experiences a learning bias. A similar result is\nalso evident in the control experiment in the right Subfigure. Here,\nwe introduce synthetic data with higher amplitudes in the mid and\nhigh-frequency ranges (resulting in ğ’‚ğ‘˜1 < ğ’‚ğ‘˜2 < ğ’‚ğ‘˜3). In response,\nthe model shifts its focus towards the key componentğ‘˜3, leading to\nÎ”ğ‘˜1 > Î”ğ‘˜2 > Î”ğ‘˜3. This learning bias aligns with recent theoretical\nanalyses of the Transformer model [30, 35, 36]. In addition, Sec. 3.1\nprovides a formal definition of this frequency bias.\n2.2.2 Debiasing the Frequency Learning for Transformer\n(Case 2).Based on the above discussion, we initially use the same\nexperimental settings for a new dataset, as shown in Figure 2(b)\n(Left). We then perform two feasibility analyses for debiasing by (1)\nmitigating the influence of high proportionality and (2) providing\nthe transformer with fine-grained frequency information.\n(1) Frequency normalization: We first decompose the frequency\ndomain and normalize the amplitudes of the frequencies to elimi-\nnate their proportional differences. Specifically, we apply the DFT,\nnormalize the amplitudes, and then use the IDFT to convert the fre-\nquency representation back into the time domain before inputting\nit into the Transformer, formulated as Xâ€²= (IDFT(Ağ‘›ğ‘œğ‘Ÿğ‘š)).\nAs depicted in Figure 2(b) (middle and right), the four input\ncomponents are adjusted so that they have the same amplitude\nvalue, shown by a blue dashed line. The middle subfigure shows\nthat frequency normalization enhances the forecasting performance\nfor the latter three frequencies, but relative errors remain high.\n(2) Frequency domain modeling: We further directly deploy the\nTransformer on the frequency domain to model the DFT matrix.\nSubsequently, we apply the IDFT to return the forecasting outcome\nto the time domain. Here, the purpose is to provide the transformer\nwith more refined and disentangled frequency features. Formally,\nXâ€² = IDFT((Ağ‘›ğ‘œğ‘Ÿğ‘š)). As shown in Figure 2(b) (right), there is a\nmarked improvement in forecasting accuracy for the latter three\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai\nfrequency components. Notably, the bias in the second frequency\ncomponent (60-75 Hz) is effectively eliminated. These findings sug-\ngest the potential for direct frequency domain modelingwith\nproportion mitigationin achieving the debiasing.\n3 FREQUENCY BIAS FORMULATION\nThis section defines the frequency bias in Sec.3.1, then describes\nthe research problem in Sec.3.2.\n3.1 Frequency Bias Definitions\nGiven the aforementioned empirical analyses, which demonstrate\nthat a frequency bias exists in key frequency components, we first\ndefine these key components in terms of two properties: 1) a key\ncomponent should have a relatively high amplitude within the\nspectrum, and 2) it should be consistent in historical observations\nand future time series, as well as robust to time shifts [4, 31].\nDefinition 1. Key Frequency Components.Given a frequency\nspectrum A with length ğ¿, A can be segmented into ğ‘ sub-frequency\nbands {ğ’˜1,ğ’˜2,..., ğ’˜ğ‘}by a sliding window, where ğ’˜ğ‘› âˆˆR1Ã—ğ‘ . The\nmaximum amplitude in the ğ‘›-th window is determined as follows:\nmax(ğ’˜ğ‘›)= max |ğ’‚ğ‘˜|: ğ’‚ğ‘˜ âˆˆğ’˜ğ‘› for ğ‘›= 1,2,...,ğ‘ (1)\nwhere ğ’˜ğ‘› denotes ğ‘  amplitudes in the ğ‘›-th window. If ğ’‚ğ‘˜ is a key\ncomponent in the ğ‘–-th window, then:\nğ’‚ğ‘˜ = max(ğ’˜ğ‘›)and Ë†ğ’‚ğ‘˜ = max(Ë†ğ’˜ğ‘›)\nËœA is a collection of all key components. Notably, ËœA should be present\nin historical A and ground truth Ë†A for accurate forecasting.\nDefinition 2. Frequency Bias in Transformer.Given that\na time series X contains ğ‘ key frequency components amplitudes\nËœA = {Ëœğ’‚1,..., Ëœğ’‚ğ‘}, for the ğ‘˜-th component Ëœğ’‚ğ‘˜ âˆˆËœA, we have ğ‘ƒ(Ëœğ’‚ğ‘˜)=\n|Ëœğ’‚ğ‘˜|Ãğ‘\nğ‘›=1 |Ëœğ’‚ğ‘›|, which refers to the proportion of Ëœğ’‚ğ‘˜ in the total sum of\namplitudes of ËœA. Frequency bias can be defined as relative error Î”ğ‘˜.\nHere, a larger proportion ğ‘ƒ(Ëœğ’‚ğ‘˜)leads to a smaller Î”ğ‘˜ and exhibits a\nhigher ranking:\nâˆ’|Î”ğ‘˜|âˆ ğ‘ƒ(Ëœğ’‚ğ‘˜) (2)\nEventually, the Transformer pays more attention to high-ranked\ncomponents during the training, as seen in Figure 2 (a) heatmaps.\n3.2 Problem Statement\nBased on the discussions in Sec. 2, we argue that if the Transformer\nassigns attention to all key frequency components ËœA equally during\nlearning, then the frequency bias could be mitigated.\nProblem 1. Debiasing Frequency Learning for Transformer.\nGiven a Transformer output ğ‘“ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘  (ğ‘‹), where ğ‘‹ contains several key\nfrequency component Ëœğ’‚ğ‘˜, our goal is to debias ğ‘“ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘  and improve\nperformance by making the relative error Î” Ëœğ’‚ğ‘˜ independent of ğ‘ƒ(Ëœğ’‚ğ‘˜):\nâˆ’|Î”ğ‘˜|Ì¸âˆ ğ‘ƒ(Ëœğ’‚ğ‘˜) (3)\nthereby ensuring a balanced response by the Transformer to different\nkey frequency components.\n4 FREDFORMER\nHere, we discuss how to tackle the problem formulated in Sec.3.2\nand proposeFredformer, aFrequency debiased Transformer model\nfor accurate time series forecasting.\nArchitecture Overview. Fredformer consists of four principal\ncomponents: (i )a DFT-to-IDFT backbone, (ii )frequency domain\nrefinement, (iii )local frequency independent learning, and (iv )\nglobal semantic frequency summarization. Figure 3 shows an ar-\nchitectural overview. The DFT-to-IDFT backbone breaks down the\ninput time series X into its frequency components using DFT and\nlearns a debiased representation of key frequency components by\nmodules (ii )( iii )and (iv ). Based on the discussion in Sec. 2.2.2\n(2), where we noted the significant potential of frequency modeling\nfor debiasing, we first refine the overall frequency spectrum into\nsub-frequencies, which we achieve through a patching operation\non DFT coefficients. Patches from different channels within the\nsame sub-frequency band are embedded as tokens. That is, each\nsub-frequency band is encoded independently, which avoids the\ninfluence of other frequency components, as discussed in Section\n2.2.2 (1). We deploy the Transformer to extract local frequency\nfeatures for each sub-band across all channels. This mitigates the\nhigher proportion crux defined in Def. 2. Finally, we summarize all\nthe frequency information, which serves as IDFT for forecasting.\nA detailed workflow of Fredformer is in Appendix C. Below, we\nprovide a description of each module.\n4.1 Backbone\nGiven X, we first use DFT to decompose X into frequency coeffi-\ncients A1 for all channels. We then extract the debiased frequency\nfeatures by using a Transformer encoder to A âˆˆRğ¶Ã—ğ¿. The fre-\nquency outputs are subsequently reconstructed to the time domain\nsignal Xâ€²by IDFT.\nXâ€²= IDFT(ğ‘“ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘  (A)), A = DFT(X))\n4.2 Frequency Refinement and Normalization\nFrom the observations described in Sec. 2.2.2, we conclude that if\nthere are significant proportional differences between different ËœAğ‘˜\nvalues in the input data, it will lead to the model overly focusing\non components with larger amplitudes. To address this issue, we\npropose frequency refinement and normalization. Specifically, a\nnon-overlapping patching operation is applied to A along the ğ¶-\naxis (i.e., channel), resulting in a sequence of local sub-frequencies\nas follows:\nW = {W1,W2,..., Wğ‘}= Patching(A), Wğ‘› âˆˆRğ¶Ã—ğ‘†\nwhere ğ‘ is the total number of the patches, while ğ‘† represents\nthe length of each patch. Mitigating information redundancy over\nfine-grained frequency bands, such as neighboring 1 Hz and 2 Hz,\nallows the model to learn the local features in each sub-frequency.\nParameter ğ‘†is adaptable to the requirements of real-world scenarios,\nfor example, an hourly sampling of daily recordings or the alpha\nwaveform typically occurring at 8-12 Hz [8].\n1A consists of two coefficient matrices: a real partR âˆˆRğ¶Ã—ğ¿ and an imaginary matrix\nI âˆˆRğ¶Ã—ğ¿. Since all operations are conducted synchronously for these two matrices,\nwe will refer to them as A in our subsequent discussions.\nFredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nDFT\nIDFT\nFrequency Domain\nModeling \nTransformer\nEncoder\nFrequency\nSummarizationFrequency\nChannel-Wise\nAttention\nNorm\nLinear Linear\nFigure 3: Overview of our framework. Fredformer employs\nDFT to transform input sequences into the frequency do-\nmain, normalizes locally, and segments into patches before\nemploying channel-wise attention, yielding final predictions\nthrough a frequency-wise summarizing layer and IDFT.\nSince patching operation allows the model to manage each Mğ‘›\nindependently, we further normalize each Wğ‘› along the ğ‘-axis:\nWâˆ—\nğ‘› = ğœ(Wğ‘›) ğ‘›= 1,2,...,ğ‘\nwhere ğœ(Â·)denotes the normalization, and it further projects the\nnumerical value of each ËœAğ‘˜ into a range of 0-1. This operation\neliminates proportionate differences in the maximum values within\nsub-frequency bands, thereby maintaining an equalÎ” across all key\ncomponents ËœA.\nLemma 1. Frequency-wise Local Normalization: Given fre-\nquency patches âˆ€Wğ‘›, Wğ‘š âˆˆW for max(Wğ‘›)> max(Wğ‘š)and\nğœ(Â·), the normalization strategy is defined by:\nWâˆ—= {ğœ(W1),...,ğœ (Wğ‘)}\nThis ensures that within each localized frequency patch Wğ‘›, the am-\nplitude differences between key frequency components are minimized,\npromoting equal attention to all key frequencies by the model:\nmax(Wâˆ—\nğ‘›)= max(Wâˆ—\nğ‘š)\nSome studies also introduce patching operations in the time do-\nmain and perform normalization within these time domain patches\n[29]. However, according to Parsevalâ€™s theorem [32], normalization\nwithin time domain patches is equivalent to normalizing across\nall frequencies. This could not address the issue of amplitude bias\namong key frequency components. A more detailed description can\nbe found in Appendix G.\n4.3 Frequency Local Independent Modeling\nGiven the normalized Wâˆ—, we deploy frequency local independent\nTransformer encoders to learn the importance of each Wâˆ—ğ‘› inde-\npendently. For W(1:ğ¶)\nğ‘› = {ğ’˜ (1)\nğ‘› ,ğ’˜ (2)\nğ‘› ,..., ğ’˜ (ğ¶)\nğ‘› }ğ‘\nğ‘›=1, a Transformer\nencoder ğ‘“ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘  (Â·)accepts each ğ’˜âˆ—(ğ‘)\nğ‘› as an input token:\n{Wâ€²(1:ğ¶)\nğ‘› }= ğ‘“ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘  (Wâˆ—(1:ğ¶)\nğ‘› )\nwhere Wâ€²(1:ğ¶)\nğ‘› is encoded by a channel-wise self-attention encoder:\nAttention(Qğ‘›,Kğ‘›,Vğ‘›)=\nSoftmax\n \nWâˆ—(1:ğ¶)\nğ‘› Wğ‘\nğ‘›(Wâˆ—(1:ğ¶)\nğ‘› Wğ‘˜ğ‘›)ğ‘‡\nâˆš\nğ‘‘\n!\nWâˆ—(1:ğ¶)\nğ‘› Wğ‘£\nğ‘›\nwhere Wğ‘\nğ‘›,Wğ‘˜ğ‘›,Wğ‘£ğ‘› âˆˆRğ‘†Ã—ğ‘€ are the weight matrices for generat-\ning the query matrix Qğ‘›, key matrix Kğ‘›, and value matrix Vğ‘›.\nâˆš\nğ‘‘\ndenotes a scaling operation. The attention module also includes\nnormalization and a feed-forward layer with residual connections\n[12], and Attention(Qğ‘›,Kğ‘›,Vğ‘›)âˆˆ Rğ¶Ã—ğ‘€ weights the correlations\namong ğ¶ channels for the ğ‘›-th sub-frequency band Mğ‘›. This de-\nsign ensures that the features of each sub-frequency are calculated\nindependently, preventing learning bias.\nLemma 2. Given Wâˆ—(1:ğ¶)\nğ‘› = {ğ’˜âˆ—(1)\nğ‘› ,ğ’˜âˆ—(2)\nğ‘› ,..., ğ’˜âˆ—(ğ¶)\nğ‘› }ğ‘\nğ‘›=1, if\nWâ€²ğ‘› = ğ‘“ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘  (Wâˆ—(1:ğ¶)\nğ‘› ), then by modeling the relationships of iden-\ntical frequencies ğ’˜ğ‘ğ‘› across different channels, for the ğ‘˜-th key com-\nponent Ëœğ’‚ğ‘˜ presents in ğ’˜ (ğ‘)\nğ‘› , we have âˆ’|Î”(ğ‘)\nğ‘˜ | âˆ {|Î”(ğ‘)\nğ‘˜ |}ğ¶\nğ‘=1. The\nTransformer encoders will focus on channel-wise correlations instead\nof the {|Î”(ğ‘)\nğ‘˜ |}ğ¾\nğ‘˜=1, i.e., debiasing âˆ’|Î”(ğ‘)\nğ‘˜ |Ì¸âˆ ğ‘ƒ(Ëœğ’‚ğ‘˜).\nLemma 2, which indicates a lowerğ‘ƒ(Ëœğ’‚ğ‘˜)does not necessarily lead\nto an increase in |Î” Ëœğ’‚ğ‘˜|, thus avoiding disproportionate attention to\nfrequency components. Channel-wise attention is proposed in the\nwork of [24, 48]. We include these studies as the baselines and the\nresults in Sec. 5.2. This work has different modeling purposes: we\ndeploy self-attention on the aligned local features, i.e., in the same\nfrequency bands across channels, for frequency debiasing.\n4.4 Frequency-wise Summarization\nGiven the learned features of the sub-frequenciesWâ€²= {ğ’˜â€²\n1,ğ’˜â€²\n2,..., ğ’˜â€²\nğ‘}\nof the historical time series X, the frequency-wise summarizing\noperation contains linear projections and IDFT:\nXâ€²= IDFT(Aâ€²) Aâ€²= Linear(Wâ€²)\nwhere Xâ€²âˆˆRğ¶Ã—ğ» is the final output of the framework.\n5 EXPERIMENTS\n5.1 PROTOCOLS\n- Datasets. We conduct extensive experiments on eight real-world\nbenchmark datasets: Weather, four ETT datasets (ETTh1, ETTh2,\nETTm1, ETTm2), Electricity (ECL), Traffic, and the Solar-Energy\ndataset [19], with all datasets being published in [24]2. The infor-\nmation these datasets provide is summarized in Table 3. And the\nfull results of four selected datasets* will be shown in Figure 2. and\nfurther details are available in Appendix B.\n- Baselines. We select 11 SOTA baseline studies. Since we are focus-\ning on Transformer, we first add seven proposed Transformer-based\nbaselines, including iTransformer [24], PatchTST [29], Crossformer\n[48], Stationary [25], Fedformer [50], Pyraformer [23], Autoformer\n2https://github.com/thuml/iTransformer\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai\nTable 1: Multivariate forecasting results with prediction lengths ğ‘† âˆˆ{96,192,336,720}for all datasets and fixed look-back length\nğ‘‡ = 96. The best and second best results are highlighted. The full results of four selected datasets* will be shown in Figure 2.\nResults are averaged from all prediction lengths. Full results for all datasets are listed in Appendix I.\nModels FredformeriTransformerRLinear PatchTSTCrossformer TiDE TimesNet DLinear SCINet FEDformerStationaryAutoformer\n(Ours) ([24]) [22] [29] [48] [11] [42] [47] [26] [50] [25] [43]\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nECL 0.1750.269 0.178 0.270 0.2190.298 0.2160.304 0.244 0.334 0.2510.344 0.1920.295 0.2120.300 0.2680.365 0.2140.327 0.1930.296 0.227 0.338\nETTh1 0.4350.426 0.454 0.447 0.4460.434 0.4690.454 0.529 0.522 0.5410.507 0.5480.450 0.4560.452 0.7470.647 0.4400.460 0.5700.537 0.496 0.487\nETTh2* 0.3650.393 0.383 0.407 0.3740.398 0.3870.407 0.942 0.684 0.6110.550 0.4140.427 0.5590.515 0.9540.723 0.4370.449 0.5260.516 0.450 0.459\nETTm1* 0.3840.395 0.407 0.410 0.4140.407 0.3870.400 0.513 0.496 0.4190.419 0.4000.406 0.4030.407 0.4850.481 0.4480.452 0.4810.456 0.588 0.517\nETTm2 0.2790.324 0.288 0.332 0.2860.327 0.2810.326 0.757 0.610 0.3580.404 0.2910.333 0.3500.401 0.5710.537 0.3050.349 0.3060.347 0.327 0.371\nTraffic 0.431 0.287 0.4280.2820.6260.378 0.5550.362 0.550 0.304 0.7600.473 0.6200.336 0.6250.383 0.8040.509 0.6100.376 0.6240.340 0.628 0.379\nWeather* 0.2460.272 0.258 0.279 0.2720.291 0.2590.281 0.259 0.315 0.2710.320 0.2590.287 0.2650.317 0.2920.363 0.3090.360 0.2880.314 0.338 0.382\nSolar-Energy* 0.2260.261 0.233 0.262 0.3690.356 0.2700.307 0.641 0.639 0.3470.417 0.3010.319 0.3300.401 0.2820.375 0.2910.381 0.2610.381 0.885 0.711\nTable 2: Full results of four selected datasets, with the best and second best results are highlighted. We compare extensive\ncompetitive models under different prediction lengths following the setting of iTransformer [ 24]. The input sequence length is\nset to 96 for all baselines. Avg means the average results from all four prediction lengths.\nModels FredformeriTransformerRLinear PatchTSTCrossformer TiDE TimesNet DLinear SCINet FEDformerStationaryAutoformer\n(Ours) [2024] [2023] [2023] [2023] [2023] [2023] [2023] [2022] [2022] [2022a] [2021]\nMetric MSE MAEMSE MAE MSEMAEMSEMAEMSE MAE MSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSE MAE\nETTh2\n96 0.293 0.3420.297 0.349 0.2880.3380.3020.3480.7450.5840.4000.4400.3400.3740.3330.3870.7070.6210.3580.3970.4760.4580.3460.388\n192 0.3710.3890.380 0.400 0.3740.3900.3880.4000.8770.6560.5280.5090.4020.4140.4770.4760.8600.6890.4290.4390.5120.4930.4560.452\n336 0.3820.4090.428 0.432 0.4150.4260.4260.4331.0430.7310.6430.5710.4520.4520.5940.5411.0000.7440.4960.4870.5520.5510.4820.486\n720 0.4150.4340.427 0.445 0.4200.4400.4310.4461.1040.7630.8740.6790.4620.4680.8310.6571.2490.8380.4630.4740.5620.5600.5150.511\nAvg 0.3650.3930.383 0.407 0.3740.3980.3870.4070.9420.6840.6110.5500.4140.4270.5590.5150.9540.7230.4370.4490.5260.5160.4500.459\nETTm1\n96 0.3260.3610.334 0.368 0.3550.3760.3290.3670.4040.4260.3640.3870.3380.3750.3450.3720.4180.4380.3790.4190.3860.3980.5050.475\n192 0.3630.3800.377 0.391 0.3910.3920.3670.3850.4500.4510.3980.4040.3740.3870.3800.3890.4390.4500.4260.4410.4590.4440.5530.496\n336 0.3950.4030.426 0.420 0.4240.4150.3990.4100.5320.5150.4280.4250.4100.4110.4130.4130.4900.4850.4450.4590.4950.4640.6210.537\n720 0.4530.4380.491 0.459 0.4870.4500.4540.4390.6660.5890.4870.4610.4780.4500.4740.4530.5950.5500.5430.4900.5850.5160.6710.561\nAvg 0.3840.3950.407 0.410 0.4140.4070.3870.4000.5130.4960.4190.4190.4000.4060.4030.4070.4850.4810.4480.4520.4810.4560.5880.517\nWeather\n96 0.1630.2070.174 0.214 0.1920.2320.1770.2180.1580.2300.2020.2610.1720.2200.1960.2550.2210.3060.2170.2960.1730.2230.2660.336\n192 0.2110.2510.221 0.254 0.2400.2710.2250.2590.2060.2770.2420.2980.2190.2610.2370.2960.2610.3400.2760.3360.2450.2850.3070.367\n336 0.2670.2920.278 0.296 0.2920.3070.2780.2970.2720.3350.2870.3350.2800.3060.2830.3350.3090.3780.3390.3800.3210.3380.3590.395\n720 0.3430.3410.358 0.349 0.3640.3530.3540.3480.3980.4180.3510.3860.3650.3590.3450.3810.3770.4270.4030.4280.4140.4100.4190.428\nAvg 0.2460.2720.258 0.279 0.2720.2910.2590.2810.2590.3150.2710.3200.2590.2870.2650.3170.2920.3630.3090.3600.2880.3140.3380.382\nSolar-Energy\n96 0.1850.2330.203 0.237 0.3220.3390.2340.2860.3100.3310.3120.3990.2500.2920.2900.3780.2370.3440.2420.3420.2150.2490.8840.711\n192 0.2270.2530.233 0.261 0.3590.3560.2670.3100.7340.7250.3390.4160.2960.3180.3200.3980.2800.3800.2850.3800.2540.2720.8340.692\n336 0.2460.2840.248 0.273 0.3970.3690.2900.3150.7500.7350.3680.4300.3190.3300.3530.4150.3040.3890.2820.3760.2900.2960.9410.723\n720 0.2470.2760.249 0.275 0.3970.3560.2890.3170.7690.7650.3700.4250.3380.3370.3560.4130.3080.3880.3570.4270.2850.2950.8820.717\nAvg 0.2260.2610.233 0.262 0.3690.3560.2700.3070.6410.6390.3470.4170.3010.3190.3300.4010.2820.3750.2910.3810.2610.3810.8850.711\n1st Count 17 17 0 2 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nTable 3: Benchmark dataset summary\nDatasetsWeather Electricity ETTh1 ETTh2 ETTm1 ETTm2 Solar Traffic#Channel 21 321 7 7 7 7 137 862#Timesteps52969 26304 17420 17420 69680 69680 52179 17544\n[43]. We also add 2 MLP-based and 2 TCN-based methods, including\nRLinear [22], DLinear [47], TiDE [11], TimesNet [42].\nFredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nTable 4: The average forecasting accuracy (MSE) on ETTh1\ndataset under 4 patch length settings.\nPatch length 8 16 32 Non\nMSE 0.417 0.425 0.440 0.449\nTable 5: Averaged results for each setting in the ablation\nstudy. \"No-CW\" refers to removing channel-wise attention,\nand \"No-FR\" refers to removing frequency refinement.\nSetting Full No-CW No-FR\nMSE MAE MSE MAE MSE MAE\nETTm1 0.384 0.396 0.418 0.419 0.539 0.485\nWeather 0.246 0.273 0.262 0.290 0.293 0.322\n- Setup and Evaluation. All baselines use the same prediction\nlength with ğ» âˆˆ{96,192,336,720}for all datasets. The look-back\nwindow ğ¿= 96 was used in our setting for fair comparisons, refer-\nring to [24, 50]. We used MSE and MAE as the forecasting metrics.\nWe further analyzed the forecasting results between the model\noutputs and the ground truth in the time and frequency domains.\nUsing heatmaps, we tracked the way in which Î”ğ‘˜ changes during\ntraining to show the debiased results of Fredformer compared\nwith various SOTA baselines.\n5.2 Results\nForecasting Results. Table 1 shows the average forecasting per-\nformance across four prediction lengths. The best results are high-\nlighted in red, and the second-best in blue. With a default look-back\nwindow of ğ¿= 96, our approach realizes leading performance levels\non most datasets, securing 14 top-1 and 2 top-2 positions across\ntwo metrics over eight datasets. More detailed results for 4 of the\neight datasets are shown in Table 2, where our method achieves 34\ntop-1 and 6 top-2 rankings out of 40 possible outcomes across the\nfour prediction lengths. More comprehensive results regarding the\ndifferent prediction length settings on all datasets and the impact of\nextending the look-back window are detailed in Appendix D and I.\nFrequency Bias Evaluation. Figure 4 is a case study visualization\nin the frequency domain, i.e., the DFT plot. The input, forecast out-\nput, and ground truth data series are shown in blue, red, and green,\nrespectively. Similar to Section 2.2, the heat map shows the relative\nerror for four selected mid-to-high frequency components over\nincreasing epochs. After training, Fredformer accurately identifies\nğ‘˜1, ğ‘˜2, and ğ‘˜3, with uniformly decreasing relative errors. Despite\na larger learning error for ğ‘˜4, Î”ğ‘˜4 consistently diminishes. This\nperformance contrasts with all the baselines, demonstrating a lack\nof effectiveness in capturing these frequency components, with\nunequal reductions in relative errors. In contrast, PatchTST demon-\nstrates a sudden improvement in component accuracy (ğ‘˜2,ğ‘˜3) dur-\ning the final stages of training. FEDformer fails to capture these\nfrequency components, possibly because its strategy of selecting\nand learning weights for only a random set of ğ‘˜ components over-\nlooks all unselected components. Notably, iTransformer overlooks\nmid-to-high frequency features, partially learning components ğ‘˜1\nMethod FEDformer PatchTST Crossformer\niTransformer Ours Ours*(NystrÃ¶m)\nComplexity ğ‘‚(ğ¿ğ¶) ğ‘‚\n\u0010\nğ¿2\nğ‘ƒ2 ğ¶\n\u0011\nğ‘‚\n\u0010\nğ¿2\nğ‘ƒ2 ğ¶\n\u0011\nğ‘‚(ğ¶2) ğ‘‚\n\u0010\nğ¿\nğ‘ƒğ¶2\n\u0011\nğ‘‚\n\u0010\nğ¿\nğ‘ƒğ¶\n\u0011\nTable 6: The theoretical computational complexity of\nTransformer-based methods.\nand ğ‘˜3 while ignoring ğ‘˜2 and ğ‘˜4, indicating a clear frequency bias.\nThis may stem from its use of channel-wise attention alongside\nglobal normalization in the time domain, as discussed in Lemma 1\nand further supported by our ablation study 5.3. This highlights\nthe effectiveness of frequency refinement and normalization.\n5.3 Ablation Study\nChannel-wise Attention and Frequency Refinement. We eval-\nuate the effectiveness of channel-wise attention and frequency\nrefinement. To this end, we remove each component by ablation\nand compare it with the original Fredformer. Table 5 shows that\nour method consistently outperforms others in all experiments,\nhighlighting the importance of integrating channel-wise attention\nwith frequency local normalization in our design. Interestingly, em-\nploying frequency local normalization alone yields better accuracy\nthan channel-wise attention alone. This suggests that minimizing\nproportional differences in amplitudes across various key frequency\ncomponents is crucial for enhancing accuracy.\nEffect of Patch Length. This ablation evaluates the impact of\npatch length using the ETTh1 dataset. We conduct four experi-\nments with ğ‘† = [8,16,32,48]patch lengths and corresponding\npatch numbers ğ‘ = [6,3,2,1]. In this context, ğ‘ = 1 means fre-\nquency normalization and channel-wise attention are applied to the\nentire spectrum without a patching operation. Table 4 shows the\nforecasting accuracy for each setting. As the patch length increases,\nthe granularity of the frequency features extracted by the model\nbecomes coarser, decreasing forecasting accuracy.\n5.4 Discussion of Applicability\nBeyond algorithmic considerations, we further discuss the practi-\ncal deployment of Fredformer in real-world scenarios, with the\nprimary challenge being memory consumption during model train-\ning. The ğ‘‚(ğ‘›2)complexity of self-attention limits the use of longer\nhistorical time series for forecasting, generating the need for in-\nnovations to reduce computational demands [21, 29, 49]. Through\npatching operations, we decrease the complexity from ğ‘‚(ğ¿ğ¶2)to\nğ‘‚(ğ¿\nğ‘ƒğ¶2). However, our channel-wise attention increases the com-\nputational costs with the number of channels, potentially limiting\npractical applicability with many channels. To address this, we pro-\npose a lightweight Fredformer, inspired by NystrÃ¶mFormer [45],\nwhich applies a matrix approximation to the attention map. This de-\nsign allows us to further reduce our complexity to ğ‘‚(ğ¿\nğ‘ƒğ¶)without\nthe need to modify the feature extraction (attention computation)\nor the data stream structure within the Transformer, unlike with\nprevious methods [23, 43, 49, 50]. Figure 5 shows a tradeoff between\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai\nFEDformer\n#epoch1 50\nF65 1150\nk1\nk2\nk3 k4\nk1\nk2\nk3\nk4\nPatchTST\n#epoch1 50\nF65 1150\nk1\nk2\nk3 k4\nk1\nk2\nk3\nk4\niTransformer\n#epoch1 50\nk1\nk2\nk3\nk4\nF65 1150\nk1\nk2\nk3 k4\nOurs (Fredformer)\nk1\nk2\nk3k4\n#epoch1 50\n650\n F115\nk1\nk2\nk3 k4\nInput Ground Truth Forecasting Relative Error\n0.9 0.1\nFigure 4: Visualizations of the learning dynamics and results for Fredformer and baselines on the ETTh1 dataset, employing\nline graphs to illustrate amplitudes in the frequency domain and heatmaps to represent training epoch errors.\nECLETTh1\nMSE MSE\nMb Mb\niTransformer\nPatchTST\nFedformer\nOurs\nCrossformer\nOurs*\niTransformer\nPatchTST\nCrossformer\nFedformer\nOurs\nFigure 5: This figure compares prediction accuracy and com-\nputational complexity (VRAM usage) among Transformer-\nbased methods, Fredformer (Ours), and its optimized variant,\nNystrÃ¶m-Fredformer (Ours*).\nthe model efficiency (VRAM usage) and accuracy in our method and\nthe baselines. The plain Fredformer achieves high accuracy with\nlow computational costs with fewer channels, such as ETTh1 with\n7 channels. However, as shown in the ECL dataset (321 channels),\nthe computational costs increase while maintaining high accuracy\nas the channel number increases. Here, NystrÃ¶m-Fredformer fur-\nther reduces computational requirements without compromising\naccuracy (the right sub-figure), showing that our model can realize\ncomputational efficiency and forecasting accuracy. Further details\nand derivations are provided in Appendix H.\n6 RELATED WORKS\nTransformer for Time Series Forecasting. Forecasting is impor-\ntant in time series analysis [1, 15]. Transformer has significantly\nprogressed in time series forecasting[18, 29, 48]. Earlier attempts\nfocused on improving the computational efficiency of Transform-\ners for time series forecasting tasks[3, 23, 49]. Several studies have\nused Transformers to model inherent temporal dependencies in\nthe time domain of time series[21, 23, 24, 29, 49]. Various studies\nhave integrated frequency decomposition and spectrum analysis\nwith the Transformer in modeling temporal variations [41, 43] to\nimprove the capacity for temporal-spatial representation. In [50],\nattention layers are designed that directly function in the frequency\ndomain to enhance spatial or frequency representation.\nModeling Short-Term Variation in Time Series. Short-term\nvariations are intrinsic characteristics of time series data and play\na crucial role in effective forecasting [ 10, 25]. Numerous deep\nlearning-based methods have been proposed to capture these tran-\nsient patterns [2, 7, 9, 28, 33, 37, 38, 40, 43]. Here, we summarize\nsome studies closely aligned with our proposed method. Pyraformer\n[23] applies a pyramidal attention module with inter-scale and intra-\nscale connections to capture various temporal dependencies. FED-\nformer [50] incorporates a Fourier spectrum within the attention\ncomputation to identify pivotal frequency components. Beyond\nTransformers, TimesNet [42] employs Inception blocks to capture\nintra-period and inter-period variations.\nChannel-wise Correlation. Understanding the cross-channel cor-\nrelation is also critical for time series forecasting. Several studies\naimed to capture intra-channel temporal variations and model the\ninter-channel correlations using Graph Neural Networks (GNNs)\n[6, 44]. Recently, Crossformer [ 48] and iTransformer [ 24] both\nadopted channel-wise Transformer-based frameworks, and exten-\nsive experimental results have demonstrated the effectiveness of\nchannel-wise attention for time series forecasting.\n7 CONCLUSION\nIn this paper, we first empirically analyzed frequency bias, delving\ninto its causes and exploring debiasing strategies. We then provided\na formulation of this bias based on our analytical insights. We pro-\nposed the Fredformer framework with three critical designs to\ntackle this bias and thus ensure unbiased learning across frequency\nbands. Our extensive experiments across eight datasets confirmed\nthe excellent performance of our proposed method. Visual anal-\nysis confirmed that our approach effectively mitigates frequency\nbias. The model analysis further illustrated how our designs aid\nfrequency debiasing and offered preliminary guidelines for future\nmodel design. Additionally, a lightweight variant of our model ad-\ndresses computational efficiency, facilitating practical application.\nFredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\n8 ACKNOWLEDGMENTS\nWe thank anonymous reviewers for their insightful comments\nand discussions. This work is supported by JSPS KAKENHI Grant-\nin-Aid for Scientific Research Number JP21H03446, JP23K16889,\nJP24K20778, NICT JPJ012368C03501, JST-AIP JPMJCR21U4, JST-\nCREST JPMJCR23M3, JST-RISTEX JPMJRS23L4.\nREFERENCES\n[1] Rob J. Hyndman Alysha M. De Livera and Ralph D. Snyder. 2011. Forecasting\nTime Series With Complex Seasonal Patterns Using Exponential Smoothing. J.\nAmer. Statist. Assoc. (2011), 1513â€“1527.\n[2] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2018. Convolutional Sequence\nModeling Revisited. (2018).\n[3] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-\nDocument Transformer. (2020). arXiv:2004.05150 [cs.CL]\n[4] S.A. Broughton and K. Bryan. 2011. Discrete Fourier Analysis and Wavelets:\nApplications to Signal and Image Processing. (2011).\n[5] Daniela Calvetti. 1991. A Stochastic Roundoff Error Analysis for the Fast Fourier\nTransform. (1991).\n[6] Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Conguri Huang,\nYunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, and Qi Zhang. 2021. Spectral Tem-\nporal Graph Neural Network for Multivariate Time-series Forecasting. (2021).\n[7] Yen-Yu Chang, Fan-Yun Sun, Yueh-Hua Wu, and Shou-De Lin. 2018. A Memory-\nNetwork Based Solution for Multivariate Time-Series Forecasting. (2018).\narXiv:1809.02105 [cs.LG]\n[8] Zheng Chen, Ziwei Yang, Lingwei Zhu, Wei Chen, Toshiyo Tamura, Naoaki\nOno, Md Altaf-Ul-Amin, Shigehiko Kanaya, and Ming Huang. 2023. Automated\nSleep Staging via Parallel Frequency-Cut Attention. IEEE Transactions on Neural\nSystems and Rehabilitation Engineering (2023), 1974â€“1985.\n[9] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.\nEmpirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.\n(2014). arXiv:1412.3555 [cs.NE]\n[10] Jesus Crespo Cuaresma, Jaroslava Hlouskova, Stephan Kossmeier, and Michael\nObersteiner. 2004. Forecasting Electricity Spot-Prices Using Linear Univariate\nTime-Series Models. Applied Energy 77 (2004), 87â€“106.\n[11] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan K Mathur, Rajat Sen, and\nRose Yu. 2023. Long-term Forecasting with TiDE: Time-series Dense Encoder.\nTransactions on Machine Learning Research (2023).\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is\nWorth 16x16 Words: Transformers for Image Recognition at Scale. InInternational\nConference on Learning Representations .\n[13] Filip Elvander and Andreas Jakobsson. 2020. Defining Fundamental Frequency\nfor Almost Harmonic Signals. IEEE TRANSACTIONS ON SIGNAL PROCESSING\n(2020).\n[14] Xiaojun Guo, Yifei Wang, Tianqi Du, and Yisen Wang. 2023. ContraNorm: A\nContrastive Learning Perspective on Oversmoothing and Beyond. (2023).\n[15] James D Hamilton. 2020. Time series analysis. (2020).\n[16] Nicholas W. Hammond, FranÃ§ois Birgand, Cayelan C. Carey, Bethany Bookout,\nAdrienne Breef-Pilz, and Madeline E. Schreiber. 2023. High-frequency Sensor Data\nCapture Short-term Variability In Fe and Mn Concentrations Due to Hypolimnetic\nOxygenation and Seasonal Dynamics in a Drinking Water Reservoir. Water\nResearch 240 (2023).\n[17] Long Steven R. Wu Manli C. Shih Hsing H. Zheng Quanan Yen Nai-Chyuan\nTung Chi Chao Huang Norden E. Shen Zheng and Liu Henry H. 1998. The\nEmpirical Mode Decomposition and the Hilbert Spectrum for Nonlinear and\nNon-stationary Time Series Analysis. Proceedings of the Royal Society of London.\nSeries A: mathematical, physical, and engineering sciences (1998), 903â€“995.\n[18] Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, and Jingyuan Wang. 2023.\nPDFormer: Propagation Delay-aware Dynamic Long-range Transformer for Traf-\nfic Flow Prediction. (2023), 4365â€“4373.\n[19] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Modeling\nLong- and Short-Term Temporal Patterns with Deep Neural Networks. (2018),\n95â€“104.\n[20] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. 2018. Modeling\nLong- and Short-Term Temporal Patterns with Deep Neural Networks. (2018),\n95â€“104.\n[21] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,\nand Xifeng Yan. 2019. Enhancing the Locality and Breaking the Memory Bottle-\nneck of Transformer on Time Series Forecasting. (2019).\n[22] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. 2023. Revisiting Long-term Time\nSeries Forecasting: An Investigation on Linear Mapping. (2023).\n[23] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and\nSchahram Dustdar. 2022. Pyraformer: Low-Complexity Pyramidal Attention for\nLong-Range Time Series Modeling and Forecasting. (2022).\n[24] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and\nMingsheng Long. 2024. iTransformer: Inverted Transformers Are Effective for\nTime Series Forecasting. In The Twelfth International Conference on Learning\nRepresentations.\n[25] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2022. Non-stationary\nTransformers: Exploring the Stationarity in Time Series Forecasting. (2022).\n[26] Liu M., Zeng A., Chen M., Xu Z., Lai Q., Ma L., and Q. Xu. 2022. SCINet: Time\nSeries Modeling and Forecasting with Sample Convolution and Interaction. (2022),\n5816â€“5828.\n[27] Sobhan Moosavi, Mohammad Hossein Samavatian, Arnab Nandi, Srinivasan\nParthasarathy, and Rajiv Ramnath. 2019. Short and Long-Term Pattern Discovery\nOver Large-Scale Geo-Spatiotemporal Data. (2019), 2905â€“2913.\n[28] Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. 2016. Phased LSTM: Acceler-\nating Recurrent Network Training for Long or Event-based Sequences. (2016).\narXiv:1610.09513 [cs.LG]\n[29] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2023.\nA Time Series is Worth 64 Words: Long-term Forecasting with Transformers.\n(2023).\n[30] Namuk Park and Songkuk Kim. 2022. How Do Vision Transformers Work?\n(2022).\n[31] John G. Proakis and Dimitris G. Manolakis. 1996. Digital Signal Processing (3rd\nEd.): Principles, Algorithms, and Applications. (1996).\n[32] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred\nHamprecht, Yoshua Bengio, and Aaron Courville. 2019. On the Spectral Bias of\nNeural Networks. 97 (2019), 5301â€“5310.\n[33] Daniel Stoller, Mi Tian, Sebastian Ewert, and Simon Dixon. 2019. Seq-U-Net:\nA One-Dimensional Causal U-Net for Efficient Sequence Modelling. (2019).\narXiv:1911.06393 [cs.LG]\n[34] James R. Thompson and James R. Wilson. 2016. Multifractal Detrended Fluctua-\ntion Analysis: Practical Applications to Financial Time Series. Mathematics and\nComputers in Simulation 126 (2016), 63â€“88.\n[35] Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du. 2023. Scan and Snap:\nUnderstanding Training Dynamics and Token Composition in 1-layer Trans-\nformer. (2023).\n[36] Peihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. 2022. Anti-\nOversmoothing in Deep Vision Transformers via the Fourier Domain Analysis:\nFrom Theory to Practice. (2022).\n[37] Zhiyuan Wang, Xovee Xu, Weifeng Zhang, Goce Trajcevski, Ting Zhong, and\nFan Zhou. 2022. Learning Latent Seasonal-Trend Representations for Time Series\nForecasting. (2022).\n[38] Qingsong Wen, Zhe Zhang, Yan Li, and Liang Sun. 2020. Fast RobustSTL: Effi-\ncient and Robust Seasonal-Trend Decomposition for Time Series with Complex\nPatterns. (2020), 2203â€“2213.\n[39] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan,\nand Liang Sun. 2023. Transformers in Time Series: A Survey. (2023).\n[40] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. 2022.\nCoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for\nTime Series Forecasting. (2022).\n[41] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H.\nHoi. 2022. ETSformer: Exponential Smoothing Transformers for Time-series\nForecasting. (2022).\n[42] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng\nLong. 2023. TimesNet: Temporal 2D-Variation Modeling for General Time Series\nAnalysis. (2023).\n[43] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer:\nDecomposition Transformers with Auto-Correlation for Long-Term Series Fore-\ncasting. (2021).\n[44] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi\nZhang. 2020. Connecting the Dots: Multivariate Time Series Forecasting with\nGraph Neural Networks. (2020).\n[45] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn\nFung, Yin Li, and Vikas Singh. 2021. NystrÃ¶mformer: A NystrÃ¶m-based Algorithm\nfor Approximating Self-Attention. (2021).\n[46] Zhi-Qin John Xu. 2020. Frequency Principle: Fourier Analysis Sheds Light on\nDeep Neural Networks. Communications in Computational Physics 28 (2020),\n1746â€“1767.\n[47] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. 2023. Are Transformers\nEffective for Time Series Forecasting? (2023).\n[48] Yunhao Zhang and Junchi Yan. 2023. Crossformer: Transformer Utilizing Cross-\nDimension Dependency for Multivariate Time Series Forecasting. (2023).\n[49] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,\nand Wancai Zhang. 2021. Informer: Beyond Efficient Transformer for Long\nSequence Time-Series Forecasting. (2021).\n[50] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022.\nFEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai\nForecasting. (2022), 1â€“12.\n[51] Yunyue Zhu and Dennis Shasha. 2002. StatStream: Statistical Monitoring of\nThousands of Data Streams in Real Time. (2002), 358â€“369.\nFredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nFredformer: Frequency Debiased Transformer for Time Series Forecasting\nâ€”â€”â€”â€”Appendixâ€”â€”â€”â€”\nContents\nAbstract 1\n1 Introduction 1\n2 Preliminary Analysis 2\n2.1 Preliminary 2\n2.2 Case Studies 3\n3 Frequency Bias Formulation 4\n3.1 Frequency Bias Definitions 4\n3.2 Problem Statement 4\n4 Fredformer 4\n4.1 Backbone 4\n4.2 Frequency Refinement and Normalization 4\n4.3 Frequency Local Independent Modeling 5\n4.4 Frequency-wise Summarization 5\n5 EXPERIMENTS 5\n5.1 PROTOCOLS 5\n5.2 Results 7\n5.3 Ablation Study 7\n5.4 Discussion of Applicability 7\n6 Related Works 8\n7 Conclusion 8\n8 ACKNOWLEDGMENTS 9\nReferences 9\nContents 11\nA Details of the case studies 12\nB More Details of the datasets 12\nC Fredformer Algorithm 12\nD Look-back window analysis 12\nE Hyperparameter sensitivity 13\nF Visualizations of the forecasting results 14\nF.1 Time domain 14\nF.2 Frequency domain 14\nG Details of the Lemma 1 15\nH NystrÃ¶m Approximation in Transformer Self-Attention Mechanism 16\nI Detailed results of all datasets 17\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai\nThe full appendix can be found at: http://arxiv.org/abs/2406.09009\nA DETAILS OF THE CASE STUDIES\nHere, we illustrate the details of how we generated the data for\ncase study 2 in Sec.2.2.2: The generation of data for Case Study\n2 from the original time series involves a sequence of steps to\nemphasize certain frequency components by manipulating their\npositions in the frequency domain. This process not only constructs\na dataset with distinct frequency characteristics but also preserves\nthe inherent noise and instability of the real data, enhancing the\nrobustness and credibility of subsequent analyses. Specifically, the\nsteps are as follows:\n(1) Apply the Discrete Fourier Transform (DFT) to the original\ntime series data to obtain its frequency components, exclud-\ning columns irrelevant for Fourier analysis (e.g., dates).\n(2) Select four prominent low-frequency components from the\nentire frequency spectrum and move them to the mid-frequency\npart. This modification aims to reduce the impact of fre-\nquency bias typically seen between low and high frequencies\nby placing important components in a non-low and non-high\nfrequency position.\n(3) Split the frequency components into three equal parts.\n(4) Rearrange these parts according to a predefined order for\nfrequency emphasis, ensuring that the first part is moved to\nthe end while keeping the original second and third parts in\ntheir order.\n(5) Apply the Inverse Discrete Fourier Transform (IDFT) to the\nrearranged frequency data to convert it back into the time\ndomain, thereby generating the modified \"mid\" frequency\ndata.\n(6) Reinsert any excluded columns (e.g., dates) to maintain the\noriginal structure of the data.\nThrough the operations described above, we have constructed\na dataset with clearly high amplitude frequency components in\nthe middle of the frequency domain. By moving significant low-\nfrequency components to the mid-frequency section, we aim to\nmitigate the effects of frequency differences that arise from the\ndominance of low and high frequencies. The advantage of creating\nartificial data through these simple modifications to real data lies\nin its ability to preserve the inherent noise and instability present\nin the real data, thereby enhancing the robustness and credibility\nfor subsequent analysis.\nB MORE DETAILS OF THE DATASETS\nWeather contains 21 channels (e.g., temperature and humidity)\nand is recorded every 10 minutes in 2020. ETT [ 49] (Electricity\nTransformer Temperature) consists of two hourly-level datasets\n(ETTh1, ETTh2) and two 15-minute-level datasets (ETTm1, ETTm2).\nElectricity [20], from the UCI Machine Learning Repository and\npreprocessed by, is composed of the hourly electricity consumption\nof 321 clients in kWh from 2012 to 2014. Solar-Energy [19] records\nthe solar power production of 137 PV plants in 2006, sampled every\n10 minutes. Traffic contains hourly road occupancy rates measured\nby 862 sensors on San Francisco Bay area freeways from January\n2015 to December 2016. More details of these datasets can be found\nin Table.7.\nAlgorithm 1 Generation of Modified Frequency Data for Case\nStudy 2\n1: procedure GenerateModifiedFreqencyData(ğ‘‘ğ‘ğ‘¡ğ‘)\n2: ğ‘‘ğ‘ğ‘¡ğ‘’ğ¶ğ‘œğ‘™ğ‘¢ğ‘šğ‘› â†ğ‘‘ğ‘ğ‘¡ğ‘[â€²ğ‘‘ğ‘ğ‘¡ğ‘’â€²] âŠ² Preserve the date column\nfor reinsertion\n3: ğ‘‘ğ‘ğ‘¡ğ‘ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ğ‘  â†ğ‘‘ğ‘ğ‘¡ğ‘.ğ‘‘ğ‘Ÿğ‘œğ‘ (ğ‘ğ‘œğ‘™ğ‘¢ğ‘šğ‘›ğ‘  = [â€²ğ‘‘ğ‘ğ‘¡ğ‘’â€²]) âŠ² Exclude\nnon-relevant columns\n4: ğ‘“ğ‘“ğ‘¡ğ·ğ‘ğ‘¡ğ‘ â†DFT(ğ‘‘ğ‘ğ‘¡ğ‘ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ğ‘  ) âŠ² Apply DFT to obtain\nfrequency components\n5: ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ğ¶ğ‘œğ‘šğ‘ğ‘œğ‘›ğ‘’ğ‘›ğ‘¡ğ‘  â†\nSelectLowFrequencyComponents(ğ‘“ğ‘“ğ‘¡ğ·ğ‘ğ‘¡ğ‘ , 4) âŠ² Select 4\nprominent low-frequency components\n6: ğ‘“ğ‘“ğ‘¡ğ·ğ‘ğ‘¡ğ‘ğ‘€ğ‘œğ‘‘ğ‘–ğ‘“ğ‘–ğ‘’ğ‘‘ â†MoveComponentsToMid(ğ‘“ğ‘“ğ‘¡ğ·ğ‘ğ‘¡ğ‘ ,\nğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘’ğ‘‘ğ¶ğ‘œğ‘šğ‘ğ‘œğ‘›ğ‘’ğ‘›ğ‘¡ğ‘  ) âŠ² Move selected components to\nmid-frequency\n7: ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘  â†Split(ğ‘“ğ‘“ğ‘¡ğ·ğ‘ğ‘¡ğ‘ğ‘€ğ‘œğ‘‘ğ‘–ğ‘“ğ‘–ğ‘’ğ‘‘ , 3) âŠ² Split frequency\ncomponents into three equal parts\n8: ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿ â†DefineOrderForFrequencyEmphasis() âŠ² Define a\nnew order for rearrangement\n9: ğ‘Ÿğ‘’ğ‘ğ‘Ÿğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘  â†Rearrange(ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘ , ğ‘œğ‘Ÿğ‘‘ğ‘’ğ‘Ÿ) âŠ² Rearrange\nparts according to the predefined order\n10: ğ‘šğ‘œğ‘‘ğ‘–ğ‘“ğ‘–ğ‘’ğ‘‘ğ¹ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘›ğ‘ğ‘¦ğ·ğ‘ğ‘¡ğ‘ â†IDFT(ğ‘Ÿğ‘’ğ‘ğ‘Ÿğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’ğ‘‘ğ‘ƒğ‘ğ‘Ÿğ‘¡ğ‘  ) âŠ²\nApply IDFT to generate modified time domain data\n11: ğ‘šğ‘œğ‘‘ğ‘–ğ‘“ğ‘–ğ‘’ğ‘‘ğ¹ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘›ğ‘ğ‘¦ğ·ğ‘ğ‘¡ğ‘.ğ‘–ğ‘›ğ‘ ğ‘’ğ‘Ÿğ‘¡ (0,â€²ğ‘‘ğ‘ğ‘¡ğ‘’â€²,ğ‘‘ğ‘ğ‘¡ğ‘’ğ‘ğ‘œğ‘™ğ‘¢ğ‘šğ‘› ) âŠ²\nReinsert the date column\n12: return ğ‘šğ‘œğ‘‘ğ‘–ğ‘“ğ‘–ğ‘’ğ‘‘ğ¹ğ‘Ÿğ‘’ğ‘ğ‘¢ğ‘’ğ‘›ğ‘ğ‘¦ğ·ğ‘ğ‘¡ğ‘\n13: end procedure\nC FREDFORMER ALGORITHM\nThe algorithm 2 outlines our overall procedure. It includes several\nparts: (i) DFT-to-IDFT Backbone, where the input data is trans-\nformed using DFT and segmented into frequency bands; (ii & iii)\nFrequency Local Independent Learning, where normalization and a\nTransformer are applied to learn dependencies and features across\nchannels; and (iv) Frequency-wise Summarizing, where the pro-\ncessed frequency information is summarized and transformed back\nto the time domain using IDFT to obtain the forecasting result.\nD LOOK-BACK WINDOW ANALYSIS\nWe conducted further tests on our method using the ETTh1 and\nWeather datasets to investigate the impact of different look-back\nwindow lengths on forecasting accuracy. Four distinct lengths were\nchosen: {96, 192, 336, 720}, with 96 corresponding to the results\npresented in the main text and the other three lengths selected to\ncompare the changes in forecasting accuracy with longer input\nsequences. Figure. 6 illustrates the variation in model forecasting\naccuracy across these input lengths. Overall, as the length of the\ninput sequence increases, so does the model forecasting accuracy,\ndemonstrating that our model is capable of extracting more features\nfrom longer input sequences. Specifically, comparing the longest\nwindow of 720 to the shortest of 96, the model forecasting accuracy\nimproved by approximately 10% (0.343 â†’0.315 for Weather and\n0.467 â†’0.449 for ETTh1).\nFredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nTable 7: Overview of Datasets\nDataset Source Resolution Channels Time Range\nWeather Autoformer[43] Every 10 minutes 21 (e.g., temperature, humidity) 2020\nETTh1 Informer[49] Hourly 7 states of a electrical transformer 2016-2017\nETTh2 Informer[49] Hourly 7 states of a electrical transformer 2017-2018\nETTm1 Informer[49] Every 15 minutes 7 states of a electrical transformer 2016-2017\nETTm2 Informer[49] Every 15 minutes 7 states of a electrical transformer 2017-2018\nElectricity UCI ML Repository Hourly 321 clientsâ€™ consumption 2012-2014\nSolar-Energy [19] Every 10 minutes 137 PV plantsâ€™ production 2006\nTraffic Informer[49] Hourly 862 sensorsâ€™ occupancy 2015-2016\nAlgorithm 2 Fredformer\n1: Input: Historical data X âˆˆRğ¶Ã—ğ¿, where ğ¶ is the number of\nchannels and ğ¿is the length of the data series.\n2: Output: Forecasting result Xâ€²\n3: Procedure:\n4: (i )DFT-to-IDFT Backbone\n5: Perform DFT on X to obtain A âˆˆRğ¶Ã—ğ¿\n6: for channel ğ‘ = 1 to ğ¶ do\n7: for frequency band ğ‘›= 1 to ğ‘ do\n8: Segment Ağ‘ into ğ‘ bands: (W(1:ğ¶)\n1 ,..., W(1:ğ¶)\nğ‘ )\n9: end for\n10: end for\n11: (ii&iii )Frequency Local Independent Learning\n12: for frequency band ğ‘›= 1 to ğ‘ do\n13: Normalize cross-channel amplitude sequences for band ğ‘›:\nWâˆ—ğ‘› = ğœ(Wğ‘›) ğ‘›= 1,2,...,ğ‘\n14: Apply Transformer to learn channel-wise dependencies and\njoint features across channels: ğ’˜â€²(1:ğ¶)\nğ‘› = ğ‘“ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘  (ğ’˜âˆ—(1:ğ¶)\nğ‘› )\n15: end for\n16: (iv )Frequency-wise Summarizing\n17: Abstract overall frequency information to form newAâ€²: Aâ€²=\nLinear(Wâ€²)\n18: Perform IDFT using Aâ€²to generate Xâ€²: Xâ€²= IDFT(Aâ€²)\nE HYPERPARAMETER SENSITIVITY\nTo evaluate our model robustness across various hyperparameter\nsettings with input/predicting length ğ¿= 96/ğ» = 720, we investi-\ngated four key hyperparameters: (1) model depth (cf depth), (2)\nfeature dimension of self-attention ( cf dim ), (3) feature dimen-\nsion within self-attention multi-heads (cf head dim), (4) number\nof multi-heads (cf heads), and (5) feature dimension of the feed-\nforward layer in the Transformer Encoder ( cf mlp ). We tested\none hundred hyperparameter combinations, with results shown\nin Figure E. The variation in the model accuracy, ranging from\n0.433 to 0.400 with an average value of 0.415, shows our preference\nfor stability in hyperparameter selection over chasing the highest\npossible accuracy. We decided to utilize the averaged accuracy as\nour benchmark result in Table I.\nVisual representations of each hyperparameter impact on model\nrobustness are detailed as follows:\n96 192 336 720\nMSE\nLook-back window Length\nETTh1 Weather\nFigure 6: The forecasting performance on two datasets,\nETTh1 and Weather, across four different look-back win-\ndow lengths. The x-axis indicates the window length, while\nthe y-axis represents the MSE loss of the forecasting results.\nFigure 7: Overall view of all trials, plotting different trials on\nthe x-axis against MSE accuracy on the y-axis. Yellow dots\nrepresent trials with poor accuracy, falling below our final\nselection MSE accuracy, while purple dots indicate trials with\nhigher accuracy than our final decision.\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai\nFigure 8: Illustration of the cf depth parameter effect across\nall trials, showcasing MSE accuracy for different values. The\nx-axis represents the parameter values, while the y-axis dis-\nplays MSE accuracy. Purple indicates trials with MSE accu-\nracy higher than our final chosen result, and yellow signifies\ntrials with lower accuracy. The box plot details the distribu-\ntion of MSE accuracy above (in yellow) or below (in purple)\nour final selection for each value.\nâ€¢Figure 8 illustrates the robustness across different cf depth\nsettings.\nâ€¢Figure 9 showcases the impact of varying cf dim on model\nperformance.\nâ€¢Figure 10 presents the model behavior with changes in cf\nhead dim.\nâ€¢Figure 11 depicts the influence of different cf heads counts.\nâ€¢Figure 12 reveals how adjustments incf mlp affect accuracy.\nF VISUALIZATIONS OF THE FORECASTING\nRESULTS\nDue to space constraints and the length of the paper, we have\nomitted a significant number of visualization results in the main\ntext. In the appendix, we provide additional visualization results to\ndemonstrate the effectiveness of our method. Here, we divide the\nvisualization results into two categories: (1) time-domain visual-\nizations and (2) frequency-domain visualizations. These categories\nhighlight two critical aspects of our model effectiveness: (1) the\naccuracy of predictions in the time domain and (2) the capability to\ncapture important components in the frequency domain.\nF.1 Time domain\nWe have included additional samples from two different channels\nof the ETTh1 dataset. Figure 13 presents a sample from channel #5,\nand Figure 14 showcases a sample from channel #2. The data char-\nacteristics across different channels vary. Our model, in contrast to\nFEDformer, adeptly learns the similarities and differences across\nvarious channels, underscoring the significance of channel-wise\nattention. Compared to iTransformer, our model captures more\nFigure 9: Depiction of the cf dim parameter influence, detail-\ning MSE accuracy across various settings. Parameter values\nare plotted on the x-axis against MSE accuracy on the y-axis.\nTrials surpassing our final result accuracy are marked in pur-\nple, whereas those falling short are in yellow. The distribu-\ntion of MSE accuracies, differentiated by outcomes exceeding\nor not meeting our final accuracy, is presented in a box plot\nfor each parameter value.\nFigure 10: Analysis of the cf head dim parameter impact,\nwith the x-axis indicating parameter values and the y-axis\nMSE accuracy. Purple highlights trials where MSE accuracy\nis above our finalized result, with yellow showing lower accu-\nracy trials. Each parameter value MSE accuracy distribution,\ncategorized by exceeding or not our final accuracy, is visual-\nized through a box plot.\ndetailed features, effectively identifying both global and local char-\nacteristics, and highlighting the importance of frequency-domain\nmodeling.\nF.2 Frequency domain\nFigure 15 visualizes another sample output of the model after 50\nepochs of training in the frequency domain, with input and ground\nFredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nFigure 11: Visualization of the cf heads parameter role, plot-\nting parameter values against MSE accuracy. Trials with\nhigher MSE accuracy than our chosen outcome are in purple;\nthose with lower accuracy are in yellow. A box plot repre-\nsents the spread of MSE accuracies for each value, segmented\ninto results that are above (yellow) or below (purple) our final\nselection.\nFigure 12: Examination of the cf mlpparameter performance,\nwith parameter values on the x-axis and MSE accuracy on the\ny-axis. Purple represents trials outperforming our final result\nin MSE accuracy, while Yellow indicates underperformance.\nThe box plot shows the distribution of MSE accuracies for\neach parameter value, divided into outcomes that surpass or\nfall short of our final chosen accuracy.\ntruth data from ETTh1. Similar to Section 2.2, the line graph dis-\nplays the frequency amplitudes, and the heat map shows the model\nrelative error for four components over increasing epochs. We fo-\ncus on mid-to-high frequency features here, where the amplitudes\nof these four key components, ğ‘˜1, ğ‘˜2, ğ‘˜3, and ğ‘˜4, are significantly\nlower than low-frequency components, successfully capturing these\ncomponents indicates the model ability to mitigate frequency bias.\nAfter training, our method accurately identifiesğ‘˜1, ğ‘˜2, and ğ‘˜3, with\nFredformer iTransformerãƒ»Real data Fedfromer\nFigure 13: Visualization of forecasting results ( ğ» = 336).\nGrey dots show the real data, and the grey zone represents\nthe last part of the look-back window. Sample from chan-\nnel #5 of the ETTh1 dataset, illustrating the unique data\ncharacteristics of this channel.\nFredformer iTransformerãƒ»Real data Fedfromer\nFigure 14: Visualization of forecasting results ( ğ» = 336).\nGrey dots show the real data, and the grey zone represents\nthe last part of the look-back window. Sample from channel\n#2 of the ETTh1 dataset, demonstrating the distinct data\nfeatures specific to this channel.\nuniformly decreasing relative errors. Despite a larger learning error\nfor ğ‘˜4, Î”ğ‘˜4 consistently diminishes. This performance contrasts\nwith all baselines, which demonstrate a lack of effectiveness in\ncapturing these frequency components, with unequal reductions in\nrelative errors.\nG DETAILS OF THE LEMMA 1\nHere, we give more details of Lemma.1 in Sec.4. First, we illustrate\nthe full-spectrum normalization as:\nWâˆ—= {ğœ(W1 ... Wğ‘)}\nThis shows why normalizing the entire spectrum together does\nnot address the disparity in amplitude across different frequency\nbands Wğ‘›, failing to remove amplitude bias between key frequency\ncomponents, that is,\nmax(Wâˆ—\nğ‘›)> max(Wâˆ—\nğ‘š) (4)\nThen, we furthermore illustrate why time domain patching opera-\ntion and normalization:\n{ğœ(Wt1)...ğœ (Wtğ‘)}= {ğœ(W1 ... Wğ‘)} (5)\nwhere Wt represents a series of time-domain patches of X. Ac-\ncording to Parsevalâ€™s theorem, the equivalence of the energy of\nthe time-domain signal after local normalization and the energy of\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai\nTable 8: Comparison of GPU Memory Usage and MSE Accuracy on ETTh1 and ECl datasets, the best results are highlighted in\nred\nModel ETTh1 Dataset ECl Dataset\nGPU Memory (MB) MSE Accuracy GPU Memory (MB) MSE Accuracy\nOurs 408 0.453 4338 0.213\nOurs(NystrÃ¶m) - - 3250 0.212\niTransformer 642 0.491 3436 0.225\nPatchTST 1872 0.454 43772 0.256\nCrossformer 2128 0.666 40066 0.280\nFEDformer 1258 0.543 3824 0.246\niTransformer\nFEDformer\nGround truthInput Forecasting\n Relative Error0.99 0.01\nPatchTST\n#epoch1 50\nk1k2k3k4\nFredformer(Proposed)\nF60 1400\nk1\nk2 k3 k4\n#epoch1 50\nk1k2k3k4F60 1400\nk1\nk2 k3 k4\n#epoch1 50\nk1k2k3k4F60 1400\nk1\nk2 k3 k4\n#epoch1 50\nk1k2k3k4F60 1400\nk1\nk2 k3 k4\nFigure 15: Visualizations of the learning dynamics and re-\nsults for our method and baselines on the ETTh1 dataset,\nemploying line graphs to illustrate amplitudes in the fre-\nquency domain and heatmaps to represent training epoch\nerrors.\nthe frequency-domain signal after global normalization results in\nsimilar:\nmax(Wâˆ—\nğ‘›)> max(Wâˆ—\nğ‘š) (6)\nwhich does not solve the issue of amplitude bias among key fre-\nquency components.\nHere, we furthermore discuss why this operation does not solve\nthe issue of amplitude bias among key frequency components:\nGiven the frequency components Wğ‘› in the frequency domain\nand their corresponding time-domain representations Wtğ‘›, Parse-\nvalâ€™s theorem provides the foundation for understanding the energy\nequivalence between the time and frequency domains. Specifically,\nit states that the total energy in the time domain is equal to the\ntotal energy in the frequency domain:\nğ‘âˆ‘ï¸\nğ‘›=1\nâˆ«\n|ğœ(Wtğ‘›)|2ğ‘‘ğ‘¡ =\nâˆ«\n|ğœ(W1,..., Wğ‘)|2ğ‘‘ğ‘“,\nwhere ğ‘ ğ‘–ğ‘”ğ‘šğ‘(ğ‘ğ‘‘ğ‘œğ‘¡)denotes the normalization operation. This\ntheorem underscores the equivalence between applying local nor-\nmalization to patches in the time domain and global normalization\nacross the frequency spectrum.\nEach point in the time domain can be expressed as the sum of\nenergies from all frequency components at that point. Therefore, a\ntime-domain patch, consisting of ğ‘† points, can be represented as\nthe sum of all frequency components over these ğ‘† points:\nPatchtime =\nğ‘†âˆ‘ï¸\nğ‘ =1\nğ‘âˆ‘ï¸\nğ‘›=1\nWğ‘›(ğ‘ ),\nwhere Wğ‘›(ğ‘ )represents the energy contribution of frequency\ncomponent ğ‘›at time point ğ‘ . Normalizing this time-domain patch\nequates to normalizing the weights of all frequency components\nacross these ğ‘† points. Mathematically, this normalization can be\nrepresented as:\nğœ(Patchtime)= ğœ\n ğ‘†âˆ‘ï¸\nğ‘ =1\nğ‘âˆ‘ï¸\nğ‘›=1\nWğ‘›(ğ‘ )\n!\n.\nHowever, normalizing within each time-domain patch does not\nguarantee that the maximum amplitudes across all frequency com-\nponents Wâˆ—ğ‘› and Wâˆ—ğ‘š are equalized across different patches. This\nleads to the critical insight:\nmax(Wâˆ—\nğ‘›)> max(Wâˆ—\nğ‘š), âˆ€ğ‘›,ğ‘š,\nIndicating that local normalization in the time domain, and by ex-\ntension, global normalization in the frequency domain, does not ef-\nfectively address the amplitude bias problem among key frequency\ncomponents. The inherent limitation is that while normalization\ncan adjust the overall energy levels within patches or across the\nspectrum, it does not inherently correct for discrepancies in the am-\nplitude distributions among different frequency components. This\nunderscores the necessity for approaches that can specifically target\nand mitigate amplitude biases to ensure equitable representation\nand processing of all frequency components.\nH NYSTRÃ–M APPROXIMATION IN\nTRANSFORMER SELF-ATTENTION\nMECHANISM\nOverview: To streamline the attention computation, we selectğ‘š\nlandmarks by averaging rows or columns of the attention matrix,\nsimplifying the matrices Qğ‘› and Kğ‘› into ËœQğ‘› and ËœKğ‘›. The NystrÃ¶m\napproximation for the ğ‘›-th channel-wise attention Ağ‘› is then cal-\nculated as Ağ‘› â‰ˆ ËœAğ‘› = ËœFğ‘› ËœAğ‘› ËœBğ‘›, where ËœFğ‘› = softmax(Qğ‘› ËœKğ‘›\nğ‘‡),\nËœAğ‘› = softmax( ËœQğ‘› ËœKğ‘›\nğ‘‡)+, ËœBğ‘› = softmax( ËœQğ‘›Kğ‘›ğ‘‡). Here, ËœAğ‘›\n+is the\nFredformer: Frequency Debiased Transformer for Time Series Forecasting KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nMoore-Penrose inverse of ËœAğ‘› [45]. This significantly reducing the\ncomputational load from ğ‘‚(ğ¿\nğ‘ƒğ¶2)to ğ‘‚(ğ¿\nğ‘ƒğ¶). Specifically:\nDetails: We reduce the computational cost of self-attention in\nthe Transformer encoder using the NystrÃ¶m method. Following,\nwe describe how to use the NystrÃ¶m method to approximate the\nsoftmax matrix in self-attention by sampling a subset of columns\nand rows.\nConsider the softmax matrix in self-attention, defined as:\nğ‘† = softmax\n \nğ‘„ğ¾ğ‘‡\nâˆšï¸ğ‘‘ğ‘\n!\nThis matrix can be partitioned as:\nğ‘† =\n\u0014ğ´ğ‘† ğµğ‘†\nğ¹ğ‘† ğ¶ğ‘†\n\u0015\nWhere ğ´ğ‘† is derived by sampling ğ‘šcolumns and rows from ğ‘†.\nBy employing the NystrÃ¶m method, the SVD of ğ´ğ‘† is given by:\nğ´ğ‘† = ğ‘ˆÎ›ğ‘‰ğ‘‡\nUsing this, an approximation Ë†ğ‘† of ğ‘† can be constructed:\nË†ğ‘† =\n\u0014ğ´ğ‘† ğµğ‘†\nğ¹ğ‘† ğ¹ğ‘†ğ´+\nğ‘†ğµğ‘†\n\u0015\nWhere ğ´+\nğ‘† is the Moore-Penrose inverse of ğ´ğ‘†.\nTo further elaborate on the approximation, given a query ğ‘ğ‘– and\na key ğ‘˜ğ‘—, let:\nK(ğ‘ğ‘–,ğ¾)= softmax\n \nğ‘ğ‘–ğ¾ğ‘‡\nâˆšï¸ğ‘‘ğ‘\n!\nK(ğ‘„,ğ‘˜ğ‘—)= softmax\n ğ‘„ğ‘˜ğ‘‡\nğ‘—\nâˆšï¸ğ‘‘ğ‘\n!\nFrom the above, we can derive:\nğœ™(ğ‘ğ‘–,ğ¾)= Î›âˆ’1\n2 ğ‘‰ğ‘‡K(ğ‘ğ‘–,ğ¾)ğ‘šÃ—1\nğœ™(ğ‘„,ğ‘˜ğ‘—)= Î›âˆ’1\n2 ğ‘ˆğ‘‡K(ğ‘„,ğ‘˜ğ‘—)ğ‘šÃ—1\nThus, the NystrÃ¶m approximation for a particular entry in Ë†ğ‘† is:\nË†ğ‘†ğ‘–ğ‘— = ğœ™(ğ‘ğ‘–,ğ¾)ğ‘‡ğœ™(ğ‘„,ğ‘˜ğ‘—)\nIn matrix form, Ë†ğ‘† can be represented as:\nË†ğ‘† = softmax\n \nğ‘„ğ¾ğ‘‡\nâˆšï¸ğ‘‘ğ‘\n!\nğ‘›Ã—ğ‘š\nğ´+\nğ‘†softmax\n \nğ‘„ğ¾ğ‘‡\nâˆšï¸ğ‘‘ğ‘\n!\nğ‘šÃ—ğ‘›\nThis method allows for the approximation of the softmax matrix\nin self-attention, potentially offering computational benefits.\nI DETAILED RESULTS OF ALL DATASETS\nHere, we show the detailed forecasting results of full datasets in\nthe Table. I. The best and second best results are highlighted. With\na default look-back window of ğ¿ = 96, our proposal shows lead-\ning performance on most datasets and different prediction length\nsettings, with 60 top-1 (29 + 31) cases out of 80 in total.\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Xihao Piao*, Zheng Chen*, Taichi Murayama, Yasuko Matsubara, and Yasushi Sakurai\nTable 9: Full results of the long-term forecasting task\n. We compare extensive competitive models under different prediction lengths following the setting of iTransformer [24]. The input\nsequence length is set to 96 for all baselines. Avg means the average results from all four prediction lengths.\nModels FredformeriTransformerRLinear PatchTSTCrossformerTiDE TimesNetDLinear SCINet FEDformerStationaryAutoformer\n(Ours) [2024] [2023] [2023] [2023] [2023] [2023] [2023] [2022] [2022] [2022a] [2021]\nMetric MSE MAEMSE MAE MSEMAEMSEMAEMSE MAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAEMSEMAE\nETTm1\n96 0.3260.3610.334 0.368 0.3550.3760.3290.3670.4040.4260.3640.3870.3380.3750.3450.3720.4180.4380.3790.4190.3860.3980.5050.475\n192 0.3630.3800.377 0.391 0.3910.3920.3670.3850.4500.4510.3980.4040.3740.3870.3800.3890.4390.4500.4260.4410.4590.4440.5530.496\n336 0.3950.4030.426 0.420 0.4240.4150.3990.4100.5320.5150.4280.4250.4100.4110.4130.4130.4900.4850.4450.4590.4950.4640.6210.537\n720 0.4530.4380.491 0.459 0.4870.4500.4540.4390.6660.5890.4870.4610.4780.4500.4740.4530.5950.5500.5430.4900.5850.5160.6710.561\nAvg 0.3840.3950.407 0.410 0.4140.4070.3870.4000.5130.4960.4190.4190.4000.4060.4030.4070.4850.4810.4480.4520.4810.4560.5880.517\nETTm2\n96 0.1770.2590.180 0.264 0.1820.2650.1750.2590.2870.3660.2070.3050.1870.2670.1930.2920.2860.3770.2030.2870.1920.2740.2550.339\n192 0.2430.3010.250 0.309 0.2460.3040.2410.3020.4140.4920.2900.3640.2490.3090.2840.3620.3990.4450.2690.3280.2800.3390.2810.340\n336 0.3020.3400.311 0.348 0.3070.3420.3050.3430.5970.5420.3770.4220.3210.3510.3690.4270.6370.5910.3250.3660.3340.3610.3390.372\n720 0.3970.3960.412 0.407 0.4070.3980.4020.4001.7301.0420.5580.5240.4080.4030.5540.5220.9600.7350.4210.4150.4170.4130.4330.432\nAvg 0.2790.3240.288 0.332 0.2860.3270.2810.3260.7570.6100.3580.4040.2910.3330.3500.4010.5710.5370.3050.3490.3060.3470.3270.371\nETTh1\n96 0.3730.3920.386 0.405 0.3860.3950.4140.4190.4230.4480.4790.4640.3840.4020.3860.4000.6540.5990.3760.4190.5130.4910.4490.459\n192 0.4330.4200.441 0.436 0.4370.4240.4600.4450.4710.4740.5250.4920.4360.4290.4370.4320.7190.6310.4200.4480.5340.5040.5000.482\n336 0.4700.4370.487 0.458 0.4790.4460.5010.4660.5700.5460.5650.5150.4910.4690.4810.4590.7780.6590.4590.4650.5880.5350.5210.496\n720 0.4670.4560.503 0.491 0.4810.4700.5000.4880.6530.6210.5940.5580.5210.5000.5190.5160.8360.6990.5060.5070.6430.6160.5140.512\nAvg 0.4350.4260.454 0.447 0.4460.4340.4690.4540.5290.5220.5410.5070.4580.4500.4560.4520.7470.6470.4400.4600.5700.5370.4960.487\nETTh2\n96 0.2930.3420.297 0.349 0.2880.3380.3020.3480.7450.5840.4000.4400.3400.3740.3330.3870.7070.6210.3580.3970.4760.4580.3460.388\n192 0.3710.3890.380 0.400 0.3740.3900.3880.4000.8770.6560.5280.5090.4020.4140.4770.4760.8600.6890.4290.4390.5120.4930.4560.452\n336 0.3820.4090.428 0.432 0.4150.4260.4260.4331.0430.7310.6430.5710.4520.4520.5940.5411.0000.7440.4960.4870.5520.5510.4820.486\n720 0.4150.4340.427 0.445 0.4200.4400.4310.4461.1040.7630.8740.6790.4620.4680.8310.6571.2490.8380.4630.4740.5620.5600.5150.511\nAvg 0.3650.3930.383 0.407 0.3740.3980.3870.4070.9420.6840.6110.5500.4140.4270.5590.5150.9540.7230.4370.4490.5260.5160.4500.459\nECL\n96 0.1470.2410.1480.2400.2010.2810.1950.2850.2190.3140.2370.3290.1680.2720.1970.2820.2470.3450.1930.3080.1690.2730.2010.317\n192 0.1650.2580.1620.2530.2010.2830.1990.2890.2310.3220.2360.3300.1840.2890.1960.2850.2570.3550.2010.3150.1820.2860.2220.334\n336 0.1770.2730.1780.2690.2150.2980.2150.3050.2460.3370.2490.3440.1980.3000.2090.3010.2690.3690.2140.3290.2000.3040.2310.338\n720 0.2130.3040.225 0.317 0.2570.3310.2560.3370.2800.3630.2840.3730.2200.3200.2450.3330.2990.3900.2460.3550.2220.3210.2540.361\nAvg 0.1750.2690.178 0.270 0.2190.2980.2160.3040.2440.3340.2510.3440.1920.2950.2120.3000.2680.3650.2140.3270.1930.2960.2270.338\nTraffic\n96 0.4060.2770.3950.2680.6490.3890.5440.3590.5220.2900.8050.4930.5930.3210.6500.3960.7880.4990.5870.3660.6120.3380.6130.388\n192 0.4260.2900.4170.2760.6010.3660.5400.3540.5300.2930.7560.4740.6170.3360.5980.3700.7890.5050.6040.3730.6130.3400.6160.382\n336 0.4320.2810.433 0.283 0.6090.3690.5510.3580.5580.3050.7620.4770.6290.3360.6050.3730.7970.5080.6210.3830.6180.3280.6220.337\n720 0.4630.3000.467 0.302 0.6470.3870.5860.3750.5890.3280.7190.4490.6400.3500.6450.3940.8410.5230.6260.3820.6530.3550.6600.408\nAvg 0.4310.2870.4280.2820.6260.3780.5550.3620.5500.3040.7600.4730.6200.3360.6250.3830.8040.5090.6100.3760.6240.3400.6280.379\nWeather\n96 0.1630.2070.174 0.214 0.1920.2320.1770.2180.1580.2300.2020.2610.1720.2200.1960.2550.2210.3060.2170.2960.1730.2230.2660.336\n192 0.2110.2510.221 0.254 0.2400.2710.2250.2590.2060.2770.2420.2980.2190.2610.2370.2960.2610.3400.2760.3360.2450.2850.3070.367\n336 0.2670.2920.278 0.296 0.2920.3070.2780.2970.2720.3350.2870.3350.2800.3060.2830.3350.3090.3780.3390.3800.3210.3380.3590.395\n720 0.3430.3410.358 0.349 0.3640.3530.3540.3480.3980.4180.3510.3860.3650.3590.3450.3810.3770.4270.4030.4280.4140.4100.4190.428\nAvg 0.2460.2720.258 0.279 0.2720.2910.2590.2810.2590.3150.2710.3200.2590.2870.2650.3170.2920.3630.3090.3600.2880.3140.3380.382\nSolar-Energy\n96 0.1850.2330.203 0.237 0.3220.3390.2340.2860.3100.3310.3120.3990.2500.2920.2900.3780.2370.3440.2420.3420.2150.2490.8840.711\n192 0.2270.2530.233 0.261 0.3590.3560.2670.3100.7340.7250.3390.4160.2960.3180.3200.3980.2800.3800.2850.3800.2540.2720.8340.692\n336 0.2460.2840.2480.2730.3970.3690.2900.3150.7500.7350.3680.4300.3190.3300.3530.4150.3040.3890.2820.3760.2900.2960.9410.723\n720 0.2470.2760.2490.2750.3970.3560.2890.3170.7690.7650.3700.4250.3380.3370.3560.4130.3080.3880.3570.4270.2850.2950.8820.717\nAvg 0.2260.2610.233 0.262 0.3690.3560.2700.3070.6410.6390.3470.4170.3010.3190.3300.4010.2820.3750.2910.3810.2610.3810.8850.711\n1st Count 29 31 4 8 1 1 2 1 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.600511908531189
    },
    {
      "name": "Transformer",
      "score": 0.5286686420440674
    },
    {
      "name": "Time series",
      "score": 0.46885189414024353
    },
    {
      "name": "Series (stratigraphy)",
      "score": 0.43515244126319885
    },
    {
      "name": "Electrical engineering",
      "score": 0.15270105004310608
    },
    {
      "name": "Voltage",
      "score": 0.14309972524642944
    },
    {
      "name": "Engineering",
      "score": 0.12531551718711853
    },
    {
      "name": "Machine learning",
      "score": 0.11751711368560791
    },
    {
      "name": "Geology",
      "score": 0.05182966589927673
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}