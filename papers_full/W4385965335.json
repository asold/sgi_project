{
  "title": "Graph-Segmenter: graph transformer with boundary-aware attention for semantic segmentation",
  "url": "https://openalex.org/W4385965335",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5091037704",
      "name": "Zizhang Wu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5025716158",
      "name": "Yuanzhu Gan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5088677357",
      "name": "Tianhao Xu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100719173",
      "name": "Fan Wang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4290652572",
    "https://openalex.org/W4283013235",
    "https://openalex.org/W2981207549",
    "https://openalex.org/W3008115128",
    "https://openalex.org/W3040318838",
    "https://openalex.org/W2911486422",
    "https://openalex.org/W2592939477",
    "https://openalex.org/W3035028692",
    "https://openalex.org/W3035339581",
    "https://openalex.org/W2602856279",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4312599212",
    "https://openalex.org/W4313166619",
    "https://openalex.org/W6635487051",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W2507296351",
    "https://openalex.org/W2125215748",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W3092263272",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W3204874447",
    "https://openalex.org/W3109301572",
    "https://openalex.org/W3110440461",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2997564896",
    "https://openalex.org/W2965391153",
    "https://openalex.org/W2989684653",
    "https://openalex.org/W2911293880",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W4214893857",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2027179862",
    "https://openalex.org/W3166177850",
    "https://openalex.org/W3211171448",
    "https://openalex.org/W3003875435",
    "https://openalex.org/W3034345703",
    "https://openalex.org/W3188906027",
    "https://openalex.org/W3205071568",
    "https://openalex.org/W2963076818",
    "https://openalex.org/W3127947687",
    "https://openalex.org/W3035526186",
    "https://openalex.org/W3109196706",
    "https://openalex.org/W3121736262",
    "https://openalex.org/W3043238202",
    "https://openalex.org/W3210796717",
    "https://openalex.org/W2895065325",
    "https://openalex.org/W3011667710",
    "https://openalex.org/W2907492528",
    "https://openalex.org/W2912083425",
    "https://openalex.org/W3107634219",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2799213142",
    "https://openalex.org/W2911495555",
    "https://openalex.org/W3034355852",
    "https://openalex.org/W3034502973",
    "https://openalex.org/W2886934227",
    "https://openalex.org/W2895340641",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W2991062542",
    "https://openalex.org/W4225827210",
    "https://openalex.org/W2993235622",
    "https://openalex.org/W2948080074",
    "https://openalex.org/W2890782586",
    "https://openalex.org/W4294558607",
    "https://openalex.org/W2951527505",
    "https://openalex.org/W2891778567",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2972321983",
    "https://openalex.org/W2609532991",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W4210257598",
    "https://openalex.org/W3157528469",
    "https://openalex.org/W3207615232",
    "https://openalex.org/W1662382123",
    "https://openalex.org/W3159637683",
    "https://openalex.org/W4308909683"
  ],
  "abstract": null,
  "full_text": "Front.Comput.Sci.\nDOI\nREVIEW ARTICLE\nGraph-Segmenter: Graph Transformer with Boundary-aware\nAttention for Semantic Segmentation\nZizhang Wu\n 1, Yuanzhu Gan1, Tianhao Xu1,2, Fan Wang1\n1 Computer Vision Perception Department of ZongMu Technology, Shanghai, 201203, China.\n2 Faculty of Electronics, Information Technology, Physics, Technical University of Braunschweig,\nBraunschweig, 38106, Germany.\n© Higher Education Press and Springer-Verlag Berlin Heidelberg 2012\nAbstract The transformer-based semantic segmentation\napproaches, which divide the image into different regions by\nsliding windows and model the relation inside each window,\nhave achieved outstanding success. However, since the re-\nlation modeling between windows was not the primary em-\nphasis of previous work, it was not fully utilized. To address\nthis issue, we propose a Graph-Segmenter, including a Graph\nTransformer and a Boundary-aware Attention module, which\nis an effective network for simultaneously modeling the more\nprofound relation between windows in a global view and vari-\nous pixels inside each window as a local one, and for substan-\ntial low-cost boundary adjustment. Specifically, we treat ev-\nery window and pixel inside the window as nodes to construct\ngraphs for both views and devise the Graph Transformer. The\nintroduced boundary-aware attention module optimizes the\nedge information of the target objects by modeling the re-\nlationship between the pixel on the object’s edge. Exten-\nsive experiments on three widely used semantic segmenta-\ntion datasets (Cityscapes, ADE-20k and PASCAL Context)\ndemonstrate that our proposed network, a Graph Transformer\nwith Boundary-aware Attention, can achieve state-of-the-art\nsegmentation performance.\nKeywords Graph Transformer, Graph Relation Network,\nBoundary-aware, Attention, Semantic Segmentation\nReceived month dd, yyyy; accepted month dd, yyyy\nE-mail: xutianhao2018@gmail.com\n1 Introduction\nSemantic segmentation [1, 2] is a primary task in the field\nof computer vision, with the goal of labeling each picture\npixel with a category that corresponds to it. As a result, in-\ntense research attention has lately been focused on it since\nit has the potential to improve a wide range of downstream\napplications [3–6], including geographic information sys-\ntems, autonomous driving, medical picture diagnostics, and\nrobotics. Modern semantic segmentation models almost fol-\nlow the same paradigm [7–10]: they consist of a basic back-\nbone for feature extraction and a head for pixel-level clas-\nsification tasks in the current deep learning era. Improving\nthe performance of the backbone network and the head of the\nsegmentation model are two of the most controversial sub-\njects in current semantic segmentation work.\nNatural language processing (NLP) has been dominated\nby transformers for a long time [11, 12], leading to a cur-\nrent spike of interest in exploring the prospect of using trans-\nformers in vision tasks, including significant advancements\nin semantic segmentation. Using the vision transformer [13],\nimages are divided into a number of non-overlapping win-\ndows/patches, and some subsequent research investigated\nmethods to enhance connections between windows /patches.\nIt has the benefit of improving the modeling power of the\nsystem [14–16].\nSwin employs [14] a shifted window approach, however,\nthe direction in which it interacts with other windows is fixed\nand unavoidably unresponsive. Segformer utilizes [15] the\narXiv:2308.07592v1  [cs.CV]  15 Aug 2023\n2 Zizhang Wu et al. Graph-Segmenter: Graph Transformer with Boundary-aware Attention for Semantic Segmentation\nFig. 1: An illustration of the proposed Graph-Segmenter\nwith boundary-aware attention for semantic segmentation.\nThe top shows the segmentation results of the previ-\nous transformer-based semantic segmentation methods (e.g.,\nSwin [14]). The bottom shows the actual segmentation\nresults of our proposed Graph-Segmenter, which achieves\npromising boundary segmentation via the hierarchical level\ngraph reasoning and efficient boundary adjustment requiring\nno additional annotation.\noverlapping patch merging approach, however it focuses pri-\nmarily on the local continuity between patches rather than the\noverall continuity between patches. The modeling of long-\ndistance interactions between windows/patches is not investi-\ngated in depth by these methodologies, which is unfortunate.\nIn addition to current work on the optimization of the back-\nbone, a number of research have contributed to the design of\nthe head in order to facilitate further optimization [17–19].\nHowever, following the majority of them is computation-\nally costly due to the fact that boundary prediction-based ap-\nproaches need extra object boundary labeling for segmenta-\ntion boundary optimization.\nWe propose a novel relation modeling method acting on\nsliding windows, using graph convolutions to establish rela-\ntionships between windows and pixels inside each window,\nwhich enhanced the backbone to address the issues above.\nIn particular, we regard each window or the pixels inside as\nnodes for the graph network and use the visual similarity\nbetween nodes to establish the edges between nodes. After\nthat, we use the graph network further to update the nodes\nand edges of the graph. So that di fferent nodes can adap-\ntively establish connections and update information in net-\nwork transmission to realize the nonlinear relationship mod-\neling between di fferent windows and di fferent pixels inside.\nIn brief, the network’s overall feature learning and charac-\nterization capabilities are further improved by enhancing the\nlong-distance nonlinear modeling capabilities between differ-\nent windows and di fferent pixels inside, as shown in Figure\n1, which leads to an evident rise in performance.\nFurthermore, we introduce an e fficient boundary-aware\nattention-enhanced segmentation head that optimizes the\nboundary of objects in the semantic segmentation task, al-\nlowing us to reduce the labeling cost even further while si-\nmultaneously improving the accuracy of the semantic seg-\nmentation in the boundary of the objects under considera-\ntion. To put it another way, we develop a lightweight lo-\ncal information-aware attention module that allows for im-\nproved boundary segmentation. By determining the weights\nof the pixels around an object’s border and applying various\nattention coefficients to distinct pixels via local perception,\nit is possible to reinforce the important pixels that are crit-\nical in categorization while weakening the interfering pix-\nels. The attention module used in this study has just a few\ncommon CNN layers, which makes it efficient for segmenta-\ntion boundary adjustments when considering the size, float-\ning point operations, and latency time of the segmentation\ndata.\nWe investigate the atypical interaction between windows\nthat are arranged hierarchically on a vision transformer. Ad-\nditionally, we improve the boundary of target instances using\nan efficient and lightweight boundary optimization approach\nthat does not need any extra annotation information and can\nadaptively alter the segmentation boundary for a variety of\nobjects. We have demonstrated in this paper that our Graph-\nSegmenter can achieve state-of-the-art performance on se-\nmantic segmentation tasks using extensive experiments on\nthree standard semantic segmentation datasets (Cityscapes\n[20], ADE-20k [21], and PASCAL Context [22]), as demon-\nstrated by extensive experiments on three standard semantic\nsegmentation datasets.\n2 Related works\n2.1 CNN-based Semantic Segmentation\nConvolutional neural networks (CNNs) based methods serve\nas the standard approaches throughout the semantic segmen-\ntation [23–28] task due to apparent advantages compared\nwith traditional methods. FCN [23] started the era of end-\nto-end semantic segmentation, introducing dense prediction\nwithout any fully connected layer. Subsequently, many FCN-\nbased methods [25, 26, 29] have been proposed to promote\nimage semantic segmentation. DNN-based methods usually\nFront. Comput. Sci. 2022, 0(0): 1-14 3\nneed to expand the receptive field through the superposition\nof convolutional layers. Besides that, the receptive field issue\nwas explored by several approaches, such as Pyramid Pooling\nModule (PPM) [25] in PSPNet and Atrous Spatial Pyramid\nPooling (ASPP) in DeepLabv3 [26], which expand the recep-\ntive field and capture multiple-range information to enhance\nthe representation capabilities. Recent CNN-based methods\nplace a premium on e ffectively aggregating the hierarchical\nfeatures extracted from a pre-trained backbone encoder using\nspecifically developed modules: DANet [27] applies a differ-\nent form of the non-local network; SFNet [30] addresses the\nmisalignment problem through semantic flow by using the\nFlow Alignment Module (FAM); CCNet [31] present a Criss-\nCross network which gathers contextual information adap-\ntively throughout the criss-cross path; GFFNet [32]; APC-\nNet [33] proposes Adaptive Pyramid Context Network for\nsemantic segmentation, which creates multi-scale contextual\nrepresentations adaptively using several well-designed Adap-\ntive Context Modules (ACMs). [28] divides the feature map\ninto different regions to extract regional features separately,\nand the bidirectional edges of the directed graph are used\nto represent the a ffinities between these regions, in order to\nhelp model region dependencies and alleviate unrealistic re-\nsults. [34] learns boundaries as an additional semantic cate-\ngory, so the network can be aware of the layout of the bound-\naries. It also proposes unidirectional acyclic graphs (UAGs)\nto simulate the function of undirected cyclic graphs (UCGs)\nto structure the image. Boundaryaware feature propagation\n(BFP) module can acquire and propagate boundary local fea-\ntures of regions to create strong and weak connections be-\ntween regions in the UAG.\n2.2 Transformer and Self-Attention\nThe attention mechanism for image classification was first\nseen in [35], and then [36] employed similar attention on\ntranslation and alignment simultaneously in the machine\ntranslation task, which was the first application of the at-\ntention mechanism to the NLP field. Inspired by the dom-\ninant performance in the NLP field later, the application of\nself-attention and transformer architectures in computer vi-\nsion are widely explored in recent years. Self-attention lay-\ners as a replacement for spatial convolutional layers achieved\nrises in terms of robustness and performance-cost trade-o ff\n[37]. Moreover, Vision Transformer (ViT) [13] shown trans-\nformer structure with simple non-overlap patches could sur-\npass state-of-the-art results, and the impressive result led to a\ntrend of vision transformers. The issue that ViT is unsuitable\nas a general backbone for semantic segmentation was further\naddressed by several approaches: [38] proposed DEtection\nTRansformer (DETR) for direct set prediction based on trans-\nformers and bipartite matching loss; based on DETR, [39]\nsuggested Deformable DETR with attention modules that fo-\ncus only on a limited number of critical sampling locations\naround a reference; after that, [40] investigated the video\ninstance segmentation (VIS) task using vision transformers,\ntermed VisTR, which view the VIS task as a direct end-to-\nend parallel sequence decoding/prediction problem. [41] ex-\npands the idea of DETR to multi-camera 3D object detection\nto make great progress.\nIn addition, the transformer-based semantic segmentation\nmethods [14, 15, 42, 43] have drawn much attention. [42] re-\nlies on a ViT [13] backbone and introduces a mask decoder\ninspired by DETR [38]. [43] reformulates the image seman-\ntic segmentation problem from a transformer-based learning\nperspective. [14] uses a shifted windowing scheme to break\nthe limited self-attention computation into non-overlapping\nlocal windows. [15] proposes a powerful semantic segmen-\ntation framework with lightweight multilayer perceptron de-\ncoders. They adopt the Overlapped Patch Merging module\nto preserve local continuity. However, these methods didn’t\nlook at the relationship between windows very well, which\nled this study to use hierarchical-level graph reasoning and\nefficient boundary adjustment. Our method puts more focus\non the graph modeling between windows within the trans-\nformer blocks, which is the first work for the semantic seg-\nmentation task.\n2.3 Graph Model\nGraph-based convolutional networks (GCNs) [44–47] can ex-\nploit the interactions of non-Euclidean data, which was ini-\ntially proposed in [48]. As graphs can be irregular, GCNs led\nto an increasing interest in recent years, and many excellent\nworks further explored GCNs from various perspectives such\nas graph attention networks [49] for guided learning, and dy-\nnamic graph [50] to learn features more adaptively. GCNs\nalso have achieved great performance in kinds of computer\nvision tasks, such as human pose estimation [51], image cap-\ntioning [52], action recognition [53], and so on. [51] adopts\ngraph atrous convolution and transformer layers to extract\nmulti-scale context and long-range information for human\npose estimation. [52] proposes an image captioning model\nbased on a dual-GCN and transformer combination. [54] pro-\nposes an adaptive graph convolutional network with atten-\ntion graph clustering for image co-saliency detection. More-\n4 Zizhang Wu et al. Graph-Segmenter: Graph Transformer with Boundary-aware Attention for Semantic Segmentation\nover, GCNs can propagate information globally and model\ncontextual information efficiently for semantic segmentation\n[44, 55–57], point cloud segmentation [58, 59] and instance\nsegmentation [60]. [55] performs graph reasoning directly in\nthe original feature space to model long-range context. [56]\nintroduces the class-wise dynamic graph convolution mod-\nule to capture long-range contextual information adaptively.\n[44] captures the global context along the spatial and chan-\nnel dimension by the graph-convolutional module, respec-\ntively. [57] combines adjacency graphs and KSC-graphs by\naffinity nodes of multi-scale superpixels for better segmen-\ntation. [58] proposes a hierarchical attentive pooling graph\nnetwork for point cloud segmentation to enhance the local\nmodeling. [61, 62] provide more detailed surveys of GCNs.\n2.4 Discussion\nDifferent from Swin [14], first of all, our proposed graph\ntransformer focuses on the nonlinear relationship modeling\nof sliding windows, including the modeling of high-order\n(greater than or equal to 2-order) relationship between slid-\ning windows and among di fferent feature points inside each\nsliding window. Secondly, our graph transformer module\nis a lightweight pluggable sliding window modeling mod-\nule, which can be embedded into some sliding window-based\ntransfomer models such as Swin [14] to improve the fea-\nture modeling and expression ability of the original model.\nFinally, the graph transformer modules plus the boundary-\naware attention modules together form our Graph-Segmenter\nnetwork, which achieves state-of-the-art performance com-\npared to other recent works on the existing segmentation\nbenchmarks.\n3 Methods\nThis section will elaborate on the design of our devised\nGraph-Segmenter, which includes the Graph Transformer\nmodule and Boundary-aware attention module.\n3.1 The Overview\nAs shown in Figure 2, our proposed coarse-to-fine-grained\nboundary modeling network, Graph-Segmenter, includes a\ncoarse-grained graph transformer module in the backbone\nand the fine-grained boundary-aware attention module in the\nhead of the semantic segmentation framework. Assume that\nthe input image passes through some CNN layers and then\nobtains the positional embedding enhanced feature maps X ∈\nRC×H×W , where C denotes the number of channels, H and\nW denote height and width in the spatial dimension of the\nimage, respectively. In order to make better use of the Trans-\nformer mechanism, inspired by [14, 16], we first divide fea-\nture maps X into M ×N window patches and define each win-\ndow as {xm,n ∈RC×H\nM ×W\nN |m ∈{1,2,..., M},n ∈{1,2,..., N}}.\nAnd then X passes through Graph Transformer (GT) Network\nmodule, generating the relation-enhanced featuresY after GT\nmodule in the backbone network. After this, we use the\nboundary-aware attention (BA) module to adjust the segmen-\ntation boundary on the feature maps Y and obtain the object\nboundary optimal feature Z. At last, we use these features\nZ to formulate the corresponding segmentation loss function\nin each pixel. In the following subsections, we will elabo-\nrate on our proposed window-aware relation transformer net-\nwork module and boundary-aware attention module. To facil-\nitate the following description, we first introduce the efficient\ngraph relation network, which plays a central role in the graph\ntransformer block and is capable of efficiently and effectively\nmodeling the nonlinear relations for abstract graph nodes.\n3.2 E fficient Graph Relation Network\nIn this section, we first introduce the preliminary work on\ngraph convolution networks (GCN), which introduces convo-\nlutional parameters that can be optimized compared to graph\nneural networks (GNN). As shown in [49], a GCN can be\ndefined as\nX = RXW, (1)\nwhere X, R and W are the input node matrix (here we still\ndenote the output of the above equation as X), the adja-\ncency matrix reflecting the relationship between nodes and\nthe learnable convolution parameter matrix respectively.\nBased on above preliminary work, we introduce an e ffi-\ncient graph relation network, which was inspired by [49, 63],\nto model the non-linear relation of higher-order among the\nsliding windows and inside each window. Given the input X,\nwe define the relation function Ras follows\nR = R(X). (2)\nAs shown in Equation (2), to establish the relationship\nRbetween different input data xi and xj, where xi,xj ∈X,\ni, j ∈{1,2,..., K}, and K is the number of nodes in X, math-\nematically, we can establish a simple linear relationship using\nlinear functions or a more complex nonlinear relationship us-\ning nonlinear functions. In general, linear relations are poor\nin terms of robustness, so here we design a quadratic mul-\nFront. Comput. Sci. 2022, 0(0): 1-14 5\nFig. 2: An overview of the proposed Graph-Segmenter with e fficient boundary adjustment for semantic segmentation, which\nincludes Global Relation Modeling, Local Relation Modeling, and Boundary-aware Attention. “GR” denotes the global\nwindow-aware relation module and “LR” denotes the local window-aware relation module. “GR” or “LR” consists of 1 ×1\nconvolution, softmax, W and 1 ×1 convolution. W is the learnable weighting matrix, corresponding to W(l) in Equation 6,\nand also equivalent to Cgr in “GR” or Clr in “LR”.\ntiplication operation that establishes a nonlinear relationship\nbetween different input nodes.\nri,j = xi ·xj\n||xi ·xj||, (3)\nwhere ri,j ∈R is the learned relation betweenxi and xj. ||·||\nrepresents a two-norm operation. Because xi ·xj ≤||xi ·xj||,\n|ri,j|≤ 1, where |·| denotes the absolute value operation, we\ndo not need to normalize the ri,j as in [64].\nWith the connection relation matrix R, we can define the\nnode update function Uof the graph as follows\nX(l+1) = U(X(l)). (4)\nNow that we have ri,j and also to design an efficient graph\nrelational network, we want this graph network node propa-\ngation update operation to be a sparse matrix operation. To\nachieve efficient computation, we use the indicative function\nand the relational matrixR. Equation (4) can be rewritten into\na form that satisfies the sparse matrix operation as follows:\nx(l+1)\ni =\nX\nx(l)\nj ∈δ(x(l)\ni )\nI(ri,j >θ) ·x(l)\nj ,i, j ∈{1,2,..., K}, (5)\nwhere I(·) is the indicative function which takesri j if the con-\ndition holds, or 0 otherwise. x(l)\nj is the jth graph node in layer\nl in the graph, which is generated in the layer-by-layer trans-\nfer process of graph convolutional network. δ(x(l)\ni ) denotes\nthe set of neighboring nodes of node x(l)\ni in layer l. Finally,\nin order to accomplish the graph convolutional network de-\nfined in Equation (1), we use the convolutional function Cto\nconvolute the node information as follows\nX(l+1) = C(X(l)) = X(l)W(l), (6)\nwhere W(l) is the learnable weighting matrix.\n3.3 Graph Transformer\nSliding windows-based methods [14,16] model the local rela-\ntion inside each window via transformer [13], most of which\ndoes not take into account the global modeling of a nonlin-\near relation between windows. In order to obtain more robust\nrelation-enhanced features, we propose Graph Transformer\n(GT) to model the relationship between di fferent windows\nand within each window. We use the visual similarity among\ndifferent windows to learn the relation of di fferent windows\nwhile using the visual similarity between di fferent pixels to\nmodel the local relation inside each sliding window. Based\non the above-devised efficient graph relation network, the de-\ntailed design of the global relation modeling module and the\nlocal one will be further elaborated on below.\n3.3.1 Global Relation Modeling\nWe propose a Global Relation (GR) Modeling module to\nmodel the global relation among di fferent sliding windows\nand make the pixel-level classification more accurate. Specif-\nically, we consider each window as one node and use the effi-\ncient graph relation network to build the relation of the graph\nnodes. As a result, we can obtain the coarse relation of the\nobjects in each image via the relation modeling of windows.\n6 Zizhang Wu et al. Graph-Segmenter: Graph Transformer with Boundary-aware Attention for Semantic Segmentation\n(a) Series\n (b) Parallel\nFig. 3: Three designs of fusion. ”GR” denotes the global\nrelation modeling module and ”LR” denotes the local relation\nmodeling module. (a) The two relation modeling modules\nare connected in two di fferent series connections. (b) Two\nrelation modeling modules are connected in parallel.\nConcretely, given the partitioned M ×N sliding windows\nin feature maps X ∈RC×H×W , we look at each sliding window\nxi,j ∈RC×H\nM ×W\nN as a node to establish a graph network. For the\nsake of narrative convenience, we denote xm,n as xi, where\ni = (m −1) ×N + n.\nWe first define the connection relation matrix of nodes\nRgr = {rgr\ni,j|i, j ∈ {1,2,··· ,M ×N}}, here we use the ma-\ntrix multiplication-based visual similarity to model the con-\nnection relation between nodes. From Equation (2), the rela-\ntion matrix Rgr = R(X). Simultaneously, in order to alleviate\nthe complexity of visual similarity computation, we devise a\nfunction Agr to reduce the channel dimension\nˆxi = Agr(xi), (7)\nwhere ˆX = {ˆxi ∈ R\nC\nrgr ×H\nM ×W\nN |i ∈ {1,2,··· ,M ×N}}is the\nhidden vector used to compute the connectivity of the graph,\nmainly to save computation and to enhance the learnability of\nthe module. rgr is the channel compression ratio. Thus, this\nallows us to derive a formula for calculating the connection\nrelation that saves significant computational complexity, as\nshown below\nRgr = Rgr( ˆX), (8)\nwhere, for ease of understanding, we rewrite the relational\nmodeling function Rin Equation (2) here as Rgr. With the\nconnection relation matrix Rgr (we rewrite R as Rgr in layer\nl of the graph neural network for simplicity in the following\nsection), based on Equation (4) and Equation (6), we can de-\nfine the node update functionUgr and the convolutional func-\ntion Cgr for global window-aware relation network as follows\nˆX\n(l+1)\n= Cgr ◦Ugr( ˆX\n(l)\n), (9)\nwhere ◦denotes the composition operations on multiple func-\ntions, generating a compound function. In order to keep the\ninput and output dimensions unchanged, we define ( Agr)−1,\nthe inverse transformation ofAgr, to restore the dimension of\nˆX in layer (l + 1), as shown below\nX(l+1) = (Agr)−1( ˆX\n(l+1)\n). (10)\n3.3.2 Local Relation Modeling\nIn order to learn the local relation inside each sliding window,\nwe conduct a Local Relation (LR) Modeling module to build\nrelations inside each sliding window. Similar to the global\nrelation modeling module, we consider each pixel as a node\nand exploit the efficient graph relation network to model the\nlocal relation among different pixels within each window.\nSpecifically, given the sliding window xi,j ∈RC×H\nM ×W\nN , we\nneed to learn the relationships among these feature points in-\nside each window xi,j. Thus, we can similarly define the en-\ntire process as follows\nx(l+1)\ni,j = (Alr)−1 ◦Clr ◦Ulr ◦Rlr ◦Alr(x(l)\ni,j), (11)\nwhere x(l)\ni,j and x(l+1)\ni,j are the feature maps in layer l and layer\nl + 1 of the local window-aware module, respectively.\n3.3.3 Module Structure\nAs shown in Figure 2, our presented graph transformer can\nbe implemented by two 1 ×1 Conv, a normalization func-\ntion (Softmax) and other basic elements (Self-Attention and\nMultilayer Perceptron) in ViT [13]. Specifically,Agr and Alr\ncan be completed by a 1 ×1 Conv, which is used to reduce\nthe computational complexity in the relation modeling pro-\ncess. Similarly, ( Agr)−1 and (Alr)−1 can be realized by an-\nother 1 ×1 Conv, aiming to resume the channel dimension\nfor module integration in the corresponding relation model-\ning network. In order to accomplish Rgr and Rlr, we use a\nmatrix multiplication and a softmax function to norm the re-\nsults and obtain relation matrix Rgr and Rlr respectively. The\nnode update function Ugr and Ulr can be accomplished by a\nmatrix multiplication simply.\n3.3.4 Fusion\nGiven a global relation modeling module and a local one, we\nfuse these two di fferent relation modeling modules to ob-\ntain better performance. As shown in Figure 3, there are\nFront. Comput. Sci. 2022, 0(0): 1-14 7\nCityscapesADE-20kPASCAL Context\nFig. 4: Some examples of the three datasets: Cityscapes (first row), ADE-20k (second row), and PASCAL Context (third row).\nthree fusion types, namely, global then local series and local\nthen global series connection (Figure 3a), parallel connection\n(Figure 3b). We will discuss in detail the impact of di fferent\nfusion methods on segmentation performance in the experi-\nmental section.\n3.4 Boundary-aware Attention\nA robust feature extraction backbone network has been con-\nstructed, as indicated above. We further devise boundary-\naware attention (BA) network to optimize the boundary re-\ngion of the corresponding object for a better overall semantic\nsegmentation effect.\nAs we all know, the edge information adjustment of ob-\njects is generally local, and is greatly a ffected by local fea-\ntures. Based on this, we design a local-aware attention mod-\nule, namely the BA module, which further improves the\nboundary segmentation accuracy of the model by enhancing\nand weakening the weight information of some local feature\npoints (or pixels, here we do not distinguish between pix-\nels and feature points), some of which are the boundaries\nof objects. Concretely, we perceive the relation between lo-\ncal pixels by the local perception module and perform local\nrelation modeling, based on which we learn to obtain the at-\ntention coefficients of each surrounding pixel. Finally, we use\nthe attention coe fficients to weigh the pixels, strengthen the\nkey pixels, and weaken the inessential ones to achieve accu-\nrate boundary segmentation.\nGiven the input Y ∈ReC×eH×eW , where eC, eH and eW denotes\nthe dimension of channel, height and width respectively, we\nuse the channel squeezing function eA to reduce the dimen-\nsion of input features, and then use the local modeling func-\ntion G to learning the local relation, where the pixels around\nthe boundary of the object is adjusted. Besides, we add non-\nTable 1: The complexity comparison of our Graph-\nSegmenter model and the original Swin [14] model with\nSwin-L as a backbone in terms of the number of model pa-\nrameters (#param) and the amount of multiplication compu-\ntation (GMac). ∗refers to reproduced results by us.\nMethod # param. GMac\nSwin∗[14] 233.66 191.45\nGraph-Segmenter (Ours) 283.46 195.63\nlinear function H inside the boundary-aware module to learn\nthe more robust relation and use the inverse function (eA)−1 to\nresume the input channel dimension. At last, we normalize\nthe learned attention coe fficient via the normalization func-\ntion N, so the whole formulation can be:\nZ = N ◦( eA)−1 ◦H ◦G ◦ eA(Y). (12)\n3.4.1 Module Structure\nAs shown in Figure 2, boundary-aware attention module\ncompose of two layers of 1 ×1 Conv, a layer of GELU and\na layer of Sigmoid. Similar to the design of the graph trans-\nformer module, we use 1×1 Conv to accomplish the channel\nsqueezing and resume functions eA and ( eA)−1. Here we sim-\nply use 7 ×7 Conv to implement the local relation modeling\nfunction G. Finally, function H and N can be completed\nthrough the common non-linear layer (GELU) and normal-\nization layer (Sigmoid), respectively.\n3.5 Discussion\nWe discuss the complexity of the proposed model in this sec-\ntion. The analysis of our designed lightweight pluggable\nGraph-Segmenter, consisting of a Graph Transformer and\n8 Zizhang Wu et al. Graph-Segmenter: Graph Transformer with Boundary-aware Attention for Semantic Segmentation\nBoundary-aware Attention, is shown in Table 1. From the ta-\nble, we can conclude that the increase of our proposed Graph-\nSegmenter model with the original Swin [14] model in terms\nof the number of model parameters (#param) and the multi-\nplicative computation is very small. In particular, in terms\nof multiplication computation, our model only increases the\ncomputation of 2.18% compared to the original model.\n4 Experiments\nIn this section, we evaluate our proposed Graph-Segmenter\non three standard semantic segmentation datasets: Cityscapes\n[20], ADE-20k [21] and PASCAL Context [22].\n4.1 Datasets and metrics\nCityscapes [20]. The dataset covers a range of 19 semantic\nclasses from 50 different cities. It has 5,000 images in total,\nwith 2,975 for training, 500 for validation, and another 1,525\nfor testing.\nADE-20k [21]. The dataset annotates 150 categories in\nchallenging scenes. It contains more than 25,000 finely anno-\ntated images, split into 20,210, 2,000 and 3,352 for training,\nvalidation and testing respectively.\nPASCAL Context[22]. The dataset is an extension of the\nPASCAL VOC 2010, which includes 4998 and 5105 photos\nfor training and validation, respectively, and gives pixel-by-\npixel semantic labels. It contains more than 400 classes from\nthree major categories (stuff, hybrids and objects). However,\nsince most classes are too sparse, we only analyze the most\ncommon 60 classes, which is the approach previous works\ndid [43].\nFigure 4 shows some examples of the three datasets.\nMetrics. We adopt mean Intersection over Union (mIoU)\nto evaluate semantic segmentation models.\n4.2 Implementation Details\nThe entire model is implemented based on the Swin [14]\ncodes that have been made publicly available. Training is\nseparated into two phases: we first establish the backbone\nnetwork using the model parameters pre-trained on Ima-\ngeNet, and then we train the backbone network with the\nGraph-Segmenter that we proposed. The trained network pa-\nrameters are utilized to initialize and train the network inte-\ngrated with the segmentation boundary optimization module,\nwhich is then used to optimize the segmentation boundary.\n4.3 Comparison with State-of-the-Art\nWe compare our proposed Graph-Segmenter with the state-\nof-the-art models, which contains CNN (e.g., ResNet-101,\nResNeSt-200) based methods [26,27,29,65], and latest trans-\nformer based methods [14, 42, 43] on Cityscapes, ADE-20k\nand PASCAL Context datasets.\nTable 2: Multi-scale inference results of semantic segmenta-\ntion on the Cityscapes validation dataset compared with state-\nof-the-art methods. ∗refers to reproduced results.\nMethod Backbone val mIoU\nFCN [23] ResNet-101 76.6\nNon-local [66] ResNet-101 79.1\nDLab.v3+ [26] ResNet-101 79.3\nDNL [65] ResNet-101 80.5\nDenseASPP [67] DenseNet 80.6\nDPC [68] Xception-71 80.8\nCCNet [31] ResNet-101 81.3\nDANet [27] ResNet-101 81.5\nPanoptic-Deeplab [69] Xception-71 81.5\nStrip Pooling [70] ResNet-101 81.9\nSeg-L-Mask/16 [42] ViT-L 81.3\nSETR-PUP [43] ViT-L 82.2\nSwin∗[14] Swin-L 82.3\nGraph-Segmenter (Ours) Swin-L 82.9\nTable 3: Results of semantic segmentation on the Cityscapes\ntest dataset compared with state-of-the-art methods. ∗ refers\nto reproduced results by us.\nMethod Backbone test mIoU\nPSPNet [25] ResNet-101 78.4\nBiSeNet [71] ResNet-101 78.9\nPSANet [72] ResNet-101 80.1\nOCNet [73] ResNet-101 80.1\nBFP [34] ResNet-101 81.4\nDANet [27] ResNet-101 81.5\nCCNet [31] ResNet-101 81.9\nSETR-PUP [43] ViT-L 81.1\nSwin∗[14] Swin-L 80.6\nGraph-Segmenter (Ours) Swin-L 81.9\nCityscapes. Table 2 and Table 3 demonstrate the results on\nCityscapes dataset. It can be seen that our Graph-Segmenter\nis +0.6 mIoU higher (82.9 vs. 82.3) than transformer-based\nmethod Swin-L [14] in Cityscapes validation dataset and\n+0.8 mIoU higher (81.9 vs. 81.1) than transformer-based\nSETR-PUP [43] in Cityscapes test dataset. In addition, we\nfurther show the accuracy of our proposed Graph-Segmenter\nin each category, as shown in Figure 6. From the figure, we\ncan see that Graph-Segmenter almost obtains the best per-\nFront. Comput. Sci. 2022, 0(0): 1-14 9\nTable 4: Multi-scale inference results of semantic segmenta-\ntion on the ADE-20k validation dataset compared with state-\nof-the-art methods. ∗refers to reproduced results by us.\nMethod Backbone val mIoU\nFCN [23] ResNet-101 41.4\nUperNet [74] ResNet-101 44.9\nDANet [27] ResNet-101 45.3\nOCRNet [29] ResNet-101 45.7\nACNet [75] ResNet-101 45.9\nDNL [65] ResNet-101 46.0\nDLab.v3+ [26] ResNeSt-101 47.3\nDLab.v3+ [26] ResNeSt-200 48.4\nSETR-PUP [43] ViT-L 50.3\nSegFormer-B5 [15] SegFormer 51.8\nSeg-L-Mask/16 [42] ViT-L 53.6\nSwin∗[14] Swin-L 53.1\nGraph-Segmenter (Ours) Swin-L 53.9\nTable 5: Results of semantic segmentation on the ADE-20k\ntest dataset compared with state-of-the-art methods. ∗ refers\nto reproduced results by us.\nMethod Backbone test score\nACNet [75] ResNet-101 38.5\nDLab.v3+ [26] ResNeSt-101 55.1\nOCRNet [29] ResNet-101 56.0\nDNL [65] ResNet-101 56.2\nSETR-PUP [43] ViT-L 61.7\nSwin∗[14] Swin-L 61.5\nGraph-Segmenter (Ours) Swin-L 62.4\nformance in all categories compared to the Swin model, and\neven for some more difficult categories (e.g., train, rider, wall)\nour model still achieves a very large performance improve-\nment. This phenomenon shows that global and local win-\ndow relation modeling and boundary-aware attention mod-\neling have further improved the ability of the network for\ndifficult-to-segment categories.\nADE-20k. Table 4 and Table 5 demonstrate the results on\nADE-20k dataset. Our Graph-Segmenter achieves 53.9 mIoU\nand 62.4 test score on the ADE-20k val and test set, surpass-\ning the previous best transformer-based model by+0.3 mIoU\n(53.6 mIoU by Seg-L-Mask/16 [42]) and+0.7 test score (61.7\nby SETR-PUP [43]). Our approach still can perform excel-\nlently in the challenging ADE-20k dataset with more cate-\ngories, which confirms our modules’ robustness. A more\nqualitative comparison is shown in Figure 5.\nPASCAL Context. To further demonstrate the e ffective-\nness of our proposed Graph-Segmenter module, we conduct\nmore experimental results on PASCAL Context dataset for\nUperNet [76] and “UperNet + CAR” [77] models. As shown\nin Table 6, we can observe that di fferent models equipped\nwith our Graph-Segmenter module can gain a consistent per-\nformance improvement. All in all, the very recent model\n“UperNet + CAR” embedding Graph-Segmenter achieves the\nbest mIoU accuracy.\nTable 6: Results of semantic segmentation on the Pascal\nContext dataset compared with state-of-the-art methods.\nMethod Backbone mIoU\nFCN [23] ResNet-101 45.74\nPSPNet [25] ResNet-101 47.80\nDANet [27] ResNet-101 52.60\nEMANet [78] ResNet-101 53.10\nSVCNet [79] ResNet-101 53.20\nBFP [34] ResNet-101 53.6\nStrip pooling [70] ResNet-101 54.50\nGFFNet [80] ResNet-101 54.20\nAPCNet [33] ResNet-101 54.70\nGRAr [28] ResNet-101 55.70\nSETR-PUP [43] ViT-L 55.83\nUperNet [76] Swin-L 57.48\nGraph-Segmenter + UperNet (Ours) Swin-L 57.80\nGraph-Segmenter + UperNet + CAR (Ours) Swin-L 59.01\nQualitative results. We further visualized and analyzed\nour devised Graph-segmenter network, as shown in Figure\n5. As shown in Figure 5, our Graph-Segmenter can obtain a\nbetter segmentation boundary compared with DeepLabV3 +\n[26] and Swin Tranformer [14].\n4.4 Ablation Study\nIn this section, we deeply investigate the performance of each\nsub-module of the model and conduct experiments on the\nCityscapes dataset.\n4.4.1 Sparsity Analysis\nWe first analyze the effect of different sparsity of the relation\nmatrix R on the experimental results. In order to eliminate the\ninfluence of other factors, we do not add the boundary-aware\nattention module in our experiments. By a simple setting of\nθ = 1\nK2\nKX\ni\nKX\nj\nri,j, the accuracy of the model with the sparse\nconstraint on the Cityscapes dataset could achieve 0.59 rise\nthan the case without it. It can be concluded that for a given\nnode, not all surrounding nodes can provide positive feature\nenhancement for the node. On the contrary, by properly filter-\ning out some nodes, the model can achieve better segmenta-\ntion performance. To further discuss the effect of the value of\n10 Zizhang Wu et al. Graph-Segmenter: Graph Transformer with Boundary-aware Attention for Semantic Segmentation\nFig. 5: Qualitative performance comparison of our proposed Graph-Segmenter with DeepLabV3+ [26] and Swin Tranformer\n[14] for semantic segmentation. Our Graph-Segmenter can obtain a better segmentation boundary.\nTable 7: mIoU performance of different connection types on\nCityscapes dataset. “ ∪” denotes parallel connection. “-T”\ndenotes Tiny.\nConnection Type Backbone val mIoU\nGR ∪LR Swin-T 76.70\nLR followed by GR Swin-T 76.44\nGR followed by LR Swin-T 76.99\nTable 8: mIoU performance of di fferent θ on Cityscape\ndataset. We set v = 1\nK2\nKX\ni\nKX\nj\nri,j for the convenience of ex-\npression. “-T” denotes Tiny.\nθ Backbone val mIoU\n2v Swin-T 76.44\nv Swin-T 76.27\n1\n2 v Swin-T 76.07\n1\n4 v Swin-T 77.64\n1\n8 v Swin-T 77.05\nθon the experimental results, we setθas different thresholds,\nrespectively, as shown in Table 8. From the table, we can\nobserve that different thresholds still have a certain degree of\ninfluence on the final segmentation e ffect of the model, and\nit is found that the model can achieve the best results when\nθ = 1\n4K2\nKX\ni\nKX\nj\nri,j. Besides, the experimental results further\nprove the e ffectiveness of the sparse setting, which can not\nonly simplify the calculation, but also improve the segmenta-\ntion accuracy of the model to a certain extent.\n4.4.2 Di fferent Connection Type Analysis\nWe analyze the influence of the di fferent sparseness of the\nrelation matrix R on the experimental results. As Table 7\nshows, we can see that the connection in the serial is bet-\nter than in parallel. Meanwhile, the best performance is\nobtained for serial connections by placing global window-\naware attention before local window-aware attention. The\nphenomenon further indicates that better segmentation results\ncan be achieved by large-scale coarse-grained relation mod-\neling among windows first and following fine-grained mod-\neling within each window.\n4.4.3 Graph Transformer and Boundary-aware Attention\nModules Analysis\nIn order to analyze the specific role of each component, we\ncompare various sub-modules to analyze the experimental\nresults separately. As Table 9 shows, the model equipped\nwith a graph transformer (Swin-GT) module or equipped\nwith boundary-aware attention (Swin-BA) module can ob-\ntain a noticeable improvement compared to the original Swin\nmodel. Besides, we can observe that BA can improve a\nTable 9: mIoU performance of di fferent components on\nCityscape dataset. “-T” denotes Tiny.\nMethod Backbone val mIoU\nSwin [14] Swin-T 75.82\nSwin-GT Swin-T 76.99\nSwin-BA Swin-T 75.94\nSwin-GTBA Swin-T 77.32\nFront. Comput. Sci. 2022, 0(0): 1-14 11\n/uni00000055/uni00000052/uni00000044/uni00000047/uni00000056/uni0000005a/uni00000045/uni00000047/uni0000005a/uni00000044/uni0000004f/uni0000004f/uni00000049/uni00000048/uni00000051/uni00000046/uni00000048/uni00000053/uni00000052/uni0000004f/uni00000048/uni00000057/uni0000004f/uni00000057/uni00000056/uni00000059/uni0000004a/uni00000057/uni00000048/uni00000055/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000056/uni0000004e/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000056/uni00000052/uni00000051/uni00000055/uni0000004c/uni00000047/uni00000048/uni00000055/uni00000046/uni00000044/uni00000055/uni00000057/uni00000055/uni00000058/uni00000046/uni0000004e/uni00000045/uni00000058/uni00000056/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000050/uni00000046/uni00000045/uni00000046\n/uni00000018/uni00000018\n/uni00000019/uni00000013\n/uni00000019/uni00000018\n/uni0000001a/uni00000013\n/uni0000001a/uni00000018\n/uni0000001b/uni00000013\n/uni0000001b/uni00000018\n/uni0000001c/uni00000013\n/uni0000001c/uni00000018\n/uni00000014/uni00000013/uni00000013\n/uni00000036/uni0000005a/uni0000004c/uni00000051\n/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b/uni00000010/uni00000036/uni00000048/uni0000004a/uni00000050/uni00000048/uni00000051/uni00000057/uni00000048/uni00000055\nFig. 6: Val mIoU comparison with Swin Tranformer [14] in each class on the Cityscapes val set. Our Graph-Segmenter\ngenerally achieves better performance. \"sw\" denotes \"sidewalk\", \"bd\" denotes \"building\", \"tl\" denotes \"tra ffic light\", \"ts\"\ndenotes \"traffic sign\", \"vg\" denotes \"vegetation\", \"mc\" denotes \"motorcycle\" and \"bc\" denotes \"bicycle\".\nlot over Swin-GT, although it only gains marginal improve-\nment over Swin. This phenomenon implies that after mod-\neling the global and local relationship, the BA module can\nachieve more accurate classification based on more robust\nfeatures than without Swin-GT enhancement. At the same\ntime, we can see that Swin embeds all of the proposed mod-\nules (GT+BA) and obtains the best performance compared to\nthe model embedding only some of our modules.\nFor analyzing the e ffect of two components in the graph\ntransformer module, we compare the sub-module results, as\nshown in Table 10. The model with the Global Relation Mod-\neling module (Swin-GR) or with the Local Relation Mod-\neling module (Swin-LR) can obtain a visible improvement\ncompared to the original model. When combining both kinds\nof sub-modules (Swin-GT), the model can achieve the best\nperformance.\n4.4.4 Di fferent Channel Compression Ratio r Analysis\nWe further analyze the performance of different compression\nratios in the graph transformer module. As indicated in Table\n11, we can see that the model obtains the best semantic seg-\nmentation accuracy when the compression ratio r (here we\nsimply set rgr and rlr to the same value and mark them as r)\nTable 10: mIoU performance of two components in the graph\ntransformer module on Cityscape dataset. “-T” denotes Tiny.\nMethod Backbone val mIoU\nSwin [14] Swin-T 75.82\nSwin-GR Swin-T 76.24\nSwin-LR Swin-T 76.28\nSwin-GT Swin-T 76.99\nis 16, which indicates that either too large or too small com-\npression rate will have an impact on the recognition progress\nof the model, so as shown in the table, we choose the same\ncompression rate of 16 in this paper.\n5 Conclusion\nThis paper presents Graph-Segmenter, which enhanced the\nvision transformer with hierarchical level graph reasoning\nand e fficient boundary adjustment requiring no additional\nannotation. Graph-Segmenter achieves state-of-the-art per-\nformance on Cityscapes, ADE-20k and PASCAL Context\nsemantic segmentation, noticeably surpassing previous re-\nsults. Extensive experiments demonstrate the e ffectiveness\nof multi-level graph modeling, features with unstructured as-\nsociations leading to evident rise; and the e ffectiveness of\nthe Boundary-aware Attention (BA) module, minor refine-\nments around boundary gave smoother masks than the previ-\nous state-of-the-art.\nTable 11: mIoU performance of Graph-Segmenter (only with\ngraph transformer module) on Cityscapes dataset for various\nchannel compression ratio r.\nr r =2 r =4 r =8 r =16 r =32\nmIoU 76.65 76.15 76.29 76.99 76.35\n12 Zizhang Wu et al. Graph-Segmenter: Graph Transformer with Boundary-aware Attention for Semantic Segmentation\nReferences\n1. Hongjia Ruan, Huihui Song, Bo Liu, Yong Cheng, and Qingshan Liu.\nIntellectual property protection for deep semantic segmentation mod-\nels. Frontiers of Computer Science, 17(1):1–9, 2023.\n2. Di Zhang, Yong Zhou, Jiaqi Zhao, Zhongyuan Yang, Hui Dong, Rui\nYao, and Huifang Ma. Multi-granularity semantic alignment distilla-\ntion learning for remote sensing image semantic segmentation. Fron-\ntiers of Computer Science, 16(4):1–3, 2022.\n3. Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Mace-\nsanu. A survey of deep learning techniques for autonomous driving.\nJournal of Field Robotics, 37(3):362–386, 2020.\n4. Di Feng, Christian Haase-Schütz, Lars Rosenbaum, Heinz Hertlein,\nClaudius Glaeser, Fabian Timm, Werner Wiesbeck, and Klaus Diet-\nmayer. Deep multi-modal object detection and semantic segmenta-\ntion for autonomous driving: Datasets, methods, and challenges. IEEE\nTransactions on Intelligent Transportation Systems, 22(3):1341–1360,\n2020.\n5. Joel Janai, Fatma Güney, Aseem Behl, Andreas Geiger, et al. Computer\nvision for autonomous vehicles: Problems, datasets and state of the art.\nFoundations and Trends in Computer Graphics and Vision, 12(1–3):1–\n308, 2020.\n6. Eduardo Arnold, Omar Y Al-Jarrah, Mehrdad Dianati, Saber Fallah,\nDavid Oxtoby, and Alex Mouzakitis. A survey on 3d object detection\nmethods for autonomous driving applications. IEEE Transactions on\nIntelligent Transportation Systems, 20(10):3782–3795, 2019.\n7. Panqu Wang, Pengfei Chen, Ye Yuan, Ding Liu, Zehua Huang, Xiaodi\nHou, and Garrison Cottrell. Understanding convolution for semantic\nsegmentation. In Winter Conference on Applications of Computer Vi-\nsion, pages 1451–1460. IEEE, 2018.\n8. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBert: Pre-training of deep bidirectional transformers for language un-\nderstanding. arXiv preprint arXiv:1810.04805, 2018.\n9. Li Wang, Dong Li, Yousong Zhu, Lu Tian, and Yi Shan. Dual super-\nresolution learning for semantic segmentation. InComputer Vision and\nPattern Recognition, pages 3774–3783. IEEE, 2020.\n10. Changqian Yu, Jingbo Wang, Changxin Gao, Gang Yu, Chunhua Shen,\nand Nong Sang. Context prior for scene segmentation. In Computer\nVision and Pattern Recognition, pages 12416–12425. IEEE, 2020.\n11. Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P\nLillicrap. Compressive transformers for long-range sequence mod-\nelling. arXiv preprint arXiv:1911.05507, 2019.\n12. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi,\nand Yee Whye Teh. Set transformer: A framework for attention-based\npermutation-invariant neural networks. InInternational Conference on\nMachine Learning, pages 3744–3753. PMLR, 2019.\n13. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,\nMatthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for\nimage recognition at scale. In International Conference on Learning\nRepresentations. OpenReview.net, 2021.\n14. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Baining Guo. Swin transformer: Hierarchical vision\ntransformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021.\n15. Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M\nAlvarez, and Ping Luo. Segformer: Simple and e fficient de-\nsign for semantic segmentation with transformers. arXiv preprint\narXiv:2105.15203, 2021.\n16. Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren,\nXiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting\nthe design of spatial attention in vision transformers. arXiv preprint\narXiv:2104.13840, 2021.\n17. Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu\nLiu, and Qi Tian. Msg-transformer: Exchanging local spatial\ninformation by manipulating messenger tokens. arXiv preprint\narXiv:2105.15168, 2021.\n18. Pichao Wang, Xue Wang, Fan Wang, Ming Lin, Shuning Chang, Wen\nXie, Hao Li, and Rong Jin. Kvt: k-nn attention for boosting vision\ntransformers. arXiv preprint arXiv:2106.00515, 2021.\n19. Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and Huaxia Xia.\nDo we really need explicit position encodings for vision transformers?\narXiv preprint arXiv:2102.10882, 2021.\n20. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld,\nMarkus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and\nBernt Schiele. The cityscapes dataset for semantic urban scene under-\nstanding. In Computer Vision and Pattern Recognition , pages 3213–\n3223. IEEE, 2016.\n21. Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba. Semantic understanding of scenes\nthrough the ade20k dataset. International Journal of Computer Vision,\n127(3):302–321, 2019.\n22. Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-\nWhan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role\nof context for object detection and semantic segmentation in the wild.\nIn Computer Vision and Pattern Recognition , pages 891–898. IEEE,\n2014.\n23. Jonathan Long et al. Fully convolutional networks for semantic seg-\nmentation. In Computer Vision and Pattern Recognition, pages 3431–\n3440. IEEE, 2015.\n24. Ying Shen, Heye Zhang, Yiting Fan, Alex Puiwei Lee, and Lin\nXu. Smart health of ultrasound telemedicine based on deeply rep-\nresented semantic segmentation. IEEE Internet of Things Journal ,\n8(23):16770–16778, 2021.\n25. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and\nJiaya Jia. Pyramid scene parsing network. In Computer Vision and\nPattern Recognition, pages 2881–2890. IEEE, 2017.\n26. Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schro ff,\nand Hartwig Adam. Encoder-decoder with atrous separable convolu-\ntion for semantic image segmentation. In European Conference on\nComputer Vision, pages 833–851. Springer, 2018.\nFront. Comput. Sci. 2022, 0(0): 1-14 13\n27. Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang,\nand Hanqing Lu. Dual attention network for scene segmentation. In\nComputer Vision and Pattern Recognition , pages 3146–3154. IEEE,\n2019.\n28. Henghui Ding, Hui Zhang, Jun Liu, Jiaxin Li, Zijian Feng, and Xudong\nJiang. Interaction via bi-directional graph of semantic region a ffinity\nfor scene parsing. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 15848–15858, 2021.\n29. Yuhui Yuan, Xiaokang Chen, Xilin Chen, and Jingdong Wang. Seg-\nmentation transformer: Object-contextual representations for semantic\nsegmentation. In European Conference on Computer Vision. Springer,\n2020.\n30. Xiangtai Li, Ansheng You, Zhen Zhu, Houlong Zhao, Maoke Yang,\nKuiyuan Yang, Shaohua Tan, and Yunhai Tong. Semantic flow for\nfast and accurate scene parsing. In European Conference on Computer\nVision, pages 775–793. Springer, 2020.\n31. Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yun-\nchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic\nsegmentation. In International Conference on Computer Vision, pages\n603–612. IEEE, 2019.\n32. Xiangtai Li, Houlong Zhao, Lei Han, Yunhai Tong, Shaohua Tan,\nand Kuiyuan Yang. Gated fully fusion for semantic segmentation.\nIn AAAI Conference on Artificial Intelligence , pages 11418–11425.\nAAAI, 2020.\n33. Junjun He, Zhongying Deng, Lei Zhou, Yali Wang, and Yu Qiao.\nAdaptive pyramid context network for semantic segmentation. InCom-\nputer Vision and Pattern Recognition, pages 7519–7528. IEEE, 2019.\n34. Henghui Ding, Xudong Jiang, Ai Qun Liu, Nadia Magnenat Thalmann,\nand Gang Wang. Boundary-aware feature propagation for scene seg-\nmentation. In International Conference on Computer Vision , pages\n6819–6829. IEEE, 2019.\n35. V olodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models\nof visual attention. In Conference on Neural Information Processing\nSystems, pages 2204–2212. MIT Press, 2014.\n36. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural\nmachine translation by jointly learning to align and translate. arXiv\npreprint arXiv:1409.0473, 2014.\n37. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam\nShazeer, Alexander Ku, and Dustin Tran. Image transformer. In Inter-\nnational Conference on Machine Learning, pages 4055–4064. PMLR,\n2018.\n38. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier,\nAlexander Kirillov, and Sergey Zagoruyko. End-to-end object detec-\ntion with transformers. In European Conference on Computer Vision,\npages 213–229. Springer, 2020.\n39. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng\nDai. Deformable DETR: Deformable transformers for end-to-end ob-\nject detection. In International Conference on Learning Representa-\ntions. OpenReview.net, 2021.\n40. Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan\nCheng, Hao Shen, and Huaxia Xia. End-to-end video instance segmen-\ntation with transformers. In Computer Vision and Pattern Recognition,\npages 8741–8750. IEEE, 2021.\n41. Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang, Yilun Wang,\nHang Zhao, and Justin Solomon. Detr3d: 3d object detection from\nmulti-view images via 3d-to-2d queries. In Conference on Robot\nLearning, pages 180–191. PMLR, 2022.\n42. Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid.\nSegmenter: Transformer for semantic segmentation. In IEEE Inter-\nnational Conference on Computer Vision. IEEE, 2021.\n43. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo,\nYabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr,\net al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. In Computer Vision and Pattern Recog-\nnition. IEEE, 2021.\n44. Li Zhang, Xiangtai Li, Anurag Arnab, Kuiyuan Yang, Yunhai Tong,\nand Philip HS Torr. Dual graph convolutional network for semantic\nsegmentation. arXiv preprint arXiv:1909.06121, 2019.\n45. Shun-Yi Pan, Cheng-You Lu, Shih-Po Lee, and Wen-Hsiao Peng.\nWeakly-supervised image semantic segmentation using graph convolu-\ntional networks. In International Conference on Multimedia and Expo,\npages 1–6. IEEE, 2021.\n46. Hao Wang, Liyan Dong, and Minghui Sun. Local feature aggregation\nalgorithm based on graph convolutional network. Frontiers of Com-\nputer Science, 16(3):1–3, 2022.\n47. Jiancan Wu, Xiangnan He, Xiang Wang, Qifan Wang, Weijian Chen,\nJianxun Lian, and Xing Xie. Graph convolution machine for context-\naware recommender system. Frontiers of Computer Science, 16(6):1–\n12, 2022.\n48. Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spec-\ntral networks and locally connected networks on graphs. In Inter-\nnational Conference on Learning Representations . OpenReview.net,\n2014.\n49. Petar Veli ˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana\nRomero, Pietro Lio, and Yoshua Bengio. Graph attention networks.\nIn International Conference on Learning Representations . OpenRe-\nview.net, 2018.\n50. Li Zhang, Dan Xu, Anurag Arnab, and Philip HS Torr. Dynamic graph\nmessage passing networks. In Computer Vision and Pattern Recogni-\ntion, pages 3726–3735. IEEE, 2020.\n51. Yiran Zhu, Xing Xu, Fumin Shen, Yanli Ji, Lianli Gao, and Heng Tao\nShen. Posegtac: Graph transformer encoder-decoder with atrous con-\nvolution for 3d human pose estimation. International Joint Conference\non Artificial Intelligence, 2021.\n52. Xinzhi Dong, Chengjiang Long, Wenju Xu, and Chunxia Xiao. Dual\ngraph convolutional networks with transformer and curriculum learn-\ning for image captioning. In ACM International Conference on Multi-\nmedia, pages 2615–2624, 2021.\n53. Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph\nconvolutional networks for skeleton-based action recognition. InAAAI\nConference on Artificial Intelligence. AAAI, 2018.\n54. Tengpeng Li, Kaihua Zhang, Shiwen Shen, Bo Liu, Qingshan Liu, and\nZhu Li. Image co-saliency detection and instance co-segmentation\nusing attention graph clustering based graph convolutional network.\n14 Zizhang Wu et al. Graph-Segmenter: Graph Transformer with Boundary-aware Attention for Semantic Segmentation\nIEEE Transactions on Multimedia, 2021.\n55. Xia Li, Yibo Yang, Qijie Zhao, Tiancheng Shen, Zhouchen Lin, and\nHong Liu. Spatial pyramid based graph reasoning for semantic seg-\nmentation. In Computer Vision and Pattern Recognition, pages 8950–\n8959. IEEE, 2020.\n56. Hanzhe Hu, Deyi Ji, Weihao Gan, Shuai Bai, Wei Wu, and Junjie\nYan. Class-wise dynamic graph convolution for semantic segmen-\ntation. In European Conference on Computer Vision , pages 1–17.\nSpringer, 2020.\n57. Yang Zhang, Moyun Liu, Jingwu He, Fei Pan, and Yanwen Guo.\nAffinity fusion graph-based framework for natural image segmenta-\ntion. IEEE Transactions on Multimedia, 2021.\n58. Chaofan Chen, Shengsheng Qian, Quan Fang, and Changsheng Xu.\nHapgn: Hierarchical attentive pooling graph network for point cloud\nsegmentation. IEEE Transactions on Multimedia , 23:2335–2346,\n2020.\n59. Yanfei Su, Weiquan Liu, Zhimin Yuan, Ming Cheng, Zhihong Zhang,\nXuelun Shen, and Cheng Wang. Dla-net: Learning dual local attention\nfeatures for semantic segmentation of large-scale building facade point\nclouds. Pattern Recognition, 123:108372, 2022.\n60. Yiding Liu, Siyu Yang, Bin Li, Wengang Zhou, Jizheng Xu, Houqiang\nLi, and Yan Lu. A ffinity derivation and graph merge for instance seg-\nmentation. In European Conference on Computer Vision, pages 686–\n703, 2018.\n61. Ziwei Zhang, Peng Cui, and Wenwu Zhu. Deep learning on graphs:\nA survey. IEEE Transactions on Knowledge and Data Engineering ,\n2020.\n62. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi\nZhang, and S Yu Philip. A comprehensive survey on graph neural\nnetworks. IEEE transactions on neural networks and learning systems,\n32(1):4–24, 2020.\n63. William L Hamilton, Rex Ying, and Jure Leskovec. Inductive represen-\ntation learning on large graphs. In Conference on Neural Information\nProcessing Systems, pages 1025–1035. MIT Press, 2017.\n64. Thomas N Kipf and Max Welling. Semi-supervised classification with\ngraph convolutional networks. In International Conference on Learn-\ning Representations. OpenReview.net, 2017.\n65. Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen\nLin, and Han Hu. Disentangled non-local neural networks. In Euro-\npean Conference on Computer Vision, pages 191–207. Springer, 2020.\n66. Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.\nNon-local neural networks. In Computer Vision and Pattern Recog-\nnition, pages 7794–7803. IEEE, 2018.\n67. Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan Yang.\nDenseaspp for semantic segmentation in street scenes. In Computer\nVision and Pattern Recognition, pages 3684–3692. IEEE, 2018.\n68. Liang-Chieh Chen, Maxwell D Collins, Yukun Zhu, George Papan-\ndreou, Barret Zoph, Florian Schro ff, Hartwig Adam, and Jonathon\nShlens. Searching for e fficient multi-scale architectures for dense im-\nage prediction. arXiv preprint arXiv:1809.04184, 2018.\n69. Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S\nHuang, Hartwig Adam, and Liang-Chieh Chen. Panoptic-deeplab:\nA simple, strong, and fast baseline for bottom-up panoptic segmen-\ntation. In Computer Vision and Pattern Recognition , pages 12475–\n12485. IEEE, 2020.\n70. Qibin Hou, Li Zhang, Ming-Ming Cheng, and Jiashi Feng. Strip pool-\ning: Rethinking spatial pooling for scene parsing. In Computer Vision\nand Pattern Recognition, pages 4003–4012. IEEE, 2020.\n71. Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu,\nand Nong Sang. Bisenet: Bilateral segmentation network for real-time\nsemantic segmentation. In European Conference on Computer Vision,\npages 325–341. Springer, 2018.\n72. Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change\nLoy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise spatial attention net-\nwork for scene parsing. In European Conference on Computer Vision,\npages 267–283. Springer, 2018.\n73. Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen,\nand Jingdong Wang. Ocnet: Object context network for scene parsing.\narXiv preprint arXiv:1809.00916, 2018.\n74. Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun.\nUnified perceptual parsing for scene understanding. In European Con-\nference on Computer Vision, pages 418–434. Springer, 2018.\n75. Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jinhui Tang,\nand Hanqing Lu. Adaptive context network for scene parsing. In\nIEEE International Conference on Computer Vision , pages 6748–\n6757. IEEE, 2019.\n76. Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun.\nUnified perceptual parsing for scene understanding. In Proceedings\nof the European Conference on Computer Vision (ECCV), pages 418–\n434, 2018.\n77. Ye Huang, Di Kang, Liang Chen, Xuefei Zhe, Wenjing Jia, Xiangjian\nHe, and Linchao Bao. Car: Class-aware regularizations for semantic\nsegmentation. arXiv preprint arXiv:2203.07160, 2022.\n78. Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen Lin, and\nHong Liu. Expectation-maximization attention networks for seman-\ntic segmentation. In Computer Vision and Pattern Recognition, pages\n9167–9176. IEEE, 2019.\n79. Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and Gang\nWang. Semantic correlation promoted shape-variant context for seg-\nmentation. In Computer Vision and Pattern Recognition, pages 8885–\n8894. IEEE, 2019.\n80. Xiangtai Li, Houlong Zhao, Lei Han, Yunhai Tong, Shaohua Tan, and\nKuiyuan Yang. Gated fully fusion for semantic segmentation. In AAAI\nConference on Artificial Intelligence, volume 34, pages 11418–11425,\n2020.\nZizhang Wu received the B.Sc. and\nM.Sc. degree in Pattern Recognition\nand Intelligent Systems from North-\neastern University in 2010 and 2012,\nrespectively. He is currently a percep-\ntion algorithm manager in the Com-\nputer Vision Perception Department of\nFront. Comput. Sci. 2022, 0(0): 1-14 15\nZongMu Technology. He is mainly re-\nsponsible for the development of core perception algorithms, opti-\nmizing algorithms, and model effects, and driving business develop-\nment with technology.\nYuanzhu Ganreceived the M.Sc. de-\ngree in Pattern Recognition and Artifi-\ncial Intelligence from Nanjing Univer-\nsity, Jiangsu, China, in 2021. He is\nnow an algorithm engineer at ZongMu\nTechnology, Shanghai, China. His cur-\nrent interests include 3D object detec-\ntion for autonomous driving.\nTianhao Xu received the B.E. degree\nin Vehicle Engineering from Jilin Uni-\nversity, Jilin, China, in 2017. He is\ncurrently pursuing the M.Sc. degree in\nElectric Mobility at Technical Univer-\nsity of Braunschweig, Braunschweig,\nGermany. His current research inter-\nests include computer vision and mi-\ncrostructural analysis in machine learning.\nFan Wang received the B.Sc. and\nM.Sc. degree in Computer Science and\nArtificial Intelligence from Northwest-\nern Polytechnical University, Xi’an,\nChina, in 1997 and 2000, respectively.\nHe is currently with ZongMu Technol-\nogy as a Vice President and Chief Tech-\nnology Officer. His current research in-\nterests include Computer Vision, Sensor Fusion, Automatic Parking,\nPlanning & Control, and L2/L3/L4 Autonomous Driving.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7988505363464355
    },
    {
      "name": "Segmentation",
      "score": 0.6224452257156372
    },
    {
      "name": "Transformer",
      "score": 0.57733154296875
    },
    {
      "name": "Pixel",
      "score": 0.5133963823318481
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4823179244995117
    },
    {
      "name": "Graph",
      "score": 0.4676012098789215
    },
    {
      "name": "Theoretical computer science",
      "score": 0.42570897936820984
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3402074873447418
    },
    {
      "name": "Computer vision",
      "score": 0.3351368308067322
    },
    {
      "name": "Voltage",
      "score": 0.09639927744865417
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 14
}