{
  "title": "The FAIIR conversational AI agent assistant for youth mental health service provision",
  "url": "https://openalex.org/W4410050779",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A3046444134",
      "name": "Stephen Obadinma",
      "affiliations": [
        "Queen's University",
        "Schwartz/Reisman Emergency Medicine Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5098954207",
      "name": "Alia Lachana",
      "affiliations": [
        "Humber College"
      ]
    },
    {
      "id": "https://openalex.org/A3093292363",
      "name": "Maia Leigh Norman",
      "affiliations": [
        "University of Waterloo",
        "Schwartz/Reisman Emergency Medicine Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2345070281",
      "name": "Jocelyn Rankin",
      "affiliations": [
        "Trent University"
      ]
    },
    {
      "id": "https://openalex.org/A2167470070",
      "name": "Joanna Yu",
      "affiliations": [
        "Schwartz/Reisman Emergency Medicine Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2128517930",
      "name": "Xiaodan Zhu",
      "affiliations": [
        "Schwartz/Reisman Emergency Medicine Institute",
        "Queen's University"
      ]
    },
    {
      "id": "https://openalex.org/A5040146145",
      "name": "Darren Mastropaolo",
      "affiliations": [
        "Trent University"
      ]
    },
    {
      "id": "https://openalex.org/A1995647806",
      "name": "Deval Pandya",
      "affiliations": [
        "Schwartz/Reisman Emergency Medicine Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2142567753",
      "name": "Roxana Sultan",
      "affiliations": [
        "University of New Brunswick",
        "Schwartz/Reisman Emergency Medicine Institute",
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A2460591431",
      "name": "Elham Dolatabadi",
      "affiliations": [
        "University of New Brunswick",
        "University of Toronto",
        "Schwartz/Reisman Emergency Medicine Institute",
        "York University"
      ]
    },
    {
      "id": "https://openalex.org/A3046444134",
      "name": "Stephen Obadinma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5098954207",
      "name": "Alia Lachana",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3093292363",
      "name": "Maia Leigh Norman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2345070281",
      "name": "Jocelyn Rankin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2167470070",
      "name": "Joanna Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128517930",
      "name": "Xiaodan Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5040146145",
      "name": "Darren Mastropaolo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1995647806",
      "name": "Deval Pandya",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2142567753",
      "name": "Roxana Sultan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2460591431",
      "name": "Elham Dolatabadi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3016794806",
    "https://openalex.org/W4302767823",
    "https://openalex.org/W2117815928",
    "https://openalex.org/W2891583124",
    "https://openalex.org/W2999286925",
    "https://openalex.org/W2084551014",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3089862415",
    "https://openalex.org/W4379469539",
    "https://openalex.org/W4386588264",
    "https://openalex.org/W4366307907",
    "https://openalex.org/W4210677910",
    "https://openalex.org/W3042196038",
    "https://openalex.org/W4320933343",
    "https://openalex.org/W2987607344",
    "https://openalex.org/W2622827166",
    "https://openalex.org/W4388856024",
    "https://openalex.org/W4385574234",
    "https://openalex.org/W6600175266",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4385570792",
    "https://openalex.org/W4385570547",
    "https://openalex.org/W6636364444",
    "https://openalex.org/W4385573298",
    "https://openalex.org/W4206499790",
    "https://openalex.org/W4366735820",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2594633041"
  ],
  "abstract": "Frontline crisis support plays a critical role in youth mental health services, where Crisis Responders (CRs) engage in conversations and assign issue tags to guide interventions. To enhance this process, we introduce FAIIR (Frontline Assistant: Issue Identification and Recommendation), an ensemble of domain-adapted transformer models trained on 780,000 conversations. FAIIR aims to reduce CR's cognitive burden, enhance issue identification accuracy, and streamline post-conversation administrative tasks. Evaluated on retrospective data, FAIIR achieves an average AUC ROC of 94%, an average F1-score of 64%, and an average recall score of 81%. During the silent testing phase, its performance remained robust, with less than a 2% drop in all metrics. CRs exhibited 90.9% agreement with its predictions, and expert agreement with FAIIR exceeded their agreement with original labels. These findings highlight FAIIR's potential to assist CRs in prioritizing urgent cases and ensuring appropriate resource allocation in crisis interventions.",
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01647-6\nThe FAIIR conversational AI agent\nassistant for youth mental health service\nprovision\nCheck for updates\nStephen Obadinma1,2 , Alia Lachana3,8,M a i aL e i g hN o r m a n2,4,8, Jocelyn Rankin5,8,J o a n n aY u2,8,\nXiaodan Zhu1,2, Darren Mastropaolo5,D e v a lP a n d y a2, Roxana Sultan2,6 & Elham Dolatabadi2,6,7\nFrontline crisis support plays a critical role in youth mental health services, where Crisis Responders\n(CRs) engage in conversations and assign issue tags to guide interventions. To enhance this process,\nwe introduce FAIIR (Frontline Assistant: Issue Identiﬁcation and Recommendation), an ensemble of\ndomain-adapted transformer models trained on 780,000 conversations. FAIIR aims to reduce CR’s\ncognitive burden, enhance issue identiﬁcation accuracy, and streamline post-conversation\nadministrative tasks. Evaluated on retrospective data, FAIIR achieves an average AUC ROC of 94%,\nan average F1-score of 64%, and an average recall score of 81%. During the silent testing phase, its\nperformance remained robust, with less than a 2% drop in all metrics. CRs exhibited 90.9% agreement\nwith its predictions, and expert agreement with FAIIR exceeded their agreement with original labels.\nThese ﬁndings highlight FAIIR’s potential to assist CRs in prioritizing urgent cases and ensuring\nappropriate resource allocation in crisis interventions.\nGlobally, one in seven young individuals aged 10–19 years old experience a\nmental health condition, making a signiﬁcant contribution to the global\nburden of disability and disease1. Suicide ranks as the fourth leading cause of\ndeath among 15–29 year olds, and in Canada, one inﬁve individuals will\nexperience a mental illness by age 251,2. Despite 70% of mental illness\nstarting during childhood or adolescence, only a fraction of young indivi-\nduals are able to access appropriate care, leading to increasing rates of youth\nhospitalizations for mental health disorders\n2–4.\nCrisis support conversations are a critical component of mental health\nservices, providing immediate, accessible, and empathetic support to youth\nin distress5,6. This approach offers signiﬁcant beneﬁts, including early\nintervention, reduced healthcare burdens, and the potential to de-escalate\nlife-threatening situations. However,several challenges persist, including\nthe high demand for these services which is particularly evident in the\nvolume of text conversations received by Kids Help Phone (KHP), a\nCanadian not-for-proﬁt e-mental health organization. Since the launch of\nits text service in 2018, KHP has facilitated over 1 million Short Message\nService (SMS) interactions, with a signiﬁcant 51% increase observed during\nthe COVID-19 pandemic in 2020. This heightened demand underscores the\nnecessity of expanding the team of Crisis Responders (CRs), composed of\nboth professionals and trained volunteers. The complexity of the issues\ndiscussed by youth during these conversations and the cognitive burden on\nCRs managing emotionally stressed individuals in potentially life-critical\nsituations\n7 add further challenges to crisis support services. Processing\nnuances in youth natural language from diverse populations and inter-\nsectionalities is not straightforward. Moreover, CRs must complete post-\nconversation surveys to identify key issues such as suicide and abuse, further\nadding to their workload and time commitment.\nIn this study, we focus on building an efﬁcient and scalable support\nmechanism leveraging Natural Language Processing (NLP) approaches to\nassist CRs. We utilize one of the largestcrisis support conversational data-\nsets, comprising 780,000 interactions by KHP. Our aim is to reduce CR’s\ncognitive burden, improve the accuracy of issue identiﬁcation, and\nstreamline post-conversation administrative tasks. Dramatic recent\nimprovements in the performance and availability of state-of-the-art large\nlanguage models (LLMs)\n8–10 have accelerated development and application\nof NLP tools to augment health services, alongside human experts11–14.I nt h e\nmental health context, NLP tools have been developed to identify signs of\ndepression\n15–17 and suicidal intentions18 within social media posts. In\naddition, they can identifythemes related to suicide19 and mental health\n1Electrical and Computer Engineering, Queen’s University, 99 University Ave, Kingston, ON, Canada.2Vector Institute, W1140-108 College Street, Schwartz\nReisman Innovation Campus, Toronto, ON, Canada.3Humber College, 205 Humber College Boulevard, Toronto, ON, Canada.4University of Waterloo, 200\nUniversity Ave W, Waterloo, ON, Canada.5Kids Help Phone, 439 University Avenue, Toronto, ON, Canada.6University of Toronto, 27 King’s College Cir, Toronto,\nON, Canada.7York University, 4700 Keele St, Toronto, ON, Canada.8These authors contributed equally: Alia Lachana, Maia Leigh Norman, Jocelyn Rankin,\nJoanna Yu. e-mail: 16sco@queensu.ca; edolatab@yorku.ca\nnpj Digital Medicine|           (2025) 8:243 1\n1234567890():,;\n1234567890():,;\nstatus20 from clinical notes. These tools are also utilized to aid in triage and\nreduce wait times on message-based suicide support platforms21. These real-\nworld applications highlight NLP’s potential in augmenting mental health\nsupport across both clinical and non-clinical communication channels.\nTo this end, we developed a human-centric classiﬁcation and infor-\nmation retrieval model called“Frontline Assistant: Issue Identiﬁcation and\nRecommendation” (FAIIR). The FAIIR tool inherits intent identiﬁcation\noutlined in the Neural Agent Assistant framework, designed to improve AI-\nenabled conversational tasks\n22, as well as state-of-the-art transformer-based\nmodels built for long conversations23. The FAIIR development processﬁrst\nincluded optimizing andﬁne-tuning a suite of transformer models that\nclassify conversations into a predeﬁned set of 19 clinically-orientated care\nparadigms, referred to as issue tags, such as suicidality and abuse (refer to\ntags deﬁned in Supplementary Table 1). Following this, the process\nencompassed ensemble techniques and human-in-the-loop validation with\nactive engagement of CRs. CRs’ involvement was crucial for validation,\nconsensus-building, reﬁning performance, and ensuring the tool’s utility. To\nuphold responsible AI developmentstandards, we conducted thorough\nevaluations for bias and fairness. Finally, we presented the outcomes of our\ninitial silent trial, detailed our implementation plans, and explored addi-\ntional potential use cases for the FAIIR tool.\nResults\nCrisis support conversational dataset\nTo develop FAIIR, we leveraged 703,975 anonymized text-based crisis\nsupport conversations exchanged between service users and CRs from\nJanuary 2018 to February 2023. Each conversation consists of multiple\nmessage exchanges (multi-turn dialogs), making it essential for the model to\nprocess long-form text effectively. Identiﬁable information was scrubbed to\nensure privacy compliance. Conversations vary signiﬁcantly in length in\nterms of tokens. Tokens in this context refer to discrete units of text that\ntypically represent words, sub-words,characters, or punctuation obtained\nafter breaking a sequence of text down so that it can be processed by a model.\nThe average and median number of tokens per conversation were 913 and\n850 respectively (Fig.1f) with the majority of conversations (53%) between\n500 and 1500 tokens, and only a small number extending above 3000 (0.7%).\nGiven this distribution, we set a 2000-token maximum input length, cov-\nering 94.4% of all conversations (Fig.1f). An optional demographic survey\nwas completed by 17% of service users (59,603 survey respondents of a total\nof n = 340,512 overall service users), with responses originating mostly from\nindividuals identifying as female (75%), heterosexual (55%), and of Eur-\nopean ancestry (77.5%) as shown in Fig.1b–e. It is essential to note that only\na small number of overall service users completed the survey, and thus the\nresults do not fully represent the distribution or demographic features of\nservice users overall.\nFAIIR issue tag prediction\nFAIIR is a multi-label classiﬁcation model that classiﬁes conversations into\n19 predeﬁned issue categories, with it being possible for multiple issue\ncategories applying for a given conversation. However, the distribution of\nissue tags in the data is highly imbalanced (Fig.1a), with the most frequent\ntag, Anxiety/Stress, appearing in over 244,000 conversations, and the least\nfrequent,Prank, appearing in only 2800 conversations. Additionally, issue\ntagging varies across conversations: 53.73% of conversations are assigned a\nsingle issue tag, while 46% contain between 2 and 9 issue tags, reﬂecting the\ncomplexity of the challengesfaced by service users (Fig.1g).\nRegarding risk levels, the majority of conversations were classiﬁed as\nhigh-risk (13%) or medium-risk (87%) (Fig.1h), based on the methodology\ndetailed in the“Methods” section:“Priorityﬂag pre-processing”.\nEnsemble of Longformer models excelled in FAIIR issue tag\npredictions\nWe ﬁrst compared four transformer-based LLMs, eachﬁne-tuned on a\ndataset containing 50,000 randomly sampled conversations (see“Methods”\nsection: “Process of selecting model architecture”). Transformer-based\nLLMs24 are advanced deep learning models designed to analyze large\nvolumes of text, such as patient-provider conversations, and identify pat-\nterns or extract insights. Among the models tested, Longformer demon-\nstrated the best performance, excellingaccuracy and ability to effectively\nhandle long conversations without losing contextual meaning.\nGiven Longformer’s superior performance, the backend of the FAIIR\nt o o lw a sb u i l tu s i n ga ne n s e m b l eo ft h r e eL o n g f o r m e r\n9 models which\ninvolves combining the predictions of separate models with slightly varied\ninitialization andﬁne-tuning processes, aiming to enhance overall accuracy\nand reliability. These models were adapted to the mental health support\ndomain through the technique of masked language modeling, a training\nprocess which aids in model understanding of speciﬁc language, termi-\nnology, and the context of words by leveraging the entire dataset to get the\nmodel to learn to predict masked tokens.\nAfter domain adaptation, the models were furtherﬁne-tuned for the\nmulti-label classiﬁcation task. This task involved training the models to\nidentify multiple relevant issues athand (see more details about predeﬁned\nissue tags in Supplementary Table 1). Theﬁne-tuning was conducted using a\nlarger dataset of 563,180 conversations— 422,385 used for training, and\n140,795 for validation. The models were then evaluated retrospectively on\nan independent test set of 140,795 conversations to conﬁrm their perfor-\nmance in a real-world context.\nThe performance of the ensemble model on the test set, broken down\nby individual issue tags, is detailed in Table1, while the average performance\nacross all issue tags is shown in Fig.2a. The majority of the area under the\nreceiver operating characteristic curve (AUC ROC) scores exceeded 0.9,\nindicating strong overall model performance, with the lowest score\nbeing 0.74.\nDue to the imbalanced nature of the data, whereby certain issue tags are\nunder- or over-represented, precision (proportion of correct positive pre-\ndictions) is relatively low (<0.65). However, recall (proportion of true\npositives identiﬁed correctly) remains high (<0.9), alongside overall accu-\nracy (0.94). This means that the model is highly effective at identifying\nrelevant tags, but may occasionally also identify irrelevant issues in con-\nversation, especially for less common topics.\nClassiﬁcation thresholds deﬁne the level of conﬁdence a model must\nhave before assigning an issue tag. The model generates 19 conﬁdence\nscores-one for each issue label-based onthe probability that a label applies to\na given conversation. A threshold is then used to set the minimum con-\nﬁdence score required to deﬁnitively assign a label. When selecting the\noptimal threshold for our context, a threshold of 0.25 was found to strike the\nbest balance between precision and recall, thereby allowing the model to\ncapture critical issues, even if irrelevant tags are sometimes included. At this\nthreshold, the sample average F1-score (a combined measure of precision\nand recall) was 0.64, with a sample average recall of 0.81 and a sample\naverage precision of 0.58. The trade-off between recall and precision is\nacceptable within this context, as it prioritizes capturing critical issues, even\nif some irrelevant issue tags are also included.\nThe model performed exceptionally well in identifying high-priority\nissues such as Suicide (F1-score = 0.73),Depression (F1-score = 0.75),\nRelationship Problems (F1-score = 0.73),Self Harm (F1-score = 0.69),\nAnxiety/Stress (F1-score = 0.69), andThird-Party conversations (F1-\nscore = 0.76). However, model performance was relatively poorer for rarer\nissues, including Other (F1-score = 0.35),Abuse, Emotional (F1-score =\n0.46), Abuse, Physical(F1-score = 0.47),Isolated (F1-score = 0.56),Prank\n(F1-score = 0.45), andTesting (F1-score = 0.53). These results suggest that\nwhile the model is effective at identifying common or critical issues, addi-\ntional reﬁnement may be required to improve performance across less\nfrequent categories.\nFAIIR tool predictions observed to be unbiased across demo-\ngraphic subgroups\nTable 2 demonstrates the performance (precision, recall, F1-score, and\naccuracy) of the FAIIR tool in predicting issue tags within 27 distinct sub-\ngroups across four demographic categories, representing 17% of overall\nhttps://doi.org/10.1038/s41746-025-01647-6 Article\nnpj Digital Medicine|           (2025) 8:243 2\nservice users (see distribution in Fig.1b–e). The standard deviation of F1-\nscores within each demographic catego r yi sl e s st h a n0 . 0 2 5( f o rG e n d e r :\n±0.023, Orientation: ±0.010, Identity: ±0.018, and Ethnicity: ±0.024), indi-\ncating a consistent model performance with a narrow gap across varying\ndemographic subgroups. The results of a one-samplet-test (p value <0.001)\nshow that there is no signiﬁcant difference between the F1-scores of indi-\nvidual demographic subgroups and the overall performance of the model.\nThis indicates that the model performs consistently across different\ndemographic groups, and any differences in scores are unlikely due to\nsampling bias.\nExpert assessment and evaluation of FAIIR predictions\nIn total, we solicited 240 annotations from 12 experts, with each con-\nversation undergoing independent assessment by three distinct assessors\nfor open review and another three for blind review. The two types of\nreview differed in the presentation of the FAIIR predictions: in the open\nreview (Supplementary Table 3), assessors were provided with FAIIR’s\npredicted issue tags as a reference, whereas in the blind review, assessors\nidentiﬁed issue tags independently without any exposure to FAIIR’s\npredictions.\nFAIIR’s predictions validated by human experts\nFor results of the blind assessment demonstrated in Fig.3, on average across\n40 conversations, 90.9% agreement (lowest: 33% and highest 100%) was\nachieved overall between CRs and the FAIIR tool, where FAIIR was able to\npredict 165 issue tags and by majority agreement it only missed 13 tags.\nAmong all issue tags, agreement was reached in the majority of instances of\nAnxiety/Stress, Bully, Relationship, 3rd Party, Suicide,a n dAbuse, Emotional;\nwith more frequent discordance for the issue tags ofGrief, Self Harm, Abuse,\nPhysical; Other,a n dEating Body Image.\nFig. 1 | Dataset statistics. a703,975 youth conversations with frontline crisis\nresponders are classiﬁed into 19 pre-deﬁned issue tags. Multiple tags may be\nassigned per conversation, as relevant. After each interaction, service users are\ninvited to complete a demographic survey, gauging the conversation’s helpfulness\nand the individual’s demographics (age range, ethnicity, and identiﬁcation with\nspeciﬁc identity groups).b The distribution of gender identity in the aggregated\nsurveys. c The distribution of sexual orientation of the aggregated surveys.d The\ndistribution of identiﬁed groups in the aggregated surveys.e The distribution of\nethnicity in the aggregated surveys.f Distribution of conversation lengths (# tokens).\ng Distribution of the number of issue tags assigned to conversations.h Distribution\nof priority labels assigned to conversations.\nhttps://doi.org/10.1038/s41746-025-01647-6 Article\nnpj Digital Medicine|           (2025) 8:243 3\nTable 1 | Theﬁne-grained performance of the FAIIR tool, an ensemble of three Longformer models, is reported for two datasets: a retrospective test set (n = 140,795) and a\nprospective silent testing set (n = 84,832)\n3rd party Abuse,\nemotional\nAbuse,\nphysical\nAbuse,\nsexual\nAnxiety/\nstress\nBully Depressed DNE Eating\nbody\nimage\nGender/\nsexual\nidentity\nGrief Isolated Other Prank Relationship Self harm Substance\nabuse\nSuicide Testing\nTest set\nAUC ROC 0.99 0.96 0.98 0.99 0.87 0.96 0.96 0.85 0.98 0.99 0.95 0.87 0.74 0.96 0.9 0.97 0.97 0.95 0.97\nThreshold\n— 0.25\nPrecision 0.64 0.35 0.33 0.47 0.61 0.3 0.66 0.51 0.51 0.52 0.34 0.47 0.28 0.34 0.63 0.55 0.32 0.63 0.41\nRecall 0.95 0.7 0.79 0.91 0.81 0.79 0.86 0.81 0.88 0.92 0.83 0.7 0.45 0.66 0.85 0.92 0.86 0.87 0.73\nF1 0.76 0.46 0.47 0.62 0.69 0.43 0.75 0.62 0.64 0.67 0.48 0.56 0.35 0.45 0.73 0.69 0.46 0.73 0.53\nThreshold\n— 0.5\nPrecision 0.69 0.53 0.51 0.57 0.79 0.46 0.79 0.67 0.64 0.6 0.45 0.66 0.4 0.47 0.77 0.64 0.42 0.75 0.61\nRecall 0.92 0.5 0.63 0.82 0.56 0.66 0.75 0.51 0.79 0.87 0.75 0.4 0.23 0.55 0.66 0.84 0.75 0.73 0.59\nF1 0.79 0.51 0.56 0.67 0.66 0.54 0.77 0.58 0.71 0.71 0.56 0.5 0.29 0.51 0.71 0.73 0.54 0.74 0.6\nSilent testing\nAUC ROC 0.99 0.96 0.99 0.99 0.86 0.97 0.95 0.85 0.98 0.98 0.95 0.87 0.73 0.97 0.9 0.97 0.97 0.94 0.95\nThreshold\n— 0.25\nPrecision 0.58 0.45 0.48 0.52 0.76 0.43 0.8 0.61 0.58 0.48 0.44 0.62 0.43 0.37 0.75 0.63 0.44 0.72 0.48\nRecall 0.9 0.48 0.7 0.82 0.53 0.71 0.77 0.47 0.8 0.84 0.75 0.38 0.21 0.46 0.67 0.85 0.79 0.74 0.48\nF1 0.7 0.46 0.57 0.63 0.62 0.54 0.79 0.53 0.67 0.61 0.56 0.47 0.28 0.41 0.71 0.72 0.57 0.73 0.48\nThreshold\n— 0.5\nPrecision 0.52 0.29 0.3 0.41 0.58 0.29 0.69 0.43 0.44 0.42 0.33 0.43 0.33 0.26 0.63 0.55 0.34 0.59 0.27\nRecall 0.94 0.69 0.84 0.91 0.79 0.86 0.86 0.78 0.88 0.89 0.82 0.67 0.43 0.57 0.86 0.92 0.87 0.87 0.62\nF1 0.67 0.41 0.44 0.57 0.67 0.43 0.77 0.56 0.58 0.57 0.47 0.52 0.37 0.36 0.72 0.68 0.49 0.7 0.38\nThreshold\n— updated\nPrecision 0.51 0.26 0.26 0.39 0.7 0.25 0.67 0.53 0.4 0.4 0.3 0.48 0.3 0.25 0.71 0.52 0.31 0.62 0.24\nRecall 0.94 0.74 0.86 0.92 0.63 0.88 0.87 0.6 0.89 0.89 0.84 0.6 0.49 0.61 0.75 0.93 0.89 0.85 0.66\nF1 0.66 0.38 0.4 0.55 0.66 0.39 0.76 0.56 0.56 0.55 0.44 0.53 0.37 0.35 0.73 0.67 0.46 0.71 0.35\nEach model was pre-trained andﬁne-tuned to identify multiple issue tags within conversations using the full training and validation set of 563,180 conversations. Results are provided as averages across all samples for classiﬁcation thresholds of 0.5, 0.25, and an updated\nthreshold (used only for the prospective silent testing dataset).\nhttps://doi.org/10.1038/s41746-025-01647-6 Article\nnpj Digital Medicine|           (2025) 8:243 4\nFAIIR understands the conversational context\nFigure 4 presents a comparison of consensus among the experts’ blind\nresponses, FAIIR tool prediction(s), and original issue tag(s) (referring to the\nissue tag(s) recorded in the training dataset, assigned following the original\nconversation). For all consensus comparison settings (Fig.4), the level of\nagreement between the FAIIR tool and expert responses (average precision\n0.62 ± 0.22, average recall 0.82 ± 0.13, and average F1-score 0.64 ± 0.11;\nrespectively) are higher than original annotations and expert responses\n(average precision 0.52 ± 0.18, average recall 0.56 ± 0.08, and average F1-\nscore 0.47 ± 0.07; respectively). Thus, when using expert responses as a\nreference for comparison, FAIIR predictions align more closely with the\nexperts’annotations than the original issue tags. The results of the unpaired\nt-test on the averaged measures reveal that the concordance between FAIIR\nand expert responses is signiﬁcantly higher (p value <0.001) compared to the\nconcordance between the original annotations and expert responses. Figure\n4 also demonstrates FAIIR’s ﬁnal performance after reﬁnement based on\nincorporating the experts’blind responses and adjusting the classiﬁcation\ndecision boundary to be less biased towards predicting the most common\ntags. Comparing the original threshold“FAIIR vs. Experts” and the updated\nthreshold (UT) “FAIIR (UT) vs. Experts” in Fig. 4, average precision\n(0.66 ± 0.2) is improved while average recall (0.76 ± 0.14) is decreased. F1-\nscore also improves for the full agreement on primary issue tags (0.53) and\npartial agreement via majority vote (0.6) settings, demonstrating beneﬁts in\nthe strictest consensus measures.\nFAIIR performance in silent trial consistent with\ndevelopment phase\nConversational data for the silent testing phase comprised 84,832 con-\nversations collected between February and September 2023. The distribu-\ntion of issue tags can be seen in Supplementary Fig. 2, demonstrating that the\nsilent testing dataset remains largely consistent with the data used for\ndeveloping the FAIIR tool. However, certain tags likeDNE are more\ncommon in the former dataset. In addition, this data naturally includes\nmore up-to-date topics and events of 2023 such as natural disasters and\npolitical crises. Table1 presents the evaluation performance of FAIIR pre-\ndictions for each issue tag and Fig.2b indicates averaged metrics across all\nissue tags for three thresholds: 0.25, 0.5, and the adjusted threshold based on\nexpert evaluation and assessment. An expected slight decrease in perfor-\nmance is noted and is attributable to the differences in distributions between\nthe development and silent testing datasets. Similar to the retrospective\ntesting setting, the majority of the AUC ROC scores are above 0.9, with the\nlowest score being 0.73. The sample averaged precision, recall, and F1 scores\nare 0.57, 0.79, and 0.62, respectively, for a threshold of 0.25, compared to\n0.58, 0.81, and 0.64 for the retrospective values, indicating a drop of\nless than 2%.\nDiscussion\nWe have successfully demonstrated the viability of employing an NLP-\nbased frontline assistant tool to augment CRs by identifying the issues that\nservice users may be experiencing in text-based support conversations. After\nanalyzing a textual conversation, the FAIIR tool is able to recommend\npotential issues from a list of 19 predeﬁned tags. FAIIR achieved an accuracy\nof 94%, a sample average F1-score of 64%, and a sample average recall score\nof 81% on the development set. Thisis a strong performance given the\ninherent subjectivity and noisiness of the data. Importantly, in the context of\nour fairness analyses, the FAIIR tool demonstrated equitable performance\nacross all demographic groups of service users.\nOur study demonstrates the robustness and generalizability of the\nFAIIR tool, which is built upon recent advances in LLMs. In our silent\ntesting phase, we observed less than a 2% drop in sample average precision,\nrecall and F1-score, demonstrating the tool’s strong potential for real-time\ndeployment.\nOur investigation revealed that domain adaptation through self-\nsupervised learning signiﬁcantly enhances tool performance, especially in\nsupervised tasks and when addressing label biases. Thisﬁnding is relevant to\nour study, where we noted potential biases in the original labeling process, as\neach conversation was labeled by a single annotator without subsequent\nreview. Consequently, a large annotated dataset for a supervised task with\nmultiple annotators per conversation would be optimal. However, given the\nscale of our study with over 780,000 individual conversations, extensive\nmanual annotation was not feasible. We sought expert assessment and\nevaluation for edge-case conversations to enrich our ground truth anno-\ntations and explore the beneﬁts of contextual learning. The experts’\nresponses showed an overall agreement of 90.9% with the FAIIR tool’s\npredictions. Notably, expert agreement with FAIIR exceeded their agree-\nment with the original labels. This observation can be attributed to the\nextensive self-supervised training ofFAIIR, which equipped the model with\na strong contextual understanding. This training allowed the model to grasp\nFig. 2 | FAIIR model performance. aAveraged performance of the FAIIR tool in\npredicting all 19 issue tags is shown for the retrospective test set (n = 140,795)\ncomparing thresholds of 0.5 (orange) and 0.25 (blue) as well as overall AUC ROC\nmodel performance (gray).b Averaged performance of the FAIIR tool across all issue\ntags in the silent testing prospective test sets (n = 84,932), evaluated using previous\ntwo classiﬁcation thresholds with the addition of an updated threshold (green). For\nsilent testing, results for the silent testing overlay retrospective results, with decreases\nin performance highlighted in red and gains shown in a lighter shade. The AUC ROC\nbar represents the average ability of the tool to distinguish between issue tags across\nall categories. The tool’s best overall performance is an F1-score of 0.64 on the\nretrospective test set and 0.62 on the silent testing prospective test set.\nhttps://doi.org/10.1038/s41746-025-01647-6 Article\nnpj Digital Medicine|           (2025) 8:243 5\nthe nuances and relationships between words and phrases within the con-\ntext. The expert evaluation not only demonstrated the strong performance\nof the model in context but also provided a valuable source of ground-truth\nissue tags for further model reﬁnement. After reﬁning the FAIIR tool by\nleveraging the blind survey outcomes, the precision metric exhibited a 4%\nincrease.\nThe distribution of issue tags in our dataset was highly imbalanced,\nwith the most frequent tag,Anxiety/Stress, appearing in more than 244,000\nconversations, while the least frequent tag,Prank, appears in only 2800\nconversations. This imbalance posed signiﬁcant challenges during our\nmodel’s training, which explains the relatively poorer performance of FAIIR\nacross less-represented issue tags. For instance, the model encountered\ndifﬁculties with applying tags such asOther, Abuse, Emotional; Abuse,\nPhysical; Isolated, Prank,a n dTesting. Some of these challenges also stem\nfrom the inherent vagueness of certain tags. For example,Isolatedcan apply\nto a broad spectrum of conversations, but its relevance may be selectively\napplied. TheOther tag presents similar difﬁculties, encompassing anything\noutside the scope of the set 19 issue tag categories. Tags likePrank and\nAbuse, Physical also suffered in performance due to their rarity in the\ndataset, making it challenging for the model to adequately learn to recognize\nthese instances. While we implemented imbalanced learning techniques\nsuch as re-weighting and balanced samples, further techniques may be\nnecessary to enhance performance. However, despite these challenges, the\ntool demonstrated exceptional performance, above that of random guessing.\nAnother challenge we encountered was the diversity and extensive\nlength across conversations in our dataset. This was largely due to the\ntherapeutic nature of the discussions, which primarily focused on mental\nhealth support and recovery. To manage this diversity, we established a\nmaximum token length of 2000, covering approximately 95% of all con-\nversations. This threshold not only accommodated the majority of our\nconversations, but also optimized the batch size for more efﬁcient training,\nthereby accelerating the training process. The decision to use a Longformer\nmodel was primarily inﬂuenced by its well-suitedness for our context: its\ndesign for longer sequences and our need to handle lengthy sequences\neffectively made this model the correct choice.\nAn additional obstacle we faced was the varying quality of conversa-\ntions, including differences in language use, grammar, and the presence of\nnoise such as typos or slang. Addressing these issues required extensive pre-\nTable 2 | The performance of FAIIR (with threshold 0.25) within subgroups of service users across four distinct demographic\ncategories\nPrecision Recall F1-score Accuracy # Conversations\nGender\nMale 0.56 0.86 0.64 0.90 1236\nFemale 0.57 0.86 0.65 0.90 5955\nTrans Male 0.56 0.85 0.64 0.90 173\nTrans Female 0.49 0.86 0.59 0.89 32\nNon-binary 0.56 0.85 0.63 0.90 501\nAgender 0.59 0.80 0.65 0.91 35\nOrientation\nHeterosexual 0.57 0.87 0.65 0.90 4042\nGay or Lesbian 0.58 0.85 0.66 0.91 516\nBisexual 0.56 0.85 0.64 0.90 1969\nAsexual 0.59 0.86 0.66 0.91 247\nUnsure 0.56 0.86 0.64 0.90 586\nIdentity\nCanadian Culture 0.61 0.84 0.67 0.91 184\nDisabled 0.55 0.91 0.63 0.90 19\nRefugee 0.55 0.82 0.63 0.90 96\nSpiritual 0.61 0.89 0.68 0.90 76\nDeaf 0.57 0.89 0.66 0.91 17\nFirst Nations 0.62 0.73 0.66 0.92 10\nInvisible Disability 0.56 0.85 0.64 0.90 1095\nOther 0.60 0.83 0.65 0.91 36\nPrefer not to Answer 0.56 0.85 0.64 0.90 1048\nEthnicity\nEuropean Ancestry 0.57 0.85 0.65 0.90 2376\nAfrican or Caribbean 0.51 0.85 0.61 0.90 134\nIndigenous 0.56 0.88 0.65 0.89 74\nEast or South-East Asian 0.55 0.86 0.63 0.90 190\nMiddle Eastern 0.53 0.87 0.62 0.89 73\nLatin American 0.55 0.88 0.64 0.90 86\nSouth Asian 0.56 0.85 0.64 0.90 87\nUnspeciﬁed 0.61 0.90 0.69 0.90 46\nDemographic information was provided by 17% of service users.\nhttps://doi.org/10.1038/s41746-025-01647-6 Article\nnpj Digital Medicine|           (2025) 8:243 6\nprocessing efforts, which introduced subjectivity and potential bias into\nthe data.\nIn conjunction with the identiﬁcation of primary issue tags, we\nestablished an explainability pipeline to facilitate the extraction of contextual\nkeywords, referred to as“natural keywords” from each conversation. These\nkeywords are dynamic tokens associated with the speciﬁc main issue tags in\na conversation. To streamline the processing of natural keywords and\nfacilitate communication with subject matter experts, we conducted word\nembedding visualization and bi-gram analysis at the aggregated level to\ndemonstrate semantic relationships and word proximity in a speciﬁcc o n -\ntext (see“Methods” section: “Outcome interpretation and visualization”).\nHowever, it is crucial to note that the reliability and meaningfulness of these\nnatural keywords require further rigorous assessment, of which our future\nwork will be comprised.\nOur study’s primary limitation is its reliance on a predeﬁned set of 19\nissue tags for the identiﬁcation of topics within a conversation. This lim-\nitation restricts the model’s ability to extract information beyond the pre-\ndeﬁned list. In conversations of this nature, CRs have identiﬁed their\neagerness to delve into dynamic and natural issue tags that are more youth-\ncentered, as opposed to being limited by this predeﬁned list. Such a dynamic\nm o d e lc o u l dp e r m i tf o rc h a n g eo v e rt i m eo rb a s e do nt h ec o n t e x to ft h e\nconversation, allowing for the toolto adapt to new issues it may not have\nencountered during training.\nOur immediate next step involvesdeploying the FAIIR tool for real-\ntime issue tag identiﬁcation. Simultaneously, we aim to enhance the pipeline\nby incorporating generative language models and decoder-based models to\nfurther improve dynamic issue tag predictions. Additionally, we hope to\nevaluate the usability and validity of the identiﬁed natural keywords via a\npanel of subject matter experts.\nGiven its positive performance and adaptiveness, the FAIIR tool\ndemonstrates promise in application to not only the use case of issue clas-\nsiﬁcation described at length in this paper, but also more broadly within the\nmental health support context. Directly relevant applications of the FAIIR\ntool may include streamlining triage processes for crisis lines by identifying\ntopics at hand, and increased robustness inﬂagging service users at risk. Via\nfuture works, we aim to explore the utility of the FAIIR tool in varied support\ncontexts, building towards our goal of supporting both users and providers\nin leveraging NLP tools to their beneﬁt.\nIn conclusion, the rising demand for youth mental healthcare and crisis\nsupport has become a pressing concern for both healthcare providers and\nusers. This growing need has prompted active efforts to develop and deploy\nsafe, trustworthy, and transparent conversational AI solutions that support\nFig. 3 | Results of expert blind review.Experts' blind review results presented in a\nmatrix format, whereby each row represents an issue tag and each column a con-\nversation. Three reviewers assess each conversation, providing feedback on the issue\ntags predicted by the FAIIR tool: indicating their agreement or disagreement, and\nidentifying missing tags, where applicable. Cells shaded in green indicate agreement\nbetween reviewer and model, while cells shaded in red represent missing tags. The\nletter “A” in the cell followed by a number indicates the total number of reviewers (of\nthree total) in agreement with model predictions. The letter“M” in the cell followed\nby a number indicates the total number of reviewers who believe this issue tag was\nmissed by the FAIIR tool.\nhttps://doi.org/10.1038/s41746-025-01647-6 Article\nnpj Digital Medicine|           (2025) 8:243 7\nproviders by reducing the administrative burden inherent to providing\nsupport and guidance to young individuals facing mental health challenges.\nOur study contributes to the ongoing exploration of solutions by show-\ncasing the development and evaluation of a front-line conversational agent\nassistant tool, while sharing lessons learned with the broader community. By\ndemonstrating the effectiveness and feasibility of such solutions, our study\npaves the way for broader adoption and implementation of conversational\nAI models in mental health and crisis support services.\nMethods\nStudy design and setting\nThe study comprised two phases. Theﬁrst phase involved building FAIIR\nfor the identiﬁcation of issue(s) that a young person might be experiencing\nfrom their textual conversations with trained CRs. Identiﬁcation was per-\nformed using a list of 19 predeﬁned issue tags. This phase laid the foundation\nfor our work, where we diligently developed,ﬁne-tuned, and evaluated\nFAIIR’s capacity as an NLP tool to understand and predict issues. In the\nsecond phase, we validated the model’se fﬁcacy and accuracy through\ntesting with domain experts and silent testing. This phase conﬁrmed the\npractical applicability and real-world utility of FAIIR for CRs. Both phases\nutilized conversations related to crisis support services at KHP.\nCuration of study dataset\nThe primary conversational dataset used for building and evaluating the\nNLP models of FAIIR was comprised of 703,975 unique, scrubbed, multi-\nturn dialog instances between service users and CRs via SMS from January\n2018 until February 2023. An additionalbatch of 84,832 conversations from\nFebruary to September 2023 was used for silent testing. It is important to\nnote that some of these dialogs may originate from service users who engage\nin multiple instances of interaction with CRs; however, encounters from the\nsame individual are not linked. In total, the training data represented con-\nversations with 340,512 individual service users and 7937 CRs. The silent\ntesting data represented 57,031 unique service users and 2038 CRs, with\nexpected overlap between the individuals represented in both datasets.\nAt the end of each conversation, service users are asked toﬁll out an\noptional demographic survey. The demographic survey captures informa-\ntion including the helpfulness of the conversation to the user and demo-\ngraphics including their age range, ethnicity or cultural group, identiﬁcation\nwith any of ten identity groups (e.g., newcomer, refugee, deaf, blind, people\nwith disabilities), and setting of current living (i.e., city, rural area, First\nNations Reserve). Approximately 17%of service users typically complete\nthis survey, most of whom identify as female, heterosexual and of European\nancestry. Most conversations areﬂagged as medium-risk as shown in Fig.1h\nwith the distribution of priority labels across the main conversational\ndataset, according to the priorityﬂagging methods further described below.\nA total of 19 pre-deﬁned issue tags currently serve to describe the range\nof topics raised by a user during a conversation, including topics such as\nDepressed, Anxiety/Stress,a n dGender/Sexual Identity. Upon conversation\nconclusion, a CR manually assigns at least one available issue tag(s) to the\nconversation. Metrics related to tags are used in aggregate for insight gen-\neration, to best follow trends in youth issues, support CRs, as well as\nreporting to funders and other agencies. It is important to note that this\nlabeling process is carried out by CRs attheir own discretion, and according\nto their training. Due to limited resources and large volumes of service user\ninquiries, issue tags typically do not undergo additional review.\nData was anonymized, undergoing a process of scrubbing identifying\ninformation such as names and locations, which were automatically\nreplaced with the placeholder[scrubbed]. In many instances, complete\nphrases and sentences were scrubbed due to the anonymization process.\nThis process therefore introducedsome noise due to the unintentional\nremoval of harmless words, liketurkey.\nPriority ﬂag pre-processing\nAt the start of each conversation, the system generates a priorityﬂag based\non the service user’s ﬁrst few words. Service users are then triaged into\ncategories of either high, medium, low-risk, or“no ground truth” via an\nalgorithm owned by Crisis Text Line. Medium risk is assigned when a user\nexpresses suicidal thoughts or self harm, and high risk is assigned when an\nindividual is deemed to be an“imminent risk”,d eﬁned as having a com-\nbination of suicidal thoughts, a plan, access to means, and a 0–48 h timeline\nto end their life. The presence of any 56 English or 73 French words in an\ninitial message from a user leads to their automatic triage to a higher priority\nlevel. According to the distribution in Fig.1h (main text), the vast majority of\nconversations (87%) wereﬂagged as medium-risk, with about 13% being\nﬂagged as high risk. Almost no conversations (0.0001%) wereﬂagged as\nlow-risk.\nFig. 4 | Issue consensus overview.Comparison of consensus among expert\nresponses, FAIIR tool predictions, and original annotations from open review.\na Precision, b recall, andc F1-score measures were averaged across all issue tags and\nconversations. FA: Primarydenotes full agreement on primary issue tags,PA: Pri-\nmary Maj. denotes partial agreement on primary issue tags via majority vote,PA:\nPrimary and Secondary Maj. denotes partial agreement on primary and secondary\nissue tags via majority vote;FA: Primary (greater than or equal to) 1denotes full\nagreement on primary issue tags via at least one vote; andFA: Primary and Secondary\n(greater than or equal to) 1denotes full agreement on primary and secondary issue\ntags via at least one vote.Average denotes the average performance across allﬁve\nconsensus criteria. One samplet-test was conducted to assess the statistical sig-\nniﬁcance between average and FAIIR tool vs. original annotations (identiﬁed by two\nasterisks). The consensus among expert responses and FAIIR predictions after\nupdating the threshold in accordance with expert assessment can be seen in the\nFAIIR (UT) vs. Expertsbars.\nhttps://doi.org/10.1038/s41746-025-01647-6 Article\nnpj Digital Medicine|           (2025) 8:243 8\nTo assess how the FAIIR model performs across the different\nassigned priority levels, similar to Table1, we collected ﬁne-grained\nperformance of the main FAIIR tool across our main metrics on the\nretrospective test set (n = 140,795). We divided the conversations in the\ntest set into two main priority levels (“Medium” and “High”) and report\nthe results across the two thresholds (0.25 and 0.5) in Supplementary\nTable 4. We observe that performance across the two risk categories has\nlittle variation, meaning there is little bias towards conversations\naccording to priority and that the model is able to handle these different\nlevels accordingly.\nDevelopment of the FAIIR tool\nWe framed the issue tag identiﬁcation task as a multi-label classiﬁcation\nproblem, where multiple labels can be assigned to a single instance. The\ndevelopment of the classiﬁer followed two distinct stages. In theﬁrst\nstage, we compared and evaluated various pre-trained transformer-\nbased language models,ﬁne-tuning them on a randomly selected subset\nof the data for classiﬁcation. Pre-training involves training models on\nlarge-scale text corpora to learn general language patterns, whileﬁne-\ntuning adapts these models to a speciﬁc task using a smaller, task-\nspeciﬁc dataset. The second step involved domain adaptation, reﬁning\nthe model to better capture the nuances of youth mental health con-\nversations. This was achieved through additional pre-training andﬁne-\ntuning on the full baseline training dataset, ensuring the classi ﬁer\neffectively recognized context-speci ﬁc language patterns and issues\nrelevant to the domain.\nDevelopment step 1: model comparisons\nWe explored four primary variants of transformer models for processing\nlengthy documents and task-oriented conversational data. These models fall\ninto two categories of “encoder-only” models— designed primarily for\nclassiﬁcation tasks and“encoder-decoder” models— which process input\ntext using an encoder and generate output using a decoder\n24.T h em o d e l s\nevaluated included Longformer9, an encoder-only model with 149M para-\nmeters, Conversational BERT, an encoder-only model with 110 M\nparameters8,25, DialogLED, an encoder-decoder model with 139 M\nparameters26, and MVP (Multi-task superVised Pre-training), an encoder-\ndecoder model with 406M parameters27.D u r i n gﬁne-tuning, these models,\nwe incorporated a classiﬁcation head, a single-layer neural network that\nconverts the model’s output into probability scores for each class label. In\nencoder-only models, this layer was applied to the [CLS] token, which\nrepresents the entire input. For encoder-decoder models, classiﬁcation was\nbased on the [EOS] token (DialogLED) or theﬁrst token in the sequence\n(MVP), following established conventions\n28.\nAll four models wereﬁne-tuned on 50,000 conversations randomly\nsampled from the dataset. We built a 60/20/20 stratiﬁed training/validation/\ntest. Theﬁne tuning on the full dataset took approximately 12 h per epoch\non four A10 NVIDIA GPUs (24GB VRAM) with 16 CPU cores, each with\nan effective batch size of 16. Learning rates were tuned within the range of\n1e−5 to 3e−5.M a xt o k e nl e n g t h sw e r ea p p l i e df o rB E R T ,D i a l o g L E D ,a n d\nMVP, while Longformer wascapped at 2048 tokens for efﬁciency. This\nlimits the length of input text that can be provided but provides faster\nprocessing. The optimal training durations for each model were determined\nthrough basic hyperparameter tuning (used toﬁnd optimal parameters such\nas learning rate and batch size), resulting in two epochs (training cycles) for\nBERT, three epochs for DialogLED,ﬁve epochs for Longformer, and two\nepochs for MVP. Threshold selection was a key consideration in deter-\nmining how labels were assigned. Since this is a multi-label classiﬁcation\ntask, where each conversation can have multiple assigned tags, we experi-\nmented with different threshold values to optimize the balance between\nprecision and recall. We systematicallyevaluated thresholds ranging from\n0.25 to 0.5 on a validation set, measuring their impact on classiﬁcation\nperformance. Our analysis indicated that a threshold of 0.25 yielded the best\ntrade-off between precision and recall,particularly for underrepresented\nlabels.\nProcess of selecting model architecture\nThe utilization of transformer-based models, such as Longformer, for\nclassifying clinical or conversational data has been extensively explored in\nthe literature. Studies, such as those of Li et al.23, have consistently\ndemonstrated that Longformer models outperform shorter-sequence\ntransformers like ClinicalBERT29 in various downstream tasks, including\nclinical document classiﬁcation. Similarly, in another study by Dai et al.30,\nwhich evaluated different approaches for classifying long documents using\ntransformer architectures such as Longformer, it was concluded that\nemploying transformer-based models designed for longer sequences is more\neffective and efﬁcient than using shorter-sequence models like BERT. This\nﬁnding is particularly relevant as BERT is constrained by a 512-token limit\nwhich prevents the processing of any te x tt h a th a sat o k e nl e n g t hg r e a t e r\nthan the limit, while Longformer’s capability to handle longer sequences (up\nto eight times longer) proves advantageous for tasks requiring a broader\ncontext, such as analyzing lengthy conversational data. Other work such as\nthose of Wang et al.\n31,Z h o n ge ta l .26 and Ji et al.32 highlight the signiﬁcance of\nadditional pre-training methods, such as masked language modeling and\nnext-turn prediction, especially in the context of dialog data. Both studies\nemphasize the differences between general domain language and dialog,\nindicating that pre-training ona large corpus of domain-speciﬁcd i a l o g sc a n\nsigniﬁcantly improve performance on downstream dialog tasks. Particularly\nZhong et al.\n26 demonstrate the beneﬁts of pre-training Longformer using\ndialog-speciﬁc window-based denoising on lengthy dialogs, resulting in a\nsubstantial improvement in state-of-the-art tasks such as long dialog\nunderstanding. Lastly, Ji et al.32 pre-train RoBERTa33, Longformer and\nXLNet34 on mental healthcare domain data for the task of mental health\nclassiﬁcation, achieving superior results in most cases to the base models,\ndemonstrating the effectiveness of signiﬁcant pre-training on mental\nhealthcare domain data in related downstream tasks.\nSupplementary Table 2 compares the performance of the four pre-\nliminary models, including Longforme r ,C o n v e r s a t i o n a lB E R T ,D i a l o g L E D ,\nand MVP, all chosen for their suitability for handling either conversational\ndata or long-documents. We usedﬁve metrics to evaluate model perfor-\nmance on the test data. Theﬁrst metric is the standard classiﬁcation\n“accuracy” which considers the total percentage of all of the 19 tags pre-\ndicted by the model correctly for each instance across the full dataset. In this\nway, to attain full accuracy for a given conversation, the model must predict\nall of the correct set of tags assigned to the conversation, and not mistakenly\npredict any tags that are not in the correct set of tags.\nDue to the sparsity of assigned tags, with conversations tending to be\ntagged with only a few tags out of the 19 total, a classiﬁer can attain a high\naccuracy by not predicting any tags, hence we use a second metric which is\nreferred to as“exact accuracy”, and assesses correctness based on the per-\ncentage of conversations where all the predicted issue tags are correct. As\nsuch, a single misidentiﬁed tag for a conversations means it is classiﬁed as an\nincorrect prediction. In addition to accuracy, we use three other metrics\nwhich we call“sample average precision”, “sample average recall”,a n d\n“sample average F1-score”.\nIn the context of multi-label classiﬁcation, the sample/example-based\naverage calculates the three scores foreach sample then averages the scores\nacross all samples. For each sample, the entire set of predicted tags is con-\nsidered in the calculation of the three scores without isolating each indivi-\ndual tag type, and are compared with the full set of true labels. This is unlike\nmicro-averaging where the scores arecalculated globally across all of the\ntotal true positives, false negatives and false positives, or macro-averaging\nwhere the scores are calculated across all of the true positive, false positive,\nand false negative counts for a speciﬁct a gi before taking the unweighted\nmean across the scores for all tags. This method of averaging provides a\nrepresentative result for the entire distribution after assessing scores for each\nsample individually. The metrics displayed in Supplementary Table 2 are\naverages across all issue samples.\nBoth Longformer and Conversational BERT exhibit comparable high\nperformance (Accuracy: 0.94 and sample average F1-score: 0.56). Con-\nversational BERT offers the advantage of being pre-trained on an extensive\nhttps://doi.org/10.1038/s41746-025-01647-6 Article\nnpj Digital Medicine|           (2025) 8:243 9\ncorpus of conversation data, while Longformer excels in capturing longer\nsequences. Therefore, we leveraged Longformer due to the nature of our\nconversations (long sequences), withthe intention of performing domain\nadaptation akin to Conversational BERT to improve its performance. The\nremaining two models, based on encoder-decoder architectures, under-\nperformed (sample average F1-score <0.35) primarily because they were not\noriginally designed for this particular multi-label classiﬁcation task. Fur-\nthermore, both encoder-decoder models encountered signiﬁcant practi-\ncality issues related to exceptionallyl e n g t h yt r a i n i n gt i m e sa n dr e s o u r c e\nlimitations, necessitating use of small batch sizes and long inference times.\nAs a result, they were deemed sub-optimal choices for this speciﬁct a s k .\nDevelopment step 2:ﬁnal model development and optimization\nFor ourﬁnal model, we employed an ensemble approach combining three\nLongformer models, each with slightly different initialization andﬁne-\ntuning settings. The choice of Longformer as our primary model was based\non its superior performance and its capacity to effectively capture long\nconversations. Each Longformer underwent initial pre-training using the\nsame approach, which included masked language modeling, where a por-\ntion of words in each conversation was masked, and the model learned to\npredict them, on the full baseline training dataset. We applied masking to\n15% of tokens per conversation and pre-trained the models for one epoch\nwith a maximum sequence length of 1500 tokens. AdamW\n35 was used as the\noptimizer, and a linear scheduler with 500 warm-up steps was applied to\nimprove training stability. Gradient accumulation was used to maintain an\neffective batch size of 64, ensuring efﬁcient use of GPU resources. This pre-\ntraining step required approximately 24 h.\nFollowing the pre-training task, the Longformer models wereﬁne-\ntuned on a label-balanced training/validation/test data split (60/20/20). Per\nrecommendation of our domain experts, we incorporated additional con-\ntext information related to the conversation’s priority. Therefore, the\nbeginning sentence included the statement:“This conversation is of < <X >\n>p r i o r i t y” with X representing one of the three priority levels assigned to\neach conversation. More on this process of generating these levels is dis-\ncussed in the “Methods” section: “Priority ﬂag pre-processing”.E a c h\nLongformer model wasﬁne-tuned for a maximum of three epochs using a\nbatch size of 16, managed through gradient accumulation, with a learning\nrate set to 2e\n−5. Standard Binary Cross Entropy loss was applied during the\nﬁne-tuning, with oversampling of conversations with less common issue\ntags speciﬁcally implemented on two of the ensemble models to address the\nclass imbalance. We used AdamW as the weight optimizer and imple-\nmented a linear scheduler with the initial 20% of training steps.\nEvaluation of FAIIR predictions\nUpon completion of the development of the FAIIR tool, we conducted two\nindependent experiments to evaluate its efﬁcacy and performance in gen-\neralization. The experiments included both expert assessment and silent\ntesting of the tool and its predictions.\nExpert assessment and evaluation\nMethods for expert assessment for FAIIR included conducting an evalua-\ntion survey completed by CRs. We invited 12 trained CRs to review 40\nchallenging conversations. The conversation selection criteria were diverse,\nfocusing on those with more than four issue tags and including ambiguous\ncases where FAIIR’s predictions were conﬁdent but incorrect based on our\nground-truth labels. Our hypothesis was that for these edge cases, FAIIR\nrequires a deep and nuanced understanding to perform well. Thus, our goal\nwas to assess the model’s ability to identify all relevant issue tags and\nnavigate language nuances.\nTwenty of the 40 total conversations annotated were picked at random\nfrom the test set to get a representative sample of the data. All 20 con-\nversations were originally labeled with four or more different issue tags. The\nreason for selecting these conversations is their coverage of a wide range of\nissues: the potential for annotators picking a different set of issue tags from\neach other is high, and perspectives to determine which issue tags truly apply\nto the conversation were of upmost importance. This was also important in\nevaluating whether the model was able to grasp all nuanced issue tags that\nmay apply less directly to a given conversation.\nThe remaining 20 of the 40 conversations were mostly originally tagged\nwith three or less issue tags, in an effort to promote a balance between\nconversations with many tags and those where only a small number may\napply. Of these 20, handfuls of conversations were selected according to\nseveral differing criteria. A number were selected manually to cover all of the\n19 different issue tags, in an effort to build consensus in the identiﬁcation of\nall tags for the model to reference. A small sample of conversations were also\nselected as purposefully ambiguous cases: mainly long conversations which\nwere only annotated with one or two issue tag(s). Although the issue tag(s)\nassigned at baseline were typically correct, these conversations were an\nopportunity to gain consensus on a spectrum of more nuanced tags for the\npurposes of modelﬁne-tuning. The last few conversations were handpicked\nbecause they were perceived to be mislabelled in some way. For these con-\nversations, the original issue tag(s) assigned appeared incorrect or incom-\nplete, in that there was another key issue tag missing. Consensus building is\nimportant for these examples, in order to improve the original tag(s)\nassigned, where incorrect. These canalso be complex cases for the model to\nnavigate, and thus served as a helpful way to evaluate the tool’sp e r f o r m a n c e .\nEach conversation was independently reviewed by six CRs, divided\ninto two groups. In the“open review”, three CRs reviewed conversations\nwith FAIIR’s predicted issue tags explicitly provided. This approach aimed\nto evaluate whether the model’s predictions were helpful, misleading, or\npartially correct in identifying the core issues within each conversation. CRs\ncould either agree or disagree with the predicted tags and suggest corrections\nor reﬁnements where necessary.\nIn the “blind review”, the remaining three CRs reviewed the same\nconversations without any prior exposure to FAIIR’s predicted tags. Instead,\nthey independently identiﬁed issue tags based solely on the conversation\ncontent. Furthermore, they categorized the identiﬁed tags into primary issue\ntags (representing the most pressing concerns) and secondary issue tags\n(minor but relevant concerns). This approach established a baseline for\ncomparison against FAIIR’s predictions, ensuring that human assessments\nwere made without any inﬂuence from the model.\nThe followingﬁve criteria were established to develop a consensus\nmeasure for comparison in the blind review setting, which is more chal-\nlenging than the open review setting. Since human annotations categorize\nissue tags as primary (most pressing concerns) and secondary (minor but\nrelevant concerns), we evaluated agreement with FAIIR’so u t p u t sb a s e do n\nthese distinctions. Notably, FAIIR doesnot explicitly differentiate between\nprimary and secondary issue tags-all predicted tags are treated equally.\nTherefore, for the purpose of comparison, we assessed agreement by\nmapping FAIIR’\ns predicted tags to human annotations and measuring\nalignment using the following criteria:\n Full agreement on primary issue tags (FA: 1°)— all primary issue tags\nidentiﬁed by human annotators are also predicted by FAIIR.\n Partial agreement on primary issue tags via majority vote (PA: 1° Maj.)\n— the majority of human annotators agree on a set of primary issue\ntags, and these tags overlap with FAIIR’s predictions.\n Partial agreement on primary and secondary issue tags via majority\nvote (PA: 1°+2° Maj.)— the majority of human annotators agree on a\nset of both primary and secondary issue tags, and these overlap with\nFAIIR’s predictions.\n Full agreement on primary issue tags via at least one vote (FA: 1°≥ 1)—\nat least one human annotator identiﬁed a primary issue tag that is also\npredicted by FAIIR.\n Full agreement on primary and secondary issue tags via at least one vote\n(FA: 1°+2° ≥ 1)— at least one human annotator identiﬁed a primary\nor secondary issue tag that is also predicted by FAIIR.\nModel reﬁnement— modifying the decision boundary\nIn our evaluation experiments, model reﬁnement involved adjusting the\ndecision boundary (threshold cutoff) to strike a balance between recall and\nhttps://doi.org/10.1038/s41746-025-01647-6 Article\nnpj Digital Medicine|           (2025) 8:243 10\nprecision. In most experiments, the FAIIR tool’s predictions showed lower\nprecision compared to recall, so we adjusted the threshold to reduce its\nfrequency of outputting the most common tags while lowering the threshold\nf o rr a r et a g s .F o re x a m p l e ,w es e tt h et h r e s h o l dt o0 . 4f o rt h et h r e em o s t\nfrequent classes:Anxiety/Stress, Depressed,a n dRelationship.F o rt h en e x t\ntwo most frequent classes,Suicideand Isolated, we adjusted the threshold to\n0.3. Theseﬁve classes encompass the majority of predicted issue tags from\nthe model, hence we targeted them for increased thresholds. The remaining\nt a g sw e r es e ta tal o w e rt h r e s h o l do f0 . 2t oe n h a n c et h em o d e l’sa b i l i t yt o\ncapture them effectively.\nSilent testing\nWe conducted silent testing to assess the FAIIR tool’s generalization per-\nformance on new and recent batches of conversations received over\n8 months. Testing on 84,832 conversations that occurred between February\nto September 2023 served as a valuable representation of how the model\nadapts to and handles the ever-changing landscape of real-world dialog. In\naddition to model evaluation, we implemented reﬁn e m e n t sa se x p l a i n e d\nearlier to strike a balance between precision and recall.\nOutcome interpretation and visualization\nWe leveraged layer-integrated gradients36, a technique to understand which\ntokens in any given conversation holdthe utmost relevance to the predicted\nissue tags. By doing so, not only can weidentify the most pertinent words\nassociated with the primary conversation topics but we also gain insight into\nthe model’s decision-making process, thereby enhancing its overall\nexplainability. To achieve this, we computed an attribution score, which are\nanalogous to an importance score, foreach token in the conversations using\ntheﬁnal model and tokenizer with respect to the sets of predeﬁned issue tags.\nAttributed tokens along the axis of issue tags that surpass a predeﬁned\nthreshold are selected as the most relevant words. We refer to these as\n“natural keywords”.T h et e r m“natural” is used because these keywords are\nnot predeﬁned; they are dynamic and align with the nuances of natural\nlanguage.\nTo create a cleaner set of natural keywords from the initial set obtained\nusing integrated gradients, we run a series ofﬁlters to remove words and\nsymbols that are irrelevant or do not add meaning or additional insights. We\nautomaticallyﬁltered stop words, punctuation, and special tokens for this\nreason. In addition, we devised a predeﬁned word list that contains natural\nkeywords. These keywords, while not categorized as stop words, consistently\noccurred very frequently across virtually every issue tag (e.g., User, Hello,\nConnect). Any keywords that fall within this list were alsoﬁltered out.\nFinally, weﬁlter keywords according to their part of speech tags, removing\nany that fall within a deﬁned set of categories. Weﬁltered conjunctions,\ndeterminers, prepositions, modal auxiliary words, and the majority of verbs\nalong with many other categories, yielding keywords that are primarily\nnouns and adjectives.\nTo streamline the processing of natural keywords and facilitate com-\nmunication with knowledge experts, we conducted word embedding\nvisualization and bi-gram analysis at the aggregated level to demonstrate\nsemantic relationships and word proximity in a speciﬁcc o n t e x t .\nVisualization of natural key words to support explainability\nIn addition to the core 19 issue tags, we built an explainability pipeline to\nenable the extraction of keywords, referred to as“natural keywords”,f r o m\neach conversation. Keywords are dynamic and context-speciﬁct o k e n s\nassociated with the main issue tags being discussed in a conversation. By\nextracting these, we can derive further insights on moreﬁne-grained\nimportant“sub-topics”, that can help to better inform issues of importance.\nSupplementary Fig. 3 illustrates the occurrence frequency of the top 100\nkeywords across 10,000 randomly selected conversations in the test set\nlabeled with theSuicidei s s u et a g .I nt o t a l ,1 2 4 , 5 7 8keywords were generated\nrespectively for theSuicideissue tag, yielding an average of 12.5 keywords per\nconversation. The top list of keywords generated generally represents ideas\nand concepts associated with the given issue tag, offering additional insights.\nF o re x a m p l e ,w en o t eh o wi nt h ec a s eo fs u i c i d e ,f r e q u e n tt o p i c sa r eh i g h -\nlighted based on common keywords, suchas emotion-related keywords (i.e.,\n“happy”, “sad”, “mood”, “anxiety”, “scared”,a n d“pain”) thus demonstrating\nthe distress that is being experienced. Other top keywords like“plan” being\nin the top 10 may show that many people texting have serious plans of\nsuicide. Location-based words like“home”, “school”,a n d“friend” also rank\nhigh, as CRs are likely trying to determine the location of the individual to try\nto give the most appropriate support. These insights allow for many\nobservations at the macro-level. Similarly, Supplementary Fig. 5 demon-\nstrates the distribution of top 25 keywords across the three different abuse\nissue tags (Abuse, Physical; Abuse, Sexual;a n dAbuse, Emotional), showing\nthe general similarities of keywords across different categories (like“friend”)\nversus those that are far more frequent in certain tags (like“assault” is to the\nAbuse, Sexualtag) or how“mom\n” and “dad” are relatively much more\nfrequent forAbuse, Physicaland Abuse, Emotionaltags compared to the\nSuicideand Abuse, Sexualtags. Further details canbe seen in Supplementary\nFig. 5.\nThe explainability pipeline in the FAIIR tool offers visualization features\nthat provide valuable insights into the semantic relationship and proximity of\nkeywords through the utilization of bi-gram analysis and word embeddings.\nFor bi-gram analysis, FAIIR provides a graph-based visualization in which\nnodes represent the keywords, while the edges between nodes illustrate the\nstrength of their relationships (the co-occurrence frequency in conversa-\nt i o n s ) .S u p p l e m e n t a r yF i g .4 ,a sa ne x a m p l e ,i l l u s t r a t e st h eo u t c o m eo fb i -\ngram analysis performed on the natural keywords in relation to theSuicide\nissue tag (the bi-gram analysis on theAbuse, Physicalissue tag is shown in\nSupplementary Materials Fig. 6). Based on theﬁgure, certain keywords like\n“thought”, “suicidal”, “home”,a n d“harm” have many connections that are\nvery strong, showing they co-occur more frequently than other pairs.\nConnections can reveal potential insights about behaviors or where common\nissues lie, which can be seen with connections like“family” and “pain”,\n“problem”, “talk”,a n d“situation”, which can potentially reveal that family\ntroubles are frequent sub-issues tied with being suicidal.\nFurther insights on extracted key words\nSupplementary Fig. 5 illustrates the occurrence frequency of the top 25\nkeywords across all conversations in the test set labeled with the three abuse\nissue tags (Abuse, Physical; Abuse, Sexual;a n dAbuse, Emotional). In total,\n1673, 2598, and 3367 conversations were used to generate 26,985, 40,522\nand 52,635 keywords, respectively, for the three aforementioned issue tags,\nyielding an average of 16.2, 15.6, and 15.6 keywords per conversation. The\ntop list of keywords generated generally represents ideas and concepts\nassociated with the given issue tag, offering additional insights. For example,\nwe note how in the case of sexual abuse, both“home” and “school” are\nfrequent keywords, with home being relatively more frequent than school.\nThis may reﬂect places where service users are more likely to have experi-\nenced abuse, thereby aiding in strategy and planning for CRs in managing\nconversations. We observe similar keywords are common across these issue\ntags, such as“friend”, “mom”,o r “dad” which reveal major overarching\ntopics and pressure points.\nIn addition to bi-gram analysis, in Supplementary Fig. 7 we demon-\nstrate the word embeddings of the top 100 most frequent keywords within\nall conversations originally labeled with the issue tagAbuse, Emotional,\nprojected in a three-dimensional space using principal component analysis\n(PCA). This enables enhanced visualization and easier exploration of key-\nwords that are most similar to a searched keyword using cosine distance\nbetween the embeddings. Fore x a m p l e ,f o rt h ek e y w o r d“ﬁght\", keywords\nlike “stop\", “help\" and“hard\" are the closest in embedding space.\nEthics statement\nThis publication was the result of a quality improvement initiative at Kids\nHelp Phone, and as a result, no REB approval was sought or obtained.\nKids Help Phone (KHP) is deeply committed to the ethical and\nresponsible use of data to enhance our services for youth, recognizing the\nimportance of ethical principles in maintaining the trust of those we serve,\nhttps://doi.org/10.1038/s41746-025-01647-6 Article\nnpj Digital Medicine|           (2025) 8:243 11\nespecially the most vulnerable. This paper is aimed exclusively at applied\nresearch to improve service delivery and accessibility, with a special focus on\nthe ethical application of Artiﬁcial Intelligence (AI) to beneﬁto u rs e r v i c e\nnetwork and frontline staff. Through this collaboration, we are dedicated to\ndeveloping technological tools that provide a personalized and user-friendly\nexperience for those seeking help. Upholding the privacy and conﬁdentiality\nof our service users is paramount; we adhere to an ethical statement aligned\nwith KHP’s privacy policy ( https://kidshelpphone.ca/privacy-policy/),\nincluding consent notice for research and rigorous data minimization. Our\nprocesses are transparent and accountable, compliant with Canadian\nprivacy regulations. We meticulously remove all direct identiﬁers from\nresearch data, adhering to industry standards for data anonymization, and\nsecurely store all research data within KHP’s infrastructure. This reﬂects our\ncommitment to the highest standards of data security, conﬁdentiality, and\nethical practice. By prioritizing ethical data use, KHP can leverage research\nto improve our services and deliver the best possible support for youth\nacross Canada, embodying our commitment to integrity, respect, and\nresponsibility in every action we take.\nData availability\nThe data that support the FAIIR tool and this study overall are not openly\navailable due to reasons of sensitivity. These data are located in controlled\naccess storage at Kids Help Phone. Pleasecontact contact@kidshelpphone.ca\nfor more information.\nCode availability\nThe code that supports the FAIIR tool and this study overall is not openly\navailable due to reasons of sensitivity and data privacy. Full code is available\nat theprivate GitHubu p o nr e q u e s t .T h ec o d eu s e dt og e n e r a t ea n dp r o c e s s\no u rd a t ai sa l s oi n c l u d e di nt h er e p o s i t ory, and contains all of the parameters\nand settings used to do. Please contact contact@kidshelpphone.ca for more\ninformation.\nR e c e i v e d :1 8J u l y2 0 2 4 ;A c c e p t e d :1 3A p r i l2 0 2 5 ;\nReferences\n1. World Health Organization. Mental health of adolescents.https://\nwww.who.int/news-room/fact-sheets/detail/adolescent-mental-\nhealth (2021).\n2. Mental Health Commision of Canada.Making the Case for Investing in\nMental Health in Canada. Tech. Rep. (Canadian Mental Health\nAssociation, National, 2016).\n3. Wiens, K. et al. A growing need for youth mental health services in\nCanada: examining trends in youth mental health from 2011 to 2018.\nEpidemiol. Psychiatr. Sci.29, e115 (2020).\n4. Canadian Institute for Health Information. Vision impairment and\nblindness. https://www.cihi.ca/en/taking-the-pulse-a-snapshot-of-\ncanadian-health-care-2023/canadians-short-on-access-to-care-for\n(2023).\n5. Gould, M. S., Kalafat, J., Harris Munfakh, J. L. & Kleinman, M. An\nevaluation of crisis hotline outcomes. Part 2: suicidal callers.Suicide\nLife Threat. Behav.37, 338–352 (2007).\n6. Hoffberg, A. S., Stearns-Yoder, K. A. & Brenner, L. A. The\neffectiveness of crisis line services: a systematic review.Front. Public\nHealth 7, 495942 (2020).\n7. Dinakar, K., Chen, J., Lieberman, H., Picard, R. & Filbin, R. Mixed-\ninitiative real-time topic modeling & visualization for crisis counseling.\nIn Proceedings of the 20th International Conference on Intelligent User\nInterfaces (IUI)(eds Carenini, G., Pan, S. & Kristensson, P. O.) 417–426\n(Association for Computing Machinery, 2015).\n8. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training\nof deep bidirectional transformers for language understanding.arXiv\nhttps://doi.org/10.48550/arXiv.1810.04805 (2018).\n9. Beltagy, I., Peters, M. E. & Cohan, A. Longformer: the long-document\ntransformer. arXiv https://api.semanticscholar.org/CorpusID:\n215737171 (2020).\n10. Touvron, H. et al. Llama 2: open foundation andﬁne-tuned chat\nmodels. arXiv https://api.semanticscholar.org/CorpusID:259950998\n(2023).\n11. Le Glaz, A. et al. Machine learning and natural language processing in\nmental health: systematic review.J. Med. Internet Res.23, e15708 (2021).\n12. Dolatabadi, E. et al. Using social media to help understand patient-\nreported health outcomes of post–COVID-19 condition: natural language\nprocessing approach.J. Med. Internet Res.25, e45767 (2023).\n13. Raza, S., Dolatabadi, E., Ondrusek, N., Rosella, L. & Schwartz, B.\nDiscovering social determinants of health from case reports using\nnatural language processing: algorithmic development and\nvalidation. BMC Digital Health1, 35 (2023).\n14. Dolatabadi, E. et al. Natural language processing for clinical laboratory\ndata repository systems: implementation and evaluation for\nrespiratory viruses.JMIR AI2, e44835 (2023).\n15. Salas-Zárate, R. et al. Detecting depression signs on social media: a\nsystematic literature review.Healthcare 10, 291 (2022).\n16. Hu, P. et al. BlueMemo: depression analysis through Twitter posts. In\nProceedings of the Twenty-Ninth International Conference on\nInternational Joint Conferences on Artiﬁcial Intelligence(ed. Bessiere,\nC.) 5252–5254 (International Joint Conferences on Artiﬁcial\nIntelligence, 2021).\n17. Yates, A., Cohan, A. & Goharian, N. Depression and self-harm risk\nassessment in online forums.arXiv https://doi.org/10.48550/arXiv.\n1709.01848 (2017).\n18. Sinha, P. P. et al. # suicidal— a multipronged approach to identify and\nexplore suicidal ideation in twitter. InProceedings of the 28th ACM\nInternational Conference on Information and Knowledge\nManagement (eds Cui, P., Rundensteiner, E., Carmel, D., He, Q. & Xu\nYu, J.) 941–950 (Association for Computing Machinery, 2019).\n19. Downs, J. et al. Detection of suicidality in adolescents with autism\nspectrum disorders: developing a natural language processing\napproach for use in electronic health records.Proc. AMIA2017,\n641–649 (2018).\n20. Tran, T. & Kavuluru, R. Predicting mental conditions based on“history\nof present illness\" in psychiatric notes with deep neural networks.J.\nBiomed. Inform.75, S138–S148 (2017).\n21. Swaminathan, A. et al. Natural language processing system for rapid\ndetection and intervention of mental health crisis chat messages.npj\nDigital Med.6, 213 (2023).\n22. Obadinma, S. et al. Bringing the state-of-the-art to customers: a\nneural agent assistant framework for customer service support. In\nProceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing: Industry Track(eds Li, Y. & Lazaridou, A.)\n440–450 (Association for Computational Linguistics, 2022).\n23. Li, Y., Wehbe, R. M., Ahmad, F. S., Wang, H. & Luo, Y. Clinical-\nlongformer and clinical-bigbird: transformers for long clinical\nsequences. https://arxiv.org/abs/2201.11838 (2022).\n24. Vaswani, A. et al. Attention is all you need. InAdvances in Neural\nInformation Processing Systems, Vol. 30 (eds Guyon, I. et al.) (Curran\nAssociates, Inc., 2017).\n25. DeepPavlov. bert-base-cased-conversational.https://huggingface.\nco/DeepPavlov/bert-base-cased-conversational (2023).\n26. Zhong, M., Liu, Y., Xu, Y., Zhu, C. & Zeng, M. DialogLM: pre-trained\nmodel for long dialogue understanding and summarization.Proc.\nAAAI Conf. Artif. Intell.36, 11765–11773 (2022).\n27. Tang, T., Li, J., Zhao, W. X. & Wen, J.-R. MVP: multi-task supervised\npre-training for natural language generation. InFindings of the\nAssociation for Computational Linguistics: ACL 2023(eds Rogers, A.,\nBoyd-Graber, J. & Okazaki, N.) 8758–8794 (Association for\nComputational Linguistics, 2023).\nhttps://doi.org/10.1038/s41746-025-01647-6 Article\nnpj Digital Medicine|           (2025) 8:243 12\n28. Kementchedjhieva, Y. & Chalkidis, I. An exploration of encoder-\ndecoder approaches to multi-label classiﬁcation for legal and\nbiomedical text. InFindings of the Association for Computational\nLinguistics: ACL 2023(eds Rogers, A., Boyd-Graber, J. & Okazaki, N.)\n5828–5843 (Association for Computational Linguistics, 2023).\n29. Huang, K., Altosaar, J. & Ranganath, R. ClinicalBERT: modeling\nclinical notes and predicting hospital readmission.https://arxiv.org/\nabs/1904.05342 (2020).\n30. Dai, X., Chalkidis, I., Darkner, S. & Elliott, D. Revisiting transformer-\nbased models for long document classiﬁcation. InFindings of the\nAssociation for Computational Linguistics: EMNLP 2022(eds\nGoldberg, Y., Kozareva, Z. & Zhang, Y.) 7212–7230 (Association for\nComputational Linguistics, 2022).\n31. Wang, P., Fang, J. & Reinspach, J. CS-BERT: a pretrained model for\ncustomer service dialogues. InProceedings of the 3rd Workshop on\nNatural Language Processing for Conversational AI(eds Papangelis,\nA. et al.) 130–142 (Association for Computational Linguistics, 2021).\n32. Ji, S. et al. Domain-speciﬁc continued pretraining of language models\nfor capturing long context in mental health.https://doi.org/10.48550/\narXiv.2304.10447 (2023).\n33. Liu, Y. et al. RoBERTa: a robustly optimized BERT pretraining\napproach. https://doi.org/10.48550/arXiv.1907.11692 (2019).\n34. Yang, Z. et al.XLNet: Generalized Autoregressive Pretraining for\nLanguage Understanding(Curran Associates Inc., 2019).\n35. Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. In\nInternational Conference on Learning Representations(eds Levine, S.,\nLivescu, K. & Mohamed, S.) (International Conference on Learning\nRepresentations, 2019).\n36. Sundararajan, M., Taly, A. & Yan, Q. Axiomatic attribution for deep\nnetworks. https://doi.org/10.48550/arXiv.1703.01365 (2017).\nAcknowledgements\nResources used in preparing this research were provided, in part, by the\nProvince of Ontario, the Government of Canada through CIFAR, and the\ncompanies sponsoring the Vector Institute (www.vectorinstitute.ai/\npartners). Kids Help Phone gratefully acknowledges the support from its\ncommunity of accelerator Catalyst Partners including The Lang Family\nFoundation and visionary investors who made this work possible (https://\nkidshelpphone.ca/Catalyst_Partners). KHP SO's research was supported\nby the Indigenous and Black Engineering/Technology (IBET) PhD Fellowship\nand NSERC Canada Graduate Scholarships— Doctoral program (CGS-D).\nX.Z.'s research was supported by the Mitchell Professorship and NSERC\nDiscovery Grant. E.D.'s research was supported by a Natural Sciences and\nEngineering Research Council of Canada (NSERC) Discovery Grant and a\nCanadian Institutes of Health Research (CIHR) Special Call from the Centre\nfor Research on Pandemic Preparedness and Health Emergencies.\nAuthor contributions\nS.O., M.L.N., J.R., J.Y. and E.D. drafted the main manuscript. S.O. and E.D.\nprepared theﬁgures and methods. A.L., J.R. and D.M. provided subject\nmatter expertise. A.L., X.Z., D.M., D.P., R.S. and E.D. provided supervision\nand guidance. All authors reviewed the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01647-6.\nCorrespondenceand requests for materials should be addressed to\nStephen Obadinma or Elham Dolatabadi.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01647-6 Article\nnpj Digital Medicine|           (2025) 8:243 13",
  "topic": "Mental health",
  "concepts": [
    {
      "name": "Mental health",
      "score": 0.6047926545143127
    },
    {
      "name": "Service (business)",
      "score": 0.5695639252662659
    },
    {
      "name": "Psychology",
      "score": 0.5267478823661804
    },
    {
      "name": "Mental health service",
      "score": 0.4838903844356537
    },
    {
      "name": "Applied psychology",
      "score": 0.4049306809902191
    },
    {
      "name": "Medical education",
      "score": 0.38937148451805115
    },
    {
      "name": "Computer science",
      "score": 0.38660016655921936
    },
    {
      "name": "Business",
      "score": 0.3505358099937439
    },
    {
      "name": "Psychiatry",
      "score": 0.20872843265533447
    },
    {
      "name": "Medicine",
      "score": 0.20677435398101807
    },
    {
      "name": "Marketing",
      "score": 0.11488521099090576
    }
  ]
}