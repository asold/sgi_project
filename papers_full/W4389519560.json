{
  "title": "ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models",
  "url": "https://openalex.org/W4389519560",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2105534490",
      "name": "Chenliang Li",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2099618248",
      "name": "He Chen",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2051706254",
      "name": "Ming Yan",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2490526124",
      "name": "Weizhou Shen",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2103589644",
      "name": "Xu Haiyang",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2141785619",
      "name": "Zhikai Wu",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2104806220",
      "name": "Zhicheng Zhang",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2102038362",
      "name": "Wenmeng Zhou",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2104866439",
      "name": "Ying-Da Chen",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2007067578",
      "name": "Chen Cheng",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2124162494",
      "name": "Hongzhu Shi",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2109058337",
      "name": "Ji Zhang",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1936961387",
      "name": "Fei Huang",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2494370702",
      "name": "Jingren Zhou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4323717348",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4285428875",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4378474282",
    "https://openalex.org/W4366328015",
    "https://openalex.org/W4378942772",
    "https://openalex.org/W4366399689",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4366327559",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4380136738"
  ],
  "abstract": "Chenliang Li, He Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, Hongzhu Shi, Ji Zhang, Fei Huang, Jingren Zhou. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 566–578\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nModelScope-Agent: Building Your Customizable Agent System with\nOpen-source Large Language Models\nChenliang Li, Hehong Chen, Ming Yan∗, Weizhou Shen, Haiyang Xu, Zhikai Wu\nZhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, Hongzhu Shi\nJi Zhang, Fei Huang, Jingren Zhou\nDAMO Academy, Alibaba Group, China\n{lcl193798, hehong.chh, ym119608, shenweizhou.swz, shuofeng.xhy, wuzhikai.wzk, zhangzhicheng.zzc,\nwenmeng.zwm, yingda.chen, chengchen.cc, hongzhu.shz, zj122146, f.huang, jingren.zhou}@alibaba-inc.com\nAbstract\nLarge language models (LLMs) have recently\ndemonstrated remarkable capabilities to com-\nprehend human intentions, engage in reason-\ning, and design planning-like behavior. To\nfurther unleash the power of LLMs to accom-\nplish complex tasks, there is a growing trend to\nbuild agent frameworks that equips LLMs, such\nas ChatGPT, with tool-use abilities to connect\nwith massive external APIs.\nIn this work, we introduce ModelScope-Agent,\na general and customizable agent framework\nfor real-world applications, based on open-\nsource LLMs as controllers. It provides a\nuser-friendly system library, with a customiz-\nable engine design to support model training\non multiple open-source LLMs, while also en-\nabling seamless integration with both model\nAPIs and common APIs in a unified way. To\nequip the LLMs with tool-use abilities, a com-\nprehensive framework has been proposed span-\nning tool-use data collection, tool retrieval,\ntool registration, memory control, customized\nmodel training, and evaluation for practical\nreal-world applications. Finally, we showcase\nModelScopeGPT1, a real-world intelligent as-\nsistant of ModelScope Community based on\nthe ModelScope-Agent framework, which is\nable to connect open-source LLMs with more\nthan 1000 public AI models and localized\ncommunity knowledge in ModelScope. The\nModelScope-Agent online demo2, library3 are\nnow publicly available.\n1 Introduction\nLarge language models (OpenAI, 2022, 2023;\nTouvron et al., 2023; Chowdhery et al., 2022)\nhave gradually become common AI assistants\n∗Corresponding author: <ym119608@alibaba-inc.com>\n1https://modelscope.cn/studios/damo/ModelscopeGPT\n2https://modelscope.cn/studios/lcl193798/Modelscope-\nAgent\n3https://github.com/modelscope/modelscope-agent\nthat demonstrate great potential in comprehend-\ning human intentions, performing complex rea-\nsoning tasks, and enabling content creation. De-\nspite the rapid advancements of open-source LLMs,\ne.g., LLaMA (Touvron et al., 2023) and Chat-\nGLM (THUDM, 2023), they still remain limited\nin performing complex tasks, such as following\nuser instructions to use external tools and capture\nup-to-date information.\nTo further unleash the power of LLMs for real-\nworld practical applications, a rising trend of cur-\nrent research (Schick et al., 2023; Shen et al., 2023;\nYang et al., 2023; Qin et al., 2023; Patil et al., 2023)\nbegins to enable LLMs with tool-use abilities to-\nwards building an AI Agent. These include Hug-\ngingGPT (Shen et al., 2023), Visual-ChatGPT (Wu\net al., 2023) and Gorilla (Patil et al., 2023) for\nconnecting with HuggingFace models, ToolAl-\npaca (Tang et al., 2023) and ToolLLaMA (Qin et al.,\n2023) for using massive common APIs such as\nweather forecast and search engine. These methods\neither directly rely on closed-source counterparts\nlike ChatGPT or focus on certain types of API tools.\nRecently, there have also been public releases of\nAI agents, such as Auto-GPT 4, LangChain5 and\nTransformers Agent (Huggingface, 2023), which\nenable LLMs, such as ChatGPT or GPT-4, to use\ntools and solve complex AI tasks. However, these\nagents are mainly built with closed-source LLMs\nand how to build a customizable agent system with\nopen-source LLMs remains largely unexplored.\nIn this work, we present ModelScope-Agent, a\ngeneral and customizable agent system for real-\nworld applications, based on open-source LLMs\nas controllers. ModelScope6 is a public ML com-\nmunity, that seeks to bring together the most ad-\nvanced machine learning models from the AI com-\nmunity, and streamlines the process of leveraging\n4https://github.com/Significant-Gravitas/Auto-GPT\n5https://github.com/langchain-ai/langchain\n6https://modelscope.cn/models\n566\nAI models in real-world applications. ModelScope-\nAgent provides a flexible and user-friendly sys-\ntem library, with a customizable engine design to\nsupport model training on multiple open-source\nLLMs, while also enabling seamless integration\nwith both model APIs and common APIs in a uni-\nfied way. It features an LLM-centric system de-\nsign, which includes open-source LLMs as core\ncontroller, and further interact with a tool-use mod-\nule and a memory module to accomplish complex\ntasks. At the core of ModelScope-Agent , the li-\nbrary supports flexible selection and training on var-\nious open-source LLMs, such as LLaMA (Touvron\net al., 2023), ChatGLM (THUDM, 2023), Chat-\nPLUG (Tian et al., 2023) and other customized\nLLMs in ModelScope. For tool use, ModelScope-\nAgent provides a default tool library, which sup-\nports diverse AI model APIs across NLP, CV , Au-\ndio and Multi-model fields, as well as massive com-\nmon APIs such as search engine. It also supports\nregistering new self-defined API plugins and auto-\nmatic API retrieval from the large tool library. It is\neasy for users to customize their most appropriate\nLLMs, local API tools and functions to develop\nreal-world applications. Moreover, a memory mod-\nule is also introduced to better store and manage the\nsystem message, user history, in-context examples,\ntool message and localized knowledge.\nTo enable the open-source LLMs to better con-\ntrol the whole agent system, we further propose\na comprehensive framework of tool-use data col-\nlection, customized model training, evaluation and\ndeployment. Notably, we release a comprehen-\nsive tool-enhanced dataset MSAgent-Bench, which\nconsists of 598k dialogues with various API cat-\negories, multi-turn API calls, API-Oriented QA,\nand API-Agnostic instructions in both English and\nChinese. A simple training strategy of Weighted\nLM, that enhances the training of generation of\nAPI name and parameters, is used to better ensure\nthe correctness of API calls. Besides, an evalua-\ntion framework is also supported in our library to\nexamine the tool-use abilities of the trained mod-\nels in different aspects. Furthermore, we applied\nModelScope-Agent in a real-world application of\nModelScope Community namely ModelScopeGPT,\nwhich is able to connect open-source LLMs with\nmore than 1000 public AI models and access lo-\ncalized community knowledge in ModelScope for\ncommunity QA.\nTo summarize, ModelScope-Agent is a general\nand customizable agent system designed for devel-\nopers to harness the power of open-source LLMs.\nThe library targets the following goals:\n• Agent based on Open-Source LLMs: the con-\ntroller of ModelScope-Agent can be flexibly\nselected from open-source LLMs that are opti-\nmized through our agent training framework.\n• Support and Customization of Diverse Tools:\nDozens of diverse model APIs and common\nAPIs are given by default. The library sup-\nports registering new self-defined APIs and\nautomatic API retrieval from the toolset.\n• Customizable of Applications: ModelScope-\nAgent can be flexibly applied in various in-\ndustry applications. The agent and training\nframework are documented describing its us-\nage, construction and optimization.\nModelScope-Agent is in continual development\nby the engineers at ModelScope and is released\nunder an Apache 2.0 license. Full documentation\nis available through the project website.\n2 The ModelScope Agent\nModelScope-Agent is designed to facilitate devel-\nopers in building customizable agent systems based\non open-source LLMs. The overall system architec-\nture is shown in Figure 1. It includes open-source\nLLMs as controller, a tool-use module and a mem-\nory module to interact with. Given human instruc-\ntion, the Agent, which adopts the selected LLM\nas the controller, will automatically plan tasks, se-\nlectively use tools, leverage knowledge in memory,\nand finally provide helpful responses to users.\n2.1 LLMs as Brain\nLLMs serve as the brain of the agent, responsible\nfor planning and decomposing user requests, se-\nlectively calling tools, performing retrieval, and\nintegrating all the information from previous steps\nto generate the final response. In order to make it\neasier for users to customize the agent with their\nown LLMs, we have added support for various\nopen-source LLMs by default, such as LLaMA,\nChatGLM and ChatPLUG, which have been op-\ntimized through our tool learning pipeline. The\ndetails of the training strategy and tool-use datasets\ncan be referred to in Section 3. ModelScope-\nAgent has integrated the LLM inference pipeline of\nthe ModelScope community, and replacing LLMs\n567\nFigure 1: The overall system architecture of ModelScope-Agent.\ncan be done by simply setting the model_name\nand model_config. In model_config, the model_id,\nmodel_revision, and model parameter settings such\nas max sequence length, should be configured.\n# LLM config \"cfg_file\"\nfrom modelscope.utils.config import Config\nmodel_cfg = Config.from_file(cfg_file)\nllm = LocalLLM(model_name, model_cfg)\nFurthermore, the ModelScope-Agent also pro-\nvides a standard way to integrate new LLM. Users\ncan add their own LLMs, by integrating the LLM\npipeline into ModelScope. After that, the agent can\nselect the new LLMs for training and inference.\n2.2 Tool Use\nTool Library The tool library is used to config-\nure and manage various collections of APIs used in\nthe agent. ModelScope-Agent can support a wide\nrange of both common APIs such as search APIs,\nand AI model APIs across NLP, CV , Audio and\nMulti-modal models in ModelScope and Hugging-\nFace. Each tool API consists of the API name, de-\nscription, parameters and request functions. Users\ncan easily choose and configure proper APIs in\nthe library to build their own agents. The default\nAPIs supported in the library can be referred to in\nAppendix A.1.\n# tool default config file \"default_file\"\ntool_cfg = Config.from_file(default_file)\nRegister and Customize New ToolThe agent\nallows users to register and customize new tools,\nwhile also supporting quick integration of newly\nregistered tools into the agent, enabling LLMs to\nselectively use the additional self-defined tools for\nspecific applications. This can be simply done\nby inheriting from a base class, namely Tool, and\ndefining a new CustomTool with the API-related\nschema of API name, description, parameters, and\nrequest functions. More details about CustomTool\ncan be referred to in Appendix A.2.\nfrom modelscope_agent.tools import Tool\nclass CustomTool(Tool):\n# logic added here\n# refer example in Appendix A.2\ntool_list = {’customo-tool’: CustomTool()}\nTool Retrieval and ExecutionDue to the large\namount of tool APIs in the tool library, a tool\nretrieval module is further introduced to recom-\nmend appropriate APIs for each instruction prompt.\nSpecifically, we use the dense vector retrieval\nmethod based on the unified multilingual text-\nembedding API 7. We vectorize both the text de-\nscriptions of the APIs and the instruction prompt\nusing the text-embedding API. The top-3 most rel-\nevant APIs with the highest vector product scores\nare selected for tool use. As a result, the schema\ninformation of the retrieved APIs will be concate-\nnated with other system prompts in the subsequent\nmemory module and sent to LLMs as input. With\nthe concatenated instruction prompt, the LLMs will\nplan and generate the API request, which will be\nexecuted by the agent. The agent will then return\nthe results to the LLMs for continuous generation.\n2.3 Memory Control\nThe memory module is used to retrieve and assem-\nble a series of contextual information as input to the\nLLMs. It consists of a knowledge retrieval submod-\nule and a prompt generator submodule, which are\n7https://help.aliyun.com/zh/dashscope/getting-started-1\n568\nresponsible for external knowledge retrieval and\ninstruction prompt generation, respectively.\nKnowledge Retrieval It enables the agent to\nget access to up-to-date and localized information\nrelated with query prompt, thereby augmenting\nLLMs with dynamic and domain-specific knowl-\nedge. We follow the same dense vector retrieval\nmethod as the previous tool retrieval module and\nsupport large-scale knowledge retrieval from local-\nized document corpus. Similarly, it allows users\nto customize by changing to other open-source re-\ntrieval frameworks.\nPrompt Generator The prompt generator is used\nto assemble all available contextual information\nsuch as system prompt, API schema, retrieved\nknowledge, conversation history, and few-shot ex-\namples. According to the type of user query and\nthe maximum length of the LLM, the users can\nselectively choose proper contextual information\nand assemble the required input to the LLM. In our\nagent, the prompt generator needs to be defined\nbefore the agent is constructed.\n2.4 Agent Pipeline\nIn summary, we build the agent by combining all\nthe modules: LLM controller, tool-use module, and\nmemory module. With agent.run, the agent can ef-\nficiently execute and complete the instruction in\na one-step generation. First, the agent retrieves\nquery-related tools through the tool retrieval and\ncombines the retrieved API schema with other con-\ntextual prompts in the memory module, to construct\na new instruction prompt. Then, the agent sends\nthis new prompt to the LLM, which plans whether\nand which API to call and generate an API request.\nNext, the agent will execute the selected API with\nthe extracted API parameters and return the API\nresults to the LLMs, which will continue to plan\nwhether to call other APIs. If another API call\nis needed, the process is repeated, otherwise, the\nLLMs generate the final response and the agent\nreturns the final result to the user.\nagent = AgentExecutor(llm, tool_cfg,\nadditional_tool_list=tool_list)\nagent.run(\"Draw a logo image of agent\")\n3 Training\n3.1 Dataset\nTo facilitate building an agent with the ability to use\ntools while upholding an optimal level of user en-\ngagement, we release a comprehensive tool dataset,\nMSAgent-Bench, utilizing ChatGPT synthetic data\nand the existing instruction-following datasets. Our\nreleased dataset encompasses 598k dialogues. Ta-\nble 1 outlines the key differences between the re-\nleased dataset and other publicly available tool\nlearning datasets, while the data distribution of\nour dataset is illustrated in Figure 2. As demon-\nstrated in the Table and Figure, we have made cer-\ntain efforts to construct a comprehensive dataset\nthat enables the effective training of an agent:\nMultilingual: We collect instances in both Chi-\nnese and English, ensuring that the trained agent is\ncapable of functioning in both languages.\nVarious API Categories:Our dataset supports\nCommon APIs that have been registered by users\nor applied through online API platforms, as well as\nmodel APIs that can call neural models.\nMulti Turn Dialog:In real-life scenarios, agents\nmay need to request more specific clarification\nfrom users to complete a task or receive additional\ninstructions after completing a previous task. Our\ndataset accounts for these scenarios and supports\nmulti-turn user-agent interactions when using tools.\nAPI-Oriented QA:An effective agent should pos-\nsess knowledge of APIs. Our dataset incorporates\nAPI document QA tasks and task planning tasks\nwhich requires agents to offer appropriate sugges-\ntions to users on how to use various APIs to solve\ncomplex tasks.\nAPI-Agnostic Instructions: To enhance the\nagent’s ability to follow common instructions and\nincrease user engagement, we have incorporated\nboth Chinese and English API-agnostic instructions\nwithin our dataset. These instructions place greater\nemphasis on the agent’s inherent capabilities rather\nthan reliance on API invocation.\nThe data was collected by prompting ChatGPT\n(gpt-3.5-turbo) to generate instructions, API re-\nquests, and answers based on the API calling re-\nsults, more details can be accessed in Appendix D.\n3.2 Model Training\nWe use the MSAgent-Bench to fine-tune multi-\nple open-source LLMs, including LLaMA (Tou-\nvron et al., 2023), Qwen (QwenLM, 2023), Chat-\nPLUG (Tian et al., 2023) etc. We train all the\nopen-source LLMs in a multi-round conversation\nmode and concatenate all the prompts and answers.\nCompared to common instruction tuning data, the\ntool learning samples focus more heavily on the\n569\nDataset Language Instance Type # Instances API type Avg. Turn Avg. Step\nAPI-Bank (Li et al., 2023)English Tool Use 264 Common API 3.27 1.92\nToolAlpaca (Tang et al., 2023)English Tool Use 3.9 K Common API 1 1.66\nGorilla (Patil et al., 2023)English Tool Use 16.4 k Model API 1 1\nGPT4Tools (Yang et al., 2023)English Tool Use 71.4 K Model API 1 1\nToolBench (Qin et al., 2023)English Tool Use 26.9 K Common API 1 4.1\nMSAgent-Bench (ours)English + Chinese Tool Use + Common Chat 598 K Common API + Model API 1.52 1.31\nTable 1: The statistics of MSAgent-Bench and other existing tool learning datasets.\nMSAgent-Bench\nModel API•Text-to-Image•Text-to-Video•Text-to-Audio•Translation•Image Chat•Universal IE…Common API•Weather•Web Search•Calculator•Map…\nAPI-Agnostic Instructions•Story Generation•Open QA•Code•Chit Chat•Paraphrase•STEM•Role Play…\nAPI-Oriented QA•Document QA•Task Planning…\nFigure 2: The instance types and distribution of our collected MSAgent-Bench.\naccuracy of tool selection and API parameter pre-\ndiction. Therefore, we propose a simple training\nstrategy, Weighted LM, which enhances the train-\ning of generation of API names and parameters,\nwhile zero-out the loss of tokens from the user\nprompt and the tool execution. More details can be\nreferred to in Appendix B.3.\nkwargs = dict(model=model, ...)\ntrainer: EpochBasedTrainer = build_trainer\n(name=args.trainer, default_args=kwargs)\ntrainer.train()\n4 Evaluation\nOur evaluation system, MSAgent-Eval, comprises\ntwo modules: an automatic evaluation framework\nthat comprehensively evaluates the API usability\nof the agents and a human evaluation framework\nimplemented by an agent arena that reflects the\npreferences of human users.\n4.1 Automatic Evaluation Framework\nIn automatic evaluation, we mainly focus on eval-\nuating the agent’s ability to generate accurate API\nrequests and the proper answers according to the\nAPI calling results. Specifically, we use the action\nexactly match score (Action EM) which measures\nwhether the agent uses the correct API as the ref-\nerence gold API, and the ROUGE-L score which\nmeasures the similarity between the generated re-\nsponse and the gold answer. Additionally, we intro-\nduce a novel metric called Argument F1 for fully\nevaluating the quality of API requests. To com-\npute Argument F1, we categorize the arguments\nin the agent’s API request into two cases, namely\nHalf match (HM) and Full match (FM), represent-\ning the correct argument but with the wrong value\nand the correct argument with the correct value,\nrespectively. Suppose the gold argument number\nin the API is |A|, and the number of arguments in\nthe agent API request is |A∗|, we compute the new\nRecall and Precision as follows:\nR = (0.5 ×# HM + # FM)/|A| (1)\nP = (0.5 ×# HM + # FM)/|A∗| (2)\nand the final argument F1 is computed as:\nF1 = 2(R ∗P)/(R + P). (3)\nA sample code for the automated evaluation of\nagents is provided below:\nfrom tool_agent_finetune import evaluation\nEM, F1, ROUGE = evaluation(refs, preds)\nExpert annotators were engaged to annotate the\nevaluation instances, with the task of providing\ndiverse instructions, manually documenting cor-\nrect API calling requests, and writing appropriate\nresponses. The statistics of our currently assem-\nbled test data is in Appendix B.1, and the auto-\nmatic evaluation scores of our trained agents can\nbe found in Appendix B.2. We also guarantee the\n570\n(a) ModelScope Intelligent Assistant\n (b) Register and Use New Tools on Alibaba Cloud\nFigure 3: Demo cases of ModelScopeGPT based on ModelScope-Agent .\nusers to upload their own annotated test examples\nto accurately evaluate the performance of agents in\ncustomized scenarios.\n4.2 Human Evaluation with Agent Arena\nInspired by the Arena for ChatBots (Zheng et al.,\n2023), we have built an accessible Agent Arena 8\nthat allows users to furnish instructions to two\nanonymous agents, based on the provided APIs.\nSubsequently, users have the opportunity to vote\non which Agent performs better in tackling the in-\nstruction with the given APIs. In accordance with\nthe framework presented by Zheng et al. (2023),\nwe adopt a system of ELO ratings and leaderboard\nmaintenance for the participating Agents.\n5 Usage Example of ModelScopeGPT\nIn this section, we showcase a successful\napplication of ModelScope Community, Mod-\nelScopeGPT9, based on our ModelScope-Agent.\nModelScope Intelligent Assistant Based on\nModelScope-Agent , we have developed an intel-\nligent assistant for the ModelScope Community,\nnamely ModelScopeGPT. It uses LLMs as a con-\ntroller to connect dozens of domain-specific AI\nmodels in the ModelScope open-source community,\ncovering NLP, CV , Audio, and Multi-Modal fields.\nTo make the pipeline more practical, we have in-\ncluded API retrieval and knowledge retrieval tools\nto automatically select proper APIs and get access\nto the local ModelScope knowledge. As shown\nin Figure 3a, ModelScopeGPT can support API\ncalls in multi-turn conversations and generate cor-\nrect API call parameters using information from\n8https://modelscope.cn/studios/LLMZOO/Chinese-\nArena/summary\n9https://modelscope.cn/studios/damo/ModelScopeGPT\n/summary\nprevious conversations. More cases can refer to\nAppendix C. As a result, ModelScopeGPT has\nachieved a total request number of over 170k from\n40k user visits within one month after its release.\nRegister and Use New ToolsAnother key fea-\nture of an agent is its generalization capability to\nunseen APIs. This allows users to quickly register\ntheir own APIs and customize their specific applica-\ntions. Therefore, we test the generalization ability\nof ModelScopeGPT by applying it to an Alibaba\nCloud application scenario. As shown in Figure 3b,\nwe first found an API for renewing an ECS in-\nstance on Alibaba Cloud. Then, we registered the\nAPI schema defined in the tool library to the agent.\nFinally, we entered the prompt \"Please help me re-\nnew an ECS...\" in the demo. The agent generated a\nrequest through planning, selected the appropriate\nAPI, called the API to renew the instance success-\nfully, and provided a reply to inform the user that\nthe renewal was completed. This test demonstrates\nthat the open-source LLM optimized based on the\nreleased API dataset has a strong generalization\nability towards unseen APIs.\n6 Conclusion\nModelScope-Agent aims to facilitate building AI\nAgent applications and research based on open-\nsource LLMs by providing a general and customiz-\nable agent framework covering flexible system de-\nsign, data collection, model training, evaluation\nand usage examples in real-world applications. It\nprovides an open-source, community-driven library\nfor AI Agent learning and best practices for build-\ning an agent system with open-source LLMs. We\nhope ModelScope-Agent can help pave the way\ntowards a new era of AI Agent.\n571\nEthics Statement\nIntended Use. ModelScope-Agent is designed\nto facilitate building AI Agent applications and\nresearch based on open-source LLMs, by providing\na general and customizable agent system.\nPotential Misuse. Although we have only trained\nwith the tool-use datasets and gone through certain\ndata filtering rules, it is still possible that the cus-\ntomized model may generate some biased, fake,\nand unsafe information. Our agent framework also\nprovides users with the freedom to select proper\nLLMs and upload their own clean data for training.\nIt is also important to design specific methods to\nimprove the safety of the agent framework in the\nfuture.\nReferences\nMichael Ahn, Anthony Brohan, Noah Brown, Yev-\ngen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol\nHausman, Alex Herzog, Daniel Ho, Jasmine Hsu,\nJulian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,\nRosario Jauregui Ruano, Kyle Jeffrey, Sally Jes-\nmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalash-\nnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey\nLevine, Yao Lu, Linda Luu, Carolina Parada, Pe-\nter Pastor, Jornell Quiambao, Kanishka Rao, Jarek\nRettinghouse, Diego Reyes, Pierre Sermanet, Nico-\nlas Sievers, Clayton Tan, Alexander Toshev, Vincent\nVanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. 2022. Do as i can,\nnot as i say: Grounding language in robotic affor-\ndances. arXiv preprint arXiv:2204.01691.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMerouane Debbah, Etienne Goffinet, Daniel Heslow,\nJulien Launay, Quentin Malartic, et al. 2023. Falcon-\n40b: an open large language model with state-of-the-\nart performance.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Pete Florence, Andy Zeng, Jonathan Tompson,\nIgor Mordatch, Yevgen Chebotar, Pierre Sermanet,\nTomas Jackson, Noah Brown, Linda Luu, Sergey\nLevine, Karol Hausman, and brian ichter. 2023. In-\nner monologue: Embodied reasoning through plan-\nning with language models. In Proceedings of The\n6th Conference on Robot Learning, volume 205 of\nProceedings of Machine Learning Research, pages\n1769–1782. PMLR.\nHuggingface. 2023. Transformers agent. Website.\nhttps://huggingface.co/docs/transformers/\ntransformers_agents.\nMinghao Li, Feifan Song, Bowen Yu, Haiyang Yu,\nZhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-\nbank: A benchmark for tool-augmented llms. arXiv\npreprint arXiv:2304.08244.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nOpenAI. 2022. Chatgpt: Optimizing language models\nfor dialogue.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nShishir G. Patil, Tianjun Zhang, Xin Wang, and\nJoseph E. Gonzalez. 2023. Gorilla: Large language\n572\nmodel connected with massive apis. arXiv preprint\narXiv:2305.15334.\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen,\nNing Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,\nHuadong Wang, Cheng Qian, Runchu Tian, Kunlun\nZhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen\nZhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi,\nYuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,\nYaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,\nXu Han, Xian Sun, Dahai Li, Jason Phang, Cheng\nYang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and\nMaosong Sun. 2023. Tool learning with foundation\nmodels. arXiv preprint arXiv:2304.08354.\nQwenLM. 2023. Qwen-7b.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhugging face. arXiv preprint arXiv:2303.17580.\nQiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han,\nQiao Liang, and Le Sun. 2023. Toolalpaca: Gener-\nalized tool learning for language models with 3000\nsimulated cases. arXiv preprint arXiv:2306.05301.\nTHUDM. 2023. Chatglm. https://github.com/\nTHUDM/ChatGLM-6B.\nJunfeng Tian, Hehong Chen, Guohai Xu, Ming Yan,\nXing Gao, Jianhai Zhang, Chenliang Li, Jiayi Liu,\nWenshen Xu, Haiyang Xu, Qi Qian, Wei Wang, Qing-\nhao Ye, Jiejing Zhang, Ji Zhang, Fei Huang, and\nJingren Zhou. 2023. Chatplug: Open-domain gen-\nerative dialogue system with internet-augmented in-\nstruction tuning for digital human. arXiv preprint\narXiv:2304.07849.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. arXiv\npreprint arXiv:2302.13971.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xi-\naodong Wang, Zecheng Tang, and Nan Duan.\n2023. Visual chatgpt: Talking, drawing and edit-\ning with visual foundation models. arXiv preprint\narXiv:2303.04671.\nRui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge,\nXiu Li, and Ying Shan. 2023. Gpt4tools: Teaching\nlarge language model to use tools via self-instruction.\narXiv preprint arXiv:2305.18752.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judg-\ning llm-as-a-judge with mt-bench and chatbot arena.\narXiv preprint arXiv:2306.05685.\n573\nA Library\nA.1 Tool List\nAPI Name (language) Description Type\nText-to-Image(en) Converts text to an image. Model API\nText-to-Image(zh) Converts text to an image. Model API\nText-to-Video(en) Converts text to a video. Model API\nText-to-Audio(en) Converts text to audio. Model API\nText-to-Audio(zh) Converts text to audio. Model API\nImage-Chat(en) Image chat. Model API\nTranslation-zh2en Translates Chinese text to English. Model API\nTranslation-en2zh Translates English text to Chinese. Model API\nUniversal-IE(zh) Extracts structured information. Model API\nText-to-Geographic(zh) Extracts geographic information. Model API\nNER(zh) Recognizes named entities in text. Model API\nAPI-Retrieval Retrieves relevant APIs Common API\nModelScope-Retrieval Retrieves modelscope docs. Common API\nTable 2: The statistics of default tool list. Supported\ninput languages for the APIs are listed in parentheses.\nA.2 CustomTool\nUser can customize their own tools by inheriting a\nbase tool and defining the tool names, descriptions,\nand parameters according to a pre-defined schema.\nMoreover, you can implement _local_call() or _re-\nmote_call() depending on your specific require-\nments. To illustrate, below is an example of a\ncustom tool:\nclass CustomTool ( Tool ):\ndescription = ’xxx ’\nname = ’xxx ’\nparameters : list = [{\n’name ’: ’xxx ’,\n’ description ’: ’xxx ’,\n’ required ’: True\n}]\ndef _local_call ():\n...\ndef _remote_call ():\n...\nB Experiment Setup\nB.1 Evaluation Benchmark\nTo assess the generalization of the trained agent,\nwe include 10 in-domain APIs that appear in the\ntraining set of ModelScope-Agent and 10 real un-\nseen APIs10. We also account for the multi-turn\nability of the agent by annotating several multi-turn\nscenarios in our evaluation benchmark. Our test\ninstances were annotated by asking the human ex-\nperts to write diverse instructions first. Then the\nhuman experts were ask to write the JSON API\nrequest and answer the instructions properly after\nobtaining the API calling results. Our final testing\n10In progress, we will include more APIs in the future.\ndataset consisted of 360 conversations with 2059\ntext snippets as the references to be compared with\nthe agent prediction, which comprise 798 API re-\nqusts and 1261 plain text answers according to the\nprevious calling results.\nB.2 Evaluation Results\nModel ROUGE-L Action EM Argument F1\nChatGPT (2-shot)∗ 36.70 34.82 25.51LLaMA 39.16 58.60 44.98ChatPLUG 46.45 68.29 55.12MSAgent-7B 51.35 87.23 68.09\nTable 3: Automatic evaluation results. ∗ represents that\nwe do not fine-tune ChatGPT but use in-context learning\nwith 2 demonstrations.\nWe compare the models trained in our proposed\nModelScopeGPT. The automatic evaluation results\nare shown in Table 3. Based on the findings ob-\ntained from our experimentation, it is evident that\nChatGPT with in-context learning yielded infe-\nrior results as compared to other models that were\nsubjected to finetuning. Furthermore, LLaMA un-\nderperformed when compared to other fine-tuned\nmodels. Our error study revealed that the lower\nperformance of ChatGPT and LLaMA could be at-\ntributed to a large proportion of Chinese test cases\nin our test set. The models (ChatPLUG, MSAgent-\n7B11) that performed better were those that predom-\ninantly focused on Chinese data. Our investigation\nrevealed that ChatGPT and LLaMA exhibited limi-\ntations in user intent recognition, which ultimately\nled to their suboptimal performance on Action\nEM. Among the models examined, MSAgent-7B\ndisplayed the most favorable performance, which\ncould be attributed to the superior performance of\nits basic model.\nB.3 Weighted LM\nWe give an example of the training strategy\nWeighted LM. As show in Figure 4, tokens with\ndifferent colors have different loss weights. For the\nuser input prompt, we set the loss weight to 0, so\nthat the model does not calculate the loss for the\nprompt. For the API-Agnostic text of the assistant,\nwe keep the loss weight as 1. Finally, for the im-\nportant text of the API calling, such as API name,\nparameters, URL, etc., we set the loss weight to 2,\nwhich can improve the generation accuracy of API\ncalling.\n11https://modelscope.cn/models/damo/ModelScope-\nAgent-7B\n574\nFigure 4: Example of training strategy for weighted LM. Different colored tokens have different loss weights.\nFigure 5: Single-step tool-use instructions, text-to-video cases. We have captured a few frames of the video to\ndisplay. Testing the model using the same semantic instruction in both English (left) and Chinese (right).\nFigure 6: Single-step tool-use instructions, image-chat cases. Testing the model using the same semantic instruction\nin both English (left) and Chinese (right).\nC Cases\nIn this section, we show the qualitative results\nabout ModelScopeGPT implementation based on\nModelScope-Agent.\nSingle-step Tool Use As shown in Figure 5 and\n6, the instruction expects the model to generate a\nvideo and chat about the image respectively. These\ninstructions can be completed with a single step of\ntool use.\nMulti-step Tool Use As shown in Figure 7, the\ninstruction expects the model to write the promo-\ntional copy first, then read it, and finally generate a\nvideo. These instructions require the model to have\nthe ability of multi-step Tool use. In the Chinese\ncase, our model accurately completed the three-\nstep tool use.\nMulti-turn Tool Use As shown in Figure 8, the\ninstruction requires the model to have the ability to\nmulti-turn conversation and use the history conver-\nsation. Our model can accurately call the API and\ncapture the content of the previous conversation to\ngenerate API parameters.\n575\nFigure 7: Multi-step tool-use instructions. We have captured a few frames of the video to display. Testing the model\nusing the same semantic instruction in both English(left) and Chinese(right).\nFigure 8: Multi-turn tool-use instructions, text-to-speech and text-to-image cases. Testing the model using the same\nsemantic instruction in both English(left) and Chinese(right).\nFigure 9: Multi-turn tool-use instructions, text-to-speech and text-to-image cases. Testing the model using the same\nsemantic instruction in both English(left) and Chinese(right).\nIn-domain Knowledge QA As shown in Figure\n9, the instruction requires the model to retrieve in-\ndomain knowledge and use the retrieved knowledge\n576\nto answer questions.\nas User\nas Agent\nAPI Gallery\nInstruction orClarification\nAPI request\nFollow-up orFinal Answer\nResult\nFigure 10: The data collection procedure of MSAgent-\nBench.\nD Data Collection Procedure\nWe collected our dataset by using prompt engineer\nto simulate the agent scenarios with two ChatG-\nPTs (gpt-3.5-turbo). One of the ChatGPTs was\nprompted to act as the user, while the other was\nassigned to act as the agent. In order to expand\nthe domains and functionalities of APIs presented\nin the training data, rather than the exsisting real\nAPIs, we also included a number of synthetic APIs\nthat were generated by ChatGPT. When these syn-\nthetic APIs were incorporated into the dialogues,\nwe prompted another ChatGPT to serve as the API\nand return the relevant calling outcomes.\nThe data collection procedure is shown in Fig-\nure 10. Initially, a set of random in-context demon-\nstrations were provided to ChatGPT for generating\nan instruction. This instruction could either be a\nregular one or one that requires solving with APIs,\ndepending on the demonstrations provided. Subse-\nquently, ChatGPT was prompt to act as an agent by\nfirst thinking about which action to undertake. If\nno API calls were deemed necessary, or if the user\nclarification is needed, the agent would respond\nwith a follow-up response to the user. Otherwise\nthe agent will send API request to the API gallery.\nAfter receiving the result of the API call, the agent\nwould assess the situation and decide on the next ac-\ntion. This iterative process of the \"user-agent-API\"\nloop would continue until the agent determined\nthat it was appropriate to terminate the conversa-\ntion with the final answer. After acquiring the raw\ndataset, we applied filtering mechanisms to elim-\ninate instances in which ChatGPT generated API\nrequests containing hallucinated API names and\nparameters that were absent from the retrieved API.\nAdditionally, we excluded instances in which Chat-\nGPT generated illegal API requests, thus resulting\nin a refined and finalized dataset.\nAs introduced in Section 3.1, we collect in-\nstances across different languages and topics, the\ndetailed statistics of our collected data are shown\nin Table 4.\nInstance Type # Instances\nChinese 532,436\nEnglish 66,444\nCommon API 211,026\nModel API 58,338\nAPI-Oriented QA 5,000\nAPI-Agnostic Instruction 329,776\nTable 4: The statistics of our collected dataset.\nE Related Work\nE.1 Large Language Models\nRecent years have witnessed rapid development in\nthe field of Large Language Models (LLMs). Typ-\nical models, such as GPT3 (Brown et al., 2020),\nGopher (Rae et al., 2021), Chinchilla (Hoffmann\net al., 2022), PaLM (Chowdhery et al., 2022) and\nLLaMA (Touvron et al., 2023), have shown im-\npressive zero and few-shot generalization abilities\non a wide range of NLP tasks, by scaling up the\nmodel and data size. A remarkable milestone is the\nrelease of ChatGPT (OpenAI, 2022) or GPT4 (Ope-\nnAI, 2023), which has greatly revolutionized the\nparadigm of AI development. As a result, a rising\ntrend of open-source LLMs has emerged to chal-\nlenge and catch up their closed-source counterparts\nlike ChatGPT and Claude, such as BLOOM (Muen-\nnighoff et al., 2022), LLaMA (Touvron et al.,\n2023), Falcon (Almazrouei et al., 2023), Chat-\nGLM (THUDM, 2023). Despite the great break-\nthrough, LLMs are trained as text generators over\nplain text corpora, thus performing less well on\nother tasks such as multi-modal tasks. It also falls\nshort on tasks that require up-to-date information,\nwhich are beyond the pretraining data. Using tools\nor external APIs can help overcome the limitations\nand harness the power of LLMs to facilitate seam-\n577\nless connections with downstream applications. In\nModelScope-Agent , we provide the whole cus-\ntomizable framework and best practices for build-\ning an agent system, which enables open-source\nLLMs to use tools and external APIs.\nE.2 Agent & Tool Learning\nThe utilization of Large Language Models (LLMs)\nas a controller to construct an agent system has\nemerged as a prominent research area. Several re-\nlated works employ prompt engineering techniques\non closed-source LLMs, such as ChatGPT (Ope-\nnAI, 2022) and Claude, to enable their applica-\ntion in specific domains. For instance, Visual-\nChatGPT (Wu et al., 2023) and HuggingGPT (Shen\net al., 2023) facilitate the HuggingFace model call-\nings accessible to OpenAI LLMs. SayCan (Ahn\net al., 2022) and inner monologue (Huang et al.,\n2023) integrate LLMs with robots to achieve\nrobotic systems. Notably, recent works such\nas Langchain and Auto-GPT encompass a wide\nrange of tools, including common APIs and neu-\nral models, and enhance long-term reasoning\nand human-agent interaction whilst solving tasks,\nwhich demonstrate the immense potential for build-\ning a generalized agent.\nNumerous endeavors have also been made\nto enable open-source LLMs to utilize tools.\nFor instance, Gorilla (Patil et al., 2023) and\nGPT4Tools (Yang et al., 2023) generate training\ndata using self-instruction techniques to train open-\nsource LLMs to effectively utilize neural mod-\nels. ToolAlpaca (Tang et al., 2023) and ToolL-\nLaMA (Qin et al., 2023) train LLAMA using com-\nmon APIs, with the distinction that ToolAlpaca\nemploys synthetic APIs from LLMS, whereas Tool-\nLLaMA utilizes real APIs.\nOverall, compared to the above-mentioned meth-\nods, ModelScope-Agent differs in the following\naspects. Firstly, our method includes a universal\ntraining framework that supports user-customized\nagent learning for open-source models to meet in-\ndustrial needs. Secondly, ModelScope-Agent can\nsupport various APIs in different fields, including\nmodel APIs and common APIs, while previous\nworks only support certain specific APIs.\nF Future Work\nIn the future, we will evolve to support more\nsophisticated agent architectures such as ReAct\nand code interpreter. In the meantime, we will\ncontinuously improve the capabilities required by\nopen-source LLMs as agents. ModelScope-Agent\nrelies on the ModelScope community and will\nadapt to more new open-source LLMs in the fu-\nture, providing more applications developed based\non ModelScope-Agent, such as personal-assistant-\nagent, story-agent, motion agent, and so on.\n578",
  "topic": "Zhàng",
  "concepts": [
    {
      "name": "Zhàng",
      "score": 0.8884564638137817
    },
    {
      "name": "Chen",
      "score": 0.7776600122451782
    },
    {
      "name": "Computer science",
      "score": 0.6154704689979553
    },
    {
      "name": "Natural language processing",
      "score": 0.4395027756690979
    },
    {
      "name": "Open source",
      "score": 0.41134709119796753
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4041592478752136
    },
    {
      "name": "Programming language",
      "score": 0.3916781544685364
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3353859484195709
    },
    {
      "name": "History",
      "score": 0.08858382701873779
    },
    {
      "name": "China",
      "score": 0.07376769185066223
    },
    {
      "name": "Archaeology",
      "score": 0.06033185124397278
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Software",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I45928872",
      "name": "Alibaba Group (China)",
      "country": "CN"
    }
  ],
  "cited_by": 10
}