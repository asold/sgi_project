{
  "title": "silpa_nlp at SemEval-2022 Tasks 11: Transformer based NER models for Hindi and Bangla languages",
  "url": "https://openalex.org/W4287889470",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5003354816",
      "name": "Sumit Kumar Singh",
      "affiliations": [
        "Indian Institute of Information Technology Allahabad"
      ]
    },
    {
      "id": "https://openalex.org/A5009386004",
      "name": "Pawankumar Jawale",
      "affiliations": [
        "Indian Institute of Information Technology Allahabad"
      ]
    },
    {
      "id": "https://openalex.org/A5060902547",
      "name": "Uma Tiwary",
      "affiliations": [
        "Indian Institute of Information Technology Allahabad"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1533057952",
    "https://openalex.org/W173870552",
    "https://openalex.org/W4236137412",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W24163152",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3011594683",
    "https://openalex.org/W1543884596",
    "https://openalex.org/W4205883286",
    "https://openalex.org/W4294294857",
    "https://openalex.org/W4287854446",
    "https://openalex.org/W3136221257",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2757931374",
    "https://openalex.org/W4229872008",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2143345705",
    "https://openalex.org/W1999897823",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2114361266",
    "https://openalex.org/W2151831732",
    "https://openalex.org/W2008056655",
    "https://openalex.org/W2785349534",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2398489001",
    "https://openalex.org/W3099919888",
    "https://openalex.org/W2533179929",
    "https://openalex.org/W2140988362",
    "https://openalex.org/W2137256672",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2963250244"
  ],
  "abstract": "We present Transformer based pretrained models, which are fine-tuned for Named Entity Recognition (NER) task. Our team participated in SemEval-2022 Task 11 MultiCoNER: Multilingual Complex Named Entity Recognition task for Hindi and Bangla. Result comparison of six models (mBERT, IndicBERT, MuRIL (Base), MuRIL (Large), XLM-RoBERTa (Base) and XLM-RoBERTa (Large) ) has been performed. It is found that among these models MuRIL (Large) model performs better for both the Hindi and Bangla languages. Its F1-Scores for Hindi and Bangla are 0.69 and 0.59 respectively.",
  "full_text": "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), pages 1536 - 1542\nJuly 14-15, 2022 ©2022 Association for Computational Linguistics\nsilpa_nlp at SemEval-2022 Tasks 11: Transformer based NER models for\nHindi and Bangla languages\nPawankumar Jawale ∗ Sumit Singh ∗ Uma Shanker Tiwary\nIndian Institute of Information Technology, Allahabad, UP, India\n{pawankumar.jawale, sumitrsch}@gmail.com\nust@iiita.ac.in\nAbstract\nWe present Transformer based pretrained mod-\nels, which are ﬁne-tuned for Named Entity\nRecognition (NER) task. Our team par-\nticipated in SemEval-2022 Task 11 Multi-\nCoNER: Multilingual Complex Named En-\ntity Recognition task for Hindi and Bangla.\nResult comparison of six models (mBERT,\nIndicBERT, MuRIL (Base), MuRIL (Large),\nXLM-RoBERTa (Base) and XLM-RoBERTa\n(Large) ) has been performed. It is found that\namong these models MuRIL (Large) model\nperforms better for both the Hindi and Bangla\nlanguages. Its F1-Scores for Hindi and Bangla\nare 0.69 and 0.59 respectively.\n1 Introduction\nNamed Entity Recognition (NER) is one of the\nhot topic in natural language processing (NLP).\nNER is a task of identiﬁcation of named entities\nfrom given sentence and their classiﬁcation into\npredeﬁned classes like Person, Location, Organi-\nsation, Corporation etc. For below sentence:\nराम Ǒदल्ली में गूगल में काम करता है।,\nराम is Person, Ǒदल्ली is Location and गूगल is\nCorporation.\nThe application of NER can be found in\nother NLP tasks such as text summarization\n(Toda and Kataoka , 2005), information retrieval,\nmachine translation ( Babych and Hartley , 2003),\nquestion-answering ( Molla Aliod et al. , 2009).\nThe researchers have come up with many ap-\nproaches for NER task such as Rule-based\n(Krupka and IsoQuest , 2005), feature-based\nSupervised approach ( Liao and Veeramachaneni ,\n2009), Unsupervised approach and Deep learning\nbased approach ( Li et al. , 2020) and Transformer\nbased approach ( Vaswani et al. , 2017).\n∗ Authors equally contributed to this work.\nThe Transformer models are good at capturing\nfeatures from lengthy sentences compared to re-\ncurrent neural networks ( Vaswani et al. , 2017).\nRoBERTa model ( Liu et al. , 2019) performed\ngood for NER task for rich resource languages like\nEnglish.\nFor Hindi and Bangla languages, we applied\nXLM-RoBERTa, which is a multilingual version\nof RoBERTa pre-trained in 100 languages (in-\ncluding Hindi and Bangla). We also applied In-\ndicBERT ( Kakwani et al. , 2020) and mBERT ( De-\nvlin et al. , 2018). At last, we applied MuRIL\n(Khanuja et al. , 2021; Sharma et al. , 2022) which\nis speciﬁcally pre-trained in the text of 17 Indic\nlanguages and it gave better result than above mod-\nels for the NER task.\nThis paper consists of a total of six sections\napart from the introduction. Section 2 brieﬂy de-\nﬁnes the problem deﬁnition and task provided by\norganizers. Section 3 discusses the work done till\nnow on the NER task. Section 4 mentions the\ndataset being used in this paper. Section 5 de-\nscribes the general Transformer architecture for\nthe NER task, along with prepossessing and post-\nprocessing. Section 6 discusses the results ob-\ntained by used models on both Hindi and Bangla\nlanguages and the error analysis. Finally, Section\n7 concludes this paperwork.\n2 Problem Deﬁnition\nThe organisers ( Malmasi et al. , 2022b) have ar-\nranged 13 tasks according to language. They have\nprovided a separate dataset for each task. Each\ndataset is comprised of training, development and\ntesting. Respective named entity tags were pro-\nvided in the training and development dataset.\nOnly tokens were provided in the testing dataset.\nParticipants were required to train the models us-\ning the training and development dataset and pre-\ndict NER tags on the testing dataset. We have\nworked on Hindi and Bangla tasks.\n1536\n3 Related Work\nSeveral works have been done on NER that can\nbe categorized under two broad categories: tradi-\ntional and deep learning methods.\n3.1 Traditional NER approaches\nIn this approach, feature engineering is carried out\nby the researchers ( Li et al. , 2020). Under this\ncategory comes the rule-based, feature-based su-\npervised learning, and unsupervised learning ap-\nproaches.\nIn the rule-based method, the hand-crafted se-\nmantic and syntactic features are provided to rec-\nognize the entities ( Krupka and IsoQuest , 2005;\nAone et al. , 1998). These rules-based systems can\nnot be extended to other domains because they\ndepend on domain-speciﬁc rules ( Appelt et al. ,\n1995).\nIn the feature-based supervised approach, fea-\nture engineering plays a critical role. Features\nsuch as word-level features ( Liao and Veeramacha-\nneni, 2009; Settles, 2004) and document-level fea-\ntures ( Ravin and Wacholder , 1997; Zhu et al. ,\n2005) are used. These features are then passed\nthrough supervised models: HMM ( Eddy, 1998),\nDecision trees ( Quinlan, 1986), SVM ( Hearst\net al. , 1998) and CRF ( Lafferty et al. , 2001) for\nthe classiﬁcation in the labeled corpus.\nIn the unsupervised approach, the lexical pat-\nterns and statistical features are computed, which\nhelps in the clustering ( Collins and Singer , 1999).\nThe clustering approach is applied as the data is\nnot labeled in these cases. They extract named en-\ntities by making clusters depending on the context\nsimilarity ( Nadeau et al. , 2006).\n3.2 Deep Learning NER approaches\nAs compared to the traditional NER approach, this\napproach does not explicitly need features. These\nmodels automatically extract the hidden features,\ndue to which the accuracies of these models are\nhigh compared to the traditional NER approaches.\nThis approach involves the work done using multi-\nlevel perceptrons, CNN ( Wu et al. , 2015), and BiL-\nSTM ( Wei et al. , 2016; Lin et al. , 2017). Recently\nthe Transformer-based models have gained signif-\nicant advancement in this ﬁeld ( Wolf et al. , 2020).\nThe Transformer-based models are good at cap-\nturing features in lengthy sentences as compared\nto recurrent neural networks. The Transformer\n(Vaswani et al. , 2017) is equipped with parallel\ntraining and made up of a pair of an encoder and a\ndecoder (to get sequence to sequence prediction).\nFor NER task encoder is used.\nIn paper ( Devlin et al. , 2019), the authors\nhave performed NER task on CoNLL-2003 dataset\n(Tjong Kim Sang and De Meulder , 2003). The au-\nthors applied both variants of BERT (Large and\nBase). The Large variant achieved an F1-score\nof 92.8 on the test set, whereas the base variant\nachieved F1-score of 92.4 on the test set. The au-\nthors have used BERT as word embedding and fed\nthis to the BiLSTM. XLM-RoBERTa ( Conneau\net al. , 2020) outperform the mBERT ( Devlin et al. ,\n2019) and XLM ( CONNEAU and Lample , 2019)\nand show strong improvements over low-resource\nlanguages.\n4 Data\nThe dataset ( Malmasi et al. , 2022a) for Hindi\nand Bangla contains six different NER entities,\nnamely Location (LOC), Person (PER), Produc-\ntion (PROD), Group (GRP), Corporation (CORP)\nand Creative Work (CW). The dataset is in stan-\ndard CONLL format, which uses BIO (Beginning-\nInside-Outside) tagging. The dataset provided\nwas of three types, namely training, develop-\nment and testing. The training and development\ndata contains tokens with tags, whereas testing\ndata contains only tokens. For both Hindi and\nBangla tracks, there were 15300 samples in train-\ning and 800 samples in development. In the test-\nTag Training Development\nB-LOC 2614 131\nB-PER 2418 133\nB-PROD 3077 169\nB-GRP 2843 148\nB-CORP 2700 134\nB-CW 2304 113\nI-LOC 1604 77\nI-PER 2836 166\nI-PROD 2295 107\nI-GRP 5821 297\nI-CORP 2917 138\nI-CW 3592 151\nO 209545 10882\nTotal 244566 12646\nTable 1: Entity distribution for Hindi track\ning dataset, for Hindi and Bangla track there were\n141565 (with 933273 total tokens) and 133119\n1537\n(with 693886 total tokens) samples, respectively.\nTables 1 and 2 shows the number of each entity in\nthe training and development dataset for Hindi and\nBangla, respectively.\nTag Training Development\nB-LOC 2351 101\nB-PER 2606 144\nB-PROD 3188 190\nB-GRP 2405 118\nB-CORP 2598 127\nB-CW 2157 120\nI-LOC 1453 61\nI-PER 3132 180\nI-PROD 1964 129\nI-GRP 4248 226\nI-CORP 2701 122\nI-CW 2844 161\nO 160250 8654\nTotal 191897 10333\nTable 2: Entity distribution for Bangla track\n5 Methodology\nThis work ﬁne tuned 6 Transformer ( Vaswani\net al. , 2017) based pre-trained model for the\ntask. IndicBERT ( Kakwani et al. , 2020) is Al-\nbert based model which is pre-trained on 11 In-\ndic languages, including Hindi and Bangla. We\nalso ﬁne-tuned XLM-RoBERTa (Base) and XLM-\nRoBERTa (Large) ( Conneau et al. , 2020), which is\npre-trained on text in 100 languages. Other mod-\nels are mBERT ( Devlin et al. , 2018) which is pre-\ntrained on text in 104 languages and MuRIL Base\nand MuRIL Large ( Khanuja et al. , 2021) which is\npre-trained on text in 17 languages with explicitly\naugmented monolingual text corpora with trans-\nlated and transliterated document pairs. All the\ncorpora describe above include Hindi and Bangla\nlanguages.\nFigure 1 shows the architecture of this work,\nwhich is divided into 3 sections: Preprocessing,\nFine tuning and Post processing.\n5.1 Preprocessing\nXLM-RoBERTa (Conneau et al. , 2020) model and\nIndicBERT ( Kakwani et al. , 2020) uses Senten-\ncePiece tokeniser ( Kudo and Richardson , 2018),\nwhich is language independent subword tokeniser\nand detokeniser. mBERT, MuRIL Base and\nMuRIL Large model uses WordPiece tokeniser\nFigure 1: Generalized transformer-based model\n(Wu et al. , 2016). As all models use subword to-\nkeniser, any token may get divided into more than\none subword. Therefore an alignment of the la-\nbel is required for that token. Each subword is as-\nsigned with the same label as the tokenised word.\nIn ﬁgure 2, the token पैंजर is divided by tokeniser\ninto two subwords: 'पै' and '◌ंजर', both subwords\ngets B-CW as their label and the token थी। is di-\nvided by tokeniser into two subwords: 'थी' and '।',\nboth subwords gets O as their label. Tokenised sen-\nFigure 2: Label alignment\ntences are added with special tokens along with\npadding tokens, and thereafter, all the tokens re-\nplaced with their ID values for feeding into the\nmodels.\n5.2 Fine tuning\nArchitecture in Figure 1 shows that a fully con-\nnected layer added on ﬁnal output hidden vector of\nthe model. This layer takes word embedding cor-\nresponding to each token generated by the model\n1538\n(base models generate word embedding of 768 di-\nmension and large models generate word embed-\nding of 1024 dimension) and maps each embed-\nding to the output layer of size (13,), which is\nthe number of unique labels of our task. Fur-\nther, we calculate loss using the cross-entropy loss\nfunction. This model is optimized with Adamw\n(Loshchilov and Hutter , 2019) and L2 weight de-\ncay of 0.01. It is ﬁne-tuned with the dynamic learn-\ning rate with linear learning rate scheduler with\nmax learning rate 4e-5, and also, batch size varies\nfrom 8 to 64 for different models subject to op-\ntimization and a dropout of 0.1 on all layers ap-\nplied. Maximum token length is taken between\n84 to 128 depending on the maximum length of\ntokenised sentences, which helps in faster train-\ning. Number of epochs for training were 30 for\nall the models in this experiment. We chose best\nmodel based on calculated F1-score on valid data.\nThis work predicts the sequence of labels by the\nargmax of the ﬁnal layer for each tokens. All\nmodels along with the fully-connected layer im-\nplemented by XXXForTokenClassiﬁcation ( Wolf\net al. , 2020), where XXX refers the corresponding\nmodel.\n5.3 Post processing\nAfter the generation of labels from the model, la-\nbels are realigned according to the detokenised\nsentence. This is reverse of label alignment, dis-\ncussed in the Preprocessing section. The labels of\nall the tokens which are ﬁrst tokens of their origi-\nnal word, are taken as generated labels.\nModel Precision Recall F1-Score\n(%) (%) (%)\nM1 47.99 45.77 46.42\nM2 51.05 48.08 48.97\nM3 62.59 61.49 61.81\nM4 70.06 69.07 69.08\nM5 47.31 45.98 46.01\nM6 51.90 47.90 49.55\nTable 3: Results of each model on Hindi test data\n(M1: mBERT, M2: IndicBERT, M3: MuRIL Base, M4:\nMuRIL Large, M5: XLM-RoBERTa Base M5: XLM-\nRoBERTa Large)\n6 Results and Analysis\nTable 3 and 4 shows the macro average of Preci-\nsion, Recall and F1-score of each model on testing\nModel Precision Recall F1-Score\n(%) (%) (%)\nM1 45.28 41.54 42.47\nM2 43.40 37.48 38.55\nM3 56.98 56.73 56.71\nM4 60.25 59.27 59.52\nM5 34.75 32.26 33.37\nM6 38.74 33.07 35.45\nTable 4: Results of each model on Bangla test data\n(M1: mBERT, M2: IndicBERT, M3: MuRIL Base, M4:\nMuRIL Large, M5: XLM-RoBERTa Base M5: XLM-\nRoBERTa Large)\ndataset for Hindi and Bangla respectively.\nTables 6 and 7 present the Entity-wise F1 score\nfor Hindi and Bangla testing dataset correspond-\ning to each NER model. It has been found that\nMuRIL (Large) is showing the highest F1 score for\neach entity. It has also been observed that the F1\nscore for CW (Creative work) is the least among\nall the entities. It indicates that predicting CW is\nthe most difﬁcult for the model.\nSentence अब तक का सबसे बड़ा\nबािलका बधू (1976 Ǒफ़ल्म)\nGold\nannotation\n[ O, O, O, O, O, B-CW, I-CW,\nI-CW, I-CW ]\nmBERT [ O, O, O, O, O, O, B-CW,\nI-CW, I-CW ]\nIndicBERT [ O, O, O, O, O, O, B-CW,\nI-CW, I-CW ]\nMuRIL [ O, O, O, O, O, B-CW, I-CW,\nI-CW, I-CW ]\nXLM-\nRoBERTa\nLarge\n[ O, O, O, O, O, O, B-CW,\nI-CW, I-CW ]\nTable 5: Comparative analysis of a sentence from test\ncorpus\nFinally, Table 5 presents the comparative results\nobtained using different transformer models. Here,\nthe MuRIL output is close to Ground annotation\ncompared to other models.\n7 Conclusion\nResults show that large models are better than\ntheir corresponding base models. MuRIL (Large)\nmodel is the best among all six models described\nabove and the second-best model is MuRIL (Base).\n1539\nEntity M1 M2 M3 M4 M5 M6\nLOC 51.13 52.44 61.52 67.43 49.06 5077\nPER 51.50 58.11 71.09 77.86 51.76 5589\nPROD 40.57 47.78 59.54 69.01 38.93 4302\nGRP 46.74 49.92 63.78 71.48 50.37 5357\nCW 38.83 31.64 51.49 56.95 33.97 3942\nCORP 49.77 53.95 63.46 71.78 51.96 5466\nAvg. 46.42 48.97 61.81 69.08 46.01 49.55\nTable 6: Entity-wise F1-score of each model for Hindi dataset\n(M1: mBERT, M2: IndicBERT, M3: MuRIL Base, M4: MuRIL Large,\nM5: XLM-RoBERTa Base, M6: XLM-RoBERTa Large)\nEntity M1 M2 M3 M4 M5 M6\nLOC 48.91 43.38 54.56 55.93 37.99 38.03\nPER 56.35 56.71 74.98 78.21 45.10 48.28\nPROD 39.28 40.66 55.03 63.54 37.60 37.53\nGRP 35.79 29.62 53.77 48.03 23.45 26.75\nCW 30.44 22.79 40.78 48.38 18.11 20.97\nCORP 44.09 38.17 61.14 63.04 38.00 41.14\nAvg. 42.47 38.55 56.71 59.52 33.37 35.45\nTable 7: Entity-wise F1-score of each model for Bangla dataset\n(M1: mBERT, M2: IndicBERT, M3: MuRIL Base, M4: MuRIL Large,\nM5: XLM-RoBERTa Base M5: XLM-RoBERTa Large)\nPredicting labels corresponding to Creative Work\n(CW) is most challenging for all the models and\npredicting labels corresponding to Person (PER)\nis easier than predicting other labels.\nReferences\nChinatsu Aone, Lauren Halverson, Tom Hampton, and\nMila Ramos-Santacruz. 1998. Sra: Description of\nthe ie2 system used for muc-7. In Seventh Message\nUnderstanding Conference (MUC-7): Proceedings\nof a Conference Held in Fairfax, Virginia, April 29-\nMay 1, 1998.\nDouglas Appelt, Jerry R Hobbs, John Bear, David Is-\nrael, Megumi Kameyama, Andrew Kehler, David\nMartin, Karen Myers, and Mabry Tyson. 1995. Sri\ninternational fastus systemmuc-6 test results and\nanalysis. In Sixth Message Understanding Confer-\nence (MUC-6): Proceedings of a Conference Held\nin Columbia, Maryland, November 6-8, 1995.\nBogdan Babych and Anthony Hartley. 2003. Im-\nproving machine translation quality with automatic\nnamed entity recognition. In Proceedings of the\n7th International EAMT Workshop on MT and Other\nLanguage Technology Tools, Improving MT through\nOther Language Technology Tools: Resources and\nTools for Building MT, EAMT ’03, page 18, USA.\nAssociation for Computational Linguistics.\nMichael Collins and Yoram Singer. 1999. Unsuper-\nvised models for named entity classiﬁcation. In\n1999 Joint SIGDAT conference on empirical meth-\nods in natural language processing and very large\ncorpora.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale .\nAlexis CONNEAU and Guillaume Lample. 2019.\nCross-lingual language model pretraining . In Ad-\nvances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nSean R. Eddy. 1998. Proﬁle hidden markov models.\nBioinformatics (Oxford, England), 14(9):755–763.\nMarti A. Hearst, Susan T Dumais, Edgar Osuna, John\nPlatt, and Bernhard Scholkopf. 1998. Support vec-\ntor machines. IEEE Intelligent Systems and their ap-\nplications, 13(4):18–28.\n1540\nDivyanshu Kakwani, Anoop Kunchukuttan, Satish\nGolla, Gokul N.C., Avik Bhattacharyya, Mitesh M.\nKhapra, and Pratyush Kumar. 2020. IndicNLPSuite:\nMonolingual Corpora, Evaluation Benchmarks and\nPre-trained Multilingual Language Models for In-\ndian Languages. In Findings of EMNLP.\nSimran Khanuja, Diksha Bansal, Sarvesh Mehtani,\nSavya Khosla, Atreyee Dey, Balaji Gopalan,\nDilip Kumar Margam, Pooja Aggarwal, Rajiv Teja\nNagipogu, Shachi Dave, Shruti Gupta, Subhash\nChandra Bose Gali, Vish Subramanian, and Partha\nTalukdar. 2021. Muril: Multilingual representations\nfor indian languages .\nGR Krupka and K IsoQuest. 2005. Description of the\nnerowl extractor system as used for muc-7. In Proc.\n7th Message Understanding Conf, pages 21–28.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing .\nJohn Lafferty, Andrew McCallum, and Fernando CN\nPereira. 2001. Conditional random ﬁelds: Prob-\nabilistic models for segmenting and labeling se-\nquence data.\nJing Li, Aixin Sun, Jianglei Han, and Chenliang Li.\n2020. A survey on deep learning for named entity\nrecognition.\nWenhui Liao and Sriharsha Veeramachaneni. 2009. A\nsimple semi-supervised algorithm for named entity\nrecognition. In Proceedings of the NAACL HLT\n2009 Workshop on Semi-Supervised Learning for\nNatural Language Processing, pages 58–65.\nBill Yuchen Lin, Frank F Xu, Zhiyi Luo, and Kenny\nZhu. 2017. Multi-channel bilstm-crf model for\nemerging named entity recognition in social media.\nIn Proceedings of the 3rd Workshop on Noisy User-\ngenerated Text, pages 160–165.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization .\nShervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta\nKar, and Oleg Rokhlenko. 2022a. MultiCoNER:\na Large-scale Multilingual dataset for Complex\nNamed Entity Recognition.\nShervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta\nKar, and Oleg Rokhlenko. 2022b. SemEval-2022\nTask 11: Multilingual Complex Named Entity\nRecognition (MultiCoNER). In Proceedings of\nthe 16th International Workshop on Semantic Eval-\nuation (SemEval-2022). Association for Computa-\ntional Linguistics.\nDiego Molla Aliod, Menno Zaanen, and Daniel Smith.\n2009. Named entity recognition for question an-\nswering. pages 51–58.\nDavid Nadeau, Peter D Turney, and Stan Matwin. 2006.\nUnsupervised named-entity recognition: Generating\ngazetteers and resolving ambiguity. In Conference\nof the Canadian society for computational studies of\nintelligence, pages 266–277. Springer.\nJ Quinlan. 1986. Induction of decision trees. mach.\nlearn.\nYael Ravin and Nina Wacholder. 1997. Extracting\nnames from natural-language text. Citeseer.\nBurr Settles. 2004. Biomedical named entity recogni-\ntion using conditional random ﬁelds and rich feature\nsets. In Proceedings of the international joint work-\nshop on natural language processing in biomedicine\nand its applications (NLPBA/BioNLP), pages 107–\n110.\nRicha Sharma, Sudha Morwal, and Basant Agarwal.\n2022. Named entity recognition using neural lan-\nguage model and crf for hindi language . Computer\nSpeech Language, 74:101356.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition . In\nProceedings of the Seventh Conference on Natu-\nral Language Learning at HLT-NAACL 2003, pages\n142–147.\nHiroyuki Toda and Ryoji Kataoka. 2005. A search\nresult clustering method using informatively named\nentities. In Proceedings of the 7th Annual ACM In-\nternational Workshop on Web Information and Data\nManagement, WIDM ’05, page 8186, New York,\nNY , USA. Association for Computing Machinery.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need .\nQikang Wei, Tao Chen, Ruifeng Xu, Yulan He, and\nLin Gui. 2016. Disease named entity recognition\nby combining conditional random ﬁelds and bidirec-\ntional recurrent neural networks. Database, 2016.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nHuggingface’s transformers: State-of-the-art natural\nlanguage processing .\nYonghui Wu, Min Jiang, Jianbo Lei, and Hua Xu. 2015.\nNamed entity recognition in chinese clinical text us-\ning deep neural network. Studies in health technol-\nogy and informatics, 216:624.\n1541\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, ukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation .\nJianhan Zhu, Victoria Uren, and Enrico Motta. 2005.\nEspotter: Adaptive named entity recognition for web\nbrowsing. In Biennial Conference on Professional\nKnowledge Management/Wissensmanagement ,\npages 518–529. Springer.\n1542",
  "topic": "Bengali",
  "concepts": [
    {
      "name": "Bengali",
      "score": 0.9302676916122437
    },
    {
      "name": "Hindi",
      "score": 0.8774593472480774
    },
    {
      "name": "Computer science",
      "score": 0.85140061378479
    },
    {
      "name": "SemEval",
      "score": 0.7669987678527832
    },
    {
      "name": "Named-entity recognition",
      "score": 0.7382006645202637
    },
    {
      "name": "Natural language processing",
      "score": 0.7341251373291016
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7074954509735107
    },
    {
      "name": "Transformer",
      "score": 0.6602694988250732
    },
    {
      "name": "Task (project management)",
      "score": 0.5510674715042114
    },
    {
      "name": "Language model",
      "score": 0.44821304082870483
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I26072440",
      "name": "Indian Institute of Information Technology Allahabad",
      "country": "IN"
    }
  ]
}