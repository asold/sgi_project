{
  "title": "Semantic-Oriented Unlabeled Priming for Large-Scale Language Models",
  "url": "https://openalex.org/W4385571891",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2140319040",
      "name": "Yanchen Liu",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2738082409",
      "name": "Timo Schick",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5092596305",
      "name": "Hinrich Schtze",
      "affiliations": [
        "LMU Klinikum",
        "Ludwig-Maximilians-Universität München"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W3116459227",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3175603587",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4287026929",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3134354193",
    "https://openalex.org/W3199958362",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W3156636935"
  ],
  "abstract": "Due to the high costs associated with finetuning large language models, various recent works propose to adapt them to specific tasks without any parameter updates through in-context learning.Unfortunately, for in-context learning there is currently no way to leverage unlabeled data, which is often much easier to obtain in large quantities than labeled examples.In this work, we therefore investigate ways to make use of unlabeled examples to improve the zero-shot performance of pretrained language models without any finetuning: We introduce Semantic-Oriented Unlabeled Priming (SOUP), a method that classifies examples by retrieving semantically similar unlabeled examples, assigning labels to them in a zero-shot fashion, and then using them for in-context learning.We also propose bag-of-contexts priming, a new priming strategy that is more suitable for our setting and enables the usage of more examples than fit into the context window.",
  "full_text": "Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP), pages 32–38\nJuly 13, 2023 ©2023 Association for Computational Linguistics\nSemantic-Oriented Unlabeled Priming for Large-Scale Language Models\nYanchen Liu1 Timo Schick2 Hinrich Schütze3\n1Harvard University 2Meta AI Research 3LMU Munich\nyanchenliu@g.harvard.edu, schick@meta.com\nAbstract\nDue to the high costs associated with finetuning\nlarge language models, various recent works\npropose to adapt them to specific tasks with-\nout any parameter updates through in-context\nlearning. Unfortunately, for in-context learn-\ning there is currently no way to leverage unla-\nbeled data, which is often much easier to ob-\ntain in large quantities than labeled examples.\nIn this work, we therefore investigate ways to\nmake use of unlabeled examples to improve the\nzero-shot performance of pretrained language\nmodels without any finetuning: We introduce\nSemantic-Oriented Unlabeled Priming (SOUP ),\na method that classifies examples by retrieving\nsemantically similar unlabeled examples, as-\nsigning labels to them in a zero-shot fashion,\nand then using them for in-context learning. We\nalso propose bag-of-contexts priming, a new\npriming strategy that is more suitable for our\nsetting and enables the usage of more examples\nthan fit into the context window.\n1 Introduction\nIn recent years, there has been a trend in NLP to-\nwards larger and larger language models (LMs)\n(Radford et al., 2018, 2019; Raffel et al., 2020;\nBrown et al., 2020; Fedus et al., 2021). Different\nfrom prior pretrained LMs that are typically fine-\ntuned for specific downstream tasks using labeled\ntraining datasets (Devlin et al., 2019; Liu et al.,\n2019), recent work proposes to use such large mod-\nels in zero- or few-shot settings without any fine-\ntuning (Brown et al., 2020; Sanh et al., 2021) due\nto the often prohibitive costs associated with train-\ning, storing and deploying large models (Strubell\net al., 2019). In particular, Brown et al. (2020) pro-\npose priming where training examples are simply\nprovided as additional context together with test\nexamples; this in-context learning does not require\nupdating the parameters of the model.\nIn prior work on in-context learning, only la-\nbeled examples are used for priming (Brown et al.,\nx = Not worth watching.\nUD\nE(x)\nNot worth the time! The movie is[MASK].\np(good) =0.3\np(bad) =0.7\nNot worth the time! The movie isbad.\nNot worth watching. The movie is[MASK].\np(good) =0.1\np(bad) =0.9\nDo not watch this movie. The movie isbad.\nNot worth watching. The movie is[MASK].\np(good) =0.3\np(bad) =0.7\n(1)\n(2)\n(3)\nFigure 1: Schematic representation of the steps involved\nin SOUP for binary sentiment classification of movie\nreviews. (1) Semantic Search: For a given input x, we\nretrieve semantically similar, unlabeled examples from a\nset UD using a sentence encoder E. (2) Self-Prediction:\nWe obtain zero-shot predictions for all similar examples\nusing natural language prompts. (3) Bag-of-Contexts\nPriming: We use the retrieved examples along with\ntheir most probable labels one at a time as in-context\nexamples to obtain predictions for x; the resulting dis-\ntributions over possible labels are finally averaged.\n2020; Lu et al., 2021; Kumar and Talukdar, 2021;\nMin et al., 2021; Jiang et al., 2021). But in many\nsettings, these are extremely scarce or even entirely\nunavailable, while unlabeled examples can easily\nbe accessed. Unfortunately, there is currently no\nway to leverage unlabeled examples for priming.\nOther approaches for leveraging unlabeled data\nsuch as domain-adaptive pretraining (Gururangan\net al., 2020) would again require finetuning.\nTherefore, we investigate how we can make use\nof unlabeled examples to improve the performance\nof large-scale language models without requiring\nchanges to their parameters: We propose a self-\nsupervised method called Semantic-Oriented Unla-\nbeled Priming (SOUP ), which uses unlabeled exam-\nples for in-context learning. Following the observa-\ntion that semantically similar examples are better\n32\ncandidates as in-context examples than dissimilar\nones (Gao et al., 2021a; Liu et al., 2021), we first\nretrieve the semantically most similar unlabeled\nexamples as contexts for a given input; then, we\nquery the language model to obtain predictions for\nthese unlabeled examples, and finally provide them\nalong with their most likely labels as additional\ncontext. Intuitively, this approach is particularly\nhelpful whenever the retrieved examples are easier\nto classify then the actual input of interest.\nWhereas in prior work, the in-context examples\nand test example are usually concatenated to form a\nsingle input that is provided to the LM, we propose\nto use one in-context example at a time and com-\npute a weighted average of the so-obtained label\ndistributions to obtain a final prediction. Besides re-\nsulting in much better performance, one benefit of\nthis methods is that we are no longer constrained by\nthe maximum sequence length of the used LM and\nthus, more neighbors can be used for priming than\nwith the usual, concatenation-based approach. We\nalso investigate an iterative variant of our approach\nwhere predictions for unlabeled examples are it-\neratively improved with SOUP . On four English\ntext classification datasets, we show that SOUP im-\nproves performance of pretrained LMs.\n2 Related Work\nFirst proposed by Brown et al. (2020), in-context\nlearning has been studied by many recent works\n(Lu et al., 2021; Kumar and Talukdar, 2021; Min\net al., 2021; Jiang et al., 2021). Concurrent with\nour work, Min et al. (2021) also propose to perform\npriming with individual examples and combine the\nresulting predictions; however, they use a differ-\nent combination technique and, similar to all prior\nwork on in-context learning, only investigate set-\ntings with labeled examples. Our approach is also\nrelated to various approaches that leverage unla-\nbeled data in few- or zero-shot settings (Xie et al.,\n2019; Gururangan et al., 2020; Schick and Schütze,\n2021a), but all of them require finetuning the un-\nderlying language model.\nWe make use of different Transformer-based sen-\ntence encoders (Reimers and Gurevych, 2019; Gao\net al., 2021b) and of textual instructions to im-\nprove model performance, an approach that was\nfirst proposed by Radford et al. (2019) and has\nsince been investigated extensively (Schick and\nSchütze, 2021a,b,c; Gao et al., 2021a, i.a.).\n3 Semantic-Oriented Unlabeled Priming\nWe introduce Semantic-Oriented Unlabeled Prim-\ning (SOUP ), our approach for in-context learning\nwith unlabeled examples. To this end, let M be a\nmasked language model (Devlin et al., 2019) where\nfor some sequence of tokens t1, . . . , tk that con-\ntains exactly one mask token, M(t |t1, . . . , tk)\ndenotes the probability that M assigns to t at the\nmasked position.1 Further, let E be a sentence\nencoder where E(x) denotes the representation as-\nsigned to x by E, and DU be a set of unlabeled\nexamples. We consider a text classification setup\nwhere for a given input x, a label y from a set Y\nhas to be predicted.\nObtaining predictions for x with SOUP consists\nof the following steps:\n1. Semantic Search: We search for unlabeled\nexamples that are semantically most similar\nto x using the sentence encoder E.\n2. Self-Prediction: We use M to obtain predic-\ntions for these neighboring examples.\n3. Bag-of-Contexts Priming: We use the neigh-\nbors and their estimated labels as additional\ncontext for priming M and compute an av-\nerage of the resulting label distributions to\nobtain a final prediction for x.\n3.1 Semantic Search\nSimilar to prior work (Gao et al., 2021a; Liu et al.,\n2021), the unlabeled examples xu ∈DU are en-\ncoded to obtain vector representations E(xu); this\ncan be done in advance for the entire set DU . We\nalso compute the representation e(x) of our test ex-\nample and use semantic search to find the k nearest\nneighbors of x according to a specific similarity\nmeasure (e.g., cosine similarity). We denote the set\nof neighbors as Nx = {x1, ..., xk}⊆ DU .\n3.2 Self-Prediction for Unlabeled Examples\nWe use M to predict the label distribution for each\nxi ∈Nx, which is done similar to prior work by\nproviding a short prompt and assigning meaning-\nful names to all labels (e.g., Radford et al., 2019;\nSchick and Schütze, 2021a,c). We use the same\nnotation as Schick and Schütze (2021a,c) in that\nwe make use of a pattern P that converts inputs x\ninto cloze questions P(x) containing a single mask,\n1We focus on masked language models, but our approach\ncan easily be transferred to autoregressive language models.\n33\nand a verbalizer v that maps each label y ∈Y to\na single token v(y) representing its meaning. We\ndefine the probability of y being the correct label\nfor x based on M (v(y) |P(x)), the probability\nthat M assigns to v(y) at the masked position in\nP(x). We normalize this probability and set\np(y |x) ∝M (v(y) |P(x))\nM (v(y) |P(ε)) (1)\nwith ε denoting an empty sequence following prior\nwork (Brown et al., 2020).\n3.3 Priming\nLet ˆNx = {(xi, ˆyi)}k\ni=1 be the selected in-context\nneighbors with their predicted labels. Based on\nthese semantically similar examples, we want to\nobtain a prediction forx. In the following, let ˆP(xi)\ndenote P(xi) with the mask token replaced by ˆyi.\nConcatenation Priming Previous work usually\nprovides all in-context examples at a time to the\nLM. That is, all examples are concatenated fol-\nlowed by the test example to obtain the input\nc = [ ˆP(x1), ˆP(x2), ...,ˆP(xk), P(x)], which is\nprovided to the LM to get the final prediction. We\nrefer to this variant as CONCAT priming.\nBag-of-Contexts Priming We propose bag-of-\ncontexts (BOC) priming where instead, we only\nuse individual examples for priming and prediction\neach time and then compute the average of the\nresulting label distributions as the final prediction.\nThe key advantage of this method lies in the fact\nthat it allows us to use more examples than fit in\nthe context window of the used model.\nFor each in-context example xi ∈N, we con-\nstruct a corresponding context ci = [ˆP(xi); P(x)],\nsimilar to CONCAT with k = 1. For each ci, we\nthen use the LM to obtain a distribution qi(y) over\npossible labels y ∈Y for x, where we employ nor-\nmalization analogous to Eq. 1. Finally, we make\nuse of a weighting function w(xi) :N →R+ and\ncompute\nqf (y) = 1\nZ ·\nk∑\ni=1\nw(xi) ·qi(y) (2)\nwith Z = ∑k\ni=1 w(xi). We obtain the final predic-\ntion for x as ˆy = arg maxy∈Y qf (y). We experi-\nment with the following two weighting functions.\nuniform: w(xi) = 1. similarity-based: w(xi) is\nthe cosine similarity between xi and x.\n3.4 Iterative S OUP\nWe also experiment with an iterative variant of\nSOUP where the labels for the unlabeled examples\nin DU are iteratively refined. To this end, we treat\neach example xu ∈DU as a test example: We\nuse SOUP to reclassify xu with DU \\{xu}as the\nset of unlabeled examples. This means for each\nexample x, we select in-context neighbors from\nDU \\{xu}as priming contexts to allow us to refine\nthe prediction for x. We can repeat this process for\nmultiple iterations.\n4 Experiments\nDatasets We evaluate SOUP on four English\ndatasets: IMDb (Maas et al., 2011) and Yelp Re-\nviews (Zhang et al., 2015) for sentiment analy-\nsis as well as AG’s News and Yahoo Questions\n(Zhang et al., 2015) for text categorization. For\neach dataset, we use one of the the patterns and ver-\nbalizers introduced by Schick and Schütze (2021a);\nfurther details can be found in Appendix A. For\nIMDb, the unlabeled in-context examples are se-\nlected from the training set of SST-2 (Socher et al.,\n2013) following Liu et al. (2021). For all other\ndatasets, the in-context examples are obtained from\nthe respective training sets.2\nExperimental Setup For our main experiments,\nwe use ALBERT-xlarge-v2 (Lan et al., 2020) as\nunderlying LM and paraphrase-MiniLM-L6-v2\n(Reimers and Gurevych, 2019) as sentence encoder.\nAs the context window of ALBERT is 512 tokens,\nwe truncate each example to 120 tokens for CON-\nCAT. To enable a fair comparison between both\npriming strategies, we also set the maximum to-\nken number for BOC to 120. We compare SOUP\nto zero-shot performance using only the patterns\nand verbalizers (“prompt only”), similar to Radford\net al. (2019) and Schick et al. (2021). We do not\ncompare to other baselines as we are not aware of\nother approaches that enable leveraging unlabeled\ndata in zero-shot settings without finetuning. For\niterative SOUP , we use 3 iterations to improve the\nlabels assigned to unlabeled data.\nResults As shown in Table 1, when using CON-\nCAT with k = 3, our method clearly performs\nworse than the prompt-only baseline. However, us-\ning our proposed BOC approach consistently out-\n2To ensure a resource-friendly evaluation, we restrict both\nthe unlabeled sets and the test sets to a maximum of 10,000\nrandomly selected examples.\n34\nk w (xi) AG’s Yahoo IMDb Yelp\nPrompt only – – 66.01 48.04 72.67 43.37\nSOUP (CONC .) 3 – 43.88 21.96 54.71 29.56\nSOUP (BOC)\n3 unif. 68.18 45.64 68.30 40.43\nsim. 68.18 45.57 68.31 40.43\n10 unif. 69.64 49.93 71.03 44.05\nsim. 69.74 49.98 71.01 43.93\n50 unif. 69.70 52.67 72.97 46.21\nsim. 70.00 52.56 72.95 46.20\niSOUP (BOC) 50 unif. 69.88 45.22 73.78 45.79\nTable 1: Accuracy with zero-shot prompting,SOUP with\nCONCAT and BOC as well as iterative SOUP (iSOUP )\nusing different numbers of neighbors (k) and both uni-\nform (“unif.”) and similarity-based (“sim.”) weighting.\nSize Method AG’s Yahoo IMDb Yelp\nxlarge Prompt only 66.01 48.04 72.67 43.37\nxlarge S OUP 69.70 52.67 72.97 46.21\nxxlarge Prompt only 73.51 57.89 76.67 45.84\nxxlarge S OUP 74.89 61.82 79.54 41.00\nTable 2: Performance of a prompt-only baseline and\nSOUP with k = 50and uniform weighting using differ-\nent model sizes\nperforms not only priming withCONCAT by a large\nmargin, but also leads to consistent improvements\nover our baseline on three out of four datasets for\nk ≥ 10. Moreover, performance grows consis-\ntently with the number of in-context examples, with\nk = 50resulting in improvements for each dataset\nconsidered. On average, similarity-based weight-\ning leads to negligible gains over uniform weight-\ning. For our iterative variant of SOUP , we therefore\nonly experiment with uniform weighting; iterative\nSOUP leads to slight improvements for two tasks,\nbut performs much worse than SOUP for Yahoo.\n5 Analysis\nWe examine the influence of both increasing the\nlanguage model’s size and replacing the Sentence\nTransformer with different encoders on the per-\nformance of SOUP . We also briefly discuss the\nefficiency of our method.\nModel Size We first focus on the impact of model\nsize on the performance of SOUP ; to this end, we\nalso evaluate our method (with k = 50 and uni-\nform weighting) and the prompt-only baseline us-\ning ALBERT-xxlarge-v2 (Lan et al., 2020), a model\nthat is about four times as large as ALBERT-xlarge-\nv2. As shown in Table 2, for our prompt-only base-\nline performance consistently improves with model\nSentence Encoder AG’s Yahoo IMDb Yelp\nparaphrase-MiniLM-L6-v2 69.70 52.67 72.97 46.21\nmsmarco-bert-base-dot-v5 69.93 53.04 74.47 45.82\nunsup-simcse-roberta-large 69.76 52.40 73.90 45.19\nTable 3: SOUP (ALBERT-xlarge-v2,k = 50, uniform\nweighting) is robust to choice of sentence encoder.\nsize for both methods. With exception of ALBERT-\nxxlarge-v2 on Yelp, for which our method surpris-\ningly leads to worse performance, SOUP consis-\ntently outperforms the baseline method.\nSentence Encoder We also investigate the im-\npact of the sentence encoder on downstream task\nperformance. As paraphrase-MiniLM-L6-v2 was\ntrained on a mixture of tasks that has some over-\nlap with the tasks we evaluate on, we additionally\nconsider msmarco-bert-base-dot-v5 (Reimers and\nGurevych, 2019), a model that was trained exclu-\nsively on MS MARCO passages (Bajaj et al., 2018),\nand unsup-simcse-roberta-large(Gao et al., 2021b),\nan encoder that was trained in a fully unsupervised\nfashion. As can be seen in Table 3, the choice\nof sentence encoder has little influence on perfor-\nmance, illustrating that performance improvements\ndo not come from the encoder being pretrained on\ndownstream task data.\nEfficiency One disadvantage of our approach is\nthat the number of required forward passes grows\nlinearly with k. After precomputing encodings and\nlabels for UD, classifying a single example with\nk = 3 took about 0.6s using a single NVIDIA\nGeForce GTX 1080Ti; for k = 10 and k = 50,\nthe required times were 1.5s and 6.8s. However,\nperformance can be improved a lot with decoder-\nonly LMs (e.g., Radford et al., 2018, 2019; Brown\net al., 2020), as this enables the precomputation of\ncontextualized representations for each xu ∈UD.\n6 Conclusion\nWe have presented SOUP , a method for unlabeled\npriming that classifies inputs by retrieving semanti-\ncally similar unlabeled examples, classifying these\nexamples in a zero-shot fashion and providing them\nas additional contexts for in-context learning. Be-\nyond that, we have proposed a new priming strategy\nthat leads to much better performance and scales to\nmore than just a few examples. We have shown that\nwith sufficiently many retrieved examples, SOUP\nconsistently leads to improved performance.\n35\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir\nRosenberg, Xia Song, Alina Stoica, Saurabh Tiwary,\nand Tong Wang. 2018. Ms marco: A human gener-\nated machine reading comprehension dataset.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. Computing\nResearch Repository, arXiv:2101.03961.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021a.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021b.\nSimcse: Simple contrastive learning of sentence em-\nbeddings.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\nNeubig. 2021. How Can We Know When Language\nModels Know? On the Calibration of Language Mod-\nels for Question Answering. Transactions of the As-\nsociation for Computational Linguistics, 9:962–977.\nSawan Kumar and Partha Talukdar. 2021. Reorder-\ning examples helps during priming-based few-shot\nlearning. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n4507–4518, Online. Association for Computational\nLinguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In Interna-\ntional Conference on Learning Representations.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021. What\nmakes good in-context examples for gpt-3? CoRR,\nabs/2101.06804.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretrain-\ning approach. Computing Research Repository ,\narXiv:1907.11692.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2021. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. Computing Research\nRepository, arXiv:2104.08786.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2021. Noisy channel language\nmodel prompting for few-shot text classification.\nComputing Research Repository, arXiv:2108.04106.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. Technical report,\nOpen AI.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, Open AI.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\n36\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak,\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian\nJiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\nThomas Wang, Trishala Neeraj, Jos Rozen, Abheesht\nSharma, Andrea Santilli, Thibault Fevry, Jason Alan\nFries, Ryan Teehan, Stella Biderman, Leo Gao, Tali\nBers, Thomas Wolf, and Alexander M. Rush. 2021.\nMultitask prompted training enables zero-shot task\ngeneralization. Computing Research Repository ,\narXiv:2110.08207.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze questions for few shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics, Kyiv, Ukraine\n(Online). International Committee on Computational\nLinguistics.\nTimo Schick and Hinrich Schütze. 2021b. Few-shot\ntext generation with pattern-exploiting training. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nAssociation for Computational Linguistics.\nTimo Schick and Hinrich Schütze. 2021c. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-diagnosis and self-debiasing: A proposal for\nreducing corpus-based bias in NLP. Transactions of\nthe Association for Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nQizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Lu-\nong, and Quoc V . Le. 2019. Unsupervised data aug-\nmentation for consistency training. Computing Re-\nsearch Repository, arXiv:1904.12848.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In C. Cortes, N. D. Lawrence, D. D. Lee,\nM. Sugiyama, and R. Garnett, editors, Advances in\nNeural Information Processing Systems 28 , pages\n649–657. Curran Associates, Inc.\n37\nA Dataset Details\nFor each task except IMDb, we use one of the\npatterns and verbalizers introduced by Schick and\nSchütze (2021a). In the following, we describe in\ndetail the patterns and verbalizers used.\nIMDb For the IMDb Large Movie Review\nDataset (Maas et al., 2011), the task is to estimate\nthe binary sentiment of a movie review based on\nthe review’s text. We use the following pattern and\nverbalizer for an input review a:\nP(a) =a. The movie is [MASK].\nv(0) =bad v(1) =good\nYelp For the Yelp Reviews Full Star dataset\n(Zhang et al., 2015), the task is to estimate the\nrating that a customer gave to a restaurant on a 1-to\n5-star scale based on their review’s text. We use\nthe following pattern for an input text a:\nP(a) =a. In summary, the restaurant is [MASK].\nAs a verbalizer v, we define:\nv(1) =terrible v(2) =bad v(3) =okay\nv(4) =good v(5) =great\nAG’s News AG’s News (Zhang et al., 2015) is a\ntask to classify a news article as belonging to one\nof the categories World (1), Sports (2), Business\n(3) or Science/Tech (4). We define the following\npattern for an input news text a:\nP(a) =a. News Category: [MASK].\nIntuitively, we use a verbalizer that maps 1–4 to\n“World”, “Sports”, “Business” and “Science”, re-\nspectively.\nYahoo Yahoo Questions (Zhang et al., 2015) is a\ntext classification dataset. Given a question and an\nanswer, the text has to be classified to one of ten\npossible categories. We make use of the following\npattern for a input question a and an answer b:\nP(a, b) =a b. Question Category: [MASK].\nOur verbalizer maps labels 1–10 to the tokens “So-\nciety”, “Science”, “Health”, “Education”, “Com-\nputer”, “Sports”, “Business”, “Entertainment”,\n“Relationship” and “Politics”.\n38",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.764609694480896
    },
    {
      "name": "Natural language processing",
      "score": 0.6837171316146851
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5166940689086914
    },
    {
      "name": "Priming (agriculture)",
      "score": 0.4942758083343506
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4657230079174042
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Germination",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2801851002",
      "name": "Harvard University Press",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I3018771216",
      "name": "LMU Klinikum",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    }
  ]
}