{
    "title": "Improving position encoding of transformers for multivariate time series classification",
    "url": "https://openalex.org/W4386465411",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5067175463",
            "name": "Navid Mohammadi Foumani",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A2750673462",
            "name": "Chang Wei Tan",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A2126304162",
            "name": "Geoffrey I. Webb",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A2181624618",
            "name": "Mahsa Salehi",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A5067175463",
            "name": "Navid Mohammadi Foumani",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A2750673462",
            "name": "Chang Wei Tan",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A2126304162",
            "name": "Geoffrey I. Webb",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A2181624618",
            "name": "Mahsa Salehi",
            "affiliations": [
                "Monash University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2555077524",
        "https://openalex.org/W3110709724",
        "https://openalex.org/W2982438846",
        "https://openalex.org/W3112330479",
        "https://openalex.org/W6702248584",
        "https://openalex.org/W3132607382",
        "https://openalex.org/W2892035503",
        "https://openalex.org/W2972810968",
        "https://openalex.org/W4205114161",
        "https://openalex.org/W3034999176",
        "https://openalex.org/W3106210592",
        "https://openalex.org/W2783323081",
        "https://openalex.org/W3177342940",
        "https://openalex.org/W3204643324",
        "https://openalex.org/W6603139312",
        "https://openalex.org/W2090805767",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W3186145246",
        "https://openalex.org/W3080921724",
        "https://openalex.org/W3115948762",
        "https://openalex.org/W2963163009",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2551393996",
        "https://openalex.org/W3203701986",
        "https://openalex.org/W3188872815",
        "https://openalex.org/W3083891030",
        "https://openalex.org/W3190461479",
        "https://openalex.org/W3042807565"
    ],
    "abstract": "Abstract Transformers have demonstrated outstanding performance in many applications of deep learning. When applied to time series data, transformers require effective position encoding to capture the ordering of the time series data. The efficacy of position encoding in time series analysis is not well-studied and remains controversial, e.g., whether it is better to inject absolute position encoding or relative position encoding, or a combination of them. In order to clarify this, we first review existing absolute and relative position encoding methods when applied in time series classification. We then proposed a new absolute position encoding method dedicated to time series data called time Absolute Position Encoding (tAPE). Our new method incorporates the series length and input embedding dimension in absolute position encoding. Additionally, we propose computationally Efficient implementation of Relative Position Encoding (eRPE) to improve generalisability for time series. We then propose a novel multivariate time series classification model combining tAPE/eRPE and convolution-based input encoding named ConvTran to improve the position and data embedding of time series data. The proposed absolute and relative position encoding methods are simple and efficient. They can be easily integrated into transformer blocks and used for downstream tasks such as forecasting, extrinsic regression, and anomaly detection. Extensive experiments on 32 multivariate time-series datasets show that our model is significantly more accurate than state-of-the-art convolution and transformer-based models. Code and models are open-sourced at https://github.com/Navidfoumani/ConvTran .",
    "full_text": "Vol:.(1234567890)\nData Mining and Knowledge Discovery (2024) 38:22–48\nhttps://doi.org/10.1007/s10618-023-00948-2\n1 3\nImproving position encoding of transformers \nfor multivariate time series classification\nNavid Mohammadi Foumani1  · Chang Wei Tan1 · Geoffrey I. Webb1 · \nMahsa Salehi1\nReceived: 28 November 2022 / Accepted: 5 June 2023 / Published online: 5 September 2023 \n© Crown 2023\nAbstract\nTransformers have demonstrated outstanding performance in many applications of \ndeep learning. When applied to time series data, transformers require effective posi-\ntion encoding to capture the ordering of the time series data. The efficacy of position \nencoding in time series analysis is not well-studied and remains controversial, e.g., \nwhether it is better to inject absolute position encoding or relative position encoding, \nor a combination of them. In order to clarify this, we first review existing absolute \nand relative position encoding methods when applied in time series classification. \nWe then proposed a new absolute position encoding method dedicated to time series \ndata called time Absolute Position Encoding (tAPE). Our new method incorporates \nthe series length and input embedding dimension in absolute position encoding. \nAdditionally, we propose computationally Efficient implementation of Relative Posi-\ntion Encoding (eRPE) to improve generalisability for time series. We then propose a \nnovel multivariate time series classification model combining tAPE/eRPE and con-\nvolution-based input encoding named ConvTran to improve the position and data \nembedding of time series data. The proposed absolute and relative position encod-\ning methods are simple and efficient. They can be easily integrated into transformer \nblocks and used for downstream tasks such as forecasting, extrinsic regression, and \nanomaly detection. Extensive experiments on 32 multivariate time-series datasets \nshow that our model is significantly more accurate than state-of-the-art convolution \nand transformer-based models. Code and models are open-sourced at https:// github. \ncom/ Navid fouma ni/ ConvT ran.\nKeywords Multivariate time series classification · Transformers · Position encoding\nResponsible editor: Charalampos Tsourakakis.\nExtended author information available on the last page of the article\n23\n1 3\nImproving position encoding of transformers for multivariate…\n1 Introduction\nA time series is a time-dependent quantity recorded over time. Time series data \ncan be univariate, where only a sequence of values for one variable is collected; \nor multivariate, where data are collected on multiple variables. There are many \napplications that require time series analysis, such as human activity recognition \n(Lockhart et al. 2011), diagnosis based on electrocardiogram (ECG), electroen-\ncephalogram (EEG), and systems monitoring problems (Bagnall eta al. 2018). \nMany of these applications are inherently multivariate in nature—various sensors \nare used to measure human’s activities; EEGs use a set of electrodes (channels) \nto measure brain signals at different locations of the brain. Hence, multivariate \ntime-series analysis methods such as classification and segmentation are of great \ncurrent interest (Bagnall et al. 2017; Fawaz et al. 2019; Ruiz et al. 2020).\nConvolutional neural networks (CNNs) have been widely employed in time \nseries classification (Fawaz et  al. 2019; Ruiz et  al. 2020). Many studies have \nshown that convolution layers tend to have strong generalization with fast conver -\ngence due to their strong inductive bias (Dai et al. 2021). While CNN-based mod-\nels are excellent for capturing local temporal/spatial correlations, these models \ncannot effectively capture and utilize long-range dependencies. Also, they only \nconsider the local order of data points in a time series rather than the order of all \ndata points globally. Due to this, many recent studies have used recurrent neural \nnetworks (RNN) such as LSTMs to capture this information (Karim et al. 2019). \nHowever, RNN-based models are computationally expensive, and their capability \nin capturing long-range dependencies are limited (Vaswani et al. 2017; Hao and \nCao 2020).\nOn the other hand, attention models can capture long-range dependencies, \nand their broader receptive fields provide more contextual information, which \ncan improve the models’ learning capacity. Not surprisingly, with the success \nof attention models in natural language processing (Vaswani et al. 2017; Devlin \net al. 2018), many previous studies have attempted to bring the power of attention \nmodels into other domains such as computer vision (Dosovitskiy et al. 2020) and \ntime series analysis (Hao and Cao 2020; Zerveas et al. 2021; Kostas et al. 2021).\nThe transformer’s core is self-attention (Vaswani et al. 2017), which is capable \nof modeling the relationship of input time series. Self-attention, however, has a \nlimitation - it cannot capture the ordering of input series. Hence, adding explicit \nrepresentations of position information is especially important for the attention \nsince the model is otherwise entirely invariant to input order, which is undesira-\nble for modeling sequential data. This limitation is even worse in time series data \nsince, unlike image and text, which use Word2Vec-like embedding, time series \ndata has less informative data context.\nThere are two main methods for encoding positional information in transform-\ners: absolute and relative. Absolute methods, such as those used in Vaswani et al. \n(2017); Devlin et al. (2018), assign a unique encoding vector to each position in \nthe input sequence based on its absolute position in the sequence. These encoding \nvectors are combined with the input encoding to provide positional information \n24\n N. M. Foumani et al.\n1 3\nto the model. On the other hand, relative methods (Shaw et  al. 2018; Huang \net al. 2018) encode the relative distance between two elements in the sequence, \nrather than their absolute positions. The model learns to compute the relative dis-\ntances between any two positions during training and looks up the corresponding \nembedding vectors in a pre-defined table to obtain the relative position embed-\ndings. These embeddings are used to directly modify the attention matrix. Posi-\ntion encoding has been verified to be effective in natural language processing and \ncomputer vision (Dufter et al. 2022). However, in time series classification, the \nefficacy is still unclear.\nThe original absolute position encoding is proposed for language modeling, \nwhere high embedding dimensions like 512 or 1024 are usually used for position \nembedding of input with a length of 512 (Vaswani et al. 2017). But, for time series \ntasks, embedding dimensions are relatively low, and the series might have a variety \nof lengths (ranging from very low to very high). In this paper, for the first time, \nwe study the efficiency (i.e. how well resources are utilized) and the effectiveness \n(i.e. how well the encodings achieve their intended purpose) of existing absolute \nand relative position encodings for time series data. We then show that the existing \nabsolute position encodings are ineffective with time series data. We introduce a \nnovel time series-specific absolute position encoding method that takes into account \nthe series embedding dimension and length. We show that our new absolute position \nencoding outperforms the existing absolute position encodings in time series clas-\nsification tasks.\nAdditionally, since the existing relative position encodings have large mem-\nory overhead and they require a large number of parameters to be trained, in time \nseries data it is very likely they overfit. We propose a novel computationally effi-\ncient implementation of relative position encoding to improve their generalisabil-\nity for time series. We show that our new relative position encoding outperforms \nthe existing relative position encodings in time series classification tasks. We then \npropose a novel time series classification model based on the combination of our \nproposed absolute/relative position encodings named ConvTran to improve the \nposition embedding of time series data. We further enriched the data embedding of \ntime series using CNN rather than linear encoding. Our extensive experiments on \n32 benchmark datasets show ConvTran is significantly more accurate than the pre-\nvious state-of-the-art in deep learning models for time series classification (TSC). \nWe believe our novel position encodings can boost the performance of other trans-\nformer-based TSC models.\n2  Related work\nIn this section, we briefly discuss the state-of-the-art multivariate time series clas-\nsification (MTSC) algorithms, as well as CNN and attention-based models that have \nbeen applied to MTSC tasks. We refer interested readers to the corresponding papers \nor the recent survey on deep learning for time series classification (Foumani et al. \n2023) for a more detailed description of these algorithms and models.\n25\n1 3\nImproving position encoding of transformers for multivariate…\n2.1  State‑of‑the‑art MTSC algorithms\nMany MTSC algorithms have been proposed in recent years (Bagnall eta al. 2018; \nRuiz et al. 2020; Fawaz et al. 2019), where many of them are adapted from their \nunivariate version. A recent survey (Ruiz et al. 2020) evaluated most of the existing \nMTSC algorithms on the UEA MTS archive, that consists of 26 equal-length time \nseries datasets. This benchmark includes a few deep learning as well as non-deep \nlearning approaches. This survey concluded that there are four main state of the art \nmethods. These are ROCKET (Dempster et al. 2020), HIVE-COTE (Bagnall et al. \n2020), CIF (Middlehurst et al. 2020) and Inception-Time (Fawaz et al. 2020).\nROCKET (Dempster et al. 2020) is a scalable TSC algorithm that uses 10,000 \nrandom convolution kernels to extract 2 features from each input time series, creat-\ning 20,000 features for each time series. Then a linear model is used for classifi-\ncation, such as ridge or logistic regression. Mini-ROCKET (Dempster et al. 2021) \nis an extension of ROCKET with some slight modifications to the feature extrac-\ntion process. It is significantly more scalable than ROCKET and uses only 10,000 \nfeatures without compromising accuracy. Multi-ROCKET (Tan et al. 2021) extends \nMini-ROCKET by leveraging the first derivative of the series as well as extracting 4 \nfeatures per kernel. It is significantly more accurate than both ROCKET and Mini-\nROCKET on 128 univariate TSC tasks. Note that neither Mini-ROCKET nor Multi-\nROCKET has previously been benchmarked on the UEA MTS archive. The adapta-\ntion for multivariate time series for ROCKET, Mini-ROCKET and Multi-ROCKET \nis done by randomly selecting different channels of the time series for each convolu-\ntional kernel.\nThe Canonical Interval Forest (CIF) (Middlehurst et al. 2020) is an interval based \nclassifier. It first extracts 25 features from random intervals of the time series and \nbuilds a time series forest with 500 trees. It is an algorithm initially designed for uni-\nvariate TSC and was adapted to multivariate TSC by expanding the random interval \nsearch space, where an interval is defined as a random dimension of the time series.\nThe Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-\nCOTE) is a meta ensemble for TSC. It forms its ensemble from classifiers of mul-\ntiple domains. Since its introduction in 2016, HIVE-COTE has gone through a few \niterations. The version used in the MTSC benchmark (Ruiz et al. 2020) comprised \nof 4 ensemble members—Shapelet Transform Classifier (STC), Time Series For -\nest (TSF), Contractable Bag of Symbolic Fourier Approximation Symbols (CBOSS) \nand Random Interval Spectral Ensemble (RISE), each of them being the state of \nthe art in their respective domains. Since these algorithms were designed for uni-\nvariate time series, the adaption for multivariate time series is not easy. Hence, they \nwere adapted for multivariate time series through ensembling over all the models \nbuilt on each dimension independently. This means that they are computationally \nvery expensive especially when the number of channels is large. Recently, the lat-\nest HIVE-COTE version, HIVE-COTEv2.0 (HC2) was proposed (Middlehurst et al. \n2011). It is currently the most accurate classifier for both univariate and multivariate \nTSC tasks (Middlehurst et al. 2011). Despite being the most accurate on 26 bench-\nmark MTSC datasets, that are relatively small, HC2 is not scalable to either large \ndatasets with long time series or datasets with many channels.\n26\n N. M. Foumani et al.\n1 3\n2.2  CNN based models\nCNNs are popular deep learning architectures for MTSC due to their ability to \nextract latent features from the time series data efficiently. Fully Convolutional Neu-\nral Network (FCN) and Residual Network (ResNet) were proposed in Wang et al. \n(2017) and evaluated in Fawaz et al. (2019). FCN is a simple convolutional network \nthat does not contain any pooling layers in convolution blocks. The output from the \nlast convolution block is averaged with a Global Average Pooling (GAP) layer and \npassed to a final softmax classifier. ResNet is one of the deepest architectures for \nMTSC (and TSC in general), containing three residual blocks followed by a GAP \nlayer and a softmax classifier. It uses residual connections between blocks to reduce \nthe vanishing gradient effect in deep learning models. ResNet was one of the most \naccurate deep learning TSC architectures on 85 univariate TSC datasets (Fawaz \net al. 2019; Bagnall et al. 2017). It was also proven to be an accurate deep learning \nmodel for MTSC (Fawaz et al. 2019; Ruiz et al. 2020).\nInception-Time is the current state-of-the-art deep learning model for both uni-\nvariate TSC and MTSC (Fawaz et al. 2020; Ruiz et al. 2020). Inception-Time is an \nensemble of five randomly initialised inception network models that each consists \nof two blocks of inception modules. Each inception module first reduces the dimen-\nsionality of a multivariate time series using a bottleneck layer with length and stride \nof 1 while maintaining the same length. Then, 1D convolutions of different lengths \nare applied to the output of the bottleneck layer to extract patterns at different sizes. \nA max pooling layer followed by a bottleneck layer are also applied to the original \ntime series to increase the robustness of the model to small perturbations. Resid-\nual connections are also used between each inception block to reduce the vanishing \ngradient effect. The output of the second inception block is passed to a GAP layer \nbefore feeding into a softmax classifier.\nRecently, Disjoint-CNN (Foumani et  al. 2021) shows that factorization of 1D \nconvolution kernels into disjoint temporal and spatial components yields accuracy \nimprovements with almost no additional computational cost. Applying disjoint tem-\nporal convolution and then spatial convolution behaves similarly to the “Inverted \nBottleneck” (Sandler et al. 2018). Like the Inverted Bottleneck, the temporal convo-\nlutions expand the number of input channels, and spatial convolutions later project \nthe expanded hidden state back to the original size to capture the temporal and spa-\ntial interaction.\n2.3  Attention based models\nSelf-attention has been demonstrated to be effective in various natural language pro-\ncessing tasks due to its higher capacity and superior ability to capture long-term \ndependencies in text (Vaswani et al. 2017). Recently, it has also been shown to be \neffective for time series classification tasks. Cross Attention Stabilized Fully Con-\nvolutional Neural Network (CA-SFCN) (Hao and Cao 2020) has applied the self-\nattention mechanism to leverage the long-term dependencies for the MTSC task. \n27\n1 3\nImproving position encoding of transformers for multivariate…\nCA-SFCN combines FCN and two types of self-attention—temporal attention \n(TA) and variable attention (VA), which interact to capture both long-range tem-\nporal dependencies and interactions between variables. With evidence that multi-\nheaded attention dominates self-attention, many models try to adapt it to the MTSC \ndomain. Gated Transformer Networks (GTN) (Liu et al. 2021), similar to CA-SFCN, \nuse two-tower multi-headed attention to capture discriminative information from the \ninput series. They merge the output of two towers using a learnable matrix named \ngating.\nInspired by the development of transformer-based self-supervised learning like \nBERT (Kostas et al. 2021), many models try to adopt the same structure for time \nseries classification (Kostas et al. 2021; Zerveas et al. 2021). BErt-inspired Neural \nData Representations (BENDER) replace the word2vec encoder in BERT with the \nwav2vec to leverage the same structure for time series data. BENDER shows that if \nwe have a massive amount of EEG data, the pre-trained model can be used effec-\ntively to model EEG sequences recorded with differing hardware. Similarly, Voice-\nto-Series with Transformer-based Attention (V2Sa) uses a large-scale pre-trained \nspeech processing model for downstream problems like time series classification \nproblems (Yang et al. 2011). Recently, a Transformer-based Framework (TST) was \nalso introduced to adopt vanilla transformers to the multivariate time series domain \n(Zerveas et al. 2021). TST uses only the encoder part of transformers and pre-train it \nwith proportionally masked data in an unsupervised manner.\n3  Background\nThis section provides a basic definition of self-attention and an overview of cur -\nrent position encoding models. Note that position encoding refers to the method that \nintegrates position information, e.g., absolute or relative. Position embedding refers \nto a numerical vector associated with position encoding.\n3.1  Problem description and notation\nGiven a time series dataset X with n samples, X = /braceleft.s1/u1D431/u1D7CF, /u1D431/u1D7D0, ..., /u1D431/u1D427\n/braceright.s1 , where \n/u1D431/u1D42D= /braceleft.s1x1 ,x2 , ...,xL\n/braceright.s1 is a dx-dimensional time series and L is the length of time series, \n/u1D431/u1D42D∈ ℝL×dx , and the set of relevant response labels Y = /braceleft.s1y1 ,y2 , ...,yn\n/braceright.s1 , yt ∈ {1, ...,c} \nand c is the number of classes. The aim is to train a neural network classifier to map \nset X to Y.\n3.2  Self‑attention\nThe first attention mechanisms were proposed in the context of natural language pro-\ncessing (Luong et al. 2015). While they still relied on a recurrent neural network at \nits core, Vaswani et al. (2017) proposed a transformer model that relies on attention \nonly. Transformers map a query and a set of key-value pairs to an output. More spe-\ncifically, for an input series, /u1D431/u1D42D= /braceleft.s1x1 ,x2 , ...,xL\n/braceright.s1 , self-attention computes an output \n28\n N. M. Foumani et al.\n1 3\nseries /u1D433/u1D42D= /braceleft.s1z1 ,z2 , ...,zL\n/braceright.s1 where zi ∈ ℝd z and is computed as a weighted sum of input \nelements:\nEach coefficient weight /u1D6FCi,j is calculated using softmax function:\nwhere e ij is an attention weight from positions j to i and is computed using a scaled \ndot-product:\nThe projections W Q , W K , W V ∈ ℝdx×dz are parameter matrices and are unique per \nlayer. Instead of computing self-attention once, Multi-Head Attention (MHA) (Vas-\nwani et al. 2017) does so multiple times in parallel, i.e., employing h attention heads. \nA linear transformation is applied to the attention head outputs and concatenated \ninto the standard dimensions.\n3.3  Position encoding\nThe self-attention layer cannot preserve time series positional information in the \ntransformer architecture since the transformer contains no recurrence and convolu-\ntion. However, the local positional information, i.e., the ordering of time series, is \nessential. The practical approach in transformer-based methods involves using mul-\ntiple encoding (Huang et al. 2020; Wu et al. 2021; Dufter et al. 2022), such as abso-\nlute or relative positional encoding, to enhance the temporal context of time-series \ninputs.\n3.3.1  Absolute position encoding\nThe original self-attention considers the absolute position (Vaswani et al. 2017), and \nadds the absolute positional embedding P =( p1 , ...,pL) to the input embedding x as:\nwhere the position embedding pi ∈ ℝdmodel  . There are several options for absolute \npositional encodings, including the fixed encodings by sine and cosine functions \nwith different frequencies called VanillaAPE and the learnable encodings through \ntrainable parameters (we refer it as Learn method) (Vaswani et al. 2017; Devlin et al. \n2018).\n(1)zi =\nL/uni2211.s1\nj=1\n/u1D6FCi,j(xjW V )\n(2)/u1D6FCi,j =\nexp (eij)\n∑L\nk=1 exp (eik)\n(3)eij=\n(xiW Q )(xjW K )T\n√\ndz\n(4)xi = xi + pi\n29\n1 3\nImproving position encoding of transformers for multivariate…\nBy using sine and cosine for fixed position encoding, the dmodel-dimensional embed-\ndings of ith time step position can be represented by the following equation:\nwhere k is in the range of [0, dmodel\n2 ] , dmodel is the embedding dimension and /u1D714k is the \nfrequency term. Variations in /u1D714k ensure that no positions < 104 are assigned similar \nembeddings.\n3.3.2  Relative position encoding\nIn addition to the absolute position embedding, recent studies in natural language pro-\ncessing and computer vision also consider the pairwise relationships between input ele-\nments, i.e., relative position Shaw et  al. (2018); Huang et  al. (2018). This type of \nmethod encodes the relative distance between the input elements xi and xj into vectors \npQ\ni,j, pK\ni,j, pV\ni,j ∈ ℝdz . The encoding vectors are embedded into the self-attention module, \nwhich modifies Eqs. (1) and (3) as\nBy doing so, the pairwise positional relation is trained during transformer training.\nShaw et  al. (2018) proposed the first relative position encoding for self-attention. \nRelative positional information is supplied to the model on two levels: values and keys. \nFirst, relative positional information is included in the model as an additional compo-\nnent to the keys. The softmax operation Eq. (3) remains unchanged from vanilla self-\nattention. Lastly, relative positional information is resupplied as a sub-component of \nthe values matrix. Besides, the authors believe that relative position information is not \nuseful beyond a certain distance, so they introduced a clip function to reduce the num-\nber of parameters. Encoding is formulated as follows to consider the distance between \ninputs i and j in computing their attention:\n(5)pi(2k)= sini/u1D714k pi(2k + 1)= cos i/u1D714k /u1D714k = 10000 −2k∕dmodel\n(6)zi =\nL/uni2211.s1\nj=1\n/u1D6FCi,j(xjW V + pV\ni,j)\n(7)eij=\n(xiW Q + pQ\ni,j)(xjW K + pK\ni,j)T\n√\ndz\n(8)eij=\n(xiW Q )(xjW K + pK\nclip(i−j,k))T\n√\ndz\n(9)zi =\nL/uni2211.s1\nj=1\n/u1D6FCi,j(xjW V + pV\nclip(i−j,k))\n(10)clip(x, k)= max (−k, min(k , x))\n30\n N. M. Foumani et al.\n1 3\nWhere pV and pK are the trainable weights of relative position encoding on values \nand keys, respectively. PV =( pV\n−k, ..., pV\nk ) and PK =( pK\n−k, ..., pK\nk ) where pV\ni , pK\ni ∈ ℝdz . \nThe scalar k is the maximum relative distance.\nHowever, this technique (Shaw) is not memory efficient. As can be seen in Eq. 8, \nit requires O(L2d) memory due to the additional relative position encoding. Huang \net  al. (2018) introduced a new method (in this paper it is called Vector method) \nof computing relative positional encoding that reduces its intermediate memory \nrequirement from O(L2d) to O(Ld) using skewing operation (Huang et  al. 2018). \nAccording to this paper, the authors also dropped the additional relative positional \nembedding corresponding to the value term and focused only on the key component. \nEncoding is formulated as follows:\nWhere Skew procedure use padding, reshaping and slicing to reduce the memory \nrequirement (Huang et al. 2018). In Table  1 we provided a summary of the parame-\nter sizes, memory, and computation complexities of various position encoding meth-\nods (including our proposed ones in this paper) for comparison purposes.\n4  Position encoding of transformers for MTSC\nWe design our position encoding methods to examine several aspects which are not \nwell studied in prior transformers-based time series classification work (see the anal-\nysis in Sec 5.4).\nAs a first step, we propose a new absolute position encoding method dedicated \nto time series data called time Absolute Position Encoding (tAPE). tAPE incorpo-\nrates the series length and input embedding dimension in absolute position encod-\ning. We then introduce efficient Relative Position Embedding (eRPE) to explore the \nindependent encoding of positions from the input encodings. After that, to study the \nintegration of eRPE into a transformer model, we compare different integration of \nposition information to the attention matrix; finally, we provide an efficient imple-\nmentation for our methods.\n4.1  Time absolute position encoding (tAPE)\nAbsolute position encoding was originally proposed for language modeling, where \nhigh embedding dimensions like 512 or 1024 are usually used for position embed-\nding of input with a length of 512 (Vaswani et al. 2017). Figure  1a shows the dot \nproduct between two sinusoidal positional embedding whose distance is K using \nEq.  (5) with various embedding dimensions. Clearly, higher embedding dimen-\nsions, such as 512 (red thick line), can better reflect the similarity between various \n(11)eij=\n(xiW Q )(xjW K )T + Srel\n√\ndz\n(12)Srel= Skew(W Q P)\n31\n1 3\nImproving position encoding of transformers for multivariate…\npositions. As shown in Fig. 1a using 64 or 128 as embedding dimensions (thin blue \nand orange lines, respectively), the dot product does not always decrease as the dis-\ntance between two positions increases. We call this the distance awareness property, \nwhich disappears when lower embedding dimensions, such as 64, are used for posi-\ntion encoding.\nWhile high embedding dimensions show a desirable monotonous decrease trend \nwhen the distance between two positions increases (see red line in Fig.  1a), they are \nnot suitable for encoding time series datasets. The reason is that most time series \ndatasets have relatively low input dimensionality (e.g., 28 out of 32 datasets have \nless than 64 input dimension), and higher embedding dimensions may yield inferior \nmodel throughput due to extra parameters (increasing the chances of overfitting the \nmodel).\nOn the other hand, in low embedding dimensions, the similarity value between \ntwo random embedding vectors is high, making the embedding vectors very similar \nto each other. In other words, we cannot fully utilise the embedding vector space \nto differentiate between two positions. Figure  1b depicts the embedding vectors of \nthe first and last position embedding for the embedding dimension equals 128 and \nlength equals 30. In this figure, almost half of the embedding vectors are the same. \nThis is called the anisotropic phenomenon (Liang et al. 2021). The anisotropic phe-\nnomenon makes the position encoding to be ineffective in low embedding dimen-\nsions as embedding vectors become similar to each other as it is shown in Fig.  1a \n(the blue line).\nHence, we require a position embedding for time series that has distance aware-\nness while simultaneously being isotropic. In order to incorporate distance aware-\nness, we propose to use the time series length in Eq. (5). In this equation, /u1D714k refers \nto the frequency of the sine and cosine functions from which the embedding vectors \nare generated. Without our modification, as series length L increases the dot product \nof positions becomes ever less regular, resulting in a loss of distance awareness. By \nincorporating the length parameter in the frequency terms in both sine and cosine \nfunctions in Eq. (5), the dot product remains smoother with a monotonous trend.\nFig. 1  Sinusoidal absolute position encoding. a The dot product of two sinusoidal position embeddings \nwhose distance is K with various embedding dimensions. b 128 dimension sinusoidal positional encod-\ning curves for positions 1 and 30 in a series of length 30\n32\n N. M. Foumani et al.\n1 3\nAs the embedding dimension dmodel value increases, it is more likely the vector \nembeddings are sampled from low-frequency sinusoidal functions, which results in \nthe anisotropic phenomenon. To alleviate this, we incorporate the dmodel parameter \ninto the frequency term in both sine and cosine functions in Eq. (5). We propose a \nnovel absolute position encoding for time series called tAPE in which /u1D714new\nk  takes \ninto account the input embedding dimension and length as follows:\nwhere L is the series length and dmodel shows the embedding dimension.\nOur new tAPE position encoding is compared with a vanilla sinusoidal posi-\ntion encoding to provide further illustration. Using dmodel = 128 dimension vector, \nFig 2a–b show the dot product (similarity) of two positions with a distance of K for \nseries with of length L = 1000 and L = 30 respectively. As depicted in Fig  2a, in \nvanilla APE, only the closest positions in the series have a monotonous decreasing \ntrend, and approximately from a distance 50 onwards ( /uni007C.varK/uni007C.var> 50 ) on both sides, the \ndecreasing similarity trend becomes less apparent as the distance between two posi-\ntions in the time series increases. However, tAPE has a more stable decreasing trend \nand more steadily reflects the distance between two positions. Meanwhile, Fig  2b \nshows the embedding vectors of tAPE are less similar to each other compared to \nvanilla APE. This is due to better utilising the embedding vector space to differenti-\nate between two positions as we discussed earlier.\nNote in Eq. (13) our /u1D714new\nk  will obviously be equal to the /u1D714k in vanilla APE when \ndmodel = L and the encodings of tAPE and vanilla APE will be the same. However, \nif dmodel ≠ L , tAPE will encode the positions in series more effectively than vanilla \nAPE due to the two properties we discussed earlier. Figure 2a shows a case in which \ndmodel < L and Fig 2b shows a case in which dmodel > L and in both cases tAPE uti-\nlises embedding space to provide an isotropic encoding, while holding the distance \n(13)\n/u1D714k = 10000−2k∕dmodel\n/u1D714new\nk = /u1D714k × dmodel\nL\nFig. 2  Comparing dot product between two position whose distance is K in a time series using tAPE and \nvanilla APE with dx = 128 dimension vector for series of length a L = 1000 b L = 30\n33\n1 3\nImproving position encoding of transformers for multivariate…\nawareness property. In other words, tAPE provides a balance between these two \nproperties in its encodings. The superiority of tAPE compared to vanilla APE and \nlearned APE on various length time series datasets is shown in the experimental \nresults section.\n4.2  Efficient relative position encoding (eRPE)\nThere are multiple extensions of the abovementioned Sect.  3.3.2 relative position \nembeddings in machine translation and computer vision (Huang et  al. 2020; Wu \net  al. 2021; Dufter et  al. 2022). However, input embeddings are the basis for all \nprevious methods of relative position encoding (adding or multiplying the position \nmatrices to the query, key, and value matrices,  as exemplified in Fig.  3a). In this \nstudy, we introduce an efficient model of relative position encoding independent of \ninput embeddings (Fig. 3b).\nIn particular, we propose the following formulation:\nwhere L is series length, A i,j is attention weight and w i−j is a learnable scalar (i.e., \nw ∈ ℝO(L) ) and represent the relative position weight between positions i and j.\nIt is worth comparing the strengths and weaknesses of relative position encod-\nings and attention to determine what properties are more desirable for relative posi-\ntion encoding of time series data. Firstly, the relative position embedding w i−j is an \n(14)/u1D6FCi =\n�\nj∈L\n⎛\n⎜\n⎜\n⎜\n⎜\n⎜⎝\nexp(ei,j)\n∑\nk∈L exp(ei,k)\n/uni23DF.l/uni23DE.x/uni23DE.x/uni23DE.x/uni23DE.x/uni23DE.x/uni23DE.x/uni23DF.m/uni23DE.x/uni23DE.x/uni23DE.x/uni23DE.x/uni23DE.x/uni23DE.x/uni23DF.r\nA i,j\n+w i−j\n⎞\n⎟\n⎟\n⎟\n⎟\n⎟⎠\nxj\nFig. 3  Self-attention modules with relative position encoding using scalar and vector parameters. Newly \nadded parts are depicted in grey\n34\n N. M. Foumani et al.\n1 3\ninput-independent parameter with static values, whereas an attention weight A i,j is \ndynamically determined by the representation of the input series. In other words, \nattention adapts to input series via a weighting strategy (input-adaptive weighting \n(Vaswani et al. 2017)). Input-adaptive-weighting enables models to capture the com-\nplicated relationships between different time points, a property that we desire most \nwhen we want to extract high-level concepts in time series. This can be for instance \nthe seasonality component in time series. However, when we have limited size data \nwe are at a greater risk of overfitting when using attention.\nSecondly, relative position embedding w i−j takes into account the relative shift \nbetween positions i and j and not their values. This is similar to translation equiva-\nlence property of convolution, which has been shown to enhance generalization (Dai \net al. 2021). We propose to consider the notation of w i−j as a scalar rather than a vec-\ntor to enable the translation equivalency without blowing up the number of param-\neters. In addition, the scalar representation of w provides the benefit that the value of \nw i−j for all (i, j) can be subsumed within the pairwise dot-product attention function, \nresulting in minimal additional computation (see Sect.  4.2.1). We call our proposed \nefficient relative position encoding as eRPE.\nTheoretically, there are many possibilities for integrating relative position infor -\nmation into the attention matrix, but we empirically found that attention models \nperform better when we add the relative position to the model after applying the \nsoftmax to the attention matrix as shown in Eq. (14) and Fig.  3b. We presume this \nis because the position values will be sharper without the softmax. And sharper \nposition embeddings seems to be beneficial in TSC task as it emphasizes more \non informative relative positions for classification compared to existing models in \nwhich softmax is applied to relative position embeddings.\n4.2.1  Efficient implementation: indexing\nTo implement the efficient version of eRFE in Eq. (14) for input time series with a \nlength of L, for each head, we create a trainable parameter w of size 2L − 1 , as the \nmaximum distance is 2L − 1 . Then for two position indices i and j, the correspond-\ning relative scalar is w i−j+L  where indexes start from 1 instead of 0 (1-base index). \nAccordingly, we need to index L2 elements from 2L − 1 vector.\nOn GPU, a more efficient way to index is to use gather , which only requires \nmemory access. At inference time, indexing the L2 elements from 2L − 1 vector can \nbe pre-computed and cached to increase the processing speed further. As shown in \nTable 1, our proposed eRPE is more efficient in terms of both memory and time \ncomplexities compared to the existing relative position encoding methods in the \nliterature.\n4.3  ConvTran\nNow we look at how we can utilize our new position encodings method to build a \ntime series classification network. According to the earlier discussion, global atten-\ntion has a quadratic complexity w.r.t. the series length. This means that if we directly \n35\n1 3\nImproving position encoding of transformers for multivariate…\napply the proposed attention in Eq. (14) to the raw time series, the computation will \nbe excessively slow for long time series. Hence, we first use convolutions to reduce \nthe series length and then apply our proposed position encodings once the feature \nmap has been reduced to a less computationally intense size. See Fig.  4 where con-\nvolution blocks comes as a first component proceeded by attention blocks.\nAnother benefit of using convolutions is that convolutions operations are very \nwell-suited to capture local patterns. By using convolutions as the first component \nin our architecture we can capture any discriminative local information that exists in \nraw time series.\nAs Shown in Fig.  4, as the first step in the convolution layers, M temporal filters \nare applied to the input data. In this step, the model extracts temporal patterns in \nthe input series. Next, the output of temporal filtering is convolved with dmodel spa-\ntial dx × M  shape filters to capture the correlations between variables in multivariate \ntime series and construct dmodel size input embeddings. Such disjoint temporal and \nspatial convolution is similar to “Inverted Bottleneck” in Sandler et  al. (2018). It \nfirst expands the number of input channels and then squeezes them. A key reason for \nthis choice is that the Feed Forward Network (FFN) in transformers (Vaswani et al. \n2017) also expands on the input size and later projects the expanded hidden state \nback to the original size to capture the spatial interactions.\nBefore feeding the input embedding to the transformer block, we add the tAPE-\ngenerated position embedding to the input embedding vector so that the model can \ncapture the temporal order of the time series. The size of the embedding vector is \ndm odel , which is the same as the input embedding. Inside the multi-head attention, \nthe inputs with the L × dmodel dimension are first converted to L × dz × 3 shape using \nTable 1  Comparing the parameter sizes, memory, and computation complexities of various position \nencoding methods\nIn our implementation dz is equal to dmodel\nMethod Parameter Memory Complexity\nAbsolute tAPE None Ldmodel Ldmodel\nVanilla APE (Vaswani et al. 2017) None Ldmodel Ldmodel\nLearn (Devlin et al. 2018) Ldmodel Ldmodel Ldmodel\nRelative Shaw Shaw et al. (2018) (2L − 1)dz L2dz + L2 L2dz\nVector (Huang et al. 2018) Ldz Ldz + L2 L2dz\neRPE 2L − 1 L + L2 L2\nFig. 4  Overall architecture of the ConvTran model\n36\n N. M. Foumani et al.\n1 3\na linear layer to get the qkv matrix in which dz indicates the model dimension and \ndefined by the user. Each of the three matrices of shape L × dz represents the Query \n(q), Key (k) and Value (v) matrices. These q, k, and v matrices are reshaped to \nh × L × dz∕h to represent the h attention heads. Each of these attention heads can be \nresponsible for capturing different patterns in time series. For instance, one attention \nhead can attend to the non-noisy data, another head can attend to the seasonal com-\nponent and another to the trend. Once we have the q, k, and v matrices, we finally \nperform the attention operation inside the Multi-Head attention block using Eq. (14).\nAccording to Eq. (14) the eRPE with the same shape of L × L is also added to \nthe attention output. We consider the notation of w i−j as a scalar (i.e., w ∈ RO(L) ) to \nenable the global convolution kernel without increasing the number of parameters. \nThe relative position embedding enables the model to learn not only the order of \ntime points, but also the relative position of pairs of time points, which can capture \nricher information than other position embedding strategies.\nThe FFN, is a multi-layer perceptron block consisting of two linear layers and \nGaussian Error Linear Units (GELUs) as an activation function. The outputs from \nthe FFN block are again added to the inputs (via skip connection) to get the final \noutput from the transformer block. Finally, just before the fully connected layer, \nmax-pooling and global average pooling (GAP) are applied to the output of the last \nlayer’s ELU activation function, which gives a more translation-equivalence model.\n5  Experimental results\nIn this section, we evaluate the performance of our ConvTran model on the UEA \ntime series repository (Bagnall eta al. 2018) and two large multivariate time series \ndatasets and compare it with the state-of-the-art models. All of our experiments were \nconducted using the PyTorch framework in Python on a computing system consist-\ning of a single Nvidia A5000 GPU with 24GB of memory and an Intel(R) Core(TM) \ni9-10900K CPU. To promote reproducibility, we have provided our source code and \nmore experimental results online.1\nWe have divided our experiments into four parts. First, we present an ablation \nstudy on various position encodings. Then, we demonstrate that our ConvTran \nmodel outperforms existing CNN and transformer-based models. Next, we compare \nthe performance of ConvTran with four state-of-the-art MTSC algorithms (includ-\ning both deep learning and non-deep learning categories) identified in Ruiz et  al. \n(2020); Middlehurst et al. (2011). We report the results provided on the archive web-\nsite2 for HiveCote2, CIF, ROCKET, and Inception-Time on 26 out of 30 UEA data-\nsets only in Sect.  5.6. Finally, we evaluate the efficiency and effectiveness of Con-\nvTran by comparing it with the current state-of-the-art model, ROCKET.\n1 https:// github. com/ Navid fouma ni/ ConvT ran.\n2 https:// times eries class ifica tion. com/ HC2. php.\n37\n1 3\nImproving position encoding of transformers for multivariate…\n5.1  Datasets\n• UEA Repository The archive consists of 30 real-world multivariate time series \ndata from a wide range of applications such as Human Activity Recognition, \nMotion classification, and ECG/EEG classification (Bagnall eta al. 2018). The \nnumber of dimensions ranges from two dimensions to 1345 dimensions. The \nlength of the time series ranges from 8 to 17,984. The datasets also have a train \nsize ranging from 12 to 25000.\n• Ford Challenge This dataset is obtained from the Kaggle challenge website. 3 It \nincludes measurements from total of 600 real-time driving sessions where each \ndriving session takes 2  min and sampled with 100ms rate. Also, the trials are \nsamples from 100 drivers of both genders, and of different ages. The training \ndata file consists of 604,329 data points each belongs to one of 500 trials. The \ntest file contains 120,840 data points belonging to 100 trials. While each data \npoint comes with a label in 0,1 and also contains 8 physiological, 12 environ-\nmental, and 10 vehicular features that are acquired while driving.\n• Actitracker human Activity Recognition This dataset describes six daily \nactivities which are collected in a controlled laboratory environment. The activi-\nties include “Walking”, “Jogging”, “Stairs”, “Sitting”, “Standing”, and “Lying \nDown” which are recorded from 36 users collected using a cell phone in their \npocket. Data has 2,980,765 samples with 3 dimensions, subject-wise split into \ntrain and test sets, and a sampling rate of 20Hz (Lockhart et al. 2011).\n5.2  Evaluation procedure\nWe use the classification accuracy as the overall metric to compare different models. \nThen we rank each model based on its classification accuracy per dataset. The most \naccurate model is assigned a rank of 1 and the worse performing model is assigned \nthe highest rank. The average ranking is taken in case of ties. Then the average rank \nfor each model is computed across all datasets in the repository.\nThis gives a direct general assessment of all the models: the lowest rank corre-\nsponds to the method that is the most accurate on average. The average ranking for \neach model is presented in the form of critical difference diagram (Demšar 2006), \nwhere models in the same clique (the black bar in the diagram) are not statistically \nsignificant. For the statistical test, we used the Wilcoxon signed-rank test with Holm \ncorrection as the post hoc test to the Friedman test (Demšar 2006).\n5.3  Parameter setting\nAdam optimization is used simultaneously with an early stopping method based on \nvalidation loss. We use the default setting for other models. We set the default value \nfor the number of temporal and spatial filters to 64 and set the length of the temporal \n3 https:// www. kaggle. com/c/ staya lert.\n38\n N. M. Foumani et al.\n1 3\nfilters to 8. The width of the spatial convolutions are set equal to the input dimen-\nsions (Foumani et al. 2021).\nSimilar to TST, the transformers based model for MTSC (Zerveas et al. 2021), \nand default transformers block (Vaswani et al. 2017), we use 8 heads to capture the \nvarieties of attention from input series. The dimension of transformers encoding is \nset to dmodel = dz = 64 and FFN in transformers block expands the input size by 4x \nand later projects the 4x-wide hidden state back to the original size.\n5.4  Ablation study on position encoding\nIn this section, firstly we compare our proposed tAPE with the exisiting absolute \nposition encodings. Secondly, we compare our proposed eRPE with the existing rel-\native position encoding methods. As a final step, we combined tAPE and eRPE into \na single framework and campare it with all possible combinations of absolute and \nrelative position encodings.\nFor this ablation study we run a single-layer transformer five times on all 30 UEA \nbenchmark datasets for classification. Figure 5a illustrates the critical difference dia-\ngram of a single-layer transformer with different absolute position encodings. Note \nin critical difference diagram methods grouped by a black line are not significantly \ndifferent from each other. In Fig.  5, None is the model without any position encod-\ning, Learn is the model with learning absolute position encoding parameters (Devlin \net al. 2018), Vanilla APE is the vanilla sinusoidal function-based encoding (Vaswani \net al. 2017), Vector is the vector-based implementation of input-dependent relative \nposition embedding (Huang et al. 2018), and our proposed models showed as tAPE \nand eRPE.\nAs depicted in Fig.  5a, tAPE has the highest rank in terms of accuracy and is \nsignificantly better than other absolute position encodings due to effectively utilising \nembedding space to provide an isotropic encoding while holding the distance aware-\nness property. As expected, the model without position encoding has the least accu-\nrate results, highlighting the importance of absolute position encoding in time series \nclassification. The vanilla APE also improves overall performance despite not being \nsignificantly accurate than Learn APE since it has fewer parameters.\nFigure 5b shows the critical difference diagram of a single-layer transformer with \ndifferent relative position encodings. As shown in this figure, eRPE has the highest \nrank and is significantly better than other encodings in terms of accuracy as it has \nless number of parameters which is less likely to overfit. It is not surprising that \nthe model without position encoding has the least accurate results, highlighting the \nimportance of relative position encoding and the translation equality property in \nFig. 5  Critical difference diagram of various position encoding over thirty datasets for the UEA MTSC \narchive based on average accuracies: a Various absolute position encodings, b Various relative position \nencodings. The lowest rank corresponds to the method that is the most accurate on average\n39\n1 3\nImproving position encoding of transformers for multivariate…\ntime series classification. The input-dependent Vector encoding also improves over-\nall performance and is significantly better than None model. Figure 6 shows the crit-\nical difference diagram for the various combinations of absolute and relative posi-\ntion encodings. As depicted in this figure, the combination of our proposed tAPE \nand eRPE is significantly more accurate than all other combinations. This shows \nthe high potential of our encoding methods to incorporate position information into \ntransformers. The combination of Learn and Vector has the least accurate results, \nmost likely due to the high number of parameters.\n5.5  Comparing with state‑of‑the‑art deep learning models\nwe compare our ConvTran with the following convolution-based and transformer-\nbased models for MTSC:\n• FCN: Fully Convolutional Neural network is one of the most accurate deep neu-\nral networks for MTSC (Fawaz et al. 2019) reported in the literature.\n• ResNet: Residual Network is also one of most accurate deep neural networks for \nboth univariate TSC and MTSC(Fawaz et al. 2019) reported in the literature.\n• Disjoint-CNN: One of the accurate and lightweight CNN-based models that fac-\ntorize convolution kernels into disjoint temporal and spatial convolutions (Foum-\nani et al. 2021).\n• Inception-Time: The most accurate deep learning univariate TSC and MTSC \nalgorithm to date. Fawaz et al. (2020); Ruiz et al. (2020).\n• TST: A transformer-based model for MTSC (Zerveas et al. 2021).\nFigure 7 shows the average rank of ConvTran on 32 MTS datasets againts all convo-\nlutional-based and/or transformer-based methods. This figure shows that on average, \nConvTran has the lowest average rank and is more accurate than all other methods. \nIt is important to observe that ConvTran is significantly more accurate than its pre-\ndecessors, i.e., a convolution based model, Disjoint-CNN as well as the transformer \nFig. 6  The average rank of various combination of absolute and relative position encodings\nFig. 7  The average rank of ConvTran against all deep learning based methods on all 32 MTS datasets. \nDatasets are sorted based on the number of training samples per-class. The highest accuracy for each \ndataset is highlighted in bold\n40\n N. M. Foumani et al.\n1 3\nbased model, TST. This indicates the effectiveness of adding tAPE and eRPE to \ntransformers. Table 2 presents the classification accuracy of each method on all 32 \ndatasets and the highest accuracy for each dataset is highlighted in bold. In this table \ndatasets are sorted based on the number of training samples per class. Considering \nFig. 7 and Table 2 we can conclude that ConvTran is the most accurate TSC method \non average on all 32 benchmark datasets and particularly has superior performance \nin datasets in which there are enough data to train (i.e., the number of training sam-\nples per class is more than 100) and wins on all 12 datasets except one.\nTable 2  Average accuracy of six deep learning based models over 32 multivariate time series datasets\nDataSets Avg Train ConvTran TST IT Disjoint-CNN FCN ResNet\nFord 17,300 0.7805 0.7655 0.7628 0.7422 0.6353 0.687\nHAR 8400 0.9098 0.8831 0.8775 0.8807 0.8445 0.8711\nFaceDetection 2945 0.6722 0.6542 0.5885 0.5665 0.5037 0.5948\nInsectwingbeat 2500 0.7132 0.6748 0.6956 0.6308 0.6004 0.65\nPenDigits 750 0.9871 0.9694 0.9797 0.9708 0.9857 0.9771\nArabicDigits 660 0.9945 0.9749 0.9872 0.9859 0.9836 0.9832\nLSST 176 0.6156 0.2846 0.4456 0.5559 0.5616 0.5725\nFingerMovement 158 0.56 0.58 0.56 0.54 0.53 0.54\nMotorImagery 139 0.56 0.48 0.53 0.49 0.55 0.52\nSelfRegSCP1 134 0.918 0.86 0.8634 0.8839 0.7816 0.8362\nHeartbeat 102 0.7853 0.6975 0.6248 0.717 0.678 0.7268\nSelfRegSCP2 100 0.5833 0.5333 0.4722 0.5166 0.4667 0.5\nPhonemeSpectra 85 0.3062 0.089 0.1586 0.2821 0.1599 0.1596\nCharacterTraject 72 0.9922 0.9825 0.9881 0.9945 0.9868 0.9945\nEthanolConcen 66 0.3612 0.151 0.3489 0.2775 0.3232 0.3155\nHandMovement 40 0.4054 0.5405 0.3783 0.5405 0.2973 0.2838\nPEMS-SF 39 0.8284 0.7572 0.8901 0.8901 0.8324 0.7399\nRacketSports 38 0.8618 0.8815 0.8223 0.8355 0.8223 0.8223\nEpilepsy 35 0.9855 0.9492 0.9928 0.8898 0.9928 0.9928\nJapaneseVowels 30 0.9891 0.9837 0.9702 0.9756 0.973 0.9135\nNATOPS 30 0.9444 0.95 0.9166 0.9277 0.8778 0.8944\nEigenWorms 26 0.5934 0.4503 0.5267 0.5934 0.4198 0.4198\nUWaveGesture 15 0.8906 0.8906 0.9093 0.8906 0.85 0.85\nLibras 12 0.9277 0.8222 0.8722 0.8577 0.85 0.8389\nArticularyWord 11 0.9833 0.9833 0.9866 0.9866 0.98 0.98\nBasicMotions 10 1 0.975 1 1 1 1\nDuckDuckGeese 10 0.62 0.5 0.36 0.5 0.36 0.24\nCricket 9 1 1 0.9861 0.9772 0.9306 0.9722\nHandwriting 6 0.3752 0.2752 0.3011 0.2372 0.376 0.18\nERing 6 0.9629 0.9296 0.9296 0.9111 0.9037 0.9296\nAtrialFibrillation 5 0.4 0.2 0.2 0.4 0.3333 0.3333\nStandWalkJump 4 0.3333 0.3333 0.4 0.3333 0.4 0.4\n41\n1 3\nImproving position encoding of transformers for multivariate…\n5.6  Benchmark against state‑of‑the‑art models\nGiven the experiments on the 32 datasets show that our ConvTran model has the \nbest performance compared to all the other convolution and transformers based \nmodels, we now proceed to benchmark it against the state-of-the-art MTSC mod-\nels, i.e., both deep learning and non-deep learning models. We compare HC2, CIF \nand ROCKET models on only 26 out of 32 MTSC benchmarking datasets (Ruiz \net al. 2020) because the other six datasets are either large in terms of training sample \nor have varied series lengths that make it almost impossible to run HC2 on them. \nFor having detailed insights into the ConvTran performance we provide a pair-wise \ncomparison between our proposed model and each of these models.\nAs shown in Fig.  8 our proposed model mostly outperforms HC2, ROCKET, \nCIF, and Inception-Time on the datasets with 100 or more training samples per class \n(marked with a blue circle). However, state-of-the-art models outperform ConvTran \non datasets with few training instances such as EigenWorms  with 26 train sample \nper-class. Indeed, as shown in Table  2, all CNN based models fail to perform com-\npetitively on the EigenWorms  dataset. Note that ConvTran is the most accurate \namong all CNNs on this dataset. This is due to the limitation of CNN-based models, \nwhich cannot capture long-term dependencies in the high length time series. Adding \na transformer improves the performance, but it still requires more training samples \nto perform as well as other models.\nIt is also interesting to observe from Fig.  8a and c that HC2 and CIF perform \nbetter than ConvTran on the EthanolConcentration dataset. Considering that \nthis dataset is based on spectra of water-and-ethanol, hence interval and shapelet-\nbased approaches which are also components of HC2 perform better. On the other \nhand, ROCKET has a few wins compared to ConvTran (Fig 8b). Most of these data-\nsets where ROCKET performs better, such as the StandWalkjump dataset have \na small number of time series instances per class. For instance, StandWalkjump  \nhas 3 classes with 12 training instances, which is 4 time series per class. This is \ninsufficient to train large number of parameters in deep learning models such as \nConvTran to achieve better performance. Note, as mentioned, these results are for \n26 datasets only, excluding six datasets for which we could not run HC2 (which has \nhigh computational complexity and is limited to be applied on variable-length time \nseries). Among excluded datasets, 4 of them are large datasets from which Con-\nvTran could have benefited. Considering this, ConvTran still achieves competetive \nperformance compared to SOTA deep and non-deep models.\n5.7  ConvTran versus rocket efficiency and effectiveness\nTo provide further insight into the efficiency of our model on datasets of varying \nsizes, we conducted additional experiments on the largest UEA dataset InsectWing-\nBeat with 25,000 series for training. We compare the training time and test accu-\nracy of our proposed ConvTran and ROCKET on random subsets of 5,000, 10,000, \n15,000, 20,000, and 25,000 training samples.\n42\n N. M. Foumani et al.\n1 3\nThe results depicted in Fig.  9 demonstrate that ROCKET has faster training time \nthan ConvTran on smaller datasets, specifically on the 5k and 10k datasets while \nachieving similar training time to ConvTran on the 15k set. However, our deep learn-\ning-based model, ConvTran, demonstrates faster training times with increasing data \nquantity, as expected. Additionally, we also observed from the figure that ConvTran \nis consistently more accurate than ROCKET on this dataset. We refer interested \nreaders to Appendix  for a more comprehensive exploration of the empirical evalua-\ntion of efficiency and effectiveness on all datasets. Notably, ConvTran demonstrates \nfaster inference time compared to ROCKET across all datasets. It is important to \nFig. 8  Pairwise comparison of ConvTran with the state of the art models: a HC2, b ROCKET, c CIF d \nand Inception-Time. The datasets with 100 training samples per class or more are marked with a blue \ncircle, while the others are marked with a red square. The three values at the top of each figure show the \nnumber of win/draw/loss from left to right\n43\n1 3\nImproving position encoding of transformers for multivariate…\nnote that all the ConvTran experiments are performed on GPUs, whereas ROCKET \nexperiments are performed on a CPU (please refer to Sect.  5 for computing system \ndetails).\n6  Conclusion\nThis paper studies the importance of position encoding for time series for the first \ntime and reviews existing absolute and relative position encoding methods in time \nseries classification. Based on the limitations of the current position encodings for \ntime series, we proposed two novel absolute and relative position encodings sep-\necifically for time series called tAPE and eRPE, respectively. We then integrated our \ntwo proposed position encodings into a transformer block and combine them with a \nconvolution layer and presented a novel deep-learning framework for multivariate \ntime series classification (ConvTran). Extensive experiments show that ConvTran \nbenefits from the position information, achieving state-of-the-art performance on \nMultivariate time series classification in deep learning literature. In future, we will \nstudy the effectiveness of our new transformer block in other transformer-based TSC \nmodels and other down stream tasks such as anomaly detection.\nAppendix\n Empirical evaluation of efficiency and effectiveness\nThe results presented in Table  3 demonstrate that ConvTran outperforms \nROCKET in terms of both train time and test accuracy on larger datasets with \nmore than 10k samples. However, ROCKET has a better train time on smaller \nFig. 9  Comparison of runtime and accuracy between ConvTran and ROCKET on UEA largest dataset \nInsectWingBeat with 25,000 training samples. The figure shows the runtime of the two models on data-\nsets with different sizes, and their corresponding classification accuracy\n44\n N. M. Foumani et al.\n1 3\nTable 3  Comparison of runtime and accuracy between ConvTran and ROCKET on 32 datasets of vary -\ning sizes\nTo facilitate easy identification, superior performance in both accuracy and runtime is highlighted in \nbold in the table. For a detailed comparison, the runtimes are shown in seconds\nDatasets Train size ROCKET ConvTran\nAccuracy Train time Test time Accuracy Train time Test time\nHAR 41,546 0.8293 5366.34 11.51 0.9098 2367.77 1.82\nFord 28,839 0.6051 6863.81 11.91 0.7805 1619.42 0.95\nInsectWingbeat 25,000 0.4182 5721.04 41.5 0.7132 1617.82 5.47\nPenDigits 7494 0.984 65.26 0.99 0.9871 401.1 0.59\nArabicDigits 6599 0.9932 75.59 10.38 0.9945 376.7 0.37\nFaceDetection 5890 0.5624 53.23 11.99 0.6722 413.39 0.83\nPhonemeSpectra 3315 0.1894 42 37.22 0.3062 202.27 0.89\nLSST 2459 0.5251 5.84 3.52 0.6156 148.07 0.48\nCharacterTrajec 1422 0.9916 8.4 7.72 0.9922 89.61 0.28\nFingerMovement 316 0.55 0.96 0.35 0.56 21.33 0.02\nMotorImagery 278 0.56 45.39 16.26 0.56 386 0.81\nArticularyWord 275 0.9933 2.09 2.19 0.9833 19.76 0.08\nJapaneseVowels 270 0.9568 0.57 0.67 0.9891 20.6 0.13\nSelfRegSCP1 268 0.8601 10.2 11.25 0.918 45.54 0.27\nPEMS-SF 267 0.8266 3.53 2.13 0.8284 28.08 0.09\nEthanolConcen 261 0.4448 14.59 14.32 0.3612 131.58 0.69\nHeartbeat 204 0.7414 4.57 4.59 0.7853 17.13 0.09\nSelfRegSCP2 200 0.5833 10.78 9.65 0.5833 50.05 0.22\nNATOPS 180 0.8944 0.6 0.58 0.9444 14.61 0.04\nLibras 180 0.8667 0.36 0.29 0.9277 11.51 0.04\nHandMovement 160 0.4189 3.31 1.7 0.4054 11.29 0.03\nRacketSports 151 0.9078 0.29 0.32 0.8618 11.86 0.03\nHandwriting 150 0.5376 0.81 3.92 0.3752 11.85 0.23\nEpilepsy 137 0.971 0.91 0.93 0.9855 10.52 0.03\nEigenWorms 128 0.8702 107.48 111.42 0.5934 225.71 0.7\nUWaveGesture 120 0.9188 1.21 3.07 0.8906 10.2 0.09\nCricket 108 1 5.79 4.07 1 32.1 0.1\nDuckDuckGeese 50 0.5 1.59 1.76 0.62 9.46 0.05\nBasicMotions 40 1 0.25 0.27 1 4.45 0.01\nERing 30 0.9851 0.17 0.7 0.9629 3.17 0.06\nAtrialFibrillation 15 0.2 0.39 0.41 0.4 1.99 0.01\nStandWalkJump 12 0.5333 1.52 1.65 0.3333 14.56 0.09\ndatasets. Nevertheless, even on small datasets, ConvTran achieves acceptable \naccuracy within a reasonable train time. It is worth noting that the performance \nof ConvTran improves as the dataset size increases, indicating that our model is \nsuitable for scaling to larger datasets.\n45\n1 3\nImproving position encoding of transformers for multivariate…\nConvTran versus non‑deep learning SOTA models\nTable  4 compares the performance of ConvTran against three non-deep learn-\ning models - ROCKET, HC2, and CIF - on different datasets with varying training \nsample sizes. The table presents the accuracy of each model on each dataset, with \nTable 4  Comparison of ConvTran and non-deep learning models (ROCKET, HC2, CIF) on varying \ntraining sample sizes\nBold Face Font Indicates Superior Accuracy, ’-’ Denotes Non-Runnable Methods due to Computation \nComplexity or Inability to Handle Various Length Series\nDatasets Train Size ConvTran ROCKET HC2 CIF\nHAR 41,546 0.9098 0.8293 – –\nFord 28,839 0.7805 0.6051 – –\nInsectWingbeat 25,000 0.7132 0.4182 – –\nPenDigits 7494 0.9871 0.984 0.9791 0.9674\nSpokenArabicDigits 6599 0.9945 0.9932 – –\nFaceDetection 5890 0.6722 0.5624 0.6603 0.6271\nPhonemeSpectra 3315 0.3062 0.1894 0.2905 0.2654\nLSST 2459 0.6156 0.5251 0.6427 0.5726\nCharacterTrajectories 1422 0.9922 0.9916 – –\nFingerMovements 316 0.56 0.55 0.53 0.52\nMotorImagery 278 0.56 0.56 0.54 0.5\nArticularyWordRecognition 275 0.9833 0.9933 0.9933 0.9833\nJapaneseVowels 270 0.9891 0.9568 – –\nSelfRegulationSCP1 268 0.918 0.8601 0.8908 0.8601\nPEMS-SF 267 0.8284 0.8266 1 1\nEthanolConcentration 261 0.3612 0.346 0.7719 0.7338\nHeartbeat 204 0.7853 0.678 0.7317 0.7805\nSelfRegulationSCP2 200 0.5833 0.5833 0.5 0.5\nNATOPS 180 0.9444 0.8944 0.8944 0.8556\nLibras 180 0.9277 0.8667 0.9333 0.9111\nHandMovementDirection 160 0.4054 0.4189 0.473 0.5946\nRacketSports 151 0.8618 0.9078 0.9078 0.8816\nHandwriting 150 0.3752 0.5376 0.5482 0.3565\nEpilepsy 137 0.9855 0.971 1 0.9855\nEigenWorms 128 0.5934 0.8702 0.9466 0.916\nUWaveGestureLibrary 120 0.8906 0.9188 0.9281 0.925\nCricket 108 1 1 1 0.9861\nDuckDuckGeese 50 0.62 0.5 0.56 0.44\nBasicMotions 40 1 1 1 1\nERing 30 0.9629 0.9593 0.9889 0.9815\nAtrialFibrillation 15 0.4 0.1333 0.2667 0.3333\nStandWalkJump 12 0.3333 0.5333 0.4667 0.4\nWins or Draw – 19 7 13 4\n46\n N. M. Foumani et al.\n1 3\nboldface indicating superior accuracy. “-” denotes non-runnable methods, either due \nto computation complexity or inability to handle various length series.\nOverall, ConvTran outperforms the non-deep learning models on 19 out of 32 \ndatasets (for the HC2 and CIF models, we only have results for 26 datasets, and \nConvTran outperforms the other models in 13 out of the 26). It performs better on \ndatasets with larger training sample sizes, such as InsectWingBeat, while other mod-\nels perform better on datasets with fewer training samples, such as StandWalkJump, \nwhich only has 12 training samples. Additionally, the table shows that some of the \nnon-deep learning models failed to handle specific datasets due to either computa-\ntional complexity or the inability to handle varying input series lengths. For exam-\nple, we were not able to run HC2 and CIF on the larger HAR, Ford, and InsectWing-\nbeat datasets due to computational complexity. They were also not designed to \nhandle varying length time series such as the CharacterTrajectories, SpokenArabic-\nDigits, and JapaneseVowels datasets.\nFunding Open Access funding enabled and organized by CAUL and its Member Institutions.\nDeclarations \nConflict of interest The authors have no competing interests to declare that are relevant to the content of \nthis article.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is \nnot permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission \ndirectly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen \nses/ by/4. 0/.\nReferences\nBagnall A, Lines J, Bostrom A, Large J, Keogh E (2017) The great time series classification bake off: \na review and experimental evaluation of recent algorithmic advances. Data Min Knowl Disc \n31(3):606–660\nBagnall A, Dau HA, Lines J, Flynn M, Large J, Bostrom A, Southam P, Keogh E (2018) The UEA multi-\nvariate time series classification archive. arXiv preprint arXiv: 1811. 00075\nBagnall A, Flynn M, Large J, Lines J, Middlehurst M (2020) On the usage and performance of the hier -\narchical vote collective of transformation-based ensembles version 1.0 (hive-cote v1. 0). In: Interna-\ntional workshop on advanced analytics and learning on temporal data, pp 3–18\nDai Z, Liu H, Le QV, Tan M (2021) Coatnet: marrying convolution and attention for all data sizes. Adv \nNeural Inf Process Syst 34:3965–3977\nDempster A, Petitjean F, Webb GI (2020) Rocket: exceptionally fast and accurate time series classifica-\ntion using random convolutional kernels. Data Min Knowl Disc 34(5):1454–1495\nDempster A, Schmidt DF, Webb GI (2021) Minirocket: A very fast (almost) deterministic transform for \ntime series classification. In: SIGKDD conference on knowledge discovery and data mining, pp \n248–257\nDemšar J (2006) Statistical comparisons of classifiers over multiple data sets. J Mach Learn Res 7:1–30\n47\n1 3\nImproving position encoding of transformers for multivariate…\nDevlin J, Chang MW, Lee K, Toutanova K (2018) Bert: Pre-training of deep bidirectional transformers \nfor language understanding. arXiv preprint arXiv: 1810. 04805\nDosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer \nM, Heigold , Gelly, S, et al (2020) An image is worth 16x16 words: Transformers for image recog-\nnition at scale. arXiv preprint arXiv: 2010. 11929\nDufter P, Schmitt M, Schütze H (2022) Position information in transformers: an overview. Comput Lin-\nguist 48(3):733–763\nFawaz HI, Forestier G, Weber J, Idoumghar L, Muller P-A (2019) Deep learning for time series classifi-\ncation: a review. Data Min Knowl Disc 33(4):917–963\nFawaz HI, Lucas B, Forestier G, Pelletier C, Schmidt DF, Weber J, Webb GI, Idoumghar L, Muller P-A, \nPetitjean F (2020) Inceptiontime: finding alexnet for time series classification. Data Min Knowl \nDisc 34(6):1936–1962\nFoumani NM, Miller L, Tan CW, Webb GI, Forestier G, Salehi M (2023) Deep learning for time series \nclassification and extrinsic regression: a current survey. arXiv preprint arXiv: 2302. 02515\nFoumani SNM, Tan CW, Salehi M (2021) Disjoint-cnn for multivariate time series classification. In: \n2021 International Conference on Data Mining Workshops, pp. 760–769\nHao Y, Cao H (2020) A new attention mechanism to classify multivariate time series. In: International \njoint conference on artificial intelligence\nHuang CZA, Vaswani A, Uszkoreit J, Shazeer N, Simon I, Hawthorne C, Dai AM, Hoffman MD, Din-\nculescu M, Eck D (2018) Music transformer. arXiv preprint arXiv: 1809. 04281\nHuang Z, Liang D, Xu P, Xiang B (2020) Improve transformer models with better relative position \nembeddings. arXiv preprint arXiv: 2009. 13658\nKarim F, Majumdar S, Darabi H, Harford S (2019) Multivariate lstm-fcns for time series classification. \nNeural Netw 116:237–245\nKostas D, Aroca-Ouellette S, Rudzicz F (2021) Bendr: using transformers and a contrastive self-super -\nvised learning task to learn from massive amounts of eeg data. Front Hum Neurosci 15\nLiang Y, Cao R, Zheng J, Ren J, Gao L (2021) Learning to remove: towards isotropic pre-trained bert \nembedding. In: International conference on artificial neural networks, pp 448–459\nLiu M, Ren S, Ma S, Jiao J, Chen Y, Wang Z, Song W (2021) Gated transformer networks for multivari-\nate time series classification. arXiv preprint arXiv: 2103. 14438\nLockhart JW, Weiss GM, Xue JC, Gallagher ST, Grosner AB, Pulickal TT (2011) Design considerations \nfor the wisdm smart phone-based sensor mining architecture. In: International workshop on knowl-\nedge discovery from sensor data, pp 25–33\nLuong MT, Pham H, Manning CD (2015) Effective approaches to attention-based neural machine transla-\ntion. arXiv preprint arXiv: 1508. 04025\nMiddlehurst M, Large J, Flynn M, Lines J, Bostrom A, Bagnall A (2021) Hive-cote 2.0: a new meta \nensemble for time series classification. Mach Learn 110(11):3211–3243\nMiddlehurst M, Large J, Bagnall A (2020) The canonical interval forest (cif) classifier for time series \nclassification. In: 2020 IEEE international conference on big data, pp 188–195\nRuiz AP, Flynn M, Large J, Middlehurst M, Bagnall A (2020) The great multivariate time series classifi-\ncation bake off: a review and experimental evaluation of recent algorithmic advances. Data Mining \nand Knowledge Discovery, pp 1–49\nSandler M, Howard A, Zhu M, Zhmoginov A, Chen LC (2018) Mobilenetv2: Inverted residuals and lin-\near bottlenecks. In: IEEE Conference on Computer Vision and Pattern Recognition, pp. 4510–4520\nShaw P, Uszkoreit J, Vaswani A (2018) Self-attention with relative position representations. arXiv pre-\nprint arXiv: 1803. 02155\nTan CW, Dempster A, Bergmeir C, Webb GI (2021) Multirocket: effective summary statistics for convo-\nlutional outputs in time series classification. arXiv e-prints, 2102\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I (2017) Atten-\ntion is all you need. Adv Neural Inf Process Syst 30\nWang Z, Yan W, Oates T (2017) Time series classification from scratch with deep neural networks: a \nstrong baseline. In: 2017 International joint conference on neural networks, pp 1578–1585\nWu K, Peng H, Chen M, Fu J, Chao H (2021) Rethinking and improving relative position encoding for \nvision transformer. In: IEEE/CVF international conference on computer vision, pp 10033–10041\nYang CHH, Tsai YY, Chen PY (2021) Voice2series: Reprogramming acoustic models for time series \nclassification. In: International Conference on Machine Learning, pp. 11808–11819\n48\n N. M. Foumani et al.\n1 3\nZerveas G, Jayaraman S, Patel D, Bhamidipaty A, Eickhoff C (2021) A transformer-based framework for \nmultivariate time series representation learning. In: SIGKDD conference on knowledge discovery \nand data mining, pp 2114–2124\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps \nand institutional affiliations.\nAuthors and Affiliations\nNavid Mohammadi Foumani1  · Chang Wei Tan1 · Geoffrey I. Webb1 · \nMahsa Salehi1\n * Navid Mohammadi Foumani \n navid.foumani@monash.edu.com\n Chang Wei Tan \n chang.tan@monash.edu\n Geoffrey I. Webb \n geoff.webb@monash.edu\n Mahsa Salehi \n mahsa.salehi@monash.edu\n1 Department of Data Science and Artificial Intelligence, Monash University, Melbourne, VIC, \nAustralia"
}