{
  "title": "Dialogue State Tracking with a Language Model using Schema-Driven Prompting",
  "url": "https://openalex.org/W3200895474",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2271616566",
      "name": "Chia-Hsuan Lee",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2005151083",
      "name": "Cheng Hao",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A1953639869",
      "name": "Mari Ostendorf",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4288094254",
    "https://openalex.org/W2954492830",
    "https://openalex.org/W2972777589",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W2784070054",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2888849322",
    "https://openalex.org/W3119822474",
    "https://openalex.org/W3000779003",
    "https://openalex.org/W2951216772",
    "https://openalex.org/W4287795696",
    "https://openalex.org/W3034533785",
    "https://openalex.org/W3100128199",
    "https://openalex.org/W3004786215",
    "https://openalex.org/W3155584966",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3100110884",
    "https://openalex.org/W3104078590",
    "https://openalex.org/W2997771882",
    "https://openalex.org/W3024509506",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4288027128",
    "https://openalex.org/W3156909481",
    "https://openalex.org/W2804010326",
    "https://openalex.org/W3021096583",
    "https://openalex.org/W2251058040",
    "https://openalex.org/W3034573951",
    "https://openalex.org/W2955810669",
    "https://openalex.org/W3119649668",
    "https://openalex.org/W3102854726",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2988252747",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W4287815000",
    "https://openalex.org/W2964057895",
    "https://openalex.org/W2964006684",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2998228050",
    "https://openalex.org/W3021016503",
    "https://openalex.org/W3099827451",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2945475330",
    "https://openalex.org/W3023786569",
    "https://openalex.org/W3045703328",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Task-oriented conversational systems often use dialogue state tracking to represent the user's intentions, which involves filling in values of pre-defined slots. Many approaches have been proposed, often using task-specific architectures with special-purpose classifiers. Recently, good results have been obtained using more general architectures based on pretrained language models. Here, we introduce a new variation of the language modeling approach that uses schema-driven prompting to provide task-aware history encoding that is used for both categorical and non-categorical slots. We further improve performance by augmenting the prompting with schema descriptions, a naturally occurring source of in-domain knowledge. Our purely generative system achieves state-of-the-art performance on MultiWOZ 2.2 and achieves competitive performance on two other benchmarks: MultiWOZ 2.1 and M2M. The data and code will be available at https://github.com/chiahsuan156/DST-as-Prompting.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4937‚Äì4949\nNovember 7‚Äì11, 2021.c‚Éù2021 Association for Computational Linguistics\n4937\nDialogue State Tracking with a Language Model\nusing Schema-Driven Prompting\nChia-Hsuan Lee\nUniversity of Washington\nchiahlee@uw.edu\nHao Cheng\nMicrosoft Research\nchehao@microsoft.com\nMari Ostendorf\nUniversity of Washington\nostendor@uw.edu\nAbstract\nTask-orientedconversationalsystemsoftenuse\ndialogue state tracking to represent the user‚Äôs\nintentions, which involves Ô¨Ålling in values\nof pre-deÔ¨Åned slots. Many approaches have\nbeen proposed, often using task-speciÔ¨Åc archi-\ntectures with special-purpose classiÔ¨Åers. Re-\ncently, good results have been obtained us-\ning more general architectures based on pre-\ntrained language models. Here, we introduce\na new variation of the language modeling ap-\nproach that uses schema-driven prompting to\nprovide task-aware history encoding that is\nused for both categorical and non-categorical\nslots. We further improve performance by\naugmenting the prompting with schema de-\nscriptions, a naturally occurring source of in-\ndomain knowledge. Our purely generative sys-\ntem achieves state-of-the-art performance on\nMultiWOZ 2.2 and achieves competitive per-\nformance on two other benchmarks: Multi-\nWOZ 2.1 and M2M. The data and code will\nbe available at https://github.com/\nchiahsuan156/DST-as-Prompting.\n1 Introduction\nIn task-oriented dialogues, systems communicate\nwithusersthroughnaturallanguagetoaccomplisha\nwiderangeoftasks,suchasfoodordering,techsup-\nport, restaurant/hotel/travel booking, etc. The back-\nbone module of a typical system is dialogue state\ntracking(DST),wheretheusergoalisinferredfrom\nthe dialogue history (Henderson et al., 2014; Shah\net al., 2018; Budzianowski et al., 2018). User goals\nare represented in terms of values of pre-deÔ¨Åned\nslots associated with a schema determined by the\ninformation needed to execute task-speciÔ¨Åc queries\nto the backend. In other words, user goals are ex-\ntracted progressively via slot Ô¨Ålling based on the\nschema throughout the conversation. In this paper,\nwe focus on multi-domain DST where the dialogue\nstate is encoded as a list of triplets in the form of\n(domain, slot, value), e.g. (‚Äúrestaurant‚Äù, ‚Äúarea‚Äù,\n‚Äúcentre‚Äù).\nThere are two broad paradigms of DST models,\nclassiÔ¨Åcation-based and generation-basedmodels,\nwhere the major diÔ¨Äerence is how the slot value is\ninferred. In classiÔ¨Åcation-based models (Ye et al.,\n2021; Chen et al., 2020), the prediction of a slot\nvalue is restricted to a Ô¨Åxed set for each slot, and\nnon-categorical slots are constrained to values ob-\nserved in the training data. In contrast, generation-\nbased models (Wu et al., 2019; Kim et al., 2020)\ndecode slot values sequentially (token by token)\nbased on the dialogue context, with the potential\nof recovering unseen values. Recently, generation-\nbased DST built on large-scale pretrained neural\nlanguage models (LM) achieve strong results with-\nout relying on domain-speciÔ¨Åc modules. Among\nthem, the autoregressive model (Peng et al., 2020a;\nHosseini-Asl et al., 2020) uses a uni-directional\nencoder whereas the sequence-to-sequence model\n(Lin et al., 2020a; Heck et al., 2020) represents the\ndialogue context using a bi-directional encoder.\nIn this study, we follow a generation-based DST\napproach using a pre-trained sequence-to-sequence\nmodel, but with the new strategy of adding task-\nspeciÔ¨Åc prompts as input for sequence-to-sequence\nDST models, inspired byprompt-basedÔ¨Åne-tuning\n(Radford et al., 2019; Brown et al., 2020a). Specif-\nically, instead of generating domain and slot sym-\nbols in the decoder, we concatenate the dialogue\ncontext with domain and slot prompts as input to\nthe encoder, where prompts are taken directly from\nthe schema. We hypothesize that jointly encoding\ndialogue context and schema-speciÔ¨Åc textual infor-\nmation can further beneÔ¨Åt a sequence-to-sequence\nDST model. This allows task-aware contextualiza-\ntion for more eÔ¨Äectively guiding the decoder to\ngenerate slot values.\nAlthough the domain and slot names typically\nhave interpretable components, they often do not\nreÔ¨ÇectstandardwrittenEnglish,e.g.‚Äú arriveby‚Äùand\n‚Äúref‚Äù. Those custom meaning representations are\n4938\ntypicallyabbreviatedand/orunder-speciÔ¨Åed,which\ncreates a barrier for eÔ¨Äectively utilizing the pre-\ntrained LMs. To address this issue, we further\nincorporate natural language schema descriptions\ninto prompting for DST, which include useful in-\nformation to guide the decoder. For example, the\ndescription of ‚Äúref‚Äù is ‚Äúreference number of the ho-\ntelbooking‚Äù;thevaluesof‚Äú has_internet‚Äùare‚Äúyes‚Äù,\n‚Äúno‚Äù, ‚Äúfree‚Äù, and ‚Äúdon‚Äôt care‚Äù.\nIn short, this work advances generation-based\nDST in two ways. First, candidate schema labels\nare jointly encoded with the dialogue context, pro-\nvidingatask-awarecontextualizationforinitializing\nthe decoder. Second, natural language descriptions\nofschemacategoriesassociatedwithdatabasedocu-\nmentation are incorporated in encoding as prompts\nto the language model, allowing uniform handling\nof categorical and non-categorical slots. When\nimplemented using a strong pretrained text-to-text\nmodel, this simple approach achieves state-of-the-\nart (SOTA) results on MultiWOZ 2.2, and perfor-\nmance is on par with SOTA on MultiWOZ 2.1 and\nM2M. In addition, our analyses provide empirical\nresults that contribute towards understanding how\nschema description augmentation can eÔ¨Äectively\nconstrain the model prediction.\n2 Related Work\n2.1 Multi-Domain Dialogue State Tracking\nTask-oriented dialogue datasets (Shah et al., 2018;\nHenderson et al., 2014), have spurred the develop-\nmentofdialoguesystems(Zhongetal.,2018;Chao\nand Lane, 2019). Recently, to further examine the\ngeneralization abilities, large scale cross-domain\ndatasets have been proposed (Budzianowski et al.,\n2018; Zang et al., 2020; Eric et al., 2019; Rastogi\net al., 2020b). ClassiÔ¨Åcation-based models (Ye\net al., 2021; Chen et al., 2020) pick the candidate\nfrom the oracle list of possible slot values. The\nassumption of the full access of the schema makes\nthem have limited generalization abilities. On the\nother hand,generation-based models (Wu et al.,\n2019; Kim et al., 2020; Lin et al., 2020a) directly\ngenerate slot values token by token, making it pos-\nsibletohandleunseendomainsandvalues. Mostof\nthese models require task-speciÔ¨Åc modular designs.\nRecently, generation-based models that are built\non large-scale autoregressive pretrained language\nmodels(Hametal.,2020;Hosseini-Asletal.,2020;\nPeng et al., 2020a) achieve promising state track-\ning results on MultiWOZ 2.0 and 2.1 when trained\non additional supervision signals or dialogue cor-\npus. Both Ham et al. (2020) and Hosseini-Asl\net al. (2020) require dialogue acts as inputs. Both\nHosseini-Asl et al. (2020) and Peng et al. (2020a)\nrequire DB search results as inputs. Peng et al.\n(2020a) also leverages other dialogue corpora to\nÔ¨Ånetune the language model. Our work requires\nonly the dialogue state labels and does not utilize\nany external dialogue datasets.\n2.2 Language Models\nLarge-scale pretrained language models have ob-\ntained state-of-the-art performance on diverse\ngeneration and understanding tasks including bi-\ndirectional encoder style language models (Devlin\net al., 2019; Liu et al., 2019), auto-regressive lan-\nguage models (Radford et al., 2019; Brown et al.,\n2020b)andmoreÔ¨Çexiblesequence-to-sequencelan-\nguage models (RaÔ¨Äel et al., 2020). To adapt to di-\nalogue tasks, variants of systems are Ô¨Ånetuned on\ndiÔ¨Äerent dialogue corpora including chit-chat sys-\ntems (Zhang et al., 2020; Adiwardana et al., 2020;\nRoller et al., 2020) and task-oriented dialogue sys-\ntems (Mehri et al., 2019; Wu et al., 2020; Hender-\nson et al., 2020; Peng et al., 2020b). We leave it as\nfuture work to leverage domain-adapted language\nmodels.\n2.3 Prompting Language Models\nExtending a language model‚Äôs knowledge via\nprompts is an active line of research. Radford\net al. (2019) obtain empirical success by using\nprompts to guide zero shot generation without Ô¨Åne-\ntuning on any prompts. RaÔ¨Äel et al. (2020) uses\ntask-speciÔ¨Åc prompts in both Ô¨Ånetuning and testing\nphase. Recent studies have also tried to automati-\ncally discover prompts rather than writing them by\nhumans (Jiang et al., 2020). Our proposed prompt-\ning method is largely inspired by this body of work.\nInsteadofpromptengineering/generation,wefocus\non using available natural language descriptions of\nschema categories associated with database docu-\nmentation as task-speciÔ¨Åc promptings for DST.\n3 Prompting Language Model for\nDialogue State Tracking\nIn this section, we Ô¨Årst set up the notations that are\nused throughout paper, and then review the genera-\ntiveDSTwiththesequence-to-sequenceframework.\nBased on that, we formally introduce our prompt-\nbased DST model and the corresponding backbone\n4939\n[User] ‚Ä¶\n[System] ‚Ä¶\n[Slot]\nref\n[Domain]\ntrain \n[Slot]\nday\ndestination location of the \ntrain, [Possible Values]\nLondon Kings Cross , ‚Ä¶\nday of the departure, \n[Possible Values]\nMonday, ‚Ä¶, Sunday \nreference  number \nof the hotel booking\nLondon \nKings \nCross\nMonday\nnone\n[User] Can you help me \nfind a train for Sunday. \nI would like to visit \nLondon Kings Street .\n[Slot]\ndestination\n[Domain]\ntrain \n[Domain]\nhotel \nNL DescriptionDomain ùíÖùíé Slot ùíîùíè Value ùíóDialogue History ùë™ùíï\n‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶\nT5Dialogue History ‚Ä¶\n(a) Generation-based DST w/ Sequential Decoding\ntrain day Monday\nDialogue History train day T5 Monday\nLondon Kings Cross\nnone\n‚Ä¶‚Ä¶\n(b) Schema-Based Prompt DST w/ Independent Decoding\ntrain destinationDialogue History\nhotel refDialogue History\nT5\nT5\n‚Ä¶\ntrain day day of the departure ‚Ä¶Dialogue History\ntrain destination destination location‚Ä¶Dialogue History\n‚Ä¶\nhotel ref reference  number‚Ä¶Dialogue History\nT5 Monday\nLondon Kings Cross\nnone\n‚Ä¶\nT5\nT5\n‚Ä¶\n(c) Natural Language Augmented Prompt DST w/ Independent Decoding\nFigure 1: Overview of generative DST approaches for multi-domain scenario. The top three Ô¨Ågures illustrate\nthree diÔ¨Äerent generative approaches considered in this paper and the bottom Ô¨Ågure includes speciÔ¨Åc examples for\ndialoguehistory,domainnames,slotnames,naturallanguagedescriptions(types,setofvalidvalues,etc.) forslots.\nSub-Ô¨Ågure (b)(c) demonstrate two prompt-based DST models proposed, where method in (c) includes additional\nnatural language description of slots considered for tracking. Domain descriptions are omitted for brevity.\npretrained model.\nNotation. For task-oriented dialogues consid-\nered in this paper, a dialogue consists of a se-\nquence of utterances alternating between two par-\nties, U1,A1,...,U T,AT, whereU and Arepresent\nthe user utterance and the system response, re-\nspectively. In a turnt, the user provides a new\nutterance Ut and the system agent responds with\nutterance At. As shown in the bottom of Fig-\nure 1, at turnt, we denote the dialogue context\nasCt = {U1,A1,‚Ä¶ ,At‚àí1,Ut}, whichexcludesthe\nlatest system responseAt. In this work, we assume\na multi-domain scenario, in which case the schema\ncontains M domains Óà∞ = {d1,‚Ä¶ ,dM} and N\nslots Óàø = {s1,‚Ä¶ ,sN} to track as examples illus-\ntrated in Figure 1.Bt, the dialogue state at turnt,\nis then deÔ¨Åned as a mapping from a pair (dm, sn)\ninto valuesv. Here, we deÔ¨ÅneBt(dm,sn) = \u001e, if\n(dm,sn) is not in the current dialogue state. In the\ngiven example of Figure 1, the pair (domain=hotel,\nslot=ref) is not in the dialogue state, and the value\n‚Äúnone‚Äù is assigned.\n3.1 Generation-based DST with the\nSequence-to-sequence Model\nThere are primarily two decoding strategies for\ngeneration-based DST in the literature for inferring\nthedialoguestateataparticularturn‚Äìsequential(a)\n4940\nandindependent(b)(c)‚Äìbothofwhichareexplored\nin the paper as illustrated in Figure 1.\nIn the Ô¨Årst case (top system (a) in Figure 1), the\ndialogue historyCt is taken as input to the encoder,\nand domain-slot-value triplets (dm, sn, v) are gen-\nerated sequentially, whereBt(dm,sn) ‚â† \u001e. This\napproach is adopted in many systems that leverage\nautoregressive LMs (Peng et al., 2020a; Hosseini-\nAsl et al., 2020). Despite being simple, this kind\nof sequential generation of multiple values is more\nlikely to suÔ¨Äer from optimization issues with de-\ncoding long sequences resulting in lower perfor-\nmance. However, given its wide adoption in the\nliterature, we still include this type of generative\nDST with the same backbone pretrained encoder-\ndecoderTransformermodelinourexperiments. To\npartially address this issue, Lin et al. (2020b) pro-\npose a domain independent decoding where the\ndecoder only have to generate a sequence of slot\nand value pairs within a speciÔ¨Åc given domain. Al-\nthough their model leverages the same backbone\nmodelasours, weempiricallyÔ¨Åndthatthisformof\nstrategy is still of limited eÔ¨Äectiveness.\nIn the second case (middle two systems (b)(c)\nin Figure 1), the values for each domain-slot pair\nare generated independently, potentially in parallel.\nThe domain and slot names (embedded as contin-\nuous representations) are either the initial hidden\nstate of the decoder (Kim et al., 2020) or the Ô¨Årst\ninputofthedecoder(Wuetal.,2019). Valuesareei-\nther generated for all possible domain-slot(dm,sn)\npairswithapossiblevalueof‚Äú none‚Äùand/orthereis\na separate gating mechanism for domain-slot com-\nbinations not currently active. Since we are inter-\nested in enriching the input with task-speciÔ¨Åc in-\nformation, we focus on extending the independent\ndecoding modeling for our prompt-based DST.\n3.2 Prompt-based DST\nIn this section, we formally present the Ô¨Çow of\nour prompt-based DST with an encoder-decoder\narchitecture. Here, we are interested in an encoder-\ndecodermodelwithabi-directionalencoder(RaÔ¨Äel\netal.,2020;Lewisetal.,2020),incontrastwiththe\nuni-directionalencoderusedinautoregressiveLMs\n(Radford et al., 2019; Brown et al., 2020a).\nThe input of the prompt-based DST is made\nup of a dialogue contextCt and a task-speciÔ¨Åc\nprompt. Here, we use two types of task-speciÔ¨Åc\nprompts, the domain-related promptX(dm), and\nslot-related promptX(sn), both of which are de-\nrived based on the given schema. We leave the\ndiscussion of two speciÔ¨Åc realizations of task-\nspeciÔ¨Åc prompts to the later part of this sec-\ntion. SpeciÔ¨Åcally, all sub-sequences are concate-\nnated with special segment tokens, i.e., ‚Äú[user]\nU1 [system] A1 ... [system] At‚àí1 [user]\nUt [domain] X(dm) [slot] X(sn)‚Äù, as in-\nput to the encoder, where[user], [system],\n[domain],[slot] are special segment tokens\nfor indicating the start of a speciÔ¨Åc user utterance,\nsystem utterance, domain-related prompt, and slot-\nrelated prompt, respectively.\nGiven this prompt-augmented input, the bi-\ndirectional encoder then outputs\nHt = Encoder(Ct,X(dm),X(sn)), (1)\nwhere Ht ‚àà ‚ÑùL√ók is the hidden states of the en-\ncoder,Lis the input sequence length, andkis the\nencoder hidden size. Then, the decoder attends to\nthe encoder hidden states and decodes the corre-\nsponding slot valueBt(dm,sn):\nBt(dm,sn) =Decoder(Ht). (2)\nThe overall learning objective of this generation\nprocessing is maximizing the log-likelihood of\nBt(dm,sn) givenCt, X(dm) andX(sn), that is\n√â\n(m,n)\nlog P(Bt(dm,sn)√∞Ct,X(dm),X(sn)). (3)\nDuring inference, a greedy decoding procedure is\ndirectly applied, i.e., only the most likely token in\nthe given model vocabulary is predicted at each\ndecoding step.\nSchema-Based Prompt. The Ô¨Årst realization of\ntask-speciÔ¨Åc prompt considered in this paper is\nbased on the domain and slot names as deÔ¨Åned\nin the task-dependent schema. As shown in (b) of\nFigure 1, given the domain nametrainand the slot\nname day, the speciÔ¨Åc prompt is in the form of\n‚Äú[domain] train[slot] day‚Äù. DiÔ¨Äerent from\n(Lin et al., 2020a; Wu et al., 2019) where the task-\nspeciÔ¨Åc information is used in the decoder side,\nour symbol-based prompt as additional input to the\nbi-directional encoder can potentially achieve task-\naware contextualizations. Observing that users of-\nten revise/repair their earlier requests in dialogues,\nwe posit that the resulting encoded representations\ncan be more eÔ¨Äectively used by the decoder for\ngenerating corresponding slot values.\nNatural Language Augmented Prompt. One\nmain drawback of symbol-based prompt is that\n4941\nDataset MWOZ 2.2 MWOZ 2.1 M2M\n# Domains 8 8 2\n# Dialogues 10438 10438 3008\n# Total Turns 143004 143048 27120\nAvg. Turns per Dial. 13.70 13.70 9.01\nAvg. Toks per Turn 13.23 13.18 8.28\n# Cat. Slots 21 0 0\n# Non-Cat. Slots 40 37 12\nDomain Desc. Y N N\nSlot Desc. Y Y N\nValue Set Y N N\nTable 1: Experiment data summary. The numbers\nare computed on all splits of the datasets. MWOZ\nstandsforMultiWOZ. Cat. Slots andNon-Cat.\nSlots stand for categorical slots and non-categorical\nslots, respectively. The rows Domain Desc. and\nSlot Desc. indicate whether the corresponding\ndataset has natural language description for domains\nand slots, respectively. The rowValue Set incates\nwhether the corresponding dataset provides possible\nvalue set for categorical slots.\nthose domain/slot names contain limited informa-\ntionthatcanbeutilizedbypretrainedLMs. Inother\nwords, those symbols from the custom schema are\ntypically under-speciÔ¨Åed and unlikely to appear in\ncorpusforLMpretraining. Fortunately,documenta-\ntioniscommonlyavailableforreal-worlddatabases,\nand it is a rich resource for domain knowledge that\nallows dialogue systems to better understand the\nmeaningsoftheabbreviateddomainandslotnames.\nThe documentation includes but is not limited to\ndomain/slot descriptions and the list of possible\nvalues for categorical slots. In this work, we ex-\nperiment with a simple approach that augments the\ninput by incorporating the domain description after\nthe domain name and the slot description (with the\nsequence of values, if any) following the slot name,\nas illustrated in the system (c) in Figure 1.\n3.3 Backbone Sequence-to-sequence Model\nOur prompt-based DST model is initialized with\nweights from a pretrained LM in an encoder-\ndecoder fashion. In this paper, we use the Text-to-\nText Transformer (T5) (RaÔ¨Äel et al., 2020) as our\nbackbone model. T5 is an encoder-decoder Trans-\nformerwithrelativepositionencodings(Shawetal.,\n2018). We refer interested readers to the original\npaper for more details.\n4 Experiments\n4.1 Datasets\nTable 1 summarizes the statistics of the datasets\nused in our experiments.\nMultiWOZ(Budzianowski et al., 2018) is a multi-\ndomaintask-orienteddialoguedatasetthatcontains\nover 10K dialogues across 8 domains. It is a col-\nlection of human-human written conversations and\nhas been one of the most popular benchmarks in\ntheDSTliterature. Sinceitsinitialrelease,manyer-\nroneous annotations and user utterances have been\nidentiÔ¨Åed and Ô¨Åxed in subsequent versions, i.e.,\nMultiWOZ 2.1 (Eric et al., 2019) and MultiWOZ\n2.2 (Zang et al., 2020). In addition, MultiWOZ\n2.1 provides 2-3 descriptions for every slot in the\ndataset. We randomly sample one of them and use\nthe same descriptions for every experiment. The\noriginal dataset does not have domain descriptions\nand possible values so these are omitted in the cor-\nresponding experiments. MultiWOZ 2.2 further\nprovides descriptions of domain and slot as well as\npossible values for categorical slots.\nMachines Talking To Machines (M2M)(Shah\net al., 2018) is a framework that combines simula-\ntion and online crowdsourcing. Templates of each\ndialogue are Ô¨Årst generated and then online work-\ners rewrite the conversations to make them human-\nreadable while preserving the meaning. It provides\n3,000 dialogues spanning 2 domains. The restau-\nrant domain is denoted asSim-R and the movie\ndomainisdenotedas Sim-M.Sincetherearenode-\nscriptions provided in the corpus, we take existing\ndescriptions from other corpora that have the same\nslots. SpeciÔ¨Åcally, descriptions for the restaurant\ndomain are taken from MultiWOZ 2.2, whereas\ndescriptions for the movie domain are taken from\nSGD (Rastogi et al., 2020b). All slots in M2M are\ncovered. Since all slots are non-categorical, the\ndescriptions do not include the possible values.\nEvaluation Metric. The standard joint goal accu-\nracy(JGA)isusedastheevaluationmetric. Ittreats\na prediction as correct only if for every domain all\nslots exactly match the ground-truth values. For\nMultiWOZ 2.1 and 2.2, we use the oÔ¨Écial evalua-\ntionscriptfromtheDSTC8challenge(Rastogietal.,\n2020a).1 For M2M, we adopt the above evaluation\nscripts with simple modiÔ¨Åcations.\n1https://github.com/google-research/\ngoogle-research/tree/master/schema_\nguided_dst#evaluation-on-multiwoz-21\n4942\nModels Pretrained-Model/ # Para. JGA\nTRADE N 48.6\nDS-DST BERT-base / (110M) 51.7\nSeq2Seq-DU BERT-base / (110M) 54.4\nSequential T5-small / (60M) 48.9\nSequential T5-base / (220M) 51.2\nIndependent T5-small / (60M) 55.2\nw. desc T5-small / (60M) 56.3\nIndependent T5-base / (220M) 56.7\nw. desc T5-base / (220M) 57.6\nTable2: ResultsonMultiWOZ2.2. Allnumbersarere-\nported in joint goal accuracy (JGA)(%).w. desc means\nthemodelistrainedwiththedescription. #Para. stands\nfor the number of model parameters.\n4.2 MultiWOZ 2.2: Fully Annotated Natural\nLanguage Augmented Prompt\nWe present the evaluation results on MultiWOZ\n2.2 in Table 2. The following baseline models are\nconsidered: TRADE (Wu et al., 2019), DS-DST\n(Zhang et al., 2019) and Seq2Seq-DU (Feng et al.,\n2020). Similar to ours, the decoding strategy of\nTRADE is independent. However, the sum of do-\nmain and slot embeddings are the Ô¨Årst input of the\ndecoder, which makes their dialogue history rep-\nresentation not task-aware contextualized. The se-\nquential decoding strategy is worse than the inde-\npendent decoding strategy by over 5% with both\nT5-smallandT5-base. EvenwithT5-small(almost\nhalf the model size of BERT-base which is used\nin most previous benchmark models), our system\nachieves the SOTA performance using the inde-\npendent decoding. As expected, T5-base systems\noutperform T5-small systems. With the augmenta-\ntion of descriptions, we improve the overall JGA\nby over 1% in both T5-small and T5-base.\n4.3 MultiWOZ 2.1: Partially Annotated\nNatural Language Augmented Prompt\nDiÔ¨Äerent from MultiWOZ 2.2 studied in the previ-\nous section, MultiWOZ 2.1 only contains natural\nlanguage descriptions for slots but not domains. In\naddition,thereisnopossibleslotvalueinformation.\nThe evaluation results on MultiWOZ 2.1 are\nshown in Table 3, where we compare with TRADE\n(Wu et al., 2019), MinTL (Lin et al., 2020a),\nSST (Chen et al., 2020), TripPy (Heck et al.,\n2020), Simple-TOD (Hosseini-Asl et al., 2020),\nSOLOIST(Pengetal.,2020a)andTripPy+SCORE\n(Yu et al., 2020). Note that both SOLOIST and\nTripPY+SCORE use external dialogue datasets to\nModels Pretrained-Model / # Para. JGA\nTRADE N 45.60\nMinTL T5-small / (60M) 50.95\nMinTL BART-large / (406M) 53.62\nSST N 55.23\nTripPy BERT-base / (110M) 55.29\nSimple-TOD2 GPT2 / (117M) 55.72\n*SOLOIST GPT-2 / (117M) 56.85\n*TripPy + SCOREROBERTA-large / (355M)60.48\nIndependent T5-small / (60M) 55.37\nw. desc T5-small / (60M) 56.12\nIndependent T5-base / (220M) 56.39\nw. desc T5-base / (220M) 56.66\nTable3: ResultsonMultiWOZ2.1. Allnumbersarere-\nported in joint goal accuracy (JGA)(%).w. desc means\nthemodelistrainedwiththedescription. *meansextra\ndialoguedataisusedtoÔ¨Ånetunethelanguagemodel. #\nPara. stands for the number of model parameters.\nÔ¨Ånetune their models.\nAs expected, we observe that T5-base models\nperform consistently better than T5-small models.\nMoreover,usingdescriptionsconsistentlyimproves\nthe performance of both models. All our models\noutperform baselines that do not use extra dialogue\ndata. ItisworthnotingthatcomparingwithMinTL\n(T5-small), our model is better by over 4% even\nwithout descriptions. Further, our T5-small system\nis even better than MinTL built on BART-LARGE\n(Lewis et al., 2020) which has substantially more\nparameters. Similar to ours, MinTL leverages a\nsequence-to-sequence LM. One diÔ¨Äerence is that\ntheir domain information is fed only to the decoder\nwhile our approaches enables task-aware contextu-\nalization by prompting the LMs with domain and\nslot information on the encoder side. Another dif-\nference is that they jointly learn DST together with\ndialogueresponsegeneration,whichprovidesmore\nsupervision signals. Therefore, the better perfor-\nmance of our systems implies that schema-driven\nprompting is eÔ¨Äective.\nLastly, compared with MultiWOZ 2.2, the per-\nformance gain brought by augmenting natural lan-\nguage descriptions is less pronounced which is\nlikely caused by the reduced information available\nin MultiWOZ 2.1 descriptions.\n4.4 M2M: Borrowed Natural Language\nAugmented Prompt\nTable 4 shows the evaluation results on M2M. In\nthis case, all natural language descriptions are di-\nrectly borrowed from dialogue datasets that are an-\n4943\nModels Sim-M Sim-R Sim-M+R\n(Rastogi et al., 2017) 96.8 94.4 ‚Äì\n(Rastogi et al., 2018) 50.4 87.1 73.8\n(Chao and Lane, 2019) 80.1 89.6 ‚Äì\n(Heck et al., 2020) 83.5 90.0 ‚Äì\nIndependent 83.3 89.6 88.0\nw. desc 81.0 90.6 86.4\nTable 4: Results on M2M. All numbers are reported\nin joint goal accuracy(JGA)(%). (Rastogi et al., 2017)\nshould be considered as a kind of oracle upper bound\nperformancebecausethetargetslotvalueisguaranteed\nto be in the candidate list and consider by the model.\nnotatedinadiÔ¨Äerentmanner. WeachievetheSOTA\nperformance on Sim-R and Sim-M+R while being\ncomparable on Sim-M. The improvements of de-\nscriptionsareonlyevidentontherestaurantdomain.\nThe lack of improvement from slot descriptions for\nthe movie domain may be because the slot descrip-\ntions do not add much beyond the slot name (com-\npared to \"category\" for the restaurant domain) or\nthatithasslotsthatgeneralizebetteracrossdomains\n(e.g. date, time, number of people).\n5 Analysis\n5.1 Breakdown Evaluation for MultiWOZ\nIn Table 5, we follow the categorization provided\nin(Zangetal.,2020)andshowthebreakdowneval-\nuation of categorical and non-categorical slots on\nMultiWOZ2.2. Aswecansee,thebreakdownaccu-\nracyscoresforbothcategoricalandnon-categorical\nslotsare prettyconsistent withthe overall JGA. For\nboth T5-small and T5-base models, models with\nsequential decoding perform worse than the corre-\nsponding models with independent decoding for\nboth categorical and non-categorical slots. In par-\nticular, the independent decoding models achieve\nmore pronounced improvement in categorical slots\nindicatingthatthetask-speciÔ¨Åcpromptisveryhelp-\nful for guiding the decoder to predict valid values.\nWhencomparingmodelsusingnaturallanguagede-\nscription with those not, we observe performance\ngains for both types of slots for T-base but only\nnon-categorical slots for T5-small. It is likely that\nthe smaller size of T5 has limited representation\ncapability to eÔ¨Äectively utilize the additional tex-\ntual description information regarding types and\npossible values.\nModels JGA CAT NON-CAT\nSequential (T5-small) 48.9 61.3 69.0\nSequential (T5-base) 51.2 62.9 70.9\nIndependent (T5-small) 55.2 71.4 75.2\nw. desc 56.3 71.1 76.2\nw. onlyslot desc 55.2 70.4 75.8\nw. onlydomain desc 54.3 70.1 75.4\nw. onlyslot + domain desc 55.9 71.2 76\nIndependent (T5-base) 56.7 71.6 76.3\nw. desc 57.6 72.4 76.8\nTable 5: Slot type breakdown results on the test set of\nMultiWOZ 2.2. All numbers are reported in joint goal\naccuracy(JGA) (%). CAT and NON-CAT correspond\nto categorical slotsJGA and non-categorical slots JGA,\nrespectively.w. descindicatesthatthemodelistrained\nwith the full description.\n5.2 Ablation Study on Schema Descriptions\nTo understand what parts of the schema descrip-\ntions are most important, we experiment with three\nkinds of description combinations on MultiWOZ\n2.2 using the T5-small conÔ¨Åguration:(i) excludes\nthe list of possible values for categorical slots(ii)\nexcludes slot descriptions(iii)excludes domain de-\nscriptions. For(i), there is an 0.4% point drop in\nJGA,validatingthatvaluesetscansuccessfullycon-\nstrain the model output, as we illustrate in Table 6.\nFor (ii), there is a 0.8% point drop in JGA. And for\n(iii), there is a 0.1% point drop in JGA. This shows\nthat slot descriptions are the mostimportant part of\nthe schema prompts and domain descriptions are\nrelatively less eÔ¨Äective. This is probably due to the\nfact that there are 61 slots in MultiWOZ 2.2 but\nonly 8 domains. Also, the domain names are all\nself-contained single words.\n5.3 The EÔ¨Äectiveness of Natural Language\nAugmented Prompt\nIn order to understand the beneÔ¨Åt of natural lan-\nguage augmented prompt, we focus on analyzing\nthe examples where the description augmented\nmodel correctly tracks the dialogue state while the\nunaugmented one fails. Based on our analysis of\nT5-base model on MultiWOZ 2.2, the most com-\nmon errors are either misses of gold slots or over-\npredictions of irrelevant slots (82.8% of all errors).\nThe remaining error cases are correct slot predic-\ntions with wrong slot values (17.2%).\nWe provide representative examples for which\nthe description augmented system correctly tracks\nthe dialogue states but not the unaugmented one\nin Table 6. In the Ô¨Årst example, the phrases in\n4944\nDatabase Train Slot Descriptions || Possible Values\narriveby arrival time of the train\ndestination destination of the train|| Birmingham New Street,London Kings Cross, ..., Stevenage\nDialogue History ... [SYS]The earliest being 19:09 and arriving by 20:54. Would that work for you?\n[USR] Yes, I think the20:54 arrival timeshould work.\nno desc. (train, day, friday) (train, departure, leicester) (train, destination, cambridge) (train, leaveat,\n19:00)\ndesc. (train, arriveby, 20:54)(train, day, friday) (train, departure, leicester) (train, destination, cam-\nbridge) (train, leaveat, 19:00)\nDialogue History [USER]I need to Ô¨Ånd a train going to Leicester that arrives by4:45 PM. Do you know of one?\nno desc. (train, arriveby, 04:45) (train, destination, leicester)\ndesc. (train, arriveby, 16:45)(train, destination, leicester)\nDialogue History [USER] Can you help me Ô¨Ånd a train for Sunday. I would like to visitLondon Kings Street.\nno desc. (train, destination, London Kings Street) (train, day, Sunday)\ndesc. (train, destination, London Kings Cross)(train, day, Sunday)\nTable 6: Examples fortrain domain dialogues where the description-augmented (‚Äúdesc.‚Äù) model make the\ncorrect state predictions but the unaugmented models (‚Äúno desc.‚Äù) fails. The correctly predicted triplets are in\nbold.\n53.33%: Annotation Errors\nDialogue History ...[SYSTEM]Outofthe21restaurantchoices,oneisthe YippeeNoodleBarwhichismoderately\npriced in the centre of town. Would you like to make a reservation?\n[USER]That sounds great, what is the postcode?\nGold ()\ndesc. Prediction (restaurant, area, centre) (restaurant, pricerange, moderate) (restaurant, name, yippee noodle\nbar\n20.00%: Unable to Capture System Information\nDialogue History ... [SYSTEM]There is TR6679.It leaves at 19:35 and arrives at 19:52. Is that good for you?\n[USER] Sounds good. May I have the travel time and ticket price, please?\nGold (train, arriveby, 19:52) (train, leaveat, 19:35)\ndesc. Prediction ()\n16.66%: Unable to Mention Slot Provided by User\nDialogue History ... [USER]Do you happen to know if there is a nightclub in the centre?\n[SYSTEM]Yes, we have FIVE nightclubs in the centre of town. Is there a particular one you‚Äôre\nlooking for?\n[USER] I don‚Äôt care which one you recommend, but can you tell me the entrance fee and\naddress?\nGold (attraction, area, centre) (attraction, type, nightclub)(attraction, name, dontcare)\ndesc. Prediction (attraction, area, centre) (attraction, type, nightclub)\n10.00%: Incorrect Value Reference\nDialogue History [USER] Hi can you help me Ô¨Ånd a very nice Italian restaurant near the centre of cambridge?\n[SYSTEM]Please specify your price range.\n[USER] It does not matter.\nGold (restaurant, area, centre) (restaurant, food, italian)(restaurant, pricerange, dontcare)\ndesc. Prediction (restaurant, area, centre) (restaurant, food, italian)(restaurant, pricerange, expensive)\nTable 7: The most common error types of our best model(t5-basew/ desc.) and corresponding examples.\nthe dialogue history are partially matched to the\nslot description ofarrivebymaking it easier for the\ndescription-augmented system to detect the men-\ntion of the correct slot. For the second example,\nthe type information in the description implicitly\nguides the model to focus on time-related informa-\ntion leading the correct output of the normalized\ntimeexpression,16:45. Incontrast,themodelwith-\nout descriptions only generates the partial answer\n4:45, ignoring PM. Lastly, \"London Kings Street\"\nis a typographical error in this case. By utilizing\nthe provided possible values included in the slot\ndescriptions, the model is able to generate the cor-\nrect slot value without spelling error, demonstrat-\ning that the natural language augmented prompt\ncansuccessfullyconstrainthemodeloutputandpo-\ntentially provides robustness to the dialogue state\ntracking system.\n5.4 Error Analysis of Natural Language\nAugmented Prompt-based DST\nHere, we further carry out error analyses into the\nnaturallanguageaugmentedprompt-basedT5-base\n4945\nmodel on MultiWOZ 2.2. As shown in Table 7, we\nrandomlysample50turnsandcategorizetheminto\ndiÔ¨Äerent types. In summary, there are four types of\nerrors: (i) The most common error type is annota-\ntion error in which the model prediction is actually\ncorrect, which is similar to the Ô¨Åndings of (Zhou\nand Small, 2019).(ii)20% of the errors come from\nmodel failing to capture information provided by\nthe system.3 (iii) 16.66% of the errors are caused\nby the model misses of at least one gold slot.(iv)\n10% of the errors are correct slot predictions with\nthe wrong corresponding values. In general, most\nerrors are likely caused by the lack of explicit mod-\neling of user-system interactions.\n6 Conclusion\nIn this work, we propose a simple but eÔ¨Äective\ntask-oriented dialogue system based on large-scale\npretrained LM. We show that, by reformulating the\ndialogue state tracking task as prompting knowl-\nedge from LM, our model can beneÔ¨Åt from the\nknowledge-rich sequence to sequence T5 model.\nBased on our experiments, the proposed natural\nlanguage augmented prompt-based DST model\nachieve SOTA on MultiWOZ 2.2 and comparable\nperformance on MultiWOZ 2.1 and M2M to recent\nSOTA models. Moreover, ouranalyses provide evi-\ndencethatthenaturallanguagepromptiseÔ¨Äectively\nutilized to constrain the model prediction.\nAcknowledgements\nThisresearchwassupportedinpartbyagrantfrom\nAllstate. We would like to thank the reviewers for\ntheir constructive feedback.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R So,\nJamieHall,NoahFiedel,RomalThoppilan,ZiYang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\netal.2020. Towardsahuman-likeopen-domainchat-\nbot. arXiv preprint arXiv:2001.09977.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\n3There is label inconsistency in the MultiWoZ as pointed\nout by (Zhou and Small, 2019). If the user conÔ¨Årms the book-\ning or gives a positive response, then the dialogue states in the\nprevious system utterance should be grounded. However, this\nrule is not always followed in the dataset construction. So to\nsome extent, this type of error is inevitable.\nAdityaRamesh,DanielZiegler,JeÔ¨ÄreyWu,Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020a.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877‚Äì1901. Curran Associates,\nInc.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020b. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nPawe≈Ç Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, I√±igo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gasic. 2018. Multiwoz-a large-\nscale multi-domain wizard-of-oz dataset for task-\noriented dialogue modelling. InProceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5016‚Äì5026.\nGuan-LinChaoandIanLane.2019. Bert-dst: Scalable\nend-to-enddialoguestatetrackingwithbidirectional\nencoder representations from transformer.\nLu Chen, Boer Lv, Chi Wang, Su Zhu, Bowen Tan,\nand Kai Yu. 2020. Schema-guided multi-domain di-\naloguestatetrackingwithgraphattentionneuralnet-\nworks. In Proceedings of the AAAI Conference on\nArtiÔ¨Åcial Intelligence, volume 34, pages 7521‚Äì7528.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristinaToutanova.2019. Bert: Pre-trainingofdeep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorthAmericanChapteroftheAssociationforCom-\nputational Linguistics: Human Language Technolo-\ngies,Volume1(LongandShortPapers) ,pages4171‚Äì\n4186.\nMihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi,\nSanchit Agarwal, Shuyag Gao, and Dilek Hakkani-\nTur. 2019. Multiwoz 2.1: Multi-domain dialogue\nstate corrections and state tracking baselines.arXiv\npreprint arXiv:1907.01669.\nYueFeng,YangWang,andHangLi.2020. Asequence-\nto-sequence approach to dialogue state tracking.\narXiv preprint arXiv:2011.09553.\nDonghoonHam,Jeong-GwanLee,YoungsooJang,and\nKee-Eung Kim. 2020. End-to-end neural pipeline\nfor goal-oriented dialogue systems using gpt-2. In\nProceedings of the 58th Annual Meeting of the As-\nsociationfor ComputationalLinguistics, pages583‚Äì\n592.\nMichael Heck, Carel van Niekerk, Nurul Lubis, Chris-\ntianGeishauser,Hsien-ChinLin,MarcoMoresi,and\nMilica Gasic. 2020. Trippy: A triple copy strategy\nfor value independent neural dialog state tracking.\nIn Proceedings of the 21th Annual Meeting of the\n4946\nSpecial Interest Group on Discourse and Dialogue,\npages 35‚Äì44.\nMatthew Henderson, I√±igo Casanueva, Nikola Mrk≈°i/uni0107,\nPei-HaoSu,Tsung-HsienWen,andIvanVuli/uni0107.2020.\nConvert: EÔ¨Écientandaccurateconversationalrepre-\nsentations from transformers. InProceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing: Findings, pages 2161‚Äì2174.\nMatthew Henderson, Blaise Thomson, and Jason D\nWilliams. 2014. The second dialog state tracking\nchallenge. In Proceedings of the 15th annual meet-\ningofthespecialinterestgroupondiscourseanddi-\nalogue (SIGDIAL), pages 263‚Äì272.\nEhsanHosseini-Asl,BryanMcCann,Chien-ShengWu,\nSemih Yavuz, and Richard Socher. 2020. A simple\nlanguage model for task-oriented dialogue. arXiv\npreprint arXiv:2005.00796.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423‚Äì438.\nSungdong Kim, Sohee Yang, Gyuwan Kim, and Sang-\nWoo Lee. 2020. EÔ¨Écient dialogue state tracking by\nselectively overwriting memory. InProceedings of\nthe58thAnnualMeetingoftheAssociationforCom-\nputational Linguistics, pages 567‚Äì582.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. InProceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871‚Äì7880, Online. Association\nfor Computational Linguistics.\nZhaojiang Lin, Andrea Madotto, Genta Indra Winata,\nand Pascale Fung. 2020a. MinTL: Minimalist trans-\nfer learning for task-oriented dialogue systems. In\nProceedings of the 2020 Conference on Empirical\nMethodsinNaturalLanguageProcessing(EMNLP) ,\npages3391‚Äì3405,Online.AssociationforComputa-\ntional Linguistics.\nZhaojiang Lin, Andrea Madotto, Genta Indra Winata,\nand Pascale Fung. 2020b. Mintl: Minimalist trans-\nfer learning for task-oriented dialogue systems. In\nProceedings of the 2020 Conference on Empirical\nMethodsinNaturalLanguageProcessing(EMNLP) ,\npages 3391‚Äì3405.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. InInternational Con-\nference on Learning Representations.\nShikib Mehri, Evgeniia Razumovskaia, Tiancheng\nZhao,andMaxineEskenazi.2019. Pretrainingmeth-\nods for dialog context representation learning. In\nProceedingsofthe57thAnnualMeetingoftheAsso-\nciation for Computational Linguistics, pages 3836‚Äì\n3845.\nBaolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayan-\ndeh, Lars Liden, and Jianfeng Gao. 2020a. Soloist:\nFew-shot task-oriented dialog with a single pre-\ntrained auto-regressive model.\nBaolin Peng, Chenguang Zhu, Chunyuan Li, Xiujun\nLi, Jinchao Li, Michael Zeng, and Jianfeng Gao.\n2020b. Few-shot natural language generation for\ntask-oriented dialog. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing: Findings, pages 172‚Äì182.\nAlec Radford, JeÔ¨Ärey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColinRaÔ¨Äel, NoamShazeer, AdamRoberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uniÔ¨Åed text-to-text trans-\nformer. Journal of Machine Learning Research,\n21:1‚Äì67.\nAbhinav Rastogi, Raghav Gupta, and Dilek Hakkani-\nTur. 2018. Multi-task learning for joint language\nunderstanding and dialogue state tracking. InPro-\nceedingsofthe19thAnnualSIGdialMeetingonDis-\ncourse and Dialogue, pages 376‚Äì384.\nAbhinav Rastogi, Dilek Hakkani-T√ºr, and Larry Heck.\n2017. Scalable multi-domain dialogue state track-\ning. In 2017 IEEE Automatic Speech Recognition\nand Understanding Workshop (ASRU), pages 561‚Äì\n568. IEEE.\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara,\nRaghavGupta,andPranavKhaitan.2020a. Schema-\nguided dialogue state tracking task at dstc8.arXiv\npreprint arXiv:2002.01359.\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara,\nRaghavGupta,andPranavKhaitan.2020b. Towards\nscalable multi-domain conversational agents: The\nschema-guided dialogue dataset. InProceedings of\nthe AAAI Conference on ArtiÔ¨Åcial Intelligence, vol-\nume 34, pages 8689‚Äì8696.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nKurt Shuster, Eric M Smith, et al. 2020. Recipes\nforbuildinganopen-domainchatbot. arXivpreprint\narXiv:2004.13637.\nPararth Shah, Dilek Hakkani-T√ºr, Gokhan T√ºr, Ab-\nhinav Rastogi, Ankur Bapna, Neha Nayak, and\nLarry Heck. 2018. Building a conversational agent\novernight with dialogue self-play. arXiv preprint\narXiv:1801.04871.\n4947\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464‚Äì468.\nChien-ShengWu,StevenCHHoi,RichardSocher,and\nCaimingXiong.2020. Tod-bert: Pre-trainednatural\nlanguage understanding for task-oriented dialogue.\nInProceedingsofthe2020ConferenceonEmpirical\nMethodsinNaturalLanguageProcessing(EMNLP) ,\npages 917‚Äì929.\nChien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-\nAsl, Caiming Xiong, Richard Socher, and Pascale\nFung. 2019. Transferable multi-domain state gener-\nator for task-oriented dialogue systems. InProceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 808‚Äì819.\nFanghua Ye, Jarana Manotumruksa, Qiang Zhang,\nShenghui Li, and Emine Yilmaz. 2021. Slot self-\nattentive dialogue state tracking. arXiv preprint\narXiv:2101.09374.\nTao Yu, Rui Zhang, Alex Polozov, Christopher Meek,\nand Ahmed Hassan Awadallah. 2020. Score: Pre-\ntraining for context representation in conversational\nsemantic parsing. In International Conference on\nLearning Representations.\nXiaoxue Zang, Abhinav Rastogi, and Jindong Chen.\n2020. Multiwoz 2.2: A dialogue dataset with addi-\ntionalannotationcorrectionsandstatetrackingbase-\nlines. In Proceedings of the 2nd Workshop on Nat-\nural Language Processing for Conversational AI,\npages 109‚Äì117.\nJian-Guo Zhang, Kazuma Hashimoto, Chien-Sheng\nWu, Yao Wan, Philip S Yu, Richard Socher, and\nCaiming Xiong. 2019. Find or classify? dual strat-\negy for slot-value predictions on multi-domain dia-\nlog state tracking.arXiv preprint arXiv:1910.03544.\nYizheZhang,SiqiSun,MichelGalley,Yen-ChunChen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and William B Dolan. 2020. Dialogpt: Large-\nscale generative pre-training for conversational re-\nsponse generation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations , pages 270‚Äì\n278.\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2018. Global-locally self-attentive dialogue state\ntracker. arXiv preprint arXiv:1805.09655.\nLi Zhou and Kevin Small. 2019. Multi-domain di-\nalogue state tracking as dynamic knowledge graph\nenhanced question answering. arXiv preprint\narXiv:1911.06192.\n4948\nSupplementary Material\nA Implementation Details\nThe backbone models we use for Ô¨Ånetuning are\nT5-small( 60M parameters) and T5-base( 220M\nparameters). We use the pretrained checkpoint\nfrom transformers library4. For T5-small, we\ntrain the model with a batch size 4, a learning rate\nof 5e-5 for 3 epochs. For T4-base, we train the\nmodel with a batch size of 64, a learning rate of\n5e‚àí4 for 2 epochs. Both models are trained using\nAdam(Loshchilov and Hutter, 2018). We don‚Äôt use\nany text or label normalization scripts like (Wu\net al., 2019; Hosseini-Asl et al., 2020).\nFor MultiWOZ 2.1 and 2.2, following many pre-\nvious works(Wu et al., 2019), sincepolice and hos-\npital domains only appear in the training set, we\nexclude them in all our experiments.\nB Descriptions\nWe show the descriptions of M2M and MultiWOZ\n2.1in Table8 and Table9\n4https://huggingface.co/t5-small ,https:\n//huggingface.co/t5-base\n4949\nTable 8: Domain and slot descriptions of M2M used in our experiments. The descriptions of the movie domain is\ntaken from (Rastogi et al., 2020a) and the descriptions of the restaurant domain is taken from (Zang et al., 2020).\nSim-M\nDomain Domain Description Slot Slot Description\nMovie A go-to provider for Ô¨Ånding movies, theatre_name the name of the theatre where the movie is playing\nsearching for show times and booking tickets movie name of the movie\ndate date of the show booking\ntime time of the show booking\nnum_people number of people to purchase tickets for\nSim-R\nDomain Domain Description Slot Slot Description\nRestaurant Ô¨Ånd places to dine and whet your appetite price_range price budget for the restaurant\nlocation the location or area of the restaurant\nrestaurant_name the name of the restaurant\ncategory the cuisine of the restaurant you are looking for\nnum_people how many people for the restaurant reservation\ndate date of the restaurant booking\ntime time of the restaurant booking\nTable 9: The randomly sampled descriptions of MultiWOZ 2.1 used in all our experiments.\nMultiWOZ 2.1\nDomain Slot Slot Description\ntaxi leaveat what time you want the taxi to leave your departure location by\ntaxi destination destination of taxi\ntaxi departure what place do you want to meet the taxi\ntaxi arriveby when you want the taxi to drop you oÔ¨Ä at your destination\nrestaurant book people number of people booking the restaurant\nrestaurant book day what day of the week to book the table at the restaurant\nrestaurant book time time of the restaurant booking\nrestaurant food food type for the restaurant\nrestaurant pricerange price budget for the restaurant\nrestaurant name name of the restaurant\nrestaurant area preferred location of restaurant\ntrain destination destination of the train\ntrain day what day you want to take the train\ntrain departure departure location of the train\ntrain arriveby what time you want the train to arrive at your destination station by\ntrain book people number of people booking for train\ntrain leaveat when you want to arrive at your destination by train\nhotel pricerange preferred cost of the hotel\nhotel type type of hotel building\nhotel parking parking facility at the hotel\nhotel book stay length of stay at the hotel\nhotel book day day of the hotel booking\nhotel book people how many people are staying at the hotel\nhotel area rough location of the hotel\nhotel stars rating of the hotel out of Ô¨Åve stars\nhotel internet whether the hotel has internet\nhotel name which hotel are you looking for\nattraction type type of attraction or point of interest\nattraction area area or place of the attraction\nattraction name name of the attraction",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8675842881202698
    },
    {
      "name": "Categorical variable",
      "score": 0.7050380706787109
    },
    {
      "name": "Schema (genetic algorithms)",
      "score": 0.692959189414978
    },
    {
      "name": "Language model",
      "score": 0.5329965353012085
    },
    {
      "name": "Natural language processing",
      "score": 0.504911482334137
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46794548630714417
    },
    {
      "name": "Task (project management)",
      "score": 0.4461730122566223
    },
    {
      "name": "Human‚Äìcomputer interaction",
      "score": 0.34782588481903076
    },
    {
      "name": "Machine learning",
      "score": 0.3211917281150818
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ]
}