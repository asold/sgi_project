{
  "title": "Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world",
  "url": "https://openalex.org/W4385570442",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2013710467",
      "name": "Sunayana Sitaram",
      "affiliations": [
        "Microsoft Research (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2162966668",
      "name": "Monojit Choudhury",
      "affiliations": [
        "Microsoft (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2940149276",
      "name": "Barun Patra",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2913883794",
      "name": "Vishrav Chaudhary",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2955136849",
      "name": "Kabir Ahuja",
      "affiliations": [
        "Microsoft Research (India)"
      ]
    },
    {
      "id": "https://openalex.org/A1965970590",
      "name": "Kalika Bali",
      "affiliations": [
        "Microsoft Research (India)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3126947648",
    "https://openalex.org/W3211686893",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4225606080",
    "https://openalex.org/W3101860695",
    "https://openalex.org/W4385570056",
    "https://openalex.org/W3100198908",
    "https://openalex.org/W4307225507",
    "https://openalex.org/W4307307956",
    "https://openalex.org/W3175746962",
    "https://openalex.org/W3207523779",
    "https://openalex.org/W4285241202",
    "https://openalex.org/W3171975879",
    "https://openalex.org/W4287890953",
    "https://openalex.org/W4287646898",
    "https://openalex.org/W4308759544",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W3175898847",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W3175327901",
    "https://openalex.org/W4285125194",
    "https://openalex.org/W4287855067",
    "https://openalex.org/W3034776473",
    "https://openalex.org/W4225691633",
    "https://openalex.org/W3135514117",
    "https://openalex.org/W3178522238",
    "https://openalex.org/W2950733326",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W4287854864",
    "https://openalex.org/W4307535987"
  ],
  "abstract": "Sunayana Sitaram, Monojit Choudhury, Barun Patra, Vishrav Chaudhary, Kabir Ahuja, Kalika Bali. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 6: Tutorial Abstracts, pages 21–26\nJuly 9, 2023 ©2023 Association for Computational Linguistics\nACL/EACL/EMNLP 2023 Tutorial Proposal\nEverything you need to know about Multilingual LLMs: Towards fair,\nperformant and reliable models for the languages of the world\nSunayana Sitaram\nMicrosoft Research India\nsunayana,sitaram@microsoft.com\nMonojit Choudhury\nMicrosoft Turing, India\nmonojitc@microsoft.com\nBarun Patra\nMicrosoft Turing, USA\nbapatra@microsoft.com\nVishrav Chaudhary\nMicrosoft Turing, USA\nvchaudhary@microsoft.com\nKabir Ahuja\nMicrosoft Research India\nt-kabirahuja@microsoft.com\nKalika Bali\nMicrosoft Research India\nkalikab@microsoft.com\n1 Tutorial content\nThis tutorial will describe various aspects of scaling\nup language technologies to many of the world’s\nlanguages by presenting the latest research in Mas-\nsively Multilingual Language Models (MMLMs).\nWe will cover topics such as data collection, train-\ning and fine-tuning of models, Responsible AI is-\nsues such as fairness, bias and toxicity, linguistic\ndiversity and evaluation in the context of MMLMs,\nspecifically focusing on issues in non-English and\nlow-resource languages. Further, we will also talk\nabout some of the real-world challenges in deploy-\ning these models in language communities in the\nfield. With the performance of MMLMs improving\nin the zero-shot setting for many languages, it is\nnow becoming feasible to use them for building lan-\nguage technologies in many languages of the world,\nand this tutorial will provide the computational lin-\nguistics community with unique insights from the\nlatest research in multilingual models. Although\npast tutorials have covered some of these topics\n(such as linguistic diversity, data and training of\nmodels), there has been a lot of interesting research\nin the recent past that the CL community will ben-\nefit from knowing about. Further, this will be the\nfirst tutorial (as per our knowledge) that will dis-\ncuss issues of deployment in language communities\nand Responsible AI in the context of multilingual\nmodels.\nThis tutorial will present a broad survey covering\nwork done by several research groups (as indicated\nin the references), including work done by the au-\nthors.\nType of the tutorial: cutting-edge\nTarget audience and pre-requisites: The target\naudience for this tutorial are researchers from in-\ndustry and academia who work on Large Language\nModels, and are interested in learning about the lat-\nest research in multilingual models to build systems\nfor non-English languages, low-resource languages\nand multilingual speakers. We will not be covering\nthe basics of LLMs, so we expect that the audience\nwill be familiar with (at least the English versions\nof) models such as BERT.\n1.1 Outline of the tutorial\nWe plan to have five talks of 30/40 minutes each,\nalong with a 10 minute introduction, with 10 min-\nutes for general discussion/spillover.\nIntroduction: We will start with a short intro-\nduction on MMLMs, describing the models that\nare available today and present the SOTA in model\nperformance on various tasks across different lan-\nguages.\nData and pre-training: The main goal of this\nsection would be to outline the techniques lever-\naged for creating a high quality corpus for pre-\ntraining strong MMLMs. We will cover the chal-\nlenges encountered in creating such a corpus as\nhighlighted in CC100 (Conneau et al., 2020), mC4\n(Xue et al., 2021), OSCAR (Ortiz Suárez et al.,\n2020), ROOTS (Laurençon et al., 2022) etc., and\nprovide an overview of the various stages of such\na dataset creation pipeline. Ensuring the quality\nof the training corpus is highly important as it is\ndirectly correlated to the performance of MMLMs\n(Kaplan et al., 2020). In addition to this, we will\nalso discuss the pre-training strategies and possi-\nble extensions for extending the recipe to multiple\nlanguages (Conneau and Lample, 2019; Artetxe\nand Schwenk, 2019) describing how scaling (both\non the data and model axis) can substantially help\nimprove model performance (Conneau et al., 2020;\n21\nXue et al., 2021), aiding in bridging the gap be-\ntween the English performance of a multilingual\nand an English only model, thereby reducing the\ncurse of Multilinguality.\nTraining paradigms and fine-tuning: We will\ndescribe different training paradigms (Eg: an Elec-\ntra based approach (Chi et al., 2022; He et al.,\n2021)) and how to leverage bitext data, discussing\nresults of using contrastive learning approaches\n(Chi et al., 2021) or extensions to Electra based\napproaches (Chi et al., 2022), as well as show-\ning the benefits of going beyond English centric\nbitexts (Patra et al., 2022). We will also discuss\nsome orthogonal approaches of training encoder-\ndecoder multilingual representation models (Liu\net al., 2020; Ma et al., 2021; ?), as well as compli-\nmentary techniques to build better encoder mod-\nels (Eg: Adapter based approaches (Pfeiffer et al.,\n2022)). We will also focus on different strate-\ngies for improving the fine-tuning performance of\nthese models. This includes techniques encour-\naging models to have more consistent predictions\nacross languages (Zheng et al., 2021), leveraging\nweight perturbations to avoid overfitting (Wu et al.,\n2022) or techniques to reduce the sharpness of loss\nminima for better generalization (Foret et al., 2021;\nBahri et al., 2022).\nPerformance evaluation and reliability: While\nthe state-of-the-art multilingual models support\naround 100 languages of the world, most existing\nmultilingual benchmarks contain evaluation data in\na handful of languages (Ahuja et al., 2022b). We\nwill discuss some potential approaches to scale up\nmultilingual evaluation like performance predic-\ntion (Lin et al., 2019; Xia et al., 2020; Ahuja et al.,\n2022c) and structure probing (Müller-Eberstein\net al., 2022; Clouâtre et al., 2022). We will also fo-\ncus on measuring the cost-performance trade-offs\nand sample efficiencies of fine-tuning MMLMs\nwith different sources of data (translation vs man-\nual collection)(Ahuja et al., 2022a). Further, we\nwill cover how to measure reliability in the con-\nfidence predictions of multilingual models under\na zero-shot and few-shot setup by studying their\ncalibration (Ahuja et al., 2022d).\nFATE issues: LLMs are known to pick up the\nbiases present in the datasets that are trained on. In\ncase of multilingual LLMs, apart from bias and fair-\nness issues at group and individual level, one also\nneed to address the issue of disparity of zero-shot\ntransfer accuracies across languages and varieties\n(Choudhury and Deshpande, 2021; Lauscher et al.,\n2020). Furthermore, there is little work done on\nthe interaction among the biases in corpora from\ndifferent languages, influence of grammatical gen-\nder (Cao and Daumé, 2021) and other syntactic and\nsemantic factors on measurement and mitigation of\nbiases, and socio-cultural aspects of biases (Sam-\nbasivan et al., 2021). In this section of the tutorial,\nwe will survey the work done so far in non-English\nFATE issues and present challenges that remain to\nbe addressed.\nDeploying to language communities : LLMs\ntoday are trained using billions of parameters, mak-\ning them infeasible to be used in low-memory foot-\nprint devices. Language communities (particularly\nthose that speak under-resourced languages) that\nmay benefit the most from Speech and NLP tech-\nnologies may not have good enough connectiv-\nity to be able to use models hosted on the cloud.\nThis necessitates the development or distillation\nof lightweight models for low-resource languages,\nand in this section, we will present research in this\ndirection (Diddee et al., 2022). We will study the\nstate of current LT to serve communities speak-\ning different languages for critical situations such\nas healthcare bots (Mondal et al., 2022). Further,\nthere are many social and cultural factors to be\ntaken into account while deploying MMLMs to\nlanguage communities, which we will also discuss\nin this section.\n1.2 Diversity considerations\nThe topic of the tutorial inherently encourages lin-\nguistic diversity. In terms of gender diversity, two\nof the tutorial presenters are female, while four are\nmale. In this tutorial, we will cover issues related to\nResponsible AI (fairness, toxicity) and deploying\nto under-resourced language communities which\nwill improve diversity considerations while build-\ning LLMs. The instructors are a mix of senior,\nmid-career and junior researchers.\n1.3 Reading list\nPlease check the references section for the reading\nlist.\n2 Instructor bios\nSunayana Sitaram is a Senior Researcher at Mi-\ncrosoft Research India, where she works on mul-\ntilingual speech and NLP. Her current research\ninterests include training and evaluation of Mas-\n22\nsively Multilingual Language Models and Respon-\nsible AI for NLP. Prior to coming to MSRI as a\nPost Doc, Sunayana completed her MS and PhD\nat the Language Technologies Institute, Carnegie\nMellon University in 2015. Sunayana’s research\nhas been published in top NLP and Speech con-\nferences including ACL, NAACL, EMNLP, Inter-\nspeech, ICASSP. She has organized special ses-\nsions and workshops on under-resourced languages,\ncode-switching, multilingual evaluation and speech\nfor social good. She has also led the creation of\nseveral benchmarks and datasets in code-switching,\nASR, NLI and TTS that have been used by research\ngroups all over the world.\nMonojit Choudhury is a Principal Applied Sci-\nentist at Microsoft Turing, prior to which he was\na Principal Researcher at Microsoft Research In-\ndia. He is also a Professor of Practice at Plak-\nsha University, and had held adjunct faculty posi-\ntions at Ashoka University, IIIT Hyderabad and IIT\nKharagpur. Over the past 15 years, Monojit has\nworked on several impactful projects on process-\ning of code-mixed text, evaluation and linguistic\nfairness of large language models, and social im-\npact through participatory design of technology for\nunder-resourced languages like Gondi, Mundari,\nIdu Mishmi and Swahili. Monojit has served as Se-\nnior Area Chair and Area chair in leading NLP and\nAI conferences including EMNLP, ACL, NAACL,\nIJCNLP and AAAI. He has organized several suc-\ncessful workshops in *ACL conferences (SUMEval\n2022, CALCS series, TextGraph series, etc.) and\nhas delivered a tutorial on Code-mixed text pro-\ncessing at EMNLP 2019. He is the general chair\nof the Panini Linguistics Olympiad and the found-\ning co-chair of Asia Pacific Linguistics Olympiad\n– programs to introduce bright young students to\nlinguistics and computational linguistics through\npuzzles. Dr. Choudhury holds PhD and B.Tech de-\ngrees in Computer Science and Engineering from\nIIT Kharagpur.\nVishrav Chaudhary is a Principal Researcher\nat Microsoft Turing where he works on scaling\nand building efficient Multilingual and Multimodal\nrepresentation and generation models. Prior to Mi-\ncrosoft, Vishrav was a Lead Researcher at FAIR\nand focused on several aspects of Machine Trans-\nlation, Quality Estimation and Cross-lingual un-\nderstanding. Over the past 10 years, Vishrav’s re-\nsearch work has been published in several leading\nNLP and AI conferences and journals including\nACL, EMNLP, NAACL, EACL, AACL, TACL,\nJMLR and AMTA. He has also organized several\nworkshops successfully including SUMEval 2022,\nAmericasNLP 2021, WMT 2021 etc. He has also\nserved as an Area Chair for EMNLP 2022. Vishrav\nhas also led creation of benchmarks and datasets\ntargeting 100+ languages which have been used to\ntrain state-of-the-art Cross Lingual Representation\nand Machine Translation models.\nBarun Patra is an Applied Scientist at Mi-\ncrosoft Turing. His research interest revolves\naround building better foundational models that\ncan help support numerous NLP tasks across dif-\nferent languages. Barun’s research work focuses\non improving the quality and efficiency of training\nthese large multilingual foundational models, help-\ning achieve state-of-the-art performance on cross-\nlingual NLP tasks.\nKabir Ahuja is a Research Fellow at Microsoft\nResearch India, where he works on building linguis-\ntically fair multilingual models covering different\naspects around their performance, calibration, eval-\nuation, interpretation, and data collection. He is\nalso interested in the analysis and interpretability\nof the computation mechanisms utilized by neural\nsequence models for solving different tasks.\nKalika Bali is a Principal Researcher at Mi-\ncrosoft Research India working in the areas of Ma-\nchine Learning, Natural Language Systems and\nApplications, as well as Technology for Emerg-\ning Markets. Her research interests lie broadly\nin the area of Speech and Language Technology\nespecially in the use of linguistic models for build-\ning technology that offers a more natural Human-\nComputer as well as Computer-Mediated interac-\ntions.\n3 Other\nEstimate of audience size: 50\nVenues: We would prefer ACL 2023 to be the\nvenue for the tutorial, but EMNLP and EACL are\nalso acceptable. We do not forsee any special re-\nquirements for technical equipment.\n3.1 Ethics statement\nThis tutorial will present current research on Mul-\ntilingual model training, evaluation, Responsible\nAI issues and deploying models in the field. Al-\nthough we aim to promote linguistic diversity by\ndiscussing issues pertaining to multilingual models\ntrained on around 100 languages, many languages\n23\nof the world are not supported by these models.\nFurther, the techniques that we will discuss mainly\napply to written languages, while unwritten lan-\nguages will be excluded from the tutorial.\nReferences\nKabir Ahuja, Monojit Choudhury, and Sandipan Dan-\ndapat. 2022a. On the economics of multilingual\nfew-shot learning: Modeling the cost-performance\ntrade-offs of machine translated and manual data. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1369–1384, Seattle, United States. Association\nfor Computational Linguistics.\nKabir Ahuja, Sandipan Dandapat, Sunayana Sitaram,\nand Monojit Choudhury. 2022b. Beyond static mod-\nels and test sets: Benchmarking the potential of pre-\ntrained models across tasks and languages. In Pro-\nceedings of NLP Power! The First Workshop on Ef-\nficient Benchmarking in NLP, pages 64–74, Dublin,\nIreland. Association for Computational Linguistics.\nKabir Ahuja, Shanu Kumar, Sandipan Dandapat, and\nMonojit Choudhury. 2022c. Multi task learning for\nzero shot performance prediction of multilingual\nmodels. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5454–5467, Dublin,\nIreland. Association for Computational Linguistics.\nKabir Ahuja, Sunayana Sitaram, Sandipan Dandapat,\nand Monojit Choudhury. 2022d. On the calibration\nof massively multilingual language models.\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transactions\nof the Association for Computational Linguistics ,\n7:597–610.\nDara Bahri, Hossein Mobahi, and Yi Tay. 2022.\nSharpness-aware minimization improves language\nmodel generalization. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7360–\n7371, Dublin, Ireland. Association for Computational\nLinguistics.\nYang Trista Cao and III Daumé, Hal. 2021. Toward\nGender-Inclusive Coreference Resolution: An Anal-\nysis of Gender and Bias Throughout the Machine\nLearning Lifecycle*. Computational Linguistics ,\n47(3):615–661.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham\nSinghal, Wenhui Wang, Xia Song, Xian-Ling Mao,\nHeyan Huang, and Ming Zhou. 2021. InfoXLM: An\ninformation-theoretic framework for cross-lingual\nlanguage model pre-training. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3576–3588, On-\nline. Association for Computational Linguistics.\nZewen Chi, Shaohan Huang, Li Dong, Shuming Ma,\nBo Zheng, Saksham Singhal, Payal Bajaj, Xia Song,\nXian-Ling Mao, Heyan Huang, and Furu Wei. 2022.\nXLM-E: Cross-lingual language model pre-training\nvia ELECTRA. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 6170–6182,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nMonojit Choudhury and Amit Deshpande. 2021. How\nlinguistically fair are multilingual pre-trained lan-\nguage models? Proceedings of the AAAI Conference\non Artificial Intelligence, 35(14):12710–12718.\nLouis Clouâtre, Prasanna Parthasarathi, Amal Zouaq,\nand Sarath Chandar. 2022. Detecting languages unin-\ntelligible to multilingual models through local struc-\nture probes.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nHarshita Diddee, Sandipan Dandapat, Monojit Choud-\nhury, Tanuja Ganu, and Kalika Bali. 2022. Too brit-\ntle to touch: Comparing the stability of quantization\nand distillation towards developing lightweight low-\nresource mt models.\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and\nBehnam Neyshabur. 2021. Sharpness-aware mini-\nmization for efficiently improving generalization. In\nInternational Conference on Learning Representa-\ntions.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. arXiv preprint arXiv:2111.09543.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. CoRR,\nabs/2001.08361.\nHugo Laurençon, Lucile Saulnier, Thomas Wang,\nChristopher Akiki, Albert Villanova del Moral,\nTeven Le Scao, Leandro V on Werra, Chenghao Mou,\nEduardo González Ponferrada, Huu Nguyen, Jörg\n24\nFrohberg, Mario Šaško, Quentin Lhoest, Angelina\nMcMillan-Major, Gérard Dupont, Stella Biderman,\nAnna Rogers, Loubna Ben allal, Francesco De Toni,\nGiada Pistilli, Olivier Nguyen, Somaieh Nikpoor,\nMaraim Masoud, Pierre Colombo, Javier de la Rosa,\nPaulo Villegas, Tristan Thrush, Shayne Longpre, Se-\nbastian Nagel, Leon Weber, Manuel Romero Muñoz,\nJian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid\nAlmubarak, Vu Minh Chien, Itziar Gonzalez-Dios,\nAitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz\nSuarez, Aaron Gokaslan, Shamik Bose, David Ife-\noluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas\nPai, Jenny Chim, Violette Lepercq, Suzana Ilic, Mar-\ngaret Mitchell, Sasha Luccioni, and Yacine Jernite.\n2022. The bigscience ROOTS corpus: A 1.6TB\ncomposite multilingual dataset. In Thirty-sixth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and\nGoran Glavaš. 2020. From zero to hero: On the\nlimitations of zero-shot language transfer with mul-\ntilingual Transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4483–4499, On-\nline. Association for Computational Linguistics.\nYu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li,\nYuyan Zhang, Mengzhou Xia, Shruti Rijhwani, Junx-\nian He, Zhisong Zhang, Xuezhe Ma, Antonios Anas-\ntasopoulos, Patrick Littell, and Graham Neubig. 2019.\nChoosing transfer languages for cross-lingual learn-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n3125–3135, Florence, Italy. Association for Compu-\ntational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nShuming Ma, Li Dong, Shaohan Huang, Dong-\ndong Zhang, Alexandre Muzio, Saksham Singhal,\nHany Hassan Awadalla, Xia Song, and Furu Wei.\n2021. Deltalm: Encoder-decoder pre-training for\nlanguage generation and translation by augmenting\npretrained multilingual encoders. arXiv preprint\narXiv:2106.13736.\nIshani Mondal, Kabir Ahuja, Mohit Jain, Jacki O’Neill,\nKalika Bali, and Monojit Choudhury. 2022. Global\nreadiness of language technology for healthcare:\nWhat would it take to combat the next pandemic?\nIn Proceedings of the 29th International Conference\non Computational Linguistics , pages 4320–4335,\nGyeongju, Republic of Korea. International Com-\nmittee on Computational Linguistics.\nMax Müller-Eberstein, Rob van der Goot, and Barbara\nPlank. 2022. Sort by structure: Language model\nranking as dependency probing. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1296–1307,\nSeattle, United States. Association for Computational\nLinguistics.\nPedro Javier Ortiz Suárez, Laurent Romary, and Benoît\nSagot. 2020. A monolingual approach to contextual-\nized word embeddings for mid-resource languages.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 1703–\n1714, Online. Association for Computational Linguis-\ntics.\nBarun Patra, Saksham Singhal, Shaohan Huang, Zewen\nChi, Li Dong, Furu Wei, Vishrav Chaudhary, and Xia\nSong. 2022. Beyond english-centric bitexts for better\nmultilingual language representation learning. arXiv\npreprint arXiv:2210.14867.\nJonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James\nCross, Sebastian Riedel, and Mikel Artetxe. 2022.\nLifting the curse of multilinguality by pre-training\nmodular transformers. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3479–3495, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nNithya Sambasivan, Erin Arnesen, Ben Hutchinson,\nTulsee Doshi, and Vinodkumar Prabhakaran. 2021.\nRe-imagining algorithmic fairness in india and be-\nyond. In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\nFAccT ’21, page 315–328, New York, NY , USA. As-\nsociation for Computing Machinery.\nChuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng\nHuang. 2022. NoisyTune: A little noise can help\nyou finetune pretrained language models better. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 680–685, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nMengzhou Xia, Antonios Anastasopoulos, Ruochen Xu,\nYiming Yang, and Graham Neubig. 2020. Predicting\nperformance for natural language processing tasks.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 8625–\n8646, Online. Association for Computational Lin-\nguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nBo Zheng, Li Dong, Shaohan Huang, Wenhui Wang,\nZewen Chi, Saksham Singhal, Wanxiang Che, Ting\n25\nLiu, Xia Song, and Furu Wei. 2021. Consistency reg-\nularization for cross-lingual fine-tuning. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3403–3417, Online.\nAssociation for Computational Linguistics.\n26",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6558041572570801
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4367421567440033
    },
    {
      "name": "World Wide Web",
      "score": 0.35651475191116333
    },
    {
      "name": "Programming language",
      "score": 0.33530938625335693
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210124949",
      "name": "Microsoft Research (India)",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I4210162141",
      "name": "Microsoft (India)",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I1290206253",
      "name": "Microsoft (United States)",
      "country": "US"
    }
  ],
  "cited_by": 7
}