{
    "title": "Auto-Sizing Neural Networks: With Applications to n-gram Language Models",
    "url": "https://openalex.org/W1928693685",
    "year": 2015,
    "authors": [
        {
            "id": "https://openalex.org/A2704763796",
            "name": "Kenton Murray",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1897503397",
            "name": "David Chiang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2047092297",
        "https://openalex.org/W2151687858",
        "https://openalex.org/W1508567213",
        "https://openalex.org/W3012264151",
        "https://openalex.org/W2124807415",
        "https://openalex.org/W932413789",
        "https://openalex.org/W2156297475",
        "https://openalex.org/W1985258458",
        "https://openalex.org/W1978259121",
        "https://openalex.org/W2120861206",
        "https://openalex.org/W2474824677",
        "https://openalex.org/W2089745520",
        "https://openalex.org/W2114766824",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2595715041",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2251682575",
        "https://openalex.org/W2093545205",
        "https://openalex.org/W1665214252",
        "https://openalex.org/W2913535645",
        "https://openalex.org/W2998704965",
        "https://openalex.org/W2164301055"
    ],
    "abstract": "Neural networks have been shown to improve performance across a range of natural-language tasks.However, designing and training them can be complicated.Frequently, researchers resort to repeated experimentation to pick optimal settings.In this paper, we address the issue of choosing the correct number of units in hidden layers.We introduce a method for automatically adjusting network size by pruning out hidden units through ∞,1 and 2,1 regularization.We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity.We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions.",
    "full_text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 908–916,\nLisbon, Portugal, 17-21 September 2015.c⃝2015 Association for Computational Linguistics.\nAuto-Sizing Neural Networks:\nWith Applications ton-gram Language Models\nKenton Murray and David Chiang\nDepartment of Computer Science and Engineering\nUniversity of Notre Dame\n{kmurray4,dchiang}@nd.edu\nAbstract\nNeural networks have been shown to\nimprove performance across a range of\nnatural-language tasks. However, design-\ning and training them can be complicated.\nFrequently, researchers resort to repeated\nexperimentation to pick optimal settings.\nIn this paper, we address the issue of\nchoosing the correct number of units in\nhidden layers. We introduce a method for\nautomatically adjusting network size by\npruning out hidden units through ℓ∞,1 and\nℓ2,1 regularization. We apply this method\nto language modeling and demonstrate its\nability to correctly choose the number of\nhidden units while maintaining perplexity.\nWe also include these models in a machine\ntranslation decoder and show that these\nsmaller neural models maintain the signif-\nicant improvements of their unpruned ver-\nsions.\n1 Introduction\nNeural networks have proven to be highly ef-\nfective at many tasks in natural language. For\nexample, neural language models and joint lan-\nguage/translation models improve machine trans-\nlation quality signiﬁcantly (Vaswani et al., 2013;\nDevlin et al., 2014). However, neural networks can\nbe complicated to design and train well. Many de-\ncisions need to be made, and performance can be\nhighly dependent on making them correctly. Yet\nthe optimal settings are non-obvious and can be\nlaborious to ﬁnd, often requiring an extensive grid\nsearch involving numerous experiments.\nIn this paper, we focus on the choice of the\nsizes of hidden layers. We introduce a method\nfor automatically pruning out hidden layer units,\nby adding a sparsity-inducing regularizer that en-\ncourages units to deactivate if not needed, so that\nthey can be removed from the network. Thus, af-\nter training with more units than necessary, a net-\nwork is produced that has hidden layers correctly\nsized, saving both time and memory when actually\nputting the network to use.\nUsing a neural n-gram language model (Bengio\net al., 2003), we are able to show that our novel\nauto-sizing method is able to learn models that are\nsmaller than models trained without the method,\nwhile maintaining nearly the same perplexity. The\nmethod has only a single hyperparameter to adjust\n(as opposed to adjusting the sizes of each of the\nhidden layers), and we ﬁnd that the same setting\nworks consistently well across different training\ndata sizes, vocabulary sizes, and n-gram sizes. In\naddition, we show that incorporating these mod-\nels into a machine translation decoder still results\nin large BLEU point improvements. The result is\nthat fewer experiments are needed to obtain mod-\nels that perform well and are correctly sized.\n2 Background\nLanguage models are often used in natural lan-\nguage processing tasks involving generation of\ntext. For instance, in machine translation, the lan-\nguage model helps to output ﬂuent translations,\nand in speech recognition, the language model\nhelps to disambiguate among possible utterances.\nCurrent language models are usually n-gram\nmodels, which look at the previous (n−1) words\nto predict the nth word in a sequence, based\non (smoothed) counts of n-grams collected from\ntraining data. These models are simple but very\neffective in improving the performance of natural\nlanguage systems.\nHowever,n-gram models suffer from some lim-\nitations, such as data sparsity and memory usage.\nAs an alternative, researchers have begun explor-\ning the use of neural networks for language mod-\neling. For modeling n-grams, the most common\napproach is the feedforward network of Bengio et\n908\nal. (2003), shown in Figure 1.\nEach node represents a unit or “neuron,” which\nhas a real valued activation. The units are orga-\nnized into real-vector valued layers. The activa-\ntions at each layer are computed as follows. (We\nassume n= 3; the generalization is easy.) The two\npreceding words, w1,w2, are mapped into lower-\ndimensional word embeddings,\nx1 = A:w1\nx2 = A:w2\nthen passed through two hidden layers,\ny = f(B1x1 + B2x2 + b)\nz = f(Cy + c)\nwhere f is an elementwise nonlinear activation\n(or transfer) function. Commonly used activation\nfunctions are the hyperbolic tangent, logistic func-\ntion, and rectiﬁed linear units, to name a few. Fi-\nnally, the result is mapped via a softmax to an out-\nput probability distribution,\nP(wn |w1 ···wn−1) ∝exp([Dz + d]wn).\nThe parameters of the model are A, B1, B2, b,\nC, c, D, and d, which are learned by minimizing\nthe negative log-likelihood of the the training data\nusing stochastic gradient descent (also known as\nbackpropagation) or variants.\nVaswani et al. (2013) showed that this model,\nwith some improvements, can be used effectively\nduring decoding in machine translation. In this pa-\nper, we use and extend their implementation.\n3 Methods\nOur method is focused on the challenge of choos-\ning the number of units in the hidden layers of a\nfeed-forward neural network. The networks used\nfor different tasks require different numbers of\nunits, and the layers in a single network also re-\nquire different numbers of units. Choosing too few\nunits can impair the performance of the network,\nand choosing too many units can lead to overﬁt-\nting. It can also slow down computations with the\nnetwork, which can be a major concern for many\napplications such as integrating neural language\nmodels into a machine translation decoder.\nOur method starts out with a large number of\nunits in each layer and then jointly trains the net-\nwork while pruning out individual units when pos-\nsible. The goal is to end up with a trained network\nwords\nw1,w2\ninput\nembeddings\nx1,x2\nhidden\ny\nhidden\nz\noutput\nP(w3 |w1w2)\nD\nC\nB1 B2\nA\nFigure 1: Neural probabilistic language model\n(Bengio et al., 2003), adapted from Vaswani et al.\n(2013).\nthat also has the optimal number of units in each\nlayer.\nWe do this by adding a regularizer to the ob-\njective function. For simplicity, consider a single\nlayer without bias, y = f(Wx). Let L(W) be\nthe negative log-likelihood of the model. Instead\nof minimizing L(W) alone, we want to mini-\nmize L(W) + λR(W), where R(W) is a con-\nvex regularizer. The ℓ1 norm, R(W) = ∥W∥1 =∑\ni,j |Wij|, is a common choice for pushing pa-\nrameters to zero, which can be useful for prevent-\ning overﬁtting and reducing model size. However,\nwe are interested not only in reducing the number\nof parameters but the number of units. To do this,\nwe need a different regularizer.\nWe assume activation functions that satisfy\nf(0) = 0 , such as the hyperbolic tangent or rec-\ntiﬁed linear unit (f(x) = max{0,x}). Then, if we\npush the incoming weights of a unityi to zero, that\nis, Wij = 0 for all j (as well as the bias, if any:\nbi = 0), then yi = f(0) = 0 is independent of the\nprevious layers and contributes nothing to subse-\nquent layers. So the unit can be removed without\naffecting the network at all. Therefore, we need a\nregularizer that pushes all the incoming connec-\ntion weights to a unit together towards zero.\nHere, we experiment with two, the ℓ2,1 norm\nand the ℓ∞,1 norm.1 The ℓ2,1 norm on a ma-\n1In the notation ℓp,q, the subscript p corresponds to the\nnorm over each group of parameters, and q corresponds to\nthe norm over the group norms. Contrary to more common\nusage, in this paper, the groups are rows, not columns.\n909\nx1\nx2\nx1\nx2\nℓ2 ℓ∞\nFigure 2: The (unsquared) ℓ2 norm and ℓ∞norm\nboth have sharp tips at the origin that encourage\nsparsity.\ntrix W is\nR(W) =\n∑\ni\n∥Wi:∥2 =\n∑\ni\n\n∑\nj\nW2\nij\n\n\n1\n2\n. (1)\n(If there are biases bi, they should be included as\nwell.) This puts equal pressure on each row, but\nwithin each row, the larger values contribute more,\nand therefore there is more pressure on larger val-\nues towards zero. The ℓ∞,1 norm is\nR(W) =\n∑\ni\n∥Wi:∥∞=\n∑\ni\nmax\nj\n|Wij|. (2)\nAgain, this puts equal pressure on each row, but\nwithin each row, only the maximum value (or val-\nues) matter, and therefore the pressure towards\nzero is entirely on the maximum value(s).\nFigure 2 visualizes the sparsity-inducing behav-\nior of the two regularizers on a single row. Both\nhave a sharp tip at the origin that encourages all\nthe parameters in a row to become exactly zero.\n4 Optimization\nHowever, this also means that sparsity-inducing\nregularizers are not differentiable at zero, mak-\ning gradient-based optimization methods trickier\nto apply. The methods we use are discussed in\ndetail elsewhere (Duchi et al., 2008; Duchi and\nSinger, 2009); in this section, we include a short\ndescription of these methods for completeness.\n4.1 Proximal gradient method\nMost work on learning with regularizers, includ-\ning this work, can be thought of as instances of\nthe proximal gradientmethod (Parikh and Boyd,\n2014). Our objective function can be split into two\nparts, a convex and differentiable part ( L) and a\nconvex but non-differentiable part ( λR). In prox-\nimal gradient descent, we alternate between im-\nproving L alone and λR alone. Let u be the pa-\nrameter values from the previous iteration. We\ncompute new parameter values w using:\nv ←u −η∇L(u) (3)\nw ←arg max\nw\n(1\n2η∥w −v∥2 + λR(w)\n)\n(4)\nand repeat until convergence. The ﬁrst update is\njust a standard gradient descent update on L; the\nsecond is known as the proximal operatorfor λR\nand in many cases has a closed-form solution. In\nthe rest of this section, we provide some justiﬁca-\ntion for this method, and in Sections 4.2 and 4.3\nwe show how to compute the proximal operator\nfor the ℓ2 and ℓ∞norms.\nWe can think of the gradient descent update (3)\non Las follows. Approximate Laround u by the\ntangent plane,\n¯L(v) = L(u) + ∇L(u)(v −u) (5)\nand move v to minimize ¯L, but don’t move it too\nfar from u; that is, minimize\nF(v) = 1\n2η∥v −u∥2 + ¯L(v).\nSetting partial derivatives to zero, we get\n∂F\n∂v = 1\nη(v −u) + ∇L(u) = 0\nv = u −η∇L(u).\nBy a similar strategy, we can derive the second\nstep (4). Again we want to move w to minimize\nthe objective function, but don’t want to move it\ntoo far from u; that is, we want to minimize:\nG(w) = 1\n2η∥w −u∥2 + ¯L(w) + λR(w).\nNote that we have not approximated R by a tan-\ngent plane. We can simplify this by substituting\nin (3). The ﬁrst term becomes\n1\n2η∥w −u∥2 = 1\n2η∥w −v −η∇L(u)∥2\n= 1\n2η∥w −v∥2 −∇L(u)(w −v)\n+ η\n2∥∇L(u)∥2\n910\nand the second term becomes\n¯L(w) = L(u) + ∇L(u)(w −u)\n= L(u) + ∇L(u)(w −v −η∇L(u)).\nThe ∇L(u)(w −v) terms cancel out, and we can\nignore terms not involving w, giving\nG(w) = 1\n2η∥w −v∥2 + λR(w) + const.\nwhich is minimized by the update (4). Thus, we\nhave split the optimization step into two easier\nsteps: ﬁrst, do the update for L (3), then do the\nupdate for λR (4). The latter can often be done\nexactly (without approximating R by a tangent\nplane). We show next how to do this for the ℓ2\nand ℓ∞norms.\n4.2 ℓ2 and ℓ2,1 regularization\nSince the ℓ2,1 norm on matrices (1) is separable\ninto the ℓ2 norm of each row, we can treat each\nrow separately. Thus, for simplicity, assume that\nwe have a single row and want to minimize\nG(w) = 1\n2η∥w −v∥2 + λ∥w∥+ const.\nThe minimum is either at w = 0 (the tip of\nthe cone) or where the partial derivatives are zero\n(Figure 3):\n∂G\n∂w = 1\nη(w −v) + λ w\n∥w∥= 0.\nClearly, w and v must have the same direction and\ndiffer only in magnitude, that is, w = α v\n∥v∥. Sub-\nstituting this into the above equation, we get the\nsolution\nα= ∥v∥−ηλ.\nTherefore the update is\nw = α v\n∥v∥\nα= max(0,∥v∥−ηλ).\n4.3 ℓ∞and ℓ∞,1 regularization\nAs above, since the ℓ∞,1 norm on matrices (2) is\nseparable into the ℓ∞ norm of each row, we can\ntreat each row separately; thus, we want to mini-\nmize\nG(w) = 1\n2η∥w −v∥2 + λmax\nj\n|xj|+ const.\n∥w∥>0 ∥w∥= 0\nFigure 3: Examples of the two possible cases for\nthe ℓ2 gradient update. Pointv is drawn with a hol-\nlow dot, and point w is drawn with a solid dot.\nbefore ℓ∞prox. op. ℓ1 projection\nFigure 4: The proximal operator for the ℓ∞norm\n(with strength ηλ) decreases the maximal compo-\nnents until the total decrease sums to ηλ. Projec-\ntion onto the ℓ1-ball (of radius ηλ) decreases each\ncomponent by an equal amount until they sum\nto ηλ.\nIntuitively, the solution can be characterized as:\nDecrease all of the maximal |xj|until the total de-\ncrease reaches ηλ or all the xj are zero. See Fig-\nure 4.\nIf we pre-sort the |xj|in nonincreasing order,\nit’s easy to see how to compute this: for ρ =\n1,...,n , see if there is a value ξ ≤xρ such that\ndecreasing all the x1,...,x ρ to ξamounts to a to-\ntal decrease of ηλ. The largest ρfor which this is\npossible gives the correct solution.\nBut this situation seems similar to another op-\ntimization problem, projection onto the ℓ1-ball,\nwhich Duchi et al. (2008) solve in linear time\nwithout pre-sorting. In fact, the two problems can\nbe solved by nearly identical algorithms, because\nthey are convex conjugates of each other (Duchi\nand Singer, 2009; Bach et al., 2012). Intuitively,\nthe ℓ1 projection of v is exactly what is cut out\nby the ℓ∞proximal operator, and vice versa (Fig-\nure 4).\nDuchi et al.’s algorithm modiﬁed for the present\nproblem is shown as Algorithm 1. It partitions the\nxj about a pivot element (line 6) and tests whether\nit and the elements to its left can be decreased to a\nvalue ξsuch that the total decrease is δ(line 8). If\nso, it recursively searches the right side; if not, the\n911\nleft side. At the conclusion of the algorithm, ρis\nset to the largest value that passes the test (line 13),\nand ﬁnally the new xj are computed (line 16) – the\nonly difference from Duchi et al.’s algorithm.\nThis algorithm is asymptotically faster than that\nof Quattoni et al. (2009). They reformulate ℓ∞,1\nregularization as a constrained optimization prob-\nlem (in which the ℓ∞,1 norm is bounded by µ) and\nprovide a solution inO(nlog n) time. The method\nshown here is simpler and faster because it can\nwork on each row separately.\nAlgorithm 1Linear-time algorithm for the proxi-\nmal operator of the ℓ∞norm.\n1: procedure UPDATE (w,δ)\n2: lo,hi ←1,n\n3: s←0\n4: while lo ≤hi do\n5: select md randomly from lo,..., hi\n6: ρ←PARTITION (w,lo,md,hi)\n7: ξ←1\nρ\n(\ns+ ∑ρ\ni=lo |xi|−δ\n)\n8: if ξ≤|xρ|then\n9: s←s+ ∑ρ\ni=lo |xi|\n10: lo ←ρ+ 1\n11: else\n12: hi ←ρ−1\n13: ρ←hi\n14: ξ←1\nρ (s−δ)\n15: for i←1,...,n do\n16: xi ←min(max(xi,−ξ),ξ)\n17: procedure PARTITION (w,lo,md,hi)\n18: swap xlo and xmd\n19: i←lo + 1\n20: for j ←lo + 1,..., hi do\n21: if xj ≥xlo then\n22: swap xi and xj\n23: i←i+ 1\n24: swap xlo and xi−1\n25: return i−1\n5 Experiments\nWe evaluate our model using the open-source\nNPLM toolkit released by Vaswani et al. (2013),\nextending it to use the additional regularizers as\ndescribed in this paper.2 We use a vocabulary size\nof 100k and word embeddings with 50 dimen-\nsions. We use two hidden layers of rectiﬁed linear\nunits (Nair and Hinton, 2010).\n2These extensions have been contributed to the NPLM\nproject.\nWe train neural language models (LMs) on two\nnatural language corpora, Europarl v7 English and\nthe AFP portion of English Gigaword 5. After tok-\nenization, Europarl has 56M tokens and Gigaword\nAFP has 870M tokens. For both corpora, we hold\nout a validation set of 5,000 tokens. We train each\nmodel for 10 iterations over the training data.\nOur experiments break down into three parts.\nFirst, we look at the impact of our pruning method\non perplexity of a held-out validation set, across a\nvariety of settings. Second, we take a closer look\nat how the model evolves through the training pro-\ncess. Finally, we explore the downstream impact\nof our method on a statistical phrase-based ma-\nchine translation system.\n5.1 Evaluating perplexity and network size\nWe ﬁrst look at the impact that the ℓ∞,1 regular-\nizer has on the perplexity of our validation set. The\nmain results are shown in Table 1. For λ ≤0.01,\nthe regularizer seems to have little impact: no hid-\nden units are pruned, and perplexity is also not af-\nfected. For λ= 1, on the other hand, most hidden\nunits are pruned – apparently too many, since per-\nplexity is worse. But for λ = 0.1, we see that we\nare able to prune out many hidden units: up to half\nof the ﬁrst layer, with little impact on perplexity.\nWe found this to be consistent across all our exper-\niments, varying n-gram size, initial hidden layer\nsize, and vocabulary size.\nTable 2 shows the same information for 5-gram\nmodels trained on the larger Gigaword AFP cor-\npus. These numbers look very similar to those on\nEuroparl: again λ = 0.1 works best, and, counter\nto expectation, even the ﬁnal number of units is\nsimilar.\nTable 3 shows the result of varying the vocabu-\nlary size: again λ= 0.1 works best, and, although\nit is not shown in the table, we also found that the\nﬁnal number of units did not depend strongly on\nthe vocabulary size.\nTable 4 shows results using the ℓ2,1 norm (Eu-\nroparl corpus, 5-grams, 100k vocabulary). Since\nthis is a different regularizer, there isn’t any rea-\nson to expect that λ behaves the same way, and\nindeed, a smaller value of λseems to work best.\n5.2 A closer look at training\nWe also studied the evolution of the network over\nthe training process to gain some insights into how\nthe method works. The ﬁrst question we want to\n912\n2-gram 3-gram 5-gram\nλ layer 1 layer 2 ppl layer 1 layer 2 ppl layer 1 layer 2 ppl\n0 1,000 50 103 1,000 50 66 1,000 50 55\n0.001 1,000 50 104 1,000 50 66 1,000 50 54\n0.01 1,000 50 104 1,000 50 63 1,000 50 55\n0.1 499 47 105 652 49 66 784 50 55\n1.0 50 24 111 128 32 76 144 29 68\nTable 1: Comparison ofℓ∞,1 regularization on 2-gram, 3-gram, and 5-gram neural language models. The\nnetwork initially started with 1,000 units in the ﬁrst hidden layer and 50 in the second. A regularization\nstrength of λ= 0.1 consistently is able to prune units while maintaining perplexity, even though the ﬁnal\nnumber of units varies considerably across models. The vocabulary size is 100k.\nλ layer 1 layer 2 perplexity\n0 1,000 50 100\n0.001 1,000 50 99\n0.01 1,000 50 101\n0.1 742 50 107\n1.0 24 17 173\nTable 2: Results from training a 5-gram neural LM\non the AFP portion of the Gigaword dataset. As\nwith the smaller Europarl corpus (Table 1), a reg-\nularization strength of λ = 0 .1 is able to prune\nunits while maintaining perplexity.\nvocabulary size\nλ 10k 25k 50k 100k\n0 47 60 54 55\n0.001 47 54 54 54\n0.01 47 58 55 55\n0.1 48 62 55 55\n1.0 61 64 65 68\nTable 3: A regularization strength of λ = 0 .1 is\nbest across different vocabulary sizes.\nλ layer 1 layer 2 perplexity\n0 1,000 50 100\n0.0001 1,000 50 54\n0.001 1,000 50 55\n0.01 616 50 57\n0.1 199 32 65\nTable 4: Results using ℓ2,1 regularization.\n0 2 4 6 8 100\n500\n1,000\nepoch\nnonzero units in hidden layer 1\n1000\n900\n800\n700\nFigure 5: Number of units in ﬁrst hidden layer over\ntime, with various starting sizes ( λ = 0.1). If we\nstart with too many units, we end up with the same\nnumber, although if we start with a smaller number\nof units, a few are still pruned away.\nanswer is whether the method is simply remov-\ning units, or converging on an optimal number of\nunits. Figure 5 suggests that it is a little of both:\nif we start with too many units (900 or 1000), the\nmethod converges to the same number regardless\nof how many extra units there were initially. But\nif we start with a smaller number of units, the\nmethod still prunes away about 50 units.\nNext, we look at the behavior over time of dif-\nferent regularization strengths λ. We found that\nnot only does λ = 1 prune out too many units, it\ndoes so at the very ﬁrst iteration (Figure 6, above),\nperhaps prematurely. By contrast, the λ = 0 .1\nrun prunes out units gradually. By plotting these\ncurves together with perplexity (Figure 6, below),\nwe can see that theλ= 0.1 run is ﬁtting the model\nand pruning it at the same time, which seems\npreferable to ﬁtting without any pruning ( λ =\n913\n0 2 4 6 8 100\n500\n1,000\nepoch\nnonzero units in hidden layer 1\nλ≤0.01\nλ= 0.1\nλ= 1\n0 2 4 6 8 100\n50\n100\nepoch\nperplexity λ= 0.01\nλ= 0.1\nλ= 1\nFigure 6: Above: Number of units in ﬁrst hid-\nden layer over time, for various regularization\nstrengths λ. A regularization strength of ≤0.01\ndoes not zero out any rows, while a strength of 1\nzeros out rows right away. Below: Perplexity over\ntime. The runs with λ ≤ 0.1 have very similar\nlearning curves, whereas λ= 1 is worse from the\nbeginning.\nneural LM\nλ none Europarl Gigaword AFP\n0 (none) 23.2 24.7 (+1.5) 25.2 (+2.0)\n0.1 24.6 (+1.4) 24.9 (+1.7)\nTable 5: The improvements in translation accuracy\ndue to the neural LM (shown in parentheses) are\naffected only slightly by ℓ∞,1 regularization. For\nthe Europarl LM, there is no statistically signiﬁ-\ncant difference, and for the Gigaword AFP LM, a\nstatistically signiﬁcant but small decrease of−0.3.\n0.01) or pruning ﬁrst and then ﬁtting (λ= 1).\nWe can also visualize the weight matrix itself\nover time (Figure 7), for λ = 0 .1. It is striking\nthat although this setting ﬁts the model and prunes\nit at the same time, as argued above, by the ﬁrst\niteration it already seems to have decided roughly\nhow many units it will eventually prune.\n5.3 Evaluating on machine translation\nWe also looked at the impact of our method on\nstatistical machine translation systems. We used\nthe Moses toolkit (Koehn et al., 2007) to build a\nphrase based machine translation system with a\ntraditional 5-gram LM trained on the target side\nof our bitext. We augmented this system with neu-\nral LMs trained on the Europarl data and the Gi-\ngaword AFP data. Based on the results from the\nperplexity experiments, we looked at models both\nbuilt with a λ= 0.1 regularizer, and without regu-\nlarization (λ= 0).\nWe built our system using the newscommentary\ndataset v8. We tuned our model using newstest13\nand evaluated using newstest14. After standard\ncleaning and tokenization, there were 155k paral-\nlel sentences in the newscommentary dataset, and\n3,000 sentences each for the tuning and test sets.\nTable 5 shows that the addition of a neural\nLM helps substantially over the baseline, with im-\nprovements of up to 2 BLEU. Using the Europarl\nmodel, the BLEU scores obtained without and\nwith regularization were not signiﬁcantly differ-\nent (p≥0.05), consistent with the negligible per-\nplexity difference between these models. On the\nGigaword AFP model, regularization did decrease\nthe BLEU score by 0.3, consistent with the small\nperplexity increase of the regularized model. The\ndecrease is statistically signiﬁcant, but small com-\npared with the overall beneﬁt of adding a neu-\nral LM.\n914\n1 iteration 5 iterations 10 iterations\nFigure 7: Evolution of the ﬁrst hidden layer weight matrix after 1, 5, and 10 iterations (with rows sorted\nby ℓ∞norm). A nonlinear color scale is used to show small values more clearly. The four vertical blocks\ncorrespond to the four context words. The light bar at the bottom is the rows that are close to zero, and\nthe white bar is the rows that are exactly zero.\n6 Related Work\nResearchers have been exploring the use of neu-\nral networks for language modeling for a long\ntime. Schmidhuber and Heil (1996) proposed a\ncharacter n-gram model using neural networks\nwhich they used for text compression. Xu and\nRudnicky (2000) proposed a word-based proba-\nbility model using a softmax output layer trained\nusing cross-entropy, but only for bigrams. Bengio\net al. (2003) deﬁned a probabilistic word n-gram\nmodel and demonstrated improvements over con-\nventional smoothed language models. Mnih and\nTeh (2012) sped up training of log-bilinear lan-\nguage models through the use of noise-contrastive\nestimation (NCE). Vaswani et al. (2013) also\nused NCE to train the architecture of Bengio et\nal. (2003), and were able to integrate a large-\nvocabulary language model directly into a ma-\nchine translation decoder. Baltescu et al. (2014)\ndescribe a similar model, with extensions like a\nhierarchical softmax (based on Brown clustering)\nand direct n-gram features.\nBeyond feed-forward neural network lan-\nguage models, researchers have explored using\nmore complicated neural network architectures.\nRNNLM is an open-source implementation of a\nlanguage model using recurrent neural networks\n(RNN) where connections between units can form\ndirected cycles (Mikolov et al., 2011). Sunder-\nmeyer et al. (2015) use the long-short term mem-\nory (LSTM) neural architecture to show a per-\nplexity improvement over the RNNLM toolkit.\nIn future work, we plan on exploring how our\nmethod could improve these more complicated\nneural models as well.\nAutomatically limiting the size of neural net-\nworks is an old idea. The “Optimal Brain Dam-\nage” (OBD) technique (LeCun et al., 1989) com-\nputes a saliency based on the second derivative of\nthe objective function with respect to each parame-\nter. The parameters are then sorted by saliency, and\nthe lowest-saliency parameters are pruned. The\npruning process is separate from the training pro-\ncess, whereas regularization performs training and\npruning simultaneously. Regularization in neural\nnetworks is also an old idea; for example, Now-\nland and Hinton (1992) mention both ℓ2\n2 and ℓ0\nregularization. Our method develops on this idea\nby using a mixed norm to prune units, rather than\nparameters.\nSrivastava et al. introduce a method called\ndropout in which units are directly deactivated at\nrandom during training (Srivastava et al., 2014),\nwhich induces sparsity in the hidden unit activa-\ntions. However, at the end of training, all units\nare reactivated, as the goal of dropout is to re-\nduce overﬁtting, not to reduce network size. Thus,\ndropout and our method seem to be complemen-\ntary.\n7 Conclusion\nWe have presented a method for auto-sizing a neu-\nral network during training by removing units us-\ning a ℓ∞,1 regularizer. This regularizer drives a\nunit’s input weights as a group down to zero, al-\nlowing the unit to be pruned. We can thus prune\nunits out of our network during training with min-\nimal impact to held-out perplexity or downstream\nperformance of a machine translation system.\nOur results showed empirically that the choice\n915\nof a regularization coefﬁcient of 0.1 was robust to\ninitial conﬁguration parameters of initial network\nsize, vocabulary size, n-gram order, and training\ncorpus. Furthermore, imposing a single regularizer\non the objective function can tune all of the hidden\nlayers of a network with one setting. This reduces\nthe need to conduct expensive, multi-dimensional\ngrid searches in order to determine optimal sizes.\nWe have demonstrated the power and efﬁcacy\nof this method on a feed-forward neural network\nfor language modeling though experiments on per-\nplexity and machine translation. However, this\nmethod is general enough that it should be applica-\nble to other domains, both inside natural language\nprocessing and outside. As neural models become\nmore pervasive in natural language processing, the\nability to auto-size networks for fast experimen-\ntation and quick exploration will become increas-\ningly important.\nAcknowledgments\nWe would like to thank Tomer Levinboim, Anto-\nnios Anastasopoulos, and Ashish Vaswani for their\nhelpful discussions, as well as the reviewers for\ntheir assistance and feedback.\nReferences\nFrancis Bach, Rodolphe Jenatton, Julien Mairal, and\nGuillaume Obozinski. 2012. Optimization\nwith sparsity-inducing penalties. Foundations and\nTrends in Machine Learning, 4(1):1–106.\nPaul Baltescu, Phil Blunsom, and Hieu Hoang. 2014.\nOxLM: A neural language modelling framework for\nmachine translation. Prague Bulletin of Mathemati-\ncal Linguistics, 102(1):81–92.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A neural probabilistic\nlanguage model. J. Machine Learning Research,\n3:1137–1155.\nJacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas\nLamar, Richard Schwartz, and John Makhoul. 2014.\nFast and robust neural network joint models for sta-\ntistical machine translation. In Proc. ACL, pages\n1370–1380.\nJohn Duchi and Yoram Singer. 2009. Efﬁcient online\nand batch learning using forward backward splitting.\nJ. Machine Learning Research, 10:2899–2934.\nJohn Duchi, Shai Shalev-Shwartz, Yoram Singer, and\nTushar Chandra. 2008. Efﬁcient projections onto\nthe ℓ1-ball for learning in high dimensions. In Proc.\nICML, pages 272–279.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ond ˇrej Bojar, Alexan-\ndra Constantin, and Evan Herbst. 2007. Moses:\nOpen source toolkit for statistical machine transla-\ntion. In Proc. ACL, Interactive Poster and Demon-\nstration Sessions, pages 177–180.\nYann LeCun, John S. Denker, Sara A. Solla, Richard E.\nHoward, and Lawrence D. Jackel. 1989. Optimal\nbrain damage. In Proc. NIPS, volume 2, pages 598–\n605.\nTomas Mikolov, Stefan Kombrink, Anoop Deoras,\nLukar Burget, and Jan Cernocky. 2011. RNNLM -\nrecurrent neural network language modeling toolkit.\nIn Proc. ASRU, pages 196–201.\nAndriy Mnih and Yee Whye Teh. 2012. A fast and\nsimple algorithm for training neural probabilistic\nlanguage models. In Proc. ICML, pages 1751–1758.\nVinod Nair and Geoffrey E Hinton. 2010. Recti-\nﬁed linear units improve Restricted Boltzmann Ma-\nchines. In Proc. ICML, pages 807–814.\nSteven J. Nowland and Geoffrey E. Hinton. 1992.\nSimplifying neural networks by soft weight-sharing.\nNeural Computation, 4:473–493.\nNeal Parikh and Stephen Boyd. 2014. Proximal al-\ngorithms. Foundations and Trends in Optimization,\n1(3):127–239.\nAriadna Quattoni, Xavier Carreras, Michael Collins,\nand Trevor Darrell. 2009. An efﬁcient projection\nfor l1,∞ regularization. In Proc. ICML, pages 857–\n864.\nJurgen Schmidhuber and Stefan Heil. 1996. Sequen-\ntial neural text compression. IEEE Transactions on\nNeural Networks, 7:142–146.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. J. Machine Learning Research,\n15(1):1929–1958.\nMartin Sundermeyer, Hermann Ney, and Ralf Schl¨uter.\n2015. From feedforward to recurrent LSTM neu-\nral networks for language modeling. Trans. Audio,\nSpeech, and Language, 23(3):517–529.\nAshish Vaswani, Yinggong Zhao, Victoria Fossum, and\nDavid Chiang. 2013. Decoding with large-scale\nneural language models improves translation. In\nProc. EMNLP, pages 1387–1392.\nWei Xu and Alexander I. Rudnicky. 2000. Can ar-\ntiﬁcial neural networks learn language models? In\nProc. International Conference on Statistical Lan-\nguage Processing, pages M1–13.\n916"
}