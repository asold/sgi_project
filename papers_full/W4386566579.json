{
  "title": "A Survey on Recent Advances in Keyphrase Extraction from Pre-trained Language Models",
  "url": "https://openalex.org/W4386566579",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2099538932",
      "name": "Mingyang Song",
      "affiliations": [
        "Beijing Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2100814453",
      "name": "Yi Feng",
      "affiliations": [
        "Beijing Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2096438337",
      "name": "Liping Jing",
      "affiliations": [
        "Beijing Jiaotong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2071940869",
    "https://openalex.org/W2251453877",
    "https://openalex.org/W3200871535",
    "https://openalex.org/W3100308117",
    "https://openalex.org/W2790109590",
    "https://openalex.org/W2028569501",
    "https://openalex.org/W2963265326",
    "https://openalex.org/W2740811004",
    "https://openalex.org/W3115085313",
    "https://openalex.org/W2169602691",
    "https://openalex.org/W2295979112",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W4228996975",
    "https://openalex.org/W2778118789",
    "https://openalex.org/W3006365540",
    "https://openalex.org/W2145049651",
    "https://openalex.org/W3090556797",
    "https://openalex.org/W2114874863",
    "https://openalex.org/W2604912255",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3023787386",
    "https://openalex.org/W1828830618",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2126734960",
    "https://openalex.org/W2063142656",
    "https://openalex.org/W2251295945",
    "https://openalex.org/W4297805475",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4287854732",
    "https://openalex.org/W4297783886",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2951064288",
    "https://openalex.org/W2251476947",
    "https://openalex.org/W2102733276",
    "https://openalex.org/W2890179025",
    "https://openalex.org/W3123806455",
    "https://openalex.org/W3035050380",
    "https://openalex.org/W3213754079",
    "https://openalex.org/W1490343430",
    "https://openalex.org/W2789995212",
    "https://openalex.org/W2963345057",
    "https://openalex.org/W2045181608",
    "https://openalex.org/W2962903510",
    "https://openalex.org/W2064418625",
    "https://openalex.org/W2604411994",
    "https://openalex.org/W2914076857",
    "https://openalex.org/W2907934657",
    "https://openalex.org/W2250954789",
    "https://openalex.org/W2962785754",
    "https://openalex.org/W2970467549",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W1525595230",
    "https://openalex.org/W2062233052",
    "https://openalex.org/W976440716",
    "https://openalex.org/W3172871932",
    "https://openalex.org/W2167329753",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2974528752",
    "https://openalex.org/W32253530",
    "https://openalex.org/W3098636015",
    "https://openalex.org/W2097385711",
    "https://openalex.org/W2962798918",
    "https://openalex.org/W2804950764",
    "https://openalex.org/W3101427524",
    "https://openalex.org/W3015316465",
    "https://openalex.org/W2950090310",
    "https://openalex.org/W3207408393",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3000623537",
    "https://openalex.org/W2113376247"
  ],
  "abstract": "Keyphrase Extraction (KE) is a critical component in Natural Language Processing (NLP) systems for selecting a set of phrases from the document that could summarize the important information discussed in the document. Typically, a keyphrase extraction system can significantly accelerate the speed of information retrieval and help people get first-hand information from a long document quickly and accurately. Specifically, keyphrases are capable of providing semantic metadata characterizing documents and producing an overview of the content of a document. In this paper, we introduce keyphrase extraction, present a review of the recent studies based on pre-trained language models, offer interesting insights on the different approaches, highlight open issues, and give a comparative experimental study of popular supervised as well as unsupervised techniques on several datasets. To encourage more instantiations, we release the related files mentioned in this paper.",
  "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 2153–2164\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nA Survey on Recent Advances in Keyphrase Extraction from\nPre-trained Language Models\nMingyang Song, Yi Feng and Liping Jing∗\nBeijing Key Lab of Trafﬁc Data Analysis and Mining\nBeijing Jiaotong University\nBeijing, China\n{mingyang.song,21112027,lpjing}@bjtu.edu.cn\nAbstract\nKeyphrase Extraction (KE) is a critical com-\nponent in Natural Language Processing (NLP)\nsystems for selecting a set of phrases from the\ndocument that could summarize the important\ninformation discussed in the document. Typ-\nically, a keyphrase extraction system can sig-\nniﬁcantly accelerate the speed of information\nretrieval and help people get ﬁrst-hand infor-\nmation from a long document quickly and ac-\ncurately. Speciﬁcally, keyphrases are capable\nof providing semantic metadata characterizing\ndocuments and producing an overview of the\ncontent of a document. In this paper, we in-\ntroduce keyphrase extraction, present a review\nof the recent studies based on pre-trained lan-\nguage models, offer interesting insights on the\ndifferent approaches, highlight open issues, and\ngive a comparative experimental study of pop-\nular supervised as well as unsupervised tech-\nniques on several datasets. To encourage more\ninstantiations, we release the related ﬁles men-\ntioned in this paper1.\n1 Introduction\nKeyphrase extraction is a fundamental task in NLP\nfor identifying and extracting a set of keyphrases\nfrom the document that could summarize the im-\nportant information discussed in the source docu-\nment (Hasan and Ng, 2014; Papagiannopoulou and\nTsoumakas, 2019). Keyphrases have enabled ac-\ncurate and fast searching for the document from a\nlarge text corpus and have exhibited their potential\nin improving many NLP tasks, such as text summa-\nrization (Zhang et al., 2004). Various information\nﬁltering and extracting techniques are becoming\ncritical with the ever-increasing amount of text data.\nOwing to its potential importance, keyphrase ex-\ntraction has received more and more attention from\n∗Corresponding author.\n1https://github.com/MySong7NLPer/\nKeyphraseExtractionSurvey\nNLP researchers. However, the keyphrase extrac-\ntion task is far from being solved: state-of-the-art\nperformance on keyphrase extraction is still lower\nthan other core NLP tasks. Our goal in this pa-\nper is to investigate the state-of-the-art models in\nkeyphrase extraction, examine the primary sources\nof errors made by existing systems, and discuss the\nchallenges ahead.\nThe ﬁrst keyphrase extraction task was organized\nby Turney (1999), which deﬁnes the keyphrase ex-\ntraction task as “the automatic selection of impor-\ntant and topical phrases from the body of a doc-\nument”. Since then, there have been numerous\nkeyphrase extraction models (Witten et al., 1999;\nTurney, 2000; Tomokiyo and Hurst, 2003; Hulth,\n2004; Wan and Xiao, 2008a; Jiang et al., 2009; Liu\net al., 2009; Grineva et al., 2009; Nguyen and Phan,\n2009; Bougouin et al., 2013; Caragea et al., 2014;\nDanesh et al., 2015; Bougouin et al., 2016; Florescu\nand Caragea, 2017a; Campos et al., 2018a; Alzaidy\net al., 2019). In the past two decades, keyphrase\nextraction methods have experienced the develop-\nment from traditional approaches to deep learning\nmethods (Hasan and Ng, 2014; Papagiannopoulou\nand Tsoumakas, 2019). With the recent develop-\nment of Pre-trained Language Models (PLMs) (De-\nvlin et al., 2019; Liu et al., 2019), many NLP tasks\nhave signiﬁcantly changed, that is, how to adopt\nand leverage pre-trained language models in the\nspeciﬁc task. Therefore, many keyphrase extrac-\ntion models (Sun et al., 2020a; Song et al., 2021)\nadopt PLMs as the embedding layer.\nWe present a comprehensive survey of recent ad-\nvances in neural keyphrase extraction. We describe\nthe neural keyphrase extraction systems based on\npre-trained language models, which depend on\ndifferent paradigms (e.g., one-stage (Wang et al.,\n2020) and two-stage (Sun et al., 2020a)), various\ntasks (e.g., classiﬁcation and ranking (Mu et al.,\n2020; Sun et al., 2020a)), different learning strate-\ngies (e.g., supervised (Song et al., 2021) and un-\n2153\nsupervised (Ding and Luo, 2021)), and variants of\npre-trained language models (e.g., BERT (Devlin\net al., 2019) and RoBERTa (Liu et al., 2019)).\nFurthermore, we re-implement and collect the\nresults of the mentioned models on several bench-\nmark keyphrase extraction datasets. We illustrate\nthe results in Table 3 and Table 2 and discuss in\nSection 6 how neural keyphrase extraction systems\nhave improved performance over past works, in-\ncluding supervised and unsupervised models. Fur-\nthermore, we provide resources, including links to\nshare the current neural keyphrase extraction sys-\ntems and links to share the code for each category\nof the neural keyphrase extraction approaches. To\nthe best of our knowledge, this is the ﬁrst survey\nfocusing on the keyphrase extraction task based on\nrecent pre-trained language models.\nOverall, this paper ﬁrst discusses previous sur-\nveys on keyphrase extraction in Section 2.1 and\ngive a brieﬂy introduction about pre-trained lan-\nguage models in Section 2.2. Then we highlight\nstandard, past, and recent benchmark keyphrase\nextraction datasets (from shared tasks and other\nresearch) in Section 3 and evaluation metrics in\nSection 4. We then describe neural keyphrase ex-\ntraction systems in Section 5. Next, we give the\nanalysis and discussion in Section 6. Finally, we\nsummarize the conclusions and future directions\nof neural keyphrase extraction in Section 7. The\nlimitations of our work is presented in Section 8.\n2 Preliminary\nIn this section, we claim the differences between\nthe current survey and the existing surveys. Next,\nwe present the background of pre-trained language\nmodels and their importance in NLP.\n2.1 Previous Surveys\nThe ﬁrst comprehensive keyphrase extraction sur-\nvey was Hasan and Ng (2014), which covered a\nvariety of unsupervised and supervised keyphrase\nextraction models, highlighted common features\nused by existing models during that time, and ex-\nplained evaluation metrics that are still in use to-\nday. Papagiannopoulou and Tsoumakas (2019)\npresent a more recent keyphrase extraction survey\nthat mainly included many unsupervised and super-\nvised models based on deep learning. Furthermore,\nPapagiannopoulou and Tsoumakas (2019) also pro-\nvides a list of popular keyphrase extraction datasets\nand a thorough empirical study.\nThe existing keyphrase extraction surveys pri-\nmarily cover early feature-engineered and neural-\nbased keyphrase extraction models (Hasan and Ng,\n2014; Papagiannopoulou and Tsoumakas, 2019).\nThere is not yet, to our knowledge, a comprehen-\nsive survey of keyphrase extraction based on pre-\ntrained language models.\n2.2 Pre-trained Language Models\nRecently, pre-trained language models have ad-\nvanced the state-of-the-art in many NLP tasks rang-\ning from textual similarity to text summarization\n(Zhang et al., 2019; Liu and Lapata, 2019; Zhong\net al., 2020) and named entity recognition (Zhou\net al., 2021). State-of-the-art pre-trained models in-\nclude LSTM-based language models (e.g., ELMo\n(Peters et al., 2018)) and Transformer-based lan-\nguage models (e.g., BERT 2 (Devlin et al., 2019)\nand RoBERTa (Liu et al., 2019)). Speciﬁcally, the\ntransformer-based models learn bidirectional repre-\nsentations for words based on a masked language\nmodel and sentence adjacency training objective\n(Devlin et al., 2019). Simply using contextual-\nized embeddings obtained from the transformer-\nbased pre-trained language models in place of tra-\nditional embeddings has resulted in state-of-the-art\nperformance on a range of NLP tasks. Therefore,\npre-trained language models have been employed\nas encoders for obtaining word-, sentence-, and\ndocument-level representations to assist the down-\nstream tasks.\n3 Keyphrase Extraction Dataset\nSince the ﬁrst shared task on KE (Turney, 1999),\nmany shared tasks and benchmark datasets for KE\nhave been created. Speciﬁcally, OpenKP (Xiong\net al., 2019), Inspec (Hulth, 2003), NUS (Nguyen\nand Kan, 2007), Krapivin (Krapivin and March-\nese, 2009), SemEval2010 (Kim et al., 2010), Se-\nmEval2017 (Augenstein et al., 2017), and KP20k\n(Meng et al., 2017) were created from scientiﬁc\narticles in English.\nCompared with other datasets, KP20k contains a\nlarge amount of annotation data, so it is often used\nas the dataset to train the neural-based KE models\nrecently. Meanwhile, in recent papers (Sun et al.,\n2020a; Song et al., 2021), Inspec (Hulth, 2003),\nNUS (Nguyen and Kan, 2007), Krapivin (Krapivin\nand Marchese, 2009), SemEval2010 (Kim et al.,\n2010), and SemEval2017 (Augenstein et al., 2017)\n2https://huggingface.co/bert-base-cased\n2154\nDataset Type Long # Doc. Avg. # Words Present KPs (%)\nKP20k (Meng et al., 2017) Scientiﬁc Paper Abstract 568.00k 188.47 57.40\nInspec (Hulth, 2003) Scientiﬁc Paper Abstract 2.00k 130.57 55.69\nSemEval2017 (Augenstein et al., 2017) Scientiﬁc Paper Abstract 0.50k 176.13 42.01\nNUS (Nguyen and Kan, 2007) Full Scientiﬁc Paper 0.21k 7644.43 67.75\nKrapivin (Krapivin and Marchese, 2009) Full Scientiﬁc Paper 2.30k 8420.76 44.74\nSemEval2010 (Kim et al., 2010) Full Scientiﬁc Paper 0.24k 7434.52 88.70\nDUC2001 (Wan and Xiao, 2008b) News Document 0.31k 724.63 97.82\nOpenKP (Xiong et al., 2019) Open Domain Web Content 147.20k 900.40 100.00\nTable 1: This table shows the statistics of different recent popular datasets. Long indicates whether the dataset\nbelongs to a long document. # Doc.is the number of documents in the dataset. Avg. # Wordsis the average number\nof words for documents in the indicated dataset. Present KPs (%)indicates the percentage of keyphrases, which\nare presented in the documents.\ndatasets are often used as the zero-shot test sets to\nverify the robustness of the KE models trained by\nthe KP20k dataset. Furthermore, KE tasks have\nalso been organized on newswire articles in En-\nglish, e.g., DUC2001 (Wan and Xiao, 2008b). Ta-\nble 1 summarizes the statistics of several commonly\nused benchmark datasets.\n4 Keyphrase Extraction Evaluation\nThis section describes evaluation metrics for mea-\nsuring recent state-of-the-art keyphrase extraction\nbaselines on commonly-used datasets. Designing\na suitable evaluation metric for the keyphrase ex-\ntraction task is by no means an easy study (Hasan\nand Ng, 2014). To score the output of a keyphrase\nextraction model, the traditional approach, which\nis also adopted by the SemEval-2010 (Kim et al.,\n2010) shared task on keyphrase extraction, is (1)\nto create a mapping between the keyphrases in the\nground-truth keyphrases and those in the model\noutput adopting exact and partial matching (Papa-\ngiannopoulou and Tsoumakas, 2019), and then (2)\nscore the output using evaluation metrics such as\nprecision (P), recall (R), and F1-score (F1).\nAs mentioned earlier, such evaluation usually\noperates based on exact matches between the pre-\ndicted and ground-truth keyphrases. However, such\na strategy cannot account for partial matches or se-\nmantic similarity. For example, if the prediction is\n\"keyphrase extraction model\" and the ground truth\nis \"keyphrase extraction system\", despite both se-\nmantic similarity and partial matching, the score\nwill be 0. These minor deviations are ubiquitous in\nkeyphrase extraction, yet they are harshly penalized\nby the \"exact match\" evaluation metrics.\n5 Neural Keyphrase Extraction Models\nwith Pre-trained Language Models\nThere are two popular pipelines in the keyphrase\nextraction task, including one-stage and two-stage\nframeworks, as illustrated in Figure 1. The former\nmainly refers to using the task reformulation to\naddress the keyphrase extraction task, which often\ntreats the keyphrase extraction task as a sequence\nlabeling task. The latter represents a more general\nframework, which usually operates in two proce-\ndures: (1) extracting a set of words/phrases that\nserve as candidate phrases using some heuristics\nand (2) determining which candidate phrases are\nkeyphrases using supervised or unsupervised meth-\nods (Hasan and Ng, 2014; Papagiannopoulou and\nTsoumakas, 2019).\nTypically, supervised methods perform better\non speciﬁc domain tasks. However, this kind of\nmethod takes a lot of labor to annotate the corpus,\nand the model after training may overﬁt and not\nwork well on other KE datasets. On the contrary,\nunsupervised methods do not need to annotate the\ncorpus and usually have better data generalization\nin different domains. Still, the performance is often\ninsufﬁcient due to the lack of annotated data. Over-\nall, we deﬁned the above two procedures as the\ncandidate keyphrase extraction and keyphrase im-\nportance estimation. In this paper, we distinguish\nthe existing methods into three categories depend-\ning on the recent state-of-the-art baselines (with\n2155\nInput Document KeyphraseImportanceEstimation\nDynamicCandidateExtraction Top-KKeyphrasesCandidateKeyphraseExtractionHeuristic-basedCandidateExtraction\nSupervised\nUnsupervised\nLearningwithAnnotatedData\nLearningwithoutAnnotatedData\nFigure 1: The overall architecture of the two-stage supervised and unsupervised keyphrase extraction framework.\npre-trained language models as the backbone), in-\ncluding two-stage unsupervised, two-stage super-\nvised, and one-stage supervised models.\n5.1 Two-Stage Unsupervised Keyphrase\nExtraction Models\nAs noted before, unsupervised keyphrase extraction\nsystems generally extract a set of phrases from the\nsource document as candidates by using heuristic\nrules. These rules are designed to avoid spurious\ninstances and keep the number of candidates to a\nminimum (Hasan and Ng, 2014). The main steps\nof the commonly used candidate keyphrases extrac-\ntion methods for the recent unsupervised keyphrase\nextraction models are as follows, (1) tokenizing\nthe document and tagging the document with part-\nof-speech (POS) tags via the StanfordCoreNLP\nTools3; (2) extracting candidate phrases based on\npart-of-speech tags by the regular expression via\nthe python package NLTK4. Furthermore, different\npruning heuristics have been designed for pruning\ncandidates that are unlikely to be keyphrases to\nobtain a better candidate set (Huang et al., 2006;\nKumar and Srinathan, 2008; El-Beltagy and Rafea,\n2009; Newman et al., 2012; You et al., 2009). After\nobtaining candidates, keyphrases are determined\nby estimating the importance of each candidate\nthrough various strategies. Here, to facilitate the\nintroduction, we divide the methods of importance\nestimation into two categories, namely, traditional\nmethods and embedding-based methods.\nTraditional unsupervised keyphrase extraction\nsystems can be mainly divided into statistics-based\n(Jones, 2004; Campos et al., 2018b), topic-based\n(Liu et al., 2009; Jardine and Teufel, 2014), and\ngraph-based (Mihalcea and Tarau, 2004; Wan and\nXiao, 2008b; Bougouin et al., 2013; Florescu\nand Caragea, 2017b) methods. Generally, these\n3https://stanfordnlp.github.io/CoreNLP\n4https://github.com/nltk\nmodels primarily use different features of doc-\numents (e.g., word frequency, position, linguis-\ntic properties, topic, length, the relationship be-\ntween words, external knowledge-based informa-\ntion, etc.) to estimate the importance of each candi-\ndate phrase and discriminate whether a candidate\nphrase is a keyphrase (Hasan and Ng, 2014; Papa-\ngiannopoulou and Tsoumakas, 2019).\nHowever, these traditional unsupervised models\nestimate the importance scores of candidate phrases\nbased on the surface-level features, ignoring the\nhigh-level features (e.g., syntactic and semantic\ninformation) of natural languages, which leads to\nextract wrong keyphrases. Therefore, recent stud-\nies focus on embedding-based models (Wang et al.,\n2015; Mahata et al., 2018a; Papagiannopoulou and\nTsoumakas, 2018; Sahrawat et al., 2020; Kulkarni\net al., 2022; Song et al., 2022b), which leverage pre-\ntrained embeddings (containing high-level features)\nto obtain phrase and document embeddings and cal-\nculate the importance scores of candidate phrases\nfor extracting keyphrases. Wang et al. (2015) is\nthe ﬁrst work to explore utilizing word embed-\nding and frequency to generate weighted edges\nbetween words, then using the weighted PageRank\nalgorithm to compute and rank candidate scores.\nKey2vec (Mahata et al., 2018a) proposes an effec-\ntive way of processing text documents for training\nmulti-word phrase embeddings that are used for\ntopic representations of scientiﬁc articles and rank-\ning of keyphrases extracted from them using the\ntopic-weighted PageRank algorithm. Mahata et al.\n(2018b) uses a combination of theme-weighted per-\nsonalized PageRank algorithm and neural phrase\nembeddings for extracting and ranking keyphrases.\nEmbedRank (Bennani-Smires et al., 2018) ranks\ncandidate phrases by measuring the semantic simi-\nlarity between each candidate phrase and document\nembeddings.\nWith the development of pre-trained language\n2156\nmodels (e.g., ELMo (Peters et al., 2018), BERT\n(Devlin et al., 2019), and RoBERta (Liu et al.,\n2019)), SIFRank 5 (Sun et al., 2020b) improves\ncandidate phrase and document embeddings from\nEmbedRank with the pre-trained language model\nELMo (Peters et al., 2018) and achieves better per-\nformance. JointGL6 (Liang et al., 2021) integrates\nboundary-aware phrase centrality (the semantic\nsimilarities are calculated between all candidate\nphrases for identifying which candidate is better)\nand phrase-document relevance (the semantic sim-\nilarities are calculated between candidate phrases\nand their corresponding document) from both local\nand global views, then used both jointly to deter-\nmine the importance of each candidate. Attention-\nRank7 (Ding and Luo, 2021) adopts a pre-trained\nlanguage model to calculate the self-attention of a\ncandidate within the context of a sentence, and the\ncross-attention between a candidate and sentences\nwithin the source document to evaluate the local\nand global importance of each candidate. MDER-\nank8 (Zhang et al., 2021) proposes to rank candi-\ndates using the similarity between the BERT em-\nbeddings of the source document and the masked\ndocument. Totally, these models achieve state-of-\nthe-art performance in the unsupervised keyphrase\nextraction task, beneﬁting from the development of\nrepresentation learning.\n5.2 Two-Stage Supervised Keyphrase\nExtraction Models\nDifferent from two-stage unsupervised approaches,\nsupervised approaches generally combine candi-\ndate keyphrase extraction and keyphrase impor-\ntance estimation via an end-to-end learning frame-\nwork, guide the whole model to rank and extract\nkeyphrases through annotated data and optimize\nthe two stages simultaneously. Therefore, to obtain\nsufﬁcient candidates, the recent supervised mod-\nels (Xiong et al., 2019; Sun et al., 2020a; Song\net al., 2021, 2022a) directly extract n-grams from\nthe document as candidates. Then propose, various\napproaches to estimate the importance scores of\ncandidates. To estimate the importance of candi-\ndate phrases, similar to unsupervised models, su-\npervised models (Xiong et al., 2019; Sun et al.,\n2020a; Song et al., 2021) also obtain phrase and\ndocument representations by adopting pre-trained\n5https://github.com/sunyilgdx/SIFRank\n6https://github.com/xnliang98/uke_ccrank\n7https://github.com/hd10-iupui/AttentionRank\n8https://github.com/linhanz/mderank\nlanguage models as the backbone, including ELMo\n(Peters et al., 2018), BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019), etc.\nFirstly, BLING-KPE (Xiong et al., 2019) for-\nmulates keyphrase extraction as an n-gram level\nkeyphrase chunking task to determine whether\na candidate is a keyphrase, which incorporates\npre-trained embeddings (i.e., ELMo (Peters et al.,\n2018)) into a convolutional transformer network\nto model n-gram representations. BLING-KPE\nachieves signiﬁcant improvement over previous\nmodels. To leverage external knowledge to as-\nsist keyphrase extraction, SMART-KPE 9 (Wang\net al., 2020) also shows that incorporating multi-\nmodal information in web pages, such as font, size,\nand DOM features, can bring further improvement\nfor open-domain web keyphrase extraction. Later,\nAinslie et al. (2020) replaces the full self-attention\nof Transformers with local-global attention, which\nsigniﬁcantly boosts the keyphrase extraction per-\nformance for long documents. SKE-BASE-RANK\n(Mu et al., 2020) proposes a span-based keyphrase\nextraction model to model the relationships be-\ntween candidates and the document in context.\nJointKPE10 (Sun et al., 2020a) proposes an open-\ndomain keyphrase extraction approach built on pre-\ntrained language models (Devlin et al., 2019; Liu\net al., 2019), which can capture both local phrase-\nness and global informativeness when extracting\nkeyphrases. JointKPE learns to rank keyphrases by\nestimating their informativeness in the whole docu-\nment and is jointly trained on the keyphrase chunk-\ning task to guarantee the phraseness of keyphrase\ncandidates. KIEMP11 (Song et al., 2021) proposes\nestimating the importance score of each candi-\ndate from multiple perspectives and introducing\na matching module to match the high-level concept\nbetween the document and candidates to enhance\nthe relevance of extracted keyphrases. To extract\nmore relevant keyphrases, HyperMatch 12 (Song\net al., 2022a) proposes a new matching framework\nand explores keyphrase extraction in the hyperbolic\nspace. Concretely, HyperMatch ﬁrst maps phrase\nand document representations into the same hy-\nperbolic space and explicitly models the relevance\nbetween candidate phrases and the document as\nthe phrase-document relevance via the Poincaré\ndistance to extract keyphrases.\n9https://github.com/victorywys/SMART-KPE\n10https://github.com/thunlp/BERT-KPE\n11https://github.com/MySong7NLPer/KIEMP\n12https://github.com/MySong7NLPer/HyperMatch\n2157\nModel\nDUC2001 Inspec SemEval2010 SemEval2017\nF1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15\nTraditional Two-Stage Models\nTF-IDF (Jones, 2004) 9.21 10.63 11.06 11.28 13.88 13.83 2.81 3.48 3.91 12.70 16.26 16.73\nY AKE (Campos et al., 2018b) 12.27 14.37 14.76 18.08 19.62 20.11 11.76 14.4 15.19 11.84 18.14 20.55\nTextRank (Mihalcea and Tarau, 2004) 11.80 18.28 20.22 27.04 25.08 36.65 3.80 5.38 7.65 16.43 25.83 30.50\nSingleRank (Wan and Xiao, 2008b) 20.43 25.59 25.70 27.79 34.46 36.05 5.90 9.02 10.58 18.23 27.73 31.73\nTopicRank (Bougouin et al., 2013) 21.56 23.12 20.87 25.38 28.46 29.49 12.12 12.90 13.54 17.10 22.62 24.87\nPositionRank (Florescu and Caragea, 2017b) 23.35 28.57 28.60 28.12 32.87 33.32 9.84 13.34 14.33 18.23 26.30 30.55\nTwo-Stage Embedding-based Unsupervised Keyphrase Extraction Models with Static Embeddings\nEmbedRankd2v (Bennani-Smires et al., 2018) 24.02 28.12 28.82 31.51 37.94 37.96 3.02 5.08 7.23 20.21 29.59 33.94\nKeyGames (Saxena et al., 2020) 24.42 28.28 29.77 32.12 40.48 40.94 11.93 14.35 14.62 - - -\nTwo-Stage Embedding-based Unsupervised Keyphrase Extraction Models with PLMs\nSIFRank (Sun et al., 2020b) 24.27 27.43 27.86 29.11 38.80 39.59 - - - 22.59 32.85 38.10\nJointGL (Liang et al., 2021) 28.62 35.52 36.29 32.61 40.17 41.09 13.02 19.35 21.72 - - -\nAttentionRank (Ding and Luo, 2021) - - - 24.45 32.15 34.49 11.39 15.12 16.66 23.59 34.37 38.21\nMDERank (Zhang et al., 2021) 23.31 26.65 26.42 27.85 34.36 36.40 13.05 18.27 20.35 20.37 31.21 36.63\nTable 2: Performance of unsupervised keyphrase extraction models on the DUC2001, Inspec, SemEval2010 and\nSemEval2017 test sets. F1 scores on the top 5, 10, and 15 keyphrases are reported. The best results are bolded.\nThe results of baseline models are those presented in the original papers or better results published in other papers\nrecently.\n5.3 One-Stage Supervised Keyphrase\nExtraction Models\nA major limitation of the above two-stage super-\nvised approaches is classifying the labels of each\ncandidate phrase independently while ignoring\nthe dependencies that could potentially exist be-\ntween candidates. Therefore, recent studies (Gol-\nlapalli et al., 2017; Basaldella et al., 2018; Wang\net al., 2018; Alzaidy et al., 2019; Sun et al., 2019;\nMu et al., 2020; Sahrawat et al., 2020) formu-\nlated keyphrase extraction as sequence labeling and\nshowed that using linear-chain Conditional Ran-\ndom Fields improved the performance over base-\nline models for this task. Then, Mu et al. (2020)\nproposes SKE-BASE-CLS and -RANK, which di-\nrectly extracts span-based phrase representations\nfrom all the document tokens via pre-trained lan-\nguage models and further learn to capture the in-\nteraction between them and their corresponding\ndocument to get better ranking results. Further-\nmore, this kind of model can extract overlapped\nkeyphrases (Mu et al., 2020).\n6 Discussion\nIn this section, we report the results of the recent\nunsupervised and supervised keyphrase extraction\nbaselines, which all adopt pre-trained language\nmodels as the backbone, as shown in Table 2 and\nTable 3. Speciﬁcally, Table 2 presents the results\nof the traditional unsupervised methods and the un-\nsupervised embedding-based keyphrase extraction\nbaselines discussed in Section 5.1 on the DUC2001\n(Wan and Xiao, 2008b), Inspec (Hulth, 2003), Se-\nmEval2010 (Kim et al., 2010), and SemEval2017\n(Augenstein et al., 2017) datasets. Embedding-\nbased two-stage models without PLMs indicate\nthat the models do not use pre-trained language\nmodels as the backbone to obtain representations.\nTable 3 shows the results of all the different cate-\ngories of the supervised keyphrase extraction sys-\ntems discussed in Section 5.2 and Section 5.3 on\nthe KP20k (Meng et al., 2017) and OpenKP (Xiong\net al., 2019) datasets.\nOur ﬁrst ﬁnding from the survey is those two-\nstage embedding-based systems with static em-\nbeddings outperform two-stage traditional meth-\nods, despite the latter’s access to different valuable\nfeatures (e.g., word frequency, position, linguis-\ntic properties, topic, length, the relationship be-\ntween words, external knowledge-based informa-\ntion, etc.). This further demonstrates the necessity\nof studying embedding-based methods.\nOur second ﬁnding is those embedding-based\nsystems with PLMs outperform embedding-based\napproaches with static embeddings in most cases.\n2158\nModel KP20k OpenKP\nF1@5 F1@10 F1@1 F1@3 F1@5\nOne-Stage Supervised Keyphrase Extraction Models\nSMART-KPE+Full (Wang et al., 2020) - - 38.0 40.1 34.4\nBERT-TagKPE† 38.8 31.7 32.1 36.1 31.4\nBERT-SpanKPE† 36.8 30.8 31.8 33.2 28.9\nRoBERTa-TagKPE‡ 39.3 32.0 36.1 38.0 33.0\nRoBERTa-SpanKPE‡ 37.3 30.9 34.7 36.1 31.3\nTwo-Stage Supervised Keyphrase Extraction Models\nBLING-KPE (Xiong et al., 2019) - - 26.7 29.2 20.9\nSKE-BASE-CLS (Mu et al., 2020) 38.6 32.6 - - -\nBERT-ChunkKPE† 41.2 33.7 34.0 35.6 31.1\nRoBERTa-ChunkKPE† 40.8 33.7 35.5 37.3 32.4\nSKE-BASE-RANK (Mu et al., 2020) 39.2 33.0 - - -\nBERT-RankKPE† 41.3 34.0 34.2 37.4 32.5\nRoBERTa-RankKPE† 41.7 34.3 36.1 39.0 33.7\nHyperMatch (Song et al., 2022a) 41.6 34.3 36.4 39.4 33.8\nBERT-JointKPE† 41.1 33.8 34.9 37.6 32.5\nRoBERTa-JointKPE† 41.9 34.4 36.4 39.1 33.8\nKIEMP (Song et al., 2021) 42.1 34.5 36.9 39.2 34.0\nTable 3: Results of different categories of supervised keyphrase extraction models on two benchmark keyphrase\ndatasets. F1 scores on the top 1, 3, 5, and 10 keyphrases are reported. † indicates the results are reported by their\ncorresponding paper (Sun et al., 2020a), and ‡ denotes that these results are re-evaluated by ourselves via the code\nwhich is provided by its corresponding paper (Sun et al., 2020a). The best results are highlighted in bold. The results\nof baseline models are those presented in the original papers or better results published in other papers recently.\nHowever, not all embedding-based systems with\nPLMs are superior to embedding-based systems\nwith static embeddings. The former generally out-\nperforms the latter when adopting the same impor-\ntance estimation strategy, but the estimation strat-\negy can signiﬁcantly affect the results of keyphrase\nextraction. To sum up, effectively using pre-trained\nembeddings to estimate the importance score of\neach candidate is a critical part of improving the\nperformance of keyphrase extraction. Furthermore,\nthere is still interesting progress to be made by\nleveraging a self-supervised learning strategy to op-\ntimize embedding-based systems. MDERank uses\na simple yet effective contrastive learning strategy\nto optimize embedding-based systems, achieving\nbetter performance.\nOur third ﬁnding is that the embedding-based\nmethods have slight improvement on long docu-\nment datasets (e.g., SemEval2010), and all unsu-\npervised methods have poor effects on long docu-\nment datasets. This demonstrates that keyphrase\nextraction from long documents is still a challeng-\ning problem.\nOur ﬁnal ﬁnding is that two-stage supervised\nkeyphrase extraction methods are superior to one-\nstage supervised keyphrase extraction methods, as\nillustrated in Table 3. In addition, the two-stage\nmethod has higher scalability and adaptability than\nthe one-stage method, such as handling long and\nextremely long documents.\n7 Conclusion and Future Directions\nWe summarize the recent neural keyphrase extrac-\ntion models based on pre-trained language mod-\nels. Our survey of models for keyphrase extraction,\ncovering both unsupervised and supervised mod-\nels, has yielded several important insights. The\nanalysis revealed that there are at least six major\nchallenges ahead.\n7.1 Improving the Quality of Generated\nCandidate Keyphrases\nMany heuristic rules have proven effective with a\nhigh recall to cover most of the gold keyphrases\n2159\nof source documents, which determines the upper\nbound of the performance of keyphrase extraction\n(Hasan and Ng, 2014). Intuitively, better candi-\ndate keyphrase extraction strategies are required\nto generate a set of candidate keyphrases with a\nhigher recall from the source document to improve\nthe upper-bound performance of keyphrase extrac-\ntion. Recent work (Jawahar et al., 2019) demon-\nstrates that the intermediate layers of BERT encode\na rich hierarchy of linguistic information, with sur-\nface features at the bottom, syntactic features in\nthe middle, and semantic features at the top, as\nmentioned in Section 2.2. They also observe that\nBERT mostly captures phrase-level information in\nthe lower layers and gradually dilutes this informa-\ntion in higher layers. In addition, the number of\ncandidate keyphrases will increase as the document\nlength increases. Therefore, how constructing can-\ndidate keyphrases using the potential knowledge of\npre-trained language models is a valuable research\ndirection.\n7.2 Improving Evaluation Metric\nAs mentioned in Section 4, the existing evaluation\nmetrics occur when a keyphrase extraction system\nextracts a keyphrase from candidates that is seman-\ntically equivalent to a ground-truth keyphrase but is\nconsidered erroneous by a scoring function because\nit fails to recognize that the predicted keyphrase\nand the corresponding gold keyphrase are semanti-\ncally equivalent.\nIn other words, an evaluation error is not made\nby a keyphrase extraction system, but a mistake due\nto an unformed scoring function (Hasan and Ng,\n2014). Therefore, a more suitable evaluation metric\nis required to evaluate the predicted keyphrases\nby adopting the semantic-based matching metric\ninstead of the exact matching evaluation metric. In\nthe future, using pre-trained language models (e.g.,\nBERT (Devlin et al., 2019) and RoBERTa (Liu\net al., 2019)) to construct a new semantic-aware\nevaluation metric similar to BERTScore (Zhang\net al., 2020) may be an interesting and valuable\nresearch direction.\n7.3 Reducing Over-Generation Error\nOver-generation errors occur when a keyphrase ex-\ntraction system correctly predicts a candidate as\na keyphrase because it contains a word that fre-\nquently appears in the associated document but at\nthe same time erroneously outputs other candidates\nas keyphrases because they have the same word in\nthe document.\nAs mentioned before, for example, if the pre-\ndiction is \"keyphrase extraction challenge\" and the\nground truth is \"keyphrase extraction system\", de-\nspite both semantic similarity and partial matching,\nthe score will be 0. These minor deviations are\nubiquitous in keyphrase extraction, yet they are\nharshly penalized by the \"exact match\" evaluation\nmetrics. There are often some non-keyphrases in\nthe candidates. Half of the content of such phrases\nis very relevant to the core information of the doc-\nument, but the other half is meaningless. These\ncandidate keyphrases are usually hard to extract\nand treated as hard samples, which is one of the\nmain reasons for reducing keyphrase extraction\nperformance. The above issues can be solved by\nmodifying the traditional evaluation metrics with\nsemantic weighting.\n7.4 Handling Long Document\nGenerally, two main challenges exist in keyphrase\nextraction systems equipped with pre-trained lan-\nguage models (e.g., BERT (Devlin et al., 2019)) as\nthe backbone when extracting keyphrases from a\nlong document, especially for an extremely long\ndocument.\nThe ﬁrst challenge is that pre-trained language\nmodels can not directly model the complete context\ninformation when facing long documents due to the\nlength limitation of pre-trained language models.\nThe second challenge is that as the length of\nthe document increases, the difﬁculty of estimating\nthe importance scores of candidate phrases also in-\ncreases (speciﬁcally for the number of candidates),\nresulting in the reduction of keyphrase extraction\naccuracy.\n7.5 Improving Domain Generalization\nFor news or scientiﬁc documents, the authors usu-\nally annotate a set of keyphrases for their articles\n(Meng et al., 2017; Augenstein et al., 2017). How-\never, there is typically a lack of keyphrases as the la-\nbel information for their corresponding documents\nin other speciﬁc domains.\nMost existing keyphrase extraction datasets and\nstudies are based on news or scientiﬁc documents\nand lack datasets and research related to other do-\nmains. Therefore, the task worthy of investigation\nis to transfer the keyphrase extraction model from\nthe scientiﬁc domain to other domains to build a\n2160\ndomain-speciﬁc keyphrase extraction model with\nvarious domain generalization strategies.\n7.6 Probing Pre-trained Language Model for\nKeyphrase Extraction\nIn addition to using transformer-based pre-trained\nlanguage models (e.g., BERT) in NLP tasks and\nend applications, research has also been done on\nBERT, especially to reveal what linguistic infor-\nmation is available in different parts of the model\n(Jawahar et al., 2019; de Vries et al., 2020; Chen\net al., 2021). It has been noted that BERT progres-\nsively acquires linguistic information roughly in\nthe same order as the classic language processing\npipeline (Tenney et al., 2019a,b): surface features\nare expressed in lower layers, syntactic features\nmore in middle layers, and semantic ones in higher\nlayers (Jawahar et al., 2019). Making full use of\nthe above hierarchy information may effectively\nimprove the performance of keyphrase extraction.\n8 Limitations\nThe main goal of this paper is to provide a survey\nof the existing models. Since we do not propose\nnew models, there are no potential social risks to\nthe best of our knowledge. Our work may ben-\neﬁt the research community by providing more\nintrospection into the current state-of-the-art neural\nkeyphrase extraction approaches with pre-trained\nlanguage models.\n9 Acknowledgments\nWe thank the three anonymous reviewers for their\nhelpful comments. This work was partly supported\nby the Fundamental Research Funds for the Cen-\ntral Universities (2019JBZ110); the National Nat-\nural Science Foundation of China under Grant\n62176020; the National Key Research and Devel-\nopment Program (2020AAA0106800); the Beijing\nNatural Science Foundation under Grant L211016;\nCAAI-Huawei MindSpore Open Fund; and Chi-\nnese Academy of Sciences (OEIP-O-202004).\nReferences\nJoshua Ainslie, Santiago Ontañón, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: encoding long and structured inputs in\ntransformers. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2020, Online, November 16-20, 2020,\npages 268–284. Association for Computational Lin-\nguistics.\nRabah Alzaidy, Cornelia Caragea, and C. Lee Giles.\n2019. Bi-lstm-crf sequence labeling for keyphrase ex-\ntraction from scholarly documents. In WWW, pages\n2551–2557. ACM.\nIsabelle Augenstein, Mrinal Das, Sebastian Riedel,\nLakshmi Vikraman, and Andrew McCallum. 2017.\nSemeval 2017 task 10: Scienceie - extracting\nkeyphrases and relations from scientiﬁc publications.\nIn SemEval@ACL, pages 546–555. Association for\nComputational Linguistics.\nMarco Basaldella, Elisa Antolli, Giuseppe Serra, and\nCarlo Tasso. 2018. Bidirectional LSTM recurrent\nneural network for keyphrase extraction. In Digi-\ntal Libraries and Multimedia Archives - 14th Italian\nResearch Conference on Digital Libraries, IRCDL\n2018, Udine, Italy, January 25-26, 2018, Proceed-\nings, volume 806 of Communications in Computer\nand Information Science, pages 180–187. Springer.\nKamil Bennani-Smires, Claudiu Musat, Andreea Hoss-\nmann, Michael Baeriswyl, and Martin Jaggi. 2018.\nSimple unsupervised keyphrase extraction using sen-\ntence embeddings. In CoNLL, pages 221–229. Asso-\nciation for Computational Linguistics.\nAdrien Bougouin, Florian Boudin, and Béatrice Daille.\n2013. Topicrank: Graph-based topic ranking for\nkeyphrase extraction. In IJCNLP, pages 543–551.\nAsian Federation of Natural Language Processing /\nACL.\nAdrien Bougouin, Florian Boudin, and Béatrice Daille.\n2016. Keyphrase annotation with graph co-ranking.\nIn COLING, pages 2945–2955. ACL.\nRicardo Campos, Vítor Mangaravite, Arian Pasquali,\nAlípio Mário Jorge, Célia Nunes, and Adam Jatowt.\n2018a. A text feature based automatic keyword ex-\ntraction method for single documents. In ECIR, vol-\nume 10772 of Lecture Notes in Computer Science,\npages 684–691. Springer.\nRicardo Campos, Vítor Mangaravite, Arian Pasquali,\nAlípio Mário Jorge, Célia Nunes, and Adam Jatowt.\n2018b. Yake! collection-independent automatic key-\nword extractor. In ECIR, volume 10772 of Lecture\nNotes in Computer Science, pages 806–810. Springer.\nCornelia Caragea, Florin Adrian Bulgarov, Andreea\nGodea, and Sujatha Das Gollapalli. 2014. Citation-\nenhanced keyphrase extraction from research papers:\nA supervised approach. In EMNLP, pages 1435–\n1446. ACL.\nBoli Chen, Yao Fu, Guangwei Xu, Pengjun Xie,\nChuanqi Tan, Mosha Chen, and Liping Jing. 2021.\nProbing bert in hyperbolic spaces. In International\nConference on Learning Representations.\n2161\nSoheil Danesh, Tamara Sumner, and James H. Martin.\n2015. Sgrank: Combining statistical and graphical\nmethods to improve the state of the art in unsuper-\nvised keyphrase extraction. In *SEM@NAACL-HLT,\npages 117–126. The *SEM 2015 Organizing Com-\nmittee.\nWietse de Vries, Andreas van Cranenburgh, and Malv-\nina Nissim. 2020. What’s so special about bert’s\nlayers? a closer look at the nlp pipeline in monolin-\ngual and multilingual models. In EMNLP (Findings),\npages 4339–4350. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT, pages 4171–4186. Association\nfor Computational Linguistics.\nHaoran Ding and Xiao Luo. 2021. Attentionrank: Un-\nsupervised keyphrase extraction using self and cross\nattentions. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1919–1928.\nSamhaa R. El-Beltagy and Ahmed A. Rafea. 2009. Kp-\nminer: A keyphrase extraction system for english and\narabic documents. Inf. Syst., 34(1):132–144.\nCorina Florescu and Cornelia Caragea. 2017a. A\nnew scheme for scoring phrases in unsupervised\nkeyphrase extraction. In ECIR, volume 10193 of\nLecture Notes in Computer Science, pages 477–483.\nCorina Florescu and Cornelia Caragea. 2017b. Posi-\ntionrank: An unsupervised approach to keyphrase\nextraction from scholarly documents. In ACL (1),\npages 1105–1115. Association for Computational\nLinguistics.\nSujatha Das Gollapalli, Xiaoli Li, and Peng Yang. 2017.\nIncorporating expert knowledge into keyphrase ex-\ntraction. In AAAI, pages 3180–3187. AAAI Press.\nMaria P. Grineva, Maxim N. Grinev, and Dmitry Li-\nzorkin. 2009. Extracting key terms from noisy and\nmultitheme documents. In WWW, pages 661–670.\nACM.\nKazi Saidul Hasan and Vincent Ng. 2014. Automatic\nkeyphrase extraction: A survey of the state of the art.\nIn ACL (1), pages 1262–1273. The Association for\nComputer Linguistics.\nChong Huang, Yonghong Tian, Zhi Zhou, Charles X.\nLing, and Tiejun Huang. 2006. Keyphrase extraction\nusing semantic networks structure analysis. InICDM,\npages 275–284. IEEE Computer Society.\nAnette Hulth. 2003. Improved automatic keyword\nextraction given more linguistic knowledge. In\nEMNLP.\nAnette Hulth. 2004. Enhancing linguistically ori-\nented automatic keyword extraction. In HLT-NAACL\n(Short Papers). The Association for Computational\nLinguistics.\nJames Jardine and Simone Teufel. 2014. Topical PageR-\nank: A model of scientiﬁc expertise for bibliographic\nsearch. In Proceedings of the 14th Conference of\nthe European Chapter of the Association for Com-\nputational Linguistics, pages 501–510, Gothenburg,\nSweden. Association for Computational Linguistics.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does bert learn about the structure of\nlanguage? In ACL (1), pages 3651–3657. Associa-\ntion for Computational Linguistics.\nXin Jiang, Yunhua Hu, and Hang Li. 2009. A ranking\napproach to keyphrase extraction. In SIGIR, pages\n756–757. ACM.\nKaren Spärck Jones. 2004. A statistical interpretation\nof term speciﬁcity and its application in retrieval. J.\nDocumentation, 60(5):493–502.\nSu Nam Kim, Olena Medelyan, Min-Yen Kan, and Tim-\nothy Baldwin. 2010. Semeval-2010 task 5 : Auto-\nmatic keyphrase extraction from scientiﬁc articles.\nIn SemEval@ACL, pages 21–26. The Association for\nComputer Linguistics.\nM. Krapivin and M. Marchese. 2009. Large dataset for\nkeyphrase extraction.\nMayank Kulkarni, Debanjan Mahata, Ravneet Arora,\nand Rajarshi Bhowmik. 2022. Learning rich repre-\nsentation of keyphrases from text. In Findings of the\nAssociation for Computational Linguistics: NAACL\n2022, Seattle, WA, United States, July 10-15, 2022,\npages 891–906. Association for Computational Lin-\nguistics.\nNiraj Kumar and Kannan Srinathan. 2008. Automatic\nkeyphrase extraction from scientiﬁc documents using\nn-gram ﬁltration technique. In ACM Symposium on\nDocument Engineering, pages 199–208. ACM.\nXinnian Liang, Shuangzhi Wu, Mu Li, and Zhoujun Li.\n2021. Unsupervised keyphrase extraction by jointly\nmodeling local and global context. In Proceedings of\nthe 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 155–164, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nYang Liu and Mirella Lapata. 2019. Text summarization\nwith pretrained encoders. In EMNLP/IJCNLP (1),\npages 3728–3738. Association for Computational\nLinguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. CoRR, abs/1907.11692.\n2162\nZhiyuan Liu, Peng Li, Yabin Zheng, and Maosong\nSun. 2009. Clustering to ﬁnd exemplar terms for\nkeyphrase extraction. In EMNLP, pages 257–266.\nACL.\nDebanjan Mahata, John Kuriakose, Rajiv Ratn Shah,\nand Roger Zimmermann. 2018a. Key2vec: Auto-\nmatic ranked keyphrase extraction from scientiﬁc\narticles using phrase embeddings. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT, New\nOrleans, Louisiana, USA, June 1-6, 2018, Volume\n2 (Short Papers), pages 634–639. Association for\nComputational Linguistics.\nDebanjan Mahata, Rajiv Ratn Shah, John Kuriakose,\nRoger Zimmermann, and John R. Talburt. 2018b.\nTheme-weighted ranking of keywords from text doc-\numents using phrase embeddings. In IEEE 1st Con-\nference on Multimedia Information Processing and\nRetrieval, MIPR 2018, Miami, FL, USA, April 10-12,\n2018, pages 184–189. IEEE.\nRui Meng, Sanqiang Zhao, Shuguang Han, Daqing He,\nPeter Brusilovsky, and Yu Chi. 2017. Deep keyphrase\ngeneration. In ACL, pages 582–592. Association for\nComputational Linguistics.\nRada Mihalcea and Paul Tarau. 2004. Textrank: Bring-\ning order into text. In EMNLP, pages 404–411. ACL.\nFunan Mu, Zhenting Yu, Lifeng Wang, Yequan Wang,\nQingyu Yin, Yibo Sun, Liqun Liu, Teng Ma, Jing\nTang, and Xing Zhou. 2020. Keyphrase extrac-\ntion with span-based feature representations. CoRR,\nabs/2002.05407.\nDavid Newman, Nagendra Koilada, Jey Han Lau, and\nTimothy Baldwin. 2012. Bayesian text segmentation\nfor index term identiﬁcation and keyphrase extraction.\nIn COLING, pages 2077–2092. Indian Institute of\nTechnology Bombay.\nChau Q. Nguyen and Tuoi T. Phan. 2009. An ontology-\nbased approach for key phrase extraction. In\nACL/IJCNLP (Short Papers), pages 181–184. The\nAssociation for Computer Linguistics.\nThuy Dung Nguyen and Min-Yen Kan. 2007.\nKeyphrase extraction in scientiﬁc publications. In\nICADL, volume 4822 of Lecture Notes in Computer\nScience, pages 317–326. Springer.\nEirini Papagiannopoulou and Grigorios Tsoumakas.\n2018. Local word vectors guiding keyphrase extrac-\ntion. Inf. Process. Manag., 54(6):888–902.\nEirini Papagiannopoulou and Grigorios Tsoumakas.\n2019. A review of keyphrase extraction. CoRR,\nabs/1905.05044.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL-HLT, pages 2227–2237. Asso-\nciation for Computational Linguistics.\nDhruva Sahrawat, Debanjan Mahata, Haimin Zhang,\nMayank Kulkarni, Agniv Sharma, Rakesh Gosangi,\nAmanda Stent, Yaman Kumar, Rajiv Ratn Shah, and\nRoger Zimmermann. 2020. Keyphrase extraction as\nsequence labeling using contextualized embeddings.\nIn Advances in Information Retrieval - 42nd Euro-\npean Conference on IR Research, ECIR 2020, Lisbon,\nPortugal, April 14-17, 2020, Proceedings, Part II,\nvolume 12036 of Lecture Notes in Computer Science,\npages 328–335. Springer.\nArnav Saxena, Mudit Mangal, and Goonjan Jain. 2020.\nKeygames: A game theoretic approach to automatic\nkeyphrase extraction. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, pages 2037–2048.\nMingyang Song, Yi Feng, and Liping Jing. 2022a. Hy-\nperbolic relevance matching for neural keyphrase\nextraction. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL 2022, Seattle, WA, United States,\nJuly 10-15, 2022, pages 5710–5720. Association for\nComputational Linguistics.\nMingyang Song, Yi Feng, and Liping Jing. 2022b. Uti-\nlizing BERT intermediate layers for unsupervised\nkeyphrase extraction. In Proceedings of the 5th In-\nternational Conference on Natural Language and\nSpeech Processing (ICNLSP 2022), pages 277–281,\nTrento, Italy. Association for Computational Linguis-\ntics.\nMingyang Song, Liping Jing, and Lin Xiao. 2021. Im-\nportance Estimation from Multiple Perspectives for\nKeyphrase Extraction. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nSi Sun, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu,\nand Jie Bao. 2020a. Joint keyphrase chunking and\nsalience ranking with bert. CoRR, abs/2004.13639.\nYi Sun, Hangping Qiu, Yu Zheng, Zhongwei Wang,\nand Chaoran Zhang. 2020b. Sifrank: A new base-\nline for unsupervised keyphrase extraction based on\npre-trained language model. IEEE Access, 8:10896–\n10906.\nZhiqing Sun, Jian Tang, Pan Du, Zhi-Hong Deng, and\nJian-Yun Nie. 2019. Divgraphpointer: A graph\npointer network for extracting diverse keyphrases.\nIn SIGIR, pages 755–764. ACM.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBert rediscovers the classical nlp pipeline. In ACL\n(1), pages 4593–4601. Association for Computational\nLinguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019b. What do you\n2163\nlearn from context? probing for sentence struc-\nture in contextualized word representations. CoRR,\nabs/1905.06316.\nTakashi Tomokiyo and Matthew Hurst. 2003. A lan-\nguage model approach to keyphrase extraction. pages\n33–40. Association for Computational Linguistics.\nPeter D. Turney. 1999. Learning to extract keyphrases\nfrom text. National Research Council Canada, In-\nstitute for Information Technology, Technical Report\nERB-1057.\nPeter D. Turney. 2000. Learning algorithms for\nkeyphrase extraction. Inf. Retr., 2(4):303–336.\nXiaojun Wan and Jianguo Xiao. 2008a. Collabrank:\nTowards a collaborative approach to single-document\nkeyphrase extraction. In COLING, pages 969–976.\nXiaojun Wan and Jianguo Xiao. 2008b. Single doc-\nument keyphrase extraction using neighborhood\nknowledge. In AAAI, pages 855–860. AAAI Press.\nRui Wang, Wei Liu, and Chris McDonald. 2015. Using\nword embeddings to enhance keyword identiﬁcation\nfor scientiﬁc publications. In Databases Theory and\nApplications - 26th Australasian Database Confer-\nence, ADC 2015, Melbourne, VIC, Australia, June 4-\n7, 2015. Proceedings, volume 9093 of Lecture Notes\nin Computer Science, pages 257–268. Springer.\nYanan Wang, Qi Liu, Chuan Qin, Tong Xu, Yijun\nWang, Enhong Chen, and Hui Xiong. 2018. Ex-\nploiting topic-based adversarial neural network for\ncross-domain keyphrase extraction. In IEEE Inter-\nnational Conference on Data Mining, ICDM 2018,\nSingapore, November 17-20, 2018, pages 597–606.\nIEEE Computer Society.\nYansen Wang, Zhen Fan, and Carolyn Penstein Rosé.\n2020. Incorporating multimodal information in open-\ndomain web keyphrase extraction. In EMNLP (1),\npages 1790–1800. Association for Computational\nLinguistics.\nIan H. Witten, Gordon W. Paynter, Eibe Frank, Carl\nGutwin, and Craig G. Nevill-Manning. 1999. Kea:\nPractical automatic keyphrase extraction. In ACM\nDL, pages 254–255. ACM.\nLee Xiong, Chuan Hu, Chenyan Xiong, Daniel Cam-\npos, and Arnold Overwijk. 2019. Open domain web\nkeyphrase extraction beyond language modeling. In\nEMNLP/IJCNLP (1), pages 5174–5183. Association\nfor Computational Linguistics.\nWei You, Dominique Fontaine, and Jean-Paul A.\nBarthès. 2009. Automatic keyphrase extraction with\na reﬁned candidate set. In Web Intelligence, pages\n576–579. IEEE Computer Society.\nLinhan Zhang, Qian Chen, Wen Wang, Chong Deng,\nShiliang Zhang, Bing Li, Wei Wang, and Xin Cao.\n2021. Mderank: A masked document embedding\nrank approach for unsupervised keyphrase extraction.\nCoRR, abs/2110.06651.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\nating text generation with BERT. In8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nXingxing Zhang, Furu Wei, and Ming Zhou. 2019. HI-\nBERT: document level pre-training of hierarchical\nbidirectional transformers for document summariza-\ntion. CoRR, abs/1905.06566.\nYongzheng Zhang, A. Nur Zincir-Heywood, and Evan-\ngelos E. Milios. 2004. World wide web site summa-\nrization. Web Intell. Agent Syst., 2(1):39–53.\nMing Zhong, Pengfei Liu, Yiran Chen, Danqing Wang,\nXipeng Qiu, and Xuanjing Huang. 2020. Extractive\nsummarization as text matching. In ACL, pages 6197–\n6208. Association for Computational Linguistics.\nXuan Zhou, Xiao Zhang, Chenyang Tao, Junya Chen,\nBing Xu, Wei Wang, and Jing Xiao. 2021. Multi-\ngrained knowledge distillation for named entity\nrecognition. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2021, Online, June 6-11,\n2021, pages 5704–5716. Association for Computa-\ntional Linguistics.\n2164",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9008809328079224
    },
    {
      "name": "Metadata",
      "score": 0.691372811794281
    },
    {
      "name": "Natural language processing",
      "score": 0.6036758422851562
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5790989398956299
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5519925951957703
    },
    {
      "name": "Information retrieval",
      "score": 0.5207483768463135
    },
    {
      "name": "Information extraction",
      "score": 0.484140008687973
    },
    {
      "name": "Natural language",
      "score": 0.4305071532726288
    },
    {
      "name": "World Wide Web",
      "score": 0.19031235575675964
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}