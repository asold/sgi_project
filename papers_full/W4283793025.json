{
  "title": "Pushing the Limits of Rule Reasoning in Transformers through Natural Language Satisfiability",
  "url": "https://openalex.org/W4283793025",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2122050085",
      "name": "Kyle Richardson",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1898665253",
      "name": "Ashish Sabharwal",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2122050085",
      "name": "Kyle Richardson",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1898665253",
      "name": "Ashish Sabharwal",
      "affiliations": [
        "Allen Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2230472587",
    "https://openalex.org/W6783166674",
    "https://openalex.org/W2607964821",
    "https://openalex.org/W6773518419",
    "https://openalex.org/W6659573362",
    "https://openalex.org/W117707387",
    "https://openalex.org/W1938103548",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3090866633",
    "https://openalex.org/W2790415926",
    "https://openalex.org/W3155766495",
    "https://openalex.org/W2111224220",
    "https://openalex.org/W6634300341",
    "https://openalex.org/W3082583995",
    "https://openalex.org/W3172364764",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W6654419050",
    "https://openalex.org/W6653384912",
    "https://openalex.org/W1896064869",
    "https://openalex.org/W6677064485",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3023533951",
    "https://openalex.org/W2973362636",
    "https://openalex.org/W3092597885",
    "https://openalex.org/W6669539177",
    "https://openalex.org/W6748262478",
    "https://openalex.org/W6761126846",
    "https://openalex.org/W2968398601",
    "https://openalex.org/W3115201256",
    "https://openalex.org/W6630812432",
    "https://openalex.org/W3174693310",
    "https://openalex.org/W6725207838",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W3195112265",
    "https://openalex.org/W3015253856",
    "https://openalex.org/W4308233124",
    "https://openalex.org/W2075335084",
    "https://openalex.org/W2963143606",
    "https://openalex.org/W2997789497",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W1620410031",
    "https://openalex.org/W2963937837",
    "https://openalex.org/W1667362966",
    "https://openalex.org/W3167592944",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3159711399",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2412947200",
    "https://openalex.org/W2010999747",
    "https://openalex.org/W3173805051",
    "https://openalex.org/W2036265926",
    "https://openalex.org/W2115133449",
    "https://openalex.org/W3084470717",
    "https://openalex.org/W3172267148",
    "https://openalex.org/W2786776430",
    "https://openalex.org/W3017374003",
    "https://openalex.org/W2016239755",
    "https://openalex.org/W2963691697",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W3105516974",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W1525961042",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W1511783537",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1572277334",
    "https://openalex.org/W1650993530",
    "https://openalex.org/W3136035550",
    "https://openalex.org/W2080042577",
    "https://openalex.org/W3201251633",
    "https://openalex.org/W2924497240",
    "https://openalex.org/W4298325241",
    "https://openalex.org/W2971107062",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W3111372685"
  ],
  "abstract": "Investigating the reasoning abilities of transformer models, and discovering new challenging tasks for them, has been a topic of much interest. Recent studies have found these models to be surprisingly strong at performing deductive reasoning over formal logical theories expressed in natural language. A shortcoming of these studies, however, is that they do not take into account that logical theories, when sampled uniformly at random, do not necessarily lead to hard instances. We propose a new methodology for creating challenging algorithmic reasoning datasets that focus on natural language satisfiability (NLSat) problems. The key idea is to draw insights from empirical sampling of hard propositional SAT problems and from complexity-theoretic studies of language. This methodology allows us to distinguish easy from hard instances, and to systematically increase the complexity of existing reasoning benchmarks such as RuleTaker. We find that current transformers, given sufficient training data, are surprisingly robust at solving the resulting NLSat problems of substantially increased difficulty. They also exhibit some degree of scale-invariance—the ability to generalize to problems of larger size and scope. Our results, however, reveal important limitations too: careful sampling of training data is crucial for building models that generalize to larger problems, and transformer models’ limited scale-invariance suggests they are far from learning robust deductive reasoning algorithms.",
  "full_text": "Pushing the Limits of Rule Reasoning in Transformers\nthrough Natural Language Satisfiability\nKyle Richardson, Ashish Sabharwal\nAllen Institute for AI, Seattle, W A, USA\n{kyler,ashishs}@allenai.org\nAbstract\nInvestigating the reasoning abilities of transformer models,\nand discovering new challenging tasks for them, has been a\ntopic of much interest. Recent studies have found these models\nto be surprisingly strong at performing deductive reasoning\nover formal logical theories expressed in natural language.\nA shortcoming of these studies, however, is that they do not\ntake into account that logical theories, when sampled uni-\nformly at random, do not necessarily lead to hard instances.\nWe propose a new methodology for creating challenging al-\ngorithmic reasoning datasets that focus on natural language\nsatisfiability (NLSat) problems. The key idea is to draw in-\nsights from empirical sampling of hard propositional SAT\nproblems and from complexity-theoretic studies of language.\nThis methodology allows us to distinguish easy from hard\ninstances, and to systematically increase the complexity of\nexisting reasoning benchmarks such as RuleTaker. We find\nthat current transformers, given sufficient training data, are\nsurprisingly robust at solving the resulting NLSat problems of\nsubstantially increased difficulty. They also exhibit some de-\ngree of scale-invariance—the ability to generalize to problems\nof larger size and scope. Our results, however, reveal important\nlimitations too: a careful sampling of training data is crucial\nfor building models that generalize to larger problems, and\ntransformer models’ limited scale-invariance suggests they are\nfar from learning robust deductive reasoning algorithms.\nIntroduction\nMotivated by the impressive performance of recent pre-\ntrained transformers (Devlin et al. 2019; Raffel et al. 2020)\non a wide range of natural language understanding (NLU)\nbenchmarks (Wang et al. 2019b,a; Xu et al. 2020), there\nhas much been recent interest in investigating the linguis-\ntic and reasoning abilities of state-of-the-art neural models\n(Linzen, Dupoux, and Goldberg 2016; Talmor et al. 2020;\nKassner, Kroje, and Sch¨utze 2020; Yanaka et al. 2020; Hup-\nkes et al. 2020; Richardson et al. 2020,inter alia). One partic-\nular thread of work focuses on probing whether transformers\ncan perform logical reasoning over formal theories expressed\nin natural language (Clark, Tafjord, and Richardson 2020).\nSpecifically, given a set of systematically constructednatural\nlanguage theories consisting of a set of explicitly stated rules\nand facts (e.g., the NL Theory in the bottom part of Figure 1\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nHard Combinatorial Problem P\n(e.g., SAT, 3-Coloring)\nNatural Language Probing Task\nT (e.g., deductive inference,\nsyntactic processing,...)\nFind\nfragments of T grounded in P\nSample har\nd instances of P\nNL Theory Γ ={Bob is round. Alan is blue, rough and\nyoung. If someone is round then they are big.\nAll rough people are green. Big people are\nnot green. }\nConjectures 1. Alan is green (entailment, Γ |= 1)\n2. Bob is green (contradiction)\nSatisfiability\nΓ has an interpretation (sat)\nΓ ∪{¬1}(unsat), indirectly proves Γ |= 1\nΓ ∪{2}(unsat), indirectly proves Γ |= ¬2\nFigure 1: TOP: An illustration of our general methodology\nfor constructing hard natural language reasoning problems\nfor a task\nT , by grounding them into a hard combinatorial\nproblem P and sampling hard instances of P. BOTTOM: An\nexample of a natural language (NL) theory (i.e., set of arbi-\ntrary facts and rules) Γ along with two example conjectures\n(i.e., propositions to be proved) and the relationship between\nentailment and satisfiability.\ncontaining fictional rules about characters Bob and Alan), the\ngoal is to see whether a model can learn to perform deductive\nreasoning over such theories by correctly answering queries\nthat require making novel inferences (e.g., predicating that\nAlan is green is true based on knowing that Alan is rough\nand applying the rule All rough people are green).\nWhile much of this recent work on behavioral probing has\ncentered around small synthetic domains and datasets (see\nalso Weston et al. (2015); Lake and Baroni (2018); Sinha\net al. (2019)), the appeal of such testing is that it can allow us\nto uncover the strengths and weaknesses of models in a cost-\neffective and controlled manner, and ultimately determine\nwhether models are inherently capable of solving certain algo-\nrithmic problems. Given that most behavioral probing studies\nare performed in a black-box fashion (Ribeiro et al. 2020)\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n11209\nand are thus limited to input-output-driven testing, however,\nthe quality and informativeness of a probing study relies on\nhaving reliable data that faithfully captures the full target\nproblem space. In particular, to demonstrate that a model\ncan learn a certain algorithmic skill, it must be demonstrated\nthat the model can solve the hardest instances of the target\nproblem. Indeed, recent work (Shin et al. 2019; Wu et al.\n2021; Tamari et al. 2021) has revealed various pitfalls asso-\nciated with synthetic data due to ad-hoc sampling strategies,\nwhich can dramatically inflate model performance by under-\nsampling difficult cases in a way that can also harm model\ngeneralization.\nIn evaluating a particular diagnostic dataset for probing\nlogical reasoning, the following question arises: are the prob-\nlems contained in the target dataset hard in some objective\ncomputational sense? For example, while knowing that Bob\nis green is false in Figure 1 requires making multiple inferen-\ntial steps (i.e., combining the fact Bob is round with the two\nrules If someone is round then they are big and Big people\nare not green), the structure of the rules involved is such that\nthere are well-known highly efficient algorithms for comput-\ning this inference.1 A natural question, then, is: can models\nperform inferences involving more complex reasoning with\nrules? Answering the hardness question, therefore, involves\ntwo additional questions: (Q1) is the formal language used to\nexpress the target problem space capable of expressing hard\nproblems (e.g., ones that go beyond simple linear chaining)?\n(Q2) is the sampling method used to generate target instances\nable to effectively capture the full problem space?\nIn this paper, we fix the formal language to be expressive\nenough such that it can, by design, represent computationally\nhard problems (thereby addressing Q1). To address Q2, we\npropose a general methodology, illustrated in the top part\nof Figure 1. Given a target probing task T such as deductive\ninference over statements expressed in natural language, the\nkey idea is to identify subsets of T that map to a known hard\ncombinatorial reasoning problem P such as Boolean satisfia-\nbility (SAT), and devise methods to sample hard instances of\nP in order to arrive at hard instances of T .\nSpecifically, we broaden the scope of Clark, Tafjord, and\nRichardson (2020) to look at natural language satisfiabil-\nity (NLSat) problems, or types of natural language deduc-\ntive reasoning problems that formally assume an underlying\nSAT semantics. Using insights from empirical sampling of\nhard SAT problems (Selman, Mitchell, and Levesque 1996)\nwe show how to systematically construct computationally\ndifficult reasoning problems by focusing on such hard rule\nfragments and by sampling from the critical phase-change\nregions of SAT. We show that such an approach has twofold\nutility: 1) distinguishing easy from hard instances that are\nconsequential for training robust models and for reliable eval-\nuation and; 2) for diagnosing and increasing the complexity\nof existing reasoning benchmarks.\nOur results are partly positive: when provided with a suf-\n1More technically, such a query can be answered via a linear-\ntime process called unit propagation (cf. Zhang and Stickel (1996)),\nwhich is often treated as a pre-processing step in many modern\ntheorem provers and SAT solvers.\nficient amount of training instances (e.g., ¿100k examples),\nrecent pretrained transformers can indeed solve non-trivial\nNLSat problems that far exceed the complexity of existing\nreasoning benchmarks (e.g., achieving ¿90% accuracy on\nquantified rule theories containing up to 70 ground variables).\nThey also exhibit some degree of generalization and scale-\ninvariance, or the ability to generalize to problems of larger\nscope (e.g., generalizing from propositional theories with 12\nvariables to ones with 30, while maintaining performance far\nabove random chance).\nAt the same time, our results also reveal important caveats:\n1) the ability of a model to solve hard reasoning problems\ncritically relies on how well its training data is sampled and;\n2) the degree to which models are scale-invariant remains lim-\nited, suggesting that models trained in the standard paradigm\nare still far from learning the underlying algorithms needed\nfor robust deductive reasoning.\nRelated Work\nOur work follows the literature on behavioral testing of neu-\nral NLU models and builds on work by Clark, Tafjord, and\nRichardson (2020) on probing deductive reasoning, which\nhas spawned a number of subsequent studies (Saha et al.\n2020; Gontier et al. 2020; Betz, V oigt, and Richardson 2021;\nBetz, Richardson, and V oigt 2021; Tafjord, Mishra, and Clark\n2021; Saparov and Mitchell 2021; Liang, Bethard, and Sur-\ndeanu 2021). While these studies demonstrate that models\nare able to solve some deductive reasoning problems, we\nobserve that existing datasets narrowly focus on the simplest\ndeductive reasoning problems when subjected to closer anal-\nysis. As we detail in Table 1, the standard RuleTaker dataset,\nwhich is based on a fragment of English that is capable of\nexpressing intractably hard algorithmic problems, is limited\nto the easiest types of deductive reasoning problems. As a\nresult, existing models lack robustness when evaluated on\nharder parts of the problem distribution as we show in Table 4\non a RuleTaker-style dataset sampled using our new sampling\nstrategy.\nTo find hard reasoning fragments of natural language,\nwe take inspiration from the literature of complexity-\ntheoretic studies of various natural language fragments (Pratt-\nHartmann 2004; Pratt-Hartmann, Third et al. 2006; Pratt-\nHartmann and Moss 2009; Thorne and Calvanese 2010). Par-\nticularly, Pratt-Hartmann (2004) looks at the computational\nproperties such as the complexity of satisfiability for various\nrule fragments of English, which is the motivation behind the\ngrounded relative-clause fragment we describe in the next\nsection. While this work focuses on a worst-case analysis of\ndifferent linguistic phenomena, we use the results as a guide\nto find the hard cases for probing the limits of models.\nTo find hard natural language satisfiability instances, we\nrely on techniques from the literature on empirical sam-\npling of combinatorial problems, where it has been observed\n(Cheeseman et al. 1991) that hard instances of different prob-\nlems lie at various critical thresholds that correlate with the\nconstrainedness of a given problem. We specifically use tech-\nniques for generating hard 3SAT problems (Selman, Mitchell,\nand Levesque 1996; Cook and Mitchell 1997) to sample hard\nproblems from the critical regions of SAT phase transitions\n11210\nAlgorithm 1: Dataset construction via random SAT\nInput: Variables set V = {v1, ...vn}of size n, natural language\ntemplates Rand variables P, 2SAT to 3SAT interpolation pa-\nrameter pint, negation parameter pneg, clause variable ratio α\nrange (αmin, αmax), STOP condition\nOutput: NLSat dataset\n1: D ←{} ▷ initialize dataset\n2: repeat\n3: P ←{} ▷ problem/clause set\n4: m ∼choose m, s.t. αmin ≤m\nn ≤αmax\n5: for i := 1 to m do ▷ generate m clauses\n6: s ∼choose clause size k ∈(3,2) with prob. (pint, 1 −pint)\n7: V′ ∼choose s unique variables from V\n8: C ←negate each v ∈V ′ with pneg ▷ new clause\n9: t ∼choose NL template from Rof size s\n10: d ←instantiate t over C using variables from P\n11: P ←P ∪{d}\n12: D ←D ∪P\n13: until dataset STOP condition is met\n(see Figure 2). To our knowledge, we are first to investigate\nthis work and using SAT-based representations of linguistic\nproblems to empirically find hard natural language reasoning\nproblems (see Hahn, Jurafsky, and Futrell (2021)).\nOur study also follows other work on training neural mod-\nels to solve hard algorithmic problems (Vinyals, Fortunato,\nand Jaitly 2015; Reed and De Freitas 2015; Cai, Shin, and\nSong 2017), including SAT (Selsam et al. 2018) and propo-\nsitional inference (Evans et al. 2018; Traylor, Feiman, and\nPavlick 2021); a key difference is our focus on algorithmic\nproblems in natural language and on probing current pre-\ntrained transformers (Devlin et al. 2019; Liu et al. 2019;\nRaffel et al. 2020). Following studies such as Reed and\nDe Freitas (2015), we also look at the ability of models to be\nscale-invariant, or scale to problems of larger size and scope.\nWithin this space, we follow Shin et al. (2019); Wu et al.\n(2021) in developing novel sampling strategies for avoid-\ning the pitfalls of randomly sampled algorithmic datasets,\nwhich can give rise to the kinds of biases observed in human-\nannotated datasets (Gururangan et al. 2018) and limit model\ngeneralization.\nDataset Construction and Methodology\nNatural language satisfiability (NLSat) is a deductive reason-\ning task that involves determining whether a set of rules ex-\npressed in language (e.g., those shown in Figure 4) have a sat-\nisfying assignment or possible interpretation (Pratt-Hartmann\n2010).\n2 Following our general methodology shown in the top\npart of Figure 1, in order to find hard deductive reasoning\nproblems of this kind, we sample hard instances from or-\ndinary SAT problems in Boolean logic and translate them\ninto natural language using a pre-defined set of English rule\nlanguages.\nIn this section, we first detail the semantics of SAT and how\n2As we show in the lower part of Figure 1, logical entailment and\nsatisfiability (or its complement) end up being intereducible notions\nfor the logics under consideration. For basic results on the connec-\ntion, see Davis, Sigal, and Weyuker (1994)[Theorem2.1,p252].\nFigure 2: Illustration of phase-change and SAT probability\nfor random 2-SAT(pint = 0) and 3-SAT(pint = 1) problems\nover a randomly sampled set of examples with varying α and\nnumber of variables (5-15).\nto identify hard SAT problems (Identifying Hard Problems\nand Algorithm 1), and then describe the two different frag-\nments of English we use for our experiments (the Grounded\nRule Language and Grounded Relative Clause Fragment;\nsee details in Figure 3); both borrow certain grounded and\nquantified rule constructs from the RuleTaker language and,\nin the latter fragment, build on some constructions studied in\nformal linguistics. Finally, we discuss ways of sampling SAT\ninstances of different hardness levels and sizes.\nIdentifying Hard Problems\nThe SAT problem is the classic NP-complete problem (Cook\n1971). We focus on 3SAT problems where k = 3, i.e., each\nformula is limited to clauses of size three. While 3 SAT is\ncomputationally hard under a worst-case analysis, this does\nnot mean that all, or even most, 3SAT instances are hard to\nsolve. Indeed, work on empirical sampling of classes of ran-\ndom k-SAT problems has revealed that whether a problem is\ndifficult crucially relies on details about the target distribution\nfrom which the problems are sampled (Selman, Mitchell, and\nLevesque 1996; Mitchell and Levesque 1996) as well as the\nparticular parameters employed during sampling.\nTo obtain hard SAT instances, we rely on a variant of the\nwell-studied random k-SAT algorithm first introduced in Sel-\nman, Mitchell, and Levesque (1996), which we illustrate in\nAlgorithm 1. In standard k-SAT, random formulae of size m\ncontaining n variables and clauses of fixed length k are ob-\ntained by selecting m clauses (starting line 5) uniformly from\nthe space of 2k(n\nk\n)\npossible clauses (where each clause is con-\nstructed by sampling k unique variables (line 7) and negating\neach with probability pneg (line 8)). While our primary focus\nis on 3-SAT, for convenience we include the possibility of\nsampling mixed 2-SAT/3-SAT problems by introducing an in-\nterpolation parameter pint (shown on line 6, (Monasson et al.\n1999)). Using a suitable fragment of natural language R (see\nnext section), our version additionally includes translating\neach random clause to expressions in natural language (lines\n11211\nGrounded Rule Language (GRL) If (no) X and (no) Y then (not) Z.\nGrounded Relative Clause Frag-\nment (RCL)\nEvery X who is (not) a/an Y is (not) a/an Z. No X who is (not) a/an Y\nis a/an Z. Everyone who is (not) a/an X and (not) a/an Y is (not) a/an Z.\nc is (not) a/an X or (not) a/an Y or (not) a/an Z.\nFigure 3: A syntactic description of the two rule languages used for our experiments.\nLanguage Example Expression Satisfying Assignment\nPropositional Logic\n(3SAT)\n(¬v1 ∨ v15 ∨ v13) ∧ (¬v13 ∨ ¬v12 ∨ ¬v1) ∧ (v1 ∨\nv15 ∨ ¬v13) ∧ ...\nv1=false, v15=false, v13=false,\nv12=true...\nNatural Language Fragments\nGrounded\nRule\nLanguage (GRL)\nIf carrot and not steak then apples, If apples and grapes\nthen no carrots. If no carrots and no steak then not ap-\nples...\nneeded: carrots, apples,\ngrapes,...\nRelative\nClause\nFragment (RCL)\nEvery doctor who is not a philosopher is a baker. No\nbaker who is a gardener is a philosopher. John is a\ndoctor or a philosopher or not a baker...\nJohn can be a doctor, a baker,\nnot a philosopher and not a gar-\ndener...\nFigure 4: Example translations of a satisfiable 3SAT problem (truncated) in boolean logic and two fragments of English\n(variables in the natural language arehighlighted). The bottom shows example interpretations of each expression that demonstrate\nsatisfiability.\n9-10).\nA key parameter in random SAT is the clause to variable\nratio α = m\nn (computed on line 4 and dictated, in part, by\nthe range αmin, αmax). This parameter gives rise to phase-\nchange behavior that has implications for problem hardness\n(Hayes 2003). Such phase changes are illustrated in Figure 2,\nwhere α (x-axis) can be used to determine the probability\nof a random formula being satisfiable (y-axis). For our pur-\nposes, such a curve suggests a principled way to identify\nhard instances, namely, by selecting formulae from the criti-\ncal region where problems have roughly 0.5 probability of\nbeing satisfiable. The motivation behind sampling in this\nmanner follows much of the work in empirical SAT, where\nis it found that such problems are constrained in a unique\nway that makes it difficult to simply guess the correct answer\nby looking at the superficial patterns, which in our context\nmakes it harder for model to exploit short-cuts (we later pro-\nvide empirical evidence that narrowly focusing on training\ninstances close to the critical region leads to more robust\nmodels that generalize to the overall distribution better than\nmodels trained via ad-hoc sampling from the entire space).\nGrounded Rule Language (GRL)\nThe Grounded Rule Languageis a straightforward translation\nof the clauses in a random Boolean formula into grounded\npropositional (if-then) rules, similar to some of the rules used\nby Clark, Tafjord, and Richardson (2020). For example, as\ndetailed in Figures 3 and 4, a clause with three literals:\n±v1 ∨ ±v2 ∨ ±v3 (1)\ncan be translated asIf (no) v1 and (no) v3 then (no) v3 (using\nthe standard rules of logical equivalence) where each variable\nvj is subsequently replaced with an English count noun (i.e.,\nany noun that can be made plural and made into a singular\nform with the determiner a/an).\nWe choose a fixed set of around 50 nouns about food\nfor our main GRL set reported in Table 1 (see examples in\nFigure 4). Each instance in our main set is characterized by a\nvarying number of SAT variables or nouns, ranging from 5\nto 12, which we discuss and motivate below.\nWe note that while the propositions in these fragments (e.g.,\ncarrot, steak) deviate slightly from propositions encountered\nin ordinary language, one interpretation of the resulting theo-\nries is that they are akin to cooking recipes: e.g., if (you have)\ncarrot and not steak then (you need to have) apples. Figuring\nout whether the set of sentences is satisfiable is equivalent\nto deciding whether there is a coherent recipe underlying the\nrules. The decision to create data in a truncated form (i.e.,\nwithout verbs) is due to the following considerations: some\nof the transformer models we probe have strict token limits\nwhich are easily exceeded when expressing the target hard\ncomputational problems using longer phrases; and leaving\nout this information does not affect the complexity of the re-\nsulting reasoning problem that we are interested in probing.3\nIn the next section, we describe our second fragment that\naims to capture more conventional linguistic constructions.\nGrounded Relative Clause Fragment (RCL)\nThe grounded relative clause fragment is characterized by the\nrelative clause rule construction Every X who is (not) a Y is\n(not) a Z, which, via its translation from first-order logic:\n∀x. X(x) ∧ ±Y(x) → ±Z(y), (2)\ncorresponds to boolean clauses of the form¬v1 ∨±v2 ∨±v3\ncontaining up to two positive literals (where each variable\ncorresponds to a count noun, or predicatesX, Y, Z). To allow\n3Similar arguments are used to justify compositional reasoning\nprobing benchmarks, such as SCAN (Lake and Baroni 2018), which\ndeviate even more sharply from ordinary natural language.\n11212\nfor clauses with up to three positive literals, we add the rule\ntemplate Everything that is (not) an X and (not) a Y is (not)\na Z, where everything universally quantifiers over the entire\ndomain.\nWe obtain a mapping to propositional logic by assuming\nfinite domains following some theoretical studies on quan-\ntifiers (Westerst˚ahl et al. 1984; Szymanik et al. 2016) and\nwork on utilizing propositional logic for various reasoning\nproblems in classical AI (Kautz, Selman et al. 1992; Kautz,\nMcAllester, and Selman 1996). More specifically, grounding\nformulas such as Equation 2 relies on having clause transla-\ntions c is (not) a/an X or (not) a/an Y or (not) a/an Z that\nallow for introducing disjunctive facts that involve constants\nor proper nouns (denoted as c); given a set of universally\nquantified rules and such disjunctive facts, all universals rules\nare expanded to group propositions over all constants out to\narrive at a final grounded formula.\nCount and proper nouns are selected from a small inven-\ntory of noun types (as above, around 50) about people and\ntheir occupations (see again Figure 3). A particular feature\nof this fragment is that through such universally quantified\nrules and their expansion to propositional logic, we can ar-\nrive at more complex reasoning problems that significantly\nincreasing the number of ground variables and size of the\ntarget problems without dramatically increasing the size of\nthe natural language input. The rules in our data are sam-\npled from random 3SAT formulae over a fixed set of (5-8)\nvariables and are coupled with an additional set of random\nclauses for disjunctive rule instances. While these resulting\nboolean formulas deviate from strict random 3SAT, the ex-\npansion of universal rules over a set of constants preserves\nthe ratio of variables and clauses, which give rise to the same\nphase-change phenomena illustrated in Figure 2, allowing us\nto find the hard cases in the critical region.\nSampling Strategies and Proposed Datasets\nA summary of our datasets is shown in Table 1, along with a\ncomparison to the standard RuleTaker dataset converted to\nSAT.4 As described above, the NLSat instances that consti-\ntute the grounded rule language (GCL5,12) and the grounded\nrelative clause fragment (RCL16,70) are characterized by the\nnumber of variables contained in their underlying Boolean\nformulae (with d\n#vars denoting the overall range), which\nare uniformly represented in each dataset to allow for later\ninspection of model performance. For each variable amount,\nthe majority of Boolean formulae are sampled from the criti-\ncal 0.5 (±0.1) probability region by heuristically controlling\nthe (αmin, αmax) clause variable ratio range in Algorithm 1\n(henceforth, hard sampling5), which we later show leads to\nadvantages over to both naive sampling (i.e., choosing in-\nstances randomly within a large range) and biased sampling\n(i.e., sampling easy instances from the extreme ends of the\nphase change) strategies (see Figure 5). Formulae and their\ntranslations are then randomly split into train and evaluation\n4More details about this conversion and technical details about\nthe RuleTaker language can be found in the appendix.\n5We also added a small number of problems around the critical\nregion in training to encourage diversity.\nsets using a 80% (train) / 20% (dev,test) ratio.\nLanguage complexity and SAT metrics\nDataset\n(d#vars)\nSize Complexity\n(NP-complete?)\nConflicts\n(avg/med.)\nDecisions\n(avg/med.)\nRuleTaker130k yes 0.0,/0.0 6.6/0.0\nGRL5,12 187k yes 3.4/4.0 5.4/4.0\nRCL16,70 219k yes 7.6/6.0 29.7/6.0\nGRL-\neval20,50\n17k yes 22.0/13.0 29.3/13.0\nTable 1: The RuleTaker dataset, while similar in terms of\ndataset size and formal language complexity as our rule lan-\nguage datasets (GRL and RCL), is substantially simpler in\nterms of two standard SAT-based empirical complexity met-\nrics: number of conflicts and decisions.\nA particular advantage of having boolean formulae associ-\nated with our target data is that we can use automatic reason-\ning tools to obtain empirical measurements of problem hard-\nness. Using the off-the-shell theorem prover Z3 (De Moura\nand Bjørner 2008), we report the average/median (avg/med.)\nnumber of decisions (e.g., number of variable assignments\nafter pre-processing) and conflicts (amount of backtracking\nperformed for obtaining more complex proofs) needed by its\nsolve method on each datasets. While such statistics are often\ntied to the internals of a solver (especially #decisions), there\nstill are some notable observations.\nWe see from Table 1 that RuleTaker, in spite of its lan-\nguage’s high theoretical complexity, is limited to the simplest\nforms of deductive inference as evidenced by having very\nfew problems involving any conflicts and decisions at all (the\nmedian number for both is 0). In sharp contrast, our new\ndatasets, via our hard sampling strategy, offer a much wider\nrange of problem difficulty. By retrofitting our randomly sam-\npled 3SAT formula to include theories with 2SAT and single\npropositions similar to RuleTaker theories, we are also able\nto construct a substantially harder RuleTaker dataset (model\nperformance to be discussed in Table 4).6\nAn important caveat is that while our new problems are\nof a higher degree of difficulty compared with problems in\ndatasets like RuleTaker, they are still of vastly low complexity\n(both in terms of number of variables and the statistics shown\nin Table 1) compared to the much harder SAT instances en-\ncountered in the mainstream SAT literature (J¨arvisalo et al.\n2012). The decision to limit problems to the number of\nvariables we did (e.g., to a maximum of 12 variables for\nGRL5,12), is partly practical, and due to considerations such\nas token limits in the models we describe next and overall\ntraining efficiency. As we will see, these problems, while sim-\nple for mainstream SAT solvers, are still quite challenging\nfor state-of-the-art transformer models and thus valuable for\nadvancing research on the latter. Following Reed and De Fre-\nitas (2015), the decision also reflects the idea that we should\n6Such retrofitting involves modifying Algorithm 1 to allow for\nsampling of clause variables with replacement (in line 7) which\nyields clauses with repeated variables that we convert into 2SAT\nand units. More details are included in the appendix.\n11213\naim to train models that can perform scale-invariant reason-\ning by generalizing from small problems. For this purpose,\nwe create an additional held-out set of considerably larger\ngrounded rule reasoning problems GRL-eval20,50 to measure\nscale-invariance.\nExperimental Setup\nTask Definition. Formally, a NLSat dataset D =\n{(p(d), l(d))}|D|\nd consists of NLSat problems p (i.e., a set\nof rules expressed in natural language) paired with a label\nl ∈ {sat, unsat}. The goal is to correctly predict the label\n(indicating satisfiability or not), thereby reducing to binary\nclassification as in Clark, Tafjord, and Richardson (2020).\nModels. Following recent studies on rule reasoning\n(Tafjord, Mishra, and Clark 2021), our investigation cen-\nters around the pre-trained text-to-text transformer T5-large\nmodel (with around 770M parameters)7 (Raffel et al. 2020).\nWe also compare against RoBERTa (with around 355M\nparameters) (Liu et al. 2019). In each case we use the imple-\nmentation from Wolf et al. (2019). Standardly, models are\nfine-tuned to generate the target labels by optimizing for the\ncross-entropy loss over the target sat and unsat tokens or\nlabels. Also standardly, model selection is by performed by\ndoing a random search (in the style of Devlin et al. (2019))\nover target hyper-parameters (focusing especially on learning\nrate, random seed, and # training iterations) and selecting\nmodels with the highest dev. score. As mentioned above, we\nalso found intermediate pre-training on 60k simpler 2SAT in-\nstances (i.e., instances sampled with pint = 0 in Algorithm 1\nwith simpler natural language rule templates containing only\n2 propositions) to be indispensable for stabilizing and im-\nproving model training efficiency on our main tasks.\nEvaluation. We train models separately on our two lan-\nguages (GRL and RCL) and their respective datasets (see\nagain Table 1) in the manner described above. We report\naccuracy across sub-samples of evaluation data character-\nized by varying numbers of variables (i.e., the Xvar column\nin Tables 2-3). To better understand model generalization,\nwe also experiment with training on small samples of data\nwith a different number of variables for GRL as well as\nevaluation on a larger held-out GRL-eval set and easy and\nhard instances, as shown in Table 2. To better understand\nhow different sampling strategies affect model performance,\nwe perform experiments that measure the effect of different\nsampling strategies as shown in Figure 5.\nLastly, to verify the difficulty of our tasks, we also ex-\nperimented a non-pretrained biLSTM encoder model imple-\nmented using AllenNLP (Gardner et al. 2018). While not\nshown in the tables, we found, consistent with the results\nof Clark, Tafjord, and Richardson (2020), that such models\nperform near random chance.8\n7At an earlier iteration we also performed experiments T5-11b\nand found comparable performance. We note that a particular appeal\nof T5 is its use of relative positional embeddings which allows us\nto evaluate on larger problems such as those in our GRL-eval set\nthat exceed the 512 token limit from pre-training.\n8As a check, we also verified that the same models obtained\nAccuracy%\nModel (sampling) easy5,10 hard5,10\nGRL10 (T5) (biased) 88.4 77.1\nGRL10 (T5) (naive) 89.7 78.7\nGRL10 (T5) (hard) 92.4 86.4\nFigure 5: Training on hard problems (our proposal) is much\nmore effective than training on problems sampled in a naive\nor biased manner. TOP: Comparison of 10 variable model\ntrained using different sampling strategies and tested across\nthe full distribution of hard (problems in critical region) and\neasy (problems at extreme of distribution) 5 to 10 variable\nproblems (dev). BOTTOM: performance of the same 10 vari-\nable models on these different categories of problems.\nResults and Findings\nGiven our new set of hard algorithmic datasets, we aim to\nanswer the following general question: How well can our\nmain transformer models solve these types of hard deductive\nreasoning problems? As we describe in this section, while\ntransformers perform well on some portion of our tasks and\nexhibit some degree of generalization, they still seem far from\nimplementing the underlying algorithms needed for robust\ndeductive reasoning. We also emphasize the following subtle\npoint: knowing whether a model effectively solves a particu-\nlar algorithmic problem or probing task such as SAT critically\nrelies on understanding and specifying the target problem\ndistribution that is being used for model development.9\nEffective sampling strategies are important for training\nrobustness and reliable evaluation. To assess the effec-\ntiveness of our hard sampling strategy based on random\n3SAT, we performed a smaller-scale experiment that involves\ntraining 10var problems in the GRL language, as summa-\nrized in Figure 5. Here we see that selecting training instances\ncomparable results to the biLSTM baselines reported by Clark,\nTafjord, and Richardson (2020) on the original RuleTaker dataset.\n9Such is also a lesson from the literature on hard SAT. Quoting\nMitchell and Levesque (1996): Random formulas have been used by\nmany researchers to empirically evaluate the performance of SAT\ntesting programs. The value of such studies depends upon careful\nselection of formula distribution... When using random formulas, an\nextensive enough study of the distribution’s parameter space must\nbe carried out ... if the results are to be meaningful.\n11214\nDev Accur\nacy % for easy / hard (i.i.d and o.o.d) instances\nmain GRL splits (5\nto 12 variables problems) GRL-eval (20-50 variables)\nModelnum var 5var\n8var 10var 12var 20var\n30var 40var 50var\nT55 var 97.5 /\n95.9 89.0 /\n83.8 83.3 / 75.4 61.5 / 67.4 65.6 /\n60.3 59.7 / 53.5 50.8 / 50.2 50.0 / 50.0\nT58 var 96.2 /\n94.0 92.4 / 87.9 87.7 /\n81.6 73.6 / 74.8 74.4 /\n67.5 67.1 / 58.3 53.5 / 51.2 50.1 / 50.0\nT510 var 93.9 /\n89.7 92.7 / 86.3 89.7 / 82.5 79.0 /\n76.7 78.6 /\n70.0 71.2 / 60.1 54.7 / 51.4 50.1 / 50.0\nT512 var 94.5 /\n91.1 91.5 / 84.9 87.7 / 80.7 77.3 / 81.0 77.8 /\n70.1 70.7 / 60.3 53.3 / 51.4 50.0 / 50.0\nT55,12 98.6 /\n98.1 96.0 /\n93.6 92.6 /\n89.6 85.0 /\n88.5 86.5 / 80.7 84.9\n/ 72.7 69.8 / 61.4 59.1 / 51.8\nTable 2: Models exhibit limited generalization. Performance (dev) of models trained on GRL problems containing differing\nnumbers of a certain size and evaluated on easy and hard cases also of varying size. LEFT: Generalization across different\ncombinations of problem sizes for training and evaluation. RIGHT: Generalization to larger instances never seen during training.\nGrounded Rule Language GRL, Accuracy%\nModel 5var 7var 8var 10var 12var Avg.\nRandom 50.0 50.0 50.0 50.0 50.0 50.0\nT55,12 98.0 95.4 94.3 90.7 88.3 93.4\nRoBERTa5,12 96.4 92.0 90.2 85.4 83.4 89.5\nGrounded Relative Clause Language RCL, Accuracy%\nModel 16,21v 25,32v 35,48v 60,70v Avg.\nRandom 50.0 50.0 50.0 51.2 50.3\nT516,70 95.9 95.3 94.7 92.9 94.7\nRoBERTa16,70 96.0 95.9 94.9 94.0 95.2\nTable 3: Models trained on hard sets are surprisingly good\nat some hard tasks in the i.i.d. setting. Performance (test)\nof models on the GRL and RCL fragments, split into perfor-\nmance on problems with differing number of variables.\nfrom the critical regions in 3SAT phase-changes (i.e., our de-\nfault hard sampling strategy) allows models to generalize\nto the overall problem distribution and across other variable\nsizes (top figure). In contrast, naively selecting from random\nparts of the distribution is not much better than selecting\nonly from extreme (easy) ends of the distribution (biased\nsampling), and both lead to models not performing as well (8\npoints lower) on the hard subset of the evaluation set. Notably,\nthe closeness of results between naive and biased sampling\nsuggests that selecting deductive reasoning data in an ad\nhoc fashion might often give rise to certain biases that harm\nrobustness in a way similar to entirely biased sampling.\nModel accuracy (%)\nevaluation Majority RT-T5 RT-RoBERTa\nRuleTaker (RT) 43.0 97.5 98.7(standard i.i.d.)\nHard RT 50.0 57.7 59.6(our methodology)\nTable 4: While RT models excel on standard RuleTaker eval-\nuation, they perform close to random on a Hard RT challenge\nset constructed using our methodology.\nThe importance of sampling for having reliable evalua-\ntions is further revealed in our experiments on sampling\nhard RuleTaker evaluation data from hard SAT, as shown\nin Table 4. While it is unclear what the exact distribution\nof problems defined in the RuleTaker domain is, the results\nin this table clearly demonstrate the efficacy of our general\nsampling framework in identifying hard datasets. They also\nshow that even small changes in problem difficulty (namely,\nthe relatively modest increase in standard empirical hardness\nmeasures, #conflicts and #decisions, as seen earlier in Ta-\nble 1) can lead to dramatic differences in performance on\nexisting benchmarks (e.g., a 39 point drop in performance for\nRoBERTa). This is reinforced by the differences in results be-\ntween the easy/hard problems shown in Table 2 (e.g., 80% vs.\n71% (avg) performance difference between easy/hard 20-40\nvariable problems for the full T55,12 model). Thus, without\na proper understanding of the full distribution of target prob-\nlems, it is often easy to draw inaccurate general conclusions\nabout model capability by inadvertently focusing on easy\ninstances.\nModels trained on hard sets can solve some hard tasks.\nWhen looking at results on the hard instances (Table 3) we\nsee that models trained on large collections of various types\n(i.e., on 150k-160k instances, see again Table 1) far outpace\nour baselines and achieve high performance on problems\nwith not too many variables (e.g., 5-7 variables problems for\nGRL and 16-48 variables problems for RCL). A particularly\nintriguing result is the higher performance of models on the\nRCL language (with around 93-94% accuracy on problems\nwith 60-70 ground variables) which was designed to be more\ncomplex by having quantified rules and constants that ex-\npand out to a much larger set of boolean variables. Given that\nthe underlying rules were constructed from random 3SAT\nformula with a relatively smaller set of variables (5-8), this\nsuggests that the model is able to learn some form of sym-\nmetry between the underlying rules and the instantiated rule\npropositions related to constants.\nModels exhibit limited generalization. Less impressive\nresults are shown in the Table 2, where we see that models\ntrained on small variable problems and fewer data fail to\ngeneralize to larger problems (e.g., generalizing from 5 vari-\nables to 10 or 12 variables). More strikingly, we see that even\nour best models fail to solve the GPL-eval evaluation task;\nwhile this is not altogether surprising, it suggests that state-\nof-the-art transformer models are still far from learning the\nunderlying algorithms associated with deductive inference.\n11215\nClosing Remarks\nWith the advent of increasingly larger pre-trained models,\nincluding those that now allow for processing of tens of thou-\nsands of tokens (Beltagy, Peters, and Cohan 2020), under-\nstanding the limits of how much aggregation of information\nover text models are capable of is an important area of study.\nGiven that the type of algorithmic tasks we study in this paper\nare concerned with the most complex forms of information\naggregation, we believe that our results can bear on these\nbigger issues about model design. When optimized for prob-\nlem hardness, we see that models on our datasets still exhibit\nlittle ability to generalize in a scale-invariant fashion that is\nrequired for effectively generalizing their reasoning abilities\nto larger problems. Moving forward, we believe that new\nmodeling approaches and architectures (e.g., ones that focus\non problem decomposition (Andreas et al. 2016; Khot et al.\n2021)) might be a fruitful avenue, which we believe our new\nalgorithmic tasks and sampling strategies for finding hard\ndatasets can assist in exploring.\nAppendix\nRuleTaker Details\nComplexity of RuleTaker Language The rules in the orig-\ninal RuleTaker language (Clark, Tafjord, and Richardson\n2020) take two general forms: grounded rules and quan-\ntified rules, a subset of which is shown in Figure 7. To\ndemonstrate the NP-completeness of the RuleTaker language,\nit suffices to show that an arbitrary 3SAT formulaF can be\nexpressed in this rule language such that F is satisfiable if\nand only if the resulting RuleTaker theory is satisfiable (under\npropositional semantics). To this end, we observe that there\nare\n4 distinct atomic forms of a 3SAT clause corresponding\nto 0, 1, 2, or 3 positive literals (after accounting for logical\nequivalences obtained via the commutativity of disjunction).\nAll of these atomic forms, denoted ±X ∨ ±Y ∨ ±Z (where\n± indicates the literal may be positive or negated), can be\nrepresented by one of the aforementioned grounded rules,\nwith appropriately placed not modifiers:\nThe original RuleTaker dataset includes instantiations\nof the above single rule that cover the 4 distinct atomic\nforms.10 A similar argument can be made for proving the\nNP-completeness of our other languages (for background on\nNL complexity, see Pratt-Hartmann (2004, 2010)).\nRetrofitting random 3SAT to RuleTaker Theories An\nexample of how we retrofit random 3SAT to create hard Rule-\nTaker instances is shown in Figure 6. Given that RuleTaker\ntheories (see again the example in Figure 1) includes both\n2SAT clauses (i.e., rules corresponding to clauses with two\npropositions, e.g., If the lion is red then it is rough, in clausal\nform: ¬A∨B) and units (i.e., clauses with single propositions,\ne.g., The lion is red or A), a particular difficulty is converting\n10Some corresponding rules from the original dataset: If the tiger\nis not big and the tiger is not blue then the tiger is cold, If the mouse\nis kind and the mouse is green then the mouse is blue, If the tiger is\nyoung and the tiger is big then the tiger is not blue, If the tiger is\nnot blue and the tiger is not young then the tiger is not green.\nrandom 3SAT formula (where each clause contains exactly 3\npropositions, e.g., A ∨ B ∨ C) to such forms.\nOur idea is to modify Algorithm 1 to allow for repeated\nclause variables that we can subsequently convert to 2SAT\nand units; technically this amounts to altering line 7 to allow\nfor sampling with replacement such that we can produce\nclauses of the following form: A ∨A ∨A that we can convert\nto facts (e.g.,The lion is red). As in the ordinary application of\nAlgorithm 1, such a procedure can be performed to produce\nboolean formulae containing a differing number of variables.\nTo keep the problems of comparable size to the original\nRuleTaker, we created problems using a mixture of 5,6,7\nboolean variables. As a consequence, these problems are\nstill of relatively low complexity comparing to the types of\nreasoning problems we pursue in our new datasets.\nRuleTaker version We use the open world assumption\n(OW A) version of RuleTaker from Tafjord, Mishra, and Clark\n(2021)11. In contrast to the initial version of the dataset\nfrom Clark, Tafjord, and Richardson (2020), which makes\na closed-world assumption (CWA) and is limited to two-\nway entailment classification, the OW A include three classes:\nYes (entailment), No (contradiction), Unknown.\nTo verify the correctness of the semantics, we compared\nagainst a manual SAT-based and SMT-based implementa-\ntion of the RuleTaker language, which is available at https:\n//github.com/allenai/language fragments. We found around\n1% mismatched labels between the official dataset due to ap-\nparent errors in the translation from the CW A dataset and per-\nformed experiments on the corrected version of the dataset.\nAcknowledgments\nWe thank the members of the Aristo team at AI2 for their\nfeedback at various stages of this work, in particular Peter\nClark and Oyvind Tafjord, as well as the Beaker team (https://\nbeaker.org/) for support with experiments. Special thanks also\nto Gregor Betz and Christian V oigt for helpful discussions.\nReferences\nAndreas, J.; Rohrbach, M.; Darrell, T.; and Klein, D. 2016.\nLearning to compose neural networks for question answering.\nIn NAACL.\nBeltagy, I.; Peters, M. E.; and Cohan, A. 2020. Long-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150.\nBetz, G.; Richardson, K.; and V oigt, C. 2021. Think-\ning Aloud: Dynamic Context Generation Improves Zero-\nShot Reasoning Performance of GPT-2. arXiv preprint\narXiv:2103.13033.\nBetz, G.; V oigt, C.; and Richardson, K. 2021. Critical Think-\ning for Language Models. Proceedings of IWCS.\nCai, J.; Shin, R.; and Song, D. 2017. Making neural program-\nming architectures generalize via recursion. In ICLR.\n11Publicly available at: https://allenai.org/data/proofwriter. We\ntrained models on the depth-3ext, which was empirically shown to\nhave high generalization across the different depth reasoning tasks\nin Clark, Tafjord, and Richardson (2020).\n11216\nStep 1: Find random 3SAT instances\nwith modification to Algorithm 1 that\nallows for clauses with repeated vari-\nables (i.e., removing the uniqueness\nconstraint on line 7 to sample with re-\nplacement)\n(v1∨¬v2∨¬v5)∧(¬v2∨¬v3∨¬v4)∧(¬v5 ∨¬v5 ∨¬v5)| {z }\nrepeat (unit)\n∧(v1 ∨v1 ∨v1)| {z }\nrepeat (unit)\n∧(v3∨\nv4 ∨¬v2) ∧(¬v2 ∨¬v3 ∨¬v1) ∧... ∧(¬v1 ∨¬v1 ∨v3)| {z }\nrepeat (2SAT)\nStep 2 Remove repeats, split formula\ninto rules (i.e., 2/3 SAT clauses) and\nfacts (i.e., units); find problems whose\nrules are satisfiable.\n(v1 ∨¬v2 ∨¬v5) ∧(¬v2 ∨¬v3 ∨¬v4) ∧(v3 ∨v4 ∨¬v2)...| {z }\nrules(sat.)\n∧¬v 5 ∧v1| {z }\nfacts\nStep 3 Translate rules and facts to En-\nglish using the RuleTaker templates\nfrom Figure 7. Treat some facts ascon-\njectures, or the queries to be proven\ngiven the Rules and Facts.\nRules: If the lion is not red| {z }\n¬v1\nand the lion is round\n|\n{z }\nv2\nthen the lion is not green| {z }\n¬v5\n. If the\nlion is round| {z }\nv2\nand the lion is young| {z }\nv3\nthen the lion is not rough| {z }\n¬v4\n...\nFacts: The lion is not green| {z }\n¬v5\n... Conjecture: The lion is red| {z }\nv1\n.\nFigure 6: An illustration of the retrofitting algorithm used to find hard RuleTaker theories (rules and facts) from random 3SAT\nusing a contrived example with grounded rules over 5 variables.\nGround Rules If the c is (not) X then the c is (not) Y.\nIf the c is (not) X and the c is (not) Y\nthen the c is (not) Z.\nQuantified Rules If something is X and (not) Y then it is\n(not) Z. If something is (not) X then it\nis (not) Y. All X, Y things are (not) Z\nFigure 7: A subset of the rule templates encountered in\nthe original RuleTaker language from Clark, Tafjord, and\nRichardson (2020).\nclausal form rule translation\n±X ∨±Y ∨±Z If the c is (not) X and the c is (not)\nY then the c is (not) Z.\nCheeseman, P. C.; Kanefsky, B.; Taylor, W. M.; et al. 1991.\nWhere the really hard problems are. In IJCAI, volume 91,\n331–337.\nClark, P.; Tafjord, O.; and Richardson, K. 2020. Transformers\nas soft reasoners over language. In IJCAI.\nCook, S. A. 1971. The complexity of theorem-proving pro-\ncedures. In Proceedings of the third annual ACM symposium\non Theory of computing, 151–158.\nCook, S. A.; and Mitchell, D. G. 1997. Finding hard instances\nof the satisfiability problem: A survey. Satisfiability Problem:\nTheory and Applications, 35: 1–17.\nDavis, M.; Sigal, R.; and Weyuker, E. J. 1994. Computabil-\nity, complexity, and languages: fundamentals of theoretical\ncomputer science. Elsevier.\nDe Moura, L.; and Bjørner, N. 2008. Z3: An efficient\nSMT solver. In International conference on Tools and Algo-\nrithms for the Construction and Analysis of Systems, 337–340.\nSpringer.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of deep bidirectional transformers for\nlanguage understanding. In NAACL.\nEvans, R.; Saxton, D.; Amos, D.; Kohli, P.; and Grefenstette,\nE. 2018. Can Neural Networks Understand Logical Entail-\nment? Proceedings of ICLR.\nGardner, M.; Grus, J.; Neumann, M.; Tafjord, O.; Dasigi, P.;\nLiu, N.; Peters, M.; Schmitz, M.; and Zettlemoyer, L. 2018.\nAllenNLP: A deep semantic natural language processing\nplatform. arXiv preprint arXiv:1803.07640.\nGontier, N.; Sinha, K.; Reddy, S.; and Pal, C. 2020. Measur-\ning systematic generalization in neural proof generation with\ntransformers. In NeurIPS.\nGururangan, S.; Swayamdipta, S.; Levy, O.; Schwartz, R.;\nBowman, S. R.; and Smith, N. A. 2018. Annotation artifacts\nin natural language inference data. In NAACL.\nHahn, M.; Jurafsky, D.; and Futrell, R. 2021. Sensitivity\nas a Complexity Measure for Sequence Classification Tasks.\nTACL.\nHayes, B. 2003. Computing Science: On the Threshold.\nAmerican Scientist, 91(1): 12–17.\nHupkes, D.; Dankers, V .; Mul, M.; and Bruni, E. 2020. Com-\npositionality decomposed: how do neural networks gener-\nalise? JAIR, 67: 757–795.\nJ¨arvisalo, M.; Le Berre, D.; Roussel, O.; and Simon, L. 2012.\nThe international SAT solver competitions. AI Magazine,\n33(1): 89–92.\nKassner, N.; Kroje, B.; and Sch¨utze, H. 2020. Are Pre-trained\nLanguage Models as Symbolic Reasoners over Knowledge?\nIn CoNLL.\nKautz, H.; McAllester, D.; and Selman, B. 1996. Encoding\nplans in propositional logic. KR, 96: 374–384.\nKautz, H. A.; Selman, B.; et al. 1992. Planning as Satisfiabil-\nity. In ECAI, volume 92, 359–363.\nKhot, T.; Khashabi, D.; Richardson, K.; Clark, P.; and Sab-\nharwal, A. 2021. Text modular networks: Learning to decom-\npose tasks in the language of existing models. Proceedings\nof NAACL.\n11217\nLake, B.; and Baroni, M. 2018. Generalization without\nsystematicity: On the compositional skills of sequence-to-\nsequence recurrent networks. In ICML, 2873–2882. PMLR.\nLiang, Z.; Bethard, S.; and Surdeanu, M. 2021. Explainable\nMulti-hop Verbal Reasoning Through Internal Monologue.\nIn NAACL, 1225–1250.\nLinzen, T.; Dupoux, E.; and Goldberg, Y . 2016. Assessing\nthe ability of LSTMs to learn syntax-sensitive dependencies.\nTACL, 4: 521–535.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V . 2019.\nRoBERTa: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692.\nMitchell, D. G.; and Levesque, H. J. 1996. Some pitfalls\nfor experimenters with random SAT. Artificial Intelligence,\n81(1-2): 111–125.\nMonasson, R.; Zecchina, R.; Kirkpatrick, S.; Selman, B.; and\nTroyansky, L. 1999. Determining computational complexity\nfrom characteristic ‘phase transitions’. Nature, 400(6740):\n133–137.\nPratt-Hartmann, I. 2004. Fragments of language. Journal of\nLogic, Language and Information, 13(2): 207–223.\nPratt-Hartmann, I. 2010. Computational complexity in natu-\nral language. The handbook of computational linguistics and\nnatural language processing, 57.\nPratt-Hartmann, I.; and Moss, L. S. 2009. Logics for the\nrelational syllogistic. The Review of Symbolic Logic, 2(4):\n647–683.\nPratt-Hartmann, I.; Third, A.; et al. 2006. More fragments\nof language. Notre Dame Journal of Formal Logic, 47(2):\n151–177.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-\ning the limits of transfer learning with a unified text-to-text\ntransformer. JMLR.\nReed, S.; and De Freitas, N. 2015. Neural programmer-\ninterpreters. arXiv preprint arXiv:1511.06279.\nRibeiro, M. T.; Wu, T.; Guestrin, C.; and Singh, S. 2020.\nBeyond accuracy: Behavioral testing of NLP models with\nCheckList. In ACL.\nRichardson, K.; Hu, H.; Moss, L.; and Sabharwal, A. 2020.\nProbing natural language inference models through semantic\nfragments. In AAAI-2020, 8713–8721.\nSaha, S.; Ghosh, S.; Srivastava, S.; and Bansal, M. 2020.\nPRover: Proof generation for interpretable reasoning over\nrules. In EMNLP.\nSaparov, A.; and Mitchell, T. M. 2021. A Generative Sym-\nbolic Model for More General Natural Language Understand-\ning and Reasoning. arXiv preprint arXiv:2105.02486.\nSelman, B.; Mitchell, D. G.; and Levesque, H. J. 1996. Gen-\nerating hard satisfiability problems. Artificial intelligence,\n81(1-2): 17–29.\nSelsam, D.; Lamm, M.; B ¨unz, B.; Liang, P.; de Moura, L.;\nand Dill, D. L. 2018. Learning a SAT solver from single-bit\nsupervision. In ICLR.\nShin, R.; Kant, N.; Gupta, K.; Bender, C.; Trabucco, B.;\nSingh, R.; and Song, D. 2019. Synthetic datasets for neural\nprogram synthesis. Proceedings of ICLR.\nSinha, K.; Sodhani, S.; Dong, J.; Pineau, J.; and Hamilton,\nW. L. 2019. CLUTRR: A diagnostic benchmark for inductive\nreasoning from text. In EMNLP.\nSzymanik, J.; et al. 2016. Quantifiers and cognition: Logical\nand computational perspectives, volume 96. Springer.\nTafjord, O.; Mishra, B. D.; and Clark, P. 2021. Proofwriter:\nGenerating implications, proofs, and abductive statements\nover natural language. ACL Findings.\nTalmor, A.; Elazar, Y .; Goldberg, Y .; and Berant, J. 2020.\noLMpics–On what Language Model Pre-training Captures.\nTACL.\nTamari, R.; Richardson, K.; Sar-Shalom, A.; Kahlon, N.; Liu,\nN.; Tsarfaty, R.; and Shahaf, D. 2021. Dyna-bAbI: unlock-\ning bAbI’s potential with dynamic synthetic benchmarking.\narXiv preprint arXiv:2112.00086.\nThorne, C.; and Calvanese, D. 2010. The data complexity of\nthe syllogistic fragments of English. In Logic, Language and\nMeaning, 114–123. Springer.\nTraylor, A.; Feiman, R.; and Pavlick, E. 2021. AND does\nnot mean OR: Using Formal Languages to Study Language\nModels’ Representations. In Proceedings of ACL.\nVinyals, O.; Fortunato, M.; and Jaitly, N. 2015. Pointer\nnetworks. In NeurIPS.\nWang, A.; Pruksachatkun, Y .; Nangia, N.; Singh, A.; Michael,\nJ.; Hill, F.; Levy, O.; and Bowman, S. R. 2019a. SuperGLUE:\nA stickier benchmark for general-purpose language under-\nstanding systems. In NeurIPS.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2019b. GLUE: A multi-task benchmark and\nanalysis platform for natural language understanding. In\nICLR.\nWesterst˚ahl, D.; et al. 1984. Some results on quantifiers.\nNotre Dame Journal of Formal Logic, 25(2): 152–170.\nWeston, J.; Bordes, A.; Chopra, S.; Rush, A. M.; van\nMerri¨enboer, B.; Joulin, A.; and Mikolov, T. 2015. Towards\nAI-complete question answering: A set of prerequisite toy\ntasks. arXiv preprint arXiv:1502.05698.\nWolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; et al.\n2019. Huggingface’s transformers: State-of-the-art natural\nlanguage processing. arXiv preprint arXiv:1910.03771.\nWu, Z.; Kreiss, E.; Ong, D. C.; and Potts, C. 2021. ReaS-\nCAN: Compositional Reasoning in Language Grounding. In\nNeurIPS 2021 Datasets and Benchmarks Track.\nXu, L.; Hu, H.; Zhang, X.; Li, L.; Cao, C.; Li, Y .; Xu, Y .;\nSun, K.; Yu, D.; Yu, C.; Tian, Y .; Dong, Q.; Liu, W.; Shi, B.;\nCui, Y .; Li, J.; Zeng, J.; Wang, R.; Xie, W.; Li, Y .; Patterson,\nY .; Tian, Z.; Zhang, Y .; Zhou, H.; Liu, S.; Zhao, Z.; Zhao, Q.;\nYue, C.; Zhang, X.; Yang, Z.; Richardson, K.; and Lan, Z.\n2020. CLUE: A Chinese Language Understanding Evaluation\nBenchmark. In COLING.\n11218\nYanaka, H.; Mineshima, K.; Bekki, D.; and Inui, K. 2020.\nDo Neural Models Learn Systematicity of Monotonicity In-\nference in Natural Language? In ACL.\nZhang, H.; and Stickel, M. E. 1996. An Efficient Algorithm\nfor Unit Propagation. Proc. of AI-MATH, 96.\n11219",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6506966352462769
    },
    {
      "name": "Satisfiability",
      "score": 0.6145530343055725
    },
    {
      "name": "Natural language",
      "score": 0.5485235452651978
    },
    {
      "name": "Deductive reasoning",
      "score": 0.5337990522384644
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5026659965515137
    },
    {
      "name": "Boolean satisfiability problem",
      "score": 0.4509645104408264
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4254686236381531
    },
    {
      "name": "Automated reasoning",
      "score": 0.42120257019996643
    },
    {
      "name": "Natural language understanding",
      "score": 0.4157392680644989
    },
    {
      "name": "Transformer",
      "score": 0.41528159379959106
    },
    {
      "name": "Machine learning",
      "score": 0.3468638062477112
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210140341",
      "name": "Allen Institute",
      "country": "US"
    }
  ],
  "cited_by": 10
}