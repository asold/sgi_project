{
  "title": "BiTeM at WNUT 2020 Shared Task-1: Named Entity Recognition over Wet Lab Protocols using an Ensemble of Contextual Language Models",
  "url": "https://openalex.org/W3101381894",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2414130666",
      "name": "Julien Knafou",
      "affiliations": [
        "HES-SO University of Applied Sciences and Arts Western Switzerland",
        "SIB Swiss Institute of Bioinformatics",
        "University of Geneva"
      ]
    },
    {
      "id": "https://openalex.org/A2107579764",
      "name": "Nona Naderi",
      "affiliations": [
        "SIB Swiss Institute of Bioinformatics",
        "HES-SO University of Applied Sciences and Arts Western Switzerland"
      ]
    },
    {
      "id": "https://openalex.org/A2531790239",
      "name": "Jenny Copara",
      "affiliations": [
        "SIB Swiss Institute of Bioinformatics",
        "HES-SO University of Applied Sciences and Arts Western Switzerland",
        "University of Geneva"
      ]
    },
    {
      "id": "https://openalex.org/A2234087575",
      "name": "Douglas Teodoro",
      "affiliations": [
        "HES-SO University of Applied Sciences and Arts Western Switzerland",
        "SIB Swiss Institute of Bioinformatics"
      ]
    },
    {
      "id": "https://openalex.org/A1751439148",
      "name": "Patrick Ruch",
      "affiliations": [
        "HES-SO University of Applied Sciences and Arts Western Switzerland",
        "SIB Swiss Institute of Bioinformatics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3005700362",
    "https://openalex.org/W3114551148",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2964242047",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3044739286",
    "https://openalex.org/W4287705547",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2962798796",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3102400851"
  ],
  "abstract": "Recent improvements in machine-reading technologies attracted much attention to automation problems and their possibilities. In this context, WNUT 2020 introduces a Name Entity Recognition (NER) task based on wet laboratory procedures. In this paper, we present a 3-step method based on deep neural language models that reported the best overall exact match F1-score (77.99%) of the competition. By fine-tuning 10 times, 10 different pretrained language models, this work shows the advantage of having more models in an ensemble based on a majority of votes strategy. On top of that, having 100 different models allowed us to analyse the combinations of ensemble that demonstrated the impact of having multiple pretrained models versus fine-tuning a pretrained model multiple times.",
  "full_text": "Proceedings of the 2020 EMNLP Workshop W-NUT: The Sixth Workshop on Noisy User-generated Text, pages 305–313\nOnline, Nov 19, 2020.c⃝2020 Association for Computational Linguistics\n305\nBiTeM at WNUT 2020 Shared Task-1: Named Entity Recognition over\nWet Lab Protocols using an Ensemble of Contextual Language Models\nJulien Knafou1,2,3, Nona Naderi1,2, Jenny Copara1,2,3, Douglas Teodoro1,2, and Patrick Ruch1,2\nEmails : {ﬁrstname.lastname}@hesge.ch\n1University of Applied Sciences and Arts of Western Switzerland\n2Swiss Institute of Bioinformatics, Geneva, Switzerland\n3University of Geneva, Switzerland\nAbstract\nRecent improvements in machine-reading\ntechnologies attracted much attention to au-\ntomation problems and their possibilities. In\nthis context, WNUT 2020 introduces a Name\nEntity Recognition (NER) task based on wet\nlaboratory procedures. In this paper, we\npresent a 3-step method based on deep neural\nlanguage models that reported the best overall\nexact match F1-score (77.99%) of the compe-\ntition. By ﬁne-tuning 10 times, 10 different\npretrained language models, this work shows\nthe advantage of having more models in an en-\nsemble based on a majority of votes strategy.\nOn top of that, having 100 different models\nallowed us to analyse the combinations of en-\nsemble that demonstrated the impact of having\nmultiple pretrained models versus ﬁne-tuning\na pretrained model multiple times.\n1 Introduction\nThe last decades have seen both the amount and the\ncomplexity of biological experiments grow. Cou-\npling this phenomenon with the improvement in\nmachine-reading technologies seem to have led\nresearchers to look for ways to automate wet lab-\noratory procedures. Such technologies should al-\nlow reproducibility while reducing human errors\nin the process. However, as current protocols are\nusually written in a natural language, a collec-\ntion of wet laboratory protocols annotated with\nentities and relations would help assess current\nmachine-reading performances in this speciﬁc set-\nting (Kulkarni et al., 2018).\nIn this context, WNUT (Workshop on Noisy\nUser-generated Text1) 2020 (Tabassum et al., 2020)\nproposes two tasks, a Named Entity Recognition\n(NER) task and a Relation Extraction (RE) task. In\nthis paper, we present a 3-step method we used\nfor the NER task. Our approach is essentially\n1http://noisy-text.github.io/2020/\nwlp-task.html\nbased on a deep neural language models supported\nby transformer-like architectures (Vaswani et al.,\n2017). First, we ﬁne-tuned 10 different pretrained\nlanguage models on the downstream task. Then, we\ngenerated 10 instances of those pretrained models,\neach time with a new random initialization of the\nlast layer, namely the classiﬁer. Finally, we used\nan ensemble strategy based on a majority of votes.\nOur approach achieves the exact-match F1-score of\n77.99% that ranks ﬁrst in the shared task.\n2 Related work\nDeep learning approaches trained on large unstruc-\ntured data have shown considerable success in NLP\nproblems, including NER (Devlin et al., 2019; Liu\net al., 2019; Lample et al., 2016; Beltagy et al.,\n2019; Jin et al., 2019). These models use the\nlearned representations over the large data and\nreuse them in a supervised setting for a downstream\ntask. For domain-speciﬁc tasks, the models that are\ntrained on large general text can be further trained\non domain speciﬁc large data and then adapted for\na downstream task (Lee et al., 2019; Gururangan\net al., 2020; Alsentzer et al., 2019) or the mod-\nels can be trained only on domain-speciﬁc data\nand then adapted for a speciﬁc task (Beltagy et al.,\n2019).\n3 Data\nThe data provided for this task is a subset of Kulka-\nrni et al. ’s corpus (Kulkarni et al., 2018). The\ndataset consists of 615 unique protocols annotated\nwith 17 types of entities and action (an example is\nshown in Figure 1).\nThe organizers provided a set of protocols for\ntraining, development, and test. They further re-\nleased a ﬁnal set of unlabelled protocols for test\nduring the competition (called test 2020). Table\n1 shows the way the dataset has been split into a\n306\nFigure 1: An example of the data.\nSplit # of protocols\nTrain 370\nDev 123\nTest 123\nTest 2020 111\nTotal 727\nTable 1: Number of protocols in WNUT-NER dataset.\nEntity Train Dev Test Test 2020Count % Count % Count % Count %\nAction 12,35525.914,01125.494,13825.325,34623.04Amount 3,4327.201,0906.931,1907.281,2235.27Concentration1,3302.79 422 2.68 535 3.27 701 3.02Device 1,7523.67 616 3.92 468 2.86 888 3.83Generic-Measure484 1.02 132 0.84 143 0.87 173 0.75Location 3,9218.231,3968.871,3268.111,6577.14Measure-Type857 1.79 324 2.06 272 1.66 720 3.10Mention 257 0.54 83 0.53 56 0.34 142 0.61Method 1,5973.36 538 3.43 581 3.561,0594.56Modiﬁer 4,5889.621,5479.831,6019.793,41614.72Numerical 832 1.75 259 1.65 231 1.41 513 2.21Reagent 11,12123.333,59422.933,99524.445,01221.60Seal 210 0.44 92 0.58 64 0.39 119 0.51Size 262 0.55 123 0.78 113 0.69 232 1.00Speed 626 1.31 239 1.52 167 1.02 238 1.03Temperature1,5923.34 486 3.09 532 3.25 744 3.21Time 2,3965.02 745 4.74 870 5.32 951 4.10pH 67 0.14 37 0.23 62 0.38 66 0.28\nTotal 47,679 15,734 16,344 23,200\nTable 2: Entity distribution across the dataset (based on\nthe Standoff format).\ntraining set, a development set,2 a test set, and the\ncompetition test (test 2020).\nIn Table 2, we see the distribution of all the\nentities by each subset. As we can see, we have 18\nentities and only two of them (Action and Reagent)\nrepresent about 50% of annotations. This table also\nshows us that entities’ proportions are fairly similar\nacross all the subsets.\n4 Method\nOur models essentially focused on transformers-\nlike (Vaswani et al., 2017) language models that\nwe ﬁne-tuned on the NER task by adding a fully\n2Protocol 621 (in the development set) is a duplicate of\nprotocol 570 (in the train set), but their labels do not totally\nmatch.\nconnected layer on top of the token representations.\nThe models include BERT (cased) (Devlin et al.,\n2019), BioBERT (BERT trained on PubMed ab-\nstracts and PMC full-text articles) (Lee et al., 2019),\nBio+ClinicalBERT (BioBERT trained on notes in\nthe MIMIC-III v1.4 database) (Alsentzer et al.,\n2019), PubMedBERT (Gu et al., 2020), RoBERTa\n(Liu et al., 2019), BioMed RoBERTa (Gururangan\net al., 2020), and XLNet (Yang et al., 2019).\nOur method has been driven in 3 steps. First, we\nchose 10 different pretrained models and ﬁne-tuned\nthem on the downstream task. Then, using a voting\nstrategy, we created ensemble models. Finally, we\nﬁne-tuned 9 more times each model, each time with\na new random initialization of the fully connected\nlayer, to see if sampling ensemble models from this\nset of models would improve the results even more.\n4.1 Transformers with a fully connected\nlayer on top of the token representations\nIn order to use transformers as a NER model, the\nonly preprocessing we had to do was to break each\nprotocol into sentences. Those sentences will then\nbe the sequences that are fed into our model. As\nthere were no overlapping entities in the text, we\nused a softmax function which allowed us to clas-\nsify each token to only one entity.\nAs transformers usually use tokenizers that work\non word bits (or sub-tokens), we had to deal with it\nby assigning a dummy entity to each sub-token that\nwas part of a word. In such cases, at training time,\nwe only assign the true entity to the ﬁrst sub-token.\nThis allowed us to build back the original text quite\neasily. Indeed, during prediction, a word will get\nthe highest probable entity label among all the sub-\ntokens’ predictions of that word. In other words,\nthe highest probable entity label will be assigned to\nall the sub-tokens of the word and the sub-tokens\nwill be merged to build back the original word with\nthe respective assigned label. Finally, in a given\n307\nPretrained Models Corpus type# Parameters\nBERT (Devlin et al., 2019) base General 110Mlarge 340MBioBERT (Lee et al., 2019) Bio 110MBio+ClinicalBERT (Alsentzer et al., 2019)Bio 110MPubMedBERT (Gu et al., 2020) Bio 110M\nRoBERTa (Liu et al., 2019)base General 110Mlarge 340MBioMed RoBERTa (Gururangan et al., 2020)Bio 110M\nXLNet (Yang et al., 2019) base General 110Mlarge 340M\nTable 3: Pretrained models features\nsequence, if two adjacent words were given the\nsame entity prediction, we would consider the two\nwords as a passage related to that entity.\nUsing the above setup, we ﬁne-tuned 10 pre-\ntrained transformers for 10 epochs using an Adam\noptimizer (Kingma and Ba, 2014), a learning rate\nof 3e−5, a batch size of 24 and a maximum se-\nquence length of 256 tokens. We used 1x T4 GPU\nfor all base models and 2x T4 GPUs for the large\nones. For a given model, it took in average roughly\n16 minutes per epoch to train, thus about 2.67 hours\nfor the 10 epochs. After each epoch, we predicted\nthe development set, computed the F 1-score and\nsaved the model if it improved the previous epoch\nscore. Table 3 shows more information about all\nthe pretrained models that we ﬁne-tuned on the\nNER task. Indeed, 4 models out of 10 were trained\non Biomedical corpus, such as PubMed and/or\nBioMed whereas the others were trained on general\ncorpora, such as Wikipedia. Another key difference\nis the model type which deﬁnes the way a given\nmodel has been trained. This includes the train-\ning task (e.g., MLM, next sentence prediction, . . . ),\nthe tokenizer algorithm, the optimizer and more.\nWe used 5 different kinds of BERT-based, 3 of\nRoBERTa-based and 2 of XLNet-based models.\nFor more details regarding the speciﬁcs of the ar-\nchitectures, please refer directly to their respective\npapers.\n4.2 An ensemble based on a voting strategy\nAs implemented in (Copara et al., 2020b,a), our\nensemble model strategy is based on a majority of\nvotes. This means that for a given ensemble model\ncomposition, each composing model has the right\nto vote. In other words, for a given protocol and\na given sequence, each model will return its pre-\ndictions which can be interpreted as passage/entity\ncombinations. Once we collected all models’ pre-\ndictions, we then counted all the passage/entity\ncombinations and validated only those that had\ncast a majority of votes.\n4.3 Sampling\nOnce we had all the models trained and ready, we\nwere wondering if we could improve efﬁciency by\nadding more voters. The idea is to repeat the ﬁrst\nstep where each time we have a new random ini-\ntialization on the fully connected layer. We ended\nup with 100 different models, corresponding to 10\ndifferent pretrained models ﬁne-tuned 10 times.\nWith only a few models to choose from, we\nwould have been able to predict all the possible\nmodel compositions; however, as using 50 mod-\nels out of 100 would have resulted in about 10 29\npossible ensembles, we had to sample randomly\nensemble model compositions. For each number\nof models taken into account in a given ensemble,\nwe took a sample size of 1000 combinations. This\nwill later allow us to show the results distribution\nof our ensemble models and examine how it will\nbehave in certain circumstances.\nThe ensemble model we chose to use for the sub-\nmission was the one that gave us the best F1-score\non the test set. It is a composition of 14 models that\nwere ﬁne-tuned on the task. It contained the follow-\ning pretrained models: 2 ×BioBERT (BioBERT\nmodels with two different random initializations or\nseeds), 2×BioClinicalBERT (2 random seeds), 3×\nPubMedBERT (3 random seeds), 2×RoBERTabase\n(2 random seeds), 1×RoBERTalarge 1×BioMed\nRoBERTa and 3×XLNetlarge (3 random seeds).\n5 Results and Discussions\nIn Table 4, we see the F1-score for all the 10 mod-\nels we ﬁne-tuned across all the 18 entities. The\nreported baseline is the CRF baseline 3 that was\nprovided for the shared task. First, we can see that\nthe ensemble model outperforms the baseline by far.\nWhen comparing all the models (ensemble apart),\nwe also notice that PubMedBERT is quite consis-\ntent as it often outperforms all the other models,\nincluding the ensemble for a few entities, namely\nMention, Seal, Temperatureand pH. Additionally,\nwhen compared to its peers, it clearly shows the\nbest micro and macro F1-scores.\nHowever, when looking at Speed, it seems that\nthe transformers-based models we used are not able\nto do a better job than the baseline. A closer look\nat the errors should be done in order to see what\ncaused such a difference with the baseline (see\nSection 5.4).\n3https://github.com/jeniyat/WNUT_2020_\nNER/tree/master/code/baseline_CRF\n308\nEntity BERT (cased) BioClinical\nBERT BioBERT RoBERTa BioMed\nRoBERTa\nPubMed\nBERT\nXLNet Ensemble Baselinebase large base large base large\nAction 88.980.2 88.510.2 88.870.2 89.060.2 88.780.3 88.750.2 88.700.2 89.290.3 89.320.1 89.110.3 90.00(0.68↑) 84.40\nAmount 85.730.2 85.140.3 85.210.4 85.960.5 85.590.3 85.530.7 85.990.4 85.660.2 85.000.3 84.470.6 86.41(0.42↑) 84.19\nConcentration 82.610.7 81.830.7 82.310.6 83.140.4 82.680.5 82.141.0 83.340.5 83.810.4 82.250.7 82.401.1 84.64(0.83↑) 78.49\nDevice 62.961.1 62.180.8 63.570.6 64.241.2 63.390.8 64.211.0 63.440.9 64.171.0 63.270.9 63.651.2 67.11(2.87↑) 58.67\nGeneric-Measure 30.441.6 29.731.9 26.371.8 30.401.7 32.121.1 32.232.0 30.541.7 31.571.8 30.331.3 31.102.1 33.17(0.94↑) 29.88\nLocation 75.530.4 75.050.4 75.540.3 76.060.7 75.850.5 76.100.6 75.320.6 76.240.3 75.620.3 75.531.0 77.50(1.26↑) 69.17\nMeasure-Type 52.751.4 53.321.4 52.421.7 54.231.1 53.891.6 52.621.1 53.610.9 52.470.6 54.611.3 55.352.0 55.36(0.01↑) 48.93\nMention 71.003.0 67.152.5 68.401.9 68.181.7 68.701.0 70.352.2 68.442.1 72.350.9 68.662.8 63.0211.1 71.79 (0.56↓) 60.18\nMethod 48.701.3 48.401.0 47.220.8 47.011.3 49.641.6 48.861.1 47.851.5 47.581.9 49.891.4 49.891.0 53.19(3.30↑) 43.63\nModiﬁer 58.830.8 57.930.7 58.800.6 59.070.6 58.680.8 59.560.8 58.450.6 59.720.6 59.470.6 58.441.2 60.27(0.55↑) 53.94\nNumerical 62.563.6 64.732.2 63.102.7 61.404.5 59.882.6 62.513.2 61.532.7 64.834.0 63.413.8 63.053.7 66.98(2.15↑) 56.96\nReagent 80.380.2 80.340.3 80.600.2 80.940.2 80.510.3 80.840.2 80.350.2 81.240.3 81.190.1 80.680.3 82.59(1.35↑) 74.98\nSeal 65.565.1 59.901.9 62.462.7 63.072.6 64.132.8 66.423.4 63.952.7 69.513.9 62.653.3 62.833.7 69.42 (0.09↓) 67.72\nSize 58.361.8 60.160.9 60.071.8 61.651.5 59.791.8 56.472.1 59.302.1 62.482.1 61.121.3 59.633.0 62.56(0.08↑) 57.14\nSpeed 83.920.8 83.641.0 82.210.8 83.060.8 84.710.7 83.671.0 84.330.6 84.131.0 84.450.7 83.432.3 84.24 (0.47↓) 85.46\nTemperature 91.470.5 90.200.6 91.360.5 90.840.8 91.320.6 90.700.6 91.390.7 92.200.5 91.490.5 90.201.4 92.11 (0.09↓) 91.76\nTime 88.750.3 88.920.7 88.780.4 88.850.5 88.700.6 89.020.7 89.280.4 88.490.6 88.870.4 88.201.5 89.49(0.21↑) 88.64\npH 72.562.3 67.073.0 72.471.4 72.412.3 71.272.8 66.782.2 72.504.1 78.041.7 69.352.8 70.772.9 77.59 (0.45↓) 65.49\nmicro F1-score 78.380.2 77.940.2 78.270.1 78.560.2 78.420.2 78.520.2 78.310.1 79.000.2 78.760.1 78.340.3 80.42(1.42↑) 74.39\nmacro F1-score 70.060.7 69.120.3 69.430.3 69.980.5 69.980.4 69.820.3 69.910.3 71.320.3 70.050.4 69.540.8 72.47(1.15↑) 66.65\nTable 4: F1-score by model on the test set. We reported averages across the 10 random seeds for all the pretrained\nmodel results, for those, subscripts represents the standard deviations. The improvements of the ensemble model\nover the best performing transformer model is shown in parentheses in the ensemble column.\nWhen comparing the micro to the macro F 1-\nscore standard deviations across all the models,\nwe can see that the macro F1-score standard devi-\nations are systematically higher. This is probably\ndue to the fact that some entities, namely Generic-\nMeasure, Mention, Seal, Size and pH, which ac-\ncount for less than 1% of the test set each (see Table\n2), seem to have a relatively high F1-score standard\ndeviations level. The same applies to Measure-\nType, Numerical and Speed that are less than 2%\nof the test set each. This is in line with the results\nreported by Dodge et al. (2020) which shows that\nresults can vary a lot across the seeds when a small\namount of data is available. Indeed, as these enti-\nties are quite rare, a simple misclassiﬁcation can\nhave a high impact on the macro F1-score. That be-\ning said, the micro F1-scores seem relatively stable\nacross all the pretrained models.\n5.1 Ensemble results analysis\nIn this section, we will try to analyse the results\nwe observe when sampling on different ensemble\nmodel compositions. These results are exclusively\ncomputed on the test set. The idea behind this\nexperiment is to try to understand the behaviour of\nsome metrics when adding more models.\nFigures 2 to 4 show the F1-score, the recall and\nprecision distributions with respect to the number\nof models taken in a given ensemble, respectively.\nThe ﬁrst thing we notice from Figures 2 to 4\nis that the more the number of models taken into\naccount in an ensemble grows, the more the metrics\nvariance tends to be smaller and steadier.\nWhen looking at Figure 3 and 4, we clearly see\nthat odd number of voters has a positive impact on\nFigure 2: Micro F 1-score distribution by number of\nmodels used in an ensemble (sample size of 1000) on\nthe test set.\nthe recall while it looks like it has a negative impact\non the precision. For the moment, this is unclear to\nus why this behaviour can be observed; however,\nwe think it could be linked to the majority rule we\nintroduced in our voting strategy where majority\nis easier to reach in an odd system. When looking\nclosely at Figure 2, it appears that the ”odd/even\nnumber effect” tends to cancel out when the num-\nber of voters increases and even number of voter\ngetting slightly better results.\nIn Figure 3, there is clearly a positive slope that\nseems to ﬂatten at the end, which means that the\nmore models we have in our ensemble, the higher\nrecall we should expect. Conversely, this trend\ndoesn’t seem that clear for precision (Figure 4)\nwhere it looks like we have a positive relation with\nodd numbers of voters, a negative one with even\nnumber of voters which at the end seem to converge\n309\nFigure 3: Micro recall distribution by number of mod-\nels used in an ensemble (sample size of 1000) on the\ntest set.\ninto a ﬂat trend for both of them. However, in\nboth ﬁgures, as already mentioned, the variance of\ntheir respective metrics seems to get steadier and\nsmaller when adding more models in the ensemble\ncomposition.\nFigures 6 to 8 show the same metrics while try-\ning to isolate the effect of adding a new pretrained\nmodel versus the effect of adding an already taken\npretrained model with a new random fully con-\nnected layer initialization.\nIn order to understand the setting of this experi-\nment, we ﬁrst build a matrix (see Figure 5) where\neach column is a pretrained model and each row\nis a ﬁne-tuned version of it. We then compare\nthe performances of ensemble models based on\ncombinations of columns to those of the ensemble\nmodels based on combinations of rows. In Figures\n6 to 8, the x−axis represents the number of row or\ncolumns taken into account.\nFor instance, the ﬁrst two boxplots are comput-\ning metrics distributions of ensembles taking either\none row or one column as an ensemble, the follow-\ning two boxplots will take a combination of either\ntwo rows or two columns as ensemble and so on\nup to 9 rows/columns combinations.\nMore precisely, the ﬁrst pink boxplot will com-\npose an ensemble taking one column of models,\nnamely, all the BERT base models to begin with,\nthen all the BERTlarge models and so on until it\ncomputes the metrics for an ensemble composed\nwith all the XLNet large models. Then, the sec-\nond pink boxplot will take the composition of 2\ncolumns, for example, it will ﬁrst compute an\nensemble with all the BERT base models and all\nthe BERTlarge models, then another with all the\nFigure 4: Micro precision distribution by number of\nmodels used in an ensemble (sample size of 1000) on\nthe test set.\n\n\nBERTbase1 BERTlarge1 ··· XLNetbase1 XLNetlarge1\nBERTbase2 BERTlarge2 ··· XLNetbase2 XLNetlarge2\n... ... ... ... ...\nBERTbase9 BERTlarge9 ··· XLNetbase9 XLNetlarge9\nBERTbase10 BERTlarge10 ··· XLNetbase10 XLNetlarge10\n\n\nFigure 5: Matrix where each column represents a pre-\ntrained model and each row represents a ﬁne-tuned\nmodel with a new random initialization of the fully con-\nnected layer.\nBERTbase models and all XLNetbase models and\nso on until it computes an ensemble containing all\nthe XLNetbase and XLNetlarge models.\nOn the other hand, the blue boxplots will com-\npose ensembles with combination of rows. This\nmeans that the ﬁrst blue boxplot will ﬁrst compute\nan ensemble composed of the ﬁrst row (BERTbase1 ,\nBERTlarge1 , . . . , XLNetbase1 , XLNetlarge1 ), then\nof the second row (BERT base2 , BERTlarge2 , . . . ,\nXLNetbase2 , XLNetlarge2 ) and so on until it com-\nputes an ensemble with the last row (BERTbase10 ,\nBERTlarge10 , . . . , XLNetbase10 , XLNetlarge10 ). In\nthe same manner, the second blue boxplot will com-\npute ensembles composed by the combinations of\ntwo rows. First, all the models in the ﬁrst and\nsecond rows, then, all the models in the ﬁrst and\nthird rows and so on until it computes an ensemble\ncomposed with all the models of the last two rows.\nIn this setting, as the maximum number of pos-\nsible combinations of row is 252 =\n(10\n5\n)\n, we were\nable to compute all the possible combinations in-\nstead of sampling them. As we have the same num-\nber of pretrained models as ﬁne-tuned versions,\nwe end up with the same number of possible com-\n310\nFigure 6: Micro F 1-score distribution using ensemble\ncomposed of either 1 to 9 different pretrained models\n(each time with 10 different ﬁne-tuning) vs. 1 to 9 dif-\nferent ﬁne-tuning using all the pretrained models.\nbinations of ensemble. That being said, for each\nnumber of models taken into account in an ensem-\nble, this allows us to compare the pink boxplot with\nthe blue one in a more convenient manner.\nIt is worth noting that the more we increase the\nnumber of columns and rows present in an ensem-\nble model, the more they share a certain number of\nmodels. For example, at 9, the pink boxplot shows\nthe distribution of the metrics for all the possible\nensemble models containing 9 columns of mod-\nels (90 models out of 100), while the blue boxplot\nshows the same metrics for 9 rows of models (also\n90 models out of 100). At this point, it is expected\nto see both boxplots converging as they both share\n64 models predictions out of 90.\nFocusing on the left part of Figure 6, we clearly\nsee the beneﬁts of using more pretrained models.\nFirst, it shows better results with only an ensemble\nof 10 different pretrained models. Then, it really\nlooks steadier as the F1-score distribution is much\nnarrower than the ensemble composed of multiple\nﬁne-tuning of the same pretrained model.\nWhen looking at Figure 7, we see that the ma-\njor difference between both distributions are the\nvariances of the recall distributions, indeed, taking\ndifferent pretrained models tends to retrieve impor-\ntant passages more systematically. The trend of\nboth selection strategies seems to be increasing, in\nother words, in both cases, the more we add models,\nthe more we retrieve important passages.\nFinally, it is interesting to see in Figure 8 that the\nprecision begins quite high and tends to decrease\nwhen we add more ﬁne-tuned models. Conversely,\nwhen taking more pretrained models, it seems the\nFigure 7: Micro recall distribution using ensemble com-\nposed of either 1 to 9 different pretrained models (each\ntime with 10 different ﬁne-tuning) vs. 1 to 9 different\nﬁne-tuning using all the pretrained models.\nFigure 8: Micro precision distribution using ensemble\ncomposed of either 1 to 9 different pretrained models\n(each time with 10 different ﬁne-tuning) vs. 1 to 9 dif-\nferent ﬁne-tuning using all the pretrained models.\nprecision has a positive relation to the number of\nmodels we use. As explained before, this relation\nis also due to the fact that we share more and more\nmodels in both ensemble selection strategies.\nThis analysis helped us to understand a bit more\nabout what was happening behind our majority\nof votes strategy, it would be interesting to take\nnotes of some of the observed behaviours and try\nto devise new strategies accordingly.\n5.2 Ofﬁcial results\nThe ofﬁcial results in terms of Precision, Recall,\nand F1 on the test 2020 set is shown in Table 5.\nEach team was allowed to submit only one run. Our\nsubmitted run was based on the ensemble model\ndescribed in sections 4.2 and 4.3. Our BiTeM team\nachieved the highest precision score in both ex-\n311\nTeam Name Exact Match Partial Match\nP R F1 P R F1\nB-NLP 77.9563.9370.25 84.85 69.5976.46\nBIO-BIO 78.4971.0674.59 83.16 75.2979.03\nBiTeM 84.7372.2577.99 88.7275.6681.67\nDSC-IITISM 64.2057.0760.42 68.52 60.9064.49\nFancy Man 76.2171.7673.92 81.15 76.4178.71\nIBS 74.2662.5567.90 79.72 67.1572.89\nKabir 78.7972.2075.35 83.73 76.7380.08\nKaushikAcharya73.6863.9868.48 79.31 68.8773.73\nmahab 50.1952.9651.54 55.09 58.1456.57\nmgsohrab 83.6970.6276.60 87.95 74.2280.50\nPublishInCovid1981.3674.1277.57 85.74 78.1181.75\nSudeshnaTCS 74.9971.4373.16 79.73 75.9577.80\nIITKGP 77.0072.9374.91 81.76 77.4379.54\nTable 5: Ofﬁcial results on Test 2020.\nact match and partial match evaluation reaching\n84.73% and 88.72%, respectively, and F1-score in\nexact match evaluation reaching 77.99% among 13\nteams. The F1-score of our model in partial match\n(81.67%) was slightly lower than the best F1-score\n(81.75%).\n5.3 Results of the ensemble model on test\n2020 data\nThe precision, recall, and F 1-score results of all\nentities and Action on the test 2020 in the exact\nmatch evaluation is represented in Table 6. The\nbest F1-score was achieved for pH. Size was the\nmost difﬁcult entity for detection.\n5.4 Error analysis\nFigure 9 shows the normalized confusion matrix\nfor the predictions (exact match) of the ensemble\nmodel on the test 2020 data. As we can see, more\nthan 78% of Size predictions are mislabelled as\nAmount. This can be due to the few number of\ntraining instances of Size entity. As we can see in\nthe following examples, 50 mL can refer to both\nSize and Amount depending on the context. In the\nﬁrst example, 50 mL refers to Amount and in the\nsecond example, it refers to Size.\nExample 5.4.1 Add more NEB –no\nβ−mercaptoethanol to ﬁnal volume of 50\nmL.\nExample 5.4.2 Transfer the aqueous phase to\na.new 50 mL Falcon tube.\nAbout 17% of the Device predictions are mis-\nlabelled as Location that can be due to the incon-\nsistencies in the annotation process, for example\nmagnetic rack is annotated as Device in a few proto-\ncols (protocol 0680, protocol 0683, protocol 0685),\nand as Location in others (protocol 32148, protocol\nEntity Precision Recall F1\nAction 90.09 82.29 86.01\nAmount 77.09 89.47 82.82\nConcentration 86.76 88.16 87.45\nDevice 80.38 56.00 66.01\nGeneric-Measure 55.65 37.87 45.07\nLocation 69.59 68.98 69.28\nMeasure-Type 73.87 46.00 56.70\nMention 67.32 74.10 70.55\nMethod 61.49 35.77 45.23\nModiﬁer 83.02 42.66 56.36\nNumerical 65.32 38.49 48.44\nReagent 82.58 82.54 82.56\nSeal 81.58 78.15 79.83\nSize 63.64 17.80 27.81\nSpeed 86.38 86.38 86.38\nTemperature 91.68 83.27 87.27\nTime 92.58 87.66 90.05\npH 96.72 90.77 93.65\nTable 6: The precision, recall, and F 1-score of the en-\nsemble model for all the entities and action on the test\n2020.\n33630). Here are two examples of magnetic rack\nannotated as Location and Device, respectively.\nExample 5.4.3 Place samples on magnetic rack,\nand incubate for 5 mins on the rack. Remove su-\npernatant.\nExample 5.4.4 Place the tube on a magnetic rack.\nSimilarly freezer is annotated interchangeably as\nLocation and Device. Generic-Measure is mostly\nconfused with Concentration label (20.4%), and\nMethod is mostly confused by Action. About 12%\nof Numerical is annotated as Concentration.\n6 Conclusion\nWith almost no preprocessing, we have seen that\ncurrent pretrained language models seem to be\nquite efﬁcient in any NER task (Copara et al.,\n2020a,b). By analysing our voting strategy, we\nhave also demonstrated the strengths as well as the\nweaknesses of such ensemble models. For instance,\nit looks like the more models we use, the more the\nperformances tend to be high and stable, however,\nit appears that new pretrained model brings more in-\nformation than ﬁne-tuning again a pretrained model\nwith a new fully connected weights random initial-\nization.\n312\nFigure 9: Normalized Confusion matrix for the ensemble model on the test 2020 data.\nWith this voting strategy, our submission\nachieved the best exact match overall F1-score of\nthe competition. This clearly shows the power of\nsuch models. With almost no knowledge on the\ntopic of wet laboratory protocols required, we think\nthat those models open opportunity to out-of-ﬁeld\nresearchers.\nIn future work, it would be interesting to improve\nthe number of pretrained models selection and ex-\nplore bootstrapping instead of ﬁne-tuning multiple\ntimes the same pretrained model. It would also\nbe interesting to see if some preprocessing tweaks\ncould help us to improve the detection performance\nof Speed where our models were outperformed by\nthe baseline.\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly Available\nClinical BERT Embeddings. In Proceedings of the\n2nd Clinical Natural Language Processing Work-\nshop, pages 72–78.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A Pretrained Language Model for Scientiﬁc\nText. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3606–3611.\nJenny Copara, Julien Knafou, Nona Naderi, Claudia\nMoro, Patrick Ruch, and Douglas Teodoro. 2020a.\nContextualized French language models for biomed-\nical named entity recognition. In Actes de la 6e\nconf´erence conjointe Journ ´ees d’ ´Etudes sur la Pa-\nrole (JEP , 33e´edition), Traitement Automatique des\nLangues Naturelles (TALN, 27e ´edition), Rencon-\ntre des ´Etudiants Chercheurs en Informatique pour\nle Traitement Automatique des Langues (R ´ECITAL,\n22e ´edition). Atelier D ´Eﬁ Fouille de Textes , pages\n36–48, Nancy, France. ATALA et AFCP.\nJenny Copara, Nona Naderi, Julien Knafou, Patrick\nRuch, and Douglas Teodoro. 2020b. Named en-\ntity recognition in chemical patents using ensem-\nble of contextual language models. arXiv preprint\narXiv:2007.12569.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\n313\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. arXiv preprint arXiv:2002.06305.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas,\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. 2020. Domain-\nspeciﬁc language model pretraining for biomedical\nnatural language processing.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of ACL.\nQiao Jin, Bhuwan Dhingra, William Cohen, and\nXinghua Lu. 2019. Probing biomedical embeddings\nfrom language models. In Proceedings of the 3rd\nWorkshop on Evaluating Vector Space Representa-\ntions for NLP, pages 82–89.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nChaitanya Kulkarni, Wei Xu, Alan Ritter, and Raghu\nMachiraju. 2018. An annotated corpus for machine\nreading of instructions in wet lab protocols. In Pro-\nceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 2 (Short Papers), pages 97–106, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 260–270.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. BioBERT: a pre-\ntrained biomedical language representation model\nfor biomedical text mining. Bioinformatics,\n36(4):1234–1240.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nJeniya Tabassum, Sydney Lee, Wei Xu, and Alan Rit-\nter. 2020. WNUT-2020 Task 1 Overview: Extract-\ning Entities and Relations from Wet Lab Protocols.\nIn Proceedings of EMNLP 2020 Workshop on Noisy\nUser-generated Text (WNUT).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st International\nConference on Neural Information Processing Sys-\ntems, pages 6000–6010.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXLNet: Generalized Autoregressive Pretraining for\nLanguage Understanding. In Advances in neural in-\nformation processing systems, pages 5753–5763.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8322768211364746
    },
    {
      "name": "Language model",
      "score": 0.7390792369842529
    },
    {
      "name": "Task (project management)",
      "score": 0.6982508897781372
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6700654029846191
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5998168587684631
    },
    {
      "name": "Natural language processing",
      "score": 0.5700955390930176
    },
    {
      "name": "Ensemble forecasting",
      "score": 0.46350640058517456
    },
    {
      "name": "Machine learning",
      "score": 0.4251532554626465
    },
    {
      "name": "Machine translation",
      "score": 0.4108765721321106
    },
    {
      "name": "Engineering",
      "score": 0.0605999231338501
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}