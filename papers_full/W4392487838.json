{
  "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
  "url": "https://openalex.org/W4392487838",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Soudani, H.",
      "affiliations": [
        "Radboud University Nijmegen"
      ]
    },
    {
      "id": null,
      "name": "Kanoulas, E.",
      "affiliations": [
        "University of Amsterdam",
        "Amsterdam University of the Arts"
      ]
    },
    {
      "id": null,
      "name": "Hasibi, F.",
      "affiliations": [
        "Radboud University Nijmegen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2397288399",
    "https://openalex.org/W2949849869",
    "https://openalex.org/W4385570290",
    "https://openalex.org/W4387849079",
    "https://openalex.org/W4389520342",
    "https://openalex.org/W3176693244",
    "https://openalex.org/W3100852180",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4391376033",
    "https://openalex.org/W4225380759",
    "https://openalex.org/W4386576727",
    "https://openalex.org/W4401042753",
    "https://openalex.org/W4384636956",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2963430447",
    "https://openalex.org/W4400526199",
    "https://openalex.org/W4401043063",
    "https://openalex.org/W4385570777",
    "https://openalex.org/W4385569970",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W3201233724",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W4386644696",
    "https://openalex.org/W4396843955",
    "https://openalex.org/W4401042333",
    "https://openalex.org/W4385572046",
    "https://openalex.org/W3033919759",
    "https://openalex.org/W3176828726"
  ],
  "abstract": "Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting\\nstrong performance across diverse tasks and domains. However, it has been\\nobserved that the performance diminishes when dealing with less-popular or\\nlow-frequency concepts and entities, for example in domain specific\\napplications. The two prominent approaches to enhance the performance of LMs on\\nlow-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning\\n(FT) over synthetic data. This paper explores and evaluates the impact of RAG\\nand FT on customizing LMs in handling low-frequency entities on question\\nanswering tasks. We conduct extensive experiments on twelve LMs of varying size\\nand type and different fine tuning, data augmentation, and retrieval models.\\nOur findings indicate that while FT boosts the performance across entities of\\nvarying popularity, RAG surpasses FT by a large margin particularly for least\\npopular factual knowledge. Additionally, the success of both RAG and FT\\napproaches is amplified by improving retrieval and data augmentation\\ntechniques. Fine tuning, while beneficial for small LMs, requires extensive\\nresources. To address this issue, we propose the new Stimulus RAG approach that\\nsurpasses the effectiveness of fine tuning based approaches, thereby\\neliminating the need for the costly data augmentation and fine tuning step for\\nenriching LMs with less popular factual knowledge. The code is available at\\n\\\\url{https://github.com/informagi/RAGvsFT}.\\n",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.4871096611022949
    },
    {
      "name": "Information retrieval",
      "score": 0.4042338728904724
    }
  ],
  "institutions": []
}