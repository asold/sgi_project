{
  "title": "Enriching Non-Autoregressive Transformer with Syntactic and Semantic Structures for Neural Machine Translation",
  "url": "https://openalex.org/W3156927220",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5100346522",
      "name": "Ye Liu",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5101555948",
      "name": "Yao Wan",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100461783",
      "name": "Jianguo Zhang",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5039522571",
      "name": "Wenting Zhao",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5036357902",
      "name": "Philip S. Yu",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2789543585",
    "https://openalex.org/W2949644922",
    "https://openalex.org/W2948629866",
    "https://openalex.org/W2964352247",
    "https://openalex.org/W2963536265",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2892213699",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W2996843693",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2962915948",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2970832665",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2920538220",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3015162217",
    "https://openalex.org/W2976965654",
    "https://openalex.org/W2981648103",
    "https://openalex.org/W2990372437",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W3100753857",
    "https://openalex.org/W1972648128",
    "https://openalex.org/W2962969034",
    "https://openalex.org/W3034579764",
    "https://openalex.org/W2990389671",
    "https://openalex.org/W2103076621",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3034892578",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W4289761690",
    "https://openalex.org/W2158847908"
  ],
  "abstract": "The non-autoregressive models have boosted the efficiency of neural machine translation through parallelized decoding at the cost of effectiveness, when comparing with the autoregressive counterparts. In this paper, we claim that the syntactic and semantic structures among natural language are critical for non-autoregressive machine translation and can further improve the performance. However, these structures are rarely considered in the existing non-autoregressive models. Inspired by this intuition, we propose to incorporate the explicit syntactic and semantic structure of languages into a non-autoregressive Transformer, for the task of neural machine translation. Moreover, we also consider the intermediate latent alignment within target sentences to better learn the long-term token dependencies. Experimental results on two real-world datasets (i.e., WMT14 En-De and WMT16 En- Ro) show that our model achieves a significantly faster speed, as well as keeps the translation quality when compared with several state-of-the-art non-autoregressive models.",
  "full_text": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1235–1244\nApril 19 - 23, 2021. ©2021 Association for Computational Linguistics\n1235\nEnriching Non-Autoregressive Transformer with Syntactic and Semantic\nStructures for Neural Machine Translation\nYe Liu1, Yao Wan2, Jian-Guo Zhang1, Wenting Zhao1, Philip S. Yu1\n1Department of Computer Science, University of Illinois at Chicago, Chicago, IL, USA\n2School of Computer Sci. & Tech., Huazhong University of Science and Technology, Wuhan, China\n{yliu279, jzhan51, wzhao41, psyu}@uic.edu, wanyao@hust.edu.cn\nAbstract\nThe non-autoregressive models have boosted\nthe efﬁciency of neural machine translation\nthrough parallelized decoding at the cost of\neffectiveness, when comparing with the au-\ntoregressive counterparts. In this paper, we\nclaim that the syntactic and semantic struc-\ntures among natural language are critical for\nnon-autoregressive machine translation and\ncan further improve the performance. How-\never, these structures are rarely considered in\nexisting non-autoregressive models. Inspired\nby this intuition, we propose to incorporate\nthe explicit syntactic and semantic structures\nof languages into a non-autoregressive Trans-\nformer, for the task of neural machine transla-\ntion. Moreover, we also consider the interme-\ndiate latent alignment within target sentences\nto better learn the long-term token dependen-\ncies. Experimental results on two real-world\ndatasets (i.e., WMT14 En-De and WMT16 En-\nRo) show that our model achieves a signiﬁ-\ncantly faster speed, as well as keeps the transla-\ntion quality when compared with several state-\nof-the-art non-autoregressive models.\n1 Introduction\nRecently, non-autoregressive models (Gu et al.,\n2018), which aim to enable the parallel genera-\ntion of output tokens without sacriﬁcing translation\nquality, have attracted much attention. Although\nthe non-autoregressive models have considerably\nsped up the inference process for real-time neural\nmachine translation (NMT) (Gu et al., 2018), their\nperformance is considerably worse than that of\nautoregressive counterparts. Most previous works\nattribute the poor performance to the inevitable con-\nditional independence issue when predicting target\ntokens, and many variants have been proposed to\nsolve it. For example, several techniques in non-\nautoregressive models are investigated to mitigate\nthe trade-off between speedup and performance,\nincluding iterative reﬁnement (Lee et al., 2018),\ninsertion-based models (Chan et al., 2019; Stern\net al., 2019), latent variable based models (Kaiser\net al., 2018; Shu et al., 2020), CTC models (Li-\nbovick´y and Helcl, 2018; Saharia et al., 2020), al-\nternative loss function based models (Wei et al.,\n2019; Wang et al., 2019; Shao et al., 2020), and\nmasked language models (Ghazvininejad et al.,\n2019, 2020).\nAlthough these works have tried to narrow the\nperformance gap between autoregressive and non-\nautoregressive models, and have achieved some\nimprovements on machine translation, the non-\nautoregressive models still suffer from syntactic\nand semantic limitations. That is, the translations\nof non-autoregressive models tend to contain inco-\nherent phrases (e.g., repetitive words), and some\ninformative tokens on the source side are absent.\nIt is because in non-autoregressive models, each\ntoken in the target sentence is generated indepen-\ndently. Consequently, it will cause the multimodal-\nity issue, i.e., the non-autoregressive models can-\nnot model the multimodal distribution of target\nsequences properly (Gu et al., 2018).\nOne key observation to mitigate the syntactic and\nsemantic error is that source and target translated\nsentences follow the same structure, which can\nbe reﬂected from Part-Of-Speech (POS) tags and\nNamed Entity Recognition (NER) labels. Brieﬂy,\nPOS, which aims to assign labels to words to in-\ndicate their categories by considering the long-\ndistance structure of sentences, can help the model\nlearn the syntactic structure to avoid generating\nthe repetitive words. Likewise, NER, which dis-\ncovers the proper nouns and verbs of sentences,\nnaturally helps the model recognize some meaning-\nful semantic tokens that may improve translation\nquality. This observation motivates us to leverage\nthe syntactic as well as semantic structures of nat-\nural language to improve the performance of non-\n1236\nTable 1: A motivating example on WMT14 En→De dataset. English with POS|NER and its corresponding German\ntranslation with POS |NER. The Blue labels show the same tags, while the Red labels show the different tags in\ntwo languages.\nEN: A republican strategy to counter the rel-election of Obama .\n| | | | | | | | | |\nEN POS: DET ADJ NOUN PART VERB DET NOUN ADP PROPN PUNCT\nEN NER: O B NORP O O O O O O B PERSON O\nDE: Eine republikanische strategie gegen die wiederwahl Obama .\n| | | | | | | |\nDE POS: DET ADJ NOUN ADP DET NOUN PROPN PUNCT\nEN NER: O B NORP O O O O B PERSON O\nautoregressive NMT. We present a motivating ex-\nample in Table 1 to better illustrate our idea. From\nthis table, we can ﬁnd that although the words are\naltered dramatically from the English sentence to\nits German translation, the corresponding POS and\nNER tags still remain similar. For example, most\nPOS tags are identical and follow the same pattern,\nexcept that PART, VERB, and ADP in the English\ndo not match the German ADP, while the NER tags\nare exactly the same in both sentences.\nIn this paper, we propose an end-to-end\nSyntactic and semantic structure-aware Non-\nAutoregressive Transformer model ( SNAT) for\nNMT. We take the structure labels and words as\ninputs of the model. With the guidance of extra\nsentence structural information, the model greatly\nmitigates the multimodality issue’s negative impact.\nThe core contributions of this paper can be summa-\nrized as that we propose 1) a syntax and semantic\nstructure-aware Transformer which takes sequen-\ntial texts and the structural labels as input and gen-\nerates words conditioned on the predicted structural\nlabels, and 2) an intermediate alignment regular-\nization which aligns the intermediate decoder layer\nwith the target to capture coarse target informa-\ntion. We conduct experiments on four benchmark\ntasks over two datasets, including WMT14 En→De\nand WMT16 En→Ro. Experimental results indi-\ncate that our proposed method achieves competitive\nresults compared with existing state-of-the-art non-\nautoregressive and autoregressive neural machine\ntranslation models, as well as signiﬁcantly reduces\nthe decoding time.\n2 Background\nRegardless of its convenience and effectiveness,\nthe autoregressive decoding methods suffer two\nmajor drawbacks. One is that they cannot generate\nmultiple tokens simultaneously, leading to inefﬁ-\ncient use of parallel hardware such as GPUs. The\nother is that beam search has been found to output\nlow-quality translation with large beam size and\ndeteriorates when applied to larger search spaces.\nHowever, non-autoregressive transformer (NAT)\ncould potentially address these issues. Particularly,\nthey aim at speeding up decoding through remov-\ning the sequential dependencies within the target\nsentence and generating multiple target tokens in\none pass, as indicated by the following equation:\nPNAT(y|x; φ) =\nm∏\nt=1\np(yt|ˆx,x; φ) , (1)\nwhere ˆx = {ˆx1,..., ˆxm}is the copied source sen-\ntence. Since the conditional dependencies within\nthe target sentence (yt depends on y<t) are removed\nfrom the decoder input, the decoder is unable to\nleverage the inherent sentence structure for pre-\ndiction. Hence the decoder is supposed to ﬁgure\nout such target-side information by itself given the\nsource-side information during training. This is a\nmuch more challenging task compared to the au-\ntoregressive counterparts. From our investigation,\nwe ﬁnd the NAT models fail to handle the target\nsentence generation well. It usually generates repet-\nitive and semantically incoherent sentences with\nmissing words. Therefore, strong conditional sig-\nnals should be introduced as the decoder input to\nhelp the model better learn internal dependencies\nwithin a sentence.\n3 Methodology\nIn this section, we present our model SNAT to\nincorporate the syntactic and semantic structure\ninformation into a NAT model as well as an inter-\nmediate latent space alignment within the target.\nFigure 1 gives an overview of the network struc-\nture of our proposed SNAT. In SNAT, the input\nsequence is segmented into sub-words by byte-pair\n1237\nsoftMaxsoftMaxsoftMaxsoftMax softMaxsoftMaxsoftMaxsoftMaxsoftMax\nContext\nIs    An@@Celotti   the  man   for   the     job   ?AUX\nEmb\nPRONPRONDETNOUNADPDETNOUNPUNCT AUXPRONPRONDETNOUNADPDETNOUNPUNCT\nIst   An@@ Celotti   der    mann fur   den job   ?AUXPRONPRONDETNOUNADPDETNOUNPUNCT\nEmbEmbEmbEmbEmbEmbEmbEmb\nMulti-Head Self-attention\nMLPMLP MLPMLP MLPMLPMLPMLPMLP\nEmb EmbEmbEmb EmbEmbEmbEmbEmb\nMLPMLP MLPMLP MLPMLPMLPMLPMLP\nMulti-Head Self-attention\nMulti-Head Positional attention\nMulti-Head Inter-attentionEncoder\nDecoder\nNx NxEmbEmbEmbEmbEmbEmbEmbEmbEmb EmbEmbEmbEmbEmbEmbEmbEmbEmb\nIs    An@@ Celotti  the     man     for     the     job    ?\nTarget Sequence y’\nSyntactic and Semantic Structure-aware Mapping\nAlignment\nFor the text, [Is Ancelotti the man for the job ?], it will be tokenized to a subword-level sequence, [Is An@@, celotti, the, man, for, the, job, ?]. The POS tag for this sentences is [AUX, PRON, PRON, DET, NOUN, ADP, DET, NOUN, PUNCT], where AUX means auxiliary, PRON means pronoun, DET means determiner, NOUN means noun, ADP means ad-position, and PUNCT means punctuation. The NER tag for this sentences is [O, B_PERSON, I_PERSON, O, O,O, O, O]\nO OO O O O OB_PERSON O OOI_PERSONO O O OB_PERSON\nO OO O O O OB_PERSON\nI_PERSON\nI_PERSON\nEmbEmbEmbEmbEmbEmbEmbEmbEmb EmbEmbEmbEmbEmbEmbEmbEmbEmb\nMLPMLPMLPMLPMLPMLPMLPMLPMLP\nFigure 1: An overview of the proposed SNAT for neural machine translation.\ntokenizer (Sennrich et al., 2016). In parallel, words\nin the input sequence are passed to POS and NER\nannotators to extract explicit syntactic and semantic\nstructures, and the corresponding embeddings are\naggregated by a linear layer to form the ﬁnal syn-\ntax and semantic structure-aware embedding. The\nSNAT model copies the structured encoder input\nas the decoder input and generates the translated\nsentences and labels.\nOne of the most important properties ofSNATis\nthat it naturally introduces syntactic and semantic\ninformation when taking the structure-aware infor-\nmation as inputs and generating both words and\nlabels. More precisely, given a source sentencex,\nas well as its label sequence Lx, the conditional\nprobability of a target translation y and its label\nsequence Ly is:\nPSNAT(y,Ly|x,Lx; ϕ)\n=\nm∏\nt=1\np\n(\nyt,Lyt |ˆx,ˆLx,x,Lx; ϕ\n)\n, (2)\nwhere x and Lx are ﬁrst fed into the encoder of\nSNATmodel. ˆx and ˆLx with length mare syntactic\nand semantic structure-aware copying of word and\nlabel from encoder inputs, respectively. We show\nthe details in the following sections.\n3.1 Syntactic and Semantic Labeling\nWe use POS and NER to introduce the syntactic and\nsemantic information existing in natural language,\nrespectively. During the data pre-processing, each\nsentence is annotated into a semantic sequence us-\ning an open-source pre-trained semantic annotator.\nIn particular, we take the Treebank style (Marcus\net al., 1999) for POS and PropBank style (Palmer\net al., 2005) for NER to annotate every token of\ninput sequence with semantic labels. Given a spe-\nciﬁc sentence, there would be predicate-argument\nstructures. Since the input sequence is segmented\ninto subword units using byte-pair encoding (Sen-\nnrich et al., 2016), we assign the same label to all\nsubwords tokenized from the same word. As shown\nin Figure 1, the word “ Ancelotti” is tokenized as\n“An@@” and “Celotti”. The corresponding POS\ntags are PRON and PRON while the corresponding\nNER tags are B PERSON and I PERSON. For the\ntext “Is An@@ Celotti the man for the job ?”, the\ncorresponding POS tag set is{AUX, PRON, PRON,\nDET, NOUN, ADP, DET, NOUN, PUNCT}and\nthe NER tag set is {O, B PERSON, I PERSON,\nO, O, O, O, O, O}. The data ﬂow of the proposed\nmodel is also shown in Figure 1.\n3.2 Encoder\nFollowing Transformer (Vaswani et al., 2017), we\nuse a stack of 6 identical Transformer blocks as\nencoder. In addition to the word embedding and po-\nsition embedding in the traditional Transformer, we\nadd structure-aware label embedding. The input to\nthe encoder block is the addition of the normalized\nword, labels (NER and POS) and position embed-\nding, which is represented as H0 = [h0\n1,..., h0\nn].\nThe input representation H0 = [h0\n1,..., h0\nn]\nis encoded into contextual layer representations\nthrough the Transformer blocks. For each layer,\nthe layer representation Hl = [hl\n1,..., hl\nn] is com-\nputed by the l-th layer Transformer block Hl =\nTransformerl(Hl−1), l ∈ {1,2,..., 6}. In each\n1238\nTransformer block, multiple self-attention heads\nare used to aggregate the output vectors of the pre-\nvious layer. A general attention mechanism can be\nformulated as the weighted sum of the value vector\nV using the query vector Q and the key vector K:\nAtt(Q,K,V) = softmax\n( QKT\n√dmodel\n)\n·V, (3)\nwhere dmodel represents the dimension of hidden\nrepresentations. For self-attention, Q, K, and V\nare mappings of previous hidden representation\nby different linear functions, i.e., Q = Hl−1Wl\nQ,\nK = Hl−1Wl\nK, and V = Hl−1Wl\nV , respectively.\nAt last, the encoder produces a ﬁnal contextual rep-\nresentation H6 = [h6\n1,..., h6\nn], which is obtained\nfrom the last Transformer block.\n3.3 Decoder\nThe decoder also consists of 6 identical Trans-\nformer blocks, but with several key differences\nfrom the encoder. More concretely, we denote the\ncontextual representations in the i-th decoder layer\nis Zi(1 ≤i≤6). The input to the decoder block\nas Z0 = [z0\n1,..., z0\nm], which is produced by the ad-\ndition of the word, labels (NER and POS) copying\nfrom encoder input and positional embedding.\nFor the target side input [ˆx,ˆLx], most previous\nworks simply copied partial source sentence with\nthe length ratio n\nm where n refers to the source\nlength and m is the target length as the decoder\ninput. More concretely, the decoder input yi at the\ni-th position is simply a copy of the ⌊n\nm ×i⌋th\ncontextual representation, i.e., x⌊n\nm ×i⌋from the en-\ncoder. From our investigation, in most cases, the\ngap between source length and target length is rel-\natively small (e.g. 2). Therefore, it deletes or du-\nplicates the copy of the last a few tokens of the\nsource. If the last token is meaningful, the deletion\nwill neglect important information. Otherwise, if\nthe last token is trivial, multiple copies will add\nnoise to the model.\nInstead, we propose a syntactic and semantic\nstructure-aware mapping method considering the\nPOS and NER labels to construct the decoder in-\nputs. Our model ﬁrst picks out the informative\nwords with NOUN and VERB POS tags, and the\nones recognized as entities by the NER module. If\nthe source length is longer than the target length,\nwe retain all informative words, and randomly\ndelete the rest of the words. On the other hand,\nif the source length is shorter than the target, we\nretain all words and randomly duplicate the infor-\nmative words. The corresponding label of a word\nis also deleted or preserved. Moreover, by copying\nthe similar structural words from the source, it can\nprovide more information to the target input than\njust copying the source token, which is greatly dif-\nferent from the target token. The POS and NER\nlabels of those structure-aware copied words from\nthe source sentence are also copied as the decoder\ninput. So by using the structure-aware mapping, we\ncan assign [ˆx,ˆLx] as decoder input.\nFor positional attention which aims to learn the\nlocal word orders within the sentence (Gu et al.,\n2018), we set positional embedding (Vaswani et al.,\n2017) as both Q and K, and hidden representations\nof the previous layer as V.\nFor inter-attention, Q refers to hidden represen-\ntations of the previous layer, whereas K and V\nare contextual vectors H6 from the encoder. We\nmodify the attention mask so that it does not mask\nout the future tokens, and every token is depen-\ndent on both its preceding and succeeding tokens\nin every layer. Therefore, the generation of each\ntoken can use bi-directional attention. The position-\nwise Feed-Forward Network (FFN) is applied after\nmulti-head attention in both encoder and decoder.\nIt consists of two fully-connected layers and a layer\nnormalization (Ba et al., 2016). The FFN takes Z6\nas input and calculates the ﬁnal representation Zf ,\nwhich is used to predict the whole target sentence\nand label:\np\n(\ny |ˆx,ˆLx,x,Lx\n)\n= f\n(\nZf W⊤\nw + bw\n)\n, (4)\nq\n(\nLy |ˆx,ˆLx,x,Lx\n)\n= f\n(\nZf W⊤\nl + bl\n)\n, (5)\nwhere f is a GeLU activation function (Hendrycks\nand Gimpel, 2016). Ww and Wl are the token em-\nbedding and structural label embedding in the input\nrepresentation, respectively. We use different FFNs\nfor POS and NER labels. To avoid redundancy, we\njust use q\n(\nLy |ˆx,ˆLx,x,Lx\n)\nto represent the two\npredicted label likelihood in general.\n3.4 Training\nWe use ( x, Lx, y∗, L∗\ny) to denote a training in-\nstance. To introduce the label information, our\nproposed SNAT contains a discrete sequential la-\ntent variable Ly1:m with conditional posterior dis-\ntribution p(Ly1:m |ˆx,ˆLx,x,Lx; ϕ). It can be ap-\nproximated using a proposal distribution q(Ly |\n1239\nˆx,ˆLx,x,Lx). The approximation also provides a\nvariational bound for the maximum log-likelihood:\nlog PSNAT = log\nm∑\nt=1\nq\n(\nLyt |ˆx,ˆLx,x,Lx; ϕ\n)\n×p\n(\nyt|Lyt ,ˆx,ˆLx,x,Lx; ϕ\n)\n≥ E\nLy1:m ∼q\n{ m∑\nt=1\nlog q\n(\nLyt |ˆx,ˆLx,x,Lx; ϕ\n)\n  \nLabel likelihood\n+\nm∑\nt=1\nlog p\n(\nyt |Lyt ,ˆx,ˆLx,x,Lx; ϕ\n)\n  \nStructure-aware word likelihood\n}\n+ H(q).\n(6)\nNote that, the resulting likelihood function, con-\nsisting of the two bracketed terms in Eq. (6), allows\nus to train the entire model in a supervised fashion.\nThe inferred label can be utilized to train the label\npredicting model qand simultaneously supervise\nthe structure-aware word model p. The label loss\ncan be calculated by the cross-entropy Hof L∗\nyt\nand Eq. (5):\nLlabel =\nm∑\nt=1\nH\n(\nL∗\nyt , q(Lyt |ˆx,ˆLx,x,Lx)\n)\n,\n(7)\nThe structure-aware word likelihood is conditioned\non the generation result of the label. Since the\nEq. (4) does not depend on the predicted label,\nwe propose to bring the structure-aware word mask\nMwl ∈R|Vword|×|Vlabel|, where |Vword|and |Vlabel|\nare vocabulary sizes of word and label, respectively.\nThe mask Mwl is deﬁned as follows:\nMwl (i,j) =\n{ 1, A(yi) =labelj,\nϵ, A(yi) ̸= labelj, (8)\nwhich can be obtained at the preprocessing stage,\nand Adenotes the open-source pre-trained POS or\nNER annotator mentioned above. It aims to penal-\nize the case when the word yi does not belong to\nthe label labelj with ϵ, which is a small number\ndeﬁned within the range of (0,1) and will be tuned\nin our experiments. For example, the word “great”\ndoes not belong to VERB. The structure-aware\nword likelihood can be reformulated as:\np(yt |Lyt ,ˆx,ˆLx,x,Lx; ϕ) =p(yt |ˆx,ˆLx,x,Lx)\n×Mwl ×q(Lyt |ˆx,ˆLx,x,Lx).\n(9)\nConsequently, the structure-aware word\nloss Lword is deﬁned as the cross-entropy\nbetween true p\n′\n(y∗\nt |L∗\nyt ) and predicted\np(yt | Lyt ,ˆx,ˆLx,x,Lx; ϕ), where p\n′\n(y∗\nt |L∗\nyt )\n∈ R|Vword|×|Vlabel| is a matrix where only item\nat the index of (y∗\nt ,L∗\nyt ) equals to 1, otherwise\nequals to 0. We reshape p\n′\n(y∗\nt |L∗\nyt ) and p(yt|Lyt )\nto vectors when calculating the loss.\nIntermediate Alignment Regularization One\nmain problem of NAT is that the decoder gener-\nation process does not depend on the previously\ngenerated tokens. Based on the bidirectional na-\nture of SNAT decoder, the token can depend on\nevery token of the decoder input. However, since\nthe input of decoder [ˆx,ˆLx] is the duplicate of en-\ncoder input [x,Lx], the generation depends on the\nencoder tokens rather than the target y∗.\nTo solve this problem, we align the output of the\nintermediate layer of the decoder with the target.\nThe alignment makes the generation of following\nlayers dependent on the coarse target-side infor-\nmation instead of the mere encoder input. This\nalignment idea is inspired by (Guo et al., 2019),\nwhich directly feeds target-side tokens as inputs\nof the decoder by linearly mapping the source to-\nken embeddings to target embeddings. However,\nusing one FFN layer to map different languages to\nthe same space can hardly provide promising re-\nsults. Thus, instead of aligning the input of decoder\nwith the target, we use the intermediate layer of\ndecoder to align with the target. In this case, our\nmodel avoids adding additional training parameters\nand manages to train the alignment together with\nSNAT model in an end-to-end fashion. Formally,\nwe deﬁne the intermediate alignment regularization\nas cross-entropy loss between the predicted word\nand the true word:\nLreg =\nm∑\nt=1\nH\n(\ny∗\nt , FFN(Zmd\nt )\n)\n, (10)\nwhere Zmd (1 < md <6) represents the output\nof each intermediate layer. Consequently, the ﬁnal\nloss of SNAT can be represented with the coefﬁ-\ncient λas:\nLSNAT = Lword + Llabel + λLreg. (11)\n4 Experiment\nIn this section, we conduct experiments to evaluate\nthe effectiveness and efﬁciency of our proposed\nmodel, with comprehensive analysis.\n1240\nTable 2: Performance of BLEU score on WMT14 En↔De and WMT16 En↔Ro tasks. “-” denotes that the results\nare not reported. LSTM-based results are from (Wu et al., 2016); CNN-based results are from (Gehring et al.,\n2017). †The Transformer (Vaswani et al., 2017) results are based on our own reproduction.\nEn→De De →En En →Ro Ro →En\nAutoregressive Models Latency Speedup\nLSTM Seq2Seq(Bahdanau et al., 2017) 24.60 - - - - -\nConv S2S(Gehring et al., 2017) 26.43 - 30.02 - - -\nTransformer†(Vaswani et al., 2017) 27.48 31.29 34.36 33.82 642ms 1.00X\nNon-autoregressive Models Latency Speedup\nNAT(Gu et al., 2018) 17.69 20.62 29.79 - 39ms 15.6X\nNAT, rescoring 10(Gu et al., 2018) 18.66 22.41 - - 79ms 7.68X\nNAT, rescoring 100(Gu et al., 2018) 19.17 23.20 - - 257ms 2.36X\niNAT(Lee et al., 2018) 21.54 25.43 29.32 - - 5.78X\nHint-NAT(Li et al., 2020) 21.11 25.24 - - 26ms 23.36X\nFlowSeq-base(Ma et al., 2019) 21.45 26.16 - 29.34 - -\nENAT-P(Guo et al., 2019) 20.26 23.23 29.85 - 25ms 24.3X\nENAT-P, rescoring 9 23.22 26.67 34.04 - 50ms 12.1X\nENAT-E 20.65 23.02 30.08 - 24ms 25.3X\nENAT-E, rescoring 19 24.28 26.10 34.51 - 49ms 12.4X\nDCRF-NAT(Sun et al., 2019) 23.44 27.22 - - 37ms 16.4X\nDCRF-NAT, rescoring 9 26.07 29.68 - - 63ms 6.1X\nDCRF-NAT, rescoring 19 26.80 30.04 - - 88ms 4.4X\nNAR-MT(rescoring 11)(Zhou and Keung, 2020) 23.57 29.01 31.21 32.06 - -\nNAR-MT(rescoring 11) + monolingual 25.53 29.96 31.91 33.46 - -\nAXE CMLM(Ghazvininejad et al., 2020) 23.53 27.90 30.75 31.54 - -\nSNAT 24.64 28.42 32.87 32.21 26.88ms 22.6X\nSNAT, rescoring 9 26.87 30.12 34.93 33.11 54.63ms 11.1X\nSNAT, rescoring 19 27.50 30.82 35.19 33.98 65.62ms 9.3X\n4.1 Experimental Setup\nData We evaluate SNAT performance on both\nthe WMT14 En-De (around 4.5M sentence pairs)\nand the WMT16 En-Ro (around 610k sentence\npairs) parallel corpora. For the parallel data, we\nuse the processed data from (Ghazvininejad et al.,\n2019) to be consistent with previous publications.\nThe dataset is processed with Moses script (Hoang\nand Koehn, 2008), and the words are segmented\ninto subword units using byte-pair encoding (Sen-\nnrich et al., 2016). The WMT14 En-De task\nuses newstest-2013 and newstest-2014 as devel-\nopment and test sets, and WMT16 En-Ro task uses\nnewsdev-2016 and newstest-2016 as development\nand test sets. We report all results on test sets. The\nvocabulary is shared between source and target\nlanguages and has ∼36k units and ∼34k units in\nWMT14 En-De and WMT16 En-Ro, respectively.\nModel Conﬁguration Our implementation is\nbased on the PyTorch sequence modeling toolkit\nFairseq.1 We follow the weights initialization\nscheme from BERT and follow the settings of the\nbase Transformer conﬁguration in (Vaswani et al.,\n1https://github.com/pytorch/fairseq\n2017) for all the models: 6 layers per stack, 8 at-\ntention heads per layer, 512 model dimensions and\n2,048 hidden dimensions. The dimension of POS\nand NER embedding is set to 512 which is the same\nas the word embedding dimension. The autoregres-\nsive and non-autoregressive model have the same\nencoder-decoder structure, except for the decoder\nattention mask and the decoding input for the non-\nautoregressive model as described in Sec. 3. We\ntry different values for the label mismatch penalty\nϵ from {0.01,0.1,0.5}and ﬁnd that 0.1 gives\nthe best performance. The coefﬁcient λis tested\nwith different values from {0.25,0.5,0.75,1}, and\nλ = 0.75 outperforms other settings. We set the\ninitial learning rate as values from {8e-6, 1e-5, 2e-\n5, 3e-5}, with a warm-up rate of 0.1 and L2 weight\ndecay of 0.01. Sentences are tokenized and the\nmaximum number of tokens in each step is set to\n8,000. The maximum iteration step is set to 30,000,\nand we train the model with early stopping.\nBaselines We choose the following models as\nbaselines: NAT is a vanilla non-autoregressive\nTransformer model for NMT which is ﬁrst intro-\nduced in (Gu et al., 2018). iNAT (Lee et al., 2018)\nextends the vanilla NAT model by iteratively read-\n1241\ning and reﬁning the translation. The number of\niterations is set to 10 for decoding. Hint-NAT (Li\net al., 2020) utilizes the intermediate hidden states\nfrom an autoregressive teacher to improve the NAT\nmodel. FlowSeq (Ma et al., 2019) adopts normal-\nizing ﬂows (Kingma and Dhariwal, 2018) as latent\nvariables for generation. ENAT (Guo et al., 2019)\nproposes two ways to enhance the decoder inputs\nto improve NAT models. The ﬁrst one leverages a\nphrase table to translate source tokens to target to-\nkens ENAT-P. The second one transforms source-\nside word embedding into target-side word em-\nbeddings ENAT-E. DCRF-NAT (Sun et al., 2019)\ndesigns an approximation of CRF for NAT models\nand further uses a dynamic transition technique\nto model positional context in the CRF. NAR-\nMT (Zhou and Keung, 2020) uses a large num-\nber of source texts from monolingual corpora to\ngenerate additional teacher outputs for NAR-MT\ntraining. AXE CMLM (Ghazvininejad et al., 2020)\ntrains the conditional masked language models us-\ning a differentiable dynamic program to assign loss\nbased on the best possible monotonic alignment\nbetween target tokens and model predictions.\n4.2 Training and Inference Details\nTo obtain the part-of-speech and named entity la-\nbels, we use industrial-strength spaCy2 to acquire\nthe label for English, German, and Romanian input.\nIn our implementation, there are 17 labels for POS\nin total, i.e., ADJ (adjective), ADV (adverb), ADP\n(ad-position), AUX (auxiliary), CCONJ (coordi-\nnating conjunction), DET (determiner), INTJ (in-\nterjection), NOUN (noun), NUM (numeral), PART\n(particle), PRON (pronoun), PROPN (proper noun),\nPUNCT (punctuation), SCONJ (subordinating con-\njunction), SYM (symbol), VERB (verb), and X\n(other). The NER task is trained on OntoNotes\nv5.0 benchmark dataset (Pradhan et al., 2013) using\nformatted BIO labels and deﬁnes 18 entity types:\nCARDINAL, DATE, EVENT, FAC, GPE, LAN-\nGUAGE, LAW, LOC, MONEY , NORP, ORDINAL,\nORG, PERCENT, PERSON, PRODUCT, QUAN-\nTITY , TIME, and WORKOF ART.\nKnowledge Distillation Similar to previous\nworks on non-autoregressive translation (Gu et al.,\n2018; Shu et al., 2020; Ghazvininejad et al.,\n2019), we use sequence-level knowledge distil-\nlation by training SNAT on translations gener-\nated by a standard left-to-right Transformer model\n2https://spacy.io/usage/models\n(i.e., Transformer-large for WMT14 EN→DE, and\nTransformer-base for WMT16 EN→RO). Specif-\nically, we use scaling NMT (Ott et al., 2018) as\nthe teacher model. We report the performance of\nstandard autoregressive Transformer trained on\ndistilled data for WMT14 EN→DE and WMT16\nEN→RO. We average the last 5 checkpoints to\nobtain the ﬁnal model. We train the model with\ncross-entropy loss and label smoothing (ϵ= 0.1).\nInference During training, we do not need to pre-\ndict the target length msince the target sentence is\ngiven. During inference, we use a simple method\nto select the target length for SNAT (Wang et al.,\n2019; Li et al., 2020). First, we set the target length\nto m′= n+ C, where nis the length of the source\nsentence and C is a constant bias term estimated\nfrom the overall length statistics of the training data.\nThen, we create a list of candidate target lengths\nwith a range of [m′−B,m′+ B] where Bis the\nhalf-width of the interval. Finally, the model picks\nthe best one from the generated 2B+ 1candidate\nsentences. In our experiments, we set the constant\nbias term C to 2 for WMT 14 EN →DE, -2 for\nWMT 14 DE→EN, 3 for WMT 16 EN→RO, and\n-3 for WMT 14 RO →EN according to the aver-\nage lengths of different languages in the training\nsets. We set Bto 4 or 9, and obtain corresponding\n9 or 19 candidate translations for each sentence.\nThen we employ an autoregressive teacher model\nto rescore these candidates.\n4.3 Results and Analysis\nExperimental results are shown in Table 2. We ﬁrst\ncompare the proposed method against autoregres-\nsive counterparts in terms of translation quality,\nwhich is measured by BLEU (Papineni et al., 2002).\nFor all our tasks, we obtain results comparable\nwith the Transformer, the state-of-the-art autore-\ngressive model. Our best model achieves 27.50\n(+0.02 gain over Transformer), 30.82 (-0.46 gap\nwith Transformer), 35.19 (+0.82 gain), and 33.98\n(+0.16 gain) BLEU score on WMT14 En↔De and\nWMT16 EN↔Ro, respectively. More importantly,\nour SNAT decodes much faster than the Trans-\nformer, which is a big improvement regarding the\nspeed-accuracy trade-off in AT and NAT models.\nComparing our models with other NAT models,\nwe observe that the best SNAT model achieves\na signiﬁcant performance boost over NAT, iNAT,\nHint-NAT, FlowSeq, ENAT, NAR-MT and AXE\nCMLM by +8.33, +5.96, +6.39, +6.05, +3.22, 3.93\n1242\nand +3.97 in BLEU on WMT14 En→De, respec-\ntively. This indicates that the incorporation of the\nsyntactic and semantic structure greatly helps re-\nduce the impact of the multimodality problem and\nthus narrows the performance gap between Autore-\ngressive Transformer (AT) and Non-Autoregressive\nTransformer (NAT) models. In addition, we see\na +0.69, +0.78, +0.68, and 0.52 gain of BLEU\nscore over the best baselines on WMT14 En→De,\nWMT14 De→En, WMT16 En→Ro and WMT16\nRo→En, respectively.\nFrom the result of our methods at the last group\nin Table 2, we ﬁnd that the rescoring technique sub-\nstantially assists in improving the performance, and\ndynamic decoding signiﬁcantly reduces the time\nspent on rescoring while further accelerating the\ndecoding process. On En→De, rescoring 9 candi-\ndates leads to a gain of +2.23 BLEU, and rescoring\n19 candidates gives a +2.86 BLEU score increment.\nDecoding Speed Following previous works (Gu\net al., 2018; Lee et al., 2018; Guo et al., 2019),\nwe evaluate the average per-sentence decoding la-\ntency on WMT14 En→De test sets with batch size\nbeing 1, under an environment of NVIDIA Titan\nRTX GPU for the Transformer model and the NAT\nmodels to measure the speedup. The latencies are\nobtained by taking an average of ﬁve runs. More\nclearly, We reproduce the Transformer on our ma-\nchine. We copy the runtime of other models but the\nspeedup ratio is between the runtime of their imple-\nmented Transformer and their proposed model. We\nthink it’s reasonable to compare the speedup ratio\nbecause it is independent of the inﬂuence caused\nby different implementation software or machines.\nAnd to clarify, the latency does not include prepro-\ncessing of tagging, because it’s a very fast process\nas executing around 7000 sentences in one second.\nWe can see from Table 2 that the best SNAT\ngets a 9.3 times decoding speedup than the Trans-\nformer, while achieving comparable or even better\nperformance. Compared to other NAT models, we\nobserve that the SNAT model is almost the fastest\n(only a little bit behind of ENAT and Hint-NAT)\nin terms of latency, and is surprisingly faster than\nDCRF-NAT with better performance.\n4.4 Ablation Analysis\nEffect of Syntactic and Semantic Structure In-\nformation We investigate the effect of using the\nsyntactic and semantic tag on the model perfor-\nmance. Experimental results are shown in Table 3.\nTable 3: The performance of different vision of SNAT\nmodels on WMT14 En→De development set.\u0014means\nselecting the label tag.\nModel POS tag NER tag BLEU\nSNAT-V1 \u0014 24.21\nSNAT-V2 \u0014 24.09\nSNAT-V3 22.84\nTable 4: The performance with respect to using differ-\nent layer of intermediate interaction. Evaluated by the\nBLEU score on WMT14 En→De|WMT14 De→En.\nMethod WMT14 En→De WMT14 De→En\nw/o 23.11 27.03\nw/Z2 24.32 28.21\nw/Z3 24.57 28.42\nIt demonstrates that incorporating POS informa-\ntion boosts the translating performance (+1.37 on\nWMT14 En→De) and NER information can also\nenhance the translating performance (+1.25 on\nWMT14 En →De). The POS label enriches the\nmodel with the syntactic structure, while the NER\nlabel supplements the semantic information to the\nmodel which are critical elements for SNATmodel\nto exhibit better translation performance.\nEffect of Intermediate Representation Align-\nment We conduct experiments for our SNAT\nmodel on WMT14 En →De with various align-\nments between decoder layers and target. As shown\nin Table 4, using the second layer Z2 in the de-\ncoder as intermediate alignment can gain +1.21\nimprovement, while using the third layer Z3 in the\ndecoder as intermediate alignment can gain +1.46\nimprovement. This is in line with our expectation\nthat aggregating layer-wise token information in\nintermediate layers can help improve the decoder’s\nability to capture token-token dependencies.\nEffect of Sentence Length To evaluate differ-\nent models on different sentence lengths, we con-\nduct experiments on the WMT14 En→De develop-\nment set and divide the sentence pairs into different\nlength buckets according to the length of the refer-\nence sentences. As shown in Table 5, the column\nof 100 calculates the BLEU score of sentences that\nthe length of the reference sentence is larger than\n50 but smaller or equal to 100. We can see that\nthe performance of vanilla NAT drops quickly as\nthe sentence length increases from 10 to 50, while\nAT model and the proposed SNAT model have\nrelatively stable performance over different sen-\n1243\nTable 5: The performance with respect to different\nsentence lengths. Evaluated by the BLEU score on\nWMT14 En→De.\nModel 10 20 30 50 100\nAT 28.35 28.32 28.30 24.26 20.73\nNAT 21.31 19.55 17.19 16.31 11.35\nSNAT 28.67 28.50 27.33 25.41 17.69\ntence lengths. This result conﬁrms the power of\nthe proposed model in modeling long-term token\ndependencies.\n5 Conclusion\nIn this paper, we have proposed a novel syntactic\nand semantic structure-aware non-autoregressive\nTransformer model SNAT for NMT. The proposed\nmodel aims at reducing the computational cost in\ninference as well as keeping the quality of transla-\ntion by incorporating both syntactic and semantic\nstructures existing among natural languages into\na non-autoregressive Transformer. In addition, we\nhave also designed an intermediate latent align-\nment regularization within target sentences to bet-\nter learn the long-term token dependencies. Com-\nprehensive experiments and analysis on two real-\nworld datasets (i.e., WMT14 En→De and WMT16\nEn→Ro) verify the efﬁciency and effectiveness of\nour proposed approach.\nAcknowledgements\nThis work is supported in part by NSF under grants\nIII-1763325, III-1909323, and SaTC-1930941.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nDzmitry Bahdanau, Philemon Brakel, Kelvin Xu,\nAnirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron\nCourville, Dzmitry Bengio, Yoshua Bahdanau,\nPhilemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan\nLowe, Joelle Pineau, Aaron Courville, and Yoshua\nBengio. 2017. An actor-critic algorithm for se-\nquence prediction. In 5th International Conference\non Learning Representations, ICLR 2017.\nWilliam Chan, Nikita Kitaev, Kelvin Guu, Mitchell\nStern, and Jakob Uszkoreit. 2019. Kermit: Gener-\native insertion-based modeling for sequences. arXiv\npreprint arXiv:1906.01604.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N. Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings\nof the 34th International Conference on Machine\nLearning, volume 70, pages 1243–1252. PMLR.\nMarjan Ghazvininejad, Vladimir Karpukhin, Luke\nZettlemoyer, and Omer Levy. 2020. Aligned cross\nentropy for non-autoregressive machine translation.\nIn Proceedings of the International Conference on\nMachine Learning, pages 9330–9338.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 6114–\n6123.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor O.K. Li, and Richard Socher. 2018. Non-\nautoregressive neural machine translation. In Inter-\nnational Conference on Learning Representations.\nJunliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and\nTie-Yan Liu. 2019. Non-autoregressive neural ma-\nchine translation with enhanced decoder input. In\nProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence, volume 33, pages 3723–3730.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nHieu Hoang and Philipp Koehn. 2008. Design of the\nmoses decoder for statistical machine translation. In\nSoftware Engineering, Testing, and Quality Assur-\nance for Natural Language Processing , pages 58–\n65.\nLukasz Kaiser, Samy Bengio, Aurko Roy, Ashish\nVaswani, Niki Parmar, Jakob Uszkoreit, and Noam\nShazeer. 2018. Fast decoding in sequence models\nusing discrete latent variables. In Proceedings of the\n35th International Conference on Machine Learn-\ning, volume 80 of Proceedings of Machine Learn-\ning Research, pages 2390–2399, Stockholmsm¨assan,\nStockholm Sweden. PMLR.\nDurk P Kingma and Prafulla Dhariwal. 2018. Glow:\nGenerative ﬂow with invertible 1x1 convolutions. In\nAdvances in neural information processing systems ,\npages 10215–10224.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural se-\nquence modeling by iterative reﬁnement. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1173–\n1182, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nZhuohan Li, Zi Lin, Di He, Fei Tian, Tao Qin, Liwei\nWang, and Tie-Yan Liu. 2020. Hint-based training\nfor non-autoregressive machine translation. In Pro-\nceedings of the International Conference on Learn-\ning Representations.\n1244\nJindˇrich Libovick ´y and Jind ˇrich Helcl. 2018. End-to-\nend non-autoregressive neural machine translation\nwith connectionist temporal classiﬁcation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 3016–\n3021, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nXuezhe Ma, Chunting Zhou, Xian Li, Graham Neu-\nbig, and Eduard Hovy. 2019. FlowSeq: Non-\nautoregressive conditional sequence generation with\ngenerative ﬂow. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 4282–4292, Hong Kong, China. As-\nsociation for Computational Linguistics.\nMitchell P Marcus, Beatrice Santorini, Mary Ann\nMarcinkiewicz, and Ann Taylor. 1999. Treebank-3.\nLinguistic Data Consortium, Philadelphia, 14.\nMyle Ott, Sergey Edunov, David Grangier, and Michael\nAuli. 2018. Scaling neural machine translation. In\nProceedings of the Third Conference on Machine\nTranslation: Research Papers, pages 1–9, Brussels,\nBelgium. Association for Computational Linguis-\ntics.\nMartha Palmer, Daniel Gildea, and Paul Kingsbury.\n2005. The proposition bank: An annotated cor-\npus of semantic roles. Computational linguistics ,\n31(1):71–106.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Compu-\ntational Linguistics, pages 311–318.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nHwee Tou Ng, Anders Bj ¨orkelund, Olga Uryupina,\nYuchen Zhang, and Zhi Zhong. 2013. Towards ro-\nbust linguistic analysis using ontonotes. In Pro-\nceedings of the Seventeenth Conference on Computa-\ntional Natural Language Learning, pages 143–152.\nChitwan Saharia, William Chan, Saurabh Saxena, and\nMohammad Norouzi. 2020. Non-autoregressive ma-\nchine translation with latent alignments. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n1098–1108, Online. Association for Computational\nLinguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1715–1725, Berlin, Germany. Asso-\nciation for Computational Linguistics.\nChenze Shao, Jinchao Zhang, Yang Feng, Fandong\nMeng, and Jie Zhou. 2020. Minimizing the bag-of-\nngrams difference for non-autoregressive neural ma-\nchine translation. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence , volume 34, pages\n198–205.\nRaphael Shu, Jason Lee, Hideki Nakayama, and\nKyunghyun Cho. 2020. Latent-variable non-\nautoregressive neural machine translation with deter-\nministic inference using a delta posterior. In AAAI,\npages 8846–8853.\nMitchell Stern, William Chan, Jamie Kiros, and Jakob\nUszkoreit. 2019. Insertion transformer: Flexible se-\nquence generation via insertion operations. In Pro-\nceedings of the 36th International Conference on\nMachine Learning, volume 97, pages 5976–5985.\nZhiqing Sun, Zhuohan Li, Haoqing Wang, Di He,\nZi Lin, and Zhihong Deng. 2019. Fast structured de-\ncoding for sequence models. In Advances in Neural\nInformation Processing Systems, pages 3011–3020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nYiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang\nZhai, and Tie-Yan Liu. 2019. Non-autoregressive\nmachine translation with auxiliary regularization. In\nProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence, volume 33, pages 5377–5384.\nBingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang\nLin, and Xu Sun. 2019. Imitation learning for non-\nautoregressive neural machine translation. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 1304–\n1312, Florence, Italy. Association for Computational\nLinguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n2016. Google’s neural machine translation system:\nBridging the gap between human and machine trans-\nlation. arXiv preprint arXiv:1609.08144.\nJiawei Zhou and Phillip Keung. 2020. Improving\nnon-autoregressive neural machine translation with\nmonolingual data. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1893–1898, Online. Association\nfor Computational Linguistics.",
  "topic": "Autoregressive model",
  "concepts": [
    {
      "name": "Autoregressive model",
      "score": 0.869368314743042
    },
    {
      "name": "Machine translation",
      "score": 0.8685718178749084
    },
    {
      "name": "Computer science",
      "score": 0.834586501121521
    },
    {
      "name": "Artificial intelligence",
      "score": 0.629629373550415
    },
    {
      "name": "Transformer",
      "score": 0.6076583862304688
    },
    {
      "name": "Natural language processing",
      "score": 0.474138468503952
    },
    {
      "name": "Security token",
      "score": 0.4465590715408325
    },
    {
      "name": "STAR model",
      "score": 0.4384026825428009
    },
    {
      "name": "Machine learning",
      "score": 0.39234524965286255
    },
    {
      "name": "Speech recognition",
      "score": 0.37735939025878906
    },
    {
      "name": "Time series",
      "score": 0.1580601930618286
    },
    {
      "name": "Autoregressive integrated moving average",
      "score": 0.13596612215042114
    },
    {
      "name": "Mathematics",
      "score": 0.09339424967765808
    },
    {
      "name": "Econometrics",
      "score": 0.09333276748657227
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39422238",
      "name": "University of Illinois Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I47720641",
      "name": "Huazhong University of Science and Technology",
      "country": "CN"
    }
  ]
}