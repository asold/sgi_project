{
    "title": "Enhancing Inference Efficiency in Large Language Models through Rapid Feed-Forward Information Propagation",
    "url": "https://openalex.org/W4399652644",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2629266920",
            "name": "Damian Gomez",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3191972092",
            "name": "Julian Escobar",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4375869868",
        "https://openalex.org/W4377164404",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4319301677",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4398782346",
        "https://openalex.org/W4396667188",
        "https://openalex.org/W4380993086",
        "https://openalex.org/W4372283945",
        "https://openalex.org/W4389983939",
        "https://openalex.org/W4380559074",
        "https://openalex.org/W4364384540",
        "https://openalex.org/W4390309921",
        "https://openalex.org/W4393024052",
        "https://openalex.org/W4396901469",
        "https://openalex.org/W4393867173",
        "https://openalex.org/W4386080903",
        "https://openalex.org/W4377163995",
        "https://openalex.org/W4390092199",
        "https://openalex.org/W4396859483",
        "https://openalex.org/W4399328912",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4399328902"
    ],
    "abstract": "The increasing complexity and computational demands of language models require innovations to enhance their efficiency and performance. The novel approach of rapid feed-forward information propagation presents significant advancements by optimizing the architecture of the Mistral Large model, leading to substantial improvements in inference speed and memory usage. Comprehensive architectural modifications, including parameter sharing and reduced layer depth, streamlined the model's processes, while the integration of additional computational pathways and mixed-precision training further optimized its efficiency. Detailed experimental results demonstrate the effectiveness of these enhancements, showing marked improvements in latency, throughput, and accuracy across various benchmark datasets. The study also highlights the model's robustness and scalability, ensuring reliable performance in diverse deployment scenarios. The implications of these findings are profound, providing a framework for developing more efficient, scalable, and high-performing language models, with broad applicability in real-world natural language processing tasks.",
    "full_text": null
}