{
  "title": "Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models",
  "url": "https://openalex.org/W3026991030",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4297607030",
      "name": "Iter, Dan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281172484",
      "name": "Guu, Kelvin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4290222843",
      "name": "Lansing, Larry",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223899260",
      "name": "Jurafsky, Dan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2761988601",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W1486649854",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2044599851",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2971141916",
    "https://openalex.org/W2952750383",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2962739339"
  ],
  "abstract": "Recent models for unsupervised representation learning of text have employed a number of techniques to improve contextual word representations but have put little focus on discourse-level representations. We propose CONPONO, an inter-sentence objective for pretraining language models that models discourse coherence and the distance between sentences. Given an anchor sentence, our model is trained to predict the text k sentences away using a sampled-softmax objective where the candidates consist of neighboring sentences and sentences randomly sampled from the corpus. On the discourse representation benchmark DiscoEval, our model improves over the previous state-of-the-art by up to 13% and on average 4% absolute across 7 tasks. Our model is the same size as BERT-Base, but outperforms the much larger BERT- Large model and other more recent approaches that incorporate discourse. We also show that CONPONO yields gains of 2%-6% absolute even for tasks that do not explicitly evaluate discourse: textual entailment (RTE), common sense reasoning (COPA) and reading comprehension (ReCoRD).",
  "full_text": "Pretraining with Contrastive Sentence Objectives Improves Discourse\nPerformance of Language Models\nDan Iter*∗\n, Kelvin Guu**, Larry Lansing**, and Dan Jurafsky*\n*Computer Science Department, Stanford University\n**Google Research\n{daniter,jurafsky}@stanford.edu\n{kguu,llansing}@google.com\nAbstract\nRecent models for unsupervised representa-\ntion learning of text have employed a num-\nber of techniques to improve contextual word\nrepresentations but have put little focus on\ndiscourse-level representations. We propose\nCONPONO 1, an inter-sentence objective for\npretraining language models that models dis-\ncourse coherence and the distance between\nsentences. Given an anchor sentence, our\nmodel is trained to predict the text k sen-\ntences away using a sampled-softmax objec-\ntive where the candidates consist of neighbor-\ning sentences and sentences randomly sam-\npled from the corpus. On the discourse rep-\nresentation benchmark DiscoEval, our model\nimproves over the previous state-of-the-art by\nup to 13% and on average 4% absolute across\n7 tasks. Our model is the same size as BERT-\nBase, but outperforms the much larger BERT-\nLarge model and other more recent approaches\nthat incorporate discourse. We also show that\nCONPONO yields gains of 2%-6% absolute\neven for tasks that do not explicitly evaluate\ndiscourse: textual entailment (RTE), common\nsense reasoning (COPA) and reading compre-\nhension (ReCoRD).\n1 Introduction\nPretraining large language models has become the\nprimary method for learning representations from\nunsupervised text corpora. Since the initial im-\nprovements demonstrated by ELMo (Peters et al.,\n2018) and BERT (Devlin et al., 2019), many al-\nternative pretraining methods have been proposed\nto best leverage unlabeled data. These methods\ninclude bi-directional language modeling (Peters\net al., 2018), masked language models (Devlin\net al., 2019), word order permutation (Yang et al.,\n∗ Work done during internship at Google.\n1Code is available at https://github.com/google-\nresearch/language/tree/master/language/conpono and\nhttps://github.com/daniter-cu/DiscoEval\n2019), more robust training (Liu et al., 2019) and\nmore efﬁcient architectures (Lan et al., 2019). How-\never, little focus has been put on learning discourse\ncoherence as part of the pretraining objective.\nWhile discourse coherence has been of great\ninterest in recent natural language processing lit-\nerature (Chen et al., 2019; Nie et al., 2019; Xu\net al., 2019), its beneﬁts have been questioned\nfor pretrained language models, some even opt-\ning to remove any sentence ordering objective\n(Liu et al., 2019). However, in a recently pub-\nlished benchmark for evaluating discourse repre-\nsentations, Chen et al. (2019) found that the best\nperforming model was surprisingly BERT, despite\ncomparing against models speciﬁcally designed\nfor discourse, such as DisSent (Nie et al., 2019)\nand a new recurrent network trained on a large\nrange of sentence ordering objectives. We show\nthat combining transformer encoders with our inter-\nsentence coherence objective, we can further im-\nprove discourse-level representations in language\nmodels.\nWe present a model that trains a sentence-level\nencoder to capture discourse relationships between\nsentences, including ordering, distance and coher-\nence. The encoder is trained by using its output to\npredict spans of text that are someksentences away\nfrom a context in either direction. The predictions\nare made discriminatively with a sampled-softmax\nthat contrasts the correct target sentence against\nnegatives, including hard examples sampled from\nthe same paragraph. Our objective is inspired by\nthe recently proposed Constrastive Predictive Cod-\ning (CPC) (van den Oord et al., 2018), but, among\nother differences, is applied on the sentence-level\nrather than the token-level and is bi-directional.\nWe call this the CONtrastive Position and Ordering\nwith Negatives Objective (CONPONO )2.\n2Also means arrange or order in Latin.\narXiv:2005.10389v1  [cs.CL]  20 May 2020\nWe evaluate our model on DiscoEval (Chen et al.,\n2019), a recently published benchmark for evalu-\nating and probing for various aspects of discourse-\nlevel semantics in representations output by dis-\ncourse models. We observe that the representations\nlearned with CONPONO outperform BERT-Large\nand achieve a new state-of-the-art despite using\nfewer parameters and training on the same data.\nFurthermore, we show that our new objective im-\nproves model performance on other tasks including\ntextual entailment, common-sense reasoning and\nreading comprehension. We compare CONPONO\nagainst BERT-Base on RTE (Giampiccolo et al.,\n2007; Bentivogli et al., 2009), COPA (Roemmele\net al., 2011) and ReCoRD (Zhang et al., 2018),\nwhile controlling for model size, training data and\ntraining time.\nOur main contributions are:\n1. We describe a novel sentence-level discourse\nobjective that is used in conjunction with a\nmasked language model for unsupervised rep-\nresentation learning for text. We show that\nthis objective can leverage the cross-attention\nand pretrained weights of a transformer model\nto learn discourse-level representations.\n2. We show that our model achieves a new state-\nof-the-art on DiscoEval, improving the results\non 5 of the 7 tasks and increasing accuracy\nby up to 13% and an average of over 4% ab-\nsolute across all tasks. We also show 2%-\n6% absolute improvements over Bert-Base on\nRTE, COPA and ReCoRD as evidence that\ndiscourse pretraining can also improve model\nperformance on textual entailment, common-\nsense reasoning and reading comprehension.\n2 Model\nFigure 1 illustrates the CONPONO model. The intu-\nition is that if the model is able to accurately predict\nthe surrounding target sentences given some anchor\ntext, then the vector representations for these sen-\ntences should also be useful for downstream tasks.\nThe input to the model is a paragraph that is split\ninto sentences. A sentence is chosen at random as\nthe anchor, and will be denoted as si. We encode\nsi with a transformer encoder to produce a vector\nci. The surrounding sentences are denoted as si+k\nwhere k ∈[−K.. −1,1 ..K], meaning the maxi-\nmum distance we use is K. We report results for\nK ∈[1..4]. These sentences, si+k, are encoded\njointly with the anchor sentence. We use just a\nsingle encoder gθ so all text is encoded with the\nsame weights. The encoded vectors are namedti+k\nbecause these are the target vectors the model tries\nto identify given the anchor and a target distance k.\nEquation 1 deﬁnes ti+k and ci as a function gθ of\nthe input sentences. Note that the C ONPONO gθ is\ndifferent from the encoder in CPC because we in-\nput both the anchor and the target into the encoder,\nrather than separate anchor and target encoders.\nti+k = gθ(si,si+k), c i = gθ(si) (1)\nGiven the anchor and targets, we deﬁne a log-\nbilinear model in equation 2 to score the plausibility\nof target ti+k being in position k from anchor ci.\nThe full set of parameters for our model is θfor the\nencoder and a Wk for each k. CPC has the same\nbi-linear form as Equation 2 but the architecture\nfor the encoders is different.\nfk(si+k,si) = exp(tT\ni+kWkci) (2)\nThe loss for each kis given in equation 3 where\nthe score for the correct target is contrasted to\nscores of random samplessj, sampled from both in-\ndocument and random sentences from the corpus,\nS.\nLk = −ES\n[\nlog fk(si+k,si)\nΣsj ∈S fk(sj,si)\n]\n(3)\nTo train CONPONO , we sample negative exam-\nples randomly from the corpus and from the same\nparagraph but different kas hard negatives. Note\nthat when |k|is greater than 1, there will be sen-\ntences between the anchor sentence and target sen-\ntence that will be purposely omitted from the input.\nThe missing context is intended to create a chal-\nlenging objective where the model may not be able\nto rely on trivial signals that often appear in con-\ntiguous sentences.\n2.1 Encoder Architectures\nFor each example we encode two text spans, the\nanchor and the target. There are three main options\nfor encoding the two spans into ci and ti+k. The\nsimplest method, and most similar to CPC is to\nencode the anchor and target separately, which we\ncall isolated encoding. With this encoder, equation\n1 will be ti+k = gθ(si+k). The major drawback of\nthis approach is that there is no token-level cross-\nattention between the anchor and the target, which\nhas been shown to generally improve text encoding\nSi-2 Si-1 Si+1 Si+2Si\nEncoder Encoder EncoderEncoder Encoder\nti-2 ti-1 ti+1 ti+2ci\nPredictions\nSr Sr’\nEncoder Encoder\ntr tr’\nRandom Negatives\nFigure 1: During training, a text segment is selected as the anchor ( Si). The anchor as well as all the targets,\nSi−2...Si+2 plus random samples Sr are encoded with the transformer masked language model. The encoded\nrepresentation of the anchor is used to predict each target at its target distance. TheSi objects are raw text sentences,\nthe encoder is the transformer model, and ci and ti are vectors.\n(Vaswani et al., 2017). Cross-attention is the mech-\nanism in neural networks that allows for attention\nto be shared between multiple inputs, in our case,\ntwo separate spans of text.\nAlternatively, we can encode the anchor and tar-\nget together and then dot product the latent vector\nwith a learned vector representation for each dis-\ntance k. We call this approach a uni-encoder. With\nthis encoder, equation 2 will be fk(si+k,si) =\nexp(tT\ni+kwk). The class matrix Wk in equation 2 is\nreplaced by a class vector wk, which has fewer pa-\nrameters. This is similar to the ordering objectives\nin BERT and ALBERT where the pooled represen-\ntation is used for a binary classiﬁcation task and the\nlearned vector representation for each distance kis\njust the softmax weights. The potential drawback\nto this method is that each pair of sentences is rep-\nresented by a single vector. This encoder may learn\na representation that is similar for all examples that\nhave the same label but does not explicitly model\nthe content of the input.\nCONPONO implements the intersection of these\ntwo approaches. The targets are concatenated to\nthe anchor when encoded, to make use of the cross-\nattention of the transformer encoder. The anchor,\nis encoded independently, though with the same\nweights. This objective allows for more freedom\nin the values of ci and ti+k, unlike the uni-encoder.\nFurthermore, since the encoder, gθ, can encode\neither one span (si) or two spans (si,si+k), it can\nbe used for downstream tasks that have either single\n(eg. SSP) or double (eg. BSO) span inputs.\n2.2 Comparing Inter-Sentence Modeling\nObjectives\nThere are different tasks that can be used for learn-\ning inter-sentence representations. BERT (Devlin\net al., 2019) included a next sentence prediction\n(NSP) task. For NSP, two spans are fed into the\nmodel with the second span either being the next\ncontiguous span of text from the source or 50% of\nthe time it is replaced with a random span from\nthe corpus. The task is a binary classiﬁcation of\nwhether the two spans are from the same source.\nALBERT (Lan et al., 2019) compares the NSP ap-\nproach to using no inter-sentence objective and to\nsentence order prediction, which for clarity we re-\nfer to as binary sentence ordering (BSO). For BSO,\nthe input is two spans that are always contiguous\nand from the same source but 50% of the time are\nin reverse order. With CONPONO we capture the\nbeneﬁts of both learning ordering between coherent\nsentences and contrasting against random negatives.\nWe make the objective even more challenging by\nalso predicting order on spans that are multiple sen-\ntences apart, and using other sentences from the\nsame paragraph as harder negatives.\n2.3 Technical details\nIn practice, we use a 512 token input which is much\nlarger than most two sentence pairs. To train on\nlonger sequence lengths, we use 4 sentences as the\nanchor and 3 sentences as the target segment. We\ntruncate longer sentences and pad tokens up to the\nsequence length as done for typical BERT input.\nThere is no overlap between the two segments and\nthe k distance refers to the number of sentences\nomitted between the two segments. For example,\nfor a paragraph we may choose s7..s10 as the an-\nchor and s1..s3 as the target for k= −4 because s3\nis 4 positions behind s7. Since most paragraphs are\nnot long enough to have many sentences in both di-\nrections of a 4 sentence anchor, we randomly select\n4 of the 8 possible ktargets for a given paragraph.\nBecause of the random sampling, we oversample\nshorter distances because they occur more consis-\ntently in the data.\nWe train with 32 input sentences, where 1 is\nthe correct target, 3 are hard negatives from the\nsame document and 28 are random sentences from\nother documents. For fair comparison, we train\non the same data as BERT, using only Wikipedia\nand BooksCorpus (Zhu et al., 2015). We initialize\nour model with BERT-Base weights and train until\nthe model has seen one-fourth as many segment\npairs as the original BERT model ( 32M total),\nso the total compute and iterations of training are\nnot signiﬁcantly greater than BERT-Base. We also\nuse a masked language model objective similar\nto BERT but dynamically mask during training for\ndifferent masks each epoch. When jointly encoding\ntwo inputs, we concatenate the input tokens and\nseparate the two spans with a “[SEP]” token to\nmimic the BERT format.\n3 Evaluation\nWe evaluate our model on the DiscoEval bench-\nmark (Chen et al., 2019) and on the RTE (Giampic-\ncolo et al., 2007; Bentivogli et al., 2009), COPA\n(Roemmele et al., 2011) and ReCoRD (Zhang et al.,\n2018) datasets. We chose the DiscoEval benchmark\nbecause it is intended to evaluate a model’s ability\nto represent the “role of a sentence in its discourse\ncontext”. We also report results on RTE, COPA\nand ReCoRD because these tasks have a discourse\nor sentence ordering aspect to them but are not\nexclusively designed for discourse evaluation.\n3.1 Discourse Evaluation\nTasks: DiscoEval (Chen et al., 2019) is a suite\nof tasks “designed to evaluate discourse-related\nknowledge in pretrained sentence representations”.\nThe benchmark is composed of seven tasks; four\nbased on sentence ordering or coherence (Sentence\nposition (SP), Binary sentence ordering (BSO), Dis-\ncource coherence (DC) and Sentence section pre-\ndiction (SSP)) and three that are based on classi-\nfying the type of relationship between a pair of\ntext sequences (Penn Discourse Tree Bank Explicit\nand Implicit (PDTB-E/I) and Rhetorical structure\ntheory (RST)). PDTB (Prasad et al., 2008) and\nRST (Carlson et al., 2001) are human annotated\ndatasets. Both are multi-class classiﬁcation tasks\nwhere PDTB is classifying a pair of sentences\nwhereas RST is predicting the class of a node\nin a document-level discourse tree. Both classes\nof tasks are critical aspects of understanding dis-\ncourse.\nBaselines: The previously best overall perform-\ning model from DiscoEval (Chen et al., 2019) was\nBERT-Large (Devlin et al., 2019). We also include\nthe results for BERT-Base because our model is\nmost comparable to BERT-Base in terms of pa-\nrameter size, training data and training compute.\nWe also evaluate RoBERTa-Base (Liu et al., 2019)\nbecause it was trained on more data, reported im-\nprovements over BERT-Base on other tasks but\ndropped the next sentence prediction objective en-\ntirely. We also compare against a BERT-Base\nmodel which we trained with binary sentence order-\ning (BERT-Base BSO) because this objective has\nbeen shown to be more useful than next sentence\nprediction (Lan et al., 2019). This BERT-Base\nBSO model was initialized with BERT weights and\ntrained on the same data but only on contiguous\nspans of text where 50% of the time we switch the\norder. This model and CONPONO are initialized\nfrom the same weights and trained on the same\nnumber of segment pairs so that the two models\ncan be compared fairly.\nIn Section 2.1 we describe different encoding\napproaches for generating the sentence-level repre-\nsentations. We report results from versions ofCON-\nPONO using each of these encoding approaches,\nlabeled isolated to represent separate encoding and\nuni-encoder to represent joint encoding of the an-\nchor and target without a separate anchor encoding.\nThe ﬁnal line in Table 1 is the combined approach\nthat we describe in Section 2.\nModeling DiscoEvalWe reuse the code from\nDiscoEval and generally maintain the same pro-\ncess for collecting our results on the benchmark,\nsuch as freezing all weights and only training a\nlogistic regression or one layer perceptron on top\nof the sentence encodings. Note that since we are\nonly interested in the vector representations of the\ninput, we drop the weight matrix Wk and only use\nthe output of the encoder. We omit the details for\nModel SP BSO DC SSP PDTB-E PDTB-I RST-DT avg.\nBERT-Base 53.1 68.5 58.9 80.3 41.9 42.4 58.8 57.7\nBERT-Large 53.8 69.3 59.6 80.4 44.3 43.6 59.1 58.6\nRoBERTa-Base 38.7 58.7 58.4 79.7 39.4 40.6 44.1 51.4\nBERT-Base BSO 53.7 72.0 71.9 80.0 42.7 40.5 63.8 60.6\nCONPONO isolated 50.2 57.9 63.2 79.9 35.8 39.6 48.7 53.6\nCONPONO uni-encoder 59.9 74.6 72.0 79.6 40.0 43.9 61.9 61.7\nCONPONO (k=2) 60.7 76.8 72.9 80.4 42.9 44.9 63.1 63.0\nCONPONO std. ±.3 ±.1 ±.3 ±.1 ±.7 ±.6 ±.2 -\nTable 1: C ONPONO improves the previous state-of-the-art on four DiscoEval tasks. The average accuracy across\nall tasks is also a new state-of-the-art, despite a small drop in accuracy for PDTB-E. BERT-Base and BERT-Large\nnumbers are reported from Chen et al. (2019), while the rest were collected for this paper. We report standard\ndeviations by running the evaluations 10 times with different seeds for the same CONPONO model weights.\nthe encoding logic for each task since that is ex-\nplained in detail in Chen et al. (2019). Here we\nonly mention our deviations from the Chen et al.\n(2019) methodology. The most salient difference\nis that we only use the pooled representation from\nour model rather than the average from multiple\nlayers of the model for the SP, BSO and DC tasks.\nFor encoding individual tasks we prefer to en-\ncode pairs of sentences together. For SP we encode\nthe ﬁrst sentence concatenated with every other sen-\ntence instead of taking the point-wise difference\nand concatenate the 5 vectors. For BSO we also\nencode the two sentences together instead of sep-\narately. For DC we split the paragraph into pairs\nof sentences and encode those together. We con-\ncatenate the 3 output vectors. For RST instead of\nembedding each sentence and doing a mean of all\nthe sentences in a subtree, we simply concatenate\nthose sentences and encode them all together as a\nsingle text span. Any text segments longer than\n512 tokens are truncated from the end.\nResults: Table 1 shows that our model outper-\nforms the previous state-of-the-art accuracy on\nDiscoEval overall. Our model excels in particu-\nlar on the sentence ordering and coherence tasks\n(SP, BSO, and DC). Note that our model parame-\nter count is the same as BERT-Base but it outper-\nforms BERT-Large, which has signiﬁcantly more\nparameters and has used much more compute for\npretraining. From the discussion in Section 2.2,\nBERT represents using the NSP objective and we\ntrain BERT-Base BSO to compare NSP, BSO and\nCONPONO directly. BERT-Base BSO scores tend\nto fall between those of BERT-Base and our model,\nimplying that the sentence ordering objective is\nimproving the models for this benchmark, but that\nbinary sentence ordering is not sufﬁcient to capture\nthe added beneﬁts of including more ﬁne-grained\nordering and negative examples.\nWe observe that CONPONO outperforms both\nthe isolated encodingand uni-encoding approaches.\nCONPONO isolated preforms signiﬁcantly worse\nthan both other approaches, suggesting that cross-\nattention between the anchor and the target is criti-\ncal to learning stronger discourse representations.\nCONPONO uni-encoder results are closer to our\ncombined encoding approach but still fall short on\nevery task. This empirical result suggests that the\nseparate encoding of the anchor during pretrain-\ning is important despite the fact that theoretically\nCONPONO could trivially reduce to the uni-coder\nrepresentation by ignoring ci.\n3.2 RTE, COPA and ReCoRD\nTasks: DiscoEval was speciﬁcally designed to eval-\nuate model performance on discourse tasks but\nthere are many other benchmarks that could also\nbeneﬁt from pretraining for improved discourse co-\nherence. We evaluate our model on three such tasks,\nRecognizing Textual Entailment (RTE) (Giampic-\ncolo et al., 2007; Bentivogli et al., 2009), Corpus of\nPlausible Alternatives (COPA) (Roemmele et al.,\n2011) and Reading Comprehension with Common-\nsense Reasoning Dataset (ReCoRD) (Zhang et al.,\n2018). We report accuracy on the validation set\nprovided by each dataset.\nEach example in RTE is a pair of sentences. The\nmodel must classify whether or not the second sen-\ntence entails the ﬁrst. Examples in COPA are com-\nposed of a single context sentence followed by two\ncandidate sentences that are either a cause or effect\nof the context sentence. The model must select the\nContext Completions\nReCoRD\n... Despite its buzz, the odds are stacked against Google’s\nChrome OSbecoming a serious rival to Windows... Chrome\nOS must face the same challenges as Linux: compatibility\nand unfamiliarity. A big stumbling block for Google will be\nwhether its system supports iTunes.\nGoogle will also be under pressure to ensure\n[Chrome OS/ iTunes / Linux] works ﬂawlessly with\ngadgets such as cameras, printers, smartphones and e-book\nreaders.\nRTE\nRabies virus infects the central nervous system, causing\nencephalopathy and ultimately death. Early symptoms of ra-\nbies in humans are nonspeciﬁc, consisting of fever, headache,\nand general malaise.\nRabies is fatal in humans.\nCOPA\nThe women met for coffee. They wanted to catch up with each other.\nThe cafe reopened in a new location.\nTable 2: These are examples from ReCoRD, RTE, and COPA that exhibit aspects of discourse coherence. For\nReCoRD, candidate entities are in italics and replaced terms in the completion are underlined. True completions\nare bold.\nmost “plausible” sentence of the two. Lastly, an\nexample in ReCoRD is a paragraph from a news\narticle, followed by several bullet points and with\nall the entities marked. The model is given a single\nsentence from later in the document with a single\nentity masked out and must select the entity from\nthe context that ﬁlls the blank. Table 2 shows ex-\namples of each with correct choices in bold.\nBaselines: We compare our model against\nBERT-Base because this is the closest model in\nterms of parameter size and training data. How-\never, since our model is initialized with BERT-Base\nweights, we also report results from BERT-Base\nBSO because it was trained on the same number\nof text examples as CONPONO . We also compare\nagainst BERT-Large to contrast to a much larger\nlanguage model. We provide results from Albert\n(Lan et al., 2019) when available to provide a state-\nof-the-art baseline that may have used more data,\ncompute and parameters. The purpose of these\nresults is not to compare against the current state-\nof-the-art but rather to better understand the im-\nprovements that can be found from adding a dis-\ncourse coherence objective to BERT-Base without\nsigniﬁcantly increasing the model size or training\ndata.\nResults: We believe that the coherence and or-\ndering aspects of these evaluation tasks are well\nﬁt to demonstrate the how our model can improve\non strong baselines such as BERT-Base. Table 3\nshows that our model achieves accuracies on RTE\nand COPA comparable to BERT-Large while hav-\ning the same number of parameters as BERT-Base.\nInterestingly, we observe improvements over the\nbaseline with BERT-Base BSO, showing that even\nModel RTE COPA\nBERT-Base 66.4 62.0\nBERT-Base BSO 71.1 67.0\nCONPONO 70.0 69.0\nBERT-Large 70.4 69.0\nALBERT 86.6 -\nTable 3: Our model improves accuracy over BERT-\nBase for RTE and COPA benchmarks. Improvements\nare comparable to BERT-Large but still lag behind\nmuch larger models trained on more data, such as AL-\nBERT. All scores are on the validation set.\nsimple discourse-level objectives could lead to no-\nticeable downstream effects. Though these im-\nprovements are modest compared to BERT-Large,\nthey are meant to highlight that our model does not\nonly improve on results for artiﬁcial sentence order-\ning tasks, but also on aspects of benchmarks used\nto generally evaluate pretrained language models\nand language understanding.\n3.2.1 ReCoRD results and models\nModel Accuracy\nBERT-Base 61.2\nCONPONO 63.2\nBERT-Large 69.8 [EM]\nTable 4: CONPONO is more effective at classifying the\nmost plausible sentence from the extended context than\nBERT-Base. We report the BERT-Large exact match\nscore, where the model selects only the target entity\nfrom the context, for reference. All scores are on the\nvalidation set.\nThe task for the ReCoRD dataset is to select the\ncorrect entity from those that appear in the context\nto ﬁll in the blank in the target. Previous models for\nReCoRD have used a similar structure to SQuAD\n(Rajpurkar et al., 2016) where the model outputs a\nvector for each token and the model learns the best\nstart and end position of the answer span based\non the softmax over all the tokens. We, instead,\ngenerate all possible target sentences by ﬁlling the\nblank with each marked entity and discriminatively\nchoose the sentence most likely to be the true “plau-\nsible” sentence from the context. This modiﬁed\ntask evaluates how our model compares to BERT-\nBase choosing the most coherent sentence from a\nset of nearly identical sentences. In Table 4 we\nshow that CONPONO does achieve a boost over\nBERT-Base but is still well below BERT-Large ex-\nact match score on the harder task of selecting the\nentities in context. The strong results from BERT-\nLarge imply that having a better representation of\nthe text with a large model is able to subsume any\nimprovement from learning plausible contexts for\nthis task.\n3.3 Ablations\nThere are three aspects of our modeling choices that\nwarrant a deeper understanding of their importance\nto the model:\n•Window size:We ablate the 4 window sizes\n(ie. choices of k). k = 1 is effectively binary\nsentence ordering with negative samples.\n•Masked Language Model Objective:We re-\nmove the MLM objective allowing the model\nto optimize only theCONPONO objective with-\nout maintaining a good token level representa-\ntion.\n•Model size:We train a smaller model that is\nalso initialized with pretrained weights.\nTo measure the effects of each of these design de-\ncisions, we report DiscoEval scores for each model\nas well as accuracy on the CONPONO classiﬁcation\ntask on a held-out set of examples. This is to show\nhow well the model is optimized as well as how\nwell it performs on downstream tasks.\nTable 5 shows the results on DiscoEval with our\nmodel and several key ablations. We observe that\nusing a window size for our objective that is larger\nthan 1 is key to seeing downstream improvements.\nWe believe that this is due to the objective being\nharder for the model because there is more vari-\nation farther from the anchor. At the same time,\nincreasing the window size beyond 2 seems to re-\nsult in similar performance. This may be because\nlarger distances from the anchor also lead to more\nambiguity. We see this reﬂected in the held-out\nclassiﬁcation accuracy being lower for examples\nwith larger distance labels in Figure 2.\nWe also note that keeping the masked language\nmodel objective during pretraining also improves\ndownstream performance. In Figure 2 we see that\nclassiﬁcation accuracy is consistently lower with\nthe MLM objective compared to without. This\nis expected because during inference, many key\nterms may be masked out, making the task harder.\nHowever, keeping this objective during pretraining\nmaintains a good token-level representation that is\nnecessary for downstream tasks.\nLastly, we try training a smaller version of our\nmodel, with only 2 hidden layers, and a 512 inter-\nmediate size. The smaller model is able to train\nmuch faster, allowing us to train on many more\nexamples and new data. However, we are unable to\nachieve similar results despite training on 24 times\nmore examples, and including CCNews (Liu et al.,\n2019), a larger and higher quality data source.\n3.4 Qualitative Analysis\nTo glean some insight into how CONPONO repre-\nsentations may differ from BERT-Base representa-\ntions, we look at the occurrence of discourse mark-\ners in the BSO-Wikipedia task of DiscoEval. We\nchoose this task because it is a simple binary classi-\nﬁcation task that has only 2 sentences as input and\nthe domain is similar to the pre-training data. We\nlook at the usage of discourse markers identiﬁed\nby Nie et al. (2017); but, when, if, before, because,\nwhile, though, after, so, although, then, also, still.\n3\nWe extract examples from the test set on which\nCONPONO output the correct label and BERT-Base\noutput the incorrect label and visa versa. For each\nset of examples, we measure the change in the oc-\ncurrence of discourse markers relative to the train-\ning data counts. Since some markers are much\nmore common than others, we take the weighted\naverage of the change in appearance rate, where the\nweights are the training data counts of each marker.\n3We omit and and as because they are very common in\nthis corpus but often are not used as connectives between the\ntwo candidate sentences for the BSO task.\nModel SP BSO DC SSP PDTB-E PDTB-I RST-DT avg.\nk=4 59.84 76.05 73.62 80.65 42.28 44.25 63.00 62.81\nk=3 60.47 76.68 72.74 80.30 43.40 44.28 62.56 62.92\nk=2 60.67 76.75 72.85 80.38 42.87 44.87 63.13 63.07\nk=1 47.56 66.03 72.62 80.15 42.79 43.55 62.31 59.29\n- MLM 54.92 75.37 68.35 80.2 41.67 43.88 61.27 60.81\nSmall 45.41 61.70 67.71 75.58 35.26 36.18 46.58 52.63\nTable 5: The ablation analysis shows the effects of differentkvalues (ie. window sizes) in our objective, removing\nthe MLM objective during pretraining and training with a small transformer encoder.\nLabel\nAccuracy\n0.00\n0.25\n0.50\n0.75\n1.00\n-4 -3 -2 -1 1 2 3 4\nSmall\nIsolated\nUni-encoder\n- MLM\nk=4\nk=3\nk=2 \nk=1\nPer Position Accuracies on Unseen Examples\nFigure 2: We can evaluate the accuracy on the C ONPONO objective for each label (ie. distance between anchor\nand target sentence) on a set of 5,000 examples held-out from training. We observe that higher accuracy does not\nnecessarily correlate with better downstream performance on DiscoEval.\nWe ﬁnd that in the set of examples that CON-\nPONO classiﬁed correctly, the rate of discourse\nmakers was 15% higher than in the training corpus.\nThis is in contrast to 11% higher among the exam-\nples that BERT classiﬁed correctly. The standard\ndeviation for random samples of the same size was\nabout 1%. This suggests that both BERT and CON-\nPONO are relying heavily on discourse markers to\nsolve the BSO-Wikipedia task.\nWhile it is expected for shallow discourse mark-\ners to be strong features for sentence ordering, we\nexpect CONPONO to also incorporate deeper fea-\ntures, such as anaphora, due to its pretraining ob-\njective. One indication of CONPONO relying on\nalternative features than BERT-Base is that there\nwas a 12% relative increase in discourse markers in\nthe CONPONO set when counting markers only in\nthe ﬁrst sentence whereas an 8% relative increase\nin the BERT set when counting markers only in\nthe second sentences. The difference in the loca-\ntion of the discourse markers in the two sets of\nexamples suggests that CONPONO and BERT uti-\nlize those features differently and that CONPONO\nmay be less likely to incorrectly classify examples\nthat use discourse markers in the ﬁrst sentence of\na BSO example. Manually inspecting a sample of\nexamples hints that there are often strong corefer-\nences between the two input sentences that indicate\nthe ordering.\nTable 6 shows two examples from theCONPONO\ncorrect set which is drawn from the BSO-Wikipedia\ntest data. In both examples, the discourse marker\nappears in the ﬁrst sentence but the second sentence\ncontains anaphora referring to an antecedent in the\nﬁrst sentence.\n4 Related Work\nSome of the largest improvements on benchmarks\nsuch as GLUE (Wang et al., 2018) have come from\nELMO’s large scale bi-directional language model-\ning (Peters et al., 2018), BERT’s masked language\nmodels (Devlin et al., 2019), XLNET’s general-\nized autoregressive pretraining (Yang et al., 2019),\nRoBERTa’s robust training (Liu et al., 2019) and\nALBERT’s parameter reduction techniques (Lan\net al., 2019). As discussed in Section 2.2, most\nlanguage model were limited to NSP or BSO for\ninter-sentence representation learning. We showed\nthat by comparing to BERT, which uses NSP and\nBERT-Base BSO which we train with the BSO\nobjective that our objective is able to improve the\ndiscourse-level representations by training on more\nﬁne-grained sentence ordering, non-contiguous\nIn 1941 [1]Vaughn joined the United States National Guard for what had been planned as a one-year assignment , but when\n[2]World War IIbroke out , he was sent abroad until the war ended in 1945 .\n[1]He decided to make music a career when he was discharged from the army at the end of[2]the war , and attended Western\nKentucky State College , now known as Western Kentucky University , majoring in music composition .\nAlthough it lasted only twenty-three years ( 19331956 ) and enrolled fewer than 1,200 students , Black Mountain College\nwas one of the most fabled experimental institutions in art education and practice .\nIt launched a remarkable number of the artists who spearheaded the avant-garde in the America of the 1960s .\nTable 6: Two examples from the DiscoEval BSO-Wikipedia test set on which CONPONO made the correct predic-\ntion but BERT-base did not.Bold terms are discourse markers, underlinedterms are co-referents. In both examples,\nthe discourse marker appears in the ﬁrst sentence but the second sentence has anaphora referring to an antecedent\nin the ﬁrst sentence.\nneighboring sentences and contrasting against ran-\ndom negatives.\nEarly approaches to sentence representation,\nsuch as Skip-Thought Vectors (Kiros et al., 2015),\nmimicked word embedding methods in addition to\nleft-to-right language modeling to use unlabeled\ndata to learn sentence level representations. Dis-\nSent (Nie et al., 2019) focused more on collecting\ndata that could be used to train a supervised clas-\nsiﬁcation model on pairs of sentences. These and\nother innovations in sentence representation lead\nto the creation of more evaluations for discourse\nand coherence representation (Chen et al., 2019;\nXu et al., 2019).\nLike other unsupervised representation learning\nmodels, CONPONO is trained to generate a latent\nvariable that encodes inter-sentence relationship\nand discourse coherence. Our objective is inspired\nby the Contrastive Predictive Coding (CPC) objec-\ntive (van den Oord et al., 2018). CPC was orig-\ninally designed to be a “universal unsupervised\nlearning approach to extract useful representations\nfrom high-dimensional data” and was previously\nimplemented on the token-level for text models.\nWe utilize the k-distance predictions of CPC be-\ncause it naturally captures discourse and sentence\nordering properties when applied on the sentence-\nlevel. Furthermore, by combining our objective\nwith a transformer encoder, our model is able to\nbeneﬁt from cross-attention between the anchor\nand the target sentences, which we show outper-\nforms encoding the anchor and target separately, as\nimplemented in CPC. In Section 3.3 we show that\nthe cross-attention is an important factor in learn-\ning a good representation for downstream tasks and\neffectively optimizing our inter-sentence objective.\n5 Discussion\nIn this paper we present a novel approach to encod-\ning discourse and ﬁne-grained sentence ordering in\ntext with an inter-sentence objective. We achieve a\nnew state-of-the-art on the DiscoEval benchmark\nand outperform BERT-Large with a model that has\nthe same number of parameters as BERT-Base. We\nalso observe that, on DiscoEval, our model beneﬁts\nthe most on ordering tasks rather than discourse re-\nlation classiﬁcation tasks. In future work, we hope\nto better understand how a discourse model can\nalso learn ﬁne-grained relationship types between\nsentences from unlabeled data. Our ablation analy-\nsis shows that the key architectural aspects of our\nmodel are cross attention, an auxiliary MLM objec-\ntive and a window size that is two or greater. Future\nwork should explore the extent to which our model\ncould further beneﬁt from initializing with stronger\nmodels and what computational challenges may\narise.\nAcknowledgments\nWe wish to thank the Stanford NLP group for\ntheir feedback. We gratefully acknowledge sup-\nport of the DARPA Communicating with Comput-\ners (CwC) program under ARO prime contract no.\nW911NF15-1-0462\nReferences\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo\nGiampiccolo, and Bernardo Magnini. 2009. The\nﬁfth PASCAL recognizing textual entailment chal-\nlenge.\nLynn Carlson, Daniel Marcu, and Mary Ellen\nOkurovsky. 2001. Building a discourse-tagged cor-\npus in the framework of rhetorical structure theory.\nIn Proceedings of the Second SIGdial Workshop on\nDiscourse and Dialogue.\nMingda Chen, Zewei Chu, and Kevin Gimpel. 2019.\nEvaluation benchmarks and learning criteria for\ndiscourse-aware sentence representations. In Pro-\nceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 649–\n662, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recog-\nnizing textual entailment challenge. In Proceedings\nof the ACL-PASCAL workshop on textual entailment\nand paraphrasing, pages 1–9. Association for Com-\nputational Linguistics.\nRyan Kiros, Yukun Zhu, Russ R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nAdvances in neural information processing systems,\npages 3294–3302.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nAllen Nie, Erin Bennett, and Noah Goodman. 2019.\nDisSent: Learning sentence representations from ex-\nplicit discourse relations. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4497–4510, Florence,\nItaly. Association for Computational Linguistics.\nAllen Nie, Erin D Bennett, and Noah D Goodman.\n2017. Dissent: Sentence representation learning\nfrom explicit discourse relations. arXiv preprint\narXiv:1710.04334.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation learning with contrastive pre-\ndictive coding. arXiv preprint arXiv:1807.03748.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nRashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-\nsakaki, Livio Robaldo, Aravind Joshi, and Bonnie\nWebber. 2008. The Penn discourse TreeBank 2.0.\nIn LREC 2008.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S. Gordon. 2011. Choice of plausible alterna-\ntives: An evaluation of commonsense causal reason-\ning. In 2011 AAAI Spring Symposium Series.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nPeng Xu, Hamidreza Saghir, Jin Sung Kang, Teng\nLong, Avishek Joey Bose, Yanshuai Cao, and Jackie\nChi Kit Cheung. 2019. A cross-domain transfer-\nable neural coherence model. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 678–687, Florence, Italy.\nAssociation for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nReCoRD: Bridging the gap between human and ma-\nchine commonsense reading comprehension. arXiv\npreprint 1810.12885.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 19–\n27.\nA Appendix\nWe include some ﬁne-grained DiscoEval results\nthat were reported as averages, as well as imple-\nmentation and reproduction details for our experi-\nments.\nA.1 SP, BSO and DC breakdown\nTable 7 shows the scores for each model per each\ndataset domain for the SP, BSO and DC tasks in\nDiscoEval.\nA.2 CONPONO pretraining details\nCONPONO is pretrained on 1.6 million examples\nrandomly sampled from Wikipedia and BooksCor-\npus. We use the same number of training examples\nfor all the ablations and training BERT-Base BSO.\nOn example consists of a single anchor and 32 can-\ndidate targets, 4 losses (1 for each of the 4 randomly\nchosen true targets (ie. k)). We use a 25% warm up\nrate and a learning rate of 5e-5. The model is ini-\ntialized with BERT-Base weights. We add a square\ninteraction weight matrix that is the same size as\nmodel output dimensions (ie. 756) that is referred\nto as Wk in Section 2. There is one such matrix\nfor each k. The maximum sequence length of the\ninput is 512, though do to some preprocessing con-\nstraints, the maximum input seen by the model is\n493.\nOur CONPONO small model has a hidden size\nof 128, an intermediate size 512, and has 2 hid-\nden layers. We train it on 38.4 million examples,\nincluding examples from CCNews. Samples are\ndrawn from each source proportional to the size\nof the source, meaning that about 70% of training\nexamples come from CCNews. Otherwise, we use\nall the same parameters as CONPONO .\nA.3 Parameter counts\nTable 8 shows the number of parameters in each\nmodel used.\nA.4 RTE, COPA and ReCoRD details\nRTE is trained for 3240 steps, with checkpoints\nevery 750 steps and a learning rate of 8e-6. The\nwarm-up proportion is 10% and the a maximum\nsequence length of 512\nCOPA is trained for 300 steps, with checkpoints\nevery 50 steps and a learning rate of 1e-5. The\nwarm-up proportion is 10% and the maximum se-\nquence length of 512.\nReCoRD is trained for 8 epochs over the train-\ning data with a learning rate of 2e-5, warm-up pro-\nportion of 10% and a maximum sequence length of\n512.\nModel Parameters\nBERT-Base 110M\nRoBERTa-Base 110M\nCONPONO [All Variants] 110M\nBERT-Large 335M\nTable 8\nSP BSO DC\nModel Wiki arxiv ROC Wiki arxiv ROC Wiki Ubuntu\nBERT-Large 50.7 47.3 63.4 70.4 66.8 70.8 65.1 54.2\nRoBERTa-Base 38.35 33.73 44.00 60.19 55.16 60.66 62.80 53.89\nBERT-Base BSO 49.23 50.92 60.80 74.67 68.56 72.22 88.80 56.41\nCONPONO - MLM 50.95 51.90 61.92 77.98 71.45 76.68 86.70 50.00\nCONPONO Small 44.90 41.23 50.10 65.03 58.89 61.19 78.10 57.32\nCONPONO isolated 49.33 44.60 56.53 59.16 57.48 56.94 71.60 54.71\nCONPONO uni-encoder 54.30 58.58 66.75 78.25 71.65 73.99 86.00 57.90\nk=4 54.07 58.30 67.15 79.04 72.21 76.89 88.38 58.85\nk=3 54.65 59.55 67.22 79.34 73.61 77.08 89.48 56.00\nk=2 54.83 58.77 68.40 79.24 74.16 76.84 89.22 56.41\nk=1 44.05 40.98 57.65 68.47 62.40 67.24 89.03 56.20\nTable 7: SP, BSO and DC are composed of separate datasets. We report the average in the main paper but show\nthe breakdown here.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7343928813934326
    },
    {
      "name": "Sentence",
      "score": 0.6844351887702942
    },
    {
      "name": "Natural language processing",
      "score": 0.6718417406082153
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6413133144378662
    },
    {
      "name": "Textual entailment",
      "score": 0.5758689641952515
    },
    {
      "name": "Focus (optics)",
      "score": 0.5539608001708984
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5297433137893677
    },
    {
      "name": "Representation (politics)",
      "score": 0.5287501215934753
    },
    {
      "name": "Softmax function",
      "score": 0.5260498523712158
    },
    {
      "name": "Comprehension",
      "score": 0.5155529975891113
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.500891923904419
    },
    {
      "name": "Linguistics",
      "score": 0.46481403708457947
    },
    {
      "name": "Language model",
      "score": 0.44831162691116333
    },
    {
      "name": "Word (group theory)",
      "score": 0.442277729511261
    },
    {
      "name": "Logical consequence",
      "score": 0.28743255138397217
    },
    {
      "name": "Deep learning",
      "score": 0.15445393323898315
    },
    {
      "name": "Mathematics",
      "score": 0.12555140256881714
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}