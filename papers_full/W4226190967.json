{
  "title": "Embeddings from protein language models predict conservation and variant effects",
  "url": "https://openalex.org/W4226190967",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5005408973",
      "name": "Céline Marquet",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5075726670",
      "name": "Michael Heinzinger",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5002122017",
      "name": "Tobias Olenyi",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5088531553",
      "name": "Christian Dallago",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5035722235",
      "name": "Michael Bernhofer",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5015317247",
      "name": "Kyra Erckert",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5090410765",
      "name": "Dmitrii Nechaev",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5064905883",
      "name": "Burkhard Rost",
      "affiliations": [
        "Technical University of Munich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2059145105",
    "https://openalex.org/W2980789587",
    "https://openalex.org/W2158714788",
    "https://openalex.org/W2211953232",
    "https://openalex.org/W2901527454",
    "https://openalex.org/W3176307508",
    "https://openalex.org/W2986717577",
    "https://openalex.org/W3166142427",
    "https://openalex.org/W2114886480",
    "https://openalex.org/W2130479394",
    "https://openalex.org/W3163595068",
    "https://openalex.org/W6780161852",
    "https://openalex.org/W2057271915",
    "https://openalex.org/W2898364362",
    "https://openalex.org/W2136513422",
    "https://openalex.org/W3157437194",
    "https://openalex.org/W4252474431",
    "https://openalex.org/W2987965949",
    "https://openalex.org/W2104418738",
    "https://openalex.org/W2889874867",
    "https://openalex.org/W2060588922",
    "https://openalex.org/W2170747616",
    "https://openalex.org/W2058568633",
    "https://openalex.org/W2774216375",
    "https://openalex.org/W2066001051",
    "https://openalex.org/W1683278196",
    "https://openalex.org/W2995514860",
    "https://openalex.org/W2143210482",
    "https://openalex.org/W2245592118",
    "https://openalex.org/W2160378127",
    "https://openalex.org/W2161888332",
    "https://openalex.org/W2057029228",
    "https://openalex.org/W2160995259",
    "https://openalex.org/W3122018424",
    "https://openalex.org/W3039901154",
    "https://openalex.org/W3038792485",
    "https://openalex.org/W2967474035",
    "https://openalex.org/W3161612534",
    "https://openalex.org/W3118936575",
    "https://openalex.org/W3196903168",
    "https://openalex.org/W3038248848",
    "https://openalex.org/W2537556928",
    "https://openalex.org/W2950629294",
    "https://openalex.org/W3179485843",
    "https://openalex.org/W3022687324",
    "https://openalex.org/W2582396271",
    "https://openalex.org/W2102652793",
    "https://openalex.org/W2058487877",
    "https://openalex.org/W2068113423",
    "https://openalex.org/W3042916618",
    "https://openalex.org/W3144701084",
    "https://openalex.org/W6818723395",
    "https://openalex.org/W3111174583",
    "https://openalex.org/W6962632179",
    "https://openalex.org/W2508408872",
    "https://openalex.org/W3010879523",
    "https://openalex.org/W2890223884",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W1985818354",
    "https://openalex.org/W2883004550",
    "https://openalex.org/W2076357933",
    "https://openalex.org/W2137886330",
    "https://openalex.org/W3098471978",
    "https://openalex.org/W3158518077",
    "https://openalex.org/W2953008890",
    "https://openalex.org/W2097889307",
    "https://openalex.org/W2913087274",
    "https://openalex.org/W3112376646",
    "https://openalex.org/W2153153865",
    "https://openalex.org/W2063274819",
    "https://openalex.org/W2885278423",
    "https://openalex.org/W3191896067",
    "https://openalex.org/W3010338076",
    "https://openalex.org/W2909727437",
    "https://openalex.org/W3211728297",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W3010387158",
    "https://openalex.org/W2950954328",
    "https://openalex.org/W2137736270",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2099589970",
    "https://openalex.org/W2155144535",
    "https://openalex.org/W2095318832",
    "https://openalex.org/W3158236124",
    "https://openalex.org/W2997591727",
    "https://openalex.org/W2079882489",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2109372707",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W2143238378"
  ],
  "abstract": "<title>Abstract</title> The emergence of SARS-CoV-2 variants stressed the demand for tools allowing to interpret the effect of single amino acid variants (SAVs) on protein function. While Deep Mutational Scanning (DMS) sets continue to expand our understanding of the mutational landscape of single proteins, the results continue to challenge analyses. Protein Language Models (pLMs) use the latest deep learning (DL) algorithms to leverage growing databases of protein sequences. These methods learn to predict missing or masked amino acids from the context of entire sequence regions. Here, we used pLM representations (embeddings) to predict sequence conservation and SAV effects without multiple sequence alignments (MSAs). Embeddings alone predicted residue conservation almost as accurately from single sequences as ConSeq using MSAs (two-state Matthew Correlation Coefficient – MCC - for ProtT5 embeddings of 0.596 ± 0.006 vs. 0.608 ± 0.006 for ConSeq). Inputting the conservation prediction along with BLOSUM62 substitution scores and pLM mask reconstruction probabilities into a simplistic logistic regression (LR) ensemble for Variant Effect Scoring without alignments (VESPA) predicted SAV effect magnitude without any optimization on DMS data. Comparing predictions for a standard set of 39 DMS experiments to other methods (incl. ESM-1v, DeepSequence, and GEMME) revealed our approach as competitive with the state-of-the-art (SOTA) methods using MSA input. No method outperformed all others, neither consistently nor in a statistically significant manner, independently of the performance measure applied (incl. two-state accuracy: Q2, MCC, Spearman and Pearson correlation). Lastly, we investigated binary effect predictions on DMS experiments for four human proteins. Overall, embedding-based methods have become competitive with methods relying on MSAs for SAV effect prediction at a fraction of the costs in computing/energy. Our method predicted SAV effects for the entire human proteome (~ 20k proteins) within 40 minutes on one Nvidia Quadro RTX 8000. All methods and data sets are freely available for local and online execution through bioembeddings.com, https://github.com/Rostlab/VESPA, and PredictProtein.",
  "full_text": "Page 1/35\nEmbeddings from protein language models predict\nconservation and variant effects\nCéline Marquet  (  celine.marquet@tum.de )\nTechnical University Munich: Technische Universitat Munchen https://orcid.org/0000-0002-8691-5791\nMichael Heinzinger \nTechnical University Munich: Technische Universitat Munchen\nTobias Olenyi \nTechnical University Munich: Technische Universitat Munchen\nChristian Dallago \nTechnical University Munich: Technische Universitat Munchen\nMichael Bernhofer \nTechnical University Munich: Technische Universitat Munchen\nKyra Erckert \nTechnical University Munich: Technische Universitat Munchen\nDmitrii Nechaev \nTechnical University Munich: Technische Universitat Munchen\nBurkhard Rost \nTechnical University Munich: Technische Universitat Munchen\nResearch Article\nKeywords:\nPosted Date: December 2nd, 2021\nDOI: https://doi.org/10.21203/rs.3.rs-584804/v3\nLicense:     This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nVersion of Record: A version of this preprint was published at Human Genetics on December 30th, 2021.\nSee the published version at https://doi.org/10.1007/s00439-021-02411-y.\nPage 2/35\nAbstract\nThe emergence of SARS-CoV-2 variants stressed the demand for tools allowing to interpret the effect of\nsingle amino acid variants (SAVs) on protein function. While Deep Mutational Scanning (DMS) sets\ncontinue to expand our understanding of the mutational landscape of single proteins, the results continue\nto challenge analyses. Protein Language Models (pLMs) use the latest deep learning (DL) algorithms to\nleverage growing databases of protein sequences. These methods learn to predict missing or masked\namino acids from the context of entire sequence regions. Here, we used pLM representations\n(embeddings) to predict sequence conservation and SAV effects without multiple sequence alignments\n(MSAs). Embeddings alone predicted residue conservation almost as accurately from single sequences\nas ConSeq using MSAs (two-state Matthews Correlation Coe\u0000cient – MCC - for ProtT5 embeddings of\n0.596±0.006 vs. 0.608±0.006 for ConSeq). Inputting the conservation prediction along with BLOSUM62\nsubstitution scores and pLM mask reconstruction probabilities into a simplistic logistic regression (LR)\nensemble for Variant Effect Score Prediction without Alignments (VESPA) predicted SAV effect\nmagnitude without any optimization on DMS data. Comparing predictions for a standard set of 39 DMS\nexperiments to other methods (incl. ESM-1v, DeepSequence, and GEMME) revealed our approach as\ncompetitive with the state-of-the-art (SOTA) methods using MSA input. No method outperformed all\nothers, neither consistently nor statistically signi\u0000cantly, independently of the performance measure\napplied (Spearman and Pearson correlation). Lastly, we investigated binary effect predictions on DMS\nexperiments for four human proteins. Overall, embedding-based methods have become competitive with\nmethods relying on MSAs for SAV effect prediction at a fraction of the costs in computing/energy. Our\nmethod predicted SAV effects for the entire human proteome (~20k proteins) within 40 minutes on one\nNvidia Quadro RTX 8000. All methods and data sets are freely available for local and online execution\nthrough bioembeddings.com, https://github.com/Rostlab/VESPA, and PredictProtein.\nIntroduction\nMany different resources capture SAV effects. Mutations in the Spike (S) surface protein of SARS-CoV-2\nhave widened the attention to the complex issue of protein variant effects (Korber et al. 2020; Laha et al.\n2020; Mercatelli and Giorgi 2020; O’Donoghue et al. 2020). The ability to distinguish between bene\u0000cial\n(=gain of function, GoF), deleterious (=loss of function, LoF) and neutral single amino acid variants\n(SAVs; also referred to as SAAV, missense mutations, or non-synonymous Single Nucleotide Variants:\nnsSNVs) continues to be a key challenge toward understanding how SAVs affect proteins (Adzhubei et al.\n2010; Bromberg and Rost 2007, 2009; Ng and Henikoff 2003; Studer et al. 2013; Wang and Moult 2001).\nRecently, an unprecedented amount of in vitro data describing the quantitative effects of SAVs on protein\nfunction has been produced through Multiplexed Assays of Variant Effect (MAVEs), such as deep\nmutational scans (DMS) (Fowler and Fields 2014; Weile and Roth 2018). However, a comprehensive atlas\nof in vitro variant effects for the entire human proteome still remains out of reach (AVE Alliance Founding\nMembers 2020). Yet, even for the existing experiments, intrinsic problems remain: (1) In vitro DMS data\ncaptures SAV effects upon molecular function much better than those upon biological processes, e.g.,\nPage 3/35\ndisease implications may be covered in databases such as the Online Mendelian Inheritance in Man\n(OMIM) (Amberger et al. 2019), but not in MaveDB (Esposito et al. 2019). (2) The vast majority of proteins\nhave several structural domains (Liu and Rost 2003, 2004a, b), hence most are likely to have several\ndifferent molecular functions. However, each experimental assay tends to measure the impact upon only\none of those functions. (3) In vivo protein function might be impacted in several ways not reproducible by\nin vitro assays.\nEvolutionary information from MSAs most important to predict SAV effects. Many in silico methods try to\nnarrow the gap between known sequences and unknown SAV effects; these include (by earliest\npublication date): PolyPhen/PolyPhen2 (Adzhubei et al. 2010; Ramensky et al. 2002), SIFT (Ng and\nHenikoff 2003; Sim et al. 2012), I-Mutant (Capriotti et al. 2005), SNAP/SNAP2 (Bromberg and Rost 2007;\nHecht et al. 2015), MutationTaster (Schwarz et al. 2010), Evolutionary Action (Katsonis and Lichtarge\n2014), CADD (Kircher et al. 2014), PON-P2 (Niroula et al. 2015), INPS (Fariselli et al. 2015), Envision (Gray\net al. 2018), DeepSequence (Riesselman et al. 2018), GEMME (Laine et al. 2019), ESM-1v (Meier et al.\n2021), and methods predicting rheostat positions susceptible to gradual effects (Miller et al. 2017). Of\nthese only Envision and DeepSequence trained on DMS experiments. Most others trained on sparsely\nannotated data sets such as disease-causing SAVs from OMIM (Amberger et al. 2019), or from databases\nsuch as the protein mutant database (PMD) (Kawabata et al. 1999; Nishikawa et al. 1994). While only\nsome methods use sophisticated algorithms from machine learning (ML; SVM, FNN) or even arti\u0000cial\nintelligence (AI; CNN), almost all rely on evolutionary information derived from multiple sequence\nalignments (MSAs) to predict variant effects. The combination of evolutionary information (EI) and\nML/AI has long been established as a backbone of computational biology (Rost 1996; Rost and Sander\n1992, 1993), now even allowing AlphaFold2 to predict protein structure at unprecedented levels of\naccuracy (Jumper et al. 2021). Nevertheless, for almost no other task is EI as crucial as for SAV effect\nprediction (Bromberg and Rost 2007). Although different sources of input information matter, when MSAs\nare available, they trump all other features (Hecht et al. 2015). Even models building on the simplest EI,\ne.g., the BLOSUM62 matrix condensing biophysical constraints into a 20x20 substitution matrix (Ng and\nHenikoff 2003) with no distinction between E481K (amino acid E at residue position 481 mutated to\namino acid K) and E484K (part of SARS-CoV-2 Delta variant), or a simple conservation weight (Reeb et al.\n2020) with no distinction of D484Q and D484K, almost reach the performance of much more complex\nand seemingly advanced methods.\nEmbeddings capture language of life written in proteins. Every year algorithms improve natural language\nprocessing (NLP), in particular by feeding large text corpora into Deep Learning (DL) based Language\nModels (LMs). These advances have been transferred to protein sequences by learning to predict masked\nor missing amino acids using large databases of raw protein sequences as input (Alley et al. 2019; Bepler\nand Berger 2019a, 2021; Elnaggar et al. 2021; Heinzinger et al. 2019; Madani et al. 2020; Ofer et al. 2021;\nRao et al. 2020; Rives et al. 2021). Processing the information learned by such protein LMs (pLMs), e.g.\nby constructing 1024-dimensional vectors of the last hidden layers, yields a representation of protein\nsequences referred to as embeddings (Fig. 1 in (Elnaggar et al. 2021)). Embeddings have succeeded as\nexclusive input to predicting secondary structure and subcellular location at performance levels almost\nPage 4/35\nreaching (Alley et al. 2019; Heinzinger et al. 2019; Rives et al. 2021) or even exceeding (Elnaggar et al.\n2021; Littmann et al. 2021c; Stärk et al. 2021) state-of-the-art (SOTA) methods using EI from MSAs as\ninput. Embeddings even succeed in substituting sequence similarity for homology-based annotation\ntransfer (Littmann et al. 2021a; Littmann et al. 2021b) and in predicting the effect of mutations on\nprotein-protein interactions (Zhou et al. 2020). The power of such embeddings has been increasing with\nthe advance of algorithms (Bepler and Berger 2021; Elnaggar et al. 2021; Rives et al. 2021). ESM-1v\ndemonstrated pre-trained pLMs predicting SAV effect without any supervision at state-of-the-art level on\nDMS data by using solely mask reconstruction probabilities (Meier et al. 2021). Naturally, there will be\nsome limit to such improvements. However, the advances over the last months prove that this limit had\nnot been reached by the end of 2020.\nHere, we analyzed ways of using embeddings from pre-trained pLMs to predict the effect of SAVs upon\nprotein function with a focus on molecular function, using experimental data from DMS (Esposito et al.\n2019) and PMD (Kawabata et al. 1999). The embeddings from the pre-trained pLMs were not altered or\noptimized to suit the subsequent 2nd step of supervised training on data sets with more limited\nannotations. In particular, we assessed two separate supervised prediction tasks: conservation and SAV\neffects. First, we utilized pre-trained pLMs (ProtBert, ProtT5, ESM-1b) as static feature encoders (without\n\u0000ne-tuning the pLMs) to derive input embeddings for developing a method predicting the conservation\nthat we could read off a family of aligned sequences (MSA) for each residue without actually generating\nthe MSA. Second, we trained a Logistic Regression (LR) ensemble to predict SAV effect using (2a) the\npredictions of the best conservation predictor (ProtT5cons) together with (2b) substitution scores of\nBLOSUM62 and (2c) substitution probabilities of the pLM ProtT5. While substitution probabilities alone\nalready correlated with DMS scores, we observed that adding conservation predictions together with\nBLOSUM62 increased performance. The resulting model for Variant Effect Score Prediction without\nAlignments (VESPA) was competitive with more complex solutions in terms of correlation with\nexperimental DMS scores and computational and environmental costs. Additionally, for a small drop in\nprediction performance, we created a computationally more e\u0000cient method, dubbed VESPA-light (or\nshort: VESPAl), by excluding substitution probabilities to allow proteome-wide analysis to complete after\nthe coffee break on a single machine (40 minutes for human proteome on one Nvidia Quadro RTX 8000).\nMethods\nData sets\nIn total, we used \u0000ve different datasets. ConSurf10k was used to train and evaluate a model on residue\nconservation prediction. Eff10k was used to train SAV effect prediction. PMD4k and DMS4 were used as\ntest sets to assess the prediction of binary SAV effects. The prediction of continuous effect scores was\nevaluated on DMS39.\nConSurf10k assessed conservation. The method predicting residue conservation used ConSurf-DB (Ben\nChorin et al. 2020). This resource provided sequences and conservation for 89,673 proteins. For all,\nPage 5/35\nexperimental high-resolution three-dimensional (3D) structures were available in the Protein Data Bank\n(PDB) (Berman et al. 2000). As standard-of-truth for the conservation prediction, we used the values from\nConSurf-DB generated using HMMER (Mistry et al. 2013), CD-HIT (Fu et al. 2012), and MAFFT-LINSi\n(Katoh and Standley 2013) to align proteins in the PDB (Burley et al. 2019). For proteins from families\nwith over 50 proteins in the resulting MSA, an evolutionary rate at each residue position is computed and\nused along with the MSA to reconstruct a phylogenetic tree. The ConSurf-DB conservation scores ranged\nfrom 1 (most variable) to 9 (most conserved). The PISCES server (Wang and Dunbrack 2003) was used to\nredundancy reduce the data set such that no pair of proteins had more than 25% pairwise sequence\nidentity. We removed proteins with resolutions >2.5Å, those shorter than 40 residues, and those longer\nthan 10,000 residues. The resulting data set (ConSurf10k) with 10,507 proteins (or domains) was\nrandomly partitioned into training (9,392 sequences), cross-training/validation (555) and test (519) sets.\nEff10k assessed SAV effects. This dataset was taken from the SNAP2 development set (Hecht et al.\n2015). It contained 100,737 binary SAV-effect annotations (neutral: 39,700, effect: 61,037) from 9,594\nproteins. The set was used to train an ensemble method for SAV effect prediction. For this, we replicated\nthe cross-validation (CV) splits used to develop SNAP2 by enforcing that clusters of sequence-similar\nproteins were put into the same CV split. More speci\u0000cally, we clustered all sequence-similar proteins (PSI-\nBLAST E-value<1e-3) using single-linkage clustering, i.e., all connected nodes (proteins) were put into the\nsame cluster. By placing all proteins within one cluster into the same CV split and rotating the splits such\nthat every split was used exactly once for testing, we ascertained that no pair of proteins between train\nand test shared signi\u0000cant sequence similarity (PIDE). More details on the dataset are given in SNAP2\n(Hecht et al. 2015).\nPMD4k assessed binary SAV effects. From Eff10k, we extracted annotations that were originally adopted\nfrom PMD (“no change” as “neutral”; annotations with any level of increase or decrease in function as\n“effect”). This yielded 51,817 binary annotated SAVs (neutral: 13,638, effect: 38,179) in 4,061 proteins.\nPMD4k was exclusively used for testing. While these annotations were part of Eff10k, all performance\nestimates for PMD4k were reported only for the PMD annotations in the testing subsets of the cross-\nvalidation splits. As every protein in Eff10k (and PMD4k) was used exactly once for testing, we could\nascertain that there was no signi\u0000cant (prediction by homology-based inference possible) sequence-\nsimilarity between PMD4k and our training splits.\nDMS4 sampled large-scale DMS in vitro experiments annotating binary SAV effects. This set contained\nbinary classi\u0000cations (effect/non-effect) for four human proteins (corresponding genes: BRAC1, PTEN,\nTPMT, PPARG) generated previously (Reeb 2020). These were selected as they were the \u0000rst proteins with\ncomprehensive DMS experiments including synonymous variants (needed to map from continuous effect\nscores to binary effect vs. neutral) resulting in 15,621 SAV annotations (Findlay et al. 2018; Majithia et al.\n2016; Matreyek et al. 2018). SAVs with bene\u0000cial effect (=gain of function) were excluded because they\ndisagree between experiments (Reeb et al. 2020). The continuous effect scores of the four DMS\nexperiments were mapped to binary values (effect/neutral) by considering the 95% interval around the\nmean of all experimental measurements as neutral, and the 5% tails of the distribution as “effect”, as\nPage 6/35\ndescribed in more detail elsewhere (Reeb et al. 2020). In total, the set had 11,788 neutral SAVs and 3,516\ndeleterious effect SAVs. Additionally, we used two other thresholds: the 90% interval from mean (8,926\nneutral vs. 4,545 effect) and the 99% interval from mean (13,506 neutral vs. 1,548 SAVs effect).\nDMS39 collected DMS experiments annotating continuous SAV effects. This set was used to assess\nwhether the methods introduced here, although trained only on binary effect data from Eff10k, had\ncaptured continuous effect scales as measured by DMS. The set was a subset of 43 DMS experiments\nassembled for the development of DeepSequence (Riesselman et al. 2018). From the original\ncompilation, we excluded an experiment on tRNA as it is not a protein, on the toxin-antitoxin complex as it\ncomprises multiple proteins and removed experiments for which only double variants existed. DMS39\ncontained 135,665 SAV scores, in total. The number of SAVs per experiment varied substantially between\nthe 39 with an average of 3,625 SAVs/experiment, a median of 1,962, a minimum of 21, and a maximum\nof 12,729. However, to avoid any additional biases in the comparison to other methods, we avoided any\nfurther \u0000ltering step.\nInput features\nFor the prediction of residue conservation, all newly developed methods exclusively trained on\nembeddings from pre-trained pLMs without \u0000ne-tuning those (no gradient was backpropagated to the\npLM). The predictions of the best performing method for conservation prediction were used in a second\nstep together with substitution scores from BLOSUM62 and substitution probabilities from ProtT5 as\ninput features to predict binary SAV effects.\nEmbeddings from pLMs. For conservation prediction, we used embeddings from the following pLMs:\nProtBert (Elnaggar et al. 2021) based on the NLP (Natural Language Processing) algorithm BERT (Devlin\net al. 2019) trained on Big Fantastic Database (BFD) with over 2.3 million protein sequences (Steinegger\nand Söding 2018), ESM-1b (Rives et al. 2021) that is conceptually similar to (Prot)BERT (both use a\nTransformer encoder) but trained on UniRef50 (The UniProt Consortium 2021) and ProtT5-XL-U50\n(Elnaggar et al. 2021) (for simplicity referred to as ProtT5) based on the NLP sequence-to-sequence\nmodel T5 (Transformer encoder-decoder architecture) (Raffel et al. 2020) trained on BFD and \u0000ne-tuned\non Uniref50. All embeddings were obtained from the bio_embeddings pipeline (Dallago et al. 2021). As\ndescribed in ProtTrans, only the encoder-side of ProtT5 was used and embeddings were extracted in half-\nprecision (Elnaggar et al. 2021). The per-residue embeddings were extracted from the last hidden layer of\nthe models with size 1024×L (1280 for ESM-1b), where L is the length of the protein sequence and 1024\n(or 1280 for ESM-1b) is the dimension of the hidden states/embedding space of ESM-1b, ProtBert and\nProtT5.\nContext-dependent substitution probabilities. The training objective of most pLMs is to reconstruct\ncorrupted amino acids from their non-corrupted protein sequence context. Repeating this task on billions\nof sequences allows pLMs to learn a probability of how likely it is to observe a token (an amino acid) at a\ncertain position in the protein sequence. After pre-training, those probabilities can be extracted from pLMs\nby masking/corrupting one token/amino acid at a time, letting the model reconstruct it based on non-\nPage 7/35\ncorrupted sequence context and repeating this for each token/amino acid in the sequence. For each\nprotein this gives a vector of length L by 20 with L being the protein’s length and 20 being the probability\ndistribution over the 20 standard amino acids. It was shown recently (Meier et al. 2021) that these\nprobabilities provide a context-aware estimate for the effect of SAVs, i.e., the reconstruction probabilities\ndepend on the protein sequence, and other methods have made use of similar probabilities (Hopf et al.\n2017; Riesselman et al. 2018). To generate input features for our SAV effect predictor, we used, as\nsuggested by Meier et al. (2021), the log-odds ratio between the probability of observing the wild-type\namino acid at a certain position and the probability of observing a speci\u0000c mutant at the same position: \nlog p Xi,mutant −log p Xi,wildtype . The term p Xi,mutant  described the probability of an\nSAV occurring at position i and p Xi,wildtype  described the corresponding probability of the wildtype\noccurrence (native amino acid). To extract these probabilities for SAV effect prediction, we only\nconsidered the pLM embeddings correlating best with conservation (ProtT5). Additionally, we extracted\nprobabilities for ProtBert on ConSurf10k to analyze in more detail the mistakes that ProtBert makes\nduring reconstruction (SOM Fig. S5, S6).\nContext-independent BLOSUM62 substitution scores. The BLOSUM substitution matrix gives a log-odds\nratio for observing an amino acid substitution irrespective of its position in the protein (Henikoff and\nHenikoff 1992), i.e., the substitution score will not depend on a speci\u0000c protein or the position of a residue\nwithin a protein but rather focuses on bio-chemical and bio-physical properties of amino acids.\nSubstitution scores in BLOSUM were derived from comparing the log-odds of amino acid substitutions\namong well conserved protein families. Typically applied to align proteins, BLOSUM scores are also\npredictive of SAV effects (Ng and Henikoff 2003; Sruthi et al. 2020).\nMethod development\nIn our three-stage development, we \u0000rst compared different combinations of network architectures and\npLM embeddings to predict residue conservation. Next, we combined the best conservation prediction\nmethod with BLOSUM62 substitution scores to develop a simple rule-based prediction of binary SAV\neffects. In the third step, we combined the predicted conservation, BLOSUM62, and substitution\nprobabilities to train a new method predicting SAV effects for binary data from Eff10k and applied this\nmethod to non-binary DMS data.\nConservation prediction ( ProtT5cons, Fig. 1A). Using either ESM-1b, ProtBert, or ProtT5 embeddings as\ninput (Fig. 1a), we trained three supervised classi\u0000ers to distinguish between nine conservation classes\ntaken from ConSurf-DB (early stop when optimum reached for ConSurf10k validation set). The objective\nof this task was to learn the prediction of family conservation from ConSurf-DB (Ben Chorin et al. 2020)\nbased on the nine conservation classes introduced by that method that range from 1 (variable) to 9\n(conserved) for each residue in a protein, i.e., this task implied a multi-class per-residue prediction. Cross-\nentropy loss together with Adam (Kingma and Ba 2014) was used to optimize each network towards\npredicting one out of nine conservation classes for each residue in a protein (per-token/per-residue task).\n(( )) (( )) ( )\n( )\nPage 8/35\nThe models were: (1) standard Logistic Regression (LR) with 9,000 (9k) free parameters; (2) feedforward\nneural network (FNN; with two FNN layers connected through so called ReLU (recti\u0000ed linear unit)\nactivations (Fukushima 1969); dropout rate 0.25; 33k free parameters); (3) standard convolutional neural\nnetwork (CNN; with two convolutional layers with a window-size of 7, connected through ReLU\nactivations; dropout rate of 0.25; 231k free parameters). To put the number of free parameters into\nperspective: the ConSurf10k data set contained about 2.7 million samples, i.e., an order of magnitude\nmore samples than free parameters of the largest model. On top of the 9-class prediction, we\nimplemented a binary classi\u0000er (conserved /non-conserved; threshold for projecting nine to two classes\noptimized on validation set). The best performing model (CNN trained on ProtT5) was referred to as\nProtT5cons.\nRule-based binary SAV effect prediction ( ProtT5beff, Fig. 1B). For rule-based binary SAV effect\n(effect/neutral) prediction, we considered multiple approaches. The \u0000rst and simplest approach was to\nintroduce a threshold to the output of ProtT5cons (no optimization on SAV data). Here, we marked all\nresidues predicted to be conserved (conservation score>5) as “effect”, all others as “neutral”. This \u0000rst\nlevel treated all 19 non-native SAVs at one sequence position equally (referred to as “19equal” in tables\nand \u0000gures). To re\u0000ne, we followed the lead of SIFT (Ng and Henikoff 2003) using the BLOSUM62\n(Henikoff and Henikoff 1992) substitution scores. This led to the second rule-based method dubbed\nBLOSUM62bin which can be considered a naïve baseline: SAVs less likely than expected (negative values\nin BLOSUM62) were classi\u0000ed as “effect”, all others as “neutral”. Next, we combined both rule-based\nclassi\u0000ers to the third rule-based method, dubbed ProtT5beff (“effect” if ProtT5cons predicts conserved,\ni.e., value >5, and BLOSUM62 negative, otherwise “neutral”, Fig. 1b). This method predicted binary\nclassi\u0000cations (effect/neutral) of SAVs without using any experimental data on SAV effects for\noptimization by merging position-aware information from ProtT5cons and variant-aware information\nfrom BLOSUM62.\nSupervised prediction of SAV effect scores ( VESPA and VESPAl). For variant effect score prediction\nwithout alignments (VESPA), we trained a balanced logistic regression (LR) ensemble method as\nimplemented in SciKit (Pedregosa et al. 2011) on the cross-validation splits of Eff10k. We rotated the ten\nsplits of Eff10k such that each data split was used exactly once for testing while all remaining splits were\nused for training. This resulted in ten individual LRs trained on separate datasets. All of those were forced\nto share the same hyper-parameters. The hyper-parameters that differed from SciKit’s defaults were: (1)\nbalanced weights: class weights were inversely proportional to class frequency in input data, (2)\nmaximum number of iterations taken for the solvers to converge was set to 600. The learning objective of\neach was to predict the probability of binary class membership (effect/neutral). By averaging their output,\nwe combined the ten LRs to an ensemble method: VESPA=ensembleofLRs= 1\n10∑10i=1LRi. The\noutput of VESPA is bound to [0,1] and by introducing a threshold can be readily interpreted as a\nprobability for an SAV to be “neutral” (VESPA<0.5) or to have “effect” (VESPA ≥ 0.5). As input for VESPA,\nwe used eleven features to derive one score for each SAV; nine were the position-speci\u0000c conservation\nprobabilities predicted by ProtT5cons; one was the variant-speci\u0000c substitution score from BLOSUM62,\nPage 9/35\nthe other the variant- and position-speci\u0000c log-odds ratio of ProtT5’s substitution probabilities. In order to\nreduce the computational costs of VESPA, we introduced the “light” version VESPAl using only\nconservation probabilities and BLOSUM62 as input and thereby circumventing the computationally more\ncostly extraction of the log-odds ratio. Both, VESPA and VESPAl were only optimized on binary effect data\nfrom Eff10k and never encountered continuous effect scores from DMS experiments during any\noptimization.\nEvaluation\nConservation prediction - ProtT5cons. To put the performance of ProtT5cons into perspective, we\ngenerated ConSeq (Berezin et al. 2004) estimates for conservation through PredictProtein (Bernhofer et\nal. 2021) using MMseqs2 (Steinegger and Söding 2018) and PSI-BLAST (Altschul et al. 1997) to generate\nMSAs. These were “estimates” as opposed to the standard-of-truth from ConSurf-DB because, although\nthey actually generated entire MSAs, the method for MSA generation was “just” MMseqs2 as opposed to\nHMMER (Mistry et al. 2013), and MAFFT-LINSi (Katoh and Standley 2013) for ConSurf-DB and the\ncomputation of weights from the MSA also required less computing resources. A random baseline\nresulted from randomly shu\u0000ing ConSurf-DB values.\nBinary effect prediction - ProtT5beff. To analyze the performance of VESPA and VESPAl, we compared\nresults to SNAP2 (Hecht et al. 2015) at the default binary threshold (score>-0.05, default value suggested\nin original publication) on PMD4k and DMS4. Furthermore, we evaluated the rule-based binary SAV effect\nprediction ProtT5beff on the same datasets. To assess to which extent performance of ProtT5beff could\nbe attributed to mistakes in ProtT5cons, we replaced residue conservation from ProtT5cons with\nconservation scores from ConSeq and applied the same two rule-based approaches as explained above\n(ConSeq 19equal: conserved predictions at one sequence position were considered “effect” for all 19 non-\nnative SAVs and ConSeq blosum62: only negative BLOSUM62 scores at residues predicted as conserved\nwere considered “effect”; all others considered “neutral” with both using the same threshold in\nconservation as for our method, i.e. conservation >5 for effect) for PMD4k and DMS4. This failed for 122\nproteins on PMD4k (3% of PMD4k) because the MSAs were deemed too small. We also compared\nProtT5beff to the baseline based only on BLOSUM62 with the same thresholds as above\n(BLOSUM62bin). Furthermore, we compared to SNAP2 at default binary threshold of effect: SNAP2\nscore>-0.05 (default value suggested in original publication). SNAP2 failed for four of the PMD4k\nproteins (0.1% of PMD4k). For the random baseline, we randomly shu\u0000ed ground truth values for each\nPMD4k and DMS4.\nContinuous effect prediction – VESPA. We evaluated the performance of VESPA and VESPAl on DMS39\ncomparing to MSA-based DeepSequence (Riesselman et al. 2018) and GEMME (Laine et al. 2019), and\nthe pLM-based ESM-1v (Meier et al. 2021). Furthermore, we evaluated log-odds ratios from ProtT5’s\nsubstitution probabilities and BLOSUM62 substitution scores as a baseline. The DeepSequence\npredictions were copied from the supplement to the original publication (Riesselman et al. 2018), GEMME\ncorrelation coe\u0000cients were provided by the authors, and ESM-1v predictions were replicated using the\nonline repository of ESM-1v. We used the publicly available ESM-1v scripts to retrieve “masked-\nPage 10/35\nmarginals” for each of the \u0000ve ESM-1v models and averaged over their outputs because this strategy\ngave best performance according to the authors. If a protein was longer than 1022 (the maximum\nsequence length that ESM-1v can process), we split the sequence into non-overlapping chunks of length\n1022. VESPA, VESPAl, and ESM-1v predictions did not use MSAs and therefore provided results for the\nentire input sequences, while DeepSequence and GEMME were limited to residues to which enough other\nprotein residues were aligned in the MSAs.\nPerformance measures. We applied the following standard performance measures.\nQ2=100∙ Numberofresiduespredictedcorrectlyin2states\nNumberofallresidues  (Eqn. 1)\nQ2 scores (Eqn. 1) described both binary predictions (conservation and SAV effect). The same held for\nF1-scores (Eqn. 6, 7) and MCC (Matthews Correlation Coe\u0000cient, Eqn. 8). We de\u0000ned conserved/effect as\nthe positive class and non-conserved/neutral as the negative class (indices “+” for positive, “-“ for\nnegative) and used the standard abbreviations of TP (true positives: number of residues predicted and\nobserved as conserved/effect), TN (true negatives: predicted and observed as non-conserved/neutral), FP\n(false positives: predicted conserved/effect, observed non-conserved/neutral), FN (false negatives:\npredicted non-conserved/neutral, observed conserved/effect).\nAccuracy+ =Precision+ =PositivePredictedValue= TP\nTP+FP (Eqn. 2)\nAccuracy− =Precision− =NegativePredictedValue= TN\nTN+FN (Eqn. 3)\nCoverage+ =Recall+ =Sensitivity= TP\nTP+FN (Eqn. 4)\nCoverage_=Recall− =Specificity= TN\nTN+FP (Eqn. 5)\nF1+ =100∙2∙\nPrecision+∙Recall+\nPrecision++Recall+\n (Eqn. 6)\nF1− =100∙2∙\nPrecision−∙Recall−\nPrecision−+Recall−\n (Eqn. 7)\nMCC= TP∙TN−FP∙FN\n√(TP+FP)∙(TP+FN)∙(TN+FP)∙(TN+FN)  (Eqn. 8)\nQ9=100∙ Numberofresiduespredictedcorrectlyin9states\nNumberofallresidues  (Eqn. 9)\nQ9 is exclusively used to measure performance for the prediction of nine classes of conservation taken\nfrom ConSurf-DB. Furthermore, we considered the Pearson correlation coe\u0000cient:\nrP =ρX,Y= cov(X,Y)\nσXσY  (Eqn. 10)\nPage 11/35\nand the Spearman correlation coe\u0000cient where raw scores (X, Y of Eqn. 10) are converted to ranks:\nrS=ρrgX,rgY =\ncov(rgX,rgY)\nσXrgXσrgY\n (Eqn. 11)\nfor continuous effect prediction.\nError estimates. We estimated symmetric 95% con\u0000dence intervals (CI, Eqn. 12) for all metrics using\nbootstrapping (Efron et al. 1996) by computing 1.96* standard deviation (SD) of randomly selected\nvariants from all test sets with replacement over n = 1000 bootstrap sets:\nCI=1.96∙SD=1.96∙\n∑(yi−\n−\ny)\n2\nn  (Eqn. 12)\nwith yi being the metric for each bootstrap sample and \n−\ny the mean over all bootstrap samples. We\nconsidered differences in performance signi\u0000cant if two CIs did not overlap.\nProbability entropy. To investigate the correlation between embeddings and conservation classes of\nConSurf-DB, we computed the entropy of pLM substitution probabilities (p) as:\nEntropy(p1,…,pn)= −∑ni=1pilog2pi (Eqn. 13)\nResults\nWe \u0000rst showed that probabilities derived from pLMs su\u0000ced for the prediction of residue conservation\nfrom pLM embeddings without using MSAs (data set ConSurf10k; method ProtT5cons). Next, we\npresented a non-parametric rule-based SAV effect prediction based on predicted conservation (IF\n“predicted conserved” THEN “predict effect”; method ProtT5beff). We re\u0000ned the rule-based system\nthrough logistic regression (LR) to predict SAV effect on variants labeled with “effect” or “neutral” (data\nset Eff10k; methods VESPA, VESPAl). Finally, we established that these new methods trained on binary\ndata (effect/neutral) from Eff10k correlated with continuous DMS experiments.\nEmbeddings predicted conservation. First, we established that protein Language Models (pLMs) capture\ninformation correlated with residue conservation without ever seeing any such labels. As a standard of\ntruth, we extracted the categorical conservation scores ranging from 1 to 9 (9: highly conserved, 1: highly\nvariable) from ConSurf-DB (Ben Chorin et al. 2020) for a non-redundant subset of proteins with\nexperimentally known structures (data set ConSurf10k). Those conservation scores correlated with the\nmask reconstruction probabilities output by ProtBert (Fig. 2). More speci\u0000cally, one amino acid was\ncorrupted at a time and ProtBert reconstructed it from non-corrupted sequence context. For instance,\nwhen corrupting and reconstructing all residues in ConSurf10k (one residue at a time), ProtBert assigned\na probability to the native and to each of the 19 non-native (SAVs) amino acids for each position in the\n√\nPage 12/35\nprotein. Using those “substitution probabilities”, ProtBert correctly predicted the native amino acid in\n45.3% of all cases compared to 9.4% for a random prediction of the most frequent amino acid (Fig. S4).\nThe entropy of these probability distributions correlated slightly with conservation (Fig. 2, Spearman’s\nR=-0.374) although never trained on such labels.\nNext, we established that residue conservation can be predicted directly from embeddings by training a\nsupervised network on data from ConSurf-DB. We exclusively used embeddings of pre-trained pLMs\n(ProtT5, ProtBert (Elnaggar et al. 2021), ESM-1b (Rives et al. 2021)), as input to relatively simple machine\nlearning models (Fig. 1). Even the simplistic logistic regression (LR) reached levels of performance within\nabout 20% of ConSeq (Berezin et al. 2004) conservation scores, which were derived from MSAs generated\nby the fast alignment method MMseqs2 (Steinegger and Söding 2017) (Fig. 3). The top prediction used\nProtT5 embeddings which consistently outperformed predictions from ESM-1b and ProtBERT\nembeddings. For all three types of embeddings, the CNN outperformed the FNN, and these outperformed\nthe LR. Differences between ProtBert and ProtT5 were statistically signi\u0000cant (at the 95% con\u0000dence\ninterval, Eqn. 12) while improvements from ProtT5 over ESM-1b were mostly insigni\u0000cant. The ranking of\nthe embeddings and models remained stable across several performance measures (F1effect, F1neutral,\nMCC, Pearson correlation coe\u0000cient, Table S1).\nConSurf-DB (Ben Chorin et al. 2020) simpli\u0000es family conservation to a single digit integer (9: highly\nconserved, 1: highly variable). We further reduced these classes to a binary classi\u0000cation (conserved/non-\nconserved) to later transfer information from conservation to binary SAV effect (effect/neutral) more\nreadily. The optimal threshold for a binary conservation prediction was 5 (>5 conserved, Fig. S1).\nHowever, performance was stable across a wide range of choices: between values from 4 to 7, MCC (Eqn.\n8) changed between 0.60 and 0.58, i.e., performance varied by 3.3% for 44.4% of all possible thresholds\n(Fig. S1). This was explained by the nine- and two-class confusion matrices (Fig. S2 and S3) for\nProtT5cons, which showed that most mistakes were made between neighboring classes of similar\nconservation, or between the least conserved classes 1 and 2.\nConservation-based prediction of binary SAV effect better for DMS4 than for PMD4k? Next, we\nestablished that we could use the predicted conservation of ProtT5cons for rule-based binary SAV effect\nprediction without any further optimization and without any MSA. In using predicted conservation to\nproxy SAV effect, we chose the method best in conservation prediction, namely the CNN using ProtT5\nembeddings (method dubbed ProtT5cons, Fig. 1B). The over-simplistic approach of considering any\nresidue predicted as conserved to have an effect irrespective of the SAV (meaning: treat all 19 non-native\nSAVs alike) was referred to as “19equal”. We re\u0000ned this rule-based approach by combining conservation\nprediction with a binary BLOSUM62 score (effect: if ProtT5cons predicted conserved AND BLOSUM62<0,\nneutral otherwise), which we referred to as ProtT5beff. For PMD4k, the following results were common to\nall measures re\u0000ecting aspects of precision and recall through a single number (F1effect, F1neutral and\nMCC). First, the expert method SNAP2 trained on Eff10k (superset of PMD4k) achieved numerically\nhigher values than all rule-based methods introduced here. Second, using the same SAV effect prediction\nfor all 19 non-native SAVs consistently reached higher values than using the BLOSUM62 values (Fig. 4\nPage 13/35\nand Table 1: 19equal higher than blosum62). For some measures (Q2, F1effect) values obtained by using\nConSeq for conservation (i.e., a method using MSAs) were higher than those for the ProtT5cons\nprediction (without using MSAs), while for others (MCC, F1neutral) this was reversed (Fig. 4, Table 1, Table\nS2).\nTable 1\nPerformance in binary SAV effect prediction *\nData set PMD4k DMS4\nMethod/Metric Q2\n(Eqn. 1)\nMCC\n(Eqn. 8)\nQ2\n(Eqn. 1)\nMCC\n(Eqn. 8)\nRandom 61.08% ± 0.41 -0.002 ± 0.016 64.27% ± 0.76 -0.001 ± 0.018\nSupervised methods        \nSNAP2bin 70.66% ± 0.39 0.280 ± 0.010 41.55% ± 0.82 0.204 ± 0.012\nVESPA 63.52% ± 0.43 0.274 ± 0.086 63.56% ± 0.79 0.346 ± 0.014\nVESPAl 63.04% ± 0.43 0.271 ± 0.085 72.59% ± 0.72 0.405 ± 0.016\nRule-based methods        \nBLOSUM62bin 56.17% ± 0.43 0.049 ± 0.010 44.47% ± 0.84 0.169 ± 0.014\nProtT5cons-19equal 68.58% ± 0.41 0.227 ± 0.010 62.20% ± 0.82 0.322 ± 0.014\nProtT5-beff 52.26% ± 0.43 0.160 ± 0.016 71.47% ± 0.75 0.369 ± 0.016\nConSeq-19equal 71.51% ± 0.39 0.206 ± 0.010 50.70% ± 0.84 0.267 ± 0.012\nConSeq blosum62 54.32% ± 0.43 0.138 ± 0.016 63.81% ± 0.8 0.318 ± 0.014\n* Data sets: The PMD4k data set contained 4k proteins from the PMD (Kawabata et al. 1999); 74% ofthe SAVs were deemed effect in a binary classi\u0000cation. DMS4 marks the \u0000rst four human proteins(BRAC1, PTEN, TPMT, PPARG) for which we obtained comprehensive experimental DMSmeasurements along with a means of converting experimental scores into a binary version(effect/neutral) using synonyms. DMS4 results are shown for a threshold of 95%: the continuouseffect scores were binarized by assigning the middle 95% of effect scores as neutral variants andSAVs resulting in effect scores outside this range as effect variants (Reeb et al. 2020). Methods:SNAP2bin: effect SNAP2-score >-0.05, otherwise neutral; VESPA: effect score ≥ 0.5, otherwise neutral;VESPAl: effect score ≥ 0.5, otherwise neutral; BLOSUM62: negative BLOSUM62 scores predicted aseffect, others as neutral; ProtT5cons|ConSeq-19equal: all 19 non-native SAVs predicted equally: effectif ProtT5cons|ConSeq predicted/labelled as conserved, otherwise neutral; ProtT5beff|ConSeq-blosum62: effect if ProtT5cons|ConSeq predicted/labelled as conserved and BLOSUM62 negative,otherwise neutral. ± values mark the 95% con\u0000dence interval (Eqn. 12). For each column, if available,signi\u0000cantly best results are highlighted in bold.\nMost performances differed substantially between PMD4k and DMS4, i.e., the \u0000rst four proteins (BRAC1,\nPTEN, TPMT, PPARG) for which we had obtained large-scale experimental DMS measures that could be\nconverted into a binary scale (effect/neutral). First, using BLOSUM62 to convert ProtT5cons into SAV-\nPage 14/35\nspeci\u0000c predictions outperformed the MSA-based conservation lookup from ConSeq, the expert method\nSNAP2 trained on PMD4k (Table 1: ProtT5beff highest rule-based), and the newly introduced VESPA.\nSecond, combining the BLOSUM62 matrix with conservation also improved ConSeq (Table 1: ConSeq:\n19equal lower than blosum62). Third, ranking across different performance measures correlated much\nbetter than for PMD4k (Tables S1-S5). As the mapping from continuous DMS effect scores to binary\nlabels might introduce systematic noise, we also investigated different thresholds for this mapping.\nHowever, results for DMS4 at intervals of 90% (Table S3) and 99% (Table S5) around the mean showed\nsimilar trends. We trained a logistic regression (LR) ensemble (VESPA) on cross-validation splits\nreplicated from the SNAP2 development set. For binary effect prediction, we introduced a threshold ( ≥ 0.5\neffect, otherwise neutral) to the output scores of VESPA. When comparing VESPA and VESPAl (light\nversion of VESPA) to the other methods on PMD4k we observed a different picture than for the rule-based\napproaches. While SNAP2 still resulted in the highest MCC (0.28±0.01), it was not signi\u0000cantly higher\nthan that of VESPA and VESPAl (MCC: 0.274±0.09 and 0.271±0.09 respectively), and its development set\noverlapped with PMD4k. When evaluating the methods on DMS4, the best performing method, VESPAl\n(MCC 0.405±0.016), outperformed SNAP2 (MCC 0.204±0.012) and VESPA (MCC 0.346±0.014) as well as\nall rule-based methods (Table 1). We observed the same trends for other intervals (Tables S3-S5).\npLMs predicted SAV effect scores without MSAs. Could VESPA, trained on binary effect data (Eff10k)\ncapture continuous SAV effect scores measured by DMS? For ease of comparison with other methods,\nwe chose all 39 DMS experiments (DMS39) with single SAV effect data assembled for the development\nof DeepSequence (Riesselman et al. 2018). Several methods have recently been optimized on DMS data,\ne.g., the apparent state-of-art (SOTA), DeepSequence trained on the MSAs of each of those 39\nexperiments. Another recent method using evolutionary information in a more advanced way than\nstandard pro\u0000les from MSAs appears to reach a similar top level without machine learning, namely\nGEMME (Laine et al. 2019), and so does a method based on probabilities from pLMs, namely ESM-1v,\nwithout using MSAs. Comparing all those to VESPA, we could not observe a single method outperforming\nall others on all DMS39 experiments (Fig. 5). The four methods compared (two using MSAs:\nDeepSequence and GEMME, two using probabilities from pLMs instead of MSAs: ESM-1v and VESPA)\nreached Spearman rank correlations above 0.4 for 36 DMS experiments. In fact, for the 11 highest\ncorrelating out of the 39 experiments, predictions were as accurate as typically the agreement between\ntwo different experimental studies of the same protein (Spearman 0.65 (Reeb et al. 2020)).\nGEMME had a slightly higher mean and median Spearman correlation (Eqn. 11) than DeepSequence,\nESM-1v, VESPA and all others tested (Fig. 6A, Table 2). When considering the symmetric 95% con\u0000dence\nintervals (Eqn. 12) almost all those differences were statistically insigni\u0000cant (Fig. 6B) except for only\nusing BLOSUM62. In terms of mean Spearman correlation, VESPA was slightly higher than\nDeepSequence, which was slightly higher than ESM-1v (Fig. 6A), but again neither was signi\u0000cantly\nbetter. The median Spearman correlation was equal for ESM-1v and VESPA and insigni\u0000cantly lower for\nDeepSequence. The fastest method, VESPAl, reached lower Spearman correlations than all other major\nmethods (Fig. 6). Ranking and relative performance after correcting for statistical signi\u0000cance were\nidentical for Spearman and Pearson correlation (Table S6).\nPage 15/35\nTable 2\nSpearman correlation between SAV effect prediction and DMS experiments *\nMethod Mean absolute\\varvecr\\varvecS\n(Eqn. 11)\nMedian absolute\\varvecr\\varvecS\n(Eqn. 11)\nMSA-based    \nDeepSequence 0.50±0.03 0.52±0.03\nGEMME 0.53±0.02 0.56±0.02\npLM-based    \nESM-1v 0.49±0.02 0.53±0.02\nVESPA 0.51±0.02 0.53±0.02\nVESPAl 0.47±0.02 0.47±0.02\n*Data sets: DMS39 (39 DMS experiments gathered for the development of DeepSequence(Riesselman et al. 2018)) with 135,665 SAV scores. Methods: DeepSequence: AI trained on MSA foreach of the DMS experiments (Riesselman et al. 2018); GEMME: using evolutionary informationcalculated from MSAs with few parameters optimized on DMS (Laine et al. 2019); ESM-1v:embedding-based prediction methods (Meier et al. 2021); VESPA: method developed here usinglogistic regression to combine predicted conservation (ProtT5cons), BLOSUM62 (Henikoff andHenikoff 1992) substitution scores, and log-odds from ProtT5 (Elnaggar et al. 2021); VESPAl: “light”version of VESPA using only predicted conservation and BLOSUM62 as input. ± values mark thestandard error.\nFor comparison, we also introduced two advances on a random baseline, namely the raw BLOSUM62\nscores and the raw ProtT5 log-odds scores (Fig. 6; Fig. S7). BLOSUM62 was consistently and statistically\nsigni\u0000cantly outperformed by all methods, while the ProtT5 log-odds averages were consistently lower,\nalbeit not with statistical signi\u0000cance. As pLM-based methods were independent of MSAs, they predicted\nSAV scores for all residues contained in the DMS39 data sets, while e.g., DeepSequence and GEMME\ncould predict only for the subset of the residues covered by large enough MSAs. This was re\u0000ected by\ndecreased coverage of methods relying on MSAs (DeepSequence and GEMME; Table S8). The Spearman\ncorrelation of ESM-1v, VESPA and VESPAl for the SAVs in regions without MSAs was signi\u0000cantly lower\nthan that in regions with MSAs available (Table S7). SAV effect predictions blazingly fast. One important\nadvantage of predicting SAV effects without using MSAs is the computational e\u0000ciency. For instance, to\npredict the mutational effects for all 19 non-native SAVs in the entire human proteome (all residues in all\nhuman proteins) took 40 minutes on one Nvidia Quadro RTX 8000 using VESPAl. In turn, this was forty\nminutes more than using BLOSUM62 alone (nearly instantaneous), but this instantaneous BLOSUM62-\nbased prediction was also much worse (Q2 for binary BLOSUM62 prediction worse than random, Table\n1). In contrast, running methods such as SNAP2 (or ConSeq) required \u0000rst to generate MSAs. Even the\nblazingly fast MMseqs2 (Steinegger and Söding 2017) needed about 90 minutes using batch-processing\non an Intel Skylake Gold 6248 processor with 40 threads, SSD and 377GB main memory. While VESPAl\ncomputed prediction scores within minutes for an entire proteome, VESPA and ESM-1v require minutes\nPage 16/35\nfor some single proteins depending on sequence length, e.g., ESM-1v took on average 170s per protein for\nthe DMS39 set while ProtT5 required on average 780s. This originated from the number of forward-\npasses required to derive predictions: while VESPAl needed only a single forward-pass through the pLM to\nderive embeddings for conservation prediction, VESPA and ESM-1v (when deriving “masked-marginals”\nas recommended by the authors) required L forward passes with L being the protein length because they\ncorrupt one amino acid at a time and try to reconstruct it. The large difference in runtime between ESM-1v\nand ProtT5 originated from the fact that ESM-1v cropped sequences after 1022, reducing the strong\nimpact of outliers, i.e., runtime of transformer-based models scales quadratically with sequence length,\nso while the shortest protein (71 residues) in the DMS39 set took only 5s to compute, the longest (3033\nresidues) took 4.5h to compute. We leave investigating the effect of splitting very long proteins into\n(overlapping) chunks to future work.\nDiscussion\nConservation predicted by embeddings without MSAs. Even a simple logistic regression (LR) su\u0000ced to\npredict per-residue conservation values from raw embeddings without using MSAs (Fig. 3, Table S1).\nRelatively shallow CNNs (with almost 100-times fewer free parameters than samples despite early\nstopping) improved over the LR to levels in predicting conservation only slightly below conservation\nassigned by ConSeq which explicitly uses MSAs (Fig. 3, Table S1). Did this imply that the pLMs extracted\nevolutionary information from unlabeled sequence databases (BFD (Steinegger and Söding 2018) and\nUniProt (The UniProt Consortium 2021))? The answer might be more elusive than it seems. The\nmethodology (pLMs) applied to predict conservation never encountered any explicit information about\nprotein families through MSAs, i.e., the pLMs used here never had an explicit opportunity to pick up\nevolutionary constraints from related proteins. The correlation between substitution probabilities derived\nfrom pLMs and conservation (Fig. 2) might suggest that pLMs implicitly learned evolutionary\ninformation.\nA possible counterargument builds around the likelihood to pick up evolutionary constraints. The pLM\nclearly learned the reconstruction of more frequent amino acids much better than that of less frequent\nones (Fig. S5). Unsurprisingly, AI is pushed most in the direction of most data. In fact, the differences\nbetween amino acid compositions were relatively small (less than factor of 10), suggesting that even an\nevent occurring at one tenth of the time may challenge pLMs. If the same pLM has to learn the\nevolutionary relation between two proteins belonging to the same family, it has to effectively master an\nevent happening once in a million (assuming an average family size of about 2.5k – thousand - in a\ndatabase with 2.5b – billion – sequences). How can the model trip over a factor of 101 and at the same\ntime master a factor of 106? Indeed, it seems almost impossible. If so, the pLM may not have learned\nevolutionary constraints, but the type of biophysical constraint that also constrain evolution. In this\ninterpretation, the pLM did not learn evolution, but the constraints “written into protein sequences” that\ndetermine which residue positions are more constrained.\nPage 17/35\nIn fact, one pLM used here, namely ProtT5, has recently been shown to explicitly capture aspects of long-\nrange inter-residue distances directly during pre-training. I.e., without ever being trained on any labeled\ndata pLMs pick up structural constraints that allow protein 3D structure prediction from single protein\nsequences (Weißenow et al. 2021). Another explanation for how ProtT5 embeddings capture\nconservation might be that pLMs picked up signals from short, frequently re-occurring\nsequence/structure motifs such as localization signals or catalytic sites that are more conserved than\nother parts of the sequence. If so, the pLM would not have to learn relationship between proteins but only\nbetween fragments, thereof reducing the factor 106 substantially. We could conceive of these motifs\nresembling some evolutionary nuclei, i.e., fragments shorter than structural domains that drove evolution\n(Alva et al. 2015; Ben-Tal and Lupas 2021; Kolodny 2021). Clearly, more work will have to shed light on\nthe e\u0000ciency of (p)LMs in general (Bommasani et al. 2021).\nTransformer-based pLMs best? We have tested a limited set of pLMs, largely chosen because those had\nappeared to perform better than many other methods for a variety of different prediction tasks. Does the\nfact that in our hands Transformer-based pLMs worked best to predict residue conservation and SAVs\nimply that those will generally outperform other model types? By no means. While we expect that the\nabout twenty approaches that we have compared in several of our recent methods (including the\nfollowing 13: ESM-1[b|v] (Meier et al. 2021; Rives et al. 2021), ProSE[*|DLM|MT] (Bepler and Berger 2019b,\n2021), Prot[Albert|Bert|Electra|Vec|T5|T5XL|T5XLNet|T5XXL] (Elnaggar et al. 2021; Heinzinger et al. 2019)\nprovided a somehow representative sampling of the existing options, our conclusions were only valid for\nembeddings extracted in a generic way from generic pLMs without any bearing on the methods\nunderlying those pLMs.\nPredicted conservation informative about SAV effects. DMS data sets with comprehensive experimental\nprobing of the mutability landscape (Hecht et al. 2013) as, e.g., collected by MaveDB (Esposito et al.\n2019) continue to pose problems for analysis, possibly due to a diversity of assays and protocols\n(Livesey and Marsh 2020; Reeb et al. 2020). Nevertheless, many such data sets capture important\naspects about the susceptibility to change, i.e., the mutability landscape (Hecht et al. 2013). As always,\nthe more carefully selected data sets become, the more they are used for the development of methods\nand therefore no longer can serve as independent data for assessments (Grimm et al. 2015; Reeb et al.\n2016). Avoiding the traps of circularity and over-\u0000tting by skipping training, our non-parametric rule-based\napproaches (ProtT5cons and ProtT5beff) suggested that predictions of SAV effects (by simply assigning\n“effect” to those SAVs where ProtT5cons predicted conserved and the corresponding BLOSUM62 value\nwas negative) outperformed ConSeq with MSAs using the same idea, and even the expert effect\nprediction method SNAP2 (Fig. 4, Table 1).\nStrictly speaking, it might be argued that one single free parameter was optimized using the data set,\nbecause for the PMD4k data set the version that predicted the same effect for all 19-SAVs appeared to\noutperform the SAV-speci\u0000c prediction using BLOSUM62 (19equal vs blosum62 in Fig. 4 and Table 1).\nHowever, not even the values computed for PMD4k could distract from the simple fact that not all SAVs\nare equal, i.e., that regardless of model performance, 19equal will not be used exclusively for any method.\nPage 18/35\nIn fact, the concept of combining predictions with BLOSUM62 values has been shown to succeed for\nfunction prediction before (Bromberg and Rost 2008; Schelling et al. 2018) in that sense it was arguably\nnot an optimizable hyperparameter. Embeddings predicted conservation (Fig. 3); conservation predicted\nSAV effects (Fig. 4). Did this imply that embeddings captured evolutionary information? Once again, we\ncould not answer this question either way directly. To repeat: our procedure/method never used\ninformation from MSAs in any way. Could it have implicitly learned this? To repeat the previous\nspeculation: embeddings might capture a reality that constrains what can be observed in evolution, and\nthis reality is exactly what is used for the part of the SAV effect prediction that succeeds. If so, we would\nargue that our simpli\u0000ed method did not succeed because it predicted conservation without using MSAs,\nbut that it captured positions biophysically “marked by constraints”, i.e., residues with higher contact\ndensity in protein 3D structures (Weißenow et al. 2021). This assumption would explain how predicted\nconservation (ProtT5cons) not using evolutionary information could predict SAV effects better than a\nslightly more correct approach (ConSeq) using MSAs to extract evolutionary information (Fig. 4:\nProtT5cons vs. ConSeq).\nSubstitution probabilities from pLMs capture aspects measured by DMS experiments. Using embeddings\nto predict SAV effects through conservation prediction succeeded but appeared like a detour. ESM-1v\n(Meier et al. 2021) pioneered a direct path from reconstruction/substitution probabilities of pLMs to SAV\neffect predictions. When comparing the ESM-1v encoder-based with the ProtT5 encoder-decoder based\nTransformer, we encountered surprising results. Previously, ProtT5 usually performed at least on par with\nprevious versions of ESM (e.g. ESM-1b (Rives et al. 2021)) or outperformed them (Elnaggar et al. 2021).\nIn contrast, the substitution probabilities of ProtT5 were clearly inferior to those from ESM-1v in their\ncorrelation with the 39 DMS experiments (Fig. 6). This reversed trend might have resulted from a\ncombination of the following facts: (1) ProtT5 is a single model while ESM-1v is an ensemble of \u0000ve\npLMs potentially leading to a smoother substitution score. (2) ESM-1v was trained on UniRef90 instead\nof BFD/UniRef50 (ProtT5) possibly providing a broader view on the mutability landscape of proteins. In\nfact, the ESM-1v authors showed a signi\u0000cant improvement when pre-training on UniRef90 instead of\nUniRef50 (Rives et al. 2021). (3) ESM-1v is a BERT-style, encoder-based Transformer while ProtT5 is\nbased on T5’s encoder-decoder structure. In previous experiments (Elnaggar et al. 2021), we only\nextracted embeddings from ProtT5’s encoder (e.g. ProtT5cons is based on encoder embeddings) because\nits decoder fell signi\u0000cantly short in all experiments. However, only T5’s decoder can output probabilities,\nso we had to fall back to ProtT5’s decoder for SAV effect predictions. This discrepancy of encoder and\ndecoder performance can only be sketched here. In short, encoder-based Transformer models always see\nthe context of the whole sequence (as does ProtT5‘s encoder and ESM-1v) while decoder-based\nTransformer models (such as ProtT5’s decoder or GPT (Radford et al. 2019)) see only single-sided\ncontext because they are generating text (sequence-to-sequence models (Sutskever et al. 2014)). This is\ncrucial for translation tasks but appeared sub-optimal in our setting. Despite this shortcoming in\nperformance, we trained VESPA based on log-odds derived from ProtT5 substitution probabilities, mainly\nbecause we started this work before the release of ESM-1v. Also, we hoped for synergy effects when\nimplementing VESPA into the PredictProtein webserver because ProtT5 is already used by many of our\nPage 19/35\npredictors. Finding the best combination of pLM substitution probabilities for SAV effect prediction will\nremain subject for future work.\nFast predictions save computing resources? Our simple protocol introduced here enabled extremely\ne\u0000cient, speedy predictions. While pre-training pLMs consumed immense resources (Elnaggar et al.\n2021), this was done in the past. The new development here were the models for the 2nd level supervised\ntransfer learning. Inputting ProtT5 embeddings to predict residue conservation (ProtT5cons) or SAV\neffects (VESPA/VESPAl) for predictions in the future will consume very little additional resources. When\nrunning prediction servers such as PredictProtein (Bernhofer et al. 2021) queried over 3,000 times every\nmonth, such investments could be recovered rapidly at seemingly small prices to pay even if performance\nwere slightly reduced. How to quantify this? At what gain in computing e\u0000ciency is which performance\nreduction acceptable? Clearly, there will not be one answer for all purposes, but the recent reports on\nclimate change strongly suggest to begin considering such questions.\nQuantitative metrics for hypothetical improvements over MSA-based methods? If methods using single\nsequences without MSAs perform as well as, or even better than, SOTA methods using MSAs, could we\nquantify metrics measuring the hypothetical improvements from embeddings? This question raised by\nan anonymous reviewer opens an interesting new perspective. Gain in speed, reduction of computational\ncosts clearly could evolve as one such metric. A related issue is related to protein design: for some\napplications the difference in speed might open new doors. Although we have no data to show for others,\nwe could imagine yet another set of metrics measuring the degree to which embedding-based methods\nrealize more protein-speci\u0000c than family-averaged predictions.\nConclusions\nEmbeddings extracted from protein Language Models (pLMs, Fig. 1), namely from ProtBert and ProtT5\n(Elnaggar et al. 2021) and ESM1-b (Rives et al. 2021), contain information that su\u0000ced to predict residue\nconservation in protein families without using multiple sequence alignments (MSAs, Fig. 3). Such\npredictions of conservation combined with BLOSUM62 scores predicted the effects of sequence variation\n(single amino acid variants, or SAVs) without optimizing any additional free parameter (ProtT5beff,\nFig. 6). Through further training on binary experimental data (effect/neutral), we developed VESPA, a\nrelatively simple, yet apparently successful new method for SAV effect prediction (Fig. 4). This method\neven worked so well on non-binary data from 39 DMS experiments that without ever using such data nor\never using MSAs, VESPA appeared competitive with the SOTA (Fig. 5, Fig. 6), although for SAV effect\npredictions embedding-based methods are still not yet outperforming the MSA-based SOTA as for other\nprediction tasks (Elnaggar et al. 2021; Littmann et al. 2021a; Littmann et al. 2021b; Littmann et al. 2021c;\nStärk et al. 2021). Embedding-based predictions are blazingly fast, thereby they save computing, and\nultimately energy resources when applied to daily sequence analysis. In combination, our results\nsuggested the major signal captured by variant effect predictions originates from some biophysical\nconstraint revealed by raw protein sequences. The ConSurf10k dataset is available at\nhttps://doi.org/10.5281/zenodo.5238537. For high-throughput predictions, methods are available\nPage 20/35\nthrough bio_embeddings (Dallago et al. 2021). For single queries VESPA and ProtT5cons will be made\navailable through the PredictProtein server (Bernhofer et al. 2021). VESPA and VESPAl are also available\nfrom github at https://github.com/Rostlab/VESPA.\nAbbreviations\nAI, Arti\u0000cial Intelligence\nAUC, Area under the curve\nBFD, Big Fantastic Database\nCNN, convolutional neural network\nDL, Deep Learning\nEI, Evolutionary Information\nDMS, deep mutational scanning\nFNN, feed forward neural network\nGoF, gain of function SAV\nLoF, loss of function SAV\nLM, Language Model\nLR, logistic regression\nMAVE, Multiplexed Assays of Variant Effect\nMCC, Matthews correlation coe\u0000cient\nML, machine learning\nMSA, multiple sequence alignments\nNLP, natural language processing\nOMIM, Online Mendelian Inheritance in Man\nPDB, Protein Data Bank\npLM, protein Language Model (used here\nESM-1b/1v, ProtBERT, ProtT5)\nPMD, protein mutant database\nProtT5beff, rule-based method developed here using ProtT5 embeddings to predict binary SAV effects\nfrom single sequences\nProtT5cons, method developed here using ProtT5 embeddings to predict residue conservation from\nsingle sequences optimizing a CNN on the unchanged pre-trained ProtT5\nReLU, recti\u0000ed linear unit\nROC, Receiver operating characteristic\nSAV, single amino-acid variant (also known as SAAV or nsSNP, or missense mutation/variant)\nSOTA, State-of-the-art\nSSD, Solid State Drive\nSVM, Support Vector Machine\nVESPA, method developed here for Variant Effect Score Prediction without Alignments\nVESPAl, light VESPA\nPage 21/35\nDeclarations\nCompeting interests\nNo author declares any competing interest.\nFunding\nThis work was supported by the DFG grant RO 1320/4-1, Software Campus Funding (BMBF 01IS17049)\nand the KONWIHR Program and by the Bavarian Ministry for Education through funding to the TUM.\nAuthor contributions\nC.M. implemented and evaluated the methods and took the lead in writing the manuscript. M.H.\nconceived, trained, and evaluated the neural networks on conservation prediction, contributed ideas and\nproofread the manuscript. T.O. and C.D contributed crucial ideas and provided valuable comments. M.B.\nhelped in generating the evaluation methods ConSeq and SNAP2. K.E. supported the work with coding\nadvice and created the original ConSurf10k data set. D.N. contributed the clusters and subsets of the\nSNAP2 development set. B.R. supervised and guided the work and co-wrote the manuscript. All authors\nread and approved the \u0000nal manuscript.\nAcknowledgements\nThanks to Tim Karl and Inga Weise (both TUM) for invaluable help with technical and administrative\naspects of this work. Thanks to Nir Ben-Tal (Tel Aviv U) and his team for the excellent services around\nConSurf-DB and ConSeq; to Yana Bromberg (Rutgers U), Max Hecht (Amazon) for advancing SNAP; to\nAdam Riesselman, John Ingraham, and Debbie Marks (Harvard) for making their collection of DMS data\navailable; to Elodie Laine (Sorbonne U) for providing the predictions of GEMME; to the group around\nFacebook AI Research for making ESM-1b and ESM-1v readily available; to the Dunbrack lab for Pisces,\nand most importantly to Martin Steinegger (Seoul Natl. Univ.) and his team for MMseqs2 and BFD.\nParticular thanks to two anonymous reviewers who helped crucially with improving this work and to the\nvaluable comments from the editors. Last, but not least, thanks to all who deposit their experimental data\nin public databases, and to those who maintain these databases.\nReferences\n1. Adzhubei IA, Schmidt S, Peshkin L, Ramensky VE, Gerasimova A, Bork P, Kondrashov AS, Sunyaev SR\n(2010) A method and server for predicting damaging missense mutations. Nat Methods 7:248–249.\ndoi: 10.1038/nmeth0410-248\nless accurate but faster.\nPage 22/35\n2. Alley EC, Khimulya G, Biswas S, AlQuraishi M, Church GM (2019) Uni\u0000ed rational protein engineering\nwith sequence-based deep representation learning. Nat Methods 16:1315–1322. doi:\n10.1038/s41592-019-0598-1\n3. Altschul SF, Madden TL, Schaffer AA, Zhang J, Zhang Z, Miller W, Lipman DJ (1997) Gapped BLAST\nand PSI-BLAST: a new generation of protein database search programs. Nucleic Acids Res 25:3389–\n3402. doi: 10.1093/nar/25.17.3389\n4. Alva V, Söding J, Lupas AN (2015) A vocabulary of ancient peptides at the origin of folded proteins.\nElife 4. doi: 10.7554/eLife.09410\n5. Amberger JS, Bocchini CA, Scott AF, Hamosh A (2019) OMIM.org: leveraging knowledge across\nphenotype-gene relationships. Nucleic Acids Res 47:D1038–D1043. doi: 10.1093/nar/gky1151\n\u0000. AVE Alliance Founding Members (2020) Atlas of Variant Effect Alliance\n7. Ben-Tal N, Lupas AN (2021) Editorial overview: Sequences and topology: 'paths from sequence to\nstructure'. Curr Opin Struct Biol 68: vi-viii. doi: 10.1016/j.sbi.2021.05.005\n\u0000. Ben Chorin A, Masrati G, Kessel A, Narunsky A, Sprinzak J, Lahav S, Ashkenazy H, Ben-Tal N (2020)\nConSurf ‐ DB: An accessible repository for the evolutionary conservation patterns of the majority of\nPDB proteins. Protein Sci 29:258–267. doi: 10.1002/pro.3779\n9. Bepler T, Berger B (2019a) Learning protein sequence embeddings using information from\nstructure.arXiv. doi: arXiv:1902.08661\n10. Bepler T, Berger B (2019b) Learning protein sequence embeddings using information from structure\nSeventh International Conference on Learning Representations\n11. Bepler T, Berger B (2021) Learning the protein language: Evolution, structure, and function. Cell Syst\n12: 654-669 e3. doi: 10.1016/j.cels.2021.05.017\n12. Berezin C, Glaser F, Rosenberg J, Paz I, Pupko T, Fariselli P, Casadio R, Ben-Tal N (2004) ConSeq: the\nidenti\u0000cation of functionally and structurally important residues in protein sequences. Bioinformatics\n(Oxford, England) 20:1322–1324. doi: 10.1093/bioinformatics/bth070\n13. Berman HM, Westbrook J, Feng Z, Gilliland G, Bhat TN, Weissig H, Shindyalov IN, Bourne PE (2000)\nThe Protein Data Bank. Nucleic Acids Res 28:235–242. doi: 10.1093/nar/28.1.235\n14. Bernhofer M, Dallago C, Karl T, Satagopam V, Heinzinger M, Littmann M, Olenyi T, Qiu J, Schutze K,\nYachdav G, Ashkenazy H, Ben-Tal N, Bromberg Y, Goldberg T, Kajan L, O'Donoghue S, Sander C,\nSchafferhans A, Schlessinger A, Vriend G, Mirdita M, Gawron P, Gu W, Jarosz Y, Trefois C, Steinegger\nM, Schneider R, Rost B (2021) PredictProtein - Predicting Protein Structure and Function for 29 Years.\nNucleic Acids Res. doi: 10.1093/nar/gkab354\n15. Bommasani R, Hudson DA, Adeli E, Altman R, Arora S, von Arx S, Bernstein MS, Bohg J, Bosselut A,\nBrunskill E, Brynjolfsson E, Buch S, Card D, Castellon R, Chatterji N, Chen A, Creel K, Quincy Davis J,\nDemszky D, Donahue C, Doumbouya M, Durmus E, Ermon S, Etchemendy J, Ethayarajh K, Fei-Fei L,\nFinn C, Gale T, Gillespie L, Goel K, Goodman N, Grossman S, Guha N, Hashimoto T, Henderson P,\nHewitt J, Ho DE, Hong J, Hsu K, Huang J, Icard T, Jain S, Jurafsky D, Kalluri P, Karamcheti S, Keeling\nG, Khani F, Khattab O, Kohd PW, Krass M, Krishna R, Kuditipudi R, Kumar A, Ladhak F, Lee M, Lee T,\nPage 23/35\nLeskovec J, Levent I, Li XL, Li X, Ma T, Malik A, Manning CD, Mirchandani S, Mitchell E, Munyikwa Z,\nNair S, Narayan A, Narayanan D, Newman B, Nie A, Niebles JC, Nilforoshan H, Nyarko J, Ogut G, Orr L,\nPapadimitriou I, Park JS, Piech C, Portelance E, Potts C, Raghunathan A, Reich R, Ren H, Rong F,\nRoohani Y, Ruiz C, Ryan J, Ré C, Sadigh D, Sagawa S, Santhanam K, Shih A, Srinivasan K, Tamkin A,\nTaori R, Thomas AW, Tramèr F, Wang RE, Wang W et al (2021) On the Opportunities and Risks of\nFoundation Models. pp arXiv:2108.07258\n1\u0000. Bromberg Y, Rost B (2007) SNAP: predict effect of non-synonymous polymorphisms on function.\nNucleic Acids Res 35:3823–3835\n17. Bromberg Y, Rost B (2008) Comprehensive in silico mutagenesis highlights functionally important\nresidues in proteins. Bioinformatics 24:i207–i212\n1\u0000. Bromberg Y, Rost B (2009) Correlating protein function and stability through the analysis of single\namino acid substitutions. BMC Bioinformatics 10:S8. doi: 10.1186/1471-2105-10-s8-s8\n19. Burley SK, Berman HM, Bhikadiya C, Bi C, Chen L, Di Costanzo L, Christie C, Dalenberg K, Duarte JM,\nDutta S, Feng Z, Ghosh S, Goodsell DS, Green RK, Guranovic V, Guzenko D, Hudson BP, Kalro T, Liang\nY, Lowe R, Namkoong H, Peisach E, Periskova I, Prlic A, Randle C, Rose A, Rose P, Sala R, Sekharan M,\nShao C, Tan L, Tao YP, Valasatava Y, Voigt M, Westbrook J, Woo J, Yang H, Young J, Zhuravleva M,\nZardecki C (2019) RCSB Protein Data Bank: biological macromolecular structures enabling research\nand education in fundamental biology, biomedicine, biotechnology and energy. Nucleic Acids Res\n47:D464–D474. doi: 10.1093/nar/gky1004\n20. Capriotti E, Fariselli P, Casadio R (2005) I-Mutant2.0: predicting stability changes upon mutation from\nthe protein sequence or structure. Nucleic Acids Res 33:W306–W310. doi: 10.1093/nar/gki375\n21. Dallago C, Schuetze K, Heinzinger M, Olenyi T, Littmann M, Lu AX, Yang KK, Min S, Yoon S, Morton JT,\nRost B (2021) Learned Embeddings from Deep Learning to Visualize and Predict Protein Sets. Curr\nProtoc 1:e113. doi: 10.1002/cpz1.113\n22. Devlin J, Chang M-W, Lee K, Toutanova K (2019) BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. arXiv:1810.04805 [cs].\n23. Efron B, Halloran E, Holmes S (1996) Bootstrap con\u0000dence levels for phylogenetic trees. Proceedings\nof the National Academy of Sciences USA 93: 13429-13434\n24. Elnaggar A, Heinzinger M, Dallago C, Rehawi G, Wang Y, Jones L, Gibbs T, Feher T, Angerer C,\nSteinegger M, Bhowmik D, Rost B (2021) ProtTrans: Towards Cracking the Language of Life’s Code\nThrough Self-Supervised Learning. MACHINE INTELLIGENCE 14:30\n25. Esposito D, Weile J, Shendure J, Starita LM, Papenfuss AT, Roth FP, Fowler DM, Rubin AF (2019)\nMaveDB: an open-source platform to distribute and interpret data from multiplexed assays of variant\neffect. Genome Biol 20:223. doi: 10.1186/s13059-019-1845-6\n2\u0000. Fariselli P, Martelli PL, Savojardo C, Casadio R (2015) INPS: predicting the impact of non-\nsynonymous variations on protein stability from sequence. Bioinformatics 31:2816–2821. doi:\n10.1093/bioinformatics/btv291\nPage 24/35\n27. Findlay GM, Daza RM, Martin B, Zhang MD, Leith AP, Gasperini M, Janizek JD, Huang X, Starita LM,\nShendure J (2018) Accurate classi\u0000cation of BRCA1 variants with saturation genome editing. Nature\n562:217–222. doi: 10.1038/s41586-018-0461-z\n2\u0000. Fowler DM, Fields S (2014) Deep mutational scanning: a new style of protein science. Nat Methods\n11:801–807. doi: 10.1038/nmeth.3027\n29. Fu L, Niu B, Zhu Z, Wu S, Li W (2012) CD-HIT: accelerated for clustering the next-generation\nsequencing data. Bioinformatics 28:3150–3152. doi: 10.1093/bioinformatics/bts565\n30. Fukushima K (1969) Visual Feature Extraction by a Multilayered Network of Analog Threshold\nElements. IEEE Transactions on Systems Science and Cybernetics 5:322–333. doi:\n10.1109/TSSC.1969.300225\n31. Gray VE, Hause RJ, Luebeck J, Shendure J, Fowler DM (2018) Cell Systems 6:116–124e3. doi:\n10.1016/j.cels.2017.11.003. Quantitative Missense Variant Effect Prediction Using Large-Scale\nMutagenesis Data\n32. Grimm DG, Azencott CA, Aicheler F, Gieraths U, Macarthur DG, Samocha KE, Cooper DN, Stenson PD,\nDaly MJ, Smoller JW, Duncan LE, Borgwardt KM (2015) The evaluation of tools used to predict the\nimpact of missense variants is hindered by two types of circularity. Hum Mutat 36:513–523. doi:\n10.1002/humu.22768\n33. Hecht M, Bromberg Y, Rost B (2013) News from the protein mutability landscape. J Mol Biol\n425:3937–3948. doi: 10.1016/j.jmb.2013.07.028\n34. Hecht M, Bromberg Y, Rost B (2015) Better prediction of functional effects for sequence variants.\nBMC Genomics 16:S1. doi: 10.1186/1471-2164-16-s8-s1\n35. Heinzinger M, Elnaggar A, Wang Y, Dallago C, Nechaev D, Matthes F, Rost B (2019) Modeling aspects\nof the language of life through transfer-learning protein sequences. BMC Bioinformatics 20:723. doi:\n10.1186/s12859-019-3220-8\n3\u0000. Henikoff S, Henikoff JG (1992) Amino acid substitution matrices from protein blocks. Proceedings of\nthe National Academy of Sciences 89: 10915-10919. doi: 10.1073/pnas.89.22.10915\n37. Hopf TA, Ingraham JB, Poelwijk FJ, Scharfe CP, Springer M, Sander C, Marks DS (2017) Mutation\neffects predicted from sequence co-variation. Nat Biotechnol 35:128–135. doi: 10.1038/nbt.3769\n3\u0000. Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool K, Bates R, Zidek\nA, Potapenko A, Bridgland A, Meyer C, Kohl SAA, Ballard AJ, Cowie A, Romera-Paredes B, Nikolov S,\nJain R, Adler J, Back T, Petersen S, Reiman D, Clancy E, Zielinski M, Steinegger M, Pacholska M,\nBerghammer T, Bodenstein S, Silver D, Vinyals O, Senior AW, Kavukcuoglu K, Kohli P, Hassabis D\n(2021) Highly accurate protein structure prediction with AlphaFold. Nature. doi: 10.1038/s41586-021-\n03819-2\n39. Katoh K, Standley DM (2013) MAFFT multiple sequence alignment software version 7:\nimprovements in performance and usability. Mol Biol Evol 30:772–780. doi:\n10.1093/molbev/mst010\nPage 25/35\n40. Katsonis P, Lichtarge O (2014) A formal perturbation equation between genotype and phenotype\ndetermines the Evolutionary Action of protein-coding variations on \u0000tness. Genome Res 24:2050–\n2058. doi: 10.1101/gr.176214.114\n41. Kawabata T, Ota M, Nishikawa K (1999) The Protein Mutant Database. Nucleic Acids Res 27:355–\n357. doi: 10.1093/nar/27.1.355\n42. Kingma DP, Ba J (2014) Adam: A Method for Stochastic Optimization. pp arXiv:1412.6980\n43. Kircher M, Witten DM, Jain P, O'Roak BJ, Cooper GM, Shendure J (2014) A general framework for\nestimating the relative pathogenicity of human genetic variants. Nat Genet 46:310–315. doi:\n10.1038/ng.2892\n44. Kolodny R (2021) Searching protein space for ancient sub-domain segments. Curr Opin Struct Biol\n68:105–112. doi: https://doi.org/10.1016/j.sbi.2020.11.006\n45. Korber B, Fischer WM, Gnanakaran S, Yoon H, Theiler J, Abfalterer W, Hengartner N, Giorgi EE,\nBhattacharya T, Foley B, Hastie KM, Parker MD, Partridge DG, Evans CM, Freeman TM, de Silva TI,\nAngyal A, Brown RL, Carrilero L, Green LR, Groves DC, Johnson KJ, Keeley AJ, Lindsey BB, Parsons\nPJ, Raza M, Rowland-Jones S, Smith N, Tucker RM, Wang D, Wyles MD, McDanal C, Perez LG, Tang H,\nMoon-Walker A, Whelan SP, LaBranche CC, Saphire EO, Monte\u0000ori DC (2020) Tracking Changes in\nSARS-CoV-2 Spike: Evidence that D614G Increases Infectivity of the COVID-19 Virus. Cell 182: 812-\n827.e19. doi: 10.1016/j.cell.2020.06.043\n4\u0000. Laha S, Chakraborty J, Das S, Manna SK, Biswas S, Chatterjee R (2020) Characterizations of SARS-\nCoV-2 mutational pro\u0000le, spike protein stability and viral transmission. Infection, Genetics and\nEvolution 85:104445. doi: 10.1016/j.meegid.2020.104445\n47. Laine E, Karami Y, Carbone A (2019) GEMME: a simple and fast global epistatic model predicting\nmutational effects. Mol Biol Evol. doi: 10.1093/molbev/msz179\n4\u0000. Littmann M, Bordin N, Heinzinger M, Schütze K, Dallago C, Orengo C, Rost B (2021a) Clustering\nFunFams using sequence embeddings improves EC purity Bioinformatics. doi:\nhttps://doi.org/10.1093/bioinformatics/btab371\n49. Littmann M, Heinzinger M, Dallago C, Olenyi T, Rost B (2021b) Embeddings from deep learning\ntransfer GO annotations beyond homology. Sci Rep 11:1160. doi: 10.1038/s41598-020-80786-0\n50. Littmann M, Heinzinger M, Dallago C, Weissenow K, Rost B (2021c) Protein embeddings and deep\nlearning predict binding residues for various ligand classes. bioRxiv: 2021.09.03.458869. doi:\n10.1101/2021.09.03.458869\n51. Liu J, Rost B (2003) Domains, motifs, and clusters in the protein universe. Curr Opin Chem Biol 7:5–\n11\n52. Liu J, Rost B (2004a) CHOP proteins into structural domain-like fragments. Proteins Struct Funct\nBioinform 55:678–688\n53. Liu J, Rost B (2004b) Sequence-based prediction of protein domains. Nucleic Acids Res 32:3522–\n3530\nPage 26/35\n54. Livesey BJ, Marsh JA (2020) Using deep mutational scanning to benchmark variant effect predictors\nand identify disease mutations. Mol Syst Biol 16:e9380. doi: 10.15252/msb.20199380\n55. Madani A, McCann B, Naik N, Shirish Keskar N, Anand N, Eguchi RR, Huang P, Socher R (2020)\nProGen: Language Modeling for Protein Generation. arXiv\n5\u0000. Majithia AR, Tsuda B, Agostini M, Gnanapradeepan K, Rice R, Peloso G, Patel KA, Zhang X, Broekema\nMF, Patterson N, Duby M, Sharpe T, Kalkhoven E, Rosen ED, Barroso I, Ellard S, Consortium UKMD,\nKathiresan S, Myocardial Infarction Genetics C, O'Rahilly S, Consortium UKCL, Chatterjee K, Florez JC,\nMikkelsen T, Savage DB, Altshuler D (2016) Prospective functional classi\u0000cation of all possible\nmissense variants in PPARG. Nat Genet 48:1570–1575. doi: 10.1038/ng.3700\n57. Matreyek KA, Starita LM, Stephany JJ, Martin B, Chiasson MA, Gray VE, Kircher M, Khechaduri A,\nDines JN, Hause RJ, Bhatia S, Evans WE, Relling MV, Yang W, Shendure J, Fowler DM (2018)\nMultiplex assessment of protein variant abundance by massively parallel sequencing. Nat Genet\n50:874–882. doi: 10.1038/s41588-018-0122-z\n5\u0000. Meier J, Rao R, Verkuil R, Liu J, Sercu T, Rives A (2021) Language models enable zero-shot prediction\nof the effects of mutations on protein function. bioRxiv: 2021.07.09.450648.. doi:\n10.1101/2021.07.09.450648\n59. Mercatelli D, Giorgi FM (2020) Geographic and Genomic Distribution of SARS-CoV-2 Mutations. Front\nMicrobiol 11. doi: 10.3389/fmicb.2020.01800\n\u00000. Miller M, Bromberg Y, Swint-Kruse L (2017) Computational predictors fail to identify amino acid\nsubstitution effects at rheostat positions. Sci Rep 7:41329. doi: 10.1038/srep41329\n\u00001. Mistry J, Finn RD, Eddy SR, Bateman A, Punta M (2013) Challenges in homology search: HMMER3\nand convergent evolution of coiled-coil regions. Nucleic Acids Res 41:e121. doi: 10.1093/nar/gkt263\n\u00002. Ng PC, Henikoff S (2003) SIFT: predicting amino acid changes that affect protein function. Nucleic\nAcids Res 31:3812–3814\n\u00003. Niroula A, Urolagin S, Vihinen M (2015) PON-P2: prediction method for fast and reliable identi\u0000cation\nof harmful variants. PLoS ONE 10:e0117380. doi: 10.1371/journal.pone.0117380\n\u00004. Nishikawa K, Ishino S, Takenaka H, Norioka N, Hirai T, Yao T, Seto Y (1994) Constructing a protein\nmutant database. Protein Eng 7:733. doi: 10.1093/protein/7.5.733\n\u00005. O’Donoghue SI, Schafferhans A, Sikta N, Stolte C, Kaur S, Ho BK, Anderson S, Procter J, Dallago C,\nBordin N, Adcock M, Rost B (2020) SARS-CoV-2 structural coverage map reveals state changes that\ndisrupt host immunity. bioRxiv: 2020.07.16.207308.. doi: 10.1101/2020.07.16.207308\n\u0000\u0000. Ofer D, Brandes N, Linial M (2021) The language of proteins: NLP, machine learning & protein\nsequences. Comput Struct Biotechnol J 19:1750–1758. doi: 10.1016/j.csbj.2021.03.022\n\u00007. Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss\nR, Dubourg V, Vanderplas J, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay É (2011)\nScikit-learn: Machine Learning in Python. J Mach Learn Res 12:2825–2830\n\u0000\u0000. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I (2019) Language Models are Unsupervised\nMultitask Learners. 24\nPage 27/35\n\u00009. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou Y, Li W, Liu PJ (2020) Exploring the\nLimits of Transfer Learning with a Uni\u0000ed Text-to-Text Transformer. arXiv:1910.10683 [cs, stat].\n70. Ramensky V, Bork P, Sunyaev S (2002) Human non-synonymous SNPs: server and survey. Nucleic\nAcids Res 30:3894–3900\n71. Rao R, Meier J, Sercu T, Ovchinnikov S, Rives A (2020) Transformer protein language models are\nunsupervised structure learners. bioRxiv: 2020.12.15.422761.. doi: 10.1101/2020.12.15.422761\n72. Reeb J (2020) Data for: Variant effect predictions capture some aspects of deep mutational\nscanning experiments. 1.. doi: 10.17632/2rwrkp7mfk.1\n73. Reeb J, Hecht M, Mahlich Y, Bromberg Y, Rost B (2016) Predicted molecular effects of sequence\nvariants link to system level of disease. PLoS Comput Biol 12:e1005047. doi:\n10.1371/journal.pcbi.1005047.. doi: 10.1371/journal.pcbi.1005047\n74. Reeb J, Wirth T, Rost B (2020) Variant effect predictions capture some aspects of deep mutational\nscanning experiments. BMC Bioinformatics 21:107. doi: 10.1186/s12859-020-3439-4\n75. Riesselman AJ, Ingraham JB, Marks DS (2018) Deep generative models of genetic variation capture\nthe effects of mutations. Nat Methods 15:816–822. doi: 10.1038/s41592-018-0138-4\n7\u0000. Rives A, Meier J, Sercu T, Goyal S, Lin Z, Liu J, Guo D, Ott M, Zitnick CL, Ma J, Fergus R (2021)\nBiological structure and function emerge from scaling unsupervised learning to 250 million protein\nsequences. Proceedings of the National Academy of Sciences 118. doi: 10.1073/pnas.2016239118\n77. Rost B (1996) PHD: predicting one-dimensional protein structure by pro\u0000le based neural networks.\nMethods Enzymol 266:525–539\n7\u0000. Rost B, Sander C (1992) Jury returns on structure prediction. Nature 360:540\n79. Rost B, Sander C (1993) Prediction of protein secondary structure at better than 70% accuracy. J Mol\nBiol 232:584–599. doi: 10.1006/jmbi.1993.1413\n\u00000. Schelling M, Hopf TA, Rost B (2018) Evolutionary couplings and sequence variation effect predict\nprotein binding sites. Proteins 86:1064–1074. doi: 10.1002/prot.25585\n\u00001. Schwarz JM, Rodelsperger C, Schuelke M, Seelow D (2010) MutationTaster evaluates disease-\ncausing potential of sequence alterations. Nat Methods 7:575–576. doi: 10.1038/nmeth0810-575\n\u00002. Sim N-L, Kumar P, Hu J, Henikoff S, Schneider G, Ng PC (2012) SIFT web server: predicting effects of\namino acid substitutions on proteins. Nucleic Acids Res 40:W452–W457. doi: 10.1093/nar/gks539\n\u00003. Sruthi CK, Balaram H, Prakash MK (2020) Toward Developing Intuitive Rules for Protein Variant\nEffect Prediction Using Deep Mutational Scanning Data. ACS Omega 5:29667–29677. doi:\n10.1021/acsomega.0c02402\n\u00004. Stärk H, Dallago C, Heinzinger M, Rost B (2021) Light Attention Predicts Protein Location from the\nLanguage of Life. bioRxiv: 2021.04.25.441334. doi: 10.1101/2021.04.25.441334\n\u00005. Steinegger M, Söding J (2017) MMseqs2 enables sensitive protein sequence searching for the\nanalysis of massive data sets. Nat Biotechnol 35:1026\nPage 28/35\n\u0000\u0000. Steinegger M, Söding J (2018) Clustering huge protein sequence sets in linear time. Nat Commun\n9:2542. doi: 10.1038/s41467-018-04964-5\n\u00007. Studer RA, Dessailly BH, Orengo CA (2013) Residue mutations and their impact on protein structure\nand function: detecting bene\u0000cial and pathogenic changes. Biochem J 449:581–594. doi:\n10.1042/BJ20121221\n\u0000\u0000. Sutskever I, Vinyals O, Le QV (2014) Sequence to sequence learning with neural networks.\nProceedings of the 27th International Conference on Neural Information Processing Systems -\nVolume 2. MIT Press, Montreal, Canada, pp 3104–3112\n\u00009. The UniProt Consortium (2021) UniProt: the universal protein knowledgebase in 2021. Nucleic Acids\nRes 49:D480–D489. doi: 10.1093/nar/gkaa1100\n90. Wang G, Dunbrack RL Jr (2003) PISCES: a protein sequence culling server. Bioinformatics 19:1589–\n1591. doi: 10.1093/bioinformatics/btg224\n91. Wang Z, Moult J (2001) SNPs, protein structure, and disease. Hum Mutat 17:263–270. doi:\nhttps://doi.org/10.1002/humu.22\n92. Weile J, Roth FP (2018) Multiplexed assays of variant effects contribute to a growing genotype–\nphenotype atlas. Hum Genet 137:665–678. doi: 10.1007/s00439-018-1916-x\n93. Weißenow K, Heinzinger M, Rost B (2021) Protein language model embeddings for fast, accurate,\nalignment-free protein structure prediction. bioRxiv: 2021.07.31.454572.. doi:\n10.1101/2021.07.31.454572\n94. Zhou G, Chen M, Ju CJT, Wang Z, Jiang JY, Wang W (2020) Mutation effect estimation on protein-\nprotein interactions using deep contextualized representation learning. NAR Genom Bioinform\n2:lqaa015. doi: 10.1093/nargab/lqaa015\nFigures\nPage 29/35\nFigure 1\nSketch of methods. Panel A sketches the conservation prediction pipeline: (I) embed protein sequence\n(“SEQ”) using a pLM (here: ProtBERT,ProtT5 (Elnaggar et al. 2021) or ESM-1b (Meier et al. 2021)). (II)\nInput embedding into supervised method (here: logistic regression, FNN or CNN) to predict conservation\nin 9-classes as de\u0000ned by ConSurf-DB (Ben Chorin et al. 2020). (III) Map nine-class predictions >5 to\nconserved (C), others to non-conserved (-). Panel B shows the use of binary conservation predictions as\ninput to SAV effect prediction by (I) considering all residue-positions predicted as conserved (C) as effect\n(E), all others as neutral (ProtT5cons-19equal and ConSeq-19equal). (II) Residues predicted as conserved\nare further split into speci\u0000c substitutions (SAVs) predicted to have an effect (E) or not (-) if the\ncorresponding BLOSUM62 score is <0, all others are predicted as neutral (ProtT5-beff, ConSeq-\nBLOSUM62).\nPage 30/35\nFigure 2\npLMs captured conservation without supervised training or MSAs. ProtBert was optimized to reconstruct\ncorrupted input tokens from non-corrupted sequence context (masked language modeling). Here, we\ncorrupted and reconstructed all proteins in the ConSurf10k dataset, one residue at a time. For each\nresidue position, ProtBert returned the probability for observing each of the 20 amino acids at that\nposition. The higher one probability (and the lower the corresponding entropy), the more certain the pLM\npredicts the corresponding amino acid at this position from non-corrupted sequence context. Within the\ndisplayed boxplots, medians are depicted as black horizontal bars; whiskers are drawn at the 1.5\ninterquartile range. The x-axis gives categorical conservation scores (1: highly variable, 9: highly\nconserved) computed by ConSurf-DB (Ben Chorin et al. 2020) from multiple sequence alignments\n(MSAs), the y-axis the probability entropy (Eqn. 13) computed without MSAs. The two were inversely\nproportional with a Spearman’s correlation of -0.374 (Eqn. 11), i.e., the more certain ProtBert’s prediction,\nthe lower the entropy and the higher the conservation for a certain residue position. Apparently, ProtBert\nhad extracted information correlated with residue conservation during pre-training without having ever\nseen MSAs or any labeled data.\nPage 31/35\nFigure 3\nConservation predicted accurately from embeddings. Data: hold-out test set of ConSurf10k (519\nsequences); panel A: nine-state per-residue accuracy (Q9, Eqn. 9) in predicting conservation as de\u0000ned by\nConSurf-DB (Ben Chorin et al. 2020); panel B: two-state per-residue accuracy (Q2, Eqn. 1; conservation\nscore>5: conserved, non-conserved otherwise). Supervised models (trained on ConSurf10k): LR: logistic\nregression (9,000=9k free parameters), FNN: feed-forward network (33k parameters), and CNN:\nconvolutional neural network (231k parameters with 0.25 dropout rate); methods: ConSeq: computation\nof conservation weight through multiple sequence alignments (MSAs) (Berezin et al. 2004); Random:\nrandom label swap. Model inputs were differentiated by color (green: ESM-1b embeddings (Rives et al.\n2021), red: ProtBert embeddings (Elnaggar et al. 2021), blue: ProtT5 embeddings (Elnaggar et al. 2021),\ngray: MSAs (MMseqs2 (Steinegger and Söding 2017) and PSI-BLAST (Altschul et al. 1997)). Black\nwhiskers mark the 95% con\u0000dence interval (±1.96 SD; Eqn. 12). ESM-1b and ProtT5 embeddings\noutperformed those from ProtBERT (Elnaggar et al. 2021); differences between ESM-1b and ProtT5 were\nnot statistically signi\u0000cant but ProtT5 consistently outperformed ESM-1b in all metrics but Q2 (Table S1).\nESM-1b and ProtT5 as input to the CNN came closest to ConSeq (Table S1).\nPage 32/35\nFigure 4\nEmbedding-based binary SAV effect prediction seemingly competitive. Data: PMD4k (red bars; 4k proteins\nfrom PMD (Kawabata et al. 1999)); DMS4 (blue bars) \u0000rst four human proteins (BRAC1, PTEN, TPMT,\nPPARG) with comprehensive experimental DMS measurements including synonyms (here 95% threshold)\n(Reeb et al. 2020). Methods: SUPERVISED: a) SNAP2bin: effect SNAP2 score>-0.05, otherwise neutral; b)\nVESPA: effect VESPA score>=0.5, otherwise neutral; c) VESPAl: effect VESPAl score>=0.5, otherwise\nneutral. RULE-BASED: d) BLOSUM62bin: irrespective of residue-position, negative BLOSUM62 scores\npredicted as effect, others as neutral; e) ProtT5cons|ConSeq 19equal: all 19 non-native SAVs predicted\nequally: effect if ProtT5cons|ConSeq predicted residue position to be conserved, otherwise neutral; f)\nProtT5beff|ConSeq blosum62: effect if ProtT5cons|ConSeq predicts conserved and BLOSUM62 negative,\notherwise neutral. BASELINE: g) Random: random shu\u0000e of experimental labels. All values for DMS4\ncomputed for binary (effect/neutral) mapping of experimental DMS values with panel A giving the two-\nstate per-residue accuracy (Q2, Eqn. 1) and panel B giving the Matthews Correlation Coe\u0000cient (MCC,\nEqn. 8). Error bars: Black bars mark the 95% con\u0000dence interval (±1.96 SD, Eqn. 12). For all methods, the\nMCC differences between the two data sets PMD4k and DMS4 were statistically signi\u0000cant (exception:\nrandom).\nPage 33/35\nFigure 5\nNo SAV effect prediction consistently best on DMS data. Data: DMS39 (39 DMS experiments gathered for\nthe development of DeepSequence (Riesselman et al. 2018)); experiments sorted by the maximum\nabsolute Spearman coe\u0000cient for each experiment. Methods: a) DeepSequence trained an unsupervised\nmodel for each DMS experiment using only MSA-input, i.e., no effect score labels were used (Riesselman\net al. 2018); b) GEMME inferred evolutionary trees and conserved sites from MSAs to predict effects\n(Laine et al. 2019); c) ESM-1v correlated log-odds of substitution probabilities (Methods) with SAV effect\nmagnitudes (Meier et al. 2021); d) VESPA (this work) trained a logistic regression ensemble on binary SAV\nclassi\u0000cation (effect/neutral) using predicted conservation (ProtT5cons), BLOSUM62 (Henikoff and\nHenikoff 1992), and log-odds of substitution probabilities from ProtT5 (Elnaggar et al. 2021) as input\n(without any optimization on DMS data). The values for the absolute Spearman correlation (Eqn. 11) are\nshown for each method and experiment. The rightmost column shows the mean absolute Spearman\ncorrelation for each method. Although some experiments correlated much better (toward left) with\npredictions than others (toward right), the spread between prediction methods appeared high for both\nextremes; DeepSequence was the only method reaching a correlation of 0 for one experiment; another\nPage 34/35\none and three experiments were predicted with correlations below 0.2 for ESM-1v and DeepSequence\nrespectively, while the vast number of the 4x39 predictions reached correlations above 0.4.\nFigure 6\nSpearman correlation between prediction and DMS experiment varied. Data and Methods as for Fig. 5\nwith addition of: VESPAl: fast version of VESPA with input limited to ProtT5cons and BLOSUM62; ProtT5-\nlogodds: raw log-odds from ProtT5 embeddings (Elnaggar et al. 2021); and raw BLOSUM62 substitution\nscores (Henikoff and Henikoff 1992). Panel A: mean absolute Spearman correlation coe\u0000cient (Eqn. 11)\nfor each method over all 39 DMS experiments; error bars highlight 0.95 con\u0000dence interval (1.96 standard\nerrors). Ignoring statistical signi\u0000cance, the numerical ranking would be: GEMME, VESPA, DeepSequence,\nESM-1v, VESPAl, ProtT5-logodds, BLOSUM62. However, the \u0000rst four did not differ by any statistical\nsigni\u0000cance, and while those ranked 5 and 6 differed from the best four, 5 was close to 4, and 6 close to\n5; only BLOSUM62, the raw substitution scores compiled as background were clearly worst. Panel B:\nboxplots on absolute Spearman correlation coe\u0000cients (Eqn. 11) for each method over the 39 DMS\nexperiments. The medians are depicted as black horizontal bars; whiskers are drawn at the 1.5\ninterquartile range.\nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nPage 35/35\nDMS39datasetscorrelation.xlsx\nDMS39predictions.xlsx\nHUGED2100346SOMRev2.pdf",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.47411826252937317
    },
    {
      "name": "Computational biology",
      "score": 0.3714532256126404
    },
    {
      "name": "Natural language processing",
      "score": 0.358743280172348
    },
    {
      "name": "Biology",
      "score": 0.25717872381210327
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I62916508",
      "name": "Technical University of Munich",
      "country": "DE"
    }
  ]
}