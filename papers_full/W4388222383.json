{
  "title": "Supervised Learning and Large Language Model Benchmarks on Mental Health Datasets: Cognitive Distortions and Suicidal Risks in Chinese Social Media",
  "url": "https://openalex.org/W4388222383",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2104650034",
      "name": "Hongzhi Qi",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2095722350",
      "name": "Qing Zhao",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2122459095",
      "name": "Jianqiang Li",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2123639189",
      "name": "Changwei Song",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1995163484",
      "name": "Wei Zhai",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2121293349",
      "name": "Luo Dan",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2100363133",
      "name": "Shuo Liu",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A4380556227",
      "name": "Yi-Jing Yu",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2080195193",
      "name": "Fan Wang",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2263872999",
      "name": "Huijing Zou",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2756905266",
      "name": "Bing-Xiang Yang",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2129459229",
      "name": "Guang-Hui Fu",
      "affiliations": [
        "Assistance Publique – Hôpitaux de Paris",
        "Centre National de la Recherche Scientifique",
        "Sorbonne Université",
        "Pitié-Salpêtrière Hospital",
        "Inserm",
        "Institut du Cerveau"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2916226351",
    "https://openalex.org/W2924664363",
    "https://openalex.org/W1922396948",
    "https://openalex.org/W2165343833",
    "https://openalex.org/W2889391310",
    "https://openalex.org/W3196324893",
    "https://openalex.org/W3032735579",
    "https://openalex.org/W3153411045",
    "https://openalex.org/W3021738738",
    "https://openalex.org/W6851775633",
    "https://openalex.org/W6854308750",
    "https://openalex.org/W3089862415",
    "https://openalex.org/W3193582868",
    "https://openalex.org/W3139172628",
    "https://openalex.org/W2969739743",
    "https://openalex.org/W3037313435",
    "https://openalex.org/W3094709634",
    "https://openalex.org/W4384920109",
    "https://openalex.org/W6838461927",
    "https://openalex.org/W4319301446",
    "https://openalex.org/W4313447794",
    "https://openalex.org/W6850135255",
    "https://openalex.org/W4318464200",
    "https://openalex.org/W4319455199",
    "https://openalex.org/W4379769651",
    "https://openalex.org/W4384926409",
    "https://openalex.org/W4376122720",
    "https://openalex.org/W4378464713",
    "https://openalex.org/W4386302642",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4324387439",
    "https://openalex.org/W6854692045",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2971092323",
    "https://openalex.org/W6846002521",
    "https://openalex.org/W4226364033",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4387356888",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W4385157257",
    "https://openalex.org/W4322631505",
    "https://openalex.org/W4392137515",
    "https://openalex.org/W4383605161",
    "https://openalex.org/W4324308091",
    "https://openalex.org/W4385373745",
    "https://openalex.org/W4353016766"
  ],
  "abstract": "Abstract In the realm of social media, users frequently convey personal sentiments, with some potentially indicating cognitive distortions or suicidal tendencies. Timely recognition of such signs is pivotal for effective interventions. In response, we introduce two novel annotated datasets from Chinese social media, focused on cognitive distortions and suicidal risk classification. We propose a comprehensive benchmark using both supervised learning and large language models, especially from the GPT series, to evaluate performance on these datasets. To assess the capabilities of the large language models, we employed three strategies: zero-shot, few-shot, and fine-tuning. Furthermore, we deeply explored and analyzed the performance of these large language models from a psychological perspective, shedding light on their strengths and limitations in identifying and understanding complex human emotions. Our evaluations underscore a performance difference between the two approaches, with the models often challenged by subtle category distinctions. While GPT-4 consistently delivered strong results, GPT-3.5 showed marked improvement in suicide risk classification after fine-tuning. This research is groundbreaking in its evaluation of large language models for Chinese social media tasks, accentuating the models' potential in psychological contexts. All datasets and code are made available at: \\url{https://github.com/HongzhiQ/SupervisedVsLLM-EfficacyEval}.",
  "full_text": "Supervised Learning and Large Language Model\nBenchmarks on Mental Health Datasets: Cognitive\nDistortions and Suicidal Risks in Chinese Social\nMedia\nHongzhi Qi \nFaculty of Information Technology, Beijing University of Technology, Beijing, 100124, China\nQing Zhao \nFaculty of Information Technology, Beijing University of Technology, Beijing, 100124, China\nJianqiang Li \nFaculty of Information Technology, Beijing University of Technology, Beijing, 100124, China\nChangwei Song \nFaculty of Information Technology, Beijing University of Technology, Beijing, 100124, China\nWei Zhai \nFaculty of Information Technology, Beijing University of Technology, Beijing, 100124, China\nLuo Dan \nCenter for Wise Information Technology of Mental Health Nursing Research, School of Nursing, Wuhan\nUniversity, Wuhan, China\nShuo Liu \nCenter for Wise Information Technology of Mental Health Nursing Research, School of Nursing, Wuhan\nUniversity, Wuhan, China\nYi Jing Yu \nCenter for Wise Information Technology of Mental Health Nursing Research, School of Nursing, Wuhan\nUniversity, Wuhan, China\nFan Wang \nCenter for Wise Information Technology of Mental Health Nursing Research, School of Nursing, Wuhan\nUniversity, Wuhan, China\nHuijing Zou \nCenter for Wise Information Technology of Mental Health Nursing Research, School of Nursing, Wuhan\nUniversity, Wuhan, China\nBing Xiang Yang \nCenter for Wise Information Technology of Mental Health Nursing Research, School of Nursing, Wuhan\nUniversity, Wuhan, China\nGuanghui Fu  (  guanghui.fu@icm-institute.org )\nSorbonne Université, Institut du Cerveau - Paris Brain Institute - ICM, CNRS, Inria, Inserm, AP-HP , Hôpital\nde la Pitié Salpêtrière, Paris, France\nArticle\nKeywords: Large language model, Deep learning, Natural language processing, Mental health, Social\nmedia\nPosted Date: November 2nd, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3523508/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nSupervised Learning and Large Language Model Benchmarks\non Mental Health Datasets: Cognitive Distortions and\nSuicidal Risks in Chinese Social Media\nHongzhi Qi a, Qing Zhao a, Jianqiang Li a, Changwei Song a, Wei Zhai a, Dan Luo b, Shuo Liu b, Yi\nJing Yub, Fan Wang b, Huijing Zou b, Bing Xiang Yang b, and Guanghui Fu *c\naFaculty of Information Technology, Beijing University of Te chnology, Beijing, 100124, China\nbCenter for Wise Information Technology of Mental Health Nursin g Research, School of\nNursing, Wuhan University, Wuhan, China\ncSorbonne Universit´ e, Institut du Cerveau - Paris Brain Inst itute - ICM, CNRS, Inria, Inserm,\nAP-HP, Hˆ opital de la Piti´ e Salpˆ etri` ere, Paris, France\nABSTRACT\nIn the realm of social media, users frequently convey personal senti ments, with some potentially indicating\ncognitive distortions or suicidal tendencies. Timely recognition of s uch signs is pivotal for eﬀective interven-\ntions. In response, we introduce two novel annotated datasets from Ch inese social media, focused on cog-\nnitive distortions and suicidal risk classiﬁcation. We propose a compr ehensive benchmark using both super-\nvised learning and large language models, especially from the GPT serie s, to evaluate performance on these\ndatasets. To assess the capabilities of the large language models, we empl oyed three strategies: zero-shot, few-\nshot, and ﬁne-tuning. Furthermore, we deeply explored and analyzed t he performance of these large language\nmodels from a psychological perspective, shedding light on their st rengths and limitations in identifying and\nunderstanding complex human emotions. Our evaluations underscore a p erformance diﬀerence between the\ntwo approaches, with the models often challenged by subtle category di stinctions. While GPT-4 consistently\ndelivered strong results, GPT-3.5 showed marked improvement i n suicide risk classiﬁcation after ﬁne-tuning.\nThis research is groundbreaking in its evaluation of large language models f or Chinese social media tasks,\naccentuating the models’ potential in psychological contexts. All datas ets and code are made available at:\nhttps://github.com/HongzhiQ/SupervisedVsLLM-EfficacyEval.\nKeywords: Large language model, Deep learning, Natural language processing, Mental health , Social media\n1. INTRODUCTION\nThe omnipresent specter of mental illness, particularly depress ion, continues to impose signiﬁcant challenges\nglobally.1 According to the World Health Organization (WHO), an estimated 3.8% of the global population\nexperiences depression. 1 Speciﬁcally in China, the prevalence of depression is notably high, w ith estimates\naround 6.9%, 2 underscoring the escalating mental health concerns in the nation. Su ch severe depression can\noften precipitate suicidal behaviors. 3 As digital avenues for communication ﬂourish, social media platforms li ke\nTwitter and Sina Weibo have evolved into reﬂective mirrors, oﬀer ing glimpses into the emotional landscapes of\ncountless users. 4 Within these platforms, a speciﬁc subset of topics recurrently s urfaces, with users frequently\nconveying deep-seated negative emotions and, alarmingly, pronounced su icidal inclinations. 5, 6\nArtiﬁcial intelligence (AI), especially the branches of deep learni ng and natural language processing technique,\nis an avenue that holds promise in addressing this challenge. 7 Over recent years, AI research has resulted in\nthe formulation of several algorithms tailored for emotion recognition withi n textual data. 8 However, these\nadvancements are not without obstacles. 9 Constructing a potent deep learning model often demands considerab le\ntime and ﬁnancial resources. The intricacies of data labeling, predomi nantly the need to enlist domain experts\nand the model’s variance in performance when shifted across diﬀeren t application areas, highlight pressing\nFurther author information: (Send correspondence to Guanghui Fu , guanghui.fu@inria.fr)\n1\nchallenges.10 This highlights a compelling need for more agile and adaptable algorithmic s olutions especially\nin medical domain. 11 It is in this context that the emergence and proliferation of large language m odels are\nparticularly noteworthy.\nLarge language models, characterized by their expansive parameters and th e depth of their training datasets,\nstand as the state-of-the-art in the framework of computational linguisti cs.12 Their potential lies in their ability to\ncomprehend and emulate human-like text nuances. Despite their pr omising potential, several studies have sought\nto validate their practical implications. For instance, Xu et al. 13 examined four public datasets related to online\nsocial media sentiment detection. However, their study focuse d solely on English data, and the classiﬁcation\ngranularity was relatively broad. To date, there is a notable gap in researc h concerning the Chinese context,\nparticularly in the area of ﬁne-grained emotion recognition, which is oft en of greater signiﬁcance. The lack of\ncomprehensive evaluations and practical tests has inadvertently led to a cautious approach, especially in sectors\ndemanding high reliability, like medicine and healthcare. 14\nMotivated by the need to better understand mental health sentimen ts on Chinese social media platforms,\nour research embarks on a rigorous evaluation of supervised learning and lar ge language models. We oﬀer the\nfollowing contributions:\n• We introduce and publicly release two new expertly-manual annotate d social media datasets in the mental\nhealth domain, speciﬁcally focusing on cognitive distortion and suici de risk classiﬁcation. These datasets\nnot only serve as valuable resources for the community but also have prof ound real-world implications,\npotentially informing strategies for suicide prevention and interv entions for cognitive distortions.\n• We propose a comprehensive benchmark using both traditional supervi sed learning and large language\nmodels on these datasets. By employing a variety of strategies, incl uding zero-shot, few-shot, and ﬁne-\ntuning, we seek to determine the most eﬀective methods for lever aging these models in the context of\nmental health tasks on Chinese social media.\n• Lastly, our study pioneers the exploration of ﬁne-tuning capabilities of GPT-3.5, leveraging real-world data.\nThis endeavor seeks to determine the adaptability and specialized performance enhancements possible with\nthe model currently unexplored in the literature.\n2. RELATED WORK\nThe intertwining of artiﬁcial intelligence (AI) with diﬀerent ﬁe lds has spurred innovations and transformations\nat an unprecedented scale. An example of this is the fusion of natural langu age processing (NLP) tools, notably\ndeep learning based model, with domains as critical as the mental healt h ﬁeld. 15 Additionally, as digital inter-\nactions burgeon, especially on social media, the urgency to understand and analyze human sentiments becomes\nparamount. In this section, we will introduce deep learning techn iques for sentiment analysis utilizing text data\n(Section 2.1). Subsequently, we will discuss the evolution, potential, and cu rrent research on large language\nmodels in this domain (Section 2.2).\n2.1 Text sentiment analysis\nIn the swiftly evolving digital era, social networking platforms hav e emerged as pivotal channels for expressing\nemotions globally. These platforms generate vast amounts of unstructured data every second. Accurately and\npromptly discerning the emotions embedded within this data pres ents a formidable challenge to computational\nalgorithms.\n8 Fu et al. 16 presented a distant supervision method designed to build syste ms that classify high\nand low suicide risk levels using Chinese social media data. This approach minimizes the need for human\nexperts of varying expertise levels to perform annotations. By inte grating this model with crucial psychological\nfeatures extracted from user blogs, they attained an F1 score of 77.98%. Singh et al. 17 employed a BERT-\nbased model for sentiment analysis on tweets sourced globally and another dataset speciﬁcally from India, both\nfocusing on the topic of COVID-19. They reported achieving an accuracy of 94% . Wan 18 introduced a method for\nsentiment analysis of comments on Weibo platforms, leveraging deep ne ural networks. The data undergoes feature\nextraction through multilevel pooling and convolution layers. Commen ts are preprocessed and transformed into\n2\ntext representations using the word2vec algorithm. Subsequently, key features are extracted from the feature\nmatrix using a CNN. For the ﬁnal classiﬁcation and sentiment analysis, th e softmax logistic regression method\nis employed. Zhang et al. 19 explored the correlations among emotion labels, social interactions, and temporal\npatterns within an annotated Twitter dataset. They introduced a fact or graph-based emotion recognition model\nthat seamlessly integrates these correlations into a uniﬁed framewor k. This model adeptly identiﬁes multiple\nemotions by applying a multi-label learning approach to Twitter datase ts. Wang et al. 20 introduced a topic\nmodeling technique, termed LDA, to examine the primary concerns e xpressed on Weibo during the COVID-19\npandemic. They assessed the emotional inclinations of these topics, d etermined their proportional distributions,\nand conducted user behavior analysis based on metrics such as likes, c omments, and retweets. Furthermore, they\nexplored shifts in user concerns and variations in engagement among resi dents from diﬀerent regions of mainland\nChina. Such insights guide public sentiment and actions during heal th emergencies, emphasizing the importance\nof vigilant social media monitoring.\nAlthough deep learning algorithms typically demonstrate impressive r esults, they often require a signiﬁcant\nvolume of labeled data to perform optimally. The distant supervision ap proach highlighted in Fu et al.’s research 16\naims to reduce the need for labeling, but it still requires the inv olvement of three diﬀerent expert groups at\nvarious expertise levels to yield desired results. Nonetheless , when applying these models to new datasets or\ntasks, domain adaptation issues often arise. These trained models can se e a decline in their eﬃcacy, making\ndeep learning algorithms both costly and inﬂexible. Given these hur dles, there’s a growing demand for eﬃcient\nand user-centric methods to assist individuals in emotion detect ion on social media platforms. The recent\nadvancements in large language models present a potential solution to thi s challenge, but their precise impact\nstill warrants examination from multiple perspectives and special ists.\n2.2 Large language model and its applications in medical domain\nThe advent of Large Language Models (LLMs), such as OpenAI’s ChatGPT,\n12 has revolutionized the ﬁeld of\nnatural language processing. 21 These LLMs demonstrate emergent abilities that signiﬁcantly outperfor m those\nof their smaller, pre-trained models. 22 Initially conceived for understanding and generating human-like t ext,\nLLMs have found diverse applications ranging from content generation, 23 medical report assistant, 24 coding\nassistance,25 education,26 and answering medical related questions. 27 The sheer scale of these models enables\nthem to generate complex, contextually relevant content. LLMs have garn ered signiﬁcant attention in medical\ndomain.14 For instance, Jiang et al. 28 developed a clinical LLM named NYUTron to assist physicians and health-\ncare administrators in making time-sensitive decisions. This mod el can process on unstructured clinical notes\nfrom electronic health record. And it can achieve good performance with AUC score ranging from 78.7–94.9%.\nThe model has been successfully deployed in a prospective trial , indicating its potential for real-world application\nin providing point-of-care guidance to physicians.\nConcurrently, research in psychology-related domains has also been con ducted by other researcher. 29 Qin\net al. 30 devised an interpretable and interactive depression detection sy stem employing large language models\n(LLMs). This innovative approach allows for the detection of mental healt h indicators through social media\nactivity and encourages users to interact with the system using natu ral language. While this facilitates a more\npersonalized understanding of an individual’s mental state, it also r aises ethical concerns. The absence of human\noversight could lead to biased outcomes, thereby posing potential ris ks to users. Additionally, if this system\nwere to become a foundational diagnostic tool for future psychological couns eling, issues related to user privacy\ncould become a point of concern. Chen et al. 31 developed a tool designed to improve the realism of psychiatrist-\npatient simulations using ChatGPT-based chatbots. Their approach inv olved using distinct prompts to enable\nlarge language models (LLMs) to emulate the roles of both a depressed patien t and a psychiatrist. The study\nconﬁrmed the feasibility of utilizing ChatGPT-driven chatbots in psychiatric contexts. However, the research\nalso acknowledged limitations: individual patients and counselors ha ve unique communication styles, and some\npatients may be reluctant to engage in conversation. These nuances pre sent a challenge for achieving truly realistic\nsimulations with ChatGPT. Addressing the simulation of diverse per sonalities in a meaningful way remains a\nkey area for further investigation. Fu et al. 32 developed a counseling support system designed to augment\nthe capabilities of non-professional counselors. The system provide s multiple features, including mental health\nanalysis, evaluation of therapist responses, and suggested interventi ons. This application serves as a valuable use\ncase for language models in the mental health sector. Ten professional psy chologists assessed the system on ﬁve\n3\ncritical dimensions, and the ﬁndings were favorable, with a 78% exper t approval rate indicating that the system\ncan deliver eﬀective treatment strategies. Ayers et al. 33 developed a ChatGPT-based chatbot and compared its\nresponses with those of physicians to patient inquiries on a social m edia forum. Notably, 78.6% of the evaluators\npreferred the chatbot’s responses, citing their speed and greater empathetic tone. However, a key limitation\nof this study lies in its exclusive focus on interactions within on line forums. Such settings may not accurately\nreﬂect the nuances of real-world patient-physician dialogues, as phys icians often tailor their responses based on\npre-existing relationships and the context of a clinical setting. I n summary, there is active research into the\nutilization of LLMs in the ﬁeld of psychology, and these research demonstr ate considerable potential. However,\ndelineating the limitations of LLMs remains a crucial issue that warrant s further investigation. Additional studies\nare needed to comprehensively evaluate the capabilities and boundari es of LLMs in psychological applications.\nXu et al. 13 present a pioneering evaluation of multiple Large Language Models (LLMs) acros s various\nmental health prediction tasks using four publicly available online text datasets. Their insights oﬀer guidance to\npractitioners on optimizing the use of LLMs for speciﬁc applications. Wh ile their research stands as a monumental\nveriﬁcation of LLMs’ potential in the mental health domain, it is noteworth y that their datasets are exclusively\nin English and do not address multi-label classiﬁcation tasks. Yang et al .34 assessed ChatGPT’s capabilities\nin mental health analysis and emotional reasoning by evaluating its perfor mance on 11 datasets across ﬁve\ntasks. The study also investigated the impact of diﬀerent emotion-bas ed prompting strategies. Experimental\nresults indicate that while ChatGPT surpasses traditional neural ne twork-based approaches, it still lags behind\nmore advanced, task-speciﬁc methods. Nevertheless, ChatGPT demon strates signiﬁcant potential in the area of\nexplainable mental health analysis. In conclusion, while the integrat ion of LLMs in medicine presents compelling\nprospects, there’s an imperative to ensure privacy and uphold ethi cal standards. Responses generated may\nnot always be ﬂawless. 35 Particularly in mental health, relying solely on LLM-driven system s for diagnosis or\nsupport introduces numerous unpredictable variables. It’s cruc ial to recognize that LLMs warrant meticulous\nscrutiny and validation. 36 Evaluation should be considered an essential discipline to facilitat e the more eﬀective\ndevelopment of large language models (LLMs). 37\n3. METHODS\nWe conducted experiments to classify suicide risk and cognitive d istortions on Chinese social media data using\nsupervised learning methods and large language models (LLMs). Within t he framework of supervised learning,\nwe explored two models BERT 38 and LSAN 39 as baseline, detailed in Section 3.1. For the large language\nmodels, we utilized zero-shot prompt, few-shot prompt, and ﬁne-tu ning methods. Subsequent sections provide a\ncomprehensive introduction of these methods.\n3.1 Baseline supervised learning model\nWe experimented with two representative models: LSAN\n39 and BERT. 38 LSAN is adept at uncovering the\nrelationships between labels, making it particularly suitable for ou r cognitive distortion recognition task. On the\nother hand, BERT represents a groundbreaking pre-trained model arc hitecture that had achieved state-of-the-art\n(SOTA) on 11 distinct NLP tasks. We discuss each in detail below:\n• LSAN: The LSAN model is engineered to utilize label semantics for ident ifying the relationships between\nlabels and documents, thereby creating a label-speciﬁc documen t representation. The model also employs\na self-attention mechanism to focus on this representation, which is derived from the document’s content.\nAn adaptive fusion strategy integrates these components eﬀectively, f acilitating the generation of a compre-\nhensive document representation suitable for multi-label text c lassiﬁcation. The LSAN model has proven\neﬀective, particularly in predicting low-frequency labels.\n• BERT: Bidirectional Encoder Representations from Transformers (B ERT) has been a pivotal development\nin natural language processing (NLP). Unlike traditional NLP models that pro cess text unidirectionally,\nBERT uses a bidirectional approach, facilitated by the Transformer arc hitecture, to understand the full\ncontext of each word. It is pre-trained using a masked language model obje ctive, where random words\nare replaced with a ‘[MASK]’ token and the model predicts the original word. This design has enabled\nBERT to set new performance standards in diverse NLP tasks, such as que stion-answering and sentiment\nanalysis, especially when ﬁne-tuned on speciﬁc task data.\n4\n3.2 Large language models\nGiven that our data is in Chinese, we explored the open-source model s ChatGLM2-6B and GLM-130B,\n40 both of\nwhich support Chinese language processing. The primary distinction between these two models lies in the number\nof parameters they possess. GPT-3.5 41 stands as a ﬂagship large-scale language model. We experimented with\nvarious prompt word constructions and sought to integrate prior knowledge from the psychological domain, along\nwith the most recent public ﬁne-tuning functionalities. GPT-4, 42 being the latest iteration, was also included in\nour assessment. Detailed introduction on these models are provide d in the subsequent sections.\n• ChatGLM2-6B: ChatGLM2-6B is an open-source bilingual language model with 6.2 billion parame ters,\noptimized for Chinese question-answering and dialogue. It employs s imilar technology to ChatGPT and is\ntrained on roughly 1TB of Chinese and English text data. The model can be ﬁ ne-tuned through various\ntechniques like supervised learning and human feedback. It also f eatures an eﬃcient tuning method based\non P-Tuning v2, requiring at least 7GB of GPU memory for customization. Du e to quantization techniques,\nit can run on consumer-grade graphics cards with only 6GB of memory.\n• GLM-130B: GLM-130B is a bilingual pre-trained language model optimized for both Englis h and Chinese,\nboasting a substantial 130 billion parameters. This model aims to provi de an open-source alternative of a\nscale comparable to GPT-3, while shedding light on the complexities of training such large-scale models.\nImpressively, GLM-130B surpasses GPT-3 175B on multiple English bench marks and outperforms ERNIE\nTITAN 3.0 260B, 43 the largest existing Chinese language model, on relevant benchmarks. A distinctive\nfeature of GLM-130B is its capability for INT4 quantization without substant ial performance degradation,\nthus facilitating eﬃcient inference on widely available GPUs.\n• GPT-3.5: GPT-3.5 is a cutting-edge language model developed by OpenAI, designed to oﬀer enhanced\nconversational capabilities. Building on the foundation of its predece ssor, GPT-3, this iteration introduces\nimprovements in both performance and cost-eﬃciency. OpenAI’s comm itment to reﬁning and advancing\nthe capabilities of their models is evident in GPT-3.5, which prov ides users with a more coherent, context-\naware, and responsive conversational experience. As part of OpenAI’s mi ssion to ensure that artiﬁcial\ngeneral intelligence beneﬁts all of humanity, GPT-3.5 is a testament to the organization’s dedication to\ninnovation and excellence in the realm of natural language processing.\n• GPT 4: GPT-4 is a groundbreaking multimodal model capable of processing bot h image and text inputs\nto generate text-based outputs. Marking a signiﬁcant advancement ove r its predecessors, GPT-4 exhibits\nhuman-level performance across a range of professional and academic bench marks, including a top 10% score\non a simulated bar exam. Built upon the Transformer architecture, the model is initially trained to predict\nsubsequent tokens in a given sequence and later undergoes a post-tr aining alignment process to improve\nits factuality and behavior. A critical component of the project invol ved the development of scalable\ninfrastructure and optimization techniques that function consiste ntly across various sizes, allowing the\nteam to extrapolate GPT-4’s performance metrics based on smaller model s. Despite its notable capabilities,\nGPT-4 does inherit certain limitations from earlier versions, suc h as occasional content ”hallucinations”\nand a constrained context window.\nThe large language model is widely recognized as being pre-trained on vast amounts of text data. However,\nthe manner in which prompt are inputted is crucial, as it directly i nﬂuences the LLM’s comprehension and\noutput for a given task. In light of this, we have formulated the followi ng prompts.\nLLM Zero-shot Prompting We initiate our exploration with prompt design tailored for tasks withi n a\nzero-shot paradigm. This process encompasses various strategies, incl uding direct task requests (acting as the\nbasic), role-deﬁnition, scene-deﬁnition, and hybrid approaches. F or illustrative purposes, the cognitive distortion\nclassiﬁcation task serves as the focal point. The design is elaborated as f ollows:\n1. Basic: A direct task directive devoid of speciﬁc contextual emphasis.\n5\n(a) English translation: “Please conduct a multi-classiﬁcation task to as certain if it encompasses any of\nthe speciﬁed 12 cognitive distortions ([list of cognitive distortions ]).”\n(b) Formulaic representation: M(T,12CD), where M stands for multi-classiﬁcation, T symbolizes the\ntask, and 12 CD represents the 12 cognitive distortions.\n2. Role-deﬁnition Prompting: The prompt delineates the role of the respondent (in this case, a psy chol-\nogist) and emphasizes reliance on psychological insights.\n(a) English translation: “Assuming the role of a psychologist and leveraging psychological insights, please\nconduct a multi-classiﬁcation task to discern if it integrates any of t he 12 cognitive distortions ([list\nof cognitive distortions]).”\n(b) Formulaic representation: R(M(T,12CD)), where R embodies the role-deﬁnition of being a psychol-\nogist.\n3. Scene-deﬁnition Prompting: The context of a social media setting is introduced, highlighting user\nidentiﬁers to preclude ambiguity.\n(a) English translation: “Considering the provided user ID and the ass ociated posts on social media,\nplease based on the post content, engage in a multi-classiﬁcation task to d etermine the presence of\nany of the 12 cognitive distortions ([list of cognitive distortions]).”\n(b) Formulaic representation: S(M(T,12CD)), with S denoting the scene, which in this scenario, pertains\nto the user’s ID and corresponding social media posts.\n4. Hybrid Prompting: A synthesis of both role and scene deﬁnitions, oﬀering an integrative i nstruction.\n(a) English translation: “With the given user ID and their respective social media posts, and adopting\nthe role of a psychologist fortiﬁed with psychological expertise, pleas e execute a multi-classiﬁcation\ntask to verify the inclusion of any of the 12 cognitive distortions ([lis t of cognitive distortions]).”\n(b) Formulaic representation: S + R(M(T,12CD)), intertwining the scene context ( S) with the role-\ndeﬁnition ( R).\nLLM Few-shot Prompting In this segment, few-shot prompting is construed as the provision of prior knowl-\nedge or a batch of n training instances to LLMs, thereby enabling them to internalize t his information and adeptly\nexecute the stipulated task. This methodology unfolds as:\n1. Background Knowledge: The model is furnished with psychological deﬁnitions supplement ed by em-\nblematic cases, followed by one of the four prompting strategies devis ed from zero-shot prompting. Prompts\nthat integrate background knowledge and employ the hybrid strategy from zero-shot prompting are detailed\nas follows:\n(a) English translation: “Given the deﬁnitions of cognitive distortions denoted by D and the prototypical\ncases represented by C, and in light of the supplied user ID and associated social media posts , you are\nassumed to be a psychological expert well-versed in the aforemention ed deﬁnitions and cases. Drawing\nfrom this backdrop, please conduct a multi-classiﬁcation task to evalu ate the correlation with any of\nthe 12 cognitive distortions ([list of cognitive distortions]).”\n(b) Formulaic representation: D +C +S +R(M(T,12CD)), where D encapsulates the background deﬁni-\ntion, and C signiﬁes the prototypical instances from academic literature, S represents scene-deﬁnition\nand R stands for role-deﬁnition.\n2. Training with n Samples per Category: In this approach, n training instances are randomly selected\nfor each category to train the LLM, followed by one of the four prompting strat egies designed from zero-shot\nprompting. These instances are represented as trainn in the following tables. Prompts that incorporate\nthe training instances employ the hybrid strategy from zero-shot pr ompting are detailed as follows:\n6\n(a) English translation: “You are provided with learning samples denote d by T .In light of the supplied\nuser ID and associated social media posts, and assuming your role as a psy chologist with the rele-\nvant expertise.Drawing from this backdrop, please conduct a multi -classiﬁcation task to evaluate the\ncorrelation with any of the 12 cognitive distortions ([list of cognitive di stortions]).”\n(b) Formulaic representation: T + S + R(M(T,12CD)), integrating T as the training set with the scene-\ndeﬁnition ( S) and role-deﬁnition ( R).\n3. Background knowledge and training with n samples per category: This approach investigates\nwhether enhancing sample diversity in few-shot prompting augment s the LLM’s comprehension of psycho-\nlogical health tasks. It incorporates psychological deﬁnitions, symbolic examples, and provides n training\ninstances per category for LLM training. A command is subsequently issue d using a previously described\nfew-shot prompting strategy. The following example integrates backgr ound knowledge and training in-\nstances, and poses a query using the hybrid strategy from zero-shot pr ompting:\n(a) English translation: “Given the deﬁnitions of cognitive distortions represented by D and the proto-\ntypical cases denoted by C, you are also provided with learning samples represented by T . Assuming\nyour expertise as a psychological expert familiar with the aforemention ed deﬁnitions and cases, and\nin consideration of the supplied user ID and associated social media p osts, please conduct a multi-\nclassiﬁcation task to evaluate the correlation with any of the 12 cognitive d istortions ([list of cognitive\ndistortions]).”\n(b) Formulaic representation: D + C + T + S + R(M(T,12CD)), where D encapsulates the background\ndeﬁnition, C signiﬁes the prototypical instances from academic literature, T is integrated as the\ntraining set, S denotes the scene-deﬁnition, and R represents the role-deﬁnition.\nLLM Fine-tuning Fine-tuning represents a potent paradigm provided by OpenAI, enab ling users to optimize\nthe performance of pre-trained models, such as GPT-3.5. While GPT-3. 5 is inherently trained on an expansive\ntext corpus, the ﬁne-tuning process sharpens its proﬁciency for specialized tasks by exposing it to additional\ntask-speciﬁc instances. Following the ﬁne-tuning, our evaluation p aradigm retained the role, scene and hybrid\ndeﬁnitions from the zero-shot prompting for consistency and comparativ e assessment:\n1. Role-deﬁnition Prompting: Post ﬁne-tuning with relevant training samples, we employed the prompt\ndelineated in the role-deﬁnition section (refer to Section 3.2).\n2. Scene-deﬁnition Prompting: Analogously, after the ﬁne-tuning process, we reverted to the promp t\nillustrated in the scene-deﬁnition segment of the zero-shot prompt ing.\n3. Hybrid Prompting: Similarly, after the ﬁne-tuning process, we adopted the prompt pr esented in the\nhybrid strategy segment of the zero-shot prompting.\n4. EXPERIMENTS AND RESULTS\n4.1 Datasets and Evaluation Metrics\nWe undertook two psychology-related classiﬁcation tasks: suicide ri sk and cognitive distortion. The suicide risk\ntask primarily diﬀerentiates between high and low suicide risks, while the cognitive distortion task focuses on\nclassiﬁcations deﬁned by Burns.\n44 We sourced our data by crawling comments from the “Zoufan” blog within\nthe Weibo social platform. Subsequently, a team of qualiﬁed psychol ogists were enlisted to annotate the data.\nGiven that this data is publicly accessible, there are no concerns related to privacy breaches.\nFor the suicide detection data, there were 648 records with low suici de risk and 601 records with high suicide\nrisk. The dataset for cognitive distortion consists of a total of 910 entries . The classiﬁcation labels employed for\nthis data are as follows: all-or-nothing thinking, over-generalization , mental ﬁlter, disqualifying the positive, mind\nreading, the fortune teller error, magniﬁcation, emotional reasoning, shou ld statements, labeling and mislabeling,\nblaming oneself and blaming others. For both sets of data, the training se t and test set are divided according\n7\nTable 1. Summary of Experimental Datasets\nTask Ntrain Ntest L L W\ncognitive distortion 728 182 12 1.27 53\nsuicide detection 999 250 1 1 47.79\nto the ratio of 4:1. The statistics of these two datasets are listed in Tab le 1, where Ntrain and Ntest denote the\nnumber of training and test samples, respectively. L is the total number of classes, L is the average number of\nlabels per sample, and W is the average number of words per sample. We utilize three evaluation metrics to\nmeasure the performance of diﬀerent algorithms for our two tasks: preci sion, recall, and F1 score. Precision is\nthe ratio of correctly predicted positive observations to the total pr edicted positives and recall (or sensitivity)\nrepresents the ratio of correctly predicted positive observations to all the actual positives. These two metrics\nprovide a comprehensive view of the algorithm’s performance in term s of its positive predictions. The F1 score\noﬀers a more holistic view of the model’s performance, especially wh en the distribution of the true positive and\ntrue negative rates is uneven.\n4.2 Experiment design\nOur experimental methodology is both hierarchical and greedy. Using cogn itive distortions as an example to\nshow our points, our evaluations spanned several dimensions:\n• Prompt Design Perspective: Initially, we assessed four prompting strategies within the zero-shot learning\nframework. Subsequently, based on their performance metrics, the top two strategies were selected for\nfurther evaluation in the few-shot learning setting across various LLMs .\n• LLM Performance Perspective: Across all zero-shot prompts, ChatGLM2-6B’s performance was found to\nbe lacking, resulting in our decision to omit it from subsequent few -shot prompting experiments. For\nGPT-3.5, its token limitation prevented us from entering ﬁve sample s for each category during few-shot\nprompting. Consequently, we reserved the train5 approach exclusively for GPT-4.\n• Fine-tuning Perspective: A discernible performance gap exists between GPT-3.5 and GPT-4. However,\nOpenAI’s recent introduction of ﬁne-tuning capabilities for GPT- 3.5 and reports from oﬃcial channels\nsuggest that, under speciﬁc conditions, GPT-3.5 might outperform GPT -4 post ﬁne-tuning. Consequently,\nour attention was centered on the ﬁne-tuning of GPT-3.5. Regrettably, t he current iteration of GPT-4\nlacks ﬁne-tuning functionalities, curtailing our capacity to asses s its potential in this dimension.\nThe detailed experimental setup is as follows:\n• LSAN: We used word2vec to train 300-dimensional embeddings for both document an d randomly-initialized\nlabel texts. The attention mechanism helped us compute word contrib utions to labels and create label-\nspeciﬁc document representations. Dot products between these document and label vectors reﬁned these\nrelationships further. These two types of document representat ions were then fused using weighted combi-\nnations. For predictions, we employed a fully connected layer, fol lowed by RELU and a sigmoid function.\nLosses were calculated using a cross-entropy function during traini ng.\n• BERT: We employ BERT to extract 768-dimensional vectors from Chinese sente nces. To mitigate over-\nﬁtting, a dropout function is applied to these sentence vectors. Su bsequently, a fully connected layer is\nintroduced to independently classify suicide risk and cognitiv e distortions. The sigmoid function serves as\nthe activation function for the output layer. Both the BERT layer and t he fully connected layer are trained\nsimultaneously.\n• LLM-zero shot: Both GPT-3.5 and GPT-4 are closed-source and available through API provid ed by\nOpenAI. We picked the gpt-3.5-turbo, one of the most capable and cost-eﬀec tive models in the GPT-3.5\nfamily, and the gpt-4, more capable than any GPT-3.5 model, able to do more c omplex tasks, and optimized\nfor chat. As for the GLM models, we employed the smaller, open-source v ariant, ChatGLM2-6B, suitable\n8\nfor deployment on consumer-grade hardware. Given the extensive param eter count of GLM-130B, it posed\ndeployment challenges due to its elevated operational costs. Furthe rmore, its API lacked the capability to\nhandle cognitive distortion multi-label classiﬁcation task, leading u s to conduct tests via its oﬃcial website.\nAcknowledging the inherent variability in LLM’s outputs, our experi mental design involved averaging the\noutcomes over ﬁve runs. For GPT-3.5, GPT-4, and ChatGLM2-6B, we adjusted t he temperature to values\nof 0.1, 0.3, 0.5, 0.7, and 0.9, conducting experiments at each setting. Given th e absence of a temperature\nsetting for GLM-130B on its platform, we simply executed ﬁve repeated r uns and computed the mean\nperformance. For zero-shot evaluations, we initiated performance valid ation on the basic strategy across\nthe LLMs, subsequently examining the eﬃcacy of role-deﬁnition, sce ne-deﬁnition, and hybrid strategies,\naiming to discern the inﬂuence of domain-speciﬁc information on LLM’s p erformance.\n• LLM-few shot: We conducted an assessment using the top two performing prompt strat egies from the\nzero-shot tests, determined by their F1-scores. The impact on perf ormance was assessed when augmenting\nthese strategies with background, trainn, and their combination (background + trainn). Speciﬁcally,\nbackground strategy denotes the incorporation of prior knowledge, trainn represents the addition of training\nsamples, where n is the number of positive samples chosen for each category. background + trainn\nsuggests simultaneous enrichment with prior knowledge and training samples. Given the varying token\ninput constraints among diﬀerent models, the sample size selecte d for each model diﬀered. In addition,\nwe also experimented with the integration of basic, role, scene, and h ybrid strategies in the zero-shot\nprompting scenario.\n• LLM-ﬁne-tunning: We ﬁne-tuned the GPT-3.5 Turbo model for predicting suicide ri sk and cognitive\ndistortions using the API interface provided by OpenAI. We utiliz ed three types of prompts: role-based,\nscene-based, and hybrid strategies.\n5. RESULTS\nIn our study, we focused on two speciﬁc tasks: suicide classiﬁcati on and multi-label classiﬁcation of cognitive\ndistortions. And the results can be seen in Table 2 and Table 3 respectively. Our analysis examined these two\ntasks in Section 5.1 and Section 5.2 respectively from three distinct aspects: training strategy, th e construction of\nprompt, and a comparative evaluation across various LLMs. Ultimately, we asses sed and compared the model’s\nperformance on these two psychological tasks to draw conclusions in Sec tion 5.3. Considering the intricate\nnature and distinctiveness of the cognitive distortion task, LLMs demon strate suboptimal performance. We\nhave included a human evaluation stage conducted by psychology experts regarding the predictions of the large\nmodels in Section 5.4.\n5.1 Suicide Risk\nTraining strategies In our training strategy comparison, we observed varying degrees of eﬀec tiveness across\ndiﬀerent models. The pre-trained BERT model exhibited a perf ormance enhancement over the LSAN model,\nregistering a 2.23% increase in F1-score. In contrast, ﬁne-tuning GPT -3.5 led to a substantial performance gain,\nachieving an F1-score of 78.45%. This represented a notable 11.5% improveme nt in F1-score when compared to\nits base model (ﬁne-tuning hybrid vs. zero-shot hybrid), bringi ng its performance closer to that of supervised\nlearning models.\nDesign of prompts Our investigation into prompt design for large language models revealed nu anced out-\ncomes across diﬀerent strategies and models. In the context of zero-sh ot prompts, we found that while the\nhybrid strategy yielded satisfactory results, the performance diﬀ erences among various types of prompts were\nnot statistically signiﬁcant. Upon enhancing the basic strategy with th ree additional strategies (role-deﬁne,\nscene-deﬁne, and hybrid), the performance diﬀerences in compari son to the basic strategy are illustrated in Ta-\nble\n4. For few-shot prompts, adding more data did not consistently improve performance; this was evident in the\nChatGLM2-6B model where additional data sometimes reduced eﬀectiven ess. Conversely, GPT-4’s performance\nremained stable irrespective of the data size. Notably, the background +trainn+hybrid strategy emerged as the\nmost eﬀective across multiple models.\n9\nTable 2. Result for suicide binary classiﬁcation task\nModel category Model name Type Sub-type Train data Test data Precisi on Recall F1-score\nSupervised learning\nLSAN train from scarch - 999\n250\n74.59% 87.50% 80.53%\nBERT ﬁne-tuning - 999 88.42% 77.78% 82.76%\nLLM\nChatGLM2-6B\nzero-shot\nbasic 0 69.07% 37.10% 48.07%\nrole-deﬁne 0 65.77% 35.81% 46.15%\nscene-deﬁne 0 64.52% 45.16% 53.01%\nhybrid 0 65.68% 46.13% 53.74%\nfew-shot\nbackground+scene-deﬁne 0 58.56% 47.26% 51.45%\nbackground+hybrid 0 60.37% 70.64% 64.41%\ntrain12+scene-deﬁne 24 67.19% 59.52% 63.04%\ntrain12+hybrid 24 64.29% 49.20% 55.56%\nbackground+train12+scene-deﬁne 24 49.74% 56.61% 52.70%\nbackground+train12+hybrid 24 58.91% 73.23% 64.78%\ntrain30+scene-deﬁne 60 57.71% 26.77% 36.14%\ntrain30+hybrid 60 50.60% 24.84% 32.60%\nbackground+train30+scene-deﬁne 60 62.97% 52.90% 57.02%\nbackground+train30+hybrid 60 60.14% 47.10% 51.88%\nGLM-130B\nzero-shot\nbasic 0 54.58% 95.81% 69.52%\nrole-deﬁne 0 55.51% 94.84% 70.02%\nscene-deﬁne 0 55.05% 93.87% 69.40%\nhybrid 0 57.37% 97.42% 72.20%\nfew-shot\nbackground+role-deﬁne 0 56.55% 90.32% 69.55%\nbackground+hybrid 0 56.91% 92.42% 70.43%\ntrain12+role-deﬁne 24 53.18% 83.23% 64.89%\ntrain12+hybrid 24 55.30% 88.39% 68.02%\nbackground+train12+role-deﬁne 24 57.84% 83.38% 68.30%\nbackground+train12+hybrid 24 60.88% 90.00% 72.61%\nGPT-3.5\nzero-shot\nbasic 0 52.00% 88.23% 65.42%\nrole-deﬁne 0 53.31% 96.13% 68.59%\nscene-deﬁne 0 52.16% 89.03% 65.76%\nhybrid 0 52.55% 92.26% 66.95%\nfew-shot\nbackground+role-deﬁne 0 54.90% 88.55% 67.76%\nbackground+hybrid 0 55.27% 89.03% 68.19%\ntrain12+role-deﬁne 24 56.34% 83.39% 67.22%\ntrain12+hybrid 24 57.19% 84.68% 68.27%\nbackground+train12+role-deﬁne 24 59.37% 81.61% 68.71%\nbackground+train12+hybrid 24 58.26% 82.90% 68.41%\nﬁne-tuning\nrole-deﬁne 999 84.76% 71.77% 77.73%\nscene-deﬁne 999 84.11% 72.58% 77.92%\nhybrid 999 84.26% 73.39% 78.45%\nGPT-4\nzero-shot\nbasic 0 57.43% 95.48% 71.72%\nrole-deﬁne 0 57.29% 97.26% 72.10%\nscene-deﬁne 0 58.81% 97.58% 73.39%\nhybrid 0 57.47% 97.42% 72.30%\nfew-shot\nbackground+scene-deﬁne 0 64.91% 73.55% 68.86%\nbackground+hybrid 0 63.24% 84.03% 72.05%\ntrain12+scene-deﬁne 24 60.70% 94.35% 73.87%\ntrain12+hybrid 24 59.77% 84.19% 69.87%\nbackground+train12+scene-deﬁne 24 65.44% 81.77% 72.63%\nbackground+train12+hybrid 24 65.65% 78.87% 71.60%\ntrain30+scene-deﬁne 60 61.11% 92.42% 73.56%\ntrain30+hybrid 60 60.79% 89.03% 72.22%\nbackground+train30+scene-deﬁne 60 63.86% 83.06% 72.16%\nbackground+train30+hybrid 60 70.16% 82.58% 75.81%\nWe also studied the impact of extra training data in few-shot scenari os and observed that using role-deﬁne and\ntrainn+role-deﬁne prompts often led to diminished performance. The role of background knowledge was model-\ndependent; in smaller models like ChatGLM2-6B, incorporating backgrou nd knowledge led to a performance\nincrease from 53.74% to 64.41%. However, this could not be universally veriﬁ ed due to token limitations. Finally,\nour comparison between few-shot and zero-shot prompts showed that few- shot prompts did not signiﬁcantly\noutperform their zero-shot counterparts.\nComparison of LLMs In our comparative analysis of large language models, we observed several tre nds\nthat highlight the complexities of model performance. Generally, GP T-4 outperformed GPT-3.5, and GLM-\n130B excelled over ChatGLM2-6B, suggesting the beneﬁts of larger model arc hitectures and more extensive\ntraining data. Yet, this trend was interrupted when GPT-3.5 under went ﬁne-tuning, outperforming GPT-4 by a\ndiﬀerential of 2.64%.Additionally, GLM-130B demonstrated a performance comp arable to GPT-4 and superior\nto GPT-3.5 for the speciﬁc task under study. These ﬁndings indicat e that while larger models typically oﬀer\nadvantages, ﬁne-tuning and task-speciﬁc capabilities can alter the pe rformance landscape signiﬁcantly.\n5.2 Cognitive Distortion\nTraining strategies Our investigation into training strategies for large language models reve aled nuanced\nperformance outcomes. Initially, the pre-trained BERT model demon strated a 2.83% performance advantage\nover LSAN trained from scratch. However, this diﬀerence was not statis tically signiﬁcant, implying that the\n10\nTable 3. Result for cognitive distortion multi-label classiﬁ cation task.\nModel category Model name Type Sub-type Train data Test data Precisi on Recall F1-score\nSupervised learning LSAN train from scratch - 728\n182\n76.79% 77.95% 76.08%\nBERT ﬁne-tuning - 728 79.85% 80.49% 78.91%\nLLM\nGLM-130B\nzero-shot\nbasic 0 9.56% 57.39% 16.39%\nrole-deﬁne 0 9.00% 65.31% 15.78%\nscene-deﬁne 0 8.75% 60.43% 15.19%\nhybrid 0 9.93% 67.83% 17.31%\nfew-shot\ntrain1+basic 12 7.59% 39.31% 12.69%\ntrain1+hybrid 12 8.18% 44.09% 13.73%\nGPT-3.5\nzero-shot\nbasic 0 10.25% 4.87% 6.49%\nrole-deﬁne 0 12.58% 4.35% 6.18%\nscene-deﬁne 0 9.2% 3.91% 5.34%\nhybrid 0 8.61% 5.39% 6.38%\nfew-shot background+hybrid 0 2.39% 7.91% 11.63%\nbackground+basic 0 24.21% 10.09% 14.06%\ntrain2+hybrid 24 13.32% 10.09% 11.46%\ntrain2+basic 24 12.51% 10.00% 11.10%\nﬁne-tuning\nrole-deﬁne 728 10.80% 8.26% 9.36%\nscene-deﬁne 728 13.71% 10.43% 11.85%\nhybird 728 11.73% 10.00% 10.80%\nGPT-4\nzero-shot\nbasic 0 16.46% 46.09% 24.18%\nrole-deﬁne 0 16.69% 42.09% 23.86%\nscene-deﬁne 0 18.12% 43.22% 25.43%\nhybrid 0 16.26% 38.87% 22.84%\nfew-shot\nbackground+basic 0 21.96% 31.04% 25.54%\nbackground+scene-deﬁne 0 22.89% 34.18% 26.59%\ntrain2+basic 24 31.25% 34.17% 32.47%\ntrain2+scene-deﬁne 24 27.00% 27.74% 27.06%\nbackground+train2+basic 24 24.35% 32.00% 27.63%\nbackground+train2+scene-deﬁne 24 24.39% 28.09% 25.94%\ntrain5+basic 60 25.62% 35.65% 29.57%\ntrain5+scene-deﬁne 60 29.46% 34.61% 31.57%\nobserved discrepancy may not be meaningful. On the other hand, ﬁne-t uning GPT-3.5 surprisingly led to a\ndecrease in performance rather than the anticipated improvement. T his underscores the complexity of model\ntraining and the need for careful consideration when implementing ﬁ ne-tuning strategies.\nTable 4. Performance Diﬀerences in Zero-Shot Enhancement Strategi es Compared to Basic Strategy\nModel Suicide Cognitive Distortion\n∆ ChatGLM2-6B role-deﬁne ↓ -1.92% —\n∆ ChatGLM2-6B scene-deﬁne ↑ +4.94% —\n∆ ChatGLM2-6B hybrid ↑ +5.67% —\n∆ GLM-130B role-deﬁne ↑ +0.5% ↓ -0.61%\n∆ GLM-130B scene-deﬁne ↓ -0.12% ↓ -1.2%\n∆ GLM-130B hybrid ↑ +2.68% ↑ +0.92%\n∆ GPT-3.5 role-deﬁne ↑ +3.17% ↓ -0.31%\n∆ GPT-3.5 scene-deﬁne ↑ +0.34% ↓ -1.15%\n∆ GPT-3.5 hybrid ↑ +1.53% ↓ -0.11%\n∆ GPT-4 role-deﬁne ↑ +0.38% ↓ -0.32%\n∆ GPT-4 scene-deﬁne ↑ +1.67% ↑ +1.25%\n∆ GPT-4 hybrid ↑ +0.58% ↓ -1.34%\nDesign of prompts In the design of prompts for large language models, our study examined the p erformance\nof both zero-shot and few-shot prompts. For zero-shot prompts, we found th at a meticulous design focusing\non scene and role settings is crucial; otherwise, a basic task-oriente d prompt is generally more eﬀective. The\nchanges in performance metrics for various strategies are shown in Table 4. In the realm of few-shot prompts,\nwe observed that prompts providing speciﬁc data points outperforme d those that simply oﬀered background\nknowledge. Interestingly, increasing the training data in thes e prompts did not lead to better performance. A\ncomparative analysis revealed that although few-shot prompts outperforme d zero-shot prompts, they still fell\n11\nshort of fully meeting the task requirements, as evidenced by GPT -4’s F1-score of approximately 30%.\nComparison of LLMs Consistently, larger models like GPT-4 outperformed their smalle r counterparts such as\nGPT-3.5. When it came to the complex tasks in our study, The performanc e of ChatGLM2-6B was insuﬃcient\nfor handling complex tasks, while GLM-130B fared better but was still out done by GPT-4. Given that our\ndataset consists of comments from social networks, the text is generall y concise. As a result, token length did\nnot substantially aﬀect the performance of the models in our tasks. Rath er, the selection of representative data\nfor prompt construction emerged as a more crucial factor than merely incr easing the number of tokens.\n5.3 Cross-Task Comparison\nAs task complexity increased from binary to multi-label classiﬁcation , large language models did not sustain their\nperformance. In contrast, supervised learning models maintained a relatively stable F1-score close to 80% across\nboth types of tasks. This highlights the limitations of large language model s in replacing supervised learning\nfor specialized tasks. While ﬁne-tuning may beneﬁt simpler task s, it does not adequately address the challenges\nposed by complex tasks, calling for further investigation into ﬁne-t uning mechanisms for large language models.\n5.4 Expert evaluation and feedback\nOwing to the subpar classiﬁcation results of cognitive distortions b y LLMs, we engaged in a manual analysis of\nthese classiﬁcation outcomes with the expertise of psychological sch olars, focusing primarily on the most eﬃca-\ncious strategy in GPT-4, the train 2+basic strategy. Based on the analysis, it was observed that, given suﬃci ent\ntextual information, GPT-4 can aptly identify cognitive distortion cat egories. The brevity and directness inher-\nent in social media texts often deprive them of ample contextual inf ormation. However, GPT-4 can introduce\nrelevant conjunctions and modality markers to infer the context (as d elineated in Example 1 of Figure\n1). Yet,\nin certain speciﬁc scenarios, the model does demonstrate errors:\n• Most prominently regarding the categorization of ”the fortune teller err or” instances arise where patients\narticulate negative anticipations and feelings of desolation about their fu ture, provide retrospectives of\ntheir past experiences, or convey apprehensions about potential chall enges in forthcoming life events. Such\narticulations primarily embody the patients’ reﬂections and should not be deemed conclusive. Yet, GPT-4\nhas mistakenly classiﬁed these under the ”the fortune teller error ” category (refer to Example 2 of Figure 1).\n• Additionally, challenges arose in the categorization of ”should statements”. Such statements predominantly\nmanifest in patients’ regrets regarding past events. However, GPT- 4 erroneously categorized patients’\nexpectations about the future as ”should statements” as well (see Examp le 3 of Figure 1).\n• In speciﬁc contexts, GPT-4 mistakenly classiﬁed patients’ negati ve self-assessments as ”blaming oneself”.\nHowever, such classiﬁcations lacked the reasoning that ascribes the r esponsibility for external events to\noneself, leading to misjudgments. The appropriate labels for these i nstances might be ”disqualifying the\npositive” or ”mental ﬁlter” (refer to Example 4 of Figure 1).\n• The model occasionally exhibits ambiguity among the categories of ”over- generalization”, ”all-or-nothing\nthinking”, and ”magniﬁcation”. Instances inherently aligned with Category A are often misclassiﬁed into\nCategory B or Category C (refer to Example 5 of Figure 1).\nOverall, due to the distinct characteristics of social media data, t he task of discerning cognitive distortions\nwithin such data is inherently challenging. Even specialists wit hin the domain of psychology inevitably introduce\ncertain subjectivity when categorizing and discerning cognitive d istortions in social media texts. In certain\ncontexts, the LLMs can be more detailed, occasionally eliminating biase s that may arise during human annotation\n(as illustrated in Example 6 of Figure 1).\n12\n \nExample 1 \nOriginal text: 走饭，我好累，好难过。不管怎么努力争取，都无法得到 \nTranslation: Zoufan, I'm so tired and sad. No matter how hard I try, I just can't get it. \nGround truth: the fortune teller error \nGPT-4: the fortune teller error \nExample 2 \nOriginal text: 饭，今天是5月了，我好像很烦、很迷茫。我现在又觉得世界不过如此我不想走\n这一遭了。我马上高考了，想到父母心真的真的会很痛。拜托、拜托，真的太难坚持了，我想睡\n了。 \nTranslation: Fan, it's already May. Feeling so overwhelmed and los t. The world seems so meh right \nnow. I can't bear the thought of going through this, especially with college entrance exams coming up. \nThinking of my parents just hurts so much. Please, it's really hard to keep going. I just wanna sleep. \nGround truth: mental filter \nGPT-4: mental filter, disqualifying the positive, the fortune teller error, blaming oneself  \nExample 3 \nOriginal text: 我好累，真的好累。考试永远过不了，我不能拿我前途去赌，我好害怕。 \nTranslation: Feeling so drained. Exams never seem to end and I can't gamble with my future. Seriously \nscared. \nGround truth: over-generalization \nGPT-4: the fortune teller error, should statements \nExample 4 \nOriginal text: 我总是在逃避，我好没用。 \nTranslation: Always running away from things. I feel so useless. \nGround truth: labeling and mislabeling \nGPT-4: all-or-nothing thinking, blaming oneself \nExample 5 \nOriginal text: 有一天我在学校里不小心把头给洗了，本来隔天洗一次的，当时很晚了头没吹干\n就断电了。加上最近的压力，我崩溃了。我实在太蠢了，而我最亲的朋友居然还在我发疯的时候\n说：不就是头没吹干吗？干嘛这样？我终于知道人的真面目是什么了，对他人的痛苦不假思索地\n冷嘲热讽，连我都是这样。 \nTranslation: One day at school, I accidentally washed my hair when I usually wash it every other day. \nIt was late, and before my hair could dry, the power went  out. With all the recent stress, I broke down \nthinking how silly I was. And my closest friend? They actually said, \"It's just wet hair, why overreact?\" \nThat's when I saw their true colors. Making light of someone's pain without a second thought. Even for \nme. \nGround truth: magnification, labeling and mislabeling \nGPT-4: all-or-nothing thinking, mind reading, magnification, emotional reasoning, labeling and \nmislabeling, blaming others \nExample 6 \nOriginal text: 感觉自己要被撕裂了，让我难受的东西像一汪水填在心里，没有一个出口。我不\n想活着了，但是我知道我不可能死，因为我死了爱我的人会痛苦。我恨他们的不理解，但我舍不\n得伤害他们，我希望自己得绝症，这样就可以给他们接受我要死去的时间。 \nTranslation: I felt like I was going to be torn apart, and the uncomfortable things filled my heart like a \npool of water, without an outlet. I don’t want to live anymore, but I know I can’t die because the people \nwho love me will suffer when I die. I hate them for not understanding, but I am reluctant to hurt them. I \nhope that I will be terminally ill so that I can give them time to accept that I am going to die. \nGround truth: mental filter, magnification \nGPT-4: mental filter, magnification, emotional reasoning, blaming oneself, blaming others \nFigure 1. Typical examples of true labels versus GPT-4 predicted labels in cognitive distortion.\n13\n6. DISCUSSION\nOur study systematically evaluated the eﬀectiveness of large language m odels (LLMs) across two mental health\nrelated tasks on Chinese social media: suicide risk classiﬁcation and cognitive distortion multi-label classiﬁcation.\nOur results also reveal the nuanced role of prompt design. While the ’ hybrid’ prompt performed well in zero-\nshot settings, the beneﬁts of increasing data in few-shot prompts we re not universally beneﬁcial. For more\nstraightforward tasks, adding background knowledge appeared to help smal ler models (ChatGLM2-6B), but its\nutility diminished in more complex models or tasks. This calls for a more customized approach to prompt\nengineering tailored to the speciﬁc task and the size of the model be ing used. If high-quality data is unavailable\nor prompt design proves challenging, allowing a LLM to directly handle t he task may still yield acceptable\nperformance. Larger language models like GPT-4 and GLM-130B generally outperf orm smaller variants such as\nGPT-3.5 and ChatGLM2-6B. However, it’s important to note that these large m odels are not always competent\nat handling complex tasks and should not be seen as replacements for supe rvised learning algorithms. For simpler\ntasks, such as the suicide risk classiﬁcation task examined in our stu dy, the performance of LLMs is satisfactory.\nInterestingly, after ﬁne-tuning, GPT-3.5 even outperforms GPT-4, achieving results that are nearly on par with\nthose obtained through supervised learning methods. While there i s often a preference for large input limits in\nlarge language models (LLMs), it’s crucial to tailor these settings to the speciﬁc task at hand. For tasks involving\nshorter text, such as our study on sentiment analysis of social network data, the long input capability of an LLM\nmay not be a primary concern. Our experiments indicate that extend ing the input data to construct few-shot\nprompts does not necessarily lead to improved performance. Theref ore, it is important to carefully consider the\nnature of the task when conﬁguring the input parameters of an LLM.\nOur study does have some limitations. For instance, due to token cons traints, we were unable to conduct\ncertain tests—particularly those involving smaller models supple mented with background knowledge—across all\ntasks. Looking ahead, we plan to conduct more comprehensive studies th at encompass a wider variety of tasks\nand models. This will allow us to draw more deﬁnitive conclusions r egarding the comparative eﬀectiveness of\nlarge language models and supervised learning algorithms. Additionally, t he ﬁne-tuning mechanisms of LLMs\nwarrant further exploration, particularly for more eﬃcient handling of complex tasks. The development of\nadvanced prompt engineering techniques could also help optimize th e performance of LLMs across various tasks.\n7. CONCLUSION\nIn this study, we evaluated the performance of multiple large language mo dels (LLMs) in two psychology-related\ntasks and compared their eﬃcacy with that of supervised learning algorit hms. Although LLMs show promise\nin various natural language processing applications, they are not yet a comp rehensive substitute for supervised\nlearning, particularly in domain-speciﬁc tasks. Fine-tuning LLMs can enhance performance on simpler tasks\nbut is less eﬀective for more complex challenges. The success of diﬀ erent training strategies and prompt designs\nis highly contingent on both the task and the size of the model, undersc oring the necessity for task-speciﬁc\ncustomization. In summary, our research suggests that while LLMs oﬀer cons iderable potential, signiﬁcant work\nremains to make them universally eﬀective across a broad array of comple x tasks.\n8. DATASET AND CODE A V AILABILITY\nThe experimental texts for our study are sourced from comments on a Sin a Weibo post by the user ”Zoufan,”\nwhich can be viewed at: https://www.weibo.com/xiaofan116?is_all=1. An expert-annotated dataset for the\nstudy of cognitive distortions and suicide risk, along with the prompt of the large language model and the super-\nvised learning model code, are now available at: https://github.com/HongzhiQ/SupervisedVsLLM-EfficacyEval.\nHere are the models mentioned earlier along with their corresponding source code and online demo links:\n• ChatGLM2-6B:\n– Source code: https://github.com/thudm/chatglm2-6b\n– Unoﬃcial demo: https://huggingface.co/spaces/mikeee/chatglm2-6b-4bit\n• GLM-130B:\n14\n– Source code: https://github.com/THUDM/GLM-130B\n– Oﬃcial online demo: https://chatglm.cn/detail\n• GPT series:\n– Web application: https://chat.openai.com/\n– GPT-3.5 Fine-tuning details: https://platform.openai.com/docs/guides/fine-tuning\n9. ACKNOWLEDGMENTS\nThis work was supported by grants from the National Natural Science Foundation of China (grant num-\nbers:72174152, 72304212 and 82071546), Fundamental Research Funds for the Central Universit ies (grant num-\nbers: 2042022kf1218; 2042022kf1037), the Young Top-notch Talent Cultivation Program of Hubei Pr ovince.\nGuanghui Fu is supported by a Chinese Government Scholarship prov ided by the China Scholarship Council\n(CSC).\n10. AUTHOS CONTRIBUTIONS\nHongzhi Qi were responsible for the experiment design and programming. Q ing Zhao and Jianqiang Li col-\nlaborated in the proposal of the AI-related aspects of the project, with Z hao focusing on data analysis and\ninterpretation and Li serving as the leader of the computer science asp ect of the project. Both also reviewed\nthe manuscript. Dan Luo and Huijing Zou contributed to the manuscript writing, carried out experimental\nveriﬁcation, and collected data. Changwei Song and Wei Zhai were respons ible for code development and served\nas auxiliary programmers. Guanghui Fu proposed the central idea of the stu dy and was a major contributor in\nwriting the manuscript. Shuo Liu, Yi Jing Yu and Fan Wang took the lead i n result evaluation and contributed\npsychological perspectives to the idea proposal. Bing Xiang Yang propose d the psychological aspects of the\nidea, performed experimental veriﬁcation, and led the project from the psychology angle. All authors read and\napproved the ﬁnal manuscript.\n11. COMPETING INTERESTS\nAll authors declare no ﬁnancial or non-ﬁnancial competing interests.\nREFERENCES\n[1] World health organization, “Depressive disorder (depression),” ( 2023).\n[2] Huang, Y., Wang, Y., Wang, H., Liu, Z., Yu, X., Yan, J., Yu, Y., Kou, C., Xu, X., Lu , J., et al., “Prevalence\nof mental disorders in China: a cross-sectional epidemiological study ,” The Lancet Psychiatry 6(3), 211–224\n(2019).\n[3] World health organization, “Suicide,” (2023).\n[4] Keles, B., McCrae, N., and Grealish, A., “A systematic review: th e inﬂuence of social media on depression,\nanxiety and psychological distress in adolescents,” International journal of adolescence and youth 25(1),\n79–93 (2020).\n[5] Robinson, J., Cox, G., Bailey, E., Hetrick, S., Rodrigues, M., Fisher, S., and Herrman, H., “Social media\nand suicide prevention: a systematic review,” Early intervention in psychiatry 10(2), 103–121 (2016).\n[6] Luxton, D. D., June, J. D., and Fairall, J. M., “Social media and sui cide: a public health perspective,”\nAmerican journal of public health 102(S2), S195–S200 (2012).\n[7] Coppersmith, G., Leary, R., Crutchley, P., and Fine, A., “Natural l anguage processing of social media as\nscreening for suicide risk,” Biomedical informatics insights 10, 1178222618792860 (2018).\n[8] Nandwani, P. and Verma, R., “A review on sentiment analysis and emotion detection from text,” Social\nNetwork Analysis and Mining 11(1), 81 (2021).\n[9] Acheampong, F. A., Wenyu, C., and Nunoo-Mensah, H., “Text-based emotio n detection: Advances, chal-\nlenges, and opportunities,” Engineering Reports 2(7), e12189 (2020).\n15\n[10] Saunders, D., “Domain adaptation and multi-domain adaptation for neural mac hine translation: A survey,”\nJournal of Artiﬁcial Intelligence Research 75, 351–424 (2022).\n[11] Laparra, E., Bethard, S., and Miller, T. A., “Rethinking domain adaptat ion for machine learning over\nclinical language,” JAMIA open 3(2), 146–150 (2020).\n[12] Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Z hang, J., Dong, Z., et al.,\n“A survey of large language models,” arXiv preprint arXiv:2303.18223 (2023).\n[13] Xu, X., Yao, B., Dong, Y., Yu, H., Hendler, J., Dey, A. K., and Wang, D., “Leve raging large language\nmodels for mental health prediction via online text data,” arXiv preprint arXiv:2307.14385 (2023).\n[14] Thirunavukarasu, A. J., Ting, D. S. J., Elangovan, K., Gutierrez , L., Tan, T. F., and Ting, D. S. W., “Large\nlanguage models in medicine,” Nature Medicine , 1–11 (2023).\n[15] Le Glaz, A., Haralambous, Y., Kim-Dufor, D.-H., Lenca, P., Billot, R., Ry an, T. C., Marsh, J., Devylder,\nJ., Walter, M., Berrouiguet, S., et al., “Machine learning and natural l anguage processing in mental health:\nsystematic review,” Journal of Medical Internet Research 23(5), e15708 (2021).\n[16] Fu, G., Song, C., Li, J., Ma, Y., Chen, P., Wang, R., Yang, B. X., and Huang, Z ., “Distant supervision for\nmental health management in social media: suicide risk classiﬁcation s ystem development study,” Journal\nof medical internet research 23(8), e26119 (2021).\n[17] Singh, M., Jakhar, A. K., and Pandey, S., “Sentiment analysis on the i mpact of coronavirus in social life\nusing the bert model,” Social Network Analysis and Mining 11(1), 33 (2021).\n[18] Wan, F., “Sentiment analysis of weibo comments based on deep neural network,” in [ 2019 international\nconference on communications, information system and comput er engineering (CISCE) ], 626–630, IEEE\n(2019).\n[19] Zhang, X., Li, W., Ying, H., Li, F., Tang, S., and Lu, S., “Emotion detection i n online social networks: a\nmultilabel learning approach,” IEEE Internet of Things Journal 7(9), 8133–8143 (2020).\n[20] Wang, J., Zhou, Y., Zhang, W., Evans, R., and Zhu, C., “Concerns expre ssed by chinese social media users\nduring the COVID-19 pandemic: content analysis of sina weibo microbl ogging data,” Journal of medical\nInternet research 22(11), e22152 (2020).\n[21] Kaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu, R., and Mc Hardy, R., “Challenges and applica-\ntions of large language models,” arXiv preprint arXiv:2307.10169 (2023).\n[22] Wei, J., Tay, Y., Bommasani, R., Raﬀel, C., Zoph, B., Borgeaud, S., Y ogatama, D., Bosma, M., Zhou, D.,\nMetzler, D., et al., “Emergent abilities of large language models,” arXiv preprint arXiv:2206.07682 (2022).\n[23] Liebrenz, M., Schleifer, R., Buadze, A., Bhugra, D., and Smith, A., “Generating scholarly content with\nChatGPT: ethical challenges for medical publishing,” The Lancet Digital Health 5(3), e105–e106 (2023).\n[24] Jeblick, K., Schachtner, B., Dexl, J., Mittermeier, A., St ¨ uber, A. T., Topalis, J., Weber, T., Wesp, P., Sabel,\nB., Ricke, J., et al., “ChatGPT makes medicine easy to swallow: An exp loratory case study on simpliﬁed\nradiology reports,” arXiv preprint arXiv:2212.14882 (2022).\n[25] Surameery, N. M. S. and Shakor, M. Y., “Use ChatGPT to solve programming b ugs,” International Journal\nof Information Technology & Computer Engineering (IJITC) ISSN: 24 55-5290 3(01), 17–22 (2023).\n[26] Kasneci, E., Seßler, K., K¨ uchemann, S., Bannert, M., Dement ieva, D., Fischer, F., Gasser, U., Groh,\nG., G¨ unnemann, S., H¨ ullermeier, E., et al., “Chatgpt for good? on opp ortunities and challenges of large\nlanguage models for education,” Learning and individual diﬀerences 103, 102274 (2023).\n[27] Yeo, Y. H., Samaan, J. S., Ng, W. H., Ting, P.-S., Trivedi, H., Vipani, A., Ay oub, W., Yang, J. D., Liran,\nO., Spiegel, B., et al., “Assessing the performance of chatgpt in answer ing questions regarding cirrhosis and\nhepatocellular carcinoma,” medRxiv , 2023–02 (2023).\n[28] Jiang, L. Y., Liu, X. C., Nejatian, N. P., Nasir-Moin, M., Wang, D., Abidin, A. , Eaton, K., Riina, H. A.,\nLaufer, I., Punjabi, P., et al., “Health system-scale language models are al l-purpose prediction engines,”\nNature , 1–6 (2023).\n[29] Farhat, F., “ChatGPT as a complementary mental health resource: a bo on or a bane,” Annals of Biomedical\nEngineering , 1–4 (2023).\n[30] Qin, W., Chen, Z., Wang, L., Lan, Y., Ren, W., and Hong, R., “Read, diagnose and chat: To-\nwards explainable and interactive LLMs-augmented depression detecti on in social media,” arXiv preprint\narXiv:2305.05138 (2023).\n16\n[31] Chen, S., Wu, M., Zhu, K. Q., Lan, K., Zhang, Z., and Cui, L., “LLM-emp owered chatbots for psychiatrist\nand patient simulation: Application and evaluation,” arXiv preprint arXiv:2305.13614 (2023).\n[32] Fu, G., Zhao, Q., Li, J., Luo, D., Song, C., Zhai, W., Liu, S., Wang, F., W ang, Y., Cheng, L., et al.,\n“Enhancing psychological counseling with large language model: A multifac eted decision-support system\nfor non-professionals,” arXiv preprint arXiv:2308.15192 (2023).\n[33] Ayers, J. W., Poliak, A., Dredze, M., Leas, E. C., Zhu, Z., Kelle y, J. B., Faix, D. J., Goodman, A. M.,\nLonghurst, C. A., Hogarth, M., et al., “Comparing physician and artiﬁcial inte lligence chatbot responses to\npatient questions posted to a public social media forum,” JAMA internal medicine (2023).\n[34] Yang, K., Ji, S., Zhang, T., Xie, Q., Kuang, Z., and Ananiadou, S., “Towar ds interpretable mental health\nanalysis with ChatGPT,” (2023).\n[35] Vaishya, R., Misra, A., and Vaish, A., “ChatGPT: Is this version good f or healthcare and research?,”\nDiabetes & Metabolic Syndrome: Clinical Research & Reviews 17(4), 102744 (2023).\n[36] Editorial, “Will ChatGPT transform healthcare?,” Nature Medicine , 505–506 (2023).\n[37] Chang, Y., Wang, X., Wang, J., Wu, Y., Zhu, K., Chen, H., Yang, L., Yi, X., Wang, C., Wang, Y., et al.,\n“A survey on evaluation of large language models,” arXiv preprint arXiv:2307.03109 (2023).\n[38] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K., “BERT: Pre-t raining of deep bidirectional trans-\nformers for language understanding,” arXiv preprint arXiv:1810.04805 (2018).\n[39] Xiao, L., Huang, X., Chen, B., and Jing, L., “Label-speciﬁc document repr esentation for multi-label text\nclassiﬁcation,” in [ Proceedings of the 2019 conference on empirical methods in natura l language processing\nand the 9th international joint conference on natural language pr ocessing (EMNLP-IJCNLP) ], 466–475\n(2019).\n[40] Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zhen g, W., Xia, X., et al.,\n“GLM-130b: An open bilingual pre-trained model,” arXiv preprint arXiv:2210.02414 (2022).\n[41] OpenAI, “Introducing chatgpt,” (2023).\n[42] OpenAI, “Gpt-4 technical report,” (2023).\n[43] Wang, S., Sun, Y., Xiang, Y., Wu, Z., Ding, S., Gong, W., Feng, S., Shang, J ., Zhao, Y., Pang, C., et al.,\n“Ernie 3.0 titan: Exploring larger-scale knowledge enhanced pre-trai ning for language understanding and\ngeneration,” arXiv preprint arXiv:2112.12731 (2021).\n[44] Burns, D. D., [ Feeling good], Signet Book (1981).\n17",
  "topic": "Social media",
  "concepts": [
    {
      "name": "Social media",
      "score": 0.6399773359298706
    },
    {
      "name": "Computer science",
      "score": 0.633469820022583
    },
    {
      "name": "Realm",
      "score": 0.5582587718963623
    },
    {
      "name": "Cognition",
      "score": 0.5467870831489563
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5204870104789734
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.4655514359474182
    },
    {
      "name": "Code (set theory)",
      "score": 0.43857502937316895
    },
    {
      "name": "Psychological intervention",
      "score": 0.41783440113067627
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4176666736602783
    },
    {
      "name": "Language model",
      "score": 0.41557246446609497
    },
    {
      "name": "Cognitive psychology",
      "score": 0.40568992495536804
    },
    {
      "name": "Machine learning",
      "score": 0.400994211435318
    },
    {
      "name": "Psychology",
      "score": 0.3594475984573364
    },
    {
      "name": "Natural language processing",
      "score": 0.3455151319503784
    },
    {
      "name": "Data science",
      "score": 0.34521132707595825
    },
    {
      "name": "World Wide Web",
      "score": 0.08364132046699524
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I37796252",
      "name": "Beijing University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I37461747",
      "name": "Wuhan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210097159",
      "name": "Assistance Publique – Hôpitaux de Paris",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I1294671590",
      "name": "Centre National de la Recherche Scientifique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I154526488",
      "name": "Inserm",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210152724",
      "name": "Institut du Cerveau",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210121705",
      "name": "Pitié-Salpêtrière Hospital",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I39804081",
      "name": "Sorbonne Université",
      "country": "FR"
    }
  ],
  "cited_by": 13
}