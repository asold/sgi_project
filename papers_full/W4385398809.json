{
  "title": "Abstractive Text Summarization using Pre-Trained Language Model \"Text-to-Text Transfer Transformer (T5)\"",
  "url": "https://openalex.org/W4385398809",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5092571152",
      "name": "Qurrota Aâ€™yuna Itsnaini",
      "affiliations": [
        "Universitas Amikom Yogyakarta",
        "Universitas Respati Yogyakarta"
      ]
    },
    {
      "id": "https://openalex.org/A2619808725",
      "name": "Mardhiya Hayaty",
      "affiliations": [
        "Universitas Amikom Yogyakarta",
        "Universitas Respati Yogyakarta"
      ]
    },
    {
      "id": "https://openalex.org/A2174481124",
      "name": "Andriyan Dwi Putra",
      "affiliations": [
        "Universitas Amikom Yogyakarta",
        "Universitas Respati Yogyakarta"
      ]
    },
    {
      "id": "https://openalex.org/A2720388732",
      "name": "Nidal A.M Jabari",
      "affiliations": [
        "Palestine Technical University - Kadoorie"
      ]
    },
    {
      "id": "https://openalex.org/A5092571152",
      "name": "Qurrota Aâ€™yuna Itsnaini",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2619808725",
      "name": "Mardhiya Hayaty",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2174481124",
      "name": "Andriyan Dwi Putra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2720388732",
      "name": "Nidal A.M Jabari",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3027042170",
    "https://openalex.org/W2964061924",
    "https://openalex.org/W3188345884",
    "https://openalex.org/W4242525163",
    "https://openalex.org/W3036583920",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3095340127",
    "https://openalex.org/W3025451744",
    "https://openalex.org/W3163152256",
    "https://openalex.org/W6771626834",
    "https://openalex.org/W2594990650",
    "https://openalex.org/W6963024689",
    "https://openalex.org/W3115006989",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3190367510",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2897803242",
    "https://openalex.org/W2000777050",
    "https://openalex.org/W3044827386",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W2952564229",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W4296785397",
    "https://openalex.org/W4288350560"
  ],
  "abstract": "Automatic Text Summarization (ATS) is one of the utilizations of technological sophistication in terms of text processing assisting humans in producing a summary or key points of a document in large quantities. We use Indonesian language as objects because there are few resources in NLP research using Indonesian language. This paper utilized PLTMs (Pre-Trained Language Models) from the transformer architecture, namely T5 (Text-to-Text Transfer Transformer) which has been completed previously with a larger dataset. Evaluation in this study was measured through comparison of the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) calculation results between the reference summary and the model summary. The experiments with the pre-trained t5-base model with fine tuning parameters of 220M for the Indonesian news dataset yielded relatively high ROUGE values, namely ROUGE-1 = 0.68, ROUGE-2 = 0.61, and ROUGE-L = 0.65. The evaluation value worked well, but the resulting model has not achieved satisfactory results because in terms of abstraction, the model did not work optimally. We also found several errors in the reference summary in the dataset used.",
  "full_text": "ILKOM Jurnal Ilmiah Vol. 15, No. 1, April 2023, pp.124-131  \nAccredited 2nd by RISTEKBRIN No. 200/M/KPT/2020; E-ISSN 2548-7779 | P-ISSN 2087-1716 \n \n  \n \n \n \n       http://dx.doi.org/10.33096/ilkom.v15i1.1532.124-131 \n124 \n \n \nAbstractive Text Summarization using Pre-Trained \nLanguage Model \"Text-to-Text Transfer Transformer \n(T5)\" \n \nQurrota Aâ€™yuna Itsnaini a,1; Mardhiya Hayaty a,2,*; Andriyan Dwi Putra a,3; Nidal A.M Jabari b,4 \naUniversitas Amikom Yogyakarta, Jl. Ring Road Utara, Yogyakarta and 55281, Indonesia \nbPalestine technical university Kadoorie, , Palestine \n1 qurrota.itsnaini@students.amikom.ac.id; 2 mardhiya_hayati@amikom.ac.id; 2 andriyan.Putra@amikom.ac.id;4 nidal.jabari@ptuk.edu.ps \n* Corresponding author \n \nArticle history: Received November 29, 2022; Revised December 02, 2022; Accepted January 21, 2023; Available online April 07, 2023 \n \n \n \nKeywords: Automatic Text Summarization; Transformer; Pre-Trained Model; T5; ROUGE. \n \nIntroduction  \nAutomatic text summarization (ATS) is a form of text processing in the field of Natural Language Processing that \nutilizes the sophistication of the internet and technology to summarize text automatically[1]. ATS functions to produce \na clear summary while maintaining the main information and the overall meaning contained therein  [2]. Text \nsummarization is grouped into two methods, extractive and abstractive. Extractive text summarization uses the \ncalculation of the score of words in sentences  [3]. At the same time, the summary text abstraction aims to produce a \nsummary by i nterpreting and analyzing the text as a whole so that it has less text but still contains the essential \ninformation conveyed in the original text [2]. \nOne of the studies was carried out using the Sequence -to-Sequence method with enhanced features for single \ndocuments [4]. It used a non-local network feature that functions to improve the traditional Sequence -to-Sequence \nstructure, thus proving that the model proposed in this study has more effective results compared to the basic model, \nwith an increase of 5.6%, 5.3%, 6.2% in three ROUGE matrix of R-1, R-2, and R-L values. \nAnother research was carried out using the transformer method and datasets from 'Wikipedia' and 'The Hindu' [5]. \nThe data was processed using the Bidirectional Encoder Representation Transformer (BERT) model, which produced \nROUGE-1, ROUGE-2, and ROUGE-L values of 41.72, 19.39, and 38.76, respectively.  \nBased on our observation, transformer architecture has a better effect than other methods in summarizing large -\nscale texts. Transformer architecture performs text summarization automatically using self -attention to calculate \ndifferent input and output representations [6]. For this reason, we would implement a transformer architecture using \nPTLMs (Pre-Trained Language Models) that have been trained with large -scale data, namely the T5 (Text -to-Text \nTransfer Transformer) model, into Indonesian language do cuments. This study aimed to prove the effectiveness of \nthe T5 pre-trained model in ATS of large-scale Indonesian online news data. \nResearch Article       Open Access (CCâ€“BY-SA) \nAbstract  \nAutomatic Text Summarization (ATS) is one of the utilizations of technological sophistication in terms of text \nprocessing assisting humans in producing a summary or key points of a document in large quantities. We use \nIndonesian language as objects becaus e there are few resources in NLP research using Indonesian  language. \nThis paper utilized PLTMs (Pre -Trained Language Models) from the transformer architecture, namely T5 \n(Text-to-Text Transfer Transformer) which has been completed previously with a larger dataset. Evaluation in \nthis study was measured through comparison of the ROUGE  (Recall-Oriented Understudy for Gisting \nEvaluation) calculation results between the reference summary and the model summary. The experiments with \nthe pre-trained t5-base model with fine tuning parameters of 220M for  the Indonesian news dataset yielded \nrelatively high ROUGE values, namely ROUGE -1 = 0.68, ROUGE -2 = 0.61, and ROUGE -L = 0.65. The \nevaluation value worked well, but the resulting model has not achieved satisfactory results because in terms of \nabstraction, the model did not work optimally. We also found several errors in the reference summary in the \ndataset used. \n125 ILKOM Jurnal Ilmiah Vol. 15, No. 1, April 2023, pp.124-131 E-ISSN 2548-7779 \n  \n \n \n Itsnaini, et. al. (Abstractive Text Summarization using Pre-Trained Language Model \"Text-to-Text Transfer Transformer (T5)\") \nMethod  \nA. Automatic Text Summarization (ATS) \nThe development of text summarization has been improving, it can be seen from the increasing number of \nresearch discussing ATS. ATS is divided into two methods, extractive and abstractive. Research on extractive forms \nusing the Aspect Based Sentiment Analysis (ABSA) process was carried out using the SumEval dataset[7]. The data \nwas extracted through the self -attention stage, combined with the original sentences that were previously extracted \nthrough Convolutional Neural Networks (CNN).  In the final stage,  the aspect -category or aspect -term data was \nconnected to obtain the feature sentiment results. The results on the aspect category with the Gated CNN self-attention \nmodel yielded scores of 81.40 and 79.77. Meanwhile, the aspect term with the same model produced 71.96 and 62.54. \n The research was carried out in an abstract form using the sequence-to-sequence method of summarizing English \nsocial media texts [8]. An intervention was conducted to the encoder to filter information better. The Evaluation acores \nof the F1 ROUGE-1, ROUGE-2, and ROUGE-L were 2.6%, 2.1%, and 2.5%, respectively. These results showed that \nthe selective method given to the encoder work ed optimally. The transformer approach for Indonesian -language \ndatasets was used to detect rude comments on online news in Indonesia[9]. The model used was the BERT model and \nthe BERT Multilingual pre -train to serve as a baseline, so that the results from the Scratch model that were trained \nobtained an F-1 average score of 50%, which was then compared to the Multilingual BERT model of 54%. \n \nTransformer. Transformer architecture changed the sequence using two necessary essential encoders and a \ndecoder. The function of the encoder was to capture the input sequence and process information with a fixed length \nand sequence, after that the decoder displayed the output previously processed on the encoder , shown in Figure 1. \nBoth of these parts could work optimally when using the attention mechanism (Attention Mechanism)  [10][11]. The \nattention Mechanism functioned to connect the encoder and decoder in processing the information provided by giving \nattention to long input sentences  [12]. So that information can be disseminated to all sequences of input sentences \nwhich were carried out selectively by the decoder with an attention mechanism [13].  \nThe research was conducted to summarize news  about COVID-19 using a transformer architecture [14] by \nutilizing the encoder-decoder model and obtaining ROUGE-1 and ROUGE-2 scores of 0.58 and 0.42, with a training \ntime of 11438 seconds, respectively. \nText-to-Text Transfer Transformer (T5).  The T5 model is a framework developed on top of popular \narchitectures such as BERT, GPT, etc., by utilizing text-to-text transfer learning, as shown in Figure 2, with examples \nof translation tasks, text similarities, and text summaries. T5 is an end -to-end trained transformer model with text as \ninput and modified text as output[15].  In its completion, the T5 model proved successful and capable of performing \nthree tasks [16]: generating, classification, and regression.  \nThe application of the T5 model to the BBC News dataset to determine which model is superior to the ROUGE \nevaluation value. Research comparing several transformer models [3],  one of which is the T5 model, which is proven \nFigure 2. How the T5 models work \nFigure 1. Transformer architecture \nwith an attention mechanism \n \n\n126 ILKOM Jurnal Ilmiah Vol. 15, No. 1, April 2023, pp.124-131 E-ISSN 2548-7779 \n  \n \n \n Itsnaini, et. al. (Abstractive Text Summarization using Pre-Trained Language Model \"Text-to-Text Transfer Transformer (T5)\") \n \nto be superior to other transformer models . The score of ROUGE-1, ROUGE-2, and ROUGE-L are 0.47, 0.33, and \n0.42 respectively.  \nTransformers, a tool or platform for downloading and training pre -trained models, has been trained on previous \ndatasets using Hugging Face[17]. Pre-training of models has been carried out on larger corpus or datasets to improve \nmodel performance[18]. The pre-trained Model T5 has several types according to the size of the parameters [15], \nincluding  : \nâ€¢ T5-Small : 60 M of parameters \nâ€¢ T5-Base : 220 M of parameters \nâ€¢ T5-Large : 770 M of parameters \nâ€¢ T5-3B : 3 B of parameters \nâ€¢ T5-11B : 11 B of parameters \nModels with smaller parameters can help train data with  limited hardware specifications. The pre-training model \nused in this study uses the \"cahya/t5 -base-indonesian-summarization-cased\" model for Indonesian texts with fine-\ntuning on the id_liputan6 dataset. Specifically, the T5-base model trains a model with a pre-trained text-to-text format \nwith parameters of 220 M [15] [19]. T5-base is applied to the encoder in the form of Indonesian news text. \nB. Experiment \nAs part of the research, we outline the research flow in  Figure 3 by pre-processing the text of the article in the \nencoder by adding the â€œsummarize:â€ prefix to facilitate the task intended by the input. The hyperparameters used in \nthis research adjust to the hardware resourcesâ€™ limitations. We implement tokenizing with a maximum of 512 words \nfor each encoder and 128 words for the decoder.  \nThe hardware specifications used can be described as follows: \nâ€¢ Python 3.6 and up \nâ€¢ Transformers dan PyTorch  \nâ€¢ Python Machine Learning base libraries \nâ€¢ High RAM GPU settings \n1) Preparing Dataset \nThis study only used 9,387 data from IndoSum's 18,774 Indonesian language news . This was due to our limited \nresources in conducting this research. But this d id not affect the evaluation process in modelling. So, the research \nobjectives could still be achieved. We divide the data into training data (90% = 8448) and testing data (10% = 939). \nTo validate during training, we divided 10% of the training data, namely 845 data, for data validation. IndoSum's news \ndata was taken from major Indonesian-language news portals, Kumparan and CNN Indonesia [20].  \n \nFigure 3. Research Flowchart Diagram \n\n127 ILKOM Jurnal Ilmiah Vol. 15, No. 1, April 2023, pp.124-131 E-ISSN 2548-7779 \n  \n \n \n Itsnaini, et. al. (Abstractive Text Summarization using Pre-Trained Language Model \"Text-to-Text Transfer Transformer (T5)\") \n2) ROUGE Evaluation \nThe evaluation used in this research was the ROUGE (Recall -Oriented Understudy for Gisting Evaluation) \nmatrix[21]. Rouge is an evaluation tool used for multi -document summarization and has great ad vantage in text \nsummarization [22]. Rouge generally compares the human summary as a model with the machine summary. In this \ncase, the evaluation used in the current study was the number of unigrams for ROUGE-1, namely between the system \nsummary and the reference, the number of bigrams for ROUGE -2 between the system and the reference summary, \nand ROUGE-L by comparing the Longest Common Subsequence (LCS) between the summary results machine with \nsummary reference to the dataset. In equation (1), it was explained that the value of LCS or the longest word compared \nto M was the number of words in the reference summary [23].  \nğ‘…ğ‘‚ğ‘ˆğºğ¸ âˆ’ ğ¿ =\nLCS\nM             (1) \nEach ROUGE value included the Recall (2), Precision (3), and F1-Score (4) matrices. \nğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ =\nW overlapped\nW ref                 (2) \nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› =\nW overlapped\nW cand                            (3) \nğ¹1 âˆ’ ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ =  \n2 ğ‘¥ ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ ğ‘¥ ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›\n(ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™+ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›)             (4) \nResults and Discussion  \n \nFigure 4. The number of steps across the epochs \n The present research used a batch size set to 8 because of the limited computational unit during training. Figure \n4 shows the number of epochs carried out during training, namely 10 (y labels) with a total of  9,509 steps (x labels) \nfrom 8448 Indonesian news data.  \nAfter training, the model was tested using a dataset test to determine the modelâ€™s success in understanding news \nsentences in the training data. The experimental summary results on the test data was shown in Table 1. \n \n\n128 ILKOM Jurnal Ilmiah Vol. 15, No. 1, April 2023, pp.124-131 E-ISSN 2548-7779 \n  \n \n \n Itsnaini, et. al. (Abstractive Text Summarization using Pre-Trained Language Model \"Text-to-Text Transfer Transformer (T5)\") \n \nFigure 5. Example of Summarized Results on Data Test  \nTable 1. Results of the ROUGE Evaluation Value from Table. 1 \n Rouge-1 Rouge-2 Rouge-3 \nRecall 0.4 0.28 0.4 \nPrecision 0.36 0.25 0.36 \nF1-Score 0.38 0.27 0.8 \n \nResults of the ROUGE Evaluation Value from Figure 5 see in Table 1 Several errors were found in the summary of \nthe news text above, including:  \nArticle: \n \nJakarta CNN Indonesia - Anthony Martial showed extraordinary individual performance when Manchester United won \nover Real Madrid via a penalty shoot -out in a pre-season match in California, United States of America. Jese Lingard \nscored Manchester United's goal in the match  in the first half injury time. But Martial played a big role behin d the \ncreation of these goals. After receiving a ball from Marouane Fellaini, Martial dribbled the ball while penetrating the \nReal Madrid penalty box.  Martial was being closely shadowed by Dani Carvajal plus supervision from Luka Modric. \nHowever, Martial w as able to weave the ball quickly from right to left foot repeatedly. After Carvajal was tricked \nRaphael Varane tried to help. But Martial coolly sent the ball to Lingard who was in front of the goal. Lingard also \nwithout difficulty struck the ball into th e goal. Madrid successfully answered Lingard's goal itself  through Casemiro's \npenalty in the second half. But in the end Manchester United won via penalty shootout . In the penalty shoot -out, two \nManchester United executors Henrikh Mkhitaryan and Daley Bli nd managed to carry out their duties while in the Los \nBlancos camp only Luismi Quezada managed to become the executor. The victory of \"\"Red Devils\"\" over Los Blancos \nhas made the positive trend of Jose Mourinho's squad on the pre -season tour continue. This was Manchester United's \nfourth win on the pre-season tour after they previously beat LA Galaxy (5-2) Real Salt Lake (2-1) and Manchester City \n(2-0). \n \nReference Summary: \n \nAnthony Martial showed extraordinary individual performance when Manchester United won over Real Madrid via a \npenalty shoot -out in a pre -season match in California, United States of America. Jese Lingard scored Manchester \nUnited's goal in the match in the first half injury time. But Martial played a big role behind the creation of these goals. \n \nModel Summary : \nKemenangan atas Real Madrid lewat drama adu penalti dalam laga pramusim di California Amerika Serikat membuat \ntren positif skuat asuhan Jose Mourinho di tur pramusim berlanjut. Ini adalah kemenangan keempat Manchester United \ndi tur pramusim setelah sebelumnya mereka menaklukkan LA Galaxy ( 5 - 2 ) Real Salt Lake ( 2 - 1 ) dan Manchester \nCity ( 2 - 0 ). \nThe victory over Real Madrid via a penalty shootout in a pre-season match in California, United States of America, has \nmade the positive trend of Jose Mourinho's squad continue on the pre -season tour. This is Manchester United's fourth \nwin on the pre-season tour after previously they beat LA Galaxy (5 - 2) Real Salt Lake (2 - 1) and Manchester City (2 - \n0). \n \n \n \n \n\n129 ILKOM Jurnal Ilmiah Vol. 15, No. 1, April 2023, pp.124-131 E-ISSN 2548-7779 \n  \n \n \n Itsnaini, et. al. (Abstractive Text Summarization using Pre-Trained Language Model \"Text-to-Text Transfer Transformer (T5)\") \nâ€¢ Paraphrasing produced by the model was not optimal. An example of a successful paraphrase was \nâ€œKemenanganâ€ at the beginning of a predicted summary sentence. â€œKemenanganâ€ was a paraphrase of the \nsentence â€œManchester United menang atas Real Madrid â€. In addition, the model summary results were \nonly in the form of main sentences that exactly match the encoder. This might be due to the absence of Word \nEmbedding before training. So, the model cannot read words that had the same context.  \nâ€¢ The model managed to read and save the meaning of the sentence â€œKemenangan Setan Merah atas \nBlancosâ€ as the meaning of â€œKemenangan Manchester United atas Real Madridâ€.  \nâ€¢ In the text of the article, some words that were not labelled were omitted and randomly removed. The model \nonly took the main sentence at the beginning and end of the paragraph.  \nâ€¢ Score Recall on ROUGE -L in Table 1 showed the number 0.4 of the calculation results of the LCS or the \nlongest word in the engine resulting in summary, namely the predicted summary compared to the summary \nof the dataset. The results were low because the difference between the two was noticeable.  \n \nWe also found several errors in the dataset, where the summary reference of the dataset was only in the form of \nrewritten main sentences; no changes to words or paraphrases were made from human summaries. An example of the \ndataset is shown in Figure 6.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 6. Examples of Errors in the Dataset \nThe reference summary of the dataset greatly influences the ROUGE evaluation value generated. It can be seen \nclearly in Figure 6 that the reference summary only contain ed the repetition of the first sentence of the paragraph. \nThere was not a single change or layout for each word. This was an important thing to consider before doing research.  \nTable 2. Rouge Value of Overall Data Test \n R-1 R-2 R-L \nRECALL 0.686 0.612 0.654 \nPRECISION 0.741 0.662 0.707 \nF-1 SCORE 0.71 0.633 0.677 \n \nAfter calculating the ROUGE value for each line of test data, which amounts to 939, we get the overall ROUGE \nvalue taken based on the average value per line. Based on the Table 2, the score value can be stated as high. However, \nArticle :\nMerdeka.com - Legendary Manchester United ex-manager Sir Alex Ferguson has accused Paul Pogba's agent Mino Raiola \nof being a liar. The criticism itself is inseparable from Ferguson's bad relationship with the Italian man. These two people \ndon't get along b ecause of the Pogba transfer. The midfielder from France used to strengthen MU before finally moving to \nJuventus in 2012. To make matters worse, he ran on a free transfer. Ferguson himself once referred to Raiola as one of the \npeople he did not like. And when asked about what happened in 2012, Ferguson could not help but criticize Raila and blame \nhim for Pogba's departure for Turin five years ago. \" Paul Pogba ? He has a bad agent . A liar \" said Ferguson as reported \nby Sportsmole . \" We know Paul Pogba wel l . We know he is a good player he is still a good player \" he said . \" We offered \nhim the best possible contract \" he continued \nReference Summary: \nMantan manajer legendaris Manchester United Sir Alex Ferguson menyebut agen Paul Pogba  Mino Raiola  sebagai seorang \ntukang bohong . Kecaman itu sendiri tidak lepas dari hubungan buruk Ferguson dengan pria Italia tersebut . Kedua orang ini \njadi tidak akur akibat transfer Pogba . Gelandang asal Prancis itu dulu memperkuat MU  sebelum akhirnya lari ke Juventus \npada tahun 2012 . Parahnya  ia lari dengan status bebas transfer . \nLegendary Manchester United exmanager Sir Alex Ferguson has accused Paul Pogba's agent Mino Raiola of being a liar. \nThe criticism itself is inseparable from Ferguson's bad relationship with th e Italian man. These two people don't get along \nbecause of the Pogba transfer. The midfielder from France used to strengthen MU before finally moving to Juventus in 2012. \nTo make matters worse, he ran on a free transfer. \n \n\n130 ILKOM Jurnal Ilmiah Vol. 15, No. 1, April 2023, pp.124-131 E-ISSN 2548-7779 \n  \n \n \n Itsnaini, et. al. (Abstractive Text Summarization using Pre-Trained Language Model \"Text-to-Text Transfer Transformer (T5)\") \n \nerrors in the dataset and model results were also essential in calculating this evaluation. We assumed the high results \nwere generated because some reference summaries only take the paragraphâ€™s main sentences. Meanwhile, the model \nproduced in this study can only paraphrase a few of the many words in the news article, so several summary models \nwere also generated from the main sentences of the paragraph.  \n \nConclusion  \nModel T5 (Text -to-Text Transfer Transformer) is the newest PLTM that requires highly qualified research \nresources. The pretrained T5 model resulting from this study obtained an average evaluation value of ROUGE -1 of \n0.68, ROUGE-2 of 0.61, and ROUGE-L of 0.65. This showed that the model succeeds in paraphrasing sentences even \nthough only a few and not optimally; at least the model did not change the meaning of the original article so that the \nsummary was easy to understand without having to read the original article. \nFor further research, some suggestions that can be done and even prepared are: \nâ€¢ Using datasets that have more data and maximum quality reference summaries. With more data, it is \nhoped that the model can store much vocabulary to be later implemented on the test data with paraphrases \nthat still make sense and don't change the meaning of the context in it. \nâ€¢ Doing word embedding after preprocessing. It  is meant to convert the word into a vector and save it to \nsee whether the next word has the same context. Then the paraphrase will work more optimally. \nâ€¢ Using a pre-trained model with more extensive parameters, such as T5-Large and some above.  \nâ€¢ Adding batch size and epoch training to produce better model results. \n \nReferences  \n[1] A. P. Widyassari et al., â€˜Review of automatic text summarization techniques & methodsâ€™, J. King Saud Univ. \n- Comput. Inf. Sci., vol. 34, no. 4, pp. 1029â€“1046, 2022, doi: 10.1016/j.jksuci.2020.05.006. \n[2] M. Allahyari et al., â€˜Text summarization techniques: A Brief Surveyâ€™, Int. J. Adv. Comput. Sci. Appl., vol. 8, \nno. 10, 2017, doi: 10.14569/ijacsa.2017.081052. \n[3] A. Gupta, D. Chugh, Anjum, and R. Katarya, â€˜Automated news summarization u sing Transformersâ€™, Lect. \nNotes Electr. Eng., vol. 840, pp. 249â€“259, 2022, doi: 10.1007/978-981-16-9012-9_21. \n[4] Z. Hao and B. Xue, â€˜2020 5th Asia-Pacific Conference on Intelligent Robot Systems, ACIRS 2020â€™, 2020 5th \nAsia-Pacific Conf. Intell. Robot Syst. ACIRS 2020, pp. 163â€“167, 2020. \n[5] M. Ramina, N. Darnay, C. Ludbe, and A. Dhruv, â€˜Topic level summary generation using BERT ind uced \nAbstractive Summarization modelâ€™, Proc. Int. Conf. Intell. Comput. Control Syst. ICICCS 2020 , no. Iciccs, \npp. 747â€“752, 2020, doi: 10.1109/ICICCS48265.2020.9120997. \n[6] A. Vaswani et al., â€˜Attention is all you needâ€™, Adv. Neural Inf. Process. Syst., vol. 2017-Decem, no. Nips, pp. \n5999â€“6009, 2017. \n[7] J. Yang and J. Yang, â€˜Aspect based sentiment analys is with Self -Attention and Gated Convolutional \nNetworksâ€™, Proc. IEEE Int. Conf. Softw. Eng. Serv. Sci. ICSESS , vol. 2020-Octob, pp. 146â€“149, 2020, doi: \n10.1109/ICSESS49938.2020.9237640. \n[8] Z. Liang, J. Du, and C. Li, â€˜Abstractive social media text summarization using selective reinforced Seq2Seq \nattention modelâ€™, Neurocomputing, vol. 410, pp. 432â€“440, 2020, doi: 10.1016/j.neucom.2020.04.137. \n[9] A. D. Rendragraha, M . A. Bijaksana, and A. Romadhony, â€˜Pendekatan metode Transformers untuk deteksi \nbahasa kasar dalam komentar berita online Indonesiaâ€™, e-Proceeding Eng., vol. 8, no. 2, pp. 3385â€“3395, 2021. \n[10] N. Kitaev, Å. Kaiser, and A. Levskaya, â€˜Reformer: The Efficien t Transformerâ€™, ICLR 2020, pp. 1â€“12, 2020, \n[Online]. Available: http://arxiv.org/abs/2001.04451. \n[11] S. Sukhbaatar, E. Grave, P. Bojanowski, and A. Joulin, â€˜Adaptive attention span in transformersâ€™, ACL 2019 \n- 57th Annu. Meet. Assoc. Comput. Linguist. Proc. Conf., no. 2015, pp. 331â€“335, 2020, doi: 10.18653/v1/p19-\n1032. \n[12] D. Britz, A. Goldie, M. T. Luong, and Q. V. Le, â€˜Massive exploration of neural machine translation \narchitecturesâ€™, EMNLP 2017 - Conf. Empir. Methods Nat. Lang. Process. Proc. , pp. 1442â€“1451, 2017, doi: \n10.18653/v1/d17-1151. \n[13] Y. Belinkov, S. Gehrmann, G. Ai, and E. Pavlick, â€˜Tutorial Proposal: Interpretability and Analysis in Neural \n131 ILKOM Jurnal Ilmiah Vol. 15, No. 1, April 2023, pp.124-131 E-ISSN 2548-7779 \n  \n \n \n Itsnaini, et. al. (Abstractive Text Summarization using Pre-Trained Language Model \"Text-to-Text Transfer Transformer (T5)\") \nNLP. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial \nAbstractsâ€™, ACL  Meet. Assoc. Comput. Lingui st., pp. 1 â€“5, 2020, [Online]. Available: \nhttps://doi.org/10.18653/v1/P17. \n[14] N. Hayatin, K. M. Ghufron, and G. W. Wicaksono, â€˜Summarization of COVID -19 news documents deep \nlearning-based using transformer architectureâ€™, Telkomnika (Telecommunication Comput. Electron. Control., \nvol. 19, no. 3, pp. 754â€“761, 2021, doi: 10.12928/TELKOMNIKA.v19i3.18356. \n[15] C. Raffel et al., â€˜Exploring the limits of transfer learning with a unified text -to-text transformerâ€™, J. Mach. \nLearn. Res., vol. 21, pp. 1â€“67, 2020. \n[16] A. Alomari, N. Idris, A. Q. M. Sabri, and I. Alsmadi, â€˜Deep reinforcement and transfer learning for abstractive \ntext summarization: A reviewâ€™, Comput. Speech Lang. , vol. 71, no. August 2021, p. 101276, 2022, doi: \n10.1016/j.csl.2021.101276. \n[17] T. Wolf et al., â€˜Transformers: State -of-the-Art Natural Language Processingâ€™, pp. 38 â€“45, 2020, doi: \n10.18653/v1/2020.emnlp-demos.6. \n[18] K. Song, X. Tan, T. Qin, J. Lu, and T. Y. Liu, â€˜MASS: Masked sequence to sequence pre-training for language \ngenerationâ€™, 36th Int. Conf. Mach. Learn. ICML 2019, vol. 2019-June, pp. 10384â€“10394, 2019. \n[19] A. Roberts, C. Raffel, and N. Shazeer, â€˜How much knowledge can you pack into the parameters of a language \nmodel?â€™, EMNLP 2020 - 2020 Conf. Empir. Methods Nat. Lang. Process. Proc. Conf., pp. 5418â€“5426, 2020, \ndoi: 10.18653/v1/2020.emnlp-main.437. \n[20] K. Kurniawan and S. Louvan, â€˜INDOSUMâ€¯: A New Benchmark Dataset for Indonesian Text Summarizationâ€™, \n2018 Int. Conf. Asian Lang. Process., pp. 215â€“220, 2018. \n[21] S. Ontoum and J. H. Chan , â€˜Automatic Text Summaration of COVID -19 Scientific Research Topics Using \nPre-trained Model from HuggingFaceÂ®â€™, pp. 1â€“8, 2022. \n[22] T. He et al., â€˜ROUGE-C: A fully automated evaluation method for multi -document summarizationâ€™, 2008 \nIEEE Int. Conf. Granul. Comput. GRC 2008, pp. 269â€“274, 2008, doi: 10.1109/GRC.2008.4664680. \n[23] Y. Yuliska and K. U. Syaliman, â€˜Literatur Review Terhadap Metode, Aplikasi dan Dataset Peringkasan \nDokumen Teks Otomatis untuk Teks Berbahasa Indonesiaâ€™, IT J. Res. Dev. , vol. 5, no.  1, pp. 19â€“31, 2020, \ndoi: 10.25299/itjrd.2020.vol5(1).4688. \n \n ",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9044727683067322
    },
    {
      "name": "Computer science",
      "score": 0.7372871041297913
    },
    {
      "name": "Natural language processing",
      "score": 0.7199934720993042
    },
    {
      "name": "Transformer",
      "score": 0.6676335334777832
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6265051364898682
    },
    {
      "name": "ROUGE",
      "score": 0.568152129650116
    },
    {
      "name": "Language model",
      "score": 0.512923002243042
    },
    {
      "name": "Text processing",
      "score": 0.4358452558517456
    },
    {
      "name": "Information retrieval",
      "score": 0.3493354618549347
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4400573182",
      "name": "Universitas Amikom Yogyakarta",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210087739",
      "name": "Universitas Respati Yogyakarta",
      "country": "ID"
    },
    {
      "id": "https://openalex.org/I2799716270",
      "name": "Palestine Technical University - Kadoorie",
      "country": "PS"
    }
  ],
  "cited_by": 11
}