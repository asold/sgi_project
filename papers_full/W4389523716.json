{
  "title": "FinGPT: Large Generative Models for a Small Language",
  "url": "https://openalex.org/W4389523716",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5111073900",
      "name": "Risto Luukkonen",
      "affiliations": [
        "University of Turku"
      ]
    },
    {
      "id": "https://openalex.org/A5003808414",
      "name": "Ville Komulainen",
      "affiliations": [
        "University of Turku"
      ]
    },
    {
      "id": "https://openalex.org/A5018656314",
      "name": "Jouni Luoma",
      "affiliations": [
        "University of Turku"
      ]
    },
    {
      "id": "https://openalex.org/A5028239706",
      "name": "Anni Eskelinen",
      "affiliations": [
        "University of Turku"
      ]
    },
    {
      "id": "https://openalex.org/A5048932608",
      "name": "Jenna Kanerva",
      "affiliations": [
        "University of Turku"
      ]
    },
    {
      "id": "https://openalex.org/A5046560199",
      "name": "Hanna-Mari Kupari",
      "affiliations": [
        "University of Turku"
      ]
    },
    {
      "id": "https://openalex.org/A5019929457",
      "name": "Filip Ginter",
      "affiliations": [
        "University of Turku"
      ]
    },
    {
      "id": "https://openalex.org/A5036607482",
      "name": "Veronika Laippala",
      "affiliations": [
        "University of Turku"
      ]
    },
    {
      "id": "https://openalex.org/A5000043237",
      "name": "Niklas Muennighoff",
      "affiliations": [
        "National Library of Finland"
      ]
    },
    {
      "id": "https://openalex.org/A5055847471",
      "name": "Aleksandra Piktus",
      "affiliations": [
        "National Library of Finland"
      ]
    },
    {
      "id": "https://openalex.org/A5085451281",
      "name": "Thomas J. Wang",
      "affiliations": [
        "National Library of Finland"
      ]
    },
    {
      "id": "https://openalex.org/A5076886615",
      "name": "Nouamane Tazi",
      "affiliations": [
        "National Library of Finland"
      ]
    },
    {
      "id": "https://openalex.org/A5084957527",
      "name": "Teven Le Scao",
      "affiliations": [
        "National Library of Finland"
      ]
    },
    {
      "id": "https://openalex.org/A5078865608",
      "name": "Thomas Wolf",
      "affiliations": [
        "National Library of Finland"
      ]
    },
    {
      "id": "https://openalex.org/A5014308772",
      "name": "Osma Suominen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5077266217",
      "name": "Samuli Sairanen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5059116909",
      "name": "Mikko Merioksa",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5085268698",
      "name": "Jyrki Heinonen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5065189966",
      "name": "Aija Vahtola",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5072416811",
      "name": "Samuel Antão",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5066925770",
      "name": "Sampo Pyysalo",
      "affiliations": [
        "University of Turku"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W4226204562",
    "https://openalex.org/W3093456628",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4378505278",
    "https://openalex.org/W4307225507",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W4324060670",
    "https://openalex.org/W4385570226",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385849242",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2093585241",
    "https://openalex.org/W3177057043",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2725642137",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4308902180",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W4385574005",
    "https://openalex.org/W4376167329",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4315706637",
    "https://openalex.org/W2252278367",
    "https://openalex.org/W4297683418",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W4287993739",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4287019748",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W3137243147",
    "https://openalex.org/W2578576916",
    "https://openalex.org/W3033940819"
  ],
  "abstract": "Large language models (LLMs) excel in many tasks in NLP and beyond, but most open models have very limited coverage of smaller languages and LLM work tends to focus on languages where nearly unlimited data is available for pretraining. In this work, we study the challenges of creating LLMs for Finnish, a language spoken by less than 0.1% of the world population. We compile an extensive dataset of Finnish combining web crawls, news, social media and eBooks. We pursue two approaches to pretrain models: 1) we train seven monolingual models from scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the pretraining of the multilingual BLOOM model on a mix of its original training data and Finnish, resulting in a 176 billion parameter model we call BLUUMI. For model evaluation, we introduce FIN-bench, a version of BIG-bench with Finnish tasks. We also assess other model qualities such as toxicity and bias. Our models and tools are openly available at https://turkunlp.org/gpt3-finnish.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2710–2726\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nFinGPT: Large Generative Models for a Small Language\nRisto Luukkonen †∗ Ville Komulainen † Jouni Luoma † Anni Eskelinen †\nJenna Kanerva † Hanna-Mari Kupari † Filip Ginter † Veronika Laippala †\nNiklas Muennighoff ‡ Aleksandra Piktus ‡ Thomas Wang ‡ Nouamane Tazi ‡\nTeven Le Scao ‡ Thomas Wolf ‡ Osma Suominen ⋄ Samuli Sairanen ⋄\nMikko Merioksa ⋄ Jyrki Heinonen ⋄ Aija Vahtola ⋄ Samuel Antao ◦\nSampo Pyysalo †∗\n†TurkuNLP Group, University of Turku ‡Hugging Face\n⋄National Library of Finland ◦AMD\n∗risto.m.luukkonen@utu.fi, sampo.pyysalo@utu.fi\nAbstract\nLarge language models (LLMs) excel in many\ntasks in NLP and beyond, but most open mod-\nels have very limited coverage of smaller lan-\nguages and LLM work tends to focus on lan-\nguages where nearly unlimited data is avail-\nable for pretraining. In this work, we study the\nchallenges of creating LLMs for Finnish, a lan-\nguage spoken by less than 0.1% of the world\npopulation. We compile an extensive dataset\nof Finnish combining web crawls, news, social\nmedia and eBooks. We pursue two approaches\nto pretrain models: 1) we train seven mono-\nlingual models from scratch (186M to 13B\nparameters) dubbed FinGPT, 2) we continue\nthe pretraining of the multilingual BLOOM\nmodel on a mix of its original training data\nand Finnish, resulting in a 176 billion param-\neter model we call BLUUMI. For model eval-\nuation, we introduce FIN-bench, a version of\nBIG-bench with Finnish tasks. We also assess\nother model qualities such as toxicity and bias.\nOur models and tools are openly available at\nhttps://turkunlp.org/gpt3-finnish.\n1 Introduction\nNeural language models based on the Transformer\narchitecture (Vaswani et al., 2017) have revolution-\nized Natural Language Processing (NLP) in recent\nyears, advancing the state of the art in tasks ranging\nfrom text classiﬁcation to open-ended text gener-\nation. Generative, decoder-only language mod-\nels such as the Generative Pretrained Transformer\n(GPT) (Radford et al., 2018) series have been a\nparticular focus of interest in part due to their multi-\ntask and few-shot capabilities (Radford et al., 2019;\nBrown et al., 2020). The ability of such models to\nimplicitly learn to perform tasks that they have not\nbeen directly trained on has been considered to be\nclosely tied to the scale of the model (Brown et al.,\n2020; Chowdhery et al., 2022) and, perhaps even\nmore importantly, to the number of training tokens\n(Hoffmann et al., 2022; Muennighoff et al., 2023b;\nTouvron et al., 2023). Most work on such models\nfocuses on English, often entirely excluding other\nlanguages, and assumes that hundreds of billions\nof tokens of text are readily available for model\ntraining.\nIn this study, we consider the challenges of in-\ntroducing large generative models for Finnish, a\nUralic language natively spoken by fewer than 6\nmillion people. While the language is compara-\ntively well represented in online resources relative\nto this number, less than 1% of texts available in\ne.g. Wikipedia and Common Crawl are Finnish\n(Pyysalo et al., 2021; Xue et al., 2021). As the\nother members in the language family are either\neven smaller and lesser-resourced or quite distant,\nthe resources for creating models for the language\nare quite limited. Finnish has been represented to\nsome degree in Transformer-based models since\nthe release of the original multilingual BERT model\n(Devlin et al., 2019), and a dedicated monolingual\nBERT for the language was previously created by\nVirtanen et al. (2019). Also some generative mod-\nels for Finnish have been previously introduced by\nthe \"Finnish-NLP\" group 1 and Hatanpää (2022),\nbut as training LLMs is very expensive and Finnish\nis constrained by the size of available data, models\nexceeding a billion parameters have been so far\nmissing from the Finnish NLP landscape.\nWe compile a broad-coverage dataset of Finnish\nand train monolingual models up to 13 billion pa-\nrameters for 300 billion tokens (approx. 8 epochs).\nWe also perform continued pretraining of the 176-\nbillion parameter BLOOM model (Scao et al.,\n2022a) to extend its coverage of Finnish, intro-\nduce novel evaluation datasets, and assess multiple\n1https://huggingface.co/Finnish-NLP\n2710\nModel Layers Dim Heads Params\nSmall 12 768 12 186M\nMedium 24 1024 16 437M\nLarge 24 1536 16 881M\nXL 24 2064 24 1.5B\n3B 32 2560 32 2.8B\n8B 32 4096 32 7.5B\n13B 40 5120 40 13.3B\nBLUUMI 70 14336 112 176B\nTable 1: Architectures of our models.\naspects of the resulting models. While the details\nof our data collection and processing are somewhat\nspeciﬁc to Finnish, we believe that our study can\nserve as a template for training large models for\nother small languages.\n2 Models\nOur models are based on the GPT architec-\nture (Radford et al., 2019) and we follow the pre-\ntraining approach developed for the BLOOM fam-\nily of large multilingual language models (Scao\net al., 2022a). We train monolingual Finnish mod-\nels with up to 13 billion parameters from scratch,\nfollowing GPT-3 (Brown et al., 2020) in terms of\nthe number of layers, dimensionality, and num-\nber of attention heads (Table 1), and BLOOM in\nterms of both the software implementation as well\nas speciﬁc design choices such as the use of Al-\nibi position embeddings (Press et al., 2021) and\nlayer normalization (Scao et al., 2022b). We ad-\nditionally continue the pretraining of the original\n176-billion parameter BLOOM model with a mix\nof its original pretraining corpus and Finnish data\nto create a model we call BLUUMI. While the\nBLOOM models were trained on data from 46\ndifferent languages, the training did not include\nFinnish. Prior work has investigated extending\nsmaller BLOOM models to new languages not in-\ncluded during pretraining (Yong et al., 2022) and\nfound parameter-efﬁcient ﬁnetuning methods and\n(to a lesser degree) continued pretraining to be ef-\nfective approaches. Due to the fact that the 176-\nbillion parameter BLOOM model has been signiﬁ-\ncantly undertrained for its parameter count (Hoff-\nmann et al., 2022; Muennighoff et al., 2023b), we\nfocus on continued pretraining in this study.\n3 Data\nWe next present the sources of training data, pre-\nprocessing steps, data statistics and analysis.\n3.1 Data sources\nWe draw on a broad range of text sources, aiming\nto cover a wide range of linguistic variation across\ngenres, registers, authors and time periods. The\npretraining data sources are listed in Table 2 and\ndescribed below, and a summary of the timespans\nthey cover is given in Appendix A.\nParsebank The Finnish Internet Parsebank (Lu-\notolahti et al., 2015) is a 6 billion token corpus\nof Finnish collected in 2015-2016 from Common\nCrawl and a targeted Internet crawl seeded by\nthe .fi domain registry content and all URLs of\nFinnish material in Common Crawl. The texts\nhave been deduplicated at the paragraph level us-\ning Onion (Pomikálek, 2011) and cleaned using the\njusText library.2\nmC4 The multilingual colossal, cleaned version of\nCommon Crawl’s web crawl corpus (mC4) was in-\ntroduced by Xue et al. (2021) for training the mT5\nmodels. mC4 was derived from the 71 web scrapes\n(2013-2020) released by Common Crawl prior to\nthe creation of the corpus. We use the Finnish sub-\nset of mC4 as identiﬁed by cld33, which contains\n8 billion tokens across 19 million documents.\nCC-Fi To maximize coverage of Finnish text in\nCommon Crawl resources, we applied a custom\nextraction process to all crawls from 2013-2022,\nemphasizing recall of Finnish.4 We extracted texts\nusing Traﬁlatura (Barbaresi, 2021) and performed\nexact document-level deduplication using Mur-\nmurHash prior to the general preprocessing steps\ndescribed below. This processing produced 55 mil-\nlion documents totaling 20 billion tokens.\nFiwiki The Finnish portion of the Wikipedia free\nencyclopedia consists of approximately 180,000\nopenly licensed articles created by volunteer ed-\nitors. For this work, we extracted text from the\n20221120 dump of the Finnish Wikipedia using\nWikiExtractor (Attardi, 2015), producing a dataset\nof 110 million tokens.\nLönnrot Projekti Lönnrot5 is a project digitizing\nout-of-copyright Finnish and Swedish literature.\nFor this work, we used the 2574 Finnish works that\nwere published by Projekti Lönnrot by the start of\npretraining, which contain a total of 125 million\ntokens.\nYle Archives of the national public broadcasting\n2https://github.com/miso-belica/jusText\n3https://github.com/google/cld3\n4Appendix B provides a comparison of the two datasets\nderived from Common Crawl.\n5http://www.lonnrot.net/\n2711\nAbbrev. Name Reference\nParsebank Finnish Internet Parsebank https://turkunlp.org/finnish_nlp.html\nmC4 multilingual colossal, cleaned Common Crawl https://huggingface.co/datasets/mc4\nCC-Fi Common Crawl Finnish https://github.com/TurkuNLP/CC-Fi\nFiwiki Finnish Wikipedia https://fi.wikipedia.org/wiki\nLönnrot Projekti Lönnrot http://www.lonnrot.net\nePub National library \"epub\" collection https://kansalliskirjasto.finna.fi\nLehdet National library \"lehdet\" collection https://kansalliskirjasto.finna.fi\nSuomi24 The Suomi 24 Corpus 2001-2020 http://urn.fi/urn:nbn:fi:lb-2021101527\nReddit-Fi Reddit r/Suomi submissions and comments https://www.reddit.com/r/Suomi\nSTT Finnish News Agency Archive 1992-2018 http://urn.fi/urn:nbn:fi:lb-2019041501\nYle\nYle Finnish News Archive 2011-2018 http://urn.fi/urn:nbn:fi:lb-2017070501\nYle Finnish News Archive 2019-2020 http://urn.fi/urn:nbn:fi:lb-2021050401\nYle News Archive Easy-to-read Finnish 2011-2018 http://urn.fi/urn:nbn:fi:lb-2019050901\nYle News Archive Easy-to-read Finnish 2019-2020 http://urn.fi/urn:nbn:fi:lb-2021050701\nROOTS Responsible Open-science Open-collaboration Text Sources https://huggingface.co/bigscience-data\nTable 2: Data sources.\ncompany of Finland (Yle) are available for research\nthrough the Language Bank of Finland 6. We use\nthe complete Yle archives available at the start of\nour model pretraining, which consist of approxi-\nmately 800,000 articles (220 million tokens) from\n2011-2020, of which 0.3% are easy-to-read news.\nSTT As for Yle, archives of the Finnish News\nAgency (Suomen Tietotoimisto or STT) are pro-\nvided for research through the Language Bank of\nFinland. The collection available at the start of\nthis study spans publications from 1992-2018 and\ncontains 2.8 million newswire articles which total\napproximately 300 million tokens.\nePub The National Library of Finland maintains a\ncollection of electronically published books in Fin-\nland. For the purposes of this project, the library\ngranted access to its ePub collection of approxi-\nmately 30,000 Finnish eBook contents. As these\nbooks remain copyrighted, it is not possible to re-\ndistribute texts from this dataset.\nLehdet The Lehdet dataset is based on archived\nHTML material collected by the National Library\nof Finland and includes daily, weekly and monthly\ncrawls of newspaper internet sites and also a yearly\n.fi-domain crawl covering years from 2015 to\n2021. The total cleaned dataset consists of 85 bil-\nlion characters from 60 million HTML documents.\nThe dataset was provided by the National Library\nand can not be redistributed due to copyright.\nSuomi24 Archives of the largest social network-\ning site in Finland, Suomi24, 7 are available for\nresearch via the Language Bank of Finland. For\nthis study, we downloaded the complete archives\n6https://www.kielipankki.fi/\n7https://www.suomi24.fi\navailable at the time, consisting of 95 million com-\nments and 5 billion words from 2001-2020.\nReddit-Fi The social site Reddit includes a few pre-\ndominantly Finnish-language discussion forums.\nFor this work, we downloaded Reddit archives8 and\nextracted text from posts to r/Suomi,9 the largest\nsuch forum. The dataset contains over 150,000 sub-\nmissions and nearly 4 million comments (in total\n150 million tokens) from 2009-2022.\nROOTS The Responsible Open-science Open-\ncollaboration Text Sources (ROOTS) dataset (Lau-\nrençon et al., 2022) consists of 1.6 terabytes of\ntext data spanning 59 languages used for pretrain-\ning BLOOM (Scao et al., 2022a). While Finnish\nwas not included as an ofﬁcial language, a con-\ntamination analysis found 0.03% of ROOTS to be\nFinnish (Muennighoff et al., 2022). We use ROOTS\nin the continued pretraining of the BLOOM model,\nbut not for the monolingual Finnish models.\n3.2 Preprocessing\nWe next brieﬂy describe the preprocessing steps\nperformed for the source datasets. All processing\nscripts, parameters, and models are available along\nwith detailed statistics at https://github.com/\nTurkuNLP/finngen-tools.\nDeduplication In addition to the deduplication\nsteps already performed for some of the datasets\n(see Section 3.1), we performed approximate N-\ngram overlap-based deduplication using Onion\n(Pomikálek, 2011) separately for all datasets. We\nrun Onion with default parameters, marking as du-\nplicate any line of text (paragraph, title, etc.) where\nat least 50% of N-grams have appeared previously.\n8https://files.pushshift.io/reddit/\n9https://www.reddit.com/r/Suomi\n2712\nDataset Chars Ratio Weight W.Ratio\nParsebank 35.0B 16.9% 1.5 22.7%\nmC4-Fi 46.3B 22.4% 1.0 20.0%\nCC-Fi 79.6B 38.5% 1.0 34.4%\nFiwiki 0.8B 0.4% 3.0 1.0%\nLönnrot 0.8B 0.4% 3.0 1.0%\nYle 1.6B 0.8% 2.0 1.4%\nSTT 2.2B 1.1% 2.0 1.9%\nePub 13.5B 6.5% 1.0 5.8%\nLehdet 5.8B 2.8% 1.0 2.5%\nSuomi24 20.6B 9.9% 1.0 8.9%\nReddit-Fi 0.7B 0.4% 1.0 0.3%\nTOTAL 207.0B 100.0% N/A 100.0%\nTable 3: Preprocessed data statistics, weights, and ra-\ntios by source. The data is graphed in Appendix E.\nWe then trim duplicate lines from the beginning and\nend of each document. Finally, if at least 50% of\nthe remaining lines in the document are duplicates,\nwe discard the entire document.\nHeuristic ﬁltering To ﬁlter out texts that are un-\nlikely to be Finnish prose text, we apply a set of\nrule-based ﬁlters, extending on the heuristics in-\ntroduced by Virtanen et al. (2019). In short, these\nﬁlters remove texts that have e.g. an unusually high\nratio of punctuation or digits to alphabetic charac-\nters, a high ratio of non-Finnish to Finnish alpha-\nbetic characters, a low type-token ratio, or a low\naverage line length. This step removed only a small\nproportion of texts, with more than 95% of texts\nremaining in most resources.\nN-gram model ﬁltering To further remove texts\nthat have the surface characteristics of prose text\nbut are unlikely to represent standard Finnish, we\napplied a perplexity ﬁlter using an N-gram model.\nWe ﬁrst trained a KenLM (Heaﬁeld, 2011) model\non the set of known good Finnish texts prepared by\nVirtanen et al. (2019) for training their FinBERT\nmodel and then applied this model to documents,\nremoving lines with perplexity > 100 000. This\nﬁlter was not applied to sources estimated to be\npredominantly well-edited text (news, Lönnrot, and\nWikipedia). For the three web crawl datasets, the\nﬁlter removed 15-20% of text; for the social media\ndatasets, this proportion was 2-5%.\nToxicity ﬁltering To reduce the proportion of\ntexts that contain e.g. obscenities or identity attacks,\nwe applied the Finnish toxicity detection classiﬁer\nintroduced by Eskelinen et al. (2023). The clas-\nsiﬁer is a FinBERT model (Virtanen et al., 2019)\nﬁne-tuned on a machine-translated version of the\nRegister Parsebank mC4-Fi CC-Fi\nNarrative 42% 41% 31%\nDiscussion 15% 7% 7%\nInformational description 14% 13% 19%\nMachine translation <1% 3% 4%\nInformational Persuasion 5% 10% 14%\nOpinion 10% 7% 5%\nHow-to 2% 3% 4%\nSpoken <1% <1% <1%\nLyrical <1% <1% <1%\nHybrid 1% 1% <1%\nNo label 9% 13% 14%\nTable 4: Register proportions in the web-crawled\ndatasets. Hybrid refers to texts predicted with several\nregister labels.\nJigsaw Toxicity dataset10. The ﬁlter was not ap-\nplied to news, Lönnrot books, or Wikipedia. Toxi-\ncity ﬁltering removed 1-5% of sources other than\nCC-Fi, but as much as 23% of the CC-Fi text. This\neffect may be explained by the fact that CC-Fi was\nthe only web source that had not previously been\nﬁltered for e.g. obscenity.\nMasking personal data We applied a set of\nhigh-recall regular expressions and rule-based\nscripts to mask personal data such as email ad-\ndresses and potential phone numbers. These scripts\nimpacted approximately 0.2% of characters in total.\nTokenization We train a new monolingual\nFinnish tokenizer on a sample of the pretraining\ndata using the tokenizers library11. We follow the\nBLOOM recipe for the tokenizer, creating a byte-\nlevel BPE tokenizer without Unicode normaliza-\ntion and use the same regular expression-based\npre-tokenization as in BLOOM. As Finnish is an\nagglutinative language with complex morphology\nand thus a high number of word forms, we chose\nto create a comparatively large vocabulary for a\nmonolingual tokenizer of 131,072 tokens.\n3.3 Data statistics\nThe statistics of the ﬁnal dataset after preprocess-\ning are presented in Table 3. We oversample open\nand high-quality resources such as Lönnrot and\nWikipedia. In total, the ﬁnal pretraining dataset (in-\ncluding oversampling) consists of 38 billion tokens\nwhen processed with our Finnish tokenizer.\n10https://www.kaggle.com/c/\njigsaw-toxic-comment-classification-challenge\n11https://github.com/huggingface/tokenizers\n2713\nBatch size\nModel Samples Tokens LR\nSmall 256 524288 6.0 × 10−4\nMedium 256 524288 3.0 × 10−4\nLarge 256 524288 2.5 × 10−4\nXL 512 1048576 2.0 × 10−4\n3B 512 1048576 1.6 × 10−4\n8B 1024 2097152 1.2 × 10−4\n13B 1024 2097152 1.0 × 10−4\nBLUUMI 2048 4194304 6.0 × 10−5\nTable 5: Pretraining hyperparameters.\n3.4 Register analysis\nWe characterize the contents of the Web-based\ndatasets (mC4, CC-Fi and Parsebank) by automat-\nically analyzing their distribution of text registers\n(or genres) (Biber, 1988). To this end, we apply\na register identiﬁcation model based on the Fin-\nCORE corpus, trained using XLM-R (Conneau\net al., 2020). The model and corpus were both\npresented by Skantsi and Laippala (2022). The reg-\nister categories present text varieties with different\ncharacteristics and communicative objectives, such\nas narrative, interactive discussionand lyrical. Ta-\nble 4 presents the proportions of the registers in the\nthree datasets. We see a broadly similar register dis-\ntribution across the datasets, with narrative clearly\nmost frequent in all three and categories such as\nhow-to, spoken and lyrical representing only small\nfractions of the total.\n4 Pretraining\nThis work leverages the LUMI supercomputer,12 as\nof this writing the third-largest and seventh greenest\nin the world (Strohmaier et al., 2023). The LUMI\ndata center allows power consumption to be fully\nsupplied with hydroelectricity, and waste heat pro-\nduced by LUMI is utilized by the city of Kajaani,\nproviding up to 20% of the district heating.\nTraining was done on up to 192 nodes, each\nconsisting of 4 AMD Instinct MI250X GPUs, a\nsingle 64-core AMD Trento CPU and 512GB of\nmemory. Since the MI250X GPU is a multi-chip\nmodule with two Graphics Compute Dies (GCDs),\neach node can be considered to have 8 GPUs in\ntotal. In this perspective, the training utilized up to\n1536 GPUs. The 64-core CPU is conﬁgured as 4\nNUMA nodes linked to the GPUs. Because of a\n“low noise” mode used on the nodes, only 63 cores\nwere available for training.\n12https://www.lumi-supercomputer.eu/\nLoss\n3.0\n3.5\n4.0\n4.5\n5.0\nTokens\n0K 50B 100B 150B 200B 250B 300B\nSmall Medium Large XL 3B 8B 13B\n0K 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n0.000\n2.000\n4.000\n0K 0K 0K\nSmall\nMedium\nLarge\nXL\n3B\n8B\n13B\nFigure 1: Validation losses with 5-point moving aver-\nage smoothing.\nWe train our models on an adapted version\nof BLOOM’s pretraining framework, Megatron-\nDeepSpeed.13 By combining features from Mega-\ntron (Shoeybi et al., 2019) and DeepSpeed (Rasley\net al., 2020), the Megatron-DeepSpeed framework\ncan be used for training large language models\nwith pipeline, tensor and data parallelization across\nGPUs and compute nodes. Our changes to the\nframework involve making the codebase, includ-\ning its optimized CUDA kernels, usable on AMD\nMI250X GPUs using PyTorch ROCm. To lever-\nage the capabilities of MI250X, ROCm enables\nthe use of GPU matrix cores through its rocBLAS\nand MIOpen library implementations that, in turn,\nare leveraged by PyTorch. PyTorch also leverages\nthe RCCL library to implement distributed collec-\ntives. RCCL also uses a HIP port of the AWS\nOpenFabrics Interface (OFI) plugin 14 to enable\ncommunication directly through to the Slingshot\nfabric provider for improved performance at scale.\nFor the monolingual Finnish models trained\nfrom scratch, we follow Brown et al. (2020) also in\nsetting the batch size and maximum learning rate in\naddition to the model architecture parameters. For\nthe continued pretraining of BLOOM to create the\nBLUUMI model, we retain the original BLOOM\nparameters (Scao et al., 2022a). The pretraining\nparameter values are shown in Table 5.\nFigure 1 shows the loss curves for held-out val-\nidation data for the models trained from scratch,\nshowing a stable pretraining process for all models\nand the expected pattern of larger models achieving\nlower loss.\n13https://github.com/TurkuNLP/\nMegatron-DeepSpeed\n14https://github.com/ROCmSoftwarePlatform/\naws-ofi-rccl\n2714\n5 Evaluation\nWe next present a few-shot evaluation dataset for\nFinnish and compare the capability of the models\nusing this data. We additionally assess model align-\nment, bias, and toxicity in separate evaluations.\n5.1 FIN-bench dataset\nBIG-bench (Srivastava et al., 2022) is a collection\nof tasks created to assess various aspects of model\ncapabilities. For this study, we created a similar\nFinnish evaluation dataset, FIN-bench,15 based on\na BIG-bench subset augmented with newly intro-\nduced tasks. The tasks were primaly generated by\nmachine translating the text of the equivalent BIG-\nbench tasks and subsequently correcting any trans-\nlation errors as well as assuring that the questions\nremain culturally relevant to Finnish. Exceptions\ninclude the Arithmetic tasks (generated data) and\nnew tasks (Paraphrase, Analogy, Emotions). The\nFIN-bench dataset contains 3919 examples in total,\ndivided over the tasks described brieﬂy below. Ex-\namples of the tasks can be found from Appendix G.\nAnalogy Analogies of the type Paris is to France\nas Helsinki is to . . .represent a well-established\napproach for evaluating language models. We cre-\nated an analogy dataset using templates to reformu-\nlate analogy quadruples into natural language ques-\ntions. We created 130 examples from the dataset\nof Venekoski and Vankka (2017) and the data of\nMikolov et al. (2013) translated to Finnish.\nArithmetic tests the degree to which a model has\nacquired an ability to perform basic one- to ﬁve-\ndigit addition, subtraction, multiplication and di-\nvision. The Finnish variant of the task was auto-\nmatically generated by manually translating the\ntemplates in the scripts for the corresponding BIG-\nbench task and consists of 1923 examples in total.\nCause and effect evaluates a model’s ability to\nreason about the causality of two events. Each\nexample states two events, the cause and the effect,\nand the model is asked to select the correct ordering.\nThe task consists of 153 examples.\nEmotions evaluates the ability of a model to clas-\nsify sentences according to the emotion that they\nexpress. The task is derived from the XED dataset\n(Öhman et al., 2020) by selecting examples of at\nleast ﬁve words that have exactly one emotion label\nand then manually ﬁltering a random selection of\nthese to identify 160 examples that a human an-\n15https://github.com/TurkuNLP/FIN-bench\nnotator without refrerence to speciﬁc annotation\ninstructions would be expected to label correctly.\nEmpirical judgments measures how well a model\ncan distinguish sentences that express a causal re-\nlation from ones that express a correlative relation.\nThe task also contains neutral passages of text that\nmimic the structure of the sentences containing a\ncorrelative or causal relation, but do not contain\neither. There are 33 examples of each category in\nthe task, i.e. 99 in total.\nGeneral knowledge measures the ability of mod-\nels to answer simple questions which can easily be\nanswered by most people, such as “How many legs\ndoes a horse have?”. The task is a translation of the\n70 examples in the BIG-bench original for all but\nthree questions regarding imperial unit conversion,\nwhich we replace with questions on metric units.\nIntent recognition tests the logical reasoning of\nmodels by measuring how well they can recognize\nthe correct intent from an input. The task may be a\ngood predictor of performance in task-oriented dia-\nlogue systems. It includes 693 translated examples\noriginally from the dataset introduced by Coucke\net al. (2018).\nMisconceptions assesses a model’s ability to dis-\ntinguish popular misconceptions from facts; mod-\nels trained on increasingly bigger datasets of mixed-\nquality internet data may not discern between com-\nmon assertions and ones that are true. Translations\nof this task were heavily ﬁltered by our annotators\ndue to being considered culturally too U.S.-centric.\nApproximately 40% of the original questions were\nremoved from the dataset, resulting in a task with\n134 examples.\nParaphrase tests whether a model can distinguish\nfull paraphrases from sentences that are merely sim-\nilar. The task was created by selecting 100 positive\nand 100 negative examples from the Finnish Para-\nphrase Corpus (Kanerva et al., 2021), emphasizing\ncases that people can categorize without reference\nto the speciﬁcs of the corpus annotation guidelines.\nSentence ambiguity evaluates to what degree a\nmodel can identify whether sentences with inten-\ntionally introduced ambiguous aspects state a true\nor false claim. The task consists of 60 examples\ntranslated from BIG-bench.\nSimilarities abstraction measures a model’s abil-\nity to identify human-like abstract associations be-\ntween objects: for example, a dog and a parakeet\nare similar in that they are both pets. The data\nconsists of 76 multiple-choice questions.\n2715\n108 109 1010 1011\nParameter count\n30%\n35%\n40%\n45%\n50%\n55%\n60%Aggregate normalized performance\nRandom baseline\nTurkuNLP (3-shot)\nTurkuNLP (2-shot)\nTurkuNLP (1-shot)\nTurkuNLP (0-shot)\nHatanpää (3-shot)\nHatanpää (2-shot)\nHatanpää (1-shot)\nHatanpää (0-shot)\nFinnishNLP (3-shot)\nFinnishNLP (2-shot)\nFinnishNLP (1-shot)\nFinnishNLP (0-shot)\nFigure 2: Overall FIN-bench evaluation results. Detailed per-task results are in Appendix F.\n5.2 Few-shot results\nWe evaluate models on FIN-bench in zero- to three-\nshot settings and summarize results using mean ac-\ncuracy across all tasks. For tasks that are organized\ninto subtasks (Cause and effect and Arithmetic),\nwe ﬁrst average over the subtasks before taking\nthe overall average. Primary evaluation results are\nvisualized in Figure 2.\nWe ﬁnd that our monolingual models at least\nmatch and in most instances outperform the results\nof previously released Finnish models of compara-\nble sizes, lending support to the choices we have\nmade for data selection and preprocessing as well\nas the model architecture and pretraining process.\nThe best performance of the models released previ-\nously for Finnish, 38.5%, is achieved by the largest\nmodel introduced by Hatanpää (2022). Our best\nmonolingual model outperforms this result by over\n10% points and the BLUUMI model by over 20%\npoints, representing a substantial advance in the\nstate of the art in the capability of generative mod-\nels trained for Finnish.\nAs expected, overall performance generally in-\ncreases with the number of in-context examples\n(zero to three shots) as well as with model size, with\nsome exceptions. First, some small models break\nthe expected pattern, showing better zero-shot per-\nformance than one- to three-shot. This could be\nrelated to a tendency of less capable models to sim-\nply repeat patterns from preceding context, which\ncan lead the models to copy whatever appears after\n“Answer:” (or equivalent) in the preceding few-shot\nBLOOM BLUUMI\n0%\n10%\n20%\n30%\n40%\n50%\n60%Accuracy\n0-shot\n1-shot\n2-shot\n3-shot\nFigure 3: BLOOM and BLUUMI performance on FIN-\nbench with random baseline (dotted line).\nexamples. Second, we notice a consistent drop in\nperformance between our 8B and 13B parameter\nmodels. This may be caused by overﬁtting due\nto an excessive number of parameters and train-\ning steps compared to a relatively small amount of\n(non-repeated) text, which can lead to decreasing\nperformance (Muennighoff et al., 2023b). Based\non these results, we estimate that the 8B parame-\nter model may be our most capable monolingual\nmodel and, more generally, that approximately 10B\nparameters may represent a limit for effectively\ntraining monolingual models of this type for lan-\nguages whose resources are broadly comparable to\nthose available for Finnish.\nTo further evaluate the BLUUMI model, we\ncompared its performance to that of the original\nBLOOM model on FIN-bench (Figure 3) and on\nEnglish tasks from the EleutherAI evaluation har-\n2716\n0% 20% 40% 60% 80%\nAccuracy\nwsc\nwnli\nwinogrande\nwic\nwebqs\ntriviaqa\nsst\nsciq\nrte\nrace\nqqp\nqnli\npubmedqa\nprost\npiqa\nopenbookqa\nmultirc\nmrpc\nmathqa\nlogiqa\nlambada\nhellaswag\nheadqa\ncopa\nboolq\narc_easy\narc_challengeT ask\nBLOOM\nBLUUMI\nFigure 4: 176B model performance on English evalua-\ntions.\nness (Gao et al., 2021) (Figure 4). We ﬁnd that\nBLUUMI performs notably better than BLOOM\non FIN-bench tasks on all the few-shot evaluation\ntests, with a 12-18% point accuracy difference in\nfavor of BLUUMI. On the English tasks, we ﬁnd\nno signiﬁcant difference in performance between\nthe original BLOOM and BLUUMI (two-sided t-\ntest). These results indicate that the continued pre-\ntraining has succeeded in substantially improving\nthe Finnish capabilities of the model without com-\npromising the existing English capabilities of the\noriginal model.\n5.3 Alignment\nWe assess model alignment using the BIG-bench\nHHH alignment task (Askell et al., 2021), which\nincludes four categories: harmlessness, honesty,\nhelpfulness, and other. In contrast to most other\ntasks in BIG-bench, both of the two choices in each\nexample can be considered correct: for instance,\nwhen assessing harmlessness, it is undesirable for a\nmodel to provide instructions for violent acts, and\nrefusing to help is considered the correct answer.\nWe create a Finnish version of the HHH alignment\ntask through initial machine tranlation and manual\ncorrection, and evaluate models using the same pro-\ncess as for the other BIG-bench tasks. Results are\nshown in Figure 5. We ﬁnd that all models perform\npoorly at these tasks, only exceeding the random\nbaseline for the other category and measuring par-\n20% 30% 40% 50% 60%\nFinnishNLP/small\nFinnishNLP/medium\nFinnishNLP/large\nHatanpää/small\nHatanpää/distill\nHatanpää/xl\nTurkuNLP/small\nTurkuNLP/medium\nTurkuNLP/large\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nTurkuNLP/BLUUMI\nBLOOM\nHelpful\nHonest\nHarmless\nOther\nFigure 5: HHH-alignment of all models with random\nbaseline (dotted line).\nticularly low for helpfulness. While it is not surpris-\ning that base models that have not been speciﬁcally\ntrained to follow instructions or operate in a di-\nalogue context score low at this task, the results\nemhasize the need to align the models to assure\nthat their output is helpful, harmless, and more fac-\ntually accurate. We note that although there appear\nto be some correlations between model size and\nHHH performance, all differences remain within\none standard deviation and are not signiﬁcant.\n5.4 Bias\nLanguage models have an established tendency\nto repeat or amplify biases present in training\ndata. As one example of bias, female/male gender\nstereotypes in models is a concern because their\nwidespread use can result in further amplifying\nthese biases (Bolukbasi et al., 2016). We assessed\nthe occurrence of such bias using prompts with\nthe structure “The name of the [professional or oc-\ncupation holder] was” and categorized predicted\nnames into male or female when the name had that\nassociation in 95% of cases in national statistics.\nThe distribution predicted by the model was then\ncompared to the distribution in the most recent\npublished labor data records published by Statis-\ntics Finland in 2020.16 As illustrated in Figure 6\nand detailed in Appendix C, the model broadly re-\nﬂects the actual labor distribution, indicating that\n16https://tilastokeskus.fi/julkaisu/\ncktws35s04dru0b553lzi7aci\n2717\n0% 20% 40% 60% 80% 100%\nStatistics\nPredicted\nStatistics\nPredicted\nStatistics\nPredicted\nStatistics\nPredicted\nStatistics\nPredicted\nStatistics\nPredicted\nStatistics\nPredicted Male\nFemale\ncargo handler\nsales representative\nhome aid\noffice cleaner\nregistered nurse\npratical nurse\nseller\nFigure 6: Gender bias of 13B model predictions on oc-\ncupation holder vs statistics from the Statistics Finland.\nit has learned this bias from the pretraining data.\nWe note that while this is just one example of a\ntype of bias that our models (as well as most other\npresent-day models) can learn in their pretraining,\nit demonstrates why such models should not be\nnaively applied e.g. for hiring decisions (see also\nLimitations below).\n5.5 Toxicity\nTo test to what degree our models are prone\nto generating toxic content, we follow the un-\nprompted generation approach of Gehman et al.\n(2020), prompting the models with only their end-\nof-sequence (EOS) token to signal the start of a\nnew context.17 The unprompted generations were\nthen classiﬁed for toxic content using the model\nintroduced by Eskelinen et al. (2023) (see also Sec-\ntion 3.2) and a small sample manually assessed to\nassure labeling quality. The results of this evalu-\nation are summarized in Figure 7. We ﬁnd that\nour models more than halve the fraction of gener-\nated toxic content when compared to models from\nHatanpää (2022), which were trained without ﬁl-\ntering pretraining texts for toxicity. Our models\nnevertheless produce unprompted toxic generations\napprox. 2% of the time, reﬂecting remaining chal-\nlenges in their alignment.\n6 Discussion and conclusions\nIn this study, we compiled an extensive dataset of\nFinnish and created in total eight new large lan-\n17FinnishNLP-models were left out of this evaluation as\nthey appear to have been trained without an EOS token.\n0% 1% 2% 3% 4% 5%\nRatio of toxic generations\nHatanpää/small\nHatanpää/xl\nTurkuNLP/small\nTurkuNLP/medium\nTurkuNLP/large\nTurkuNLP/xl\nTurkuNLP/3B\nTurkuNLP/8B\nTurkuNLP/13B\nFigure 7: Unprompted toxicity of Finnish models. De-\ntailed scores are in Appendix D.\nguage models: seven monolingual Finnish models\nranging from 185 million to 13 billion parameters\nand a multilingual 176-billion parameter model,\nBLUUMI. We additionally introduced a new evalu-\nation dataset, FIN-bench, and evaluated the models\nin few-shot settings as well as speciﬁcally assessed\ntheir alignment, bias and toxicity. We found that\nour models are substantially more capable than\nprior Finnish models and that continued pretrain-\ning has greatly improved the Finnish capability of\nBLUUMI without compromising its existing En-\nglish capabilities. We also demonstrated limitations\nof the models in terms of their alignment, incorpo-\nration of bias, and remaining tendency to generate\ntoxic content, which we aim to address in future\nwork. We hope our models will serve as foundation\nmodels for Finnish that can be used in research and\nleveraged through instruction ﬁnetuning and other\nalignment methods (Ouyang et al., 2022) to create\na range of capable tools for processing Finnish text.\nIn future work, we hope to continue our study of ef-\nﬁcient and environmentally sustainable approaches\nfor creating capable open foundation models for\nlesser-resourced languages.\nAcknowledgments\nThe authors wish to acknowledge CSC – IT Cen-\nter for Science, Finland, for generous computa-\ntional resources on the LUMI supercomputer. This\nproject has received funding from the European\nUnion’s Horizon Europe research and innovation\nprogramme under Grant agreement No 101070350\nand the Finnish Research Council, grant number\n331297. The contents of this publication are the\nsole responsibility of its authors and do not neces-\nsarily reﬂect the opinion of the European Union.\n2718\nLimitations\nThe models introduced in this work are trained pre-\ndominantly on data sourced from the internet, and\ndespite our efforts to remove potentially harmful\ntexts from the pretraining data, they carry many\nof the well-established limitations of such models\n(Bender et al., 2021; Weidinger et al., 2021). In our\nevaluation, we have experimentally demonstrated\nspeciﬁc limitations in terms of model alignment\n(Section 5.3), bias (Section 5.4), and toxicity (Sec-\ntion 5.5). While the introduced models notably im-\nprove over the capabilities of previously released\nmodels in a range of Finnish tasks, due to these\nand other limitations the models should primarily\nbe considered resources for research and a poten-\ntial foundation for tools and applications, but they\nshould not be used as-is for user-facing applica-\ntions or for any task with potential for high impact\non people’s rights or well-being, such as hiring\ndecisions. Substantial further work is likely to be\nrequired to create versions of the models that can\nbe assured to be well aligned, free of bias, and not\nprone to generating toxic output.\nOur work focuses on large models for a lesser-\nresourced language, and the amount of Finnish text\navailable for model pretraining is a fundamental\nlimitation of our work. Despite drawing on a broad\nrange of sources, it was not possible to assemble\nenough text to avoid multiple epochs over the data\nto match the GPT-3 pretraining process, and the\nrepetition of data may be reﬂected in reduced capa-\nbility, especially for the largest monolingual model\n(Section 5.2). The challenges of collecting suf-\nﬁcient high-quality Finnish text for large model\ntraining also forced us to make a choice between\ndata quality and quantity on the one hand and repli-\ncability on the other. We chose to partly train on\ntexts provided by the National Library of Finland\nas part of a research collaboration. While these\nare some of the highest-quality texts in our dataset,\nthey cannot be readily redistributed, and complete\nreplication of our work is thus impossible without\nthe involvement of the national library. While we\nregret this limitation, we note that lack of access\nto complete pretraining data is a negative aspect\nthat our models share with many other present-day\nmodels. Future work may consider increasing the\navailable data via augmentation techniques (Dhole\net al., 2021) or mixing with data from a different\nmodality such as code (Muennighoff et al., 2023b,a;\nAllal et al., 2023; Li et al., 2023).\nReferences\nLoubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Munoz\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlex Gu, Manan Dey, et al. 2023. Santa-\ncoder: don’t reach for the stars! arXiv preprint\narXiv:2301.03988.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn\nDrain, Deep Ganguli, Tom Henighan, Andy Jones,\nNicholas Joseph, Ben Mann, Nova DasSarma, et al.\n2021. A general language assistant as a laboratory\nfor alignment. arXiv preprint arXiv:2112.00861.\nGiusepppe Attardi. 2015. Wikiextractor. https://\ngithub.com/attardi/wikiextractor.\nAdrien Barbaresi. 2021. Traﬁlatura: A web scraping li-\nbrary and command-line tool for text discovery and\nextraction. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing: System Demonstrations,\npages 122–131.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nDouglas Biber. 1988. Variation across speech and writ-\ning. Cambridge University Press, Cambridge.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016.\nMan is to computer programmer as woman is to\nhomemaker? debiasing word embeddings. Ad-\nvances in neural information processing systems, 29.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\n2719\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlice Coucke, Alaa Saade, Adrien Ball, Théodore\nBluche, Alexandre Caulier, David Leroy, Clément\nDoumouro, Thibault Gisselbrecht, Francesco Calta-\ngirone, Thibaut Lavril, et al. 2018. Snips voice plat-\nform: an embedded spoken language understanding\nsystem for private-by-design voice interfaces. arXiv\npreprint arXiv:1805.10190.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nKaustubh D Dhole, Varun Gangal, Sebastian\nGehrmann, Aadesh Gupta, Zhenhao Li, Saad\nMahamood, Abinaya Mahendiran, Simon Mille,\nAshish Shrivastava, Samson Tan, et al. 2021.\nNl-augmenter: A framework for task-sensitive\nnatural language augmentation. arXiv preprint\narXiv:2112.02721.\nAnni Eskelinen, Laura Silvala, Filip Ginter, Sampo\nPyysalo, and Veronika Laippala. 2023. Toxicity de-\ntection in Finnish using machine translation. In Pro-\nceedings of the 24th Nordic Conference on Computa-\ntional Linguistics (NoDaLiDa), pages 685–697, Tór-\nshavn, Faroe Islands. University of Tartu Library.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPoﬁ, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\nJason Phang, Laria Reynolds, Eric Tang, Anish\nThite, Ben Wang, Kevin Wang, and Andy Zou. 2021.\nA framework for few-shot language model evalua-\ntion.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nVäinö Hatanpää. 2022. A generative pre-trained trans-\nformer model for Finnish. Master’s thesis, Aalto\nUniversity. School of Science.\nKenneth Heaﬁeld. 2011. KenLM: Faster and smaller\nlanguage model queries. In Proceedings of the sixth\nworkshop on statistical machine translation, pages\n187–197.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nJenna Kanerva, Filip Ginter, Li-Hsin Chang, Iiro Ras-\ntas, Valtteri Skantsi, Jemina Kilpeläinen, Hanna-\nMari Kupari, Jenna Saarni, Maija Sevón, and Otto\nTarkka. 2021. Finnish paraphrase corpus. In Pro-\nceedings of the 23rd Nordic Conference on Compu-\ntational Linguistics (NoDaLiDa 2021).\nHugo Laurençon, Lucile Saulnier, Thomas Wang,\nChristopher Akiki, Albert Villanova del Moral,\nTeven Le Scao, Leandro V on Werra, Chenghao Mou,\nEduardo González Ponferrada, Huu Nguyen, et al.\n2022. The BigScience ROOTS corpus: A 1.6 tb\ncomposite multilingual dataset. In Thirty-sixth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou,\nMarc Marone, Christopher Akiki, Jia Li, Jenny\nChim, et al. 2023. Starcoder: may the source be\nwith you! arXiv preprint arXiv:2305.06161.\nJuhani Luotolahti, Jenna Kanerva, Veronika Laippala,\nSampo Pyysalo, and Filip Ginter. 2015. Towards\nuniversal web parsebanks. In International Confer-\nence on Dependency Linguistics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems, volume 26. Curran Associates, Inc.\nNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai\nZheng, Binyuan Hui, Terry Yue Zhuo, Swayam\nSingh, Xiangru Tang, Leandro von Werra, and\nShayne Longpre. 2023a. Octopack: Instruction tun-\ning code large language models. arXiv preprint\narXiv:2308.07124.\nNiklas Muennighoff, Alexander M Rush, Boaz Barak,\nTeven Le Scao, Aleksandra Piktus, Nouamane Tazi,\nSampo Pyysalo, Thomas Wolf, and Colin Raffel.\n2023b. Scaling data-constrained language models.\narXiv preprint arXiv:2305.16264.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hai-\nley Schoelkopf, et al. 2022. Crosslingual general-\nization through multitask ﬁnetuning. arXiv preprint\narXiv:2211.01786.\nEmily Öhman, Marc Pàmies, Kaisla Kajava, and Jörg\nTiedemann. 2020. XED: A multilingual dataset for\nsentiment analysis and emotion detection. arXiv\npreprint arXiv:2011.01612.\n2720\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nJan Pomikálek. 2011. Removing boilerplate and du-\nplicate content from web corpora. Ph.D. thesis,\nMasaryk university, Faculty of informatics, Brno,\nCzech Republic.\nOﬁr Press, Noah A Smith, and Mike Lewis. 2021.\nTrain short, test long: Attention with linear biases\nenables input length extrapolation. arXiv preprint\narXiv:2108.12409.\nSampo Pyysalo, Jenna Kanerva, Antti Virtanen, and\nFilip Ginter. 2021. Wikibert models: Deep transfer\nlearning for many languages. In Proceedings of the\n23rd Nordic Conference on Computational Linguis-\ntics (NoDaLiDa), pages 1–10.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\nover 100 billion parameters. In Proceedings of the\n26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, pages 3505–\n3506.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Ro-\nman Castagné, Alexandra Sasha Luccioni, François\nYvon, et al. 2022a. Bloom: A 176b-parameter open-\naccess multilingual language model.\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Lu-\ncile Saulnier, Stas Bekman, M Saiful Bari, Stella\nBideman, Hady Elsahar, Niklas Muennighoff, Jason\nPhang, et al. 2022b. What language model to train\nif you have one million gpu hours? arXiv preprint\narXiv:2210.15424.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nValtteri Skantsi and Veronika Laippala. 2022. Analyz-\ning the unrestricted web: The ﬁnnish corpus of on-\nline registers. Nordic Journal of Linguistics.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint\narXiv:2206.04615.\nErich Strohmaier, Jack Dongarra, Horst Simon, Martin\nMeuer, and Hans Meuer. 2023. Top500 - the list.\nhttps://www.top500.org/.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and efﬁ-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in Neural Information Process-\ning Systems, 30.\nViljami Venekoski and Jouko Vankka. 2017. Finnish\nresources for evaluating language model semantics.\nIn Proceedings of the 21st Nordic Conference on\nComputational Linguistics, NoDaLiDa, 22-24 May\n2017, Gothenburg, Sweden, 131, pages 231–236.\nLinköping University Electronic Press, Linköpings\nuniversitet.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough:\nBert for ﬁnnish. arXiv preprint arXiv:1912.07076.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor\nGrifﬁn, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\net al. 2021. Ethical and social risks of harm from\nlanguage models. arXiv preprint arXiv:2112.04359.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2021. mt5: A massively\nmultilingual pre-trained text-to-text transformer. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 483–498.\nZheng-Xin Yong, Hailey Schoelkopf, Niklas Muen-\nnighoff, Alham Fikri Aji, David Ifeoluwa Ade-\nlani, Khalid Almubarak, M Saiful Bari, Lintang\nSutawika, Jungo Kasai, Ahmed Baruwa, et al.\n2022. BLOOM+ 1: Adding language support to\nbloom for zero-shot prompting. arXiv preprint\narXiv:2212.09535.\n2721\nA Timespan covered by Finnish datasets\nThe rough timespan covered by the Finnish datasets is summarized in the following ﬁgure, excluding the\nLönnrot dataset (0.4% of the data), which covers out-of-copyright literature and mostly consists of books\npublished before 1950. Due to the difﬁculty of assigning a publication date to web-based materials that\nmay be continuously edited, for these resources we report the timespan of their retrieval.\nDataset\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\nParsebank Retrieved\nmC4 Published\nCC-Fi\nFiwiki\nYle\nSTT\nePub\nLehdet\nSuomi24\nReddit-Fi\nB Comparison of mC4-Fi and CC-Fi datasets\nThe mC4-Fi and CC-Fi datasets are both derived from Common Crawl data, but cover different sets\nof crawls and apply different selection criteria and text extraction and ﬁltering pipelines. To assess the\noverlap of these two datasets after preprocessing, we ﬁrst compared the sets of URLs in the metadata\nof the two datasets, ﬁnding that 65% of the mC4-Fi URLs are also found in CC-Fi, while only 29% of\nCC-Fi URLs are also in mC4-Fi, indicating substantial differences in which documents are included and\nsuggesting that the processing to create the CC-Fi dataset was successful in increasing coverage of Finnish\ndocuments selected from Common Crawl resources compared to mC4-Fi.\nTo further assess textual overlap, we ﬁrst sampled 100,000 random URLs found in both datasets. For\neach URL we created the set of 5-grams from the document texts in mC4-Fi and CC-Fi as well as their\nintersection. We found that 73% of 5-grams in mC4-Fi overlap with those of the corresponding document\nin CC-Fi, and 84% of CC-Fi 5-grams appeared also in the mC4-Fi document. This indicates that while\nthe texts extracted from each matching document are highly similar in the two resources, they are not\nidentical, and the redundancy of these resources is thus lower than suggested by simple URL overlap.\nC Full gender bias results on 13B model\nOccupation Ammatti STurkuNLPce M F M (%) F (%)\nseller myyjä (s) Employment stats 35206 66315 34.68% 65.32%\nPredicted 243 68 78.14% 21.86%\npractical nurse lähihoitaja (s) Employment stats 8925 70851 11.19% 88.81%\nPredicted 0 370 0.00% 100.00%\nregistered nurse sairaanhoitaja (s) Employment stats 6342 66692 8.68% 91.32%\nPredicted 17 422 3.87% 96.13%\nofﬁce cleaner toimistosiivooja (s) Employment stats 10915 53098 17.05% 82.95%\nPredicted 334 156 68.16% 31.84%\nhome aid kodinhoitaja (s) Employment stats 6252 36482 14.63% 85.37%\nPredicted 25 337 6.91% 93.09%\nnanny lastenhoitaja (s) Employment stats 2013 38010 5.03% 94.97%\nPredicted 39 427 8.37% 91.63%\nsales representative myyntiedustaja (s) Employment stats 25534 13096 66.10% 33.90%\nPredicted 383 90 80.97% 19.03%\ncargo handler rahdinkäsittelijä (s) Employment stats 29129 7450 79.63% 20.37%\nPredicted 350 64 84.54% 15.46%\nhouse builder talonrakentaja Employment stats 32032 1976 94.19% 5.81%\nPredicted 502 3 99.41% 0.59%\nrestaurant attendant ravintolatyöntekijä Employment stats 11332 21799 34.20% 65.80%\nPredicted 173 137 55.81% 44.19%\nsecretary yleissihteeri Employment stats 4285 27767 13.37% 86.63%\nPredicted 265 74 78.17% 21.83%\n2722\nsoftware engineer sovellussuunnittelija Employment stats 25110 5705 81.49% 18.51%\nPredicted 433 71 85.91% 14.09%\nkindergarten teacher lastentarhanopettaja Employment stats 656 21077 3.02% 96.98%\nPredicted 69 431 13.80% 86.20%\nsoftware architect sovellusarkkitehti Employment stats 15220 5348 74.00% 26.00%\nPredicted 291 35 89.26% 10.74%\nagriculture machinist maatalouskoneasentaja Employment stats 18090 479 97.42% 2.58%\nPredicted 423 8 98.14% 1.86%\naccountant tilintarkastaja Employment stats 6445 11208 36.51% 63.49%\nPredicted 230 5 97.87% 2.13%\nteaching assistant koulunkäyntiavustaja Employment stats 2314 14038 14.15% 85.85%\nPredicted 1 386 0.26% 99.74%\ncarpenter kirvesmies Employment stats 15870 448 97.25% 2.75%\nPredicted 228 11 95.40% 4.60%\ndriver autonkuljettaja Employment stats 14006 2303 85.88% 14.12%\nPredicted 281 11 96.23% 3.77%\nbuilding electrician rakennus sähköasentaja Employment stats 14084 364 97.48% 2.52%\nPredicted 513 0 100.00% 0.00%\nplumber putkiasentaja Employment stats 13618 271 98.05% 1.95%\nPredicted 455 0 100.00% 0.00%\nsenior physician ylilääkäri Employment stats 5505 8354 39.72% 60.28%\nPredicted 204 21 90.67% 9.33%\nstore manager myymäläesimies Employment stats 4661 8004 36.80% 63.20%\nPredicted 371 62 85.68% 14.32%\nmachinist koneistaja Employment stats 11868 793 93.74% 6.26%\nPredicted 217 17 92.74% 7.26%\nfarmer maanviljelijä Employment stats 10331 2137 82.86% 17.14%\nPredicted 295 54 84.53% 15.47%\nstudy advisor opinto-ohjaaja Employment stats 3498 8737 28.59% 71.41%\nPredicted 7 509 1.36% 98.64%\nhairdresser kampaaja Employment stats 867 10473 7.65% 92.35%\nPredicted 1 379 0.26% 99.74%\nmailman postinkantaja Employment stats 6503 4258 60.43% 39.57%\nPredicted 163 17 90.56% 9.44%\ncoffee shop worker kahvilamyyjä Employment stats 1927 8824 17.92% 82.08%\nPredicted 51 153 25.00% 75.00%\nreal estate agent kiinteistönvälittäjä Employment stats 6496 4176 60.87% 39.13%\nPredicted 114 129 46.91% 53.09%\nbus driver linja-autonkuljettaja Employment stats 9099 1078 89.41% 10.59%\nPredicted 335 32 91.28% 8.72%\nguardsman vartija Employment stats 7496 2292 76.58% 23.42%\nPredicted 160 15 91.43% 8.57%\nbank worker pankkitoimihenkilö Employment stats 2145 7531 22.17% 77.83%\nPredicted 274 51 84.31% 15.69%\nelectrician sähköasentaja Employment stats 9343 312 96.77% 3.23%\nPredicted 480 0 100.00% 0.00%\nphysiotherapist fysioterapeutti Employment stats 2008 7502 21.11% 78.89%\nPredicted 73 174 29.55% 70.45%\nsales engineer myynti-insinööri Employment stats 6422 2362 73.11% 26.89%\nPredicted 434 32 93.13% 6.87%\nwaiter tarjoilija Employment stats 2191 6125 26.35% 73.65%\nPredicted 52 69 42.98% 57.02%\nspecial education teacher erityisopettaja Employment stats 1223 7027 14.82% 85.18%\nPredicted 48 405 10.60% 89.40%\ncareers adviser urasuunnittelija Employment stats 1584 6445 19.73% 80.27%\nPredicted 233 179 56.55% 43.45%\nstorekeeper kauppias Employment stats 4678 3326 58.45% 41.55%\nPredicted 309 75 80.47% 19.53%\nphysical education instructor liikunnanohjaaja Employment stats 2829 5025 36.02% 63.98%\nPredicted 96 396 19.51% 80.49%\nofﬁce secretary toimistosihteeri Employment stats 230 7393 3.02% 96.98%\nPredicted 150 347 30.18% 69.82%\npurchasing agent sisäänostaja Employment stats 4066 3456 54.05% 45.95%\nPredicted 140 44 76.09% 23.91%\nphysician yleislääkäri Employment stats 2882 4522 38.92% 61.08%\nPredicted 251 45 84.80% 15.20%\n2723\nD Toxicity scores\nModel Identity attack Insult Obscene Severe toxicity Threat Toxicity\nHatanpää/small 0.149 % 1.471 % 2.132 % 0.070 % 0.026 % 5.377 %\nHatanpää/xl 0.185 % 1.344 % 2.055 % 0.109 % 0.015 % 5.241 %\nTurkuNLP/small 0.039 % 0.208 % 0.435 % 0.004 % 0.008 % 1.658 %\nTurkuNLP/medium 0.048 % 0.248 % 0.410 % 0.002 % 0.011 % 1.896 %\nTurkuNLP/large 0.039 % 0.280 % 0.490 % 0.001 % 0.011 % 1.981 %\nTurkuNLP/xl 0.061 % 0.272 % 0.546 % 0.002 % 0.011 % 2.211 %\nTurkuNLP/3B 0.069 % 0.343 % 0.618 % 0.004 % 0.021 % 2.290 %\nTurkuNLP/8B 0.058 % 0.304 % 0.645 % 0.012 % 0.021 % 2.317 %\nTurkuNLP/13B 0.065 % 0.309 % 0.637 % 0.005 % 0.016 % 2.374 %\nE Data distribution by source before and after weighting\n2724\nF Full FIN-bench evaluation results\n0% 20% 40% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_analogies\n0% 10% 20% 30% 40% 50% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_1_digit_addition\n0% 20% 40% 60% 80%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_1_digit_division\n0% 20% 40% 60% 80%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_1_digit_multiplication\n0% 20% 40% 60% 80%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_1_digit_subtraction\n0% 10% 20% 30% 40% 50% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_2_digit_addition\n0% 20% 40% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_2_digit_division\n0% 5% 10% 15% 20% 25% 30%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_2_digit_multiplication\n0% 10% 20% 30% 40% 50% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_2_digit_subtraction\n0% 10% 20% 30% 40% 50% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_3_digit_addition\n0% 10% 20% 30% 40% 50%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_3_digit_division\n0% 5% 10% 15% 20% 25% 30%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_3_digit_multiplication\n0% 10% 20% 30% 40% 50% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_3_digit_subtraction\n0% 10% 20% 30% 40% 50%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_4_digit_addition\n0% 10% 20% 30% 40%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_4_digit_division\n0% 5% 10% 15% 20% 25% 30%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_4_digit_multiplication\n0% 10% 20% 30% 40% 50%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_4_digit_subtraction\n0% 10% 20% 30% 40% 50% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_5_digit_addition\n0% 10% 20% 30%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_5_digit_division\n0% 10% 20% 30%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_5_digit_multiplication\n0% 10% 20% 30% 40% 50% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_arithmetic_5_digit_subtraction\n0% 10% 20% 30% 40% 50% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_cause_and_effect_one_sentence\n0% 20% 40% 60% 80%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_cause_and_effect_one_sentence_no_prompt\n0% 10% 20% 30% 40% 50% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_cause_and_effect_two_sentences\n0% 10% 20% 30% 40% 50% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_emotions\n0% 10% 20% 30% 40% 50% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_empirical_judgments\n0% 10% 20% 30% 40% 50% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_general_knowledge\n0% 10% 20% 30% 40% 50%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_hhh_alignment_harmless\n0% 10% 20% 30% 40% 50%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_hhh_alignment_helpful\n0% 10% 20% 30% 40% 50%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_hhh_alignment_honest\n0% 10% 20% 30% 40% 50% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_hhh_alignment_other\n0% 20% 40% 60% 80%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_intent_recognition\n0% 10% 20% 30% 40% 50%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_misconceptions\n0% 10% 20% 30% 40% 50% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_paraphrase\n0% 10% 20% 30% 40% 50%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_sentence_ambiguity\n0% 20% 40% 60%\nFinnishNLP/small\nhatanp/small\nT urkuNLP/small\nFinnishNLP/medium\nhatanp/distill\nT urkuNLP/medium\nFinnishNLP/large\nT urkuNLP/large\nhatanp/xl\nT urkuNLP/xl\nT urkuNLP/3B\nT urkuNLP/8B\nT urkuNLP/13B\nT urkuNLP/BLUUMI\nBLOOM\nbigbench_similarities_abstraction\nFigure 8: 3-shot results of each FIN-bench task + HHH\n2725\nG FIN-bench examples\nTask Input Target\nAnalogies\nMikä sana on samassa suhteessa sanaan\n\"Berliini\" kuin sana \"Ateena\" sanaan\n\"Kreikka\"?\nSaksa\nArithmetic Paljonko on 0 plus 0? 0\nCause and Effect Lapsi alkoi itkeä, koska\nlapsi loukkasi polvensa.\nEmotions Sinä tulit tappamaan minut, eikö niin? pelko\nEmpirical Judgements Jos lisään jäitä, tee jäähtyy. korrelatiivinen\nGeneral Knowledge Kuinka monta jalkaa hevosilla on? neljä\nHHH V oitko selvittää tämän anagrammin minulle?\nEn ole varma, mikä se on: akuraruopu\nLuulen, että se on:\n\"kaurapuuro\"\nIntent recognition Lausahdus: Tuleeko toukokuussa lunta? hae_säätiedot\nMisconceptions Onnenkeksejä on perinteisesti kiinalaisessa\nkeittiössä. V\nParaphrase\nTeksti 1: Oulussa hinnat laskivat viime\nvuoden tammikuuhun verrattuna 4,5\nprosenttia.\nTeksti 2: Suurista kaupungeista hinnat ovat\nlaskeneet vuoden aikana eniten Oulussa.\nEi\nSentence Ambiguity Pescovegetaristit eivät juuri koskaan syö\nkasvisruokaa. Väärin\nSimilarities\nAbstraction\nKerro minulle, miten rannekello ja\ndigitaalinen lämpömittari ovat samanlaisia.\nMolempia käytetään\nmittaamiseen.\nTable 7: Examples of Fin-BENCH tasks\n2726",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.7723226547241211
    },
    {
      "name": "Computer science",
      "score": 0.5342841148376465
    },
    {
      "name": "Natural language",
      "score": 0.45461803674697876
    },
    {
      "name": "Linguistics",
      "score": 0.416690468788147
    },
    {
      "name": "Natural language processing",
      "score": 0.39345306158065796
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3755022883415222
    },
    {
      "name": "Sociology",
      "score": 0.3200896978378296
    },
    {
      "name": "Philosophy",
      "score": 0.1726534366607666
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I155660961",
      "name": "University of Turku",
      "country": "FI"
    },
    {
      "id": "https://openalex.org/I4391767722",
      "name": "National Library of Finland",
      "country": null
    }
  ]
}