{
  "title": "Orthogonal Subspace Learning for Language Model Continual Learning",
  "url": "https://openalex.org/W4389520299",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2104684343",
      "name": "Xiao Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2156190191",
      "name": "Tian-ze Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2676942995",
      "name": "Qiming Ge",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122840233",
      "name": "Han Xia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107271714",
      "name": "Rong Bao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1965480461",
      "name": "Rui Zheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1964204209",
      "name": "Qi Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117552295",
      "name": "Tao Gui",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2161482855",
      "name": "Xuanjing Huang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W2926477959",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4318751794",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4385570539",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W2902456977",
    "https://openalex.org/W3035691277",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W4385571224",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4285157946",
    "https://openalex.org/W3138136049",
    "https://openalex.org/W2902625698",
    "https://openalex.org/W4223458030",
    "https://openalex.org/W4386794445",
    "https://openalex.org/W4288336773",
    "https://openalex.org/W4376167151",
    "https://openalex.org/W4312238419",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3171057731",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4387561459",
    "https://openalex.org/W2473930607",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W4309953708",
    "https://openalex.org/W4319049883",
    "https://openalex.org/W4366400290",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W4365475020",
    "https://openalex.org/W3205616434",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W4295883599",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3037967334",
    "https://openalex.org/W2170240176"
  ],
  "abstract": "Benefiting from massive corpora and advanced hardware, large language models (LLMs) exhibit remarkable capabilities in language understanding and generation. However, their performance degrades in scenarios where multiple tasks are encountered sequentially, also known as catastrophic forgetting. In this paper, we propose orthogonal low-rank adaptation (O-LoRA), a simple and efficient approach for continual learning in language models, effectively mitigating catastrophic forgetting while learning new tasks. Specifically, O-LoRA learns tasks in different (low-rank) vector subspaces that are kept orthogonal to each other in order to minimize interference. Our method induces only marginal additional parameter costs and requires no user data storage for replay. Experimental results on continual learning benchmarks show that our method outperforms state-of-the-art methods. Furthermore, compared to previous approaches, our method excels in preserving the generalization ability of LLMs on unseen tasks.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10658‚Äì10671\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nOrthogonal Subspace Learning for Language Model Continual Learning\nXiao Wang‚ãÜ‚àó, Tianze Chen ‚ãÜ‚àó, Qiming Ge ‚ãÜ, Han Xia ‚ãÜ,\nRong Bao‚ãÜ, Rui Zheng‚ãÜ, Qi Zhang‚ãÜ‚Ä†, Tao Gui‚ô¶‚Ä†, Xuanjing Huang‚ãÜ‚ô£\n‚ãÜSchool of Computer Science, Fudan University, Shanghai, China\n‚ô¶Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China\n‚ô£International Human Phenome Institutes (Shanghai)\n{xiao_wang20,qz,tgui}@fudan.edu.cn\nAbstract\nBenefiting from massive corpora and advanced\nhardware, large language models (LLMs)\nexhibit remarkable capabilities in language\nunderstanding and generation. However, their\nperformance degrades in scenarios where mul-\ntiple tasks are encountered sequentially, also\nknown as catastrophic forgetting. In this paper,\nwe propose orthogonal low-rank adaptation\n(O-LoRA), a simple and efficient approach\nfor continual learning in language models,\neffectively mitigating catastrophic forgetting\nwhile learning new tasks. Specifically, O-LoRA\nlearns tasks in different (low-rank) vector\nsubspaces that are kept orthogonal to each other\nin order to minimize interference. Our method\ninduces only marginal additional parameter\ncosts and requires no user data storage for\nreplay. Experimental results on continual\nlearning benchmarks show that our method\noutperforms state-of-the-art methods. Further-\nmore, compared to previous approaches, our\nmethod excels in preserving the generalization\nability of LLMs on unseen tasks.\n1 Introduction\nLearning tasks sequentially is crucial for de-\nveloping real-world NLP models (Wang et al.,\n2023b; Xi et al., 2023), as it enables continu-\nous evolution when encountering new tasks or\nknowledge. Although pre-trained models (Devlin\net al., 2019; Brown et al., 2020; Raffel et al., 2020;\nOpenAI, 2023) have achieved tremendous success\non static tasks (Wang et al., 2022a, 2023c), learning\nmultiple tasks sequentially, commonly referred to\nas continual learning, remains challenging (Wu\net al., 2022; Luo et al., 2023). As a model\nlearns new tasks, it tends to forget or lose the\nknowledge it had acquired for earlier tasks, leading\nto a phenomenon known as catastrophic forgetting\n(McCloskey and Cohen, 1989).\n‚àó ‚àóEqual contribution\n‚Ä†‚Ä†Corresponding Author\nOriginal \nupdate direction\nGradient \nsubspace of \nprevious tasks\nOrthogonal\nupdate direction\nFigure 1: Illustration highlighting the intuition of our\napproach. O-LoRA mitigates catastrophic forgetting\nof past task knowledge by constraining the gradient\nupdates of the current task to be orthogonal to the\ngradient subspace of the past tasks.\nExisting continual learning works (Ke and Liu,\n2022; Wang et al., 2023a) can be mainly catego-\nrized into rehearsal-based, regularization-based,\nand architecture-based approaches. Rehearsal-\nbased approaches (Lopez-Paz and Ranzato, 2017;\nde Masson D‚ÄôAutume et al., 2019) allow access\nto a memory buffer with examples from prior\ntasks and train the model jointly with the current\ntask. Unfortunately, storing and replaying data\nfrom previous tasks may raise privacy concerns,\nespecially when sensitive or personally identifiable\ninformation is involved. Regularization-based\napproaches (Kirkpatrick et al., 2017; Li and Hoiem,\n2017; Smith et al., 2023) introduce additional\nterms in the loss function to penalize changes\nin important weights, aiming to protect earlier\nlearned tasks. They often struggle to handle long\ntask sequences. Architecture-based approaches\n(Wang et al., 2023e; Razdaibiedina et al., 2023)\ndynamically expand the model capacity or isolate\nexisting model weights to reduce interference.\n10658\nHowever, such approaches essentially learn differ-\nent expert models for different tasks, limiting their\ngeneralization to unseen tasks.\nExisting methods typically update all tasks\nwithin a shared vector space, directly affecting\nthe model‚Äôs hidden layer outputs. Recent studies\n(Farajtabar et al., 2020; Saha et al., 2021) have\nhighlighted a promising approach to address this\nissue. By taking gradient steps in the orthogonal\ndirection to the gradient subspaces associated with\npast tasks, we can effectively mitigate catastrophic\nforgetting as it prevents interference with the past\ntask loss functions. However, previous approaches\neither require storing historical data (Chaudhry\net al., 2019), which raises data privacy concerns,\nor historical data gradients (Farajtabar et al., 2020),\nwhich becomes impractical for large-scale models.\nIn this work, we propose orthogonal low-rank\nadaptation (O-LoRA) 1, a simple and efficient\napproach for continual learning in language models.\nOur key insight is rooted in the nature of LoRA:\nlarge pre-trained models primarily fine-tune within\na specific low-rank subspace. With this premise,\nwe hypothesize that the gradient subspaces from\nprevious tasks can be effectively captured by the\nLoRA parameters. In the context of continual\nlearning, we incrementally learn new tasks in\nan orthogonal subspace while fixing the LoRA\nparameters learned from past tasks. Figure 1\nprovides a visual representation of how O-LoRA\nminimizes catastrophic forgetting.\nOur method offers three advantages: (1) Data\nprivacy-friendliness: We require no storage of\nuser data for replay, addressing concerns associated\nwith privacy. (2) Model parameter-friendliness:\nBy introducing only marginal cost of additional\nparameters, our approach enables the learning of\nnew tasks without compromising the performance\nof previous tasks. (3) Generalization-friendliness:\nOur method does not rely on task IDs during\ntesting, making it compatible with instruction\ntuning paradigm (Wang et al., 2022b), thus\npreserving LLMs‚Äô generalization ability on unseen\ntasks.\nOur main contributions are summarized as\nfollows:\n‚Ä¢ We introduce O-LoRA, a simple and efficient\napproach for continual learning in language\n1The dataset, code can be found athttps://github.com/\ncmnfriend/O-LoRA\nmodels, incrementally learning new tasks in\northogonal subspaces.\n‚Ä¢ Our method significantly outperforms prior\nSOTA methods on standard continual learning\nbenchmarks.\n‚Ä¢ Experimental results show that our method\npreserves the generalization ability of large\nlanguage models on unseen tasks, which was\nlacking in previous approaches.\n2 Background\n2.1 Continual Learning Setup\nContinual learning (Ke and Liu, 2022; Wang et al.,\n2023b) focuses on developing learning algorithms\nto accumulate knowledge on non-stationary data.\nIn supervised continual learning, a sequence of\ntasks {D1,..., DT }arrive in a streaming fashion.\nEach task Dt =\n{(\nxt\ni,yt\ni\n)}nt\ni=1 contains a separate\ntarget dataset, where xt\ni ‚ààXt , yt\ni ‚ààYt. A single\nmodel needs to adapt to them sequentially, with\nonly access to Dt at the t-th task. In general,\ngiven a prediction model hŒò parameterized by\nŒò, continual learning seeks to optimize for the\nfollowing objective across all tasks:\nmax\nŒò\nT‚àë\nk=1\n‚àë\nx,y‚ààDk\nlog pŒò(y|x) (1)\nIn this study, we tackle a more challenging setting.\nDuring the training phase, the model is prohibited\nfrom accessing any historical data. In the testing\nphase, the model predicts a sample‚Äôs label without\nknowing which task it belongs to.\n2.2 LoRA\nWhen pre-trained models (PTMs) adapt to specific\ntasks, Hu et al. (2021) has demonstrated that\nweight updates in PTMs exhibit a low \"intrin-\nsic dimension.\" For a pre-trained weight matrix\nWinit ‚àà Rd√ók, LoRA constrains its update by\nrepresenting it with a low-rank decomposition\nWinit + ‚àÜW = Winit + AB, where A ‚ààRd√ór,\nB ‚ààRr√ók, and the rank r ‚â™min(d,k). Winit\nremains fixed during training and does not receive\ngradient updates, while Aand Bcontain trainable\nparameters. To illustrate the modified forward pass\nof LoRA, consider the operationh= Winitx. With\nLoRA, the modified forward pass becomes:\nh= Winitx+ ‚àÜWx = Winitx+ ABx (2)\n10659\nTransformer-Based Language Model\nMulti-Head Attention\nK Q V\nInput\nO-LoRA\nWhat is the sentiment of the following paragraph?\nOption: very negative, negative, neutral, positive, \nvery positive\nI love Bocchi the Rock!! I have watched all the \nthree seasons in one night!! No episodes missed!!\nFrozen Trainable\nOutput\nAnswer: very positive\nLoRA weights of \nprevious tasks\nLoRA weights of \nthe current task\nùê¥ùë°ùê¥1 ùê¥2\nùêµùë°ùêµ1 ùêµ2\n√ó √ó √ó¬∑¬∑¬∑\nOrthogonal \nregularization\nWeights\nof the \npretrained \nmodel\nSentiment \nAnalysis\nQuestion \nAnswering\nTopic\nClassification\n¬∑¬∑¬∑\nFigure 2: The framework of O-LoRA for language model continual learning. First, allowing the integration of\nhuman expertise and enhancing generalization by instruction tuning. Next, approximate gradient subspaces of each\ntask respectively uning LoRA. For each sequentially incoming task, we incrementally learn a new LoRA while\nenforcing orthogonality between the current task‚Äôs LoRA and the past ones.\n3 Orthogonal Low-rank Adaptation\nIn this section, we introduce O-LoRA, illustrated in\nFigure 2. First, we adopt instruction tuning as our\ntraining paradigm. Then, we incrementally learn\nnew tasks within an orthogonal subspace while\nkeeping the LoRA parameters fixed for past tasks.\nLastly, we conduct a comparative analysis of our\nmethod compared to existing approaches.\n3.1 Instruction Schema\nInstruction-following capability is essential to\nLLMs as an interface between humans and AI\nmodels (Wang et al., 2022b; Ouyang et al., 2022;\nWang et al., 2023d). We choose instruction\ntuning as our training paradigm for two reasons:\n1) Incorporating human expertise: The models\ncan leverage prior knowledge and benefit from\nhuman expertise by providing explicit instructions,\nleading to more efficient learning. 2) Enhanced\ngeneralization: The explicit guidance helps models\ncapture the underlying principles, enabling better\ngeneralization to unseen situations.\nAll task instructions follow the same uniform\nschema, which is composed of 1) Task Definition\nprovides a detailed guide on how an input text (e.g.,\na sentence or a document) is expected to be mapped\nto an output text. 2) Options are the output label\nconstraints for a task, which represent the set of\npossible outputs that can be generated by the model\nfor a given input. 3) Text is the input sentence of\na task instance. This sequence is then fed into\nthe pre-trained language model along with the task\ninstruction and options. 4) Answer is the expected\noutput of the given sample.\n3.2 Continual Learning in Orthogonal\nSubspaces\nPrevious methods exhibit a common feature: all\ntasks undergo updates within a shared vector space,\ndirectly impacting the hidden layer outputs of the\nmodel. Catastrophic forgetting happens in neural\nnetworks when the gradient updates with respect\nto a new task are applied to the model without\nconsidering previous tasks.\nFarajtabar et al. (2020) propose the Orthogonal\nGradient Descent (OGD) method for mitigating\nthis problem, which constrains the parameters to\nmove within the orthogonal space to the gradients\nof previous tasks. With limited access to previous\ntask data, OGD approximates the current gradient\nof previous data with the gradient in the previous\nconvergence parameters. However, OGD needs\nto store gradients of all previous data. This can\n10660\nbe especially intractable for large-scale language\nmodels with billions of parameters (Raffel et al.,\n2020; Brown et al., 2020), which have become a\nstandard in the NLP field.\nIs it possible to approximate the gradient\ndirection of previous tasks without storing histor-\nical gradients? In this study, we leverage the\nlow-rank subspace of LoRA (Hu et al., 2021)\nas a proxy for the gradient subspace of past\ntasks. Our fundamental insight is rooted in\nthe nature of LoRA: large pre-trained models\nprimarily fine-tune within a specific low-rank\nsubspace. This characteristic behavior suggests\nthat the LoRA parameters are not mere numerical\nadjustments but encapsulate crucial model update\ndirections. Therefore, we hypothesize that the\ngradient subspaces of previous tasks are succinctly\nrepresented by the LoRA parameters. By learning\nwithin a subspace orthogonal to the LoRA subspace\nassociated with previous tasks, we can prevent\ninterference with past task loss functions, thus\nmitigating catastrophic forgetting.\nWe propose O-LoRA, which incrementally\nlearns new tasks in a direction orthogonal to the\nLoRA subspace of past tasks while fixing the\nprevious parameters. For each task, we introduce\na set of LoRA parameters denoted as {At,Bt},\nwhere A ‚àà Rd√ór, B ‚àà Rr√ók, and the rank\nr ‚â™min(d,k). We approximate the parameter\nupdate subspace Ut for the t-th task as the subspace\nspanned by the column vectors of At:\nAt = [a1\nt ,a2\nt ,...,a r\nt ] (3)\nUt = span{a1\nt ,a2\nt ,...,a r\nt } (4)\nLet Bt = [b1\nt ,b2\nt ,...,b r\nt ], where bi\nt ‚ààBt represents\nthe linear weighting coefficients of the column\nvectors in At.\nTo ensure the orthogonality between the sub-\nspace Uand the subspace W, we need to satisfy:\n<u,w> = 0,‚àÄu‚ààU,w ‚ààW. (5)\nTherefore, achieving orthogonality between the\nLoRA subspaces of task i(Ui) and task t(Ut) can\nbe expressed as:\nOi,t = AT\ni At = 0. (6)\nFinally, our training objective is defined as:\n‚àë\nx,y‚ààDt\nlog pŒò(y|x) + Œª1\nt‚àí1‚àë\ni=1\nLorth(Ai,At) (7)\nRF PE TIF UT\nEWC (Kirkpatrick et al., 2017) ‚úì ‚úì\nA-GEM (Chaudhry et al., 2018) ‚úì\nMBPA++ (de Masson D‚ÄôAutume et al., 2019)‚úì\nIDBR (Huang et al., 2021) ‚úì\nL2P (Wang et al., 2022c) ‚úì ‚úì ‚úì\nLwF (Li and Hoiem, 2017) ‚úì\nOGD (Farajtabar et al., 2020) ‚úì ‚úì\nLFPT5 (Qin and Joty, 2021) ‚úì ‚úì\nEIP (Wang et al., 2023e) ‚úì ‚úì ‚úì\nPP (Razdaibiedina et al., 2023) ‚úì ‚úì\nO-LoRA ‚úì ‚úì ‚úì ‚úì\nTable 1: The comparison between O-LoRA and other\ncontinual learning methods. Specifically, RF indicates\nwhether the method is rehearsal-free. PE indicates\nwhether the method is parameter efficient.TIF indicates\nwhether task-id is available during inference. UT\nindicates whether the method can be applied to solve\nunseen tasks.\nLorth(Ai,At) =\n‚àë\nj,k\n‚à•Oi,t[j,k]‚à•2 (8)\nwhere Oi,t[j,k] denotes the element at the j-th row\nand k-th column of Oi,t, and Œª1 is the weights of\nthe orthogonality loss. During the training process,\nto mitigate forgetting of past knowledge, we fix\nthe previous LoRA parameters {Ai,Bi|i < t}.\nFollowing Hu et al. (2021), we only apply LoRA\nto the attention weights of queries (Wq) and values\n(Wv).\nWhile the number of LoRA parameters grows\nwith the number of tasks during training, we can\nmerge the updates corresponding to the LoRA\nparameters into the initial parameters to avoid GPU\nmemory inflation.\nWinit := Winit +\nt‚àë\ni=1\nAiBi. (9)\n3.3 Comparisons Between O-LoRA and\nOther Methods\nIn this section, we compare O-LoRA with other\nexisting continual learning methods across several\ndimensions: rehearsal-free, parameter efficiency,\navailability of task-id during inference, and appli-\ncability to unseen tasks. As shown in Table 1, O-\nLoRA demonstrates three distinct advantages: data\nprivacy-friendliness, model parameter-friendliness,\nand generalization-friendliness.\nData privacy-friendliness . Rehearsal-based\nmethods (de Masson D‚ÄôAutume et al., 2019; Huang\net al., 2021), which rely on storing past task\ndata in a buffer and replaying it during training,\n10661\nare not suitable for scenarios with data privacy\nconcerns. Additionally, as the number of training\ntasks increases, the cost of training new tasks using\nreplay-based methods also grows. In contrast,\nour method does not require storing historical\ndata, alleviating concerns regarding data privacy.\nMoreover, since we only modify the training loss,\nthere is no additional training cost incurred.\nModel parameter-friendliness. Many previous\nmethods (Kirkpatrick et al., 2017; Farajtabar\net al., 2020) train the entire model parameters\nfor each task, while our method only introduces\nmarginal additional parameters for each task.\nO-LoRA has lower requirements in terms of\ncomputational resources and GPU memory during\ntraining. Additionally, because the training of\nLoRA freezes the pre-trained model parameters, it\nis less prone to forgetting the knowledge acquired\nduring pre-training.\nGeneralization-friendliness. Traditional meth-\nods (Kirkpatrick et al., 2017; Chaudhry et al.,\n2018; Wang et al., 2022c), primarily designed for\nclassification tasks, often fall short in generalizing\nto unseen tasks due to their narrow task-specific\nfocus. In contrast, O-LoRA employs instruction\ntuning (Wang et al., 2022b) as its training paradigm.\nBy incorporating explicit instructions or demon-\nstrations, the model can capture the underlying\nprinciples or constraints of a task. This explicit\nguidance helps the model generalize beyond the\nspecific examples in the training data, enabling it\nto handle unseen situations more effectively. The\nintegration of human expertise through instruction\ntuning enhances the generalization capabilities of\nO-LoRA.\n4 Experiments\n4.1 Experimental Setup\n4.1.1 Datasets\nStandard CL Benchmark We evaluate our ap-\nproach using the CL benchmark for language\nmodels, which consists of five text classification\ndatasets introduced by Zhang et al. (2015): AG\nNews, Amazon reviews, Yelp reviews, DBpedia\nand Yahoo Answers. We adopt the CL setup for the\nT5 model, following LFPT5 (Qin and Joty, 2021),\nand explore three different orders of the benchmark.\nAppendix A.2 provides the task details, and the\nsequences of tasks used in our experiments are\nprovided in Appendix A.3.\nLarge number of tasks Our method‚Äôs perfor-\nmance on longer task sequences, posing a greater\nchallenge, is evaluated through experiments on\na continual learning benchmark of 15 datasets\n(Razdaibiedina et al., 2023). This includes five\ntasks from CL benchmark, four from GLUE\nbenchmark (MNLI, QQP, RTE, SST2) (Wang et al.,\n2018), five from SuperGLUE benchmark (WiC,\nCB, COPA, MultiRC, BoolQ) (Wang et al., 2019),\nand the IMDB movie reviews dataset (Maas et al.,\n2011). Following Razdaibiedina et al. (2023), we\nselect 1000 random samples for training each task\nand hold out 500 samples per class for validation.\nUnseen tasks Generation To assess the impact\nof our approach on LLMs‚Äô generalization ability,\nwe initially train an LLM on the Alpaca dataset\n(Taori et al., 2023), an open-source multitask\ninstruction tuning dataset. We then use the pre-\ntrained LLM for sequential training on the standard\nCL benchmark (Zhang et al., 2015). Our zero-\nshot benchmark, MMLU (Hendrycks et al., 2020),\ncovers 57 subjects across various domains such as\nSTEM, humanities, and social sciences, assessing\nworld knowledge and problem-solving abilities\nacross various difficulty levels.\n4.1.2 Metrics\nLet ai,j be the testing accuracy on the i-th task after\ntraining on j-th task, the metrics for evaluating is\nAverage Accuracy (AA), the average accuracy of\nall tasks after training on the last task, 1\nT\n‚àëT\ni=1 ai,T\n4.1.3 Baselines\nWe evaluate O-LoRA against 10 baseline methods.\nImportantly, among these baselines, only prompt-\nbased methods are exceptions; all others utilize the\nLoRA framework. This uniformity in the founda-\ntion ensures consistent parameter settings between\nO-LoRA and its comparatives, guaranteeing a fair\ncomparison.\n‚Ä¢ SeqFT (de Masson D‚ÄôAutume et al., 2019): train\nall model parameters on a sequence of tasks\n(without adding any regularization or replaying\nsamples from the previous tasks).\n‚Ä¢ SeqLoRA: fixed-size LoRA parameters are\ntrained on a sequence of tasks (without adding\nany regularization or replaying samples from the\nprevious tasks).\n‚Ä¢ IncLoRA: incremental learning of new LoRA\nparameters on a sequential series of tasks\n10662\nStandard CL Benchmark Large Number of Tasks\nOrder-1 Order-2 Order-3 avg Order-4 Order-5 Order-6 avg\nSeqFT 18.9 24.9 41.7 28.5 7.4 7.4 7.5 7.4\nSeqLoRA 44.6 32.7 53.7 43.7 2.3 0.6 1.9 1.6\nIncLoRA 66 64.9 68.3 66.4 63.3 58.5 61.7 61.2\nReplay 55.2 56.9 61.3 57.8 55 54.6 53.1 54.2\nEWC 48.7 47.7 54.5 50.3 45.3 44.5 45.6 45.1\nLwF 54.4 53.1 49.6 52.3 50.1 43.1 47.4 46.9\nL2P 60.3 61.7 61.1 60.7 57.5 53.8 56.9 56.1\nLFPT5 67.6 72.6 77.9 72.7 70.4 68.2 69.1 69.2\nO-LoRA 75.4 75.7 76.3 75.8 72.3 64.8 71.6 69.6\nProgPrompt 75.2 75 75.1 75.1 78.0 77.7 77.9 77.9\nPerTaskFT 70.0 70.0 70.0 70.0 78.1 78.1 78.1 78.1\nMTL 80.0 80.0 80.0 80.0 76.5 76.5 76.5 76.5\nTable 2: Summary of the results on two standard CL benchmarks with T5-large model. Averaged accuracy after\ntraining on the last task is reported. All results are averaged over 3 runs.\n(without adding any regularization or replaying\nsamples from the previous tasks).\n‚Ä¢ Replay: finetune the whole model with a\nmemory buffer, and replay samples from old\ntasks when learning new tasks to avoid forgetting.\n‚Ä¢ EWC (Kirkpatrick et al., 2017): finetune the\nwhole model with a regularization loss that\nprevents updating parameters that could interfere\nwith previously learned tasks.\n‚Ä¢ LwF (Li and Hoiem, 2017): constrains the shared\nrepresentation layer to be similar to its original\nstate before learning the new task.\n‚Ä¢ L2P (Wang et al., 2022c): uses the input to\ndynamically select and update prompts from the\nprompt pool in an instance-wise fashion.\n‚Ä¢ LFPT5 (Qin and Joty, 2021): continuously train\na soft prompt that simultaneously learns to solve\nthe tasks and generate training samples, which\nare subsequently used in experience replay.\n‚Ä¢ ProgPrompt (Razdaibiedina et al., 2023): adopts\na task-specific soft prompt for each distinct\ntask, sequentially appending it to prior learned\nprompts. In essence, it trains individual models\nper task, leveraging the task ID to select the\nappropriate model during inference.\n‚Ä¢ PerTaskFT: train a separate model for each task.\n‚Ä¢ MTL: train a model on all tasks as multi-task\nlearning. This method is the upper bound of\ncontinual learning.\n4.1.4 Implementation Details\nO-LoRA is a model-agnostic CL method that can\nbe used with any transformer-based model. In our\nexperiments, we use two language models adopted\nby the previous lines of works in CL for NLP:\nencoder-decoder T5 model (Raffel et al., 2020) and\ndecoder-only LLaMA model (Touvron et al., 2023).\nTo compare O-LoRA to the recent CL approaches\n(Wang et al., 2022c; Qin and Joty, 2021), we use the\npre-trained T5-large model. To validate the impact\nof our approach on the generalization ability of\nLLMs for unseen tasks, we use pre-trained LLaMA-\n7B model. All experimental results are reported as\nthe average of 3 runs. For more detailed settings,\nrefer to the Appendix A.1.\n4.2 Main Results\nTable 2 presents a performance comparison of O-\nLoRA and baseline continual learning methods on\ntwo CL benchmarks. Following LFPT5, we report\nthe results of three independent runs with different\ntask orders on the CL benchmark.\nResults on Standard Continual Learning\nBenchmarks On all task orders of the standard\nCL benchmark, O-LoRA consistently outperforms\nprevious methods by a significant margin. Overall,\nO-LoRA achieves a performance improvement of\nover 24% compared to LFPT5, the previous state-\n10663\nMMLU CL\nLLaMA-7B 34.4 /\nAlpaca-LoRA 37.5 /\nAlpaca-LoRA-CL 23.3 46.7\nAlpaca-inc-LoRA-CL 28.6 33.1\nAlpaca-OLoRA-CL 33.6 76.8\nTable 3: Performance comparison of different continual\nlearning methods applied to the Alpaca-LoRA-LLaMA\nmodel. These methods are evaluated on MMLU(zero-\nshot) and CL benchmmark(order 1).\nof-the-art method. Our approach demonstrates\ncomparable performance to multi-task learning and\nsignificantly outperforms PerTaskFT, indicating\nthat our method not only effectively avoids catas-\ntrophic forgetting but also leverages knowledge\nfrom past tasks for efficient learning of new tasks.\nPerformance with Large Number of Tasks\nOn a more challenging benchmark with a large\nnumber of tasks, O-LoRA outperforms the state-\nof-the-art method, LFPT5, in terms of the average\nperformance across three orders of tasks. While\nProgPrompt performs better than our method\nin handling long sequence tasks, its inherent\nconstraints cannot be overlooked. ProgPrompt\nis strictly tied to tasks it‚Äôs trained on and leans\nheavily on task IDs during inference, limiting its\ngeneralization ability and making it less adaptive\nfor LLMs. It is worth noting that almost all existing\ncontinual learning methods perform significantly\nlower than PerTaskFT and MTL, indicating that\ncontinual learning for a large number of tasks\nremains a challenging problem.\nImpact on the Generalization Ability of LLMs\nWe investigate the impact of O-LoRA on the\ngeneralization of Large Language Models through\ncontinual learning experiments. We start with\na fine-tuned LLaMA-7B language model on the\nAlpaca dataset, then test models with and without\nthe O-LoRA constraint on the MMLU benchmark.\nWith MMLU being a four-classification problem,\na 25% accuracy equates to random guessing.\nAccording to Table 3, models without O-LoRA\n(Alpaca-LoRA-CL, Alpaca-LoRA-inc-CL) achieve\naccuracies of 23.3% and 28.6% respectively,\ncomparable to random guesses. In contrast,\nmodels with O-LoRA average at 33.6% accuracy,\ndemonstrate the effectiveness of O-LoRA in\nmaintaining generalization for unseen tasks.\n0 0.5 1 1.5 2 2.50\n20\n40\n60\n80\n100 Œª1=0.5\nŒª1=0\nChanges in prediction loss\nNumber of batches\nFigure 3: Histogram of prediction loss changes after\ntraining on a new task. The O-LoRA constraint (Œª1 =\n0.5) helps reduce the changes in comparison to when it\nis not present (Œª1 = 0).\n4.3 Discussions\nDoes O-LoRA preserve the loss of previous\ntasks while training new tasks? We assessed\nthe efficiency of the low-rank subspace of LoRA\nin approximating the gradient subspace of previous\ntasks. In our evaluation, we applied an orthogo-\nnality constraint with a weight of 0.5 ( Œª1 = 0.5).\nIn comparison, without the constraint ( Œª1 = 0 ),\nnew LoRA parameters are added for new tasks\nwith historical LoRA, and model parameters are\nkept fixed. As Figure 3 shows, the O-LoRA\nconstraint helps keep the loss of previous samples\nlow, proving that the O-LoRA constraint effectively\ncounteracts catastrophic forgetting.\nHow does O-LoRA influence the output of\neach layer in the model? We examine the\nvariation in hidden states for past task samples\nin models trained with and without O-LoRA\nconstraints, using the T5-base model. Figure 4\ndemonstrates that the O-LoRA constraint mini-\nmizes the variations, hence reducing the forgetting\nof internal knowledge. We found that lower layers\nencode more generic semantic knowledge that\ncan be shared across tasks. Conversely, higher\nlayers encode task-specific semantic knowledge\nand change a lot during new task learning. The\ndecoder can capture relevant information from\nthese rich semantic representations, proving the\nminimal impact of our method on past tasks.\nHow do different PLMs influence perfor-\nmances? We evaluate the performance of models\nacross varying parameter sizes (T5-base, T5-large,\nT5-XL) and distinct architectures (T5, LLaMA)\nusing the standard continual learning benchmark.\nOur findings are as follows: 1) In the T5\nseries, O-LoRA‚Äôs average accuracy improves as\nthe parameter size increases. 2) Larger network\n10664\n1 2 3 4 5 6 7 8 9 10 11 12\n0\n0.2\n0.4\n0.6\n0.8\n1\n(a) Encoder Layer\nChanges in hidden states\n1 2 3 4 5 6 7 8 9 10 11 12\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n(b) Decoder Layer\nChanges in hidden states\nFigure 4: Variation in hidden states across different\nlayers of the T5-base model with and without O-LoRA\nconstraints. (a) illustrates the changes in the hidden\nstates of each layer in the T5 encoder after training on\na new task. (b) demonstrates the changes in the hidden\nstates of each layer in the T5 decoder. Light blue color\nindicates the utilization of orthogonality loss, while dark\nblue represents the absence of orthogonal loss in the\ntraining objective.\nsizes appear to counteract catastrophic forgetting,\napproaching the proficiency levels of multitask\nlearning. 3) Notably, even with a greater parameter\ncount in the LLaMA-7B model, the T5-3B model\nregistered a higher average accuracy. This implies\nthat encoder-decoder architectures might be more\nresistant to forgetting.\nWhat is the Optimal Rank r for O-LoRA?\nTo investigate the influence of the rank parameter\n(r) on the performance of O-LoRA, we conduct\nexperiments using T5-Base on a standard CL\nbenchmark. Table 5 presents the results of varying\nr values. Increasing the rank r improves the average\naccuracy of the model to a certain extent. However,\nwe observe that there is not a significant difference\nin performance between r=2 and r=16, indicating\nthat the gradient space of the model has a relatively\nlow intrinsic dimensionality.\nOrder\nModel 1 2 3 avg MTL\nT5-base 73.9 75.8 74.5 74.7 78.5\nT5-large 75.4 75.7 76.3 75.8 80.0\nT5-xl 78.9 79.0 77.9 78.6 79.9\nLLaMA-7B 76.8 75.7 75.7 76.1 77.1\nTable 4: Comparison of different PLMs‚Äô performances\nacross three orders in a standard continual learning\nbenchmark. Results also include average accuracy\n(\"avg\") and multitask learning performance (\"MTL\").\nOrder\nr-dim 1 2 3 avg\n2 74.2 71.1 73.6 73.0\n4 73.0 72.7 74.1 73.3\n8 75.6 71.7 71.9 73.1\n16 74.5 73.4 74.8 74.2\nstd 0.92 0.89 1.07 0.47\nTable 5: Comparisons of different rank r of LoRA.\nThis experiment is conducted based on T5-Base on the\nstandard continual learning benchmark.\n5 Related Work\n5.1 Continual Learning\nContinual learning (Ke and Liu, 2022; Wang\net al., 2023a) aims to develop learning algorithms\nthat can accumulate knowledge on non-stationary\ndata. Existing works can be broadly categorized\ninto rehearsal-based, regularization-based, and\narchitecture-based approaches. For an in-depth\ndiscussion on continual learning in the era of large\nlanguage models, readers may refer to (Wang et al.,\n2023b).\nRehearsal-based approaches (Lopez-Paz and\nRanzato, 2017; de Masson D‚ÄôAutume et al., 2019;\nHan et al., 2020; Bai et al., 2022) leverage a\nmemory buffer that stores examples from previous\ntasks, training the model jointly with the current\ntask. Experience replay (ER) (Rolnick et al.,\n2019) is a common strategy employed in rehearsal-\nbased approaches and serves as a strong baseline.\nHowever, the storage and replay of data from\nprevious tasks raise privacy concerns, particularly\nwhen dealing with sensitive information.\nRegularization-based approaches (Kirkpatrick\net al., 2017; Li and Hoiem, 2017; Farajtabar et al.,\n2020; Smith et al., 2023) incorporate additional\n10665\nterms into the loss function to penalize changes in\ncrucial weights. For instance, Orthogonal Gradient\nDescent (OGD) (Farajtabar et al., 2020) constrains\nthe parameters to move within the orthogonal\nspace defined by the gradients of previous tasks.\nHowever, OGD requires storing gradients of all\nhistorical data, which becomes infeasible for large\nlanguage models. Another work introduces C-\nLoRA (Smith et al., 2023) for continual learning\nof text-conditioned images, which regularizes the\nsimilarity of new LoRA parameters with historical\nversions, limiting their learning plasticity to new\ntasks.\nArchitecture-based approaches (Wang et al.,\n2023e; Razdaibiedina et al., 2023) focus on\ndynamically expanding model capacity or isolating\nexisting model weights to mitigate interference\nbetween new and old tasks. Progressive Prompts\n(Razdaibiedina et al., 2023) learns separate prompts\nfor each incoming task and sequentially con-\ncatenates them with previously learned prompts.\nHowever, such approaches essentially train distinct\nexpert models for different tasks, which restricts\ntheir generalization ability to unseen tasks.\nIn contrast to existing methods, our approach\noffers unique advantages in terms of data privacy,\nmodel parameter efficiency, and generalization\ncapability, as discussed in the previous sections.\n5.2 Parameter Efficient Tuning\nParameter Efficient Tuning (PET) (He et al.,\n2021) has emerged as a significant research\ndirection aimed at optimizing model performance\nwhile minimizing computational resources and\nannotation efforts. Various approaches have been\nproposed to achieve parameter efficiency in tuning,\nincluding adapters (Houlsby et al., 2019), prompt\nlearning (Lester et al., 2021), LoRA (Hu et al.,\n2021), and fine-tuning subsets of the model (Zaken\net al., 2021). One particularly promising approach\nis the use of low-rank adapters, which have\ndemonstrated effectiveness in adapting models to\nnew tasks with minimal additional parameters.\nBuilding upon LoRA, we propose an efficient\ncontinual learning neural architecture in this work.\nOur approach involves layering low-rank adapters\non the key and value projection matrices of\ntransformer blocks. By leveraging the benefits\nof low-rank adapters, we aim to strike a balance\nbetween model performance and computational\nefficiency in the context of continual learning.\n6 Conclusion\nIn this paper, we introduce O-LoRA, a novel ap-\nproach that leverages orthogonal subspace learning\nfor continual learning in language models. O-\nLoRA systematically addresses catastrophic forget-\nting by adopting an incremental learning strategy\nwithin orthogonal subspaces. Distinguished by\nits data privacy considerations, efficient model\nparameter utilization, and robust generalization to\nnovel tasks, our method stands out. Empirical eval-\nuations underscore O-LoRA‚Äôs efficacy in tackling\nthe intricacies of continual learning.\nLimitations\nWhile our method has demonstrated effectiveness\nin empirical evaluations, there are a few limitations\nto consider. Firstly, its performance and appli-\ncability in more complex scenarios with a large\nnumber of tasks, such as hundreds of tasks, require\nfurther investigation. Additionally, although our\nmethod does not rely on task identification during\ninference, it still requires task identification during\ntraining to train different LoRA parameters for\neach task. Exploring methods for task-agnostic\ntraining would be a valuable future direction. By\naddressing these limitations, we can enhance the\nscalability and task-agnostic capabilities of our\napproach, further advancing the field of continual\nlearning for language models.\nAcknowledgements\nThe authors express their gratitude to the anony-\nmous reviewers for their insightful comments.\nWe also wish to acknowledge Hang Yan. Even\nthough he wasn‚Äôt directly involved in this study,\nhis foundational guidance in continual learning\nhas been pivotal to our research. This work was\npartially funded by Shanghai Academic Research\nLeader Program 22XD1401100.\nReferences\nGuirong Bai, Shizhu He, Kang Liu, and Jun Zhao. 2022.\nIncremental intent detection for medical domain\nwith contrast replay networks. In Findings of the\nAssociation for Computational Linguistics: ACL\n2022, pages 3549‚Äì3556.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\n10666\nlearners. Advances in neural information processing\nsystems, 33:1877‚Äì1901.\nArslan Chaudhry, Marc‚ÄôAurelio Ranzato, Marcus\nRohrbach, and Mohamed Elhoseiny. 2018. Efficient\nlifelong learning with a-gem. arXiv preprint\narXiv:1812.00420.\nArslan Chaudhry, Marcus Rohrbach, Mohamed El-\nhoseiny, Thalaiyasingam Ajanthan, Puneet Kumar\nDokania, Philip H. S. Torr, and Marc‚ÄôAurelio\nRanzato. 2019. Continual learning with tiny episodic\nmemories. ArXiv, abs/1902.10486.\nCyprien de Masson D‚ÄôAutume, Sebastian Ruder,\nLingpeng Kong, and Dani Yogatama. 2019. Episodic\nmemory in lifelong language learning. Advances in\nNeural Information Processing Systems, 32.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for language\nunderstanding. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 4171‚Äì4186, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nMehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang\nLi. 2020. Orthogonal gradient descent for continual\nlearning. In International Conference on Artificial\nIntelligence and Statistics, pages 3762‚Äì3773. PMLR.\nXu Han, Yi Dai, Tianyu Gao, Yankai Lin, Zhiyuan\nLiu, Peng Li, Maosong Sun, and Jie Zhou. 2020.\nContinual relation learning via episodic memory\nactivation and reconsolidation. In Proceedings of\nthe 58th Annual Meeting of the Association for\nComputational Linguistics, pages 6429‚Äì6440.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2021. Towards a\nunified view of parameter-efficient transfer learning.\narXiv preprint arXiv:2110.04366.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language\nunderstanding. arXiv preprint arXiv:2009.03300.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efficient transfer learning for nlp.\nIn International Conference on Machine Learning,\npages 2790‚Äì2799. PMLR.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. 2021. Lora: Low-rank adaptation of large\nlanguage models. ArXiv, abs/2106.09685.\nYufan Huang, Yanzhe Zhang, Jiaao Chen, Xuezhi Wang,\nand Diyi Yang. 2021. Continual learning for text clas-\nsification with information disentanglement based\nregularization. arXiv preprint arXiv:2104.05489.\nZixuan Ke and Bin Liu. 2022. Continual learning of\nnatural language processing tasks: A survey. ArXiv,\nabs/2211.12701.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A\nRusu, Kieran Milan, John Quan, Tiago Ramalho,\nAgnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the national academy of sciences ,\n114(13):3521‚Äì3526.\nBrian Lester, Rami Al-Rfou, and Noah Constant.\n2021. The power of scale for parameter-efficient\nprompt tuning. In Proceedings of the 2021\nConference on Empirical Methods in Natural\nLanguage Processing, pages 3045‚Äì3059, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nZhizhong Li and Derek Hoiem. 2017. Learning without\nforgetting. IEEE transactions on pattern analysis\nand machine intelligence, 40(12):2935‚Äì2947.\nDavid Lopez-Paz and Marc‚ÄôAurelio Ranzato. 2017.\nGradient episodic memory for continual learning.\nAdvances in neural information processing systems,\n30.\nYun Luo, Zhen Yang, Xuefeng Bai, Fandong Meng,\nJie Zhou, and Yue Zhang. 2023. Investigating\nforgetting in pre-trained representations through\ncontinual learning. arXiv preprint arXiv:2305.05968.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher\nPotts. 2011. Learning word vectors for sentiment\nanalysis. In Proceedings of the 49th Annual\nMeeting of the Association for Computational\nLinguistics: Human Language Technologies, pages\n142‚Äì150, Portland, Oregon, USA. Association for\nComputational Linguistics.\nMichael McCloskey and Neal J. Cohen. 1989.\nCatastrophic interference in connectionist networks:\nThe sequential learning problem. Psychology of\nLearning and Motivation, 24:109‚Äì165.\nOpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray,\net al. 2022. Training language models to follow\ninstructions with human feedback. Advances in\nNeural Information Processing Systems, 35:27730‚Äì\n27744.\nChengwei Qin and Shafiq Joty. 2021. Lfpt5: A\nunified framework for lifelong few-shot language\nlearning based on prompt tuning of t5. arXiv preprint\narXiv:2110.07298.\n10667\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21:1‚Äì67.\nAnastasia Razdaibiedina, Yuning Mao, Rui Hou,\nMadian Khabsa, Mike Lewis, and Amjad Almahairi.\n2023. Progressive prompts: Continual learning for\nlanguage models. In The Eleventh International\nConference on Learning Representations.\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz,\nTimothy Lillicrap, and Gregory Wayne. 2019.\nExperience replay for continual learning. Advances\nin Neural Information Processing Systems, 32.\nGobinda Saha, Isha Garg, and Kaushik Roy. 2021.\nGradient projection memory for continual learning.\nArXiv, abs/2103.09762.\nJames Seale Smith, Yen-Chang Hsu, Lingyu Zhang,\nTing Hua, Zsolt Kira, Yilin Shen, and Hongxia Jin.\n2023. Continual diffusion: Continual customization\nof text-to-image diffusion with c-lora. arXiv preprint\narXiv:2304.06027.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and\nefficient foundation language models. arXiv preprint\narXiv:2302.13971.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019. Superglue: A\nstickier benchmark for general-purpose language\nunderstanding systems. Advances in neural\ninformation processing systems, 32.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nLiyuan Wang, Xingxing Zhang, Hang Su, and Jun\nZhu. 2023a. A comprehensive survey of continual\nlearning: Theory, method and application. ArXiv,\nabs/2302.00487.\nXiao Wang, Shihan Dou, Limao Xiong, Yicheng Zou,\nQi Zhang, Tao Gui, Liang Qiao, Zhanzhan Cheng,\nand Xuanjing Huang. 2022a. Miner: Improving\nout-of-vocabulary named entity recognition from an\ninformation theoretic perspective. arXiv preprint\narXiv:2204.04391.\nXiao Wang, Yuansen Zhang, Tianze Chen, Songyang\nGao, Senjie Jin, Xianjun Yang, Zhiheng Xi, Rui\nZheng, Yicheng Zou, Tao Gui, et al. 2023b.\nTrace: A comprehensive benchmark for continual\nlearning in large language models. arXiv preprint\narXiv:2310.06762.\nXiao Wang, Weikang Zhou, Qi Zhang, Jie Zhou,\nSongyang Gao, Junzhe Wang, Menghan Zhang,\nXiang Gao, Yunwen Chen, and Tao Gui. 2023c.\nFarewell to aimless large-scale pretraining: Influ-\nential subset selection for language model. arXiv\npreprint arXiv:2305.12816.\nXiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze\nChen, Yuansen Zhang, Rui Zheng, Junjie Ye,\nQi Zhang, Tao Gui, et al. 2023d. Instructuie:\nMulti-task instruction tuning for unified information\nextraction. arXiv preprint arXiv:2304.08085.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran,\nAnjana Arunkumar, David Stap, Eshaan Pathak,\nGiannis Karamanolakis, Haizhi Lai, Ishan Purohit,\nIshani Mondal, Jacob Anderson, Kirby Kuznia,\nKrima Doshi, Kuntal Kumar Pal, Maitreya Patel,\nMehrad Moradshahi, Mihir Parmar, Mirali Purohit,\nNeeraj Varshney, Phani Rohitha Kaza, Pulkit\nVerma, Ravsehaj Singh Puri, Rushang Karia, Savan\nDoshi, Shailaja Keyur Sampat, Siddhartha Mishra,\nSujan Reddy A, Sumanta Patro, Tanay Dixit, and\nXudong Shen. 2022b. Super-NaturalInstructions:\nGeneralization via declarative instructions on 1600+\nNLP tasks. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 5085‚Äì5109, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nZhicheng Wang, Yufang Liu, Tao Ji, Xiaoling Wang,\nYuanbin Wu, Congcong Jiang, Ye Chao, Zhencong\nHan, Ling Wang, Xu Shao, and Wenqiu Zeng.\n2023e. Rehearsal-free continual language learning\nvia efficient parameter isolation. ArXiv.\nZifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,\nRuoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot,\nJennifer Dy, and Tomas Pfister. 2022c. Learning to\nprompt for continual learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 139‚Äì149.\nTongtong Wu, Massimo Caccia, Zhuang Li, Yuan-\nFang Li, Guilin Qi, and Gholamreza Haffari. 2022.\nPretrained language model in continual learning: A\ncomparative study. In International Conference on\nLearning Representations.\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen\nDing, Boyang Hong, Ming Zhang, Junzhe Wang,\nSenjie Jin, Enyu Zhou, et al. 2023. The rise and\npotential of large language model based agents: A\nsurvey. arXiv preprint arXiv:2309.07864.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg.\n2021. Bitfit: Simple parameter-efficient fine-tuning\n10668\nfor transformer-based masked language-models.\narXiv preprint arXiv:2106.10199.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text\nclassification. Advances in neural information\nprocessing systems, 28.\nA Appendix\nA.1 Implementation Details\nAll of our experiments on t5 models were con-\nducted on a machine equipped with 8 NVIDIA\nGeForce RTX 3090 and were implemented using\nDeepSpeed repository. For all orders of task\nstreams, We trained the models with one epoch,\na constant learning rate of 1e-3, a batch size of\n64(a batch size of 8 per GPU), a dropout rate of\n0.1, and a weight decay rate of 0. Only the values\nof Œª1 and Œª2 are different among order 1 to 6. For\norder 1, order 2 and order 3, we set Œª1 = 0.5, 0.5,\n0.5, 0.5, Œª2 = 0, 0, 0, 0. For every task in order\n4(MNLI, CB, WiC, COPA, QQP, BoolQA, RTE,\nIMDB, Yelp, Amazon, SST-2, DBpedia, Agnews,\nMultiRC, Yahoo), we set Œª1 = 0.5, 0.5, 0.5, 0.5,\n0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 5, 5, 5, 5, and Œª2\n= 0, 0, 0.1, 0, 0, 0, 0.3, 0.1, 0.05, 0, 0.1, 0.1, 0.1,\n0, 0.1 respectively. For order 5(MultiRC, BoolQA,\nWiC, MNLI, CB, COPA, QQP, RTE, IMDB, SST-2,\nDBpedia, Agnews, Yelp, Amazon, Yahoo), we set\nŒª1 = 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, andŒª2 =\n0, 0.1, 0, 0.1, 0.1, 0, 0.1, 0.3, 0.1, 0.5, 0, 0.1, 0, 0.1,\n0.1 respectively. For order 6(Yelp, Amazon, MNLI,\nCB, COPA, QQP, RTE, IMDB, SST-2, DBpedia,\nAgnews, Yahoo, MultiRC, BoolQA, WiC), we set\nŒª1 = 0.5, 0.5, 0.02, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n0.5, 0.5, 0.5, 0.5, 0.5, and Œª2 = 0, 0, 0, 0.1, 0, 0,\n0.3, 0, 0.1, 0.1, 0, 0.1, 0, 0.1, 0.3 respectively.\nA.2 Datasets\nTable 4 shows details of the 15 datasets we used for\nour CL experiments, along with their evaluation\nmetrics. Overall, we used datasets from CL\nbenchmark (Zhang et al., 2015), GLUE (Wang\net al., 2018) and SuperGLUE (Wang et al., 2019)\nbenchmarks, and added IMDB movie reviews\ndataset, following (Razdaibiedina et al., 2023).\nA.3 Task Sequence Orders\nWe report task orders used for our CL experiments\nacross T5 and LLaMA models in Table 5.\nA.4 Task Instructions\nTable 6 shows prompts for different tasks. NLI\ndenotes natural language inference, including\nMNLI, RTE and CB. SC denotes sentiment\nanalysis, including Amazon, Yelp, SST-2 and\nIMDB. TC denotes topic classification, including\nAG News, Dbpedia and Yahoo.\nA.5 Detailed results of MMLU Zero-shot\n10669\nDataset name Category Task Domain Metric\n1. Yelp CL Benchmark sentiment analysis Yelp reviews accuracy\n2. Amazon CL Benchmark sentiment analysis Amazon reviews accuracy\n3. DBpedia CL Benchmark topic classification Wikipedia accuracy\n4. Yahoo CL Benchmark topic classification Yahoo Q&A accuracy\n5. AG News CL Benchmark topic classification news accuracy\n6. MNLI GLUE NLI various accuracy\n7. QQP GLUE paragraph detection Quora accuracy\n8. RTE GLUE NLI news, Wikipedia accuracy\n9. SST-2 GLUE sentiment analysis movie reviews accuracy\n10. WiC SuperGLUE word sense disambiguation lexical databases accuracy\n11. CB SuperGLUE NLI various accuracy\n12. COPA SuperGLUE QA blogs, encyclopedia accuracy\n13. BoolQA SuperGLUE boolean QA Wikipedia accuracy\n14. MultiRC SuperGLUE QA various accuracy\n15. IMDB SuperGLUE sentiment analysis movie reviews accuracy\nTable 6: The details of 15 datasets used in our CL experiments. NLI denotes natural language inference, QA denotes\nquestions and answers task. First five tasks correspond to the standard CL benchmark, all other tasks are used in\nlong-sequence experiments.\nOrder Model Task Sequence\n1 T5, LLaMA dbpedia ‚Üí amazon ‚Üí yahoo ‚Üí ag\n2 T5, LLaMA dbpedia ‚Üí amazon ‚Üí ag ‚Üí yahoo\n3 T5, LLaMA yahoo ‚Üí amazon ‚Üí ag ‚Üí dbpedia\n4 T5 mnli ‚Üí cb ‚Üí wic ‚Üí copa ‚Üí qqp ‚Üí boolqa ‚Üí rte ‚Üí imdb ‚Üí\nyelp ‚Üí amazon ‚Üí sst-2 ‚Üí dbpedia ‚Üí ag ‚Üí multirc ‚Üí yahoo\n5 T5 multirc ‚Üí boolqa ‚Üí wic ‚Üí mnli ‚Üí cb ‚Üí copa ‚Üí qqp ‚Üí rte\n‚Üí imdb ‚Üí sst-2 ‚Üí dbpedia ‚Üí ag ‚Üí yelp ‚Üí amazon ‚Üí yahoo\n6 T5 yelp ‚Üí amazon ‚Üí mnli ‚Üí cb ‚Üí copa ‚Üí qqp ‚Üí rte ‚Üí imdb ‚Üí\nsst-2 ‚Üí dbpedia ‚Üí ag ‚Üí yahoo ‚Üí multirc ‚Üí boolqa ‚Üí wic\nTable 7: Six different orders of task sequences used for continual learning experiments. Orders 1-3 correspond\nto the standard CL becnhmark adopted by prior works. Orders 4-6 are long-sequence orders spanning 15 tasks,\nfollowing (Razdaibiedina et al., 2023).\nTask Prompts\nNLI What is the logical relationship between the \"sentence 1\" and the \"sentence 2\"?\nChoose one from the option.\nQQP Whether the \"first sentence\" and the \"second sentence\" have the same meaning?\nChoose one from the option.\nSC What is the sentiment of the following paragraph? Choose one from the option.\nTC What is the topic of the following paragraph? Choose one from the option.\nBoolQA According to the following passage, is the question true or false? Choose one\nfrom the option.\nMultiRC According to the following passage and question, is the candidate answer true\nor false? Choose one from the option.\nWiC Given a word and two sentences, whether the word is used with the same sense\nin both sentence? Choose one from the option.\nTable 8: Instructions for different tasks.\n10670\nMMLU-task LLaMA-7B Alpaca-LoRA Alpaca-LoRA-CL Alpaca-inc-LoRA-CLAlpaca-OLoRA-CL\nmath 27 25.9 20.4 23.4 21.6\nhealth 38.2 40.9 24.8 30.7 36.1\nphysics 30.8 32.5 22.2 27.7 30.3\nbusiness 40.3 50.3 25.2 32.3 38.7\nbiology 33.5 38.1 21.8 29.1 35.7\nchemistry 25.4 27.7 17.2 20.1 23.8\ncomputer science 30.6 35 27.2 30.8 31.3\neconomics 31.3 31.4 23 27 28\nengineering 28.3 32.4 22.8 23.4 24.1\nphilosophy 32.6 34.5 23 26.1 32.1\nother 37.2 46.9 24.6 33.5 42.1\nhistory 39 45.8 25.1 33.3 42.3\ngeography 31.8 44.4 18.2 25.8 33.3\npolitics 37.7 40.1 21.8 28.2 33.6\npsychology 39.4 40.4 22.8 28.3 35\nculture 40.1 49.1 25.9 32.5 41\nlaw 31.9 32.5 23.6 28.5 32.4\nWeighted Avg. 34.4 37.5 23.3 28.6 33.6\nTable 9: Detailed zero-shot results on MMLU benchmark of different CL methods.\n10671",
  "topic": "Forgetting",
  "concepts": [
    {
      "name": "Forgetting",
      "score": 0.8150525689125061
    },
    {
      "name": "Computer science",
      "score": 0.7740395665168762
    },
    {
      "name": "Generalization",
      "score": 0.5843371748924255
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5738843679428101
    },
    {
      "name": "Machine learning",
      "score": 0.5245756506919861
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.5023791790008545
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.4891236424446106
    },
    {
      "name": "Linear subspace",
      "score": 0.4399409592151642
    },
    {
      "name": "Subspace topology",
      "score": 0.43689581751823425
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    }
  ]
}