{
  "title": "The Promises and Pitfalls of LLMs as Feedback Providers: A Study of Prompt Engineering and the Quality of AI-Driven Feedback",
  "url": "https://openalex.org/W4387221424",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5017859484",
      "name": "Lucas Jasper Jacobsen",
      "affiliations": [
        "Leuphana University of Lüneburg"
      ]
    },
    {
      "id": "https://openalex.org/A2478082358",
      "name": "Kira Elena Weber",
      "affiliations": [
        "Universität Hamburg"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3179163952",
    "https://openalex.org/W4321351832",
    "https://openalex.org/W4379919049",
    "https://openalex.org/W6724158187",
    "https://openalex.org/W4281611690",
    "https://openalex.org/W4367369313",
    "https://openalex.org/W4366783381",
    "https://openalex.org/W4324046518",
    "https://openalex.org/W3208686870",
    "https://openalex.org/W2137247190",
    "https://openalex.org/W2133012565",
    "https://openalex.org/W2913142182",
    "https://openalex.org/W564614949",
    "https://openalex.org/W6730284005",
    "https://openalex.org/W4256465275",
    "https://openalex.org/W4221163727",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W2030441548",
    "https://openalex.org/W2788534938",
    "https://openalex.org/W6907700918",
    "https://openalex.org/W4366980161",
    "https://openalex.org/W2098201144",
    "https://openalex.org/W2564331714",
    "https://openalex.org/W4311738254",
    "https://openalex.org/W2968981890",
    "https://openalex.org/W1982499326",
    "https://openalex.org/W1983362322",
    "https://openalex.org/W4223626855",
    "https://openalex.org/W4381149642",
    "https://openalex.org/W7011223190",
    "https://openalex.org/W3027748857",
    "https://openalex.org/W2888524687",
    "https://openalex.org/W2913482472",
    "https://openalex.org/W2955196867",
    "https://openalex.org/W3177058820",
    "https://openalex.org/W2981863007",
    "https://openalex.org/W2969717756",
    "https://openalex.org/W2035212726",
    "https://openalex.org/W4318257512",
    "https://openalex.org/W2296794688",
    "https://openalex.org/W2089801766",
    "https://openalex.org/W2746986659",
    "https://openalex.org/W2122410182",
    "https://openalex.org/W4248496639",
    "https://openalex.org/W2497994374"
  ],
  "abstract": "Artificial intelligence (AI) in higher education (HE) is reshaping teaching and learning, and feedback provided by large language models (LLMs) seems to have an impact on student learning. However, few empirical studies have compared the quality of LLM feedback with the feedback quality of real persons. Therefore, this study addresses the following questions: What prompts are needed to ensure high-quality LLM feedback in HE? How does feedback from novices, experts, and LLMs differ in terms of quality and content accuracy? We developed a learning goal with three errors and a theory-based manual to evaluate prompt quality. Specifically, three prompts of varying quality were created and used to generate feedback from ChatGPT-4. We provided the highest-quality prompt to novices and experts. Our results showed that only the best prompt produced consistently high-quality feedback. Additionally, LLM and expert feedback were significantly better than novice feedback, with LLM feedback being both faster and better than expert feedback in the categories of explanation, questions, and specificity. This suggests that LLM feedback can be a high-quality and efficient alternative to expert feedback. However, we postulate that prompt quality is crucial, highlighting the need for prompting guidelines and human expertise.",
  "full_text": "1 \n \nThe Promises and Pitfalls of LLMs as Feedback \nProviders:  \nA Study of Prompt Engineering and the Quality of \nAI-Driven Feedback1 \n1Both authors contributed equally to this work. \n \nLucas Jasper Jacobsen2 & Kira Elena Weber3 \n \n2Leuphana Universität Lüneburg \n3Universität Hamburg \n \n2 \n \nAbstract \nArtificial intelligence (AI) in higher education (HE) is reshaping teaching and learning, and \nfeedback provided by l arge language models (LLMs) seems to have an impact on student \nlearning. However, few empirical studies have compared the quality of LLM feedback with the \nfeedback quality of real persons. Therefore, this study addresses the following questions: What \nprompts are needed to ensure high- quality LLM feedback in HE? How does feedback from \nnovices, experts, and LLMs differ in terms of quality and content accuracy? We developed a \nlearning goal with three errors and a theory -based manual to evaluate prompt quality. \nSpecifically, three prompts of varying quality were created and used to generate feedback from \nChatGPT-4. We provided the highest -quality prompt to novices and experts. Our results \nshowed that only the best prompt produced consistently high- quality feedback. Additionally, \nLLM and expert feedback were significantly better than novice feedback, with LLM feedback \nbeing both faster and better than expert feedback in the categories of explanation, questions, \nand specificity. This suggests that LLM feedback can be a high-quality and efficient alternative \nto expert feedback. However, we postulate that prompt quality is crucial, highlighting the need \nfor prompting guidelines and human expertise. \nKeywords: AI; Feedback; Prompt Engineering; Teacher Education\n1 \n \n1. Introduction \nAI is transforming not only numerous industries but also the education sector, leveraging \ntechnologies like machine learning and natural language processing to improve individual \nlearning experiences (Harry & Sayudin, 2023). Accordingly, the use of Artificial Intelligence \n(AI) in higher education (AIEd) has grown significantly over the past five years (Crompton & \nBurke, 2023) and is now reshaping the academic environment. This development has sparked \nextensive debates regarding its impact on the future of t eaching and learning (von Garrel & \nMayer, 2023). For example, several studies have examined the impact of automated writing \nevaluation feedback tools on students ’ writing performance ( for a  meta-analysis, see  \nFleckenstein et al., 2023). In general, feedback is considered an integral part of educational \nprocesses in higher education ( HE), but as Henderson et al. (2019) noted, it raises issues as \nwell: \nFeedback is a topic of hot debate in universities. Everyone agrees that it is important. \nHowever, students report a lot of dissatisfaction: they don’ t get what they want from \nthe comments they receive on their work and they don’ t find it timely. Teaching staff \nfind it burdensome, are concerned that students do not engage with it and wonder \nwhether the effort they put in is worthwhile. (p. 3) \nThe provision of feedback represents both an opportunity and a challenge, as it can also have \nnegative consequences for learning processes (Kluger & DeNisi, 1996). Therefore, high-\nquality feedback is needed in HE , characterized by features such as concreteness, activation, \nand empathy (Prilop et al., 2019). Unfortunately, the human and financial resources required \nto provide high- quality feedback are often lacking  (Demszky et al., 2023), which is why AI \nfeedback is promising, offering the potential to optimize teaching and learning processes in \n2 \n \nHE. According to the United Nations Educational, Scientific and Cultural Organization  \n(UNESCO) (2024), AI tools should be used to enhance the professional development of \nteachers, allowing them “to practice skills and receive feedback” (p. 42).  \nFollowing up on these ideas, this study will specifically look at the potential of Large Language \nModel (LLM-)based feedback in HE by comparing it to novice and expert  feedback. For \nteachers to use AI tools, they need adequate “ application skills” (UNESCO, 2024, p. 22). \nConsequently, we present a theory-driven and evidence-based manual for prompt engineering \nthat can facilitate teachers ’ use of AI a nd improve their ability to apply it in the educational \ncontext. We address the following questions in this study: 1 ) What kind s of prompt s are \nrequired to ensure high- quality AI feedback? 2)  What are the differences between novice, \nexpert, and AI feedback in terms of feedback quality? \n2. Theoretical Background \n2.1 Artificial Intelligence in Higher Education \nAlthough AIEd has been around for about three decades, educators struggle with understanding \nhow to use it for pedagogical purposes and its implications for teaching and learning in HE \n(Zawacki-Richter et al., 2019). However, the field of AIEd is growing and developing rapidly, \nand there is an urgent need to improve academic understanding of its potential and limitations \n(Crompton & Burke, 2023). It has been argued that AIEd, including LLMs like ChatGPT, can \nincrease student engagement, collaboration, and the accessibility of education (Cotton et al., \n2023). It has been used for various purposes, including assessment/evaluation, predicti on, AI \nassistance, intelligent tutoring, and managing student learning (Crompton & Burke, 2023). \nNevertheless, empirical research on this new technology, particularly in the context of HE, is \n3 \n \nstill in its infancy, and more research is needed. This paper contributes to the ongoing discourse \nby developing a theory-driven manual for analyzing the quality of prompts, thereby ensuring \nhigh-quality output. Furthermore, we critically examine the quality of LLM  feedback in HE, \nespecially in comparison to novice and expert feedback. \n2.1.1 Prompt Engineering for Large Language Models in Higher Education \nTo effectively use LLMs in HE, it is crucial to recognize the importance of prompt engineering. \nResearch must answer the question of how to write prompts that yield high -quality output. In \nsimple terms, prompt engineering is the process of designing effective questions or stimuli, \nknown as “ prompts,” for LLMs. The aim is to get clear, relevant answers. Essentially, the \nprocess involves fine -tuning questions for LLMs  to produce the desired results. Although \nprompt engineering is a fairly new research topic, findings have consistently suggested that the \nquality of the output  of LLMs is not merely determined by their foundational algorithms or \ntraining data. Equally crucial is the clarity and accuracy of the prompts they are given (Bsharat \net al., 2023; Lo, 2023; Zamfrescu-Pereira et al., 2023).  \nStudies have highlighted different aspects of prompt engineering (e.g., ChatGPT & Enkin \n2023; Lo 2023; Zamfrescu -Pereira et al., 2023 ). For example , Kipp (2023) noted that four \nprimary elements (context, question, format, and examples) should serve as modular guidelines \nfor constructing effective prompts. Enkin (2023) proposed five factors that influence prompt \nselection: user intent , model understanding, domain specificity , clarity and specificity, and \nconstraints. In addition, Lo (2023) developed the CLEAR framework , which comprises five \nkey elements of effective prompts: concise, logical, explicit, adaptive, and reflective.  \nThe ability to develop prompts is crucial  to support future skills in an increasingly AI -\ninfluenced world (UNESCO, 2024). However, creating effective prompts can be challenging \n4 \n \nand may lead to unexpected results (Zamfrescu -Pereira et al., 2023). To the best of our \nknowledge, there are no manual s for analyzing the quality of  prompts within HE , and no \ninvestigations have been performed to determine whether such guidelines actually improve the \noutput of LLMs. This study aims to develop such a manual and investigate whether there are \ndifferences in output when feeding an LLM (ChatGPT-4) with different kinds of prompts. \n2.2 Feedback \nFeedback is widely recognized as an integral component of individual and institutional learning \nand developmental processes ( Prilop et al., 2019; Weber  et al., 2019) and thus as a crucial \ncomponent in HE (Henderson et al., 2019). Feedback is defined as information offered to an \nindividual concerning their current performance to facilitate improvement in future endeavors \n(Narciss, 2013), and individuals often struggle to effectively reflect on, manage, and adjust \ntheir actions or tasks in the absence of appropriate feedback (Weber et al., 2018). In the context \nof teacher education, pre -service teachers receive feedback after actual classroom practice or \nspecific skill training, which could be from peers with a similar knowledge base (novices) or \nfrom experts with knowledge authority ( Weber et al., 2018;  Lu, 2010). However, while  the \nincorporation of feedback sessions in teacher education is becoming increasingly prevalent \n(Weber et al., 2018; Kraft et al., 2018), feedback designs are often compromised, as feedback \nfrom novices is not as high in quality as expert feedback ( Weber et al., 2019). In addition,  \neducators \n(experts) frequently express concerns about a lack of time for delivering high-quality \nfeedback (Demszky et al., 2023). \n2.2.1 Feedback Quality \nEricsson et al. (1993) underscored that substantial enhancements in performance are achievable \nonly through high-quality feedback. Similarly, Prilop et al. (2021) showed that the quality of \n5 \n \nfeedback is crucial  to its acceptance and for facilitating the continuous development of \nprofessional competencies among teachers. Regarding  the quality of feedback, Prilop et al.  \n(2019, 2021) provided criteria for effective feedback for teachers based on various studies in \nother domains (e.g., Gielen & De Wever, 2015; Prins et al., 2006). Summarizing these criteria, \neffective feedback should consistently be specific , empathetic, and engaging (Prilop et al., \n2019; Prilop & Weber, 2023). On a cognitive level (specific and engaging), numerous studies \n(e.g., Strijbos  et al., 2010) have suggested that effective feedback should incorporate both \nevaluative and tutorial components. Therefore, individuals providing feedback should assess a \nparticular situation with a firm emphasis on content, offer and explain alternative actions, and \npose engaging questions. At the affective -motivational level (empathetic), the delivery of \nfeedback is crucial. Ultimately, according to Prins et al. (2006), effective peer feedback should \nbe presented in first person. This perspective suggests that feedback is subjective and open to \ndialogue rather than an indisputable fact. In our previous research ( Prilop et al. , 2021), we \nfound that critiques should always be counterbalanced by positive evaluations. Regarding the \ncriteria for high-quality feedback, a few studies (Prins et al., 2006; Weber et al. , 2019) have \nexamined the impact of expertise on feedback quality by comparing the feedback provided by \nnovices and experts. \n2.2.2 Novice and Expert Feedback \nHattie and Timperley (2007) emphasize d that feedback can be provided by different agents, \nsuch as experts or novices. The disparity in the quality of feedback given by experts and novices \nhas been systematically examined in a few  studies. Prins et al. (2006) compared expert and \nnovice feedback in medical education, finding that experts utilized more criteria, provided \nmore situation-specific comments and positive remarks, and frequently adopted a first -person \nperspective style. They also observed that a significant portion of novices either did not pose \n6 \n \nany reflective questions (59%) or failed to offer alternative suggestions (44%). Similar \nobservations were made in the domain of teacher education by Weber et al.  (2019). \nSpecifically, they reported that expert feedback was more specific, question- rich, and first -\nperson-perspective-oriented than pre -service teachers ’ feedback at the bachelor level. Pre-\nservice teachers seldom included specific descriptions of teaching situations in their feedback \nand rarely utilized activating questions. In sum, expert feedback seems to be of higher quality \nthan novice feedback. However, the provision of adaptive feedback is resource intensive  if \ndone manually for every learner’s task solution, and accordingly, experts in HE often struggle \nto provide high-quality feedback due to insufficient resources (Henderson et al., 2019). LLM  \nfeedback offers a potential solution (Sailer et al., 2023), but it remains unclear whether  LLM \nfeedback is qualitatively equivalent to expert feedback in HE. \n \n2.2.3 Large Language Models as Feedback Providers \n \nThe integration of AI into education is changing teaching methods, curriculum planning, and \nstudent engagement (Wang et al., 2024). Recent studies have investigated the use of LLMs to \ngenerate adaptive feedback.  For example, in their meta -analysis, Fleckenstein et al. (2023) \nestablished that the utilization of automated feedback could enhance students’ writing progress. \nZhu et al. (2020) examined an LLM-powered feedback system in a high school climate activity \ntask and found that it helped students refine their scientific arguments. Sailer et al. (2023) \ninvestigated the impact of adaptive feedback on pre -service teachers’ diagnostic reasoning, \nshowing that while it improved justification quality in written assignments, it did not enhance \ndiagnostic accuracy. In contrast, s tatic feedback negatively affected learning in dyads. \nAdditionally, Bernius et al. (2022) used natural language processing models to generate \nfeedback for student responses in large courses, reducing grading effort by up to 85% and being \nperceived as highly precise. Kasneci et al. (2023) highlighted how LLMs can assist university \n7 \n \nand high school teachers with research and writing tasks, improving efficiency and reducing \nthe time spent on personalized feedback (Kasneci et al., 2023).  In a recent study, Dai et al. \n(2024) investigated the ability of two GPT model versions  (GPT-3.5 and GPT -4) to provide \nfeedback on students’ open-ended writing assignments. The feedback generated by GPT -3.5 \nand GPT -4 was compared to that of human instructors, evaluating three key aspects: \nreadability, the presence of effective feedback components, and reliability in assessing student \nperformance. The results indicated that (1) both GPT -3.5 and GPT -4 consistently produced \nmore readable feedback than human instructors, (2) GPT -4 outperformed GPT-3.5 as well as \nhuman instructors by delivering feedback enriched with crucial components such as feeding-\nup, feeding-forward, and self-regulation strategies, and (3) GPT-4 exhibited superior feedback \nreliability compared to GPT -3.5. Considering the results of these previous studies, LLMs \nappear to be promising feedback givers. However, there is still a lack of empirical evidence in \nthe context of teacher education as well as  on the quality of feedback in terms of the criteria \nfor effective feedback (specific, empathetic, and engaging). Moreover, our study addresses the \nimportance of prompt engineering when using LLMs as feedback providers. \n3. Aim of the Study \nOur paper seeks to enhance the current understanding of LLM -based feedback in HE by \naddressing the following research questions: \na. What kinds of prompts are required to ensure high-quality LLM feedback? \nb. What are the differences between novice, expert, and AI feedback in terms of \nfeedback quality (specific, empathetic, and engaging)?\n \n \n8 \n \nFigure 1 shows our heuristic working model, which includes the quality of prompts, the quality \nof feedback, and potential outcomes that should be investigated in future studies.  \nFigure 1: Heuristic Working Model adapted from Narciss (2008) and Pekrun et al. (2023). \n \n4. Method \n4.1 Development of a Theory-Driven Prompt Manual  \nWe developed a theory-driven coding manual to analyze prompt quality for LLMs, integrating \nvarious prompt engineering approaches. Our design followed Kipps’ (2023) four key elements \nof prompt engineering and considered five factors influencing prompt selection highlighted by \nChatGPT and Enkin (2023). Lastly, we applied Lo’s (2023) CLEAR framework to refine each \nprompt module. This resulted in a manual with eight distinct categories of prompt quality (see \nTable 1). \nTable 1: Prompt manual to ensure the development of high-quality prompts. \nCategory Subcategory Good  Code Average  Code Suboptimal  Code \n\n9 \n \nContext Role The role of the \nLLM and of the \nperson asking \nthe question is \nexplained \n2 Only one of \nthe roles is \nexplained \n1 Neither the \nrole of the \nLLM nor the \nrole of the \nperson asking \nthe question is \nexplained \n0 \n Target \naudience \nThere is a \nclearly defined \nand described \ntarget audience \n2 The target \naudience is \nroughly \ndescribed \n1 The target \naudience is not \nspecified \n0 \n Channel The channel is \nclearly \ndescribed \n2 The channel \nis roughly \ndescribed \n1 The channel is \nnot mentioned \n0 \nMission Mission/Quest\nion \nThe mission of \nthe LLM is \nclearly \ndescribed \n2 The mission \nof the LLM \nis roughly \ndescribed \n1 The mission of \nthe LLM is not \nclear \n0 \nClarity and \nspecificity \nFormat and \nconstraints \nStylistic \nproperties as \nwell as length \nspecifications \nare described \n2 Either \nstylistic \nproperties \nare \ndescribed or \na length \nspecificatio\nn is given \n1 Neither \nstylistic \nproperties nor \nlength \nspecifications \nare given \n0 \n Conciseness The prompt \ncontains only \ninformation that \nis directly \nrelated and \nrelevant to the \noutput. It is \nclear and \nconcise \n2 The prompt \nis concise \nwith little \nsuperfluous \ninformation \n1 The prompt \ncontains a lot \nof information \nthat is \nirrelevant to \nthe \nmission/questi\non \n0 \n10 \n \n Domain \nspecificity \nTechnical terms \nare used \ncorrectly and \ngive the LLM \nthe opportunity \nto refer to them \nin the answer \n2 Technical \nterms are \nused \nsporadically \nor without \nexplanation \n1 No specific \nvocabulary \nthat is relevant \nto the subject \narea of the \nquestion is \nused \n0 \n Logic The prompt has \na very good \nreading flow, \ninternal logical \ncoherence, a \nvery coherent \nsequence of \ninformation, \nand a clearly \nunderstandable \nconnection \nbetween the \ncontent and \nmission \n2 The prompt \nfulfills only \nsome of the \nconditions \nof the \ncoding “2” \n1 The prompt is \nillogically \nconstructed \n0 \nSubsequently, we developed three prompts of different quality (poor, medium, good) using our \nprompting manual. Following Wittwer et al. (2020), we formulated a learning goal with three \ntypes of errors (l earning goal: s tudents will recognize a right triangle and understand the \nPythagorean theorem [type of errors: no activity verb; instructional rather than learning goal; \nand multiple learning goals in a single statement]) and asked ChatGPT-4 to provide feedback \non the abovementioned learning goal. \n4.2 Assessment of Feedback Quality \nTo analyze the quality of LLM feedback and answer our first research question, we conducted \na quantitative feedback analysis. We adapted the coding scheme of Prilop et al. (2019) based \n11 \n \non the feedback quality index developed by Prins et al. (2006). Each feedback instance served \nas a unit of analysis and enabled a thorough content evaluation. The original scheme comprises \nsix categories: evaluation criteria, specificity, suggestions, questions, first-person perspective, \nand valence (positive/negative). The feedback is assigned a rating of “2” for high quality, “1” \nfor average, and “0” for suboptimal. A detailed explanation of this process can be found in \nPrilop et al. (2020). We added three categories: e rrors, explanations, and e xplanations of \nsuggestions. The error category was necessary due to the tendency of LLMs  to hallucinate \n(Alkaissi & McFarlane, 2023; Ji et al., 2022), with points deducted in this area. Hallucination \nin LLMs  refers to the generation of information or responses that appear plausible but are \nfactually incorrect or not based on the given input or data. The category explanation was based \non the manual by Wu and Schunn (2021). Finally, suggestions were divided into two \ncategories: presence of suggestion and explanation of suggestion to improve coding accuracy \n(see Table 2 for the coding manual and inter-coder reliability). \nTable 2: Content analysis of feedback quality: Categories, examples, and inter-coder reliability \n(Fleiss kappa). \nCategory Good \nfeedback \ndefinition \nCod\ne \nAverage \nfeedback \ndefinition \nCod\ne \nSub-optimal \nfeedback \ndefinition \nCod\ne \nκ Good feedback \nexample \nAssessment \ncriteria \nAspects of a \ngood learning \ngoal are \naddressed \nusing \ntechnical \nterms/theoreti\ncal models \n  \n2 Aspects of a \ngood learning \ngoal are \naddressed \nwithout \ntechnical \nterms/theoreti\ncal models \n1 Aspects of a \ngood learning \ngoal are not \naddressed \n0 .8\n1 \n“However, the \nlearning goal, \nas currently \nstated, has \nroom for \nimprovement. \nThe verb \n‘recognize’ is \non the lower \nend of Bloom’s \ntaxonomy; it’s \n12 \n \nmore about \nrecall than \napplication or \nanalysis.” \n(LLM feedback \n3.30) \nSpecificity All three \nerror types \nare named \nand explicitly \nexplained \n2 Two types of \nerrors are \nnamed and \nexplicitly \nexplained \n1 One type of \nerror is \nnamed and \nexplicitly \nexplained \n0 .8\n1 \n“Your goal \ncontains two \nseparate \nobjectives: […] \nNext, the verbs \nyou’ve chosen, \n‘recognize’ and \n‘understand,’ \nare a bit vague \nin the context \nof Bloom’s \ntaxonomy […] \nAnd how do \nyou envision \nthis learning \ngoal relating \nback to the \nlearner? \n[…]”(LLM \nfeedback 3.28) \nExplanation A detailed \nexplanation is \ngiven \nregarding \nwhy the \naspects of a \ngood learning \ngoal are \nrelevant \n2 A brief \nexplanation is \ngiven of why \nthe aspects of \na good \nlearning goal \nare relevant \n1 No \nexplanation is \ngiven \nregarding \nwhy the \naspects of a \ngood learning \ngoal are \nrelevant \n0 .8\n6 \n \n \n“According to \nbest practices, \nit’s beneficial \nto focus on just \none learning \ngoal at a time. \nThis makes it \nclearer for both \nyou and the \nstudents, \nstreamlining \nthe assessment \n13 \n \nprocess.” (LLM \nfeedback 3.14) \nPresence of \nsuggestions \nfor \nimprovement \nAlternatives \nare suggested \nin a \ncognitively \nstimulating \nway \n2 Alternatives \nare presented \nin concrete \nterms \n1 No \nalternatives \nare named \n0 .8\n6 \n“A more \ntargeted \nlearning goal \nwill focus on \njust one of \nthese. Which \none is your \npriority?” \n(LLM feedback \n3.28) \nExplanation \nof \nsuggestions  \nAlternatives \nare explained \nin detail \n2 Alternatives \nare briefly \nexplained \n1 Alternatives \nare not \nexplained \n0 .8\n2 \n“This would \nalign the goal \nmore closely \nwith achieving \ndeeper \nunderstanding \nand skill \nutilization. \n[…]This goal is \nlearner-\ncentered, \ncontains only \none focus, and \ninvolves \nhigher-level \nthinking skills. \nIt also makes \nthe intended \nlearning \noutcome clear.” \n(LLM feedback \n3.30) \nErrors The feedback \nincludes \nseveral \n–2 The feedback \nincludes one \nerror \n–1 Feedback \ndoes not \ninclude errors \n0 .9\n0 \n \n14 \n \ncontent errors \nregarding \nlearning \ngoals \nregarding \nlearning \ngoals \nregarding \nlearning \ngoals \nQuestions Activating \nquestion \nposed \n  \n2 Clarifying \nquestion \nposed \n1 No questions \nposed \n0 1.\n00 \n“So, what \nspecific skill or \nunderstanding \nare you hoping \nyour students \nwill gain by the \nend of this \nlesson?” (LLM \nfeedback 3.28) \nFirst person Written in \nfirst person \nthroughout \nfeedback \n  \n2 Occasionally \nwritten in \nfirst person \n1 Not written in \nfirst person \n0 .9\n0 \n“I appreciate \nthe effort \nyou’ve put into \nformulating this \nlearning goal \nfor your future \nteachers. \n[…]Let me \nshare my \nthoughts with \nyou. Firstly, I \nnoticed […]” \n(LLM feedback \n3.23) \nValence Balance \nbetween \npositive and \nnegative \nfeedback \n2 Mainly \npositive \nfeedback \n1 Mainly \nnegative \nfeedback \n0 .7\n6 \n“I don’t think \nthis learning \ngoal is well \nworded. \n[...]However, I \nlike that your \nlearning goal is \nformulated in a \nvery clear and \nstructured \n15 \n \nway.” (Novice \nfeedback 13) \n \n4.3 Coding of the Feedback \nThe LLM feedback (20 pieces of feedback from low-quality prompts, 20 from medium-quality \nprompts, and 20 from high-quality prompts) was coded by three trained student coders. These \ncoders were trained by a member of the research team and initially coded a sample of 20 \nfeedback comments. Any discrepancies were discussed and resolved following the method \ndescribed by Zottmann et al. (2013). The feedback was then randomly assigned for coding. \nFleiss’ kappa (κ) was used to measure agreement between coders, resulting in significant kappa \nvalues (see Table 2), indicating reliable coding. Based on the analysis, it became clear which \nprompt provided better results. Subsequently, the high-quality prompt was presented to 30 pre-\nservice teachers (novices), seven teacher trainers, two education al science professors, one \nteacher trainer, and one headmaster  (experts), who also formulated feedback. This feedback \nwas coded by the same coders. \n4.4 Analysis Method \nWe used our prompt manual to analyze the prompt quality of our three different prompts. We \nthen analyzed differences between LLM feedback (n = 30) , expert feedback (n = 11) , and \nnovice feedback (n = 30)  (independent variables) concerning the different subdimensions of \nfeedback quality (dependent variables) using one-way analyses of variance (ANOVAs), \nfollowed by Bonferroni post hoc tests. All statistical calculations were performed using SPSS \n26, and we set the significance level at p < .05 for all tests.  \n16 \n \n5. Results \n5.1 Differences between Prompts and their Output \nRegarding the first research question, we fed ChatGPT-4 with different types of prompts  (to \nsee the prompts, please view the supplementary material) and analyzed the outcome in terms \nof quality as well as the accuracy of the feedback. The first prompt achieved low quality (5 out \nof 16 points according to the prompt manual). The second prompt contained more details than \nthe first and therefore achieved slightly higher quality (8 out of 16 points). The third prompt \nhad the highest quality, scoring 15 out of a possible 16 points. We generated feedback 20 times \nfor each prompt and coded the results using our feedback quality manual. To  compare the \nfeedback, we conducted an ANOVA with Bonferroni post hoc  tests. Our results showed \nsignificant differences between the prompts regarding feedback quality for all subdimensions \nexcept valence and presence of suggestions (for more details about descriptive data, see Table \n3). Bonferroni-adjusted post hoc tests revealed that the feedback generated with prompt 3 (most \nsophisticated prompt) performed significantly (p < .001) better in the subcategory assessment \ncriteria than prompt 1 (M\nDiff = 1.50, 95%  CI[1.10, 1.90]) and prompt 2 (M Diff = 0.90, 95%  \nCI[0.50, 1.30]). We found the same effect for the categories explanation (prompt 1: M Diff = \n0.75, 95% CI[0.41, 1.09], p < .001; prompt 2: M Diff = 0.40, 95% CI[0.06, 0.74], p < .05), first \nperson (prompt 1: M Diff = 1.05, 95%  CI[0.63, 1.47], p < .001; prompt 2: M Diff = 0.95, 95%  \nCI[0.53, 1.37], p < .001), and questions (prompt 1: MDiff = 0.70, 95% CI[0.28, 1.12], p < .001; \nprompt 2: M Diff = 1.00, 95% CI[0.58, 1.42], p < .001). Furthermore, the feedback generated \nwith prompt 3 was significantly (p < .001) better than that generated with prompt 1 for the \ncategories explanation of suggestion (MDiff = 0.60, 95% CI[0.23, 0.97]) and specificity (MDiff \n= 1.25, 95% CI[0.90, 1.60]). For the category error, prompt 2 generated significantly (p < .001) \n17 \n \nmore errors than prompt 1 (MDiff = –0.85, 95% CI[–1.34, –0.36]) and prompt 2 (MDiff = –0.95, \n95% CI[–1.44, –0.46]).  \nTable 3: Quality of the feedback generated using the three different prompts.  \nCategory \nConcreteness   \nSubcategory \nAssessment criteria Explanation   \n  \nM SD Min. Max. M SD Min. Max.   \nPrompt 1 0.45 .76 0 2 0.25 .44 0 1   \nPrompt 2 1.05 .39 0 2 0.60 .50 0 1   \nPrompt 3 1.95 .22 1 2 1.00 .32 0 2   \n  Empathy   \n  First person Valence   \n  M SD Min. Max. M SD Min. Max.   \nPrompt 1 0.00 .00 0 0 0.85 .56 0 2   \nPrompt 2 0.10 .45 0 2 1.00 .00 1 1   \nPrompt 3 1.05 .83 0 2 1.00 .00 1 1   \n  Activation   \n  Questions Presence of suggestions for \nimprovement \nExplanation of suggestions \n  M SD Min. Max. M SD Min. Max. M SD Min. Max. \nPrompt 1 1.20 .52 0 2 1.15 .75 0 2 0.50 .51 0 1 \nPrompt 2 0.90 .72 0 2 1.15 .37 1 2 1.25 .55 0 2 \nPrompt 3 1.90 .31 1 2 1.50 .51 1 2 1.10 .31 1 2 \n  Correctness   \n  Specificity Errors   \n  M SD Min. Max. M SD Min. Max.   \nPrompt 1 0.10 .30 0 1 –0.40 .50n  –1 0   \nPrompt 2 1.05 .39 0 2 –1.25 .79 –2 0   \n18 \n \nPrompt 3 1.35 .59 0 2 –0.30 .57 –2 0   \n \nThese quantitative results are illustrated using examples of feedback generated using the low -\nquality prompt (prompt 1) and the high-quality prompt (prompt 3). The feedback on prompt 1 \nreceived zero points for feedback quality from our coders. While it vaguely listed some errors \nand mentioned aspects of a learning goal, the feedback lacked  clarity and would not help a \nnovice improve their learning goal. In contrast, the feedback on prompt 3 received 12 out of 16 \npoints. High-quality sections of this feedback are annotated and explained (comments in italics \nand gray). The following example illustrates the difference in feedback quality when an LLM \nis provided with prompts of varying quality, although it is not intended as an in -depth \nqualitative analysis. \n \nLLM feedback generated using Prompt 1: \nThe learning goal “ Students will recognize a right triangle and understand the Pythagorean \ntheorem” is clear but could benefit from further specificity. Recognizing right triangles and \nunderstanding the Pythagorean theorem are two distinct skills, and clarifying the level of \nunderstanding and application expected can enhance the goal. While the goal is relevant to \nmathematics education, it would be beneficial to outline how success will be measured. \nBreaking the goal into subgoals, such as identification, understanding, and application, could \nhelp in creating appropriate assessments. Finally, aligning this goal with students ’ grade level \nand setting a specific time frame might provide a more structured educational pathway. This \nfeedback aims to enhance the clarity and measurability of the goal, ensuring it is tailored to the \nstudents’ needs. [→ The LLM does not write in first person throughout the feedback; the LLM \nprovides no alternatives, poses no questions, and recognizes only one error] \n \n19 \n \nLLM feedback generated using Prompt 3: \nI appreciate the effort you’ve put into formulating this learning goal. It’s clear that you aim to \nteach a foundational concept in geometry that has a wide range of applications. Let me share \nmy thoughts with you. [→ The LLM writes in first person throughout the feedback] \nFirstly, I noticed that the goal contains two distinct objectives: recognizing right triangles and \nunderstanding the Pythagorean theorem. [→ The LLM  recognizes the first error: multiple \nlearning goals in a single statement] This could lead to a scattered focus. [→ The LLM explains \nthe relevance of learning goals] How might you refine it to contain just one learning goal? [→ \nThe LLM suggests alternatives in a cognitively stimulating way and poses a question] \nIn terms of action verbs, “ recognize” and “understand” are quite broad and don’ t lend \nthemselves to easy assessment. [→ The LLM recognizes the second error: No activity verb] \nBloom’s taxonomy might help you choose more specific verbs that clarify what you want \nstudents to do. [→ The LLM uses theoretical models to address good learning goals] Perhaps \n“identify” for recognizing right triangles and “apply” for using the Pythagorean theorem would \nbe more measurable and actionable? [→ The LLM presents alternatives]  \nThe goal is aimed at the learner, which is a positive, but I wonder if it’s concrete enough. How \nwill you measure “understanding” in this context? Could you link this more closely to a specific \nlearning outcome, like solving a problem using the Pythagorean theorem? [→ The LLM  \nrecognizes the third error: instructional rather than learning goal; and poses a question]  \nLastly, the connection between content and goal is somewhat present but could be more \nexplicit. Could the goal focus more on what students will actually do with their knowledge \nabout right triangles and the Pythagorean theorem in practice? [→ The LLM poses a thought-\nprovoking question]  \n20 \n \nSo, what do you think about these points? How would you modify the learning goal to make it \nmore specific, actionable, and closely related to measurable outcomes? [→ The LLM poses \nthought provoking questions] \n5.2 Differences between Novice, LLM, and Expert Feedback \nTo compare LLM feedback with novice and expert feedback, we provided the highest -quality \nprompt (prompt 3) to pre-service teachers and experts (see Section 4.3 for expert details). An \nANOVA with Bonferroni post hoc  tests revealed significant differences among the groups in \nfeedback quality across all subdimensions except empathy, valence, and first person (see Table \n4 for descriptive data). The Bonferroni -adjusted post hoc tests confirmed previous findings \n(Weber et al., 2019, Prilop et al., 2021), indicating that expert feedback was more concrete, \nactivating, and correct but not more empathetic than that of novices. Expert feedback showed \nsignificantly higher quality (p < .001) in the subcategories assessment criteria , explanation, \nquestions, presence of suggestions, explanation of suggestions, and specificity. The comparison \nbetween novice and LLM feedback showed that LLM feedback outperformed novice feedback \nin all subcategories except valence and first person. Regarding the difference between LLM  \nand expert feedback, the Bonferroni  adjusted post hoc tests revealed that the LLM f eedback \nhad higher quality than expert feedback in the subcategories explanation (MDiff = 0.46, 95% CI \n[0.17, 0.74], p < .001) , questions (MDiff = 0.50, 95% CI [0.07, 0.93], p < .05) , and specificity \n(MDiff = 0.96, 95% CI [0.52, 1.41]).  \n \nTable 4: Quality of the novice, expert, and LLM feedback. \n  \nConcreteness   \n  \nAssessment criteria Explanation   \n21 \n \n  \nM SD Min. Max. M SD Min. Max.   \nPeers 0.63 .81 0 2 0.10 .31 0 1   \nExperts 1.64  .51  1  2  0.55  .52  0  1    \nChatGPT-4 1.97 .18 1 2 1.00 .26 0 2   \n  Empathy   \n  First person Valence   \n  M SD Min. Max. M SD Min. Max.   \nPeers 1.10 .71 0 2 1.10  .30  1  2    \nExperts 1.18  .60  0  2  1.25  .50  1  2    \nChatGPT-4 1.10 .76 0 2 1.00  .39  0  2    \n  Activation   \n  Questions Presence of suggestions for \nimprovement \nExplanation of suggestions \n  M SD Min. Max. M SD Min. Max. M SD Min. Max. \nPeers 0.17  .38  0 1  0.87  .82  0  2  0.30  .54  0  2  \nExperts 1.36  .81  0  2  1.73  .47  1  2  0.82  .60  0  2  \nChatGPT-4 1.86  .44  0  2  1.57  .50  1  2  1.13  .35  1  2  \n  Correctness   \n  Specificity Errors   \n  M SD Min. Max. M SD Min. Max.   \nPeers 0.17  .38 0 1   –.73  .87 –2  0   \nExperts  0.64  .67  0  2   –.18 .60   –2 0   \nChatGPT-4 1.60  .56 0  2  –.17  .46 –2  0    \n6. Discussion \nThe findings of this study offer compelling insights into the utility and effectiveness of LLM -\nbased feedback in HE. Currently, novice feedback, in the form of peer feedback, is often used \nin HE, but it is not always conducive to learning (Kluger & DeNisi, 1996). Moreover, it is \nchallenging for experts to provide high- quality feedback in HE due to a lack of human and \nfinancial resources (Demszky et al., 2023). LLM feedback can provide an enriching and \neconomical alternative. A particularly promising result of our study is that feedback generated \nby the LLM surpassed novice feedback in quality and even rivaled that of experts. Accordingly, \n22 \n \nour results align with those of Dai et al. (2024) while underlining the importance of prompting \nwhen using LLMs. \nOur first research question addressed what kinds of prompts are needed to generate high-quality \nLLM f eedback. One key finding of our study was the importance of  prompt quality in \ndetermining the quality of  LLM-based feedback. While LLMs  can generate high -quality \nfeedback, the output is dependent on the context, mission, specificity, and clarity of the prompts \nprovided. The study revealed that only the prompt with the highest quality could induce the \nLLM to generate consistent high- quality feedback. When considering the category error,  \nprompt 2 was revealed to be a wolf in sheep’ s clothing, having good stylistic properties but \nresulting in significantly more errors than prompt 1 and more errors than any other prompt or \nfeedback provider in this study. This illustrates the potential of LLMs  to hallucinate (Alkaissi \n& McFarlane, 2023; Ji et al., 2022) and underscores the importance of careful, theory- driven \nprompt design. The ability to craft high-quality prompts is a skill that educators need to master \n(e.g. Zamfrescu-Pereira et al., 2023; UNESCO, 2024), necessitating a manual or guidelines. In \nour study, we designed a prompt manual which could and should be used by educators who \nwork with LLMs. \nWith regard to research question 2, our study supports previous findings ( Prilop et al., 2021; \nWeber et al., 2019) showing that expert feedback is of higher quality than novice feedback. We \nfound that experts outperformed pre-service teachers in the categories concreteness, activation, \nand correctness but not in the category empathy. The same was true when we compared LLM \nand novice feedback. By comparing LLM  feedback with expert feedback , we complement \nthese findings, providing new insights regarding feedback processes in HE. Our results show \nthat LLM feedback can outperform expert feedback in the categories explanation, questions , \nand specificity. This attests to the transformative potential of LLMs  in educational settings, \n23 \n \noffering the promise of scalable, high- quality feedback that could revolutionize the way \neducators assess student work. Furthermore, the LLM -based feedback was produced in \nsignificantly less time than expert feedback (in our study, ChatGPT-4 produced an average of \n49 pieces of feedback in the same amount of time that an expert produced one), heralding \nefficiency gains that could free up educators for more personalized or creative pedagogical \nendeavors. However, considering our proposed heuris tic model, futu re studies should \ninvestigate how LLM-based feedback is perceived by students and whether students’ learning \nexperiences and learning gains can be enhanced by LLM feedback.  \n \nOverall, our findings support the results of Dai et al. (2024) and lend credence to the promise \nof LLMs as a viable alternative to expert feedback in HE. However, we must also consider the \nscope and limitations of LLMs. While they can quickly analyze and generate feedback based \non set parameters, LLMs  lack the nuanced understanding of individual learner s’ psychology, \nneeds, and the socio- cultural context within which learning occurs . LLMs seem to perform \nparticularly well with task -related feedback (Dai et al., 2024), which corresponds to the \nfeedback-level observed in this study. Nevertheless, it is crucial to recognize that expertise is \nnot solely a function of accurate or quick feedback. Experts bring a depth of experience, \nprofessional judgment, and a personal touch to their interactions with students.  These qualities \nare currently beyond the reach of AI systems  and may prove irreplaceable in educational \nsettings that value not only the transfer of knowledge but also the building of relationships and \ncharacter. Even if efficiency and quality are the only benchmarks, there was one outlier with \nmultiple errors among the 20 feedback  comments generated by the highest -quality prompt. \nThus, we posit that experts are still needed but that their tasks should be shifted from providing \nfeedback to monitoring and revising LLM feedback.  \n24 \n \nFuture studies should investigate how the quality of expert feedback can be enhanced by using \nLLMs and how this intertwined approach is perceived by students and educators in HE. Going \nbeyond the promise of efficiency and quality, and considering Russel and Norvig’ s (2010) \nwarning that every researcher in the field of AI should be aware of the ethical ramifications of \ntheir projects, it becomes evident that the ethical and data-related dimensions of LLMs cannot \nbe ignored in HE. While LLMs are not subjectively biased, the data on which they are trained \ndoes have inherent biases. Moreover, there are potential concerns about data security, privacy, \nand intellectual property, particularly in a learning environment where sensitive information \nmay be discussed. As educators and policymakers consider  implementing LLMs in HE, these \nethical questions need careful attention and possibly regulatory oversight. In sum, we come to \nthe same conclusion as Zawacki -Richter et al. (2019): “ We should not strive for what is \ntechnically possible, but always ask ourselves what makes pedagogical sense” (p. 21). \n6.2 Limitations and Implications \nThis study takes an in-depth look at the efficacy of LLMs as a tool for generating feedback in \nHE. An important limitation of our study that warrants discussion is the restricted focus on a \nsingle learning goal and a limited set of errors for which feedback was generated. This narrow \nscope may limit the generalizability of our findings. While we found that the LLM outperforms \nboth novices and experts in providing high- quality feedback for the specific errors we \nexamined, it remains an open question whether these findings would hold true across a broader \nrange of academic subjects and tasks in HE. Educational settings are diverse, encompassing a \nwide array of subjects, each with their own unique types of content and forms of assessment. \nTherefore, it would be  risky to assume that the efficacy of an LLM  in our context would be \nuniversally applicable across all educational environments. Future research should aim to \ndiversify the types of tasks and the corresponding feedback. This would provide a more \n25 \n \ncomprehensive understanding of where LLM-based feedback can be most effectively and \nappropriately utilized in HE. Until such broader research is conducted, the application of our \nfindings should be considered preliminary and best suited for contexts similar to the one we \nstudied. For this reason, we are conducting a study in which we compare the feedback quality \nof three different LLMs giv ing feedback to 153 pre -service teachers regarding their learning \ngoals in their first teaching practicum. \nAnother practical implication  of this study is that the relevance of prompt engineering may \ncreate a barrier to entry for educators less familiar with the nuances of designing effective \nprompts, thus necessitating further training or guidance.\n  \n6.3 Conclusion \nIn conclusion, there is compelling evidence supporting the use of LLMs as tools for feedback \nin HE, including its quality and efficiency. However, its application is not without pitfalls. \nOverall, we find that LLMs have the potential to be useful tools, but educators must be skilled \nin prompt engineering and adept at utilizing the tool to achieve optimal results. As Azaria et al. \n(2023) emphasized in the title of their article, “ChatGPT is a Remarkable Tool - For Experts.” \nThe dependence on prompt quality, ethical challenges, and the irreplaceable nuanced inputs \nfrom human experts make it a tool to be used cautiously. Future research should explore these \ndimensions in more detail, possibly leading to a balanced hybrid appr oach that combines the \nstrengths of both  LLM and human expertise in educational feedback mechanisms. The \nendeavor to incorporate LLMs in HE is not a question of replacement but of augmentation. \nHow we navigate this balance will determine the efficacy of such technological solutions in \ntruly enriching the educational landscape. \n \n  \n26 \n \nReferences \nAlkaissi, H., & McFarlane, S. I. (2023). Artificial hallucinations in ChatGPT: Implications in \nScientific Writing. Cureus, 15(2), e35179. https://doi.org/10.7759/cureus.35179 \nAzaria, A., Azoulay, R., & Reches, S. (2023). ChatGPT is a remarkable tool–For experts. \nhttps://doi.org/10.48550/arXiv.2306.03102 \nBaidoo-Anu, D., & Ansah, L. O. (2023). Education in the era of generative artificial \nintelligence (AI): Understanding the potential benefits of ChatGPT in promoting \nteaching and learning. https://www.researchgate.net/publication/369385210 \nBernius, J. P., Krusche, S., & Bruegge, B. (2022). Machine learning based feedback on \ntextual student answers in large courses. Computers and Education: Artificial \nIntelligence, 3, 100081. https://doi.org/10.1016/j.caeai.2022.100081 \nBsharat, S. M., Myrzakhan, A., & Shen, Z. (2024). Principled instructions are all you need \nfor questioning LLaMA-1/2, GPT-3.5/4 [Preprint]. arXiv. \nhttps://arxiv.org/abs/2312.16171 \nChatGPT, & Ekin, S. (2023). Prompt engineering for ChatGPT: A quick guide to techniques, \ntips and best practice. https://doi.org/10.36227/techrxiv.22683919 \nCrompton, H., & Burke, D. (2023). Artificial intelligence in higher education: The state of \nthe field. International Journal of Educational Technology in Higher Education, \n20(1). https://doi.org/10.1186/s41239-023-00392-8\n \nCotton, D. R. E., Cotton, P. A., & Shipway J. R. (2023). Chatting and cheating: Ensuring \nacademic integrity in the era of ChatGPT. Innovations in Education and Teaching \nInternational. 61(2), 228–239. https://doi.org/10.1080/14703297.2023.2190148 \nDai, W., Tsai, Y. S., Lin, J., Aldino, A., Jin, H., Li, T., Gasevic, D., Chen, G. (2024). \nAssessing the proficiency of large language models in automatic feedback generation: \n27 \n \nAn evaluation study. Computers and Education: Artificial Intelligence, 7, 100299. \nhttps://doi.org/10.1016/j.caeai.2024.100299 \nDemszky, D., Liu, J., Hill, H. C., Jurafsky, D., & Piech, C. (2023). Can automated feedback \nimprove teachers’ uptake of student ideas? Evidence from a randomized controlled \ntrial in a large-scale online course. Educational Evaluation and Policy Analysis, \n46(3), 483-505. 016237372311692. https://doi.org/10.3102/01623737231169270 \nEricsson, K. A., Krampe, R. T., & Tesch-Römer, C. (1993). The role of deliberate practice in \nthe acquisition of expert performance. Psychological Review, 100(3), 363–406. \nFleckenstein, J., Liebenow, L. W., & Meyer, J. (2023). Automated feedback and writing: A \nmulti-level meta-analysis of effects on students’ performance. Frontiers in Artificial \nIntelligence, 6, 1162454. https://doi.org/10.3389/frai.2023.1162454 \nFleiss, J. L., & Cohen, J. (1973). The equivalence of weighted kappa and the intraclass \ncorrelation coefficient as measures of reliability. Educational and Psychological \nMeasurement, 33(3), 613–619. https://doi.org/10.1177/001316447303300309 \nGielen, M., & De Wever, B. (2015). Structuring peer assessment: Comparing the impact of \nthe degree of structure on peer feedback content. Computers in Human Behavior, 52, \n315–325. \nHarry, A., & Sayudin, S. (2023). Role of AI in e ducation. Interdisciplinary Journal and \nHummanity (INJURITY), 2(3), 260–268. \nHammerness, K. M., Darling-Hammond, L., Bransford, J., Berliner, D. C., Cochran-Smith, \nM., McDonald, M., & Zeichner, K. M. (2005). How teachers learn and develop. In L. \nDarling-Hammond, J. Bransford, P. LePage, K. Hammerness, & H. Duffy (Eds.), \nPreparing teachers for a changing world: What teachers should learn and be able to \ndo (pp. 358–389). Jossey-Bass. \n28 \n \nHattie, J., & Timperley, H. (2007). The power of feedback. Review of Educational Research , \n77(1), 81–112. \nHenderson, M., Ajjawi, R., Boud, D., & Molloy, E. (Eds.). (2019). The impact of feedback in \nhigher education: Improving assessment outcomes for learners. Springer International \nPublishing. https://doi.org/10.1007/978-3-030-25112-3 \nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, J., Dai, W., Madotto, A.,  & \nFung, P. (2022) . Survey of hallucination in n atural language generation. ACM \nComputing Surveys., 55(12), 1–38. https://doi.org/10.48550/arXiv.2202.03629 \nKasneci, E., Sessler, K., Küchemann, S., Bannert, M.,  Dementieva, D.,  Fischer, F., Gasser, \nU., Groh, G., Günnemann, S., Hüllermeier, E., Krusche, S., Kutyniok, G., Michaeli, T., \nNerdel, C., Pfeffer, J., Poquet, O., Sailer, M., Schmidt, A., Seidel , T., ... Kasneci, G. \n(2023). ChatGPT for good? On opportunities and challenges of large language models \nfor education. Learning and Individual Differences 103, \nhttps://doi.org/10.1016/j.lindif.2023.102274\n \nKluger, A. N., & DeNisi, A. (1996). The effects of feedback interventions on performance: A \nhistorical review, a meta -analysis and a preliminary feedback intervention theory. \nPsychological Bulletin, 119(2), 254–284. \nKraft, M. A., Blazar, D., & Hogan, D. (2018). The effect of teacher coaching on instruction \nand achievement: A meta- analysis of the causal evidence. Review of Educational \nResearch, 88(4), 547–588. \nKasneci, E., Sessler, K., Küchemann, S., Bannert, M., Dementieva, D.,  Fischer, F., Gasser, U., \nGroh, G., Günnemann, S., Hüllermeier, E.,  Krusche, S.,  Kutyniok, G.,  Michaeli, T., \nNerdel, C., Pfeffer, J., Poquet, O., Sailer, M., Schmidt, A., Seidel, T., ... & Kasneci, G. \n(2023). ChatGPT for good? On opportunities and challenges of large language models \n29 \n \nfor education. Learning and Individual Differences , 103, \nhttps://doi.org/10.1016/j.lindif.2023.102274 \nKipp, Michael (2023). Wie sag ich’s meiner KI? Hintergründe und Prinzipien zum #Prompting \nbei #ChatGPT [video]. https://www.youtube.com/watch?v=cfl7q1llkso&t=2382s  \nKrause, G. (2019). Training zur Förderung von Kompetenzen für die Arbeit mit \nVideofeedback. In  G. Uhde & B.  Thies (Eds). Kompetenzentwicklung im \nLehramtsstudium durch professionelles Training  (pp. 83–108). \nhttps://doi.org/10.24355/dbbs.084-201901231126-0  \nLo, L. S. (2023). The CLEAR path: A framework for enhancing information literacy through \nprompt engineering, The Journal of Academic Librarianship, 49(4). \nhttps://doi.org/10.1016/j.acalib.2023.102720. \nLu, H.-L. (2010). Research on peer -coaching in preservice teacher education – A review of \nliterature. Teaching and Teacher Education, 26(4), 748–753. \nNarciss, S. (2008). Feedback strategies for interactive learning tasks. In J. M. Spector, M. D. \nMerrill, J. J. G. van Merrienboer, & M. P. Driscoll (Eds.), Handbook of research on \neducational communications and technology  (3rd ed., pp. 125–144). Lawrence \nErlbaum Associates. \nNarciss, S. (2013). Designing and evaluating tutoring feedback strategies for digital learning \nenvironments on the basis of the interactive feedback model. Digital Education Review, \n23. \nPekrun, R., Marsh, H. W., Elliot, A. J., Stockinger, K., Perry, R. P., Vogl, E., Goetz, T., van \nTilburg, W. A. P., Lüdtke, O., & Vispoel, W. P. (2023). A three-dimensional taxonomy \nof achievement emotions. Journal of Personality and Social Psychology, 124(1), 145–\n178. https://doi.org/10.1037/pspp0000448 \n30 \n \nPrilop, C. N., Weber, K., & Kleinknecht, M. (2019a). Entwicklung eines video - und \ntextbasierten Instruments zur Messung kollegialer Feedbackkompetenz von \nLehrkräften [Development of a video-and text-based instrument for the assessment of \nteachers' peer feedback competence]. In T. Ehmke, P.Kuhl, & M. Pietsch (Eds.), Lehrer. \nBildung. Gestalten: Beiträge zur empirischen Forschung in der Lehrerbildung (pp. 153-\n163). Weinheim Basel: Beltz Juventa Verlag.  \nPrilop, C. N., Weber, K. E. & Kleinknecht, M. (2019). How digital reflection and feedback \nenvironments contribute to pre -service teachers’ beliefs during a teaching practicum. \nStudies in Educational Evaluation, 62, 158– 170. \nhttps://doi.org/10.1016/j.stueduc.2019.06.005\n \nPrilop, C. N., Weber, K. E., & Kleinknecht, M. (2020). Effects of digital video-based feedback \nenvironments on pre -service teachers’ feedback competence. Computers in Human \nBehavior, 102, 120–131. https://doi.org/10.1016/j.chb.2019.08.011 \nPrilop, C. N., Weber, K. E. & Kleinknecht, M. (2021). The role of expert feedback in the \ndevelopment of pre-service teachers’ professional vision of classroom management in \nan online blended learning environment. Teaching and Teacher Education, 99(4). \nhttps://doi.org/10.1016/j.tate.2020.103276\n \nPrilop, C. N., & Weber, K. E. (2023). Digital video- based peer feedback training: The effect \nof expert feedback on pre -service teachers’ peer feedback beliefs and peer feedback \nquality. Teaching and Teacher Education, 127, 104099. \nhttps://doi.org/10.1016/j.tate.2023.104099 \nPrins, F., Sluijsmans, D., & Kirschner, P. A. (2006). Feedback for general practitioners in \ntraining: Quality, styles and preferences. Advances in Health Sciences Education, 11, \n289–303. \n31 \n \nRussel, S., & Norvig, P. (2010). Artificial intelligence – A modern approach. Pearson \nEducation. \nSailer, M., Bauer, E., Hofmann, R., Kiesewetter, J., Glas, Julia., Gurevych, I., & Fischer, F. \n(2023). Adaptive feedback from artificial neural networks facilitates pre-service \nteachers’ diagnostic reasoning in simulation-based learning. Learning and Instruction, \n83. https://doi.org/10.1016/j.learninstruc.2022.101620 \nStrijbos, J.W., Narciss, S., & Dünnebier, K. (2010). Peer feedback content and sender ’s \ncompetence level in academic writing revision tasks: Are they critical for feedback \nperceptions and efficiency? Learning and Instruction, 20(4), 291–303. \nTsai, Y.-S., Rates, D., Moreno- Marcos, P. M., Muñoz -Merino, P. J., Jivet, I., Scheffel, M., \nDrachsler, D., Kloos, C. D., Gašević, D. (2020). Learning analytics in European higher \neducation—Trends and barriers. Computers & Education, 155. \nhttps://doi.org/10.1016/j.compedu.2020.103933 \nUnited Nations Educational, Scientific and Cultural Organization ( UNESCO). (2024). AI \ncompetency framework for teachers. https://www.unesco.org/en/open-access/cc-sa \nWang, S., Wang, F., Zhu, Z., Wang, J., Tran, T., & Du, Z. (2024). Artificial intelligence in \neducation: A systematic literature review. Expert Systems with Applications , 252(Part \nA), 124167. https://doi.org/10.1016/j.eswa.2024.124167 \nWeber, K. E., Gold, B., Prilop, C. N. & Kleinknecht, M. (2018). Promoting pre -service \nteachers’ professional vision of classroom management during practical school \ntraining: Effects of a structured online - and video-based self-reflection and feedback \nintervention. Teaching and Teacher Education, 76, 39- 49. \nhttps://doi.org/10.1016/j.tate.2018.08.008\n \nWeber, K. E., Prilop, C. N. & Kleinknecht, M. (2019). Effects of blended and video -based \ncoaching approaches on preservice teachers' self -efficacy and perceived competence \n32 \n \nsupport. Learning, Culture and Social Interaction, 22, 103- 118. \nhttps://doi.org/10.1016/j.lcsi.2019.100318 \nWittwer, J., Kratschmayr, L., & Voss, T. (2020). Wie gut erkennen Lehrkräfte typische Fehler \nin der Formulierung von Lernzielen? Unterrichtswissenschaft , 48(1), 113–128. \nhttps://doi.org/10.1007/s42010-019-00056-5 \nWu, Y., & Schunn, C. D. (2021). From plans to actions: A process model for why feedback \nfeatures influence feedback implementation. Instructional Science, 49(3), 365–394. \nZamfrescu-Pereira, J. D., Wong, R., Hartmann, B., & Yang, Q. (2023). Why Johnny can’t \nprompt: How non-AI experts try (and fail) to design LLM prompts. In Proceedings of \nthe 2023 CHI Conference on Human Factors in Computing Systems (CHI ‘23), April \n23–28, 2023, Hamburg, Germany (pp. 1–21). ACM. \nhttps://doi.org/10.1145/3544548.3581388 \nZawacki-Richter, O., Marín, V. I., Bond, M., & Gouveneur, F. (2019) . Systematic review of \nresearch on artificial intelligence applications in higher education – Where are the \neducators? International Journal of  Educational Technology in Higher Education, \n16(1). https://doi.org/10.1186/s41239-019-0171-0 \nZhu, M., Liu, O, L., & Lee, H. -S. (2020). The effect of automated feedback on revision \nbehavior and learning gains in formative assessment of scientific argument writing. \nComputers & Education, 143. https://doi.org/10.1016/j.compedu.2019.103668\n \nZottmann, J. M., Stegmann, K., Strijbos, J.- W., Vogel, F., Wecker, C., & Fischer, F. (2013). \nComputer-supported collaborative learning with digital video cases in teacher \neducation: The impact of teaching experience on knowledge convergence. Computers \nin Human Behavior, 5, 2100–2108. ",
  "topic": "Quality (philosophy)",
  "concepts": [
    {
      "name": "Quality (philosophy)",
      "score": 0.78700852394104
    },
    {
      "name": "Computer science",
      "score": 0.5767689347267151
    },
    {
      "name": "Empirical research",
      "score": 0.43067318201065063
    },
    {
      "name": "Peer feedback",
      "score": 0.41020381450653076
    },
    {
      "name": "Psychology",
      "score": 0.31765609979629517
    },
    {
      "name": "Mathematics education",
      "score": 0.17040157318115234
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}