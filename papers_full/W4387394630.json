{
  "title": "Herd’s Eye View: Improving Game AI Agent Learning with Collaborative Perception",
  "url": "https://openalex.org/W4387394630",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2096121218",
      "name": "Andrew Nash",
      "affiliations": [
        "Memorial University of Newfoundland",
        "Memorial"
      ]
    },
    {
      "id": "https://openalex.org/A2148816035",
      "name": "Andrew Vardy",
      "affiliations": [
        "Memorial",
        "Memorial University of Newfoundland"
      ]
    },
    {
      "id": "https://openalex.org/A2639092209",
      "name": "Dave Churchill",
      "affiliations": [
        "Memorial",
        "Memorial University of Newfoundland"
      ]
    },
    {
      "id": "https://openalex.org/A2096121218",
      "name": "Andrew Nash",
      "affiliations": [
        "Memorial University of Newfoundland"
      ]
    },
    {
      "id": "https://openalex.org/A2148816035",
      "name": "Andrew Vardy",
      "affiliations": [
        "Memorial University of Newfoundland"
      ]
    },
    {
      "id": "https://openalex.org/A2639092209",
      "name": "Dave Churchill",
      "affiliations": [
        "Memorial University of Newfoundland"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6745935785",
    "https://openalex.org/W4317438814",
    "https://openalex.org/W4297411803",
    "https://openalex.org/W6847464493",
    "https://openalex.org/W4286337300",
    "https://openalex.org/W3209119436",
    "https://openalex.org/W6742348326",
    "https://openalex.org/W6746052068",
    "https://openalex.org/W4320032424",
    "https://openalex.org/W3048770793",
    "https://openalex.org/W4289645237",
    "https://openalex.org/W3022701300",
    "https://openalex.org/W2983089965",
    "https://openalex.org/W4296568148",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W2906786529",
    "https://openalex.org/W6784476834",
    "https://openalex.org/W6843100744",
    "https://openalex.org/W4313595897",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6754616581",
    "https://openalex.org/W4284701604",
    "https://openalex.org/W4221167196",
    "https://openalex.org/W3201193904",
    "https://openalex.org/W4224912471",
    "https://openalex.org/W4228999178",
    "https://openalex.org/W4206138072",
    "https://openalex.org/W4295719664",
    "https://openalex.org/W4312641958",
    "https://openalex.org/W4383108631",
    "https://openalex.org/W4286544732",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W3117234758",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W4312894406",
    "https://openalex.org/W3212352499",
    "https://openalex.org/W4312939270",
    "https://openalex.org/W4312571135",
    "https://openalex.org/W4386634496",
    "https://openalex.org/W3109395584",
    "https://openalex.org/W4323647389",
    "https://openalex.org/W4290056039",
    "https://openalex.org/W2889694290",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W4288601764",
    "https://openalex.org/W4383108597",
    "https://openalex.org/W4320085062",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4319300075",
    "https://openalex.org/W4293868201",
    "https://openalex.org/W3093693419",
    "https://openalex.org/W2963587345",
    "https://openalex.org/W4312604822",
    "https://openalex.org/W4319299982"
  ],
  "abstract": "We present a novel perception model named Herd's Eye View (HEV) that adopts a global perspective derived from multiple agents to boost the decision-making capabilities of reinforcement learning (RL) agents in multi-agent environments, specifically in the context of game AI. The HEV approach utilizes cooperative perception to empower RL agents with a global reasoning ability, enhancing their decision-making. We demonstrate the effectiveness of the HEV within simulated game environments and highlight its superior performance compared to traditional ego-centric perception models. This work contributes to cooperative perception and multi-agent reinforcement learning by offering a more realistic and efficient perspective for global coordination and decision-making within game environments. Moreover, our approach promotes broader AI applications beyond gaming by addressing constraints faced by AI in other fields such as robotics. The code is available at https://github.com/andrewnash/Herds-Eye-View",
  "full_text": "Herd’s Eye View: Improving Game AI Agent Learning with Collaborative\nPerception\nAndrew Nash, Andrew Vardy, David Churchill\nDepartment of Computer Science, Memorial University, St. John’s, Canada\nanash@mun.ca, av@mun.ca, dave.churchill@gmail.com\nAbstract\nWe present a novel perception model named Herd’s Eye View\n(HEV) that adopts a global perspective derived from multi-\nple agents to boost the decision-making capabilities of rein-\nforcement learning (RL) agents in multi-agent environments,\nspecifically in the context of game AI. The HEV approach\nutilizes cooperative perception to empower RL agents with\na global reasoning ability, enhancing their decision-making.\nWe demonstrate the effectiveness of the HEV within simu-\nlated game environments and highlight its superior perfor-\nmance compared to traditional ego-centric perception mod-\nels. This work contributes to cooperative perception and\nmulti-agent reinforcement learning by offering a more re-\nalistic and efficient perspective for global coordination and\ndecision-making within game environments. Moreover, our\napproach promotes broader AI applications beyond gam-\ning by addressing constraints faced by AI in other fields\nsuch as robotics. The code is available at https://github.com/\nandrewnash/Herds-Eye-View\nIntroduction\nGame environments traditionally grant AI agents access to\nextensive global information from the game engine. While\nthis configuration assists in efficient decision-making, it\ndoes not accurately represent the restrictions encountered\nby AI applications outside of gaming, where comprehensive\naccess to a system’s software or engine is not feasible. Con-\nsequently, game AI techniques that rely predominantly on\ngame engine data may limit their potential contribution to\nbroader AI applications, as their dependency on perfect in-\nformation and global environmental data is often unrealistic\nin other contexts such as robotics and autonomous vehicles.\nIn response to these challenges, our work delves into the\napplication of more constrained, realistic perception mod-\nels for game AI. We take inspiration from publications like\nthe ViZDoom platform (Wydmuch, Kempka, and Ja´skowski\n2019) and the Obstacle Tower Challenge (Juliani et al. 2019)\nthat have embraced the shift towards game AI with real-\nworld constraints. ViZDoom and Obstacle Tower have uti-\nlized visual data as the primary input for AI agents, en-\nabling them to navigate complex 3D environments. These\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nperception-based game AI agents reinforce the importance\nof models without access to game engine information.\nResearch in autonomous vehicles has made extensive\nstrides in AI perception models, particularly using inter-\nmediary environmental representations like the Bird’s Eye\nView (BEV). The BEV model provides an overhead per-\nspective of the environment, often in the form of a semantic\nobstacle grid, from a single “ego” vehicle’s standpoint. This\nconcept has become a key component in many self-driving\nsystems (Ma et al. 2023).\nDrawing on these past works, we propose a similar inter-\nmediary representation for game AI: the Herd’s Eye View\n(HEV) model. Differing from the BEV’s ego-centric per-\nspective, the HEV model offers a shared world-centric per-\nception derived from multiple agents. This shared percep-\ntion model aligns closer to real-world AI applications, where\nmultiple systems often work together to understand and nav-\nigate their environment.\nThe HEV model presents dual advantages. First, it mirrors\nthe constraints faced by AI outside of gaming, contribut-\ning to the development of more believable AI behavior in\ngames. Second, it alleviates the computational demands as-\nsociated with the BEV model, where each agent maintains\nits own unique view of the environment, instead, only a sin-\ngle shared global view is utilized.\nEmulating the successful methodologies of the ViZDoom\nproject and the Obstacle Tower paper, we also incorporate\nReinforcement Learning (RL) into our approach. RL enables\nus to test the effectiveness of HEV in both low-level con-\ntrol tasks and high-level planning challenges concurrently in\ncomplex environments. Importantly, similar to the Obstacle\nTower approach, our agents are assessed not solely on their\nability to navigate familiar environments, but also on their\nability to handle unique variations of these environments.\nThis highlights the importance of generalization in adapting\nto novel scenarios within the same environment.\nTo assess the effectiveness of the HEV model, we con-\nduct two sets of experiments in three simulated Multi-Agent\nReinforcement Learning (MARL) game environments. The\nfirst compares the accuracy of HEV world-centric predic-\ntions with BEV ego-centric predictions. The second exper-\niment evaluates the efficiency of policies learned by RL\nagents trained on HEV perspective views compared to those\ntrained on BEV perspective views.\nProceedings of the Nineteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE2023)\n306\nOur work makes the following contributions:\n1. We propose a baseline model for performing semantic\nsegmentation in a fixed “HEV” world-centric view.\n2. We demonstrate the effectiveness of the HEV fixed\nworld viewpoint in improving collaborative perception\nand MARL in games.\nOur exploration of more realistic perception models pro-\nvides significant insights for game AI development, stress-\ning the wider applicability of these techniques beyond the\ngaming industry.\nRelated Works\nBirds Eye View Semantic Segmentation\nIn autonomous vehicle research, the bird’s-eye view seman-\ntic segmentation task involves predicting pixel-level seman-\ntic labels for a top-down ego-centric view of an environ-\nment. Segmentation classes are typically dedicated to vehi-\ncles, driveable areas, and obstacles. In prior BEV research,\na significant point of distinction lies in the method used for\ntransforming 2D perspective-view features into 3D space or\ndirectly onto the BEV plane. Many previous works have\nleveraged explicit geometric reasoning in their perspective\ntransformation (Reiher, Lampe, and Eckstein 2020; Phil-\nion and Fidler 2020; Hu et al. 2021). An approach that has\nrecently gained popularity is the Cross-View Transformer\n(CVT) (Zhou and Kr ¨ahenb¨uhl 2022) model, which implic-\nitly models scene geometry. The CVT model leverages a\ncamera-aware cross-view attention mechanism to implicitly\nlearn a mapping from individual camera views to a canon-\nical map-view representation for map-view semantic seg-\nmentation. The model consists of a convolutional image en-\ncoder for each view and cross-view transformer layers for\ninferring the segmentation in a simple, easily parallelizable,\nand real-time manner. BEVFormer (Li et al. 2022c) uses a\nsimilar cross-attention model to extract spatiotemporal BEV\ninformation. BEVSegFormer (Peng et al. 2022) uses a de-\nformable transformer-based encoder. There are many pub-\nlications in this research area using similar architectures of\ntransformers to shift perspective view(s) to BEV , Ma et al.\nprovides a recent review of these architectures.\nThe HEV semantic segmentation task poses a unique\nchallenge compared to the BEV task since the agent trans-\nlations are unknown; this requires the model to geometri-\ncally reason about multiple camera views to localize. For\nour baseline approach, we leverage the CVT model pro-\nposed by (Zhou and Kr ¨ahenb¨uhl 2022). The CVT model is\nwell suited for the HEV task because of its global attention\nmechanism. Many BEV publications such as BEVFormer\n(Li et al. 2022c) and BEVSegFormer (Peng et al. 2022) aim\nto optimize this global attention mechanism since in ego-\ncentric tasks, a camera view only overlaps with a consistent\nsubsection of the map-view. Conversely, in our HEV world-\ncentric use case, global attention is an advantage because\na camera view can overlap with any part of the map-view.\nAdditionally, we expect that the model’s performance can\nbe further improved by incorporating additional information\nfrom other sensors, such as lidar and radar, as demonstrated\nby recent works (Harley et al. 2022).\nCollaborative Perception Datasets\nAutonomous vehicle datasets have been widely used in\ncollaborative perception research, comprising various sen-\nsory inputs, including cameras, lidar, and GPS (Han et al.\n2023), from multiple vehicles in a vehicle-to-vehicle envi-\nronment (Xu et al. 2022c; Yu et al. 2022). Some datasets,\nsuch as those proposed in (Li et al. 2022a; Mao et al.\n2022), include infrastructure sensors, resulting in a vehicle-\nto-infrastructure data model. Others, such as the dataset pre-\nsented in (Xu et al. 2022b), employ a vehicle-to-everything\nmodel. The CoPerception-UA Vs dataset (Hu et al. 2022)\nemploys five images from five drones flying in forma-\ntion. It is worth noting that these datasets are all sourced\nfrom CARLA (Dosovitskiy et al. 2017) in Unreal En-\ngine, a widely used open-source platform for simulating au-\ntonomous vehicles.\nThe HEV datasets sourced from our simulated environ-\nments are uniquely challenging in the field of collaborative\nperception, as the agents are equipped with only one or two\ncameras. Unlike previously proposed collaborative percep-\ntion problems, the HEV task does not provide the agents\nwith the transformation component of their pose. The un-\nknown position of each camera view within the global co-\nordinate frame adds a significant challenge to the semantic\nsegmentation prediction task and other downstream tasks.\nCollaborative Perception Methods\nCollaborative perception has been explored in recent years,\nimproving the capability of single-agent perception models\n(Li et al. 2022b; Hu et al. 2022; Lei et al. 2022; Su et al.\n2023; Zhou et al. 2022). In conventional collaborative per-\nception, intermediate representations produced by sensors or\nneural networks from multiple viewpoints are propagated\namong a team of robots, such as a group of vehicles (Hu\net al. 2022; Xu et al. 2022a) or a swarm of drones (Zhou et al.\n2022; Hu et al. 2022). The existing works commonly learn\na collaboration module, produced by a Graph Neural Net-\nwork (Zhou et al. 2022; Li et al. 2021), Convolutional Neu-\nral Network (Li et al. 2022b; Qiao and Zulkernine 2023), or\na Transformer (Xu et al. 2022a; Hu et al. 2022) to combine\nmultiple robot intermediate representations.\nPrior research has focused on robots equipped with mul-\ntiple sensors, requiring sensor data fusion on a per-agent ba-\nsis before information exchange among agents (Han et al.\n2023). However, in this work, we focus on robots with only\none or two cameras and no additional sensors, making our\napproach more amenable to smaller, simpler robot swarms.\nSince we focus on simpler robots, we do not utilize a collab-\noration module, and instead fuse all camera views together\nin a single cross-attention module.\nMethodology\nHerd’s Eye View\nIn the Herd’s Eye View (HEV) semantic segmentation\ntask, we are given a set of n monocular camera views,\n(Ik, Kk, Rk)n\nk=1 consisting of an input image Ik ∈\nRH×W×3, camera intrinsics Kk ∈ R3×3, and extrinsic ro-\ntation Rk ∈ R3×3 with respect to the agent base. The goal\n307\nBackbone\nCross \nAttention\nConv\nControl \nPolicy\n(MA-POCA)\nActions\nWorld-Centric HEV\nMulti-Robot Cameras \nMulti-Robot\nCamera Features\nWorld-Centric\nMap Embedding \nQKV\nConv\nFigure 1: A visualization of the proposed HEV approach in the Dungeon Escape environment: Agent camera views are extracted\nvia a backbone model, then combined in a cross-attention module, then decoded into a world-centric semantic segmentation.\nThe resulting semantic segmentation can be used as an observation for a swarm of robots.\nof the HEV task is to predict a binary semantic segmenta-\ntion mask y ∈ {0, 1}h×w×C in the global coordinate frame,\nwhere C is the desired number of segmentation classes.\nThe HEV task adds additional ambiguity to the well-studied\nBEV task as each camera view is at an unknown translation\nand orientation with respect to the global coordinate frame.\nWe define a BEV as a single-agent perception transformed\ninto an ego-centric view, whereas the HEV is a collabora-\ntive perception transformed into a fixed world-centric view.\nA comparison of the ego-centric views tested and the fixed\nword-centric view can be seen in Figure 3.\nOur approach, seen in Figure 1, follows three steps:\n1. Collect multiple views of the environment from robot\ncameras.\n2. Use a collaborative perception model to obtain the HEV ,\nthe world-centric semantic segmentation of the environ-\nment.\n3. Input the HEV to a Reinforcement Learning (RL) control\npolicy to obtain agent control commands.\nOur goal is to establish a baseline HEV perception model\nto extract information from the multiple camera views and\nproject them onto a fixed world-centric view. We propose\na baseline perception model using the Cross-View Trans-\nformer (CVT) (Zhou and Kr¨ahenb¨uhl 2022) and use seman-\ntic segmentation as our downstream task. The Cross-View\nTransformer is a recent approach that uses a cross-view at-\ntention module, first proposed by (Zhou and Kr ¨ahenb¨uhl\n2022), enabling the agents to reason about their environ-\nment in an ego-centric coordinate frame. We extend the\nCVT model to further improve its accuracy and speed for\nthe HEV use case. We name our baseline model the Herd’s\nEye View Cross-View Transformer (HEV-CVT). We use a\nworld-centric map embedding and tune positional embed-\ndings, output sizes, and the number of transformer layers to\nfit our proposed HEV environments.\nData Collection\nWe use identical Unity simulation environments to source\nthe datasets for training the HEV semantic segmentation\ntask and MARL task. To collect the HEV ground truth for\nboth tasks, we use our own custom fork of MBaske’s Unity\nGrid-Sensor Library (Baske 2021) which allows the collec-\ntion of HEV world-centric grid-sensors. The only difference\nbetween ego-centric based agents and world-centric based\nagents is the location of their grid-sensor and the perspec-\ntive at which they take their actions (e.g., forward for the\nword-centric agent is always North, but forward for the ego-\ncentric agent is with respect to their current orientation). All\nagents are trained on ground-truth sensors, calculated using\nthe bounding boxes that are individually tuned to each ob-\nject. The resolution of the grid-sensor is adjusted to accom-\nmodate the complexity and size of the environment as seen\nin Table??. Example observations of world-centric and ego-\ncentric based agents can be seen in Figure 3\nOur simulations are conducted in three different Unity\nML-Agents environments:\nCollaborative Push Block:Three agents are required to\npush white blocks to a green goal area on a randomly se-\nlected side of a square area. There are blocks sized one, two\nand three, each requiring the respective amount of agents to\npush into the goal area (Cohen et al. 2022).\nDungeon Escape:As a Green Dragon slowly walks to-\nwards an exit portal, one of the three agents must collide\nwith it in order to sacrifice itself and spawn a key. The key\n308\nFigure 2: Images of the three environments used to test the HEV collaborative perception and reinforcement learning algorithms.\nThe top left is the Dungeon Escape environment. The top right is the Collaborative Push Block environment. The bottom is the\nPlanar Construction environment.\nmust then be picked up by one of the two remaining agents\nand brought to the exit door to escape the dungeon (Cohen\net al. 2022). Once any agent escapes, all agents win.\nPlanar Construction:Six agents collaborate to push red\npucks into desired positions. Desired positions are randomly\nassigned to valid coordinates within the arena, and are ob-\nserved via a Grid-Sensor, similar to the Push Block environ-\nment (Strickland, Churchill, and Vardy 2019). In each round\na new random amount of pucks from 2 to 16 are spawned.\nWe utilize the open-source Collaborative Push Block and\nDungeon Escape environments from ML-Agents (Cohen\net al. 2022), which are already native to Unity and only\nchange the sensor input of agents. We recreate the Planar\nconstruction task (Vardy 2018; Strickland, Churchill, and\nVardy 2019; Vardy 2020, 2022, 2023) based on Strickland,\nChurchill, and Vardy’s work in the CWaggle simulator but\nadapt the environment to Unity ML-Agents. All three envi-\nronments can be seen in Figure 2. For MARL training, we\nuse the HEV ground truth as model input and identical re-\nward functions to the original implementations. Specifically,\nthe agents are trained using the Multi-Agent POsthumous\nCredit Assignment (MA-POCA) algorithm (Cohen et al.\n2022) in Unity ML-Agents. By using identical reward func-\ntions, we aim to create a fair comparison between the per-\nformance of agents using HEV and those using traditional\nsensor frames in cooperative scenarios.\nThe MARL task enables us to train the CVT models,\nwhich can perform semantic segmentation in an ego-centric\nor world-centric view. To collect the data necessary for train-\ning the CVT models, we run the trained MA-POCA mod-\nels and collect the camera view, camera intrinsics, and rota-\ntion extrinsic from each agent at each step of the simulation,\nalong with the ground truth HEV and BEV . By collecting\ndata from various environments and introducing variations,\nwe aim to create diverse and robust datasets for training the\nCVT models.\nImplementation Details\nThe Cross-View Transformer is adapted from Zhou and\nKr¨ahenb¨uhl for the Herds Eye View Collaborative Percep-\ntion task. The first stage of the network passes each in-\nput image from agents into a feature extractor, we use\nan EfficientNet-B4 (Tan and Le 2019), which outputs two\nmulti-resolution patch embeddings of size (28, 60) and (14,\n30). Each patch is passed into a Cross-View Transformer\nconvolution stack as in the original implementation. We\nfound fewer convolution stacks significantly degrade the\nHEV-CVTs ability to localize, and more are not necessary.\nThe patch embedding act as image features and are used in\nthe keys and as the values for the Cross-View Transformer.\nWe encode the rotation Rk ∈ R3×3 of the agent’s camera\ninto a D-Dimensional positional embedding using a multi-\nlayer perceptron. We useD = 64for all of our experiments.\nThe positional embedding is combined with the image fea-\nture to compute the keys for the cross-view transformer. The\nworld-centric map embedding operates similarly to the orig-\n309\nFigure 3: Example scene and corresponding agent observations from the Collaborative Push Block environment. The top image\nshows a debug camera (not available for agent observation). The bottom left shows the HEV world-centric observation of the\nblue agent. The bottom middle shows the BEV-centric observation of the blue agent. The bottom right shows the BEV-forward\nobservation of the blue agent. Blue is the controller agent, green is ally agents, and red shades are differently-sized push blocks.\nAgents Agent Cameras Grid Size\nCollaborative Push Block 3 1-left, 1-right 32x32\nDungeon Escape 2-3 1-left, 1-right 32x32\nPlanar Construction 6 1-forward 32x64\nTable 1: HEV simulated environment parameters.\ninally proposed map-view embedding. The key difference\nwith our approach is we do not subtract camera location em-\nbeddings from the map embedding, instead, we directly use\nthe learned map embedding as queries. The camera locations\nwith respect to the world are unknown for the HEV task, and\nwe found subtracting rotation embeddings did not improve\nperformance. The transformer architecture refines its world-\ncentric estimate through two rounds of computation, each\nresulting in new latent embeddings used as queries.\nThe cross-view transformer computes softmax-cross-\nattention (Vaswani et al. 2017) between the image feature\nkeys, values and world-centric queries. This setup allows\nworld coordinates from the world-centric map embedding to\nattend to one or more image locations, allowing the model to\nreason about the environment from multiple image features.\nThe multi-head attention mechanism uses 4-heads like the\noriginal implementation but with half the embedding size of\ndhead = 32.\n310\nFigure 4: Sample HEV-CVT prediction from the Dungeon Escape environment validation dataset. The two left columns show\neach of the three agents’ unique camera views, each row contains the images from the left and right cameras of the same agent.\nThe top right shows the HEV-CVT prediction confidence heat map of the agents and dragon, the ground truth is directly to the\nleft (agents are blue, the dragon is red). The bottom right shows a world-view camera not available to agents to help readers\nunderstand the scene.\nThe cross-view transformer output is 8x8 for square en-\nvironments and 8x8 and 8x16 for rectangular environments,\nthis then passes through a decoder consisting of three up-\nconvolutional layers to a final size of 64x64 and 64x128.\nThis is purposely larger than is required for RL observa-\ntion size, as smaller sizes can create ambiguity for some\nobject occupancy resulting in decreased performance. These\nlarger HEV-CVT sizes can easily be down-sampled to match\nthe required RL observation sizes of 32x32 and 32x64. We\nthreshold the output prediction confidences, keeping predic-\ntions with a confidence greater than 0.4. The prediction con-\nfidences prior to thresholding can be seen in Figure 4 as a\nheat map (lighter is higher confidence).\nOur training process is similar to the original implemen-\ntation by Zhou and Kr ¨ahenb¨uhl, we also use focal loss (Lin\net al. 2017) and the AdamW (Loshchilov and Hutter 2017)\noptimizer with a one-cycle learning rate scheduler (Smith\nand Topin 2018). All models are trained with a batch size of\n4 for 25 epochs. Training lasts approximately 8 hours on a\nsingle RTX 3090 GPU before converging.\nExperiments and Results\nCollaborative Perception\nThe HEV-CVT model must accurately localize the position\nof each agent based on the overlap of camera frames, which\nare located at unknown positions. An example of this in the\nDungeon Escape environment can be seen in Figure 4. The\ncameras are recorded at resolution480×224, and we use the\ncamera intrinsics of a Raspberry Pi Camera Module 3. Con-\nsistent with prior works (Ma et al. 2023), we show the re-\nsult Intersection over Union (IoU) metric for the HEV-CVT\nmodel trained on each environment in Table ??. We com-\npare the performance of the baseline CVT model on world-\ncentric, ego-centric, and ego-forward coordinate frames.\nIn the Collaborative Push Block environment, three\nagents are equipped with two forward-facing cameras and\nare tasked with predicting the occupancy of all push blocks,\nagents and the goal area. In the Dungeon Escape environ-\nment, three agents are equipped with two forward-facing\ncameras and are tasked with predicting the occupancy of the\ndragon, agents and key. In the Planar Construction environ-\nment, six agents are equipped with a single forward-facing\ncamera and are tasked with predicting the occupancy of all\npucks and agents.\nOur results shown in Table ?? demonstrate the world-\ncentric coordinate frame consistently outperforms the ego-\ncentric coordinate frames in all environments. The Collabo-\nrative Push Block and Dungeon Escape environments show\nthe largest performance improvements, with up to 32.72%\nand 17.46% improvement in IoU, respectively. These re-\nsults suggest that the world-centric HEV approach is effec-\ntive in addressing the challenges of collaborative perception\nin multi-agent environments. This result is especially appar-\nent in the Collaborative Push Block environment, where the\nHEV-CVT model easily localizes itself based on the large\ngoal location seen in most camera views for a near-perfect\n96.94% IoU score. The landmarks in the Dungeon Escape\nenvironment, the exit door and portal are in randomized lo-\ncations which makes localization harder than the Push Block\nenvironment, reflected by the steep drop in IoU scores.\nThe standard ML-Agents environments were not as chal-\n311\nWorld-Centric Ego-Centric Ego-Forward\nCollaborative Push Block 96.94% 63.87% 64.22%\nDungeon Escape 43.53% 13.47% 26.07%\nPlanar Construction 48.37% 35.45% 10.16%\nTable 2: HEV-CVT validation IoU results per coordinate frame in each environment (higher is better).\nWorld-Centric Ego-Centric Ego-Forward\nCollaborative Push Block 100.1 ± 40.6 137.9 ± 53.5 124.9 ± 47.4\nDungeon Escape 15.1 ± 0.81 17.3 ± 0.87 18.4 ± 1.44\nPlanar Construction 176.9 ± 40.8 233.8 ± 73.7 239.8 ± 75.8\nTable 3: MA-POCA mean episode length ± standard deviation per coordinate frame in each environment (lower is better).\nlenging for the CVT models as there were not many per-\nmutations of the environment layout. By contrast, our cus-\ntom Planar Construction environment presents a more com-\nplex challenge as we randomly change the coloring of six\nwall and floor components at every time step of the envi-\nronment during data collection. Additionally, the locations\nof pucks to be pushed are randomized, and the environment\narea is twice the size of the ML-Agents environments. De-\nspite the additional challenge the HEV-CVT model still per-\nforms well in the Planar Construction environment scoring\n48.37% on the HEV task. This result shows the CVT models\ncan localize based on the overlap in views between cameras\nas much of the validation set contains wall colors and puck\nlayouts never before seen.\nMulti-Agent Reinforcement Learning\nIn order to compare the performance of the fixed world-\ncentric coordinate frames with other commonly used coor-\ndinate frames, we conduct experiments in all three proposed\nenvironments. To ensure a fair comparison between the per-\nformance of agents using different coordinate frames, we\nuse identical reward functions to each environment’s orig-\ninal implementation and identical grid sizes.\nTable ?? compares the performance of agents using dif-\nferent coordinate frames in all three proposed environments.\nWe find consistently lower episode lengths with world-\ncentric based agents compared to ego-centric. We opt to use\nepisode length as our performance metric, as it directly re-\nflects the speed of task completion. While alternative metrics\nsuch as cumulative or mean reward are also commonly used,\nthese primarily reflect minor negative rewards assigned per\ntime step, providing less insight into an agent’s efficiency in\nour context.\nOur experiments highlight a common challenge faced by\nBEV-based agents in all three environments, often an object\nnecessary to take the optimal action was missing from the\nagent’s view, leading to sub-optimal decision-making and\nincreased episode lengths. This was especially apparent in\nthe Push Block environment where often one of the three\nagents would not observe the size three block (requiring all\nthree agents to push it), causing two agents to be waiting for\nthe third agent to join them at the block, wasting time. Con-\nversely, we found HEV-based agents in the Push Block en-\nvironment stuck close together and consistently pushed the\nhighest value blocks together first.\nThe HEV-based agents were able to leverage the multi-\nple viewpoints available to them, enabling them to better\nperceive their environment and take more optimal actions.\nThis issue was particularly evident in the Push Block en-\nvironment, where the improved perception of world-centric\nagents resulted in significantly lower episode lengths than\nego-based agents.\nOverall, these findings suggest that the HEV framework\noffers a superior perception model in MARL environments,\nproviding agents with a more comprehensive understanding\nof their surroundings, leading to improved decision-making\nand better overall performance.\nConclusion\nWe have proposed a new perception model called Herd’s\nEye View that provides a global view of the environment, en-\nabling better global coordination and cooperation in MARL\nscenarios. We conduct two sets of experiments in three\nsimulated multi-agent environments. Our first experiment\nfocuses on the perception aspect of HEV and shows the\nsame Cross-View Transformer model performs better on the\nworld-centric HEV task than its BEV ego-centric counter-\npart. Our second experiment focuses on the effectiveness\nof the HEV perspective view compared to BEV perspec-\ntive views for MARL agents. We find that RL agents trained\non world-centric perspective views learn more efficient poli-\ncies than those trained on ego-centric perspective views. Our\nwork opens up new possibilities for advanced perception\nmodels in MARL game environments, which can greatly en-\nhance the performance of multi-agent systems by enabling\nbetter collaboration and coordination.\n312\nReferences\nBaske, M. 2021. Grid Sensors for Unity ML-Agents - Ver-\nsion 2.0. https://github.com/mbaske/grid-sensor. Accessed:\n2021-08-21.\nCohen, A.; Teng, E.; Berges, V .-P.; Dong, R.-P.; Henry, H.;\nMattar, M.; Zook, A.; and Ganguly, S. 2022. On the Use and\nMisuse of Abosrbing States in Multi-agent Reinforcement\nLearning. RL in Games Workshop AAAI 2022.\nDosovitskiy, A.; Ros, G.; Codevilla, F.; Lopez, A.; and\nKoltun, V . 2017. CARLA: An Open Urban Driving Simula-\ntor. In Proceedings of the 1st Annual Conference on Robot\nLearning, 1–16.\nHan, Y .; Zhang, H.; Li, H.; Jin, Y .; Lang, C.; and Li, Y . 2023.\nCollaborative Perception in Autonomous Driving: Methods,\nDatasets and Challenges. ArXiv, abs/2301.06262.\nHarley, A. W.; Fang, Z.; Li, J.; Ambrus, R.; and Fragkiadaki,\nK. 2022. Simple-BEV: What Really Matters for Multi-\nSensor BEV Perception? In arXiv:2206.07959.\nHu, A.; Murez, Z.; Mohan, N.; Dudas, S.; Hawke, J.; Badri-\nnarayanan, V .; Cipolla, R.; and Kendall, A. 2021. FIERY:\nFuture Instance Segmentation in Bird’s-Eye view from Sur-\nround Monocular Cameras. In Proceedings of the Interna-\ntional Conference on Computer Vision (ICCV).\nHu, Y .; Fang, S.; Lei, Z.; Zhong, Y .; and Chen, S. 2022.\nWhere2comm: Communication-Efficient Collaborative Per-\nception via Spatial Confidence Maps. InThirty-sixth Confer-\nence on Neural Information Processing Systems (Neurips).\nJuliani, A.; Khalifa, A.; Berges, V .-P.; Harper, J.; Teng, E.;\nHenry, H.; Crespi, A.; Togelius, J.; and Lange, D. 2019. Ob-\nstacle Tower: A Generalization Challenge in Vision, Con-\ntrol, and Planning. arXiv:1902.01378.\nLei, Z.; Ren, S.; Hu, Y .; Zhang, W.; and Chen, S. 2022.\nLatency-Aware Collaborative Perception. In2022 European\nConference on Computer Vision (ECCV), 316–332. ISBN\n978-3-031-19823-6.\nLi, Y .; Ma, D.; An, Z.; Wang, Z.; Zhong, Y .; Chen, S.; and\nFeng, C. 2022a. V2X-Sim: Multi-Agent Collaborative Per-\nception Dataset and Benchmark for Autonomous Driving.\nIEEE Robotics and Automation Letters, 7(4): 10914–10921.\nLi, Y .; Ren, S.; Wu, P.; Chen, S.; Feng, C.; and Zhang, W.\n2021. Learning Distilled Collaboration Graph for Multi-\nAgent Perception. In Thirty-fifth Conference on Neural In-\nformation Processing Systems (NeurIPS 2021).\nLi, Y .; Zhang, J.; Ma, D.; Wang, Y .; and Feng, C. 2022b.\nMulti-Robot Scene Completion: Towards Task-Agnostic\nCollaborative Perception. In 6th Annual Conference on\nRobot Learning.\nLi, Z.; Wang, W.; Li, H.; Xie, E.; Sima, C.; Lu, T.; Qiao, Y .;\nand Dai, J. 2022c. BEVFormer: Learning Bird’s-Eye-View\nRepresentation from Multi-Camera Images via Spatiotem-\nporal Transformers. arXiv preprint arXiv:2203.17270.\nLin, T.-Y .; Goyal, P.; Girshick, R. B.; He, K.; and Doll´ar, P.\n2017. Focal Loss for Dense Object Detection.2017 IEEE In-\nternational Conference on Computer Vision (ICCV), 2999–\n3007.\nLoshchilov, I.; and Hutter, F. 2017. Fixing Weight Decay\nRegularization in Adam. CoRR, abs/1711.05101.\nMa, Y .; Wang, T.; Bai, X.; Yang, H.; Hou, Y .; Wang, Y .;\nQiao, Y .; Yang, R.; Manocha, D.; and Zhu, X. 2023. Vision-\nCentric BEV Perception: A Survey. arXiv:2208.02797.\nMao, R.; Guo, J.; Jia, Y .; Sun, Y .; Zhou, S.; and Niu, Z. 2022.\nDOLPHINS: Dataset for Collaborative Perception enabled\nHarmonious and Interconnected Self-driving. In Proceed-\nings of the Asian Conference on Computer Vision (ACCV) ,\n4361–4377.\nPeng, L.; Chen, Z.; Fu, Z.; Liang, P.; and Cheng, E. 2022.\nBEVSegFormer: Bird’s Eye View Semantic Segmentation\nFrom Arbitrary Camera Rigs. arXiv:2203.04050.\nPhilion, J.; and Fidler, S. 2020. Lift, Splat, Shoot: Encoding\nImages From Arbitrary Camera Rigs by Implicitly Unpro-\njecting to 3D. In Proceedings of the European Conference\non Computer Vision.\nQiao, D.; and Zulkernine, F. 2023. Adaptive Feature Fusion\nfor Cooperative Perception Using LiDAR Point Clouds. In\nProceedings of the IEEE/CVF Winter Conference on Appli-\ncations of Computer Vision (WACV), 1186–1195.\nReiher, L.; Lampe, B.; and Eckstein, L. 2020. A Sim2Real\nDeep Learning Approach for the Transformation of Images\nfrom Multiple Vehicle-Mounted Cameras to a Semantically\nSegmented Image in Bird’s Eye View. In 2020 IEEE 23rd\nInternational Conference on Intelligent Transportation Sys-\ntems (ITSC).\nSmith, L. N.; and Topin, N. 2018. Super-Convergence: Very\nFast Training of Neural Networks Using Large Learning\nRates. arXiv:1708.07120.\nStrickland, C.; Churchill, D.; and Vardy, A. 2019. A Rein-\nforcement Learning Approach to Multi-Robot Planar Con-\nstruction. In IEEE International Symposium on Multi-Robot\nand Multi-Agent Systems, 238–244.\nSu, S.; Li, Y .; He, S.; Han, S.; Feng, C.; Ding, C.; and Miao,\nF. 2023. Uncertainty Quantification of Collaborative Detec-\ntion for Self-Driving. In IEEE International Conference on\nRobotics and Automation (ICRA).\nTan, M.; and Le, Q. V . 2019. EfficientNet: Rethinking\nModel Scaling for Convolutional Neural Networks. CoRR,\nabs/1905.11946.\nVardy, A. 2018. Orbital Construction: Swarms of Simple\nRobots Building Enclosures. In 2018 IEEE 3rd Interna-\ntional Workshops on Foundations and Applications of Self*\nSystems (FAS*W), 147–153.\nVardy, A. 2020. Robot Distancing: Planar Construction with\nLanes. In Dorigo, M.; St ¨utzle, T.; Blesa, M. J.; Blum, C.;\nHamann, H.; Heinrich, M. K.; and Strobel, V ., eds., Swarm\nIntelligence, 229–242. Cham: Springer. ISBN 978-3-030-\n60376-2.\nVardy, A. 2022. The Lasso Method for Multi-Robot Forag-\ning. In 19th Conference on Robots and Vision (CRV), 106–\n113. IEEE Xplore.\nVardy, A. 2023. The Swarm within the Labyrinth: Pla-\nnar Construction by a Robot Swarm. Artificial Life and\nRobotics, 28: 117–126.\n313\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention Is All You Need. CoRR, abs/1706.03762.\nWydmuch, M.; Kempka, M.; and Ja´skowski, W. 2019. ViZ-\nDoom Competitions: Playing Doom from Pixels. IEEE\nTransactions on Games, 11(3): 248–259. The 2022 IEEE\nTransactions on Games Outstanding Paper Award.\nXu, R.; Tu, Z.; Xiang, H.; Shao, W.; Zhou, B.; and Ma, J.\n2022a. CoBEVT: Cooperative Bird’s Eye View Semantic\nSegmentation with Sparse Transformers. In Conference on\nRobot Learning (CoRL).\nXu, R.; Xiang, H.; Tu, Z.; Xia, X.; Yang, M.-H.; and Ma, J.\n2022b. V2X-ViT: Vehicle-to-Everything Cooperative Per-\nception with Vision Transformer. In Proceedings of the Eu-\nropean Conference on Computer Vision (ECCV).\nXu, R.; Xiang, H.; Xia, X.; Han, X.; Li, J.; and Ma, J. 2022c.\nOPV2V: An Open Benchmark Dataset and Fusion Pipeline\nfor Perception with Vehicle-to-Vehicle Communication. In\n2022 IEEE International Conference on Robotics and Au-\ntomation (ICRA).\nYu, H.; Luo, Y .; Shu, M.; Huo, Y .; Yang, Z.; Shi, Y .; Guo,\nZ.; Li, H.; Hu, X.; Yuan, J.; and Nie, Z. 2022. DAIR-V2X:\nA Large-Scale Dataset for Vehicle-Infrastructure Coopera-\ntive 3D Object Detection. In IEEE/CVF Conf. on Computer\nVision and Pattern Recognition (CVPR).\nZhou, B.; and Kr ¨ahenb¨uhl, P. 2022. Cross-view Trans-\nformers for real-time Map-view Semantic Segmentation. In\nCVPR.\nZhou, Y .; Xiao, J.; Zhou, Y .; and Loianno, G. 2022. Multi-\nrobot collaborative perception with graph neural networks.\nIEEE Robotics and Automation Letters, 7(2): 2289–2296.\n314",
  "topic": "Reinforcement learning",
  "concepts": [
    {
      "name": "Reinforcement learning",
      "score": 0.6887060403823853
    },
    {
      "name": "Perception",
      "score": 0.6791523098945618
    },
    {
      "name": "Computer science",
      "score": 0.6460455656051636
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.6422401070594788
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5901300311088562
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5410107970237732
    },
    {
      "name": "Human–computer interaction",
      "score": 0.4505314826965332
    },
    {
      "name": "Psychology",
      "score": 0.15434995293617249
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130438778",
      "name": "Memorial University of Newfoundland",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210126948",
      "name": "Memorial",
      "country": "RU"
    }
  ]
}