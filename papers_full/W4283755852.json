{
  "title": "Graph transformer for cross-lingual plagiarism detection",
  "url": "https://openalex.org/W4283755852",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2740239922",
      "name": "Oumaima Hourrane",
      "affiliations": [
        "University of Hassan II Casablanca"
      ]
    },
    {
      "id": "https://openalex.org/A2226767099",
      "name": "El Habib Benlahmar",
      "affiliations": [
        "University of Hassan II Casablanca"
      ]
    },
    {
      "id": "https://openalex.org/A2740239922",
      "name": "Oumaima Hourrane",
      "affiliations": [
        "University of Hassan II Casablanca"
      ]
    },
    {
      "id": "https://openalex.org/A2226767099",
      "name": "El Habib Benlahmar",
      "affiliations": [
        "University of Hassan II Casablanca"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2400223370",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2029097226",
    "https://openalex.org/W2149182189",
    "https://openalex.org/W6603477831",
    "https://openalex.org/W13806629",
    "https://openalex.org/W2019102947",
    "https://openalex.org/W6629600108",
    "https://openalex.org/W1498747508",
    "https://openalex.org/W1542647698",
    "https://openalex.org/W2120779048",
    "https://openalex.org/W3005896003",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2986143601",
    "https://openalex.org/W2593644299",
    "https://openalex.org/W2945673882",
    "https://openalex.org/W2963227052",
    "https://openalex.org/W2584483805",
    "https://openalex.org/W2250621296",
    "https://openalex.org/W6633232144",
    "https://openalex.org/W2101287987",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W1518684876",
    "https://openalex.org/W6632555176",
    "https://openalex.org/W22168010",
    "https://openalex.org/W1625582487",
    "https://openalex.org/W2617604077",
    "https://openalex.org/W6731744468",
    "https://openalex.org/W7051469422",
    "https://openalex.org/W2589884733",
    "https://openalex.org/W3017228117",
    "https://openalex.org/W2099733782",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2134168908",
    "https://openalex.org/W2277931188",
    "https://openalex.org/W1869906767",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1496503889",
    "https://openalex.org/W1544505227",
    "https://openalex.org/W2572474373",
    "https://openalex.org/W630532510",
    "https://openalex.org/W1553277444",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W2267186426",
    "https://openalex.org/W2962843304",
    "https://openalex.org/W2951166594"
  ],
  "abstract": "&lt;span lang=\"EN-US\"&gt;The existence of vast amounts of multilingual textual data on the internet leads to cross-lingual plagiarism which becomes a serious issue in different fields such as education, science, and literature. Current cross-lingual plagiarism detection approaches usually employ syntactic and lexical properties, external machine translation systems, or finding similarities within a multilingual set of text documents. However, most of these methods are conceived for literal plagiarism such as copy and paste, and their performance is diminished when handling complex cases of plagiarism including paraphrasing. In this paper, we propose a new graph-based approach that represents text passages in different languages using knowledge graphs. We put forward a new graph structure modeling method based on the Transformer architecture that employs precise relation encoding and delivers a more efficient way for global graph representation. The mappings between the graphs are learned both in semi-supervised and unsupervised training mechanisms. The results of our experiments in Arabicâ€“English, Frenchâ€“English, and Spanishâ€“English plagiarism detection show that our graph transformer method surpasses the state-of-the-art cross-lingual plagiarism detection approaches with and without paraphrasing cases, and provides further insights on the use of knowledge graphs on a language-independent model.&lt;/span&gt;",
  "full_text": "IAES International Journal of Artificial Intelligence (IJ-AI) \nVol. 11, No. 3, September 2022, pp. 905~915 \nISSN: 2252-8938, DOI: 10.11591/ijai.v11.i3.pp905-915 ï²     905 \n \nJournal homepage: http://ijai.iaescore.com \nGraph transformer for cross-lingual plagiarism detection \n \n \nOumaima Hourrane, El Habib Benlahmar \nLaboratory Information Technology and Modeling, Faculty of Sciences Ben Msik, Hassan II University of Casablanca, Casablanca, \nMorocco \n \n \nArticle Info  ABSTRACT \nArticle history: \nReceived Sep 8, 2021 \nRevised Apr 21, 2022 \nAccepted May 20, 2022 \n \n The existence of vast amounts of multilingual textual data on the internet \nleads to cross -lingual plagiarism which becomes a serious issue in different \nfields such as education, science, and literature. Current cross -lingual \nplagiarism detection approaches usu ally employ syntactic and lexical \nproperties, external machine translation systems, or finding similarities \nwithin a multilingual set of text documents. However, most of these methods \nare conceived for literal plagiarism such as copy and paste, and their \nperformance is diminished when handling complex cases of plagiarism \nincluding paraphrasing. In this paper, we propose a new graph -based \napproach that represents text passages in different languages using \nknowledge graphs. We put forward a new graph structur e modeling method \nbased on the Transformer architecture that employs precise relation \nencoding and delivers a more efficient way for global graph representation. \nThe mappings between the graphs are learned both in semi -supervised and \nunsupervised training mechanisms. The results of our experiments in \nArabicâ€“English, Frenchâ€“English, and Spanish â€“English plagiarism detection \nshow that our graph transformer method surpasses the state -of-the-art cross-\nlingual plagiarism detection approaches with and without para phrasing \ncases, and provides further insights on the use of knowledge graphs on a \nlanguage-independent model. \nKeywords: \nCross-lingual plagiarism \nGraph neural network \nGraph transformer \nKnowledge graphs \nThis is an open access article under the CC BY-SA license. \n \nCorresponding Author: \nOumaima Hourrane \nLaboratory Information Technology and Modeling, Faculty of Sciences Ben Msik, Hassan II University of \nCasablanca \nMorocco \nEmail: oumaima.hourrane@gmail.com \n \n \n1. INTRODUCTION \nPlagiarism is the use of original text data without providing adequate references. This phenomenon \nis accentuated when the root of plagiarism is in a different language, which is known as cross -lingual \nplagiarism. Although some research works have been carried out on monolingual plagiarism analysis, to our \nawareness, cross -lingual plagiarism analysis is still an emerging natural language processing task that has \nbeen studied in the literature. The task can be described as follows: Given a suspect document in a certain \nlanguage, we are interested in checking if it is plagiarize d from one or a set of original documents written in \nother language. \nCurrent cross -lingual plagiarism detection approaches usually employ syntactic and lexical \nproperties, external machine translation (MT) systems, or computing similarities between multili ngual \ndocuments. Yet, these methods are conceived for literal plagiarism such as copy and paste, and their \nperformance is diminished when handling complex cases of plagiarism including paraphrasing. The literal \nplagiarism form â€œcopy and pasteâ€ in theory, t he most easily detected and identifiable textual similarity. \nCertainly, the detection of this form is similar to checking the identity between two texts. To ingenuously \n\n      ï²          ISSN: 2252-8938 \nInt J Artif Intell, Vol. 11, No. 3, September 2022: 905-915 \n906 \ncarry out this analysis automatically, it is required to carry out a word -for-word comparison of texts. Since \nthis process is far too time -consuming to be integrated into commercially oriented or online solutions, as is \nthe case with most of the anti -plagiarism tools, alternative methods had to be produced, which is one of the \ngoals of this present work. \nFurthermore, it is an accepted fact that automatically detecting textual semantic similarities such as \nparaphrase does not amount to detecting a possibility of plagiarism. Plagiarism is copying or paraphrasing \ntext without citing the original  reference, but in the case of textual similarity, we cannot know if the texts are \nsimilar literally or semantically, and consequently to correlate this similarity with plagiarism. It will then be \nup to a human to identify whether or not any similarities d etected count as plagiarism. Certainly, they can be \nresulting from coincidences or from properly cited references. In this work, we do not pass judgment or make \nany decisions; we only focus on finding similar passages between two texts.  \nWe can describe pla giarism detection process as a system composed of two consecutive tasks [1]. \nThe first task is the candidate source retrieval of suspicious documents to compare later, and the second task \nis the detailed comparison, which is finding alignments of similar passages of pairs of documents, between \nthe suspect document being processed and each of the sources returned by the first task. This paper focuses \nonly on the second task: the cross -lingual comparison between suspect texts and a fixed number of candidate \nsource texts. \nIn this paper, we proposed cross -lingual graph transformer -based analysis (CL -GTA), an approach \nfor cross -lingual plagiarism detection that aim to represent the whole context by using knowledge graphs \nsimultaneously to broaden and connect the concepts in  a textual document. For graph representation, we \npropose a new model called Graph -Transformer that depends completely on the multi -head attention \nmechanism [2]. The graph transformer enables direct representation of relations between any two nodes \nwithout considering their remoteness in the graph. At last, we evaluate our method and compare it against the \nstate-of-the-art using a dataset composed of manually and automatically created paraphrases, we also \nevaluate the performance of the analysis using paraphrases only. \nThe rest of the paper is structured as follows. In Section 2 we cite the state -of-the-art methods in \ncross-lingual plagiarism detection. In Section 3 we describe the background on transformers and graph neural \nnetworks. Then we describe the knowledge graph creation and the graph transformer model for graph \nrepresentation, and then we conclude the section with the general framework for cross -lingual plagiarism \ndetection. We evaluate in section 4 our approach for Spanish â€“English, Frenchâ€“English, and Arab icâ€“English \ncorpora, and comparing our results with various state -of-the-art approaches. We also show the results of \ndetecting only paraphrases. \n \n \n2. RELATED WORK \nThis section reviews the methods of cross -language similarity computing that have been employed \nfor cross-Lingual plagiarism detection. An effectual algorithm for cross -languages with lexical and syntactic \nsimilarities is the cross -language character n -gram (CL -CnG) [3]. It is basically similar to some other \nmonolingual plagiarism detection models [4], [5]. This model is syntax-based that employs character n-grams \nto model texts, namely, after text segmentation into 3 -grams, the authors transformed it into tf -idf matrices of \ncharacter 3 -grams, after that, they used a weighting mechanism and cosine similarity as a metric for \nsimilarity computing. \nVarious methods exist that use parallel corpora, which is called cross -language alignment -based \nsimilarity analysis (CL -ASA) [6], [7] . This type of analysis is usually based on a statistical Machine \nTranslation system. It determines how a text passage is probably the translation of other text using a \nstatistical bilingual dictionary â€“ generated with parallel a corpus which contains translation pairs. To make \ntext alignment, this method takes into account the translation probability distributions and the  variances in \nsize of parallel texts in distinct languages. \nThere are two other approaches employing concepts from knowledge graphs like in this paper, they \nare referred to as cross -language thesaurus-based similarity analysis (CL -TSA) models. The first ap proach is \ncalled MLPlag [8], where the authors used EuroWordNet ontology [9] that changes words into language -\nindependent forms. They also presented two measures of similarity: Symmetric Similarity Measures \n(MLPlag SYM) which is derived in part from the traditional vector space model (VSM), the second measure \nis Asymmetric Similarity Measure (MLPlag ASYM) which is the opposite of the previous measure. other \nsimilar method employs a multilingual semantic graph t o construct knowledge graphs that represent the \ncontext of documents [10]. \nOther cross language similarity analysis is the cross -language explicit semantic analysis (CL -ESA) \n[11]. It is built on the classic explicit semantic analysis (ESA) model. This approach models the semantic \nInt J Artif Intell  ISSN: 2252-8938 ï² \n \nGraph transformer for cross-lingual plagiarism detection (Oumaima Hourrane) \n907 \nmeaning of a text by an embedding based on the vocabulary retrieved from Wikipedia, to find a document \nwithin a multilingual corpus. \nOne of the obvious w ays to analyze the cross -language plagiarism is the Translation + Monolingual \nAnalysis (T+MA). For example, in [12], the system is simply divided into two components. The MT system \nthat translates suspicious documents into English, they employ the transformer framework for the MT. The \nsecond component is the source retrieval it receives as inputs to the translated suspicious documentâ€™s n -grams \nand returns documents ids from the reference English collection. Finally, the system performs the comparison \nbetween translated suspicious documen ts and the sources. In other approach [13], OpenNMT Library [14] is \nused to train an MT model as a n additional requirement to estimate the pairwise similarity between \nsentences. For the last model, they fine -tuned the Bidirectional Encoder Representations from Transformers \n(BERT) Multilingual model [15] for the sentence pair classification and put the linear layer for on top of the \npooled output of BERT. \nIn recent years, more approaches based on word embedding have been proposed for the cross -\nlingual semantic similarity. Lo and Simard  [16] uses BERT with a similarity metric for cross -lingual \nsemantic textual similarity. The metric is based on a unified adequacy -oriented Machine Translation quality \nevaluation and estimation metric for multi -languages. Another approach [17] uses word embeddings for \ncross-lingual textual similarity detection instead of lexical dictionaries. They present syntactic weighting in \nthe sentence embedding. By using the Multivac toolkit that includes word2vec, paragraph vector and \nbilingual distributed representation features, and t hen assigning weights to the Part Of Speech tag of each \nword in the sentence. Asghari et al.  [18] used Continuous Bag of Words Model (CBOW) and skip -gram \nmodels, and employed an averaging approach to combine word embedding to create of sentenc e embeddings, \nwhich are then compared using Cosine Similarity metric between source and suspect documents. Finally, an \napproach called Language -Agnostic Sentence Representation (LASER) [13] provides a Bidirectional Long \nshort-term memory (BiLSTM) encoder which was trained on 93 languages, so they obtain sentence \nembeddings from the encoder via max -pooling of the last layer outputs and applying cosine similarity on \ncorresponding sentence embeddings of each sentence pair. \n \n \n3. METHODOLOGY \n3.1.  Preliminaries \n3.1.1. Transformer \nTransformer [2] is a neural network mod el primarily employed for neural Machine Translation \nsystems. It uses a self -attention mechanism for creating both the encoder and the decoder [19]â€“[21], that \ndirectly represents relationships between words in a sentence, regardless of their particular position. The \nencoder consists of multiple identical layers and sub -layers. The first layer is a multi -head self -attention \nmechanism, an d the second layer is a position -wise fully connected feed -forward network. The multi -head \nattention puts together multiple dot -product attention layers that supports parallel running. Each dot -product \nattention layer takes a set of queries, keys, values ( q, k, v) as inputs. After that, it calculates the dot products \nof the query with all keys and applies a Softmax function to get the weights on the values. By stacking the set \nof (q, k, v)s into matrices (Q, K, V), it accepts highly optimized matrix multipl ications. More precisely, the \noutputs can be structured as a matrix: \n \nğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„, ğ¾, ğ‘‰ )  =  ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘„ğ¾ğ‘‡ /âˆšğ‘‘)ğ‘‰  \n \nWhere d is the dimension of k and q. By arranging k attention layers into the multi -head attention, the outputs \nof all a ttention heads are combined and projected to the original dimension of x, followed by feed -forward \nlayers, residual connection, and layer normalization, The output matrix can be written as:  \n \nğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–ğ»ğ‘’ğ‘ğ‘‘(ğ‘„, ğ¾, ğ‘‰)  =  ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(â„ğ‘’ğ‘ğ‘‘1, . . . , â„ğ‘’ğ‘ğ‘‘ğ‘˜ )ğ‘Šğ‘œ ,  \n \n                             â„ğ‘’ğ‘ğ‘‘ =  ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›( ğ‘„ğ‘Šğ‘–\nğ‘„, ğ¾ğ‘Šğ‘–\nğ¾,   ğ‘‰ğ‘Šğ‘–\nğ‘‰)  \n \nWhere ğ‘Šğ‘–\nğ‘„, ğ‘Šğ‘–\nğ¾, ğ‘Šğ‘–\nğ‘‰, are the projection matrices of head i. \nTo clarify the whole procedure, we denote the mechanism presented previously as a single function \ndenoted as ğ‘†ğ‘ğ‘¡ğ‘¡(x, y1:m) . Given an input sentence  ğ‘¥1:ğ‘›, the self -attention encoder iteratively calculates the \nsentence representation by: ğ‘¥ğ‘–\nğ¿ =  ğ‘†ğ‘ğ‘¡ğ‘¡(ğ‘¥ğ‘–\nğ¿, x1:ğ‘›\nğ¿âˆ’1).  Where L is the number of layers and ğ‘¥1:ğ‘›\n0  is word \nembeddings. Thus, this representation can build a direct relation with other long -distance representations. In \norder to preserve the sequential order of words, the position encoding technique is proposed [2] to show the \n      ï²          ISSN: 2252-8938 \nInt J Artif Intell, Vol. 11, No. 3, September 2022: 905-915 \n908 \nposition order to the model. Therefore, the input representation will be the concatenation of word embedding \nand position encoding. \n \n3.1.2. Graph neural networks (GNNs) \nGraph neural networks have gained attention in different domains, such as know ledge graphs, social \nnetworks, citation networks, and drug discovery. Graph neural networks build representations of entities and \nedges in graph data. Their key process lies in message -passing process between the entities, where each node \ngathers features from its neighbors to update its representation of the local graph structure around it. The \nmessage passing operation iteratively updates the hidden features Hv of a node v, by concatenating the \nhidden states of vâ€™s neighboring entities and edges. In each layer, the following equation is applied: \n \nhv\nk =  Ïƒ (Wkâˆ‘\nhukâˆ’1\n|N(v)| + Bkhv\nkâˆ’1)  Where: k =  1, â€¦ , k âˆ’ 1  \n \nThe first part bellow of the equation is averaging all the neighbors of node v: Wkâˆ‘\nhukâˆ’1\n| N(v)| , while the \nsecond part is the embedding layer of node v multiplied by a bi as Bk that is a trainable weight matrix \ngenerally, this part is called a self -loop activation for node v and can be described as follow: Bkhv\nkâˆ’1. Then \nthe non -linearity activation such as sigmoid function is performed on the two parts. After L operations o f \nmessage-passing, the hidden states of the last layer K are used as the embeddings of the entities, and can be \ndescribed as zv =  hv\nK. \n \n3.1.3. Knowledge graph \nA knowledge graph is a graph relating entities and concepts and can assist a machine to learn human \ncommon-sense. The core of our approach is to use a graph representation that allows an alignment across \nlanguages. To build knowledge graphs for this purpos e we employ Extended Open Multilingual Wordnet \n[22] which offers a wider set of concepts in several languages to date We will  present this semantic network \nin the next paragraph, then in Section 3.2.2, we introduce the steps needed to obtain our multilingual \nknowledge graphs of documents. \n \nWordNet. Wordnet is a wide electronic lexical database for English [23], [24], with a hierarchical formation \nof concepts, where more specific concepts derive information from their neighbors, more general concepts. \nNouns, verbs, adjectives, and adverbs are clustered into sets of synonyms denoted as synsets, each \nrepresenting a discrete concept. Synsets are interrelated using conceptual lexical and semantic relations. \nSecondly, WordNet labels the semantic relations among words, whereas the groupings of words in a \ndictionary do not follow any specif ic pattern other than meaning similarity. As mentioned before, we use \nExtended Multilingual Wordnet with large wordnets over 26 languages and smaller ones for 57 languages. It \nis made by combining wordnets with open data from Wiktionary, and the Unicode Co mmon Locale Data \nRepository. \n \n3.2.  Model architecture \n3.2.1. Creating knowledge graphs \nIn this section, we present the steps to create the Knowledge Graphs. We build the knowledge graph \nby searching WordNet for paths connecting pairs of synsets in V. At f irst, we preprocess the text segment \nusing tokenization, multi-word extraction, lemmatization, part-of-speech tagging (POS), and to obtain the list \nof tuples (lemma, tag). Next, we create an initially empty knowledge graph G =  (V, E), such that V =  E =\nâˆ…. We populate the vertex set V with the set of all the synsets in WordNet which contain any <lemma, tag> \ntuple T in the text segment language L. \nFinally, for each pair {v, vâ€™} âˆˆ  V such that v and vâ€™ do not share any lexicalization in T, foreach path \nin WordNet  ğ‘£ â†’ ğ‘£1  â†’  . . . â†’   ğ‘£ğ‘›  â†’  ğ‘£â€², we set: ğ‘‰ âˆ¶= ğ‘‰ â‹ƒ  {ğ‘£1, . . . , ğ‘£ğ‘›} and ğ¸ âˆ¶=  ğ¸ â‹ƒ{(ğ‘£, ğ‘£1), . . . , (ğ‘£ğ‘›, ğ‘£â€™)}. \nConsequently, we put all the path nodes and edges to graph G. The length of each path is limited to a \nmaximum of three [25]. Finally, we obtain a knowledge graph that represents the semantic context of the text \nby populating the graph with intermediate relations and nodes. \n \n3.2.2. Knowledge graph notation \nWe denote E and R as the set of entities and relations respectively. A triple is defined as (h, r, t) , \nwhere â„ âˆˆ ğ¸ is the head, ğ‘Ÿ âˆˆ ğ‘…  is the relation, and ğ‘¡ âˆˆ ğ¸ is the tail of the triple. Let ğ‘¥ğ‘– represent the set of \nall triples that are true in a world, and ğ‘¥ğ‘– â€™ represent the false ones. A knowledge graph is a subset of ğ‘¥ğ‘– . \n \nInt J Artif Intell  ISSN: 2252-8938 ï² \n \nGraph transformer for cross-lingual plagiarism detection (Oumaima Hourrane) \n909 \n3.2.3. Graph-transformer \nAfter creating graph knowledge, the next step consists of representing the graphs by weighting all \nconcepts (entities) and semantic relations. Current graph neural networks calculate the node representation \nusing a function of the input node and all its the recep tive field of adjacent neighborhoods, which leads to \ninefficient long -distance information exchange. Therefore, we propose a new mechanism as shown in  \nFigure 1, and known as Graph Transformer which enables relation aware global communication. \n \n \n \n \nFigure 1. Graph-transformer architecture \n \n \nNode representation. The most essential characteristic of this model is that it has a fully connected interface \nof random input graphs. |Each node can directly send and get information to anoth er node no matter if they \nare directly connected or not. This is achieved by the relation -enhanced global attention setting. In short, the \nrelation between any node pair is presented as the shortest relation path between them. These paths between \nthe two entities are used then as an input to relation encoding process, we denote the resulting learned vector \nas rij: the relation between node i and j. As the vanilla multi -head attention, we compute our attention as \nfollow:[ğ‘Ÿğ‘–â†’ğ‘— ; ğ‘Ÿğ‘—â†’ğ‘–] =  ğ‘Šğ‘Ÿğ‘Ÿğ‘–ğ‘— , where we divide the relation encoding into forward encoding ri>ğ‘— and backward \nencoding encoding  rj>ğ‘–. The node vectors are initiated by the sum of the node embedding and position \nencoding. Multiple layers of the global attention are then combined to calculate t he final node representation. \nFor each layer, a node vector is updated based on all other node vectors and the corresponding relation \nencodings. \n \nRelation encoding. In this work, to represent the relationship between two nodes we used the shortest path \napproach, because it usually offers the nearest and most crucial relationship between them. Based on the \nsequential characteristic of this relationship, we used a bi -directional Gated Recurrent Unit (GRU) [26] to get \na probabilistic representation of it. We denote the shortest path between node i and node j as  pij: \n \nğ‘Ÿğ‘–>ğ‘— = ğºğ‘…ğ‘ˆğ‘“(ğ‘Ÿğ‘–>ğ‘—âˆ’1, ğ‘ğ‘–ğ‘— )  \n \nğ‘Ÿğ‘—>ğ‘– = ğºğ‘…ğ‘ˆğ‘“(ğ‘Ÿğ‘—>ğ‘–âˆ’1, ğ‘ğ‘–ğ‘— )  \n \nThe final relation encoding is expressed as [ğ‘Ÿğ‘–>ğ‘— . ğ‘Ÿğ‘—>ğ‘–]which is the addition of the last hidden states of \nforward and backward GRUs. \n\n      ï²          ISSN: 2252-8938 \nInt J Artif Intell, Vol. 11, No. 3, September 2022: 905-915 \n910 \nSequence decoder. After the graph encoding, we learn a mapping between two graphs: G â†’ Gâ€² ,  whereG =\n (node1, . . . , noden). This mapping is learned both in semi -supervised and unsupervised training \nmechanisms. We use the encoder -decoder mechanism to map the node vectors into low dimensional space. \nThe encoder learns the node representation of the input sentence and the decod er employs this representation \nto rebuild in reverse order the sentence. The sequence decoder reflects the same process as the transformer \ndecoder. We update the hidden state at each time step by computing a multi -head attention mechanism over \nthe output o f the encoder and the previously generated words. Finally, we minimize the error between input \nsentences and reconstructed output-sentence during the training as follow: Erec =  â€–s âˆ’ Ë†sâ€–2. \n \n3.2.4. Cross-language plagiarism detection framework \nWe explain in this  section in detail the framework for cross -lingual plagiarism detection. It is originally \nproposed by [10] as well as the post -processing analysis of similarities between text segments. As shown in \nFigure 2. Given a sour ce document ğ‘‘ğ¿ in a language L and a suspicious document ğ‘‘ğ¿0  in a language L0, we \nprocess documents in a four main step: \n(i) Text segmentation. We first segment the documents to be compared, to obtain the sets of segments ğ‘†ğ¿ \nand ğ‘†ğ¿0  by using a sliding window of five sentences with a two sentences step to produce the segments.  \n(ii) Creating knowledge graphs.  Next, we implement the procedure presen ted in Section 3.1 to create the \ngraph sets G and ğº0of the text segments ğ‘†ğ¿and ğ‘†ğ¿0 . \n(iii) Graph representation. It is the global graph representation as presented in Section 3.3. \n(iv) Knowledge graphs similarity. Find K nearest vectors by cosine similarity from source documents. \n(v) Post-processing analysis of similarities. After obtaining the set of the similarities between the text \nchunks of the source document ğ‘‘ğ¿ and suspected documentsğ·ğ¿0, we use the method proposed by [27] to \nanalyze the similarity scores and identify which segments of the suspected document are cases of \nplagiarism. Briefly, for each text chunk of  ğ‘‘ğ¿, we get the top five most similar chunks of document   ğ‘‘ğ¿. \nThen, iteratively we run the algorithm until convergence that aggregates the segments of ğ‘ƒğº  with a \ndistance Î´ lower than a threshold ğ‘¡â„ğ‘Ÿğ‘’ğ‘ 1. At last, we select as plagiarism the cases which combine more \nthan ğ‘¡â„ğ‘Ÿğ‘’ğ‘ 2 text segments. A function offsets of fers the start and end offsets of the plagiarism case. We \nuse this algorithm to evaluate all the models compared in the evaluation section.  \n \n \n \n \nFigure 2. Cross-language plagiarism detection framework \n\nInt J Artif Intell  ISSN: 2252-8938 ï² \n \nGraph transformer for cross-lingual plagiarism detection (Oumaima Hourrane) \n911 \n4. EXPERIMENTS \nWe evaluate and compare our CL -GTA for plagiarism detection model with several state -of-the-art \napproaches in the task of cross -lingual plagiarism analysis. Given a collection of source documents DL0 in a \nlanguage L0 and a suspect document dL in a language  L, we would like to identify all the plagiarized \nsegments of dL from the source documents DL0. We used as an evaluation metric the scores: precision, \nrecall, granularity, and plagdet [28]. \n \n4.1.  Evaluation metrics \nWe used as an evaluation metric the scores: precision, recall, granularity, and plagdet [28]. We \ndenote S as the set of plagiarism cases in the suspect documents and R as the set of plagiarized sequences. \nThe characters for a plagiarized case are denoted as s âˆˆ S. Likewise, the characters for a plagiarized text are \nrepresented as r âˆˆ R. Following these notations, and we measure the precision and the recall at the character \nlevel of R under S as follow: \n- The precision represents the fraction of fragments found which cases of plagiarism are really. It \nmeasures the number of characters correctly returned as plagiarized on the total number of characters \nreturned. Theprecision can be expressed as follow: ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›(ğ‘†, ğ‘…) =  \n1\n|ğ‘…| âˆ‘\n|â‹ƒ ğ‘ âˆ©ğ‘Ÿğ‘ âˆˆğ‘† |\n|ğ‘Ÿ|ğ‘Ÿâˆˆğ‘… . \n- The recall represents the fraction of plagiarized text that has been found. It measures the nu mber of \ncharacters correctly returned as plagiarized over the number total number of characters to be returned as \nplagiarized. The recall can be expressed as follow:ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™(ğ‘†, ğ‘…) =\n1\n|ğ‘…ğ‘†| âˆ‘\n|â‹ƒ ğ‘ âˆ©ğ‘Ÿğ‘Ÿâˆˆğ‘… |\n|ğ‘ |ğ‘ âˆˆğ‘† . \n- The ğ‘­ğ’ğ’‚ğ’„ğ’“ğ’ is an F-measure macro which takes into account the size of the plagiarized passages instead \nof only considering the absolute number of plagiarized passages. This is the harmonic mean between \nprecision and recall. It can be expressed as follow:ğ¹ğ‘šğ‘ğ‘ğ‘Ÿğ‘œ =  \n2.ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›(ğ‘†,ğ‘…).ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™(ğ‘†,ğ‘…)\nğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›(ğ‘†,ğ‘…)+ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™(ğ‘†,ğ‘…)  \nThere is an issue that plagiarism detectors sometimes report overlapping or multiple detections for a \nsingle plagiarism case, and precision and recall do not perform  well for that. We approach this problem by \nmeasuring the detectorâ€™s granularity. \n- The granularity is a measure first introduced in the work of Potthast et al. [28]. It determines whether a \nfragment is detected in whole or in pieces. This measure penalizes cases where passages, which are \nfound, plagiarized, overlap. The granularity can be expressed as follow: ğºğ‘Ÿğ‘ğ‘›ğ‘¢ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦ (ğ‘†, ğ‘…) =  \n1\n|ğ‘†ğ‘…| , \nwhere ğ‘†ğ‘… âˆˆ ğ‘† are cases identified by detectors in R, and ğ‘…ğ‘† âˆˆ ğ‘… are detections of S. \n- Plagdet (plagiarism detection)  is a measure combining precision and recall oriented for plagiarism \ndetection and granularity. Plagdet is expressed as follow: ğ‘ƒğ‘™ğ‘ğ‘”ğ‘‘ğ‘’ğ‘¡(ğ‘†, ğ‘…) =  \nğ¹ğ‘šğ‘ğ‘ğ‘Ÿğ‘œ\nlog2(1 + ğ‘”ğ‘Ÿğ‘ğ‘›ğ‘¢ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦(ğ‘†,ğ‘…)). \n \n4.2.  Construction and properties of the corpus \nIn this work, our dataset consists of English, French, and Spanish documents. We decide to reuse \nalready existing collections of parallel and comparable corpora in order to constitute a base for our corpus. \nThese datasets are presented as follow: \n- Europarl [29] is a corp us for cross -language and MT research. This corpus embodies about 10,000 \nparallel documents of the European Parliament exchanging transcriptions, in French, English and Spanish \nlanguages. \n- JRC-Acquis [30] is usually used in cross -language and MT tasks. This corpus represents extracts  of \nAcquis Communautaire. It consists of 10,000 parallel documents, available in French, English, and \nSpanish languages. \n- Wikipedia is usually used as a parallel corpus of multiple languages. We chose to use 10,000 of French, \nEnglish, and Spanish aligned documents. In total, this corpus contains 30,000 documents. \n- PAN 2011 corpus has been used for the cross -lingual plagiarism detection competition of PAN at CLEF \n[27]. This corpus con tains portions of writings of similar books in multiple languages. These texts are \nfrom books freely accessible on the Gutenberg Project website, available as Spanish â€“English (ES â€“EN) \npairs. \n- Conference papers.  We used the processed conference papers corpus [31]. These are English â€“French \nconference papers that were first pub lished in one language and then translated by their authors to be \npublished in other language. A sum of 35 pairs of Englishâ€“French conference papers was retrieved. \n- Englishâ€“Arabic parallel corpora.  The corpus of the Arabic -English case was taken from differ ent \nparallel corpora. It consists of 547 aligned passages from 58,911 pairs from the United Nations Parallel \nCorpora [32], the OPUS collection of translated texts from the web [33] and King Saud University corpus \n[34]. We used another corpus prepared by [35] which has roughly 2085 of paraphrased translated pairs \nwhich will be used when evaluating only paraphrasing cases. \n      ï²          ISSN: 2252-8938 \nInt J Artif Intell, Vol. 11, No. 3, September 2022: 905-915 \n912 \n4.3.  Evaluation protocol \nTo precisely evaluate the methods of detecting cross -lingual plagiarism on our corpus, we present as \nfollow, a dedicated evaluation protocol. We denote a parallel or comparable corpus as C, which is made up of \nN pairs of documents, such that for each document ğ‘‘ğ‘– âˆˆ  ğ· a corresponding document ğ‘‘0ğ‘– âˆˆ  ğ·0 exists, where \ni is an integer between 1 and N, we decide to compare all the N documents available in D to M documents \nof ğ·0. This has the advantage of avoiding m aking ğ‘2comparisons and thus having much too long \ncomputation times in the case of too large corpora. Each document ğ‘‘ğ‘– is compared to its corresponding \ndocument ğ‘‘ğ‘– and to M-1 other documents randomly selected with replacement in ğ·0. The ğ‘‘0 document can \nthus be set more than once. M is set at 1,000 documents agreeing with the state of the art [36]. \nThe graph encoder and sequence decoder use randomly initialized node and word embeddings \nrespectively. To prevent overfitting, we apply dropout with the drop rate of 0.2 [34]. We apply a special UNK \ntoken to replace with a rate of 0.33 the input nodes. We used Adam optimizer for parameter optimization  \nwith ğ‘ğ‘’ğ‘¡ğ‘1 = 0.9  and ğ‘ğ‘’ğ‘¡ğ‘1 = 0.999 [37]. We adopt the same learning rate of a standard transformer [2], \nand then we encode all shortest paths into vector representation by the relation encoding. \nFor a fair evaluation, we compared our CL -GTA model with the state -of-the-art Cross -Language \nCharacter N-Gram CL-C3G [3], Cross-Language Alignment-based SimilarityAnalysis CL -ASA [38], Cross-\nLanguage Explicit Semantic Analysis CL -ESA [11], and Cross -Language Knowledge Graph Analysis (CL -\nKGA) in [39] models. We also used the length model of [40] as a baseline. \n \n \n5. RESULTS AND DISCUSSION \nThe results of our experiments were broken down in two folds: (i) we compared our model with the \nstate-of-the-art approaches, assessing the performance when detecting the cross -lingual plagiarism cases of \nour corpora ES -EN, FR -EN, AR -EN; (ii) we examined the performance on solely the cross -lingual \nparaphrasing cases of plagiarism for the Spanishâ€“English and Arabicâ€“English partitions. \n \n5.1.  Comparison with the state of the art \n5.1.1. Results \nTable 1 shows the results obtained for Spanish â€“English (ES -EN), French â€“English (FR -EN), and \nArabicâ€“English (AR-EN) partition, For the Spanish â€“English (ES-EN). Partition, the CL -GTA approach has \nthe best Plagdet score of 0.62, followed by the l ength Model with a score of 0.604, then the Cross -Language \nConceptual Thesaurus -base Similarity CL -CTS model with a score of 0.584. The difference between the \nscores of all the other approaches is not huge, except for the CL -C3G with a score of 0.169. This  is far lower \nthan the State -of-the-art CL-GTA. For or the French â€“English (FR-EN), same as the Spanish â€“English cases, \nthe CL-GTA reaches the best Plagdet score of 0.584, followed by the Length Model with a score of 0.553, \nthen the Cl -CTS with a score of 0. 584. For the Arabic â€“English partition, the results prove a different \noutcome, with the CL -CTS in the first place with a Plagdet score of 0.534 followed by our model CL -GTA \nwith a score of 0.522. \n \n \nTable 1. Results of comparison with the state of the art \n Method Length Model CL-C3G CL-ASA CL-ESA CL-CTS CL-GTA \n(1). Spanishâ€“English Plagdet 0.075 0.018 0.056 0.070 0.092 0.112 \nPrecision 0.149 0.048 0.153 0.160 0.186 0.203 \nRecall 0.058 0.020 0.041 0.019 0.063 0.085 \nGranularity 1.000 1.000 1.001 1.061 1.000 1.000 \n(2). Frenchâ€“English Plagdet 0.553 0.065 0.405 0.395 0.504 0.584 \nPrecision 0.469 0.067 0.343 0.300 0.452 0.506 \nRecall 0.683 0.306 0.029 0.356 0.633 0.690 \nGranularity 1.007 1.099 1.103 1.112 1.017 1.000 \n(3). Arabicâ€“English Plagdet 0.520 0.192 0.690 0.303 0.534 0.522 \nPrecision 0.401 0.203 0.465 0.278 0.452 0.409 \nRecall 0.598 0.025 0.758 0.423 0.633 0.576 \nGranularity 1.010 1.082 1.203 1.105 1.007 1.101 \n \n \n5.1.2. Discussion \nThe results for French â€“English compared to Spanish â€“English were similar but with reduced \nperformance. Spanish, French, and English do not share many grammatical characteristics. For all partitions, \nthe CL-C3G got the lowest result, since syntactic and lexi cal features are important for high character n -gram \noverlap. After that comes CL-ESA, since it is based on computing similarities with document collections, the \nInt J Artif Intell  ISSN: 2252-8938 ï² \n \nGraph transformer for cross-lingual plagiarism detection (Oumaima Hourrane) \n913 \nmodel obtained a higher number of false positives. The CL-ASA is comparable with CL-ESA but with higher \nprecision. The third best in ranking is the CL -CTS, which is also based on knowledge graphs, but with \nclassical weighting for nodes. The length model offered higher performance compared to the state -of-the-art. \nHowever, our CL-GTA model obtained the best results, suggesting that the proposed model benefits from the \nexplicit relation encoding which provides a more efficient way for global graph representation, leading to \nbetter results to measure cross-lingual similarity. \nRegarding the Arabicâ€“English case, we got a slightly different outcome with the CL-CTS better than \nour model CL -GTA, since the two approaches are based on graphs, the main difference is that the CL -CTS \nused different Knowledge graph sources and different techniques of graph represe ntation. In this case, the \nclassical weighting proves Superior for the Arabic language. However, by changing the weights and using a \nfar richer Knowledge graphs construction, the CL-GTA can prove to be effective prospectively. \n \n5.2.  Cross-language plagiarism detection with paraphrasing \n5.2.1. Results \nTable 2 shows the results of the paraphrasing cases evaluation on the PAN competition (PAN -PC-\n11) corpus. Our model CL -GTA reaches the state-of-the-at on this task by a Plagdet score of 0.112, followed \nby the CL-CTS model with a score of 0.092; then the Length Model in the third place with a score of 0.075. \nWe report as well the results of the paraphrasing cases of the Arabic â€“English paraphrased translated pairs. \nSame as the previous results, our method CL -GTA proved superior with a Plagdet score of 0.108, followed \nwith the Length Model with a score of 0.105, then the CL -CTS model with a score of 0.099. The difference \nbetween the scores of all the other approaches is not big for both datasets, except for the CL -C3G with a \nscore of 0.021. This is far again lower than the State-of-the-art CL-GTA. \n \n \nTable 2. Results of the cross-language plagiarism detection with paraphrasing \n Method Length Model CL-C3G CL-ASA CL-ESA CL-CTS CL-GTA \n(1). Spanishâ€“English Plagdet 0.075 0.018 0.056 0.070 0.092 0.112 \n Precision 0.149 0.048 0.153 0.160 0.186 0.203 \n Recall 0.058 0.020 0.041 0.019 0.063 0.085 \n Granularity 1.000 1.000 1.001 1.061 1.000 1.000 \n(2). Frenchâ€“English Plagdet 0.553 0.065 0.405 0.395 0.504 0.584 \n Precision 0.469 0.067 0.343 0.300 0.452 0.506 \n Recall 0.683 0.306 0.029 0.356 0.633 0.690 \n Granularity 1.007 1.099 1.103 1.112 1.017 1.000 \n(3). Arabicâ€“English Plagdet 0.520 0.192 0.690 0.303 0.534 0.522 \n Precision 0.401 0.203 0.465 0.278 0.452 0.409 \n Recall 0.598 0.025 0.758 0.423 0.633 0.576 \n Granularity 1.010 1.082 1.203 1.105 1.007 1.101 \n \n \n5.2.2. Discussion \nAs mentioned before, the PAN -PC-11 dataset contains cross -lingual paraphrasing cases of \nplagiarism, which is a more complex form of plagiarism to detect since it restates the text using other terms \nin order to conceal plagiarism. We conducted hereby a further experiment to examine only paraphrasing \ncases of plagiarism extracted from the corpus. We observe that the differences between the results of the \nmodels were identical to the previous results using the entire dataset at a smaller scale. CL -GTA obtained \nhigher performance compared to the other baselines. \nWe conducted another experiment on the Arabic â€“English para phrased translated partition. In \ncontrary to the previous results on literal plagiarism case, our model overcame the CL -CTS model with a \nscore of 0.108, which proves that in terms of semantic similarity, is better represented with our graph \ntransformer architecture. This result is true even when representing the linear text sequence and this is due to \nthe attention mechanism of the transformer, which allows a model to focus on the most relevant parts of the \ngraph, thus representing the global graph dependen cies in an efficient way. This is the main goal of this \npaper. \n \n \n6. CONCLUSION \nTo conclude, in this paper, we have introduced a new approach for detecting cross -lingual semantic \ntextual similarities based on knowledge graph representations and we have also augmented a state -of-the-art \nmethod by introducing these representations. We r eferred to our method as CL-GTA. We then introduced the \nnotion of graph transformer, which is a new graph representation method based on the transformer \narchitecture that employed explicit relation encoding and offers a more efficient way to represent glob al \n      ï²          ISSN: 2252-8938 \nInt J Artif Intell, Vol. 11, No. 3, September 2022: 905-915 \n914 \ngraph dependencies. To build knowledge graphs, we used extended open multilingual wordnet since it \nprovides a wide set of concepts and languages to date. We then constructed a knowledge graph that \nrepresents the semantic context of the text segment, by creating the graph with intermediate edges and \nvertices. The next step was to represent the graphs by weighting all concepts (entities) and semantic relations, \nby using our graph transformer based on the attention mechanism. The mappings between the graphs  are \nlearned both in semi -supervised and unsupervised training mechanisms. After the graph representation step, \nwe explain more in detail the framework for cross -lingual plagiarism detection as well as the post -processing \nanalysis of similarities between t ext segments. To measure the efficiency of our methods, we compare our \nCL-GTA for plagiarism detection model with multiple states -of-the- art approaches in the task of cross -\nlingual plagiarism detection. We used as evaluation metrics the scores: precision,  recall, granularity, and \nplagdet. The experimental results show that the use of the graph transformer mechanism provided our model \nwith state- of-the-art performance on the Spanish â€“English, French â€“English, and Arabic â€“English pairs. The \nexperiments also demonstrated its advantage with cross-language paraphrasing cases for the Spanish â€“English \nand Arabicâ€“English pairs. For future work, we will further improve the model to reaches the state -of-the-art \non the Arabicâ€“English literal translated cases, we will as expand the experiment to cover more languages and \ncontinue exploring the use of our proposed graph transformer and multilingual Knowledge graphs for other \ncross-lingual similarity tasks such as multilingual text classification and cross-lingual information retrieval. \n \n \nREFERENCES \n[1] K. Leilei, Q. Haoliang, W. Shuai, D. Cuixia, W. Suhong, and H. Yong, â€œApproaches for candidate document retrieval and \ndetailed comparison of plagiarism detection,â€ 2012. \n[2] A. Vaswani et al., â€œAttention is all you need,â€ 2017. \n[3] P. McNamee and J. Mayfield, â€œCharacter N -Gram Tokenization for European Language Text Retrieval,â€ Information Retrieval, \nvol. 7, no. 1/2, pp. 73â€“97, Jan. 2004, doi: 10.1023/b:inrt.0000009441.78971.be. \n[4] P. Clough, â€œOld and new challenges in automatic plagiarism detection,â€ National Plagiarism Advisory Service, 2003. \n[5] H. A. Maurer, F. Kappe, and B. Zaka, â€œPlagiarism -a survey,â€ Journal of Universal Computer Science , vol. 12, no. 8,  \npp. 1050â€“1084, 2006. \n[6] A. Barron-Cedeno, P. Rosso, D. Pinto, and A. Juan, â€œOn cross-lingual plagiarism analysis using a statistical model,â€ 2008. \n[7] D. Pinto, J. Civera, A. BarrÃ³n -CedeÃ±o, A. Juan, and P. Rosso, â€œA statistical approach  to crosslingual natural language tasks,â€ \nJournal of Algorithms, vol. 64, no. 1, pp. 51â€“60, Jan. 2009, doi: 10.1016/j.jalgor.2009.02.005. \n[8] Z. Ceska, M. Toman, and K. Jezek, â€œMultilingual Plagiarism Detection,â€ in Artificial Intelligence: Methodology, Sy stems, and \nApplications, Springer Berlin Heidelberg, pp. 83â€“92. \n[9] C. Jacquin, E. Desmontils, and L. Monceaux, â€œFrench EuroWordNet Lexical Database Improvements,â€ in Computational \nLinguistics and Intelligent Text Processing, Springer Berlin Heidelberg, 2007, pp. 12â€“22. \n[10] M. Franco-Salvador, P. Gupta, and P. Rosso, â€œCross-Language Plagiarism Detection Using a Multilingual Semantic Network,â€ in \nLecture Notes in Computer Science, Springer Berlin Heidelberg, 2013, pp. 710â€“713. \n[11] E. Gabrilovich and S. Mar kovitch, â€œComputing semantic relatedness using wikipedia -based explicit semantic analysis,â€ IJCAI, \nvol. 7, pp. 1606â€“1611, 2007. \n[12] O. Bakhteev, A. Ogaltsov, A. Khazov, K. Safin, and R. Kuznetsova, â€œCrosslang: the system of cross -lingual plagiarism \ndetection,â€ 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), pp. 1â€“9, 2019. \n[13] D. Zubarev and I. Sochenkov, â€œCross -language text alignment for plagiarism detection based on contextual and context -free \nmodels,â€ in Papers from the Annual International Conference â€œDialogue,â€ 2019, pp. 809â€“820. \n[14] G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. Rush, â€œOpenNMT: Open -Source Toolkit for Neural Machine Translation,â€ 2017, \ndoi: 10.18653/v1/p17-4012. \n[15] J. Devlin, C. Ming -Wei, K. Lee, and K. T outanova, â€œ(BERT): Pre -training of deep bidirectional transformers for language \nunderstanding,â€ in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational \nLinguistics: Human Language Technologies, 2018, pp. 4171â€“4186. \n[16] C. Lo and M. Simard, â€œFully Unsupervised Crosslingual Semantic Textual Similarity Metric Based on BERT for Identifying \nParallel Data,â€ in Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL) , 2019,  \npp. 206â€“215, doi: 10.18653/v1/k19-1020. \n[17] J. Ferrero, L. Besacier, D. Schwab, and F. AgnÃ¨s, â€œUsing Word Embedding for Cross -Language Plagiarism Detection,â€ 2017,  \ndoi: 10.18653/v1/e17-2066. \n[18] H. Asghari, O. Fatemi, S. Mohtaj, H. Faili, and P. Rosso, â€œOn the use of word embedding for cross language plagiarism \ndetection,â€ Intelligent Data Analysis, vol. 23, no. 3, pp. 661â€“680, Apr. 2019, doi: 10.3233/ida-183985. \n[19] J. Cheng, L. Dong, and M. Lap ata, â€œLong Short -Term Memory-Networks for Machine Reading,â€ 2016, doi: 10.18653/v1/d16 -\n1053. \n[20] W. KryÅ›ciÅ„ski, R. Paulus, C. Xiong, and R. Socher, â€œImproving Abstraction in Text Summarization,â€ 2018,  \ndoi: 10.18653/v1/d18-1207. \n[21] H. Chen et al. , â€œLow -Dose CT With a Residual Encoder -Decoder Convolutional Neural Network,â€ IEEE Transactions on \nMedical Imaging, vol. 36, no. 12, pp. 2524â€“2535, Dec. 2017, doi: 10.1109/tmi.2017.2715284. \n[22] F. Bond and R. Foster, â€œLinking and extending an open multilingual w ordnet,â€ in Proceedings of the 51st Annual Meeting of the \nAssociation for Computational Linguistics, 2013, pp. 1352â€“1362. \n[23] G. A. Miller, â€œWordnet: a lexical database for english,â€ Communications of the ACM, vol. 38, no. 11, pp. 39â€“41, 1995. \n[24] C. Fellbaum, â€œA Semantic Network of English: The Mother of All WordNets,â€ in EuroWordNet: A multilingual database with \nlexical semantic networks, Springer Netherlands, 1998, pp. 137â€“148. \n[25] R. Navigli and S. P. Ponzetto, â€œMultilingual wsd with just a few  lines of code: the babelnet api,â€ in Proceedings of the 50th \nAnnual Meeting of the Association for Computational Linguistics, 2012, pp. 67â€“72. \n \nInt J Artif Intell  ISSN: 2252-8938 ï² \n \nGraph transformer for cross-lingual plagiarism detection (Oumaima Hourrane) \n915 \n[26] K. Cho et al. , â€œLearning Phrase Representations using RNN Encoder -Decoder for Statistical Machine Translat ion,â€ 2014,  \ndoi: 10.3115/v1/d14-1179. \n[27] P. Gupta, A. BarrÃ³n -CedeÃ±o, and P. Rosso, â€œCross -Language High Similarity Search Using a Conceptual Thesaurus,â€ in \nInformation Access Evaluation. Multilinguality, Multimodality, and Visual Analytics , Springer Ber lin Heidelberg, 2012,  \npp. 67â€“75. \n[28] M. Potthast, B. Stein, A. B. Ìon-C. Ìƒno, and P. Rosso, â€œAn evaluation framework for plagiarism detection,â€ in COLING 2010, 23rd \nInternational Conference on Computational Linguistics, 2010, pp. 997â€“1005. \n[29] P. Koehn, â€œEuroparl: A parallel corpus for statistical machine translation,â€ in Proceedings of Machine Translation Summit X: \nPapers, 2005, pp. 79â€“86. \n[30] R. Steinberger et al., â€œThe JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages,â€ 2006. \n[31] J. Ferrero, L. Besacier, D. Schwab, and F. AgnÃ¨s, â€œDeep Investigation of Cross -Language Plagiarism Detection Methods,â€ 2017, \ndoi: 10.18653/v1/w17-2502. \n[32] M. Ziemski, M. Junczys -Dowmunt, and B. Pouliquen, â€œThe united nations parallel corpus v1. 0,â€ Proceedings of the Tenth \nInternational Conference on Language Resources and Evaluation (LRECâ€™16), pp. 3530â€“3534, 2016. \n[33] J. Tiedemann, â€œParallel data, tools and interfaces in opus,â€ Lrec, pp. 2214â€“2218, 2012. \n[34] K. Alotaibi, â€œThe Relationship Between Self -Regulated Learning and Academic Achievement for a Sample of Community \nCollege Students at King Saud University,â€ Education Journal, vol. 6, no. 1, p. 28, 2017, doi: 10.11648/j.edu.20170601.14. \n[35] S. Alzahrani and H. Aljuaid, â€œIdentifying cross-lingual plagiarism using rich semantic features and deep neural networks: A study \non Arabic -English plagiarism cases,â€ Journal of King Saud University - Computer and Information Sciences , vol. 34, no. 4,  \npp. 1110â€“1123, Apr. 2022, doi: 10.1016/j.jksuci.2020.04.009. \n[36] A. BarrÃ³n -CedeÃ±o, M. Potthast, P. Rosso, B. Stein, and A. Eiselt, â€œCorpus and evaluation measures for automatic plagiarism \ndetection,â€ 2010. \n[37] D. P. Kingma and J. L. Ba, â€œAdam: A method for stochastic optimization,â€ 2015. \n[38] A. BarrÂ´on-CedeËœno, P. Rosso, E. Agirre, and G. Labaka, â€œPlagiarism detection across distant language pairs,â€ in Proceedings of \nthe 23rd International Conference on Computational Linguistics (Coling 2010), 2010, pp. 37â€“45. \n[39] M. Franco-Salvador, P. Rosso, and M. Montes -y-GÃ³mez, â€œA systematic study of knowledge graph analysis for cross -language \nplagiarism detection,â€ Information Processing & Management , vol. 52, no. 4, pp. 550 â€“570, Jul. 2016,  \ndoi: 10.1016/j.ipm.2015.12.004. \n[40] B. Pouliquen, R. Steinberger, and C. Ignat, â€œAutomatic identification of document translations in large multilingual document  \ncollections,â€ Proceedings of the International Conference Recent Advances in Natural Language Processing (RANLPâ€™03) ,  \npp. 401â€“408, 2006. \n \n \nBIOGRAPHIES OF AUTHORS \n \n \nOumaima Hourrane     is a PhD candidate in the Department of Mathematics and \nComputer Science at Faculty of Science Ben M'Sik, Hassan II University, Casablanca, \nMorocco. She majored in Computer Engineering in her master study at the National School of \nApplied Science, Safi, Mo rocco in 2016. Her research interests include Machine Learning, \nArtificial Intelligence, Natural Language Processing, and Information Retrieval. She has \npublished multiple research papers in national and international journals as well as conference \nproceedings. She can be contacted by email: oumaima.hourrane@gmail.com. \n  \n \nEl Habib Benlahmar     is a Full Professor of Higher Education in the Department \nof Mathematics and Computer Science at Faculty of Science Ben M'Sik, Hassan II University, \nCasablanca, Morocco since 2008. He received his PhD in Computer Science from the \nNational school For Comput er Science (ENSIAS), Rabat, Morocco, in 2007. His research \ninterests span Web semantic, Natural Language Processing, Information Retrieval, Mobile \nplatforms, and Data Science. He is the author of over 70 research studies published in national \nand internati onal journals, as well as conference proceedings and book chapters. He can be \ncontacted by email: h.benlahmer@gmail.com. \n  \n \n",
  "topic": "Plagiarism detection",
  "concepts": [
    {
      "name": "Plagiarism detection",
      "score": 0.886854887008667
    },
    {
      "name": "Computer science",
      "score": 0.8436152935028076
    },
    {
      "name": "Transformer",
      "score": 0.6704626679420471
    },
    {
      "name": "Natural language processing",
      "score": 0.6366150379180908
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5572215914726257
    },
    {
      "name": "Graph",
      "score": 0.47863712906837463
    },
    {
      "name": "Theoretical computer science",
      "score": 0.30156007409095764
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99297268",
      "name": "University of Hassan II Casablanca",
      "country": "MA"
    }
  ]
}