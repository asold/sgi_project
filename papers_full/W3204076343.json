{
    "title": "CvT: Introducing Convolutions to Vision Transformers",
    "url": "https://openalex.org/W3204076343",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2108023597",
            "name": "Haiping Wu",
            "affiliations": [
                "McGill University"
            ]
        },
        {
            "id": "https://openalex.org/A1909904576",
            "name": "Bin Xiao",
            "affiliations": [
                "Microsoft (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A4212569983",
            "name": "Noel Codella",
            "affiliations": [
                "Microsoft (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2129904144",
            "name": "Mengchen Liu",
            "affiliations": [
                "Microsoft (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2500950449",
            "name": "Dai Xiyang",
            "affiliations": [
                "Microsoft (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2098733522",
            "name": "Lu Yuan",
            "affiliations": [
                "Microsoft (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2082114532",
            "name": "Lei Zhang",
            "affiliations": [
                "Microsoft (Germany)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3035022492",
        "https://openalex.org/W6774054309",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W6785727093",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6757585730",
        "https://openalex.org/W6790434664",
        "https://openalex.org/W6786708909",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W6802372869",
        "https://openalex.org/W3097777922",
        "https://openalex.org/W6790690058",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W6737664043",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W6775298725",
        "https://openalex.org/W6640454917",
        "https://openalex.org/W6762718338",
        "https://openalex.org/W6786585107",
        "https://openalex.org/W6786423403",
        "https://openalex.org/W6778485988",
        "https://openalex.org/W6790375769",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W6728184133",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W6786026227",
        "https://openalex.org/W6779602356",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W1977295328",
        "https://openalex.org/W2533598788",
        "https://openalex.org/W6785665406",
        "https://openalex.org/W6781346214",
        "https://openalex.org/W6763509872",
        "https://openalex.org/W6784094891",
        "https://openalex.org/W6789425149",
        "https://openalex.org/W2963351113",
        "https://openalex.org/W6764990469"
    ],
    "abstract": "We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both designs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (\\ie shift, scale, and distortion invariance) while maintaining the merits of Transformers (\\ie dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (\\eg ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7\\% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely removed in our model, simplifying the design for higher resolution vision tasks. Code will be released at \\url{this https URL}.",
    "full_text": "CvT: Introducing Convolutions to Vision Transformers\nHaiping Wu1,2* Bin Xiao2† Noel Codella2 Mengchen Liu2 Xiyang Dai2\nLu Yuan2 Lei Zhang2\n1McGill University 2Microsoft Cloud + AI\nhaiping.wu2@mail.mcgill.ca, {bixi, ncodella, mengcliu, xidai, luyuan, leizhang}@microsoft.com\nAbstract\nWe present in this paper a new architecture, named Con-\nvolutional vision Transformer (CvT), that improves Vision\nTransformer (ViT) in performance and efﬁciency by intro-\nducing convolutions into ViT to yield the best of both de-\nsigns. This is accomplished through two primary modiﬁca-\ntions: a hierarchy of Transformers containing a new convo-\nlutional token embedding, and a convolutional Transformer\nblock leveraging a convolutional projection. These changes\nintroduce desirable properties of convolutional neural net-\nworks (CNNs) to the ViT architecture ( i.e. shift, scale,\nand distortion invariance) while maintaining the merits of\nTransformers (i.e. dynamic attention, global context, and\nbetter generalization). We validate CvT by conducting ex-\ntensive experiments, showing that this approach achieves\nstate-of-the-art performance over other Vision Transform-\ners and ResNets on ImageNet-1k, with fewer parame-\nters and lower FLOPs. In addition, performance gains\nare maintained when pretrained on larger datasets ( e.g.\nImageNet-22k) and ﬁne-tuned to downstream tasks. Pre-\ntrained on ImageNet-22k, our CvT-W24 obtains a top-1 ac-\ncuracy of 87.7% on the ImageNet-1k val set. Finally, our\nresults show that the positional encoding, a crucial com-\nponent in existing Vision Transformers, can be safely re-\nmoved in our model, simplifying the design for higher res-\nolution vision tasks. Code will be released at https:\n//github.com/leoxiaobin/CvT.\n1. Introduction\nTransformers [31, 10] have recently dominated a wide\nrange of tasks in natural language processing (NLP) [32].\nThe Vision Transformer (ViT) [11] is the ﬁrst computer vi-\nsion model to rely exclusively on the Transformer archi-\ntecture to obtain competitive image classiﬁcation perfor-\nmance at large scale. The ViT design adapts Transformer\n*This work is done when Haiping Wu was an intern at Microsoft.\n†Corresponding author\nCvT ViT BiT\n(a)\n78\n80\n82\n84\n86\n88ImageNet top-1 accuracy (%)\n(a)\n20M\n32M\n277M\n86M\n307M\n25M\n928M\nCvT ViT BiT\n20 40 60 80\nModel Paramters (M)\n(b)\n80.0\n80.5\n81.0\n81.5\n82.0\n82.5\nImageNet top-1 accuracy (%)\nCvT\nDeiT\nT2T\nPVT\nTNT\nFigure 1: Top-1 Accuracy on ImageNet validation com-\npared to other methods with respect to model parame-\nters. (a) Comparison to CNN-based model BiT [18] and\nTransformer-based model ViT [11], when pretrained on\nImageNet-22k. Larger marker size indicates larger archi-\ntectures. (b) Comparison to concurrent works: DeiT [30],\nT2T [41], PVT [34], TNT [14] when pretrained on\nImageNet-1k.\narchitectures [10] from language understanding with mini-\nmal modiﬁcations. First, images are split into discrete non-\noverlapping patches (e.g. 16 ×16). Then, these patches are\ntreated as tokens (analogous to tokens in NLP), summed\nwith a special positional encoding to represent coarse spa-\ntial information, and input into repeated standard Trans-\nformer layers to model global relations for classiﬁcation.\nDespite the success of vision Transformers at large scale,\nthe performance is still below similarly sized convolutional\nneural network (CNN) counterparts ( e.g., ResNets [15])\nwhen trained on smaller amounts of data. One possible rea-\nson may be that ViT lacks certain desirable properties in-\nherently built into the CNN architecture that make CNNs\nuniquely suited to solve vision tasks. For example, im-\nages have a strong 2D local structure: spatially neighbor-\ning pixels are usually highly correlated. The CNN archi-\n1\narXiv:2103.15808v1  [cs.CV]  29 Mar 2021\nMethod Needs Position Encoding (PE) Token Embedding Projection for Attention Hierarchical Transformers\nViT [11], DeiT [30] yes non-overlapping linear no\nCPVT [6] no (w/ PE Generator) non-overlapping linear no\nTNT [14] yes non-overlapping (patch+pixel) linear no\nT2T [41] yes overlapping (concatenate) linear partial (tokenization)\nPVT [34] yes non-overlapping spatial reduction yes\nCvT (ours) no overlapping (convolution) convolution yes\nTable 1: Representative works of vision Transformers.\ntecture forces the capture of this local structure by using\nlocal receptive ﬁelds, shared weights, and spatial subsam-\npling [20], and thus also achieves some degree of shift,\nscale, and distortion invariance. In addition, the hierarchi-\ncal structure of convolutional kernels learns visual patterns\nthat take into account local spatial context at varying levels\nof complexity, from simple low-level edges and textures to\nhigher order semantic patterns.\nIn this paper, we hypothesize that convolutions can be\nstrategically introduced to the ViT structure to improve\nperformance and robustness, while concurrently maintain-\ning a high degree of computational and memory efﬁciency.\nTo verify our hypothesises, we present a new architecture,\ncalled the Convolutional vision Transformer (CvT), which\nincorporates convolutions into the Transformer that is in-\nherently efﬁcient, both in terms of ﬂoating point operations\n(FLOPs) and parameters.\nThe CvT design introduces convolutions to two core sec-\ntions of the ViT architecture. First, we partition the Trans-\nformers into multiple stages that form a hierarchical struc-\nture of Transformers. The beginning of each stage consists\nof a convolutional token embedding that performs an over-\nlapping convolution operation with stride on a 2D-reshaped\ntoken map ( i.e., reshaping ﬂattened token sequences back\nto the spatial grid), followed by layer normalization. This\nallows the model to not only capture local information, but\nalso progressively decrease the sequence length while si-\nmultaneously increasing the dimension of token features\nacross stages, achieving spatial downsampling while con-\ncurrently increasing the number of feature maps, as is per-\nformed in CNNs [20]. Second, the linear projection prior\nto every self-attention block in the Transformer module is\nreplaced with our proposed convolutional projection, which\nemploys a s ×s depth-wise separable convolution [5] oper-\nation on an 2D-reshaped token map. This allows the model\nto further capture local spatial context and reduce seman-\ntic ambiguity in the attention mechanism. It also permits\nmanagement of computational complexity, as the stride of\nconvolution can be used to subsample the key and value ma-\ntrices to improve efﬁciency by 4 ×or more, with minimal\ndegradation of performance.\nIn summary, our proposed Convolutional vision Trans-\nformer (CvT) employs all the beneﬁts of CNNs: local re-\nceptive ﬁelds, shared weights, and spatial subsampling ,\nwhile keeping all the advantages of Transformers: dynamic\nattention, global context fusion, and better generalization .\nOur results demonstrate that this approach attains state-of-\nart performance when CvT is pre-trained with ImageNet-\n1k, while being lightweight and efﬁcient: CvT improves the\nperformance compared to CNN-based models (e.g. ResNet)\nand prior Transformer-based models (e.g. ViT, DeiT) while\nutilizing fewer FLOPS and parameters. In addition, CvT\nachieves state-of-the-art performance when evaluated at\nlarger scale pretraining ( e.g. on the public ImageNet-22k\ndataset). Finally, we demonstrate that in this new design, we\ncan drop the positional embedding for tokens without any\ndegradation to model performance. This not only simpliﬁes\nthe architecture design, but also makes it readily capable of\naccommodating variable resolutions of input images that is\ncritical to many vision tasks.\n2. Related Work\nTransformers that exclusively rely on the self-attention\nmechanism to capture global dependencies have dominated\nin natural language modelling [31, 10, 25]. Recently, the\nTransformer based architecture has been viewed as a viable\nalternative to the convolutional neural networks (CNNs) in\nvisual recognition tasks, such as classiﬁcation [11, 30], ob-\nject detection [3, 45, 43, 8, 28], segmentation [33, 36], im-\nage enhancement [4, 40], image generation [24], video pro-\ncessing [42, 44] and 3D point cloud processing [12].\nVision Transformers. The Vision Transformer (ViT) is\nthe ﬁrst to prove that a pure Transformer architecture can\nattain state-of-the-art performance ( e.g. ResNets [15], Ef-\nﬁcientNet [29]) on image classiﬁcation when the data is\nlarge enough ( i.e. on ImageNet-22k, JFT-300M). Speciﬁ-\ncally, ViT decomposes each image into a sequence of tokens\n(i.e. non-overlapping patches) with ﬁxed length, and then\napplies multiple standard Transformer layers, consisting of\nMulti-Head Self-Attention module (MHSA) and Position-\nwise Feed-forward module (FFN), to model these tokens.\nDeiT [30] further explores the data-efﬁcient training and\ndistillation for ViT. In this work, we study how to combine\n2\nFigure 2: The pipeline of the proposed CvT architecture. (a) Overall architecture, showing the hierarchical multi-stage\nstructure facilitated by the Convolutional Token Embedding layer. (b) Details of the Convolutional Transformer Block,\nwhich contains the convolution projection as the ﬁrst layer.\nCNNs and Transformers to model both local and global de-\npendencies for image classiﬁcation in an efﬁcient way.\nIn order to better model local context in vision Trans-\nformers, some concurrent works have introduced design\nchanges. For example, the Conditional Position encod-\nings Visual Transformer (CPVT) [6] replaces the prede-\nﬁned positional embedding used in ViT with conditional\nposition encodings (CPE), enabling Transformers to pro-\ncess input images of arbitrary size without interpolation.\nTransformer-iN-Transformer (TNT) [14] utilizes both an\nouter Transformer block that processes the patch embed-\ndings, and an inner Transformer block that models the re-\nlation among pixel embeddings, to model both patch-level\nand pixel-level representation. Tokens-to-Token (T2T) [41]\nmainly improves tokenization in ViT by concatenating mul-\ntiple tokens within a sliding window into one token. How-\never, this operation fundamentally differs from convolutions\nespecially in normalization details, and the concatenation\nof multiple tokens greatly increases complexity in compu-\ntation and memory. PVT [34] incorporates a multi-stage\ndesign (without convolutions) for Transformer similar to\nmulti-scales in CNNs, favoring dense prediction tasks.\nIn contrast to these concurrent works, this work aims\nto achieve the best of both worlds by introducing convolu-\ntions, with image domain speciﬁc inductive biases, into the\nTransformer architecture. Table 1 shows the key differences\nin terms of necessity of positional encodings, type of token\nembedding, type of projection, and Transformer structure in\nthe backbone, between the above representative concurrent\nworks and ours.\nIntroducing Self-attentions to CNNs. Self-attention\nmechanisms have been widely applied to CNNs in vision\ntasks. Among these works, the non-local networks [35] are\ndesigned for capturing long range dependencies via global\nattention. The local relation networks [17] adapts its weight\naggregation based on the compositional relations (similar-\nity) between pixels/features within a local window, in con-\ntrast to convolution layers which employ ﬁxed aggrega-\ntion weights over spatially neighboring input feature. Such\nan adaptive weight aggregation introduces geometric pri-\nors into the network which are important for the recogni-\ntion tasks. Recently, BoTNet [27] proposes a simple yet\npowerful backbone architecture that just replaces the spa-\ntial convolutions with global self-attention in the ﬁnal three\nbottleneck blocks of a ResNet and achieves a strong per-\nformance in image recognition. Instead, our work performs\nan opposite research direction: introducing convolutions to\nTransformers.\nIntroducing Convolutions to Transformers. In NLP\nand speech recognition, convolutions have been used to\nmodify the Transformer block, either by replacing multi-\nhead attentions with convolution layers [38], or adding\nadditional convolution layers in parallel [39] or sequen-\ntially [13], to capture local relationships. Other prior work\n[37] proposes to propagate attention maps to succeeding\nlayers via a residual connection, which is ﬁrst transformed\nby convolutions. Different from these works, we propose\nto introduce convolutions to two primary parts of the vi-\nsion Transformer: ﬁrst, to replace the existing Position-wise\nLinear Projection for the attention operation with our Con-\nvolutional Projection, and second, to use our hierarchical\nmulti-stage structure to enable varied resolution of 2D re-\nshaped token maps, similar to CNNs. Our unique design\naffords signiﬁcant performance and efﬁciency beneﬁts over\n3\nprior works.\n3. Convolutional vision Transformer\nThe overall pipeline of the Convolutional vision Trans-\nformer (CvT) is shown in Figure 2. We introduce two\nconvolution-based operations into the Vision Transformer\narchitecture, namely the Convolutional Token Embedding\nand Convolutional Projection. As shown in Figure 2 (a), a\nmulti-stage hierarchy design borrowed from CNNs [20, 15]\nis employed, where three stages in total are used in this\nwork. Each stage has two parts. First, the input image\n(or 2D reshaped token maps) are subjected to the Convo-\nlutional Token Embeddinglayer, which is implemented as a\nconvolution with overlapping patches with tokens reshaped\nto the 2D spatial grid as the input (the degree of overlap\ncan be controlled via the stride length). An additional layer\nnormalization is applied to the tokens. This allows each\nstage to progressively reduce the number of tokens (i.e. fea-\nture resolution) while simultaneously increasing the width\nof the tokens ( i.e. feature dimension), thus achieving spa-\ntial downsampling and increased richness of representation,\nsimilar to the design of CNNs. Different from other prior\nTransformer-based architectures [11, 30, 41, 34], we do not\nsum the ad-hod position embedding to the tokens. Next,\na stack of the proposed Convolutional Transformer Blocks\ncomprise the remainder of each stage. Figure 2 (b) shows\nthe architecture of the Convolutional Transformer Block,\nwhere a depth-wise separable convolution operation [5],\nreferred as Convolutional Projection, is applied for query,\nkey, and value embeddings respectively, instead of the stan-\ndard position-wise linear projection in ViT [11]. Addition-\nally, the classiﬁcation token is added only in the last stage.\nFinally, an MLP (i.e. fully connected) Head is utilized upon\nthe classiﬁcation token of the ﬁnal stage output to predict\nthe class.\nWe ﬁrst elaborate on the proposed Convolutional Token\nEmbedding layer. Next we show how to perform Convolu-\ntional Projectionfor the Multi-Head Self-Attention module,\nand its efﬁcient design for managing computational cost.\n3.1. Convolutional Token Embedding\nThis convolution operation in CvT aims to model local\nspatial contexts, from low-level edges to higher order se-\nmantic primitives, over a multi-stage hierarchy approach,\nsimilar to CNNs.\nFormally, given a 2D image or a 2D-reshaped output to-\nken map from a previous stage xi−1 ∈RHi−1×Wi−1×Ci−1\nas the input to stage i, we learn a function f(·) that maps\nxi−1 into new tokensf(xi−1) with a channel sizeCi, where\nf(·) is 2D convolution operation of kernel size s ×s, stride\ns −o and p padding (to deal with boundary conditions).\nThe new token map f(xi−1) ∈RHi×Wi×Ci has height and\nwidth\nHi =\n⌊Hi−1 + 2p −s\ns −o + 1\n⌋\n, Wi =\n⌊Wi−1 + 2p −s\ns −o + 1\n⌋\n.\n(1)\nf(xi−1) is then ﬂattened into size HiWi ×Ci and normal-\nized by layer normalization [1] for input into the subsequent\nTransformer blocks of stage i.\nThe Convolutional Token Embedding layer allows us to\nadjust the token feature dimension and the number of to-\nkens at each stage by varying parameters of the convolution\noperation. In this manner, in each stage we progressively\ndecrease the token sequence length, while increasing the\ntoken feature dimension. This gives the tokens the ability\nto represent increasingly complex visual patterns over in-\ncreasingly larger spatial footprints, similar to feature layers\nof CNNs.\n3.2. Convolutional Projection for Attention\nThe goal of the proposed Convolutional Projection layer\nis to achieve additional modeling of local spatial context,\nand to provide efﬁciency beneﬁts by permitting the under-\nsampling of K and V matrices.\nFundamentally, the proposed Transformer block with\nConvolutional Projection is a generalization of the origi-\nnal Transformer block. While previous works [13, 39] try\nto add additional convolution modules to the Transformer\nBlock for speech recognition and natural language process-\ning, they result in a more complicated design and addi-\ntional computational cost. Instead, we propose to replace\nthe original position-wise linear projection for Multi-Head\nSelf-Attention (MHSA) with depth-wise separable convo-\nlutions, forming the Convolutional Projection layer.\n3.2.1 Implementation Details\nFigure 3 (a) shows the original position-wise linear projec-\ntion used in ViT [11] and Figure 3 (b) shows our proposed\ns ×s Convolutional Projection. As shown in Figure 3 (b),\ntokens are ﬁrst reshaped into a 2D token map. Next, a Con-\nvolutional Projection is implemented using a depth-wise\nseparable convolution layer with kernel size s. Finally, the\nprojected tokens are ﬂattened into 1D for subsequent pro-\ncess. This can be formulated as:\nxq/k/v\ni = Flatten (Conv2d (Reshape2D(xi), s)) ,\n(2)\nwhere xq/k/v\ni is the token input for Q/K/V matrices at\nlayer i, xi is the unperturbed token prior to the Convolu-\ntional Projection, Conv2d is a depth-wise separable con-\nvolution [5] implemented by: Depth-wise Conv2d →\nBatchNorm2d →Point-wise Conv2d, and s refers\nto the convolution kernel size.\nThe resulting new Transformer Block with the Convo-\nlutional Projection layer is a generalization of the original\n4\nFigure 3: (a) Linear projection in ViT [11]. (b) Convolutional projection. (c) Squeezed convolutional projection. Unless\notherwise stated, we use (c) Squeezed convolutional projection by default.\nTransformer Block design. The original position-wise lin-\near projection layer could be trivially implemented using a\nconvolution layer with kernel size of 1 ×1.\n3.2.2 Efﬁciency Considerations\nThere are two primary efﬁciency beneﬁts from the design\nof our Convolutional Projection layer.\nFirst, we utilize efﬁcient convolutions. Directly using\nstandard s×s convolutions for the Convolutional Projection\nwould require s2C2 parameters and O(s2C2T) FLOPs,\nwhere C is the token channel dimension, and T is the num-\nber of tokens for processing. Instead, we split the standard\ns ×s convolution into a depth-wise separable convolution\n[16]. In this way, each of the proposed Convolutional Pro-\njection would only introduce an extra of s2C parameters\nand O(s2CT ) FLOPs compared to the original position-\nwise linear projection, which are negligible with respect to\nthe total parameters and FLOPs of the models.\nSecond, we leverage the proposed Convolutional Projec-\ntion to reduce the computation cost for the MHSA opera-\ntion. The s ×s Convolutional Projection permits reducing\nthe number of tokens by using a stride larger than 1. Fig-\nure 3 (c) shows the Convolutional Projection, where the key\nand value projection are subsampled by using a convolu-\ntion with stride larger than 1. We use a stride of 2 for key\nand value projection, leaving the stride of 1 for query un-\nchanged. In this way, the number of tokens for key and\nvalue is reduced 4 times, and the computational cost is re-\nduced by 4 times for the later MHSA operation. This comes\nwith a minimal performance penalty, as neighboring pix-\nels/patches in images tend to have redundancy in appear-\nance/semantics. In addition, the local context modeling of\nthe proposed Convolutional Projection compensates for the\nloss of information incurred by resolution reduction.\n3.3. Methodological Discussions\nRemoving Positional Embeddings: The introduction of\nConvolutional Projections for every Transformer block,\ncombined with the Convolutional Token Embedding, gives\nus the ability to model local spatial relationships through the\nnetwork. This built-in property allows dropping the position\nembedding from the network without hurting performance,\nas evidenced by our experiments (Section 4.4), simplifying\ndesign for vision tasks with variable input resolution.\nRelations to Concurrent Work: Recently, two more re-\nlated concurrent works also propose to improve ViT by in-\ncorporating elements of CNNs to Transformers. Tokens-\nto-Token ViT [41] implements a progressive tokenization,\nand then uses a Transformer-based backbone in which the\nlength of tokens is ﬁxed. By contrast, our CvT implements\na progressive tokenization by a multi-stage process – con-\ntaining both convolutional token embeddings and convolu-\ntional Transformer blocks in each stage. As the length of\ntokens are decreased in each stage, the width of the tokens\n(dimension of feature) can be increased, allowing increased\nrichness of representations at each feature spatial resolu-\ntion. Additionally, whereas T2T concatenates neighboring\ntokens into one new token, leading to increasing the com-\nplexity of memory and computation, our usage of convolu-\ntional token embedding directly performs contextual learn-\ning without concatenation, while providing the ﬂexibility\nof controlling stride and feature dimension. To manage the\ncomplexity, T2T has to consider a deep-narrow architecture\ndesign with smaller hidden dimensions and MLP size than\nViT in the subsequent backbone. Instead, we changed pre-\nvious Transformer modules by replacing the position-wise\nlinear projection with our convolutional projection\nPyramid Vision Transformer (PVT) [34] overcomes the\ndifﬁculties of porting ViT to various dense prediction tasks.\nIn ViT, the output feature map has only a single scale with\nlow resolution. In addition, computations and memory cost\nare relatively high, even for common input image sizes. To\naddress this problem, both PVT and our CvT incorporate\npyramid structures from CNNs to the Transformers struc-\nture. Compared with PVT, which only spatially subsam-\nples the feature map or key/value matrices in projection, our\nCvT instead employs convolutions with stride to achieve\nthis goal. Our experiments (shown in Section 4.4) demon-\n5\nOutput Size Layer Name CvT-13 CvT-21 CvT-W24\nStage1\n56 ×56 Conv. Embed. 7 ×7, 64, stride4 7 ×7, 192, stride4\n56 ×56\nConv. Proj.\nMHSA\nMLP\n\n\n3 ×3,64\nH1 = 1, D1 = 64\nR1 = 4\n\n×1\n\n\n3 ×3,64\nH1 = 1, D1 = 64\nR1 = 4\n\n×1\n\n\n3 ×3,192\nH1 = 3, D1 = 192\nR1 = 4\n\n×2\nStage2\n28 ×28 Conv. Embed. 3 ×3, 192, stride2 3 ×3, 768, stride2\n28 ×28\nConv. Proj.\nMHSA\nMLP\n\n\n3 ×3,192\nH2 = 3, D2 = 192\nR2 = 4\n\n×2\n\n\n3 ×3,192\nH2 = 3, D2 = 192\nR2 = 4\n\n×4\n\n\n3 ×3,768\nH2 = 12, D2 = 768\nR2 = 4\n\n×2\nStage3\n14 ×14 Conv. Embed. 3 ×3, 384, stride2 3 ×3, 1024, stride2\n14 ×14\nConv. Proj.\nMHSA\nMLP\n\n\n3 ×3,384\nH3 = 6, D3 = 384\nR3 = 4\n\n×10\n\n\n3 ×3,384\nH3 = 6, D3 = 384\nR3 = 4\n\n×16\n\n\n3 ×3,1024\nH3 = 16, D3 = 1024\nR3 = 4\n\n×20\nHead 1 ×1 Linear 1000\nParams 19.98 M 31.54 M 276.7 M\nFLOPs 4.53 G 7.13 G 60.86 G\nTable 2: Architectures for ImageNet classiﬁcation. Input image size is 224 ×224 by default. Conv. Embed.: Convolutional\nToken Embedding. Conv. Proj.: Convolutional Projection. Hi and Di is the number of heads and embedding feature\ndimension in the ith MHSA module. Ri is the feature dimension expansion ratio in the ith MLP layer.\nstrate that the fusion of local neighboring information plays\nan important role on the performance.\n4. Experiments\nIn this section, we evaluate the CvT model on large-scale\nimage classiﬁcation datasets and transfer to various down-\nstream datasets. In addition, we perform through ablation\nstudies to validate the design of the proposed architecture.\n4.1. Setup\nFor evaluation, we use the ImageNet dataset, with 1.3M\nimages and 1k classes, as well as its superset ImageNet-22k\nwith 22k classes and 14M images [9]. We further trans-\nfer the models pretrained on ImageNet-22k to downstream\ntasks, including CIFAR-10/100 [19], Oxford-IIIT-Pet [23],\nOxford-IIIT-Flower [22], following [18, 11].\nModel Variants We instantiate models with different pa-\nrameters and FLOPs by varying the number of Transformer\nblocks of each stage and the hidden feature dimension used,\nas shown in Table 2. Three stages are adapted. We de-\nﬁne CvT-13 and CvT-21 as basic models, with 19.98M and\n31.54M paramters. CvT-X stands for Convolutional vision\nTransformer with X Transformer Blocks in total. Addition-\nally, we experiment with a wider model with a larger token\ndimension for each stage, namely CvT-W24 (W stands for\nWide), resulting 298.3M parameters, to validate the scaling\nability of the proposed architecture.\nTraining AdamW [21] optimizer is used with the weight\ndecay of 0.05 for our CvT-13, and 0.1 for our CvT-21 and\nCvT-W24. We train our models with an initial learning\nrate of 0.02 and a total batch size of 2048 for 300 epochs,\nwith a cosine learning rate decay scheduler. We adopt the\nsame data augmentation and regularization methods as in\nViT [30]. Unless otherwise stated, all ImageNet models are\ntrained with an 224 ×224 input size.\nFine-tuning We adopt ﬁne-tuning strategy from ViT [30].\nSGD optimizor with a momentum of 0.9 is used for ﬁne-\ntuning. As in ViT [30], we pre-train our models at resolu-\ntion 224 ×224, and ﬁne-tune at resolution of 384 ×384.\nWe ﬁne-tune each model with a total batch size of 512,\nfor 20,000 steps on ImageNet-1k, 10,000 steps on CIFAR-\n10 and CIFAR-100, and 500 steps on Oxford-IIIT Pets and\nOxford-IIIT Flowers-102.\n4.2. Comparison to state of the art\nWe compare our method with state-of-the-art classiﬁca-\ntion methods including Transformer-based models and rep-\nresentative CNN-based models on ImageNet [9], ImageNet\nReal [2] and ImageNet V2 [26] datasets in Table 3.\nCompared to Transformer based models, CvT achieves\na much higher accuracy with fewer parameters and FLOPs.\nCvT-21 obtains a 82.5% ImageNet Top-1 accuracy, which\nis 0.5% higher than DeiT-B with the reduction of 63% pa-\nrameters and 60% FLOPs. When comparing to concurrent\nworks, CvT still shows superior advantages. With fewer\nparamerters, CvT-13 achieves a 81.6% ImageNet Top-1 ac-\ncuracy, outperforming PVT-Small [34], T2T-ViTt-14 [41],\nTNT-S [14] by 1.7%, 0.8%, 0.2% respectively.\nOur architecture designing can be further improved in\nterms of model parameters and FLOPs by neural architec-\nture search (NAS) [7]. In particular, we search the proper\nstride for each convolution projection of key and value\n(stride = 1 , 2) and the expansion ratio for each MLP\nlayer ( ratioMLP = 2 , 4). Such architecture candidates\nwith FLOPs ranging from 2.59G to 4.03G and the num-\n6\n#Param. image FLOPs ImageNet Real V2\nMethod Type Network (M) size (G) top-1 (%) top-1 (%) top-1 (%)\nConvolutional Networks\nResNet-50 [15] 25 2242 4.1 76.2 82.5 63.3\nResNet-101 [15] 45 2242 7.9 77.4 83.7 65.7\nResNet-152 [15] 60 2242 11 78.3 84.1 67.0\nTransformers\nViT-B/16 [11] 86 3842 55.5 77.9 83.6 –\nViT-L/16 [11] 307 3842 191.1 76.5 82.2 –\nDeiT-S [30][arxiv 2020] 22 2242 4.6 79.8 85.7 68.5\nDeiT-B [30][arxiv 2020] 86 2242 17.6 81.8 86.7 71.5\nPVT-Small [34][arxiv 2021] 25 2242 3.8 79.8 – –\nPVT-Medium [34][arxiv 2021] 44 2242 6.7 81.2 – –\nPVT-Large [34][arxiv 2021] 61 2242 9.8 81.7 – –\nT2T-ViTt-14 [41][arxiv 2021] 22 2242 6.1 80.7 – –\nT2T-ViTt-19 [41][arxiv 2021] 39 2242 9.8 81.4 – –\nT2T-ViTt-24 [41][arxiv 2021] 64 2242 15.0 82.2 – –\nTNT-S [14][arxiv 2021] 24 2242 5.2 81.3 – –\nTNT-B [14][arxiv 2021] 66 2242 14.1 82.8 – –\nConvolutional Transformers\nOurs: CvT-13 20 2242 4.5 81.6 86.7 70.4\nOurs: CvT-21 32 2242 7.1 82.5 87.2 71.3\nOurs: CvT-13↑384 20 3842 16.3 83.0 87.9 71.9\nOurs: CvT-21↑384 32 3842 24.9 83.3 87.7 71.9\nOurs: CvT-13-NAS 18 2242 4.1 82.2 87.5 71.3\nConvolution Networks22k BiT-M↑480 [18] 928 4802 837 85.4 – –\nTransformers22k\nViT-B/16↑384 [11] 86 3842 55.5 84.0 88.4 –\nViT-L/16↑384 [11] 307 3842 191.1 85.2 88.4 –\nViT-H/16↑384 [11] 632 3842 – 85.1 88.7 –\nConvolutional Transformers22k\nOurs: CvT-13↑384 20 3842 16 83.3 88.7 72.9\nOurs: CvT-21↑384 32 3842 25 84.9 89.8 75.6\nOurs: CvT-W24↑384 277 3842 193.2 87.7 90.6 78.8\nTable 3: Accuracy of manual designed architecture on ImageNet [9], ImageNet Real [2] and ImageNet V2 matched fre-\nquency [26]. Subscript 22k indicates the model pre-trained on ImageNet22k [9], and ﬁnetuned on ImageNet1k with the input\nsize of 384 ×384, except BiT-M [18] ﬁnetuned with input size of 480 ×480.\nber of model parameters ranging from 13.66M to 19.88M\nconstruct the search space. The NAS is evaluated directly\non ImageNet-1k. The searched CvT-13-NAS, a bottleneck-\nlike architecture withstride = 2, ratioMLP = 2at the ﬁrst\nand last stages, andstride = 1, ratioMLP = 4at most lay-\ners of the middle stage, reaches to a 82.2% ImageNet Top-1\naccuracy with fewer model parameters than CvT-13.\nCompared to CNN-based models, CvT further closes the\nperformance gap of Transformer-based models. Our small-\nest model CvT-13 with 20M parameters and 4.5G FLOPs\nsurpasses the large ResNet-152 model by 3.2% on Ima-\ngeNet Top-1 accuracy, while ResNet-151 has 3 times the\nparameters of CvT-13.\nFurthermore, when more data are involved, our wide\nmodel CvT-W24* pretrained on ImageNet-22k reaches to\n87.7% Top-1 Accuracy on ImageNet without extra data\n(e.g. JFT-300M), surpassing the previous best Transformer\nbased models ViT-L/16 by 2.5% with similar number of\nmodel parameters and FLOPs.\n4.3. Downstream task transfer\nWe further investigate the ability of our models to trans-\nfer by ﬁne-tuning models on various tasks, with all models\nbeing pre-trained on ImageNet-22k. Table 4 shows the re-\nsults. Our CvT-W24 model is able to obtain the best per-\nformance across all the downstream tasks considered, even\nwhen compared to the large BiT-R152x4 [18] model, which\nhas more than 3×the number of parameters as CvT-W24.\n4.4. Ablation Study\nWe design various ablation experiments to investigate\nthe effectiveness of the proposed components of our archi-\ntecture. First, we show that with our introduction of con-\nvolutions, position embeddings can be removed from the\n7\nModel Param\n(M)\nCIFAR\n10\nCIFAR\n100 Pets Flowers\n102\nBiT-M [18] 928 98.91 92.17 94.46 99.30\nViT-B/16 [11] 86 98.95 91.67 94.43 99.38\nViT-L/16 [11] 307 99.16 93.44 94.73 99.61\nViT-H/16 [11] 632 99.27 93.82 94.82 99.51\nOurs: CvT-13 20 98.83 91.11 93.25 99.50\nOurs: CvT-21 32 99.16 92.88 94.03 99.62\nOurs: CvT-W24 277 99.39 94.09 94.73 99.72\nTable 4: Top-1 accuracy on downstream tasks. All the mod-\nels are pre-trained on ImageNet-22k data\nMethod Model Param\n(M) Pos. Emb. ImageNet\nTop-1 (%)\na DeiT-S 22 Default 79.8\nb DeiT-S 22 N/A 78.0\nc CvT-13 20 Every stage 81.5\nd CvT-13 20 First stage 81.4\ne CvT-13 20 Last stage 81.4\nf CvT-13 20 N/A 81.6\nTable 5: Ablations on position embedding.\nmodel. Then, we study the impact of each of the proposed\nConvolutional Token Embedding and Convolutional Projec-\ntion components.\nRemoving Position Embedding Given that we have in-\ntroduced convolutions into the model, allowing local con-\ntext to be captured, we study whether position embed-\nding is still needed for CvT. The results are shown in Ta-\nble 5, and demonstrate that removing position embedding\nof our model does not degrade the performance. There-\nfore, position embeddings have been removed from CvT\nby default. As a comparison, removing the position em-\nbedding of DeiT-S would lead to 1.8% drop of ImageNet\nTop-1 accuracy, as it does not model image spatial relation-\nships other than by adding the position embedding. This\nfurther shows the effectiveness of our introduced convolu-\ntions. Position Embedding is often realized by ﬁxed-length\nlearn-able vectors, limiting the trained model adaptation of\nvariable-length input. However, a wide range of vision ap-\nplications take variable image resolutions. Recent work\nCPVT [6] tries to replace explicit position embedding of\nVision Transformers with a conditional position encodings\nmodule to model position information on-the-ﬂy. CvT is\nable to completely remove the positional embedding, pro-\nviding the possibility of simplifying adaption to more vision\ntasks without requiring a re-designing of the embedding.\nMethod Conv.\nEmbed.\nPos.\nEmbed.\n#Param\n(M)\nImageNet\ntop-1 (%)\na 19.5 80.7\nb \u0013 19.9 81.1\nc \u0013 \u0013 20.3 81.4\nd \u0013 20.0 81.6\nTable 6: Ablations on Convolutional Token Embedding.\nMethod Conv. Proj. KV .\nstride\nParams\n(M)\nFLOPs\n(G)\nImageNet\ntop-1 (%)\na 1 20 6.55 82.3\nb 2 20 4.53 81.6\nTable 7: Ablations on Convolutional Projection with differ-\nent strides for key and value projection. Conv. Proj. KV .:\nConvolutional Projection for key and value. We apply Con-\nvolutional Projection in all Transformer blocks.\nMethod Conv. Projection Imagenet\ntop-1 (%)Stage 1 Stage 2 Stage 3\na 80.6\nb \u0013 80.8\nc \u0013 \u0013 81.0\nd \u0013 \u0013 \u0013 81.6\n#Blocks 1 2 10\nTable 8: Ablations on Convolutional Projection v.s.\nPosition-wise Linear Projection. \u0013 indicates the use of\nConvolutional Projection, otherwise use Position-wise Lin-\near Projection.\nConvolutional Token Embedding We study the effec-\ntiveness of the proposed Convolutional Token Embedding,\nand Table 6 shows the results. Table 6d is the CvT-13\nmodel. When we replace the Convolutional Token Embed-\nding with non-overlapping Patch Embedding [11], the per-\nformance drops 0.8% (Table 6a v.s. Table 6d). When po-\nsition embedding is used, the introduction of Convolutional\nToken Embedding still obtains 0.3% improvement (Table 6b\nv.s. Table 6c). Further, when using both Convolutional\nToken Embedding and position embedding as Table 6d, it\nslightly drops 0.1% accuracy. These results validate the in-\ntroduction of Convolutional Token Embedding not only im-\nproves the performance, but also helps CvT model spatial\nrelationships without position embedding.\nConvolutional Projection First, we compare the pro-\nposed Convolutional Projection with different strides in Ta-\nble 7. By using a stride of 2 for key and value projection,\nwe observe a 0.3% drop in ImageNet Top-1 accuracy, but\n8\nwith 30% fewer FLOPs. We choose to use Convolutional\nProjection with stride 2 for key and value as default for less\ncomputational cost and memory usage.\nThen, we study how the proposed Convolutional Pro-\njection affects the performance by choosing whether to use\nConvolutional Projection or the regular Position-wise Lin-\near Projection for each stage. The results are shown in Ta-\nble 8. We observe that replacing the original Position-wise\nLinear Projection with the proposed Convolutional Projec-\ntion improves the Top-1 Accuracy on ImageNet from 80.6%\nto 81.5%. In addition, performance continually improves as\nmore stages use the design, validating this approach as an\neffective modeling strategy.\n5. Conclusion\nIn this work, we have presented a detailed study of in-\ntroducing convolutions into the Vision Transformer archi-\ntecture to merge the beneﬁts of Transformers with the ben-\neﬁts of CNNs for image recognition tasks. Extensive ex-\nperiments demonstrate that the introduced convolutional to-\nken embedding and convolutional projection, along with the\nmulti-stage design of the network enabled by convolutions,\nmake our CvT architecture achieve superior performance\nwhile maintaining computational efﬁciency. Furthermore,\ndue to the built-in local context structure introduced by con-\nvolutions, CvT no longer requires a position embedding,\ngiving it a potential advantage for adaption to a wide range\nof vision tasks requiring variable input resolution.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\nLayer normalization, 2016. 4\n[2] Lucas Beyer, Olivier J H ´enaff, Alexander Kolesnikov, Xi-\naohua Zhai, and A ¨aron van den Oord. Are we done with\nimagenet? arXiv preprint arXiv:2006.07159, 2020. 6, 7\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision , pages 213–229. Springer, 2020.\n2\n[4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. arXiv\npreprint arXiv:2012.00364, 2020. 2\n[5] Franc ¸ois Chollet. Xception: Deep learning with depthwise\nseparable convolutions. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n1251–1258, 2017. 2, 4\n[6] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and\nHuaxia Xia. Do we really need explicit position encodings\nfor vision transformers? arXiv preprint arXiv:2102.10882,\n2021. 3, 8\n[7] Xiyang Dai, Dongdong Chen, Mengchen Liu, Yinpeng\nChen, and Lu YUan. Da-nas: Data adapted pruning for efﬁ-\ncient neural architecture search. In European Conference on\nComputer Vision, 2020. 6\n[8] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen.\nUp-detr: Unsupervised pre-training for object detection with\ntransformers. arXiv preprint arXiv:2011.09094, 2020. 2\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009. 6, 7\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\n2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) , pages\n4171–4186, Minneapolis, Minnesota, 2019. Association for\nComputational Linguistics. 1, 2\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1, 2, 4, 5, 6, 7, 8\n[12] Nico Engel, Vasileios Belagiannis, and Klaus Dietmayer.\nPoint transformer. arXiv preprint arXiv:011.00931 , 2020.\n2\n[13] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par-\nmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng-\ndong Zhang, Yonghui Wu, et al. Conformer: Convolution-\naugmented transformer for speech recognition. arXiv\npreprint arXiv:2005.08100, 2020. 3, 4\n[14] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer.arXiv preprint\narXiv:2103.00112, 2021. 1, 3, 6, 7\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 1, 2, 4, 7\n[16] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017. 5\n[17] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Lo-\ncal relation networks for image recognition. arXiv preprint\narXiv:1904.11491, 2019. 3\n[18] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig transfer (bit): General visual representation learning.\narXiv preprint arXiv:1912.11370, 6(2):8, 2019. 1, 6, 7\n[19] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 6\n[20] Yann Lecun, Patrick Haffner, L´eon Bottou, and Yoshua Ben-\ngio. Object recognition with gradient-based learning. In\nContour and Grouping in Computer Vision. Springer, 1999.\n2, 4\n[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 6\n9\n[22] Maria-Elena Nilsback and Andrew Zisserman. Automated\nﬂower classiﬁcation over a large number of classes. In In-\ndian Conference on Computer Vision, Graphics and Image\nProcessing, Dec 2008. 6\n[23] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and\nC. V . Jawahar. Cats and dogs. InIEEE Conference on Com-\nputer Vision and Pattern Recognition, 2012. 6\n[24] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. In International Conference on Machine\nLearning, pages 4055–4064. PMLR, 2018. 2\n[25] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. 2018. 2\n[26] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and\nVaishaal Shankar. Do imagenet classiﬁers generalize to im-\nagenet? In International Conference on Machine Learning,\npages 5389–5400. PMLR, 2019. 6, 7\n[27] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani. Bottle-\nneck transformers for visual recognition. arXiv preprint\narXiv:2101.11605, 2021. 3\n[28] Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani.\nRethinking transformer-based set prediction for object detec-\ntion. arXiv preprint arXiv:2011.10881, 2020. 2\n[29] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, pages 6105–6114. PMLR,\n2019. 2\n[30] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020. 1, 2, 4, 6,\n7\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Isabelle Guyon,\nUlrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob\nFergus, S. V . N. Vishwanathan, and Roman Garnett, editors,\nAdvances in Neural Information Processing Systems 30: An-\nnual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA , pages\n5998–6008, 2017. 1, 2\n[32] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel R. Bowman. GLUE: A multi-task\nbenchmark and analysis platform for natural language un-\nderstanding. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019. OpenReview.net, 2019. 1\n[33] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille,\nand Liang-Chieh Chen. Max-deeplab: End-to-end panop-\ntic segmentation with mask transformers. arXiv preprint\narXiv:2012.00759, 2020. 2\n[34] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for\ndense prediction without convolutions. arXiv preprint\narXiv:2102.12122, 2021. 1, 3, 4, 5, 6, 7\n[35] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 7794–7803, 2018. 3\n[36] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,\nBaoshan Cheng, Hao Shen, and Huaxia Xia. End-to-\nend video instance segmentation with transformers. arXiv\npreprint arXiv:2011.14503, 2020. 2\n[37] Yujing Wang, Yaming Yang, Jiangang Bai, Mingliang\nZhang, Jing Bai, Jing Yu, Ce Zhang, Gao Huang, and Yunhai\nTong. Evolving attention with residual convolutions. arXiv\npreprint arXiv:2102.12895, 2021. 3\n[38] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin,\nand Michael Auli. Pay less attention with lightweight and dy-\nnamic convolutions. arXiv preprint arXiv:1901.10430, 2019.\n3\n[39] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song\nHan. Lite transformer with long-short range attention. arXiv\npreprint arXiv:2004.11886, 2020. 3, 4\n[40] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Bain-\ning Guo. Learning texture transformer network for image\nsuper-resolution. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , pages\n5791–5800, 2020. 2\n[41] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 1, 3, 4,\n5, 6, 7\n[42] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning\njoint spatial-temporal transformations for video inpainting.\nIn European Conference on Computer Vision , pages 528–\n543. Springer, 2020. 2\n[43] Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng\nLi, and Hao Dong. End-to-end object detection with adaptive\nclustering transformer. arXiv preprint arXiv:2011.09315 ,\n2020. 2\n[44] Luowei Zhou, Yingbo Zhou, Jason J. Corso, Richard Socher,\nand Caiming Xiong. End-to-end dense video captioning with\nmasked transformer. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , June\n2018. 2\n[45] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020. 2\n10"
}