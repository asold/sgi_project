{
    "title": "Can Generative LLMs Create Query Variants for Test Collections? An Exploratory Study",
    "url": "https://openalex.org/W4384652592",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4299020643",
            "name": "Alaofi, Marwah",
            "affiliations": [
                "RMIT University"
            ]
        },
        {
            "id": null,
            "name": "Gallagher, Luke",
            "affiliations": [
                "RMIT University"
            ]
        },
        {
            "id": "https://openalex.org/A2745052425",
            "name": "Sanderson, Mark",
            "affiliations": [
                "RMIT University"
            ]
        },
        {
            "id": "https://openalex.org/A3163758984",
            "name": "Scholer Falk",
            "affiliations": [
                "RMIT University"
            ]
        },
        {
            "id": "https://openalex.org/A2032392804",
            "name": "Thomas Paul",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4284664032",
        "https://openalex.org/W2388783363",
        "https://openalex.org/W4284669679",
        "https://openalex.org/W3198734960",
        "https://openalex.org/W6605614091",
        "https://openalex.org/W2096249996",
        "https://openalex.org/W3094045953",
        "https://openalex.org/W2624553223",
        "https://openalex.org/W1977437814",
        "https://openalex.org/W2021581601",
        "https://openalex.org/W2740321901",
        "https://openalex.org/W2952866723",
        "https://openalex.org/W4297162632"
    ],
    "abstract": "This paper explores the utility of a Large Language Model (LLM) to automatically generate queries and query variants from a description of an information need. Given a set of information needs described as backstories, we explore how similar the queries generated by the LLM are to those generated by humans. We quantify the similarity using different metrics and examine how the use of each set would contribute to document pooling when building test collections. Our results show potential in using LLMs to generate query variants. While they may not fully capture the wide variety of human-generated variants, they generate similar sets of relevant documents, reaching up to 71.1% overlap at a pool depth of 100.",
    "full_text": "Can Generative LLMs Create Query Variants for Test Collections?\nAn Exploratory Study\nMarwah Alaofi\nRMIT University\nMelbourne, Australia\nmarwah.alaofi@student.rmit.edu.au\nLuke Gallagher\nRMIT University\nMelbourne, Australia\nluke.gallagher@rmit.edu.au\nMark Sanderson\nRMIT University\nMelbourne, Australia\nmark.sanderson@rmit.edu.au\nFalk Scholer\nRMIT University\nMelbourne, Australia\nfalk.scholer@rmit.edu.au\nPaul Thomas\nMicrosoft\nAdelaide, Australia\npathom@microsoft.com\nABSTRACT\nThis paper explores the utility of a Large Language Model (LLM)\nto automatically generate queries and query variants from a de-\nscription of an information need. Given a set of information needs\ndescribed as backstories, we explore how similar the queries gener-\nated by the LLM are to those generated by humans. We quantify\nthe similarity using different metrics and examine how the use of\neach set would contribute to document pooling when building test\ncollections. Our results show potential in using LLMs to generate\nquery variants. While they may not fully capture the wide variety\nof human-generated variants, they generate similar sets of relevant\ndocuments, reaching up to 71.1% overlap at a pool depth of 100.\nCCS CONCEPTS\n‚Ä¢ Information systems ‚Üí Test collections; Query representa-\ntion.\nKEYWORDS\nInformation retrieval; test collections; query variants; LLMs\nACM Reference Format:\nMarwah Alaofi, Luke Gallagher, Mark Sanderson, Falk Scholer, and Paul\nThomas. 2023. Can Generative LLMs Create Query Variants for Test Col-\nlections?: An Exploratory Study. In Proceedings of the 46th International\nACM SIGIR Conference on Research and Development in Information Retrieval\n(SIGIR ‚Äô23), July 23‚Äì27, 2023, Taipei, Taiwan. ACM, New York, NY, USA,\n5 pages. https://doi.org/10.1145/3539618.3591960\n1 INTRODUCTION AND BACKGROUND\nInformation Retrieval (IR) has been dedicated to delivering relevant\ninformation in response to user queries. The realization of this objec-\ntive has been facilitated by the use of offline test collections, which\noften provide a single representation (query) for each information\nneed. The single query assumption is convenient for numerous\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR ‚Äô23, July 23‚Äì27, 2023, Taipei, Taiwan\n¬© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-9408-6/23/07. . . $15.00\nhttps://doi.org/10.1145/3539618.3591960\nreasons. It helps to make the judging process economically viable\n(along with the system pooling approach in the Cranfield paradigm\nof test collection construction [19]), and it has provided a consis-\ntent, reusable environment for the development of retrieval systems\nand evaluation measures. The importance of query variations for\nenumerating relevant documents in a test collection dates back\nseveral decades [18] and previous tracks at TREC have explored the\nsignificance of such variance [6]. More recently, there has been a\nline of research providing further insights from the user perspective\nwith the advent of crowd-sourcing technologies [2, 13].\nQuery variants are alternative formulations of the same infor-\nmation need. For example, ‚Äúwhat hiking options are there in summer\nin sangre de cristo‚Äù and ‚Äúsangre de Cristo, new mexico hiking‚Äù are\nboth query variants generated in response to the same information\nneed, i.e., finding information for a hiking trip in the Sangre de\nCristo mountain region during summer. Bailey et al. [2] showed\nthat given the same backstory, users generate about 57 query vari-\nants on average - which is anticipated to increase as the number\nof participants grows. Similar findings are reported by Mackenzie\net al. [12] for queries generated to find additional information in\nresponse to document summaries.\nThe impact of query variants on retrieval has been empirically\ndemonstrated in prior research. Culpepper et al. [7] showed that\nvariants impact effectiveness substantially more than that due to\ntopic or ranking models. Penha et al. [17] tested the impact of vari-\nants using neural and transformer-based answer retrieval models.\nTheir experimental results demonstrated a 20% effectiveness drop\non average.\nAlaofi et al. [1] empirically demonstrated the impact of query\nvariants on a commercial search engine and different inverted in-\ndexes. Their results point to a substantial retrieval inconsistency\nand a concerning impact of variants on document retrievability.\nQuery variants have also been demonstrated to have a compara-\nble impact on the pool size as that of systems, calling to consider\nincorporating them when building test collections [14]. The current\nabstraction of one query per topic in the majority of test collections\nraises two concerns about how realistic system evaluations are:\n(1) is limiting system evaluation to a single representation of the\ninformation need appropriate, and; (2) how the test collection is con-\nstructed in the first place. Can we offer a solution to (1) through the\nuse of the LLMs to generate human-like queries? Or perhaps have\narXiv:2501.17981v1  [cs.IR]  29 Jan 2025\nSIGIR ‚Äô23, July 23‚Äì27, 2023, Taipei, Taiwan Marwah Alaofi, Luke Gallagher, Mark Sanderson, Falk Scholer, and Paul Thomas\nthem at least act in place to generate similar pools for constructing\ntest collections (2)?\nQuery variants have primarily been obtained through crowd-\nsourcing [2, 12], a process that is expensive to scale and may not\nbe an accurate representation of query variants as the information\nneeds are not naturally derived. A study by Zhang et al. [22], uses\nclick graphs to collect query variants, based on the assumption\nthat queries leading to the same click originate from the same\ninformation need. It is not clear if this assumption always holds true\nas a shared click may not necessarily indicate a shared intent and\nmany shared intents may not lead to a shared click. This requires\nexternal labeling which is difficult to achieve objectively, and as in\nthe case of crowd-sourcing is expensive to scale.\nIn-Context Learning (ICL) [5] emerges as a promising Natural\nLanguage Processing paradigm where no large domain-specific\ndatasets are required to fine-tune LLMs on a specific downstream\ntask. Instead, the LLMs are conditioned using a ‚Äòcontext‚Äô which is\nsimply a textual description of the task with a few or even zero\nexamples - often referred to as a few-, one-, or zero-shot learning.\nThis approach achieves promising results and has surpassed some\nof the state-of-the-art models in some tasks. This holds the promise\nof addressing the challenge posed by the scarcity of large query\nvariant datasets.\nICL has been recently used in IR, mainly to generate synthetic\nqueries given documents [3, 8, 10]. The synthetic query-document\npairs are then used to train a retrieval model. This approach builds\non earlier efforts which, prior to indexing, used fine-tuned LLMs\nto extend documents by generating relevant queries, an approach\nthat was simple yet effective to surpass state-of-the-art models\non retrieval benchmarks [15]. Though the advances this research\ndirection has made, it specifically aims to harness the power of\nLLMs to boost effectiveness scores by using some ‚Äòrepresentative‚Äô\nqueries, which may undergo some automated quality/consistency\nfiltering [8, 9], resulting in a query set that improves performance\nbut may not necessarily represent users.\nThe aim of this study is to explore using an LLM (GPT-3.5) as\nan alternative method to generate query variants given backstories\n(i.e., information need statements). We aim to compare the LLM-\ngenerated queries to human-generated ones. We approach this by\nquantifying the direct similarity between the two sets of queries\nand examining how they behave when used for pool construction.\nIn particular, we pose the following questions:\nRQ1 Can an LLM, with one-shot learning, generate queries that\nare similar or perhaps identical to the ones generated by\nhumans?\nRQ2 How do both sets compare when used for document pooling\nwhen constructing test collections?\n2 EXPERIMENT DESIGN\nWe detail the query sets and metrics used in the experiment.\n2.1 Query Sets, Model Prompting, and Runs\nWe use two sets of query variants: human and GPT-3.5 generated.\nThe human-generated query variants ‚Äì referred to as thehuman\nset ‚Äì were collected via crowd-sourcing as part of the UQV100 test\ncollection [2], which has one hundred backstories to describe one\n(a) <Task Description>(b) Example<Input Backstory><Output Query Variants>\n<Input Backstory>  \nGPT-3.5q1q2:qnOutput QueryVariants\nPrompt Template \n‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶\n=>\n=>\n(c)\nFigure 1: The prompt used to feed the GPT-3.5 model.\nYounormallywearcasualclothestowork,buthaveanimportantpresentationtomake,anddecidetowearajacketandtie.Youknowthatthe\"windsorknot\"isrecognizedasbeingthemoststylishwayoftyingatie,buthavenoideahowtodoone,andwouldliketofindout.\nBackstory\nHuman-generated queries1. how to tie a windsorknot2. windsorknot tutorial3. windsorknot how to4. windsortie knot tying instructions5. what is a windsorknot\nGPT-3.5-generated queries1. how to tie a windsorknot2. instructions for tying a windsorknot3. windsorknot directions4. youtubewindsorknot how to tie a tie5. how do imake a windsorknot\nFigure 2: Randomly selected example variants generated by\nhumans and GPT-3.5 (ùë°ùëíùëöùëù = 0.5). The variant in bold is repro-\nduced by GPT-3.5 and the one in red appears less appropriate,\nalthough similar ones are generated by humans, e.g., ‚Äúwind-\nsor knot wiki‚Äù.\nhundred search topics derived from the TREC 2013 and 2014 web\ntrack. Crowd workers were asked to read a backstory and formulate\nan initial query for the search task.\nTo generate the GPT sets , we use the same backstories from\nUQV100 to prompt the model. We use the text-davinci-003 model.1\nThis model is trained similarly to InstructGPT [16] using reinforce-\nment learning with reward models fine-tuned on human prefer-\nences.2 We experiment with different temperature settings ùë°ùëíùëöùëù =\n{0.0, 0.5, 1.0}, a parameter that controls how deterministic the model\nis in generating the text.\nWe prompt the model using the template in Figure 1. The prompt\nhas (a) a task description , (b) an example, and (c) an input backstory\nfor the model to generate the corresponding query variants. The\ntask description is a natural language specification of the task,\nwhich provides some context and details to guide the model to-\nward the expected distribution of the queries per backstory and the\naverage number of words per query, with specific values of these\nsettings based on prior research in query variant analysis [2, 12].\nWe follow a one-shot learning approach, in which the prompt\ncontains an example input backstory with its associated human-\ngenerated output queries, randomly selected from UQV100. The\nsame random example (i.e., topic 275) is used to prompt the model\nto generate query variants for the remaining 99 backstories. In\norder to avoid the influence of observed data, topic 275 has been\nexcluded from our analysis.\nWe investigated zero-shot learning with no examples provided to\nthe model. However, this approach resulted in the model producing\n1Last accessed on 2 February 2023\n2https://platform.openai.com/docs/model-index-for-researchers\nCan Generative LLMs Create Query Variants for Test Collections? SIGIR ‚Äô23, July 23‚Äì27, 2023, Taipei, Taiwan\nTable 1: Query (Q) statistics of the human set and the three\nGPT sets under different temperature ùë°ùëíùëöùëù settings.\nQuery\nVariant Set\nNumber of Variants Avg.\nWords/QTotal Unique Min. Max. Avg.\nHuman 10726 5681 19 101 57.38 5.34\nGPT (ùë°ùëíùëöùëù = 0.0) 4803 3638 11 172 36.75 5.95\nGPT (ùë°ùëíùëöùëù = 0.5) 3061 2999 12 88 30.29 4.86\nGPT (ùë°ùëíùëöùëù = 1.0) 2725 2719 12 48 27.46 4.65\nlong variants that closely resembled natural language questions.\nFew-shot learning, where multiple examples are provided to the\nmodel, might have produced better results [ 5], but as we were\nlimited by the number of available backstories we opted to use\none-shot learning. The distribution of all query sets are presented\nin Table 1. Some example query variants from the human set and\none of the GPT sets for a given backstory are shown in Figure 2.\nWe generate runs for the human set and the GPT sets using\nAnserini [21] BM25 (ùëè = 0.4, ùëò1 = 0.9) on the ClueWeb12-B corpus3 -\nwhich is also the corpus that was used to create relevance judgments\nfor the UQV100 test collection. The prompt template we use and the\ngenerated GPT sets are publicly available to aid reproducibility.4\n2.2 Metrics for Query Similarity\nIn addressing RQ1, we assume that the human set is the ideal set of\nquery variants and measure how similar the GPT sets are to that set.\nWe quantify that by measuring the average Jaccard Index between\nthe human set and the three GPT sets in which the overlapping\nqueries are an exact match between the two sets.\nAs keyword-based ranking models treat queries with slight vari-\nations equally, we incrementally relax the matching condition using\ntext transformations over the two sets and report the average Jac-\ncard Index score using the unique queries generated after each\ntransformation. Specifically, the overlap between the two sets is\nquantified by initially determining the exact match of raw queries\nin both sets. This matching condition is then relaxed to cumula-\ntively allow for variations in punctuation (T1), word forms (T2),\nstop words (T3), and word order (T4). We also show theCoverage\nRatio, which quantifies the proportion of queries from the human\nset that are successfully reproduced in the GPT sets using the afore-\nmentioned matching conditions.\n2.3 Metrics for Retrieval Similarity\nTo address RQ2, we find the overlap between the union of docu-\nments returned from the variants of the human set and the GPT sets.\nThis measure quantifies the similarity of the two sets in their utility\nfor constructing document pools. The overlap is quantified using\nthe Jaccard Index, measured at different depths. While we are inter-\nested in measuring the overall overlap in documents (regardless of\ntheir relevance judgments), we believe that the impact of the over-\nlap of relevant documents in particular holds greater importance\nin regards to system evaluation when constructing test collections.\n3https://www.lemurproject.org/clueweb12.php/\n4https://github.com/MarwahAlaofi/SIGIR-23-SRP-UQV100-GPT-Query-Variants\nT0 T1 T2 T3 T4\n/Q_uery Transformations\n0.05\n0.10\n0.15\n0.20Jaccard Index\ntemp = 0.0\ntemp = 0.5\ntemp = 1.0\nT0 T1 T2 T3 T4\n/Q_uery Transformations\n0.05\n0.10\n0.15\n0.20Coverage Ratio\ntemp = 0.0\ntemp = 0.5\ntemp = 1.0\nFigure 3: Jaccard index (left) and coverage ratio (right) be-\ntween the human and the GPT sets under different tempera-\nture ùë°ùëíùëöùëù settings. T0 denotes the original query set.\nThat is, if a GPT set fails to retrieve irrelevant documents that are\nretrieved by the human set, those documents will remain unjudged\nand thus treated as irrelevant in most effectiveness metrics, unless\nusing metrics that account for uncertainty (e.g., RBP).\nWe use the relevance judgments provided with the UQV100 col-\nlection and measure the overlap by considering relevant documents\nalone, i.e., those rated as Essential; Very Useful ; Mostly Useful or\nSlightly Useful .\nDifferent properties of the document pool are computed, mainly\nthe pool size growth following [14], to measure the diversity of the\nGPT sets in comparison to the human set. We hypothesize that a\ndiverse set of query variants given a topic is likely to retrieve differ-\nent documents leading to a higher growth rate than a set of similar\nqueries. This diversity is also examined using Rank-Biased Over-\nlap (RBO) [20] to quantify the consistency between the retrieved\ndocuments of the query variants given a topic. A topic RBO score\nis the average score over all topic-variant pairs. Different query\neffectiveness metrics are also computed for comparison.\n3 RESULTS AND DISCUSSION\nWe examine query and retrieval similarity.\n3.1 Query Sets Similarity\nFigure 3 shows the overlap between the human set and the GPT\nsets as measured by the Jaccard index and coverage ratio under\nfour matching conditions. Results indicate a minimum of 7.1% Jac-\ncard index between the GPT sets and the human set, with GPT\nsets demonstrating exact coverage of at least 10.3% of the human-\ngenerated queries. As expected, the queries demonstrate a greater\ndegree of overlap as they undergo successive transformations, ulti-\nmately reaching a maximum Jaccard index of 13.5% and coverage\nratio of 18.7% when using a temperature of 0.5.\nWhile the observed overlap does not seem to indicate a high\nsimilarity, they should be interpreted within the limitation of the\nUQV100 human set - which is still somewhat artificial. That is,\nit cannot be conclusively stated that the unique query variants\ngenerated by GPT cannot be written by humans should we have\nmore participants. The use of the UQV100 human-generated queries\nas a reference point to an ideal set of variants is not realistic, and\nwhile this comparison helps understand the capability of GPT, it\nmay limit our interpretation as an exhaustive set of query variants\ngiven a topic would never exist. Incorporating human evaluation to\nassess the extent to which the GPT sets approximate human queries\nSIGIR ‚Äô23, July 23‚Äì27, 2023, Taipei, Taiwan Marwah Alaofi, Luke Gallagher, Mark Sanderson, Falk Scholer, and Paul Thomas\nTable 2: Average effectiveness metrics, RBO and pool properties at depth 10 for the human set and the GPT sets given all variants\nacross all topics. RBP and RBO are measured using ùëù = 0.9. Entries annotated with ‚Ä† and ‚Ä° respectively indicate statistical\nsignificance for a Bonferroni pairwise ùë°-test at ùëù < 0.05 and ùëù < 0.01 compared to the human query set baseline. Topic 275 (the\nexample used to prompt the model) was removed from the computation and results are replicated independently.\nVariant Set P@10 NDCG@10 RBP RBO Pool Properties\nSize Relevant Unjudged\nHuman set 0.443 0.274 0.406 +0.111 0.201 190.69 0.30 0.13\nGPT (ùë°ùëíùëöùëù = 0.0) 0.386 ‚Ä° 0.246‚Ä† 0.358 +0.254‚Ä° 0.235‚Ä† 94.42 0.29 0.33\nGPT (ùë°ùëíùëöùëù = 0.5) 0.393 ‚Ä° 0.249‚Ä† 0.360 +0.238‚Ä° 0.220 93.55 0.29 0.31\nGPT (ùë°ùëíùëöùëù = 1.0) 0.384 ‚Ä° 0.240‚Ä° 0.355 +0.263‚Ä° 0.235‚Ä† 105.21 0.27 0.37\n10 20 30 40 50 60 70 80 90 100\nPool Depth\n0.23\n0.25\n0.27\n0.29\nDocument\nOverlap\ntemp = 0.0\ntemp = 0.5\ntemp = 1.0\n10 20 30 40 50 60 70 80 90 100\nPool Depth\n0.4\n0.5\n0.6\n0.7\nRel. Document\nOverlap\ntemp = 0.0\ntemp = 0.5\ntemp = 1.0\nFigure 4: The average Jaccard index between the documents\nretrieved by the human set and the GPT sets at different\ndepths given all documents (left) and relevant documents\nonly (right).\n1 5 9 13 17 21 25 29 33 37 41 45\nNumber of /Q_uery Variants\n50\n100\n150No. of Documents\nHuman Set\ntemp = 0.0\ntemp = 0.5\ntemp = 1.0\nFigure 5: The average pool size at depth 10 as more variants\nare added. The growth lines are cut at 48, which is the maxi-\nmum number of variants generated by GPT at ùë°ùëíùëöùëù = 1.\nmay yield more precise conclusions. This is, however, a question to\nbe explored in future research.\n3.2 Retrieval Similarity\nFigure 4 shows the overlap between the document pools generated\nin response to the human set and the GPT sets at different depths\nmeasured by the Jaccard index. A relatively high average overlap of\ndocument pools is observed between the GPT sets and the human\nset, which increases as we increase the depth. When considering\nrelevant documents only, the overlap is considerably high. With a\ntemperature of 1.0, for example, the pools overlap at 43.7% at depth\n10. This increases to 71.1% when examining the pools at depth 100.\nEffectiveness metrics, RBO scores and pool properties are given\nin Table 2. It is evident that the human-generated variants yield\na larger pool size, almost double that generated by any GPT sets,\nand which also grows faster (see Figure 5). This indicates a possible\nhigher diversity (e.g., more distinct query terms) in the human\nset. This observation is supported by a lower consistency of the\nvariants from the human set, as measured by RBO (significant with\nthe temperature set to 0.0 or 1.0).\nHuman-generated variants are significantly more effective across\nall metrics. GPT queries, on the other hand, have higher residuals\nwhich indicate that, given more judgments, they may achieve higher\neffectiveness scores. This is also reflected in the higher proportion\nof the unjudged documents returned in the GPT generated pools.\nIt would be interesting to further investigate the unjudged por-\ntion of the GPT sets to understand whether they retrieve relevant\ndocuments that were not found through the human set.\n4 CONCLUSIONS AND FUTURE WORK\nIn this paper, we posed the following questions:\nRQ1 Can an LLM, with one-shot learning, generate queries that\nare similar or perhaps identical to the ones generated by\nhumans?\nRQ2 How do both sets compare when used for document pooling\nwhen constructing test collections?\nWe found that for RQ1, GPT reproduced a reasonable portion\nof the human-generated queries. The similarity to human queries\nis yet to be fully understood given the limitation of the human set.\nFor RQ2, we found that GPT queries seem to have a substantial\noverlap in the pool of documents, particularly when we consider\nthe relevant set alone. At 71.1% overlap at depth 100, GPT shows\npotential for replacing human query variants with synthetically\ngenerated ones during document pool construction.\nThis work presents a new opportunity to conveniently expand\nexisting test collections, particularly those resembling TREC, which\nhave information need statements that can be employed to con-\ndition LLMs. Further research could explore advanced prompting\ntechniques and compare our approach of using an LLM to generate\nquery variants from backstories with previous query simulation\nmethods (e.g., [4, 11]), which were used to generate query variants\nfrom source documents.\nACKNOWLEDGMENTS\nMarwah Alaofi is supported by a scholarship from Taibah Uni-\nversity, Saudi Arabia. This work was also supported by the Aus-\ntralian Research Council (DP180102687, CE200100005). We thank\nthe anonymous reviewers for their helpful feedback.\nCan Generative LLMs Create Query Variants for Test Collections? SIGIR ‚Äô23, July 23‚Äì27, 2023, Taipei, Taiwan\nREFERENCES\n[1] Marwah Alaofi, Luke Gallagher, Dana Mckay, Lauren L. Saling, Mark Sanderson,\nFalk Scholer, Damiano Spina, and Ryen W. White. 2022. Where Do Queries Come\nFrom?. In Proceedings of the 45th International ACM SIGIR Conference on Research\nand Development in Information Retrieval . Association for Computing Machinery,\n2850‚Äì2862. https://doi.org/10.1145/3477495.3531711\n[2] Peter Bailey, Alistair Moffat, Falk Scholer, and Paul Thomas. 2016. UQV100: A\nTest Collection with Query Variability. InProceedings of the 39th International\nACM SIGIR Conference on Research and Development in Information Retrieval .\nAssociation for Computing Machinery, 725‚Äì728. https://doi.org/10.1145/2911451.\n2914671\n[3] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022.\nInPars: Unsupervised Dataset Generation for Information Retrieval. InProceedings\nof the 45th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval . Association for Computing Machinery, 2387‚Äì2392. https:\n//doi.org/10.1145/3477495.3531863\n[4] Timo Breuer, Norbert Fuhr, and Philipp Schaer. 2022. Validating Simulations\nof User Query Variants. In Advances in Information Retrieval - 44th European\nConference on IR Research . Springer, 80‚Äì94. https://doi.org/10.1007/978-3-030-\n99736-6_6\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and\nDario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in\nNeural Information Processing Systems . Curran Associates, Inc., 1877‚Äì1901.\n[6] Chris Buckley and Janet A Walz. 1999. The TREC-8 Query Track. In Proceeding\nof Text Retrieval conference . NIST Special Publication, 500‚Äì246.\n[7] J. Shane Culpepper, Guglielmo Faggioli, Nicola Ferro, and Oren Kurland. 2021.\nTopic Difficulty: Collection and Query Formulation Effects. ACM Transactions on\nInformation Systems 40, 1, Article 19 (2021), 36 pages. https://doi.org/10.1145/\n3470563\n[8] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,\nKelvin Guu, Keith B Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot\ndense retrieval from 8 examples. (2022). arXiv:2209.11755\n[9] Mitko Gospodinov, Sean MacAvaney, and Craig Macdonald. 2023. Doc2Query‚Äì:\nWhen Less is More. In Advances in Information Retrieval - 45th European Confer-\nence on IR Research . Springer, 414‚Äì422. https://doi.org/10.1007/978-3-031-28238-\n6_31\n[10] Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo,\nJakub Zavrel, and Rodrigo Nogueira. 2023. InPars-v2: Large Language Models as\nEfficient Dataset Generators for Information Retrieval. (2023). arXiv:2301.01820\n[11] Chris Jordan, Carolyn Watters, and Qigang Gao. 2006. Using Controlled Query\nGeneration to Evaluate Blind Relevance Feedback Algorithms. In Proceedings\nof the 6th ACM/IEEE-CS Joint Conference on Digital Libraries . Association for\nComputing Machinery, 286‚Äì295. https://doi.org/10.1145/1141753.1141818\n[12] Joel Mackenzie, Rodger Benham, Matthias Petri, Johanne R. Trippas, J. Shane\nCulpepper, and Alistair Moffat. 2020. CC-News-En: A Large English News\nCorpus. In Proceedings of the 29th ACM International Conference on Information\nand Knowledge Management . Association for Computing Machinery, 3077‚Äì3084.\nhttps://doi.org/10.1145/3340531.3412762\n[13] Alistair Moffat, Peter Bailey, Falk Scholer, and Paul Thomas. 2017. Incorporating\nUser Expectations and Behavior into the Measurement of Search Effectiveness.\nACM Transactions on Information Systems 35, 3, Article 24 (2017), 38 pages. https:\n//doi.org/10.1145/3052768\n[14] Alistair Moffat, Falk Scholer, Paul Thomas, and Peter Bailey. 2015. Pooled\nEvaluation Over Query Variations: Users Are as Diverse as Systems. In Pro-\nceedings of the 24th ACM International Conference on Information and Knowl-\nedge Management . Association for Computing Machinery, 1759‚Äì1762. https:\n//doi.org/10.1145/2806416.2806606\n[15] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\nexpansion by query prediction. (2019). arXiv:1904.08375\n[16] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,\nPeter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training\nlanguage models to follow instructions with human feedback. In Advances in\nNeural Information Processing Systems . Curran Associates, Inc., 27730‚Äì27744.\n[17] Gustavo Penha, Arthur C√¢mara, and Claudia Hauff. 2022. Evaluating the Ro-\nbustness of Retrieval Pipelines with Query Variation Generators. In Advances in\nInformation Retrieval - 44th European Conference on IR Research . Springer, 397‚Äì412.\nhttps://doi.org/10.1007/978-3-030-99736-6_27\n[18] Karen Sp√§rck Jones and R. Graham Bates. 1977. Report on a Design Study for\nthe ‚ÄúIdeal‚Äù Information Retrieval Test Collection . British Library Research and\nDevelopment Report No. 5428. University of Cambridge.\n[19] Ellen M. Voorhees. 2002. The Philosophy of Information Retrieval Evaluation. In\nWorkshop CLEF for European Languages . 355‚Äì370.\n[20] William Webber, Alistair Moffat, and Justin Zobel. 2010. A Similarity Measure\nfor Indefinite Rankings. ACM Transactions on Information Systems 28, 4, Article\n20 (2010), 38 pages. https://doi.org/10.1145/1852102.1852106\n[21] Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini: Enabling the Use of\nLucene for Information Retrieval Research. InProceedings of the 40th International\nACM SIGIR Conference on Research and Development in Information Retrieval .\nAssociation for Computing Machinery, 1253‚Äì1256. https://doi.org/10.1145/\n3077136.3080721\n[22] Hongfei Zhang, Xia Song, Chenyan Xiong, Corby Rosset, Paul N. Bennett, Nick\nCraswell, and Saurabh Tiwary. 2019. Generic Intent Representation in Web\nSearch. In Proceedings of the 42nd International ACM SIGIR Conference on Research\nand Development in Information Retrieval . Association for Computing Machinery,\n65‚Äì74. https://doi.org/10.1145/3331184.3331198"
}