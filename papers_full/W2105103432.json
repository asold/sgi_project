{
  "title": "Language Models for Image Captioning: The Quirks and What Works",
  "url": "https://openalex.org/W2105103432",
  "year": 2015,
  "authors": [
    {
      "id": "https://openalex.org/A2105712977",
      "name": "Jacob Devlin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2005151083",
      "name": "Cheng Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098366404",
      "name": "Hao Fang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099518497",
      "name": "Saurabh Gupta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2042328760",
      "name": "Li Deng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122755126",
      "name": "Xiaodong He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2090483681",
      "name": "Geoffrey Zweig",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105617194",
      "name": "Margaret Mitchell",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2962706528",
    "https://openalex.org/W1947481528",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2250489405",
    "https://openalex.org/W1489525520",
    "https://openalex.org/W1897761818",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W1895989618",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2146574666",
    "https://openalex.org/W68733909",
    "https://openalex.org/W2171361956",
    "https://openalex.org/W2951183276"
  ],
  "abstract": "Two recent approaches have achieved state-of-the-art results in image caption-ing. The first uses a pipelined process where a set of candidate words is gen-erated by a convolutional neural network (CNN) trained on images, and then a max-imum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this pa-per, we compare the merits of these dif-ferent language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine is-sues in the different approaches, includ-ing linguistic irregularities, caption repe-tition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the bench-mark COCO dataset. However, the gains we see in BLEU do not translate to human judgments. 1",
  "full_text": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 100–105,\nBeijing, China, July 26-31, 2015.c⃝2015 Association for Computational Linguistics\nLanguage Models for Image Captioning: The Quirks and What Works\nJacob Devlin⋆, Hao Cheng♠, Hao Fang♠, Saurabh Gupta♣,\nLi Deng, Xiaodong He⋆, Geoffrey Zweig⋆, Margaret Mitchell⋆\nMicrosoft Research\n⋆ Corresponding authors: {jdevlin,xiaohe,gzweig,memitc}@microsoft.com\n♠ University of Washington\n♣ University of California at Berkeley\nAbstract\nTwo recent approaches have achieved\nstate-of-the-art results in image caption-\ning. The ﬁrst uses a pipelined process\nwhere a set of candidate words is gen-\nerated by a convolutional neural network\n(CNN) trained on images, and then a max-\nimum entropy (ME) language model is\nused to arrange these words into a coherent\nsentence. The second uses the penultimate\nactivation layer of the CNN as input to a\nrecurrent neural network (RNN) that then\ngenerates the caption sequence. In this pa-\nper, we compare the merits of these dif-\nferent language modeling approaches for\nthe ﬁrst time by using the same state-of-\nthe-art CNN as input. We examine is-\nsues in the different approaches, includ-\ning linguistic irregularities, caption repe-\ntition, and data set overlap. By combining\nkey aspects of the ME and RNN methods,\nwe achieve a new record performance over\npreviously published results on the bench-\nmark COCO dataset. However, the gains\nwe see in BLEU do not translate to human\njudgments.\n1 Introduction\nRecent progress in automatic image captioning\nhas shown that an image-conditioned language\nmodel can be very effective at generating captions.\nTwo leading approaches have been explored for\nthis task. The ﬁrst decomposes the problem into\nan initial step that uses a convolutional neural net-\nwork to predict a bag of words that are likely to\nbe present in a caption; then in a second step, a\nmaximum entropy language model (ME LM) is\nused to generate a sentence that covers a mini-\nmum number of the detected words (Fang et al.,\n2015). The second approach uses the activations\nfrom ﬁnal hidden layer of an object detection CNN\nas the input to a recurrent neural network lan-\nguage model (RNN LM). This is referred to as a\nMultimodal Recurrent Neural Network (MRNN)\n(Karpathy and Fei-Fei, 2015; Mao et al., 2015;\nChen and Zitnick, 2015). Similar in spirit is the\nthe log-bilinear (LBL) LM of Kiros et al. (2014).\nIn this paper, we study the relative merits of\nthese approaches. By using an identical state-of-\nthe-art CNN as the input to RNN-based and ME-\nbased models, we are able to empirically com-\npare the strengths and weaknesses of the lan-\nguage modeling components. We ﬁnd that the\napproach of directly generating the text with an\nMRNN1 outperforms the ME LM when measured\nby BLEU on the COCO dataset (Lin et al., 2014),2\nbut this recurrent model tends to reproduce cap-\ntions in the training set. In fact, a simple k-nearest\nneighbor approach, which is common in earlier re-\nlated work (Farhadi et al., 2010; Mason and Char-\nniak, 2014), performs similarly to the MRNN. In\ncontrast, the ME LM generates the most novel\ncaptions, and does the best at captioning images\nfor which there is no close match in the training\ndata. With a Deep Multimodal Similarity Model\n(DMSM) incorporated,3 the ME LM signiﬁcantly\noutperforms other methods according to human\njudgments. In sum, the contributions of this pa-\nper are as follows:\n1. We compare the use of discrete detections\nand continuous valued CNN activations as\nthe conditioning information for language\nmodels trained to generate image captions.\n2. We show that a simple k-nearest neighbor re-\ntrieval method performs at near state-of-the-\nart for this task and dataset.\n3. We demonstrate that a state-of-the-art\n1In our case, a gated recurrent neural network (GRNN) is\nused (Cho et al., 2014), similar to an LSTM.\n2This is the largest image captioning dataset to date.\n3As described by Fang et al. (2015).\n100\nMRNN-based approach tends to reconstruct\npreviously seen captions; in contrast, the\ntwo stage ME LM approach achieves similar\nor better performance while generating\nrelatively novel captions.\n4. We advance the state-of-the-art BLEU scores\non the COCO dataset.\n5. We present human evaluation results on the\nsystems with the best performance as mea-\nsured by automatic metrics.\n6. We explore several issues with the statistical\nmodels and the underlying COCO dataset, in-\ncluding linguistic irregularities, caption repe-\ntition, and data set overlap.\n2 Models\nAll language models compared here are trained\nusing output from the same state-of-the-art CNN.\nThe CNN used is the 16-layer variant of VGGNet\n(Simonyan and Zisserman, 2014) which was ini-\ntially trained for the ILSVRC2014 classiﬁcation\ntask (Russakovsky et al., 2015), and then ﬁne-\ntuned on the Microsoft COCO data set (Fang et\nal., 2015; Lin et al., 2014).\n2.1 Detector Conditioned Models\nWe study the effect of leveraging an explicit de-\ntection step to ﬁnd key objects/attributes in images\nbefore generation, examining both an ME LM ap-\nproach as reported in previous work (Fang et al.,\n2015), and a novel LSTM approach introduced\nhere. Both use a CNN trained to output a bag of\nwords indicating the words that are likely to ap-\npear in a caption, and both use a beam search to\nﬁnd a top-scoring sentence that contains a subset\nof the words. This set of words is dynamically ad-\njusted to remove words as they are mentioned.\nWe refer the reader to Fang et al. (2015) for a\nfull description of their ME LM approach, whose\n500-best outputs we analyze here. 4 We also in-\nclude the output from their ME LM that leverages\nscores from a Deep Multimodal Similarity Model\n(DMSM) during n-best re-ranking. Brieﬂy, the\nDMSM is a non-generative neural network model\nwhich projects both the image pixels and caption\ntext into a comparable vector space, and scores\ntheir similarity.\nIn the LSTM approach, similar to the ME LM\napproach, we maintain a set of likely wordsDthat\n4We will refer to this system as D-ME.\nhave not yet been mentioned in the caption un-\nder construction. This set is initialized to all the\nwords predicted by the CNN above some thresh-\nold α.5 The words already mentioned in the\nsentence history h are then removed to produce\na set of conditioning words D\\{ h}. We in-\ncorporate this information within the LSTM by\nadding an additional input encoded to represent\nthe remaining visual attributes D\\{ h}as a con-\ntinuous valued auxiliary feature vector (Mikolov\nand Zweig, 2012). This is encoded as f(sh−1 +∑\nv∈D\\{h}gv + Uqh,D), where sh−1 and gv are\nrespectively the continuous-space representations\nfor last word h−1 and detector v ∈D\\{ h}, U is\nlearned matrix for recurrent histories, and f(·) is\nthe sigmoid transformation.\n2.2 Multimodal Recurrent Neural Network\nIn this section, we explore a model directly con-\nditioned on the CNN activations rather than a set\nof word detections. Our implementation is very\nsimilar to captioning models described in Karpa-\nthy and Fei-Fei (2015), Vinyals et al. (2014), Mao\net al. (2015), and Donahue et al. (2014). This\njoint vision-language RNN is referred to as a Mul-\ntimodal Recurrent Neural Network (MRNN).\nIn this model, we feed each image into our\nCNN and retrieve the 4096-dimensional ﬁnal hid-\nden layer, denoted as fc7. The fc7 vector is\nthen fed into a hidden layer H to obtain a 500-\ndimensional representation that serves as the ini-\ntial hidden state to a gated recurrent neural net-\nwork (GRNN) (Cho et al., 2014). The GRNN\nis trained jointly with H to produce the caption\none word at a time, conditioned on the previous\nword and the previous recurrent state. For decod-\ning, we perform a beam search of size 10 to emit\ntokens until an END token is produced. We use\na 500-dimensional GRNN hidden layer and 200-\ndimensional word embeddings.\n2.3 k-Nearest Neighbor Model\nBoth Donahue et al. (2015) and Karpathy and Fei-\nFei (2015) present a 1-nearest neighbor baseline.\nAs a ﬁrst step, we replicated these results using the\ncosine similarity of the fc7 layer between each\ntest set imagetand training imager. We randomly\nemit one caption fromt’s most similar training im-\nage as the caption of t. As reported in previous\nresults, performance is quite poor, with a BLEU\n5In all experiments in this paper, α=0.5.\n101\nFigure 1: Example of the set of candidate captions for an\nimage, the highest scoring mcaptions (green) and the con-\nsensus caption (orange). This is a real example visualized in\ntwo dimensions.\nscore of 11.2%.\nHowever, we explore the idea that we may be\nable to ﬁnd an optimal k-nearest neighbor consen-\nsus caption . We ﬁrst select the k = 90 nearest\ntraining images of a test image tas above. We de-\nnote the union of training captions in this set as\nC = c1,...,c 5k.6 For each caption ci, we com-\npute the n-gram overlap F-score between ci and\neach other caption in C. We deﬁne the consen-\nsus caption c∗to be caption with the highest mean\nn-gram overlap with the other captions in C. We\nhave found it is better to only compute this average\namong ci’sm= 125most similar captions, rather\nthan all of C. The hyperparameters kand mwere\nobtained by a grid search on the validation set.\nA visual example of the consensus caption is\ngiven in Figure 1. Intuitively, we are choosing\na single caption that may describe many different\nimages that are similar to t, rather than a caption\nthat describes the single image that is most similar\nto t. We believe that this is a reasonable approach\nto take for a retrieval-based method for captioning,\nas it helps ensure incorrect information is not men-\ntioned. Further details on retrieval-based methods\nare available in, e.g., (Ordonez et al., 2011; Ho-\ndosh et al., 2013).\n3 Experimental Results\n3.1 The Microsoft COCO Dataset\nWe work with the Microsoft COCO dataset (Lin\net al., 2014), with 82,783 training images, and\nthe validation set split into 20,243 validation im-\nages and 20,244 testval images. Most images con-\ntain multiple objects and signiﬁcant contextual in-\nformation, and each image comes with 5 human-\n6Each training image has 5 captions.\nLM PPLX BLEU METEOR\nD-ME† 18.1 23.6 22.8\nD-LSTM 14.3 22.4 22.6\nMRNN 13.2 25.7 22.6\nk-Nearest Neighbor - 26.0 22.5\n1-Nearest Neighbor - 11.2 17.3\nTable 1:Model performance on testval. †: From (Fang et al.,\n2015).\nD-ME+DMSMa plate with a sandwich and a cup of coffee\nMRNN a close up of a plate of food\nD-ME+DMSM+MRNNa plate of food and a cup of coffee\nk-NN a cup of coffee on a plate with a spoon\nD-ME+DMSMa black bear walking across a lush green forest\nMRNN a couple of bears walking across a dirt road\nD-ME+DMSM+MRNNa black bear walking through a wooded area\nk-NN a black bear that is walking in the woods\nD-ME+DMSMa gray and white cat sitting on top of it\nMRNN a cat sitting in front of a mirror\nD-ME+DMSM+MRNNa close up of a cat looking at the camera\nk-NN a cat sitting on top of a wooden table\nTable 2:Example generated captions.\nannotated captions. The images create a challeng-\ning testbed for image captioning and are widely\nused in recent automatic image captioning work.\n3.2 Metrics\nThe quality of generated captions is measured au-\ntomatically using BLEU (Papineni et al., 2002)\nand METEOR (Denkowski and Lavie, 2014).\nBLEU roughly measures the fraction of N-grams\n(up to 4 grams) that are in common between a hy-\npothesis and one or more references, and penalizes\nshort hypotheses by a brevity penalty term. 7 ME-\nTEOR (Denkowski and Lavie, 2014) measures un-\nigram precision and recall, extending exact word\nmatches to include similar words based on Word-\nNet synonyms and stemmed tokens. We also re-\nport the perplexity (PPLX) of studied detection-\nconditioned LMs. The PPLX is in many ways\nthe natural measure of a statistical LM, but can be\nloosely correlated with BLEU (Auli et al., 2013).\n3.3 Model Comparison\nIn Table 1, we summarize the generation perfor-\nmance of our different models. The discrete de-\ntection based models are preﬁxed with “D”. Some\nexample generated results are show in Table 2.\nWe see that the detection-conditioned LSTM\nLM produces much lower PPLX than the\ndetection-conditioned ME LM, but its BLEU\nscore is no better. The MRNN has the lowest\nPPLX, and highest BLEU among all LMs stud-\n7We use the length of the reference that is closest to the\nlength of the hypothesis to compute the brevity penalty.\n102\nRe-Ranking Features BLEU METEOR\nD-ME † 23.6 22.8\n+ DMSM † 25.7 23.6\n+ MRNN 26.8 23.3\n+ DMSM + MRNN 27.3 23.6\nTable 3: Model performance on testval after re-ranking.\n†: previously reported and reconﬁrmed BLEU scores from\n(Fang et al., 2015). +DMSM had resulted in the highest score\nyet reported.\nied in our experiments. It signiﬁcantly improves\nBLEU by 2.1 absolutely over the D-ME LM base-\nline. METEOR is similar across all three LM-\nbased methods.\nPerhaps most surprisingly, the k-nearest neigh-\nbor algorithm achieves a higher BLEU score than\nall other models. However, as we will demonstrate\nin Section 3.5, the generated captions perform sig-\nniﬁcantly better than the nearest neighbor captions\nin terms of human quality judgements.\n3.4 n-best Re-Ranking\nIn addition to comparing the ME-based and RNN-\nbased LMs independently, we explore whether\ncombining these models results in an additive im-\nprovement. To this end, we use the 500-best list\nfrom the D-ME and add a score for each hypoth-\nesis from the MRNN. 8 We then re-rank the hy-\npotheses using MERT (Och, 2003). As in previous\nwork (Fang et al., 2015), model weights were opti-\nmized to maximize BLEU score on the validation\nset. We further extend this combination approach\nto the D-ME model with DMSM scores included\nduring re-ranking (Fang et al., 2015).\nResults are show in Table 3. We ﬁnd that com-\nbining the D-ME, DMSM, and MRNN achieves a\n1.6 BLEU improvement over the D-ME+DMSM.\n3.5 Human Evaluation\nBecause automatic metrics do not always corre-\nlate with human judgments (Callison-Burch et al.,\n2006; Hodosh et al., 2013), we also performed hu-\nman evaluations using the same procedure as in\nFang et al. (2015). Here, human judges were pre-\nsented with an image, a system generated caption,\nand a human generated caption, and were asked\nwhich caption was “better”.9 For each condition,\n5 judgments were obtained for 1000 images from\nthe testval set.\n8The MRNN does not produce a diverse n-best list.\n9The captions were randomized and the users were not\ninformed which was which.\nResults are shown in Table 4. The D-\nME+DMSM outperforms the MRNN by 5 per-\ncentage points for the “Better Or Equal to Hu-\nman” judgment, despite both systems achieving\nthe same BLEU score. The k-Nearest Neighbor\nsystem performs 1.4 percentage points worse than\nthe MRNN, despite achieving a slightly higher\nBLEU score. Finally, the combined model does\nnot outperform the D-ME+DMSM in terms of hu-\nman judgments despite a 1.6 BLEU improvement.\nAlthough we cannot pinpoint the exact reason\nfor this mismatch between automated scores and\nhuman evaluation, a more detailed analysis of the\ndifference between systems is performed in Sec-\ntions 4 and 5.\nHuman Judgements\nBetter Better\nApproach or Equal BLEU\nD-ME+DMSM 7.8% 34.0% 25.7\nMRNN 8.8% 29.0% 25.7\nD-ME+DMSM+MRNN 5.7% 34.2% 27.3\nk-Nearest Neighbor 5.5% 27.6% 26.0\nTable 4:Results when comparing produced captions to those\nwritten by humans, as judged by humans. These are the per-\ncent of captions judged to be “better than” or “better than or\nequal to” a caption written by a human.\n4 Language Analysis\nExamples of common mistakes we observe on the\ntestval set are shown in Table 5. The D-ME system\nhas difﬁculty with anaphora, particularly within\nthe phrase “on top of it”, as shown in examples\n(1), (2), and (3). This is likely due to the fact that is\nmaintains a local context window. In contrast, the\nMRNN approach tends to generate such anaphoric\nrelationships correctly.\nHowever, the D-ME LM maintains an explicit\ncoverage state vector tracking which attributes\nhave already been emitted. The MRNN implicitly\nmaintains the full state using its recurrent layer,\nwhich sometimes results in multiple emission mis-\ntakes, where the same attribute is emitted more\nthan once. This is particularly evident when coor-\ndination (“and”) is present (examples (4) and (5)).\n4.1 Repeated Captions\nAll of our models produce a large number of cap-\ntions seen in the training and repeated for differ-\nent images in the test set, as shown in Table 6\n(also observed by Vinyals et al. (2014) for their\nLSTM-based model). There are at least two po-\ntential causes for this repetition.\n103\nD-ME+DMSM MRNN\n1 a slice of pizza sitting on top of ita bed with a red blanket on top of it\n2 a black and white bird perched on\ntop of it\na birthday cake with candles on top\nof it\n3 a little boy that is brushing his\nteeth with a toothbrush in her\nmouth\na little girl brushing her teeth with a\ntoothbrush\n4 a large bed sitting in a bedrooma bedroom with a bed and a bed\n5 a man wearing a bow tie a man wearing a tie and a tie\nTable 5:Example errors in the two basic approaches.\nSystem Unique Seen In\nCaptions Training\nHuman 99.4% 4.8%\nD-ME+DMSM 47.0% 30.0%\nMRNN 33.1% 60.3%\nD-ME+DMSM+MRNN 28.5% 61.3%\nk-Nearest Neighbor 36.6% 100%\nTable 6: Percentage unique (Unique Captions) and novel\n(Seen In Training) captions for testval images. For example,\n28.5% unique means 5,776 unique strings were generated for\nall 20,244 images.\nFirst, the systems often produce generic cap-\ntions such as “a close up of a plate of food”, which\nmay be applied to many publicly available im-\nages. This may suggest a deeper issue in the train-\ning and evaluation of our models, which warrants\nmore discussion in future work. Second, although\nthe COCO dataset and evaluation server10 has en-\ncouraged rapid progress in image captioning, there\nmay be a lack of diversity in the data. We also note\nthat although caption duplication is an issue in all\nsystems, it is a greater issue in the MRNN than the\nD-ME+DMSM.\n5 Image Diversity\nThe strong performance of the k-nearest neighbor\nalgorithm and the large number of repeated cap-\ntions produced by the systems here suggest a lack\nof diversity in the training and test data.11\nWe believe that one reason to work on image\ncaptioning is to be able to captioncompositionally\nnovel images, where the individual components of\nthe image may be seen in the training, but the en-\ntire composition is often not.\nIn order to evaluate results for only compo-\nsitionally novel images, we bin the test images\nbased on visual overlap with the training data.\nFor each test image, we compute the fc7 cosine\nsimilarity with each training image, and the mean\nvalue of the 50 closest images. We then compute\nBLEU on the 20% least overlapping and 20% most\n10http://mscoco.org/dataset/\n11This is partially an artifact of the manner in which the\nMicrosoft COCO data set was constructed, since each image\nwas chosen to be in one of 80 pre-deﬁned object categories.\nCondition Train/Test Visual Overlap\nBLEU\nWhole 20% 20%\nSet Least Most\nD-ME+DMSM 25.7 20.9 29.9\nMRNN 25.7 18.8 32.0\nD-ME+DMSM+MRNN 27.3 21.7 32.0\nk-Nearest Neighbor 26.0 18.4 33.2\nTable 7:Performance for different portions of testval, based\non visual overlap with the training.\noverlapping subsets.\nResults are shown in Table 7. The D-\nME+DMSM outperforms the k-nearest neighbor\napproach by 2.5 BLEU on the “20% Least” set,\neven though performance on the whole set is com-\nparable. Additionally, the D-ME+DMSM out-\nperforms the MRNN by 2.1 BLEU on the “20%\nLeast” set, but performs 2.1 BLEU worse on\nthe “20% Most” set. This is evidence that D-\nME+DMSM generalizes better on novel images\nthan the MRNN; this is further supported by the\nrelatively low percentage of captions it gener-\nates seen in the training data (Table 6) while still\nachieving reasonable captioning performance. We\nhypothesize that these are the main reasons for\nthe strong human evaluation results of the D-\nME+DMSM shown in Section 3.5.\n6 Conclusion\nWe have shown that a gated RNN conditioned di-\nrectly on CNN activations (an MRNN) achieves\nbetter BLEU performance than an ME LM or\nLSTM conditioned on a set of discrete activations;\nand a similar BLEU performance to an ME LM\ncombined with a DMSM. However, the ME LM\n+ DMSM method signiﬁcantly outperforms the\nMRNN in terms of human quality judgments. We\nhypothesize that this is partially due to the lack of\nnovelty in the captions produced by the MRNN.\nIn fact, a k-nearest neighbor retrieval algorithm\nintroduced in this paper performs similarly to the\nMRNN in terms of both automatic metrics and hu-\nman judgements.\nWhen we use the MRNN system alongside the\nDMSM to provide additional scores in MERT re-\nranking of the n-best produced by the image-\nconditioned ME LM, we advance by 1.6 BLEU\npoints on the best previously published results on\nthe COCO dataset. Unfortunately, this improve-\nment in BLEU does not translate to improved hu-\nman quality judgments.\n104\nReferences\nMichael Auli, Michel Galley, Chris Quirk, and Ge-\noffrey Zweig. 2013. Joint language and transla-\ntion modeling with recurrent neural networks. In\nProc. Conf. Empirical Methods Natural Language\nProcess. (EMNLP), pages 1044–1054.\nChris Callison-Burch, Miles Osborne, and Philipp\nKoehn. 2006. Re-evaluation the role of bleu in\nmachine translation research. In EACL, volume 6,\npages 249–256.\nXinlei Chen and C. Lawrence Zitnick. 2015. Mind’s\neye: A recurrent visual representation for image cap-\ntion generation. In Proc. Conf. Comput. Vision and\nPattern Recognition (CVPR).\nKyunghyun Cho, Bart van Merrienboer, Caglar Gul-\ncehre, Fethi Bougares, Holger Schwenk, and Yoshua\nBengio. 2014. Learning phrase representations\nusing RNN encoder-decoder for statistical machine\ntranslation. CoRR.\nMichael Denkowski and Alon Lavie. 2014. Meteor\nuniversal: language speciﬁc translation evaluation\nfor any target language. In Proc. EACL 2014 Work-\nshop Statistical Machine Translation.\nJeff Donahue, Lisa Anne Hendricks, Sergio Guadar-\nrama, Marcus Rohrbach, Subhashini Venugopalan,\nKate Saenko, and Trevor Darrell. 2014. Long-term\nrecurrent convolutional networks for visual recogni-\ntion and description. arXiv:1411.4389 [cs.CV].\nJeffrey Donahue, Lisa Anne Hendricks, Sergio Guadar-\nrama, Marcus Rohrbach, Subhashini Venugopalan,\nKate Saenko, and Trevor Darrell. 2015. Long-term\nrecurrent convolutional networks for visual recogni-\ntion and description. In Proc. Conf. Comput. Vision\nand Pattern Recognition (CVPR).\nHao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Sri-\nvastava, Li Deng, Piotr Doll ´a, Margaret Mitchell,\nJohn C. Platt, C. Lawrence Zitnick, and Geoffrey\nZweig. 2015. From captionons to visual concepts\nand back. In Proc. Conf. Comput. Vision and Pat-\ntern Recognition (CVPR).\nAli Farhadi, Mohsen Hejrati, Mohammad Amin\nSadeghi, Peter Young, Cyrus Rashtchian, Julia\nHockenmaier, and David Forsyth. 2010. Every pic-\nture tells a story: generating sentences from images.\nIn Proc. European Conf. Comput. Vision (ECCV) ,\npages 15–29.\nMicah Hodosh, Peter Young, and Julia Hockenmaier.\n2013. Framing image description as a ranking task:\ndata models and evaluation metrics. J. Artiﬁcial In-\ntell. Research, pages 853–899.\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In Proc. Conf. Comput. Vision and Pattern\nRecognition (CVPR).\nRyan Kiros, Ruslan Salakhutdinov, and Richard Zemel.\n2014. Multimodal neural language models. In Proc.\nInt. Conf. Machine Learning (ICML).\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar,\nand C. Lawrence Zitnick. 2014. Microsoft COCO:\nCommon objects in context. arXiv:1405.0312\n[cs.CV].\nJunhua Mao, Wei Xu, Yi Yang, Jiang Wang, and\nAlan L. Yuille. 2015. Deep captioning with multi-\nmodal recurrent neural networks (m-RNN). In Proc.\nInt. Conf. Learning Representations (ICLR).\nRebecca Mason and Eugene Charniak. 2014. Domain-\nspeciﬁc image captioning. In CoNLL.\nTomas Mikolov and Geoffrey Zweig. 2012. Context\ndependent recurrent neural network language model.\nIn SLT, pages 234–239.\nFranz Josef Och. 2003. Minimum error rate training in\nstatistical machine translation. In ACL, ACL ’03.\nVicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\n2011. Im2Text: Describing images using 1 million\ncaptioned photogrphs. In Proc. Annu. Conf. Neural\nInform. Process. Syst. (NIPS).\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: a method for automatic\nevaluation of machine translation. In Proc. Assoc.\nfor Computational Linguistics (ACL) , pages 311–\n318.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,\nSanjeev Satheesh, Sean Ma, Zhiheng Huang, An-\ndrej Karpathy, Aditya Khosla, Michael Bernstein,\nAlexander C. Berg, and Li Fei-Fei. 2015. ImageNet\nLarge Scale Visual Recognition Challenge. Interna-\ntional Journal of Computer Vision (IJCV).\nKaren Simonyan and Andrew Zisserman. 2014. Very\ndeep convolutional networks for large-scale image\nrecognition. arXiv preprint.\nOriol Vinyals, Alexander Toshev, Samy Bengio, and\nDumitru Erhan. 2014. Show and tell: a neural im-\nage caption generator. In Proc. Conf. Comput. Vi-\nsion and Pattern Recognition (CVPR).\n105",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9517147541046143
    },
    {
      "name": "Computer science",
      "score": 0.6940396428108215
    },
    {
      "name": "Image (mathematics)",
      "score": 0.5689435005187988
    },
    {
      "name": "Natural language processing",
      "score": 0.4142306447029114
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38406288623809814
    },
    {
      "name": "Linguistics",
      "score": 0.3739343285560608
    },
    {
      "name": "Speech recognition",
      "score": 0.36835089325904846
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}