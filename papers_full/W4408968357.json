{
  "title": "Fine-tuning large language models for domain adaptation: exploration of training strategies, scaling, model merging and synergistic capabilities",
  "url": "https://openalex.org/W4408968357",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A1978197011",
      "name": "Wei Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4293662007",
      "name": "Rachel K. Luu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2250330840",
      "name": "Markus J. Buehler",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3164045210",
    "https://openalex.org/W6858023062",
    "https://openalex.org/W4387521449",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4320713584",
    "https://openalex.org/W4398777692",
    "https://openalex.org/W4387773384",
    "https://openalex.org/W4390195781",
    "https://openalex.org/W4255703567",
    "https://openalex.org/W4308150738",
    "https://openalex.org/W4387934665",
    "https://openalex.org/W4388263589",
    "https://openalex.org/W4401722919",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W4399990959",
    "https://openalex.org/W4378771755",
    "https://openalex.org/W4404782689",
    "https://openalex.org/W2626804490",
    "https://openalex.org/W4404783473",
    "https://openalex.org/W2147774191",
    "https://openalex.org/W1977259876",
    "https://openalex.org/W4239515156",
    "https://openalex.org/W2963518130",
    "https://openalex.org/W2135293965",
    "https://openalex.org/W6849590751",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4385571232",
    "https://openalex.org/W4393161118",
    "https://openalex.org/W4386072096",
    "https://openalex.org/W4402269933",
    "https://openalex.org/W6772383348",
    "https://openalex.org/W4394729416",
    "https://openalex.org/W4390849566",
    "https://openalex.org/W4321604984",
    "https://openalex.org/W4389502177",
    "https://openalex.org/W4392975665",
    "https://openalex.org/W6857464309",
    "https://openalex.org/W4389267611"
  ],
  "abstract": "Abstract The advancement of Large Language Models (LLMs) for domain applications in fields such as materials science and engineering depends on the development of fine-tuning strategies that adapt models for specialized, technical capabilities. In this work, we explore the effects of Continued Pretraining (CPT), Supervised Fine-Tuning (SFT), and various preference-based optimization approaches, including Direct Preference Optimization (DPO) and Odds Ratio Preference Optimization (ORPO), on fine-tuned LLM performance. Our analysis shows how these strategies influence model outcomes and reveals that the merging of multiple fine-tuned models can lead to the emergence of capabilities that surpass the individual contributions of the parent models. We find that model merging is not merely a process of aggregation, but a transformative method that can drive substantial advancements in model capabilities characterized by highly nonlinear interactions between model parameters, resulting in new functionalities that neither parent model could achieve alone, leading to improved performance in domain-specific assessments. We study critical factors that influence the success of model merging, such as the diversity between parent models and the fine-tuning techniques employed. The insights underscore the potential of strategic model merging to unlock novel capabilities in LLMs, offering an effective tool for advancing AI systems to meet complex challenges. Experiments with different model architectures are presented, including the Llama 3.1 8B and Mistral 7B family of models, where similar behaviors are observed. Exploring whether the results hold also for much smaller models, we use a tiny LLM with 1.7 billion parameters and show that very small LLMs do not necessarily feature emergent capabilities under model merging, suggesting that model scaling may be a key component. In open-ended yet consistent chat conversations between a human and AI models, our assessment reveals detailed insights into how different model variants perform, and shows that the smallest model achieves a high intelligence score across key criteria including reasoning depth, creativity, clarity, and quantitative precision. Other experiments include the development of image generation prompts that seek to reason over disparate biological material design concepts, to create new microstructures, architectural concepts, and urban design based on biological materials-inspired construction principles. We conclude with a series of questions about scaling and emergence that could be addressed in future research.",
  "full_text": null,
  "topic": "Adaptation (eye)",
  "concepts": [
    {
      "name": "Adaptation (eye)",
      "score": 0.7117904424667358
    },
    {
      "name": "Fine-tuning",
      "score": 0.6734062433242798
    },
    {
      "name": "Computer science",
      "score": 0.6169540882110596
    },
    {
      "name": "Scaling",
      "score": 0.5997056365013123
    },
    {
      "name": "Domain adaptation",
      "score": 0.571763813495636
    },
    {
      "name": "Language model",
      "score": 0.5587923526763916
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5547856688499451
    },
    {
      "name": "Training (meteorology)",
      "score": 0.4270651042461395
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2849646806716919
    },
    {
      "name": "Psychology",
      "score": 0.1343662142753601
    },
    {
      "name": "Physics",
      "score": 0.07644972205162048
    },
    {
      "name": "Neuroscience",
      "score": 0.057023197412490845
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 26
}