{
    "title": "A multimodal vision foundation model for clinical dermatology",
    "url": "https://openalex.org/W4411099958",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2751735091",
            "name": "Yan, Siyuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1986518859",
            "name": "Yu Zhen",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Primiero, Clare",
            "affiliations": [
                "University of Queensland"
            ]
        },
        {
            "id": null,
            "name": "Vico-Alonso, Cristina",
            "affiliations": [
                "The Alfred Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A1926312414",
            "name": "Wang Zhong-hua",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1908722779",
            "name": "Yang Li-tao",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A4288628820",
            "name": "Tschandl, Philipp",
            "affiliations": [
                "Medical University of Vienna"
            ]
        },
        {
            "id": "https://openalex.org/A2088584508",
            "name": "Hu Ming",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A2995034569",
            "name": "Ju, Lie",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Tan, Gin",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A3015094658",
            "name": "Tang Vincent",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Ng, Aik Beng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2673844803",
            "name": "Powell David",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A4227702423",
            "name": "Bonnington, Paul",
            "affiliations": [
                "University of Queensland"
            ]
        },
        {
            "id": "https://openalex.org/A3003914543",
            "name": "See, Simon",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Magnaterra, Elisabetta",
            "affiliations": [
                "University of Florence"
            ]
        },
        {
            "id": "https://openalex.org/A3176478882",
            "name": "Ferguson Peter",
            "affiliations": [
                "Royal Prince Alfred Hospital",
                "Melanoma Institute Australia"
            ]
        },
        {
            "id": "https://openalex.org/A2654689820",
            "name": "Nguyen Jennifer",
            "affiliations": [
                "The Alfred Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A4288628807",
            "name": "Guitera, Pascale",
            "affiliations": [
                "The Alfred Hospital",
                "Royal Prince Alfred Hospital"
            ]
        },
        {
            "id": null,
            "name": "Banuls, Jose",
            "affiliations": [
                "Hospital General Universitario de Alicante Doctor Balmis"
            ]
        },
        {
            "id": "https://openalex.org/A4323217825",
            "name": "Janda, Monika",
            "affiliations": [
                "University of Queensland"
            ]
        },
        {
            "id": "https://openalex.org/A4295897452",
            "name": "Mar, Victoria",
            "affiliations": [
                "The Alfred Hospital",
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A4288628810",
            "name": "Kittler, Harald",
            "affiliations": [
                "Medical University of Vienna"
            ]
        },
        {
            "id": "https://openalex.org/A3168814198",
            "name": "Soyer H. Peter",
            "affiliations": [
                "University of Queensland",
                "Princess Alexandra Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2748644016",
            "name": "Ge, Zongyuan",
            "affiliations": [
                "Monash University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2164273268",
        "https://openalex.org/W4390956542",
        "https://openalex.org/W4394618970",
        "https://openalex.org/W2792680748",
        "https://openalex.org/W2141393487",
        "https://openalex.org/W3025011581",
        "https://openalex.org/W3105070630",
        "https://openalex.org/W2892053105",
        "https://openalex.org/W4386367858",
        "https://openalex.org/W4387259074",
        "https://openalex.org/W1963543311",
        "https://openalex.org/W2008378711",
        "https://openalex.org/W4392805036",
        "https://openalex.org/W4400272378",
        "https://openalex.org/W4392377312",
        "https://openalex.org/W4365143687",
        "https://openalex.org/W4291023040",
        "https://openalex.org/W3114632476",
        "https://openalex.org/W4313156423",
        "https://openalex.org/W6803870738",
        "https://openalex.org/W4386697749",
        "https://openalex.org/W4392850630",
        "https://openalex.org/W6863424749",
        "https://openalex.org/W4398201291",
        "https://openalex.org/W4392947521",
        "https://openalex.org/W4400889241",
        "https://openalex.org/W4402220065",
        "https://openalex.org/W4379878309",
        "https://openalex.org/W3120430728",
        "https://openalex.org/W3172942063",
        "https://openalex.org/W6772383348",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W2962843773",
        "https://openalex.org/W4382372249",
        "https://openalex.org/W2794825826",
        "https://openalex.org/W4391652257",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W4366208220",
        "https://openalex.org/W4394832225",
        "https://openalex.org/W4405669265",
        "https://openalex.org/W4401397240",
        "https://openalex.org/W3038745921",
        "https://openalex.org/W3081500256",
        "https://openalex.org/W3105082150",
        "https://openalex.org/W3124636969",
        "https://openalex.org/W4401577856",
        "https://openalex.org/W2802128429",
        "https://openalex.org/W2983291152",
        "https://openalex.org/W2890137987",
        "https://openalex.org/W2038729760",
        "https://openalex.org/W4391109864",
        "https://openalex.org/W6852542133",
        "https://openalex.org/W3205077529",
        "https://openalex.org/W2346705140",
        "https://openalex.org/W2963946669",
        "https://openalex.org/W2742893201",
        "https://openalex.org/W2072629969",
        "https://openalex.org/W4407150885",
        "https://openalex.org/W6891952891",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4323066538",
        "https://openalex.org/W4386065384",
        "https://openalex.org/W3154010931",
        "https://openalex.org/W3175183715",
        "https://openalex.org/W4221146266",
        "https://openalex.org/W4290852327",
        "https://openalex.org/W4391528827",
        "https://openalex.org/W1997074238",
        "https://openalex.org/W4311027007",
        "https://openalex.org/W4301914798",
        "https://openalex.org/W4292828966",
        "https://openalex.org/W6803376173",
        "https://openalex.org/W3018295606",
        "https://openalex.org/W3135547872",
        "https://openalex.org/W6757817989",
        "https://openalex.org/W4365443922",
        "https://openalex.org/W3036298167",
        "https://openalex.org/W4385302721",
        "https://openalex.org/W4386755427",
        "https://openalex.org/W4399739169",
        "https://openalex.org/W2797527544",
        "https://openalex.org/W2002507614",
        "https://openalex.org/W2083927153",
        "https://openalex.org/W3102785203"
    ],
    "abstract": "Abstract Diagnosing and treating skin diseases require advanced visual skills across domains and the ability to synthesize information from multiple imaging modalities. While current deep learning models excel at specific tasks such as skin cancer diagnosis from dermoscopic images, they struggle to meet the complex, multimodal requirements of clinical practice. Here we introduce PanDerm, a multimodal dermatology foundation model pretrained through self-supervised learning on over 2 million real-world skin disease images from 11 clinical institutions across 4 imaging modalities. We evaluated PanDerm on 28 diverse benchmarks, including skin cancer screening, risk stratification, differential diagnosis of common and rare skin conditions, lesion segmentation, longitudinal monitoring, and metastasis prediction and prognosis. PanDerm achieved state-of-the-art performance across all evaluated tasks, often outperforming existing models when using only 10% of labeled data. We conducted three reader studies to assess PanDerm’s potential clinical utility. PanDerm outperformed clinicians by 10.2% in early-stage melanoma detection through longitudinal analysis, improved clinicians’ skin cancer diagnostic accuracy by 11% on dermoscopy images and enhanced nondermatologist healthcare providers’ differential diagnosis by 16.5% across 128 skin conditions on clinical photographs. These results show PanDerm’s potential to improve patient care across diverse clinical scenarios and serve as a model for developing multimodal foundation models in other medical specialties, potentially accelerating the integration of artificial intelligence support in healthcare.",
    "full_text": "Nature Medicine | Volume 31 | August 2025 | 2691–2702\n 2691\nnature medicine\nhttps://doi.org/10.1038/s41591-025-03747-y\nArticle\n e-mail: zongyuan.ge@monash.edu\nA list of authors and their affiliations appears at the end of the paper\nA multimodal vision foundation model for \nclinical dermatology\nDiagnosing and treating skin diseases require advanced visual skills across \ndomains and the ability to synthesize information from multiple imaging \nmodalities. While current deep learning models excel at specific tasks \nsuch as skin cancer diagnosis from dermoscopic images, they struggle \nto meet the complex, multimodal requirements of clinical practice. Here \nwe introduce PanDerm, a multimodal dermatology foundation model \npretrained through self-supervised learning on over 2 million real-world \nskin disease images from 11 clinical institutions across 4 imaging modalities. \nWe evaluated PanDerm on 28 diverse benchmarks, including skin cancer \nscreening, risk stratification, differential diagnosis of common and rare skin \nconditions, lesion segmentation, longitudinal monitoring, and metastasis \nprediction and prognosis. PanDerm achieved state-of-the-art performance \nacross all evaluated tasks, often outperforming existing models when \nusing only 10% of labeled data. We conducted three reader studies to assess \nPanDerm’s potential clinical utility. PanDerm outperformed clinicians by \n10.2% in early-stage melanoma detection through longitudinal analysis, \nimproved clinicians’ skin cancer diagnostic accuracy by 11% on dermoscopy \nimages and e nh an ced n on de rm at ol ogist healthcare providers’ differential \ndiagnosis by 16.5% across 128 skin conditions on clinical photographs. \nThese results show PanDerm’s potential to improve patient care across \ndiverse clinical scenarios and serve as a model for developing multimodal \nfoundation models in other medical specialties, potentially accelerating the \nintegration of artificial intelligence support in healthcare.\nReceived: 15 October 2024\nAccepted: 22 April 2025\nPublished online: 6 June 2025\n Check for updates\nThere is a pressing need to fully harness the potential of artificial intel-\nligence (AI) in diagnosing and managing skin diseases. Although deep \nlearning has shown remarkable performance, often matching or sur-\npassing dermatologists, current AI models for dermatology remain lim-\nited to isolated tasks, such as diagnosing skin cancer from dermoscopic \nimages1. These models struggle to integrate various data types and \nimaging modalities, reducing their utility in different real-world clinical \nsettings. Dermatology, like internal medicine, is inherently complex, \nencompassing a broad spectrum of conditions from common derma-\ntoses to life-threatening malignancies, necessitating a comprehensive, \npatient-centered approach that integrates various clinical workflows.\nIn clinical practice, diagnosing and treating skin conditions \ninvolves a range of tasks, including total-body skin cancer detection \nand risk assessment2–5, differential diagnosis of hundreds of dermato-\nlogical conditions such as inflammatory dermatoses and pigmentary \ndisorders6, multimodal image analysis7,8, pathology interpretation9,10, \nmonitoring lesion changes 11,12 and predicting outcomes 13,14. The \nabsence of integrated AI solutions capable of supporting these various \nworkflows currently hampers the practical impact of AI in dermatology. \nRecent advances in foundation models have emerged as a promising \ndirection to address this challenge15,16.\nFoundation models are large-scale neural networks pretrained \non vast, diverse data using self-supervised learning techniques, often \nleveraging weakly labeled or unlabeled data 17–19. Built on rich knowl-\nedge representations, these models have shown impressive perfor -\nmance across medical fields such as ophthalmology 20, radiology 21 \nNature Medicine | Volume 31 | August 2025 | 2691–2702 2692\nArticle https://doi.org/10.1038/s41591-025-03747-y\nTBP tiles\n757,890 (35.3%)\nDermatopathology tiles \n547,047 (25.4%)\nClinical images\n460,328 (21.4%)\nDermoscopic images\n384,441 (17.9%)\nHOP and MYM\n38,110.0\n(9.9%)\nMMT_derm\n316,499.0\n(82.3%)\nNSSI\n29,832.0\n(7.8%)\nHOP and MYM\n405,856.0\n(53.6%)\nISIC2024\n352,034.0\n(46.4%)\nACEMID_path\n80,312.0\n(14.7%)\nTCGA–SKCM\n377,764.0\n(69.1%)\nUAH89k\n88,971.0\n(16.3%)\nEdu1\n81,947.0\n(17.8%)\nEdu2\n67,430.0\n(14.6%)\nMMT_clinical\n310,951.0\n(67.5%)\nPanDerm\nScreening\nLongitudinal data\nanalysis and prognosis\nFew-shot \nlearning\nPhenotype\nassessment\nRisk \nidentification\nDiagnosis and\nsegmentation\nHigh risk\nLow risk\nMetastasis\nprediction\nLesion-change \ndetection\nSurvival\nanalysis\nLesion\nsegmentation\nSkin cancer \ndiagnosis\nMelanoma\nscreening\nClinical images\nDermatopathology tiles\nMultimodal inputs\nDiverse downstream clinical task evaluation\nClinical utility evaluation\nTBP screening \nstudy \nHuman–AI \ncollaboration\n (2 reader studies)\nEarly melanoma \ndetection \n(1 reader study)\nDermoscopic images\n200\n300\n400\n500\n600\n700\n800\nPretraining epochs\n89\n90\n91\n92\n93\n94\n95\nAverage AUROC\nacross 8 datasets\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\nPretraining data size\n(in millions of images)\n88\n89\n90\n91\n92\n93\n94\n95\nAverage AUROC\nacross 8 datasets\nPanDerm\n(this work)\nSwAVDerm\nResNet50\nMILAN\nDINOv2\nMAE\nImage\nencoder\nCLIP\nencoder\nVisible latent alignment\nMask\nregressor\nMasked latent \nalignment\nMask queries\nPanDerm\n(this work)\na\nb c\nd\ne\nf\ng\nCross-attention\nStudent\nTeacher\n0\n100,000\n200,000\n300,000\n400,000\n500,000\n600,000\nCount\nUOA\nMUW\nUSB\nAlfred\nTUHH\nHCdB\nMSKCC\nEdu\nUCSC\nUQ\nMMT\nInstitution\n7,372\n11,894\n61,598\n80,312\n88,971\n99,979\n121,786\n149,377\n377,764\n523,203\n627,450\n3D TBP tiles\nSkin cancer\ndiag. (HAM)\nSkin cancer\ndiag. (BCN)\nSkin cancer\ndiag. (MSKCC)\nSkin cancer\ndiag. (HIBA)\nMelanocyte\nlesion change\ndet. (SDDI1)\nMelanocyte\nlesion change\ndet. (SDDI2)\nMalignant\nlesion change\ndet. (SDDI2)\nMetas. pred.1\n(ComBineMel)\nMetas. pred.2\n(ComBineMel)\nMetas. prog.1\n(ComBineMel-S)\nMetas. prog.2\n(ComBineMel-S)\nMetas. prog.3\n(ComBineMel-S)\nSkin lesion\nseg. (ISIC2018)\nSkin lesion\nseg. (HAM)\nSkin cancer\ndiag. (PAD)\nSkin cancer\ndiag. (DDI)\nSkin cancer\ndiag. (DermC)\nMulti-class.\n(MMT-74)\nMulti-class.\n(MMT-09)\nMulti-class.\n(DermNet)\nMalignant\nlesion det.\n(ISIC2024)\nPhotodamage\nassessment\n(HOP&M)\nNevus count\n(MYM)\nRisk prediction\n(HOP&M)\nMelanoma\nscreening\n(HOP&M)\nMelanoma\nscreening\n(HOP&M)\nSkin cancer\ndiag. (CPTAC)\nModalities\nClinical image\nTBP\nDermatopathology\nDermoscopic image\nMethods\nSwAVDerm\nDINOv2\nSL_Imagenet\nPanDerm (this work)\nSkin tumor\nclass. (patch)\n84\n88\n93\n68\n73\n78\n69\n73\n77\n85\n90\n95\n61\n66\n71\n66\n71\n77\n69\n76\n84\n89\n93\n97\n63\n68\n73\n88\n92\n96\n86\n90\n94\n83\n87\n91\n84\n88\n92\n88\n91\n95\n66\n71\n77\n74\n79\n85\n77\n82\n88\n36\n42\n49\n61\n66\n71\n49\n55\n62\n81\n85\n90\n83\n86\n90\n93\n96\n99\n63\n67\n71\n59\n65\n72\n81\n85\n90\n81\n86\n91\n94\n97\n100\nN/A\nVisible and masked patch\nFig. 1 | Overview of this study. a–c, Pretraining dataset: 2.1 million dermatological \nimages from 11 clinical sources across 4 modalities, shown by modality (a), source \n(b) and institution (c). d, PanDerm interprets multiple imaging modalities for \nvarious dermatology tasks, evaluated in real-world melanoma screening and \nthree reader studies. Image types include dermatopathology (microscopic \nbiopsy specimens), clinical (wide-field lesion and surrounding skin), dermoscopic \n(close-up dermoscope images) and TBP tiles (lesion crops). e, Architecture: \nViT-large encoder, regressor and CLIP-based teacher model, with representation \nreconstruction and CLIP latent alignment objectives. f, Performance versus \npretraining data size and epochs (average AUROC on 8 benchmarks) compared \nwith alternative strategies. g, PanDerm outperforms existing models on 28 \nevaluation datasets across 4 modalities. All icons in d are from Flaticon.com, \nexcept for the risk stratification, lesion change detection and survival analysis \nicons, which are from Microsoft PowerPoint.\nNature Medicine | Volume 31 | August 2025 | 2691–2702\n 2693\nArticle https://doi.org/10.1038/s41591-025-03747-y\nand pathology22–25. Through comprehensive pretraining on large and \ndiverse data, these models develop versatile representations that can \neffectively adapt to various clinical scenarios, outperforming previous \ndeep learning models in downstream tasks. Their strong feature repre-\nsentations also enable data-efficient applications26,27, requiring fewer \nlabeled samples, which is particularly crucial for medical domains in \nwhich expert-annotated data are often limited.\nHowever, developing effective foundation models for dermatology \npresents unique challenges. The performance of foundation models is \ninherently linked to the scale of their parameters and training data28–30. \nIn general computer vision, foundation models are pretrained on mas-\nsive datasets such as ImageNet31 or JFT-300M (ref. 32) and most exist-\ning dermatology AI models still rely on these models for downstream \nadaptation. Some efforts have focused on self-supervised learning \nspecifically for dermatology using public datasets33,34 or web-sourced \nskin images35. However, these approaches are often limited by dataset \nsize, diversity or the lack of real patient data. Moreover, while recent \nadvances in medical foundation models have shown promise in vari-\nous specialties, they cannot fully address dermatology’s unique needs. \nSpecialty-specific foundation models20,21,23 typically focus on single \nimaging modalities, while general biomedical models, despite their \nbroad scope, struggle with domain-specific data scarcity and integrat-\ning heterogeneous modalities for comprehensive clinical analysis.\nHere we introduce PanDerm, a general-purpose, multimodal der-\nmatology foundation model. Uniquely designed to integrate multiple \nimaging modalities, PanDerm is pretrained on over 2 million images \nsourced from 11 institutions across multiple countries, covering 4 imag-\ning modalities spanning diverse dermatological conditions (Fig. 1a–c). \nIn the pretraining stage, PanDerm uses the masked latent modeling and \ncontrastive language-image pre-training (CLIP)36 feature alignment \nfor self-supervised learning (Fig. 1e and Methods), showing supe -\nrior data scalability and training efficiency compared with existing \nself-supervised algorithms (Fig. 1f). The model achieves unified repre-\nsentation learning across total-body photography (TBP) and clinical, \ndermoscopic and dermatopathology images, enabling comprehensive \npatient analysis throughout diverse clinical workflows (Fig. 1d).\nWe systematically evaluate PanDerm across 28 benchmarks \n(Fig. 1g), covering a diverse array of clinical tasks, including screening, \nrisk stratification, phenotype assessment, nevus counting, longitudinal \nmonitoring, lesion change detection, diagnosis of both common and \nrare skin conditions and skin lesion segmentation, as well as recurrence \nprediction and prognosis. PanDerm achieves state-of-the-art perfor-\nmance on all tasks, often using only 5–10% of the labeled training data \ntypically required. Through three reader studies, we show that this \nunified multimodal approach outperforms clinicians in early-stage \nmelanoma detection, enhances clinicians’ diagnostic accuracy in skin \ncancer diagnosis and supports nonspecialist healthcare providers \nin the differential diagnosis of diverse skin conditions. These find-\nings highlight the potential of specialty-specific foundation models \nto advance medical practice by integrating diverse modalities, with \nbroader implications for AI development across healthcare specialties.\nResults\nAblation studies and training strategy comparisons\nT o evaluate PanDerm’s effectiveness, we conducted systematic analyses \nexamining how model performance scales with training data and com-\nputational resources (datasets described in Supplementary Table 1). \nFirst, compared with existing dermatology-specific models, PanDerm \nshowed strong scalability as training data increased from 0.8 to 1.8 mil-\nlion skin images (Fig. 1f, left). Notably, it achieved superior performance \nto SwAVDerm35, a leading dermatology self-supervised learning model, \nusing 33% less training data. When compared with other self-supervised \ntraining techniques, PanDerm showed remarkable computational \nefficiency, requiring only 200 training epochs to achieve the best \nperformance, compared with 500–800 epochs needed by leading \nmethods such as MILAN37, DINOv2 (ref. 38) and MAE19 (Fig. 1f, right). \nFurthermore, PanDerm also surpassed vision-language models such \nas CLIP36, MONET39 and biomedical-specific CLIP (BiomedCLIP)40 in \nbenchmark evaluations (Supplementary Table 1), while showing emer-\ngent capabilities in dermatology similar to those of DINOv2 in natural \nimages, with linear probing performance comparable to full-parameter \nfine-tuning (Supplementary Table 2). When evaluated against gener-\nalist biomedical foundation models, PanDerm showed substantial \nadvantages across different dermatological tasks. Compared with a \nrepresentative model in this category, BiomedGPT41, PanDerm showed \n20.9% better area under the receiver operating characteristic curve \n(AUROC) in melanoma detection, 34.7% higher weighted F1 score in \ndifferentiating between skin conditions and 19.6% improved weighted \nF1 in analyzing microscopic skin tissue images (Extended Data Table 1). \nEven using computationally efficient methods, PanDerm maintained \nits advantages, outperforming both linear-probe and fine-tune ver-\nsions of BiomedGPT by 14.3% and 5.1%, respectively, in linear probing \n(Supplementary Table 3). On the basis of these promising results, we \nexpanded our evaluation to compare PanDerm with three representa-\ntive AI models: SL-Imagenet31 and DINOv2 (ref. 38) (both widely used \nfoundation models pretrained on natural images with a ViT-Large42 \nbackbone), and SwAVDerm35 (a self-supervised model pretrained on \na large skin image dataset from search engines).\nDiagnostic performance and generalization ability across \ndatasets\nWe systematically evaluated PanDerm diagnostic performance across \n10 public datasets from 4 imaging modalities and 7 international sites \n(Fig. 2a). These datasets covered multi-class classification of pigmented \nneoplastic lesions and binary melanoma diagnosis tasks. PanDerm \nconsistently outperformed all other models, achieving significant \nimprovements on 9 of 10 datasets, with average gains of 5.1%, 8.0%, 4.2% \nand 0.9% on dermoscopic, clinical, TBP and pathology datasets, respec-\ntively (Fig. 2a). On representative dermoscopy and clinical benchmarks \nsuch as HAM10000 (ref. 34) and PAD-UFES-20 (ref. 43), PanDerm sur-\npassed the next-best models by 4.7% (P < 0.001) and 9.0% (P < 0.001), \nrespectively (Fig. 2a, Supplementary Table 4 and Extended Data Fig. 1).\nPanDerm showed strong performance even with limited training \ndata, achieving comparable results to other models while using only \n10–30% of the labeled images (Fig. 2b and Supplementary Tables 5–10). \nAdditional results for other tasks are presented in Extended Data Fig. 2. \nT o test PanDerm’s generalization applicability, we evaluated its perfor-\nmance on melanoma diagnosis using images from seven external medi-\ncal centers, representing patient populations different from the training \ndata. PanDerm showed significant superiority over all pretrained mod-\nels, achieving higher AUROC scores across all external datasets (Fig. 2c). \nNotably, it maintained high performance even on clinical photographs \nthat were not used during training, with AUROC gains of 4.0%, 2.6% and \n2.1% on the three external clinical datasets (all P < 0.001).\nBeyond skin cancer diagnosis, we evaluated PanDerm’s ability to \ndiagnose a broader range of skin conditions commonly seen in clinical \npractice. We tested on three complementary datasets: the public Derm-\nNet dataset44 covering 23 common conditions, along with two internal \ndatasets (MMT-09 with 9 conditions and MMT-74 with 74 conditions) \ncomprising 38,476 clinical images across 9 broad and 74 fine-grained \nskin conditions. These datasets comprehensively cover inflammatory \ndiseases, infections, various types of skin tumors and other frequently \nencountered skin problems. As shown in Fig. 2d, PanDerm achieved \nweighted F1 improvements of 3.2%, 7.1% and 8.2% on MMT-09, DermNet \nand MMT-74, respectively, compared with the next-best models (all \nP < 0.001). PanDerm’s advantage grew larger as the number of conditions \nincreased, showing its strong capability to handle complex, multi-disease \nscenarios. PanDerm also outperformed all other pretrained models on all \nmetrics across the three datasets (all P < 0.001; Supplementary Table 11). \nIn the DermNet dataset, PanDerm exceeded the next-best model’s area \nNature Medicine | Volume 31 | August 2025 | 2691–2702 2694\nArticle https://doi.org/10.1038/s41591-025-03747-y\nunder the precision–recall curve (AUPR) by 14.7%. Further details on \nthe experimental setup, datasets and metrics are provided in Methods.\nShort-term lesion change detection in sequential dermoscopic \nimages\nMonitoring suspicious melanocytic lesions over a 3-month period is a \nwidely accepted procedure for early melanoma detection, as changes \noften prompt excision to rule out melanoma, while stability can be \nreassuring12. We evaluated PanDerm’s ability to detect subtle changes in \nlesions over time by analyzing pairs of sequential dermoscopic images. \nT o ensure accurate comparison despite variations in imaging condi-\ntions, we developed a comprehensive image-processing system that \nstandardizes image quality and alignment (Extended Data Fig. 3). \nThis processing system, combined with PanDerm’s advanced lesion \nchange detection capabilities45, significantly improved change detec-\ntion accuracy from 0.596 (95% confidence interval (CI) 0.567–0.624) \nto 0.706 (95% CI 0.686–0.725) in sequential digital dermoscopic \nimaging data (SDDI1) (Fig. 3a,c) (P < 0.001) and from 0.683 (95% CI \n0.517–0.894) to 0.767 (95% CI 0.649–0.886) in SDDI2 (Fig. 3b,c, left) \n(P < 0.001). Using the optimized pipeline for all models, PanDerm \nachieved AUROC improvements of 4.3% in SDDI1 (P  < 0.001) and  \n3.7% in SDDI2 over the next-best model (Fig. 3c, middle). For lesions  \nlater diagnosed as malignant, PanDerm achieved an AUROC of  \n0.840 (95% CI 0.769–0.911), surpassing the next-best model by 15.0% \na\nHAM10000 (Austria)(n = 10,015; c\n = 7)\nBCN20000 (Barcelona)\n(n = 12,413; c\n = 9)\nMSKCC (United States)\n(n = 8,984; c\n = 2)\nHIBA (Argentina)(n = 1,635; c\n = 2)\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAUROC or W_F1\nP < 0.001\nP < 0.001\nP = 0.005\nP = 0.005\nDermoscopic images\nPAD (Brazil)\n(n = 2,298; c\n = 6)\nDDI (United States)(n = 647; c\n = 2)\nDermC (mix)\n(n = 839; c\n = 2)\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAUROC and W_F1\nP < 0.001\nP = 0.020\nP = 0.015\nClinical images\nISIC2024 (mix)\n(n = 49,025; c\n = 2)\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAUROC and W_F1\nP < 0.001\nTBP images\nPATCH16 (Australia)(n = 40,393; c\n = 16)\nWSI (mix)\n(n = 300; c\n = 2)\n0.7\n0.8\n0.9\n1.0\nAUROC and W_F1\nP < 0.001\nP = 0.140\nPathology images\nSL_ImageNet\nDINOv2\nSwAV_Derm\nPanDerm\ndc\nb\nMMT-74\n(n = 38,476; c\n = 74)\nMMT-09\n(n = 38,476; c\n = 9)\nDermNet\n(n = 19,559; c\n = 23)\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nW_F1\nP < 0.001\nP < 0.001\nP < 0.001\nMulti-skin condition classification\nHIBA (Argentina)(n = 1,635; c\n = 2)\nMSKCC (United States)\n(n = 8,984; c\n = 2)\nPH2 (Portugal)\n(n = 200; c\n = 2)\nDermD (mix)\n(n = 839; c\n = 2)\nPAD-UFE (Brazil)\n(n = 2,298; c\n = 2)\nDermC (mix)\n(n = 839; c\n = 2)\nMed-Node (mix) (n = 170; c\n = 2)\n0.6\n0.7\n0.8\n0.9\n1.0\nAUROC\nP < 0.001\nP < 0.001\nP = 0.023\nP < 0.001\nP < 0.001\nP < 0.001\nP < 0.001\nGeneralization to external sites\n10\n20\n30\n50\n100\nTraining data (%)\n(n = 8,207)\n0.81\n0.84\n0.87\n0.89\n0.92\nW_F1\nHAM10000, dermoscopic (c = 7)\n10\n20\n30\n50\n100\nTraining data (%)\n(n = 1,063)\n0.73\n0.79\n0.84\n0.89\n0.95\nAUROC\nHIBA, dermoscopic (c = 2)\n10\n20\n30\n50\n100\nTraining data (%)\n(n = 1,493)\n0.51\n0.58\n0.64\n0.70\n0.77\n0.83\nW_F1\nPAD, clinical (c = 6)\n10\n20\n30\n50\n100\nTraining data (%)\n(n = 22,373)\n0.77\n0.81\n0.84\n0.87\n0.90\nW_F1\nPATCH_16, pathology (c = 16)\nSL_Imagenet\nDINOv2\nSwAV_Derm\nPanDerm\nFig. 2 | PanDerm’s versatile capacity in diverse diagnosis tasks. a, Performance \ncomparison of PanDerm versus other pretrained models on 10 pigmented skin \nlesion datasets across multiple centers and modalities. n, data size; c, class \nnumber. Metrics: AUROC for binary class (c = 2) and W_F1 score for multi-class \n(c > 2) datasets. The dashed lines indicate the average model performance across \ndatasets. b, Comparison between PanDerm and other pretrained models in label \nefficiency generalization on four representative datasets, showing performance \nat various training data percentages. The vertical dashed lines indicate the data \nquantity needed for PanDerm to match existing model performance. c, External \nvalidation for melanoma diagnosis across 7 datasets. d, Performance evaluation \nof general skin condition classification (up to 74 classes) using clinical images. \nThe error bars in a, c and d show 95% CIs; bar centers in a, c and d represent \nmean values; dots in b represent mean values. Estimates were computed using \nnonparametric bootstrapping with 1,000 bootstrap replicates. P values were \ncalculated using a two-sided t-test.\nNature Medicine | Volume 31 | August 2025 | 2691–2702\n 2695\nArticle https://doi.org/10.1038/s41591-025-03747-y\n(P < 0.01) (Fig. 3c, right). Further details on the lesion change detection \nmethod and dataset details are provided in Methods and Supple-\nmentary Tables 12–14.\nMelanoma metastasis prediction and survival analysis\nWe explored PanDerm’s potential to predict melanoma progression \nfrom dermoscopic images, an emerging approach that could provide \nvaluable prognostic information at the time of diagnosis13,14,46 (Fig. 3e). \nWe evaluated this capability using 680 dermoscopic images from 370 \npatients with invasive primary melanoma across multiple international \ncenters (Fig. 3f). PanDerm showed exceptional accuracy in distinguish-\ning melanomas likely to metastasize, achieving an AUROC of 0.964 (95% \nCI 0.937–0.991), surpassing the next-best model by 2.0% (P = 0.073) \n(Fig. 3d). It also showed strong capability in differentiating between \nlocal and distant metastases, outperforming existing methods by 2.8% \n(P < 0.05) in the weighted F1 score (Supplementary Table 15).\n0\n10\n20\n30\n40\n50\n60\nFollow-up time (months)\n0\n10\n20\n30\n40\n50\n60\nCount\nAny change\n72%\nNo change\n28%\nChange in malignant\n17.7%\nAll others\n82.3%\nAny change\n44.8%\nNo change\n55.2%\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAUROC\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nDefault\nWith warp\nWith mask\nWith whole pipeline\nSL_Imagenet\nDINOv2\nSwAVDerm\nPanDerm\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAUROC\nP = 0.073\nInvasive\n melanoma\nMetastasis?\nPrognosis\nControl\n55.4%\nLocal MS\n21.3%\nDistant MS\n23.2%\nba\nc d\ne f g h\ni j\nAblation on preprocessing Change detection in all lesions Change detection in \n malignant lesion\nSDDI1\n(n = 585,c = 2)\nSDDI2\n(n = 458, c = 2)\nComBineMel\n(n = 680, c = 2)\nSDDI1\n(n = 585, c = 2)\nSDDI2\n(n = 458, c = 2)\nSDDI2\n(n = 458, c = 2)\nAUC, 3 years AUC, 5 years AUC, 7 years AUC, 3 years AUC, 5 year AUC, 7 years\n0\n2\n4\n6\n8\n10\n12\nTime (years)\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nRFI (%)\nn = 305\nlog-rank test: P < 0.001\nLow risk\nHigh risk\n–1\n0\n5\n10\n20\nHazard ratio (95% CI)\nLocation or body area\nMelanoma subtype\nSex\nBreslow thickness\nAge at diagnosis\nUlceration\nDermal mitosis\nAI metastasis prediction\n1.02\n1.03\n1.07\n1.17\n1.52\n1.54\n1.59\n5.63\nHR\n0.51\n0.59\n0.75\n0.002\n0.01\n0.17\n0.001\n<0.001\nP value\n0.7\n0.8\n0.9\n1.0\nTime-dependent AUC\nSingle clinical variables\nMulti-clinical variables\nPanDerm_single\nPanDerm_multi\n0.7\n0.8\n0.9\n1.0\nTime-dependent AUC\nSL_Imagenet\nDINOv2\nSwavDerm\nPanDerm\nFollow-up\nFig. 3 | Short-term lesion change detection and metastasis prognosis results. \na, SDDI1 dataset (n = 585 dermoscopic images) statistics: ratio of changed \nlesions, ratio of changed malignant lesions during follow-up, and follow-up \ntime distribution. b, Ratio of changed lesions in the SDDI2 dataset (n = 458 \ndermoscopic images). c, Ablation study on preprocessing methods using SDDI1 \nand SDDI2 ‘Default’ (direct input), ‘With warp’ (registration only), ‘With mask’ \n(lesion segmentation) and ‘With whole pipeline’ (complete preprocessing as \nin Extended Data Fig. 3). For change detection in SDDI1 and SDDI2, all models \nwere evaluated using the whole preprocessing pipeline. d, Performance of \nbinary metastasis prediction (control versus metastasis) in ComBineMel \n(n = 680 dermoscopic images) by AUROC. e, Scheme of PanDerm for melanoma \nmetastasis and prognosis prediction. MS, metastasis. f, Distribution of \nmetastasis types in the ComBineMel dataset (n = 680 dermoscopic images).  \ng, Kaplan–Meier curves for the RFI in invasive melanoma patients (ComBineMel \n(n = 305 patients)), stratified by PanDerm prediction scores. h, Forest plot of HRs \nfor PanDerm; stratified groups in invasive melanoma patients. i, Time-dependent \nAUC of PanDerm versus clinical variable score combinations in ComBineMel. \nj, Time-dependent AUC comparison of PanDerm and other pretrained models \nin ComBineMel. The error bars in c, d, i and j and error bands in g show 95% CIs; \nthe bar centers indicate means. All estimates were derived from fivefold cross-\nvalidation. P values in d were derived from two-sided t-tests and those in h from \nWald tests within Cox proportional hazards models. Icons in e from Flaticon.com.\nNature Medicine | Volume 31 | August 2025 | 2691–2702 2696\nArticle https://doi.org/10.1038/s41591-025-03747-y\nT o validate PanDerm’s clinical utility for patient risk stratification, \nwe conducted survival analyses using Kaplan–Meier analysis and Cox \nproportional hazards regression. Patients classified as high risk by \nPanDerm showed significantly shorter recurrence-free intervals (RFIs) \ncompared with those in the low-risk group (hazard ratio (HR) 5.63; 95% \nCI 2.87–11.02, P < 0.001) (Fig. 3g). When compared alongside standard \nclinical risk factors—including sex, age, Breslow thickness, ulcera -\ntion, dermal mitosis, location and melanoma subtype—PanDerm’s \npredictions emerged as the strongest indicator of recurrence risk in \nmultivariate Cox regression (Fig. 3h ). It maintained high predictive \naccuracy over extended follow-up periods, with time-dependent areas \nunder the curve (AUCs) of 0.950 (95% CI 0.910–0.991), 0.931 (95% CI \n0.887–0.976) and 0.909 (95% CI 0.880–0.937) at 3 years, 5 years and \n7 years, exceeding multi-clinical variables by 6.8%, 2.9% and 5.0%, \nrespectively (Fig. 3i). Combining PanDerm’s predictions with clinical \nfactors further improved long-term prognostic accuracy in AUCs at \n5 years and 7 years. PanDerm also consistently outperformed other AI \napproaches (Fig. 3j), showing improvements of 2.3%, 3.0% and 2.5% at \n3 years, 5 years and 7 years, respectively. Further details are provided \nin Methods and Supplementary Tables 16 and 17.\nRisk assessment and malignant lesion screening using TBP\nWe next evaluated PanDerm’s capability in analyzing whole-body imag-\ning (TBP)2,3,47 (Fig. 4a). Unlike close-up imaging of individual lesions, TBP \nenables comprehensive patient-level analysis, particularly for critical \nmelanoma risk factors such as photodamage and nevus count4,5,48. In a \ncohort of 480 patients with 196,933 lesions from Australia, PanDerm \nachieved a weighted F1 score of 0.896 (95% CI 0.879–0.913) for photo-\ndamage assessment and an AUROC of 0.983 (95% CI 0.979–0.987) for \nnevus counting, surpassing all other models (P < 0.05 and P < 0.001, \nrespectively; Fig. 4b,c,g). Even with limited training data (10% of the full \ndataset), PanDerm maintained superior performance (Extended Data \nFig. 2). In lesion-specific risk stratification, PanDerm also ranked first \nwith an AUROC of 0.705 (95% CI 0.698–0.712) and balanced accuracy \n(BACC) of 0.657 (95% CI 0.6513–0.663), with all results statistically \nsignificant (P < 0.001; Fig. 4d,h).\nIn a clinical validation study, PanDerm effectively identified malig-\nnant lesions among a large number of benign ones (216 malignant \nversus 197,716 benign lesions) from the high-risk melanoma of patients \n(HOP) study49 and mind your model (MYM) study 50 cohort (Fig. 4e). \nUsing TBP images alone, PanDerm achieved a sensitivity of 0.893, \noutperforming the next-best model by 4.2% (Fig. 4j, left). When clini-\ncal measurements were available for all models, PanDerm maintained \nits advantage with a 3.5% higher sensitivity (Fig. 4j, right), reaching a \nsensitivity of 0.893. Significantly, it detected malignant lesions in 79 out \nof 80 patients while reducing unnecessary examinations by 60.8% com-\npared with melanographers (3,498 versus 8,913 lesions recommended \nfor detailed examination) (Fig. 4j,k and Supplementary Table 18).\nWe observed that PanDerm’s analysis approach aligned well with \nestablished clinical practice, particularly the ‘ugly duckling’ (UD) \nconcept51 of identifying atypical lesions through comparison with \na patient’s other lesions. This was shown through UMAP visualiza -\ntion (Fig. 4f ), where PanDerm’s feature effectively separated suspi -\ncious lesions. The clustering patterns in PanDerm’s risk assessment \n(Fig. 4l) showed correspondence closely with human screening pat -\nterns (Fig. 4i), illustrating its exceptional performance in malignant \nlesion screening. Additional details are provided in Methods, Supple-\nmentary Tables 18–21 and Extended Data Fig. 4.\nSkin lesion segmentation\nWe evaluated PanDerm’s performance on skin lesion segmentation \nusing the ISIC2018 (ref. 52) and HAM10000 (ref. 34) datasets. Com-\npared with existing methods including SL-Imagenet, autoSMIM33 and \nBATFormer33, PanDerm achieved significantly higher performance, \nsurpassing the next best by 3.1% and 1.9% in the Jaccard index on both \ndatasets (P < 0.001; Extended Data Fig. 5a,b). PanDerm’s performance \nwas particularly noteworthy in label-limited scenarios, matching the \nnext-best model while using only 5% of the training data (104 and 350 \nimages for ISIC2018 and HAM10000, respectively; Extended Data \nFig. 5c,d). When compared with MedSAM53, a medical image segmen-\ntation foundation model, PanDerm showed slightly better accuracy \n(0.5% improvement, P = 0.025 and 0.112; Supplementary Table 22). This \nis particularly impressive as PanDerm achieves this performance with-\nout specialized training for image segmentation. In addition, PanDerm \noffers practical advantages in clinical settings, processing images about \nfour to five times faster than MedSAM while using less computational \nresources (Supplementary Table 23). Visual examples and detailed \nperformance metrics are provided in Extended Data Fig. 6 and Sup-\nplementary Tables 22–25.\nReader studies\nT o assess PanDerm’s clinical applicability, we conducted three reader \nstudies evaluating its capabilities across different aspects and modali-\nties of dermatological diagnosis, as follows.\nEarly melanoma detection compared with clinicians.  T o examine \nPanDerm’s capability in early melanoma detection, we compared it with \n12 human reviewers (7 experienced dermatologists and 5 dermatologist \ntrainees) using sequential dermoscopic images from Alfred Hospital54, \nfeaturing multiple follow-up images of the same lesions over time. The \nstudy evaluated two key aspects: overall diagnostic accuracy and early \nmelanoma detection capability. In terms of overall accuracy, PanDerm \noutperformed the average human reviewer by 10.2% and surpassed \nthe best-performing human by 3.6%. For early detection, we assessed \nthe time point of the first suspicious changes detected in sequential \nimages relative to clinical diagnosis and biopsy confirmation. PanDerm \nshowed superior ability in this challenging task, correctly identifying \n77.5% (69 out of 89) of melanoma lesions at the first imaging time point, \ncompared with only 32.6% (29 correct diagnoses) for human reviewers \n(Extended Data Fig. 7). Individual dots in the histograms represent the \nearliest correct diagnosis time points for both PanDerm and human \nreviewers, visualizing the comparative early detection performance.\nHuman–AI collaboration for skin cancer diagnosis . We evaluated \nPanDerm’s impact on clinicians’ diagnostic accuracy across seven \npigmented lesion classes using dermoscopic images (Fig. 5a). The \nstudy included 41 clinicians with varying levels of competency who \nevaluated cases both with and without PanDerm’s multi-probability \nprediction support. PanDerm’s assistance significantly increased over-\nall diagnostic accuracy from 0.69 (95% CI 0.65–0.73) to 0.80 (95% CI \n0.76–0.84, P < 0.001; Fig. 5b). Notably, clinicians with lower compe -\ntency levels showed the greatest improvement, with accuracy gains \nof 17% (P = 0.0082) for those with low competency and 12% (P < 0.001) \nfor those with medium competency, while highly competent clinicians \nshowed a 6% improvement (P  = 0.039; Fig. 5c  and Supplementary \nTable 26). Class-specific analysis revealed significant accuracy improve-\nments in 4 of 7 lesion classes (P  < 0.05; Fig. 5d and Supplementary \nTable 27). For melanoma diagnosis specifically, PanDerm enhanced cli-\nnician accuracy from 0.69 (95% CI 0.64–0.74) to 0.83 (95% CI 0.79–0.87, \nP < 0.001). In addition, PanDerm alone achieved diagnostic accuracy \ncomparable to that of clinicians with PanDerm assistance (0.81 versus \n0.80; P = 0.779).\nHuman–AI collaboration for 128 skin condition diagnoses. We con-\nducted a comprehensive reader study evaluating PanDerm’s diagnostic \ncapabilities across 128 skin conditions using clinical photos. The study \nincluded 37 readers from 5 countries (Fig. 6b) and comprised 2 groups \n(Fig. 6a): the dermatology group (n  = 20; 11 dermatology trainees \nand 9 specialists) and the generalist group (n  = 17; 7 pre-vocational \ntrainees, 8 GPs, 1 nurse and 1 clinical trial assistant). This grouping \nNature Medicine | Volume 31 | August 2025 | 2691–2702\n 2697\nArticle https://doi.org/10.1038/s41591-025-03747-y\nrepresents the distinction in specialty training backgrounds between \ndermatology-trained practitioners and those with general medical \ntraining. Each reader assessed up to 50 cases from a 200-case pool, \nproviding their top 3 diagnoses both with and without PanDerm’s \nassistance. Four experienced dermatologists developed a standardized \nontology for condition categorization (Extended Data Fig. 8). Perfor-\nmance was assessed primarily using 2 metrics: a 4-point diagnostic \nassessment scale for top 1 diagnosis (4, exact ontology match, to 1, sig-\nnificant mismatch) and top 3 diagnostic accuracy, with 3 independent \ndermatologists scoring and resolving discrepancies through panel \nreview. PanDerm’s assistance significantly improved the average top 1  \ndiagnostic scores of all readers from 2.83 to 3.08 (P  < 0.001; Fig. 6c) \nand top 3 diagnostic accuracy from 54% to 63.4% (P < 0.001; Fig. 6d), \nwhile increasing readers’ diagnostic confidence (2.17 to 2.42, P < 0.001; \nFig. 6e). The impact was particularly pronounced in the generalist \ngroup, showing higher diagnosis modification rates (28.6% versus 12.9% \nin the dermatology group; Fig. 6f) and greater improvements in both \ntop 1 diagnostic scores (generalist group, +0.45; dermatology group, \nMalignant\n216\nBenign\n196,716\nLow risk\n3,924\nModerate risk\n483 \nSevere risk\n615\nLow risk\n61,513\nHigh risk\n135,420\nPhotodamage risk\nasessment\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nW_F1\nP = 0.025\nSL_Imagenet\nDINOv2\nSwAVDerm\nPanDerm\n2,450\n1,886\n1,974\n2,233\nMalignant lesion\nscreening\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nMalignant sensitivity\n0.679\n0.679\n0.571\n0.714\nTBP tiles only\n3,601\n3,092\n3,212\n3,498\nMalignant lesion\nscreening\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nMalignant sensitivity\n0.821\n0.821\n0.857\n0.893\nTBP tiles with metadata\nMalignant lesion\ndetection\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nFound malignant lesion\n19/28\n19/28\n16/28\n20/28\nTBP tiles only\nMalignant lesion\ndetection\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nFound malignant lesion\n23/28\n23/28\n24/28\n25/28\nTBP tiles with metadata\nHigh risk\nLow risk\n0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nLesion image tiles\n3D TBP data \nPanDerm\nMalignant lesion\nscreening \nTraining and validation\n480 patients\n196,933 lesions\nTriage for dermoscopic\nexamination \nRisk prediction\nSkin phenotype\n quantification \nHolistic analysis\nPathology examination\nTask adaptation\nApplication\nHigh risk\nLow risk\nLongest diameter: XXX\nContrast: XXX\nBorder irregularity: XXX\nColor variation: XXX \nNevus confidence: XXX\n…\nRisk score\na\nb c d e f\ng\nj k l\nSL_Imagenet\nDINOv2\nSwAVDerm\nPanDerm\nRisk prediction\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUROC\nP < 0.001\nRisk prediction\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nBACC\nP < 0.001\nh\nNevus\n2,643\nNon-nevus\n25,584\nNevus counting\n0.80\n0.85\n0.90\n0.95\n1.00\n1.05\nAUROC\nP < 0.001 i\nFig. 4 | Skin phenotype assessment and malignant lesion screening using TBP . \na, Illustration of PanDerm processing multimodal TBP data for skin phenotype \nquantification, risk prediction and malignant lesion screening. b,c, Class \ndistribution of skin phenotype quantification for photodamage risk assessment \n(n = 5,022 TBP tiles) (b) and nevus counting (n = 28,227 TBP tiles) (c) in datasets. \nd,e, Class distribution of risk prediction (d) and benign and malignant lesions \n(n = 196,933 TBP tiles) (e). g, Photodamage risk assessment and nevus counting \nperformance by W_F1 and AUROC. h, Risk prediction performance by AUROC \nand BACC. Error bars in g and h show 95% CIs; bar centers represent mean values. \nEstimates were computed with nonparametric bootstrapping using 1,000 \nbootstrap replicates. P values were calculated with a two-sided t-test. j, Malignant \nlesion screening performance by sensitivity. Left: using only TBP data; right: \nintegrating measurement information. The numbers below the bars indicate the \nrecommended suspicious lesion count. k, Number of malignant lesions detected \nin the test set. f, UMAP plot of PanDerm screening results for test lesions. i, UMAP \nplot of human screening results for test lesions. l, UMAP plot of PanDerm risk \nprediction results for test lesions. All icons in a are from Flaticon.com, except the \nrisk prediction icon, which is from Microsoft PowerPoint.\nNature Medicine | Volume 31 | August 2025 | 2691–2702 2698\nArticle https://doi.org/10.1038/s41591-025-03747-y\n+0.25; Fig. 6g) and top 3 accuracy (generalist group, +16.5%; derma -\ntology group, +10.3%; Fig. 6h). Analysis by condition classes showed \nconsistent improvements across inflammatory, neoplastic and other \ncategories (P < 0.05; Fig. 6i,j), with inflammatory conditions show -\ning the largest gains (+0.36 in top 1 diagnostic scores, +14.2% in top 3 \naccuracy). Furthermore, when used independently, PanDerm achieved \nhigher diagnostic accuracy than both unassisted readers (top 1 scores: \n3.6 versus 2.83; P < 0.001) and human–AI collaboration (top 1 scores: \n3.6 versus 3.08; P < 0.001). Further details on the setup, methodology, \nreader statistics and datasets of all three reader studies are provided \nin Methods, Extended Data Fig. 9 and Supplementary Tables 26–29.\nDiscussion\nDespite significant advances in AI technology, its application in clinical \nmedicine remains fragmented and underutilized. Current AI systems \nare often restricted to isolated tasks and are unable to address the \ndiverse demands of medical decision-making. This limits the potential \nof AI in supporting clinicians in disease diagnosis and management. \nDermatology, with its complex requirements, including holistic patient \nassessment, lesion-specific analysis and potential use of various imag-\ning modalities, serves as an ideal use case for showing AI’s capabilities \nacross multiple interconnected clinical tasks. Success in this domain \ncould pave the way for broader adoption of AI models across healthcare.\nIn this study, we introduce PanDerm, a versatile dermatology \nfoundation model trained through self-supervised learning on over \ntwo million multimodal dermatological images. Central to PanDerm’s \ndevelopment was the curation of a large and diverse image dataset \nsourced primarily from in-house collections and carefully selected \npublic repositories. This approach contrasts with previous efforts, \nsuch as SwAVDerm35, which relied on web-sourced skin data, inad-\nvertently incorporating images from commonly used benchmarks \nsuch as ISIC55 and DermNet44, increasing the risk of data leakage and \ncompromising evaluation validity. Our strategy minimizes this risk, \nensuring that benchmark evaluations accurately reflect real-world \nmodel performance.\nT o evaluate PanDerm’s clinical utility, we conducted validations \nacross 28 benchmark datasets, spanning comprehensive skin cancer \nassessment and a diverse set of primary care dermatological condi -\ntions. For skin cancer-related assessment, PanDerm outperformed \nexisting models in specialized tasks across various modalities, includ-\ning risk stratification of lesions, phenotype assessment, detection of \nlesion changes and malignancy, multi-class cancer diagnosis, lesion \nsegmentation, and metastasis prediction and prognosis. In particular, \nPanDerm achieved the most results using only 10% of the task-specific \ntraining data typically required by existing models, helping alleviate the \nscarcity of specialist-labeled data in medical AI. In primary care derma-\ntology settings, PanDerm also outperformed comparative models in \ndiagnosing a diverse set of conditions such as inflammatory diseases, \ninfectious conditions and frequently encountered dermatoses. These \ncapabilities stem from its rich knowledge representation, developed \nthrough pretraining on varied dermatological image modalities and \nconditions, leading to consistent and significant performance improve-\nments across tasks and modalities.\nThree reader studies further supported these benchmark findings, \nsuggesting PanDerm’s potential to assist clinical practice across dif-\nferent healthcare settings and specialty training backgrounds. In skin \ncancer diagnosis, PanDerm showed capabilities to improve diagnostic \naccuracy across clinicians of varying competence levels and identify \nconcerning lesions before clinician detection—potentially facilitating \nearlier intervention. In general dermatology, PanDerm improved read-\ners’ differential diagnosis across various skin conditions (for example, \ninflammatory dermatoses, cutaneous neoplasms and pigmentary \ndisorders), with more substantial benefits observed among general-\nists (for example, primary care providers) evaluating inflammatory \nconditions—a considerable portion of everyday dermatological consul-\ntations. Given limited specialist access in primary care settings where \nmost skin conditions are initially evaluated56,57, these findings indicate \nPanDerm’s potential to address dermatological expertise gaps across \nhealthcare settings through both its technical capabilities and clinical \napplications. Importantly, across both human–AI collaboration stud-\nies, PanDerm alone performed equivalently to clinicians with PanDerm \nassistance in skin cancer diagnosis and even outperformed human–AI \ncollaboration in differential diagnosis, similar to observations in a \nprevious study 58 showing ‘no significant difference between large \nlanguage model (LLM)-augmented physicians and LLM alone’ . This \nphenomenon probably stems from clinicians’ selective incorporation \nof AI recommendations rather than blind adherence, representing a \nbalanced clinical implementation in which practitioners maintain their \ndiagnostic autonomy while still benefiting from AI support.\nThe scaling behavior observed in PanDerm’s performance aligns \nwith recent foundation model trends20,22,23,59, although achieving this \nin dermatology required addressing unique challenges in medical data \nacquisition and integration. Our analysis revealed two key technical \ninsights: first, using CLIP36 as a teacher model achieved superior train-\ning data efficiency (Fig. 1f ), outperforming the most representative \nmethod, DINOv2 (ref. 38)—particularly valuable given healthcare’s \ndataset limitations compared with the typical requirement of DINOv2 \nof 142 million images. Second, the masked feature reconstruction \napproach proved more effective at capturing subtle diagnostic features \nthan methods such as MAE 19. These advantages enabled PanDerm \na b c d\n0\n0.25\n0.50\n0.75\n1.00\nLow\nMedium\nHigh\nExperience\nAccuracy\n0\n0.25\n0.50\n0.75\n1.00\nNo help\nWith help\nMode\nAccuracy\n41\n40\n20\n1,040\n0\n1,000\n2,000\n3,000\nMax questions\nMedian questions\nMin questions\nNumber of users\nTotal questions\nValue\n3,320\n0\n0.25\n0.50\n0.75\n1.00\nAKIEC\nBCC\nBKL\nDF MEL\nNV VASC\nDiagnostic class\nAccuracy\nMode\nNo help\nWith help\nFig. 5 | Performance of PanDerm in human–AI collaborative skin cancer \ndiagnosis using dermoscopic images. a, Reader study overview: 41 users \nanswered 3,320 questions on the ISIC2018 Task 3 test set (n = 1,511 images,  \n7 classes). b, Diagnostic accuracy comparison: without versus with PanDerm \nsupport (P < 0.001; two-sided paired t-test; n = 41 readers). c, Accuracy \ncomparison without versus with PanDerm by experience level based on \nexperience per experience: low (n = 11), medium (n = 21) and high (n = 9).  \nd, Accuracy comparison without versus with PanDerm by diagnostic class based \non readings per class: MEL (n = 332), BCC (n = 166), AKIEC (n = 166), BKL (n = 166), \nNV (n = 498), DF (n = 166) and VASC (n = 166). The error bars represent 95% CIs; \nbar centers represent means.\nNature Medicine | Volume 31 | August 2025 | 2691–2702\n 2699\nArticle https://doi.org/10.1038/s41591-025-03747-y\nto improve upon both traditional models 42,60 and recent generalist \nmedical models such as BiomedGPT41. While generalist models advance \nbroader biomedical AI, our results suggest that specialty-specific \nfoundation models designed with clinical workflows in mind may offer \nmore practical solutions for specialties in which multiple imaging \nmodalities are crucial.\nDespite promising results, we acknowledge several limitations \nin our evaluation scope and methodology. While our validation cov -\nered approximately 200 skin conditions across major categories \n(for example, inflammatory diseases, infections, neoplasms, benign \ngrowths, pigmented lesions and vascular anomalies), this represents \nonly a fraction of known dermatological conditions (over 1,000 diag-\nnoses) and is smaller than some previous studies (for example, ref. 6 \nwith 445 conditions), with limited coverage of rare genetic disorders, \ncomplex systemic diseases and clinical variants. Regarding model \nrobustness and fairness, while our benchmark evaluations (Supplemen-\ntary Tables 30 and 31) show consistent performance across different \nsettings (anatomical locations, age groups, genders and skin tones), \nseveral constraints exist: the evaluation mainly reflects overall accuracy \nrather than disease-specific analysis, has varying disease coverage \nacross anatomical locations and focuses primarily on single imaging \nmodalities. A more comprehensive evaluation framework61 integrating \nthese aspects will be necessary for further assessing PanDerm’s robust-\nness. Furthermore, recent studies 62–64 have highlighted important \nchallenges in dermatological AI systems, particularly in human–AI \ninteractions. Although our evaluations show stable cross-skin-tone \nperformance without explicitly balanced training data (as shown to \nbe necessary in a previous study 63), comprehensive bias assessment \nUnassisted\n(mean = 2.17)\nAssisted\n(mean = 2.42)\n1\n2\n3\n4\nDiagnostic confidence\nP < 0.001\nChanged\nUnchanged\n0\n20\n40\n60\n80\n100\nChange ratio in diagnosis\nafter AI (%)\n28.6%\n71.4%\n12.9%\n87.1%\nGeneralist\nDermatology\nUnassisted\n(mean = 54.2%)\nAssisted\n(mean = 63.4%)\n0\n20\n40\n60\n80\n100\nTop 3 accuracy (%)\nP < 0.001\nUnassisted\n(mean = 2.83)\nAssisted\n(mean = 3.08)\n1\n2\n3\n4\nDiagnostic assessment score\nP < 0.001\nUnassisted\nAssisted\n0\n20\n40\n60\n80\n100\nGeneralist group\nP < 0.001\nUnassisted\nAssisted\nDermatology group\nP < 0.001\nGroup\nUnassisted\nAssisted\nImprovement\nGeneralist\n51.1%\n67.6%\n+16.5%\nDermatology\n55.0%\n65.3%\n+10.3%\nTop 3 accuracy (%)\nUnassisted\nAssisted\n1\n2\n3\n4\nGeneralist group\nP < 0.001\nUnassisted\nAssisted\nDermatology group\nP < 0.001\nGroup\nUnassisted\nAssisted\nImprovement\nGeneralist\n2.739\n3.184\n+0.45\nDermatology\n2.876\n3.129\n+0.25\nDiagnostic assessment\nscore\nUnassisted\nAssisted\n0\n20\n40\n60\n80\n100\nInflammatory\nP < 0.001\nUnassisted\nAssisted\nNeoplastic\nP = 0.001\nUnassisted\nAssisted\nOther\nP = 0.017\nMajor class\nUnassisted\nAssisted\nImprovement\nInflammatory\n47.7%\n61.9%\n+14.2%\nNeoplastic\n61.2%\n69.4%\n+8.2%\nOther\n63.0%\n73.0%\n+10%\nTop 3 accuracy (%)\nUnassisted\nAssisted\n1\n2\n3\n4\nInflammatory\nP < 0.001\nUnassisted\nAssisted\nNeoplastic\nP < 0.001\nUnassisted\nAssisted\nOther\nP = 0.045\nMajor class\nUnassisted\nAssisted\nImprovement\nInflammatory\n2.75\n3.11\n+0.36\nNeoplastic\n2.87\n3.18\n+0.31\nOther\n3.15\n3.31\n+0.16\nDiagnostic assessment\nscore\nca d fe\nb g h\ni j\nDermatology\n54.1% (n = 20)\nDerm specialists\n24.3% (n = 9)\nDerm trainees\n29.7% (n = 11)\nGeneralist\n45.9% (n = 17)\nGP\n18.9% (n = 7)\nNursing and othe rs\n8.1%  (n = 3)\nPre-vocational\n18.9% (n = 7)\nAustralia\n59.5%\n(n = 22)\nSpain\n16.2%\n(n = 6)\nAustria\n10.8%\n(n = 4)\nCroatia\n8.1%\n(n = 3)\nItaly\n5.4%\n(n = 2)\nFig. 6 | Performance of PanDerm in human–AI collaborative assessment of \n128 skin conditions using clinical images. a, Reader demographics (n = 37 \nreaders): dermatology group (n = 20 readers) including residents and specialists, \nand generalist group (n = 17 readers) including pre-vocational trainees, general \npractitioners, nurses and clinical trial assistants. Each reviewed up to 50 of 200 \ncases. b, Geographic distribution of readers. c–e, Reader-wise analysis (each \ndata point represents one reader, n = 37 readers): comparisons without versus \nwith PanDerm support for: top 1 diagnostic assessment score (1–4) (c), top 3 \ndiagnostic accuracy (d) and diagnostic confidence score (1–4) (e). f, Diagnosis \nchange ratio after PanDerm support by specialization group. g,h, Class-wise \nanalysis (each data point represents one skin condition class): comparisons \nwithout versus with PanDerm support by specialization groups for the top 1 \ndiagnostic assessment score (1–4) (g) and top 3 diagnostic accuracy (h) (n = 128 \nclasses per group). i,j, Comparisons without versus with PanDerm support by \ndisease category for the top 1 diagnostic assessment score (1–4) (i) and the top \n3 diagnostic accuracy (j), stratified by inflammatory (n = 78 classes), neoplastic \n(n = 37 classes) and other (n = 13 classes) conditions. P values in c–e were \ncalculated using two-sided paired t-test across readers, while P values in g–j \nwere calculated using two-sided paired t-test across classes. In all the boxplots, \nthe horizontal lines represent medians and the white dots represent means. \nThe upper and lower box limits indicate the 1st and 3rd quartiles, with whiskers \nextending to 1.5 times the interquartile range. Error bars represent 95% CIs.\nNature Medicine | Volume 31 | August 2025 | 2691–2702 2700\nArticle https://doi.org/10.1038/s41591-025-03747-y\nrequires metrics beyond overall accuracy. Another study 64 further \nrevealed that equitable stand-alone performance may not translate to \nunbiased human–AI collaboration, which is crucial for clinical deploy-\nment. T o address these limitations, future work should develop stand-\nardized protocols for cross-demographic evaluations using more \ncomprehensive fairness metrics and investigate biases in human–AI \ncollaborative settings. International collaborations such as ISIC55 will \nbe crucial for creating representative datasets and establishing robust \nfairness standards.\nIn conclusion, PanDerm shows the potential of multimodal \nspecialty-specific foundation models in addressing the diverse clini -\ncal needs across specialized and routine clinical practice in dermatol -\nogy. Through comprehensive pretraining on diverse dermatological \nimages and validation across multiple clinical scenarios, the model \nshowed robust performance across different use cases. Our devel-\nopment approach, combining systematic data curation, advanced \nself-supervised learning and rigorous clinical validation, provides a \nframework for developing medical AI systems that can adapt to vary -\ning levels of clinical expertise and healthcare settings. These findings \nsuggest promising directions for developing foundation models in \nother medical specialties in which the integration of diverse imaging \nmodalities and complex clinical workflows is crucial for patient care.\nOnline content\nAny methods, additional references, Nature Portfolio reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author con -\ntributions and competing interests; and statements of data and code \navailability are available at https://doi.org/10.1038/s41591-025-03747-y.\nReferences\n1. Kittler, H., Pehamberger, H., Wolff, K. & Binder, M. Diagnostic \naccuracy of dermoscopy. Lancet Oncol. 3 3, 159–65 (2002).\n2. Primiero, C. A. et al. A narrative review: opportunities and \nchallenges in artificial intelligence skin image analyses using  \ntotal body photography. J. Invest. Dermatol. 144, 1200–1207 \n(2024).\n3. Primiero, C. A. et al. A protocol for annotation of total body \nphotography for machine learning to analyze skin phenotype and \nlesion classification. Front. Med. 11, 1380984 (2024).\n4. Olsen, C. M. et al. Risk stratification for melanoma: models \nderived and validated in a purpose-designed prospective cohort. \nJ. Natl Cancer Inst. 110, 1075–1083 (2018).\n5. Usher-Smith, J. A., Emery, J., Kassianos, A. P. & Walter, F. M. Risk \nprediction models for melanoma: a systematic review. Cancer \nEpidemiol. Biomark. Prev. 23, 1450–1463 (2014).\n6. Liu, Y. et al. A deep learning system for differential diagnosis of \nskin diseases. Nat. Med. 26, 900–908 (2020).\n7. Yap, J., Yolland, W. & Tschandl, P. Multimodal skin lesion \nclassification using deep learning. Exp. Dermatol. 27, 1261–1267 \n(2018).\n8. Luo, N. et al. Artificial intelligence-assisted dermatology \ndiagnosis: from unimodal to multimodal. Comput. Biol. Med. \nhttps://doi.org/10.1016/j.compbiomed.2023.107413 (2023).\n9. Rapini, R. P. Practical Dermatopathology (Elsevier Health Sciences, \n2021).\n10. Song, A. H. et al. Artificial intelligence for digital and \ncomputational pathology. Nat. Rev. Bioeng. 1, 930–949 (2023).\n11. Kittler, H. et al. Identification of clinically featureless incipient \nmelanoma using sequential dermoscopy imaging. Arch. \nDermatol. 142, 1113–1119 (2006).\n12. Altamura, D., Avramidis, M. & Menzies, S. W. Assessment of the \noptimal interval for and sensitivity of short-term sequential digital \ndermoscopy monitoring for the diagnosis of melanoma. Arch. \nDermatol. 144, 502–506 (2008).\n13. Tiodorovic, D. et al. Dermatoscopic patterns of cutaneous \nmetastases: a multicentre cross-sectional study of the International \nDermoscopy Society. J. Eur. Acad. Dermatol. Venereol. 38,  \n1432–1438 (2024).\n14. Lallas, K. et al. Prediction of melanoma metastasis using \ndermatoscopy deep features: an international multicenter cohort \nstudy. J. Clin. Oncol. https://doi.org/10.1200/JCO.2024.42.16_\nsuppl.9565 (2024).\n15. Gui, H., Omiye, J. A., Chang, C. T. & Daneshjou, R. The promises \nand perils of foundation models in dermatology. J. Invest. \nDermatol. 144, 1440–1448 (2024).\n16. Moor, M. et al. Foundation models for generalist medical artificial \nintelligence. Nature 616, 259–265 (2023).\n17. Krishnan, R., Rajpurkar, P. & Topol, E. J. Self-supervised learning in \nmedicine and healthcare. Nat. Biomed. Eng. 6, 1346–1352  \n(2022).\n18. Jaiswal, A., Babu, A. R., Zadeh, M. Z., Banerjee, D. & Makedon, F. A \nsurvey on contrastive self-supervised learning. Technologies 9, 2 \n(2020).\n19. He, K. et al. Masked autoencoders are scalable vision learners. \nIn Proc. IEEE/CVF Conference on Computer Vision and Pattern \nRecognition 16000–16009 (IEEE, 2022).\n20. Zhou, Y. et al. A foundation model for generalizable disease \ndetection from retinal images. Nature 622, 156–163 (2023).\n21. Pai, S. et al. Foundation model for cancer imaging biomarkers. \nNat. Mach. Intell. 6, 354–367 (2024).\n22. Xu, H. et al. A whole-slide foundation model for digital pathology \nfrom real-world data. Nature 630, 181–188 (2024).\n23. Chen, R. J. et al. Towards a general-purpose foundation model for \ncomputational pathology. Nat. Med. 30, 850–862 (2024).\n24. Vorontsov, E. et al. A foundation model for clinical-grade \ncomputational pathology and rare cancers detection. Nat. Med. \n30, 2949–2935 (2024).\n25. Wang, X. et al. A pathology foundation model for cancer \ndiagnosis and prognosis prediction. Nature 634, 970–978  \n(2024).\n26. Azizi, S. et al. Robust and data-efficient generalization of \nself-supervised machine learning for diagnostic imaging. Nat. \nBiomed. Eng. 7, 756–779 (2023).\n27. Azizi, S. et al. Big self-supervised models advance medical image \nclassification. In Proc. IEEE/CVF International Conference on \nComputer Vision 3478–3488 (IEEE, 2021).\n28. Achiam, J. et al. Gpt-4 technical report. Preprint at https://arxiv.org/ \nabs/2303.08774 (2023).\n29. Zhai, X., Kolesnikov, A., Houlsby, N. & Beyer, L. Scaling vision \ntransformers. In Proc. IEEE/CVF Conference on Computer Vision \nand Pattern Recognition 12104–12113 (IEEE, 2022).\n30. Kaplan, J. et al. Scaling laws for neural language models. Preprint \nat https://arxiv.org/abs/2001.08361 (2020).\n31. Russakovsky, O. et al. ImageNet large scale visual recognition \nchallenge. Int. J. Comput. Vis. 115, 211–252 (2015).\n32. Sun, C., Shrivastava, A., Singh, S. & Gupta, A. Revisiting \nunreasonable effectiveness of data in deep learning era. In Proc. \nIEEE International Conference on Computer Vision 843–852 (IEEE, \n2017).\n33. Wang, Z., Lyu, J. & Tang, X. autoSMIM: automatic superpixel- \nbased masked image modeling for skin lesion segmentation. IEEE \nTrans. Med. Imaging 42, 3501–3511 (2023).\n34. Tschandl, P., Rosendahl, C. & Kittler, H. The HAM10000  \ndataset, a large collection of multi-source dermatoscopic  \nimages of common pigmented skin lesions. Sci. Data 5, 180161 \n(2018).\n35. Shen, Y. et al. Optimizing skin disease diagnosis: harnessing \nonline community data with contrastive learning and clustering \ntechniques. NPJ Digit. Med. 7, 28 (2024).\nNature Medicine | Volume 31 | August 2025 | 2691–2702\n 2701\nArticle https://doi.org/10.1038/s41591-025-03747-y\n36. Radford, A. et al. Learning transferable visual models from natural \nlanguage supervision. In Proc. International Conference on \nMachine Learning (eds Meila, M. & Zhang, T.) 8748–8763 (PMLR, \n2021).\n37. Hou, Z., Sun, F., Chen, Y.-K., Xie, Y. & Kung, S.-Y. Milan: masked \nimage pretraining on language assisted representation. Preprint \nat https://arxiv.org/abs/2208.06049 (2022).\n38. Oquab, M. et al. DINOv2: learning robust visual features without \nsupervision. Trans. Mach. Learn. Res. https://openreview.net/\nforum?id=a68SUt6zFt (2024).\n39. Kim, C. et al. Transparent medical image AI via an image–text \nfoundation model grounded in medical literature. Nat. Med. 30, \n1154–1165 (2024).\n40. Zhang, S. et al. A multimodal biomedical foundation model \ntrained from fifteen million image–text pairs. NEJM AI. 2, \nAIoa2400640 (2025).\n41. Zhang, K. et al. A generalist vision–language foundation model for \ndiverse biomedical tasks. Nat. Med. 30, 3129–3141 (2024).\n42. Dosovitskiy, A. et al. An image is worth 16×16 words: transformers \nfor image recognition at scale. In Proc. 9th International \nConference on Learning Representations https://openreview.net/\nforum?id=YicbFdNTTy (OpenReview.net, 2021).\n43. Pacheco, A. G. et al. PAD-UFES-20: a skin lesion dataset \ncomposed of patient data and clinical images collected from \nsmartphones. Data Brief 32, 106221 (2020).\n44. Dermnet. Dermnet https://dermnet.com/ (2023).\n45. Zhang, B. et al. Short-term lesion change detection for melanoma \nscreening with novel Siamese neural network. IEEE Trans. Med. \nImaging 40, 840–851 (2020).\n46. Sacchetto, L. et al. Skin melanoma deaths within 1 or 3 years  \nfrom diagnosis in Europe. Int. J. Cancer 148, 2898–2905  \n(2021).\n47. Kurtansky, N. R. et al. The SLICE-3D dataset: 400,000 skin lesion \nimage crops extracted from 3D TBP for skin cancer detection. Sci. \nData 11, 884 (2024).\n48. Arnold, M. et al. Global burden of cutaneous melanoma \nattributable to ultraviolet radiation in 2012. Int. J. Cancer 143, \n1305–1314 (2018).\n49. Primiero, C. A. et al. Evaluation of the efficacy of 3D total-body \nphotography with sequential digital dermoscopy in a high-risk \nmelanoma cohort: protocol for a randomised controlled trial. BMJ \nOpen 9, e032969 (2019).\n50. Koh, U. et al. ‘Mind your moles’ study: protocol of a prospective \ncohort study of melanocytic naevi. BMJ Open 8, e025857  \n(2018).\n51. Grob, J. & Bonerandi, J. The ‘ugly duckling’sign: identification of \nthe common characteristics of nevi in an individual as a basis for \nmelanoma screening. Arch. Dermatol. 134, 103–104 (1998).\n52. Milton, M. A. A. Automated skin lesion classification using \nensemble of deep neural networks in ISIC 2018: skin lesion \nanalysis towards melanoma detection challenge. Preprint at \nhttps://arxiv.org/abs/1901.10802 (2019).\n53. Ma, J. et al. Segment anything in medical images. Nat. Commun. \n15, 654 (2024).\n54. Yu, Z. et al. Early melanoma diagnosis with sequential dermoscopic \nimages. IEEE Trans. Med. Imaging 41, 633–646 (2021).\n55. Codella, N. C. et al. Skin lesion analysis toward melanoma \ndetection: a challenge at the 2017 International Symposium \non Biomedical Imaging (ISBI), hosted by the International Skin \nImaging Collaboration (ISIC). In Proc. IEEE 15th International \nSymposium on Biomedical Imaging (eds Meijering, E. & Summers, \nR.) 168–172 (IEEE, 2018).\n56. Seth, D., Cheldize, K., Brown, D. & Freeman, E. E. Global burden of \nskin disease: inequities and innovations. Curr. Dermatol. Rep. 6, \n204–210 (2017).\n57. Federman, D. G., Concato, J. & Kirsner, R. S. Comparison of \ndermatologic diagnoses by primary care practitioners and \ndermatologists: a review of the literature. Arch. Fam. Med. 8, 170 \n(1999).\n58. Goh, E. et al. GPT-4 assistance for improvement of physician \nperformance on patient care tasks: a randomized controlled trial. \nNat. Med. 31, 1233–1238 (2025).\n59. Yan, S. et al. Derm1M: a million-scale vision-language \ndataset aligned with clinical ontology knowledge for \ndermatology. Preprint at https://arxiv.org/abs/2503.14911 (2025).\n60. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for \nimage recognition. In Proc. IEEE Conference on Computer Vision \nand Pattern Recognition 770–778 (IEEE, 2016).\n61. Yan, S. et al. Towards trustable skin cancer diagnosis via rewriting \nmodel’s decision. In Proc. IEEE/CVF Conference on Computer \nVision and Pattern Recognition 11568–11577 (IEEE, 2023).\n62. Groh, M. et al. Evaluating deep neural networks trained on clinical \nimages in dermatology with the Fitzpatrick 17k dataset. In Proc. \nIEEE/CVF Conference on Computer Vision and Pattern Recognition \n1820–1828 (IEEE, 2021).\n63. Daneshjou, R. et al. Disparities in dermatology AI performance on \na diverse, curated clinical image set. Sci. Adv. 8, eabq6147 (2022).\n64. Groh, M. et al. Deep learning-aided decision support for diagnosis \nof skin disease across skin tones. Nat. Med. 30, 573–583 (2024).\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2025\nSiyuan Yan1,2, Zhen Yu1, Clare Primiero3, Cristina Vico-Alonso4, Zhonghua Wang1, Litao Yang1,2, Philipp Tschandl    5, \nMing Hu1,2, Lie Ju1, Gin Tan6, Vincent Tang7, Aik Beng Ng    7, David Powell    6, Paul Bonnington8, Simon See7, \nElisabetta Magnaterra9, Peter Ferguson10,11, Jennifer Nguyen4, Pascale Guitera4,12, Jose Banuls13,14, Monika Janda15, \nVictoria Mar4,16,19, Harald Kittler    5,19, H. Peter Soyer    3,17,19 & Zongyuan Ge    1,18,19 \n1AIM for Health Lab, Faculty of Information Technology, Monash University, Melbourne, Victoria, Australia. 2Faculty of Engineering, Monash University, \nMelbourne, Victoria, Australia. 3Frazer Institute, The University of Queensland, Dermatology Research Centre, Brisbane, Queensland, Australia.  \n4Victorian Melanoma Service, Alfred Hospital, Melbourne, Victoria, Australia. 5Department of Dermatology, Medical University of Vienna, Vienna, \nNature Medicine | Volume 31 | August 2025 | 2691–2702 2702\nArticle https://doi.org/10.1038/s41591-025-03747-y\nAustria. 6eResearch Centre, Monash University, Melbourne, Victoria, Australia. 7NVIDIA AI Technology Center, Singapore, Singapore. 8The University \nof Queensland, Brisbane, Queensland, Australia. 9Section of Dermatology, Department of Health Sciences, University of Florence, Florence, Italy. \n10Melanoma Institute Australia, Sydney, New South Wales, Australia. 11Tissue Pathology and Diagnostic Oncology, Royal Prince Alfred Hospital and \nNSW Health Pathology, Sydney, New South Wales, Australia. 12Sydney Melanoma Diagnostic Centre, Royal Prince Alfred Hospital, Sydney, New South \nWales, Australia. 13Department of Dermatology, Hospital General Universitario de Alicante, ISABIAL, Alicante, Spain. 14Department of Clinical Medicine, \nUniversidad Miguel Hernández, Sant Joan D’Alacant, Spain. 15Centre for Health Services Research, Faculty of Medicine, The University of Queensland, \nBrisbane, Queensland, Australia. 16School of Public Health and Preventive Medicine, Monash University, Melbourne, Victoria, Australia. 17Dermatology \nDepartment, Princess Alexandra Hospital, Brisbane, Queensland, Australia. 18Airdoc-Monash Research, Monash University, Melbourne, Victoria, Australia. \n19These authors jointly supervised this work: Victoria Mar, Harald Kittler, H. Peter Soyer, Zongyuan Ge.  e-mail: zongyuan.ge@monash.edu\nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\nMethods\nEthics statement\nThe MYM study was approved by the Metro South Health Human \nResearch Ethics Committee on 21 April 2016 (approval number: HREC/16/\nQPAH/125). Ethics approval has also been obtained from the University \nof Queensland Human Research Ethics Committee (approval number: \n2016000554), Queensland University of T echnology Human Research \nEthics Committee (approval number: 1600000515) and QIMR Berg-\nhofer (approval number: P2271). The HOP study has received approval \nfrom the Human Research Ethics Committee (HREC) from Metro South \nHealth HREC (HREC/17/QPAH/816) and the University of Queensland \nHREC (2018000074). The ComBineMel dataset is part of the Computer \nBiomarkers Evaluation of Invasive Melanoma (ComBine Mel) study. \nThe study was approved by the Alfred Hospital Ethics Committee on  \n8 August 2023 (approval number: HREC/98200/Alfred-2023). The study \nfollows the National Statement on Ethical Conduct in Human Research \n(2007) protocols. The SDDI2 dataset has been approved by the Ethics \nReview Board of the Medical University of Vienna. The MMT data study \nis part of a research agreement study with Monash eResearch Centre and \nwas approved through the Monash University Human Research Ethics \nCommittee. The naevus surveillance study images (NSSI) dataset is part \nof the Brisbane Naevus Morphology Study, circa 2009–2014. The study \nfollowed the Declaration of Helsinki protocols and was approved by the \nPrincess Alexandra Hospital human research ethics committee. The  \nACEMID pathology (ACEMID_path) pilot study has received approval \nfrom the Alfred Hospital Ethics Committee (approval number: 746/23) \nto share data accrued for registered trial ACTRN12619001706167  \n(ACEMID) under the Metro South Human Research Committee proto-\ncol HREC/2019/QMS/57206 and the University of Queensland Human \nResearch Ethics Committee protocol 2019003077. The SDDI_Alfred \nstudy has received approval from the Alfred Hospital Ethics Committee \n(approval number: 198/19) for the use of sequential dermoscopic imag-\ning data. Only de-identified retrospective data were used for research, \nwithout the active involvement of patients.\nPretraining dataset for developing PanDerm\nWe curated an extensive pretraining dataset comprising 2,149,706 \nunlabeled multimodal skin images to develop PanDerm. This diverse \ndataset encompasses 4 imaging modalities and 11 data sources: 757,890 \n(35.3%) TBP tiles, 537,047 (25.4%) dermatopathology tiles, 460,328 \n(21.4%) clinical images and 384,441 (17.9%) dermoscopic images. This \nmultimodality approach provides a comprehensive representation of \nskin lesions, enabling the model to learn robust features across differ-\nent visual representations.\nMYM cohort (TBP). The MYM cohort50 is an in-house dataset studying \nthe natural history of melanocytic nevi from 193 Australian partici -\npants recruited from the electoral roll. Three-dimensional (3D) TBP \nwas conducted using VECTRA WB360 (Canfield Scientific), capturing \n92 cross-polarized two-dimensional (2D) images with standardized \nlighting to create a 3D avatar. The average lesion tiles per subject was \napproximately 500. The final dataset comprises 405,856 automatically \ndetected lesion image tiles ≥2 mm in diameter. Demographic informa-\ntion is available in Supplementary Table 32.\nHOP cohort (TBP).  The HOP study 49 is an in-house sequential \ndataset of high-risk melanoma individuals with 314 participants. \nThree-dimensional TBP imaging used the VECTRA WB360 system \nfollowing the same protocol as MYM. Demographic and clinical data \nwere collected through standardized questionnaires. More details \nabout demographic information are available in Supplementary \nTable 33.\nMYM and HOP cohort (dermoscopic).  These datasets also contain  \n38,110 dermoscopic images from suspicious lesions, providing \ncomplementary visualization of surface and subsurface structures \npotentially indicative of various skin conditions, particularly melanoma.\nMMT dataset.  The MMT dataset is an in-house collection amassed \nfrom over 150 clinics across Australia and New Zealand over a 15-year \nperiod. This extensive dataset primarily consists of paired polarized \ndermoscopic and clinical images. From this comprehensive collec -\ntion, we curated a subset containing 316,399 dermoscopic images and \n310,951 clinical images, providing a rich source of pretraining data for \ntraining purposes.\nACEMID pathology pilot study. This dataset comprises 54 patients \nfrom Queensland, Princess Alexandra Hospital (PAH) (48.1%) and  \nNew South Wales Melanoma Institute Australia (NSW MIA) (51.9%),  \nwith 57.4% males, aged 19–75 years (mean 53.4). Most patients (81.5%) \nwere classified as ‘very high’ risk for melanoma, while others were ‘high’ \nrisk (14.8%) or ‘low or average’ risk (1.9%). Lesions were predominantly \nnevi (68.5%, including common, dermal and congenital, and dysplas-\ntic, variants), melanomas (24.1%, mostly in situ) and other lesions \n(7.4%). While 66.7% had single lesions examined, others had 2–5 lesions \nper patient. Notable diagnostic variability between pathologists was \nobserved. More details are available in Supplementary Table 34.\nNSSI. NSSI is an in-house sequential collection of 29,832 dermoscopic \nimages from 1,254 individuals in Brisbane, Australia (2009–2014). \nImages were collected using a digital dermatoscope attached to a \nFotofinder ATBM imaging system (768 × 576 pixels at 96 dpi). The \nstudy included up to 7 time points per participant at 6-month intervals \nover 3 years. Individual lesions maintained consistent identification \nnumbers across visits. See Supplementary Table 35.\nEdu1 and Edu2.  The Educational source 1 (Edu1) and Educational \nsource 2 (Edu2) datasets comprise 81,947 and 67,430 clinical images, \nrespectively, from in-house educational resources. They cover inflam-\nmatory and autoimmune disorders (psoriasis, atopic dermatitis), \ninfections (herpes simplex, molluscum contagiosum, tinea corporis), \npigmentary disorders (melasma, vitiligo), nail conditions (psoriatic nail \ndisease, onychomycosis), vascular lesions (port-wine stains, pyogenic \ngranulomas), and both benign and malignant tumors (melanoma, basal \ncell carcinoma, squamous cell carcinoma), including rare conditions \nand genetic disorders.\nISIC2024. ISIC2024 (ref. 47) is an open-source TBP-based dataset for \nidentifying skin cancers among lesions cropped from 3D total-body \nphotographs. We selected a subset containing 352,034 tile images, \nstratified by institutions.\nTCGA-SKCM. The Cancer Genome Atlas—skin cutaneous melanoma \n(TCGA-SKCM) dataset65 from The Cancer Genome Atlas project charac-\nterized the mutational landscape of human skin cutaneous melanoma. \nIt contains 475 slides processed into 377,764 patch images.\nUAH89k. The UAH89k dataset66 includes 269 histopathology whole \nslide images from Heidelberg University, MVZ for Histology, Cytology \nand Molecular Diagnostics Trier, and the Institute for Dermatopathol-\nogy, enriching the model’s understanding of skin conditions at the \nmicroscopic level.\nDetail of model architecture and pretraining\nPanDerm is a self-supervised learning model designed for the der -\nmatology field, built upon the success of existing self-supervised \nlearning techniques in the natural image domain 67. At its core, the \narchitecture comprises a ViT-Large visual encoder42, a mask regressor \nand a CLIP-Large36 teacher model. The ViT-Large encoder, with its 24 \ntransformer blocks and 1,024 dimensional embeddings, processes \nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\n224 × 224-pixel images, while the CLIP-Large teacher model handles \nslightly smaller 196 × 196-pixel inputs. The training process incorporates \ntwo primary objectives: masked latent alignment and visible latent \nalignment loss. Initially, the input image undergoes masking, with the \nmask ratio proportional to the encoder’s complexity (50% for ViT-Large). \nThe encoder then processes visible patches to produce latent repre-\nsentations, while the regressor predicts the latent representations of \nmasked patches using these visible latent and mask tokens. The model \nfocuses on the encoder–regressor structure without a separate decoder \ncomponent. The regressor assumes the responsibility of predicting the \nlatent representations of masked patches, allowing for more efficient \nprocessing and learning. For target supervision, the unmasked image \nis fed through the CLIP model, generating supervision divided accord-\ning to visible and masked patch locations. The visible latent alignment \nloss is directly applied to the latent representations of visible patches \ncomputed by the encoder. Concurrently, the masked latent alignment \nloss acts on the latent representations of masked patches predicted by \nthe regressor. Both of these loss functions use CLIP latent representa-\ntions as their supervision signals. The regressor in PanDerm operates \nsimilarly to a cross-attention mechanism. It uses learnable mask tokens \nas queries, while the keys and values are derived from the concatenation \nof visible patch representations and the output of previous layers. This \ndesign allows the regressor to effectively infer the content of masked \nregions based on the context provided by visible areas. Optimization \nprimarily focuses on aligning the visible and masked patch predictions \nwith their corresponding CLIP latent supervisions. This approach ena-\nbles PanDerm to extract rich, semantically meaningful representations \nfrom dermatological images without relying on explicit labels.\nFor pretraining, we continued to train the model (initially trained \non ImageNet-1K) on our dataset of over two million unlabeled multi -\nmodal skin images, representing diverse dermatological conditions. \nWe set the batch size on each graphics processing unit (GPU) to 480, \nwith an effective batch size of 1,920. Following masked image modeling \npractices68, we used a 50% mask ratio. T o train our model, we used \nAdamW as the optimizer with an initial learning rate of 1.5 × 10 −3. We \napply simple data augmentation such as random resized cropping \nand horizontal flipping during pretraining. We trained our model \nfor 500 epochs with a warmup of 20 epochs. The pretraining phase \nused 4 80-GB NVIDIA H100 GPUs and took approximately 5 days and \n7 h. We chose the last epoch checkpoint as our final model weights. \nPlease refer to Supplementary Table 36 for more detailed pretraining \nhyperparameter configurations.\nTarget representations (teacher model) of PanDerm. We tested dif-\nferent teacher models, including CLIP-base, CLIP-large, BiomedCLIP40 \nand MONET39 (dermatology-specific CLIP). CLIP-large outperformed \nbiomedical-specific and dermatology-specific CLIP models, probably \nowing to the limited data scale of skin images in medical-domain CLIP \nmodels. Our model with CLIP-large teachers significantly improved \nperformance and outperformed CLIP-large itself. See Supplementary \nTable 1 for detailed results.\nLinear probing versus fine-tuning for PanDerm.  We explored \nwhether PanDerm’s features are ready for downstream tasks without \nfine-tuning, similar to DINOv2 (ref. 38) in the natural image domain. \nOur model using simple linear probing performed comparably with \nexpensive full-parameter fine-tuning, suggesting that PanDerm’s \nfeatures are already well suited for diverse downstream multimodal \nskin-related tasks without requiring further training. Detailed results \nare in Supplementary Table 2.\nDownstream evaluation details\nCompeting self-supervised learning baselines. For self-supervised \nlearning methods comparison, we evaluated DINOv2 (ref. 38), MAE19 \nand MILAN 37, all using the same ViT-Large backbone. We used the \nrecommended hyperparameter configurations for these models and \ncontinued pretraining from their natural image training weights on \nour pretraining dataset. Subsequently, we fine-tuned these models \nusing identical hyperparameter setups to ensure a fair comparison.\nFine-tuning and linear probing. In adapting PanDerm to downstream \ntasks, only the encoder model is used. For most tasks, PanDerm’s fea-\nture quality suffices to achieve competitive performance using simple \nlinear probing. This involves applying a linear classifier (that is, logistic \nregression) to the top of extracted features from the PanDerm encoder \nto evaluate its performance on downstream tasks. For more challenging \ntasks requiring higher performance, we opted to fine-tune the Pan -\nDerm encoder. The fine-tuning tasks include the three reader studies, \nshort-term change detection, skin lesion segmentation, skin cancer \ndetection in ISIC2024 and TBP-based risk stratification. For all other \ntasks, we used linear probing. For linear probing, following practices \nrecommended by the self-supervised learning community, we fix the ℓ2  \nregularization coefficient λ  to MC/100, where M  is the embedding \ndimension and C is the number of classes, and use the L-BFGS solver \nwith a maximum of 1,000 iterations. For fine-tuning, we adhere to \nthe BEiT V2 setting68, using cross-entropy loss with a learning rate of \n5 × 10−4. We train models for 50 epochs with a warmup of 10 epochs. The \nmodel showing the best performance on the validation set is selected \nas the final model. For detailed hyperparameter configurations, please \nrefer to Supplementary Table 37. In the following sections, we describe \ntasks with more specific methodological details.\nSequential data preprocessing for lesion change detection.  Our \nproposed sequential data-preprocessing method consists of dark \ncorner removal, skin inpainting, hair removal, image registration and \nlesion segmentation. For the first two steps, we follow the approach \noutlined in a previous study69. Given an image with or without dark cor-\nner artifacts, we convert it to grayscale and extract the contour using \nthe OpenCV70 binary threshold function (threshold = 100) with the \nfindContours function (RETR_TREE mode and CHAIN_APPROX_SIMPLE \nmethod). We identify the largest contour by calculating the area of all \nexisting contours, capture a circular area using the minEnclosingCircle \nfunction, scale to 80% of the original radius and inpaint using the T elea \nalgorithm (radius = 10). For hair removal, we convert to grayscale, apply \na black hat morphological operation with a 17 × 17 structuring element, \nthe threshold to create a binary mask, and inpaint. For image registra-\ntion, we implement the AKAZE71 feature-based approach: detecting key \npoints (descriptor size = 0, threshold = 9 × 10 −5, octaves = 4), match-\ning using the Brute Force matcher with Hamming distance, refining \nwith RANSAC to estimate a EuclideanTransform model and warping \nusing skimage.transform.warp with reflection padding and linear \ninterpolation.\nSiamese network for change detection. Similar to a previous study45, \nwe use a simple Siamese network architecture for change detection, \nin which two identical visual encoders with shared weights from our \nfoundation model process a pair of sequential lesion images captured \nover a short time frame. Each encoder extracts features from its respec-\ntive image. These learned features are then concatenated and passed \nthrough two fully connected layers, followed by a softmax layer for \nfinal classification. For training this Siamese network in our binary \nchange detection task, we use a contrastive loss function. This loss \nis particularly well suited for Siamese networks as it helps the model \nlearn to distinguish between pairs of images that have changed and \nthose that have not. The contrastive loss encourages the network to \nminimize the distance between feature representations of image pairs \nwith no significant changes while maximizing the distance for pairs that \nshow meaningful changes. This approach allows the network to learn \na similarity metric between image pairs, rather than simply classifying \nindividual images.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\nMelanoma metastasis prediction and survival analysis.  We use a \nlinear probing classifier on our foundation model to predict mela -\nnoma metastasis using dermoscopic images from the private Com -\nBineMel dataset. Our evaluation encompasses two scenarios: binary \nmetastasis prediction and multi-class metastasis prediction. In the \nbinary classification, we aim to differentiate between the presence \nof any metastasis (including local, satellite and in-transit metastases, \nlymph node recurrence, and distant metastasis) and its absence. The \nmulti-class prediction presents a more complex challenge, categoriz-\ning cases into three groups: control (no metastasis); local, satellite \nand in-transit metastases; and distant metastasis. T o enhance the \nrobustness and mitigate potential data selection bias, we perform five \niterations of dataset splitting into training and testing sets, stratified \nby melanoma stage. The model is trained using these fivefold data. We \nlinear probed PanDerm with the setting mentioned above. We then \ngenerated out-of-fold predictions for all lesions and compare these \nwith the ground truth for performance evaluation.\nSubsequently, we conduct a multivariate Cox regression analysis, \nincorporating the metastasis prediction score and clinical variables \n(age, sex, Breslow thickness, ulceration, dermal mitosis, melanoma \nsubtype and lesion location) to predict the RFI. This analysis focuses \non earlier stages of melanoma (stages I–II). We visualize the relative \ncontribution of individual variables to prognosis prediction using a \nforest plot. T o analyze the correlation between variables and RFI, we \nuse the Kaplan–Meier method. Patients are stratified into low-risk and \nhigh-risk groups based on their binary metastasis prediction scores \n(median value). The log-rank test is used to assess the classifier’s ability \nto predict survival. T o evaluate the predictive accuracy at various time \npoints, we generate time-dependent receiver operating characteristic \ncurves and calculate AUCs at 3 years, 5 years and 7 years.\nMelanoma screening using TBP. The melanoma screening algorithm \nis designed to identify high-risk lesions among whole-body images, aid-\ning clinicians in efficiently detecting potential malignancies. Lesions \nflagged as high risk undergo further triage and dermoscopic exami -\nnation. The screening model integrates three modules: a risk predic-\ntion head, a UD detection head and a machine learning module, using \nboth TBP image data (image tiles) and metadata for comprehensive \npredictions. We first fine-tune our foundation model, equipped with \nthe risk prediction head, using TBP image tiles to classify lesions as \nhigh risk or low risk. All lesion images are resized to 224 × 224 pixels \nand subjected to data augmentation, including color and geometric \ntransformations. The risk prediction head, comprising a single linear \nlayer, identifies lesions as high risk if subjected to dermoscopy exami-\nnation and low risk otherwise. The UD detection head leverages the ‘UD \nsign’ , an effective diagnostic strategy that compares all lesions from the \nsame patient to identify outliers. This approach capitalizes on lesion \ncontextual information. We use the fine-tuned foundation model to \nextract deep learning features, which are then processed by the UD \ndetection head. This module calculates the distance between each \nlesion’s features and the average features of all lesions from the same \npatient, using the interquartile range method to select outlier lesions. \nThe machine learning module, an extra tree classifier, is trained using \nTBP metadata, which include 32 measurements for each lesion from \nthe 3D TPB machine. This module directly predicts malignancy based \non pathology labels. The final screening result combines predictions \nfrom all three modules. A lesion is flagged as suggestive of malignancy \nif any module yields a positive prediction. We evaluate the screening \nperformance at both the lesion and patient levels to ensure compre -\nhensive accuracy assessment.\nWeakly supervised slide classification.  Weakly supervised slide \nclassification tasks are approached using the established two-stage \nmultiple instance learning framework: (1) extracting instance-level \nfeatures from tissue regions within the whole slide image (WSI) and \n(2) developing an order-invariant aggregation method to consolidate \npatch-level data into slide-level representation. For preprocessing, we \nuse the CLAM toolbox72 for tissue segmentation, partitioning regions \ninto 256 × 256 nonoverlapping sections at ×20 magnification, then \nresizing to 224 × 224 and normalizing using ImageNet parameters. \nT o evaluate pretrained encoders, we implement the attention-based \nmultiple instance learning algorithm73 with consistent configurations. \nOur implementation features a two-tier gated ABMIL structure with \nan initial FC layer mapping to 512-dimensional space, followed by \nintermediate layers with 384 hidden units. We incorporate dropout \nregularization (rates 0.10 and 0.25), use the AdamW optimizer74 with \na cosine learning rate schedule (initial rate 1 × 10 −4, weight decay \n1 × 10−5), and use cross-entropy loss. Training runs for 20 epochs with \nearly stopping based on validation loss. We ensure robust evaluation \nthrough fivefold cross-validation, stratifying by both case and label \nattributes.\nSkin lesion segmentation.  For skin lesion segmentation, we use a \nconventional segmentation paradigm, using a network encoder con-\nnected to a segmentation decoder and head. Our proposed PanDerm \nserves as the encoder in this setup. We benchmark PanDerm against \nthree established models: ViT-Large42, autoSMIM33 and BATFormer75. \nBoth ViT and PanDerm use an UperNet decoder, following the official \nViT implementation. For autoSMIM and BATFormer, we adhere to \ntheir official repository settings. ViT-Large and autoSMIM encod -\ners are initialized with ImageNet pretrained weights. T o ensure a fair \ncomparison, all images are resized to 224 × 224. We apply online data \naugmentation, including color jittering, random rotation and random \nflipping, to mitigate overfitting. The training uses an AdamW optimizer \nwith an initial learning rate of 5 × 10−4 and a weight decay of 0.01, with \nthe learning rate decaying according to a cosine schedule. The models \nare trained for 100 epochs, and we save the model that achieves the \nbest evaluation metrics on the validation set.\nEarly melanoma detection (reader study 1). We fine-tuned our foun-\ndation model on the private SDDI–Alfred dataset 54 using a tenfold \ncross-validation approach. We used cross-entropy loss with a learn -\ning rate of 5 × 10−4. We train models for 50 epochs with a warmup of \n10 epochs. The model showing the best AUROC on the validation set \nis selected as the final model. We then used an out-of-fold prediction \napproach to generate melanoma predictions for all sequential images. \nFor each image sequence, we recorded the time point at which the \nmodel first made a correct diagnosis of melanoma; otherwise, the \nmodel was considered to have failed in detecting the melanoma. While \nbiopsy serves as our reference standard, we aimed to explore the algo-\nrithm’s potential to detect early signs of melanoma progression. Our \nstudy focused on identifying suspicious changes in sequential images \nbefore clinical diagnosis, with the goal of enabling earlier intervention \nwhen melanomas are most treatable. For the human evaluation, 12 \nclinicians—seven dermatologists with over 5 years of experience and \nfive dermatology residents with less than 5 years of experience—were \ninvited to assess the serial dermoscopic data. The images were pre -\nsented to the reviewers using Qualtrics (Provo), with the reviewers \nblinded to the true diagnoses. For each case, information such as the \npatient’s age, sex, lesion location and date of imaging was provided. \nInitially, only the first dermoscopic image in the sequence was shown, \nand reviewers were asked to classify the lesion as either benign or \nmalignant. As they progressed through the sequence, side-by-side \nimage comparisons were made available to assess changes over time. \nOnce a diagnosis was submitted, it could not be revised. T o mitigate \nbias, we included ten single time-point melanoma images, preventing \nreviewers from assuming that the first image in a series was benign. We \nthen compared the diagnostic performance of the clinicians with our \nmodel, focusing on the time point at which a malignant diagnosis was \nfirst made by either the clinicians or the algorithm.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\nHuman–AI collaboration for skin cancer diagnosis.  The reader \nstudy was conducted using DermaChallenge, a web-based platform \ndeveloped and hosted by the Medical University of Vienna for online \neducation on dermatoscopy, as described in previous studies76,77.  To  \nensure proper authentication and data management, readers were \nrequired to register with a unique username, valid email address and \npassword. Active users on the platform, who previously actively agreed \nto be contacted, were recruited via a single email. Before commencing \nthe study phase, all users had to finish three introduction levels to be \nfamiliarized with the platforms’ user interface and image types. The \nnumber of correct answers in the first iteration of these levels, normal-\nized against the mean score of the entire DermaChallenge platform \nuser base, served as a score of experience. Users were grouped into \n‘low’ (n = 11), ‘medium’ (n = 21) and ‘high’ (n = 9) experience based on \nquantiles with cuts at 0.25 and 0.75 probability (R stats::quantile() func-\ntion). Within the study level, users were shown batches of 10 images, \nrandomly selected from a pool of 1,511 images, that is, the ISIC 2018 Task \n3 test set, with a predefined diagnosis distribution (actinic keratosis \nand intraepidermal carcinoma (AKIEC): 1, basal cell cacinoma (BCC): 1, \nbenign keratinocytic lesion (BKL): 1, dermatofibroma (DF): 1, vascular \nlesion (VASC): 1, melanoma (MEL): 2, melanocytic nevus (NV): 3). For \neach image, a user had to choose one diagnosis out of seven options, \nand subsequently again after assistance from our foundation model, \npresented as multi-class probabilities visualized as bars and numbers \nfor each class. Readers had the flexibility to complete multiple survey \nrounds with different image batches at their discretion; incompletely \nanswered batches were omitted. The study was conducted online from \n20 August to 12 September 2024, during which we collected data from \n41 raters. Our foundation model for decision support used a weighted \nrandom sampler strategy, following the approach from76 but exclud-\ning test-time augmentation. The model showed robust performance, \nachieving an 80.4% mean (macro-averaged) recall, with notably high \nrecall rates for critical skin lesions: 87.2% for melanoma and 86.0% \nfor BCC.\nHuman–AI collaboration for 128 skin condition diagnoses.  The \nreader study was conducted using a web-based platform developed \nfor online dermatological assessment. A total of 37 healthcare profes-\nsionals participated in the study, categorized into two groups based on \nspecialization: a dermatology group (n = 20) comprising 9 dermatol-\nogy specialists and 11 specialty trainees, and a generalist group (n = 17) \nincluding 7 GPs, 7 general medicine practitioners and 3 other health-\ncare professionals (nursing, clinical trial assistants) who manage skin \nconditions within their broader practice scope. This grouping strat -\negy reflects the real-world clinical setting in which nondermatologist \nhealthcare professionals routinely perform initial skin assessments. \nThe diverse range of 128 skin conditions enabled the evaluation of \ndiagnostic performance between dermatologically trained profession-\nals and those with general medical training. Readers were presented \nwith clinical images and asked to provide their assessment through \na structured questionnaire. Each participant rated image quality on \na 5-point scale (from ‘not at all’ to ‘completely’ assessable), provided \na primary diagnosis through free-text entry and optionally listed two \ndifferential diagnoses ranked by likelihood. Diagnostic confidence \nwas recorded on a 4-point scale (1, not at all confident; 2, somewhat \nconfident; 3, confident; 4, highly confident). Following their initial \nassessment, readers were shown PanDerm’s top 3 predicted diagno-\nses and given the opportunity to maintain or modify their original \ndiagnosis and differential diagnoses, followed by a reassessment of \ntheir confidence using the same 4-point scale. The study collected \n1,342 responses between 1 July and 2 October 2025. Before the evalu-\nation, four experienced dermatologists collaboratively developed a \nstandard ontology to systematically categorize the 128 skin conditions \nand facilitate expert evaluation (Extended Data Fig. 8). The evaluation \nprocess involved multiple expert assessors who independently scored \ndiagnostic accuracy using a 4-point scale: 4, direct match with the \npredefined term in the ontology; 3, match within the same diagnos-\ntic category in the ontology; 2, inconsequential misdiagnosis; and 1, \nsignificant mismatch, potentially dangerous misdiagnosis. T o ensure \nrobust assessment, each case was evaluated by three assessors, with \ncases showing significant scoring discordance (differences between 3/4 \nand 1/2) reviewed in consensus meetings to establish final scores. For \nthe top 3 accuracy evaluation, both human readers and AI assistance \nwere evaluated based on whether the correct diagnosis appeared within \ntheir top 3 diagnostic choices.\nEvaluation metrics. For multi-class tasks, we primarily use a weighted \nF1 score, which averages class-specific F1 scores (harmonic means of \nprecision and recall) weighted by class size. It addresses class imbal-\nance in multi-class scenarios. For binary classification, we primarily use \nAUROC, measuring the model’s ability to distinguish between classes \nacross all classification thresholds. An AUROC of 1.0 indicates perfect \nclassification, while 0.5 suggests random guessing. This metric is par-\nticularly useful for imbalanced datasets and when we need to evaluate \ntrade-offs between true-positive and false-positive rates. For the three \nreader studies, we report accuracy (top 1 or top 3). In skin lesion seg-\nmentation, we use the Dice similarity coefficient and Jaccard index to \nassess segmentation quality. For TBP-based melanoma screening, we \nprimarily report the sensitivity (recall) in malignant lesions, focusing \non the model’s ability to correctly identify malignant cases.\nStatistical analysis.  For skin tumor patch classification, melanoma \nslide classification, reader studies, metastasis prediction and skin \nlesion segmentation, we conduct k -fold cross-validation owing to \neither a relatively small sample size or following conventional prac-\ntice. We compute the mean and standard deviation of performance \nacross the folds, then calculate the standard error by dividing the \nstandard deviation by the square root of the number of folds. The \n95% CI is derived using 1.96 times the standard error. T o assess sta -\ntistical significance, we conduct two-sided t -tests comparing Pan -\nDerm’s performance against the baseline model for each task. For \nthe remaining datasets, we use nonparametric bootstrapping with \n1,000 replicates to estimate 95% CIs for each model’s performance. \nT o compare models, we implement pairwise permutation tests, con-\nducting 1,000 permutations per pair and recalculating performance \nmetrics after each permutation. We derive two-sided P  values to \nevaluate the null hypothesis that paired observations stem from \nidentical distributions. In addition, we perform t -tests to assess the \nstatistical significance of inter-model performance variations. Our \nnull hypothesis posits no discernible difference between PanDerm’s \nperformance and that of its competitors. P  < 0.05 was regarded as \nstatistically significant.\nSkin cancer and general skin condition classification datasets\nHAM10000 (7 classes).  The HAM10000 (ref. 34 ) dataset contains \n10,015 dermoscopic images across 7 classes: actinic keratoses, basal \ncell carcinoma, benign keratosis, dermatofibroma, melanocytic nevi, \nmelanoma and vascular lesions. It is stratified into 60% training, 20% \nvalidation and 20% test sets. For human–AI collaboration, we used the \nofficial dataset. All other experiments used the clean version from a \nprevious study78, which prevents data leakage by ensuring that lesions \nfrom the same patient are not split across sets.\nBCN20000 (9 classes). The BCN20000 (ref. 79) dataset comprises \n12,413 dermoscopic images in 9 categories: nevus, melanoma, basal \ncell carcinoma, seborrheic keratosis, actinic keratosis, solar lentigo, \nsquamous cell carcinoma, dermatofibroma and vascular lesions, \nincluding lesions in hard-to-diagnose locations. It is similarly strati -\nfied (60–20–20 split). We used the clean version of BCN20000, which, \nlike the HAM10000, addresses data leakage issues.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\nMSKCC (2 classes).  The Memorial Sloan Kettering Cancer Center \n(MSKCC)55 dataset is curated from the MSKCC data from the ISIC \narchive55, containing 8,984 dermoscopic images with melanoma and \nother classes.\nHIBA (2 classes).  The HIBA55 dataset is curated from the HIBA data \nfrom the ISIC archive 55, containing 1,635 dermoscopic images with \nmelanoma and other classes.\nPAD-UFES-20 (6 classes).  The PAD-UFES-20 (ref. 43) dataset from \nBrazil contains 2,298 close-up clinical images with 6 classes, including \nactinic keratosis, basal cell carcinoma of the skin, malignant melanoma, \nmelanocytic nevus of the skin, squamous cell carcinoma and sebor -\nrheic keratosis.\nDDI (2 classes). We grouped the classes of the diverse dermatology \nimages (DDI) dataset63 into melanoma and others. The dataset contains \n647 clinical images from the United States.\nDerm7pt (2 classes). Derm_D is a subset of Derm7pt (ref. 80), contain-\ning 839 dermoscopic images, and Derm_C contains 839 clinical images \nwith melanoma and other classes.\nISIC2024 (2 classes). ISIC2024 (ref. 47) is a multicenter dataset with \nskin lesion crops from TBP. We chose holdout data with 49,025 crop \nimages with three institutions (FNQH Cairns, Alfred Hospital, Mela -\nnoma Institute Australia) as the evaluation dataset.\nPH2 (3 classes). PH2 (ref. 81) is a clinical image dataset from Portugal \nwith 200 images and 3 classes. We reorganize it to a binary melanoma \ndetection task.\nMed-Node (2 classes). The Med-Node82 dataset contains 170 clinical \nimages. We reorganize it to a binary melanoma detection task.\nDermNet (23 classes). DermNet44 contains 19,559 clinical images; this \ndataset consists of images of 23 types of skin diseases and captures \ncommon clinical presentations including inflammatory conditions \n(eczema, psoriasis), infections (bacterial, viral, fungal) and neoplastic \ndiseases.\nFitzpatrick17K (114 classes).  The Fitzpatrick17K (ref. 62 ) dataset \ncomprises 16,577 clinical images annotated with both dermatologi -\ncal diagnoses and Fitzpatrick skin types (I–VI). It encompasses 114 \ndistinct conditions (minimum of 53 images per condition) spanning \nmajor dermatological categories: inflammatory dermatoses (psoriasis, \nlichen planus, various eczematous conditions), cutaneous malignan-\ncies (melanoma, morpheiform and solid-cystic variants of BCC, SCC), \npapulosquamous disorders (pityriasis rosea, pityriasis rubra pilaris), \nautoimmune conditions (lupus erythematosus, bullous diseases), \nbenign neoplasms (seborrheic keratosis, dermatofibroma) and various \nother clinically significant entities (acanthosis nigricans, granuloma \nannulare, necrobiosis lipoidica).\nMMT-09 (9 classes). The dataset is an in-house clinical dataset with \n9 skin condition classes, including benign keratinocytic, malignant \nkeratinocytic, melanocytic, inflammatory conditions and benign \ntumors, vascular lesion, basal cell carcinoma, malignant keratinocytic, \nmelanoma and squamous cell carcinoma. We chose 38,476 images as \nour evaluation dataset.\nMMT-74 (74 classes). The MMT-74 dataset (Supplementary Table 38) \nis a comprehensive in-house clinical collection comprising 38,476 \ndermatological images across 74 detailed skin condition classes, build-\ning upon and refining the broader 9-class structure of MMT-09. This \nstructured dataset encompasses diverse dermatological conditions, \nincluding detailed classifications of basal cell carcinoma variants \n(nodular, pigmented, superficial and recurrent), melanocytic lesions \nwith specific pattern recognition (such as acral patterns and various \nnevus types), inflammatory disorders (dermatitis, psoriasis), benign \nproliferations (including seborrheic keratosis variants) and vascu -\nlar lesions (angiomas, telangiectasias). The dataset was specifically \ndesigned to evaluate deep learning models’ performance across a \ndiverse and clinically relevant range of skin conditions, with categories \nspanning inflammatory, infective, benign proliferations, melanocytic \nand eczema classifications.\nSD-128 (128 classes). This dataset encompasses 5,619 clinical images \ncovering 128 dermatological conditions spanning the complete spec-\ntrum of clinical practice. The dataset provides substantial coverage \nof inflammatory dermatoses, ranging from common presentations \n(such as psoriasis and atopic dermatitis) to less common entities \n(such as leukocytoclastic vasculitis). It includes diverse infectious \ndiseases of bacterial, viral and fungal etiologies, as well as a compre -\nhensive range of proliferative lesions from benign nevi to malignant \nmelanomas. The collection also extends to appendageal disorders, \nphysical-trauma-related changes, nail disorders and hair-loss condi-\ntions. This extensive compilation represents both frequently encoun-\ntered conditions in everyday practice and challenging rare cases, \nproviding a robust resource for clinical diagnostic support. This dataset \ncontains 5,619 clinical images encompassing diverse dermatological \nconditions commonly encountered in clinical practice. The dataset \nprovides substantial coverage of inflammatory conditions from com-\nmon presentations (psoriasis, atopic dermatitis) to less common enti-\nties (leukocytoclastic vasculitis); various infectious diseases spanning \nbacterial, viral and fungal etiologies; and a range of proliferative lesions \nfrom benign nevi to malignant melanomas as well as appendageal dis-\norders and physical-trauma-related changes. We used 10% of the data \nstratified by disease labels for benchmark evaluation. In addition, we \nselected 200 images stratified by disease classes for our reader study.\nSkin tumor patch classification (PATCH16) (16 classes) . The skin \ntumor patch classification task 66 consists of tissue patches of 378 \nhistopathology WSIs from the archive of the Institute of Pathology, \nHeidelberg University, the MVZ for Histology, Cytology and Molecular \nDiagnostics Trier and the Institute for Dermatopathology Hannover \nfor classification of 16 categories including 4 tumor types and 12 nor-\nmal tissue structures. We obtained a total of 129,364 image patches \nof 100 × 100 μm (395 × 395) size. The dataset was stratified by label, \nwith 55% allocated for training, 15% for validation and 30% for testing.\nMelanoma slide classification (WSI) (2 classes). The melanoma slide \nclassification task83 from the National Cancer Institute’s Clinical Prot-\neomic Tumor Analysis Consortium Cutaneous Melanoma (CPTAC-CM) \ncohort consists of histopathology WSIs for cancer detection. After \nselecting labeled WSIs, we obtained 302 slides (71 normal, 231 tumor). \nFor training and evaluation, we used a fivefold cross-validation strategy \nwith label-stratified splits to maintain class balance.\nEarly melanoma detection based on SDDI–Alfred (2 classes). The \ndataset (Supplementary Table 39) consists of 179 serial dermoscopic \nimaging sequences from 122 patients, totaling 730 dermoscopic \nimages. The patients were recruited from a private specialist derma-\ntology clinic, with follow-up periods ranging from January 2007 to \nDecember 2019. The study population showed distinct characteris -\ntics between melanoma and benign groups: patients with melanoma \nhad a mean age of 56.6 years (s.d. = 11.8) compared with 49.6 years \n(s.d. = 11.4) in the benign group, with slightly different gender distri -\nbutions (53.9% male in melanoma versus 40.0% male in benign cases). \nBoth melanoma and benign lesions that underwent short- or long-term \nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\nSDDI at least once before biopsy were included. The dataset is well \nbalanced, with 90 benign lesions and 89 malignant lesions. Of the 89 \nmelanomas, 34 (38.2%) were invasive, with a mean Breslow thickness of \n0.5 mm, while 55 (61.8%) were in situ. The melanoma subtypes included \ninvasive superficial spreading melanoma (SSM) (36.0%), in situ SSM \n(31.4%), unspecified in situ (18.0%), lentigo maligna (12.3%) and inva-\nsive lentigo maligna melanoma (LMM) (2.2%). The benign lesions were \npredominantly dysplastic nevi (40.0%), followed by compound nevi \n(27.8%), junctional nevi (18.9%) and intradermal nevi (8.9%). Anatomi-\ncally, lesions were most commonly located on the lower limb (29.2% \nmelanoma, 26.7% benign) and back (23.5% melanoma, 25.6% benign). \nAll lesions were monitored via digital dermoscopy, excised owing to \nclinical concerns and confirmed by pathological examination. The \nnumber of images per sequence varied from 1 to 12, with an average of \napproximately 4 images per sequence.\nLongitudinal and melanoma metastasis datasets\nShort-term lesion change detection based on SDDI1 (2 classes). \nThe SDDI1 (ref. 55) dataset is sourced from the ‘Repeated Dermoscopic \nImages of Melanocytic Lesions’ by University Hospital Basel, available \nin the ISIC archive. It comprises 116 sequential lesions, each with a \nsequence length of 5, from 66 patients. The dataset is categorized into \ntwo classes for lesion change detection.\nLesion change detection based on SDDI2 (2 classes).  SDDI2 is an \nin-house dataset from the Medical University of Vienna. It contains 229 \nsequential dermoscopic images with a sequence length of 2. The data-\nset includes both binary change labels and more fine-grained malignant \nchange labels. This dataset is also used for short-term lesion-change \ndetection.\nMelanoma metastasis and survival prediction (2 or 3 classes). \nThe ComBineMel dataset encompasses 680 dermoscopic images of \ninvasive melanoma from 370 patients recruited across 10 hospital \nsites in multiple countries, including Australia and 5 European nations. \nFor large melanomas, multiple images were captured to ensure com-\nprehensive coverage of the entire lesion area. The study population is \nincluded in Supplementary Table 40. Regarding disease staging, the \nmajority of cases were classified as stage I (70.5%), followed by stage III \n(16.5%), stage II (12.2%) and stage IV (0.8%). In terms of T classification, \nT1a was the most common (59.2%), followed by T2a (18.6%) and T4b \n(13.2%). Sentinel lymph node biopsy was not performed in most cases \n(71.6%), with 10.8% positive and 17.6% negative results among those \ntested. For nodal status, N1 disease was the most common (10.8%), \nfollowed by N2 (3.8%) and N3 (1.8%). Regarding metastasis status, 248 \n(67.0%) of cases showed no metastasis, while 66 (17.8%) presented \nwith metastasis at the time of diagnosis. In addition, 56 (15.1%) of cases \ndeveloped metastasis during the follow-up period.\nSkin lesion segmentation based on ISIC2018 and HAM10000. The \nskin lesion segmentation task is evaluated using two publicly available \ndatasets. The ISIC2018 dataset52 comprises 3,694 dermoscopic images \nwith 2,594 images for training, 100 for validation and 1,000 for testing. \nWe follow this official dataset split for our experiments. The HAM10000 \ndataset34 includes 10,015 dermoscopic images, each with correspond-\ning binary segmentation labels. A randomized selection approach is \nadopted, with 64% of the images used for training, 16% for validation \nand the remaining 20% for testing.\n3D TBP datasets\nThis dataset comprises 3D TBP images captured using the VECTRA \nWB360 system (Canfield Scientific). The system uses 92 cameras \nto simultaneously capture cross-polarized 2D images with stand -\nardized lighting within seconds, which are then merged to create \na high-fidelity 3D avatar of each patient’s entire skin surface. From \nthese 3D avatars, individual lesion tiles were exported for further \nanalysis. Unlike stand-alone clinical photographs, TBP represents a \nhigher-order imaging modality in which 2D tiles are systematically \nderived from 3D reconstructions, maintaining spatial relationships. \nThe standardized acquisition with calibrated lighting enables the \ncapture of the entire body surface with overlapping views, providing \nconsistent anatomical landmarks and contextual information for \ncomprehensive assessment, including skin phenotype patterns, lesion \nmeasurements and ‘UD’ sign application. The images undergo calibra-\ntion and stitching, resulting in standardized 2D tiles with consistent \nquality across all body regions.\nPhotodamage risk assessment datasets (3 classes). This in-house \ndataset84 contains image tiles (693 × 693 pixels) created from 92 raw 2D \nphotos, each representing approximately 10 cm2 of cutaneous surface. \nTiles with <33% skin surface were excluded using pixel color analysis. \nManual review removed out-of-focus images, tiles with multiple body \nsites or identifying features. The final dataset comprises 5,022 image \ntiles from MYM50 and HOP49 studies, labeled as low, moderate or severe \nphotodamage risk labeled primarily by dermatology students.\nNevus counting datasets (2 classes) . This dataset, derived from \nthe in-house MYM50 study, contains 28,227 lesion tiles annotated as \nnevus or nonnevus. Three expert physicians independently labeled \nlesions on-screen, with consensus determined by ≥2 clinicians’ agree-\nment. A senior dermatologist manually identified nevi in-clinic using a \ndermatoscope, serving as the gold standard for the test set. T o ensure \nconsistency, lesions under underwear, on the scalp or on foot soles \nwere excluded, and only lesions ≥2 mm were considered. A minimum \n1-month interval was maintained between on-screen and in-clinic \nlabeling sessions.\nRisk prediction and TBP screening datasets (2 classes). This dataset \ncomprises 2,038 TBP scans from 480 patients, collected from the MYM \nand HOP studies. The raw TBP scans include nevi images and a variety \nof nonrelevant images such as normal skin, scars and freckles. T o focus \nonly on nevi, we applied filtering parameters based on built-in Vectra \ndata settings: majorAxisMM ≥ 2, deltaLBnorm ≥ 4.5, out_of_bounds_\nfraction ≤ 0.25, dnn_lesion_confidence ≥ 50 and nevi_confidence > 80. \nThis process resulted in 196,933 lesion image tiles. We stratified the \ndata by the patient for training, validation and testing: 360 patients \nfor training (146,752 images), 40 patients for validation (19,483 images) \nand 80 patients for testing (30,698 images, including 28 malignant \nlesions). Of the total dataset, 216 images represent malignant lesions, \nwith 40 confirmed melanoma cases.\nMeasurements in TBP. Alongside the image tiles, Vectra provides a \nrange of measurements for each lesion, mainly including size, color \nand shape. Our TBP screening model incorporates 32 such measure-\nments: ‘ A’ , ‘ Aext’ , ‘B’ , ‘Bext’ , ‘C’ , ‘Cext’ , ‘H’ , ‘Hext’ , ‘L’ , ‘Lext’ , ‘areaMM2’ , \n‘area_perim_ratio’ , ‘color_std_mean’ , ‘deltaA’ , ‘deltaB’ , ‘deltaL’ , ‘deltaLB’ , \n‘deltaLBnorm’ , ‘dnn_lesion_confidence’ , ‘eccentricity’ , ‘location_simple’ , \n‘majorAxisMM’ , ‘minorAxisMM’ , ‘nevi_confidence’ , ‘norm_border’ , \n‘norm_color’ , ‘perimeterMM’ , ‘radial_color_std_max’ , ‘stdL’ , ‘stdLExt’ , \n‘symm_2axis’ and ‘symm_2axis_angle’ .\nComputing hardware and software\nScripts for data collection and processing were written in Python (ver-\nsion 3.9.19) using the libraries Pandas (version 2.2.2), Numpy (version \n1.26.4) and Pillow (version 10.3.0). For self-supervised pretraining, \nwe used 4 × 80 GB NVIDIA H100 GPUs configured for multi-GPU \nsingle-node training using DistributedDataParallel (DDP) as imple -\nmented by Python (v.3.9.13), PyT orch (v.2.2.1, CUDA 11.8) and T orchvi-\nsion (v.0.17.1). The CAE-v2 code is used as the codebase to develop \nour foundation model, which can be found in its official repository \nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\n(https://github.com/Atten4Vis/CAE). For downstream task evaluation, \nall experiments were conducted on 4 × 49 GB NVIDIA 6000 Ada GPUs. \nWe used Python (v.3.9.19), PyT orch (v.2.2.2, CUDA 11.8) and T orchvision \n(v.0.17.2) for fine-tuning tasks, and Python (v.3.10.14), PyT orch (v.2.2.2, \nCUDA 11.8) and T orchvision (v.0.17.2) for linear probing tasks. We used \nScikit-learn (v1.2.1) for logistic regression in the linear probing setting. \nImplementation of other comparative pretrained models was modified \nbased on the official configuration in their respective repositories: MAE \n(https://github.com/facebookresearch/mae), SL_ImageNet (https://\nhuggingface.co/timm/vit_large_patch16_224.orig_in21k ), DINOv2 \n(https://github.com/facebookresearch/dinov2), SwAVDerm (https://\ngithub.com/shenyue-98/SwAVDerm), autoSMIM (https://github.\ncom/Wzhjerry/autoSMIM), BATFormer (https://github.com/xianlin7/\nBATFormer), MedSAM (https://github.com/bowang-lab/MedSAM ), \nResNet50 (https://pytorch.org/vision/main/models/generated/torch-\nvision.models.resnet50.html), MILAN (https://github.com/zejiangh/\nMILAN), CLIP (https://github.com/openai/CLIP), BiomedCLIP (https://\nhuggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_\npatch16_224) and MONET (https://github.com/suinleelab/MONET/\ntree/main).\nReporting summary\nFurther information on research design is available in the Nature \nPortfolio Reporting Summary linked to this article.\nData availability\nMost datasets used in this study are publicly available. These data -\nsets used for skin lesion diagnosis and segmentation tasks can be \naccessed through various repositories. The ISIC archive (https://\nwww.isic-archive.com/) hosts several datasets, including MSKCC and \nHIBA. Other widely used benchmark datasets are available through \ntheir respective portals: BCN20000 (https://figshare.com/articles/\njournal_contribution/BCN20000_Dermoscopic_Lesions_in_the_\nWild/24140028/1), PAD-UFES-20 (https://www.kaggle.com/datasets/\nmahdavi1202/skin-cancer), DDI (https://ddi-dataset.github.io/index.\nhtml), Derm7pt (https://derm.cs.sfu.ca/Welcome.html), ISIC2024 \n(https://www.kaggle.com/competitions/isic-2024-challenge), \nMed-Node (https://www.kaggle.com/datasets/prabhavsanga/\nmed-node), DermNet (https://www.kaggle.com/datasets/shub -\nhamgoel27/dermnet), WSI (https://portal.gdc.cancer.gov/projects/\nTCGA-SKCM), PATCH16 (https://heidata.uni-heidelberg.de/dataset.\nxhtml?persistentId=doi:10.11588/data/7QCR8S), ISIC2018_task1 and \nHAM10000 (https://challenge.isic-archive.com/data/), SDDI1 (https://\napi.isic-archive.com/collections/328/), PH2 (https://www.fc.up.pt/\naddi/ph2%20database.html), SD-128 (https://huggingface.co/datasets/\nresyhgerwshshgdfghsdfgh/SD-198) and UAH89k (https://heidata.\nuni-heidelberg.de/404.xhtml;jsessionid=6a9c0981ef8e0874c5dca6\ne1600a). Access to in-house datasets is restricted due to patient pri -\nvacy considerations. These include MMT for dermoscopic and clinical \nimage pretraining and downstream multi-skin condition classification, \nNSSI for sequential dermoscopic image pretraining, ACEMID_path \nfor dermatopathology pretraining, Edu1 and Edu2 for clinical image \npretraining, SDDI2 for lesion change detection, SDDI_Alfred for reader \nstudy 1 (early melanoma detection) and the TBP data from MYM and \nHOP studies for all TBP-based pretraining and evaluation. Researchers \ninterested in accessing these datasets should direct their requests to \nthe corresponding author. All requests will receive a response within \n2 weeks of submission. Requests will be evaluated according to institu-\ntional and departmental policies to ensure compliance with intellectual \nproperty rights and patient privacy obligations. The availability of \nthese data may be subject to additional restrictions or requirements.\nCode availability\nWe have made the encoder code and weights available for downstream \ntask applications. They are available via GitHub at https://github.com/\nSiyuanYan1/PanDerm. We have documented all experiments in detail in \nMethods to enable independent replication. T o facilitate the broader \nuse of our model, we have provided tutorial Jupyter notebooks and \ndownstream evaluation code suitable for a wide scientific audience. \nThese resources have been made available to ensure transparency and \nto promote further research in this field.\nReferences\n65. Guan, J., Gupta, R. & Filipp, F. V. Cancer systems biology of TCGA \nSKCM: efficient detection of genomic drivers in melanoma.  \nSci. Rep. 5, 7857 (2015).\n66. Kriegsmann, K. et al. Deep learning for the detection of \nanatomical tissue structures and neoplasms of the skin on \nscanned histopathological tissue sections. Front. Oncol. 12, \n1022967 (2022).\n67. Zhang, X. et al. Cae v2: context autoencoder with clip latent \nalignment. Trans. Mach. Learning Res. (2023).\n68. Peng, Z. et al. Beit v2: masked image modeling with \nvector-quantized visual tokenizers. In Proc. 11th International \nConference on Learning Representations https://openreview.net/\nforum?id=VB75Pi89p7 (OpenReview.net, 2023).\n69. Pewton, S. W. & Yap, M. H. Dark corner on skin lesion image \ndataset: does it matter? In Proc. IEEE/CVF Conference on \nComputer Vision and Pattern Recognition 4831–4839 (IEEE, 2022).\n70. Bradski, G. et al. The OpenCv library. Dr. Dobb’s J. Softw. Tools 3, \n122–125 (2000).\n71. Alcantarilla, P. F. & Solutions, T. Fast explicit diffusion for \naccelerated features in nonlinear scale spaces. IEEE Trans. Pattern \nAnal. Mach. Intell. 34, 1281–1298 (2011).\n72. Lu, M. Y. et al. Data-efficient and weakly supervised computational \npathology on whole-slide images. Nat. Biomed. Eng. 5, 555–570 \n(2021).\n73. Ilse, M., Tomczak, J. & Welling, M. Attention-based deep multiple \ninstance learning. In Proc. International Conference on Machine \nLearning (eds Dy, J. & Krause, A.) 2127–2136 (PMLR, 2018).\n74. Loshchilov, I. et al. Decoupled weight decay regularization. In \nProc. 7th International Conference on Learning Representations \nhttps://openreview.net/forum?id=Bkg6RiCqY7 (OpenReview.net, \n2019).\n75. Lin, X., Yu, L., Cheng, K.-T. & Yan, Z. BATFormer: towards \nboundary-aware lightweight transformer for efficient medical \nimage segmentation. IEEE J. Biomed. Health Inform. 27, 3501–3512 \n(2023).\n76. Tschandl, P. et al. Human–computer collaboration for skin cancer \nrecognition. Nat. Med. 26, 1229–1234 (2020).\n77. Barata, C. et al. A reinforcement learning model for AI-based \ndecision support in skin cancer. Nat. Med. 29, 1941–1946  \n(2023).\n78. Gröger, F. et al. Towards reliable dermatology evaluation \nbenchmarks. In Proc. Machine Learning for Health (ML4H) 101–128 \n(PMLR, 2023).\n79. Hernández-Pérez, C. et al. BCN20000: dermoscopic lesions in the \nwild. Sci. Data 11, 641 (2024).\n80. Kawahara, J., Daneshvar, S., Argenziano, G. & Hamarneh, G. \nSeven-point checklist and skin lesion classification using \nmultitask multimodal neural nets. IEEE J. Biomed. Health Inform. \n23, 538–546 (2018).\n81. Mendonça, T. et al. in Dermoscopy Image Analysis (eds Celebi, M. E. \net al.) Ch. 13 420–438 (CRC Press, 2015).\n82. Giotis, I. et al. MED-NODE: a computer-assisted melanoma \ndiagnosis system using non-dermoscopic images. Expert Syst. \nAppl. 42, 6578–6585 (2015).\n83. Clark, K. et al. The Cancer Imaging Archive (TCIA): maintaining \nand operating a public information repository. J. Digit. Imaging 26, \n1045–1057 (2013).\nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\n84. Kahler, S. et al. Automated photodamage assessment from 3D \ntotal body photography for an objective assessment of melanoma \nrisk. Australas. J. Dermatol. 64, 8 (2023).\nAcknowledgements\nWe thank the following readers who participated in the reader  \nstudy 3: A. Abid, A. Brkic, M. Buljan, G. C. Minano, C. Chamberlin,  \nM. ElSharouni, J. Fulgencio, K. Gacina, E. Garcia, V. Giang, P. Gramp,  \nC. Huang, E. Karlsen, D. Kostner, J. Lai, F. Leo, S. Lim, E. Magnaterra,  \nI. Matheus, R. M. Salvador, C. Mueller, N. Muller, J. Nguyen,  \nM. Onteniente, N. Punchihewa, P. Reztsova, B. R. Trujillo, C. Rosendahl, \nH. Schaider, C. Shen, J. Smith, C. Stone, T. Sun, D. T. Flores, C. Ventura, \nA. Wolber and M. Yan.\nAuthor contributions\nS.Y., H.K. and Z.G. conceived the study and designed the experiments. \nS.Y., C.V.-A., M.H., L.Y., L.J., H.K., V.M., M.J., H.P.S. and Z.G. contributed \nto data acquisition, preprocessing and organization. S.Y. performed \nmodel development and pretraining. S.Y., Z.Y., Z.W., L.Y., H.K. and P.T. \ncontributed to downstream task evaluation. S.Y., Z.Y., C.V.-A., E.M., P.G., \nJ.B. and V.M. contributed to the metastasis prediction and prognosis \ntasks. S.Y. and Z.Y. performed experimental analyses regarding \nTBP-based screening. Z.W. and S.Y. performed experimental analysis \nregarding lesion segmentation. S.Y., L.Y. and P.F. contributed to \ndermatopathology image analysis tasks. S.Y., H.K., P.T., V.M. and H.P.S. \ncontributed to the two human–AI collaboration reader studies. S.Y. \nalso contributed to all remaining tasks of experimental analysis. H.K. \nand P.T. developed the web-based reader platforms and conducted \nthe human–AI reader study. V.M., J.N. and Z.Y. contributed to the early \nmelanoma detection reader study data. H.K., P.T., C.V.-A., C.P., V.M., \nM.J. and H.P.S. contributed clinical inputs to this research. G.T., V.T., \nA.B.N., D.P., P.B. and S.S. provided computing resources and data \nmanagement. All authors contributed to the drafting and revising of \nthe paper.\nFunding\nOpen access funding provided by Monash University.\nCompeting interests\nZ.G., V.M., H.P.S., M.J. and P.G. are chief investigators for the Australian \nCentre of Excellence for Melanoma Imaging and Diagnosis (ACEMID), \nwhich was established via an Australian Cancer Research Foundation \nMajor Infrastructure Grant, with research activities supported by \nNHMRC grants (Cohort Study Grant APP2001517, Centre of Research \nExcellence Grant APP2044753, Synergy Grant APP2009923) and \nMRFF Targeted Health System and Community Organisation Research \nGrant (APP1175082). Z.G. is on the scientific advisory board and a \nconsultant for Optain Health. Although Airdoc has philanthropic \ndonation to the AIM for Health Lab, the company was not involved in \nany aspect of this research. H.P.S. reported equity in e-derm-consult \nGmbH and MoleMap NZ Limited, consulting fees from Canfield \nScientific Inc and a patent (PCT/AU/2013/000394) licensed to Trajan \nMedical and Scientific via Uniquest, all outside the submitted work. He \nis also an executive board member of the International Dermoscopy \nSociety and the Australian Melanoma Clinical Outcome Registry. \nM.J. holds National Health and Medical Research Council (NHMRC) \nTRIP Fellowships (APP2006551, APP2009923 and APP2034422). P.T. \nhas received speaker fees from AbbVie and unrestricted educational \ngrants from Lilly. He is an executive board member of the International \nDermoscopy Society and past president of the Austrian Society of \nDermatopathology. S.S., V.T. and A.B.N. are employees of NVIDIA \nand own Restricted Stocks. H.K. has received speaker fees from \nFotofinder, MSD, Novartis and Pelpharma; license fees from Casio; \nand equipment from Fotofinder, Casio and Heine. He has served as an \nadvisor for Fotofinder, La Roche-Posay and AI Medical Technology, and \nis a member of the executive board of the International Dermoscopy \nSociety. P.G. has received honoraria from Metaoptima PTY and travel \nstipend from L’Oreal. V.M. is supported by an NHMRC Investigator \nGrant (APP2034976). V.M. has received Victorian Medical Research \nAcceleration Fund support for the SMARTI Trial with matched \ncontribution from MoleMap; speaker fees from Novartis, Bristol Myers \nSquibb, Merck and Janssen; and conference travel support from L’Oreal \nand has participated in advisory boards for MSD, L’Oreal and SkylineDx. \nV.M. is a board member of the Melanoma and Skin Cancer Trials Group \nand an advisory member for the Melanoma and Skin Cancer Advocacy \nNetwork. The other authors declare no competing interests.\nAdditional information\nExtended data is available for this paper at https://doi.org/10.1038/\ns41591-025-03747-y.\nSupplementary information The online version contains \nsupplementary material available at https://doi.org/10.1038/s41591-\n025-03747-y.\nCorrespondence and requests for materials should be addressed to \nZongyuan Ge.\nPeer review information Nature Medicine thanks Matthew Groh, \nMichael Marchetti and the other, anonymous, reviewer(s) for their \ncontribution to the peer review of this work. Primary Handling Editors: \nLorenzo Righetto and Saheli Sadanand, in collaboration with the \nNature Medicine team.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\nExtended Data Fig. 1 | Performance of PanDerm versus other pretrained \nmodels on 10 pigmented skin lesion datasets across multiple centers and \nmodalities. a. Performances are measured by weighted F1 (W F1).  \nb. Performances are measured by AUROC. c. Performances are measured by \nAUPR. d. Perfor- mances are measured by BACC. n: data size, c: class number. \nDashed lines show the average performance of each model across different \ndatasets. Estimates were computed using nonparametric bootstrapping with \n1000 bootstrap replicates. P-values calculated using a two-sided t-test. Error bar, \n95% CIs; bar centers, means.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\nExtended Data Fig. 2 | Label efficiency generalization results on additional \ntasks. a. Label efficiency analysis for photodamage risk assessment using T otal \nBody Photography (TBP) images. Results demonstrate model performance with \nlimited labeled data available. PanDerm outperformed the second-best models \nusing only 10% of labeled images. b. Label efficiency analysis for melanoma \nclassification using whole slide dermatopathology images. Results illustrate \nmodel performance with limited labeled data. PanDerm surpassed the second-\nbest models using less than 30% of labeled images.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\nExtended Data Fig. 3 | Longitudinal dermoscopic image-based lesion \nchange detection using PanDerm. For comparing subtle changes in paired \nlesions during short-term follow-up (for example, 3 months), images undergo \ndark corner detection and removal, skin inpainting, registration, and lesion \nsegmentation. This allows models to focus on subtle differences between lesions \nat different time points. Panda icon from Flaticon.com.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\nExtended Data Fig. 4 | SHAP (SHapley Additive exPlanations) value plot.  \nIt shows the impact of various measurement variables captured by the 3D TBP \nmachine on the model output. The plot displays the relative importance and \ndirectional influence of each feature, with colors indicating high (red) to low \n(blue) feature values, and the x-axis representing the SHAP value or impact on the \nmodel’s prediction. Features are ordered by their overall importance, with ‘nevi \nconfidence’ having the highest impact and ‘stdLExt’ the lowest.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\nExtended Data Fig. 5 | Quantitative skin lesion segmentation results. \n a, b. Segmentation performance measured by dice score (DSC) and Jaccard \nindex ( JAC) for PanDerm and baseline models on ISIC2018 (n=2,074 dermoscopic \nimages) and HAM10000 (n=7,011 dermoscopic images) datasets. c, d. Label \nefficiency generalization performance for PanDerm and baselines, showing \nmean DSC and JAC on ISIC2018 and HAM10000 datasets. Error bars in a, b \nindicate 95% confidence intervals; bar centers represent mean values; points in c, \nd denote mean values. All estimates are derived from five replicas with different \nseeds. Statistical significance was assessed using two-sided t-tests.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\nExtended Data Fig. 6 | Qualitative skin lesion segmentation results. a. Comparison of PanDerm against baseline models on challenging examples from HAM10000. \nRed contours indicate ground truth masks, while cyan contours show model predictions. b. PanDerm segmentation results on a random selection of images from \nHAM10000.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\nExtended Data Fig. 7 | Early melanoma detection results (reader study 1).  \nComparing PanDerm to 12 clinicians (7 experienced dermatologists, 5 \ndermatology residents). X-axis: 89 melanoma lesion IDs; Y-axis: lesion image \nsequence length. Points on the histogram represent the initial time points of \ncorrect melanoma diagnoses. Points below y=0 correspond to melanoma lesions \nundetected throughout the sequence.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\nExtended Data Fig. 8 | Sunburst plot of standard ontology on SD-128 dataset. Four experienced dermatologists collaboratively developed the standard ontology to \nsystematically categorize the 128 skin conditions and facilitate expert evaluation in reader study3.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\nExtended Data Fig. 9 | Demographic distribution of participants in reader study 3. a. Specialty distribution of participants. b. Career stage distribution of \nparticipants. c. Experience levels distribution by years.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-025-03747-y\nExtended Data Table 1 | Performance comparison between PanDerm and BiomedGPT across different dermatological tasks \nand modalities\nModels are evaluated on various dermatological tasks spanning TBP, dermoscopic images, clinical photographs, and dermatopathology. Evaluation metrics include Area Under the Receiver \nOperating Characteristic curve (AUROC) for binary classification tasks and Weighted F1 score for multi-class classification. Performance is reported with 95% confidence intervals in \nparentheses. The best performance for each task is bolded. *** p < 0.001 compared to PanDerm. P-values calculated using a two-sided t-test.\nﬁﬁﬁﬁﬁﬁ\n\n"
}