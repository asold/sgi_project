{
  "title": "A Co-Interactive Transformer for Joint Slot Filling and Intent Detection",
  "url": "https://openalex.org/W3092270604",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5029082837",
      "name": "Libo Qin",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5084759965",
      "name": "Tailu Liu",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5019108029",
      "name": "Wanxiang Che",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5079993571",
      "name": "Bingbing Kang",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5067025788",
      "name": "Sendong Zhao",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5057382261",
      "name": "Ting Liu",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2124895976",
    "https://openalex.org/W2166293310",
    "https://openalex.org/W2094472029",
    "https://openalex.org/W2917128112",
    "https://openalex.org/W3017465475",
    "https://openalex.org/W648947103",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964071174",
    "https://openalex.org/W2891533927",
    "https://openalex.org/W2803609229",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W1550863320",
    "https://openalex.org/W2970400306",
    "https://openalex.org/W2575101493",
    "https://openalex.org/W2970450228",
    "https://openalex.org/W2803392141",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2077302143",
    "https://openalex.org/W2963003781",
    "https://openalex.org/W2963558486",
    "https://openalex.org/W2024632416",
    "https://openalex.org/W2963974889",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2963066655",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2971167298",
    "https://openalex.org/W2804945011"
  ],
  "abstract": "Intent detection and slot filling are two main tasks for building a spoken language understanding (SLU) system. The two tasks are closely related and the information of one task can be utilized in the other task. Previous studies either model the two tasks separately or only consider the single information flow from intent to slot. None of the prior approaches model the bidirectional connection between the two tasks simultaneously. In this paper, we propose a Co-Interactive Transformer to consider the cross-impact between the two tasks. Instead of adopting the self-attention mechanism in vanilla Transformer, we propose a co-interactive module to consider the cross-impact by building a bidirectional connection between the two related tasks. In addition, the proposed co-interactive module can be stacked to incrementally enhance each other with mutual features. The experimental results on two public datasets (SNIPS and ATIS) show that our model achieves the state-of-the-art performance with considerable improvements (+3.4% and +0.9% on overall acc). Extensive experiments empirically verify that our model successfully captures the mutual interaction knowledge.",
  "full_text": "A CO-INTERACTIVE TRANSFORMER FOR JOINT SLOT FILLING AND INTENT\nDETECTION\nLibo Qin⋆ Tailu Liu⋆ Wanxiang Che† Bingbing Kang Sendong Zhao Ting Liu\nResearch Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China\nABSTRACT\nIntent detection and slot ﬁlling are two main tasks for build-\ning a spoken language understanding (SLU) system. The two\ntasks are closely related and the information of one task can\nbeneﬁt the other. Previous studies either implicitly model the\ntwo tasks with multi-task framework or only explicitly con-\nsider the single information ﬂow from intent to slot. None of\nthe prior approaches model the bidirectional connection be-\ntween the two tasks simultaneously in a uniﬁed framework.\nIn this paper, we propose a Co-Interactive Transformer which\nconsiders the cross-impact between the two tasks. Instead\nof adopting the self-attention mechanism in vanilla Trans-\nformer, we propose a co-interactive module to consider the\ncross-impact by building a bidirectional connection between\nthe two related tasks, where slot and intent can be able to\nattend on the corresponding mutual information. The exper-\nimental results on two public datasets show that our model\nachieves the state-of-the-art performance.\nIndex Terms— Spoken Language Understanding, Intent\nDetection, Slot Filling, Co-Interactive Transformer\n1. INTRODUCTION\nSpoken language understanding (SLU)typically consists of\ntwo typical subtasks including intent detection and slot ﬁll-\ning, which is a critical component in task-oriented dialogue\nsystems [1]. For example, given “ watch action movie”, in-\ntent detection can be seen an classiﬁcation task to identity an\noverall intent class label (i.e., WatchMovie) and slot ﬁlling\ncan be treated as a sequence labeling task to produce a slot la-\nbel sequence (i.e., O, B-movie-type, I-movie-type).\nSince slots and intent are highly closed, dominant SLU sys-\ntems in the literature [2, 3, 4, 5, 6] proposed joint model to\nconsider the correlation between the two tasks. Existing joint\nmodels can be classiﬁed into two main categories. The ﬁrst\nstrand of work [2, 3] adopted a multi-task framework with\na shared encoder to solve the two tasks jointly. While these\nmodels outperform the pipeline models via mutual enhance-\nment, they just modeled the relationship implicitly by sharing\n⋆ Equal contributions.\n† Corresponding author.\nparameters. The second strand of work [4, 5, 6] explicitly ap-\nplied the intent information to guide the slot ﬁlling task and\nachieve the state-of-the-art performance. However, they only\nconsidered the single information ﬂow from intent to slot.\nWe consider addressing the limitation of existing works\nby proposing a Co-Interactive Transformer for joint slot ﬁll-\ning and intent detection. Different from the vanilla Trans-\nformer [7], the core component in our framework is a pro-\nposed co-interactive module to model the relation between\nthe two tasks, aiming to consider the cross-impact of the two\ntasks and enhance the two tasks in a mutual way. Speciﬁcally,\nin each co-interactive module, we ﬁrst apply the label atten-\ntion mechanism [8] over intent and slot label to capture the\ninitial explicit intent and slot representations, which extracts\nthe intent and slot semantic information. Second, the explicit\nintent and slot representations are fed into a co-interactive at-\ntention layer to make mutual interaction. In particular, the\nexplicit intent representationsare treated as queries and slot\nrepresentations are considered as keys as well as values to\nobtain the slot-aware intent representations. Meanwhile, the\nexplicit slot representationsare used as queries and intent rep-\nresentations are treated as keys as well as values to get the\nintent-aware slot representations. These above operations can\nestablish the bidirectional connection across intent and slots.\nThe underlying intuition is that slot and intent can be able to\nattend on the corresponding mutual information with the co-\ninteractive attention mechanism.\nThe experimental results on two benchmarks SNIPS [9]\nand ATIS [4] show that our framework achieves signiﬁcant\nimprovement compared to all baselines. In addition, we incor-\nporate the pre-trained model (BERT) [10] in our framework,\nwhich can achieve a new state-of-the-art performance. Code\nfor this paper are publicly available at https://github.\ncom/kangbrilliant/DCA-Net.\n2. APPROACH\nThis section describes the details of our framework. As shown\nin Figure 1, it mainly consists of a shared encoder ( §2.1), a\nco-interactive module ( §2.2) that explicitly establishes bidi-\nrectional connection between the two tasks, and two separate\ndecoders (§2.3) for intent detection and slot ﬁlling.\narXiv:2010.03880v3  [cs.CL]  8 Mar 2021\nShared Module\nSlot FillingIntent Detection\n(a)\nLabel Attention Layer Feed-Forward  Layer\nWI Intent Label \nAttention\nSlot Label \nAttentionWS\nConcat Concat\nAdd&NormFFN Layer\nCo-Interactive Attention Layer\nAdd&Norm\nVS\nQS\nLinear\nSoftmaxT\nKS\nAdd&Norm\nVI\nQI\nLinear\nSoftmax\nTKI\nSlot \nDecoder\nSL\nIntent\nDecoder\nx1\nx2\nx3\nx4\nBiLSTM\nBiLSTM\nBiLSTM\nBiLSTM\nN×\nShared Encoder §2.1 Co-Interactive Module §2.2 Decoder §2.3\nAdd&Norm\n)(ˆ L\nIH\n)(ˆ L\nSH\nFig. 1. The illustration of the co-interactive transformer.\n2.1. Shared Encoder\nWe use BiLSTM [11] as the shared encoder, which aims to\nleverage the advantages of temporal features within word or-\nders. BiLSTM consists of two LSTM layers. For the input\nsequence {x1,x2,...,x n}(nis the number of tokens.), BiL-\nSTM reads it forwardly and backwardly to produce a series\nof context-sensitive hidden states H = {h1,h2,..., hn}by\nrepeatedly applying the recurrence hi = BiLSTM (φemb(xi),\nhi−1), where φemb(·) represents the embedding function.\n2.2. Co-Interactive Module\nThe Co-Interactive module is the core component of our\nframework, aiming to build the bidirectional connection be-\ntween intent detection and slot ﬁlling.\nIn vanilla Transformer, each sublayer consists of a self-\nattention and a feed-forward network (FFN) layer. In contrast,\nin our co-interactive module, we ﬁrst apply a intent and slot\nlabel attention layer to obtain the explicit intent and slot rep-\nresentation. Then, we adopt a co-interactive attention layer\ninstead of self-attention to model the mutual interaction ex-\nplicitly. Finally, we extend the basic FFN for further fusing\nintent and slot information in an implicit method.\n2.2.1. Intent and Slot Label Attention Layer\nInspired by Cui et al. [8] that successfully captures label rep-\nresentations, we perform label attention over intent and slot\nlabel to get the explicit intent representation and slot rep-\nresentation. Then, they are fed into co-interactive attention\nlayer to make a mutual interaction directly. In particular,\nwe use the parameters of the fully-connected slot ﬁlling de-\ncoder layer and intent detection decoder layer as slot embed-\nding matrix WS ∈ Rd×|Slabel|and intent embedding ma-\ntrix WI ∈ Rd×|Ilabel|(d represents the hidden dimension;⏐⏐Slabel⏐⏐and\n⏐⏐Ilabel⏐⏐represents the number of slot and intent\nlabel, respectively), which can be regarded as the distribution\nof labels in a certain sense.\nIntent and Slot RepresentationsIn practice, we use H ∈\nRn×d as the query, Wv ∈Rd×|vlabel|(v ∈{I or S}) as the\nkey and value to obtain intent representations Hv with intent\nlabel attention:\nA = softmax(HWv), (1)\nHv = H + AWv, (2)\nwhere I denotes the intent and S represents the slot.\nFinaly, HI ∈Rn×d and HS ∈Rn×d are the obtained\nexplicit intent representation and slot representation, which\ncapture the intent and slot semantic information, respectively.\n2.2.2. Co-Interactive Attention Layer\nHS and HI are further used in next co-interactive attention\nlayer to model mutual interaction between the two tasks. This\nmakes the slot representation updated with the guidance of\nassociated intent and intent representations updated with the\nguidance of associated slot, achieving a bidirectional connec-\ntion with the two tasks.\nIntent-Aware Slot and Slot-Aware Intent Representation\nSame with the vanilla Transformer, we map the matrix HS\nand HI to queries (QS, QI), keys (KS, KI) and values (VS,\nVI) matrices by using different linear projections. To obtain\nthe slot representations to incorporate the corresponding in-\ntent information, it is necessary to align slot with its closely\nrelated intent. We treat QS as queries, KI as keys and VI as\nvalues. The output is a weighted sum of values:\nCS = softmax\n(QSK⊤\nI√dk\n)\nVI, (3)\nH\n′\nS = LN(HS + CS) , (4)\nwhere LN represents the layer normalization function [12].\nSimilarly, we treat QI as queries, KS as keys and VS\nas values to obtain the slot-aware intent representation H\n′\nI.\nH\n′\nS ∈Rn×d and H\n′\nI ∈Rn×d can be considered as leveraging\nthe corresponding slot and intent information, respectively.\n2.2.3. Feed-forward Network Layer\nIn this section, we extend feed-forward network layer to im-\nplicitly fuse intent and slot information. We ﬁrst concatenate\nHI\n′and HS\n′to combine the slot and intent information.\nHIS = H′\nI ⊕H′\nS, (5)\nwhere HIS = (h1\nIS, h2\nIS,... , hn\nIS) and ⊕is concatenation.\nThen, we follow Zhang et al. [3] to use word features for\neach token, which is formated as:\nht\n(f,t) = ht−1\nIS ⊕ht\nIS ⊕ht+1\nIS . (6)\nFinally, FFN layer fuses the intent and slot information:\nFFN(H(f,t)) = max(0,H(f,t)W1 + b1)W2 + b2, (7)\nˆHI = LN(H\n′\nI + FFN(H(f,t))), (8)\nˆHS = LN(H\n′\nS + FFN(H(f,t))), (9)\nwhere H(f,t) = (h1\n(f,t),h2\n(f,t),..., ht\n(f,t)); ˆHI and ˆHS is the\nobtained updated intent and slot information that aligns cor-\nresponding slot and intent features, respectively.\n2.3. Decoder for Slot Filling and Intent Detection\nIn order to conduct sufﬁcient interaction between the two\ntasks, we apply a stacked co-interactive attention network\nwith multiple layers. After stacking L layer, we obtain\na ﬁnal updated slot and intent representations ˆH(L)\nI =\n(ˆh(L)\n(I,1),ˆh(L)\n(I,2),..., ˆh(L)\n(I,n)), ˆH(L)\nS (ˆh(L)\n(S,1),ˆh(L)\n(S,2),..., ˆh(L)\n(S,n)).\nIntent Detection We apply maxpooling operation [13] on\nˆH(L)\nI to obtain sentence representation c, which is used as\ninput for intent detection:\nˆyI = softmax\n(\nWIc + bS\n)\n, (10)\noI = argmax( yI), (11)\nwhere ˆyI is the output intent distribution; oI represents the\nintent label and WI are trainable parameters of the model.\nSlot FillingWe follow E et al. [14] to apply a standard CRF\nlayer to model the dependency between labels, using:\nOS = WS ˆH(L)\nS + bS, (12)\nP(ˆy|OS) =\n∑\ni=1 exp f(yi−1,yi,OS)∑\ny′\n∑\ni=1 exp f(y′\ni−1,y′\ni,OS), (13)\nwhere f(yi−1,yi,OS) computes the transition score from\nyi−1 to yi and ˆy represents the predicted label sequence.\n3. EXPERIMENTS\n3.1. Dataset\nWe conduct experiments on two benchmark datasets. One is\nthe public ATIS dataset [16] and another is SNIPS [9]. Both\ndatasets are used in our paper following the same format and\npartition as in Goo et al. [4] and Qin et al. [6].\nIn the paper, the hidden units of the shared encoder and\nthe co-interactive module are set as 128. We use 300d GloVe\npre-trained vector [17] as the initialization embedding. The\nnumber of co-interactive module is 2. L2 regularization\nused on our model is 1 ×10−6 and the dropout ratio of\nco-interactive module is set to 0.1. We use Adam [18] to\noptimize the parameters in our model.\nFollowing Goo et al. [4] and Qin et al. [6], intent detection\nand slot ﬁlling are optimized simultaneously via a joint learn-\ning scheme. In addition, we evaluate the performance of slot\nﬁlling using F1 score, intent prediction using accuracy, the\nsentence-level semantic frame parsing using overall accuracy.\n3.2. Main Results\nTable 1 shows the experiment results. We have the follow-\ning observations: 1) Compared with baselines Slot-Gated\nand Stack-Propagation that only leverage intent information\nto guide the slot ﬁlling, our framework gain a large im-\nprovement. The reason is that our framework consider the\ncross-impact between the two tasks where the slot informa-\ntion can be used for improving intent detection. It’s worth\nnoticing that the parameters between our model and Stack-\nPropagation is of the same magnitude, which further veriﬁes\nthat contribution of our model comes from the bi-directional\ninteraction rather than parameters factor. 2) SF-ID Network\nand CM-Net also can be seen as considering the mutual in-\nteraction between the two tasks. Nevertheless, their models\ncannot model the cross-impact simultaneously, which limits\ntheir performance. Our framework outperforms CM-Net by\n6.2% and 2.1% on overall acc on SNIPS and ATIS dataset,\nrespectively. We think the reason is that our framework\nachieves the bidirectional connection simultaneously in a\nuniﬁed network. 3) Our framework + BERToutperforms the\nStack-Propagation +BERT, which veriﬁes the effectiveness\nof our proposed model whether it’s based on BERT or not.\n3.3. Analysis\nImpact of Explicit RepresentationsWe remove the intent\nattention layer and replace HI with H. This means that we\nonly get the slot representation explicitly, without the intent\nsemantic information. We name it as without intent attention\nlayer. Similarly, we perform the without slot attention layer\nexperiment. The result is shown in Table 2, we observe that\nthe slot ﬁlling and intent detection performance drops, which\ndemonstrates the initial explicit intent and slot representations\nare critical to the co-interactive layer between the two tasks.\nCo-Interactive Attention vs. Self-Attention Mechanism\nWe use the self-attention layer in the vanilla Transformer\ninstead of the co-interactive layer in our framework, which\ncan be seen as no explicit interaction between the two tasks.\nModel SNIPS ATIS\nSlot (F1) Intent (Acc) Overall (Acc) Slot (F1) Intent (Acc) Overall (Acc)\nSlot-Gated Atten [4] 88.8 97.0 75.5 94.8 93.6 82.2\nSF-ID Network [14] 90.5 97.0 78.4 95.6 96.6 86.0\nCM-Net [15] 93.4 98.0 84.1 95.6 96.1 85.3\nStack-Propagation [6] 94.2 98.0 86.9 95.9 96.9 86.5\nOur framework 95.9 98.8 90.3 95.9 97.7 87.4\nStack-Propagation + BERT [6] 97.0 99.0 92.9 96.1 97.5 88.6\nOur framework + BERT 97.1 98.8 93.1 96.1 98.0 88.8\nTable 1. Slot ﬁlling and intent detection results on two datasets.\nModel SNIPS ATIS\nSlot (F1) Intent (Acc) Overall (Acc) Slot (F1) Intent (Acc) Overall (Acc)\nwithout intent attention layer 95.8 98.5 90.1 95.6 97.4 86.6\nwithout slot attention layer 95.8 98.3 89.4 95.5 97.6 86.7\nself-attention mechanism 95.1 98.3 88.4 95.4 96.6 86.1\nwith intent-to-slot 95.6 98.4 89.3 95.8 97.1 87.2\nwith slot-to-intent 95.4 98.7 89.4 95.5 97.7 87.0\nOur framework 95.9 98.8 90.3 95.9 97.7 87.4\nTable 2. Ablation experiments on the SNIPS and ATIS datasets.\nSpeciﬁcally, we concatenate the HS and HI output from\nthe label attention layer as input, which is fed into the self-\nattention module. The results are shown in Table 2, we\nobserve that our framework outperforms the self-attention\nmechanism. The reason is that self-attention mechanismonly\nmodel the interaction implicitly while our co-interactive layer\ncan explicitly consider the cross-impact between two tasks.\nBidirectional Connection vs. One Direction Connection\nWe only keep one direction of information ﬂow from intent\nto slot or slot to intent. We achieve this by only using one\ntype of information representation representation as queries\nto attend another information representations. We name it as\nwith intent-to-slot and with slot-to-intent. From the results\nin Table 2. We observe that our framework outperforms with\nintent-to-slot and with slot-to-intent. We attribute it to the rea-\nson that modeling the mutual interaction between slot ﬁlling\nand intent detection can enhance the two tasks in a mutual\nway. In contrast, their models only consider the interaction\nfrom single direction of information ﬂow.\n4. RELATED WORK\nDifferent classiﬁcation methods, such as support vector ma-\nchine (SVM) and RNN [19, 20], have been proposed to solve\nIntent detection. Meanwhile, the popular methods are condi-\ntional random ﬁelds (CRF) [21] and recurrent neural networks\n(RNN) [22, 23] are proposed to solve slot ﬁlling task.\nRecently, many dominant joint models [2, 3, 4, 5, 24, 6,\n25] are proposed to consider the closely correlated relation-\nship between two correlated tasks. The above studies either\nadopt a multi-task framework to model the relationship be-\ntween slots and intent implicitly or leverages intent informa-\ntion to guide slot ﬁlling tasks explicitly. Compared with their\nmodels, we propose a co-interactive transformer framework,\nwhich simultaneously considers the cross-impact and estab-\nlish a directional connection between the two tasks while they\nonly consider the single direction information ﬂow or implic-\nitly model the relationship into a set of shared parameters.\nMeanwhile, Wang et al. [26], E et al. [14], and Liu et\nal. [15] propose models to promote slot ﬁlling and intent\ndetection via mutual interaction. Compared with their meth-\nods, the main differences are as following: 1) E et al. [14]\nintroduce a SF-ID network, which includes two sub-networks\niteratively achieve the ﬂow of information between intent\nand slot. Compared with their models, our framework build\na bidirectional connection between the two tasks simultane-\nously in an uniﬁed framework while their frameworks must\nconsider the iterative task order. 2) Liu et al. [15] propose a\ncollaborative memory block to implicitly consider the mutual\ninteraction between the two tasks, which limits their per-\nformance. In contrast, our model proposes a co-interactive\nattention module to explicitly establish the bidirectional con-\nnection in a uniﬁed framework.\n5. CONCLUSION\nIn our paper, we proposed a co-interactive transformer for\njoint model slot ﬁlling and intent detection, which enables\nto fully take the advantage of the mutual interaction knowl-\nedge. Experiments on two datasets show the effectiveness of\nthe proposed models and our framework achieves the state-\nof-the-art performance.\n6. ACKNOWLEDGEMENTS\nThis work was supported by the National Key R&D Program\nof China via grant 2020AAA0106501 and the National Natu-\nral Science Foundation of China (NSFC) via grant 61976072\nand 61772153. This work was supported by the Zhejiang\nLab’s International Talent Fund for Young Professionals.\n7. REFERENCES\n[1] Gokhan Tur and Renato De Mori, Spoken language un-\nderstanding: Systems for extracting semantic informa-\ntion from speech, John Wiley & Sons, 2011.\n[2] Bing Liu and Ian Lane, “Attention-based recurrent neu-\nral network models for joint intent detection and slot ﬁll-\ning,” arXiv preprint arXiv:1609.01454, 2016.\n[3] Xiaodong Zhang and Houfeng Wang, “A joint model of\nintent determination and slot ﬁlling for spoken language\nunderstanding.,” in Proc. of IJCAI, 2016.\n[4] Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li\nHuo, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun-Nung\nChen, “Slot-gated modeling for joint slot ﬁlling and in-\ntent prediction,” in Proc. of NAACL, 2018.\n[5] Changliang Li, Liang Li, and Ji Qi, “A self-attentive\nmodel with gate mechanism for spoken language under-\nstanding,” in Proc. of EMNLP, 2018.\n[6] Libo Qin, Wanxiang Che, Yangming Li, Haoyang Wen,\nand Ting Liu, “A stack-propagation framework with\ntoken-level intent detection for spoken language under-\nstanding,” in Proc. of EMNLP, Nov. 2019.\n[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin, “Attention is all you need,”\nin NIPS. 2017.\n[8] Leyang Cui and Yue Zhang, “Hierarchically-reﬁned la-\nbel attention network for sequence labeling,” inProc. of\nEMNLP, 2019.\n[9] Alice Coucke, Alaa Saade, Adrien Ball, Th ´eodore\nBluche, Alexandre Caulier, David Leroy, Cl ´ement\nDoumouro, Thibault Gisselbrecht, Francesco Calta-\ngirone, Thibaut Lavril, et al., “Snips voice platform:\nan embedded spoken language understanding system\nfor private-by-design voice interfaces,” arXiv preprint\narXiv:1805.10190, 2018.\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova, “Bert: Pre-training of deep bidirec-\ntional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[11] Sepp Hochreiter and J ¨urgen Schmidhuber, “Long short-\nterm memory,” Neural computation, vol. 9, no. 8, 1997.\n[12] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton, “Layer normalization,” 2016.\n[13] Yoon Kim, “Convolutional neural networks for sentence\nclassiﬁcation,” in Proc. of EMNLP, Oct. 2014.\n[14] Haihong E, Peiqing Niu, Zhongfu Chen, and Meina\nSong, “A novel bi-directional interrelated model for\njoint intent detection and slot ﬁlling,” in Proc. of ACL,\n2019.\n[15] Yijin Liu, Fandong Meng, Jinchao Zhang, Jie Zhou,\nYufeng Chen, and Jinan Xu, “CM-net: A novel col-\nlaborative memory network for spoken language under-\nstanding,” in Proc. of EMNLP, 2019.\n[16] Charles T Hemphill, John J Godfrey, and George R Dod-\ndington, “The atis spoken language systems pilot cor-\npus,” in Speech and Natural Language: Proceedings of\na Workshop Held at Hidden Valley, Pennsylvania, June\n24-27, 1990, 1990.\n[17] Jeffrey Pennington, Richard Socher, and Christopher\nManning, “Glove: Global vectors for word represen-\ntation,” in Proc. of EMNLP, 2014.\n[18] Diederik P Kingma and Jimmy Ba, “Adam: A\nmethod for stochastic optimization,” arXiv preprint\narXiv:1412.6980, 2014.\n[19] Patrick Haffner, Gokhan Tur, and Jerry H Wright, “Opti-\nmizing svms for complex call classiﬁcation,” inIn Proc.\nof ICASSP, 2003.\n[20] Ruhi Sarikaya, Geoffrey E Hinton, and Bhuvana Ram-\nabhadran, “Deep belief nets for natural language call-\nrouting,” in Proc. of ICASSP, 2011.\n[21] Christian Raymond and Giuseppe Riccardi, “Genera-\ntive and discriminative algorithms for spoken language\nunderstanding,” in Eighth Annual Conference of the In-\nternational Speech Communication Association, 2007.\n[22] Puyang Xu and Ruhi Sarikaya, “Convolutional neural\nnetwork based triangular crf for joint intent detection\nand slot ﬁlling,” in Proc. of ASRU, 2013.\n[23] Kaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu, Geof-\nfrey Zweig, and Yangyang Shi, “Spoken language un-\nderstanding using long short-term memory neural net-\nworks,” in SLT, 2014.\n[24] Sendong Zhao, Ting Liu, Sicheng Zhao, and Fei Wang,\n“A neural multi-task learning framework to jointly\nmodel medical named entity recognition and normaliza-\ntion,” in Proc. of AAAI, 2019.\n[25] Rui Zhang, C ´ıcero Nogueira dos Santos, Michihiro Ya-\nsunaga, Bing Xiang, and Dragomir Radev, “Neural\ncoreference resolution with deep biafﬁne attention by\njoint mention detection and mention clustering,” in\nProc. of ACL, July 2018.\n[26] Yu Wang, Yilin Shen, and Hongxia Jin, “A bi-model\nbased rnn semantic frame parsing model for intent de-\ntection and slot ﬁlling,” in Proc. of NAACL, 2018.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8320066928863525
    },
    {
      "name": "Computer science",
      "score": 0.8117011189460754
    },
    {
      "name": "Mutual information",
      "score": 0.5388116240501404
    },
    {
      "name": "Task (project management)",
      "score": 0.5177949666976929
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3659444749355316
    },
    {
      "name": "Human–computer interaction",
      "score": 0.34729820489883423
    },
    {
      "name": "Voltage",
      "score": 0.18264025449752808
    },
    {
      "name": "Electrical engineering",
      "score": 0.07153844833374023
    },
    {
      "name": "Engineering",
      "score": 0.0661439299583435
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ]
}