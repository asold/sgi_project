{
  "title": "Prediction Of O-Glycosylation Site Using Pre-Trained Language Model And Machine Learning",
  "url": "https://openalex.org/W4361010297",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2554009257",
      "name": "Alhasan Alkuhlani",
      "affiliations": [
        "Ain Shams University"
      ]
    },
    {
      "id": "https://openalex.org/A2231641322",
      "name": "Walaa Gad",
      "affiliations": [
        "Ain Shams University"
      ]
    },
    {
      "id": "https://openalex.org/A2084162820",
      "name": "Mohamed Roushdy",
      "affiliations": [
        "Future University in Egypt"
      ]
    },
    {
      "id": "https://openalex.org/A4213849588",
      "name": "Abdel Badeeh Salem",
      "affiliations": [
        "Ain Shams University"
      ]
    },
    {
      "id": "https://openalex.org/A2554009257",
      "name": "Alhasan Alkuhlani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2231641322",
      "name": "Walaa Gad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2084162820",
      "name": "Mohamed Roushdy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4213849588",
      "name": "Abdel Badeeh Salem",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6603692524",
    "https://openalex.org/W6601977772",
    "https://openalex.org/W6608146008",
    "https://openalex.org/W4220673041",
    "https://openalex.org/W2170747616",
    "https://openalex.org/W2914496529",
    "https://openalex.org/W2922210059",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W2164834993",
    "https://openalex.org/W2088456241",
    "https://openalex.org/W4225868104",
    "https://openalex.org/W3009178113",
    "https://openalex.org/W2107815233",
    "https://openalex.org/W1546928902",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W3158236124",
    "https://openalex.org/W3166142427",
    "https://openalex.org/W2141818629",
    "https://openalex.org/W2103525038",
    "https://openalex.org/W2063768466",
    "https://openalex.org/W3120137197",
    "https://openalex.org/W2951433247",
    "https://openalex.org/W2076746539",
    "https://openalex.org/W2166720212"
  ],
  "abstract": "O-glycosylation is a typical type of protein post-translational modifications (PTMs), which is linked to several diseases and has significant roles in many biological processes. Identification of O-glycosylation sites is important to know the mechanism of the O-glycosylation process. However, the identification of PTM sites by laboratory experimental tools is time and money-consuming. Thus, the utilization of computational and artificial intelligence is becoming essential to predict O-glycosylation sites. In this paper, we proposed a new model to improve O-glycosylation site prediction using a transformer-based protein language model and machine learning. The dataset was collected and prepared from a recent data source called OGP (O-glycoprotein repository). The TAPE (Tasks Assessing Protein Embeddings) protein language model was used to feature extraction from the peptide sequences using the embedding strategy. Then, feature selection was implemented using the linear support vector machine (SVM) to select informative features. The XGBoost ensemble-based machine learning method was utilized for classification and prediction. The proposed model achieved high-performance results with 0.7761 accuracy, 0.7391 sensitivity, 0.8130 specificity, 0.8295 AUC, and 0.5537 MCC when compared with the traditional machine learning methods. On an independent dataset, the proposed method performed better than the latest available methods for predicting O-glycosylation sites.",
  "full_text": "IJICIS, Vol.23, No.1, 41-52 \nDOI: 10.21608/ijicis.2023.160986.1218 \n*Corresponding Author: Alhasan Alkuhlani \nComputer Science Department, Faculty of Computer and Information Sciences, Ain shams University, Cairo, Egypt  \nEmail address: Alhasan.alkuhlani@gmail.com \n \n \n \nInternational Journal of Intelligent \nComputing and Information Sciences \n \n \n \nPREDICTION OF O-GLYCOSYLATION SITE USING PRE-TRAINED \nLANGUAGE MODEL AND MACHINE LEARNING \n \nAlhasan Alkuhlani* \n \nComputer Science \nDepartment., \nFaculty of Computer and \nInformation Sciences, Ain \nshams University, \nCairo, Egypt \nAlhasan.alkuhlani@gmail.com \nWalaa Gad \n \nInformation system \nDepartment., \nFaculty of Computer and \nInformation Sciences,Ain \nshams University, \nCairo, Egypt \nwalaagad@cis.asu.edu.eg \nMohamed Roushdy \n \nComputer Science \nDepartment., \nFaculty of Computers and \nInformation Technology, \nFuture University in Egypt, \nCairo, Egypt  \nmohamed.roushdy@fue.edu.eg \n \nAbdel-badeeh M. \nSalem \n \nComputer Science \nDepartment., \nFaculty of Computer and \nInformation Sciences, Ain \nshams University, \nCairo, Egypt \nabsalem@cis.asu.edu.eg \nReceived 2022-09-05; Revised 2023-12-24; Accepted 2023-02-27 \nAbstract: O-glycosylation is a typical type of protein post-translational modifications (PTMs), which is \nlinked to several diseases and has significant roles in many biological processes.  Identification of O -\nglycosylation sites is important to know the mechanism of the O -glycosylation process. However, the \nidentification of PTM sites by laboratory experimental tools is time and money -consuming. Thus, the \nutilization of computational and ar tificial intelligence is becoming essential to predict O -glycosylation \nsites. In this paper, we proposed a new model to improve O-glycosylation site prediction using a \ntransformer-based protein language model and machine learning. The dataset was collected  and \nprepared from a recent data source called OGP (O-glycoprotein repository). The TAPE (Tasks Assessing \nProtein Embeddings) protein language model was used to feature extraction from the peptide sequences \nusing the embedding strategy. Then, feature selec tion was implemented using the linear support vector \nmachine (SVM) to select informative features. The XGBoost ensemble -based machine learning method \nwas utilized for classification and prediction. The proposed model achieved high -performance results \nwith 0.7761 accuracy, 0.7391 sensitivity, 0.8130 specificity, 0.8295 AUC, and 0.5537 MCC  when \ncompared with the traditional machine learning methods. On an independent  dataset, the proposed \nmethod performed better than the latest available methods for predicting O-glycosylation sites.   \n \nKeywords: protein language model , machine learning, XGBoost, Bioinformatics , O-glycosylation site \nprediction.  \n \n1. Inroduction  \n \n1.1. Overview  \n \nhttps://ijicis.journals.ekb.eg/ \n \n42  Alhasan Alkuhlani et al. \nPost-translational modifications (PTMs) of proteins are the chemical alterations that take place after the \nprotein is produced.  Protein PTMs are fundamental for the structure, maturation, and functions of \nproteins. Thus, detecting and comprehending PTMs is crucial in cell biology research as well as disease \nprevention and t reatment [1]. Glycosylation is  a common type of PTMs in which a complex group of \nglycan is enzymatically linked to the amino acids of proteins. Recent studies reported that abnormal \nglycosylation causes various diseases like cancer, diabetes, and immunity diseases [2â€“4]. \n \nOne of the major types of glycosylation is O-glycosylation, in which a glycan is joined to the hydroxyl \ngroup of a protein's serine (S) or threonine (T) amino acid.  Prediction of glycosylation sites aids in \nunderstanding the biological process of glycosylation as well as helps in the treatment of diseases that are \nassociated with it [2,5]. In order to improve O -glycosylation site identification as well as to reduce \nexperimental effort and cost, computational intelligence and machine learning techniques have been \nconsidered for O-glycosylation site prediction. Bioinformatics applications are increasingly centered on \nartificial intelligence, including machine learning and deep learning. \n \nO-glycosylation site prediction has undergone a great deal of research and advancement, but more work \nis still needed in this area due to the importance of this task, performance improvement needed as well as \nthe enormous amount of data that is continually being revised.  Most of the previous studies for O -\nglycosylation site prediction use various traditional feature extraction met hods to encode protein or \npeptide sequence information. Sequence-based, structural -based, evolutionary, and multiple sequence \nalignments (MSAs) are some examples of these approach categories.  As the peptide sequence is a \nsequence of alphabet characters, we here benefit from the embedding of the deep learning language model \ncalled Protein language models (PLMs) that exist in the natural language processing (NLP) field. PLMs \nare transformed-based language models derived from state -of-the-art NLP language models like BERT, \nALBERT, and XLNet that are trained on huge protein sequences [6]. The deep learning transformer in \nPLMs used to capture the contextual information embedd ed in the amino acids protein sequence. PLMs \nemployed the masked language model that has the ability to create context information around each amino \nacid and assess its significance in that context  [7,8]. Multiple protein language models have been \nsuccessfully applied for protein sequence embedding and analysis such as ProtBERT, ProtAlbert, \nProtXLNet [9], ESM [10], and TAPE [11].   \n \n1.2. Literature Review \n \nPreviously many computational methods have been applied successfully for O -glycosylation site \nprediction using machine learning methods. NetOGlyc tool [12] was presented for predicting the mucin-\ntype of O-glycosylation sites by neural networks classifier and using amino acid composition, and surface \naccessibility features. Oglyc method [13] was built for O-glycosylation site prediction by support vector \nmachine (SVM) method and based on binary profile features and physicochemical properties of protein \npeptides. Caragea et al. constructed EnsembleGly [14] method for the prediction of O-glycosylation sites \nusing ensembles of SVM that outperformed ot her classifiers. CKSAAP_OGlySite tool [15] used SVM \nand â€œcomposition of k -spaced amino acid pairs (CKSA AP)â€ properties  for O-glycosylation site \nprediction. GPP tool was proposed by Hamby  [16] to identify O-glycosylation sites using pairwise \nsequence patterns of amino acid sequences and the random forest (RF) classifier. GlycoEP predictor [3] \nwas developed to identify O-glycosylation sites using multiple classifiers where the SVM model \noutperformed the other models. \n \nPREDICTION OF O-GLYCOSYLATION SITE USING PRE-TRAINED LANGUAGE MODEL AND MACHINE \nLEARNING                                                                                                                                                                                 \n43 \n \nGlycoMine [17] used the random forest classifier to identify O-glycosylation sites using heterogeneous \nfunctional and sequence features. O -GlcNAcPRED-II tool [4] was presented for O-glycosylation site  \nprediction using the rotation forest ensembled method that divided the extracted features into four \npredictors. They implemented the fuzzy under-sampling method (KPCA -FUS) and K -means principal \ncomponent analysis oversampling approach for creating a balanced dataset. SPRINT-Gly prediction tool \n[18] by deep neural networks on human and mouse datasets and us ed various feature  extraction \ntechniques. GlycoMine_PU tool [5] was proposed to predict O -glycosylation using positive unlabeled \n(PU) learning approach. Multiple sequences, functional, and structural-based features were extracted from \nprotein sequences. Zhu et al. developed Captor predictor [19] for o-glycosylation site prediction using \nSVM and utilizing multiple sequence-based feature extraction methods on the OGP dataset.  \n \nIn this study, we propose a new method to improve the O-glycosylation site prediction. Firstly, the latest \ndata is collected and preprocessed from the OGP repository. Then, we use the TAPE (Tasks Assessing \nProtein Embeddings) pre-trained protein language model for feature embedding and representation from \nthe peptide sequences.  After that, machine learning algorithms are used to feature selection, model \nconstruction, and prediction.  Linear SVM is used for selecting the best extracted features followed by \nthe XGBoost ensemble classifier for training a nd prediction. Finally, cross -validation and independent \ntesting are employed to estimate and compare the proposed model based on MCC, accuracy, sensitivity, \nspecificity, and AUC performance metrics.  The paper contributes to developing a new method for O-\nglycosylation site prediction using machine learning and a pre -trained protein language model. All the \nprevious studies used s equence, structural, and evolutionary-based features extracted from the protein \nsequences. We here utilize the protein language model for feature embedding instead of the traditional \nmethods for feature extraction. The proposed method achieved high-performance results when compared \nwith the previous recent studies on the same dataset. The remaining of the paper is organized as follows: \nsection 2 describes the materials and methods used in this work including the dataset, protein language \nmodel, machine learning method, and evaluation measures. Section 3 presents the experimental \nperformance results and discussion. The last section presents the conclusions and future work. \n \n2. Materials and Methods \n \n2.1. Overview \n \nThe General diagram for the proposed method for O-glycosylation site prediction is illustrated in Figure. \n1. The figure involves four blocks in which each one representing the main step of the proposed method. \nThe first step represents collecting and preprocessing the used dataset from the OGP repository. The data \nis split into training and independent datasets. Secondly, the TAPE protein language model is used to \nextract features from the peptide sequences using embedding. After that machine learning techniques are \nutilized for feature selection and classification. Linear SVM is implemented for selecting informatic and \nsignificant features followed by  the XGBoost method for training the proposed model. Finally, cross -\nvalidation and independent testing approaches are employed for evaluating and comparing the proposed \nmethod based on five performance metrics.  \n \n44  Alhasan Alkuhlani et al. \n \n \nFigure 1. Overall flowchart of the proposed method.  \n \n2.2. Dataset Collection and Preprocessing \n \nIn this work, the dataset is collected from the O-glycoprotein repository  (OGP) [20] \n(http://www.oglyp.org/download.php). OGP is a database for O-glycosylation sites that contains 2133 O-\nglycoproteins with 9354 verified O-glycosylation sites. We employ only the human O-glycoproteins that \ninclude 1476 glycoproteins with 7038 verified O-glycosylation sites. After that, the CD-HIT program \n[21] is used to remove redundancy with an identity of over 30%. As result, the used O-glycoproteins are \nreduced to 1173 O -glycoproteins with 4526 verified O -glycosylation sites . O-glycosylation sites may \noccur in any serine (S) or threonine (T) amino acids in the glycoproteins sequence. However, not all these \nsites are considered O-glycosylation sites. We consider the verified O-glycosylation sites acquired from \nOGP as positive sites or O-glycosylation sites. All the other serine (S) or threonine (T) sites are considered \nnegative sites or non -O-glycosylation sites. The s liding window approach was used for sample \nconstruction that divides the sequence into fragments called peptides. Like the Captor study [19] that we \ncompare with, window size 31 is used to construct the O -glycosylation sites in which 15 amino acids \ndownstream and 15 amino acids upstream around the serine (S) or threonine (T) amino acid. Thus, each \nsample has a length of 31 amino acids. If the length of the sample is less than 31, we extend the sample \nwith the non-known amino acid â€œXâ€.  \n \nThe redundancy samples with a similarity of over 30% are also removed using the CD -HIT program to \navoid classification overfitting. The number of positive samples result ing after redundancy removal  is \n2816 positive samples. To improve prediction performance, 2816 negative samples are selected for \nconstructing a balanced O-glycosylation training dataset. To fairly compare with the recent previous \nstudies, we use the same independent dataset that was used in the Captor study for evaluation and \ncomparison. The independent dataset does not include in the training set and holds 230 positive vs 230 \nnegative samples.  Figure 2. shows the window size for each sample with a length of 31 in which the \nserine (S) or threonine (T) amino acid is at center with 15 residues from its left and 15 residues from its \nright. The figure also illustrates the two-sample logos [22] of the frequencies of amino acids  around \npositive O-glycosylation sites compared to negative sites using all sample sequences. The figure shows \nthat the Proline (P) amino acids are enriched around the O-glycosylation sites, especially in the sites (-1, \n\nPREDICTION OF O-GLYCOSYLATION SITE USING PRE-TRAINED LANGUAGE MODEL AND MACHINE \nLEARNING                                                                                                                                                                                 \n45 \n \n+3, +1, +2, +4, -3, -2). The Alanine (A) amino acids are also enriched around O -glycosylation sites in \npositions (from -3 to +2). The threonine (T) amino acid is also enriched in position (+1).  It is also shown \nthat Leucine (L) is the most depleted amino acid near the O-glycosylation site. \n \n \n \nFigure 2. Two-sample logo for the frequencies of amino acids around the O-glycosylation sites. \n \n2.3. Feature Representation \n \nProtein sequences are fragmented into multiple to represent our peptide samples. Twenty symbol letters \nrepresent the amino acids of each sample including {A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, \nW, Y}. In addition, the letter â€˜Xâ€™ is used to represent unknown amino acids like {U, Z, O, B}. These \nsamples should be encoded to numerical format for training machine learning model s. this numerical \nformat represents the key properties and characteristics of the samples. Embedding features extracted \nfrom the TAPE [11] protein language model (PLM) from peptide sequences (samples) are used to \nrepresent the features. These embedding features contain information about the amino acid conversation \nin the peptide sequences. The process of TAPE embedding starts by dividing each sample sequence into \ntokens in which each character (amino acid) is represented by one token. Given the sample S = {t1, t2, â€¦, \ntn} where tj is a token in the position j in the sample S and n is the sampleâ€™s length. Embedding function \nFemb can be represented as: \n[ğ‘£1, ğ‘£2, â€¦ , ğ‘£ğ‘›] =  ğ¹ğ‘’ğ‘šğ‘ (ğ‘¡1, ğ‘¡2, â€¦ , ğ‘¡ğ‘›)       (1) \nWhere vj is the features embedded for token tj which are numeric vectors. \nTAPE PLM is based on the BERT masked language model that is calculated by the product of the \nconditional probabilities of tokens in each site given all other tokens in the sequence by replacing the \ntoken at each position with the masked token. This enables conditional non-independence between tokens \nto be obtained. The masked language model is formulated as: \nğ‘(ğ‘¡) = âˆ ğ‘(ğ‘¡ğ‘–\nğ‘›\nğ‘–\n|ğ‘¡1, ğ‘¡2, â€¦ , ğ‘¡ğ‘–âˆ’1, ğ‘¡ğ‘–+1, â€¦ , ğ‘¡ğ‘›)       (2) \n\n46  Alhasan Alkuhlani et al. \nWhere ti is the token in site i and n is the size of the sample sequence. \n \nThe TAPE protein language model is trained on a large database called the Pfam database. The Pfam \ndatabase contains over 31 protein sequences . The last hidden layer in the TAPE transformer is used to \nextract the embedded features. The length of extracted features for each token is 768. In our case , the \nlength of each sample is 31 tokens. So, the size of extracted futures using TAPE is 768 * 31 =  23,808 \nfeatures. The properties of the TAPE transformer are shown in Table 1. The table illustrate s the key \nproperties of the TAPE transformer including: the number of layers, size of hidden layers, parameters â€™ \nnumber, and number of attention heads. \n \nTable 1. Properties of TAPE Transformer. \n \nProperty Value \nnumber of layers 12 \nsize of hidden layers 768 \nnumber of parameters 92M \nnumber of attention heads 12 \n \n2.4. Feature Selection  \n \nThe extracted features from TAPE embedding are large (23,808 features) that can include noisy and \nirrelevant features that have a possibly undesirable effect on prediction results. Thus,  significant and \nrelevant features are selected by feature selection methods in order to avoid overfitting in the training \nprocess as well as to enhance prediction performance. To select the important features, we employed the \nlinear SVM feature selection approach that is comparable with conventional feature selection methods, \nlike information gain  and odds ratio  [23]. The data sample vector can be represented as ğ‘¥ğ‘– =\n(ğ‘¥ğ‘–1, ğ‘¥ğ‘–2, â€¦ , ğ‘¥ğ‘–ğ‘›) where n is the feature size for each sample. In the linear-kernel SVM, the predictor can \nbe represented:  \nğ‘¦Ì‚ =  âˆ‘ ğ›¼ğ‘–ğ¾(ğ‘¥ğ‘–, ğ‘¥) + ğ‘\nğ‘™\nğ‘–\n           (3) \nFor the linear kernel: \nğ¾(ğ‘¥ğ‘–, ğ‘¥) =  ğ‘¥ğ‘–\n  ğ‘‡. ğ‘¥                    (4) \nThe linear-kernel SVM predictor can be rewritten as: \nğ‘¦Ì‚ =  âˆ‘ ğ‘¤ğ‘—ğ‘¥(ğ‘—) + ğ‘           (5)\nğ‘›\nğ‘—\n \nWhere b is the b ias scaler, ğ›¼ is the i nitial values for the coefficients , and  ğ‘¤ğ‘—=âˆ‘ ğ›¼ğ‘–ğ‘¥ğ‘–\n(ğ‘—)ğ‘™\nğ‘–  . For feature \nselection, the absolute value |ğ‘¤ğ‘—| represents the weight for feature j. The features with the highest  \ncoefficient values of |ğ‘¤ğ‘—| are selected as optimal features for classification. As the features with low \nabsolute values of ğ‘¤ğ‘— have a low impact on the predictions . This indicates that these features are not \nimportant for training or classification, and as a result, they could be skipped over in the training stage as \nPREDICTION OF O-GLYCOSYLATION SITE USING PRE-TRAINED LANGUAGE MODEL AND MACHINE \nLEARNING                                                                                                                                                                                 \n47 \n \nwell [23]. We used LinearSVC and SelectFromModel functions in the Scikit-learn Python library [24] for \nfeature ranking and selecting the best features. The top 675 features are selected from the 23,808 extracted \nfeatures by the linear SVM.  \n \n2.5. Classification using XGBoost \n \nExtreme gradient boosting (XGBoost) is a state -of-the-art ensemble-based algorithm that is developed \nfor data classification. It has been proved that it outperformed the other traditional classifiers due to its \nscalability, efficiency, and speed.  XGBoost is based on gradient boosting (GB) and decision tree  \nalgorithms [25]. Distributed, parallel, and out-of-core computing make XGBoost faster than the other \nmachine learning algorithms. Both GB and XGBoost execute boosting learners using the gradient descent \nloss technique. GB and XGBoost can be clarified as follows  [26]. Given the dataset D=[x,y] where x \nrepresents the features and y represents the independent classes. In GB, assuming there are K boosts and \nB additive functions to predict the results. The ğ‘¦ğ‘–Ì‚ represent the prediction for the sample i in boost b and \nğ‘“ğ‘ denotes to the tree structure that has weight wj. Then the final prediction for the sample i is represented \nby: \nğ‘¦ğ‘–Ì‚ = âˆ‘ ğ‘“ğ‘(ğ‘¥ğ‘–)\nğµ\nğ‘=1\n       (6) \n The loss of the XGBoost prediction model is minimized by the gradient descent algorithm . XGBoost \nmultiple have hyperparameters that can be adjusted to avoid overfitting and improve performance. The \noptimal configuration for the XGBoost hyperparameters is clarified in section  3.1. We implement \nXGBoost by XGBoost python library [25]. \n \n2.6. Model Evaluation \n \nFirstly, ten-fold cross-validation is implemented on the training dataset in which the dataset is partitioned \nrandomly into ten folds. Then every fold is selected for the test and the other nine are for training and the \nresult is calculated by the average of the testing result. Secondly, an independent (blind) dataset from the \nbeginning is utilized for independent testing and compares the proposed model with the previous studies. \nFive common performance metrics are employed involving Matthewâ€™s correlation coefficient (MCC ), \nsensitivity, specificity, and accuracy. Moreover, AUC (Area Under the receiver operating characteristic \nCurve (ROC)) computes the classifier's capability to separate binary data by displaying the true positive \nrate against the false positive rate. These metrics are represented as: \n \n \nğ‘€ğ¶ğ¶ = ğ‘¡ğ‘ Ã— ğ‘¡ğ‘› âˆ’ ğ‘“ğ‘ Ã— ğ‘“ğ‘›\nâˆš(ğ‘¡ğ‘ + ğ‘“ğ‘)(ğ‘¡ğ‘ + ğ‘“ğ‘›)(ğ‘¡ğ‘› + ğ‘“ğ‘)(ğ‘¡ğ‘› + ğ‘“ğ‘›)\n              (7) \n \nAccuracy =  ğ‘¡ğ‘ + ğ‘¡ğ‘›\nğ‘¡ğ‘ + ğ‘“ğ‘ + ğ‘¡ğ‘› + ğ‘“ğ‘›              (8) \n \n \nSensitivity = ğ‘¡ğ‘\nğ‘“ğ‘› + ğ‘¡ğ‘              (9) \n \n48  Alhasan Alkuhlani et al. \nSpecificity =  ğ‘¡ğ‘›\nğ‘“ğ‘ + ğ‘¡ğ‘›              (10) \nwhere tp denotes the positive sitesâ€™ number that are truly classified, fp (false positive) denotes the positive \nsitesâ€™ number that are untruly classified , tn denotes the negative sites â€™ number that are truly classified, \nand fn denotes the negative sitesâ€™ number that are untruly classified.  \n \n3. Results and Discussion \n \n3.1. Parameter Setting \n \nXGBoost classifier has various hyperparameters that can be tuned to avoid training overfitting as well as \nto improve prediction performance. Table 2. shows the XGBoost hyperparameters, their experimented \nvalue ranges, and the optimal value used in the model construction.  \n \nTable 2. Hyperparameters setting for XGBoost. \n \nParameter Value Range Optimal Value \nlearning_rate 0 to 1 0.1 \nmax_depth 1 to 10 2 \nmin_child_weight 1 to 10 3 \nsubsample 0 to 1 0.7 \nbooster gbtree, gblinear Gbtree \n \nThe first tuned parameter is the learning rate that represents the step size shrinkage for overfitting reducing \nwhere the default value is 0.3. We tried the values 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, and 0.9 and the \noptimal performance was with the learning rate value 0.1. The second parameter is the max_depth which \ndenotes the maximum depth of the tree. If the value of max_depth increases, the model will be prone to \noverfitting. We tried the integer values between 1 and 10 and the optimal results were with the value 2. \nThe third parameter is the min_child_weight which represents the minimum allowable summation of \nchild weight. The model will be more conservative the greater min child's weight is. We tried the integer \nvalues between 1 and 10 and the opti mal results were with the value 3. The fourth parameter is the \nsubsample which represents the subsamples of the model before constructing the tree. The XGBoost \nwould randomly sample 50% of the training data before constructing trees if it was set to 0.5. I n each \nboosting cycle, subsampling will take place once. We tried the values 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, \nand 0.9 and the optimal performance was with the learning rate value 0.7. The last parameter is the booster \nwhich represents the method of boosting in which there are two values (gbtree used for tree-based model \nand gblinear used for the linear-based model). we used the gbtree booster. \n \n3.2. Evaluation using Cross-validation \n \nMany folds of cross-validation are implemented on the training dataset. In every implementation, data is \nrandomly split into m-fold in which m-1 is employed for training and one for testing. Three-fold, five-\nPREDICTION OF O-GLYCOSYLATION SITE USING PRE-TRAINED LANGUAGE MODEL AND MACHINE \nLEARNING                                                                                                                                                                                 \n49 \n \nfold, eight-fold, and ten-fold cross-validations are implemented. Table 3. Illustrates the results of these \nimplementations. We can observe  that the ten -fold cross-validation has the highest performance result \namong all the implemented cross -validations. The performance results are 0.748 accuracy, 0.747 3 \nsensitivity, 0.7484 AUC, and 0.4962 MCC.  \n \nTable 3. Cross-validation performance results on the training dataset. \nFold \nnumber Accuracy Sensitivity Specificity  AUC MCC \nthree-fold 0.7363 0.7335 0.7401 0.7368 0.4737 \nfive-fold 0.7404 0.7323 0.7493 0.7408 0.4815 \neight-fold 0.7383 0.7340 0.7429 0.7384 0.4773 \nten-fold 0.7480 0.7473 0.7494 0.7484 0.4962 \n \nFrom the table, it is observed that the five -fold has the closest performance results to the ten -fold. The \nten-fold improved the MCC and sensitivity by 0.01 % over The five-fold cross-validation. The three-fold \nand eight-fold are lower than the ten-fold by about 0.02% MCC.   \n \n3.3. Comparing with Machine Learning Algorithms \n \nXGBoost is compared to five machine learning algorithms: SVM, RF, Linear Regression (LR), NaÃ¯ve \nBase (NB), and KNN on the independent dataset. Table 4. illustrates the performance comparison between \ndifferent machine learning with our proposed method. The comparison shows that XGBoo st achieved \nhigh-performance results with 0.7761 accuracy, 0.7391 sensitivity, 0.8130 specificity, 0.8295 AUC, and \n0.5537 MCC. Figure 3. It is observed that XGBoost outperforms the other traditional machine learning \nalgorithms including SVM, RF, LR, NB, and KNN in terms of accuracy, specificity, AUC, and MCC  \nperformance metrics. In terms of sensitivity, KNN outperform s the other classifiers but with low \nperformance with the other metrics. The random forest classifier come s after XGBoost in performance. \nThat is mean that the tree-based classifiers performed better than the other machine learning algorithms. \n \nTable 4. Performance of XGBoost model with SVM, RF, LR, NB, and KNN on the independent dataset. \nClassifier Accuracy Sensitivity Specificity  AUC MCC \nSVM 0.7457 0.7609 0.7304 0.8273 0.4915 \nRF 0.75 0.7348 0.7652 0.8229 0.5002 \nLR 0.713 0.7391 0.6870 0.7841 0.4267 \nNB 0.7391 0.6696 0.8087 0.8104 0.483 \nKNN 0.7196 0.7696 0.6696 0.7844 0.4413 \nXGBoost 0.7761 0.7391 0.8130 0.8295 0.5537 \n \n3.4. Comparing to the Existing Tools \n \nThe proposed method is compared with two recent tools for O-glycosylation site prediction which include \nCaptor [19] and OGP [20] on the independent dataset. for fairly comparison, we used the same \nindependent set that was used in Captor. The performance results of the comparison are shown in          \n50  Alhasan Alkuhlani et al. \nFigure 3. It is clearly found that our proposed method outperforms Captor and OGP in terms of sensitivity, \nspecificity, AUC, and MCC. In terms of accuracy, the Captor exceeds our method by 1%. The proposed \nmethod improved the s ensitivity by 13% over Captro and 24% over OGP. The proposed met hod also \nimproved the specificity by 1% over Captro and 2% over OGP. Additionally, the proposed method \nincreased AUC by 12% compared to OGP and by 3% over Captro. The MCC was also improved by 40% \ncompared to OGP and about 30% compared to Captro by the proposed method. \n  \n \n \nFigure 3. Comparison of performance results of the proposed method against the existing tools on the independent dataset. \n \n4. Conclusions and Future Work \n \nIn this paper, we proposed a model for O -glycosylation site prediction using a pre -trained protein \nlanguage model and machine learning. The dataset is collected from the OGP repository and then it was \npreprocessed. The TAPE protein language model is employ ed for feature extraction by embedding \nstrategy. The extracted features are then reduced by the linear SVM method to avoid overfitting and \nimprove performance. XGBoost machine learning method was used for training and classification. The \nten-fold cross-validation and independent test were employed to evaluate and validate with accuracy, \nsensitivity, specificity, AUC, and MCC performance measures. The proposed model was compared with \nthe traditional machine learning methods using the independent dataset in which it outperformed the other \nmachine learning models. On the same independent dataset that is used by the Captro tool, the proposed \nmethod was compared to the latest tools for O -glycosylation site prediction including Capto r and OGP \ntools. The comparison results showed that the prop osed method outperformed other existing tools. This \nindicates that features extracted from protein language model embedding perform better than features \nextracted from traditional feature extraction methods like physicochemical, evolutionary, MSA, or \nstructural-based features. In the future, we plan to use other protein language models and machine learning \ntechniques to predict glycosylation sites. \n \nReferences \n[1] W. He, L. Wei, Q. Zou, Research progress in protein posttranslational modification site prediction, \nBrief. Funct. Genomics. 18 (2018) 220â€“229. \n[2] A. Alkuhlani, W. Gad, M. Roushdy, A. -B.M. Salem, Intelligent Techniques Analysis for \n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nAccuracy Sensitivity Specificity AUC MCC\nOGP Captor proposed method\nPREDICTION OF O-GLYCOSYLATION SITE USING PRE-TRAINED LANGUAGE MODEL AND MACHINE \nLEARNING                                                                                                                                                                                 \n51 \n \nGlycosylation Site Prediction, Curr. Bioinform. 16 (2021) 774â€“788. \n[3] J.S. Chauhan, A. Rao, G.P.S. Raghava, In silico platform for prediction of N-, O-and C-glycosites \nin eukaryotic protein sequences, PLoS One. 8 (2013) e67008. \n[4] C. Jia, Y. Zuo, Q. Zou, O -GlcNAcPRED-II: an integrated classification algorithm for identifying \nO-GlcNAcylation site s based on fuzzy undersampling and a K -means PCA oversampling \ntechnique, Bioinformatics. 34 (2018) 2029â€“2036. \n[5] F. Li, Y. Zhang, A.W. Purcell, G.I. Webb, K. -C. Chou, T. Lithgow, C. Li, J. Song, Positive -\nunlabelled learning of glycosylation sites in the human proteome, BMC Bioinformatics. 20 (2019) \n112. \n[6] C. Marquet, M. Heinzinger, T. Olenyi, C. Dallago, K. Erckert, M. Bernhofer, D. Nechaev, B. Rost, \nEmbeddings from protein language models predict conservation and variant effects, Hum. Genet. \n(2021).  \n[7] D. Ofer, N. Brandes, M. Linial, The language of proteins: NLP, machine learning & protein \nsequences, Comput. Struct. Biotechnol. J. 19 (2021) 1750â€“1758.  \n[8] T. Bepler, B. Berger, Learning the protein language: Evolution, structure, and function, Cell Syst. \n12 (2021) 654-669.e3.  \n[9] A. Elnaggar, M. Heinzinger, C. Dallago, G. Rehawi, Y. Wang, L. Jones, T. Gibbs, T. Feher, C. \nAngerer, M. Steinegger, D. Bhowmik, B. Rost, ProtTrans: Towards Cracking the Language of \nLifeâ€™s Code Through Self-Supervised Learning, Ieee Trans Pattern Anal. Mach. Intell. 14 (2021).  \n[10] A. Rives, J. Meier, T. Sercu, S. Goyal, Z. Lin, J. Liu, D. Guo, M. Ott, C.L. Zitnick, J. Ma, R. \nFergus, Biological structure and function emerge from scaling unsupervised learning to 250 million \nprotein sequences, Proc. Natl. Acad. Sci. U. S. A. 118 (2021).  \n[11] R. Rao, N. Bhattacharya, N. Thomas, Y. Duan, X. Chen, J. Canny, P. Abbeel, Y.S. Song, \nEvaluating protein transfer learning with TAPE, Adv. Neural Inf. Process. Syst. 32 (2019) 1â€“20. \n[12] K. Julenius, A. MÃ¸lgaard, R. Gupta, S. Brunak, Prediction, conservation analysis, and structural \ncharacterization of mammalian mucin -type O-glycosylation sites, Glycobiology. 15 (2005) 153 â€“\n164. \n[13] S. Li, B. Liu, R. Zeng, Y. Cai, Y. Li, Predicting O -glycosylation sites in mammalian proteins by \nusing SVMs, Comput. Biol. Chem. 30 (2006) 203â€“208. \n[14] C. Caragea, J. Sinapov, A. Silvescu, D. Dobbs, V. Honavar, Glycosylation site prediction using \nensembles of Support Vector Machine classifiers, BMC Bioinformatics. 8 (2007) 438. \n[15] Y.-Z. Chen, Y.-R. Tang, Z.-Y. Sheng, Z. Zhang, Prediction of mucin-type O-glycosylation sites in \nmammalian proteins using the composition of k-spaced amino acid pairs, BMC Bioinformatics. 9 \n(2008) 101. \n[16] S.E. Hamby, J.D. Hirst, Pred iction of glycosylation sites using random forests, BMC \nBioinformatics. 9 (2008) 500. \n[17] F. Li, C. Li, M. Wang, G.I. Webb, Y. Zhang, J.C. Whisstock, J. Song, GlycoMine: a machine \nlearning-based approach for predicting N-, C-and O-linked glycosylation in the human proteome, \nBioinformatics. 31 (2015) 1411â€“1419. \n[18] G. Taherzadeh, A. Dehzangi, M. Golchin, Y. Zhou, M.P. Campbell, SPRINT -Gly: Predicting N-\n52  Alhasan Alkuhlani et al. \nand O-linked glycosylation sites of human and mouse proteins by using sequence and predicted \nstructural properties, Bioinformatics. 35 (2019) 4140â€“4146. \n[19] Y. Zhu, S. Yin, J. Zheng, Y. Shi, C. Jia, O -glycosylation site prediction for Homo sapiens by \ncombining properties and sequence features with support vector machine, J. Bioinform. Comput. \nBiol. 20 (2022) 2150029. \n[20] J. Huang, M. Wu, Y. Zhang, S. Kong, M. Liu, B. Jiang, P. Yang, W. Cao, OGP: a repository of \nexperimentally characterized O-Glycoproteins to facilitate studies on O-Glycosylation, Genomics, \nProteomics \\& Bioinforma. 19 (2021) 611â€“618. \n[21] L. Fu, B. Niu, Z. Zhu, S. Wu, W. Li, CD -HIT: accelerated for clustering the next -generation \nsequencing data, Bioinformatics. 28 (2012) 3150â€“3152. \n[22] V. Vacic, L.M. Iakoucheva, P. Radivojac, Two Sample Logo: a graphical representation of the \ndifferences between two sets of sequence alignments, Bioinformatics. 22 (2006) 1536â€“1537. \n[23] J. Brank, M. Grobelnik, N. Milic -Frayling, D. Mladenic, Feature selection using support vector \nmachines, WIT Trans. Inf. Commun. Technol. 28 (2002). \n[24] F. Pedregosa, G. Varoq uaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. \nPrettenhofer, R. Weiss, V. Dubourg, others, Scikit -learn: Machine learning in Python, J. Mach. \nLearn. Res. 12 (2011) 2825â€“2830. \n[25] T. Chen, C. Guestrin, Xgboost: A scalable tree boosting  system, in: Proc. 22nd Acm Sigkdd Int. \nConf. Knowl. Discov. Data Min., 2016: pp. 785â€“794. \n[26] M.M. Bassiouni, I. Hegazy, N. Rizk, E. -S.A. El-Dahshan, A.M. Salem, deep learning approach \nbased on transfer learning with different classifiers for ecg diagnos is, Int. J. Intell. Comput. Inf. \nSci. 22 (2022) 44â€“62. \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6433358192443848
    },
    {
      "name": "Glycosylation",
      "score": 0.5343267917633057
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4175885319709778
    },
    {
      "name": "Natural language processing",
      "score": 0.3729500472545624
    },
    {
      "name": "Chemistry",
      "score": 0.15000668168067932
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}