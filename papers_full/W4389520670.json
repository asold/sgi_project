{
  "title": "Enabling Large Language Models to Generate Text with Citations",
  "url": "https://openalex.org/W4389520670",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5033296587",
      "name": "Tianyu Gao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5111465448",
      "name": "H. W. Yen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101383362",
      "name": "Jiatong Yu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5051064208",
      "name": "Danqi Chen",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4311731003",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4221164017",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W4225933709",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4287855110",
    "https://openalex.org/W4319049323",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4385573102",
    "https://openalex.org/W4389519041",
    "https://openalex.org/W3170180819",
    "https://openalex.org/W4385573898",
    "https://openalex.org/W4287332927",
    "https://openalex.org/W4313483736",
    "https://openalex.org/W2788496822",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W4381432831",
    "https://openalex.org/W3100292568",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4389519118",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4389518954",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2950681488",
    "https://openalex.org/W4389518925",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3169841173",
    "https://openalex.org/W3034188538",
    "https://openalex.org/W2932893307",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4389519050",
    "https://openalex.org/W4281736354",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W3201081655",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W4366341216",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4385570481"
  ],
  "abstract": "Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs’ Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions—fluency, correctness, and citation quality—and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement—For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6465–6488\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nEnabling Large Language Models to Generate Text with Citations\nTianyu Gao Howard Yen Jiatong Yu Danqi Chen\nDepartment of Computer Science & Princeton Language and Intelligence\nPrinceton University\n{tianyug,hyen,jiatongy,danqic}@cs.princeton.edu\nAbstract\nLarge language models (LLMs) have emerged\nas a widely-used tool for information seeking,\nbut their generated outputs are prone to halluci-\nnation. In this work, our aim is to allow LLMs\nto generate text with citations, improving their\nfactual correctness and verifiability. Existing\nwork mainly relies on commercial search en-\ngines and human evaluation, making it chal-\nlenging to reproduce and compare different\nmodeling approaches. We propose ALCE, the\nfirst benchmark for Automatic LLMs’ Citation\nEvaluation. ALCE collects a diverse set of\nquestions and retrieval corpora and requires\nbuilding end-to-end systems to retrieve sup-\nporting evidence and generate answers with\ncitations. We develop automatic metrics along\nthree dimensions—fluency, correctness, and ci-\ntation quality—and demonstrate their strong\ncorrelation with human judgements. Our exper-\niments with state-of-the-art LLMs and novel\nprompting strategies show that current systems\nhave considerable room for improvement—For\nexample, on the ELI5 dataset, even the best\nmodels lack complete citation support 50% of\nthe time. Our analyses further highlight promis-\ning future directions, including developing bet-\nter retrievers, advancing long-context LLMs,\nand improving the ability to synthesize infor-\nmation from multiple sources.1\n1 Introduction\nLarge language models (LLMs; Brown et al., 2020;\nOpenAI, 2023) have gained increasing popularity\nas a tool for information seeking. While they gener-\nate engaging and coherent responses, their outputs\nare prone to hallucination and often contain fac-\ntually incorrect information (Ji et al., 2023). This\nmakes it harder for users to trust and verify LLM-\ngenerated outputs without any supporting evidence.\nIn this work, we study a new generation\nparadigm for LLMs, in which we require LLMs\n1Our code and data are available at https://github.\ncom/princeton-nlp/ALCE.\nWhen did the US break away from England?\nThe US took the first step towards gaining independence from \nGB when it declared independence on July 2, 1776 (although the \nevent is now commemorated on July 4, 1776, the date when the \nDeclaration of Independence was oﬀicially adopted by Congress) \n[1][2]. The Treaty of Paris was later signed on September 3, 1783, \nformally separating the United States from the British Empire [3].\nQuestion\nModel output\nLLM\nCorpus\nRetrieve\n[1] Decolonization of \nthe Americas:  … in-\ndependence from GB \non July 2, 1776 …\n[3] American Revolu-\ntion: … The Treaty of \nParis was signed \nSeptember 3, 1783 ...\n[2] Decolonization: …  \nIt was oﬀicially adop-\nted by Congress on \nJuly 4, 1776 …\n…\nFigure 1: The task setup of ALCE. Given a question,\nthe system generates text while providing citing pas-\nsages from a large retrieval corpus. Each statement may\ncontain multiple citations (e.g., [1][2]).\nto provide citations to one or a few text passages\nfor any statement they generate (Figure 1). Incor-\nporating citations brings several benefits: (1) users\ncan easily verify LLMs’ claims with the provided\ncitations; (2) LLMs can generate text that faithfully\nfollows cited passages, which has the promise to\nimprove correctness and alleviate hallucination.\nMultiple commercial systems have adopted this\nparadigm: Bing Chat2 and perplexity.ai3 respond to\nuser questions in natural language with references\nto Web pages. Nakano et al. (2021); Menick et al.\n(2022) share a similar motivation, but they mainly\nexperiment with commercial search engines and\nclosed-source models, making their results difficult\nto evaluate. Retrieval-augmented LMs (Borgeaud\net al., 2022; Izacard et al., 2022) incorporate re-\ntrieved passages during both training and infer-\nence, but do not guarantee faithfulness to retrieved\npassages or explicitly provide citations. Addition-\nally, previous studies mostly rely on human eval-\nuation (Nakano et al., 2021; Menick et al., 2022;\nLiu et al., 2023), which is expensive and difficult to\nreproduce. We argue that the absence of automated\nevaluation hinders the advances of such systems.\n2https://www.bing.com/new\n3https://www.perplexity.ai\n6465\nDataset Corpus (#passages) Question type Example\nASQA Wikipedia (21M) Factoid (ambiguous) Q: When did the US break away from England?\nA: The US declared independence on July 2, 1776 [1][2] ... The\nTreaty of Paris was later signed on September 3, 1783 [3].\nQAMPARI Wikipedia (21M) Factoid (list) Q: Which films have Gong Li as a member of their cast?\nA: The Story of Qiu Ju [1], Farewell My Concubine [2], The\nMonkey King 2 [3], Mulan [3], Saturday Fiction [3] ...\nELI5 Sphere (899M) Why/How/What Q: How do student loans affect getting a mortgage?\nA: Student loans can affect the debt to income ratio [1], which is\na key factor in determining the amount that ... [2][3]\nTable 1: The three datasets used in our ALCE benchmark. These datasets cover a wide range of question types and\nthe corresponding corpora span from Wikipedia to Web-scale document collection.\nWe present ALCE, the first reproducible bench-\nmark for automatically evaluating LLMs’ gener-\nations with citations. ALCE assumes a natural-\nlanguage question and a retrieval corpus, and re-\nquires building end-to-end systems to retrieve rel-\nevant passages from the corpus, generate a re-\nsponse to the question, and cite corresponding sup-\nporting passages. We compile three datasets that\ncover different types of questions and corpora—\nASQA (Stelmakh et al., 2022), QAMPARI (Ru-\nbin et al., 2022), and ELI5 (Fan et al., 2019)—as\nshown in Table 1. Different from previous bench-\nmarks (Lee et al., 2019; Bohnet et al., 2022), ALCE\nevaluates long-text generation, focusing on auto-\nmatically evaluating citation quality, and allows\nciting multiple passages for individual statements.\nWe design automatic evaluation methods in three\ndimensions: fluency, correctness, and citation\nquality. Specifically, we use MAUVE (Pillutla\net al., 2021) to measure fluency, propose tailored\ncorrectness metrics for each dataset, and adopt a\nnatural language inference (NLI) model (Honovich\net al., 2022) to measure citation quality. We show-\ncase how the three dimensions together contribute\nto a robust evaluation, preventing systems from ex-\nploiting shortcuts. Additionally, we conduct human\nevaluation and demonstrate a strong correlation\nwith our automatic metrics.\nWe experiment on multiple systems with state-\nof-the-art LLMs and retrievers and also propose\nnovel prompting strategies to synthesize retrieved\ntext into text generation. Although all systems are\ncapable of providing fluent and coherent responses,\nthere remains substantial room for improvement in\nterms of correctness and citation quality: For exam-\nple, on the ELI5 dataset, around 50% generations\nof our ChatGPT and GPT-4 baselines are not fully\nsupported by the cited passages. Additionally, we\nfind that (1) a closed-book model (generating an-\nswers without accessing any retrieved documents)\nwith post-hoc citing achieves good correctness but\nmuch worse citation quality; (2) although interac-\ntive retrieval approaches (Yao et al., 2023; Schick\net al., 2023) offer more flexibility in when/what\nto retrieve, they do not improve the performance\non this challenging benchmark; (3) summarizing\nthe retrieved passages in a shorter text improves\ncorrectness but not citation quality; (4) reranking\nmultiple generations boosts citation quality mea-\nsured by human evaluation; (5) incorporating more\nretrieved passages in context does not help Chat-\nGPT but improves GPT-4 performance.\nOur extensive analyses highlight three major\nchallenges of building LLMs to generate text with\ncitations: (1) the retrieval quality is crucial to the\nfinal performance and has substantial room for im-\nprovement; (2) LLMs’ limited context window re-\nstricts the number of passages they can incorporate;\n(3) current LLMs struggle to synthesize multiple\ndocuments in context without being distracted by\nirrelevant ones, although better instruction tuning\nbrings significant improvement. These challenges\npose promising research directions for developing\nbetter systems integrating retrieval and LLMs.\n2 Task Setup and Datasets\nOur task is formalized as follows: Given a query\nq and a corpus of text passages D, the system\nis required to return an output S, which consists\nof nstatements s1, ..., sn, and each statement si\ncites a list of passages Ci = {ci,1,ci,2,... }4, where\nci,j ∈D. In this work, we segment LLMs’ output\ninto statements by sentence boundaries. 5 While\nLLMs may include sentences that do not require a\ncitation, such as “I’m happy to help”, we observe\nthat almost all sentences that LLMs output provide\n4In practice, we allow at most 3 citations for each state-\nment as more citations usually do not help.\n5QAMPARI requires a list as the answer, and we choose\neach entity in the generated list as a statement.\n6466\nvaluable information and require citations, similar\nto findings in Liu et al. (2023). In this work, cita-\ntions are enclosed by box brackets such as [1][2].\nWe divide the corpus Dinto 100-word passages\nfollowing previous works on open-domain question\nanswering (Karpukhin et al., 2020; Petroni et al.,\n2021; Piktus et al., 2021), in contrast to commer-\ncial systems like Bing Chat, which cite entire Web\npages. We take 100-word passages because it is\neasier for humans to verify, and allows for more\nretrieved passages to fit in LLMs’ limited context.\nWe choose QA datasets so that (1) they contain\nfactual questions, in which references are impor-\ntant; (2) questions require long-text answers that\ncover multiple aspects; (3) answering the questions\nrequires synthesizing multiple sources. We select\nthree datasets (Table 1) and introduce them below.\nSee §B for additional statistics.\nASQA (Stelmakh et al., 2022) is a long-form fac-\ntoid dataset. As shown in Figure 1, each question is\nan ambiguous question from AmbigQA (Min et al.,\n2020) that requires multiple short answers to cover\ndifferent aspects, and the dataset provides a long-\nform answer that covers all short answers. Since\nmost questions can be answered by Wikipedia, we\nuse the 2018-12-20 Wikipedia snapshot as D.\nQAMPARI (Rubin et al., 2022) is a factoid QA\ndataset constructed from Wikipedia, where the an-\nswer is a list of entities that are drawn from differ-\nent passages. Same as ASQA, we use the 2018-12-\n20 Wikipedia as the corpus.\nELI5 (Fan et al., 2019) is a long-form QA\ndataset built on the Reddit forum “Explain Like I’m\nFive”.6 Most ELI5 questions are how/why/what\nquestions that require long answers and multiple\npassages as evidence. Due to the diverse topics dis-\ncussed in the questions, we use Sphere (Piktus et al.,\n2021)—a filtered version of Common Crawl7—as\nthe corpus. The ELI5 dataset is widely used in\nrelated work due to its challenging nature (Nakano\net al., 2021; Menick et al., 2022; Liu et al., 2023).\nWe randomly select 1,000 examples from the\ndevelopment set of each dataset for ALCE. Our\nbenchmark primarily assesses the citation capabili-\nties of existing LLMs and does not provide training\ndata, as there are no available examples that pro-\nvide supervision for citations in these datasets.\n6https://www.reddit.com/r/explainlikeimfive/\n7https://commoncrawl.org. We also filter out any Web\npages from Reddit.\n3 Automatic Evaluation\nOur benchmark measures the following three di-\nmensions of system responses:\n• Fluency: whether the model’s generated text is\nfluent and coherent.\n• Correctness: whether the answer is accurate\nand covers all aspects of interest.\n• Citation quality: whether the answer is well\nsupported by the cited passages and no irrelevant\npassages are cited.\nIn the following, we present automatic metrics for\neach dimension and discuss why the combination\nof the three metrics provides a robust evaluation.\n3.1 Fluency\nWe use MAUVE (Pillutla et al., 2021) to evaluate\nthe fluency of the output (§C). We deploy MAUVE\nfor ASQA and ELI5 and omit it for QAMPARI, as\nQAMPARI only requires a list of short answers as\nthe response and LLMs consistently adhere to the\nformat in our experiments. As MAUVE is sensitive\nto output length and text style, and most LLMs are\ncapable of producing fluent text, we mainly employ\nit as a sanity check as long as the MAUVE scores\nare high enough.\n3.2 Correctness\nOur objective is to measure the informativeness and\nutility of the generation to the question. Liu et al.\n(2023) propose to directly evaluateperceived utility\nby humans, a process difficult to automate. There-\nfore, we use correctness—whether the response is\naccurate compared to a ground truth answer—as a\nproxy. Evaluating the correctness of long-form gen-\neration is a challenging task (Krishna et al., 2021),\nand we describe our strategy for each dataset be-\nlow. Figure 2 illustrates the metrics and we include\nadditional implementation details in §C.\nFor ASQA, we follow Stelmakh et al. (2022)\nand calculate the recall of correct short answers by\nchecking whether the short answers (provided by\nthe dataset) are exact substrings of the generation\n(exact match recall; EM recall).\nFor QAMPARI, we follow Rubin et al. (2022)\nand calculate the precision and recall of the model\nprediction, by checking the exact match to the gold\nanswer list. We add one additional adjustment:\nconsidering that users often want to know only a\nfew example answers of the question, our evalua-\ntion considers recall to be 100% if the prediction\nincludes at least 5 correct answers (recall-5).\n6467\nASQA \n   Exact Match Recall\nELI5 \n   Claim Recall\nWhen did the US break away from England?\nQuestion\nShort answers (from the dataset)\nJuly 2, 1776 September 3, 1783July 4, 1776\n… it declared independence on July 3, 1776 … The Treaty of \nParis was signed on September 3, 1783 …\nModel output\nHow do student loans aﬀect getting a mortgage?\nQuestion\nThey do not appear in credit history and do not aﬀect debt to \nincome ratio. Thus they do not aﬀect getting a mortgage.\nModel output\nClaims (generated by text-davinci-003 based on gold answers)\nStudent loan does not appear in credit history.\nStudent loan can aﬀect the debt to income ratio.\nDebt to income ratio aﬀects mortgage applications.\nString exact match\nRecall=33.3%\nNLI\nClaim recall=33.3%\nQAMPARI \n   Precision, Recall\nWhich films have Gong Li as a member of their cast?\nQuestion\nShort answers (from the dataset)\nThe Story of Qiu Ju Saturday FictionMulan\nSaturday Fiction, The Great Wall, Mulan, Shadow.\nModel output\nString exact match\nRecall=66.6%\nPrecision=50%\nFigure 2: Evaluation of correctness (details in §3.2).\nUnlike ASQA and QAMPARI, the ELI5 dataset\ndoes not provide short entity answers. Fan et al.\n(2019) use ROUGE for evaluation, which does\nnot reflect the correctness well (Krishna et al.,\n2021; §A). Inspired by works in summariza-\ntion evaluation (Zhang and Bansal, 2021; Kamoi\net al., 2023; Wang et al., 2020), we use Instruct-\nGPT (text-davinci-003; Ouyang et al., 2022)\nto generate three “sub-claims”. Then we use\nTRUE8 (Honovich et al., 2022), a T5-11B (Raf-\nfel et al., 2020) model fine-tuned on a collection of\nnatural language inference (NLI) datasets, to check\nwhether the model output entails the sub-claims\n(claim recall). TRUE targets factual correctness\nand has been used by previous works in similar\ncontext (Bohnet et al., 2022; Gao et al., 2023). We\ndemonstrate that claim recall provides a more ac-\ncurate measure of correctness than existing metrics\n(more details in §A).\n3.3 Citation Quality\nWe evaluate citation qualities using two metrics: (1)\ncitation recall, which determines if the output is en-\ntirely supported by cited passages, and (2) citation\nprecision, which identifies any irrelevant citations.\nAlthough we prioritize citation recall as it entails\na well-supported and truthful answer, enhancing\nprecision is crucial for better user satisfaction, re-\nducing the need for human review of extraneous\n8https://huggingface.co/google/t5_xxl_true_\nnli_mixture. Details in §C.\n[1]        \n[2]             \nFor this question, citation precision = 4 / 6 = 66%\n{statement 1} [1][2]. {statement 2} [3].{statement 3} [2][4][5].    \nModel output\n“entailment”Premise: passage [1][2]\nHypothesis: {statement 1}\nNLI\nmodel\nCitation Recall \n{statement 2} [3]\n{statement 1} [1][2]\n{statement 3} [2][4][5]    \nFor this question, citation recall = 2 / 3 = 66%\nCitation Precision \nDetect “irrelevant” citation: one citation alone does not support the claim, and \nremoving it does not affect other citations combined to support the claim.\nWhen did the US break away from England?\nQuestion\n{statement 1} [1][2]      : \n{statement 3} [2][4][5]      : \n                         [2]        [4][5]        \n[4]        [2][5]        \n[5]        [2][4]\n[2] is “irrelevant’’. Precision = 0\nPrecision = 1\n{statement 2} [3] If recall = 0, then precision = 0\nRecall = 1 if the concatenation of all cited passages fully supports the segment.\nWe use an NLI model to determine “fully support”.\nPrecision = 1\nPrecision = 1\nPrecision = 1\n[2]        \n[1]             \nFigure 3: Evaluation of citation quality (details in §3.3).\nWe use an NLI model to verify whether a statement is\nsupported by its citations.\npassages. Figure 3 provides an illustrated example.\nWe use the NLI model TRUE (Honovich et al.,\n2022) again to automatically examine whether the\ncited passages entail the model generation. We con-\nduct human evaluation (§6) to demonstrate strong\nhuman correlation of our metric.\nCitation recall. We calculate the citation recall of\neach statement (0 or 1) and average over all state-\nments in the model response. For each statementsi,\nits citation recall is 1 if and only if there is at least\none citation ( Ci ̸= ∅) and ϕ(concat(Ci),si) = 1,\nwhere ϕ(premise,hypothesis) is the NLI model\nthat outputs 1 if the premise entails the hypoth-\nesis, and 0 otherwise; concat(Ci) concatenates all\npassages in Ci together (details in §C). The NLI\nevaluation is in accordance with the attributable to\nidentified sources (AIS) framework (Rashkin et al.,\n2023): ϕ(concat(Ci),si) = 1implies that si is true\nbased solely on concat(Ci).\nCitation precision. Our citation precision evalua-\ntion detects citations that are irrelevant, but it does\nnot require citing a minimal set. We follow this de-\nsign because human writing often cites redundant\nsources to enhance credibility; human readers may\nalso appreciate multiple citations, especially when\nit pertains to critical claims such as medical advice.\nWe calculate the citation precision for each ci-\ntation (0 or 1) and average over all citations in the\n6468\nInstruction: Write an accurate, engaging, and\nconcise answer for ...\n<Retrieve for the question>\nDocument [1](Title: American Decolonization)\n...\nDocument [2](Title: Decolonization) ...\nDocument [3](Title: American Revolution) ...\n...\nQuestion: When did US break away from England?\nAnswer: The United States took the first step\ntowards gaining independence ... [1][2]. The\nTreaty of Paris was later signed ... [3].\nTable 2: An example of our VANILLA method. Differ-\nent colors represent prompt, model generation, and\n<actions>. We also provide two in-context demonstra-\ntions before the test example.\nresponse. We first define if a citation is “irrele-\nvant”. Intuitively, a citation ci,j is “irrelevant” if (a)\nci,j itself cannot support si and (b) removing ci,j\ndoes not affect the rest of the citations to support\nsi. Formally, ci,j is “irrelevant” if and only if\n(a) ϕ(ci,j,si) = 0, AND\n(b) ϕ(concat(Ci \\{ci,j}),si) = 1.\nci,j has a precision of 1 if si has recall =1 and\nci,j is not irrelevant. For example (Figure 3),\nwhen s3 cites three references [2][4][5] and\nrecall=1, [2] is “irrelevant” if ϕ([2],s3) = 0and\nϕ([4][5],s3) = 1. For condition (b) to work, we\nset recall =1 as a prerequisite for precision = 1.\nNote that this algorithm overlooks the scenario\nwhen one citation partially supports the statement.\nWe discuss the details in §E.\n3.4 ALCE is Robust to Shortcut Cases\nWe showcase how the ALCE evaluation is robust\nto two possible shortcuts in §D: (1) using the top-1\nretrieved passage as the response and citing itself,\nand (2) using the first two sentences of the top-1\npassage. Both cases have almost-perfect citation\nscores, but (1) has low fluency due to its unnaturally\nlong length compared to human answers, and (2)\nhas low correctness due to low coverage.\n4 Modeling\nIn this section, we discuss three major modeling\ncomponents for an ALCE system—retrieval, syn-\nthesis, and post-editing.\n4.1 Retrieval\nWe explore simple, off-the-shelf retrievers. We use\ndense retrievers for Wikipedia, including GTR (Ni\net al., 2022) and DPR (Karpukhin et al., 2020);\nwe use BM25 for Sphere. For each question, we\nretrieve the top-100 passages.\n4.2 Synthesis\nWe focus on how to prompt an LLM to interact\nwith the retriever, and synthesize and cite the ev-\nidence (without fine-tuning internal parameters).\nOne noteworthy challenge is that existing LLMs\nall have limited context window and thus can only\nfit a handful of passages.\nVANILLA . We simply provide the model with the\ntop-k9 passages and instruct the model to cite ac-\ncordingly (Table 2). We also use in-context learn-\ning (Brown et al., 2020) and prepend two demon-\nstrations. The complete instruction is in Table 23.\nSUMM /SNIPPET . With a 4K context window, we\ncan at most safely fit k= 5passages. As shown in\nFigure 4, top-5 retrieved passages can only cover\n56.8% percent of the answers in ASQA.\nTo tackle this limitation, we propose to provide\nsummaries or snippets of passages instead of the\nfull text (summaries are abstractive but snippets are\nspans from passages). We acquire summaries and\nsnippets by prompting ChatGPT with instructions\n(prompts in Table 25 and 26).10 Then we replace\nall passages with summaries/snippets. Summaries\nor snippets significantly reduce the passage length,\nallowing for more passages to fit in: for ASQA,\nthey reduce passage length by 6×on average.\nThough SUMM /SNIPPET allows for more re-\ntrieved passages, they are lossy compressions. To\nalleviate this problem, we propose INTERACT , an\ninteractive prompting scheme to allow the model to\ncheck the full text of certain passages. At each step,\nthe model can execute one of three actions: (1)\n“Check: Document [1][2]” to check the full text\nof the corresponding documents; (2) “Output:” to\noutput a statement of the answer; (3) “End.” to end\nthe generation. §C provides more details.\nINLINE SEARCH . The above methods all dis-\nplay retrieval results at the beginning. In INLI -\nNESEARCH , we allow LLMs to call “search” dur-\ning the generation process (Yao et al., 2023; Press\net al., 2022; Jiang et al., 2023). At each step, the\nmodel can execute one of three actions: “Search:\n9We can fit at most k = 3 for models with 2K window\nand at most k = 5for models with 4K context window.\n10We also query ChatGPT whether the passage is relevant\nto the question, and filter out passages that are “irrelevant”.\n6469\nInstruction: ...\n<Retrieve for question “...”>\nQuestion: When did US break away from England?\nSearch: Declaration of Independence\n<Search the query among the top-100 passages>\nDocument [1](Title: ...) ...\nOutput: The United States ... [1].\n<Remove Document [1] from context>\nSearch: Treaty of Paris\n<Search the query among the top-100 passages>\nDocument [3](Title: ...) ...\nOutput: The Treaty of Paris ... [3].\n<Remove Document [3] from context>\nEnd.\nTable 3: An example of INLINE SEARCH .\n{query}” to search among the top-100 passages11\nby using GTR; the “Output” and “End” actions are\nthe same as INTERACT . For each “Search” action,\nwe display the best retrieved passage in the context.\nThe passage is removed after one action to save\ncontext space. Table 3 shows an example.\nCLOSED BOOK . We also add a simple closed-\nbook baseline, where the model is only prompted\nwith the instruction and the question, without any\nretrieved passages provided. Consequently, this\nvariant does not cite any evidences.\n4.3 Post-editing\nIn this section we discuss two strategies for refining\nthe output to further improve its quality.\nRERANK . We randomly sample nsample = 4 re-\nsponses for each question, and select the best re-\nsponse using the automatic citation recall score.\nwe expect RERANK to improve the citation quality.\nPOST CITE . For each statement, we find the best\nmatching passage among the top-100 retrieved pas-\nsages using GTR and cite it. We combine this with\nCLOSED BOOK in our experiments.\n5 Experiments\nWe describe experiment details in §C. We use Chat-\nGPT (gpt-3.5-turbo-0301) with a 4K context\nwindow for most main experiments and ablations.\nWe also report results with ChatGPT-16K ( gpt-\n3.5-turbo-16k-0613) and GPT-4 (gpt-4-0613;\n8K context window). For open-source models,\nwe test LLaMA (Touvron et al., 2023a) and its\ninstruction-tuned versions, including Alpaca (Taori\net al., 2023), Vicuna (Chiang et al., 2023), and\n11We do not search over the entire corpus because{query}\nmay leave out certain context in the question and searching\namong the already-retrieved passages gives better results.\nFluency Correct. Citation\n(MAUVE) (EM Rec.) Rec. Prec.\nChatGPT\nVANILLA(5-psg) 66.6 40.4 73.6 72.5\nw/ RERANK 77.0 40.2 84.8 81.6\nSUMM(10-psg) 70.0 43.3 68.9 61.8\nw/ INTERACT 69.0 39.1 73.4 66.5\nSNIPPET(10-psg) 69.8 41.4 65.3 57.4\nINLINESEARCH 58.7 32.4 58.3 58.2\nCLOSEDBOOK 52.7 38.3 26.7 26.7\nGPT-4(VANILLAprompting)\nGPT-4 (5-psg) 67.1 41.3 68.5 75.6\nGPT-4 (20-psg) 64.9 44.4 73.0 76.5\nLLaMA(VANILLAprompting)\nLLaMA-13B (3-psg) 68.4 26.9 10.6 15.4\nVicuna-13B (3-psg) 82.6 31.9 51.1 50.1\nChat-13B (5-psg) 72.4 35.2 38.4 39.4\nChat-70B (5-psg) 88.3 41.5 62.9 61.3\nTable 4: Experiments on ASQA. For CLOSED BOOK ,\nwe use POST CITE to get citations. k-psg: putting top-\nk passages from the retrieval results into the context.\nChat-13B and Chat-70B refer to LLaMA-2-Chat.\nOasst (Köpf et al., 2023). They all have a 2K con-\ntext window. We use short instructions for LLaMA\n(Table 24) to save context budget. Additionally,\nwe test LLaMA-2-Chat, which were also trained to\nfollow instructions (Touvron et al., 2023b). These\nmodels have a context window of 4K tokens, which\nallows for 5 passages per question.\n5.1 Main Results\nWe present the main results on three datasets in\nTable 4, 5, and 6 respectively (full results in §G.6).\nWe first note that all models achieve good fluency\nscores (except some models on ELI5 mainly due to\ntheir longer generations). We summarize the main\ntakeaways from the experiments below.\nVANILLA achieves strong performance. Despite\nits simplicity, VANILLA (putting retrieved passages\nin context) achieves close-to-the-best performance\namong all prompting strategies.\nUsing summaries or snippets improves correct-\nness. We see a universal trend that SUMM or SNIP -\nPET improves correctness, though on ASQA and\nELI5, such an improvement comes at a cost of\ncitation quality due to the lossy compression. Com-\nbining INTERACT with SUMM /SNIPPET does not\nbring improvement, and we hypothesize that check-\ning the full passages offers limited benefit and cur-\nrent LLMs are not proficient in an interactive usage.\nRetrieving text on the fly does not improve per-\nformance. All datasets show that VANILLA out-\nperforms INLINE SEARCH on citation quality (and\n6470\nCorrectness Citation\nRec.-5 Prec. Rec. Prec.\nChatGPT\nVANILLA (5-psg) 20.8 20.8 20.5 20.9\nw/ RERANK 22.8 21.4 21.2 21.4\nSUMM (10-psg) 23.6 21.2 23.6 25.7\nSNIPPET (10-psg) 24.5 21.5 22.9 24.9\nw/ INTERACT 21.9 23.0 21.9 23.4\nINLINE SEARCH 17.2 20.4 14.9 14.9\nCLOSED BOOK 32.9 19.8 10.0 10.0\nGPT-4 (VANILLA prompting)\nGPT-4 (5-psg) 22.2 25.0 25.9 27.0\nGPT-4 (20-psg) 29.6 26.2 27.4 28.5\nLLaMA (VANILLA prompting)\nLLaMA-13B (3-psg) 9.7 9.1 6.7 7.1\nVicuna-13B (5-psg) 14.0 15.9 12.5 13.4\nChat-13B (5-psg) 21.1 18.2 9.6 9.7\nChat-70B (5-psg) 21.8 18.4 15.1 15.6\nTable 5: Experiments on QAMPARI. “Rec.-5”: we set\nthe recall to be 100% if the prediction includes at least\n5 correct answers.\non correctness for ASQA and ELI5). By manually\nexamining the examples, we find that it is challeng-\ning to ask detailed questions without seeing any\npassages. To improve INLINE SEARCH , one may\nneed to provide more context about the questions\nin advance or encourage the model to call retrievers\nwith more detailed and diverse queries.\nRERANK boosts citation quality. We observe that\nRERANK leads to consistent improvement in cita-\ntion quality (on ASQA and ELI5). As the automatic\nscores may be biased in RERANK , we also conduct\nhuman evaluation (§6) and verify its effectiveness.\nCLOSED BOOK +POST CITE delivers strong cor-\nrectness but poor citation quality. CLOSED -\nBOOK outperforms VANILLA in correctness on\nELI5 and QAMPARI, and has only a 2% gap on\nASQA. However, CLOSED BOOK cannot provide\nany citation; when combined with POST CITE , the\ncitation quality remains inadequate. For instance,\ncitation recall of CLOSED BOOK +POST CITE is\nlower than VANILLA by 47% on ASQA.\nTo understand why CLOSED BOOK achieves bet-\nter correctness and why POST CITE cannot deliver\nsatisfying citation quality, we manually examine\nmodel outputs and find that: (1) open-book mod-\nels are easily distracted by irrelevant passages and\ngenerate responses with lower correctness, a phe-\nnomenon also observed by Shi et al. (2023); (2)\nCLOSED BOOK often generates texts that are cor-\nrect but not similar to any retrieved passages, mak-\ning it difficult to match a citation post-hoc.\nFluency Correct. Citation\n(MAUVE) (Claim) Rec. Prec.\nChatGPT\nVANILLA(5-psg) 57.2 12.0 51.1 50.0\nw/ RERANK 56.1 11.4 69.3 67.8\nSUMM(10-psg) 40.3 12.5 51.5 48.2\nSNIPPET(10-psg) 62.9 14.3 50.4 45.0\nw/ INTERACT 68.0 13.3 47.8 45.0\nINLINESEARCH 49.7 13.4 45.6 43.7\nCLOSEDBOOK 32.6 18.6 15.5 15.5\nGPT-4(VANILLAprompting)\nGPT-4 (5-psg) 38.4 14.2 44.0 50.1\nGPT-4 (20-psg) 41.5 18.3 48.5 53.4\nLLaMA(VANILLAprompting)\nLLaMA-13B (3-psg) 50.0 3.9 3.1 5.3\nVicuna-13B (3-psg) 58.2 10.0 15.6 19.6\nChat-13B (5-psg) 34.7 13.4 17.3 15.8\nChat-70B (5-psg) 38.6 12.8 38.3 37.9\nTable 6: Experiments on ELI5. We use claim recall\nfor the correctness evaluation. Chat-13B and Chat-70B\nrefer to LLaMA-2-Chat.\nGPT-4 brings limited improvement but is better\nat using long context. We evaluate GPT-4 with\nVANILLA and different numbers of passages (more\nresults in §G.6). GPT-4 brings consistent (but lim-\nited) improvement on correctness, but often at a\ncost of citation quality. GPT-4 can also incorporate\nmore passages due to its longer context window,\nwhich boosts both correctness and citation qual-\nity. On the contrary, including more passages with\nChatGPT-16K does not improve the results (Ta-\nble 7), suggesting that processing more passages\nis non-trivial and GPT-4 is better at synthesizing\ninformation from its long context than ChatGPT.\n5.2 Comparison of Different LLMs\nTable 7 compares different LLMs on ASQA\nusing VANILLA (more results in §G.6). No-\ntably, instruction-tuned models (Vicuna-13B and\nLLaMA-2-Chat) outperform the original LLaMA\nmodels in correctness and considerably enhance\nthe citation quality. We observe that while the orig-\ninal LLaMA models are able to copy facts from\nthe context, they struggle with accurately citing the\nsources or simply do not cite. Notably, the best\nopen-source model, LLaMA-2-70B-Chat, achieves\ncomparable correctness score as the OpenAI mod-\nels, but still lags behind in citation quality.\n5.3 Retrieval Analysis\nThe retrieval results play a crucial role to the cor-\nrectness and the citation quality. Figure 4 presents\nthe retrieval recall@kwith different datasets and\n6471\n5 20 50 100\nk\n30\n40\n50\n60\n70\n80Recall@k\n Vanilla (5-psg)\nOracle\nASQA\nDPR\nGTR\n5 20 50 100\nk\n0\n20\n40\n60\nVanilla (5-psg)\nOracle\nQAMPARI\nDPR\nGTR\n5 20 50 100\nk\n10\n20\n30\nVanilla (5-psg)\nOracle\nELI5\nBM25\nFluency Correct. Citation\n(MAUVE) (EM) Rec. Prec.\nChatGPT(VANILLA)\nDPR (5-psg) 61.8 36.1 65.0 65.6GTR (1-psg) 69.5 38.4 56.0 64.0GTR (3-psg) 66.6 39.6 72.8 73.9GTR (5-psg) 66.6 40.4 73.6 72.5\nFigure 4: Retrieval recall@kon ASQA (EM recall), QAMPARI (recall-5), and ELI5 (claim recall). Retrieval recall\nserves as an upper bound for model performance, and we compare them with two models’ correctness results in the\nfigure (dashed lines): “Vanilla (5-psg)” is ChatGPT VANILLA with top-5 passages in context; “Oracle” is the same\nmodel except that it uses 5 gold passages (§G.1), whose recall matches Recall@100 on all three datasets.\nFluency Correct. Citation\n(MAUVE) (EM Rec.) Rec. Prec.\nOpen-source(max #tokens=2K-4K)\nLLaMA-13B (3-psg) 68.4 26.9 10.6 15.4\nVicuna-13B (3-psg) 82.6 31.9 51.1 50.1\nChat-13B (5-psg) 72.4 35.2 38.4 39.4\nChat-70B (5-psg) 88.3 41.5 62.9 61.3\nChatGPT(max #tokens=4K)\nChatGPT (3-psg) 66.6 39.6 72.8 73.9\nChatGPT (5-psg) 66.6 40.4 73.6 72.5\nChatGPT-16K(max #tokens=16K)\nChatGPT (5-psg) 60.3 36.1 76.2 76.5\nChatGPT (10-psg) 56.3 36.7 75.3 75.0\nChatGPT (20-psg) 56.7 36.1 73.7 73.5\nGPT-4(max #tokens=8K)\nGPT-4 (5-psg) 67.1 41.3 68.5 75.6\nGPT-4 (10-psg) 71.5 43.1 72.0 75.5\nGPT-4 (20-psg) 64.9 44.4 73.0 76.5\nTable 7: Comparison of different LLMs on ASQA\n(GTR+VANILLA ). LLaMA-13B and Vicuna-13B have\na context limit of 2,048 tokens, and thus can only use a\nshort version of instructions and at most top-3 passages.\nChat-13B and Chat-70B refer to LLaMA-2-Chat.\nretrievers. As the number of passages increases, re-\ntrieval recall steadily improves. Additionally, Fig-\nure 4 shows the correctness performance of two\nmodels: (1) ChatGPT VANILLA with top-5 pas-\nsages (our primary baseline); (2) an oracle ver-\nsion of the same model employing 5 gold passages\n(§G.1; the 5 gold passages match the retrieval re-\ncall@100). Notably, both models’ correctness lags\nbehind the corresponding retrieval recall (except\nfor ELI5 top-5). The discrepancy suggests that de-\nspite the presence of accurate answers in context,\nLLMs struggle to utilize them in their outputs.\nWe compare the impact of different retrievers\nand different numbers of passages to LLMs. Fig-\nure 4 (right) shows that GTR outperforms DPR\nin both correctness and citation quality, emphasiz-\ning the importance of deploying better retrievers.\nContrary to the retrieval recall trend in Figure 4,\nmore passages in context do not yield substantial\nimprovement for ChatGPT. Specifically, correct-\nness plateaus at top-1 passage and citation qual-\nity plateaus at top-3. GPT-4 (Table 7) exhibits\nan increasing trend with more passages, but the\nimprovement is not proportional to the retrieval\nperformance. This indicates the limited ability of\nLLMs in utilizing multiple passages within context.\n5.4 Other Ablations\nWe provide additional ablations in §G. In summary,\nwe find that (1) using comprehensive instructions\nenhances the citation quality of instruction-tuned\nmodels (§G.2); (2) including at least one demon-\nstration improves the performance (§G.3); (3) fine-\ntuned models (FiD; Izacard and Grave, 2021) with\nPOST CITE lag behind LLMs in both correctness\nand citation quality and fail to generalize (§G.4).\n6 Human Evaluation\nTo verify that our automatic evaluation correlates\nwith human judgement, we conduct human eval-\nuation on selected models and request workers to\njudge model generations on three dimensions simi-\nlar to Liu et al. (2023)—(1) utility: a 1-to-5 score\nindicating whether the generation helps answer the\nquestion; (2) citation recall: the annotator is given\na sentence and all passages that the sentence cited,\nand is asked to judge whether the passages fully\nsupport the sentence; (3) citation precision: given\na sentence and one of its citations, the annotator\nis asked to judge whether the citation “fully sup-\nports”, “partially supports”, or “does not support”\nthe sentence. Each citation gets a precision score\n1 if the output sentence has a citation recall of 1\nand this citation at least “partially supports” it. See\nAppendix F for more details.\nModel outputs score high utility. The utility\nscores do not differ significantly between models,\nranging 3.7-3.9 for ASQA and 3.5-3.6 for ELI5.\nUpon inspection, all tested models are mostly able\n6472\nHuman scores ALCE scores\nRec. Prec. Rec. Prec.\nChatGPT VANILLA 74.7 76.6 75.3 74.4\nw/ RERANK 79.3 81.9 83.9 80.8\nVicuna-13B VANILLA 51.6 51.5 50.3 50.1\nTable 8: Human citation quality evaluation vs. ALCE\ncitation quality evaluation on ASQA.\nHuman scores ALCE scores\nRec. Prec. Rec. Prec.\nChatGPT VANILLA 50.8 52.4 52.8 50.4\nw/ RERANK 59.7 60.6 63.0 60.6\nVicuna-13B VANILLA 13.4 19.2 13.6 18.1\nTable 9: Human citation quality evaluation vs. ALCE\ncitation quality evaluation on ELI5.\nto output fluent answers that are related to the ques-\ntion, despite differences in factual correctness.\nOur automatic evaluation of citation quality\nstrongly correlates with human judgements. As\nshown in Table 8 (ASQA) and Table 9 (ELI5), the\nrelative rankings induced by human and our auto-\nmatic metrics are consistent. The absolute citation\nscores from human and ALCE are very close except\nfor RERANK (which uses the automated citation\nrecall for reranking). This suggests that an im-\nprovement on ALCE citation metrics translates to\nimprovement on human preferences. Furthermore,\nthe Cohen’s kappa coefficient between human and\nALCE suggests substantial agreement for citation\nrecall (0.698) and moderate agreement for citation\nprecision (0.525). We also show in §G.5 that our\nautomatic evaluation achieves high accuracy when\ntreating human annotations as gold labels (85.1%\nfor citation recall and 77.6% for citation precision).\n7 Related Work\nEvaluating citations. Generating text with ci-\ntations is closely related to attribution. Rashkin\net al. (2023) define the “attributable to identified\nsources” (AIS) score to measure how faithful a gen-\nerated text is to its sources. Bohnet et al. (2022) ap-\nply AIS scores on a single-document short-answer\nQA dataset. Honovich et al. (2022); Yue et al.\n(2023) study automatic evaluations for the AIS\nscore. A concurrent work (Liu et al., 2023) conduct\nhuman evaluation on commercial generative search\nengines to examine their citation qualities.\nScientific citation text generation (Funkquist\net al., 2022) is a related task to ALCE where the\nmodel is provided the papers-to-cite and context\nand is required to recover the citing text. It is differ-\nent from ALCE as all citations are provided and the\nmodel only needs to perform the summarization.\nRetrieval-augmented LMs. Many studies have ex-\nplored augmenting LMs with externally retrieved\ninformation. Guu et al. (2020); Borgeaud et al.\n(2022); Izacard et al. (2022) pre-train language\nmodels with retrieved passages, while Khandel-\nwal et al. (2020); Zhong et al. (2022) augment\nLLMs’ output by interpolating it with a kNN mod-\nule; though none of them explicitly provide cita-\ntions to the retrieved sources. Other works prompt\nor fine-tune LLMs to “retrieve on-the-fly” (Parisi\net al., 2022; Schick et al., 2023; Shuster et al., 2022;\nJiang et al., 2023; Yao et al., 2023; Press et al.,\n2022), which offers flexibility of when and what to\nsearch. Gao et al. (2023); He et al. (2022) propose\nto first generate text without accessing external doc-\numents and then retrieve relevant documents and\nrevise the generation to be consistent.\nAmong previous explorations, Nakano et al.\n(2021); Menick et al. (2022) are the closest to our\nsetting, where LLMs are trained to answer ques-\ntions while providing citations. However, they do\nnot explore retrieval strategies and simply use com-\nmercial search engines, which are not reproducible,\nand their models and training data are closed-\nsource. To the best of our knowledge, we are the\nfirst to implement end-to-end systems that retrieve,\nsynthesize, and cite documents with LLMs.\n8 Conclusion\nWe propose ALCE, the first automatic benchmark\nfor evaluating LLM generations with citations. We\ndeploy automatic metrics to measure fluency, cor-\nrectness, and citation quality, and verify their effi-\ncacy via human evaluation. We explore a variety of\nstrategies for incorporating citations in LLMs and\ndemonstrate that current systems have considerable\nroom for improvement on ALCE.\nOur experiments highlight a number of promis-\ning research directions, including (1) enhancing re-\ntrieval and refining retrieval integrations in LLMs,\n(2) developing long-context LLMs, and (3) advanc-\ning LLMs’ ability to synthesize multiple sources.\nWhat’s even more intriguing is that these research\nproposals extend beyond the ALCE setup (for ex-\nample, long-context LLMs have numerous exciting\napplications), and ALCE can serve as a valuable\ntestbed for their development.\n6473\nLimitations\nOur evaluation still has room for improvement: (1)\nMAUVE is found to be sensitive to output length\nand may provide unstable results; (2) for the ELI5’s\ncorrectness evaluation, the automatically generated\nclaims may not cover all possible answers due to\nthe open-ended nature of the questions; (3) our ci-\ntation quality evaluation is limited by the accuracy\nof the NLI model; for citation precision, the NLI\nmodel cannot detect the case of “partially support”\nand thus leads to a lower citation precision score\nthan the human evaluation.\nAlthough we believe our curated datasets closely\nresemble the distribution of real-world user ques-\ntions, we acknowledge that they do not cover more\nchallenging scenarios, such as multi-hop reasoning,\nmath reasoning, and code completion.\nIn our experiments, we focus on prompting\nLLMs without updating their model weights. Train-\ning a model directly to incorporate citations re-\nmains challenging due to the lack of supervised\ndata. However, we observe that certain human-\ninstruction datasets contain examples similar to our\ntask setup. We leave the exploration of training\nLLMs to generate citations for future work.\nAcknowledgments\nWe appreciate the helpful feedback from the mem-\nbers of the Princeton NLP group. We thank Alexan-\nder Wettig, Nelson Liu, Tianyi Zhang, Yu Meng,\nSadhika Malladi, Yangsibo Huang, Zhiyuan Zeng,\nand Dan Friedman for the valuable discussion. We\nthank Surge AI (especially Anna Folinsky and Ed-\nwin Chen) for their support with the human eval-\nuation. Tianyu Gao is supported by an IBM PhD\nFellowship. This research is supported by an NSF\nCAREER award (IIS-2239290), a Sloan Research\nFellowship, and Microsoft Azure credits through\nthe “Accelerate Foundation Models Academic Re-\nsearch” Initiative.\nReferences\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni,\nDaniel Andor, Livio Baldini Soares, Jacob Eisenstein,\nKuzman Ganchev, Jonathan Herzig, Kai Hui, et al.\n2022. Attributed question answering: Evaluation and\nmodeling for attributed large language models. arXiv\npreprint arXiv:2212.08037.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. InInter-\nnational Conference on Machine Learning (ICML),\nvolume 162, pages 2206–2240.\nSamuel Bowman, Gabor Angeli, Christopher Potts, and\nChristopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. In\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Process-\ning Systems (NeurIPS).\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. ELI5:\nLong form question answering. In Association for\nComputational Linguistics (ACL), pages 3558–3567.\nMartin Funkquist, Ilia Kuznetsov, Yufang Hou, and\nIryna Gurevych. 2022. CiteBench: A benchmark for\nScientific Citation Text Generation. arXiv preprint\narXiv:2212.09577.\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-\ncent Y Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan,\net al. 2023. RARR: Researching and revising what\nlanguage models say, using language models. In\nAssociation for Computational Linguistics (ACL).\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. REALM: Retrieval-\naugmented language model pre-training. In Interna-\ntional Conference on Machine Learning (ICML).\nHangfeng He, Hongming Zhang, and Dan Roth. 2022.\nRethinking with retrieval: Faithful large language\nmodel inference. arXiv preprint arXiv:2301.00303.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations (ICLR).\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and\nYossi Matias. 2022. TRUE: Re-evaluating factual\n6474\nconsistency evaluation. In North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies (NAACL-HLT), pages\n3905–3920.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Atlas: Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1–38.\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing\nSun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. 2023. Ac-\ntive retrieval augmented generation. arXiv preprint\narXiv:2305.06983.\nRyo Kamoi, Tanya Goyal, Juan Diego Rodriguez,\nand Greg Durrett. 2023. WiCE: Real-World En-\ntailment for Claims in Wikipedia. arXiv preprint\narXiv:2303.01432.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations (ICLR).\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018.\nScitail: A textual entailment dataset from science\nquestion answering. In Conference on Artificial In-\ntelligence (AAAI), volume 32.\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\nHurdles to progress in long-form question answering.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4940–4957, Online. Association for Computa-\ntional Linguistics.\nAndreas Köpf, Yannic Kilcher, Dimitri von Rütte,\nSotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\nAbdullah Barhoum, Nguyen Minh Duc, Oliver\nStanley, Richárd Nagyfi, Shahul ES, Sameer Suri,\nDavid Glushkov, Arnav Dantuluri, Andrew Maguire,\nChristoph Schuhmann, Huu Nguyen, and Alexander\nMattick. 2023. Openassistant conversations – de-\nmocratizing large language model alignment. arXiv\npreprint arXiv:2304.07327.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Association for Com-\nputational Linguistics (ACL), pages 6086–6096.\nNelson F Liu, Tianyi Zhang, and Percy Liang. 2023.\nEvaluating verifiability in generative search engines.\narXiv preprint arXiv:2304.09848.\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-\nGillingham, Geoffrey Irving, et al. 2022. Teaching\nlanguage models to support answers with verified\nquotes. arXiv preprint arXiv:2203.11147.\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. AmbigQA: Answering am-\nbiguous open-domain questions. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 5783–\n5797, Online. Association for Computational Lin-\nguistics.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. WebGPT: Browser-assisted question-\nanswering with human feedback. arXiv preprint\narXiv:2112.09332.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo\nHernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,\nKeith Hall, Ming-Wei Chang, and Yinfei Yang. 2022.\nLarge dual encoders are generalizable retrievers. In\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 9844–9855.\nOpenAI. 2023. GPT-4 Technical Report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural In-\nformation Processing Systems (NeurIPS), 35:27730–\n27744.\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. TALM:\nTool augmented language models. arXiv preprint\narXiv:2205.12255.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\n6475\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nAleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nDmytro Okhonko, Samuel Broscheit, Gautier Izac-\nard, Patrick Lewis, Barlas O ˘guz, Edouard Grave,\nWen-tau Yih, et al. 2021. The Web Is Your Oyster–\nKnowledge-Intensive NLP against a Very Large Web\nCorpus. arXiv preprint arXiv:2112.09924.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaid\nHarchaoui. 2021. MAUVE: Measuring the gap be-\ntween neural text and human text using divergence\nfrontiers. In Advances in Neural Information Pro-\ncessing Systems.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv preprint arXiv:2210.03350.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text Trans-\nformer. The Journal of Machine Learning Research\n(JMLR), 21(140).\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions\nfor machine comprehension of text. In Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2383–2392.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\nLora Aroyo, Michael Collins, Dipanjan Das, Slav\nPetrov, Gaurav Singh Tomar, Iulia Turc, and David\nReitter. 2023. Measuring Attribution in Natural Lan-\nguage Generation Models. Computational Linguis-\ntics, pages 1–64.\nSamuel Joseph Amouyal Ohad Rubin, Ori Yoran, Tomer\nWolfson, Jonathan Herzig, and Jonathan Berant.\n2022. QAMPARI: An Open-domain Question An-\nswering Benchmark for Questions with Many An-\nswers from Multiple Paragraphs. arXiv preprint\narXiv:2205.12665.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021.\nGet your vitamin C! robust fact verification with con-\ntrastive evidence. In North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (NAACL-HLT), pages 624–\n643.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed Chi, Nathanael Schärli,\nand Denny Zhou. 2023. Large language models can\nbe easily distracted by irrelevant context. In Interna-\ntional Conference on Machine Learning (ICML).\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs,\nStephen Roller, Arthur Szlam, and Jason Weston.\n2022. Language models that seek for knowledge:\nModular search & generation for dialogue and\nprompt completion. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n373–393.\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\nWei Chang. 2022. ASQA: Factoid questions meet\nlong-form answers. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 8273–8288, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford Alpaca:\nAn Instruction-following LLaMA model.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies (NAACL-HLT) ,\npages 809–819.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a. LLaMA: Open and Effi-\ncient Foundation Language Models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models.\n6476\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\nAsking and answering questions to evaluate the fac-\ntual consistency of summaries. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5008–5020, Online. Asso-\nciation for Computational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL-HLT).\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao. 2023.\nReact: Synergizing reasoning and acting in language\nmodels. In International Conference on Learning\nRepresentations (ICLR).\nXiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su,\nand Huan Sun. 2023. Automatic evaluation of at-\ntribution by large language models. arXiv preprint\narXiv:2305.06311.\nShiyue Zhang and Mohit Bansal. 2021. Finding a bal-\nanced degree of automation for summary evaluation.\nIn Empirical Methods in Natural Language Process-\ning (EMNLP), pages 6617–6632.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019.\nPAWS: Paraphrase adversaries from word scrambling.\nIn North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies (NAACL-HLT), pages 1298–1308.\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022. Train-\ning language models with memory augmentation. In\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 5657–5673.\n6477\nA Generating Claims for ELI5\nWe elect not to use ROUGE-L as our main cor-\nrectness metrics since it does not account for the\ndifferent ways of expressing the same answer and it\ncan be easily gamed (Krishna et al., 2021). We fur-\nther illustrate this issue in Table 10. A system can\neasily achieve high ROUGE-L score by retrieving\nand returning the top passage from a BM25 index.\nHowever, the claims evaluation metric does not\nreward this approach since the output often lacks\ndifferent aspects of the answers.\nROUGE-L Claim recall\nChatGPT VANILLA 20.6 12.0\nChatGPT ORACLE 21.2 21.3\nLLaMa-13B VANILLA 16.2 3.9\nTop-1 passage 19.1 3.0\nTable 10: Comparison between ROUGE-L and claim\nrecall scores on ELI5.\nInstead, we leverage the original answers to gen-\nerate sub-claims and use them to serve as an esti-\nmate of the different aspects of the answers that\nwe expect the model to cover. This approach is\ninspired by works in summarization evaluation and\nclaim verification (Zhang and Bansal, 2021; Kamoi\net al., 2023; Wang et al., 2020).\nSpecifically, we use text-davinci-003 to gen-\nerate the sub-claims. We first manually annotate\nthree question and answer pairs from the original\nELI5 training set with 3 sub-claims each. Then,\nwe prompt text-davinci-003with these pairs as\ndemonstrations. The full prompt with an example\nis shown in Table 22.\nInstructGPT generates coherent and faithful\nsub-claims. To ensure that the generated sub-\nclaims are of good quality, we manually inspect\na random sample of 40 answers and their generated\nsub-claims (totaling to 120 sub-claims). For each\nsub-claim, we assign a score of 1 if it is relevant to\nthe question and faithful to the facts presented in\nthe ground truth, and 0 otherwise. We found that\n112 out of the 120 (93.33%) sub-claims received a\nscore of 1, meaning that our generated sub-claims\nare of high quality and faithful to the ground truth.\nFurthermore, the average number of words in the\ngenerated sub-claims is 14 words, and they are\ntypically just one sentence long. This is aligned\nwith the intent behind the metric—to capture short\nfactual claims made by the original answer.\nNLI model accurately predicts the entailment\nof sub-claims. We further analyze our sub-claim\nevaluation metrics by checking the error rate of\nthe final prediction of the NLI model. To this end,\nwe first manually annotate the entailment scores\nbetween 40 outputs and their sub-claims (in total\nof 120 pairs; these are the same questions from the\nprevious analysis). We then use the NLI model\nto obtain the entailment scores for the output and\nsub-claims. Using the human annotations as the\nground truth label, we found that the NLI model\nachieved an accuracy of 80.0%.\nB Dataset Statistics\nFor ASQA, human answers have an average length\nof 65 words. For QAMPARI, each question has\non average 13 answers. For ELI5, human answers\nhave an average length of 131 words.\nC Implementation Details\nNLI model. We use the version of TRUE\nmodel from https://huggingface.co/google/\nt5_xxl_true_nli_mixture, which is trained on\nSNLI (Bowman et al., 2015), MNLI (Williams\net al., 2018), Fever (Thorne et al., 2018), Sci-\ntail (Khot et al., 2018), PAWS (Zhang et al., 2019),\nand VitaminC (Schuster et al., 2021). This model\nuses the following prompt: “premise: {PREMISE}\nhypothesis: {}” and outputs “1” if the premise\nentails the hypothesis. We format each passage\n(when used as premise) by the format of “Title:\n{TITLE}\\n{TEXT}” and concatenate all passages\nwith “\\n” as a separator.\nMAUVE. When running MAUVE, we concatenate\nthe question and the model output (or human an-\nswer) by space. We truncate both the references\nand the model generations to 100 words, as we\nfound MAUVE results are unstable beyond this\nlength for ELI5 (this is due to that ELI5 has a lot\nof extremely long human answers).\nExact match for ASQA and QAMPARI. Both\nASQA and QAMPARI provide aliases for their\nshort answers. We normalize the response and the\nshort answers similarly to Rajpurkar et al. (2016)\nand report the score with the best-matching aliases.\nFor ASQA, Stelmakh et al. (2022) also propose a\nQA-based evaluation which we found to be not as\nstable, and thus we do not report it in our paper.\nOutput truncation. Before evaluation, we trun-\n6478\ncate model output by new lines, as non-instruction-\ntuned models may generate more content after new\nlines that are irrelevant.\nINTERACT . Empirically, we found that models\ntend to execute too many consecutive “check” ac-\ntions, so we force the model to always “ output”\nafter each “check”. We limit the maximum number\nof passages to check as 3 to avoid exceeding the\nlength limit. The full passages are removed from\nthe context after one action to save context space.\nTable 27 provides an example for INTERACT .\nMain experiments. For all experiments except\nChatGPT RERANK , we run each model three times\nwith different seeds and each time we sample two\ndemonstrations from a pool of four. We report the\naveraged scores for all experiments in the main\npaper and we report the standard deviations in Ap-\npendix G.6.\nDecoding methods. Based on preliminary experi-\nments we choose the following decoding methods:\nFor ChatGPT and GPT-4, we use sampling with\ntemperature 0.5; for all open-source models, we\nuse Nucleus sampling (Holtzman et al., 2020) and\nset top_p = 0.95.\nD ALCE Catches Shortcut Cases\nFluency Correct. Citation\n(MAUVE) (EM Rec.) Rec. Prec.\nChatGPT 66.6 40.4 73.6 63.0\nTop-1 passage 20.8 35.1 99.4 99.4\nFirst 2 sents 67.2 18.9 98.7 98.7\nTable 11: ASQA cheating cases. “ChatGPT”: the Chat-\nGPT VANILLA model with GTR-retrieved top-5 pas-\nsages. “Top-1 passage”: use the top-1 retrieved passage\nas the response. “First 2 sents”: use the first 2 sentences\nof the top-1 retrieved passage.\nTable 11 demonstrates the experiments to show\nthat ALCE is robust to shortcut cases. Using the\ntop-1 passages or first two sentences of the top-1\npassages induces almost perfect citation quality,\nbut fluency and correctness are dramatically lower.\nE Citation Recall Discussion\nOur citation precision evaluation cannot detect a\ncitation that partially supports the statement and\nhence will falsely penalize it. Consider a state-\nment s3 and its citations [2][4][5]: if [2]entails\npartial information of s3 that [4][5] also entails,\n[2]will be counted as “irrelevant” while it should\nnot be penalized. Liu et al. (2023) conduct hu-\nman evaluation on citation precision in a different\nway: For each citation, they ask annotators to judge\nwhether the citation (1) fully support, (2) partially\nsupport, or (3) does not supportsi. One citation ci,j\nis precise if (a) ci,j fully supports si or (b) Ci fully\nsupports si, ci,j partially supports si, and no c∈Ci\nalone fully supports si. This evaluation solved the\ncorner case we mentioned in the main paper (one\ncitation partially supports the claim but is identi-\nfied as “irrelevant”). However, it is challenging to\nconduct such evaluation automatically, as there is\nno existing model that can judge whether a cita-\ntion “partially” supports a claim. We also explore\nprompting ChatGPT to conduct such a task, which\nyields poor results. We defer it to future work to\ncollect supervised data to train a better ϕthat can\ndetect “partial support”.\nF Human Evaluation\nWe employ Surge AI ( https://www.surgehq.\nai/) for our human evaluation. The average pay to\nworkers is 20 USD per hour. We randomly sample\n100 examples from ASQA and ELI5 and annotate\noutputs of selected models: ChatGPT VANILLA ,\nChatGPT RERANK , and Vicuna-13B VANILLA .\nF.1 Utility\nTo check if the model output is useful to down-\nstream users, we measure the utility of the response\nS. We first show the query qand model response\nSto the worker and ask them to rate their agree-\nment with the statement \"The response is a helpful\nand informative answer to the query\" on a Likert\nscale of 1-5, corresponding to Strongly Disagree,\nDisagree, Neutral, Agree, and Strongly Agree.\nF.2 Citation Recall\nThe annotators are shown the question q, the state-\nment si, and all of its citations Ci, and they rate if\nthe joint set of citations fully support the statement\n(recall=1) or if they do not support all the claims\n(recall=0). We calculate the overall recall score\nfor the generation by taking an average of all the\nstatements’ recall scores.\nF.3 Citation Precision\nWe show the question qand a pair of a statement\nsi and one of its citation ci,j ∈Ci to the annotator.\nWe ask the annotator if the citation fully supports,\n6479\nR@1 R@3 R@5 R@20 R@100\nDPR 29.6 44.5 51.5 64.6 74.1\nGTR 35.1 50.7 56.8 70.3 78.4\nOracle 63.8 72.8 78.4 - -\nTable 12: Retrieval results for ASQA (EM recall).\nR@1 R@3 R@5 R@20 R@100\nDPR 6.7 13.5 17.6 30.4 47.6\nGTR 14.6 24.7 31.6 49.7 65.6\nOracle 44.3 58.7 65.6 - -\nTable 13: Retrieval results for QAMPARI (recall-5).\npartially supports, or does not support the factual\nclaims in si. Citation ci,j has a citation precision\nof 1 if si has a recall of 1, and ci,j fully or partially\nsupports si. Finally, we take an average of preci-\nsion scores of all citations in the statement Sto\nobtain the citation precision score.\nG More Experiments\nG.1 Retrieval Analysis\nOracle. Since the original datasets do not contain\ngold passages at the same granularity level as our\nsetting (100-word passages), we approximate gold\npassages by running the following algorithm on the\ntop-100 retrieved passages. We first calculate the\nrecall score for each passage. Then, we sort the\npassages using their recall score and take the top 5\npassages as our initial oracle set. Finally, we iterate\nthrough all passages that were not initially in the or-\nacle set and try to replace the passages in the oracle\nset in a greedy fashion: we calculate the change in\nthe recall score of the oracle set for every possible\nreplacement and proceed with the replacement that\nresults in the largest recall improvement. The set\nof 5 oracle passages were able to match the recall\nscores of the top-100 retrieved passages.\nDetailed retrieval results. We show detailed re-\ntrieval results in Tables 12, 13, and 14.\nG.2 Effect of Instructions\nTable 15 shows results of using a full instruction\n(Table 23) and a short version of the instruction\n(Table 24). We see that the full version induces\nstronger correctness and citation recall, while the\ntwo instructions lead to similar citation precision.\nR@1 R@3 R@5 R@20 R@100\nBM25 3.0 6.6 9.6 19.3 31.8\nOracle 25.3 29.7 31.8 - -\nTable 14: Retrieval results for ELI5 (claim recall).\nFluency Correct. Citation\n(MAUVE) (EM Rec.) Rec. Prec.\nChatGPT(VANILLA, 5-doc)\nShort instruction 64.1 39.5 69.6 73.2\nFull instruction 66.6 40.4 73.6 72.5\nTable 15: Effect of different instructions on ASQA.\nG.3 Effect of Demonstrations\nTable 16 shows results on effect of different num-\nbers of demonstrations. We see that numbers of\ndemonstrations do not affect ChatGPT’s correct-\nness but using at least one demonstration ensures\nhigh citation recall. For the original LLaMA model,\nTable 16 shows the trend that more demonstrations\nlead to better performance.\nG.4 Fine-tuned Models\nTo better understand the differences between fine-\ntuned models and prompted large language mod-\nels, we train state-of-the-art question answer-\ning model, Fusion-in-Decoder (FiD; Izacard and\nGrave (2021)), and evaluate it in conjunction with\nPOST CITE . Due to the lack of training data with\ncitation annotation, we first train a T5-base FiD\nmodel for 5 epochs on the ASQA training set with\na batch size of 64 and a learning rate of 1e-4. Dur-\ning evaluation, we use POST CITE to add citations\nto the output. We also use k= 5passages during\nboth training and evaluation of the FiD model.\nThen, we evaluate this model on both ASQA\n(in-domain) and ELI5 (out-of-domain), and the re-\nsults can be found in Tables 17 and 18. Note that\nthis is not a direct comparison, as ALCE assumes\nonly evaluation data available and uses only few-\nshot data for prompting. As the results show, the\nFiD baseline still significantly lags behind prompt-\ning ChatGPT in both correctness and citation qual-\nity (even though it is trained on 4000+ examples).\nWhen tested on another dataset (ELI5), FiD per-\nforms even worse, showing that it is challenging\nto solve the problem by fine-tuning a small pre-\ntrained model.\n6480\nFluency Correct. Citation\n(MAUVE) (EM) Rec. Prec.\nChatGPT(VANILLA)\n#demo = 0 74.5 41.9 69.3 73.4\n#demo = 1 68.9 39.8 74.6 73.2\n#demo = 2 66.6 40.4 73.6 72.5\nTable 16: Different demonstrations on ASQA.\nFluency Correct. Citation\n(MAUVE) (EM Rec.) Rec. Prec.\nChatGPT 66.6 40.4 73.6 72.5\nFiD + POSTCITE 75.8 28.4 58.1 58.0\nTable 17: Comparison of Fusion-in-Decoder with Chat-\nGPT on ASQA. Both models use top-5 GTR passages.\nG.5 More Human Evaluation\nWe evaluate the accuracy of our automatic metrics\nby treating the human annotations as gold labels.\nFor citation recall, ALCE achieves an accuracy of\n85.1%; for citation precision, ALCE has an accu-\nracy of 77.6%. Regarding detecting insufficient\ncitations, ALCE has a recall of 82.3% and a pre-\ncision of 84.2%; regarding detecting “irrelevant”\ncitations, ALCE has a recall of 75.6% and a pre-\ncision of 66.1%—ALCE is effective in detecting\n“irrelevant” citations, but due to the limitation of\nthe NLI model (cannot detect “partial support”), it\nhas a relatively high false positive rate.\nG.6 Main Results\nWe show full results of our experiments along with\nthe standard deviation in Tables 19, 20, and 21. We\nrepeat all experiments with three different random\nseeds. However, for ChatGPT RERANK , we use\nonly one seeded run since each run repeats the\ngeneration step four times, and more experiments\nwould incur significant costs.\nH Prompts\nWe show detailed prompts used in our paper in\nTables 23, 24, 25, 26, 27, 28, and 29.\nI Examples\nIn Tables 30 and 31 we show some examples of\nquestions and model generated outputs.\nFluency Correct. Citation\n(MAUVE) (Claim) Rec. Prec.\nChatGPT 57.2 12.0 51.1 50.0\nFiD + POSTCITE 25.2 4.4 39.3 39.3\nTable 18: Comparison of Fusion-in-Decoder with Chat-\nGPT on ELI5. Both models use top-5 GTR passages.\n6481\nFluency Correct. Citation\n(MAUVE) (EM Rec.) Rec. Prec. ROUGE-L Length\nChatGPT\nVANILLA (5-psg) 66.8 (2.0) 40.4 (0.6) 73.6 (1.1) 72.5 (1.8) 37.0 (0.4) 40.0 (3.1)\nw/ RERANK 77.0 (0.0) 40.2 (0.0) 84.8 (0.0) 81.6 (0.0) 36.9 (0.0) 40.8 (0.0)\nSUMM (10-psg) 70.0 (1.2) 43.3 (0.8) 68.8 (0.6) 61.8 (1.1) 36.9 (0.2) 49.8 (4.3)\nw/ INTERACT 69.0 (2.7) 39.1 (0.5) 73.4 (0.2) 66.5 (4.9) 35.7 (0.2) 34.0 (0.9)\nSNIPPET (10-psg) 69.8 (2.5) 41.4 (0.6) 65.3 (0.6) 57.4 (0.9) 36.4 (0.4) 43.0 (3.5)\nINLINE SEARCH 58.7 (1.3) 32.4 (0.6) 58.3 (1.3) 58.3 (1.3) 58.2 (1.1) 23.7 (1.1)\nCLOSED BOOK 52.7 (4.9) 38.2 (0.1) 26.7 (1.1) 26.7 (1.1) 37.1 (0.3) 61.1 (4.5)\nORACLE (5-psg) 64.4 (0.6) 48.9 (1.2) 74.5 (0.6) 72.7 (1.0) 38.2 (1.0) 37.4 (3.0)\nChatGPT-16K\nVANILLA (5-psg) 60.3 (−) 36.1 (−) 76.2 (−) 76.5 (−) 36.2 (−) 24.7 (−)\nVANILLA (10-psg) 56.3 (−) 36.7 (−) 75.3 (−) 75.0 (−) 35.6 (−) 23.5 (−)\nVANILLA (20-psg) 56.7 (−) 36.1 (−) 73.7 (−) 73.5 (−) 35.5 (−) 23.1 (−)\nGPT-4\nVANILLA (5-psg) 67.1 (−) 41.3 (−) 68.5 (−) 75.6 (−) 39.2 (−) 31.8 (−)\nVANILLA (10-psg) 71.5 (−) 43.1 (−) 72.0 (−) 75.5 (−) 39.7 (−) 33.8 (−)\nVANILLA (20-psg) 64.9 (−) 44.4 (−) 73.0 (−) 76.5 (−) 40.1 (−) 34.3 (−)\nOpen-source\nLLaMA-7B VANILLA (3-psg) 69.8 (2.0) 22.6 (0.9) 6.2 (2.7) 9.2 (2.9) 29.1 (0.2) 61.3 (14.3)\nAlpaca-7B VANILLA (3-psg) 84.2 (2.7) 32.1 (1.7) 12.3 (7.2) 14.1 (7.0) 33.1 (0.8) 51.7 (12.8)\nVicuna-7B VANILLA (3-psg) 82.9 (5.0) 34.6 (0.7) 40.3 (0.5) 42.6 (1.0) 35.9 (0.7) 48.9 (6.6)\nLLaMA-13B VANILLA (3-psg) 68.4 (6.4) 26.9 (0.4) 10.6 (4.7) 15.4 (5.2) 29.8 (0.5) 67.1 (19.1)\nw/ RERANK 60.9 (14.5) 25.2 (2.5) 28.1 (9.3) 37.0 (7.2) 27.9 (2.4) 50.5 (14.3)\nLLaMA-13B SUMM (10-psg) 76.8 (4.7) 33.3 (0.7) 19.6 (3.9) 23.7 (4.7) 32.1 (0.3) 54.4 (1.5)\nLLaMA-13B SNIPPET (10-psg) 72.0 (0.8) 31.3 (1.1) 18.2 (3.1) 21.1 (3.6) 30.8 (0.4) 50.5 (4.5)\nLLaMA-13B ORACLE (3-psg) 69.5 (11.4) 34.3 (0.9) 10.8 (4.9) 15.8 (5.9) 30.6 (0.1) 67.3 (17.9)\nVicuna-13B VANILLA (3-psg) 82.6 (9.4) 31.9 (3.9) 51.1 (1.4) 50.1 (2.5) 34.9 (1.3) 39.1 (6.6)\nw/ RERANK 73.5 (2.1) 32.9 (1.3) 71.9 (1.9) 65.4 (1.5) 34.6 (0.3) 35.7 (4.2)\nVicuna-13B SUMM (10-psg) 67.7 (0.3) 43.2 (0.1) 52.7 (2.6) 50.0 (2.1) 36.7 (0.2) 66.0 (1.2)\nVicuna-13B SNIPPET (10-psg) 81.4 (3.0) 42.1 (1.2) 53.4 (1.9) 48.7 (1.6) 36.9 (0.4) 61.2 (7.4)\nVicuna-13B ORACLE (3-psg) 72.9 (3.5) 42.5 (1.6) 52.2 (0.8) 50.7 (1.6) 36.5 (0.9) 38.7 (3.5)\nLLaMA-33B VANILLA (3-psg) 83.7 (5.4) 31.0 (0.8) 19.5 (5.3) 23.0 (5.3) 32.3 (0.6) 44.1 (9.3)\nw/ RERANK 82.1 (3.0) 31.3 (1.1) 41.3 (6.4) 44.7 (5.5) 32.5 (0.9) 39.4 (8.0)\nLLaMA-33B SUMM (10-psg) 72.0 (3.0) 33.1 (1.9) 34.7 (5.8) 35.2 (6.0) 31.1 (0.8) 43.7 (5.0)\nLLaMA-33B SNIPPET (10-psg) 70.8 (3.1) 30.9 (1.4) 31.4 (4.2) 31.5 (5.3) 30.1 (0.7) 42.8 (3.6)\nLLaMA-33B ORACLE (3-psg) 82.6 (7.1) 39.3 (2.9) 20.2 (6.2) 23.9 (6.3) 33.1 (0.9) 42.0 (9.3)\nOasst-33B VANILLA (3-psg) 82.9 (2.7) 34.8 (1.5) 36.2 (1.7) 38.3 (2.7) 35.5 (0.7) 45.2 (6.3)\nw/ RERANK 83.2 (2.4) 35.1 (1.4) 66.7 (0.2) 64.3 (1.0) 35.0 (0.6) 41.8 (6.0)\nOasst-33B SUMM (10-psg) 74.3 (4.6) 40.9 (1.1) 45.5 (1.9) 44.0 (2.9) 35.8 (0.6) 54.3 (4.8)\nOasst-33B SNIPPET (10-psg) 79.3 (1.0) 40.1 (0.9) 45.0 (1.3) 43.3 (2.2) 35.8 (0.2) 50.9 (4.1)\nOasst-33B ORACLE (3-psg) 85.1 (2.8) 44.3 (2.4) 37.0 (1.0) 39.6 (1.5) 36.5 (1.1) 44.2 (5.8)\nLLaMA-2-7B-Chat VANILLA (5-psg) 80.1 (6.5) 33.9 (2.1) 50.9 (4.5) 47.5 (3.7) 35.1 (0.9) 42.3 (10.1)\nLLaMA-2-13B-Chat VANILLA (5-psg) 72.4 (6.3) 35.2 (1.2) 38.4 (5.9) 39.4 (4.8) 35.8 (0.9) 38.0 (6.4)\nLLaMA-2-70B-Chat VANILLA (5-psg) 88.3 (4.1) 41.5 (0.8) 62.9 (1.4) 61.3 (2.1) 37.1 (0.4) 52.9 (9.5)\nTable 19: ASQA full results.\n6482\nCorrectness Citation\nRec.-5 Prec. Rec. Prec. Num Pred.\nChatGPT\nVANILLA (5-psg) 20.8 (2.2) 20.8 (0.2) 20.5 (0.7) 20.9 (0.7) 5.0 (0.5)\nw/ RERANK 22.8 (0.0) 21.4 (0.0) 21.2 (0.0) 21.4 (0.0) 5.4 (0.0)\nSUMM (10-psg) 23.6 (0.9) 21.2 (0.5) 23.6 (0.7) 25.7 (0.8) 6.7 (0.4)\nSNIPPET (10-psg) 24.5 (1.4) 21.5 (1.8) 22.9 (1.6) 24.9 (0.4) 7.2 (0.9)\nw/ INTERACT 21.9 (0.9) 23.0 (0.4) 21.9 (1.2) 23.4 (0.9) 6.7 (0.4)\nINLINE SEARCH 17.2 (1.1) 20.4 (0.8) 14.9 (0.8) 14.9 (0.8) 6.7 (0.2)\nCLOSED BOOK 32.9 (1.1) 19.8 (1.6) 10.0 (0.4) 10.0 (0.4) 17.0 (2.9)\nORACLE 37.0 (3.1) 36.9 (0.6) 24.1 (1.2) 24.6 (1.3) 5.3 (0.6)\nChatGPT-16K\nVANILLA (5-psg) 21.1 (−) 22.0 (−) 20.7 (−) 21.2 (−) 4.9 (−)\nVANILLA (10-psg) 23.4 (−) 21.9 (−) 21.6 (−) 22.0 (−) 5.7 (−)\nVANILLA (20-psg) 26.4 (−) 21.1 (−) 19.4 (−) 19.7 (−) 7.6 (−)\nGPT-4\nVANILLA (5-psg) 22.2 (−) 25.0 (−) 25.9 (−) 27.0 (−) 4.4 (−)\nVANILLA (10-psg) 26.8 (−) 25.1 (−) 26.2 (−) 27.2 (−) 5.7 (−)\nVANILLA (20-psg) 29.6 (−) 26.2 (−) 27.4 (−) 28.5 (−) 6.8 (−)\nOpen-source\nLLaMA-7B VANILLA (3-psg) 7.8 (3.4) 7.4 (2.7) 5.1 (0.5) 5.7 (0.8) 5.7 (0.6)\nAlpaca-7B VANILLA (3-psg) 9.4 (3.7) 9.5 (3.6) 6.4 (0.5) 6.8 (0.5) 5.1 (0.1)\nVicuna-7B VANILLA (3-psg) 11.3 (1.4) 13.3 (2.3) 10.1 (0.6) 10.9 (0.5) 3.9 (0.3)\nLLaMA-13B VANILLA (3-psg) 9.7 (3.6) 9.1 (3.1) 6.7 (0.9) 7.1 (0.9) 5.9 (0.6)\nw/ RERANK 10.0 (3.3) 10.7 (3.3) 9.9 (1.2) 10.2 (1.1) 5.4 (0.5)\nLLaMA-13B SUMM (10-psg) 14.8 (2.5) 12.6 (1.5) 7.4 (0.5) 8.0 (0.6) 8.1 (0.9)\nLLaMA-13B SNIPPET (10-psg) 17.7 (1.4) 15.7 (0.9) 8.8 (0.7) 9.9 (0.6) 8.2 (0.4)\nLLaMA-13B ORACLE (3-psg) 16.8 (6.6) 15.4 (5.6) 7.7 (1.0) 8.3 (1.1) 5.7 (0.7)\nVicuna-13B VANILLA (3-psg) 14.0 (0.6) 15.9 (1.7) 12.5 (0.8) 13.4 (0.7) 4.7 (0.3)\nw/ RERANK 13.0 (0.7) 17.2 (2.2) 17.3 (0.8) 17.7 (0.6) 4.4 (0.3)\nVicuna-13B SUMM (10-psg) 21.1 (1.4) 17.1 (0.3) 15.7 (0.2) 17.8 (0.1) 6.9 (0.7)\nVicuna-13B SNIPPET (10-psg) 21.9 (0.8) 18.2 (0.3) 16.8 (0.3) 19.7 (0.6) 7.5 (0.4)\nVicuna-13B ORACLE (3-psg) 25.9 (1.6) 28.4 (2.6) 15.8 (1.4) 16.8 (1.4) 4.9 (0.5)\nLLaMA-33B VANILLA (3-psg) 14.7 (3.3) 12.0 (2.2) 7.9 (0.7) 8.3 (0.6) 7.2 (0.7)\nw/ RERANK 14.0 (3.4) 13.9 (2.6) 10.7 (0.6) 11.1 (0.5) 6.4 (0.7)\nLLaMA-33B SUMM (10-psg) 19.0 (1.9) 14.8 (0.8) 12.5 (0.2) 15.0 (0.3) 7.6 (0.6)\nLLaMA-33B SNIPPET (10-psg) 19.6 (1.1) 15.7 (0.1) 12.8 (1.1) 15.2 (1.2) 7.8 (0.5)\nLLaMA-33B ORACLE (3-psg) 23.9 (6.9) 20.3 (5.2) 9.8 (1.2) 10.4 (1.2) 6.8 (0.9)\nOasst-33B VANILLA (3-psg) 15.5 (1.5) 14.9 (1.4) 9.0 (1.6) 10.1 (1.8) 5.6 (0.3)\nw/ RERANK 14.1 (1.1) 15.8 (1.0) 15.0 (1.6) 15.9 (1.6) 4.7 (0.3)\nOasst-33B SUMM (10-psg) 21.0 (0.6) 17.5 (1.0) 12.9 (1.2) 16.6 (1.2) 7.1 (0.4)\nOasst-33B SNIPPET (10-psg) 22.0 (0.4) 17.4 (0.3) 13.6 (1.7) 17.7 (1.6) 7.5 (0.1)\nOasst-33B ORACLE (3-psg) 26.9 (3.7) 26.0 (3.3) 11.7 (1.0) 12.9 (1.2) 5.6 (0.4)\nLLaMA-2-7B-Chat VANILLA (5-psg) 16.2 (1.3) 15.3 (1.6) 10.6 (0.9) 10.9 (1.0) 5.5 (0.0)\nLLaMA-2-13B-Chat VANILLA (5-psg) 21.1 (0.9) 18.2 (0.5) 9.6 (1.5) 9.7 (1.5) 6.5 (0.3)\nLLaMA-2-70B-Chat VANILLA (5-psg) 21.8 (0.7) 18.4 (0.1) 15.1 (1.2) 15.6 (1.3) 7.1 (0.2)\nTable 20: QAMPARI full results.\n6483\nFluency Correct. Citation\n(MAUVE) (Claim) Rec. Prec. ROUGE-L Length\nChatGPT\nVANILLA (5-psg) 57.2 (1.6) 12.0 (0.6) 51.1 (4.2) 50.0 (4.8) 20.6 (0.2) 91.5 (6.5)\nw/ RERANK 56.1 (0.0) 11.4 (0.0) 69.3 (0.0) 67.8 (0.0) 20.3 (0.0) 103.4 (0.0)\nSUMM (10-psg) 40.2 (1.2) 12.5 (0.2) 51.5 (1.1) 48.2 (2.0) 20.3 (0.1) 90.0 (6.6)\nSNIPPET (10-psg) 62.9 (2.2) 14.3 (0.1) 50.4 (1.1) 45.0 (2.w) 21.0 (0.1) 100.0 (6.8)\nw/ INTERACT 68.0 (5.8) 13.3 (0.5) 47.8 (3.3) 45.0 (3.1) 20.1 (0.2) 99.8 (6.1)\nINLINE SEARCH 49.7 (4.6) 13.4 (1.1) 45.6 (2.5) 43.7 (3.9) 20.4 (0.3) 103.0 (18.1)\nCLOSED BOOK 32.6 (1.1) 18.6 (0.5) 15.4 (0.3) 15.4 (0.3) 22.8 (0.1) 108.3 (8.9)\nORACLE (5-psg) 59.4 (4.1) 21.3 (0.2) 57.8 (3.7) 56.0 (3.8) 21.2 (0.3) 93.0 (7.8)\nChatGPT-16K\nVANILLA (5-psg) 31.6 (−) 14.4 (−) 44.6 (−) 44.1 (−) 21.4 (−) 87.6 (−)\nVANILLA (10-psg) 26.6 (−) 14.4 (−) 45.5 (−) 43.3 (−) 21.5 (−) 87.5 (−)\nVANILLA (20-psg) 31.6 (−) 15.9 (−) 43.4 (−) 40.9 (−) 21.7 (−) 92.6 (−)\nGPT-4\nVANILLA (5-psg) 38.4 (−) 14.2 (−) 44.0 (−) 50.1 (−) 20.6 (−) 79.6 (−)\nVANILLA (10-psg) 39.9 (−) 15.7 (−) 49.5 (−) 54.2 (−) 21.2 (−) 88.2 (−)\nVANILLA (20-psg) 41.5 (−) 18.3 (−) 48.5 (−) 53.4 (−) 22.2 (−) 97.0 (−)\nOpen-source\nLLaMA-7B VANILLA (3-psg) 28.6 (17.9) 1.6 (0.9) 1.2 (0.0) 2.7 (0.1) 12.2 (1.3) 46.9 (1.2)\nAlpaca-7B VANILLA (3-psg) 45.9 (5.3) 9.2 (0.1) 4.5 (1.6) 5.2 (1.9) 18.8 (0.3) 67.1 (1.2)\nVicuna-7B VANILLA (3-psg) 43.2 (3.9) 10.0 (0.5) 12.6 (2.3) 16.3 (2.6) 19.1 (0.4) 68.7 (2.0)\nLLaMA-13B VANILLA (3-psg) 50.0 (2.0) 3.9 (0.4) 3.1 (0.9) 5.3 (1.3) 16.1 (0.5) 63.3 (2.0)\nw/ RERANK 46.7 (2.9) 4.3 (0.4) 9.7 (2.1) 15.0 (2.2) 16.1 (0.7) 63.0 (2.3)\nLLaMA-13B SUMM (10-psg) 28.6 (1.8) 2.9 (0.1) 2.5 (0.8) 3.8 (0.8) 8.5 (0.3) 33.1 (0.6)\nLLaMA-13B SNIPPET (10-psg) 48.4 (3.1) 5.7 (0.9) 5.8 (0.6) 7.6 (0.9) 15.1 (1.1) 60.2 (3.2)\nLLaMA-13B ORACLE (3-psg) 49.5 (2.4) 6.4 (0.6) 3.7 (0.7) 6.5 (1.0) 16.8 (0.5) 64.5 (1.7)\nVicuna-13B VANILLA (3-psg) 58.2 (25.1) 10.0 (0.3) 15.6 (2.2) 19.6 (2.0) 19.1 (0.3) 69.6 (0.6)\nw/ RERANK 45.9 (4.3) 9.2 (0.0) 31.7 (2.9) 38.2 (1.6) 18.6 (0.5) 69.7 (1.0)\nVicuna-13B SUMM (10-psg) 22.4 (3.0) 4.9 (0.1) 9.7 (1.3) 12.2 (1.2) 9.3 (0.4) 33.0 (3.7)\nVicuna-13B SNIPPET (10-psg) 48.1 (5.3) 11.2 (1.4) 27.2 (3.6) 27.9 (1.9) 18.4 (1.9) 76.8 (8.7)\nVicuna-13B ORACLE (3-psg) 41.6 (3.1) 17.1 (0.4) 20.2 (3.0) 26.5 (3.0) 20.0 (0.3) 72.0 (0.3)\nLLaMA-33B VANILLA (3-psg) 58.8 (4.3) 6.2 (0.0) 9.3 (3.0) 12.1 (4.2) 16.9 (0.2) 60.0 (1.3)\nw/ RERANK 65.9 (2.5) 6.0 (0.7) 22.5 (5.2) 26.1 (6.9) 17.5 (0.4) 61.0 (1.2)\nLLaMA-33B SUMM (10-psg) 23.3 (2.0) 3.0 (0.2) 6.2 (0.5) 8.2 (0.7) 7.5 (0.4) 26.2 (2.3)\nLLaMA-33B SNIPPET (10-psg) 53.2 (4.0) 7.4 (1.3) 13.7 (0.5) 15.1 (0.4) 14.4 (1.7) 53.3 (8.5)\nLLaMA-33B ORACLE (3-psg) 63.7 (2.8) 11.4 (0.5) 11.9 (2.6) 15.4 (3.6) 17.9 (0.2) 61.7 (2.6)\nOasst-33B VANILLA (3-psg) 46.8 (7.6) 9.5 (0.2) 16.0 (2.5) 21.6 (3.5) 18.6 (0.3) 67.8 (1.5)\nw/ RERANK 52.1 (6.1) 8.5 (0.5) 34.4 (2.9) 41.5 (2.5) 18.2 (0.3) 67.0 (1.5)\nOasst-33B SUMM (10-psg) 24.8 (2.8) 3.9 (0.3) 12.3 (0.2) 16.3 (0.3) 9.1 (0.3) 31.6 (1.5)\nOasst-33B SNIPPET (10-psg) 50.7 (4.6) 10.7 (1.2) 25.8 (3.3) 26.7 (2.3) 17.8 (1.8) 69.6 (8.6)\nOasst-33B ORACLE (3-psg) 50.7 (12.1) 15.8 (0.1) 20.8 (2.8) 28.0 (3.2) 19.4 (0.1) 70.3 (1.1)\nLLaMA-2-7B-Chat VANILLA (5-psg) 27.8 (3.0) 10.9 (0.2) 19.8 (1.2) 15.0 (1.4) 20.5 (0.2) 87.8 (8.1)\nLLaMA-2-13B-Chat VANILLA (5-psg) 34.7 (1.5) 13.4 (0.4) 17.3 (1.3) 15.8 (1.4) 20.9 (0.2) 88.3 (6.3)\nLLaMA-2-70B-Chat VANILLA (5-psg) 38.6 (4.8) 12.8 (1.0) 38.3 (2.4) 37.9 (1.9) 21.3 (0.1) 110.8 (5.6)\nTable 21: ELI5 full results.\n6484\nRead the original question and passage, and generate 3 additional claims that are supported by the\npassage and answer the question.\nOriginal question: What’s the difference between Shia vs. Sunni Islam?\nPassage: The main difference between Shia and Sunni Muslim is related to ideological heritage and\nissues of leadership. This difference is first formed after the death of the Prophet Muhammad in 632\nA.D. The ideological practice of the Sunni branch strictly follows Prophet Muhammad and his\nteachings, while the Shia branch follows Prophet Muhammad’s son-in-law Ali. Nowadays, Sunni and\nShia are the major branches of Islam.\nClaim 1: The major branches of Islam are Sunni and Shia.\nClaim 2: Prophet Muhammad died in 632 A.D.\nClaim 3: The ideological practice of the Sunni branch strictly follows Prophet Muhammad and his\nteachings.\nOriginal question: What causes Bi-polar disorder?\nPassage: Bipolar disorder is an emotional disorder that causes extreme mood swings between\nexcitement and depression. The spectrum of mood swing may span from days to months. We are still not\ncertain of the exact factors that cause such disorder, but genetics is considered a major factor.\nClaim 1: One symptom of Bi-polar disorder is extreme mood swings between excitement and depression.\nClaim 2: Genetics could be one of the major factors that causes Bi-polar disorder.\nClaim 3: The mood swing from Bi-polar disorder can last days to months.\nOriginal question: How do we hear differences in sound besides volume and pitch?\nPassage: Pitch refers to the frequency of soundwave, and volumn refers to the amplitude of the\nsoundwave. Besides volumn and pitch, we can also tell the difference between sounds based on the\ntone of sound. For example, we can differentiate the sound of different instruments based on the\ntone of the sounds.\nClaim 1: Volume of sound is the amplitude of the soundwave.\nClaim 2: Pitch is the frequency of soundwave.\nClaim 3: We can use the tone of the sounds to differentiate the sound of different instruments.\nOriginal question: How are we able to discern whether a sound is coming from in front of us or\nbehind us?\nPassage: There are multiple explanations for why we can localize sounds. One explanation is that\nsounds travelling to the corresponding side of one’s ear will be slightly louder. Another\nexplanation is that there is a slight difference in the hitting time to one’s left and right ear\nbased on the sound’s direction. However, these explanation means that when a sound is exactly in\nfront of someone or exactly behind someone, he or she can not tell the difference.\nClaim 1: We can localize sounds by recognizing that the sound travelling to the corresponding side\nof one’s ear will be slightly louder.\nClaim 2: We can also localize sounds by recognizing the difference in hitting time to one’s left and\nright ear based on the sound’s direction.\nClaim 3: We cannot tell the difference between a sound that is exactly in front of us or exactly\nbehind us.\nTable 22: Prompt used to generate the sub-claims for ELI5 questions. Blue text is model generation. Brown text\nis the ELI5 example that we want to generate sub-claims for. We construct the prompt by manually writing the\nsub-claims for three questions from the training set.\nInstruction: Write an accurate, engaging, and concise answer for the given question using only\nthe provided search results (some of which might be irrelevant) and cite them properly. Use an\nunbiased and journalistic tone. Always cite for any factual claim. When citing several search\nresults, use [1][2][3]. Cite at least one document and at most three documents in each sentence.\nIf multiple documents support the sentence, only cite a minimum sufficient subset of the\ndocuments.\nTable 23: Instruction for VANILLA .\nInstruction: Write a high-quality answer for the given question using only the provided search\nresults and cite them properly using [1][2][3].\nTable 24: Short instruction for VANILLA .\n6485\nSummarize the following document within 50 words with the question of interest \"{QUESTION}\"\nReturn \"irrelevant\" if the document is irrelevant to the question. Try to keep all the important\ndates, numbers, and names.\nTitle: {TITLE}\nText: {TEXT}\nSummary:\nTable 25: Prompts for SUMM .\nGiven the follow passage and the question \"{QUESTION}\", extract a useful span from the passage\nthat can answer the question. Resolve all the coreference issues to make the extracted span\nunderstandable standalone. If the passage is not helpful for answering the question, return\n\"irrelevant\".\nTitle: {TITLE}\nText: {TEXT}\nExtracted span:\nTable 26: Prompts for SNIPPET .\nInstruction: Write an accurate, engaging, and concise answer for the given question using only\nthe provided search results and cite them properly. Use an unbiased and journalistic tone.\nAlways cite for any factual claim.\nYou are provided summaries/snippets of the search results. You can use \"Check: Document [1][2]\"\nto check the corresponding full documents (you should only check relevant documents and you can\nat most check 3 documents at a time) and use \"Output:\" to output a sentence in the answer. In the\nanswer, cite properly by using [1][2][3]. Cite at least one document and at most three documents\nin each sentence. If multiple documents support the sentence, only cite a minimum sufficient\nsubset of the documents. Use \"End\" to end the generation.\n<Retrieve for question “...”>\n<Get summaries/snippets for the passages and delete those that are “irrelevant”>\nDocument [1](Title: ...) {SUMMARY OR SNIPPET}\n...\nQuestion: When did US break away from England?\nCheck: Document [1][2]\nDocument [1] {FULL TEXT}\nDocument [2] {FULL TEXT}\nOutput: The United States ... [1] ... [2].\n<Remove the full text of [1][2] from context>\nCheck: Document [3]\nDocument [3] {FULL TEXT}\nOutput: The Treaty of Paris ... [3].\n<Remove the full text of [3] from context>\nEnd.\nTable 27: An example for INTERACT .\nInstruction: Write an accurate, engaging, and concise answer for the given question using only\nthe provided search results and cite them properly. Use an unbiased and journalistic tone.\nAlways cite for any factual claim.\nYou can use \"Search: key words\" to check the most relevant document’s full text and use\n\"Output:\" to output a sentence in the answer. In the answer, cite properly by using [1][2][3].\nCite at least one document and at most three documents in each sentence. If multiple documents\nsupport the sentence, only cite a minimum sufficient subset of the documents. Use \"End\" to end\nthe generation.\nTable 28: Instruction for INLINE SEARCH .\nInstruction: Write an accurate, engaging, and concise answer for the given question. Use an\nunbiased and journalistic tone.\nTable 29: Instruction for CLOSED BOOK .\n6486\nInstruction: Write an accurate, engaging, and concise answer for ...\nDocument [1](Title: How to Treat and Prevent Food Poisoning - MsPrepper) just a typical gastro\nupset. Salmonella is most commonly caused by eating undercooked or raw foods like eggs or meat.\nYou know how your mom always warned you not to eat raw cookie dough? This is why. Most people do\neat cookie dough and they are fine, but salmonella is a risk. If you do contract salmonella, you\ncould start to feel bad within in a couple of hours after eating contaminated food, and sometimes\nit could take a day or two. Common symptoms are nausea and vomiting, loose stools (sometimes\nbloody), flu like symptoms, and stomach cramps. To treat\nDocument [2](Title: FDA Issues Warning About Eating Raw Cookie Dough, But Not For Salmonella\nRisks) FDA Issues Warning About Eating Raw Cookie Dough, But Not For Salmonella Risks Used to\nlicking the spoon or placating yourself with full-on chunks of raw cookie dough? The Food and\nDrug Administration issued a warning on Tuesday that strongly advises against continuing the\nhabit. The agency asserted that consuming raw batter of any kind, whether for bread, cookies or\npizza, could make a person sick. While you may have been warned in the past against eating raw\ndough due to the risk of contracting salmonella from raw eggs, the FDA is citing raw flour as the\nculprit for a\nDocument [3](Title: It’s Probably OK to Eat Raw Cookie Dough — As Long As You’re Smart About It -\nThe Crux - Very Top Secret Information) First, when most people think about health risks and\ncookie dough, they think about raw egg. Eggs can be contaminated with salmonella bacteria, and\nfood safety recommendations encourage people to cook eggs until the white and yolk are firm in\norder to kill any bacteria. However, anyone making cookies can do things to reduce this risk by\nusing pasteurized egg products. When my kids and I make cookie dough, we never use regular eggs.\nInstead, we use shell eggs that have been pasteurized to kill any harmful bacteria without\nactually cooking the egg itself. (A great public health innovation, if\nDocument [4](Title: How Dangerous Is It to Eat Raw Cookie Dough? | Men’s Health) Can Eating Raw\nCookie Dough Really Make You Sick? Scientists reveal the truth about this supposedly dangerous\ndelicacy By Katherine Dempsey There are few things more tempting in life than eyeing a bowl of\ncookie dough and deciding whether or not to stick your finger in for a scoop. It’s a bit like\nplaying Russian roulette. You could get lucky and enjoy the delicious dough without conseqence,\nbut there’s always the risk of getting serously sick with a food-borne illness. That’s because\nmultiple ingredients within the dough could be contaminated by pathogens such as Salmonella and\nE. coli, says Soohyoun\nDocument [5](Title: How Dangerous Is It to Eat Raw Cookie Dough? | Men’s Health) is usually\npasteurized, so it’s not likely that the egg would make you sick. However, other ingredients in\nthe dough could potentially harbor pathogens. Experts say that a prime suspect in a 2009 E. coli\noutbreak linked to prepackaged cookie dough was actually the flour. Nuts and chocolate have also\nbeen linked to Salmonella outbreaks. Bottom line: You’re better off skipping raw cookie dough.\nAt least you’ll have killer cookies as a consolation. The article How Bad Is It To Eat Raw Cookie\nDough? originally ran on Prevention.com The 9 Foods Most Likely to Make You Sick Mmm, Just Don’t\n...\nQuestion: Why is it bad to eat cookie dough for risk of salmonella but things like Cookie Dough\nBites are ok?\nAnswer: Raw cookie dough is not recommended to be eaten due to the risk of salmonella [1][2].\nEating raw flour is also a risk for food poisoning [2]. The egg in cookie dough is usually\npasteurized, so it’s not likely that the egg would make you sick, but other ingredients in the\ndough could potentially harbor pathogens [4][5]. However, prepackaged cookie dough like Cookie\nDough Bites is safe to eat because the dough is made with pasteurized egg products and\nheat-treated flour [2][3]..\nGround truth claims:\n1. Cookie Dough Bites are safe to eat since they do not contain eggs.\n2. Salmonella is a bacteria found in eggs.\n3. Eating raw cookie dough carries the risk of salmonella infection.\nTable 30: ELI5 example 1.\n6487\nInstruction: Write an accurate, engaging, and concise answer for ...\nDocument [1](Title: Is Snapchat really worth $19 billion? - CSMonitor.com) reporting that the Los\nAngeles-based company is aiming to raise $500 million at a valuation of $16 billion to $19 billion,\nmaking it the third most highly valued tech start-up backed by venture capitalists. The Chinese\nhandset maker Xiaomi is valued at $45 billion, while Uber is estimated to be valued at about $40\nbillion, according to data from CB Insights. Read MoreVC investment hits $86B thanks to Uber, Xiaomi\nSnapchat was valued at $10 billion in August, according to a Dow Jones report. Some of its investors\nfrom previous rounds include Benchmark, Lightspeed Venture Partners and Kleiner Perkins Caufield\nDocument [2](Title: What Are Venture Capital Investments? – DollarsAndSense.my) Ever wondered how\nhighly valued technology giants like Google and Facebook were able to grow so fast and pay their\nemployees so well in such a short amount of time, or how still growing start-ups like Uber are able\nto lose 1.2 billion US dollars in just the first half of this year alone and still command a\nvaluation upwards of 50 billion US dollars? The answer lies with a special category of investment\nactivity known as venture capital. Venture capitalists are professional investors who invest in a\nnumber of highly scalable high-risk technology ventures hoping to make a multi-fold\nDocument [3](Title: Opinion | What Dara Khosrowshahi Must Do to Save Uber - The New York Times) at a\ndiscount. These are troubling signs. Every start-up must one day fulfill the market’s demand that\nit turn a profit, but Uber has never figured out how to do that. While ride sharing in some form\nwill probably survive, it’s more likely that without some drastic changes, Uber won’t be around in\nthree to five years. Mr. Khosrowshahi must avoid the mistakes of his predecessor by accepting that\n“pivots” (Silicon Valley-speak for the desperate changes troubled companies make to reassure their\nventure capitalist funders) are not the answer. None of the pivots Mr. Kalanick tried — like\non-demand delivery\nDocument [4](Title: Snapchat raising funding round at $19 billion valuation: Report) Snapchat\nraising funding round at $19 billion valuation: Report The mobile messaging app Snapchat is said to\nbe seeking funding at a $19 billion valuation, according to a report, citing a person with knowledge\nof the matter. Bloomberg is reporting that the Los Angeles-based company is aiming to raise $500\nmillion at a valuation of $16 billion to $19 billion, making it the third most highly valued tech\nstart-up backed by venture capitalists. The Chinese handset maker Xiaomi is valued at $45 billion,\nwhile Uber is estimated to be valued at about $40 billion, according to data from CB Insights.\nDocument [5](Title: Unicorns And Wall Street | MoneyTips) not start showing profits soon. Even Uber,\nwith its massive $62-billion-plus valuation, will test investor’s patience if it does not start\nshowing signs of a profit in the next few years. Cracks are starting to show in terms of pre-IPO\ndevaluations. Dropbox and Snapchat have seen a nearly one-quarter drop in their pre-IPO valuations,\nwhile Square’s pre-IPO value has dropped by approximately one-third. Larger investors and venture\ncapitalists are sending a message to the unicorns: Outline a solid path to profits and start\nproducing them soon, or expect your valuation to be slashed. Diving ahead with an IPO is not\n...\nQuestion: How are firms like snapchat, uber etc valued so highly while still not making a profit? Do\nventure capitalists not expect some form of repayment within a number of years?\nAnswer: Venture capitalists invest in a number of highly scalable high-risk technology ventures\nhoping to make a multi-fold return on their investment in a short amount of time [2]. Highly valued\ncompanies like Snapchat, Uber, and Xiaomi are able to grow so fast and command high valuations\nbecause they have the potential to generate significant revenue in the future [2][4]. Venture\ncapitalists expect a return on their investment within a number of years, but they also invest in the\nlong-term potential of the company [2]. It is important to note that every start-up must eventually\nturn a profit, but some start-ups like Uber have not yet figured out how to do that [3][5].\nGround truth claims:\n1. Firms like Snapchat and Uber need to establish their brand and amass users before introducing\nads.\n2. Introducing ads too early can deter potential users.\n3. Uber is reinvesting a lot of money to make their service better.\nTable 31: ELI5 example 2.\n6488",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7467490434646606
    },
    {
      "name": "Correctness",
      "score": 0.7129409909248352
    },
    {
      "name": "Citation",
      "score": 0.6569564342498779
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.620024561882019
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5775676965713501
    },
    {
      "name": "Data science",
      "score": 0.5256950855255127
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5019114017486572
    },
    {
      "name": "Fluency",
      "score": 0.47122514247894287
    },
    {
      "name": "Information retrieval",
      "score": 0.4517882466316223
    },
    {
      "name": "Natural language processing",
      "score": 0.3415295481681824
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34059593081474304
    },
    {
      "name": "World Wide Web",
      "score": 0.2370004951953888
    },
    {
      "name": "Psychology",
      "score": 0.10163271427154541
    },
    {
      "name": "Programming language",
      "score": 0.09854021668434143
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Mathematics education",
      "score": 0.0
    }
  ],
  "institutions": []
}