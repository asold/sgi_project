{
  "title": "It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
  "url": "https://openalex.org/W3085177480",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2738082409",
      "name": "Timo Schick",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "Sulzer (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2035156685",
      "name": "Hinrich Schütze",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2998696444",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963674932",
    "https://openalex.org/W1724438581",
    "https://openalex.org/W2111316763",
    "https://openalex.org/W95183648",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W1489949474",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3029040966",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2898662126",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W806995027",
    "https://openalex.org/W2991382858",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2962847482",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2927746189",
    "https://openalex.org/W2998554035",
    "https://openalex.org/W2995335514"
  ],
  "abstract": "When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much \"greener\" in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 2339–2352\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n2339\nIt’s Not Just Size That Matters:\nSmall Language Models Are Also Few-Shot Learners\nTimo Schick1,2 and Hinrich Schütze1\n1 Center for Information and Language Processing, LMU Munich, Germany\n2 Sulzer GmbH, Munich, Germany\ntimo.schick@sulzer.de\nAbstract\nWhen scaled to hundreds of billions of pa-\nrameters, pretrained language models such as\nGPT-3 (Brown et al., 2020) achieve remark-\nable few-shot performance. However, enor-\nmous amounts of compute are required for\ntraining and applying such big models, result-\ning in a large carbon footprint and making\nit difﬁcult for researchers and practitioners to\nuse them. We show that performance similar\nto GPT-3 can be obtained with language mod-\nels that are much “greener” in that their pa-\nrameter count is several orders of magnitude\nsmaller. This is achieved by converting textual\ninputs into cloze questions that contain a task\ndescription, combined with gradient-based op-\ntimization; exploiting unlabeled data gives fur-\nther improvements. We identify key factors re-\nquired for successful natural language under-\nstanding with small language models.1\n1 Introduction\nPretraining ever-larger language models (LMs) on\nmassive corpora has led to large improvements in\nNLP (Radford et al., 2018; Devlin et al., 2019; Liu\net al., 2019; Raffel et al., 2020, i.a.). A standard\napproach is to replace the pretrained model’s out-\nput layer with a task-speciﬁc head and ﬁnetune\nthe entire model on a set of labeled training data.\nHowever, language modeling is not only a pow-\nerful pretraining objective, but many tasks can be\nreformulated as cloze questions (e.g., by append-\ning phrases such as “the correct answer is __”),\nallowing pretrained LMs to solve them without any\nor with only very few labeled examples (Radford\net al., 2019; Schick and Schütze, 2021).\nRecently, Brown et al. (2020) introduced GPT-3,\na pretrained LM with an enormous 175 billion pa-\nrameters, and showed that it has amazing few-shot\nabilities: By reformulating tasks as LM problems,\n1Our implementation is publicly available at https://\ngithub.com/timoschick/pet.\n102 103 104 105 106\n50\n60\n70\n80\nGPT -3PET\niPET\nParameters (Millions)\nSuperGLUE Performance\nFigure 1: Performance on SuperGLUE with 32 train-\ning examples. ALBERT with PET/iPET outperforms\nGPT-3 although it is much “greener” in that it has\nthree orders of magnitude fewer parameters.\nGPT-3 achieves near state-of-the-art results for\nsome SuperGLUE (Wang et al., 2019) tasks given\njust 32 labeled examples. This is achieved through\npriming: GPT-3 is given a few demonstrations of\ninputs and corresponding outputs as context for its\npredictions, but no gradient updates are performed.\nWhile being straightforward to use, this method\nhas two major drawbacks:\n• It requires a gigantic LM to work well, mak-\ning it unusable in many real-world scenar-\nios and resulting in a large carbon footprint\n(Strubell et al., 2019).\n• It does not scale to more than a few exam-\nples as the context window of most LMs is\nlimited to a few hundred tokens.2\nAn alternative to priming is pattern-exploiting\ntraining (PET) (Schick and Schütze, 2021), which\ncombines the idea of reformulating tasks as cloze\nquestions with regular gradient-based ﬁnetuning.\nWhile PET additionally requires unlabeled data, un-\nlabeled data is much easier to obtain than labeled\n2While GPT-3 can process up to 2,048 tokens, this is still\nnot enough to ﬁt ≥32 examples for some SuperGLUE tasks.\n2340\nexamples for many real-world applications. Cru-\ncially, PET only works when the answers to be\npredicted by the LM correspond to a single token\nin its vocabulary; this is a severe limitation as many\ntasks cannot easily be worded that way.\nIn this work, we adapt PET for tasks that require\npredicting multiple tokens. We then show that in\ncombination with ALBERT (Lan et al., 2020), PET\nand its iterative variant (i PET) both outperform\nGPT-3 on SuperGLUE with 32 training examples,\nwhile requiring only 0.1% of its parameters (Fig-\nure 1). Moreover, training with PET can be per-\nformed in several hours on a single GPU without\nrequiring expensive hyperparameter optimization.\nFinally, we show that similar performance can also\nbe achieved without unlabeled data and provide\na detailed analysis of the factors contributing to\nPET’s strong performance: its ability to combine\nmultiple task formulations, its resilience to word-\nings that are hard to understand, its usage of la-\nbeled data, and characteristics of the underlying\nLM. Given PET’s “green” properties, we see our\nwork as an important contribution to an environ-\nmentally sound NLP.\n2 Related Work\nEnabling LMs to perform zero-shot learning by pro-\nviding task descriptions was proposed by Radford\net al. (2019) and has been applied to text classiﬁ-\ncation (Puri and Catanzaro, 2019), commonsense\nknowledge mining (Davison et al., 2019) and ar-\ngumentative relation classiﬁcation (Opitz, 2019).\nIt is also commonly used for probing the knowl-\nedge contained within LMs (Trinh and Le, 2018;\nPetroni et al., 2019; Talmor et al., 2020; Schick and\nSchütze, 2020; Ettinger, 2020, i.a.).\nAs ﬁnding ways to reformulate tasks as cloze\nquestions that are understood well by LMs is difﬁ-\ncult (Jiang et al., 2020), Schick and Schütze (2021)\npropose PET, a method that uses knowledge distil-\nlation (Hinton et al., 2015) and self-training (e.g.,\nScudder, 1965; Yarowsky, 1995; Brin, 1999; Mc-\nClosky et al., 2006) to easily combine several re-\nformulations. Our modiﬁed version of PET uses\nmasked language models (Devlin et al., 2019) to\nassign probabilities to sequences of text; this is sim-\nilar to using them in a generative fashion (Wang\nand Cho, 2019) and has previously been inves-\ntigated by Salazar et al. (2020) and Ghazvinine-\njad et al. (2019). In contrast to PET, which uses\ngradient-based optimization, Radford et al. (2019)\nP(x)\nOil prices rise ? __ , Oil prices fall back.\nx2 x1\nY es\nNo\nentailment\nnot_entailment\ny v(y)\nqp(y |x)\nFigure 2: Application of a PVP p = (P, v) for recog-\nnizing textual entailment: An inputx = (x1, x2) is con-\nverted into a cloze question P(x); qp(y |x) for each y\nis derived from the probability ofv(y) being a plausible\nchoice for the masked position.\nand Brown et al. (2020) investigate priming, where\nexamples are given as context but no parameter\nupdates are performed.\nFinally, our focus on reducing the amount of\ncompute required for few-shot learning is closely\nrelated to other efforts in Green AI (Schwartz et al.,\n2020a) that aim to improve model efﬁciency, in-\ncluding techniques for knowledge distillation (e.g.,\nHinton et al., 2015; Sanh et al., 2019; Jiao et al.,\n2020; Mao et al., 2020; Anderson and Gómez-\nRodríguez, 2020), pruning (Han et al., 2015, 2016;\nSanh et al., 2020) and quantization (Gong et al.,\n2014; Zafrir et al., 2019; Stock et al., 2021) as\nwell as early exit strategies for inference (Liu et al.,\n2020; Schwartz et al., 2020b; Xin et al., 2020).\n3 Pattern-Exploiting Training\nLet M be a masked language model (MLM), T its\nvocabulary and __ ∈T the mask token; we denote\nthe set of all token sequences as T∗. For some\nz ∈T∗ containing at least k masks and t ∈T,\nwe denote with qk\nM (t |z) the probability that M\nassigns to t at the kth masked position in z; the\nmodel’s logits before applying softmax are denoted\nwith sk\nM (t |z). We consider the task of mapping\ninputs x ∈X to outputs y ∈Y , for which PET\nrequires a set of pattern-verbalizer pairs (PVPs).\nEach PVP p = (P, v) consists of\n• a pattern P : X →T∗that maps inputs to\ncloze questions containing a single mask;\n• a verbalizer v : Y →T that maps each output\nto a single token representing its task-speciﬁc\nmeaning in the pattern.\nAs illustrated in Figure 2, the core idea of PET\nis to derive the probability of y being the correct\noutput for x from the probability of v(y) being\n2341\nthe “correct” token at the masked position inP(x).\nBased on this intuition, a conditional probability\ndistribution qp of y given x is deﬁned as\nqp(y |x) = exp sp(y |x)∑\ny′∈Y exp sp(y′|x) (1)\nwhere sp(y |x) = s1\nM (v(y) |P(x)) is the raw\nscore of v(y) at the masked position in P(x).\nFor a given task, identifying PVPs that perform\nwell is challenging in the absence of a large devel-\nopment set. Therefore, PET enables a combination\nof multiple PVPs P = {p1, . . . ,pn}as follows:\n1. For each PVP p, a MLM is ﬁnetuned on train-\ning examples (x, y) by minimizing the cross\nentropy between y and qp(y |x). In prac-\ntice, Schick and Schütze (2021) train three\nMLMs per pattern as performance can vary\nsubstantially between runs.\n2. The ensemble of ﬁnetuned MLMs is used to\nannotate a set of unlabeled examples; each un-\nlabeled example x ∈X is annotated with soft\nlabels based on the probability distribution\nqP(y |x) ∝exp\n∑\np∈P\nwp ·sp(y |x) (2)\nsimilar to Eq. 1 where wp is a weighting term\nthat is proportional to the accuracy achieved\nwith p on the training set before training.\n3. The resulting soft-labeled dataset is used to\ntrain a regular sequence classiﬁer by minimiz-\ning cross entropy between its output and qP.\nAs steps (2) and (3) above closely resemble knowl-\nedge distillation (Hinton et al., 2015), we also refer\nto them simply as distillation. Importantly, this\nprocess does not require holding the entire ensem-\nble of MLMs in memory at the same time as each\nmodel’s predictions can be computed sequentially;\ntherefore, it is not more memory expensive than\nusing a single model.\nTo give MLMs trained on different patterns fur-\nther opportunity to learn from one another, Schick\nand Schütze (2021) also propose i PET, an itera-\ntive variant of PET in which several generations of\nmodels are trained on datasets of increasing size\nthat are labeled by previous generations. This is\nachieved as follows: First, an ensemble of MLMs\nis trained as in regular PET. For each model Mi, a\nrandom subset of other models is used to generate\nP2(x)\nAwful pizza! It was __ __ .\nx\nq1\nM (terri |z) <<< q2\nM (•ble |z)\n(a) z =\nAwful pizza! It was __ •ble .\nx\nq1\nM (terri |z′)\n(b) z′=\nFigure 3: Inference for a verbalization consisting of the\ntwo tokens terri and •ble. (a) We ﬁrst compute the prob-\nability of each token at its position in the cloze question\nP2(x) and identify the token with the highest probabil-\nity. (b) We insert this token into the cloze question and\ncompute the probability of the remaining token.\na new training set Ti by assigning labels to those\nunlabeled examples for which the selected subset\nof models is most conﬁdent in its prediction. Each\nMi is then retrained on Ti; this process is repeated\nseveral times, each time increasing the number of\nexamples in Ti by a constant factor. For further\ndetails, we refer to Schick and Schütze (2021).\n3.1 P ET with Multiple Masks\nAn important limitation ofPET is that the verbalizer\nv must map each output to a single token, which\nis impossible for many tasks. We thus generalize\nverbalizers to functions v : Y →T∗; this requires\nsome modiﬁcations to inference and training.3 We\nfurther generalize PET in that we do not assume\nthe output space to be identical for each input: for\neach x ∈X, we denote with Yx ⊆Y the set of\npossible outputs givenx as input. Given a PVPp =\n(P, v), we deﬁne l(x) = maxy∈Yx |v(y)|to be the\nmaximum number of tokens required to express\nany output in Yx and Pk(x) to be P(x) with the\nmask token replaced by k masks.\nAs a running example, we consider the task of bi-\nnary sentiment classiﬁcation for restaurant reviews\nwith labels Y = {+1, −1}. We use the pattern\nP(x) = x. It was __ .and a verbalizer v that maps\n+1 to the single token great and −1 to the sequence\nterri •ble, i.e., we assume that the MLM’s tokenizer\nsplits the word “terrible” into the two tokens terri\nand •ble. For this example, l(x) = 2 for all x;\nP2(x) is illustrated in Figure 3 (a).\n3While PET can easily be adapted to generative MLMs\n(e.g., Lewis et al., 2020; Raffel et al., 2020), we stick with\nregular MLMs as they are more lightweight and performed\nbetter on simple cloze tasks in preliminary experiments.\n2342\nInference For x ∈X, y ∈Yx and |v(y)|= k,\nwe redeﬁne qp(y |x) in an autoregressive fashion:\nStarting from Pk(x), we perform k consecutive\npredictions, where we always select the next token\nto predict based on the MLM’s conﬁdence. That is,\nwe set qp(y |x) = q(v(y) |Pk(x)) where\nq(t1 ... tk|z) =\n{\n1 if k = 0\nqj\nM (tj|z) ·q(t′|z′) if k ≥1 (3)\nwith j = arg maxk\ni=1 qi\nM (ti |z), z′ is z except\nz′\nj = tj and t′= t1 ... tj−1tj+1 ... tk. Note that un-\nlike in original PET (Eq. 1), qp is not a probability\ndistribution as its values do not sum to one.\nFor our sentiment classiﬁcation example, Fig-\nure 3 illustrates how qp(−1 |x) is computed: As\n|v(y)|= |{terri, •ble}|= 2, we ﬁrst use z = P2(x)\nto compute the probability of each token in v(y)\n(Figure 3a). We then choose the token with the\nhighest probability, put it in place of the corre-\nsponding mask token, and use the resulting cloze\nquestion z′to compute the probability of the re-\nmaining token (Figure 3b). The overall score for\ny = −1 is then computed as\nqp(−1 |x) = q2\nM (•ble |z) ·q1\nM (terri |z′)\nTraining Computing qp(y |x) as in Eq. 3 for\neach training example(x, y) would be prohibitively\nexpensive. To enable computation of all required\nprobabilities in a single forward pass, we approx-\nimate qp(y |x) by (i) always inserting the maxi-\nmum number of mask tokens required to express\nany output and (ii) for each y′ ∈Yx, predicting\nall tokens in v(y′) = t1 . . . tk in parallel, where\nwe simply ignore the model’s predictions for all\nl(x) −k superﬂuous mask tokens:\n˜qp(y′|x) =\nk∏\ni=1\nqi\nM (ti |Pl(x)(x)) (4)\nFor our running example, this means we approxi-\nmate the scores qp(y |x) by computing\n˜qp(+1 |x) = q1\nM (great |z)\n˜qp(−1 |x) = q1\nM (terri |z) ·q2\nM (•ble |z)\nwhich can be done in a single forward pass as it\nonly requires processing the cloze question z =\nP2(x) shown in Figure 3 (a) once.\nAs ˜qp is not a probability distribution over Yx,\ncross entropy is not an ideal training objective as it\ncan also be minimized by reducing the probability\nassigned to sequences z /∈v(Yx) that are not part\nof the output space, despite this having no effect on\nthe model’s prediction. We instead opt for multi-\nclass hinge loss (Weston and Watkins, 1999; Dogan\net al., 2016) and minimize:\n∑\ny′∈Yx\nmax\n(\n0; 1−log ˜qp(y|x)+ log ˜qp(y′|x)\n)\n(5)\nThat is, we require the difference between the log\nprobability of y and the log probability of any out-\nput y′∈Yx \\{y}to be at least 1.\n4 Experiments\nWe compare PET and GPT-3 on SuperGLUE\n(Wang et al., 2019), a natural language under-\nstanding benchmark consisting of eight challeng-\ning tasks. We cannot evaluate PET using the exact\nsame training data as GPT-3 because for most tasks,\nGPT-3 uses a different set of training examples for\neach test example and for the other tasks, train-\ning sets were not available upon request; however,\nthe exact choice of examples has little impact on\nGPT-3’s performance.4 We thus create new train-\ning sets by randomly selecting 32 examples for\neach task using a ﬁxed random seed.\nWe additionally create sets of up to 20,000 un-\nlabeled examples for each task; this is done by\nremoving all labels from the original training sets.\nWe refer to the resulting sets of training examples\nand unlabeled examples as FewGLUE.5\n4.1 Tasks\nBelow, we describe each of the SuperGLUE tasks\nand our corresponding PVPs. We use a vertical\nbar (|) to mark boundaries between text segments.\nOf the eight tasks considered, only COPA, WSC\nand ReCoRD require the use of PET with multiple\nmasks as introduced in Section 3.1.\nBoolQ (Clark et al., 2019) is a QA task where\neach example consists of a passage p and a yes/no\nquestion q. We use the following patterns:\n• p. Question: q? Answer: __.\n• p. Based on the previous passage,q? __.\n• Based on the following passage,q? __. p\n4Based on personal correspondence with the authors.\n5FewGLUE is publicly available at https://github.\ncom/timoschick/fewglue.\n2343\nWe deﬁne two verbalizers mapping questions\ncontaining a true statement to yes/true and others\nto no/false, respectively, for a total of 6 PVPs.\nCB (De Marneffe et al., 2019) and RTE (Dagan\net al., 2006) are textual entailment tasks like MNLI,\nso we use PVPs similar to Schick and Schütze\n(2021). For a premise p and hypothesis h, we use\nh? |__, p , “h”?|__, “p” , h? |__. p , “h”?|__. “p”\nand a verbalizer that maps entailment to yes,\ndisagreement to no and neutral to maybe.\nGiven a premise p, the task in COPA (Gordon\net al., 2012) is to determine the cause or effect of\nthe premise given two options c1 and c2. For deter-\nmining the effect, we use the following patterns:\n“c1” or “c2”? p, so __., c1 or c2? p, so __.\nFor determining the cause, we use the same pat-\nterns but replace so with because. The verbalizer\nfor c1 and c2 is the identity function.\nFor WiC (Pilehvar and Camacho-Collados, 2019),\ngiven a word w and two sentences s1 and s2 in\nwhich it occurs, the task is to decide if w is used\nwith the same sense in both sentences. We use:\n• “s1” / “s2”. Similar sense of “w”? __.\n• s1 s2 Does w have the same meaning in both\nsentences? __\n• w. Sense (1) (a) “s1” (__) “s2”\nFor the ﬁrst two patterns, we use yes as verbaliza-\ntion for words used in the same sense and no for\nother words; for the third pattern, we use b and 2.\nFor WSC (Levesque et al., 2011), each example\nconsists of a sentence s with a marked pronoun p\nand noun n, and the task is to determine whether p\nrefers to n. We follow (Raffel et al., 2020; Brown\net al., 2020) and treat WSC as a generative task.\nWe highlight p in s by putting it in asterisks and\nuse the following patterns:\n• s The pronoun ‘∗p∗’ refers to __.\n• s In the previous sentence, the pronoun ‘∗p∗’\nrefers to __.\n• s In the passage above, what does the pronoun\n‘∗p∗’ refer to? Answer: __.\nWe use the identity function as verbalizer for\nn. Note that WSC is different from other tasks\nin that it requires free-form completion. This in\nturn requires some modiﬁcations during train-\ning and inference that are discussed in Appendix A.\nMultiRC (Khashabi et al., 2018) is a QA task.\nGiven a passage p, a question q and an answer\ncandidate a, the task is to decide whether a is a\ncorrect answer for q. We use the same verbalizer\nas for BoolQ and similar patterns:\n• p. Question: q? Is ita? __.\n• p. Question: q? Is the correct answer “a”? __.\n• p. Based on the previous passage,q? Is “a” a\ncorrect answer? __.\nFor ReCoRD (Zhang et al., 2018), given a passage\np and a cloze question q, the task is to decide which\nof a given set of answer candidates is the correct re-\nplacement for the placeholder in the cloze question.\nAs this task is already presented in the form of a\ncloze question, there is little room for designing\nPVPs, so we only use a trivial one: the concatena-\ntion of p and q as pattern and the identity function\nas verbalizer. With only one PVP, there is no need\nto perform knowledge distillation so we directly\nuse the resulting model as our ﬁnal classiﬁer.\n4.2 Setup\nAs underlying LM for PET we choose ALBERT-\nxxlarge-v2 (Lan et al., 2020), the best-performing\nMLM on SuperGLUE when training is performed\non the regular, full size training sets. We use the\nsame model, supplemented by a sequence classi-\nﬁcation head, as our ﬁnal classiﬁer. We run PET\non the FewGLUE training sets for all SuperGLUE\ntasks. We do not use any development set to op-\ntimize hyperparameters; instead we use the exact\nsame setup and hyperparameters as Schick and\nSchütze (2021). For COPA, WSC and ReCoRD,\nwe use our proposed modiﬁcation of PET to sup-\nport verbalizers mapping labels to multiple tokens;\nfor all other tasks, we use regular PET. We train\niPET on all tasks except COPA and WSC, as their\nunlabeled sets contain well below 1,000 examples,\nas well as ReCoRD, for which iPET makes no sense\nas we only use a single PVP. For these three tasks,\nwe simply reuse the results of regular PET.\n4.3 Results\nOur main results are shown in Table 1. As can be\nseen, ALBERT with PET performs similar to the\nlargest GPT-3 model, which is larger by a factor\n2344\nParams BoolQ CB COPA RTE WiC WSC MultiRC ReCoRD Avg\nModel (M) Acc. Acc. / F1 Acc. Acc. Acc. Acc. EM / F1a Acc. / F1 –\ndev\nGPT-3 Small 125 43.1 42.9 / 26.1 67.0 52.3 49.8 58.7 6.1 / 45.0 69.8 / 70.7 50.1\nGPT-3 Med 350 60.6 58.9 / 40.4 64.0 48.4 55.0 60.6 11.8 / 55.9 77.2 / 77.9 56.2\nGPT-3 Large 760 62.0 53.6 / 32.6 72.0 46.9 53.0 54.8 16.8 / 64.2 81.3 / 82.1 56.8\nGPT-3 XL 1,300 64.1 69.6 / 48.3 77.0 50.9 53.0 49.0 20.8 / 65.4 83.1 / 84.0 60.0\nGPT-3 2.7B 2,700 70.3 67.9 / 45.7 83.0 56.3 51.6 62.5 24.7 / 69.5 86.6 / 87.5 64.3\nGPT-3 6.7B 6,700 70.0 60.7 / 44.6 83.0 49.5 53.1 67.3 23.8 / 66.4 87.9 / 88.8 63.6\nGPT-3 13B 13,000 70.2 66.1 / 46.0 86.0 60.6 51.1 75.0 25.0 / 69.3 88.9 / 89.8 66.9\nGPT-3 175,000 77.5 82.1 / 57.2 92.0 72.9 55.3 75.0 32.5 / 74.8 89.0 / 90.1 73.2\nPET 223 79.4 85.1 / 59.4 95.0 69.8 52.4 80.1 37.9 / 77.3 86.0 / 86.5 74.1\niPET 223 80.6 92.9 / 92.4 95.0 74.0 52.2 80.1 33.0 / 74.0 86.0 / 86.5 76.8\ntest\nGPT-3 175,000 76.4 75.6 / 52.0 92.0 69.0 49.4 80.1 30.5 / 75.4 90.2 / 91.1 71.8\nPET 223 79.1 87.2 / 60.2 90.8 67.2 50.7 88.4 36.4 / 76.6 85.4 / 85.9 74.0\niPET 223 81.2 88.8 / 79.9 90.8 70.8 49.3 88.4 31.7 / 74.1 85.4 / 85.9 75.4\nSotA 11,000 91.2 93.9 / 96.8 94.8 92.5 76.9 93.8 88.1 / 63.3 94.1 / 93.4 89.3\nTable 1: Results on SuperGLUE for GPT-3 primed with 32 randomly selected examples and for P ET / iPET with\nALBERT-xxlarge-v2 after training on FewGLUE. State-of-the-art results when using the regular, full size training\nsets for all tasks (Raffel et al., 2020) are shown in italics.\nof 785. On average, PET performs 18 points bet-\nter compared to GPT-3 Med, a model of similar\nsize. i PET brings further improvements for 3 out\nof the 5 tasks that we use i PET for, most notably\nfor CB, but results in a slight performance drop\nfor MultiRC. Despite PET’s strong performance, it\nstill clearly performs worse than a state-of-the-art\nmodel trained on the regular, full size SuperGLUE\ntraining set.\n5 Analysis\nWe investigate the importance of several factors\nfor few-shot performance: the choice of patterns\nand verbalizers, the usage of both unlabeled and\nlabeled data, and properties of the underlying lan-\nguage model. We also look into our proposed mod-\niﬁcation for PET to work with multiple masks and\ncompare it to various baselines. Finally, we mea-\nsure how choosing different sets of training exam-\nples affects performance. Our analysis focuses on\nPET as GPT-3 is not publicly available.6\n5.1 Patterns\nThe way in which tasks are reformulated as cloze\nquestions can have a huge impact on performance\n(Jiang et al., 2020; Schick and Schütze, 2021).\nThese reformulations can be arbitrarily complex;\nfor example, the pattern used by GPT-3 for WSC\ncontains an introductory section of almost 30\nwords; it is unclear if and how this formulation\nhas been optimized.7 To investigate the importance\n6We could not obtain access to OpenAI’s GPT-3 API.\n7While the authors use a different terminology, GPT-3 also\nmakes use of PVPs (Brown et al., 2020, pp. 50–61).\nof patterns and verbalizers, we compare three sets\nof PVPs: our initial set as deﬁned in Section 4.1\n(denoted pours), the single PVP used by GPT-3\n(pGPT-3), and the combination of both (pcomb).\nWe train ALBERT using PET with all three sets\nof patterns; results for selected SuperGLUE tasks\nare shown in Table 2 (top). As can be seen, the\nPVP used by GPT-3 outperforms our PVPs on\nRTE whereas our initial set of patterns performs\nmuch better on MultiRC. These large differences\nin performance highlight the importance of ﬁnd-\ning good ways to express tasks as cloze questions.\nAs it is difﬁcult to ascertain which patterns per-\nform well without trying them on a large set of\nexamples, a key challenge for few-shot approaches\nis to compensate for PVPs that the LM fails to\nunderstand well. As seen in the performance of\nthe model trained with pcomb, PET is able to do\nso: not only does combining all PVPs compensate\nfor the worse performance of pours on RTE and of\npGPT-3 on MultiRC, it even further improves aver-\nage performance across the three tasks compared\nto the best-performing set of patterns. This clearly\ndemonstrates the potential of carefully engineer-\ning a set of suitable patterns as opposed to just\nchoosing a single formulation without means of\nevaluating its effectiveness.\n5.2 Unlabeled Data Usage\nUnlike GPT-3, PET requires unlabeled data to dis-\ntill the knowledge of all models based on individual\nPVPs into a single classiﬁer; for i PET, unlabeled\ndata is additionally used to generate training sets\nfor future generations. The underlying assumption\n2345\nCB RTE MultiRC Avg\nModel Acc. / F1 Acc. EM / F1a –\nPET (pours) 85.1 / 59.4 69.8 37.9 / 77.3 66.6\nPET (pGPT-3) 83.3 / 58.1 71.8 25.4 / 68.3 63.1\nPET (pcomb) 84.5 / 59.0 74.7 39.1 / 77.7 68.3\nPET (pours) ¬dist 83.9 / 76.2 66.4 38.9 / 76.2 68.0\nPET (pcomb) ¬dist 83.9 / 76.2 72.9 39.6 / 76.6 70.4\nTable 2: Results on selected tasks for various sets of\nPVPs for regular PET and for an ensemble of PET mod-\nels with no knowledge distillation (“¬dist”)\n1 2 3 dist.\n60\n70\n80\n90\niPET Generation\nTask Performance BoolQ CB (Acc)\nRTE MultiRC (F1a)\nFigure 4: Average performance ( ± standard devia-\ntion) of all MLMs trained on individual patterns for\nthree generations and of the distilled classiﬁer (“dist.”)\nacross three individual training runs\nis that unlabeled data can easily be obtained, which\nmay not always be the case in real-world settings.\nWe thus investigate the importance of unlabeled\ndata for regular PET. To this end, we compare\nthe performance of the ﬁnal classiﬁer in PET to\nthat of directly using the ensemble of models cor-\nresponding to individual PVPs. While using this\nensemble entirely removes the need for unlabeled\ndata, the ensemble for k PVPs is larger than the\ndistilled model by a factor of 3 ·k as we follow the\ndefault setting of PET and train three models per\nPVP. However, even for a large number of PVPs\nthe ensemble is smaller than GPT-3 by two orders\nof magnitude.\nResults without distillation can be seen in Ta-\nble 2 (bottom). Averaged across the three tasks, the\nensemble performs even better than the distilled\nclassiﬁer. This shows that if the goal is only to\nachieve good performance, then unlabeled data is\nnot necessary; however, it is required to obtain a\nsingle, lightweight model as ﬁnal classiﬁer.\nFigure 4 illustrates the beneﬁt of training mul-\ntiple generations with i PET. For all tasks except\nMultiRC, there are substantial improvements from\nCB RTE MultiRC Avg\nModel Acc. / F1 Acc. EM / F1a –\nPET 85.1 / 59.4 69.8 37.9 / 77.3 66.6\nunsupervised 33.5 / 23.1 55.0 3.9 / 60.3 38.5\nsupervised 60.7 / 42.5 50.2 4.3 / 49.8 43.0\nPET (XLNet) 88.7 / 83.0 60.4 21.4 / 66.6 63.4\nPriming (XLNet) 56.3 / 37.7 49.5 – / – –\nTable 3: Results on selected tasks for various ways of\nusing the labeled examples available in FewGLUE\nthe ﬁrst to the second generation, whereas the third\ngeneration achieves only slight additional improve-\nments. On average, standard deviation is reduced\nin later generations, illustrating that the models\nlearn from each other and their predictions con-\nverge. The ﬁnal distillation step brings further im-\nprovements for all tasks except MultiRC and re-\nduces standard deviation across three training runs\nto almost zero, illustrating that PET and iPET are\neffective means of reducing ﬁnetuning instability\n(Dodge et al., 2020).\nOf course, there are further ways to lever-\nage unlabeled data such as keeping an auxiliary\nlanguage modeling objective during ﬁnetuning\n(Chronopoulou et al., 2019). While we leave in-\nvestigating the impact of additionally using such\nmethods to future work, we note that they can easily\nbe applied to PET while there is no straightforward\nway to combine them with priming.\n5.3 Labeled Data Usage\nWe next investigate the effect of how labeled data is\nused, which is one of the key differences between\npriming and PET. We ﬁrst compare PET with reg-\nular supervised training (i.e., without using any\npatterns), and with a fully unsupervised model (i.e.,\nan ensemble using all PVPs but no labeled train-\ning examples). Given 32 examples, PET clearly\noutperforms both baselines (Table 3).\nWe next compare PET directly to priming. How-\never, we cannot do so using ALBERT as it is only\nable to process sequences of up to 512 tokens,\nwhich is not enough for a set of 32 examples; we\ninstead use XLNet (Yang et al., 2019) for this com-\nparison. As shown in Table 3, XLNet in general\nperforms worse than ALBERT. More importantly,\nXLNet with PET performs much better than prim-\ning. We were not able to obtain results with priming\non MultiRC because the 32 examples in FewGLUE\nwould require more than 10,000 tokens, so process-\ning them with a standard Transformer (Vaswani\n2346\nBoolQCB AccCB F1COPARTEWiCWSC\nMultiRC EMMultiRC F1aReCoRD AccReCoRD F1\nPET\n175B\n13B\n6.7B\n2.7B\nXL\nLarge\nMed\nSmall\n-30\n-20\n-10\n±0\n+10\n+20\n+30\nFigure 5: Accuracy differences between priming with\n32 examples and one-shot priming for all GPT-3 mod-\nels as well as between ALBERT with PET (without dis-\ntillation) and unsupervised ALBERT (bottom row)\net al., 2017) is infeasible due to the quadratic com-\nplexity of self-attention. This highlights another\nimportant issue with priming: It does not scale well\nto more than a few examples; even GPT-3 is only\nable to process sequences of up to 2,048 tokens.\nWhile there are some Transformer variants that can\ndeal with much longer contexts (e.g., Kitaev et al.,\n2020; Beltagy et al., 2020), it has yet to be investi-\ngated to what extent such models make good use\nof priming examples over long context spans.\nWe further investigate the effectiveness of prim-\ning by looking at results obtained with GPT-3 more\nclosely. To this end, Figure 5 shows the perfor-\nmance difference between priming GPT-3 with 32\nexamples and priming it with just a single exam-\nple for each task and model size.8 As can be seen,\npriming with 32 examples only slightly improves\nperformance for most tasks and model sizes. For\nsome tasks, adding more examples even leads to\nworse performance, especially for smaller models.\nFor ReCoRD, even the largest model’s performance\nslightly drops when adding more examples.\nThe bottom row of Figure 5 shows the perfor-\nmance difference between ALBERT trained with\nPET (without distillation) and a fully unsupervised\nALBERT model on all tasks. While results are\nnot directly comparable due to different underlying\nmodels and PVPs, PET results in much stronger\nperformance improvements compared to priming\nand does not worsen results for any task.\n8We do not compare priming to zero-shot performance as\nfor unknown reasons, zero-shot GPT-3 performs well below\nrandom guessing for some tasks (e.g., 0.0% accuracy for WiC).\nTo not overestimate the beneﬁt of priming, we therefore show\ngains from providing 32 examples compared to just one.\nCB RTE MultiRC Avg\nModel Params Acc. / F1 Acc. EM / F1a –\nALBERT 223M 87.5 / 78.7 74.7 38.9 / 76.2 71.8\nRoBERTa 355M 85.7 / 77.5 62.8 23.3 / 70.0 63.7\nGPT-2 345M 73.2 / 73.7 47.7 12.4 / 57.4 52.0\nTable 4: Results on selected tasks for P ET without\nknowledge distillation combined with various LMs us-\ning pGPT-3 for CB/RTE and pours for MultiRC\n5.4 Model Type\nWe next look into the impact of the underlying LM\non PET by comparing ALBERT with RoBERTa\nlarge (Liu et al., 2019) and GPT-2 medium (Rad-\nford et al., 2019). As GPT-2 is a unidirectional\nmodel similar to GPT-3, it can only process pat-\nterns where the mask token is the very last to-\nken. We therefore use pGPT-3 for CB and RTE;\nfor MultiRC, we stick with our original set of pat-\nterns as they already fulﬁll this requirement. We\nalso do not perform distillation and instead report\nthe ensemble’s performance as there is no estab-\nlished way of equipping GPT-2 with a sequence\nclassiﬁcation head.\nResults for training all three LMs with PET in\nTable 4 show that using ALBERT as underlying\nLM is crucial for PET’s strong performance; ex-\nchanging ALBERT with RoBERTa results in an\naverage performance drop of 8 points. However,\nRoBERTa still clearly outperforms GPT-3 13B,\nwhich is larger by two orders of magnitude. Im-\nportantly, PET with GPT-2 performs much worse\nthan with the two other models. As anticipated by\nBrown et al. (2020), a reason for this drop in per-\nformance may be that like GPT-3, GPT-2 is unidi-\nrectional, making tasks that require comparing two\nsequences a challenge. However, it is important\nto note that there are also other substantial differ-\nences between GPT-2 and the other two models,\nmost notably the pretraining dataset. Regardless of\nwhether unidirectionality is the reason for GPT-2’s\nbad performance, bidirectionality of the underlying\nLM is important for PET as it removes the need for\nthe mask token to be at the very end and thus allows\nfor more ﬂexibility in the creation of patterns.\n5.5 P ET with Multiple Masks\nWe modiﬁed PET to work for outputs that require\nmore than a single token. To investigate the impact\nof this modiﬁcation, we look at the three tasks for\nwhich this is required: COPA, WSC and ReCoRD.\nWe compare our decoding strategy of predicting to-\n2347\nCOPA WSC ReCoRD Avg\nModel Acc. Acc. Acc. / F1 –\nPET 95.0 80.1 86.0 / 86.5 87.1\nPET ¬dist (max-ﬁrst) 90.0 80.8 86.0 / 86.5 85.7\nPET ¬dist (ltr) 89.0 79.8 84.7 / 85.3 84.6\nPET ¬dist (parallel) 77.0 80.8 82.5 / 83.1 80.2\nuntrained 72.5 59.9 84.7 / 85.4 72.5\nTable 5: Results on selected tasks for our proposed vari-\nant of PET as well as other decoding strategies and for\nuntrained ALBERT\nkens in order of the probability assigned to them, to\nwhich we refer as max-ﬁrst, with two alternatives:\ndecoding left-to-right (ltr) as is common for many\nautoregressive language models, and decoding all\ntokens simultaneously (parallel) as is done during\ntraining. Additionally, we compare PET with un-\ntrained ALBERT to measure the effectiveness of\nour proposed training loss.\nResults are shown in Table 5. PET clearly out-\nperforms untrained ALBERT for the three tasks.\nNot performing distillation hurts performance for\nCOPA, but leads to slight improvements on WSC;\nfor ReCoRD, we did not perform distillation in the\nﬁrst place as we only use a single PVP. Our decod-\ning strategy is clearly superior to parallel decoding\nexcept for WSC, for which most predictions consist\nonly of one or two tokens, and performs slightly\nbetter than left-to-right decoding.\n5.6 Training Examples\nRecall that we conduct our experiments with train-\ning examples from FewGLUE, a randomly selected\nsubset of the original SuperGLUE training exam-\nples. We used a ﬁxed random seed s0 to generate\nFewGLUE. Let Σi be the randomly selected sub-\nset of SuperGLUE for random seed si, so Σ0 =\nFewGLUE. In this subsection, we create two ad-\nditional subsets of SuperGLUE, Σ1 and Σ2, based\non different seeds. This allows us to investigate\nhow different sets of training examples affect per-\nformance. To this end, we run PET for CB, RTE\nand MultiRC using the three Σi. To measure only\nthe effect of varying the training set while ignoring\nunlabeled examples, we do not use distillation.\nTable 6 shows that for all tasks, changing the\nset of training examples can result in large per-\nformance differences for PET. This highlights the\nimportance of using the same set of examples when\ncomparing different few-shot approaches, which\nis why we make the particular set of examples in\nFewGLUE publicly available. However, we note\nCB RTE MultiRC Avg\nModel Acc. / F1 Acc. EM / F1a –\nGPT-3 82.1 / 57.2 72.9 32.5 / 74.8 65.4\nPET ¬dist (Σ0) 83.9 / 76.2 66.4 38.9 / 76.2 68.0\nPET ¬dist (Σ1) 82.1 / 57.4 61.4 39.2 / 77.9 63.2\nPET ¬dist (Σ2) 87.5 / 84.0 61.4 34.7 / 76.3 67.6\nTable 6: Results on selected tasks for GPT-3 and for\nPET using training sets Σ0, Σ1, Σ2\nthat the average performance of PET is similar to\nthat of GPT-3 for all seeds.\nWhile our results may seem contrary to the in-\nsight that for GPT-3, the exact choice of examples\ndoes not play a major role, we suspect this to be\ndue to the fact that priming beneﬁts much less from\ntraining examples than PET (cf. Section 5.3); ac-\ncordingly, the inﬂuence of the exact set of training\nexamples on the model’s performance is smaller.\n6 Conclusion\nWe have proposed a simple yet effective modiﬁ-\ncation of PET, enabling us to use it for tasks that\nrequire predicting multiple tokens. In extensive\nexperiments, we have identiﬁed several factors re-\nsponsible for the strong performance of PET com-\nbined with ALBERT: the possibility to concurrently\nuse multiple patterns for transforming examples\ninto cloze questions, the ability to compensate for\npatterns that are difﬁcult to understand, the usage\nof labeled data to perform parameter updates, and\nthe underlying LM itself.\nWe have shown that using PET, it is possible to\nachieve few-shot text classiﬁcation performance\nsimilar to GPT-3 on SuperGLUE with LMs that\nhave three orders of magnitude fewer parameters.\nThis not only lowers ﬁnancial cost, but above all\nreduces environmental impact immensely and leads\nto a much smaller carbon footprint. We see this as\nan important contribution to achieving the goal of\nan environmentally more friendly NLP. To enable\ncomparisons with our work, we make our code,\nmodels and datasets publicly available.\nFor future work, it would be interesting to see\nwhether PET also works for generative tasks when\ncombined with generative LMs and whether further\nimprovements are possible in multi-task settings.\nAcknowledgments This work was funded by the\nEuropean Research Council (ERC #740516). We\nthank the anonymous reviewers for their helpful\ncomments.\n2348\nReferences\nMark Anderson and Carlos Gómez-Rodríguez. 2020.\nDistilling neural networks for greener and faster de-\npendency parsing. In Proceedings of the 16th In-\nternational Conference on Parsing Technologies and\nthe IWPT 2020 Shared Task on Parsing into En-\nhanced Universal Dependencies , pages 2–13, On-\nline. Association for Computational Linguistics.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. Com-\nputing Research Repository, arXiv:2004.05150.\nSergey Brin. 1999. Extracting patterns and relations\nfrom the world wide web. In The World Wide Web\nand Databases, pages 172–183, Berlin, Heidelberg.\nSpringer Berlin Heidelberg.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nAlexandra Chronopoulou, Christos Baziotis, and\nAlexandros Potamianos. 2019. An embarrassingly\nsimple approach for transfer learning from pre-\ntrained language models. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2089–2095, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifﬁculty of natural yes/no questions. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2924–2936, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The PASCAL recognising textual entailment\nchallenge. In Machine learning challenges. evalu-\nating predictive uncertainty, visual object classiﬁca-\ntion, and recognising tectual entailment, pages 177–\n190. Springer.\nJoe Davison, Joshua Feldman, and Alexander Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1173–1178, Hong Kong, China. As-\nsociation for Computational Linguistics.\nMarie-Catherine De Marneffe, Mandy Simons, and\nJudith Tonhauser. 2019. The CommitmentBank:\nInvestigating projection in naturally occurring dis-\ncourse. In Proceedings of Sinn und Bedeutung 23.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language mod-\nels: Weight initializations, data orders, and\nearly stopping. Computing Research Repository ,\narXiv:2002.06305.\nÜrün Dogan, Tobias Glasmachers, and Christian Igel.\n2016. A uniﬁed view on multi-class support vector\nclassiﬁcation. J. Mach. Learn. Res., 17(45):1–32.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 6112–\n6121, Hong Kong, China. Association for Computa-\ntional Linguistics.\nYunchao Gong, Liu Liu, Ming Yang, and Lubomir\nBourdev. 2014. Compressing deep convolutional\nnetworks using vector quantization. Computing Re-\nsearch Repository, arXiv:1412.6115.\nAndrew Gordon, Zornitsa Kozareva, and Melissa\nRoemmele. 2012. SemEval-2012 task 7: Choice\nof plausible alternatives: An evaluation of common-\nsense causal reasoning. In *SEM 2012: The First\nJoint Conference on Lexical and Computational Se-\nmantics – Volume 1: Proceedings of the main con-\nference and the shared task, and Volume 2: Pro-\nceedings of the Sixth International Workshop on Se-\nmantic Evaluation (SemEval 2012), pages 394–398,\nMontréal, Canada. Association for Computational\nLinguistics.\nSong Han, Huizi Mao, and William J Dally. 2016.\nDeep compression: Compressing deep neural net-\n2349\nworks with pruning, trained quantization and huff-\nman coding. International Conference on Learning\nRepresentations (ICLR).\nSong Han, Jeff Pool, John Tran, and William Dally.\n2015. Learning both weights and connections for\nefﬁcient neural network. In Advances in Neural In-\nformation Processing Systems , volume 28. Curran\nAssociates, Inc.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. Com-\nputing Research Repository, arXiv:1503.02531.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2020. TinyBERT: Distilling BERT for natural lan-\nguage understanding. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4163–4174, Online. Association for Computational\nLinguistics.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking be-\nyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 252–262, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In Inter-\nnational Conference on Learning Representations.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In Interna-\ntional Conference on Learning Representations.\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The Winograd schema challenge. In\nAAAI Spring Symposium: Logical Formalizations of\nCommonsense Reasoning, volume 46, page 47.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nWeijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao,\nHaotang Deng, and Qi Ju. 2020. FastBERT: a self-\ndistilling BERT with adaptive inference time. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 6035–\n6044, Online. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\n2019. RoBERTa: A robustly optimized BERT pre-\ntraining approach. Computing Research Repository,\narXiv:1907.11692.\nYihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang,\nYang Wang, Quanlu Zhang, Yaming Yang, Yunhai\nTong, and Jing Bai. 2020. LadaBERT: Lightweight\nadaptation of BERT through hybrid model compres-\nsion. In Proceedings of the 28th International Con-\nference on Computational Linguistics , pages 3225–\n3234, Barcelona, Spain (Online). International Com-\nmittee on Computational Linguistics.\nDavid McClosky, Eugene Charniak, and Mark Johnson.\n2006. Effective self-training for parsing. In Pro-\nceedings of the Human Language Technology Con-\nference of the NAACL, Main Conference, pages 152–\n159, New York City, USA. Association for Compu-\ntational Linguistics.\nJuri Opitz. 2019. Argumentative relation classiﬁcation\nas plausibility ranking. In Preliminary proceedings\nof the 15th Conference on Natural Language Pro-\ncessing (KONVENS 2019): Long Papers, pages 193–\n202, Erlangen, Germany. German Society for Com-\nputational Linguistics & Language Technology.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming\nLin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in PyTorch.\nIn NIPS Autodiff Workshop.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP).\nMohammad Taher Pilehvar and Jose Camacho-\nCollados. 2019. WiC: the word-in-context dataset\nfor evaluating context-sensitive meaning represen-\ntations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 1267–1273, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRaul Puri and Bryan Catanzaro. 2019. Zero-shot\ntext classiﬁcation with generative language models.\nComputing Research Repository, arXiv:1912.10165.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\n2350\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 2699–2712, Online. Association for Compu-\ntational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled ver-\nsion of BERT: smaller, faster, cheaper and lighter.\nIn Proceedings of the 5th Workshop on Energy Ef-\nﬁcient Machine Learning and Cognitive Computing,\nNeurIPS 2019.\nVictor Sanh, Thomas Wolf, and Alexander Rush.\n2020. Movement pruning: Adaptive sparsity by ﬁne-\ntuning. In Advances in Neural Information Process-\ning Systems, volume 33, pages 20378–20389. Cur-\nran Associates, Inc.\nTimo Schick and Hinrich Schütze. 2020. Rare words:\nA major problem for contextualized embeddings and\nhow to ﬁx it by attentive mimicking. In Proceedings\nof the Thirty-Fourth AAAI Conference on Artiﬁcial\nIntelligence.\nTimo Schick and Hinrich Schütze. 2021. Exploit-\ning cloze questions for few shot text classiﬁcation\nand natural language inference. In Proceedings of\nthe 16th Conference of the European Chapter of\nthe Association for Computational Linguistics, Kyiv,\nUkraine (Online). International Committee on Com-\nputational Linguistics.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and\nOren Etzioni. 2020a. Green AI. Commun. ACM,\n63(12):54–63.\nRoy Schwartz, Gabriel Stanovsky, Swabha\nSwayamdipta, Jesse Dodge, and Noah A. Smith.\n2020b. The right tool for the job: Matching model\nand instance complexities. In Proceedings of the\n58th Annual Meeting of the Association for Com-\nputational Linguistics , pages 6640–6651, Online.\nAssociation for Computational Linguistics.\nH Scudder. 1965. Probability of error of some adap-\ntive pattern-recognition machines. IEEE Transac-\ntions on Information Theory, 11(3):363–371.\nPierre Stock, Angela Fan, Benjamin Graham, Edouard\nGrave, Rémi Gribonval, Herve Jegou, and Armand\nJoulin. 2021. Training with quantization noise for\nextreme model compression. In International Con-\nference on Learning Representations.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Florence, Italy.\nAssociation for Computational Linguistics.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. oLMpics – on what lan-\nguage model pre-training captures. Transactions\nof the Association for Computational Linguistics ,\n8:743–758.\nTrieu H. Trinh and Quoc V . Le. 2018. A simple method\nfor commonsense reasoning. Computing Research\nRepository, arXiv:1806.02847.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nAlex Wang and Kyunghyun Cho. 2019. BERT has\na mouth, and it must speak: BERT as a Markov\nrandom ﬁeld language model. In Proceedings of\nthe Workshop on Methods for Optimizing and Eval-\nuating Neural Language Generation , pages 30–36,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019. SuperGLUE: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems, volume 32. Curran As-\nsociates, Inc.\nJason Weston and Chris Watkins. 1999. Support vec-\ntor machines for multi-class pattern recognition. In\nESANN, volume 99, pages 219–224.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nJi Xin, Rodrigo Nogueira, Yaoliang Yu, and Jimmy Lin.\n2020. Early exiting BERT for efﬁcient document\nranking. In Proceedings of SustaiNLP: Workshop on\nSimple and Efﬁcient Natural Language Processing ,\npages 83–88, Online. Association for Computational\nLinguistics.\n2351\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. In H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d'Alché-Buc,\nE. Fox, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 32, pages 5753–\n5763. Curran Associates, Inc.\nDavid Yarowsky. 1995. Unsupervised word sense dis-\nambiguation rivaling supervised methods. In 33rd\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 189–196, Cambridge, Mas-\nsachusetts, USA. Association for Computational\nLinguistics.\nOﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8BERT: Quantized 8bit BERT.\nIn NeurIPS EMC2 Workshop.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nReCoRD: Bridging the gap between human and ma-\nchine commonsense reading comprehension. Com-\nputing Research Repository, arXiv:1810.12885.\nA Training Details\nOur implementation can be found in the supple-\nmentary material. It extends the original implemen-\ntation of PET by Schick and Schütze (2021) which,\nin turn, is based on the Transformers library (Wolf\net al., 2020) and PyTorch (Paszke et al., 2017). All\ndependencies are listed in requirements.txt.\nDetailed instructions on how our results can be re-\nproduced using this implementation can be found\nin README.md.\nUnless explicitly stated differently, we use the\nexact same set of hyperparameters as Schick and\nSchütze (2021) (Table 7) with the only difference\nthat for iPET, we only train 3 generations of models\nto speed up training. All of our experiments were\nconducted using a single GPU with 11GB RAM\n(NVIDIA GeForce GTX 1080 Ti). With this GPU,\ntraining a single PET model for 250 steps took ap-\nproximately 45 minutes. Depending on the task,\nlabeling unlabeled examples took 0.2–1.5 hours per\nmodel. Training the ﬁnal classiﬁer for 5,000 steps\non the soft-labeled dataset took 2.5 hours on aver-\nage. Below, we list task-speciﬁc implementation\ndetails for all tasks in SuperGLUE.\nCOPA For COPA, we randomly switch the two\noptions c1 and c2 during training with a probabil-\nity of 50% to make the input more diverse; for\ninference, we always keep the original order. For\ndistilling the ﬁnal PET model, we obtain logits for\nunlabeled examples x from individual PVPs p as\nsp(y |x) = log qp(y |x); we use the input format\nproposed by Liu et al. (2019).\nWiC Similar to COPA, we randomly switch the\ninput sentences s1 and s2 during training. Given\na word w and two sentences s1 and s2, we use the\nsequence w: s1 |s2 as input for the ﬁnal sequence\nclassiﬁcation model, where |marks the boundary\nbetween two text segments.\nWSC Unlike other SuperGLUE tasks, the WSC\nformulation of Raffel et al. (2020) and Brown et al.\n(2020) requires free-form completion, meaning that\nfor each sentence s and pronoun p, we only have\na single correct choice n that the model needs to\npredict, but we do not provide any alternatives.\nDuring training, we thus use regular cross entropy\nloss between n and ˜qp(n |s, p) as deﬁned in Eq. 4.\nHowever, in many cases this would allow the LM\nto easily identify the correct target based on the\nnumber of masks provided, so we modify each\ntarget by randomly adding up to three additional\nmask tokens, for which we require the model to\npredict a special <pad> token. For inference, we\nalways just add a single mask token to ensure con-\nsistent results across multiple evaluations and per-\nform greedy decoding as described in Section 3.\nWe then follow Raffel et al. (2020) to map the out-\nput produced by the LM to a labely ∈{true, false}.\nFor distillation, given an unlabeled example x we\nset sp(y |x) = 1 if the model’s output forx was\nmapped to y and sp(y |x) = 0 otherwise. We\nprovide inputs to the ﬁnal PET model in the for-\nmat s |n where |is the boundary between two text\nsegments and mark p in s with asterisks.\nMultiRC Deviating from the hyperparameters\nused by Schick and Schütze (2021), we use a maxi-\nmum sequence length of 512 tokens for MultiRC\nboth during training and inference because we\nfound many passages to be much longer than 256\ntokens. Input for the ﬁnal sequence classiﬁcation\nmodel is of the formp |q |a where p is the passage,\nq is the question, a is the answer candidate and we\nuse |to mark boundaries between text segments.\nReCoRD For ReCoRD, we again use a maxi-\nmum sequence length of 512 because many pas-\nsages require more than 256 tokens. For some ques-\ntions q, the ReCoRD training set contains a huge\nnumber of answer candidates. To facilitate train-\ning, we split each example into multiple examples\nas follows: let C be the set of answer candidates\n2352\nParameter Value\nadam_epsilon 1e-8\ngradient_accumulation_steps 8\nlearning_rate 1e-5\nmax_grad_norm 1.0\nmax_seq_length 256\npet_max_steps 250\nsc_max_steps 5,000\nper_gpu_train_batch_size 2\ndistillation_temperature 2\nweight_decay 0.01\nTable 7: Hyperparameters for P ET from Schick and\nSchütze (2021)\nDataset Metrics |Unlabeled| | Dev| | Test|\nBoolQ Acc. 9,427 3,270 3,245\nCB Acc./F1 20,000 57 250\nCOPA Acc. 400 100 500\nMultiRC F1 a/EM 5,100 953 1,800\nReCoRD F1/EM 20,000 10,000 10,000\nRTE Acc. 20,000 278 300\nWiC Acc. 6,000 638 1,400\nWSC Acc. 554 104 146\nTable 8: Important statistics for all datasets used\nwith C+ ⊂C being the set of correct answers. We\ncreate a training example for each c ∈C+ by ran-\ndomly selecting up to 9 negative examples from\nC \\C+ for a total of 10 answer candidates.\nB Dataset Details\nFor each task and number of examples t, we create\nthe FewGLUE training set T by shufﬂing the en-\ntire original training set with a ﬁxed random seed\nand collecting the ﬁrst 32 examples of the shufﬂed\ndataset. Following (Raffel et al., 2020; Brown et al.,\n2020), we select only positive examples for WSC;\nfor both MultiRC and ReCoRD, we follow Brown\net al. (2020) and select a total of 32 questions –\nwhich corresponds to more than 32 training exam-\nples – to enable a fair comparison with GPT-3.\nThe unlabeled datasets for all tasks are ob-\ntained by collecting up to 20, 000 examples from\ntheir training sets and removing the labels. As\nthe training sets for RTE and CB are very small,\nfor both tasks we additionally select random un-\nlabeled examples from the MNLI training set\nfor a total of 20, 000 examples. For evaluation,\nwe use the ofﬁcial validation and test sets for\nall tasks that are available at https://super.\ngluebenchmark.com/tasks. All datasets in-\ncluded in SuperGLUE are in English. Additional\ndetails for each dataset are given in Table 8.\nPreprocessing We do not perform any prepro-\ncessing, except shortening all examples to the max-\nimum sequence length. This is done using the\nlongest ﬁrst strategy implemented in the Transform-\ners library. All input sequences are truncatedbefore\napplying patterns.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.706666111946106
    },
    {
      "name": "Task (project management)",
      "score": 0.6908901333808899
    },
    {
      "name": "Language model",
      "score": 0.6308457255363464
    },
    {
      "name": "Shot (pellet)",
      "score": 0.6061740517616272
    },
    {
      "name": "Key (lock)",
      "score": 0.5841854810714722
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5353299379348755
    },
    {
      "name": "Footprint",
      "score": 0.5334736704826355
    },
    {
      "name": "Natural language processing",
      "score": 0.4983956813812256
    },
    {
      "name": "Carbon footprint",
      "score": 0.4672513008117676
    },
    {
      "name": "Chemistry",
      "score": 0.08671993017196655
    },
    {
      "name": "Geography",
      "score": 0.07947242259979248
    },
    {
      "name": "Ecology",
      "score": 0.07854962348937988
    },
    {
      "name": "Engineering",
      "score": 0.0631970763206482
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Greenhouse gas",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ]
}