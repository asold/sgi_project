{
  "title": "Retrieval augmented scientific claim verification",
  "url": "https://openalex.org/W4392226889",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2101005902",
      "name": "Hao Liu",
      "affiliations": [
        "Montclair State University"
      ]
    },
    {
      "id": "https://openalex.org/A2036204426",
      "name": "ali soroush",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2765106379",
      "name": "Jordan G. Nestor",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A1965038804",
      "name": "Elizabeth Park",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A4286553684",
      "name": "Betina Idnay",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A3164167264",
      "name": "Yilu Fang",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2097599421",
      "name": "Jane Pan",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2146991823",
      "name": "Stan Liao",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2421954267",
      "name": "Marguerite Bernard",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2099815532",
      "name": "Yifan Peng",
      "affiliations": [
        "Weill Cornell Medicine",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2113027333",
      "name": "Chunhua Weng",
      "affiliations": [
        "Columbia University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2620100062",
    "https://openalex.org/W3105753785",
    "https://openalex.org/W3120711451",
    "https://openalex.org/W2081814119",
    "https://openalex.org/W2134568263",
    "https://openalex.org/W2127581560",
    "https://openalex.org/W49437421",
    "https://openalex.org/W2162197046",
    "https://openalex.org/W2026368897",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W1994339957",
    "https://openalex.org/W3133622905",
    "https://openalex.org/W2418587547",
    "https://openalex.org/W3021052948",
    "https://openalex.org/W2357069867",
    "https://openalex.org/W2883380892",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W1572319397",
    "https://openalex.org/W2069870183",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W1537823303",
    "https://openalex.org/W4252076394"
  ],
  "abstract": "Abstract Objective To automate scientific claim verification using PubMed abstracts. Materials and Methods We developed CliVER, an end-to-end scientific Claim VERification system that leverages retrieval-augmented techniques to automatically retrieve relevant clinical trial abstracts, extract pertinent sentences, and use the PICO framework to support or refute a scientific claim. We also created an ensemble of three state-of-the-art deep learning models to classify rationale of support, refute, and neutral. We then constructed CoVERt, a new COVID VERification dataset comprising 15 PICO-encoded drug claims accompanied by 96 manually selected and labeled clinical trial abstracts that either support or refute each claim. We used CoVERt and SciFact (a public scientific claim verification dataset) to assess CliVERâ€™s performance in predicting labels. Finally, we compared CliVER to clinicians in the verification of 19 claims from 6 disease domains, using 189 648 PubMed abstracts extracted from January 2010 to October 2021. Results In the evaluation of label prediction accuracy on CoVERt, CliVER achieved a notable F1 score of 0.92, highlighting the efficacy of the retrieval-augmented models. The ensemble model outperforms each individual state-of-the-art model by an absolute increase from 3% to 11% in the F1 score. Moreover, when compared with four clinicians, CliVER achieved a precision of 79.0% for abstract retrieval, 67.4% for sentence selection, and 63.2% for label prediction, respectively. Conclusion CliVER demonstrates its early potential to automate scientific claim verification using retrieval-augmented strategies to harness the wealth of clinical trial abstracts in PubMed. Future studies are warranted to further test its clinical utility.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5181738138198853
    },
    {
      "name": "Information retrieval",
      "score": 0.4974558651447296
    },
    {
      "name": "Data science",
      "score": 0.33468031883239746
    }
  ]
}