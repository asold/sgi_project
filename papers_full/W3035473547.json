{
    "title": "Influence Paths for Characterizing Subject-Verb Number Agreement in LSTM Language Models",
    "url": "https://openalex.org/W3035473547",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2341035762",
            "name": "Kaiji Lu",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A9243146",
            "name": "Piotr Mardziel",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2786195626",
            "name": "Klas Leino",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2076493382",
            "name": "Matt Fredrikson",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2147094257",
            "name": "Anupam Datta",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2795355797",
        "https://openalex.org/W2805390961",
        "https://openalex.org/W2888169584",
        "https://openalex.org/W2069143585",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2121029939",
        "https://openalex.org/W1689711448",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1951216520",
        "https://openalex.org/W2952208026",
        "https://openalex.org/W1787224781",
        "https://openalex.org/W2922523190",
        "https://openalex.org/W2510508396",
        "https://openalex.org/W2963029978",
        "https://openalex.org/W2964222268",
        "https://openalex.org/W2963430224",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W2962851944",
        "https://openalex.org/W2594633041",
        "https://openalex.org/W2782630856",
        "https://openalex.org/W2964089344",
        "https://openalex.org/W2951257191"
    ],
    "abstract": "LSTM-based recurrent neural networks are the state-of-the-art for many natural language processing (NLP) tasks. Despite their performance, it is unclear whether, or how, LSTMs learn structural features of natural languages such as subject-verb number agreement in English. Lacking this understanding, the generality of LSTM performance on this task and their suitability for related tasks remains uncertain. Further, errors cannot be properly attributed to a lack of structural capability, training data omissions, or other exceptional faults. We introduce *influence paths*, a causal account of structural properties as carried by paths across gates and neurons of a recurrent neural network. The approach refines the notion of influence (the subject’s grammatical number has influence on the grammatical number of the subsequent verb) into a set of gate or neuron-level paths. The set localizes and segments the concept (e.g., subject-verb agreement), its constituent elements (e.g., the subject), and related or interfering elements (e.g., attractors). We exemplify the methodology on a widely-studied multi-layer LSTM language model, demonstrating its accounting for subject-verb number agreement. The results offer both a finer and a more complete view of an LSTM’s handling of this structural aspect of the English language than prior results based on diagnostic classifiers and ablation.",
    "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4748–4757\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n4748\nInﬂuence Paths for Characterizing Subject-Verb Number Agreement in\nLSTM Language Models\nKaiji Lu\nCarnegie Mellon University\nkaijil@andrew.cmu.edu\nPiotr Mardziel\nCarnegie Mellon University\npiotrm@gmail.com\nKlas Leino\nCarnegie Mellon University\nkleino@cs.cmu.edu\nMatt Fedrikson\nCarnegie Mellon University\nmfredrik@cmu.edu\nAnupam Datta\nCarnegie Mellon University\ndanupam@cmu.edu\nAbstract\nLSTM-based recurrent neural networks are the\nstate-of-the-art for many natural language pro-\ncessing (NLP) tasks. Despite their perfor-\nmance, it is unclear whether, or how, LSTMs\nlearn structural features of natural languages\nsuch as subject-verb number agreement in En-\nglish. Lacking this understanding, the general-\nity of LSTMs on this task and their suitabil-\nity for related tasks remains uncertain. Fur-\nther, errors cannot be properly attributed to\na lack of structural capability, training data\nomissions, or other exceptional faults. We in-\ntroduce inﬂuence paths , a causal account of\nstructural properties as carried by paths across\ngates and neurons of a recurrent neural net-\nwork. The approach reﬁnes the notion of in-\nﬂuence (the subject’s grammatical number has\ninﬂuence on the grammatical number of the\nsubsequent verb) into a set of gate-level or\nneuron-level paths. The set localizes and seg-\nments the concept (e.g., subject-verb agree-\nment), its constituent elements (e.g., the sub-\nject), and related or interfering elements (e.g.,\nattractors). We exemplify the methodology\non a widely-studied multi-layer LSTM lan-\nguage model, demonstrating its accounting for\nsubject-verb number agreement. The results\noffer both a ﬁner and a more complete view\nof an LSTM’s handling of this structural as-\npect of the English language than prior results\nbased on diagnostic classiﬁers and ablation.\n1 Introduction\nTraditional rule-based NLP techniques can cap-\nture syntactic structures, while statistical NLP tech-\nniques, such as n-gram models, can heuristically\nintegrate semantics of a natural language. Mod-\nern RNN-based models such as Long Short-Term\nMemory (LSTM) models are tasked with incorpo-\nrating both semantic features from the statistical\nassociations in their training corpus, and structural\nfeatures generalized from the same.\nIn:\nOut:\nboys behind the tree (run)The\ns0 s1 s2 s3 s4\nCell c0\n1\nCandidate Cell ˜c1\n1\nCell c1\n1\nCandidate Cell ˜c0\n1\nHidden h0\n1\nc1\n2 c1\n3 c1\n4\nh1\n4\nc0\n4\n˜c1\n4\n˜c0\n4\nh0\n4\nagreement\ns4(run) - s4(runs)\ngrammatical number\nboys − boy+boys\n2\nFigure 1: Subject-verb agreement task for a 2-layer\nLSTM language model, and primary paths across var-\nious LSTM gates implementing subject-verb number\nagreement. A language model assigns score sto each\nword. Agreement is the score of the correctly num-\nbered verb minus that of the incorrectly numbered verb.\nDespite evidence that LSTMs can capture syntac-\ntic rules in artiﬁcial languages (Gers and Schmid-\nhuber, 2001), it is unclear whether they are as ca-\npable in natural languages (Linzen et al., 2016;\nLakretz et al., 2019) in the context of rules such as\nsubject-verb number agreement, especially when\nnot supervised for the particular feature. The incon-\ngruence derives from this central question: does an\nLSTM language model’s apparent performance in\nsubject-verb number agreement derive from statis-\ntical heuristics (like n-gram models) or from gener-\nalized knowledge (like rule-based models)?\nRecent work has begun addressing this ques-\ntion (Linzen et al., 2016) in the context of lan-\nguage models: models tasked with modeling the\nlikelihood of the next word following a sequence\nof words as expected in a natural language (see Fig-\nure 1, bottom). Subject-verb number agreementdic-\ntates that the verb associated with a given subject\n4749\nshould match its number (e.g., in Figure 1, the verb\n“run” should match with the subject “boys”). Giu-\nlianelli et al. (2018) showed that the subject gram-\nmatical number is associated with various gates in\nan LSTM, and Lakretz et al. (2019) showed that ab-\nlation (disabling activation) of an LSTM model at\ncertain locations can reduce its accuracy at scoring\nverbs of the correct grammatical number.\nInﬂuence offers an alternate means of exploring\nproperties like number agreement. We say an input\nis inﬂuential on an outcome when changing just\nthe input and nothing else induces a change on the\noutcome. In English grammar, the number of a sub-\nject is inﬂuential on the number of its verb, in that\nchanging the number of that subject while keep-\ning all other elements of a sentence ﬁxed would\nnecessitate a change in the number of the verb.\nAlgorithmic transparency literature offers formal\ndeﬁnitions for empirically quantifying notions of\ninﬂuence for systems in general (Datta et al., 2016)\nand for deep neural networks speciﬁcally (Leino\net al., 2018; Sundararajan et al., 2017).\nThe mere fact that subject number is inﬂuential\non verb number as output by an LSTM model is\nsufﬁcient to conclude that it incorporates the agree-\nment concept in some way but does not indicate\nwhether it operates as a statistical heuristic or as\na generalized rule. We address this question with\ninﬂuence paths, which decompose inﬂuence into\na set of paths across the gates and neurons of an\nLSTM model. The approach has several elements:\n1. Deﬁne an input parameter to vary the concept-\nspeciﬁc quantity under study (e.g., the gram-\nmatical number of a particular noun, bottom-\nleft node in Figure 1) and a concept-speciﬁc\noutput feature to measure the parameter’s ef-\nfect on (e.g, number agreement with the pa-\nrameterized noun, bottom-right node in Fig-\nure 1).\n2. Apply a gradient-based inﬂuence method to\nquantify the inﬂuence of the concept param-\neter on the concept output feature; as per\nthe chain rule, decompose the inﬂuence into\nmodel-path-speciﬁc quantities.\n3. Inspect and characterize the distribution of\ninﬂuence across the model paths.\nThe paths demonstrate where relevant state infor-\nmation necessitated by the concept is kept, how\nit gets there, how it ends up being used to affect\nthe model’s output, and how and where related\nconcepts interfere.\nOur approach is state-agnostic in that it does not\nrequire a priori an assumption about how or if the\nconcept will be implemented by the LSTM. This\ndiffers from works on diagnostic classiﬁers where\na representation of the concept is assumed to ex-\nist in the network’s latent space. The approach\nis also time-aware in that paths travel through\ncells/gates/neurons at different stages of an RNN\nevaluation. This differs from previous ablation-\nbased techniques, which localize the number by\nclearing neurons at some position in an RNN for\nall time steps.\nOur contributions are as follows:\n•We introduceinﬂuence paths, a causal account\nof the use of concepts of interest as carried by\npaths across gates and neurons of an RNN.\n•We demonstrate, using inﬂuence paths, that\nin a multi-layer LSTM language model, the\nconcept of subject-verb number agreement is\nconcentrated primarily on a single path (the\nred path in Figure 1), despite a variety of sur-\nrounding and intervening contexts.\n•We show that attractors (intervening nouns\nof opposite number to the subject) do not di-\nminish the contribution of the primary subject-\nverb path, but rather contribute their own in-\nﬂuence of the opposite direction along the\nequivalent primary attractor-verb path (the\nblue path in the ﬁgure). This can lead to incor-\nrect number prediction if an attractor’s contri-\nbution overcomes the subject’s.\n•We corroborate and elaborate on existing re-\nsults localizing subject number to the same\ntwo neurons which, in our results, lie on the\nprimary path. We further extend and gener-\nalize prior compression/ablation results with\na new path-focused compression test which\nveriﬁes our localization conclusions.\nOur results point to generalized knowledge as the\nanswer to the central question. The number agree-\nment concept is heavily centralized to the primary\npath despite the varieties of contexts. Further, the\nprimary path’s contribution is undiminished even\namongst interfering contexts; number errors are\nnot attributable to lack of the general number con-\ncept but rather to sufﬁciently inﬂuential contexts\npushing the result in the opposite direction.\n4750\n2 Background\nLSTMs Long short-term memory networks\n(LSTMs) (Hochreiter and Schmidhuber, 1997)\nhave proven to be effective for modeling sequences,\nsuch as language models, and empirically, this ar-\nchitecture has been found to be optimal compared\nto other second-order RNNs (Greff et al., 2017).\nLSTMs utilize several types of gates and internal\nstates including forget gates (f), input gates (i),\noutput gates (o), cell states (c), candidate cell state\n(˜c), and hidden states (h). Each gate is designed\nto carry out a certain function, or to ﬁx a certain\ndrawback of the vanilla RNN architecture. E.g.,\nthe forget gate is supposed to determine how much\ninformation from the previous cell state to retain\nor “forget”, helping to ﬁx the vanishing gradient\nproblem (Hochreiter, 1998).\nNumber Agreement in Language Models The\nnumber agreement (NA) task, as described by\nLinzen et al. (2016), is an evaluation of a language\nmodel’s ability to properly match the verb’s gram-\nmatical number with its subject. This evaluation is\nperformed on sentences speciﬁcally designed for\nthe exercise, with zero or more words between the\nsubject and the main verb, termed the context. The\ntask for sentences with non-empty contexts will be\nreferred to as long-term number agreement.\n“Human-level” performance for this task can\nbe achieved with a 2-layer LSTM language\nmodel (Gulordava et al.), indicating that the lan-\nguage model incorporates grammatical number de-\nspite being trained only for the more general word\nprediction task. Attempts to explain or localize the\nnumber concept within the model include (Lakretz\net al., 2019), where ablation of neurons is applied\nto locate speciﬁc neurons where such information\nis stored; and (Giulianelli et al., 2018; Hupkes et al.,\n2018), where diagnostic classiﬁers are trained on\ngate activations to predict the number of the subject\nto see which gates or timesteps the number concept\nexhibits itself. These works also look at the special\ncases involving attractors—intervening nouns with\ngrammatical number opposite to that of the sub-\nject (deemed instead helpful nouns if their number\nagrees with the subject)—such as the word “tree”\nin Figure 1. Both frameworks provide explanations\nas to why attractors lower the performance of NA\ntasks. However, they tend to focus on the activa-\ntion patterns of gates or neurons without justifying\ntheir casual relationships with the concept of gram-\nmatical number, and do not explicitly identify the\nexact temporal trajectory of how the number of the\nsubject inﬂuences the number of the verb.\nOther relevant studies that look inside RNN mod-\nels to locate speciﬁc linguistic concepts include\nvisualization techniques such as (Karpathy et al.,\n2015), and explanations for supervised tasks involv-\ning LSTMs such as sentiment analysis (Murdoch\net al., 2018).\nAttribution Methods Attribution methods quan-\ntitatively measure the contribution of each of a\nfunction’s individual inputs to its output. Gradient-\nbased attribution methods compute the gradient of\na model with respect to its inputs to describe how\nimportant each input is towards the output predic-\ntions. These methods have been applied to assist in\nexplaining deep neural networks, predominantly in\nthe image domain (Leino et al., 2018; Sundarara-\njan et al., 2017; Bach et al., 2015; Simonyan et al.,\n2013). Some such methods are also axiomatically\njustiﬁed to provide a causal link between inputs (or\nintermediate neurons) and the output.\nAs a starting point in this work, we consider In-\ntegrated Gradients (IG) (Sundararajan et al., 2017).\nGiven a baseline, x0, the attribution for each in-\nput at point, x, is the path integral taken from the\nbaseline to xof the gradients of the model’s output\nwith respect to its inputs. The baseline establishes\na neutral point from which to make a counterfac-\ntual comparison; the attribution of a feature can be\ninterpreted as the share of the model’s output that\nis due to that feature deviating from its baseline\nvalue. By integrating the gradients along the linear\ninterpolation from the baseline to x, IG ensures\nthat the attribution given to each feature issensitive\nto effects exhibited by the gradient at any point\nbetween the baseline and instance x.\nLeino et al. (2018) generalize IG to better focus\nattribution on concepts other than just model out-\nputs, by use of a quantity of interest (QoI) and a\ndistribution of interest (DoI). Their measure, Dis-\ntributional Inﬂuence, is given by Deﬁnition 1. The\nQoI is a function of the model’s output express-\ning a particular output behavior of the model to\ncalculate inﬂuence for; in IG, this is ﬁxed as the\nmodel’s output. The DoI speciﬁes a distribution\nover which the inﬂuence should faithfully summa-\nrize the model’s behavior; the inﬂuences are found\nby taking an expected value over DoI.\nDeﬁnition 1 (Distributional Inﬂuence). With quan-\ntity of interest, q, and distribution of interest, D,\n4751\nthe inﬂuence, χ, of the inputs on the quantity of\ninterest is:\nχ(q,D) = E\n⃗ x∼D\n[∂q\n∂x(⃗ x)\n]\nThe directed path integral used by IG can be im-\nplemented by setting the DoI to a uniform dis-\ntribution over the line from the baseline to ⃗ x:\nD = Uniform\n(\n⃗ x0⃗ x\n)\n, for baseline, ⃗ x0, and then\nmultiplying χby ⃗ x−⃗ x0. Conceptually, by mul-\ntiplying by ⃗ x−⃗ x0, we are measuring the attribu-\ntion, i.e., the contribution to the QoI, of ⃗ x−⃗ x0 by\nweighting its features by theirinﬂuence. We use the\nframework of Leino et al. in this way to deﬁne our\nmeasure of attribution for NA tasks in Section 3.\nDistributional Inﬂuence can be approximated\nby sampling according to the DoI. In particular,\nwhen using D = Uniform\n(\n⃗ x0⃗ x\n)\nas noted above,\nDeﬁnition 1 can be computationally approximated\nwith a sum of nintervals as in IG:\nχ≈\nn∑\ni=1\n∂q\n∂x\n(i\nn⃗ x+\n(\n1 −i\nn\n)\n⃗ x0\n)\nOther related works include Fiacco et al. (2019),\nwhich employs the concept of neuron paths based\non coﬁring of neurons instead of inﬂuence, also on\ndifferent NLP tasks from ours.\n3 Methods\nOur method for computing inﬂuence paths begins\nwith modeling a relevant concept, such as grammat-\nical number, in the inﬂuence framework of Leino\net al. (Deﬁnition 1) by deﬁning a quantity of in-\nterest that corresponds to the grammatical number\nof the verb, and deﬁning a component of the input\nembedding that isolates the subject’s grammatical\nnumber (Section 3.1). We then decompose the in-\nﬂuence measure along the relevant structures of\nLSTM (gates or neurons) as per standard calculus\nidentities to obtain a deﬁnition for inﬂuence paths\n(Section 3.2).\n3.1 Measuring Number Agreement\nFor the NA task, we view the initial fragment con-\ntaining the subject as the input, and the word distri-\nbution at the position of its corresponding verb as\nthe output.\nFormally, each instance in this task is a se-\nquence of d-dimensional word embedding vectors,\nw\ndef\n= ⟨⃗ wi⟩i, containing the subject and the corre-\nsponding verb, potentially with intervening words\nin between. We assume the subject is at position t\nand the verb at position t+ n. The output score of\na word, w, at position iwill be written si(w). If w\nhas a grammatical number, we writew+ and w−to\ndesignate wwith its original number and the equiv-\nalent word with the opposite number, respectively.\nQuantity of Interest We instrument the output\nscore with a QoI measuring the agreement of the\noutput’s grammatical number to that of the subject:\nDeﬁnition 2 (Number Agreement Measure) .\nGiven a sentence, w, with verb, w, whose correct\nform (w.r.t. grammatical number) isw+, the quan-\ntity of interest, q, measures the correctness of the\ngrammatical number of the verb:\nq(w)\ndef\n= st+n\n(\nw+)\n−st+n\n(\nw−)\nIn plain English, q captures the weight that the\nmodel assigns to the correct form of was opposed\nto the weight it places on the incorrect form. Note\nthat the number agreement concept could have rea-\nsonably been measured using a different quantity\nof interest. E.g., considering the scores of all vo-\ncabulary words of the correct number and incorrect\nnumber in the positive and negative terms, respec-\ntively, is an another alternative. However, based on\nour preliminary experiments, we found this alter-\nnative does not result in meaningful changes to the\nreported results in the further sections.\nDistribution of Interest We also deﬁne a com-\nponent of the embedding of the subject that cap-\ntures its grammatical number, and a distribution\nover the inputs that allows us to sensitively measure\nthe inﬂuence of this concept on our chosen quantity\nof interest. Let ⃗ w0 be the word embedding mid-\nway between its numbered variants, i.e., ⃗ w++ ⃗ w−\n2 .\nThough this vector will typically not correspond\nto any English word, we interpret it as a number-\nneutral version of ⃗ w. Various works show that\nlinear arithmetic on word embeddings of this sort\npreserves meaningful word semantics as demon-\nstrated in analogy parallelograms (Mikolov et al.,\n2013). Finally, given a sentence, w, let w0\nt be\nthe sentence w, except with the word embedding\n⃗ wt replaced with its neutral form ⃗ w0\nt. We see that\nw−w0\nt captures the part of the input corresponding\nto the grammatical number of the subject, ⃗ wt.\nDeﬁnition 3 (Grammatical Number Distribution).\nGiven a singular (or plural) noun,wt, in a sentence,\nw, the distribution density of sentences, Dw, exer-\ncising the noun’ssingularity (or plurality) linearly\n4752\ninterpolates between the neutral sentence, w0\nt, and\nthe given sentence, w:\nDw\ndef\n= Uniform\n(\nw0\ntw\n)\nIf ⃗ wt is singular, our counterfactual sentences span\nw with number-neutral ⃗ w0\nt all the way to its singu-\nlar form ⃗ wt = ⃗ w+\nt . We thus call this distribution\na singularity distribution. Were wt plural instead,\nwe would refer to the distribution as a plurality\ndistribution. Using this distribution of sentences\nas our DoI thus allows us to measure the inﬂuence\nof w −w0\nt (the grammatical number of a noun at\nposition t) on our quantity of interest sensitively\n(in the sense that Sundararajan et al. deﬁne their\naxiom of sensitivity for IG (Sundararajan et al.,\n2017)).\nSubject-Verb Number Agreement Putting\nthings together, we deﬁne our attribution measure.\nDeﬁnition 4 (Subject-Verb Number Agreement\nAttribution). The measure of attribution, α, of a\nnoun’s grammatical number on the subject-verb\nnumber agreement is deﬁned in terms of the DoI,\nDw, and QoI, q, as in Deﬁnitions 3 and 2, respec-\ntively.\nα(w) = (w −w0\nt) χ(q,Dw)\nEssentially, the attribution measure weights the\nfeatures of the subject’s grammatical number by\ntheir Distributional Inﬂuence, χ. Because Dw\nis a uniform distribution over the line segment\nbetween w and w0\nt, as with IG, the attribution\ncan be interpreted as each feature’s net contribu-\ntion to the change in the QoI, q(w) −q(w0\nt), as∑\niχ(w)i = q(w) −q(w0\nt) (i.e., Deﬁnition 4 sat-\nisﬁes the axiom Sundararajan et al. term complete-\nness (Sundararajan et al., 2017)).\nIn Figure 1, for instance, this deﬁnition mea-\nsures the attribution from the plurality of the sub-\nject (“boys”), towards the model’s prediction of the\ncorrectly numbered verb (“run”) versus the incor-\nrectly numbered verb (“runs”). Later in this paper\nwe will also investigate the attribution of interven-\ning nouns on this same quantity. We expect the\ninput attribution to be positive for all subjects and\nhelpful nouns, and negative for attractors, which\ncan be veriﬁed by the P+columns of Table 1 (the\ndetails of this experiment are introduced in Sec-\ntion 4).\n3.2 Inﬂuence Paths\nInput attribution as deﬁned by IG (Sundararajan\net al., 2017) provides a way of explaining a model\nby highlighting the input dimensions with large\nattribution towards the output. Distributional Inﬂu-\nence (Leino et al., 2018) with a carefully chosen\nQoI and DoI (Deﬁnition 4) further focuses the in-\nﬂuence on a concept at hand, grammatical number\nagreement. Neither, however, demonstrate how\nthese measures are conveyed by the inner workings\nof a model. In this section we deﬁne a decomposi-\ntion of the inﬂuence into paths of a model, thereby\nassigning attribution not just to inputs, but also to\nthe internal structures of a given model.\nWe ﬁrst deﬁne arbitrary deep learning models\nas computational graphs, as in Deﬁnition 5. We\nthen use this graph abstraction to deﬁne a notion\nof inﬂuence for a path through the graph. We posit\nthat any natural path decomposition should satisfy\nthe following conservation property: the sum of the\ninﬂuence of each path from the input to the output\nshould equal the inﬂuence of the input on the QoI.\nWe then observe that the chain rule from calculus\noffers one such natural decomposition, yielding\nDeﬁnition 6.\nDeﬁnition 5 (Model). A model is an acyclic graph\nwith a set of nodes, edges, and activation functions\nassociated with each node. The output of a node,\nn, on input x is n(x)\ndef\n= fn(n1(x),··· ,nm(x))\nwhere n1,··· ,nm are n’s predecessors andfn is\nits activation function. If ndoes not have predeces-\nsors (it is an input), its activation is fn(x). We as-\nsume that the domains and ranges of all activation\nfunctions are real vectors of arbitrary dimension.\nWe will write n1 →n2 to denote an edge (i.e.,\nn1 is a direct predecessor of n2), and n1 →∗n2 to\ndenote the set of all paths from n1 to n2. The par-\ntial derivative of the activation of n2 with respect\nto the activation of n1 will be written ∂n2\n∂n1\n.\nThis view of a computation model is an exten-\nsion of network decompositions from attribution\nmethods using the natural concept of “layers” or\n“slices” (Dhamdhere et al., 2018; Leino et al., 2018;\nBach et al., 2015). This decomposition can be tai-\nlored to the level of granularity we wish to expose.\nMoreover, in RNN models where no single and\nconsistent “natural layer” can be found due to the\nvariable-length inputs, a more general graph view\nprovides the necessary versatility.\nDeﬁnition 6 (Path Inﬂuence). Expanding Deﬁni-\ntion 4 using the chain rule, the inﬂuence of input\n4753\nnode, s, on target node, t, in a model, G, is:\nχs = E\nx∼D(x)\n[∂t\n∂s(x)\n]\n= E\nx∼D(x)\n\n ∑\np∈(s→∗t)\n∏\n(n1→n2)∈p\n∂n2\n∂n1\n(x)\n\n\n=\n∑\np∈(s→∗t)\nE\nx∼D(x)\n\n ∏\n(n1→n2)∈p\n∂n2\n∂n1\n(x)\n\n\n  \nχp\ns\nNote that the same LSTM can be modeled with\ndifferent graphs to achieve a desired level of ab-\nstraction. We will use two particular levels of gran-\nularity: a coarse gate-level abstraction where nodes\nare LSTM gates, and a ﬁne neuron-level abstrac-\ntion where nodes are the vector elements of those\ngates. Though the choice of abstraction granularity\nhas no effect on the represented model semantics,\nit has implications on graph paths and the scale of\ntheir individual contributions in a model.\nGate-level and Neuron-level Paths We de-\nﬁne the set of gate-level nodes to include:{\nfl\nt, il\nt, ol\nt, cl\nt, ˜cl\nt, hl\nt : t<T, l<L\n}\n, where T\nis the number of time steps (words) and L is num-\nber of LSTM layers. The node set also includes\nan attribution-speciﬁc input node ( w −w0\nt) and\nan output node (the QoI). An example of this is\nillustrated in Figure 2. We exclude intermediate\ncalculations (the solid nodes of Figure 2, such as\nft⊙ct−1) as their inclusion does not change the set\nof paths in a graph. We can also break down each\nvector node into scalar components and further de-\ncompose the gate-level model into a neuron-level\none: {fl\nti, il\nti, ol\nti, cl\nti, ˜cl\nti, hl\nti : t < T, i <\nH, l < L}, where H is the size of each gate\nvector. This decomposition results in an exponen-\ntially large number of paths. However, since many\nfunctions between gates in an LSTM are element-\nwise operations, neuron-level connections between\nmany neighboring gates are sparse.\nPath Reﬁnement While the neuron-level path\ndecomposition can theoretically be performed on\nthe whole network, in practice we choose to spec-\nify a gate-level path ﬁrst, then further decompose\nthat path into neuron-level paths. We also collapse\nselected vector nodes, allowing us to further local-\nize a concept on a neuron level while avoiding an\nexplosion in the number of paths. The effect of this\npipeline will be empirically justiﬁed in Section 4.\nc\nh ˜c\ni\nx\no\nf\nc\nh\nSubject Intervening Noun\n˜c\ni\nx\no\nf\nc\nh ˜c\ni\nx\no\nf\nc\nh\nc\nh ˜c\ni\no\nf\nc\nh ˜c\ni\no\nf\nc\nh ˜c\ni\no\nf\nc\nh\nQoI\nl0\nl1\nT t t + n − 1 t + nt − 1\nFigure 2: Inﬂuence path diagram in a NA task for the\n2-layer LSTM model. The red path shows the path with\nthe greatest attribution (the primary path) from the sub-\nject; The blue path shows the primary path from the\nintervening noun.\n4 Evaluation\nIn this section we apply inﬂuence path decompo-\nsition to the NA task. We investigate major gate-\nlevel paths and their inﬂuence concentrations in\nSection 4.2. We further show the relations between\nthese paths and the paths carrying grammatical\nnumber from intervening nouns (i.e. attractors &\nhelpful nouns) in Section 4.3. In both we also in-\nvestigate high-attribution neurons along primary\npaths allowing us to compare our results to prior\nwork.\n4.1 Dataset and Model\nWe study the exact combination of language model\nand NA datasets used in the closely related prior\nwork of Lakretz et al. (2019). The pre-trained lan-\nguage model of Gulordava et al. and Lakretz et al.\nis a 2-layer LSTM trained from Wikipedia articles.\nThe number agreement datasets of Lakretz et al.\nare several synthetically generated datasets varying\nin syntactic structures and in the number of nouns\nbetween the subject and verb.\nFor example, nounPP refers to sentences con-\ntaining a noun subject followed by a prepositional\nphrase such as in Figure 1. Each NA task has\nsubject number (and intervening noun number if\npresent) realizations along singular (S) and plural\n(P) forms. In listings we denote subject number (S\nor P) ﬁrst and additional noun (if any) number sec-\nond. Details including the accuracy of the model\non the NA tasks are summarized by Lakretz et al.\n(2019). Our evaluation replicates part of Table 2 in\n4754\nsaid work.\n4.2 Decomposing Number Agreement\nWe begin with the attribution of subject number on\nits corresponding verb, as decomposed per Deﬁ-\nnition 6. Among all NA tasks, the gate-level path\ncarrying the most attribution is one following the\nsame pattern with differences only in the size of\ncontexts. With indices t and t+ n referring to\nthe subject and verb respectively, this path, which\nwe term the primary path of subject-verb number\nagreement, is as follows:\nxt(DoI) ·˜c0 ·c0 ·h0 ·˜c1 ·\n(\nc1)∗\n·h1 ·QoI\nThe primary path is represented by the red path in\nFigure 2. The inﬂuence ﬁrst passes through the\ntemporary cell state ˜c0, the only non-sigmoid cell\nstates capable of storing more information than sig-\nmoid gates, since i,f,o ∈(0,1) while the tanh\ngate ˜c∈(−1,1). Then the path passes through c0,\nh0, and similarly to c1 through ˜c1 , jumping from\nthe ﬁrst to the second layer. The path then stays\nat c1, through the direct connections between cell\nstates of neighbouring time steps, as though it is\n“stored” there without any interference from subse-\nquent words. As a result, this path is intuitively the\nmost efﬁcient and simplistic way for the model to\nencode and store a “number bit.”\nThe extent to which this path can be viewed as\nprimary is measured by two metrics. The results\nacross a subset of syntactic structures and number\nconditions mirroring those in Lakretz et al. (2019)\nare shown in Table 1. We include 3 representative\nvariations of the task. The metrics are:\n1. t-value: probability that a given path has\ngreater attribution than a uniformly sampled\npath on a uniformly sampled sentence.\n2. Positive/Negative Share (±Share): expected\n(over sentences) fraction of total positive (or\nnegative) attribution assigned to the given pos-\nitive (or negative) path.\nPer Table 1 (From Subject, Primary Path), we make\nour ﬁrst main observation:\nObservation 1. The same one primary path con-\nsistently carries the largest amount positive attri-\nbution across all contexts as compared to all other\npaths.\nEven in the case of its smallest share (nounPPAdv),\nthe 3% share is large when taking into account\nmore than 40,000 paths in total. Sentences with sin-\ngular subjects (top part of Table 1) have a slightly\nstronger concentration of attribution in the pri-\nmary path than plural subjects (bottom part of Ta-\nble 1), possibly due to English plural (inﬁnitive)\nverb forms occurring more frequently than singu-\nlar forms, thus less concentration of attribution is\nneeded due to the “default signal” in place.\nPrimary Neurons We further decompose the pri-\nmary path into inﬂuence passing through each neu-\nron. Since only connections between second layer\ncell states are sparse, we only decompose the seg-\nment of the primary path from c1\nt to c1\nt+n, result-\ning in a total of 650 (the number of hidden units)\nneuron-level paths. (We leave the non-sparse de-\ncompositions for future work). The path for neuron\ni, for example, is represented as:\nxt(DoI) ·˜c0 ·c0 ·h0 ·˜c1 ·\n(\nc1\ni\n)∗\n·h1 ·QoI\nTo compare the attribution of an individual neuron\nwith all other neurons, we employ a similar afore-\nmentioned t-value, where each neuron-level path\nis compared against other neuron-level paths.\nThe results of the neuron-level analysis are\nshown in Table 1 (From Subject, Primary Neuron).\nOut of the 650 neuron-level paths in the gate-level\nprimary path, we discover two neurons with con-\nsistently the most attribution (neurons 125 and 337\nof the second layer). This indicates the number\nconcept is concentrated in only two neurons.\nComparison with Lakretz et al. (2019) Unco-\nincidentally, both neurons match the units found\nthrough ablation by Lakretz et al., who use the\nsame model and dataset (neurons 988 and 776 are\nneurons 125 and 337 of the second layer). This\naccordance to some extent veriﬁes that the neurons\nfound through inﬂuence paths are functionally im-\nportant. However, the t-values shown in Table 1\nshow that both neuron 125 and 337 are inﬂuential\nregardless of the subject number, whereas Lakretz\net al. assign a subject number for each of these two\nneurons due to their disparate effect in lowering\naccuracy in ablation experiments. One possible rea-\nson is that the ablation mechanism used in (Lakretz\net al., 2019) assumes that a “neutral number state”\ncan be represented by zero-activations for all gates,\nwhile in reality the network may encode the neutral\nstate differently for different gates.\nAnother major distinction of our analysis from\nLakretz et al. (2019) regards simple cases with no\n4755\nTask C\nFrom Subject From Intervening Noun\nP+ |P| Primary Path Primary NeuronP+ |P| Primary Path Primary Neuron\n+Share t t125 t337 ±Share t t125 t337\nSimple S 1.0 16 0.47 1.0 0.99 1.0 - - - - - -\nnounPP SS 1.0 6946 0.1 1.0 1.0 1.0 0.82 16 0.31(+) 0.9 0.78 0.98\nnounPP SP 1.0 6946 0.1 1.0 1.0 1.0 0.23 16 0.24(-) 0.23 0.06 0.15\nnounPPAdv SS 1.0 41561 0.07 1.0 1.0 1.0 0.92 152 0.09(+) 0.96 0.85 1.0\nnounPPAdv SP 1.0 41561 0.07 1.0 1.0 1.0 0.32 152 0.09(-) 0.14 0.13 0.01\nSimple P 1.0 16 0.33 0.93 0.97 0.99 - - - - - -\nnounPP PS 1.0 6946 0.05 0.91 0.99 1.0 0.06 16 0.28(-) 0.21 0.22 0.12\nnounPP PP 1.0 6946 0.05 0.92 0.99 1.0 0.95 16 0.31(+) 0.9 0.97 0.79\nnounPPAdv PS 1.0 41561 0.03 0.93 0.99 1.0 0.32 152 0.04(-) 0.28 0.41 0.16\nnounPPAdv PP 1.0 41561 0.03 0.92 0.99 1.0 0.83 152 0.07(+) 0.92 0.99 0.84\nTable 1: Statistics for attribution of primary paths and neurons from the subject/intervening noun: P+ is the\npercentage of sentences with positive input attribution. Task andCcolumns refer to sentence structures in Lakretz\net al. (2019). |P|is the total number of paths; tand ±Share are t-values and positive/negative share, respectively.\nFor calculating t125 and t337 of primary neurons (125 and 337), we exclude these two neurons to avoid comparing\nthem with each other.\nword between subjects and verbs. Unlike Lakretz\net al., who claim that the two identiﬁed neurons\nare “long-term neurons”, we discover that these\ntwo neurons are also the only neurons important\nfor short-term number agreement. This localization\ncannot be achieved by diagnostic classiﬁers used\nby Lakretz et al., indicating that the signal can be\nbetter uncovered using inﬂuence-based paths rather\nthan association-based methods such as ablation.\n4.3 Decomposing from Intervening Nouns\nNext we focus on NA tasks with intervening nouns\nand make the following observation:\nObservation 2. The primary subject-verb path\nstill accounts for the largest positive attribution\nin contexts with either attractors or helpful nouns.\nA slightly worse NA task performance (Lakretz\net al., 2019) in cases of attractors ( SP, PS) indi-\ncates that they interfere with prediction of the cor-\nrect verb. In contrast, we also observe that helpful\nnouns ( SS, PP) contribute positively to the cor-\nrect verb number (although they should not from a\ngrammar perspective).\nPrimary Path from the Intervening Noun We\nadapt our number agreement concept (Deﬁnition 2)\nby focusing the DoI on the intervening noun,\nthereby allowing us to decompose its inﬂuence on\nthe verb number not grammatically associated with\nit. In Table 1 (From Intervening Noun) we discover\na similar primary path from the intervening noun:\nObservation 3. Attribution towards verb number\nfrom intervening nouns follows the same primary\npath as the subject but is of lower magnitude and\nTask C Compression Scheme\nCsi Cs Ci Csi Cs Ci C\nnounPP SS .66 .77 .95 .93 .71 .77 .95\nnounPP SP .64 .36 .94 .64 .75 .40 .74\nnounPP PS .34 .24 .92 .40 .69 .18 .80\nnounPP PP .39 .66 .91 .76 .68 .58 .97\nnounPP mean .51 .51 .93 .68 .70 .48 .87\nnounPPAdvSS .70 .86 .98 .73 .56 .43 1.0\nnounPPAdvSP .70 .43 .99 .50 .60 .27 .88\nnounPPAdvPS .38 .22 .98 .76 .79 .56 .96\nnounPPAdvPP .39 .67 .98 .84 .83 .76 1.0\nnounPPAdvmean .54 .55 .99 .71 .69 .50 .96\nTable 2: Model compression accuracy under various\ncompression schemes. Cis the uncompressed model.\nreﬂects either positive or negative attribution in\ncases of helpful nouns or attractors, respectively.\nThis disparity in magnitude is expected since the\nlanguage model possibly identiﬁes the subject as\nthe head noun through the prepositions such as\n“behind” in Figure 1, while still needing to track\nthe number of the intervening noun in possible\nclausal structures. Such need is comparably weaker\ncompared to tracking numbers of subjects, possibly\nbecause in English, intervening clauses are rarer\nthan intervening non-clauses. Similar arguments\ncan be made for neuron-level paths.\n4.4 Model Compression\nThough the primary paths are the highest contrib-\nutors to NA tasks, it is possible that collections\nof associated non-primary paths account for more\nof the verb number concept. We gauge the extent\nto which the primary paths alone are responsible\nfor the concept with compression/ablation exper-\n4756\niments. We show that the computations relevant\nto a speciﬁc path alone are sufﬁcient in maintain-\ning performance for the NA task. We compress\nthe model by specifying node sets to preserve, and\nintervene on the activations of all other nodes by\nsetting their activations to constant expected values\n(average over all samples). We choose the expected\nvalues instead of full ablation (setting them to zero),\nas ablation would nullify the function of Sigmoid\ngates. For example, to compress the model down\nto the red path in Figure 2, we only calculate the ac-\ntivation for gates ˜c0\nt and ˜c1\nt for each sample, while\nsetting the activation of all other ˜c,f,o,i to their\naverage values over all samples. In Table 2, we list\nvariations of the compression schemes based on\nthe following preserved node sets:\nC\ndef\n=\n{\nfl\nt,il\nt,ol\nt,˜cl\nt : tsub <t<t verb,l ∈{0,1}\n}\nCs\ndef\n=\n{\n˜c0\ntsub ,˜c1\ntsub\n}\nCi\ndef\n=\n{\n˜c0\ntint ,˜c1\ntint\n}\nCsi\ndef\n= Cs ∪Ci\nFor example, column Csi in Table 2 shows the ac-\ncuracy when the compressed model only retains\nthe primary path from both the subject and the in-\ntervening noun while the computations of all other\npaths are set to their expected values; while in Csi,\nall paths but the paths in Csi are kept.\nWe observe that the best compressed model is\nCi, where the primary path from the intervening\nnoun is left out; it performs even better than the\noriginal model; the increase comes from the cases\nwith attractors (PS, SP). This indicates that elimi-\nnating the primary path from the attractor improves\nthe model. The next best models apart from Care\nCs and Csi, where primary paths are kept. Com-\npressed models without the primary subject-verb\npath (Csi, Cs, Ci) have performances close to ran-\ndom guessing.\nObservation 4. Accuracy under path-based model\ncompression tests corroborate that primary paths\naccount for most of the subject number agreement\nconcept of the LSTM.\nBy comparing the SP and PS rows of Csi, Cs, Cs,\nand Ci, we observe the effect of attractors in mis-\nguiding the model into giving wrong predictions.\nSimilarly, we see that helpful nouns (SS, PP) help\nguide the models to make more accurate predic-\ntions, though this is not grammatically justiﬁed.\n5 Conclusions\nThe combination of ﬁnely-tuned attribution and gra-\ndient decomposition lets us investigate the handling\nof the grammatical number agreement concept at-\ntributed to paths across LSTM components. The\nconcentration of attribution to a primary path and\ntwo primary cell state neurons and its persistence in\na variety of short-term and long-term contexts, even\nwith confounding attractors, demonstrates that the\nconcept’s handling is, to a large degree, general and\nlocalized. Though the heuristic decisioning aspect\nof an LSTM is present in the large quantities of\npaths with non-zero inﬂuence, their overall contri-\nbution to the concept is insigniﬁcant as compared to\nthe primary path. Node-based compression results\nfurther corroborate these conclusions.\nWe note, however, that our results are based on\ndatasets exercising the agreement concept in con-\ntexts of a limited size. We speculate that the pri-\nmary path’s attribution diminishes with the length\nof the context, which would suggest that at some\ncontext size, the handling of number will devolve\nto be mostly heuristic-like with no signiﬁcant pri-\nmary paths. Though our present datasets do not\npose computational problems, the number of paths,\nat both the neuron and the gate level, is exponential\nwith respect to context size. Investigating longer\ncontexts, the diminishing dominance of the primary\npath, and the requisite algorithmic scalability re-\nquirements are elements of our ongoing work.\nWe also note that our method can be expanded\nto explore number agreement in more complicated\nsentences with clausal structures, or other syntac-\ntic/semantic signals such as coreference or gender\nagreement.\nAcknowledgement This work was developed\nwith the support of NSF grant CNS-1704845 as\nwell as by DARPA and the Air Force Research\nLaboratory under agreement number FA8750-15-\n2-0277. The U.S. Government is authorized to\nreproduce and distribute reprints for Governmental\npurposes not withstanding any copyright notation\nthereon. The views, opinions, and/or ﬁndings ex-\npressed are those of the author(s) and should not\nbe interpreted as representing the ofﬁcial views or\npolicies of DARPA, the Air Force Research Lab-\noratory, the National Science Foundation, or the\nU.S. Government. We gratefully acknowledge the\nsupport of NVIDIA Corporation with the donation\nof the Titan V GPU used for this work.\n4757\nReferences\nSebastian Bach, Alexander Binder, Gr ´egoire Mon-\ntavon, Frederick Klauschen, Klaus-Robert M ¨uller,\nand Wojciech Samek. 2015. On pixel-wise explana-\ntions for non-linear classiﬁer decisions by layer-wise\nrelevance propagation. PloS one, 10(7):e0130140.\nAnupam Datta, Shayak Sen, and Yair Zick. 2016. Algo-\nrithmic transparency via quantitative input inﬂuence:\nTheory and experiments with learning systems. In\n2016 IEEE symposium on security and privacy (SP),\npages 598–617. IEEE.\nKedar Dhamdhere, Mukund Sundararajan, and Qiqi\nYan. 2018. How important is a neuron? arXiv\npreprint arXiv:1805.12233.\nJames Fiacco, Samridhi Choudhary, and Carolyn Rose.\n2019. Deep neural model inspection and compari-\nson via functional neuron pathways. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 5754–5764.\nF. A. Gers and E. Schmidhuber. 2001. Lstm recur-\nrent networks learn simple context-free and context-\nsensitive languages. Trans. Neur. Netw., 12(6):1333–\n1340.\nMario Giulianelli, Jack Harding, Florian Mohnert,\nDieuwke Hupkes, and Willem Zuidema. 2018. Un-\nder the Hood: Using Diagnostic Classiﬁers to In-\nvestigate and Improve how Language Models Track\nAgreement Information.\nKlaus Greff, Rupesh K Srivastava, Jan Koutn´ık, Bas R\nSteunebrink, and J ¨urgen Schmidhuber. 2017. Lstm:\nA search space odyssey. IEEE transactions on neu-\nral networks and learning systems , 28(10):2222–\n2232.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. Colorless green re-\ncurrent networks dream hierarchically.\nSepp Hochreiter. 1998. The vanishing gradient prob-\nlem during learning recurrent neural nets and prob-\nlem solutions. International Journal of Uncer-\ntainty, Fuzziness and Knowledge-Based Systems ,\n6(02):107–116.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nDieuwke Hupkes, Sara Veldhoen, and Willem Zuidema.\n2018. Visualisation and’diagnostic classiﬁers’ re-\nveal how recurrent and recursive neural networks\nprocess hierarchical structure. Journal of Artiﬁcial\nIntelligence Research, 61:907–926.\nAndrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015.\nVisualizing and understanding recurrent networks.\narXiv preprint arXiv:1506.02078.\nYair Lakretz, German Kruszewski, Theo Desbordes,\nDieuwke Hupkes, Stanislas Dehaene, and Marco Ba-\nroni. 2019. The emergence of number and syn-\ntax units in lstm language models. arXiv preprint\narXiv:1903.07435.\nKlas Leino, Shayak Sen, Anupam Datta, Matt Fredrik-\nson, and Linyi Li. 2018. Inﬂuence-directed expla-\nnations for deep convolutional networks. In 2018\nIEEE International Test Conference (ITC), pages 1–\n8. IEEE.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of lstms to learn syntax-\nsensitive dependencies. Transactions of the Associa-\ntion for Computational Linguistics, 4:521–535.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nW James Murdoch, Peter J Liu, and Bin Yu. 2018. Be-\nyond word importance: Contextual decomposition\nto extract interactions from lstms. arXiv preprint\narXiv:1801.05453.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisser-\nman. 2013. Deep inside convolutional networks: Vi-\nsualising image classiﬁcation models and saliency\nmaps.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Pro-\nceedings of the 34th International Conference on\nMachine Learning-Volume 70 , pages 3319–3328.\nJMLR. org."
}