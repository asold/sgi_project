{
  "title": "CTAL: Pre-training Cross-modal Transformer for Audio-and-Language Representations",
  "url": "https://openalex.org/W3198549210",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A1990683121",
      "name": "Hang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2230419058",
      "name": "Wenbiao Ding",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115397990",
      "name": "Yu Kang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2912826920",
      "name": "TianQiao Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2240745335",
      "name": "Zhongqin Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096515818",
      "name": "Zitao Liu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2979476256",
    "https://openalex.org/W2191779130",
    "https://openalex.org/W2939710050",
    "https://openalex.org/W1651753422",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2916113431",
    "https://openalex.org/W2973192491",
    "https://openalex.org/W2937142395",
    "https://openalex.org/W3041561163",
    "https://openalex.org/W2962770129",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2962788625",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2950501607",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2981991061",
    "https://openalex.org/W2950761309",
    "https://openalex.org/W3099782249",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3015326490",
    "https://openalex.org/W2803193013",
    "https://openalex.org/W4287634424",
    "https://openalex.org/W2953084091",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2972909277",
    "https://openalex.org/W3148040514",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2937584914",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W2963710346",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2982223350",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2113507809",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W2972463723",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2973049979",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2131744502",
    "https://openalex.org/W2964010806",
    "https://openalex.org/W2738581557",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W3025035610",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W2968124245"
  ],
  "abstract": "Existing audio-language task-specific predictive approaches focus on building complicated late-fusion mechanisms. However, these models are facing challenges of overfitting with limited labels and low model generalization abilities. In this paper, we present a Cross-modal Transformer for Audio-and-Language, i.e., CTAL, which aims to learn the intra-modality and inter-modality connections between audio and language through two proxy tasks on a large amount of audio-and-language pairs: masked language modeling and masked cross-modal acoustic modeling. After fine-tuning our pre-trained model on multiple downstream audio-and-language tasks, we observe significant improvements across various tasks, such as, emotion classification, sentiment analysis, and speaker verification. On this basis, we further propose a specially-designed fusion mechanism that can be used in fine-tuning phase, which allows our pre-trained model to achieve better performance. Lastly, we demonstrate detailed ablation studies to prove that both our novel cross-modality fusion component and audio-language pre-training methods significantly contribute to the promising results. The code and pre-trained models are available at https://github.com/tal-ai/CTAL_EMNLP2021.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3966–3977\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n3966\nCTAL: Pre-training Cross-modal Transformer for Audio-and-Language\nRepresentations\nHang Li, Wenbiao Ding, Yu Kang, Tianqiao Liu, Zhongqin Wu, Zitao Liu∗\nTAL Education Group, Beijing, China\n{lihang4,dingwenbiao,kangyu,liutianqiao}@tal.com\n{wuzhongqin,liuzitao}@tal.com\nAbstract\nExisting approaches for audio-language task-\nspeciﬁc prediction focus on building compli-\ncated late-fusion mechanisms. However, these\nmodels face challenges of overﬁtting with lim-\nited labels and poor generalization. In this pa-\nper, we present a Cross-modal Transformer\nfor Audio-and-Language, i.e., CTAL, which\naims to learn the intra- and inter- modali-\nties connections between audio and language\nthrough two proxy tasks from a large num-\nber of audio-and-language pairs: masked\nlanguage modeling and masked cross-modal\nacoustic modeling. After ﬁne-tuning our\nCTAL model on multiple downstream audio-\nand-language tasks, we observe signiﬁcant\nimprovements on different tasks, including\nemotion classiﬁcation, sentiment analysis, and\nspeaker veriﬁcation. Furthermore, we design\na fusion mechanism in the ﬁne-tuning phase,\nwhich allows CTAL to achieve better perfor-\nmance. Lastly, we conduct detailed ablation\nstudies to demonstrate that both our novel\ncross-modality fusion component and audio-\nlanguage pre-training methods contribute to\nthe promising results. The code and pre-\ntrained models are available at https://\ngithub.com/tal-ai/CTAL_EMNLP2021.\n1 Introduction\nSpeech processing (SP) requires the understanding\nof a set of acoustic and language content, includ-\ning phonemes, tones, words and semantic mean-\nings. Different from human, who beneﬁt from self-\nlearning through real-world experiences, current\nSP methods are more like narrow experts relying\nheavily on a large number of task-speciﬁc human\nannotations and a small change may cause the learn-\ning process to start all over again. In recent years,\npre-training for single modality, such as language\nand audio signals, is widely explored due to its\nease-of-use and competent generalization to vari-\nous downstream tasks.\n∗ The corresponding author: Zitao Liu.\nIn the ﬁeld of NLP, pre-trained models, such\nas BERT (Devlin et al., 2019), RoBERTa (Liu\net al., 2019), XLNet (Yang et al., 2019) and GPT2\n(Radford et al., 2019), share the same idea of ﬁrst\nleveraging large-scale unlabeled corpus to perform\ncontextualized language model pre-training then\nﬁne-tuning the model to adapt to downstream tasks,\nsuch as machine reading comprehension (Lai et al.,\n2017), question answering (Rajpurkar et al., 2016)\nand natural language inference (Bowman et al.,\n2015), etc. Following the success of pre-trained\nmodels in NLP, BERT-like models are also applied\nto SP community (Schneider et al., 2019; Baevski\net al., 2020a,b), where robust audio representations\nare learned through an audio-style self-supervised\ncontext prediction task.\nDespite these inﬂuential unimodal methods, for\ntasks at the intersection of audio and language, such\nas speech emotion classiﬁcation (Livingstone and\nRusso, 2018; Busso et al., 2008), speaker veriﬁca-\ntion (Panayotov et al., 2015) and sentiment analy-\nsis (Zadeh et al., 2018), large-scale pre-training for\nthe modality-pair of audio and language is barely\nexplored. The previous attempt is to train task-\nspeciﬁc experts upon the concatenation of language\nrepresentations and audio representations in a late\nfusion manner (Ramirez et al., 2011; Glodek et al.,\n2011; Zadeh et al., 2017; Yoon et al., 2019, 2018;\nXu et al., 2019; Li et al., 2020b, 2021), without any\ngeneric audio-and-language pre-training. These\ntask-speciﬁc experts will suffer from the overﬁtting\nproblem when trained with limited data. Mean-\nwhile, due to the heterogeneity across language\nand audio modalities, late fusion of high-level rep-\nresentations lacks surface-level cross-modal align-\nment and complementation during the pre-training\nphase.\nTherefore, we propose CTAL, a pre-training\ncross-modal Transformer for audio-and-language\nrepresentations, and has shown its strong perfor-\nmance on three established audio-and-language\n3967\ntasks: emotion classiﬁcation (Busso et al., 2008),\nsentiment analysis (Zadeh et al., 2018) and speaker\nveriﬁcation (Panayotov et al., 2015). We propose\nmultimodal Transformer as our backbone model,\nwhich consists of two modules, a language stream\nencoding module which adapts word as input el-\nement, and a text-referred audio stream encod-\ning module which accepts both frame-level Mel-\nspectrograms and token-level output embeddings\nfrom the language stream encoding module as in-\nput elements. In order to learn both intra- and\ninter- modalities connections, we pre-train our\nmodel with two tasks: (1) masked language mod-\neling (MLM); and (2) masked cross-modal acous-\ntic modeling (MCAM). Different from unimodal\npre-training, e.g., masked acoustic modeling in\nMOCKINGJAY (Liu et al., 2020), our cross-modal\npre-training is able to reconstruct masked audio\nfeatures from both intra- and inter-modalities infor-\nmation. On the basis of our pre-trained model, a\nregularization term based on feature orthogonality\nis introduced during the model ﬁne-tuning stage,\nwhich is designed to ensure that features of differ-\nent modalities provide information from different\nperspectives. Please notice that this orthogonal reg-\nularization mechanism is general and not limited\nto audio-language tasks.\nThe main contributions of our paper are listed as\nfollows:\n• We present CTAL, a pre-training framework\nfor learning audio-and-language representa-\ntions with Transformer, which considers both\nintra- and inter- modalities connections. To\nthe best of our knowledge, we are the ﬁrst\nto introduce the pre-training cross audio-and-\nlanguage modalities.\n• We propose a novel cross-modality fusion\nmechanism at the ﬁne-tuning stage, which\nforces our pre-trained model learn compos-\nite features from different views.\n• Comprehensive empirical results demonstrate\nthat our CTAL achieves the state-of-the-art\nresults on various downstream SP tasks, such\nas emotion classiﬁcation, sentiment analysis,\nand speaker veriﬁcation. We conduct detailed\nablation studies and analysis to show the ef-\nfectiveness of our model components and our\npre-training strategies. To encourage repro-\nducible results, we put our code publicly avail-\nable at https://github.com/tal-ai/CTAL_\nEMNLP2021.\n2 Related Work\n2.1 Unimodal Pre-training\nThere has been a long interest around self-\nsupervised representation learning. Previous works\nhave explored alternative approaches to improve\nword embedding (Mikolov et al., 2013; Le and\nMikolov, 2014; Pennington et al., 2014), which\nis a low-level linguistic representation. After\nthat, pre-trained NLP models based on multi-layer\nTransformers, such as BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019), XLNet (Yang et al.,\n2019) and GPT2 (Radford et al., 2019), beneﬁt\nfrom context-sensitive representation learning on\nlarge-scale corpus, showing signiﬁcant improve-\nments in various downstream language understand-\ning tasks. Self-supervised learning in speech pro-\ncessing has also shown increasing promise. Follow-\ning BERT, many approaches (Jiang et al., 2019; Liu\net al., 2021, 2020; Chi et al., 2021) are proposed\nto learn high-level acoustic representations rather\nthan surface features such as log Mel-spectrograms\nor waveform, which can reveal the abundant infor-\nmation within audio signals.\n2.2 Multimodal Pre-training\nWhile pre-training for audio-and-language repre-\nsentations has rarely been studied, several attempts\nhave been made to pre-train models for vision-\nand-language tasks on visual question answering\n(Antol et al., 2015) and visual commonsense rea-\nsoning (Zellers et al., 2019) datasets. In general,\nthese vision-and-language pre-training methods\ncan be divided into two categories, according to\ntheir different encoder architectures as follows: (a)\nprior works like ViLBERT (Lu et al., 2019) and\nLXMERT (Tan and Bansal, 2019), apply two uni-\nmodal networks to encode input text and images\nrespectively and adapt cross-modal interactions in a\nsymmetric fusion manner; (b) the other category of\npre-training frameworks like VisualBert (Li et al.,\n2019), Unicoder-VL (Li et al., 2020a) and UNITER\n(Chen et al., 2020), concatenate vision and lan-\nguage features as a uniﬁed single-stream input and\nutilize a universal encoder to learn joint multimodal\nrepresentations.\nHowever, transfer above algorithms directly\nfrom vision-and-language to audio-and-language\nﬁeld faces challenges, including: (1) uniﬁed archi-\ntecture is not suitable for audio-language modali-\n3968\nties, since both text and audio signals are generally\nlong sequences, and cross-modal aggregation at\nthe very beginning phase with Transformer self-\nattention mechanism will lead to higher compu-\ntational complexity; (2) audio signals are more\ninformative than language texts, which contain\nboth semantic information of text content and per-\nsonal feelings. Thus, it is not suitable to apply the\nsymmetric cross-modal fusion module proposed in\nprior vision-and-language pre-training researches.\nBased on these facts, we design our backbone\nmodel with a language stream encoding module\nand a text-referred audio stream encoding module,\nwhich allow necessary intra- and inter-modality\nconnections during pre-training with less computa-\ntional cost.\nThe closest work to our approach is from Haque\net al. (2019) and our approach differs from it in\ntwo aspects. First, we use a more explicit, multi-\ncomponent design for the cross-modality connec-\ntions (i.e., with a text-referred audio stream encod-\ning module and a novel cross-modality fusion com-\nponent). Second, we employ different pre-training\ntasks which accept both text and audio frames as\ninput to conduct contextualized masked language\nmodeling and masked cross-modal acoustic model-\ning tasks.\n3 Our Approach\nIn this section, we ﬁrst present our cross-modal\npre-training framework CTAL, including details of\ntext and audio pre-processing and encoding mod-\nules for separate modalities. Then we present our\npre-training tasks. In the end, we propose a novel\nfusion mechanism which can be utilized in the ﬁne-\ntuning stage. Following conventions, we use bold\nupper case letters to represent matrices and bold\nlower case letters to represent vectors.\n3.1 The CTAL Framework\nWe build our cross-modal Transformer by extend-\ning the original Transformer (Vaswani et al., 2017)\ninto the multimodal paradigm. As shown in Fig-\nure 1, CTAL takes audio sequences and their corre-\nsponding text sequences as the input. Each audio\nsequence is represented as a sequence of frames,\nand each text sequence is represented as a sequence\nof tokens. Then we encode the input to the linguis-\ntic embedding and audio embedding, and feed them\ninto a text encoding module and a text-referred\naudio encoding module respectively to generate\nﬁnal language representations and text referred au-\ndio representations. Following the notations used\nby Vaswani et al. (2017), we adapt Q, K and V\nas queries, keys and values for attention mecha-\nnism, MultiHead(Q, K, V) as multi-head attention,\nFFN(X) as position-wise feed forward networks\nand LayerNorm(X) as layer normalization.\n3.1.1 Input Embeddings\nLinguistic Embedding. To encode any input text\nwith a modest size (30K units) of subword vo-\ncabulary, we follow the text pre-processing of\nRoBERTa, which tokenizes each input text w =\n{w0, w1, ..., wT }with byte-level byte-pair encod-\ning (BBPE) (Radford et al., 2019). Besides, we\nalso add the special tokens <s> and </s> to repre-\nsent start and end tokens. Then we sum up each\ntoken embedding and its corresponding position\nembedding to get the ﬁnal input token embeddings\n{ew0, ew1, ...,ewT }for language modality. T is\nthe total length of input tokens.\nAudio Embedding. The input audio signal is ﬁrst\ntransformed into frames of width 50ms and step\n12.5ms. Then the 80 dimension Mel-spectrograms\nare extracted from each frame and concatenated\nwith their ﬁrst order derivatives, making the fea-\nture dimension to 160. In this way, the raw signal\nis converted into sequence of frame-level acoustic\nsurface features {a0, a1, ..., aT}, where T is the\ntotal number of frames. For simplicity, we denote\nthis audio feature sequence as input acoustic fea-\ntures after this section. Then, we feed these surface\nfeatures to a projection layer and add them with\nthe position embeddings to obtain the input audio\nembeddings {ea0, ea1, ...,eaT}for audio modality.\n3.1.2 Text Encoding Module\nAs shown in Figure 1, we apply the original Trans-\nformer encoder to language stream inputs, each\nlanguage stream encoding layer consists of one\nmulti-head self-attention sublayer and one position-\nwise feed forward sublayer. We stack N such lan-\nguage encoding layer and use the output of k-th\nlayer as the input to the (k + 1)-th layer. We ini-\ntialize H0\nw with {ew0, ew1, ...,ewT }and obtain the\nlanguage representations for the k-th layer with the\nfollowings:\nˆHk+1\nw = MultiHead(Q = Hk\nw, K = Hk\nw, V = Hk\nw)\n˜Hk+1\nw = LayerNorm(ˆHk+1\nw + Hk\nw)\nHk+1\nw = LayerNorm(FFN(˜Hk+1\nw ) +˜Hk+1\nw )\n3969\n<mask><s> there was in<mask> the art … </s>Token\nEmbedding\nLanguage\nPosition\nEmbedding\n30 1 2 54 6 7 …\n+ + + + + + + + ++\nLanguage\nStream\nEncoding\nModule\nMulti-head Self-Attention\nAdd & Layer Norm\nPosition-wise Feed Forward\nAdd & Layer Norm\nx N\nha0\nLanguage\nRepresentations\nno legality\nMasked Language Modeling\n+ + + + + + + + +++\n0 1 2 … 24 25 26 27 …28T\nAudio\nEmbedding\nAudio\nPosition\nEmbedding\nNx\nkey\nValue\nMulti-head Self-Attention\nAdd & Layer Norm\nMulti-head Cross-modal Attention\nAdd & Layer Norm\nPosition-wise Feed Forward\nAdd & Layer Norm\nx N Text-referred\nAudio\nStream\nEncoding\nModule\n…hw0\nhw1\nhw2\nhw3\nhw4\nhw5\nhw6\nhw7\nhwT\nha1\nha2\nha24\nha25\nha26 ha27\nha28 ha/u1D4AF…\nText-referred\nAudio\nRepresentations\nMasked Cross-modal Acoustic Modeling\n…\nFrame-level\nMel-spectrograms\nFeatures\n…\n/u1D4AF\n…\n…\n…\n…\nFigure 1: The proposed CTAL pre-training framework.\nWe get the ﬁnal outputHN\nw ∈RT×dw from our lan-\nguage stream encoding module, where dw denotes\nthe hidden size of the language stream represen-\ntations. The ﬁrst token of every text sequence is\nalways a special start token (<s>), and the ﬁnal\nhidden state corresponding to this token is always\nused as the aggregated text sequence representation\nfor classiﬁcation tasks.\n3.1.3 Text-Referred Audio Encoding Module\nFor text-referred audio encoding module, we\nﬁrst initialize hidden representations H0\na with\n{ea0, ea1, ...,eaT}, and pass them to a stack of N\ntext-referred audio encoding layers to acquire the\nﬁnal audio stream representations HN\na .\nOur text-referred audio encoding module is dif-\nferent from the original Transformer decoder by\nmodifying two kinds of multi-head attention mech-\nanism. Firstly, in order to learn the bi-directional\nintra-modality representation for audio, we get rid\nof the future mask in the masked multi-head self-\nattention. Speciﬁcally for the (l + 1)-th layer:\nˆHl+1\na = MultiHead(Q = Hl\na, K = Hl\na, V = Hl\na)\n˜Hl+1\na = LayerNorm(ˆHl+1\na + Hl\na)\nSecondly, we apply multi-head cross-modal atten-\ntion which accepts the ﬁnal language stream repre-\nsentations as keys and values in each layer to apply\nthe inter-modality interactions:\n¯Hl+1\na = MultiHead(Q = ˜Hl+1\na , K = HN\nw , V = HN\nw )\n¨Hl+1\na = LayerNorm(¯Hl+1\na + ˜Hl+1\na )\nHl+1\na = LayerNorm(FFN(¨Hl+1\na ) +¨Hl+1\na )\nFinally, we obtain the text-referred audio repre-\nsentation of N-th layer HN\na ∈RT×da, where da\ndenotes the hidden size of the audio stream repre-\nsentations.\n3.2 Pre-training Tasks\n3.2.1 Masked Language Modeling\nFor language stream, we take the MLM task for\nlanguage intra-modality learning. As shown in Fig-\nure 1, the MLM task setup is almost the same as\nRoBERTa (Liu et al., 2019), we dynamically mask\nout the input tokens with a probability of 15%.\nMasked tokens are replaced with a special <mask>\ntoken 80% of the time, a random token 10%, and\nunaltered 10%. The goal of MLM is to predict\nthese masked tokens based on the observed tokens.\nHere, we do not introduce acoustic information for\nmasked token prediction, since semantic informa-\ntion of text can be well enough captured through\nlanguage input. It is redundant to introduce cross-\nmodal inputs here and it is demonstrate through the\nablation study discussed in Section 5.1.\n3.2.2 Masked Cross-modal Acoustic\nModeling\nFor audio stream, we propose MCAM to train the\ntext-referred audio representations. Prior research\nby Baevski et al. (2020b) indicates that the per-\nformance of acoustic pre-trained models on down-\nstream tasks is improved with the increment in size\nof continuous masked frames during pre-training\nphase. However, due to the complexity of audio\nsignals, the long-term dependencies in audio se-\n3970\nquences is hard to be captured with acoustic fea-\ntures alone. To mitigate that problem, we propose\nMCAM to capture effective information of audio\nthrough learning both intra- and inter- modalities\nconnections between audio and language.\nTo implement MCAM, we ﬁrst split the audio\nin separate segments according to C consecutive\nframes per segment, where C is uniformly sam-\npled from 20 to 50. Then we randomly select 15%\nof these segments and for each of them, we mask\nit all to zero 80% of the time, replace it with the\nother C randomly selected frames within the au-\ndio 10% of the time, and keep it unchanged for\nthe remaining cases. In this manner, we prevent\nthe model exploiting local smoothness of acoustic\nframes and the model is required to make infer-\nence based on global information rather than local\nmessages. Finally, the goal is to reconstruct these\nmasked acoustic features based on the remaining\nacoustic features and the language stream prompt,\nby minimizing the ℓ1 loss between the original\nmasked acoustic features and the predicted ones.\nOverall, our ﬁnal objective is to minimize the\nsum of the loss functions above.\n3.3 Fine-Tuning CTAL\nCTAL is designed to be a generic pre-training\nmodel for various audio-language tasks. It is\nrelatively simple to ﬁne-tune CTAL for various\ndownstream tasks with just one additional output\nlayer. To further combine information from dif-\nferent modalities, we propose a novel and ﬂexible\nfusion mechanism at the ﬁne-tuning stage. We de-\nnote HN\nw ∈RT×d and HN\na ∈RT×d as the ﬁnal\nrepresentation from text encoding module and text-\nreferred audio encoding module. We assume that\nboth modules have the same hidden size d.\nIn SP tasks, we use the compressed hidden vec-\ntors to represent both the language and audio input\nsequences. Following the idea from Wang (2018),\nwhich proves that max pooling mechanism tends to\nmake false negatives while attention pooling mech-\nanism prefers making false positives, we come up\nwith both attention-pooling layer and max-pooling\nlayer to let them complement each other. After\napplying attention-pooling and max-pooling to au-\ndio stream ﬁnal representations HN\na , we obtain\nhattn\na ∈Rd and hmax\na ∈Rd respectively.\nhattn\na = Softmax(vattn\na ·tanh(Wattn\na ·HN\na )) ·HN\na\nhmax\na = MaxPool(HN\na )\nwhere vattn\na and Wattn\na are parameters in the audio\nattention-pooling layer.\nAs discussed in Section 3.1.2, for language\nstream, we adapt the ﬁnal hidden state of the start\ntoken hw0 ∈Rd as the aggregated text sequence\nrepresentation hattn\nw for attention-pooling, and we\nconduct additional max-pooling for text stream out-\nput HN\nw to obtain hmax\nw .Then we fuse the aggre-\ngated sequence representations from two modali-\nties as follows:\nhfuse = (hattn\na + hattn\nw ) ⊕(hmax\na + hmax\nw )\nwhere ⊕denotes the vector concatenation, and\nthe ﬁnal hidden state hfuse is always used as the\naudio-and-language representation for classiﬁca-\ntion tasks.\n3.3.1 Orthogonal Regularization\nOne key characteristic of multimodal learning is the\ngenerated representations of different modalities\nare supposed to depict a sample from different point\nof views. In order to encourage the two modules\nto get representations from different perspectives,\nwe introduce a regularization term which aims at\nachieving the representation orthogonality during\nthe ﬁne-tuning stage:\nLOrth = |hattn\na\nT hattn\nw |\n∥hattna ∥∥hattnw ∥+ |hmax\na\nT hmax\nw |\n∥hmaxa ∥∥hmaxw ∥\n4 Experimental Setup and Results\n4.1 Pre-training Details\nWe pre-train our CTAL on the public dataset Lib-\nriSpeech (Panayotov et al., 2015), which is a\ndataset of reading English speech, including both\naudio recordings and corresponding authorized\ntranscripts. It has 7 subsets in total (train-clean-\n100, train-clean-360, train-other-500, dev-clean,\ndev-other, test-clean, test-other). The subsets with\n“clean” in their names contain audios with higher\nrecording quality, while the other subsets have low-\nquality recordings. We use all three training subsets\nfor pre-training, including approximately 960 hours\nof speech and 280K utterances.\nFollowing Radford et al. (2019), we consider\ntraining a BBPE tokenizer on the LibriSpeech\ncorpus with additional special tokens (<s>, </s>,\n<mask>, <pad>) as our language stream tokeniz-\ners. We tokenize the input text into token se-\nquence as described in Section 3.1.1. For au-\ndio stream, we use Librosa (McFee et al., 2015),\n3971\nwhich is a well-established audio analysis Python\npackage, to extract the 160-dimension input acous-\ntic feature for each frame as described in Sec-\ntion 3.1.1. We denote the number of layers (i.e.,\nlanguage stream encoding layer and text-referred\naudio stream encoding layer) as N, the number of\nself-attention heads as A, and the number of hid-\nden size as H. We primarily report results on two\nmodel sizes: CTALBASE (N=3, A=12, H=768)\nand CTALLARGE (N=6, A=12, H=768). The to-\ntal number of parameters for CTALBASE is 60M\nand 110M for CTALLARGE. We take the Adam\n(Kingma and Ba, 2015) as the optimizer with initial\nlearning rate of 5e-5 and a linear-decayed learning\nrate schedule with warm up (Devlin et al., 2019).\nWe pre-train our model using 4 16G-V100 GPUs\nwith a batch size of 16 for 1,000,000 steps, and the\nwhole pre-training process takes roughly 48 hours.\n4.2 Fine-tuning on Downstream Tasks\nWe transfer our pre-trained CTAL model to three\nestablished SP tasks, with simple and necessary\nmodiﬁcations on the output layers, loss function\nand training strategy.\n4.2.1 Emotion Classiﬁcation\nIn emotion classiﬁcation task, given a speech clip,\nthe model is asked to predict which emotion cat-\negory the speech belongs to. Here, we conduct\nexperiments on the widely-used dataset IEMOCAP\n(Busso et al., 2008). The dataset was recorded\nfrom ten actors, divided into ﬁve sessions, and\neach session has dialogues between two speakers\nwith different genders. The dataset contains au-\ndio, transcriptions, and video recordings, and we\nonly use audio and transcriptions in our study. The\nrecorded dialogues have been sliced into utterances\nand labeled in 10 categories by three annotators\nand utterances without any text content are ﬁltered\nout in our experiment. For consistent comparison\nwith previous works, we follow the settings with\nXu et al. (2019), which use four emotions (angry,\nhappy, neutral and sad) for classiﬁcation and per-\nform 5-fold cross-validation over sessions, where\neach session is used as the test set in turn and re-\nmaining as training dataset. We adopt two widely\nused metrics for evaluation: weighted accuracy\n(W A) that is the overall classiﬁcation accuracy and\nunweighted accuracy (UA) that is the average re-\ncall over all four classes. We report the averaged\nW A and UA over the 5-fold cross-validation exper-\niments, and higher WA and UA results represent\nMethods W A↑ UA ↑\nLSTM_alignment (Xu et al., 2019) 0.6900 0.7014\nMRDE (Yoon et al., 2018) 0.6702 0.6764\nMHA (Yoon et al., 2019) 0.6780 0.6880\nCTALBASE 0.7286 0.7370\nCTALLARGE 0.7395 0.7463\nTable 1: Comparison to the SOTA methods on the\nIEMOCAP dataset.\nbetter model performances.\nTo ﬁne-tune on IEMOCAP, we represent the\ninput sequence (for a pair of audio and text) as\ndescribed in Section 4.1, and use the ﬁnal hid-\nden vector hfuse as the audio-and-language rep-\nresentation. The only new parameters introduced\nduring ﬁne-tuning are classiﬁcation layer weights\nW ∈R4×d and CTAL ﬁne-tuning is driven by the\ncross-entropy loss between the predicted class and\nthe gold label. We use a batch size of 4 and ﬁne-\ntune for 20 epochs over each fold with one 16G-\nV100 GPU. We take AdamW (Loshchilov and Hut-\nter, 2018) as the optimizer in ﬁne-tuning stage, the\nlearning rate is initialized as 1e-5 and we apply a co-\nsine annealing learning rate schedule (Loshchilov\nand Hutter, 2017) to reach the optimum.\nWe select multiple models that claim to achieve\nthe SOTA results on IEMOCAP dataset as our\nbaselines. Please notice that previous methods\nare speciﬁcally designed for the task with no pre-\ntraining stage. Xu et al. (2019) aims to produce\nmore strong multimodal representations by learn-\ning the alignment between speech frames and\ntext words using an attention mechanism, i.e.,\n“LSTM_alignment”. Yoon et al. (2018) uses a\ndual-RNNs to encode the information from audio\nand text separately, then combines them by simple\nrepresentations concatenation to predict emotion\nclasses, i.e., “MDRE”. Yoon et al. (2019) proposes\na multi-hop attention mechanism to infer the cor-\nrelation between audio and language modalities\nbased on the output hidden representations of two\nbi-directional long short-term memory encoders,\nand output the ﬁnal classiﬁcation result from the\nconcatenation of audio and language representa-\ntions, i.e., “MHA”.\nTable 1 presents our experimental results on\nIEMOCAP dataset. Since some prior works ex-\nperiment with different train/test split, we re-\nimplement baseline models with their published\n3972\nMethods Acc2 ↑ F1 ↑ MAE ↓ Corr ↑\nMulT 0.7966 0.8008 0.6367 0.6292\nCTALBASE 0.8036 0.8055 0.6061 0.6828\nCTALLARGE 0.8077 0.8101 0.6027 0.6809\nTable 2: Comparison to the SOTA methods on the\nCMU-MOSEI dataset.\ncodes12. Both CTALBASE and CTALLARGE out-\nperform all three baselines by a substantial margin,\nobtaining 3.86% and 4.95% respective absolute\nWA improvement, and 3.56% and 4.49% respec-\ntive absolute UA improvement over the prior state\nof the art.\n4.2.2 Sentiment Analysis\nThe goal of the sentiment analysis task is to predict\nthe degree of positive and negative sentiment. Com-\npared to the emotion classiﬁcation task, sentiment\nanalysis is a regression task rather than a classiﬁ-\ncation task. We adopt CMU-MOSEI (Zadeh et al.,\n2018) dataset for evaluation, which contains 23,454\nmovie review video clips from YouTube. We use\nonly audio and corresponding transcriptions as in-\nput in our experiments. Each sample in the dataset\nis labeled with a sentiment score from -3 (strongly\nnegative) to 3 (strongly positive) by human annota-\ntors. We follow the same experimental protocol as\nMuIT (Tsai et al., 2019), with the same train/test\ndata split and the same evaluation metrics, which\nincludes two classiﬁcation metrics: (1) binary ac-\ncuracy (i.e., Acc2: accuracy over positive/negative\nsentiments classiﬁcation), and F1 score; (2) two re-\ngression metrics: mean absolute error (MAE), and\nthe Pearson correlation coefﬁcient (Corr) between\nmodel’s predictions and human annotations. Since\nthe prior top results reported on the CMU-MOSEI\ndataset are all achieved using all three modalities,\nso does MulT3, we prune the vision-related com-\nponents in MulT and re-train the model using only\naudio and text information.\nDuring ﬁne-tuning on sentiment analysis, we in-\ntroduce additional parameters w ∈Rd to project\nthe ﬁnal hidden representation hfuse to the senti-\nment score, and the model is trained to minimize\nthe ℓ1 loss between the predicted scores and the\n1MDRE:https://github.com/david-yoon/\nmultimodal-speech-emotion.git\n2LSTM_alignment:https://github.com/didi/\ndelta\n3MulT:https://github.com/yaohungt/\nMultimodal-Transformer\ngold annotations. The other ﬁne-tuning settings\nover CMU-MOSEI are almost the same as IEMO-\nCAP. As show in Table 2, we observe improve-\nments across all 4 metrics for CTAL over MulT\nbaseline under both base and large settings.\n4.2.3 Speaker Veriﬁcation\nSpeaker veriﬁcation focuses on verifying the\nspeaker identity of an utterance through compar-\ning it with the pre-recorded voiceprint information.\nIn this experiment, we adopt LibriSpeech (Panay-\notov et al., 2015) for evaluation, which includes\n292K utterances collected from more than 2,438\nspeakers. Following the same experiment setting\nwith prior works (Wan et al., 2018; Jung et al.,\n2019), we ﬁne-tune our pre-trained model with\nall training splits (train-clean-100, train-clean-360\nand train-other-500), and evaluate it with test-clean\npart, which contains 40 brand new speakers to the\ntraining part. Please note that, although the train\nset for our speaker veriﬁcation task is identical\nwith the one we used for pre-training, the speaker\nidentity information and test-clean data are not re-\nleased during the pre-training process. Thus, it is\nfair to perform comparisons between our models\nwith other prior works. We add a classiﬁer over the\nhead of fused embeddings hfuse and adopt cross-\nentropy loss to ﬁne-tune it. The output size of the\nclassiﬁer is same to the number of unique speakers\nin train set.\nMethods EER ↓\nGE2E (Wan et al., 2018) 0.0379\nRawNet (Jung et al., 2019) 0.0253\nCTALBASE 0.0194\nCTALLARGE 0.0155\nTable 3: Comparison to the SOTA methods on the Lib-\nriSpeech dataset.\nFor evaluation, we utilize the representation be-\nfore classiﬁer as the input audio’s identity embed-\nding. Cosine distance of each paired audio embed-\ndings is used as the indicator for the ﬁnal decision.\nSimilar to prior studies, we report the equal er-\nror rate (EER) as the evaluation metric, and lower\nEER represents better model performance. We\nchoose two SOTA models as our baselines (Wan\net al., 2018; Jung et al., 2019) where GE2E (Wan\net al., 2018) designs a general loss function that\nemphasizes examples that are difﬁcult to verify\nat each step of the training process, and RawNet\n(Jung et al., 2019) proposes an end-to-end network\n3973\nthat input raw audio waveforms to extract speaker\nembeddings. The comparison results are shown\nin Table. 3. From the table, we observe that our\nCTALBASE outperforms GE2E and RawNet by\n1.85% and 0.59% respectively, and CTALLARGE\noutperforms two baselines by 2.24% and 0.98%\nrespectively.\n5 Analysis\n5.1 Ablation Studies\nWe present the ablation result of different key com-\nponents in CTAL in Table 4. For experimental\nefﬁciency, all of the ablation experiments are con-\nducted with CTALLARGE.\nOverall, the pre-training of CTAL improves the\nperformance across all the three downstream tasks\n(by comparing settings “w/o Pre-training” and\nCTALLARGE), and we ﬁnd that CTALLARGE sig-\nniﬁcantly outperforms CTALBASE across all tasks.\nBesides, with the increment in the size of pre-\ntraining data, CTAL achieves better performances\non all evaluation metrics except Acc2 and F1 in\nsentiment analysis task (by comparing settings (a)\n“pre-train with train-clean-360” andCTALLARGE).\nThe effectiveness of the asymmetry encoder design\nfor audio-and-language representations is demon-\nstrated by comparing CTALLARGE to LXMERT\nand VisualBERT, where all models are designed\nto have similar size of parameters.\nBy comparing (b) “w/o MLM” to “w/o Pre-\ntraining” and (c) “w/o MCAM” to “w/o Pre-\ntraining”, we see the beneﬁts of pre-training on\nMCAM and MLM respectively. However, by com-\nparing (b) and (c) with CTALLARGE, both of them\nsuffer dramatically performance decrease over all\ndownstream tasks. This indicates the importance of\njoint-training with MLM and MCAM tasks during\nthe pre-training stage. So far, the effectiveness of\npre-training and different tasks are demonstrated.\nSetting (d) “w/o Orthogonal Fusion” re-\nmoves our proposed cross-modality orthogonal-\nfusion component and by comparing it with\nCTALLARGE, we observe that the model’s per-\nformance decreases on all three downstream tasks,\nwhich proves its effectiveness. Setting (e) “w/o au-\ndio Outputs” and (f) “w/o language Outputs” only\nuse the output embeddings from either audio or\nlanguage encoding module for downstream ﬁne-\ntuning. Through comparing them to (d), we ﬁnd\neach kind of embeddings contributes to the Audio-\nand-Language tasks and the best performance is\nachieved through the appropriate fusion of both\nparts. At last, setting (g) “w/o Cross-modal Pre-\ntraining” utilizes unimodal pre-training models,\nRoBERTa and Mockingjay pre-trained with Lib-\nriSpeech dataset, and fuses their output embed-\ndings for the downstream tasks. To be noticed,\n“w/o Cross-modal Pre-training” is chosen to have\nthe same model size as CTALLARGE for the com-\nparison purpose. Additionally, we present the\nperformance of each single modality pre-trained\nmodel, Mockinjay and RoBERTa, to demonstrate\nthe advantages of multimodal pre-training. From\nthe results, we ﬁnd our approach still holds better\nperformance across all three tasks, which proves\nthe importance of introducing inter-modality learn-\ning during pre-training phase.\n5.2 Effect of Pre-training\nWe analyze the effect of pre-trained CTAL by visu-\nalizing its performance on downstream tasks versus\ndifferent proportions of training data being used.\n10% 30% 50% 70% 100%\nAmount of training data\nWA\n0.50\n0.59\n0.64\n0.66\n0.67\n0.55\n0.64\n0.66\n0.69 0.68\n0.58\n0.67\n0.70\n0.71\n0.74\nMDRE\nMHA\nCTAL\n(a) W A vs proportions\n10% 30% 50% 70% 100%\nAmount of training data\nUA\n0.50\n0.59\n0.64\n0.66\n0.68\n0.54\n0.64\n0.67\n0.69 0.69\n0.59\n0.68\n0.70\n0.73\n0.75\nMDRE\nMHA\nCTAL (b) UA vs proportions\nFigure 2: Results of models on different proportions of\ntraining data on IEMOCAP in terms of W A and UA.\n10% 30% 50% 70% 100%\nAmount of training data\nMAE\n0.73\n0.69\n0.66 0.65\n0.64\n0.68\n0.64\n0.61\n0.61\n0.60\nMulT\nCTAL\n(a) MAE vs proportions\n10% 30% 50% 70% 100%\nAmount of training data\nCorrelation\n0.51\n0.58\n0.62 0.62\n0.63\n0.59\n0.65\n0.67\n0.68 0.68\nMulT\nCTAL (b) Correlation vs proportions\nFigure 3: Results of models on different proportions of\ntraining data on CMU-MOSEI in terms of MAE and\nCorrelation.\nIn Figure 2a and Figure 2b, we show the perfor-\nmance on IEMOCAP dataset. First of all, on both\nmetrics, CTAL outperforms all baselines across dif-\nferent proportions of training data. Secondly, the\n3974\nSettings MLM MCAM Orthognal\nFusion\nCross-\nmodal\nPre-train\nText\nOutputs\nAudio\nOutputs\nPre-train\n960\nHours\nPre-train\n360\nHours\nEmotion\nClassiﬁcation\n(IEMOCAP)\nSentiment\nAnalysis\n(MOSEI)\nSpeaker\nVeriﬁcation\n(LibriSpeech)\nW A↑ UA ↑ Acc2 ↑ F1 ↑ MAE ↓ Corr ↑ EER ↓\nw/o pre-training √ √ √ 0.7004 0.7110 0.7804 0.7809 0.6654 0.6086 0.0366\n(a) √ √ √ √ √ √ √ 0.7262 0.7386 0.8127 0.8150 0.6050 0.6804 0.0204\n(b) √ √ √ √ √ √ 0.7077 0.7185 0.7834 0.7842 0.6629 0.6096 0.0244\n(c) √ √ √ √ √ 0.7080 0.7171 0.7812 0.7809 0.6442 0.6440 0.0327\n(d) √ √ √ √ √ √ 0.7338 0.7444 0.7948 0.7939 0.6035 0.6832 0.0176\n(e) √ √ √ √ √ 0.6497 0.6586 0.7804 0.7795 0.6235 0.6639 -\n(f) √ √ √ √ √ 0.7315 0.7412 0.7989 0.7915 0.6065 0.6750 0.0190\n(g) √ (MAM) √ √ √ √ 0.7116 0.7270 0.7820 0.7834 0.6323 0.6527 0.0306\nMockingjay (MAM) √ √ 0.5505 0.5672 0.6887 0.7199 0.8056 0.3556 0.0551\nRoBERTa √ √ √ 0.6377 0.6411 0.7451 0.7412 0.6598 0.5760 -\nLXMERT (LXMERT) √ √ √ √ √ 0.7145 0.7222 0.7749 0.7740 0.6405 0.6430 0.0320\nVisualBERT (VisualBERT) √ √ √ √ √ 0.6778 0.6848 0.7769 0.7722 0.6621 0.6243 0.0375\nCTALBASE\n√ √ √ √ √ √ √ 0.7286 0.7370 0.8036 0.8055 0.6061 0.6828 0.0194\nCTALLARGE\n√ √ √ √ √ √ √ 0.7395 0.7463 0.8077 0.8101 0.6027 0.6809 0.0155\nTable 4: The results for performing ablation study with CTALLARGE. Notation “(MAM)” represents the acoustic\nstream encoding module is pre-trained with mask audio modeling (MAM) task. The EER is not reported for setting\n(d) and RoBERTa, because it does not make sense to perform speaker veriﬁcation with only semantic embeddings.\nFigure 4: Visualization of 10 speakers embeddings via\nt-SNE. Different colors represent different speakers.\nﬁgures show that CTAL only needs half the amount\nof training data to achieve a better performance\nthan baselines. The results on MOSEI dataset are\nshown in Figure 3a and Figure 3b, and the same\nconclusion can also be drawn.\nIn Figure 4, we use t-SNE (Van der Maaten and\nHinton, 2008) to visualize the speaker embeddings\nin test set extracted from pre-trained CTAL without\ntraining on downstream tasks. Here, each point\nrepresents an utterance and different speakers have\ndifferent colors. We can observe that the model can\nhave some capability to distinguish utterances of\ndifferent speakers with only pre-training.\n6 Conclusion\nIn this work, we proposed CTAL, a novel pre-\ntraining cross-modal Transformer to learn effec-\ntive representations for audio-and-language tasks.\nIt is pre-trained with two pre-training tasks on a\nlarge-scale dataset of audio-and-language pairs. Ex-\ntensive empirical analysis demonstrates that our\npre-trained model improves various speech under-\nstanding performance signiﬁcantly and achieves\nnew SOTA results. Besides, we show the effec-\ntiveness of different model components and the\ncompetent generalization capability via detailed\nablation studies and analysis.\nAcknowledgements\nThis work was supported in part by National\nKey R&D Program of China, under Grant No.\n2020AAA0104500 and in part by Beijing Nova\nProgram (Z201100006820068) from Beijing Mu-\nnicipal Science & Technology Commission.\nReferences\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. 2015. Vqa: Visual question an-\nswering. In Proceedings of the IEEE International\nConference on Computer Vision, pages 2425–2433.\nAlexei Baevski, Steffen Schneider, and Michael Auli.\n2020a. vq-wav2vec: Self-supervised learning of dis-\ncrete speech representations. In 8th International\nConference on Learning Representations.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael Auli. 2020b. wav2vec 2.0: A frame-\nwork for self-supervised learning of speech represen-\ntations. In Advances in Neural Information Process-\ning Systems 33: Annual Conference on Neural In-\nformation Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual.\n3975\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642.\nCarlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe\nKazemzadeh, Emily Mower, Samuel Kim, Jean-\nnette N Chang, Sungbok Lee, and Shrikanth S\nNarayanan. 2008. Iemocap: Interactive emotional\ndyadic motion capture database. Language Re-\nsources and Evaluation, 42(4):335–359.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. UNITER: Universal image-text\nrepresentation learning. In European Conference on\nComputer Vision, pages 104–120. Springer.\nPo-Han Chi, Pei-Hung Chung, Tsung-Han Wu, Chun-\nCheng Hsieh, Yen-Hao Chen, Shang-Wen Li, and\nHung-yi Lee. 2021. Audio albert: A lite bert for\nself-supervised learning of audio representation. In\nIEEE Spoken Language Technology Workshop, SLT\n2021, Shenzhen, China, January 19-22, 2021, pages\n344–350.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186.\nMichael Glodek, Stephan Tschechne, Georg Lay-\nher, Martin Schels, Tobias Brosch, Stefan Scherer,\nMarkus Kächele, Miriam Schmidt, Heiko Neumann,\nGünther Palm, et al. 2011. Multiple classiﬁer sys-\ntems for the classiﬁcation of audio-visual emotional\nstates. In International Conference on Affective\nComputing and Intelligent Interaction , pages 359–\n368. Springer.\nAlbert Haque, Michelle Guo, Prateek Verma, and\nLi Fei-Fei. 2019. Audio-linguistic embeddings for\nspoken sentences. In IEEE International Confer-\nence on Acoustics, Speech and Signal Processing ,\npages 7355–7359. IEEE.\nDongwei Jiang, Xiaoning Lei, Wubo Li, Ne Luo, Yux-\nuan Hu, Wei Zou, and Xiangang Li. 2019. Im-\nproving transformer-based speech recognition us-\ning unsupervised pre-training. arXiv preprint\narXiv:1910.09932.\nJee-weon Jung, Hee-Soo Heo, Ju-ho Kim, Hye-jin\nShim, and Ha-Jin Yu. 2019. RawNet: Advanced\nend-to-end deep neural network using raw wave-\nforms for text-independent speaker veriﬁcation. In\nThe 20th Annual Conference of the International\nSpeech Communication Association , pages 1268–\n1272.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In The 3rd Inter-\nnational Conference on Learning Representations.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard H. Hovy. 2017. RACE: large-scale\nreading comprehension dataset from examinations.\nIn Proceedings of the 2017 Conference on Em-\npirical Methods in Natural Language Processing,\nEMNLP 2017, Copenhagen, Denmark, September 9-\n11, 2017, pages 785–794.\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning , pages\n1188–1196. PMLR.\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, and\nDaxin Jiang. 2020a. Unicoder-VL: A universal en-\ncoder for vision and language by cross-modal pre-\ntraining. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , volume 34, pages 11336–\n11344.\nHang Li, Wenbiao Ding, Zhongqin Wu, and Zitao Liu.\n2021. Learning ﬁne-grained cross modality excite-\nment for speech emotion recognition. InInterspeech\n2021, 22th Annual Conference of the International\nSpeech Communication Association , pages 3375–\n3379.\nHang Li, Yu Kang, Wenbiao Ding, Song Yang, Song-\nfan Yang, Gale Yan Huang, and Zitao Liu. 2020b.\nMultimodal learning for classroom activity detec-\ntion. In IEEE International Conference on Acous-\ntics, Speech and Signal Processing , pages 9234–\n9238. IEEE.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. VisualBERT: A\nsimple and performant baseline for vision and lan-\nguage. arXiv preprint arXiv:1908.03557.\nAndy T. Liu, Shang-Wen Li, and Hung-yi Lee. 2021.\nTERA: self-supervised learning of transformer en-\ncoder representation for speech. IEEE ACM Trans.\nAudio Speech Lang. Process., 29:2351–2366.\nAndy T Liu, Shu-wen Yang, Po-Han Chi, Po-chun\nHsu, and Hung-yi Lee. 2020. Mockingjay: Unsu-\npervised speech representation learning with deep\nbidirectional transformer encoders. In IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing, pages 6419–6423. IEEE.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nSteven R Livingstone and Frank A Russo. 2018. The\nryerson audio-visual database of emotional speech\nand song (ravdess): A dynamic, multimodal set of\nfacial and vocal expressions in north american en-\nglish. PloS one, 13(5):e0196391.\n3976\nIlya Loshchilov and Frank Hutter. 2017. SGDR:\nstochastic gradient descent with warm restarts. In\nThe 5th International Conference on Learning Rep-\nresentations.\nIlya Loshchilov and Frank Hutter. 2018. Fixing weight\ndecay regularization in adam.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. ViLBERT: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems 32: Annual Conference on Neural Infor-\nmation Processing Systems 2019, NeurIPS 2019, De-\ncember 8-14, 2019, Vancouver, BC, Canada , pages\n13–23.\nBrian McFee, Colin Raffel, Dawen Liang, Daniel PW\nEllis, Matt McVicar, Eric Battenberg, and Oriol Ni-\neto. 2015. librosa: Audio and music signal analysis\nin python. In Proceedings of the 14th Python in Sci-\nence Conference, volume 8, pages 18–25. Citeseer.\nTomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient estimation of word represen-\ntations in vector space. In 1st International Con-\nference on Learning Representations, ICLR 2013,\nScottsdale, Arizona, USA, May 2-4, 2013, Workshop\nTrack Proceedings.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and\nSanjeev Khudanpur. 2015. Librispeech: an asr\ncorpus based on public domain audio books. In\n2015 IEEE International Conference on Acoustics,\nSpeech and Signal Processing , pages 5206–5210.\nIEEE.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1532–1543.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392.\nGeovany A Ramirez, Tadas Baltrušaitis, and Louis-\nPhilippe Morency. 2011. Modeling latent discrim-\ninative dynamic of multi-dimensional affective sig-\nnals. In International Conference on Affective Com-\nputing and Intelligent Interaction , pages 396–406.\nSpringer.\nSteffen Schneider, Alexei Baevski, Ronan Collobert,\nand Michael Auli. 2019. wav2vec: Unsupervised\npre-training for speech recognition. In Interspeech\n2019, 20th Annual Conference of the International\nSpeech Communication Association , pages 3465–\n3469.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing, pages 5099–5110.\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ Zico Kolter, Louis-Philippe Morency, and Ruslan\nSalakhutdinov. 2019. Multimodal transformer for\nunaligned multimodal language sequences. In Pro-\nceedings of the Annual Meeting of the Association\nfor Computational Linguistics.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-SNE. Journal of Machine\nLearning Research, 9(11).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nLi Wan, Quan Wang, Alan Papir, and Ignacio Lopez\nMoreno. 2018. Generalized end-to-end loss for\nspeaker veriﬁcation. In IEEE International Confer-\nence on Acoustics, Speech and Signal Processing ,\npages 4879–4883. IEEE.\nYun Wang. 2018. Polyphonic sound event detection\nwith weak labeling. PhD thesis.\nHaiyang Xu, Hui Zhang, Kun Han, Yun Wang, Yiping\nPeng, and Xiangang Li. 2019. Learning alignment\nfor multimodal emotion recognition from speech. In\nInterspeech 2019, 20th Annual Conference of the\nInternational Speech Communication Association ,\npages 3569–3573.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, pages 5754–5764.\nSeunghyun Yoon, Seokhyun Byun, Subhadeep Dey,\nand Kyomin Jung. 2019. Speech emotion recogni-\ntion using multi-hop attention mechanism. In IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing, pages 2822–2826. IEEE.\nSeunghyun Yoon, Seokhyun Byun, and Kyomin Jung.\n2018. Multimodal speech emotion recognition us-\ning audio and text. In 2018 IEEE Spoken Language\nTechnology Workshop, pages 112–118. IEEE.\nAmir Zadeh, Minghai Chen, Soujanya Poria, Erik Cam-\nbria, and Louis-Philippe Morency. 2017. Tensor\nfusion network for multimodal sentiment analysis.\n3977\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1103–1114. Association for Computational Linguis-\ntics.\nAmir Zadeh, Paul Pu Liang, Soujanya Poria, Pra-\nteek Vij, Erik Cambria, and Louis-Philippe Morency.\n2018. Multi-attention recurrent network for human\ncommunication comprehension. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 32.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. 2019. From recognition to cognition: Vi-\nsual commonsense reasoning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6720–6731.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.845732569694519
    },
    {
      "name": "Overfitting",
      "score": 0.5713065266609192
    },
    {
      "name": "Transformer",
      "score": 0.5500961542129517
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.5357285141944885
    },
    {
      "name": "Speech recognition",
      "score": 0.4951430857181549
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4806067645549774
    },
    {
      "name": "Language model",
      "score": 0.43598008155822754
    },
    {
      "name": "Natural language processing",
      "score": 0.40378931164741516
    },
    {
      "name": "Artificial neural network",
      "score": 0.15397953987121582
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}