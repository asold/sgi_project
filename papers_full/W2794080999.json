{
  "title": "ST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing",
  "url": "https://openalex.org/W2794080999",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4309591803",
      "name": "Lin, Chen-Hsuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287574859",
      "name": "Yumer, Ersin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3086600291",
      "name": "Wang Oliver",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2750992427",
      "name": "Shechtman, Eli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753776991",
      "name": "Lucey, Simon",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2949099979",
    "https://openalex.org/W2949886837",
    "https://openalex.org/W2439114332",
    "https://openalex.org/W2521028896",
    "https://openalex.org/W2952815469",
    "https://openalex.org/W2563100679",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2546066744",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2605287558",
    "https://openalex.org/W2157285372",
    "https://openalex.org/W2952742185",
    "https://openalex.org/W2951261569",
    "https://openalex.org/W2558578635",
    "https://openalex.org/W2605195953",
    "https://openalex.org/W2605135824",
    "https://openalex.org/W2952186111",
    "https://openalex.org/W2070604790",
    "https://openalex.org/W2118877769",
    "https://openalex.org/W2173520492",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2952695679",
    "https://openalex.org/W2519536754",
    "https://openalex.org/W2949496494",
    "https://openalex.org/W2592101326",
    "https://openalex.org/W2962947361",
    "https://openalex.org/W2950701417",
    "https://openalex.org/W2593978077",
    "https://openalex.org/W2950477723",
    "https://openalex.org/W2342877626",
    "https://openalex.org/W2953221084",
    "https://openalex.org/W2951932151",
    "https://openalex.org/W125693051",
    "https://openalex.org/W2949440248",
    "https://openalex.org/W2035379092",
    "https://openalex.org/W2323139888"
  ],
  "abstract": "We address the problem of finding realistic geometric corrections to a foreground object such that it appears natural when composited into a background image. To achieve this, we propose a novel Generative Adversarial Network (GAN) architecture that utilizes Spatial Transformer Networks (STNs) as the generator, which we call Spatial Transformer GANs (ST-GANs). ST-GANs seek image realism by operating in the geometric warp parameter space. In particular, we exploit an iterative STN warping scheme and propose a sequential training strategy that achieves better results compared to naive training of a single generator. One of the key advantages of ST-GAN is its applicability to high-resolution images indirectly since the predicted warp parameters are transferable between reference frames. We demonstrate our approach in two applications: (1) visualizing how indoor furniture (e.g. from product images) might be perceived in a room, (2) hallucinating how accessories like glasses would look when matched with real portraits.",
  "full_text": "ST-GAN: Spatial Transformer Generative Adversarial Networks\nfor Image Compositing\nChen-Hsuan Lin1* Ersin Yumer2,3* Oliver Wang2 Eli Shechtman2 Simon Lucey1,3\n1Carnegie Mellon University 2Adobe Research 3Argo AI\nchlin@cmu.edu meyumer@gmail.com {owang,elishe}@adobe.com slucey@cs.cmu.edu\nAbstract\nWe address the problem of ﬁnding realistic geometric\ncorrections to a foreground object such that it appears nat-\nural when composited into a background image. To achieve\nthis, we propose a novel Generative Adversarial Network\n(GAN) architecture that utilizes Spatial Transformer Net-\nworks (STNs) as the generator, which we call Spatial Trans-\nformer GANs (ST-GANs). ST-GANs seek image realism by\noperating in the geometric warp parameter space. In par-\nticular, we exploit an iterative STN warping scheme and\npropose a sequential training strategy that achieves better\nresults compared to naive training of a single generator.\nOne of the key advantages of ST-GAN is its applicability to\nhigh-resolution images indirectly since the predicted warp\nparameters are transferable between reference frames. We\ndemonstrate our approach in two applications: (1) visual-\nizing how indoor furniture (e.g. from product images) might\nbe perceived in a room, (2) hallucinating how accessories\nlike glasses would look when matched with real portraits.\n1. Introduction\nGenerative image modeling has progressed remarkably\nwith the advent of convolutional neural networks (CNNs).\nMost approaches constrain the possible appearance varia-\ntions within an image by learning a low-dimensional em-\nbedding as an encoding of the natural image subspace and\nmaking predictions from this at the pixel level. We refer to\nthese approaches here as direct image generation. Gener-\native Adversarial Networks (GANs) [7], in particular, have\ndemonstrated to be an especially powerful tool for realis-\ntic image generation. They consist of a generator network\n(G) that produces images from codes, and a discriminator\nnetwork (D) that distinguishes real images from fake ones.\nThese two networks play a minimax game that results in G\ngenerating realistic looking images and Dbeing unable to\ndistinguish between the two when equilibrium is reached.\n*Work done during CHL’s internship at Adobe Research.\nNatural Image\nManifold\n…\n…\nFigure 1: Composite images easily fall outside the natural\nimage manifold due to appearance and geometric discrep-\nancies. We seek to learn geometric corrections that sequen-\ntially warp composite images towards the intersection of the\ngeometric and natural image manifolds.\nDirect image generation, however, has its limitations. As\nthe space of all images is very high-dimensional and image\ngeneration methods are limited by ﬁnite network capacity,\ndirect image generation methods currently work well only\non restricted domains (e.g. faces) or at low resolutions.\nIn this work, we leverage Spatial Transformer Networks\n(STNs) [11], a special type of CNNs capable of perform-\ning geometric transformations on images, to provide a sim-\npler way to generate realistic looking images – by restrict-\ning the space of possible outputs to a well-deﬁned low-\ndimensional geometric transformation of real images. We\npropose Spatial Transformer Generative Adversarial Net-\nworks (ST-GANs), which learn Spatial Transformer genera-\ntors within a GAN framework. The adversarial loss enables\nus to learn geometric corrections resulting in a warped im-\nage that lies at the intersection of the natural image man-\n1\narXiv:1803.01837v1  [cs.CV]  5 Mar 2018\nifold and the geometric manifold – the space of geomet-\nric manipulations speciﬁc to the target image (Fig. 1). To\nachieve this, we advocate a sequential adversarial training\nstrategy to learn iterative spatial transformations that serve\nto break large transformations down into smaller ones.\nWe evaluate ST-GANs in the context image compositing,\nwhere a source foreground image and its mask are warped\nby the Spatial Transformer generator G, and the resulting\ncomposite is assessed by the discriminator D. In this setup,\nDtries to distinguish warped composites from real images,\nwhile Gtries to fool Dby generating as realistic looking as\npossible composites. To the best of our knowledge, we are\nthe ﬁrst to address the problem of realistic image generation\nthrough geometric transformations in a GAN framework.\nWe demonstrate this method on the application of composit-\ning furniture into indoor scenes, which gives a preview of,\nfor example, how purchased items would look in a house.\nTo evaluate in this domain, we created a synthetic dataset\nof indoor scene images as the background with masked ob-\njects as the foreground. We also demonstrate ST-GANs in\na fully unpaired setting for the task of compositing glasses\non portrait images. A large-scale user study shows that our\napproach improves the realism of image composites.\nOur main contributions are as follows:\n• We integrate the STN and GAN frameworks and in-\ntroduce ST-GAN, a novel GAN framework for ﬁnding\nrealistic-looking geometric warps.\n• We design a multi-stage architecture and training strat-\negy that improves warping convergence of ST-GANs.\n• We demonstrate compelling results in image composit-\ning tasks in both paired and unpaired settings as well\nas its applicability to high-resolution images.\n2. Related Work\nImage compositingrefers to the process of overlaying\na masked foreground image on top of a background im-\nage. One of the main challenges of image compositing is\nthat the foreground object usually comes from a different\nscene than the background, and therefore it is not likely to\nmatch the background scene in a number of ways that neg-\natively effects the realism of the composite. These can be\nboth appearance differences (due to lighting, white balance,\nand shading differences) and geometric differences (due to\nchanges in camera viewpoint and object positioning).\nExisting photo-editing software features various image\nappearance adjustment operations for that allows users to\ncreate realistic composites. Prior work has attempted to\nautomate appearance corrections ( e.g. contrast, saturation)\nthrough Poisson blending [26] or more recent deep learning\napproaches [42, 30]. In this work, we focus on the sec-\nond challenge: correcting for geometric inconsistencies be-\ntween source and target images.\nSpatial Transformer Networks (STNs)[11] are one\nway to incorporate learnable image warping within a deep\nlearning framework. A Spatial Transformer module con-\nsists of a subnetwork predicting a set of warp parameters\nfollowed by a (differentiable) warp function.\nSTNs have been shown effective in resolving geometric\nvariations for discriminative tasks as well as a wide range of\nextended applications such as robust ﬁlter learning [4, 13],\nimage/view synthesis [41, 6, 24, 37], and 3D representa-\ntion learning [14, 35, 40]. More recently, Inverse Compo-\nsitional STNs (IC-STNs) [17] advocated an iterative align-\nment framework. In this work, we borrow the concept of\niterative warping but do not enforce recurrence in the geo-\nmetric prediction network; instead, we add different genera-\ntors at each warping step with a sequential training scheme.\nGenerative Adversarial Networks (GANs)[7] are a\nclass of generative models that are learned by playing a\nminimax optimization game between a generator network\nGand a discriminator network D. Through this adversarial\nprocess, GANs are shown to be capable of learning a gen-\nerative distribution that matches the empirical distribution\nof a given data collection. One advantage of GANs is that\nthe loss function is essentially learned by the discriminator\nnetwork, which allows for training in cases where ground\ntruth data with strong supervision is not available.\nGANs are utilized for data generation in various do-\nmains, including images [27], videos [31], and 3D vox-\nelized data [33]. For images in particular, it has been\nshown to generate compelling results in a vast variety\nof conditional image generation problems such as super-\nresolution [16], inpainting [25], image-to-image transla-\ntion [10, 44, 19], and image editing/manipulation [43].\nRecently, STNs were also sought to be adversarially\ntrained for object detection [32], where adversarial exam-\nples with feature deformations are generated to robustify\nobject detectors. LR-GAN [36] approached direct image\ngeneration problems with additional STNs onto the (di-\nrectly) generated images to factorize shape variations. We\nexplore the context of STNs with GANs in the space of\nconditional image generation from given inputs, which is\na more direct integration of the two frameworks.\n3. Approach\nOur goal isrealistic geometric correctionfor image com-\npositing given a background image IBG and foreground ob-\nject IFG with a corresponding mask MFG. We aim to cor-\nrect the camera perspective, position and orientation of the\nforeground object such that the resulting composite looks\nnatural. The compositing process can be expressed as:\nIcomp = IFG ⊙MFG + IBG ⊙(1 −MFG)\n= IFG ⊕IBG . (1)\nIBGIFG\nIcomp(p0)\n(a)\ncomposite\nwarpp0\nIFG(p0)\n G2\nΔp2\nIFG(p1)\nwarp\np0\nG1\nΔp1\nIFG\nIBG\nupdate\n…\nIFG(p0)\nwarp\nupdate\np1 p2\nIFG(pN)\nwarp\npN\n…\nIcomp(pN)\np0\nG1\nΔp1\nIFG\nIBG\nupdate\nIFG(p0)\nwarp\np1\n(b) (c)\n…\n…\nFigure 2: Background. (a) Given an initial composite transformation p0, the foreground image and mask is composited\nonto the background image using (1). (b) Using Spatial Transformer Networks(STNs), a geometric prediction network\nG1 predicts an update ∆p1 conditioned on the foreground and background images, resulting in the new parameters p1. The\nupdate is performed with warp composition (3). (c) Our ﬁnal form is an iterative STN to predict a series of accumulative\nwarp updates on the foreground such that the resulting composite image falls closer to the natural image manifold.\nFor simplicity, we further introduce the notation ⊕to rep-\nresent compositing (with MFG implied within IFG). Given\nthe composite parameters p0 (deﬁning an initial warp state)\nof IFG, we can rewrite (1) as\nIcomp(p0) = IFG(p0) ⊕IBG , (2)\nwhere images are written as functions of the warp parame-\nters. This operator is shown in Fig. 2(a).\nIn this work, we restrict our geometric warp function to\nhomography transformations, which can represent approxi-\nmate 3D geometric rectiﬁcations for objects that are mostly\nplanar or with small perturbations. As a result, we are mak-\ning an assumption that the perspective of the foreground ob-\nject is close to the correct perspective; this is often the case\nwhen people are choosing similar, but not identical, images\nfrom which to composite the foreground object.\nThe core module of our network design is an STN\n(Fig. 2(b)), where the geometric prediction network Gpre-\ndicts a correcting update ∆p1. We condition Gon both the\nbackground and foreground images, since knowing how an\nobject should be transformed to ﬁt a background scene re-\nquires knowledge of the complex interaction between the\ntwo. This includes geometry of the object and the back-\nground scene, the relative camera position, and semantic\nunderstanding of realistic object layouts (e.g. having a win-\ndow in the middle of the room would not make sense).\n3.1. Iterative Geometric Corrections\nPredicting large displacement warp parameters from im-\nage pixels is extremely challenging, so most prior work on\nimage alignment predict local geometric transformations in\nan iterative fashion [9, 21, 2, 34, 18]. Similarly, we propose\nto use iterative STNs to predict a series of warp updates,\nshown in Fig. 2(c). At the ith iteration, given the input im-\nage Iand the previous warp statepi−1, the correcting warp\nupdate ∆pi and the new warp state pi can be written as\n∆pi = Gi\n(\nIFG(pi−1),IBG\n)\npi = pi−1 ◦∆pi , (3)\nwhere Gi(·) is the geometric prediction network and ◦de-\nnotes composition of warp parameters. This family of itera-\ntive STNs preserves the original images from loss of infor-\nmation due to multiple warping operations [17].\n3.2. Sequential Adversarial Training\nIn order for STNs to learn geometric warps that map im-\nages closer to the natural image manifold, we integrate them\ninto a GAN framework, which we refer to as ST-GANs. The\nmotivation for this is two-fold. First, learning a realistic ge-\nometric correction is a multi-modal problem (e.g. a bed can\nreasonably exist in multiple places in a room); second, su-\npervision for these warp parameters are typically not avail-\nable. The main difference of ST-GANs from conventional\nGANs is that (1) Ggenerates a set of low-dimensional warp\nparameter updates instead of images (the whole set of pixel\nvalues); and (2) Dgets as input the warped foreground im-\nage composited with the background.\nTo learn gradual geometric improvements toward the\nnatural image manifold, we adopt a sequential adversarial\ntraining strategy for iterative STNs (Fig. 3), where the geo-\nmetric predictor Gcorresponds to the stack of generatorsGi.\nWe start by training a single G1, and each subsequent new\nGi-1\nIcomp(pi-1)\nD\nGi\nΔpi\nupdate\nIFG(pi-1)warp\npi\nwarp\nIcomp(pi)\nD\nIFG(pi)\nΔpi-1\npi-2… update\n…\n…IFG\nIBG\n…\npi-1\nFigure 3: Sequential adversarial training of ST-GAN.\nWhen learning a new warp state pi, only the new generator\nGi is updated while the previous ones are kept ﬁxed. A sin-\ngle discriminator (learned from all stages) is continuously\nimproved during the sequential learning process.\ngenerator Gi is added and trained by ﬁxing the weights of\nall previous generators {Gj}j=1···i−1. As a result, we train\nonly Gi and Dby feeding the resulting composite image at\nwarp state Icomp(pi) into the discriminator Dand match-\ning it against the real data distribution. This learning phi-\nlosophy shares commonalities with the Supervised Descent\nMethod [34], where a series of linear regressors are solved\ngreedily, and we found it makes the overall training faster\nand more robust. Finally, we ﬁne-tune the entire network\nend-to-end to achieve our ﬁnal result. Note that we use the\nsame discriminator Dfor all stages of the generator Gi, as\nthe fundamental measure of “geometric fakeness” does not\nchange over iterations.\n3.3. Adversarial Objective\nWe optimize the Wasserstein GAN (WGAN) [1] objec-\ntive for our adversarial game. We note that ST-GAN is\namenable to any other GAN variants [22, 39, 3], and that\nthe choice of GAN architecture is orthogonal to this work.\nThe WGAN minimax objective at the ith stage is\nmin\nGi\nmax\nD∈D E\nx∼Pfake\npi∼Ppi|pi−1\n[\nD\n(\nx(pi)\n)]\n− E\ny∼Preal\n[\nD(y)\n]\n, (4)\nwhere y = Ireal and x = Icomp are drawn from the real\ndata and fake composite distributions, and D is the set of\n1-Lipschitz functions enforced by adding a gradient penalty\nterm Lgrad [8]. Here, pi (where Gi is implied, deﬁned in (3))\nis drawn from the posterior distribution conditioned onpi−1\n(recursively implied). When i = 1 , the initial warp p0 is\ndrawn from Ppert, a predeﬁned distribution for geometric\ndata augmentation.\nWe also constrain the warp update ∆pi to lie within a\ntrust region by introducing an additional penalty Lupdate =\n∥∆pi∥2\n2 . This is essential since ST-GAN may learn triv-\nial solutions to remove the foreground ( e.g. by translating\nit outside the image or shrinking it into nothing), leaving\nbehind only the background image and in turn making the\ncomposite image realistic already.\nWhen training ST-GAN sequentially, we update Dand\nGi alternating the respective loss functions:\nLD= Ex,pi\n[\nD\n(\nx(pi)\n)]\n−Ey\n[\nD(y)\n]\n+ λgrad ·Lgrad (5)\nLGi = −Ex,pi\n[\nD\n(\nx(pi)\n)]\n+ λupdate ·Lupdate , (6)\nwhere λgrad and λupdate are the penalty weights for theDgra-\ndient and the warp update∆pi respectively, andGi and ∆pi\nare again implied through (3). When ﬁne-tuning ST-GAN\nwith N learned updates end-to-end, the generator objective\nis the sum of that from each Gi, i.e. LG= ∑N\ni=1 LGi .\n4. Experiments\nWe begin by describing the basic experimental settings.\nWarp parameterizations. We parameterize a homogra-\nphy with the sl(3) Lie algebra [23], i.e. the warp parameters\np ∈sl(3) and homography matrices H ∈SL(3) are related\nthrough the exponential map. Under this parameterization,\nwarp composition can be expressed as the addition of pa-\nrameters, i.e. pa ◦pb ≡pa + pb ∀pa,pb ∈sl(3).\nModel architecture. We denote the following: C(k) is\na 2D convolutional layer with k ﬁlters of size 4 ×4 and\nstride 2 (halving the feature map resolution) and L(k) is a\nfully-connected layer with koutput nodes. The input of the\ngenerators Gi has 7 channels: RGBA for foreground and\nRGB for background, and the input to the discriminator D\nis the composite image with 3 channels (RGB). All images\nare rescaled to120×160, but we note that the parameterized\nwarp can be applied to full-resolution images at test time.\nThe architecture of Gis C(32)-C(64)-C(128)-C(256)-\nC(512)-L(256)-L(8), where the output is the 8-dimensional\n(in the case of a homography) warp parameter update ∆p.\nFor each convolutional layer in G, we concatenate a down-\nsampled version of the original image (using average pool-\ning) with the input feature map. For D, we use a PatchGAN\narchitecture [10], with layout C(32)-C(64)-C(128)-C(256)-\nC(512)-C(1). Nonlinearity activations are inserted between\nall layers, where they are ReLU forGand LeakyReLU with\nslope 0.2 for D. We omit all normalization layers as we\nfound them to deteriorate training performance.\nInitial\nST-GAN\nGround\ntruth\nproject\nx\ny\nz\n(a)\n(b)\nFigure 4: (a) We create a synthetic dataset of 3D cube ren-\nderings and validate the efﬁcacy of ST-GAN by attempting\nto correct randomly generated geometric perturbations. (b)\nST-GAN is able to correct the cubes to a right perspective,\nalbeit a possible translational offset from the ground truth.\n4.1. 3D Cubes\nTo begin with, we validate whether ST-GANs can make\ngeometric corrections in a simple, artiﬁcial setting. We\ncreate a synthetic dataset consisting of a 3D rectangular\nroom, an axis-aligned cube inside the room, and a perspec-\ntive camera (Fig. 4(a)). We apply random 3-DoF trans-\nlations to the cube and 6-DoF perturbations to the cam-\nera, and render the cube/room pair separately as the fore-\nground/background (of resolution 120 ×160). We color all\nsides of the cube and the room randomly.\nWe perturb the rendered foreground cubes with random\nhomography transformations as the initial warpp0 and train\nST-GAN by pairing the original cube as the ground-truth\ncounterpart for D. As shown in Fig. 4(b), ST-GAN is able\nto correct the perturbed cubes scale and perspective distor-\ntion w.r.t. the underlying scene geometry. In addition, ST-\nGAN is sometimes able to discover other realistic solutions\n(e.g. not necessarily aligning back to the ground-truth loca-\ntion), indicating ST-GAN’s ability to learn the multi-modal\ndistribution of correct cube placements in this dataset.\n4.2. Indoor Objects\nNext, we show how ST-GANs can be applied to practi-\ncal image compositing domains. We choose the application\nof compositing furniture in indoor scenes and demonstrate\nits efﬁcacy on both simulated and real-world images. To\ncollect training data, we create a synthetic dataset consist-\ning of rendered background scenes and foreground objects\nwith masks. We evaluate on the synthetic test set as well as\nhigh-resolution real world photographs to validate whether\nST-GAN also generalizes to real images.\nCategory Training set Test set\n# 3D inst. # pert. # 3D inst. # pert.\nBed 3924 11829 414 1281\nBookshelf 508 1280 58 137\nCabinet 9335 31174 1067 3518\nChair 196 609 22 60\nDesk 64 1674 73 214\nDresser 285 808 31 84\nRefrigerator 3802 15407 415 1692\nSofa 3604 11165 397 1144\nTotal 22303 73946 2477 8130\nTable 1: Dataset statistics for the indoor object experiment,\nreporting the number of object instances chosen for pertur-\nbation, and the ﬁnal number of rendered perturbed samples.\nremove\nocclusion\nremove\nobject\nperturb camera & remove occlusion\ncrop object &\ncomposite\nFigure 5: Rendering pipeline.Given an indoor scene and a\ncandidate object, we remove occluding objects to create an\nocclusion-free scenario, which we do the same at another\nperturbed camera pose. We further remove the object to\ncreate a training sample pair with mismatched perspectives.\nData preparation. We render synthetic indoor scene im-\nages from the SUNCG dataset [29], consisting of 45,622 in-\ndoor scenes with over 5M 3D object instances from 37 cat-\negories [28]. We use the selected 41,499 scene models and\nthe 568,749 camera viewpoints from Zhang et al. [38] and\nutilize Mitsuba [12] to render photo-realistic images with\nglobal illumination. We keep a list of candidate 3D objects\nconsisting of all instances visible from the camera view-\npoints and belonging to the categories listed in Table 1.\nThe rendering pipeline is shown in Fig. 5. During the\nprocess, we randomly sample an object from the candidate\nlist, with an associated camera viewpoint. To emulate an\nocclusion-free compositing scenario, occlusions are auto-\nmatically removed by detecting overlapping object masks.\nWe render one image with the candidate object present (as\nthe “real” sample) and one with it removed (as the back-\nground image). In addition, we perturb the 6-DoF camera\npose and render the object with its mask (as the foreground\nimage) for compositing. We thus obtain a rendered object as\nviewed from a different camera perspective; this simulates\nthe image compositing task where the foreground and back-\nground perspectives mismatch. We note that a homography\ncorrection can only approximate these 3D perturbations, so\nthere is no planar ground-truth warpto use for supervision.\nCategory Initial SDM [34] Homogra- ST-GAN ST-GAN ST-GAN ST-GAN ST-GAN Ground\nwarp phyNet [5] (non-seq.) (warp 1) (warp 2) (warp 4) (end-to-end) truth\nBed 35.5 % 30.5 % 30.2 % 32.8 % 32.8 % 46.8 % 32.8 % 32.2 % 75.0 %\nBookshelf 21.1 % 33.9 % 35.1 % 16.7 % 26.4 % 26.2 % 39.5 % 42.6 % 68.9 %\nCabinet 20.9 % 19.8 % 35.0 % 36.6 % 14.3 % 31.2 % 44.4 % 50.0 % 74.3 %\nChair 32.8 % 36.8 % 47.6 % 50.9 % 62.3% 42.7 % 50.0 % 58.6 % 68.7 %\nDesk 18.9 % 13.1 % 36.1 % 35.4 % 29.2 % 29.0 % 39.4 % 40.7 % 65.1 %\nDresser 14.9 % 18.6 % 20.7 % 16.7 % 24.6 % 27.4 % 29.7 % 48.4 % 66.1 %\nRefrigerator 37.1 % 21.4 % 50.0 % 37.7 % 28.6 % 47.1 % 39.7 % 51.7 % 81.6 %\nSofa 15.9 % 31.0 % 42.4 % 28.9 % 37.0 % 54.9 % 56.1 % 51.8 % 78.2 %\nAverage 24.6 % 25.6 % 37.1 % 31.9 % 31.9 % 38.2 % 41.5 % 47.0 % 72.6 %\nTable 2: AMT User studiesfor the indoor objects experiment. Percentages represent the how often the images in each\ncategory were classiﬁed as “real” by Turkers. We can see that our ﬁnal model, ST-GAN (end-to-end), substantially improves\nover geometric realism when averaged across all classes. Our realism performance improves with the number of warps trained\nas well as after the end-to-end ﬁne-tuning. The ground truth numbers serve as a theoretical upper bound for all methods.\nWe report the statistics of our rendered dataset in Table 1.\nAll images are rendered at 120 ×160 resolution.\nSettings. Similar to the prior work by Lin & Lucey [17],\nwe train ST-GAN for N = 4 sequential warps During\nadversarial training, we rescale the foreground object ran-\ndomly from Unif(0.9,1.1) and augment the initial warp p0\nwith a translation sampled from N(0,0.05) scaled by the\nimage dimensions. We set λupdate = 0.3 for all methods.\nBaselines. One major advantage of ST-GAN is that\nit can learn from “realism” comparisons without ground-\ntruth warp parameters for supervision. However, prior ap-\nproaches require supervision directly on the warp param-\neters. Therefore, we compare against self-supervised ap-\nproaches trained with random homography perturbations on\nforeground objects as input, yielding warp parameters as\nself-supervision. We reemphasize that such direct supervi-\nsion is insufﬁcient in this application as we aim to ﬁnd the\nclosest point on a manifold of realistic looking composites\nrather than ﬁtting a speciﬁc paired model. Our baselines are\n(1) HomographyNet [5], a CNN-based approach that learns\ndirect regression on the warp parameters, and (2) Super-\nvised Descent Method (SDM) [34], which greedily learns\nthe parameters through cascaded linear regression. We train\nthe SDM baseline for 4 sequential warps as well.\nQuantitative evaluation. As with most image generation\ntasks where the goal is realism, there is no natural quantita-\ntive evaluation possible. Therefore, we carry out a percep-\ntual study on Amazon Mechanical Turk (AMT) to assess\ngeometric realism of the warped composites. We randomly\nchose 50 test images from each category and gather data\nfrom 225 participants. Each participant was shown a com-\nposite image from a randomly selected algorithm (Table 2),\nand was asked whether they saw any objects whose shape\ndoes not look natural in the presented image.\nWe report the AMT assessment results in Table 2. On\naverage, ST-GAN shows a large improvement of geomet-\nric realism, and quality improves over the sequential warps.\nWhen considering that the warp is restricted to homogra-\nphy transformations, these results are promising, as we are\nnot correcting for more complicated view synthesis effects\nfor out-of-plane rotations such as occlusions. Additionally,\nST-GAN, which does not require ground truth warp param-\neters during training, greatly outperforms other baselines,\nwhile SDM yields no improvement and HomographyNet in-\ncreases realism, but to a lesser degree.\nAblation studies. We found that learning iterative\nwarps is advantageous: compared with a non-iterative ver-\nsion with the same training iterations (non-seq. in Table 2),\nST-GAN (with multiple generators) approaches geometric\nrealism more effectively with iterative warp updates. In ad-\ndition, we trained an iterative HomographyNet [5] using the\nsame sequential training strategy as ST-GAN but found lit-\ntle visual improvement over the non-iterative version; we\nthus focus our comparison against the original [5].\nQualitative evaluation. We present qualitative results in\nFig. 6. ST-GAN visually outperforms both baselines trained\nwith direct homography parameter supervision, which is\nalso reﬂected in the AMT assessment results. Fig. 7 shows\nhow ST-GAN updates the homography warp with each of its\ngenerators; we see that it learns gradual updates that makes\na realism improvement at each step. In addition, we illus-\ntrates in Fig. 8 the effects ST-GAN learns, including gradual\nchanges of the object perspective at different composite lo-\ncations inside the room, as well as a “snapping” effect that\npredicts a most likely composite location given a neighbor-\nhood of initial locations. These features are automatically\nlearned from the data, and they can be useful when imple-\nmented in interactive settings.\nSDM\nHomogra-\nphyNet\nST-GAN\nOriginal\nrendering\nPerturbed\nsample\n(initial)\nFigure 6: Qualitative evaluationon the indoor rendering test set. Compared to the baselines trained with direct homography\nsupervision, ST-GAN creates more realistic composites. We ﬁnd that ST-GAN is able to learn common object-room relation-\nships in the dataset, such as beds being against walls. Note that ST-GANs corrects the perspectives but not necessarily scale,\nas objects often exist at multiple scales in the real data. We observe that ST-GAN occasionally performs worse for unusual\nobjects (e.g. with peculiar colors, last column).\ninitial 1st update 2nd update 3rd update 4th update\nFigure 7: Visualization of iterative updates in ST-GAN,\nwhere objects make gradual improvements that reaches\ncloser to realism in an incremental fashion.\n(a)\n(b)\nSample\ncomposite\nDifferent\ninitial locations\nST-GAN\noutput\nFigure 8: Dragging and snapping.(a) When an object is\ndragged across the scene, the perspective changes with the\ncomposite location to match that of the camera’s. (b) ST-\nGAN “snaps” objects to where it would be frequently com-\nposited (e.g. a bookshelf is usually laid against the wall).\nST-\nGAN\n120×160\n1920×1200\nST-\nGAN\n120×160\n2560×1700\nFigure 9: Real world high-resolution test results.Here\nwe show our method applied to real images. The inputs\nare scaled down and fed to the network and then the warp\nparameters are applied at full resolution.\nFinally, to test whether ST-GAN extends to real images,\nwe provide a qualitative evaluation on photographic, high-\nresolution test images gathered from the Internet and manu-\nally masked (Fig 9). This is feasible since the warp param-\neters predicted from the low-resolution network input are\ntransferable to high-resolution images. As a consequence,\nST-GAN is indirectly applicable to various image resolu-\ntions and not strictly limited as with conventional GAN\nframeworks. Our results demonstrates the utilization of ST-\nGAN for high-quality image generation and editing.\nfaces withoutglasses (as background) faces withglasses (as real data)\nFigure 10: The split of CelebA for the background and the\nreal images, as well as the crafted glasses as the foreground.\n4.3. Glasses\nFinally, we demonstrate results in an entirely unpaired\nsetting where we learn warping corrections for compositing\nglasses on human faces. The lack of paired data means that\nwe do not necessarily have pictures of the same people both\nwith and without glasses (ground truth).\nData preparation. We use the CelebA dataset [20]\nand follow the provided training/test split. We then use\nthe “eyeglasses” annotation to separate the training set into\ntwo groups. The ﬁrst group of people with glasses serve\nas the real data to be matched against in our adversarial\nsettings, and the group of people without glasses serves as\nthe background. This results in 152249 training and 18673\ntest images without glasses, and 10521 training images with\nglasses. We hand-crafted 10 pairs of frontal-facing glasses\nas the foreground source (Fig. 10). We note that there are\nno annotations about where or how the faces are placed, and\nwe do not have any information where the different parts of\nthe glasses are in the foreground images.\nIn this experiment, we train ST-GAN with N = 5 se-\nquential warps. We crop the aligned faces into 144 ×144\nimages and resize the glasses to widths of 120 pixels ini-\ntialized at the center. During training, we add geometric\ndata augmentation by randomly perturbing the faces with\nrandom similarity transformations and the glasses with ran-\ndom homographies.\nResults. The results are shown in Fig. 11. As with\nthe previous experiments, ST-GAN learns to warp the fore-\nground glasses in a gradual fashion that improves upon real-\nism at each step. We ﬁnd that our method can correctly align\nglasses onto the people’s faces, even with a certain amount\nof in-plane rotations. However, ST-GAN does a poorer job\non faces with too much out-of-plane rotation.\nWhile such an effect is possible to achieve by taking ad-\nvantage of facial landmarks, our results are encouraging as\nno information was given about the structure of either do-\nmain, and we only had access to unpaired images of people\nwith and without glasses. Nonetheless, ST-GAN was able to\nlearn a realism manifold that drove the Spatial Transformer\ngenerators. We believe this demonstrates great potential to\nextend ST-GANs to other image alignment tasks where ac-\nquiring paired data is very challenging.\n(a)\n(b)\ninitial compositeoriginal 1st update 2nd update 3rd update 4th update 5th update\nFigure 11: Glasses compositing results. (a) The glasses\nprogressively moves into a more realistic position. (b) ST-\nGAN learns to warp various kinds of glasses such that the\nresulting positions are usually realistic. The top rows indi-\ncates the initial composite, and the bottom rows indicates\nthe ST-GAN output. The last 4 examples shows failure\ncases, where glasses fail to converge onto the faces.\n5. Conclusion\nWe have introduced ST-GANs as a class of methods to\nmodel geometric realism. We have demonstrated the poten-\ntial of ST-GANs on the task of image compositing, showing\nimproved realism in a large-scale rendered dataset, and re-\nsults on fully unpaired real-world image data. It is our hope\nthat this work will open up new revenues to the research\ncommunity to continue to explore in this direction.\nDespite the encouraging results ST-GAN achieves, there\nare still some limitations. We ﬁnd that ST-GAN suffers\nmore when presented imbalanced data, particularly rare ex-\namples (e.g. white, thick-framed glasses in the glasses ex-\nperiment). In addition, we also ﬁnd convergence of ST-\nGAN to fail with more extreme translation or in-plane rota-\ntion of objects. We believe a future analysis of the conver-\ngence properties of classical image alignment methods with\nGAN frameworks is worthy of investigation in improving\nthe robustness of ST-GANs.\nReferences\n[1] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan.\narXiv preprint arXiv:1701.07875, 2017. 4\n[2] S. Baker and I. Matthews. Lucas-kanade 20 years on: A uni-\nfying framework. International journal of computer vision ,\n56(3):221–255, 2004. 3\n[3] D. Berthelot, T. Schumm, and L. Metz. Began: Boundary\nequilibrium generative adversarial networks. arXiv preprint\narXiv:1703.10717, 2017. 4\n[4] J. Dai, H. Qi, Y . Xiong, Y . Li, G. Zhang, H. Hu, and\nY . Wei. Deformable convolutional networks.arXiv preprint\narXiv:1703.06211, 2017. 2\n[5] D. DeTone, T. Malisiewicz, and A. Rabinovich. Deep image\nhomography estimation. arXiv preprint arXiv:1606.03798 ,\n2016. 6, 11\n[6] Y . Ganin, D. Kononenko, D. Sungatullina, and V . Lempitsky.\nDeepwarp: Photorealistic image resynthesis for gaze manip-\nulation. In European Conference on Computer Vision, pages\n311–326. Springer, 2016. 2\n[7] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio. Gen-\nerative adversarial nets. In Advances in neural information\nprocessing systems, pages 2672–2680, 2014. 1, 2\n[8] I. Gulrajani, F. Ahmed, M. Arjovsky, V . Dumoulin, and\nA. Courville. Improved training of wasserstein gans. arXiv\npreprint arXiv:1704.00028, 2017. 4, 11\n[9] B. K. Horn and B. G. Schunck. Determining optical ﬂow.\nArtiﬁcial intelligence, 17(1-3):185–203, 1981. 3\n[10] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros. Image-to-\nimage translation with conditional adversarial networks. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2017. 2, 4\n[11] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial\ntransformer networks. In Advances in Neural Information\nProcessing Systems, pages 2017–2025, 2015. 1, 2\n[12] W. Jakob. Mitsuba renderer, 2010. http://www.mitsuba-\nrenderer.org. 5, 10\n[13] X. Jia, B. De Brabandere, T. Tuytelaars, and L. V . Gool. Dy-\nnamic ﬁlter networks. In Advances in Neural Information\nProcessing Systems, pages 667–675, 2016. 2\n[14] A. Kanazawa, D. W. Jacobs, and M. Chandraker. Warpnet:\nWeakly supervised matching for single-view reconstruction.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 3253–3261, 2016. 2\n[15] D. Kingma and J. Ba. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980, 2014. 11\n[16] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang. Deep\nlaplacian pyramid networks for fast and accurate super-\nresolution. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2017. 2\n[17] C.-H. Lin and S. Lucey. Inverse compositional spatial trans-\nformer networks. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2017. 2, 3, 6\n[18] C.-H. Lin, R. Zhu, and S. Lucey. The conditional lucas &\nkanade algorithm. In European Conference on Computer Vi-\nsion, pages 793–808. Springer, 2016. 3, 11\n[19] M.-Y . Liu, T. Breuel, and J. Kautz. Unsupervised\nimage-to-image translation networks. arXiv preprint\narXiv:1703.00848, 2017. 2\n[20] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face\nattributes in the wild. In Proceedings of International Con-\nference on Computer Vision (ICCV), 2015. 8\n[21] B. D. Lucas and T. Kanade. An iterative image registration\ntechnique with an application to stereo vision. In Proceed-\nings of the 7th International Joint Conference on Artiﬁcial\nIntelligence - Volume 2, IJCAI’81, pages 674–679, 1981. 3\n[22] X. Mao, Q. Li, H. Xie, R. Y . Lau, Z. Wang, and S. P. Smol-\nley. Least squares generative adversarial networks. arXiv\npreprint ArXiv:1611.04076, 2016. 4\n[23] C. Mei, S. Benhimane, E. Malis, and P. Rives. Homography-\nbased tracking for central catadioptric cameras. InIntelligent\nRobots and Systems, 2006 IEEE/RSJ International Confer-\nence on, pages 669–674. IEEE, 2006. 4, 10\n[24] E. Park, J. Yang, E. Yumer, D. Ceylan, and A. C.\nBerg. Transformation-grounded image generation network\nfor novel 3d view synthesis. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2017. 2\n[25] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A.\nEfros. Context encoders: Feature learning by inpainting.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2536–2544, 2016. 2\n[26] P. P ´erez, M. Gangnet, and A. Blake. Poisson image editing.\nIn ACM Transactions on graphics (TOG), volume 22, pages\n313–318. ACM, 2003. 2\n[27] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-\nsentation learning with deep convolutional generative adver-\nsarial networks. arXiv preprint arXiv:1511.06434, 2015. 2\n[28] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor\nsegmentation and support inference from rgbd images.Com-\nputer Vision–ECCV 2012, pages 746–760, 2012. 5, 10\n[29] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and\nT. Funkhouser. Semantic scene completion from a single\ndepth image. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2017. 5, 10\n[30] Y .-H. Tsai, X. Shen, Z. Lin, K. Sunkavalli, X. Lu, and M.-H.\nYang. Deep image harmonization. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2017. 2\n[31] C. V ondrick, H. Pirsiavash, and A. Torralba. Generating\nvideos with scene dynamics. In Advances In Neural Infor-\nmation Processing Systems, pages 613–621, 2016. 2\n[32] X. Wang, A. Shrivastava, and A. Gupta. A-fast-rcnn: Hard\npositive generation via adversary for object detection. arXiv\npreprint arXiv:1704.03414, 2017. 2\n[33] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum.\nLearning a probabilistic latent space of object shapes via 3d\ngenerative-adversarial modeling. In Advances in Neural In-\nformation Processing Systems, pages 82–90, 2016. 2\n[34] X. Xiong and F. De la Torre. Supervised descent method\nand its applications to face alignment. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 532–539, 2013. 3, 4, 6, 11\n[35] X. Yan, J. Yang, E. Yumer, Y . Guo, and H. Lee. Perspective\ntransformer nets: Learning single-view 3d object reconstruc-\ntion without 3d supervision. In Advances in Neural Informa-\ntion Processing Systems, pages 1696–1704, 2016. 2\n[36] J. Yang, A. Kannan, D. Batra, and D. Parikh. Lr-gan:\nLayered recursive generative adversarial networks for image\ngeneration. ICLR, 2017. 2\n[37] R. Yeh, Z. Liu, D. B. Goldman, and A. Agarwala. Seman-\ntic facial expression editing using autoencoded ﬂow. arXiv\npreprint arXiv:1611.09961, 2016. 2\n[38] Y . Zhang, S. Song, E. Yumer, M. Savva, J.-Y . Lee, H. Jin,\nand T. Funkhouser. Physically-based rendering for indoor\nscene understanding using convolutional neural networks. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2017. 5, 10\n[39] J. Zhao, M. Mathieu, and Y . LeCun. Energy-based genera-\ntive adversarial network. arXiv preprint arXiv:1609.03126,\n2016. 4\n[40] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsuper-\nvised learning of depth and ego-motion from video. arXiv\npreprint arXiv:1704.07813, 2017. 2\n[41] T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros. View\nsynthesis by appearance ﬂow. In European Conference on\nComputer Vision, pages 286–301. Springer, 2016. 2\n[42] J.-Y . Zhu, P. Krahenbuhl, E. Shechtman, and A. A. Efros.\nLearning a discriminative model for the perception of real-\nism in composite images. In Proceedings of the IEEE Inter-\nnational Conference on Computer Vision, pages 3943–3951,\n2015. 2\n[43] J.-Y . Zhu, P. Kr ¨ahenb¨uhl, E. Shechtman, and A. A. Efros.\nGenerative visual manipulation on the natural image mani-\nfold. In Proceedings of European Conference on Computer\nVision (ECCV), 2016. 2\n[44] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-\nto-image translation using cycle-consistent adversarial net-\nworks. arXiv preprint arXiv:1703.10593, 2017. 2\nAppendix\nA.1. Indoor Object Experiment: Rendering Details\nWe describe additional details regarding the rendering of\nthe SUNCG dataset [29] for our experiment. In addition to\nMitsuba [12] for rendering photo-realistic textures, we also\nutilize the OpenGL toolbox provided by Song et al. [29],\nwhich supports rendering of instance segmentation.\nCandidate object selection. For each of the provided\ncamera viewpoints from Zhang et al . [38], we render an\ninstance segmentation of all objects visible in the camera\nviewpoint. For each of these objects, we also separately\nrender a binary object mask by removing all other existing\nobjects (including the ﬂoor/ceiling/walls).\nWe use these information to exclude objects that are not\nideal for our compositing experiment, including those that\nare too tiny or only partially visible in the camera view.\nTherefore, we include objects into the candidate selection\nlist that match the criteria:\n• The entire object mask is visible within the camera.\n• The object mask occupies at least 10% of all pixels.\n• At least 50% of the object mask is visible within the\ninstance segmentation mask.\n• The object belongs to one of the NYUv2 [28] cate-\ngories of refrigerators, desks, bookshelves, cabinets,\nbeds, dressers, sofas, or chairs.\nOcclusion removal. For all the objects in the candi-\ndate list, we remove the occluding objects (from the associ-\nated camera viewpoint) by overlapping the object mask onto\nthe instance segmentation mask. All overlapped pixels with\ndifferent instance labels are detected to be associated with\nan occluding object. Since there may be “hidden” occlu-\nsions that are occluded in the ﬁrst place, we repeat the same\nprocess after the initial detected occlusions are removed to\nreveal the remaining occlusions. This is repeated until no\nmore occluding objects w.r.t. the candidate object is present.\nIn order to create a cleaner space for compositing ob-\njects, we also use a “thicker” object mask for the above re-\nmoval procedure. To achieve this, we dilate the object mask\nwith a 3 ×3 all-ones kernel for 10 times (i.e. “thicken” the\nobject mask by 10 pixels).\nCamera perturbation. For each of the provided camera\nviewpoints, we generate a camera perturbation by adding\na random 3D-translation sampled from Unif (−1,1) in the\nforward-backward direction, one sampled from Unif(−1,1)\nin the left-right direction (both scaled in meters as deﬁned\nin the dataset), and a random azimuth rotation sampled from\nUnif(−30,30) (degrees).\nAfter generating a camera perturbation, the same occlu-\nsion removal process described above is performed to en-\nsure the wholeness of the object from the perturbed per-\nspective. The candidate object rendered from the perturbed\nview serves as the foreground source for our experiment.\nHowever, if it becomes only partially or not visible, then\nthe rendering is discarded.\nRendering. We use Mitsuba to render120 ×160 realistic\ntextures and the OpenGL toolbox to render object masks at\n240 ×320 followed by ×2 downscaling for anti-aliasing.\nA.2. Warp Parameterization Details\nWe follow Mei et al. [23] to parameterize homography\nwith the sl(3) Lie algebra. Given a warp parameter vector\np = [p1,p2,p3,p4,p5,p6,p7,p8]⊤∈sl(3), the transforma-\ntion matrix H ∈SL(3) can be written as\nH(p) = exp\n\n\n\n\np1 p2 p3\np4 −p1 −p8 p5\np6 p7 p8\n\n\n\n , (7)\nwhere exp is the exponential map (i.e. matrix exponential).\nH is the identity transformation when p is an all-zeros vec-\ntor. Warp composition can thus be expressed as the addition\nof parameters, i.e. pa ◦pb ≡pa + pb ∀pa,pb ∈sl(3);\nfurthermore, det(H) = 1 ∀H ∈SL(3).\nThe exponential map is also Taylor-expandable as\nH(p) = exp(X(p)) = lim\nK→∞\nK∑\nk=0\nXk(p)\nk! . (8)\nWe implement the sl(3) parameterization using the Taylor\napproximation expression with K = 20.\nA.3. Training Details\nFor all experiments, we set the batch size for all exper-\niments to be 20. Unless otherwise speciﬁed, we initialize\nall learnable weights in the networks from N(0,0.01) and\nall biases to be 0. All deep learning approaches are trained\nwith Adam optimization [15]. We set λgrad = 10 following\nGulrajani et al. [8].\nWe describe settings for speciﬁc experiments as follows.\n3D cubes. We create 4000 samples of 3D cube/room\npairs with random colors, as described in the paper. For the\ninitial warp p0, we generate random homography perturba-\ntions p0 by sampling each element of p0 from N(0,0.1),\ni.e. p0 ∼N(0,0.1I). This is applied to a canonical frame\nwith x and y coordinates normalized to [−1,1] and sub-\nsequently transformed back to the image frame. We train\nST-GAN with 4 sequential warps, each for 50K iterations\n(with perturbations generated on the ﬂy) with the learning\nrates for both Gand Dto be 10−4. We set λupdate = 0.1 in\nthis experiment.\nIndoor objects. For the self-supervised baselines (Ho-\nmographyNet [5] and SDM [34]), we generate random ho-\nmography perturbations p0 using the same noise model as\nthat from the 3D cubes experiment.\nWe train HomographyNet for 200K iterations (with per-\nturbations generated on the ﬂy) with a learning rate of10−4.\nFor SDM, we vectorize the grayscale images to be the fea-\nture as was practiced for image alignment [18]; in our case,\nwe concatenate those of the background and masked fore-\nground as the ﬁnal extracted feature. We generate 750K\nperturbed examples (more than 10 perturbed examples per\ntraining sample) to train each linear regressor. Also as was\npracticed [34, 18], we add an ℓ2 regularization term to the\nSDM least-squares objective function and search for the\npenalty factor by evaluating on a separate validation set.\nWe initialize each of the ST-GAN generators Gi with\nthe pretrained HomographyNet as we ﬁnd it to be better-\nconditioned. During adversarial training, we train each Gi\nfor 40K iterations with the learning rate for Gi to be 10−6\nand that of Dto be 10−4. In the ﬁnal end-to-end ﬁne-\ntuning stage, we train all Gi for 40K iterations using the\nsame learning rates ( 10−6 for all Gi and 10−4 for D). The\nnon-sequential ST-GAN baseline is trained for 160K itera-\ntions with the same learning rates. We set λupdate = 0.3 in\nthis experiment.\nGlasses. For data augmentation, we perturb the faces\nwith random similarity transformations from N(0,0.1) for\nrotation (radian) and N(0,0.05) for translation (scaled by\nthe image dimensions, in both x and y directions). The\nglasses are perturbed using the same random homography\nnoise model as used in the 3D cubes experiment.\nWe train ST-GAN with 5 sequential warps, each for 50K\niterations with the learning rates for both Gand Dto be\n10−5. As a preconditioning step, we also pretrain the dis-\ncriminator Dusing only the initial fake samples and real\nsamples for 50K iterations with the same learning rate. We\nset λupdate = 1 in this experiment.\nA.4. Additional Indoor Object Results\nWe include additional qualitative results from the indoor\nobject experiment in Fig. 12. Compared to the baselines,\nST-GAN consistently predicts more realistic geometric cor-\nrections in most cases.\nA.5. Additional Glasses Results\nWe also include additional qualitative results from the\nglasses experiment in Fig. 13. We re-emphasize that the\ntraining data here is unpaired and there is no information\nin the dataset about where the glasses are placed. De-\nspite these, ST-GAN is able to consistently match the initial\nglasses foreground to the background faces.\nSDM\nHomogra-\nphyNet\nST-GAN\nOriginal\nrendering\nPerturbed\nsample\n(initial)\nSDM\nHomogra-\nphyNet\nST-GAN\nOriginal\nrendering\nPerturbed\nsample\n(initial)\nFigure 12: Additional qualitative results from the indoor object experiment (test set). The yellow arrows in the second row\npoint to the composited foreground objects.\nFigure 13: Additional qualitative results from the glasses experiment (test set). The top row indicates the initial composite,\nand the bottom row indicates the ST-GAN output.",
  "topic": "Image warping",
  "concepts": [
    {
      "name": "Image warping",
      "score": 0.8484293222427368
    },
    {
      "name": "Hallucinating",
      "score": 0.7803667783737183
    },
    {
      "name": "Computer science",
      "score": 0.770596981048584
    },
    {
      "name": "Transformer",
      "score": 0.6622896194458008
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5888939499855042
    },
    {
      "name": "Computer vision",
      "score": 0.5455668568611145
    },
    {
      "name": "Image translation",
      "score": 0.5409016013145447
    },
    {
      "name": "Generator (circuit theory)",
      "score": 0.5327810049057007
    },
    {
      "name": "Exploit",
      "score": 0.5231117606163025
    },
    {
      "name": "Generative grammar",
      "score": 0.45918673276901245
    },
    {
      "name": "Adversarial system",
      "score": 0.418843150138855
    },
    {
      "name": "Image (mathematics)",
      "score": 0.4185664653778076
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}