{
  "title": "Enhanced Transformer Model for Data-to-Text Generation",
  "url": "https://openalex.org/W2983105506",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A1880586979",
      "name": "Li Gong",
      "affiliations": [
        "SYSTRA (France)",
        "Systran (France)"
      ]
    },
    {
      "id": "https://openalex.org/A4256966621",
      "name": "Josep Crego",
      "affiliations": [
        "Systran (France)",
        "SYSTRA (France)"
      ]
    },
    {
      "id": "https://openalex.org/A280727668",
      "name": "Jean Sénellart",
      "affiliations": [
        "Systran (France)",
        "SYSTRA (France)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4234417935",
    "https://openalex.org/W2167984972",
    "https://openalex.org/W1681454004",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2295177601",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962944953",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2963290255",
    "https://openalex.org/W2963592583",
    "https://openalex.org/W2964165364",
    "https://openalex.org/W1521413921",
    "https://openalex.org/W2739046565",
    "https://openalex.org/W2058043539",
    "https://openalex.org/W2057900969",
    "https://openalex.org/W2962905474",
    "https://openalex.org/W2003170434",
    "https://openalex.org/W2963091658",
    "https://openalex.org/W2116716943",
    "https://openalex.org/W2119874156",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2139079654",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1985610876",
    "https://openalex.org/W2964121744"
  ],
  "abstract": "Neural models have recently shown significant progress on data-to-text generation tasks in which descriptive texts are generated conditioned on database records. In this work, we present a new Transformer-based data-to-text generation model which learns content selection and summary generation in an end-to-end fashion. We introduce two extensions to the baseline transformer model: First, we modify the latent representation of the input, which helps to significantly improve the content correctness of the output summary; Second, we include an additional learning objective that accounts for content selection modelling. In addition, we propose two data augmentation methods that succeed to further improve performance of the resulting generation models. Evaluation experiments show that our final model outperforms current state-of-the-art systems as measured by different metrics: BLEU, content selection precision and content ordering. We made publicly available the transformer extension presented in this paper.",
  "full_text": "Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 148–156\nHong Kong, China, November 4, 2019.c⃝2019 Association for Computational Linguistics\nwww.aclweb.org/anthology/D19-56%2d\n148\nEnhanced Transformer Model for Data-to-Text Generation\nLi Gong, Josep Crego, Jean Senellart\nSYSTRAN / 5 rue Feydeau, 75002 Paris, France\nfirstname.lastname@systrangroup.com\nAbstract\nNeural models have recently shown signiﬁ-\ncant progress on data-to-text generation tasks\nin which descriptive texts are generated con-\nditioned on database records. In this work,\nwe present a new Transformer-based data-to-\ntext generation model which learns content se-\nlection and summary generation in an end-\nto-end fashion. We introduce two extensions\nto the baseline transformer model: First, we\nmodify the latent representation of the input,\nwhich helps to signiﬁcantly improve the con-\ntent correctness of the output summary; Sec-\nond, we include an additional learning objec-\ntive that accounts for content selection mod-\nelling. In addition, we propose two data aug-\nmentation methods that succeed to further im-\nprove performance of the resulting generation\nmodels. Evaluation experiments show that our\nﬁnal model outperforms current state-of-the-\nart systems as measured by different metrics:\nBLEU, content selection precision and con-\ntent ordering. We made publicly available the\ntransformer extension presented in this paper1.\n1 Introduction\nData-to-text generation is an important task in\nnatural language generation (NLG). It refers to\nthe task of automatically producing a descriptive\ntext from non-linguistic structured data (tables,\ndatabase records, spreadsheets,etc.). Table 1 illus-\ntrates an example of data-to-text NLG, with statis-\ntics of a NBA basketball game (top) and the corre-\nsponding game summary (bottom).\nTraditional approaches perform the summary\ngeneration in two separate steps: content se-\nlection (“what to say”) (Duboue and McKeown,\n2001, 2003) and surface realization (“how to say\nit”) (Stent et al., 2004; Reiter et al., 2005). Af-\nter the emergence of sequence-to-sequence (S2S)\n1https://github.com/gongliym/\ndata2text-transformer\nlearning, a variety of data-to-text generation mod-\nels are proposed (Lebret et al., 2016; Mei et al.,\n2015; Wiseman et al., 2017) and trained in an\nend-to-end fashion. These models are actually\nconditional language models which generate sum-\nmaries conditioned on the latent representation of\ninput tables. Despite producing overall ﬂuent text,\nWiseman et al. (2017) show that NLG models per-\nform poorly on content-oriented measures.\nDifferent from other NLG tasks (e.g., machine\ntranslation), data-to-text generation faces several\nadditional challenges. First, data-to-text genera-\ntion models have to select the content before gen-\nerating text. In machine translation, the source\nand target sentences are semantically equivalent to\neach other, whereas in data-to-text generation, the\nmodel initially selects appropriate content from\nthe input data to secondly generate ﬂuent sen-\ntences that incorporate the selected content. Sec-\nond, the training data in data-to-text generation\ntask is often very limited. Unlike machine trans-\nlation, where training data consist of translated\nsentence pairs, data-to-text generation models are\ntrained from examples composed of structured\ndata and its corresponding descriptive summary,\nwhich are much harder to produce.\nIn this paper, we tackle both challenges previ-\nously discussed. We introduce a new data-to-text\ngeneration model which jointly learns content se-\nlection and text generation, and we present two\ndata augmentation methods. More precisely, we\nmake the following contributions:\n1. We adapt the Transformer (Vaswani et al.,\n2017) architecture by modifying the input\ntable representation (record embedding) and\nintroducing an additional objective function\n(content selection modelling).\n2. We create synthetic data following two\ndata augmentation techniques and investigate\n149\ntheir impacts on different evaluation metrics.\nWe show that our model outperforms current\nstate-of-the-art systems on BLEU, content selec-\ntion precision and content ordering metrics.\n2 Related Work\nAutomatic summary generation has been a topic\nof interest for a long time (Reiter and Dale, 1997;\nTanaka-Ishii et al., 1998). It has interesting appli-\ncations in many different domains, such as sport\ngame summary generation (Barzilay and Lapata,\n2005; Liang et al., 2009), weather-forecast gen-\neration (Reiter et al., 2005) and recipe genera-\ntion (Yang et al., 2016).\nTraditional data-to-text generation approaches\nperform the summary generation in two separate\nsteps: content selection and surface realization.\nFor content selection, a number of approaches\nwere proposed to automatically select the ele-\nments of content and extract ordering constraints\nfrom an aligned corpus of input data and output\nsummaries (Duboue and McKeown, 2001, 2003).\nIn (Barzilay and Lapata, 2005), the content selec-\ntion is treated as a collective classiﬁcation problem\nwhich allows the system to capture contextual de-\npendencies between input data items. For surface\nrealization, Stent et al. (2004) proposed to trans-\nform the input data into an intermediary structure\nand then to generate natural language text from it;\nReiter et al. (2005) presented a method to generate\ntext using consistent data-to-word rules. Angeli\net al. (2010) broke up the two steps into a sequence\nof local decisions where they used two classiﬁers\nto select content form database and another clas-\nsiﬁer to choose a suitable template to render the\ncontent.\nMore recently, work on this topic has focused\non end-to-end generation models. Konstas and\nLapata (2012) described an end-to-end generation\nmodel which jointly models content selection and\nsurface realization. Mei et al. (2015) proposed a\nneural encoder-aligner-decoder model which ﬁrst\nencodes the entire input record dataset then the\naligner module performs the content selection for\nthe decoder to generate output summary. Some\nother work extends the encoder-decoder model\nto be able to copy words directly from the in-\nput (Yang et al., 2016; Gu et al., 2016; Gulcehre\net al., 2016). Wiseman et al. (2017) investigates\ndifferent data-to-text generation approaches and\nintroduces a new corpus (ROTOWIRE , see Table 1)\nfor the data-to-text generation task along with\na series of automatic measures for the content-\noriented evaluation. Based on (Wiseman et al.,\n2017), Puduppully et al. (2019) incorporates con-\ntent selection and planing mechanisms into the\nencoder-decoder system and improves the state-\nof-the-art on the ROTOWIRE dataset.\n3 Data-to-Text Generation Model\nIn this section, we ﬁrst formulate the data-to-\ntext generation problem and introduce our data-\nto-text generation baseline model. Next, we detail\nthe extensions introduced to our baseline network,\nnamely Record Embedding and Content Selection\nModelling.\nProblem Statement\nThe objective of data-to-text generation is to gen-\nerate a descriptive summary given structured data.\nInput of the model consists of a table of records\n(see Table 1, top and middle). Let s = {ri}I\ni=1\nbe a set of records, each record ri consists of four\nfeatures:\n•Entity: the name of player or team (e.g.,\nCeltics, LeBron James)\n•Type: the table header (e.g., WIN, PTS)\n•Value: the value in the table (e.g., 14, Boston)\n•Info: game information (e.g., H/W, V/L)\nwhich represents the team or player is Home-\nor Vis-team and Win- or Loss-team.\nNote that there is no order relationship in s.\nThe output t (see Table 1, bottom) is a text\ndocument which is a descriptive summary for the\nrecord set s. Note t = t1 ...t J with J as the doc-\nument length. Pairs ( s, t) constitute the training\ndata for data-to-text generation systems. Data-to-\ntext generation probability is given by:\nP(t|s,θ) =\nJ∏\nj=1\nP(tj|s,t<j; θ) (1)\nwhere t<j = t1 ...t j−1 is the generated partial\ndocument and θis the model parameters.\nData-to-Text Transformer Model\nIn this section, we present how we adapt the Trans-\nformer model for the data-to-text generation tasks.\nFirst, the input embedding of Transformer encoder\n150\nis replaced by our record embedding to better in-\ncorporate the record information. Second, a new\nlearning objective is added into our model to im-\nprove its content-oriented performance.\n3.1 Record Embedding\nThe input of data-to-text model encoder is a se-\nquence of records. Each record is a tuple of four\nfeatures ( Entity, Type, Value, Info). Inspired by\nprevious work (Yang et al., 2016; Wiseman et al.,\n2017; Puduppully et al., 2019), we embed features\ninto vectors, and use the concatenation of feature\nembeddings as the embedding of record.\nri = [ri,1; ri,2; ri,3; ri,4] (2)\nwhere ri ∈Rdim is the ith record embedding in\nthe input sequence and ri,j ∈R\ndim\n4 is the jth fea-\nture embedding in ri.\nSince there is no order relationship within the\nrecords, the positional embedding of the Trans-\nformer encoder is removed.\n3.2 Content Selection Modeling\nBesides record embedding, we also add a new\nlearning objective into the Transformer model.\nAs presented before, we need to select the con-\ntent from the input records before generating the\noutput summary. Some records are generally im-\nportant no mater the game context, such as the\nteam name record and team score record, whereas\nthe importance of some other records depend on\nthe game context. For example, a player having\nthe highest points in the game is more likely to\nbe mentioned in the game summary. Within the\nTransformer architecture, the self-attention mech-\nanism can generate the latent representation for\neach record by jointly conditioning on all other\nrecords in the input dataset. A binary prediction\nlayer is added on top of the Transformer encoder\noutput (as shown in Figure 1) to predict whether\nor not one record will be mentioned in the target\nsummary.\nThe architecture of our data-to-text Transformer\nmodel is shown in Figure 1. As presented be-\nfore, the encoder takes the record embedding as\ninput and generates the latent representation for\neach record in the input sequence. The output of\nencoder is then used to predict the importance of\neach record and also serves as the context of the\ndecoder. The decoder of our model is the same as\nthe original Transformer model in machine trans-\nlation. It predicts the next word conditioned on\nthe encoder output and the previous tokens in the\nsummary sequence.\nIn content selection modeling, the input record\nsequences together with its label sequences are\nused to optimize the encoder by minimizing the\ncross-entropy loss. In language generation train-\ning, the encoder and decoder are trained together\nto maximize the log-likelihood of the training\ndata. The two learning objectives are trained al-\nternatively2.\nTransformer\nEncoder\nr J\n<latexit sha1_base64=\"JBdZO07IximDjYGIGbzvj4OxBGM=\">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KkkV9Fj0Ip4q2g9oQ9lsJ+3SzSbsboQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IBFcG9f9dlZW19Y3Ngtbxe2d3b390sFhU8epYthgsYhVO6AaBZfYMNwIbCcKaRQIbAWjm6nfekKleSwfzThBP6IDyUPOqLHSg+rd9Uplt+LOQJaJl5My5Kj3Sl/dfszSCKVhgmrd8dzE+BlVhjOBk2I31ZhQNqID7FgqaYTaz2anTsipVfokjJUtachM/T2R0UjrcRTYzoiaoV70puJ/Xic14ZWfcZmkBiWbLwpTQUxMpn+TPlfIjBhbQpni9lbChlRRZmw6RRuCt/jyMmlWK955pXp/Ua5d53EU4BhO4Aw8uIQa3EIdGsBgAM/wCm+OcF6cd+dj3rri5DNH8AfO5w8pvo23</latexit>\nr 1\n<latexit sha1_base64=\"w+fMBncFYKx0kEKsDtLgWG6j6to=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mqoMeiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0oPpev1xxq+4cZJV4OalAjka//NUbxCyNUBomqNZdz02Mn1FlOBM4LfVSjQllYzrErqWSRqj9bH7qlJxZZUDCWNmShszV3xMZjbSeRIHtjKgZ6WVvJv7ndVMTXvsZl0lqULLFojAVxMRk9jcZcIXMiIkllClubyVsRBVlxqZTsiF4yy+vklat6l1Ua/eXlfpNHkcRTuAUzsGDK6jDHTSgCQyG8Ayv8OYI58V5dz4WrQUnnzmGP3A+fwAD2o2e</latexit>\nr 2\n<latexit sha1_base64=\"qRYq8+LBOBKrAYqfcugvOTPcEds=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mqoMeiF48V7Qe0oWy2k3bpZhN2N0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj+hQ8pAzaqz0oPq1frniVt05yCrxclKBHI1++as3iFkaoTRMUK27npsYP6PKcCZwWuqlGhPKxnSIXUsljVD72fzUKTmzyoCEsbIlDZmrvycyGmk9iQLbGVEz0sveTPzP66YmvPYzLpPUoGSLRWEqiInJ7G8y4AqZERNLKFPc3krYiCrKjE2nZEPwll9eJa1a1buo1u4vK/WbPI4inMApnIMHV1CHO2hAExgM4Rle4c0Rzovz7nwsWgtOPnMMf+B8/gAFXo2f</latexit>\n...\n<latexit sha1_base64=\"D8NZwhGRc3SadH+lq9NyH2X2S6M=\">AAAB7HicbVBNS8NAFHzxs9avqkcvi0XwVJIq6LHoxWMF0xbaUDbbTbt0swm7L0IJ/Q1ePCji1R/kzX/jts1BWwcWhpk37HsTplIYdN1vZ219Y3Nru7RT3t3bPzisHB23TJJpxn2WyER3Qmq4FIr7KFDyTqo5jUPJ2+H4bua3n7g2IlGPOEl5ENOhEpFgFK3k9wYJmn6l6tbcOcgq8QpShQLNfuXL5lgWc4VMUmO6nptikFONgkk+Lfcyw1PKxnTIu5YqGnMT5PNlp+TcKgMSJdo+hWSu/k7kNDZmEod2MqY4MsveTPzP62YY3QS5UGmGXLHFR1EmCSZkdjkZCM0ZyokllGlhdyVsRDVlaPsp2xK85ZNXSate8y5r9YerauO2qKMEp3AGF+DBNTTgHprgAwMBz/AKb45yXpx352MxuuYUmRP4A+fzB/K2jsY=</latexit>\nPred_layer\n0/1\n<latexit sha1_base64=\"TLXKA4mwwbXo2dx+G5FnabsZdt0=\">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4qkkV9Fj04rGi/YA2lM120y7dbMLuRCihP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEikMuu63s7K6tr6xWdgqbu/s7u2XDg6bJk414w0Wy1i3A2q4FIo3UKDk7URzGgWSt4LR7dRvPXFtRKwecZxwP6IDJULBKFrpwT33eqWyW3FnIMvEy0kZctR7pa9uP2ZpxBUySY3peG6CfkY1Cib5pNhNDU8oG9EB71iqaMSNn81OnZBTq/RJGGtbCslM/T2R0ciYcRTYzoji0Cx6U/E/r5NieO1nQiUpcsXmi8JUEozJ9G/SF5ozlGNLKNPC3krYkGrK0KZTtCF4iy8vk2a14l1UqveX5dpNHkcBjuEEzsCDK6jBHdShAQwG8Ayv8OZI58V5dz7mrStOPnMEf+B8/gBWT40s</latexit>\n0/1\n<latexit sha1_base64=\"TLXKA4mwwbXo2dx+G5FnabsZdt0=\">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4qkkV9Fj04rGi/YA2lM120y7dbMLuRCihP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEikMuu63s7K6tr6xWdgqbu/s7u2XDg6bJk414w0Wy1i3A2q4FIo3UKDk7URzGgWSt4LR7dRvPXFtRKwecZxwP6IDJULBKFrpwT33eqWyW3FnIMvEy0kZctR7pa9uP2ZpxBUySY3peG6CfkY1Cib5pNhNDU8oG9EB71iqaMSNn81OnZBTq/RJGGtbCslM/T2R0ciYcRTYzoji0Cx6U/E/r5NieO1nQiUpcsXmi8JUEozJ9G/SF5ozlGNLKNPC3krYkGrK0KZTtCF4iy8vk2a14l1UqveX5dpNHkcBjuEEzsCDK6jBHdShAQwG8Ayv8OZI58V5dz7mrStOPnMEf+B8/gBWT40s</latexit>\n0/1\n<latexit sha1_base64=\"TLXKA4mwwbXo2dx+G5FnabsZdt0=\">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4qkkV9Fj04rGi/YA2lM120y7dbMLuRCihP8GLB0W8+ou8+W/ctjlo64OBx3szzMwLEikMuu63s7K6tr6xWdgqbu/s7u2XDg6bJk414w0Wy1i3A2q4FIo3UKDk7URzGgWSt4LR7dRvPXFtRKwecZxwP6IDJULBKFrpwT33eqWyW3FnIMvEy0kZctR7pa9uP2ZpxBUySY3peG6CfkY1Cib5pNhNDU8oG9EB71iqaMSNn81OnZBTq/RJGGtbCslM/T2R0ciYcRTYzoji0Cx6U/E/r5NieO1nQiUpcsXmi8JUEozJ9G/SF5ozlGNLKNPC3krYkGrK0KZTtCF4iy8vk2a14l1UqveX5dpNHkcBjuEEzsCDK6jBHdShAQwG8Ayv8OZI58V5dz7mrStOPnMEf+B8/gBWT40s</latexit>\nTransformer\nDecoder\ny 1\n<latexit sha1_base64=\"gXLzr9lA6QyErQrPkt90wKdvXMk=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mqoMeiF48V7Qe0oWy2m3bpZhN2J0Io/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6xCzhfkSHSoSCUbTSQ9b3+uWKW3XnIKvEy0kFcjT65a/eIGZpxBUySY3pem6C/oRqFEzyaamXGp5QNqZD3rVU0YgbfzI/dUrOrDIgYaxtKSRz9ffEhEbGZFFgOyOKI7PszcT/vG6K4bU/ESpJkSu2WBSmkmBMZn+TgdCcocwsoUwLeythI6opQ5tOyYbgLb+8Slq1qndRrd1fVuo3eRxFOIFTOAcPrqAOd9CAJjAYwjO8wpsjnRfn3flYtBacfOYY/sD5/AEOhI2l</latexit>\ny T\n<latexit sha1_base64=\"JJSaiNpL04FogISMGU77Y0OBc24=\">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KkkV9Fj04rFiv6ANZbOdtEs3m7C7EULpT/DiQRGv/iJv/hu3bQ7a+mDg8d4MM/OCRHBtXPfbWVvf2NzaLuwUd/f2Dw5LR8ctHaeKYZPFIladgGoUXGLTcCOwkyikUSCwHYzvZn77CZXmsWyYLEE/okPJQ86osdJj1m/0S2W34s5BVomXkzLkqPdLX71BzNIIpWGCat313MT4E6oMZwKnxV6qMaFsTIfYtVTSCLU/mZ86JedWGZAwVrakIXP198SERlpnUWA7I2pGetmbif953dSEN/6EyyQ1KNliUZgKYmIy+5sMuEJmRGYJZYrbWwkbUUWZsekUbQje8surpFWteJeV6sNVuXabx1GAUziDC/DgGmpwD3VoAoMhPMMrvDnCeXHenY9F65qTz5zAHzifP0OQjcg=</latexit>\n...\n<latexit sha1_base64=\"D8NZwhGRc3SadH+lq9NyH2X2S6M=\">AAAB7HicbVBNS8NAFHzxs9avqkcvi0XwVJIq6LHoxWMF0xbaUDbbTbt0swm7L0IJ/Q1ePCji1R/kzX/jts1BWwcWhpk37HsTplIYdN1vZ219Y3Nru7RT3t3bPzisHB23TJJpxn2WyER3Qmq4FIr7KFDyTqo5jUPJ2+H4bua3n7g2IlGPOEl5ENOhEpFgFK3k9wYJmn6l6tbcOcgq8QpShQLNfuXL5lgWc4VMUmO6nptikFONgkk+Lfcyw1PKxnTIu5YqGnMT5PNlp+TcKgMSJdo+hWSu/k7kNDZmEod2MqY4MsveTPzP62YY3QS5UGmGXLHFR1EmCSZkdjkZCM0ZyokllGlhdyVsRDVlaPsp2xK85ZNXSate8y5r9YerauO2qKMEp3AGF+DBNTTgHprgAwMBz/AKb45yXpx352MxuuYUmRP4A+fzB/K2jsY=</latexit>\nSoftmax\n...\n<latexit sha1_base64=\"D8NZwhGRc3SadH+lq9NyH2X2S6M=\">AAAB7HicbVBNS8NAFHzxs9avqkcvi0XwVJIq6LHoxWMF0xbaUDbbTbt0swm7L0IJ/Q1ePCji1R/kzX/jts1BWwcWhpk37HsTplIYdN1vZ219Y3Nru7RT3t3bPzisHB23TJJpxn2WyER3Qmq4FIr7KFDyTqo5jUPJ2+H4bua3n7g2IlGPOEl5ENOhEpFgFK3k9wYJmn6l6tbcOcgq8QpShQLNfuXL5lgWc4VMUmO6nptikFONgkk+Lfcyw1PKxnTIu5YqGnMT5PNlp+TcKgMSJdo+hWSu/k7kNDZmEod2MqY4MsveTPzP62YY3QS5UGmGXLHFR1EmCSZkdjkZCM0ZyokllGlhdyVsRDVlaPsp2xK85ZNXSate8y5r9YerauO2qKMEp3AGF+DBNTTgHprgAwMBz/AKb45yXpx352MxuuYUmRP4A+fzB/K2jsY=</latexit>\ny 1\n<latexit sha1_base64=\"gXLzr9lA6QyErQrPkt90wKdvXMk=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mqoMeiF48V7Qe0oWy2m3bpZhN2J0Io/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6xCzhfkSHSoSCUbTSQ9b3+uWKW3XnIKvEy0kFcjT65a/eIGZpxBUySY3pem6C/oRqFEzyaamXGp5QNqZD3rVU0YgbfzI/dUrOrDIgYaxtKSRz9ffEhEbGZFFgOyOKI7PszcT/vG6K4bU/ESpJkSu2WBSmkmBMZn+TgdCcocwsoUwLeythI6opQ5tOyYbgLb+8Slq1qndRrd1fVuo3eRxFOIFTOAcPrqAOd9CAJjAYwjO8wpsjnRfn3flYtBacfOYY/sD5/AEOhI2l</latexit>\ny 2\n<latexit sha1_base64=\"UmY8miGJFsYtImgQ4UOSFc3rPPg=\">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mqoMeiF48V7Qe0oWy2m3bpZhN2J0Io/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6xCzhfkSHSoSCUbTSQ9av9csVt+rOQVaJl5MK5Gj0y1+9QczSiCtkkhrT9dwE/QnVKJjk01IvNTyhbEyHvGupohE3/mR+6pScWWVAwljbUkjm6u+JCY2MyaLAdkYUR2bZm4n/ed0Uw2t/IlSSIldssShMJcGYzP4mA6E5Q5lZQpkW9lbCRlRThjadkg3BW355lbRqVe+iWru/rNRv8jiKcAKncA4eXEEd7qABTWAwhGd4hTdHOi/Ou/OxaC04+cwx/IHz+QMQCI2m</latexit>\n<eos>\n<latexit sha1_base64=\"xDzQvAFdLmBTev1IjeBitAarx+g=\">AAAB9XicbVDLSgNBEJyNrxhfUY9eBoPgKexGQQ8iQS8eI5gHJGuYnfQmQ2YfzPSqYcl/ePGgiFf/xZt/4yTZgyYWNBRV3XR3ebEUGm3728otLa+sruXXCxubW9s7xd29ho4SxaHOIxmplsc0SBFCHQVKaMUKWOBJaHrD64nffAClRRTe4SgGN2D9UPiCMzTSfQfhCRHTC4j05bhbLNllewq6SJyMlEiGWrf41elFPAkgRC6Z1m3HjtFNmULBJYwLnURDzPiQ9aFtaMgC0G46vXpMj4zSo36kTIVIp+rviZQFWo8Cz3QGDAd63puI/3ntBP1zNxVhnCCEfLbITyTFiE4ioD2hgKMcGcK4EuZWygdMMY4mqIIJwZl/eZE0KmXnpFy5PS1Vr7I48uSAHJJj4pAzUiU3pEbqhBNFnskrebMerRfr3fqYteasbGaf/IH1+QPyi5LM</latexit>\n<bos>\n<latexit sha1_base64=\"8AaeR7MZfhElgpaTf2flakQjM2E=\">AAAB9XicbVBNSwMxEM3Wr1q/qh69BIvgqexWQQ8iRS8eK9gPaNeSTbNtaDZZklm1LP0fXjwo4tX/4s1/Y9ruQVsfDDzem2FmXhALbsB1v53c0vLK6lp+vbCxubW9U9zdaxiVaMrqVAmlWwExTHDJ6sBBsFasGYkCwZrB8HriNx+YNlzJOxjFzI9IX/KQUwJWuu8AewKA9CJQ5nLcLZbcsjsFXiReRkooQ61b/Or0FE0iJoEKYkzbc2PwU6KBU8HGhU5iWEzokPRZ21JJImb8dHr1GB9ZpYdDpW1JwFP190RKImNGUWA7IwIDM+9NxP+8dgLhuZ9yGSfAJJ0tChOBQeFJBLjHNaMgRpYQqrm9FdMB0YSCDapgQ/DmX14kjUrZOylXbk9L1assjjw6QIfoGHnoDFXRDaqhOqJIo2f0it6cR+fFeXc+Zq05J5vZR3/gfP4A7fOSyQ==</latexit>\nFigure 1: Model Architecture\n4 Data Augmentation Methods\nIn data-to-text generation task, the model needs to\nnot only generate ﬂuent text, but also generate text\nwhich is coherent with the input records. Several\ncontent-oriented evaluation metrics are proposed\nin (Wiseman et al., 2017) to evaluate such cohe-\nsion, including the precision of record generation\nand the recall rate with respect to the records in\ngold summary.\nIn this section, we present two data augmenta-\ntion methods: synthetic data generation and train-\ning data selection. Each of them has different im-\npacts on the content-oriented evaluation results.\n4.1 Synthetic Data Generation\nIn order to improve the cohesion between the in-\nput records and output summary, we need more\ndata to enhance the encoder-decoder attention of\nthe decoder. Here we introduce a method to gen-\nerate synthetic training data.\nWe ﬁrst randomly change the values of records\nand the changed record set (s′) is then used to gen-\nerate automatic summary (t′) by a trained data-to-\ntext system. The synthetic data pairs ( s′, t′) are\nthen used to improve such system.\n2An alternative approach is joint training that achieves\ncomparable results.\n151\nThis idea is inspired by the back-translation\ntechnique widely used in neural machine transla-\ntion, with two important differences:\nFirst, back-translation, typically employs\nmonolingual human texts, which are easy found.\nIn our case, since it is difﬁcult to ﬁnd additional\nstructured (table) data for the same kind of game\nmatches, we use the existing data sets and intro-\nduce variations in the values of the table records.\nIn order to keep the data cohesion in the table, the\nchange is constrained with the following rules:\n•only numeric values are changed. Non-\nnumeric values such as the position of a\nplayer or the city name of a team are kept the\nsame.\n•after the change, the team scores should not\nviolate the win/loss relation\n•the changed values should stay in the normal\nrange of its value type. It should not bigger\nthan its maximum value or smaller than its\nminimum value through all games.\nOur data generation technique doubles the amount\nof training data available for learning.\nSecond, another difference with the back-\ntranslation technique is the “translation direc-\ntion”. In machine translation, the additional\nmonolingual text used is found in target lan-\nguage, and back-translated into the source lan-\nguage. Thus, ensuring that the target side of the\nsynthetic data follows the same distribution as real\nhuman texts. In our case, the target side of syn-\nthetic data is also automatically generated which\nis known to introduce noise in the resulting net-\nwork.\n4.2 Training Data Selection\nA deﬁciency of data-to-text NLG systems is the\npoor coverage of relations produced in the gener-\nated summaries. In order to increase the coverage,\na simple solution consists of learning to produce\na larger number of relations. Here, we present a\nstraightforward method to bias our model to out-\nput more relations by means of ﬁne-tuning on the\ntraining examples containing a greater number of\nrelations.\nWe use an information extraction (IE) system\nto extract the number of relations of each train-\ning summary. Then, we select for ﬁne-tuning our\nbaseline model the subset of training data in which\neach summary contains at least N relations. In\nthis work, we take advantage of the IE system 3\nprovided by (Puduppully et al., 2019), and the dis-\ntribution of the number of relations in the training\nsummary is illustrated in Figure 2.\nFigure 2: relation count distribution in training data.\n5 Experimental Setup\n5.1 Data and Preprocessing\nWe run the experiments with the R OTOWIRE\ndataset (Wiseman et al., 2017), a dataset of NBA\nbasketball game summaries, paired with their cor-\nresponding box- and line-score tables. Table 1 il-\nlustrates an example of the dataset. In the box-\nscore table, each team has at most 13 players and\neach player is described by 23 types of values.\nIn the line-score table, each team has 15 differ-\nent types of values. In addition, the date of each\ngame is converted into the day of the week (such\nas “Saturday”) as an additional record. In the pre-\nprocessing step, the input box- and line-score ta-\nbles are converted into a ﬁx-length sequence of\nrecords. Each sequence contains 629 records.4 As\nfor the associate summaries, the average length is\n337 tokens, and the vocabulary size is 11.3K. The\nROTOWIRE dataset contains 4853 summaries in\ntotal, in which 3398 summaries are for training,\n727 for validation and 728 for test.\nIn content selection modelling, we need the la-\nbels of input records to indicate which records in\nthe input will be mentioned in the output sum-\nmary. Here we use a very simple method to gener-\n3The model is publicly available athttps://github.\ncom/ratishsp/data2text-plan-py\n4In the 629 records, 598 records are for players, 30\nrecords for teams and 1 record for the date.\n152\nNAME POS MIN PTS FGM FGA FGPCT FG3M FG3A FG3PCT FTM FTA FTPCT OREB DREB REB AST TO STL BLK PFMatt Barnes F 26 0 0 3 0 0 3 0 0 0 0 1 4 5 4 1 0 0 0Blake Grifﬁn F 34 24 10 17 59 0 0 0 4 5 80 4 2 6 8 4 1 0 3DeAndre Jordan C 34 9 4 8 50 0 0 0 1 4 25 5 11 16 0 1 1 2 4JJ Redick G 34 23 9 15 60 5 8 63 0 0 0 0 3 3 2 1 1 0 2Chris Paul G 36 27 6 16 38 4 6 67 11 12 92 1 2 3 9 2 2 1 3Glen Davis N/A 13 2 1 2 50 0 0 0 0 0 0 0 4 4 1 0 3 0 0Jamal Crawford N/A 29 17 5 16 31 3 8 38 4 6 67 0 2 2 2 1 2 1 2Hedo Turkoglu N/A 6 0 0 0 0 0 0 0 0 0 0 0 2 2 0 1 0 0 1Reggie Bullock N/A 14 2 1 1 100 0 0 0 0 0 0 0 0 0 1 0 0 0 0Jordan Farmar N/A 12 2 1 3 33 0 1 0 0 0 0 0 0 0 2 1 0 0 3Jared Cunningha N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/AChris Douglas-R N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/AEkpe Udoh N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/AGiannis Antetok F 38 18 8 12 67 0 1 0 2 3 67 1 8 9 6 3 2 0 3Johnny O’Bryant F 6 4 2 3 67 0 0 0 0 0 0 0 0 0 0 0 0 0 2Larry Sanders C 26 10 5 6 83 0 0 0 0 2 0 2 5 7 3 0 1 1 5O.J. Mayo G 23 3 1 6 17 0 2 0 1 3 33 0 1 1 3 2 0 0 4Brandon Knight G 27 8 3 10 30 2 6 33 0 0 0 1 4 5 5 4 0 0 3Jared Dudley N/A 30 16 7 12 58 2 4 50 0 0 0 2 6 8 3 3 2 0 2Zaza Pachulia N/A 20 5 1 3 33 0 0 0 3 4 75 2 5 7 2 2 0 0 1Jerryd Bayless N/A 28 16 7 13 54 2 3 67 0 0 0 1 3 4 2 1 0 0 4Khris Middleton N/A 24 12 5 10 50 1 5 20 1 1 100 1 3 4 2 0 1 0 2Kendall Marshal N/A 18 10 4 6 67 1 3 33 1 2 50 0 1 1 3 3 0 0 0Damien Inglis N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/AJabari Parker N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/ANate Wolters N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A\nTEAM-NAME CITY PQTR1 PQTR2 PQTR3 PQTR4 PTS FGPCT FG3PCT FTPCT REB AST TOV WINS LOSSES\nClippers Los Angeles 28 22 32 24 106 46 46 74 41 29 12 19 8Bucks Milwaukee 24 28 31 19 102 53 33 53 46 29 18 14 14\nThe Los Angeles Clippers (19-8) defeated the Milwaukee Bucks (14-14) 106-102 on Saturday. Los\nAngeles has won three of their last four games. Chris Paul paced the team with a game-high 27 points and\nnine assists. DeAndre Jordan continued his impressive work on the boards, pulling down 16 rebounds,\nand Blake Grifﬁn and J.J. Redick joined Paul in scoring over 20 points. The Clippers have a tough\nstretch of their schedule coming up with the Spurs, Hawks, Warriors and Raptors all on this week’s\ndocket. Even with the loss, Milwaukee ﬁnished their four-game Western Conference road trip 2-2, a job\nwell done by the developing squad. In the three games since Jabari Parker went down with a season-\nending ACL injury, coach Jason Kidd has cut the umbilical cord they had on Giannis Antetokounmpo.\nHe played over 37 minutes for the second straight game Saturday, which is ten more minutes than his\nseason average of 27 minutes per game. Larry Sanders returned to the starting lineup after sitting out\nThursday’s game on a league mandated one-game suspension. Ersan Ilyasova (concussion) and John\nHenson (foot) remain out, and it seems Ilyasova may be closer to returning than Henson.\nTable 1: An example of box-score (top), line-score (middle) and the corresponding summary (bottom) from\nROTOWIRE dataset. The deﬁnition of table header could be found at https://github.com/harvardnlp/\nboxscore-data\nate such labels. First, we label the entity records 5.\nAn entity record is labeled as 1 if its value is men-\ntioned in the associated summary, otherwise it is\nlabeled as 0. Second, for each player or team men-\ntioned in the summary, the rest of its values in the\ntable are labeled as1 if they occur in the same sen-\ntence in the summary.\n5.2 Evaluation metrics\nThe model output is evaluated with BLEU (Pa-\npineni et al., 2002) as well as several content-\noriented metrics proposed by (Wiseman et al.,\n2017) including three following aspects:\n5Record whose Value feature is an entity (see Section 3),\nfor example: “LeBron James|NAME|LeBron James|H/W”.\nThe labeling is according to the Value feature\n•Relation Generation (RG) evaluates the num-\nber of extracted relations in automatic sum-\nmaries and their correctness (precision) w.r.t\nthe input record dataset;\n•Content Selection (CS) evaluates the preci-\nsion and recall rate of extracted relations in\nautomatic summaries w.r.t that in the gold\nsummaries;\n•Content Ordering (CO) evaluates the normal-\nized Damerau-Levenshtein Distance (Brill\nand Moore, 2000) between the sequence of\nextracted relations in automatic summaries\nand that in the gold summaries.\nAll these content-oriented metrics are based on\nan IE system which extracts record relations from\n153\nModel RG CS CO BLEU# P% P% R% DLD%\nGOLD 23.32 94.77 100 100 100 100\nTEMPL 54.29 99.9226.61 59.16 14.42 8.51\nWS-2017 23.95 75.1028.11 35.86 15.33 14.57\nNCP-2019 33.88 87.5133.52 51.21 18.57 16.19\nDATA-TRANS 23.31 79.8136.90 43.06 22.75 20.60\n+DATAGEN 22.5982.4939.4842.84 23.32 19.76\n+DATASEL 26.9479.5435.2747.49 22.22 19.97\n+BOTH 24.24 80.5237.33 44.66 23.04 20.22\nTable 2: Automatic evaluation on R OTOWIRE devel-\nopment set using relation generation (RG) count (#)\nand precision (P%), content selection (CS) precision\n(P%) and recall (R%), content ordering (CO) in nor-\nmalized Damerau-Levenshtein distance (DLD%), and\nBLEU.\nsummaries. For the purpose of comparison, we\ndirectly use the publicly available IE system of\n(Puduppully et al., 2019) to evaluate our models.\n5.3 Training Details\nIn all experiments, we use our model with 1 en-\ncoder layer and 6 decoder layers, 512 hidden\nunits (hence, the record feature embedding size\nis 128, see Section 3), 8 heads, GELU activa-\ntions (Hendrycks and Gimpel, 2016), a dropout\nrate of 0.1 and learned positional embedding for\nthe decoder. The model is trained with the Adam\noptimizer (Kingma and Ba, 2014), learning rate is\nﬁxed to 10−4 and batch size is 6. As for inference,\nwe use beam size 4 for all experiments, and the\nmaximum decoding length is 600.\nWe implement all our models in Pytorch, and\ntrain them on 1 GTX 1080 GPU.\n6 Results\nThe results of our model on the development\nset are summarized in Table 2. G OLD repre-\nsents the evaluation result on the gold summary.\nThe RG precision rate is 94.77%, indicating that\nthe IE system for evaluation is not perfect but\nhas very high precision. After that, results of\nthree contrast systems are reported, where TEMPL\nand WS-2017 are the updated results 6 of Wise-\nman et al. (2017) models. T EMPL is template-\nbased generator model which generates a sum-\nmary consisting of 8 sentences: a general de-\nscription sentence about the teams playing in the\ngame, 6 player-speciﬁc sentences and a conclusion\nsentence. WS-2017 reports an encoder-decoder\n6Here we all use the IE system of (Puduppully et al., 2019)\nwhich is improved from the original IE system of (Wiseman\net al., 2017)\nmodel with conditional copy mechanism. NCP-\n2019 is the best system conﬁguration (NCP+CC)\nreported in (Puduppully et al., 2019) which is a\nneural content planning model enhanced with con-\nditional copy mechanism. As for our model, re-\nsults with four conﬁgurations are reported.\nDATA-TRANS represents our data-to-text\nTransformer model (as illustrated in Figure 1)\nwithout any data augmentation. Comparing to\nNCP-2019, our model performs 3.4% higher\non content selection precision, 4.2% higher on\ncontent ordering metric and 4.4 points higher\non BLEU. Our model performs better on the\nCO metric, we attribute this improvement to that\nour model generates nearly the same number of\nrelations as the gold summary which reduces\nthe edit distance between the two sequences of\nrelations. However, our model is 7.7% lower on\nRG precision. And on the CS recall rate, our\nmodel is 8.2% lower than NCP-2019. This is\nprobably due to the fact that NCP-2019 generates\nmuch more records than our model (33.88 vs.\n23.31) which could result higher coverage on the\nrelations in gold summary.\nComparing to T EMPL and WS-2017, our\nmodel is much better on BLEU and CS precision.\nOur model generates nearly the same number of\nrelations as WS-2017, but with 7.2% higher on\nrecall rate and 7.4% higher on CO metric.\nBy synthetic data generation (+DATA GEN ), we\ngenerate synthetic table records as described in se-\ncion 4.1. These synthetic table records are then\nused as input to the DATA-TRANS model to gener-\nate summaries. All training table records are used\nto generate synthetic data. The synthetic data is\nthen combined with the original training data to\nﬁne-tune the DATA-TRANS model. From Table 2,\nwe can see that the RG and CS precisions are both\nimproved by 2.7% and 2.6% respectively. There is\nno signiﬁcant change on others metrics. The CO\nmetric is slightly improved due to higher RG and\nCS precisions. The CS recall rate is slightly de-\ngraded with the number of extracted relations.\nBy training data selection (+DATA SEL ), we se-\nlect the data whose summary contains the num-\nber of relations N >= 16 as the new training\ndata. The result training data size is 2242 (original\nsize: 3398). It is then used to ﬁne-tune the D ATA-\nTRANS model. As shown in Table 2, as expected,\nthe model after ﬁne-tuning generates more rela-\ntions in the output summaries. The average num-\n154\nModel RG CS CO BLEU# P% P% R% DLD%\nTEMPL 54.23 99.9426.99 58.16 14.92 8.46\nWS-2017 23.72 74.8029.49 36.18 15.42 14.19\nNCP-2019 34.28 87.4734.18 51.22 18.58 16.50\nDATA-TRANS 24.12 79.1736.48 42.74 22.40 20.16\n+DATAGEN 24.0183.8938.9842.85 23.02 19.48\n+DATASEL 27.4780.7035.3346.25 21.87 20.03\n+BOTH 24.80 81.0837.10 43.78 22.51 20.14\nTable 3: Automatic evaluation on R OTOWIRE test set.\nModel RG CS CO BLEU# P% P% R% DLD%\nDATA-TRANS 23.31 79.8136.90 43.06 22.75 20.60\n-CSOBJ 23.37 72.7032.67 41.99 21.14 20.28\n-RECEMB 18.00 63.1432.94 37.71 21.15 20.24\nTable 4: Ablation results on R OTOWIRE dev set.\nber of relations in the output summaries increases\nfrom 23.31 to 26.94. Respectively, the CS recall is\nincreased from 43.06% to 47.49%. However, the\nCS precision is slightly degraded by 1.6%.\nFinally, we combine both of the data augmen-\ntation methods (+ BOTH ). Synthetic data genera-\ntion improves the RG and CS precisions. Train-\ning data selection improves the CS recall rate by\nmaking the model generate more relations. To\ncombine the two methods, we choose to ﬁne-tune\nthe + DATA GEN model with the selected train-\ning data of + DATA SEL (so this conﬁguration is\nactually + DATA GEN +DATA SEL ). As shown in\nTable 2, all content-oriented evaluation metrics\nare improved compared to D ATA-TRANS but not\nas much as each single of the data augmentation\nmethod. This conﬁguration is like a trade-off be-\ntween the two data augmentation conﬁgurations.\nResults on the test set are reported in Table 3.\nThey follow the same pattern as those found on\nthe development set. Our D ATA-TRANS model\noutperforms all other contrast systems on BLEU,\nCS precision and content ordering metrics. The\nsynthetic data generation method helps to improve\nthe RG and CS precisions. The training data se-\nlection method improves the CS recall by mak-\ning the model generate more relations. Combining\nthese two data augmentation methods, all content-\noriented evaluation results are improved compared\nto DATA-TRANS . However, there is no signiﬁcant\nchange on BLEU.\n7 Ablation Experiments\nNext we evaluate the extensions introduced in our\ndata-to-text Transformer model (D ATA-TRANS )\nby means of ablation experiments. This is:\n•The concatenation of feature embeddings as\ninput of the encoder presented in Section 3.1\nin order to generate a better representation of\nthe input records.\n•The secondary learning objective presented\nin Sectioin 3.2 aiming at improving the\ncontent-oriented results.\nRemoving the content selection additional ob-\njective function In this conﬁguration, we keep\nthe same data embedding and the model architec-\nture as the DATA-TRANS , but the model is trained\nwithout the content selection objective. The eval-\nuation results are shown in Table 4 (-CS OBJ ). We\ncan see that the CS precision and CS recall are de-\ngraded by 4.2% and 1% respectively. The model\nextracts nearly the same number of records as the\nbaseline system, but with much lower precision.\nThe content ordering metric is also degraded by\n1.6%. Surprisingly, there is no signiﬁcant change\non BLEU.\nRemoving Record Encoding In this conﬁgu-\nration, the record encoding is removed from the\nDATA-TRANS model. Instead, we directly use the\nValue feature (see Section 3) sequence as the in-\nput. To keep model size unchanged, the dimen-\nsion of embedding for the Value feature sequence\nis four times bigger than the original feature em-\nbedding size (see Equation 2). In addition, we also\nadd back the positional embedding for the input\nsequence. Since the record sequence has a ﬁxed\nlength of 629, the positional embedding could help\nto build a 1-to-1 mapping from the position in\nrecord sequence and the position in the real table.\nThe model is trained with the same data and the\nsame conﬁguration as DATA-TRANS . From the re-\nsults in Table 4 (-REC EMB ), we can see that with-\nout record embedding all content-oriented evalu-\nation results are degraded, especially the RG pre-\ncision and CS recall. And again, the model still\nachieves comparable BLEU score with D ATA-\nTRANS which demonstrates the effectiveness of\nTransformer model on language modeling.\nAn example output of - REC EMB system is\nshown in Table 5 (left). The generation has high\nprecision at the beginning, and many erroneous re-\nlations are generated after several sentences. Our\nDATA-TRANS performs much better, but we can\nalso observe such problem. The generation has\nhigh precision at the beginning and the quality de-\ngraded after several sentences. We believe this is\n155\nThe Los Angeles Clippers (19-8) defeated the Milwaukee\nBucks (14-14) 106-102 on Saturday. Milwaukee has won\nfour straight games. They were paced by J.J. Redick’s game\nwith 23 points, ﬁve assists and ﬁve rebounds. Chris Paul had\na nice game with 27 points and nine assists to go along with a\ndouble-double with nine points and nine assists. The Clippers\nshot 53 percent from the ﬁeld and 46 percent from beyond\nthe arc. Milwaukee will wrap up their two-game road trip\nin Houston against the Grizzlies on Tuesday. Milwaukee has\nlost four straight games. They’ve lost ﬁve of their last ﬁve\ngames. Chris Paul (ankle) and Blake Grifﬁn (knee) sat out\nSaturday’s game. The Clippers missed their last two games\nwith a hamstring strain. Jordan had to leave the lead the team\nwith a foot injury but were able to return for the Clippers to\naction on Friday.\nThe Los Angeles Clippers (19-8) defeated the Milwaukee\nBucks (14-14) 106-102 on Saturday. Los Angeles stopped\ntheir two-game losing streak with the win. Jamal Crawford\npaced the team with a game-high 17 points in 29 minutes off\nthe bench. Crawford shot 9-of-16 from the ﬁeld and 3-of-8\nfrom downtown. He had nine assists, two rebounds and two\nsteals in 29 minutes. Blake Grifﬁn had 24 points, eight as-\nsists, six rebounds and one steal in 34 minutes. The Clippers\nwill go on the road to face the Denver Nuggets on Monday.\nMilwaukee has lost two straight, and are now 9-2 in their last\n10 games. Jabari Parker (ankle) didn’t play Saturday as he\nrecorded a double-double with 18 points and nine rebounds.\nGiannis Antetokounmpo (8-12 FG, 2-1 3Pt, 2-3 FT) and nine\nrebounds in 38 minutes off the bench. The Clippers will stay\nhome and host the Brooklyn Nets on Monday.\nTable 5: Example output from D ATA-TRANS (right) and ablation model - REC EMB (left). The corresponding\nbox- and line-table are given in Table 1. Text that accurately reﬂects a record in the associated table data is in blue,\nerroneous text is in red. Text in black is not contradictory to the table records and text in orange is self-contradictory\nwithin the summary.\ncaused by the error accumulation effect in autore-\ngressive decoding.\nAnother problem we have observed, not only\nin Table 5 but also in other output summaries, is\nrepetition and self-contradictory. In the left ex-\nample of Table 5, it contains two sentences (in\norange color) which are completely contradictory\nwith each other. And in the right example, the sen-\ntence in orange color contains contradictory infor-\nmation within the sentence.\n8 Conclusions\nWe presented a Transformer-based data-to-text\ngeneration model. Experimental results have\nshown that our two modiﬁcations on the Trans-\nformer model signiﬁcantly improve the content-\noriented evaluation metrics. In addition, we pro-\nposed two data augmentation methods, each of\nthem improves different aspects of the model. Our\nﬁnal model outperforms current state-of-the-art\nsystem on BLEU, content selection precision and\ncontent ordering metics. And we believe it has\ngreat potential for the future work. In the next step,\nwe would like to apply some experimental tech-\nniques of machine translation such as right-to-left\ndecoding and system ensemble to the data-to-text\ngeneration task.\nReferences\nGabor Angeli, Percy Liang, and Dan Klein. 2010. A\nsimple domain-independent probabilistic approach\nto generation. In Proceedings of the 2010 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 502–512. Association for Com-\nputational Linguistics.\nRegina Barzilay and Mirella Lapata. 2005. Collective\ncontent selection for concept-to-text generation. In\nProceedings of the conference on Human Language\nTechnology and Empirical Methods in Natural Lan-\nguage Processing, pages 331–338. Association for\nComputational Linguistics.\nEric Brill and Robert C Moore. 2000. An improved er-\nror model for noisy channel spelling correction. In\nProceedings of the 38th Annual Meeting on Associa-\ntion for Computational Linguistics, pages 286–293.\nAssociation for Computational Linguistics.\nPablo A Duboue and Kathleen R McKeown. 2001.\nEmpirically estimating order constraints for content\nplanning in generation. In Proceedings of the 39th\nAnnual Meeting on Association for Computational\nLinguistics, pages 172–179. Association for Com-\nputational Linguistics.\nPablo A Duboue and Kathleen R McKeown. 2003. Sta-\ntistical acquisition of content selection rules for nat-\nural language generation. In Proceedings of the\n2003 conference on Empirical methods in natural\nlanguage processing , pages 121–128. Association\nfor Computational Linguistics.\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor OK\nLi. 2016. Incorporating copying mechanism in\nsequence-to-sequence learning. arXiv preprint\narXiv:1603.06393.\n156\nCaglar Gulcehre, Sungjin Ahn, Ramesh Nallap-\nati, Bowen Zhou, and Yoshua Bengio. 2016.\nPointing the unknown words. arXiv preprint\narXiv:1603.08148.\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nIoannis Konstas and Mirella Lapata. 2012. Unsuper-\nvised concept-to-text generation with hypergraphs.\nIn Proceedings of the 2012 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 752–761. Association for Computational Lin-\nguistics.\nR´emi Lebret, David Grangier, and Michael Auli. 2016.\nNeural text generation from structured data with ap-\nplication to the biography domain. arXiv preprint\narXiv:1603.07771.\nPercy Liang, Michael I Jordan, and Dan Klein. 2009.\nLearning semantic correspondences with less super-\nvision. In Proceedings of the Joint Conference of the\n47th Annual Meeting of the ACL and the 4th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing of the AFNLP: Volume 1-Volume 1 , pages\n91–99. Association for Computational Linguistics.\nHongyuan Mei, Mohit Bansal, and Matthew R Walter.\n2015. What to talk about and how? selective gen-\neration using lstms with coarse-to-ﬁne alignment.\narXiv preprint arXiv:1509.00838.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019.\nData-to-text generation with content selection and\nplanning. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , volume 33, pages 6908–\n6915.\nEhud Reiter and Robert Dale. 1997. Building applied\nnatural language generation systems. Natural Lan-\nguage Engineering, 3(1):57–87.\nEhud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,\nand Ian Davy. 2005. Choosing words in computer-\ngenerated weather forecasts. Artiﬁcial Intelligence,\n167(1-2):137–169.\nAmanda Stent, Rashmi Prasad, and Marilyn Walker.\n2004. Trainable sentence planning for complex in-\nformation presentation in spoken dialog systems. In\nProceedings of the 42nd annual meeting on associa-\ntion for computational linguistics, page 79. Associ-\nation for Computational Linguistics.\nKumiko Tanaka-Ishii, K ˆoiti Hasida, and Itsuki Noda.\n1998. Reactive content selection in the generation\nof real-time soccer commentary. In Proceedings of\nthe 17th international conference on Computational\nlinguistics-Volume 2, pages 1282–1288. Association\nfor Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nSam Wiseman, Stuart M Shieber, and Alexander M\nRush. 2017. Challenges in data-to-document gen-\neration. arXiv preprint arXiv:1707.08052.\nZichao Yang, Phil Blunsom, Chris Dyer, and Wang\nLing. 2016. Reference-aware language models.\narXiv preprint arXiv:1611.01628.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8126901388168335
    },
    {
      "name": "Transformer",
      "score": 0.7844735383987427
    },
    {
      "name": "Correctness",
      "score": 0.7362563610076904
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5377703905105591
    },
    {
      "name": "Text generation",
      "score": 0.48941078782081604
    },
    {
      "name": "Machine learning",
      "score": 0.4434874355792999
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.42157623171806335
    },
    {
      "name": "Model selection",
      "score": 0.41504818201065063
    },
    {
      "name": "Natural language processing",
      "score": 0.4114055335521698
    },
    {
      "name": "Data mining",
      "score": 0.37620094418525696
    },
    {
      "name": "Voltage",
      "score": 0.13061600923538208
    },
    {
      "name": "Algorithm",
      "score": 0.1304911971092224
    },
    {
      "name": "Engineering",
      "score": 0.07027363777160645
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210127725",
      "name": "SYSTRA (France)",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210096917",
      "name": "Systran (France)",
      "country": "FR"
    }
  ],
  "cited_by": 35
}