{
  "title": "Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering",
  "url": "https://openalex.org/W4389524594",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5114245479",
      "name": "Inderjeet Nair",
      "affiliations": [
        "University of Michigan"
      ]
    },
    {
      "id": "https://openalex.org/A5108334266",
      "name": "Shwetha Somasundaram",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5103163885",
      "name": "Apoorv Saxena",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5083954626",
      "name": "Koustava Goswami",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2072222992",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W4252372656",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W4330337479",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3210277894",
    "https://openalex.org/W3100436891",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2070076837",
    "https://openalex.org/W3034364750",
    "https://openalex.org/W3167303745",
    "https://openalex.org/W3100107515",
    "https://openalex.org/W2128463176",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2609826708",
    "https://openalex.org/W2021403037",
    "https://openalex.org/W4385572983",
    "https://openalex.org/W2949428332",
    "https://openalex.org/W3161820423",
    "https://openalex.org/W4389524599",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4221166196",
    "https://openalex.org/W3099524945",
    "https://openalex.org/W2181157858",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4322760070",
    "https://openalex.org/W4224438163",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2788448041",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3203321135",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2086511124",
    "https://openalex.org/W3035391452",
    "https://openalex.org/W4221150602",
    "https://openalex.org/W3118999024",
    "https://openalex.org/W4327644588",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4386566495",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3042185737",
    "https://openalex.org/W3136035550",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W4225080353",
    "https://openalex.org/W3034671305"
  ],
  "abstract": "We address the task of evidence retrieval for long document question answering, which involves locating relevant paragraphs within a document to answer a question. We aim to assess the applicability of large language models (LLMs) in the task of zero-shot long document evidence retrieval, owing to their unprecedented performance across various NLP tasks. However, currently the LLMs can consume limited context lengths as input, thus providing document chunks as inputs might overlook the global context while missing out on capturing the inter-segment dependencies. Moreover, directly feeding the large input sets can incur significant computational costs, particularly when processing the entire document (and potentially incurring monetary expenses with enterprise APIs like OpenAI’s GPT variants). To address these challenges, we propose a suite of techniques that exploit the discourse structure commonly found in documents. By utilizing this structure, we create a condensed representation of the document, enabling a more comprehensive understanding and analysis of relationships between different parts. We retain 99.6% of the best zero-shot approach’s performance, while processing only 26% of the total tokens used by the best approach in the information seeking evidence retrieval setup. We also show how our approach can be combined with *self-ask* reasoning agent to achieve best zero-shot performance in complex multi-hop question answering, just ≈ 4% short of zero-shot performance using gold evidence.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14593–14606\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nDrilling Down into the Discourse Structure with LLMs for Long Document\nQuestion Answering\nInderjeet Nair*1, Shwetha Somasundaram*2, Apoorv Saxena2, Koustava Goswami2\n1University of Michigan, Ann Arbor, MI\n2Adobe Research, India\ninair@umich.edu\n{shsomasu,apoorvs,koustavag}@adobe.com\nAbstract\nWe address the task of evidence retrieval for\nlong document question answering, which in-\nvolves locating relevant paragraphs within a\ndocument to answer a question. We aim to\nassess the applicability of large language mod-\nels (LLMs) in the task of zero-shot long docu-\nment evidence retrieval, owing to their unprece-\ndented performance across various NLP tasks.\nHowever, currently the LLMs can consume lim-\nited context lengths as input, thus providing\ndocument chunks as inputs might overlook the\nglobal context while missing out on capturing\nthe inter-segment dependencies. Moreover, di-\nrectly feeding the large input sets can incur sig-\nnificant computational costs, particularly when\nprocessing the entire document (and potentially\nincurring monetary expenses with enterprise\nAPIs like OpenAI’s GPT variants). To address\nthese challenges, we propose a suite of tech-\nniques that exploit the discourse structure com-\nmonly found in documents. By utilizing this\nstructure, we create a condensed representation\nof the document, enabling a more comprehen-\nsive understanding and analysis of relationships\nbetween different parts. We retain99.6% of the\nbest zero-shot approach’s performance, while\nprocessing only 26% of the total tokens used\nby the best approach in the information seeking\nevidence retrieval setup. We also show how our\napproach can be combined withself-ask reason-\ning agent to achieve best zero-shot performance\nin complex multi-hop question answering, just\n≈4% short of zero-shot performance using\ngold evidence.\n1 Introduction\nLong Document Question Answering (LDQA) is\na complex task that involves locating relevant evi-\ndence from lengthy documents to provide accurate\nanswers to specific questions (Dasigi et al., 2021).\nLDQA is challenging for the following reasons -\n* Equal contribution\n1 Work done at Adobe Research, India\na) Long documents often exceed the maximum to-\nken limit of existing transformer-based Pretrained\nLanguage Models (PLMs) (Devlin et al., 2019; Liu\net al., 2019; Lewis et al., 2020; Raffel et al., 2020),\nposing a challenge in directly processing their con-\ntent to extract pertinent information (Dong et al.,\n2023). b) The information required to answer a\nquestion is often dispersed across different sections\nor paragraphs within the document which may re-\nquire sophisticated reasoning process to identify\nand extract the relevant information (Nie et al.,\n2022). c) Processing the entire document to find\nanswers can be computationally expensive and in-\nefficient (Dong et al., 2023).\nOne popular approach for LDQA is the retrieve-\nthen-read method (Zheng et al., 2020; Gong et al.,\n2020; Nie et al., 2022; Ainslie et al., 2020, 2023),\nwhere relevant paragraphs are retrieved from the\ndocument to provide the answer. A major draw-\nback of existing works is reliance on supervised\nfine-tuning for the evidence selection phase, ex-\nhibiting poor generalization on out-of-distribution\ndata (Thakur et al., 2021).\nGiven the remarkable few-shot/zero-shot perfor-\nmance and enhanced generalization capabilities\ndemonstrated by Large Language Models (LLMs)\nacross various Natural Language Generation and\nUnderstanding tasks (Brown et al., 2020; Chen\net al., 2021; Rae et al., 2022; Hoffmann et al., 2022;\nChowdhery et al., 2022), we investigate the poten-\ntial of leveraging these LLMs for zero-shot evi-\ndence retrieval. Notably, LLMs that have been in-\nstruction fine-tuned (Wei et al., 2022a; Chung et al.,\n2022) or trained using Reinforcement Learning\nwith Human Feedback (Bai et al., 2022; Ouyang\net al., 2022) exhibit exceptional generalization per-\nformance even on unseen tasks (Ouyang et al.,\n2022; Min et al., 2022; OpenAI, 2023). Thus, we\nexplore the feasibility of utilizing LLMs for zero-\nshot evidence retrieval. However, LLMs, which\nare based on transformer architecture (Vaswani\n14593\net al., 2017), are limited by their context length\nand suffer from expensive inference times that in-\ncrease quadratically with the number of tokens\nin the input. Additionally, utilizing enterprise\nLLM solutions such as OpenAI’s gpt-3.5-turbo,\ntext-davinci-003, gpt-4, etc.1 to process an en-\ntire long document without optimizations would in-\ncur significant monetary costs. This highlights the\nneed for an LLM-based evidence retrieval solution\nthat can achieve faster and more cost-effective infer-\nence by selectively processing relevant portions of\nthe document, without compromising downstream\nperformance.\nTo overcome these challenges, we harness the\ninherent discourse structure commonly present in\nlong documents. This structure encompasses the or-\nganization of topics, semantic segments, and infor-\nmation flow, enabling effective information search\nand knowledge acquisition for question answer-\ning. (Guthrie et al., 1991; Meyer et al., 1980; Taylor\nand Beach, 1984; Cao and Wang, 2022; Dong et al.,\n2023; Nair et al., 2023). Utilizing this valuable\nstructure, we construct a condensed representation\nof the document by replacing the content within\neach section with a corresponding summary. This\ncondensed representation is then fed to the LLM,\nenabling efficient processing of tokens while al-\nlowing the model to comprehensively analyze the\nentire input context for identifying relevant sec-\ntions. Thereafter, the content within each relevant\nsection is further processed by the LLM for fine-\ngrained evidence retrieval. We call our proposed\napproach D3 (Drilling Down into the Discourse)\ndue to the nature of the solution described above.\nOur approach undergoes evaluation in two dis-\ntinct settings: Information Seeking and Multi-hop\nReasoning in Question Answering. In the infor-\nmation seeking experiments, our approach retains\nthe best zero-shot state-of-the-art (SoTA) results,\nwhile only utilizing 26% of the tokens employed\nby the SoTA approach. Additionally, we exam-\nine the robustness of our model across various\ndocument lengths and analyze the number of to-\nkens required and latency for different zero-shot\napproaches. Moreover, we explore the integration\nof our approach with other zero-shot techniques\nwithin an agent framework designed to break down\nintricate queries into a sequence of simpler follow-\nup queries.\n1https://openai.com/pricing\n2 Related Work\n2.1 LLMs in Retrieve-Then-Read Approaches\nThe retrieve-then-read (Green Jr et al., 1961; Chen\net al., 2017; Wang et al., 2018; Das et al., 2019; Guu\net al., 2020) approach is a widely adopted technique\nin open-domain (V oorhees et al., 1999; Dunn et al.,\n2017; Joshi et al., 2017; Zhu et al., 2021), multi-\ndocument question answering (Yang et al., 2018;\nPerez et al., 2020; Ferguson et al., 2020) and long-\ndocument question answering (Pereira et al., 2023).\nIn this approach, LLMs are utilized specifically for\nthe reader component, which generates responses\nbased on the relevant fragments retrieved by the re-\ntriever (Pereira et al., 2023). Although LLMs have\nbeen utilized as decision-making agents in browser\ninteractions for document retrieval (Nakano et al.,\n2022), their direct application for fine-grained evi-\ndence retrieval has not been extensively explored to\nthe best of our knowledge. On that front, our paper\nis the first to evaluate the applicability of LLMs for\nevidence retrieval.\n2.2 Chaining LLMs Runs for Question\nAnswering\nChaining in LLMs refers to the task of breaking\ncomplex overarching task into a sequence of fine-\ngrained targetted sub-tasks where the information\ngenerated by a particular run of the sequence is\npassed to the subsequent runs (Wu et al., 2022a,b).\nThis allows for realizing powerful machine learn-\ning applications without requiring any changes to\nthe model architecture (Tan et al., 2021; Betz et al.,\n2021; Reynolds and McDonell, 2021). LangChain\nhas implemented procedures using chaining for ev-\nidence retrieval and question answering in long\ndocuments. They employ three chaining variants\n(map-reduce, map-rerank, and refine) 2, which\nprocesses document chunks individually and ag-\ngregate the information from each chunk to derive\nthe final answer. This implementation, however,\nprocesses the entire document input resulting in\nsignificant compute and monetary cost.\n2.3 Evidence Retrieval for LDQA\nPrior evidence retrieval approaches typically em-\nploy following two mechanims which are trained\nby supervised fine-tuning - local processing to han-\ndle individual document chunks with occasional\ninformation flow between them (Gong et al., 2020)\n2https://python.langchain.com/docs/modules/\nchains/document/\n14594\nRelevant SectionsQuestion: What are the baselines outperformed by this work?\nSTEP 1Finding relevant sections using the condensed discourse-structure aware representation (§3.3.1) of  the document\nExperiments, Results and Analysis\nDocument section structure:* Introduction: The study introduces a hierarchical intent annotation scheme ...* Related Work: The study introduces the AntiScamdataset, designed ... ...Question:\"What are the baselines outperformed by this work?”List all section names that may be relevant for answering the question. Respond with comma-separated section name list. Provide an empty response if none of the sections are relevant.\nSTEP 2 Selecting relevant paragraphs from all the paragraphs in relevant sections (from STEP 1)\n[26,27,28,41]\nSTEP 3Question answering using relevant paragraphs (from STEP 2)\n0: The interest in non-collaborative tasks ..1: To better understand user utterances ...2: Traditional task-oriented dialog systems ... Question:\"What are the baselines outperformed by this work?”Find paragraph ids that contains relevantinformation for answering the question. Respond with comma-separated id list. Provide an empty response if none of the paragraphs are relevant.\nUsing the “Text\" as the context, provide a very short answer to the text following \"Question\". Answer only \"Unanswerable\" when not enough information is provided in the documents. If the question is boolean, respond only with \"yes\" or \"no”Text: We observe that MISSA outperforms two baseline models (TransferTransfo and hybrid model) ... Question: \"What are the baselines outperformed by this work?”Answer: \nTransferTransfo and Hybrid Relevant Paragraph IDsAnswer\nA well-structured long document with sections, subsections , etc. \nFigure 1: An illustration of the end-end pipeline of D3. Given a question and a long document with discourse\nstructure that indicates sections, subsections etc., we first identify sections that are relevant for answering the\nquestion. Following this step we select relevant paragraphs from the paragraphs in the relevant sections. In the final\nstep, we pass these relevant paragraphs to an LLM for question answering.\nand global processing to aggregate the informa-\ntion from each chunk to identify relevant para-\ngraphs (Zheng et al., 2020; Ainslie et al., 2020;\nNie et al., 2022; Ainslie et al., 2023). Inspired\nby this strategy, our method represent each section\nusing its corresponding summary in a local process-\ning step and, in the global processing mechanism,\nwe utilize a suitable verbalizer to concatenate the\nsummaries from each section.\n3 D3: Drilling Down into the Discourse\n3.1 Problem Formulation\nIn LDQA, a question q is asked for a document\nD = [p1, p2, . . . , pn], where pi(1 ≤ i ≤ n) is\nthe ith paragraph in the natural reading order of\nD. The task of LDQA is to retrieve a set of rele-\nvant paragraphs Eq ⊆D and generate a free-form\nanswer a based on q and D (Dasigi et al., 2021;\nNie et al., 2022). Due to the length of the docu-\nments, often exceeding 5K tokens, we employ the\nretrieve-then-read strategy. This approach involves\nfirst determining Eq and subsequently generating\na using only q and Eq.\n3.2 Motivation\nThe cognitive strategy employed by humans to\nsearch for relevant information from a document\nentails a systematic approach of first categorizing\nthe information within the document to determine\nrelevant coarse segments and then conducting a\ndeeper analysis of the relevant categories to extract\nfine-grained segments (Guthrie and Kirsch, 1987;\nGuthrie et al., 1991; Guthrie and Mosenthal, 1987).\nLong documents often possess a well-structured\ndiscourse that categorizes information coherently\nbased on topical similarity (Cao and Wang, 2022;\nNair et al., 2023; Dong et al., 2023). This inherent\ndiscourse structure serves as a valuable framework,\nenabling effective categorization of information\nand facilitating a clear and logical flow of ideas\nwithin the document.\nDrawing from these insights, we posit that en-\ncapsulating the essence of a section through its\nname and content summary would yield valuable\ncues in determining its relevance for answering spe-\ncific questions. Thereafter, we emulate the above-\ndescribed cognitive process by fine-grained anal-\nysis of relevant sections to extract evidence para-\ngraphs. This methodology offers three key advan-\ntages:\n(1) By condensing each section with its name and\nsummary, we can effectively reduce the document’s\ntoken count, enabling LLMs to analyze the entire\ncontext and make accurate inferences.\n(2) By efficiently filtering out irrelevant sections in\nthe initial stage, our method reduces the number of\ntokens processed.\n(3) Our method is applicable for any instruction-\nfollowing LLMs (Ouyang et al., 2022; Min et al.,\n2022; OpenAI, 2023), enabling zero-shot applica-\ntion without the need for architectural modifica-\ntions.\n3.3 Methodology\nInstead of representing a document D as a or-\ndered set of constituent paragraphs, we repre-\n14595\nsent D = [S1, S2, . . . , Sk]3, where Si(1 ≤i ≤\nk) denotes ith section, such that, name(Si) and\nparagraphs(Si) denotes its name / heading and\nthe list of constituent paragraphs respectively\n(paragraphs(Si) = [pi,j]|Si|\nj=1 where |Si|denotes\nnumber of constituent paragraphs). Note that,∑k\ni=1 |Si|= n. Inspired by the cognitive process\nof knowledge acquisition / information search for\nquestion answering, our approach first finds the\nrelevant sections that may answer the question and\nthen, analyses the paragraphs from the relevant sec-\ntions for fine-grained evidence paragraph retrieval.\n(Figure 1)\n3.3.1 Finding Relevant Sections\nThe crux of this step is to represent the con-\ntent in each section Si by the summary of\nparagraphs(Si). Summarization (El-Kassas et al.,\n2021) refers to the task of generating a concise\nsummary for a given input that captures its main\nidea within a limited number of tokens, effectively\nconveying its topical essence. We denote the sum-\nmarization operation by S, for which we have used\nbart-large (Lewis et al., 2020) fine-tuned over\nCNN/Daily-Mail Corpus (Nallapati et al., 2016).\nThereafter, we represent the entire document as\nfollows:\n* Section: name(S1)\nS(paragraphs(S1))\n* Section: name(S2)\nS(paragraphs(S2))\n···\n* Section: name(Sk)\nS(paragraphs(Sk))\nAn instruction is passed to an LLM involving the\nabove representation to identify all the sections that\nare relevant to q. Due to this condensed represen-\ntation, the LLM can process the entire document\ncontext enabling comprehensive analysis of long\nrange dependencies for accurate inference. Let the\nset of sections identified as relevant be denoted by\nRq ⊆D.\n3.3.2 Fine-Grained Evidence Retrieval\nThe objective of this step is to infer the set of rele-\nvant paragraphs from Rq. Here, we explain mul-\n3In practice, documents often have a hierarchical discourse\nstructure, consisting of multiple levels of sections (Nair et al.,\n2023). To handle this, we can flatten the structure using a pre-\norder traversal approach. When verbalizing a specific section,\nwe concatenate the names of all sections along the path from\nthe root node to that particular node in the discourse structure.\nThis flattening process allows us to represent the document as a\nlist of sections while considering the hierarchical relationships\namong sections.\ntiple zero-shot strategies to achieve this step. We,\nfirst, obtain a set of all paragraphs Pq associated\nwith Rq.\nPq =\n⋃\nS∈Rq\nparagraphs(S)\nThereafter, one of the following strategy can be\nemployed for fine-grained retrieval:\n1. MONO T5: This employs MonoT5 (Nogueira\net al., 2020), which a sequence-to-sequence\nmodel trained over the task of Document Re-\nranking (Nguyen et al., 2016), to select the\nmost relevant paragraphs from Pq.\n2. BASE : Each paragraph from Pq is marked\nwith an identifier and then, these identifier\nannotated paragraphs are concatenated with\na newline separator. Thereafter, we prompt\nthe LLM in a zero-shot manner to generate\nall paragraph identifiers whose corresponding\nparagraph is relevant to q. If the number of\nparagraphs in Pq exceeds the maximum con-\ntext length of LLM, we make multiple LLM\ncalls. In each call, we fit the maximum num-\nber of paragraphs that can fit into the con-\ntext length, ensuring that paragraphs are not\n‘chopped’.\n3. HIER BASE : In our approach, we adopt a two-\nstep process to capture the essence of each\nparagraph. Firstly, we represent paragraphs\nusing their corresponding summaries obtained\nthrough S. Following that, we employ the\nBASE strategy to identify potentially relevant\ncandidates. In the next stage, we apply the\nBASE technique once again, this time consid-\nering the original content of the paragraphs,\nto pinpoint the most relevant ones.\nIn our experimental evaluation, we also ex-\nplore the effectiveness of chaining these strategies\nin succession. One such method, called BASE\n+ M ONO T5, combines the BASE strategy with\nMONO T5. This approach initially identifies a rel-\nevant set of paragraphs using the BASE strategy\nand subsequently employs MONO T5 to refine the\nselection further, retaining only the most relevant\nones from the initially identified set.\nFor most of the experiments presented in the\nupcoming sections, we use bart-large (Lewis\net al., 2020) trained over the CNN/Daily-Mail Cor-\npus (Nallapati et al., 2016) for S. For retrieval\n14596\nApproach Answering Performance Evidence Tokens API\nExtractive Abstractive Yes/No Unanswerable Overall F1 Processed Calls\nHUMAN (Dasigi et al., 2021) 58.92 39.71 78.98 69.44 60.92 71.62 - -\nCGSN (Nie et al., 2022) 34.75 14.39 68.14 71.84 39.44 53.98 - -\nLED* (Nie et al., 2022) 52.41 23.44 76.96 77.91 52.87 - - -\ngpt-3.5-turbo* 54.86 27.74 81.50 95.76 57.99 - - -\nZERO -SHOT / UNSUPERVISED METHODS\nMONO T5 (Nogueira et al., 2020) 42.84 25.84 82.23 69.09 47.21 34.23 - -\nDPR (Karpukhin et al., 2020) 31.58 18.57 78.46 84.33 42.11 19.32 - -\ncross-encoder-ms-marco-MiniLM-L-12-v2 38.69 23.25 78.04 71.42 43.48 30.76 - -\nPARAGRAPH 45.20 26.02 76.13 72.56 47.92 32.02 8519.37 47.24\nCHUNK 45.96 29.61 84.30 65.57 49.00 35.59 5411.44 2.44\nMAP-REDUCE 21.37 19.65 76.47 90.28 39.26 12.84 12730.13 48.24\nMAP-REDUCE OPTIMIZED 47.45 26.05 82.79 71.32 50.13 50.11 7491.97 3.69\nD3-BASE 42.90 23.65 74.35 79.61 47.45 49.92 1980.94 1.99\nTable 1: Comparison of various zero-shot approaches against SoTA methods for QASPER dataset. The\nsimplest algorithm from D3 family yields competitive value across several metrics while being zero-shot and\nrequiring least number of tokens. *: Inference obtained using gold evidence.\nand question answering, we utilize the highly ca-\npable gpt-3.5-turbo model, known for its re-\nmarkable performance across a wide range of NLP\ntasks, all while being more cost-effective (Ye et al.,\n2023) when compared against text-davinci-003.\nTo identify the most relevant sections§3.3.1, we\nprompt the LLM with the following instruction:\nDocument section structure:\n{Condensed representation described in §3.3.1}\nQuestion:\n{q}\nList all section names that may be relevant for\nanswering the question. Respond with\ncomma-separated section name list. Provide an\nempty response if none of the sections are\nrelevant.\nFor the BASE strategy described in §3.3.2, we em-\nploy the following prompt:\n{Paragraphs annotated with identifier (§3.3.2)}\nQuestion:\n{q}\nFind paragraph ids that contains relevant\ninformation for answering the question. Respond\nwith comma-separated id list. Provide an empty\nresponse if none of the paragraphs are relevant.\n4 Baselines:\nWe consider the following zero-shot approaches\nfor performance comparison:\n(1) MONO T5: In this approach, MONO T5 is\ndirectly applied to re-rank the paragraphs of\nD = [p1, p2, . . . , pn] based on q.\n(2) DPR : Dense Passage Retrieval\n(DPR) (Karpukhin et al., 2020) is a retrieval\napproach that leverages dense representations.\nThis method utilizes a bi-encoder architecture to\ngenerate embeddings for both the documents and\nthe query independently which are used for finding\nthe most relevant documents.\n(3) cross-encoder-ms-marco-MiniLM-L-12-v2:\nThis model is a cross-encoder reranker which\nemploys Siamese learning and BERT-like\narchitecture. This model is offered in\nthe sentence-transformers (Reimers and\nGurevych, 2019) library. While the library pro-\nvides many different model checkpoints, we chose\ncross-encoder-ms-marco-MiniLM-L-12-v2 as\nit yielded highest F1 score for evidence retrieval.\n(4) PARAGRAPH : Every paragraph in D is\nprocessed by the LLM independently to assess its\nrelevance to q through a boolean prompt.\n(5) CHUNK : The document is partitioned into\nconsecutive fragments, aiming to accommodate as\nmany paragraphs as possible within a predefined\ntoken limit called as chunk size ( 3500). Subse-\nquently,BASE is applied for each fragment.\n(6) MAP-REDUCE : This approach, widely\nadopted in the literature, involves evaluating the\nrelevance of each paragraph to q and subsequently\nprocessing the relevant paragraphs together in a\nsingle call to the LLM. However, we observed\nthat using LangChain’s implementation directly\nled to subpar performance in terms of evidence\nretrieval F1-score and inference cost. This can\nbe attributed to the incompatibility of the prompt\nwith the target domain, resulting in significant\nperformance degradation due to gpt-3.5-turbo’s\nhigh sensitivity to the prompt (Ye et al., 2023).\n(7) MAP-REDUCE OPTIMIZED : For better\nalignment with our target task, we made crucial\nmodifications to the original implementation.\nBuilding upon the observation that CHUNK\noutperforms PARAGRAPH in terms of performance,\nwe decided to process document chunks instead\nof individual paragraphs using the BASE tech-\n14597\nnique. Following the initial stage, where relevant\nparagraphs are identified, we concatenate them\nand subject them to the same strategy (BASE ) for\nfurther processing.\n5 Experiments and Results\nWe mainly assess the applicability of our method\nin two scenarios: (1) §5.1: Information-seeking\nsetting (Dasigi et al., 2021) and (2) §5.1: Multi-\nhop Reasoning in Question Answering (Yang et al.,\n2018). Thereafter, we conduct an extensive analy-\nsis of our approach’s performance across various\nconfigurations, as discussed in §5.3.1, and exam-\nine its effectiveness in different document length\ncategories, as outlined in §5.3.2. Thereafter, we\njustify the need for the inclusion of global con-\ntext modeling in §5.3.3, and highlight the signif-\nicance of incorporating discourse information in\n§5.3.4. Finally, §5.3.5 we compare our methods\nperformance with the best performing zero-shot ap-\nproach for different categories and identify scope\nfor improvement. In our analyses, we will also\ninclude insights into the inference cost and latency\nassociated with utilizing large language models\n(LLMs) for the evidence retrieval stage. This will\nencompass the monetary cost and the processing ef-\nficiency measured in terms of the number of tokens\nprocessed and the number of LLM inferences. It is\nimportant to note that these measurements pertain\nexclusively to LLMs and do not encompass smaller\nfine-tuned models. Unless otherwise specified, we\nwould be using BASE approach for fine-grained\nretrieval and the overall approach would be called\nD3-BASE . Due to the monetary cost associated\nwith gpt-based LLMs, we experiment with 150\nrandomly sampled documents for all the experi-\nments.\n5.1 Performance for Information-Seeking\nSetting\nWe assess the performance of the models on the\nQASPER dataset (Dasigi et al., 2021), which com-\nprises information-seeking questions designed for\nlengthy research papers. The dataset includes a set\nof ground truth evidence paragraphs and answers.\nThe questions are categorized as extractive, abstrac-\ntive, yes/no, and unanswerable, and our proposed\nmethod must accurately discern the question’s in-\ntent to generate a relevant response.\nTable 1 presents the results of various ap-\nproaches, including a fine-tuned state-of-the-art\nS1: Understanding text and voice\nquestions from users...\nS2: BIBREF0 introduced the task of\nidentifying well-formed...\nS3: Our dataset enables us to...\nUnanswerable. The text does not\nmention a specific baseline method\nRetrieval\nQA\nTokens used: 563\nS1: To evaluate model\nperformance, we apply our trained\nmodels to...\nThe baseline method is evaluating\nthe original ill-formed question using\nthe automatic metrics.\nRetrieval\nQA\nTokens used: 1896\nS1: To evaluate model\nperformance, we apply our trained\nmodels to...\nThe baseline method is evaluating\nthe original ill-formed question using\nthe automatic metrics.\nRetrieval\nQA\nTokens used: 7561\nWhat is the baseline method?\nTotal OpenAI calls: 1 Total OpenAI calls: 3 Total OpenAI calls: 5\n(A) Embedding-based retriever (MonoT5) (B) D3 (Ours) (C) Langchain Map-Reduce\nFigure 2: Qualitative comparison of D3-BASE with\npublicly available Zero-shot Approaches such as\nMONO T5 and Langchain’s MAP-REDUCE : \"Tokens\nused\" refers to the total number of tokens processed\nby the evidence retrieval and question answering stage\nto generate the final answer. Similarly \"Total OpenAI\nCalls\" also computes the number of API calls over both\nthe tasks.\n(SoTA) model (Nie et al., 2022), fine-tuned LED\nmodel evaluated on gold evidence, and zero-shot\ngpt-3.5-turbo model evaluated on gold evidence.\nAmong them, our simplest approach, D3-BASE ,\nachieves competitive performance in terms of Ev-\nidence F1 score. Notably, it retains 99.6% of the\nperformance of the best zero-shot approach, MAP-\nREDUCE OPTIMIZED , while processing only 26%\nof the tokens required by the latter.\nThe original implementation of MAP-REDUCE\nsuffers from two main limitations. Firstly, it pro-\ncesses each paragraph independently, overlooking\nthe effectiveness of chunk-level processing over\nparagraph-level processing. Secondly, it employs\nsuboptimal few-shot prompting to retrieve relevant\nsources, resulting in increased processing costs and\npoor performance when the few-shot prompting\nis not well-aligned with the domain. Due to these\nsignificant processing costs and the underper-\nformance of PARAGRAPH and MAP-REDUCE ,\nwe exclude their evaluations from subsequent\nanalyses. Similarly, among the non-LLM base-\nlines, we exclude the evaluations of DPR and\ncross-encoder-ms-marco-MiniLM-L-12-v2\ndue to their poor performance.\nWhile it is possible to enhance the performance\nof our approach, D3, with alternative configura-\ntions (as shown in Section 5.3.1), it remains an\nexcellent choice for rapid testing in different do-\nmains. Its cost-effectiveness and minimal latency\nin terms of API calls make it highly suitable for\nsuch purposes.\n14598\nApproach Evidence Answer Tokens API\nF1 F1 Processed Calls\nCGSN (Nie et al., 2022) 92.02 57.80 - -\nLED* (Nie et al., 2022) - 58.94\ngpt-3.5-turbo* - 47.01 - -\nZERO -SHOT APPROACHES\nMONO T5 62.33 37.91 - -\nCHUNK 40.79 36.81 6702.78 2.45\nMAP-REDUCE OPTIMIZED 39.52 37.93 8329.81 3.58\nD3-BASE 31.05 23.55 3141.29 2.10\nZERO -SHOT APPROACHES AUGMENTED WITH SELF -ASK\nMONO T5 33.56 40.36 719.54 1.62\nCHUNK 20.66 39.59 9822.49 5.0\nMAP-REDUCE OPTIMIZED 30.19 38.32 12253.42 6.83\nD3-BASE 26.87 43.45 5376.29 5.17\nTable 2: Comparison of various zero-shot approaches\nfor HOTPOTQA-Doc Dataset. While directly apply-\ning D3-BASE leads to poor performance, combining\nthis with self-ask prompting methodology yields best\nperformance while processing least number of tokens\nwhen compared against other zero-shot LLM-based\nmethods. *: Inference obtained using gold evidence.\n5.2 Performance for Questions Requiring\nMulti-Hop Reasoning\nHere, we use HOTPOTQA-Doc dataset (Yang et al.,\n2018; Nie et al., 2022), where the objective is to\nanswer a complex query involving multi-hop rea-\nsoning given two long documents. We have in-\nvestigated the performance of different zero-shot\napproaches using two schemes: (a) Direct Pro-\ncessing: Queries are directly fed to the Zero-Shot\nretrievers to get the relevant evidences. (b) self-ask\nbased Processing: By leveraging the power of elic-\nitive prompting (Yao et al., 2022; Press et al., 2022;\nWei et al., 2022b), we employ the technique of\nself-ask (Press et al., 2022). This approach entails\ndecomposing a complex query into a series of sim-\npler questions, which collectively form the basis\nfor the final answer. Through iterative questioning,\nthe agent analyzes prior answers and previously\nposed questions to generate subsequent inquiries.\nLeveraging the zero-shot retrieval approach, the\nagent obtains relevant answers for each question.\nThe results for this experiment are tabulated at\nTable 2. We note the following observations:\n• Evidence F1 poorly correlated with Answer\nF1: Considering same question answering\nmodel were used (gpt-3.5-turbo) for each\nof the zero-shot approaches, we find that the\nanswer performance of MAP-REDUCE OPTI -\nMIZED aligns with that of MONO T5, albeit\nwith noticeably lower evidence retrieval effi-\ncacy. It’s worth noting that during dataset con-\nstruction, only the paragraphs utilized for gen-\nerating answers were preserved, which does\nnot imply the irrelevance of other paragraphs\nin addressing the question. This observation\nhighlights the presence of this artifact.\n• Augmenting with self-ask boosts perfor-\nmance: This highlights the fact that zero-shot\nretrievers are better positioned to retrieve frag-\nments for simpler queries and self-ask effec-\ntively uses them to get better performance. In\nfact, our approach D3 −BASE is very close in\nperformance to zero-shot question answering\nwith gold evidence.\n5.3 Ablations & Analyses\n5.3.1 Performance of different configurations\nof D3\nWe investigated several configurations to determine\npossible directions to improve the performance.\nFollowing explorations were performed (Table 3):\n• Variations in fine-grained retrieval : Al-\nthough employing MonoT5 can reduce infer-\nence costs, it also adversely affects evidence\nretrieval performance, as evidenced by the\ntable. Conversely, D3−HIER BASE demon-\nstrates a enhancement in performance with\nonly a marginal increase in inference cost.\n• Using LLMs for summarization : We re-\nplaced fine-tuned summarizer with enter-\nprise LLMs such as gpt-3.5-turbo4 and\ntext-davinci-003 (Ouyang et al., 2022) and\nan open-source LLM, vicuna-13b (Chiang\net al., 2023). While the performance increased\nalong several metrics, there is additional pro-\ncessing cost to get the condensed represen-\ntation of the document (very minimal when\nsmaller bart-large based summarizer was\nused).\n• Exploring alternative LLMs for retrieval:\nIn this analysis, we observe a decline in per-\nformance when utilizing other LLMs, high-\nlighting the superiority of gpt-3.5-turbo as\nthe optimal choice for retrieval tasks.\n• Investigating alternative LLMs for ques-\ntion answering: We observe a performance\nboost when employing text-davinci-003.\nHowever, it is important to consider the higher\nmonetary cost associated with using this API\ncompared to gpt-3.5-turbo.\n4https://openai.com/blog/chatgpt\n14599\nApproach Answering Performance Evidence Tokens API\nExtractive Abstractive Yes/No Unanswerable Overall F1 Processed Calls\nOUR PRIMARY APPROACH\nD3-BASE 42.90 23.65 74.35 79.61 47.45 49.92 1980.94 1.99\nVARIATIONS IN FINE -GRAINED EVIDENCE RETRIEVAL\nMONO T5 34.86 20.47 67.59 88.64 43.33 32.73 844.09 1.0\nMONO T5+B ASE 39.61 22.31 74.32 87.35 47.19 44.39 1520.35 1.95\nBASE +MONO T5 39.62 23.66 74.54 82.33 46.42 40.23 1980.94 1.99\nHIER BASE 45.48 24.14 71.55 86.18 49.48 50.09 2125.67 2.85\nREPLACING FINE -TUNED SUMMARIZATION MODEL WITH INSTRUCTION ALIGNED LLM\ngpt-3.5-turbo 43.89 27.31 79.81 75.55 49.28 51.19 2106.57 2.0\ntext-davinci-003 43.04 27.04 79.28 76.32 48.61 50.37 2239.97 2.03\nvicuna-13b 43.03 27.70 75.16 79.55 49.09 50.09 2208.63 2.14\nVARYING LLM S FOR RETRIEVAL\ntext-davinci-003 41.45 24.12 79.08 77.72 47.11 37.53 2673.26 2.0\nvicuna-13b 23.35 15.38 74.07 86.88 36.52 28.10 1810.48 1.85\nVARYING LLM S FOR QUESTION ANSWERING\ntext-davinci-003 49.99 20.33 77.86 90.82 52.51 49.92 1980.94 1.99\nvicuna-13b 31.71 21.34 62.29 60.43 37.06 49.92 1980.94 1.99\nTable 3: Performance for different D3 configurations for QASPER dataset.\n5.3.2 Performance across Different Document\nLength categories\nWe divided the test set into different categories\nbased on their respective lengths. We notice the\nadvantage of our method along three aspects:\n• Evidence F1: Our approach consistently\nachieves competitive performance in evidence\nretrieval across various document length cate-\ngories (Figure 3 (a)).\n• Evidence Retrieval Cost: Our approach sig-\nnificantly reduces the number of processed\ntokens compared to other methods in all doc-\nument length categories (Figure 3 (b)). This\ncost-efficient characteristic makes it an excel-\nlent choice for minimizing both inference and\nmonetary costs, regardless of the document’s\nlength .\n• Latency: Irrespective of the document’s\nlength, our approach maintains a minimal la-\ntency by making approximately 2 API calls to\nthe LLM (Figure 3 (c)). This efficient perfor-\nmance further highlights its desirability and\nsuitability for various applications.\n5.3.3 Need for Global Context Modeling\nIn this section, we investigate the effect of chang-\ning the chunk size of the baseline CHUNK in terms\nof evidence precision, recall and F1-score. As we\nsee from Figure 4, the precision and F1-score in-\ncreases at the cost of modest decrease in recall as\nthe chunk-size increasing. This observation under-\nscores the fact that larger chunk sizes enable the\nmodel to capture longer-range relationships and\ncontextual information, resulting in improved per-\nformance. By processing the entire document in a\nsingle pass, D3 benefits from accessing the global\ncontext and long-range relationships, leading to\nenhanced performance.\n5.3.4 Role of Discourse Headings and\nSub-headings\nApproach Evidence Tokens API\nF1 Processed Calls\nD3-BASE 49.92 1980.94 1.99\nD3-BASE × 43.11 2183.55 1.88\nTable 4: Performance of different methods with\nand without section information (denoted by ×) for\nQASPER dataset.\nTo assess the importance of section headings /\nsub-headings in the discourse structure for retrieval,\nwe replaced them with randomly initialized Univer-\nsally Unique Identifiers (UUIDs) and test evidence\nretrieval performance over the QASPER dataset\n(Table 4). The significant decrease in the perfor-\nmance shows that section headings / sub-headings\nare crucial in conveying the topical essence of a\nsection, which is needed for accurate retrieval.\n14600\n(a) Evidence F1 across different docu-\nment length categories\n(b) Tokens Processed for retrieval across\ndifferent document length categories\n(c) API calls for retrieval across different\ndocument length categories\nFigure 3: Analysing the performance of different approaches across different length categories along three\nmetrics for QASPER Dataset\nFigure 4: Evidence Precision, Recall and F1 over\ndifferent chunk lengths for QASPER Dataset\n5.3.5 Evidence Retrieval Performance over\nQuestion Categories\nIn this section, we compare D3-BASE with the best\nperforming zero-shot baseline MAP -REDUCE OPTI -\nMIZED (MRO) over different question categories\nin QASPER to identify differences in model perfor-\nmance (Table 5). While our approach is precise in\nidentifying the evidence paragraphs, it occasionally\nfalls short in identifying all pertinent evidence (i.e.\nlower recall). This indicates that representing a sec-\ntion with a summary leads to loss of information\nand the LLM may miscategorize its relevancy to a\nquestion due to this loss of information. Designing\nan effective strategy to prevent loss of vital infor-\nmation for a particular question may be a future\nresearch direction.\nQuestion Evidence Evidence Evidence\nCategory Precision Recall F1\n(D3 / MRO) ( D3 / MRO) ( D3 / MRO)\nExtractive 52.63 / 52.19 71.9 / 81.49 56.39 / 57.46\nAbstractive 41.25 / 45.1 52.51 / 71.57 41.78 / 50.12\nYes/No 43.6 / 38.79 59.6 / 55.31 45.53 / 40.18\nUnanswerable 42.94 / 25.74 44.4 / 25.46 43.43 / 25.10\nTable 5: Comparison of evidence retrieval perfor-\nmance of D3-BASE with the best performing zero-\nshot baseline MAP -REDUCE OPTIMIZED (MRO)\n6 Conclusion\nWe demonstrated a zero-shot approach for evidence\nretrieval which leverages the discourse structure of\nthe document for information categorization which\nnot only yielded competitive performance for infor-\nmation seeking and question answering with multi-\nhop reasoning setting, but also processed lowest\nnumber of tokens resulting in significant compute\nand cost savings. This approach demonstrates sev-\neral desirable characteristics such as robustness in\nevidence retrieval performance, lower latency, etc.\nall across different document length ranges.\n7 Limitations\n• Although our approach demonstrated compet-\nitive performance in the information seeking\nsetup, there is room for improvement, partic-\nularly when confronted with questions that\nrequire intricate multi-hop reasoning. Since\nwe represent the document by summarizing\neach section, there is a potential loss of criti-\ncal information that is essential for addressing\ncomplex queries necessitating multi-hop rea-\n14601\nsoning. Moving forward, we aim to explore\nmethods that allow for accurate section se-\nlection while minimizing inference costs and\nmitigating information loss.\n• Our experimental analyses primarily focus on\nenterprise-level language models, which re-\nquire enterprise compute credits. In the future,\nwe plan to explore the capabilities of more\nadvanced open-source models as they become\navailable, which may offer enhanced perfor-\nmance and accessibility.\n• While our experiments have primarily cen-\ntered around single-document use cases, we\nhave yet to delve into the realm of retrieval\ninvolving multiple documents or collections.\nThis area remains unexplored, and we antici-\npate investigating strategies and techniques to\neffectively handle such scenarios.\n• Although our evaluations provided insight into\nhow various summarizers impacted the final\ndownstream performance, the current study\ndid not inherently assess the quality of sum-\nmarization. In future work, we aim to assess\nthe summarization’s faithfulness to the origi-\nnal content and its impact on end-to-end per-\nformance.\nReferences\nJoshua Ainslie, Tao Lei, Michiel de Jong, Santiago\nOntañón, Siddhartha Brahma, Yury Zemlyanskiy,\nDavid Uthus, Mandy Guo, James Lee-Thorp, Yi Tay,\net al. 2023. Colt5: Faster long-range transform-\ners with conditional computation. arXiv preprint\narXiv:2303.09752.\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs in\ntransformers. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 268–284, Online. Association\nfor Computational Linguistics.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan,\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El-Showk, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\nNanda, Catherine Olsson, Dario Amodei, Tom\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\nBen Mann, and Jared Kaplan. 2022. Training a help-\nful and harmless assistant with reinforcement learn-\ning from human feedback.\nGregor Betz, Kyle Richardson, and Christian V oigt.\n2021. Thinking aloud: Dynamic context generation\nimproves zero-shot reasoning performance of gpt-2.\narXiv preprint arXiv:2103.13033.\nMichele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis,\nScott Yih, Sebastian Riedel, and Fabio Petroni. 2022.\nAutoregressive search engines: Generating substrings\nas document identifiers. Advances in Neural Infor-\nmation Processing Systems, 35:31668–31683.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nShuyang Cao and Lu Wang. 2022. HIBRIDS: Atten-\ntion with hierarchical biases for structure-aware long\ndocument summarization. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 786–807,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1870–1879,\nVancouver, Canada. Association for Computational\nLinguistics.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.\n2023. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality. See https://vicuna.\nlmsys. org (accessed 14 April 2023).\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\n14602\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nRajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,\nand Andrew McCallum. 2019. Multi-step retriever-\nreader interaction for scalable open-domain question\nanswering. In International Conference on Learning\nRepresentations.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\nNoah A. Smith, and Matt Gardner. 2021. A dataset\nof information-seeking questions and answers an-\nchored in research papers. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 4599–4610, On-\nline. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nZican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin\nZhao. 2023. A survey on long text modeling with\ntransformers. arXiv preprint arXiv:2302.14502.\nMatthew Dunn, Levent Sagun, Mike Higgins, V Ugur\nGuney, V olkan Cirik, and Kyunghyun Cho. 2017.\nSearchqa: A new q&a dataset augmented with\ncontext from a search engine. arXiv preprint\narXiv:1704.05179.\nWafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea,\nand Hoda K Mohamed. 2021. Automatic text sum-\nmarization: A comprehensive survey. Expert systems\nwith applications, 165:113679.\nJames Ferguson, Matt Gardner, Hannaneh Hajishirzi,\nTushar Khot, and Pradeep Dasigi. 2020. IIRC: A\ndataset of incomplete information reading compre-\nhension questions. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1137–1147, Online. As-\nsociation for Computational Linguistics.\nHongyu Gong, Yelong Shen, Dian Yu, Jianshu Chen,\nand Dong Yu. 2020. Recurrent chunking mecha-\nnisms for long-text machine reading comprehension.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 6751–\n6761, Online. Association for Computational Lin-\nguistics.\nBert F Green Jr, Alice K Wolf, Carol Chomsky, and\nKenneth Laughery. 1961. Baseball: an automatic\nquestion-answerer. In Papers presented at the May\n9-11, 1961, western joint IRE-AIEE-ACM computer\nconference, pages 219–224.\nJohn T Guthrie, Tracy Britten, and K Georgene Barker.\n1991. Roles of document structure, cognitive strategy,\nand awareness in searching for information. Reading\nResearch Quarterly, pages 300–324.\nJohn T Guthrie and Irwin S Kirsch. 1987. Distinctions\nbetween reading comprehension and locating infor-\nmation in text. Journal of educational psychology,\n79(3):220.\nJohn T Guthrie and Peter Mosenthal. 1987. Literacy\nas multidimensional: Locating information and read-\ning comprehension. Educational Psychologist, 22(3-\n4):279–297.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929–3938. PMLR.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601–1611, Vancouver,\nCanada. Association for Computational Linguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\n14603\nLangChain. Langchain.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nBonnie J. F. Meyer, David M. Brandt, and George J.\nBluth. 1980. Use of top-level structure in text: Key\nfor reading comprehension of ninth-grade students.\nReading Research Quarterly, 16(1):72–103.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022. MetaICL: Learning to learn\nin context. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2791–2809, Seattle, United States.\nAssociation for Computational Linguistics.\nInderjeet Nair, Aparna Garimella, Balaji Vasan Srini-\nvasan, Natwar Modani, Niyati Chhaya, Srikrishna\nKaranam, and Sumit Shekhar. 2023. A neural CRF-\nbased hierarchical approach for linear text segmen-\ntation. In Findings of the Association for Compu-\ntational Linguistics: EACL 2023 , pages 883–893,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\nChess, and John Schulman. 2022. Webgpt: Browser-\nassisted question-answering with human feedback.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar Gulçehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of the 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. Ms marco: A human generated machine read-\ning comprehension dataset. choice, 2640:660.\nYuxiang Nie, Heyan Huang, Wei Wei, and Xian-Ling\nMao. 2022. Capturing global structural information\nin long document question answering with compres-\nsive graph selector network. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing , pages 5036–5047, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nRodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and\nJimmy Lin. 2020. Document ranking with a pre-\ntrained sequence-to-sequence model. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 708–718, Online. Association\nfor Computational Linguistics.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nJayr Pereira, Robson Fidalgo, Roberto Lotufo, and Ro-\ndrigo Nogueira. 2023. Visconde: Multi-document\nqa with gpt-3 and neural reranking. In European\nConference on Information Retrieval, pages 534–543.\nSpringer.\nEthan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun\nCho, and Douwe Kiela. 2020. Unsupervised question\ndecomposition for question answering. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n8864–8880, Online. Association for Computational\nLinguistics.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv preprint arXiv:2210.03350.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Anto-\nnia Creswell, Nat McAleese, Amy Wu, Erich Elsen,\nSiddhant Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d’Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew Johnson,\nBlake Hechtman, Laura Weidinger, Iason Gabriel,\n14604\nWilliam Isaac, Ed Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\nray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling\nlanguage models: Methods, analysis insights from\ntraining gopher.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research, 21:1–\n67.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\ngramming for large language models: Beyond the\nfew-shot paradigm. In Extended Abstracts of the\n2021 CHI Conference on Human Factors in Com-\nputing Systems, CHI EA ’21, New York, NY , USA.\nAssociation for Computing Machinery.\nBowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric\nXing, and Zhiting Hu. 2021. Progressive generation\nof long text with pretrained language models. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n4313–4324, Online. Association for Computational\nLinguistics.\nYi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara\nBahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao,\nJai Gupta, et al. 2022. Transformer memory as a\ndifferentiable search index. Advances in Neural In-\nformation Processing Systems, 35:21831–21843.\nBarbara M. Taylor and Richard W. Beach. 1984. The ef-\nfects of text structure instruction on middle-grade stu-\ndents’ comprehension and production of expository\ntext. Reading Research Quarterly, 19(2):134–146.\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA heterogeneous benchmark for zero-shot evaluation\nof information retrieval models. In Thirty-fifth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 2).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nEllen M V oorhees et al. 1999. The trec-8 question\nanswering track report. In Trec, volume 99, pages\n77–82.\nShuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo\nWang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry\nTesauro, Bowen Zhou, and Jing Jiang. 2018. R 3:\nReinforced ranker-reader for open-domain question\nanswering. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 32.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022a. Finetuned language\nmodels are zero-shot learners. In International Con-\nference on Learning Representations.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022b. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nTongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff\nGray, Alejandra Molina, Michael Terry, and Carrie J\nCai. 2022a. Promptchainer: Chaining large language\nmodel prompts through visual programming. In Ex-\ntended Abstracts of the 2022 CHI Conference on\nHuman Factors in Computing Systems, CHI EA ’22,\nNew York, NY , USA. Association for Computing\nMachinery.\nTongshuang Wu, Michael Terry, and Carrie Jun Cai.\n2022b. Ai chains: Transparent and controllable\nhuman-ai interaction by chaining large language\nmodel prompts. In Proceedings of the 2022 CHI\nConference on Human Factors in Computing Sys-\ntems, CHI ’22, New York, NY , USA. Association for\nComputing Machinery.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Izhak Shafran,\nKarthik R Narasimhan, and Yuan Cao. 2022. React:\nSynergizing reasoning and acting in language models.\nIn NeurIPS 2022 Foundation Models for Decision\nMaking Workshop.\nJunjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao,\nShichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong,\nYang Shen, et al. 2023. A comprehensive capability\nanalysis of gpt-3 and gpt-3.5 series models. arXiv\npreprint arXiv:2303.10420.\nBo Zheng, Haoyang Wen, Yaobo Liang, Nan Duan,\nWanxiang Che, Daxin Jiang, Ming Zhou, and Ting\nLiu. 2020. Document modeling with graph attention\nnetworks for multi-grained machine reading compre-\nhension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6708–6718, Online. Association for Computa-\ntional Linguistics.\n14605\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming\nZheng, Soujanya Poria, and Tat-Seng Chua. 2021.\nRetrieving and reading: A comprehensive survey on\nopen-domain question answering. arXiv preprint\narXiv:2101.00774.\n14606",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.741791844367981
    },
    {
      "name": "Question answering",
      "score": 0.6636636257171631
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6398441791534424
    },
    {
      "name": "Exploit",
      "score": 0.6389375925064087
    },
    {
      "name": "Information retrieval",
      "score": 0.628655195236206
    },
    {
      "name": "Task (project management)",
      "score": 0.5491148233413696
    },
    {
      "name": "Natural language processing",
      "score": 0.4603689908981323
    },
    {
      "name": "Document retrieval",
      "score": 0.44785669445991516
    },
    {
      "name": "Representation (politics)",
      "score": 0.4412088394165039
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4352370500564575
    },
    {
      "name": "Computer security",
      "score": 0.10384264588356018
    },
    {
      "name": "Engineering",
      "score": 0.08664581179618835
    },
    {
      "name": "Geography",
      "score": 0.08501198887825012
    },
    {
      "name": "Political science",
      "score": 0.08197438716888428
    },
    {
      "name": "Politics",
      "score": 0.07066336274147034
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I27837315",
      "name": "University of Michigan",
      "country": "US"
    }
  ]
}