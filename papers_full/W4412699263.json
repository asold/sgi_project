{
  "title": "Performance of the ChatGPT-4o Language Model in Solving the Ophthalmology Specialization Exam",
  "url": "https://openalex.org/W4412699263",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5093346577",
      "name": "Barbara Sławińska",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2620234156",
      "name": "Dawid Jasiński",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105042211",
      "name": "Aleksander Jaworski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3006370255",
      "name": "Natalia Jasińska",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2027714849",
      "name": "Wojciech Jaworski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093880455",
      "name": "Oliwia Sysło",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5116889382",
      "name": "Nikola Rubik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2395875664",
      "name": "Izabela Jastrzebska",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5106253797",
      "name": "Konrad Haraziński",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5108177528",
      "name": "Weronika Goliat",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5116889383",
      "name": "Maksym Gmur",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1986793388",
      "name": "Michal Gajewski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5106253796",
      "name": "Zuzanna Błecha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5117285409",
      "name": "Nicole Maryniak",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Ada Latkowska",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2576404523",
    "https://openalex.org/W4388237469",
    "https://openalex.org/W4402311284",
    "https://openalex.org/W4386874181",
    "https://openalex.org/W4401252816",
    "https://openalex.org/W4387597556",
    "https://openalex.org/W4409748472",
    "https://openalex.org/W4391983664",
    "https://openalex.org/W4400131816"
  ],
  "abstract": "Background Artificial intelligence (AI), particularly language models such as ChatGPT, is gaining importance in medical education and knowledge assessment. Previous studies have demonstrated the growing effectiveness of AI in solving medical exams, including the Final Medical Examination (LEK) and Polish State Specialization Exam (PES) in various specialties, raising questions about its usefulness as a tool to support specialist training processes. Objective The aim of this study was to assess the effectiveness of the latest ChatGPT-4o model in solving the PES in ophthalmology. The analysis focused on the accuracy of the answers and the model's declared confidence level to evaluate its potential educational usefulness. Methods The study was based on the official PES ophthalmology exam (Spring 2024), consisting of 120 multiple-choice questions. The ChatGPT-4o model was familiarized with the exam regulations and questions, which were input in Polish. The effectiveness of the answers was assessed based on the Medical Education Center (CEM) answer key, as well as the model's declared confidence level (on a scale of 1 to 5). The questions were divided into clinical and theoretical categories. Data were analyzed statistically using the chi-square test and the Mann-Whitney U test. Results The model provided 94 correct answers (78.3%), exceeding the passing threshold. No significant difference in effectiveness was observed between clinical and non-clinical questions (p = 0.709). The analysis of the confidence level revealed that correct answers were significantly more often provided with higher confidence (p < 0.001), suggesting that the model's self-assessment could be an indicator of answer accuracy. Conclusions ChatGPT-4o demonstrated high effectiveness in the PES ophthalmology exam, confirming the potential of AI in specialist education. The confidence level of answers could serve as a useful tool in assessing the reliability of responses. Despite promising results, expert supervision and further research in various medical fields are necessary before wider implementation of AI models in medical education.",
  "full_text": "Review began\n 07/17/2025 \nReview ended\n 07/27/2025 \nPublished\n 07/28/2025\n© Copyright \n2025\nSławińska et al. This is an open access\narticle distributed under the terms of the\nCreative Commons Attribution License CC-\nBY 4.0., which permits unrestricted use,\ndistribution, and reproduction in any\nmedium, provided the original author and\nsource are credited.\nDOI:\n 10.7759/cureus.88908\nPerformance of the ChatGPT-4o Language Model\nin Solving the Ophthalmology Specialization\nExam\nBarbara Sławińska \n, \nDawid Jasiński \n, \nAleksander Jaworski \n, \nNatalia Jasińska \n, \nWojciech Jaworski \n,\nOliwia Sysło \n, \nNikola Rubik \n, \nIzabela \nJastrzebska \n, \nKonrad Haraziński \n, \nWeronika Goliat \n,\nMaksym \nGmur \n, \nMichal Gajewski \n, \nZuzanna Błecha \n, \nNicole Maryniak \n, \nAda Latkowska \n1.\n Department of Medicine, Specialist Medical Center in Polanica-Zdrój Named After St. John Paul II, Polanica-Zdrój,\nPOL \n2.\n Department of Paediatric Cardiology, Saint John Paul II Upper Silesian Child Health Centre, Public Clinical\nHospital no.6 of the Medical University of Silesia in Katowice, Katowice, POL \n3.\n Department of Plastic Surgery,\nSpecialist Medical Center in Polanica-Zdrój Named After St. John Paul II, Polanica-Zdrój, POL \n4.\n Faculty of Cybernetics,\nMilitary University of Technology, Warsaw, POL \n5.\n Department of Internal Medicine, Independent Public Provincial\nIntegrated Hospital in Szczecin, Szczecin, POL \n6.\n Faculty of Medicine, Academy of Silesia, Katowice, POL \n7.\nDepartment of Medicine, Academy of Silesia, Katowice, POL \n8.\n Department of Internal Medicine, Międzyleski\nSpecialized Hospital in Warsaw, Warsaw, POL \n9.\n Department of Medicine, Medical University of Silesia in Katowice,\nKatowice, POL \n10.\n Department of Medicine, Provincial Hospital in Poznań, Poznań, POL \n11.\n Faculty of Medicine,\nMedical University of Silesia in Katowice, Katowice, POL \n12.\n Department of Medicine, Zagłębiowskie Centrum\nOnkologii Szpital Specjalistyczny im. Sz. Starkiewicza w Dąbrowie Góniczej, Dąbrowa Górnicza, POL \n13.\n Faculty of\nMedicine, Wroclaw Medical University, Wrocław, POL\nCorresponding author: \nAleksander Jaworski, \naxjaw7@gmail.com\nAbstract\nBackground\nArtificial intelligence (AI), particularly language models such as ChatGPT, is gaining importance in medical\neducation and knowledge assessment. Previous studies have demonstrated the growing effectiveness of AI\nin solving medical exams, including the Final Medical Examination (LEK) and Polish State Specialization\nExam (PES) in various specialties, raising questions about its usefulness as a tool to support specialist\ntraining processes.\nObjective\nThe aim of this study was to assess the effectiveness of the latest ChatGPT-4o model in solving the PES in\nophthalmology. The analysis focused on the accuracy of the answers and the model's declared confidence\nlevel to evaluate its potential educational usefulness.\nMethods\nThe study was based on the official PES ophthalmology exam (Spring 2024), consisting of 120 multiple-\nchoice questions. The ChatGPT-4o model was familiarized with the exam regulations and questions, which\nwere input in Polish. The effectiveness of the answers was assessed based on the Medical Education Center\n(CEM) answer key, as well as the model's declared confidence level (on a scale of 1 to 5). The questions were\ndivided into clinical and theoretical categories. Data were analyzed statistically using the chi-square test\nand the Mann-Whitney U test.\nResults\nThe model provided 94 correct answers (78.3%), exceeding the passing threshold. No significant difference\nin effectiveness was observed between clinical and non-clinical questions (p = 0.709). The analysis of the\nconfidence level revealed that correct answers were significantly more often provided with higher confidence\n(p < 0.001), suggesting that the model’s self-assessment could be an indicator of answer accuracy.\nConclusions\nChatGPT-4o demonstrated high effectiveness in the PES ophthalmology exam, confirming the potential of\nAI in specialist education. The confidence level of answers could serve as a useful tool in assessing the\nreliability of responses. Despite promising results, expert supervision and further research in various\nmedical fields are necessary before wider implementation of AI models in medical education.\nCategories:\n Other, Medical Education, Ophthalmology\nKeywords:\n ai, artificial intelligence in medicine, deep learning artificial intelligence, ophthalmology teaching,\nstandardized medical exam\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n10\n10\n11\n12\n13\n \nOpen Access Original Article\nHow to cite this article\nSławińska B, Jasiński D, Jaworski A, et al. (July 28, 2025) Performance of the ChatGPT-4o Language Model in Solving the Ophthalmology\nSpecialization Exam. Cureus 17(7): e88908. \nDOI 10.7759/cureus.88908\nIntroduction\nSince the release of OpenAI’s first generation of ChatGPT (OpenAI, San Francisco, CA), three years have\npassed. During this period, the company has released five major versions of this chatbot, with numerous\nsubversions and updates. Chronologically, they were GPT-3.5 (2022), GPT-4 (2023), GPT-4 with tools (2023),\nGPT-4 Turbo (2023), and the most recent version released in May 2024, GPT-4o. The letter \"o\" in the name\nstands for \"omni,\" suggesting the model's ability to work across multiple modalities, such as text, image,\nsound, and video. GPT-4o, in addition to processing text, images, speech, and sound in real-time, features\ngreater speed and lower cost, significantly improved context understanding in multimodal communication\ncompared to ChatGPT-4 Turbo, emotion recognition, image and chart interpretation, and voice\ncommunication capabilities. GPT-4o is available to users of both the free and Plus versions of ChatGPT\n(though the Plus version has access to full features and tools).\nThe artificial intelligence (AI) that underpins ChatGPT has become a transformative force in many sectors.\nAccording to a report from the Financial Times, OpenAI has reached 500 million active weekly users \n[1]\n.\nOne of the sectors where ChatGPT has found the widest application is healthcare \n[2]\n. AI models, including\nChatGPT, assist in disease diagnosis, managing medical computer systems, creating educational platforms\nfor medical professionals, and supporting healthcare worker training \n[3]\n. While most of these solutions are\nused in countries more developed than Poland, an increasing number of studies are investigating the\nusefulness of AI in broadly defined medicine. Jaworski et al. showed that ChatGPT-4o, after being\nfamiliarized with the exam regulations, was able to successfully pass the medical and dental final exams \n[4]\n.\nThis study was conducted in May 2024. A previous study from 2023, using an older version of the chatbot,\nKufel et al. showed that ChatGPT was unable to pass the radiology Polish State Specialization Exam (PES)\n[5]\n.\nThis shows the pace of AI development and the OpenAI product. Trying to keep up with the development of\nAI is challenging and requires regular studies that systematically document its progress in real-time.\nThe aim of this study was to examine how the ChatGPT-4o language model performed on the PES\nophthalmology exam questions. Special attention was given to the accuracy of the responses and the\nsubjective confidence assessment provided by the model. The analysis was carried out based on the official\nanswer key published by the Medical Education Center (CEM) and the comparison of answers generated by\nChatGPT.\nMaterials And Methods\nThis study was conducted from 01.06.2025 to 07.06.2025, using the GPT-4o model. The subject of the study\nwas one ophthalmology specialization exam (Spring 2024), randomly selected from the available exams in\nthe archive database of the CEM in Łódź. The selected exam consisted of 120 questions with five distractors,\neach containing one correct answer. None of the 120 questions were invalidated by the Examination Board,\nmeaning all questions were consistent with the current state of knowledge.\nThe questions were divided into two categories: clinical cases and others. The \"clinical cases\" category\nincluded questions with patient scenario descriptions requiring interpretation of symptoms, test results, and\ndecision-making for diagnostic and therapeutic actions. The \"others\" category included questions assessing\ntheoretical knowledge, standards of treatment, or facts not related to a specific clinical case.\nThe classification was performed by two independent researchers, and any discrepancies were resolved by a\nthird independent researcher. Before presenting the exam questions to ChatGPT, the model was familiarized\nwith the exam regulations, including the number of questions, possible answers, and the number of correct\nanswers.\nThe responses were assessed according to the official answer key provided by CEM. All questions and\nanswers were documented. Additionally, after each question was input into the model, ChatGPT was asked:\n\"On a scale of one to five, how confident were you in your answer to this question?\" The aim of this question\nwas to estimate the confidence level of ChatGPT when providing an answer. ChatGPT could respond in the\nfollowing way: 1 - no confidence, 2 - low confidence, 3 - moderate confidence, 4 - high confidence, and 5 -\ncomplete confidence.\nAll questions were input into ChatGPT, and each interaction with the model was documented. To maintain\nconsistency with the content of the PES ophthalmology exam questions, all communication with ChatGPT\nwas conducted in Polish.\nFor statistical analysis, the STATISTICA program (StatSoft, Tulsa, OK) was used. The chi-square test was\napplied to compare the number of correct and incorrect answers between clinical and other questions. The\nMann-Whitney U test was used to assess the confidence level between correct and incorrect answers. p-\nvalues less than 0.05 were considered statistically significant.\n \n2025 Sławińska et al. Cureus 17(7): e88908. DOI 10.7759/cureus.88908\n2\n of \n9\nDue to the multi-center nature of the study, the authors collaborated using remote communication\nmethods, such as Microsoft Teams (Microsoft® Corp., Redmond, WA), Zoom (Zoom Communications, Inc.,\nSan Jose, CA), Facebook Messenger (Meta Platforms, Inc., Menlo Park, CA), emails, and Google Docs\n(Google, Inc., Mountain View, CA). All parts of the study prepared by individual teams were reviewed by\nother authors, allowing each researcher to contribute to every section of the paper using the aforementioned\nremote communication tools.\nResults\nThe ChatGPT-4o model provided 94 correct answers (78.3%) and 26 incorrect answers (21.7%) (Figure \n1\n and\nTable \n1\n). No significant differences in response effectiveness were observed between clinical and non-\nclinical questions (p = 0.709, \nχ\n² = 0.139) (Table \n2\n). The confidence level for correct answers was higher than\nfor incorrect ones, indicating the potential use of confidence levels as an indicator of response accuracy (U =\n93; p < 0.001).\nFIGURE\n 1: General summary of GPT-4o\nData are presented as N (%), where N represents the number of correct or incorrect responses, and (%) indicates\nthe percentage of all questions they represent.\nQuestion number\nOfficial answer\nChatGPT answer\nChatGPT confidence level (1-5)\n1\nD\nD\n5\n2\nA\nA\n5\n3\nB\nB\n4\n4\nA\nA\n5\n5\nC\nC\n5\n6\nD\nD\n4\n7\nB\nB\n5\n8\nC\nC\n4\n9\nB\nB\n5\n10\nE\nE\n5\n11\nE\nC\n5\n \n2025 Sławińska et al. Cureus 17(7): e88908. DOI 10.7759/cureus.88908\n3\n of \n9\n12\nD\nD\n5\n13\nC\nC\n3\n14\nB\nB\n3\n15\nE\nE\n5\n16\nD\nD\n5\n17\nA\nA\n5\n18\nD\nD\n3\n19\nC\nC\n4\n20\nD\nD\n5\n21\nD\nD\n5\n22\nC\nC\n5\n23\nA\nA\n5\n24\nB\nB\n5\n25\nB\nB\n5\n26\nA\nE\n5\n27\nE\nE\n5\n28\nB\nB\n5\n29\nA\nA\n4\n30\nE\nD\n5\n31\nD\nC\n3\n32\nD\nE\n3\n33\nB\nD\n3\n34\nC\nC\n4\n35\nB\nB\n5\n36\nE\nE\n3\n37\nE\nE\n5\n38\nE\nE\n5\n39\nE\nE\n5\n40\nE\nE\n3\n41\nE\nE\n4\n42\nE\nE\n5\n43\nA\nA\n5\n44\nB\nB\n3\n45\nC\nE\n5\n46\nC\nC\n4\n47\nD\nD\n5\n48\nD\nD\n4\n49\nC\nC\n5\n50\nD\nD\n3\n51\nB\nB\n5\n \n2025 Sławińska et al. Cureus 17(7): e88908. DOI 10.7759/cureus.88908\n4\n of \n9\n52\nE\nE\n5\n53\nC\nC\n5\n54\nE\nD\n5\n55\nE\nE\n5\n56\nE\nE\n5\n57\nA\nA\n5\n58\nE\nE\n5\n59\nE\nE\n4\n60\nA\nA\n5\n61\nC\nC\n5\n62\nA\nC\n3\n63\nC\nC\n5\n64\nA\nA\n5\n65\nE\nE\n5\n66\nA\nA\n5\n67\nB\nB\n5\n68\nD\nD\n3\n69\nD\nD\n4\n70\nE\nD\n4\n71\nC\nC\n5\n72\nC\nC\n4\n73\nA\nA\n5\n74\nB\nA\n3\n75\nE\nE\n5\n76\nA\nA\n5\n77\nA\nA\n5\n78\nB\nE\n5\n79\nD\nD\n5\n80\nE\nE\n5\n81\nB\nB\n5\n82\nC\nD\n5\n83\nD\nC\n5\n84\nB\nB\n3\n85\nA\nA\n4\n86\nE\nE\n5\n87\nB\nB\n4\n88\nD\nD\n4\n89\nC\nC\n5\n90\nB\nA\n4\n \n2025 Sławińska et al. Cureus 17(7): e88908. DOI 10.7759/cureus.88908\n5\n of \n9\n91\nE\nA\n5\n92\nB\nC\n5\n93\nB\nB\n3\n94\nB\nC\n5\n95\nC\nC\n4\n96\nC\nC\n5\n97\nD\nD\n5\n98\nC\nE\n3\n99\nD\nD\n5\n100\nD\nC\n5\n101\nE\nE\n5\n102\nE\nC\n5\n103\nC\nC\n3\n104\nB\nB\n4\n105\nA\nD\n5\n106\nE\nE\n3\n107\nB\nB\n4\n108\nE\nE\n5\n109\nA\nA\n5\n110\nC\nC\n5\n111\nB\nB\n5\n112\nA\nA\n5\n113\nE\nE\n5\n114\nD\nE\n5\n115\nC\nC\n5\n116\nA\nA\n5\n117\nB\nC\n5\n118\nC\nE\n5\n119\nC\nC\n5\n120\nE\nE\n5\nTABLE\n 1: Comparison of responses given by the ChatGPT-4o model to PES ophthalmology exam\nquestions along with the declared confidence level\nThis table presents the model's responses in relation to the correct answers according to the official answer key published by the Medical Examination\nCenter (CEM) in Łódź. Additionally, for each question, the confidence level declared by the model is included, based on the question: \"On a scale of 1 to 5,\nhow confident were you in your answer?\" This scale includes 1 - no confidence, 2 - low confidence, 3 - moderate confidence, 4 - high confidence, and 5 -\ncomplete confidence.\n \n2025 Sławińska et al. Cureus 17(7): e88908. DOI 10.7759/cureus.88908\n6\n of \n9\nQuestion type\nCorrect answer N (%)\nIncorrect answer N (%)\np-value\nχ² value\nClinical cases\n40 (80)\n10 (20)\n0.709\n0.139\nOther\n54 (77.1)\n16 (22.9)\nTABLE\n 2: Effectiveness of ChatGPT-4o depending on the question type: clinical cases vs. others\nData are presented as absolute numbers (N) and percentages (%). No statistically significant differences were found between the groups (p = 0.709; \nχ\n² =\n0.139).\nBased on the analysis of the declared confidence levels in the answers provided by the ChatGPT-4o model, a\nsignificant difference was observed between correct and incorrect answers. The graph shows the distribution\nof confidence levels (on a scale of 1 to 5) for both categories of answers. The model exhibited a significantly\nhigher confidence level for correct answers, while lower confidence values dominated in incorrect answers\n(Figure \n2\n).\nFIGURE\n 2: The declared confidence level of the ChatGPT-4o model\ndepending on the accuracy of the answers\nThe values are presented separately for correct and incorrect responses. A higher confidence level was observed\nfor correct answers.\nDiscussion\nThe PES exam in ophthalmology is one of the key stages in the process of acquiring specialization for\ndoctors in this field. It is characterized by a high level of difficulty and requires both theoretical knowledge\nand practical problem-solving skills in clinical scenarios. This study analyzes how the advanced language\nmodel ChatGPT-4o performed on ophthalmology exam tasks. The model achieved a score of 78.3%, which\nexceeds the passing threshold and is significantly higher than the results obtained by earlier versions of\nChatGPT models in similar studies in other medical fields. This study is the only one available in the\nliterature analyzing the effectiveness of ChatGPT for the PES exam in ophthalmology. Our results align with\nthe observations published by Jaworski et al. in the context of the Final Medical Examination (LEK) exam,\nwhere ChatGPT-4 achieved 77.6%, surpassing the passing threshold (56%) and consistently answering\nclinical and general questions \n[6]\n. A similar effectiveness (78.3%) was observed in the PES exam for\nophthalmology. It is worth noting that newer versions of the model - from ChatGPT-3.5 to 4 and 4o - show a\nclear increase in effectiveness, indicating progress in the medical understanding of AI models. However,\n \n2025 Sławińska et al. Cureus 17(7): e88908. DOI 10.7759/cureus.88908\n7\n of \n9\nunlike the results from the LEK, in our study, the model’s confidence level showed a significant correlation\nwith the accuracy of its answers. This distribution suggests that the model is partially able to recognize its\ncompetencies and signal uncertainty, which could have practical implications when used as a tool for\nsupporting learning or clinical decision-making. It is important to emphasize that these differences were\nstatistically significant, as confirmed by the Mann-Whitney U test. The consistency of these results across\ndifferent exams underscores the potential of AI as a tool to support medical education, but also indicates the\nneed for cautious implementation of AI systems with appropriate expert oversight. Our study results can\nalso be compared with observations published in a paper on the cardiology specialization exam \n[7]\n. This\nstudy evaluated the effectiveness of ChatGPT-3.5 and ChatGPT-4.0 in solving 120 questions from the PES\nexam in cardiology. ChatGPT-4.0 achieved an effectiveness rate of 76.7%, which is comparable to the result\nobtained in our ophthalmology study (78.3%). In both cases, the model exceeded the passing threshold of\n60%, confirming its educational potential even in more challenging specialist exams. A key common point in\nboth studies is the high accuracy of answers to questions based on clinical knowledge. In the cardiology\npaper, the authors observed that the GPT-4 model performed better with case-based questions than with\nquestions requiring the memorization of detailed numerical data or classifications. Blecha et al. conducted a\nstudy aimed at evaluating the performance of the DeepSeek-R1 (DeepSeek AI, Zhejiang, China) and\nChatGPT-4o models on the Infectious Diseases PES exam. They demonstrated that both models are capable\nof passing the exam, making them useful educational tools \n[8]\n. The capabilities of different ChatGPT\nversions in taking medical exams have also been examined, referring among others to allergology \n[9]\n,\ndermatology \n[10]\n, and nuclear medicine \n[11]\n. A similar pattern is observed in our analysis - clinical questions\ndid not significantly differ in effectiveness from other types, and the model’s confidence level was\nsignificantly higher with correct answers. Ultimately, it should be noted that while the results are promising,\nthere are still limitations associated with the use of language models in medical education. These models\ncan generate seemingly convincing but incorrect answers, which, in a clinical context, could lead to\nundesirable decisions. Therefore, their role should be considered as supportive - in education, training, and\ncase analysis - with maintaining appropriate academic supervision.\nConclusions\nThe ChatGPT-4o model demonstrated high effectiveness in solving PES ophthalmology exam questions in\nthe Spring 2025 session, achieving a result significantly exceeding the passing threshold. The results suggest\nthat such AI models can be valuable tools in supporting specialist education, especially in terms of self-\nrepetition, exam simulations, and testing knowledge levels in real-world conditions. Due to their ability to\ngenerate human-like responses and self-assess confidence, models like ChatGPT-4o can also function as\ninteractive teaching assistants, aiding the learning process by identifying knowledge gaps and providing\ntargeted feedback. However, their integration into the medical education system requires further research -\nboth in terms of validating effectiveness in other medical fields and developing safe and ethical\nimplementation strategies. It is also necessary to develop standards for expert oversight of content\ngenerated by AI models and define the limits of their application, ensuring they support the educational\nprocess without replacing critical thinking and expert supervision.\nAdditional Information\nAuthor Contributions\nAll authors have reviewed the final version to be published and agreed to be accountable for all aspects of the\nwork.\nConcept and design:\n  \nAleksander Jaworski, Barbara Sławińska, Oliwia Sysło, Nikola Rubik, Izabela\nJastrzebska, Konrad Haraziński, Weronika Goliat, Maksym \nGmur, Michal Gajewski, Zuzanna Błecha, Nicole\nMaryniak, Dawid Jasiński, Wojciech Jaworski, Natalia Jasińska, Ada Latkowska\nAcquisition, analysis, or interpretation of data:\n  \nAleksander Jaworski, Barbara Sławińska, Oliwia Sysło,\nNikola Rubik, Izabela \nJastrzebska, Konrad Haraziński, Weronika Goliat, Maksym \nGmur, Michal Gajewski,\nZuzanna Błecha, Nicole Maryniak, Dawid Jasiński, Wojciech Jaworski, Natalia Jasińska, Ada Latkowska\nDrafting of the manuscript:\n  \nAleksander Jaworski, Barbara Sławińska, Oliwia Sysło, Nikola Rubik, Izabela\nJastrzebska, Konrad Haraziński, Weronika Goliat, Maksym \nGmur, Michal Gajewski, Zuzanna Błecha, Nicole\nMaryniak, Dawid Jasiński, Wojciech Jaworski, Natalia Jasińska, Ada Latkowska\nCritical review of the manuscript for important intellectual content:\n  \nAleksander Jaworski, Barbara\nSławińska, Oliwia Sysło, Nikola Rubik, Izabela \nJastrzebska, Konrad Haraziński, Weronika Goliat, Maksym\nGmur, Michal Gajewski, Zuzanna Błecha, Nicole Maryniak, Dawid Jasiński, Wojciech Jaworski, Natalia\nJasińska, Ada Latkowska\nSupervision:\n  \nAleksander Jaworski, Barbara Sławińska, Oliwia Sysło, Nikola Rubik, Izabela \nJastrzebska,\nKonrad Haraziński, Weronika Goliat, Maksym \nGmur, Michal Gajewski, Zuzanna Błecha, Nicole Maryniak,\nDawid Jasiński, Wojciech Jaworski, Natalia Jasińska, Ada Latkowska\n \n2025 Sławińska et al. Cureus 17(7): e88908. DOI 10.7759/cureus.88908\n8\n of \n9\nDisclosures\nHuman subjects:\n All authors have confirmed that this study did not involve human participants or tissue.\nAnimal subjects:\n All authors have confirmed that this study did not involve animal subjects or tissue.\nConflicts of interest:\n In compliance with the ICMJE uniform disclosure form, all authors declare the\nfollowing: \nPayment/services info:\n All authors have declared that no financial support was received from\nany organization for the submitted work. \nFinancial relationships:\n All authors have declared that they have\nno financial relationships at present or within the previous three years with any organizations that might\nhave an interest in the submitted work. \nOther relationships:\n All authors have declared that there are no\nother relationships or activities that could appear to have influenced the submitted work.\nReferences\n1\n. \nChatGPT jest już w 3 milionach firm. OpenAI rośnie w siłę i wprowadza nowe funkcje\n. (2025). Accessed:\nJune 25, 2025: \nhttps://www.bankier.pl/wiadomosc/ChatGPT-jest-juz-w-3-milionach-firm-OpenAI-rosnie-\nw-sile-i-wprowadza-nowe-funkcje-89...\n.\n2\n. \nHamet P, Tremblay J: \nArtificial intelligence in medicine\n. Metabolism. 2017, 69S:S36-40.\n10.1016/j.metabol.2017.01.011\n3\n. \nGenc AC, Cekic D, Issever K, et al.: \nCan artificial intelligence predict COVID-19 mortality?\n. Eur Rev Med\nPharmacol Sci. 2023, 27:9866-71. \n10.26355/eurrev_202310_34163\n4\n. \nJaworski A, Jasiński D, Sławińska B, et al.: \nGPT-4o vs. human candidates: performance analysis in the Polish\nfinal dentistry examination\n. Cureus. 2024, 16:e68813. \n10.7759/cureus.68813\n5\n. \nKufel J, Paszkiewicz I, Bielówka M, et al.: \nWill ChatGPT pass the Polish specialty exam in radiology and\ndiagnostic imaging? Insights into strengths and limitations\n. Pol J Radiol. 2023, 88:e430-4.\n10.5114/pjr.2023.131215\n6\n. \nJaworski A, Jasiński D, Jaworski W, et al.: \nComparison of the performance of artificial Intelligence versus\nmedical professionals in the Polish final Medical Examination\n. Cureus. 2024, 16:e66011.\n10.7759/cureus.66011\n7\n. \nWójcik S, Rulkiewicz A, Pruszczyk P, Lisik W, Poboży M, Domienik-Karłowicz J: \nReshaping medical\neducation: performance of ChatGPT on a PES medical examination\n. Cardiol J. 2024, 31:442-50.\n10.5603/cj.97517\n8\n. \nBłecha Z, Jasiński D, Jaworski A, et al.: \nPerformance of GPT-4o and DeepSeek-R1 in the Polish infectious\ndiseases specialty exam\n. Cureus. 2025, 17:e82870. \n10.7759/cureus.82870\n9\n. \nBielówka M, Kufel J, Rojek M, et al.: \nEvaluating ChatGPT-3.5 in allergology: performance in the Polish\nSpecialist Examination\n. Alergologia Polska. 2024, 11:42-7. \n10.5114/pja.2024.135380\n10\n. \nRojek M, Kufel J, Bielówka M, et al.: \nExploring the performance of ChatGPT-3.5 in addressing\ndermatological queries: a research investigation into AI capabilities\n. Przegl Dermatol. 2024, 111:26-30.\n10.5114/dr.2024.140796\n11\n. \nKufel J, Bielówka M, Rojek M, et al.: \nAssessing ChatGPT's performance in national nuclear medicine\nspecialty examination: an evaluative analysis\n. Iran J Nucl Med. 2024, 32:60-5.\n \n2025 Sławińska et al. Cureus 17(7): e88908. DOI 10.7759/cureus.88908\n9\n of \n9",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.9593648910522461
    },
    {
      "name": "Ophthalmology",
      "score": 0.5096547603607178
    },
    {
      "name": "Medical physics",
      "score": 0.3355732560157776
    },
    {
      "name": "Optometry",
      "score": 0.32727178931236267
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210102835",
      "name": "John Paul II Hospital",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I166182418",
      "name": "Medical University of Silesia",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I2800249161",
      "name": "Military University of Technology in Warsaw",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I104588304",
      "name": "University of Szczecin",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I4210105686",
      "name": "Academy of Fine Arts in Katowice",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I4210142757",
      "name": "Międzyleski Szpital Specjalistyczny w Warszawie",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I864159182",
      "name": "University of Silesia in Katowice",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I4210137953",
      "name": "Centrum Onkologii",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I385303915",
      "name": "Wroclaw Medical University",
      "country": "PL"
    }
  ]
}