{
  "title": "Investigating the Impact of Prompt Engineering on the Performance of Large Language Models for Standardizing Obstetric Diagnosis Text: Comparative Study",
  "url": "https://openalex.org/W4391644728",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A120759433",
      "name": "Lei Wang",
      "affiliations": [
        "BGI Group (China)",
        "BGI Research"
      ]
    },
    {
      "id": "https://openalex.org/A5007187211",
      "name": "Wenshuai Bi",
      "affiliations": [
        "BGI Research",
        "BGI Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2098726048",
      "name": "Suling Zhao",
      "affiliations": [
        "BGI Research",
        "BGI Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2976670035",
      "name": "Yinyao Ma",
      "affiliations": [
        "The People's Hospital of Guangxi Zhuang Autonomous Region"
      ]
    },
    {
      "id": "https://openalex.org/A5093889360",
      "name": "Longting Lv",
      "affiliations": [
        "BGI Group (China)",
        "BGI Research"
      ]
    },
    {
      "id": "https://openalex.org/A2489410104",
      "name": "Chenwei Meng",
      "affiliations": [
        "BGI Group (China)",
        "BGI Research"
      ]
    },
    {
      "id": "https://openalex.org/A2343030852",
      "name": "Jingru Fu",
      "affiliations": [
        "BGI Research",
        "BGI Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2518879500",
      "name": "Hanlin Lv",
      "affiliations": [
        "BGI Research",
        "BGI Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A120759433",
      "name": "Lei Wang",
      "affiliations": [
        "BGI Group (China)",
        "BGI Research"
      ]
    },
    {
      "id": "https://openalex.org/A5007187211",
      "name": "Wenshuai Bi",
      "affiliations": [
        "BGI Group (China)",
        "BGI Research"
      ]
    },
    {
      "id": "https://openalex.org/A2098726048",
      "name": "Suling Zhao",
      "affiliations": [
        "BGI Group (China)",
        "BGI Research"
      ]
    },
    {
      "id": "https://openalex.org/A2976670035",
      "name": "Yinyao Ma",
      "affiliations": [
        "The People's Hospital of Guangxi Zhuang Autonomous Region"
      ]
    },
    {
      "id": "https://openalex.org/A5093889360",
      "name": "Longting Lv",
      "affiliations": [
        "BGI Group (China)",
        "BGI Research"
      ]
    },
    {
      "id": "https://openalex.org/A2489410104",
      "name": "Chenwei Meng",
      "affiliations": [
        "BGI Group (China)",
        "BGI Research"
      ]
    },
    {
      "id": "https://openalex.org/A2343030852",
      "name": "Jingru Fu",
      "affiliations": [
        "BGI Group (China)",
        "BGI Research"
      ]
    },
    {
      "id": "https://openalex.org/A2518879500",
      "name": "Hanlin Lv",
      "affiliations": [
        "BGI Research",
        "BGI Group (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2004910511",
    "https://openalex.org/W3115611942",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4387356888",
    "https://openalex.org/W4380730209",
    "https://openalex.org/W4312091558",
    "https://openalex.org/W2913713488",
    "https://openalex.org/W4385573087",
    "https://openalex.org/W4361806442",
    "https://openalex.org/W4361230825",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4388933388",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4387561190",
    "https://openalex.org/W4287265459",
    "https://openalex.org/W4387225871",
    "https://openalex.org/W2952370363",
    "https://openalex.org/W3081505754",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W4287643567",
    "https://openalex.org/W4403515058"
  ],
  "abstract": "Background The accumulation of vast electronic medical records (EMRs) through medical informatization creates significant research value, particularly in obstetrics. Diagnostic standardization across different health care institutions and regions is vital for medical data analysis. Large language models (LLMs) have been extensively used for various medical tasks. Prompt engineering is key to use LLMs effectively. Objective This study aims to evaluate and compare the performance of LLMs with various prompt engineering techniques on the task of standardizing obstetric diagnostic terminology using real-world obstetric data. Methods The paper describes a 4-step approach used for mapping diagnoses in electronic medical records to the International Classification of Diseases, 10th revision, observation domain. First, similarity measures were used for mapping the diagnoses. Second, candidate mapping terms were collected based on similarity scores above a threshold, to be used as the training data set. For generating optimal mapping terms, we used two LLMs (ChatGLM2 and Qwen-14B-Chat [QWEN]) for zero-shot learning in step 3. Finally, a performance comparison was conducted by using 3 pretrained bidirectional encoder representations from transformers (BERTs), including BERT, whole word masking BERT, and momentum contrastive learning with BERT (MC-BERT), for unsupervised optimal mapping term generation in the fourth step. Results LLMs and BERT demonstrated comparable performance at their respective optimal levels. LLMs showed clear advantages in terms of performance and efficiency in unsupervised settings. Interestingly, the performance of the LLMs varied significantly across different prompt engineering setups. For instance, when applying the self-consistency approach in QWEN, the F1-score improved by 5%, with precision increasing by 7.9%, outperforming the zero-shot method. Likewise, ChatGLM2 delivered similar rates of accurately generated responses. During the analysis, the BERT series served as a comparative model with comparable results. Among the 3 models, MC-BERT demonstrated the highest level of performance. However, the differences among the versions of BERT in this study were relatively insignificant. Conclusions After applying LLMs to standardize diagnoses and designing 4 different prompts, we compared the results to those generated by the BERT model. Our findings indicate that QWEN prompts largely outperformed the other prompts, with precision comparable to that of the BERT model. These results demonstrate the potential of unsupervised approaches in improving the efficiency of aligning diagnostic terms in daily research and uncovering hidden information values in patient data.",
  "full_text": "Original Paper\nInvestigating the Impact of Prompt Engineering on the\nPerformance of Large Language Models for Standardizing\nObstetric Diagnosis Text: Comparative Study\nLei Wang1, MS; Wenshuai Bi1, MS; Suling Zhao1, MS; Yinyao Ma2, MS, MD; Longting Lv1, MS; Chenwei Meng1,\nMS; Jingru Fu1, MS; Hanlin Lv1, PhD, MD\n1BGI Research, Shenzhen, China\n2The People's Hospital of Guangxi Zhuang Autonomous Region, Guangxi, China\nCorresponding Author:\nHanlin Lv, PhD, MD\nBGI Research\nBuilding 11, Beishan Industrial Zone\nYantian District\nShenzhen, 518083\nChina\nPhone: 86 18707190886\nEmail: lvhanlin@genomics.cn\nAbstract\nBackground: The accumulation of vast electronic medical records (EMRs) through medical informatization creates significant\nresearch value, particularly in obstetrics. Diagnostic standardization across different health care institutions and regions is vital\nfor medical data analysis. Large language models (LLMs) have been extensively used for various medical tasks. Prompt engineering\nis key to use LLMs effectively.\nObjective: This study aims to evaluate and compare the performance of LLMs with various prompt engineering techniques on\nthe task of standardizing obstetric diagnostic terminology using real-world obstetric data.\nMethods: The paper describes a 4-step approach used for mapping diagnoses in electronic medical records to the International\nClassification of Diseases, 10th revision, observation domain. First, similarity measures were used for mapping the diagnoses.\nSecond, candidate mapping terms were collected based on similarity scores above a threshold, to be used as the training data set.\nFor generating optimal mapping terms, we used two LLMs (ChatGLM2 and Qwen-14B-Chat [QWEN]) for zero-shot learning\nin step 3. Finally, a performance comparison was conducted by using 3 pretrained bidirectional encoder representations from\ntransformers (BERTs), including BERT, whole word masking BERT, and momentum contrastive learning with BERT (MC-BERT),\nfor unsupervised optimal mapping term generation in the fourth step.\nResults: LLMs and BERT demonstrated comparable performance at their respective optimal levels. LLMs showed clear\nadvantages in terms of performance and efficiency in unsupervised settings. Interestingly, the performance of the LLMs varied\nsignificantly across different prompt engineering setups. For instance, when applying the self-consistency approach in QWEN,\nthe F1-score improved by 5%, with precision increasing by 7.9%, outperforming the zero-shot method. Likewise, ChatGLM2\ndelivered similar rates of accurately generated responses. During the analysis, the BERT series served as a comparative model\nwith comparable results. Among the 3 models, MC-BERT demonstrated the highest level of performance. However, the differences\namong the versions of BERT in this study were relatively insignificant.\nConclusions: After applying LLMs to standardize diagnoses and designing 4 different prompts, we compared the results to\nthose generated by the BERT model. Our findings indicate that QWEN prompts largely outperformed the other prompts, with\nprecision comparable to that of the BERT model. These results demonstrate the potential of unsupervised approaches in improving\nthe efficiency of aligning diagnostic terms in daily research and uncovering hidden information values in patient data.\n(JMIR Form Res 2024;8:e53216) doi: 10.2196/53216\nKEYWORDS\nobstetric data; similarity embedding; term standardization; large language models; LLMs\nJMIR Form Res 2024 | vol. 8 | e53216 | p. 1https://formative.jmir.org/2024/1/e53216\n(page number not for citation purposes)\nWang et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nIntroduction\nThe advancement of medical informatization has resulted in the\naccumulation of vast amounts of electronic medical records\n(EMRs) in hospitals, giving rise to medical big data [1]. These\ndata hold significant research value. Using obstetrics as an\nexample, the implementation of China’s “three-child” policy\nin 2021 has led to an increasing proportion of women with\nadvanced maternal age and multiparity. Studies indicate that as\nmaternal age and parity increase, the occurrence of pregnancy\ncomplications and adverse pregnancy outcomes also tends to\nrise, posing new challenges for obstetrics across health care\ninstitutions at all levels [2]. Extracting valuable information\nfrom obstetric EMRs could significantly benefit clinical research\naimed at improving pregnancy success rates.\nHowever, due to varying writing habits among doctors,\ndiagnostic descriptions in medical records lack standardization,\nwhich hinders the analysis and use of medical data.\nConsequently, mapping clinical diagnostic descriptions to a\nstandard terminology database is vital for medical data analysis.\nThis process enables the standardization of medical terms across\ndifferent health care institutions and regions, preventing\nmisunderstandings and confusion caused by varying\nterminologies. It positively impacts health care quality, reduces\nmedical costs, enhances doctor-patient relationships, and\npromotes the development of medical science.\nThe emergence of large language models (LLMs), represented\nby ChatGPT, has caused a surge in interest in their application\nacross various fields of research. In the medical domain, LLMs\nhave been extensively used for tasks such as intelligent medical\nhistory collection and preliminary diagnosis, personalized\ntreatment and drug recommendations, medical record\ndocumentation and report generation, literature retrieval and\nanalysis, and medical education and training [3-6]. Kanjee et\nal [7] assessed ChatGPT’s ability to accurately diagnose\nchallenging medical cases and suggested that generative artificial\nintelligence (AI) models hold promise as potential aids to human\ndiagnostic cognition. Research by Agbavor and Liang [8]\ndemonstrated that GPT-3–generated text embeddings can\nreliably distinguish Alzheimer disease patients from healthy\ncontrols and infer cognitive test scores of patients, potentially\nenhancing early dementia diagnosis. Palanica et al [9] explored\nChatGPT’s potential applications in psychological counseling,\nemotional support, and mental illness screening while discussing\nrelated challenges and future research directions.\nLLMs have also played a crucial role in medical research.\nClinical research often involves large amounts of unlabeled\nnatural language data, and LLMs’ zero-shot learning ability\nallows them to effectively process such data. Agrawal et al [10]\nshowed that ChatGPT excels in extracting zero-shot and\nfew-shot information from clinical texts. Hu et al [11] revealed\nChatGPT’s potential in zero-shot clinical entity recognition\ntasks. Furthermore, Lamichhane's [12] 3 text-based experiments\non mental health classification demonstrated ChatGPT’s\npotential in zero-shot text classification tasks.\nLLMs are chatbot technologies based on natural language\nprocessing and deep learning; they learn language patterns and\nknowledge from a large amount of text data to realize natural\nconversations with humans. The key to effectively using LLMs\nis to set an optimal prompt [13].\nIn few-shot learning, designing appropriate prompts can help\nLLMs learn better from a small number of training samples and\nimprove performance [13]. Even in zero-shot learning scenarios,\nappropriate prompts can guide LLMs to use contextual\ninformation to output correct results [14]. Prompt engineering\nhas been widely used in various fields of natural language\nprocessing, such as question answering, text generation, and\nsentiment classification, as well as other tasks. By carefully\ndesigning prompts, LLMs can better understand the task\nrequirements and context and generate more accurate and useful\noutputs [13,15,16]. In addition, prompt engineering is an\nefficient method that does not rely on large-scale computing\nresources. It can narrow the gap between the pretraining and\nfine-tuning stages, improve the model’s learning ability and\ngeneralization ability on a small amount of data, and fully\nexploit the model’s potential performance [15].\nChain-of-thought (CoT) prompts were proposed by Wei et al\n[17], who experimented with the effect of CoT prompts on\nmultiple tasks, including mathematical problems, logical\nreasoning, reading comprehension, and common sense\nreasoning; they compared it with other prompt engineering\ntechniques and pointed out that CoT prompts could significantly\nimprove the model’s performance on these tasks and even allow\nthe model to show complex reasoning abilities, such as\ninduction, deduction, and analogy. The basic idea of CoT\nprompts is that, when giving a question or task, instead of\ndirectly asking the model to give an answer or result, the user\nasks the model to give a CoT, that is, a series of intermediate\nreasoning steps in which each step is a complete sentence, and\nthe last step is the answer or result. The advantage of this is that\nit can make the model better understand the meaning and goal\nof the question or task, avoid irrelevant or wrong outputs, and\nalso make it easier for human users to check and evaluate the\nmodel’s output.\nThe goal of self-consistency prompts is to improve the quality\nand consistency of the generated results by requiring the model\nto make consistency judgments on the previously generated text\n[18]. When using self-consistency prompts, the user first\nprovides an initial text as a prompt and then lets the model\ncontinue to generate the subsequent text. Next, the user replaces\nthe “greedy decoding” in the CoT prompt with sampling from\nthe language model’s decoder to generate a set of diverse\nreasoning paths; finally, the user marginalizes the reasoning\npaths and aggregates them by selecting the most consistent\nanswer in the final answer. This can force the model to maintain\nself-consistency when generating text, avoiding contradictions\nand incoherence.\nThis paper delves into the potential of LLMs for zero-shot or\nunsupervised learning in the domain of standardizing diagnostic\nterminology in obstetrics. By leveraging a composite approach\nthat merges different prompt engineering techniques with LLMs,\nour goal is to identify the most fitting pipeline for unsupervised\nscenarios.\nJMIR Form Res 2024 | vol. 8 | e53216 | p. 2https://formative.jmir.org/2024/1/e53216\n(page number not for citation purposes)\nWang et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nAs most of the LLMs used in the Chinese domain use the\nChinese version of the International Classification of Diseases,\n10th revision (ICD-10-CN), as their core training corpus [19],\nin order to compare the performance of LLMs and supervised\nlearning algorithms horizontally on a baseline, we used standard\ndiagnostic terminology in the ICD-10-CN as the alignment\ntarget throughout this study.\nMethods\nTask Overview\nThe approach can be divided into 4 steps: (1) mapping the\ndiagnosis in EMRs to the observation domain of the ICD-10-CN\nvia embedded similarity; (2) collecting the candidate mapping\nterms with similarity above the threshold as the training data\nset; (3) using 2 LLMs, ChatGLM2 [20] and Qwen-14B-Chat\n(QWEN) [21], with zero-shot learning to generate the optimal\nmapping terms; and (4) using 3 pretrained bidirectional encoder\nrepresentations from transformers (BERTs), BERT [22], whole\nword masking BERT (BERT-WWM) [23], and momentum\ncontrastive learning with BERT (MC-BERT) [24], for\nunsupervised generation of the optimal mapping terms for\nperformance comparison. The entire workflow is illustrated in\nFigure 1.\nFigure 1. The 4-step approach of this study. For optimal term selection, the combination of a large language model (LLM) and prompt engineering\ncontributes to the unsupervised learning approach to select the optimal terms from 10 candidates. BERT: bidirectional encoder representations from\ntransformers; BERT-WMM: whole word masking BERT; MC-BERT: momentum contrastive learning with BERT; QWEN: Qwen-14B-Chat.\nData Preparation\nIn this study, the raw data were collected from the obstetric\nEMR data of the People’s Hospital of Guangxi Zhuang\nAutonomous Region from April 2014 to April 2022; these data\ncontained only diagnostic reports. Sample data are shown in\nTextbox 1.\nTextbox 1. A sample data of diagnoses for ID 720444 is listed below with a translated version. All data processed in this research were in Chinese.\nDischarge diagnoses\n1. 头位顺产\n2. 单胎活产\n3. 孕1产1妊娠39+4周\n4. 羊水偏少\nTranslations\n1. Vertex delivery\n2. Singleton live birth\n3. Pregnancy: G1P1, 39+4 weeks\n4. Oligohydramnios\nThe raw data set underwent data preprocessing by removing\npunctuation marks and meaningless special symbols to avoid\npotential interference with subsequent word segmentation\noperations.\nWe implemented LLMs in an intranet security environment.\nBoth ChatGLM2 and QWEN were used exclusively on\nphysically isolated graphical processing units, with access\nfacilitated via OpenAI format and FastAPI (built on PyTorch\n2.0). Temperature settings for the LLMs were configured at 0,\nwith max_token parameters tailored on a task-by-task basis.\nThe standard vocabulary referred to in the following text consists\nof the diagnostic categories belonging to the observation domain\nof ICD-10-CN.\nJMIR Form Res 2024 | vol. 8 | e53216 | p. 3https://formative.jmir.org/2024/1/e53216\n(page number not for citation purposes)\nWang et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nEmbedding Learning\nWe used the conditional random fields (CRF) model [24] to\nsegment the text and obtained original-diagnosis raw data\naligned with standard vocabulary terms. The principle of CRF\nis to treat word segmentation as a character position\nclassification problem. Character position information is often\ndefined as follows: B represents the beginning of a word, M\ndenotes the middle of a word, E signifies the end of a word, and\nS indicates a single-character word. Feature functions are\nconstructed to describe the relationship between each character\nand label and the transition between adjacent labels. Using\ntraining data, we learn the weights of feature functions to\nmaximize conditional probability. The Viterbi algorithm predicts\nnew input sequences and finds the most probable label sequence;\naccording to the label sequence, we construct word segmentation\nresults from characters between B and E and single characters\nS. As shown in Textbox 2, we conducted CRF word\nsegmentation on diagnoses in EMRs.\nTextbox 2. Sample of word segmentation with the conditional random fields model. The data below represent a preliminary diagnosis of placenta\nabruption.\nOriginal word: 初步 诊 断 为 胎 盘 早剥;\nAfter CRF annotation: 初/B 步/M 诊/M 断/E 为/S 胎/B 盘/M 早/M 剥/E\nAccording to the label, the word segmentation result is as follows: 初步 诊 断/为/胎 盘 早剥.\nTo calculate the similarity between diagnoses in the raw data\nset and terms in the standard vocabulary, we used the\nBERT-medicine model to transform diagnoses and terms into\nembeddings for storage. The BERT-medicine model is\nspecifically designed to improve the model’s understanding of\nmedical terms and symptoms by introducing a medical\ndomain–specific vocabulary list, lexicon, and pretraining tasks.\nThe main structure of the BERT-medicine model is the BERT,\nand the main inputs are the raw word vectors of each word or\nphrase in the text. In this study, we used diagnoses in the raw\ndata set as the input text sequences. The BERT model extracted\nthe contextual information of the text through a self-attention\nmechanism and learned the bidirectional linguistic\nrepresentations, so as to obtain a semantic representation of\neach word in its context. The final output embedding vector is\nrepresented by the sum of character embedding, partition\nembedding, and position embedding, which constitute the input\nsequence.\nSimilarity Computation\nThe feature embedding of the diagnosis is denoted by \n , the\nfeature embedding of the standard terms is denoted by \n , and\ntheir similarity is calculated using the cosine similarity with the\nfollowing formula:\nThe proposed approach is evaluated through the following steps:\nStandard terms with a similarity score higher than 0.9 are\nconsidered candidates for diagnosis keywords and are then\nverified by medical experts. The normalized precision and recall\nare calculated, and the precision-recall curve is obtained. Since\na particular diagnosis might have multiple similar standard\nterms, we aimed to identify as many similar terms as possible,\nand we thus expected high recall and precision. To obtain\ncandidate terms, we collected the original diagnosis and the 10\nmost similar standard terms having a similarity score greater\nthan or equal to 0.855.\nOptimal Term Selection\nTo comprehensively evaluate the performance of LLMs in the\nstandardization of obstetric diagnostic terminology, we used 4\ndifferent prompts, with the prompt design ranging from simple\nto complex. This started with the prompt trained on zero samples\n(the zero-shot learning prompt); next were the prompt trained\non a small number of samples, the in-context learning prompt,\nthe CoT prompt, and finally the self-consistency prompt. The\nspecific flow chart of LLM training is shown in Figure 2.\nJMIR Form Res 2024 | vol. 8 | e53216 | p. 4https://formative.jmir.org/2024/1/e53216\n(page number not for citation purposes)\nWang et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nFigure 2. Prompt examples under 4 distinct prompt engineering methods: (A) the zero-shot learning prompt, (B) the in-context learning prompt, (C)\nthe chain-of-thought prompt, and (D) the self-consistency prompt. In the experiment, the raw data being in Chinese led us to use Chinese prompts,\nwhich are translated for readability purposes. However, “diagnosis” and “candidate terms” are displayed in their original Chinese format. LLM: large\nlanguage model.\nThe zero-shot learning prompt was meant to guide the LLMs’\noutput by directly telling them the purpose of this study. The\ntarget task of this study was to find the standard-term expression\nfor the diagnosis, that is, to let the LLMs determine the word\nwith the highest similarity. Therefore, we directly told the LLMs\nto find the most similar word to the input word among the\ncandidate words for the standard term. The LLMs determined\nthe similarity between words based on their own learned\nknowledge, and then output the word with the highest similarity\nto the input word as the output result.\nThe purpose of in-context learning prompts is to give context\nhints and let LLMs learn by analogy from few shots to output\nresults that more closely meet the requirements [25]. Its input\nis in the form of {question, answer}, that is, in the input, the\nquestion and result are given to the LLM as a template, and it\nanswers the same type of questions in a specific way according\nto the specific answer.\nThe input form of CoT prompts is similar to in-context learning\nprompts, that is, {question, answer}, with the difference that\nthe answer contains the intermediate steps of thinking. In order\nto reduce human costs, we used LLMs to generate CoT prompts,\nand then encapsulated them into the prompt inputs.\nThe key method of self-consistency prompts in this study was\nto input the CoT prompts from the previous section multiple\ntimes, obtain multiple results, randomly sample a group of\noutput results, and use the majority voting method to decide the\nfinal result. Next, we will demonstrate the experimental process\nwith different prompts through specific examples, shown in\nFigure 3.\nJMIR Form Res 2024 | vol. 8 | e53216 | p. 5https://formative.jmir.org/2024/1/e53216\n(page number not for citation purposes)\nWang et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nFigure 3. Detailed illustration of the technical intricacies underlying this study. The process of mapping nonstandardized local diagnostic text to\nstandardized International Classification of Diseases, 10th revision, Chinese version (ICD-10-CN) terms involves preliminary similarity-based selection\nthrough the vector database, followed by optimal solution selection performed by large language models (LLMs) based on semantic comprehension.\nEvaluation\nThe evaluation metrics in this study to assess the model’s\nperformance were precision, recall, and F1-score [26]. We\nclassified words that matched the original word and the standard\nword as positives, and those that did not match as negatives.\nThere were 4 possible classification outcomes: true positive, in\nwhich the model correctly identified a positive as positive; false\nnegative, in which the model mistakenly classified a positive\nas negative; true negative, in which the model correctly\nidentified a negative as negative; and false positive, in which\nthe model mistakenly classified a negative as positive. Using\nthese classification outcomes, we could calculate precision,\nrecall, and F1-score to evaluate the model’s performance in\nstandardizing diagnoses.\nPrecision, recall, and F1-score (the reconciled mean of precision\nand recall) were defined as follows:\nEthical Considerations\nThe study was approved by the People’s Hospital of the Guangxi\nZhuang Autonomous Region in China (KT-KJT-2021-67), and\nall pregnancy data were deidentified and anonymized.\nResults\nOverview\nFor similarity computation, according to experimental tests, an\naverage precision of 0.88 met the requirement for high precision\nand recall. The corresponding threshold value at this point was\n0.855. Therefore, the threshold value for calculations of\nsimilarity was determined to be 0.855, which was used to filter\nout standard terms that were not similar enough to the diagnosis.\nAfter collecting the candidate data set, we used 2 LLMs and 4\ntechniques for prompt engineering. Subsequently, we mapped\nthe LLM outputs to the most suitable candidate terms from the\nICD-10-CN standard vocabulary, enabling us to calculate\nprecision, recall, and F1-score. In order to undertake entity\nnormalization, we selected the classic BERT series, comprising\nBERT, MC-BERT, and BERT-WWM, as our comparison\nmodels. We then compared their performance with the results\nobtained using the LLMs with 4 different prompts. The\noutcomes of this comparison are presented in Table 1.\nJMIR Form Res 2024 | vol. 8 | e53216 | p. 6https://formative.jmir.org/2024/1/e53216\n(page number not for citation purposes)\nWang et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nTable 1. Metric performance comparison across large language model and bidirectional encoder representations from transformers (BERT) series.\nF1-score, %Recall, %Precision, %Model and prompt engineering approach\n91.9491.9591.93BERTa\n92.3592.3792.34Momentum contrastive learning with BERT (MC-BERT)a\n92.1592.1792.13BERT-whole word maskinga\nChatGLM2\n81.7989.9075.02Zero shot\nBERT\n85.8586.6085.13In context\n82.5188.9386.52Chain of thought\n89.3190.1188.53Self consistency\nQwen-14B-Chat\n85.5386.7284.01Zero shot\n89.6991.1888.25In context\n90.6091.3089.92Chain of thought\n91.5192.1390.91Self consistency\naPrompt engineering not applicable to these models.\nIt is evident from the table that the LLMs and BERT displayed\ncomparable performance at their optimal levels, indicating that\nthe LLMs provided a performance and time advantage under\nunsupervised conditions. Furthermore, the LLMs exhibited\nvaried performance under different prompt engineering setups.\nTaking QWEN as an example, the implementation of the\nself-consistency approach improved the F1-score by 5% and\nprecision by 7.9% compared to the zero-shot method. Similarly,\nthe same proportion of correctly generated responses was\nobserved in ChatGLM2’s performance, with a range from 9.19%\nto 18.02%. Thus, QWEN achieved better performance than\nChatGLM2 in all 4 prompt engineering approaches.\nThe BERT series were additional comparison models and\nexhibited more comparable results in this task. Among the 3\nmodels shown in Table 2, MC-BERT delivered the best\nperformance. However, in this study, the disparity between the\n3 versions of BERT was relatively small.\nJMIR Form Res 2024 | vol. 8 | e53216 | p. 7https://formative.jmir.org/2024/1/e53216\n(page number not for citation purposes)\nWang et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nTable 2. Cluster results of standardized terms. Original words in Chinese translated to English via ChatGPT.\nWord 6Word 5Word 4Word 3Word 2Word 1Word 0ID\nThalassemiaCombined tha-\nlassemia\nMajor thalassemiaIntermedia tha-\nlassemia\nδ-β-Thalassemiaβ-Thalassemiaα-Thalassemia2\nChronic amniotic\nfluid–type fetal\ndistress\nChronic fetal -\nheart type fetal\ndistress\nChronic fetal dis-\ntress\nAcute amniotic\nfluidtype fetal\ndistress\nAcute fetal heart-\ntype fetal distress\nAcute fetal dis-\ntress\nAcute mixed-type\nfetal distress\n23\nFetal kidney mal-\nformations\nFetal structural\nanomalies\nFetal malforma-\ntions\nFetal ear malfor-\nmations\nFetus with multiple\nmalformations\nFetal limb malfor-\nmations\nFetal cardiac mal-\nformations\n55\nUterine multiple\nleiomyoma\nUterine leiomy-\noma\nUterine mucosal\nleiomyoma\nUterine submu-\ncosal leiomyoma\nUterine intramural\nleiomyoma\nUterine subseros-\nal leiomyoma\nUterine interstitial\nleiomyoma\n73\n————aBiliary stonesHepatobiliary\nstones\nIntrahepatic bile\nduct stones\n76\n————Moderate pul-\nmonary arterial hy-\npertension\nMild pulmonary\narterial hyperten-\nsion\nSevere pulmonary\narterial hyperten-\nsion\n25\n————Pelvic outlet steno-\nsis\nPelvic stenosisCentral pelvic\nstenosis\n20\n————Chronic bronchitisAcute tracheitisAcute bronchitis36\n————Pregnancy-related\nurethral infection\nPregnancy-relat-\ned urinary tract\ninfection\nPregnancy-related\nreproductive tract\ninfection\n91\naNot applicable.\nAdditional Research\nIn this study, we used the Louvain algorithm to mine terms from\nthe standard data set output by the LLMs and obtained 1100\nrelatively common diagnostic terms. In the medical field,\ndifferent medical institutions and professionals may use different\nterms to describe the same or similar clinical diagnoses, which\ncan cause difficulties and misunderstandings in data exchange,\nstatistics, and analysis. Therefore, standardizing clinical\ndiagnostic terms is an important task. The standardized terms\ncan be used to unify treatment plans and disease statistics, as\nwell as to build clinical diagnostic knowledge bases. The data\nin our study were clustered into 107 clusters, and each cluster\nwas analyzed separately, resulting in a diagnostic clustering\ntable. Part of the results of the clustering table are shown in\nTable 2.\nDiscussion\nPrincipal Results\nThis paper proposes an effective unsupervised standardization\nmethod for obstetric diagnosis. Through a multi-metrics\ncomparison of different LLMs under various prompt engineering\nstrategies, we found that unsupervised LLMs coupled with\neffective prompt engineering can achieve performance\ncomparable to supervised learning.\nA comparison of different prompt engineering strategies showed\nthat although the models’baseline performance under zero-shot\nsettings varied, they generally showed significant improvement\nafter incorporating strategies such as CoT, which also highlights\nthe importance of effective prompts for LLMs.\nThe goal of our alignment in this study is the ICD-10-CN\nterminology, which belongs to the core vocabulary of the\nChinese medical field. LLMs trained on Chinese language data\nusually include it as part of the training corpus [19], and the\nperformance of the baseline model allows prompt engineering\nto further improve the alignment performance.\nComparison With Prior Work\nCompared to previous research that primarily relied on\nBERT-based methods to map diagnostic descriptions from\nEMRs to standard terminologies, this study explores a novel\napproach based on LLMs. Among BERT models, we identified\nMC-BERT as the top performer, achieving an F1-score of\n0.9235.\nBeyond the conventional BERT methods, we examined 4\nmainstream prompt strategies and found that the self-consistency\nmethod outperformed the others, achieving an F1-score of\n0.9233. This level of performance matches that of supervised\nlearning, opening up new possibilities for terminology mapping\nresearch in the medical domain.\nLimitations\nAs all data were sourced from real-world patient information,\nand even though we anonymized the data through multiple\nstrategies and only used a portion of the diagnostic text\ninformation without any personal identifying information, there\nis still a risk associated with uploading patient data to an open\nnetwork. Additionally, as our research objective was to align\nand standardize Chinese text based on Chinese target\nterminologies, the choice of LLMs used in this study was\nlimited. The development of LLMs in the Chinese domain is\nJMIR Form Res 2024 | vol. 8 | e53216 | p. 8https://formative.jmir.org/2024/1/e53216\n(page number not for citation purposes)\nWang et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nadvancing rapidly, and there are many newly released versions\nthat we have yet to explore.\nMoreover, our alignment target was for scientific exploration.\nIn future studies, we will attempt to train target vocabulary that\nis more suited to the scientific research context into the model\nthrough methods such as global optimization and exploring\nsemantic alignment scenarios.\nConclusions\nThis paper investigates the capability of LLMs in standardizing\nclinical medical terms. By using LLMs to standardize diagnostic\nterms extracted from real-world obstetric EMRs and designing\n4 different prompts for LLMs, we were able to compare their\noutput results with those of the BERT model. Our findings\ndemonstrate that QWEN mostly achieved the best performance\nand had precision on par with the BERT model, which illustrates\nthat an unsupervised approach improved the efficiency of\naligning diagnostic terms in daily research and to uncover the\nhidden value of patient data information.\nAcknowledgments\nThis study was supported by Guangxi Key Research and Development Program (AB22035056). We thank the China National\nGeneBank for technical support.\nData Availability\nThe data sets generated during and/or analyzed during this study are not publicly available due to privacy and ethical restrictions\nbut are available from the corresponding author on reasonable request.\nConflicts of Interest\nNone declared.\nReferences\n1. Jensen PB, Jensen LJ, Brunak S. Mining electronic health records: towards better research applications and clinical care.\nNat Rev Genet. May 02, 2012;13(6):395-405. [doi: 10.1038/nrg3208] [Medline: 22549152]\n2. Morales-Suárez-Varela M, Clemente-Bosch E, Peraita-Costa I, Llopis-Morales A, Martínez I, Llopis-González A. Maternal\nphysical activity during pregnancy and the effect on the mother and newborn: a systematic review. J Phys Act Health. Jan\n01, 2021;18(1):130-147. [doi: 10.1123/jpah.2019-0348] [Medline: 33361475]\n3. Sallam M. ChatGPT utility in healthcare education, research, and practice: systematic review on the promising perspectives\nand valid concerns. Healthcare (Basel). Mar 19, 2023;11(6):9. [FREE Full text] [doi: 10.3390/healthcare11060887] [Medline:\n36981544]\n4. Lee P, Carey G, Isaac K. The AI Revolution in Medicine: GPT-4 and Beyond. London, UK. Pearson Education; Apr 14,\n2023.\n5. Kung TH, Cheatham M, Medenilla A, Sillos C, De Leon L, Elepaño C, et al. Performance of ChatGPT on USMLE: potential\nfor AI-assisted medical education using large language models. PLOS Digit Health. Feb 2023;2(2):e0000198. [FREE Full\ntext] [doi: 10.1371/journal.pdig.0000198] [Medline: 36812645]\n6. Jeblick K, Schachtner B, Dexl J, Mittermeier A, Stüber AT, Topalis J, et al. ChatGPT makes medicine easy to swallow:\nan exploratory case study on simplified radiology reports. Eur Radiol. Oct 05, 2023:1-9. [doi: 10.1007/s00330-023-10213-1]\n[Medline: 37794249]\n7. Kanjee Z, Crowe B, Rodman A. Accuracy of a generative artificial intelligence model in a complex diagnostic challenge.\nJAMA. Jul 03, 2023;330(1):78-80. [FREE Full text] [doi: 10.1001/jama.2023.8288] [Medline: 37318797]\n8. Agbavor F, Liang H. Predicting dementia from spontaneous speech using large language models. PLOS Digit Health. Dec\n2022;1(12):e0000168. [FREE Full text] [doi: 10.1371/journal.pdig.0000168] [Medline: 36812634]\n9. Palanica A, Flaschner P, Thommandram A, Li M, Fossat Y. Physicians' perceptions of chatbots in health care: cross-sectional\nweb-based survey. J Med Internet Res. Apr 05, 2019;21(4):e12887. [FREE Full text] [doi: 10.2196/12887] [Medline:\n30950796]\n10. Agrawal M, Hegselmann S, Lang H, Kim Y, Sontag D. Large language models are few-shot clinical information extractors.\nIn: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics; 2022;1998-2022.\n11. Hu Y, Ameer I, Zuo X, Peng X, Zhou Y, Li Z. Zero-shot clinical entity recognition using ChatGPT. arXiv. Posted online\nMarch 29, 2023. [doi: 10.48550/arXiv.2303.16416]\n12. Lamichhane B. Evaluation of ChatGPT for NLP-based mental health applications. arXiv. Preprint posted online March 28,\n2023. [doi: 10.48550/arXiv.2303.15727]\n13. Liu P, Yuan W, Fu J, Jiang Z, Hayashi H, Neubig G. Pre-train, prompt, and predict: a systematic survey of prompting\nmethods in natural language processing. ACM Comput Surv. Jan 16, 2023;55(9):1-35. [doi: 10.1145/3560815]\nJMIR Form Res 2024 | vol. 8 | e53216 | p. 9https://formative.jmir.org/2024/1/e53216\n(page number not for citation purposes)\nWang et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\n14. Zhang Z, Zhang A, Li M, Smola A. Automatic chain of thought prompting in large language models. arXiv. Preprint posted\nonline October 7, 2023. [doi: 10.48550/arXiv.2210.03493]\n15. Lester B, Al-Rfou R, Constant N. The power of scale for parameter-efficient prompt tuning. arXiv. Preprint posted online\nApril 18, 2021. [doi: 10.18653/v1/2021.emnlp-main.243]\n16. White J, Fu Q, Hays S, Sandborn M, Olea C, Gilbert H. A prompt pattern catalog to enhance prompt engineering with\nChatGPT. arXiv. Preprint posted online February 21, 2023. [doi: 10.1007/978-1-4842-9852-7_4]\n17. Wei J, Wang X, Schuurmans D, Bosma M, Ichter B, Xia F. Chain-of-thought prompting elicits reasoning in large language\nmodels. Adv Neural Inf Process Syst. 2022;35:24824-24837. [doi: 10.48550/iv.2201.11903]\n18. Wang X, Wei J, Schuurmans D, Le Q, Chi E, Narang S. Self-consistency improves chain of thought reasoning in language\nmodels. arXiv. Preprint posted online March 21, 2022. [doi: 10.48550/arXiv.2203.11171]\n19. Boyle J, Kascenas A, Lok P, Liakata M. Automated clinical coding using off-the-shelf large language models. arXiv.\nPreprint posted online October 10, 2023. [doi: 10.48550/arXiv.2310.06552]\n20. Du Z, Qian Y, Liu X, Ding M, Qiu J, Yang Z. GLM: general language model pretraining with autoregressive blank infilling.\narXiv. Preprint posted online March 18, 2021. [doi: 10.48550/arXiv.2103.10360]\n21. Bai J, Bai S, Chu Y, Cui Z, Dang K, Deng X. Qwen technical reports. arXiv. Preprint posted online September 28, 2023.\n[doi: 10.48550/arXiv.2309.16609]\n22. Devlin J, Chang M, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding.\nIn: Proceedings of the 2019 Annual Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies. Presented at: NAACL-HLT 2019; January 2019, 2019;4171-4187; Minneapolis,\nMN.\n23. Cui Y, Che W, Liu T, Qin B, Yang Z. Pre-Training With Whole Word Masking for Chinese BERT. IEEE/ACM Trans.\nAudio Speech Lang. Process. 2021;29:3504-3514. [doi: 10.1109/TASLP.2021.3124365]\n24. Zhang N, Jia Q, Yin K, Dong L, Gao F, Hua N. Conceptualized representation learning for Chinese biomedical text mining.\narXiv. Preprint posted online August 25, 2020. [doi: 10.48550/arXiv.2008.10813]\n25. Dong Q, Li L, Dai D, Zheng C, Wu Z, Chang B. A survey on in-context learning. arXiv. Preprint posted online December\n31, 2022. [doi: 10.48550/arXiv.2301.00234]\n26. Powers D. Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation. arXiv.\nPreprint posted online October 11, 2020. [doi: 10.48550/arXiv.2010.16061]\nAbbreviations\nAI: artificial intelligence\nBERT: bidirectional encoder representations from transformers\nBERT-WMM: whole word masking bidirectional encoder representations from transformers\nCoT: chain-of-thought\nCRF: conditional random fields\nEMR: electronic medical record\nICD-10-CN: Chinese version of the International Classification of Diseases, 10th revision\nLLM: large language model\nMC-BERT: momentum contrastive learning with bidirectional encoder representations from transformers\nQWEN: Qwen-14B-Chat\nEdited by A Mavragani; submitted 29.09.23; peer-reviewed by M Harris, H Chen; comments to author 20.12.23; revised version\nreceived 25.12.23; accepted 11.01.24; published 08.02.24\nPlease cite as:\nWang L, Bi W, Zhao S, Ma Y, Lv L, Meng C, Fu J, Lv H\nInvestigating the Impact of Prompt Engineering on the Performance of Large Language Models for Standardizing Obstetric Diagnosis\nText: Comparative Study\nJMIR Form Res 2024;8:e53216\nURL: https://formative.jmir.org/2024/1/e53216\ndoi: 10.2196/53216\nPMID: 38329787\n©Lei Wang, Wenshuai Bi, Suling Zhao, Yinyao Ma, Longting Lv, Chenwei Meng, Jingru Fu, Hanlin Lv. Originally published\nin JMIR Formative Research (https://formative.jmir.org), 08.02.2024. This is an open-access article distributed under the terms\nof the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use,\nJMIR Form Res 2024 | vol. 8 | e53216 | p. 10https://formative.jmir.org/2024/1/e53216\n(page number not for citation purposes)\nWang et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\ndistribution, and reproduction in any medium, provided the original work, first published in JMIR Formative Research, is properly\ncited. The complete bibliographic information, a link to the original publication on https://formative.jmir.org, as well as this\ncopyright and license information must be included.\nJMIR Form Res 2024 | vol. 8 | e53216 | p. 11https://formative.jmir.org/2024/1/e53216\n(page number not for citation purposes)\nWang et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX",
  "topic": "Medical diagnosis",
  "concepts": [
    {
      "name": "Medical diagnosis",
      "score": 0.6094147562980652
    },
    {
      "name": "Computer science",
      "score": 0.5781962871551514
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49109935760498047
    },
    {
      "name": "Standardization",
      "score": 0.4575348198413849
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.43061399459838867
    },
    {
      "name": "Medical record",
      "score": 0.4179285764694214
    },
    {
      "name": "Natural language processing",
      "score": 0.3786754012107849
    },
    {
      "name": "Data science",
      "score": 0.33889031410217285
    },
    {
      "name": "Machine learning",
      "score": 0.32842719554901123
    },
    {
      "name": "Medicine",
      "score": 0.2554605007171631
    },
    {
      "name": "Pathology",
      "score": 0.11166629195213318
    },
    {
      "name": "Radiology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4387153646",
      "name": "BGI Research",
      "country": null
    },
    {
      "id": "https://openalex.org/I100135526",
      "name": "BGI Group (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210115919",
      "name": "The People's Hospital of Guangxi Zhuang Autonomous Region",
      "country": "CN"
    }
  ],
  "cited_by": 17
}