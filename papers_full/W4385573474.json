{
  "title": "Adapting a Language Model While Preserving its General Knowledge",
  "url": "https://openalex.org/W4385573474",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5013224749",
      "name": "Zixuan Ke",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5109589823",
      "name": "Yijia Shao",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5101191761",
      "name": "Haowei Lin",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5100385219",
      "name": "Xu Hu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5007723627",
      "name": "Lei Shu",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5100339921",
      "name": "Bing Liu",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2980360762",
    "https://openalex.org/W4288351176",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W3104263050",
    "https://openalex.org/W2952357537",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3163479207",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W4287121455",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W4223490341",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2914845972",
    "https://openalex.org/W2963168371",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2963981420",
    "https://openalex.org/W1964613733",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W4287816361",
    "https://openalex.org/W4286974574",
    "https://openalex.org/W2971196067",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W3015982254",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2808556605",
    "https://openalex.org/W2279376656",
    "https://openalex.org/W4304206638",
    "https://openalex.org/W3088107006",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3106070274",
    "https://openalex.org/W4312744085",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W4206281850",
    "https://openalex.org/W1682403713"
  ],
  "abstract": "Domain-adaptive pre-training (or DA-training for short), also known as post-training, aimsto train a pre-trained general-purpose language model (LM) using an unlabeled corpus of aparticular domain to adapt the LM so that end-tasks in the domain can give improved performances. However, existing DA-training methods are in some sense blind as they do not explicitly identify what knowledge in the LM should be preserved and what should be changed by the domain corpus. This paper shows that the existing methods are suboptimal and proposes a novel method to perform a more informed adaptation of the knowledge in the LM by (1) soft-masking the attention heads based on their importance to best preserve the general knowledge in the LM and (2) contrasting the representations of the general and the full (both general and domain knowledge) to learn an integrated representation with both general and domain-specific knowledge. Experimental results will demonstrate the effectiveness of the proposed approach.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10177–10188\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nAdapting a Language Model While Preserving its General Knowledge\nZixuan Ke1, Yijia Shao2, Haowei Lin2, Hu Xu3, Lei Shu1∗ and Bing Liu1\n1Department of Computer Science, University of Illinois at Chicago\n2Wangxuan Institute of Computer Technology, Peking University\n3Meta AI\n1{zke4,liub}@uic.edu\n2{shaoyj, linhaowei}@pku.edu.cn\n3huxu@fb.com\nAbstract\nDomain-adaptive pre-training (or DA-training\nfor short), also known as post-training, aims\nto train a pre-trained general-purpose language\nmodel (LM) using an unlabeled corpus of a\nparticular domain to adapt the LM so that end-\ntasks in the domain can give improved per-\nformances. However, existing DA-training\nmethods are in some sense blind as they do\nnot explicitly identify what knowledge in the\nLM should be preserved and what should\nbe changed by the domain corpus. This pa-\nper shows that the existing methods are sub-\noptimal and proposes a novel method to per-\nform a more informed adaptation of the knowl-\nedge in the LM by (1) soft-masking the atten-\ntion heads based on their importance to best pre-\nserve the general knowledge in the LM and (2)\ncontrasting the representations of the general\nand the full (both general and domain knowl-\nedge) to learn an integrated representation with\nboth general and domain-specific knowledge.\nExperimental results will demonstrate the ef-\nfectiveness of the proposed approach.1\n1 Introduction\nPre-trained general-purpose language mod-\nels (LMs) like BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019), and GPT-3 (Brown\net al., 2020) have become a standard component\nin almost all NLP applications. Researchers have\nalso found that domain-adaptive pre-training (or\nDA-training for short) using an unlabeled corpus\nin a specific domain to adapt an LM can further\nimprove the end-task performance in the domain\n(Gururangan et al., 2020; Xu et al., 2019a,b;\nSun et al., 2019; Alsentzer et al., 2019). Note\nthat domain-adaptive pre-training is also called\npost-training (Xu et al., 2019a).\nExisting DA-training methods simply apply the\nsame pre-training objective, i.e., themask language\n∗Now at Google Research leishu@google.com\n1https://github.com/UIC-Liu-Lab/DGA\nmodel (MLM) loss, to further train an LM using\na domain corpus. These methods are sub-optimal\nbecause they do not explicitly identify what should\nbe preserved and what should be updated in the\nLM by the domain corpus.\nThis paper argues that a good DA-training\nmethod has two needs. On the one hand, the gen-\neral language knowledge learned in the LM should\nbe preserved as much as possible because the target\ndomain data is typically not large enough to be suf-\nficient to learn the general knowledge well. For ex-\nample, some words and their contexts may appear\ninfrequently in a particular domain. The knowledge\nabout them cannot be learned accurately based on\nthe domain data alone. When these words and con-\ntexts appear in an end-task, the system will have\ndifficulties. Thus, we need to rely on the knowledge\nabout them in the LM. Since existing DA-training\nupdates the LM with little guidance, such useful\ngeneral knowledge may be corrupted. On the other\nhand, due to polysemy (same word with different\nmeanings in different domains) and the fact that\ndifferent domains also have their special word us-\nages and contexts, the LM should be specialized or\nadapted to the target domain. A good DA-training\nshould balance these two needs to adapt the LM to\nthe target domain with minimal corruption to the\ngood general knowledge in the LM.\nThis paper proposes a novel technique to enable\na more informed adaptation to (1) preserve the gen-\neral knowledge in the LM as much as possible,\nand (2) update the LM to incorporate the domain-\nspecific knowledge of the target domain as needed.\nThe focus of the existing DA-training research has\nbeen on (2). As we argued above, (1) is also im-\nportant as focusing only on (2) may destroy some\nuseful general knowledge and produce sub-optimal\nresults for end-tasks. To achieve (1), the system\nshould constrain the gradient update of each atten-\ntion head2 based on its importance to the general\n2We will see in Sec. 4 that constraining the neurons in\n10177\nknowledge so that the general knowledge in LM\ncan be preserved as much as possible. With (1),\n(2) will be able to change the part of the general\nknowledge that needs to be updated to adapt the\nLM to suit the target domain.3\nIn this paper, we propose a novel model called\nDGA (DA-training - General knowledge preserva-\ntion and LM Adaptation) for the purpose. The key\nidea of the proposed method is to preserve the gen-\neral language knowledge in the LM while adapting\nthe LM to a specific domain. However, it is not\nobvious how this can be done, i.e., how to find\nthose parameters that are important for the general\nknowledge and how to protect them. This paper\nproposes a novel proxy-based method to achieve\nthe objectives. It works as follows. DGA first esti-\nmates the importance of each attention head in the\nLM via the newly proposed proxy KL-divergence\nloss (Sec. 3.1). This importance score reflects how\nimportant each attention head is to the general\nknowledge. Based on the importance scores, it\nperforms two key functions: The first function uses\nthe scores to soft-mask (rather than binary-mask or\ncompletely block) the gradient update to prevent\nimportant general knowledge in LM from being\nunnecessarily corrupted. This is related to prun-\ning of unimportant attention heads (Michel et al.,\n2019). However, pruning is not directly applica-\nble to DA-training as we will show in Sec. 2. The\nproposed soft-masking constrains only the back-\nward gradient flow in training. It is not necessary\nto soft-mask the forward pass in either training\nor inference. This is important because using the\nknowledge in the full network encourages maximal\nintegration of pre-trained general knowledge and\nthe target domain-specific knowledge. The second\nfunction contrasts the representation for the general\nknowledge in the LM and the full (including both\nthe general and the domain-specific) knowledge to\nlearn an integrated representation (Sec. 3.2).4\nIn summary, this paper makes two key contribu-\ntions.\n(1). It proposes the idea of informed adaptation\nto integrate the specialized knowledge in the target\nother layers is unnecessary.\n3This is very different from continual learning (CL) (Chen\nand Liu, 2018) as CL needs to preserve the past knowledge\nto deal with catastrophic forgetting (McCloskey and Cohen,\n1989). DA-training can and should change/adapt the general\nknowledge in the original LM to suit the target domain.\n4Contrasting the general and only the domain-specific\nknowledge gives poorer results (see Sec. 4.3) as it causes\nthe two types of knowledge to split rather than to integrate.\ndomain into the LM with minimal corruption to the\nuseful general knowledge in the original LM.\n(2). It proposes a new model DGA with two\nnovel functions to enable better DA-training. DGA\nestimates the attention head importance to protect\nthe important general knowledge in the LM and\nintegrates the specialized knowledge in the target\ndomain into the LM through contrasting the general\nand the full knowledge.\nTo the best of our knowledge, none of these has\nbeen reported in the literature before.\nExtensive experiments have been conducted in\n6 different domains and on 10 baselines to demon-\nstrate the effectiveness of the proposed DGA.\n2 Related Work\nDomain-adaptive pre-training (DA-training).\nResearchers have applied DA-training to many do-\nmains, e.g., reviews (Xu et al., 2019a,b), biomedi-\ncal text (Lee et al., 2020), news and papers (Guru-\nrangan et al., 2020), and social media (Chakrabarty\net al., 2019). However, they all use the same mask\nlanguage model (MLM) loss. We argue that it is\nsub-optimal and it is also important to preserve the\ngeneral knowledge in the LM as much as possible\nand integrate it with the target domain knowledge.\nNetwork pruning as importance computation. It\nis known that many parameters in a neural network\nare redundant and can be pruned (Li et al., 2021;\nLai et al., 2021). This has also been shown for\npre-trained Transformer (Chen et al., 2020a; Lin\net al., 2020; Gao et al., 2021b; Michel et al., 2019;\nV oita et al., 2019). A popular pruning method is\nto discard the parameters with small absolute val-\nues (Han et al., 2015; Guo et al., 2016). Other\nmethods prune the network at a higher level. In\na Transformer-based model, these include prun-\ning the attention head (Michel et al., 2019; V oita\net al., 2019; McCarley et al., 2019) and pruning\nsub-layers in a standard Transformer layer (Fan\net al., 2020; Sajjad et al., 2020). However, the\nabove methods are not directly applicable to us as\nwe need to compute the head importance for the\nLM using unlabeled domain data, while the above\napproaches are all for supervised end-tasks. We\npropose to use a proxy KL-divergence loss for our\npurpose. Note that it is possible to prune other sub-\nlayers in the Transformer. However, as shown in\nSec. 4.3, estimating the importance for other layers\ndoes not improve the performance.\nContrastive learning. Contrastive learning (Chen\n10178\net al., 2020b; He et al., 2020) can learn good repre-\nsentations by maximizing the similarity of positive\npairs and minimizes that of negative pairs:\nLcontrast = −log e(sim(qi,q+\ni )/τ)\n∑N\nj=1 e(sim(qi,q+\nj )/τ) , (1)\nwhere N is the batch size, τ is a temperature pa-\nrameter, sim(·) is a similarity metric, and qi and\nq+\ni are representations for positive pairs xi and x+\ni\n(typically, x+\ni is an augmented sample of xi, e.g.,\ngenerated via cropping, deletion or synonym re-\nplacement (Gao et al., 2021a)). In the unsupervised\ncontrastive loss, the negative samples are the other\nsamples in the batch, indicated in the denominator.\nWe mainly use contrasive loss to contrast the\nrepresentations of the important general knowledge\nin the original LM and the full knowledge (both the\ngeneral and domain-specific knowledge) to achieve\na good integration of the general knowledge and\nthe domain specific knowledge.\n3 Proposed DGA System\nAs discussed earlier, DGA goes beyond the MLM\nloss to perform two more functions: (1) preserv-\ning the important general knowledge in the LM\nby soft-masking the attention heads based on their\nimportance. This helps avoid potential corruption\nof the general knowledge in the LM in DA-training\n(Sec. 3.1). However, the challenge is how to iden-\ntify the general knowledge in the LM and how to\nprotect it. We will propose a method to do that.\n(2) encouraging the model to learn integrated rep-\nresentations of the target domain and the general\nknowledge in the LM (Sec. 3.2). It is also not obvi-\nous how this can be done. We propose a contrastive\nlearning based method to do it. Figure 1 gives an\noverview of DGA.\n3.1 Preserving General Knowledge by\nSoft-Masking Attention Heads\nMulti-head attention. Multi-head attention is ar-\nguably the most important component in the Trans-\nformer model (Vaswani et al., 2017). We omit\ndetails of other parts and refer the reader to the\noriginal paper. Formally, let x = x(1),...,x (T) be\na sequence of T real vectors where x(t) ∈Rd and\nlet q∈Rd be a query vector. The attention mecha-\nnism is defined as\natt(x,q) =Wo\nT∑\nt=1\nα(t)(q)Wvx(t), (2)\nFigure 1: Illustration of DGA.(A) shows the importance\ncomputation. This is done by adding a gate vector gl\nmultiplying with the multi-head attention (Eq. 5) and\naveraging its training gradients (Eq. 6). (B) shows DGA\ntraining. In backward pass, attention heads are soft-\nmasked based on their importance I (Eqs. 9 and 10)\nto try to preserve the general knowledge in the LM as\nmuch as possible. In forward pass, the added gate vector\nis removed except for feature learning in the contrastive\nloss. The contrastive loss is computed by contrasting\nthe general knowledge with importance (ogen in Eq. 12)\napplied and the full knowledge without applying the\nimportance (ofull in Eq. 14). The final objective of DGA\nconsists of MLM loss and contrastive loss. Note that we\nomit the details of other parts of Transformer and only\nfocus on the multi-head attention mechanism.\nwhere\nα(t)(q) =softmax(qTWT\nq Wkx(t)\n√\nd\n). (3)\nThe projection matrices Wo,Wv,Wq,Wk ∈Rd×d\nare learnable parameters. The query vector is from\nthe same sequence as x in self-attention. A Trans-\nformer contains Lidentical layers. For layer l, Hl\ndifferent attention heads are applied in parallel to\nenable the Transformer to be trained on more data.\nSimply put, multi-head attention (mhatt) is the si-\nmultaneous application of multiple attention heads\nin a single Transformer architecture. They are then\napplied in parallel to obtain multi-head attention.5\nmhattl(x,q) =\nHl∑\nh=1\nattlh(x,q), (4)\n5We follow the notation in (Michel et al., 2019), where the\nnotation in Eq. 4 is equivalent to the “concatenation” formula-\ntion in (Vaswani et al., 2017).\n10179\nwhere hindicates the hth attention head. Note that\nthe input x is different in each layer since the input\nof a given layer is the output of last layer. To ease\nthe notation, we use the input x for all layers.\nHead importance. Researchers have found that\nnot all attention heads are important (Michel et al.,\n2019). We introduce a gate vector, gl, where each\ncell is a gate variable, glh, to the attention head\nsummation for detecting the importance of atten-\ntion heads. The resulting importance scores are\nused to soft-mask the heads in DA-training.\ngmhattl(x,q) =\nHl∑\nh=1\nglh,⊗attlh(x,q) (5)\nwhere ⊗is the element-wise multiplication. A\ngradient-based head importance detection method\nis proposed in (Michel et al., 2019). Given a dataset\nD = {(ym,xm)}M\nm=1 of M samples (ym is the\nlabel of xm as Michel et al. (2019) worked on\nsupervised learning), the importance of a head is\nestimated with a gradient-based proxy score\nIlh = 1\nM\nM∑\nm=1\n|∇glh|, (6)\nwhere ∇glh is the gradient of gate variable glh,\n∇glh = ∂Limpt(ym,xm)\n∂glh\n, (7)\nwhere Limpt is a task-specific/domain-specific loss\nfunction. The gradient can be used as the impor-\ntance score because changing glh is liable to have\na large effect on the model if Ilh has a high value.\nAlthough Eq. 6 offers a way to compute the\nimportance of attention heads w.r.t. a given loss\nLimpt, we are unable to directly apply it: If we\nuse the domain data at hand and the MLM loss\nas Limpt, ∇glh only indicates the importance score\nfor domain-specific knowledge. However, our goal\nis to estimate the attention heads importance for\nthe general knowledge in LM which requires the\ndata used in training the LM to compute the Limpt.\nIn practice, such data is not accessible to users of\nthe LM. Further, label is needed in Eq. 6 but our\ndomain corpus is unlabeled in DA-training. To ad-\ndress these issues, we propose to compute a proxy\nKL-divergence loss for Limpt.\nProxy KL-divergence loss. We need a proxy for\nLimpt such that its gradient ( ∇glh) can be used to\ncompute head importance without using the LM’s\noriginal pre-training data. We propose to use model\nrobustness as the proxy, i.e., we try to detect heads\nthat are important for LM’s robustness. Its gradi-\nent, ∇glh, then indicates the robustness and thus\nthe importance to the LM model. Our rationale\nis as follows: If an Ilh (the average of |∇glh|, see\nEq. 6) has a high value, it indicates that it is impor-\ntant to the LM’s robustness because its change can\ncause the LM to change a great deal. It is thus an\nimportant head to the LM. In contrast, if Ilh has a\nsmall value, it is a less or not important head to the\nLM.\nTo compute the robustness of the LM, we take\na subset (a hyper-parameter) of the target do-\nmain data {xsub\nm }(no label in DA-training) and\ninput xsub\nm twice to the LM and compute the KL-\ndivergence of the two resulting representations,\nLimpt = KL(f1(xsub\nm ),f2(xsub\nm )), (8)\nwhere f1 and f2 are the LM with different dropout\nmasks. Note that we don’t need to add any addi-\ntional dropouts to implement f because indepen-\ndently sampled dropout masks are used as input in\nthe Transformer. In training a Transformer, there\nare dropout masks placed on fully-connected layers\nand attention probabilities. Thus, simply feeding\nthe same input to the Transformer twice will get\ntwo representations with different dropout masks.\nSince dropout is similar to adding noise, the dif-\nference between the two representations can be re-\ngarded as the robustness of the Transformer model.\nFigure 1 (A) shows how we compute the impor-\ntance of each attention head using the gradient of\nthe gate vector gl.\nSoft-masking attention heads in DA-training.\nRecall we want to preserve the general knowledge\nin the LM during DA-training using head impor-\ntance Ilh. Given the attention head att(x,q) and\nDA-training loss LDA-train (typically the MLM loss;\nwe also propose an additional loss in Sec. 3.2),\nwe can “soft mask” its corresponding gradient\n(∇attlh\n6) using the head importance value Ilh,\n∇′\nattlh = (1−Inorm\nlh ) ⊗∇attlh, (9)\nwhere Inorm\nlh is from Ilh via normalization\nInorm\nlh = |Tanh(Normalize(Ilh))|. (10)\nNormalize makes the Ilh have a mean of 0 and\nstandard deviation of 1. Absolute value of Tanh\n6∇attlh indicates the gradient for attention headattlh(x,q),\ndistinguished from ∇glh in Eq. 6 which is the gradient for the\ngate variable glh\n10180\nensures that Ilh takes values in the interval [0,1].\nEq. 9 means to constrain the gradient of the corre-\nsponding head attlh(x,q) by element-wise multi-\nplying one minus the head importance to the gra-\ndient. It is “soft-masking” because Ilh is a real\nnumber in [0,1] (instead of binary {0, 1}), which\ngives the model the flexibility to adjust the attention\nhead. This is useful because although some heads\nare important to the LM, they may conflict with\nthe knowledge in the target domain and thus need\nadjusting. Also note that the soft masks here affect\nonly the backward pass and are not used in forward\npass (so that forward pass can use the full network\nand encourage maximal integration of pre-trained\ngeneral and domain-specific knowledge) except for\nfeature learning using contrastive learning (see be-\nlow). Figure 1 (B) shows that attention heads are\nsoft-masked during training.\n3.2 Contrasting General and Full Knowledge\nWe now present how to integrate the general knowl-\nedge in the LM and the domain-specific knowl-\nedge in the target domain by contrasting the general\nknowledge and the full knowledge (both general\nand domain-specific). We first introduce how we\nobtain such knowledge from the LM for the input\nx, and then discuss how we contrast them.\nObtaining the general knowledge for the in-\nput sequence x from the LM is by extracting the\nrepresentation of combining the attention heads\nand their importance scores ( Inorm\nlh in Eq. 10) in\nthe forward pass. The intuition is that since the\nimportance scores show how important each atten-\ntion head is to the general knowledge, the resulting\nrepresentation reflects the main general knowledge\nused by x. Formally, we plug Inorm\nlh (soft-masks)\nas the gate variable glh in Eq. 5,\ngmhattgen\nl (x,q) =\nHl∑\nh=1\nInorm\nlh ⊗attlh(x,q). (11)\nGiven the attention heads for general knowledge,\nwe can plug it into the whole Transformer to obtain\nthe final general knowledge (taking the average of\neach token’s output in the input sequence).\nogen = Transformer(gmhattgen(x,q)). (12)\n(See ogen also in Figure 1 (B)).\nObtaining the full (both general and domain-\nspecific) knowledge in x is similar. The only dif-\nference is that we extract the representation of x\nwithout applying the importance (soft-masks) on\nattention heads in the forward pass,\ngmhattfull\nl (x,q) =\nHl∑\nh=1\nattlh(x,q). (13)\nSimilarly, we can plug it into the Transformer,\nofull = Transformer(gmhattfull(x,q)). (14)\n(See ofull also in Figure 1 (B)). Note that it is possi-\nble to use (1 −Inorm\nlh ) as the importance of domain-\nspecific knowledge and contrast it with the general\nknowledge. However, this produces poorer results\n(see Table 3) as explained in footnote 4.\nContrasting general and full knowledge. It\nis known that contrastive learning helps learn a\ngood isotropic representation that is good for down-\nstream tasks, with the help of positive and nega-\ntive instances. We contrast the general (ogen) and\nfull (ofull) representations (as positive and nega-\ntive instances) for the same input x to make them\ndifferent, which encourages the learning of domain-\nspecific knowledge in ofull that is not already in the\ngeneral knowledge and yet related to and integrated\nwith the general knowledge (ogen) of the input.\nWe construct contrastive instances as follows:\nfor an input xm, three contrastive instances are pro-\nduced. Anchor om and positive instance o+\nm are\nboth full knowledge from Eq. 14, obtained based\non two independently sampled dropout masks in\nthe Transformer (recall that this can be achieved by\ninputting xm twice (see Sec. 3.1). We regard o+\nm\nand om as positive instances because the dropout\nnoise has been shown to be good positive instances\nfor improving alignment in training sentence em-\nbedding (Gao et al., 2021a). Negative instance o−\nm\nis the general knowledge for xm from the LM ob-\ntained via Eq. 12. With om, o+\nm, and o−\nm, our\ncontrastive loss is (sim(·) is the cosine similarity),\nLcontrast = −log esim(om,o+\nm)/τ\n∑N\nj=1(esim(om,o+\nj )/τ + esim(om,o−\nj )/τ)\n.\n(15)\nCompared to Eq. 1, the second term is added in\nthe denominator, i.e., general knowledge represen-\ntations as additional negative samples/instances.\nFigure 1 (B) shows a red arrow pointed from ofull\nto itself, indicating the positive instances are from\ninputting twice. The dashed red arrow pointing\nto ogen indicates the negative instances contrasting\nthe specialized and general knowledge.\n10181\nUnlabeled Domain Datasets End-Task Classification Datasets\nSource Dataset Size Dataset Task #Training #Testing #Classes\nReviews\nYelp Restaurant 758MBRestaurant Aspect Sentiment Classification (ASC) 3,452 1,120 3\nAmazon Phone 724MBPhone Aspect Sentiment Classification (ASC) 239 553 2\nAmazon Camera 319MBCamera Aspect Sentiment Classification (ASC) 230 626 2\nAcademic Papers\nACL Papers 867MB ACL Citation Intent Classification 1,520 421 6\nAI Papers 507MB AI Relation Classification 2,260 2,388 7\nPubMed Papers 989MBPubMed Chemical-protein Interaction Prediction 2,667 7,398 13\nTable 1: Statistics for domain post-training datasets and end task supervised classification datasets (more detail of\neach task is given in Appendix A).\n3.3 DGA Objectives\nDGA is a pipelined model: First, a subset of the\ndomain data is used to estimate the attention head\nimportance (Ilh in Sec. 3.1). Second, given the\nattention head importance, we compute the final\ndomain-adaptive loss by combining the conven-\ntional Masked Language Model (MLM) loss (in-\nclude the proposed soft-masking for general knowl-\nedge) and the proposed contrastive loss:\nLDA-train = LMLM + λ1Lcontrast, (16)\nwhere λ1 is the hyper-parameter to adjust the im-\npact of the added term.\n4 Experiments\nWe follow the experiment setup in (Gururangan\net al., 2020). RoBERTa (Liu et al., 2019)7 is used\nas the LM. In each experiment, we first DA-train\nthe LM and then fine-tune it on the end-task. The\nfinal evaluation is based on the end-task results.\n4.1 Datasets and Baselines\nDatasets: Table 1 shows the statistics of the un-\nlabeled domain datasets for DA-training and their\ncorresponding end-task classification datasets. We\nuse 6 unlabeled domain datasets:8 3 of them are\nabout reviews: Yelp Restaurant (Xu et al., 2019a),\nAmazon Phone (Ni et al., 2019), Amazon Camera\n(Ni et al., 2019); 3 of them are academic papers:\nACL Papers (Lo et al., 2020), AI Papers (Lo et al.,\n2020), and PubMed Papers9. Each unlabeled do-\nmain dataset has a corresponding end-task classifi-\n7https://huggingface.co/roberta-base\n8We down-sampled the PubMed due to its huge original\nsize. In general, our datasets are much smaller comparing to\nprevious work (Gururangan et al., 2020) (which used more\nthan 11GB of data for each domain). Our experiments showed\nthat a smaller dataset is sufficient and more data does not help.\nIt also requires much less computation resource.\n9https://pubmed.ncbi.nlm.nih.gov/\ncation dataset10: Restaurant11 (Xu et al., 2019a),\nPhone (Ding et al., 2008; Hu and Liu, 2004), Cam-\nera (Ding et al., 2008; Hu and Liu, 2004)12, ACL\n(ACL-ARC in Jurgens et al. (2018)), AI (SCIERC\nin Luan et al. (2018)), and PubMed (CHEMPORT\nin Kringelum et al. (2016)).\nBaselines. We consider 10 baselines.\n(1). Non-DA-training (RoBERTa) (Liu et al.,\n2019) uses the original RoBERTa for the end-task\nfine-tuning without any DA-training.\n(2). DA-training using masked language\nmodel loss (MLM) is the existing DA-training\nmethod. To our knowledge, existing DA-training\nsystems are all based on the MLM loss.\n(3). DA-training using adapter-tuning (MLM\n(Adapter)) adds adapter layers between layers of\nTransformer for DA-training. An adapter (Houlsby\net al., 2019) has two fully connected layers and a\nskip connection. During DA-training, the Trans-\nformer is fixed, only the adapters are trained. The\nbottleneck (adapter) size is set to 64 (Houlsby et al.,\n2019). During end-task fine-tuning, both RoBERTa\nand adapters are trainable for fair comparison.\n(4). DA-training using prompt-tuning (MLM\n(Prompt)) (Lester et al., 2021) adds a sequence of\nprompt tokens to the end of the original sequence.\nIn DA-training, RoBERTa (the LM) is fixed and\nonly the prompt tokens are trained. In end-task\nfine-tuning, both LM and the trained prompt are\ntrainable. We initialize 100 tokens and set the learn-\ning rate of the prompt token to 0.3 in DA-training,\nfollowing the setting in Lester et al. (2021).\n(5). Knowledge distillation (MLM+KD) (Hin-\n10Note that our results are different from those presented\nin Table 5 of (Gururangan et al., 2020) because we observe\nvery high variances due to very small original test sets and\nthus re-partition the training and test set (by enlarging the test\nset and reducing the training set slightly)\n11To be consistent with existing research (Tang et al., 2016),\nexamples with conflict polarities (both positive and negative\nsentiments are expressed about an aspect term) are not used.\n12Note that Ding et al. (2008) and Hu and Liu (2004) con-\ntain 9 and 5 domains, respectively. We extract those domains\nrelated to “Phone” and “Camera” from them.\n10182\nton et al., 2015) minimizes the representational\ndeviation between the general knowledge in LM\nand the specialized knowledge in DA-training. We\ncompute the KL divergence between the representa-\ntions (the output before the masked language model\nprediction head) of each word of the two models\n(LM and DA-trained) as the distillation loss.\n(6). Adapted distillation through attention\n(MLM+AdaptedDeiT) is derived from DeiT (Tou-\nvron et al., 2021), a distillation method for visual\nTransformer (ViT) (Dosovitskiy et al., 2020). We\nadapt DeiT to a text-based and unsupervised model\nby distilling the LM representation13 to the added\ndistillation token and change ViT to RoBERTa.\n(7, 8). DA-training using sequence-level\ncontrastive learning (MLM+SimCSE and\nMLM+InfoWord). SimCSE is a contrastive learn-\ning method for sentence embedding (Gao et al.,\n2021a). We use its unsupervised version where\npositive samples are from the same input with\ndifferent dropout masks and negative samples are\nother instances in the same batch. InfoWord (Kong\net al., 2020) is another contrastive learning method\ncontrasts the span-level local representation and\nsequence-level global representation.\n(9, 10). DA-training using token-aware\ncontrastive learning (MLM+TaCL and\nMLM+TaCO). TaCL (Su et al., 2021) and\nTaCO (Fu et al., 2022) are two recent methods\nto improve BERT pre-training with token-aware\ncontrastive loss.14 We change the backbone to\nRoBERTa for fair comparison.\n4.2 Implementation Detail\nArchitecture. We adopt RoBERTaBASE as our\nbackbone LM (12 layers and 12 attention heads\nin each layer). A masked language model head is\napplied for DA-training. The end-task fine-tuning\nof RoBERTa follows the standard practice. For\nthe three ASC tasks (see Table 1), we adopt the\nASC formulation in (Xu et al., 2019a), where the\naspect (e.g., “ sound”) and review sentence (e.g.,\n“The sound is great”) are concatenated via </s>.\nHyperparameters. Unless otherwise stated, the\nsame hyper-parameters are used in all experiments.\nThe maximum input length is set to 164 which is\nlong enough for all datasets. Adam optimizer is\n13We take the average of its token’s output as sequence’s\nrepresentation. The same for SimCSE baseline.\n14TaCL and TaCO are not a DA-training model. It pre-trains\nan LM to improve it using the same data as that for training\nthe LM. We switch the data to our target domain data.\nused for both DA-training and end-task fine-tuning.\nThe max sequence length is set to 164, which is\nlong enough for our end-tasks and only needs mod-\nerate computational resources.\nDomain-adaptive pre-training (DA-training).\nThe learning rate is set to 1e-4 and batch size is\n256. We train 2.5K steps for each domain, roughly\na full pass through the domain data, following (Gu-\nrurangan et al., 2020; Xu et al., 2019a). The subset\nof data {xsub\nm }for computing Limpt to determine\nhead importance in Sec. 3.1 is set to 1.64 Million\ntokens, which is sufficient in our experiments. λ1\nin Eq. 16 is set to 1 and τ in Eq. 15 is set to 0.05.\nEnd-task fine-tuning. The learning rate is set\nto 1e-5 and batch size to 16. We train on end-task\nfine-tuning datasets for 5 epochs for Restaurant; 10\nepochs for ACL, AI and PubMed; and 15 epochs\nfor Phone and Camera. We simply take the results\nfor the last epoch as we empirically found that\nthe above number of epochs gives us stable and\nconvergence results.\n4.3 Evaluation Results and Ablation Study\nWe report the end-task results of the 10 baselines\non the 6 datasets in Table 2.\nSuperiority of DGA. Our DGA consistently out-\nperforms all baselines. Thanks to the proposed\nmore informed adaptation, DGA improves over\nthe widely used traditional DA-training baseline\nMLM. We also see that MLM markedly outper-\nforms RoBERTa (non-DA-training) on average (see\nthe last column). We discuss more observations\nabout the results bellow.\n(1). Training the entire LM in DGA helps\nachieve much better results. Using adapter (MLM\n(adapter)) and prompt (MLM (prompt)) have mixed\nresults. This is because adapter and prompt do not\nhave sufficient trainable parameters, which are also\nrandomly initialized and can be difficult to train.\n(2). DGA is also better than distillation-based\nsystems: MLM+AdaptedDeiT and MLM+KD,\nwhich try to preserve the past knowledge. This\nis not surprising because the goal of DA-training is\nnot simply preserving the previous knowledge but\nalso to adapt/change it as needed to suit the target\ndomain. DGA is specifically designed for this with\nsoft-masking and contrasting of knowledge.\n(3). The contrastive learning in DGA is more\neffective than the other contrastive alternatives\n(MLM+SimCSE, MLM+TaCL, MLM+TaCO and\nMLM+InfoWord). This indicates contrasting the\n10183\nDomain Camera Phone Resturant AI ACL PubMed AvgModel MF1 Acc. MF1 Acc. MF1 Acc. MF1 Acc. MF1 Acc. Micro-F1\nRoBERTa 78.82 87.03 83.75 86.08 79.81 87.00 60.98 71.85 66.11 71.26 72.38 73.64\nMLM 84.39 89.90 82.59 85.50 80.84 87.68 68.97 75.95 68.75 73.44 72.84 76.40\nMLM (Adapter) 83.62 89.23 82.71 85.35 80.19 87.14 60.55 71.38 68.87 72.92 71.68 74.60\nMLM (Prompt) 85.52 90.38 84.17 86.53 79.00 86.45 61.47 72.36 66.66 71.35 73.09 74.98\nMLM+KD 82.79 89.30 80.08 83.33 80.40 87.25 67.76 75.46 68.19 72.73 72.35 75.26\nMLM+AdaptedDeiT86.86 91.37 83.08 85.64 79.70 86.84 69.72 76.83 69.11 73.35 72.69 76.86\nMLM+SimCSE 84.91 90.35 83.46 86.08 80.88 87.59 69.10 76.25 69.89 74.30 72.77 76.84\nMLM+TaCL 81.98 88.88 81.87 84.92 81.12 87.50 64.04 73.18 63.18 70.31 69.46 73.61\nMLM+TaCO 84.50 90.22 82.63 85.32 79.27 86.68 59.73 71.22 63.66 70.36 72.38 73.69\nMLM+InfoWord 87.95 91.92 84.58 86.84 81.24 87.82 68.29 75.92 68.58 73.68 73.21 77.31\nDGA 88.52 92.49 85.47 87.45 81.83 88.20 71.99 78.06 71.01 74.73 73.65 78.74\nTable 2: We report the macro-F1 (MF1) and accuracy results for all datasets, except for CHEMPORT in the PubMed\ndomain, for which we use micro-F1 following Gururangan et al. (2020); Dery et al. (2021); Beltagy et al. (2019).\nThe results are averages of 5 random seeds (the standard deviation is reported in Appendix B). The average column\n(Avg) is the average over the MF1 (or Micro-F1 for PubMed) for all datasets.\ngeneral and full knowledge for knowledge integra-\ntion is important.\nEffectiveness of the proxy KL-divergence loss.\nWe use the proposed proxy KL-divergence loss to\ncompute the head importance to identify the gen-\neral language knowledge in the LM without using\nthe LM’s original pre-training data (Sec. 3.1).\nFor evaluation, we are interested in how good\nthe proxy is. Since we don’t have the data that pre-\ntrains RoBERTa, it is not obvious how to assess\nthe quality of the proxy directly. Here, we provide\nsome indirect evidences to show the effectiveness\nof the proxy for computing the importance of units\nto the general knowledge in the LM.\nWe conduct a separate experiment to compare\nthe attention heads’ importance score vectors after\napplying the proxy using the data from different\ndomains. For each domain i, we compare its impor-\ntance vector with the importance vector of every\nother domain, and then average the cosine similari-\nties to get the value for domain i. We get 0.92 for\nRestaurant, the same 0.91 for ACL, AI, and Phone,\n0.89 for PubMed and 0.92 for Camera. We see that\ndifferent domains give similar importance values,\nwhich indirectly show that our proxy can identify\nthe common general knowledge.\nWe also compute the importance score distribu-\ntions of the proxy. For each of the 6 domains, after\napplying the proxy, around 20% of the attention\nheads are heavily protected ( 0.8 ≤Inorm\nlh ≤1.0)\nand another 20% moderately protected ( 0.6 ≤\nInorm\nlh < 0.8), which indicate the general knowl-\nedge. While Phone, AI, Camera and Restaurant\nshare a similar distribution, ACL and PubMed pro-\ntect slightly less. This is understandable as PubMed\nand ACL (medical or NLP publications) are prob-\nably less common than the other domains and the\ngeneral knowledge in the LM covers them less.\nAblation study. To better understand DGA, We\nwant to know (1) whether constraining the neurons\nin other layers are helpful (the proposed DGA only\nconstrains the attention heads), and (2) where the\ngain of DGA is from. To answer (1), we constrain\nthe training of different layers in a standard Trans-\nformer. In Table 3 (rows 3-5), “ H”, “I”, and “O”\nrefer to attention head, intermediate layer, output\nlayer in a standard Transformer layer, respectively.\n“E” refers to the embedding layers. The brack-\nets with combination of “H, I, O, E” indicate the\nlocation we apply the soft-masking (DGA only ap-\nplies soft-masking in the attention head). We can\nsee their results are similar or worse than DGA,\nimplying that attention heads are more indicative\nof important knowledge. To answer (2), we con-\nduct the following ablation experiments: (i) DGA\n(w/o contrast), without the contrastive loss, but\nonly soft-masking the backward pass according\nto the attention head importance. (ii) DGA (ran-\ndom masking) with randomly generated attention\nhead importance scores and using them to do soft-\nmasking and contrastive learning. (iii) Ensemble\n(LM+MLM) performs the end-task fine-tuning on\nboth the MLM DA-trained RoBERTa (conventional\nDA-training) and the original RoBERTa (LM) by\nconcatenating their outputs and taking the average.\n(iv) DGA (domain-specific) refers to the variant\nthat contrasts domain-specific and general knowl-\nedge (see Sec. 3.2).15\n15We don’t have DGA(w/o soft-masking) because our con-\n10184\nDomain Camera Phone Resturant AI ACL PubMed AvgModel MF1 Acc. MF1 Acc. MF1 Acc. MF1 Acc. MF1 Acc. Micro-F1\nRoBERTa 78.82 87.03 83.75 86.08 79.81 87.00 60.98 71.85 66.11 71.26 72.38 73.64\nMLM 84.39 89.90 82.59 85.50 80.84 87.68 68.97 75.95 68.75 73.44 72.84 76.40\nDGA (H, I) 86.79 91.60 84.21 86.40 81.32 87.91 71.07 77.36 69.50 73.82 73.34 77.71\nDGA (H, I, O) 88.04 92.01 85.85 87.63 81.45 87.79 71.54 77.61 70.52 74.58 73.10 78.42\nDGA (H, I, O, E) 87.05 91.60 83.74 86.11 80.64 87.61 72.64 78.17 71.24 74.96 73.54 78.14\nDGA (w/o contrast)86.19 90.89 84.48 86.65 81.70 87.93 68.25 75.49 69.31 73.73 72.72 77.11\nDGA (random mask)82.07 89.30 83.86 86.33 80.60 87.52 69.51 76.64 69.59 73.73 72.92 76.43\nEnsemble (LM+MLM)85.22 90.64 85.15 87.23 79.86 86.98 65.10 74.43 68.56 73.44 72.60 76.08\nDGA (domain-specific)88.06 92.04 83.45 85.82 81.72 87.90 68.00 75.57 70.91 75.06 73.17 77.55\nDGA 88.52 92.49 85.47 87.45 81.83 88.20 71.99 78.06 71.01 74.73 73.65 78.74\nTable 3: Ablation results - averages of 5 random seeds. The standard deviations are reported in Appendix B.\nTable 3 shows that the full DGA always gives the\nbest result, indicating every component contributes.\nAdditional observations are as follows:\n(1) DGA’s gain is partially from the novel soft-\nmasking: we can see that on average, DGA (w/o\ncontrast) outperforms conventional DA-training\n(MLM). Besides, our gradient-based mask is infor-\nmative: we can see DGA (random mask) is worse\nthan DGA (w/o contrast) on all datasets. DGA (w/o\ncontrast) is even better than Ensemble, which di-\nrectly combines the information given by both the\noriginal LM and the traditional DA-trained model\nduring end-task fine-tuning\n(2) Besides soft-masking, contrasting the gen-\neral and full knowledge also helps. We can see\nDGA outperforms DGA (w/o contrast) and DGA\n(domain-specific) in all datasets.\n5 Conclusion\nThis paper argued that an effective DA-training\nmethod should effectively integrate the target do-\nmain knowledge to the general knowledge in the\nLM. Existing approaches do not explicitly do this.\nThis paper proposed a novel method DGA to\nachieve it (1) by estimating the attention heads\nimportance in LM and using the importance scores\nto soft-mask the attention heads in DA-training to\npreserve the important knowledge in LM as much\nas possible, and (2) by contrasting the general and\nthe full knowledge. Extensive experiment results\ndemonstrated the effectiveness of the proposed ap-\nproach DGA.\n6 Limitations\nWhile effective, DGA has some limitations. First,\nthe main focus of DGA is to adapt an LM to a\ntrastive learning relies on soft-masking. If removed, con-\ntrastive loss will not have the additional negative samples and\nour DGA becomes MLM+SimCSE.\ngiven target domain. It does not consider the gen-\neralization to other domains. For example, it will\nbe interesting to incrementally or continually adapt\nan LM to more and more domains to make the LM\nmore useful. Second, the importance of parame-\nters for general knowledge in the LM is computed\nusing a proxy method based on model robustness.\nAlthough it is quite effective, it is interesting to\nexplore other approaches to further improve it. We\nwill work on these in our future work as specializ-\ning and improving an LM is an important problem.\nAcknowledgments\nThe work of Zixuan Ke and Bing Liu was sup-\nported in part by three National Science Founda-\ntion (NSF) grants (IIS-1910424, IIS-1838770, and\nCNS-2225427).\nReferences\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019. Publicly available clinical bert\nembeddings. arXiv preprint arXiv:1904.03323.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT:\nA pretrained language model for scientific text.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems.\nTuhin Chakrabarty, Christopher Hidey, and Kathleen\nMcKeown. 2019. Imho fine-tuning improves claim\ndetection. arXiv preprint arXiv:1905.07000.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia\nLiu, Yang Zhang, Zhangyang Wang, and Michael\nCarbin. 2020a. The lottery ticket hypothesis for pre-\ntrained bert networks. Advances in neural informa-\ntion processing systems, 33:15834–15846.\n10185\nTing Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey Hinton. 2020b. A simple framework for\ncontrastive learning of visual representations. In In-\nternational conference on machine learning, pages\n1597–1607. PMLR.\nZhiyuan Chen and Bing Liu. 2018. Lifelong machine\nlearning. Synthesis Lectures on Artificial Intelligence\nand Machine Learning, 12(3):1–207.\nLucio M Dery, Paul Michel, Ameet Talwalkar, and Gra-\nham Neubig. 2021. Should we be pre-training? an\nargument for end-task aware training as an alternative.\narXiv preprint arXiv:2109.07437.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT.\nXiaowen Ding, Bing Liu, and Philip S Yu. 2008. A\nholistic lexicon-based approach to opinion mining.\nIn Proceedings of the 2008 international conference\non web search and data mining.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2020.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint\narXiv:2010.11929.\nAngela Fan, Edouard Grave, and Armand Joulin. 2020.\nReducing transformer depth on demand with struc-\ntured dropout. In International Conference on Learn-\ning Representations.\nZhiyi Fu, Wangchunshu Zhou, Jingjing Xu, Hao Zhou,\nand Lei Li. 2022. Contextual representation learning\nbeyond masked language modeling. In ACL.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021a.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. arXiv preprint arXiv:2104.08821.\nYang Gao, Nicolo Colombo, and Wei Wang. 2021b.\nAdapting by pruning: A case study on bert. arXiv\npreprint arXiv:2105.03343.\nYiwen Guo, Anbang Yao, and Yurong Chen. 2016. Dy-\nnamic network surgery for efficient dnns. Advances\nin neural information processing systems, 29.\nSuchin Gururangan, Ana Marasovic, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nACL.\nSong Han, Jeff Pool, John Tran, and William Dally.\n2015. Learning both weights and connections for\nefficient neural network. Advances in neural infor-\nmation processing systems, 28.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss Girshick. 2020. Momentum contrast for un-\nsupervised visual representation learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 9729–9738.\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2(7).\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In\nICML.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of ACM\nSIGKDD.\nDavid Jurgens, Srijan Kumar, Raine Hoover, Daniel A.\nMcFarland, and Dan Jurafsky. 2018. Measuring the\nevolution of a scientific field through citation frames.\nTACL.\nLingpeng Kong, Cyprien de Masson d’Autume, Lei Yu,\nWang Ling, Zihang Dai, and Dani Yogatama. 2020.\nA mutual information maximization perspective of\nlanguage representation learning. In ICLR.\nJens Kringelum, Sonny Kim Kjaerulff, Søren Brunak,\nOle Lund, Tudor I Oprea, and Olivier Taboureau.\n2016. Chemprot-3.0: a global chemical biology dis-\neases mapping. Database, 2016.\nCheng-I Jeff Lai, Yang Zhang, Alexander H Liu, Shiyu\nChang, Yi-Lun Liao, Yung-Sung Chuang, Kaizhi\nQian, Sameer Khurana, David Cox, and Jim Glass.\n2021. Parp: Prune, adjust and re-prune for self-\nsupervised speech recognition. NeurIPS, 34.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In EMNLP.\nJiaoda Li, Ryan Cotterell, and Mrinmaya Sachan. 2021.\nDifferentiable subset pruning of transformer heads.\nTransactions of the Association for Computational\nLinguistics, 9:1442–1459.\nZi Lin, Jeremiah Zhe Liu, Zi Yang, Nan Hua, and Dan\nRoth. 2020. Pruning redundant mappings in trans-\nformer models via spectral-normalized identity prior.\narXiv preprint arXiv:2010.01791.\nBing Liu. 2015. Sentiment analysis: Mining opinions,\nsentiments, and emotions . Cambridge University\nPress.\n10186\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel S. Weld. 2020. S2ORC: the semantic\nscholar open research corpus. In ACL.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identification of entities,\nrelations, and coreference for scientific knowledge\ngraph construction. In ACL.\nJS McCarley, Rishav Chakravarti, and Avirup Sil. 2019.\nStructured pruning of a bert-based question answer-\ning model. arXiv preprint arXiv:1910.06360.\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learn-\ning and motivation.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? Advances\nin neural information processing systems, 32.\nJianmo Ni, Jiacheng Li, and Julian J. McAuley. 2019.\nJustifying recommendations using distantly-labeled\nreviews and fine-grained aspects. In EMNLP, pages\n188–197. Association for Computational Linguistics.\nHassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav\nNakov. 2020. On the effect of dropping layers\nof pre-trained transformer models. arXiv preprint\narXiv:2004.03844.\nYixuan Su, Fangyu Liu, Zaiqiao Meng, Lei Shu, Ehsan\nShareghi, and Nigel Collier. 2021. Tacl: Improving\nbert pre-training with token-aware contrastive learn-\ning. arXiv preprint arXiv:2111.04198.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to fine-tune bert for text classification? In\nChina national conference on Chinese computational\nlinguistics, pages 194–206. Springer.\nDuyu Tang, Bing Qin, and Ting Liu. 2016. Aspect level\nsentiment classification with deep memory network.\nIn EMNLP.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Hervé\nJégou. 2021. Training data-efficient image trans-\nformers & distillation through attention. In Inter-\nnational Conference on Machine Learning , pages\n10347–10357. PMLR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-\nhead self-attention: Specialized heads do the heavy\nlifting, the rest can be pruned. arXiv preprint\narXiv:1905.09418.\nHu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2019a.\nBERT post-training for review reading comprehen-\nsion and aspect-based sentiment analysis. In NAACL-\nHLT.\nHu Xu, Bing Liu, Lei Shu, and Philip S Yu. 2019b. Re-\nview conversational reading comprehension. arXiv\npreprint arXiv:1902.00821.\n10187\nDomain Camera Phone Resturant AI ACL PubMed\nModel MF1 Acc. MF1 Acc. MF1 Acc. MF1 Acc. MF1 Acc. Micro-F1\nRoBERTa ±0.0403±0.0179±0.0210±0.0154±0.0117±0.0049±0.0646±0.0347±0.0192±0.0096±0.0071\nMLM ±0.0479±0.0298±0.0165±0.0103±0.0096±0.0056±0.0117±0.0086±0.0218±0.0118±0.0035\nMLM (adapter)±0.0165±0.0110±0.0265±0.0181±0.0102±0.0068±0.0551±0.0288±0.0142±0.0099±0.0055\nMLM (prompt)±0.0243±0.0138±0.0126±0.0087±0.0060±0.0035±0.0301±0.0124±0.0068±0.0108±0.0028\nMLM+KD ±0.0295±0.0158±0.0320±0.0230±0.0099±0.0070±0.0345±0.0224±0.0292±0.0155±0.0093\nMLM+AdaptedDeiT±0.0187±0.0122±0.0160±0.0101±0.0048±0.0022±0.0250±0.0179±0.0065±0.0079±0.0086\nMLM+SimCSE±0.0114±0.0077±0.0098±0.0065±0.0029±0.0016±0.0086±0.0056±0.0054±0.0071±0.0027\nMLM+TaCL ±0.0218±0.0103±0.0230±0.0159±0.0105±0.0059±0.0275±0.0156±0.0713±0.0394±0.0118\nMLM+TaCO±0.0456±0.0232±0.0166±0.0134±0.0077±0.0052±0.0675±0.0380±0.0207±0.0128±0.0099\nMLM+InfoWord±0.0267±0.0139±0.0272±0.0191±0.0170±0.0089±0.0344±0.0219±0.0070±0.0079±0.0072\nDGA ±0.0095±0.0047±0.0127±0.0094±0.0052±0.0040±0.0127±0.0081±0.0079±0.0080±0.0034\nTable 4: Standard deviations of the corresponding metrics of the proposed DGA model and the baselines on the six\nexperiments.\nDomain Camera Phone Resturant AI ACL PubMed\nModel MF1 Acc. MF1 Acc. MF1 Acc. MF1 Acc. MF1 Acc. Micro-F1\nRoBERTa ±0.0403±0.0179±0.0210±0.0154±0.0117±0.0049±0.0646±0.0347±0.0192±0.0096±0.0071\nMLM ±0.0479±0.0298±0.0165±0.0103±0.0096±0.0056±0.0117±0.0086±0.0218±0.0118±0.0035\nDGA (H, I) ±0.0373±0.0210±0.0032±0.0039±0.0054±0.0045±0.0095±0.0048±0.0094±0.0073±0.0049\nDGA (H, I, O)±0.0167±0.0092±0.0182±0.0155±0.0055±0.0033±0.0093±0.0075±0.0080±0.0070±0.0056\nDGA (H, I, O, E)±0.0237±0.0123±0.0270±0.0187±0.0099±0.0050±0.0109±0.0089±0.0067±0.0057±0.0079\nDGA (w/o contrast)±0.0433±0.0251±0.0135±0.0106±0.0060±0.0040±0.0197±0.0119±0.0132±0.0093±0.0050\nDGA (random mask)±0.0879±0.0413±0.0335±0.0235±0.0096±0.0044±0.0153±0.0090±0.0105±0.0059±0.0052\nEnsemble ±0.0332±0.0178±0.0199±0.0139±0.0035±0.0031±0.0236±0.0103±0.0061±0.0028±0.0046\nDGA (domain-specific)±0.0137±0.0070±0.0259±0.0200±0.0031±0.0018±0.0128±0.0071±0.0108±0.0067±0.0043\nDGA ±0.0095±0.0047±0.0127±0.0094±0.0052±0.0040±0.0127±0.0081±0.0079±0.0080±0.0034\nTable 5: Standard deviations of the corresponding metrics of the proposed DGA model and the ablation on the six\nexperiments.\nA Datasets Details\nTable 2 in the main paper has given the number of\nexamples in each dataset. Here we provide addi-\ntional details about the 4 types of end-tasks.\n(1) (Phone, Camera and Restaurant) Aspect\nSentiment Classification (ASC) is defined as fol-\nlows (Liu, 2015): given an aspect or product feature\n(e.g., picture quality in a camera review) and a re-\nview sentence containing the aspect in a domain\nor product category (e.g., camera), classify if the\nsentence expresses a positive, negative, or neutral\n(no opinion) sentiment or polarity about the aspect\n(for Phone and Camera, there are only negative and\npositive polarities in the data).\n(2) (ACL) Citation Intent Classification is de-\nfined as follows: given a citing sentence (a sen-\ntence contains a citation), classify if the sentence\nexpresses a citation function among “background”,\n“motivation”, “uses”, “extension” and “comparison\nor contrast future”.\n(3) (AI) Relation Classification is defined as\nfollows: given a within-sentence word sequence\nspans containing a pair of entities, classify if the\nspan expresses a relation among “feature of”, “con-\njunction”, “evaluate for”, “hyponym of”, “used\nfor”, “part of” and “compare”.\n(4) (PubMed) Chemical-protein Interaction\nClassification is defined as follows: given a span\ncontaining a pair of chemical and protein, classify\nif the span expresses a chemical-protein interac-\ntion among “downregulator”, “substrate”, “indirect-\nupregulator”, “indirect-downregulator”, “agnon-\nist”, “activator”, “product of”, “agonist-activator”,\n“inhibitor”, “upregulator”, “substrate product of”,\n“agonist-inhibitor”and “antagonist”.\nB Standard Deviations\nTable 4 reports the standard deviations of the cor-\nresponding results in Table 2 (in the main paper)\nof DGA and the considered baselines over 5 runs\nwith random seeds. We can see the results of DGA\nare stable. Some baselines (e.g., RoBERTa in AI,\nMLM in Camera and MLM+TaCL in ACL) can\nhave quite large standard deviations.\nTable 5 reports the standard deviations of the\ncorresponding results in Table 3 (in the main paper)\nof DGA and the considered baselines over 5 runs\nwith random seeds. We can see the results of DGA\nare stable. Some baselines (e.g., DGA (random\nmask) and DGA (w/o contrast) in Camera) can\nhave quite large standard deviations.\n10188",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7969324588775635
    },
    {
      "name": "Domain adaptation",
      "score": 0.7554134726524353
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.7369664907455444
    },
    {
      "name": "General knowledge",
      "score": 0.7359462380409241
    },
    {
      "name": "Domain knowledge",
      "score": 0.7191199660301208
    },
    {
      "name": "Masking (illustration)",
      "score": 0.6798100471496582
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.5525868535041809
    },
    {
      "name": "Artificial intelligence",
      "score": 0.538170337677002
    },
    {
      "name": "Natural language processing",
      "score": 0.5127133131027222
    },
    {
      "name": "Representation (politics)",
      "score": 0.5114985704421997
    },
    {
      "name": "General purpose",
      "score": 0.4743170440196991
    },
    {
      "name": "Closed captioning",
      "score": 0.46405044198036194
    },
    {
      "name": "Language model",
      "score": 0.4470750689506531
    },
    {
      "name": "Machine learning",
      "score": 0.34390121698379517
    },
    {
      "name": "Mathematics",
      "score": 0.08660981059074402
    },
    {
      "name": "Image (mathematics)",
      "score": 0.08009055256843567
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Computer architecture",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    }
  ]
}