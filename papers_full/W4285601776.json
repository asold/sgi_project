{
    "title": "DictBERT: Dictionary Description Knowledge Enhanced Language Model Pre-training via Contrastive Learning",
    "url": "https://openalex.org/W4285601776",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3006229866",
            "name": "Qianglong Chen",
            "affiliations": [
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2242133483",
            "name": "Feng Lin Li",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2113052374",
            "name": "Guohai Xu",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2051706254",
            "name": "Ming Yan",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2109058337",
            "name": "Ji Zhang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2112981592",
            "name": "Yin Zhang",
            "affiliations": [
                "Zhejiang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6791858558",
        "https://openalex.org/W6864014924",
        "https://openalex.org/W6863994431",
        "https://openalex.org/W3096104421",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2973840669",
        "https://openalex.org/W6748176785",
        "https://openalex.org/W2890894339",
        "https://openalex.org/W2970986510",
        "https://openalex.org/W3114219454",
        "https://openalex.org/W6759363029",
        "https://openalex.org/W3091617571",
        "https://openalex.org/W2898695519",
        "https://openalex.org/W2952087486",
        "https://openalex.org/W2799054028",
        "https://openalex.org/W6863631769",
        "https://openalex.org/W3167128508",
        "https://openalex.org/W3092049183",
        "https://openalex.org/W2759211898",
        "https://openalex.org/W2946345909",
        "https://openalex.org/W6755811877",
        "https://openalex.org/W3003186568",
        "https://openalex.org/W3115295967",
        "https://openalex.org/W3115965961",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W2968908603",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W4303468996",
        "https://openalex.org/W3173169192",
        "https://openalex.org/W3151929433",
        "https://openalex.org/W3103291112",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4287555517",
        "https://openalex.org/W3175604467",
        "https://openalex.org/W3176108833",
        "https://openalex.org/W2998385486",
        "https://openalex.org/W3114916066",
        "https://openalex.org/W3176887776",
        "https://openalex.org/W2923014074"
    ],
    "abstract": "Although pre-trained language models (PLMs) have achieved state-of-the-art performance on various natural language processing (NLP) tasks, they are shown to be lacking in knowledge when dealing with knowledge driven tasks. Despite the many efforts made for injecting knowledge into PLMs, this problem remains open. To address the challenge, we propose DictBERT, a novel approach that enhances PLMs with dictionary knowledge which is easier to acquire than knowledge graph (KG). During pre-training, we present two novel pre-training tasks to inject dictionary knowledge into PLMs via contrastive learning: dictionary entry prediction and entry description discrimination. In fine-tuning, we use the pre-trained DictBERT as a plugin knowledge base (KB) to retrieve implicit knowledge for identified entries in an input sequence, and infuse the retrieved knowledge into the input to enhance its representation via a novel extra-hop attention mechanism. We evaluate our approach on a variety of knowledge driven and language understanding tasks, including NER, relation extraction, CommonsenseQA, OpenBookQA and GLUE. Experimental results demonstrate that our model can significantly improve typical PLMs: it gains a substantial improvement of 0.5%, 2.9%, 9.0%, 7.1% and 3.3% on BERT-large respectively, and is also effective on RoBERTa-large.",
    "full_text": "DictBERT: Dictionary Description Knowledge Enhanced Language Model\nPre-training via Contrastive Learning\nQianglong Chen1;2 , Feng-Lin Li2 , Guohai Xu2 , Ming Yan2 , Ji Zhang2 , Yin Zhang1\u0003\n1College of Computer Science and Technology, Zhejiang University, China\n2Alibaba Group, China\n{chenqianglong, zhangyin98}@zju.edu.cn,{guohai.xgh, ym119608, zj122146}@alibaba-inc.com,\nmaillifenglin@gmail.com\nAbstract\nAlthough pre-trained language models (PLMs)\nhave achieved state-of-the-art performance on var-\nious natural language processing (NLP) tasks, they\nare shown to be lacking in knowledge when deal-\ning with knowledge driven tasks. Despite the many\nefforts made for injecting knowledge into PLMs,\nthis problem remains open. To address the chal-\nlenge, we propose DictBERT, a novel approach\nthat enhances PLMs with dictionary knowledge\nwhich is easier to acquire than knowledge graph\n(KG). During pre-training, we present two novel\npre-training tasks to inject dictionary knowledge\ninto PLMs via contrastive learning: dictionary en-\ntry predictionand entry description discrimination.\nIn ﬁne-tuning, we use the pre-trained DictBERT\nas a plugin knowledge base (KB) to retrieve im-\nplicit knowledge for identiﬁed entries in an input\nsequence, and infuse the retrieved knowledge into\nthe input to enhance its representation via a novel\nextra-hop attention mechanism. We evaluate our\napproach on a variety of knowledge driven and lan-\nguage understanding tasks, including NER, rela-\ntion extraction, CommonsenseQA, OpenBookQA\nand GLUE. Experimental results demonstrate that\nour model can signiﬁcantly improve typical PLMs:\nit gains a substantial improvement of 0.5%, 2.9%,\n9.0%, 7.1% and 3.3% on BERT-large respectively,\nand is also effective on RoBERTa-large.\n1 Introduction\nPre-trained language models (PLMs) such as BERT [Devlin\net al., 2019], RoBERTa[Liu et al., 2019] and ALBERT [Lan\net al., 2019] have been prevailing in both academic and indus-\ntrial community due to their state-of-the-art performance on\nvarious natural language processing (NLP) tasks. However,\nas they capture only a general language representation learned\nfrom large-scale corpora, they are shown to be lacking in\nknowledge when dealing with knowledge driven tasks [Tal-\nmor et al., 2019; Mihaylov et al., 2018]. To address this\nchallenge, many efforts, such as ERNIE-THU [Zhang et al.,\n\u0003Corresponding Author: Yin Zhang.\n2019], KEPLER [Wang et al., 2021c], KnowBERT [Peters et\nal., 2019], K-Adapter [Wang et al., 2021b] and ERICA [Qin\net al., 2021] have been made for injecting knowledge into\nPLMs for further improvement.\nHowever, existing knowledge enhanced PLMs (i.e., K-\nPLMs) still suffer from several deﬁciencies. First, few meth-\nods pay attention to knowledge itself, including what type\nof knowledge is needed and the feasibility of acquiring such\nknowledge. On the one hand, some models take for granted\nthe use of knowledge graph (KG), which is difﬁcult to ac-\nquire in practice and shown to be less effective than dictio-\nnary knowledge [Xu et al., 2021; Chen et al., 2020]. On the\nother hand, many methods use Wikipedia, which is easier to\naccess but often noisy and of low knowledge density. Sec-\nond, current K-PLMs mainly focus on one or two types of\nknowledge-driven tasks. Although they are shown to be use-\nful on a few speciﬁc tasks, their language understanding abil-\nity was either not further validated on GLUE[Liu et al., 2020;\nWang et al., 2021b] or not improved [Zhang et al., 2019].\nThat is, the application scope of such K-PLMs is limited.\nInspired by the hint that dictionary knowledge can be even\nmore effective than structured knowledge[Chen et al., 2020],\nwe leverage dictionary sources as external knowledge to en-\nhance PLMs. In our experience, this enjoys several beneﬁts.\nFirst, it is consistent with human reading habit and cogni-\ntive process. In the process of reading, when encountering\nunfamiliar words, people usually consult dictionaries or en-\ncyclopedias. Second, compared with long Wikipedia texts,\ndictionary knowledge is more concise and of high knowledge\ndensity. Third, dictionary knowledge is much easier to ac-\ncess, which is of key importance for applying K-PLMs in\npractice. Even in the case of lacking a dictionary, it can be\nacquired through simply constructing a generator to summa-\nrize the description explaining a word.\nCorrespondingly, we propose DictBERT, an effective ap-\nproach that enhances PLMs with dictionary knowledge via\ncontrastive learning. In the pre-training stage, we inject dic-\ntionary knowledge into PLMs through two novel pre-training\ntasks: dictionary entry prediction, in which we use a descrip-\ntion to predict its masked entry and learn entry representa-\ntions from descriptive texts; and entry description discrimi-\nnation, where we use contrastive learning to improve the ro-\nbustness of entry representations by constructing positive and\nnegative samples with dictionary synonyms and antonyms.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4086\nDuring ﬁne-tuning, we ﬁrst identify dictionary entries from\na given input, then use DictBERT as a plugin KB to retrieve\ncorresponding entry information. For the fusion of retrieved\nentry information and original input, we propose a novel\nextra-hop attention mechanism to enhance its representation\nfor downstream tasks.\nThe main contributions of our paper are as follows:\n•We propose DictBERT, a novel approach enhancing\nPLMs with dictionary knowledge, which is able to effec-\ntively not only integrate external knowledge into but also\nimprove the language understanding ability of PLMs.\n•For pre-training, we present two novel pre-training tasks\nwith contrastive learning, namely dictionary entry pre-\ndiction and entry description discrimination, for inject-\ning dictionary knowledge into PLMs. For ﬁne-tuning,\nwe present three knowledge infusion mechanisms to uti-\nlize the retrieved knowledge from pre-trained DictBERT\nfor improving downstream tasks.\n•We conducted a series of experiments on NER, relation\nextraction (RE), CommonsenseQA, OpenBookQA and\nGLUE. Experimental results show that our model can\nsigniﬁcantly improve typical PLMs (BERT-large and\nRoBERTa-large).\n2 Related Work\nKnowledge Enhanced PLMs. To alleviate the problem of\nlacking knowledge for PLMs, a popular approach is to inject\nfactual knowledge through infusing pre-trained entity embed-\ndings [Zhang et al., 2019; Peters et al., 2019] or incorpo-\nrating symbolic knowledge triples [Liu et al., 2020]. One\nproblem of using pre-trained entity embeddings, as pointed\nout by CoLAKE [Sun et al., 2020 ], is the separation be-\ntween entity embedding and language embedding. To tackle\nthis problem, we use textual entry descriptions to predict\ntheir masked entries in the entry prediction pre-training task.\nBeing different from KnowBERT [Peters et al., 2019] and\nKEPLER [Wang et al., 2021c] that use structured KGs, we\nuse semi-structured dictionary knowledge. Inspired by K-\nAdapter [Wang et al., 2021b], we also use the PLM enhanced\nwith dictionary knowledge as a plugin for downstream tasks.\nIt should be noted that Dict-BERT [Yu et al., 2021] and our\nwork are at the same period. There are many differences be-\ntween them in pre-training and ﬁne-tuning, and our results are\nsuperior to those of Dict-BERT.\nContrastive Learning. The main idea of contrastive learn-\ning is to improve the robustness of representations through\nbringing closer positive samples and pushing away negative\nones. It has been widely used to obtain better sentence rep-\nresentations [Logeswaran and Lee, 2018; Wu et al., 2020;\nWang et al., 2021a; Qin et al., 2021]. While [Logeswaran\nand Lee, 2018] take a sentence B that follows A as a positive\nexample and randomly chooses a sentence C from other doc-\numents as a negative, [Wang et al., 2021a] construct positive\nand negative examples via replacing representative tokens in\na sentence with WordNet, and [Wu et al., 2020] present mul-\ntiple sentence-level data augmentation strategies. In addi-\ntion, [Qin et al., 2021] leverage contrastive pre-training to\nEntry word forest\nDescription a large area of land covered with trees and plants,\nusually larger than a wood, or the trees and plants\nthemselves\nSynonyms jungle, woodland\nAntonyms desert, wasteland\nTable 1: A sample of dictionary entries.\nimprove the ability of PLMs on capturing relational facts in\ntexts. Being differently, we use synonyms and antonyms in\ndictionary to construct contrastive pairs, and use contrastive\npre-training to learn a better dictionary entry representation\nfor downstream tasks.\n3 The Proposed Approach\n3.1 Dictionary Description Knowledge\nA dictionary is a resource that lists the words of a language,\nclariﬁes their meanings through explanatory descriptions, and\noften speciﬁes their pronunciation, origin, usage, synonyms\nand antonyms, etc. Table 1 shows an example about the en-\ntry word “forest”. In this paper, we use four kinds of infor-\nmation for pre-training: each entry, its description(s), syn-\nonym(s) and antonym(s). We leverage dictionary entry words\nand their meanings (i.e., explanatory descriptions) for knowl-\nedge injection pre-training. Also, in order to improve the ro-\nbustness of entry representation, we use the synonyms and\nantonyms of an entry word for contrastive learning.\n3.2 Pre-training DictBERT\nAs shown in Figure 1, we use two novel pre-training tasks: (1)\ndictionary entry prediction and (2) entry description discrim-\nination, to capture the different aspects of dictionary knowl-\nedge through further training a PLM.\nDictionary Entry Prediction. For entry word prediction,\nwe follow the design of masked language modeling (MLM)\nin BERT [Devlin et al., 2019], but impose constraints on the\ntokens to be masked. Originally, given an input sequence, the\nMLM task randomly masks a certain percentage of the in-\nput tokens with a special [MASK] symbol, and then tries to\nrecover them. Inspired by [Tsukagoshi et al., 2021], to effec-\ntively learn entry representations, we take as input the con-\ncatenation of each entry word e = {t1;:::;t i;:::;t m}and its\ndescription desc = {w1;w2;:::;w n}in a dictionary D, per-\nform masking only on the tokens of entry ein a chosen input\nsample s = {[CLS]e[SEP]desc[SEP]}, and at last predict\nthe masked entry tokens based on the corresponding descrip-\ntion desc. Note that if an entry econsists of multiple tokens,\nall of the component tokens will be masked. In the case of\npolysemy, where an entry ehas multiple meanings (i.e., de-\nscriptions), we construct an input sample for each meaning in\na similar way. We formulate the entry token prediction as:\nP(t1;:::;t i;:::;t m|s\\{t1;:::;t i;:::;t m}) (1)\nwhere the ti is the i-th token of e, and s\\{t1;:::;t i;:::;t m}\ndenotes the sample swith entry tokens ti:::m being masked.\nWe initialize our model with the pre-trained checkpoint of\nBERT-large and keep MLM as one of our objectives, which\nuses the cross-entropy loss as loss function Ldep.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4087\nPredicted Tokens in Entry \n \nTransformer Encoder\nBERT-large\nCambridge \nDictionary \nPre-training Corpus\nWeight Initialization\n[CLS]\nMasked \nEntry [SEP] Desc [SEP]\nTok1\nPre-training Task1 \nDictionary Entry Prediction\nPostive Sample \n \nTransformer Encoder\n[CLS]\nPostive Sample \nEntry [SEP] Desc [SEP]\nSequence Representation\nNegative Sample \n \nTransformer Encoder\n[CLS]\nNegative Sample \nEntry [SEP] Desc [SEP]\nSequence Representation\nContrastive\nLoss\nPre-training Task2 \nEntry Description Discrimination\n+ Pull\nPush\nTok2\nFigure 1: DictBERT pre-training. We take the Cambridge dictionary as our knowledge source. The pre-training tasks include dictionary\nentry predictionand entry description discrimination. In the former task, we mask only the entry tokens in the chosen input samples. In\nthe latter task, we try to obtain better entry representations through contrastive learning.\nPositive [CLS] woodland [SEP] Land covered with wood or trees\n[SEP]\nNegative [CLS] desert [SEP] arid land with little or no vegetation\n[SEP]\nTable 2: The positive sample “woodland” and negative example\n“desert” of the entry “forest”.\nEntry Description Discrimination. To better capture the\nsemantics of dictionary entries, we introduce entry descrip-\ntion discrimination, which tries to improve the robustness of\nentry representations through contrastive learning. Speciﬁ-\ncally, we construct positive (resp. negative) samples as fol-\nlows: given an entry word e and its description desc, we\nobtain its synonyms Ds = {esyn}(resp. antonyms Da =\n{eant}) from the dictionary source, and treat the concatena-\ntion of each esyn (resp. eant) and its description descsyn\n(resp. descant) as a positive (resp. negative) sample. Take\nthe entry “forest” in Table 1 for example, “woodland” and\n“desert” are one of its synonyms and antonyms, respectively.\nThe corresponding positive and negative samples are shown\nin Table 2. In our experiments, we use the same number of\n(e.g., 5) positive and negative samples. Note that we currently\nonly utilize the antonyms of an entry word to construct strict\nnegative samples, and will explore the construction of nega-\ntive samples through random selection in the future.\nWe use hori, hsyn, hant to indicate the representations of\nthe original, the positive, and the negative input sample. To\nbring closer hori and hsyn, and push away hori and hant,\nwe develop a contrastive objective, where (eori, esyn) is con-\nsidered a positive pair and (eori, eant) is considered negative.\nWe usehc, which denotes the hidden state of the special sym-\nbol [CLS], to indicate the representation of an input sample.\nWe deﬁne a contrastive objective Ledd:\nhc\nori = Encoder(eori) (2)\nhc\nsyn = Encoder(esyn) (3)\nhc\nant = Encoder(eant) (4)\nf(hc\ni ;hc\nj) =exp(hc\ni hc\nj) (5)\nLedd = −\nX\ne∈D\nlog f(eori;esyn)\nf(eori;esyn) +f(eori;eant) (6)\nwhere the f(x;y) denotes the exponentiation of the dot prod-\nuct between hidden states x and y.\nWe sum the dictionary entry prediction task loss and the\nentry description discrimination task loss, and ﬁnally obtain\nthe overall loss function L:\nL= \u00151Ldep + \u00152Ledd (7)\nwhere Ldep and Ledd denote the loss functions of the two\ntasks. In our experiments, we set \u00151 = 0:4 and \u00152 = 0:6.\n3.3 Fine-tuning with DictBERT\nInspired by [Petroni et al., 2019], we use DictBERT as a\nplugin with a backbone PLM during ﬁne-tuning (i.e., it is\nfrozen). In this way, we can enjoy the ﬂexibility of train-\ning different DictBERTs for different dictionaries and avoid\nthe catastrophic forgetting problem of continuous training.\nSpeciﬁcally, we ﬁrst identify dictionary entries from a given\ninput, then use DictBERT as a KB to retrieve corresponding\nentry information (i.e., entry embeddings), and ﬁnally inject\nthe retrieved entry information into the original input to get\nan enhanced representation for downstream tasks. In the case\nan input consists of more than one sequence (e.g., NLI), we\nprocess each input sequence individually and then feed them\ninto the downstream task speciﬁc layer for subsequent pro-\ncessing. To better leverage the retrieved implicit knowledge\non downstream tasks, we introduce three different kinds of\nknowledge infusion mechanisms (See Figure 2): (1) pooled\noutput concatenation, (2) extra-hop attention and (3) layer-\nwise extra-hop attention.\nPooled Output Concatenation. As shown in Figure 2 (a),\nwe directly concatenate the pooled output of the backbone\nBERT (i.e., hc) and the sum of entry embeddings retrieved\nfrom DictBERT (i.e., ^h). Then, we feed the concatenation\n(i.e., [hc; ^h]) into a task speciﬁc layer for downstream tasks.\nExtra-hop Attention. The simplest way to incorporate\nidentiﬁed entries into original text is to sum up their embed-\ndings and concatenate the summation with the text represen-\ntation. However, this method can not tell which entry is more\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4088\nBERT DictBERT\nInput\nhc Extra-hop ATT.\nTask Specific Layer\n(b) DictBERT + Extra-hop Attention\nTask Specific Layer\nBERT DictBERT\nInput\nhc\nĥ [hc; ĥ]\n(a) DictBERT + Concatenation\n \n. \n. \n.Layer 1\nLayer L  \n. \n. \n.Layer 1\nLayer L\nBERT DictBERT \nExtra-hop ATT.\n. \n. \nExtra-hop ATT.\nInput\nTask Specific Layer\nIdentified \nEntries\n(c) DictBERT + Layer-wise Extra-hop Attention\nIdentified\nEntries\ne1 \ne2 \neK\nsum\nIdentified \nEntries\n[hc; ĥ]\nhc e1e2eK\nMean Pooling\nhc ĥ\n[hc; ĥ]Projection Head\nProjection Head\nProjection Head\nĥ \ne1 \ne2 \neK\ne1 \ne2 \neK\ne1 \ne2 \neK\ne1 \ne2 \neK\nFigure 2: Downstream task ﬁne-tuning with DictBERT, where we present three different knowledge infusion mechanisms.\nimportant, and which sense is more suitable in the case of\npolyseme entries. Therefore, we further propose an extra-hop\nattention mechanism to address this deﬁciency. As shown in\nFigure 2 (b), we follow Transformer-XH [Zhao et al., 2020]\nto use hc, the hidden state of the [CLS] token in an input\nquery as the “attention hub”, which attends to each entry word\nidentiﬁed in the same input. With the attentive weights, our\nmethod focuses on more important entries or meanings when\nintegrating them as external knowledge into the original input\nquery. The extra hop attention mechanism is formulated as:\n^h=\nKX\ni=1\nATT(hc;ei) (8)\nwhere ei denotes the DictBERT output ofith identiﬁed entry,\nK is the number of identiﬁed entries in the input query, and\n^h denotes the weighted sum of retrieved entry embeddings.\nAfter we obtain the ^h, we use [hc; ^h] for the ﬁnal inference.\nLayer-wise Extra-hop Attention. To further improve per-\nformance, we extend the extra-hop attention at the last layer\nto each inner layer, making it become layer-wise. As shown\nin Figure 2 (c), we compute the attention score at each layer,\nand ﬁnally use their mean for implicit entry knowledge injec-\ntion. Speciﬁcally, the layer-wise extra-hop attention can be\nformulated as:\n^hl =\nKX\ni=1\nATT(hl;el\ni) (9)\n^h= 1\nL\nLX\nl=1\n^hl (10)\nwhere ^hl denotes the weighted sum of l-th layer outputs of\nDictBERT. With the ﬁnal implicit^hobtained via Equation 10,\nwe use [hc; ^h] in a similar way for downstream tasks.\n4 Experiments\n4.1 Datasets and Tasks\nPre-training Dictionary Source. To pre-train DictBERT,\nwe use the Cambridge Dictionary1, which includes 315K en-\n1https://dictionary.cambridge.org\ntry words, as our pre-training corpus. We construct input\nsamples for the two pre-training tasks, namely dictionary en-\ntry prediction and entry description discrimination, as intro-\nduced in the section of the proposed approach.\nCoNLL2003 & TACRED. We use these two traditional\nknowledge-driven tasks, CoNLL2003 [Tjong Kim Sang and\nDe Meulder, 2003 ] and TACRED [Zhang et al., 2017], to\nhave a quick check on the effectiveness of our approach.\nCommonsenseQA & OpenBookQA. We use Common-\nsenseQA [Talmor et al., 2019] and OpenBookQA [Mihaylov\net al., 2018] to evaluate the ability of DictBERT acting as\nKBs and providing implicit knowledge to downstream tasks.\nGLUE. We follow existing knowledge enhanced PLMs\nsuch as KEPLER and KnowBERT to use GLUE[Wang et al.,\n2018] to evaluate the general natural language understanding\ncapability of our approach.\n4.2 Experimental Settings\nFor pre-training, we use the BERT-large-uncased and\nRoBERTa-large model as backbone and set the learning rate\nto 1e−5, dropout rate to 0:1, max-length of tokens to 128,\nbatch size to32, and number of epochs to10. We use AdamW\nas the optimizer. For ﬁne-tuning, we adopt cross-entropy loss\nas the loss function, set batch size to32 and number of epochs\nto 30. We run 5 times for each task and report their average.\n4.3 Baselines\nBERT & RoBERTa. We adopt BERT-large[Devlin et al.,\n2019] instead of BERT-base as baseline because the former\nis more difﬁcult to improve. To be more convincing, we also\nuse the more adequately trained RoBERTa-large [Liu et al.,\n2019] for comparison in our experiments.\nEnhanced BERT & RoBERTa. For CommonsenseQA,\nwe use BERT+AMS [Ye et al., 2019 ], BERT+OMCS,\nRoBERTa+CSPT, RoBERTa+KE, G-DAUG [Yang et al.,\n2020] as baselines for comparison. For OpenbookQA, we use\nAristoBERTv7, AristoRoBERTav7 and BERT Multi-Task as\nbaselines for comparison.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4089\nModel CoNLL2003 TACRED\nBERT-large 92.8 70.1\nDictBERT + Concat(K) 93.1 72.3\nDictBERT + EHA(K) 93.2 72.7\nDictBERT + EHA(K+V) 93.3 72.8\nDictBERT + LW A(K+V) 93.3 73.0\nTable 3: Experimental results on CoNLL2003 (NER) and TACRED\n(relation extraction).\nModel CSQA OBQA\nBERT-large 56.7 60.4\nBERT-large + AMS 62.2 -\nBERT-large + OMCS 62.5 -\nBERT-large Multi-Task - 63.8\nAristoBERTv7-large 64.6 72.0\nRoBERTa-large 72.1 71.8\nRoBERTa-large + CSPT 69.6 -\nRoBERTa-large + G-DAUG-Combo 72.6 -\nRoBERTa-large + KE 73.3 -\nAristoRoBERTav7-large - 77.8\nDictBERT + Concat(K) 62.7 64.4\nDictBERT + EHA(K) 65.1 66.3\nDictBERT + EHA(K+V) 65.4 66.7\nDictBERT + LW A(K+V) 65.7 67.5\nDictRoBERTa + Concat(K) 75.7 75.2\nDictRoBERTa + EHA(K) 77.5 77.6\nDictRoBERTa + EHA(K+V) 77.8 78.1\nDictRoBERTa + LW A(K+V) 78.5 78.3\nTable 4: Experimental results on CommonsenseQA (CSQA) and\nOpenBookQA (OBQA).\nKnowBERT & KEPLER. For GLUE, we use KnowBERT\nand KEPLER as baselines for comparison. KnowBERT [Pe-\nters et al., 2019] enhances contextual word representations\nthrough embedding structured, human-curated knowledge\ninto BERT-base through entity linking and word-to-entity at-\ntention. KEPLER [Wang et al., 2021c] encodes textual en-\ntity descriptions with RoBERTa-base as their embeddings,\nand then jointly optimizes the knowledge embedding and lan-\nguage modeling objectives.\n4.4 DictBERT Variants\nWe evaluate different variants of DictBERT in our ex-\nperiments. DictBERT+Concat(K) uses the concatenation\nmechanism, DictBERT+EHA(K) and DictBERT+EHA\n(K+V) adopt the extra-hop attention mechanism, and Dict-\nBERT+LWA(K+V)uses layer-wise attention. The symbolK\nindicates the use of entry word to retrieve entry embeddings\nfrom DictBERT, K+V denotes that we use both entry word\nand its corresponding description for knowledge retrieval.\n4.5 Experimental Results and Analysis\nTraditional Knowledge Driven Task Results.Firstly, we\nevaluate DictBERT on NER and relation extraction, the most\ncommonly used knowledge driven tasks. As shown in Ta-\nble 3, our approach is ﬁnally able to improve the performance\non CoNLL2003 and TACRED by 0.5% and 2.9% compared\nwith the strong baseline BERT-large. Further, we can observe\nthat all the three knowledge infusion mechanisms are help-\nful, and the layer-wise attention achieve the best results. This\nindicates the identiﬁcation and explanation of important en-\ntities in an input sample are of key importance. Meanwhile,\nwe found that the use of additional entry description (i.e., the\nK+V setting) can help retrieve better entry embeddings.\nKnowledge Driven QA Task Results. We further assess\nDictBERT on knowledge driven QA tasks, namely Common-\nsenseQA and OpenBookQA, and report the results in Ta-\nble 4. Compared with BERT-large, our basic setting Dict-\nBERT+Concat gains a signiﬁcant improvement of 6.0% and\n4.0% on the two tasks, respectively. Further, we observe\nthat the extra-hop attention brings an evident increase (2.4%\nand 1.9%), verifying again the importance of identifying at-\ntentive weights of entries in an input sample. Lastly, Dict-\nBERT+LW A(K+V) achieves the best result on both tasks,\nbringing a ﬁnal gain of 9.0% and 7.1% compared to the\nBERT-large baseline. To be more convincing, we also\ncompare DictRoBERTa with the original RoBERTa-large on\nCommonsenseQA and OpenBookQA. As shown in Table 4,\nthe conclusion also holds for RoBERTa. Similarly, Dic-\ntRoBERTa+LW A(K+V) achieves the best results, which can\nultimately improve over 6.4% and 6.5%, respectively.\nGLUE Results. We also evaluate DictBERT on GLUE to\nexamine whether it can improve the general natural language\nunderstanding ability of PLMs. Table 5 shows that com-\npared with BERT-large our basic setting DictBERT+Concat\nachieves an average improvement of 2.4%, indicating the ef-\nfectiveness of injecting dictionary knowledge for language\nunderstanding. Similarly, the extra-hop attention and the use\nof additional entry description (i.e., the K+V setting) con-\ntribute to further improvement, and DictBERT+LW A(K+V)\nachieves the best results, bringing a ﬁnal increase of 3.3% on\naverage. With the baseline being RoBERTa-large, our best\nmodel can achieve an average increase of 0.9% on GLUE,\nvalidating the effectiveness and broad applicability of our ap-\nproach. As for other K-PLMs, KnowBERT enhanced with\nWordNet and 470K Wikipedia entities can improve BERT-\nbase by 2.0%, which is smaller than the performance gain\n(3.3%) brought by our method on BERT-large. KEPLER-\nwiki can only improve RoBERTa-base by 0.2% when using\n5M Wikidata entities for knowledge (entity) embedding and\nextra 13GB text data for MLM. With the only 5M entity de-\nscriptions for MLM, there is an obvious performance drop for\nKEPLER-OnlyDesc. Therefore, our approach is more effec-\ntive in improving language understanding ability with exter-\nnal knowledge (we use only 315K dictionary entries).\nAblation Study. We perform ablation studies on the dif-\nferent components of DictBERT. Firstly, we evaluate BERT-\nlarge+Concat(K) and BERT-large+LW A(K+V), which di-\nrectly use BERT-large, instead of our pre-trained Dict-\nBERT, as the plugin. As we can see, the improvement is\nrather marginal, conﬁrming the necessity of injecting exter-\nnal knowledge. Secondly, we assess the effectiveness of the\ntwo per-training tasks: DictBERT(DEP)+Concat and Dict-\nBERT(DEP+EDD)+Concat. As shown in Table 6, contrastive\nlearning is helpful to some degree (0.4% on average), and\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4090\nModel Single-sentence Similarity Paraphrase Inference AvgCoLA SST-2 QQP STS-B MRPC QNLI MNLI RTE\nBERT-base 52.1 93.5 88.9 85.8 88.9 90.5 84.6 66.4 81.3\nBERT-large 60.5 94.9 89.3 87.6 89.3 92.7 85.9 70.1 83.8\nRoBERTa-base 63.6 94.8 91.9 91.2 90.2 92.7 87.5 80.9 86.6\nRoBERTa-large 67.8 96.7 90.2 92.0 93.0 95.4 90.2 87.2 89.0\nKnowBERT-WordNet+Wiki 54.6 93.6 90.3 89.1 88.2 91.5 85.7 73.8 83.3\nKEPLER-only Desc 55.8 94.4 90.8 90.2 88.5 92.4 85.9 78.3 84.5\nKEPLER-wiki 63.6 94.5 91.7 91.2 89.3 92.4 87.2 85.2 86.8\nDictBERT + Concat(K) 64.1 95.4 90.3 90.5 91.8 95.1 87.1 75.2 86.2\nDictBERT + EHA(K) 64.2 95.6 90.3 91.5 92.0 95.7 88.4 76.6 86.8\nDictBERT + EHA(K+V) 64.5 95.6 90.4 91.6 92.1 95.8 88.5 76.8 86.9\nDictBERT + LW A(K+V) 64.7 95.7 90.4 91.7 92.3 96.0 88.7 77.1 87.1\nDictRoBERTa + Concat(K) 68.1 97.1 90.3 91.2 92.5 96.2 90.5 88.1 89.2\nDictRoBERTa + EHA(K) 68.3 97.3 90.4 91.7 92.7 96.4 90.8 88.6 89.5\nDictRoBERTa + EHA(K+V) 68.5 97.5 90.5 91.8 92.8 96.5 90.9 89.1 89.7\nDictRoBERTa + LW A(K+V) 68.6 97.8 90.8 92.1 93.2 96.8 91.1 89.4 89.9\nTable 5: Experimental results on the GLUE development set. The parameter of DictBERT is based on BERT-large. For parameter initializa-\ntion, KnowBERT uses the BERT-base, while KEPLER uses RoBERTa-base.\nModel CSQA OBQA GLUE\nBERT-large 56.7 60.4 83.8\nBERT-large + Concat(K) 57.1 60.6 83.9\nBERT-large + LW A(K+V) 57.3 60.7 84.1\nPre-training\nBERT-large (dict corpus)+Concat(K) 61.9 63.8 85.5\nDictBERT(DEP)+Concat(K) 62.2 64.1 85.8\nDictBERT(DEP+EDD)+Concat(K) 62.7 64.4 86.2\nFine-tuning\nDictBERT-only 62.6 64.1 85.7\nDictBERT + Concat(K) 62.7 64.4 86.2\nDictBERT + EHA(K) 65.1 66.3 86.8\nDictBERT + EHA(K+V) 65.4 66.7 86.9\nDictBERT + LW A(K+V) 65.7 67.5 87.1\nDictBERT plus + LW A(K+V) 66.1 67.7 87.2\nTable 6: Ablation study results on CSQA, OBQA and GLUE.\nmasking only entry tokens is better than masking tokens of\nboth entries and descriptions (+0.3% for all the three). Fi-\nnally, we examine the necessity of using DictBERT as a plu-\ngin KB instead of directly using it for downstream task ﬁne-\ntuning (DictBERT-only), and whether the dictionary size mat-\nters (DictBERT plus). As shown in Table 6, all of our three\nknowledge infusion mechanisms can further improve the per-\nformance of DictBERT-only, indicating the use of DictBERT\nas a plugin is rewarding. To assess the effect of dictionary\nsize, we use the union of the Cambridge Dictionary, the Ox-\nford Dictionary and the Wiktionary, which totals more than\n1M unique entry words. The results show that DictBERT\nplus+LW A(K+V) can further improve the performance of the\nthree task sets (+0.23% on average).\n4.6 Discussions\nExperimental results show that DictBERT can not only in-\ntegrate external knowledge into but also improve the lan-\nguage understanding ability of PLMs. It is worth mentioning\nthat DictBERT is assessed on very strong baselines (BERT-\nlarge and RoBERTa-large, rather than the base counterparts\nadopted by many other K-PLMs), which indicates the effec-\ntiveness of our method from another side. Last but not least,\nour approach can be easily applied in practice: dictionary\nsource is relatively easy to acquire, through either crawling\nor simple generative models. As for computation cost, Dict-\nBERT as a KB plugin can be further simpliﬁed to be Dict-\nBERT as a lookup table, with each entry in a dictionary being\nmapped to an embedding in advance, largely accelerating the\ninference speed. Through generating dictionary entry embed-\ndings in advance by using the plugin, the complexity of our\napproach is similar to that of the backbone PLM.\n5 Conclusion\nIn this paper, we propose DictBERT, an effective approach\nthat enhances PLMs with dictionary knowledge through two\nnovel pre-training tasks and an attention-based knowledge in-\nfusion mechanism during downstream task ﬁne-tuning. We\nalso demonstrate its effectiveness through an adequate set of\nexperiments. Importantly, our approach can be easily applied\nin practice. In the future, we are going to further explore more\neffective pre-training tasks and knowledge infusion mecha-\nnisms for injecting knowledge into multilingual pre-trained\nlanguage models.\nAcknowledgments\nWe thank the anonymous reviewers for their helpful com-\nments on this paper. This work was supported by Na-\ntional Key R&D Program of China (No. 2018AAA0101900),\nthe NSFC projects (No. 62072399, No. U19B2042,\nNo. 61402403), Chinese Knowledge Center for Engineering\nSciences and Technology, MoE Engineering Research Cen-\nter of Digital Library, Alibaba Group, Alibaba-Zhejiang Uni-\nversity Joint Research Institute of Frontier Technologies, and\nthe Fundamental Research Funds for the Central Universities\n(No. 226-2022-00070).\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4091\nReferences\n[Chen et al., 2020] Qianglong Chen, Feng Ji, Haiqing Chen,\nand Yin Zhang. Improving commonsense question an-\nswering by graph-based iterative retrieval over multiple\nknowledge sources. In COLING, pages 2583–2594, 2020.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional transformers for language understand-\ning. In NAACL-HLT, pages 4171–4186, 2019.\n[Lan et al., 2019] Zhenzhong Lan, Mingda Chen, Sebastian\nGoodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-\ncut. Albert: A lite bert for self-supervised learning of lan-\nguage representations. arXiv:1909.11942, 2019.\n[Liu et al., 2019] Yinhan Liu, Myle Ott, Naman Goyal,\nJingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\nRoberta: A robustly optimized bert pretraining approach.\narXiv:1907.11692, 2019.\n[Liu et al., 2020] Weijie Liu, Peng Zhou, Zhe Zhao, et al.\nK-bert: Enabling language representation with knowledge\ngraph. In AAAI, pages 2901–2908, 2020.\n[Logeswaran and Lee, 2018] Lajanugen Logeswaran and\nHonglak Lee. An efﬁcient framework for learning\nsentence representations. In ICLR, 2018.\n[Mihaylov et al., 2018] Todor Mihaylov, Peter Clark, Tushar\nKhot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answer-\ning. In EMNLP, pages 2381–2391, 2018.\n[Peters et al., 2019] Matthew E. Peters, Mark Neumann,\nRobert L Logan, Roy Schwartz, Vidur Joshi, Sameer\nSingh, and Noah A. Smith. Knowledge enhanced contex-\ntual word representations. In EMNLP, pages 43–54, 2019.\n[Petroni et al., 2019] Fabio Petroni, Tim Rockt ¨aschel, Se-\nbastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang\nWu, and Alexander Miller. Language models as knowl-\nedge bases? In EMNLP, pages 2463–2473, 2019.\n[Qin et al., 2021] Yujia Qin, Yankai Lin, Ryuichi Takanobu,\nZhiyuan Liu, Peng Li, Heng Ji, Minlie Huang, Maosong\nSun, and Jie Zhou. ERICA: Improving entity and relation\nunderstanding for pre-trained language models via con-\ntrastive learning. In ACL, pages 3350–3363, 2021.\n[Sun et al., 2020] Tianxiang Sun, Yunfan Shao, Xipeng Qiu,\nQipeng Guo, Yaru Hu, Xuanjing Huang, and Zheng\nZhang. Colake: Contextualized language and knowledge\nembedding. In COLING, pages 3660–3670, 2020.\n[Talmor et al., 2019] Alon Talmor, Jonathan Herzig,\nNicholas Lourie, and Jonathan Berant. CommonsenseQA:\nA question answering challenge targeting commonsense\nknowledge. In NAACL-HLT, pages 4149–4158, 2019.\n[Tjong Kim Sang and De Meulder, 2003] Erik F. Tjong\nKim Sang and Fien De Meulder. Introduction to the\nCoNLL-2003 shared task: Language-independent named\nentity recognition. In NAACL-HLT, pages 142–147, 2003.\n[Tsukagoshi et al., 2021] Hayato Tsukagoshi, Ryohei\nSasano, and Koichi Takeda. DefSent: Sentence embed-\ndings using deﬁnition sentences. In ACL, 2021.\n[Wang et al., 2018] Alex Wang, Amanpreet Singh, Julian\nMichael, Felix Hill, Omer Levy, and Samuel Bowman.\nGLUE: A multi-task benchmark and analysis platform for\nnatural language understanding. In EMNLP, 2018.\n[Wang et al., 2021a] Dong Wang, Ning Ding, Piji Li, and\nHai-Tao Zheng. Cline: Contrastive learning with semantic\nnegative examples for natural language understanding. In\nACL, pages 2332–2342, 2021.\n[Wang et al., 2021b] Ruize Wang, Duyu Tang, Nan Duan,\nZhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao,\nDaxin Jiang, and Ming Zhou. K-Adapter: Infusing Knowl-\nedge into Pre-Trained Models with Adapters. In Findings\nof ACL, pages 1405–1418, 2021.\n[Wang et al., 2021c] Xiaozhi Wang, Tianyu Gao, Zhaocheng\nZhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian\nTang. Kepler: A uniﬁed model for knowledge embedding\nand pre-trained language representation. TACL, 2021.\n[Wu et al., 2020] Zhuofeng Wu, Sinong Wang, Jiatao Gu,\nMadian Khabsa, Fei Sun, and Hao Ma. Clear: Contrastive\nlearning for sentence representation. arXiv:2012.15466,\n2020.\n[Xu et al., 2021] Yichong Xu, Chenguang Zhu, Ruochen\nXu, Yang Liu, Michael Zeng, and Xuedong Huang. Fusing\ncontext into knowledge graph for commonsense question\nanswering. In Findings of ACL, pages 1201–1207, 2021.\n[Yang et al., 2020] Yiben Yang, Chaitanya Malaviya, Jared\nFernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-\nPing Wang, Chandra Bhagavatula, Yejin Choi, and Doug\nDowney. Generative data augmentation for commonsense\nreasoning. In Findings of EMNLP, pages 1008–1025,\n2020.\n[Ye et al., 2019] Zhi-Xiu Ye, Qian Chen, Wen Wang, and\nZhen-Hua Ling. Align, mask and select: A simple method\nfor incorporating commonsense knowledge into language\nrepresentation models. arXiv:1908.06725, 2019.\n[Yu et al., 2021] Wenhao Yu, Chenguang Zhu, Yuwei Fang,\nDonghan Yu, Shuohang Wang, Yichong Xu, Michael\nZeng, and Meng Jiang. Dict-bert: Enhancing language\nmodel pre-training with dictionary. arXiv:2110.06490,\n2021.\n[Zhang et al., 2017] Yuhao Zhang, Victor Zhong, Danqi\nChen, Gabor Angeli, and Christopher D. Manning.\nPosition-aware attention and supervised data improve slot\nﬁlling. In EMNLP, pages 35–45, 2017.\n[Zhang et al., 2019] Zhengyan Zhang, Xu Han, Zhiyuan Liu,\nXin Jiang, Maosong Sun, and Qun Liu. ERNIE: Enhanced\nlanguage representation with informative entities. In ACL,\npages 1441–1451, 2019.\n[Zhao et al., 2020] Chen Zhao, Chenyan Xiong, Corby Ros-\nset, Xia Song, Paul Bennett, and Saurabh Tiwary.\nTransformer-xh: Multi-evidence reasoning with extra hop\nattention. In ICLR, 2020.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4092"
}