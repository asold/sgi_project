{
    "title": "Evaluating AI Proficiency in Nuclear Cardiology: Large Language Models take on the Board Preparation Exam",
    "url": "https://openalex.org/W4400742652",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5093714242",
            "name": "Valerie Builoff",
            "affiliations": [
                "Cedars-Sinai Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2789828716",
            "name": "Aakash Shanbhag",
            "affiliations": [
                "Cedars-Sinai Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2803874639",
            "name": "Robert JH. Miller",
            "affiliations": [
                "Cedars-Sinai Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2151404425",
            "name": "Damini Dey",
            "affiliations": [
                "Cedars-Sinai Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2808753923",
            "name": "Joanna X. Liang",
            "affiliations": [
                "Cedars-Sinai Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2123058493",
            "name": "Kathleen Flood",
            "affiliations": [
                "American Nuclear Society"
            ]
        },
        {
            "id": "https://openalex.org/A2169610124",
            "name": "Jamieson M Bourque",
            "affiliations": [
                "University of Virginia Health System"
            ]
        },
        {
            "id": "https://openalex.org/A48204586",
            "name": "Panithaya Chareonthaitawee",
            "affiliations": [
                "Mayo Clinic in Arizona"
            ]
        },
        {
            "id": "https://openalex.org/A2116633371",
            "name": "Lawrence M. Phillips",
            "affiliations": [
                "New York University"
            ]
        },
        {
            "id": "https://openalex.org/A2777100658",
            "name": "Piotr J Slomka",
            "affiliations": [
                "Cedars-Sinai Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A5093714242",
            "name": "Valerie Builoff",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2789828716",
            "name": "Aakash Shanbhag",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2803874639",
            "name": "Robert JH. Miller",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2151404425",
            "name": "Damini Dey",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2808753923",
            "name": "Joanna X. Liang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2123058493",
            "name": "Kathleen Flood",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2169610124",
            "name": "Jamieson M Bourque",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A48204586",
            "name": "Panithaya Chareonthaitawee",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2116633371",
            "name": "Lawrence M. Phillips",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2777100658",
            "name": "Piotr J Slomka",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4384561707",
        "https://openalex.org/W4323050332",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4380291159",
        "https://openalex.org/W4381092249",
        "https://openalex.org/W4385827730",
        "https://openalex.org/W4387326101",
        "https://openalex.org/W4376640725",
        "https://openalex.org/W4394767601",
        "https://openalex.org/W4392621058",
        "https://openalex.org/W4393946650",
        "https://openalex.org/W4392397923",
        "https://openalex.org/W4393352397",
        "https://openalex.org/W4396831262",
        "https://openalex.org/W4283076002",
        "https://openalex.org/W3037831233",
        "https://openalex.org/W4291802080",
        "https://openalex.org/W4398244881",
        "https://openalex.org/W4400456285",
        "https://openalex.org/W2347200775",
        "https://openalex.org/W4403134104"
    ],
    "abstract": "ABSTRACT Background Previous studies evaluated the ability of large language models (LLMs) in medical disciplines; however, few have focused on image analysis, and none specifically on cardiovascular imaging or nuclear cardiology. Objectives This study assesses four LLMs - GPT-4, GPT-4 Turbo, GPT-4omni (GPT-4o) (Open AI), and Gemini (Google Inc.) - in responding to questions from the 2023 American Society of Nuclear Cardiology Board Preparation Exam, reflecting the scope of the Certification Board of Nuclear Cardiology (CBNC) examination. Methods We used 168 questions: 141 text-only and 27 image-based, categorized into four sections mirroring the CBNC exam. Each LLM was presented with the same standardized prompt and applied to each section 30 times to account for stochasticity. Performance over six weeks was assessed for all models except GPT-4o. McNemar’s test compared correct response proportions. Results GPT-4, Gemini, GPT4-Turbo, and GPT-4o correctly answered median percentiles of 56.8% (95% confidence interval 55.4% - 58.0%), 40.5% (39.9% - 42.9%), 60.7% (59.9% - 61.3%) and 63.1% (62.5 – 64.3%) of questions, respectively. GPT4o significantly outperformed other models (p=0.007 vs. GPT-4Turbo, p&lt;0.001 vs. GPT-4 and Gemini). GPT-4o excelled on text-only questions compared to GPT-4, Gemini, and GPT-4 Turbo (p&lt;0.001, p&lt;0.001, and p=0.001), while Gemini performed worse on image-based questions (p&lt;0.001 for all). Conclusion GPT-4o demonstrated superior performance among the four LLMs, achieving scores likely within or just outside the range required to pass a test akin to the CBNC examination. Although improvements in medical image interpretation are needed, GPT-4o shows potential to support physicians in answering text-based clinical questions.",
    "full_text": null
}