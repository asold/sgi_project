{
    "title": "Unraveling the Dilemma of AI Errors: Exploring the Effectiveness of Human and Machine Explanations for Large Language Models",
    "url": "https://openalex.org/W4394782008",
    "year": 2024,
    "authors": [
        {
            "id": null,
            "name": "Pafla, Marvin",
            "affiliations": [
                "University of Waterloo"
            ]
        },
        {
            "id": "https://openalex.org/A3163669449",
            "name": "Larson, Kate",
            "affiliations": [
                "University of Waterloo"
            ]
        },
        {
            "id": "https://openalex.org/A2614835791",
            "name": "Hancock, Mark",
            "affiliations": [
                "University of Waterloo"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2795530988",
        "https://openalex.org/W3010300896",
        "https://openalex.org/W3199643281",
        "https://openalex.org/W4365999041",
        "https://openalex.org/W3163443091",
        "https://openalex.org/W2981731882",
        "https://openalex.org/W2950504429",
        "https://openalex.org/W4243636675",
        "https://openalex.org/W3001062618",
        "https://openalex.org/W3159250634",
        "https://openalex.org/W3093718041",
        "https://openalex.org/W2594475271",
        "https://openalex.org/W2995983596",
        "https://openalex.org/W3202538470",
        "https://openalex.org/W2896487960",
        "https://openalex.org/W4376988296",
        "https://openalex.org/W3209901185",
        "https://openalex.org/W2962772482",
        "https://openalex.org/W4200580215",
        "https://openalex.org/W4289123308",
        "https://openalex.org/W3128621548",
        "https://openalex.org/W2044140042",
        "https://openalex.org/W3016099278",
        "https://openalex.org/W1994606570",
        "https://openalex.org/W3032816739",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W3138819813",
        "https://openalex.org/W2963095307",
        "https://openalex.org/W3197347140",
        "https://openalex.org/W4387344927",
        "https://openalex.org/W2973136764",
        "https://openalex.org/W4365998993",
        "https://openalex.org/W2945295328",
        "https://openalex.org/W3163411042",
        "https://openalex.org/W2563296158",
        "https://openalex.org/W2282821441",
        "https://openalex.org/W2945976633",
        "https://openalex.org/W3132191748",
        "https://openalex.org/W4280639483",
        "https://openalex.org/W3005086430",
        "https://openalex.org/W4288083725",
        "https://openalex.org/W3095444354",
        "https://openalex.org/W2604980658",
        "https://openalex.org/W3207598588",
        "https://openalex.org/W3164846705",
        "https://openalex.org/W4298235707",
        "https://openalex.org/W2942444880",
        "https://openalex.org/W2964125683",
        "https://openalex.org/W6637031373",
        "https://openalex.org/W3032875465",
        "https://openalex.org/W2999637955",
        "https://openalex.org/W3133543405",
        "https://openalex.org/W3101792976",
        "https://openalex.org/W3103751997",
        "https://openalex.org/W1583837637",
        "https://openalex.org/W3104149808",
        "https://openalex.org/W3134751001"
    ],
    "abstract": "The field of eXplainable artificial intelligence (XAI) has produced a plethora of methods (e.g., saliency-maps) to gain insight into artificial intelligence (AI) models, and has exploded with the rise of deep learning (DL). However, human-participant studies question the efficacy of these methods, particularly when the AI output is wrong. In this study, we collected and analyzed 156 human-generated text and saliency-based explanations collected in a question-answering task (N=40) and compared them empirically to state-of-the-art XAI explanations (integrated gradients, conservative LRP, and ChatGPT) in a human-participant study (N=136). Our findings show that participants found human saliency maps to be more helpful in explaining AI answers than machine saliency maps, but performance negatively correlated with trust in the AI model and explanations. This finding hints at the dilemma of AI errors in explanation, where helpful explanations can lead to lower task performance when they support wrong AI predictions.",
    "full_text": "Unraveling the Dilemma of AI Errors\nExploring the Effectiveness of Human and Machine Explanations for Large Language Models\nMarvin Pafla\nmpafla@uwaterloo.ca\nUniversity of Waterloo\nWaterloo, Ontario, Canada\nKate Larson\nkate.larson@uwaterloo.ca\nUniversity of Waterloo\nWaterloo, Ontario, Canada\nMark Hancock\nmark.hancock@uwaterloo.ca\nUniversity of Waterloo\nWaterloo, Ontario, Canada\nABSTRACT\nThe field of eXplainable artificial intelligence (XAI) has produced a\nplethora of methods (e.g., saliency-maps) to gain insight into artifi-\ncial intelligence (AI) models, and has exploded with the rise of deep\nlearning (DL). However, human-participant studies question the effi-\ncacy of these methods, particularly when the AI output is wrong. In\nthis study, we collected and analyzed 156 human-generated text and\nsaliency-based explanations collected in a question-answering task\n(ùëÅ = 40) and compared them empirically to state-of-the-art XAI ex-\nplanations (integrated gradients, conservative LRP, and ChatGPT)\nin a human-participant study (ùëÅ = 136). Our findings show that\nparticipants found human saliency maps to be more helpful in ex-\nplaining AI answers than machine saliency maps, but performance\nnegatively correlated with trust in the AI model and explanations.\nThis finding hints at the dilemma of AI errors in explanation, where\nhelpful explanations can lead to lower task performance when they\nsupport wrong AI predictions.\nCCS CONCEPTS\n‚Ä¢ Human-centered computing ‚ÜíEmpirical studies in HCI ;\nNatural language interfaces ; User studies; Empirical studies in vi-\nsualization; Heat maps ; ‚Ä¢ Computing methodologies ‚ÜíNatural\nlanguage processing.\nKEYWORDS\nexplainable artificial intelligence (XAI), large language models\n(LLMs), saliency maps, text-explanations, local explanations, post-\nhoc explanations, question-answering task, Stanford Question An-\nswering Dataset (SQuAD 1.1v), explainability, dilemma of AI errors,\nexplanation confirmation bias, explanation evaluation, human ex-\nplanation, machine explanation, human-participant study\nACM Reference Format:\nMarvin Pafla, Kate Larson, and Mark Hancock. 2024. Unraveling the Dilemma\nof AI Errors: Exploring the Effectiveness of Human and Machine Expla-\nnations for Large Language Models. In Proceedings of the CHI Conference\non Human Factors in Computing Systems (CHI ‚Äô24), May 11‚Äì16, 2024, Hon-\nolulu, HI, USA. ACM, New York, NY, USA, 20 pages. https://doi.org/10.1145/\n3613904.3642934\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nThis is the author‚Äôs version of the work. It is posted here for your personal use. Not\nfor redistribution. The definitive Version of Record was published in Proceedings of the\nCHI Conference on Human Factors in Computing Systems (CHI ‚Äô24), May 11‚Äì16, 2024,\nHonolulu, HI, USA , https://doi.org/10.1145/3613904.3642934.\n1 INTRODUCTION\nWith the rise of deep learning (DL) [49] and large language models\n(LLMs), artificial intelligence (AI) has become ever more prominent\nin our lives, prompting a growing demand to make AI more ex-\nplainable [44, 50, 53, 86, 89]. To fill this demand, the field of XAI has\nproduced numerous metrics and visualizations (e.g., [32, 48, 69]).\nHowever, there are doubts that these machine explanations, or XAI\nexplanations (e.g., saliency maps), are actually able to explain LLMs\n(and other neural nets) and their behaviour [ 50, 53, 86], and it is\nunclear how people interpret and use them. XAI has been criticized\nfor cherry-picking explanations for correct predictions [67] and a\nlack of proper control conditions that compare XAI explanations\nwith other or no explanations [53, 54, 79]. There is a general lack\nof thorough human-participant studies [53, 70].\nTo address existing limitations, this research empirically assesses\nstate-of-the-art machine explanations for LLMs through a study in-\nvolving human participants, with a specific focus on saliency maps\nas a primary mechanism for local explanations in LLMs. Despite\nprevious studies on saliency maps and feature-based visualizations\nin classification tasks with incorrect predictions (e.g., [13, 41, 90]),\nthis investigation pioneers the examination of saliency maps in\ntext generation tasks, considering both correct and incorrect predic-\ntions. Moreover, while text explanations often come from human\nexperts (e.g., [13, 41]), this study enhances the human-centered ex-\nplainable AI (HCXAI) framework proposed by Morrison et al. [57]\nby gathering 156 human-generated explanations (both saliency-\nand text-based) prior to comparing them with XAI explanations in\na comprehensive human-participant study. This methodology en-\nables a deeper exploration of human explanation strategies within\nthe context of a question-answering task, utilizing the SQuAD 1.1v\ndataset [64].\nOur findings show that both performance and subjective ex-\nperiences with AI are significantly influenced by the accuracy of\nAI-generated answers. Specifically, explanations for correct answers\nwere perceived as more helpful, of higher quality, and required less\nmental effort compared to those for incorrect answers. In terms of\nsaliency-based explanations, human-generated explanations were\ndeemed more helpful than those produced by machines (conser-\nvative LRP and integrated gradients (IG)). However, when consid-\nering general satisfaction with the explanations or trust in the AI\nacross various conditions (encompassing both correct and incor-\nrect answers), these differences between conditions disappeared.\nThis tendency highlights the need to evaluate interaction effects:\nparticipants found human saliency maps and answer-only maps\nto be of higher quality than maps created with the help of IG for\ncorrect predictions .\narXiv:2404.07725v1  [cs.HC]  11 Apr 2024\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Pafla, et al.\nWhile we did not reveal significant differences in performance\nscores across conditions, we found that there was a strong correla-\ntion between trust, satisfaction, and performance scores, indicating\nthat participants who were less trusting of the AI and less satis-\nfied with the explanations generally performed better. While this\nconnection might be supported by the AI‚Äôs designed accuracy rate\nof 50% (and hence required a certain distrust in the AI to perform\nwell), it might explain why explanations generated with integrated\ngradients‚Äîwhich achieved the lowest scores for helpfulness, qual-\nity, satisfaction, and trust‚Äîlead participants to achieve the highest\nperformance of all conditions. Indeed, we found that participants in\ntwo control conditions without any explanations did achieve better\nperformance than common explanation types (conservative LRP,\nhuman saliency, and ChatGPT explanations). In this context, we\ndiscuss the the dilemma of incorrect AI predictions and the dangers\nof explanation confirmation bias.\nWe make four contributions in this paper:\n(1) We collected and analyzed 156 human-generated saliency-\nbased and text explanations from an online, crowd-sourcing\ntask (ùëÅ = 40). Our analysis show that human saliency-based\nexplanations have little overlap with machine explanations\nand that human text explanations mostly copied or para-\nphrased source text (text extractions).\n(2) In a large human-participant study online ( ùëÅ = 136), we\nevaluated different human and machine saliency-based and\ntext explanations. We found that the correctness of AI predic-\ntions had a strong, significant effect on all measures (perfor-\nmance, time, quality, helpfulness, and mental effort), machine\nsaliency maps were significantly less helpful than human\nsaliency maps, participants trusted text extractions more\nthan ChatGPT explanations, and that measures of explana-\ntion satisfaction, trust in the AI, and explanation helpfulness\nwere negatively correlated with performance scores.\n(3) We discuss the dilemma of machine explanations: ‚Äúgood‚Äù ex-\nplanations for incorrect AI predictions can lead to negative\neffects such as over-reliance on AI resulting in decreased per-\nformance in collaborative human-AI tasks. Furthermore, we\ndiscuss how participants might try to match their intuition\nof relevance with saliency maps themselves as a heuristic\nmechanism to evaluate explanations. We discuss the dangers\nof explanation confirmation bias.\n(4) We provide recommendations for designers and researchers\nto use saliency maps and mitigate the dangers of explanation\nconfirmation bias.\n2 RELATED RESEARCH\nWe describe the related work that introduces the need for XAI,\nresearch on the difficulties with explaining deep learning systems,\nand work on evaluating explanations in XAI.\n2.1 Explaining AI\nThe field of eXplainable artificial intelligence (XAI), tasked with\nexplaining, or providing insight, into AI models, has grown tremen-\ndously since DARPA announced its XAI program in 2017 [ 8, 14,\n36, 83]. This growth has introduced a plethora of research papers\nthat have investigated and reviewed the facets and desiderata of\n‚Äúexplainability‚Äù (e.g., interpretability, understandability, or trans-\nparency) [1, 14, 23, 53, 75, 83], distinguished between inherently\ntransparent models (i.e., ‚Äúglassbox‚Äù models like decision trees or\nsupport vector machines (SVMs)) and uninterpretable ones (i.e.,\n‚Äúopaque‚Äù models like deep neural nets) [44, 48, 50, 86], and has pro-\nvided different types of explanation, those that explain the model\nitself (i.e., global) or its prediction (i.e., local) [25, 29]. The timing\nfor this development is urgent as decisions made by more power-\nful AI can be highly consequential for people (e.g., in healthcare\ndiagnoses or financial loan applications) [ 50]. The necessity for\nexplainability is further underscored by legal requirements like\nthe General Data Protection Regulation (GDPR) [84], which man-\ndates that AI decisions be interpretable to ensure transparency and\naccountability.\n2.1.1 Deep Learning. Through the advent of the field of deep learn-\ning [49], and its ability to train large neural networks using large\namounts of computational resources, much attention has been paid\nto explaining deep neural nets [50, 68, 69]. To make these opaque\nmodels more explainable, research has provided text explanations,\nvisualizations, and explanations by examples [25, 50, 68, 69], or has\ntried to find simpler surrogate models (e.g., linear models) with\nsimilar accuracy/performance but inherent interpretability [25, 68].\nIn the realm of natural language processing (NLP), the domain of\nthis study, Danilevsky et al . [25] list different explanation tech-\nniques (e.g., feature importance or surrogate model), operations\n(e.g., gradient-based saliency or attention), and visualizations (e.g.,\nsaliency maps or natural language explanations).\nWhile attention mechanisms, a key feature of the Transformer\narchitecture [80], have been suggested as a way to inspect more\ncomplex models [11, 60, 68, 82], the use of attention as explanation\nremains controversial [28, 42, 78, 87]. However, given the focus\non input features and their importance to the overall prediction,\nresearch has suggested using saliency methods instead [15]. These\nmethods rely on gradients (e.g., integrated gradients [76] or layer-\nwise relevance propagation (LRP) [ 56]) and usually fall into the\nclass of post-hoc explanations that are produced after the model‚Äôs\nprediction/generation [25, 65, 68]. For example, in the domain of\nNLP, words in a movie review, that contributed positively or nega-\ntively to the sentiment of the review, can be highlighted [9]. In this\nresearch, our goal is to evaluate the effectiveness of saliency-based\nexplanations in human-participant studies, besides text explana-\ntions, which have become the predominant method to explain large\nlanguage models [25].\n2.2 Difficulties with Explanation in Deep\nLearning\nDespite these efforts in the field of XAI and the hope that explana-\ntion can mitigate human biases [85], there is doubt that current AI\nmodels like neural nets can provide explanations: Lipton [50] ar-\ngues that ‚Äútoday‚Äôs predictive models are not capable of reasoning at\nall‚Äù because AI models like neural networks only model correlation,\nand thus are unable to provide causal explanations [1, 37, 38, 50, 86].\nHowever, the social sciences, according to Miller [53], tell us that\npeople desire causes to explain events, not associative or statisti-\ncal relationships, as is common in the field of XAI. There is some\nUnraveling the Dilemma of AI Errors CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nevidence that this mismatch between people‚Äôs expectation of ex-\nplanation and XAI explanations can lead to situations where XAI\nexplanations are misused, misinterpreted, and over-trusted espe-\ncially when facing the danger of information overload [44].\nIn the context of large language models, the application of tra-\nditional XAI techniques is limited due to the models‚Äô complex,\nsub-symbolic nature. Studies such as Poursabzi-Sangdeh et al. [62]\nand Sokol and Flach [75] focus on simpler, inherently interpretable\nmodels (e.g., linear regression or decision trees) that, unlike neural\nnetworks, provide transparent explanation mechanisms. Addition-\nally, model-agnostic approaches like SHAP [51] and LIME [66], as\nwell as the weight of evidence framework, encounter difficulties\nin high-dimensional feature spaces typical of text generation tasks\n[6, 24]. Vilone and Longo [83] find that the vast majority of articles\nusing neural nets compared different explanation methodologies\nfor the classification of either text or images, rather than their gen-\neration. Against this backdrop, our study evaluates and contrasts\nhuman-generated explanations with those produced by machines in\na question-answering setting, seeking effective explanation method-\nologies for text generation.\nIn addition to some technical concerns around the efficacy of\nXAI explanations like saliency maps, attention, or LIME/SHAP (e.g.,\nAdebayo et al. [2], Jain and Wallace [42], Slack et al. [73]), some\nresearch views XAI explanations methods generally more critically\n[22, 31, 67, 83], especially when moving away from concerns of\nfaithfulness (i.e., is feature importance consistent with the internal\ndecision model?) towards plausibility (i.e., does feature importance\nalign with human intuition?) [27]. Rudin [67] argues that it is not\nenough to highlight which parts of the input were relevant in\nproducing an output as these ‚Äúexplanations‚Äù do not show how the\ninputs were used to compute the output [67]. In a similar vein, Fok\nand Weld [33] states that explanations based on feature importance\nmay provide some indication of the relevance of an individual\nfeature, but typically do not allow decision makers to verify AI\nrecommendations in decision making tasks.\nRudin [67] highlights a critical issue in XAI: explanations often\npertain only to correct predictions, potentially misleading users\nwith undue confidence. The necessity of considering AI prediction\naccuracy in research designs has been emphasized to assess if ex-\nplanations can aid in discerning AI errors [5]. Initial findings, such\nas those by Morrison et al. [57], indicate that the perceived help-\nfulness of human explanations diminishes when associated with\nincorrect predictions or explanations. Moreover, Bansal et al. [13]\nfound that explanations, irrespective of their correctness, could\nlead to higher acceptance rates of AI predictions, sometimes even\nenticing users to accept incorrect predictions as shown in studies\n(e.g., [20, 41, 77]). Zhang et al. [90] argue that local explanations\nmight not be suitable for trust calibration. Our study explores the\neffectiveness of human versus machine-generated explanations in\nhelping users discern between correct and incorrect AI predictions\nin text-based applications, like question-answering, by evaluating\ntheir impact through subjective and performance metrics relative\nto AI answer accuracy.\nBecause of the difficulty in explaining individual predictions,\nexplainability tools have been considered ‚Äúa false hope‚Äù to open\nopaque boxes in AI [ 34] and authors have called to stop the use\nof opaque box models in high-stakes scenarios altogether [67] or\nto rely on established, randomized trials to determine the effec-\ntiveness of AI models [34]. These calls might be supported by the\nfact that XAI explanations have been used to deceive people to\nobstruct or force users to take certain actions [22], or have caused\nother, more unintended, downstream effects (like the false sense\nof confidence mentioned above). Ehsan and Riedl [31] name these\neffects ‚Äúexplainability pitfalls‚Äù [31].\n2.3 Evaluating XAI Explanations and Human\nExplanations\nTo investigate the efficacy of XAI explanation, there has been\na strong call for more human-computer interaction (HCI) stud-\nies with participants using realistic and interactive prototypes\n[1, 24, 29, 44, 53, 62, 72, 83, 86, 89]. Unfortunately, evaluations in the\nfield of AI often rely on proxy scores (e.g., performance measures\nsuch as F1) or perturbation analysis (e.g., Samek et al. [69]), rather\nthan human ratings or qualitative methods, to evaluate the quality\nof explanations [ 72]. For example, Schmidt and Biessmann [71]\n‚Äúdefine‚Äù trust to be a combination of accuracy and performance\nscores and Edmonds et al. [30] equated the trust participants have\nin explanations with their ability to predict a robot‚Äôs next action.\nHowever, those scores can diverge from human ratings (e.g.,\nperceived consistency) [72] and fail to capture trust‚Äôs ‚Äúmultidisci-\nplinary and multifaceted nature‚Äù [81]. Vereschak et al. [81] contend\nthat trust, conceptualized as an attitude rooted in vulnerability and\npositive expectations rather than mere behavior, is inadequately as-\nsessed in many studies due to the absence of realistic outcomes and\nthe lack of emphasis on participants‚Äô attitudes towards AI systems.\nThey also highlight the potential for priming effects from setting\npositive expectations and the significant role of first impressions in\ntrust formation [81], which can, under the guise of transparency,\nbe exploited by untrustworthy AI to deceive participants about its\ncompetence and mislead users into accepting flawed outputs [12].\nTo address these challenges and mitigate AI over-reliance, Bu√ßinca\net al. [20] advocate for cognitive forcing functions that prompt\nusers to critically evaluate AI decisions.\nFurthermore, XAI explanations are seldom benchmarked against\nhuman explanations, including preferred formats like contrastive\nexplanations [53]. Predictions labeled as ‚Äúhuman-generated‚Äù are\noften viewed as more fair and trustworthy than those labeled as\n‚ÄúAI-generated‚Äù [55], yet XAI faces challenges in emulating human-\nlike explanations, particularly in generating counterfactuals for\ntext generation (not in classification such as in Mothilal et al. [58])\nwhich require complex causal reasoning [ 37, 38]. Morrison et al.\n[57] found that causal human explanations can mitigate AI over-\nreliance and enhance decision-making accuracy, despite the risk\nof incorrect rationalizations. Unlike previous studies, our research\nconducts a direct comparison of XAI and human explanations,\nfocusing on saliency maps, and explores the efficacy of human text\nand ChatGPT explanations.\nMethodological issues complicate XAI evaluations, including a\nfrequent omission of explanation design limitations [ 8] and the\nabsence of proper control groups in many studies [ 53, 54, 79]. A\nmeta-review highlighted this challenge, identifying only nine suit-\nable studies from hundreds due to necessary criteria like measuring\nhuman performance without AI or between-subjects designs, and\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Pafla, et al.\nfound no significant effect on classification performance when XAI\nexplanations were provided, compared to just AI assistance [70].\nThe diversity of explanations and metrics adds to the complexity,\nas effectiveness varies widely [72, 75]. The integration of human-\ncentered and functionality-grounded evaluation metrics is rare,\nyet crucial for understanding the discrepancies in XAI effective-\nness [19, 91]. Our study addresses these complexities by directly\ncomparing human and XAI explanations across multiple question-\nanswering tasks including both correct and incorrect answers, ex-\namining the interplay between subjective and performance metrics.\n2.4 Research Motivation\nOur research delves into the disparity between human explana-\ntions and state-of-the-art XAI explanations by leveraging human-\ncentred XAI (HCXAI). Following Morrison et al. [57]‚Äôs approach,\nwe first gather human explanations in question-answering tasks to\nunderstand what type of explanations humans produce. We then\nexpand on their earlier work and compare these human explana-\ntions (human saliency and human text explanations) to different\nXAI-generated explanations. The aim is to assess and compare the\nefficacy of human and XAI explanations, taking into consideration\nthe accuracy of the predictions involved.\nFurthermore, existing studies, such as Mok et al. [55]‚Äôs, show a\npreference for human-generated predictions over AI‚Äôs, even when\nthey are identical. However, it‚Äôs not yet clear how this preference\nchanges when both human and XAI explanations for different pre-\ndictions are framed as AI-generated. Moreover, while many studies\nprovide feature-based visualizations or saliency maps (e.g., rele-\nvance of certain variables as in Jacobs et al. [41], Zhang et al. [90]),\nthey are usually focused on classification tasks. To the best of our\nknowledge, our study describes the first study investigating XAI\nexplanations with incorrect predictions in tasks including text gen-\neration like question-answering (Bansal et al. [13] use expert ex-\nplanations for their multiple-choice task and saliency maps for a\nclassification task). Lastly, given the lack of studies that include\nproper control conditions and both subjective and performative\nmetrics our goal is to thoroughly evaluate current SOTA XAI ex-\nplanation techniques in text generation.\n3 STUDY DESIGN\nIn this study, our goal was to compare the efficacy of human and\nmachine explanations of language predictions. To collect and gener-\nate these explanations, we needed to ground our study in a specific\ntask that requires specific explanation design. In this section, we\ndescribe our task and its explanation goals, and how we developed\nappropriate explanations to fulfill those goals. At the end of this\nsection, we list our research questions and provide an overview of\nthe study.\n3.1 Task Design\nWhile explanations in text classification (e.g., sentiment analysis,\nwhich predicts text to have either a positive or negative sentiment)\nhave used saliency-based methods to highlight which words con-\ntribute most to the classification (e.g., Arras et al. [9]), and these\nmethods can help humans understand model predictions by discov-\nering its biases (e.g., [4]), it becomes more difficult to determine\nthe role and purpose of explanation in text generation. Here, ex-\nplaining why an AI produced a specific text after a prompt entails\naddressing broader and more open-ended considerations that may\nextend beyond the immediate task or human-AI interaction, such\nas the training data‚Äôs influence on the generated content.\nIn our work, we focus on question-answering, a popular and\nchallenging benchmark test, where what is to be explained (i.e., an\nanswer) is text, rather than a numerical value or a label. Specifically,\nwe rely on the SQuAD 1.1v dataset for study tasks [ 64], which\nrequires AI and humans to produce an answer given a question\nabout a source text. While the SQuAD is an extractive dataset in\nwhich the answer to the question lies directly in the source text, we\nanticipated the same challenges explanation faces in text generation\n(e.g., saliency maps not mapping neatly to sentiment scores as in\nclassification). Nonetheless, the extractive nature of answers limits\nthe scope of answers that can be given which can simplify study\ndesign. Furthermore, explanations that highlight source text can\nhelp participants notice important features which serve as means\nto verify extracted answers [5, 33]. An example question and source\ntext (in addition to an AI answer and explanation) of the SQuAD\ncan be found in the task overview in Figure 1a.\nIn previous research, explanation goals are often implicit and\nabstract (e.g., ‚Äúexplaining model predictions‚Äù or ‚Äúexplaining model\narchitectures‚Äù) or simply focus on performance measures [67, 72].\nBy grounding our evaluation in question-answering we explicitly\nformulate our explanation goals (as recommended by Mohseni et al.\n[54]):\n(1) Our first goal is to explain why the answer to a question is\ncorrect. Consequently, to achieve this verification the focus\nof our study is on local explanations that can help users\ndetermine whether the AI actually produced the right answer\nand achieve performance which is higher than that of the\nparticipant or AI alone [33].\n(2) Our second goal isincreasing the trust and satisfaction of users .\nWe expect users are more satisfied with AI explanations and\nare able to trust AI more if the AI is able to produce good\nexplanations.\n3.2 Explanation Design\nSurvey literature proposes various explanation methodologies (e.g.,\nfeature-importance, surrogate models, raw-examples, and natu-\nral language explanations) for clarifying deep learning models‚Äô\ndecisions [25, 35]. However, the suitability of surrogate models\nand example-driven explanations for large language models and\ntext generation tasks is debatable. Surrogate models, exemplified\nby LIME, predominantly address classification tasks due to limi-\ntations in managing extensive feature spaces [ 83]. Furthermore,\nlarge language models are powerful because of their size which\nmakes finding a simpler model counter-intuitive [18]. Despite sug-\ngestions for using raw examples for neural nets [ 25], we argue\nthese strategies are again more aligned with classification, con-\nsidering the absence of a structured knowledge base and reliance\non unstructured data in our context. Our research incorporates\nnatural language explanations generated via ChatGPT [3] to study\nthe efficacy of text explanations, yet our primary focus remains\nUnraveling the Dilemma of AI Errors CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\n8/23/23, 1:56 PM Preview - Qualtrics Survey | Qualtrics Experience Management\nhttps://uwaterloo.yul1.qualtrics.com/jfe/preview/previewId/5deda29e-4d83-48c1-82c3-ff80ffad4d63/SV_bloMTtEB8WR3dzw/BL_08Spyd4mUWCS1Zc?Q_SurveyVersionID=current 1/1\nPowered by QualtricsA\nQuestion:\nWhat title was given to Genghis Khan posthumously?\nSource text:\nAs a result, by 1206 Tem√ºjin had managed to unite or subdue the Merkits,\nNaimans, Mongols, Keraites, Tatars, Uyghurs, and other disparate smaller tribes\nunder his rule. It was a monumental feat for the \"Mongols\" (as they became known\ncollectively). At a Khuruldai, a council of Mongol chiefs, Tem√ºjin was\nacknowledged as \"Khan\" of the consolidated tribes and took the new title \"Genghis\nKhan\". The title Khagan was not conferred on Genghis until after his death, when\nhis son and successor, √ñgedei, took the title for himself and extended it\nposthumously to his father (as he was also to be posthumously declared the\nfounder of the Yuan dynasty). This unification of all confederations by Genghis\nKhan established peace between previously warring tribes and a single political\nand military force under Genghis Khan.\nAI answer:\n \nFounder of the Yuan dynasty.\nAI explanation:\n \nAs a result, by 1206 Tem√ºjin had managed to unite or subdue the Merkits,\nNaimans, Mongols, Keraites, Tatars, Uyghurs, and other disparate smaller tribes\nunder his rule. It was a monumental feat for the \" Mongols \" (as they became\nknown collectively). At a Khuruldai, a council of Mongol chiefs, Tem√ºjin was\nacknowledged as \"Khan\" of the consolidated tribes and took the new title \"Genghis\nKhan\". The title Khagan was not conferred on Genghis until after his death, when\nhis son and successor, √ñgedei, took the title for himself and extended it\nposthumously to his father (as he was also to be posthumously declared the\nfounder of the Yuan dynasty). This unification of all confederations by Genghis\nKhan established peace between previously warring tribes and a single political\nand military force under Genghis Khan.\nDo you think the AI answer is correct?\nHow confident are you in your decision?\nPlease rate the quality of the AI explanation.\nPlease rate whether you found the AI explanation helpful.\nPlease rate the mental effort required to decide if the AI answer is correct.\nTiming\nYes\nNo, please provide the correct answer:\n0 10 20 30 40 50 60 70 80 90 100\n \n Low Quality\nSomewhat\nLow Quality\nNeither Low\nNor High\nQuality\nSomewhat\nHigh\nQuality\nHigh\nQuality\nPlease provide a\nsubjective rating for\nthe quality of the\nexplanation.\n Disagree\nSomewhat\nDisagree\nNeither\nDisagree Or\nAgree\nSomewhat\nAgree Agree\nThe explanation helps\nme to understand why\nthe model came up\nwith its answer.\nThe explanation helps\nme to decide if the\nanswer is correct.\n \nVery,\nvery\nlow\nmental\neffort\nVery\nlow\nmental\neffort\nLow\nmental\neffort\nRather\nlow\nmental\neffort\nNeither\nlow nor\nhigh\nmental\neffort\nRather\nhigh\nmental\neffort\nHigh\nmental\neffort\nVery\nhigh\nmental\neffort\nVery,\nvery\nhigh\nmental\neffort\nMental\neffort\nThese page timer metrics will not be displayed to the recipient.\nFirst Click 4.798 seconds\nLast Click 4.798 seconds\nPage Submit0 seconds\nClick Count1 clicks\n‚Üí\nShare PreviewRestart Block  \n Tools  Óòô\n(a) Machine saliency map generated by con-LRP\n9/14/23, 10:02 PM Preview - Qualtrics Survey | Qualtrics Experience Management\nhttps://uwaterloo.yul1.qualtrics.com/jfe/preview/previewId/5deda29e-4d83-48c1-82c3-ff80ffad4d63/SV_bloMTtEB8WR3dzw/BL_2nKxVSRJkLNqUaq?Q_SurveyVersionID= 1/1\nPowered by QualtricsA\nQuestion:\nWhat is the upper range of annual fees for non-boarding students in British public\nschools?\nSource text:\nPrivate schools generally prefer to be called independent schools, because of their\nfreedom to operate outside of government and local government control. Some of\nthese are also known as public schools. Preparatory schools in the UK prepare\npupils aged up to 13 years old to enter public schools. The name \"public school\" is\nbased on the fact that the schools were open to pupils from anywhere, and not\nmerely to those from a certain locality, and of any religion or occupation.\nAccording to The Good Schools Guide approximately 9 per cent of children being\neducated in the UK are doing so at fee-paying schools at GSCE level and 13 per\ncent at A-level. Many independent schools are single-sex (though this is becoming\nless common). Fees range from under ¬£3,000 to ¬£21,000 and above per year for\nday pupils, rising to ¬£27,000+ per year for boarders. For details in Scotland, see\n\"Meeting the Cost\".\nAI answer:\n \nFees range from under ¬£3,000 to ¬£21,000 and above per year for day pupils.\nAI explanation:\n \nThe fees rise to ¬£27,000+ per year for boarders.\nDo you think the AI answer is correct?\nHow confident are you in your decision?\nPlease rate the quality of the AI explanation.\nPlease rate whether you found the AI explanation helpful.\nPlease rate the mental effort required to decide if the AI answer is correct.\nTiming\nYes\nNo, please provide the correct answer:\n0 10 20 30 40 50 60 70 80 90 100\n \n Low Quality\nSomewhat\nLow Quality\nNeither Low\nNor High\nQuality\nSomewhat\nHigh\nQuality\nHigh\nQuality\nPlease provide a\nsubjective rating for\nthe quality of the\nexplanation.\n Disagree\nSomewhat\nDisagree\nNeither\nDisagree Or\nAgree\nSomewhat\nAgree Agree\nThe explanation helps\nme to understand why\nthe model came up\nwith its answer.\nThe explanation helps\nme to decide if the\nanswer is correct.\n \nVery,\nvery\nlow\nmental\neffort\nVery\nlow\nmental\neffort\nLow\nmental\neffort\nRather\nlow\nmental\neffort\nNeither\nlow nor\nhigh\nmental\neffort\nRather\nhigh\nmental\neffort\nHigh\nmental\neffort\nVery\nhigh\nmental\neffort\nVery,\nvery\nhigh\nmental\neffort\nMental\neffort\nThese page timer metrics will not be displayed to the recipient.\nFirst Click 0 seconds\nLast Click 0 seconds\nPage Submit0 seconds\nClick Count0 clicks\n‚Üí\nShare PreviewRestart Block  \n Tools  Óòô (b) Text explanation provided by human participant\nFigure 1: Screen shots of two question-answering tasks. Participants were exposed to questions, source texts, answers, and\nexplanations, and asked to evaluate the answers with the help of the explanation and source text. Despite presenting both human\nand machine-generated saliency- and text-based explanations across multiple between-subjects conditions, all participants\nwere informed that the answers were AI-generated. The example on the left featured an answer from Bert, our language model,\nwith a saliency map produced via conservative-LRP, while the right showcased a human-provided answer and contrastive\nexplanation.\non feature-importance explanations like saliency-maps, which are\nelaborated in the subsequent section.\n3.2.1 Gradient-based explanations. Considering the debate over\nthe reliability of attention weights [15, 78] and their potential com-\nplexity for non-experts, alongside the risk of information overload\nin layer-wise attention visualizations [ 46], this study prioritizes\ngradient-based explanations of feature importance as recommended\nby Bastings and Filippova[15]. These are chosen for their simplicity\nand efficiency relative to methods like surrogate models or occlu-\nsion analysis [7].\nAncona et al. [7] list gradient * input (GI), integrated gradients\n(IG), layer-wise relevance propagation (LRP), and DeepLIFT as\ngradient-based explanations in their overview. With this list at\nhand, the authors developed a unified framework that describes\nconditions of equivalence between them (e.g., equivalence between\nLRP and GI if Rectified Linear Units (ReLUs) are used). The model\nwe used in our study, BERT [26], uses Tanh and has additive bias\nwhich is why the conditions of equivalence do not hold. Nonethe-\nless, Ancona et al. [7] find a strong similarity between gradient-\nbased explanations in their evaluations.\nWe chose to implement conservative-LRP and IG in our study for\nseveral reasons. Ali et al. [4] specifically investigate Transformer\nmodels [78], the main architecture behind language models like\nGPT [3, 18, 63] and Bert [ 26], and find that the layer normaliza-\ntion and the attention heads of the Transformer models break the\nrelevance conservation in LRP which, for example, leaves some\nattention heads under- or over-represented in the explanation. In\ntheir evaluation, the researchers show that their adaptation of LRP,\nconservative-LRP, achieves better performance compared to GI and\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Pafla, et al.\nother measures across a wide range of benchmarks [4]. Given that\nDeepLIFT will likely struggle with the same propagation issues\nas LRP in Transformers due to the backward pass, we rely on this\nadaption of LRP by Ali et al . [4]. Furthermore, we evaluate inte-\ngrated gradients (IG) as our second saliency-based method as it is a\ncommonly used method that does not rely on full backward pass\nlike LRP or DeepLIFT but it is still able to fulfill useful requirements\nlike the completeness axiom [ 7, 76]. Though we are aware that\nresearch has found that smoothing algorithms such as smoothgrad\n[74] can increase plausibility in comparison to integrated gradients\nfor transformers [27], there is no evidence yet that smoothgrad\noutperforms IG when considering the entire input prefix (as high-\nlighted by Ding and Koehn [27]). Hence, in our study, we do not\nconsider such algorithms.\n3.3 Research Questions\nIn this study, our overarching goal is to provide empirical evidence\nfor the efficacy of human and machine text and saliency-based\nexplanations. Thus, our first goal was to collect human explana-\ntions and study the kind of explanations humans provide. As a\nsecond goal, we wanted to compare collected explanations to SOTA\nmachine explanations (IG, con-LRP, ChatGPT). Finally, once expla-\nnations were collected, generated, and analyzed, we were able to\ncompare them empirically in a human-participant study including\na wide range of metrics such as trust, curiosity, and satisfaction, in\naddition to more common performance measures.\nWith these overall goals, we address the following research ques-\ntions:\n‚Ä¢RQ1: What explanations do humans provide when explain-\ning their answers to a question?\n‚Ä¢RQ2: How different are human and machine saliency-based\nexplanations?\n‚Ä¢RQ3: How are human and machine text and saliency-based\nexplanations empirically evaluated in terms of a wide range\nof factors such as trust, satisfaction, performance, and confi-\ndence, when evaluated in a human-participant task including\nboth correct and incorrect answers?\nThe study consisted of three parts. In the first part, text- and\nsaliency-based explanations were collected from humans and gener-\nated using XAI techniques. In the second part, aimed at answering\nRQ1 and RQ2, human text explanations were classified and human\nand machine saliency maps were compared. In the third part, a new\ngroup of participants was asked to evaluate previously collected and\ngenerated explanations, allowing us to investigate RQ3. A detailed\noverview of the study‚Äôs structure is illustrated in Figure 2.\n4 PART 1: COLLECTING/GENERATING\nEXPLANATIONS\nIn this section, we describe how we generated XAI explanations\nand collected human explanations in a crowdsourcing task for our\nhuman-participant study described in section 6.\n4.1 Generating XAI Explanations\n4.1.1 Language Model. We used the large Bert model [26], an ar-\nchitecture based on the Transformer [80], with around 335 million\nparameters for our study. We used a pre-trained model that we\ndownloaded from Hugging Face [40]. We did not further fine-tune\nthe downloaded model.\n4.1.2 Saliency-based XAI explanations. We leveraged integrated\ngradients [76] and conservative LRP [4] to produce two saliency-\nbased visualizations per question-answering task. The attributions\nfrom each method were produced using the code bases ofpytorch/\ncaptum [45] and cdpierse/transformers-interpret [61] for IG,\nand AmeenAli/XAI_Transformers [4] for LRP. For both methods,\nwe produced the attributions of relevance with respect to the em-\nbedding layer (similar to Captum[21], Pierse [61]). The attributions\nwere retrieved for both the start and the end token of the answer\n(which is the output of the language model). To increase inter-\npretability and make explanations accessible to participants we did\nnot show participants two separate visualizations but averaged the\nattributions for both tokens.\nWe chose to produce attributions with regards to the actual pre-\ndicted start and end tokens. This decision stands in contrast with\nAli et al . [4] who produce explanations for ground truth labels,\nwhich we think biases the evaluation of explanations, as expla-\nnations are only shown for correct ground truth answers (which\nare often not available in deployed AI applications). Furthermore,\nwe use the same composition scheme as Ding and Koehn [27] to\ncombine attribution scores of multiple tokens into one when one\nword is split up into multiple tokens. As the baseline for integrated\ngradients, we chose 0-vectors for the different embedding layers,\nsimilar to Pierse [61]. Furthermore, we averaged the attributions of\npunctuation tokens (e.g., ‚Äò. ‚Äô) with the closest letter word.\n4.1.3 ChatGPT explanations. We leveraged ChatGPT to explain the\nanswers of our language model. For each explanation, we provided\nthe source text, the question, and the answer. While we acknowl-\nedge that ChatGPT does not generate explanations based on our\nunderlying language model (see subsubsection 4.1.1), but instead\ngenerates explanations on a conceptual text level, we stress that\nhuman explanation is often text-based. Our motivation to leverage\nChatGPT then was to use the current SOTA language model to pro-\nduce the best type of machine text-based explanations that we can\ncompare against human text explanations. Generally, our goal was\nto analyze human explanation strategies, and compare and evaluate\ndifferent types of machine and human explanations (e.g., saliency-\nbased or text-based). In the supplementary material, we discuss\nhow we derived our prompt to produce ChatGPT explanations. The\nspecific prompt to ChatGPT was:\n\"You are a question-answering assistant that explains\nanswers to questions. You will be provided with source\ntexts that contain approximately 150 words, a ques-\ntion about these texts, and an answer. Your job is to\nexplain why the provided answer is correct. Provide\nexplanations in one or two sentences. Write the ex-\nplanations in an instructional tone. \"\n4.2 Collecting Human Explanations from\nHuman Participants\n4.2.1 Participants and remuneration. The study was executed on-\nline, engaging participants from Prolific for a survey administered\nUnraveling the Dilemma of AI Errors CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nPart 1: Collecting Human Explanations\nPart 1: Generating AI Explanations\nConservative \n-LRP\nùë•ùëñ ‚àíùë•ùëñ\n‚Ä≤ √ó‡∂±\nùúïùêπ ùë•‚Ä≤ +Œ± ùë•‚àíùë•‚Ä≤\nùúïùë•ùëñ\nùëë\nIntegrated Gradients\nChatGPT\nPart 2: Analyzing Explanations Part 3: Human Participant Study\nHuman \nSaliency\nConservative\n-LRP\nIntegrated \nGradients\nAnswer-Only \nHighlighted\nText \nExplanation\nChatGPT\nText \nExtraction\nBetween-subjects\nQuestion\nAnswer\nExplanation\nSource text\nWithin-subjects\n4 correct answers\n4 incorrect answers\nControl\nAnswer-Only \nHighlighted\nText \nExtraction\nHuman Saliency\nText Explanation\nSaliency Overlap of 75 Saliency Maps\nClassifications of 82 Text Explanations\nLorem ipsum dolor sit amet. \nLorem ipsum dolor sit amet. \n0            1          0        0       1\n0            0.5      0      0.25    0\n21% overlap \nbetween human \nand machine \nsaliency maps\n60 extractions - copying directly or \nparaphrasing source text\n6 explanations ‚Äì clarifies confusing \ninformation or contrasts information\n9 misunderstandings - provides an \nexplanation supporting a wrong \nanswer \n7 poor attempts - provides an \nunrelated, irrelevant or incomplete \nexplanation attempt\nPlease highlight important text passages \nin this text that explain your answer.  \nThe title Khagan was not conferred on Genghis until \nafter his death, when his son and successor, √ñgedei, \ntook the title for himself and extended it posthumously \nto his father (as he was also to be posthumously \ndeclared the founder of the Yuan dynasty). This \nunification of all confederations by Genghis Khan \nestablished peace between previously warring tribes and \na single political and military force under Genghis Khan.\nPlease explain why your answer is correct.\nProcedure\nInstructions\nConsent\nChecks\nExample\nCuriosity\nPerformance\nConfidence\nQuality\nHelpfulness\nMental Effort \nSatisfaction\nTrust\nDemographics\nDebrief\n8 Question-Answering Tasks\nFigure 2: Our study, divided into three parts, aimed to assess the efficacy of various human and machine-generated saliency-\nand text-based explanations through empirical research. Initially (left section), we gathered human explanations from 40\nparticipants in a crowdsourcing task, where they were tasked with creating saliency-based explanations by highlighting source\ntext and providing text explanations. Subsequently, we utilized techniques such as conservative-LRP, integrated gradients,\nand ChatGPT to generate machine explanations. Analysis (middle section) revealed a limited overlap (21%) between human\nand machine-generated saliency maps, with the latter being more dispersed. Through thematic analysis, we developed a\ncoding scheme to classify human explanations into four categories, which was then independently applied. In the final part\n(right section), 136 participants evaluated the collected and generated explanations across seven conditions, including four\nsaliency-based and three text-based, including control conditions. This evaluation measured both objective (e.g., performance)\nand subjective (e.g., satisfaction) metrics, exposing participants to both correct and incorrect answers within a between-subjects\ndesign.\nvia Qualtrics. Data from 40 participants were collected in this first\npart of the study, with ages ranging from 20 to 53 years (median\nage 27) and all fluent in English. Gender distribution was even, with\n20 male and 20 female participants, confirmed by self-reports. At-\ntention and comprehension checks were conducted, leading to the\nexclusion of eight additional participants for failing these checks,\nwhose data were subsequently discarded. Upon completing the ex-\nperiment, participants were redirected to Prolific for compensation\nat a rate of ¬£9.00 per hour, with the average participation time being\n15 minutes.\n4.2.2 Dataset. For our study, we selected 40 random question-\nanswering (QA) tasks from the SQuAD 1.1v dataset [64], ensuring\na mix of correct and incorrect AI responses, with half of the tasks\ndeliberately chosen for their incorrect answers by the AI model.\nThis selection was achieved by evaluating the entire SQuAD vali-\ndation set with our language model, identifying 364 tasks where\nthe model‚Äôs answers had an F1 score of 0. Each task was manually\nreviewed to confirm suitability for participants and accuracy of AI\nresponses, excluding those with problematic questions, divergent\nground truth answers, politically sensitive content, or discrepancies\nbetween AI responses and scoring metrics.\nThe SQuAD 1.1v dataset, recognized for its widespread use and\nrelevance in question-answering research as indicated by download\nmetrics on Hugging Face [88], was chosen for its suitability for an\nonline format, with source texts averaging 200 words. We further\nrefined our selection to tasks with source texts ranging from 125 to\n175 words to maintain participant engagement. The human perfor-\nmance on SQuAD 1.1v is 77% and 86.8% for exact match metric and\nthe F1 score, respectively.\n4.2.3 Qualtrics survey. Excerpts of the Qualtrics survey can be seen\nin Figure 2 (left section). When collecting human saliency maps,\nwe displayed the entire source text again and asked participants\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Pafla, et al.\nto highlight text passages that explain participants‚Äô answers. This\nhighlighting was done with the help of the ‚Äúhighlight‚Äù question\ntype provided by Qualtrics. For the collection of text explanations,\nwe simply asked participants to explain why their answer was\ncorrect in a text box.\n4.2.4 Procedure. Participants were recruited via Prolific and di-\nrected to a Qualtrics survey, where they were briefed, asked for\nconsent, and underwent instruction, attention, and comprehension\nchecks before being debriefed at the end of the study. Demographic\ninformation was also collected. In this study part, participants en-\ngaged with four questions and corresponding source texts, tasked\nwith answering and explaining their responses (i.e., why the an-\nswer is correct). For each question, they were randomly assigned\nto either generate text explanations or highlight relevant portions\nin the source text to explain their answers.\n5 PART 2: ANALYZING\nCOLLECTED/GENERATED EXPLANATIONS\nIn subsequent sections, we address RQ1 and RQ2 through two\napproaches: initially conducting a qualitative analysis of human\ntext explanations to categorize the types provided, followed by\na comparative examination of human versus machine-generated\nsaliency-based explanations.\nWe gathered 74 saliency-based and 82 text explanations, total-\ning 156. Compared to the SQuAD 1.1v official human benchmarks\n[64], participant performance was lower, with a 59.0% exact match\nscore and 68.8% F1 score, versus official scores of 77.0% and 86.8%,\nrespectively. This discrepancy may be attributed to our study not en-\ncouraging one-word answers, which adversely affects performance\non both metrics. Manual validation revealed 88.5% of participant\nanswers were correct, aligning with official metrics. This corrected\naccuracy rate will be referenced in subsequent analyses.\nThe length of explanations was similar for saliency-based and\ntext-based explanations. The average number of words highlighted\nby participants was 14.57 ( ùëÜùê∑ = 11.61). The average number of\nwords in text-based explanations was 15.59 ( ùëÜùê∑ = 9.62). We ran\ntwo logistic regression models finding no statistical significance\nbetween the length of explanation and answer correctness for both\nsaliency-based explanations (ùëÇùëÖ = 0.9654, ùê∂ùêº = [0.9152,1.0184],\nùëß = ‚àí1.29, ùëù = .20) and text-based explanations ( ùëÇùëÖ = 1.0127,\nùê∂ùêº = [0.9370,1.0946], ùëß = 0.32, ùëù = .75). Another logistic regression\nmodel revealed that there was a small significant effect (ùëÖ2 = 0.0538,\nùëß = ‚àí2.493, ùëù = .0127) between the amount of time participants\nspent on a question-answering task and the answer correctness\n(ùëÇùëÖ = 0.9937, ùê∂ùêº = [0.9888,0.9987]): participants that spent less\ntime on a question had a higher chance of solving it correctly.\n5.1 Qualitative Evaluation of Human\nExplanation Strategies\nAdopting Morrison et al .‚Äôs [57] human-centered XAI methodol-\nogy, we conducted a coding reliability thematic analysis (TA) for\nevaluating human explanation strategies [ 16, 17], applying two\niterations of a coding book to our dataset, diverging from Morrison\net al. [57]‚Äôs single iteration. Initially, one researcher analyzed text\nexplanations to identify explanation strategies, leading to a collab-\norative development of a nine-code scheme. Upon applying these\ncodes, we identified issues of confusion and overlap, prompting a\nrefinement to four mutually exclusive codes for clearer thematic\ncategorization. These four codes were:\n‚Ä¢EXTRACTION ‚Äì copying source text directly or paraphras-\ning it; often accompanied by pointing at a text passage (e.g.,\n‚Äúin the third sentence it says [...]‚Äù)\n‚Ä¢EXPLANATION ‚Äì clarifies confusing or additional informa-\ntion and relates it to the answer or contrasts the correct\nanswer with incorrect ones\n‚Ä¢POOR ‚Äì provides an unrelated, irrelevant or incomplete\nexplanation attempt\n‚Ä¢MISUNDERSTANDING ‚Äî provides an explanation support-\ning an incorrect answer\nTwo raters coded the entire dataset of text explanations (ùëõ= 82).\nThe level of agreement was 86.6% (Cohen‚Äôs ùúÖ = .711, ùëß = 9.77,\nùëù < .01). In total, there was disagreement for eleven instances after\ncoding, which the two raters openly discussed until an agreement\nwas found following the coding reliability approach.\nThe analysis revealed 60 extractions, 6 explanations, 9 misun-\nderstandings, and 7 poor attempts among the responses, with ex-\ntractions being the predominant form. This prevalence might re-\nflect participants‚Äô preference for efficiency or the limitations of the\nSQuAD 1.1v dataset in facilitating comprehensive explanations. In-\ndeed, the best type of ‚Äúexplanation‚Äù might be to point at the answer\nand state its location in the text (e.g., ‚Äúthe answer is correct because\nit says it here‚Äù).\nThis challenge of generating explanations underscores their\ncontextual nature, particularly in the use of contrastive explana-\ntions where participants juxtapose potential but incorrect answers\nagainst the correct ones, necessitating imagination to identify plau-\nsible alternatives. For example in Figure 1b, one participant con-\ntrasted the right answer ‚Äú¬£21.000‚Äù with the incorrect, but potential\nand likely answer ‚Äú¬£27.000‚Äù, given context, by explaining that the\nsecond value relates to ‚Äúborders\" in contrast to ‚Äúnon-boarders‚Äù.\nHowever, the source texts in the SQuAD 1.1v dataset might not\nalways provide these contrastive cases, participants might not eas-\nily see them, or not find them relevant at all. Our coding scheme\napplication allowed us to view all participant responses, including\nincorrect ones, facilitating the identification of possible contrastive\nanswers. The effectiveness of these identified contrasts as explana-\ntions ultimately relies on the perception of participants in study\npart 3.\nEvaluations of explanations for incorrect answers revealed two\nmain types: those stemming from clear misunderstandings of the\ntext and those of generally poor quality due to carelessness or time\nconstraints. When reading these poor explanations, it became quite\nclear that a) these explanations were indeed of poor quality and that\nb) there is reason to be careful and verify given answers thoroughly.\nGiven the structure of the task, we were not able to identify any\ndeceitful explanations (as there was no motivation to do so).\nAmong the text explanations collected, half did not directly ad-\ndress the context but rather elucidated on how specific text elements\nUnraveling the Dilemma of AI Errors CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nshould be interpreted. For instance, a participant noted that a word\nin parentheses typically signifies its translation, aligning with the\nquestion‚Äôs requirement. This fact highlights that explanations ex-\ntend beyond mere context, encompassing external interpretative\nframeworks (such as social or syntactical conventions) that may lie\noutside of the provided data to the AI.\nThis qualitative analysis left us with information on what types\nof explanations (e.g., actual text explanations vs. extractions) partici-\npants provide and expect, answeringRQ1, and six text explanations\nthat we can use for study part 3.\n5.2 Comparing Human with Machine\nSaliency-Maps\nIn this section, we compare saliency maps and determine the over-\nlap between collected human saliency maps from study part 1 and\nmachine saliency maps generated by conservative-LRP and inte-\ngrated gradients. One motivation for this analysis was to deduct\nguidelines to advance machine saliency maps and improve their ap-\npeal for study part 3. For example, we found that machine saliency\nmaps can look scattered and simultaneously highlight words across\nthe entire text input paragraph which is why it might useful to\ndetermine a maximum number of words per saliency map that are\nhighlighted.\nAnother motivation for this analysis was to develop and compare\ndifferent overlap metrics that other researchers can utilize when\ncomparing human and machine saliency maps. Humans tend to\nhighlight sparsely and overlap metrics need to account for the fact\nthat overlap metrics can be artificially high for long input texts. To\novercome this issue, we only included words that humans high-\nlighted in our overlap calculations. We provide further discussion\non overlap metric calculations in the supplementary material.\nMathematically speaking, when defining ùë•ùëÅ and ùë¶ùëÅ as two vec-\ntors holding the attribution scores of ùëÅ words for human and AI\nsaliency maps for a question-answering task, respectively, then we\ncan calculate the overlap only for words that humans highlighted:\nùëöùëíùëéùëõ(1 ‚àíùëéùëèùë†(ùë•ùëÅ ‚àíùë¶ùëÅ ))for all ùëÅ where ùë• ‚ààùëãùëÅ : ùë• = 1. This\napproach yielded 21.55% overlap for matched answer correctness\nand 15.86% for mismatched answer correctness for conservative-\nLRP, and 23.68% and 20.82% for integrated gradients, respectively,\nanswering RQ2.\nHowever, these results only hold when including all AI attribu-\ntion scores for all words in the overlap calculation. Analysis indi-\ncated, as shown in Figure 3, that conservative-LRP demonstrates\ngreater overlap with human maps than integrated gradients when\naccounting for a limited number of top attribution scores (e.g., for\n15 words). Given the average of 14.57 words highlighted by humans\nin this study and the diminishing returns in overlap beyond 15\nwords for conservative-LRP, we opted to present only the top 15\nattribution scores for both conservative-LRP and integrated gradi-\nents in subsequent parts of the study. With this decision, our hope\nwas to make AI saliency maps more ‚Äúhuman-like‚Äù.\n6 PART 3: HUMAN PARTICIPANT STUDY\nAfter having collected, generated, and analyzed explanations, our\ngoal was to evaluate human- and machine-generated explanations\nempirically in a human-participant study using a mixed design,\n0 20 40 60 80 100 120\nNumber of Strongest AI Highlights Included\n0.00\n0.05\n0.10\n0.15\n0.20Overlap in % with Human Highlights\nOverlap Between AI and Human Saliency Maps\nConservative-LRP\nIntegrated Gradients\nFigure 3: The overlap between AI-generated and human\nsaliency maps varies with the number of AI attributions con-\nsidered in the analysis. Techniques such as conservative-LRP\nand integrated gradients assign scores to each word, with\nfull inclusion resulting in the greatest overlap with human\nmaps. However, to minimize explanation clutter and align\nthe visualization of attribution scores with the human av-\nerage (approximately 15 words, indicated by the blue line),\nit is observed that the marginal benefit of including addi-\ntional attributions beyond the first 15 diminishes, leading to\na plateau in overlap. Consequently, for empirical evaluation\nof machine-generated saliency maps, only the 15 most sig-\nnificant attribution scores were visualized.\naddressing RQ3 in this study part. We describe the methodology\nof our human participant study in this section. The study site,\nremuneration, and underlying dataset (i.e., the SQuAD 1.1v dataset)\nare the same as in study part 1 (collection of human explanations).\n6.1 Participants\nIn this study, data from 136 participants, aged 18-61 (median age\n28) and fluent in English, were analyzed. The gender distribution\nwas balanced via Prolific, with 66 female, 67 male, and 3 non-binary\nparticipants. Attention and comprehension checks were conducted,\nexcluding 23 individuals who failed the attention check and three\nwho failed the comprehension check, with their data being dis-\ncarded. Participants were compensated through Prolific at a rate of\n¬£6.68 per hour, with a median participation duration of 22 minutes.\nThose who correctly answered all eight questions received a bonus\nof ¬£0.50, aiming to create a sense of ‚Äúvulnerability‚Äù and emphasize\nthe importance of their decisions (see Vereschak et al. [81]).\n6.2 Study Design\nOur study employed a 9 Explanation Type (between-subjects) √ó\n2 Answer Correctness (within-subjects) mixed design, ensuring\na minimum of 15 participants for each between-subjects condition.\nAnswer Correctness determined the correctness of answers pro-\nvided by our language model (for machine and control explanations)\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Pafla, et al.\nor human participants (for human explanations), with explanations\nalways aligning with the answer‚Äôs correctness to avoid mismatched\nexplanations (i.e., we did not provide incorrect explanations to cor-\nrect answers and vice-versa). Each participant encountered an equal\ndistribution of four correct and four incorrect answers, without be-\ning informed of the AI‚Äôs 50% accuracy rate to focus on explanation\nevaluation without priming, adhering to guidelines by Vereschak\net al. [81].\nExplanation Type captures the explanation type participants\nwere exposed to, three of which were text and four of which were\nsaliency-based explanations (and one control in each group for a\ntotal of nine for performance and time measures only):\nSaliency-based explanations:\n‚Ä¢Conservative LRP/Con-LRP (Machine) [4] describes the\nSOTA method to produce saliency maps for Transformers.\n‚Ä¢Integrated Gradients/IG (Machine) [76] are a common\nmethod to produce saliency maps for neural nets.\n‚Ä¢Human saliency-based explanations are the highlights par-\nticipants provided in study part 1.\n‚Ä¢Answer-only (Control) explanations simply highlight the\nAI answer in the text. This condition serves as a control\ncondition for other types of saliency-based conditions.\n‚Ä¢No saliency (Control) is provided to participants to deter-\nmine the correctness of AI answers. This condition serves as\na way to compare performance scores and time commitment.\nText explanations:\n‚Ä¢Text explanations were provided by previous participants\nin study part 1. These explanations clarify confusing or ex-\ntra information in the source text or contrast the correct\nanswer with other potential answers that are incorrect. For\ntext explanations that supported an incorrect answer, we\nselected explanations tagged as misunderstandings (see sub-\nsection 5.1; we did not select poor explanation attempts).\n‚Ä¢ChatGPT (Machine) are explanations provided by Chat-\nGPT [3]. The exact prompt we used to produce these expla-\nnations can be found in subsubsection 4.1.3.\n‚Ä¢Text extractions (Control) are sentences in the source text\nsupporting AI answers. These extractions are usually text\ncopied from the source text directly, or paraphrased, and\nusually point to specific passages in the text, similar to how\nhuman participants provided text extractions in study part 1.\nThis condition serves as a control condition for other types\nof text-based conditions.\n‚Ä¢No explanation (Control) is provided to participants to\ndetermine the correctness of AI answers. This condition\nserves as a way to compare performance scores and time\ncommitment.\n6.3 Selecting Question-Answering Tasks and\nExplanations for Human Participant Study\nIn this study, we tailored our selection of question-answering tasks\nto include both human and machine explanations, aiming for a full\nfactorial design that encompasses correct and incorrect answers\nfor each explanation type. This necessitated a selective approach\ndue to a limited number of text explanations and incorrect answers\nprovided by participants, coupled with the use of a single AI model\nproducing a singular outcome per question. To achieve a balanced\ndesign, we segregated two sets of four questions each for saliency-\nbased and text-based explanations, mindful that incorrect answers\nmight differ across conditions due to variations in human and AI\nanswers. For text-based tasks, selection was straightforward as only\nfour tasks included both text explanations and misunderstandings.\nOn the other hand, for saliency-based explanations related to in-\ncorrect answers, we matched four human-generated saliency maps\nwith AI saliency maps, and for correct answers, we randomly chose\nfour from twenty tasks humans and the AI model had answered\ncorrectly, ensuring a balanced dataset that allows for an in-depth\nanalysis of explanation types across across correct and incorrect\nanswer instances.\n6.4 Dependent Variables\n6.4.1 Objective performance measures. We define two dependent\nvariables to measure performance: Performance and Time. Per-\nformance is a binary variable that captures whether participants\ncorrectly assessed whether the answer the AI provided is correct or\nnot. Participants were not informed about AI correctness (and they\ncould not assess the correctness of their own assessment). Time is\na numerical variable that captures the amount of time in seconds\nparticipants spent on each question-answering task.\n6.4.2 Subjective affect scales and measures. Following introductory\ninstructions and an explanation example, participants‚Äô curiosity\nwas gauged using a modified, unvalidated checklist from Hoffman\net al. [39] incorporating a 5-point Likert scale. For each question-\nanswering task, participants evaluated their confidence they have\nin their AI answer assessment on a 0-100 scale, the explanation‚Äôs\nquality and helpfulness on 5-point Likert scales following Lage et al.\n[47] and Schuff et al. [72] respectively, and mental effort using the\n9-point Likert validated PAAS scale [59]. Post-evaluation of eight AI\nanswers, participants rated their satisfaction with the explanations\nusing a validated 8-item satisfaction scale and their trust in the AI\non an unvalidated 8-item scale on a 5-point Likert scale by Hoffman\net al. [39], excluding the 5th item (‚ÄúThe [tool] is efficient in that\nit works very quickly. ‚Äù) due to immediate display of AI responses.\nAverage scores for satisfaction, trust, and curiosity were calculated.\nThe choice of scales struck a balance between encompassing a\nbroad spectrum of metrics outlined by RQ3 and maintaining an\nappropriate duration for participant involvement in the experiment,\nwith further details provided in our supplementary materials. The\nscales from Doshi-Velez and Kim[29], Schuff et al. [72], and Paas\n[59], covering quality, helpfulness, and mental effort with concise\nitems, were apt for repeated use across various question-answering\ntask rounds. Notably, Schuff et al . [72] offered helpfulness met-\nrics specifically crafted for question-answering contexts. Given\nour explanations goals to center user experience and our focus\non local explanations, we applied the satisfaction scale by Hoff-\nman et al. [39], which posits that satisfaction is a contextual and\nretrospective assessment made by users regarding explanations.\nFinally, we opted for the trust scale by Hoffman et al. [39] over the\nUnraveling the Dilemma of AI Errors CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nlengthy human-computer trust (HCT) scale [52], which, although\nunvalidated, amalgamates elements from several validated scales,\nincluding HCT and Jiun-Yin Jian and Drury [43]‚Äôs trust scale, and\nis tailored for the XAI context.\n6.5 Deception\nAll participants were informed that both answers and explanations\noriginated from AI, even if they were provided by humans. The pur-\npose of this deception was to evaluate the effectiveness of various\nexplanation types without biases related to their origin (e.g., human\nvs. machine). Given that XAI explanations lack contrastive or causal\nexplanations that humans can provide, and that AI-generated pre-\ndictions are often viewed as less fair and trustworthy than those\nfrom humans, employing deception was crucial to mitigate these\nbiases. The impact of this deception was considered minimal, with\nparticipants undergoing debriefing post-study and consent being\nreaffirmed.\n6.6 Procedure\nStudy participants followed the same procedure as participants in\nstudy part 1: information letter, consent form, study instructions,\ntwo attention checks, and a task comprehension check. At the end of\nthe study they were debriefed about the study and informed if they\nhad been deceived. Furthermore, we collected simple demographic\ninformation (i.e., age and gender). The AI was introduced in a\nneutral tone to avoid positively skewed expectations and reduce\nbias in the evaluations of explanations [81]. Before encountering\nquestion-answering tasks, we asked participants to evaluate their\ncuriosity in the AI and explanations.\nIn this third study part, participants encountered eight questions\nalongside corresponding source texts, answers, and explanations\ndependent on Explanation Type in a between-subjects design,\nwith the pretense that all responses and explanations were AI-\ngenerated. Their primary task was to assess the correctness of each\nanswer with the aid of the provided explanation. Following each\nquestion, participants rated their confidence in their assessment of\nthe AI answer, the explanation‚Äôs quality, its helpfulness in assessing\nanswer correctness, and the mental effort required for evaluation.\nFinally, we ask participants to fill out the explanation satisfaction\nand trust scale. A detailed procedure is depicted in Figure 2.\n6.7 Hypotheses\nTo the best of our knowledge, this was the first study to compare\nhuman saliency maps to AI saliency maps, with no prior knowledge\non their relative impact on performance, trust, and satisfaction. We\nhypothesized that participants would favor human over AI expla-\nnations, regardless of their format (text or saliency-based), and\nposited that any form of explanation would outperform control\nconditions (answer-only and text extractions), despite some con-\ntradictory evidence (e.g., [44, 70]). \"Better\" outcomes were defined\nacross dimensions of satisfaction and trust, performance and time,\nand quality, helpfulness, and mental effort. Our hypotheses were\norganized around these metrics, instrumentalizing RQ3.\n‚Ä¢Human saliency, IG, Con-LRP > Control : Participants in\nhuman saliency, integrated gradients and conservative-LRP\nconditions reach higher levels of satisfaction, trust, perfor-\nmance, quality, and helpfulness, and require less time &\nmental effort to solve the task than in the saliency-based\ncontrol condition.\n‚Ä¢Human saliency > IG, Con-LRP : Participants in the hu-\nman saliency condition reach higher levels of satisfaction,\ntrust, performance, quality, and helpfulness, and require less\ntime & mental effort to solve the task than in the integrated\ngradients and conservative-LRP condition.\n‚Ä¢Text explanation, ChatGPT > Text extraction : Partici-\npants in the text explanations and ChatGPT explanations\nreach higher levels of satisfaction, trust, performance, qual-\nity, and helpfulness, and require less time & mental effort to\nsolve the task than the text extraction control condition.\n‚Ä¢Text explanation > ChatGPT : Participants in the text ex-\nplanations conditions reach higher levels of satisfaction,\ntrust, performance, quality, and helpfulness, and require less\ntime & mental effort to solve the task than in the ChatGPT\nexplanations.\nIn Figure 4, we represent our hypotheses, analysis methods, and\nresults in the same diagram to clarify the connection between them.\nTo describe both our hypothesized and measured differences in the\nstudy, we use alphabetic letters to represent groups where group ‚Äúa‚Äù\nis more (less for mental effort and time) than group ‚Äúb‚Äù which is in\nturn more (less for mental effort and time) than group ‚Äúc‚Äù. Whenever\nthere is a significant difference between two conditions, those two\nconditions cannot appear in the same group. In our analysis, if\nthere is a third condition with no significant difference to either\nof the two, mutually-significant conditions being in two different\ngroups (e.g., ‚Äúa‚Äù and ‚Äúb‚Äù), this condition would be in a group with a\ncombined label (e.g., ‚Äúab‚Äù).\nFor example, for saliency-based explanations we define three\ngroups (a, b, and c) and hypothesize that human saliency will be in\nthe first group (group a), con-LRP and IG will be in the second (group\nb), the the control condition will be in the last group (group c). We\nhypothesize that there will be a significant difference between all\nconditions in different groups: human saliency will be significantly\ndifferent from con-LRP and IG, and human-saliency, con-LRP, and\nIG will be different from the control condition. This exact logic is\nrepresented in the first two hypotheses we define above in text.\nSimilarly, we define our hypotheses with text-based explanations\nwith three groups. We define these groups of hypotheses for each\nof the scales we measured (performance, satisfaction, etc.).\n6.7.1 Primary results and discussion. As readers will note in the\nnext section, the results did mostly not confirm our hypotheses.\nRather, we found that control conditions performed as well as the\nbest saliency maps (human saliency) and text explanations while\nthere were no significant main effects of Explanation Type for\nsaliency-based explanations on measures of satisfaction and trust.\nHence, we explore our data to find out why promising explanation\ntypes (e.g., conservative-LRP, human saliency, and ChatGPT) under-\nperformed.\nWe found some interesting effects that we illustrate in Figure 4\nand further describe in the next section: for text-based explanations,\nwe found a significant difference for trust between conditions, and\nfor saliency-based explanations we found a significant effect for\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Pafla, et al.\nHuman \nSaliency\nIntegrated \nGradients\nAnswer-Only \nHighlighted\nConservative\n-LRP\nHypotheses\na\nb\nc\nb Text \nExtraction\nText \nExplanation\nChatGPT\na\nb\nc\nPerformance\nSatisfaction\nTrust\nCuriosity\nQuality\nHelpfulness\nMental \nEffort\nTime\nMethods\nPerformance Satisfaction, Trust, Curiosity Quality, Helpfulness Mental Effort Time\nScale\nLikert\nTest\nMixed model\nFrequency Ordinal Ordinal Ordinal Numerical\n- 5-point 5-point 9-point -\nChi-squared ANOVA Ordinal Mixed Model Ordinal Mixed \nModel\nLinear Mixed \nModel\n- - Mixed Mixed Mixed\nResults\nHuman \nSaliency\nConservative\n-LRP\nIntegrated \nGradients\nAnswer-Only \nHighlighted\nHelpfulness\n**\na\na\nb\nANOVA \nùëã2(3) = 14.80, ùëù < .01 \nb\n***\n***\n***\nTrust\nChatGPT\nText \nExtraction\nText \nExplanation\na\nab\nb\n*\nANOVA \n(ùêπ2,42) = 3.51, ùëù < .05 \nNo main \neffects for\nPerformance\nSatisfaction\nCuriosity\nQuality\nMental \nEffort \nHuman \nSaliency\nConservative\n-LRP\nIntegrated \nGradients\nAnswer-Only \nHighlighted\nTime\n***\n**\na\na\nab\nb\nANOVA \nùêπ(3,57) = 4.48, ùëù < .01 \nHuman \nSaliency\nConservative\n-LRP\nIntegrated \nGradients\nAnswer-Only \nHighlighted\nQuality\na\na\nb\nANOVA \nùëã2(3) = 5.08, ùëù = .17\nab\n*\n***\nInteraction effect \nwith AI answer \ncorrectness:\nSignificant effects \nfound for correct \nanswers\nFigure 4: Connection between hypotheses, scales, tests, and results. We hypothesize (top left corner) that, for each of the\nmeasures (performance, satisfaction, etc.), the existence of three groups (a, b, and c) that include conditions who are significantly\ndifferent to all conditions in all other groups on this measure. In the top right, we provide a table that includes basic information\nabout the measures we used in this study including the scale of the measure, whether it is a Likert scale, what test we applied to\nhypotheses of the measure, and whether we ran a mixed model for the measure which included repeated measurements from\nparticipants (i.e., eight question-answering tasks). While we were not able to confirm most of our hypotheses, we represent the\nmost interesting findings of the study: there was a significant difference for trust for text-based explanations, and significant\ndifferences for quality, helpfulness, and time for saliency-based explanations. We present significant effects that partially\nconfirm or contradict our hypotheses with the help of black and grey parentheses, respectively.\nquality of correct answers (interaction), for helpfulness, and time.\nFor these measures, we found that there was no significant differ-\nence between human saliency and the control condition, which\nis why they were grouped together in group ‚Äúa‚Äù, while integrated\ngradients was significantly different from this group and hence was\nin its own group ‚Äúb‚Äù.\nIn section 8, we discuss the nuanced differences in perceived help-\nfulness and time efficiency between human and machine-generated\nsaliency maps, despite the overall lack of significant effects on trust\nand satisfaction. These findings emerged from analyses incorpo-\nrating mixed models that accounted for both correct and incorrect\nanswers. We emphasize the significance of including variables like\nAnswer Correctness in study designs and its influence on both\nobjective and subjective evaluation metrics. Furthermore, we ex-\nplore the challenge and dilemma of crafting effective explanations\nfor incorrect AI predictions and the tendency for explanation con-\nfirmation bias among participants when assessing explanations.\n7 PART 3: RESULTS OF EMPIRICAL STUDY\nIn this section, we present the results of our human-participant\nstudy, targeting RQ3. We compare human and machine text and\nsaliency-based explanation on a wide range of factors, organizing\nthe ensuing subsections accordingly. Because participants were\nprompted with the same question-answer tasks for all saliency-\nbased explanations, and a different set was used for all text expla-\nnations, we perform separate analyses on each of these groups of\nexplanation types. An overview of all the measures taken and what\ntests were used for each of them can be found in Figure 4.\n7.1 Satisfaction, Trust, Curiosity\nWe performed one-way analyses of variance (ANOVAs) on the\nmeasures of satisfaction and trust (measured upon completion of\nall tasks) and curiosity (measured before all tasks) to compare both\nthe 4 saliency-based explanations and separately to compare the 3\ntext explanations (Figure 5, top row). All tests showed that these\nmeans were not significantly different, except for the trust measure\nof text-based explanations, ùêπ2,42 = 3.51, ùëù < .05. Pairwise ùë°-tests\nwith Bonferroni correction revealed that there was a statistically\nsignificant difference between trust averages for text extractions\nand ChatGPT explanations, ùë°(27.6)= 2.85, ùëù < .05.\n7.2 Performance (Accuracy)\nTable 1, shows the number of times participants evaluated the AI an-\nswers correctly (Performance) depending on whether the shown\nUnraveling the Dilemma of AI Errors CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\n3.5\n3.8\n3.0\n3.5\n3.4\n3.7\n3.4\n*\n2.7\n3.1\n3.1\n2.8\n3.0\n3.2\n2.6\n3.7\n3.9\n4.1\n3.6\n4.1\n4.2\n3.9\nSÓÅëÓÅ§ÓÅôÓÅ£ÓÅñÓÅëÓÅìÓÅ§ÓÅôÓÅüÓÅû TÓÅ¢ÓÅ•ÓÅ£ÓÅ§ CÓÅ•ÓÅ¢ÓÅôÓÅüÓÅ£ÓÅôÓÅ§ÓÅ©\n1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\nText Extraction\nChatGPT\nText Explanation\nAnswer-only (Control)\nHuman Saliency\nIntegrated Gradients\nConservative-LRP\n0.72\n0.72\n0.81\n0.72\n0.79\n0.77\n0.68\n0.68\n0.72\n**\n***\n85.3\n74.5\n119.2\n101.1\n103.4\n89.7\n94.7\n114.6\n80.7\nPÓÅïÓÅ¢ÓÅñÓÅüÓÅ¢ÓÅùÓÅëÓÅûÓÅìÓÅï TÓÅôÓÅùÓÅï (in seconds)\n0.00 0.25 0.50 0.75 1.00 0 50 100 150\nNo expl. (Control)\nText Extraction\nChatGPT\nText Explanation\nNo saliency (Control)\nAnswer-only (Control)\nHuman Saliency\nIntegrated Gradients\nConservative-LRP\n***\n*\n3.0\n4.1\n2.8\n4.3\n2.6\n3.4\n2.6\n3.8\n2.6\n3.5\n2.7\n3.7\n2.8\n3.5\n***\n***\n***\n**\n4.1\n4.2\n3.0\n3.5\n3.4\n3.6\n3.6\n4.5\n4.0\n5.1\n4.6\n4.2\n5.0\n5.0\nÓÇìÓÅëÓÅúÓÅôÓÅ§ÓÅ© HÓÅïÓÅúÓÅ†ÓÅñÓÅ•ÓÅúÓÅûÓÅïÓÅ£ÓÅ£ MÓÅïÓÅûÓÅ§ÓÅëÓÅú EÓÅñÓÅñÓÅüÓÅ¢ÓÅ§\n1 2 3 4 5 1 2 3 4 5 1 3 5 7 9\nText Extraction\nChatGPT\nText Explanation\nAnswer-only (Control)\nHuman Saliency\nIntegrated Gradients\nConservative-LRP\nFigure 5: We present the main results of our study for saliency-maps (light blue background) and text-based explanations\n(red background). For each of our between-subjects conditions, we present the mean and 95% confidence intervals (CIs) for\nsatisfaction, trust, and curiosity in the first row, performance and time in the second row, and quality, helpfulness and mental\neffort in the third row. In the second row, we include measures for two extra control conditions, No saliency (Control) and No\nexplanation (Control), in which participants evaluated the same set of AI answers in a question-answering task than in the\nother conditions, but without any explanation. In the last row, we display the means and CIs for both incorrect (blue) and\ncorrect (black) AI answers. As can been in the plots, non-overlapping CIs indicate significant effects: between text extraction and\nChatGPT for trust, between human and machine saliency maps for helpfulness, between human saliency maps and integrated\ngradients for time.\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Pafla, et al.\nPerformance\nAI answer Correct Incorrect\nCorrect 515 29\nIncorrect 283 261\nTable 1: Frequency table for performance depending on the\ncorrectness of the AI answer (across all conditions). AI cor-\nrectness significantly influenced outcomes; participants of-\nten accepted correct AI answers, while performance dropped\nto around 50% for incorrect ones. Grey cells indicate that\nparticipants complied with the AI.\nPerformance\nExplanation Type n Correct Incorrect\nConservative LRP 120 87 33\nIntegrated Gradients 120 97 23\nHuman saliency 128 92 36\nAnswer-only (Control) 120 86 34\nNo explanation (Control) 120 95 25\nText explanations 120 92 28\nChatGPT 120 81 39\nText extractions (Control) 120 81 39\nNo explanation (Control) 120 87 33\nTable 2: Frequency table for performance split by condition.\nDespite varying conditions, the ratios of correct to incorrect\nresponses were consistent. Notably, control conditions lack-\ning explanations yielded results comparable to those with the\nmost effective saliency-based and text-based explanations.\nAI answer was correct or not and Figure 5 (middle-left) shows these\nvalues as a ratio. Whether the AI was correct or not had a highly sig-\nnificant effect on participant performance,ùëã2 (1)= 250.87, ùëù < .001.\nTable 2, shows the number of times participants evaluated the AI\nanswers correctly (Performance) for each explanation condition.\nAs can be seen in the table, the ratios are similar across condi-\ntions. Chi-squared tests reveal no significant difference between\nsaliency-based conditions for performance, ùëã2 (4)= 5.08, ùëù = .28\nand no significant difference between text-based conditions for\nperformance, ùëã2 (3)= 3.43, ùëù = .33. We note that there is a ten-\ndency for control conditions without explanations to perform as\nwell as the best saliency-based (IG) and the best text-based expla-\nnations (text explanations). Based on the two control conditions,\nthe question set for text-based explanations (performance = 0.72) is\nslightly more difficult than the set for saliency-based explanations\n(performance = 0.79) as can be seen in Figure 5 (middle-left).\n7.3 Quality, Helpfulness, Mental Effort, and\nTime\nNext, we evaluate the effect our explanation conditions and AI\nanswer correctness have on quality, helpfulness, mental effort and\ntime. Since we took these measures after every question (i.e., there\nare repeated measures for every question for every participant),\nwe ran a mixed-model ANOVA to investigate effects. For quality,\nhelpfulness, and mental effort, we ran an ordinal logistic regression\nanalysis to best model ordinal Likert scales. We present the results\nin Table 3. The ordinal logistic regression analysis for helpfulness\nfor saliency-based conditions did not satisfy the proportional odds\nassumption. However, a linear model yielded similar results (albeit\nthe interaction became non-significant). For time, we fitted a linear\nmixed-model to the log transformed time data. We present the re-\nsults in Table 4. As can be seen in Table 3 and Table 4, we found that\nwhether the AI answer was correct or not (described by Answer\nCorrectness) impacted participants across all measures for both\ntypes of explanations. Participants perceived questions with correct\nanswers to be higher quality, more helpful, lower mental effort, and\nthey took less time.\nFurthermore, we found a significant main effect for saliency-\nbased explanations on helpfulness and time. Pairwise post-hoc\nùë°-tests with Bonferroni corrections revealed that there was a sig-\nnificant difference for helpfulness between human saliency and\ncon-LRP, ùë°(230)= 4.48, ùëù < .001, between human saliency and\nIG, ùë°(212)= 6.85, ùëù < .001, between answer-only and con-LRP,\nùë°(236)= 3.63, ùëù < .01, and between answer-only and IG, ùë°(226)=\n5.97, ùëù < .001. In short, machine saliency maps were significantly\nless helpful than human and control saliency maps. Similarly, we\nfound a significant difference for time between human saliency and\nIG, ùë°(183)= ‚àí4.48, ùëù < .001 and answer-only and IG, ùë°(197)=\n‚àí3.29, ùëù < .01. This finding implies that saliency maps generated\nby integrated gradients required significantly more time from par-\nticipants than human and control saliency maps to evaluate the AI\nanswer.\nWe also found a significant interaction between Explanation\nType and Answer Correctness for quality for saliency-based ex-\nplanations. When looking at the CIs for quality in Figure 5 (bottom),\nthe reader can see that, while CIs are aligned vertically for incorrect\nAI answers (blue), CIs do not all overlap for correct AI answers\n(black). Post-hoc, pairwise ùë°-tests for correct AI answers (black)\nfor saliency-based conditions revealed a significant difference be-\ntween quality measures for human saliency and IG, ùë°(112)= 4.33,\nùëù < .001, and answer-only and IG, ùë°(104)= 3.20, ùëù < .05.\nWe also checked for two potential confounding variables: the\nSkill of participants in answering the questions and the Diffi-\nculty of the questions. The Skill of participants was measured by\ntheir average Performance across all eight questioning-answering\ntask. The Difficulty of a question was measured by the average\nPerformance of all question-takers for that question in the con-\ntrol conditions without any explanation (where explanation was\nnot able to influence the difficulty of the task). When adding both\nvariables as random variables to our models (see supplementary\nmaterial) we were able to reproduce our results. However, we ad-\nditionally found that the interaction effect between Explanation\nType and Answer Correctness became significant for helpfulness\nwhich does not change our subsequent interpretation of the results.\nFinally, our study design did not allow participants to check the\ncorrectness of AI answers after their assessment, excluding the\npotential confound of participants‚Äô knowledge of correctness on\nmeasures discussed in this section.\nUnraveling the Dilemma of AI Errors CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nExplanation Measure Variable DF ùëã2 p Significance\nSaliency-based Quality Explanation Type 3 5.08 0.17\nAnswer Correctness 1 100.73 < .001 ***\nExplanation Type:Answer Correctness 3 8.16 < .05 *\nHelpfulness Explanation Type 3 14.80 <0.01 **\nAnswer Correctness 1 73.62 < .001 ***\nExplanation Type:Answer Correctness 3 7.08 0.07\nMental Effort Explanation Type 3 5.05 0.17\nAnswer Correctness 1 38.47 < .001 ***\nExplanation Type:Answer Correctness 3 5.28 0.15\nText-based Quality Explanation Type 2 0.31 0.86\nAnswer Correctness 1 47.55 < .001 ***\nExplanation Type:Answer Correctness 2 0.80 0.67\nHelpfulness Explanation Type 2 0.15 0.93\nAnswer Correctness 1 44.76 < .001 ***\nExplanation Type:Answer Correctness 2 1.21 0.55\nMental Effort Explanation Type 2 3.01 0.22\nAnswer Correctness 1 38.19 < .001 ***\nExplanation Type:Answer Correctness 2 0.80 0.67\nTable 3: Mixed-model, type 2, ANOVAs for saliency-based and text-based explanations based on ordinal log regression models.\nWe found that the within-subjects variable Answer Correctness(i.e., was the AI answer that was provided to participants\ncorrect or incorrect) had a strong significant effect on all measures (i.e., quality, helpfulness, mental effort) for both types of\nexplanations. The between-subjects variable Explanation Typehad a significant effect on helpfulness for saliency-based\nconditions. We found a significant interaction effects between Answer Correctnessand Explanation Typefor saliency-based\nexplanations.\nExplanation Measure Effect DF Df.res F p Significance\nSaliency-based Time Explanation Type 3 57 4.48 <0.01 **\nAnswer Correctness 1 423 35.66 < .001 ***\nExplanation Type:Answer Correctness 3 423 0.71 0.55\nText-based Time Explanation Type 2 42 0.70 0.50\nAnswer Correctness 1 312 29.36 < .001 ***\nExplanation Type:Answer Correctness 2 312 0.04 0.96\nTable 4: Mixed-model, type 2, ANOVAs for saliency-based and text-based explanations based on linear mixed models. We found\nthat the within-subjects variable Answer Correctness(i.e., was the AI answer that was provided to participants correct or\nincorrect) had a strong significant effect on time for both types of explanations. The between-subjects variable Explanation\nType had a significant effect on time for saliency-based conditions. We found no significant interaction effects between Answer\nCorrectness and Explanation Typefor either type of explanation.\n7.4 Which Factors Are Related to Performance?\nFinally, we investigate the variables that are strongly related to\nperformance. One of the strongest predictors for performance is\nwhether the provided AI were correct or not, ùëü(1086)= 0.48, ùëù <\n.001. Nonetheless, we find that participants have some intuition\nabout their performance as confidence values mildly correlate with\nperformance, ùëü(1086)= 0.15, ùëù < .001. Even more interesting are\nthe negative correlations between performance and satisfaction,\nùëü(104)= ‚àí0.25, ùëù < .01, performance and trust, ùëü(104)= ‚àí0.38,\nùëù < .001, and performance and helpfulness,ùëü(104)= ‚àí0.26, ùëù < .01.\nThe moderate negative correlation between performance and trust,\nwith higher performance among participants distrusting the AI,\nwas surprising but also reflecting the context of our study where\nhalf of the answers provided were incorrect.\n8 DISCUSSION\nThe main findings of our study are as follows (answering RQ3):\n‚Ä¢Participants trusted text extractions more than ChatGPT\nexplanations.\n‚Ä¢When provided answers were correct, participants performed\nbetter and quicker, perceived the explanation to be higher\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Pafla, et al.\nquality and more helpful, and had lower perceived mental\neffort to evaluate the answer.\n‚Ä¢Machine saliency maps (con-LRP and IG) were significantly\nless helpful than human and control saliency maps.\n‚Ä¢Saliency maps created with IG required significantly more\ntime from participants to evaluate the answer than human\nor control saliency maps.\n‚Ä¢Participants found human saliency maps and answer-only\nmaps to be of higher quality than maps created with IG for\ncorrect answers, but not for incorrect answers (interaction).\n‚Ä¢Measures of explanation satisfaction, trust in the AI, and\nexplanation helpfulness were negatively correlated with per-\nformance scores.\n8.1 Comparing Machine Explanations with\nHuman Explanations\nHuman saliency-based explanations received higher evaluations\nacross various metrics compared to machine-generated explana-\ntions, particularly in terms of helpfulness, where they were signifi-\ncantly more favored by participants (mean difference of 1.2 on a\n5-point Likert scale between the human saliency and integrated\ngradients conditions). Furthermore, we found a significant effect of\nsaliency-based explanations on time and on the perceived quality\nof explanations for correct answers where human saliency-based\nexplanations notably reduced task completion time and were per-\nceived as higher quality for correct answers when compared to\nmachine-generated integrated gradients. However, no significant\ndifference was found between human saliency-based explanations\nand a control condition highlighting the answer sentence. This sug-\ngests that while saliency-based explanations may not be the most\neffective method for explaining answers in question-answering\ntasks, human-generated saliency explanations are still preferred\nover machine alternatives for their helpfulness and efficiency. Fu-\nture research needs to confirm this finding in further studies in-\ncluding other tasks (e.g., more open-ended question-answering or\ntext summarization).\nIn the evaluation of text-based explanations, a significant vari-\nance in trust scores emerged, particularly between text extractions\nand ChatGPT explanations, without a corresponding difference in\nsatisfaction or performance metrics. As to the origin of this result\nwe can only speculate, but we found that ChatGPT explanations\nwere more verbose, potentially requiring additional time for com-\nprehension, and leading to lower trust scores due to the complexity\nof information and perceived opportunity for deception. Alterna-\ntively, participants could simply have a preference for succinct\nexplanations, as evidenced by the predominance of text extractions\nin study part 1, indicating a preference for short text ‚Äúpointing‚Äù at\nthe right section in the source text. To close this knowledge gap,\nfuture research needs to explicitly investigate the impact of the\nlength of text explanations on trust scores.\n8.2 The Importance of Including Incorrect AI\nPredictions in Study Design\nThe correctness of AI answers markedly influenced participants‚Äô\nperformance, time investment, perceived explanation quality, help-\nfulness, and mental effort, overshadowing the impact of the study\nconditions themselves (see Table 3). Notably, while it is expected\nthat incorrect answers would demand more time and cognitive\nresources, the discovery that answer correctness more significantly\naffected the perceived quality and helpfulness of explanations than\nthe explanation type itself suggests a tight coupling between AI\nprediction and explanation that we further discuss below.\nBesides the importance of answer correctness as a factor itself,\nincluding it for the sake of revealing other effects can be fruitful. In\nour study, similar to Bu√ßinca et al. [20], we learned that participants\nthat found explanations less helpful and had higher distrust in the\nAI had a higher chance of performing better. Furthermore, we found\nthat human and control saliency maps were perceived as higher\nquality than maps generated by IG but only for correct answers .\nWe also found main effects for helpfulness and time which did\nnot become present before including Answer Correctness as a\nvariable in our mixed models. Thus, including both correct and\nincorrect predictions when evaluating AI and XAI explanations in\nmixed models might be crucial for XAI researchers to gain insight.\n8.3 The Dilemma of Explanation: Incorrect\nAnswers\nBesides the strong effects AI answer correctness had on multiple\nmeasures, these effects point at a general dilemma in explanation:\nexplanations are perceived as higher quality and more helpful when\nthey support a correct answer, but explanations for incorrect an-\nswers can either make people aware of the incorrectness of an\nanswer (e.g., when the explanation is of poor quality) in the best\ncase, or entice people to accept an incorrect answer in the worst\ncase (e.g., when the explanation looks ‚Äúcredible enough‚Äù).\nThis dilemma is generally understudied in the field of XAI, espe-\ncially in within-subjects designs with exposure to multiple rounds\nof AI predictions and post-exposure evaluations. Future research\ncould investigate the effects of ‚Äúbad‚Äù explanations for correct an-\nswers versus ‚Äúgood‚Äù explanations for incorrect ones. In our study,\nwe provided explanations in line with the answer (e.g., they would\nsupport the answer even if it is incorrect) and found that higher\ntrust in the AI correlated with poorer participant performance, sug-\ngesting a lack of thorough evaluation of AI responses. Additionally,\na negative relationship between perceived helpfulness and actual\nperformance was observed, indicating that higher trust in AI or\nperceived helpfulness of explanations could foster over-reliance on\nAI, as corroborated by other authors (e.g., Bu√ßinca et al. [20], Jacobs\net al. [41], van der Waa et al. [77]). This relationship underscores\nthe risk of participants accepting incorrect answers based on con-\nvincing explanations, emphasizing the dangers of the dilemma of\nAI errors and the need for critical engagement with AI-generated\ncontent.\nIn examining the integrated gradients (IG) condition, the in-\nfluence of saliency-based explanations on trust-related measures\nsuch as time and helpfulness (see Vereschak et al. [81]) becomes\nevident, thereby revealing the dilemma. IG explanations, found to\nbe less helpful and efficient compared to human-generated ones,\nparadoxically led to the highest performance levels in the study.\nWe speculate that participants, upon encountering IG explanations\ndeemed moderately helpful (average Likert score of 3 out of 5) and\nless efficient, engaged in more thorough scrutiny of AI responses,\nUnraveling the Dilemma of AI Errors CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nthereby enhancing their task performance. This increased critical\nassessment could have deterred quick acceptance of incorrect AI\nanswers. Contrarily, had IG explanations been perceived as more\nhelpful, participants might have more readily accepted incorrect\nanswers, as seen in other explanation conditions. This scenario\nunderscores the complex relationship between explanation helpful-\nness, participant skepticism, and performance accuracy, illustrating\na key dilemma in the effectiveness of XAI explanations.\n8.4 Explanation Confirmation Bias\nEmploying saliency maps for explaining generative text predictions\npresents challenges. At best, saliency maps offer users orientation\nwithin text corpora and disclose biases to machine learning prac-\ntitioners [4, 9]. At worst, they fail to elucidate the AI prediction\nprocess for humans, as documented by Fok and Weld [33], Lipton\n[50], Miller [53]. We suggest that saliency maps might foster and\nencourage what we call explanation confirmation bias where\nevaluators‚Äô preconceived expectations about the explanation are\nmatched with the actual explanation when evaluating explanations.\nFor example, participants might find that a saliency map highlights\nthe same words that they would highlight leading to a positive per-\nception towards the explanation. Morrison et al. [57] found some\nevidence for this bias in their study, noting in their qualitative re-\nview of comments that explanations could reassure participants\nand give them confidence if they supported the assessment of par-\nticipants, independent of whether that assessment was actually\ncorrect or not. In contrast, participants critiqued the AI or were\nincorrectly influenced by the AI if the AI was not in line with their\nassessment.\nWhen we generated machine explanations (conservative-LRP\nand integrated gradients) and tried to evaluate these explanations\nbefore our study, we displayed the same bias: when the machine-\ngenerated saliency map would highlight the same words we would\nhave highlighted had we been an AI we considered the explanation\nto be of high quality. If not, we were disappointed and frustrated.\nIn our study, we found that participants perceived human saliency\nmaps to be of higher quality than saliency maps produced with\nthe help of integrated gradients but only for correct answers . This\nfinding suggests a consensus among participants regarding the\nexpected highlighted words, with higher quality attributed to ex-\nplanations where the saliency maps matched these expectations.\nThe preference for human explanations in cases of correct answers\nindicates a shared criterion for important words that machine ex-\nplanations, particularly those created by integrated gradients, fail\nto consistently meet.\n9 RECOMMENDATIONS\nBased on our three study parts, we make the following recommen-\ndations for designers and researchers.\n9.1 Recommendations for Designers\n‚Ä¢Do not call saliency maps explanations. Given that ma-\nchine explanations were less helpful than human machine\nexplanations, which in turn were less performant than con-\ntrol conditions with no explanations, the efficacy of saliency\nmaps as explanation in text generation comes under question.\nFurthermore, given the dangers of explanation confirmation\nbias that we describe above we recommend not to speak of\nexplanation when providing saliency maps to users.\n‚Ä¢Provide saliency maps to help users explore data. A\nsaliency map can help people orient themselves in data. In\nour study, text of up to 175 words might have been too short\nto provide benefits, but for longer texts, saliency maps might\nenable efficient evaluation of AI predictions (as a starting\npoint). Atrey et al. [10] argue that saliency maps can help\ndesigners of AI to explore (rather than explain) agent behav-\nior in tasks concerning deep reinforcement learning (RL).\nSimilarly, human users of AI might use saliency maps to\nexplore (rather than explain) data input to the AI in tasks\nincluding text generation.\n‚Ä¢Encourage users to thoroughly evaluate AI predictions.\nIndependent of whether saliency maps are provided to users,\nencourage users to thoroughly evaluate AI predictions. To\nthat end, Bu√ßinca et al. [20] provides an excellent discussion\non cognitive forcing functions that can help trigger the ana-\nlytical processing system of users and decrease overreliance\non AI. In low stakes environments, it might be possible to\nforgo explanation mechanisms and forcing functions alto-\ngether and instead provide efficient tools to users to correct\nfaulty AI output. In cases where the AI is expected to outper-\nform human decision-makers, AI decisions can strategically\nbe accepted (see Fok and Weld [33] for a discussion on dif-\nferent forms of reliance).\n9.2 Recommendations for Researchers\n‚Ä¢Compare machine explanations to human explana-\ntions. To thoroughly evaluate XAI explanations we rec-\nommend to collect human explanations of the same type.\nBy restricting participants to the same type of explanation\n(e.g., saliency maps) when collecting explanations, this type\nmatching between human and machine explanations allows\nresearchers to provide a proper baseline to their investiga-\ntions. For example, in our study we learned that human\nsaliency maps are more helpful than machine saliency maps,\nbut found no significant performance differences between\ndifferent types of explanations and control conditions.\n‚Ä¢Evaluate machine explanations for incorrect AI predic-\ntions. We highly recommend that incorrect AI predictions\nare included when evaluating XAI explanations. Ideally, a\nfactorial design is provided in which explanations follow or\ndo not follow correct or incorrect AI predictions. This design\nallows researchers to investigate the effects of ‚Äúgood‚Äù expla-\nnations with incorrect predictions and ‚Äúbad‚Äù explanations\nwith correct predictions on AI compliance rates and whether\npeople exhibit explanation confirmation bias.\n10 LIMITATIONS\nAs every study, our study comes with limitations. As Schemmer et al.\n[70] has shown in their meta-analysis, effect sizes for XAI methods\nvs. AI assistance on performance can be very small (e.g., standard-\nized mean differences of 0.07). Our study was too under-powered\nto find these differences or exclude them (through equivalence\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Pafla, et al.\ntests). Future research needs to have even larger sample sizes to\ninvestigate the efficacy of different types of explanations.\nSecond, our research was conducted on naive participants that\nwere not informed or trained on what constitutes an explanation\nand what does not. Specifically, when we ask participants to rate\nexplanations, we show them only one type of explanation in a\nbetween-study design which makes it impossible for participants\nto compare different explanations with each other. Future research\nneeds to explicitly compare different types of explanations and\nallow participants to contrast them. Understanding the mental\nmodels participants hold of tasks, answers, and explanations will\ngive insight into how participants expect explanation to be.\nThird, as is the problem with many studies concerning the eval-\nuation of local explanations, we focused on the evaluation of indi-\nvidual explanations rather than the ‚Äúsocio-technical‚Äù system [19]\nthat makes up the human-AI team. Specifically, we did not give\nmeans to participants to interact with AI and explanations. As such,\nfuture research needs to adapt interactive, theory-driven AI such as\nMothilal et al.‚Äôs [58] counterfactual explanations to text generation.\nIdeally, human participants would have the ability to ask the AI\nmodel why some other potential answer is not the correct answer,\nforcing the AI to contrast its answer with contrastive cases. With\nthis ability, we might be able to deal with incorrect AI answers\nmore efficiently.\n11 CONCLUSION\nIn our study, we collected 156 human explanations for a question-\nanswering task and compared them to machine-generated expla-\nnations in a human-participant study ( ùëÅ = 136), covering both\ncorrect and incorrect answers. Analysis revealed a 21% overlap\nbetween human and machine (conservative-LRP and integrated\ngradients) saliency maps and humans primarily using text extrac-\ntions to justify their answers. These extractions, along with text\nexplanations, misunderstandings, and poor explanation attempts,\nwere categorized using a coding scheme by two independent raters.\nWhen evaluating explanations empirically, we found that the cor-\nrectness of the shown answer had a strong, significant effect on all\nmeasures (i.e., performance, time, quality, helpfulness, and mental\neffort), machine saliency maps (con-LRP and IG) were significantly\nless helpful than human and control saliency maps, participants\ntrusted text extractions more than ChatGPT explanations, and that\nmeasures of explanation satisfaction, trust in the AI, and expla-\nnation helpfulness were negatively correlated with performance\nscores. These findings hint at a dilemma in explanation, where\neffective explanation can actually decrease performance when sup-\nporting incorrect answers or predictions. In this context, we discuss\nthe danger of explanation confirmation bias in the evaluation of\nexplanations.\nACKNOWLEDGMENTS\nThank you to Ali Haider Rizvi, Reza Hadi Mogavi, Ville M√§kel√§,\nStacey Scott, the University of Waterloo‚Äôs Touchlab, UW-MAS,\nEngHCI group, and the Games Institute. This work was made pos-\nsible by the Natural Sciences and Engineering Research Council of\nCanada (NSERC Discovery Grant 2016-04422 and 2018-03962).\nREFERENCES\n[1] Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian Y. Lim, and Mohan Kankan-\nhalli. 2018. Trends and Trajectories for Explainable, Accountable and Intel-\nligible Systems: An HCI Research Agenda. In Proceedings of the 2018 CHI\nConference on Human Factors in Computing Systems (Montreal QC, Canada)\n(CHI ‚Äô18) . Association for Computing Machinery, New York, NY, USA, 1‚Äì18.\nhttps://doi.org/10.1145/3173574.3174156\n[2] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt,\nand Been Kim. 2018. Sanity Checks for Saliency Maps. In Advances in\nNeural Information Processing Systems , S. Bengio, H. Wallach, H. Larochelle,\nK. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Curran As-\nsociates, Inc. https://proceedings.neurips.cc/paper_files/paper/2018/file/\n294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf\n[3] Open AI. 2022. Introducing ChatGPT . https://openai.com/blog/chatgpt\n[4] Ameen Ali, Thomas Schnake, Oliver Eberle, Gr√©goire Montavon, Klaus-Robert\nM√ºller, and Lior Wolf. 2022. XAI for Transformers: Better Explanations Through\nConservative Propagation. In Proceedings of the 39th International Conference on\nMachine Learning (Proceedings of Machine Learning Research, Vol. 162) , Kamalika\nChaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan\nSabato (Eds.). PMLR, 435‚Äì451. https://proceedings.mlr.press/v162/ali22a.html\n[5] Ahmed Alqaraawi, Martin Schuessler, Philipp Wei√ü, Enrico Costanza, and Nadia\nBerthouze. 2020. Evaluating Saliency Map Explanations for Convolutional Neural\nNetworks: A User Study. In Proceedings of the 25th International Conference on\nIntelligent User Interfaces (Cagliari, Italy) (IUI ‚Äô20). Association for Computing Ma-\nchinery, New York, NY, USA, 275‚Äì285. https://doi.org/10.1145/3377325.3377519\n[6] David Alvarez Melis, Harmanpreet Kaur, Hal Daum√© III, Hanna Wallach, and\nJennifer Wortman Vaughan. 2021. From Human Explanation to Model Inter-\npretability: A Framework Based on Weight of Evidence. Proceedings of the AAAI\nConference on Human Computation and Crowdsourcing 9, 1 (Oct. 2021), 35‚Äì47.\nhttps://doi.org/10.1609/hcomp.v9i1.18938\n[7] Marco Ancona, Enea Ceolini, Cengiz √ñztireli, and Markus Gross. 2018. Towards\nBetter Understanding of Gradient-Based Attribution Methods for Deep Neural\nNetworks. In International Conference on Learning Representations . https://\nopenreview.net/forum?id=Sy21R9JAW\n[8] Sule Anjomshoae, Amro Najjar, Davide Calvaresi, and Kary Fr√§mling. 2019.\nExplainable Agents and Robots: Results From a Systematic Literature Review.\nIn Proceedings of the 18th International Conference on Autonomous Agents and\nMultiAgent Systems (Montreal QC, Canada) (AAMAS ‚Äô19). International Founda-\ntion for Autonomous Agents and Multiagent Systems, Richland, SC, 1078‚Äì1088.\nhttps://dl.acm.org/doi/10.5555/3306127.3331806\n[9] Leila Arras, Gr√©goire Montavon, Klaus-Robert M√ºller, and Wojciech Samek. 2017.\nExplaining Recurrent Neural Network Predictions in Sentiment Analysis. In\nProceedings of the 8th Workshop on Computational Approaches to Subjectivity,\nSentiment and Social Media Analysis . Association for Computational Linguistics,\nCopenhagen, Denmark, 159‚Äì168. https://doi.org/10.18653/v1/W17-5221\n[10] Akanksha Atrey, Kaleigh Clary, and David Jensen. 2020. Exploratory Not\nExplanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforce-\nment Learning. In International Conference on Learning Representations . https:\n//openreview.net/forum?id=rkl3m1BFDB\n[11] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine\nTranslation by Jointly Learning to Align and Translate. In 3rd International\nConference on Learning Representations, ICLR 2015, San Diego, CA, USA, May\n7-9, 2015, Conference Track Proceedings , Yoshua Bengio and Yann LeCun (Eds.).\nhttp://arxiv.org/abs/1409.0473\n[12] Nikola Banovic, Zhuoran Yang, Aditya Ramesh, and Alice Liu. 2023. Being\nTrustworthy is Not Enough: How Untrustworthy Artificial Intelligence (AI) Can\nDeceive the End-Users and Gain Their Trust. Proc. ACM Hum.-Comput. Interact.\n7, CSCW1, Article 27 (April 2023), 17 pages. https://doi.org/10.1145/3579460\n[13] Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece\nKamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the Whole Exceed Its\nParts? The Effect of AI Explanations on Complementary Team Performance. In\nProceedings of the 2021 CHI Conference on Human Factors in Computing Systems\n(Yokohama, Japan) (CHI ‚Äô21) . Association for Computing Machinery, New York,\nNY, USA, Article 81, 16 pages. https://doi.org/10.1145/3411764.3445717\n[14] Alejandro Barredo Arrieta, Natalia D√≠az-Rodr√≠guez, Javier Del Ser, Adrien Ben-\nnetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel\nMolina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Ex-\nplainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities\nand Challenges Toward Responsible AI. Information Fusion 58 (2020), 82‚Äì115.\nhttps://doi.org/10.1016/j.inffus.2019.12.012\n[15] Jasmijn Bastings and Katja Filippova. 2020. The Elephant in the Interpretability\nRoom: Why Use Attention As Explanation When We Have Saliency Methods?.\nIn Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP . Association for Computational Linguistics, Online,\n149‚Äì155. https://doi.org/10.18653/v1/2020.blackboxnlp-1.14\n[16] Virginia Braun and Victoria Clarke. 2019. Reflecting on Reflexive Thematic\nAnalysis. Qualitative Research in Sport, Exercise and Health 11, 4 (2019), 589‚Äì597.\nUnraveling the Dilemma of AI Errors CHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA\nhttps://doi.org/10.1080/2159676X.2019.1628806\n[17] Virginia Braun, Victoria Clarke, Nikki Hayfield, and Gareth Terry. 2019.Thematic\nAnalysis. Springer Singapore, Singapore, 843‚Äì860. https://doi.org/10.1007/978-\n981-10-5251-4_103\n[18] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,\nChris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\nIn Advances in Neural Information Processing Systems , H. Larochelle, M. Ran-\nzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates,\nInc., 1877‚Äì1901. https://proceedings.neurips.cc/paper_files/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n[19] Zana Bu√ßinca, Phoebe Lin, Krzysztof Z. Gajos, and Elena L. Glassman. 2020. Proxy\nTasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI\nSystems. In Proceedings of the 25th International Conference on Intelligent User\nInterfaces (Cagliari, Italy) (IUI ‚Äô20) . Association for Computing Machinery, New\nYork, NY, USA, 454‚Äì464. https://doi.org/10.1145/3377325.3377498\n[20] Zana Bu√ßinca, Maja Barbara Malaya, and Krzysztof Z. Gajos. 2021. To Trust\nor to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in\nAI-Assisted Decision-Making. Proc. ACM Hum.-Comput. Interact. 5, CSCW1,\nArticle 188 (April 2021), 21 pages. https://doi.org/10.1145/3449287\n[21] Captum. 2020. Interpreting BERT Models (Part 1) . https://captum.ai/tutorials/\nBert_SQUAD_Interpret#Interpreting-BERT-Models-(Part-1)\n[22] Michael Chromik, Malin Eiband, Sarah Theres V√∂lkel, and Daniel Buschek. 2019.\nDark Patterns of Explainability, Transparency, and User Control for Intelligent\nSystems.. In IUI workshops , Vol. 2327. https://ceur-ws.org/Vol-2327/IUI19WS-\nExSS2019-7.pdf\n[23] Michael Chromik and Martin Schuessler. 2020. A Taxonomy for Human Subject\nEvaluation of Black-Box Explanations in XAI. In Proceedings of the Workshop on\nExplainable Smart Systems for Algorithmic Transparency in Emerging Technologies\nco-located with 25th International Conference on Intelligent User Interfaces (IUI\n2020), Cagliari, Italy, March 17, 2020 (CEUR Workshop Proceedings, Vol. 2582) ,\nAlison Smith-Renner, Styliani Kleanthous, Brian Y. Lim, Tsvi Kuflik, Simone\nStumpf, Jahna Otterbacher, Advait Sarkar, Casey Dugan, and Avital Shulner Tal\n(Eds.). CEUR-WS.org. http://ceur-ws.org/Vol-2582/paper9.pdf\n[24] Roberto Confalonieri, Ludovik Coba, Benedikt Wagner, and Tarek R. Besold. 2021.\nA Historical Perspective of Explainable Artificial Intelligence.WIREs Data Mining\nand Knowledge Discovery 11, 1 (2021), e1391. https://doi.org/10.1002/widm.1391\n[25] Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and\nPrithviraj Sen. 2020. A Survey of the State of Explainable AI for Natural Language\nProcessing. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the\nAssociation for Computational Linguistics and the 10th International Joint Confer-\nence on Natural Language Processing . Association for Computational Linguistics,\nSuzhou, China, 447‚Äì459. https://aclanthology.org/2020.aacl-main.46\n[26] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-Training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association\nfor Computational Linguistics, Minneapolis, Minnesota, 4171‚Äì4186. https://doi.\norg/10.18653/v1/N19-1423\n[27] Shuoyang Ding and Philipp Koehn. 2021. Evaluating Saliency Methods for\nNeural Language Models. In Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies. Association for Computational Linguistics, Online, 5034‚Äì5052.\nhttps://doi.org/10.18653/v1/2021.naacl-main.399\n[28] Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong Sun. 2017. Visualizing and\nUnderstanding Neural Machine Translation. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers) .\nAssociation for Computational Linguistics, Vancouver, Canada, 1150‚Äì1159. https:\n//doi.org/10.18653/v1/P17-1106\n[29] Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Inter-\npretable Machine Learning. https://doi.org/10.48550/ARXIV.1702.08608\n[30] Mark Edmonds, Feng Gao, Hangxin Liu, Xu Xie, Siyuan Qi, Brandon Rothrock,\nYixin Zhu, Ying Nian Wu, Hongjing Lu, and Song-Chun Zhu. 2019. A Tale of Two\nExplanations: Enhancing Human Trust by Explaining Robot Behavior. Science\nRobotics 4, 37 (2019), eaay4663. https://doi.org/10.1126/scirobotics.aay4663\n[31] Upol Ehsan and Mark O. Riedl. 2021. Explainability Pitfalls: Beyond Dark Patterns\nin Explainable AI. https://doi.org/10.48550/ARXIV.2109.12480\n[32] Shi Feng and Jordan Boyd-Graber. 2019. What Can AI Do for Me? Evaluating\nMachine Learning Interpretations in Cooperative Play. In Proceedings of the 24th\nInternational Conference on Intelligent User Interfaces (Marina del Ray, California)\n(IUI ‚Äô19) . Association for Computing Machinery, New York, NY, USA, 229‚Äì239.\nhttps://doi.org/10.1145/3301275.3302265\n[33] Raymond Fok and Daniel S. Weld. 2023. In Search of Verifiability: Explanations\nRarely Enable Complementary Performance in AI-Advised Decision Making.\nArXiv abs/2305.07722 (2023). https://doi.org/10.48550/arXiv.2305.07722\n[34] Marzyeh Ghassemi, Luke Oakden-Rayner, and Andrew L. Beam. 2021. The False\nHope of Current Approaches to Explainable Artificial Intelligence in Health Care.\nThe Lancet Digital Health 3, 11 (01 11 2021), e745‚Äìe750. https://doi.org/10.1016/\nS2589-7500(21)00208-9\n[35] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca\nGiannotti, and Dino Pedreschi. 2018. A Survey of Methods for Explaining Black\nBox Models. ACM Comput. Surv. 51, 5, Article 93 (Aug. 2018), 42 pages. https:\n//doi.org/10.1145/3236009\n[36] David Gunning, Eric Vorm, Jennifer Yunyan Wang, and Matt Turek.\n2021. DARPA‚Äôs Explainable AI (XAI) Program: A Retrospective.\nApplied AI Letters 2, 4 (2021), e61. https://doi.org/10.1002/ail2.61\narXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/ail2.61\n[37] Joseph Y. Halpern and Judea Pearl. 2005. Causes and Explanations: A Structural-\nModel Approach. Part I: Causes. The British Journal for the Philosophy of Science\n56, 4 (12 2005), 843‚Äì887. https://doi.org/10.1093/bjps/axi147\n[38] Joseph Y. Halpern and Judea Pearl. 2005. Causes and Explanations: A Structural-\nModel Approach. Part II: Explanations. The British Journal for the Philosophy of\nScience 56, 4 (12 2005), 889‚Äì911. https://doi.org/10.1093/bjps/axi148\n[39] Robert R. Hoffman, Shane T. Mueller, Gary Klein, and Jordan Litman. 2018.\nMetrics for Explainable AI: Challenges and Prospects. https://doi.org/10.48550/\nARXIV.1812.04608\n[40] HuggingFace. 2023. Large, uncased BERT with whole-word masking, finetuned\non SQuAD. https://huggingface.co/bert-large-uncased-whole-word-masking-\nfinetuned-squad\n[41] Maia Jacobs, Melanie F. Pradier, Thomas H. McCoy, Roy H. Perlis, Finale Doshi-\nVelez, and Krzysztof Z. Gajos. 2021. How Machine-Learning Recommendations In-\nfluence Clinician Treatment Selections: The Example of Antidepressant Selection.\nTranslational Psychiatry 11, 1 (04 Feb. 2021), 108. https://doi.org/10.1038/s41398-\n021-01224-x\n[42] Sarthak Jain and Byron C. Wallace. 2019. Attention Is Not Explanation. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers). Association for Computational Linguistics, Minneapolis, Minnesota,\n3543‚Äì3556. https://doi.org/10.18653/v1/N19-1357\n[43] Ann M. Bisantz Jiun-Yin Jian and Colin G. Drury. 2000. Foundations for an\nEmpirically Determined Scale of Trust in Automated Systems. International\nJournal of Cognitive Ergonomics 4, 1 (2000), 53‚Äì71. https://doi.org/10.1207/\nS15327566IJCE0401_04\n[44] Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach,\nand Jennifer Wortman Vaughan. 2020. Interpreting Interpretability: Under-\nstanding Data Scientists‚Äô Use of Interpretability Tools for Machine Learning. In\nProceedings of the 2020 CHI Conference on Human Factors in Computing Systems\n(Honolulu, HI, USA) (CHI ‚Äô20) . Association for Computing Machinery, New York,\nNY, USA, 1‚Äì14. https://doi.org/10.1145/3313831.3376219\n[45] Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh,\nJonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi\nYan, and Orion Reblitz-Richardson. 2020. Captum: A Unified and Generic Model\nInterpretability Library for PyTorch. arXiv:2009.07896\n[46] Todd Kulesza, Simone Stumpf, Margaret Burnett, Sherry Yang, Irwin Kwan, and\nWeng-Keen Wong. 2013. Too Much, Too Little, or Just Right? Ways Explanations\nImpact End Users‚Äô Mental Models. In 2013 IEEE Symposium on Visual Languages\nand Human Centric Computing . 3‚Äì10. https://doi.org/10.1109/VLHCC.2013.\n6645235\n[47] Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman,\nand Finale Doshi-Velez. 2019. An Evaluation of the Human-Interpretability of\nExplanation. CoRR abs/1902.00006 (2019). arXiv:1902.00006 http://arxiv.org/abs/\n1902.00006\n[48] Vivian Lai, Han Liu, and Chenhao Tan. 2020. ‚ÄúWhy is ‚ÄòChicago‚Äô Deceptive?‚Äù\nTowards Building Model-Driven Tutorials for Humans. In Proceedings of the 2020\nCHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA)\n(CHI ‚Äô20) . Association for Computing Machinery, New York, NY, USA, 1‚Äì13.\nhttps://doi.org/10.1145/3313831.3376873\n[49] Yann LeCun, Y. Bengio, and Geoffrey Hinton. 2015. Deep Learning. Nature 521\n(05 2015), 436‚Äì44. https://doi.org/10.1038/nature14539\n[50] Zachary C. Lipton. 2018. The Mythos of Model Interpretability: In Machine\nLearning, the Concept of Interpretability is Both Important and Slippery. Queue\n16, 3 (June 2018), 31‚Äì57. https://doi.org/10.1145/3236386.3241340\n[51] Scott M Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model\nPredictions. In Advances in Neural Information Processing Systems , I. Guyon,\nU. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett\n(Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_\nfiles/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf\n[52] Maria Madsen and Shirley Gregor. 2000. Measuring Human-Computer Trust. In\n11th Australasian Conference on Information Systems , Vol. 53. Citeseer, 6‚Äì8.\nCHI ‚Äô24, May 11‚Äì16, 2024, Honolulu, HI, USA Pafla, et al.\n[53] Tim Miller. 2019. Explanation in Artificial Intelligence: Insights From the Social\nSciences. Artificial Intelligence 267 (2019), 1‚Äì38. https://doi.org/10.1016/j.artint.\n2018.07.007\n[54] Sina Mohseni, Niloofar Zarei, and Eric D. Ragan. 2021. A Multidisciplinary\nSurvey and Framework for Design and Evaluation of Explainable AI Systems.\nACM Trans. Interact. Intell. Syst. 11, 3‚Äì4, Article 24 (Sept. 2021), 45 pages. https:\n//doi.org/10.1145/3387166\n[55] Lillio Mok, Sasha Nanda, and Ashton Anderson. 2023. People Perceive Algo-\nrithmic Assessments as Less Fair and Trustworthy Than Identical Human As-\nsessments. Proc. ACM Hum.-Comput. Interact. 7, CSCW2, Article 309 (Oct. 2023),\n26 pages. https://doi.org/10.1145/3610100\n[56] Gr√©goire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek,\nand Klaus-Robert M√ºller. 2019. Layer-Wise Relevance Propagation: An Overview .\nSpringer International Publishing, Cham, 193‚Äì209. https://doi.org/10.1007/978-\n3-030-28954-6_10\n[57] Katelyn Morrison, Donghoon Shin, Kenneth Holstein, and Adam Perer. 2023.\nEvaluating the Impact of Human Explanation Strategies on Human-AI Visual\nDecision-Making. Proc. ACM Hum.-Comput. Interact. 7, CSCW1, Article 48 (April\n2023), 37 pages. https://doi.org/10.1145/3579481\n[58] Ramaravind K. Mothilal, Amit Sharma, and Chenhao Tan. 2020. Explaining\nMachine Learning Classifiers Through Diverse Counterfactual Explanations. In\nProceedings of the 2020 Conference on Fairness, Accountability, and Transparency\n(Barcelona, Spain) (FAT* ‚Äô20). Association for Computing Machinery, New York,\nNY, USA, 607‚Äì617. https://doi.org/10.1145/3351095.3372850\n[59] Fred Paas. 1992. Training Strategies for Attaining Transfer of Problem-Solving\nSkill in Statistics: A Cognitive-Load Approach. Journal of Educational Psychology\n84 (12 1992), 429‚Äì434. https://psycnet.apa.org/doi/10.1037/0022-0663.84.4.429\n[60] Pafla, Marvin. 2020. Researching Human-AI Collaboration Through the Design of\nLanguage-Based Query Assistance . Master‚Äôs thesis. http://hdl.handle.net/10012/\n16250\n[61] Charles Pierse. 2021. Transformers Interpret . https://github.com/cdpierse/\ntransformers-interpret\n[62] Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wort-\nman Wortman Vaughan, and Hanna Wallach. 2021. Manipulating and Mea-\nsuring Model Interpretability. In Proceedings of the 2021 CHI Conference on\nHuman Factors in Computing Systems (Yokohama, Japan) (CHI ‚Äô21) . Associa-\ntion for Computing Machinery, New York, NY, USA, Article 237, 52 pages.\nhttps://doi.org/10.1145/3411764.3445315\n[63] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2018. Language Models are Unsupervised Multitask Learners. (2018).\nhttps://openai.com/blog/better-language-models/\n[64] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.\nSQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceed-\nings of the 2016 Conference on Empirical Methods in Natural Language Pro-\ncessing. Association for Computational Linguistics, Austin, Texas, 2383‚Äì2392.\nhttps://doi.org/10.18653/v1/D16-1264\n[65] V. Ramanishka, A. Das, J. Zhang, and K. Saenko. 2017. Top-Down Visual Saliency\nGuided by Captions. In 2017 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) . IEEE Computer Society, Los Alamitos, CA, USA, 3135‚Äì3144.\nhttps://doi.org/10.1109/CVPR.2017.334\n[66] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. ‚ÄúWhy Should I\nTrust You?‚Äù: Explaining the Predictions of Any Classifier. InProceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining (San Francisco, California, USA) (KDD ‚Äô16) . Association for Computing\nMachinery, New York, NY, USA, 1135‚Äì1144. https://doi.org/10.1145/2939672.\n2939778\n[67] Cynthia Rudin. 2019. Stop Explaining Black Box Machine Learning Models for\nHigh Stakes Decisions and Use Interpretable Models Instead. Nature Machine\nIntelligence 1, 5 (01 May 2019), 206‚Äì215. https://doi.org/10.1038/s42256-019-\n0048-x\n[68] Wojciech Samek, Gr√©goire Montavon, Sebastian Lapuschkin, Christopher J. An-\nders, and Klaus-Robert M√ºller. 2021. Explaining Deep Neural Networks and\nBeyond: A Review of Methods and Applications.Proc. IEEE 109, 3 (2021), 247‚Äì278.\nhttps://doi.org/10.1109/JPROC.2021.3060483\n[69] Wojciech Samek, Thomas Wiegand, and Klaus-Robert M√ºller. 2017. Explainable\nArtificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning\nModels. CoRR abs/1708.08296 (2017). http://arxiv.org/abs/1708.08296\n[70] Max Schemmer, Patrick Hemmer, Maximilian Nitsche, Niklas K√ºhl, and Michael\nV√∂ssing. 2022. A Meta-Analysis of the Utility of Explainable Artificial Intelligence\nin Human-AI Decision-Making. In Proceedings of the 2022 AAAI/ACM Conference\non AI, Ethics, and Society (Oxford, United Kingdom) (AIES ‚Äô22) . Association for\nComputing Machinery, New York, NY, USA, 617‚Äì626. https://doi.org/10.1145/\n3514094.3534128\n[71] Philipp Schmidt and Felix Biessmann. 2019. Quantifying Interpretability and\nTrust in Machine Learning Systems. arXiv:1901.08558\n[72] Hendrik Schuff, Heike Adel, Peng Qi, and Ngoc Thang Vu. 2023. Challenges in\nExplanation Quality Evaluation. arXiv:2210.07126\n[73] Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju.\n2020. Fooling LIME and SHAP: Adversarial Attacks on Post Hoc Explanation\nMethods. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society\n(New York, NY, USA) (AIES ‚Äô20) . Association for Computing Machinery, New\nYork, NY, USA, 180‚Äì186. https://doi.org/10.1145/3375627.3375830\n[74] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda B. Vi√©gas, and Martin Watten-\nberg. 2017. SmoothGrad: Removing Noise by Adding Noise. CoRR abs/1706.03825\n(2017). arXiv:1706.03825 http://arxiv.org/abs/1706.03825\n[75] Kacper Sokol and Peter Flach. 2020. One Explanation Does Not Fit All. KI -\nK√ºnstliche Intelligenz 34, 2 (01 06 2020), 235‚Äì250. https://doi.org/10.1007/s13218-\n020-00637-y\n[76] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic Attribution\nfor Deep Networks. InProceedings of the 34th International Conference on Machine\nLearning - Volume 70 (Sydney, NSW, Australia)(ICML‚Äô17). JMLR.org, 3319‚Äì3328.\nhttps://dl.acm.org/doi/10.5555/3305890.3306024\n[77] Jasper van der Waa, Elisabeth Nieuwburg, Anita Cremers, and Mark Neerincx.\n2021. Evaluating XAI: A Comparison of Rule-Based and Example-Based Expla-\nnations. Artificial Intelligence 291 (2021), 103404. https://doi.org/10.1016/j.artint.\n2020.103404\n[78] Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui.\n2020. Attention Interpretability Across NLP Tasks. https://openreview.net/\nforum?id=BJe-_CNKPH\n[79] Nadya Vasilyeva, Daniel Wilkenfeld, and Tania Lombrozo. 2017. Contextual\nUtility Affects the Perceived Quality of Explanations. Psychonomic Bulletin &\nReview 24, 5 (01 06 2017), 1436‚Äì1450. https://doi.org/10.3758/s13423-017-1275-y\n[80] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nYou Need. In Advances in Neural Information Processing Systems , I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),\nVol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/\n2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[81] Oleksandra Vereschak, Gilles Bailly, and Baptiste Caramiaux. 2021. How to Evalu-\nate Trust in AI-Assisted Decision Making? A Survey of Empirical Methodologies.\nProc. ACM Hum.-Comput. Interact. 5, CSCW2, Article 327 (Oct. 2021), 39 pages.\nhttps://doi.org/10.1145/3476068\n[82] Jesse Vig. 2019. A Multiscale Visualization of Attention in the Transformer Model.\nCoRR abs/1906.05714 (2019). arXiv:1906.05714 http://arxiv.org/abs/1906.05714\n[83] Giulia Vilone and Luca Longo. 2021. Notions of Explainability and Evaluation\nApproaches for Explainable Artificial Intelligence. Information Fusion 76 (2021),\n89‚Äì106. https://doi.org/10.1016/j.inffus.2021.05.009\n[84] Sandra Wachter, Brent Mittelstadt, and Chris Russell. 2018. Counterfactual\nExplanations Without Opening the Black Box: Automated Decisions and the\nGDPR. Harvard Journal of Law and Technology 31, 2 (2018), 841‚Äì887. https:\n//doi.org/10.48550/arXiv.1711.00399\n[85] Danding Wang, Qian Yang, Ashraf Abdul, and Brian Y. Lim. 2019. Design-\ning Theory-Driven User-Centric Explainable AI. In Proceedings of the 2019 CHI\nConference on Human Factors in Computing Systems (Glasgow, Scotland Uk)\n(CHI ‚Äô19) . Association for Computing Machinery, New York, NY, USA, 1‚Äì15.\nhttps://doi.org/10.1145/3290605.3300831\n[86] Daniel S. Weld and Gagan Bansal. 2019. The Challenge of Crafting Intelligible\nIntelligence. Commun. ACM 62, 6 (May 2019), 70‚Äì79. https://doi.org/10.1145/\n3282486\n[87] Sarah Wiegreffe and Yuval Pinter. 2019. Attention Is Not Not Explanation. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China,\n11‚Äì20. https://doi.org/10.18653/v1/D19-1002\n[88] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz, and Jamie\nBrew. 2019. HuggingFace‚Äôs Transformers: State-of-the-Art Natural Language\nProcessing. ArXiv (2019). https://arxiv.org/abs/1910.03771\n[89] Qian Yang, Aaron Steinfeld, Carolyn Ros√©, and John Zimmerman. 2020. Re-\nExamining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult\nto Design. In Proceedings of the 2020 CHI Conference on Human Factors in Comput-\ning Systems (Honolulu, HI, USA)(CHI ‚Äô20). Association for Computing Machinery,\nNew York, NY, USA, 1‚Äì13. https://doi.org/10.1145/3313831.3376301\n[90] Yunfeng Zhang, Q. Vera Liao, and Rachel K. E. Bellamy. 2020. Effect of Confi-\ndence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision\nMaking. In Proceedings of the 2020 Conference on Fairness, Accountability, and\nTransparency (Barcelona, Spain) (FAT* ‚Äô20). Association for Computing Machin-\nery, New York, NY, USA, 295‚Äì305. https://doi.org/10.1145/3351095.3372852\n[91] Jianlong Zhou, Amir H. Gandomi, Fang Chen, and Andreas Holzinger. 2021.\nEvaluating the Quality of Machine Learning Explanations: A Survey on Methods\nand Metrics. Electronics 10, 5 (2021). https://doi.org/10.3390/electronics10050593"
}