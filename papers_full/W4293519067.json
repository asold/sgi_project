{
  "title": "Dynamic Multi-Scale Network for Dual-Pixel Images Defocus Deblurring with Transformer",
  "url": "https://openalex.org/W4293519067",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2098958711",
      "name": "Dafeng Zhang",
      "affiliations": [
        "Samsung (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2097823643",
      "name": "Xiaobing Wang",
      "affiliations": [
        "Samsung (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2866634454",
    "https://openalex.org/W3137278571",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6787172444",
    "https://openalex.org/W3109494165",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W3174970555",
    "https://openalex.org/W2147298660",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W4287330714",
    "https://openalex.org/W2901996700",
    "https://openalex.org/W1916731006",
    "https://openalex.org/W2969717429",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2767829160",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W2161804069",
    "https://openalex.org/W2911581663",
    "https://openalex.org/W2953106684",
    "https://openalex.org/W3204896399",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W1986489799",
    "https://openalex.org/W2008990620",
    "https://openalex.org/W4229862590",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W4308909683",
    "https://openalex.org/W54257720",
    "https://openalex.org/W4225672218",
    "https://openalex.org/W2963182372",
    "https://openalex.org/W2964030969",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W3175448204",
    "https://openalex.org/W2059639161",
    "https://openalex.org/W2292934097",
    "https://openalex.org/W3170697543",
    "https://openalex.org/W4293433089",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2520164769",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W2276154416",
    "https://openalex.org/W3177732058",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W4312812783",
    "https://openalex.org/W2954432404"
  ],
  "abstract": "Recent works achieve excellent results in defocus deblurring task based on dual-pixel data using convolutional neural network (CNN), while the scarcity of data limits the exploration and attempt of vision transformer in this task. In addition, the existing works use fixed parameters and network architecture to deblur images with different distribution and content information, which also affects the generalization ability of the model. In this paper, we propose a dynamic multi-scale network, named DMTNet, for dual-pixel images defocus deblurring. DMTNet mainly contains two modules: feature extraction module and reconstruction module. The feature extraction module is composed of several vision transformer blocks, which uses its powerful feature extraction capability to obtain richer features and improve the robustness of the model. The reconstruction module is composed of several Dynamic Multi-scale Sub-reconstruction Module (DMSSRM). DMSSRM can restore images by adaptively assigning weights to features from different scales according to the blur distribution and content information of the input images. DMTNet combines the advantages of transformer and CNN, in which the vision transformer improves the performance ceiling of CNN, and the inductive bias of CNN enables transformer to extract more robust features without relying on a large amount of data. DMTNet might be the first attempt to use vision transformer to restore the blurring images to clarity. By combining with CNN, the vision transformer may achieve better performance on small datasets. Experimental results on the popular benchmarks demonstrate that our DMTNet significantly outperforms state-of-the-art methods.",
  "full_text": "DMTNet: Dynamic Multi-scale Network for Dual-pixel Images Defocus\nDeblurring with Transformer\nDafeng Zhang1* Xiaobing Wang1*\n1 Samsung Research China - Beijing (SRC-B)\n{dafeng.zhang, x0106.wang}@samsung.com\nAbstract\nRecent works achieve excellent results in defocus deblur-\nring task based on dual-pixel data using convolutional neu-\nral network (CNN), while the scarcity of data limits the ex-\nploration and attempt of vision transformer in this task. In\naddition, the existing works use fixed parameters and net-\nwork architecture to deblur images with different distribu-\ntion and content information, which also affects the gen-\neralization ability of the model. In this paper, we propose\na dynamic multi-scale network, named DMTNet, for dual-\npixel images defocus deblurring. DMTNet mainly contains\ntwo modules: feature extraction module and reconstruc-\ntion module. The feature extraction module is composed\nof several vision transformer blocks, which uses its pow-\nerful feature extraction capability to obtain richer features\nand improve the robustness of the model. The reconstruc-\ntion module is composed of several Dynamic Multi-scale\nSub-reconstruction Module (DMSSRM). DMSSRM can re-\nstore images by adaptively assigning weights to features\nfrom different scales according to the blur distribution and\ncontent information of the input images. DMTNet combines\nthe advantages of transformer and CNN, in which the vision\ntransformer improves the performance ceiling of CNN, and\nthe inductive bias of CNN enables transformer to extract\nmore robust features without relying on a large amount of\ndata. DMTNet might be the first attempt to use vision trans-\nformer to restore the blurring images to clarity. By com-\nbining with CNN, the vision transformer may achieve better\nperformance on small datasets. Experimental results on the\npopular benchmarks demonstrate that our DMTNet signifi-\ncantly outperforms state-of-the-art methods.\n1. Introduction\nIn modern cameras, dual-pixel sensors (photodiodes) are\nused for autofocus [16, 17, 42]. When the photoelectric sig-\nnals of the left and right photodiode is coincident, it indi-\n*Equal contribution.\nFigure 1. The results of defocus deblurring on the Canon DP\ndataset [1]. Under different parameter capacities, our DMTNet\nachieves the best performance than existing works.\ncates that the object is on the depth of field (DoF) of the\ncamera and the image is clear. Conversely, when the sig-\nnals from the left and right sensors are phase shifted, defo-\ncus blur will appear in the photos. Defocus blur may affect\nthe performance of subsequent computer vision tasks. For\nexample, in image semantic or instance segmentation tasks,\npixels in the defocus blurring region cannot be segmented\ncorrectly. Especially in the driverless cars, if lane line or\nfreespace cannot be detected correctly in the defocus blur-\nring region, it will lead to the wrong decision in the vehicle\nidentification system and threaten the safety of passengers.\nTherefore, defocus deblurring is a fundamental and neces-\nsary research to avoid the above problems.\nA conventional defocus deblurring method first esti-\nmates the defocus blurring map, and then recovers the blur-\nring image to clarity according to the blurring map, which\nis called two-stage method. The performances of the two-\nstage methods are affected by the results of defocus blur-\nring map estimation and deblurring algorithm simultane-\n1\narXiv:2209.06040v1  [cs.CV]  13 Sep 2022\nously. Abuolaim et al. [1] propose an end-to-end defocus\ndeblurring convolution neural network (DPDNet) without\nblurring map estimation. At the same time, they propose\na defocus deblurring dataset, named Canon DP data, based\non the dual-pixel (DP) camera, which includes left and right\ndefocus blurring images and corresponding sharp image.\nCompared with the two-stage methods, DPDNet achieves\nthe best performance on Canon DP dataset and takes less\ntime. However, the blurring image and the sharp image in\nCannon DP data are taken under different camera configu-\nration, and there is a problem that the input blurring images\nare not aligned with the ground truth. To solve this prob-\nlem, Abuolaim et al. [2] further explore the causes of de-\nfocus blur in the modern cameras. They use standard com-\nputer graphics-generated imagery to generate a more real-\nistic synthetic DP dataset, in which the input defocus blur-\nring images and ground truth are aligned. They train a new\nrecurrent convolution network, named RDPDNet, with the\nsynthetic DP dataset to improve the performance.\nDPDNet and RDPDNet are the current state-of-the-art\nsolutions. They all use convolution neural networks (CNN)\nas the basic operation to restore the blurring image to\nclear. Thanks to the powerful inductive bias of CNN, both\nDPDNet and RDPDNet have achieved exciting results on\nCanon DP dataset. The hard inductive biases of CNN im-\nproves the performance of sample-efficient learning, while\nlocal constraints of inductive biases lowers the performance\nceiling [8]. It means that the CNN has bad generalization\nand robustness on unknown data. Vision transformer breaks\nthe local constraints of CNN, improves feature extraction\ncapabilities, and significantly exceeds the performance of\nCNN in multiple vision tasks. However, the vision trans-\nformer needs to be trained on large scale dataset to give full\nplay to its advantages [11].\nAnother significant drawback of DPDNet and RDPDNet\nis that a neural network with the same parameters and ar-\nchitecture is used to restore the blurring images with dif-\nferent blur distribution or content information, which will\ncause serious problems. The deblurring performance is sat-\nisfactory on images with a specific blur distribution, but the\nrobustness to other distribution is very bad. This is why\nDPDNet and RDPDNet have poor generalization and per-\nformance. Collecting a large amount of real data may alle-\nviate the problem, but it is extremely expensive.\nIn order to obtain better deblurring performance and im-\nprove the robustness of the model, this paper proposes a\ndynamic multi-scale network, named DMTNet, for dual-\npixel images defocus deblurring. DMTNet mainly contains\ntwo modules: feature extraction module and reconstruc-\ntion module. The feature extraction module is composed\nof several vision transformer blocks, which uses its pow-\nerful feature extraction capability to obtain richer features\nand improve the robustness of the model. The reconstruc-\ntion module is composed of several Dynamic Multi-scale\nSub-reconstruction Module (DMSSRM). DMSSRM can re-\nstore images by adaptively assigning weights to features\nfrom different scales according to the blur distribution and\ncontent information of the input images. Compared with\nthe DPDNet or RDPDNet with the fixed weights and archi-\ntecture, the adaptive dynamic network can significantly im-\nprove the performance of deblurring. The feature extraction\nmodule based on the vision transformer is used to extract ro-\nbust features, while the reconstruction module is composed\nof CNN to restore the blurring images to clarity. Therefore,\nour DMTNet takes advantage of both the powerful feature\nextraction capability of vision transformer and the inductive\nbias capability of CNN. The transformer improves the per-\nformance ceiling of CNN, and the inductive bias property\nof CNN enables transformer to extract more robust features\nwithout relying on a large amount of data.\nWe demonstrate the effectiveness of our DMTNet in\ndual-pixel defocus deblurring dataset (Canon DP data),\nwhich achieves the state-of-the-art performance. As shown\nin Figure 1, we significantly improve the performance of de-\nfocus deblurring on the Canon DP testing data without using\nadditional training data. When we only use the transformer\nbased feature extraction module to restore the defocus blur-\nring images, our method achieves comparable performance\nto SOTA method (DPDNet), while number of parameters\nare reduced by 96.44%. Especially, when the number of pa-\nrameters of our method is comparable to the SOTA method\n(RDPD+), our DMTNet-B achieves 26.63 dB and 0.834 on\nPSNR and SSIM respectively, 1.24dB higher than RDPD+.\nIn the reconstruction module, the number of DMSSRM can\nbe flexibly increased or decreased according to the task and\nperformance requirements. When we use four cascaded\nDMSSRM to restore the defocus blurring image, the de-\nblurring performance will be further improved.\nOur contributions can be summarized as follows:\n• We propose a feature extraction module based on vi-\nsion transformer, which improves the feature extrac-\ntion ability and robustness of the model;\n• We design an efficient dynamic multi-scale sub-\nreconstruction module (DMSSRM) based on CNN,\nwhich adaptively selects and fuses features from dif-\nferent scales according to blur distribution and content\ninformation of the input images to obtain higher per-\nformance.\n• We propose a defocus blurring image restoration net-\nwork named DMTNet, which is mainly composed of\nfeature extraction module and reconstruction module.\nDMTNet combines the strength of the vision trans-\nformer and CNN, in which vision transformer im-\nproves the performance ceiling of CNN and the induc-\ntive bias of CNN enables transformer to extract more\n2\nrobust features without relying on a large amount of\ndata.\n• Our DMTNet achieves state-of-the-art performance on\nCanon DP data without any additional dataset and sig-\nnificantly outperforms existing works. We also imple-\nment several ablation studies to verify the effectiveness\nof our DMTNet.\n2. Related Work\n2.1. Defocus deblurring\nDefocus deblurring methods can be divided into two cat-\negories: a two-stage method [12, 18, 25, 36, 40, 45] using\ndefocus blurring map to guide deblurring and an end-to-end\none-stage method [1, 2, 37]. In the two-stage method, in\norder to generate the defocus blurring map, Yi et al. [49]\ntransform the blur detection problem into a segmentation\nproblem. They propose a sharpness metric based on LBP\n(local binary patterns) and a robust segmentation algorithm\nto separate in- and out-focus image regions. Shi et al. [40]\ndirectly establish correspondence between sparse edge rep-\nresentation and blur strength estimation via the proposed\na robust and effective blur feature to generate the defocus\nblurring map. Lee et al. [25] propose the first end-to-end\nconvolutional neural network (CNN) architecture to esti-\nmate the defocus blurring map and achieve the state-of-the-\nart performance than the traditional methods. After getting\nthe defocus blurring map, they use the deconvolution ap-\nproaches [13,20,27] to recover shaper information from the\ndefocus blurring image.\nThe two-stage defocus deblurring methods heavily rely\non the accuracy of defocus blurring map, which limits the\nperformance of deblurring. To avoid the influence of defo-\ncus blurring map on deblurring performance, Abuolaim et\nal. [1] propose an end-to-end defocus deblurring framework\n(DPDNet). DPDNet is U-Net-like neural network architec-\nture. In order to train DPDNet, they propose a defocus de-\nblurring dataset, named Canon DP data, based on the dual-\npixel camera, which includes left and right defocus blur-\nring images and corresponding sharp image. And DPDNet\nachieves the best performance on Canon DP dataset and\ntakes less time than the traditional methods. However, the\nCanon DP data has the problem that the input images and\nthe ground true are not aligned. Abuolaim et al. [2] further\ngenerate a more realistic synthetic DP dataset to solve the\nmisalignment, in which the input defocus blurring images\nand ground truth are aligned. They train a new recurrent\nconvolution network, named RDPDNet, with the synthetic\nand Canon DP dataset to improve the performance. The\nRDPDNet is modified from DPDNet, in which the number\nof channels are reduced to half at each block to speed up\nthe inference time and convLSTM units are added to learn\ntemporal dependencies present in the image sequence.\n2.2. Vision transformer\nSince the success of AlexNet [21, 24] on ImageNet\nLarge Scale Visual Recognition Challenge (ILSVRC) [9],\nconvolutional neural network has been applied to vari-\nous computer vision tasks. For example, high level vi-\nsion tasks include: classification [15, 41, 44], object detec-\ntion [14, 31, 38, 39] and segmentation [7, 34]. The low level\nvision tasks include: image deblurring [46,51], image super\nsegmentation [10, 28] and image denoising [5]. And thanks\nto the powerful inductive bias of CNN, deep learning has\nachieved exciting results in computer vision tasks.\nVIT [11] is the first work that uses transformer in com-\nputer vision tasks. It surpasses performance of CNN in\nclassification task by virtue of self-attention and large sacle\ndatasets. However, the computational efficiency of self-\nattention suffers from the high resolution of images. To\naddress above problem, Swin Transformer [32] proposes a\nshifted windowing scheme in the self-attention, which im-\nproves the efficiency by limiting self-attention computation\nto non-overlapping local windows. It is worth mentioning\nthat the effectiveness of Swin Transformer has been ver-\nified in a number of visual tasks, including image classi-\nfication, semantic segmentation [29], object detection and\nvideo action recognition [33]. In addition, some works have\nalso verified the effectiveness of transformer in low vision\ntasks. Chen et al . [6] propose a pre-trained transformer\nmodel (IPT) with multi-heads and multi-tails to restore the\nimage with the noise, raining and low-resolution. Google\nbrain team [22] develop a novel approach for diverse high fi-\ndelity image colorization based on conditional transformer,\nnamed Colorization Transformer (ColTran). The above\nworks demonstrate that the transformer can achieve better\nperformance in low-level vision tasks.\n3. METHODOLOGY\n3.1. Network Architecture\nWe propose a dynamic multi-scale network, named\nDMTNet, for dual-pixel images defocus deblurring, as\nshown in Figure 2. DMTNet mainly contains two mod-\nules: feature extraction module and reconstruction mod-\nule. The feature extraction module is composed of sev-\neral vision transformer blocks, while the reconstruction\nmodule is composed of several Dynamic Multi-scale Sub-\nreconstruction Module (DMSSRM) based on CNN. DMT-\nNet combines the advantages of transformer and CNN, in\nwhich the vision transformer improves the performance\nceiling of CNN, and the inductive bias of CNN enables\ntransformer to extract more robust features without relying\non a large amount of data. In this paper, we input the dual-\npixel defocus blurring images ( IR and IL) into DMTNet\nand output one sharp image (IS).\nWe concatenate the left IL ∈ RH×W×3 and right IR ∈\n3\nWindows Multi-Head\nAttention\nLayer Norm\nMulti\nLayer Perceptron\nLayer Norm\nSwin Transformer Block\n(a) Swin Transformer Block (b) Reconstruction module\nOutput image\nPatch partition\nLinear embedding\nSwin\nTransformer \nBlock\n×NT\nLeft\nRight \nInput images Feature extraction module Reconstruction module Up-sampling\nDMSSRM DMSSRM…\n×ND\n+\nAdaptive RGM (scale 1)\nRGM (scale 2)\nRGM (scale 3)\nDynamic Multi-scale Sub-reconstruction Module (DMSSRM)\n1x1 conv\nH/4 × W/4 ×C\nH/4 × W/4 ×C\nH/8 × W/8 ×C\nH/16 ×W/16 ×C\nH/4 × W/4 ×C\nH/4 × W/4 ×CA\nB\nC\nPixelShuffle \nfor down-sampling\nDePixelShuffle\nfor up-sampling\n Element-wise addition\nAdaptive\nThe adaptive multi-scale \nselection module\nRGM \nResidual Group Module\n⁎ \n⁎ \n⁎ \nFigure 2. The network architecture of the proposed DMTNet for dual-pixel images defocus deblurring.\nRH×W×3 views of the dual-pixel data to get the inputFC ∈\nRH×W×(2∗3) of DMTNet. Then, we send the FC into the\npatch partition module to get the token embeddingsFP ,\nFC = CONCAT (IR, IL) (1)\nFP = DPatch (FC) (2)\nwhere (H, W) is the resolution of the input images, 3 andC\nis the number of channels. CONCAT is the operation that\nconcatenate the right and left views of input dual-pixel on\nchannel dimension. DPatch (·) denotes patch partition mod-\nule, which is used to obtain token embeddings as the input\nof transformer. In the patch partition module, we use 2D\nconvolution operation with the fixed kernel and stride size\nto obtain the input featuresF′\nP ∈ RH/P×W/P ×C with spec-\nified patch size P. Then we reshape the features F′\nP into a\nsequence of flattened 2D patches and layer normalize it to\nget F′\nPN ∈ RN×C, and N = W H/P2. Then we reverse\nF′\nPN to get the token embeddings FP ∈ RH/P×W/P ×C.\nWe take the FP as the input of the feature extraction mod-\nule constructed by the transformer blocks to extract more\nrobust features FE ∈ RH/P×W/P ×C,\nFE = DFeatureExtraction (FP ) (3)\nwhere DFeatureExtraction (·) represents feature extraction\nmodule, which is composed of NT vision transformer\nblocks. Inspired by Swin Transformer, self-attention based\non the window scheme is used in each transformer block\nto improve computational efficiency. Then, we send the ro-\nbust features FE into the reconstruction module to restore\nthe blurring image to clarity,\nFS = DReconstruction(FE) (4)\nwhere FS ∈ RH/P×W/P ×C represents sharp features from\nreconstruction module, and DReconstruction(·) denotes\nreconstruction module, which contains ND DMSSRM.\nDMSSRM is a dynamic multi-scale selection network,\nwhich dynamically fuses multi-scale features according to\nthe content information of the input images. Finally, sharp\nfeatures are up-sampled via an up-sampling module to out-\nput the sharp image,\nIS = DUp (FS) (5)\n4\nwhere DUp (·) and IS ∈ RH×W×3 denote up-sampling\nmodule and the final sharp image respectively. We use De-\nPixelShuffle [30,47] to preserve the information and reduce\nthe parameters in the up-sampling module.\nSummarily, IS can also be represented as follows,\nIS = DMT Net(IR, IL) (6)\nwhere DMT Net(·) denotes the function of DMTNet.\n3.2. Feature Extraction Module\nThe feature extraction module is composed of several\nvision transformer blocks, which uses its powerful fea-\nture extraction capability to obtain robust features. In the\nstandard global self-attention, the computational complex-\nity is quadratic with the number of tokens, making it un-\nsuitable for low-level vision tasks with high-resolution. In-\nspired by the Swin Transformer, we replaced the standard\nglobal attention with the non-overlapping windows based\nself-attention for performing efficient modeling in low-\nlevel vision tasks. The computational complexity formulas\nfor standard multi-head self-attention (MSA) and window-\nbased multi-head self-attention (WMSA) are as follows,\nΩ(MSA ) = 4hwC2 + 2(hw)2C (7)\nΩ(W MSA) = 4hwC2 + 2W2hwC (8)\nΩ(MSA )\nΩ(W MSA) ≈ hw\n3W2 (9)\nwhere (h, w) is the resolution of the input features, C is\nthe number of channels and W is the size of windows. In\nthis paper, C ≪ hw and C ≈ W2. According to Eq.(9),\nthe window-based multi-head self-attention significantly re-\nduces the computational complexity of the model.\nAs shown in Figure 2(a), each transformer block is\ncomposed of a window-based multi-head self-attention\n(WMSA) and a multi-layer perceptron (MLP). We perform\nlayer normalization on the input features of WMSA and\nMLP modules, add residual connections to alleviate the gra-\ndient disappearance. The transformer block is formulated\nas,\nX = W MSA(LN(X)) + X (10)\nX = MLP (LN(X)) + X (11)\nwhere W MSA(·) is the WMSA module, LN(·) is the\nLayerNorm (LN) layer and MLP (·) is the MLP module.\nSpecifically, X ∈ RW2×C is the local window feature.\n3.3. Reconstruction Module\nThe reconstruction module is composed of several\nDMSSRM, as shown in Figure 2. Compared with the\nDPDNet or RDPDNet with the fixed weights and archi-\ntecture, DMSSRM restores images by adaptively assign-\ning weights to features from different scales according to\nthe blur distribution and content information of the input\nimages. The reconstruction module is a multi-stage learn-\ning process, which gradually restores the blurring image\nthrough the cascaded DMSSRM module.\nFi = DDMSSRM i (Fi−1), i = 1, 2, ··· , D, (12)\nwhere DDMSSRM i (·) indicates the i-th DMSRGM module\nin the reconstruction module. Fi ∈ RH/P×W/P ×C repre-\nsents the features extracted by thei-th DMSRGM. F0 = FE\nrepresents the features extracted by the feature extraction\nmodule. Then, we add a global residual connections to\nspeed up the gradient feedback in reconstruction module.\nThe output of reconstruction module is formulated as,\nFS = FD + F0 (13)\nwhere FD denotes the reconstructed features of the last\nDMSRGM.\n3.3.1 Dynamic Multi-scale Sub-reconstruction Module\n(DMSSRM)\nBoth DPDNet and RDPDet adopts U-Net-like neural net-\nwork architecture to remove the blur in the input images,\nand improve the deblurring performance by extracting and\nfusing multi-scale features. However, if the neural network\nwith the fixed weights and architecture is used to restore\nthe blurring image with different blurring distribution and\ncontent information will lead to the bad robustness in real\nscene. Therefore, we propose an Dynamic Multi-scale Sub-\nreconstruction Module (DMSSRM), which can adaptively\nassign weights to features from different scales according\nto the blurring distribution and content information of the\ninput images.\nDMSSRM is composed of multi-scale feature extraction\nmodule, adaptive multi-scale selection module and fusion\nmodule, as shown in Figure 2(b). In each scale branch,\nwe use the same configured residual group module (RGM)\nwithout Channel Attention to restore the detailed informa-\ntion in the blurring image [30, 52]. Then, we use the adap-\ntive multi-scale selection module to adaptively fuse multi-\nscale features according to the blurring distribution and con-\ntent information of the input images. Finally, we use the\nfusion module to fuse and exchange information from dif-\nferent scales.\nFi = DDMSSRM i (Fi−1)\n= W(Fscale 1 ∗ α1 + ··· + Fscale n ∗ αn + Fi−1)\n(14)\nwhere DDMSSRM i (·) denotes DMSSRM module. Fi\nand Fi−1 are the output and input of i-th DMSSRM.\nFscale 1, ··· , Fscale n represent the features from different\n5\nGAP 1x1 conv Softmax \nA\nB\nC1 ×1 ×C 1 ×1 ×3\nFigure 3. Adaptive multi-scale selection module.\nscales respectively in DMSSRM, while α1, ··· , αnrepre-\nsent the weights for each scale predicted by adaptive multi-\nscale selection module. W is the weight of convolution lay-\ners with kernel size of 1 ×1 of fusion module and used to\nfuse the multi-scale features.\n3.3.2 Adaptive multi-scale selection module\nAdaptive multi-scale selection module is proposed to pre-\ndict weights for features from each scale based on image\ncontents information and blur distribution. As shown in\nFigure 3, adaptive multi-scale selection module is com-\nposed of global average pooling layer and convolution op-\neration with kernel size of 1×1, and assigns corresponding\nweights to the features from each scale through softmax ac-\ntivation function. Global average pooling layer aggregates\nthe input feature maps into a 1 ×1×C vector, it means that\nwe use features with 1×1×C to represent the blurring distri-\nbution and content information of the input features. Then,\nthe weights of each scale are predicted using 1×1 convolu-\ntion and softmax,\nFC = W(GAP(Fi−1)) (15)\nαj = exp(FCj )\nPS\nj=1 exp(FCj )\n(16)\nwhere GAP(·) denotes global average pooling, W is the\nweight of convolution layers, αj represents the weight co-\nefficient of the j-th scale, S is the number of scales.\n4. EXPERIMENTS\n4.1. Dataset\nWe train and test DMTNet on Canon DP datase without\nusing additional data. Abuolaim et al. [1] collecte 500 pairs\nof blurring and clear images using the dual-pixel camera, in\nwhich each pair contains two blurring images and one clear\nimage. Following DPDNet, we divide the data into 70%\ntraining, 15% validation, and 15% testing sets. The resolu-\ntion of each image is 1680×1120. To speed up the training,\nwe slide a window with a size of 512 ×512 and 60% over-\nlap in the training sets to extract more image patches. Data\naugmentation is performed on training data, which includes\nhorizontal and vertical flips.\n4.2. Implementation Details\nDMTNet is composed of a patch partition module, a fea-\nture extraction module, a reconstruction module and an up-\nsampling module. In the patch partition module, we set the\npatch size to 4 ×4. The feature extraction module is com-\nposed of 5 transformer blocks, and the number of channels\nis the same as patch partition module, which is 96. The re-\nconstruction module consists of 4 DMSSRM. In DMSSRM,\nwe use residual group module (RGM) to extract features\non each scale. The residual module (RM) is composed of\ntwo 3×3 convolution and P ReLUactivation functions, the\nnumber of channels is 64. And we use 10 RM to con-\nstruct residual block module (RBM) and use 5 RBM to form\nresidual group module (RGM). The initial learning rate is\n1e-4, and we use the cosine annealing learning rate sched-\nuler [35] with about 1,000 epochs. We use the Adam [19]\nwith β1 = 0 .9 and β2 = 0 .999 to optimize the Charbon-\nnier loss function [23]. We use PyTorch 1.6, NVIDIA V100\nGPU with CUDA11.0 to accelerate training.\n4.3. Evaluation and Comparison\n4.3.1 Comparisons with State-of-the-art Methods\nThe quantitative results of our DMTNet on dual-pixel defo-\ncus deblurring task achieve SOTA performance compared\nto other models, which is present in Table 1. EBDB [18],\nDMENet [25] and JNB [40] are two-stage defocus deblur-\nring algorithms, which first estimate the defocus map and\nthen use the generated defocus map to guide the deblur-\nring process. EBDB and JNB are traditional defocus blur-\nring detection algorithms, while DMENet is a method based\non deep learning. They are combined with non-blind de-\nblurring algorithms [13, 20] to remove the blurring in the\nimages. DPDNet [1], DPDNet+ and PDPD+ [2] are end-\nto-end algorithms, and they are all trained on dual-pixel\ndatasets. DPDNet is trained on Canon DP training data,\nwhile DPDNet+ and PDPD+ are trained on both Canon and\nsynthetic DP data. Our DMTNet-B is trained only on the\nCanon DP Dataset without using additional synthetic data.\nDMTNet-B indicates that using 2 DMSSRM as the recon-\nstruction module, while DMTNet-H uses 4 DMSSRM. We\ndivide the test data into three scene categories: indoor, out-\ndoor and combined. Our DMTNet significantly improves\ndefocus deblurring performance and achieves best results\nfor all metrics. Specifically, when the number of parame-\nters of our DMTNet is comparable to the SOTA method, our\nDMTNet-B achieves 29.21, 24.18 and 26.63 dB on three\nscenarios respectively, being 4.0%, 6.4% and 4.9% better\nthan the previous method (RDPD+). When using more\nDMSSRM to restore defocus blurring images, our perfor-\nmance is further improved. MRNet [4] won the first place\non the NTIRE 2021 Challenge for Defocus Deblurring Us-\ning Dual-pixel Images (@CVPR 2021). However, our\n6\nMethod Indoor Outdoor Combined Param↓ Flops↓PSNR↑ SSIM↑ MAE↓ PSNR↑ SSIM↑ MAE↓ PSNR↑ SSIM↑ MAE↓\nEBDB [18] 25.77 0.772 0.040 21.25 0.599 0.058 23.45 0.683 0.049 - -\nDMENet [25] 25.70 0.789 0.036 21.51 0.655 0.061 23.55 0.720 0.049 26.71M -\nJNB [40] 26.73 0.828 0.031 21.10 0.608 0.064 23.84 0.715 0.048 - -\nDPDNet [1] 27.48 0.849 0.029 22.90 0.726 0.052 25.13 0.786 0.041 34.52M 1883.74G\nDPDNet+ [1] 27.65 0.852 0.028 22.72 0.719 0.054 25.12 0.784 0.042 34.52M 1883.74G\nRDPD+ [3] 28.10 0.843 0.027 22.82 0.704 0.053 25.39 0.772 0.040 27.51M 612.05G\nIFAN [26] 28.66 0.868 0.025 23.46 0.743 0.049 25.99 0.804 0.037 14.48M 855.31G\nKPAC [43] 27.36 0.849 0.030 22.47 0.707 0.055 24.85 0.776 0.043 1.58M 348.63G\nUformer-B [48] 28.23 0.860 0.026 23.10 0.728 0.051 25.65 0.795 0.039 50.88M 3066.94G\nRestormer [50] 29.48 0.895 0.023 23.97 0.773 0.047 26.66 0.833 0.035 26.13M 4059.08G\nMRNet [4] 29.53 0.896 0.022 24.31 0.789 0.045 26.85 0.841 0.034 48.25M 2597.38G\nDMTNet-T 27.73 0.850 0.027 23.38 0.724 0.051 25.50 0.786 0.039 1.23M 147.37G\nDMTNet-S 28.77 0.880 0.025 23.90 0.765 0.046 26.27 0.821 0.036 12.57M 688.35G\nDMTNet-B 29.21 0.891 0.023 24.18 0.780 0.046 26.63 0.834 0.035 24.46M 1294.41G\nDMTNet-L 29.50 0.892 0.023 24.31 0.783 0.045 26.83 0.836 0.034 36.35M 1900.46G\nDMTNet-H 29.50 0.895 0.022 24.45 0.791 0.045 26.91 0.841 0.034 48.24M 2506.51G\nTable 1. Dual-pixel defocus deblurring comparison on Canon DP dataset with PSNR, SSIM and MAE. DPDNet (ECCV2020), IFAN\n(CVPR2021), KPAC (ICCV2021), Uformer-B (CVPR2022), Restormer (CVPR2022), DMTNet-T, DMTNet-S, DMTNet-B, DMTNet-L\nand DMTNet-H are trained on Canon DP data. DMTNet-S, DMTNet-B, DMTNet-L and DMTNet-H use 1, 2, 3, 4 DMSSRM as the\nreconstruction module respectively, while DMTNet-T without using DMSSRM. DPDNet+ and RDPD+ (ICCV2021) are trained with\nCanon and synthetic DP data. Compared to the other SOTA methods, our DMSSRM more effectively and efficiently removes defocus\nblurring by using the dual-pixel images.\nInput EBDB DMENet JNB DPDNet DPDNet+ RDPD+ DMTNet(our) GT\nFigure 4. Qualitative comparisons with different motion deblurring methods on Canon DP testing dataset. The first column is the input\nblurring image, and the last column is the corresponding ground truth sharp image. The columns in between are the results of different\nmethods. We present zoomed-in cropped patches in yellow and blue boxes. Compared to the state-of-the-art methods, the images restored\nby DMTNet are sharper.\nDMTNet-H achieves better performance and fewer Flops\nthan MRNet. The visual results are presented in Figure 4,\nwhile the images restored by our DMTNet are clearer.\n4.4. Ablation Study\n4.4.1 Impact of DMSSRM number\nIn the reconstruction module, we explore the influence of\nthe number of DMSSRM on deblurring performance. In\nfact, the later DMSSRM module is used to relearn and re-\nfine the previously fused features. As shown in Figure 1\nand Table 1, the PSNR is positively correlated with num-\nbers of DMSSRM, while the total number of parameters\nincreases gradually. When we only use the transformer\nbased feature extraction module (reconstruction module\nis not used) to restore the defocus blurring images, our\nmethod achieves comparable performance to SOTA method\n(DPDNet), while number of parameters are reduced by\n7\n96.44%. When we use one DMSSRM as the reconstruc-\ntion module, we significantly improve the deblurring perfor-\nmance. And combining with the feature extraction module,\nwe improve the PSNR by 0.77dB, proving the effectiveness\nof our DMSSRM. Especially, when the number of parame-\nters of our method is comparable to the SOTA method, our\nDMTNet achieves 26.91 dB on PSNR, being 0.06dB higher\nthan SOTA method and significantly improving the deblur-\nring performance.\n4.4.2 Impact of feature extraction module with trans-\nformer\nOur DMTNet is a Transformer-CNN strategy, which com-\nbines advantage of both the powerful feature extraction ca-\npability of vision transformer and the inductive bias capa-\nbility of CNN. We also perform ablation experiments on\nCanon DP dataset to verify the effectiveness of the strategy.\nThe experimental results are shown in Table 2. In the non-\ntransformer module (or named CNN based module), we use\ntwo convolutional neural networks with kernel size of 3×3\nand stride size of 2 replace the transformer block for ex-\ntracting the features. As shown in Table 2, the transformer\nbased feature extraction module (method 3) is 0.21dB better\nthan the CNN based module (method 1). The experimental\nresults also proves that the transformer can extract robust\nfeatures without relying on a large amount of dataset com-\nbining with CNN.\n4.4.3 Impact of the dynamic multi-scale on DMSSRM\nWe visualize the features and weights of the two DMSSRM,\nas shown in Figure 5. In the first DMSSRM, scale 1 en-\nhances the texture information of features, but overexpo-\nsure occurs. Therefore, it is adjusted by the adaptive mod-\nule to make the texture clearer. Scale 2 is responsible for\npreserving the features of clear areas. Scale 3 contains\na lot of noise information, so it is assigned the minimum\nweight (1.1043e-07≈0.0). In the second DMSSRM, Scale\n1 is mainly responsible for enhancing and maintaining the\ntexture of clear areas. Scale 2 is responsible for restor-\ning the features of smooth and blurred areas. Scale 3 re-\nstores the entire image from a global perspective. Figure 5\nqualitatively demonstrate that DMSSRM can restore images\nby adaptively assigning weights to features from different\nscales according to the content information of the feature\nmaps.\nDynamic multi-scale selection is leitmotif of our\nDMSSRM. We also conduct an ablation study about dy-\nnamic multi-scale to quantitatively demonstrate that dy-\nnamic multi-scale network can achieve better performance\nthan fixed weights and architecture. Compared with the\nfixed weights and architecture, DMSSRM restore images\nby adaptively assigning weights to features from different\nMethod Transformer Dynamic PSNR SSIM\n1 ✓ 26.42 0.830\n2 ✓ 26.47 0.832\n3 ✓ ✓ 26.63 0.834\nTable 2. Comparison on Canon DP dataset with/without trans-\nformer module and adaptive multi-scale selection module (using\n×2 DMSSRM for quick verification).\nFigure 5. Feature visualization of DMSSRM on different scales.\nscales according to the blur distribution and content infor-\nmation of the input images. We remove the adaptive multi-\nscale selection module in DMSSRM as the fixed architec-\nture. As shown in Table 2, DMSSRM with dynamic multi-\nscale selection (method 3) achieves 26.63dB on Canon DP\ndataset, and is 0.16dB better than the fixed architecture\n(method 2).\n5. CONCLUSION\nIn this paper, we propose a Transformer-CNN combined\ndefocus deblurring model DMTNet. And DMTNet might\nbe the first attempt to use vision transformer to restore\nthe blurring images to clarity. DMTNet mainly contains\ntwo modules: feature extraction module and reconstruction\nmodule. The feature extraction module is composed of sev-\neral vision transformer blocks, which is used to extract the\nrobust features. The reconstruction module is composed\nof several Dynamic Multi-scale Sub-reconstruction Module\n(DMSSRM). DMSSRM can restore images by adaptively\nassigning weights to features from different scales accord-\ning to the blur distribution and content information of the in-\nput images. Compared with the fixed architecture, the adap-\ntive dynamic network can significantly improve the perfor-\nmance of deblurring. Our DMTNet combines advantage\nof both the powerful feature extraction capability of vision\ntransformer and the inductive bias capability of CNN. The\ntransformer improves the performance ceiling of CNN, and\nthe inductive bias of CNN enables transformer to extract\nmore robust features without relying on a large amount of\ndata. Experimental results on popular benchmark demon-\nstrate that our DMTNet significantly outperforms existing\nsolutions and achieves state-of-the-art performance withou\nusing additional synthetic data.\n8\nReferences\n[1] Abdullah Abuolaim and Michael S Brown. Defocus deblur-\nring using dual-pixel data. In European Conference on Com-\nputer Vision, pages 111–126. Springer, 2020. 1, 2, 3, 6, 7\n[2] Abdullah Abuolaim, Mauricio Delbracio, Damien Kelly,\nMichael S Brown, and Peyman Milanfar. Learning to re-\nduce defocus blur by realistically modeling dual-pixel data.\narXiv preprint arXiv:2012.03255, 2020. 2, 3, 6\n[3] Abdullah Abuolaim, Mauricio Delbracio, Damien Kelly,\nMichael S Brown, and Peyman Milanfar. Learning to re-\nduce defocus blur by realistically modeling dual-pixel data.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 2289–2298, 2021. 7\n[4] Abdullah Abuolaim, Radu Timofte, and Michael S Brown.\nNtire 2021 challenge for defocus deblurring using dual-\npixel images: Methods and results. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 578–587, 2021. 6, 7\n[5] Tim Brooks, Ben Mildenhall, Tianfan Xue, Jiawen Chen,\nDillon Sharlet, and Jonathan T Barron. Unprocessing images\nfor learned raw denoising. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 11036–11045, 2019. 3\n[6] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. InPro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12299–12310, 2021. 3\n[7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nProceedings of the European conference on computer vision\n(ECCV), pages 801–818, 2018. 3\n[8] St ´ephane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari\nMorcos, Giulio Biroli, and Levent Sagun. Convit: Improving\nvision transformers with soft convolutional inductive biases.\narXiv preprint arXiv:2103.10697, 2021. 2\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009. 3\n[10] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou\nTang. Learning a deep convolutional network for image\nsuper-resolution. In European conference on computer vi-\nsion, pages 184–199. Springer, 2014. 3\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2, 3\n[12] Laurent D’Andr `es, Jordi Salvador, Axel Kochale, and Sabine\nS¨usstrunk. Non-parametric blur map regression for depth of\nfield extension. IEEE Transactions on Image Processing ,\n25(4):1660–1673, 2016. 3\n[13] DA Fish, AM Brinicombe, ER Pike, and JG Walker. Blind\ndeconvolution by means of the richardson–lucy algorithm.\nJOSA A, 12(1):58–65, 1995. 3, 6\n[14] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 1440–1448,\n2015. 3\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 3\n[16] Charles Herrmann, Richard Strong Bowen, Neal Wadhwa,\nRahul Garg, Qiurui He, Jonathan T Barron, and Ramin\nZabih. Learning to autofocus. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2230–2239, 2020. 1\n[17] Jinbeum Jang, Yoonjong Yoo, Jongheon Kim, and Joonki\nPaik. Sensor-based auto-focusing system using multi-scale\nfeature extraction and phase correlation matching. Sensors,\n15(3):5747–5762, 2015. 1\n[18] Ali Karaali and Claudio Rosito Jung. Edge-based defocus\nblur estimation with adaptive scale selection. IEEE Trans-\nactions on Image Processing, 27(3):1126–1137, 2017. 3, 6,\n7\n[19] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\n2014. 6\n[20] Dilip Krishnan and Rob Fergus. Fast image deconvolution\nusing hyper-laplacian priors. Advances in neural information\nprocessing systems, 22:1033–1041, 2009. 3, 6\n[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. Advances in neural information processing systems ,\n25:1097–1105, 2012. 3\n[22] Manoj Kumar, Dirk Weissenborn, and Nal Kalchbrenner.\nColorization transformer. arXiv preprint arXiv:2102.04432,\n2021. 3\n[23] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-\nHsuan Yang. Fast and accurate image super-resolution with\ndeep laplacian pyramid networks. IEEE transactions on pat-\ntern analysis and machine intelligence , 41(11):2599–2613,\n2018. 6\n[24] Yann LeCun, Bernhard Boser, John S Denker, Donnie\nHenderson, Richard E Howard, Wayne Hubbard, and\nLawrence D Jackel. Backpropagation applied to handwrit-\nten zip code recognition.Neural computation, 1(4):541–551,\n1989. 3\n[25] Junyong Lee, Sungkil Lee, Sunghyun Cho, and Seungyong\nLee. Deep defocus map estimation using domain adaptation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 12222–12230, 2019.\n3, 6, 7\n[26] Junyong Lee, Hyeongseok Son, Jaesung Rim, Sunghyun\nCho, and Seungyong Lee. Iterative filter adaptive network\nfor single image defocus deblurring. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2034–2042, 2021. 7\n[27] Anat Levin, Yair Weiss, Fredo Durand, and William T Free-\nman. Understanding blind deconvolution algorithms. IEEE\n9\ntransactions on pattern analysis and machine intelligence ,\n33(12):2354–2367, 2011. 3\n[28] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and\nKyoung Mu Lee. Enhanced deep residual networks for single\nimage super-resolution. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition workshops,\npages 136–144, 2017. 3\n[29] Ailiang Lin, Bingzhi Chen, Jiayu Xu, Zheng Zhang, and\nGuangming Lu. Ds-transunet: Dual swin transformer\nu-net for medical image segmentation. arXiv preprint\narXiv:2106.06716, 2021. 3\n[30] Shuai Liu, Chenghua Li, Nan Nan, Ziyao Zong, and Ruixia\nSong. Mmdm: Multi-frame and multi-scale for image\ndemoir´eing. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops, pages\n434–435, 2020. 5\n[31] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian\nSzegedy, Scott Reed, Cheng-Yang Fu, and Alexander C\nBerg. Ssd: Single shot multibox detector. In European con-\nference on computer vision, pages 21–37. Springer, 2016. 3\n[32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. arXiv preprint arXiv:2103.14030, 2021. 3\n[33] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Han Hu. Video swin transformer. arXiv\npreprint arXiv:2106.13230, 2021. 3\n[34] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 3431–3440, 2015. 3\n[35] Ilya Loshchilov and Frank Hutter. Sgdr: Stochas-\ntic gradient descent with warm restarts. arXiv preprint\narXiv:1608.03983, 2016. 6\n[36] Xiaojiao Mao, Chunhua Shen, and Yu-Bin Yang. Image\nrestoration using very deep convolutional encoder-decoder\nnetworks with symmetric skip connections.Advances in neu-\nral information processing systems, 29:2802–2810, 2016. 3\n[37] Liyuan Pan, Shah Chowdhury, Richard Hartley, Miaomiao\nLiu, Hongguang Zhang, and Hongdong Li. Dual pixel explo-\nration: Simultaneous depth estimation and image restoration.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4340–4349, 2021. 3\n[38] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\nFarhadi. You only look once: Unified, real-time object de-\ntection. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 779–788, 2016. 3\n[39] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. Advances in neural information process-\ning systems, 28:91–99, 2015. 3\n[40] Jianping Shi, Li Xu, and Jiaya Jia. Just noticeable defocus\nblur detection and estimation. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 657–665, 2015. 3, 6, 7\n[41] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 3\n[42] Przemysław ´Sliwi´nski and Paweł Wachel. A simple model\nfor on-sensor phase-detection autofocusing algorithm. Jour-\nnal of Computer and Communications, 1(06):11, 2013. 1\n[43] Hyeongseok Son, Junyong Lee, Sunghyun Cho, and Seungy-\nong Lee. Single image defocus deblurring using kernel-\nsharing parallel atrous convolutions. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 2642–2650, 2021. 7\n[44] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 1–9, 2015.\n3\n[45] Chang Tang, Xinzhong Zhu, Xinwang Liu, Lizhe Wang, and\nAlbert Zomaya. Defusionnet: Defocus blur detection via re-\ncurrently fusing and refining multi-scale deep features. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 2700–2709, 2019. 3\n[46] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Ji-\naya Jia. Scale-recurrent network for deep image deblurring.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 8174–8182, 2018. 3\n[47] Thang Vu, Cao Van Nguyen, Trung X Pham, Tung M Luu,\nand Chang D Yoo. Fast and efficient image quality enhance-\nment via desubpixel convolutional neural networks. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV) Workshops, pages 0–0, 2018. 5\n[48] Zhendong Wang, Xiaodong Cun, Jianmin Bao, and\nJianzhuang Liu. Uformer: A general u-shaped transformer\nfor image restoration. arXiv preprint arXiv:2106.03106 ,\n2021. 7\n[49] Xin Yi and Mark Eramian. Lbp-based segmentation of\ndefocus blur. IEEE transactions on image processing ,\n25(4):1626–1638, 2016. 3\n[50] Syed Waqas Zamir, Aditya Arora, Salman Khan, Mu-\nnawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang.\nRestormer: Efficient transformer for high-resolution image\nrestoration. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 5728–\n5739, 2022. 7\n[51] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar\nHayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling\nShao. Multi-stage progressive image restoration. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 14821–14831, 2021. 3\n[52] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng\nZhong, and Yun Fu. Image super-resolution using very\ndeep residual channel attention networks. In Proceedings of\nthe European conference on computer vision (ECCV), pages\n286–301, 2018. 5\n10",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.792613685131073
    },
    {
      "name": "Deblurring",
      "score": 0.7404662370681763
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7010481357574463
    },
    {
      "name": "Transformer",
      "score": 0.5918322205543518
    },
    {
      "name": "Convolutional neural network",
      "score": 0.581789493560791
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.540507972240448
    },
    {
      "name": "Feature extraction",
      "score": 0.5364869832992554
    },
    {
      "name": "Computer vision",
      "score": 0.5012564659118652
    },
    {
      "name": "Pixel",
      "score": 0.4825703203678131
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4153267443180084
    },
    {
      "name": "Image processing",
      "score": 0.29104167222976685
    },
    {
      "name": "Image restoration",
      "score": 0.23622721433639526
    },
    {
      "name": "Image (mathematics)",
      "score": 0.1096494197845459
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210155230",
      "name": "Samsung (China)",
      "country": "CN"
    }
  ]
}