{
  "title": "Bias of AI-generated content: an examination of news produced by large language models",
  "url": "https://openalex.org/W4392362609",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2104808083",
      "name": "Xiao Fang",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A2801420517",
      "name": "Shangkun Che",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2970389273",
      "name": "Minjia Mao",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A2111912569",
      "name": "Hongzhe Zhang",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A1967110011",
      "name": "Ming Zhao",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A2531678738",
      "name": "Xiaohang Zhao",
      "affiliations": [
        "Shanghai University of Finance and Economics"
      ]
    },
    {
      "id": "https://openalex.org/A2104808083",
      "name": "Xiao Fang",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A2801420517",
      "name": "Shangkun Che",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2970389273",
      "name": "Minjia Mao",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A2111912569",
      "name": "Hongzhe Zhang",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A1967110011",
      "name": "Ming Zhao",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A2531678738",
      "name": "Xiaohang Zhao",
      "affiliations": [
        "Shanghai University of Finance and Economics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2006447892",
    "https://openalex.org/W2789645134",
    "https://openalex.org/W2981869278",
    "https://openalex.org/W3213100366",
    "https://openalex.org/W4372259645",
    "https://openalex.org/W2809770202",
    "https://openalex.org/W3041561133",
    "https://openalex.org/W2971307358",
    "https://openalex.org/W2963381846",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W3101767999",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W3188785232",
    "https://openalex.org/W3137986835",
    "https://openalex.org/W2900571251",
    "https://openalex.org/W2970036856",
    "https://openalex.org/W1990074061",
    "https://openalex.org/W6763240421",
    "https://openalex.org/W2143668817",
    "https://openalex.org/W2171235719",
    "https://openalex.org/W3046292371",
    "https://openalex.org/W2950888501",
    "https://openalex.org/W2910210452",
    "https://openalex.org/W4372335106",
    "https://openalex.org/W4366376877",
    "https://openalex.org/W4206729245",
    "https://openalex.org/W1493790738"
  ],
  "abstract": "Abstract Large language models (LLMs) have the potential to transform our lives and work through the content they generate, known as AI-Generated Content (AIGC). To harness this transformation, we need to understand the limitations of LLMs. Here, we investigate the bias of AIGC produced by seven representative LLMs, including ChatGPT and LLaMA. We collect news articles from The New York Times and Reuters, both known for their dedication to provide unbiased news. We then apply each examined LLM to generate news content with headlines of these news articles as prompts, and evaluate the gender and racial biases of the AIGC produced by the LLM by comparing the AIGC and the original news articles. We further analyze the gender bias of each LLM under biased prompts by adding gender-biased messages to prompts constructed from these news headlines. Our study reveals that the AIGC produced by each examined LLM demonstrates substantial gender and racial biases. Moreover, the AIGC generated by each LLM exhibits notable discrimination against females and individuals of the Black race. Among the LLMs, the AIGC generated by ChatGPT demonstrates the lowest level of bias, and ChatGPT is the sole model capable of declining content generation when provided with biased prompts.",
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports\nBias of AI‑generated content: \nan examination of news produced \nby large language models\nXiao Fang 1*, Shangkun Che 2*, Minjia Mao 1, Hongzhe Zhang 3, Ming Zhao 1 & \nXiaohang Zhao 4\nLarge language models (LLMs) have the potential to transform our lives and work through the \ncontent they generate, known as AI‑Generated Content (AIGC). To harness this transformation, \nwe need to understand the limitations of LLMs. Here, we investigate the bias of AIGC produced by \nseven representative LLMs, including ChatGPT and LLaMA. We collect news articles from The New \nYork Times and Reuters, both known for their dedication to provide unbiased news. We then apply \neach examined LLM to generate news content with headlines of these news articles as prompts, and \nevaluate the gender and racial biases of the AIGC produced by the LLM by comparing the AIGC and \nthe original news articles. We further analyze the gender bias of each LLM under biased prompts \nby adding gender‑biased messages to prompts constructed from these news headlines. Our study \nreveals that the AIGC produced by each examined LLM demonstrates substantial gender and racial \nbiases. Moreover, the AIGC generated by each LLM exhibits notable discrimination against females \nand individuals of the Black race. Among the LLMs, the AIGC generated by ChatGPT demonstrates \nthe lowest level of bias, and ChatGPT is the sole model capable of declining content generation when \nprovided with biased prompts.\nKeywords AI-generated content (AIGC), Large language model (LLM), Generative AI, ChatGPT, Bias, \nGender bias, Racial bias, Prompt\nLarge language models (LLMs), such as ChatGPT and LLaMA, are large-scale AI models trained on massive \namounts of data to understand human  languages1,2. Once pre-trained, LLMs can generate content in response \nto prompts provided by their users. Because of their generative capabilities, LLMs belong to generative AI \nmodels and the content produced by LLMs constitutes a form of AI-Generated Content (AIGC). Compared to \nthe content created by humans, AIGC can be produced far more efficiently at a much lower cost. As a result, \nLLMs have the potential to facilitate and revolutionize various kinds of work in organizations: from generating \nproperty descriptions for real estate agents to creating synthetic patient data for drug  discovery3. To realize the \nfull benefit of LLMs, we need to understand their  limitations3. LLMs are trained on archival data produced by \nhumans. Consequently, AIGC could inherit and even amplify biases presented in the training data. Therefore, \nto harness the potential of LLMs, it is imperative to examine the bias of AIGC produced by them.\nIn general, bias refers to the phenomenon that computer systems “systematically and unfairly discriminate \nagainst certain individuals or groups of individuals in favor of others” 4, p. 332. In the context of LLMs, AIGC is \nconsidered biased if it exhibits systematic and unfair discrimination against certain population groups, particu-\nlarly underrepresented population groups. Among various types of biases, gender and racial biases have garnered \nsubstantial attention across multiple  disciplines 5–9, due to their profound impacts on individuals and wide-\nranging societal implications. Accordingly, we investigate the gender and racial biases of AIGC by examining \ntheir manifestation in AIGC at the word, sentence, and document levels. At the word level, the bias of AIGC is \nmeasured as the degree to which the distribution of words associated with different population groups (e.g., male \nand female) in AIGC deviates from a reference  distribution10,11. Clearly, the choice of the reference distribution \nis critical to the bias evaluation. Previous studies utilize the uniform distribution as the reference  distribution11, \nwhich might not reflect the reality of content production. For example, a news article about the Argentina men’s \nsoccer team winning the 2022 World Cup likely contains more male-specific words than female-specific words, \nthereby deviating from the uniform distribution. The reason for this deviation may be attributed to the fact that \nOPEN\n1University of Delaware, Newark, USA. 2Tsinghua University, Beijing, China. 3Chinese University of Hong Kong, \nShenzhen, China. 4Shanghai University of Finance and Economics, Shanghai, China.  *email: xfang@udel.edu ; \ncsk19@mails.tsinghua.edu.cn\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nthe news is about men’s sports and male players, rather than favoring one population group over another. Ide -\nally, we would derive a reference distribution from unbiased content. However, in reality, completely unbiased \ncontent may not exist. One viable solution is to proxy unbiased content with news articles from news agencies \nthat are highly ranked in terms of their dedication to provide accurate and unbiased news, and obtain a reference \ndistribution from these news articles. To evaluate the word-level bias of AIGC created by an LLM, we apply the \nLLM to generate news content with headlines of these news articles as prompts. We then compare the distribu-\ntion of words associated with different population groups in the generated content to the reference distribution. \nIt is noted that LLMs have been utilized to generate news. For example, a recent work employs headlines from \nthe New Y ork Times as prompts for producing LLM-generated  news12. Additionally, using LLMs to write the \nfirst draft of articles is also  reported13.\nAt the sentence level, the bias of AIGC is evaluated as the difference between sentences related to each inves-\ntigated population group in AIGC and their counterparts in the news articles produced by the selected news \nagencies, in terms of their expressed sentiments and toxicities. In this study, we define a document as a news \narticle that is either produced by a news agency or generated by a LLM. And the document-level bias of AIGC \nis assessed as the difference between documents in AIGC and their counterparts produced by the selected news \nagencies, in terms of their expressed semantics regarding each investigated population group. These three levels \nof evaluations vary in their unit of analysis, spanning from words to sentences and documents, and examine the \nbias of AIGC from different aspects of content generation. Specifically, the word-level analysis uncovers the bias \nof AIGC in word choices, the sentence-level investigation reveals its bias in assembling words to express senti -\nments and toxicities, while the document-level examination discloses its bias in organizing words and sentences \nto convey the meaning and themes of a document. Moreover, a malicious user could intentionally supply biased \nprompts to an LLM and induce it to generate biased content. Therefore, it is necessary to analyze the bias of AIGC \nunder biased prompts. To this end, we add biased messages to prompts constructed from news headlines and \nexamine the gender bias of AIGC produced by an LLM under biased prompts. In addition, a well-performed LLM \ncould refuse to generate content when presented with biased prompts. Accordingly, we also assess the degree to \nwhich an LLM is resistant to biased prompts. Figure 1 depicts the framework for evaluating the bias of AIGC.\nIt is important to note that bias is not a unique challenge limited to AIGC. Within journalism  research14, bias \nis a prevalent issue observed in news articles. An illustrative example is when both a man and a woman contrib-\nute equally to an event, yet prior research reveals a tendency in news reporting to place the man’s contribution \nfirst. This position bias results in greater visibility for the man. Other studies delve into association bias between \ndifferent genders and specific words in language  generation15. Despite numerous efforts proposing debiasing \nmethods, researchers  argue16 that these approaches merely mask bias rather than truly eliminating it from the \nFigure 1.  Framework for Evaluating Bias of AIGC. (a) We proxy unbiased content with the news articles \ncollected from The New Y ork Times and Reuters. Please see the Section of “Data” for the justification of \nchoosing these news agencies. We then apply an LLM to produce AIGC with headlines of these news articles as \nprompts and evaluate the gender and racial biases of AIGC by comparing it with the original news articles at the \nword, sentence, and document levels. (b) Examine the gender bias of AIGC under biased prompts.\n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nembedding. As language models grow in size, they become more susceptible to acquiring biases against mar -\nginalized populations from internet  texts17. Our evaluation differs from existing studies that primarily focus on \nevaluating the bias manifested in short phrases  (e.g., a word or a sentence) generated by language  models18–21. \nFor example, Nadeem et al.19 develop the context association test to evaluate the bias of language models, which \nessentially asks an language model to choose an appropriate word or sentence in a given context (e.g. Men tend \nto be more [soft or determined] than women). Our study, on the other hand, evaluates the bias embodied in \ndocuments (i.e., news articles) produced by an LLM, in terms of its word choices as well as expressed sentiments, \ntoxicities, and semantics in these documents.\nResults\nWord level bias\nGender bias\nMeasured using the average Wasserstein distance defined by Eq. (4), the word level gender biases of the exam -\nined LLMs are: Grover 0.2407 (95% CI [0.2329, 0.2485], N  = 5671), GPT-2 0.3201 (95% CI [0.3110, 0.3291], \nN = 5150), GPT-3-curie 0.1860 (95% CI [0.1765, 0.1954], N  = 3316), GPT-3-davinci 0.1686 (95% CI [0.1604, \n0.1768], N = 3807), ChatGPT 0.1536 (95% CI [0.1455, 0.1617], N = 3594), Cohere 0.1965 (95% CI [0.1890, 0.2041], \nN = 5067), and LLaMA-7B 0.2304 (95% CI [0.2195, 0.2412], N = 2818) (Fig 2a). Overall, the AIGC generated by \neach investigated LLM exhibits substantial word level gender bias. Among them, ChatGPT achieves the lowest \nscore of gender bias. Specifically, its score of 0.1536 indicates that, on average, the absolute difference between \nthe percentage of male (or female) specific words out of all gender related words in a news article generated \nby ChatGPT and that percentage in its counterpart collected from The New Y ork Times or Reuters is 15.36%. \nNotably, among the four GPT models, the word level gender bias decreases as the model size increases from \n774M (GPT-2) to 175B (GPT-3-davinci, ChatGPT). Moreover, although GPT-3-davinci and ChatGPT have the \nsame model size, the outperformance of ChatGPT over GPT-3-davinci highlights the effectiveness of its RLHF \n(reinforcement learning from human feedback) feature in mitigating gender bias.\nFigure 2a presents the overall gender bias of each investigated LLM, which reveals its magnitude of deviation \nfrom the reference human-writing in terms of its utilization of gender related words. It is interesting to zoom \nFigure 2.  Gender Bias at Word Level. (a) Word level gender bias of an LLM and its 95% confidence interval \n(error bar), measured using the average Wasserstein distance defined by Eq. (4). For example, the gender bias \nscore of 0.2407 by Grover indicates that, on average, the absolute difference between the percentage of male (or \nfemale) specific words out of all gender related words in a news article generated by Grover and that percentage \nin its counterpart collected from The New Y ork Times or Reuters is 24.07%. (b) Percentage of female prejudice \nnews articles generated by an LLM. We define a news article generated by an LLM as exhibiting female prejudice \nif the percentage of female specific words in it is lower than that percentage in its counterpart collected from The \nNew Y ork Times or Reuters. (c) Decrease of female specific words in female prejudice news articles generated by \nan LLM and its 95% confidence interval (error bar). For example, the score of -39.64% by Grover, reveals that, \naveraged across all female prejudice news articles generated by Grover, the percentage of female specific words is \nreduced from x% in their counterparts collected from The New Y ork Times and Reuters to (x − 39.64)% in those \ngenerated by Grover.\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nin and uncover the bias of each LLM against the underrepresented group (i.e., females in the focal analysis). To \nthis end, we define a news article generated by an LLM as showing female prejudice if the percentage of female \nspecific words in it is lower than that percentage in its counterpart collected from The New Y ork Times or Reu-\nters. Figure 2b reports the proportion of female prejudice news articles generated by each LLM: Grover 73.89% \n(N = 3037), GPT-2 69.24% (N = 2828), GPT-3-curie 56.04% (N = 1954), GPT-3-davinci 56.12% (N = 2183), Chat-\nGPT 56.63% (N = 2045), Cohere 59.36% (N = 2879), and LLaMA-7B 62.26% (N = 1627). Let us consider Grover’s \nperformance of 73.89% as an example. This figure suggests that, for a news article obtained from the New Y ork \nTimes or Reuters that includes female specific words, there is a probability of 0.7389 that the percentage of female \nspecific words in its corresponding news article generated by Grover is lower than that percentage found in the \noriginal article. Moreover, Figure 2c shows the extent of the decrease in female specific words in female preju -\ndice news articles generated by each investigated LLM. As shown, such measurements for the LLMs are: Grover \n− 39.64% (95% CI [−  40.97%, − 38.30%], N = 2244), GPT-2 − 43.38% (95% CI [−  44.77%, − 41.98%], N = 1958), \nGPT-3-curie − 26.39% (95% CI [−  28.00%, − 24.78%], N = 1095), GPT-3-davinci − 27.36% (95% CI [−  28.88%, \n− 25.84%], N = 1225), ChatGPT −  24.50% (95% CI [−  25.98%, − 23.01%], N = 1158), Cohere −  29.68% (95% CI \n[− 31.02%, − 28.34%], N = 1709), and LLaMA-7B − 32.61% (95% CI [− 34.41%, − 30.81%], N = 1013). Take the \nmeasurement score of − 39.64% by Grover as an example. It shows that, averaged across all female prejudice news \narticles generated by Grover, the percentage of female specific words is decreased from x% in their counterparts \ncollected from The New Y ork Times and Reuters to (x − 39.64)% in those generated by Grover. As analyzed in \nFig. 2b,c, all investigated LLMs exhibit noticeable bias against females at the word level. Among them, ChatGPT \nperforms the best in terms of both the proportion of female prejudice news articles generated and the decrease \nof female specific words in these articles. Moreover, the performance of the GPT models generally\nimproves as the model size increases and the RLHF feature is beneficial for reducing the word level bias \nagainst females. However, it is noticeable that RLHF is a double-edged sword. If malicious users exploit RLHF \nwith biased human feedback, it may increase the word level bias.\nRacial bias\nThe word level racial biases of the investigated LLMs, quantified using the average Wasserstein distance defined \nby Eq. (4 ), are presented in Fig.  3a and listed as follows: Grover 0.3740 (95% CI [0.3638, 0.3841], N  = 5410), \nGPT-2 0.4025 (95% CI [0.3913, 0.4136], N = 4203), GPT-3-curie 0.2655 (95% CI [0.2554, 0.2756], N = 3848), \nGPT-3-davinci 0.2439 (95% CI [0.2344, 0.2534], N = 3854), ChatGPT 0.2331 (95% CI [0.2236, 0.2426], N = 3738), \nCohere 0.2668 (95% CI [0.2578, 0.2758], N = 4793), and LLaMA-7B 0.2913 (95% CI [0.2788, 0.3039], N = 2764). \nOverall, the AIGC generated by each investigated LLM exhibits notable racial bias at the word level. Among \nthem, ChatGPT demonstrates the lowest racial bias, scoring 0.2331. This score indicates that, on average, the \nabsolute difference between the percentage of words related to an investigated race (White, Black, or Asian) out \nof all race related words in a news article generated by ChatGPT and that percentage in its counterpart collected \nfrom The New Y ork Times or Reuters is as high as 23.31%. We observe a similar performance trend as what was \ndiscovered for gender bias. That is, among the four GPT models, both larger model size and the RLHF feature \nare advantageous in mitigating racial bias.\nFigure 3b provides a more detailed analysis for each racial group. Specifically, Fig.  3b and its correspond-\ning data reported in Table 1 show the percentage difference of race related words between the AIGC generated \nby each investigated LLM and the corresponding original news articles collected from New Y ork Times and \nReuters for each racial group. For example, on average, the percentage of words associated with the White race \nis increased from w % in an original news article to (w + 20.07)% in its corresponding news article generated \nby Grover. However, this increase is accompanied by a decrease in the percentage of words associated with the \nBlack race from b % in an original news article to (b  − 11.74)% in its corresponding news article generated by \nGrover as well as a decrease in the percentage of words associated with the Asian race from a % in an original \nnews article to (a − 8.34)% in its corresponding news article generated by Grover. Indeed, the AIGC generated \nby each examined LLM demonstrates significant bias against the Black race at the word level whereas only the \nAIGC generated by Grover and GPT-2 exhibits significant bias against the Asian race at the word level (Table 1).\nTo gain deeper insights into the bias of the LLMs against the Black race, we define a news article generated by \nan LLM as exhibiting Black prejudice if the percentage of Black-race specific words within it is lower than the cor-\nresponding percentage found in its counterpart collected from The New Y ork Times or Reuters. Figure 3c reports \nthe proportion of Black prejudice news articles generated by each examined LLM: Grover 81.30% (N  = 2578), \nGPT-2 71.94% (N = 2042), GPT-3-curie 65.61% (N = 2027), GPT-3-davinci 60.94% (N = 2038), ChatGPT 62.10% \n(N = 2008), Cohere 65.50% (N = 2423), and\nLLaMA-7B 65.16% (N  = 1395). For example, Grover’s performance score indicates that, for a news article \nobtained from the New Y ork Times or Reuters that contains Black-race specific words, there is a probability of \n0.8130 that the percentage of Black-race specific words in its corresponding news article generated by Grover is \nlower than that percentage found in the original article. Figure  3d further reports the extent of the decrease in \nBlack-race specific words in Black prejudice news articles generated by each investigated LLM: Grover − 48.64% \n(95% CI [− 50.10%, − 47.18%], N = 2096), GPT-2 − 45.28% (95% CI [− 46.97%, − 43.59%], N = 1469), GPT-3-curie \n− 35.89% (95% CI [− 37.58%, − 34.21%], N = 1330), GPT-3-davinci − 31.94% (95% CI [− 33.56%, − 30.31%], \nN = 1242), ChatGPT −  30.39% (95% CI [−  31.98%, − 28.79%], N = 1247), Cohere −  33.58% (95% CI [−  35.09%, \n− 32.08%], N = 1587), and LLaMA-7B − 37.18% (95% CI [− 39.24%, − 35.12%], N  = 909). Taking the perfor -\nmance score of Grover as an example, it shows that, averaged across all Black prejudice news articles generated \nby Grover, the percentage of Black-race specific words is reduced from x% in their counterparts collected from \nThe New Y ork Times and Reuters to (x − 48.64)% in those generated by Grover. In sum, all investigated LLMs \nexhibit a significant bias against the Black race at the word level. Among them, ChatGPT consistently emerges \n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nas a top performer in terms of both the proportion of Black prejudice news articles generated and the reduction \nof Black-race specific words in these articles.\nSentence level bias\nGender bias on sentiment\nMeasured using Eq. (5), the sentence level gender biases on sentiment of the examined LLMs are: Grover 0.1483 \n(95% CI [0.1447, 0.1518], N = 5105), GPT-2 0.1701 (95% CI [0.1655, 0.1748], N = 4456), GPT-3-curie 0.1487 \n(95% CI [0.1437, 0.1537], N = 3053), GPT-3-davinci 0.1416 (95% CI [0.1371, 0.1461], N = 3567), ChatGPT 0.1399 \n(95% CI [0.1354, 0.1444], N = 3362), Cohere 0.1396 (95% CI [0.1356, 0.1436], N = 4711), LLaMA-7B 0.1549 (95% \nCI [0.1492, 0.1605], N  = 2527) (Fig. 4a). As reported, the AIGC generated by each investigated LLM exhibits \nsubstantial gender bias on sentiment. For example, the best performing LLM in this aspect, Cohere, attains \n0.1396, which shows that, on average, the maximal absolute difference between the average sentiment score of \nsentences pertaining to a population group (i.e., male or female) in a news article generated by Cohere and that \nscore in its counterpart collected from The New Y ork Times or Reuters is 0.1396. Among the four GPT models, \ngender bias on sentiment decreases as the model size increases. In addition, the outperformance of ChatGPT \nover GPT-3-davinci demonstrates the effectiveness of its RLHF feature in mitigating gender bias on sentiment.\nFigure 4a reveals the magnitude of sentiment difference between each investigated LLM and the benchmark \nhuman-writing across both male and female population groups. Next, we zoom in and examine the bias of each \nLLM against females. In this context, we define a news article generated by an LLM as exhibiting female prejudice \nwith respect to sentiment if the average sentiment score of sentences related to females in that article is lower \nthan the average sentiment score of sentences associated with females in its counterpart obtained from The New \nY ork Times or Reuters. It is important to note that sentiment scores range from − 1 to 1, where − 1 represents \nFigure 3.  Racial Bias at Word Level. (a) Word level racial bias of an LLM and its 95% confidence interval (error \nbar), measured using the average Wasserstein distance defined by Eq. (4). For example, the racial bias score of \n0.3740 by Grover indicates that, on average, the absolute difference between the percentage of words related to \nan investigated race (White, Black, or Asian) out of all race-related words in a news article generated by Grover \nand that percentage in its counterpart collected from The New Y ork Times or Reuters is as high as 37.40%. (b) \nAverage difference between the percentage of White (or Black or Asian)-race specific words in a news article \ngenerated by an LLM and that percentage in its counterpart collected from The New Y ork Times or Reuters. \nError bar reflects 95% confidence interval. (c) Percentage of Black prejudice news articles generated by an LLM. \nWe define a news article generated by an LLM as showing Black prejudice if the percentage of Black-race specific \nwords in it is lower than that percentage in its counterpart collected from The New Y ork Times or Reuters. \n(d) Decrease of Black-race specific words in Black prejudice news articles generated by an LLM and its 95% \nconfidence interval (error bar). For example, the score of − 48.64% by Grover, reveals that, averaged across all \nBlack prejudice news articles generated by Grover, the percentage of Black-race specific words is reduced from \nx% in their counterparts collected from The New Y ork Times and Reuters to (x − 48.64)% in those generated by \nGrover.\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nthe most negative sentiment, 0 indicates neutrality, and 1 reflects the most positive sentiment. Therefore, a \nlower sentiment score indicates a more negative sentiment towards females. Figure 4 b presents the proportion \nof female prejudice news articles with respect to sentiment generated by each LLM: Grover (42.55%, N = 1081), \nGPT-2 (51.33%, N = 1087), GPT-3-curie (40.30%, N = 871), GPT-3-davinci (41.80%, N = 976), ChatGPT (39.50%, \nN = 914), Cohere (45.85%, N = 1457), LLaMA-7B (48.85%, N = 741). Take Grover’s performance of 42.55% as an \nexample. This figure suggests that, for a news article obtained from the New Y ork Times or Reuters that includes \nsentences related to females, there is a probability of 0.4255 that its corresponding news article generated by \nGrover exhibits more negative sentiment towards females than the original article. Furthermore, Fig.  4c shows \nthe degree of sentiment score reduction in the female prejudice news articles generated by each LLM: Grover \n− 0.1441 (95% CI [− 0.1568, − 0.1315], N = 460), GPT-2 − 0.1799 (95% CI [− 0.1938 − 0.1661], N = 558), GPT-\n3-curie − 0.1243 (95% CI [− 0.1371, − 0.1114], N = 351), GPT-3-3-davinci − 0.1350 (95% CI [− 0.1495, − 0.1205], \nN = 408), ChatGPT − 0.1321 (95% CI [− 0.1460, − 0.1182], N = 361), Cohere − 0.1458 (95% CI [− 0.1579, − 0.1337], \nN = 668), LLaMA-7B − 0.1488 (95% CI [− 0.1647, − 0.1329], N = 362). Taking Grover’s measurement score of \n− 0.1441 as an example, it reveals that, on average, the average sentiment score of sentences related to females in \na female prejudice news article generated by Grover is reduced by 0.1441, compared to its counterpart collected \nfrom The New Y ork Times or Reuters. For the articles collected from The New Y ork Times and Reuters, 80% of \ntheir sentiment scores towards females range from − 0.05 to 0.26. Taking this range into account, female prejudice \nnews articles generated by each investigated LLM exhibit considerably more negative sentiment towards females \nthan their counterparts collected from The New Y ork Times and Reuters.\nRacial bias on sentiment\nThe sentence level racial biases on sentiment of the investigated LLMs, quantified using Eq.  5, are presented in \nFig. 5a and listed as follows: Grover 0.1480 (95% CI [0.1443, 0.1517], N = 4588), GPT-2 0.1888 (95% CI [0.1807, \n0.1969], N = 1673), GPT-3-curie 0.1494 (95% CI [0.1446, 0.1542], N  = 3608), GPT-3-davinci 0.1842 (95% CI \n[0.1762, 0.1922], N = 1349), ChatGPT 0.1493 (95% CI [0.1445, 0.1541], N = 3581), Cohere 0.1348 (95% CI [0.1310, \n0.1386], N = 4494), LLaMA-7B 0.1505 (95% CI [0.1448, 0.1562], N  = 2545). In general, the AIGC generated by \neach investigated LLM exhibits a degree of racial bias on sentiment at the sentence level. Among them, Cohere \nhas the lowest racial bias on sentiment. It attains 0.1348 in this aspect, which shows that, on average, the maxi -\nmal absolute difference between the average sentiment score of sentences pertaining to a population group (i.e., \nWhite, Black, or Asian) in a news article generated by Cohere and that score in its counterpart collected from \nThe New Y ork Times or Reuters is 0.1348.\nAfter examining the magnitude of sentiment difference between each investigated LLM and the benchmark \nhuman-writing across all three racial groups, we focus on uncovering the bias of each LLM against the Black \nTable 1.  Percentage difference of race related words between AIGC generated by LLM and original news \narticles.\nLLM Mean (%) 95% CI N p\nWhite\n Grover 20.07 [18.79%, 21.35%] 5410 < 0.001\n GPT-2 3.62 [2.08%, 5.16%] 4203 < 0.001\n GPT-3-curie 4.67 [3.44%, 5.91%] 3848 < 0.001\n GPT-3-davinci 2.47 [1.31%, 3.63%] 3854 < 0.001\n ChatGPT 2.35 [1.21%, 3.49%] 3738 < 0.001\n Cohere 2.60 [1.51%, 3.70%] 4793 < 0.001\n LLaMA-7B 2.65 [1.1%, 4.20%] 2764 < 0.001\nBlack\n Grover −  11.74 [− 12.83%, − 10.64%] 5410 < 0.001\n GPT-2 −  1.97 [− 3.34%, − 0.60%] 4203 < 0.01\n GPT-3-curie −  4.13 [− 5.25%, − 3.00%] 3848 < 0.001\n GPT-3-davinci −  2.27 [− 3.30%, − 1.24%] 3854 < 0.001\n ChatGPT −  1.93 [− 2.98%, − 0.88%] 3738 < 0.001\n Cohere −  2.68 [− 3.66%, − 1.71%] 4793 < 0.001\n LLaMA-7B −  2.80 [− 4.19%, − 1.41%] 2764 < 0.001\nAsian\n Grover −  8.34 [− 9.14%, − 7.53%] 5410 < 0.001\n GPT-2 −  1.65 [− 2.63%, − 0.67%] 4203 < 0.001\n GPT-3-curie −  0.55 [− 1.25%, 0.15%] 3848 0.124\n GPT-3-davinci −  0.20 [− 0.89%, 0.49%] 3854 0.573\n ChatGPT −  0.42 [− 1.05%, 0.21%] 3738 0.190\n Cohere 0.08 [− 0.62%, 0.77%] 4793 0.824\n LLaMA-7B 0.15 [− 0.82%, 1.12%] 2764 0.76\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nrace. To this end, we define a news article generated by an LLM as exhibiting Black prejudice with respect to \nsentiment if the average sentiment score of sentences related to the Black race in that article is lower than the \naverage sentiment score of sentences associated with the Black race in its counterpart obtained from The New \nY ork Times or Reuters. Here, a lower sentiment score indicates a more negative sentiment towards the Black \nrace. Figure 5b reports the proportion of Black prejudice news articles with respect to sentiment generated by \neach LLM: Grover (46.14%, N = 869), GPT-2 (46.93%, N = 962), GPT-3-curie (44.36%, N = 978), GPT-3-davinci \n(43.02%, N = 1119), ChatGPT (39.22%, N = 1110), Cohere (48.85%, N = 1435), LLaMA-7B (45.55%, N = 732). For \nexample, Grover’s performance of 46.14% indicates that, for a news article obtained from the New Y ork Times \nor Reuters that contains sentences associated with the Black race, there is a probability of 0.4614 that its cor -\nresponding news article generated by Grover exhibits more negative sentiment towards the Black race than the \noriginal article. Figure 5c further reports the decrease of sentiment score in Black prejudice news articles gener-\nated by each LLM: Grover − 0.1443 (95% CI [− 0.1584, − 0.1302], N = 401), GPT-2 − 0.1570 (95% CI [− 0.1702, \n− 0.1437], N = 451), GPT-3-curie − 0.1351 (95% CI [− 0.1476, − 0.1226], N = 433), GPT-3-davinci − 0.1249 (95% \nCI [− 0.1356, − 0.1143], N = 481), ChatGPT − 0.1236 (95% CI [− 0.1353, − 0.1119], N = 435), Cohere − 0.1277 (95% \nCI [− 0.1374, − 0.1180], N = 700), LLaMA-7B − 0.1400 (95% CI [− 0.1548, − 0.1253], N = 333). As reported, Black \nprejudice news articles generated by each investigated LLM demonstrate considerably more negative sentiment \ntowards the Black race than their counterparts collected from The New Y ork Times and Reuters. Taking Grover \nas an example, on average, the average sentiment score of sentences related to the Black race in a Black prejudice \nnews article generated by Grover is decreased by 0.1443, compared to its counterpart collected from The New \nY ork Times or Reuters. Similar to our findings regarding gender bias on sentiment, among the examined LLMs, \nChatGPT produces the lowest proportion of Black prejudice news articles and demonstrates the least decrease \nin sentiment scores towards the Black race in such articles.\nCohereChatGPTGPT-3-\ndavinci\nGPT-3-\ncurie\nGPT-2Grover\nGenderB ias on Sentiment\n0.18\n0.16\n0.14\n0.12\n0.10\nLLaMA-7B\nPercentage of Female Prejudice News Articles( Sentiment) 60%\n50%\n40%\n30%\n20%\n10%\n0%\nCohereChatGPTGPT-3-\ndavinci\nGPT-3-\ncurie\nGPT-2Grover\nDecrease of Sentiment Scorei nF emale Prejudice News Articles\n-0.2\n-0.15\n-0.1\n-0.05\nCohereChatGPT\nGPT-3-\ndavinci\nGPT-3-\ncurieGPT-2Grover\na\nb c\nLLaMA-7B\nLLaMA-7B\nFigure 4.  Gender Bias on Sentiment at Sentence Level. (a) An LLM’s gender bias on sentiment and its 95% \nconfidence interval (error bar), measured using Eq. (5). For example, Grover attains 0.1483, which indicates \nthat, on average, the maximal absolute difference between the average sentiment score of sentences pertaining to \na population group (i.e., male or female) in a news article generated by Cohere and that score in its counterpart \ncollected from The New Y ork Times or Reuters is 0.1483. (b) Percentage of female prejudice news articles with \nrespect to sentiment generated by an LLM. We define a news article generated by an LLM as exhibiting female \nprejudice with respect to sentiment if the average sentiment score of sentences related to females in the article \nis lower than the average sentiment score of sentences associated with females in its counterpart obtained from \nThe New Y ork Times or Reuters. (c) Sentiment score reduction in female prejudice news articles generated by \nan LLM and its 95% confidence interval (error bar). For example, the measurement score of -0.1441 by Grover, \nmeans that, on average, the average sentiment score of sentences related to females in a female prejudice news \narticle generated by Grover is reduced by 0.1441, compared to its counterpart collected from The New Y ork \nTimes and Reuters.\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nWe obtain qualitatively similar experimental results pertaining to sentence level gender and racial biases on \ntoxicity of the examined LLMs, and report them in Appendix A.\nDocument level bias\nGender bias\nMeasured using Eq. (9 ), the document level gender biases of the examined LLMs are: Grover 0.2377 (95% CI \n[0.2297, 0.2457], N = 5641), GPT-2 0.2909 (95% CI [0.2813, 0.3004], N = 5057), GPT-3-curie 0.2697 (95% CI \n[0.2597, 0.2796], N = 4158), GPT-3-davinci 0.2368 (95% CI [0.2281, 0.2455], N  = 4871), ChatGPT 0.2452 (95% \nCI [0.2370, 0.2535], N = 5262), Cohere 0.2385 (95% CI [0.2305, 0.2465], N = 5549), and LLaMA-7B 0.2516 (95% \nCI [0.2406, 0.2626], N  = 3436) (Fig.  6a). In general, the AIGC generated by each investigated LLM exhibits \nsubstantial document level gender bias. The LLM attaining the lowest gender bias level is GPT-3-davinci, which \nachieves a score of 0.2368. This score indicates that, on average, the absolute difference between the percentage of \nmale (or female) pertinent topics out of all gender pertinent topics in a news article generated by GPT-3-davinci \nand the corresponding percentage in its counterpart collected from The New Y ork Times or Reuters is 23.68%. \nAmong the three GPT models from GPT-2 to GPT-3-davinci, a significant decrease of document level gender \nbias can be observed as the model size increases.\nFollowing the practice in previous sections, we next zoom in and reveal the gender bias of each LLM against \nthe underrepresented group. To this end, we define a news article generated by an LLM as being female preju -\ndice at the document level if the percentage of female pertinent topics in it is lower than that percentage in \nits counterpart collected from The New Y ork Times or Reuters. Figure  6b reports the proportion of docu-\nment level female prejudice news articles generated by each LLM : Grover (37.60%, N  = 5641), GPT-2 (41.33%, \nCohereChatGPTGPT-3-\ndavinci\nGPT-3-\ncurie\nGPT-2Grover\nRacial Bias on Sentiment\n0.20\n0.18\n0.16\n0.14\n0.12\n0.10\nLLaMA-7B\nDecrease ofS entiment Scorei n\nBlackP rejudice News Articles\n0.00\n-0.05\n-0.10\n-0.15\n-0.20\na\nb c\n50%\n40%\n30%\n20%\n10%\n0%\nPercentage of B lack Prejudice News Articles (Sentiment)\nCohereChatGPTGPT-3-\ndavinci\nGPT-3-\ncurie\nGPT-2Grover\nCohereChatGPTGPT-3-\ndavinci\nGPT-3\n-curie\nGPT-2Grover\nLLaMA-7B\nLLaMA-7B\nFigure 5.  Racial Bias on Sentiment at Sentence Level. (a) An LLM’s racial bias on sentiment and its 95% \nconfidence interval (error bar), measured using Eq. (5). The racial bias score of 0.1480 by Grover indicates \nthat, on average, the maximal absolute difference between the average sentiment score of sentences pertaining \nto a population group (i.e., White, Black, or Asian) in a news article generated by Grover and that score in its \ncounterpart collected from The New Y ork Times or Reuters is 0.1480. (b) Percentage of Black prejudice news \narticles with respect to sentiment generated by an LLM. We define a news article generated by an LLM as \nexhibiting Black prejudice with respect to sentiment if the average sentiment score of sentences related to the \nBlack race in that article is lower than the average sentiment score of sentences associated with the Black race \nin its counterpart obtained from The New Y ork Times or Reuters. (c) Decrease of sentiment score in Black \nprejudice news articles generated by an LLM and its 95% confidence interval (error bar). Taking Grover as an \nexample, on average, the average sentiment score of sentences related to the Black race in a Black prejudice news \narticle generated by Grover is decreased by 0.1443, compared to its counterpart collected from The New Y ork \nTimes or Reuters.\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nN = 5057), GPT-3-curie (29.97%, N = 4158), GPT-3-davinci (28.74%, N = 4871), ChatGPT (25.86%, N = 5262), \nCohere (29.92%, N  = 5549), and LLaMA-7B (35.91%, N  = 3436). To interpret the numbers, consider Grover’s \nperformance of 37.60% as an example. This figure suggests that, for a news article obtained from The New Y ork \nTimes or Reuters, there is a probability of 0. 3760 that the percentage of female pertinent topics in it is higher \nthan the percentage found in the corresponding news article generated by Grover. In addition, Fig. 6c presents \nthe reduction degree in female pertinent topics in female prejudice news articles generated by each investi-\ngated LLM. As shown, such measurements for the LLMs are: Grover − 38.11% (95% CI [− 39.55%, − 36.67%], \nN = 2121), GPT-2 − 43.80% (95% CI [− 45.32%, − 42.29%], N = 2090), GPT-3-curie -30.13% (95% CI [− 31.84%, \n− 28.42%], N = 1246), GPT-3-davinci − 29.56% (95% CI [− 31.24%, − 27.88%], N = 1400), ChatGPT − 26.67% (95% \nCI [− 28.18%, − 25.17%], N = 1361), Cohere − 31.89% (95% CI [− 33.44%, − 30.35%], N = 1660), and LLaMA-7B \n-31.40% (95% CI [−  33.19%, −  29.62%], N  = 1234). Take the measurement score of −  38.11% by Grover as an \nexample. It shows that, averaged across all female prejudice news articles generated by Grover, the percentage \nof female pertinent topics is decreased from x % in their counterparts collected from The New Y ork Times and \nReuters to (x  − 38.11)% in those generated by Grover. Figure  6b,c indicate that, all investigated LLMs exhibit \nnotable bias against females at the document level. Among them, ChatGPT performs the best in terms of both the \nproportion of female prejudice news articles generated and the decrease of female prejudice news articles gener-\nated and the decrease of female pertinent topics in these articles, a phenomenon consistent with what we have \nobserved for gender bias at the word level. Moreover, the performance of the GPT models generally improves as \nthe model size increases and the RLHF feature is beneficial for reducing the document level bias against females.\nRacial bias\nThe document level racial biases of the investigated LLMs, quantified using Eq. (9 ), are presented in Figure  7a \nand listed as follows: Grover 0.4853 (95% CI [0.4759, 0.4946], N = 5527), GPT-2 0.5448 (95% CI [0.5360, 0.5536], \nN = 6269), GPT-3-curie 0.3126 (95% CI [0.3033, 0.3219], N  = 4905), GPT-3-davinci 0.3042 (95% CI [0.2957, \n0.3128], N = 5, 590), ChatGPT 0.3163 (95% CI [0.3077, 0.3248], N  = 5567), Cohere 0.2815 (95% CI [0.2739, \n0.2892], N = 6539), and LLaMA-7B 0.3982 (95% CI [0.3857, 0.4108], N  = 3194). Overall, the AIGC generated \nLLaMA-7BCohereChatGPTGPT-3-\ndavinci\nGPT-3-\ncurie\nGPT-2Grover\n0.35\n0.30\n0.25\n0.20\n0.15\nWassersteinD istance\nLLaMA-7BCohereChatGPTGPT-3-\ndavinci\nGPT-3-\ncurie\nGPT-2Grover\n50%\n40%\n30%\n20%\n10%Percentage of Female Prejudice News Articles\nLLaMA-7BCohereChatGPTGPT-3-\ndavinci\nGPT-3-\ncurie\nGPT-2Grover\nDecrease of Female Specific Topicsi n\nFemale Prejudice News Articles\n-20%\n-30%\n-40%\n-60%\na\nb c\nFigure 6.  Gender Bias at Document Level. (a) Document level gender bias of an LLM and its 95% confidence \ninterval (error bar), measured using Eq. (9). For example, the gender bias score of 0.2377 by Grover indicates \nthat, on average, the absolute difference between the percentage of male (or female) pertinent topic out of all \ngender pertinent topics in a news article generated by Grover and that percentage in its counterpart collected \nfrom The New Y ork Times or Reuters is 23.77%. (b) Percentage of document level female prejudice news articles \ngenerated by an LLM. We define a news article generated by an LLM as exhibiting female prejudice at the \ndocument level if the percentage of female pertinent topics in it is lower than that percentage in its counterpart \ncollected from The New Y ork Times or Reuters. (c) Decrease of female pertinent topics in document level female \nprejudice news articles generated by an LLM and its 95% confidence interval (error bar). For example, the score \nof − 37.60% by Grover, reveals that, averaged across all document level female prejudice news articles generated \nby Grover, the percentage of female pertinent topics is reduced from x% in their counterparts collected from The \nNew Y ork Times and Reuters to (x − 37.60)% in those generated by Grover.\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nby each investigated LLM exhibits significant racial bias at the document level. Among them, Cohere has the \nlowest racial bias score 0.2815, which is interpreted as follows: on average, the absolute difference between the \npercentage of topics pertaining to an investigated race (White, Black, or Asian) out of all race pertinent topics in \na news article generated by Cohere and that percentage in its counterpart collected from The New Y ork Times \nor Reuters could be as high as 28.15%. Among the four GPT models, we observe a similar performance trend as \nwhat was discovered for gender bias. That is, larger model size is advantageous in mitigating racial bias.\nTo gain deeper insights into the bias of the LLMs against the Black race, we define a news article generated \nby an LLM as exhibiting Black prejudice at the document level if the percentage of Black-race pertinent topics \nwithin it is lower than the corresponding percentage found in its counterpart collected from The New Y ork \nTimes or Reuters. Figure  7b reports the proportion of document level Black prejudice news articles generated \nby each examined LLM: Grover (26.67%, N = 5527), GPT-2 (32.70%, N = 6269), GPT-3-curie (36.70%, N = 4905), \nGPT-3-davinci (27.50%, N = 5590), ChatGPT (24.23%, N = 5567), Cohere (27.66%, N = 6539), and LLaMA-7B \n(41.86%, N = 3194). For example, Grover’s performance score indicates that, for a news article obtained from \nthe New Y ork Times or Reuters, there is a probability of 0.2667 that the percentage of Black-race pertinent top-\nics in its corresponding news article generated by Grover is lower than that percentage found in the original \narticle. Figure  7c further reports the reduction degree of Black-race pertinent topics in document level Black \nprejudice news articles generated by each investigated LLM: Grover − 35.69% (95% CI [− 37.26%, − 34.13%], \nN = 1474), GPT-2 − 40.97% (95% CI [− 42.32%, − 39.63%], N = 2050), GPT-3-curie − 31.01% (95% CI [− 32.34%, \n− 29.69%], N = 1800), GPT-3-davinci − 25.99% (95% CI [− 27.30%, − 24.69%], N = 1537), ChatGPT − 21.48% (95% \nCI [− 22.64%, − 20.33%], N = 1349), Cohere − 22.01% (95% CI [− 23.09%, − 20.93%], N = 1809), and LLaMA-7B \n− 32.91% (95% CI [− 34.53%, − 31.29%], N = 1337). Taking the performance score of Grover as an example, it \nshows that, averaged across all document level Black prejudice news articles generated by Grover, the percentage \nof Black-race pertinent topics is reduced from x% in their counterparts collected from The New Y ork Times and \nReuters to (x − 35.69)% in those generated by Grover. In summary, all investigated LLMs exhibit a significant bias \nagainst the Black race at the document level. Among them, ChatGPT consistently emerges as the best performer \nFigure 7.  Racial Bias at Document Level. (a) Document level racial bias of an LLM and its 95% confidence \ninterval (error bar), measured using Eq. (9). For example, the racial bias score of 0.4853 by Grover indicates that, \non average, the absolute difference between the percentage of topics pertaining to an investigated race (White, \nBlack, or Asian) out of all race pertinent topics in a news article generated by Grover and that percentage in its \ncounterpart collected from The New Y ork Times or Reuters is as high as 48.53%. (b) Percentage of document \nlevel Black prejudice news articles generated by an LLM. We define a news article generated by an LLM as \nshowing Black prejudice at the document level if the percentage of Black-race pertinent topics in it is lower than \nthat percentage in its counterpart collected from The New Y ork Times or Reuters. (c) Decrease of Black-race \npertinent topics in document level Black prejudice news articles generated by an LLM and its 95% confidence \ninterval (error bar). For example, the score of − 35.69% by Grover, reveals that, averaged across all document \nlevel Black prejudice news articles generated by Grover, the percentage of Black-race pertinent topics is reduced \nfrom x% in their counterparts collected from The New Y ork Times and Reuters to (x − 35.69)% in those \ngenerated by Grover.\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nin terms of both the proportion of document level Black prejudice news articles generated and the reduction of \nBlack-race pertinent topics in these articles.\nBias of AIGC under biased prompts\nMalicious users could intentionally supply biased prompts to LLMs and induce them to generate contents that \nalign with their biased ideologies. Such deliberate and harmful manipulation can yield adverse impacts on \nsociety, which include the propagation of misinformation, reinforcement of biases, and further widening of \nsocietal divides. Therefore, it is crucial to examine the bias of AIGC when presented with biased prompts. With \nthis objective, we simulate the actions of a malicious user who subscribes to Androcentrism and injects biased \nmessages into prompts provided to an LLM. For example, given the news headline of “ Argentina Wins the 2022 \nWorld Cup, Defeating France” , the corresponding gender-biased prompt is:\nSuppose you are a journalist who believe in Androcentrism, please write a news article using the title “ Argen-\ntina Wins the 2022 World Cup, Defeating France”.\nOut of all the analyzed LLMs, only ChatGPT demonstrates the capability to refuse content generation when \nfaced with gender-biased prompts. As an illustration, when presented with the aforementioned biased prompt, \nChatGPT responds as follows: “ As an AI language model, I do not promote or believe in any form of discrimina-\ntion, bias or prejudice. Androcentrism, which is the belief in male superiority, is not something that I support. ” \nSpecifically, when confronted with gender-biased prompts, ChatGPT declines content generation in 89.13% of \ninstances.\nWe compare the gender bias of AIGC produced by an LLM under biased prompts with the gender bias of \nAIGC generated by the same LLM under unbiased prompts at the word, sentence, and document levels. Because \nChatGPT dismisses 89.13% of requests with biased prompts, the AIGC produced by ChatGPT under biased \nprompts comprises of news articles generated in reaction to the remaining 10.87% of biased requests. Figure  8 \nreports the comparison at the word level. In particular, Fig. 8a presents a comparison between the proportion of \nfemale prejudice news articles generated by each investigated LLM under unbiased prompts and the proportion \nof female prejudice news articles produced by the LLM using biased prompts. Here, a news article is considered as \ndemonstrating female prejudice if the percentage of female specific words in it is lower than that percentage in its \ncounterpart collected from The New Y ork Times or Reuters. When provided with biased prompts, the proportions \nof female prejudice news articles generated by the examined LLMs are listed as follows (yellow bars in Fig.  8a): \nGrover 71.30% (N  = 3105, ∆ = − 2.59%), GPT-2 70.48% (N  = 2846, ∆ = 1.24%), GPT-3-curie 58.08% (N  = 1775, \n∆ = 2.04%), GPT-3-davinci 57.40% (N = 2249, ∆ = 1.28%), ChatGPT 63.59% (N = 412, ∆ = 6.96%), Cohere 57.16% \n(N = 2841, ∆ = − 2.20%), LLaMA-7B 54.82% (N = 2446, ∆ = − 7.44%). Note that evaluation results corresponding \nto blue bars in Fig. 8 are reported in the Subsection of “Word Level Bias” . As shown in Fig. 8a, when presented \nwith biased prompts, ChatGPT demonstrates the most notable increase in the proportion of female prejudice \nnews articles. Specifically, the percentage of female prejudice news articles generated by ChatGPT under biased \nprompts is 63.59%, indicating a rise of 6.96% compared to the percentage of female prejudice news articles \ngenerated by ChatGPT using unbiased prompts. For early LLMs, such as Grover and GPT-2, there is a minimal \ndisparity in results between unbiased and biased prompts. This lack of distinction can be attributed to the limited \nunderstanding of biased language by these early models. However, in the case of ChatGPT, a notable susceptibil-\nity to exploitation by malicious users emerges, highlighting a need for future research to address this concern.\nFigure 8b compares the degree to which female specific words decrease in female prejudice news articles \ngenerated by each examined LLM, when supplied with unbiased prompts versus biased prompts. The reported \ndecreases for the LLMs under biased prompts are as follows (yellow bars in Fig.  8b): Grover − 39.15% (95%CI \n[− 40.44%, − 37.86%], N = 2214, ∆ = 0.49%, p = 0.605), GPT-2 − 42.80% (95%CI [− 44.16%, − 41.44%], N = 2006, \n∆ = 0.58%, p = 0.559), GPT-3-curie − 25.72% (95%CI [− 27.31%, − 24.13%], N  = 1031, ∆ = 0.67%, p = 0.562), \nFigure 8.  Gender bias comparison at word level: unbiased prompt versus biased prompt (a) Comparison \nbetween the proportion of female prejudice news articles generated by an LLM under unbiased prompts and \nthe proportion of female prejudice news articles produced by the LLM using biased prompts. (b) Comparison \nbetween the decrease of female specific words in female prejudice news articles generated by an LLM under \nunbiased prompts and the decrease of female specific words in female prejudice news articles generated by the \nLLM under biased prompts. Error bar indicates 95% confidence interval.\n12\nVol:.(1234567890)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nGPT-3-davinci −  25.72%(95%CI [−  27.17%, −  24.27%], N = 1291, ∆  = 1.64%, p = 0.125), ChatGPT −  32.85% \n(95%CI [− 36.12%, − 29.59%], N = 262, ∆ = − 8.35%, p < 0.001), Cohere − 29.63% (95%CI [− 30.97%, − 28.28%], \nN = 1624, ∆ = 0.05%, p = 0.959), LLaMA-7B − 34.51% (95%CI [− 35.97%, − 33.04%], N  = 1341, ∆ = − 1.90%, \np = 0.105). Take ChatGPT as an example. Its score of − 32.85% shows that, averaged across all female prejudice \nnews articles generated by ChatGPT under biased prompts, the percentage of female specific words is decreased \nfrom x% in their counterparts collected from The New Y ork Times and Reuters to (x − 32.85)% in those gener-\nated by ChatGPT under biased prompts. Recall that such score attained by ChatGPT under unbiased prompts is \n− 24.50%, indicating a reduction of 8.35% by ChatGPT using biased prompts. Notably, among all the examined \nLLMs, only ChatGPT under biased prompts generates significantly less female specific words than ChatGPT \nwith unbiased prompts (p < 0.001). Overall, our experimental results suggest the follow findings. First, among \nthe examined LLMs, only ChatGPT demonstrates the ability to decline content generation when presented with \nbiased prompts. Second, while ChatGPT declines a substantial portion of content generation requests involving \nbiased prompts, it produces significantly more biased content than other studied LLMs in response to biased \nprompts that successfully navigate its screening process. That is, compared to other LLMs, once a biased prompt \npasses through the screening process of ChatGPT, it produces a news article aligned more closely with the biased \nprompt, thereby yielding more biased content.\nFigure 9 presents the comparison at the sentence level. In particular, Fig. 9a compares the percentage of female \nprejudice news articles with respect to sentiment generated by each LLM in response to unbiased prompts and \nthe percentage of female prejudice news articles with respect to sentiment generated by the LLM under biased \nprompts. In this context, a news article generated by an LLM is defined as showing female prejudice with respect \nto sentiment if the average sentiment score of sentences related to females in that article is lower than the aver-\nage sentiment score of sentences associated with females in its counterpart obtained from The New Y ork Times \nor Reuters. As illustrated by yellow bars in Fig.  9a, when presented with biased prompts, the percentages of \nfemale prejudice news articles with respect to sentiment generated by the LLMs are: Grover 45.60% ( N = 1274, \n∆ = 3.05%), GPT-2 53.90% (N = 1039, ∆ = 2.57%), GPT-3-curie 42.38% (N = 741∆ = 2.08%, GPT-3-davinci 41.17% \n(N = 1042, ∆ = − 0.63%), ChatGPT 47.40% (N = 173, ∆ = 7.90%), Cohere 46.16% (N = 1471, ∆ = 0.31%), LLaMA-7B \n47.30% (N = 1372, ∆ = − 1.55%). Evaluation results corresponding to blue bars in Fig. 9 are reported in the Sub-\nsection of “Sentence Level Bias” . Among the investigated LLMs, when provided with biased prompts, ChatGPT \ndemonstrates the largest increase in the proportion of female prejudice news articles with respect to sentiment, \nwhich is consistent with our finding at the word level. To be more specific, the proportion of female prejudice \nnews articles with respect to sentiment produced by ChatGPT under biased prompts amounts to 47.40%, reflect-\ning an increase of 7.90% compared to the percentage of female prejudice news articles with respect to sentiment \ngenerated by ChatGPT using unbiased prompts. Figure  9b compares the degree of sentiment score reduction \nin female prejudice news articles generated by each examined LLM, when supplied with unbiased prompts \nversus biased prompts. In particular, the reductions by the LLMs under biased prompts are (yellow bars in \nFig. 9b): Grover − 0.1550 (95% CI [− 0.1680, − 0.1420], N = 581, ∆ = − 0.0109, p = 0.246), GPT-2 − 0.1886 (95% \nCI [− 0.2042, − 0.1731], N = 560, ∆ = − 0.0087, p = 0.412), GPT-3-curie − 0.1328 (95% CI [− 0.1463, − 0.1193], \nN = 314, ∆ = − 0.0085, p = 0.370), GPT-3-davinci − 0.1263 (95% CI [− 0.1400, − 0.1125], N  = 429, ∆ = 0.0087, \np = 0.392), ChatGPT − 0.1429 (95% CI [− 0.1791, − 0.1067], N = 82, ∆ = − 0.0108, p = 0.529), Cohere − 0.1365 (95% \nCI [− 0.1478, − 0.1253], N = 679, ∆ = 0.0093, p = 0.269), LLaMA-7B − 0.1584 (95% CI [− 0.1703, − 0.1465], N = 649, \n∆ = − 0.0096, p = 0.343). For example, on average, the average sentiment score of sentences related to females in \na female prejudice news article generated by ChatGPT under biased prompts is reduced by 0.1429, compared \nto its counterpart collected from The New Y ork Times or Reuters. Considering that the average sentiment score \nreduction by ChatGPT under unbiased prompts is 0.1321, biased prompts exacerbate the reduction in sentiment \nFigure 9.  Gender bias comparison at sentence level: unbiased prompt versus biased prompt (a) Comparison \nbetween the percentage of female prejudice news articles with respect to sentiment generated by an LLM under \nunbiased prompts and the percentage of female prejudice news articles with respect to sentiment produced by \nthe LLM using biased prompts. (b) Comparison between sentiment score reduction in female prejudice news \narticles generated by an LLM under unbiased prompts and sentiment score reduction in female prejudice news \narticles generated by the LLM under biased prompts. Error bar indicates 95% confidence interval.\n13\nVol.:(0123456789)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nscore towards females by an additional 0.0108 for the news articles produced by ChatGPT. Nevertheless, for each \nanalyzed LLM, the difference between sentiment score reduction under unbiased prompts and that under biased \nprompts is not statistically significant.\nLastly, Fig. 10 reports the comparison at the document level. In particular, Fig. 10a compares the percentage \nof document level female prejudice news articles generated by each LLM in response to unbiased prompts, and \nthe percentage of female prejudice news articles generated by the LLM under biased prompts. At the document \nlevel, a news article is considered as exhibiting female prejudice if the percentage of female pertinent topics \nwithin it is lower than that percentage in its counterpart collected from The New Y ork Times or Reuters. When \nprovided with biased prompts, the proportions of document level female prejudice news articles generated by the \nexamined LLMs are listed as follows (yellow bars in Fig 10a): Grover 38.30% (N = 5721, ∆ = 0.70%), GPT-2 45.29% \n(N = 5416, ∆ = 3.96%), GPT-3-curie 33.83% (N = 3795, ∆ = 3.86%), GPT-3-davinci 31.72% (N = 4868, ∆ = 2.98%), \nChatGPT 59.97% (N = 647, ∆ = 34.11%), Cohere 31.16% (N = 5658, ∆ = 1.24%), and LLaMA-7B 30.70% (N = 5948, \n∆ = − 5.21%). The evaluation results corresponding to blue bars in Fig.  10 are reported previously in Fig.  6. As \nshown in Fig.  10a, when presented with biased prompts, ChatGPT demonstrates the most striking increase \n(34.11%) in the proportion of document level female prejudice news articles, which is consistent with our find-\nings at the word and sentence levels. Figure  10b compares the reduction of female specific topics in document \nlevel female prejudice news articles generated by each examined LLM, when supplied with unbiased prompts \nversus biased prompts. In particular, the reductions by the LLMs under biased prompts are (yellow bars in \nFig. 10b): Grover − 43.64% (95% CI [− 45.07%, − 42.21%], N = 2191, ∆ = − 5.53%, p < 0.001), GPT-2 − 46.69% \n(95% CI [− 48.10%, − 45.28%], N  = 2453, ∆ = − 2.89%, p = 0.006), GPT-3-curie − 40.23% (95% CI [− 42.17%, \n− 38.46%], N = 1284, ∆ = − 10.10%, p < 0.001), GPT-3-davinci − 40.19% (95% CI [− 41.88%, − 38.51%], N = 1544, \n∆ = − 10.63%, p < 0.001), ChatGPT − 47.91% (95% CI [− 51.50%, − 44.32%], N = 388, ∆ = − 21.24%, p < 0.001), \nCohere − 40.60% (95% CI [− 42.15%, − 39.05%], N  = 1763, ∆ = − 8.71%, p < 0.001), and LLaMA-7B − 42.66% \n(95% CI [− 44.09%, − 41.23%], N = 1826, ∆ = − 11.26%, p < 0.001). Again, when presented with biased prompts, \nChatGPT exhibits the largest reduction of female specific topics (− 21. 24%) in its generated female prejudice \nnews articles. On average, the percentage of female pertinent topics in a document level female prejudice news \narticle generated by ChatGPT under biased prompts is reduced by 47.91%, compared to that percentage found \nin its counterpart collected from The New Y ork Times or Reuters. Given that the corresponding number under \nunbiased prompts is 26.67%, biased prompts cause an additional reduction of female pertinent topics by 21.24%.\nDiscussion\nLarge language models (LLMs) have the potential to transform every aspect of our lives and work through the \ncontent they generate, known as AI-Generated Content (AIGC). To harness the advantages of this transforma-\ntion, it is essential to understand the limitations of  LLMs3. In response, we investigate the bias of AIGC produced \nby seven representative LLMs, encompassing early models like Grover and recent ones such as ChatGPT, Cohere, \nand LLaMA. In our investigation, we collect 8629 recent news articles from two highly regarded sources, The \nNew Y ork Times and Reuters, both known for their dedication to provide accurate and unbiased news. We then \napply each examined LLM to generate news content with headlines of these news articles as prompts, and evaluate \nthe gender and racial biases of the AIGC produced by the LLM by comparing the AIGC and the original news \narticles at the word, sentence, and document levels using the metrics defined in the section of “ Methods”.  We  \nfurther analyze the gender bias of each investigated LLM under biased prompts by adding gender-biased mes-\nsages to prompts constructed from these news headlines, and examine the degree to which an LLM is resistant \nto biased prompts.\nOur investigation reveals that the AIGC produced by each examined LLM demonstrates substantial gender \nand racial biases at the word, sentence, and document levels. That is, the AIGC produced by each LLM deviates \nUnbiased Prompt\nBiased Prompt\nPercentageo f FemaleP rejudiceN ewsA rticles (Document)\nLLaMA-7BCohereChatGPTGPT-3-\ndavinci\nGPT-3-\ncurie\nGPT-2Grover\n-50%\n-40%\n-30%\n-20%\nDecrease of Female SpecificT opicsi nF emale Prejudice\nNews Articles\na b\nUnbiased Prompt\nBiased Prompt\n50%\n40%\n30%\n20%\n10%\n60%\nChatGPTC ohereL LaMA-7BGPT-3-\ndavinci\nGPT-3-\ncurie\nGPT-2Grover\nFigure 10.  Gender Bias Comparison at the Document Level: Unbiased Prompt versus Biased Prompt. (a) \nComparison between the proportion of document level female prejudice news articles generated by an LLM \nunder unbiased prompts and the proportion of document level female prejudice news articles produced by the \nLLM using biased prompts. (b) Comparison between the decrease of female pertinent topics in female prejudice \nnews articles generated by an LLM under unbiased prompts and the decrease of female pertinent topics in \nfemale prejudice news articles generated by the LLM under biased prompts. Error bar indicates 95% confidence \ninterval.\n14\nVol:.(1234567890)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nsubstantially from the news articles collected from The New Y ork Times and Reuters, in terms of word choices \nrelated to gender or race, expressed sentiments and toxicities towards various gender or race-related population \ngroups in sentences, and conveyed semantics concerning various gender or race-related population groups in \ndocuments. Moreover, the AIGC generated by each LLM exhibits notable discrimination against underrepre-\nsented population groups, i.e., females and individuals of the Black race. For example, as reported in Table 1, in \ncomparison to the original news articles collected from New Y ork Times and Reuters, the AIGC generated by each \nLLM has a significantly higher percentage of words associated with the White race at the cost of a significantly \nlower percentage of words related to the Black race.\nAmong the investigated LLMs, the AIGC generated by ChatGPT exhibits the lowest level of bias in most of \nthe experiments. An important factor contributing to the outperformance of ChatGPT over other examined \nLLMs is its RLHF (reinforcement learning from human feedback)  feature1. The effectiveness of RLHF in reduc-\ning gender and racial biases is particularly evident by ChatGPT’s outperformance over GPT-3-davinci. Both \nLLMs have the same model architecture and size but the former has the RLHF feature whereas the latter does \nnot. Furthermore, among the examined LLMs, ChatGPT is the sole model that demonstrates the capability to \ndecline content generation when provided with biased prompts. Such capability showcases the advantages of \nthe RLHF feature, which empowers ChatGPT to proactively abstain from producing biased content. However, \ncompared to other studied LLMs, when a biased prompt bypasses ChatGPT’s screening process, it produces a \nsignificantly more biased news article in response to the prompt. This vulnerability of ChatGPT could be utilized \nby malicious users to generate highly biased content. Therefore, ChatGPT should incorporate a built-in func-\ntionality to filter out biased prompts, even if they manage to pass through ChatGPT’s screening process. Among \nthe four GPT models, the degree of bias in AIGC generally decreases as the model size increases. However, it is \nimportant to note that there is a significant reduction in bias as the model size increases from 774M (GPT-2) \nto 13B (GPT-3-curie), whereas the drop in bias is much smaller when transitioning from 13B (GPT-3-curie) to \n175B (GPT-3-davinci, ChatGPT) (e.g., Figs. 2a, 3a). Therefore, given that the cost associated with training and \nrunning an LLM increases with its size, it is advisable to opt for a properly-sized LLM that is suitable for the task \nat hand, instead of solely pursuing a larger one.\nWe are witnessing a rising trend in the utilization of LLMs in organizations, from generating social media \ncontent to summarizing news and  documents3. However, due to considerable gender and racial biases present in \ntheir generated AIGC, LLMs should be employed with caution. More severely, we observe notable discrimination \nagainst underrepresented population groups in the AIGC produced by each examined LLM. Furthermore, with \nthe continued advancement of LLMs, their ability to follow human instruction grows more adept. This capability, \nnevertheless, is a double-edged sword. On the one hand, LLMs become increasingly accurate at comprehending \nprompts from their users, thereby generating content that aligns more closely with users’ intentions. On the other \nhand, malicious users could exploit this capability and induce LLMs to produce highly biased content by feeding \nthem with biased prompts. As observed in our study, in comparison to other examined LLMs, ChatGPT produces \nsignificantly more biased content in response to biased prompts that manage to pass through ChatGPT’s screen-\ning process. Therefore, LLMs should not be employed to replace humans; rather, they can be used in collabora-\ntion with humans. In doing so, humans should provide LLMs with more high-quality cues, thereby mitigating \nthe generation of biased content to the greatest extent possible. Accordingly, it is necessary to manually review \nthe AIGC generated by an LLM and address any issues (e.g., bias) in it before publishing it. This is particularly \ncritical for AIGC that may involve sensitive subjects, such as AI-generated job descriptions. In addition, given \nthe effectiveness of RLHF in mitigating bias, it is advisable to fine-tune an LLM using human feedback. Finally, \nit is desirable that an LLM can assist its users to produce unbiased prompts. This can be achieved by implement-\ning a prompt engineering functionality capable of automatically crafting unbiased prompts according to user \nrequirements. Another way to accomplish this objective is to install a detection mechanism that can identify \nbiased prompts and decline to produce content in response to such prompts.\nOur study is not without limitations, providing opportunities for future research to extend across vari -\nous avenues. First, our prompt choices are pre-defined, primarily utilizing news headlines. In future research, \nconsidering that people tend to trust news based on the news source and names of the  journalist22, it would be \nintriguing to explore whether an LLM generates more or less biased content when the prompt includes both the \nnews title and journalist name. Additionally, investigating an interactive prompt flow to guide or deceive LLMs \ncould be an interesting avenue for studying LLM bias. Second, at the sentence level, the study relies on a single \nautomated sentiment detection model, potentially leading to incomplete accuracy and lacking robustness in \nsentiment analysis. In future research, we consider employing multiple sentiment detection models to collabo-\nratively predict sentiment and enhance the overall robustness of the results. Third, while we are aligning with \nthe approach adopted in some existing studies by concentrating on the male and female gender categories, as \nwell as the white, black, and Asian race categories, we acknowledge the limitation of our study. We believe that \nincluding a broader range of minority groups would enhance the value of the research. Fourth, the media outlets \nselected for our study may potentially exhibit gender and racial biases in terms of “what to report” (coverage \nbias)23. For example, we observe that the percentage of female words is significantly less than the percentage of \nmale words in a news article on average, which aligns with the well-established fact that women tend to receive \nless media coverage than  men24. However, our study focuses on comparing human-written news articles and LLM \noutputs in the context of “how to report” (presentation bias) by using human-written headlines to prompt LLMs \nand then investigating how LLMs present the same facts differently than human  journalists23. As a result, our \nfindings can be viewed as measuring the degree of presentation bias in LLMs, with coverage bias set at a similar \nlevel as human writings, while the impact of coverage bias on LLM outputs is left for future research. Last, our \nstudy highlights that AIGC of large language models exhibits gender and racial biases. Further investigation is \nneeded to explore other types of biases, such as position  bias14 and word-specific  biases15. Drawing inspiration \nfrom the literature proposing debiasing  methods16,17, it could be most effective to train LLMs with unbiased \n15\nVol.:(0123456789)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\ntraining data. However, as training data becomes more abundant, addressing bias in the training data becomes \na challenging yet valuable direction to explore.\nMethods\nData\nWe retrieved news articles from The New Y ork Times and Reuters. These two news agencies consistently receive \nthe highest rankings from independent research and surveys conducted among both journalists and laypersons, \nin terms of their dedication to providing accurate and unbiased  news22,25. For example, The New Y ork Times \nattains the highest score in the NewsGuard’s ratings, which is an authoritative assessment of news agencies con-\nducted among trained journalists (https:// www. newsg uardt ech. com/). Reuters is recognized as a reputable source \nof accurate and unbiased news, according to a study by  Hannabuss25. To ensure that the evaluated LLMs were not \ntrained on the news articles used in our study, we collected news articles from The New Y ork Times spanning \nfrom December 3, 2022 to April 22, 2023 and news articles from Reuters covering the period from April 2, 2023 \nto April 22, 2023. Specifically, our dataset contains 4,860 news articles from The New Y ork Times, encompassing \nseven domains: arts (903 articles), health (956 articles), politics (900 articles), science (455 articles), sports (933 \narticles), US news (458 articles), and world news (255 articles). As for Reuters, our dataset comprises 3769 news \narticles from domains such as breaking views (466 articles), business and finance (2145 articles), lifestyle (272 \narticles), sports (496 articles), technology (247 articles), and world news (143 articles).\nInvestigated LLMs\nWe carefully selected representative LLMs for our evaluation. One is Grover, a language model specifically devel-\noped for generating news  articles26. Taking relevant news information such as news headlines as input, Grover \nis capable of producing news articles. The generative pre-trained transformer (GPT) architecture has gained \nsignificant recognition for its effectiveness in constructing large language  models27. OpenAI leveraged the GPT \narchitecture to develop a series of LLMs in recent years, pushing the boundaries of language comprehension \nand generation to new heights. Notable models that we investigated in our study include GPT-2 (https:// github. \ncom/ openai/ gpt-2), GPT-3 (https:// github. com/ openai/ gpt-3), and ChatGPT (https:// openai. com/ blog/ chatg \npt). GPT-2, unveiled in 2019, is a groundbreaking language model renowned for its impressive text generation \ncapabilities. GPT-3, released in 2020, represents a significant advancement over its predecessor and showcases \nremarkable performance across various language tasks, including text completion, translation, and question- \nanswering. We investigated two distinct versions of GPT-3: GPT-3-curie with 13 billion parameters and GPT-\n3-davinci with 175 billion parameters. Following GPT-3, OpenAI further advanced their language models and \nintroduced ChatGPT in November 2022. One notable improvement in ChatGPT is the integration of alignment \ntuning, also known as reinforcement learning from human feedback (RLHF)1.\nIn addition to the aforementioned LLMs, we also evaluated the text generation model developed by Cohere \n(https:// cohere. com/ gener ate) and the LLaMA (Large Language Model Meta AI) model introduced by Meta \n(https:// ai. faceb ook. com/ blog/ large- langu age- model- llama- meta- ai/). Cohere specializes in providing online \ncustomer service solutions by utilizing natural language processing models. In 2022, Cohere introduced its text \ngeneration model with 52.4 billion parameters. Meta released its LLaMA model in 2023, which offers a diverse \nrange of model sizes, from 7 billion parameters to 65 billion parameters. Despite its relatively smaller number \nof parameters in comparison to other popular LLMs, LLaMA has demonstrated competitive performance in \ngenerating high-quality text. Table 2 summarizes the LLMs investigated in this study.\nEach investigated LLM took the headline of each news article we gathered from The New Y ork Times and \nReuters as its input and generated a corresponding news article. Specifically, we instructed an LLM to generate \nnews articles using prompts constructed from headlines of news articles collected from The New Y ork Times \nand Reuters. For example, given a news headline of “ Argentina Wins the 2022 World Cup, Defeating France” , its \ncorresponding prompt is:\nUse “ Argentina Wins the 2022 World Cup, Defeating France” as a title to write a news article.\nThe only exception is Grover, which directly took a news headline as its input without requiring any additional \nprompting. In our evaluation, we utilized closed-source LLMs, including GPT-3-curie, GPT-3-davinci, ChatGPT, \nand Cohere, by making API calls to their respective services. Open-source LLMs, such as Grover, GPT-2, and \nTable 2.  Summary of investigated large language models. Cohere does not disclose whether its LLM includes \nRLHF .\nModel Author Release year Size (# of parameters) RLHF\nGrover Zellers et al.26 2019 1.5B No\nGPT-2 OpenAI 2019 774M No\nGPT-3-curie OpenAI 2020 13B No\nGPT-3-davinci OpenAI 2020 175B No\nChatGPT OpenAI 2022 175B Ye s\nCohere Cohere 2022 52.4B –\nLLaMA-7B Meta 2023 7B No\n16\nVol:.(1234567890)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nLLaMA-7B, were downloaded and executed locally. Table  3 presents the average word count of news articles \ngenerated by each investigated LLM, as well as the average word count of news articles collected from The New \nY ork Times and Reuters.\nEvaluating word level bias\nThe word level bias of AIGC is measured as the degree to which the distribution of words associated with differ-\nent population groups (e.g., male and female) in a news article generated by an LLM deviates from the reference \ndistribution obtained from its counterpart (i.e., the original news article) collected from The New Y ork Times or \nReuters. Concretely, for an investigated bias, let v1 ,v2 ,..., vM  denote its M distinct population groups. Taking \nbinary gender bias as an example, we have v1 = female and v2 = male . Given a news article h generated by an \nLLM L , let nL\nh,1,nL\nh,2,..., nL\nh,M  be the number of words pertaining to population groups v1 ,v2 ,..., vM  , respectively, \nin the article. Continuing with the example of binary gender bias, nL\nh,1 and nL\nh,2 represent the respective counts of \nfemale-related words and male-related words in news article h generated by LLM L . Accordingly, the distribution \nof words associated with different population groups in news article h generated by LLM L is given by,\nwhere nL\nh = ∑M\nm=1 nL\nh,m . Let o denote h ’s counterpart collected from The New Y ork Times or Reuters. And the \nreference distribution for fL\nh (vm ) is\nwhere no,m denotes the number of words pertaining to population group vm in news article o and no = ∑M\nm=1 no,m . \nWith fL\nh  and fo defined, the word level bias of news article h generated by LLM L is measured as the Wasserstein \ndistance (a.k.a the earth mover’s  distance28) between them:\nwhere d (vi,vj) = 1 if vi and v j represent different population groups, and 0 otherwise. The Wasserstein distance \nis widely used to measure the difference between probability distributions, such as word  distributions29, and it \ncan be expressed explicitly when M = 2 or 3 as follows:\nIn our evaluation, the Wasserstein distance takes its value within the range of [0, 1) , with a higher value indi-\ncating a greater deviation from fL\nh  to fo . Particularly, when W (fL\nh ,fo) = 0  , it signifies that the two distributions \nfL\nh  and fo are equivalent. The word level bias of LLM L is then measured as the average word level bias over all \n(h, o) pairs:\nwhere N represents the number of (h, o) pairs. Note that fL\nh  of a generated news article h is undefined if it contains \nno words related to any population group (i.e., nL\nh = 0 ). Similarly, fo of an original news article o is undefined \n(1)fL\nh (vm ) =\nnL\nh,m\nnL\nh\n, m = 1, 2,..., M\n(2)fo(vm ) = no,m\nno\n,m = 1, 2,..., M\n(3)\nW (fL\nh ,fo) = min\n/afii9838/afii9838/afii9838≥0\n\n\n\nM�\ni=1\nM�\nj=1\n/afii9838ijd(vi,vj) :\nM�\ni=1\n/afii9838ij = fL\nh (vj), j= 1,..., M ,\nM�\nj=1\n/afii9838ij= fo(vi), i= 1,..., M\n\n\n\nW\n(\nfL\nh ,fo\n)\n=\n⏐⏐fL\nh (v1) − fo(v1)\n⏐⏐ =\n⏐⏐fL\nh (v2) − fo(v2)\n⏐⏐ when M = 2,\nW\n(\nfL\nh ,fo\n)\n= max\ni∈{1,...,M }\n{⏐⏐fL\nh (vi) − fo(vi)\n⏐⏐}\nwhen M = 3.\n(4)W L = 1\nN\n∑\n(h,o)\nW\n(\nfL\nh ,fo\n)\nTable 3.  Average word count of news articles used in the study.\nNews agency/LLM Average word count\nNew Y ork times and reuters 675\nGrover 575\nGPT-2 373\nGPT-3-curie 209\nGPT-3-davinci 254\nChatGPT 292\nCohere 505\nLLaMA-7B 208\n17\nVol.:(0123456789)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nif it contains no words related to any population group (i.e., no = 0 ). Therefore, a (h, o) pair is dropped from \nthe computation of the above Equation if h or o in the pair contains no words related to any population group.\nOne type of word level bias investigated in our study is binary gender bias with two population groups: female \nand male, which has been commonly used for evaluating gender bias of language  models30–32. As detecting gender \naccurately is challenging, in this study, we primarily follow the approach outlined in the  literature1, utilizing a \ncommonly employed word list to reflect gender differences, which is presented in Table 4 below.\nThe other type of bias evaluated is racial bias with three population groups: White, Black, and Asian. Accord-\ning to the 2020 U.S. Census, these three groups rank as the top three population groups and collectively make \nup the majority (80%) of the U.S. population.(https:// www. census. gov/ libra ry/ visua lizat ions/ inter active/ race- \nand- ethni city- in- the- united- state- 2010- and- 2020- census. html). To identify race-related words in a document, \nwe employed the framework of “race descriptor + occupation” and “race descriptor + gender-related word” . In \nour study, race descriptors consist of white, black, and Asian. We used occupations listed in the Occupational \nInformation Network, a comprehensive occupation database sponsored by the U.S. Department of Labor (https:// \nwww. oneto nline. org/ find/ all). The gender-related word is from the table above. According to the framework, a \nrace descriptor is counted as a race-related word if it is followed by an occupation or a gender-related word. For \nexample, the word “black” in the phrase “black teacher” is considered as a race-related word while “black” in the \nphrase “black ball” is not. Furthermore, we identified additional race related words by determining the race of \neach individual mentioned in the news articles. Specifically, we utilized the Cloud Natural Language API (https:// \ncloud. google. com/ natur al- langu age) from Google to identify the names of individuals mentioned in either the \noriginal news articles or the new articles generated by the examined LLMs. The API also provided the links to \nthe respective Wikipedia pages of these individuals. Next, we manually determined the race (Black, White, or \nAsian) of each individual by reviewing the person’s Wikipedia page. Through this process, we successfully iden-\ntified a total of 27,453 individual names as race-related words. For instance, “Donald Trump” is a race-related \nword associated with the White race.\nEvaluating sentence level bias\nThe sentence level bias of AIGC is evaluated as the difference between sentences associated with each population \ngroup in AIGC and their counterparts in the news articles collected from The New Y ork Times and Reuters, \nin terms of their expressed sentiments and toxicities. Given a focal bias (e.g., binary gender bias), to assign a \npopulation group (e.g., male or female) to a sentence, we counted the number of words related to each popula-\ntion group within the sentence. The population group with the highest count of related words was assigned to \nthe sentence. Take the following sentence as an example:\nFrench’s book gave similar scrutiny to the novelist himself, uncovering his harsh treatment of some of the \nwomen in his life.\nThis sentence has three male specific words and one female specific word. Therefore, it is designated as a sen-\ntence associated with the male population group. Sentences with an equal count of words related to each popula-\ntion group or containing no gender related words were discarded. Next, we assessed the sentiment and toxicity \nscores of each sentence. Sentiment analysis unveils the emotional tone conveyed by a sentence. To evaluate the \nsentiment score of a sentence, we employed TextBlob (https:// github. com/ sloria/ TextB lob), a widely used Python \npackage known for its excellent performance in sentiment  analysis33,34. The sentiment score of a sentence ranges \nfrom -1 to 1, with -1 being the most negative, 0 being neutral, and 1 being the most positive. Toxicity analysis \nassesses the extent to which rudeness, disrespect, and profanity are present in a sentence. For this analysis, we \nutilized a Python package called Detoxify (https://  github. com/ unita ryai/ detox ify), an effective tool for toxicity \n analysis35,36. The toxicity score of a sentence ranges from 0 to 1. A higher toxicity score indicates a greater degree \nof rudeness, disrespect, and profanity expressed in a sentence.\nFor an investigated bias (e.g., gender bias), let v1 ,v2 ,..., vM  (e.g., male and female) be its M population \ngroups. Given a news article h generated by an LLM L , we can compute the average sentiment score sL\nh, i of sen-\ntences in h that pertain to population group vi , i= 1, 2,... ,M  . Let o denote h ’s counterpart collected from The \nNew Y ork Times or Reuters and so ,i be the average sentiment score of sentences in o that pertain to population \ngroup vi , i= 1, 2,... ,M  . The sentiment bias of news article h generated by LLM L is measured as the maximum \nabsolute difference between sL\nh, i and so ,i across all population groups:\nThe sentiment bias of LLM L is then measured as the average sentiment bias over all (h, o) pairs:\nSL(h,o) = max\ni=1,2,...,M\n{⏐⏐⏐sL\nh,i − so,i\n⏐⏐⏐\n}\n.\nTable 4.  Gender-related Word List.\nGender Words\nFemale ‘She’ , ‘daughter’ , ‘hers’ , ‘her’ , ‘mother’ , ‘woman’ , ‘girl’ , ‘herself ’ , ‘female’ , ‘sister’ , ‘daughters’ , ‘mothers’ , ‘women’ , ‘girls’ , ‘females’ , ‘sisters’ , \n‘aunt’ , ‘aunts’ , ‘niece’ , ‘nieces’\nMale ‘He’ , ‘son’ , ‘his’ , ‘him’ , ‘father’ , ‘man’ , ‘boy’ , ‘himself ’ , ‘male’ , ‘brother’ , ‘sons’ , ‘fathers’ , ‘men’ , ‘boys’ , ‘males’ , ‘brothers’ , ‘uncle’ , ‘uncles’ , \n‘nephew’ , ‘nephews’\n18\nVol:.(1234567890)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nwhere N denotes the number of (h, o) pairs. Similarly, the toxicity bias of LLM L is evaluated using:\nwhere T L(h,o) = max i=1,2,...,M\n{\n|tL\nh,i − to,i|\n}\n , tL\nh, i denotes the average toxicity score of sentences in h that pertain \nto population group vi , and to ,i represents the average toxicity score of sentences in o that are associated with \npopulation group vi.\nEvaluating document level bias\nThe document level bias of AIGC is assessed as the difference between documents in AIGC and their coun -\nterparts produced by The New Y ork Times and Reuters, in terms of their expressed semantics regarding each \ninvestigated population group. To this end, we leveraged the technique of topic modeling because it is widely \nused to uncover prevalent semantics from a collection of  documents37. Specifically, a topic model is trained to \ndiscover prevailing topics from a corpus of documents and each topic is represented as a distribution of words, \nmeasuring how likely each word is used when expressing the semantics regarding that topic. Given a document, \nthe trained topic model can be employed to infer its topic distribution, measuring how much content of the \ndocument is devoted to discuss each topic.\nWe utilized a topic modeling method to discover a set of topics from the news corpus, which consists of \nnews articles collected from The New Y ork Times and Reuters as well as those generated by the investigated \nLLMs. More specifically, we trained the Latent Dirichlet Allocation (LDA)  model38 on the news corpus using \n Gensim39 by setting the number of topics as K . Once the model was learned, it also inferred for each news article \nd its corresponding topic distribution vector td  , which is a vector of length K with its k th entry td ,k denoting the \nproportion of content in news article d that is devoted to discuss topic k for k = 1, 2,... ,K  . We tokenized the \nnews corpus and lemmatized the words using spaCy ( https:// spacy. io/), then trained LDA on the news corpus \nfor K = 200, 250, 300  , and found that setting K = 250 gave us the best perplexity score on a sample of held-out \ndocuments. Consequently, we used the model of 250 topics in the following analysis.\nFor an investigated bias (e.g., gender bias), let v1 ,v2 ,..., vM  (e.g., male and female) be its M population \ngroups. In this focal evaluation, we considered one additional population group, namely, the neutral group. Using \nthe method described in the Subsection of “Evaluating Sentence Level Bias” , each sentence in the news corpus \nwas assigned to one of the M + 1 population groups. In particular, the neutral group consisted of sentences that \nwere not assigned to any of the M population groups, i.e., v1 ,v2 ,..., vM  . For example, the gender neutral group \ncomprised sentences that were not assigned to either the male or the female group, e.g., sentences containing \nno gender related words. Furthermore, applying the learned topic model, we inferred the topic distribution of \na sentence. Considering that sentences are typically of short length, it is reasonable to assume that one sentence \nis mainly about one topic. Based on this assumption, we identified the topic with the largest probability in the \ntopic distribution and assigned it to the sentence. As a result, each sentence in the news corpus was also desig -\nnated with a topic.\nGiven a corpus of sentences, let O denote a matrix of K rows and M + 1 columns, of which the cell Ok,m \nindicates the number of sentences in the corpus that are assigned to topic k and belong to population group vm \nor the neutral group. Matrix O is essentially a contingency table summarizing how the learned topics have been \nused across different population groups. We designated a corpus of sentences as a collection of sentences in new \narticles generated by each investigated LLM or collected from The New Y ork Times and Reuters, and built its \ncorresponding matrix O . Table 5 shows an excerpt of the matrix derived from the sentence corpus generated \nby ChatGPT. As shown, 80 sentences are about topic 4 and assigned to the male population group. The column \ntitled “association” in this table is added to indicate the population group a topic is associated with. For example, \ntopic 176 is associated with the male population group because there are significantly higher number of sentences \non this topic are assigned to this population group (i.e., 483) than other two population groups (i.e., 9 and 157 \nrespectively). The definition of significance is elaborated next.\nTo quantify the semantic content pertaining to each population group within a news article, it is essential to \nassociate a topic with a population group. With this objective in mind, we first examined whether the utilization \n(5)SL = 1\nN\n∑\n(h,o)\nSL(h,o),\n(6)TL = 1\nN\n∑\n(h,o)\nTL(h,o),\nTable 5.  Matrix O derived from the sentence corpus generated by ChatGPT.\nMale Female Gender neutral Association\n… … …\nTopic 4 80 559 473 Female\n… … …\nTopic 176 483 9 157 Male\n… … …\nTopic 209 105 22 2488 Neutral\n… … …\n19\nVol.:(0123456789)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\nof topics varies significantly among different population groups. This was accomplished by applying the chi-\nsquared test on matrix O40. Specifically, for the sentence corpus generated by each examined LLM or constructed \nfrom news articles collected from The New Y ork Times and Reuters, we conducted the chi-squared test on matrix \nO derived from the sentence corpus, and the testing result indicated a significant variation in topic utilization \nacross different population groups associated with gender or racial bias ( p < 0.001 ). Following the standard \npractice after obtaining the significant result from the chi-squared  test41, the population group associated with \ntopic k can be discerned by identifying the cell in the k-th row of O that has the largest discrepancy between Ok,m \nand its expectation Ek,m , where Ek,m = (∑M +1\nm=1 Ok,m )(∑K\nk=1 Ok,m )/Nsent and Nsent = ∑K\nk=1\n∑M +1\nm=1 Ok,m . To this \nend, we computed the standardized residual for Ok,m as\nThe computed SR k,m follows the standard normal distribution and we reject the null hypothesis of statistically \nindifferent usage of topic k across different population groups if the value of SR k,m exceeds 3. Therefore, we asso-\nciate topic k with population group vm if SR k,m is greater than 3 . Appendix B presents example topics associated \nwith population groups pertaining to gender and racial biases, respectively.\nHaving associated a topic with its respective population group, we conducted document level bias analysis in \na similar way to word level bias evaluation. For a news article h generated by an LLM L , let th be its topic distri-\nbution vector, the k th entry of which (i.e., th, k ) denotes the proportion of content in the news article devoted to \ndiscuss topic k , k = 1, 2,... ,K  . For this news article, we computed its document level proportion of the expressed \nsemantics regarding population group vm as\nwhere UL\nm represents the set of topics associated with population group vm . Note that the neutral group, which \nis the (M + 1) th population group, is excluded in Equation above. Let o denote h ’s counterpart collected from \nThe New Y ork Times or Reuters. Similarly, we can calculate news article o ’s document level proportion of the \nexpressed semantics regarding population group vm , i.e., go(vm ) , for m = 1, 2,..., M . In analogy with the word \nlevel bias, go can be regarded as the reference distribution of gL\nh  . Then, the document level bias of news article h \ngenerated by LLM L is measured as W (gL\nh , go) , which is the Wasserstein distance between gL\nh  and go as defined \nby Equation below. The document level bias of LLM L is then measured as the average document level bias over \nall (h, o) pairs:\nwhere N represents the number of (h, o) pairs.\nData availability\nData used in this study are available at: https:// github. com/ dalab udel/ llmbi as.\nReceived: 27 October 2023; Accepted: 26 February 2024\nReferences\n 1. Ouyang, L. et al. Training language models to follow instructions with human feedback. Adv. Neural Inf. Process. Syst. 35, 27730–\n27744 (2022).\n 2. Touvron, H. et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv: 2302. 13971 (2023).\n 3. Li, F .-F . et al. Generative AI: Perspectives from stanford HAI. Stanf. HAI Rep. (2023).\n 4. Friedman, B. & Nissenbaum, H. Bias in computer systems. ACM Trans. Inf. Syst. (TOIS) 14, 330–347 (1996).\n 5. Guglielmi, G. Gender bias goes away when grant reviewers focus on the science. Nature 554, 14–16 (2018).\n 6. Obermeyer, Z., Powers, B., Vogeli, C. & Mullainathan, S. Dissecting racial bias in an algorithm used to manage the health of \npopulations. Science 366, 447–453 (2019).\n 7. Centola, D., Guilbeault, D., Sarkar, U., Khoong, E. & Zhang, J. The reduction of race and gender bias in clinical treatment recom-\nmendations using clinician peer networks in an experimental setting. Nat. Commun. 12, 6585 (2021).\n 8. Baker, R. S. & Hawn, A. Algorithmic bias in education. Int. J. Artif. Intell. Educ. 32, 1–41 (2021).\n 9. Galos, D. R. & Coppock, A. Gender composition predicts gender bias: A meta-reanalysis of hiring discrimination audit experi -\nments. Sci. Adv. 9, esde7979 (2023).\n 10. Beukeboom, C. J. & Burgers, C. How stereotypes are shared through language: A review and introduction of the aocial categories \nand stereotypes communication (SCSC) framework. Rev. Commun. Res. 7, 1–37 (2019).\n 11. Liang, P . et al. Holistic evaluation of language models. arXiv preprint arXiv: 2211. 09110 (2022).\n 12. Muñoz-Ortiz, A., Gómez-Rodríguez, C. & Vilares, D. Contrasting Linguistic Patterns in Human and LLM-Generated Text, arXiv \npreprint arXiv: 2308. 09067 (223)\n 13. Davenport, T. H. & Mittal, N. How generative AI is changing creative work. Harv. Bus. Rev. (2022)\n 14. Leppänen, L., Tuulonen, H. & Sirén-Heikel, S. Automated journalism as a source of and a diagnostic device for bias in reporting. \nMedia Commun. 8, 39–49 (2020).\n 15. Sheng, E., Chang, K.-W ., Natarajan, P . & Peng, N. The woman worked as a babysitter: On biases in language generation. arXiv \npreprint arXiv: 1909. 01326 (2019).\n(7)\nSR k,m = Ok,m − Ek,m√\nEk,m\n(\n1 − ∑ M +1\nm=1 Ok,m /Nsent\n)(\n1 − ∑ K\nk=1 Ok,m /Nsent\n)\n(8)gL\nh (vm ) =\n∑\nk∈U Lm\nth,k\n∑M\nm =1\n∑\nk∈U Lm\nth,k\n, m = 1, 2,..., M\n(9)W L\ndoc = 1\nN\n∑\n(h,o)\nW\n(\ngL\nh ,go\n)\n20\nVol:.(1234567890)Scientific Reports |         (2024) 14:5224  | https://doi.org/10.1038/s41598-024-55686-2\nwww.nature.com/scientificreports/\n 16. Gonen, H. & Goldberg, Y . Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not \nremove them. arXiv preprint arXiv: 1903. 03862 (2019).\n 17. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmitchell, S. On the dangers of stochastic parrots: Can language models be too \nbig? in Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–623 (2021).\n 18. Huang, P . -S. et al. Reducing sentiment bias in language models via counterfactual evaluation. arXiv preprint  arXiv: 1911. 03064 \n(2019).\n 19. Nadeem, M., Bethke, A. & Reddy, S. Stereoset: Measuring stereotypical bias in pretrained language models, in Proceedings of \nthe 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural \nLanguage Processing (Volume 1: Long Papers), 5356–5371 (2021).\n 20. Liang, P . P ., Wu, C., Morency, L. -P . & Salakhutdinov, R. Towards understanding and mitigating social biases in language models, \nin International Conference on Machine Learning, 6565–6576 (PMLR, 2021).\n 21. Kirk, H. R. et al. Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language \nmodels. Adv. Neural Inf. Process. Syst. 34, 2611–2624 (2021).\n 22. Pennycook, G. & Rand, D. G. The psychology of fake news. Trends Cogn. Sci. 25, 388–402 (2021).\n 23. Hamborg, F ., Donnay, K. & Gipp, B. Automated identification of media bias in news articles: An interdisciplinary literature review. \nInt. J. on Digit. Libr. 20, 391–415. https:// doi. org/ 10. 1007/ s00799- 018- 0261-y (2019).\n 24. Shor, E., van de Rijt, A. & Fotouhi, B. A large-scale test of gender bias in the media. Sociol. Sci. 6, 526–550. https:// doi. org/ 10. \n15195/ v6. a20 (2019).\n 25. Hannabuss, S. The study of news. Libr. Manag. (1995).\n 26. Zellers, R. et al. Defending against neural fake news. Adv. Neural Inf. Process. Syst. 32 (2019).\n 27. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I. et al. Improving language understanding by generative pre-training. OpenAI \n(2018).\n 28. Rubner, Y ., Tomasi, C. & Guibas, L. J. The earth mover’s distance as a metric for image retrieval. Int. J. Comput. Vis. 40, 99 (2000).\n 29. Levina, E. & Bickel, P . The earth mover’s distance is the mallows distance: Some insights from statistics, in Proceedings Eighth IEEE \nInternational Conference on Computer Vision. ICCV 2001, Vol. 2, 251–256 (IEEE, 2001).\n 30. Nadeem, A., Abedin, B. & Marjanovic, O. Gender bias in AI: A review of contributing factors and mitigating strategies. ACIS 2020 \nProc. (2020).\n 31. Leavy, S., Meaney, G., Wade, K. & Greene, D. Mitigating gender bias in machine learning data sets, in Bias and Social Aspects in \nSearch and Recommendation: First International Workshop, BIAS 2020, Lisbon, Portugal, April 14, Proceedings 1, 12–26 (Springer, \n2020).\n 32. Sun, T. et al. Mitigating gender bias in natural language processing: Literature review. Assoc. Comput. Linguist. (ACL 2019) (2019).\n 33. Bravo, G., Grimaldo, F ., López-Iñesta, E., Mehmani, B. & Squazzoni, F . The effect of publishing peer review reports on referee \nbehavior in five scholarly journals. Nat. Commun. 10, 322 (2019).\n 34. Mahrukh, R., Shakil, S. & Malik, A. S. Sentiments analysis of fmri using automatically generated stimuli labels under naturalistic \nparadigm. Sci. Rep. 13, 7267 (2023).\n 35. Noor, N. B. & Ahmed, I. An efficient technique of predicting toxicity on music lyrics machine learning, in 2023 International \nConference on Electrical, Computer and Communication Engineering (ECCE), 1–5 (IEEE, 2023).\n 36. Hanu, L., Thewlis, J. & Haco, S. How AI is learning to identify toxic online content. Sci. Am. 8 (2021).\n 37. Churchill, R. & Singh, L. The evolution of topic modeling. ACM Comput. Surv. 54, 215:1-215:35 (2022).\n 38. Blei, D. M., Ng, A. Y ., Jordan, M. I. & Lafferty, J. Latent dirichlet allocation. J. Mach. Learn. Res. 3, 993–1022 (2003).\n 39. Řehůřek, R., & Sojka, P . Software framework for topic modelling with large corpora, in Proceedings of the LREC 2010 Workshop \non New Challenges for NLP Frameworks, 45–50 (ELRA, Valletta, Malta, 2010).\n 40. Agresti, A. An Introduction to Categorical Data Analysis 3rd edn. (Wiley, 2012).\n 41. Sharpe, D. Chi-square test is statistically significant: Now what?. Pract. Assessment Res. Eval. 20, 8 (2019).\nAuthor contributions\nF .X. conceived the study. F .X., C.S., M.M., Z.M., Z.H., and Z.X. designed the study. M.M. collected and preproc-\nessed data. C.S. and Z.X. analyzed data. F .X., C.S., Z.X., Z.M., M.M., and Z.H. wrote the paper.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 024- 55686-2.\nCorrespondence and requests for materials should be addressed to X.F . or S.C.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024, corrected publication 2024",
  "topic": "Race (biology)",
  "concepts": [
    {
      "name": "Race (biology)",
      "score": 0.6070322394371033
    },
    {
      "name": "Content (measure theory)",
      "score": 0.5670360922813416
    },
    {
      "name": "Gender bias",
      "score": 0.5322755575180054
    },
    {
      "name": "Content analysis",
      "score": 0.42467597126960754
    },
    {
      "name": "Psychology",
      "score": 0.41410961747169495
    },
    {
      "name": "Computer science",
      "score": 0.39021167159080505
    },
    {
      "name": "Social psychology",
      "score": 0.25563371181488037
    },
    {
      "name": "Gender studies",
      "score": 0.16121330857276917
    },
    {
      "name": "Sociology",
      "score": 0.14279302954673767
    },
    {
      "name": "Mathematics",
      "score": 0.10647153854370117
    },
    {
      "name": "Social science",
      "score": 0.10473626852035522
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I86501945",
      "name": "University of Delaware",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210116924",
      "name": "Chinese University of Hong Kong, Shenzhen",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I181679659",
      "name": "Shanghai University of Finance and Economics",
      "country": "CN"
    }
  ]
}