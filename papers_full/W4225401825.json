{
    "title": "Parameter-Efficient Sparsity for Large Language Models Fine-Tuning",
    "url": "https://openalex.org/W4225401825",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2119530036",
            "name": "Yuchao Li",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2625368959",
            "name": "Fuli Luo",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2475478252",
            "name": "Chuanqi Tan",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2096829243",
            "name": "Wang Mengdi",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2159562265",
            "name": "Songfang Huang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2116106766",
            "name": "Shen, Li",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2096933339",
            "name": "Junjie Bai",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3038012435",
        "https://openalex.org/W3205949070",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4288480287",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W2764043458",
        "https://openalex.org/W3104216863",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W2965862774",
        "https://openalex.org/W3174702398",
        "https://openalex.org/W3005842225",
        "https://openalex.org/W3173390350",
        "https://openalex.org/W3104688113",
        "https://openalex.org/W3034340181",
        "https://openalex.org/W4297813615",
        "https://openalex.org/W4287777801",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3015982254",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W4386453467",
        "https://openalex.org/W2949960976"
    ],
    "abstract": "With the dramatically increased number of parameters in language models, sparsity methods have received ever-increasing research focus to compress and accelerate the models. While most research focuses on how to accurately retain appropriate weights while maintaining the performance of the compressed model, there are challenges in the computational overhead and memory footprint of sparse training when compressing large-scale language models. To address this problem, we propose a Parameter-efficient Sparse Training (PST) method to reduce the number of trainable parameters during sparse-aware training in downstream tasks. Specifically, we first combine the data-free and data-driven criteria to efficiently and accurately measure the importance of weights. Then we investigate the intrinsic redundancy of data-driven weight importance and derive two obvious characteristics i.e. low-rankness and structuredness. Based on that, two groups of small matrices are introduced to compute the data-driven importance of weights, instead of using the original large importance score matrix, which therefore makes the sparse training resource-efficient and parameter-efficient. Experiments with diverse networks (i.e. BERT, RoBERTa and GPT-2) on dozens of datasets demonstrate PST performs on par or better than previous sparsity methods, despite only training a small number of parameters. For instance, compared with previous sparsity methods, our PST only requires 1.5% trainable parameters to achieve comparable performance on BERT.",
    "full_text": "Parameter-Efficient Sparsity for Large Language Models Fine-Tuning\nYuchao Li, Fuli Luo , Chuanqi Tan, Mengdi Wang,\nSongfang Huang , Shen Li , Junjie Bai\nAlibaba Group\n{laiyin.lyc, lfl259702, chuanqi.tcq, didou.wmd, songfang.hsf, litan.ls, j.bai}@alibaba-inc.com\nAbstract\nWith the dramatically increased number of parame-\nters in language models, sparsity methods have re-\nceived ever-increasing research focus to compress\nand accelerate the models. While most research\nfocuses on how to accurately retain appropriate\nweights while maintaining the performance of the\ncompressed model, there are challenges in the com-\nputational overhead and memory footprint of sparse\ntraining when compressing large-scale language\nmodels. To address this problem, we propose a\nParameter-efficient Sparse Training (PST) method\nto reduce the number of trainable parameters during\nsparse-aware training in downstream tasks. Specifi-\ncally, we first combine the data-free and data-driven\ncriteria to efficiently and accurately measure the\nimportance of weights. Then we investigate the\nintrinsic redundancy of data-driven weight impor-\ntance and derive two obvious characteristics i.e.\nlow-rankness and structuredness. Based on that,\ntwo groups of small matrices are introduced to\ncompute the data-driven importance of weights, in-\nstead of using the original large importance score\nmatrix, which therefore makes the sparse training\nresource-efficient and parameter-efficient. Experi-\nments with diverse networks (i.e. BERT, RoBERTa\nand GPT-2) on dozens of datasets demonstrate PST\nperforms on par or better than previous sparsity\nmethods, despite only training a small number of\nparameters. For instance, compared with previ-\nous sparsity methods, our PST only requires 1.5%\ntrainable parameters to achieve comparable perfor-\nmance on BERT.\n1 Introduction\nMany applications in natural language processing have been\nfollowing a paradigm, which first pre-trains a large language\nmodel and then fine-tunes it towards multiple downstream\ntasks. Despite its great success, such large-scale language\nmodels with millions to billions of parameters need a huge\nmemory footprint and computational overhead in fine-tuning\ndownstream datasets and also the inference stage, which pre-\nvents them from being directly applied to various tasks.\nMethod Extra Train\nParam. Need Data Importance Criteria\nMaP 0× # |W|\nMvP 1× ! −W ∗G\nPST 0.01 ∼0.02× ! |W|+AB +R+C\nTable 1: Comparison between different sparsity methods. MaP and\nMvP represent the representative data-free and data-driven meth-\nods, respectively. W represents the weights, G represents the cor-\nresponding gradient. A, B, R and C denote our proposed small\nmatrices. We simplify the importance criteria for clear analysis.\nTo mitigate the computational and memory burden in the\nlanguage model inference, one promising direction is prun-\ning [McCarley et al., 2019; Zhang and He, 2020], which re-\nmoves unimportant weights/channels/layers independently to\nreduce the computation and memory overhead. Among these,\nunstructured pruning, i.e. sparsity, is widely studied since it\ncan achieve a higher compression ratio with competitive per-\nformance.\nPrevious sparsity methods propose various criteria to com-\npute the importance of each weight, which can be roughly\nclassified to two categories, data-free [Han et al., 2015;\nTanaka et al., 2020 ] and data-driven [Sanh et al., 2020;\nWang et al., 2020a ]. The comparison is shown in Ta-\nble 1. Data-free criterion methods compute the importance of\nweight based on the weight itself without any data involved,\nsuch as magnitude pruning (MaP) [Han et al., 2015]. Al-\nthough data-free criteria have high computational and mem-\nory efficiency, they ignore that the role of each weight varies\nwidely across different downstream tasks, which leads to\ndegradation in model performance. Typical data-driven cri-\nteria methods focus on designing precise important criteria to\ncompute the importance scores based on the specific dataset,\nwhich is proved to succeed in reducing the computation infer-\nence cost of the language model without a performance drop.\nHowever, these data-driven criteria introduce extra computa-\ntion and trainable parameters to obtain the importance mea-\nsurement, which dramatically increases the memory footprint\nand computational overhead during sparsity-aware training.\nFor example, movement pruning (MvP) [Sanh et al., 2020]\ncomputes the importance by multiplying the weights and their\ngradients and therefore needs extra memory to save impor-\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4223\ntance scores matrix, which has the same size as the weights.\nGraSP [Wang et al., 2020a ] introduces extra computational\noverhead to compute the hessian-gradient product.\nIn this paper, we propose a Parameter-efficient Sparse\nTraining (PST) method to reduce the number of parameters\ninvolved in the weight importance computation, which can\ntackle the resource requirement issue in the sparse training\nwhile computing the accurate importance score. Consider-\ning the efficiency of data-free criteria and the accurateness\nof data-driven criteria, the combination of them is adopted\nto leverage the advantage of both. After that, to reduce the\nnumber of extra trainable parameters, i.e. importance scores\nintroduced by data-driven criteria, the training of the huge im-\nportance matrix is converted to the tuning of multiple small\nmatrices, based on the two following basic observations,\n• Low-rankness: we analyze the rank of weights and gra-\ndients based on previous works and observe that all of\nthem have extremely low ranks, which means that the\nrank of the importance score matrix (combination of\nweight and gradient matrix) is also small. Therefore it\ncan be represented by a set of rank-decomposition ma-\ntrices (i.e., A and B in Table 1 and Fig. 1).\n• Structuredness: we investigate the distribution of\nsparse weights and observe the phenomenon that there\nare some rows/columns less important than the others in\ngeneral, which inspires us to introduce a set of small ma-\ntrices to measure the importance of each row/column in\nweight. (i.e., R and C in Table 1 and Fig. 1)\nTwo sets of small matrices are introduced to represent the\nlow-rankness and structuredness in the data-driven impor-\ntance scores, respectively. The computation of importance\nscores in the specific downstream task is reformulated by\nthese small matrices. With the replacement, the resource re-\nquirement for data-driven criteria computation is dramatically\nreduced. Moreover, we further reduce the number of train-\nable parameters by representing the update of weights with a\nlow-rank decomposition, which optimizes a set of low-rank\nmatrices instead of weight to capture the change of it.\nOur contributions can be summarized as follows:\n• We propose the Parameter-efficient Sparse Training\n(PST) method, which reduces the number of trainable\nparameters for the large language model sparse training\nand thus optimizes the fine-tuning and inference process\nin a parameter-efficient way.\n• We exploit both the low-rankness and structuredness in\nthe data-driven importance score and thus replace it with\nseveral small matrices. This leads to a novel research\narea, how to compress the redundancy of the importance\nscore to efficiently obtain the importance of weights.\n• Extensive experiments demonstrate the effectiveness of\nour method across various typical pre-trained large lan-\nguage models (e.g., BERT, RoBERTa, and GPT-2) upon\ndiverse datasets. In particular, compared with previous\nworks, PST obtains 98.5% trainable parameter saving\nwith a 0.12 average score improvement in GLUE.\nS\nW+×U V\nA B R C\nM\n× ++|W+UV|+ Top-v\nMovement Pruning(Data-Driven)\nParameter-Efficient SparseTraining\n⊙X Y× =\nW Trainable ParameterFrozen ParameterActivation\nM\nTop-vMagnitude Pruning(Data-Free) ⊙X Y\n10\n=×\n|W|\nW M\nTop-v⊙X Y=×\nFigure 1: The framework of magnitude pruning, movement prun-\ning, and our PST method. The magnitude pruning only optimizes\nthe weight W, and the movement pruning simultaneously optimizes\nthe weight W and importance score S to compute the sparse binary\nmask M. In our PST method, the update of weight is replaced by\ntwo small matrices (U and V ), and the data-driven importance score\nis decomposed into two sets of small matrices ( i.e., A,B and R,C)\nbased on its low-rankness and structuredness.\n2 Related Works\nParameter-efficient fine-tuning. Parameter-efficient fine-\ntuning reduces the number of trainable parameters by opti-\nmizing various lightweight modules instead of original pre-\ntrained weight. For instance, [Houlsby et al., 2019 ] intro-\nduced a trainable adapter with small number of parameters\nto achieve the parameter-efficient fine-tuning. [Lester et al.,\n2021] proposed efficient prompt tuning which only optimized\na small task-specific vector. [He et al., 2021 ] presented a\nunified framework that employs multiple modules from pre-\nvious works. Besides, [Guo et al., 2020 ] proposed only\nupdating a small number of elements in the trainable vec-\ntors for parameter-efficient fine-tuning. [Hu et al., 2021 ]\nintroduced two low-rank matrices to approximate parame-\nter updates. However, finetuned models produced by these\nmethods have the same number of weight as the pre-trained\nmodel, which still leads to huge computation and memory\noverhead when inference. Different from them, we propose a\nparameter-efficient sparse training method to prune the unim-\nportant weights in the language model during training, which\nreduces the resource requirement of network inference.\nParameter-efficient inference. There are several popular\nlanguage model compression techniques, e.g., pruning, quan-\ntization, and low-rank decomposition. Among these, prun-\ning is widely-used, which reduces the number of parame-\nters in the network inference. Structured pruning directly re-\nmoves structured weights (e.g., attention heads [McCarley et\nal., 2019 ], channels [Wang et al., 2020b ] or layers [Zhang\nand He, 2020]) to compress and accelerate the large language\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4224\nmodels. By contrast, unstructured pruning, i.e. sparsity,\nremoves the individual unimportant weights independently.\nPrevious works proposed various criteria to select insignifi-\ncant weights for pruning, such as absolute weight [Gordon\net al., 2020], taylor approximation [Molchanov et al., 2019],\nhessian-gradient product [Wang et al., 2020a] and data-free\nsaliency scores [Tanaka et al., 2020]. However, these meth-\nods either propose a computation-efficient importance crite-\nrion but lead to worse network performance (i.e., magnitude\npruning), or design an accurate importance criterion which\nmay need huge computation overhead (i.e., movement prun-\ning and GraSP). Unlike these methods, our approach exploits\nintrinsic redundancy of the weight importance matrix and\npropose the parameter-efficient sparse training to obtain the\nbetter sparse network with lower resource requirement.\n3 Proposed Method\n3.1 Preliminaries\nWe first establish a general notation for analyzing the spar-\nsity methods. Generally, for a weight matrix W ∈Rn×k,\na network sparse strategy introduces an importance score\nS ∈Rn×k to determine which weights should be removed.\nBased on S, a binary mask M ∈ {0, 1}n×k can be gener-\nated for computation Y = (W ⊙M)X, where Y ∈Rn×m\nand X ∈Rk×m are the output and input of the layer, respec-\ntively. ⊙denotes the Hadamard product. A common strategy\nis to keep the top-v of the weight W based on the importance\nscore S. Thus, we define a function f(S, v) which selects the\nv largest values in S to generate the binary mask M:\nMi,j = f(S, v)i,j =\n\u001a1, S i,jin top-v,\n0, otherwise. (1)\nIn this work, we focus on iterative sparse training, which re-\nmoves the unimportant weights and updates the importance\nscore step-by-step. Previous methods prove that this strat-\negy enables the network to recover from the information loss\ndue to sparsity. Thus, the optimized process of the language\nmodel fine-tuning is:\nmin\nW,S\nL(W ⊙f(S, v); D), s.t. v\nn ∗k ≤1 −p (2)\nwhere Dis the observed dataset, Lrepresents the loss func-\ntion, and p denotes the target compression ratio. The up-\ndate of S depends on various sparse strategies. For ex-\nample, movement pruning [Sanh et al., 2020] uses S(t) =\n−\ntP\ni=1\n( δL\nδW )(i) ⊙W(i) to compute the importance score.\n3.2 Parameter-Efficient Sparse Training\nAs presented in [Zhao et al., 2020] and [Zhang et al., 2021],\nthe final binary mask generated by the trainable importance\nscore is similar to that directly produced by the magnitude\npruning, and the difference between them depends on the spe-\ncific dataset. It means that the importance of each weight de-\npends on its absolute value and its role in the downstream\ntasks. Thus, we propose a new importance score S(t) =\n|W(t)|+ ∆S(t), where |W(t)|and ∆S(t) represent the data-\nfree and data-driven importance of weight at the tth-step,\n(a) Attention Query Layer\n (b) Attention Output Layer\n(c) FFN Input Layer\n (d) FFN Output Layer\nFigure 2: For each figure, the right sub-figure is the visualization\nof the binary mask M in the first block of BERT on SST-2 when\nsparsity is 90%. The left sub-figure is the corresponding sparsity\ndistribution of column(blue) and row(orange). The x-axis repre-\nsents the sparsity ratio and the y-axis represents the percentage of\ncolumns/rows whose sparsity ratio belongs to each interval.\nrespectively. Inspired by the works in [Sanh et al., 2020;\nZhang et al., 2021], we can directly optimize the importance\nscore by SGD to obtain the data-driven importance score∆S,\nand thus the importance score at the tth-step is re-written as:\nS(t) = |W(t)|−α\ntX\ni=1\n( δL\nδW )(i) ⊙W(i), (3)\nwhere α is a hyper-parameter to trade-off the data-free and\ndata-driven importance score. For data-free importance score\n|W(t)|, it does not need any extra parameters, which is\nresource-efficient. Therefore, we only consider the compres-\nsion of data-driven importance score −α\ntP\ni=1\n( δL\nδW )(i) ⊙W(i)\nto achieve the parameter-efficient sparse training.\nLow-Rankness. As we known, rank(W ⊙ δL\nδW ) ≤\nrank(W) ∗rank( δL\nδW ), which means that the rank of data-\ndriven importance score depends on the rank of W and δL\nδW .\nPrevious work [Hu et al., 2021 ] proves that the gradient\nof weight δL\nδW has a low intrinsic rank, which even can be\none or two in the language models. Thus the rank of the\ndata-driven importance score matrix is close to the rank of\nthe weight matrix. Existing literature [Oymak et al., 2019;\nLi et al., 2021] shows that in the neural network, the trained\nlarge weight W often naturally bears approximate low-rank\nweight structures. According to that, we can derive the data-\ndriven importance score also has a low intrinsic rank. Thus,\nwe introduce two small low-rank matrices A ∈Rn×r1 and\nB ∈Rr1×k to represent the low intrinsic rank part of data-\ndriven importance score ∆S, where r1 is a hyper-parameter,\ncontrolling the number of trainable parameters for impor-\ntance score. To make the data-driven importance score of\neach weight the same at the beginning, A and B are initial-\nized with Gaussian initialization and zero initialization re-\nspectively, and are directly optimized by SGD.\nStructuredness. Generally, sparsity methods remove the\nweights without any constraint, which means that the distri-\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4225\nbution of the sparse result (binary maskM) is uncontrollable.\nHowever, as shown in Fig. 2, the binary maskM produced by\nimportance score S shows the obvious structural pattern. For\ninstance, the right sub-figure in Fig. 2(a) shows that there are\nmany rows with extremely few weights reserved. To quan-\ntify such a phenomenon, we compute the sparsity ratio of\neach column/row in binary M, then obtain their histograms\nby dividing the sparsity ratio into several intervals and com-\nputing the percentage of columns and rows whose sparsity ra-\ntios belong to corresponding intervals. The left sub-figure in\nFig. 2(a) demonstrates that there are about 30% rows in which\nall weights are removed, while most columns have a similar\nsparsity ratio. In contrast, Fig. 2(b) shows that most columns\nhave very high sparsity ratios. Therefore, we conclude that\nthe weights of the columns/rows differ significantly in impor-\ntance. Based on the observation, we propose two structural\nimportance score matrices R ∈Rn×1 and C ∈R1×k to mea-\nsure the importance of each column/row in the weight. The\nupdate of them is:\nR(t) = −\ntX\ni=0\nkX\nj=0\n[( δL\nδW )(i) ⊙W(i)]:,j,\nC(t) = −\ntX\ni=0\nnX\nj=0\n[( δL\nδW )(i) ⊙W(i)]j,\n(4)\nIn summary, the data-driven importance score becomes:\n∆S(t) = α1A(t)B(t) + α2(R(t) + C(t)), (5)\nwhere the α1 and α2 are the hyper-parameters to trade-off the\nlow-rankness and structural importance score, respectively.\nTo further reduce the resource-requirement of the sparse\ntraining, we follow [Hu et al., 2021] to constrain the update\nof weight by representing it with a low-rank decomposition\nW(t) = W(0) + βU (t)V (t), where U ∈Rn×r2 , V ∈Rr2×k\nand r2 controls the trainable parameters of weight. Therefore,\nthe importance score in our method is:\nS(t) = |W(0) +βU (t)V (t)|+α1A(t)B(t) +α2(R(t) +C(t)). (6)\nBased on that, the computation of each layer becomes:\nY =[(W(0) + βU (t)V (t)) ⊙f(|W(0) + βU (t)V (t)|\n+ α1A(t)B(t) + α2(R(t) + C(t)), v)]X.\n(7)\nIt should be noted that, after fine-tuning, all weights are\nfinalized and the inference procedure will be Y = W∗X,\nwhere W∗ is sparse, W∗ = [(W(0)+βU (t)V (t))⊙f(|W(0)+\nβU (t)V (t)|+ α1A(t)B(t) + α2(R(t) + C(t)), v)]. Therefore,\nthe inference procedure is parameter- and resource-efficient.\nThe optimized process of our sparse training is:\nmin\nU,V,A,B,R,C\nL((W(0) + βUV ) ⊙f(|W(0) + βUV |\n+ α1AB| {z }\nLow−Rankness\n+ α2(R + C)| {z }\nStructuredness\n, v); D),\ns.t. v\nn ∗k ≤1 −p\n(8)\nIn addition, the number of trainable parameters in our method\nis (n + k) ∗(r1 + r2 + 1), which is extremely smaller than\nthe original number 2 ∗n ∗k when r1 and r2 is small.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nSparsity Ratio\n84\n86\n88\n90Acc. MaP\nMvP\nPST\nFT\n(a) MRPC\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nSparsity Ratio\n87\n88\n89\n90\n91\n92\n93Acc. MaP\nMvP\nPST\nFT (b) SST-2\nFigure 3: Comparison between different sparsity methods with dif-\nferent sparsity ratios on BERTbase.\n4 Experiments\n4.1 Evaluation Setup\nDatasets and Backbone Models. We conduct experiments\nwith BERT[Devlin et al., 2019], RoBERTa[Liu et al., 2019],\nand GPT-2 [Radford et al., 2019 ] in various downstream\ntasks. For BERT and RoBERTa, we use GLUE benchmarks\n[Wang et al., 2018] for evaluation. For GPT-2, we evaluate it\non the E2E, DART, and WebNLG.\nImplementation Details. For BERTbase, we set batch size\n= 32 and perform a hyperparameter search over learning rate\n∈{3e-5, 5e-5, 1e-4, 5e-4} and epoch ∈{20, 40}on QNLI,\nSST-2, CoLA, STS-B, MRPC, RTE and epoch ∈ {10,20}\non MNLI, QQP. Moreover, we use a batch size of 16 for\nRoBERTa, as well as a hyperparameter search over learn-\ning rate ∈{1e-5, 2e-5, 3e-5, 5e-5}. Epoch search space is\nthe same as BERTbase. For GPT-2, we train the model for 5\nepochs using a batch size of 8 and an initial learning rate of\n1e-4. At training time, we use the AdamW optimizer and a\nlinear learning rate scheduler. All models are initialized with\nthe pre-trained weights. We follow the[Zhu and Gupta, 2018]\nto use a cubic sparsity scheduling. We also add a few steps of\nwarm-up at the beginning of training (10% training steps) and\ncool-down at the end of training (30% training steps), which\nempirically improve the performance especially in high spar-\nsity regimes. For PST, we set β = α1 = α2 = 1 and\nr1 = r2 = 8.1\n4.2 Results\nBERT and RoBERTa. Table 2 shows that our method\nachieves the largest reduction of trainable parameters with\non-par or better performance than previous methods. We\ninitialize the importance score by the absolute value of the\npre-trained weights for movement pruning to avoid obtain\nterrible performance. For instance, we achieve 0.73 average\nscore improvement with 98.9% trainable parameter saving on\nRoBERTalarge when the sparsity ratio is 90%. Moreover, we\nobserve that MaP outperforms other methods with little or\nno loss with respect to the fine-tuned dense model at the low\nsparsity ratio (50%). However, when increasing the sparsity\nratio to 90%, it obtains an obvious performance drop whether\nin BERT or RoBERTa. In contrast, our method PST performs\n1Our code is available at https://github.com/alibaba/AliceMind/\ntree/main/S4/PST and https://github.com/yuchaoli/PST.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4226\nModel Method Sparsity\nRatio\nTrainable\nParam. MNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.\nBERTbase\nFine-tune 0% 110.00M 84.72 87.80 91.49 93.00 58.55 88.68 89.45 62.82 82.06\nMaP 50% 110.00M 83.58 87.80 91.47 90.94 60.11 89.78 90.73 67.15 82.70\nMvP 50% 194.93M 82.26 87.33 90.83 90.83 57.66 89.43 91.06 67.15 82.07\nPST 50% 2.91M 80.97 85.77 89.77 91.28 57.60 84.63 90.72 67.87 81.08\nMaP 90% 110.00M 79.75 82.83 85.06 87.04 40.74 81.72 82.78 54.87 74.35\nMvP 90% 194.93M 80.06 85.37 86.53 87.04 40.46 84.35 84.28 58.84 75.87\nL0 Regu∗ 90% 194.93M 77.90 81.90 - - - - - - -\nPST 90% 2.91M 76.73 83.93 86.03 88.65 42.49 81.70 85.57 62.82 75.99\nRoBERTabase\nFine-tune∗ 0% 125.00M 87.60 91.90 92.80 94.80 63.60 91.20 90.20 78.70 86.40\nMaP 90% 125.00M 80.85 84.90 85.70 88.99 19.13 83.58 83.82 55.23 72.78\nMvP 90% 209.93M 81.40 86.42 87.13 89.68 38.12 85.85 85.71 56.32 76.33\nPST 90% 2.91M 76.70 83.83 87.26 90.02 38.08 84.94 87.34 60.29 76.06\nRoBERTalarge\nFine-tune∗ 0% 355.00M 90.20 92.20 94.70 96.40 68.00 92.40 90.90 86.60 88.90\nMaP 90% 355.00M 79.37 83.29 85.83 89.68 14.94 80.21 82.77 58.12 71.78\nMvP 90% 682.36M 82.91 85.94 88.27 90.83 32.50 84.20 85.20 59.93 76.22\nPST 90% 7.77M 81.40 85.21 87.64 90.83 39.29 84.95 87.07 59.21 76.95\nTable 2: Results of different network sparsity methods with BERT base and RoBERTalarge on the GLUE benchmark. ∗indicates numbers\npublished in prior works. Bold number represents the best results under the same sparsity ratio.\nMethod Sparsity Trainable E2E DART WebNLG\nRatio Param. BLEU MET NIST BLEU MET TER BLEU MET TER\nFine-tune 0% 354.92M 68.36 46.41 8.66 46.00 0.39 0.46 47.60 0.39 0.50\nMaP 90% 354.92M 68.42 46.08 8.64 44.72 0.37 0.50 37.38 0.30 0.64\nMvP 90% 656.91M 69.24 46.36 8.73 45.11 0.37 0.50 38.32 0.32 0.63\nPST 90% 7.77M 70.04 46.51 8.81 45.27 0.37 0.49 44.57 0.34 0.53\nTable 3: GPT-2 medium performance on E2E, DART and WebNLG with different methods. For all metrics except TER, higher is better.\npoorly with the low sparsity ratio but obtains better perfor-\nmance than other methods at a higher sparsity ratio, which\nis also shown in Fig. 3. Meanwhile, although RoBERTa\nachieves better performance than BERT after fine-tuning, the\nmodel after sparse training performs worse than BERT. We\nfind that RoBERTa has a smaller learning rate than BERT\non downstream tasks, which indicates that RoBERTa relies\nmore on pre-trained weights than BERT. The sparsity meth-\nods make some weights become zeros. These weight changes\nin RoBERTa may have a greater impact on downstream tasks.\nWe have to note that it is not a common phenomenon, the\nlarger models are usually more stable than smaller models in\nthe field of model compression [Li et al., 2020].\nGPT-2. We further verify that our method can also prevail\non the NLG model. As shown in Table 3, our PST achieves\nthe best performance while training an extremely smaller\nnumber of parameters in three downstream tasks. In particu-\nlar, compared with MvP, we obtain 6.25 BLEU improvement\nwhile saving 98.8% trainable parameters on WebNLG.\n4.3 Ablation Study\nImportance score. The design of importance score plays\na crucial role in our proposed PST. We combine the data-\nfree and data-driven importance score, and decompose data-\ndriven importance score into two sets of small matrices based\non its low-rankness and structuredness. Precisely, we com-\npare seven different importance scores on BERT base in Ta-\nble 5. We adjust the r1 and r2 to make all of the meth-\nr1\nr2 4 8 16\n4 84.07 84.88 85.52\n8 85.86 85.57 85.76\n16 86.45 86.75 86.21\n(a) MRPC\nr1\nr2 4 8 16\n4 88.42 88.53 88.76\n8 88.65 88.65 88.53\n16 88.76 88.99 87.96\n(b) SST-2\nTable 4: Comparison on BERTbase with different rank r1 and r2.\nods have the same number of trainable parameters. The re-\nsults show that the proposed importance score achieves the\nbest performance in various downstream tasks. Furthermore,\nstructuredness is more important than low-rankness for im-\nportance score compared with line 2 and 3.\nRank r1 and r2. Table 4 shows the effect of the rank r1\nand r2. We observe that although the model performance in-\ncreases as the rank increases, higher is not necessarily better.\nWhen the one rank is lower (i.e., r1 = 4 or r2 = 4 ), an-\nother rank increases will improve the model accuracy. But\nwhen the one rank is large enough (i.e., r1 = 16 or r2 = 16),\nthe increase of another one does not necessarily improve the\nmodel performance. This suggests that the rank r1 and r2 can\nalso be searched to explore the most suitable configuration\nfor different downstream tasks.\n4.4 Analysis\nDistribution of sparse weights. Fig. 4(a) shows an\noverview of the distribution of the remaining weights of MaP,\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4227\nS (Importance Score) QNLI SST-2 CoLA STS-B MRPC RTE Avg.\n|W(0) + βUV |+ α1AB + α2(R + C) 86.03 88.65 42.49 81.7 85.57 62.82 74.54\n|W(0) + βUV |+ α1AB 85.61 88.42 32.60 78.80 83.44 61.01 71.65\n|W(0) + βUV |+ α2(R + C) 85.58 88.19 37.71 81.67 85.34 62.82 73.55\n|W(0) + βUV | 85.83 88.19 37.66 80.08 84.96 61.37 73.02\nα1AB + α2(R + C) 85.48 87.50 32.90 80.52 84.95 62.82 72.36\nα1AB 83.56 84.63 22.02 69.84 81.66 54.15 65.98\nα2(R + C) 85.10 87.27 34.93 81.50 85.12 61.73 72.61\nTable 5: Comparison on BERTbase of different importance scores with same number of trainable parameters (p = 90%).\n−0.2 −0.1 0.0 0.1 0.2\nWeight\n0.0%\n2.0%\n4.0%\n6.0%\n8.0%%\nMaP\nMvP\nPST\n(a) Distribution of sparse weights\n (b) Scores and weights in MaP\n(c) Scores and weights in MvP\n (d) Scores and weights in PST\nFigure 4: Distribution of sparse weights of MaP, MvP and PST, re-\nspectively (p = 90%).\nMvP and PST respectively at the same layer with a spar-\nsity ratio of 90%. Compared with MaP that tends to remove\nweights close to zero and MvP that removes weights with the\nlarger values, PST has a smoother distribution, which holds\nweights both with larger and smaller values. Fig. 4(b)(c)(d)\ndisplay the weight against the importance score of MaP, MvP,\nand PST, respectively. The pruned and remaining weights are\ngrey and blue dot respectively. We observe that the PST re-\nflects the characteristics of both the data-free (MaP) and data-\ndriven (MvP) methods. MaP computes the importance score\nof weights based on their absolute values and thus shows a v-\nshaped curve. MvP removes any weights regardless of their\nabsolute values (except zero). However, PST not only consid-\ners the absolute value of weight but also remains the weight\nwith a low absolute value, and therefore shows a combination\nof their two distributions.\nSimilarity of binary mask. We use the Hamming distance\nto compute the similarity of binary mask M among different\nmethods. Fig. 5 shows that the sparse binary mask M of PST\nis closer to MaP than MvP, which means that the data-free\nimportance score accounts for a greater proportion in PST.\nMoreover, as shown in Fig. 5(c) and Fig. 5(d), the similarity\nbetween MaP and PST decreases when the depth of layers\n0 2 4 6 8 10\nLayer\n82.0\n82.5\n83.0\n83.5\n84.0\n84.5%\nMaP-PST\nMvP-PST\nMaP-MvP\n(a) Attention Query Layer\n0 2 4 6 8 10\nLayer\n82.0\n82.5\n83.0\n83.5\n84.0\n84.5%\nMaP-PST\nMvP-PST\nMaP-MvP (b) Attention Output Layer\n0 2 4 6 8 10\nLayer\n82\n83\n84\n85\n86\n87%\nMaP-PST\nMvP-PST\nMaP-MvP\n(c) FFN Input Layer\n0 2 4 6 8 10\nLayer\n82.0\n82.5\n83.0\n83.5\n84.0\n84.5%\nMaP-PST\nMvP-PST\nMaP-MvP (d) FFN Output Layer\nFigure 5: Similarity of the binary mask M between MaP, MvP and\nPST, respectively (p= 90%).\nin the FFN module increases. It demonstrates that the PST\ngradually reduces the impact of data-free importance score\nwith the deepening of the layer. However, with the increase\nof the depth of layers, the similarity between MvP and PST\nincreases in the input layer of the FFN module and decreases\nin the output layer of the FFN module. It indicates that the\nimportance score of PST explores the new information that is\ndifferent from MaP and MvP in the output layer.\n5 Conclusion\nIn this paper, we propose a parameter-efficient sparse train-\ning (PST) method to reduce the number of trainable parame-\nters and the resource requirements during sparse-aware fine-\ntuning of large language models. We first combine the data-\nfree and data-driven criteria to compute the importance of\nweights. Then we discover two characteristics (i.e., low-\nrankness and structuredness) of data-driven importance score,\nand therefore introduce two sets of parameter-efficient matri-\nces to replace the original large importance score matrix. Ex-\ntensive experiments on various language models demonstrate\nthe effectiveness of PST in reducing the computational com-\nplexity and resource requirements in sparse fine-tuning.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4228\nReferences\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 4171–4186, 2019.\n[Gordon et al., 2020] Mitchell A Gordon, Kevin Duh, and\nNicholas Andrews. Compressing bert: Studying the ef-\nfects of weight pruning on transfer learning. Association\nfor Computational Linguistics, page 143, 2020.\n[Guo et al., 2020] Demi Guo, Alexander M Rush, and Yoon\nKim. Parameter-efficient transfer learning with diff prun-\ning. arXiv preprint arXiv:2012.07463, 2020.\n[Han et al., 2015] Song Han, Huizi Mao, and William J\nDally. Deep compression: Compressing deep neural net-\nworks with pruning, trained quantization and huffman cod-\ning. arXiv preprint arXiv:1510.00149, 2015.\n[He et al., 2021] Junxian He, Chunting Zhou, Xuezhe Ma,\nTaylor Berg-Kirkpatrick, and Graham Neubig. Towards a\nunified view of parameter-efficient transfer learning.arXiv\npreprint arXiv:2110.04366, 2021.\n[Houlsby et al., 2019] Neil Houlsby, Andrei Giurgiu, Stanis-\nlaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly.\nParameter-efficient transfer learning for nlp. In Inter-\nnational Conference on Machine Learning, pages 2790–\n2799. PMLR, 2019.\n[Hu et al., 2021] Edward J Hu, Yelong Shen, Phillip Wallis,\nZeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large\nlanguage models. arXiv preprint arXiv:2106.09685, 2021.\n[Lester et al., 2021] Brian Lester, Rami Al-Rfou, and Noah\nConstant. The power of scale for parameter-efficient\nprompt tuning. arXiv preprint arXiv:2104.08691, 2021.\n[Li et al., 2020] Zhuohan Li, Eric Wallace, Sheng Shen,\nKevin Lin, Kurt Keutzer, Dan Klein, and Joey Gonzalez.\nTrain big, then compress: Rethinking model size for ef-\nficient training and inference of transformers. In ICML,\npages 5958–5968, 2020.\n[Li et al., 2021] Yuchao Li, Shaohui Lin, Jianzhuang Liu,\nQixiang Ye, Mengdi Wang, Fei Chao, Fan Yang, Jincheng\nMa, Qi Tian, and Rongrong Ji. Towards compact cnns\nvia collaborative compression. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6438–6447, 2021.\n[Liu et al., 2019] Yinhan Liu, Myle Ott, Naman Goyal,\nJingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:\nA robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\n[McCarley et al., 2019] JS McCarley, Rishav Chakravarti,\nand Avirup Sil. Structured pruning of a bert-based ques-\ntion answering model. arXiv preprint arXiv:1910.06360,\n2019.\n[Molchanov et al., 2019] Pavlo Molchanov, Arun Mallya,\nStephen Tyree, Iuri Frosio, and Jan Kautz. Importance\nestimation for neural network pruning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 11264–11272, 2019.\n[Oymak et al., 2019] Samet Oymak, Zalan Fabian,\nMingchen Li, and Mahdi Soltanolkotabi. General-\nization guarantees for neural networks via harnessing\nthe low-rank structure of the jacobian. arXiv preprint\narXiv:1906.05392, 2019.\n[Radford et al., 2019] Alec Radford, Jeffrey Wu, Rewon\nChild, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9, 2019.\n[Sanh et al., 2020] Victor Sanh, Thomas Wolf, and Alexan-\nder M Rush. Movement pruning: Adaptive sparsity by\nfine-tuning. In Advances in Neural Information Process-\ning Systems, 2020.\n[Tanaka et al., 2020] Hidenori Tanaka, Daniel Kunin,\nDaniel L Yamins, and Surya Ganguli. Pruning neural\nnetworks without any data by iteratively conserving\nsynaptic flow. Advances in Neural Information Processing\nSystems, 33, 2020.\n[Wang et al., 2018] Alex Wang, Amanpreet Singh, Julian\nMichael, Felix Hill, Omer Levy, and Samuel Bowman.\nGlue: A multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP, 2018.\n[Wang et al., 2020a] Chaoqi Wang, Guodong Zhang, and\nRoger Grosse. Picking winning tickets before training by\npreserving gradient flow. In International Conference on\nLearning Representations, 2020.\n[Wang et al., 2020b] Ziheng Wang, Jeremy Wohlwend, and\nTao Lei. Structured pruning of large language models. In\nProceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing, 2020.\n[Zhang and He, 2020] Minjia Zhang and Yuxiong He. Ac-\ncelerating training of transformer-based language models\nwith progressive layer dropping. Advances in Neural In-\nformation Processing Systems, 33, 2020.\n[Zhang et al., 2021] Yuxin Zhang, Mingbao Lin, Fei Chao,\nYan Wang, Yongjian Wu, Feiyue Huang, Mingliang Xu,\nYonghong Tian, and Rongrong Ji. Lottery jackpots exist\nin pre-trained models. arXiv:2104.08700, 2021.\n[Zhao et al., 2020] Mengjie Zhao, Tao Lin, Fei Mi, Martin\nJaggi, and Hinrich Sch ¨utze. Masking as an efficient alter-\nnative to finetuning for pretrained language models. arXiv\npreprint arXiv:2004.12406, 2020.\n[Zhu and Gupta, 2018] Michael H Zhu and Suyog Gupta. To\nprune, or not to prune: Exploring the efficacy of prun-\ning for model compression. International Conference on\nLearning Representations, 2018.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n4229"
}