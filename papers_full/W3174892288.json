{
  "title": "Future-Guided Incremental Transformer for Simultaneous Translation",
  "url": "https://openalex.org/W3174892288",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2312944217",
      "name": "Shaolei Zhang",
      "affiliations": [
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2014643161",
      "name": "Yang Feng",
      "affiliations": [
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2251786545",
      "name": "Liangyou Li",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2312944217",
      "name": "Shaolei Zhang",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2014643161",
      "name": "Yang Feng",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Institute of Computing Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2890698823",
    "https://openalex.org/W2952992734",
    "https://openalex.org/W2129336405",
    "https://openalex.org/W3091044183",
    "https://openalex.org/W2964078338",
    "https://openalex.org/W2251955814",
    "https://openalex.org/W2529548870",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W2951456627",
    "https://openalex.org/W6768570733",
    "https://openalex.org/W2972356804",
    "https://openalex.org/W2251766657",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2886751231",
    "https://openalex.org/W2964213727",
    "https://openalex.org/W2970074184",
    "https://openalex.org/W2948117287",
    "https://openalex.org/W2951642234",
    "https://openalex.org/W2419292002",
    "https://openalex.org/W2964059644",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2975711469",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3116878451"
  ],
  "abstract": "Simultaneous translation (ST) starts translations synchronously while reading source sentences, and is used in many online scenarios. The previous wait-k policy is concise and achieved good results in ST. However, wait-k policy faces two weaknesses: low training speed caused by the recalculation of hidden states and lack of future source information to guide training. For the low training speed, we propose an incremental Transformer with an average embedding layer (AEL) to accelerate the speed of calculation of the hidden states during training. For future-guided training, we propose a conventional Transformer as the teacher of the incremental Transformer, and try to invisibly embed some future information in the model through knowledge distillation. We conducted experiments on Chinese-English and German-English simultaneous translation tasks and compared with the wait-k policy to evaluate the proposed method. Our method can effectively increase the training speed by about 28 times on average at different k and implicitly embed some predictive abilities in the model, achieving better translation quality than wait-k baseline.",
  "full_text": "Future-Guided Incremental Transformer for Simultaneous Translation\nShaolei Zhang1,2, Yang Feng1,2*, Liangyou Li3\n1Key Laboratory of Intelligent Information Processing\nInstitute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)\n2 University of Chinese Academy of Sciences, Beijing, China\n3 Huawei Noah’s Ark Lab\nfzhangshaolei20z, fengyangg@ ict.ac.cn\nliliangyou@huawei.com\nAbstract\nSimultaneous translation (ST) starts translations syn-\nchronously while reading source sentences, and is used in\nmany online scenarios. The previous wait-k policy is con-\ncise and achieved good results in ST. However, wait-k pol-\nicy faces two weaknesses: low training speed caused by the\nrecalculation of hidden states and lack of future source in-\nformation to guide training. For the low training speed, we\npropose an incremental Transformer with an average embed-\nding layer (AEL) to accelerate the speed of calculation of the\nhidden states during training. For future-guided training, we\npropose a conventional Transformer as the teacher of the in-\ncremental Transformer, and try to invisibly embed some fu-\nture information in the model through knowledge distillation.\nWe conducted experiments on Chinese-English and German-\nEnglish simultaneous translation tasks and compared with the\nwait-k policy to evaluate the proposed method. Our method\ncan effectively increase the training speed by about 28 times\non average at different k and implicitly embed some predic-\ntive abilities in the model, achieving better translation quality\nthan wait-k baseline.\nIntroduction\nSimultaneous translation(ST) (Cho and Esipova 2016; Gu\net al. 2017; Ma et al. 2019; Arivazhagan et al. 2019), a vari-\nant of machine translation, aims to output the translations\nwhile reading source sentences, which is more suitable for\ninput-output synchronization tasks (such as online transla-\ntion, live subtitle and simultaneous interpretation).\nRecently, wait-k policy (Ma et al. 2019) is a widely used\nread / write policy, which ﬁrst waits forksource tokens, and\nthen translates concurrently with the rest of source sentence.\nWait-k policy was trained by a “preﬁx-to-preﬁx” architec-\nture, and need to recalculate the hidden states of all previ-\nous source tokens when a new source token is received. The\nwait-k policy achieved excellent results in ST and success-\nfully integrated some implicit anticipation through “preﬁx-\nto-preﬁx” training.\nHowever, there are still two shortcomings in the adop-\ntion of source information. According to whether the to-\nken is read, all source tokens can be divided into two cate-\ngories: consumed and future. First, for the consumed source,\n*Corresponding author: Yang Feng.\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nwait-k needs to re-calculate the hidden states of all previous\nsource tokens at each decoding step, making the computa-\ntional cost increase quadratically (Dalvi et al. 2018; Chen\net al. 2020). The growth factor of the computational cost\nin training is proportional to the length of the target sen-\ntence. Second, for the future source, since wait-k policy is\ntrained with “preﬁx-to-preﬁx” architecture, some source to-\nkens will lag behind due to the different word order, which\nis not considered in training. Although “preﬁx-to-preﬁx” ar-\nchitecture makes wait-k policy have some implicit anticipa-\ntion, Ma et al. (2019) pointed that the acquisition of implicit\nanticipation is data-driven, since the training data contains\nmany preﬁx-pairs in the similar form. We consider that the\ndata-driven approach is inefﬁcient and uncontrollable. Dur-\ning training, wait-k policy lacks the guidance from future\nsource information, to gain a stronger predictive ability.\nTo address the above two problems, we propose a\nFuture-Guided Incremental Transformer with average\nembedding layer (AEL) and knowledge distillation (Hinton,\nVinyals, and Dean 2015). The proposed method greatly ac-\ncelerate the training speed, meanwhile plenty exploit the fu-\nture information to guide training and enable the model to\nobtain a stronger predictive ability.\nTo avoid the high complexity caused by recalculation\nof the consumed source hidden states, inspired by Zhang,\nXiong, and Su (2018), we propose the incremental Trans-\nformer, including a unidirectional encoder and a decoder\nwith an average embedding layer. The average embedding\nlayer is added into decoder to summarize the consumed\nsource information, by calculating the average embedding\nof all consumed source tokens. Therefore, each token can\nattend to all consumed tokens through the unidirectional en-\ncoder and AEL, avoiding the recalculation at the same time.\nTo utilize future source information to enhance the pre-\ndictive ability, we encourage the model to embed some\nfuture information through knowledge distillation (Hinton,\nVinyals, and Dean 2015; Ravanelli, Serdyuk, and Bengio\n2018; Novitasari et al. 2019). Unlike some previous meth-\nods of adding ‘predict operation’ to ST, out method do not\nexplicitly predict the next word or verb, but implicitly em-\nbed the future information in the model. While training in-\ncremental Transformer (student), we simultaneously trained\na conventional Transformer for full-sentence NMT as the\nteacher of incremental Transformer. Thus, the incremental\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n14428\nTransformer can learn some future information from the\nconventional Transformer. While testing, we only use incre-\nmental Transformer for ST, so that it does not introduce any\nwaiting time or any calculations.\nExperiment results on the Chinese-English, German-\nEnglish simultaneous translation tasks show our method out-\nperforms the baseline.\nIn summary, our contributions are two-fold:\n• Our method does not need to recalculate the hidden states\nof encoder, and also allows each source token to attend\nto the complete consumed source. In training, our method\ncan greatly accelerate the training speed about 28 times.\n• Our method provides a way to embed future information\nin the incremental model, and effectively enhances the\npredictive ability of the incremental model without adding\nany waiting time or parameters during the inference time.\nBackground\nWe propose our method based on full-sentence NMT and\nwait-k policy (Ma et al. 2019), so we ﬁrst brieﬂy introduce\nthem.\nFull-Sentence NMT\nTransformer (Vaswani et al. 2017) is currently the most\nwidely used model for full-sentence NMT. Transformer con-\nsists of two parts, encoder and decoder, each of which con-\ntains N repeated independent structures. The input sentence\nis x = (x1;\u0001\u0001\u0001 ;xn), where xi 2Rdmodel and dmodel rep-\nresents the representation dimension. The encoder maps x\nto a sequence of hidden states z = (z1;\u0001\u0001\u0001 ;zn). Given z\nand the previous target tokens, the decoder predicts the next\noutput token yt, and ﬁnally the entire output sequence is\ny = (y1;\u0001\u0001\u0001 ;ym).\nThe self-attention in conventional Transformer is calcu-\nlated as following:\neij = Q(xi) K(xj)T\npdk\n(1)\n\u000bij = exp eijPn\nl=1 exp eil\n(2)\nwhere eij measures similarity between inputs, \u000bij is the at-\ntention weight, Q(\u0001) and K(\u0001) are the projection functions\nfrom the input space to the query space and the key space,\nrespectively, anddk represents the dimensions of the queries\nand keys. Then, the value is weighted by\u000bij to calculate the\nhidden state zi:\nzi =\nnX\nj=1\n\u000bijV (xj) (3)\nwhere V (\u0001) is a projection function from the input space to\nthe value space. The ﬁnal encoder output is a hidden states\nsequence z 2 Rn\u0002dz , where dz is the dimension of the\nhidden states. The per-layer complexity of self-attention is\nO(n2 \u0001d) (Vaswani et al. 2017), where n is the sequence\nlength and dis the representation dimension.\nFigure 1: The architecture of the proposed method. The\nlower part is the incremental Transformer, while the upper\npart is the conventional Transformer. A knowledge distilla-\ntion is applied between the hidden states for future-guidance.\nWait-k Policy\nWait-k policy (Ma et al. 2019) refers to waiting forksource\ntokens ﬁrst, and then reading and writing alternately, i.e., the\noutput always delays ktokens after the input.\nDeﬁne g(t) as a monotonic non-decreasing function of t,\nwhich represents the number of source tokens read in when\noutputting the target token yt. For the wait-k policy, g(t) is\ncalculated as:\ng(t) = minfk+ t\u00001;jxjg;t = 1;2;\u0001\u0001\u0001 (4)\nTo simulate “preﬁx-to-preﬁx” training, the source tokens\nparticipating in self-attention is limited to less than g(t):\ne(t)\nij =\n(\nQ(xi)K(xj)T\npdk\nif i;j \u0014g(t)\n\u00001 otherwise\n(5)\n\u000b(t)\nij =\n(\nexp e(t)\nij\nPn\nl=1 exp e(t)\nil\nif i;j \u0014g(t)\n0 otherwise\n(6)\nThe hidden state of ith source token at decoding step t is\ncalculated as:\nz(t)\ni =\nnX\nj=1\n\u000b(t)\nij V (xj) (7)\nThe new hidden states is z(T) 2Rn\u0002dz\u0002T , where T repre-\nsents the total number of decoding steps. Since the source\ntoken that read in changed at different decoding step, the\nhidden states sequence zt at each step needs to be recalcu-\nlated. The per-layer complexity of self-attention in wait-k\npolicy is up to O(n3 \u0001d), which greatly increase by ntimes\ncompared with full-sentence NMT.\nThe Proposed Method\nOur method is based on wait-k policy and consists of\ntwo components: incremental Transformer and conventional\nTransformer (full-sentence NMT). The architecture of the\nproposed method is shown in Figure 1. Conventional Trans-\nformer is a standard Transformer (Vaswani et al. 2017), used\nas the teacher of incremental Transformer for knowledge\ndistillation. Incremental Transformer is the proposed struc-\nture for ST, and the architecture of the incremental Trans-\nformer is shown in Figure 2.\n14429\nFigure 2: The architecture of the proposed incremental\nTransformer. The rightmost column represents the last layer\nof the decoder, including the average embedding layer\nIncremental Transformer contains a unidirectional en-\ncoder (left-to-right) and a decoder with Average Embedding\nLayer (AEL). To avoid the recalculation of the source hidden\nstates, we applied a unidirectional encoder, in which each to-\nken can only pay attention to the previous tokens. To estab-\nlish the attention to the later tokens in the consumed source,\nan average embedding layer is added to the last layer of de-\ncoder, compensating for the lack of attention. The model can\nattend all consumed source through unidirectional encoder\nand AEL, without much more complexity. Speciﬁc details\nare introduced following.\nIncremental Transformer\nUnidirectional Encoder Since the wait-k poliy with the\nbidirectional encoder take a high training complexity caused\nby recalculation, we apply a unidirectional encoder (left-to-\nright), where each source token can only focus on its pre-\nvious tokens. The self-attention in unidirectional encoder is\ncalculated as:\neij =\n(\nQ(xi)K(xj)T\npdk\nif j \u0014i\u0014g(t)\n\u00001 otherwise\n(8)\n\u000bij =\n\u001a exp eijPn\nl=1 exp eil\nif j \u0014i\u0014g(t)\n0 otherwise (9)\nDue to the characteristics of wait-k policy: g(t) =\nmin fk+ t\u00001;jxjg, g(t) changes linearly over the decod-\ning step t. The calculation of \u000bij can be decomposed into\na unidirectional attention among all source tokens, and then\nmask out the part outside the g(t) through a mask matrix.\nFigure 3: The architecture of average embedding layer. For\nclarity, we show an example with only four tokens (n = 4)\nand wait-2 policy (k= 2).\nDecoder with AEL The unidirectional encoder only need\nto calculate the representation of the new source token,\navoiding the complicated recalculation. But obviously, the\nprice is that the front token lacks some attention to its later\ntokens. To make up for this, we propose an average embed-\nding layer to summarize the information of all consumed\nsources. Since applying AEL in more decoder layers will\ngradually increase computational complexity, we only add\nAEL into the last layer of the decoder after trade-off between\nthe computational complexity and translation quality.\nAs shown in Figure 3, through AEL, the average embed-\nding of all consumed source is added into the unidirectional\nhidden states to focus on the later tokens. The inputs of the\naverage embedding layer are hidden statesz = (z1;\u0001\u0001\u0001 ;zn)\nand input embedding E = (E1;\u0001\u0001\u0001 ;En). First, AEL per-\nforms an average operation on the input embedding:\nAi = 1\ni\niX\nj=1\nEj (10)\nwhere Ai 2Rdmodel is the average embedding of the ﬁrst i\ntokens. Since the average is not a complicated calculation,\nwe can use the mask matrix to parallelize average operation.\nTo map A from the embedding space to the hidden states\nspace, we applied a linear layer to get f:\nfi = WAi (11)\nwhere W 2 Rdmodel\u0002dmodel is a trainable parameter ma-\ntrix, and fi represents the average information of the ﬁrst i\ntokens. Then, f is added to the hidden states of the tokens\nhave been read in:\nhij =\n\u001a\nfi + zj j \u0014i\n0 otherwise (12)\nwhere hij represents the new hidden state of the jth token\nwhen reading the ﬁrst i source tokens. Through AEL, the\nincremental hidden states is h 2Rn\u0002n\u0002dmodel.\nThrough unidirectional encoder and AEL, the incremen-\ntal hidden states include the information of both previous\ntokens and later tokens. In the subsequent cross-attention, at\nthe decoding step t, the decoder does multi-head attention\nwith the slice hg(t) in the incremental hidden state, where\ng(t) the number of source tokens read in at t.\n14430\nKnowledge Distillation\nThe most critical issue for ST is to achieve both high transla-\ntion quality and low latency. With a guaranteed low latency,\nour method enables the model to predict future implicitly\nand capture some future source information that helps to de-\ntermine sentence structure and translate.\nAs shown in the Figure 1, We introduced a conventional\nTransformer as the teacher of the incremental Transformer,\nand shorten the distance between the hidden states of them.\nDuring training, the incremental Transformer encodes the\nincremental source, while the conventional Transformer can\nencode the complete source. Through knowledge distilla-\ntion, conventional Transformer can teach the incremental\nTransformer to encode some future source information. For\nbetter distillation effect, we apply L2 regularization term\nbetween the hidden states of them, where is closer to the\nsource. The L2 regularization term is calculated as:\nL\n\u0000\nzincr;zfull \u0001\n= 1\nn\nnX\ni=1\n\r\r\rzincr\ni \u0000zfull\ni\n\r\r\r\n2\n(13)\nwhere zincr and zfull represent the hidden states of incre-\nmental Transformer and conventional Transformer, respec-\ntively.\nBoth incremental Transformer and conventional Trans-\nformer are trained with cross-entropy loss. The cross-\nentropy losses of incremental Transformer L(\u0012incr) and\nconventional Transformer L(\u0012full ) on train data D are re-\nspectively expressed as:\nL(\u0012incr) =\u0000\nX\n(x;y?)2D\nlog pincr (y? j(x;\u0012incr)) (14)\nL(\u0012full ) =\u0000\nX\n(x;y?)2D\nlog pfull (y? j(x;\u0012full )) (15)\nThen, the total loss Lis calculated as:\nL= L(\u0012incr) +L(\u0012full ) +\u0015L\n\u0000\nzincr;zfull \u0001\n(16)\nwhere \u0015is an hyper-parameter controlling the importance of\nthe penalty term, we set\u0015= 0:1 in our experiments. We con-\nducted experiment to compare the performance between pre-\ntraining a ﬁxed conventional Transformer and jointly train-\ning the incremental Transformer and conventional Trans-\nformer in Table 1, and ﬁnally apply jointly training them.\nExperiments\nDatasets\nWe conducted experiments on Chinese !English and Ger-\nman !English datasets.\nChinese !English (Zh-En) The training set consists\nof about 1.25M sentence pairs from LDC corpora 1. We\nuse MT02 as the validation set and MT03, MT04, MT05,\nMT06, MT08 as the test sets, each with 4 English references.\nWe ﬁrst tokenize and lowercase English sentences with the\n1The corpora include LDC2002E18, LDC2003E07,\nLDC2003E14, Hansards portion of LDC2004T07, LDC2004T08\nand LDC2005T06.\nTeacher Student\nBLEU AL BLEU\nk= 9 Pre-training 45.13 9.81 40.57\nJoint training 44.91 9.63 41.86\nk= 7 Pre-training 45.13 7.81 39.71\nJoint training 44.88 8.11 40.73\nk= 5 Pre-training 45.13 6.50 38.39\nJoint training 44.84 6.26 40.00\nk= 3 Pre-training 45.13 4.62 37.00\nJoint training 44.62 4.43 38.28\nk= 1 Pre-training 45.13 2.34 32.11\nJoint training 44.58 2.32 34.20\nTable 1: Comparison between pre-training a ﬁxed conven-\ntional Transformer and jointly training incremental Trans-\nformer (Student) and conventional Transformer (Teacher),\ntesting on Zh-En validation set. We show the performance\nof the ﬁnal teacher model and student model. Note that the\nteacher model is evaluated on full-sentence NMT.\nMoses2, and segmente the Chinese sentences with the Stan-\nford Segmentor 3. We apply BPE (Sennrich, Haddow, and\nBirch 2016) with 30K merge operations on all texts.\nGerman !English (De-En) The training set consists of\nabout 4.5M sentence pairs from WMT15 4 De-En task. We\nuse news-test2013(3000 sentence pairs) as the validation set\nand news-test2015(2169 sentence pairs) as the test set. We\napply BPE with 32K merge operations, and the vocabulary\nis shared across languages.\nSystems Setting\nWe conducted experiments on the following systems:\nbi-Transformer: ofﬂine model. Full-sentence NMT\nbased on Transformer with bidirectional encoder.\nuni-Transformer: ofﬂine model. Full-sentence NMT\nbased on Transformer with unidirectional encoder.\nbaseline(bi): wait-k policy based on Transformer with\nbidirectional encoder (Ma et al. 2019).\nbaseline(uni): wait-k policy based on Transformer with\nunidirectional encoder.\n+Teacher: only add a conventional Transformer as the\nteacher model based on Transformer with unidirectional en-\ncoder. The encoder of teacher model is bidirectional.\n+AEL: only add average embedding layer we proposed\nbased on Transformer with unidirectional encoder.\n+AEL+Teacher: add both AEL and teacher model based\non Transformer with unidirectional encoder.\nThe implementation of our method is adapted from\nFairseq Library (Ott et al. 2019). The parameters of the in-\ncremental Transformer we proposed are exactly the same\nas the standard wait-k (Ma et al. 2019), while the conven-\ntional Transformer is the same as the original Transformer\n(Vaswani et al. 2017).\n2http://www.statmt.org/moses/\n3https://nlp.stanford.edu/\n4http://www.statmt.org/wmt15/translation-\ntask.html\n14431\nMT03 MT04 MT05 MT06 MT08 A VERAGE \u0001 Training Time\n(secs/b)BLEU AL BLEU\nofﬂine bi-transformer 44.56 45.69 45.28 44.63 34.51 28.83 42.93 0.31\nuni-transformer 43.22 44.40 43.12 42.31 32.51 28.82 41.11 0.31\nk= 9\nbaseline(bi) 40.35 42.21 40.21 40.78 32.45 9.99 39.20 9.92\nbaseline(uni) 39.42 42.08 40.33 40.12 31.59 9.99 38.71 0.31\n+AEL 40.77 42.27 40.11 40.77 32.17 10.09 39.22 +0.51 0.41\n+Teacher 41.52 43.05 41.75 41.59 33.12 9.74 40.21 +0.99 0.78\n+AEL+Teacher 41.75 43.03 41.63 41.76 33.06 9.73 40.25 +1.54 0.80\nk= 7\nbaseline(bi) 40.27 41.94 39.90 40.35 31.84 8.05 38.86 10.26\nbaseline(uni) 38.79 41.12 38.77 39.13 30.61 8.01 37.68 0.31\n+AEL 39.81 41.66 38.81 40.14 31.16 8.17 38.32 +0.63 0.41\n+Teacher 40.51 41.81 40.35 40.90 32.16 8.31 39.15 +1.46 0.79\n+AEL+Teacher 40.41 42.08 40.29 40.44 32.94 8.10 39.23 +1.55 0.81\nk= 5\nbaseline(bi) 40.12 41.46 39.58 40.21 31.57 6.34 38.59 10.70\nbaseline(uni) 37.09 39.62 37.78 37.66 29.82 6.27 36.39 0.31\n+AEL 38.74 40.11 38.36 39.04 30.30 6.06 37.31 +0.92 0.41\n+Teacher 39.47 40.42 38.82 39.78 30.05 6.24 37.71 +1.31 0.82\n+AEL+Teacher 40.15 41.53 39.58 40.59 31.29 5.98 38.63 +2.23 0.83\nk= 3\nbaseline(bi) 37.08 39.11 36.69 37.20 28.28 4.15 35.67 11.11\nbaseline(uni) 35.94 36.98 34.64 34.80 26.48 4.42 33.77 0.31\n+AEL 37.40 38.72 36.64 36.59 28.06 4.11 35.48 +1.71 0.41\n+Teacher 37.42 38.94 37.13 37.37 29.58 4.53 36.09 +2.32 0.84\n+AEL+Teacher 38.15 38.88 37.14 37.46 28.98 4.41 36.12 +2.35 0.86\nk= 1\nbaseline(bi) 32.67 34.51 32.55 32.04 24.79 2.45 31.31 15.11\nbaseline(uni) 31.99 33.75 31.47 31.56 23.86 2.71 30.53 0.31\n+AEL 32.97 34.41 32.37 32.04 24.16 2.29 31.19 +0.66 0.41\n+Teacher 33.95 34.51 33.07 33.17 25.14 2.35 31.97 +1.44 0.84\n+AEL+Teacher 34.21 35.10 33.11 33.72 25.19 2.37 32.27 +1.74 0.86\nTable 2: Translation quality (4-gram BLEU), latency (AL), and training speed (seconds/batch) on Zh-En simultaneous transla-\ntion. Since our proposed method and baseline belong to the ﬁxed policy, there is almost no difference in latency. Therefore, we\ndisplay the results in the form of table to highlight the details of the improvement in translation quality and training speed.\nComparison between Joint Training and\nPre-training\nBefore the main experiment, we compared the performance\nof ‘+Teacher’ between pre-training a ﬁxed conventional\nTransformer or jointly training incremental Transformer and\nconventional Transformer on Zh-En validation set.\nAs shown in Table 1, jointly training makes the model\nget better performance than pre-training. The reason is that\nthe teacher model is for full-sentence MT, while the student\nmodel is for ST, and the two have inherent differences in the\nhidden states distribution. Since the decoding policy is incre-\nmental at the inference time, we should not let the incremen-\ntal Transformer learn from the conventional Transformer\nwithout any difference, but narrow the distance between\nthem, helping the student model maintain the characteris-\ntics of incremental decoding. Similarly, (Dalvi et al. 2018;\nMa et al. 2019) pointed out that if the full-sentence NMT\nmodel is directly used for ST, the translation quality will be\nsigniﬁcantly reduced. Besides, with joint-training, the per-\nformance of the ﬁnal teacher model will not be greatly af-\nfected, which can still guide the student model. Therefore,\nwe jointly train the incremental Transformer and conven-\ntional Transformer with the loss in Eq.(16).\nComparison with Baseline\nWe set standard wait-k policy as the baseline and compare\nwith it. For evaluation metric, we use BLEU (Papineni et al.\n2002) and AL5 (Ma et al. 2019) to measure translation qual-\nity and latency, respectively. Table 2 reports translation qual-\nity (BLEU), latency (AL) and training time of our method,\nbaseline and ofﬂine model on Zh-En simultaneous transla-\ntion, and ‘A VERAGE’ is average on all test sets. Table 3\nreports the result on De-En simultaneous translation.\nWe ﬁrst notice that the training speed of the baseline(bi)\nis too slow, where the training time of each batch is about\n36.84 times (average on differentk) that of the ofﬂine model.\nAs kdecreases, the training time will gradually increase, un-\ntil k = 1, the training time even increase by 48.74 times.\nWhen k is smaller, the number of tokens waiting at the\nbeginning is less, and the number of recalculation of en-\ncoder hidden states increases rapidly. After adopting AEL in\nTransformer with unidirectional encoder, our method avoids\nthe recalculation of encoder hidden states and also makes up\nfor the lack of attention of the unidirectional encoder. The\ntraining speed of ‘+AEL’ is about 27.86 times (average on\n5The calculation of AL is as https://github.com/\nSimulTrans-demo/STACL.\n14432\nAL BLEU \u0001\nofﬂine bi-Transformer 28.60 31.42\nuni-Transformer 28.70 30.12\nk= 9\nbaseline(bi) 9.36 28.48\nbaseline(uni) 9.24 28.10\n+AEL+Teacher 9.25 29.42 +1.32\nk= 7\nbaseline(bi) 7.44 28.09\nbaseline(uni) 7.83 27.84\n+AEL+Teacher 7.90 28.38 +0.54\nk= 5\nbaseline(bi) 5.58 26.38\nbaseline(uni) 5.78 25.73\n+AEL+Teacher 5.74 26.97 +1.24\nk= 3\nbaseline(bi) 3.48 24.18\nbaseline(uni) 3.91 24.04\n+AEL+Teacher 3.95 24.39 +0.35\nk= 1\nbaseline(bi) 1.60 18.48\nbaseline(uni) 1.32 18.29\n+AEL+Teacher 1.31 19.36 +1.07\nTable 3: Translation quality (BLEU) and latency (AL) on\nDe-En simultaneous translation.\ndifferent k) faster than that of baseline(bi), while the trans-\nlation quality is equivalent to that of baseline(bi).\nAfter adding the conventional Transformer to guide incre-\nmental Transformer, ‘+Teacher’ improved about 1.5 BLEU\n(average on different k) over the baseline(uni). Note that in\nthe case of low latency (smallerk), our method improves es-\npecially. When k is very small, the model waits for a very\nfew tokens, so that the prediction of the future is more im-\nportant at a low latency. In general, after applying AEL and\nTeacher model, the training speed of ‘+AEL+Teacher’ is in-\ncreased by about 13.67 times, and translation quality im-\nproves about 1.88 BLEU on Zh-En and 0.91 BLEU on De-\nEn (average on different k).\nFor the case of different waiting time kbetween training\nand testing, (Ma et al. 2019) pointed out that the best re-\nsults when testing with wait-jpolicy are often from a model\ntrained with a larger wait-i policy (where i > j), which\nshows that the model trained with more source information\nperforms better. Table 4 shows the results of the proposed\nmethod using wait-i policy during training and testing with\nwait-j policy. The best results are basically obtained when\nj = i, since future-guided methods inspires incremental\nTransformer learn implicit future information. It is worth\nmentioning that the best result for wait-1 testing still comes\nfrom wait-7 training model. We presume the reason is that\nalthough wait-1 model learns some future information, the\ndelay of one token still contains too little information.\nImpact of the Knowledge Distillation\nOur method applies knowledge distillation with a L2 reg-\nularization term. We reduce the dimension of the hidden\nstates with t-Distributed Stochastic Neighbor Embedding (t-\nSNE) technique, and show the distribution in Figure 4. With\nthe L2 regularization term, the hidden states are fused with\neach other, which shows the source information extracted\nby incremental Transformer and conventional Transformer\nTrain k\nTest k 1 3 5 7 9\n1 32.27 35.07 35.95 36.17 35.77\n3 32.65 36.12 38.05 38.70 39.99\n5 31.95 35.35 38.63 38.62 39.28\n7 32.74 36.04 38.37 39.23 39.64\n9 31.91 35.49 37.91 38.99 40.25\nTable 4: Results(average BLEU over all Zh-En test sets) of\nproposed method ‘+AEL+Teacher’ using wait-ipolicy dur-\ning training and wait-jpolicy during testing.\n(a) without L2 regularization\n (b) with L2 regularization\nFigure 4: The distribution of the hidden states of incremen-\ntal Transformer and conventional Transformer on the Zh-En\nvalidation set. Red stars represents the hidden states of the\nincremental Transformer, while the blue dots represents the\nhidden states of the conventional Transformer.\nis more closer. Therefore, L2 regularization term success-\nfully makes incremental Transformer learn some future in-\nformation from conventional Transformer.\nBesides, to ensure that most of the improvement brought\nby ‘+Teacher’ comes from the knowledge distillation be-\ntween the full-sentence / incremental encoder, not due to the\nknowledge distillation between bidirectional / unidirectional\nencoder, we report the results of using teacher model with\nunidirectional / bidirectional encoder in Table 6. When using\nTransformer with unidirectional encoder as the teacher, our\nmethod can be improved about 1.5 BLEU. When the unidi-\nrectional encoder was replaced by the bidirectional encoder,\nthe translation quality was only slightly further improved\nabout 0.2 BLEU. When both the teacher model and the\nstudent model use unidirectional encoder, the improvement\nbrought by knowledge distillation is still obvious, which\nshows that most of the improvement brought by our pro-\nposed method ‘+Teacher’ comes from the knowledge distil-\nlation between the conventional Transformer (full-sentence)\nand the incremental Transformer.\nPrediction Accuracy\nTo verify that our method implicitly embeds some future in-\nformation through knowledge distillation, we tested the to-\nken prediction accuracy of ‘+Teacher’ and baseline(bi) on\nZh-En validation set. We ﬁrst use GIZA++ 6 to align the to-\nkens between the generated translation and the source sen-\n6https://github.com/moses-ST/giza-pp.git\n14433\nk= 1 k= 3 k= 5 k= 7 k= 9\nbaseline +Teacher baseline +Teacher baseline +Teacher baseline +Teacher baseline +Teacher\nAbsent 54.88 59.82 61.34 63.26 63.54 65.38 70.72 71.80 70.48 71.57\nPresent 82.47 83.32 84.76 85.22 85.33 86.04 85.94 86.51 86.25 86.92\nTable 5: 1-gram score of baseline and ‘+Teacher’ on Absent. set and Present. set, respectively. ‘+Teacher’ indicates apply-\ning a conventional Transformer for future-guidance. ‘Absent’ represents the aligned source token has not been read in when\ngenerating the target token, ‘Present’ represents the aligned source token has been read in when generating the target token.\nA VG BLEU \u0001\nk= 9\nbaseline(uni) 38.71\n+uni-Teacher 39.72 +1.01\n+bi-Teacher 40.21 +1.50\nk= 7\nbaseline(uni) 37.68\n+uni-Teacher 38.95 +1.27\n+bi-Teacher 39.15 +1.46\nk= 5\nbaseline(uni) 36.39\n+uni-Teacher 37.50 +1.11\n+bi-Teacher 37.71 +1.32\nk= 3\nbaseline(uni) 33.77\n+uni-Teacher 36.02 +2.25\n+bi-Teacher 36.09 +2.32\nk= 1\nbaseline(uni) 30.53\n+uni-Teacher 32.08 +1.55\n+bi-Teacher 31.97 +1.44\nTable 6: Comparison between the teacher model using uni-\ndirectional / bidirectional encoder, test on Zh-En test set.\n‘+uni-Teacher’ indicates using unidirectional encoder, while\n‘+bi-Teacher’ indicates using bidirectional encoder.\ntence. As a result, the ith target token is aligned with the\njth source token. All the generated target tokens are divided\ninto two sets:Present and Absent. If j \u0014min (i+ k\u00001;n),\nthe aligned source token of the ith generated token has\nbeen read when generating, thus the generated token be-\nlongs to Present set. In contrast, if j >min (i+ k\u00001;n),\nthe aligned source token has not been read when generat-\ning, thus the generated token belongs to Absent set, i.e., the\ngenerated target token is implicitly predicted by the model.\nFinally, the 1-gram score is calculated on both sets.\nThe results are shown in Table 5. After applying future-\nguidance with the teacher model, the token prediction accu-\nracy improves. Our method improves more obviously when\nkis smaller, since the small kgreatly limits the information\nthat the model can read in. When k is small, the predictive\nability from data-driven becomes unreliable, and it is espe-\ncially important to explicitly introduce the future-guidance.\nIn addition, the accuracy on the Present. set does not de-\ncrease and improves slightly.\nRelated Work\nThe current research of ST is mainly divided into: precise\nread / write policy and stronger predictive ability.\nFor read / write policy, earlier methods were based on\nsegmented translation (Bangalore et al. 2012; Cho and Es-\nipova 2016; Siahbani et al. 2018). Gu et al. (2017) used re-\ninforcement learning to train an agent to decide read / write.\nRecently, Dalvi et al. (2018) proposed STATIC-RW, ﬁrst\nperforming S’s READs, then alternately performing RW’s\nWRITEs and READs. Ma et al. (2019) proposed a wait-k\npolicy, wherein begin synchronizing output after reading k\ntokens. Zheng et al. (2019a) trained an agent by the input\nsentences and gold read / write sequence generated by rules.\nZheng et al. (2019b) introduces a “delay” token f\"ginto\nthe target vocabulary, and introduced limited dynamic pre-\ndiction. Arivazhagan et al. (2019) proposed MILK, which\nuses a variable based on Bernoulli distribution to determine\nwhether to output. Ma et al. (2020) proposed MMA, the im-\nplementation of MILK based on Transformer.\nMost of the previous methods use the unidirectional en-\ncoder (Arivazhagan et al. 2019; Ma et al. 2020) or fune-\ntuning a trained model (Dalvi et al. 2018) to reduce the\ncomputational cost. We proposed AEL to compensate for\nthe lack of attention caused by unidirectional encoder.\nFor predicting future, Matsubara et al. (2000) applied pat-\ntern recognition to predict verbs in advance. Grissom II et al.\n(2014) used a Markov chain to predict the next word and\nﬁnal verb to eliminate delay bottlenecks between different\nword orders. (Oda et al. 2015) predict unseen syntactic con-\nstituents to help generate complete parse trees and perform\nsyntax-based simultaneous translation. Alinejad, Siahbani,\nand Sarkar (2018) added a Predict operation to the agent\nbased on Gu et al. (2017), predicting the next word as an ad-\nditional input. However, most of previous methods predict\na speciﬁc word through a language model, while directly\npredicting speciﬁc words is prone to large errors which will\ncause mistakes in subsequent translations. Unlike the previ-\nous method, our method attempt to implicitly embed some\nfuture information in the model through future-guidance,\navoiding the impact of inaccurate predictions.\nConclusion\nIn order to accelerate the training speed of the wait-k pol-\nicy and use future information to guide the training, we\npropose future-guided incremental Transformer for simulta-\nneous translation. With incremental Transformer and AEL,\nour method greatly accelerates the training speed about 28\ntimes, meanwhile attends to all consumed source tokens.\nWith future-guided training, the incremental Transformer\nsuccessfully embeds some implicit future information and\nhas a stronger predictive ability, without adding any latency\nor parameters in the inference time. Experiments show the\nproposed method outperform the baseline and achieve better\nperformance on both training speed and translation quality.\n14434\nAcknowledgements\nWe thank all the anonymous reviewers for their insightful\nand valuable comments. This work was supported by Na-\ntional Key R&D Program of China (NO. 2018YFC0825201\nand NO. 2017YFE0192900 ).\nReferences\nAlinejad, A.; Siahbani, M.; and Sarkar, A. 2018. Predic-\ntion Improves Simultaneous Neural Machine Translation. In\nProceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, 3022–3027. Brus-\nsels, Belgium: Association for Computational Linguistics.\ndoi:10.18653/v1/D18-1337. URL https://www.aclweb.\norg/anthology/D18-1337.\nArivazhagan, N.; Cherry, C.; Macherey, W.; Chiu, C.-c.;\nYavuz, S.; Pang, R.; Li, W.; and Raffel, C. 2019. Mono-\ntonic Inﬁnite Lookback Attention for Simultaneous Machine\nTranslation. 1313–1323. doi:10.18653/v1/p19-1126.\nBangalore, S.; Rangarajan Sridhar, V . K.; Kolan, P.;\nGolipour, L.; and Jimenez, A. 2012. Real-time Incremen-\ntal Speech-to-Speech Translation of Dialogs. In Proceed-\nings of the 2012 Conference of the North American Chap-\nter of the Association for Computational Linguistics: Hu-\nman Language Technologies, 437–445. Montr ´eal, Canada:\nAssociation for Computational Linguistics. URL https:\n//www.aclweb.org/anthology/N12-1048.\nChen, Y .; Li, L.; Jiang, X.; Chen, X.; and Liu, Q. 2020.\nA General Framework for Adaptation of Neural Machine\nTranslation to Simultaneous Translation. In Proceedings of\nthe 1st Conference of the Asia-Paciﬁc Chapter of the As-\nsociation for Computational Linguistics and the 10th In-\nternational Joint Conference on Natural Language Pro-\ncessing, 191–200. Suzhou, China: Association for Compu-\ntational Linguistics. URL https://www.aclweb.org/\nanthology/2020.aacl-main.23.\nCho, K.; and Esipova, M. 2016. Can neural machine trans-\nlation do simultaneous translation? URL http://arxiv.\norg/abs/1606.02012.\nDalvi, F.; Durrani, N.; Sajjad, H.; and V ogel, S. 2018. In-\ncremental Decoding and Training Methods for Simultane-\nous Translation in Neural Machine Translation. In Pro-\nceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 2 (Short Papers) ,\n493–499. New Orleans, Louisiana: Association for Com-\nputational Linguistics. doi:10.18653/v1/N18-2079. URL\nhttps://www.aclweb.org/anthology/N18-2079.\nGrissom II, A.; He, H.; Boyd-Graber, J.; Morgan, J.; and\nDaum´e III, H. 2014. Don’t Until the Final Verb Wait: Rein-\nforcement Learning for Simultaneous Machine Translation.\nIn Proceedings of the 2014 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP), 1342–1352.\nDoha, Qatar: Association for Computational Linguistics.\ndoi:10.3115/v1/D14-1140. URL https://www.aclweb.\norg/anthology/D14-1140.\nGu, J.; Neubig, G.; Cho, K.; and Li, V . O. 2017. Learn-\ning to Translate in Real-time with Neural Machine Trans-\nlation. In Proceedings of the 15th Conference of the Euro-\npean Chapter of the Association for Computational Linguis-\ntics: Volume 1, Long Papers, 1053–1062. Valencia, Spain:\nAssociation for Computational Linguistics. URL https:\n//www.aclweb.org/anthology/E17-1099.\nHinton, G.; Vinyals, O.; and Dean, J. 2015. Distilling the\nKnowledge in a Neural Network.\nMa, M.; Huang, L.; Xiong, H.; Zheng, R.; Liu, K.; Zheng,\nB.; Zhang, C.; He, Z.; Liu, H.; Li, X.; Wu, H.; and Wang, H.\n2019. STACL: Simultaneous Translation with Implicit An-\nticipation and Controllable Latency using Preﬁx-to-Preﬁx\nFramework. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, 3025–3036.\nFlorence, Italy: Association for Computational Linguistics.\ndoi:10.18653/v1/P19-1289. URL https://www.aclweb.\norg/anthology/P19-1289.\nMa, X.; Pino, J. M.; Cross, J.; Puzon, L.; and Gu, J.\n2020. Monotonic Multihead Attention. In International\nConference on Learning Representations. URL https:\n//openreview.net/forum?id=Hyg96gBKPS.\nMatsubara, Shigeki Iwashima, K.; Kawaguchi, N.; Toyama,\nK.; and Inagaki, Y . 2000. Simultaneous Japenese-English\nInterpretation Based on Early Predictoin of English Verb. In\nProceedings of the 4th Symposium on Natural Languauge\nProcessing(SNLP-2000), 268–273.\nNovitasari, S.; Tjandra, A.; Sakti, S.; and Nakamura, S.\n2019. Sequence-to-Sequence Learning via Attention Trans-\nfer for Incremental Speech Recognition 3835–3839.\nOda, Y .; Neubig, G.; Sakti, S.; Toda, T.; and Nakamura, S.\n2015. Syntax-based Simultaneous Translation through Pre-\ndiction of Unseen Syntactic Constituents. In Proceedings\nof the 53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 1: Long Pa-\npers), 198–207. Beijing, China: Association for Computa-\ntional Linguistics. doi:10.3115/v1/P15-1020. URL https:\n//www.aclweb.org/anthology/P15-1020.\nOtt, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.;\nGrangier, D.; and Auli, M. 2019. fairseq: A Fast, Extensible\nToolkit for Sequence Modeling. In Proceedings of the 2019\nConference of the North American Chapter of the Associ-\nation for Computational Linguistics (Demonstrations), 48–\n53. Minneapolis, Minnesota: Association for Computational\nLinguistics. doi:10.18653/v1/N19-4009. URL https:\n//www.aclweb.org/anthology/N19-4009.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a Method for Automatic Evaluation of Machine Trans-\nlation. In Proceedings of the 40th Annual Meeting of the As-\nsociation for Computational Linguistics, 311–318. Philadel-\nphia, Pennsylvania, USA: Association for Computational\nLinguistics. doi:10.3115/1073083.1073135. URL https:\n//www.aclweb.org/anthology/P02-1040.\nRavanelli, M.; Serdyuk, D.; and Bengio, Y . 2018. Twin Reg-\nularization for online speech recognition.Proceedings of the\n14435\nAnnual Conference of the International Speech Communi-\ncation Association, INTERSPEECH 2018-Septe(1): 3718–\n3722. ISSN 19909772. doi:10.21437/Interspeech.2018-\n1407.\nSennrich, R.; Haddow, B.; and Birch, A. 2016. Neural\nMachine Translation of Rare Words with Subword Units.\nIn Proceedings of the 54th Annual Meeting of the Associ-\nation for Computational Linguistics (Volume 1: Long Pa-\npers), 1715–1725. Berlin, Germany: Association for Com-\nputational Linguistics. doi:10.18653/v1/P16-1162. URL\nhttps://www.aclweb.org/anthology/P16-1162.\nSiahbani, M.; Shavarani, H.; Alinejad, A.; and Sarkar, A.\n2018. Simultaneous Translation using Optimized Segmen-\ntation. In Proceedings of the 13th Conference of the As-\nsociation for Machine Translation in the Americas (Volume\n1: Research Papers), 154–167. Boston, MA: Association\nfor Machine Translation in the Americas. URL https:\n//www.aclweb.org/anthology/W18-1815.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017.\nAttention is All you Need. In Guyon, I.; Luxburg,\nU. V .; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan,\nS.; and Garnett, R., eds., Advances in Neural Informa-\ntion Processing Systems 30, 5998–6008. Curran Asso-\nciates, Inc. URL http://papers.nips.cc/paper/\n7181-attention-is-all-you-need.pdf.\nZhang, B.; Xiong, D.; and Su, J. 2018. Accelerating Neu-\nral Transformer via an Average Attention Network. In Pro-\nceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) ,\n1789–1798. Melbourne, Australia: Association for Com-\nputational Linguistics. doi:10.18653/v1/P18-1166. URL\nhttps://www.aclweb.org/anthology/P18-1166.\nZheng, B.; Zheng, R.; Ma, M.; and Huang, L. 2019a. Sim-\npler and Faster Learning of Adaptive Policies for Simul-\ntaneous Translation. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), 1349–1354. Hong\nKong, China: Association for Computational Linguistics.\ndoi:10.18653/v1/D19-1137. URL https://www.aclweb.\norg/anthology/D19-1137.\nZheng, B.; Zheng, R.; Ma, M.; and Huang, L. 2019b. Si-\nmultaneous Translation with Flexible Policy via Restricted\nImitation Learning. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics, 5816–\n5822. Florence, Italy: Association for Computational Lin-\nguistics. doi:10.18653/v1/P19-1582. URL https://www.\naclweb.org/anthology/P19-1582.\n14436",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8352805376052856
    },
    {
      "name": "Computer science",
      "score": 0.7330977916717529
    },
    {
      "name": "Embedding",
      "score": 0.6079407334327698
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.595223069190979
    },
    {
      "name": "Speedup",
      "score": 0.4952832758426666
    },
    {
      "name": "Speech recognition",
      "score": 0.4529501497745514
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4440060257911682
    },
    {
      "name": "Natural language processing",
      "score": 0.4228411316871643
    },
    {
      "name": "Machine learning",
      "score": 0.33118101954460144
    },
    {
      "name": "Voltage",
      "score": 0.2018170952796936
    },
    {
      "name": "Engineering",
      "score": 0.10732695460319519
    },
    {
      "name": "Electrical engineering",
      "score": 0.09680509567260742
    },
    {
      "name": "Parallel computing",
      "score": 0.07597625255584717
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    }
  ]
}