{
  "title": "The Web Can Be Your Oyster for Improving Language Models",
  "url": "https://openalex.org/W4385571655",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2129999472",
      "name": "Junyi Li",
      "affiliations": [
        "University of Monterrey",
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2133263866",
      "name": "Tianyi Tang",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2307999729",
      "name": "Wayne Xin Zhao",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research"
      ]
    },
    {
      "id": "https://openalex.org/A2104795587",
      "name": "Jingyuan Wang",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2777189136",
      "name": "Jian-Yun Nie",
      "affiliations": [
        "University of Monterrey"
      ]
    },
    {
      "id": "https://openalex.org/A3212238123",
      "name": "Ji-Rong Wen",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3169283738",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W4288243162",
    "https://openalex.org/W3104942706",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4226408727",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4288401178",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W4221141423",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2898695519",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W4288725442",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3104748221",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4389519118",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W4221164017",
    "https://openalex.org/W3015322406",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W3186138538",
    "https://openalex.org/W2962881743",
    "https://openalex.org/W3205626500",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W3034337319",
    "https://openalex.org/W3118999024",
    "https://openalex.org/W4288090189",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W4309416490",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W4288262459",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W3174269049",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4225933709",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4226265091",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4310418667",
    "https://openalex.org/W3102999298",
    "https://openalex.org/W4385570792",
    "https://openalex.org/W2131965512",
    "https://openalex.org/W3175475697",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4285429195"
  ],
  "abstract": "Pretrained language models (PLMs) encode a large amount of world knowledge. However, as such knowledge is frozen at the time of model training, the models become static and limited by the training data at that time. In order to further improve the capacity of PLMs for knowledge-intensive tasks, we consider augmenting PLMs with the large-scale web using search engine. Unlike previous augmentation sources (e.g., Wikipedia data dump), the web provides broader, more comprehensive and constantly updated information. In this paper, we present a web-augmented PLM ‚Äì UniWeb, which is trained over 16 knowledge-intensive tasks in a unified text-to-text format. Instead of simply using the retrieved contents from web, our approach has made two major improvements. Firstly, we propose an adaptive search engine assisted learning method that can self-evaluate the confidence level of PLM‚Äôs predictions, and adaptively determine when to refer to the web for more data, which can avoid useless or noisy augmentation from web. Secondly, we design a pretraining task, i.e., continual knowledge learning, based on salient spans prediction, to reduce the discrepancy between the encoded and retrieved knowledge. Experiments on a wide range of knowledge-intensive tasks show that our model significantly outperforms previous retrieval-augmented methods.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 728‚Äì746\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nThe Web Can Be Your Oyster for Improving Language Models\nJunyi Li1,3,5, Tianyi Tang1, Wayne Xin Zhao1,5‚àó\n, Jingyuan Wang4 ,\nJian-Yun Nie3 and Ji-Rong Wen1,2,5\n1Gaoling School of Artificial Intelligence, Renmin University of China\n2School of Information, Renmin University of China 3DIRO, Universit√© de Montr√©al\n4School of Computer Science and Engineering, Beihang University\n5Beijing Key Laboratory of Big Data Management and Analysis Methods\n{lijunyi,steven_tang}@ruc.edu.cn batmanfly@gmail.com\nAbstract\nPretrained language models (PLMs) encode a\nlarge amount of world knowledge. However, as\nsuch knowledge is frozen at the time of model\ntraining, the models become static and lim-\nited by the training data at that time. In order\nto further improve the capacity of PLMs for\nknowledge-intensive tasks, we consider aug-\nmenting PLMs with the large-scale web using\nsearch engine. Unlike previous augmentation\nsources (e.g., Wikipedia data dump), the web\nprovides broader, more comprehensive and con-\nstantly updated information. In this paper, we\npresent a web-augmented PLM ‚Äì UNIWEB,\nwhich is trained over 16 knowledge-intensive\ntasks in a unified text-to-text format. Instead\nof simply using the retrieved contents from\nweb, our approach has made two major im-\nprovements. Firstly, we propose an adaptive\nsearch engine assisted learning method that\ncan self-evaluate the confidence level of PLM‚Äôs\npredictions, and adaptively determine when to\nrefer to the web for more data, which can avoid\nuseless or noisy augmentation from web. Sec-\nondly, we design a pretraining task, i.e., contin-\nual knowledge learning, based on salient spans\nprediction, to reduce the discrepancy between\nthe encoded and retrieved knowledge. Experi-\nments on a wide range of knowledge-intensive\ntasks show that our model significantly outper-\nforms previous retrieval-augmented methods.\nOur code and data can be accessed at this link\nhttps://github.com/RUCAIBox/UniWeb\n1 Introduction\nWith large-scale neural networks, pretrained lan-\nguage models (PLMs) (Brown et al., 2020; Zhao\net al., 2023) can encode a large amount of world\nknowledge, showing phenomenal capability in\nknowledge-intensive tasks such as fact checking\nand open-domain question answering (QA). How-\never, this capacity is naturally limited by the in-\nformation contained in pretraining or finetuning\n‚àóCorresponding author\nQuestion Which popular Korean show was recently\ngreen lit for a new season?\nAnswer Squid Game\nWikipedia There are no results for the question.\nWeb [...] Netflix announce Sunday that the wildly\npopular South Korean show is green lit for a second season.\n‚ÄúAnd now, Gi-hun returns‚Äù ‚ÄúThe Front Man returns. Season\n2 is coming.‚Äù ‚ÄúSquid Game‚Äù is a fictional drama from South\nKorea in which contestants who are desperately in need of\nmoney play deadly children‚Äôs games to win cash prizes. [...]\nURL: https://www.cnn.com/2022/06/1\n2/media/squid-game-season-2/index.html\nT5 w/o Web The Walking Dead /enc-37\nT5 w/ Web Squid Game /enc-34\nTable 1: An example showing that the web covers both\nmore comprehensive (e.g., Korean show) and up-to-date\n(e.g., recently) information than Wikipedia. Based on\nthe latest news returned by Google Search, T5-LARGE\ncan answer the question correctly.\ndatasets (usually fixed once collected), which are\nneither up-to-date nor complete (Komeili et al.,\n2021; Ji et al., 2022). Although model scal-\ning (Brown et al., 2020; Chowdhery et al., 2022;\nThoppilan et al., 2022) is a viable way to improve\nthe knowledge capacity of PLMs, it still uses static\npretraining datasets, and also leads to significantly\nlarger computational costs with increased model\nsizes. As a result, the outdated or incomplete\nknowledge encoded by PLMs may lead to hallu-\ncination or incorrect generations even though the\nresults look plausible (Ji et al., 2022).\nRecently, by drawing the idea from semi-\nparametric approaches (Zhao et al., 2022; Guu\net al., 2020; Lewis et al., 2020b; Borgeaud et al.,\n2022), retrieval-augmented approaches have been\nproposed to equip PLMs with the ability to directly\naccess an external database. As a major knowledge\nresource, Wikipedia has been widely used in pre-\nvious work. While being highly accurate and well-\nstructured, Wikipedia only covers limited informa-\ntion, both in scope and in time. Besides, even for\n728\nthe topics that Wikipedia covers, grounding PLMs‚Äô\ndecisions on a single source of knowledge may\ncreate biases (Wagner et al., 2016). Considering\nthese issues, it is time to look beyond Wikipedia (or\nsimilar single-source databases) and access more\nbroader, in-depth, and up-to-date knowledge from\nmore sources. Inspired by (Komeili et al., 2021;\nPiktus et al., 2021), we select the web as the re-\ntrieval resource for enlarging the knowledge capac-\nity of PLMs. To motivate our approach, Table 1\npresents a sample question that T5 successfully an-\nswers with the support of the web (providing the\nlatest news), but not Wikipedia. As we can see,\ntimely and relevant supporting evidence is the key\nto solve such tasks for PLMs.\nIn this paper, we aim to capitalize on the web as a\nsource of up-to-date and comprehensive knowledge\nto solve a wide range of knowledge-intensive tasks.\nUnlike previous web-augmented studies (Nakano\net al., 2021; Menick et al., 2022) that mostly fo-\ncus on single tasks, we seek to develop a unified\nframework to integrate the use of the web in PLMs\nfor multi-task solving. Although the idea of lever-\naging the web for improving PLMs is appealing,\nit is non-trivial to develop an effective solution.\nFirst, PLMs do not always need external evidence\nfor task solving, especially considering the fact\nthat the web contains noisy, biased, or harmful in-\nformation (Luccioni and Viviano, 2021). Simply\nretrieving knowledge without considering the ex-\nample difficulty and PLMs‚Äô own capabilities may\nsteer models towards unexpected outputs. Second,\nPLMs are usually pretrained at an earlier time on a\nlimited corpus, leading to a discrepancy between\nthe encoded knowledge and the retrieved knowl-\nedge (i.e., web contents). Therefore, we need more\nprincipled approaches to properly integrating the\nnew knowledge into PLMs.\nTo address the above issues, we present a web-\naugmented PLMs, UNIWEB, to improve the ca-\npacity in knowledge-intensive tasks. Instead of\nusing neural network-based retriever, we employ\na commercial search engine (i.e., Google Search)\nto obtain high-quality and comprehensive retrieval\nresults from the web. Based on this idea, we make\ntwo major technical contributions. First, we pro-\npose a search engine assisted learning method that\ncan selectively query the web only when PLM is\nunconfident in its predictions. For this purpose,\nwe design a self-evaluation mechanism to estimate\nthe confidence level of PLMs on the task exam-\nples. Secondly, to reduce the discrepancy between\nthe encoded and retrieved knowledge, we design\na pretraining task, continual knowledge learning,\nto integrate the retrieved knowledge into PLMs by\npredicting the salient masked spans in web docu-\nments. To train the UNIWEB model, we convert\ndifferent knowledge-intensive tasks into a unified\ntext-to-text format, and conduct supervised multi-\ntask training over 16 tasks across seven categories.\nTo the best of our knowledge, our model is the\nfirst unified web-augmented PLM for a wide range\nof knowledge-intensive tasks. Extensive experi-\nments show that PLMs can significantly benefit\nfrom such an approach and a single unified PLM\n(UNIWEB) is able to achieve (near) state-of-the-art\nperformance on all 16 tasks.\n2 Related Work\nRetrieval-Augmented PLMs. Augmenting a pre-\ntrained language model with retrieval has been ex-\ntensively studied in existing literature (Lewis et al.,\n2020b; Borgeaud et al., 2022; Izacard et al., 2022;\nLee et al., 2019; Guu et al., 2020). For example,\nREALM (Guu et al., 2020) and RAG (Lewis et al.,\n2020b), incorporate a differentiable retriever into\npretrained models, leading to promising results on\nquestion answering. However, these studies usually\nrely on a sub-optimal retriever to access a static\nand limited knowledge resource, i.e., Wikipedia.\nBy contrast, our model utilizes the well-developed\nsearch engine to gain broader, more in-depth, and\nup-to-date knowledge from the web. Several stud-\nies have also looked at how Internet can help the\nmodels, but only focus on single tasks such as ques-\ntion answering (Nakano et al., 2021; Menick et al.,\n2022) and dialogue (Komeili et al., 2021). We-\nbGPT (Nakano et al., 2021) uses human feedback\nto optimize answer quality by hiring massive label-\ners to judge the accuracy of answers. Komeili et al.\n(2021) retrieves knowledge from the web for every\ndialogue without considering the necessity. Piktus\net al. (2021) only presents an empirical study to in-\nvestigate the impact of replacing Wikipedia with a\nlarge-scale web-like corpus and adopting different\nretrieval models. We are also aware of some related\nstudies (Jiang et al., 2023), but we have taken a dif-\nferent active approach for knowledge retrieval. In\nthis paper, we develop a unified language model for\nsolving a wide spectrum of knowledge-intensive\ntasks. Our model can selectively decide whether\nto access the web, and continuously learn from the\n729\nSelf-Evaluation\nRetrieve?\nCommonsense \nReasoning\nCommonsense QA\nSlot \nFilling\nDialogue Fact \nChecking\nNatural Language \nInference\nThe football manager who \nrecruited David Beckham \nmanaged Manchester United \nduring what timeframe?\n1995-96 Manchester \nUnited F.C. season\nThe 1995 -96 season was \nManchester United's fourth \nseason in the Premier League \n..... Alex Ferguson had sold \nexperienced players Paul Ince, \nMark Hughes. Instead, he had \ndrafted in young players like \nNicky Butt, David Beckham, \nPaul Scholes and the Neville \nbrothers , Gary and Phil.\nNo\nYes\ntop-ùêæ\nSearch Engine Assisted Learning\nknowledge\nOpen-domain QA\nLanguage\nModels\nKnowledge-Intensive Tasks Pretraining Objectives\nContinual Knowledge Learning\nThe [MASK] season was \nManchester United‚Äôs [MASK] \nseason in the ... sold experienced \nplayers Paul Ince, [MASK]. \nInstead, he had drafted in young \nplayers like Nicky Butt, David \nBeckham, [MASK] and .....\n‚ü∂Output: 1995 -96, fourth, Mark \nHughes, Paul Scholes\nKnowledge-Intensive Learning\nContext: [Knowledge] Input:\n[Question] ‚ü∂Output: 1995 -96\nfilter\nFigure 1: Overview of our proposed web-augmented pretrained language model UNIWEB.\nretrieved knowledge.\nKnowledge-Intensive Learning. Recent work\nhas shown that PLMs‚Äô parameters have implic-\nitly stored linguistic or factual knowledge (Petroni\net al., 2019; Roberts et al., 2020). However, the\nimplicitly encoded knowledge is limited by the\nmodel‚Äôs scale and training data, contradicting the\ndynamic nature of the world. Hence, many re-\nsearchers propose to fuse relevant external knowl-\nedge from texts with the encoded knowledge of\nPLMs to deal with knowledge-intensive tasks such\nas open-domain QA (Guu et al., 2020; Lewis et al.,\n2020b), entity linking (Wu et al., 2019), fact veri-\nfication (Liu et al., 2019b), and commonsense rea-\nsoning (Lin et al., 2020). Wikipedia has been the\nmost widely used knowledge source for these tasks,\nwhich is still limited despite its wide coverage. In-\nstead, we rely on the real-time web. The existing\nstudies usually design task-specific training, archi-\ntecture, and knowledge fusion method to exploit\nknowledge sources. In this work, we aim to de-\nvelop a single unified framework that can be used\nfor most knowledge-intensive tasks.\n3 Task Formulation\nKnowledge-intensive tasks (Yin et al., 2022) aim\nto leverage external knowledge resources to accom-\nplish a broad range of tasks such as open-domain\nquestion answering and fact verification.\nFollowing prior work (Lewis et al., 2020b; Guu\net al., 2020), we employ a retrieval-augmented gen-\neration framework that consists of two components:\na retriever Rand a generator G. Given an input\ntext Xsuch as a question, the retriever Rlearns to\nretrieve a set of top-Kpassages P= {p1,...,p K}\nfrom a knowledge resource. Conditioned on the\ninput text Xand the retrieved passages P, the gen-\nerator Gaims to generate the output text Y. The\nmodel is trained to maximize the joint likelihood:\nPr(Y|X) =\n‚àë\nR,G\nPr(P|X)Pr(Y|P,X). (1)\nTo implement the framework, previous studies\nusually adopt a trainable neural retriever based on a\n(single) knowledge resource such as Wikipedia or\nknowledge bases. However, such an approach can\nonly access limited, static knowledge. In this paper,\nwe rely on a general, off-the-shelf search engine\nas the retriever to access both comprehensive and\nup-to-date knowledge from the whole web.\n4 Approach\nOur proposed web-augmented PLM, UNIWEB, is\ndepicted in Figure 1. We first transform knowledge-\nintensive tasks into a unified text-to-text paradigm\nand consider the web as a general form of knowl-\nedge source. Based on the retrieved knowledge, we\nfurther design two training objectives to build our\nmodel. In the next sections, we will describe our\nmethod in detail.\n4.1 Knowledge-Intensive Tasks Unification\nPrevious retrieval-augmented approaches usually\nadopt diverse architectures and different types of\nknowledge resources (Yin et al., 2022). Instead,\nwe aim to leverage the general knowledge source\n(i.e., the web) to develop a unified framework that\ncan fulfill various (or most) knowledge-intensive\ntasks. Specifically, we unify 16 typical knowledge-\nintensive tasks across 7 task families, including fact\nchecking, slot filling, dialogue, open-domain ques-\ntion answering, commonsense question answering,\ncommonsense reasoning, and natural language in-\nference. We convert these tasks as a general text-\nto-text transformation for training a unified PLM.\n730\nThese tasks are mainly from the studies (Petroni\net al., 2020; Piktus et al., 2021), in which the orig-\ninal tasks of fact checking, slot filling, dialogue,\nand open-domain QA are designed specifically\nbased on the retrieved knowledge from Wikipedia,\nwhile other tasks of commonsense QA, common-\nsense reasoning, and natural language inference\nfocus on some more specific commonsense knowl-\nedge, going beyond Wikipedia. We consider these\nknowledge-intensive tasks as typical NLP tasks to\nshow that the large-scale web can be specially use-\nful for satisfying diverse information needs. More\ndetails about each task can be found in Appendix A.\n4.2 Web-based Knowledge Retrieval\nUnlike prior work that retrieves documents from\noffline corpora such as Wikipedia (Guu et al.,\n2020; Lewis et al., 2020b), we propose to retrieve\ncomprehensive and up-to-date information from\nthe online web through a general-purpose search\nengine. Although it is intuitive to extend the\nretrieval-augmented framework with the web as the\nknowledge resource, it is non-trivial to effectively\nleverage the knowledge found on the web. The\ndocuments on the web have inconsistent quality,\nand contain noisy, biased, or even harmful con-\ntents (Luccioni and Viviano, 2021). Low-quality\ncontent may steer PLMs towards seemingly plausi-\nble but factually incorrect outputs (Ji et al., 2022).\nOn the other hand, compared to a local neural re-\ntriever, black-box search engines can only be ac-\ncessed through queries, which is less controllable\nand not easy to filter out noisy contents from the\nsearch results. In addition, PLMs do not always\nneed external knowledge for task solving, espe-\ncially for easy tasks. Therefore, we should request\nfor more knowledge only when needed.\n4.2.1 PLM Knowledge Evaluation\nTo address the above challenges, it is essential to\nevaluate PLMs‚Äô own capabilities in a task and the\nnecessity to refer to external knowledge. In our\napproach, we consider a non-trivial question before\nretrieval: does a PLM need to retrieve knowledge\nfor a specific task instance? For this purpose, we in-\nvestigate whether or not PLMs can correctly answer\nquestions without using external evidence. Ac-\ncording to the recent study (Kadavath et al., 2022),\nPLMs can self-evaluate the confidence level of their\ngeneration results (e.g., True or False). Hence, we\npropose to utilize the self-evaluation mechanism to\ndetermine whether it is necessary to access addi-\ntional web information.\nSelf-Evaluation. Specifically, we hypothesize that\nwhen a model ‚Äúknows‚Äù the true output (i.e., confi-\ndent about its output) for a specific input, sampling\nthe outputs many times would result in an output\ndistribution with small entropy. Following Kada-\nvath et al. (2022), we sample n(n= 200) different\noutputs for each input and estimate the entropy of\nthe output distribution as follows:\nH( ÀÜY|X) = E[‚àílog Pr( ÀÜY|X)] (2)\n= E\nÔ£Æ\nÔ£∞‚àí\n‚àë\nwi‚ààÀÜY\nlog Pr(wi|X,w<i)\nÔ£π\nÔ£ª,\nwhere ÀÜY= ‚ü®w1,...,w i,...,w m‚ü©is the output text\ngenerated by the model G. Then, we set an entropy\nthreshold Œ∑. If H( ÀÜY|X) is higher than Œ∑, it means\nthat the model is unconfident about its outputs and\nneeds supporting evidence from the web, otherwise,\nit does not. We will further demonstrate the pre-\ndictive power of the entropy (Eq. (2)) in estimating\nthe model confidence for knowledge retrieval.\n4.2.2 Web Knowledge Retrieval\nIn active learning (Ren et al., 2021), a prediction\nmodel can interactively query for labeling exam-\nples with low confidence levels. This learning\nmethod can not only reduce the cost of data la-\nbeling, but also remove those noisy and unhelpful\ndata that models cannot benefit from. Inspired by\nthis, we propose a search engine assisted learning\napproach, in which PLMs choose those hard cases\nthat they cannot solve (assessed by self-evaluation)\nto query the off-the-shelf search engine for knowl-\nedge retrieval. Different from active learning, our\napproach does not directly query for the final an-\nswer (largely reducing the labeling efforts), but\ninstead the supporting evidence for solving the\ntask. After retrieving knowledge from the web,\nit is critical to filter out noisy contents and select\nthe most helpful and relevant knowledge that can\nenhance PLMs‚Äô confidence to generate correct out-\nputs. Therefore, we elaborate a two-stage filter\nmechanism to filter the retrieved knowledge.\nSearch Engine Assisted Learning. Specifically,\nfor those hard examples, we take their input text\nX verbatim as a search query and issue a call\nto Google Search via API. For each query, we\nretrieve top-K HTML pages and parse them to\nobtain clean texts, resulting in a set of passages\n731\nP= {p1,...,p K}. To filter out noisy and irrele-\nvant information, in the first stage, we chunk each\npassage into paragraphs, compute the cosine sim-\nilarity between input and paragraph embeddings,\nand select the five most relevant paragraphs to form\nthe final passage. In the second stage, we adopt the\nsame method as self-evaluation (Eq. 2) to compute\nthe model confidence given the input and each pro-\ncessed passage and select those passages with high\nconfidence as the final evidence.\n4.3 Knowledge-based Model Pretraining\nIn most previous work, the retrieval model is either\npretrained using self-supervised objective such as\nMLM (Guu et al., 2020; Borgeaud et al., 2022)\nor trained for specific tasks (Lewis et al., 2020b).\nIn this work, we focus on explicitly training web-\naugmented PLMs in a supervised and massively\nmulti-task fashion (Aribandi et al., 2022) using the\nmixture of knowledge-intensive tasks (Section 4.1).\nBesides, to integrate the retrieved knowledge into\nPLMs, we design a continual knowledge learning\ntask based on the retrieved passages.\nKnowledge-Intensive Learning. This pretraining\nobjective uses the retrieved knowledge and labeled\ndata from the unified knowledge-intensive tasks.\nFormally, given an input textXand retrieved pas-\nsages P, this objective is to minimize the negative\nlog-likelihood loss over the output text Y:\nLKIL = ‚àí\nm‚àë\ni=1\nlog Pr(wi|w<i,X,P), (3)\nwhere wi denotes the i-th token of the output textY.\nWe concatenate the input text Xand retrieved pas-\nsages Pusing the manually-written task-specific\nprompts (shown in Appendix A). Pretrained on the\nunified knowledge-based text-to-text format, our\nmodel can be easily applied to diverse knowledge-\nintensive tasks. It has been reported that ensem-\nbling many tasks, distributions and domains during\npretraining can improve PLMs‚Äô generalization to\nnew tasks (Aribandi et al., 2022).\nContinual Knowledge Learning. Due to the lim-\nited pretraining on single static corpus, the knowl-\nedge encoded in PLMs has a discrepancy with the\nretrieved knowledge from the web. Thus, to reduce\nthe discrepancy and integrate the newly retrieved\nknowledge into PLMs, we design a self-supervised\npretraining task, i.e., continual knowledge learning.\nFor most knowledge-intensive tasks such as slot\nfilling and fact verification, named entities are of\nspecial importance. Thus, this pretraining task aims\nto predict the salient masked spans ( i.e., named\nentities) in retrieved passages. Firstly, we use a\nBERT-based (Devlin et al., 2019) tagger trained on\nCoNLL-2003 data (Sang and De Meulder, 2003) to\nidentify name entities and then mask entities such\nas ‚ÄúUnited States‚Äù. Then, our model will be trained\nto predict these masked spans by minimizing the\nmasked span prediction loss:\nLCKL = ‚àí\nK‚àë\nk=1\nm‚àë\nj=1\nlog Pr(sj|Àúpk), (4)\nwhere sj is the j-th masked span for the passage\npk, and Àúpk denotes the unmasked tokens in pk.\n5 Experiments\nIn this section, we detail the experimental setup and\nthen highlight the main observations of our results.\n5.1 Experimental Setup\nKnowledge Source. In large-scale pretraining, we\nleverage an open massive web corpus CCNet (Wen-\nzek et al., 2020) to provide documents with diverse\ntopics, approximating the realistic web. Following\nPiktus et al. (2021), we select the CCNet snapshot\ncorresponding to the August 2019 Common Crawl\nsnapshot which covers a wide range of 134M web\ndocuments and finally yields 906M passages of 100\ntokens. CCNet processes Common Crawl through\ndeduplication, language identification and quality\nfiltering based on perplexity calculated by a lan-\nguage model. In downstream fine-tuning, we test\nwith the off-the-shelf search engine, i.e., Google\nSearch, to retrieve documents from the real-time\nweb. Specifically, we utilize the input text verbatim\nas query and request a call to Google Search via\nAPI1. Besides, for the Wikipedia-based baselines,\nwe use the 2019/08/01 Wikipedia snapshot from\nthe KILT benchmark (Petroni et al., 2020), consist-\ning of 5.9M documents split into 22.2M passages\nof 100 tokens. This data snapshot is temporally the\nclosest to the CCNet corpus for fair comparison.\nPretraining Tasks. As described in Section 4.1,\nwe unify 16 knowledge-intensive tasks across seven\ntask families during pretraining:\n‚Ä¢ Fact Checking: FEVER (Thorne et al., 2018).\n1https://developers.google.com/custom-search\n732\nModels\nFact Checking Slot Filling Dialogue Open-domain QA\nFEVER T-REx zsRE WoW NQ HotpotQA TriviaQA ELI5\nw/o Retrieval\nBARTLARGE 78.93 45.06 9.14 12.86 21.75 15.37 32.39 20.55\nT5LARGE 80.31 50.63 10.34 12.67 28.50 18.98 35.90 20.60\nw/ Wikipedia\nREALM 76.22 53.35 39.38 - 40.40 22.23 65.44 10.23\nRAG 86.31 59.20 44.74 13.11 44.39 26.97 71.27 14.05\nBART+DPR 86.74 59.16 30.43 15.19 41.27 25.18 58.55 17.41\nBART+DPRMULTI 86.32 78.50 57.95 15.33 39.75 31.77 59.60 17.07\nFID+DPRMULTI 88.99 82.19 71.53 15.66 49.86 36.90 71.04 16.45\nw/ CCNet\nFID+DPRMULTI 85.74 52.06 28.47 15.22 45.15 27.29 67.49 16.14\nFID+DPRCCN ET 87.43 57.02 36.55 15.29 48.61 31.64 73.06 15.76\nFID+BM25 89.12 62.12 43.92 17.28 46.05 34.10 78.21 15.59\nw/ Web\nUniWeb 91.69 83.58 72.42 20.87 54.37 40.73 77.01 18.34\nTable 2: Evaluation results on the test set for fact checking, slot filling, dialogue, and open-domain QA. We report\nAccuracy for FEVER, T-REx, and zsRE; EM for NQ, HotpotQA, and TriviaQA; ROUGE-L for ELI5 and F1-score\nfor WoW. These results come from no-retrieval models (top section), Wikipedia/CCNet-based models (middle\nsection), and Web-based models (bottom section). Bold and underline denote the best and second best methods.\n‚Ä¢ Slot Filling: T-REx (ElSahar et al., 2018) and\nzero-shot RE (Levy et al., 2017).\n‚Ä¢ Dialogue: Wizard-of-Wikipedia (Dinan et al.,\n2019).\n‚Ä¢ Open-domain QA: TriviaQA (Joshi et al., 2017),\nNatural Questions (Kwiatkowski et al., 2019),\nHotpotQA (Yang et al., 2018), and ELI5 (Shuster\net al., 2020).\n‚Ä¢ Commonsense QA: CommonsenseQA (Talmor\net al., 2019), SocialIQa (Sap et al., 2019), Cos-\nmosQA (Huang et al., 2019), and PIQA (Bisk\net al., 2020).\n‚Ä¢ Commonsense Reasoning: NumerSense (Lin\net al., 2020) and WinoGrande (Sakaguchi et al.,\n2020).\n‚Ä¢ Natural Language Inference: Œ±NLI (Bhagavat-\nula et al., 2020) and HellaSwag (Zellers et al.,\n2019).\nWe convert these tasks into a unified text-to-text\nformat. We take the input text as query to retrieve\ntop 10 passages from CCNet. After pre-processing,\nwe mix the training set of these datasets to pretrain\nour model. We present the statistics of datasets and\npre-processing details in Appendix A.\nBaselines. We compare UniWeb to a wide range\nof models as follows:\n‚Ä¢ BART (Lewis et al., 2020a) and T5 (Raffel et al.,\n2020). These are two representative text-to-text\nPLMs for solving knowledge-intensive tasks. We\nadopt the large version for a fair comparison.\n‚Ä¢ REALM (Guu et al., 2020) and RAG (Lewis\net al., 2020b). They are two well-known retrieval-\naugmented PLMs combining with a nonparamet-\nric memory of Wikipedia via a neural retriever.\n‚Ä¢ Fusion-in-Decoder (FID) (Izacard and Grave,\n2020). It is based on T5 where the encoder en-\ncodes the input text with each passage and the\ndecoder combines the encoded representations.\n‚Ä¢ Maillard et al. (2021) and Piktus et al. (2021)\nequip BART and FID with retrieval models,i.e.,\nBM25 (Robertson et al., 2009), DPR (Karpukhin\net al., 2020), DPRMULTI trained in a multi-task\nfashion, and DPRCCN ET trained on CCNet.\nNote that these models are trained on individual\ntasks and datasets, while our model is pretrained in\na multi-task manner. We use BM25 to retrieve pas-\nsages from CCNet during pretraining. The BM25\nand DPR indices are collected from the previous\nword (Piktus et al., 2021). Since it lacks the re-\ntrieval supervision to train DPR for those tasks in\nTable 3, we only report the BM25 results. The\nimplementation details are shown in Appendix B.\nEvaluation Metrics. We adopt various tasks and\ndatasets in our experiments, which need to be eval-\nuated differently. Following Petroni et al. (2020),\nwe use Exact Match (EM) for datasets with extrac-\ntive (i.e., Natural Questions, TriviaQA) or short ab-\nstractive output text (i.e., HotpotQA); for datasets\n733\nModels\nCommonsense QA Commonsense Reasoning NLI\nCSQA SocialIQA CosmosQA PIQA NumerSense WinoGrande HellaSwag Œ±NLI\nw/o Retrieval\nBARTLARGE 62.50 74.00 75.11 77.40 55.30 62.40 76.60 75.12\nT5LARGE 72.56 74.16 79.23 78.67 59.71 76.48 79.84 77.48\nw/ Wikipedia\nREALM 63.11 62.52 71.33 70.65 57.34 62.12 73.21 71.40\nRAG 69.51 68.32 76.55 75.23 59.22 63.35 75.01 74.45\nBART+BM25 70.16 70.83 76.14 77.04 57.50 65.09 76.34 74.66\nFID+BM25 73.63 74.36 78.83 79.65 62.30 76.72 79.96 77.94\nw/ CCNet\nFID+BM25 73.63 73.64 79.63 81.66 66.70 76.80 81.96 77.74\nw/ Web\nUniWeb 75.34 73.17 80.96 79.77 69.23 78.74 82.12 77.23\nTable 3: Evaluation results at Accuracy on the dev set for commonsense QA, commonsense reasoning, and natural\nlanguage inference (NLI). Bold and underline numbers denote the best and second best performance. Following\nPiktus et al. (2021), since it lacks the retrieval supervision to train DPR, we only report the BM25 results.\nwith long abstractive output text, we use ROUGE-\nL (Lin, 2004) for ELI5 and F1-score for Wizard\nof Wikipedia; we use Accuracy for the remaining\ntasks. To compute EM and F1-score, we conduct\npost-processing on the gold and predicted output\ntexts such as lowercasing, stripping, punctuation,\nand duplicate whitespace (Rajpurkar et al., 2016).\n5.2 Main Results\nTable 2 and Table 3 show the results ofUNIWEB\nand baselines on 16 knowledge-intensive tasks.\nFirst, on almost all knowledge-intensive tasks,\ncombining PLMs with explicit retrieved knowledge\ncan achieve higher performance. From Wikipedia\nand CCNet to the web, we can observe that a\nbroader coverage of knowledge will lead to better\nresults. Compared to BART and T5, retrieval-based\nmodels benefit from the retrieved knowledge.\nSecond, the tasks in Table 2 are specially de-\nsigned based on the knowledge from Wikipedia.\nThus, there is a strong bias towards Wikipedia as\nthe knowledge resource. We can observe that CC-\nNet only achieves comparable results or even suf-\nfers from a large performance drop. However, for\nthe tasks in Table 3 requiring knowledge beyond\nWikipedia, CCNet is more competitive.\nFinally, our UNIWEB model achieves the best\nresults on most knowledge-intensive tasks. On one\nhand, our model is trained in a multi-task manner,\nwhich can benefit from knowledge sharing across\ntasks. On the other hand, our model can access\nbroad and up-to-date knowledge from the web via\nthe fine-tuned search engine. The web knowledge\nModels zsRE WoW CSQA PIQA Œ±NLI\nUniWeb 72.42 20.87 75.34 79.77 77.23\nw/ Wikipedia 70.23 16.34 62.77 77.45 74.46\nw/ CCNet 43.25 17.23 70.89 79.45 76.01\nw/o SE 68.34 19.17 67.44 76.80 73.90\nw/o CKL 69.70 19.09 66.70 76.57 75.01\nTable 4: Ablation study on five tasks.\ncan fulfill more diverse information needs. More-\nover, the search engine works much better than\ntraditional sub-optimal retrieval methods that rely\non end-to-end training or word matching.\n5.3 Detailed Analysis\nWe report detailed analysis of UniWeb in several\ndatasets ‚Äì we have similar finding in other datasets.\nAblation Study. Our UNIWEB model is the first\nunified PLM using the web as knowledge source\nfor knowledge-intensive tasks. To examine the im-\nportance of the web, we design two counterparts:\n(1) w/ Wikipedia or (2) w/ CCNet replaces the web\nwith Wikipedia or CCNet and adopts BM25 to\nretrieve documents. Besides, to avoid the nega-\ntive impact of noisy and biased information, we\nadopt the self-evaluation method to adaptively ac-\ncess knowledge from the web. Thus, we remove\nthis method to test its effect (w/o SE). Finally, we\nremove the pretraining task, i.e., continuous knowl-\nedge learning, to test its importance ( w/o CKL).\nThe results are shown in Table 4. We can see that\nreplacing the web with Wikipedia or CCNet suffers\nfrom a large performance drop. Besides, the self-\n734\nQuestion: With France and Argentina set to battle it out on Sunday in the World Cup final 2022, which teams will go head\nto head for the third place?\nGold Answer: Croatia and Morocco\nTop-1 Wikipedia Passage Top-1 CCNet Passage Top-1 Web Passage\n... Third place play-off The Nether-\nlands defeated Brazil 3‚Äì0 to secure\nthird place, the first for the Dutch\nteam in their history. Overall, Brazil\nconceded 14 goals in the tournament;\nthis was the most by a team at any\nsingle World Cup since 1986, and\nthe most by a host nation in history...\nhttps://en.wikipedia.org\n/wiki/2014_FIFA_World_Cup\n... France and Belgium go head-to-head\nin the first semi-finals of World Cup\n2018. Both teams have impressed in\nRussia so far, but only one can make\nit through to Sunday‚Äôs final. How-\never, Les Bleus have won four of their\nfive matches at World Cup 2018 and\nshown flashes of quality in the process...\nhttps://myarsenalblog.com\n/category/uncategorized\n... Third place for Croatia Zlatko Dalic‚Äôs\nCroatia followed up their runners-up\neffort at the Russia 2018 World Cup\nwith third place in Qatar as Mislav\nOrsic‚Äôs fine effort secured victory over\nthe tournament‚Äôs surprise package Mo-\nrocco at Khalifa International Stadium...\nhttps://ca.sports.yahoo.com\n/news/today-world-cup-argen\ntina-head-085045315.html\nPrediction: The Netherlands and Brazil Prediction: France and Belgium Prediction: Croatia and Morocco /enc-34\nTable 5: A qualitative example showing the top-1 retrieved passages from Wikipedia, CCNet, and web, and their\ncorresponding model prediction. The words in red denote the keywords related to the question.\nÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩÔøΩ ÔøΩÔøΩ ÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\n(a)\n1-5 6-10 11-15 16-20\ntop-K\n40\n50\n60\n70\n80Accuracy (%)\nT-Rex\nCSQA\nPIQA (b)\nFigure 2: (a) Entropy of samples in HotpotQA; (b)\nAccuracy w.r.tdifferent top-Kdocuments.\nevaluation method benefits our model a lot in terms\nof knowledge filtering. The pretraining task also\nimproves the knowledge capacity of our model.\nSensitivity Analysis. In the self-evaluation mecha-\nnism, we use entropy to evaluate the model confi-\ndence. To verify its effectiveness, we present the\ndistribution of H( ÀÜY|X) depending on whether or\nnot the model gets the question correct. As shown\nin Figure 2(a), the average entropy of the questions\nfor which our model gets correct is lower than that\nof questions for which our model gets incorrect.\nThis indicates that the entropy has some predictive\npower of model confidence. Besides, the quality of\nretrieved documents will largely affect the predic-\ntion of our model. Thus, in Figure 2(b), we test the\nmodel accuracy by varying the top-Ksearch results\nin the set of {1-5, 6-10, 11-15, 16-20}. We can see\nthat PLM performance drops with the increase of\nrank of documents, thus the decrease of document\nquality. However, the retrieved top 6-10 passages\nalso achieve comparable results to the top 1-5 ones.\nThis is the motivation of our setting K = 10.\n5.4 Case Study\nIn this section, we perform the qualitative analysis\non REALTIME QA (Kasai et al., 2022), a bench-\nmark requiring real-time, up-to-date, and compre-\nhensive knowledge with a broad range of topics\n(such as politics, business, sports, and entertain-\nment) to solve questions. The evaluation results\nare shown in Appendix C. Our UniWeb model with\nGoogle Search performs the best. We present an ex-\nample in Table 5 about ‚ÄúWorld Cup final 2022‚Äù in\nthe sports topic. By using the question text as query,\nwe can retrieve top-1 passages from Wikipedia,\nCCNet, and web. Since Wikipedia and CCNet are\nboth static and limited knowledge resources, the\nretrieved passages are not fresh in time (‚Äú2014‚Äù\nand ‚Äú2018‚Äù) even though they are on the same\ntopic ‚ÄúWorld Cup‚Äù. The typical retrieval methods\n(BM25 or DPR) are largely reliant on fuzzy se-\nmantic matching, also leading to incorrect retrieval.\nWhile, retrieving from the web using search engine\ncan ensure our model to obtain the most up-to-date\nand relevant information, based on which it can\ngenerate the correct answer ‚ÄúCroatia and Morocco‚Äù.\nWe present more examples in Appendix D.\n6 Conclusion\nThis paper presented a unified web-augmented\nframework for a wide range of knowledge-intensive\ntasks, called UNIWEB. We convert 16 tasks into\na text-to-text generation task for training. We pro-\npose a search engine assisted learning method to se-\nlectively retrieve documents from the web through\nGoogle Search. Furthermore, to reduce the discrep-\nancy between the encoded and retrieved knowledge,\n735\nwe design a pretraining task, i.e., continual knowl-\nedge learning, to integrate the retrieved knowledge\ninto LLMs. Experiments on 16 tasks show the ef-\nfectiveness of our web-augmented model compared\nto previous retrieval-augmented models. In future\nwork, we will investigate the effect of web content\nin detail and consider applying our model to more\ntypes of downstream tasks.\n7 Limitations\nFor web-augmented models including our work,\nthe deterioration of search results from search\nengine highlights the importance of deriving an\neffective method to interact with the huge web.\nSearch engines are often perceived as black-box\nand non-transparent for end users. Therefore, many\nworks proposed ‚Äúleaning to search‚Äù to decompose\ncomplex questions into simpler queries, which\nmay improve the performance of web-based mod-\nels (Nakano et al., 2021; Komeili et al., 2021).\nIn our model, we used a commercial search en-\ngine as the retriever to work with the whole web\nas a knowledge source. Since the web is not cu-\nrated and well-structured like Wikipedia, we may\nencounter unexpected safety issues, including mis-\ninformation and harmful contents. While we have\nrelied on the security control of the search engine,\nmore attention should be paid to better understand\nthe risks and provide effective ways to mitigate\nthem. We hope our simple approach and strong\nresults could encourage more future work by the\ncommunity to tackle these questions. To encour-\nage the community to investigate the question and\nensure reproducibility, after the reviewing process,\nwe will release the search URLs used in our exper-\niments.\nAs for the potential concern, since we use the\nsearch engine to access real-time information, we\ndo not have a tight control over retrieved results as\ntraditional end-to-end retrieval (Guu et al., 2020;\nLewis et al., 2020b). Not only the changes of search\nengine logic, but also the newly published infor-\nmation, might create discrepancies over the course\nof time. This is also an issue we have to tackle to\nbuild a stable web-based solution for PLMs.\nAcknowledgments\nThis work was partially supported by National\nNatural Science Foundation of China under Grant\nNo. 62222215 and 72222022, Beijing Natural Sci-\nence Foundation under Grant No. 4222027, Bei-\njing Outstanding Young Scientist Program under\nGrant No. BJJWZYJH012019100020098, and the\nOutstanding Innovative Talents Cultivation Funded\nPrograms 2021 of Renmin University of China.\nXin Zhao is the corresponding author.\nReferences\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,\nHuaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon-\nglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni,\nJai Gupta, Kai Hui, Sebastian Ruder, and Donald\nMetzler. 2022. Ext5: Towards extreme multi-task\nscaling for transfer learning. In International Confer-\nence on Learning Representations.\nChandra Bhagavatula, Ronan Le Bras, Chaitanya\nMalaviya, Keisuke Sakaguchi, Ari Holtzman, Han-\nnah Rashkin, Doug Downey, Wen-tau Yih, and Yejin\nChoi. 2020. Abductive commonsense reasoning. In\n8th International Conference on Learning Represen-\ntations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nYonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng\nGao, and Yejin Choi. 2020. PIQA: reasoning about\nphysical commonsense in natural language. In The\nThirty-Fourth AAAI Conference on Artificial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 7432‚Äì\n7439. AAAI Press.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International conference on ma-\nchine learning, pages 2206‚Äì2240. PMLR.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nDanqi Chen and Wen-tau Yih. 2020. Open-domain ques-\ntion answering. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics: Tutorial Abstracts, ACL 2020, Online, July\n5, 2020, pages 34‚Äì37. Association for Computational\nLinguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\n736\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171‚Äì4186. Association for Computational\nLinguistics.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof Wikipedia: Knowledge-powered conversational\nagents. In Proceedings of the International Confer-\nence on Learning Representations (ICLR).\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. In Advances in Neural Information Pro-\ncessing Systems, volume 32. Curran Associates, Inc.\nZi-Yi Dou and Nanyun Peng. 2022. Zero-shot com-\nmonsense question answering with cloze transla-\ntion and consistency optimization. arXiv preprint\narXiv:2201.00136.\nHady ElSahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon S. Hare, Fr√©d√©rique\nLaforest, and Elena Simperl. 2018. T-rex: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Evalu-\nation, LREC 2018, Miyazaki, Japan, May 7-12, 2018.\nEuropean Language Resources Association (ELRA).\nZhijiang Guo, Michael Schlichtkrull, and Andreas Vla-\nchos. 2022. A survey on automated fact-checking.\nTransactions of the Association for Computational\nLinguistics, 10:178‚Äì206.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International Con-\nference on Machine Learning , pages 3929‚Äì3938.\nPMLR.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and\nYejin Choi. 2019. Cosmos QA: machine reading\ncomprehension with contextual commonsense rea-\nsoning. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019, pages 2391‚Äì\n2401. Association for Computational Linguistics.\nMinlie Huang, Xiaoyan Zhu, and Jianfeng Gao. 2020.\nChallenges in building intelligent open-domain di-\nalog systems. ACM Transactions on Information\nSystems (TOIS), 38(3):1‚Äì32.\nGautier Izacard and Edouard Grave. 2020. Leverag-\ning passage retrieval with generative models for\nopen domain question answering. arXiv preprint\narXiv:2007.01282.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2022. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys.\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing\nSun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. 2023. Ac-\ntive retrieval augmented generation. arXiv preprint\narXiv:2305.06983.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2017, Vancouver, Canada, July 30 - August 4, Volume\n1: Long Papers, pages 1601‚Äì1611. Association for\nComputational Linguistics.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\nEli Tran-Johnson, et al. 2022. Language models\n(mostly) know what they know. arXiv preprint\narXiv:2207.05221.\nVladimir Karpukhin, Barlas OÀòguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. arXiv preprint\narXiv:2004.04906.\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi,\nRonan Le Bras, Akari Asai, Xinyan Yu, Dragomir\nRadev, Noah A Smith, Yejin Choi, and Kentaro Inui.\n2022. Realtime qa: What‚Äôs the answer right now?\narXiv preprint arXiv:2207.13332.\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2021.\nInternet-augmented dialogue generation. arXiv\npreprint arXiv:2107.07566.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur P. Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Trans. Assoc. Comput. Linguistics, 7:452‚Äì\n466.\n737\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised\nopen domain question answering. arXiv preprint\narXiv:1906.00300.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettle-\nmoyer. 2017. Zero-shot relation extraction via read-\ning comprehension. In Proceedings of the 21st Con-\nference on Computational Natural Language Learn-\ning (CoNLL 2017), Vancouver, Canada, August 3-4,\n2017, pages 333‚Äì342. Association for Computational\nLinguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871‚Äì7880.\nAssociation for Computational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt√§schel, et al. 2020b. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459‚Äì9474.\nBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xi-\nang Ren. 2020. Birds have four legs?! numersense:\nProbing numerical commonsense knowledge of pre-\ntrained language models. In Proceedings of EMNLP.\nTo appear.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74‚Äì81.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019a.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZhenghao Liu, Chenyan Xiong, Maosong Sun, and\nZhiyuan Liu. 2019b. Fine-grained fact verification\nwith kernel graph attention network. arXiv preprint\narXiv:1910.09796.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nAlexandra Luccioni and Joseph Viviano. 2021. What‚Äôs\nin the box? an analysis of undesirable content in the\ncommon crawl corpus. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), pages 182‚Äì189.\nJean Maillard, Vladimir Karpukhin, Fabio Petroni, Wen-\ntau Yih, Barlas OÀòguz, Veselin Stoyanov, and Gargi\nGhosh. 2021. Multi-task retrieval for knowledge-\nintensive tasks. arXiv preprint arXiv:2101.00117.\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-\nGillingham, Geoffrey Irving, et al. 2022. Teaching\nlanguage models to support answers with verified\nquotes. arXiv preprint arXiv:2203.11147.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint\narXiv:2112.09332.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Mail-\nlard, et al. 2020. Kilt: a benchmark for knowl-\nedge intensive language tasks. arXiv preprint\narXiv:2009.02252.\nFabio Petroni, Tim Rockt√§schel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander H Miller, and\nSebastian Riedel. 2019. Language models as knowl-\nedge bases? arXiv preprint arXiv:1909.01066.\nAleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nDmytro Okhonko, Samuel Broscheit, Gautier Izacard,\nPatrick Lewis, Barlas OÀòguz, Edouard Grave, Wen-tau\nYih, et al. 2021. The web is your oyster‚Äìknowledge-\nintensive nlp against a very large web corpus. arXiv\npreprint arXiv:2112.09924.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1‚Äì140:67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions\nfor machine comprehension of text. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2016, Austin,\nTexas, USA, November 1-4, 2016, pages 2383‚Äì2392.\nThe Association for Computational Linguistics.\nPengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao\nHuang, Zhihui Li, Brij B Gupta, Xiaojiang Chen, and\nXin Wang. 2021. A survey of deep active learning.\nACM computing surveys (CSUR), 54(9):1‚Äì40.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the pa-\nrameters of a language model? arXiv preprint\narXiv:2002.08910.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends¬Æ in Information Re-\ntrieval, 3(4):333‚Äì389.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2020. Winogrande: An adver-\nsarial winograd schema challenge at scale. In The\n738\nThirty-Fourth AAAI Conference on Artificial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 8732‚Äì\n8740. AAAI Press.\nErik F Sang and Fien De Meulder. 2003. Introduction\nto the conll-2003 shared task: Language-independent\nnamed entity recognition. arXiv preprint cs/0306050.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le\nBras, and Yejin Choi. 2019. Social iqa: Common-\nsense reasoning about social interactions. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing, EMNLP-IJCNLP 2019, Hong Kong, China,\nNovember 3-7, 2019, pages 4462‚Äì4472. Association\nfor Computational Linguistics.\nKurt Shuster, Da Ju, Stephen Roller, Emily Dinan, Y-\nLan Boureau, and Jason Weston. 2020. The dialogue\ndodecathlon: Open-domain knowledge and image\ngrounded conversational agents. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 2453‚Äì2470. Association for Computa-\ntional Linguistics.\nShane Storks, Qiaozi Gao, and Joyce Y Chai. 2019.\nRecent advances in natural language inference: A\nsurvey of benchmarks, resources, and approaches.\narXiv preprint arXiv:1904.01172.\nMihai Surdeanu and Heng Ji. 2014. Overview of the\nenglish slot filling track at the tac2014 knowledge\nbase population evaluation. In Proc. Text Analysis\nConference (TAC2014).\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4149‚Äì4158. Association for Computational\nLinguistics.\nTianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong\nWen. 2022. Mvp: Multi-task supervised pre-training\nfor natural language generation. arXiv preprint\narXiv:2206.12131.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction and\nVERification. In NAACL-HLT.\nClaudia Wagner, Eduardo Graells-Garrido, David Gar-\ncia, and Filippo Menczer. 2016. Women through the\nglass ceiling: gender asymmetries in wikipedia. EPJ\nData Science, 5:1‚Äì24.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm√°n, Ar-\nmand Joulin, and Edouard Grave. 2020. Ccnet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data. In Proceedings of The 12th Language\nResources and Evaluation Conference, LREC 2020,\nMarseille, France, May 11-16, 2020 , pages 4003‚Äì\n4012. European Language Resources Association.\nLedell Wu, Fabio Petroni, Martin Josifoski, Sebastian\nRiedel, and Luke Zettlemoyer. 2019. Scalable zero-\nshot entity linking with dense entity retrieval. arXiv\npreprint arXiv:1911.03814.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. HotpotQA: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nDa Yin, Li Dong, Hao Cheng, Xiaodong Liu, Kai-Wei\nChang, Furu Wei, and Jianfeng Gao. 2022. A survey\nof knowledge-intensive nlp with pre-trained language\nmodels. arXiv preprint arXiv:2202.08772.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In Proceedings\nof the 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n4791‚Äì4800. Association for Computational Linguis-\ntics.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020. DIALOGPT : Large-scale\ngenerative pre-training for conversational response\ngeneration. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 270‚Äì278, Online. As-\nsociation for Computational Linguistics.\nWayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong\nWen. 2022. Dense text retrieval based on pretrained\nlanguage models: A survey. CoRR, abs/2211.14876.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023. A survey of large language models. CoRR,\nabs/2303.18223.\n739\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming\nZheng, Soujanya Poria, and Tat-Seng Chua. 2021.\nRetrieving and reading: A comprehensive survey on\nopen-domain question answering. arXiv preprint\narXiv:2101.00774.\nAppendix\nWe provide some experiment-related information\nas supplementary materials. The appendix is orga-\nnized into three sections:\n‚Ä¢ Details of pretraining tasks are presented in\nAppendix A;\n‚Ä¢ Model architecture and pretraining details are\npresented in Appendix B;\n‚Ä¢ Supplementary experiments are presented in\nAppendix C;\n‚Ä¢ Examples with retrieved knowledge are pre-\nsented in Appendix D.\nA Pretraining Tasks\nAs described in Section 4.1, to pretrain our model,\nwe unify 16 knowledge-intensive tasks across seven\ncategories into a general text-to-text format:\n‚Ä¢ Fact checking is the task of assessing whether a\nnatural language claim is true (Guo et al., 2022).\nIt requires deep knowledge about the claim. We\nconsider the claim as input and the classification\nlabel (e.g., true/false) as output.\n‚Ä¢ Slot filling aims to complete the missing infor-\nmation for certain relations of entities (Surdeanu\nand Ji, 2014) (e.g., subject entity Star Trek and\nrelation creator). It requires entity disambigua-\ntion and the relational knowledge for entities. We\nmodel the structured string ‚Äúsubject entity [SEP]\nrelation‚Äù as input and the object entity as output.\n‚Ä¢ Dialogue focuses on building an engaging chat-\nbot that can discusses a wide range of open-ended\ntopics such as whether (Huang et al., 2020). It\nrequires models to know about the background\nknowledge for the conversational topics. We con-\nsider the dialogue history as input and the next\nutterance as output.\n‚Ä¢ Open-domain question answering is the task\nof producing answers to factoid questions in nat-\nural language (Zhu et al., 2021). The questions\ncould be about nearly anything relying on world\nknowledge. We consider the question as input\nand the answer as output.\n‚Ä¢ Commonsense question answering aims to test\nif models can answer questions regarding com-\nmonsense knowledge that everyone knows (Dou\nand Peng, 2022). Similarly, we consider the ques-\ntion as input and the answer as output.\n740\n‚Ä¢ Commonsense reasoning is intended to utilize\ncommonsense knowledge to reason about certain\naspects of the given text (Sakaguchi et al., 2020).\nTherefore, we consider the given text as input\nand the prediction as output.\n‚Ä¢ Natural language inference is the task of deter-\nmining whether the given ‚Äúhypothesis‚Äù logically\nfollows from the ‚Äúpremise‚Äù (Storks et al., 2019).\nIt acquires deep knowledge about the relationship\nbetween hypothesis and premise. We consider\nthe premise as input and the hypothesis as output.\nFor each category, we choose several representa-\ntive tasks to construct our pretraining corpus. The\ndetailed information of these included tasks is listed\nin Table 6. To mitigate the huge disparity between\ndataset sizes, we follow (Raffel et al., 2020) to use\nthe temperature-scaled mixing strategy with a rate\nof T = 2 for setting the proportion of data coming\nfrom each task. During pretraining, for each task\nexample, we use BM25 to retrieve top-10 passages\nfrom CCNet as our external knowledge. The input\ntexts are concatenated with the retrieved passages\nusing manually-written prompts. The final input is\nconstructed in the following format:\nContext: [passage1]...[passage10]\n[Task Instruction]: [the original input text]\nOption 1: [option1]...Option n: [optionn]\nThe ‚ÄúOption‚Äù string is applied only when the input\ntext is provided with several candidate answers.\nThe blanks ‚Äú[passagen]‚Äù and ‚Äú[optionn]‚Äù is filled\nwith the retrieved passages and candidate answers.\nThe blank ‚Äú[Task Instruction]‚Äù aims to indicate\nthe task for our model, which is task-specific and\ndetailed in Table 7.\nB Implementation Details\nOur UniWeb model uses a Transformer with\n12 layers in both encoder and decoder ( 406M\nparameters), the same as the model size of\nBARTLARGE (Lewis et al., 2020a). The hidden\nsize is 1,024 and the inner hidden size of the feed-\nforward network is 4,096. We employ the byte-\npair-encoding (BPE) tokenizer, and the vocabulary\nsize is 50,267. We initialize the backbone with the\nMVP model (Tang et al., 2022), a supervised pre-\ntrained PLM, to provide a good starting point for\ngeneration following previous work (Dong et al.,\n2019; Zhang et al., 2020). We pretrain the model\nwith batch size 8,192 on Tesla A100 40GB GPUs.\nAlgorithm 1 The pseudo code for UNIWEB.\nRequire: A search engine ( i.e., Google Search)\nconnecting with the large-scale web\n1: Input: Training data D\n2: Output: Model parameters Œò\n3: Initialize Œò\n4: while not convergence do\n5: for iteration = 1 to |D|do\n6: Acquire an input-output pair ‚ü®X,Y‚ü©\n‚ñ∑Self-Evaluation\n7: Compute the entropyH( ÀúY|X) of the sam-\npled output distribution (Eq. 2)\n‚ñ∑Search Engine Assisted Learning\n8: if H >Œ∑then\n9: Use Xas a query to the search engine\n10: Return top-Kpassages P\n11: else\nThe passages Pare null ‚àÖ\n12: end if\n‚ñ∑ Knowledge-Intensive Tasks\n13: Generate the output text ÀúYand compute\nthe loss L1 based on Xand P(Eq. 3)\n‚ñ∑ Continual Knowledge Learning\n14: Mask salient spans of Pfor the CKL pre-\ntraining and compute the loss L2 (Eq. 4)\n‚ñ∑ Model Optimization\n15: Compute the gradients and update model\nparameters Œò based on L1 and L2\n16: end for\n17: end while\n18: return Œò\nFor our model, the maximum length of both in-\nput and output sequences is set to1,024 for support-\ning examples to contain more tokens. We optimize\nthe model with a constant learning rate of2 √ó10‚àí5\nusing standard sequence-to-sequence cross-entropy\nloss. We apply the AdamW optimizer (Loshchilov\nand Hutter, 2019) with Œ≤1 = 0 .9, Œ≤2 = 0 .98,\nœµ = 1 √ó10‚àí6 to improve training stability (Liu\net al., 2019a). The weight decay coefficient is\n0.1. For testing, we select the checkpoint with\nthe highest validation performance. According to\nthe results shown in Figure 2(a), we set the en-\ntropy threshold Œ∑ as 4.0. The overall pipeline of\nour model is listed in Algorithm 1.\nSince the tasks of fact checking, slot filling, dia-\nlogue, and open-domain QA are specially designed\nbased on the knowledge from Wikipedia, we re-\nquire the search engine to retrieve the top-1 passage\nfrom the website https://en.wikipedia.org.\n741\nTask Families Tasks #Train #Validation #Test\nFact Checking FEVER (Thorne et al., 2018) 134,287 14,342 10,100\nSlot Filling\nT-REx (ElSahar et al., 2018) 2,999,272 26,833 5,000\nzsRE (Levy et al., 2017) 154,826 3,771 4,966\nDialogue WoW (Dinan et al., 2019) 63,734 3,054 2,944\nOpen-domain QA\nNQ (Kwiatkowski et al., 2019) 108,890 6,008 1,444\nTriviaQA (Joshi et al., 2017) 1,835,943 168,358 6,586\nHotpotQA (Yang et al., 2018) 88,869 5,600 5,569\nELI5 (Shuster et al., 2020) 804,370 18,037 600\nCommonsense QA\nCSQA (Talmor et al., 2019) 9,741 1,221 1,140\nSocialIQa (Sap et al., 2019) 33,410 1,954 2,059\nCosmosQA (Huang et al., 2019) 25,262 2,985 6,963\nPIQA (Bisk et al., 2020) 16,113 1,838 3,084\nCommonsense Reasoning\nNumerSense (Lin et al., 2020) 10,444 200 3,146\nWinoGrande (Sakaguchi et al., 2020) 40,398 1,267 1,767\nNatural Language Inference\nHellaSwag (Zellers et al., 2019) 39,905 10,042 10,003\nŒ±NLI (Bhagavatula et al., 2020) 169,654 1,532 3,059\nTable 6: The statistics of our 16 knowledge-intensive tasks.\nTasks Task Instructions\nFact Checking Verify the following claim\nSlot Filling Predict the missing fact\nOpen-domain\nQA Answer the following question\nCommonsense\nQA Answer the following question\nDialogue Response to the following dialogue\nNatural Language\nInference Inference on the following context\nCommonsense\nReasoning Reason about the following sentence\nTable 7: Task instructions for each task category.\nC Supplementary Experiments\nRealTime QA. Previous QA systems mostly as-\nsume that answers are static regardless of the time\nof query (Chen and Yih, 2020). In this section,\nwe use the REALTIME QA benchmark (Kasai\net al., 2022) to test models about real-time, instan-\ntaneous information. At each week, REALTIME\nQA will retrieve news articles and ~30 human-\nwritten, multiple-choice questions from news web-\nsites (CNN, THE WEEK, and USA Today), which\ncovers diverse topics such as politics, business,\nsports, and entertainment. We adopt the origi-\nModels\nREALTIME QA\nOriginal NOTA\nT5 40.0 33.3\nGPT-3 56.7 23.3\nRAG+DPR 10.0 16.7\nRAG+Google Search 63.3 50.0\nUniWeb +Google Search 66.7 56.7\nTable 8: Accuracy results for the questions at week\nfrom 2022/12/11 through 2022/12/17. We utilize DPR\nto retrieve top-5 documents from Wikipedia and use\nGoogle Search to retrieve top-5 news articles.\nnal and NOTA (none of the above) settings and\ntest our models over questions from 2022/12/11\nthrough 2022/12/17. The results are shown in Ta-\nble 8. Since one of the original choices is randomly\nreplaced with ‚Äúnone of the above‚Äù, the NOTA set-\nting results in a distinct performance degradation.\nBesides, due to the real-time nature of the ques-\ntions, only using DPR to retrieve texts from static\nWikipedia achieves worse results. Our UniWeb\nmodel with Google Search performs the best. This\nindicates that UniWeb can answer questions based\non the real-time information, rather than relying on\npast information from pretraining.\nSelf-Evaluation Criteria. To evaluate the model\nconfidence in task examples, we adopt the entropy\n742\nas criterion in Section 4.2.1. In this part, we test\nwith more kinds of criteria compared to the entropy\nfollowing Kadavath et al. (2022). First, we consider\na sample-enhanced prompting method, where we\ngenerate five samples with beam search and ask the\nmodel about the validity of the first sample with\nthe highest score. We show an example at below:\nQuestion: Who is the third\npresident of the United States?\nPossible Answer: James Monroe\nHere are some brainstormed ideas:\nThomas Jefferson\nJohn Adams\nThomas Jefferson\nGeorge Washington\nIs the possible answer:\n(A) True\n(B) False\nThe possible answer is:\nIf the model self-evaluate the possible answer is\nFalse, our model will leverage the search engine\nto access the web, otherwise not. We show the prob-\nability of predicting True depending on whether\nthe model gets the question correct in Figure 3(a).\nHowever, according to Kadavath et al. (2022), this\nself-evaluation method is mainly suitable for ques-\ntion answering tasks with short-form answers but\nbenefits less on question answering tasks with long-\nform answers. Second, we consider using loss as\nthe criterion to evaluate the model confidence. This\napproach is to generate a sample, and then look at\nthe model‚Äôs loss on this sample, averaged over all\ntokens, like the knowledge-intensive learning loss\n(Eq. 3). If the loss for an example is higher than a\nthreshold (e.g., 0.5), we consider that the model is\nunconfident about this example and we will query\nthe web to retrieve knowledge. In Figure 3(b), we\nshow the loss of samples that the model gets correct\nor incorrect.\nD Case Study\nIn Table 9, we present three examples from Triv-\niaQA (Joshi et al., 2017), CommonsenseQA (Tal-\nmor et al., 2019), and NumerSense (Lin et al.,\n2020). The first TriviaQA dataset is specially de-\nsigned based on the knowledge from Wikipedia.\nTherefore, we can observe that Wikipedia con-\ntains the most relevant passage about the topic\nÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ ÔøΩÔøΩÔøΩÔøΩ\nÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\n(a)\nÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ ÔøΩ\nÔøΩÔøΩÔøΩÔøΩ\nÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩ\nÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ\nÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ (b)\nFigure 3: (a) Probability of True for prompts in Hot-\npotQA; (b) Loss of samples in HotpotQA.\n‚ÄúUS nuclear reactor accident in 1979‚Äù. In addition,\nthe web can provide another source of knowledge\nabout this topic. Although CCNet covers this con-\ntent, it does not give a clear answer to this question\n(i.e., full name of the US nuclear reactor). The sec-\nond CommonsenseQA dataset involves questions\nrelated to commonsense knowledge going beyond\nWikipedia. Therefore, Wikipedia can only provide\na fuzzy description passage about ‚ÄúGuitar‚Äù. The\nweb and CCNet return diverse knowledge but the\npassage returned by search engine is more helpful.\nThe thrid NumerSense dataset requires models to\nreason about the number. For the third example,\nCCNet provides a passage with incorrect informa-\ntion. While, the web and Wikipedia return passages\nabout the rule of ‚Äútic-tac-toe‚Äù, which can result in\nthe correct answer ‚Äúthree‚Äù.\n743\nQuestion: Which US nuclear reactor had a major accident in 1979?\nGold Answer: Three Mile Island Unit 2 reactor\nTop-1 Wikipedia Passage Top-1 CCNet Passage Top-1 Web Passage\n... The Three Mile Island accident was\na partial meltdown of the Three Mile\nIsland, Unit 2 (TMI-2) reactor in\nPennsylvania, United States. It began at\n4 a.m. on March 28, 1979. It is the most\nsignificant accident in U.S. commercial\nnuclear power plant history. On the\nseven-point International Nuclear\nEvent Scale, it is rated Level 5 ‚Äì\nAccident with Wider Consequences...\nhttps://en.wikipedia.org/wiki\n/Three_Mile_Island_accident\n... The US and former Soviet Union\nhad been operating nuclear power for\n267 and 162 reactor-years respectively\nbefore a major accident occurred. At\nthe time of the Three Mile Island\naccident in 1979, the US had 52\nnuclear power stations, which had been\noperating for 267 reactor years, or\nan average of 5.1 years per reactor...\nhttps://chinadialogue.net/\narticle/show/single/en/5808\n-Chinese-nuclear-disaster-\nhighly-probable-by-2-3-\n... The Three Mile Island Unit\n2 reactor , near Middletown, Pa.,\npartially melted down on March\n28, 1979. This was the most se-\nrious accident in U.S. commercial\nnuclear power plant operating his-\ntory, although its small radioactive\nreleases had no detectable health\neffects on plant workers or the public...\nhttps://www.nrc.gov/reading\n-rm/doc-collections/fact-\nsheets/3mile-isle.html\nQuestion: What do people typically do while playing guitar?\nCandidate Answers: A. cry B. hear sounds C. singing D. arthritis E. making music\nGold Answer: singing\nTop-1 Wikipedia Passage Top-1 CCNet Passage Top-1 Web Passage\n... The guitar is a fretted musical instru-\nment that typically has six strings. It\nis usually held flat against the player‚Äôs\nbody and played by strumming or\nplucking the strings with the dominant\nhand, while simultaneously pressing se-\nlected strings against frets with the fin-\ngers of the opposite hand. A plec-\ntrum or individual finger picks may\nalso be used to strike the strings...\nhttps://en.wikipedia.org/\nwiki/Guitar\n... I was playing a brand-new game\nthat had no rules and nothing estab-\nlished. I was really shy about it at\nfirst, because I hadn‚Äôt looked out into\nthe world to find other people who,\nof course, had done things like this.\nI heard Fred Frith play, and I knew\nhe played his guitar with objects not\ntypically associated with the guitar...\nhttps://www.premierguitar.\ncom/articles/24026-janet-\nfeder-prepared-for-all-genres\n... Practicing the guitar regularly can en-\nhance your concentration and expand\nyour attention span. It takes an ade-\nquate focus to become an expert gui-\ntarist. Focusing becomes a habit for\nyour mind and will help you concentrate\nbetter on other everyday chores too...\nhttps://www.chasingsound.\ncom/posts/10-health-bene\nfits-of-playing-guitar\nQuestion: How do you win at tic-tac-toe get <mask> of your symbols in a row?\nGold Answer: three\nTop-1 Wikipedia Passage Top-1 CCNet Passage Top-1 Web Passage\n... Tic-tac-toe (American English),\nnoughts and crosses (Commonwealth\nEnglish), or Xs and Os (Canadian or\nIrish English) is a paper-and-pencil\ngame for two players who take turns\nmarking the spaces in a three-by-\nthree grid with X or O. The player\nwho succeeds in placing three of\ntheir marks in a horizontal, verti-\ncal, or diagonal row is the winner...\nhttps://en.wikipedia.org/\nwiki/Tic-tac-toe\n... You just make a 4x4 box instead of\na 3x3 box. Then the same rules ap-\nply, only you need to get 4 in a row\nto win. When playing, does putting\nmy symbol in the middle guarantee\nme winning? No. With both play-\ners playing optimally, the result is al-\nways a draw. How many X‚Äôs and\nO‚Äôs do I need to play tic tac toe on\na board game? Since the board it-\nself has nine spaces, I recommend that\nyou have nine for both X‚Äôs and O‚Äôs...\nhttps://www.wikihow.com/\nPlay-Tic-Tac-Toe\n... 1. The game requires two players, X\nand O. 2. The game board is a set 3x3\ngrid in which players will place their\nsymbol to claim that segment. 3. X typi-\ncally players first, then players alternate\nturns. 4. The goal is to claim three seg-\nments of the grid in a row, either hori-\nzontally, vertically, or diagonally. 5. No\nadditional sides can be added to the grid.\n6. The game is over either when one\nplayer achieves three segments in a row,\nor when the grid is filled without any-\none achieving three segments in a row....\nhttps://www.siammandalay.\ncom/blogs/puzzles/how-to-\nwin-tic-tac-toe-tricks-to\n-always-win-noughts-\ncrosses\nTable 9: Three qualitative example from TriviaQA, CommonsenseQA, and NumerSense. We present the top-1\nretrieved passages from Wikipedia, CCNet, and web. The words in red denote the keywords related to the question.\n744\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\n7\n‚ñ°\u0013 A2. Did you discuss any potential risks of your work?\n7\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\n1\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\n5\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\n5\n‚ñ° B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n‚ñ° B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n‚ñ° B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n‚ñ° B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n‚ñ°\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\nLeft blank.\nC ‚ñ°\u0013 Did you run computational experiments?\n5\n‚ñ°\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n745\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nAppendix\n‚ñ° C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNot applicable. Left blank.\n‚ñ°\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n5\nD ‚ñ°\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n‚ñ° D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n‚ñ° D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nNo response.\n‚ñ° D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n‚ñ° D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n‚ñ° D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n746",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8401473164558411
    },
    {
      "name": "Salient",
      "score": 0.6198978424072266
    },
    {
      "name": "Language model",
      "score": 0.6037372350692749
    },
    {
      "name": "Task (project management)",
      "score": 0.5901890397071838
    },
    {
      "name": "Information retrieval",
      "score": 0.4724809527397156
    },
    {
      "name": "ENCODE",
      "score": 0.43905553221702576
    },
    {
      "name": "Machine learning",
      "score": 0.4309746026992798
    },
    {
      "name": "Search engine",
      "score": 0.42540475726127625
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3993211090564728
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I70931966",
      "name": "Universit√© de Montr√©al",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210096250",
      "name": "Beijing Institute of Big Data Research",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I82880672",
      "name": "Beihang University",
      "country": "CN"
    }
  ],
  "cited_by": 5
}