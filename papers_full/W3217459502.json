{
  "title": "Pretrained Natural Language Processing Model for Intent Recognition (BERT-IR)",
  "url": "https://openalex.org/W3217459502",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2639548751",
      "name": "Vasima Khan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3081353786",
      "name": "Tariq Azfar Meenai",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6637134683",
    "https://openalex.org/W6609289286",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W6606714050",
    "https://openalex.org/W6600020421",
    "https://openalex.org/W6604009900",
    "https://openalex.org/W6683567821",
    "https://openalex.org/W6608040171",
    "https://openalex.org/W6637031373",
    "https://openalex.org/W2983070576",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2965460197",
    "https://openalex.org/W3035451444",
    "https://openalex.org/W2053463056",
    "https://openalex.org/W2399175133",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2956865104",
    "https://openalex.org/W2963608065",
    "https://openalex.org/W1550206324",
    "https://openalex.org/W2597655663",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W1550863320",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2951583236",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2522720655",
    "https://openalex.org/W2091693228",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W1492071093",
    "https://openalex.org/W2995852188",
    "https://openalex.org/W2963033987",
    "https://openalex.org/W2405622356",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W4288624561",
    "https://openalex.org/W2152763300",
    "https://openalex.org/W2766008290",
    "https://openalex.org/W2166183437",
    "https://openalex.org/W1972595521"
  ],
  "abstract": "Abstract Intent Recognition (IR) is considered a key area in Natural Language Processing (NLP). It has crucial usage in various applications. One is the Search Engine- Interpreting the context of text searched by the user improves the response time and helps the search engines give appropriate outputs. Another can be Social Media Analytics-Analysing profiles of users on different social media platforms has become a necessity in today’s applications like recommendation systems in the online world, digital marketing, and a lot more. Many researchers are using different techniques for achieving intent recognition but getting high accuracy in intent recognition is crucial. In this work, named BERT-IR, a pre-trained Natural Language Processing model called as BERT model, along with few add-ons, is applied for the task of Intent Recognition. We have achieved an accuracy of 97.67% on a widely used dataset which shows the capability and efficiency of our work. For comparison purposes, we have applied primarily used Machine Learning techniques, namely Naive Bayes, Logistic Regression, Decision Tree, Random Forest, and Gradient Boost as well as Deep Learning Techniques used for intent recognition like Recurrent Neural Network, Long Short Term Memory Network, and Bidirectional Long Short Term Memory Network on the same dataset and evaluated the accuracy. It is found out that BERT-IR’s accuracy is far better than that of the other models implemented.",
  "full_text": "Human-Centric Intelligent Systems\nVol.1(3-4); December (2021),pp. 66–74\nDOI: https://doi.org/10.2991/hcis.k.211109.001; eISSN 2667-1336\nhttps://www.atlantis-press.com/journals/hcis\nResearch Article\nPretrained Natural Language Processing Model\nfor Intent Recognition (BERT -IR)\nVasima Khan1,*, Tariq Azfar Meenai2\n1Department of Computer Science & Engineering, Sagar Institute of Science & Technology (SISTec), Bhopal, Madhya Pradesh, India\n2Department of Electronics & Communication, Smith Infotech Pvt. Ltd., Bhopal, Madhya Pradesh, India\nARTICLE INFO\nArticle History\nReceived 15 July 2021\nAccepted 25 October 2021\nKeywords\nIntent recognition\nintent detection\nnatural language processing\nBERT\ndeep learning\ndeep neural network\nABSTRACT\nIntent Recognition (IR) is considered a key area in Natural Language Processing (NLP). It has crucial usage in various applications.\nOne is the Search Engine- Interpreting the context of text searched by the user improves the response time and helps the search\nengines give appropriate outputs. Another can be Social Media Analytics-Analysing profiles of users on different social media\nplatforms has become a necessity in today’s applications like recommendation systems in the online world, digital marketing,\nand a lot more. Many researchers are using different techniques for achieving intent recognition but getting high accuracy in\nintent recognition is crucial. In this work, named BERT-IR, a pre-trained Natural Language Processing model called as BERT\nmodel, along with few add-ons, is applied for the task of Intent Recognition. We have achieved an accuracy of 97.67% on a widely\nused dataset which shows the capability and efficiency of our work. For comparison purposes, we have applied primarily used\nMachine Learning techniques, namely Naive Bayes, Logistic Regression, Decision Tree, Random Forest, and Gradient Boost\nas well as Deep Learning Techniques used for intent recognition like Recurrent Neural Network, Long Short Term Memory\nNetwork, and Bidirectional Long Short Term Memory Network on the same dataset and evaluated the accuracy. It is found out\nthat BERT-IR’s accuracy is far better than that of the other models implemented.\n© 2021The Authors. Publishing services by Atlantis Press International B.V .\nThis is an open access article distributed under the CC BY-NC 4.0 license (http://creativecommons.org/licenses/by-nc/4.0/).\n1. INTRODUCTION\n1.1. Overview\nIn a recent scenario, research work corresponding to the domain\nof NLP and Computational Linguistics undergoes drastic improve-\nm e n t sd u et ow h i c haw i d er a n g eo fa p p l i c a t i o n so ft h i sf i e l dh a s\nbeen evolved. Significant factors like the availability of enormous\ndata for training Machine Learning Models, enhancement capa-\nble enough model-building techniques, and tremendous growth\nin computational power caused these improvements. Natural Lan-\nguage Processing is a study of human-computer interactions which\ncomes under Artificial Intelligence [26]. It relates to the analysis of\ndata generated by natural language. Since human language contains\ncolloquialism, variability, ambiguity and is interpreted as per the\ncontext, it is associated with several issues in both spoken and\nwritten form. As the application areas in the domain of NLP are\nwidely increased, numerous algorithms are getting invented.\nThe intent defines the context of the text, which is usually a\ncombination of a verb and a noun. A few instances could be\nSearchRestaurant, OrderFood, and others [17]. Finding out the con-\ntext corresponding to the user’s text is known as Intent Recognition,\nalso known as Intent Classification or Detection. When we deal with\na Machine Learning scenario, Intent Recognition is a classification\n*Corresponding author. Email:drvasimakhan88@gmail.com\nPeer review under responsibility of KEO (Henan) Education Technology Co. Ltd\ntask in which there are predefined intents to which we classify user\ntext [5]. As already specified that human language contains several\nconstructs which are very complicated to handle, which makes this\ntask of intent recognition a highly complex problem [19].\nNumerous NLP applications, including Intent Recognition, use\npre-trained models with self-attention encoder architectures [7,17].\nSuch models are self-supervised trained on a massive amount of\ntext taken from Wikipedia [34]. After fine-tuning, these pre-trained\nmodels have been used in several downstream tasks, including NLP ,\nwhich have given breakthrough results. However, it has been shown\nin previous work [23,31] that certain deficiencies are present in\nterms of accuracy if fine-tuning is done directly for finding the\nintent. One possible reason could be the length of the text message\nbecause sometimes only keywords are not enough to detect the\nintent [2]. Another reason could be a large number of intents. Many\ntechniques are used to detect intent from text, from traditional\nmachine learning techniques like SVM, AdaBoost, and Logistic\nRegression to deep learning techniques like RNN, CNN, and LSTM.\nHowever, as conveyed earlier, performing this task of intent recog-\nnition with high accuracy is difficult.\n1.2. Author’s Contribution\nIn our research work, we have used a recent model known as\nBERT (Bidirectional Encoder Representations from Transformers),\nwhich is a language representation model [7,21,22]. This model\nis developed to pre-trained deep bidirectional representations [7].\nV. Khan and T.A. Meenai / Human-Centric Intelligent Systems 1(3-4) 66–74 67\nWe have applied this model with some add-ons while fine-tuning\nto the task of intent recognition. Although BERT is a simple and\nsuccessful model, using this for the task of intent recognition has\nbeen proposed by our work for the first time. Hence it is a significant\nauthor’s contribution with respect to research.\nAlso, there were many issues like data format compatibility, the\ndifference in the kind of words used for previous training in BERT\nand those which were present in our application data, acquiring\nhigh accuracy, and a lot more. After dealing with all the issues and\nperforming rigorous fine-tuning, we obtained very high accuracy\nand performance. This was the author’s contribution in terms of\nimplementation.\nAs we talk in terms of experimental view, our model’s outstanding\nperformance has been shown by the results we got. Our model has\ngiven excellent results compared to other previous approaches used\ni nt h et a s ko fi n t e n tR e c o g n i t i o n .T h i sw o r kc o u l db et h eb a s e l i n e\nf o ral o to fr e c e n tk e ya p p l i c a t i o na r e a sl i k eH u m a n - C o m p u t e r\nInteractions, Duplicate Question Problem, Stock Prediction and\nmany more.\n1.3. Organization of Research Article\nThe organization of the remaining article is as follows. In the next\nsection, previous research done has been explained. The baseline\nm o d e lu s e di no u rw o r kh a sb e e ne x p l a i n e db r i e f l yi nSection 3.\nSection 4discusses our approach for intent detection.Section 5out-\nlines the experimental details, including dataset used, data prepro-\ncessing, implementation setups, and model training. Experimental\nresults containing the error and comparative analysis are reported\nin Section 6.A tl a s t ,t h ep a p e ri sc o n c l u d e di nSection 7.A l s o ,w e\nhave mentioned the scope for future work in the same section.\n2. EXISTING APPROACHES\n2.1. Traditional Approaches\nDuring the last few years, various researchers have considered\nintent detection a Semantic Utterance Classification (SUC) task [6].\nInitially, a semantic recognition approach based on rules and statis-\ntical features-based classification techniques was advised for intent\ndetection [1]. Even though the rule-based technique is particular\nin terms of accuracy and needs comparatively much less data for\ntraining, it has many issues. The main problem with this method is\nthat it has to be reconstructed from scratch [17].\n2.2. ML Techniques\nStatistical Feature classification technique requires that out of the\nhuge text, key features need to be extracted [17]. If key features\na r ee x t r a c t e dm a n u a l l y ,i tc a u s e st h ec o s tt ob e c o m ev e r yh i g h ,a t\nt h es a m et i m ei tc a n n o tg i v e sa c c u r a t er e s u l t si nt e r m so ff e a t u r e s\nselected which ends up in sparse data issues. Naive Bayes [20],\nAdaboost [27], Support Vector Machine (SVM) [10], and logistic\nregression [9] are few traditional technique examples. Traditional\napproaches to intent recognition are not capable of finding the\nactual context of raw text data taking into account the amateurish-\nness of the text data [17].\n2.3. DL Approaches\nAs the advancement in the deep learning field has gradually\nhappened, Researchers applied many deep learning techniques\nin intent recognition problems, for instance, Convolution Neural\nNetworks (CNN), Recurrent Neural Networks (RNN), Long Short-\nTerm Memory (LSTM) Network, Gated Recurrent Unit (GRU),\nword embedding, Attention Mechanism and Capsule Neural\nNetworks [33]. In contrast to traditional techniques, deep learning\nmethods gave better results but still, there is excellent scope\nfor improvement in terms of performance. Work done in these\napproaches are as follows:\n2.3.1. CNN\nPrimitively, Image processing was the central area where CNN\nhas been used [15]. Afterward, several researchers applied it to\nNLP tasks and got good results as well. [14] proposed improved\nperformance model for text classification task using CNN. Further,\nCNN is used to recognize user queries by extracting features in the\nform of vectors [11]. This technique to extract features is better\nthan traditional feature extraction methods in terms of both per-\nformance and effort. Still, there are numerous drawbacks of CNN\nwith regards to representation due to which CNN is not considered\na good choice for intent recognition.\n2.3.2. RNN\nIn contrast to CNN, RNN can store a set of ordered words with the\nhelp of which it can learn relations among words corresponding\nto a context. Because of this ability of RNN, it is applied to solve\nthe intent recognition problem, which gives good results in few\ncases [4] .R N Nd o e sn o tg i v eg o o dp e r f o r m a n c ei nf e wc a s e sb e c a u s e\nit suffers from the problems of gradient vanishing or gradient\nexplosion.\n2.3.3. LSTM & GRU\n[12] proposes an approach that uses LSTM, an extension to RNN,\nwhich contains storage to governs the data to be kept or deleted.\nIntent Recognition task is accomplished using LSTM over Air Travel\nInformation System (ATIS) in a proposed work which shows better-\nment in error rate [24]. Furthermore, an extension of LSTM named\nG R Ui sa p p l i e dt ot h ei n t e n tr e c o g n i t i o nt a s k ,w h i c hc a nr e t a i nm o r e\nno. of ordered words [8]. [25]s h o w st h ec o m p a r i s o no fG R Ua n d\nLSTM for this task trained on the ATIS dataset that ends up in the\nfact that both are the same from a performance perspective, but\nLSTM has a more complex model than GRU.\n2.3.4. Word embeddings\nAnother technique has been continuously used in this task of intent\nrecognition, known as word embedding, which works on the con-\ncept of gradual learning and is hence capable of solving sparse data\nproblems [3]. [13] shown that when intent classification is done\nwith word embedding has improved descriptive capability. It has\nbeen demonstrated that using rich word embedding for addressing\nintent detection tasks gives better results [13]. It is used jointly with\nother approaches for intent recognition.\n68 V. Khan and T.A. Meenai / Human-Centric Intelligent Systems 1(3-4) 66–74\n2.4. Combined Approaches\nAs researchers are becoming aware that various machine and deep\nlearning approaches are improvising in addressing intent recogni-\ntion, numerous scholars worked in the direction of using a com-\nbination of these techniques for the same task. An approach is\nproposed that defines a self-attention mechanism that expresses\nword sequences using matrices [16].\nAn improvised version of LSTM is used, known as Bidirectional\nLSTM, which works by combining processing results in both left\nand right directions. In this approach, weighted summation of\nLSTM hidden layers is used to represent sentence relation [16].\nThis work can be applied for recognizing intent in case of multiple\nintents. [18] proposed another method for multi-intent detection\nin which word frequency-inverse document frequency (TF-IDF)\nis used along with word embedding to figure out the relationship\namong words in a sentence saved in a matrix.\nHere we have explained the previous work done in the domain of\nintent recognition. Some methods give better performance than\nb e f o r e ,b u tt h e r ei ss t i l lan e e df o ri m p r o v e m e n ti nt e r m so fa c c u -\nracy as far as intent recognition is concerned. In the next section,\nbaseline model used is described.\n3. BASELINE MODEL USED\nA novel model is known as “Bidirectional Encoder Representations\nfrom Transformers (BERT)” has been used as a baseline model for\nour research work. In contrast to previous models, BERT Model has\nthe unique feature of being bidirectional, due to which while under-\nstanding the text, it is considering context to its left and the right [7].\nSince it is bidirectional, its performance on various language tasks\nis outstanding. BERT is ideationally simple and experimentally\npowerful. Numerous applications of NLP are applying the BERT\nm o d e la si ti sv e r ys u c c e s s f u li nt e r m so fp e r f o r m a n c e[7]. BERT\nmodel works in two stages called “pre-training” and “fine-tuning” .\nIn the first stage, training has been done on a vast amount of text\nw h i c hi sn o tl a b e l e d .F u r t h e r ,d u r i n gt h es e c o n ds t a g eo fp r o c e s s i n g ,\nthe model is initialized with the weights of the pre-trained model\nand further fine-tuned with task-specific data, which is labeled.\nBaseline model used is explained briefly. In the coming section, our\napproach is described.\n4. OUR APPROACH: BERT -IR\nWe have used the BERT Model in combination with the DNN layer\nfor intent recognition [7]. In our approach BERT-IR, that is, BERT\nfor intent recognition, we use a pre-trained BERT model to be\napplied for intent recognition by fine-tuning it with some add-on\nl a y e r i n g .T h ed i s t i n c t i v ef e a t u r eo fB E R T - I Ri si t su s a g eo ft h eB E R T\nmodel for intent recognition and its way of fine-tuning, which is\ngiving outstanding results.\n4.1. Pre-Training\nAs already mentioned, our approach is using the BERT model for\npre-training. BERT model has done pre-training in two phases:\nTask 1: In the first phase, “Masked Language Modelling (MLM)”\nis used for training [29]. In this method, some tokens are masked\nout of the total, and a prediction of these tokens is made, which\nis initially taken from the Cloze task defined in ancient work [28].\nFinally, the softmax function is applied over the predicted value to\nconvert it into probabilities for further processing.\nTask 2: Next Sentence Prediction (NSP): Our downstream task is\nintent recognition, for which the connection between two sentences\nand between words of each sentence needs to be taken care of. That\nis why this task cannot be performed only via language model-\ning. For training purposes in this scenario, when the connection\nbetween sentences needs to be kept in mind, BERT pre-trained\nthe model, predicting the following sentence of a given sentence.\nFurther, while choosing the following sentence during training,\nfor half of the samples, we choose a random sentence for a given\nsentence, and for the rest half, we choose a sentence that follows the\ngiven sentence.\n4.2. Mechanism for Fine-tuning\nFine-tuning is performed by BERT-IR, which makes this work\nsuccessful in addressing intent recognition because of the Trans-\nformer’s “self-attention mechanism” along with the usage of correct\nd a t a .W eh a v eu s e dd e n s el a y e r sw i t hd r o p o u tt om a k eo u rm o d e lf o r\nthe task of intent recognition. Experiments have been performed\nto find out the values of hyperparameters to fine-tune our model.\nThese two stages are clubbed together, keeping in mind bidirec-\ntional processing within sequences for encoding. Model details are\ns h o w ni nt h ei m p l e m e n t a t i o ns e c t i o n .\n4.3. Model Architecture\nThe architecture of BERT is based on a bidirectional version of\nthe Transformer described in previous work [30]. Nevertheless, in\nBERT, the transformer encoder has multiple layers. Since in our\nmethod BERT-IR, BERT and Transformer are implemented in the\nsame way as described previously [7,30], we are not describing the\nwhole structure and processing of these approaches in detail.\nThere are two versions of the BERT model: Base Version and Large\nVersion. In the base version, twelve layers are implemented with\nhidden sizes like 768 and twelve attention heads leading to the\n110M model’s parameter. In contrast to the base version, twenty-\nfour layers and the hidden size as 1024 and sixteen attention heads\nare used in a large size model ends up in 340M parameters. The base\nv e r s i o ni sc h o s e nf o ro u rw o r kt oj u s t i f yt h ec o m p a r i s o n s .\nThe overall architecture of our approach is shown inFigure 1.A s\ninput, we have used sentence pair representation in which we pro-\nvide the input text and output label sequentially. A “sentence” could\nbe any random sequence of text despite a meaningful one during\nour consideration. The text containing the ordered collection of\nwords along with the label is known as “sequence” . The pre-trained\nmodel is trained on word-piece embedding containing 30,522 token\nvocabularies [32]. Standard notations are used in which unique\ntoken [CLS] is for specifying starting of sequence, and as we have\nclubbed two sentences in a sequence, [SEP] is used to separate\nthese two sentences. The critical value of word embedding, namely\nw1 to w4, is calculated by adding a token, segment, and position\nV. Khan and T.A. Meenai / Human-Centric Intelligent Systems 1(3-4) 66–74 69\nFigure 1 Model architecture.\nembedding, taken as input from the text. These are pre-trained\nusing BERT which outputs a bunch of word vectors (specified as\nH1 to H7) for masked language modelling. This part shows the\npre-training phase. Fine-tuning phase is shown as the right part\nof Figure 1. Here, we form the embedding from the actual input\nand output of our task that is sentence along with the actual intent,\nnamely e1 to e4. The combination of these two phases forms the\nfinal model which performs the required task as shown inFigure 1.\n5. EXPERIMENTAL DETAILS\n5.1. Research Data\nOur research presented the performance of our proposed approach\nby comparing our results with few previous ML and DL models\nnamed Naive Bayes, Logistic Regression, Decision Tree, Random\nForest, Gradient Boost, CNN, RNN, LSTM, GRU, and a combined\na p p r o a c h .W eh a v ea p p l i e dt h e s em o d e l sa n do u rm o d e lo na\nwell-known dataset called “SNIPS Natural Language Understand-\ning bench-mark1 (SNIPS-NLU)” .Table 1carries the dataset details.\nOur dataset consists of a bulk of spoken language text collected\nfrom several sources to make it closer to actual lingual text. The\ntraining of the acoustic model takes a considerable amount of\nlingual data, which is a transcript of an audio clip of extremely\nlong duration. To make training data versatile, it is assembled\nfrom numerous random sources. To remove errors from the data,\ncorrespondence of audio and transcript is checked. After all this\nprocessing has been done, the dataset becomes precise in a form\ncompatible with the training process. Further, three sets have been\nformed from the overall dataset that is training set, testing set, and\nvalidation set.\nSeven intents have different complexity in this dataset [33]:\nI. SearchCreativeWork (For instance: Can you show me some\nfine artwork)\nII. GetWeather (For instance: It seems Bhopal will have heavy rain\ntoday)\nIII. BookRestaurant (For instance: I am in a mood to have Chinese\nfood)\nIV . PlayMusic (For instance: Let us listen to Bollywood songs)\nV . AddToPlaylist (For instance: This track should be present\nw h i l ey o ua r et r a v e l i n g )\nTable 1 Dataset detail\nDataset “SNIPS-NLU”\nVocab size 30,522\nMaximum position embeddings 512\nIntents 7\nTraining samples 13,084\nValidation samples 700\nTest samples 700\nVI. RateBook (For instance: In contrast to the previous one, I like\nSydney Sheldon’s recent addition very much)\nVII. SearchScreeningEvent (For instance: I want you to see the\ntimings of Sharukh Khan’s show in Canada next month)\n5.2. Text Preprocessing\nIn context to the lingual text data collected from random sources,\npreprocessing is necessary [29]. Preprocessing is needed since this\nkind of data is usually not in a form to which we can directly apply\nthe training process to make the model. Nevertheless, as far as\nour data is concerned, it is already defined in terms of informal\ntexts and impurities. So, we needed to perform few basic steps for\npreprocessing, like removing special characters. we have done three\nimportant steps in preprocessing:\n1. Tokenize the data\n2. Converting tokens into numbers\n3. Padding is added\nBesides these essential steps, since we were dealing with text in only\nEnglish, text in other languages has been removed from the dataset\nusing the available tools. In addition to this, messages containing\nless than three words have been eliminated from the dataset. Fur-\nthermore, the same messages present in the data are taken only once.\nSo, we have assigned an index to every message to avoid repetition.\n5.3. Model’s Summary\nWhile constructing our model, Bert layer is used as the base layer.\nFor taking input data, we have used an input layer. After the BERT\nlayer, we have included two dense layers along with dropout. For\nclubbing the Bert layer with the layers added afterwards, a lambda\nlayer included in between.\n70 V. Khan and T.A. Meenai / Human-Centric Intelligent Systems 1(3-4) 66–74\nTable 2 Our model’s summary\nLayer type Output shape Param\nInput layer [(None, 38)] 0\nBert model layer (None, 38, 768) 108890112\nLambda layer (None, 768) 0\nDropout layer (None, 768) 0\nDense layer (None, 768) 590592\nDropout layer (None, 768) 0\nDense layer (None, 7) 5383\nTotal params: 109,486,087; Trainable params: 109,486,087 height; Non-trainable\nparams: 0.\nTable 3 Hyperparameters used in other ML models\nModel name Hyper parameter Values taken\nNaive Alpha 1.0\nbayes Algorithm Gaussian\nLogistic C 1.0\nregression Max. iteration 50, 100, 150\nPenalty Euclidean distance (L2)\nTolerance 0.01, 0.001, 0.0001\nDecision tree Max. depth 10, 20, 50,8 0\nRandom Max. depth 10, 20, 50,8 0\nforest No. of trees 3, 5, 10,1 5\nGradient Max. depth 10, 20, 50,8 0\nboost No. of trees 3, 5, 10,1 5\nLearning rate 0.1, 0.01, 0.001, 0.05, 0.005\nTolerance 0.01, 0.001, 0.0001\nSeveral combinations of different kind of layers are tried out but\nt h ed e m o n s t r a t e dm o d e li sf o u n do u tt ob et h eb e s ta sf a ra sp e r f o r -\nm a n c ei sc o n c e r n e d .O u rm o d e l ’ ss u m m a r yi ss h o w ni nTable 2.\n5.4. Hyperparameters Used for\nImplementation\nAs mentioned above, we have implemented five Machine learning\nand three Deep Learning techniques and our approach on the\nsame dataset. Here, we have specified the values of the hyperpa-\nrameters corresponding to Machine Learning models and Deep\nLearning models used, which are considered for experimental work\nin Tables 3& 4. Also, values of hyperparameters taken for tuning\nused in our model are shown inTable 5. The best value has been\ns h o w ni nb o l dt e x ti nt h ef o l l o w i n gT a b l e s .\n5.5. Training of Model\nModels based on the previously specified techniques and our\napproach have been trained using predefined libraries, namely\nScikit-learn, Tensorflow, and Keras. For training in BERT-IR,\nwe have used Adam optimizer with a learning rate of 0.00005.\nA validation split of 0.1 has been used for choosing the best value\nof various hyperparameters. To detect intents, we set no. of epochs\nas 50 and a batch size as 16. Intending to eliminate the problem of\noverfitting, we have done dropout in the Dense layer with a dropout\nrate of 0.1 and early stopping with two epochs as patience.\nW h i l ef i n e - t u n i n gt h em o d e l ,w eh a v ec h e c k e dt h ep e r f o r -\nmance measures of the model by putting different values to the\nhyperparameters shown above inTable 4. Table 6demonstrates this\nprocess of fine-tuning concerning the dropout rate as an illustration.\nIt is found out that we have got the best results for a dropout rate\nTable 4 Hyperparameters used in other DL models\nModel name Hyper parameter Values taken\nRNN Learning rate 0.00001, 0.0001, 0.0005\nOptimizer SGD, Adam,R M S\nActivatin function ReLu, SeLu, Sigmoid, Tanh\nValidation split 10%, 20%, 30%\nBatch size 8, 16,2 4\nNo. of epochs 10, 20, 30,5 0\nLoss optimizer Sparse categorical crossentropy\nLSTM Learning rate 0.00001, 0.0001, 0.0005\nOptimizer SGD, Adam,R M S\nActivatin function ReLu, SeLu, Sigmoid, Tanh\nStopping condition Early stopping with patience = 2\nValidation split 10%, 20%, 30%\nBatch size 8, 16,2 4\nNo. of epochs 10, 20, 30,5 0\nLoss optimizer Sparse categorical crossentropy\nBidirectional Learning rate 0.00001, 0.0001, 0.0005\nLSTM Optimizer SGD, Adam,R M S\nActivatin function ReLu, SeLu, Sigmoid, Tanh\nStopping condition Early stopping with patience = 2\nValidation split 10%, 20%, 30%\nBatch size 8, 16,2 4\nNo. of epochs 10, 20, 30, 50\nLoss optimizer Sparse categorical crossentropy\nTable 5 Hyperparameters used in our approach, BERT-IR\nHyper parameter Values\nLearning rate 0.0005, 0.005, 0.001, 0.0001\nOptimizer Adam\nActivation function ReLU, SeLU, Sigmoid, Tanh\nStopping condition\nwhile training\nEarly stopping with monitoring parameter =\nValidation accurac yand patience =2\nValidation split 10%, 20%, 30%\nBatch size 8, 16,2 4\nNo. of epochs 5, 10, 20, 50\nLoss optimization Sparse categorical crossentropy\nTable 6 BERT-IR’s fine tuning based on dropout rate\nDropout rate Accuracy Average Average Average\nprecision recall F1 score\n10% 97.67% 97.85% 97.72% 97.77%\n20% 97.57% 97.78% 97.64% 97.64%\n30% 97.00% 97.18% 97.15% 97.08%\n50% 97.00% 97.09% 97.21% 97.08%\nof 10%. Similarly, we have fine-tuned our model concerning every\nhyperparameter.\n6. EXPERIMENTAL RESULTS\n6.1. Metrics Used\nS i n c et h et a s kI Rc a nb ec o n s i d e r e da sac l a s s i f i c a t i o nt a s k ,w eh a v e\nused the following metrics to access our approach:\nA c c u r a c y :A c c u r a c yi st h ep e r c e n t a g eo ft i m e sw ea r ep r e d i c t i n g\ncorrectly. In terms of a mathematical equation, we can define it as:\nAccuracy = (TP + TN)/(TP + TN + FP + FN) (1)\nPrecision (P): Precision is the percentage of correct predictions out\nof total positive predictions. In terms of a mathematical equation,\nwe can define it as:\nPrecision = TP/(TP + FP) (2)\nV. Khan and T.A. Meenai / Human-Centric Intelligent Systems 1(3-4) 66–74 71\n(a) Train Data (b) Test Data\nFigure 2 Number of texts for each intent.\nTable 7 BERT-IR’s intent wise performance\nIntents Precision Recall F1 score\nSearchCreativeWork 94.50% 100.00% 97.17%\nGetWeather 100.00% 99.99% 100.00%\nBookRestaurant 99.98% 100.00% 99.98%\nPlayMusic 100.00% 89.71% 94.58%\nAddToPlaylist 98.92% 100.00% 99.45%\nRateBook 99.97% 99.03% 99.51%\nSearchScreeningEvent 90.26% 95.32% 92.72%\nFigure 3 Number of true positive for each intent in test data.\nRecall (R): Recall is the percentage of correct predictions out of total\nactual positive predictions. In terms of a mathematical equation, we\ncan define it as:\nRecall = TP/(TP + FN) (3)\nF1 Score (F1): F1 score is the measure of the balance between\nprecision and recall. In terms of a mathematical equation, we can\ndefine it as:\nF1 Score= 2 ∗ (Precision ∗ Recall)/(Precision + Recall) (4)\nwhere, TP = True Positive\nTN = True Negative\nFP = False Positive\nFN = False Negative\n6.2. BERT -IR’s Performance\nThe above-shown results show that using BERT-IR is superior to\nthe previous work. To present our work clearly, it is analyzed from\nanother perspective. For that, metrics of our model for individual\nintents in the dataset are demonstrated.Figure 2aand 2b show the\ntotal number of text corresponding to each intent present in the\ntrain and test dataset, respectively.\nThe Table 7shows the precision, recall, and F1 score of individual\nintents of our model. Also, fromTable 7,w ec a ns a yt h a tf e wl a b e l s\nare hard to predict compared to the others. Some text examples are\nclassified wrongly as some intent instead of the actual one due to\nsimilarity between the intents. For example, many times, the intent\ntype “PlayMusic” is predicted as “ AddToPlaylist” . It is demonstrated\nin Figure 3, which tells the number of true positives for each intent\nwhile testing.\n6.3. Comparative Analysis\nIn this research, we experimented with various machine learning\nand deep learning models mentioned above and our approach\nBERT-IR on the SNIPS English dataset. The comparison of our\nmodel with other models from a performance’s point of view is\nrepresented in the tables below.\nBased on the comparison demonstrated in theTables 8and 9,o u r\nmodel BERT-IR is far more potent in terms of performance than the\no t h e rm o d e l sw eh a v ei m p l e m e n t e d .B E R T - I Rh a sg o tat e s ta c c u r a c y\nof 97.67%, which is the highest among all other models.\nAlso, Figures 4a–4d and 5a–5d shows the graphs for comparing\nthe Accuracy, Recall, Precision, and F1-Score of various ML and\nDL models implemented in our work with our model respectively.\nI tc a nb es e e nf r o mt h eg r a p h st h a to u rm o d e li sh a v i n gt h eb e s t\nperformance with respect to all the metrics in comparison with the\nother ML and DL models implemented.\n72 V. Khan and T.A. Meenai / Human-Centric Intelligent Systems 1(3-4) 66–74\n(a) Test Accuracy (b) Average Precision\n(c) Average Recall (d) Average F1-Score\nFigure 4 Comparison of metrics of ML models and BERT-IR.\n(a) Test Accuracy (b) Average Precision\n(c) Average Recall (d) Average F1-Score\nFigure 5 Comparison of metrics of DL models and BERT-IR.\nV. Khan and T.A. Meenai / Human-Centric Intelligent Systems 1(3-4) 66–74 73\nTable 8 Comparison of our model with other DL models\nModel name Test accuracy Average\nprecision\nAverage\nrecall\nAverage\nF1 score\nRNN 92.83% 93.18% 93.25% 93.08%\nLSTM 94.32% 94.40% 94.37% 94.34%\nBidirectional LSTM 88.15% 88.44% 88.56% 87.97%\nBERT-IR 97.67% 97.85% 97.72% 97.77%\nTable 9 Comparison of our model with other ML models\nModel name Test accuracy Average\nprecision\nAverage\nrecall\nAverage\nF1 score\nNaive Bayes 95.28% 95.30% 95.28% 95.24%\nLogistic regression 95.85% 95.90% 96.01% 95.89%\nDecision tree 93.0% 93.24% 93.01% 93.11%\nRandom forest 95.42% 96.23% 96.43% 96.28%\nGradient boost 95.28% 95.48% 95.28% 95.36%\nBERT-IR 97.67% 97.85% 97.72% 97.77%\n7. CONCLUSION\nIn our approach, namely BERT-IR, we have applied a pre-trained\nnatural language processing model for intent recognition. As per\nour knowledge and understanding, this is the first time the BERT\nmodel is used for intent recognition with the add-ons we have done\nin this work. We have achieved an accuracy of 97.67%, which is\nvery high compared to the previous work done for this task. For\ncomparing the performance of our approach, we have implemented\nmany other machine learning models. It is found out from the\nresults that our approach has given high performance than other\nmodels in terms of various performance metrics used. Furthermore,\nour model is easy to use and also helpful when the data is not very\nhuge as the dataset used for training in our work is medium-sized.\nIn the future, we would like to extend our work for implementing\nchatbots which is a handy and vital task in the field of Natural\nLanguage Processing in the current scenario.\nCONFLICTS OF INTEREST\nThe authors declare they have no conflicts of interest.\nAUTHORS’ CONTRIBUTION\nVasima Khan and Tariq Azfar Meenai both come up with the\nidea of using BERT for Intent Recognition. Vasima Khan devel-\noped the theory and the algorithm that can be implemented. Tariq\nAzfar Meenai implemented the work using the algorithm. Both the\nauthors analysed the work done. Then they discussed the results and\ncontributed to the final manuscript.\nREFERENCES\n[1] A.S. Ahmad, M.Y. Hassan, M.P . Abdullah, H.A. Rahman, F. Hussin,\nH. Abdullah, et al., A review on applications of ANN and SVM for\nbuilding electrical energy consumption forecasting, Renewable and\nSustainable Energy Reviews 33 (2014), 102–109.\n[2] S .B a o ,H .H e ,F .W a n g ,H .W u ,H .W a n g ,P L A T O :P r e - t r a i n e dd i a l o g u e\ngeneration model with discrete latent variable, Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics,\nAssociation for Computational Linguistics, Online, 2020, pp. 85–96.\n[3] Y. Bengio, R. Ducharme, P . Vincent, C. Janvin, A neural probabilistic\nlanguage model, The journal of machine learning research 3 (2003),\n1137–1155.\n[4] A. Bhargava, A. Celikyilmaz, D. Hakkani-Tür, R. Sarikaya, Easy con-\ntextual intent prediction and slot detection, 2013 IEEE international\nconference on acoustics, speech and signal processing, IEEE, Vancou-\nver, BC, Canada, 2013, pp. 8337–8341.\n[5] A. Celikyilmaz, D. Hakkani-Tur, G. Tur, A. Fidler, D. Hillard,\nExploiting distance based similarity in topic models for user\nintent detection, 2011 IEEE Workshop on Automatic Speech\nRecognition & Understanding, IEEE, Waikoloa, HI, USA, 2011,\npp. 425–430.\n[6] Y.N. Dauphin, G. Tur, D. Hakkani-Tur, L. Heck, Zero-shot learning\nfor semantic utterance classification, arXiv preprint arXiv:1401.0509,\n2013.\n[ 7 ] J .D e v l i n ,M . W .C h a n g ,K .L e e ,K .T o u t a n o v a ,B e r t :P r e - t r a i n i n go f\ndeep bidirectional transformers for language understanding, arXiv\npreprint arXiv:1810.04805, 2018.\n[8] R. Dey, F.M. Salem, Gate-variants of gated recurrent unit (GRU)\nneural networks, 2017 IEEE 60th international midwest symposium\non circuits and systems (MWSCAS), IEEE, Boston, MA, USA, 2017,\npp. 1597–1600.\n[9] A. Genkin, D.D. Lewis, D. Madigan, Large-scale bayesian logis-\ntic regression for text categorization, technometrics 49 (2007),\n291–304.\n[10] P . Haffner, G. Tur, J.H. Wright, Optimizing SVMs for complex call clas-\nsification, 2003 IEEE International Conference on Acoustics, Speech,\nand Signal Processing, 2003. Proceedings.(ICASSP’03)., IEEE, Hong\nKong, China, 2003, pp. I–I.\n[11] H.B. Hashemi, A. Asiaee, R. Kraft, Query intent detection using con-\nvolutional neural networks, International Conference on Web Search\nand Data Mining, Workshop on Query Understanding, ACM, 2016,\npp. 1–5.\n[12] S .H o c h r e i t e r ,J .S c h m i d h u b e r ,L o n gs h o r t - t e r mm e m o r y ,N e u r a lc o m -\nputation 9 (1997), 1735–1780.\n[13] J.K. Kim, G. Tur, A. Celikyilmaz, B. Cao, Y.Y. Wang, Intent detection\nusing semantically enriched word embeddings, 2016 IEEE Spoken\nLanguage Technology Workshop (SLT), IEEE, San Diego, CA, USA,\n2016, pp. 414–419.\n[14] Y. Kim, Convolutional neural networks for sentence classification,\narXiv, 2014.\n[15] Y. LeCun, L. Bottou, Y. Bengio, P . Haffner, Gradient-based learning\napplied to document recognition, Proceedings of the IEEE 86 (1998),\n2278–2324.\n[16] Z. Lin, M. Feng, C.N. dos Santos, M. Yu, B. Xiang, B. Zhou, et al.,\nA structured self-attentive sentence embedding, 5th International\nConference on Learning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings. OpenReview.net.\nAvailable from: https://openreview.net/forum?id=BJC_jUqxe.\n[17] J .L i u ,Y .L i ,M .L i n ,R e v i e wo fi n t e n td e t e c t i o nm e t h o d si nt h eh u m a n –\nmachine dialogue system, Journal of Physics: Conference Series, IOP\nPublishing, 1267 (2019), 012059.\n[18] Q .L i u ,J .W a n g ,D .Z h a n g ,Y .Y a n g ,N .W a n g ,T e x tf e a t u r e se x t r a c t i o n\nbased on TF-IDF associating semantic, 2018, pp. 2338–2343.\n[19] T.L. Luong, M.S. Cao, D.T. Le, X.H. Phan, Intent extraction from social\nmedia texts using sequential segmentation and deep learning models,\n2017 9th International Conference on Knowledge and Systems Engi-\nneering (KSE), IEEE, Hue, Vietnam, 2017, pp. 215–220.\n[20] A. McCallum, K. Nigam, A comparison of event models for\nnaive bayes text classification, AAAI-98 workshop on learning\nfor text categorization, AAAI Press, Madison, Wisconsin, 1998,\npp. 41–48.\n[21] M.E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nL. Zettlemoyer, Deep contextualized word representations, Pro-\nceedings of the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), Association\nfor Computational Linguistics, New Orleans, Louisiana, 2018,\npp. 2227–2237.\n74 V. Khan and T.A. Meenai / Human-Centric Intelligent Systems 1(3-4) 66–74\n[22] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, Improving\nlanguage understanding with unsupervised learning, Technical report,\nOpenAI, 2018.\n[23] H. Rashkin, E.M. Smith, M. Li, Y.L. Boureau, Towards empathetic\nopen-domain conversation models: a new benchmark and dataset,\nProceedings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, Association for Computational Linguistics,\nFlorence, Italy, 2019, pp. 5370–5381.\n[24] S. Ravuri, A. Stolcke, Recurrent neural network and LSTM models\nfor lexical utterance classification, Sixteenth Annual Conference of\nthe International Speech Communication Association, Dresden, Ger-\nmany, 2015, pp. 135–139.\n[25] S. Ravuri, A. Stolcke, A comparative study of recurrent neural network\nmodels for lexical domain classification, 2016 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP),\nIEEE, Shanghai, China, 2016, pp. 6075–6079.\n[26] F. Ren, Y. Bao, A review on human-computer interaction and intel-\nligent robots, International Journal of Information Technology &\nDecision Making 19 (2020), 5–47.\n[27] R.E. Schapire, Y. Singer, BoosTexter: a boosting-based system for text\ncategorization, Machine learning 39 (2000), 135–168.\n[28] W .L. Taylor, “cloze procedure”: a new tool for measuring readability,\nJournalism quarterly 30 (1953), 415–433.\n[29] O . T .T r a n ,T . C .L u o n g ,U n d e r s t a n d i n gw h a tt h eu s e r ss a yi nc h a t b o t s :\na case study for the Vietnamese language, Engineering Applications of\nArtificial Intelligence 87 (2020), 103322.\n[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez,\net al., Attention is all you need, Advances in neural information\nprocessing systems, Curran Associates, Inc., Long Beach, CA, USA,\n2017, pp. 5998–6008.\n[31] T. Wolf, V . Sanh, J. Chaumond, C. Delangue, Transfertransfo: a trans-\nfer learning approach for neural network based conversational agents,\narXiv preprint arXiv:1901.08149, 2019.\n[32] Y. Wu, M. Schuster, Z. Chen, Q.V . Le, M. Norouzi, W . Macherey, et al.,\nGoogle’s neural machine translation system: bridging the gap between\nhuman and machine translation, arXiv preprint arXiv:1609.08144,\n2016.\n[33] C .Z h a n g ,Y .L i ,N .D u ,W .F a n ,P .Y u ,J o i n ts l o tf i l l i n ga n di n t e n t\ndetection via capsule neural networks, Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, Associa-\ntion for Computational Linguistics, Florence, Italy, 2019, 5259–5267.\n[34] Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,\net al., Aligning books and movies: towards story-like visual explana-\ntions by watching movies and reading books, 2015 IEEE International\nConference on Computer Vision (ICCV), IEEE, Santiago, Chile, 2015,\npp. 19–27.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7047236561775208
    },
    {
      "name": "Natural language processing",
      "score": 0.6582027673721313
    },
    {
      "name": "Language model",
      "score": 0.49677878618240356
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48570379614830017
    },
    {
      "name": "Speech recognition",
      "score": 0.4764326214790344
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.4235595464706421
    },
    {
      "name": "History",
      "score": 0.1174086332321167
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 15
}