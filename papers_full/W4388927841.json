{
  "title": "nach0: Multimodal Natural and Chemical Languages Foundation Model",
  "url": "https://openalex.org/W4388927841",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2470781784",
      "name": "Livne, Micha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287569552",
      "name": "Miftahutdinov, Zulfat",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3162211506",
      "name": "Tutubalina, Elena",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4294982374",
      "name": "Kuznetsov, Maksim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3001058804",
      "name": "Polykovskiy, Daniil",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Brundyn, Annika",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Jhunjhunwala, Aastha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4289060666",
      "name": "Costa, Anthony",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3089404883",
      "name": "Aliper, Alex",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224846694",
      "name": "Aspuru-Guzik, Alan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2590144500",
      "name": "Zhavoronkov, Alex",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2599674900",
    "https://openalex.org/W4323709074",
    "https://openalex.org/W2489487449",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W4226064176",
    "https://openalex.org/W2561675875",
    "https://openalex.org/W2607303097",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W4212774754",
    "https://openalex.org/W4205807230",
    "https://openalex.org/W4376166811",
    "https://openalex.org/W2533774353",
    "https://openalex.org/W4239019441",
    "https://openalex.org/W4389524022",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3168090480",
    "https://openalex.org/W4307079201"
  ],
  "abstract": "Large Language Models (LLMs) have substantially driven scientific progress in various domains, and many papers have demonstrated their ability to tackle complex problems with creative solutions. Our paper introduces a new foundation model, nach0, capable of solving various chemical and biological tasks: biomedical question answering, named entity recognition, molecular generation, molecular synthesis, attributes prediction, and others. nach0 is a multi-domain and multi-task encoder-decoder LLM pre-trained on unlabeled text from scientific literature, patents, and molecule strings to incorporate a range of chemical and linguistic knowledge. We employed instruction tuning, where specific task-related instructions are utilized to fine-tune nach0 for the final set of tasks. To train nach0 effectively, we leverage the NeMo framework, enabling efficient parallel optimization of both base and large model versions. Extensive experiments demonstrate that our model outperforms state-of-the-art baselines on single-domain and cross-domain tasks. Furthermore, it can generate high-quality outputs in molecular and textual formats, showcasing its effectiveness in multi-domain setups.",
  "full_text": "- R X U Q D O \u0003 1 D P H\nnach0: Multimodal Natural and Chemical Languages\nFoundation Model†\nMicha Livne,a† Zulfat Miftahutdinov,b† Elena Tutubalina,c† Maksim Kuznetsov,b† Daniil\nPolykovskiy,b Annika Brundyn,a Aastha Jhunjhunwala,a Anthony Costa,a Alex Aliper,d Alán\nAspuru-Guzik,e and Alex Zhavoronkovc‡\nLarge Language Models (LLMs) have substantially driven scientific progress in various domains, and\nmany papers have demonstrated their ability to tackle complex problems with creative solutions. Our\npaper introduces a new foundation model, nach0, capable of solving various chemical and biological\ntasks: biomedical question answering, named entity recognition, molecular generation, molecular\nsynthesis, attributes prediction, and others. nach0 is a multi-domain and multi-task encoder-decoder\nLLM pre-trained on unlabeled text from scientific literature, patents, and molecule strings to incor-\nporate a range of chemical and linguistic knowledge. We employed instruction tuning, where specific\ntask-related instructions are utilized to fine-tune nach0 for the final set of tasks. To train nach0\neffectively, we leverage the NeMo framework, enabling efficient parallel optimization of both base\nand large model versions. Extensive experiments demonstrate that our model outperforms state-of-\nthe-art baselines on single-domain and cross-domain tasks. Furthermore, it can generate high-quality\noutputs in molecular and textual formats, showcasing its effectiveness in multi-domain setups.\n1 Introduction\nLarge-scale pre-training of language models (LMs), such as\nBERT1, T5 2, BART 3 and GPT 4, on vast amounts of text data\nhas yielded impressive results on a variety of natural language\nprocessing (NLP) tasks. These models’ success can be attributed\nto their ability to learn deeply contextualized representations of\ninput tokens through self-supervision at scale1. Recently , founda-\ntion models have built upon the concept of self-supervised learn-\ning by pre-training a single model over unlabeled data that can\nbe easily adapted to any task5.\nThe application of neural network architectures and LMs has\nsignificantly advanced the field of chemistry , particularly in\ndomain-specific information retrieval, drug development, and\nclinical trial design 6–15. These developments include neu-\nral molecular fingerprinting, generative approaches to small\nmolecule design 11–13, prediction of pharmacological properties,\na NVIDIA, 2788 San Tomas Expressway, Santa Clara, 95051, CA, US\nb Insilico Medicine Canada Inc., 3710-1250 René-Lévesque west, Montreal, Quebec,\nCanada\nc Insilico Medicine Hong Kong Ltd., Unit 310, 3/F, Building 8W, Phase 2, Hong Kong\nScience Park, Pak Shek Kok, New Territories, Hong Kong\nd Insilico Medicine AI Ltd., Level 6, Unit 08, Block A, IRENA HQ Building, Masdar City,\nAbu Dhabi, United Arab Emirates\ne University of Toronto, Lash Miller Building 80 St. George Street, Toronto, Ontario,\nCanada. Email: alan@aspuru.com\n† These authors contributed equally to this work.\n‡ Email: alex@insilicomedicine.com\nand drug repurposing13,14. The clinical development of a drug is\na time and money consuming process that typically requires sev-\neral years and a billion-dollar budget to progress from phase 1\nclinical trials to the patients 16. The use of state-of-the-art neu-\nral network approaches and language models has the potential to\nfacilitate the drug development process considerably .\nA number of LMs have been proposed for the biomedical do-\nmain, utilizing a variety of model families: for instance, re-\nsearchers have developed BioBERT 17, based on BERT with 110\nmillion parameters, and SciFive, based on T5-base and T5-large\nwith 220 and 770 million parameters respectively , using biomedi-\ncal literature from PubMed. NVIDIA has also developed BioMega-\ntron models in the biomedical domain using a more extensive\nset of PubMed-derived free text, ranging from 345 million to 1.2\nbillion parameters. However, the datasets used in these mod-\nels cover mainly biomedical natural language texts and contain\nbiomedical named entities like drugs, genes, and cell lines names\nbut omit important chemical structure descriptions in SMILES\nformat. Enriching biomedical datasets with chemical structures\nis an important and challenging task. Recently , LMs such as\nGalactica18, based on Transformer architecture in a decoder-only\nsetup19 with 120 billion parameters in its largest setup, and\nMolT520, based on T5-base and T5-large, were proposed to ad-\ndress this limitation. Both modes were pre-trained with natu-\nral language and chemical data, creating a shared representation\nspace, yet were not fine-tuned on a diverse set of chemical tasks\n+ P V S O B M \u0001 / B N F \r \u0001 < Z F B S > \r \u0001 < W P M \u000f > \r\n1–15 | 1\narXiv:2311.12410v3  [cs.CL]  2 May 2024\nFig. 1 A Venn diagram that shows the relationships between fine-tuning\ndata used in our study and related work. It is important to highlight\nthat the majority of models typically treat the chemical space and the\nsemantic space in the natural language domain independently. Novel\ncross-domaindatasetssuchasMol-Instructions 25 andMolT5data 20 have\nasked whether it is possible to unify representations of natural language\nand molecules for NLP and molecule generation tasks within a single\nmodel. In this work, we seek to answer this question.\nwith instruction tuning in a multi-task fashion. The Venn dia-\ngram in Fig. 1 provides a summary of the existing LMs. Further-\nmore, simple language models trained with molecular structures\ncan reproduce complex molecular distributions21, and even their\n3D structure of molecules, materials and proteins using a GPT\nframework22.\nIn this paper, we propose a unified encoder-decoder trans-\nformer named nach0 for natural language, chemical general-\nization and cross-domain tasks. We pre-train on both natu-\nral language and chemical data using Self Supervised Learning\nand employ nach0 as the foundation model for a wide range\nof downstream tasks (Fig. 2). The tasks include well-known\nNLP problems such as information extraction, question answer-\ning, textual entailment, molecular structures and description\ngeneration, chemical property prediction, and reaction predic-\ntions. Inspired by Raffel et al. 2 , Chung et al. 23 , we follow\nthe intuition that tasks can be described via natural language\ninstructions, such as “What reactants could be used to synthe-\nsize O=C(NC1CCN(Cc2ccccc2)CC1)c1c(Cl)cccc1[N+](=O)[O-]”\nor “describe a molecule C1=CC(=CC=C1C[C@H](C(=O)[O-])N)O”.\nPrompt design and instruction tuning are employed for model\ntraining using NVIDIA’s Neural Modules (NeMo) framework 24,\nwhich provides scientists with a way to train and deploy LLMs\nusing NVIDIA GPUs. Extensive evaluation in both in-domain and\ncross-domain setup demonstrates that nach0 is a powerful tool\nfor the chemistry domain.\nContribution Our contributions are three-fold:\n1. We introduce a biochemical foundation model nach0 and\npre-train base and large versions of nach0 on molecu-\nlar structures and textual data from scientific articles and\npatents.\n2. We fine-tune nach0 in a supervised and multi-task manner,\nusing a combination of diverse tasks specified through natu-\nral language prompts.\n3. Through the experimental validation on benchmark\nFig. 2 Datasets used for training and evaluation. Colour represents the\ntype of tasks. Yellow and blue datasets are single-domain, typically re-\nquiring regression/classification losses or generation in the target domain\n(natural language or SMILES strings). Gradients from yellow to blue rep-\nresent cross-domain generation tasks that require natural language input\nand SMILES output, or vise versa.\ndatasets, focusing on both single-domain and cross-domain\ntasks, we show that our model achieves competitive results\nwith state-of-the-art encoder-decoder models specialized for\nsingle domain.\n2 Methods\n2.1 Framework nach0\nThe aim of nach0 is to create a unified transformer capable of\nperforming natural language, chemical generalization, and trans-\nlation tasks simultaneously . Fig. 3 shows a diagram of our\nframework with several input/output examples. The model’s\nrepresentations are learned from extensive and diverse chemical\nSMILES data and related textual data from scientific articles and\npatents. Similar to Raffel et al.2 , Chung et al.23 , nach0 follows an\nencoder-decoder architecture that takes textual input and gener-\nates target responses. To train the model on a mixture of datasets\npartitioned into different tasks, we formulate all the tasks in a\n“text-to-text” format, where the model is given some text as a\ncontext or condition and produces the output in a text format.\nEach dataset is associated with multiple prompt templates used\nto format datasets’ instances into input and target pairs. In par-\nticular, we train nach0 on three types of tasks (Fig. 2):\n• NLP tasks: named entity recognition (NER), PICO extrac-\ntion, textual entailment, relation extraction, sentence simi-\nlarity , document classification, question answering (yes/no,\nmulti-choice, open);\n• chemistry-related (CHEM) tasks: molecular property pre-\ndiction, molecular generation, forward reaction prediction,\nreagent prediction, retrosynthesis;\n• cross-domain (NLP ↔CHEM) tasks: description-guided\nmolecule design, molecular description generation;\nFig. 3 shows our model and prompt format. Details on\ntrain/test splits are presented in Table 1. Datasets’ descriptions\n2 | 1–15\n+ P V S O B M \u0001 / B N F \r \u0001 < Z F B S > \r \u0001 < W P M \u000f > \r\n\nFig. 3 A diagram of nach0 which is a text-to-text framework. The\nmodel takes text as input and is trained to generate the desired target\ntext for each specific task. This unified approach enables us to utilize\nthe same model architecture, loss function, hyperparameters, and other\ncomponents across our diverse range of mono-domain (NLP, CHEM) and\ncross-domain (NLP↔CHEM) tasks.\nTable 1 List of datasets used in our study. We note that ESOL, FreeSolv,\nLipophilicity, BBBP, HIV, BACE are included in the MoleculeNet bench-\nmark26; QM9, MoleculeNetandUSPTO_500MTdataarecollectedfrom\nMol-Instructions25.\nTask Dataset Link Train/Test\nsplit\nNER\nBC5CDR-Chemical 27 link predefined\nBC5CDR-Disease 27 link predefined\nNCBI-disease28 link predefined\nBC2GM 29 link predefined\nJNLPBA30 link predefined\nPICO EBM PICO31 link predefined\nTextual Entailment MedNLI32 link predefined\nSciTail33 link predefined\nRelation Extraction\nChemProt34 link predefined\nDDI35 link predefined\nGAD36 link predefined\nSentence similarity BIOSSES37 link predefined\nDocument Classifica-\ntion\nHoC38 link predefined\nQuestion answering\n(Yes/No)\nPubMedQA39 link predefined\nBioASQ40 link predefined\nMolecular property\nprediction\nESOL26\nlink predefined\nFreeSolv26\nLipophilicity26\nBBBP26\nHIV26\nBACE26\nQM925 link random\nMolecular genera-\ntion\nMOSES12 link predefined\nForward Reaction\nPrediction Mol-Instructions25 link random\nReagent Prediction\nRetrosynthesis\nDescription-guided\nmolecule design Mol-Instructions25 link random\nMolecular descrip-\ntion generation\nwith example instances are reported in Supplementary Informa-\ntion, Sec. 2.\nGiven the presence of textual and molecular modalities, dif-\nferent tokenization technique is a crucial aspect of dataset de-\nsign. One way to represent molecular structures is a simplified\nmolecular-input line-entry system (SMILES) string41. SMILES de-\nscribe a molecule as a sequence of atoms in a depth-first traversal\norder and uses special symbols to depict branching, cycle open-\ning/closing, bond types, and stereochemistry . We use the follow-\ning tokenization:\n• Textual domain sub-word tokens adopted from FLAN-T5 23\nfor natural language sequences;\n• Tokenization for SMILES: we annotate each SMILES token\nwith special symbols: <sm_{token}> and extend the vocab-\nulary with such tokens.\n2.2 Model and Training Configuration\nIn our study , we predominantly employ a model featuring the de-\nfault T5 architecture, which is derived from Raffel et al. 2 . Our\nexperimentation involves two model sizes: a base model consist-\ning of 250 million parameters, characterized by 12 layers, a hid-\nden state of 768 dimensions, a feed-forward hidden state of 3072\ndimensions, and 12 attention heads; and a larger model with 780\nmillion parameters, consisting of 24 layers, a hidden state of 1024\ndimensions, a feed-forward hidden state of 4096 dimensions, and\n16 attention heads.\nFor both models, we conduct pre-training with a language mod-\neling (LM) objective and subsequent fine-tuning. The base mod-\nels were trained using NVIDIA A4000 and A5000 GPUs, while\nthe larger models were trained on NVIDIA’s DGX cloud platform.\nBoth the pre-training and fine-tuning stages were executed using\nthe subsequent hyperparameters: a batch size of 1024, a learning\nrate set to 1e-4, and a weight decay of 0.01. The pre-training\nstage lasted for a single epoch, whereas the fine-tuning stage for\n10 epochs.\nTo execute the pre-training phase of our model with the LM ob-\njective, we leveraged two textual data sources in addition to one\nchemical data source. These textual data sources encompassed\nabstract texts extracted from Pubmed and patent descriptions de-\nrived from USPTO. All the textual data underwent a filtering pro-\ncess, eliminating documents that were not related to the chem-\nistry domain. Consequently , the number of documents was cur-\ntailed to 13M for abstracts and 119K for patents. The chemical\ndata component was sourced from the ZINC dataset, encompass-\ning approximately 100 million documents. In aggregate, the tex-\ntual data set contained 355M tokens for abstracts and 2.9B tokens\nfor patents, whereas the chemical data encompassed 4.7B tokens.\nThe entirety of the investigations in this paper was conducted\nusing the multi-task model, with the exception of the ablation\npart. Each multi-task model underwent fine-tuning by leveraging\nthe entire spectrum of available datasets, encompassing all do-\nmains, as elucidated in Sec. 1. For data mixing and balancing we\nfollowed the “Examples-proportional mixing strategy” from Raf-\nfel et al.2 . The outcomes of these models are explicitly detailed in\nSec. 3. Conversely , in the context of ablation studies, fine-tuning\nwas specifically performed utilizing only those datasets relevant\nto the corresponding domain, as detailed in the discussion.\n+ P V S O B M \u0001 / B N F \r \u0001 < Z F B S > \r \u0001 < W P M \u000f > \r\n1–15 | 3\n2.3 Nemo, Parallel Training, NVIDIA Cluster\nThe training was performed using NVIDIA NeMo Toolkit42, which\nconsists of pre-built modules for end-to-end workflows in Auto-\nmatic Speech Recognition (ASR), NLP, and Text-to-Speech (TTS)\nsynthesis. NeMo uses PyTorch Lightning for optimized multi-\nnode/multi-GPU (MNMG) mixed-precision training. In this work,\nwe leveraged the NeMo NLP collection to train and evaluate our\nLMs. We trained our model on a variety of tasks such as infor-\nmation extraction, question answering, molecular property pre-\ndiction, and description-guided molecule design using the NeMo\ntoolkit. A custom connector was added to extend the vocabulary\nsize of the pre-trained model when continuing the training of the\nmodel with chemistry and biomedical datasets. The original vo-\ncabulary was extended to match the target vocabulary which was\nlarger. The corresponding embedding matrix was initialized with\nlearned embeddings of the original model. The extra tokens were\ninitialized by re-using the first embeddings.\nData was parsed using Mem-Map Datasets from the NeMo\ntoolkit to allow efficient data handling. The mem-map dataset\nrelies on memory mapping directly to files, allowing the handling\nof very large datasets with small memory footprints and optimal\nreading speed. The data was loaded as raw text files and the\ntokenization occurred on-the-fly . Pre-fetching of the data miti-\ngated the effects of online tokenization when compared to pre-\ntokenized data. The model was trained using tensor and pipeline\nparallelism43, both of which are model parallel methods for dis-\ntributed training and are implemented in the NeMo toolkit for\nefficient scaling of large language model training.\n3 Results and discussion\n3.1 Use case: End-to-end drug discovery\nIn the first case study , we generate molecular structures\nagainst Diabetes mellitus (DM) using just one model, nach0:\ndiscover biological targets with potential therapeutic activ-\nity , analyze the mechanism of action, generate molecular\nstructure, propose one-step synthesis, and predict molecular\nproperties. In a series of questions, we generate the model’s\nresponses using top-p sampling with values from 0.3 to 0.7\nand step equals 0.05 and ask an expert chemist to pick the\nbest response (Fig. 4). In total, we generate 200 SMILES\non the molecule generation prompt and select one structure,\nCC(C)(C)NC(=O)CN1CCC(C(=O)Nc2cccc(-c3nc4ccccc4n3Cc3cc\nccc3)c2)CC1, as the most promising based on a chemical\nexpert knowledge perspective. This semi-automated approach\nis efficient for discovering novel molecules and assessing their\nproperties. We predict that further iterations of this model will\nrequire less supervision, and medicinal chemists will start using\nit as a side-car for generating and validating ideas.\n3.2 Use case: Chemistry42 generative model\nChemistry42 is Insilico Medicine’s AI drug discovery platform\nthat efficiently generates novel active molecules using 42 gen-\nerative models 44. In this experiment, we apply nach0 to one\nof the published case study setups available on demand at\ndemo.chemistry42.com—Structure-Based Design of Janus Kinase\nFig. 4 Input request from a human (gray color) and nach0’s response\n(blue color).\n3 Inhibitors. In Chemistry42, we use 3LXK crystal structure, phar-\nmacophore hypothesis, and a set of physicochemical properties to\nset up the search space for the generative models. All generative\nmodels search the chemical space to find the best possible struc-\ntures.\nChemistry42 provides a set of filters ans reward modules. The\n2D modules comprise of various tools including Medicinal Chem-\nistry Filters (MCFs), Lipinski’s Rule of Five (Ro5), and descrip-\ntors for Drug-likeness, Weighted atom-type portion, Drug-likeness\nand Novelty , the synthetic accessibility (SA) scores. Additionally ,\nChemistry42 use the Self-Organizing Maps (SOM) Classifier Mod-\nule to navigate the generation of molecular structures towards a\nspecific target class in the chemical space. The Structure Morph-\ning module, another integral part of 2D modules, is utilized to\ntackle metabolic instability issues.\nThe 3D modules include the ConfGen Module, which is respon-\nsible for generating conformational ensembles for each molecu-\nlar structure. Subsequently , these molecules are ranked based\non their intrinsic rigidity using a flexibility assessment tool. The\n3D similarity between the generated structures and a reference\nmolecule is evaluated using the 3D-Descriptors Module. The\nPharmacophore Module is then used to find any matches with\nthe specified pharmacophore hypothesis. The Shape Similarity\nModule plays its part in evaluating the 3D shape similarity to a\nreference molecule. Lastly , the Pocket Module and the Pocket-\nLigand Interaction (PLI) modules are used to assess how well the\nmolecules fit the chosen binding site.\nIn this experiment, we replaced all 42 generative models with\nnach0 and generated a set of structures using a prompt “Generate\na random druglike small inhibitor molecule for the Janus Kinase\n4 | 1–15\n+ P V S O B M \u0001 / B N F \r \u0001 < Z F B S > \r \u0001 < W P M \u000f > \r\n\nTable 2 Comparison between nach0 and Chemistry42 models on JAK3 inhibitors generation. nach0 can discover multiple molecules passing all\nconstraints, even though it only uses implicit knowledge about the protein target. Discovery rate (percentage of good molecules from all generated\nmolecules) indicates that our models acts better than random combinatorial generator when solving the problem.\nCombinatorial\ngenerator\nnach0 Chemistry42\nTime 24 hours 45 minutes 72 hours\nTotal molecules 73,000 7,200 382,000\nGood molecules 30 8 5,841\nDiscovery rate 0.04% 0.11% 1.53%\nBest molecule\n3 JAK3 that contains a classic kinase hinge binding motif”. Note\nthat nach0 does not have access to the specific crystal structure\nand other required properties, so the model generated molecules\nusing solely its knowledge about JAK3.\nIn Tab. 2, we compare generation results using a combinatorial\ngenerator45, Chemistry42 44, and our model. In just 45 minutes\n(consisting of 15 minutes for generation and 30 minutes for scor-\ning in Chemistry42), our model discovered 8 molecules satisfying\nall the 2D and 3D requirements; see Ivanenkov et al. 44 for more\ndetails on requirements. All these structures have a hinge binder\nand properly bind in the active site. While our model can dis-\ncover multiple molecules satisfying all constraints, the discovered\nstructures are currently worse than those found in 72 hour gen-\nerations in Chemistry42, since nach0 does not yet learn from the\nreinforcement learning feedback during generation and because\nit does not have exact knowledge of the experiment setup. In fu-\nture work, we will expand our model with reinforcement learning\ncapabilities to improve generation quality .\n3.3 Comparison of multi-task models\nTable 3 compares nach0 base and large models with two exist-\ning NLP encoder-decoder models (general-domain FLAN 23 and\ndomain-specific SciFive46), and a multi-domain encoder-decoder\nmodel MolT5 20. The table contains metrics for each task and\nmodel, with the results of the top-performing base model empha-\nsized in bold. First, FLAN base and nach0 base exhibit similar\nresults on NLP tasks on average, demonstrating superior perfor-\nmance on different tasks. With single-domain models for tasks\nsuch as NER or NLI, where molecule information is not required,\ntraditional LMs may indeed provide the best results. However,\nwhen it comes to molecular tasks that involve molecular data,\nnach0 has distinct advantages over similar-scale models due to its\nspecialized architecture and ability to effectively incorporate and\nprocess molecule-related information. In particular, nach0 bene-\nfits from training on diverse datasets and the proposed tokeniza-\ntion approach, outperforming baselines (including FLAN) with a\nsignificant gap in molecular tasks. For regression tasks, nach0\nshows the best results on both RMSE and R2 scores. Moreover,\nin the molecular generation task, nach0 substantially surpasses\nFLAN by the FCD metric, which assesses the closeness of the gen-\nerated molecules distribution to the ground truth. We added this\nexplanation to the manuscript. Second, as expected, large nach0\nperformed best among all the models. In terms of base mod-\nels, nach0 base achieved the best results on chemical and cross-\ndomain tasks over existing models, confirming that pre-training\non two types of data with different tokens can be effective.\nFurthermore, we conducted zero-shot experiments involving\nnach0, FLAN, and SciFive (all base versions) in an information\nretrieval task. The objective was to detect whether an abstract is\nrelevant to a given disease or gene query . The dataset used for\nthese experiments, along with its specific details, can be found in\nTutubalina et al. 47 . In these experiments, we employed the fol-\nlowing prompt: “Given the following passage, answer the ques-\ntion: Is the following text related to the synonym? Passage: text”.\nTo evaluate the models’ performance, we utilized precision (P),\nrecall (R), and F-measure (F1). Our findings indicate that nach0\nachieved an F1 score of 82.24% (with a recall of 96.32% and pre-\ncision of 71.76%), while FLAN and SciFive achieved F1 scores of\n82.24% and 77.20%, respectively . However, it is worth noting\nthat the supervised BERT-based pipeline from Tutubalinaet al. 47\nachieved a higher F1 score of 88.81%. Based on these results,\nwe can conclude that these models exhibit the ability to perform\nslightly different NLP tasks in a zero-shot setup. However, they\nstill fall significantly behind supervised models in terms of perfor-\nmance.\n3.4 Ablations\nTo examine the impact of cross-domain data on multi-task fine-\ntuning, we conducted training on mono-domain data. The results\nof four pre-trained checkpoints (SciFive, FLAN, MolT5, nach0)\nfine-tuned exclusively on NLP data are presented in Supplemen-\ntary Information, Sec. 1. When considering average performance\non the NLP group, nach0, SciFive, and FLAN exhibit similar re-\nsults, MolT5 achieves lower scores compared to the other models.\nNext, we investigate how chemical tasks groups combination\neffects on joint model performance in comparison with individ-\n+ P V S O B M \u0001 / B N F \r \u0001 < Z F B S > \r \u0001 < W P M \u000f > \r\n1–15 | 5\nTable 3 Full results of nach0 on NLP, CHEM and cross-domain tasks in comparison with FLAN (250M parameters), SciFive (220M parameters),\nMolT5 (220M parameters). All models are trained in a multi-task fashion. Bold number is the highest score on each dataset and the underscore\nstands for the second best result over base models only. We mark the results of Nach0 Large with a green color to indicate improvements over Nach0\nBase.\nDataset Metric MolT5 SciFive FLAN nach0\nBase Large\nBC5-chem\nF-1↑\n77.82% 91.02% 88.03% 90.96% 92.78%\nBC5-disease 71.62% 82.24% 78.29% 81.67% 85.51%\nNCBI-disease 74.96% 84.22% 81.37% 84.30% 85.82%\nBC2GM 53.47% 69.55% 62.53% 71.12% 80.41%\nJNLPBA 63.06% 72.99% 70.74% 73.70% 79.80%\nEBM PICO F1↑ 67.37% 67.32% 69.48% 67.60% 94.44%\nMedNLI Accuracy↑ 58.69% 70.29% 79.66% 73.40% 89.22%\nSciTail 56.54% 80.73% 90.68% 84.12% 93.87%\nChemProt\nF-1↑\n70.52% 75.83% 84.38% 83.61% 94.46%\nDDI 56.02% 59.53% 85.96% 88.69% 93.13%\nGAD 52.10% 64.53% 66.93% 75.47% 78.24%\nBIOSSES Pearson↑ 24.55% 56.51% 61.21% 52.58% 52.37%\nHoC F-1↑ 70.24% 72.49% 72.37% 80.40% 85.86%\nPubMedQA F-1↑ 49.12% 59.44% 62.80% 58.76% 74.21%\nBioASQ 61.71% 80.29% 87.14% 79.43% 89.21%\nMedMCQA and MMLU Accuracy↑ 25.97% 25.06% 25.42% 26.61% 46.10%\nMedMCQA-Open BLEU-2↑ 4.52% 5.83% 5.10% 6.30% 2.26%\nReagent prediction Accuracy@top1↑ 1.10% 3.80% 4.00% 6.30% 13.08%\nRetrosynthesis Accuracy@top1↑ 15.00% 31.00% 31.00% 53.00% 56.26%\nForward reaction prediction Accuracy@top1↑ 27.00% 60.00% 59.00% 88.00% 89.94%\nBACE BA↑ 0.58 0.65 0.65 0.74 0.71\nBBBP BA↑ 0.55 0.66 0.6 0.67 0.68\nHIV BA↑ 0.5 0.53 0.53 0.56 0.60\nHFE R2↑ -0.36 0.51 0.55 0.77 0.78\nRMSE↓ 1.1 0.4 0.37 0.19 0.19\nHOMO-LUMO R2↑ 0.98 0.99 0.99 1.00 1.00\nRMSE↓ 0.0008 0.0003 0.0003 0.0001 0.0001\nLOGD R2↑ -0.6 -0.27 -0.32 0.28 0.28\nRMSE↓ 2.4 1.9 1.9 1.1 1.1\nLOGS R2↑ -0.49 0.31 0.001 0.48 0.48\nRMSE↓ 1.4 0.63 0.91 0.48 0.48\nMOSES\nValid↑ 98.30% 95.79% 97.63% 99.86% 99.93%\nUnique@10000↑ 99.93% 99.94% 99.95% 99.92% 99.97%\nFCD/Test↓ 0.5212 0.5778 0.5289 0.3106 0.3038\nSNN/Test↑ 0.5745 0.5688 0.5742 0.6118 0.6222\nFrag/Test↑ 0.9974 0.9967 0.9965 0.9985 1.00\nScaf/Test↑ 0.8748 0.8737 0.8823 0.9205 0.9292\nIntDiv↑ 0.8460 0.8464 0.8462 0.8478 0.8585\nFilters↑ 98.89% 98.67% 98.68% 99.54% 99.67%\nNovelty↑ 93.92% 93.98% 93.67% 87.60% 93.87%\nDescription-guided molecule design BLEU-2↑ 30.32% 44.17% 43.64% 48.97% 48.76%\nMolecular description generation BLEU-2↑ 35.61% 39.56% 38.58% 43.91% 41.73%\nual models trained on each separate chemical tasks group—on\npredictive tasks group, on reaction tasks group and molecular\ngeneration/cross-domain tasks group. We perform the same ex-\nperiments with MolT5 model to elaborate on how pretraining\ndata and special chemical tokens affect the quality of the model\non chemical tasks.\nThe results of this ablation study can be found in Tab. 4 and\nshow that nach0 benefits from combining chemical tasks group—\nmodel trained on the whole set of chemical data without NLP\noutperforms in total set of metrics models trained on distinct task\ngroups. It is important to mention that despite the joint model\nshowing worse metrics than the model trained only on molecular\ngeneration and cross-domain tasks, it works better since it does\nnot overfit on training data—the novelty metric is more prevail\nhere over all other molecule generation metrics.\nAlso, experiments show that the special chemical tokens and\npre-training on both natural language and chemical data improve\nthe model quality—nach0 outperforms MolT5 baseline or show\nequal metrics on each chemical task group. We miss some MolT5\nmetrics on molecule generation task since it produces non-valid\nSMILES sequences.\n3.5 Comparison with ChatGPT\nRecently , a comprehensive benchmark for biomedical text gen-\neration and mining problems with ChatGPT was conducted, re-\nvealing its poor performance on several biomedical NLP bench-\nmark datasets48,49. Chen et al. 49 specifically evaluated ChatGPT\non a BLURB benchmark 50, which encompasses BC5-chem, BC5-\ndisease, NCBI-disease, BC2GM, JNLPBA, EMB-PICO, ChemProt,\nDDI, GAD, BIOSSES, HoC, PubMedQA, BioASQ. In particular,\nChatGPT got an average BLURB score of 48.27 on NER, while\nfine-tuned BERT achieved 86.27. For more details on evaluation\nscores, please refer to Chen et al. 49 .\n6 | 1–15\n+ P V S O B M \u0001 / B N F \r \u0001 < Z F B S > \r \u0001 < W P M \u000f > \r\n\nTable 4 Performance of nach0 on chemical tasks groups in comparison with MolT5. We list the scores for each task (see Supplementary Information\nabout datasets and metrics). Bold number is the best result on each dataset. All models are base models.\nDataset Metric nach0 MolT5\nAll Pred. React. Mol. Gen. All Pred. React. Mol. Gen.\nPrediction tasks\nBACE BA ↑ 0.74 0.67 - - 0.58 0.52 - -\nBBBP BA ↑ 0.67 0.62 - - 0.55 0.57 - -\nHIV BA ↑ 0.56 0.65 - - 0.5 0.51 - -\nHFE R2 ↑ 0.77 0.015 - - -0.36 -0.74 - -\nRMSE ↓ 0.19 0.81 - - 1.1 1.4 - -\nHOMO-LUMO R2 ↑ 1.0 1.0 - - 0.98 0.94 - -\nRMSE ↓ 1e-4 1e-5 - - 7e-4 2e-4 - -\nLOGD R2 ↑ 0.28 0.27 - - -0.6 -2.9 - -\nRMSE ↓ 1.1 1.1 - - 2.4 5.7 - -\nLOGS R2 ↑ 0.48 0.32 - - -0.49 -1.2 - -\nRMSE ↓ 0.48 0.62 - - 1.4 2.0 - -\nReaction tasks\nReagent prediction Accuracy ↑ 0.063 - 0.14 - 0.011 - 0.13 -\nRetrosynthesis Accuracy ↑ 0.53 - 0.39 - 0.15 - 0.39 -\nForward reaction prediction Accuracy ↑ 0.88 - 0.89 - 0.27 - 0.89 -\nMolecular generation and cross-domain tasks\nMolecule generation\nValidity ↑ 99.86% - - 99.99% 98.3% - - 0.0%\nUnique@10000 ↑ 99.92% - - 99.81% 99.93% - - N/A\nFCD/Test ↓ 0.3106 - - 0.2411 0.5212 - - N/A\nSNN/Test ↑ 0.6118 - - 0.6551 0.5745 - - N/A\nFrag/Test ↑ 0.9985 - - 0.9988 0.9974 - - N/A\nScaf/Test ↑ 0.9205 - - 0.9403 0.8748 - - N/A\nIntDiv ↑ 0.8478 - - 0.8493 0.846 - - N/A\nFilters ↑ 99.54% - - 99.95% 98.89% - - N/A\nNovelty ↑ 87.6% - - 64.34% 93.92% - - N/A\nDescription-guided molecule gen. BLEU-2 ↑ 48.97% - - 52.90% 30.32% - - 30.78%\nMolecular description generation BLEU-2 ↑ 43.91% - - 46.22% 35.61% - - 31.32%\nIn our evaluation setup, we focus on three specific datasets:\nEMB-PICO, MedMCQA-Open, and molecular description genera-\ntion (Mol-Instructions). The inclusion of EMB-PICO dataset was\ndriven by its practical importance. This dataset involves the task\nof identifying and extracting specific fragments of text related to\nthe Population/Patient/Problem (P), Intervention (I), Compara-\ntor (C), and Outcome (O) elements from unstructured biomedi-\ncal texts, such as research articles and clinical trial reports. It is\nworth noting that the clinical trial domain holds particular sig-\nnificance for inClinico, a transformer-based artificial intelligence\nsoftware platform designed to predict the outcome of Phase II\nclinical trials10. The molecular generation task is relevant to the\nChemistry42 platform44.\nTo evaluate the zero-shot performance, we had to limit the\nevaluation to a subset of 2000 samples from the test set for each\nof the three datasets, considering the computational constraints\nof ChatGPT. As well we utilized the GPT-3.5-turbo model through\nthe OpenAI API and multi-task nach0 base for evaluation pur-\nposes. In the case of the PICO dataset, ChatGPT achieved a word-\nlevel F1 score of 64.43%, comparable to the results obtained by\nfine-tuned nach0 base on this subset (F1 score of 67.60%). For\nMedMCQA-Open, ChatGPT achieved a BLEU2 score of 1.68%,\nwhile the fine-tuned nach0 base attained a BLEU2 score of 6.30%.\nIn the molecular description generation task, ChatGPT achieved\na BLEU2 score of 2.23%, whereas the fine-tuned nach0 base ex-\ncelled with a BLEU2 score of 42.80%. Based on our preliminary\nfindings, it is evident that utilizing ChatGPT directly leads to sub-\npar performance compared to models trained specifically on the\ndomain-specific dataset, how it was done in nach0.\n3.6 Discussion\nIn this study , we pretrained and fine-tuned T5 models, which\nhave an encoder-decoder architecture. Nevertheless, a broad\nrange of model families, including T5, BERT-based BioMega-\ntron51, decoder-only PaLM 52 and GPT 4, exist. To determine\nthe most suitable architecture for pre-training and fine-tuning on\nchemical-related data, it may be necessary to evaluate these al-\nternatives. We suggest it as a potential topic for future research.\nThere have been several efforts to train large language mod-\nels (LLMs) on biomedical corpora, particularly on PubMed. No-\ntable examples include BioGPT (347M and 1.5B)53, PubMedGPT\n(2.7B)54, and Galactica (120B)18. Through our experiments with\nscaling from a base model (250M) to a large model (780M), we\ndemonstrated the benefits of scale on several datasets. Based on\nour findings, we can conclude that scaling can further enhance\nthe chemical capabilities of models, particularly in terms of gen-\neration and reasoning skills.\n3.6.1 Limitations\nKey LLM capabilities for chemistry\nAlthough our LM was able to reach state-of-the-art performance\non several chemistry-related benchmarks, our human evaluations\nclearly suggested that these models are not at the chemist ex-\npert level. In order to bridge this gap, several new LLM capabil-\nities need to be researched and developed including (i) knowl-\nedge alignment between textual and chemical sources as well as\n+ P V S O B M \u0001 / B N F \r \u0001 < Z F B S > \r \u0001 < W P M \u000f > \r\n1–15 | 7\ndomain-specific knowledge graphs; (ii) ability to perform chemi-\ncal reasoning and provide explanations for their predictions; (iii)\nability to learn from and adapt to feedback from human experts,\n(iv) ability to generate novel chemical reactions and materials.\nMolecular representations\nOne limitation of our LM is its focus on string representations of\nmolecules, specifically the SMILES notation. Although SMILES\nis a widely used notation for representing molecules, it provides\nonly 2D information of the molecule, missing the 3D geometry\nand spatial arrangement of atoms and bonds in a molecule. This\ncan result in inaccuracies in predicting molecular properties and\ninteractions. To address these limitations, it would be beneficial\nto incorporate additional modalities of molecules, such as the\nmolecular graphs in terms of 2D or 3D representations, in the\ntraining of the language model.\nAnother significant drawback of the SMILES format is the ab-\nsence of a one-to-one translation between molecules and SMILES\nstrings. Typically , a molecule can have multiple SMILES repre-\nsentations that differ from each other due to factors such as the\nstarting atom, molecular graph traversal, and kekulization. In\npractice, SMILES strings are often converted to a canonical form\nusing an unambiguous algorithm. A molecular representation\ncalled SELFIES55,56 was defined from scratch to be attractive as a\nsequential representation for molecules. All random SELFIES are\nvalid molecular representations. SELFIES was extened to treat\nmolecular groups as well 57. As SELFIES have been repeatedly\nshown to have advantages over other representations in the con-\ntext of generative models, exploring their use as the main repre-\nsentation for a language model is a future potential direction.\nPrompt design\nOur language model has a limitation in that it heavily relies on\nthe quality and specificity of the prompts, as well as the poten-\ntial for biases in both the training data and the prompts them-\nselves. To enhance the performance of the model, incorporating\ndomain-specific and information-rich prompts is essential. One\npotential approach to achieving this is by leveraging the knowl-\nedge of domain experts to design effective biomedical prompts.\nYet, over-reliance on domain-specific prompts may lead to a lack\nof diversity in the model’s responses, which can limit its useful-\nness.\nChemical diversity\nMol-Instructions includes cross-domain datasets that consist of\ncompounds and their corresponding descriptions collected from\nPubChem. PubChem is a publicly available database administered\nby the National Center for Biotechnology Information (NCBI). It\nis important to note that the datasets primarily encompass current\ndrugs and known chemical probes, representing only a fraction of\nthe vast predicted chemical space. Furthermore, these datasets do\nnot encompass testing on novel chemical diversity distinct from\nmolecules documented in the literature.\n4 Conclusion\nOur study integrates a diverse range of one-domain and multi-\ndomain task types and biomolecular text instructions to address\nthe landscape of chemical research on drug design, reaction pre-\ndiction, and retrosynthesis and leverage the advancements in\nNLP and LLMs. EThe multi-domain training approach allows our\nmodel, nach0, to leverage a broader understanding of both chem-\nical and linguistic knowledge. xtensive experiments and two case\nstudies demonstrate that nach0’s capabilities in translating be-\ntween natural language and chemical language enable it to tackle\ntasks effectively . Considering the unique training methodology\nand the broader scope of tasks that our model can effectively han-\ndle, we believe our work presents a significant contribution to the\nfield.\nBased on our findings, we foresee several promising directions\nfor future research. One direction could involve such as protein\nsequences, which would require adding special tokens into the\nmodel similar to SMILES. This task could be easily achieved with\nGroup SELFIES. New modalities require collecting diverse tasks\nwith natural language prompts for fine-tuning. A second direc-\ntion involves extending NLP datasets and conducting zero-shot\nevaluations to assess the reasoning and generalization capabili-\nties of nach0. Finally , exploring the fusion of information from\ntextual sequences and relevant knowledge graphs as input in a\nself-supervised approach remains an area to be explored.\nAuthor Contributions\nThese authors contributed equally: Micha Livne, Zulfat Miftahut-\ndinov, Elena Tutubalina, Maksim Kuznetsov.\nET, DP, AA, and AZ contributed to the conception and design of\nthe work. ET, ZM, and MK contributed to the data acquisition and\ncuration. ZM, MK, ML, AC, AB, and AJ contributed to the tech-\nnical implementation with the NeMo framework, provided tech-\nnical and infrastructure guidance. ET, ZM, and MK contributed\nto the evaluation framework used in the study . All authors con-\ntributed to the drafting and revising of the manuscript.\nConflicts of interest\nThe authors declare no competing interests. This study is a col-\nlaboration of NVIDIA and Insilico Medicine employees.\nData availability\nAll datasets used in the study for pre-training and fine-tuning are\npublicly available.\nCode availability\nThe nach0 framework is available for research purposes:\n• nach0 Base is available via https://huggingface.co/\ninsilicomedicine/nach0_base;\n• nach0 Large is available via https://huggingface.co/\ninsilicomedicine/nach0_large;\n• For pre-processing scripts, see https://github.com/\ninsilicomedicine/nach0.\n8 | 1–15\n+ P V S O B M \u0001 / B N F \r \u0001 < Z F B S > \r \u0001 < W P M \u000f > \r\n\n5 Supplementary\n5.1 NLP Ablation\nTo examine the impact of cross-domain data on multi-task fine-\ntuning, we conducted training on mono-domain data. The re-\nsults of four pre-trained checkpoints fine-tuned exclusively on\nNLP data are presented in Supplementary Information, Tab. 5.\nSeveral noteworthy observations can be made based on these\nfindings.\nFirstly , when considering average performance, nach0, SciFive,\nand FLAN exhibit similar results. However, each model demon-\nstrates superior performance on different tasks. FLAN, being\na general-domain model, outperforms others in textual entail-\nment, binary QA, and sentence similarity . On the other hand, the\ndomain-specific SciFive shows best results in NER, while nach0 –\nin relation extraction, classification, and multi-choice QA.\nSecondly , MolT5 achieves lower scores compared to the other\nmodels. This can be related to the pre-training strategy , where\nmolecules and natural language texts share the same tokens in\nthe semantic space. In contrast, nach0 utilizes specialized tok-\nenization for molecular data, which does not significantly impact\noverall performance on NLP tasks compared to SciFive and FLAN.\n5.2 Chemistry: Tasks and Datasets\nWe’ve integrated several chemical domain tasks from widely-used\nbenchmarks and datasets. It covers distribution match, molecu-\nlar property prediction, reaction prediction and related problems.\nWhere it’s possible, we use the provided standard train/valida-\ntion/test split procedures, otherwise, we employ the random data\nsplit. We choose this data preparation strategy to enable compar-\nison with baseline models, however, we don’t guarantee that one\ncan’t find chemical objects with similar structures in the different\nsubsets.\n5.2.1 MOSES\nMOSES dataset 45 is a benchmarking platform that provides a\nlarge dataset and set of metrics to compare generative models\non an unconditional molecular generation task. The dataset pro-\nvided by MOSES contains almost 2 million samples filtered by\nMCF, PAINS, and additional rules. The metrics set estimates the\nquality of the generative model from several points of view: va-\nlidity of generated structures, molecular distribution matching\nquality , and the ability of the model to produce novel, diverse\nmolecules.\nEvaluation metric: The MOSES benchmark provides estab-\nlished set metrics for assessing the ability of models to produce\nunique, diverse, valid molecules similar to ground-truth distribu-\ntion. In our work, we adopt several metrics: uniqueness, validity ,\nnovelty , internal diversity , similarity to a nearestneighbor (SNN),\nfragment similarity , scaffold similarity and FCD58. We’ve gener-\nated 30000 new molecules to compute these metrics.\nExample on molecular distribution matching:\ninput text with prompt : Generate random molecule from\nMOSES dataset.\noutput text: CC1C2CCC(C2)C1CN(CCO)C(=O)c1ccc(Cl)cc1.\n5.2.2 Mol-Instructions\nThe recently published Mol-Instructions dataset 25 covers three\nsignificant modalities: molecule-oriented instructions, protein-\noriented instructions, and biomolecular text instructions. In our\nstudy , we specifically focus on the first subset, which is the most\nrelevant and contains chemical tasks.\nExample on descriptor-guided molecule generation:\ninput text with prompt: Synthesize a molecule that matches the\ngiven characteristics: The molecule is the (R)-enantiomer of ace-\nprometazine. It is an enantiomer of a (S)-aceprometazine.\noutput text: CC(=O)c1cc2c(cc1)Sc1ccccc1N2C[C@@H](C)N(C)C.\nExample on molecular description generation:\ninput text with prompt : What can you tell me about this\nmolecule?: C(=C(Cl)Cl)(Cl)Cl.\noutput text: The molecule is a manufactured chemical that is\nwidely used for dry cleaning of fabrics and for metal-degreasing.\nIt is also used to make other chemicals and is used in some con-\nsumer products.\nExample on forward reaction prediction:\ninput text with prompt : With the provided re-\nactants and reagents, propose a potential product:\nCO.C[Si](C)(C)C#Cc1ccc(C=O)cc1.ClCCl.O=C([O-])[O-].[K+].[K+].\noutput text: C#Cc1ccc(C=O)cc1.\nExample on reagent prediction:\ninput text with prompt : Please propose potential reagents that\nmight have been utilized in the provided chemical reaction:\nCOC(=O)c1c(F)cc(NC(=O)c2cc(C(C)C)c(C(C)C)s2)cc1F ≫\nCC(C)c1c(C(C)C)sc(C(=O)Nc2cc(F)c(C(=O)O)c(F)c2)c1.\noutput text: [Na+].[OH-]\nExample on retrosynthesis:\ninput text with prompt: Provide a list of potential reactants that\nmay have produced the given product.: Cc1ccc(-c2ccccc2N)cc1\noutput text: Cc1ccc(B(O)O)cc1.Nc1ccccc1I\n5.2.3 Property Prediction\nWe adopt several binary classification and regression tasks from\nthe MoleculeNet benchmark to assess the model’s ability to pre-\ndict molecular properties.\nEvaluation metric: Binary classification tasks include BBBP,\nHIV, and BACE datasets from MoleculeNet 26 and use balanced\naccuracy as the main metric. Regression tasks involve ESOL, Free-\nSOLV and Lipo datasets from MoleculeNet 26, QM9 dataset from\nMolInstructions25 and rely on the R2 metric. In our work, we uti-\nlized the code provided by the MoleculeNet benchmark to prepare\ndata splits.\nExample on the BBBP classification task:\ninput text with prompt : Can\nCN(C)[C@H]1[C@@H]2C[C@H]3C(=C(O)c4c(O)cccc4[C@@]3\n(C)O)C(=O)[C@]2(O)C(=O)C(=C(/O)NCN5CCCC5)C1=O penetrate\nthe BBB?\noutput text: 1\nExample on HIV classification task:\ninput text : Is CCC1=[O+][Cu-3]2([O+]=C(CC)C1)[O+]=C(CC)\nCC(CC)=[O+]2 an HIV inhibitor?\noutput text: 0\nExample on BACE classification task:\n+ P V S O B M \u0001 / B N F \r \u0001 < Z F B S > \r \u0001 < W P M \u000f > \r\n1–15 | 9\ninput text with prompt : Please evaluate the ability\nof S(=O)(=O)(CCCCC)C[C@@H](NC(=O)c1cccnc1)C(=O)\nN[C@H]([C@H](O)C[NH2+]Cc1cc(ccc1)CC)Cc1cc(F)cc(F)c1 to\ninhibit human beta-secretase\noutput text: 1\nExample on logS prediction task:\ninput text with prompt : Given molecule with SMILES\nOCC2OC(Oc1ccccc1CO)C(O)C(O)C2O, predict its logS\noutput text: 1.083897\nExample on HFE prediction task:\ninput text with prompt : What hydration free energy does\nCOc1cc(c(c(c1O)OC)Cl)C=O have?\noutput text: -1.013714\nExample on logD prediction task:\ninput text with prompt : What is the lowest unoccu-\npied molecular orbital (LUMO) energy of this molecule? :\nO=C1OC2C3CC1OC32\noutput text: 0.0035\nExample on HOMO-LUMO prediction task:\ninput text with prompt : lipophilic is\nCOc1cc(OC)c(cc1NC(=O)CCC(=O)O)S(=O)\n(=O)NCc2ccccc2N3CCCCC3?\noutput text: -0.720000\n5.3 NLP: Tasks and Datasets\n5.3.1 Named entity recognition\nNamed entity recognition (NER) is a fundamental aspect of nat-\nural language processing, involving the identification and classi-\nfication of entities in a given text into predefined categories. In\nbiomedical NER, the focus lies in extracting mentions of diseases,\ngenes, chemicals, and other biologically relevant entity types. To\nconduct this study , we carefully selected five datasets:\n• BC2GM 29;\n• BC5CDR-Disease 27;\n• BC5CDR-Chemical 27;\n• JNLPBA 30;\n• NCBI-Disease 28.\n5.3.1.1 BC2GM The BC2GM dataset encompasses an exten-\nsive collection of over 20,000 sentences extracted from the MED-\nLINE database, spanning the years 1991 to 2003. Each document\nin this dataset is annotated with gene mention spans, amounting\nto a total of 24,583 mentions.\n5.3.1.2 BC5CDR The BioCreative V CDR dataset was specifi-\ncally designed for named entity recognition tasks involving dis-\nease and chemical entity types. It contains 12,850 disease and\n15,935 chemical mentions, drawn from 1,500 PubMed articles.\n5.3.1.3 JNLPBA The JNLPBA involves gene mention annota-\ntions across more than 2,000 PubMed abstracts. The creation\nof this dataset entailed a meticulous search on the MEDLINE\ndatabase, using specific MeSH terms such as ’human’, ’blood cells’,\nand ’transcription factors’. In total, JNLPBA comprises 59,963\ngene mention spans.\n5.3.1.4 NCBI-Disease The NCBI-disease corpus, developed by\nthe National Center for Biotechnology Information (NCBI), con-\nstitutes a vast collection of 793 PubMed abstracts that have un-\ndergone meticulous annotation by domain experts. These anno-\ntations include disease names and their corresponding concept\nIDs, sourced from the Medical Subject Headings (MeSH) vocabu-\nlary59.\nIn order to train the neural network in a text-to-text format, we\ndesigned five prompts. Each prompt asks to highlight the spans\ncorresponding to mentions of specific entity . In order to achieve\nthis, we insert specific tokens before and after the mention of an\nentity in the text.\nEvaluation metric: the evaluation of the NER task’s quality is\nperformed using the entity level F-measure.\nExample:\ninput text with prompt : Please find all instances of diseases in\nthe given text. Each mention should be surrounded by \"diso*\" and\n\"*diso\": Identification of APC2, a homologue of the adenomatous\npolyposis coli tumor suppressor;\noutput text: Identification of APC2 , a homologue of the diso*\nadenomatous polyposis coli tumour *diso suppressor.\n5.3.2 Question Answering\nQuestion Answering (QA) is an important area of NLP research.\nThe objective of QA is to develop intelligent systems that can un-\nderstand and accurately answer questions posed in natural lan-\nguage. Within the biomedical domain, QA refers to the specific\napplications and models designed to address questions related to\nbiomedical and healthcare information. It is required for model\nto understand and respond to questions pertaining to medical\nknowledge, clinical data, scientific literature, drug information,\nand other relevant biomedical topics. In this study , we conducted\nexperiments on four biomedical QA datasets:\n• BioASQ 40;\n• PubMedQA 39;\n• MedMCQA 60;\n• MMLU 61.\nThe first two datasets are employed to evaluate the neural net-\nwork’s ability to answer binary Yes/No questions, while the re-\nmaining two datasets are used in scenarios that involve multi-\nchoice and open question answering.\n5.3.2.1 BioASQ and PubMedQA BioASQ (Biomedical Ques-\ntion Answering) is a widely recognized dataset in the biomedical\ndomain, specifically designed for evaluating question answering\nsystems. Following the 50 we restrict the dataset to yes/no ques-\ntions. We use the official train/dev/test split where each contains\n670/75/140 questions respectively .\nSimilar to BioASQ, the PubMedQA dataset as well presents\nquestions with limited number of answers. In contrast to the pre-\nvious dataset, the answers to the questions in PubMedQA are se-\nlected from yes, no, or maybe. We use the original train/dev/test\nsplit with 450, 50, and 500 questions, respectively .\n10 | 1–15\n+ P V S O B M \u0001 / B N F \r \u0001 < Z F B S > \r \u0001 < W P M \u000f > \r\n\n5.3.2.2 MedMCQA and MMLU For multiple choice question\nanswering, we employ the concatenation of the MedMCQA and\nMMLU datasets from 25, resulting in a total of 12,398 multiple-\nchoice questions. As25 does not provide train/dev/test partitions,\nwe randomly split the dataset into a ratio of 75:25.\nTo perform open question answering, we adopted a dataset in-\ntroduced in 25, which comprises 27,574 question-answer pairs.\nThis dataset was curated from the MedMCQA dataset.\nEvaluation metric: to evaluate the performance of yes/no\nand multiple-choice question-answering tasks, we utilized the\naccuracy metric. For open-ended question-answering tasks, we\nadopted the BLEU-2 metric as our evaluation criterion.\nYes/No QA example:\ninput text with prompt: Given a passage: De novo DNA methy-\nlation in Arabidopsis thaliana is catalyzed by the methyltrans-\nferase DRM2, a homolog of the mammalian de novo methyltrans-\nferase DNMT3. Here we describe DNA methyltransferase genes\nfrom both Arabidopsis and maize that show a high level of se-\nquence similarity to Dnmt3, suggesting that they encode plant\nde novo methyltransferases. Relative to all known eukaryotic\nmethyltransferases, these plant proteins contain a novel arrange-\nment of the motifs required for DNA methyltransferase catalytic\nactivity . The N termini of these methyltransferases contain a se-\nries of ubiquitin-associated (UBA) domains. BLASTX searches and\nphylogenetic analysis suggested that five cDNAs belonged to four\nclasses (Dnmt1, Dnmt2, CMT and Dnmt3) of DNA methyltrans-\nferase genes,\nanswer the question: Are there any DNMT3 proteins present in\nplants?;\noutput text: Yes.\nMulti-choice QA example:\ninput text with prompt: Which of the following is antifibrinolytic\ndrug: What of the following is the right choice?\n(A) Tenecteplase\n(B) Heparin\n(C) Urokinase\n(D) Tranexaemic acid\noutput text: The final answer is (D).\nOpen ended QA example:\ninput text with prompt: 1,25 dihydrocholecalciferol acts on?\noutput text: Intranuclear receptors\n5.3.3 Relation Extraction\nRelation extraction (RE) is a NLP task that involves identifying\nand classifying the relationships between entities mentioned in a\ntext. In the biomedical domain, RE refers to the specific applica-\ntion of RE techniques and models to extract and classify relation-\nships between biomedical entities mentioned in text. Biomedical\nRE focuses on identifying and categorizing the associations be-\ntween various biomedical entities, including genes, proteins, dis-\neases, drugs, and other molecular entities. For experiments, we\nuse three corpora:\n• ChemProt 34;\n• DDI 35;\n• GAD 36.\n5.3.3.1 ChemProt The ChemProt dataset is a widely used\nbenchmark for the task of chemical-protein RE. The dataset com-\nprises PubMed abstracts that are annotated with chemical-protein\ninteractions, where the chemicals typically represent drug com-\npounds or small molecules, and the proteins denote specific bio-\nlogical targets or enzymes. Each annotated interaction is labeled\nwith the corresponding chemical and protein mentions, along\nwith the following types of relationship: upregulator, downreg-\nulator, antagonist, agonist, and substrate. The training set of the\ndataset contains 9,995 relation pairs, and the test set contains\n5,744 relation pairs.\n5.3.3.2 DDI The DDI (Drug-Drug Interaction) corpus is a\ndataset designed for the purpose of identifying drug-drug inter-\nactions mentioned in biomedical texts. The corpus consists of an-\nnotated sentences or text passages that describe interactions be-\ntween pairs of drugs. Each annotated interaction is labeled with\nthe names of the drugs involved and the specific type of inter-\naction. We employ the train/test split produced in 50, where the\ntraining set contains 4,021 relation pairs and the test set contains\n979 relation pairs.\n5.3.3.3 GAD The GAD dataset is a comprehensive collection\nof genetic association information that was semi-automatically\ncompiled using the Genetic Association Archive. In our study ,\nwe utilize an existing preprocessed version of GAD and its corre-\nsponding train/test split, which was created by Lee et al. 17. The\ntraining set of the dataset consists of 4,796 relation pairs, while\nthe testing set includes 534 relation pairs.\nIn our experimental framework, we adopt a binary classifica-\ntion approach for relation extraction. Here, the positive class in-\ndicates the presence of the specified type of relationship between\ntwo entities.\nEvaluation metric: to evaluate the quality of RE tasks we utilize\nthe F-1 measure of positive class.\nExample:\ninput text with prompt : does the Chlorprothixene and lithium\nare said to have mechanism type of interaction in the following\npassage:\nChlorprothixene may increase the plasma-level of concomi-\ntantly given lithium. In order to avoid lithium intoxication,\nlithium plasma levels should be monitored closely . If chlorproth-\nixene is given concomitantly with opioids, the opioid dose should\nbe reduced (by approx. 50%), because chlorprothixene ampli-\nfies the therapeutic actions and side-effects of opioids massively .\nAvoid the concomitant use of chlorprothixene and tramadol (Ul-\ntram). Massive seizures may be encountered with this combi-\nnation. Consider additive sedative effects and confusional states\nto emerge, if chlorprothixene is given with benzodiazepines or\nbarbituates. Choose particular low doses of these drugs. Exert\nparticular caution in combining chlorprothixene with other anti-\ncholinergic drugs (tricyclic antidepressants and antiparkinsonian\nagents): Particularly the elderly may develop delirium, high fever,\nsevere obstipation, even ileus and glaucoma.\noutput text: Yes\n+ P V S O B M \u0001 / B N F \r \u0001 < Z F B S > \r \u0001 < W P M \u000f > \r\n1–15 | 11\n5.3.4 Textual Entailment\nTextual entailment (TE) is a natural language processing task that\ninvolves determining the logical relationship between two pieces\nof text: a text fragment known as the \"premise\" and another text\nfragment known as the \"hypothesis.\" The task is to decide whether\nthe meaning of the hypothesis can be logically inferred or entailed\nfrom the meaning of the premise. For conducting our experi-\nments, we utilize the following corpora:\n• MedNLI 32;\n• SciTail 33;\n5.3.4.1 MedNLI MedNLI (Medical Natural Language Infer-\nence) is a specialized dataset designed to facilitate research in\nnatural language inference within the medical and healthcare do-\nmain. It consists of pairs of sentences, where each pair comprises\na premise and a hypothesis. The premise represents a clinical or\nbiomedical context, while the hypothesis is a medical statement\nor claim that may or may not logically follow from the premise.\nEach sentence pair is annotated with one of three labels: \"en-\ntailment,\" indicating that the hypothesis can be logically inferred\nfrom the premise; \"contradiction,\" suggesting that the hypothesis\ncontradicts the information in the premise; and \"neutral,\" signi-\nfying that there is no logical relationship between the two sen-\ntences. The dataset comprises a total of 12,627 sentence pairs in\nthe training set and 1,422 sentence pairs in the testing set.\n5.3.4.2 SciTail The SciTail dataset is similar to the MedNLI\ndataset was designed for the task of natural language inference.\nExcept that it covers a broader scientific domain. The train part\nof the corpora contains 24900 sentence pairs and the test part of\nthe corpora contains 2126.\nEvaluation metric: to evaluate the quality of TE tasks we uti-\nlize the Accuracy score.\nExample:\ninput text with prompt : Given that \"At [**Hospital 1456**]\nHospital the patient was experiencing 10 out of 10 chest pain and\nreceived nitropaste two inches, three sublingual nitroglycerins,\nmorphine 4 mg intravenously , Lopressor 5 mg intravenously .\"\nDoes it follow that \" The patient is asymptomatic.\"\nyes or no?\noutput text: No\n5.3.5 Sentence similarity\nTextual similarity tasks in the biomedical domain involve assess-\ning the degree of semantic similarity or relatedness between pairs\nof biomedical texts. The goal of these tasks is to determine how\nclosely two pieces of text, such as sentences or documents, are se-\nmantically or conceptually aligned. To conduct our experiments,\nwe employ the BIOSSES dataset37.\n5.3.5.1 BIOSSES The BIOSSES (Biomedical Sentence Similar-\nity Benchmark) dataset is a specialized dataset designed to evalu-\nate sentence similarity models in the biomedical domain. It con-\ntains pairs of biomedical sentences that are carefully selected to\nrepresent different levels of semantic similarity . Each sentence\npair is annotated with a similarity score that represents the de-\ngree of semantic relatedness between the two sentences. The\nscores are typically on a continuous scale, indicating how similar\nor dissimilar the sentences are in meaning. The dataset comprises\na total of 80 sentence pairs in the training set and 20 sentence\npairs in the testing set.\nEvaluation metric: to evaluate the quality of Textual Similarity\ntasks we utilize the Pearson corellation score.\nExample:\ninput text with prompt : Please assess the similarity between\nthese two sentences on a scale of 0.0 (lowest) to 4.0 (highest).\nFirst sentence: \"It has recently been shown that Craf is essential\nfor Kras G12D-induced NSCLC.\" Second sentence:\"It has recently\nbecome evident that Craf is essential for the onset of Kras-driven\nnon-small cell lung cancer. \"\noutput text: 4.0\n5.3.6 Document Classification\nIn the biomedical domain, the document classification task in-\nvolves categorizing entire documents, such as scientific articles,\nresearch papers, or clinical reports, into predefined categories or\nclasses. The goal is to automatically assign each document to the\nmost relevant category based on its content and subject matter.\nFor our experimental purposes, we utilize the Hallmarks of Can-\ncer dataset.\n5.3.6.1 Hallmarks of Cancer The Hallmarks of Cancer (HoC)\ndataset serves as a document classification task, centered around\nthe concept of cancer hallmarks as established in the referenced\nwork38. This corpus comprises PubMed abstracts, each labeled\nwith binary annotations, denoting the presence of specific dis-\ncussions related to individual cancer hallmarks. We utilize the\ntrain/test split from 50 which comprises 13917 sentences in train\npart and 3547 sentences in test part.\nEvaluation metric: to evaluate the quality of Document Clas-\nsification tasks we utilize the F-1 score.\nExample:\ninput text with prompt : Pick one category for the following\ntext. The options are - activating invasion and metastasis, avoid-\ning immune destruction, cellular energetics, enabling replicative\nimmortality , evading growth suppressors, genomic instability and\nmutation, inducing angiogenesis, resisting cell death, none, sus-\ntaining proliferative signaling, tumor promoting inflammation.\nBiopsy of a skin lesion showed lymphoproliferative infiltration\nof the dermis with a follicular and angiocentric growth pattern\nand regional epidermal necrosis.\noutput text: resisting cell death\n5.3.7 PICO extraction\nPICO extraction is an essential NLP task that aims to automat-\nically identify and extract specific fragments of text pertaining\nto the Patient (P), Intervention (I), Comparator (C), and Out-\ncome (O) elements from unstructured biomedical texts, such as\nresearch articles and clinical trial reports. Typically , Compara-\ntor labels are omitted from the annotations, as they conform to\nestablished clinical trial norms, with \"placebo\" as the passive con-\ntrol and \"standard of care\" as the active control. To conduct our\nstudy , we leveraged the EBM PICO31 dataset for this purpose.\n12 | 1–15\n+ P V S O B M \u0001 / B N F \r \u0001 < Z F B S > \r \u0001 < W P M \u000f > \r\n\n5.3.7.1 EBM PICO The EBM PICO dataset was specifically cre-\nated to facilitate PICO extraction tasks. It employs token-level la-\nbeling, where each token is categorized into one of the PIO classes\n(Patient, Intervention, Outcome). The dataset comprises a total\nof 4,800 labeled abstracts for training purposes and 200 labeled\nabstracts for testing purposes.\nTo conduct the PICO extraction task in a text-to-text format,\nwe adopted the same prompt style as used for the Named Entity\nRecognition (NER) dataset.\nEvaluation metric: to evaluate the quality of PICO extraction\ntasks we utilize the word-level F-1 score.\nExample:\ninput text with prompt: Please find all instances of Interventions\nin the given text. Each mention should be surrounded by \"In-\ntervention*\" and \"*Intervention\": Study protocol : Rehabilitation\nincluding Social and Physical activity and Education in Children\nand Teenagers with Cancer ( RESPECT )\noutput text: Study protocol : Intervention* Rehabilitation in-\ncluding Social and Physical activity and Education *Intervention\nin Children and Teenagers with Cancer ( RESPECT ) .\nNotes and references\n1 J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, Proceedings\nof the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), Minneapo-\nlis, Minnesota, 2019, pp. 4171–4186.\n2 C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y. Zhou, W. Li and P. J. Liu, Journal of Machine\nLearning Research, 2020, 21, 1–67.\n3 M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed,\nO. Levy , V. Stoyanov and L. Zettlemoyer, Proceedings of the\n58th Annual Meeting of the Association for Computational\nLinguistics, Online, 2020, pp. 7871–7880.\n4 T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry , A. Askell,\nS. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse,\nM. Chen, E. Sigler, M. Litwin, S. Gray , B. Chess, J. Clark,\nC. Berner, S. McCandlish, A. Radford, I. Sutskever and\nD. Amodei, Advances in Neural Information Processing Sys-\ntems, 2020, pp. 1877–1901.\n5 R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora,\nS. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill\net al., arXiv preprint arXiv:2108.07258, 2021.\n6 E. Tutubalina, Z. Miftahutdinov, V. Muravlev and A. Shneyder-\nman, Proceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP): Industry Track,\nAbu Dhabi, UAE, 2022, pp. 596–605.\n7 Z. Miftahutdinov, A. Kadurin, R. Kudrin and E. Tutubalina,\nBioinformatics, 2021, 37, 3856–3864.\n8 Z. Miftahutdinov, A. Kadurin, R. Kudrin and E. Tutubalina,\nLecture Notes in Computer Science (including subseries Lecture\nNotes in Artificial Intelligence and Lecture Notes in Bioinformat-\nics), 2021, 12656 LNCS, 451–466.\n9 E. Tutubalina, A. Kadurin and Z. Miftahutdinov,COLING 2020\n- 28th International Conference on Computational Linguistics,\nProceedings of the Conference, 2020, 6710–6716.\n10 A. Aliper, R. Kudrin, D. Polykovskiy , P. Kamya, E. Tutubalina,\nS. Chen, F. Ren and A. Zhavoronkov,Clinical Pharmacology &\nTherapeutics, 2023, n/a,.\n11 E. Putin, A. Asadulaev, Q. Vanhaelen, Y. Ivanenkov, A. V. Al-\nadinskaya, A. Aliper and A. Zhavoronkov, Molecular pharma-\nceutics, 2018, 15, 4386–4397.\n12 D. Polykovskiy , A. Zhebrak, D. Vetrov, Y. Ivanenkov, V. Al-\nadinskiy , P. Mamoshina, M. Bozdaganyan, A. Aliper, A. Zha-\nvoronkov and A. Kadurin, Molecular pharmaceutics, 2018, 15,\n4398–4405.\n13 R. Shayakhmetov, M. Kuznetsov, A. Zhebrak, A. Kadurin,\nS. Nikolenko, A. Aliper and D. Polykovskiy ,Frontiers in Phar-\nmacology, 2020, 11, 269.\n14 A. Aliper, S. Plis, A. Artemov, A. Ulloa, P. Mamoshina and\nA. Zhavoronkov, Molecular pharmaceutics , 2016, 13, 2524–\n2530.\n15 M. Kuznetsov and D. Polykovskiy ,Proceedings of the AAAI Con-\nference on Artificial Intelligence, 2021, 35, 8226–8234.\n16 H. Dowden and J. Munro, Nature Reviews Drug Discovery ,\n2019, 18, 495–496.\n17 J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So and J. Kang,\nBioinformatics, 2020, 36, 1234–1240.\n18 R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn,\nE. Saravia, A. Poulton, V. Kerkez and R. Stojnic, Galactica: A\nLarge Language Model for Science, 2022.\n19 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. u. Kaiser and I. Polosukhin, Advances in Neural\nInformation Processing Systems, 2017.\n20 C. Edwards, T. Lai, K. Ros, G. Honke, K. Cho and H. Ji, Pro-\nceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, 2022, pp. 375–413.\n21 D. Flam-Shepherd, K. Zhu and A. Aspuru-Guzik, Nature Com-\nmunications, 2022, 13, 3293.\n22 D. Flam-Shepherd and A. Aspuru-Guzik, arXiv preprint\narXiv:2305.05708, 2023.\n23 H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay , W. Fedus,\nE. Li, X. Wang, M. Dehghani, S. Brahma et al., arXiv preprint\narXiv:2210.11416, 2022.\n24 O. Kuchaiev, J. Li, H. Nguyen, O. Hrinchuk, R. Leary , B. Gins-\nburg, S. Kriman, S. Beliaev, V. Lavrukhin, J. Cook, P. Cas-\ntonguay , M. Popova, J. Huang and J. M. Cohen, CoRR, 2019,\nabs/1909.09577, year.\n25 Y. Fang, X. Liang, N. Zhang, K. Liu, R. Huang, Z. Chen, X. Fan\nand H. Chen, The Twelfth International Conference on Learn-\ning Representations, 2024.\n26 Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse,\nA. S. Pappu, K. Leswing and V. Pande,Chemical science, 2018,\n9, 513–530.\n27 J. Li, Y. Sun, R. J. Johnson, D. Sciaky , C.-H. Wei, R. Leaman,\nA. P. Davis, C. J. Mattingly , T. C. Wiegers and Z. Lu,Database,\n2016, 2016, baw068.\n+ P V S O B M \u0001 / B N F \r \u0001 < Z F B S > \r \u0001 < W P M \u000f > \r\n1–15 | 13\n28 R. I. Do ˘gan, R. Leaman and Z. Lu, Journal of biomedical infor-\nmatics, 2014, 47, 1–10.\n29 L. Smith, L. K. Tanabe, R. J. n. Ando, C.-J. Kuo, I.-F. Chung,\nC.-N. Hsu, Y.-S. Lin, R. Klinger, C. M. Friedrich, K. Ganchev\net al., Genome biology, 2008, 9, 1–19.\n30 N. Collier, T. Ohta, Y. Tsuruoka, Y. Tateisi and J.-D. Kim, Pro-\nceedings of the International Joint Workshop on Natural Lan-\nguage Processing in Biomedicine and its Applications (NLP-\nBA/BioNLP), Geneva, Switzerland, 2004, pp. 73–78.\n31 B. Nye, J. J. Li, R. Patel, Y. Yang, I. J. Marshall, A. Nenkova\nand B. C. Wallace, Proceedings of the conference. Association\nfor Computational Linguistics. Meeting, 2018, p. 197.\n32 C. Shivade et al. , Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, Brussels,\nBelgium. Association for Computational Linguistics, 2019, pp.\n1586–1596.\n33 T. Khot, A. Sabharwal and P. Clark, Proceedings of the AAAI\nConference on Artificial Intelligence, 2018.\n34 M. Krallinger, O. Rabal, S. A. Akhondi, M. P. Pérez, J. Santa-\nmaría, G. P. Rodríguez, G. Tsatsaronis, A. Intxaurrondo, J. A.\nLópez, U. Nandal et al., Proceedings of the sixth BioCreative\nchallenge evaluation workshop, 2017, pp. 141–146.\n35 M. Herrero-Zazo, I. Segura-Bedmar, P. Martínez and T. De-\nclerck, Journal of biomedical informatics, 2013, 46, 914–920.\n36 À. Bravo, J. Piñero, N. Queralt-Rosinach, M. Rautschka and\nL. I. Furlong, BMC bioinformatics, 2015, 16, 1–17.\n37 G. So ˘gancıo˘glu, H. Öztürk and A. Özgür,Bioinformatics, 2017,\n33, i49–i58.\n38 D. Hanahan and R. A. Weinberg, cell, 2000, 100, 57–70.\n39 Q. Jin, B. Dhingra, Z. Liu, W. Cohen and X. Lu, Proceedings\nof the 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP), 2019, pp.\n2567–2577.\n40 A. Nentidis, K. Bougiatiotis, A. Krithara and G. Paliouras, Ma-\nchine Learning and Knowledge Discovery in Databases: In-\nternational Workshops of ECML PKDD 2019, Würzburg, Ger-\nmany , September 16–20, 2019, Proceedings, Part II, 2020, pp.\n553–568.\n41 D. Weininger, Journal of Chemical Information and Computer\nSciences, 1988, 28, 31–36.\n42 E. Harper, S. Majumdar, O. Kuchaiev, L. Jason, Y. Zhang,\nE. Bakhturina, V. Noroozi, S. Subramanian, K. Nithin, H. Joce-\nlyn, F. Jia, J. Balam, X. Yang, M. Livne, Y. Dong, S. Naren and\nB. Ginsburg, NeMo: a toolkit for Conversational AI and Large\nLanguage Models, 2019, https://github.com/NVIDIA/NeMo.\n43 D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley , M. Patwary ,\nV. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer,\nB. Catanzaro, A. Phanishayee and M. Zaharia, Proceedings of\nthe International Conference for High Performance Comput-\ning, Networking, Storage and Analysis, New York, NY, USA,\n2021.\n44 Y. A. Ivanenkov, D. Polykovskiy , D. Bezrukov, B. Zagribelnyy ,\nV. Aladinskiy , P. Kamya, A. Aliper, F. Ren and A. Zhavoronkov,\nJournal of Chemical Information and Modeling, 2023, 63, 695–\n701.\n45 D. Polykovskiy , A. Zhebrak, B. Sanchez-Lengeling, S. Golo-\nvanov, O. Tatanov, S. Belyaev, R. Kurbanov, A. Artamonov,\nV. Aladinskiy , M. Veselov et al. , Frontiers in pharmacology ,\n2020, 11, 565644.\n46 L. N. Phan, J. T. Anibal, H. Tran, S. Chanana, E. Ba-\nhadroglu, A. Peltekian and G. Altan-Bonnet, arXiv preprint\narXiv:2106.03598, 2021.\n47 E. Tutubalina, Z. Miftahutdinov, V. Muravlev and A. Shneyder-\nman, Proceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing: Industry Track, 2022,\npp. 596–605.\n48 R. Tang, X. Han, X. Jiang and X. Hu, arXiv preprint\narXiv:2303.04360, 2023.\n49 Q. Chen, H. Sun, H. Liu, Y. Jiang, T. Ran, X. Jin, X. Xiao,\nZ. Lin, H. Chen and Z. Niu,Bioinformatics, 2023, 39, btad557.\n50 Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu,\nT. Naumann, J. Gao and H. Poon, ACM Transactions on Com-\nputing for Healthcare (HEALTH), 2021, 3, 1–23.\n51 H.-C. Shin, Y. Zhang, E. Bakhturina, R. Puri, M. Patwary ,\nM. Shoeybi and R. Mani, Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Processing\n(EMNLP), 2020, pp. 4700–4706.\n52\n53 R. Luo, L. Sun, Y. Xia, T. Qin, S. Zhang, H. Poon and T.-Y. Liu,\nBriefings in Bioinformatics, 2022, 23, bbac409.\n54 E. e. a. Bolton, Stanford University, 2022.\n55 M. Krenn, F. Häse, A. Nigam, P. Friederich and A. Aspuru-\nGuzik, Machine Learning: Science and Technology , 2020, 1,\n045024.\n56 M. Krenn, Q. Ai, S. Barthel, N. Carson, A. Frei, N. C. Frey ,\nP. Friederich, T. Gaudin, A. A. Gayle, K. M. Jablonka, R. F.\nLameiro, D. Lemm, A. Lo, S. M. Moosavi, J. M. Nápoles-\nDuarte, A. Nigam, R. Pollice, K. Rajan, U. Schatzschneider,\nP. Schwaller, M. Skreta, B. Smit, F. Strieth-Kalthoff, C. Sun,\nG. Tom, G. Falk von Rudorff, A. Wang, A. D. White, A. Young,\nR. Yu and A. Aspuru-Guzik, Patterns, 2022, 3, 100588.\n57 A. H. Cheng, A. Cai, S. Miret, G. Malkomes, M. Phielipp and\nA. Aspuru-Guzik, Digital Discovery, 2023, 2, 748–758.\n58 K. Preuer, P. Renz, T. Unterthiner, S. Hochreiter and G. Klam-\nbauer, Journal of Chemical Information and Modeling , 2018,\n58, 1736–1741.\n59 C. E. Lipscomb, Bulletin of the Medical Library Association ,\n2000, 88, 265.\n60 A. Pal, L. K. Umapathi and M. Sankarasubbu, Conference on\nHealth, Inference, and Learning, 2022, pp. 248–260.\n61 D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika,\nD. Song and J. Steinhardt, arXiv e-prints, 2020, arXiv–2009.\n14 | 1–15\n+ P V S O B M \u0001 / B N F \r \u0001 < Z F B S > \r \u0001 < W P M \u000f > \r\n\nTable 5 Performance of nach0 on NLP tasks in comparison with FLAN, SciFive, MolT5. We list the scores for each task (see Sec. 5.3 about datasets\nand metrics). All models are base models.\nnach0 FLAN-T5 SciFive MolT5\nNamed Entity Recognition 80.63% 75.01% 81.14% 56.48%\nBC5-chem 91.14% 87.56% 91.81% 64.28%\nBC5-disease 81.72% 76.61% 82.33% 61.56%\nNCBI-disease 84.43% 79.46% 85.33% 54.74%\nBC2GM 72.44% 61.75% 72.76% 45.87%\nJNLPBA 73.42% 69.68% 73.45% 55.93%\nPICO extraction 67.10% 68.94% 67.62% 66.39%\nEBM PICO 67.10% 68.94% 67.62% 66.39%\nTextual Entailment 86.03% 87.53% 86.96% 55.63%\nMedNLI 81.28% 81.75% 82.90% 55.67%\nSciTail 90.77% 93.31% 91.01% 55.58%\nRelation Extraction 84.06% 73.84% 73.22% 63.38%\nChemProt 89.40% 84.48% 82.77% 75.98%\nDDI 89.67% 72.85% 66.08% 63.23%\nGAD 73.11% 64.19% 70.82% 50.93%\nSentence similarity 27.45% 32.78% 1.17% 14.95%\nBIOSSES 27.45% 32.78% 1.17% 14.95%\nDocument Classification 83.83% 75.48% 82.49% 70.99%\nHoC 83.83% 75.48% 82.49% 70.99%\nQuestion answering (Yes/No) 63.87% 65.04% 63.66% 51.6%\nPubMedQA 51.32% 50.36% 52.04% 47.20%\nBioASQ 76.43% 79.71% 75.29% 56.00%\nQuestion answering (Multi Choice) 27.71% 25.61% 26.29% 25.54%\nMedMCQA and MMLU 27.71% 25.61% 26.29% 25.54%\nQuestion answering (Open) 2.43% 2.34% 2.25% 1.83%\nMedMCQA-Open 2.43% 2.34% 2.25% 1.83%\n+ P V S O B M \u0001 / B N F \r \u0001 < Z F B S > \r \u0001 < W P M \u000f > \r\n1–15 | 15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.796034574508667
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.7937930226325989
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6210270524024963
    },
    {
      "name": "Task (project management)",
      "score": 0.5884157419204712
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5231579542160034
    },
    {
      "name": "Encoder",
      "score": 0.49706271290779114
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4913422167301178
    },
    {
      "name": "Natural language processing",
      "score": 0.4481823742389679
    },
    {
      "name": "Knowledge base",
      "score": 0.4292270243167877
    },
    {
      "name": "Programming language",
      "score": 0.12675252556800842
    },
    {
      "name": "Engineering",
      "score": 0.07033079862594604
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}