{
  "title": "Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models",
  "url": "https://openalex.org/W3170962005",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2558037601",
      "name": "Yuxuan Lai",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2106334948",
      "name": "Yijia Liu",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2142380681",
      "name": "Yansong Feng",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2159562265",
      "name": "Songfang Huang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2098869281",
      "name": "Dongyan Zhao",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2938830017",
    "https://openalex.org/W3173220653",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2971871542",
    "https://openalex.org/W2252066972",
    "https://openalex.org/W2950118387",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3122515622",
    "https://openalex.org/W3102725307",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W2099964107",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W25062297",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2904197672",
    "https://openalex.org/W3098065087",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3177365697",
    "https://openalex.org/W2963831883",
    "https://openalex.org/W3035490055",
    "https://openalex.org/W2970323499",
    "https://openalex.org/W3037983807",
    "https://openalex.org/W2962835228",
    "https://openalex.org/W3035051781",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W3034379414",
    "https://openalex.org/W2962904552",
    "https://openalex.org/W2997200074"
  ],
  "abstract": "Yuxuan Lai, Yijia Liu, Yansong Feng, Songfang Huang, Dongyan Zhao. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 1716–1731\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n1716\nLattice-BERT: Leveraging Multi-Granularity Representations in Chinese\nPre-trained Language Models\nYuxuan Lai1,2,∗, Yijia Liu3, Yansong Feng1,2,†, Songfang Huang3 and Dongyan Zhao1,2\n1Wangxuan Institute of Computer Technology, Peking University, China\n2The MOE Key Laboratory of Computational Linguistics, Peking University, China\n3Alibaba Group\n{erutan, fengyansong, zhaody}@pku.edu.cn\n{yanshan.lyj, songfang.hsf}@alibaba-inc.com\nAbstract\nChinese pre-trained language models usually\nprocess text as a sequence of characters, while\nignoring more coarse granularity, e.g., words.\nIn this work, we propose a novel pre-training\nparadigm for Chinese — Lattice-BERT, which\nexplicitly incorporates word representations\nalong with characters, thus can model a sen-\ntence in a multi-granularity manner. Specif-\nically, we construct a lattice graph from the\ncharacters and words in a sentence and feed\nall these text units into transformers. We de-\nsign a lattice position attention mechanism to\nexploit the lattice structures in self-attention\nlayers. We further propose a masked seg-\nment prediction task to push the model to learn\nfrom rich but redundant information inherent\nin lattices, while avoiding learning unexpected\ntricks. Experiments on 11 Chinese natural\nlanguage understanding tasks show that our\nmodel can bring an average increase of 1.5%\nunder the 12-layer setting, which achieves new\nstate-of-the-art among base-size models on the\nCLUE benchmarks. Further analysis shows\nthat Lattice-BERT can harness the lattice struc-\ntures, and the improvement comes from the ex-\nploration of redundant information and multi-\ngranularity representations.1\n1 Introduction\nPre-trained Language Models (PLMs) have\nachieved promising results in many Chinese Nat-\nural Language Understanding (NLU) tasks (Cui\net al., 2019; Liu et al., 2020; Sun et al., 2020).\nThese models take a sequence of ﬁne-grained units\n— Chinese characters — as the input, following\nthe English PLMs’ practice (Devlin et al., 2019,\nBERT).\n∗Work done during an internship at Alibaba DAMO\nAcademy.\n†Corresponding author.\n1Our code will be available at https://github.\ncom/alibaba/pretrained-language-models/\nLatticeBERT.\nᎸ\nstudy\nᑪ\ninvestigate\nኞ\nalive\nၚ\nlive\nஉ\nvery\n꧌\nﬁll\nਫ\nsolid\nᎸᑪኞ\ngraduate student\nᎸᑪ\nresearch\nኞၚ\nlife\n꧌ਫ\nfulﬁlling\nFigure 1: An illustration of the word lattice for sentence\n研究生活很充实/Research Life is very fulﬁlling.\nHowever, the meanings of many Chinese words\ncannot be fully understood through direct compo-\nsitions of their characters’ meanings. For exam-\nple, 老板/boss does not mean 老/elder 板/board.2\nThe importance of word-level inputs in Chinese\nhas been addressed in different tasks, including\nrelation classiﬁcation (Li et al., 2019), short text\nmatching (Lai et al., 2019; Chen et al., 2020; Lyu\net al., 2021), trigger detection (Lin et al., 2018), and\nnamed entity recognition (Zhang and Yang, 2018;\nGui et al., 2019; Li et al., 2020a). The coarse-\ngrained inputs beneﬁt these tasks by introducing\nword-level semantics with multi-granularity repre-\nsentations, which is potentially complementary in\ncharacter-level Chinese PLMs.\nIn this work, we discuss how to pre-train a Chi-\nnese PLM over a word lattice structure to exploit\nmulti-granularity inputs. We argue that by incorpo-\nrating the coarse-grained units into PLM, models\ncould learn to utilize the multi-granularity infor-\nmation for downstream tasks. Speciﬁcally, we or-\nganize characters and words in sentences as word\nlattices (see Figure 1), which enable the models to\nexplore the words from all possible word segmen-\ntation results.\nHowever, it is not straightforward to learn a\nBERT-like PLM over the word lattices. The major\nchallenges are two-folded. Firstly, BERT’s orig-\ninal input is a sequence of characters ordered by\ntheir positions, making it difﬁcult to consume the\nword lattices and preserve the positional relation-\n2For clarity, we use 中文/English translation to represent\nan example in Chinese with its translation followed by.\n1717\nᎸᑪኞၚஉ꧌ਫ\nResearch life is very fulﬁlling.\nWord Lattice\nᎸ\u0003\u0003\nVWXG\\\nᑪ\nLQYHVWLJDWH\nኞ\u0003\u0003\nDOLYH\nၚ\u0003\u0003\u0003\u0003\nOLYH\nᎸᑪ\nUHVHDUFK\nኞၚ\u0003\nOLIH\nᎸᑪኞ\nJUDGXDWH\u0003\nVWXGHQW\n>6(3@>&/6@ உ\u0003\u0003\u0003\u0003\nYHU\\ [M] [M] [M] >6(3@\n(QG\u00103RV 0 1 2 3 4 2 3 3 5 6 7 8 8\n6WDUW\u00103RV 0 1 2 3 4 1 1 2 5 6 7 8 7 9\n9\nSOP\nTransformers with LPA\nOriginal Sentence\nLattice BERT\nMSP\nSegment-1 Segment-2 Segment-3\n꧌ਫ\nIXOƉOOLQJ\nਫ\nVROLG\n꧌\nƉOO\nᎸ\nstudy\nᑪ\ninvestigate\nኞ\nalive\nၚ\nlive\nஉ\nvery\n꧌\nﬁll\nਫ\nsolid\nᎸᑪኞ\ngraduate student\nᎸᑪ\nresearch\nኞၚ\nlife\n꧌ਫ\nfulﬁlling\nFigure 2: An illustration of our pre-training framework, Lattice-BERT.\nship between multi-granularity units. Secondly, the\nconventional masked language modeling (MLM)\ntask may make the word-lattice based PLMs learn\nunexpected tricks. The reason is that such a word\nlattice naturally introduces redundancy, that is, one\ncharacter can be contained in multiple text units.\nIn MLM, models may refer to the other text units\noverlapping with the randomly masked one instead\nof the real context, which brings information leak-\nages.\nTo address these challenges, we propose a\nLattice-based Bidirectional Encoder Representa-\ntion from Transformers (Lattice-BERT). Speciﬁ-\ncally, we design a lattice position attention (LPA)\nto help the transformers directly exploit positional\nrelationship and distances between text units in lat-\ntices. Moreover, we propose a masked segment\nprediction (MSP) task to avoid the potential leak-\nage between overlapping text units in language\nmodeling. With LPA and MSP, the Lattice-BERT\ncould harness the multi-granularity structures in\nlattices, thus, directly utilize the lattice structures\nto aggregate the coarse-grained word information\nto beneﬁt various downstream tasks.\nWe evaluate our model on 11 Chinese NLU tasks\nin various paradigms, including the CLUE bench-\nmarks (Xu et al., 2020) as well as two sequence la-\nbeling tasks. Compared with the baseline that only\ntakes characters as inputs, Lattice-BERT bring an\naverage increase of 1.5% and 2.0% under the set-\ntings of 12 and 6 layers, respectively. The 12-layer\nLattice-BERT model beats all other base-size mod-\nels on CLUE benchmarks.3 Morever, we show that\nLattice-BERT can harness the multi-granularity in-\n3https://www.cluebenchmarks.com/rank.\nhtml, until Oct. 31st, 2020.\nputs and utilize word-level semantics to outperform\nvanilla ﬁne-grained PLMs.\nOur contributions can be summarized as 1)\nWe propose Lattice-BERT to leverage multi-\ngranularity representations from word lattices in\nChinese PLMs. 2) We design lattice position at-\ntention and masked segment prediction to facilitate\nChinese PLMs to exploit the lattice structures. 3)\nLattice-BERT brings remarkable improvements on\n11 Chinese tasks and achieves new state of the arts\namong base-size models at the CLUE benchmarks.\n2 Lattice-BERT\nThis section, we detail the implementation of\nLattice-BERT, and its overall framework is pre-\nsented in Figure 2.\n2.1 Preliminary: BERT\nBERT (Devlin et al., 2019, Bidirectional En-\ncoder Representations from Transformers) is a\npre-trained language model comprising a stack\nof multi-head self-attention layers and fully con-\nnected layers. For each head in the lth multi-head\nself-attention layer, the output matrix Hout,l ={\nhout,l\n1 ,hout,l\n2 ,..., hout,l\nn\n}\n∈Rn×dk satisﬁes:\nhout,l\ni =\nn∑\nj=1\n(\nexp αl\nij∑\nj′exp αl\nij′\nhin,l\nj Wv,l\n)\nαl\nij = 1√2dk\n(\nhin,l\ni Wq,l\n)(\nhin,l\nj Wk,l\n)T\n(1)\nwhere Hin,l =\n{\nhin,l\n1 ,hin,l\n2 ,..., hin,l\nn\n}\n∈Rn×dh\nis the input matrix, and Wq,l,Wk,l,Wv,l ∈\n1718\nRdh×dk are learnable parameters. n and dh are\nsequence length and hidden size, and the attention\nsize dk = dh/nh, where nh is the number of atten-\ntion heads.\nTo capture the sequential features in languages,\nprevious PLMs adopt position embedding in either\ninput representations (Devlin et al., 2019; Lan et al.,\n2020) or attention weights (Yang et al., 2019; Wei\net al., 2019; Ke et al., 2020). For the input-level\nposition embedding, the inputs of the ﬁrst layer\nare ˜hin,0\ni = hin,0\ni + Pi, where Pi is the embed-\nding of the ith position. The other works incorpo-\nrate position information in attention weights, i.e.,\n˜αl\nij = αl\nij + f(i,j), where f is a function of the\nposition pair (i,j).\nThe BERT model is pre-trained on an unlabeled\ncorpus with reconstruction losses, i.e., Masked Lan-\nguage Modeling (MLM) and Next Sentence Pre-\ndiction (NSP), and then ﬁne-tuned on downstream\ntasks to solve speciﬁc NLU tasks. Readers could\nrefer to Devlin et al. (2019) for details.\n2.2 Multi-granularity Inputs: Word Lattices\nWe adopt a word lattice to consume all possible seg-\nmentation results of a sentence in one PLM. Each\nsegmentation can be a mixture of characters and\nwords. As shown in Figure 1, a word lattice is a di-\nrected acyclic graph, where the nodes are positions\nin the original sentences, and each directed edge\nrepresents a character or a plausible word. Word\nlattices incorporate all words and characters so that\nmodels could explicitly exploit the inputs of both\ngranularities, despite some of the words are redun-\ndant. In the rest of this work, we use lattice tokens\nto refer to text units, including the characters and\nwords, contained in lattice graphs.\nAs shown in Figure 2, we list the lattice tokens\nin a line and consume these tokens to transform-\ners straightforwardly. However, the challenges of\nlearning PLMs as BERT over the lattice-like in-\nputs include: 1) encoding the lattice tokens while\npreserving lattice structures; 2) avoiding potential\nleakage brought by redundant information.\n2.3 Interaction: Lattice Position Attention\nSince the original BERT is designed for sequence\nmodeling, it is not straightforward for BERT to con-\nsume a lattice graph. The word lattices encode not\nonly the character sequences but also nested and\noverlapping words from different segmentations.\nTo accurately incorporate positional information\nfrom lattice graphs into the interactions between\nSource\nT.2 (left-det) T.1 (self) T.7 (right-det.)\nT.3 (left-ovl.) T.6 (right-ovl.)T.5 (cted. by)\nT.4 (containing)\nFigure 3: An illustration of the positional relations.\nEach rectangle represents a lattice token, correspond-\ning to a character span in the original sentence. T.1 ∼\nT.7 are the target tokens with the seven different posi-\ntion relation to the Source token.\ntokens, we extend the attention-level position em-\nbedding and propose lattice position attention.\nThe lattice position attention aggregates the at-\ntention score of token representations, αij in Eq. 1,\nwith three position related attention terms, encod-\ning the absolute positions, the distance, and the\npositional relationship, which can be formulated\nas:\n˜αij = αij + attij + bij + rij (2)\nThe attij in Eq. 2 is the attention weight between\nthe absolute positions:\nattij = 1√2dk\n([\nPS\nsi ; PE\nei\n]\nWq)([\nPS\nsj ; PE\nej\n]\nWk\n)T\n[·; ·] means the concatenation of vectors.\nWq,Wk ∈ R2de×dk are learnable parame-\nters, de and dk are embedding size and attention\nsize. si,ei are positions of start and end characters\nof the ith token. Taking the word 研究/research\nin Figure 1 as an example, it starts at the ﬁrst\ncharacter and ends at the second one, thus, its\nsi and ei are 1 and 2, respectively. PS and\nPE are learnable position embedding matrices.\nPS\nt ,PE\nt ∈Rde is the tth embedding vector of PS\nor PE. The attij exploit the prior of attention\nweight between the start and end positions of the\ntoken pairs.\nThe bij in Eq. 2 is the attention term for the\ndistance between the ith and jth tokens, which con-\nsists of four scaling terms considering the combi-\nnations of the start and end positions:\nbij = bss\nsj−si + bse\nsj−ei + bes\nej−si + bee\nej−ei\nbss\nt reﬂects the attention weight brought by the rel-\native distance tbetween the start positions of two\ntokens. The other terms, i.e., bse\nt , bes\nt , and bee\nt , have\nsimilar meanings. In practice, the distance t is\nclipped into [−128,128].\nrij in Eq. 2 is a scaling term represents the posi-\ntional relation between the ith and jth tokens. We\n1719\nconsider seven relations, including (1) self, (2) left\nand detached, (3) left and overlapped, (4) contain-\ning, (5) contained by, (6) right and overlapped, (7)\nright and detached. Figure 3 shows an illustration\nof these 7 relations. Formally, for theith and jth to-\nkens, they are overlapped meanssi ≤sj <ei ≤ej\nor sj ≤si < ej ≤ei, and if ei < sj or ej < si,\nthey are detached. If si ≤ sj ≤ ej ≤ ei and\ni̸= j, the ith token contains the jth token and the\njth token is contained by the ith token. Intuitively,\nonly two detached tokens can be concurrent in one\nChinese word segmentation result. Moreover, the\ncontaining relation reﬂects a sort of lexical hier-\narchy in the lattices. We think rij can explicitly\nmodel the positional relations between tokens in\nlattice graphs.\nWe argue that the attention scores for distances\nand token relations capture different aspects of the\nmulti-granularity structures in lattice graphs, thus,\nmeeting the needs of various downstream tasks,\nsuch as distance for coreference resolution and po-\nsitional relation for named entity recognition. With\nthe information of absolute positions, distances,\nand positional relations, PLMs could accurately\nexploit the lattice structures in attention layers.\nThe lattice position attention weights are shared\nover all layers. bij, rij, Wq, and Wk are diverse\nin different attention heads to capture diverse atten-\ntion patterns. We follow Ke et al. (2020) to reset\nthe positional attention scores related to [CLS] to-\nkens, which is the special token as preﬁx of the\ninput sequences to capture the overall semantics.\n2.4 Pre-training Tasks: Masked Segment\nPrediction\nVanilla BERT is trained to predict the randomly\nmasked tokens in the sentences, i.e., the masked\nlanguage modeling (MLM). For the case of con-\nsuming multi-granularity inputs, the input tokens\nare redundant which means a character can occur in\nits character forms and multiple words it belongs to.\nDirectly adopting the randomly masking strategy\nmay simplify the prediction problem in our case\nbecause the masked token can be easily guessed\nvia peeking the unmasked tokens overlapping with\nthe masked one. Taking the word 研究/research\nin Figure 2 as an example, supposing the masked\ninput is [M]/究/研究, the model will consult 研究\nrather than the context to predict the masked token,\n研.\nWe investigate this problem and ﬁnd that the\ntokens within a minimal segment of the lattice pro-\nvide strong clue for the prediction of other tokens.\nA segment is a connected subgraph of a lattice\nwhere no token exists outside the subgraph that\noverlaps with any token inside the subgraph. To\nidentify these minimal segments, we enumerate\nthe character-level tokens in sentence order, check-\ning if all the word-level tokens which contain this\ncharacter end at this character. If so, all the tokens\ncontaining previous and current characters are con-\nsidered as a segment, and the next segment starts\nfrom the next character, see the example in Figure 2.\nAfter the segment detection, we propose a masked\nsegment prediction (MSP) task as a replacement of\nthe MLM in the original BERT. In MSP, we mask\nout all the tokens in a segment and predict all these\ntokens (see Figure 2) to avoid the potential leakage.\nIn addition to MSP, we also pre-train our models\nwith the sentence order prediction (SOP) task in\nLan et al. (2020), where the model predicts whether\ntwo consecutive sentences are swapped in inputs.\n2.5 Downstream tasks with Lattice-BERT\nWe explore four kinds of downstream tasks,\ni.e., sentence/sentence-pair classiﬁcation, multi-\nple choices, sequence labeling, and span selection\nmachine reading comprehension (MRC). For the\nsentence/sentence-pair classiﬁcation, both vanilla\nand Lattice-BERT classify input instances base\non logistic regressions over the representation of\n[CLS] tokens in the last layer. The circumstances\nare similar in multiple choice tasks, where soft-\nmax regressions are conducted over the represen-\ntations of [CLS] tokens to choose the best options.\nHowever, for the span selection MRC, and the se-\nquence labeling tasks like named entity recognition\n(NER), models need to perform token-wise classi-\nﬁcation. Vanilla BERT predicts labels for the input\ncharacters, but lattice-BERT has additional words.\nIn Lattice-BERT, we extract the character chains\n(word pieces for numbers and English words) from\nlattices for training and prediction for a fair com-\nparison with vanilla BERT. Pilot studies show that\nthis strategy performs comparably with the more\ncomplex strategies, which supervise the labels over\nwords and obtain a character’s label via ensembles\nof all tokens containing that character.\n2.6 Implementation\nLattice Construction. We construct the word\nlattices based on a vocabulary consisting of 102K\nhigh-frequency open domain words. All the sub-\n1720\nstrings of the input sequence that appear in the vo-\ncabulary are considered lattices tokens of the input.\nWith Aho-Corasick automaton (Aho and Corasick,\n1975), this construction procedure can complete in\nlinear time to the size of the corpus and the vocab-\nulary.4 To deal with English words and numbers\nwhere the substrings are meaningless, we use the\ncharacter sequences for those out-of-vocabulary\nnon-Chinese inputs and remain the in-vocabulary\nwords and word pieces.\nWe construct word lattices using all possible\nwords according to a vocabulary instead of more\nsophisticated lattice construction strategies. Previ-\nous research efforts (Lai et al., 2019; Chen et al.,\n2020; Li et al., 2020b) on lattice construction sug-\ngests that using all possible words usually yields\nbetter performance. We think an overly-designed\nlattice construction method may bias our model on\ncertain types of text, and would probably harm the\ngeneralization. So, in our case, we let the model\nlearn by itself to ﬁlter the noise introduced by using\nall possible words during pre-training on a large-\nscale corpus.\nPre-training Details. To compare with previous\npre-training works, we implement the base-size\nmodels, which contains 12 layers, 768-dimensional\nof hidden size, and 12 attention heads. To demon-\nstrate how lattice gains in shallower architectures\nand provide lightweight baselines, we also conduct\nthe lite-size models with 6 layers, 8 attention heads,\nand the hidden size of 512.\nTo avoid the large vocabulary introducing too\nmany parameters in embedding matrix, we adopt\nthe embedding decomposition trick following Lan\net al. (2020, ALBERT). Consequently, the pa-\nrameters of Lattice-BERT is 100M in base-size,\nonly 11% more than its character-level counterpart\n(90M), and smaller than the RoBERTa-base (Liu\net al., 2019) (102M) and AMBERT (Zhang and\nLi, 2020) (176M). The modeling of positional re-\nlation and distances in lattice position attention\nintroduces only 12K parameters.\nA collection of Chinese text, including Chinese\nWikipedia, Zhihu, and web news, is used in our\nBERT models’ pre-training stage. The total num-\nber of characters in our unlabeled data is 18.3G.\nWe follow Liu et al. (2019) and train the PLMs\nwith a large batch size of 8K instances for 100K\n4Formally, the time complexity isO((N+ M) ×L). Nis\nthe corpus size, M is the vocabulary size, and Lis the average\nlength of the words in characters, which is a small constant.\nsteps. The hyper-parameters and details are given\nin Appendix C.\n3 Experiments\nWe present the details of the Lattice-BERT ﬁne-\ntuning results on 11 Chinese NLU tasks. An-\nswering the following questions: (1) Whether\nthe Lattice-BERT performs better than mono-\ngranularity PLMs and other multi-granularity\nPLMs? (2) How the proposed lattice position at-\ntention and masked segment prediction contribute\nto the downstream tasks? (3) How Lattice-BERT\noutperforms the original character-level PLMs?\n3.1 Tasks\nWe test our models on 11 Chinese NLU tasks, in-\ncluding the text classiﬁcation and Machine Reading\nComprehension (MRC) tasks in the Chinese Lan-\nguage Understanding Evaluation benchmark (Xu\net al., 2020, CLUE), and two additional tasks to\nprobe the effectiveness in sequence labeling.\nCLUE text classiﬁcation: natural language in-\nference CMNLI, long text classiﬁcation IFLY-\nTEK (IFLY.), short text classiﬁcation TNEWS, se-\nmantic similarity AFQMC, coreference resolution\n(CoRE) CLUEWSC 2020 (WSC.), and key word\nrecognition (KwRE) CSL.\nCLUE MRC : Span selection based MRC\nCMRC 2018 (CMRC), multiple choice questions\nC3, and idiom cloze ChID.\nSequence Labeling : Chinese word segmen-\ntation (CWS) MSR dataset from SIGHAN2005\n(Emerson, 2005), and named entity recognition\n(NER) MSRA-NER (Levow, 2006).\nWe probe our proposed Lattice-BERT model\nthoroughly with these various downstream tasks.\nThe statistics and hyper-parameters of each task are\nelaborated in Appendix B. We tune learning rates\non validation sets and report test results with the\nbest developing performances for CLUE tasks. 5\nFor MSR and MSRA-NER, we run the settings\nwith the best learning rates ﬁve times and report the\naverage scoresto ensure the reliability of results.\n3.2 Compared Systems\nRoBERTa(Cui et al., 2020) is the Chinese version\nRoBERTa model (Liu et al., 2019), which adopts\n5We report accuracy / exact-match scores for CLUE tasks,\nand label-F1 / F1 for NER / CWS tasks. CLUE results see\nhttps://www.cluebenchmarks.com/rank.html.\n1721\nTask\nCLUE-Classiﬁcation CLUE-MRC Seq. Labeling\navg.NLI TC SPM CoRE KwRE avg. MRC avg. CWS NER\nCMNLI TNEWS IFLY . AFQMC WSC. CSL CMRC ChID C 3 MSR MSRA\nbase-size settings\nRoBERTa 80.5 67.6 60.3 74.0 76.9 84.7 74.0 75.2 83.6 66.5 75.1 98.2 96.8 78.5\nNEZHA 81.1 67.4 59.5 74.5 - 83.7 - 72.2 84.4 71.8 76.1 - - -\nBERT-word 80.0 68.2 60.0 73.5 75.5 85.2 73.7 41.3 80.9 67.0 63.1 - - -\nAMBERT 81.9 68.6 59.7 73.9 78.3 85.7 74.7 73.3 86.6 69.6 76.5 - - -\nBERT-Our 80.3 67.7 62.2 74.0 79.3 81.6 74.2 72.7 84.1 68.6 75.1 98.4 96.5 78.7\nLBERT 81.1 68.4 62.9 74.8 82.4 84.0 75.6 74.0 86.6 72.7 77.8 98.6 97.1 80.2\nlite-size settings\nBERT-Our 77.9 66.7 60.7 72.1 62.4 78.7 69.7 68.3 78.7 61.6 69.5 98.1 95.5 74.6\nLBERT 79.1 68.2 61.9 72.4 70.0 81.9 72.3 69.9 81.3 63.6 71.6 98.4 96.2 76.6\nTable 1: The results on testing sets of 11 Chinese tasks. The bold numbers are the best scores in each column.\nthe whole word masking trick and external pre-\ntraining corpus, known as RoBERTa-wwm-ext.6\nNEZHA (Wei et al., 2019) is one of the best Chi-\nnese PLMs with a bag of tricks, which also explores\nattention-level position embedding.\nAMBERT (Zhang and Li, 2020) is the state-of-\nthe-art multi-granularity Chinese PLM, with two\nseparated encoders for words and characters.\nBERT-word is a Chinese PLM baseline, taking\nwords as single-granularity inputs. We obtain the\nresults from Zhang and Li (2020) directly.\nBERT-ouris our implemented BERT model, with\nthe same pre-training corpus, model structures,\nhyper-parameters, and training procedure with\nLattice-BERT, but taking characters as inputs. We\nalso adopt the whole word masking trick.\nLBERT is our proposed Lattice-BERT model, with\nword lattices as inputs, equipping with lattice posi-\ntion attentions and masked segment prediction.\n3.3 Main Results\nIn Table 1, we can see in text classiﬁcation, MRC,\nand sequence labeling tasks, with bothbase and lite\nsizes, LBERT works better than our character-level\nbaselines consistently. LBERT-base outperforms\nall previous base-size PLMs in average scores and\nobtain the best performances in 7 of the 11 tasks.\nComparing with the mono-granularity PLMs\nin base-size, LBERT takes beneﬁts from word-\nlevel information and outperforms its character-\nlevel counterpart, BERT-our, by 1.5% averagely.\nMeanwhile, LBERT performs better than the word-\nlevel model, BERT-word, remarkably on CLUE\ntasks. We think the lattice inputs incorporate\n6https://huggingface.co/hfl/\nchinese-roberta-wwm-ext\ncoarse-grained semantics while avoiding segmen-\ntation errors by combining multiple segmentation\nresults. Therefore, with the multi-granularity treat-\nments in word lattices, PLMs obtain better per-\nformances in downstream tasks than the mono-\ngranularity settings.\nFurthermore, LBERT outperforms the previous\nstate-of-the-art (sota) multi-granularity PLM, AM-\nBERT (Zhang and Li, 2020), by 0.9% in text clas-\nsiﬁcation and 1.3% in MRC, averagely. Different\nfrom modeling the characters and words separately,\nthe graph representations of word lattices could\nenhance the interaction between multi-granularity\ntokens and utilize all possible segmentation results\nsimultaneously. As a result, LBERT achieves a\nnew sota among the base-size models on the CLUE\nleaderboard as well as the sub-leaderboards for text\nclassiﬁcation and MRC tasks.7\nWith lite-size settings, LBERT brings 2.0% im-\nprovement over BERT-our on average, which is\nlarger than the case inbase-size. In CWS, TNEWS,\nand CSL, the lite-size LBERT even outperforms\nthe base-size BERT-our. With more coarse-grained\ninputs, the shallower architectures do not require\ncomplicated interactions to identify character com-\nbinations but utilizing word representations explic-\nitly, thus, narrowing the gap with the deeper ones.\n3.4 Analysis\nAblation Study. We conduct ablation experi-\nments to investigate the effectiveness of our pro-\nposed lattice position attention (LPA) and masked\nsegment prediction (MSP) in downstream tasks.\nTo reduce the computational costs, we base our\n7https://www.cluebenchmarks.com/rank.\nhtml, the sota by the time of submission, Oct. 31st, 2020.\n1722\nWSC. NER. (EF1) CMRC avg.\nBERT-our 66.3 (1.9) 92.8 (0.2) 57.2 (0.7) 72.1\nLBERT 75.3 (1.3) 94.1 (0.1) 64.5 (0.5) 78.0\n–Rel. 75.7 (1.2) 93.7 (0.2) 63.8 (0.8) 77.7\n–Dis. 73.8 (0.7) 93.8 (0.2) 63.1 (0.4) 76.9\n–Dis. –Rel. 72.8 (0.5) 93.6 (0.1) 61.7 (0.4) 76.0\n–MSP 72.2 (1.0) 93.9 (0.1) 63.0 (0.7) 76.4\nTable 2: Ablation results in lite-size settings with stan-\ndard deviation in subscripts. EF1 is the entity-level F1\nscore. -Dis. and -Rel. represent the ablation of the\nrelative distances and positional relations in LPA, re-\nspectively. The small numbers in brackets are standard\ndeviation scores over ﬁve runs.\npre-training settings on lite-size with the sequence\nlength of 128 characters. We select one task from\neach of the task clusters. We use the entity-level F1\nscore for NER to highlight the inﬂuence on bound-\nary prediction. We report theaverage scoresover 5\nruns and use the development sets for CLUE tasks.\nWe can see in Table 2 that the ablation of either\nmodule (–Dis.–Rel. & –MSP) leads to a substantial\ndrop in the average scores. In particular, replacing\nMSP with vanilla MLM, the average score of –\nMSP drops by 1.6%. For the WSC. task, where\nlong-range dependency is required to resolve the\ncoreference, the gap is high up to 3.1%. We trace\nthis drop into the pre-training procedure and ob-\nserve the MLM accuracy for the –MSP setting on\nthe development set is 88.3%. However, if we mask\nthe tokens within the segment and avoid potential\nleakages, the accuracy drastically drops to 48.8%,\nmuch lower than the performance of LBERT train-\ning with MSP (56.6%). This gap provides evidence\nthat the MSP task prevents the PLMs from tricking\nthe target by peeking the overlapping text units in\none segment, thus encourages the PLMs to charac-\nterize the long-range dependency.\nFor the LPA method, without the positional re-\nlation (–Rel.), the entity-level F1 score on NER\ndecreases by 0.4%, and the performance on CMRC\ndecreases by 0.7%. The performance drops are\nsimilar to the case without distance information\n(–Dis.). Without either of them (–Dis. –Rel.),\nthe gaps widen to 0.5% and 2.8%, respectively.\nThe boundary predictions in NER and CMRC are\nmore sensitive to the local linguistic structures like\nnested words or overlapping ambiguity. With the\npositional relation and distance charaterized in at-\ntention, LBERT could accurately model the inter-\naction between the nested and overlapping tokens\nin different segmentation results. Meanwhile, the\naccuracy of WSC. remarkably drops without dis-\ntance information. The performance drops by 7.5%\nand 5.8% when the number of characters between\nthe pronouns and candidate phrases is larger than\n30, or between 20 to 30, respectively. For the rest\ncases, the drop is only 0.4%. With explicitly model-\ning of distance, LBERT predicts the long-distance\ncoreference relations more accurately. Averagely,\nwithout the positional relation and distance mod-\neling in LPA, the performance drops by 2.0% on\nthe three tasks, showing the importance of LPA in\nassisting the PLMs to exploit the multi-granularity\nstructures in word lattices.\nHow LBERT Improves Fine-grained PLMs?\nWe compare the prediction results of LBERT and\nthe character-level BERT-our inbase-size on devel-\nopment sets to investigate how the LBERT outper-\nforms the vanilla ﬁne-grained PLMs. Intuitively,\nthe word-level tokens in lattices provide coarse-\ngrained semantics, which argument the character-\nlevel inputs.\nWe observe in TNEWS, the short text classiﬁ-\ncation task, LBERT brings more improvement in\nthe shorter instances, where the statements may\nbe too short to provide enough context for pre-\ndictions. By dividing the development set into\nﬁve bins with equal size according to the sentence\nlength, LBERT outperforms BERT-our by 2.3%\nand 1.3% in the shortest and second shortest bins,\nrespectively, larger than the average gain on the\nrest instances (0.6%). We think the redundant to-\nkens in word lattices provide rich context for the\nsemantics of these short statements. For example,\nfor the short title 我们村的电影院/the cinema in\nour village, with the redundant words, 电影/movie,\n影院/cinema, and 电影院/cinema, introduced in\nthe lattice, LBERT classiﬁes the instance as enter-\ntainment news instead of news stories.\nAnother case is the CSL task, where the target\nis to predict whether the candidate words are key-\nwords for a given paragraph. For those instances,\nwhere LBERT identiﬁes more than two word-level\ntokens from each candidate word averagely, which\naccounts for 47% of the dataset, the performance\ngain is 3.0%, signiﬁcantly larger than the average\nimprovement of the rest, 1.0%. We think LBERT\nunderstands the key words from various aspects by\nexploiting the redundant expressions in lattices. For\nexample, from the keyword candidate 太阳能电\n池/solar battery, the 太阳/solar, 太阳能/solar en-\nergy, 电池/battery, and 太阳能电池/solar battery\n1723\nFigure 4: Visualization of the attention scores of 研\n究生活很充实/Research life is very fulﬁlling . The\nthree lines from top to bottom are the attentions before\nﬁne-tuning and after ﬁne-tuning with MSRA-NER and\nTNEWS tasks, respectively.\nare lattice tokens. With these word-level tokens,\nLBERT could match this candidate with the expres-\nsions in the paragraph like 阳极/positive electrode,\n光/light, 电子/electron, 离子/ion, etc.\nOn the other side, for MSRA-NER, LBERT re-\nduces the errors in identifying entities with nested\nstructures. Averagely, the number of error cases\nwhere the predicted entities are nested with the\ngolden ones are reduced by 25% in LBERT. For\nexample, the organization entity 解放巴勒斯坦\n运动/Palestine National Liberation Movement is\nnested with the location entity 巴勒斯坦/Palestine\nand ends with an indicator to organizations, 运\n动/movement. The character-level baseline model\nmistakenly recognizes the 巴勒斯坦/Palestine and\n动/move as a location and an organization, sepa-\nrately. While LBERT identiﬁes this entity correctly\nafter integrating the words, 解放/liberate, 巴勒斯\n坦/Palestine, and 运动/movement. With the pre-\ntrained multi-granularity representations, LBERT\nfuse the contextual information from words and\ncharacters simultaneously, and detects the correct\nentity in success.\nHow does LBERT harness multi-Granularity\nrepresentations? LBERT consumes all the\nwords and characters from input sequences simulta-\nneously, but how does the model utilizes such multi-\ngranularity representations during pre-training and\ndownstream tasks? To investigate this, we use the\naverage attention scores that each lattice token re-\nceives among all layers and all heads to represent\nits importance.\nAs the example shown in Figure 4, before ﬁne-\ntuning, LBERT focuses on tokens including活/live,\n充实/ fulﬁlling, 研究/research, 研究生/graduate\nstudent, 究/investigate, etc. Before ﬁne-tuning on\nspeciﬁc tasks, the model captures various aspects of\nthe sentence. After ﬁne-tuning with MSRA-NER,\nthe most focused words become 充实/fulﬁlling,\nQuestion: What is the game with the theme song which is\nsung by Chen Yiting and is composed by Zeng Zhihao?\nDocument: ··· He was recommended by Lu Shengfei, the\nNo.7 music director of the Cape, as the composer of the\ntheme song The South of China[err]. ··· In cooperation\nwith singer Chen Yiting, Zeng Zhihao was responsible for\nthe composition of the theme song Wish and the promotion\nsong Youth Love forThe Legend of Sword and Fairy V[ans].\nTable 3: An example in CMRC. Given the Question,\nLBERT predicts the corrected answer[ans] in the Docu-\nment, while BERT-our predicts the wrong answer[err].\n很/very, 生活/life, and 研究/research, i.e., the to-\nkens from the golden segmentation result, “研究|生\n活|很|充实”, which is intuitively beneﬁcial for the\nNER tasks. The attention score of the wrong seg-\nmented word, 研究生/graduate student, drops re-\nmarkably.\nOn the other hand, after ﬁne-tuning with the\nnews title classiﬁcation task, TNEWS, LBERT\ntends to focus on 充实/fulﬁlling, 研究生/graduate\nstudent, 生活/life, etc. Although these tokens can\nnot co-exist in one Chinese word segmentation re-\nsult, LBERT can still utilize the redundant informa-\ntion from various plausible segmentations to iden-\ntify the topics of inputs. These results indicate that\nLattice-BERT can well manage the lattice inputs\nby shifting the attention to different aspects among\nthe multi-granularity representations according to\nspeciﬁc downstream tasks.\nCase Study. Table 3 shows an example in\nCMRC, a span selection MRC task, where models\nchoose a text span from the given document to an-\nswer the question. In this case, the question asks\nfor a game, restricted by its theme song. BERT-\nour incorrectly outputs a theme song, The Song\nof China, since there is no expression in the docu-\nment explicitly related to game. However, LBERT\nﬁnd the correct answer, The Legend of Sword and\nFairy V. One possible reason is that The Legend of\nSword and Fairy is an entry in the vocabulary for\nlattice construction. LBERT may have learned this\nword as an entity for a famous video game from the\ncontext in pre-training by explicitly exploiting its\nrepresentation as a whole. With the coarse-grain\ntext units in pre-training, LBERT directly encodes\nknowledge about these units to beneﬁt the down-\nstream tasks.\nComputational Costs. For fair comparisons, we\nensure LBERT and the character-level baselines\n1724\n(i.e., BERT-our) have the same training epochs\nwhen the training steps are equal following previ-\nous works (Diao et al., 2020; Zhang and Li, 2020).\nThus, comparing with BERT-our, 35% more text\nunits are introduced in the pre-training instances\nof LBERT, which introduces 48% more compu-\ntational resources comparing with BERT-our to\nprocess the additional word-level tokens (See Ap-\npendix C). To illustrate the gains attribute to the\nincorporation of lattices instead of additional com-\nputations, we investigate the lite-size BERT-our\nwith longer input sequences in pre-training, which\nhas the same computational costs as LBERT. We\nﬁnd LBERT still outperforms BERT-our by 2.2%\naveragely on CLUE classiﬁcation tasks. More de-\ntails are elaborated in Appendix D.\n4 Related Works\nRecently, several works utilize the lattice structures\nto explore multi-granularity information in Chinese\nNLU tasks. Buckman and Neubig (2018) incorpo-\nrate lattice into recurrent neural networks based lan-\nguage modeling to capture marginal possibilities\nacross all possible paths. In NER, Lattice-LSTM\n(Zhang and Yang, 2018), graph neural networks\n(Gui et al., 2019), and ﬂat-lattice transformers (Li\net al., 2020a) are adopted to incorporate words from\nlattice inputs. Lai et al. (2019) adapt convolutional\nneural networks to lattice for matching based ques-\ntion answering. Chen et al. (2020) adopt graph\nmatching networks to perform multi-granularity in-\nteraction between lattices for text similarity. These\nworks are designed to explore word lattices in spe-\nciﬁc tasks. We explore the multi-granularity repre-\nsentations with word lattices in PLMs, investigating\nthe previously attempted downstream tasks as well\nas other tasks, e.g., MRC. We design LPA to meet\nvarious interaction needs of the downstream tasks\nand propose MSP to avoid the leakages.\nIn the ﬁeld of Chinese PLMs, some efforts\nincorporate coarse-grained information with the\ncharacter-level inputs. ERNIE 1.0 (Sun et al.,\n2019) and BERT-wwm (Cui et al., 2019) propose\nto mask the words, entities, and phrases as a whole\nin the MLM task to encourage the modeling of\ncoarse-grained features. ZEN (Diao et al., 2020)\nadopt auxiliary networks to integrate n-gram rep-\nresentations. BERT-MWA (Li et al., 2020b) pro-\npose word-aligned attention to use multiple seg-\nmentation boundaries. Different from their meth-\nods, we propose Lattice-BERT to consume multi-\ngranularity tokens simultaneously into one PLM\nvia lattice graphs. Thus, Lattice-BERT explicitly\nexploits the representations of the coarse-grained\nunits, as well as the interactions among word- and\ncharacter-level tokens. The proposed MSP task can\nbe treated as an extension of the whole word mask-\ning (Cui et al., 2019), while considering the span\ninformation like Joshi et al. (2020, SpanBERT)\naccording the lattice structures. The concurrent\nwork, Zhang and Li (2020, AMBERT) investigate\nmulti-granularity inputs similarly, but they use two\ntransformer encoders to separately deal the word\nand character sequences. We treat the words and\ncharacters as lattice graphs, which enables thor-\nough interactions among multi-granularity tokens\nand utilizes all potential segmentation results.\n5 Conclusions\nIn this paper, we propose Lattice-BERT to leverage\nmulti-granularity representations of input sentences\nfor Chinese PLMs. Speciﬁcally, Lattice-BERT\ntakes a word-lattice as input, modeling the repre-\nsentations of words and characters simultaneously.\nWe design the lattice position attention to embed\nthe multi-granularity structure into transformers\nand propose the masked segment prediction task to\navoid potential leakage in original MLM caused by\nthe redundancy information in lattices. We conduct\nextensive experiments on 11 Chinese NLU tasks\nand observe consistent gains over character-level\nbaselines, achieving new sota on CLUE bench-\nmarks. We show that Lattice-BERT can well man-\nage the lattice inputs and utilize multi-granularity\nrepresentations to augment the character-level in-\nputs. We believe the lattice structure can be adapted\nto integrate the phrase and word representations\ninto the word-piece based PLMs in other languages,\nwhich we leave for future exploration.\nAcknowledgments\nThis work is supported in part by the Na-\ntional Hi-Tech RD Program of China (No.\n2020AAA0106600), the NSFC under grant agree-\nments (61672057, 61672058). For any correspon-\ndence, please contact Yansong Feng.\n1725\nReferences\nAlfred V Aho and Margaret J Corasick. 1975. Efﬁ-\ncient string matching: an aid to bibliographic search.\nCommunications of the ACM, 18(6):333–340.\nJacob Buckman and Graham Neubig. 2018. Neural lat-\ntice language models. Transactions of the Associa-\ntion for Computational Linguistics, 6:529–541.\nLu Chen, Yanbin Zhao, Boer Lyu, Lesheng Jin, Zhi\nChen, Su Zhu, and Kai Yu. 2020. Neural graph\nmatching networks for Chinese short text matching.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n6152–6158, Online. Association for Computational\nLinguistics.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi-\njin Wang, and Guoping Hu. 2020. Revisiting pre-\ntrained models for Chinese natural language process-\ning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020 , pages 657–668,\nOnline. Association for Computational Linguistics.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin,\nZiqing Yang, Shijin Wang, and Guoping Hu. 2019.\nPre-training with whole word masking for chinese\nbert. arXiv preprint arXiv:1906.08101.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nShizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and\nYonggang Wang. 2020. ZEN: Pre-training Chinese\ntext encoder enhanced by n-gram representations. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020 , pages 4729–4740, Online.\nAssociation for Computational Linguistics.\nThomas Emerson. 2005. The second international Chi-\nnese word segmentation bakeoff. In Proceedings of\nthe Fourth SIGHAN Workshop on Chinese Language\nProcessing.\nTao Gui, Yicheng Zou, Qi Zhang, Minlong Peng, Jin-\nlan Fu, Zhongyu Wei, and Xuanjing Huang. 2019.\nA lexicon-based graph neural network for Chinese\nNER. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n1040–1050, Hong Kong, China. Association for\nComputational Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64–77.\nGuolin Ke, Di He, and Tie-Yan Liu. 2020. Rethinking\npositional encoding in language pre-training. arXiv\npreprint arXiv:2006.15595.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nYuxuan Lai, Yansong Feng, Xiaohan Yu, Zheng Wang,\nKun Xu, and Dongyan Zhao. 2019. Lattice cnns for\nmatching based chinese question answering. In The\nThirty-Third AAAI Conference on Artiﬁcial Intelli-\ngence, AAAI 2019, The Thirty-First Innovative Ap-\nplications of Artiﬁcial Intelligence Conference, IAAI\n2019, The Ninth AAAI Symposium on Educational\nAdvances in Artiﬁcial Intelligence, EAAI 2019, Hon-\nolulu, Hawaii, USA, January 27 - February 1, 2019,\npages 6634–6641. AAAI Press.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nGina-Anne Levow. 2006. The third international Chi-\nnese language processing bakeoff: Word segmen-\ntation and named entity recognition. In Proceed-\nings of the Fifth SIGHAN Workshop on Chinese\nLanguage Processing, pages 108–117, Sydney, Aus-\ntralia. Association for Computational Linguistics.\nXiaonan Li, Hang Yan, Xipeng Qiu, and Xuanjing\nHuang. 2020a. FLAT: Chinese NER using ﬂat-\nlattice transformer. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 6836–6842, Online. Association\nfor Computational Linguistics.\nYanzeng Li, Bowen Yu, Xue Mengge, and Tingwen\nLiu. 2020b. Enhancing pre-trained Chinese charac-\nter representation with word-aligned attention. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 3442–\n3448, Online. Association for Computational Lin-\nguistics.\nZiran Li, Ning Ding, Zhiyuan Liu, Haitao Zheng,\nand Ying Shen. 2019. Chinese relation extraction\nwith multi-grained information and external linguis-\ntic knowledge. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4377–4386, Florence, Italy. Associa-\ntion for Computational Linguistics.\nHongyu Lin, Yaojie Lu, Xianpei Han, and Le Sun.\n2018. Nugget proposal networks for Chinese event\ndetection. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1565–1574, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\n1726\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang,\nQi Ju, Haotang Deng, and Ping Wang. 2020. K-\nBERT: enabling language representation with knowl-\nedge graph. In The Thirty-Fourth AAAI Conference\non Artiﬁcial Intelligence, AAAI 2020, The Thirty-\nSecond Innovative Applications of Artiﬁcial Intelli-\ngence Conference, IAAI 2020, The Tenth AAAI Sym-\nposium on Educational Advances in Artiﬁcial Intel-\nligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 2901–2908. AAAI Press.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nBoer Lyu, Lu Chen, Su Zhu, and Kai Yu. 2021. Let:\nLinguistic knowledge enhanced graph transformer\nfor chinese short text matching. In AAAI.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Florence, Italy.\nAssociation for Computational Linguistics.\nYu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng,\nHao Tian, Hua Wu, and Haifeng Wang. 2020. Ernie\n2.0: A continual pre-training framework for lan-\nguage understanding. In AAAI, pages 8968–8975.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nJunqiu Wei, Xiaozhe Ren, Xiaoguang Li, Weny-\nong Huang, Yi Liao, Yasheng Wang, Jiashu\nLin, Xin Jiang, Xiao Chen, and Qun Liu. 2019.\nNezha: Neural contextualized representation for\nchinese language understanding. arXiv preprint\narXiv:1909.00204.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie\nCao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu,\nCong Yu, Yin Tian, Qianqian Dong, Weitang Liu,\nBo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao\nWang, Weijian Xie, Yanting Li, Yina Patterson,\nZuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua\nLiu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui\nZhang, Zhengliang Yang, Kyle Richardson, and\nZhenzhong Lan. 2020. CLUE: A Chinese language\nunderstanding evaluation benchmark. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 4762–4772, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, pages 5754–5764.\nXinsong Zhang and Hang Li. 2020. Ambert: A pre-\ntrained language model with multi-grained tokeniza-\ntion. arXiv preprint arXiv:2008.11869.\nYue Zhang and Jie Yang. 2018. Chinese NER us-\ning lattice LSTM. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1554–\n1564, Melbourne, Australia. Association for Compu-\ntational Linguistics.\n1727\nA Ethical Considerations\nSimilar to other pre-trained language models\n(Strubell et al., 2019), the study of Lattice-BERT\ninevitably involves lots of computing time/power.\nThe incorporation of multi-granularity tokens gen-\nerally introduces more computational costs than\ncharacter-level PLMs (Diao et al., 2020; Zhang and\nLi, 2020). In this section, we elaborate our efforts\nin reducing the energy costs as well as discuss the\nenergy comparison with the previous state-of-the-\nart multi-granularity Chinese PLM, Zhang and Li\n(2020, AMBERT).\nEfforts in Reducing the Energy Costs. In this\nwork, our efforts in reducing the energy costs can\nbe summarized in two folds, i.e., (1) adopting more\nefﬁcient experiment procedures, and (2) reporting\nperformances of lite-size models to encourage the\nfollowers to compare in lightweight architectures.\nFrom the perspective of experiment procedures,\nwe adopt the mixed-precision arithmetic meth-\nods to speed up pre-training. We also utilize\nthe two-phase pre-training procedures (Devlin\net al., 2019, BERT), where the model processes\nsmaller sequence lengths in 90% steps (See Ap-\npendix C.1). In downstream tasks, we search the\nlearning rates with relatively larger strides with\nheuristic strategies to avoid pointless attempts (See\nAppendix C.2). We adopt ablation studies in lite-\nsize. The lightweight architecture with six-layer\nmodels and 128-character inputs could save much\npower comparing with training the full-lengthbase-\nsize models repeatedly.\nFrom the perspective of reporting strategies, we\nreport the performance of base-size models to-\ngether with lite-size models. As far as we know,\nall previous Chinese PLMs only report base- or\nlarge-size settings (Wei et al., 2019; Diao et al.,\n2020; Sun et al., 2020; Cui et al., 2020; Zhang\nand Li, 2020). Thus, the followers have to im-\nplement at least a 12-layer pre-training model to\nmake a fair comparison. This reporting of lite-size\nperformances facilitate the followers to fast vali-\ndate in lite-settings by directly comparing with our\nLBERT-lite and BERT-our-lite. For the text clas-\nsiﬁcation and sequence labeling tasks, we use the\nmodel trained with 128-sequence length. Thus the\nfollowers could make a fair comparison with our\nmethods without pre-training a full-length model\n(See Appendix C.2). Speciﬁcally, the 128-length\nLBERT-lite model (33M parameters) only costs\n3.8 days pre-training with 8 ×NVIDIA V100 16G\ncards, 1/4 of the original base-size Google-BERT\nmodel.8 In conclusion, by reporting the perfor-\nmances of lite-size models, we encourage follow-\ners to compare with our models with less computa-\ntional costs.\nEnergy Comparison. We make the energy com-\nparison between LBERT and the previous state-\nof-the-art multi-granularity Chinese PLM, Zhang\nand Li (2020, AMBERT), from the following per-\nspectives: (1) How many computational costs\nare introduced to process the additional coarse-\ngrained word-level inputs in pre-training. (2) How\nmany additional parameters are introduced by the\nword-level vocabulary comparing with vanilla ﬁne-\ngrained Chinese PLMs.\nSpeciﬁcally, from the view of additional compu-\ntational costs, comparing with the character-level\nBERT with other settings ﬁxed, AMBERT (Zhang\nand Li, 2020) introduces 100% more computational\ncosts by adopting two encoders to deal with word-\nand character-level inputs separately. Our proposed\nLBERT-base adopts a uniform architecture to con-\nsume words and characters from word-lattices si-\nmultaneously. Therefore, LBERT only introduces\n48% more computational costs comparing with the\ncorresponding character-level baselines (See Ap-\npendix C.1). On the other side, from the view of\nparameter scales, AMBERT has 176M parameters,\nwhere additional 68M parameters are introduced\nby the embedding of word-level tokens. With the\nembedding decomposition trick, LBERT only in-\ntroduces 10M parameters by the word-level units in\nvocabulary. As a result, LBERT-base has 100M pa-\nrameters, comparable to the character-level BERT-\nour (90M) and RoBERTa (102M) models, and\nmuch smaller than AMBERT (176M). In conclu-\nsion, with fewer parameters and smaller compu-\ntational costs, LBERT is more efﬁcient than AM-\nBERT.\nB Dataset Statistics\nIn the Tasks section, we list eleven downstream\ntasks. This note presents several basic statistics, in-\ncluding the sentence length in characters, numbers\nof tokens in lattices, and dataset scales, which is\nshown in Table 5. For the cloze task, ChID, we fo-\ncus on a small context of each blank in ﬁne-tuning,\n8We estimate the pre-training cost of Google-BERT ac-\ncording to Table-3 in Strubell et al. (2019).\n1728\nthus, we do not report the statistics of the whole\npassages.\nWe show the number of training instances\nand the training/development/test split in Table 5\nas well. Speciﬁcally, we count the number of\nquestions/blanks in MRC tasks, the number of\nwords/entities for CWS and NER tasks, and the\nnumbers of sentences/sentence pairs in the CLUE\nclassiﬁcation tasks. Following the previous effort\n(Diao et al., 2020), we use the test set for the vali-\ndation of the MSR-CWS dataset.\nC Implement Details\nOur code is available at https://github.\ncom/alibaba/pretrained-language-models/\nLatticeBERT. Here, we specify some issues in\npre-training and ﬁne-tuning.\nC.1 Pre-training Details.\nLattice-BERT models. We train Lattice-BERT\nwith Adam optimizer (Kingma and Ba, 2015). The\nhyper-parameters of the Lattice-BERT models are\nshown in Table 4. The ratio of masked tokens in\nlanguage modeling is 15%. The embedding sizes\nare different from hidden sizes because we factor-\nize embedding matrices following Lan et al. (2020,\nALBERT) to reduce the additional parameters in-\ntroduced by word-level inputs. For the base-size\nmodels, the numbers of parameters in the character-\nlevel BERT-our and LBERT are 90M and 100M,\nrespectively, less than the corresponding RoBERTa-\nbase (102M) and AMBERT-base (176M). The num-\nbers of parameters for lite-size models are 23M and\n33M for BERT-our and LBERT models, respec-\ntively.\nTo speed up the pre-training, we adopt the two-\nphase procedures of Devlin et al. (2019, BERT),\nwhich ﬁrst pre-trains the model with a sequence\nlength of 128 characters per instance for 90% steps,\nand then trains the rest 10% steps with a sequence\nlength of 512 characters per instance. We base our\ncode on the optimized version of BERT released\nby NVIDIA by leveraging the mixed-precision\narithmetic and the multi-GPU techniques. For\nthe base-size LBERT models, the two pre-training\nphases take about 9.2 and 6.7 days, separately, with\nNVIDIA 8 ×V100 16G cards. For lite-size mod-\nels, the time consumption for the two phases take\n4.0 and 3.1 days. We discuss the energy resources\nin the Ethical Considerations section.\nTo make fair comparison, we expand the maxi-\nHyper-param LBERT-base LBERT-lite\nNumber of Layers 12 6\nHidden Size 768 512\nEmbedding Size 128 128\nFFN Inner Hidden Size 3072 2048\nAttention Heads 12 8\nAttention Head Size 64 64\nDropout 0.1 0.1\nAttention Dropout 0.1 0.1\nActivation Func. GELU GELU\nWarmup Steps 5K 5K\nPeak Learning Rate 6e-4 6e-4\nBatch Size 8192 8192\nMax Steps 100K 100K\nLearning Rate Decay Linear Linear\nAdam ϵ 1e-6 1e-6\nAdam β1 0.9 0.9\nAdam β2 0.999 0.999\nTable 4: Hyper-parameters for pre-training.\nmum size of input tokens in pre-training of LBERT\nto process the additional word-level lattice tokens,\nfollowing previous multi-granularity PLMs in Chi-\nnese (Diao et al., 2020; Zhang and Li, 2020). 9\nParticularly, we ensure each training instance of\ncharacter-level baselines (i.e., BERT-our) and the\nLattice-BERT models contains the same number\nof character-level tokens, which is 128 and 512\nfor the ﬁrst and second pre-training phase, respec-\ntively. As a result, from the view of the pre-training\ndata creation, ﬁxing the corpus size, the numbers of\ninstances per epoch are the same between character-\nlevel BERT-our and LBERT in pre-training given\nthe same corpus. Thus, in the same steps, we can\ntrain BERT-our and LBERT for the same number\nof epochs. In another word, BERT-our and LBERT\nprocess the same size of corpus with the same train-\ning steps.\nIn practice, based on the statistics on the pre-\ntraining corpus, we expand the input size by 35%.\nFor example, the instances in the ﬁrst pre-training\nphase of LBERT have 173 lattice tokens, where 128\nChinese characters are expected to be contained.\nFor the second pre-training phase of LBERT, the\ninput token size is 692.\nHowever, via an empirical estimation, this ex-\ntension makes the pre-training procedure of the\nLattice-BERT models cost 48% more computa-\ntional resources than the corresponding BERT-our\nsettings.10 This increment of time complexity is\n9Under the context of Lattice-BERT, theinput token length\nor the max sequence length means the maximum number of\nlattice tokens per instance that the Lattice-BERT consumes.\n10Theoretically, the time complexity of the attention layers\nto the input length is O\n(\nn2)\n, and that of the fully connected\n1729\nTask Sentence Length Dataset Scale Hyper-parameters\ntp995-C tp995-T Avg-C Avg-T Train Dev. Test max-len. #ep. bs\nCLUE Text Classiﬁcation tasks\nCMNLI 136 179 56.1 74.1 391.8K 12.2K 13.9K 256 4 16\nTNEWS 104 133 41.2 53.0 53.4K 10.0K 10.0K 256 5 16\nIFLY . 1028 1316 291.1 384.9 12.1K 2.6K 2.6K 512 5 8\nAFQMC 78 106 29.7 39.6 34.3K 4.3K 3.9K 256 5 16\nWSC. 147 187 71.7 91.8 1.2K 0.3K 0.3K 256 10 16\nCLS 812 949 298.3 384.4 20.0K 3.0K 3.0K 512 5 8\nCLUE MRC tasks\nCMRC 991 1301 526.7 662.9 10.1K 1.0K 3.2K 512 5 8\nChID – – – – 577.2K 23.0K 23.0K 64 2 24\nC3 1088 1479 224.9 329.1 11.9K 4.3K 3.9K 512 8 12\nSequence Labeling tasks\nMSR-CWS 171 238 48.6 66.6 2.37M - 106.9K 512 5 8\nMSRA-NER 174 247 49.0 67.1 34.0K 3.8K 7.7K 512 10 8\nTable 5: Statistics of the datasets and hyper-parameters for downstream tasks. tp995-C and tp995-T are the top\n99.5% length of the input sequence in characters and the top 99.5% number of lattice tokens, respectively. Avg-C\nand Avg-T are the average numbers of the characters and the lattice tokens per instance. max-len., #ep., and bs\nrepresent the max sequence length of the BERT model (max lattice tokens that Lattice-BERT models consume),\nthe number of epochs, and the batch size, respectively.\nmuch lower than other multi-granularity PLMs like\nAMBERT (Zhang and Li, 2020), which introduces\n100% additional computational resources compar-\ning with the corresponding character-level setting.\nWe further compare the performances of BERT-\nour and LBERT in the downstream tasks under\nthe same pre-training computational costs in Ap-\npendix D, where LBERT still outperforms BERT-\nour by a large margin.\nBERT-our Baselines. The pre-training settings\nof BERT-our are almost the same as our proposed\nLBERT. These settings provide a fair comparison\nto support the argument that the improvements are\nattributed to better utilizing the multi-granularity\ninformation in word lattices. The differences be-\ntween BERT-our and LBERT in model architec-\ntures are: (1) BERT-our use the same vocabulary\nand tokenization functions as the Chinese version\nof Google BERT (Devlin et al., 2019), and taking\ncharacter level inputs, while LBERT takes word\nlattices as inputs. (2) BERT-our has shorter input\nsequences to ensure the Lattice-BERT and BERT-\nour training for the same epochs in the same steps.\n(3) LBERT incorporates the positional relations be-\ntween lattice tokens in attention layers, which is\nlayers is O(n). Thus, the overall increment of time cost is\nbetween 35% and 82% (1.35 ×1.35≈1.82). To empirically\nestimate the complexity, we use the ratio of pre-training time\nbetween taking 128 and 173 input lengths separately, under\nthe setting of lite-size BERT-our.\nnot suitable for BERT-our with sequential inputs.\nThe other components in lattice position attention,\nincluding the absolute position attention and the\ndistance information, are adopted in BERT-our as\nwell. (4) LBERT adopts whole segment prediction\nto avoid potential leakage, while BERT-our adopts\nthe whole word masking (Cui et al., 2019, wwm)\ntrick to utilize the word level information.\nVocabulary. The vocabulary used to construct\nlattices is a superset of the vocabulary in BERT-\nour and the Chinese version of Google-BERT (De-\nvlin et al., 2019). Particular, the vocabulary of\nLBERT consists of 21K tokens (including charac-\nters and word pieces) from the vanilla vocabulary\nof BERT-our and 81K additional high-frequency\nwords from the pre-training corpus. To obtain the\nhigh-frequency words, we randomly sample 10%\nof the pre-training corpus, running an in-house built\ntokenizer, and counting the word frequency after\ntokenization. All the English tokens in this vocab-\nulary are lower-cased, which means LBERT can\nbe seen as an uncased model. However, LBERT\nis a Chinese PLM, thus, only a few English tokens\nexist in pre-training and ﬁne-tuning.\nC.2 Fine-tuning Details.\nHyper-Parameters and Settings We tune the\nlearning rates in (8e-6, 1e-5, 1.5e-5, 2e-5, 3e-5,\n5e-5, 8e-5, 1e-4, 1.5e-4) on the development sets\n1730\ndataset best lr. avg. std. best lr. avg. std. best lr. avg. std. best lr. avg. std.\nBERT-our-base BERT-our-lite LBERT-base LBERT-lite\nCMNLI 1.5e-5 80.6 0.4 8.0e-5 77.6 0.2 1.0e-5 81.6 0.2 3.0e-5 79.2 0.2\nTNEWS 1.5e-5 66.7 0.2 5.0e-5 65.4 0.2 3.0e-5 67.8 0.3 3.0e-5 67.2 0.2\nIFLY . 3.0e-5 60.8 0.9 1.0e-4 59.3 0.2 2.0e-5 61.7 0.1 8.0e-5 60.0 0.3\nAFQMC 2.0e-5 74.3 0.5 5.0e-5 72.3 0.6 1.0e-5 74.6 0.3 3.0e-5 73.1 0.4\nWSC. 5.0e-5 81.5 1.3 1.5e-4 66.3 1.9 2.0e-5 85.5 1.1 8.0e-5 75.3 1.3\nCSL 1.5e-5 81.4 0.3 1.0e-4 78.5 0.7 1.0e-5 83.3 0.2 1.0e-4 81.4 0.4\nCMRC/F1 5.0e-5 86.9 0.2 1.5e-4 83.7 0.4 3.0e-5 87.4 0.5 8.0e-5 84.9 0.2\nCMRC/EM - 67.6 0.4 - 62.6 0.4 - 68.3 0.6 - 64.9 0.6\nChID 1.5e-5 84.6 - 5.0e-5 79.0 - 1.0e-5 86.4 - 2.0e-5 81.7 -\nC3 5.0e-5 69.1 0.3 5.0e-5 61.9 0.6 5.0e-5 72.7 0.4 8.0e-5 63.0 0.5\nMSRA-NER/EF1 2.0e-5 94.6 0.8 5.0e-5 92.8 0.2 5.0e-5 95.6 0.2 5.0e-5 94.1 0.1\nMSRA-NER/LF1 - 96.5 0.5 - 95.5 0.1 - 97.1 0.1 - 96.2 0.1\nMSR-CWS 3.0e-5 98.4 0.0 8.0e-5 98.1 0.0 8.0e-5 98.6 0.0 8.0e-5 98.4 0.0\nAverage - 79.1 - - 75.1 - - 80.6 - - 77.2 -\nTable 6: The full experimental results, with best learning rates (best lr.), corresponding average (avg.) and standard\ndeviation (std.) for ﬁve runs. For the NER task, EF1 and LF1 are entity-level F1 scores and label-level F1 scores,\nrespectively. We average the multiple metrics within each task (e.g., CMRC) before averaging over tasks.\nwith the other hyper-parameters, including max\nsequence length of the BERT model (max-len),\nthe number of epochs (ep.), and the batch size\n(batch), ﬁxed. The hyper-parameters in ﬁne-tuning\nare shown in Table 5.\nSpeciﬁcally, for the sequence labeling tasks, in-\ncluding CWS and NER, we choose the best learn-\ning rates based on the label-level F1 scores on the\ndevelopment sets. In practice, we adopt heuristic\nstrategies to avoid pointless attempts. For exam-\nple, if a model performs monotonic increasing in\na down-stream task with the learning rates of 2e-5,\n3e-5, and 5e-5, we will not try the learning rates\nlower than 2e-5. The best learning rates, together\nwith the average and the standard deviation of the\nscores for ﬁve runs, are shown in Table 6. We\nreport the average and deviation scores on develop-\nment sets for CLUE tasks and those on the test sets\nfor sequence labeling tasks (NER & CWS). In the\nChID task, we run the models once with the best\nlearning rates for efﬁciency, because its training set\nis too large.\nIn text classiﬁcation and sequence labeling tasks,\nthe max sequence length are the same between\nLattice-BERT and the character-level BERT-our,\nthus, their time complexities are the same in ﬁne-\ntuning. However, in MRC tasks, where the docu-\nments are relatively longer, it is essential to read\nthe long passages as complete as possible. Thus, to\nmake fair comparisons, we expand the size of input\nlattices in LBERT settings to ensure the numbers\nof Chinese characters per instances are the same\nbetween LBERT and BERT-our settings.\nSelection of Pre-training Models. We adopt the\nPLMs after the ﬁrst pre-training phase, i.e., pre-\ntraining with the 128-character input size, for the\ntext classiﬁcation, sequence labeling, and ChID\ntasks. The details of the two-phase pre-training\nprocedure refer to Appendix C.1. Pilot experi-\nments demonstrate that, for these tasks, PLMs with\nshorter pre-training lengths perform comparably\nwith the full-length versions. We think this is be-\ncause the sentence lengths are relatively small in\nthese tasks except for IFLY ., the long text classiﬁ-\ncation task, where the ﬁrst few sentences are more\ncrucial for the predictions. This strategy makes our\nexplorations much more efﬁcient. We adopt the\nfull-length pre-training versions for the CMRC and\nC3 tasks.\nDetailed Implementation. For the implementa-\ntion of ﬁne-tuning tasks, we adopt the simplest\nmethods following the CLUE ofﬁcial examples.11\nFor example, we adopt logistic regressions over\nthe [CLS] tokens for classiﬁcations and softmax\nregressions over the [CLS] tokens of all options\nfor multiple choices in C3 and ChID. To deal with\nthe long documents in MRC tasks, we truncate doc-\nument to at most 512 characters in C3. For CMRC,\nwe split the document into segments of at most\n512 characters with the stride of 128 characters.\n11https://github.com/CLUEbenchmark/\nCLUE/tree/master/baselines/models_\npytorch\n1731\nCLUE-Classiﬁcation Seq. Lab.\navg.NLI TC SPM CoRE KwRE avg. CWS NER\nPara. Cmplx. Epoch. CMNLI TNEWS IFLY . AFQMC WSC. CSL MSR MSRA\nBERT-our 23M T N 77.6 65.4 59.3 72.3 66.3 78.5 69.9 98.1 95.5 76.7\n+seql 23M 1.48T 1.35N 77.6 65.3 59.3 72.4 68.4 80.1 70.5 98.1 95.6 77.1\n+seql-EmbDe 31M 1.48T 1.35N 78.4 65.8 59.7 71.6 71.4 79.9 71.1 98.3 95.8 77.6\nLBERT 33M 1.48T N 79.2 67.2 60.0 73.1 75.3 81.4 72.7 98.4 96.2 78.9\nTable 7: The performances on development sets in lite-size settings. Para., Cmplx., and Epoch. are the parameter\nsizes, time complexities, and the training epochs, respectively. We mark the time complexity and the training\nepochs of the BERT-our as T and N.\nThen the selected spans from each segment are ag-\ngregated to ﬁnd the ﬁnal answer to the question.\nFurthermore, for ChID, the contexts no far than 32\ncharacters to the masked idiom are incorporated.\nWe conduct data augmentation for TNEWS and\nCSL tasks. In TNEWS, we use both keywords\nand titles for classiﬁcations. Moreover, for the\nCSL task, we concatenate all the keywords. These\naugmentations are also conducted in the previous\nworks, which is either explicitly mentioned in the\npaper (Zhang and Li, 2020, AMBERT), or can be\ninferred from the performances (Wei et al., 2019,\nNEZHA).12\nD Further Experiments\nMentioned in §3.4 and Appendix C.1, we expand\nthe maximum size of input tokens in pre-training\nof LBERT to process the additional word-level lat-\ntice tokens, following previous multi-granularity\nPLMs in Chinese (Diao et al., 2020; Zhang and\nLi, 2020). We expand the input token length for\nthe pre-training of LBERT by 35%, which makes\nthe LBERT and BERT-our have the same training\nepochs when the training steps are equal, but in-\ntroducing 48% more computational resources (dis-\ncussed in Appendix C). Meanwhile, the additional\nword-level tokens in the vocabulary of LBERT in-\ntroduce 11% and 43% more parameters in the em-\nbedding matrix for the base-size and lite-size set-\ntings, respectively. To illustrate the gains of LBERT\nattribute to the incorporation of lattices instead of\nadditional computations or parameters, we investi-\ngate the BERT-our with longer input sequences in\npre-training and the BERT-our model without the\nembedding decomposition trick, which has more\nparameters in embedding.\n12The TNEWS performance in NEZHA is 67.4%. How-\never, without data augmentation, even the large models like\nALBERT-xxlarge and RoBERTa-wwm-large could not obtain\nan accuracy more than 60% (Xu et al., 2020).\nTo reduce computational costs, we base the ex-\nperiments on the ﬁrst pre-training phase and thelite-\nsize setting (see Appendix C.1 for details). Since\nwe do not conduct full-length pre-training, we use\nCLUE classiﬁcation and sequence labeling tasks.\nWe report the average scoreson development sets\nover ﬁve runs. The compared system is listed be-\nlow:\n+seql is the character-level BERT-our taking the\nsame sequence length (i.e., 173) as LBERT. Com-\nparing with LBERT, this setting results in the\nsame time consumption, together with 35% more\ncharacter-level tokens per instance comparing to\nLBERT models, which results in 35% more cor-\npus/epochs to process with the same training steps.\n-EmbDe is the character-level BERT-our without\nembedding decomposition tricks, i.e., the size of\nembedding matrix is the same as the hidden size,\n512. In the lite-size setting, wihtout embedding\ndecomposition, the character-level BERT-our have\n8M more parameters, and the total parameter scale\nis 31M, comparable to that of LBERT (33M).\nResults. As we can see in Table 7, LBERT re-\nmarkably outperforms the BERT-our settings, even\nafter the extension of time complexity and embed-\nding parameters. Speciﬁcally, on CLUE classiﬁ-\ncation tasks, LBERT outperforms BERT-our+seql\nand BERT-our+seql-EmbDe by 2.2% and 1.6%,\nrespectively. On sequence labeling tasks, the im-\nprovements of +seql and -EmbDe are also marginal.\nThese results demonstrate that the performance\ngains of LBERT attribute to the usage of multi-\ngranularity representations in word lattices instead\nof the additional pre-training time complexities and\nembedding parameters.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7668266296386719
    },
    {
      "name": "Granularity",
      "score": 0.7289915680885315
    },
    {
      "name": "Computational linguistics",
      "score": 0.6103420853614807
    },
    {
      "name": "Natural language processing",
      "score": 0.5772209763526917
    },
    {
      "name": "Language model",
      "score": 0.5521301031112671
    },
    {
      "name": "Artificial intelligence",
      "score": 0.506271243095398
    },
    {
      "name": "Programming language",
      "score": 0.1352233588695526
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    }
  ]
}