{
    "title": "Stabilizing Transformers for Reinforcement Learning",
    "url": "https://openalex.org/W2980433389",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4226201802",
            "name": "Parisotto, Emilio",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4293853833",
            "name": "Song, H. Francis",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4225639001",
            "name": "Rae, Jack W.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222227475",
            "name": "Pascanu, Razvan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4213802994",
            "name": "Gulcehre, Caglar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287304911",
            "name": "Jayakumar, Siddhant M.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3175250689",
            "name": "Jaderberg, Max",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Kaufman, Raphael Lopez",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4225638991",
            "name": "Clark, Aidan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4294619311",
            "name": "Noury, Seb",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4223112191",
            "name": "Botvinick, Matthew M.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202207762",
            "name": "Heess, Nicolas",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202140545",
            "name": "Hadsell, Raia",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2907502844",
        "https://openalex.org/W3038057344",
        "https://openalex.org/W2594466397",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2963948945",
        "https://openalex.org/W2963970792",
        "https://openalex.org/W2963884015",
        "https://openalex.org/W2963870701",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2975395546",
        "https://openalex.org/W2908064123",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2889326796",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2963983719",
        "https://openalex.org/W2950635152",
        "https://openalex.org/W2794752562",
        "https://openalex.org/W2963374479",
        "https://openalex.org/W1810943226",
        "https://openalex.org/W2963985863",
        "https://openalex.org/W1771459135",
        "https://openalex.org/W2963045354",
        "https://openalex.org/W2951032747",
        "https://openalex.org/W2963631907",
        "https://openalex.org/W2302255633",
        "https://openalex.org/W2911935295",
        "https://openalex.org/W2963168530",
        "https://openalex.org/W2964043796",
        "https://openalex.org/W2902286283",
        "https://openalex.org/W2963636093",
        "https://openalex.org/W2963573053",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2593841437",
        "https://openalex.org/W2436711315",
        "https://openalex.org/W104184427",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2945020056",
        "https://openalex.org/W1533861849"
    ],
    "abstract": "Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP), achieving state-of-the-art results in domains such as language modeling and machine translation. Harnessing the transformer's ability to process long time horizons of information could provide a similar performance boost in partially observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL, trained using the same losses, has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical. GTrXL offers an easy-to-train, simple-to-implement but substantially more expressive architectural alternative to the standard multi-layer LSTM ubiquitously used for RL agents in partially observable environments.",
    "full_text": "Preprint\nSTABILIZING TRANSFORMERS\nFOR REINFORCEMENT LEARNING\nEmilio Parisotto∗, H. Francis Song, Jack W. Rae, Razvan Pascanu, Caglar Gulcehre,\nSiddhant M. Jayakumar, Max Jaderberg, Rapha¨el Lopez Kaufman, Aidan Clark, Seb Noury,\nMatthew M. Botvinick, Nicolas Heess, Raia Hadsell\nDeepMind, London, UK\neparisot@cs.cmu.edu,\n{songf,jwrae,razp,caglarg,sidmj,jaderberg,\nrlopezkaufman,aidanclark,snoury,botvinick,heess,raia}@google.com\nABSTRACT\nOwing to their ability to both effectively integrate information over long time\nhorizons and scale to massive amounts of data, self-attention architectures have re-\ncently shown breakthrough success in natural language processing (NLP), achiev-\ning state-of-the-art results in domains such as language modeling and machine\ntranslation. Harnessing the transformer’s ability to process long time horizons of\ninformation could provide a similar performance boost in partially observable re-\ninforcement learning (RL) domains, but the large-scale transformers used in NLP\nhave yet to be successfully applied to the RL setting. In this work we demon-\nstrate that the standard transformer architecture is difﬁcult to optimize, which was\npreviously observed in the supervised learning setting but becomes especially pro-\nnounced with RL objectives. We propose architectural modiﬁcations that substan-\ntially improve the stability and learning speed of the original Transformer and\nXL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), sur-\npasses LSTMs on challenging memory environments and achieves state-of-the-art\nresults on the multi-task DMLab-30 benchmark suite, exceeding the performance\nof an external memory architecture. We show that the GTrXL, trained using the\nsame losses, has stability and performance that consistently matches or exceeds\na competitive LSTM baseline, including on more reactive tasks where memory\nis less critical. GTrXL offers an easy-to-train, simple-to-implement but substan-\ntially more expressive architectural alternative to the standard multi-layer LSTM\nubiquitously used for RL agents in partially observable environments.\n1 I NTRODUCTION\nIt has been argued that self-attention architectures (Vaswani et al., 2017) deal better with longer tem-\nporal horizons than recurrent neural networks (RNNs): by construction, they avoid compressing the\nwhole past into a ﬁxed-size hidden state and they do not suffer from vanishing or exploding gradients\nin the same way as RNNs. Recent work has empirically validated these claims, demonstrating that\nself-attention architectures can provide signiﬁcant gains in performance over the more traditional\nrecurrent architectures such as the LSTM (Dai et al., 2019; Radford et al., 2019; Devlin et al., 2019;\nYang et al., 2019). In particular, the Transformer architecture (Vaswani et al., 2017) has had break-\nthrough success in a wide variety of domains: language modeling (Dai et al., 2019; Radford et al.,\n2019; Yang et al., 2019), machine translation (Vaswani et al., 2017; Edunov et al., 2018), summa-\nrization (Liu & Lapata), question answering (Dehghani et al., 2018; Yang et al., 2019), multi-task\nrepresentation learning for NLP (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019), and\nalgorithmic tasks (Dehghani et al., 2018).\nThe repeated success of the transformer architecture in domains where sequential information pro-\ncessing is critical to performance makes it an ideal candidate for partially observable RL problems,\nwhere episodes can extend to thousands of steps and the critical observations for any decision often\n∗DeepMind and Machine Learning Department, Carnegie Mellon University.\n1\narXiv:1910.06764v1  [cs.LG]  13 Oct 2019\nPreprint\nspan the entire episode. Yet, the RL literature is dominated by the use of LSTMs as the main mecha-\nnism for providing memory to the agent (Espeholt et al., 2018; Kapturowski et al., 2019; Mnih et al.,\n2016). Despite progress at designing more expressive memory architectures (Graves et al., 2016;\nWayne et al., 2018; Santoro et al., 2018) that perform better than LSTMs in memory-based tasks and\npartially-observable environments, they have not seen widespread adoption in RL agents perhaps\ndue to their complex implementation, with the LSTM being seen as the go-to solution for environ-\nments where memory is required. In contrast to these other memory architectures, the transformer\nis well-tested in many challenging domains and has seen several open-source implementations in a\nvariety of deep learning frameworks 1.\nMotivated by the transformer’s superior performance over LSTMs and the widespread availability\nof implementations, in this work we investigate the transformer architecture in the RL setting. In\nparticular, we ﬁnd that the canonical transformer is signiﬁcantly difﬁcult to optimize, often resulting\nin performance comparable to a random policy. This difﬁculty in training transformers exists in the\nsupervised case as well. Typically a complex learning rate schedule is required (e.g., linear warmup\nor cosine decay) in order to train (Vaswani et al., 2017; Dai et al., 2019), or specialized weight\ninitialization schemes are used to improve performance (Radford et al., 2019). These measures do\nnot seem to be sufﬁcient for RL. In Mishra et al. (2018), for example, transformers could not solve\neven simple bandit tasks and tabular Markov Decision Processes (MDPs), leading the authors to\nhypothesize that the transformer architecture was not suitable for processing sequential information.\nHowever in this work we succeed in stabilizing training with a reordering of the layer normaliza-\ntion coupled with the addition of a new gating mechanism to key points in the submodules of the\ntransformer. Our novel gated architecture, the Gated Transformer-XL (GTrXL) (shown in Figure 1,\nRight), is able to learn much faster and more reliably and exhibit signiﬁcantly better ﬁnal perfor-\nmance than the canonical transformer. We further demonstrate that the GTrXL achieves state-of-\nthe-art results when compared to the external memory architecture MERLIN (Wayne et al., 2018)\non the multitask DMLab-30 suite (Beattie et al., 2016). Additionally, we surpass LSTMs signiﬁ-\ncantly on memory-based DMLab-30 levels while matching performance on the reactive set, as well\nas signiﬁcantly outperforming LSTMs on memory-based continuous control and navigation envi-\nronments. We perform extensive ablations on the GTrXL in challenging environments with both\ncontinuous actions and high-dimensional observations, testing the ﬁnal performance of the various\ncomponents as well as the GTrXL’s robustness to seed and hyperparameter sensitivity compared to\nLSTMs and the canonical transformer. We demonstrate a consistent superior performance while\nmatching the stability of LSTMs, providing evidence that the GTrXL architecture can function as a\ndrop-in replacement to the LSTM networks ubiquitously used in RL.\n2 T RANSFORMER ARCHITECTURE AND VARIANTS\nThe transformer network consists of several stacked blocks that repeatedly apply self-attention to the\ninput sequence. The transformer layer block itself has remained relatively constant since its original\nintroduction (Vaswani et al., 2017; Liu et al., 2018; Radford et al., 2019). Each layer consists of two\nsubmodules: an attention operation followed by a position-wise multi-layer network (see Figure 1\n(left)). The input to the transformer block is an embedding from the previous layerE(l−1) ∈RT×D,\nwhere T is the number of time steps, D is the hidden dimension, and l ∈[0,L] is the layer index\nwith Lbeing the total number of layers. We assumeE(0) is an arbitrarily-obtained input embedding\nof dimension [T,D], e.g. a word embedding in the case of language modeling or an embedding of\nthe per-timestep observations in an RL environment.\nMulti-Head Attention:The Multi-Head Attention (MHA) submodule computes in parallelHsoft-\nattention operations for every time step. A residual connection (He et al., 2016a) and layer normal-\nization (Ba et al., 2016) are then applied to the output (see Appendix C for more details):\nY\n(l)\n= MultiHeadAttention(E(l−1)), ˆY(l) = E(l−1) + Y\n(l)\n, Y (l) = LayerNorm( ˆY(l)), (1)\nMulti-Layer Perceptron:The Multi-Layer Perceptron (MLP) submodule applies a 1 ×1 temporal\nconvolutional network f(l) (i.e., kernel size 1, stride 1) over every step in the sequence, producing a\n1e.g. https://github.com/kimiyoung/transformer-xl, https://github.com/\ntensorflow/tensor2tensor\n2\nPreprint\nFigure 1: Transformer variants, showing just a single layer block (there are Llayers total). Left:\nCanonical Transformer(-XL) block with multi-head attention and position-wise MLP submodules\nand the standard layer normalization (Ba et al., 2016) placement with respect to the residual con-\nnection (He et al., 2016a). Center: TrXL-I moves the layer normalization to the input stream of the\nsubmodules. Coupled with the residual connections, there is a gradient path that ﬂows from output\nto input without any transformations. Right: The GTrXL block, which additionally adds a gating\nlayer in place of the residual connection of the TrXL-I.\nnew embedding tensor E(l) ∈RT×D. As in Dai et al. (2019), the network output does not include\nan activation function. After the MLP, there is a residual update followed by layer normalization:\nE\n(l)\n= f(l)(Y(l)), ˆE(l) = Y(l) + E\n(l)\n, E (l) = LayerNorm( ˆE(l)). (2)\nRelative Position Encodings:The basic MHA operation does not take sequence order into account\nexplicitly because it is permutation invariant. Positional encodings are a widely used solution in\ndomains like language where order is an important semantic cue, appearing in the original trans-\nformer architecture (Vaswani et al., 2017). To enable a much larger contextual horizon than would\notherwise be possible, we use the relative position encodings and memory scheme used in Dai et al.\n(2019). In this setting, there is an additional T-step memory tensor M(l) ∈RT×D, which is treated\nas constant during weight updates. The MHA submodule then becomes:\nY\n(l)\n= RelativeMultiHeadAttention(StopGrad(M(l−1)),E(l−1)) (3)\nˆY(l) = E(l−1) + Y\n(l)\n, Y (l) = LayerNorm( ˆY(l)) (4)\nwhere StopGrad is a stop-gradient function that prevents gradients ﬂowing backwards during back-\npropagation. We refer to Appendix C for a more detailed description.\n3 G ATED TRANSFORMER ARCHITECTURES\nWhile the transformer architecture has achieved breakthrough results in modeling sequences for\nsupervised learning tasks (Vaswani et al., 2017; Liu et al., 2018; Dai et al., 2019), a demonstration\nof the transformer as a useful RL memory has been notably absent. Previous work has highlighted\ntraining difﬁculties and poor performance (Mishra et al., 2018). When transformers have not been\nused for temporal memory but instead as a mechanism for attention over the input space, they have\nhad success—notably in the challenging multi-agent Starcraft 2 environment (Vinyals et al., 2019).\nHere, the transformer was applied solely across Starcraft units and not over time.\nMultiplicative interactions have been successful at stabilizing learning across a wide variety of ar-\nchitectures (Hochreiter & Schmidhuber, 1997; Srivastava et al., 2015; Cho et al., 2014). Motivated\nby this, we propose the introduction of powerful gating mechanisms in place of the residual con-\nnections within the transformer block, coupled with changes to the order of layer normalization in\nthe submodules. As will be empirically demonstrated, the “Identity Map Reordering” and gating\nmechanisms are critical for stabilizing learning and improving performance.\n3\nPreprint\n3.1 I DENTITY MAP REORDERING\nOur ﬁrst change is to place the layer normalization on only the input stream of the submodules, a\nmodiﬁcation described in several previous works (He et al., 2016b; Radford et al., 2019; Baevski\n& Auli, 2019). The model using this Identity Map Reordering is termed TrXL-I in the following,\nand is depicted visually in Figure 1 (center). A key beneﬁt to this reordering is that it now enables\nan identity map from the input of the transformer at the ﬁrst layer to the output of the transformer\nafter the last layer. This is in contrast to the canonical transformer, where there are a series of\nlayer normalization operations that non-linearly transform the state encoding. Because the layer\nnorm reordering causes a path where two linear layers are applied in sequence, we apply a ReLU\nactivation to each sub-module output before the residual connection (see Appendix C for equations).\nThe TrXL-I already exhibits a large improvement in stability and performance over TrXL (see Sec-\ntion 4.3.1). One hypothesis as to why the Identity Map Reordering improves results is as fol-\nlows: assuming that the submodules at initialization produce values that are in expectation near\nzero, the state encoding is passed un-transformed to the policy and value heads, enabling the\nagent to learn a Markovian policy at the start of training (i.e., the network is initialized such that\nπ(·|st,...,s 1) ≈π(·|st) and Vπ(st|st−1,...,s 1) ≈Vπ(st|st−1)). In many environments, reac-\ntive behaviours need to be learned before memory-based ones can be effectively utilized, i.e., an\nagent needs to learn how to walk before it can learn how to remember where it has walked.\n3.2 G ATING LAYERS\nWe further improve performance and optimization stability by replacing the residual connections in\nEquations 4 and 2 with gating layers. We call the gated architecture with the identity map reordering\nthe Gated Transformer(-XL) (GTrXL). The ﬁnal GTrXL layer block is written below:\nY\n(l)\n= RelativeMultiHeadAttention(LayerNorm([StopGrad(M(l−1)),E(l−1)])) (5)\nY(l) = g(l)\nMHA(E(l−1),ReLU(Y\n(l)\n)) (6)\nE\n(l)\n= f(l)(LayerNorm(Y(l))) (7)\nE(l) = g(l)\nMLP(Y(l),ReLU(E\n(l)\n)) (8)\nwhere g is a gating layer function. A visualization of our ﬁnal architecture is shown in Figure 1\n(right), with the modiﬁcations from the canonical transformer highlighted in red. In our experiments\nwe ablate a variety of gating layers with increasing expressivity:\nInput: The gated input connection has a sigmoid modulation on the input stream, similar to the\nshort-cut-only gating from He et al. (2016b):\ng(l)(x,y) = σ(W(l)\ng x) ⊙x+ y\nOutput: The gated output connection has a sigmoid modulation on the output stream:\ng(l)(x,y) = x+ σ(W(l)\ng x−b(l)\ng ) ⊙y\nHighway: The highway connection (Srivastava et al., 2015) modulates both streams with a sigmoid:\ng(l)(x,y) = σ(W(l)\ng x+ b(l)\ng ) ⊙x+ (1 −σ(W(l)\ng x+ b(l)\ng )) ⊙y\nSigmoid-Tanh: The sigmoid-tanh (SigTanh) gate (Van den Oord et al., 2016) is similar to the Output\ngate but with an additional tanh activation on the output stream:\ng(l)(x,y) = x+ σ(W(l)\ng y−b) ⊙tanh(U(l)\ng y)\nGated-Recurrent-Unit-type gating: The Gated Recurrent Unit (GRU) (Chung et al., 2014) is a\nrecurrent network that performs similarly to an LSTM (Hochreiter & Schmidhuber, 1997) but has\nfewer parameters. We adapt its powerful gating mechanism as an untied activation function in depth:\nr= σ(W(l)\nr y+ U(l)\nr x), z = σ(W(l)\nz y+ U(l)\nz x−b(l)\ng ), ˆh= tanh(W(l)\ng y+ U(l)\ng (r⊙x)),\ng(l)(x,y) = (1 −z) ⊙x+ z⊙ˆh.\n4\nPreprint\nFigure 2: Average return on DMLab-30, re-scaled such that a human has mean 100 score on each\nlevel and a random policy has 0. Left: Results averaged over the full DMLab-30 suite. Right:\nDMLab-30 partitioned into a “Memory” and “Reactive” split (described in Appendix D). The\nGTrXL has a substantial gain over LSTM in memory-based environments, while even slightly sur-\npassing performance on the reactive set. We plot 6-8 hyperparameter settings per architecture (see\nAppendix B). MERLIN scores obtained from personal communication with the authors.\nGated Identity Initialization:We have claimed that the Identity Map Reordering aids policy op-\ntimization because it initializes the agent close to a Markovian policy / value function. If this is\nindeed the cause of improved stability, we can explicitly initialize the various gating mechanisms to\nbe close to the identity map. This is the purpose of the bias b(l)\ng in the applicable gating layers. We\nlater demonstrate in an ablation that initially setting b(l)\ng >0 can greatly improve learning speed.\n4 E XPERIMENTS\nIn this section, we provide experiments on a variety of challenging single and multi-task RL do-\nmains: DMLab-30 (Beattie et al., 2016), Numpad and Memory Maze (see Fig. 8). Crucially we\ndemonstrate that the proposed Gated Transformer-XL (GTrXL) not only shows substantial improve-\nments over LSTMs on memory-based environments, but suffers no degradation of performance on\nreactive environments. The GTrXL also exceeds MERLIN (Wayne et al., 2018), an external mem-\nory architecture which used a Differentiable Neural Computer (Graves et al., 2016) coupled with\nauxiliary losses, surpassing its performance on both memory and reactive tasks.\nFor all transformer architectures except when otherwise stated, we train relatively deep 12-layer\nnetworks with embedding size 256 and memory size 512. These networks are comparable to the\nstate-of-the-art networks in use for small language modeling datasets (see enwik8 results in (Dai\net al., 2019)). We chose to train deep networks in order to demonstrate that our results do not\nnecessarily sacriﬁce complexity for stability, i.e. we are not making transformers stable for RL\nsimply by making them shallow. Our networks have receptive ﬁelds that can potentially span any\nepisode in the environments tested, with an upper bound on the receptive ﬁeld of 6144 (12 layers ×\n512 memory (Dai et al., 2019)). Future work will look at scaling transformers in RL even further,\ne.g. towards the 52-layer network in Radford et al. (2019). See App. B for experimental details.\nFor all experiments, we used V-MPO (Anonymous Authors, 2019), an on-policy adaptation of Max-\nimum a Posteriori Policy Optimization (MPO) (Abdolmaleki et al., 2018a;b) that performs approx-\nimate policy iteration based on a learned state-value function V(s) instead of the state-action value\nfunction used in MPO. Rather than directly updating the parameters in the direction of the policy\ngradient, V-MPO uses the estimated advantages to ﬁrst construct a target distribution for the policy\nupdate subject to a sample-based KL constraint, then calculates the gradient that partially moves\n5\nPreprint\nthe parameters toward that target, again subject to a KL constraint. V-MPO was shown to achieve\nstate-of-the-art results for LSTM-based agents on the multi-task DMLab-30 benchmark suite.\n4.1 T RANSFORMER AS EFFECTIVE RL M EMORY ARCHITECTURE\nWe ﬁrst present results of the best performing GTrXL variant, the GRU-type gating, against a com-\npetitive LSTM baseline, demonstrating a substantial improvement on the multi-task DMLab-30 do-\nmain (Beattie et al., 2016). Figure 2 shows mean return over all levels as training progresses, where\nthe return is human normalized as done in previous work (meaning a human has a per-level mean\nscore of 100 and a random policy has a score of 0), while Table 1 has the ﬁnal performance at 10 bil-\nlion environment steps. The GTrXL has a signiﬁcant gap over a 3-layer LSTM baseline trained using\nthe same V-MPO algorithm. Furthermore, we included the ﬁnal results of a previously-published\nexternal memory architecture, MERLIN (Wayne et al., 2018). Because MERLIN was trained for\n100 billion environment steps with a different algorithm, IMPALA (Espeholt et al., 2018), and also\ninvolved an auxiliary loss critical for the memory component to function, the learning curves are\nnot directly comparable and we only report the ﬁnal performance of the architecture as a dotted\nline. Despite the differences, our results demonstrate that the GTrXL can match the state-of-the-art\non DMLab-30. An informative split between a set of memory-based levels and more reactive ones\n(listed in Appendix D) reveals that our model speciﬁcally has large improvements in environments\nwhere memory plays a critical role. Meanwhile, GTrXL also shows improvement over LSTMs on\nthe set of reactive levels, as memory can still be effectively utilized in some of these levels.\n4.2 S CALING WITH MEMORY HORIZON\nWe next demonstrate that the GTrXL scales better compared to an LSTM when an environment’s\ntemporal horizon is increased, using the “Numpad” continuous control task of Humplik et al. (2019)\nwhich allows an easy combinatorial increase in the temporal horizon. In Numpad, a robotic agent is\nsituated on a platform resembling the 3x3 number pad of a telephone (generalizable toN×N pads).\nThe agent can interact with the pads by colliding with them, causing them to be activated (visualized\nin the environment state as the number pad glowing). The goal of the agent is to activate a speciﬁc\nsequence of up to N2 numbers, but without knowing this sequence a priori. The only feedback\nthe agent gets is by activating numbers: if the pad is the next one in the sequence, the agent gains\na reward of +1, otherwise all activated pads are cleared and the agent must restart the sequence.\nEach correct number in the sequence only provides reward once, i.e. each subsequent activation of\nthat number will no longer provide rewards. Therefore the agent must explicitly develop a search\nstrategy to determine the correct pad sequence. Once the agent completes the full sequence, all pads\nare reset and the agent gets a chance to repeat the sequence again for more reward. This means\nhigher reward directly translates into how well the pad sequence has been memorized. An image of\nthe scenario is provided in Figure 3. There is the restriction that contiguous pads in the sequence\nmust be contiguous in space, i.e. the next pad in the sequence can only be in the Moore neighborhood\nof the previous pad. Furthermore, no pad can be pressed twice in the sequence.\nWe present two results in this environment in Figure 3. The ﬁrst measures the ﬁnal performance of\nthe trained models as a function of the pad size. We can see that LSTM performs badly on all 3 pad\nsizes, and performs worse as the pad size increases from 2 to 4. The GTrXL performs much better,\nand almost instantly solves the environment with its much more expressive memory. On the center\nand right images, we provide learning curves for the 2 ×2 and 4 ×4 Numpad environments, and\nshow that even when the LSTM is trained twice as long it does not reach GTrXL’s performance.\n4.3 G ATING VARIANTS + IDENTITY MAP REORDERING\nWe demonstrated that the GRU-type-gated GTrXL can achieve state-of-the-art results on DMLab-\n30, surpassing both a deep LSTM and an external memory architecture, and also that the GTrXL\nhas a memory which scales better with the memory horizon of the environment. However, the ques-\ntion remains whether the expressive gating mechanisms of the GRU could be replaced by simpler\nalternatives. In this section, we perform extensive ablations on the gating variants described in Sec-\ntion 3.2, and show that the GTrXL (GRU) has improvements in learning speed, ﬁnal performance\nand optimization stability over all other models, even when controlling for the number of parameters.\n6\nPreprint\nFigure 3: Numpad results demonstrating that the GTrXL has much better memory scaling properties\nthan LSTM. Left: As the Numpad environment’s memory requirement increases (because of larger\npad size), the GTrXL suffers much less than LSTM. However, because of the combinatorial nature\nof Numpad, the GTrXL eventually also starts dropping in performance at 4x4. We plot mean and\nstandard error of the last 200 episodes after training each model for 0.15B, 1.0B and 2.0B environ-\nment steps for Numpad size 2, 3 and 4, respectively.Center, Right:Learning curves for the GTrXL\non 2 ×2 and 4 ×4 Numpad. Even when the LSTM is trained for twice as long, the GTrXL still has\na substantial improvement over it. We plot 5 hyperparameter settings per model for learning curves.\nModel Mean Human Norm. Mean Human Norm., 100-capped\nLSTM 99.3 ±1.0 84.0 ±0.4\nTrXL 5.0 ±0.2 5.0 ±0.2\nTrXL-I 107.0 ±1.2 87.4 ±0.3\nMERLIN@100B 115.2 89.4\nGTrXL (GRU) 117.6 ±0.3 89.1 ±0.2\nGTrXL (Input) 51.2 ±13.2 47.6 ±12.1\nGTrXL (Output) 112.8 ±0.8 87.8 ±0.3\nGTrXL (Highway) 90.9 ±12.9 75.2 ±10.4\nGTrXL (SigTanh) 101.0 ±1.3 83.9 ±0.7\nTable 1: Final human-normalized return averaged across all 30 DMLab levels for baselines and\nGTrXL variants. We also include the 100-capped score where the per-level mean score is clipped at\n100, providing a metric that is proportional to the percentage of levels that the agent is superhuman.\nWe see that the GTrXL (GRU) surpasses LSTM by a signiﬁcant gap and exceeds the performance\nof MERLIN (Wayne et al., 2018) trained for 100 billion environment steps. The GTrXL (Output)\nand the proposed reordered TrXL-I also surpass LSTM but perform slightly worse than MERLIN\nand are not as robust as GTrXL (GRU) (see Sec. 4.3.2). We sample 6-8 hyperparameters per model.\nWe include standard error over runs.\n4.3.1 P ERFORMANCE ABLATION\nWe ﬁrst report the performance of the gating variants in DMLab-30. Table 1 and Figure 4 show the\nﬁnal performance and training curves of the various gating types in both the memory / reactive split,\nrespectively. The canonical TrXL completely fails to learn, while the TrXL-I improves over the\nLSTM. Of the gating varieties, the GTrXL (Output) can recover a large amount of the performance\nof the GTrXL (GRU), especially in the reactive set, but as shown in Sec. 4.3.2 is generally far\nless stable. The GTrXL (Input) performs worse than even the TrXL-I, reinforcing the identity map\npath hypothesis. Finally, the GTrXL (Highway) and GTrXL (SigTanh) are more sensitive to the\nhyperparameter settings compared to the alternatives, with some settings doing worse than TrXL-I.\n4.3.2 H YPERPARAMETER AND SEED SENSITIVITY\nBeyond improved performance, we next demonstrate a signiﬁcant reduction in hyperparameter and\nseed sensitivity for the GTrXL (GRU) compared to baselines and other GTrXL variants. We use the\n“Memory Maze” environment, a memory-based navigation task in which the agent must discover\nthe location of an apple randomly placed in a maze of blocks. The agent receives a positive reward\nfor collecting the apple and is then teleported to a random location in the maze, with the apple’s\nposition held ﬁxed. The agent can make use of landmarks situated around the room to return as\nquickly as possible to the apple for subsequent rewards. Therefore, an effective mapping of the\nenvironment results in more frequent returns to the apple and higher reward.\n7\nPreprint\nFigure 4: Learning curves for the gating mechanisms, along with MERLIN score at 100 billion\nframes as a reference point. We can see that the GRU performs as well as any other gating mech-\nanism on the reactive set of tasks. On the memory environments, the GRU gating has a signiﬁcant\ngain in learning speed and attains the highest ﬁnal performance at the fastest rate. We plot both\nmean (bold) and the individual 6-8 hyperparameter samples per model (light).\nFigure 5: Sensitivity analysis of GTrXL variants versus TrXL and LSTM baselines. We sample 25\ndifferent hyperparameter sets and seeds and plot the ranked average return at 3 points during training\n(0.5B, 1.0B and 2.0B environment steps). Higher and ﬂatter lines indicate more robust architectures.\nThe same hyperparameter sampling distributions were used across models (see Appendix B). We\nplot human performance as a dotted line.\nModel Mean Human\nNorm. Score # Param.\nLSTM 99.3 ±1.0 9.25M\nTrXL 5.0 ±0.2 28.6M\nTrXL-I 107.0 ±1.2 28.6M\nThin GTrXL (GRU) 111.5 ±0.6 22.4M\nGTrXL (GRU) 117.6 ±0.3 66.4M\nGTrXL (Input) 51.2 ±13.2 34.9M\nGTrXL (Output) 112.8 ±0.8 34.9M\nGTrXL (Highway) 90.9 ±12.9 34.9M\nGTrXL (SigTanh) 101.0 ±1.3 41.2M\nTable 2: Parameter-Controlled Comparisons. Pa-\nrameter count given in millions. The standard er-\nror of the means of the 6-8 runs per model re-\nported in brackets.\nModel % Diverged\nLSTM 0%\nTrXL 0%\nTrXL-I 16%\nGTrXL (GRU) 0%\nGTrXL (Output) 12%\nTable 3: Percentage of the 25 parameter\nsettings where the training loss diverged\nat any point within 2 billion environ-\nment steps. We do not report numbers\nfor GTrXL gating types that were un-\nstable in DMLab-30. For diverged runs\nwe plot the returns in Figure 5 as 0 af-\nterwards.\nWe chose to perform the sensitivity ablation on Memory Maze because (1) it requires the use of long-\nrange memory to be effective and (2) it includes both continuous and discrete action sets (details in\nAppendix A) which makes optimization more difﬁcult. In Figure 5, we sample 25 independent\nV-MPO hyperparameter settings from a wide range of values and train the networks to 2 billion\nenvironment steps (see Appendix B). Then, at various points in training (0.5B, 1.0B and 2.0B), we\nrank all runs by their mean return and plot this ranking. Models with curves which are both higher\n8\nPreprint\nFigure 6: Learning curves comparing a thinner GTrXL (GRU) with half the embedding dimension\nof the other presented gated variants and TrXL baselines. The Thin GTrXL (GRU) has fewer pa-\nrameters than any other model presented but still matches the performance of the best performing\ncounterpart, the GTrXL (Output), which has over 10 million more parameters. We plot both mean\n(bold) and 6-8 hyperparameter settings (light) per model.\nFigure 7: Ablation of the gated identity initialization on Memory Maze by comparing 10 runs of a\nmodel run with the bias initialization and 10 runs of a model without. Every run has independently\nsampled hyperparameters from a distribution. We plot the ranked mean return of the 10 runs of each\nmodel at 1, 2, and 4 billion environment steps. Each mean return is the average of the past 200\nepisodes at the point of the model snapshot. We plot human performance as a dotted line.\nand ﬂatter are thus more robust to hyperparameters and random seeds. Our results demonstrate that\n(1) the GTrXL (GRU) can learn this challenging memory environment in much fewer environment\nsteps than LSTM, and (2) that GTrXL (GRU) beats the other gating variants in stability by a large\nmargin, thereby offering a substantial reduction in necessary hyperparameter tuning. The values in\nTable 3 list what percentage of the 25 runs per model had losses that diverged to inﬁnity. We can\nsee that the only model reaching human performance in 2 billion environment steps is the GTrXL\n(GRU), with 10 runs having a mean score 8 and above.\n4.3.3 P ARAMETER COUNT -CONTROLLED COMPARISONS\nFor the ﬁnal gating ablation, we compare transformer variants while tracking their total parameter\ncount to control for the increase in capacity caused by the introduction of additional parameters in the\ngating mechanisms. To demonstrate that the advantages of the GTrXL (GRU) are not due solely to an\nincrease in parameter count, we halve the number of attention heads (which also effectively halves\nthe embedding dimension due to the convention that the embedding size is the number of heads\nmultiplied by the attention head dimension). The effect is a substantial reduction in parameter count,\nresulting in less parameters than even the canonical TrXL. Fig. 6 and Tab. 2 compare the different\nmodels to the “Thin” GTrXL (GRU), with Tab. 2 listing the parameter counts. The Thin GTrXL\n(GRU) matches every other model (within variance) except the GTrXL (GRU), even matching the\nnext best-performing model, the GTrXL (Output), with over 10 million less parameters.\n4.3.4 G ATED IDENTITY INITIALIZATION ABLATION\nAll applicable gating variants in the previous sections were trained with the gated identity initializa-\ntion (initial experiments found values b(l)\ng = 2 for GRU-type gating and b(l)\ng = 1 for other gating\n9\nPreprint\ntypes to work well). We observed in initial Memory Maze results that the gated identity initialization\nsigniﬁcantly improved optimization stability and learning speed. Figure 7 compares an otherwise\nidentical 4-layer GTrXL (GRU) trained with ( b(l)\ng = 2 ) and without ( b(l)\ng = 0 ) the gated identity\ninitialization, with 10 hyperparameter samples per initial bias setting. Similarly to the previous sen-\nsitivity plots, we plot the ranked mean return of all 10 runs at various times during training. As\ncan be seen from Fig. 7, there is a signiﬁcant gap caused by the bias initialization, suggesting that\npreconditioning the transformer to be close to Markovian results in large learning speed gains.\n5 R ELATED WORK\nGating has been shown to be effective to address the vanishing gradient problem and thus improve\nthe learnability of recurrent models. LSTM networks (Hochreiter & Schmidhuber, 1997; Graves,\n2013) rely on an input, forget and output gate that protect the update of the cell. GRU (Chung et al.,\n2014; Cho et al., 2014) is another popular gated recurrent architecture that simpliﬁes the LSTM\ncell, reducing the number of gates to two. Finding an optimal gating mechanism remains an active\narea of research, with other existing proposals (Krause et al., 2016; Kalchbrenner et al., 2015; Wu\net al., 2016), as well as works trying to discover optimal gating by neural architecture search (Zoph\n& Le, 2017) More generally, gating and multiplicative interactions have a long history (Rumelhart\net al., 1986). Gating has been investigated previously for improving the representational power of\nfeedforward and recurrent models (Van den Oord et al., 2016; Dauphin et al., 2017), as well as\nlearnability (Srivastava et al., 2015; Zilly et al., 2017). Initialization has also played a crucial role in\nmaking deep models trainable (LeCun et al., 1998; Glorot & Bengio, 2010; Sutskever et al., 2013).\nThere has been a wide variety of work looking at improving memory in reinforcement learning\nagents. External memory approaches typically have a regular feedforward or recurrent policy in-\nteract with a memory database through read and write operations. Priors are induced through the\ndesign of the speciﬁc read/write operations, such as those resembling a digital computer (Wayne\net al., 2018; Graves et al., 2016) or an environment map (Parisotto & Salakhutdinov, 2018; Gupta\net al., 2017). An alternative non-parametric perspective to memory stores an entire replay buffer\nof the agent’s past observations, which is made available for the agent to itself reason over either\nthrough ﬁxed rules (Blundell et al., 2016) or an attention operation (Pritzel et al., 2017). Others have\nlooked at improving performance of LSTM agents by extending the architecture with stacked hier-\narchical connections / multiple temporal scales and auxiliary losses (Jaderberg et al., 2019; Stooke\net al., 2019) or allowing an inner-loop update to the RNN weights (Miconi et al., 2018). Other\nwork has examined self-attention in the context of exploiting relational structure within the input-\nspace (Zambaldi et al., 2019) or within recurrent memories (Santoro et al., 2018).\n6 C ONCLUSION\nIn this paper we provided evidence that conﬁrms previous observations in the literature that standard\ntransformer models, despite the recent successes in supervised learning (Devlin et al., 2019; Dai\net al., 2019; Yang et al., 2019; Radford et al., 2019), are too unstable to train in the RL setting\nand often fail to learn completely (Mishra et al., 2018). We presented a new architectural variant\nof the transformer model, the GTrXL, which has increased performance, more stable optimization,\nand greater robustness to initial seed and hyperparameters than the canonical architecture. The key\ncontributions of the GTrXL are reordered layer normalization modules, enabling an initially Markov\nregime of training, and a gating layer instead of the standard residual connections. We performed\nextensive ablation experiments testing the robustness, ease of optimization and ﬁnal performance\nof the gating layer variations, as well as the effect of the reordered layer normalization. These\nresults empirically demonstrate that the GRU-type gating performs best across all metrics, exhibiting\ncomparable robustness to hyperparameters and random seeds as an LSTM while still maintaining a\nperformance improvement. Furthermore, the GTrXL (GRU) learns faster, more stably and achieves\na higher ﬁnal performance (even when controlled for parameters) than the other gating variants on\nthe challenging multitask DMLab-30 benchmark suite.\nHaving demonstrated substantial and consistent improvement in DMLab-30, Numpad and Memory\nMaze over the ubiquitous LSTM architectures currently in use, the GTrXL makes the case for wider\nadoption of transformers in RL. A core beneﬁt of the transformer architecture is its ability to scale\nto very large and deep models, and to effectively utilize this additional capacity in larger datasets. In\nfuture work, we hope to test the limits of the GTrXL’s ability to scale in the RL setting by providing\nit with a large and varied set of training environments.\n10\nPreprint\nACKNOWLEDGMENTS\nWe thank Alexander Pritzel, Chloe Hillier, Vicky Langston and many others at DeepMind for dis-\ncussions, feedback and support during the preparation of the manuscript.\nREFERENCES\nAbbas Abdolmaleki, Jost Tobias Springenberg, Jonas Degrave, Steven Bohez, Yuval Tassa, Dan\nBelov, Nicolas Heess, and Martin Riedmiller. Relative Entropy Regularized Policy Iteration.\narXiv preprint, 2018a. URL https://arxiv.org/pdf/1812.02256.pdf.\nAbbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Mar-\ntin Riedmiller. Maximum a Posteriori Policy Optimisation. Int. Conf. Learn. Represent., 2018b.\nURL https://arxiv.org/pdf/1806.06920.pdf.\nAnonymous Authors. V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete\nand Continuous Control. Under review, Int. Conf. Learn. Represent., 2019.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.Int.\nConf. Learn. Represent., 2019.\nCharles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K¨uttler,\nAndrew Lefrancq, Simon Green, V´ıctor Vald´es, Amir Sadik, et al. Deepmind lab. arXiv preprint\narXiv:1612.03801, 2016.\nCharles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z Leibo,\nJack Rae, Daan Wierstra, and Demis Hassabis. Model-free episodic control. arXiv preprint\narXiv:1606.04460, 2016.\nPeter Buchlovsky, David Budden, Dominik Grewe, Chris Jones, John Aslanides, Frederic Besse,\nAndy Brock, Aidan Clark, Sergio Gomez Colmenarejo, Aedan Pope, Fabio Viola, and Dan\nBelov. TF-Replicator: Distributed Machine Learning for Researchers. arXiv preprint, 2019.\nURL http://arxiv.org/abs/1902.00465.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder\nfor statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of\ngated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive language models beyond a ﬁxed-length context. InProceedings of the\n57th Annual Meeting of the Association for Computational Linguistics, pp. 2978–2988, Florence,\nItaly, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL\nhttps://www.aclweb.org/anthology/P19-1285.\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with\ngated convolutional networks. In Proceedings of the 34th International Conference on Machine\nLearning-Volume 70, pp. 933–941. JMLR. org, 2017.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal\ntransformers. Int. Conf. Learn. Represent., 2018.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019.\n11\nPreprint\nSergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at\nscale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Pro-\ncessing, pp. 489–500, 2018.\nLasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, V olodymyr Mnih, Tom Ward, Yotam\nDoron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. Impala:\nScalable distributed deep-rl with importance weighted actor-learner architectures. InInternational\nConference on Machine Learning, pp. 1406–1415, 2018.\nXavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neu-\nral networks. In In Proceedings of the International Conference on Artiﬁcial Intelligence and\nStatistics (AISTATS10). Society for Artiﬁcial Intelligence and Statistics, 2010.\nGoogle. Cloud TPU, 2018. URL https://cloud.google.com/tpu/.\nAlex Graves. Generating sequences with recurrent neural networks, 2013.\nAlex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-\nBarwi´nska, Sergio G ´omez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,\net al. Hybrid computing using a neural network with dynamic external memory. Nature, 538\n(7626):471, 2016.\nSaurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik. Cognitive\nmapping and planning for visual navigation. InProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp. 2616–2625, 2017.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\n770–778, 2016a.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\nnetworks. In European conference on computer vision, pp. 630–645. Springer, 2016b.\nMatteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van\nHasselt. Multi-task Deep Reinforcement Learning with PopArt. arXiv preprint, 2018. URL\nhttps://arxiv.org/pdf/1809.04474.pdf.\nSepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\nJan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro A. Ortega, Yee Whye Teh, and\nNicolas Heess. Meta reinforcement learning as task inference. arXiv preprint, 2019. URL\nhttp://arxiv.org/abs/1905.06424.\nMax Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia\nCastaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al. Human-\nlevel performance in 3d multiplayer games with population-based reinforcement learning. Sci-\nence, 364(6443):859–865, 2019.\nNal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid long short-term memory. 07 2015.\nSteven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent expe-\nrience replay in distributed reinforcement learning. Int. Conf. Learn. Represent., 2019.\nBen Krause, Liang Lu, Iain Murray, and Steve Renals. Multiplicative lstm for sequence modelling,\n2016.\nYann LeCun, L ´eon Bottou, Genevieve B. Orr, and Klaus-Robert M ¨uller. Efﬁcient backprop. In\nNeural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop ,\npp. 9–50, London, UK, UK, 1998. Springer-Verlag. ISBN 3-540-65311-2. URL http://dl.\nacm.org/citation.cfm?id=645754.668382.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. Generating wikipedia by summarizing long sequences. Int. Conf. Learn. Represent. ,\n2018.\n12\nPreprint\nYang Liu and Mirella Lapata. Text summarization with pretrained encoders. Conference on Empir-\nical Methods in Natural Language Processing.\nThomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: training plastic neural\nnetworks with backpropagation. In International Conference on Machine Learning , pp. 3556–\n3565, 2018.\nNikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A Simple Neural Attentive Meta-\nLearner. Int. Conf. Learn. Represent., 2018. URL https://openreview.net/pdf?id=\nB1DmUzWAW.\nV olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim\nHarley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement\nlearning. In International conference on machine learning, pp. 1928–1937, 2016.\nMichael Neunert, Abbas Abdolmaleki, Markus Wulfmeier, Thomas Lampe, Nicolas Heess, Roland\nHafner, and Martin Riedmiller. Continuous-Discrete Deep Reinforcement Learning for Hybrid\nControl in Robotics. To appear in Conf. on Robot Learn. (CoRL), 2019.\nEmilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured memory for deep reinforcement\nlearning. Int. Conf. Learn. Represent., 2018.\nAlexander Pritzel, Benigno Uria, Sriram Srinivasan, Adri `a Puigdom `enech Badia, Oriol Vinyals,\nDemis Hassabis, Daan Wierstra, and Charles Blundell. Neural episodic control. In International\nConference on Machine Learning, pp. 2827–2836, 2017.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\nD. E. Rumelhart, G. E. Hinton, and J. L. McClelland. Parallel distributed processing: Explorations\nin the microstructure of cognition, vol. 1. chapter 2: A General Framework for Parallel Distributed\nProcessing, pp. 45–76. MIT Press, Cambridge, MA, USA, 1986. ISBN 0-262-68053-X. URL\nhttp://dl.acm.org/citation.cfm?id=104279.104286.\nAdam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber,\nDaan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy Lillicrap. Relational recurrent neural\nnetworks. In Advances in Neural Information Processing Systems, pp. 7299–7310, 2018.\nRupesh Kumar Srivastava, Klaus Greff, and J¨urgen Schmidhuber. Highway networks.arXiv preprint\narXiv:1505.00387, 2015.\nAdam Stooke, Valentin Dalibard, Siddhant M Jayakumar, Wojciech M Czarnecki, and Max Jader-\nberg. Perception-prediction-reaction agents for deep reinforcement learning. SPIRL Workshop,\n2019.\nIlya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-\nization and momentum in deep learning. In Sanjoy Dasgupta and David McAllester (eds.), Pro-\nceedings of the 30th International Conference on Machine Learning , volume 28 of Proceedings\nof Machine Learning Research, pp. 1139–1147, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR.\nURL http://proceedings.mlr.press/v28/sutskever13.html.\nAaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, and Koray\nKavukcuoglu. Conditional image generation with pixelcnn decoders. In Advances in neural\ninformation processing systems, pp. 4790–4798, 2016.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Informa-\ntion Processing Systems, pp. 5998–6008, 2017. URL https://papers.nips.cc/paper/\n7181-attention-is-all-you-need.pdf .\n13\nPreprint\nOriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojtek\nCzarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, Timo Ewalds,\nDan Horgan, Manuel Kroiss, Ivo Danihelka, John Agapiou, Junhyuk Oh, Valentin Dal-\nibard, David Choi, Laurent Sifre, Yury Sulsky, Sasha Vezhnevets, James Molloy, Trevor\nCai, David Budden, Tom Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Toby Pohlen,\nDani Yogatama, Julia Cohen, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lil-\nlicrap, Chris Apps, Koray Kavukcuoglu, Demis Hassabis, and David Silver. AlphaStar:\nMastering the Real-Time Strategy Game StarCraft II. https://deepmind.com/blog/\nalphastar-mastering-real-time-strategy-game-starcraft-ii/ , 2019.\nGreg Wayne, Chia-Chun Hung, David Amos, Mehdi Mirza, Arun Ahuja, Agnieszka Grabska-\nBarwinska, Jack Rae, Piotr Mirowski, Joel Z. Leibo, Adam Santoro, Mevlana Gemici, Malcolm\nReynolds, Tim Harley, Josh Abramson, Shakir Mohamed, Danilo Rezende, David Saxton, Adam\nCain, Chloe Hillier, David Silver, Koray Kavukcuoglu, Matt Botvinick, Demis Hassabis, and\nTimothy Lillicrap. Unsupervised predictive memory in a goal-directed agent. arXiv preprint\narXiv:1803.10760, 2018.\nYuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Ruslan R Salakhutdinov. On multi-\nplicative integration with recurrent neural networks. InAdvances in neural information processing\nsystems, pp. 2856–2864, 2016.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\nXlnet: Generalized autoregressive pretraining for language understanding. Advances in Neural\nInformation Processing Systems, 2019.\nVinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin,\nKarl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, Murray Shanahan, Victoria\nLangston, Razvan Pascanu, Matthew Botvinick, Oriol Vinyals, and Peter Battaglia. Deep re-\ninforcement learning with relational inductive biases. In International Conference on Learning\nRepresentations, 2019. URL https://openreview.net/forum?id=HkxaFoC9KQ.\nJulian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutn ´ık, and J¨urgen Schmidhuber. Recurrent\nhighway networks. In Proceedings of the 34th International Conference on Machine Learning-\nVolume 70, pp. 4189–4198. JMLR. org, 2017.\nBarret Zoph and Quoc V . Le. Neural architecture search with reinforcement learning. In ICLR,\n2017. URL https://arxiv.org/abs/1611.01578.\n14\nPreprint\nFigure 8: Left: The Numpad environment, showing the controllable “sphere” robot and a full 3x3\npad. Pads are activated when the robot collides with their center. The robot can move on the plane\nas well as jump to avoid pressing numbers. Right: Top down view of “Memory Maze”: (1) Central\nchamber, (2) blocks among which the apple is placed, (3) landmarks the agent can use to locate the\napple, (4) one of the possible location of the apple.\nAPPENDIX\nA E NVIRONMENT DETAILS\nNumpad: Numpad has three actions, two of which move the sphere towards some direction in the\nx,y plane and the third allows the agent to jump in order to get over a pad faster. The observation\nconsists of a variety of proprioceptive information (e.g. position, velocity, acceleration) as well as\nwhich pads in the sequence have been correctly activated (these will shut off if an incorrect pad is\nlater hit), and the previous action and reward. Episodes last a ﬁxed 500 steps and the agent can\nrepeat the correct sequence any number of times to receive reward. Observations were processed\nusing a simple 2-layer MLP with tanh activations to produce the transformer’s input embedding.\nDMLab-30: Ignoring the “jump” and “crouch” actions which we do not use, an action in the native\nDMLab action space consists of 5 integers whose meaning and allowed values are given in Table 4.\nFollowing previous work on DMLab (Hessel et al., 2018), we used the reduced action set given in\nTable 5 with an action repeat of 4. Observations are 72 ×96 RGB images. Some levels require\na language input, and for that all models use an additional 64-dimension LSTM to process the\nsentence.\nIn Wayne et al. (2018), the DMLab Arbitrary Visuomotor Mapping task was speciﬁcally used to\nhighlight the MERLIN architecture’s ability to utilize memory. In Figure 9 we show that, given a\nsimilarly reduced action set as used in Wayne et al. (2018), see Table 6, the GTrXL architecture can\nalso reliably attain human-level performance on this task.\nACTION NAME RANGE\nLOOK LEFT RIGHT PIXELS PER FRAME [-512, 512]\nLOOK DOWN UP PIXELS PER FRAME [-512, 512]\nSTRAFE LEFT RIGHT [-1, 1]\nMOVE BACK FORW ARD [-1, 1]\nFIRE [0, 1]\nTable 4: Native action space for DMLab. See https://github.com/deepmind/lab/\nblob/master/docs/users/actions.md for more details.\nMemory Maze:An action in the native Memory Maze action space consists of 8 continuous actions\nand a single discrete action whose meaning and allowed values are given in Table 7. Unlike for\nDMLab, we used a hybrid continuous-discrete distribution (Neunert et al., 2019) to directly output\npolicies in the game’s native action space. Observations are72 ×96 RGB images.\n15\nPreprint\nFigure 9: Learning curves for the DMLab Arbitrary Visuomotor Mapping task using a reduced\naction set.\nACTION NATIVE DML AB ACTION\nForward (FW) [ 0, 0, 0, 1, 0]\nBackward (BW) [ 0, 0, 0, -1, 0]\nStrafe left [ 0, 0, -1, 0, 0]\nStrafe right [ 0, 0, 1, 0, 0]\nSmall look left (LL) [-10, 0, 0, 0, 0]\nSmall look right (LR) [ 10, 0, 0, 0, 0]\nLarge look left (LL ) [-60, 0, 0, 0, 0]\nLarge look right (LR) [ 60, 0, 0, 0, 0]\nLook down [ 0, 10, 0, 0, 0]\nLook up [ 0, -10, 0, 0, 0]\nFW + small LL [-10, 0, 0, 1, 0]\nFW + small LR [ 10, 0, 0, 1, 0]\nFW + large LL [-60, 0, 0, 1, 0]\nFW + large LR [ 60, 0, 0, 1, 0]\nFire [ 0, 0, 0, 0, 1]\nTable 5: Simpliﬁed action set for DMLab from Hessel et al. (2018).\nACTION NATIVE DML AB ACTION\nSmall look left (LL) [-10, 0, 0, 0, 0]\nSmall look right (LR) [ 10, 0, 0, 0, 0]\nLook down [ 0, 10, 0, 0, 0]\nLook up [ 0, -10, 0, 0, 0]\nNo-op [ 0, 0, 0, 0, 0]\nTable 6: Simpliﬁed action set for DMLab Arbitrary Visuomotor Mapping (A VM). This action set is\nthe same as the one used for A VM in Wayne et al. (2018) but with an additional no-op, which may\nalso be replaced with the Fire action.\nImage Encoder:For DMLab-30 and Memory Maze, we used the same image encoder as in (Anony-\nmous Authors, 2019) for multitask DMLab-30. The ResNet was adapted from Hessel et al. (2018)\nand each of its layer blocks consists of a (3 ×3, stride 1) convolution, followed by (3 ×3, stride 2)\nmax-pooling, followed by 2 3 ×3 residual blocks with ReLU non-linearities.\n16\nPreprint\nACTION NAME RANGE\nLOOK LEFT RIGHT [-1.0, 1.0]\nLOOK DOWN UP [-1.0, 1.0]\nSTRAFE LEFT RIGHT [-1.0, 1.0]\nMOVE BACK FORW ARD [-1.0, 1.0]\nHAND ROTATEAROUND RIGHT [-1.0, 1.0]\nHAND ROTATEAROUND UP [-1.0, 1.0]\nHAND ROTATEAROUND FORW ARD [-1.0, 1.0]\nHAND PUSH PULL [-10.0, 10.0]\nHAND GRIP {0, 1}\nTable 7: Hybrid action set for Memory Maze, consisting of 8 continuous actions and a single discrete\naction.\nAgent Output:As in (Anonymous Authors, 2019), in all cases we use a 256-unit MLP with a linear\noutput to get the policy logits (for discrete actions), Gaussian distribution parameters (for continuous\nactions) or value function estimates.\nB E XPERIMENTAL DETAILS\nFor all experiments, beyond sampling independent random seeds, each run also has V-MPO hyper-\nparameters sampled from a distribution (see Table 8). The sampled hyperparameters are kept ﬁxed\nacross all models for a speciﬁc experiment, meaning that if one of the ϵα sampled is 0.002, then all\nmodels will have 1 run with ϵα = 0.002 and so on for the rest of the samples. The exception is for\nthe DMLab-30 LSTM, where a more constrained range was found to perform better in preliminary\nexperiments. Each model had 8 seeds started, but not all runs ran to completion due to compute\nissues. These hyperparameter settings were dropped randomly and not due to poor environment\nperformance. We report how many seeds ran to completion for all models. At least 6 seeds ﬁnished\nfor every model tested. We list architecture details by section below.\nHyperparameter Environment\nDMLab-30 Numpad Memory Maze\nBatch Size 128 128 128\nUnroll Length 95 95 95\nDiscount 0.99 0.99 0.99\nAction Repeat 4 1 4\nPixel Control Cost 2 ×10−3 - -\nTarget Update Period 10 10 10\nInitial η 1.0 10.0 1.0\nInitial α 5.0 - 5.0\nInitial αµ - 1.0 1.0\nInitial αΣ - 1.0 1.0\nϵη 0.1 0.1 0.1\nϵα (log-uniform) LSTM [0.001, 0.025)\nTrXL Variants [0.001, 0.1) - [0.001, 0.1)\nϵαµ (log-uniform) - [0.005, 0.01) [0.005,0.01)\nϵαΣ (log-uniform) - [ 5 ×10−6, 4 ×10−4) [ 5 ×10−6, 4 ×10−5)\nTable 8: V-MPO hyperparameters per environment.\nB.1 T RAINING SETUP\nAll experiments in this work were carried out in an actor-learner framework (Espeholt et al., 2018)\nthat utilizes TF-Replicator (Buchlovsky et al., 2019) for distributed training on TPUs in the 16-core\nconﬁguration (Google, 2018). “Actors” running on CPUs performed network inference and interac-\ntions with the environment, and transmitted the resulting trajectories to the centralised “learner“.\n17\nPreprint\nModel # Layers Head Dim. # Heads Hidden\nDim. Memory Size Runs\nCompleted\nLSTM 3 - - 256 - 8\nTrXL 12 64 8 256 512 6\nTrXL-I 12 64 8 256 512 6\nGTrXL (GRU) 12 64 8 256 512 8\nGTrXL (Input) 12 64 8 256 512 6\nGTrXL (Output) 12 64 8 256 512 7\nGTrXL (Highway) 12 64 8 256 512 7\nGTrXL (SigTanh) 12 64 8 256 512 6\nThin GTrXL (GRU) 12 64 4 128 512 8\nTable 9: DMLab-30 Ablation Architecture Details. We report the number of runs per model that ran\nto completion (i.e. 10 billion environment steps). We follow the standard convention that the hid-\nden/embedding dimension of transformers is equal to the head dimension multiplied by the number\nof heads. (Sec. 4.1 & Sec. 4.3).\nModel # Layers Head Dim. # Heads Hidden\nDim. Memory Size Runs\nCompleted\nLSTM 3 - - 256 - 5\nGTrXL (GRU) 12 64 8 256 512 5\nTable 10: Numpad Architecture Details. (Sec. 4.2).\nModel # Layers Head Dim. # Heads Hidden\nDim. Memory Size\nLSTM 3 - - 256 -\nTrXL 12 64 8 256 512\nTrXL-I 12 64 8 256 512\nGTrXL (GRU) 12 64 8 256 512\nGTrXL (Output) 12 64 8 256 512\nTable 11: Sensitivity ablation architecture details (Sec. 4.3.2).\nModel # Layers Head Dim. # Heads Hidden\nDim. Memory Size Runs\nCompleted\nGTrXL (GRU) 4 64 4 256 512 8\nTable 12: Gated identity initialization ablation architecture details (Sec. 4.3.4).\nC M ULTI -HEAD ATTENTION DETAILS\nC.1 M ULTI -HEAD ATTENTION\nThe Multi-Head Attention (MHA) submodule computes in parallel H soft-attention operations for\nevery time step, producing an output tensor Y(l) ∈ RT×D. MHA operates by ﬁrst calculating\nthe query Q(l) ∈RH×T×d, keys K(l) ∈RH×T×d, and values V(l) ∈RH×T×d (where d= D/H)\nthrough trainable linear projectionsW(l)\nQ , W(l)\nK , and W(l)\nV , respectively, and then using the combined\nQ, K, V, tensors to compute the soft attention. A residual connection (He et al., 2016a) to the\nresulting embedding E(l) is then applied and ﬁnally layer normalization (Ba et al., 2016).\n18\nPreprint\nFigure 10: The 25 hyperparameter settings sampled for the sensitivity ablation (Sec. 4.3.2). X-axis\nis in log scale and values are sampled from the corresponding ranges given in Table 8.\nMultiHeadAttention(E(l−1)):\nQ(l),K(l),V (l) = W(l)\nQ E(l−1),W(l)\nK E(l−1),W(l)\nV E(l−1) (9)\nα(l)\nhtm = QhtdKhmd (10)\nW(l)\nhtm = MaskedSoftmax(α(l),axis=m) (11)\nY\n(l)\nhtd = W(l)\nhtmV(l)\nhmd (12)\nˆY(l) = E(l−1) + Linear(Y\n(l)\n) (13)\nY(l) = LayerNorm( ˆY(l)) (14)\nwhere we used Einstein summation notation to denote the tensor multiplications, MaskedSoftmax is\na causally-masked softmax to prevent addressing future information, Linear is a linear layer applied\nper time-step and we omit reshaping operations for simplicity.\nC.2 R ELATIVE MULTI -HEAD ATTENTION\nThe basic MHA operation does not take sequence order into account explicitly because it is permu-\ntation invariant, so positional encodings are a widely used solution in domains like language where\norder is an important semantic cue, appearing in the original transformer architecture (Vaswani et al.,\n2017). To enable a much larger contextual horizon than would otherwise be possible, we use the\nrelative position encodings and memory scheme described in Dai et al. (2019). In this setting, there\nis an additional T-step memory tensor M(l) ∈RT×D, which is treated as constant during weight\nupdates.\nRelativeMultiHeadAttention(M(l−1),E(l−1)):\n˜E(l−1) = [M(l−1),E(l−1)] (15)\nQ(l),K(l),V (l) = W(l)\nQ E(l−1),W(l)\nK ˜E(l−1),W(l)\nV ˜E(l−1) (16)\nR= W(l)\nR Φ (17)\nα(l)\nhtm = QhtdKhmd + QhtdRhmd + uh∗dKhtm + vh∗dRhmd (18)\nW(l)\nhtm = MaskedSoftmax(α(l),axis=m) (19)\nY\n(l)\nhtd = W(l)\nhtmV(l)\nhmd (20)\nˆY(l) = E(l−1) + Linear(Y\n(l)\n) (21)\nY(l) = LayerNorm( ˆY(l)) (22)\n19\nPreprint\nwhere Φ is the standard sinusoid encoding matrix, u(l),v(l) ∈RH×d are trainable parameters, the\n∗represents the broadcast operation, and WR is a linear projection used to produce the relative\nlocation-based keys (see Dai et al. (2019) for a detailed derivation).\nC.3 I DENTITY MAP REORDERING\nThe Identity Map Reordering modiﬁes the standard transformer formulation as follows: the layer\nnorm operations are applied only to the input of the sub-module and a non-linear ReLU activation\nis applied to the output stream.\nY\n(l)\n= RelativeMultiHeadAttention(LayerNorm([StopGrad(M(l−1)),E(l−1)])) (23)\nY(l) = E(l−1) + ReLU(Y\n(l)\n) (24)\nE\n(l)\n= f(l)(LayerNorm(Y(l))) (25)\nE(l) = Y(l) + ReLU(E\n(l)\n) (26)\nSee Figure 1 (Center) for a visual depiction of the TrXL-I.\nD DML AB-30 M EMORY /REACTIVE PARTITION\nMemory Reactive\nrooms select nonmatching object\nrooms watermaze\nexplore obstructed goals small\nexplore goal locations small\nexplore object rewards few\nexplore obstructed goals large\nexplore goal locations large\nexplore object rewards many\nrooms collect good objects train\nrooms exploit deferred effects train\nrooms keys doors puzzle\nlanguage select described object\nlanguage select located object\nlanguage execute random task\nlanguage answer quantitative question\nlasertag one opponent large\nlasertag three opponents large\nlasertag one opponent small\nlasertag three opponents small\nnatlab ﬁxed large map\nnatlab varying map regrowth\nnatlab varying map randomized\nskymaze irreversible path hard\nskymaze irreversible path varied\npsychlab arbitrary visuomotor mapping\npsychlab continuous recognition\npsychlab sequential comparison\npsychlab visual search\nexplore object locations small\nexplore object locations large\nTable 13: Partition of DMLab-30 levels into a memory-based and reactive split of levels.\n20"
}