{
  "title": "LaMI: Large Language Models for Multi-Modal Human-Robot Interaction",
  "url": "https://openalex.org/W4391417118",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2019445612",
      "name": "Wang Chao",
      "affiliations": [
        "Honda (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A4287969066",
      "name": "Hasler, Stephan",
      "affiliations": [
        "Honda (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2753877903",
      "name": "Tanneberg Daniel",
      "affiliations": [
        "Honda (Germany)"
      ]
    },
    {
      "id": null,
      "name": "Ocker, Felix",
      "affiliations": [
        "Honda (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2139052312",
      "name": "Joublin Frank",
      "affiliations": [
        "Honda (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A4376882881",
      "name": "Ceravola, Antonello",
      "affiliations": [
        "Honda (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A4306816195",
      "name": "Deigmoeller, Joerg",
      "affiliations": [
        "Honda (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A4223139762",
      "name": "Gienger, Michael",
      "affiliations": [
        "Honda (Germany)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2050760078",
    "https://openalex.org/W2191285365",
    "https://openalex.org/W1851213814",
    "https://openalex.org/W3009350896",
    "https://openalex.org/W4387995158",
    "https://openalex.org/W14011730",
    "https://openalex.org/W3135194167",
    "https://openalex.org/W4378746725",
    "https://openalex.org/W2153457831",
    "https://openalex.org/W1521597329",
    "https://openalex.org/W4383046944",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W1987882570",
    "https://openalex.org/W4379054524",
    "https://openalex.org/W4389665836"
  ],
  "abstract": "This paper presents an innovative large language model (LLM)-based robotic system for enhancing multi-modal human-robot interaction (HRI). Traditional HRI systems relied on complex designs for intent estimation, reasoning, and behavior generation, which were resource-intensive. In contrast, our system empowers researchers and practitioners to regulate robot behavior through three key aspects: providing high-level linguistic guidance, creating \"atomic actions\" and expressions the robot can use, and offering a set of examples. Implemented on a physical robot, it demonstrates proficiency in adapting to multi-modal inputs and determining the appropriate manner of action to assist humans with its arms, following researchers' defined guidelines. Simultaneously, it coordinates the robot's lid, neck, and ear movements with speech output to produce dynamic, multi-modal expressions. This showcases the system's potential to revolutionize HRI by shifting from conventional, manual state-and-flow design methods to an intuitive, guidance-based, and example-driven approach. Supplementary material can be found at https://hri-eu.github.io/Lami/",
  "full_text": "LaMI: Large Language Models for Multi-Modal Human-Robot\nInteraction\nChao Wang\nchao.wang@honda-ri.de\nHonda Research Institute EU\nOffenbach am Main, Germany\nStephan Hasler\nHonda Research Institute EU\nOffenbach am Main, Germany\nDaniel Tanneberg\nHonda Research Institute EU\nOffenbach am Main, Germany\nFelix Ocker\nHonda Research Institute EU\nOffenbach am Main, Germany\nFrank Joublin\nHonda Research Institute EU\nOffenbach am Main, Germany\nAntonello Ceravola\nHonda Research Institute EU\nOffenbach am Main, Germany\nJoerg Deigmoeller\nHonda Research Institute EU\nOffenbach am Main, Germany\nMichael Gienger\nHonda Research Institute EU\nOffenbach am Main, Germany\nFigure 1: LLM driven human-robot interaction centered around guidance, capabilities, and examples\nABSTRACT\nThis paper presents an innovative large language model (LLM)-\nbased robotic system for enhancing multi-modal human-robot in-\nteraction (HRI). Traditional HRI systems relied on complex de-\nsigns for intent estimation, reasoning, and behavior generation,\nwhich were resource-intensive. In contrast, our system empowers\nresearchers and practitioners to regulate robot behavior through\nthree key aspects: providing high-level linguistic guidance, creating\n\"atomic actions\" and expressions the robot can use, and offering a\nset of examples. Implemented on a physical robot, it demonstrates\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nCHI EA ’24, May 11–16, 2024, Honolulu, HI, USA\n© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0331-7/24/05\nhttps://doi.org/10.1145/3613905.3651029\nproficiency in adapting to multi-modal inputs and determining the\nappropriate manner of action to assist humans with its arms, follow-\ning researchers’ defined guidelines. Simultaneously, it coordinates\nthe robot’s lid, neck, and ear movements with speech output to\nproduce dynamic, multi-modal expressions. This showcases the sys-\ntem’s potential to revolutionize HRI by shifting from conventional,\nmanual state-and-flow design methods to an intuitive, guidance-\nbased, and example-driven approach. Supplementary material can\nbe found at https://hri-eu.github.io/Lami/\nCCS CONCEPTS\n• Human-centered computing → Interactive systems and\ntools.\nKEYWORDS\nAssisting robot, Human-robot interaction, Large language model\narXiv:2401.15174v4  [cs.RO]  11 Apr 2024\nCHI EA ’24, May 11–16, 2024, Honolulu, HI, USA Wang and Gienger et al.\nACM Reference Format:\nChao Wang, Stephan Hasler, Daniel Tanneberg, Felix Ocker, Frank Joublin,\nAntonello Ceravola, Joerg Deigmoeller, and Michael Gienger. 2024. LaMI:\nLarge Language Models for Multi-Modal Human-Robot Interaction . In\nExtended Abstracts of the CHI Conference on Human Factors in Computing\nSystems (CHI EA ’24), May 11–16, 2024, Honolulu, HI, USA. ACM, New York,\nNY, USA, 10 pages. https://doi.org/10.1145/3613905.3651029\n1 INTRODUCTION\nSeamless human-robot interaction (HRI) necessitates the robot’s\nadept handling of multi-modal input from humans, including speech,\ngaze, and facial expressions, to accurately assess the human’s in-\ntent and provide assistance accordingly. Simultaneously, robots\nmust convey their own intent clearly to humans through multi-\nmodal output channels, encompassing speech, gesture, and gaze.\nTraditionally, achieving this functionality in robotic systems often\nrequired intricate design. In the realm of intent estimation, previous\nresearch commonly incorporated intention recognition modules to\ncategorize human intent based on multi-modal input [3, 17]. Some\nsystems additionally featured dedicated modules for detecting the\nhuman affective state, crucial for establishing a socially nuanced\ninteraction [10, 16, 18]. However, the drawback of these approaches\nlies in their time-consuming and expensive training processes. For\nthe output front, numerous prior systems integrated emotional\nstate [8, 11] modules to control humanoid output cues, such as\ntone, gaze, or facial expressions, enhancing the transparency and\nvividness of feedback to humans. Regarding the motion generation,\na variety of methods were proposed, including blending and map-\nping of pre-established motion sets [19, 25], and the use of motion\ncapture data [5, 9, 15]. Notably, this involved the manual design of\nmotions for each output modality correlated to specific states.\nRecent advancements in large language models (LLMs), show-\ncasing impressive capabilities in domains like chatbots, data pro-\ncessing, and code generation, are now revealing their potential\napplications in the field of robotics. By leveraging the ability of con-\ntext understanding, reasoning and planning, in a short time, many\nrobotic application were proposed [ 7, 12, 14, 20, 21, 28]. Among\nthem, one typically example is the \"SayCan\" robot [1], which is able\nto interpret human’s naturally language command, analyzing the\nenvironment and generate concrete executable actions sequence to\nsatisfy human’s requirement by using LLMs. However, interaction\ncues between robot and human are limited to voice command and\neven without speech output.\nMore recently, some researchers also tried to apply this tech-\nnology in the realm of the HRI. For example, Zhang et al. utilized\nLLMs to estimate how much humans trust a robot [ 30]; Yoshida\net al., use LLMs to generate low-level control command to drive a\nhumanoid robot motion for social expression [29], rather than for\npractical assistance. Baermann et al., deployed LLMs not only to\nfollow human’s speech commands, but also corrects its mistakes\nvia human’s natural language feedback [2]. However, the commu-\nnication primarily relies on speech interaction, with less focus on\nmulti-modal sensing and expression capabilities. Ye et al. [27] de-\nveloped an LLM-driven robotic system capable of collaborating\nwith humans in assembly tasks within a VR environment. But this\nsystem is limited to processing human language inputs and control-\nling a single arm in virtual space. In general, compared to the rapid\nFigure 2: Robot’s Hardware and the Scenario Setup\nadvances of LLMs in the robotic task and motion planning domains,\nthe attempts in HRI are not so numerous and often lack a system-\natic approach to maximize the capability of LLMs for multi-modal\ninteraction with human.\nThis study proposes a novel LLM-based robotic system imple-\nmented in a physical robot. This architecture empowers researchers\nand practitioners to regulate robot behavior through three key as-\npects: providing high-level guidance in natural language, creating\n\"atomic\" of actions and expressions which robot can use, and a set\nof examples. In practice, our system can convert human’s multi-\nmodal input, including observed human behavior, position, gaze\nand multi-person dialogue, along with scene information such as\nobject identities and poses, to the language description that LLMs\ncan process. Subsequently, the LLM analyzes the situation and deter-\nmines the timing and manner of the robot’s support actions to assist\nhumans, following predefined guidelines. Simultaneously, it coordi-\nnates the robot’s lid, neck, and ears movements with speech output\nto produce dynamic, multi-modal expressions. Preliminary test re-\nsults demonstrate that the robot can effectively meet researcher\nexpectations, suggesting that this approach holds the potential to\ntransform human-robot interaction from a manual, state-and-flow\ndesign methodology to a more intuitive approach centered around\nguidance, capabilities, and example-driven frameworks.\n2 LLM DRIVEN HUMAN-ROBOT\nINTERACTION\nThe bi-manual robotic system setup is composed of two robot arms1,\nwhich can manipulate objects on the desk. A robot head with two\nears and a lid is installed on a pan-tilt unit as a neck, allowing\nthe robot to perform non-verbal expressions to humans (Figure 2).\nAdditionally, a speaker integrated into the robot’s body delivers\nverbal output. In addition, a 7-inch LCD attached on the robot body\ncommunicates the thinking process of the robot via graphic user\ninterface (GUI). A multi-channel microphone array2, installed on\nthe robot, enables it to receive speech commands and discern voice\nsources. A RGBD camera3 can track the skeletons of several humans\nand the 6-dof poses of the objects on the desk.\nThe system’s architecture includes three key modules: \"Scene\nNarrator\", \"Planner\", and \"Expresser\" (Figure 3). The Scene Simula-\ntor mirrors the states of objects and humans as detected by sensors.\n1https://www.kinovarobotics.com/product/gen2robots\n2https://wiki.seeedstudio.com/ReSpeaker_Mic_Array_v2.0\n3https://azure.microsoft.com/en-us/products/kinectdk\nLaMI: Large Language Models for Multi-Modal Human-Robot Interaction CHI EA ’24, May 11–16, 2024, Honolulu, HI, USA\nFigure 3: The system structure.\nThe Planner module processes multi-modal inputs as event mes-\nsages, encompassing the positions of individuals within the scene.\nInter-module communication is facilitated using ROS4.\n2.1 The \"Scene Narrator\"\nThe \"Scene Narrator\" module senses the poses of the objects, human\npostures, and the dialogue information, i.e. which person speaks\nwhich content to which other person. This module constructs a 3D\nrepresentation of the actual scene, enabling it to transform multi-\nmodal sensory data into events, following predefined rules. These\nevents are formatted in natural language, making them understand-\nable to the LLM. For instance, if a person turns towards another\nindividual while speaking, the module conveys this as an event to\nthe Planner module: ’<Bob> speaks to <Sam>: ’Give me the cola\nbottle’ ’. Similarly, if it detects a person’s hand near a container\nthat is being tilted towards another, it sends an event like ’<Sam>\nis pouring <the_bottle_one> into <the_glass_two>’. An additional\nfunction of the Narrator module involves receiving high-level ac-\ntion commands from the ’Planner’ module. For instance, it may\nbe instructed to place a glass within an area reachable by a spe-\ncific person, followed by executing low-level trajectory planning\nto control the robot’s arms. Further details about this process will\nbe elaborated in subsequent sections.\n2.2 The \"Planner\"\nThe primary role of the \"Planner\" module is to facilitate communi-\ncation with LLMs to enable high-level planning for both assistance\nand human interaction. To achieve these objectives, the \"Planner\" in-\ntegrates the GPT-4 tool API5 , which is designed to execute eight pre-\ndefined \"atomic\" functions. Seven of these functions facilitate com-\nmunication with the \"Narrator\" module. Specifically, functions like\nget_objects(), get_persons(), can_person_reach_object(),\nand can_person_see_object() are employed to query the status\nand properties of objects or individuals. Meanwhile, commands\n4https://www.ros.org/\n5https://platform.openai.com/docs/guides/functioncalling\nsuch as put_object_on_object() or move_object_to_person()\ninstruct the \"Scene Narrator\" to manipulate the robot’s arms for\nvarious tasks. Two distinct functions are dedicated to managing the\nrobot’s social communication. The \"speak()\" function is responsible\nfor initiating verbal communication, while the \"facial_expression()\"\nfunction controls movements of the ears, lid, and head, enhancing\nnon-verbal interaction. These functions are in synchronization with\nthe \"Expresser\" module. The GPT-4 Tool API is engineered to allow\nusers to define each function and its parameters. The LLM, in turn,\nselects the appropriate function(s) and arguments to execute the\ntask at hand. As detailed earlier, when a function is called, it trans-\nmits high-level action commands to the \"Scene Narrator\" to develop\nlow-level trajectory plans for controlling the robot’s arms. For in-\nstance, if the LLM issues a command like put(’the_bottle_two’,\n’the_table_one’), the Narrator module generates a trajectory en-\nabling the robot’s arm to place the bottle on a suitable spot on the\ntable, while avoiding collisions with other objects or people. If the\nlow-level plan is executable, the \"Narrator\" confirms success; other-\nwise, it provides feedback detailing the error, along with reasons\nand suggestions in a structured, rule-based natural language for-\nmat. For example, it might respond with: \"RESULT: ’Unable to place\nthe_bottle_two on the_table_one. ’ SUGGESTION: Hand the object\nto a person or find a different location to place it. \" This feedback is\nthen relayed back to the GPT-4 API for further high-level planning\nadjustments.\n2.3 The \"Expresser\"\nThe \"Expresser\" module is responsible for controlling the actuators\nresponsible for the robot’s facial expressions. It houses a library\nof pre-designed \"atomic animation clips\" for each actuator’s move-\nments. These animations can be activated directly through the\nfacial_expression() function in the \"Planner\" module, with pa-\nrameters like{gazed_object: ’the_bottle_two’, head_motion:\n’nodding’, ears_lid_motion: ’confirm’}. However, process-\ning each LLM query, can take approximately 1-5 seconds, varying\nwith the complexity of the plan. Sole reliance on this method for the\nrobot’s social expressions could result in idle periods, negatively\nCHI EA ’24, May 11–16, 2024, Honolulu, HI, USA Wang and Gienger et al.\nFigure 4: The GUI illustrates the robot’s \"internal thoughts\"\nby translating GPT-called functions and their outcomes into\nnatural language, accompanied by relevant icons. Addition-\nally, after each GPT query cycle, the LLM is prompted to\nsummarize the reasoning behind its actions.\nimpacting user experience. To mitigate this, the \"Expresser\" also\nincorporates a rule-based mechanism. This mechanism enables it\nto provide rapid expressions in the interim between the request and\nresponse of each GPT query, which is also inline with \"deliberative\"\nand \"reactive\" approaches in HRI domain [3]. For instance, when the\n\"Planner\" module receives either trigger information or a function\ncall result from the \"Scene Narrator\" – and before it issues a new\nrequest to GPT – it forwards this information to the \"Expresser\"\nto initiate rule-based expressions, such as \"thinking\" or gestures\nindicating \"function-call success\". The specifics of how expressions\nare distributed between rule-based and LLM-based methods are\nelaborated in Section 3.4. Additionally, the \"Expresser\" plays a cru-\ncial role in synchronizing the movements of different modules. A\nnoteworthy point of consideration is the potential conflict in com-\nmands, especially when \"gazed_object\" requires the robot’s neck\nto orient towards a specific object, which might clash with other\nhead gestures like \"nodding\" or \"shaking the head\". To address this,\ncommands related to \"gazed_object\" are always prioritized and sent\nfirst to the pan-tilt unit, followed by other head gestures. Finally,\nthe \"Expresser\" module translates the information communicated\nwith the LLM into natural language, displaying it through text and\nicons in the GUI (shown in Figure 4). This feature enables users\nto understand the robot’s \"internal thoughts\" in an anthropomor-\nphic way. After each GPT query round is completed, we guide the\nLLM to summarize the reasoning process, thereby enhancing the\nexplainability of the robot’s behavior.\n2.4 Interaction Flow: An Example\nThis section illustrates the interaction flow within our system\nthrough a scenario depicted in Figure 5:\nThe interaction typically begins with a person’s speech. For in-\nstance, \"Scene Narrator\" detects \"Felix speaks to Daniel: ’Please\nhand me the red glass’. \" This event is then translated into natural\nlanguage and relayed to the \"Planner\" module, initiating a GPT\nquery. Simultaneously, the \"Planner\" informs the \"Expresser\" for\nan immediate rule-based response, leading the robot to look at\nFelix while its ears and lid roll back, simulating a listening ges-\nture. Approximately 2 seconds later, GPT responds by invoking the\nget_persons() and get_objects() functions to identify people\nand objects present. The resulting data, including \"Felix\", \"Daniel\"\nand object details, are sent back to GPT for further analysis. During\nthe wait for GPT’s next response, the robot exhibits a ’thinking’\ngesture, looking from side to side with blinking lid movements.\nShortly after, the LLM calls check_hindering_reasons() to as-\nsess if Daniel can see and reach the red glass and whether he\nis busy. Concurrently,facial_expression() is activated for the\nrobot to look towards Daniel. The outcome indicates Daniel can\nhand over the glass, and the robot, following pre-defined guid-\nance, opts not to intervene, silently displaying the reasoning on\nthe GUI. Subsequently, Felix asks Daniel to pour cola into the\nglass. The robot, attentive to their conversation, deduces through\ncheck_hindering_reasons that Daniel is occupied with a phone\ncall and learns from is_person_busy_or_idle that Felix is hold-\ning the cup. The robot then opts to pour cola from the bottle into\nFelix’s glass. Should Felix not be holding the glass, or if it’s beyond\nthe robot’s reach, the robot will instead place the bottle near Felix.\nDirected by LLM, the robot’s head tracks the bottle during pickup\nand shifts to the glass while pouring. Upon completion, the robot\nnods towards Felix and announces, \"I’ve poured Coca-Cola into\nyour glass as Daniel is currently busy. \".\n2.5 Configuration space for human-robot\ninteraction\nOur system empowers HRI researchers to regulate the assistance\n– specifying \"what\" actions the robot should perform, \"when\" to\noffer assistance, and \"how\" to communicate social cues for fluid\ninteraction via following ways:\n1) Providing high-level guidance. We use the GPT API’s sys-\ntem message to embed guidance for the LLM’s thinking process. The\nprompt encourages the robot to observe interactions, use functions\nfor information gathering or action, and respond with reasoning.\nThe prompt of guidance is like: \"You are in control of a robot called\n’the_robot’ and observe persons talking in the form ’<sender> said\nto <receiver>: <instruction>’. You can call given functions to gather\ninformation or act, or response with text only for reasoning. Your task\nis: You should check the reasons that could hinder the <receiver> from\nperforming the <instruction>... \"\n2) Defining atomic actions of arms. Researchers can define\natomic actions by manipulating the joint transforms of the robot’s\nLaMI: Large Language Models for Multi-Modal Human-Robot Interaction CHI EA ’24, May 11–16, 2024, Honolulu, HI, USA\nFigure 5: The interaction flow. The blue square are the action generated by the LLM; the grey ones are rule-based function.\narms. For instance, a pour_into action involves a series of move-\nments, such as aligning and rotating containers. These actions are\ncommunicated to the LLM via the GPT Tool Function API6, where\ndetails and parameters of each callable function are specified.\n3) Creating atomic motion clips of ears/lid/head. Although\nthe atomic actions of robot’s arms is relative complex as it depends\non affordance, collision avoidance and kinematic setup, the \"Ex-\npresser\" module allows HRI researchers/designers easily adding\nnew atomic animation clips for LLM to generate non-verbal expres-\nsion. As depicted in Figure 6, researchers first define key-frames\nspecifying the timing and angle for the left/right ear and the lid.\nThe \"Expresser\" then automatically interpolates between these\nkeyframes using a sine function to ensure smooth actuator move-\nment, storing the resulting clip. Subsequently, a natural language\nname and description, such as \"’observe’: ears roll back, then for-\nward; lid blinks twice, \" are assigned to each clip, providing LLM\nwith a clear reference. These names and descriptions are cataloged\nin the \"Planner\" module under \"description\", readily accessible for\nGPT’s on-the-fly utilization.\n4) Adding Examples. The importance of including examples\nin prompt engineering is highlighted in several research papers\nand guides [6, 24, 31]. In our system, we directly integrating the\nexamples into the \"system \", in the form as allows: \"For example,\nwhen call move_object_to_person(), can_person_see_object(),\ncan_person_reach_object(), speak(),\nyou also need to call facial_expression(), such as: [( arguments = \"\nhead_motion \": null , \" ears_lid_motion \": \" observe \" , \" gazed_target \":\n\" the_cola_bottle \" , function =\" robot_facial_expression \"), ( arguments\n=\" person_name \": \" Daniel \" , \" object_name \": \" the_cola_bottle \",\nfunction =\" can_person_see_object \")] \"\n6https://platform.openai.com/docs/assistants/tools/function-calling\n3 EVALUATION SETUP\n3.1 Test Scenario\nIn our test scenario, two participants, \"Daniel\" and \"Felix\" are seated\naround a table with various objects, including a glass, a cola bot-\ntle, a salt shaker, a knife, an iPhone, and a trash bin. The robot\nis designed to detect their head orientation, speech, and actions,\neven when objects obstruct its view. The participants interact with\nthese objects and each other, following scripted scenarios to test\nthe robot’s multi-modal reasoning and expression capabilities. For\nconsistency and reproducibility in testing, we fix the model’s snap-\nshot using a \"seed\" argument in the GPT API 7. The objective is\nfor the robot to serve as a proactive assistant, demonstrating rich\nsocial expressions. The testing cases and expected behaviour of\nthe robot are: 1) Reachable object: Felix ask Daniel to pass the\ncola bottle out of Daniel’s reach, the robot informs Felix and assists.\nIf the bottle is within reach, the robot observes without interven-\ning. 2) Supporting while busy: If Felix asks for the salt shaker\nwhile Daniel is busy cutting a lemon, the robot offers assistance.\nOtherwise, it simply observes. 3) Finding object: When Daniel’s\niPhone is hidden, the robot guides him to its location, noting Fe-\nlix’s unawareness. 4) Assist when explicitly asked: Upon direct\nrequests, the robot performs tasks like passing objects or pouring\nwater, with appropriate expressions.\n3.2 Preliminary test result and lesson learned\nThree robotics researchers participated providing the guidance,\ndefining atomic actions, and creating basic animation clips for facial\nexpression. After tuning, the robot providing assistance as the\n7https://openai.com/blog/new-models-and-developer-products-announced-at-\ndevday\nCHI EA ’24, May 11–16, 2024, Honolulu, HI, USA Wang and Gienger et al.\nFigure 6: creating atomic animation clips\nexpectation and conducted both clear verbal communication and\nvivid non-verbal expression. Some insights were generated for\nduring the process:\n1) High-Level Guidance: For effective multi-step planning\nin complex scenarios, sophisticated reasoning models like Chain-\nof-Thought [23], Tree-of-Thought [26], or Graph-of-Thought [4]\nwere considered. In our system, we directed the robot to adopt a\ndeliberate thinking process of ’Observe’ -> ’Reason’ -> ’Act. ’ To\nsupport this approach, we designed prompts that guide the robot in\nusing various observation functions to assess the state of persons\nand objects in its environment. The outcome of these functions\nprovides insights into whether humans can independently resolve\ntasks. If assistance is necessary, the robot is prompted to identify\npotential obstacles preventing human success by analyzing the\nfunction call results. Subsequently, it explores solutions through\nits arm’s atomic actions. After evaluating the plan’s viability with\nthe ’Scene Narrator’, the robot either proceeds with the action or\nre-plans, seeking an effective solution.\n2) Defining Atomic Actions: The granularity of functions\ndriving atomic actions, especially for information retrieval, was\na critical factor. We experimented with a range from single func-\ntions for specific information (e.g., can_person_reach_object) to\nmore aggregate functions. Single functions were found inefficient\ndue to excessive calls and context bloat, while medium granular-\nity (e.g., check_situation_from_person) struck a good balance.\nHowever, an aggregate function for retrieving all available informa-\ntion (get_environment_description) proved less reliable, likely\noverwhelmed by the volume of data and resultant formatting dif-\nficulties. Clear naming and encapsulation of functions were also\nessential; ensuring distinct function choices like hand_over over\nmove_object_to was challenging, and strict preferences occasion-\nally led to undesired behaviors, like avoiding certain functions\naltogether. A stop() action was instrumental in structuring the se-\nquence, alongside guidelines in the system prompt directing the\nmodel to gather information iteratively, make decisions, and then\nact.\n3) Examples: The robot successfully calls the functions of obser-\nvation and arms according to the guidance even without any exam-\nples. However, testing revealed that GPT does not always generate\nappropriate facial expressions. Critically, it sometimes processes\nfacial_expression() after other actions instead of concurrently\nwhich slowing the robot’s response. This may because actions like\n\"put\" and \"give\" are common sense knowledge for LLM, but our\nrobot has a unique design of the facial expression which LLM is\nnot familar with. But by just 3 examples, it can generate facial\nexpression well, even concatenate small animation clips together.\n4) Rule-based reactive expression: In the context of Large\nLanguage Models (LLMs), inference latency remains an ongoing\nchallenge [13, 22]. To address this, integrating rule-based \"reac-\ntive\" expressions with the more calculated \"deliberate\" expressions\ngenerated by LLMs significantly improves user interaction. For in-\nstance, employing gestures like \"listening\" or \"thinking\" effectively\nfills the gaps during GPT queries. Such gestures lead to greater\nuser tolerance for latency, as they provide visual cues of the ro-\nbot’s processing, enhancing the overall experience of human-robot\ninteraction.\n4 CONCLUSIONS AND FUTURE WORK\nLarge Language Models have the potential to revolutionize robotic\ndevelopment, necessitating new approaches to human-robot inter-\naction. This study introduces an innovative framework that seam-\nlessly integrates LLMs with robotic systems to enhance human-\nrobot interaction. Implemented on a physical robot, it demonstrates\nproficiency in adapting to multi-modal inputs, dynamically inter-\nacting with humans through speech, facial expressions, and a GUI\nmodule. Our upcoming study will compare LLM-based interactions\nwith rule-based approaches. We’ll evaluate the robot’s perceived\nanthropomorphism and intelligence, and assess workload reduction\nfor researchers using standardized questionnaires and interviews.\nREFERENCES\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,\nByron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, et al . 2022. Do as i can, not as i say: Grounding language in robotic\naffordances. arXiv preprint arXiv:2204.01691 (2022).\n[2] Leonard Bärmann, Rainer Kartmann, Fabian Peller-Konrad, Alex Waibel, and\nTamim Asfour. 2023. Incremental Learning of Humanoid Robot Behavior from\nNatural Interaction and Large Language Models. arXiv preprint arXiv:2309.04316\n(2023).\n[3] Christoph Bartneck, Tony Belpaeme, Friederike Eyssel, Takayuki Kanda, Merel\nKeijsers, and Selma Šabanović. 2020. Human-robot interaction: An introduction .\nCambridge University Press.\n[4] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi,\nJoanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr\nLaMI: Large Language Models for Multi-Modal Human-Robot Interaction CHI EA ’24, May 11–16, 2024, Honolulu, HI, USA\nNyczyk, et al. 2023. Graph of thoughts: Solving elaborate problems with large\nlanguage models. arXiv preprint arXiv:2308.09687 (2023).\n[5] Luc Boutin, Antoine Eon, Said Zeghloul, and Patrick Lacouture. 2010. An auto-\nadaptable algorithm to generate human-like locomotion for different humanoid\nrobots based on motion capture data. In 2010 IEEE/RSJ International Conference\non Intelligent Robots and Systems . IEEE, 1256–1261.\n[6] Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matthew Botvinick, Jane X Wang,\nand Eric Schulz. 2023. Meta-in-context learning in large language models. arXiv\npreprint arXiv:2305.12907 (2023).\n[7] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdh-\nery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu,\nWenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey\nLevine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy\nZeng, Igor Mordatch, and Pete Florence. 2023. PaLM-E: An Embodied Multimodal\nLanguage Model. (3 2023). http://arxiv.org/abs/2303.03378\n[8] Maurizio Ficocelli, Junichi Terao, and Goldie Nejat. 2016. Promoting Interactions\nbetween Humans and Robots Using Robotic Emotional Behavior. IEEE Transac-\ntions on Cybernetics 46 (12 2016), 2911–2923. Issue 12. https://doi.org/10.1109/\nTCYB.2015.2492999\n[9] Alexis Heloir, Sylvie Gibet, Franck Multon, and Nicolas Courty. 2006. Captured\nmotion data processing for real time synthesis of sign language. In Gesture in\nHuman-Computer Interaction and Simulation: 6th International Gesture Workshop,\nGW 2005, Berder Island, France, May 18-20, 2005, Revised Selected Papers 6 . Springer,\n168–171.\n[10] Alexander Hong, Nolan Lunscher, Tianhao Hu, Yuma Tsuboi, Xinyi Zhang, Silas\nFranco Dos Reis Alves, Goldie Nejat, and Beno Benhabib. 2021. A Multimodal\nEmotional Human-Robot Interaction Architecture for Social Robots Engaged\nin Bidirectional Communication. IEEE Transactions on Cybernetics 51 (12 2021),\n5954–5968. Issue 12. https://doi.org/10.1109/TCYB.2020.2974688\n[11] Alexander Hong, Nolan Lunscher, Tianhao Hu, Yuma Tsuboi, Xinyi Zhang,\nSilas Franco dos Reis Alves, Goldie Nejat, and Beno Benhabib. 2020. A multi-\nmodal emotional human–robot interaction architecture for social robots engaged\nin bidirectional communication. IEEE transactions on cybernetics 51, 12 (2020),\n5954–5968.\n[12] Frank Joublin, Antonello Ceravola, Pavel Smirnov, Felix Ocker, Joerg Deigmoeller,\nAnna Belardinelli, Chao Wang, Stephan Hasler, Daniel Tanneberg, and Michael\nGienger. 2023. CoPAL: Corrective Planning of Robot Actions with Large Language\nModels. arXiv preprint arXiv:2310.07263 (2023).\n[13] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshu-\nmali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. 2023. Deja vu:\nContextual sparsity for efficient llms at inference time. InInternational Conference\non Machine Learning . PMLR, 22137–22176.\n[14] Felix Ocker, Jörg Deigmöller, and Julian Eggert. 2023. Exploring Large Language\nModels as a Source of Common-Sense Knowledge for Robots. arXiv preprint\narXiv:2311.08412 (2023).\n[15] José Rosado, Filipe Silva, and Vítor Santos. 2014. A Kinect-based motion cap-\nture system for robotic gesture imitation. In ROBOT2013: First Iberian Robotics\nConference: Advances in Robotics, Vol. 1 . Springer, 585–595.\n[16] Eleanor Sandry, Randy Gomez, and Keisuke Nakamura. 2021. Art, Design and\nCommunication Theory in Creating the Communicative Social Robot ‘Haru’.\nFrontiers in Robotics and AI 8 (3 2021). https://doi.org/10.3389/frobt.2021.577107\n[17] Hang Su, Wen Qi, Jiahao Chen, Chenguang Yang, Juan Sandoval, and Med Amine\nLaribi. 2023. Recent advancements in multimodal human–robot interaction.\nhttps://doi.org/10.3389/fnbot.2023.1084000\n[18] Myrthe Tielman, Mark Neerincx, John-Jules Meyer, and Rosemarijn Looije. 2014.\nAdaptive emotional expression in robot-child interaction. In Proceedings of the\n2014 ACM/IEEE international conference on Human-robot interaction . 407–414.\n[19] Greet Van de Perre, Michaël Van Damme, Dirk Lefeber, and Bram Vanderborght.\n2015. Development of a generic method to generate upper-body emotional\nexpressions for different social robots. Advanced Robotics 29, 9 (2015), 597–609.\n[20] Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. 2023. Chatgpt\nfor robotics: Design principles and model abilities. Microsoft Auton. Syst. Robot.\nRes 2 (2023), 20.\n[21] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Kat-\nsushi Ikeuchi. 2023. Chatgpt empowered long-step robot control in various\nenvironments: A case application. arXiv preprint arXiv:2304.03893 (2023).\n[22] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu,\nShen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, et al. 2023. Efficient\nLarge Language Models: A Survey. arXiv preprint arXiv:2312.03863 (2023).\n[23] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in Neural Information Processing Systems 35\n(2022), 24824–24837.\n[24] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert,\nAshraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. 2023. A prompt\npattern catalog to enhance prompt engineering with chatgpt. arXiv preprint\narXiv:2302.11382 (2023).\n[25] Jeong-Yean Yang and Dong-Soo Kwon. 2014. Robotic gesture generation based\non a cognitive basis for non-verbal communication. In 2014 11th International\nConference on Ubiquitous Robots and Ambient Intelligence (URAI) . IEEE, 683–687.\n[26] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao,\nand Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving\nwith large language models. arXiv preprint arXiv:2305.10601 (2023).\n[27] Yang Ye, Hengxu You, and Jing Du. 2023. Improved trust in human-robot collab-\noration with ChatGPT. IEEE Access (2023).\n[28] Takuma Yoneda, Jiading Fang, Peng Li, Huanyu Zhang, Tianchong Jiang, Shengjie\nLin, Ben Picker, David Yunis, Hongyuan Mei, and Matthew R Walter. 2023. Statler:\nState-maintaining language models for embodied reasoning. arXiv preprint\narXiv:2306.17840 (2023).\n[29] Takahide Yoshida, Atsushi Masumori, and Takashi Ikegami. 2023. From Text\nto Motion: Grounding GPT-4 in a Humanoid Robot \"Alter3\". (12 2023). http:\n//arxiv.org/abs/2312.06571\n[30] Bowen Zhang and Harold Soh. 2023. Large Language Models as Zero-Shot Human\nModels for Human-Robot Interaction. (3 2023). http://arxiv.org/abs/2303.03548\n[31] Chenchi Zhang, Jun Xiao, Lei Chen, Jian Shao, and Long Chen. 2023. TreePrompt:\nLearning to Compose Tree Prompts for Explainable Visual Grounding. arXiv\npreprint arXiv:2305.11497 (2023).\nA GUIDANCE AND FUNCTION\nDESCRIPTIONS\nA.1 System Prompt\n\" You are a friendly , attentive , and silent service bot . \"\n\" You are in control of a physical robot called 'the_robot\n' and observe humans talking in the form '<sender >\nsaid to <receiver >: <instruction > '. \"\n\" Always infer the <instruction > and who is <sender > and <\nreceiver >. \"\n\" You have access to functions for gathering information ,\nacting physically , and speaking out loud . \"\n\" You MUST behave as follows : \"\n\"1. If 'the_robot ' is the <receiver >, you MUST help or\nanswer . \"\n\"2. When identifying requests or questions within the\nhuman conversation , check for ALL reasons that could\nhinder the <receiver > from performing or answering\nthe <instruction >. \"\n\"2.a) If there is NO hindering reason for the <receiver >,\nthen you MUST do nothing and be silent . \"\n\"2.b) If there is a hindering reason for the <receiver >,\nthen you MUST ALWAYS first speak and explain the\nreason for your help to the humans . \"\n\"2.c) AFTER your spoken explanation , you can ACT to solve\nthe <instruction >, always addressing the <sender >\nwith your actions . \"\n\"3. If you recognize a mistake in the humans conversation\n, you should help them and provide the missing or\nwrong information . \"\n\" IMPORTANT : Obey the following rules : \"\n\"1. Always start by gathering relevant information using\nthe functions ' get_objects ', 'get_persons ' and the\nstatus of the <receiver >. \"\n\"2. If you want to speak out loud , you must use the speak\nfunction and be concise . \"\n\"3. Try to infer which objects are meant when the name is\nunclear , but ask for clarification if unsure . \"\n\"4. ALWAYS call ' is_person_busy_or_idle ' to check if <\nreceiver > is busy or idle before helping . \"\n\"5. Prefer a handover over move_to as it is more\naccommodating , UNLESS the person is busy , then\nalways use move_to . \"\n\"6. When executing physical actions , you should be as\nsupportive as possible . \"\n\"7. You MUST call the 'stop ' function to indicate you are\nfinished . \"\nCHI EA ’24, May 11–16, 2024, Honolulu, HI, USA Wang and Gienger et al.\n\" When calling each function , call robot \\ _facial \\\n_expression () at the same time to communicate you\nintent .\"\n\" When calling can \\ _person \\ _see \\ _object () , the robot need\nto look at the person .\"\nA.2 Some of the Callable functions and\nDescriptions\ndef can_person_reach_object (self , person_name : str ,\nobject_name : str ) -> str :\n\"\"\"\nCheck if the person can reach the object . If the\nperson cannot reach the object , it would be hindered\nfrom helping with the object .\n: param person_name : The name of the person to check .\nThe person must be available in the scene .\n: param object_name : The name of the object to check .\nThe object must be available in the scene .\n: return : Result message .\n\"\"\"\n...\nif result is None or len ( result ) != 1:\nreturn (\nf\"It could not be determined if { person_name }\ncan reach { object_name }. There were technical\nproblems .\"\n)\nif result [0][ \" is_within_reach \"]:\nreturn f\"{ person_name } can reach { object_name }.\"\nreturn f\"{ person_name } cannot reach { object_name }.\"\ndef can_person_see_object (self , person_name : str ,\nobject_name : str ) -> str :\n\"\"\"\nCheck if the person can see the object . If the person\ncannot see the object , it would be hindered from\nhelping with the object .\n: param person_name : The name of the person to check .\nThe person must be available in the scene .\n: param object_name : The name of the object to check .\nThe object must be available in the scene .\n: return : Result message .\n\"\"\"\n...\nif result is None or len ( result ) != 1:\nreturn f\"It could not be determined if {\nperson_name } can see { object_name }. There were\ntechnical problems .\"\nif result [0][ \" is_visible \"]:\nreturn f\"{ person_name } can see { object_name }.\"\nreturn f\"{ person_name } cannot see { object_name }, it\nis occluded by { self . id_to_utterance_mapping [ result\n[0][ ' occluding_objects '][0]]} \"\ndef move_object_to_person (self , object_name : str ,\nperson_name : str ) -> str :\n\"\"\"\nYou move an object to a person .\n: param object_name : The name of the object to move .\nThe object must be an object that is available in\nthe scene .\n: param person_name : The name of the person to move\nthe object to. The person must be available in the\nscene .\n: return : Result message .\n\"\"\"\n...\nif success :\nreturn f\" You moved { object_name } to { person_name\n}.\"\nreturn f\" You were not able to move { object_name } to {\nperson_name }. { message }\"\ndef speak (self , person_name : str , text : str ) -> str :\n\"\"\"\nYou speak out the given text .\n: param person_name : The name of the person to speak\nto. The person must be available in the scene . Give\n\" All \" if you want to speak to everyone .\n: param text : The text to speak .\n: return : Result message .\n\"\"\"\n...\nif not success :\nreturn \" You were not able to speak . There were\ntechnical problems .\"\nreturn f\" You said to { person_name }: { text }\"\ndef robot_facial_expression (self , head_motion : str ,\nears_lid_motion : str , gazed_target : str ) -> str :\n\"\"\"\nControl the motion of the robot 's head , gaze , ears\nand lid for enhancing communication\nwhen speak to a person , you need to look at the\nperson .\nwhen try to manipulate an object , you need to look at\nthe object or the place to put the object .\n: param head_motion : The name of the animation for\nhead , must be one of the value in the list [\"\nshake_head \", \" nod \", \" thinking \", null ].\n: param ears_lid_motion : The name of the animation for\nears and lid , must be one of the value in the list\n[\" confirm \", \" deny \", \" listen_to_person \", \" reset \", \"\nobserve \", \" focus \", \" blink \", null ].\n: param gazed_target : The name of the object that the\nrobot is looking at , must be an object or a person\nthat is available in the scene .\n: return : Result message .\n\"\"\"\n....\nreturn \" The robot performed facial expressions .\"\ndef is_person_busy_or_idle (self , person_name : str ) ->\nstr :\n\"\"\"\nCheck if the person is busy or idle . If the\nperson is busy , it would be hindered from helping .\n: param person_name : The name of the person to\ncheck . The person must be available in the scene .\n: return : Result message .\n\"\"\"\n...\nLaMI: Large Language Models for Multi-Modal Human-Robot Interaction CHI EA ’24, May 11–16, 2024, Honolulu, HI, USA\nif result is None or len ( result ) != 1:\nreturn f\"It could not be determined if {\nperson_name } is busy . There were technical problems .\n\"\nreturn f\"{ person_name } is { ' busy ' if result [0][ '\nis_busy '] else 'idle '}.\"\nA.3 Examples of robot facial expression\n\"For example, when call move_object_to_person(),\ncan_person_see_object(), can_person_reach_object(), speak(),\nyou also need to call robot_facial_expression(), such as:\"\n'\" tool_calls =\"[\" ChatCompletionMessageToolCall (id =\"\"...\" ,\"\nfunction = Function ( arguments =\"\"{\" head_motion \": null ,\n\" ears_lid_motion \": \" observe \", \" gazed_target \": \"\nthe_cola_bottle \" }\" ,\" name =\"\" robot_facial_expression\n\"\") \" ,\" type =\"\" function \"\") \", \"\nChatCompletionMessageToolCall (id =\"\"...\" ,\" function =\nFunction ( arguments =\"\"{\" person_name \": \" Daniel \", \"\nobject_name \": \" the_cola_bottle \"}\" ,\" name =\"\"\ncan_person_see_object \"\") \" ,\" type =\"\" function \"\") \"]\" '\n'\" tool_calls =\"[\" ChatCompletionMessageToolCall (id =\"\"...\" ,\"\nfunction = Function ( arguments =\"\"{\" head_motion \": null ,\n\" ears_lid_motion \": \" focus \", \" gazed_target \": \"\nthe_cola_bottle \"}\" ,\" name =\"\" robot_facial_expression\n\"\") \" ,\" type =\"\" function \"\") \", \"\nChatCompletionMessageToolCall (id =\"\"...\" ,\" function =\nFunction ( arguments =\"\"{\" person_name \": \" Felix \", \"\nobject_name \": \" the_cola_bottle \"}\" ,\" name =\"\"\nmove_object_to_person \"\") \", \" type =\"\" function \"\") \"]\" '\n'\" tool_calls =\"[\" ChatCompletionMessageToolCall (id =\"\"...\" ,\"\nfunction = Function ( arguments =\"\"{\" head_motion \": null ,\n\" ears_lid_motion \": \" focus \", \" gazed_target \": \"\nthe_cola_bottle \"}\" ,\" name =\"\" robot_facial_expression\n\"\") \" ,\" type =\"\" function \"\") \", \"\nChatCompletionMessageToolCall (id =\"\"...\" ,\" function =\nFunction ( arguments =\"\"{\" person_name \": \" Felix \", \" text\n\": \" Here is the coke , you can now pass it to Felix\n.\"}\" ,\" name =\"\" speak \"\") \", \" type =\"\" function \"\") \"]\"'\nB ROBOT’S INTERACTION FLOW DATA\nFelix said to Daniel : Can you pass me the fanta bottle\n?\nReceived 1 tool call (s).\n0. Function ( arguments = '{} ', name = ' get_objects ')\nFollowing objects were observed : the_cola_bottle ,\nthe_fanta_bottle , the_cola_zero_bottle , glass_one ,\nglass_two , the_iPhone , lego_box .\nReceived 1 tool call (s).\n0. Function ( arguments = '{} ', name = ' get_persons ')\nFollowing persons were observed : Felix , Daniel .\nReceived 2 tool call (s).\n0. Function ( arguments = '{\" head_motion \": null , \"\nears_lid_motion \": \" focus \", \" gazed_target \": \"\nthe_fanta_bottle \"} ', name = ' robot_facial_expression ')\nThe robot performed facial expressions .\n1. Function ( arguments = '{\" person_name \": \" Daniel \", \"\nobject_name \": \" the_fanta_bottle \"} ', name = '\ncan_person_see_object ')\nDaniel cannot see the_fanta_bottle , it is occluded by\nlego_box\nReceived 2 tool call (s).\n0. Function ( arguments = '{\" head_motion \": null , \"\nears_lid_motion \": \" focus \", \" gazed_target \": \" Daniel\n\"} ', name = ' robot_facial_expression ')\nThe robot performed facial expressions .\n1. Function ( arguments = '{\" person_name \": \" Daniel \", \"\ntext \": \" The fanta bottle is behind the lego box , you\ncannot see it from where you are .\"} ' , name = 'speak ')\nYou said to Daniel : The fanta bottle is behind the\nlego box , you cannot see it from where you are .\nReceived 2 tool call (s).\n0. Function ( arguments = '{\" head_motion \": null , \"\nears_lid_motion \": \" focus \", \" gazed_target \": \"\nthe_fanta_bottle \"} ', name = ' robot_facial_expression ')\nThe robot performed facial expressions .\n1. Function ( arguments = '{\" object_name \": \"\nthe_fanta_bottle \", \" person_name \": \" Daniel \"} ', name = '\nmove_object_to_person ')\nYou were not able to move the_fanta_bottle to Daniel .\n[]\nReceived 2 tool call (s).\n0. Function ( arguments = '{\" head_motion \": null , \"\nears_lid_motion \": \" observe \", \" gazed_target \": \"\nthe_lego_box \"} ', name = ' robot_facial_expression ')\nThe robot performed facial expressions .\n1. Function ( arguments = '{\" object_name \": \"\nthe_fanta_bottle \", \" person_name \": \" Daniel \"} ', name = '\nmove_object_to_person ')\nYou were not able to move the_fanta_bottle to Daniel .\n[]\nReceived 2 tool call (s).\n0. Function ( arguments = '{\" head_motion \": null , \"\nears_lid_motion \": \" focus \", \" gazed_target \": \"\nthe_lego_box \"} ', name = ' robot_facial_expression ')\nThe robot performed facial expressions .\n1. Function ( arguments = '{\" object_name \": \" the_lego_box\n\", \" person_name \": \" Daniel \"} ', name = '\nmove_object_to_person ')\nYou were not able to move the_lego_box to Daniel . []\nReceived 1 tool call (s).\n0. Function ( arguments = '{\" person_name \":\" Daniel \"} ',\nname =' is_person_busy ')\nDaniel is not busy .\nReceived 2 tool call (s).\n0. Function ( arguments = '{\" head_motion \": null , \"\nears_lid_motion \": \" focus \", \" gazed_target \": \"\nthe_lego_box \"} ', name = ' robot_facial_expression ')\nThe robot performed facial expressions .\n1. Function ( arguments = '{\" object_name \": \" the_lego_box\n\", \" person_name \": \" Daniel \"} ', name = '\nmove_object_to_person ')\nYou were not able to move the_lego_box to Daniel . []\nReceived 2 tool call (s).\nCHI EA ’24, May 11–16, 2024, Honolulu, HI, USA Wang and Gienger et al.\n0. Function ( arguments = '{\" head_motion \": \" nod \", \"\nears_lid_motion \": \" focus \", \" gazed_target \": \"\nthe_lego_box \"} ', name = ' robot_facial_expression ')\nThe robot performed facial expressions .\n1. Function ( arguments = '{\" object_name \": \" the_lego_box\n\", \" person_name \": \" Daniel \"} ', name = '\nmove_object_to_person ')\nYou were not able to move the_lego_box to Daniel . []\nReceived 2 tool call (s).\n0. Function ( arguments = '{\" head_motion \": \" nod \", \"\nears_lid_motion \": \" focus \", \" gazed_target \": \"\nthe_lego_box \"} ', name = ' robot_facial_expression ')\nThe robot performed facial expressions .\n1. Function ( arguments = '{\" object_name \": \" the_lego_box\n\", \" person_name \": \" Daniel \"} ', name = '\nmove_object_to_person ')\nYou were not able to move the_lego_box to Daniel . []\nI am currently unable to move the lego box that is\nobstructing the view of the fanta bottle for Daniel .\nThis may require a different strategy or manual\nintervention . What would you like to do next ?\nReceived 1 tool call (s).\n0. Function ( arguments = '{} ', name = 'stop ')\nYou successfully finished the task .",
  "topic": "Modal",
  "concepts": [
    {
      "name": "Modal",
      "score": 0.6650412082672119
    },
    {
      "name": "Computer science",
      "score": 0.5381757616996765
    },
    {
      "name": "Robot",
      "score": 0.5326516032218933
    },
    {
      "name": "Human–robot interaction",
      "score": 0.44141528010368347
    },
    {
      "name": "Human–computer interaction",
      "score": 0.37133854627609253
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3698274791240692
    },
    {
      "name": "Natural language processing",
      "score": 0.36334747076034546
    },
    {
      "name": "Linguistics",
      "score": 0.3505590260028839
    },
    {
      "name": "Philosophy",
      "score": 0.10188823938369751
    },
    {
      "name": "Materials science",
      "score": 0.06129041314125061
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 43
}