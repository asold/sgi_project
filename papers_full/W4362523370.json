{
    "title": "A COVID-19 medical image classification algorithm based on Transformer",
    "url": "https://openalex.org/W4362523370",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2111094588",
            "name": "Keying Ren",
            "affiliations": [
                "Tianjin University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2232565645",
            "name": "Geng Hong",
            "affiliations": [
                "Tianjin University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2105902370",
            "name": "Xiaoyan Chen",
            "affiliations": [
                "Tianjin University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2164288045",
            "name": "zichen wang",
            "affiliations": [
                "Tianjin University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2111094588",
            "name": "Keying Ren",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2232565645",
            "name": "Geng Hong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2105902370",
            "name": "Xiaoyan Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2164288045",
            "name": "zichen wang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4200052220",
        "https://openalex.org/W3101606529",
        "https://openalex.org/W3006110666",
        "https://openalex.org/W3006882119",
        "https://openalex.org/W4200138904",
        "https://openalex.org/W2611650229",
        "https://openalex.org/W3004227146",
        "https://openalex.org/W3196462926",
        "https://openalex.org/W3135243128",
        "https://openalex.org/W3095044166",
        "https://openalex.org/W3215193005",
        "https://openalex.org/W3027682070",
        "https://openalex.org/W4214756141",
        "https://openalex.org/W4281712277",
        "https://openalex.org/W4306795667",
        "https://openalex.org/W4321260760",
        "https://openalex.org/W4296068600",
        "https://openalex.org/W4313525707",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3204538018",
        "https://openalex.org/W4312950730",
        "https://openalex.org/W6603287499",
        "https://openalex.org/W2798251715",
        "https://openalex.org/W3028070348",
        "https://openalex.org/W3014361272",
        "https://openalex.org/W6825589429",
        "https://openalex.org/W1647645866",
        "https://openalex.org/W3018775276",
        "https://openalex.org/W3083753334",
        "https://openalex.org/W4221067622",
        "https://openalex.org/W3013601031",
        "https://openalex.org/W3017855299",
        "https://openalex.org/W4206608885",
        "https://openalex.org/W3156862851",
        "https://openalex.org/W3033616466",
        "https://openalex.org/W3139833881",
        "https://openalex.org/W4220772768",
        "https://openalex.org/W3017403618",
        "https://openalex.org/W3164076340",
        "https://openalex.org/W3011149445",
        "https://openalex.org/W3092624683",
        "https://openalex.org/W3096956107",
        "https://openalex.org/W3101156210",
        "https://openalex.org/W3105081694"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2023) 13:5359  | https://doi.org/10.1038/s41598-023-32462-2\nwww.nature.com/scientificreports\nA COVID‑19 medical image \nclassification algorithm based \non Transformer\nKeying Ren , Geng Hong , Xiaoyan Chen * & Zichen Wang \nCoronavirus 2019 (COVID‑19) is a new acute respiratory disease that has spread rapidly throughout the \nworld. This paper proposes a novel deep learning network based on ResNet‑50 merged transformer \nnamed RMT‑Net. On the backbone of ResNet‑50, it uses Transformer to capture long‑distance feature \ninformation, adopts convolutional neural networks and depth‑wise convolution to obtain local \nfeatures, reduce the computational cost and acceleration the detection process. The RMT‑Net includes \nfour stage blocks to realize the feature extraction of different receptive fields. In the first three \nstages, the global self‑attention method is adopted to capture the important feature information and \nconstruct the relationship between tokens. In the fourth stage, the residual blocks are used to extract \nthe details of feature. Finally, a global average pooling layer and a fully connected layer perform \nclassification tasks. Training, verification and testing are carried out on self‑built datasets. The RMT‑\nNet model is compared with ResNet‑50, VGGNet‑16, i‑CapsNet and MGMADS‑3. The experimental \nresults show that the RMT‑Net model has a Test_ acc of 97.65% on the X‑ray image dataset, 99.12% \non the CT image dataset, which both higher than the other four models. The size of RMT‑Net model is \nonly 38.5 M, and the detection speed of X‑ray image and CT image is 5.46 ms and 4.12 ms per image, \nrespectively. It is proved that the model can detect and classify COVID‑19 with higher accuracy and \nefficiency.\nIn recent years, medical images analysis has been widely used in the diagnosis field due to its non-invasive and \nfast. Traditional manual diagnosis methods are time-consuming and laborious, and each doctor may have dif-\nferent diagnostic principles, resulting in the diversity of diagnosis results. Therefore, automatic classification of \ncoronavirus ID-19 lesions in clinical Settings is quite necessary, which is the motivation of this study.\nRecent studies have shown that COVID-19 can be quickly and effectively diagnosed by observing the relevant \nfeatures of lung CT/X-ray scan  images1–4. The related algorithms based on deep learning are recognized as the \nmost effective approach to implement image classification of quantitatively and qualitatively with advantages \nof the workload reduction and misdiagnosis decrease by manual  diagnosis5,6. On this Background, Many deep \nlearning methods have been used to diagnose COVID-19. The medical image classification method based on \nCNN has achieved good results. Wang et al.7 proposed a lightweight residual projection-expansion-projection \nextension (PEPX) architecture named COVID-Net. The accuracy of the three classification tasks (COVID-19, \nnormal and pneumonia) is 92.4% , in the four-category task (COVID-19, viral pneumonia, bacterial pneumonia, \nand normal) is 83.5%. Chen et al. 8 proposed a lightweight convolutional neural network model named multi-\nscale gated multi-head attention depth-wise separable CNN(MGMADS-CNN). It achieved accuracy of 96.75% \non X-ray images. Song et al.9 proposed a Details Relation Extraction neural model (DRE-Net),which is based on \nthe pre-trained ResNet50 and added the Feature Pyramid Network (FPN), to extract the top-K details in the CT \nimages and obtain the image-level predictions. The DRE-Net model performed binary classification experiment \n(COVID-19 and bacterial pneumonia) on 1485 CT images. The accuracy of model achieved 94.0%. Oulefki et al10 \nproposed a COVID-19 segmentation method, which enhanced image contrast by combining linear and loga-\nrithmic splices parameter, and used an image segmentation method to minimize the over-segmentation regions \nto segment CT tomography images. The method has strong robustness and simplicity with accuracy of 98%. At \nthe same time, Oulefki et al.11 proposed a novel 3D visualization segmentation technique based on virtual real-\nity, which has achieved good results in the recognition, measurement and analysis of COVID-19. Pathak et al.12 \nused transfer learning to classify COVID-19. The cost-sensitive top-2 smooth loss function is used to eliminate \nnoise and unbalance of dataset categories. Experimental results show that this method has achieved remarkable \nOPEN\nCollege of Electronic Information and Automation, Tianjin University of Science and Technology, Tianjin 300222, \nChina. *email: cxywxr@tust.edu.cn\n2\nVol:.(1234567890)Scientific Reports |         (2023) 13:5359  | https://doi.org/10.1038/s41598-023-32462-2\nwww.nature.com/scientificreports/\nclassification effect. The above medical image classification method based on CNN mainly uses local spatial \ninformation and ignores “global” indication, resulting in sub-optimal performance classification.\nIn recent years, Vision Transformer has made a breakthrough in the field of computer vision. These models \nbased on global attention have become an effective method of medical diagnosis because they can learn the \ndependencies of global features. Al et al. 13 adopted the Vision Transformer architecture as the backbone. The \nencoder of this algorithm consists of two branches: one to process the original image and the other to process \nthe enhanced original image. Experimental results show that the proposed method is robust in a small amount of \ntraining data. Chetoui et al.14 fine-tuned several ViT models for multi-class classification problems (COVID-19, \npneumonia, and normal cases). Experimental results show that this method is superior to using CNN architec-\nture to detect COVID-19 on CXR images, and can effectively identify infected areas of COVID-19.Y ang et al.15 \nproposed covid-vision-transformer (CovidViT), applying transformer architecture and self-focus mechanisms \nto Covid-19 diagnosis. He used all the output from the encoder to achieve better results, and demonstrated that \nthe transformer-based model was better than CNN at Covid-19 identification. Y aqoob et al.16 proposed a deep \nlearning pipeline based on vision transformer that can accurately diagnose COVID-19 from chest CT images. \nThe accuracy rate was 98% on three open source CT scan datasets. Okolo et al.17 proposed input enhanced ViT \n(IEViT). The architecture introduces skip connection, using CNN to output the entire image, and then con-\nnecting to the output of each Transformer encoder layer. Experimental results show that the performance of \nIEViT model is superior to VIT. Cai et al.18 proposed Multi-MedVit, a COVID-19 diagnostic framework based \non multi-input transformer, and demonstrated that multi-scale data input enhanced data helps improve model \nstability. Experiments show that the performance of Multi-MedVit is better than that of VGG16, ResNet50 and \nother CNN-based methods. These literature indicate that transformer has advantages over CNN in the field of \nmedical image classification. However, if only the Transformer structure is used to extract features, the parameters \nof the network will be greatly increased. In order to combine the advantages of CNN and Vision Transformer, \nWe propose the ResNet Mixed with Transformer (RMT-Net).\nRMT-Net integrates Transformer on the basis of ResNet-50 to capture the long-distance dependence relation-\nship in the feature map, and uses convolutional neural network to obtain local features. Depth-wise convolution \nis introduced in RMT-Net to reduce computation and improve detection speed. The RMT-Net model is only \n38.5 M, and the detection speed of X-ray images and CT images is 5.46 ms and 4.12 ms for per image, realizing \na high-precision new coronary pneumonia medical image classification algorithm.\nIt is worth mentioning that our contributions can be summarized as follows\n• We propose a CNN-Transformer network structure, which has the ability to capture global features and local \nfeatures.\n• We introduce Depth-wise convolution in the last stage of the network to reduce the number of model param-\neters.\n• We maintain ResNet’s network architecture. The feature extraction capability of the network is improved \nby reducing the spatial size of features and increasing the number of channels, while the model size is kept \nwithin the ideal range.\n• We verified the effectiveness of RMT-Net as an image classification algorithm for COVID-19, and achieved \ngood results on both X-ray image datasets and CT image datasets.\nThe rest of this paper is organized as follows:The “Methodology” Section introduces the details of our proposed \nRMT-Net, including the overall structure and the mathematical mechanism of each module. In “Dataset prepara-\ntions” Section, we introduce the experimental environment and datasets. In “Experimental result and analysis” \nSection, we verify the proposed method and compare it with other models. Finally, we summarize this paper \nand discuss the possible research direction in the future.\nMethodology\nRMT‑Net model. Aiming at the problem of insufficient classification accuracy of COVID-19 X-ray and \nCT images, this paper proposes a fast and accurate RMT-Net, which is a novel deep learning network based on \nResNet-50 merged Transformer.\nThe RMT-Net structure is shown in Fig. 1. In order to enhance the migration and generalization ability, RMT-\nNet adopts the backbone of ResNet-50 with four different stages to extract features with different scales. In order \nto generate different hierarchical representations in the overall network, we successively stack three stage blocks \nwith the same input resolution to extract features of different scales.\nDue to transformer cannot transform the scale of feature map, patch aggregation is adopted to construct \ndownsampling to realize the hierarchical structure of the network. A downsampling is carried out before each \nstage, which is realized by 2 × 2 convolution with stride 2. The size of the input image is 256 × 256 × 3 . After \nthe first downsampling of Stem, a 128 × 128 feature map is obtained, and then a double downsampling opera -\ntion is performed after each stage. After the average pooling and fully connected layer, the classification results \nare output.\nStem. As a basic building block for processing the input data, Stem can preprocess the feature informa-\ntion of the input image, including segmentation, spatial dimension reduction, feature linear transformation \nand so on. Stem transforms image x∈ RH×W ×Cinto two-dimensional image patches xp ∈ RN ×(p2×C) , which \ncan be regarded as N = (H × W ) ÷ P2flattened two-dimensional sequence blocks, and the dimension of each \nsequence block is P2 × C . Where P is the sequence block size and C is the feature channel dimension. Posi-\n3\nVol.:(0123456789)Scientific Reports |         (2023) 13:5359  | https://doi.org/10.1038/s41598-023-32462-2\nwww.nature.com/scientificreports/\ntion Embedding performs a linear transformation (that is, the fully connected layer) on each two-dimensional \nsequence, and compresses the two-dimensional sequence into a one-dimensional feature vector.\nTransformer. Transformer consists of two parts: encoder and decoder. The encoder is mainly composed of \nmulti-head self-attention module and position Feedforward Network (FFN)19 . To address the difficulty of train-\ning deep networks, Transformer uses a residual connection in each sub-module. For the decoder, the self-atten-\ntion module in the decoder is adjusted to ensure the order between the output vectors unchanged. The composi-\ntion of Transformer is shown in Fig. 2. In this paper, Vision Transformer(VIT)20 and Visual Transformer(VT)21 \nare mainly used as lightweight Transformer structures, which can reduce the parameters of the model and keep \nthe performance of the model unchanged.\nFigure 1.  RMT-Net model structure.\nInput\nEmbedding\nInputs\nInput\nEmbedding\nOutputs\n⊕ ⊕\nMulti-Head\nSelf-Attention\nAdd & Norm\nFeed\nForward\nAdd & Norm\nMulti-Head\nSelf-Attention\nAdd &N orm\nFeed\nForward\nAdd &N orm\nAdd &N orm\nMasked\nMulti-Head\nSelf-Attention\nPositional\nEncoding\nPositional\nEncoding\nEncoder\nDecoder\nFigure 2.  Architecture of the standard  Transformer19.\n4\nVol:.(1234567890)Scientific Reports |         (2023) 13:5359  | https://doi.org/10.1038/s41598-023-32462-2\nwww.nature.com/scientificreports/\nVIT. VIT is adopted in stage 1 for global feature inference in early stages. In order to obtain a linear input \nsequence, the input image needs to be divided into patches of fixed size, and linear embedding and position \nembedding are performed for each patch and then input to the standard Transformer encoder. For image clas-\nsification, an additional learnable “classification marker” needs to be added to the first position of the sequence \nbefore training. The Transformer encoder consists of two modules, Multi-head Self-Attention(MHSA) and \nMultilayer Perceptron (MLP). Each module adopts residual connection and applies LayerNorm (LN) for nor -\nmalization. The MLP contains the GELU activation function and two fully connected layers. Equation (1)is the \ncalculation process of each part.\nwhere E ∈ RD×(P2×C) , Epos∈ RD ×(N +1) . All of the following means that the use of R is a set of real numbers \nunless otherwise stated. In CNN, each layer feature with locality, two-dimensional neighborhood structure and \nshift-invariant. In VIT, the self-attention layer is the extracted global features, while only the MLP layer is of local, \nshift-invariant. Therefore, VIT is used for global feature inference in Stage 1. Compared with CNN, VIT can pay \nmore attention to global features and quickly extract features that are beneficial to the network in the early stage.\nVT. With the deepening of the network, the number of features gradually increases. In order to achieve global \nfeature modeling and reduce network parameters at the same time, VT module is adopted in Stage 2. VT is a \nnew method to represent and process high-level semantics in images. Different from VIT, VT first uses convolu-\ntional layer to extract the underlying features. The VT module consists of three steps: (1) Group the features into \ndifferent semantic concepts to generate a compact set of visual tokens. The grouped semantic information can \nmake the module pay more attention to the semantic information that is beneficial to the network and ignore \nthe useless background information, and then reasonably allocate the computing cost of the entire module. The \nabove operations can be instantiated as Eq. (2).\nwhere W A ∈ RC×L forms semantic groups from X,SoftMax(·) is the softmax activation function,Xrepresents the \nfeature map.\nFor the input feature map X, VT uses point convolution to map each pixel xp ∈ R c in the feature map into L \ngroups, and then uses spatial pooling to obtain tokens. All tokens are converted into weights by soxfmax and \nmultiplied with the original feature map X  to obtain the reassigned attention map. However, many high-level \nsemantic information is sparse in practical applications, and each semantic information may only appear in a few \nimages. Therefore, modeling these high-level semantics independently can be a waste of computational resources. \nTo solve this problem, VT concatenates all layers, so each layer uses the output of the previous layer as input, in \nthis way the visual tokens can be gradually refined. Formally, we can define it as Eq. (3).\nHere W T→R → RC×C.\nTo establish the relationship between semantics, a transformer is applied. It can be expressed by the formula \nEq. (4).\nwhereTin, T\n′\nout, Tout ∈ RL×C  is Visual Tokens, (TinK )(TinQ)T ∈ RL×L is K and Q in Transformers, F1 ,F2 ∈ RL×C is \ntwo point convolution, and σ( ·) is the relu activation function.\nProjecting these visual tokens into the pixel space to obtain the enhanced feature map. As shown in Eq. 5.\nwhere Xout, Xin∈ RH ×W ×C represents the output and input feature map, XinW Q ∈ RH ×W ×C represents the \nQ value calculated by the input feature map, (TW K) ∈ RL×Crepresents the K value calculated from the token. \nW Q ∈ RC×C , W K ∈ RC×C represents the learning weight of Q and K. The result of the multiplication of K and Q \ndetermines how the information from visual tokens is projected into the original feature map.\nThe above is the calculation process of VT. VT can readjust the input feature map according to the semantic \nimportance, and provide the basis for subsequent classification by focusing on favorable semantic information.\nLMHSA. MHSA and Lightweight Multi-head Self-attention (LMHSA) modules are applied in Stage 3 to pro-\ncess the features extracted from the first two stages. The local features are refined by convolution residual blocks \nto improve the classification accuracy of the network. LMHSA is a lightweight multi-head self-attention model \n(1)\nZ0 =[ Xclass;X 1\nP E;X 2\nP E,...;X N\nP E ]+ Epos\nZ\n′\nℓ = MSA (LN (Zℓ−1 )) + Zℓ−1\nZℓ = MLP (LN (Z\n′\nℓ)) + Z\n′\nℓ\ny = LN (Z0\nℓ )\n(2)T = SoftMaxHW (XW A )T X\n(3)\nW R = TinW T →R\nT = SoftMaxHW (XW R )T\n(4)\nT\n′\nout = Tin + SoftMaxL((TinK )(TinQ )T )Tin\nTout = T\n′\nout + σ( T\n′\noutF1)F2\n(5)Xout = Xin+ SoftMaxL((XinW Q )(TW K )T )T\n5\nVol.:(0123456789)Scientific Reports |         (2023) 13:5359  | https://doi.org/10.1038/s41598-023-32462-2\nwww.nature.com/scientificreports/\nwith fewer parameters and easier to deploy than the original  MHSA22. In order to reduce the amount of com-\nputation, LMHSA uses depth-wise convolution with kernel size k × k and stride k to reduce the spatial size of K \nand V before performing the attention operation, and uses a learnable relative position bias B when computing \nMHSA. The calculation process of LMHSA module can be expressed as Eq. 6.\nwhere bias B ∈ Rn× n\nk2 is a learnable parameter. The learned relative positional bias can also be transferred to \nB\n′\n∈ Rm1×m2 of size m1 × m2 by bicubic interpolation. MHSA is often applied with multiple LMHSA modules, \nthat is, multiple Lightweight Attention functions (consistent with the number of “heads”) are applied to the \ninput. Each head outputs a sequence of size X, and then concatenates the h sequences into an n × d sequence, \nas the output of LMHSA.\nDataset preparations\nData collection. The datasets used in the experiment were collected from GitHub  website23, Kaggle \n website24, Kesci  website25 and Wuhan Tongji  Hospital26. The above datasets were annotated by hospital experts \nin a scientific and rigorous manner. The distribution of different samples of COVID-19 X-ray and CT images \nis shown in Fig.  3. According to the distribution of datasets, X-ray images were classified into four categories: \nnormal, bacterial pneumonia, viral pneumonia and COVID-19 pneumonia, and CT images were classified into \ntwo categories: normal and COVID-19 pneumonia. On the basis of previous  work8 , the dataset is extended \nwith more images. Figure 3a shows the X-ray image of normal lungs, (b) shows the X-ray image of COVID-19 \ninfected lungs, (c) shows the X-ray image of the lung infected with virus, and (d) shows the X-ray image of the \nlung infected with bacteria. Figure 3e,f show CT sections of normal lung and COVID-19 virus-infected lung.\nDataset settings. The distribution of collected datasets has the problem of data imbalance, which makes \nthe classifier tend to the class with a large number of samples, which is not conducive to the generalization \ncharacteristics and the objective judgment of the model. The data enhancement methods adopted in this paper \nmainly include affine  transformation27 , image  mirror28 and position  transformation29 . The data distribution \nbefore after data enhancement is shown in Table 1.\nIn Table 1, there are 25,100 X-ray images including 6450 normal, 6280 viral pneumonia, 6230 bacterial pneu-\nmonia and 6140 with COVID-19 X-ray images. The lung CT images include 8500 normal and 9000 with COVID-\n19. The augmented dataset can improve the generalization and the reliability abilities of the model. It is significant \nto enhance the robustness of the model and overcome the imbalance problem of positive and negative samples.\n(6)Light weight Attention(Q , K , V ) = SoftMax\n(QK T\n√dk\n+B\n)\nV\nFigure 3.  X-ray images (a–d) and CT images (e–f).\n6\nVol:.(1234567890)Scientific Reports |         (2023) 13:5359  | https://doi.org/10.1038/s41598-023-32462-2\nwww.nature.com/scientificreports/\nExperimental result and analysis\nThe training, validation and testing experiments were undertaken on the platform of Intel Core i7-9700k with \nWindows 10 64-bit operating system and NVIDIA GeForce GTX 1080Ti GPU. The models are built by deep \nlearning frameworks pytorch 1.9. In order to verify the effectiveness of RMT-Net, another four comparative \nmodels (ResNet-50, VGGNet-16, i-CapsNet 30 and MGMADS-3 8 are conducted on the declared platform and \nframework.\nPerformance metrics. In this paper, three indicators are used to evaluate the performance of the model. \nSpecificity (TNR)31, sensitivity (TPR)31 and accuracy (ACC)31 can be represented by Eq. (7).\nIn general, high specificity means a low rate of misdiagnosis, and high sensitivity means a low rate of missed \ndiagnosis. The higher the accuracy, the better the classification effect.\nTraining process visualization. In order to visually display the training process of RMT-Net model, the \nloss values of the first 100 epochs during training on X-ray and CT images were selected for visualization, and \nthe changes of Train_ acc value and Train_ loss value are shown in Fig. 4.\nIt can be seen that with the progress of training, the Train_ acc and Train_ loss curve drop rapidly, and the \nRMT-Net can achieve good training results in a short time and basically keep stable. At 100 epoch, the RMT-Net \nmodel has a Train_ acc value of 99.64% and a Train_ loss value of 0.0132 on the X-ray image dataset, 99.87% and \n0.0102 on the CT image dataset. The RMT-Net model achieves the best training results on both X-ray and CT \nimage datasets. Compared to the other models listed in Table2. The RMT-Net model achieves the best training \nresults on both X-ray and CT image datasets. The trend and amplitude of the curve are excellent, which verifies \nthe stability of the RMT-Net model.\nIt can be seen from Table 2 that in four-classification task of X-ray image, the Val_ loss of RMT-Net is 0.0126, \nwhich is lower than the other models. The Val_ acc value of RMT-Net is 98.84%, which is higher than the other \n(7)\nTNR = TN\nTN + FP\nTPR = TP\nTP + FN\nACC = TP + TN\nTP + TN + FP+ FN\nTable 1.  The dataset distribution before and after augmentation.\nGroup Category Before After\nFour classes (total: 25,100)\nNormal_X 3256 6450\nVirus_ X 3196 6280\nBacteria _ X 3122 6230\nCovid-19_ X 3089 6140\nBinary classes (total: 17,500)\nNormal_ CT 4250 8500\nCovid-19_ X 4050 9000\nFigure 4.  Train_loss and Train_acc curves of X-ray and CT images.\n7\nVol.:(0123456789)Scientific Reports |         (2023) 13:5359  | https://doi.org/10.1038/s41598-023-32462-2\nwww.nature.com/scientificreports/\nmodels. For binary classification task of CT image , the Val_ loss of RMT-Net is 0.0114 and the Val_ acc is 99.24%. \nBased on the above content, the RMT-Net model has higher accuracy than the other four models in both training \nand validation stages, and has a good recognition effect on X-ray and CT images.\nRMT‑Net performances tests. In addition to comparing the training and validation results of the model, \nthe evaluation indicators include the model size, specificity, sensitivity and detection accuracy. The comparative \nexperimental results are shown in Table 3.\nAs can be seen from Table 3, the model size of RMT-Net is about 40M, which is smaller than the other four \nmodels. In terms of model classification performance, the RMT-Net model has higher specificity, sensitivity and \naccuracy. In X-ray images, the accuracy of RMT-Net on the test set was 96.75%, and its specificity was improved \nby 1.02%, sensitivity by 5.24%, and accuracy by 4.51% compared with ResNet-50.On CT images, RMT-Net \nachieved 99.12% accuracy on the test set, with specificity improved by 3.2%, sensitivity improved by 3.28%, and \naccuracy improved by 3.87% compared to ResNet-50.\nInference speed. In order to verify whether the reasoning speed of the proposed RMT-Net meets the actual \nrequirement. We conducted a comparison experiment between the proposed RMT-Net and the other four mod-\nels, and the comparison results are shown in Table 3. The table shows on X-ray image data, the detection speed \nfor each image of ResNet-50, VGGNet-16, i-CapsNet, MGMADS-3 and RMT-Net models is 12.24 ms, 10.09 ms, \n8.58 ms, 6.06 ms and 5.46 ms. The detection speed of RMT-Net is clearly faster than the other networks. For \nexample, RMT-Net is 55.4% faster than ResNet-50, 45.9% faster than VGGNet-16, 36.4% faster than i-CapsNet \nand 9.9% faster than MGMADS-3 . On CT image data, the detection speed are 10.37 ms, 7.83 ms, 5.79 ms, 4.23 \nms and 4.12 ms. The detection speed of RMTNet is improved by 60.3% compared with ResNet, 47.4% compared \nwith VGGNet-16, 28.8% compared with i-CapsNet, and 2.6% compared with MGMADS-3.\nIn addition to the reduction in model size, we believe there are two other factors to improve the speed:(1) The \noverall structure of RMT-Net is different from that of classic transformer. We adopt pyramid structure, which \ncan greatly increase the computational efficiency of the algorithm by decreasing the spatial dimension step by \nstep. (2) In terms of micro-design, we adopt the lightweight self-attention structure, and adopt the depth-wise \nconvolution in the last stage of the network to further lightweight model. This is one of the reasons for the high \ncomputational efficiency of the algorithm.\nTable 2.  Comparation to the other four models. Bold value highlights the gain effect of our method in the \ntable.\nDatasets Methods Train_ loss Train_ acc (%) Val_ loss Val_ acc (%)\nFour classes (X-ray images)\nResNet-50 0.1987 98.56 0.2018 93.29\nVGGNet-16 0.2145 98.14 0.2453 93.05\ni-CapsNet 0.1584 98.86 0.1862 93.25\nMGMADS-3 0.0139 99.62 0.0140 96.25\nRMT-Net 0.0132 99.64 0.0126 98.84\nBinary classes (CT images)\nResNet-50 0.1454 99.01 0.1752 96.25\nVGGNet-16 0.1463 98.95 0.1568 93.75\ni-CapsNet 0.1285 98.98 0.1366 95.37\nMGMADS-3 0.0025 99.93 0.0136 98.09\nRMT-Net 0.0102 99.87 0.0114 99.24\nTable 3.  The test results of RMT-Net compared to the other four models. Bold value highlights the gain effect \nof our method in the table.\nDatasets Methods Size(M) Specificity (%) Sensitivity (%) Test _ acc (%) Speed (ms)\nFour classes (X-ray images)\nResNet-50 285 97.24 92.84 93.14 12.24\nVGGNet-16 146 93.54 92.25 92.62 10.09\ni-CapsNet 84 92.62 92.86 93.15 8.58\nMGMADS-3 43.6 98.06 96.60 96.75 6.09\nRMT-Net 40.8 98.26 98.08 97.65 5.46\nBinary classes (CT images)\nResNet-50 275 96.14 95.48 95.25 10.37\nVGGNet-16 154 96.45 94.16 94.38 7.83\ni-CapsNet 82 94.67 95.32 95.62 5.79\nMGMADS-3 43.6 98.17 98.05 98.25 4.23\nRMT-Net 38.5 99.34 98.76 99.12 4.12\n8\nVol:.(1234567890)Scientific Reports |         (2023) 13:5359  | https://doi.org/10.1038/s41598-023-32462-2\nwww.nature.com/scientificreports/\nFigure 5 shows the speed and accuracy of RMT-Net. It can be seen obviously that the detection speeds are \nimproved to a new level either on X-ray images or CT images. It is further verifed that the proposed model can \ndetect and classify COVID-19 faster.\nComparison to the related literatures. In order to verify the performance of RMT-Net, this paper com-\npares RMT-Net with other classification models, as shown in Table 4. The numbers in bracket of the third col-\numn represents 2, 3, and 4 categories.\nAs shown in Table 4, the RMT-Net proposed in this paper achieves better classification results than other \nmodels in both the four-classification of X-ray images and the second-classification of CT images. In X-ray image \nclassification, the accuracy rate of RMT-Net is 97.65 models.\nConclusion. In the paper, a new model named RMT-Net is proposed, which is based on ResNet-50 and \nTransformer. RMT-Net uses Transformers to capture long-distance dependencies, CNN to obtain local features, \nand depth-wise convolution to reduce the amount of computation and stage block structure to make the network \nmore scalable, enhance the receptive field and improve the transfer ability. Compared with other classification \nmodels, the RMT-Net model shows excellent performance in terms of classification accuracy, model size, and \ndetection speed. With the changes of COVID-19, people are facing great challenges on the unpredictable vari-\nations. The X-ray or CT images, or even NMR images can capture more details of the disease, which definitely \nwill enrich the dataset samples, therefore, adaptive network with higher accuracy and faster detection is worthy \nof further research.\nFigure 5.  Performance of models on validation set.\n9\nVol.:(0123456789)Scientific Reports |         (2023) 13:5359  | https://doi.org/10.1038/s41598-023-32462-2\nwww.nature.com/scientificreports/\nData availability\nThe datasets generated during and/or analysed during the current study are available from the corresponding \nauthor on reasonable request.\nCode availability\nAccession codes. The proposed RMT-Net backbone network is available publicly for open accessat RMT- Net \nsource.\nReceived: 27 September 2022; Accepted: 28 March 2023\nReferences\n 1. Gayathri, J., Abraham, B., Sujarani, M. & Nair, M. S. A computer-aided diagnosis system for the classification of covid-19 and \nnon-covid-19 pneumonia on chest x-ray images by integrating cnn with sparse autoencoder and feed forward neural network. \nComput. Biol. Med. 141, 105134 (2022).\n 2. Hussain, E. et al. Corodet: A deep learning based classification for covid-19 detection using chest x-ray images. Chaos Solitons \nFractals 142, 110495 (2021).\n 3. Xie, X. et al. Chest ct for typical 2019-ncov pneumonia: Relationship to negative rt-pcr testing. Radiology  296, E41–E45 (2020).\n 4. Bernheim, A. et al. Chest ct findings in coronavirus disease-19 (covid-19): Relationship to duration of infection. Radiology  295, \n685 (2020).\n 5. Hassan, H. et al. Review and classification of ai-enabled covid-19 ct imaging models based on computer vision tasks. Comput. Biol. \nMed. 141, 105123 (2022).\n 6. Wang, X. et al. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localiza-\ntion of common thorax diseases. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2097–2106 \n(2017).\n 7. Wang, L., Lin, Z. Q. & Wong, A. Covid-net: A tailored deep convolutional neural network design for detection of covid-19 cases \nfrom chest x-ray images. Sci. Rep. 10, 1–12 (2020).\n 8. Hong, G. et al. A multi-scale gated multi-head attention depthwise separable cnn model for recognizing covid-19. Sci. Rep. 11, \n1–13 (2021).\n 9. Song, Y . et al. Deep learning enables accurate diagnosis of novel coronavirus (covid-19) with ct images. IEEE/ACM Trans. Comput. \nBiol. Bioinf. 18, 2775–2780 (2021).\n 10. Oulefki, A., Agaian, S., Trongtirakul, T. & Laouar, A. K. Automatic covid-19 lung infected region segmentation and measurement \nusing ct-scans images. Pattern Recogn. 114, 107747 (2021).\n 11. Oulefki, A. et al. Virtual reality visualization for computerized covid-19 lesion segmentation and interpretation. Biomed. Signal \nProcess. Control 73, 103371 (2022).\n 12. Pathak, Y ., Shukla, P . K., Tiwari, A., Stalin, S. & Singh, S. Deep transfer learning based classification model for covid-19 disease. \nIrbm 43, 87–92 (2022).\nTable 4.  Comparisons with related literatures.\nLiteratures Methods Images (classes) Dataset Quant Test _acc (%)\nMukherjee32 Shallow CNN X-ray(2) 260 96.92\nAbbas33 DeTrac X-ray(2) 1764 95.12\nGupta34 COVID-WideNet X-ray(2) 13,942 91\nHemdan35 COVIDX-Net X-ray(2) 50 91\nApostolopoulos36 MobileNet v2(transfer learning) X-ray(2) 1419 87.02\nWu37 ASA-CoroNet X-ray(3) 994 97.59\nOzturk38 DarkCovidNet X-ray(3) 1442 96.78\nAslan39 Deep Learning &Machine Learning X-ray(3) 2905 96.29\nQuan40 DenseCapsNet X-ray(3) 750 90.7\nChen8 MGMADS-3 X-ray(4) 17,439 96.75\nWang7 COVID-Net X-ray(4) 13,975 93.3\nKhan41 CoroNet X-ray(4) 1300 89.6\nProposed RMT-Net X-ray(4)  25,100 97.65\nChen42 UNet++ CT(2) 35,355 98.85\nRahimzadeh43 Feature Pyramid Network CT(2) 63,849 98.49\nChen8 MGMADS-3 CT(2) 10,839 98.25\nYang44 Ednc CT(2) 2458 97.55\nSong9 DRE-Net CT(2) 1485 94.0\nSingh45 MODE-CNN CT(2) 150 93.25\nHeidarian46 COVID-FACT CT(2) 23,409 90.82\nWang27 DeCovNet CT(2) 630 90.1\nLi47 COVNet CT(2) 4356 90\nAmyar48 Encoder-Decoder with multi-layer perceptron CT(2) 1044 86\nProposed RMT-Net CT(2) 17,500 99.12\n10\nVol:.(1234567890)Scientific Reports |         (2023) 13:5359  | https://doi.org/10.1038/s41598-023-32462-2\nwww.nature.com/scientificreports/\n 13. Al Rahhal, M. M. et al. Covid-19 detection in ct/x-ray imagery using vision transformers. J. Personal. Med. 12, 310 (2022).\n 14. Chetoui, M. & Akhloufi, M. A. Explainable vision transformers and radiomics for covid-19 detection in chest x-rays. J. Clin. Med. \n11, 3013 (2022).\n 15. Y ang, H., Wang, L., Xu, Y . & Liu, X. Covidvit: A novel neural network with self-attention mechanism to detect covid-19 through \nx-ray images. Int. J. Mach. Learn. Cybern.https:// doi. org/ 10. 1007/ s13042- 022- 01676-7 (2022).\n 16. Y aqoob, A., Basit, A., Rahman, A., Hannan, A. & Ullah, K. Detection of covid-19 in high resolution computed tomography using \nvision transformer. In 2022 International Conference on Frontiers of Information Technology (FIT), 82–87 (2022).\n 17. Okolo, G. I., Katsigiannis, S. & Ramzan, N. Ievit: An enhanced vision transformer architecture for chest x-ray image classification. \nComput. Methods Programs Biomed. 226, 107141 (2022).\n 18. Cai, Y . et al. Multi-medvit: A deep learning approach for the diagnosis of covid-19 with the ct images. In 2022 IEEE International \nConference on Bioinformatics and Biomedicine (BIBM), 2247–2252 (2022).\n 19. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. Syst. 30 (2017).\n 20. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on \nLearning Representations (2021).\n 21. Wu, B. et al. Visual transformers: Where do transformers really belong in vision models? In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, 599–609 (2021).\n 22. Guo, J. et al. Cmt: Convolutional neural networks meet vision transformers. In Proceedings of the IEEE/CVF Conference on Computer \nVision and Pattern Recognition, 12175–12185 (2022).\n 23. Cohen, J. P ., Morrison, P . & Dao, L. Covid-19 image data collection. arXiv: 2003. 11597 (2020).\n 24. Kermany, D. et al. Labeled optical coherence tomography (oct) and chest x-ray images for classification. Mendeley Data  2, 651 \n(2018).\n 25. Sosososo. Heywhale network. https:// www. kesci. com/ mw/ datas et/ 5e746 ec998 d4a80 02d2b 0861 (2020).\n 26. Y ang, X. et al. Covid-ct-dataset: A ct scan dataset about covid-19. arXiv preprint arXiv:  2003. 13865 (2020).\n 27. Wang, X. et al. A weakly-supervised framework for covid-19 classification and lesion localization from chest ct. IEEE Trans. Med. \nImaging 39, 2615–2625 (2020).\n 28. Apostolopoulos, I. D., Aznaouridis, S. I. & Tzani, M. A. Extracting possibly representative covid-19 biomarkers from x-ray images \nwith deep learning approach and image data related to pulmonary diseases. J. Med. Biol. Eng. 40, 462–469 (2020).\n 29. Farooq, M. & Hafeez, A. Covid-resnet: A deep learning framework for screening of covid19 from radiographs. arXiv preprint \narXiv: 2003. 14395 (2020).\n 30. Chen, X. et al. Research on cervical cancer image recognition method based on i-capsnet. Chin. J. Sens. Actuators 33, 1752–1758 \n(2020).\n 31. Gu, Q., Zhu, L. & Cai, Z. Evaluation measures of the classification performance of imbalanced data sets. In International Symposium \non Intelligence Computation and Applications, 461–471 (Springer, 2009).\n 32. Mukherjee, H. et al. Shallow convolutional neural network for covid-19 outbreak screening using chest x-rays. Cognit. \nComput.https:// doi. org/ 10. 1007/ s12559- 020- 09775-9 (2021).\n 33. Abbas, A., Abdelsamea, M. M. & Gaber, M. M. Classification of covid-19 in chest x-ray images using detrac deep convolutional \nneural network. Appl. Intell. 51, 854–864 (2021).\n 34. Gupta, P . et al. Covid-widenet-a capsule network for covid-19 detection. Appl. Soft Comput. 122, 108780 (2022).\n 35. Hemdan, E. E.-D., Shouman, M. A. & Karar, M. E. Covidx-net: A framework of deep learning classifiers to diagnose covid-19 in \nx-ray images. arXiv preprint arXiv: 2003. 11055 (2020).\n 36. Apostolopoulos, I. D. & Mpesiana, T. A. Covid-19: Automatic detection from x-ray images utilizing transfer learning with convo-\nlutional neural networks. Phys. Eng. Sci. Med. 43, 635–640 (2020).\n 37. Wu, F ., Yuan, J., Li, Y ., Li, J. & Y e, M. Asa-coronet: Adaptive self-attention network for covid-19 automated diagnosis using chest \nx-ray images. In Workshop on Healthcare AI and COVID-19, 11–20 (PMLR, 2022).\n 38. Ozturk, T. et al. Automated detection of covid-19 cases using deep neural networks with x-ray images. Comput. Biol. Med. 121, \n103792 (2020).\n 39. Aslan, M. F ., Sabanci, K., Durdu, A. & Unlersen, M. F . Covid-19 diagnosis using state-of-the-art cnn architecture features and \nbayesian optimization. Comput. Biol. Med. 142, 105244 (2022).\n 40. Quan, H. et al. Densecapsnet: Detection of covid-19 from x-ray images using a capsule neural network. Comput. Biol. Med. 133, \n104399 (2021).\n 41. Khan, A. I., Shah, J. L. & Bhat, M. M. Coronet: A deep neural network for detection and diagnosis of covid-19 from chest x-ray \nimages. Comput. Methods Progr. Biomed. 196, 105581 (2020).\n 42. Chen, J. et al. Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomog-\nraphy. Sci. Rep. 10, 1–11 (2020).\n 43. Rahimzadeh, M., Attar, A. & Sakhaei, S. M. A fully automated deep learning-based network for detecting covid-19 from a new \nand large lung ct scan dataset. Biomed. Signal Process. Control 68, 102588 (2021).\n 44. Y ang, L., Wang, S.-H. & Zhang, Y .-D. Ednc: Ensemble deep neural network for covid-19 recognition. Tomography 8, 869–890 \n(2022).\n 45. Singh, D. et al. Classification of covid-19 patients from chest ct images using multi-objective differential evolution-based convo -\nlutional neural networks. Eur. J. Clin. Microbiol. Infect. Dis. 39, 1379–1389 (2020).\n 46. Heidarian, S. et al. Covid-fact: A fully-automated capsule network-based framework for identification of covid-19 cases from chest \nct scans. Front. Artif. Intell. 4, 598932 (2021).\n 47. Li, L. et al. Using artificial intelligence to detect covid-19 and community-acquired pneumonia based on pulmonary ct: Evaluation \nof the diagnostic accuracy. Radiology 296, E65–E71 (2020).\n 48. Amyar, A., Modzelewski, R., Li, H. & Ruan, S. Multi-task deep learning based ct imaging analysis for covid-19 pneumonia: Clas-\nsification and segmentation. Comput. Biol. Med. 126, 104037 (2020).\nAcknowledgements\nThis work was supported by The National Natural Science Foundation of China under the Grant Number \n61903724, the Natural Science Foundation of Tianjin under Grant Number 18YFZCGX00360 and the Tianjin \nResearch Innovation Project for Postgraduate Students under Grant No. KYS202108.\nAuthor contributions\nK.R., X.C and Z.W wrote the main manuscript text. G.H. supplemented the experiments needed in the paper. \nAll authors reviewed the manuscript and contributed equally.\nCompeting interests \nThe authors declare no competing interests.\n11\nVol.:(0123456789)Scientific Reports |         (2023) 13:5359  | https://doi.org/10.1038/s41598-023-32462-2\nwww.nature.com/scientificreports/\nAdditional information\nCorrespondence and requests for materials should be addressed to X.C.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023"
}