{
  "title": "TransformerFusion: Monocular RGB Scene Reconstruction using Transformers",
  "url": "https://openalex.org/W3181700833",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222252355",
      "name": "Božič, Aljaž",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227243619",
      "name": "Palafox, Pablo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3092207269",
      "name": "Thies, Justus",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3157528917",
      "name": "Dai, Angela",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4214023352",
      "name": "Nießner, Matthias",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2624503621",
    "https://openalex.org/W3107457716",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3105612746",
    "https://openalex.org/W2950493473",
    "https://openalex.org/W3172610252",
    "https://openalex.org/W2964254721",
    "https://openalex.org/W2519683295",
    "https://openalex.org/W2964009301",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3165610079",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W2950557037",
    "https://openalex.org/W2229412420",
    "https://openalex.org/W2594519801",
    "https://openalex.org/W3012728190",
    "https://openalex.org/W2963911235",
    "https://openalex.org/W3155473636",
    "https://openalex.org/W3117476483",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3035507572",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1964057156",
    "https://openalex.org/W2336961836",
    "https://openalex.org/W2032871891",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2795014656",
    "https://openalex.org/W3107527198",
    "https://openalex.org/W3109929860",
    "https://openalex.org/W3102132650",
    "https://openalex.org/W3034961469",
    "https://openalex.org/W3109428934",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2988715931",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2904332125",
    "https://openalex.org/W2983954598",
    "https://openalex.org/W3034626012",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2009422376",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W2769312834"
  ],
  "abstract": "We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach. From an input monocular RGB video, the video frames are processed by a transformer network that fuses the observations into a volumetric feature grid representing the scene; this feature grid is then decoded into an implicit 3D scene representation. Key to our approach is the transformer architecture that enables the network to learn to attend to the most relevant image frames for each 3D location in the scene, supervised only by the scene reconstruction task. Features are fused in a coarse-to-fine fashion, storing fine-level features only where needed, requiring lower memory storage and enabling fusion at interactive rates. The feature grid is then decoded to a higher-resolution scene reconstruction, using an MLP-based surface occupancy prediction from interpolated coarse-to-fine 3D features. Our approach results in an accurate surface reconstruction, outperforming state-of-the-art multi-view stereo depth estimation methods, fully-convolutional 3D reconstruction approaches, and approaches using LSTM- or GRU-based recurrent networks for video sequence fusion.",
  "full_text": "TransformerFusion: Monocular RGB Scene\nReconstruction using Transformers\nAljaž Božiˇc 1 Pablo Palafox 1 Justus Thies 1,2 Angela Dai 1 Matthias Nießner 1\n1Technical University of Munich\n2Max Planck Institute for Intelligent Systems, Tübingen, Germany\naljazbozic.github.io/transformerfusion\nAbstract\nWe introduce TransformerFusion, a transformer-based 3D scene reconstruction\napproach. From an input monocular RGB video, the video frames are processed\nby a transformer network that fuses the observations into a volumetric feature\ngrid representing the scene; this feature grid is then decoded into an implicit 3D\nscene representation. Key to our approach is the transformer architecture that\nenables the network to learn to attend to the most relevant image frames for each\n3D location in the scene, supervised only by the scene reconstruction task. Features\nare fused in a coarse-to-ﬁne fashion, storing ﬁne-level features only where needed,\nrequiring lower memory storage and enabling fusion at interactive rates. The\nfeature grid is then decoded to a higher-resolution scene reconstruction, using\nan MLP-based surface occupancy prediction from interpolated coarse-to-ﬁne 3D\nfeatures. Our approach results in an accurate surface reconstruction, outperforming\nstate-of-the-art multi-view stereo depth estimation methods, fully-convolutional 3D\nreconstruction approaches, and approaches using LSTM- or GRU-based recurrent\nnetworks for video sequence fusion.\n1 Introduction\nMonocular 3D reconstruction is a core task in 3D computer vision, aiming to reconstruct a complete\nand accurate 3D geometry of an object or an environment from only 2D observations captured by an\nRGB camera. A geometric understanding is key to applications such as robotic or autonomous vehicle\nnavigation or interaction, as well as model creation and scene editing for augmented and virtual reality.\nIn addition, geometric scene reconstructions form the basis for 3D scene understanding, supporting\ntasks such as 3D object detection, semantic, and instance segmentation [34, 35, 36, 29, 7, 43, 15, 16].\nWhile state-of-the-art SLAM systems [ 3, 41] achieve robust and scale-accurate camera tracking\nleveraging both visual and inertial measurements, dense and complete 3D reconstruction of large-\nscale environments from monocular video remains a very challenging problem – particularly for\ninteractive settings. Simultaneously, notable progress has been made on multi-view depth estimation,\nestimating depth from pairs of images by averaging features extracted from the images in a feature\ncost volume [42, 17, 19, 38, 13]. Unfortunately, averaging features across a full video sequence can\nlead to equal-weight treatment of each individual frame, despite some frames possibly containing\nless information in various regions (e.g., from motion blur, rolling shutter artifacts, very glancing or\npartial views of objects), making high-ﬁdelity scene reconstruction challenging.\nInspired by the recent advances in natural language processing (NLP) that leverage transformer-based\nmodels for sequence to sequence modelling [40, 11, 2], we propose a transformer-based method that\nfuses a sequence of RGB input frames into a 3D representation of a scene at interactive rates. Key to\nPreprint. Under review.\narXiv:2107.02191v1  [cs.CV]  5 Jul 2021\nFigure 1: TransformerFusion is an online scene reconstruction method that takes a monocular RGB\nvideo as input. The features extracted from each observed image are fused incrementally with a\ntransformer architecture. This fusion approach learns to attend to the most relevant image frames for\neach 3D location (see view attention color maps of the most relevant frame) achieving state-of-the-art\nreconstruction results.\nour approach is a learned feature fusion of the video frames using a transformer-based architecture,\nwhich learns to attend to the most informative image features to reconstruct a local 3D region of\nthe scene. A new observed RGB frame is encoded into a 2D feature map, and unprojected into a\n3D volume, where our transformer learns a fused 3D feature for each location in the 3D volume\nfrom the image view features. This enables extraction of the most informative view features for\neach location in the 3D scene. The 3D features are fused in coarse-to-ﬁne fashion, providing both\nimproved reconstruction performance as well as interactive runtime. These features are then decoded\ninto high-resolution scene geometry with an MLP-based surface occupancy prediction.\nIn summary, our main contributions to achieve robust and accurate scene reconstructions are:\n• Learned multi-view feature fusion in the temporal domain using a transformer network that\nattends to only the most informative features of the image views for reconstructing each\nlocation in a scene.\n• A coarse-to-ﬁne hierarchy of our transformer-based feature fusion that enables an online\nreconstruction approach running at interactive frame-rates.\n2 Related Work\nMulti-view depth estimation. Estimating depth from multi-view image observations has been\nlong-studied in computer vision. COLMAP [37] introduced a patch matching based approach which\nachieves impressive accuracy and remains established as one of the most popular methods for multi-\nview stereo. While COLMAP offers robust depth estimation for distinctive features in images,\nthe patch matching struggles to densely reconstruct areas without many distinctive color features,\nsuch as ﬂoor and walls. Recently, learning-based approaches that build data-driven priors from\nlarge-scale datasets have improved depth estimation in these challenging scenarios. Some proposed\nmethods rely only on a 2D network with multiple images concatenated as input [42]. Several recent\napproaches instead build a shared 3D feature cost volume in reference camera space using feature\naveraging [13, 17, 19, 25, 26]. These approaches estimate the reference frame’s depth within a\nlocal window of frames, but some also propagate information from previously estimated depth maps\nby using probabilistic ﬁltering [ 25], a Gaussian process [ 17], or an LSTM bottleneck layer [ 13].\nSuch multi-view depth estimation approaches predict single-view depth maps, which must be fused\ntogether to construct a geometric 3D representation of the observed scene.\n3D reconstruction from monocular RGB input. Multi-view depth estimation approaches can\nbe combined with depth fusion approaches, such as volumetric fusion [ 6], to obtain a volumetric\n2\nreconstruction of the observed scene. MonoFusion [ 33] is one of the ﬁrst methods using depth\nestimate from a real-time variant of PatchMatch stereo [1]. However, fusing noisy depth estimates\ncauses artifacts in the 3D reconstruction, which lead to the development of recent approaches that\ndirectly predict the 3D surface reconstruction instead of per-frame depth estimates. One of the\nﬁrst approaches to predict 3D surface occupancy from two input RGB images is SurfaceNet [ 20],\nwhich converts volumetrically averaged colors into 3D surface occupancies using a 3D convolutional\nnetwork. Atlas [ 28] extends this approach to a multi-view setting, while also leveraging learned\nfeatures instead of colors. Recently, NeuralRecon [ 39] proposed a real-time 3D reconstruction\nframework, adding GRU units distributed in 3D to fuse reconstructions from different local windows\nof frames. Our approach also fuses together learned features from RGB frame input in an online\nfashion, but our transformer-based multi-view feature fusion enables relying only on the most\ninformative features from the observed frames for a particular spatial location in the reconstructed\nscene, producing more accurate 3D reconstructions.\nTransformers in computer vision. The transformer architecture [40] has achieved profound im-\npact in many computer vision tasks in addition to its natural language processing origins. For a\ndetailed survey, we refer the reader to [22]. In computer vision, transformers have been leveraged\nsuccessfully for tasks such as object detection [4], video classiﬁcation [44], image classiﬁcation [12],\nimage generation [30], and human reconstruction [45]. In this work, we propose transformer-based\nfeature fusion for 3D scene reconstruction from a monocular video. Given a sequence of observed\nRGB frames, our approach learns to attend to the most informative features from each image to\npredict a dense occupancy ﬁeld.\n3 End-to-end 3D Reconstruction using Transformers\nGiven a set of N RGB images Ii ∈RW×H×3 of a scene with corresponding camera intrinsic\nparameters Ki ∈R3×3 and extrinsic poses Pi ∈R4×4, our method reconstructs the scene geometry\nby predicting occupancy values o∈[0,1] for every 3D point in the scene. Fig. 2 shows an overview\nof our approach. Each input image Ii is processed by a 2D convolutional encoderΘ, extracting coarse\nand ﬁne image features (Φc\ni and Φf\ni , respectively):\nΘ : Ii ∈RW×H×3 ↦→(Φc\ni ,Φf\ni )\nFrom these 2D image features, we construct a 3D feature grid in world space. To this end, we\nregularly sample grid points in 3D at a coarse resolution of every vc = 30 cm and a ﬁne resolution of\nvf = 10 cm. For these coarse and ﬁne sample points, we query corresponding 2D features in all N\nimages and predict fused coarse ψc and ﬁne 3D features ψf using transformer networks [40]:\nTc : (Φc\n1,..., Φc\nN ) ↦→(ψc,wc)\nTf : (Φf\n1 ,..., Φf\nN ) ↦→(ψf ,wf )\nNote that we also store the intermediate attention weights wc and wf of the ﬁrst transformer layers\nfor efﬁcient view selection, which is explained in Sec. 3.4.\nTo further improve the features in the 3D spatial domain, we apply 3D convolutional networksCc and\nCf , at the coarse and ﬁne level, respectively:\nCc : {ψc}C×C×C ↦→{˜ψc}C×C×C\nCf : {( ˜ψc,ψf )}F×F×F ↦→{˜ψf }F×F×F\nFinally, to predict the scene geometry occupancy for a point p ∈R3, the coarse ˜ψc and ﬁne features\n˜ψf are trilinearly interpolated and a multi-layer perceptron Smaps these features to occupancies:\nS: ( ˜ψc, ˜ψf ) ↦→o∈[0,1]\nThis extraction of surface occupancies is inspired by convolutional occupancy networks [ 32] and\nIFNets [5]. From this occupancy ﬁeld we extract a surface mesh with Marching cubes [27]. Note that\nin addition to surface occupancy, we also predict occupancy masks for near-surface locations at the\ncoarse and ﬁne levels. These masks are used for coarse-to-ﬁne surface ﬁltering (see Sec. 3.2), which\n3\nimproves reconstruction performance with a focus on the surface geometry prediction and enables\ninteractive runtime.\nWe train our approach in end-to-end fashion by supervising the surface occupancy predictions using\nthe following loss:\nL= Lc + Lf + Lo,\nwhere Lc and Lf denote binary cross-entropy (BCE) losses on occupancy mask predictions for\nnear-surface locations at the coarse and ﬁne levels, respectively (see Sec. 3.2), and Lo denotes a BCE\nloss for surface occupancy prediction (see Sec. 3.3).\nFigure 2: Method overview: given multiple input images, we compute coarse and ﬁne level features.\nUsing a transformer architecture, we separately fuse these coarse and ﬁne features in a voxel grid. To\nimprove the spatial features, we use a reﬁnement network for both the coarse and the ﬁne features.\nFrom these feature grids, we extract an occupancy ﬁeld using a lightweight MLP.\n3.1 Learning Temporal Feature Fusion via Transformers\nFor a spatial location p ∈R3 in the scene reconstruction, we learn to fuse coarse ψc and ﬁne level\nfeatures ψf from the N coarse and ﬁne feature images (Φc\ni and Φf\ni , respectively), which are extracted\nby the 2D encoder Θ. Speciﬁcally, we train two instances of a transformer model, one for fusing\ncoarse-level features ψc and one for fusing ﬁne-level features ψf . Both transformers Tc and Tf share\nthe same architecture. Thus, for simplicity, we omit the coarse and ﬁne notation in the following.\nOur transformer model T is independently applied to each sample point in world space. For a point\np, the transformer network takes a series of 2D features φi as input that are bilinearly sampled from\nthe feature maps Φi at the corresponding projective image location. The projective image location\nis computed via a full-perspective projection Πi(p) = π(Ki(Rip + ti)), assuming known camera\nintrinsics Ki and extrinsics Pi = (Ri,ti). To inform the transformer about invalid features (i.e.,\na sample point is projected outside an image), we also provide the pixel validity vi ∈{0,1}as\ninput. In addition to these 2D features φi, we concatenate the projected depth di = (Rip + ti)z,\nand the viewing ray ri = (p −ci)/||p −ci||2 to the input (ci ∈R3 denoting the camera center of\nview i). These input features are converted to an embedding vector θi ∈RD using a linear layer\nθi = FCN(φi,di,vi,ri), before feeding it into the transformer network that then predicts a fused\nfeature ψ∈RD:\nT : (θ1,...,θ N ) ↦→(ψ,w)\nAs described above, wdenotes the attention values of the initial attention layer, which are used for\nview selection to speed-up fusion (see Sec. 3.4).\nTransformer architecture. We followed [ 12] when designing the transformer architecture T.\nIt consists of 8 modules of feed-forward and attention layers, using multi-head attention with 4\nattention heads and embedding dimension D= 256. Feed-forward layers process the temporal inputs\nindependently, and contain ReLU activation, linear layers with residual connection, and layer norm.\n4\nThe model returns both fused feature ψ∈RD and attention weights w∈RN over all temporal inputs\nfrom the initial attention layer that are later used for selecting which views to maintain over longer\nsequences of input image views.\n3.2 Spatial Feature Reﬁnement\nWhile the transformer network fuses 2D observations in the temporal domain, we additionally imbue\nexplicit spatial reasoning by applying a 3D CNN to spatially reﬁne the fused features {ψc}C×C×C\nand {ψf }F×F×F that are computed by the transformers Tc and Tf on the coarse and ﬁne grid,\nrespectively. The coarse features {ψc}C×C×C are reﬁned by a 3D CNN Cc consisting of 3 residual\nblocks that maintain the same spatial resolution and produce reﬁned features {˜ψc}C×C×C. These\nfeatures are upsampled to a ﬁne grid resolution using nearest-neighbor upsampling, and concatenated\nwith fused features at ﬁne level {ψf }F×F×F . A ﬁne-level 3D CNN Cf is then applied to the\nconcatenated features, resulting in reﬁned ﬁne features {˜ψf }F×F×F . Both, coarse ˜ψc and ﬁne\nfeatures ˜ψf are used for surface occupancy prediction.\nCoarse-to-ﬁne surface ﬁltering. The reﬁned features are also used to predict occupancy masks\nfor near-surface locations at both coarse and ﬁne levels, thus, ﬁltering out free-space regions and\nsparsifying the volume, such that the higher-resolution and computationally expensive ﬁne-scale\nsurface extraction is performed only in regions close to the surface. To achieve this, additional\n3D CNN layers Mc and Mf are applied to the reﬁned features, outputting a near-surface mask\nmc,mf ∈[0,1] for every grid point:\nMc : {˜ψc}C×C×C ↦→{mc}C×C×C\nMf : {˜ψf }F×F×F ↦→{mf }F×F×F\nOnly spatial regions where bothmc and mf are larger than 0.5, i.e., close to the surface, are processed\nfurther to compute the ﬁnal surface reconstruction; other regions are determined to be free space. This\nimproves the overall reconstruction performance by focusing the capacity of the surface prediction\nnetwork to close-to-the-surface regions and enables a signiﬁcant runtime speed-up.\nIntermediate supervision of near-surface masks mc and mf is employed using masks mc\ngt and mf\ngt\ngenerated from the ground truth scene reconstruction, denoting the grid point as near-surface if there\nexists ground truth surface in the radius of vc or vf from the point. Binary cross entropy losses\nLc = BCE(mc,mc\ngt) and Lf = BCE(mf ,mf\ngt) are applied.\n3.3 Surface Occupancy Prediction\nThe ﬁnal surface reconstruction is predicted by decoding the coarse and ﬁne feature grids to occupancy\nvalues o ∈[0,1], with values o ≥0.5 representing occupied points and values o <0.5 represent-\ning free-space points. For a point p ∈R3, we compute its feature representation by trilinearly\ninterpolating coarse and ﬁne grid features:\nψc\np = Trilinear(p,{˜ψc}C×C×C)\nψf\np = Trilinear(p,{˜ψf }F×F×F )\nWe concatenate the interpolated features and predict the point’s occupancy aso= S(ψc\np,ψf\np), where\nSis a multi-layer perceptron (MLP) with 3 modules of feed-forward layers, containing ReLU\nactivation, linear layer with residual connection, and layer norm.\nSurface occupancy supervision. We train on 1.5 ×1.5 ×1.5 m volumetric chunks of scenes for\ntraining efﬁciency. To supervise the surface occupancy loss, 1k points are sampled inside the chunk,\nwith 80% of samples drawn from a truncation region at most 10 cm from the surface, and 20%\nsampled uniformly inside the chunk. Ground truth occupancy values ogt are computed using the\nScanNet RGB-D reconstructions [8]. For uniform samples it is straightforward to generate unoccupied\npoint samples by sampling points in free space in front of the visible surface, but it is unknown\nwhether a point sample is occupied when it lies behind seen surfaces. In order to prevent artifacts\nbehind walls, we follow the data processing applied in [28] and additionally label point samples as\noccupied, if they are sampled in areas where an entire vertical column of voxels is occluded in the\nscene. A binary cross entropy loss Lo = BCE(o,ogt) is then applied to the occupancy predictions o.\n5\n3.4 View Selection for Online Scene Reconstruction\nWe aim to consider all N frames as input to our transformer for each 3D location in a scene; however,\nthis becomes extremely computationally expensive with long videos or large-scale scenes, which\nprohibits online scene reconstruction. Instead, we proceed with the reconstruction incrementally, pro-\ncessing every video frame one-by-one, while keeping only a small number K = 16 of measurements\nfor every 3D point. We visualize this online approach in Fig. 1.\nDuring training, for efﬁciency, we use only Kt random images for each training volume. At test\ntime, we leverage the attention weights wc and wf of the initial transformer layers to determine\nwhich views to keep in the set of Kmeasurements. Speciﬁcally, for a new RGB frame, we extract its\n2D features, and run feature fusion for every coarse and ﬁne grid point inside the camera frustum.\nThis returns the fused feature and also the attention weights over all currently accumulated input\nmeasurements. Whenever the maximum number of Kmeasurements is reached, a selection is made\nby dropping out a measurement with lowest attention weight before adding new measurements in the\nlatest frame. This guarantees a low number of input measurements, speeding up fusion processing\ntimes considerably. Furthermore, by using coarse-to-ﬁne ﬁltering, described in Sec. 3.2, we can\nfurther accelerate fusion by only considering higher resolution points in the area near the estimated\nsurface. Together with incremental processing that results in high performance beneﬁts, our approach\nperforms per-frame feature fusion at about 7 FPS despite an unoptimized implementation.\n3.5 Training Scheme\nOur approach has been implemented using the PyTorch library [31]. The architecture details of the\nused networks are speciﬁed in the supplemental document. To train our approach we use ScanNet\ndataset [8], an RGB-D dataset of indoor apartments. We follow the established train-val-test split.\nFor training, we randomly sample 1.5 ×1.5 ×1.5 m volume chunks of the train scenes, sampling less\nchunks in free space and more samples in areas with non-structural objects, i.e. not only consisting of\nﬂoor or walls. This results in ≈165k training chunks. For each chunk, we randomly sample Kt = 8\nRGB images among all frames that include the chunk in their camera frustums.\nThe 2D convolutional encoder Θ for image feature extraction is implemented as a ResNet-18 [14]\nnetwork, pre-trained on ImageNet [24]. During training, a batch size of 4 chunks is used with an\nAdam [23] optimizer with β1 = 0.9, β2 = 0.999, ϵ= 10−8 and weight regularization of 10−4. We\nuse a learning rate of 10−4 with 5k warm-up steps at initialization, and square root learning rate\ndecay afterwards. When computing the losses of coarse and ﬁne surface ﬁltering predictions, a higher\nweight of 2.0 is applied to near-surface voxels, to increase recall and improve overall robustness.\nTraining takes about 30 hours using an Intel Xeon 6242R Processor and an Nvidia RTX 3090 GPU.\n4 Experiments\nMetrics. To evaluate our monocular scene reconstruction, we use several measures of reconstruction\nperformance. We evaluate geometric accuracy and completion, with accuracy measuring the average\npoint-to-point error from predicted to ground truth vertices, completion measuring the error in the\nopposite direction, and chamfer as the average of accuracy and completion. To account for possibly\ndifferent mesh resolutions among methods, we uniformly sample200k points over mesh faces of every\nreconstructed mesh. Additionally, we threshold these point-to-point errors and compute precision\nand recall by computing the ratio of point-to-point matches within distance ≤5cm. Since it is easy to\nmaximize either precision (by predicting only a few but accurate points) or recall (by over-completing\nreconstructions with noisy surface), we found the most reliable metric to be F-score, determined by\nboth precision and recall.\nOur ground truth reconstructions are obtained by automated 3D reconstruction [ 9] from RGB-D\nvideos of real-world environments and, thus, they are often incomplete due to unobserved and\noccluded regions in the scene. To avoid penalizing methods for reconstructing a more complete scene\nw.r.t. the available ground truth, we apply an additional occlusion mask at evaluation.\nAs most state of the art, particularly for depth estimation, rely on a pre-sampled set of keyframes\n(based on sufﬁcient translation or rotation difference between camera poses), we evaluate all ap-\nproaches based on sequences of sampled keyframes, using the keyframe selection of [13].\n6\nTable 1: Quantitative comparison with baselines and ablations on test set of Scannet dataset [8].\nMethod Acc ↓ Compl ↓ Chamfer ↓ Prec ↑ Recall ↑ F-score ↑\nRevisitingSI [18] 14.29 16 .19 15 .24 0 .346 0 .293 0 .314\nMVDepthNet [42] 12.94 8 .34 10 .64 0 .443 0 .487 0 .460\nGPMVS [17] 12.90 8 .02 10 .46 0 .453 0 .510 0 .477\nESTDepth [26] 12.71 7 .54 10 .12 0 .456 0 .542 0 .491\nDPSNet [19] 11.94 7 .58 9 .77 0 .474 0 .519 0 .492\nDELTAS [38] 11.95 7 .46 9 .71 0 .478 0 .533 0 .501\nDeepVideoMVS [13] 10.68 6.90 8.79 0 .541 0 .592 0 .563\nCOLMAP [37] 10.22 11 .88 11 .05 0 .509 0 .474 0 .489\nNeuralRecon [39] 5.09 9.13 7 .11 0 .630 0.612 0.619\nAtlas [18] 7.16 7 .61 7 .38 0 .675 0 .605 0 .636\nOurs: w/o TRSF, avg 7.23 9 .74 8 .48 0 .635 0 .501 0 .557\nOurs: w/o TRSF, pred 6.11 11 .12 8 .61 0 .686 0 .512 0 .583\nOurs: w/o spatial ref. 10.46 16 .91 13 .68 0 .479 0 .295 0 .361\nOurs: 4 images, RND 8.01 10 .28 9 .15 0 .587 0 .445 0 .502\nOurs: 4 images 6.80 8 .40 7 .60 0 .661 0 .524 0 .581\nOurs: 8 images, RND 6.74 8 .55 7 .64 0 .665 0 .544 0 .596\nOurs: 8 images 6.17 7 .69 6 .93 0 .704 0 .584 0 .636\nOurs: 16 images, RND 5.80 8 .56 7 .18 0 .711 0 .584 0 .638\nOurs: w/o C2F ﬁlter 6.57 7 .69 7 .13 0 .678 0 .592 0 .631\nOurs 5.52 8 .27 6.89 0 .728 0.600 0.655\n4.1 Comparison with State of the Art\nIn Tab. 2, we compare our approach with state-of-the-art methods. All methods are trained on the\nScanNet dataset [8], using the ofﬁcial train/val/test split. We use the pre-trained models provided by\nthe authors for MVDepthNet [42], GPMVS [17] and DPSNet [19] which are ﬁne-tuned on ScanNet.\nFor baselines that predict depth in a reference camera frame instead of directly reconstructing 3D\nsurface, a volumetric fusion method [6] is used to fuse different depth maps into a 3D truncated signed\ndistance ﬁeld. The single-view depth prediction method RevisitingSI [ 18] suffers from the more\nchallenging task formulation without the use of multiple views, leading to noisier depth predictions\nand inconsistencies between frames. Multi-view depth estimation methods leverage the additional\nview information for improved performance, with the LSTM-based approach of DeepVideoMVS [13]\nachieving the best performance among these approaches. Reconstruction quality further improves\nwith methods that directly predict the 3D surface geometry, such as NeuralRecon [39] and Atlas [28].\nOur transformer-based feature fusion approach enables more robust reconstruction and outperforms\nall existing methods in both chamfer distance and F-score. The performance improvement can also\nbe clearly seen in the qualitative comparisons in Fig. 3.\n4.2 Ablations\nTo demonstrate the effectiveness of our design choices, we conducted a quantitative ablation study\nwhich is shown in Tab. 2 and discussed in the following.\nWhat is the impact of learning to fuse features from different views with transformers? We\nevaluate the effect of our learned feature fusion by replacing the transformer blocks with a multi-layer\nperceptron (MLP) that processes input image observations independently. The per-view outputs of this\nMLP are fused using an average (w/o TRSF , avg) or using a weighted average with weights predicted\nby the MLP (w/o TRSF , pred). We ﬁnd that our transformer-based view fusion effectively learns\nto attend to the most informative views for a speciﬁc location, resulting in signiﬁcantly improved\nperformance over these averaging-based feature fusion alternatives.\nDoes spatial feature reﬁnement help reconstruction performance? Spatial feature reﬁnement is\nindeed very important for reconstruction quality. It enables the model to aggregate feature information\n7\nin spatial domain and produce more spatially consistent and complete reconstructions, without it (w/o\nspatial ref.) the geometry completion (and recall metric) are considerably worse.\nHow important is coarse-to-ﬁne ﬁltering? Predicting the coarse and ﬁne near-surface masks\nprovides an additional performance improvement compared to the model without it (w/o C2F ﬁlter),\nas it allows more focus on surface geometry. Furthermore, this enables a speed-up of the fusion\nruntime by a factor of approximately 3.5, resulting in processing times of 7 FPS (instead of 2 FPS).\nHow many views should be used for feature fusion? In our experiments, we use a limited number\nof K = 16 frame observations to inform the feature for every 3D grid location. We ﬁnd that these\nviews all contribute, with performance degrading somewhat with sparser sets of observations (K = 8\nor K = 4). The number of frames is limited because of execution time and memory consumption for\nbigger scenes.\nHow effective is frame selection using attention weights? The Kframes for each 3D grid feature\nare selected based on the computed attention weights and are updated during scanning. To evaluate\nthis frame selection, we compare against a frame selection scheme that randomly selects frames that\nobserve the 3D location (RND), which results in a noticeable drop in performance for both chamfer\nand F-score. The performance difference is even larger when using less views for fusion ( K = 8\nor K = 4), where view selection becomes even more important. In Fig. 1, we visualize the most\nimportant view for locations in the scene, selected by the highest attention weight. Relatively smooth\ntransitions between selected views among neighboring 3D locations suggest that view selection is\nspatially consistent. To illustrate the frame selection, we also visualize all selected frames with\ncorresponding attention weights for speciﬁc 3D locations in the supplemental document.\n4.3 Limitations\nUnder severe occlusions and partial observation of the scene, our method can struggle to reconstruct\ndetails of certain objects, such as chair legs, monitor stands, or books on the shelves. Furthermore,\ntransparent objects, such as glass windows without frames, are often inaccurately reconstructed as\nempty space. We show qualitative examples of these failure cases in the supplemental material. These\nchallenging scenarios are often not properly reconstructed even when using ground truth RGB-D\ndata, and we believe that using self-supervised losses [10] for monocular scene reconstruction could\nbe an interesting future research direction. Additionally, higher resolution geometric ﬁdelity could\npotentially be achieved by sparse operations in 3D or learning local geometric priors on detailed\nsynthetic data [21].\n5 Conclusion\nWe introduced TransformerFusion for monocular 3D scene reconstruction, leveraging a new\ntransformer-based approach for online feature fusion from RGB input views. A coarse-to-ﬁne\nformulation of our transformer-based feature fusion improves the effective reconstruction perfor-\nmance as well as the runtime. Our feature fusion learns to exploit the most informative image view\nfeatures for geometric reconstruction, achieving state-of-the-art reconstruction performance. We\nbelieve that our interactive scanning approach provides exciting avenues for future research, and\nenables new possibilities in learning multi-view perception and 3D scene understanding.\nAcknowledgments\nThis project is funded by the Bavarian State Ministry of Science and the Arts and coordinated by\nthe Bavarian Research Institute for Digital Transformation (bidt), a TUM-IAS Rudolf Mößbauer\nFellowship, the ERC Starting Grant Scan2CAD (804724), and the German Research Foundation\n(DFG) Grant Making Machine Learning on Static and Dynamic 3D Data Practical.\n8\nFigure 3: Qualitative comparison of scene reconstructions on test set of ScanNet dataset [8]; note that\nonly RGB input is used by each method while the ground truth is reconstructed using the input depth.\n9\nReferences\n[1] M. Bleyer, C. Rhemann, and C. Rother. Patchmatch stereo-stereo matching with slanted support windows.\nIn Bmvc, volume 11, pages 1–11, 2011.\n[2] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n[3] C. Campos, R. Elvira, J. J. G. Rodríguez, J. M. Montiel, and J. D. Tardós. Orb-slam3: An accurate\nopen-source library for visual, visual-inertial and multi-map slam. arXiv preprint arXiv:2007.11898, 2020.\n[4] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object detection\nwith transformers. In European Conference on Computer Vision, pages 213–229. Springer, 2020.\n[5] J. Chibane, T. Alldieck, and G. Pons-Moll. Implicit functions in feature space for 3d shape reconstruction\nand completion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, jun\n2020.\n[6] B. Curless and M. Levoy. A volumetric method for building complex models from range images. In\nProceedings of the 23rd annual conference on Computer graphics and interactive techniques , pages\n303–312, 1996.\n[7] A. Dai and M. Nießner. 3dmv: Joint 3d-multi-view prediction for 3d semantic scene segmentation. In\nProceedings of the European Conference on Computer Vision (ECCV), pages 452–468, 2018.\n[8] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner. Scannet: Richly-annotated 3d\nreconstructions of indoor scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 5828–5839, 2017.\n[9] A. Dai, M. Nießner, M. Zollhöfer, S. Izadi, and C. Theobalt. Bundlefusion: Real-time globally consistent\n3d reconstruction using on-the-ﬂy surface reintegration. ACM Transactions on Graphics (ToG), 36(4):1,\n2017.\n[10] A. Dai, C. Diller, and M. Nießner. Sg-nn: Sparse generative neural networks for self-supervised scene\ncompletion of rgb-d scans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 849–858, 2020.\n[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers\nfor language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[12] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\nderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at\nscale. arXiv preprint arXiv:2010.11929, 2020.\n[13] A. Düzçeker, S. Galliani, C. V ogel, P. Speciale, M. Dusmanu, and M. Pollefeys. Deepvideomvs: Multi-view\nstereo on video with recurrent spatio-temporal fusion, 2020.\n[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n[15] J. Hou, A. Dai, and M. Nießner. 3d-sis: 3d semantic instance segmentation of rgb-d scans. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4421–4430, 2019.\n[16] J. Hou, A. Dai, and M. Nießner. Revealnet: Seeing behind objects in rgb-d scans. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2098–2107, 2020.\n[17] Y . Hou, J. Kannala, and A. Solin. Multi-view stereo by temporal nonparametric fusion. InProceedings of\nthe IEEE/CVF International Conference on Computer Vision, pages 2651–2660, 2019.\n[18] J. Hu, M. Ozay, Y . Zhang, and T. Okatani. Revisiting single image depth estimation: Toward higher\nresolution maps with accurate object boundaries. 2019.\n[19] S. Im, H.-G. Jeon, S. Lin, and I. S. Kweon. Dpsnet: End-to-end deep plane sweep stereo. arXiv preprint\narXiv:1905.00538, 2019.\n[20] M. Ji, J. Gall, H. Zheng, Y . Liu, and L. Fang. Surfacenet: An end-to-end 3d neural network for multiview\nstereopsis. In Proceedings of the IEEE International Conference on Computer Vision, pages 2307–2315,\n2017.\n[21] C. Jiang, A. Sud, A. Makadia, J. Huang, M. Nießner, T. Funkhouser, et al. Local implicit grid representations\nfor 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 6001–6010, 2020.\n[22] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah. Transformers in vision: A survey.\narXiv preprint arXiv:2101.01169, 2021.\n[23] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\n10\n[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural\nnetworks. Advances in neural information processing systems, 25:1097–1105, 2012.\n[25] C. Liu, J. Gu, K. Kim, S. G. Narasimhan, and J. Kautz. Neural rgb (r) d sensing: Depth and uncertainty\nfrom a video camera. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10986–10995, 2019.\n[26] X. Long, L. Liu, W. Li, C. Theobalt, and W. Wang. Multi-view depth estimation using epipolar spatio-\ntemporal network. CVPR, 2021.\n[27] W. E. Lorensen and H. E. Cline. Marching cubes: A high resolution 3d surface construction algorithm.\nACM siggraph computer graphics, 21(4):163–169, 1987.\n[28] Z. Murez, T. van As, J. Bartolozzi, A. Sinha, V . Badrinarayanan, and A. Rabinovich. Atlas: End-to-end 3d\nscene reconstruction from posed images. In ECCV, 2020. URL https://arxiv.org/abs/2003.10432.\n[29] Y . Nie, J. Hou, X. Han, and M. Nießner. Rfd-net: Point scene understanding by semantic instance\nreconstruction. arXiv preprint arXiv:2011.14744, 2020.\n[30] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and D. Tran. Image transformer. In\nInternational Conference on Machine Learning, pages 4055–4064. PMLR, 2018.\n[31] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,\nL. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint\narXiv:1912.01703, 2019.\n[32] S. Peng, M. Niemeyer, L. Mescheder, M. Pollefeys, and A. Geiger. Convolutional occupancy networks. In\nEuropean Conference on Computer Vision (ECCV), Cham, Aug. 2020. Springer International Publishing.\n[33] V . Pradeep, C. Rhemann, S. Izadi, C. Zach, M. Bleyer, and S. Bathiche. Monofusion: Real-time 3d\nreconstruction of small scenes with a single web camera. In 2013 IEEE International Symposium on Mixed\nand Augmented Reality (ISMAR), pages 83–88. IEEE, 2013.\n[34] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d classiﬁcation and\nsegmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n652–660, 2017.\n[35] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a\nmetric space. arXiv preprint arXiv:1706.02413, 2017.\n[36] C. R. Qi, O. Litany, K. He, and L. J. Guibas. Deep hough voting for 3d object detection in point clouds. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 9277–9286, 2019.\n[37] J. L. Schönberger, E. Zheng, M. Pollefeys, and J.-M. Frahm. Pixelwise View Selection for Unstructured\nMulti-View Stereo. In European Conference on Computer Vision (ECCV), 2016.\n[38] A. Sinha, Z. Murez, J. Bartolozzi, V . Badrinarayanan, and A. Rabinovich. Deltas: Depth estimation by\nlearning triangulation and densiﬁcation of sparse points. In ECCV, 2020. URL https://arxiv.org/\nabs/2003.08933.\n[39] J. Sun, Y . Xie, L. Chen, X. Zhou, and H. Bao. NeuralRecon: Real-time coherent 3D reconstruction from\nmonocular video. CVPR, 2021.\n[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.\nAttention is all you need. arXiv preprint arXiv:1706.03762, 2017.\n[41] L. von Stumberg, V . Usenko, and D. Cremers. Direct sparse visual-inertial odometry using dynamic\nmarginalization. In International Conference on Robotics and Automation (ICRA), May 2018.\n[42] K. Wang and S. Shen. Mvdepthnet: Real-time multiview depth estimation neural network. In 2018\nInternational conference on 3d vision (3DV), pages 248–257. IEEE, 2018.\n[43] W. Wang, R. Yu, Q. Huang, and U. Neumann. Sgpn: Similarity group proposal network for 3d point\ncloud instance segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 2569–2578, 2018.\n[44] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 7794–7803, 2018.\n[45] P. Zins, Y . Xu, E. Boyer, S. Wuhrer, and T. Tung. Learning implicit 3d representations of dressed humans\nfrom sparse views. arXiv preprint arXiv:2104.08013, 2021.\n11\nFigure 4: Visualization of selected camera views with self-supervised attention weights (lower\nweights are visualized as blue and higher as red) for speciﬁc 3D locations (highlighted as green) in\nthe scene.\nA Additional Results\nVisualization of Frame Selection. In Fig. 4, we show a 3D reconstruction of a scene from the test\nset of the ScanNet dataset [8]. To visualize the view selection approach presented in the main paper\nthat is based on the attention weights of the used transformer networks, we render the camera views\nthat are selected for a speciﬁc 3D location (green point) with corresponding attention weights (color\ntemperature corresponding to the weight). On the right, we show the corresponding input images.\nAdditional Ablation Studies. As analyzed in the main document, the different algorithmic parts of\nour methods play an important role. Fig. 5 shows qualitative results for the ablation study. Speciﬁcally,\none can clearly see the impact of the spatial reﬁnement as well as the temporal feature fusion via our\ntransformer architecture. For qualitative results of the setting without using transformers for feature\nfusion we use predicted weights for weighted averaging of features via an MLP.\nIn Tab. 2, we conducted an additional quantitative ablation study w.r.t. the input to the transformer\nnetworks. As can be seen, both the projected depth as well as the view ray help the transformer to\nbetter fuse the features for the task of 3D reconstruction.\nIn Fig. 6, we show a comparison of our view selection scheme to the baseline that takes random\nviews from the view candidates (views that contain a speciﬁc point). Speciﬁcally, we vary the number\n12\nFigure 5: Qualitative comparison of ablations of our approach on test set of ScanNet dataset [8]; note\nthat only RGB input is used by each method while the ground truth is reconstructed using the input\ndepth.\nof views that can be selected. As can be seen, the reconstruction quality gap between the baseline\nand our method increases with less views which is to be expected since the random selection is more\nlikely to miss important views.\nTable 2: Quantitative ablation study on the transformer inputs, conducted on the test set of the\nScanNet dataset [8].\nMethod Acc ↓ Compl ↓ Chamfer ↓ Prec ↑ Recall ↑ F-score ↑\nOurs: w/o projected depth 8.06 10 .02 9 .04 0 .594 0 .475 0 .525\nOurs: w/o view ray 5.71 8 .59 7 .15 0 .706 0 .559 0 .621\nOurs 5.52 8 .27 6 .89 0 .728 0 .600 0 .655\n13\nFigure 6: Comparison of our attention-based view selection scheme to a random selection of frames\nfrom the frame candidates based on the F-score.\nAdditional Qualitative Results. In Fig. 11 further examples are shown that demonstrate the\nreconstruction capability of our approach. We show a top down, as well as a view from inside the\ndifferent reconstructed room scenes.\nB Reproducibility\nTable 3: Runtime analysis of our per-frame fea-\nture fusion.\nTask Duration\nImage loading / feature extraction 21.50ms\nCoarse feature fusion 11.81ms\nNear-surface mask prediction 24.18ms\nFine feature fusion 73.03ms\nTotal 130.52ms\nTable 4: Runtime analysis of the per-chunk mesh\nextraction.\nTask Duration\nCoarse feature reﬁnement 28.56ms\nFine feature reﬁnement 140.79ms\nMLP (occupancy prediction) 25.21ms\nMarching cubes [27] 48.74ms\nTotal 243.29ms\nRuntime Analysis. In this section we provide further details about the runtime of our approach.\nWe benchmarked our approach using an Intel Xeon 6242R Processor and an Nvidia RTX 3090 GPU.\nFor every new frame coarse-to-ﬁne image features need to be extracted, and fused into global coarse\nand ﬁne feature volumes. In Tab. 3, we report execution times of the different feature fusion steps.\nCoarse features are fused into the entire camera frustum, containing all coarse voxels that fall into\nvalid depth range [0.3m,5m]. Fine feature on the other hand are fused only in near-surface areas, as\npredicted by coarse ﬁltering. The execution times are averaged over a representative video sequence\nof the ScanNet dataset.\nNote that surface reconstruction doesn’t need to be extracted for every frame. It can either be done at\nthe end, when all image features are already fused into the feature volume, or incrementally every\ncouple of frames, on a per-chunk basis, if interactive feedback is desired. In Tab. 4, we report\nexecution times for a chunk of size 1.5 ×1.5 ×1.5 m. Both, coarse and ﬁne features are spatially\nreﬁned using a 3D CNN and surface occupancy is computed using the occupancy MLP at a voxel\nresolution of 2 cm, but only for near-surface voxels, as predicted by coarse and ﬁne near-surface\nmasks. Finally, the mesh is extracted using Marching cubes [ 27]. Note that our implementation\nuses high-level PyTorch routines, as well as CPU code (e.g., for Marching Cubes) and, thus, the\nimplementation is not optimized for runtime. A more optimized implementation can be achieved\nvia customized CUDA code. Another interesting avenue towards higher frame rates is the use of\nsparse 3D convolutions instead of dense 3D convolutions. Feature fusion timings are reported for\nour default reconstruction setting, when we store K = 16 views for every feature grid voxel. The\n14\nfeature fusion execution can be further accelerated by using less views. The frames per second (FPS)\nincrease from 7.66 FPS for 16 views to 10.17 FPS for 8 and to 12.28 FPS for 4 views.\nNetwork Architectures. In Fig. 7, we depict the architectures of the neural networks used in\nour approach. For both, the coarse and ﬁne layer, we use independent feature fusion and feature\nreﬁnement networks. The building blocks used in these networks are detailed in Fig. 8.\nFigure 7: Overview of the used neural networks. Note that we are using a feature fusion and feature\nreﬁnement network per level (coarse and ﬁne).\nFigure 8: Low-level network details of the building blocks of our pipeline (see Fig. 7).\nReproducibility of Experiments. To ensure the reproducibility of our experiments, we ran our\napproach and ablations 3 times. The resulting F-score mean and standard deviation for different\nexperiments is shown is Fig. 9, with standard deviations visualized as error bars.\n15\nFigure 9: The F-score mean and standard deviation of multiple experiments of our approach and\nablations.\nC Limitations\nIn Fig. 10, we present some limitations of our approach. When objects are only partially observed,\nwith a large occluded region, the occluded parts can be missing or lack detail. Examples are missing\nchair legs, or inaccurate reconstruction of small details such as books. Another challenging scenario\nfor our method is reconstruction of transparent objects, such as glass windows. Most of the time\nthese transparent surfaces get labeled as free-space instead.\nFigure 10: Limitations of our approach are the lack of detail at partially observed and occluded\nobjects, and inaccurate reconstruction of transparent surfaces, such as glass windows.\nD Data\nTo train and evaluate our method, we use the ScanNet dataset [8], which is available under a non-\ncommercial academic license1. ScanNet collects data of static indoor environments, and the ScanNet\nauthors report that consent was obtained from the people whose private spaces were scanned. The\nScanNet scenes and locations have been anonymized.\n1http://kaldir.vc.in.tum.de/scannet/ScanNet_TOS.pdf\n16\nFigure 11: Qualitative results of representative scenes from the test-set of the ScanNet dataset [8].\n17",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.7927960753440857
    },
    {
      "name": "Computer science",
      "score": 0.7785881757736206
    },
    {
      "name": "Computer vision",
      "score": 0.7199033498764038
    },
    {
      "name": "RGB color model",
      "score": 0.6249093413352966
    },
    {
      "name": "Monocular",
      "score": 0.5855053067207336
    },
    {
      "name": "Grid",
      "score": 0.5744057297706604
    },
    {
      "name": "Transformer",
      "score": 0.48497676849365234
    },
    {
      "name": "3D reconstruction",
      "score": 0.47704213857650757
    },
    {
      "name": "Convolutional neural network",
      "score": 0.42219263315200806
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.42209282517433167
    },
    {
      "name": "Iterative reconstruction",
      "score": 0.42185837030410767
    },
    {
      "name": "Engineering",
      "score": 0.09070345759391785
    },
    {
      "name": "Geography",
      "score": 0.08062708377838135
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 31
}