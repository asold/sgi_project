{
  "title": "An End-to-End Transformer-Based Automatic Speech Recognition for Qur’an Reciters",
  "url": "https://openalex.org/W4307873377",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5070447055",
      "name": "Mohammed Hadwan",
      "affiliations": [
        "Buraydah Colleges",
        "Qassim University",
        "Taiz University"
      ]
    },
    {
      "id": "https://openalex.org/A5045319295",
      "name": "Hamzah A. Alsayadi",
      "affiliations": [
        "Ain Shams University",
        "Ibb University"
      ]
    },
    {
      "id": "https://openalex.org/A5042737917",
      "name": "Salah Al-Hagree",
      "affiliations": [
        "Ibb University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W437348484",
    "https://openalex.org/W2900459877",
    "https://openalex.org/W2508749082",
    "https://openalex.org/W2312423791",
    "https://openalex.org/W3134657947",
    "https://openalex.org/W2903486309",
    "https://openalex.org/W3171055641",
    "https://openalex.org/W6631043263",
    "https://openalex.org/W3196788922",
    "https://openalex.org/W6759363799",
    "https://openalex.org/W6795588760",
    "https://openalex.org/W6690535375",
    "https://openalex.org/W3194921972",
    "https://openalex.org/W6784988500",
    "https://openalex.org/W3212140044",
    "https://openalex.org/W3047190796",
    "https://openalex.org/W6813026924",
    "https://openalex.org/W3082451925",
    "https://openalex.org/W2997510749",
    "https://openalex.org/W6739062951",
    "https://openalex.org/W6746678508",
    "https://openalex.org/W6788092415",
    "https://openalex.org/W2990982140",
    "https://openalex.org/W2182232004",
    "https://openalex.org/W6736930304",
    "https://openalex.org/W6729473640",
    "https://openalex.org/W4207003888",
    "https://openalex.org/W37145402",
    "https://openalex.org/W3167101391",
    "https://openalex.org/W6696934422",
    "https://openalex.org/W6746892368",
    "https://openalex.org/W6747398299",
    "https://openalex.org/W6702097088",
    "https://openalex.org/W6751474321",
    "https://openalex.org/W2062826588",
    "https://openalex.org/W4213428809",
    "https://openalex.org/W6795303654",
    "https://openalex.org/W3164885417",
    "https://openalex.org/W4285240134",
    "https://openalex.org/W4226523368",
    "https://openalex.org/W6753742271",
    "https://openalex.org/W6768086526",
    "https://openalex.org/W6754299077",
    "https://openalex.org/W2778756730",
    "https://openalex.org/W1482792282",
    "https://openalex.org/W2767014232",
    "https://openalex.org/W4231115139",
    "https://openalex.org/W4236377898",
    "https://openalex.org/W3162477961",
    "https://openalex.org/W3043910663",
    "https://openalex.org/W2620325601",
    "https://openalex.org/W2243689466",
    "https://openalex.org/W2559655401"
  ],
  "abstract": "The attention-based encoder-decoder technique, known as the trans-former, is used to enhance the performance of end-to-end automatic speech recognition (ASR). This research focuses on applying ASR end-to-end transformer-based models for the Arabic language, as the researchers’ community pays little attention to it. The Muslims Holy Qur’an book is written using Arabic diacritized text. In this paper, an end-to-end transformer model to building a robust Qur’an vs. recognition is proposed. The acoustic model was built using the transformer-based model as deep learning by the PyTorch framework. A multi-head attention mechanism is utilized to represent the encoder and decoder in the acoustic model. A Mel filter bank is used for feature extraction. To build a language model (LM), the Recurrent Neural Network (RNN) and Long short-term memory (LSTM) were used to train an n-gram word-based LM. As a part of this research, a new dataset of Qur’an verses and their associated transcripts were collected and processed for training and evaluating the proposed model, consisting of 10 h of .wav recitations performed by 60 reciters. The experimental results showed that the proposed end-to-end transformer-based model achieved a significant low character error rate (CER) of 1.98% and a word error rate (WER) of 6.16%. We have achieved state-of-the-art end-to-end transformer-based recognition for Qur’an reciters.",
  "full_text": "This work is licensed under a Creative Commons Attribution 4.0 International License,\nwhich permits unrestricted use, distribution, and reproduction in any medium, provided\nthe original work is properly cited.\nechT PressScienceComputers, Materials & Continua\nDOI: 10.32604/cmc.2023.033457\nArticle\nAn End-to-End Transformer-Based Automatic Speech Recognition for\nQur’an Reciters\nMohammed Hadwan1,2,*, Hamzah A. Alsayadi3,4 and Salah AL-Hagree5\n1Department of Information Technology, College of Computer, Qassim University, Buraydah, 51452, Saudi Arabia\n2Department of Computer Science, College of Applied Sciences, Taiz University, Taiz, 6803, Y emen\n3Computer Science Department, Faculty of Computer and Information Sciences, Ain Shams University,\nCairo, 11566, Egypt\n4Computer Science Department, Faculty of Sciences, Ibb University, Y emen\n5Department of Computer Sciences & Information, Ibb University, Y emen\n*Corresponding Author: Mohammed Hadwan. Email: m.hadwan@qu.edu.sa\nReceived: 17 June 2022; Accepted: 19 August 2022\nAbstract: The attention-based encoder-decoder technique, known as the\ntrans-former, is used to enhance the performance of end-to-end automatic\nspeech recognition (ASR). This research focuses on applying ASR end-to-\nend transformer-based models for the Arabic language, as the researchers’\ncommunity pays little attention to it. The Muslims Holy Qur’an book is\nwritten using Arabic diacritized text. In this paper, an end-to-end transformer\nmodel to building a robust Qur’anvs. recognition is proposed. The acoustic\nmodel was built using the transformer-based model as deep learning by\nthe PyTorch framework. A multi-head attention mechanism is utilized to\nrepresent the encoder and decoder in the acoustic model. A Mel filter bank is\nused for feature extraction. To build a language model (LM), the Recurrent\nNeural Network (RNN) and Long short-term memory (LSTM) were used to\ntrain an n-gram word-based LM. As a part of this research, a new dataset of\nQur’an verses and their associated transcripts were collected and processed\nfor training and evaluating the proposed model, consisting of 10 h of .wav\nrecitations performed by 60 reciters. The experimental results showed that\nthe proposed end-to-end transformer-based model achieved a significant low\ncharacter error rate (CER) of 1.98% and a word error rate (WER) of 6.16%.\nWe have achieved state-of-the-art end-to-end transformer-based recognition\nfor Qur’an reciters.\nKeywords: Attention-based encoder-decoder; recurrent neural network; long\nshort-term memory; qur’an reciters recognition; diacritized arabic text\n1 Introduction\nNearly 300 million people are native speakers of the Arabic language, which is a member of the\nSemitic language family that includes other well-known languages such as Hebrew and Aramaic [1].\n3472 CMC, 2023, vol.74, no.2\nThe Arabic alphabet consists of 28 letters, all of which represent consonants and are written from\nright to left. The three-letter root system is the most distinctive feature of Semitic languages. The\npattern system is another important aspect of the Arabic language when a pattern is imposed on the\nfundamental root. For example, “wrote” and “writer” in the Arabic language is Katab\nand Kateb\nwhich have the same three root letters that are. For more details about Semitic languages\nrefer to [1]. The Arabic script is a modified abjad [2] in which letters represent short consonants and\nlong vowels, but short vowels and consonant length are not commonly shown in writing. Diacritics\n“Tashkeel” is an optional character that can be used to denote missing vowels and consonant length.\nThe Arabic script contains various diacritics, which are critical in achieving typographic text usability\nstandards such as homogeneity, clarity, and readability. Any modification to the letter’s diacritic of the\nword can radically alter its meaning [3].\nThe Holy Qur’an, Islam’s fundamental religious book, is divided into 114 chapters, each of which\nconsists of verses. Besides its religious importance for Muslims, it is largely recognized as the best\nwork in Arabic literature and has had a tremendous influence on the Arabic language [4,5]. Qur’an\nrecitation “Qira’at” is a daily practice of Muslims as a part of their faith. The Qur’an was passed\ndown from generation to generation through recitation. The Qur’an has seven canonical qira’at [6].\nThe correct recitation of the Holy Qur’an depends mainly on another discipline known as Tajweed.\nTajweed [7] specifies the correct way of reciting the Qur’an, how to correctly pronounce each individual\nsyllable, the pause places in Qur’an verses, in addition to elisions, where long or short pronunciation\nis needed, where letters should be kept separately, and where they should be sounded together, and so\non. All Muslims around the world, be they Arabic native speaker or non-Arabic speakers, recite and\nlisten to Qur’an in the Arabic language.\nAutomatic Speech Recognition (ASR) is the use of computers to recognize and process a person’s\nspeech. In research and industry, there has been a substantial growth in interest in ASR mostly directed\ntoward the English language. For the Arabic language, little attention is paid to exploring ASR where\nseveral attempts can be found in the literature such as in [8–11]. Published review papers of ASR for\nthe Arabic language can be found in [10,12–14]. Because of the various drawbacks of traditional ASR\nmodels, the end-to-end model is an important research path in speech recognition. There are very few\nattempts to use ASR end-to-end transformer-based, as we only found two studies [15,16] published\nrecently and none of them tackle the problem of Arabic diacritized texts. An attempt is made in [17]\nto explore the effect on recognition accuracy of ASR for Arabic diacritized and non-diacritized texts\nusing convolution neural network (CNN) and long short-term memory (LSTM). The same Arabic\ndiacritized and non-diacritized texts were used for the conducted experiments. The results showed\nthat the word error rate (WER) for non-diacritized texts was 14.96% compared to 28.48% for the\ndiacritized texts, the lower is the better. This proved that the diacritized Arabic text is challenging and\nneeds to be explored deeply. As mentioned before, Qur’an is written using diacritics, which encouraged\nus to explore and propose the end-to-end transformer-based to solve this challenging problem.\nTo the best of researchers’ knowledge, the end-to-end transformer-based model was never\nproposed for ASR Qur’an reciters. To fill this gap, this research work proposes a novel deep learning\nmodel for the Qur’anvs. recognition with an end-to-end model.\nThe main contributions of this work are summarized as follows:\n• A new deep learning approach of the Qur’anvs. recognition with an end-to-end model. The\nmain idea of this model is to recognize the Arabic diacritized text.\n• The end-to-end transformer architecture and multi-head attention are proposed to recognize\nthe voice of Qur’an reciters.\nCMC, 2023, vol.74, no.2 3473\n• The look-ahead model is used to build word-based and character-based language models. These\nmodels are trained based on Recurrent Neural Network (RNN) and LSTM.\n• A new dataset was collected and processed to be used for training and evaluating the proposed\nmodel and will be available publicly for further research.\n• The presented model represents a state-of-the-art of ASR for the Holy Qur’an recognition.\nThis paper is organized as follows. Section 2 offers background information about the current\nresearch and Section 3 includes a description of the proposed model. In Section 4, the experimental\nsetup and the used dataset are introduced. Then, Section 5 is devoted to the results and discussion.\nFinally, the conclusion of this research is in Section 6.\n2 Related Work\nIn this section, the literature related to the proposed model is presented. The attention paid to the\nASR methods focused to recognized the voice of Qur’an reciters.\nAlkhateeb [18] used two classifiers, (i) K-Nearest Neighbors (KNN), and (ii) Artificial Neural\nNetwork (ANN) to recognize the Holy Qur’an reciters. MFCC is used to analyze the audio dataset.\nPitch was used as a feature to train KNN and ANN. The results showed that ANN reached an accuracy\nrate up to 97.7% while KNN showed an accuracy rate of 97.03%.\nNahar et al. in [19] have used a recognition model to identify the “Qira’ah” (type of reading) from\nthe related Holy Qur’an audio wave. The proposed model was created in 3 stages: (i) the extraction\nand labeling of MFCC features from an acoustic signal, (ii) training the SVM learning model with the\nidentified features, and (iii) detecting “Qira’ah”. With a success rate of 96 percent, the experimental\nfindings demonstrated the power of the introduced SVM-based recognition model.\nLataifeh et al. [20] compared the performance of classicalvs. deep-based classifiers. The study\noffers a comparison between the accuracy of the automatically proposed method in contradiction of\nhuman expert listeners’ in recognizing reliable reciters from imitators. Results showed that the accuracy\nof selected classical and deep-based classifiers reached 98.6% compared to 61% of human experts.\nArabic diversified dataset is introduced lately by Lataifeh et al. [21] to have a unified dataset that can\nbe used to assess the introduced method and models for Qur’anic research.\nMohammed et al. in [22] provided a technique for Qur’an reciters rules recognition to detect the\nMedd rule and Ghunnah using phoneme duration. The used dataset was gathered from 10 Qur’anic\nreciters in order to compute the Medd and Ghunnah durations in the right recitation. The developed\napproach was then utilized to identify the Medd and Ghunnah as Tajweed norms in Quran recitation.\nIn Gunawan et al. [23], for Qur’anic reciter identification, the features of Mel Frequency\nCepstral Coefficients (MFCC) were extracted from the recorded audio, and after training a Gaussian\nMixture Model (GMM), Gaussian Supervectors (GSVs) were formed using model parameters such\nas the mean vector and the main diagonal of the covariance matrix. This model can be applied to\nprotocol classification, feature learning, anomalous protocol identification, and unknown protocol\nclassification. The researchers in [24] used a Support Vector Machine (SVM) and threshold scoring\nsystem to recognize different Tajweed rules automatically. 70- dimensional filter banks were used for\nfeature extraction. A new dataset collected by the authors was used for the experiments and very\npromising results were obtained.\nIn [25], the authors used features such as MFCC and Pitch for learning process to recognize the\nQur’an reciters. Several machine learning algorithms such as Random Forest, Naïve Bayes and J48\n3474 CMC, 2023, vol.74, no.2\nwere used. The obtained results show the ability of proposed model to detect Qur’an reciter based on\nthe used dataset. The best recognition accuracy was 88% when using Naïve Bayes.\nAsda et al. in [26] proposed a reciters recognition system based on feature extraction from the\nMFCC and an ANN. a small dataset of five reciters was used for training and testing. They obtained\n91.2% recognition accuracy for reciters. Bezoui et al. in [27] extracted characteristics from Quranic\nverse recitation using the MFCC approach. Preprocessing, framing, windowing, DFT, Mel Filter-\nbank, Logarithm, and Discrete Cosine Transform DCT are all part of MFCCs. An autonomous\nreciter recognition system based on text-independent speaker recognition approach employing MFCC\nis introduced by [28]. The dataset utilized a sample of 200 recordings made by 20 reciters. For clean\nsamples, they achieved an 86.5% identification rate.\nIn [29], the Hidden Markov Model (HMM) algorithm approach is used in this work to develop\na model utilizing the MFCC feature extraction. With conventional sentence percentages, HMM is\nutilized in reciters voice recognition. The dataset utilized in this study is voice data extracted from\nthe voice of a known and related Quran reciter. The test results on the created model have an average\npercentage of test data correctness of 80%.\nBased on Quranic recitations, researchers in [30] reported the construction of a recognizer for the\nallophonic sounds of Classical Arabic. The Cambridge HTK tools were used to create this recognizer.\nAn acoustic HMM represented with three emission states is used to model each allophonic sound. For\neach emitting state, a continuous probability distribution based on 16 Gaussian mixed distributions is\nemployed. The results demonstrate that without employing any special language model, identification\nrates attained a high degree of accuracy (88% on average), which is highly promising and encouraging.\nWe can conclude that, based on this comprehensive review of the ASR literature on Qur’anic\nreciters recognition, it is clear that no attempts have been made to use an end-to-end transformer-\nbased model for recognizing and identifying Qur’an reciters. Therefore, we are trying to fill this gap\nin the literature by exploring the end-to-end transformer-based model for Qur’an reciters.\n3 Theoretical Background\nA convolution neural network (CNN) was established as a biologically inspired visual perception\nmodel. It can be used to feed ASR tasks to improve the recognition accuracy. Even though CNN\nleverages structural locality from the feature space to decrease spectral variation in acoustic features\nthrough pooling adaptation at a local frequency region, the CNN may identify the local structure in the\ninput data. CNN can take advantage of long-term dependencies between speech frames by leveraging\nprevious knowledge of speech signals. Furthermore, CNN has new properties above DNN, such as\nlocalization, weight sharing, and pooling. In the convolution unit, the locality is employed to handle\nnoise where is used [31,32]. Additionally, locality minimizes the network weights that must be learned.\nWeight sharing with the locality is used to decrease translational variance. The same feature values\ncomputed at separate places are pooled together and represented by a single value in pooling. This\nis quite useful for dealing with tiny frequency shifts that are frequently available in speech signals.\nOther models, such as GMMs and DNNs, find it harder to manipulate this shifting process. As a\nresult, ASR researchers have recently employed localization in both frequency and time axes in speech\nsignals [31,32].\nChiu et al. [33] reported that CNN achieves 30% enhancement over GMMs and 12% enhancement\nover DNNs, using 700 h of speech data. Rao et al. [34] showed that the CNN-based end-to-end ASR\napproach has given promising results. Thus, we used a CNN-based end-to-end ASR approach for\nCMC, 2023, vol.74, no.2 3475\nbuilding the employed model. Researchers in [35] proposed a sequential CNN to test a collected dataset\nthat includes 312 phonemes of the short vowel Arabic Alphabet/a/ “Alif”. The obtained results revealed\nthat CNN gave high accuracy of 100% and 0.27 of loss.\nLSTMs are a form of RNN that is utilized for RNN evolution. The LSTM method can save\ninformation over a long period using long-term dependencies to find and exploit long-range contexts.\nThe conventional RNN includes a single neural network, whereas the LSTM has four cooperating\nlayers, each with its own communication link [12,36]. Researchers in [37] discussed the Siamese LSTM\nnetwork used to authenticate the Qur’an recitation for testing Qur’an memorization. They contrasted\nthe MaLSTM with the Siamese Classifier model. Several feature extraction approaches, such as\nMFCC, MFSC, and Delta, were investigated. The model was developed and tested using four readers’\ndata who recited 48 verses from the Qur’an’s final ten suras (chapters). The best model used the\nMaLSTM with an additional fully connected layer with MFCC and delta features and received an\nF1-score of 77.35%.\nIn ASR, we use the coming context as well if the transcription for all utterances is obtained at\nthe training time. An LSTM calculates an input sequence X= x1, x3, ... , xT and the corresponding\noutput sequence Y= y1, y2, ... , yL using the activation of network units is calculated given the\nT-length of the speech feature sequence ot−1, an LSTM is employed in the training stage with\nsubsampling. It is utilized to create the following high-level feature h1:T0 as presented inEq. (1).\nh\nt = LSTM (xt, ot−1) (1)\nwhere the subsampling is denoted by h. X is the input feature that will be handled to create the hidden\nstates ht based on the processes frame-wise. To reduce the computational cost, LSTM presents the\noutputs. Therefore, in ASR, the input length is not equivalent to the output length [31].\nLSTM models are considered state-of-the-art ASR systems [32]. Moreover, deep LSTM networks\nachieve better performance and accuracy in ASR tasks [33,34]. For building the employed model, the\nLSTM-based end-to-end ASR method is used.\nIn encoder-decoder, all features are represented using a context vector. The context vector is used\nto generate the word sequence during the time. The attention-based model [35,36] is utilized to adjust\nthe encoder-decoder model to become more accurate. It uses the attention layer to obtain a time-\nvariant and dynamic context vectorci. This layer is located between the decoder and encoder [37].\nThe dynamic context vector is formulated as follows inEq. (2).\nc\ni =\n∑\nt\nαitvt (2)\nwhere αi1, αi, ... , αit denote to the weights. These weights will be calculated in a dynamic process as\nfollows inEq. (3),\nαit = exp (s (vt, ui−2))∑ T\nk=1 exp (s (vk, ui−2))\n(3)\nThe attention score is expressed bys(vt;u (i − 2)) notation, t denotes the input position, andi denotes\nthe output position. The scoring function is implemented using the dot product or bi-linear form as\nin Eqs. (4)and (5), respectively.\ns (v\nt, ui−2) = vT\nt ui−2 (4)\ns (vt, ui−2) = vT\nt Mui−2 (5)\n3476 CMC, 2023, vol.74, no.2\nwhere M is the corresponding parameter.\nThe conditional probability of a word sequence is formulated as follows inEq. (6):\nP (w1:T |x1:T ) ≃\n∏ M\ni=1\nP (wi|wi−1, ui−2, c) ≃\n∏ M\ni=1\nP (wi| ui−1) (6)\nThe attention layer uses the context vector ct to find the frames dynamically.\n4 System Description\nThe Espresso toolkit is used to build the end-to-end model, which is based on the end-to-end\ntransformer. Its underlying deep learning engine is called PyTorch. The stages for the employed model\nare depicted inFig. 1, beginning with the preprocessing step of the corpus data, lexicon, and language\nmodeling. The following step is the extraction of the features, followed by training and language\nmodeling construction. Finally, multi-head attention is used for the encoder and decoder. The data\npreprocessing, features ex-traction, and language modeling are performed as presented in [17,38].\nEncoder\nTesting Set\n External Data\n Training Set\nData Preprocessing\nFeature Extraction\n LM (RNN-LM and LSTM-LM)\nAttention Layer\nDecoder\nRecognized Units\nAttention Layer\nEncoder Features\nCNN layers\nFigure 1:The steps of the proposed model\n4.1 Feature Extraction\nFeatures extraction is very important aspect for machine learning systems [39–44]. We utilize the\nMel frequency filter bank (MFF Bank) technique to build the acoustic features. An MFF Bank is a\nmethod to simulate the human logarithmic audio perception which is considered a suitable method for\nASR. The frequency is converted into Mel Scale for calculating the Mel frequency filter bank. Then,\nthe Mel frequency cepstrum (MFC) is utilized to represent the short-term power spectrum of a phone\nusing the log power spectrum transformation [38]. The filter bank is used to derive the MFC feature on\ntop of the FFT. In general, the signal is a set of short frames of around 20 milliseconds. These frames\nwill be split into values as frequency bands that represent the weights. In this work, the pitch features\nand the 40-dimensional Log Mel-filter bank (a total of 43 dimensions) from the raw speech data were\ngenerated. It is utilized for training the 10-hours dataset that is used in this research.\n4.2 Language Modeling\nASR requires a Language Model (LM) to compute a priori probability of a word sequence. The\nacquired text data is utilized to train and create the proposed model’s external LM. An external neural\nCMC, 2023, vol.74, no.2 3477\nLM, named the Look-ahead model [45], is utilized to generate the trained word-based and character-\nbased language models. This external LM outperforms multilayer LMs [45]. Using prefix trees, a word-\nbased LM is turned into a character-based model. The RNNLM and LSTM-LM are used to construct\nthe word-based LM, with the RNN-LM calculating the probability of the following character based\non all preceding words and the word’s prefix. The LSTM-LM, on the other hand, is used to forecast\nword probability [46].\nIn the decoding step, the trained word-based and character LMs are combined with the estimated\nprobabilities of the look-ahead word’s prefix. In this research, the look-ahead word-based LM allows\nbatches to make training faster than existing LMs. The look-ahead word-based LM probabilities\nare determined in each recognition phase according to the decoding of the word prefixes. The\nprefix trees are used to transform a word-based LM into a character-based LM [17,31]. We used a\ncompletely parallelized version of the decoding technique for GPUs that was presented in Espresso\nfor Parallelization LM. The character-based LM in [45] is created from a word-based LM using the\nprefix tree technique.\n4.3 End-to-end Transformer-based Architecture\nThe transformer is a new end-to-end model, which uses the encoder-decoder based to build ASR\n[47]. The transformer-based model uses the mechanism of self-attention to transform the sequences\nby applying attention matrices to the acoustic feature. In this work, we proposed a transformer-based\nmodel as a state-of-the-art model for Qur’an reciters recognition, as shown inFig. 2. The employed\ntransformer-based model comprises two parts: an encoder with a set of blocks and a decoder with a\ns e to fb l o c k s .\nYT\nPrevious Token (Y[1:t-1)\nAcoustic Feature (XF)\nMulti-head Attention\nAdd & Norm\nEncoder\nMulti-head Attention\nAdd & Norm\nDecoder\nFully Connected + \nSoftmax\nCNN Layers\nPositional \nEncoding\nPositional \nEncoding\nEmbedding\nXS\nFigure 2:Transformer-based architecture\nThe encoder model converts the feature vector to an intermediate of encoded features. This feature\nvector represents an input of the encoder with 43-dimensional feature frames. Based on the encoded\nfeatures and previous predictions, the decoder model is used to generate a new prediction. Both\nmodels include attention and feedforward network techniques. In addition, our ASR includes encoder\nfrontend, positional encoding, ASR training, and ASR inference.\n3478 CMC, 2023, vol.74, no.2\n4.3.1 Encoder Frontend\nIn this step, we used CNN at the input layer to subsample the acoustic feature (X-fbank) into\n(X-sub). CNN consists of four layers, kernel size (3; 3), and convolution of 2-dimensions are used for\neach layer for the feature and time frame axis.\n4.3.2 Encoder\nThe encoder contains a set of blocks with different layers: a multi-head self-attention mechanism\nand a position-wise feed-forward network. A layer normalization [35] is applied after each layer using\nresidual connections. The subsampled sequences (X-sub), that are generated by the previous step,\nrepresent the input to the encoder blocks. The encoder transforms (X-sub) to (Q, K, V) using a self-\nattention layer with a Softmax as follows inEq. (7):\nSelf Attention(Q, K, V) = softmax\n(Q ∗ KT\n√\ndk\n)\n∗ V (7)\nQ ∈ Rnq∗ dq\n, K ∈ Rnk∗ dk\n, and Q ∈ Rnv∗ dv\ndenote queries, keys, and values respectively.d\n∗\nis the\ndimensions of values, keys, and queries, andn∗ is sequence lengths.\nWe used multi-head attention (MHA) to perform multiple attention networks. MHA yielded from\nall concatenated self-attention heads as follows inEqs. (8)and (9):\nMHA (Q, K, V) = [H1, H2, ... Hh] Wh (8)\nHi = Self Attention(Qi, Ki, Vi) (9)\nwhere h denotes the attention heads number in a single layer andi is theith head in the layer. The MHA\noutput is normalized before being sent into the Feed Forward (FF) sub-layer linked network, which\nis implemented for every point individually as inEq. (10).\nFF (h [t]) = max (0, h [t] ∗ W\n1 + b1) W2 + b2 (10)\nwhere h[t] denotes thetth position of the input H to the FF sublayer.\n4.3.3 Decoder\nThe decoder is processed as the encoder. Besides, it obtains the probability of the next unit’s\nsequence (YL) from the previous unit’s sequence (YL-1) and the output of the encoder (Hi), like LM\nmodel. The decoder has multiple self-attention to transform the encoder features and previous units\n(YL-1) into prediction (YL) at each time. In the decoder, the attention between the encoder output\nsequence (feature) and the previous sequence is computed using MHA. The residual connections are\nused in each sublayer. The encoder and the decoder are trained efficiently as an end-to-end model.\n4.3.4 Positional Encoding\nThe encoder features, the output of the encoder, are updated by positional encoding to address the\npositional context of units. Transformers utilize the sinusoidal positional encoding with time location\nas follows inEqs. (11)and (12).\nPE\n(n,2i) = sin\n(\nn\n1000\n2i\ndmodel\n)\n(11)\nCMC, 2023, vol.74, no.2 3479\nPE(n,2i+1) = cos\n(\nn\n1000\n2i\ndmodel\n)\n(12)\nwhere n denotes a word’s position in the text andi denotes the position with the dimension of the\nembedding vector.\n4.3.5 Training of ASR Transformer\nIn the training step, the prediction of all unit frames is predicted by the acoustic model as\nP¬t(Y|X), where Y is the transcription of the units and X is the acoustic features. The training loss is\ncalculated using the multi-objective function as follows inEq. (13).\nLasr =− log pt (Y|X) (13)\nwhere Pt is the probability predicted by the transformer decoder.\nMHA does not calculate the values of the lower triangular in the attention matrix. Thus, the output\nsequence does not send into positions in the query sequence.\n4.3.6 Transformer ASR Decoding\nIn the decoding step, we integrate the end-to-end model with the employed language model\n(LM) to enhance the predicted units. The external LM is utilized in this paper based on RNN and\nLSTM. The end-to-end transformer and shallow fusion are combined and computed based on the\ntwo posterior distributions summed over units as follows inEq. (14):\nYfinal = log Pasr + λ log Plm (14)\nFor every timestamp t, the prediction, Yfinal, is created by interpolating the distribution over\nvocabulary (V) given by log PASR and the same distribution given by log Plm. In addition, we apply for\nthe coverage and end-of-sentence threshold technique used in inference to improve the performance\nas presented in [17,38].\n5 Experimental Setup and Dataset\nA novel end-to-end transformer model is proposed for Qur’anvs. recognition. The proposed\nmodel is trained and evaluated using a collected dataset of Qur’an verses described in Section\n4.1. LM is trained and evaluated on a dataset from several websites. For evaluation purposes, the\ntraditional character error rate (CER) and word error rate (WER) are used to report the accuracy\nof recognition, while perplexity and out of vocabulary (OOV) are reported for LM. A Python code\nfor data preprocessing is written. In addition, the Espresso toolkit is used to write the recipe for\nimplementing the entire models. Experiments are conducted using a laptop with GeForce GTX 1060\n6 GB as GPU, Intel i7-8750H as CPU, 16 GB as RAM, and CUDA version 10.0.\n5.1 End-to-end Transformer-based Architecture\nFor the Qur’an dataset used in this research, 10 h of mp3 Qur’an verses recited by 60 reciters\nwere collected from the A2Y outh.com website and its associated transcripts. The collected dataset\nis used for training and evaluating the proposed models.Tab. 1shows more details about the dataset,\nincluding the suras names, the average number of sounds, #wav files, and the total minutes. The dataset\n3480 CMC, 2023, vol.74, no.2\nis distributed into three parts: (i) 70% for the training set, (ii) 10% for the development set, and (iii) 20\nfor the testing set.\nExtensive preprocessing occurs to the collected dataset as follows:\n1- The original audio files are converted from .mp3 format into .wav format.\n2- All audio file are resampled into 16 kHz.\n3- All transcript files (the text that corresponds to the audio file) are converted to Buckwalter\nformat.\nwhere Buckwalter transliteration may be thought of as the binary code for English, making it simple\nf o rm a c h i n e st op a r s e .\nTable 1: The dataset details\nSuras name Average of seconds #wav files Total of minutes\nAl-Zalzala 55 60 55\nAl-Adiyat 58 60 58\nAl-Qaria 59 60 59\nAl-Takathur 47 60 47\nAl-Asr 25 60 25\nAl-Humaza 48 60 48\nAl-Fil 36 60 36\nQuraish 31 60 31\nAl-Maun 37 60 37\nAl-Kauther 22 60 22\nAl-Kafiroon 39 60 39\nAn-Nasr 28 60 28\nAl-Masadd 34 60 34\nAl-Ikhlas 20 60 20\nAl-Falaq 30 60 30\nAn-Nas 33 60 33\nTotal 602 960 602\n5.2 End-to-End Transformer-Based Model Configuration\nIn this section, the proposed model configuration is presented, which includes different steps as\nfollows. In the preprocessing step, a Python code is written to convert audio files from mp3 into wav\ntype and unify the sample rate of all audio files. Then, we wrote Python code to convert corpus, lexicon,\nLM data to Buckwalter format, and link each transcript in our corpus with its audio file to make them\nsuitable for the Espresso recipe. The Espresso toolkit uses Kaldi-format to do the training, testing,\nwav.scp, and utt2spk files. All data are cleaned by deleting numbers, extra empty spaces, newlines, and\nsingle-character words.\nFor generating the acoustic features step, the Kaldi-format for feature extraction is used. 40-\ndimensional log Mel-frequency filter bank features and pitch features (3) are calculated every 20\nmilliseconds (ms) from the raw speech data. The output of this stage will be sent into the data\nconversion stage to store all data in one JSON file. This file represents the input of the acoustic model.\nThen, the n-gram RNN-LM and LSTM-LM were trained using the training and collected data. This\nCMC, 2023, vol.74, no.2 3481\ndata consists of 250 k words and 50 k sentences. The LM is trained based on the Pytorch library using\n3-layer LSTM with 1200 units for every layer. Moreover, the LM is trained using 35 epochs with 2000\nas batch-size and 40 as the max length of the string. In addition, we use the stochastic gradient descent\n(SGD) optimizer for training purposes.\nIn the pre-encoder step, the encoder receives the feature vectors with 43 as the dimension. Then,\nthe CNN is used to prepare the input of the encoder. CNN includes 4 convolutional layers with (3;\n3) kernels for each layer. In end-to-end transformer training step, the acoustic model is employed\nby 6 encoders, where 12 encoder blocks are set for each employed encoder. The model dimension is\nconfigured by d-model= 512 and multi-head attention blocks use 4 attention heads. In addition, the\nacoustic model includes 6 decoders and multi-head attention blocks that use 4 attention heads. The\nscheduled sampling is implemented byp = 0.5 to adapt epochs starting from epoch 6. The temporal\nsmoothing schema withp = 0.05 was applied to help the model ignore a sub-unit in the transcript\ndepending on the errors of beam search. The Adam optimizer has been used in this model as an\noptimization method.\nIn the recognition step, we combined the external LM and shallow fusion to enhance the\nperformance and accuracy. In addition, we added the end-of-sentence threshold and coverage methods\nfor improving the quality of the Arabic ASR approach. Each model is with 10 k decoded as the optimal\nbatch size. For enhancing the accuracy and performance, The LM fusion weight is allotted by 0.45 as\nan optimal parameter and combined with the look-ahead word-based LM. Moreover, assigned the\nend-of-sentence (EOS) threshold by 1.5 and beam size by 20 as the optimal size.\n5.3 Evaluation Metrics\nThe accuracy performance metric is used for evaluating the performance of ASR approaches.\nMoreover, the perplexity metric is used for evaluating the performance of LM. This section describes\nthese evaluation metrics in detail.\n5.3.1 ASR Evaluation\nThe performance evaluation of ASR is usually presented in terms of two criteria: (1) Character\nError Rate (CER), which represents the percentage of the character-level errors of the recognized\nunits, and (2) Word Error Rate (WER), which represents the percentage of the word-level errors of the\nrecognized units. These criteria are defined as follows inEqs. (15)and (16):\nCER = S + D + I\nN × 100 (15)\nWER = S + D + I\nN × 100 (16)\nwhere N represents all words in the set of evaluation utterances, substitutions (S) denote the number\nof misrecognized words, deletions (D) denote the number of deleted words in the recognition result,\nand I is the number of inserted words in the recognition result.\n5.3.2 Language Model Evaluation\nThe performance evaluation of LM uses the perplexity and OOV measures. If set M contains all\nof the tokens in LM data and set T contains all tokens in the test data, then OVV is the number of\n3482 CMC, 2023, vol.74, no.2\ntokens in T’s complement divided by the number of tokens in T according to [17,31], as shown below\nin Eqs. (17)and (18).\nOOV = #token incomplement of T\n#token in T (17)\nPerplexity =\n(∏ K\ni=1\nP\n(\ntokeni| tokenj<i\n))− 1\nk\n(18)\nwhere P (token i| token j< I) denotes the probability of ith throw of the training of LM based on the\nfirst i− 1t o k e n s .\n6 Results and Discussion\nThe employed end-to-end transformer is a state-of-the-art model for Qur’anvs. recognition. ASR\nin this research recognizes the reciters’ voice-through two main parts-the Encoder and the decoder.\nIn the encoder, the features were extracted and passed to the CNN layers and then to the attention\nlayers before sending them to the decoder. While in the decoder the RNN-LM and LSTM-LM were\nused to encode the features and pass them to the attention layer to reach the units that recognized the\nreciters’ voices. Experimental results of this model on a collected verses dataset shows the outstanding\nperformance of the proposed model. The used dataset comprises 60 reciters with 16 verses for each\nreciter. To achieve the best result, we trained the model many times with different sizes of encoder and\ndecoder, size of layers, learning weights, epochs, and smoothing parameters. In addition, the inference\nis performed with different LM weights, EOS, and the size of the beam. Experimental results are\nconducted twice: i) completely, for the development set and testing set; and ii) partially, for each verse\nin the development set and testing set. In addition, we evaluated LM based on testing data.\nThe developed LM is evaluated based on the test data.Tab. 2shows the perplexity and OOV results\nfor the language model.\nTable 2: Perplexity and OOV results\nPerplexity OOV\n1.36 1.07\nThe transformer-based model with/without LM is evaluated for a complete development set and\ntesting set.Tab. 3shows CER and WER results on the complete development set and testing set using\na transformer-based model without LM.\nTable 3: The CER and WER results of the transformer-based model without LM\nModel Development set Testing set\nTransformer-based without LM CER WER CER WER\n3.64 6.72 3.83 12.52\nAccording toTab. 3, the transformer-based model without LM yields results of 3.64% and 3.83%\nof CER for the development set and testing set, respectively. Moreover, the model achieves results of\n6.72% and 12.52% of WER for the development set and testing set, respectively.Tab. 4shows CER\nCMC, 2023, vol.74, no.2 3483\nand WER results on the complete development set and testing set using the transformer-based model\nwith LM.\nTable 4: The CER and WER results of the transformer-based model with LM\nModel Development set Testing set\nTransformer-based with LM CER WER CER WER\n1.58 3.83 1.98 6.16\nAccording to Tab. 4, the transformer-based model with LM yields results of 1.58% and 1.98%\nof CER for the development set and testing set, respectively. The model with LM achieves results of\n3.83% and 6.16% of WER for the development set and testing set, respectively. Thus, the model with\nLM obtains the best results. Based on the results inTab. 4, it is clearly that the CER of the model\nwith LM is reduced by 2.06% for the development set and 1.85% for the testing set when compared\nwith the same model but without LM. Moreover, the WER for the model with LM is 2.89% as for\nthe development set and 6.36% as WER for testing set gain on top of the model without LM. The\ntransformer-based model with/without LM for the complete and partial development set and testing\nset is shown inTab. 5for all tested verses.\nTable 5: The CER and WER results of the transformer-based model with/without LM\nVerse name Model without LM Model with LM\nCER WER CER WER\nAl-Zalzala 1.78 4.50 0.84 1.88\nAl-Adiyat 5.72 14.23 2.95 4.67\nAl-Qaria 5.91 15.88 3.36 5.29\nAl-Takathur 0.30 1.78 0.0 0.0\nAl-Asr 1.64 4.63 0.42 1.47\nAl-Humaza 2.76 5.41 1.18 2.53\nAl-Fil 2.07 5.09 0.95 2.32\nQuraish 2.89 6.03 1.49 3.18\nAl-Maun 3.09 9.07 1.81 4.73\nAl-Kauther 0.28 1.02 0.0 0.0\nAl-Kafiroon 8.06 26.61 4.74 11.55\nAn-Nasr 6.90 33.91 2.93 16.52\nAl-Masadd 4.04 12.82 2.21 6.23\nAl-Ikhlas 3.69 10.23 1.94 6.56\nAl-Falaq 7.74 38.75 4.83 25.66\nAn-Nas 4.43 10.42 2.03 5.97\nBased on Tab. 5, it is noticed that the Al-Takathur and Al-Kauther verses have the best CER\nand WER which got 0.0%, while Al-Asr and Al-Zalzala verses have 1.47% and 1.88% for WER,\n3484 CMC, 2023, vol.74, no.2\nrespectively. In addition, Al-Falaq verse has the worst WER result 25.66%. Also, the employed model\nachieved 25.66% for Al-Takathur and Al-Kauther gained top on the worst result.\n7 Conclusion\nIn this research, a high-performance approach for Qur’an verses recognition is presented. A\ntransformer-based model is proposed to develop this approach using an end-to-end model. This model\nis a new deep learning model to recognize the Arabic diacritized speech. The Mel filter bank with 40-\ndimensional for building acoustic features was utilized. A CNN is used as an encoder frontend at the\ninput layer for subsampling the acoustic feature. The acoustic model is built as an encoder-decoder\nmodel using multi-head attention. In addition, the look-ahead model is used to employ the word-\nbased and character-based language models language modeling (LM). We have used RNN to train\nand build RNN-LM and LSTM-LM. The employed model represents the state-of-the-art on Qur’an\nverses recognition. We presented a new Qur’an verse dataset including 10 h of Qur’an verses recited\nby 60 reciters. The proposed model was trained and evaluated based on a collected dataset of Qur’an\nverses. The results obtained are 1.98% for CER and 6.16% for WER which show the power of the\nproposed model for verses recognition. For further improvements, the researchers have the following\nsuggestions: (i) applying on-the-fly for feature extraction, (ii) applying transducer model training and\ndecoding on the employed model and (iii) investigating the suitability of the proposed models on larger\ndatasets.\nAcknowledgement: The authors would like to thanks the Chair of Prince Faisal for Artificial\nintelligence research (CPFAI) for funding this research work through the project number QU-CPFAI-\n2-10-5. Also would like to extend their appreciation to the Deputyship for Research& Innovation,\nMinistry of Education and the Deanship of Scientific Research, Qassim University for their support\nf o rt h i sr e s e a r c h .\nFunding Statement: The research work was supported by the Chair of Prince Faisal for Artificial\nIntelligent research (CPFIA), Qassim University through the Project Number QU-CPFAI-2-10-5.\nConflicts of Interest:The authors declare that they have no conflicts of interest to report regarding the\npresent study.\nReferences\n[1] S. Weninger, The Semitic Languages an International Handbook, Berlin, Germany: De Gruyter Mouton,\n2011. [Online]. Available: https://www.degruyter.com/document/doi/10.1515/9783110251586/html?lang=\nen.\n[2] N. Alsunaidi, L. Alzeer, M. Alkatheiri, A. Habbabah, M. Alattaset al., “Abjad: Towards interactive\nlearning approach to Arabic reading based on speech recognition,”Procedia Computer Science, vol. 142,\nno. 1, pp. 198–205, 2018.\n[3] H. Mohamed and A. Lazrek, “Design of Arabic diacritical marks,”International Journal of Computer\nScience Issues, vol. 8, no. 3, pp. 262–271, 2011.\n[4] K. Y . Jung, “The linguistic impact of the Quran on Arabic,”Arabic Language&Literature, vol. 17, no. 1,\npp. 1–20, 2013.\n[5] A. J. Arberry, “The Koran interpreted: A translation,”Journal of the American Oriental Society, vol. 85,\nno. 2, pp. 289–298, 1965.\nCMC, 2023, vol.74, no.2 3485\n[6] M. A. S. Khalil and N. H. Yusof, “The difference in Qur’anic readings in the interpretation of Al-Tabari\nand its effect on jurisprundential rulings: An analytical study,”Jurnal Islam dan Masyarakat Kontemporari,\nvol. 16, no. 1, pp. 111–126, 2018.\n[7] A. H. Ishaq and R. Nawawi, “Ilmu Tajwid dan implikasinya terhadap ilmu qira’ah,”QOF,v o l .1 ,n o .1 ,\npp. 15–24, 2017.\n[8] I. K. Tantawi, M. A. M. Abushariah and B. H. Hammo, “A deep learning approach for automatic speech\nrecognition of the Holy Qur’¯an recitations,”International Journal of Speech Technology, vol. 24, no. 4, pp.\n1017–1032, 2021.\n[9] H. Tabbal, W . El Falou and B. Monla, “Analysis and implementation of a Quranic verses delimitation\nsystem in audio files using speech recognition techniques,” inInt. Conf. on Information and Communication\nTechnologies: From Theory to Applications, ICTTA 2006, Damascus, Syria, pp. 2979–2984, 2006.\n[10] N. O. Balula, M. Rashwan and S. Abdou, “Automatic speech recognition (ASR) systems for learning\nArabic language and Al-Quran recitation: A review,”International Journal of Computer Science and Mobile\nComputing, vol. 10, no. 7, pp. 91–100, 2021.\n[11] F . Thirafi and D. P . Lestari, “Hybrid HMM-BLSTM-based acoustic modeling for automatic speech\nrecognition on Quran recitation,”inThe 2018 Int. Conf. on Asian Language Processing, IALP 2018, Institute\nof Electrical and Electronics Engineers Inc., Bandung, Indonesia, pp. 203–208, 2019.\n[12] A. A. Abdelhamid, H. Alsayadi, I. Hegazy and Z. T. Fayed, “End-to-end Arabic speech recognition: A\nreview,” inThe 19th Conf. of Language Engineering (ESOLEC’19), Alexandria, Egypt, 2020.\n[13] S. R. Shareef and Y . F . Irhayim, “A review: Isolated Arabic words recognition using artificial intelligent\ntechniques,” Journal of Physics: Conference Series, vol. 1897, pp. 1–13, 2021.\n[14] N. J. Ibrahim, M. Y . I. Idris, M. Y . Z. M. Yusoff and A. Anuar, “The problems, issues and future challenges\nof automatic speech recognition for Quranic verse recitation: A review,”AlBayan, vol. 13, no. 2, pp. 168–\n196, 2015.\n[15] A. Hussein, S. Watanabe and A. Ali, “Arabic speech recognition by end-to-end, modular systems and\nhuman,” Computer Speech and Language, vol. 71, no. 1, pp. 1–39, 2022.\n[16] W . Lin, M. Madhavi, R. K. Das and H. Li, “Transformer-based Arabic dialect identification,” in2020 Int.\nConf. on Asian Language Processing, IALP, Kuala lumpur, Malaysia, pp. 203–208, 2020.\n[17] H. A. Alsayadi, A. A. Abdelhamid, I. Hegazy and Z. T. Fayed, “Non-diacritized Arabic speech recognition\nbased on CNN-LSTM and attention-based models,”Journal of Intelligent and Fuzzy Systems, vol. 41, no.\n6, pp. 1–13, 2021.\n[18] J. H. Alkhateeb, “A machine learning approach for recognizing the Holy Quran reciter,”International\nJournal of Advanced Computer Science and Applications, vol. 11, no. 7, pp. 268–271, 2020.\n[19] K. M. O. Nahar, R. M. Al-Khatib, M. A. Al-Shannaq and M. M. Barhoush, “An efficient Holy Quran\nrecitation recognizer based on SVM learning model,”Jordanian Journal of Computers and Information\nTechnology, vol. 6, no. 4, pp. 392–414, 2020.\n[20] M. Lataifeh, A. Elnagar, I. Shahin and A. B. Nassif, “Arabic audio clips: Identification and discrimination\nof authentic cantillations from imitations,”Neurocomputing, vol. 418, no. 2, pp. 1–48, 2020.\n[21] M. Lataifeh and A. Elnagar, “Ar-DAD: Arabic diversified audio dataset,”Data in Brief, vol. 33, no. 1, pp.\n162–177, 2020.\n[22] A. Mohammed, M. S. B. Sunar and M. S. H. Salam, “Recognition of Holy Quran recitation rules using\nphoneme duration,”Lecture Notes on Data Engineering and Communications Technologies,v o l .5 ,n o .1 ,p p .\n1–12, 2018.\n[23] T. S. Gunawan, N. A. M. Saleh and M. Kartiwi, “Development of Quranic reciter identification system\nusing MFCC and GMM classifier,”International Journal of Electrical and Computer Engineering,v o l .8 ,\nno. 1, pp. 372–378, 2018.\n[24] A. M. Alagrami and M. M. Eljazzar, “SMARTAJWEED automatic recognition of Arabic Quranic recita-\ntion rules,” inInt. Conf. on Computer Science, Engineering and Applications, London, United Kingdom,\npp. 145–152, 2020.\n3486 CMC, 2023, vol.74, no.2\n[25] R. U. Khan, A. M. Qamar and M. Hadwan, “Quranic reciter recognition: A machine learning approach,”\nAdvances in Science, Technology and Engineering Systems, vol. 4, no. 6, pp. 173–176, 2019.\n[26] T. M. H. Asda, T. S. Gunawan, M. Kartiwi and H. Mansor, “Development of Quran reciter identification\nsystem using MFCC and neural network,”Indonesian Journal of Electrical Engineering and Computer\nScience, vol. 1, no. 1, pp. 168–175, 2016.\n[27] M. Bezoui, A. Elmoutaouakkil and A. Beni-Hssane, “Feature extraction of some Quranic recitation\nusing Mel-Frequency Cepstral Coeficients (MFCC),” inInt. Conf. on Multimedia Computing and Systems,\nMarrakech, Morocco, pp. 127–131, 2017.\n[28] M. A. Hussaini and R. W . Aldhaheri, “An automatic qari recognition system,” inInt. Conf. on Advanced\nComputer Science Applications and Technologies, ACSAT 2012, NW Washington, DC, United States, pp.\n524–528, 2012.\n[29] O. V . Putra, F . R. Pradana and J. I. Q. Adiba, “Mad reading law classification using Mel Frequency Cepstal\nCoefficient (MFCC) and Hidden Markov Model (HMM),”Procedia of Engineering and Life Science,v o l .\n2, no. 1, pp. 1–7, 2021.\n[30] Y . O. M. Elhadj, M. Alghamdi and M. Alkanhal, “Approach for recognizing allophonic sounds of the\nclassical Arabic based on Quran recitations,”Theory and Practice of Natural Computing, vol. 8273, no. 1,\npp. 57–67, 2013.\n[31] H. A. Alsayadi, A. A. Abdelhamid, I. Hegazy and Z. T. Fayed, “Arabic speech recognition using end-to-end\ndeep learning,”IET Signal Processing, vol. 15, no. 8, pp. 521–534, 2021.\n[32] H. Sak, A. Senior and F . Beaufays, “Long short-term memory recurrent neural network architectures for\nlarge scale acoustic modeling,” inProc. of the Annual Conf. of the Int. Speech Communication Association,\nINTERSPEECH, Brno, Czech Republic, pp. 1–5, 2014.\n[33] C. C. Chiu, T. N. Sainath, Y . Wu, R. Prabhavalkar, P . Nguyenet al.,“State-of-the-art speech recognition\nwith sequence-to-sequence models,” inICASSP , IEEE Int. Conf. on Acoustics, Speech and Signal Process-\ning, Calgary, Alberta, Canada, pp. 1–5, 2018.\n[34] K. Rao, H. Sak and R. Prabhavalkar, “Exploring architectures, data and units for streaming end-to-end\nspeech recognition with RNN-transducer,” in2017 IEEE Automatic Speech Recognition and Understanding\nWorkshop, ASRU 2017, Okinawa, Japan, pp. 193–199, 2017.\n[35] W . Chan, N. Jaitly, Q. Le and O. Vinyals, “Listen, attend and spell: A neural network for large vocabulary\nconversational speech recognition,” inICASSP , IEEE Int. Conf. on Acoustics, Speech and Signal Processing,\nShanghai, China, pp. 4960–4964, 2016.\n[36] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho and Y . Bengio, “Attention-based models for speech\nrecognition,” inAdvances in Neural Information Processing Systems, Montreal, Canada: MIT Press, pp.\n1–9, 2015.\n[37] C. Wu, “Structured deep neural networks for speech recognition,” Ph.D. Dissertation, University of\nCambridge, United Kingdom, 2018.\n[38] M. Sahidullah and G. Saha, “Design, analysis and experimental evaluation of block based transformation\nin MFCC computation for speaker recognition,”Speech Communication, vol. 54, no. 4, pp. 543–565, 2012.\n[39] E. -S. M. El-kenawy and M. Eid, “Hybrid gray wolf and particle swarm optimization for feature selection,”\nInternational Journal of Innovative Computing, Information & Control, vol. 16, no. 1, pp. 831–844, 2020.\n[40] A. Takieldeen, E. El-kenawy, M. Hadwan and M. Zaki, “Dipper throated optimization algorithm forun-\nconstrained function and feature selection,”Computers, Materials & Continua, vol. 72, no. 1, pp. 1465–1481,\n2022.\n[41] M. M. Eid, E. -S. M. El-Kenawy and A. Ibrahim, “A binary sine cosine-modified whale optimization\nalgorithm for feature selection,” in4th National Computing Colleges Conf. (NCCC 2021), Taif, Saudi\nArabia, IEEE, pp. 1–6, 2021.\n[42] S. S. M. Ghoneim, T. A. Farrag, A. A. Rashed, E. -S. M. El-Kenawy and A. Ibrahim, “Adaptive dynamic\nmeta-heuristics for feature selection and classification in diagnostic accuracy of transformer faults,”IEEE\nAccess, vol. 9, pp. 78324–78340, 2021.\nCMC, 2023, vol.74, no.2 3487\n[43] D. S. Khafaga, A. A. Alhussan, E. M. El-kenawy, A. E. Takieldeen, T. M. Hassanet al.,“Meta-heuristics\nfor feature selection and classification in diagnostic breast cancer,”Computers, Materials & Continua,v o l .\n73, no. 1, pp. 749–765, 2022.\n[44] E. -S. M. El-Kenawy, S. Mirjalili, F . Alassery, Y . Zhang, M. Eidet al.,“Novel meta-heuristic algorithm for\nfeature selection, unconstrained functions and engineering problems,”IEEE Access, vol. 10, pp. 40536–\n40555, 2022.\n[45] T. Hori, J. Cho and S. Watanabe, “End-to-end speech recognition with word-based RNN language models,”\nin 2018 IEEE Spoken Language Technology Workshop, SLT, Athens, Greece, pp. 389–396, 2019.\n[46] Y . Wang, T. Chen, H. Xu, S. Ding, H. Lvet al.,“Espresso: A fast End-to-end neural speech recognition\ntoolkit,” in2019 IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019,S e n t o s a ,\nSingapor, pp. 1–8, 2019.\n[47] L. Dong, S. Xu and B. Xu, “Speech-transformer: A no-recurrence sequence-to-sequence model for speech\nrecognition,”in ICASSP , IEEE Int. Conf. on Acoustics, Speech and Signal Processing, Calgary, AB, Canada,\npp. 5884–5888, 2018.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7666369676589966
    },
    {
      "name": "End-to-end principle",
      "score": 0.7627642154693604
    },
    {
      "name": "Computer science",
      "score": 0.7334598302841187
    },
    {
      "name": "Language model",
      "score": 0.7051774859428406
    },
    {
      "name": "Encoder",
      "score": 0.6067851781845093
    },
    {
      "name": "Speech recognition",
      "score": 0.6030619144439697
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5868005752563477
    },
    {
      "name": "Word error rate",
      "score": 0.523827850818634
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4564535617828369
    },
    {
      "name": "Deep learning",
      "score": 0.4324900507926941
    },
    {
      "name": "Artificial neural network",
      "score": 0.3654825687408447
    },
    {
      "name": "Natural language processing",
      "score": 0.3519211411476135
    },
    {
      "name": "Voltage",
      "score": 0.1592327058315277
    },
    {
      "name": "Engineering",
      "score": 0.12238085269927979
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210092650",
      "name": "Buraydah Colleges",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I156216236",
      "name": "Qassim University",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I36197038",
      "name": "Taiz University",
      "country": "YE"
    },
    {
      "id": "https://openalex.org/I107720978",
      "name": "Ain Shams University",
      "country": "EG"
    },
    {
      "id": "https://openalex.org/I78137547",
      "name": "Ibb University",
      "country": "YE"
    }
  ],
  "cited_by": 21
}