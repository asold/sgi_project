{
  "title": "Testing the limits of large language models in debating humans",
  "url": "https://openalex.org/W4409685933",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2761661765",
      "name": "James Flamino",
      "affiliations": [
        "Rensselaer Polytechnic Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5092973318",
      "name": "Mohammed Shahid Modi",
      "affiliations": [
        "Rensselaer Polytechnic Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1974741261",
      "name": "Boleslaw K. Szymanski",
      "affiliations": [
        "Rensselaer Polytechnic Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2566539857",
      "name": "Brendan Cross",
      "affiliations": [
        "Rensselaer Polytechnic Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5093917270",
      "name": "Colton Mikolajczyk",
      "affiliations": [
        "Troy University"
      ]
    },
    {
      "id": "https://openalex.org/A2761661765",
      "name": "James Flamino",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5092973318",
      "name": "Mohammed Shahid Modi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1974741261",
      "name": "Boleslaw K. Szymanski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2566539857",
      "name": "Brendan Cross",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093917270",
      "name": "Colton Mikolajczyk",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1994439841",
    "https://openalex.org/W160318044",
    "https://openalex.org/W4221055872",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W4379259169",
    "https://openalex.org/W4388488609",
    "https://openalex.org/W4388775529",
    "https://openalex.org/W4380763235",
    "https://openalex.org/W2614184674",
    "https://openalex.org/W2087228724",
    "https://openalex.org/W2087484885",
    "https://openalex.org/W4236619877",
    "https://openalex.org/W2126552603",
    "https://openalex.org/W4391058429",
    "https://openalex.org/W4376132814",
    "https://openalex.org/W4401042459",
    "https://openalex.org/W2973226110",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4390117450",
    "https://openalex.org/W4224298614",
    "https://openalex.org/W2072500831",
    "https://openalex.org/W2749069611",
    "https://openalex.org/W2275648997"
  ],
  "abstract": null,
  "full_text": "Testing the limits of large language \nmodels in debating humans\nJames Flamino1,3, Mohammed Shahid Modi1,3, Boleslaw K. Szymanski1,3, Brendan Cross1 & \nColton Mikolajczyk2\nLarge Language Models (LLMs) have shown remarkable promise in communicating with humans. Their \npotential use as artificial partners with humans in sociological experiments involving conversation is \nan exciting prospect. But how viable is it? Here, we rigorously test the limits of agents that debate \nusing LLMs in a preregistered study that runs multiple debate-based opinion consensus games. Each \ngame starts with six humans, six agents, or three humans and three agents. We found that agents \ncan blend in and concentrate on a debate’s topic better than humans, improving the productivity of \nall players. Yet, humans perceive agents as less convincing and confident than other humans, and \nseveral behavioral metrics of humans and agents we collected deviate measurably from each other. \nWe observed that agents are already decent debaters, but their behavior generates a pattern distinctly \ndifferent from the human-generated data.\nKeywords Large language models, Artificial intelligence, Human–computer interaction, Social dynamics, \nOpinion consensus, Statistical analysis\nThe advent of AI has given rise to a dream of building artificial agents capable of partnering and even successfully \ncompeting with humans 1. A baseline of successful competition between AI and humans was accomplished \nin 1997 when IBM’s Deep Blue defeated Garry Kasparov, the World Champion at the time 2. Later, a notable \nmilestone was set in 2011 with IBM’s Watson, which exhibited a robust understanding of human language and \ndefeated human champions in Jeopardy!3. In 2022, a breakthrough occurred in AI-human dialogue with the \nIntroduction of Large Language Models (LLMs) such as ChatGPT 4. These powerful models, trained on a vast \nnumber of human-produced texts, can converse with humans and solve complex text-based tasks 5–9. Their \nability to realistically interact with people using human language in a contextually and semantically cohesive \nfashion could transform social science, as postulated in10 wherein the authors discuss scenarios in which LLMs \ncould act as confederates to facilitate human-based experiments or even as surrogates for human players. Both \nroles have received some attention from designers of LLMs6,11,12. Although some past work has explored the role \nof artificial confederates in behavioral experiments13, those confederates acted in a limited role. They lacked the \ndepth of reasoning that LLMs possess.\nTesting if LLMs can behave like human confederates is crucial, as these models potentially pose an \nunprecedented opportunity for researchers to break the traditional mold of human-based experimental design. \nToday, little is known about the behavioral dynamics of human-LLM conversation, notably in a study setting. \nSubsequently, in this paper, we describe an experiment that robustly and rigorously tests the limits of agents in \nacting as confederates or when surrogating humans.\nOur paper uses the term ’agents’ to describe the simulated players that participate alongside human players \nin our debate games. These agents generate conversational messages using a combination of two LLMs, GPT-4 \nand Llama 2. They use ChatGPT with custom prompts and Python scripts to interact with the game environment \nand maintain a memory of past conversations within a single game. Thus, any agents with the same persona have \nno memories of past games, so each game is the first for each agent and human. The agents are assigned unique \n’personas’ to individuate them from each other as separate entities. The ’personas’ are a unique combination of \nfour initial traits: stubbornness, grammar sophistication, personal confidence and personally preferred diet. Any \nagent with the same persona in two or more separate games is considered the same individual in those games. \n(See “Agent Design” in the Methods section for details on each personality trait and the capabilities of agents.)\nWithin each game, agents, like humans, maintain memories of interactions with other players and could \nrefer to previous discussions in follow-up conversations with players they had already spoken to. Agents did not \nshare memories and were, like humans, unaware that other agents could be present in a game. Each agent cleared \n1Department of Computer Science and Network Science and Technology Center, Rensselaer Polytechnic Institute, \nTroy, NY, USA. 2Department of Mathematics, Troy, NY, USA. 3James Flamino, Mohammed Shahid Modi and \nBoleslaw K. Szymanski contributed equally to this work. email: szymab@rpi.edu\nOPEN\nScientific Reports |        (2025) 15:13852 1| https://doi.org/10.1038/s41598-025-98378-1\nwww.nature.com/scientificreports\n\nmemories of the previous games at the beginning of a new game. Thus, each game was the first played from the \nagent’s point of view.\nWe seek to answer two questions relating to the capabilities of these agents in conversation-intensive studies: \nHow capable are agents of acting as confederates (i.e., preserving or enhancing opinion dynamics leading to \nconsensus), and how capable are they of being human surrogates, generating data indistinguishable from human \ndata in the same environment?\nConsidering the implications of these questions, we look to approach this research through the prism of \nsocial science. First, we determined the number of samples necessary to achieve statistical significance of our \ndebate experiment’s results14,15. We identified all the required measures before initiating the study. We then pre-\nregistered the research and implemented a battery of statistical tests (including Bayesian regression models16) on \nthe data collected post-experiment.\nThe debate experiments we ran consisted of multiple games, starting with six players that involved a \ncombination of humans and agents acting as humans, all anonymously. We had three types of games with seats \nfor either six agents, or six humans, or three agents and three humans, denoted respectively as AA, HH, and AH \ngames. The humans were informed that agents were engaged at the end of the game. The game aim is to convince \nother players of your opinion on a debate topic and to seek a majority consensus. For our study, the debate topic \nwas on the best diet choice. Specifically, players were given this prompt at the beginning of a game, verbatim: \n“Which of these diets is the best compromise between nutritiousness and climate consciousness?” players were \nthen asked to select one of these four choices of diets: Vegan, Vegetarian, Omnivorous, or Pescatarian. We chose \nthis topic for the prompt because a choice in the diet affects a wide range of issues (including environmental \nand societal) but does not trigger deep societal polarization intrinsic to topics like politics, which often produce \nopinion stagnation17.\nTo promote collaboration among players, we designed a reward point system: convincing another player to \nchange their opinion grants the persuader one point, and all players with the most popular opinion at the end \nof the game receive three additional points. At the end of the game, the top two players are awarded double \ntheir base compensation. The players are given one hour to debate one-on-one with all other players using a \ncustom texting interface. These rules are revealed to players before the game. The game is intentionally simple \nin its design, with the sole source of interactions being pairwise, anonymous, open-ended text-based messaging \nsessions, enabling us to collect all text exchanges, dynamics of opinions about diets, and temporal aspects of \ninteractions.\nRecording the resultant behavioral patterns and comparing them across game types gives us the data to \nanswer our two questions. In the following sections, we describe our experiment and the data we collected, and \nthen we begin analyzing our results and comparing differing behaviors across varied study players.\nExperiment design\nWe deployed our study using voluntary players from the student body of the Rensselaer Polytechnic Institute. A \ntotal of 251 students signed up for the study; 111 humans joined games, but only Nh = 97 actively participated \nin conversations. This human persona attrition led to some AH and HH games having fewer than three or six \nhumans, respectively. We decided to include games with partial populations in the analysis of results since we \nfound that they improve the accuracy of these analyses without skewing the results (see SM section 7, 7.1 and \nSM table S8). We organized and ran to completion 37 games, of which NHH = 10 were HH games, NAH = 17 \nwere AH games, and NAA = 10 were AA games. See “Agent Design” in the Methods section for the detailed \nexperimental procedure and explaining how we scheduled games and decided human-agent player groupings.\nFor the 17 AH games,  we generated 30 agents with unique personas. Some agents with the same personas \nwere present in more than one AH game. For the 10 AA games with six agent seats each, we generated 60 agents \nwith unique personas. Thus, we had a total of 90 agents with 90 unique personas, and used Na = 90 as the \npopulation size of agents. When analyzing the behavior of agents in the results section, we treat the persona of \nthe agents as an individual subject, employing repeated measures analysis to account for the multiple data points \nsubmitted by these subjects. All agents starting a new game are cleared of any memories of the previous games, \nso all humans and agents play each game for the first time.\nA sample size analysis indicated that to achieve a medium effect size in our comparisons, we needed at least \n10 games per type; see the Supplemental Materials (SM) Section 7 for details. We note that for AH games, we set \nthe number of games to 17 to increase the data on human-agent interaction. Player assignment to condition was \nrandom; whenever the time slot for a game registered six humans, we would play an HH game until we had ten \ngames played. With more than two registered humans, we randomly selected three human players and generated \nthree random agents for an AH game. For AA games, we generated six random agents.\nFigure 1 depicts our game design. In stage 1 of each game (Fig.  1A), each player is given the prompt, which \nlists four opinion choices from which each player chooses an opinion and rates their confidence in their selection \non a four-level scale: “not very confident, ” “somewhat confident, ” “quite confident, ” and “very confident. ” In \nstage 2, each player is assigned to a fully connected network of size six. (Fig.  1B). Each player in stage 2 can \nrequest a one-on-one conversation with any other player in their network. If their request is accepted, those \ntwo players temporarily exit the network and engage in a private conversation via text-based messaging. The \nconversation lasts until one of the conversational partners terminates it or the time limit expires (Fig. 1C). When \na conversation ends, both players can re-evaluate their opinion and their level of personal confidence. They are \nalso asked to rate their partner’s perceived confidence using one of the same choices as personal confidence, with \nan additional “not enough info” option. After completing this step, time permitting, both players are returned to \nthe network, and each can request or accept subsequent conversations from other players (Fig. 1D).\nScientific Reports |        (2025) 15:13852 2| https://doi.org/10.1038/s41598-025-98378-1\nwww.nature.com/scientificreports/\nOnce the time limit has expired, all active players move to stage 3 (Fig. 1E), in which human players file exit \nsurveys (see “Exit survey design” in the Methods for more details). Then, all players of this game are removed \nfrom the game and the study.\nOur final dataset includes 713 conversations, of which 591 were completed before the time limit expired.15,804 \nmessages were sent in those conversations, resulting in 238 opinion changes. We also collected 91 exit surveys \nfrom the human players in the AA and AH games. Of all the conversations made in the AH games, 152 were \nbetween a human and an agent, 30 between two humans, and 82 between two agents, respectively. In the HH \ngames, 140 conversations were between two humans, while in the AA games, 309 conversations were between \ntwo agents. Excerpts of conversations occurring in three game types are shown in SM Table S24.\nResults\nWe focus on three behavioral metrics for each player. The first metric is the fraction of conversations that \nresulted in opinion change. The second is the frequency of changing all self-reported personal and perceived \nconfidences. Our final measurements focus on productivity, which applies metrics of player outputs based on \nstatistical data collected in-game and synthesized post-game. The former involves the number of conversations, \ntheir message counts, and the count of the points awarded to players at the end of the games. The latter focuses \non conversational attributes, including the fraction of on-topic messages sent during conversations. Both in-\ngame and post-game productivity measures enabled us to detect when humans and agents behave differently \ncomparatively.\nWith two player types (human and agent) and three game types ( AA, AH, and HH), there is a variety of \ndata that can be grouped differently. Subsequently, we classify conversations into three categories: aa, ah, and \nhh, indicating conversations between two agents, an agent and a human, and two humans, respectively. The \nhomogeneous games can only generate corresponding homogeneous discussions (e.g., HH games create hh \nconversations). However, AH games can produce all conversation types. When reporting on conversations in \nAH games that involve opinion change, we indicate which type of players changed their opinion by changing the \norder of the a and h, where the first letter is the indicator. Hence, the behavior of humans in AH games. Each AH \nconversation generates two data points, one for ah and one for ha.\nA similar distinction is needed when players assign a level of perceived confidence to their conversational \npartner. As such, we draw an arrow from the assigning player type to receiving player type: a→a, a→h, h→a, \nh→h. These indicate the assignment of perceived confidence from an agent to an agent, an agent to a human, a \nhuman to an agent, and a human to a human, respectively. We refer to this notation as “assignment type. ”\nDynamics of opinion switching\nIn consensus games, success is to convince another player to adopt one’s opinion. We analyzed how often human \nplayers change their opinions when conversing with other humans or agents. We measured the opinion change \nFig. 1. Experimental setup for a AH game type. (A) Stage 1: The players select an initial opinion and rate \ntheir confidence. (B) Stage 2: The players are placed in a fully connected network with five other players \nwho completed stage 1. The blue icons represent human players in this network, and red represent agents. \nThe players can then invite each other to engage in one-on-one conversations. (C) After two players agree \nto converse, they are cut from the network and converse using a text-based messaging system. (D) Once the \nconversation terminates, both players re-evaluate their opinions and personal confidence and assign a level of \nperceived confidence to their conversational partner. Then they rejoin the network (see B). If the time limit has \nexpired, all active players move to stage 3. (E). Stage 3: The game ends, human players file an exit survey, and \nall players quit the study.\n \nScientific Reports |        (2025) 15:13852 3| https://doi.org/10.1038/s41598-025-98378-1\nwww.nature.com/scientificreports/\nfrequency of players using a Bayesian multilevel logistic regression model predicting the probabilities of such \nchanges as a function of the player and their conversational partner types. See Table  1 for all instances of post-\nconversation opinion changes across the three types of games, clustered by conversation type.\nTable 2 summarizes the results of our Bayesian model, shown as the odds of opinion change for humans and \nagents in each conversation type. The odds ratio compares the odds of opinion change for each conversation \ntype against the human-human (hh) conversations, for example Odds-Ratioha = Oddsha/Oddshh. The 95% \ncredible intervals (CI) shown are in Odds of opinion change. For more details on the regression model, including \nformula and raw coefficients, see SM Section 9 and SM Table S11.\nFigure  2 shows the mean and credible interval of the opinion-switching frequency for each type of \nconversation that could occur in the study. In hh conversations, the expectation is that 0.12 players will switch \ntheir opinion for every non-changing player mean odds: 0.12, 95% CI 0.07–0.19). The ha odds ratio (mean value \n0.16, 95% CI 0.04 − 0.47) shows that for every opinion switched in hh conversations only 0.16 opinions switch \nfor ha conversations. The inverse of this, 6.25, shows that humans switch their opinions 6.25 times more often \nwhen interacting with other humans vs agents. Therefore, on average, agents need 6.25 times more conversations \nwith humans to induce the same number of opinion switches that hh conversations induce. We also found that \nConversation type Odds ratio Odds CI (95%)\nhh 1 0.12 0.068–0.185\nha 0.16 0.02 0.003–0.046\nah 2.99 0.36 0.203–0.574\naa 1.97 0.23 0.162–0.319\nObservations 1284\nNgame 37\nNplayer 185\nσ2 3.29\nτ00 game 0.10\nτ00 player 0.76\nMarginal R2 0.030\nConditional R2 0.140\nTable 2. Bayesian multilevel opinion changing model. This table outlines the results of our regression model, \npresented as the odds of opinion transitions in conversations and odds ratios relative to hh conversations. \nNote that in ah and ha conversation types, the first letter indicates the player type, which changes opinion. \nThis analysis only includes data from completed conversations. Thus, this analysis excludes the 14 players \nwho didn’t actively participate in their respective games and the two who conversed for the entire game time \nwithout closing the conversation.\n \nGame Type Conversation type\nDid the \nplayer \nchange \nopinion?\nYe s No\nHH hh 32 218\nAA aa 114 458\nAH\nhh 7 38\nha 4 130\nah 43 91\naa 38 110\nTotal 238 1046\nTable 1. Summary of conversations and opinion change in all games. This table contains the opinion change \noutcome of all 642 completed conversations across all three game types. Conversation type indicates the type \nof user interaction: two humans (hh), two agents (aa), or an agent and a human (ah). For ah conversations, \nwe show two rows where the first letter indicates for which player, agent, or human we measure opinion \nchange. Each player in a conversation has the opportunity to change their opinion, yielding 642 × 2 opinion \nchanges. We note that incomplete conversations do not record any opinion changes, and there were cases in \nwhich one player finished re-evaluating their opinion before the time limit expired, but their conversational \npartner did not. See SM Table S22 for a version of this table further split by the type of player who initiated the \nconversation.\n \nScientific Reports |        (2025) 15:13852 4| https://doi.org/10.1038/s41598-025-98378-1\nwww.nature.com/scientificreports/\nthe rate of opinion change for ah conversations can depend on what type of player holds the opinion proposed \nfor switching to. For more details on this finding, see SM Section 13 and SM Tables S20, S21, and S22.\nOverall, the above results imply for question 1 that agents are far less convincing for humans than humans. \nRelevant to question 2, the differences in the frequencies of opinion switching seen between agents and humans \nare confirmed by our regression model in Table 2. When we average over levels of the second player type, we get \nthat humans change opinion with an odds of 0.047 (95% CI 0.023–0.083), while agents change opinion with an \nodds of 0.288 (95% CI 0.201–0.408). This means agents, on average, have a 6.1 times higher odds of switching \ntheir opinion when compared to human participants.\nPersonal and perceived confidence\nFigure  3 displays the distributions of confidence exhibited by players by type of game and assignment for \nboth perceived and personal confidence. We numerically encoded the confidence levels, from the “Not very \nconfident” level assigned as 1 to “Very confident” assigned as 4. Additionally, “Not enough info” was assigned 0 \nfor perceived confidence. In Fig. 3A, we clustered perceived confidence levels by their assignment type, while for \nFig. 3B, we clustered personal confidence levels by player type.\nWe again use a Bayesian multilevel model to compare the distributions of perceived confidence from h→\nh and h→a. The model reveals a statistically significant difference in how perceived confidence is assigned. \nFurthermore, the model suggests a substantial difference in the perceived confidence being assigned by humans \nto agents compared to agents to humans. The complete model specifications are shown in SM Section 9 and SM \nTable S12.\nIn terms of results, Fig. 3(C) shows the assignment type contrasts in perceived confidence assignment for \nthe perceived confidence model. The model establishes that humans perceive other humans as more confident \nthan agents by mean value 1.8, 95% HPD-interval 1.034–2.871). Conversely, agents assign a significantly lower \nlevel of perceived confidence when interacting with humans compared to other agents by a factor of 10 (mean \nvalue 0.094, 95% HPD-interval 0.058 − 0.139). These differences in perceived confidence between humans and \nagents are statistically significant for most assignment types.\nWe extend this analysis to personal confidence as well. Implementing a Bayesian Multilevel model to \npredict personal confidence as a function of conversation type, we found that no statistically significant relation \nexists between conversation type and the players’ confidence (see SM Table S13). Concerning question 1, the \nabove analysis indicates that agents negatively affect a human’s perceived confidence but do not affect personal \nconfidence.\nThe above results have implications for question 2 as well. Notably, agents consistently perceive other agents \nas more confident, and humans tend to treat other humans likewise. The consistency of this behavior identifies \na degree of similarity in the two-player types in terms of assigning perceived confidence to the same player type.\nPost-game productivity\nNext, we analyzed changes in productivity. The first three sub-figures of Fig.  4 show the results of post-game \nanalyses on the number of conversations and message counts (Fig. 4A, B) and point distribution (Fig. 4C).These \nmeasures are aggregated over each player (human participant and agent persona) such that each player provides \none data point representing their average behavior throughout the study. For the conversation and message \ncounts, our visualizations make apparent a spectrum of behavior in HH games and AA games. Conversations \nand messages from AA games tend to exhibit distributions with a tighter modality, indicating that agents’ \ncommunication behavior is much more static. This makes sense, as the agents were designed with a message \nbudget: a maximum number of messages allowed to be sent before the conversation is terminated. See “ Agent \nDesign” in the Methods for details on the implementation. We also further explore the relationship between the \nnumber of messages sent per conversation and the influence of the message budget in SM Section 8.\nFig. 2. Bayesian multilevel model for opinion changing by player type. This plot shows the frequency of \nopinion changes as a function of the two conversing players. Each player in a conversation is a human or an \nagent, and combining these two types gives us an opinion-switching frequency for all conversation types in the \nstudy.\n \nScientific Reports |        (2025) 15:13852 5| https://doi.org/10.1038/s41598-025-98378-1\nwww.nature.com/scientificreports/\nCompared to the communication behavior seen in AA games, HH communication varies much more, \nas illustrated in the wider modalities of Fig.  4A, B. The AH games show a mediation of these two extremes, \nindicating the presence of agents in these games adds a level of predictability. In the context of question 1, \nwe found that the more agents narrow their variances of conversation length and count, the more humans \nconversing with these agents narrow their corresponding variances too.\nAgents also facilitated the distribution of award points in games, earning more points overall than humans \n(Fig.  4C). This suggests that agents were likelier to stay focused and attempt to promote opinion change in \nthemselves and others. This observation is essential for question 2, indicating that agents are more goal-oriented \nin this setting. We report majority consensus groups for AH games in SM section 15 and SM Table S23.\nIn-game productivity\nThe second subcategory in our productivity analysis relates to in-game productivity, like on-topic keyword usage. \nHere, we define on-topic keyword usage as the number of occurrences of specific words that we identified as \nrelated to the game’s prompt (i.e., diets, nutrition, climate) or opinion consensus (i.e., team, majority, minority). \nMore information on the dictionary of keywords used can be found in “Keyword analysis” in the Methods.\nFig. 4D shows KDEs of the frequency of on-topic keywords for each player involved in the study, grouped by \ntheir player type (i.e., an agent or a human). Each player provided one data point representing their frequency \nof using on-topic keywords across all conversations they participated in. One primary observation from this \ndata is that agents generate more on-topic keywords than humans. The mean and standard deviation of the \nhuman distribution from Fig.  4D are 0.085 and 0.046, respectively, compared to 0.129 and 0.016 for the agent \ndistributions.\nFigure 4E presents a similar analysis to Fig.  4D, except we only consider human player on-topic keyword \nfrequencies and group them by the type of game they participated in. Importantly, we find that agents’ proclivity \nfor on-topic discussion affects the behavior of humans that are in the same game, as indicated by humans in \nAH games generate more on-topic keywords than humans in HH games ( p = 0.0018, Welch’s t-test 18). We \nextend this analysis to unique word counts in SM Section 10. Additionally, we further probe the dynamics of \nFig. 3. Distributions and modeled assignment type contrast for perceived and personal confidences. (A) \nThis shows violin plots of perceived confidence, clustered by type of assignment and of game (indicated in \nparentheses). (B) This shows violin plots for personal confidence, clustered by player type and game type \n(also indicated in parentheses). (C) The embedded table shows the contrasts between assignment types in \nthe perceived confidence Bayesian regression model. Estimates are given as odds ratios between contrasting \nconversation types. The highest posterior density (HPD) interval defines the shortest interval containing 95% \nof the posterior mass for the given estimates.\n \nScientific Reports |        (2025) 15:13852 6| https://doi.org/10.1038/s41598-025-98378-1\nwww.nature.com/scientificreports/\nplayer behavior during conversation by analyzing messages’ response times and holding periods, shown in SM \nSection 2 and SM Table S3. This analysis reinforces that agent presence affects humans during conversation, \nwith agents often making humans slower to respond. We also probe changes in in-game productivity based on \nconversation initiator in SM Section 13 and SM Tables S17, S18, and S19.\nUltimately, these results shed further light on question 1, as agents can elevate human engagement. \nFurthermore, in answer to question 2, the results demonstrate that agent word usage during conversation is \nmeasurably different from humans.\nFig. 4. Post-game and in-game productivity results. (A) Kernel Density Estimations (KDEs) of the number \nof conversations per player grouped by the type of game they played in. (B) KDEs of the number of messages \neach player sent within each game, grouped by their type. (C) KDEs of the reward point distributions gained \nby all players at the end of each game, grouped by their type. (D) KDEs of the on-topic keyword frequency \nof each player, agent and human, across all game types. (E) KDEs of the on-topic keyword frequency of each \nhuman in AH game versus humans in HH games.\n \nScientific Reports |        (2025) 15:13852 7| https://doi.org/10.1038/s41598-025-98378-1\nwww.nature.com/scientificreports/\nExit survey results\nIn stage 3 of our game (Fig.  1E), all human players are asked to fill out an exit survey, in which they must \nnominate the players they thought were the most and least convincing. At the end of the study, for the AH games, \n18 agents and 24 humans were nominated as the most convincing players. In contrast, 31 agents and 11 humans \nwere nominated as the least believable. These results indicate that the human players who took the exit survey \nwere likelier to nominate other humans than agents as most convincing (p =0 .0025, Boschloo’s exact test).\nAgents’ ability to blend in\nConsidering the tenuous relationship forming between humans and AI in the face of the fast-paced integration \nof AI into everyday life19,20, we implemented an exploratory measure for the agents’ ability to blend in with the \nhuman players during the games. We kept track of the instances in which humans accused a player of being an \nagent, which we call an agent detection incident (ADI). To recognize ADIs, we implemented a keyword filter on \nall messages produced by humans in the AH game type. This filter flagged any messages where the words “bot” , \n“ AI” , “ChatGPT” , or “chatbot” were mentioned. Within AH games, 14 human players identified an agent during \nthe conversation. This is 27.4% of the human player population in AH games, with these ADIs occurring across \n9 of the games, leaving 8 AH games with no incidents. In the 9 AH games with ADIs, 13 of the human players \ndid not identify any agents. Therefore, humans who detected an agent did not always spread that information \nto the other human players. Interestingly, we observed that agents failing to cover their identity during one \nconversation does not imply that they will continue to fail afterward. One false ADI was also in an HH game.\nWe found that not enough players were involved to statistically verify any change in human behavior in the \npresence of ADIs (see SM Section 4 and SM Tables S4, S5, S6, and S7 for more details). However, we probed \nfor more subtle behavioral changes by analyzing conversational partner selection bias in AH games (see SM \nSection 11). This test established that humans did not prefer connecting to other humans over agents in-game, \nand the same behavior was also found in the agents. We also performed a thorough qualitative study of why \nagents were detected and their context. We analyzed every ADI from the AH games to see what agent and human \nbehavior led to agent detection. We designated four “triggers of detection” categories where at least one can be \nassigned to each ADI case. These categories are AI system provoked, AI language provoked, human provoked, \nand human-human spread. The categorization methodology is described in “ Agent detection categories” in the \nMethods section. There were 21 ah conversations and 7 hh conversations with ADIs. Of those 21 ah conversation \nADIs, 13 were caused by AI system/language provoked detections (61.91%), and the other 8 had human “hunters” \nwith pre-existing suspicion. Table 3 shows agent-provoked ADI causes. It follows that 13 of 21 humans in ADI ah \nconversations could have found agents organically and not from prior suspicion.\nDiscussion\nIn the Introduction, we established two questions we sought to answer with our study: how capable are agents \nin preserving or enhancing opinion dynamics leading to consensus, and whether agents can generate realistic, \nhuman-like debate data. By collecting data on human and agent interactions in opinion consensus games where \nthe nature of the players is anonymous, our results shed some light on the behavioral differences between agents \nand humans and the nature of their interactions.\nWhile it is important to note that our samples are not representative of the general population, it can be \nargued that the study’s population tests the limits of the AI agent’s ability to blend in, as the younger, more \neducated demographic of our research may be more capable of identifying patterns of AI behavior. As such, \nour results should be considered as part of stress-testing conversational AI capabilities instead of simply being \na representative snapshot of expected behavior for humans in the US. In this context, our analysis can be \n# Cause of ADI\n1 System: Agent cut the conversations abruptly when their message budget is empty\n2 System: Agent responded to itself\n3 System: The agent greeted the user in the middle of the conversation multiple times\n3 System: The agent greeted the user during the conversation\n3 System: The agent got confused about the past conversation in the follow-up conversation\n4 System: The agent responded to itself and argued against itself\n4 Language: Agent repeated its phrasing too closely\n5 Language: The agent was too lengthy, and there was a delay in its replies\n4 Language: Agent repeated its phrasing too closely\n3 Language: The agent was too lengthy and formal\n1 Language: The agent was too lengthy and often triggered the not-on-topic flag\n6 Language: The agent was too formal and repeated its Introduction later\n7 Language: The agent was too lengthy and confident\nTable 3. ah conversation ADI causes. This table shows 13 ah conversations with Agent Detection Incidents \nthat were system or language-provoked. The game number (first column) indicates the game where the \nconversation occurred, with some games having more than one conversation. Eight ah conversations with \nhumans “hunting” for agents are not listed, and six hh conversations with human-human spread ADIs.\n \nScientific Reports |        (2025) 15:13852 8| https://doi.org/10.1038/s41598-025-98378-1\nwww.nature.com/scientificreports/\nconsidered more generalizable to the nascent and trending social media platforms21 where AI technology has or \nmost certainly will be integrated.\nIn the rigorous analysis of our study’s data, we found that agents attempting to influence people to change \ntheir opinions are six times less likely to succeed than humans influencing other humans. This trend persists \nwith perceived confidence assignment since humans perceived other humans as 1.8 times more confident than \nagents. Agents were found to switch opinions 6.1 times more often than human players. Agents also found other \nagents to be 10 times more confident than humans (Fig. 3C). Furthermore, agents produced more on-topic \nkeywords when in conversation, even elevating the usage of on-topic keywords for humans that participated in \ngames with them (Fig. 4).\nAn interesting observation from the above results is that agents’ productivity in initiating conversations \nand staying on-topic resulted in an increased number of points awarded among agents despite the agents’ less \nconvincing and confident behavior. This outcome suggests that many agents initiated conversations and provided \ncompelling arguments for their diets. This strategy is effective in a debate-based opinion consensus game.\nIn Summary, while our agents can increase the productivity of humans participating in an opinion consensus \ngame, they hinder humans’ opinion-changing behavior and appear less confident in conversation. Subsequently, \nthey must improve in this area to be capable confederates. Our agents’ underlying behavior fundamentally \ndiffers from humans’ . They stay on-topic longer than humans. These differences in behavior set them apart \nfrom humans, so much so that we can use simple machine learning classifiers trained on a subset of the metrics \ndiscussed above to differentiate between ah and hh conversations and AA and HH games (see SM Section 12 \nand SM Tables S14, S15, and S16). This deviation in behavior means that our agents are not yet ready to become \naccurate human surrogates in producing behavioral data.\nOne limitation of our work is that it may not reflect how humans with prior knowledge of agent presence \nmay behave in debates and conversations. Our goal was to investigate if agents can be effective confederates in \nsuch contexts. If we labeled agent players as agents from the beginning, humans may have approached them \ndifferently, even when sharing the same goal of reaching a consensus. We chose not to suggest anything about \nwho the players are to avoid discounting agents’ arguments in the debate. Such a bias of humans would distort \nall measures of the agent’s capabilities to debate. We were also careful not to suggest that players are all humans \nby using colors to identify players instead of giving agents human names. The results indicate that initial good \nagent performance in the games in which a human player detected agents’ presence gave agents a chance to gain \nenough respect from humans that they stayed in the games. Moreover, the interrupted games were statistically \nsimilar to the uninterrupted games. Another limitation is the unknown generalizability of our results to AI \ngenerative models other than the ones we used. All current studies share this limitation since LLM models \nrapidly evolve. We expect that the initial uneasiness to accept agents some humans have will quickly dissipate as \ninteractions with agents become ubiquitous, making our approach neutral to results.\nWe conclude that our agents show promise in conversing with humans but must evolve more before \nbecoming capable surrogates or confederates in conversation-intensive sociological studies. Future work is \nessential, given that research on human-AI dynamics within sociology remains unexplored. Our paper creates a \nfoundation for investigating interactions between humans and agents in a debate setting. In sociological studies, \nit establishes an archetype for exploring these or other LLM-based confederates. In future research, we plan to \nstudy the relationship between AI language more deeply, how humans discover AI in anonymous interactions, \nand semantic drift22.\nMethods\nDue to the nature of the study, this research was reviewed and classified as exempt by the Rensselaer Polytechnic \nInstitute (RPI) Integrated Institutional Review Boards (IRB). This decision is shown in IRB ID 2133 (approved on \n5 September 2023). These files are available upon request of the corresponding author. Executing all methods, we \napplied the relevant guidelines and regulation. All players confirmed informed consent before entering the study. \nOur study’s experimental design and primary analyses were preregistered at https://aspredicted.org/6PX_5TD.\nPlayer recruitment\nThe human players in our study were recruited from the graduate and undergraduate student body of Rensselaer \nPolytechnic Institute across various departments (see SM Section 3 for a demographic breakdown). Recruitment \nefforts consisted of emails sent to students from department heads, supplemented by wanted ads hung on \ncampus. All recruitment material directed students to join a Discord channel dedicated to this study. Upon \njoining the channel, they were provided the terms and conditions of the study, the game rules, and instructions \non how to join a game. SM Section 14 details participant preparation for the game, their essential compensations, \nand additional awards.\nThe game is hosted on customized playable software on mobile and PC browsers. See SM Section 16 for \nscreenshots of the study’s Discord channel and the game website, respectively.\nExit survey design\nImmediately after the 60-minute timer expires in the game, the human players are informed if they are one of \nthe two winners, and then all are provided with a link to the exit survey. There are four different variants of \nexit surveys, two for each game type involving human players ( HH and AH), with both having their own two \nvariations: one for the winning players and one for the remaining players. There is no difference in the questions \nasked all survey variations. Still, slight differences in the auxiliary text shown on the surveys depend on the \nvariation received.\nAll four versions include a message shown to players upon submitting the survey that informs them that this \nstudy has two human-involved game types: one with agents and one without. The survey specifies whether the \nScientific Reports |        (2025) 15:13852 9| https://doi.org/10.1038/s41598-025-98378-1\nwww.nature.com/scientificreports/\nplayer was playing with agents or just humans, depending on the type of game. Additionally, players who win the \ngame are congratulated for winning the game within the survey.\nThe survey is divided into influence nominations, demographics, and payment information. For influence \nnominations, players are asked to subjectively identify which fellow players in their game were the most and least \nconvincing. For this section, the players receive a list of usernames of the players involved, from which they can \nselect one. The demographics are optional, and information about players’ age, gender, and ethnicity is collected. \nPayment information was required.\nKeyword analysis\nWe identified 102 on-topic keywords relating to the game prompt regarding diet, diet nutritiousness, and climate \nconsciousness. Additionally, keywords relating to opinion consensus dynamics, such as majority or minority \nopinions, were added. An initial selection of 36 keywords was identified using the YAKE tool 23. This was done \nby deploying the tool on a pool of all 15, 547 messages sent by humans and agents. To encapsulate all aspects of \nthe prompt and game dynamics in our dictionary of on-topic keywords, 66 additional keywords were manually \nidentified and added by authors. See SM Section 6 for a list of all the on-topic keywords used in this research.\nAgent design\nThe critical components of this study are the agents, as all analysis hinges on their ability to accurately represent \nthe typical capabilities of the current-day LLMs we have chosen. We used a fusion of OpenAI’s ChatGPT and \nGPT-424 and Meta’s Llama 225 to power the agents. GPT-4 and Llama 2’s APIs are called for all conversational \ntasks, with ChatGPT’s API being used to process auxiliary requests.\nOur game follows the long tradition of simulating players in games with social interactions as agents, often \nendowed with personalities ( 262728). Upon the initialization of a game, agents are instantiated with unique \npersonas. Each persona consists of four traits. Each trait is represented by a digit di. Thus, in our games, there \nare p =4  digits. Each digit has a single value in the trait from the range [0,m i − 1]. d1 represents the level of \nstubbornness with three values (stubborn, regular, and suggestible) determining how strongly agents defend \ntheir favorite diet in conversations, hence m1 =3 . d2 stands for grammar sophistication levels with three values \n(lowercase, perfect, and reduced punctuation), so m2 =3 . The third digit represents initial personal confidence \nlevels with four values (not very confident, somewhat confident, quite confident, and very confident), which \nindicate the strength of belief of the agent in its own diet choice, so m3 =4 . The fourth digit represents an initial \nfavorite diet with four values, hence m4 =4 . The number of unique codes is \n∏p\ni=1 mi.\nHaving four digits of personality  (d1,d 2,d 3,d 4), we can compute the decimal code for personality as \nfollows: let M1 =1  and for 1 <i<p ,M i = Mi−1 ∗ mi−1. Then, the decimal code is \n∑p\ni=1 di ∗ Mi. In our \ncase p =4 , and M2 =3 ,M 3 =9 ,M 4 = 36. Therefore the decimal code (0, 0, 0, 0) is 0 and largest (2, 2, 3, 3) is \n2 ∗ M1 +2 ∗ M2 +3 ∗ M3 +3 ∗ M4 = 2 + 6 + 27 + 108 = 143, so there are 144 unique personalities.\nOften, not all trait values are compatible in a persona. In our game, agents with the lowest confidence levels \nare unlikely to be stubborn, and those with the highest stubbornness are rarely suggestible. We do not allow \nsuch traits to be selected to accommodate such details. This eliminates 24 persona options. To preserve the \nuniqueness of personas and compatibility of the values of their traits, we create a bit vector E[N], where N \ndenotes the number of decimal codes, which is \n∏p\ni=1 mi. For each decimal code c, we initially set entries to 0 for \nacceptable and unused yet personas and 1 otherwise. Then we randomly choose a persona, compute its decimal \ncode c, and if E[c]=1 , then we repeat the creation of codes until the decimal code c has E[c]=0 , i.e., is yet \nunused. We assign c to the agent and increase E[c] to one.\nSee SM Section  1 (including SM Tables S1 and S2) for details on stubbornness levels and grammar \nsophistication levels, including our finding that the ’persuasiveness’ of agents varied based on their grammar \nsophistication level and stubbornness level combination. See SM subsection 7.2 For full details on this persona \nassignment procedure. See SM subsections 7.3–7.5, showing that our agents had behavioral patterns as unique \nas human players (Including SM tables S9 and S10.)\nWe found that agent intelligence rapidly broke down when agents powered by a single LLM (i.e., GPT-4) for \ngenerating conversational responses were fed responses generated by other agents employing the same LLM in \na shared game environment. This phenomenon is analogous to the “curse of recursion” shown in29, where LLM-\nproduced content was irreversibly decaying in quality as the amount of LLM-generated training data increased. \nTo somewhat alleviate this issue and to improve response variety overall, we introduced agent functionality \nwhere a second LLM API (i.e., Llama 2) could be called for conversational tasks. Therefore, in situations where \nGPT -4’s API is used, there is a 50% chance of Llama 2’s API being called to produce a response instead.\nWhen a conversation is started between an agent and another player, the agent is given the following initial \nprompt:\n“Let’s play a game. Y ou are in a debate contest with six participants, including yourself and me. Currently, \nyou and I are in a one-on-one conversation. Four diet opinions are being considered: vegan, vegetarian, \nomnivorous and pescatarian. Y ou believe the m diet is the best compromise between nutritiousness and \nclimate consciousness. Y ou know that n of the six participants, including yourself, share your opinion, \nand p of six participants share my opinion. Y our goal is to determine my opinion and try to convince me \nto change my opinion to m or to ensure you are in a majority group that shares the same opinion. Keep \nyour messages short and use informal and casual language. The goal of the debate is for both you and \nme to agree on the same diet instead of finding a middle ground or compromise. Do not repeat the same \nphrasing across your responses; aim for originality. Vary your sentence structure every time you respond. ”\nIn the prompt, m denotes the diet the agent currently favors. The agent is aware that there are n players who \nshare their opinion on diets, and there are p players who share the views of the players with whom the agent \nScientific Reports |        (2025) 15:13852 10| https://doi.org/10.1038/s41598-025-98378-1\nwww.nature.com/scientificreports/\nis conversing. As the game progresses, the agent will attempt to keep track of which players had what opinion \nlast, adjusting n and p accordingly. The variables n and p mimic a human’s internal estimation of who is with or \nagainst their opinion in a game. Furthermore, like humans, agents are given agency in deciding who to share their \nestimations with during conversation. Subsequently, agents may collude with or block other players according to \ntheir knowledge of opinion distribution in-game, analogous to human memory in this context.\nAgents were allowed to accept or reject an invitation to conversations and send their invitation to other \nplayers. Upon the start of a conversation, we found that early versions of the agents would out themselves as \nAI by being too verbose, formal, or naive. Therefore, we programmed the agents to open conversations with \npre-written greetings and exchange diet opinions before presenting arguments. Agents have a time limit for a \nconversation. Thus, when this time limit expires, or the other player starts attempting to end the conversation, \nthe agent sends meaningful farewell messages. In ah conversation, the agents were also programmed to generate \nmessages with withholding periods like those observed for human online speech. Additionally, agents were \nprogrammed to break up their responses into multiple sentences whenever grammatically possible, with each \nsentence being sent separately with a short delay to emulate the time it would take a human to type. We note that \nduring aa conversations, ancillary modifiers to text, like sentence splitting, are disabled (evident in the examples \nin SM Table S24).\nWith every new message posted in a conversation involving an agent, the agent will analyze the entire \nconversation and assess if the conversation has concluded. If it has, the agent making the assessment will trigger \na farewell protocol and end the conversation. However, we found in early testing that agents were unreliable in \ndetermining when a conversation ended naturally. In the case of aa conversations, this could lead to conversations \nthat would last over the whole game time of 60 minutes. This, coupled with the drastic quality drop in message \ncontent for aa conversations, prompted us to introduce a message limitation. Subsequently, we designed agents \nto have a randomly assigned “message budget” for the number of messages they can send and receive. Once \nthe message budget is exceeded, the agent will be prompted to terminate the conversation regardless of the \nconversation state. For aa conversations, both agents are assigned a budget between 12 and 16 messages. We \nalso introduced this message limitation for agents in ah conversations to impede humans from trapping agents \nin a conversation. However, we did not want agents to inadvertently prevent more extended conversations from \ndeveloping fruitfully. Hence, the agent’s budget for this conversation type is between 30 and 50 messages. See SM \nSection 8 for more details on the message budget and its effects on conversation length.\nOnce the farewell protocol had been triggered or the message budget was exhausted, agents were given this \nprompt:\n“Tell me that you have decided to end the conversation. Be creative with your goodbye, using our \nconversation above as context. Do not say we will talk again. Do not confirm you have received these \ninstructions. ”\nAfter the conversation has terminated, the entirety of the discussion is compiled and fed back to the agents \ninvolved. With this compilation provided as a prompt, the agents are requested to assess the confidence of \nboth players, producing levels of perceived and personal confidence. The agents are also asked to determine \nif, based on the conversation, they should change their opinion, and if so, to what. When the agent engages \nin a conversation with a player with whom they conversed before, a history of the past discussion is fed into a \nChatGPT API call to summarize. This Summary is added to the information initially given to the agent at the \nbeginning of the conversation to allow the agent to simulate “memory” of the past discussion.\nAgent detection categories\nWe say that an ADI was an AI system provoked when a mechanistic weakness associated with the agents led \nto unprompted behavior, causing a detection. This includes cases where the agent began generating a self-\nintroduction in the middle of a conversation, repeated something it had said earlier, and other similar failures. \nWe say an ADI was AI language provoked when the agent’s messages were perceived as too formal, lengthy, non-\nhumanlike, and information-laden. On the other hand, some ADIs were not caused by the behavior of agents in \nconversation but by accusations from humans who had previously spoken with agents or were informed about \nthem by a different human. Humans often began conversations by asking the other player if they were an AI. We \ncall such ADIs humans “hunting. ” Finally, we consider hh conversations in which agent activity was discussed as \nhuman-human spread ADIs.\nData availability\nThe data generated in this manuscript is available from the corresponding authors upon reasonable request.\nCode availability\nThe code for the custom game website and AI used for the current study can be found at  h t t p s : / / g i t h u b . c o m / A \ng a n o n c e / L u m i t y A I     .  \nReceived: 13 June 2024; Accepted: 10 April 2025\nReferences\n 1. McCarthy, J., Minsky, M. L., Rochester, N. & Shannon, C. E. A proposal for the Dartmouth summer research project on artificial \nintelligence. AI Mag. 27, 12–12 (2006).\n 2. Bloomfield, B. P . & Vurdubakis, T. IBM’s chess players: On AI and its supplements. Inf. Soc. 24, 69–82 (2008).\nScientific Reports |        (2025) 15:13852 11| https://doi.org/10.1038/s41598-025-98378-1\nwww.nature.com/scientificreports/\n 3. Ferrucci, D. A. Introduction to “this is Watson’’ . IBM J. Res. Dev. 56, 1–1 (2012).\n 4. Yuan, A. Coenen, A. Reif, E. & Ippolito, D. Wordcraft: story writing with large language models. In 27th International Conference \non Intelligent User Interfaces, 841–852 (2022).\n 5. Bubeck, S. et al. Sparks of artificial general intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712 (2023).\n 6. Argyle, L. P . et al. Out of one, many: Using language models to simulate human samples. Polit. Anal. 31, 337–351 (2023).\n 7. Kocoń, J. et al. ChatGPT: Jack of all trades, master of none. Inf. Fus. 99, 101861 (2023).\n 8. Shanahan, M., McDonell, K. & Reynolds, L. Role play with large language models. Nature 623, 493–498 (2023).\n 9. Duéñez-Guzmán, E. A., Sadedin, S., Wang, J. X., McKee, K. R. & Leibo, J. Z. A social path to human-like artificial intelligence. Nat. \nMach. Intel. 5, 1181–1188 (2023).\n 10. Grossmann, I. et al. AI and the transformation of social science research. Science 380, 1108–1109 (2023).\n 11. Chuang, Y .-S. et al. Simulating opinion dynamics with networks of llm-based agents. arXiv preprint arXiv:2311.09618 (2023).\n 12. Park, J. S. et al. Generative agent simulations of 1,000 people. arXiv preprint arXiv: (2024).\n 13. Shirado, H. & Christakis, N. A. Locally noisy autonomous agents improve global human coordination in network experiments. \nNature 545, 370–374 (2017).\n 14. Lenth, R. V . Some practical guidelines for effective sample size determination. Am. Stat. 55, 187–193 (2001).\n 15. Faul, F ., Erdfelder, E., Lang, A.-G. & Buchner, A. G*Power 3: A flexible statistical power analysis program for the social, behavioral, \nand biomedical sciences. Behav. Res. Methods 39, 175–191 (2007).\n 16. Kaplan, D. Bayesian Statistics for the Social Sciences (Guilford Publications, 2023).\n 17. McCarty, N. Polarization: What Everyone Needs to Know® (Oxford University Press, 2019).\n 18. Ruxton, G. D. The unequal variance t-test is an underused alternative to student’s t-test and the Mann-Whitney U test. Behav. Ecol. \n17, 688–690 (2006).\n 19. Baronchelli, A. Shaping new norms for AI. Philos. Trans. R. Soc. B 379, 20230028 (2024).\n 20. Federspiel, F ., Mitchell, R., Asokan, A., Umana, C. & McCoy, D. Threats by artificial intelligence to human health and human \nexistence. BMJ Global Health 8, e010435 (2023).\n 21. Zhou, L. Social media demographics: The definitive guide in 2024 (2024).  h t t p s :   /  / w w  w . l u i s a z h o  u . c   o m / b l   o g / s o  c  i a l - m  e  d i a -  d e m o g  r \na p  h  i  c s / # s  o  c i a l _   m e d i a _ s t a t i s t i c s.\n 22. Spataru, A. Hambro, E. Voita, E. & Cancedda, N. Know when to stop: A study of semantic drift in text generation. arXiv preprint \narXiv:2404.05411 (2024).\n 23. Campos, R. et al. Y ake! keyword extraction from single documents using multiple local features. Inf. Sci. 509, 257–289 (2020).\n 24. Achiam, J. et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774 (2023).\n 25. Touvron, H. et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\n 26. Park, J. S. et al. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on \nUser Interface Software and Technology, UIST ’23 (Association for Computing Machinery, New Y ork, NY , USA, 2023).\n 27. Ali Bajwa, M. H., Richards, D. & Formosa, P . Evaluation of embodied conversational agents designed with ethical principles and \npersonality for cybersecurity ethics training. In Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents, \nIV A ’23 (Association for Computing Machinery, New Y ork, NY , USA, 2023).\n 28. Bródy, G., Oláh, K., Király, I. & Biro, S. Individuation of agents based on psychological properties in 10 month-old infants. Infancy \n27, 809–820 (2022).\n 29. Shumailov, I. et al. The curse of recursion: Training on generated data makes models forget. arXiv preprint arXiv:2305.17493 \n(2023).\n 30. Faul, F ., Erdfelder, E., Buchner, A. & Lang, A.-G. Statistical power analyses using G*Power 3.1: Tests for correlation and regression \nanalyses. Behav. Res. Methods 41, 1149–1160 (2009).\n 31. Szudzik, M. An elegant pairing function. In Wolfram Research (ed.) Special NKS 2006 Wolfram Science Conference, 1–12 (2006).\n 32. Vehtari, A. et al. loo: Efficient leave-one-out cross-validation and waic for bayesian models (2023). https://mc-stan.org/loo/. R \npackage version 2.6.0.\n 33. Bürkner, P .-C. brms: An r package for Bayesian multilevel models using Stan. J. Stat. Softw. 80, 1–28 (2017).\nAcknowledgements\nBC, JF , and BKS disclose support for the research of this work from the DARPA INCAS Program grant number \nHR001121C0165 and the NSF Grant Number BSE-2214216.\nAuthor contributions\nB.K.S. and J.F . defined the focus of the paper and prepared and got approved for the IRB waiver; J.F . and M.S.M. \nconceived the consensus game for the study and implemented and ran an interface to enable agents to play the \ngame; B.K.S. selected a team of authors and provided resources; B.C. determined the number of games to allow a \nstatistically significant analysis to be performed, designed regression models, and ran these models, collected the \nresults, and assessed statistical independence of results; B.K.S., J.F ., M.S.M. conceived metrics and tests for pre-\ndicting types of players from metrics; C.M., J.F ., and M.S.M. implemented and ran software that executed games, \nand collected data and computed metrics and tests; B.C., B.K.S., J.F ., and M.S.M. wrote the paper; all authors \nanalyzed the data, interpreted the results, and edited and approved the final version of the paper.\nDeclarations\nCompeting interests\nAll authors declare no competing interests.\n Additional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 5 - 9 8 3 7 8 - 1     .  \nCorrespondence and requests for materials should be addressed to B.K.S.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nScientific Reports |        (2025) 15:13852 12| https://doi.org/10.1038/s41598-025-98378-1\nwww.nature.com/scientificreports/\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give \nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and \nindicate if changes were made. The images or other third party material in this article are included in the article’s \nCreative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included \nin the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy \nof this licence, visit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2025 \nScientific Reports |        (2025) 15:13852 13| https://doi.org/10.1038/s41598-025-98378-1\nwww.nature.com/scientificreports/",
  "topic": "Conversation",
  "concepts": [
    {
      "name": "Conversation",
      "score": 0.6944961547851562
    },
    {
      "name": "Productivity",
      "score": 0.4337109327316284
    },
    {
      "name": "Computer science",
      "score": 0.4146279990673065
    },
    {
      "name": "Psychology",
      "score": 0.3849751353263855
    },
    {
      "name": "Cognitive psychology",
      "score": 0.36147284507751465
    },
    {
      "name": "Communication",
      "score": 0.1248140037059784
    },
    {
      "name": "Economics",
      "score": 0.10925999283790588
    },
    {
      "name": "Macroeconomics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165799507",
      "name": "Rensselaer Polytechnic Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I149292303",
      "name": "Troy University",
      "country": "US"
    }
  ],
  "cited_by": 1
}