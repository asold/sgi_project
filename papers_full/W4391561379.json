{
  "title": "Leveraging large language models for predictive chemistry",
  "url": "https://openalex.org/W4391561379",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5027355573",
      "name": "Kevin Maik Jablonka",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5028051805",
      "name": "Philippe Schwaller",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5000326701",
      "name": "Andres Ortega‐Guerrero",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5075317126",
      "name": "Berend Smit",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6800751262",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4385572894",
    "https://openalex.org/W3198449425",
    "https://openalex.org/W4319996831",
    "https://openalex.org/W6849941170",
    "https://openalex.org/W4311409687",
    "https://openalex.org/W4386168831",
    "https://openalex.org/W4385671288",
    "https://openalex.org/W4283031227",
    "https://openalex.org/W3030978062",
    "https://openalex.org/W2953641512",
    "https://openalex.org/W4362664882",
    "https://openalex.org/W4379184641",
    "https://openalex.org/W3030068589",
    "https://openalex.org/W4281619372",
    "https://openalex.org/W4318952054",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3023402054",
    "https://openalex.org/W3023937119",
    "https://openalex.org/W2947423323",
    "https://openalex.org/W4312129726",
    "https://openalex.org/W4385571886",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W3009321976",
    "https://openalex.org/W4306179830",
    "https://openalex.org/W2883583109",
    "https://openalex.org/W3118507387",
    "https://openalex.org/W2529996553",
    "https://openalex.org/W2997100726",
    "https://openalex.org/W2908350418",
    "https://openalex.org/W2977044154",
    "https://openalex.org/W2530805533",
    "https://openalex.org/W3163360581",
    "https://openalex.org/W2554191423",
    "https://openalex.org/W4311281379",
    "https://openalex.org/W2160592148",
    "https://openalex.org/W3156578609",
    "https://openalex.org/W2911997094",
    "https://openalex.org/W3175955239",
    "https://openalex.org/W4282053982",
    "https://openalex.org/W4319310661",
    "https://openalex.org/W4226050570",
    "https://openalex.org/W2900694120",
    "https://openalex.org/W6913078555",
    "https://openalex.org/W6967534406",
    "https://openalex.org/W2789949436",
    "https://openalex.org/W2955219525",
    "https://openalex.org/W3116783766",
    "https://openalex.org/W2887447356",
    "https://openalex.org/W3008588639",
    "https://openalex.org/W2905012389",
    "https://openalex.org/W2116105292",
    "https://openalex.org/W4214868967",
    "https://openalex.org/W3128429991",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W3048908832",
    "https://openalex.org/W4305016511",
    "https://openalex.org/W4226145240",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2008505552",
    "https://openalex.org/W2076498053",
    "https://openalex.org/W1975875968",
    "https://openalex.org/W2096541451",
    "https://openalex.org/W2800793736",
    "https://openalex.org/W2790960441",
    "https://openalex.org/W2785942661",
    "https://openalex.org/W2784918212",
    "https://openalex.org/W3181860256",
    "https://openalex.org/W3103092523",
    "https://openalex.org/W3045928028",
    "https://openalex.org/W3100220443"
  ],
  "abstract": null,
  "full_text": "Nature Machine Intelligence | Volume 6 | February 2024 | 161–169\n 161\nnature machine intelligence\nhttps://doi.org/10.1038/s42256-023-00788-1\nArticle\nLeveraging large language models for \npredictive chemistry\nKevin Maik Jablonka1,2,3,4, Philippe Schwaller    5, Andres Ortega-Guerrero    1 & \nBerend Smit    1 \nMachine learning has transformed many fields and has recently found \napplications in chemistry and materials science. The small datasets \ncommonly found in chemistry sparked the development of sophisticated \nmachine learning approaches that incorporate chemical knowledge for \neach application and, therefore, require specialized expertise to develop. \nHere we show that GPT-3, a large language model trained on vast amounts \nof text extracted from the Internet, can easily be adapted to solve various \ntasks in chemistry and materials science by fine-tuning it to answer chemical \nquestions in natural language with the correct answer. We compared this \napproach with dedicated machine learning models for many applications \nspanning the properties of molecules and materials to the yield of chemical \nreactions. Surprisingly, our fine-tuned version of GPT-3 can perform \ncomparably to or even outperform conventional machine learning \ntechniques, in particular in the low-data limit. In addition, we can perform \ninverse design by simply inverting the questions. The ease of use and high \nperformance, especially for small datasets, can impact the fundamental \napproach to using machine learning in the chemical and material sciences. \nIn addition to a literature search, querying a pre-trained large language \nmodel might become a routine way to bootstrap a project by leveraging the \ncollective knowledge encoded in these foundation models, or to provide a \nbaseline for predictive tasks.\nOne of the fascinating advances in machine learning has been the \ndevelopment of large language models (LLMs), so-called foundation \nmodels1–6. These models are appealing because of their simplicity; given \na phrase, they return text that completes phrases in natural language \nsuch that, in many instances, one cannot tell that a machine wrote it.\nFrom a scientific point of view, the most striking examples \nare that these foundation models can write sensible abstracts \nfor scientific articles or even code for particular programming \ntasks 7–12. Recently, it has been shown that these models can also \nsolve relatively simple tabular regression and classification tasks 13. \nHowever, as these models were not explicitly trained on these tasks, \nit is a remarkable result 5.\nThat these models can solve simple tasks they are not trained for \nmade us wonder whether they can also answer scientific questions \nfor which we do not have an answer. As most chemistry problems can \nbe represented in text form, we should be able to train these mod -\nels to answer questions that chemists have. For example, ‘If I change \nthe metal in my metal–organic framework, will it be stable in water?’  \nReceived: 16 May 2023\nAccepted: 22 December 2023\nPublished online: 6 February 2024\n Check for updates\n1Laboratory of Molecular Simulation (LSMO), Institut des Sciences et Ingénierie Chimiques, École Polytechnique Fédérale de Lausanne (EPFL), Sion, \nSwitzerland. 2Center for Energy and Environmental Chemistry Jena (CEEC Jena), Friedrich Schiller University Jena, Jena, Germany. 3Laboratory of Organic \nand Macromolecular Chemistry (IOMC), Friedrich Schiller University Jena, Jena, Germany. 4Helmholtz Institute for Polymers in Energy Applications,  \nJena, Germany. 5Laboratory of Artificial Chemical Intelligence (LIAC), École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland. \n e-mail: berend.smit@epfl.ch\nNature Machine Intelligence | Volume 6 | February 2024 | 161–169 162\nArticle https://doi.org/10.1038/s42256-023-00788-1\nmetals will form a solid solution or multiple phases. Hence, the ques-\ntion we would like to ask is: ‘What is the phase of <composition of the \nhigh-entropy alloy>?’ and our model should give a text completion \nfrom the set of possible answers {single phase, multi-phase}.\nIn Extended Data Table 1, we provide the set of questions and \nanswers we used to fine-tune the GPT-3 model. These are questions \nand answers on high-entropy alloys for which the phase has been \nexperimentally determined. The model tuning via the OpenAI API \ntypically takes a few minutes and gives us a new model, which takes \nas input ‘Sm0.75Y0.25’ and gives as text completion ‘1’ , which cor -\nresponds to single phase. This simple example already gives some \nremarkable results. We selected this example to directly compare its \nperformance with the current state-the-art machine learning models \nwith descriptors specially developed to mimic the relevant chemistry \nfor this application24. In Fig. 2, we show that with only around 50 data \npoints, we get a similar performance to the model of ref. 24, which was \ntrained on more than 1,000 data points.\nClassification\nThese results made us wonder whether similar results can be obtained \nfor other properties. Hence, we looked at a range of very different prop-\nerties of molecules, materials and chemical reactions. We focused on \nthose applications for which conventional machine learning methods \nhave been developed and generally accepted as benchmarks in their \nfield. In addition, we also compared our model with the top-performing \nones on tasks from the Matbench25 suite of benchmarks (Supplemen-\ntary Note 6.15).\nExtended Data Table 2 compares the performance of a fine-tuned \nGPT-3 model with baselines (which can be found in Supplementary Note \n6). For doing so, we fit the learning curves for the GPT-3 models and for \nthe baselines and measure where they intersect, that is, we determine \nthe factor of how much more (or fewer) data we would need to make \nthe best baseline perform equal to the GPT-3 models in the low-data \nregime of the learning curves. The full learning curves for all models \ncan be found in Supplementary Information (Supplementary Note 6).\nFor molecules, we investigated properties ranging from gaps \nbetween highest occupied (HOMO) and lowed unoccupied (LUMO) \nmolecular orbitals and solubility in water to the performance in organic \nphotovoltaics. For materials, we focused on the properties of alloys, \nmetal–organic frameworks and polymers. Finally, for reactions,  \nSuch questions are often impossible to answer using theory or require \nhighly sophisticated simulations or experiments.\nWe will always have very little (experimental) data for chemistry \nand material science applications. Hence, it is important that meaning-\nful results can already be obtained with tens to hundreds of data points. \nWe know from previous work on applications on text classification or \ngeneration that this works particularly well using models from the \nGenerative Pre-trained Transformer 3 (GPT-3) family 5, which were \ntrained by the artificial intelligence company OpenAI. In this work, we \nshow that these models—when provided with example data—perform \nsurprisingly well for various chemistry questions, even outperforming \nthe state-of-the-art machine learning models specifically developed \nfor these tasks. It is important to realize that while language mod -\nels have been used in chemistry before to predict properties 14–17 or \ndesign molecules18–20, they have conventionally been pre-trained on \nchemistry-specific tasks. In contrast, the models we investigate here \nhave been trained on text corpi compiled mainly from the Internet \nbut still can adapt to various tasks. Although ref. 8  has probed the \ninherent chemistry knowledge of LLMs, we focus on how those models \nperform when they are fine-tuned—that is, the weights are updated—on  \nsome task-specific dataset. Note that this task-specific fine-tuning \nmakes the models less dependent on the prompt structure than \nin-context learning21,22.\nWe benchmark our model on various datasets and applications to \nillustrate that these models can answer a wide range of scientific ques-\ntions—ranging from the properties of materials, to how to synthesize \nmaterials and how to design materials (Fig. 1). In selecting these ques-\ntions, we included some that have been addressed with machine learn-\ning. This allowed us to benchmark against state-of-the-art machine \nlearning approaches specifically developed for these applications.\nLanguage-interfaced fine-tuning for \nclassification and regression\nApproach\nBefore discussing the different applications in detail, let us first discuss \nhow we fine-tune23 the GPT-3 model in practice for a simple but highly \nnon-trivial example. High-entropy alloys have attracted much inter-\nest as a novel class of structural metals. Interestingly, one has a sheer \ninfinite number of possible combinations of metals. From a practical \npoint of view, it is important to know whether a given combination of \nDatasets Tasks\n‘What is the \ntransition wavelength of \n2-phenyldiazenylaniline’\n‘What is the lipophilicity \nof COc1cc (N2CCN (C) CC2) c3nc\n(cc (N (C) C) c3c1) C (=O) Nc4ccc\n(cc4) N5CCOCC5?’\n‘What is a molecule \nwith E isomer transition \nwavelength of 325 nm,\nZ isomer transition \nwavelength of 286 nm?’\nMolecules\nMaterials\nReactions\nCl\nNN\n‘Low‘\nClassi/f.shortication Regression Inverse design\n3.3\nGPT-3 GPT-3 GPT-3\nNO\nCl\nN\nP\nO\nO\nO\nN N\nN\nN\nNN NH2\nN\nO\nN\nH\nN\nO N\nON\nN N N\nN\nFig. 1 | Overview illustration of the datasets and tasks addressed in this work. \nIn this work, we benchmark GPT-3 on datasets spanning the chemical space from \nmolecules over materials to reactions (Supplementary Note 1). On these datasets, \nwe investigate different tasks ranging from classification, that is, predicting \na class (for example, ‘high’ , ‘low’) given a text representation of a molecule, \nmaterial or reaction, to regression, that is, prediction of floating point numbers, \nto inverse design—the prediction of molecules. Metal–organic framework \nrendering created with iRASPA\n60.\nNature Machine Intelligence | Volume 6 | February 2024 | 161–169\n 163\nArticle https://doi.org/10.1038/s42256-023-00788-1\nwe considered two key cross-coupling reactions in organic chemistry. \nExtended Data Table 2 shows that in the low-data regime, our GPT-3 \nmodel is typically at least as good as the conventional machine learn-\ning model and often needs fewer data. In the high-data regime, the \nconventional machine learning models often catch up with the GPT-3 \nmodel. This makes sense, as for a given size of the dataset, the need \nfor additional data and correlations (inductive biases)26 captured by \nGPT-3 might be less needed.\nWe have to mention that we did not optimize the fine-tuning of \nthe GPT-3 model, that is, we did not try to optimize how a sentence is \npresented to the model; one can envision that specific tokenization \ncan have better results for chemical sentences9,16,27,28. Also, we did not \ntune the number of times we show an example to a model (that is, the \nnumber of epochs or the learning rate).\nBeyond fine-tuning of OpenAI models\nImportantly, we are also not limited to fine-tuning; in Supplementary \nNote 5, we show that we can even achieve good performance with -\nout fine-tuning by incorporating examples directly into the prompt \n(so-called in-context learning 5,29, that is, learning during inference \ntime). This works particularly well with the largest GPT-3 models and \nGPT-4. We are also not limited to using models from OpenAI. In Supple-\nmentary Notes 7 and 8, we also show that we could obtain good results \nby fine-tuning the open-source LLM’s parameter-efficient fine-tuning \ntechniques on consumer hardware and provide a Python package that \nmakes it easy to apply this approach to new problems.\nRepresentation sensitivity\nAn interesting question is how to represent a molecule or material. Most \nof the literature reports use International Union of Pure and Applied \nChemistry (IUPAC) names. For machine learning applications, there has \nbeen a lot of effort to represent a chemical with unique line encodings \n(for example, simplified molecular-input line-entry system (SMILES) 30  \nor self-referencing embedded strings (SELFIES) 31,32). As the GPT-3 \nmodel has been trained on natural text, one might expect that chemi-\ncal names are preferred over line representations such as SMILES or \nSELFIES. Therefore, we investigated different representations for our \nmolecular property prediction tasks (see also Supplementary Note 4).  \nInterestingly, our results (Supplementary Note 6) show that good \nresults are obtained irrespective of the representation. The fact that \nwe often get the best performance using the IUPAC name of the mol-\necule makes fine-tuning GPT-3 for a particular application relatively \nsimple for non-specialists.\nRegression\nA more challenging task than classification is to make a regression \nmodel, which would allow us to predict the value of a continuous prop-\nerty such as the Henry coefficient for the adsorption of a gas in a porous \nmaterial. As we are using a pre-trained language model, performing \nactual regression that predicts real numbers (∈ℝ ) is impossible (with-\nout changes to the model architecture and training procedure). How-\never, in most, if not all, practical applications, the accuracy for which \nwe can make predictions is always limited. For example, for the Henry \ncoefficient of a material, an accuracy of 1% (or a certain number of \ndecimal points) is sufficient for most applications (see Supplementary \nNote 10 for discussion on this error source). Hence, we use molecules \nwith Henry coefficients rounded to this accuracy as a training set and \nassume that the GPT-3 model can interpolate these numbers. Of course, \none could also convert this into a classification problem by making \ntiny bins. For this more challenging regression task, we need more \ndata for tuning the GPT-3 model, and we still get a performance that \ncan approach the state of the art, but as this approach requires much \nmore data, the advantage, except for the ease of training, is less. We \nobtain a similar conclusion for other regression problems (see Sup -\nplementary Note 10) and imbalanced classification cases (Supple -\nmentary Note 6.8).\nInverse design\nOne can argue that the ultimate goal of machine learning in chemistry is \nto create a model that can generate molecules with a desired set of prop-\nerties. This is also known as inverse design33. Broadly speaking, there \nare two approaches. If we have large datasets, we can train generative \nmodels such as variational autoencoders34,35 or generative adversarial \nneural networks36,37. Without large datasets, evolutionary techniques \nsuch as genetic algorithms can generate novel, potentially interesting \nmolecules38–41. Those evolutionary methods work best if one can limit \nthe underlying chemistry; for example, finding the optimal functional \ngroup on a material with a well-defined backbone42.\nGiven that the GPT-3 model can predict the properties of mol -\necules and materials with a small dataset, trying an inverse design \nstrategy is tempting. This would be particularly important in the early \nstages of research; one often has a small set of experimental data points \nand a limited understanding. Yet, we could leverage a fine-tuned GPT-3 \nmodel to generate suggestions for novel materials with similar or even \nbetter performance. This would be an important step forward. Par -\nticularly as the tuning of such a natural language model is much more \naccessible than the training of conventional machine learning models. \nHere we investigate this setting: Can a fine-tuned GPT-3 propose valid \nmolecules that satisfy the constraints or desired properties specified \nin a prompt in natural language? Again, we are illustrating the potential \nfor a few case studies.\nMolecular photoswitches are organic molecules with extended \naromatic systems that make them responsive to light. Upon radiation, \nthey switch reversibly between different isomers (which changes some \nproperties, such as dipole moments). This reversible switching makes \nthem interesting molecules for applications ranging from sensing to \ndrug discovery. These molecules are complex, making sufficiently \naccurate predictions using first-principles theory very expensive. \n0.9\nAccuracy\nGPT-3 (non-Google test set)\nGPT-3\nRF\nAutomatminer\nCrabNet\n0.8\n0.7\n0 50 100 150 200\nNumber of training points\nFig. 2 | Accuracy of our GPT -3 model for predicting solid-solution \nformation in high-entropy alloys. The figure compares the model’s accuracy \nas a function of the number of training points. The dashed horizontal line \nindicates the performance reported in ref. 24  using random forest (RF) with \na dataset of 1,252 points and 10-fold cross-validation, that is, corresponding \nto a training set size of around 1,126 points. The dotted line shows the \nperformance of a simple rule-based baseline ‘if present in the composition, \nclassify as single phase, else multi-phase’ . The yellow line we obtained using \nthe Automatminer\n25, which uses as input the chemical composition. The \nAutomatminer then returns the best featurization and model among those \nthat are implemented using automated machine learning with genetic \nprogramming (as implemented in the TPOT package\n61). We additionally tested \na neural network, CrabNet (red line, default settings) 62, that performs well \nusing compositions as input. The blue line is the performance of our GPT-3 \nmodel (with error bands showing s.e.m.). This figure shows that we reach \nsimilar accuracy to the model of ref. 24  with as little as around 50 data points. \nIn addition, we also investigated a separate training and test set, for which the \nlearning curve is shown in green. In this case, we tested on only compounds \nfor which we could not find an exact match with a Google search. The learning \ncurves for other metrics can be found in Supplementary Note 6.13.\nNature Machine Intelligence | Volume 6 | February 2024 | 161–169 164\nArticle https://doi.org/10.1038/s42256-023-00788-1\nYet, it is important to have some guidance to identify promising mol-\necules, and machine learning models have been developed for this.  \nOne of the important properties of these photoswitches is the wave-\nlength at which there is a maximum in the adsorption spectrum for the \nE and Z isomers. Hence, we fine-tuned GPT-3 with the same data used by \nref. 43. As we have shown above, we can fine-tune GPT-3 to accurately \nanswer questions like ‘What is the pi–pi* transition wavelength of \nCN1C(/N=N/C2=CC=CC=C2)=C(C)C=C1C?’ .\nFor GPT-3, inverse design is as simple as training the model with \nquestion and completion reversed. That is, answer the question ‘What \nis a photoswitch with transition wavelengths of 324 nm and 442 nm, \nrespectively’ with a text completion that should be a SMILES string of \na meaningful molecule. This approach should be contrasted with the \napproach used by ref. 43, in which a library of molecules is generated, \nand their machine learning model (a Gaussian process regression) is \nused to evaluate the transition wavelengths of each material. If one \nhas a lot of knowledge about the system, one can design large specific \nlibraries that contain many promising molecules, including molecules \nwith transition wavelengths of 324.0 nm and 442 nm. But, such a brute \nforce technique is not what we understand as inverse design, as it, \nby definition, cannot predict a molecule that we did not include in  \nour library.\nA simple test to see whether our model can generate new struc -\ntures is to ask it to generate molecules with transition wavelengths \nsimilar to those from the dataset reported by ref. 43 . Extended Data \nFig. 1 shows a representative sample of the molecules generated by \nthe model. As expected, many molecules come from the training set \n(coloured orange in the figure). Importantly, many molecules are not in \nthe training set, and, interestingly, some are not even in the PubChem \ndatabase of known chemicals. In Fig. 3, we show that for the molecules, \nthe transition wavelength is within a mean absolute percentage error \nof around 10%. Note that as the Gaussian process regression (GPR) \nmodel of ref. 43 was shown to perform comparably to, if not better \nthan, more costly density functional theory simulations, we chose to \nuse their model to compute the transition wavelengths for the gener-\nated molecules.\nIt is interesting to quantify how novel our newly generated mol -\necules are. We compare these molecules to those collected in ref. 43. We \nquantify the similarity by computing the distance between molecular \nfingerprints. Figure 4 visualizes this by laying out the resulting approxi-\nmate nearest-neighbour graph in two dimensions. The orange and \ngreen spheres represent molecules from the ref. 43 dataset, the blue \nspheres show the novel ones, and the pink ones are not part of the \nPubChem database. As expected, we find many new structures that \nare derivatives of molecules in the ref. 43 database. However, we also \nfind branches that are not part of the library of ref. 43, indicating that \nthe model generated novel kinds of compounds.\nIn generating these molecules, we adjusted the so-called softmax \ntemperature in the sampling step of GPT-3 models. This temperature \nis conventionally used to generate more natural text. If we set this \ntemperature to zero, we will generate text with the most frequently \nused words. We can increase the temperature to make the text more \nnatural, making it more likely that less commonly used synonyms \nare chosen. For chemistry, if we aim to complete a SMILES starting \nwith carbon, the zero-temperature solution would always complete \nthe symbol that most commonly follows carbon (‘(’ in the QMugs \ndataset). In contrast, too-high temperatures would randomly choose  \nany element.\nThe impact of this temperature parameter is shown in Fig. 3. At low \ntemperatures, the generated molecules often come from the training \nset and only show a low diversity. Across all temperatures, the gen -\nerated molecules seem synthesizable, as judged by a low synthetic \naccessibility (SA) score44. Increasing the temperature gives us more \ndiverse and novel structures, but one can also expect more structures \nthat make no chemical sense, that is, are invalid.\nStretching the limits\nThe results on the photoswitches illustrate the potential of LLMs for \nchemistry. T o obtain more insight into whether we can trust these \nGPT-3 predictions, we carried out some experiments where we tried \nto stretch the limits.\nWe have already seen that we can obtain good results independent \nof how we represent a molecule (IUPAC names, SMILES or SELFIES), \nbut can GPT-3 interpret an abstract representation of molecules we \ninvented? A previous study45 developed a machine learning approach \nto design dispersants using a coarse-grained approach. This dispersant \nwas a linear copolymer with four monomer types and a chain length \nbetween 16 and 48 units, giving a chemical design space of 58 million \ndifferent dispersants. One important goal in this work was to find \ndispersants with the right binding free energy, that is, which poly -\nmer length and which monomer sequence is optimal. As there is no \n1.0\n1.0\n0.5\n0\n40\n20\n20\n50\n0\n50\n0\n0 2 0 2\n0\n20\n0\n0\n0\nAll Novel\n0.5 1.0 1.5\nFraction of valid\n SMILESFraction in trainFréchet distance\nZ isomer\n MAE (nm)\nE isomer\nMAE (nm)\nTemperature\nTemperature\n0.5\nFig. 3 | Photoswitch inverse design metrics as a function of temperature. \nThe fraction of valid SMILES indicates the fraction of generated SMILES that \ncan successfully be parsed using RDKit (note that it does not plateau at 0, but \napproximately 0.1)\n63. We then determine the fraction of those that are already \npart of the training set and find that at low temperature GPT-3 tends to restate \nmolecules from the training set. T o quantitatively capture the similarity of the \ndistribution of the generated molecules to the ones from the training set, we \ncompute the Fréchet ChemNet distance\n64, which quantifies both diversity and \ndistribution match51 and goes through a minimum at intermediate temperatures. \nFor quantifying how well the generated molecules match the desired transition \nwavelengths, we use the GPR models reported by ref. 43 to predict the transition \nwavelengths. The dashed horizontal lines indicate those models’ mean absolute \nerror (MAE). Across all temperatures, we found high average synthesizability \n(synthetic accessibility, SA, score\n44 smaller than 3). Error bands indicate s.e.m.\nNature Machine Intelligence | Volume 6 | February 2024 | 161–169\n 165\nArticle https://doi.org/10.1038/s42256-023-00788-1\nway the GPT-3 model knows about the properties or representations \nof the coarse-grained polymers, it is interesting to see if we can get \nany sensible result if we ask the question ‘What is the adsorption free \nenergy of coarse-grained dispersant AAAABBBBDDDDAAAACCCC’ or \nas inverse design, ‘Give me a structure of a coarse-grained dispersant \nwith a free energy of 17’ . Interestingly, for the prediction of the adsorp-\ntion free energy, the GPT-3 model outperforms the models developed \nby ref. 45. In addition, it can also successfully carry out the inverse \ndesign and generate monomer sequences that give the desired com-\nposition and, with a mean percentage error of around 22%, the desired \nadsorption free energy (the approximation of the ground truth we use \nalready has a mean percentage error of around 9%, see Supplementary  \nNote 11.1 for details).\nIn the case of the photoswitches, we have seen that the GPT-3 \nmodel can generate new molecules that are quite different from the \ntraining set. T o explore in detail how far we can stretch the limits of \nwhat new molecules we can generate, we choose an application for \nwhich quantum calculations are known to predict the experimental \nvalues sufficiently accurately. The HOMO–LUMO gap is such an appli-\ncation. The HOMO–LUMO gap is relevant, for instance, in electronic \napplications that aim to excite a molecule at a specific energy. This  \nHOMO–LUMO gap can be predicted accurately using semi-empirical \nquantum mechanics (GFN2-xTB46), which is computationally afford-\nable enough for us to compute for all generated molecules (Supple -\nmentary Note 77). Moreover, the QMugs dataset 47,48 has listed these \nHOMO–LUMO calculations for 665,000 molecules.\nIn Supplementary Note 11.3, we show that with the training of only \n500 samples, we can get a reasonable estimate of the HOMO–LUMO \ngap of the molecules in the QMugs dataset. Also, by reverting the ques-\ntion, we have our model trained for inverse design. In Supplementary \nNote 11.3, we show that by asking the model ‘What is a molecule with a \nHOMO–LUMO gap of 3.5 eV’ , we get similar to the photoswitches—a set \nof novel molecules. These novel molecules are not part of our training \nset and not even part of the QMugs dataset.\nWe now conduct some experiments on a dummy task to test how \nwell the GPT-3 model can extrapolate to HOMO–LUMO gaps for which \nit has not received any training. T o mimic this situation, we retrained \nour inverse design model using a dataset that has only molecules with \nHOMO–LUMO gaps smaller than 3.5 eV, and subsequently query the \nmodel with a question that requires the GPT-3 model to extrapolate \n(and, for example, to find that very small molecules are associated with \nlarge HOMO–LUMO gaps; a task we selected for only demonstration \npurposes and that can be exploited by generating small molecules).  \nWe do this by asking more than 1,000 times the question: ‘What is a mol-\necule with a HOMO–LUMO gap of <XX>’ , where each time we slightly \nchange the value of the HOMO–LUMO gap, that is, we sample XX from \na Gaussian centred at 4 eV. Interestingly, the GPT-3 model does pro-\nvide structures with a distribution of which our quantum calculations \nconfirm that a meaningful fraction has a HOMO–LUMO gap >4.0 eV. \nAgain, this is a remarkable result. In our training set, there was not a \nsingle molecule with a bandgap >3.5 eV, which shows that the GPT-3 \nmodel can make extrapolations. We can do a similar experiment for \nthe photoswitches, for which we might have a library of photoswitches \nwhose transition wavelengths are all below 350 nm. For practical appli-\ncations, however, it can often be essential to have adsorption at larger \nwavelengths. In this case, we can successfully use a fine-tuned GPT-3 \nmodel to generate photoswitch molecules that adsorb at lower energy \n(Supplementary Fig. 75, which we also validated with time-dependent \ndensity functional theory in Supplementary Note 11.2.2).\nThese findings inspired us to do an inverse design experiment to \ndesign molecules with properties that take us far from the training set49. \nWe are interested in molecules that have a HOMO–LUMO gap >5 eV. \nFrom the distribution of HOMO–LUMO gaps in the QMugs database \n(Fig. 5), we see that the average bandgap is around 2.58 eV. Only a hand-\nful of molecules in this database have a HOMO–LUMO gap above 5 eV.\nHence, this is a challenging inverse design problem, as only a few \nmaterials in the database have the desired properties. Here our experi-\nment is the quantum calculation, and we typically assume that we can \nevaluate hundreds to thousands of materials in a reasonable time. \nFrom a machine learning point of view, a set of thousands of materials \nis in a very low-data regime. However, from an experimental point of \nview, this is a large but sometimes doable effort. Of course, this is a \nsomewhat arbitrary limit, and in Supplementary Fig. 83, we also give \ndata for fewer experiments.\nWe start with the training using a set of hundreds of molecules \nrandomly selected from the QMugs dataset (blue distribution in  \nChemical space\nN NF F\nN N FS\nN N F\nF\nF\nN NS\nF\nDataset, not generated\nIn dataset\nIn PubChem\nNovel\nA\nA\nD\nN N\nN\nO\nN N\nO\nNO\nN N\nN\nO\nCl\nN\nO\nNNF\nO\nN N\nN\nO\nNO2\nN N\nN\nO\nCl\nN N\nF\nF\nF\nN N\nF\nF\nN N\nF\nF\nO\nC\nB\nNNH\nN\nN\nNH\nO\nN\nHNN\nO\nN\nNH\nNN\nN\nH\nO\nB\nC\nD\nFig. 4 | TMAP visualization of the generated photoswitches and the training \nset. The tree map (TMAP) algorithm builds a nearest-neighbour graph, which is \nthen embedded in two dimensions. Therefore, similar molecules are connected \nwith an edge. We colour the points depending on whether they are part of the \noriginal dataset of ref. 43 but not generated (green) or part of the dataset and \ngenerated by our model (orange). Our models can also generate molecules that \nhave not been part of the photoswitch dataset (note that the model was only \ntrained on 92 molecules from this database). In some cases, those molecules \nhave been reported before and are part of the PubChem database (blue) or \nare not part of the PubChem database (pink). From this figure, we see that the \ngenerated molecules sometimes substitutions for molecules in the dataset. In \nother cases, newly generated molecules introduce a completely new scaffold. \nFor this visualization, we used the TMAP\n65 algorithm on photoswitch molecules \ndescribed using MinHash fingerprint with 2,048 permutations66.\nNature Machine Intelligence | Volume 6 | February 2024 | 161–169 166\nArticle https://doi.org/10.1038/s42256-023-00788-1\nFig. 5). These selected molecules will have bandgap distribution similar \nto the QMugs dataset. We then query for HOMO–LUMO gaps, now \naround 1,000 times requesting a molecule with a bandgap taken from \na normal distribution with shifted mean (mean 4.0 eV, s.d. 0.2 eV). We \nevaluated these new molecules (green curve in Fig. 5 ), which indeed \nshows a shift of the distribution to higher HOMO–LUMO gaps. In the \nnext iteration, we retrain the model with the new data and query again \nhigher HOMO–LUMO gaps. Figure 5 shows that we have achieved our \naim after four iterations.\nConcluding remarks\nOur results raise a very important question: how can a natural lan -\nguage model with no prior training in chemistry outperform dedi -\ncated machine learning models, as we were able to show in the case \nof high-entropy alloys in Fig. 2  and for various molecule, material \nand chemical reaction properties in Extended Data Table 2? T o our \nknowledge, this fundamental question has no rigorous answer. The \nfact that we get good results independent of the chemical represen-\ntation illustrates that these language models are very apt at extract-\ning correlations from any text15. For example, we found promising \nresults using both conventional chemical names and entirely hypo-\nthetical representations. In both cases, the model could quantitatively \ncorrelate the pattern of repeating units correctly to different kinds  \nof properties.\nOf course, if we say that the GPT-3 model is successful, it implies \nonly that we have established that the GPT-3 model has identified cor-\nrelations in the current training data that can be successfully exploited \nto make predictions. However, this does not imply that the correlations \nare always meaningful or related to cause–effect relationships. Hence, \nour research does not stop here. The next step will be to use GPT-3 to \nidentify these correlations and ultimately get a deeper understanding. \nIn this context, we argue that GPT-3 is only a tool to make more effective \nuse of the knowledge scientists have collected over the years. It is also \nimportant to mention that while the training corpus contains chemistry \ninformation, many, if not most, scientific articles and results (including \nall failed or partially successful experiments50) have not been seen by \nGPT-3. Hence, one can expect an even more impressive performance \nif these data are added to the training data.\nAs we show in this Article, a machine learning system built using \nGPT-3 works impressively well for a wide range of questions in chemis-\ntry—even for those for which we cannot use conventional line represen-\ntations such as SMILES. Compared with conventional machine learning, \nit has many advantages. GPT-3 can be used for many different applica-\ntions. Each application uses the same approach, in which the training \nand use of the model are based on questions formulated in natural \nlanguage. This raises the bar for future machine learning studies, as any \nnew models should at least outperform this simple approach instead.\nThe other important practical point is that using a GPT-3 model in \na research setting is similar to a literature search. It will allow chemists \nto leverage the chemical knowledge we have collected. GPT-3 has been \ndesigned to discover correlations in text fragments, and the fact that \nthese correlations are extremely relevant to chemistry opens many \npossibilities for chemists and material scientists alike.\nMethods\nFor all the results shown in the main text, we used the smallest ada \nvariant of GPT-3 available via the OpenAI API. For fine-tuning, we used \nthe same setting for all case studies (8 epochs, learning rate multiplier \nof 0.02). Error bands show, if not otherwise indicated, the standard \nerror of the mean.\nData efficiency comparison\nT o compare the data-efficiency of the GPT-3 models with our baselines, \nwe fitted all learning curves to power laws (−a exp(−bx + c)). We then \nused these power laws to find where the best-performing baseline \nshows the same performance as the best GPT-3-based approach at \nthe first learning curve point (that performs better than random, as \nmeasured using the Cohen’s kappa (κ) metric).\nValidity checks\nT o check the validity of the generated SMILES we use the is_valid \nmethod from the Guacamol package51, which effectively considers a \nSMILES as valid if it can be parsed using RDKit.\nGPT-J model\nWe also performed some of our experiments by fine-tuning the \nGPT-J-6B model52,53 (which has been trained on the Pile dataset 54) on \nconsumer hardware using 8-bit quantization55 and 8-bit optimizers56 \nin addition to the low-rank adaptation (LoRA) technique57.\nData availability\nAll data used in this work was obtained from public sources and can \nbe downloaded from GitHub (https://github.com/kjappelbaum/\ngptchem)58.\nCode availability\nAll code created in this work is available on GitHub. The gptchem reposi-\ntory (https://github.com/kjappelbaum/gptchem)58 contains all experi-\nments with the OpenAI API. The chemlift repository (https://github.\ncom/lamalab-org/chemlift)59 contains an implementation supporting \nopen-source LLMs.\nReferences\n1. Bommasani, R. et al. On the opportunities and risks of foundation \nmodels. Preprint at https://arxiv.org/abs/2108.07258 (2021).\n2. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf.  \nProcess. Syst. https://proceedings.neurips.cc/paper/2017/file/ \n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf (2017).\n3. Chowdhery, A. et al. PaLM: scaling language modeling with \npathways. J. Mach. Learn. Res. 24, 1–113 (2023).\nIteration 1\nIteration 2\nIteration 3\nIteration 4\nIteration 5\nQMugs\nDensity\n1.5\n1.0\n0.5\n0\n0 2 4\nHOMO–LUMO gap (eV)\n6 8\nFig. 5 | Iteratively biased generation of molecules towards large HOMO–\nLUMO gaps using GPT -3 fine-tuned on the QMugs dataset of draws. We start \nby fine-tuning GPT-3 on a sample of the QMugs dataset and use this model to \nquery for around 1,000 gaps from a normal distribution with shifted mean \n(mean 4.0 eV, s.d. 0.2 eV). We then iteratively select the high-gap samples of the \ngenerated molecules and fine-tune the model on these data (that is, starting from \nthe second generation, the model is fine-tuned on molecules it itself generated). \nSmooth curves show kernel-density estimates; the plot is truncated at 10 eV, but \nthe models also generate some molecules with larger HOMO–LUMO gaps.  \nWe chose a comparatively large number of evaluations for this figure to \nincrease the clarity of the visualization. For the initialization, we evaluated 2,162 \ncompounds using xTB, followed by 1,670, 250 and 1,572. If we limit the number \nof quantum chemistry evaluations to or lower than 100, we can still successfully \nshift the distribution, as shown in Supplementary Fig. 83.\nNature Machine Intelligence | Volume 6 | February 2024 | 161–169\n 167\nArticle https://doi.org/10.1038/s42256-023-00788-1\n4. Hoffmann, J. et al. An empirical analysis of compute-optimal \nlarge language model training. Adv. Neural Inf. Process. Syst. 35, \n30016–30030 (2022).\n5. Brown, T. et al. Language models are few-shot learners. Adv. \nNeural Inf. Process. Syst. 33, 1877–1901 (2020).\n6. Edwards, C. N., Lai, T., Ros, K., Honke, G. & Ji, H. Translation \nbetween molecules and natural language. in Conference On \nEmpirical Methods In Natural Language Processing (eds Goldberg, \nY. et al.) 375–413 (Association for Computational Linguistics, \n2022).\n7. Hocky, G. M. & White, A. D. Natural language processing models \nthat automate programming will transform chemistry research \nand teaching. Digit. Discov. 1, 79–83 (2022).\n8. White, A. D. et al. Assessment of chemistry knowledge in large \nlanguage models that generate. Digit. Discov. 2, 368–376  \n(2023).\n9. Taylor, R. et al. Galactica: a large language model for science. \nPreprint at https://arxiv.org/abs/2211.09085 (2022).\n10. Dunn, A. et al. Structured information extraction from complex \nscientific text with fine-tuned large language models. Adv. Neural \nInf. Process. Syst. 35, 11763–11784 (2022).\n11. Choudhary, K. & Kelley, M. L. ChemNLP: a natural \nlanguage-processing-based library for materials chemistry text \ndata. J. Phys. Chem. C 127, 17545–17555 (2023).\n12. Jablonka, K. M. et al. 14 examples of how LLMs can transform \nmaterials science and chemistry: a reflection on a large language \nmodel hackathon. Digit. Discov. 2, 1233–1250 (2023).\n13. Dinh, T. et al. LIFT: language-interfaced fine-tuning for \nnon-language machine learning tasks. Adv. Neural Inf. Process. \nSyst. 35, 11763–11784 (2022).\n14. Karpov, P., Godin, G. & Tetko, I. V. Transformer-CNN: Swiss knife for \nQSAR modeling and interpretation. J. Cheminform. 12, 17 (2020).\n15. Tshitoyan, V. et al. Unsupervised word embeddings capture latent \nknowledge from materials science literature. Nature 571, 95–98 \n(2019).\n16. Born, J. & Manica, M. Regression transformer enables concurrent \nsequence regression and generation for molecular language \nmodelling. Nat. Mach. Intell. 5, 432–444 (2023).\n17. Yüksel, A., Ulusoy, E., Ünlü, A. & Doğan, T. SELFormer: molecular \nrepresentation learning via SELFIES language models. Mach. \nLearn. Sci. Technol. 4, 025035 (2023).\n18. van Deursen, R., Ertl, P., Tetko, I. V. & Godin, G. GEN: highly \nefficient SMILES explorer using autodidactic generative \nexamination networks. J. Cheminform.12, 22 (2020).\n19. Flam-Shepherd, D., Zhu, K. & Aspuru-Guzik, A. Language models \ncan learn complex molecular distributions. Nat. Commun. 13, \n3293 (2022).\n20. Grisoni, F. Chemical language models for de novo drug design: \nchallenges and opportunities. Curr. Opin. Struct. Biol. 79, 102527 \n(2023).\n21. Ramos, M. C., Michtavy, S. S., Porosoff, M. D. & White, A. D. \nBayesian optimization of catalysts with in-context learning. \nPreprint at https://arxiv.org/abs/2304.05341 (2023).\n22. Guo, T. et al. What indeed can GPT models do in chemistry?  \nA comprehensive benchmark on eight tasks. Preprint at  \nhttps://arxiv.org/abs/2305.18365 (2023).\n23. Howard, J. & Ruder, S. Universal language model fine-tuning for \ntext classification. In Proc. 56th Annual Meeting of the Association \nfor Computational Linguistics (Volume 1: Long Papers) 328–339 \n(Association for Computational Linguistics, 2018); https://\naclanthology.org/P18-1031\n24. Pei, Z., Yin, J., Hawk, J. A., Alman, D. E. & Gao, M. C. \nMachine-learning informed prediction of high-entropy solid \nsolution formation: beyond the Hume–Rothery rules. npj Comput. \nMater. https://doi.org/10.1038/s41524-020-0308-7 (2020).\n25. Dunn, A., Wang, Q., Ganose, A., Dopp, D. & Jain, A. Benchmarking \nmaterials property prediction methods: the Matbench test set \nand Automatminer reference algorithm. npj Comput. Mater.  \nhttps://doi.org/10.1038/s41524-020-00406-3 (2020).\n26. Goldblum, M., Finzi, M., Rowan, K. & Wilson, A. The no  \nfree lunch theorem, Kolmogorov complexity, and the role of \ninductive biases in machine learning. ICLR 2024 Conference, \nOpenReview https://openreview.net/forum?id=X7nz6ljg9Y  \n(2023).\n27. Schwaller, P. et al. Molecular transformer: a model for \nuncertainty-calibrated chemical reaction prediction. ACS Cent. \nSci. 5, 1572–1583 (2019).\n28. Winter, B., Winter, C., Schilling, J. & Bardow, A. A smile is all \n you need: predicting limiting activity coefficients from SMILES \nwith natural language processing. Digit. Discov. 1, 859–869 \n(2022).\n29. Dai, D. et al. Why can GPT learn in-context? Language models \nsecretly perform gradient descent as meta-optimizers. Preprint at \nhttps://arxiv.org/abs/2212.10559 (2022).\n30. Weininger, D. SMILES, a chemical language and information \nsystem. 1. Introduction to methodology and encoding rules.  \nJ. Chem. Inf. Comput. Sci. 28, 31–36 (1988).\n31. Krenn, M., Häse, F., Nigam, A., Friederich, P. & Aspuru-Guzik, A. \nSelf-referencing embedded strings (SELFIES): a 100% robust \nmolecular string representation. Mach. Learn. Sci. Technol. 1, \n045024 (2020).\n32. Krenn, M. et al. SELFIES and the future of molecular string \nrepresentations. Patterns 3, 100588 (2022).\n33. Sanchez-Lengeling, B. & Aspuru-Guzik, A. Inverse molecular \ndesign using machine learning: generative models for matter \nengineering. Science 361, 360–365 (2018).\n34. Yao, Z. et al. Inverse design of nanoporous crystalline reticular \nmaterials with deep generative models. Nat. Mach. Intell. 3,  \n76–86 (2021).\n35. Gómez-Bombarelli, R. et al. Automatic chemical design using a \ndata-driven continuous representation of molecules. ACS Cent. \nSci. 4, 268–276 (2018).\n36. Kim, B., Lee, S. & Kim, J. Inverse design of porous materials using \nartificial neural networks. Sci. Adv. 6, eaax9324 (2020).\n37. Lee, S., Kim, B. & Kim, J. Predicting performance limits of methane \ngas storage in zeolites with an artificial neural network. J. Mater. \nChem. A 7, 2709–2716 (2019).\n38. Nigam, A., Friederich, P., Krenn, M. & Aspuru-Guzik, A. \nAugmenting genetic algorithms with deep neural networks for \nexploring the chemical space. In ICLR (2019).\n39. Jablonka, K. M., Mcilwaine, F., Garcia, S., Smit, B. & Yoo, B.  \nA reproducibility study of ‘augmenting genetic algorithms with \ndeep neural networks for exploring the chemical space’. Preprint \nat https://arxiv.org/abs/2102.00700 (2021).\n40. Chung, Y. G. et al. In silico discovery of metal-organic frameworks \nfor precombustion CO\n2 capture using a genetic algorithm. Sci. \nAdv. 2, e1600909 (2016).\n41. Lee, S. et al. Computational screening of trillions of metal–\norganic frameworks for high-performance methane storage. ACS \nAppl. Mater. Interfaces 13, 23647–23654 (2021).\n42. Collins, S. P., Daff, T. D., Piotrkowski, S. S. & Woo, T. K. Materials \ndesign by evolutionary optimization of functional groups in \nmetal–organic frameworks. Sci. Adv. https://doi.org/10.1126/\nsciadv.1600954 (2016).\n43. Griffiths, R.-R. et al. Data-driven discovery of molecular \nphotoswitches with multioutput Gaussian processes. Chem. Sci. \n13, 13541–13551 (2022).\n44. Ertl, P. & Schuffenhauer, A. Estimation of synthetic accessibility \nscore of drug-like molecules based on molecular complexity and \nfragment contributions. J. Cheminform. 1, 8 (2009).\nNature Machine Intelligence | Volume 6 | February 2024 | 161–169 168\nArticle https://doi.org/10.1038/s42256-023-00788-1\n45. Jablonka, K. M., Jothiappan, G. M., Wang, S., Smit, B. & Yoo, B. \nBias free multiobjective active learning for materials design and \ndiscovery. Nat. Commun. https://doi.org/10.1038/s41467-021-\n22437-0 (2021).\n46. Bannwarth, C., Ehlert, S. & Grimme, S. GFN2-xTB—an accurate \nand broadly parametrized self-consistent tight-binding \nquantum chemical method with multipole electrostatics and \ndensity-dependent dispersion contributions. J. Chem. Theory \nComput. 15, 1652–1671 (2019).\n47. Isert, C., Atz, K., Jiménez-Luna, J. & Schneider, G. QMugs: \nquantum mechanical properties of drug-like molecules  \nhttps://doi.org/10.3929/ethz-b-000482129 (2021).\n48. Isert, C., Atz, K., Jiménez-Luna, J. & Schneider, G. QMugs, \nquantum mechanical properties of drug-like molecules. Sci. Data \n9, 273 (2022).\n49. Westermayr, J., Gilkes, J., Barrett, R. & Maurer, R. J. \nHigh-throughput property-driven generative design of  \nfunctional organic molecules. Nat. Comput. Sci. 3, 139–148 \n(2023).\n50. Jablonka, K. M., Patiny, L. & Smit, B. Making the collective \nknowledge of chemistry open and machine actionable.  \nNat. Chem. 14, 365–376 (2022).\n51. Brown, N., Fiscato, M., Segler, M. H. & Vaucher, A. C. GuacaMol: \nbenchmarking models for de novo molecular design. J. Chem. Inf. \nModel. 59, 1096–1108 (2019).\n52. Wang, B. Mesh-Transformer-JAX: model-parallel implementation \nof transformer language model with JAX. GitHub https://github.\ncom/kingoflolz/mesh-transformer-jax (2021).\n53. Wang, B. & Komatsuzaki, A. GPT-J-6B: a 6 billion parameter \nautoregressive language model. GitHub https://github.com/\nkingoflolz/mesh-transformer-jax (2021).\n54. Gao, L. et al. The Pile: an 800BG dataset of diverse text for \nlanguage modeling. Preprint at https://arxiv.org/abs/2101.00027 \n(2020).\n55. Dettmers, T., Lewis, M., Belkada, Y. & Zettlemoyer, L. GPT3.int8(): \n8-bit matrix multiplication for transformers at scale. Adv. Neural \nInf. Process. Syst. 35, 30318–30332 (2022).\n56. Dettmers, T., Lewis, M., Shleifer, S. & Zettlemoyer, L. 8-bit \noptimizers via block-wise quantization. in The Tenth International \nConference on Learning Representations (2022).\n57. Hu, E. J. et al. LoRA: low-rank adaptation of large language \nmodels. in International Conference On Learning Representations \n(2021).\n58. Jablonka, K. M. kjappelbaum/gptchem: initial release. Zenodo \nhttps://doi.org/10.5281/zenodo.7806672 (2023).\n59. Jablonka, K. M. chemlift. Zenodo https://doi.org/10.5281/\nzenodo.10233422 (2023).\n60. Dubbeldam, D., Calero, S. & Vlugt, T. J. iRASPA: GPU-accelerated \nvisualization software for materials scientists. Mol. Simul. 44, \n653–676 (2018).\n61. Le, T. T., Fu, W. & Moore, J. H. Scaling tree-based automated \nmachine learning to biomedical big data with a feature set \nselector. Bioinformatics 36, 250–256 (2020).\n62. Wang, A. Y.-T., Kauwe, S. K., Murdock, R. J. & Sparks, T. D. \nCompositionally restricted attention-based network for materials \nproperty predictions. npj Comput. Mater. 7, 77 (2021).\n63. RDKit contributors. RDKit: Open-source Cheminformatics; (2023) \nhttp://www.rdkit.org\n64. Preuer, K., Renz, P., Unterthiner, T., Hochreiter, S. & Klambauer, G. \nFréchet ChemNet distance: a metric for generative models for \nmolecules in drug discovery. J. Chem. Inf. Model. 58, 1736–1741 \n(2018).\n65. Probst, D. & Reymond, J.-L. Visualization of very large \nhigh-dimensional data sets as minimum spanning trees.  \nJ. Cheminform. 12, 12 (2020).\n66. Probst, D. & Reymond, J.-L. A probabilistic molecular fingerprint \nfor big data settings. J. Cheminform. 10, 66 (2018).\n67. Ertl, P. & Rohde, B. The Molecule Cloud—compact visualization of \nlarge collections of molecules. J. Cheminform. 4, 12 (2012).\n68. Wang, Y., Wang, J., Cao, Z. & Farimani, A. B. Molecular contrastive \nlearning of representations via graph neural networks. Nat. Mach. \nIntell. 4, 279–287 (2022).\n69. Breuck, P.-P. D., Evans, M. L. & Rignanese, G.-M. Robust model \nbenchmarking and bias-imbalance in data-driven materials \nscience: a case study on MODNet. J. Phys. Condens. Matter 33, \n404002 (2021).\n70. Hollmann, N., Müller, S., Eggensperger, K. & Hutter, F. TabPFN: a \ntransformer that solves small tabular classification problems in a \nsecond. Preprint at https://arxiv.org/abs/2207.01848 (2022).\n71. Griffiths, R.-R. et al. Gauche: a library for Gaussian processes in \nchemistry. in ICML 2022 2nd AI for Science Workshop https://\nopenreview.net/forum?id=i9MKI7zrWal (2022)\n72. Chen, T. & Guestrin, C. XGBoost: a scalable tree boosting \nsystem. in Proc. 22nd ACM SIGKDD International Conference on \nKnowledge Discovery and Data Mining 785–794 (ACM, 2016).\n73. Moosavi, S. M. et al. Understanding the diversity of the metal- \norganic framework ecosystem. Nat. Commun. 11, 4068 (2020).\n74. Moosavi, S. M. et al. A data-science approach to predict the heat \ncapacity of nanoporous materials. Nat. Mater. 21, 1419–1425 (2022).\n75. Probst, D., Schwaller, P. & Reymond, J.-L. Reaction classification \nand yield prediction using the differential reaction fingerprint \nDRFP. Digit. Discov. 1, 91–97 (2022).\n76. Raffel, C. et al. Exploring the limits of transfer learning with a \nunified text-to-text transformer. J. Mach. Learn. Res. 21,  \n5485–5551 (2020).\n77. Radford, A. et al. Language models are unsupervised multitask \nlearners. OpenAI blog 1, 9 (2019).\n78. Mobley, D. L. & Guthrie, J. P. FreeSolv: a database of experimental \nand calculated hydration free energies, with input files.  \nJ. Comput. Aided Mol. Des. 28, 711–720 (2014).\n79. Delaney, J. S. ESOL: estimating aqueous solubility directly from \nmolecular structure. J. Chem. Inf. Comput. Sci. 44, 1000–1005 (2004).\n80. Mitchell, J. B. O. DLS-100 solubility dataset. University of  \nSt Andrews https://risweb.st-andrews.ac.uk:443/portal/en/\ndatasets/dls100-solubility-dataset(3a3a5abc-8458-4924-\n8e6c-b804347605e8).html (2017).\n81. Walters, P. Predicting aqueous solubility—it’s harder than it looks. \nPractical Cheminformatics https://practicalcheminformatics.\nblogspot.com/2018/09/predicting-aqueous-solubility-its.html (2018).\n82. Bento, A. P. et al. The ChEMBL bioactivity database: an update. \nNucleic Acids Res. 42, D1083–D1090 (2014).\n83. Gaulton, A. et al. ChEMBL: a large-scale bioactivity database for \ndrug discovery. Nucleic Acids Res. 40, D1100–D1107 (2012).\n84. Nagasawa, S., Al-Naamani, E. & Saeki, A. Computer-aided \nscreening of conjugated polymers for organic solar cell: \nclassification by random forest. J. Phys. Chem. Lett. 9,  \n2639–2646 (2018).\n85. Kawazoe, Y., Yu, J.-Z., Tsai, A.-P. & Masumoto, T. (eds) \nNonequilibrium Phase Diagrams of Ternary Amorphous Alloys \nLandolt-Börnstein: Numerical Data and Functional Relationships \nin Science and Technology—New Series (Springer, 2006).\n86. Zhuo, Y., Tehrani, A. M. & Brgoch, J. Predicting the band gaps \nof inorganic solids by machine learning. J. Phys. Chem. Lett. 9, \n1668–1673 (2018).\n87. Ahneman, D. T., Estrada, J. G., Lin, S., Dreher, S. D. & Doyle, A. G. \nPredicting reaction performance in C–N cross-coupling using \nmachine learning. Science 360, 186–190 (2018).\n88. Perera, D. et al. A platform for automated nanomole-scale \nreaction screening and micromole-scale synthesis in flow. \nScience 359, 429–434 (2018).\nNature Machine Intelligence | Volume 6 | February 2024 | 161–169\n 169\nArticle https://doi.org/10.1038/s42256-023-00788-1\nAcknowledgements\nK.M.J., A.O.-G. and B.S. were supported by the MARVEL National Centre \nfor Competence in Research funded by the Swiss National Science \nFoundation (grant agreement ID 51NF40-182892). P.S. acknowledges \nsupport from NCCR Catalysis (grant number 180544), a National \nCentre of Competence in Research funded by the Swiss National \nScience Foundation. The research of K.M.J. and B.S. was also supported \nby the USorb-DAC Project, which is funded by a grant from The \nGrantham Foundation for the Protection of the Environment to RMI’s \nclimate tech accelerator programme, Third Derivative. In addition, the \nwork of K.M.J. was supported by the Carl-Zeiss Foundation.\nAuthor contributions\nK.M.J. developed the machine learning approach with feedback from \nP.S. and B.S. K.M.J. and B.S. wrote the article. A.O.-G. contributed to the \ndensity functional theory calculations.\nFunding\nOpen access funding provided by EPFL Lausanne.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nExtended data is available for this paper at https://doi.org/10.1038/\ns42256-023-00788-1.\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s42256-023-00788-1.\nCorrespondence and requests for materials should be addressed to \nBerend Smit.\nPeer review information Nature Machine Intelligence  \nthanks Guillaume Godin, Glen Hocky and the other,  \nanonymous, reviewer(s) for their contribution to the peer review  \nof this work.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard  \nto jurisdictional claims in published maps and institutional  \naffiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons license, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons license, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this license, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2024\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00788-1\nExtended Data Fig. 1 | Molecule Cloud for randomly generated photoswitch \nmolecules. Molecule Cloud generated using the tool reported by Ertl and \nRohde\n67. Aquamarine background indicates samples from molecules in  \nthe database reported by Griffiths et al.43 that our model did not generate,  \ncoral indicates the molecules our model generated and that are part of  \nGriffiths et al43’s database, light steel blue background indicates samples  \nthat are generated by our model and that are not part of the database of  \nGriffiths et al.43 but part of the PubChem database. Pale violet-red background \nindicates molecules that our model generated but that are part neither of \nPubChem nor the database of Griffiths et al.\n43.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00788-1\nExtended Data Table 1 | Example prompts and completions for predicting the phase of high-entropy alloys\nThese models have been trained using a self-supervised approach, that is, to predict the next token given an input text sequence. This implies we offer the list of questions and answers as \none large string. The program learns that in our string ‘###’ indicates the end of a prompt and ‘@@@’ the end of a completion. Here, we used the fact that learning one character is cheaper and \neasier, hence 0=multi-phase.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00788-1\nExtended Data Table 2 | Data-efficiency comparison of best-performing GPT-3-based approaches with best-performing \nbaselines\nFor the best comparison, we also split into (pre-trained) deep-learning (DL)-based baselines (here, MolCLR68, ModNet69, CrabNet62, and TabPFN70) and baselines not using (pre-trained) \ndeep-learning approaches (n-Gram, Gaussian Process Regression, XGBoost, random forests, automated machine learning optimized for materials science25) on hand-tuned feature sets. \nFor the analysis in this table, we fit the learning curves for the GPT-3 models and for the baselines and measure where the learning curves intersect, that is, we determine the factor of how \nmuch more (or less) data we would need to make the best baseline perform equal to the GPT-3 models in the low-data regime of the learning curves. Full learning curves for all models can \nbe found in Supplementary Note 6. In parentheses, we mention the baseline we considered for each case study. In doing so, we use the following acronyms: t for TabPFN70, m for MolCLR68, \nn for n-Gram, g for GPR71, x for XGBoost72 on molecular descriptors such as fragprints71, xmo for XGBoost model similar to the one in Moosavi et al.73, xj for an XGBoost model similar to the \none in Jablonka et al.45, mo for the atom-centered model from Moosavi et al.74, c for CrabNet62, prf for the random forest model reported by Pei et al.24, a for automatminer25, mod for ModNet69, \ndrfp for differentiable reaction fingerprints75 as input for a GPR71. For the case studies on reaction datasets, we did not consider a deep learning baseline. There are several caveats to this \nanalysis. First, focusing on the low-data regime might not always be the most relevant perspective. Second, we only focus on the binary classification setting in this table. Third, we focus on \nthe F1 macro score for this table (all cases are class-balanced). Fourth, we consider the performance of the GPT-3 model for ten training data points as a reference. We provide more details in \nSupplementary Note 6. The version of GPT-3 we utilized in this work has been trained on data up to Oct 2019 that mostly comes from web scraping (Common Crawl76 and WebText77) along \nwith books corpora and Wikipedia. Structured datasets, however, have not been part of the training. Also, note that our approach works well on representations that have not been used for \nthe original datasets (for example, SELFIES, InChI). For the case studies on reaction datasets, we did not consider a deep learning baseline, hence the corresponding values have been omitted \nin the table. For computing the table, we utilized data reported in Refs. 78–88.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.43564170598983765
    },
    {
      "name": "Chemistry",
      "score": 0.35705530643463135
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5124864",
      "name": "École Polytechnique Fédérale de Lausanne",
      "country": "CH"
    }
  ]
}