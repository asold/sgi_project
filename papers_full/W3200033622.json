{
    "title": "The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders",
    "url": "https://openalex.org/W3200033622",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2096691967",
            "name": "Han He",
            "affiliations": [
                "Emory University"
            ]
        },
        {
            "id": "https://openalex.org/A2125846880",
            "name": "Jinho D. Choi",
            "affiliations": [
                "Emory University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4230872509",
        "https://openalex.org/W3102085674",
        "https://openalex.org/W3035375600",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963246595",
        "https://openalex.org/W3037282604",
        "https://openalex.org/W2973154008",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2964303116",
        "https://openalex.org/W2970529259",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3034598693",
        "https://openalex.org/W2963828549",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2963571341",
        "https://openalex.org/W3033187248",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2970862333",
        "https://openalex.org/W2103076621",
        "https://openalex.org/W2970290899",
        "https://openalex.org/W2963453233",
        "https://openalex.org/W2963854351",
        "https://openalex.org/W3035229828",
        "https://openalex.org/W3209042722",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963121782",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W3034503989",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2970352191",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W3034779619",
        "https://openalex.org/W2094061585"
    ],
    "abstract": "Multi-task learning with transformer encoders (MTL) has emerged as a powerful technique to improve performance on closely-related tasks for both accuracy and efficiency while a question still remains whether or not it would perform as well on tasks that are distinct in nature. We first present MTL results on five NLP tasks, POS, NER, DEP, CON, and SRL, and depict its deficiency over single-task learning. We then conduct an extensive pruning analysis to show that a certain set of attention heads get claimed by most tasks during MTL, who interfere with one another to fine-tune those heads for their own objectives. Based on this finding, we propose the Stem Cell Hypothesis to reveal the existence of attention heads naturally talented for many tasks that cannot be jointly trained to create adequate embeddings for all of those tasks. Finally, we design novel parameter-free probes to justify our hypothesis and demonstrate how attention heads are transformed across the five tasks during MTL through label analysis.",
    "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5555–5577\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n5555\nThe Stem Cell Hypothesis:\nDilemma behind Multi-Task Learning with Transformer Encoders\nHan He\nDepartment of Computer Science\nEmory University\nAtlanta, GA 30322 USA\nhan.he@emory.edu\nJinho D. Choi\nDepartment of Computer Science\nEmory University\nAtlanta, GA 30322 USA\njinho.choi@emory.edu\nAbstract\nMulti-task learning with transformer encoders\n(MTL) has emerged as a powerful technique to\nimprove performance on closely-related tasks\nfor both accuracy and efﬁciency while a ques-\ntion still remains whether or not it would per-\nform as well on tasks that are distinct in nature.\nWe ﬁrst present MTL results on ﬁve NLP tasks,\nPOS, NER, DEP, CON, and SRL, and depict its\ndeﬁciency over single-task learning. We then\nconduct an extensive pruning analysis to show\nthat a certain set of attention heads get claimed\nby most tasks during MTL, who interfere with\none another to ﬁne-tune those heads for their\nown objectives. Based on this ﬁnding, we pro-\npose the Stem Cell Hypothesis to reveal the ex-\nistence of attention heads naturally talented for\nmany tasks that cannot be jointly trained to cre-\nate adequate embeddings for all of those tasks.\nFinally, we design novel parameter-free probes\nto justify our hypothesis and demonstrate how\nattention heads are transformed across the ﬁve\ntasks during MTL through label analysis.\n1 Introduction\nTransformer encoders (TEs) have established re-\ncent state-of-the-art results on many core NLP tasks\n(He and Choi, 2019; Yu et al., 2020; Zhang et al.,\n2020). However, their architectures can be viewed\n“over-parameterized” as downstream tasks may not\nneed all those parameters, prone to cause an over-\nhead in computation. One promising approach to\nmitigate this overhead is multi-task learning (MTL)\nwhere a TE is shared across multiple tasks; thus, it\nneeds to be run only once to generate ﬁnal embed-\ndings for all tasks (Clark et al., 2019b).\nDespite the success in MTL on closely-related\ntasks such as language understanding (Wang et al.,\n2018) or relation extraction (Chen et al., 2020; Lin\net al., 2020), MTL on core NLP tasks (e.g., tagging,\nparsing, labeling) whose decoders are very distinct\nhas not been well-studied. This work employs the\nstate-of-the-art decoders on ﬁve core tasks for MTL\nand thoroughly analyzes interactions among those\ntasks to explore a possibility of reducing the compu-\ntation overhead from TEs. Surprisingly, our exper-\niments depict that models jointly trained by MTL\ngive lower accuracy than ones trained individually,\nthat is against ﬁndings from previous work. In fact,\nmodels jointly trained with all ﬁve tasks perform\nthe worst among any other combination (Section 3).\nThese experimental results urge us to ﬁgure out\nwhy MTL on core tasks with a shared TE leads to\nworse performance than its single-task counterparts.\nOur exploration begins by detecting essential heads\nfor each task by forcing the TE to use as few atten-\ntion heads as possible while maintaining accuracy\nsimilar to a fully-utilized encoder. Our experiments\nreveal that all ﬁve tasks rely on almost the same set\nof attention heads. Hence, they compete for those\nheads during MTL, causing to blur out features ex-\ntracted by individual tasks. Thus, we propose the\nStem Cell Hypothesis, likening these talented atten-\ntion heads to stem cells, which cannot be ﬁne-tuned\nfor multiple tasks that are very distinct (Section 4).\nTo validate this hypothesis, many parameter-free\nprobes are designed to observe how every attention\nhead is updated while trained individually or jointly.\nIntriguingly, we ﬁnd that heads not ﬁne-tuned for\nany task can still give remarkably high performance\nto predict certain linguistic structures, conﬁrming\nthe existence of stem cells inherently more talented;\nit is consistent with previous work stating that TEs\ncarry on a good amount of syntactic and semantic\nknowledge (Tenney et al., 2019; Liu et al., 2019a;\nJawahar et al., 2019; Hewitt and Manning, 2019).\nAfter single-task learning, probing results typically\nimprove along with the task performance, illustrat-\ning that the stem cells are developed into more task-\nspeciﬁc experts. On the contrary, MTL often drops\nboth probing and task performance, supporting our\nhypothesis that attention heads lose expertise when\nexposed to multiple teaching signals that may con-\nﬂict to one another (Section 5).\n5556\nThe Stem Cell Hypothesis is proposed to shed light\non a possible direction to MTL research using TEs,\ncomprising an unbearable amount of parameters,\nby wisely assigning attention heads to downstream\ntasks. Although most analysis in this study is based\non BERT, we also provide extensive experimen-\ntal results and visualization of other recent TEs\nincluding RoBERTa (Liu et al., 2019c), ELECTRA\n(Clark et al., 2020) and DeBERTa (He et al., 2020)\nin §A.4 to further demonstrate the generality of our\nhypothesis. To the best of knowledge, this is the\nﬁrst time that a comprehensive analysis of attention\nheads is made for MTL on those core tasks by in-\ntroducing novel parameter-free probing methods.1\n2 Related Work\nA small portion of our work overlaps with multi-\ntask learning. MTL with pre-trained transformers\nspeciﬁcally in NLP (Wang et al., 2018; Clark et al.,\n2019b; Liu et al., 2019b; Kondratyuk and Straka,\n2019; Chen et al., 2020; Lin et al., 2020) has been\nwidely studied. Most work focus on neural architec-\nture design to encourage beneﬁcial message pass-\ning across tasks. Our MTL framework adopts con-\nventional architecture and applies tricks of batch\nsampling (Wang et al., 2019) and loss balancing.\nMost of our work falls into the analysis of BERT,\nespecially from a linguistic view. Since BERT was\nintroduced, studies on explaining why BERT works\nhave never stopped. The most related studies are\nthose trying to study the linguistic structures learnt\nby BERT. Among them, Tenney et al. (2019) and\nLiu et al. (2019a) showed part-of-speech, syntactic\nchunks and roles can be discovered from BERT\nembeddings. Using a supervised probe, Hewitt\nand Manning (2019) successfully discover full de-\npendency parse trees. The encoded dependency\nstructure is also supported by Jawahar et al. (2019)\nusing probes on embeddings. Apart from these\nparameterized probes, parameter-free approaches\n(Clark et al., 2019a; Wu et al., 2020) also agree with\nthe existence of rich linguistic knowledge in BERT,\nwhich is closely related to our probing methods.\nWhat remains unclear is the impact of ﬁne-\nturning on TEs. Using supervised probes, Peters\net al. (2019) claim that ﬁne-tuning adapts BERT\nembeddings to downstream tasks, which is later\nchallenged by Hewitt and Liang (2019) since su-\n1All our resources including source codes and models are\npublic available athttps://github.com/emorynlp/\nstem-cell-hypothesis.\npervised probe itself can encode knowledge. Then,\nZhao and Bethard (2020) propose a methodology\nto test such encoding of a linguistic phenomenon\nby comparing the probing performance before and\nafter ﬁne-tuning. Our probing methods align with\nthese unsupervised probes while focus more on\nexplaining the impact of multi-task learning.\n3 Multi-Task Learning\nOur goal of MTL is to build a joint model sharing\nthe same encoder but using a distinct decoder for\neach task that outperforms its single-task counter-\nparts while being faster and more memory efﬁcient.\nOur model adapts hard parameter sharing (Caruana,\n1993) such that all decoders take the same hidden\nstates generated by the shared encoder as input and\nmake task-speciﬁc predictions in parallel.\n3.1 Shared Encoder\nFor main experiments, BERT (Devlin et al., 2019)\nis used as the shared encoder although our approach\ncan be adapted to any transformer encoders (§A.4).\nEvery token gets split into subtokens by BERT;\neventually, the average of the last layer’s hidden\nstates generated for those subtokens is used as the\nﬁnal embedding of that token. Additionally, word\ndropout is applied for generalization by replacing\nrandom subtokens with [MASK] during training.\n3.2 Task-Speciﬁc Decoders\nFive tasks are experimented, part-of-speech tagging\n(POS), named entity recognition ( NER), depen-\ndency parsing (DEP), constituency parsing (CON),\nand semantic role labeling (SRL). For each task, a\nstate-of-the-art decoder is adopted (except forPOS)\nto provide a modern benchmark for MTL on these\ntasks, and simpliﬁed to build an efﬁcient model.\nPOS A linear layer is used as a POS decoder that\ntakes the ﬁnal embedding of each token from BERT\nand generates the output vector where each dimen-\nsion gives the score of a particular POS tag.\nNER The biafﬁne decoder (Yu et al., 2020) is\nused for NER. For simpliﬁcation, document context,\nfastText and character-level embeddings as well as\nvariational BiLSTM encoding from the original\napproach are removed.\nDEP The biafﬁne decoder (Dozat and Manning,\n2017) is used for DEP as well. For simpliﬁcation,\npart-of-speech tags, character-level embeddings\nand the variational BiLSTM are removed in our\n5557\nPOS NER DEP CON SRL MTL-5\nPOS 98.32 ± 0.02 98.28 ± 0.01 98.28 ± 0.02 98.30 ± 0.02 98.27 ± 0.02 98.25 ± 0.01\nNER 89.34 ± 0.24 89.04 ± 0.14 89.43 ± 0.14 88.38 ± 0.06 89.18 ± 0.25 88.94 ± 0.10\nDEP 94.04 ± 0.02 94.06 ± 0.07 94.24 ± 0.03 94.12 ± 0.04 94.12 ± 0.03 93.84 ± 0.08\nCON 94.23 ± 0.03 94.38 ± 0.04 94.33 ± 0.02 94.43 ± 0.03 94.25 ± 0.05 94.10 ± 0.05\nSRL 82.92 ± 0.10 82.39 ± 0.07 82.05 ± 0.03 83.17 ± 0.02 82.93 ± 0.08 82.30 ± 0.13\nTable 1: Performance of single-task learning (main diagonal highlighted in gray), multi-task learning on all 5 tasks\n(MTL-5), and multi-task learning on every pair of the tasks (non-diagonal cells; e.g.,DEP’th row inNER’th column\nis the DEP result of the joint model between DEP and NER). See also Table 12 for similar results of other TEs.\napproach. Also, the ﬁnal embedding of [CLS]\nfrom BERT is used to represent the root node.\nCON The two-stage CRF decoder is used forCON\n(Zhang et al., 2020). The unlabeled bracket scorer\nis optimized using a tree-structure CRF objective\non unlabeled constituents. The encoding layer from\nthe original approach is substituted by BERT. Also,\n[CLS] and [SEP] in BERT are used to represent\n[BOS] and [EOS], respectively.\nSRL The end-to-end span ranking decoder (He\net al., 2018) is used for SRL. The attention-based\nspan representations are replaced by the averaged\nembeddings as suggested by Xia et al. (2019). For\nsimpliﬁcation, a linear layer is used as the ranker in-\nstead of the biafﬁne one since they have shown sim-\nilar performance in our preliminary experiments.\n3.3 Data and Loss Balancing\nDuring multi-task training, batches from different\ntasks are shufﬂed together and randomly sampled\nto optimize the shared encoder and the correspond-\ning decoder. Following Wang et al. (2019), a task is\nsampled based on a probability proportional to its\ndataset size raised to the power of 0.8. To balance\nthe losses of all tasks, a running average of every\ntask is monitored and its loss is updated as follow:\nL′\nt =\n∑\n∀i ¯Li\n¯Lt\n·Lt\nLt is the current loss of the taskt, ¯Lt is the running\naverage of the most recent 5 losses of t, and L′\nt is\nthe updated loss of t. This balancing method nor-\nmalizes the loss of each task to the same magnitude\nand has shown to prevent MTL from being biased\nto speciﬁc tasks in our preliminary experiments.\n3.4 MTL Experiments\nOur models are experimented on the OntoNotes 5\n(Weischedel et al., 2013) using the data split sug-\ngested by Pradhan et al. (2013). Table 1 illustrates\nperformance of all models using the following eval-\nuation metrics - POS: accuracy, NER: span-level\nlabeled F1, DEP: labeled attachment score, CON:\nconstituent-level labeled F1, SRL: micro-averaged\nF1 of (predicate, argument, label). Every model\nis trained 3 times and their average score and stan-\ndard deviation on the test set are reported. ForDEP,\nthe gold trees fromCON are converted into the Stan-\nford dependencies v3.3.0 (de Marneffe and Man-\nning, 2008). Detailed descriptions about the exper-\nimental settings are provided in Appendix A.2.\nSingle-task learning models are ﬁrst trained then\ncompared to the MTL model trained on all 5 tasks\n(MTL-5). Interestingly, MTL-5 is outperformed by\nits single-task counterparts for all tasks. Due to the\nhigh complexity of MTL-5, it is hard to tell which\ncombinations of tasks introduce negative transfer.\nThus, we conduct MTL on every pair of the tasks to\nobserve if there is any task combination that yields\na positive result (non-diagonal cells in Table 1).\nAmong the 10 pairwise task combinations, none\nof them derives a win-win situation. NER results\nare generally improved with MTL although results\non the other tasks are degraded, implying that NER\ntakes advantage of the other tasks by hurting their\nperformance. SRL is also beneﬁted from CON al-\nthough it is not the case for the other way around.\nResults of other recent TEs reveal similar patterns\nas shown in Appendix A.4.\n4 Pruning Analysis\nTo answer why MTL leads to suboptimal results in\nSection 3, pruning strategies are applied to BERT\nsuch that only attention heads absolutely necessary\nto get the best performance are kept for every task.\nThis allows us to see if there exists a common set of\nheads that multiple tasks want to claim and train for\nonly their objectives, which can cause conﬂicts for\nthose heads to be shared across all tasks.\n4.1 Pruning based on L0-Regularization\nBERT is essentially a stack of multi-head attention\nlayers and there is a wide consensus that different\nlayers learn distinct knowledge (Lin et al., 2019;\n5558\nPerformance % of Attention Heads Kept PS/S\nSTL STL-SP STL-DP MTL-DP STL-SP STL-DP MTL-DP STL STL-DP\nPOS 98.32±0.02 98.22±0.03 98.35±0.02 98.28±0.01 53.24±4.07 40.51±1.61 50.70±1.20 405 1,245\nNER 89.04±0.14 88.87±0.10 89.05±0.08 88.78±0.13 57.87±2.63 49.77±7.13 50.70±1.20 661 700\nDEP 94.24±0.03 94.08±0.10 94.22±0.06 93.92±0.06 63.66±5.02 50.00±2.08 50.70±1.20 241 601\nCON 94.43±0.03 94.16±0.08 94.24±0.03 94.16±0.05 45.37±0.40 44.91±0.40 50.70±1.20 191 397\nSRL 82.93±0.08 83.01±0.05 83.11±0.16 82.77±0.10 82.41±5.99 53.24±4.07 50.70±1.20 299 326\nTable 2: Results of single-task learning (STL), STL with static pruning (STL-SP), STL with dynamic pruning\n(STL-DP), and multi-task learning on the 5 tasks with dynamic pruning (MTL-DP). PS/S: processed samples per\nsecond for speed comparison. The STL Performance column is equivalent to the main diagonal in Table 1. See\nalso Table 13 for pruning results of other recent TEs.\nHewitt and Manning, 2019; Jawahar et al., 2019;\nLiu et al., 2019a; Tenney et al., 2019). Inspired by\nthis, we analyze if each head learns features unique\nto process different tasks. First, L0-regularization\nis applied (Louizos et al., 2018) to encourage BERT\nto use as few heads as possible during training. In\nparticular, a binary variable zj is assigned to the\nj’th head and multiplied to the output of that head\n(see Vaswani et al. (2017) for Q, K, V, dk):\nAttention(j)(Q, K, V) = zj ·softmax(QK⊤\n√dk\n)V\nUnfortunately, these binary variables z = {zj :\n∀j=[1,ℓ]}(ℓ: total # of heads) are discrete and non-\ndifferentiable so cannot be directly learnt using\ngradient based optimization. To allow for efﬁcient\ncontinuous optimization, each zj is then relaxed\nas a random variable drawn independently from a\ncontinuous random distribution. Speciﬁcally, the\nrelaxed z is re-parameterized by its inverse of the\ncumulative density function (CDF) as Gα(u). It is\nsampled as follows, where αis a learnable parame-\nter of the inverse CDF,U is the uniform distribution\nover the interval [0, 1] and u = {uj : ∀j=[1,ℓ]}de-\nnotes the iid samples from it:\nu ∼U(0, 1) ⇒ z = Gα(u)\nThen, the Hard Concrete Distribution (Louizos\net al., 2018) is chosen for z, which gives the\nfollowing form of Gα(u) that is differentiable,\nwhere (l, r) deﬁnes the interval that gα(u) can be\nstretched into (l <0, r >1):\ngα(u) = sigmoid(log u −log(1 −u) + α)\nGα(u) = min(1, max(0, gα(u) ×(r −l) + l))\nBy sampling u and applying the Monte Carlo ap-\nproximation, the learnable L0-objective is obtained\nin a closed form, which gets jointly optimized with\na task speciﬁc loss or the balanced MTL loss:\nEu∼U(0,1) [z] = sigmoid\n(\nα−log −l\nr\n)\nEu∼U(0,1) [L0] =\nn∑\nj=1\nE[zj]\n(1)\n4.2 Pruning Strategies\nTwo types of pruning strategies, static and dynamic,\nare applied for the attention head analysis:\nStatic Pruning We refer to the conventional two-\nstage train-then-prune as static pruning (SP) since it\nﬁne-tunes the encoder ﬁrst then freezes the decoder\nfor pruning (V oita et al., 2019).\nDynamic Pruning Since SP requires twice the\nefforts to obtain a pruned model, we propose a new\nmethod that simultaneously ﬁne-tunes and prunes.\nThis strategy is referred to as dynamic pruning\n(DP) since the decoder dynamically adapts to the\nencoder that is being pruned during training, as op-\nposed to SP which instead freezes the decoder. DP\nis found to be more effective in our experiments.\nAll pruning models are trained for 3 runs with\ndifferent random seeds and the best checkpoints by\nscores on development sets are kept. Once trained,\nEu∼U(0,1) [z] ∈(0, 1) is used as a measure of how\nmuch each head is being utilized.\n4.3 Pruning Experiments\nTable 2 shows single-task learning (STL) results\nusing SP and DP on the 5 tasks. Our DP strategy\nconsistently performs better than the SP strategy as\nit shows higher accuracy on all tasks and prunes sig-\nniﬁcantly greater numbers of heads except for CON.\nCompared to the STL models without any pruning,\nthe STL-DP models perform well or slightly better\nfor POS/NER/SRL due to theL0-regularization, yet\nuse ≈50% fewer numbers of heads.\n5559\n(a) POS\n (b) NER\n (c) DEP\n (d) CON\n (e) SRL\n1 2 3 4 5 6 7 8 9 10 11 12\nHeads\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12 Layers\n(f) 3-run utilization of the MTL-DP model, where each run is\nencoded in a RGB channel. Darker indicates higher utilization.\n1 2 3 4 5 6 7 8 9 10 11 12\nHeads\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12 Layers\n(g) Average head utilization rates among the 5 tasks in 3 runs.\nDarker cells indicate higher utilization rates.\nFigure 1: Head utilization of the STL-DP models (a - e, g) and the MTL-DP model (f). The detailed quantiﬁcation\nof correlations is shown in Table 3. See also Figure 9,11 and 13 for similar visualization of other recent TEs.\nComparing the STL-DP models across different\ntasks, SRL requires more heads than DEP and CON,\nwhich require more than POS. This aligns with the\nintuition behind their difﬁculty levels as semantic\n> syntactic > lexical relations. On the other hand,\nNER requires more heads than CON because of the\nworld knowledge it needs to capture from the data;\nthus, this knowledge is more scattered.\nFinally, DP is applied to MTL-5 (MTL-DP),\nwhich shows slightly higher accuracy than MTL-5\nin Table 1 except for NER by pruning 50% of the\nheads. This might imply that all tasks want to claim\na similar set of heads even though about a half of\nthe heads are underutilized during MTL training.\n4.4 Pruning Visualization\nTo visualize the utilization of heads across runs, the\nutilization rate z(r)\nj,t of the j’th head in ther’th run\nfor the task t is encoded to the RGB channels:\nR/G/Bj,t = 255 ×(1 −z(1/2/3)\nj,t )\nFor instance, RGB of (0, 0, 0) is black indicating\nthat the head is 100% utilized in all 3 runs. Based\non this scheme, the head utilizations of all STL-DP\nmodels as well as the MTL-DP model are plotted in\nFigures 1a ∼1f. To depict the overlaps of utilized\nheads across tasks, z’s are averaged over all STL-\nDP models for all runs then plotted as a grayscale\nheatmap (Figure 1g) using the following scheme:\nHj = 1\n15\n3∑\nr=1\n5∑\nt=1\nz(r)\nj,t\nConsistent head utilization by runs As shown\nin Figures 1a ∼1e and the main diagonal in Ta-\nble 3, the head utilization per task seems quite\nsimilar across different runs, especially for syn-\ntactic/semantic tasks such as DEP/CON/SRL. The\nhead utilization of POS seems to be random be-\ncause it is a simple task so that high performance\ncan be achieved by a small set of the re-utilized\nheads. This consistency across different runs is an\nessential prerequisite for the following analyses.\nConsistent head utilization across tasks Fig-\nures 1a ∼1e show that all of these tasks are mostly\nutilizing heads from layers 5 to 8 (looking like M).\n5560\nPOS NER DEP CON SRL MTL-5\nPOS 75.51 74.77 76.45 83.54 73.32 78.34\nNER 74.77 73.28 73.57 75.23 74.44 66.99\nDEP 76.45 73.57 89.89 89.27 91.20 84.39\nCON 83.54 75.23 89.27 83.99 83.90 80.71\nSRL 73.32 74.44 91.20 83.90 81.01 80.07\nMTL-5 78.34 66.99 84.39 80.71 80.07 85.88\nTable 3: Adjusted R-squared of 3-run head utilization\nrates using the third run as the dependent variable (main\ndiagonal highlighted in gray) and Pearson Correlation\nCoefﬁcient of averaged head utilization rates between\neach pair of models (non-diagonal cells).\nIn contrast to Jawahar et al. (2019) and Tenney et al.\n(2019), our ﬁndings suggest that the middle layers\nalso provide rich surface and semantic features,\nwhich are aligned with Liu et al. (2019a) showing\nthat both POS and chunking tasks perform the best\nwhen heads from the middle layers are utilized.\nConsistent head utilization by STL and MTL\nFigures 1f and 1g illustrate almost the identical uti-\nlization patterns, implying that the MTL-DP model\nre-uses a very similar set of heads used by the STL-\nDP models. According to Vaswani et al. (2017), the\nrepresentation capacity of every head is limited by\nthe design of multi-head attention. Since (1) a sim-\nilar set of heads are used across multiple tasks and\n(2) the limited representation capacity of individual\nheads conﬁnes them to only speciﬁc tasks, forcing\nthem for MTL leads to worse results. Given this\nanalogy, we propose the following hypothesis:\nThere exists a subset of attention heads in a trans-\nformer called “stem cells” that are commonly used\nby many tasks, which cannot be jointly trained for\nmultiple tasks that are very different in nature.\nWe refer to this claim as the Stem Cell Hypothesis\nand seek to test it through the probing analysis.\n5 Probing Analysis\nThis paper hypothesizes the existence of stem cells,\nwhich cannot be trained to create adequate embed-\ndings to be shared by multiple tasks that are not so\nsimilar. This section provides empirical evidence\nto this hypothesis by probing what roles each atten-\ntion head plays once ﬁne-tuned for end tasks.\n5.1 Probing Methods\nPrevious studies on probing transformer encoders\nhave focused on layer-level analysis limited to su-\npervised probing (Hewitt and Manning, 2019; Ten-\nney et al., 2019; Lin et al., 2019; Jawahar et al.,\n2019; Zhao and Bethard, 2020). This section intro-\nduces probes on the head-level instead to analyze\nthe impact of ﬁne-tuning on every individual head.\nSince developing supervised probes on hundreds of\nheads requires extensive resource, parameter-free\nprobing methods are used in this study.\nAttention Probes Attention between two words\noften matches a certain linguistic relation that gives\na good indicator to knowledge encoded in the head.\nOur decoders for DEP and SRL learn relationships\nbetween head/dependent words and predicate/argu-\nment words respectively, which can be directly ben-\neﬁted from these attentions. Thus, the attention ma-\ntrix from each head is used as the probe of that head.\nFollowing Clark et al. (2019a), an undirected edge\nis created between each word and its most attending\nword. First, the subtoken-subtoken attention matrix\nis converted into a word-word matrix by averaging\nthe attention probabilities of each multi-subtoken\nword. The arg max of each row r in the attention\nmatrix is then calculated, denoted as gr, and evalu-\nated on the basis of each task.\nFor DEP, directions of the gold arcs are removed\nand compared against the predicted arcs as follows\n(h|d: the index of a head |dependent word, (h, d):\nan undirected arc from the gold tree, n: # of arcs):\n1\nn\n∑\n∀(h,d)\n1 (gh = d ∥gd = h)\nFor SRL, we design a new probing method to evalu-\nate how each word in the argument span is attended\nto the head word in its predicate (p: the index of a\npredicate head word, Tp: word indices in the span\nof p’s argument,m: # of predicate-argument pairs):\n1\nm\n∑\n∀p,∀Tp\n1 (gp ∈Tp ∥∃t ∈Tp : gt = p)\nOnly head words in the predicates are used for this\nanalysis, which affects verb-particle constructions\n(e.g., only throw is used forthrow away). Moreover,\nnot all words in an argument span are necessarily\nimportant to add meaning to its predicate. We will\nexplore these aspects in the future.\nAttended-Value Probes POS/NER/CON can be\nviewed as tasks to ﬁnd and label spans in a sentence,\nwhere the span is a word for POS, a sequence of\nconsecutive words for NER and CON, where a span\ncan be overlapped with another span for CON. For\n5561\nthese tasks, we again present a new probing method,\ndepicted in Algorithm 1, that predicts the label of\neach span based on its representation:\nAlgorithm 1: Attended-Value Probing\nFunction PseudoCluster(H, S):\nC ←0 ∈Rm×d, n ←0 ∈Rm×1\nforeach (b, e, ℓ) ∈S do\nCℓ ←Cℓ + mean(Hb, . . . ,He)\nnℓ ←nℓ + 1\nreturn C/n\nFunction AVProbe(C, H):\nreturn arg max(cossim(C, H))\nThe attended-value matrix H ∈Rn×d is created\nby multiplying the attention matrix A ∈Rn×n to\nthe value matrix V ∈ Rn×d (Section 4.1) such\nthat H = AV(n: sentence length, d: embedding\nsize, abbreviated dk). Sis the set of gold spans, m\nis the total number of labels, (b, e, ℓ) denotes the\nindices of the beginning word, the ending word,\nand the label respectively, and cossim is a cosine\nsimilarity function with broadcasting enabled.\nWith Algorithm 1, the centroid of each label is\nobtained through PseudoCluster then used to\npredict labels of all spans. Note that for CON, only\nconstituents on the height-3 (right above the POS-\nlevel) are used for this analysis. We experimented\nwith constituents on higher levels, which did not\nshow good correlation with model performance as\nthe spans got longer and noisy. We plan to design\nanother probing method for deeper analysis inCON.\n5.2 Probing Experiments\nProbing experiments are conducted on all attention\nheads in the pre-trained BERT (Devlin et al., 2019)\nand ﬁne-tuned models trained by single-task learn-\ning (STL; diagonal in Table 1), pairwise multi-task\nlearning (MTL; other cells in Table 1), and 5-task\nmultitask learning (MTL-5; last column in Table 1)\nusing the two probing methods, attention probes\nand attended-value probes (Section 5.1). For each\nmodel, the head with the highest probing accuracy\namong 144 heads (12 heads per layer, 12 layers) is\nselected per label. Since every model is developed\n3 times using different random seeds for better gen-\neralization (Section 3.4), 3 heads are selected per\nlabel, which are averaged to get the ﬁnal probing\nscore for that label. The full probing results with\nrespect to all labels are described in Appendix A.3.\nTwo important observations are found from these\nexperiments. (1) Even without ﬁne-tuning, certain\nheads perform remarkably well on particular labels,\nconﬁrming the existence of stem cells (Sec. 5.2.1).\n(2) Most heads show higher performance once ﬁne-\ntuned; nonetheless, MTL does not always enhance\nthem for all tasks. In fact, MTL models show im-\nprovement on only a few labels (Sec. 5.2.2), while\nMTL-5 models show no beneﬁt for most labels.2\n5.2.1 Pluripotent Stem Cells\nThe probing results of pre-trained attention heads\nfrom BERT (before ﬁne-tuning) are visualized\nto verify the existence and pluripotency of stem\ncells. These results show very high accuracies\nfor many labels, conﬁrming the existence of stem\ncells. As they reside in the same pre-trained model,\ntheir pluripotency is therefore implied. Speciﬁ-\ncally, the number of probing tasks is 203 (POS:49,\nNER:19, SRL:67, DEP:45, CON:23 as shown in Ap-\npendix A.3.3) which is larger than the number of\nheads (144) in BERT-base by itself. Not all of them\nprovide task speciﬁc knowledge as shown in our\npruning experiments (Section 4), so the number of\nutilized heads is even smaller. As a result, some\nheads must play multiple roles in different tasks.\nDependency Parsing For DEP, probing results\nfrom the best performing heads with respect to their\nlayers for all labels are plotted in Figure 2, some of\nwhich are even comparable to supervised results.\n1 2 3 4 5 6 7 8 9 10 11 12\nlayer\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0accuracy\npunct\nprep\npobj\nnsubj\ndetroot\nnn\namod\ndobj\nadvmod\naux\ncc\nconj\ndep\nposs\nccomp\ncop\nmark\nxcomp\nnum\nrcmod\nadvcl\nneg\nauxpass\nnsubjpass\npossessive\npcomp\ndiscourse\npartmod\nappos\nprt\nnumber\nquantmod\nparataxis\ninfmod\ntmod\nexpl\nmwe\nnpadvmod iobjpredet\nacomp\ncsubj\npreconj\ncsubjpass\nFigure 2: DEP layer analysis of pre-trained heads.\nThe best performing head of BERT ﬁnds the ROOT\nof a sentence with a 96.25% accuracy without any\nsupervision, demonstrating its ability to convey the\nconcept. Furthermore, the identiﬁcation of ROOT\n2The probing results for DEP and SRL are omitted here due\nto the space limit but explained in Appendix A.3.\n5562\nhappens mostly at the early stage of inference, i.e.\nin layers 2 and 3. This ﬁnding may conﬂict with the\nidea of syntactic features getting learned in middle\nlayers (Jawahar et al., 2019). It takes the argument\nfrom Tenney et al. (2019) a step further suggesting\nthat syntax can be encoded in early layers of TEs.\nSemantic Role Labeling As shown in Figure 3,\nprobing shows promising results on many semantic\nroles. Speciﬁcally, numbered arguments (ARG0-4)\nare recognized in layers 5 to 7, while modiﬁers are\nidentiﬁed in layers 8 to 10 with > 80% accuracies,\nincluding ARGM-MOD (modals), ARGM-DIR (di-\nrectional), ARGM-EXT (extent), ARGM-LVB (light-\nverb), and ARGM-COM (comitative). Unlike DEP\nthat most labels are learned within the ﬁrst 7 layers,\nSRL requires 7+ layers to be learned such that no\nrole reaches the peak before layer 5. This implies\nthat semantic roles take more efforts to be learned\nthan syntactic dependencies.\n5 6 7 8 9 10\nlayer\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0accuracy\nARG1 ARG0\nARG2\nARGM-TMP\nARGM-DIS\nARGM-ADV\nARGM-MOD\nARGM-LOC\nARGM-MNR\nARGM-NEG\nR-ARG1\nR-ARG0\nC-ARG1\nARGM-PRP\nARGM-DIR\nARG3\nARG4\nARGM-CAU\nARGM-PRD\nARGM-ADJ\nARGM-EXT\nARGM-PNC\nARGM-GOL\nARGM-LVB\nR-ARGM-LOC\nR-ARGM-TMP\nR-ARG2\nC-ARG2\nC-ARG0\nARGM-REC\nFigure 3: SRL layer analysis of pre-trained heads.\n5.2.2 Stem Cells Specialization\nThough stem cells are pluripotent, they develop\ninto specialized ones in STL and lose specialities\nin MTL according to the following comparisons\nof best performing heads across BERT, STL and\nMTL models.\nPart-of-Speech Tagging Figure 4 compares the\nheads in the STL model against the other models;\nthe y-axis shows the probing results from the model\nin the x-axis subtracted by the results of the STL\nmodel. Labels (sorted by frequency) with negative\nscores for BERT imply that STL performs better\non those labels than BERT (without getting ﬁne-\ntuned), whereas negative labels with the other mod-\nels (e.g., NER, DEP) imply that the joint models\nperform worse than the STL model on those labels.\nBERT NER DEP CON SRL MTL-5\n50\n40\n30\n20\n10\n0\nNN\nIN\nDT\nNNP\nJJ\n.\nNNS\nPRP\nRB\n,\nVB\nVBD\nHYPH\nRP\n:\nFW\nNFP\nWP$\nXX\nSYM\nFigure 4: POS probing results comparison.\nFor POS, MTL degrades performance for most la-\nbels compared to STL. Even without getting ﬁne-\ntuned, the pre-trained BERT model performs very\nwell on punctuation labels, which is expected. The\nperformance on WP$ (possessive wh-pronoun) is\nsigniﬁcantly improved with NER, DEP, and SRL\nas a possessive wh-pronoun (e.g., whose) often fol-\nlows a name or is used in a relative clause that plays\nan important role in DEP and SRL.\nNamed Entity Recognition For NER, BERT de-\ntects PERCENT, MONEY, LAW, LANGUAGE, NORP\n(national|religious|political groups) and PRODUCT\nwith over 90% probing accuracy, probably due to\nthe rich set of those entities present in pre-training\ndata. Although most joint models degrade probing\nresults for nearly every entity type, POS and DEP\nimprove upon more entity types than the other tasks\n(Figure 5), which is consistent with the results il-\nlustrated in Table 1.\nBERT POS DEP CON SRL MTL-5\n25\n20\n15\n10\n5\n0\n5\nGPE\nPER\nORG\nDATE\nCARD\nNORP\n%\n$\nTIME\nORD\nLOC\nWOA\nFAC\nQUA\nPRDT\nEVNT\nLAW\nLANG\nFigure 5: NER probing results comparison.\n5563\nConstituency Parsing As shown in Figure 6,\nPOS improves the most number of constituent\ntypes but also causes the largest drop among the\nMTL models for the most frequent type, NP (noun\nphrase). This contributes to related constituent\ntypes such as RB (adverb) in ADVP (RB phrase),\nWRB (wh-adverb) in WHADVP (WRB phrase), and\nUH (interjection) in INTJ (UH phrase). Its dramatic\ndecrease on the NP performance might be due to\nthe internal lexical complexity in NP.\nBERT POS NER DEP SRL MTL-5\n30\n20\n10\n0\n10\nNP\nADVP\nADJP\nVP\nNML\nWHNP\nINTJ\nQP\nWHADVP\nPRT\nPP\nCONJP\nX\nWHADJP\nMETA\nUCP\nS\nLST\nFRAG\nSBAR\nFigure 6: CON probing results comparison.\nRegarding NER, its boost on ADVP can be due\nto temporal entities (TIME, DATE) nested within\nADVP such as (ADVP (NPone year) ago), where\none year ago is a DATE entity. As for PP (preposi-\ntion phrase), it usually follows the induction rule\nof PP →IN/TO + NP, where the NP is often an\nnamed entity (e.g., (PP (TO to) (NP Mary))). Re-\ngarding DEP, it mainly improves wh-phrases like\nWHNP and WHADVP, which correspond to nsubj\nand advmod dependency relations, respectively.\nRegarding SRL, it slightly improves NML (nominal\nmodiﬁers) and FRAG (fragment), which may be\nascribed to the strength of the span-based SRL not\nrequiring constituency structures for decoding.\nNote that in these probing analyses, we selected\nthe best performing heads from each model inde-\npendently as their locations are not regular any-\nmore. Without the pruning objective (Equation 1),\nthe locations of the best performing heads are non-\nregular possibly due to the knowledge transfer be-\ntween stem cells and non-stem cells. Thus, knowl-\nedge transferring from stem cells to non-stem cells\nbecomes much easier when the models are free to\nuse as many heads as they want. In fact, when ﬁne-\ntuned without the pruning objective, many stem\ncell attention heads transfer their knowledge to\nnon-stem cell heads to get specialized. It is a phe-\nnomenon frequently observed in many previous\nworks (Tenney et al., 2019; Liu et al., 2019a; Jawa-\nhar et al., 2019) and this work (Section 5.2.2) that\ncertain layers achieve best performance of certain\ntasks. Given that the stem cells of BERT are mostly\nin middle layers (Section 4.4), we believe that the\nbest performing layers or heads in lower or higher\nlayers are the results of transfer learning on stem\ncells. In reality, a stem cell also moves from its\noriginal area (e.g., bone marrow) to another area\n(e.g., bone surface) to get specialized.\n6 Conclusion\nThis study analyzes interference on the 5 core tasks\nby highlighting naturally talented attention heads,\nwhose importance turns out to be invariant for many\ndownstream tasks. The Stem Cell Hypothesis states\nthat these talented heads are like stem cells that\ncan develop into experts but not all-rounders. Our\nhypothesis is validated by several novel parameter-\nfree probes, revealing the interfered representations\nof stem cells. We will adapt this work to more tasks\nand languages for broader generality in the future.\nReferences\nRichard A. Caruana. 1993. Multitask Learning: A\nKnowledge-Based Source of Inductive Bias. In Pro-\nceedings of the 10th International Conference on\nMachine Learning, ICML’93, pages 41–48.\nDaoyuan Chen, Yaliang Li, Kai Lei, and Ying Shen.\n2020. Relabel the Noise: Joint Extraction of Enti-\nties and Relations via Cooperative Multiagents. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 5940–\n5950, Online. Association for Computational Lin-\nguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019a. What Does BERT\nLook at? An Analysis of BERT’s Attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Urvashi Khandelwal,\nChristopher D. Manning, and Quoc V . Le. 2019b.\nBAM! Born-Again Multi-Task Networks for Natu-\nral Language Understanding. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5931–5937, Florence,\nItaly. Association for Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\n5564\nMarie-Catherine de Marneffe and Christopher D. Man-\nning. 2008. The Stanford Typed Dependencies Rep-\nresentation. In Coling 2008: Proceedings of the\nworkshop on Cross-Framework and Cross-Domain\nParser Evaluation, pages 1–8, Manchester, UK. Col-\ning 2008 Organizing Committee.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep Biafﬁne Attention for Neural Dependency\nParsing. In Proceedings of the 5th International\nConference on Learning Representations, ICLR’17.\nHan He and Jinho D. Choi. 2019. Establishing Strong\nBaselines for the New Decade: Sequence Tagging,\nSyntactic and Semantic Parsing with BERT. In\nProceedings of the 33rd International Florida Ar-\ntiﬁcial Intelligence Research Society Conference ,\nFLAIRS’19.\nLuheng He, Kenton Lee, Omer Levy, and Luke Zettle-\nmoyer. 2018. Jointly Predicting Predicates and Ar-\nguments in Neural Semantic Role Labeling. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 364–369, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention.\nJohn Hewitt and Percy Liang. 2019. Designing and\nInterpreting Probes with Control Tasks. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2733–2743, Hong\nKong, China. Association for Computational Lin-\nguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A\nStructural Probe for Finding Syntax in Word Repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What Does BERT Learn about the Structure\nof Language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657, Florence, Italy. Associa-\ntion for Computational Linguistics.\nDan Kondratyuk and Milan Straka. 2019. 75 Lan-\nguages, 1 Model: Parsing Universal Dependencies\nUniversally. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2779–2795, Hong Kong, China. As-\nsociation for Computational Linguistics.\nYing Lin, Heng Ji, Fei Huang, and Lingfei Wu. 2020.\nA Joint Neural Model for Information Extraction\nwith Global Features. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7999–8009, Online. Association\nfor Computational Linguistics.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen Sesame: Getting inside BERT’s Linguistic\nKnowledge. In Proceedings of the 2019 ACL Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP , pages 241–253, Florence,\nItaly. Association for Computational Linguistics.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic Knowledge and Transferability of Contextual\nRepresentations. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019b. Multi-Task Deep Neural Net-\nworks for Natural Language Understanding. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4487–\n4496, Florence, Italy. Association for Computational\nLinguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019c.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nChristos Louizos, Max Welling, and Diederik P.\nKingma. 2018. Learning Sparse Neural Networks\nthrough L0 Regularization. In International Confer-\nence on Learning Representations.\nMatthew E. Peters, Sebastian Ruder, and Noah A.\nSmith. 2019. To Tune or Not to Tune? Adapt-\ning Pretrained Representations to Diverse Tasks. In\nProceedings of the 4th Workshop on Representation\nLearning for NLP (RepL4NLP-2019) , pages 7–14,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nHwee Tou Ng, Anders Björkelund, Olga Uryupina,\nYuchen Zhang, and Zhi Zhong. 2013. Towards Ro-\nbust Linguistic Analysis using OntoNotes. In Pro-\nceedings of the Seventeenth Conference on Computa-\ntional Natural Language Learning , pages 143–152,\n5565\nSoﬁa, Bulgaria. Association for Computational Lin-\nguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT Rediscovers the Classical NLP Pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing Multi-Head\nSelf-Attention: Specialized Heads Do the Heavy\nLifting, the Rest Can Be Pruned. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797–5808, Florence,\nItaly. Association for Computational Linguistics.\nAlex Wang, Jan Hula, Patrick Xia, Raghavendra Pappa-\ngari, R. Thomas McCoy, Roma Patel, Najoung Kim,\nIan Tenney, Yinghui Huang, Katherin Yu, Shuning\nJin, Berlin Chen, Benjamin Van Durme, Edouard\nGrave, Ellie Pavlick, and Samuel R. Bowman. 2019.\nCan You Tell Me How to Get Past Sesame Street?\nSentence-Level Pretraining Beyond Language Mod-\neling. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 4465–4476, Florence, Italy. Association\nfor Computational Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A Multi-Task Benchmark and Analysis Plat-\nform for Natural Language Understanding. In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nRalph Weischedel, Martha Palmer, Mitchell Marcus,\nEduard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-\nanwen Xue, Ann Taylor, Jeff Kaufman, Michelle\nFranchini, et al. 2013. Ontonotes release 5.0\nldc2013t19. Linguistic Data Consortium, Philadel-\nphia, PA.\nZhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. 2020.\nPerturbed Masking: Parameter-free Probing for Ana-\nlyzing and Interpreting BERT. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4166–4176, Online. As-\nsociation for Computational Linguistics.\nQingrong Xia, Zhenghua Li, and Min Zhang. 2019.\nA Syntax-aware Multi-task Learning Framework for\nChinese Semantic Role Labeling. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5382–5392, Hong Kong,\nChina. Association for Computational Linguistics.\nJuntao Yu, Bernd Bohnet, and Massimo Poesio. 2020.\nNamed Entity Recognition as Dependency Parsing.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n6470–6476, Online. Association for Computational\nLinguistics.\nYu Zhang, Houquan Zhou, and Zhenghua Li. 2020.\nFast and Accurate Neural CRF Constituency Parsing.\nIn Proceedings of the Twenty-Ninth International\nJoint Conference on Artiﬁcial Intelligence, IJCAI-\n20, pages 4046–4053. International Joint Confer-\nences on Artiﬁcial Intelligence Organization. Main\ntrack.\nYiyun Zhao and Steven Bethard. 2020. How does\nBERT’s attention change when you ﬁne-tune? An\nanalysis methodology and a case study in negation\nscope. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4729–4747, Online. Association for Computa-\ntional Linguistics.\n5566\nA Appendix\nA.1 Corpus Statistics\nTable 4 and 5 describes statistics of the POS, NER,\nDEP, CON, SRL datasets used in our experiments.\nSentences Tokens\nTRN 75,187 1,299,312\nDEV 9,603 163,104\nTST 9,479 169,579\nTable 4: POS, DEP, CON, SRL statistics of OntoNotes 5.\nTRN/DEV/TST: training/development/evaluation set.\nSentences Tokens\nTRN 59,924 1,088,503\nDEV 8,528 147,724\nTST 8,262 152,728\nTable 5: NER statistics of OntoNotes 5. TRN/DEV/TST:\ntraining/development/evaluation set.\nA.2 Hyper-Parameter Conﬁguration\nThe hyper-parameters used in our models are de-\nscribed in Table 6.\nBERT Encoder\nname bert-base-cased\nlayers tag 12\nhidden size 768\nsubword dropout 0.2\nAdam Optimizer\nencoder lr 5e-5\ndecoder lr 1e-3\nϵ 1e-8\nepochs 30\nwarm up 10\nNER Decoder\nMLP size 150\ndropout 0.5\nDEP Decoder\narc MLP size 500\nrel MLP size 100\ndropout 0.33\nCON Decoder\nspan MLP size 500\nlabel MLP size 100\ndropout 0.33\nSRL Decoder\nargument ratio 0.8\npredicate ratio 0.4\nspan width size 20\nmax Span width 30\nlabel MLP size 100\ndropout 0.2\nTable 6: Hyper-parameters settings.\nA.3 Extra BERT Probing Results\nA.3.1 Dependency Parsing\nBERT POS NER CON SRL MTL-5\n30\n20\n10\n0\n10 prep\npobj\nnsub\ndet\nroot\nnn\namod\ndobj\nadvm\naux\ncc\nconj\ndep\nposs\nccom\ncop\nmark\nxcom\nnum\nrcmo\nFigure 7: DEP probing results comparison.\nCompared to STL, a similar trending withPOS and\nNER can be observed that MTL improves only cer-\ntain relations as shown in Figure 7. Among the 4\ntasks, POS improves the least tags in terms of both\noverall accuracy and probing accuracy. It improves\ndobj (direct object) and expl (expletive) possi-\nbly due to that its decoder needs to assign a verb\ntag to the ROOT verb and EX (existential there) to\n“there”, enhancing the representations of these two.\nRegarding NER, it mainly improves modiﬁers that\nmodify nouns which comprises named entities. In\nthe case of CON, modiﬁers and complement argu-\nments are improved, most of which usually reside\nin NP (Noun Phrase) or VP (Verb Phrase) phrases,\nplacing upper bounds on the distance of dependen-\ncies. As regards SRL, it improves subjects and\nclausal relations.\nA.3.2 Semantic Role Labeling\nBERT POS NER DEP CON MTL-5\n50\n40\n30\n20\n10\n0\n10 ARG1\nARG0\nARG2\nM-TMP\nM-DIS\nM-ADV\nM-MOD\nM-LOC\nM-MNR\nM-NEG\nR-A1\nR-A0\nC-A1\nM-PRP\nM-DIR\nARG3\nARG4\nM-CAU\nM-PRD\nM-ADJ\nFigure 8: SRL probing results comparison.\n5567\nIn comparison to STL illustrated in Figure 8, POS\nand CON mainly improves ARG0-3 (agent, patient,\ninstrument, benefactive, attribute and starting point)\nand some modiﬁers including ARGM-TMP (tempo-\nral), ARGM-CAU (cause), ARGM-PRD (secondary\npredication), ARGM-EXT (extent), ARGM-PNC\n(purpose) and ARGM-REC (reciprocals). Both POS\nand CON reveal syntactic functions which appear to\ncoordinate attentions on semantic roles in a similar\nway. Regarding NER and DEP, they improve argu-\nments that include referent or pronouns (R-ARG1,\nR-ARG0) and modiﬁers ( ARGM-NEG which is\nnegation, ARGM-CAU, ARGM-PRD, ARGM-EXT),\npossibly due to the biafﬁne decoders they employ\nanalogously enhance the heads.\nA.3.3 Detailed Probing Results\nWith STL as the baseline, probing results for each\ntask are recorded in Table 7 to 11 respectively. In\neach table, the ﬁrst column shows the labels sorted\nby their frequencies in the test set, the second col-\numn shows the mean probing results over 3 runs\nfor STL which is the baseline, other columns show\nthe probing results for BERT and MTL models\nwith their differences against the STL baseline (in\nparentheses, green indicates improvements and red\nindicates decreases). BERT probing scores are usu-\nally much lower than STL, which is expected as\nit is not ﬁne-tuned on the task data. MTL probing\nscores are also generally lower than STL, implying\ninterference with other joint tasks.\n5568\nSTL BERT NER DEP CON SRL MTL-5\nNN 91.44 44.11(–47.33) 78.22(–13.22) 75.61(–15.83) 75.88(–15.56) 76.80(–14.64) 54.75(–36.69)\nIN 89.79 53.45(–36.34) 88.46(–1.33) 82.43(–7.36) 71.27(–18.52) 65.22(–24.57) 53.86(–35.93)\nDT 93.93 65.15(–28.78) 73.36(–20.57) 74.13(–19.80) 81.62(–12.31) 75.69(–18.24) 64.11(–29.82)\nNNP 90.21 67.16(–23.05) 81.29(–8.92) 71.09(–19.12) 73.17(–17.04) 80.42(–9.79) 62.11(–28.10)\nJJ 83.59 37.96(–45.63) 71.73(–11.86) 71.10(–12.49) 70.09(–13.50) 77.91(–5.68) 63.35(–20.24)\n. 95.02 91.03(–3.99) 95.36(+0.34) 95.82(+0.80) 95.91(+0.89) 95.99(+0.97) 94.61(–0.41)\nNNS 91.75 66.19(–25.56) 80.43(–11.32) 76.77(–14.98) 77.16(–14.59) 84.17(–7.58) 63.66(–28.09)\nPRP 93.76 65.69(–28.07) 91.17(–2.59) 82.31(–11.45) 84.87(–8.89) 87.68(–6.08) 74.13(–19.63)\nRB 83.33 28.92(–54.41) 75.44(–7.89) 67.30(–16.03) 63.78(–19.55) 67.86(–15.47) 54.27(–29.06)\n, 95.23 86.94(–8.29) 89.07(–6.16) 78.15(–17.08) 70.08(–25.15) 93.50(–1.73) 69.87(–25.36)\nVB 90.17 69.52(–20.65) 84.21(–5.96) 73.87(–16.30) 76.77(–13.40) 80.49(–9.68) 68.76(–21.41)\nVBD 90.97 56.33(–34.64) 85.68(–5.29) 70.51(–20.46) 71.36(–19.61) 70.25(–20.72) 60.42(–30.55)\nCC 91.40 68.26(–23.14) 90.09(–1.31) 71.91(–19.49) 68.02(–23.38) 83.39(–8.01) 67.49(–23.91)\nVBZ 92.44 74.91(–17.53) 90.40(–2.04) 78.07(–14.37) 80.85(–11.59) 77.62(–14.82) 66.56(–25.88)\nVBP 90.02 56.63(–33.39) 84.35(–5.67) 72.21(–17.81) 73.85(–16.17) 71.60(–18.42) 60.45(–29.57)\nVBN 88.81 53.78(–35.03) 74.71(–14.10) 67.99(–20.82) 70.24(–18.57) 76.35(–12.46) 55.83(–32.98)\nCD 93.10 61.67(–31.43) 74.84(–18.26) 88.25(–4.85) 89.97(–3.13) 89.95(–3.15) 66.87(–26.23)\nVBG 92.63 65.44(–27.19) 85.51(–7.12) 78.36(–14.27) 82.53(–10.10) 82.19(–10.44) 66.12(–26.51)\nTO 92.38 85.24(–7.14) 87.95(–4.43) 83.07(–9.31) 79.26(–13.12) 78.24(–14.14) 74.63(–17.75)\nMD 92.70 65.72(–26.98) 91.93(–0.77) 90.69(–2.01) 90.29(–2.41) 89.55(–3.15) 89.29(–3.41)\nPRP$ 92.33 76.90(–15.43) 89.37(–2.96) 88.84(–3.49) 86.96(–5.37) 86.48(–5.85) 80.86(–11.47)\nUH 93.66 75.95(–17.71) 87.03(–6.63) 76.42(–17.24) 83.97(–9.69) 85.52(–8.14) 73.79(–19.87)\nHYPH 91.98 90.15(–1.83) 92.01(+0.03) 89.90(–2.08) 88.20(–3.78) 92.47(+0.49) 91.23(–0.75)\nPOS 98.85 93.01(–5.84) 90.80(–8.05) 98.21(–0.64) 97.86(–0.99) 98.37(–0.48) 92.02(–6.83)\n’ 89.05 76.84(–12.21) 74.25(–14.80) 77.14(–11.91) 71.12(–17.93) 85.48(–3.57) 70.51(–18.54)\n“ 89.54 76.98(–12.56) 79.12(–10.42) 81.55(–7.99) 68.74(–20.80) 87.44(–2.10) 69.67(–19.87)\nWDT 87.26 68.74(–18.52) 85.27(–1.99) 82.54(–4.72) 83.23(–4.03) 80.71(–6.55) 80.34(–6.92)\nWP 90.19 76.18(–14.01) 86.99(–3.20) 81.88(–8.31) 83.79(–6.40) 77.35(–12.84) 75.89(–14.30)\nWRB 89.15 72.35(–16.80) 87.38(–1.77) 86.68(–2.47) 83.99(–5.16) 79.75(–9.40) 78.44(–10.71)\nRP 82.13 74.33(–7.80) 72.20(–9.93) 79.06(–3.07) 82.93(+0.80) 76.13(–6.00) 71.68(–10.45)\n: 84.58 66.48(–18.10) 76.22(–8.36) 85.89(+1.31) 80.03(–4.55) 85.33(+0.75) 84.02(–0.56)\nJJR 87.18 56.62(–30.56) 71.87(–15.31) 73.65(–13.53) 72.29(–14.89) 72.22(–14.96) 58.62(–28.56)\nNNPS 84.39 72.11(–12.28) 75.45(–8.94) 71.02(–13.37) 79.74(–4.65) 79.81(–4.58) 79.96(–4.43)\nEX 95.33 89.25(–6.08) 92.18(–3.15) 90.55(–4.78) 90.56(–4.77) 90.77(–4.56) 88.49(–6.84)\nJJS 83.41 58.95(–24.46) 77.58(–5.83) 80.64(–2.77) 76.57(–6.84) 79.33(–4.08) 69.87(–13.54)\nRBR 87.29 67.71(–19.58) 66.07(–21.22) 66.07(–21.22) 69.96(–17.33) 65.17(–22.12) 58.00(–29.29)\n-LRB- 91.54 83.76(–7.78) 89.68(–1.86) 89.51(–2.03) 88.83(–2.71) 88.66(–2.88) 85.96(–5.58)\n-RRB- 92.18 83.67(–8.51) 88.78(–3.40) 88.78(–3.40) 87.07(–5.11) 89.97(–2.21) 86.22(–5.96)\n$ 97.30 95.38(–1.92) 95.38(–1.92) 94.99(–2.31) 94.80(–2.50) 96.92(–0.38) 94.99(–2.31)\nPDT 87.15 84.94(–2.21) 83.54(–3.61) 82.73(–4.42) 81.12(–6.03) 84.74(–2.41) 82.53(–4.62)\nRBS 86.31 72.32(–13.99) 72.62(–13.69) 77.68(–8.63) 71.73(–14.58) 77.98(–8.33) 74.41(–11.90)\nFW 72.51 64.95(–7.56) 65.98(–6.53) 69.07(–3.44) 76.97(+4.46) 68.04(–4.47) 70.79(–1.72)\nNFP 90.56 86.67(–3.89) 88.89(–1.67) 93.33(+2.77) 90.00(–0.56) 92.22(+1.66) 86.67(–3.89)\nWP$ 92.99 89.47(–3.52) 95.62(+2.63) 94.74(+1.75) 92.11(–0.88) 94.74(+1.75) 92.98(–0.01)\nXX 83.34 81.25(–2.09) 82.29(–1.05) 86.46(+3.12) 83.33(–0.01) 82.29(–1.05) 81.25(–2.09)\nSYM 91.11 86.67(–4.44) 88.89(–2.22) 94.45(+3.34) 88.89(–2.22) 93.33(+2.22) 91.11(-0.00)\nADD 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nLS 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nAFX 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nTable 7: Probing results for POS. STL: the basline, BERT:pre-trained BERT before ﬁne-tuning.\n5569\nSTL BERT POS DEP CON SRL MTL-5\nGPE 91.40 76.43(–14.97) 82.02(–9.38) 74.70(–16.70) 79.97(–11.43) 84.97(–6.43) 79.73(–11.67)\nPERSON 85.90 85.51(–0.39) 85.38(–0.52) 86.67(+0.77) 83.65(–2.25) 83.96(–1.94) 83.60(–2.30)\nORG 73.81 67.80(–6.01) 74.95(+1.14) 69.01(–4.80) 68.04(–5.77) 72.09(–1.72) 69.99(–3.82)\nDATE 87.39 63.17(–24.22) 62.51(–24.88) 78.49(–8.90) 73.53(–13.86) 82.25(–5.14) 83.37(–4.02)\nCARDINAL 76.33 66.63(–9.70) 66.41(–9.92) 64.81(–11.52) 79.22(+2.89) 73.05(–3.28) 67.06(–9.27)\nNORP 93.34 82.28(–11.06) 89.57(–3.77) 88.47(–4.87) 82.72(–10.62) 89.58(–3.76) 90.37(–2.97)\nPERCENT 99.81 99.71(–0.10) 98.95(–0.86) 99.71(–0.10) 99.43(–0.38) 99.90(+0.09) 99.71(–0.10)\nMONEY 98.30 97.77(–0.53) 96.60(–1.70) 97.98(–0.32) 96.60(–1.70) 97.13(–1.17) 96.07(–2.23)\nTIME 79.25 77.36(–1.89) 78.14(–1.11) 72.48(–6.77) 78.77(–0.48) 77.51(–1.74) 77.05(–2.20)\nORDINAL 90.94 89.74(–1.20) 90.26(–0.68) 92.65(+1.71) 90.43(–0.51) 88.03(–2.91) 84.61(–6.33)\nLOC 74.86 69.83(–5.03) 76.35(+1.49) 72.81(–2.05) 82.87(+8.01) 74.68(–0.18) 76.91(+2.05)\nWORK_OF_ART 78.11 76.51(–1.60) 74.50(–3.61) 76.31(–1.80) 74.50(–3.61) 79.52(+1.41) 67.87(–10.24)\nFAC 82.96 77.78(–5.18) 86.42(+3.46) 81.73(–1.23) 74.07(–8.89) 85.43(+2.47) 86.67(+3.71)\nQUANTITY 95.55 83.81(–11.74) 95.56(+0.01) 92.70(–2.85) 87.62(–7.93) 94.92(–0.63) 89.84(–5.71)\nPRODUCT 85.09 81.58(–3.51) 86.84(+1.75) 88.60(+3.51) 76.75(–8.34) 84.65(–0.44) 78.95(–6.14)\nEVENT 77.25 68.25(–9.00) 74.60(–2.65) 71.43(–5.82) 66.14(–11.11) 71.43(–5.82) 70.37(–6.88)\nLAW 91.67 92.50(+0.83) 87.50(–4.17) 90.83(–0.84) 86.67(–5.00) 90.00(–1.67) 87.50(–4.17)\nLANGUAGE 98.48 95.45(–3.03) 100.00(+1.52) 98.48(-0.00) 98.48(-0.00) 98.48(-0.00) 98.48(-0.00)\nTable 8: Probing results for NER. STL: the basline, BERT:pre-trained BERT before ﬁne-tuning.\n5570\nSTL BERT POS NER CON SRL MTL-5\nprep 72.80 72.62(–0.18) 69.98(–2.82) 70.90(–1.90) 73.11(+0.31) 76.60(+3.80) 78.02(+5.22)\npobj 95.64 96.35(+0.71) 95.01(–0.63) 91.70(–3.94) 94.67(–0.97) 93.45(–2.19) 88.59(–7.05)\nnsubj 76.92 76.36(–0.56) 66.45(–10.47) 73.78(–3.14) 73.75(–3.17) 85.61(+8.69) 70.69(–6.23)\ndet 94.07 93.70(–0.37) 93.22(–0.85) 93.87(–0.20) 95.68(+1.61) 95.72(+1.65) 91.66(–2.41)\nroot 96.28 96.25(–0.03) 95.83(–0.45) 96.49(+0.21) 95.10(–1.18) 97.77(+1.49) 95.18(–1.10)\nnn 89.08 89.47(+0.39) 84.77(–4.31) 86.99(–2.09) 92.59(+3.51) 89.99(+0.91) 89.98(+0.90)\namod 92.76 93.33(+0.57) 91.38(–1.38) 91.63(–1.13) 95.36(+2.60) 92.70(–0.06) 93.50(+0.74)\ndobj 93.76 94.55(+0.79) 94.88(+1.12) 93.60(–0.16) 93.46(–0.30) 92.95(–0.81) 89.07(–4.69)\nadvmod 70.58 69.12(–1.46) 66.11(–4.47) 71.85(+1.27) 69.23(–1.35) 68.96(–1.62) 67.92(–2.66)\naux 84.70 84.53(–0.17) 84.03(–0.67) 85.78(+1.08) 84.22(–0.48) 86.06(+1.36) 84.02(–0.68)\ncc 59.17 57.86(–1.31) 58.10(–1.07) 58.73(–0.44) 57.11(–2.06) 54.40(–4.77) 64.18(+5.01)\nconj 66.66 64.82(–1.84) 43.20(–23.46) 50.76(–15.90) 65.78(–0.88) 48.50(–18.16) 43.52(–23.14)\ndep 41.44 42.77(+1.33) 32.82(–8.62) 41.88(+0.44) 41.22(–0.22) 42.01(+0.57) 38.78(–2.66)\nposs 78.20 77.96(–0.24) 79.99(+1.79) 81.39(+3.19) 77.72(–0.48) 72.42(–5.78) 79.02(+0.82)\nccomp 71.24 71.47(+0.23) 50.24(–21.00) 69.80(–1.44) 65.29(–5.95) 66.38(–4.86) 61.40(–9.84)\ncop 88.80 87.09(–1.71) 87.47(–1.33) 89.91(+1.11) 88.74(–0.06) 88.04(–0.76) 85.88(–2.92)\nmark 89.78 90.88(+1.10) 89.44(–0.34) 91.01(+1.23) 90.29(+0.51) 87.65(–2.13) 89.75(–0.03)\nxcomp 68.11 70.53(+2.42) 65.81(–2.30) 72.80(+4.69) 73.20(+5.09) 73.33(+5.22) 71.54(+3.43)\nnum 84.94 87.61(+2.67) 83.58(–1.36) 87.48(+2.54) 91.50(+6.56) 88.02(+3.08) 90.49(+5.55)\nrcmod 51.72 52.62(+0.90) 41.62(–10.10) 41.96(–9.76) 52.22(+0.50) 59.36(+7.64) 51.52(–0.20)\nadvcl 52.26 54.93(+2.67) 41.78(–10.48) 53.70(+1.44) 49.97(–2.29) 42.06(–10.20) 48.50(–3.76)\nneg 82.06 83.40(+1.34) 81.66(–0.40) 82.67(+0.61) 84.82(+2.76) 81.09(–0.97) 78.43(–3.63)\nauxpass 97.37 97.51(+0.14) 97.18(–0.19) 97.26(–0.11) 95.74(–1.63) 95.66(–1.71) 96.24(–1.13)\nnsubjpass 79.34 83.76(+4.42) 73.26(–6.08) 79.43(+0.09) 73.45(–5.89) 77.24(–2.10) 74.59(–4.75)\npossessive 99.23 99.23(-0.00) 99.23(-0.00) 99.20(–0.03) 99.26(+0.03) 99.20(–0.03) 99.33(+0.10)\npcomp 87.55 90.06(+2.51) 83.03(–4.52) 86.95(–0.60) 85.11(–2.44) 84.57(–2.98) 80.15(–7.40)\ndiscourse 74.04 73.80(–0.24) 50.24(–23.80) 62.18(–11.86) 56.72(–17.32) 73.14(–0.90) 52.05(–21.99)\npartmod 60.08 59.47(–0.61) 59.52(–0.56) 60.49(+0.41) 63.14(+3.06) 64.85(+4.77) 62.58(+2.50)\nappos 54.16 47.88(–6.28) 41.85(–12.31) 47.30(–6.86) 54.65(+0.49) 50.12(–4.04) 43.55(–10.61)\nprt 96.44 95.99(–0.45) 96.39(–0.05) 96.69(+0.25) 96.83(+0.39) 95.30(–1.14) 96.09(–0.35)\nnumber 80.75 78.43(–2.32) 77.71(–3.04) 82.86(+2.11) 81.12(+0.37) 81.41(+0.66) 82.57(+1.82)\nquantmod 75.49 73.06(–2.43) 72.65(–2.84) 78.32(+2.83) 80.18(+4.69) 73.06(–2.43) 75.41(–0.08)\nparataxis 46.48 43.92(–2.56) 30.69(–15.79) 45.24(–1.24) 43.30(–3.18) 47.79(+1.31) 33.69(–12.79)\ninfmod 71.38 71.20(–0.18) 68.35(–3.03) 70.13(–1.25) 70.49(–0.89) 73.24(+1.86) 68.18(–3.20)\ntmod 84.89 87.64(+2.75) 69.23(–15.66) 78.02(–6.87) 76.01(–8.88) 80.31(–4.58) 62.64(–22.25)\nexpl 85.73 86.27(+0.54) 86.93(+1.20) 86.82(+1.09) 84.75(–0.98) 84.75(–0.98) 84.10(–1.63)\nmwe 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nnpadvmod 86.24 87.16(+0.92) 88.07(+1.83) 82.42(–3.82) 84.25(–1.99) 83.03(–3.21) 81.19(–5.05)\niobj 93.30 94.02(+0.72) 91.12(–2.18) 89.67(–3.63) 91.85(–1.45) 93.66(+0.36) 88.95(–4.35)\npredet 91.72 91.30(–0.42) 91.93(+0.21) 93.17(+1.45) 92.34(+0.62) 87.78(–3.94) 90.47(–1.25)\nacomp 89.60 89.17(–0.43) 88.33(–1.27) 89.60(-0.00) 89.81(+0.21) 87.05(–2.55) 89.17(–0.43)\ncsubj 56.90 57.76(+0.86) 57.19(+0.29) 59.48(+2.58) 53.45(–3.45) 51.72(–5.18) 55.17(–1.73)\npreconj 76.81 78.26(+1.45) 65.22(–11.59) 72.46(–4.35) 82.61(+5.80) 83.33(+6.52) 75.36(–1.45)\ncsubjpass 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 77.78(–22.22) 66.67(–33.33)\nTable 9: Probing results for DEP. STL: the basline, BERT:pre-trained BERT before ﬁne-tuning.\n5571\nSTL BERT POS NER DEP SRL MTL-5\nNP 85.72 55.97(–29.75) 65.53(–20.19) 72.55(–13.17) 67.25(–18.47) 71.80(–13.92) 57.58(–28.14)\nADVP 68.61 68.61(-0.00) 70.25(+1.64) 77.86(+9.25) 70.47(+1.86) 61.15(–7.46) 66.59(–2.02)\nADJP 67.83 53.25(–14.58) 69.33(+1.50) 65.40(–2.43) 57.69(–10.14) 64.42(–3.41) 63.43(–4.40)\nVP 79.29 62.83(–16.46) 66.39(–12.90) 70.30(–8.99) 78.07(–1.22) 54.85(–24.44) 69.70(–9.59)\nNML 82.92 74.89(–8.03) 81.51(–1.41) 82.17(–0.75) 81.66(–1.26) 84.24(+1.32) 76.02(–6.90)\nWHNP 77.87 62.26(–15.61) 75.12(–2.75) 73.27(–4.60) 78.82(+0.95) 70.63(–7.24) 71.14(–6.73)\nINTJ 76.61 72.67(–3.94) 84.84(+8.23) 78.11(+1.50) 68.79(–7.82) 73.13(–3.48) 74.11(–2.50)\nQP 94.55 90.45(–4.10) 93.49(–1.06) 93.73(–0.82) 91.70(–2.85) 91.65(–2.90) 88.28(–6.27)\nWHADVP 78.99 69.43(–9.56) 80.16(+1.17) 75.84(–3.15) 84.04(+5.05) 72.63(–6.36) 76.95(–2.04)\nPRT 80.85 79.46(–1.39) 80.90(+0.05) 81.20(+0.35) 83.63(+2.78) 73.71(–7.14) 76.34(–4.51)\nPP 73.52 69.86(–3.66) 79.00(+5.48) 78.31(+4.79) 72.83(–0.69) 69.86(–3.66) 71.00(–2.52)\nCONJP 90.87 91.78(+0.91) 90.41(–0.46) 85.84(–5.03) 88.58(–2.29) 88.13(–2.74) 87.67(–3.20)\nX 59.35 60.98(+1.63) 52.85(–6.50) 53.66(–5.69) 57.73(–1.62) 55.29(–4.06) 56.91(–2.44)\nWHADJP 95.37 91.67(–3.70) 93.52(–1.85) 96.29(+0.92) 94.44(–0.93) 94.44(–0.93) 91.67(–3.70)\nMETA 98.61 95.83(–2.78) 97.22(–1.39) 95.83(–2.78) 97.22(–1.39) 97.22(–1.39) 98.61(-0.00)\nUCP 100.00 100.00(-0.00) 98.25(–1.75) 100.00(-0.00) 98.25(–1.75) 100.00(-0.00) 100.00(-0.00)\nS 96.29 94.44(–1.85) 96.29(-0.00) 94.44(–1.85) 96.29(-0.00) 98.15(+1.86) 94.44(–1.85)\nLST 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nFRAG 83.33 100.00(+16.67) 100.00(+16.67) 94.44(+11.11) 94.44(+11.11) 88.89(+5.56) 100.00(+16.67)\nSBAR 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nSQ 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nWHPP 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nTOP 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nTable 10: Probing results for CON. STL: the basline, BERT:pre-trained BERT before ﬁne-tuning.\n5572\nSTL BERT POS NER DEP CON MTL-5\nARG1 77.75 71.88(–5.87) 78.78(+1.03) 77.86(+0.11) 77.08(–0.67) 79.58(+1.83) 74.58(–3.17)\nARG0 73.24 74.46(+1.22) 75.95(+2.71) 71.61(–1.63) 71.17(–2.07) 74.16(+0.92) 70.13(–3.11)\nARG2 89.69 80.43(–9.26) 89.81(+0.12) 89.65(–0.04) 90.15(+0.46) 89.86(+0.17) 87.36(–2.33)\nARGM-TMP 73.24 60.55(–12.69) 74.48(+1.24) 70.57(–2.67) 76.58(+3.34) 75.59(+2.35) 68.55(–4.69)\nARGM-DIS 70.40 50.18(–20.22) 69.56(–0.84) 75.67(+5.27) 69.58(–0.82) 77.51(+7.11) 60.41(–9.99)\nARGM-ADV 59.01 40.28(–18.73) 61.31(+2.30) 64.83(+5.82) 56.48(–2.53) 63.88(+4.87) 54.36(–4.65)\nARGM-MOD 82.53 80.46(–2.07) 81.77(–0.76) 84.41(+1.88) 82.29(–0.24) 81.59(–0.94) 78.54(–3.99)\nARGM-LOC 79.45 58.99(–20.46) 78.24(–1.21) 80.68(+1.23) 72.88(–6.57) 79.35(–0.10) 74.22(–5.23)\nARGM-MNR 80.67 69.91(–10.76) 79.33(–1.34) 80.52(–0.15) 80.20(–0.47) 80.89(+0.22) 77.23(–3.44)\nARGM-NEG 86.47 79.39(–7.08) 85.51(–0.96) 90.54(+4.07) 90.81(+4.34) 84.75(–1.72) 88.40(+1.93)\nR-ARG1 69.92 67.43(–2.49) 64.77(–5.15) 71.28(+1.36) 74.75(+4.83) 72.09(+2.17) 75.41(+5.49)\nR-ARG0 81.20 75.64(–5.56) 76.89(–4.31) 81.71(+0.51) 83.36(+2.16) 80.30(–0.90) 78.36(–2.84)\nC-ARG1 51.72 47.46(–4.26) 51.25(–0.47) 46.86(–4.86) 45.51(–6.21) 45.84(–5.88) 46.11(–5.61)\nARGM-PRP 80.48 69.05(–11.43) 78.81(–1.67) 82.06(+1.58) 77.38(–3.10) 83.49(+3.01) 80.16(–0.32)\nARGM-DIR 93.89 89.29(–4.60) 93.73(–0.16) 95.16(+1.27) 92.86(–1.03) 94.92(+1.03) 94.76(+0.87)\nARG3 91.00 82.89(–8.11) 91.98(+0.98) 90.91(–0.09) 87.34(–3.66) 90.91(–0.09) 89.04(–1.96)\nARG4 94.78 93.96(–0.82) 95.51(+0.73) 95.05(+0.27) 94.32(–0.46) 94.32(–0.46) 93.96(–0.82)\nARGM-CAU 62.34 55.37(–6.97) 63.37(+1.03) 66.76(+4.42) 68.74(+6.40) 67.98(+5.64) 65.73(+3.39)\nARGM-PRD 57.72 52.14(–5.58) 61.87(+4.15) 59.66(+1.94) 61.35(+3.63) 59.40(+1.68) 57.85(+0.13)\nARGM-ADJ 85.07 76.00(–9.07) 82.53(–2.54) 82.80(–2.27) 82.67(–2.40) 84.40(–0.67) 82.93(–2.14)\nARGM-EXT 87.37 86.93(–0.44) 90.85(+3.48) 87.80(+0.43) 89.32(+1.95) 88.89(+1.52) 83.44(–3.93)\nARGM-PNC 82.89 69.74(–13.15) 84.21(+1.32) 84.21(+1.32) 82.89(-0.00) 83.77(+0.88) 79.82(–3.07)\nARGM-GOL 86.76 76.71(–10.05) 89.95(+3.19) 88.58(+1.82) 78.54(–8.22) 85.84(–0.92) 84.93(–1.83)\nARGM-LVB 98.59 97.18(–1.41) 98.12(–0.47) 98.12(–0.47) 98.12(–0.47) 97.18(–1.41) 97.18(–1.41)\nR-ARGM-LOC 82.05 84.62(+2.57) 78.97(–3.08) 86.16(+4.11) 74.36(–7.69) 73.85(–8.20) 79.49(–2.56)\nR-ARGM-TMP 74.08 71.43(–2.65) 69.84(–4.24) 82.01(+7.93) 80.42(+6.34) 75.66(+1.58) 77.25(+3.17)\nR-ARG2 73.12 70.97(–2.15) 77.96(+4.84) 77.42(+4.30) 74.19(+1.07) 69.89(–3.23) 74.73(+1.61)\nC-ARG2 44.90 36.73(–8.17) 45.58(+0.68) 43.54(–1.36) 43.54(–1.36) 44.22(–0.68) 45.58(+0.68)\nC-ARG0 59.80 47.06(–12.74) 61.76(+1.96) 56.86(–2.94) 51.96(–7.84) 53.92(–5.88) 54.90(–4.90)\nARGM-REC 84.31 70.59(–13.72) 85.29(+0.98) 78.43(–5.88) 80.39(–3.92) 85.29(+0.98) 80.39(–3.92)\nARGM-COM 88.89 81.48(–7.41) 90.12(+1.23) 92.59(+3.70) 87.66(–1.23) 87.66(–1.23) 87.66(–1.23)\nC-ARGM-ADV 54.55 54.55(-0.00) 48.48(–6.07) 54.55(-0.00) 54.55(-0.00) 51.52(–3.03) 57.58(+3.03)\nR-ARGM-MNR 72.73 63.64(–9.09) 63.64(–9.09) 81.82(+9.09) 69.70(–3.03) 78.79(+6.06) 66.67(–6.06)\nARG5 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nC-ARGM-TMP 71.43 42.86(–28.57) 61.90(–9.53) 71.43(-0.00) 57.14(–14.29) 61.90(–9.53) 66.67(–4.76)\nR-ARGM-CAU 91.67 75.00(–16.67) 83.33(–8.34) 100.00(+8.33) 100.00(+8.33) 91.67(-0.00) 91.67(-0.00)\nC-ARGM-CAU 66.67 66.67(-0.00) 66.67(-0.00) 77.78(+11.11) 66.67(-0.00) 66.67(-0.00) 77.78(+11.11)\nC-ARG3 100.00 100.00(-0.00) 88.89(–11.11) 88.89(–11.11) 88.89(–11.11) 88.89(–11.11) 100.00(-0.00)\nR-ARG4 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nC-ARGM-EXT 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nR-ARGM-ADV 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nC-ARGM-LOC 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nC-ARGM-MNR 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nC-ARGM-MOD 100.00 50.00(–50.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nR-ARGM-PRP 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nARGM-PRX 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nARGA 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nR-ARGM-DIR 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nR-ARGM-PRD 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nC-ARGM-PRD 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nR-ARGM-EXT 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nC-ARGM-PRP 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nR-ARG3 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nARGM-PRR 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nARGM-DSP 100.00 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00) 100.00(-0.00)\nTable 11: Probing results for SRL. STL: the basline, BERT:pre-trained BERT before ﬁne-tuning.\n5573\nA.4 Results for Other Transformers\nWe also applied our pruning and probing methods\non 3 recent transformer encoders, RoBERTa (Liu\net al., 2019c), ELECTRA (Clark et al., 2020) and\nDeBERTa (He et al., 2020), to further demonstrate\nthe generality of our hypothesis. For all of them,\nwe use the base size version which has 12 layers\nwith 144 attention heads in total. Although we did\nnot tune hyper-parameters speciﬁcally for any of\nthem and re-used the same hyper-parameters of\nBERT, their results turned out to be as interesting\nas BERT results.\nTheir STL and MTL results are shown in Ta-\nble 12. Unsurprisingly, MTL-5 is outperformed by\nits single-task counterparts for all tasks and for all\ntransformer encoders, raising the dilemma behind\ntransformer-based MTL.\nTheir pruning results are shown in Table 13. Al-\nthough the results could be better tuned for each\ntransformer encoder, our DP strategy is still able to\nprune roughly 50% heads while keeping compara-\nble performance.\nTheir visualization of head utilization is illus-\ntrated in Figure 9, 11 and 13. Similar patterns\namong each transformer encoder can be observed,\nsupporting our claim that the MTL-DP model re-\nuses a very similar set of heads used by the STL-DP\nmodels.\nTheir probing results are illustrated in Figure 10,\n12 and 14, which also align with our ﬁndings.\nSpeciﬁcally, the DEP probing results on trans-\nformer encoders are already very high even without\nﬁne-tuning on actual dependency treebanks (Fig-\nure 10f, 12f and 14f), demonstrating the pluripo-\ntency of stem cells. Other ﬁgures show similar\npatterns that stem cell heads get specialized during\nSTL but lose specialities in MTL, supporting our\nStem Cell Hypothesis.\n5574\nPOS NER DEP CON SRL MTL-5\nPOS 98.33 ± 0.01 98.30 ± 0.01 98.34 ± 0.00 98.33 ± 0.02 98.31 ± 0.02 98.29 ± 0.02\nNER 89.44 ± 0.04 88.93 ± 0.16 89.84 ± 0.14 89.65 ± 0.02 89.37 ± 0.19 89.66 ± 0.17\nDEP 94.48 ± 0.05 94.46 ± 0.04 94.56 ± 0.05 94.55 ± 0.02 94.44 ± 0.02 94.38 ± 0.03\nCON 94.82 ± 0.01 94.76 ± 0.04 94.88 ± 0.03 94.89 ± 0.05 94.88 ± 0.01 94.75 ± 0.01\nSRL 84.04 ± 0.05 83.31 ± 0.11 84.13 ± 0.08 84.14 ± 0.02 83.52 ± 0.07 83.56 ± 0.10\n(a) RoBERTa (Liu et al., 2019c) performance.\nPOS NER DEP CON SRL MTL-5\nPOS 97.94 ± 0.04 97.97 ± 0.02 97.94 ± 0.01 97.95 ± 0.01 97.96 ± 0.05 97.93 ± 0.01\nNER 88.62 ± 0.03 88.22 ± 0.05 88.46 ± 0.20 88.82 ± 0.14 88.25 ± 0.11 88.38 ± 0.04\nDEP 94.52 ± 0.08 94.47 ± 0.08 94.61 ± 0.01 94.65 ± 0.03 94.53 ± 0.07 94.39 ± 0.04\nCON 95.01 ± 0.01 95.04 ± 0.02 95.10 ± 0.06 95.14 ± 0.03 95.08 ± 0.07 94.91 ± 0.04\nSRL 84.35 ± 0.07 83.44 ± 0.05 84.55 ± 0.06 84.61 ± 0.11 83.81 ± 0.11 84.09 ± 0.06\n(b) ELECTRA (Clark et al., 2020) performance.\nPOS NER DEP CON SRL MTL-5\nPOS 98.33 ± 0.02 98.26 ± 0.02 98.32 ± 0.01 98.32 ± 0.01 98.31 ± 0.02 98.30 ± 0.02\nNER 89.29 ± 0.18 89.02 ± 0.21 89.67 ± 0.09 89.65 ± 0.25 89.29 ± 0.36 89.28 ± 0.18\nDEP 94.50 ± 0.04 94.54 ± 0.02 94.64 ± 0.03 94.61 ± 0.07 94.56 ± 0.04 94.42 ± 0.03\nCON 94.91 ± 0.04 94.94 ± 0.02 94.96 ± 0.01 95.06 ± 0.01 94.98 ± 0.03 94.82 ± 0.03\nSRL 84.26 ± 0.05 83.42 ± 0.15 84.34 ± 0.04 84.36 ± 0.09 83.62 ± 0.06 83.37 ± 0.01\n(c) DeBERTa (He et al., 2020) performance.\nTable 12: Performance of single-task learning (main diagonal highlighted in gray), multi-task learning on all 5\ntasks (MTL-5), and multi-task learning on every pair of the tasks (non-diagonal cells; e.g., DEP’th row inNER’th\ncolumn is the DEP result of the joint model between DEP and NER).\nPerformance % of Attention Heads Kept\nSTL STL-DP MTL MTL-DP STL-DP MTL-DP\nPOS 98.33 ± 0.01 98.35 ± 0.02 98.29 ± 0.02 98.38 ± 0.01 41.90 ± 0.40 53.47 ± 1.39\nNER 88.93 ± 0.16 89.08 ± 0.16 89.66 ± 0.17 89.52 ± 0.05 58.33 ± 3.48 53.47 ± 1.39\nDEP 94.56 ± 0.05 94.29 ± 0.01 94.38 ± 0.03 94.48 ± 0.03 64.58 ± 1.21 53.47 ± 1.39\nCON 94.89 ± 0.05 94.59 ± 0.05 94.75 ± 0.01 94.78 ± 0.06 55.33 ± 2.00 53.47 ± 1.39\nSRL 83.52 ± 0.07 83.53 ± 0.06 83.56 ± 0.10 83.57 ± 0.14 45.14 ± 0.70 53.47 ± 1.39\n(a) RoBERTa (Liu et al., 2019c) pruning results.\nPerformance % of Attention Heads Kept\nSTL STL-DP MTL MTL-DP STL-DP MTL-DP\nPOS 97.94 ± 0.04 98.01 ± 0.01 97.93 ± 0.01 98.03 ± 0.02 35.88 ± 2.12 44.67 ± 2.44\nNER 88.22 ± 0.05 88.04 ± 0.24 88.38 ± 0.04 88.11 ± 0.22 58.33 ± 2.78 44.67 ± 2.44\nDEP 94.61 ± 0.01 94.36 ± 0.04 94.39 ± 0.04 94.40 ± 0.01 58.80 ± 1.06 44.67 ± 2.44\nCON 95.14 ± 0.03 94.82 ± 0.02 94.91 ± 0.04 94.86 ± 0.07 52.78 ± 2.50 44.67 ± 2.44\nSRL 83.81 ± 0.11 83.71 ± 0.10 84.09 ± 0.06 84.07 ± 0.06 41.20 ± 0.40 44.67 ± 2.44\n(b) ELECTRA (Clark et al., 2020) pruning results.\nPerformance % of Attention Heads Kept\nSTL STL-DP MTL MTL-DP STL-DP MTL-DP\nPOS 98.33 ± 0.02 98.37 ± 0.01 98.30 ± 0.02 98.39 ± 0.01 50.93 ± 3.13 66.67 ± 1.84\nNER 89.02 ± 0.21 88.72 ± 0.18 89.28 ± 0.18 89.08 ± 0.07 66.67 ± 3.67 66.67 ± 1.84\nDEP 94.64 ± 0.03 94.42 ± 0.02 94.42 ± 0.03 94.60 ± 0.01 54.63 ± 2.00 66.67 ± 1.84\nCON 95.06 ± 0.01 94.93 ± 0.03 94.82 ± 0.03 94.94 ± 0.13 61.11 ± 1.20 66.67 ± 1.84\nSRL 83.62 ± 0.06 83.78 ± 0.13 83.37 ± 0.01 83.37 ± 0.00 55.09 ± 0.40 66.67 ± 1.84\n(c) DeBERTa (He et al., 2020) pruning results.\nTable 13: Results of single-task learning (STL), STL with static pruning (STL-SP) and multi-task learning on the\n5 tasks with/without dynamic pruning (MTL/MTL-DP). The STL Performance column is equivalent to the main\ndiagonal in Table 12.\n5575\n(a) POS\n (b) NER\n (c) DEP\n (d) CON\n (e) SRL\n1 2 3 4 5 6 7 8 9 10 11 12\nHeads\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12 Layers\n(f) 3-run utilization of the MTL-DP model, where each run is\nencoded in a RGB channel. Darker indicates higher utilization.\n1 2 3 4 5 6 7 8 9 10 11 12\nHeads\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12 Layers\n(g) Average head utilization rates among the 5 tasks in 3 runs.\nDarker cells indicate higher utilization rates.\nFigure 9: Head utilization of the RoBERTa (Liu et al., 2019c) STL-DP models (a - e, g) and the MTL-DP model\n(f).\nRoBERTa NER DEP CON SRL MTL-5 STL\n40\n30\n20\n10\n0\n10\n20\nIN\nNNP\nJJ\n.\nNNS\nRB\n,\nVBD\nCC\nVBN\nCD\nVBG\nTO\nUH\nPOS\nWDT\n:\nEX\nJJS\nNFP\nWP$\n(a) POS\nRoBERTa POS DEP CON SRL MTL-5 STL\n10\n5\n0\n5\nGPE\nPER\nORG\nDATE\nCARD\nNORP\n%\n$\nTIME\nORD\nLOC\nWOA\nFAC\nQUA\nPRDT\nEVNT\nLAW\nLANG (b) NER\nRoBERTa POS NER CON SRL MTL-5\n60\n50\n40\n30\n20\n10\n0\n10\n20 prep\npobj\nnsub\ndet\nroot\nnn\namod\ndobj\nadvm\naux\ncc\nconj\ndep\nposs\nccom\ncop\nmark\nxcom\nnum\nrcmo (c) DEP\nRoBERTa POS NER DEP SRL MTL-5\n10\n5\n0\n5\n10\n15\n20\n25\nNP\nADVP\nADJP\nVP\nNML\nWHNP\nINTJ\nQP\nWHADVP\nPRT\nPP\nCONJP\nX\nWHADJP\nMETA\nUCP\nS\nLST\nFRAG\nSBAR\n(d) CON\nRoBERTa POS NER DEP CON MTL-5\n40\n20\n0\n20\nARG1\nARG0\nARG2\nM-TMP\nM-DIS\nM-ADV\nM-MOD\nM-LOC\nM-MNR\nM-NEG\nR-A1\nR-A0\nC-A1\nM-PRP\nM-DIR\nARG3\nARG4\nM-CAU\nM-PRD\nM-ADJ (e) SRL\n1 2 3 4 5 6 7 8 9 10 11 12\nLayers\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy\npunct\nprep\npobj\nnsubj\ndet\nroot\nnn\namod\ndobj\nadvmod\naux\ncc\nconj\ndep\nposs\nccomp\ncop\nmark\nxcomp\nnum\nrcmodadvcl\nneg\nauxpass\nnsubjpass\npossessive\npcomp\ndiscourse\npartmod\nappos\nprt\nnumber\nquantmod\nparataxis\ninfmod\ntmod\nexpl\nmwe\nnpadvmod\niobjpredet\nacomp\ncsubj\npreconj\ncsubjpass (f) DEP layer analysis.\nFigure 10: The RoBERTa (Liu et al., 2019c) probing results comparison (a - e) and layer analysis of pre-trained\nheads (g).\n5576\n(a) POS\n (b) NER\n (c) DEP\n (d) CON\n (e) SRL\n1 2 3 4 5 6 7 8 9 10 11 12\nHeads\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12 Layers\n(f) 3-run utilization of the MTL-DP model, where each run is\nencoded in a RGB channel. Darker indicates higher utilization.\n1 2 3 4 5 6 7 8 9 10 11 12\nHeads\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12 Layers\n(g) Average head utilization rates among the 5 tasks in 3 runs.\nDarker cells indicate higher utilization rates.\nFigure 11: Head utilization of the ELECTRA (Clark et al., 2020) STL-DP models (a - e, g) and the MTL-DP\nmodel (f).\nELECTRA NER DEP CON SRL MTL-5 STL\n10\n0\n10\n20\n30 NN\nDT\nNNP\nJJ\n.\nNNS\nPRP\nRB\nVB\nVBD\nCC\nVBZ\nVBP\nVBN\nCD\nVBG\nMD\nPRP$\nUH\nHYPH\nPOS\n(a) POS\nELECTRA POS DEP CON SRL MTL-5 STL\n20\n15\n10\n5\n0\n5\nGPE\nPER\nORG\nDATE\nCARD\nNORP\n%\n$\nTIME\nORD\nLOC\nWOA\nFAC\nQUA\nPRDT\nEVNT\nLAW\nLANG (b) NER\nELECTRA POS NER CON SRL MTL-5\n40\n30\n20\n10\n0\n10\n20\nprep\npobj\nnsub\ndet\nroot\nnn\namod\ndobj\nadvm\naux\ncc\nconj\ndep\nposs\nccom\ncop\nmark\nxcom\nnum\nrcmo (c) DEP\nELECTRA POS NER DEP SRL MTL-5\n20\n15\n10\n5\n0\n5\n10\n15\nNP\nADVP\nADJP\nVP\nNML\nWHNP\nINTJ\nQP\nWHADVP\nPRT\nPP\nCONJP\nX\nWHADJP\nMETA\nUCP\nS\nLST\nFRAG\nSBAR\n(d) CON\nELECTRA POS NER DEP CON MTL-5\n30\n20\n10\n0\n10\n20\n30\nARG1\nARG0\nARG2\nM-TMP\nM-DIS\nM-ADV\nM-MOD\nM-LOC\nM-MNR\nM-NEG\nR-A1\nR-A0\nC-A1\nM-PRP\nM-DIR\nARG3\nARG4\nM-CAU\nM-PRD\nM-ADJ (e) SRL\n1 2 3 4 5 6 7 8 9 10 11 12\nLayers\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\npunct\nprep\npobj\nnsubj\ndet\nroot\nnn\namod\ndobj\nadvmod\naux\ncc\nconj\ndep\nposs\nccomp\ncop\nmarkxcomp\nnum\nrcmod\nadvcl\nneg\nauxpass\nnsubjpass\npossessive\npcomp\ndiscourse\npartmod\nappos\nprt\nnumber\nquantmod\nparataxis\ninfmod\ntmod\nexpl\nmwe\nnpadvmod\niobj\npredet\nacomp\ncsubj\npreconj\ncsubjpass (f) DEP layer analysis.\nFigure 12: The ELECTRA (Clark et al., 2020) probing results comparison (a - e) and layer analysis of pre-trained\nheads (g).\n5577\n(a) POS\n (b) NER\n (c) DEP\n (d) CON\n (e) SRL\n1 2 3 4 5 6 7 8 9 10 11 12\nHeads\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12 Layers\n(f) 3-run utilization of the MTL-DP model, where each run is\nencoded in a RGB channel. Darker indicates higher utilization.\n1 2 3 4 5 6 7 8 9 10 11 12\nHeads\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12 Layers\n(g) Average head utilization rates among the 5 tasks in 3 runs.\nDarker cells indicate higher utilization rates.\nFigure 13: Head utilization of the DeBERTa (He et al., 2020) STL-DP models (a - e, g) and the MTL-DP model\n(f).\nDeBERTa NER DEP CON SRL MTL-5 STL\n30\n20\n10\n0\n10\n20 IN\nNNP\nJJ\nPRP\nVB\nVBP\nCD\nVBG\nUH\nHYPH\nPOS\n'\n``\nWDT\n:\nRBR\n-RRB-\n$\nRBS\nNFP\nSYM\n(a) POS\nDeBERTa POS DEP CON SRL MTL-5 STL\n10\n5\n0\n5\n10\n15\nGPE\nPER\nORG\nDATE\nCARD\nNORP\n%\n$\nTIME\nORD\nLOC\nWOA\nFAC\nQUA\nPRDT\nEVNT\nLAW\nLANG (b) NER\nDeBERTa POS NER CON SRL MTL-5\n40\n30\n20\n10\n0\n10 prep\npobj\nnsub\ndet\nroot\nnn\namod\ndobj\nadvm\naux\ncc\nconj\ndep\nposs\nccom\ncop\nmark\nxcom\nnum\nrcmo (c) DEP\nDeBERTa POS NER DEP SRL MTL-5\n20\n15\n10\n5\n0\n5\n10 NP\nADVP\nADJP\nVP\nNML\nWHNP\nINTJ\nQP\nWHADVP\nPRT\nPP\nCONJP\nX\nWHADJP\nMETA\nUCP\nS\nLST\nFRAG\nSBAR\n(d) CON\nDeBERTa POS NER DEP CON MTL-5\n30\n20\n10\n0\n10\n20\nARG1\nARG0\nARG2\nM-TMP\nM-DIS\nM-ADV\nM-MOD\nM-LOC\nM-MNR\nM-NEG\nR-A1\nR-A0\nC-A1\nM-PRP\nM-DIR\nARG3\nARG4\nM-CAU\nM-PRD\nM-ADJ (e) SRL\n1 2 3 4 5 6 7 8 9 10 11 12\nLayers\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy\npunct\nprep\npobj\nnsubj\ndet\nroot\nnn\namod\ndobj\nadvmod\naux\ncc\nconj\ndep\nposs\nccomp\ncop\nmarkxcomp\nnum\nrcmod\nadvcl\nneg\nauxpass\nnsubjpass\npossessive\npcomp\ndiscourse\npartmod\nappos\nprt\nnumber\nquantmod\nparataxis\ninfmod tmod\nexpl\nmwe\nnpadvmod\niobj predet\nacomp\ncsubj\npreconj csubjpass (f) DEP layer analysis.\nFigure 14: The DeBERTa (He et al., 2020) probing results comparison (a - e) and layer analysis of pre-trained\nheads (g)."
}