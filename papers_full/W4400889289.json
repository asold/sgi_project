{
  "title": "Large Language Models Take on Cardiothoracic Surgery: A Comparative Analysis of the Performance of Four Models on American Board of Thoracic Surgery Exam Questions in 2023",
  "url": "https://openalex.org/W4400889289",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2476751903",
      "name": "Zain Khalpey",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166440711",
      "name": "Ujjawal Kumar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125302643",
      "name": "Nicholas King",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2924022766",
      "name": "Alyssa Abraham",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3207226917",
      "name": "Amina H. Khalpey",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4389244295",
    "https://openalex.org/W4372047097",
    "https://openalex.org/W4377009978",
    "https://openalex.org/W4368367885",
    "https://openalex.org/W4391580609",
    "https://openalex.org/W2908201961",
    "https://openalex.org/W2937037822",
    "https://openalex.org/W2604788695",
    "https://openalex.org/W2789970635",
    "https://openalex.org/W2558050786",
    "https://openalex.org/W4391823231",
    "https://openalex.org/W4387950352",
    "https://openalex.org/W4388085114",
    "https://openalex.org/W4385724150",
    "https://openalex.org/W4367675464"
  ],
  "abstract": "Objectives Large language models (LLMs), for example, ChatGPT, have performed exceptionally well in various fields. Of note, their success in answering postgraduate medical examination questions has been previously reported, indicating their possible utility in surgical education and training. This study evaluated the performance of four different LLMs on the American Board of Thoracic Surgery's (ABTS) Self-Education and Self-Assessment in Thoracic Surgery (SESATS) XIII question bank to investigate the potential applications of these LLMs in the education and training of future surgeons. Methods The dataset in this study comprised 400 best-of-four questions from the SESATS XIII exam. This included 220 adult cardiac surgery questions, 140 general thoracic surgery questions, 20 congenital cardiac surgery questions, and 20 cardiothoracic critical care questions. The GPT-3.5 (OpenAI, San Francisco, CA) and GPT-4 (OpenAI) models were evaluated, as well as Med-PaLM 2 (Google Inc., Mountain View, CA) and Claude 2 (Anthropic Inc., San Francisco, CA), and their respective performances were compared. The subspecialties included were adult cardiac, general thoracic, congenital cardiac, and critical care. Questions requiring visual information, such as clinical images or radiology, were excluded. Results GPT-4 demonstrated a significant improvement over GPT-3.5 overall (87.0% vs. 51.8% of questions answered correctly, p < 0.0001). GPT-4 also exhibited consistently improved performance across all subspecialties, with accuracy rates ranging from 70.0% to 90.0%, compared to 35.0% to 60.0% for GPT-3.5. When using the GPT-4 model, ChatGPT performed significantly better on the adult cardiac and general thoracic subspecialties (p < 0.0001). Conclusions Large language models, such as ChatGPT with the GPT-4 model, demonstrate impressive skill in understanding complex cardiothoracic surgical clinical information, achieving an overall accuracy rate of nearly 90.0% on the SESATS question bank. Our study shows significant improvement between successive GPT iterations. As LLM technology continues to evolve, its potential use in surgical education, training, and continuous medical education is anticipated to enhance patient outcomes and safety in the future.",
  "full_text": null,
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.7807506918907166
    },
    {
      "name": "Cardiothoracic surgery",
      "score": 0.7361429929733276
    },
    {
      "name": "Cardiac surgery",
      "score": 0.6608628630638123
    },
    {
      "name": "Surgery",
      "score": 0.39533179998397827
    },
    {
      "name": "General surgery",
      "score": 0.3694208264350891
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210090013",
      "name": "Cambridge School",
      "country": "PT"
    },
    {
      "id": "https://openalex.org/I4210130412",
      "name": "HonorHealth",
      "country": "US"
    }
  ]
}