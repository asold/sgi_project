{
  "title": "Fuse It More Deeply! A Variational Transformer with Layer-Wise Latent Variable Inference for Text Generation",
  "url": "https://openalex.org/W4287888031",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2119320860",
      "name": "Jinyi Hu",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Center for Information Technology",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2636556068",
      "name": "Xiaoyuan Yi",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2101949055",
      "name": "Wenhao Li",
      "affiliations": [
        "Center for Information Technology",
        "Tsinghua University",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2157167650",
      "name": "Maosong Sun",
      "affiliations": [
        "Tsinghua University",
        "Center for Information Technology",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2105409468",
      "name": "Xing Xie",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2753738274",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W2964222296",
    "https://openalex.org/W3034557228",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2587284713",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2808437126",
    "https://openalex.org/W3035100081",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2963997607",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2970134931",
    "https://openalex.org/W4288356932",
    "https://openalex.org/W3098708719",
    "https://openalex.org/W2963600562",
    "https://openalex.org/W3119469378",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963987720",
    "https://openalex.org/W4293469690",
    "https://openalex.org/W2997297439",
    "https://openalex.org/W2962717182",
    "https://openalex.org/W3120243996",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W1909320841",
    "https://openalex.org/W2160204597",
    "https://openalex.org/W2964346351",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4287332927",
    "https://openalex.org/W2755124548",
    "https://openalex.org/W3013310839",
    "https://openalex.org/W3041956526",
    "https://openalex.org/W2889924956",
    "https://openalex.org/W2964669873",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2962917899",
    "https://openalex.org/W2924334974",
    "https://openalex.org/W2998453866",
    "https://openalex.org/W2586756136",
    "https://openalex.org/W2753215597",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W4295177495",
    "https://openalex.org/W3022187094",
    "https://openalex.org/W2910135751",
    "https://openalex.org/W1779483307",
    "https://openalex.org/W4298343152",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2785896739"
  ],
  "abstract": "Jinyi Hu, Xiaoyuan Yi, Wenhao Li, Maosong Sun, Xing Xie. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 697 - 716\nJuly 10-15, 2022 ¬©2022 Association for Computational Linguistics\nFuse It More Deeply! A Variational Transformer with Layer-Wise\nLatent Variable Inference for Text Generation\nJinyi Hu1,2,3, Xiaoyuan Yi5, Wenhao Li1,2,3, Maosong Sun1,2,3,4‚àó, Xing Xie5\n1 Department of Computer Science and Technology, Tsinghua University, Beijing, China\n2 Beijing National Research Center for Information Science and Technology\n3 Institute for ArtiÔ¨Åcial Intelligence, Tsinghua University, Beijing, China\n4 International Innovation Center of Tsinghua University, Shanghai, China\n5 Microsoft Research Asia\nhu-jy21@mails.tsinghua.edu.cn, xiaoyuanyi@microsoft.com\nAbstract\nThe past several years have witnessed Vari-\national Auto-Encoder‚Äôs superiority in various\ntext generation tasks. However, due to the se-\nquential nature of the text, auto-regressive de-\ncoders tend to ignore latent variables and then\nreduce to simple language models, known as\nthe KL vanishing problem, which would fur-\nther deteriorate when V AE is combined with\nTransformer-based structures. To ameliorate\nthis problem, we propose D ELLA , a novel\nvariational Transformer framework. D ELLA\nlearns a series of layer-wise latent variables\nwith each inferred from those of lower layers\nand tightly coupled with the hidden states by\nlow-rank tensor product. In this way, D ELLA\nforces these posterior latent variables to be\nfused deeply with the whole computation path\nand hence incorporate more information. We\ntheoretically demonstrate that our method can\nbe regarded as entangling latent variables to\navoid posterior information decrease through\nlayers, enabling D ELLA to get higher non-\nzero KL values even without any annealing or\nthresholding tricks. Experiments on four un-\nconditional and three conditional generation\ntasks show that D ELLA could better alleviate\nKL vanishing and improve both quality and di-\nversity compared to several strong baselines.\n1 Introduction\nVariational Autoencoder (V AE) (Kingma and\nWelling, 2014; Rezende et al., 2014) has proven to\nbe successful in generating various kinds of text,\nsuch as stylistic text (Hu et al., 2017; John et al.,\n2019), dialogue (Zhao et al., 2017), story (Yu et al.,\n2020) and poetry (Yi et al., 2020). The sequen-\ntial nature of the text leads to typically used auto-\nregressive decoders in V AE for language genera-\ntion. However, such strong decoders tend to evade\nthe difÔ¨Åculty of learning meaningful latent codes\nby heavily relying on previously generated words\n‚àóCorresponding author. Email: sms@tsinghua.edu.cn\nEmbedding Layer\nTransformer Decoder Layer 1\nTransformer Decoder Layer 2\n‚ãÆ ‚ãÆ\nTransformer Decoder Layer L\nSoftmax Layer\n(c)\nx5x1 x2 x3 x4\nùëß\n(b)\nùëß\n(a)\nùëß\nFigure 1: Existing paradigms of Transformer V AE.\nand hence ignore latent variables (Bowman et al.,\n2016), known as KL vanishing or posterior col-\nlapse. This problem causes two drawbacks: (a) the\nposterior distribution quickly turns into the prior\none (usually standard Gaussian), falling to build\nexpressive latent representations; (b) the decoder\nreduces to a naive language model, resulting in\nmonotonous generated text (Fu et al., 2019).\nTo ameliorate this problem, researchers have de-\nsigned various techniques. Among them, three\nbroadly used methods include weakening de-\ncoders (Bowman et al., 2016; Semeniuta et al.,\n2017; Zhao et al., 2017), KL annealing (Bow-\nman et al., 2016; Fu et al., 2019) and KL thresh-\nold (Kingma et al., 2016; Higgins et al., 2017; Li\net al., 2019). Nonetheless, the weakening of de-\ncoders restrains models‚Äô language modelling capa-\nbility; annealing hyperparameters are hard to tune;\nKL threshold introduces a non-smooth objective\nwith some optimization difÔ¨Åculties.\nIn the era of RNN, V AE can be easily incor-\nporated by using the latent variable as the initial\ndecoder state, while how to combine V AE with re-\ncently prevalent Transformer (Vaswani et al., 2017)\narchitectures, which have made a breakthrough in\ntext generation, still remains an open challenge.\n697\nAs shown in Fig.1, existing methods of inte-\ngrating Transformer into V AE fall into three main\nparadigms: (a) directly adding latent variables to\ninput token embeddings (abbr. Embedding) (Li\net al., 2020a); (b) using latent variables as a sepa-\nrate memory token vector to be attended by self-\nattention in each layer (abbr. Memory) (Fang\net al., 2021); (c) combining latent variables with\nthe last-layer decoder states before output softmax\n(abbr. Softmax) (Wang and Wan, 2019). However,\nparadigm (a) brings noise for self-attention. In\nparadigm (b), memory vectors tend to be ignored\nby attention, even exacerbating KL vanishing. In\nparadigm (c), latent variables couldn‚Äôt deeply in-\nterfere with the whole computation path. Sec.3.3\npresents more detailed analyses.\nTo better incorporate Transformer into V AE and\ntheoretically ameliorate the KL vanishing prob-\nlem, we propose DELLA 1, a novel variational trans-\nformer framework. DELLA learns a series of layer-\nwise latent variables in a Transformer encoder, and\neach is inferred from those of lower layers and then\ntightly coupled with the hidden states in the corre-\nsponding decoder layer by low-rank tensor product.\nOur method theoretically stimulates the entangle-\nment of latent variables and hence allows propa-\ngation of undiminished latent information through\nlayers. As a result, DELLA forces posterior latent\nvariables to be deeply fused with the entire compu-\ntation path and encode richer information of input\ntext, achieving higher KL values even without any\nannealing or threshold training tricks.\nIn summary, our contributions are as follow:\n(i) We are the Ô¨Årst to propose layer-wise in-\nferred latent variables in Transformer-based ar-\nchitecture to mitigate KL vanishing; We ( ii) in-\nnovatively inject latent variables using low-rank\ntensor product, ( iii) provide a theoretical valid-\nity of our method and ( iv) demonstrate its effec-\ntiveness on four unconditional and three condi-\ntional generation tasks. Our codes are available\nat https://github.com/OpenVLG/DELLA.git.\n2 Related Work\nThanks to the representation capacity of latent\nspace, V AE has been widely adopted for both im-\nage generation (van den Oord et al., 2017; Vahdat\nand Kautz, 2020) and text generation (Bowman\net al., 2016; Hu et al., 2017). In the early stage,\nV AE was combined with RNN decoders for gener-\n1 DELLA : DEeply Fused Layer-wise LAtent Variables\nating a broad range of text, varying from dialogue\n(Serban et al., 2016), image caption (Wang et al.,\n2017), text summarization (Gupta et al., 2017) to\nstory (Yu et al., 2020) and poetry (Yi et al., 2020).\nIn this case, latent variables are usually utilized as\neither the initial decoder state (Li et al., 2018) or\ninput at each time step (Gupta et al., 2017).\nIn spite of extensive applications, V AE suffered\nfrom KL vanishing in the scenario of text genera-\ntion (Bowman et al., 2016). Several lines of tech-\nniques have been proposed to alleviate this prob-\nlem. The Ô¨Årst line is to avoid a too fast decrease of\nthe KL divergence by re-weighting. KL annealing\n(Bowman et al., 2016) linearly increased the weight\nof KL term from 0 to 1 during the warm-up period.\nFu et al. (2019) further proposed cyclical anneal-\ning, which repeats the warm-up process multiple\ntimes. The second line guarantees a positive lower\nbound of the KL term. KL thresholding (Kingma\net al., 2016) achieved a Ô¨Åxed minimum by combin-\ning a hinge loss, while BN-V AE (Zhu et al., 2020)\nlearned more Ô¨Çexible ones via batch normalization.\nŒ¥-V AE (Razavi et al., 2019) chose to restrain the\nfamily of posterior distributions. The third line\naims to constraint decoders to force a more infor-\nmative latent variable. Wang et al. (2017) intro-\nduced an auxiliary BOW (bag-of-words) loss. He\net al. (2019) added additional training loops for the\nencoder. Yang et al. (2017) adopted dilated CNN as\ndecoder, and Dieng et al. (2019) added skip connec-\ntions to the decoder. Although the above methods\nmitigate KL vanishing to some extent, it is still\nchallenging for either tuning or optimization.\nIn these years, the powerful Transformer has\nbeen integrated with V AE to beneÔ¨Åt diverse tasks,\nincluding text classiÔ¨Åcation (Gururangan et al.,\n2019), story generation (Wang and Wan, 2019;\nFang et al., 2021) and dialogue generation (Lin\net al., 2020). Optimus (Li et al., 2020a) further\nbridged the pre-trained BERT (Devlin et al., 2019)\nand GPT-2 (Radford et al., 2019) with V AE for\npre-training. Most existing works inject latent vari-\nables into the Transformer decoder by the three\nparadigms, Embedding (Li et al., 2020a), Mem-\nory (Li et al., 2020a; Fang et al., 2021) and Soft-\nmax(Wang and Wan, 2019), as discussed in Sec. 1,\nwhile these methods shallowly fuse the latent vari-\nables with hidden states. To achieve deeper fusion\nand ameliorate KL vanishing, we propose DELLA .\nThe most relevant architecture to our model is\nhierarchical V AE (S√∏nderby et al., 2016; Klushyn\n698\net al., 2019; Vahdat and Kautz, 2020; Child, 2020),\nwhich is mainly designed for image generation and\nnot suitable for text. For text generation, hierarchi-\ncal latent variables are either independent of each\nother (Serban et al., 2016), or corresponding to dif-\nferent text granularities (sentence or word level),\nwhile our DELLA learns conditionally inferred and\nlayer-wise latent variables based on Transformer.\n3 Preliminaries\n3.1 Transformer\nTransformer (Vaswani et al., 2017) represents\nan input sequence x = {x1,...,x i,...,x n}as\ncontextualized distributed hidden states h =\n{h1,...,h i,...,h n}by a series of stacked layers,\nand states in the l-th layer, h(l), are calculated with\nscaled dot-product attention:\nAttention(Q,K,V ) = softmax\n(QT K‚àö\nd\n)\nVT ,\n(1)\nwhere Q,K,V stand for Query, Key, Value, re-\nspectively, which are projected from outputs of the\nprevious layer: Q = Wqh(l‚àí1), K = Wkh(l‚àí1),\nV = Wvh(l‚àí1). d is the dimension of hidden\nstates. In practice, multiple groups of states are\ncalculated with different attention parameters and\nthen concatenated, known as multi-head attention.\n3.2 V AE\nAs a kind of generative model, V AE estimates the\nintractable data distribution p(x) by deriving and\nmaximizing its lower bound as:\nlog p(x) ‚â•LELBO (x; Œ∏,œÜ) =\nEqœÜ(z|x)[log pŒ∏(x|z)]‚àíKL(qœÜ(z|x)||p(z)), (2)\nwhere zis the latent variable and p(z) is the prior\ndistribution of latent variable which is commonly\nassumed as standard Gaussian; the posterior dis-\ntribution p(z|x) is approximated by an inference\nnetwork (encoder) qœÜ(z|x); pŒ∏(x|z) is a generator\n(decoder) to generate textxfrom the latent variable\nz; Œ∏and œÜare corresponding parameters.\nThe whole lower bound in Eq.(2), called Ev-\nidence Lower BOund (ELBO), consists of two\nterms: the reconstruction loss,\nLE = ‚àíEqœÜ(z|x) [log pŒ∏(x|z)] , (3)\nwhich helps reconstruct the input given the poste-\nrior latent variable z, and the KL divergence,\nLR = KL (qœÜ(z|x)‚à•p(z)) . (4)\nIn practice, V AE is considered as a regularized\nAuto-encoder, and a hyper-parameter Œ≤ is intro-\nduced to control the strength of KL, Œ≤LR, usually\nused in KL annealing methods (Fu et al., 2019).\n3.3 Incorporate Transformer into V AE\nFor Transformer encoder, the posteriorzis mapped\nfrom the text representation, which can be the pool-\ning of all hidden states in the last layer (Fang et al.,\n2021), or state of a special token (Li et al., 2020a),\ne.g., [CLS]. Then zis injected into Transformer\ndecoder by the paradigms discussed in Sec. 1.\nNow we take a further step and investigate why\nintrinsically these three paradigms, namely Embed-\nding, Memory and Softmax, would perform poorly.\nEmbedding: DeÔ¨Åne ei,ej as two token em-\nbeddings and Œ±i,j as the attention weight of i-th\nand j-th tokens. From Eq.(1), we have Œ±i,j =\n(Wqei)T (Wkej) = eT\ni (Wq)T Wkej, which is\nfurther abbreviated as ‚ü®ei,ej‚ü©. Such Embedding\nparadigm directly adds zto token embeddings as:\nŒ±‚Ä≤\ni,j =\n[\nWq(ei + z)\n]T [\nWk(ej + z)\n]\n= ‚ü®ei,ej‚ü©+ ‚ü®ei,z‚ü©+ ‚ü®z,ej‚ü©+ ‚ü®z,z‚ü©,\n(5)\nwhere we can Ô¨Ånd that a redundant term, ‚ü®z,z‚ü©, is\nintroduced, bringing extra noise for attention mech-\nanism. Moreover, information in zcould diminish\nwith propagation through layers (Fig. 2), aggravat-\ning KL vanishing.\nMemory: This paradigm treats z as an addi-\ntional memory token and places it at the beginning\nof xto be attended by other tokens via attention.\nNevertheless, as mentioned in Sec. 1, the powerful\nTransformer decoder may only rely on preceding\ndecoded tokens. Consequently, with no explicit\nconstraints (e.g., auxiliary loss), such a memory\ntoken is more likely to be ignored by self-attention\n(Fig. 6 & 7), even exacerbating KL vanishing.\nSoftmax: This paradigm Ô¨Årst adds zto the last-\nlayer hidden states h, and then projectsz+hinto a\nlogit vector p‚ààRv over the vocabulary, wherevis\nvocab size. In this method, latent variables do not\ninteract with hidden states until the last layer, which\nerodes the effect of latent variables (see Fig. 2).\n4 Methodology\nAs demonstrated in Sec. 3, existing three paradigms\nmake latent variables gradually diminish through\nlayers, be ignored by self-attention or inadequately\ninteract with hidden states, which would not miti-\ngate but even worsen the KL vanishing problem.\n699\nTo deeply fuse latent variables with the whole\ncomputation path of Transformer, we propose\nDELLA to learn a series of layer-wise posterior\nlatent variables which are conditionally inferred in\nencoder, and injected into hidden states in decoder\nby low-rank tensor product. We present layer-wise\nlatent variables in Sec. 4.1, describe the tensor\nproduct fusion in Sec. 4.2, give the theoretical veri-\nÔ¨Åcation of DELLA ‚Äôs effectiveness for ameliorating\nKL vanishing in Sec. 4.3, and then extend DELLA\nto Conditional V AE (CV AE) in Sec. 4.4.\n4.1 Layer-wise Latent Variables\nDifferent from previous work where only one la-\ntent variable zis calculated and shared by (Li et al.,\n2020a) or projected to (Fang et al., 2021) decoder\nlayers, we involve a series of latent variablesz=\n{z1,z2,..., zL}, where Lis the number of Trans-\nformer layers. Then we reformulate the prior and\nposterior distributions as p(z) =‚àèL\nl=1 p(zl|z<l),\nq(z|x) = ‚àèL\nl=1 q(zl|z<l,x), respectively, with\neach zl still following Gaussian distribution. Then\nwe rewrite LR in Eq.(4) similar to Vahdat and\nKautz (2020):\nLR = KL(q(z|x)||p(z))\n=\nL‚àë\nl=1\nEq(z<l|x) [KL(q(zl|x,z<l)||p(zl|z<l))] .\n(6)\nWhen l= 1, p(z1|z<1) =p(z1) is the standard\nGaussian distribution, q(z1|x,z<1) = q(z1|x).\nWe give detailed derivations in Appendix B.1.\nThese latent variableszl are calculated (inferred)\nlayer by layer using representations of the corre-\nsponding layer. Concretely, inl-th layer, we use the\nhidden state of the Ô¨Årst token in text x, as its l-th-\nlayer representation, denoted as x(l) ‚ààRd, where\ndis hidden size. Then we represent latent variables\nin lower layers as z<l and obtain it by:\nz<l = tanh(W(l)\nhhz<l‚àí1 + W(l)\nih zl‚àí1), (7)\nwhere Whh,Wih ‚ààRp√óp, so z<l ‚ààRp and pis the\ndimension of latent variable. z0 and z<0 are set as\nzero vector. We calculate the mean and variance\nvectors of p(zl|z<l) and q(zl|z<l,x) by:\n( ¬µp\nlog(œÉ2\np)\n)\n= W(l)\np z<l,\n( ¬µq\nlog(œÉ2\nq)\n)\n= W(l)\nq\n(z<l\nx(l)\n)\n,\n(8)\nwhere Wp ‚ààRp√ó2p, Wp ‚ààRp√ó2p.\nThe latent variable zl is sampled from the pos-\nterior distribution q(zl|z<l,x) = N(¬µq,œÉ2\nqI)\nfor training, and from the prior one q(zl|z<l) =\nN(¬µp,œÉ2\npI) for testing. Since hidden states in\neach layer belong to different vector spaces, the\nparameters to calculate each z<l, e.g., W(l)\np and\nW(l)\nq , do not share throughout different layers.\n4.2 Low-rank Tensor Product\nWe inject the latent variable zl, which is obtained\nbased on l-th encoder layer, into the correspond-\ning l-th decoder layer. Instead of simply using zl\nas a memory token as discussed in Sec. 3.3, we\nresort to low-rank tensor product, which has been\nsuccessfully utilized for fusing multimodal repre-\nsentations (Liu et al., 2018), to deeply fuse latent\nvariables with hidden states in the decoder.\nIn detail, we conduct low-rank tensor product on\nzl and xi‚Äôsl-th-layer value vector v(l)\ni as:\nÀúv(l)\ni = (\nr‚àë\nj=1\nW(l,j)\nv v(l)\ni ) ‚ó¶(\nr‚àë\nj=1\nW(l,j)\nz zl), (9)\nwhere r is a hyper-parameter, ‚ó¶means element-\nwise multiplication, Wv ‚ààRd√ód.Wz ‚ààRp√ód are\nlearnable parameters which are shared across all\npositions (i) but not shared with layers ( l), con-\nsidering distinct vector spaces in different layers,\nas mentioned in Sec. 4.1. Then the fused Value\nÀúV(l) = {Àúv(l)\n1 ,..., Àúv(l)\nn }is used in Eq.(1)\nIn this way, layer-wise zl is conditionally in-\nferred from latent variables in previous encoder\nlayers, together with l-th-layer text representation,\nand then explicitly fused with the corresponding de-\ncoder layer, yielding a deeper intervention through-\nout the whole computation path of Transformer.\n4.3 Why Could DELLA Work Well?\nTo theoretically interpret the advantage of layer-\nwise latent variables which contributes most to\nDELLA (Table 4), we give the following theorem:\nTheorem 1 For an observation x and a se-\nquence of latent variables z1,z2,... zL, satis-\nfying p(z) = ‚àèL\nl=1 p(zl|z<l), and q(z|x) =‚àèL\nl=1 q(zl|z<l,x), then the expectation of the KL\nterm, Ep(x)[LR] is an upper bound of:\n‚àí\nL‚àí1‚àë\ni=2\nI(zL; ... ; zi|zi‚àí1) ‚àíI(zL; ... ; z1|x),\n(10)\n700\nwhere I is the interaction information2.\nSee Appendix B.2 for proof. Based on Theorem 1,\nminimizing LR approximatively means maximiz-\ning each interaction information term in Eq.(10),\nwhich forces the entanglement of all latent varibles\nz1; ... ; zL given the observation x, alleviating the\ndiminishing of information encoded in latent vari-\nables when propagating through layers.\n4.4 Extension to CV AE\nDELLA could also be applied to CV AE for condi-\ntional generation tasks like storytelling. Given an\nobservation xand its condition c, we can optimize:\nlog p(x|c) ‚â•EqœÜ(z|x,c)[log pŒ∏(x|z,c)] (11)\n‚àíKL(qœÜ(z|x,c)||p(z|c)),\nand then replace the prior distribution q(zl|x,z<l)\nand posterior distribution p(zl|z<l) in Eq.(6) with\nq(zl|x,c,z<l) and p(zl|z<l,c), respectively.\nIn this case, we encode the condition c with\nthe same encoder. Similarly, we can obtain the\nrepresentation of cat l-th layer, denoted as c(l) ‚àà\nRd, and then calculate the mean and log variance\nof p(zl|z<l,c) and q(zl|z<l,x,c) by:\n( ¬µp\nlog(œÉ2\np)\n)\n= ÀÜW\n(l)\np\n(z<l\nc(l)\n)\n,\n( ¬µq\nlog(œÉ2\nq)\n)\n= ÀÜW\n(l)\nq\nÔ£´\nÔ£≠\nz<l\nx(l)\nc(l)\nÔ£∂\nÔ£∏,\n(12)\nwhere ÀÜW\n(l)\np ‚ààR(p+d)√ó2p, ÀÜW\n(l)\nq R(p+2d)√ó2p.\n5 Experiment\n5.1 Dataset\nWe consider four datasets for language modelling\nand unconditional generation, including the Yelp,\nand Yahoo (Yang et al., 2017; He et al., 2019), Penn\nTreebank (PTB) (Marcus et al., 1993), and SNLI\n(Bowman et al., 2015), and three datasets for con-\nditional generation tasks, including summarization\ngeneration with CNN/DailyMail (CNN/DM) (See\net al., 2017), story generation with WritingPrompts\n(WP) (Fan et al., 2018) and paraphrase generation\nwith Quora 3. Detailed data statistics are listed in\nTable 7. Due to the limited computation capability,\nwe use 165,157 samples in CNN/DM and 22,2614\nin WP with the max length of 900 for training.\n2https://en.wikipedia.org/wiki/Interaction_information\n3https://quoradata.quora.com/First-Quora-Dataset-\nRelease-Question-Pairs\n5.2 Implementation Details\nWe use pretrained language models as the backbone\nand Ô¨Åne-tune them on each task mentioned above\nwith our DELLA as in (Li et al., 2020a). For uncon-\nditional generation and story generation, encoder\nand decoder shared the same parameters initialized\nwith 12-layer GPT-2 (Radford et al., 2019). For\nsummarization and paraphrase generation, parame-\nters are not sharedand initialized with BART-base\n(Lewis et al., 2020). We set the dimension of latent\nvariable as 32 for all V AE-based models and use\ncyclical annealing for training, following (Li et al.,\n2020a). More details are given in Appendix A.1.\n5.3 Baseline\nWe make a comprehensive comparison with strong\nTransformer-based baselines. We do not consider\nRNN-based models that are inferior to Transformer\nfor text generation as shown in (Li et al., 2020a).\nFinetuned Pretrained Models. To manifest the\nsuitability of DELLA for different pretrained lan-\nguage models, we compare it with Ô¨Åne-tuned GPT2\non unconditional generation and story generation,\nand with Ô¨Åne-tuned BART-base on summarization\ngeneration and paraphrase generation.\nOptimus (Li et al., 2020a): a large-scale V AE\nmodel which takes a pre-trained BERT as encoder\nand pretrained GPT-2 as decoder. This model is\nÔ¨Årst pretrained as a V AE, which simultaneously uti-\nlizes the two paradigms, Embedding and Memory\nas introduced in Sec. 3.3, for injecting latent vari-\nables, with both KL annealing and KL threshold\ntricks, and then Ô¨Åne-tuned on downstream tasks.\nTransformer-based V AE. Besides Optimus, we\nalso compre the three paradigms, namely Embed-\nding (Li et al., 2020a), Memory (Fang et al., 2021)\nand Softmax (Wang and Wan, 2019), and incorpo-\nrate each paradigm into the same pre-trained model\nas DELLA on each dataset for fair comparison.\n5.4 Metrics\nFor unconditional generation tasks, we consider\nthree types of metrics. (a) Representation Learn-\ning Capability: we report PPL, ELBO, KL, mu-\ntual information (MI) (Alemi et al., 2016) and acti-\nvate units (AU) (Burda et al., 2016). These metrics\nmeasure V AE‚Äôs ability to mitigate KL vanishing\nand learn meaningful representations. Different\nfrom traditional language models like GPT-2, V AE-\nbased models could not produce exact PPL due to\nrandomness, so we use importance-weighted sam-\n701\nModel Representation Learning Generation Quality Generation Diversity\nPPL‚Üì ELBO‚Üì KL‚Üë MI‚Üë AU‚Üë BLEU‚Üë CND‚Üì MAUVE‚Üë SB‚Üì Dist‚Üë JS‚Üì\nDataset: Yelp\nGPT-2 22.13 - - - - 56.92 0.68 0.12 65.90 17.96 0.51\nOptimus 22.79 344.10 15.09 7.67 - - - - - - -\nEmbed 19.98 327.28 4.77 4.14 6 56.34 0.31 0.42 65.27 15.59 0.44\nMemory 19.95 326.60 5.70 5.30 11 57.37 0.27 0.46 63.90 16.91 0.39\nSoftmax 20.14 328.13 7.50 6.29 13 56.83 0.30 0.45 64.26 16.51 0.40\nDELLA 12.35 239.83 29.47 10.78 23 57.15 0.13 0.55 60.02 17.63 0.43\nDataset: Yahoo\nGPT-2 24.17 - - - - 44.25 0.55 0.15 54.06 21.07 0.28\nOptimus 23.11 293.34 17.45 8.85 - - - - - - -\nEmbed 22.18 286.85 3.63 3.03 3 42.27 0.45 0.31 54.15 20.80 0.32\nMemory 22.03 285.47 4.87 4.62 18 45.20 0.46 0.37 54.59 21.87 0.33\nSoftmax 22.35 287.44 6.35 5.52 19 44.28 0.44 0.34 54.49 21.65 0.32\nDELLA 11.49 201.34 27.84 12.31 21 44.67 0.19 0.38 48.53 21.88 0.31\nTable 1: Evaluation results for language modelling and unconditional generation. Results of Optimus are directly\ncopied from the original paper with Œª= 0.5. SB means Self-BLEU.\nples to estimate PPL, following He et al. (2019).\nWe set the threshold in AU to 0.2 to further distin-\nguish different models. (b) Generation Quality:\nwe report BLEU (Papineni et al., 2002), CND (Li\net al., 2020b) and MAUVE (Pillutla et al., 2021).\nCND and MAUVE measure the divergence be-\ntween human-authored text and the generated one.\n(c) Generation Diversity: we report Self-BLEU\n(Zhu et al., 2018), Dist (Li et al., 2016) and JS (Jac-\ncard similarity) (Wang and Wan, 2018) to assess\nthe diversity and novelty of generated text.\nFor conditional generation tasks , we report\nBLEU, Rouge-1, Rouge-2, Rouge-L (Lin and\nHovy, 2002), and BERTScore (Zhang et al., 2020)\nto evaluate the quality of generated texts, as well\nas the same diversity metrics used in unconditional\ngeneration. We also report KL and AU value to\npresent representation learning capability. More\ndetails of metrics are provided in Appendix A.3.\n5.5 Results\n5.5.1 Unconditional Generation\nWe present results on Yelp and Yahoo in Table 1\nand leave the those on PTB and SNLI in the Ap-\npendix A.5 due to space limitations. We also show\nthe learning curves of ELBO and KL in Fig. 5.\nAs shown in Table 1, DELLA achieves notably\nimprovement on almost all the metrics, especially\nsuperior on representation learning metrics. Much\nhigher KL, MI and AU, and a big gap in PPL ob-\ntained by DELLA indicate the latent variables en-\ncode more meaningful text information and won‚Äôt\ndiminish when propagating through Transformer\nlayers, which strongly supports our motivation that\nfusing latent variables with hidden states more\ndeeply could effectively alleviate the KL vanishing\nproblem. Such results also empirically verify the\ntheoretical advantage of our model (Theorem 1),\ndemonstrating entangled layer-wise latent variables\ncan preserve more encoded knowledge for decoder.\nWe will show that zcan involve more information\nwhen injected into more layers in Sec. 5.8.\nBesides, DELLA also gets good performance\n(comparable BLEU and much better CND and\nMAUVE) on generation quality. With more in-\nformative latent variables, DELLA could achieve\na better ELBO and hence further boost the learn-\ning of data distribution p(x) in Eq.(2), leading to\nsatisfactory quality of generated texts.\nGenerally, DELLA also outperforms baseline\nmodels on generation diversity. The reason is two-\nfold: randomly sampled latent variables zshould\nbring diversity, while the V AE-based baselines tend\nto ignore zas mentioned before, losing some ran-\ndomness. In contrast, latent variables are deeply\nfused in DELLA , maintaining enough randomness.\nBesides, each latent variable is sampled in corre-\nsponding layer, and thus such a sampling process\naccumulates and enhances randomness, further ben-\neÔ¨Åting diversity while keeping good quality.\n5.5.2 Conditional Generation\nWe report the results of WP and CNN/DM in Ta-\nble 2, and leave those of Quora in Appendix A.5.\nAs we can see, DELLA performs better on most\nquality metrics, but gets a little worse on diversity\n702\nModel Quality Diversity KL‚Üë AU‚ÜëBLEU‚Üë Rouge-1‚Üë Rouge-2‚Üë Rouge-L‚Üë BERTScore‚Üë SB‚Üì Dist‚Üë JS‚Üì\nDataset: WritingPrompts\nGPT-2 27.89 27.72 7.96 14.30 78.12 53.78 22.99 0.51 - -\nEmbed 39.67 36.17 7.96 15.78 81.64 64.55 14.31 0.73 2.35 3\nMemory 40.79 36.13 8.04 16.16 81.68 67.56 12.90 0.80 0.07 0\nSoftmax 41.04 36.14 8.12 16.30 81.75 67.02 13.08 0.78 0.32 0\nDELLA 41.39 35.46 8.78 17.20 81.77 56.28 20.91 0.60 28.14 8\nDataset: CNN/DM\nBart-base 48.74 41.33 19.82 29.63 87.75 29.94 43.68 0.10 - -\nEmbed 44.10 40.43 19.41 29.43 87.60 29.60 44.04 0.10 0.0 0\nMemory 46.02 41.18 19.74 29.64 87.78 29.79 43.92 0.11 0.0 0\nSoftmax 44.40 40.94 19.63 29.61 87.00 29.64 44.11 0.10 0.0 0\nDELLA 49.18 41.27 19.85 29.84 88.09 29.07 44.24 0.09 0.91 1\nTable 2: Evaluation results for conditional generation.\nDataset: WritingPrompts\nModel Fluency Coherence Novelty\nGPT2 1.83 2.12 2.50\nEmbed 2.16 2.33 2.67\nMemory 2.45 2.28 2.78\nSoftmax 2.48 2.42 2.85\nDELLA 2.51 2.38 2.89\nDataset: CNN/DM\nModel InformativenessCoherence Novelty\nBart-base 3.12 4.32 3.52\nEmbed 2.88 4.08 3.50\nMemory 2.95 4.23 3.48\nSoftmax 2.91 4.33 3.50\nDELLA 3.05 4.33 3.56\nTable 3: Human evaluation results on conditional gen-\neration. The scores range from 1 (worst) to 5 (best).\nThe p-value is 0.002 and Kappa score is 0.64 which\nindicates acceptable inter-annotator agreement.\ncompared to GPT-2. This is because GPT-2 may\nproduce some ill-formed contents which ‚Äòimprove‚Äô\ndiversity by cheating the metrics but also lead to\nmuch worse quality (lower BLEU and Rouge).\nEven so, on both WP and CNN/DM, DELLA still\nbeats all previous V AE paradigms in diversity, man-\nifesting the effectiveness of our DELLA .\nIn addition, all baselines methods suffer from\nseverer KL vanishing problems on conditional gen-\neration tasks than on the unconditional ones. This\nis because the given condition text could aggravate\nthe reliance of these models on preceding gener-\nated tokens and the condition, and therefore bypass\nlatent variables. By contrast, DELLA could learn\nmore informative z and hence keep a relatively\nhigher KL value even given the condition text.\nModel PPL‚Üì ELBO‚Üì KL‚Üë MI‚Üë AU‚Üë\nDELLA 12.35 239.83 29.47 10.78 23\n-LTP 12.68 249.32 28.52 9.77 21\n-LW 19.88 324.45 20.12 7.23 18\nSeparate 14.17 286.30 28.82 9.88 16\nl= 1KL 12.55 266.97 0.15 0.15 0\nl= 12KL 12.48 263.38 0.73 0.61 0\nEmbed(384) 20.11 327.29 0.55 0.38 0\nMemory(384)20.09 326.24 0.46 0.25 0\nSoftmax(384) 20.15 330.24 5.04 7.15 0\nTable 4: Ablation study on Yelp dataset. LTP: low-rank\ntensor product. LW: layer-wise latent variables. Sep-\narate: latent variables in each layer are independent.\nl = 1 or 4 KL means we only compute KL loss on\nz1 or zL, respectively. 384 means the dimension of\nlatent variable used in baseline are 12 √ó32 = 384.\n5.6 Human Evaluation\nTo better verify the effectiveness of DELLA , we\nalso conduct human evaluation on the two condi-\ntional generation tasks. For each model, we gen-\nerated 30 samples on each task, and invite 5 com-\npetent annotators to score these samples in terms\nof three criteria, Fluency, Coherence and Novelty\nfor story generation, and Informativeness, Coher-\nence and Novelty for summarization generation.\nAs shown in Table 3,DELLA obtains satisfactory\nperformance in quality, and is consistently superior\nto all baselines on diversity and novelty. See Ap-\npendix A.4 for more detailed evaluation protocols.\n5.7 Ablation Study\nTable 4 shows the results of ablation study on Yelp.\nWe can Ô¨Ånd both tensor product and the layer-wise\nlatent variables beneÔ¨Åt the learning of informative\n703\n1 3 5 7 9 11\nlayers\n12\n14\n16\n18\n20\n22PPL\nstart layer\nend layer\n1 3 5 7 9 11\nlayers\n240\n260\n280\n300\n320\n340ELBO\nstart layer\nend layer\n1 3 5 7 9 11\nlayers\n0\n5\n10\n15\n20\n25\n30KL\nstart layer\nend layer\nFigure 2: PPL, ELBO end KL on Yelp with different numbers of latent variables. The values start layer iand end\nlayer jmeans latent variables are produces and utilized only from i-th layer to the last layer, or from the Ô¨Årst layer\nto j-th layer of the encoder respectively.\nModel PPL‚Üì ELBO‚Üì KL‚Üë MI‚Üë AU‚Üë\nEmbed 22.21 339.12 0.03 0.03 0\n+BOW 19.98 326.51 2.75 2.48 4\n+Annealing 19.98 327.28 4.77 4.14 6\n+Annealing + BOW20.59 332.44 19.51 9.12 28\n+Annealing + BN21.14 338.59 21.09 8.98 25\nMemory 22.16 338.68 0.00 0.01 0\n+BOW 19.87 326.00 3.89 3.59 8\n+Annealing 19.95 326.60 5.70 5.30 11\n+Annealing + BOW20.41 331.09 18.76 9.14 28\n+Annealing + BN20.25 331.59 18.11 9.07 24\nSoftmax 22.43 333.93 0.47 0.3 0\n+BOW 20.53 331.89 10.16 5.57 28\n+Annealing 20.14 328.13 7.50 6.29 13\n+Annealing + BOW21.14 335.48 17.51 8.46 28\n+Annealing + BN20.95 337.10 21.25 9.15 25\nDELLA 17.18 312.45 9.39 5.32 6\n+BOW 13.98 289.94 11.59 9.25 8\n+Annealing 12.35 239.83 29.47 10.78 23\n+Annealing+BOW12.82 249.98 32.79 11.26 26\nBackbone: GPT-2 medium (24 layers)\nEmbed 18.33 317.44 2.13 1.44 3\nMem 18.30 317.24 4.47 4.26 10\nSoftmax 18.47 318.80 5.80 5.03 12\nDELLA 11.01 230.96 17.09 23.69 27\nTable 5: Results on Yelp for transformer-bsaed V AE\nwith BOW loss, KL annealing and batch normalization\ntricks, and use 24-layer GPT2-medium as backbone.\nHere we Ô¨Åx Œ≥in batch normlization as 1.\nlatent variables, while the latter contributes the\nmost to DELLA . To further verify the performance\ngain originating from Theorem 1 instead of simply\nincreasing the number or the dimension of latent\nvariables, we conduct two groups of experiments.\nFirst, we remove the conditional dependence be-\ntween layer-wise latent variables by independently\nsampling each zl in both training and testing. We\ncan see that removing dependence causes a sig-\nniÔ¨Åcant performance drop. Besides, we keep the\ndependence between zl but optimize only one of\nthe KL terms in Eq.(6), and Ô¨Ånd all representation\ncapability metrics deteriorate, especially KL, MI\nand AU. Such results effectively demonstrate the\nnecessity of using and optimizing the conditional\ninference of layer-wise latent variables, supporting\nour theoretical interpretation of DELLA .\nSecond, we enlarge the dimension of zl used in\nthe three paradigms to 384 (12 √ó32), equal to the\ntotal latent dimension used in DELLA . The results\nshow that simply increasing the dimension of latent\nvariables brings a more sparse latent space, even\nexacerbating the KL vanishing problem.\n5.8 Analysis\nTraining TricksTo reveal the robustness of our\nmodel, we evaluate the inÔ¨Çuence of three com-\nmonly used training tricks to relieve KL vanish-\ning, i.e., BOW (bag-of-words) loss (Wang et al.,\n2017), batch normalization (Zhu et al., 2020) and\nKL annealing (Fu et al., 2019), to the performance\nof DELLA and the three paradigms. As shown in\nTable 5, previous methods suffer KL vanishing seri-\nously without annealing or BOW loss, getting KL,\nMI and AU almost 0. Though not good as using\nannealing, DELLA still maintains acceptable per-\nformance and mitigates KL vanishing even without\nany training tricks. Bow and batch normalization\ndramatically prevent low KL divergence, but ob-\nstruct the optimization and thus cause higher PPL.\nNumber of Latent VariablesWe observe the\nchange of PPL, KL and ELBO with different num-\nbers of latent variables. We conduct two groups of\nexperiments where we produce and utilize layer-\nwise latent variables starting from and ending at\ndifferent layers. As shown in Fig. 2, incorporating\nmore latent variables could continuously improve\nperformance, consistent to our claim in Sec. 4.\nWith the same number of latent variables, start-\ning from a higher layer is better than ending at a\nlower layer, which indicates that latent variables\n704\nSentence 1: Two girls are playing baseball on the beach0.1: Two girls are playing basketballon the beach.0.3: Two girls are playing musicin the park.0.5: Two girls are singingon the beach.0.7: Crowds of people are dancing in the park0.9: Lots of people are dancing in the parkSentence 2: Many people are dancing in front of building\nSentence 1: A man is taking beautiful photos by the river0.1: A man is taking photos by the beautiful river0.3: A man is taking picturesby the river0.5: A man is enjoying the picnicby the river0.7: A goat is enjoying the dayby the river0.9: A goat is enjoying the dayon the farmSentence 2: A goat is enjoying the sun on the farm\nFigure 3: Interpolating latent space. The sentence\nin each row is generated with a latent variable inter-\npolated from those of sentence 1 and sentence 2.\nPrompt:Afterreceivingperfectscoreonyourfinalexam,yourschoolisaskingyoutogiveaspeechonthesubjectoftheexam.Littledoesanyoneknowyouguessedeverysingleanswer.Memory:Theroomwassilentexceptforme.Icouldhearsomewhisperingbehindusaswecontinuedtopaceaboutcampus,buttherewasnoneed.Weknewbetterthanthatanymomentthatwouldcome,andatleastiftheywerearound,wewouldn'thavetobesoparanoid.DELLA:\"Heymister!\"Ijustgotbackfromworkandthewholeclasswasalreadyreadyforquestion.SogoaheadItookthestage.Theteachersmiledencouragingly.Standingupinthefrontofdesk,Istartedtalkingaboutallsortsofthingsaboutansweringtestquestion.\nFigure 4: Generation examples of Memory and D ELLA\nbased on the prompt from test set of WritingPrompts.\n10000 20000 30000 40000 50000\nT raining Steps\n200\n250\n300\n350\n400\nR econstruction Loss\nModel\nEmbed\nMemory\nSoftmax\nDELLA\n10000 20000 30000 40000 50000\nT raining Steps\n0\n20\n40\n60\n80\n100\nKL Divergence\nModel\nEmbed\nMemory\nSoftmax\nDELLA\nFigure 5: Reconstruction loss and KL Divergence\nthroughout training process.\ngenerated from higher layers encode more help-\nful information compared to those from lower lay-\ners, manifesting disadvantages of the two previous\nparadigms, Softmax (starting from the last layer)\nand Embedding (ending at the Ô¨Årst layer).\nModel size We compare the performance of\nDELLA and three paradigms with 24-layer GPT2-\nmedium as backbone. As shown in Table 5, with\nthe increasing of model size, DELLA consistently\nachieves better performance than baselines.\n5.9 Case Study\nV AE captures text representations in a smooth la-\ntent space. We take two sentences x1 and x2 and\nsample two posterior latent variables z(1) and z(2)\nfrom p(z(1)|x1) and p(z(2)|x2), and get interpo-\nlated latent variables with z= œÑz(1) + (1‚àíœÑ)z(2).\nWe generate multiple sentences with a continuously\nchanged œÑ from 0 to 1. As shown in Fig. 3, sen-\ntences generated from interpolated zmix the se-\nmantics of the two initial sentences and smoothly\nchange from x1 to x2, showing DELLA ‚Äôs ability of\nlearning a Ô¨Çexible latent space.\nFig. 4 shows the generation examples of DELLA\nand one of baseline, Memory, given the same\nprompt WritingPrompts. We observe that the gen-\nerated text of Memory is irrelevant to the prompt,\nwhile DELLA generates coherent and vivid text.\n6 Conclusion\nIn this paper, we propose a novel variational Trans-\nformer framework DELLA . Our framework learns\na series of layer-wise latent variables with iterative\ndependence. These latent variables are condition-\nally inferred and injected into corresponding de-\ncoder layers by low-rank tensor product for deeper\nfusion. The experiments on both unconditional and\nconditional generation tasks demonstrate DELLA ‚Äôs\nability to signiÔ¨Åcantly mitigate KL vanishing and\nimprove generated text‚Äôs quality and diversity. In\nthe future, we plan to explore further the potential\nof DELLA in larger pretrained language models.\nAcknowledgement\nThanks for the anonymous reviewers for their com-\nments. This work is supported by the National Key\nR&D Program of China (No. 2020AAA0106502)\nand International Innovation Center of Tsinghua\nUniversity, Shanghai, China.\n705\nReferences\nAlexander A. Alemi, Ian Fischer, Joshua V . Dillon, and\nKevin Murphy. 2016. Deep variational information\nbottleneck. In International Conference on Learn-\ning Representations.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632‚Äì642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nSamuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew Dai, Rafal Jozefowicz, and Samy Bengio.\n2016. Generating sentences from a continuous\nspace. In Proceedings of The 20th SIGNLL Con-\nference on Computational Natural Language Learn-\ning, pages 10‚Äì21, Berlin, Germany. Association for\nComputational Linguistics.\nYuri Burda, Roger Grosse, and Ruslan Salakhutdinov.\n2016. Importance weighted autoencoders. In Inter-\nnational Conference on Learning Representations.\nRewon Child. 2020. Very deep vaes generalize autore-\ngressive models and can outperform them on images.\nIn International Conference on Learning Represen-\ntations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171‚Äì4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAdji B. Dieng, Yoon Kim, Alexander M. Rush, and\nDavid M. Blei. 2019. Avoiding latent variable col-\nlapse with generative skip models. In International\nConference on ArtiÔ¨Åcial Intelligence and Statistics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889‚Äì898, Melbourne, Australia. Association\nfor Computational Linguistics.\nLe Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen\nDong, and Changyou Chen. 2021. Transformer-\nbased conditional variational autoencoder for con-\ntrollable story generation. arXiv: Computation and\nLanguage.\nHao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao,\nAsli Celikyilmaz, and Lawrence Carin. 2019. Cycli-\ncal annealing schedule: A simple approach to mit-\nigating KL vanishing. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 240‚Äì250, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nAnkush Gupta, Arvind Agarwal, Prawaan Singh, and\nPiyush Rai. 2017. A deep generative framework for\nparaphrase generation. In National Conference on\nArtiÔ¨Åcial Intelligence.\nSuchin Gururangan, Tam Dang, Dallas Card, and\nNoah A. Smith. 2019. Variational pretraining for\nsemi-supervised text classiÔ¨Åcation. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics , pages 5880‚Äì5894, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nJunxian He, Daniel Spokoyny, Graham Neubig, and\nTaylor Berg-Kirkpatrick. 2019. Lagging inference\nnetworks and posterior collapse in variational au-\ntoencoders. In International Conference on Learn-\ning Representations.\nIrina Higgins, Loic Matthey, Arka Pal, Christopher P.\nBurgess, Xavier Glorot, Matthew Botvinick, Shakir\nMohamed, and Alexander Lerchner. 2017. beta-vae:\nLearning basic visual concepts with a constrained\nvariational framework. In International Conference\non Learning Representations.\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan\nSalakhutdinov, and Eric P Xing. 2017. Toward con-\ntrolled generation of text. In International Con-\nference on Machine Learning , pages 1587‚Äì1596.\nPMLR.\nVineet John, Lili Mou, Hareesh Bahuleyan, and Olga\nVechtomova. 2019. Disentangled representation\nlearning for non-parallel text style transfer. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 424‚Äì434.\nDiederik P. Kingma and Max Welling. 2014. Auto-\nencoding variational bayes. In International Confer-\nence on Learning Representations.\nDurk P Kingma, Tim Salimans, Rafal Jozefowicz,\nXi Chen, Ilya Sutskever, and Max Welling. 2016.\nImproved variational inference with inverse autore-\ngressive Ô¨Çow. Advances in neural information pro-\ncessing systems, 29:4743‚Äì4751.\nAlexej Klushyn, Nutan Chen, Richard Kurle, Botond\nCseke, and Patrick van der Smagt. 2019. Learning\nhierarchical priors in vaes. In Neural Information\nProcessing Systems.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871‚Äì7880, Online. Association\nfor Computational Linguistics.\n706\nBohan Li, Junxian He, Graham Neubig, Taylor Berg-\nKirkpatrick, and Yiming Yang. 2019. A surprisingly\neffective Ô¨Åx for deep latent variable modeling of text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3603‚Äì\n3614, Hong Kong, China. Association for Computa-\ntional Linguistics.\nChunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiu-\njun Li, Yizhe Zhang, and Jianfeng Gao. 2020a. Opti-\nmus: Organizing sentences via pre-trained modeling\nof a latent space. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 4678‚Äì4699, Online. As-\nsociation for Computational Linguistics.\nJianing Li, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng.\n2020b. On the relation between quality-diversity\nevaluation and distribution-Ô¨Åtting goal in text gen-\neration. In International Conference on Machine\nLearning.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 110‚Äì119, San Diego, California. Association\nfor Computational Linguistics.\nJuntao Li, Yan Song, Haisong Zhang, Dongmin Chen,\nShuming Shi, Dongyan Zhao, and Rui Yan. 2018.\nGenerating classical Chinese poems via conditional\nvariational autoencoder and adversarial training. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing , pages\n3890‚Äì3900, Brussels, Belgium. Association for\nComputational Linguistics.\nChin-Yew Lin and Eduard Hovy. 2002. Manual and\nautomatic evaluation of summaries. In Proceed-\nings of the ACL-02 Workshop on Automatic Summa-\nrization, pages 45‚Äì51, Phildadelphia, Pennsylvania,\nUSA. Association for Computational Linguistics.\nZhaojiang Lin, Genta Indra Winata, Peng Xu, Zihan\nLiu, and Pascale Fung. 2020. Variational transform-\ners for diverse response generation. arXiv: Compu-\ntation and Language.\nZhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-\nnarasimhan, Paul Pu Liang, AmirAli Bagher Zadeh,\nand Louis-Philippe Morency. 2018. EfÔ¨Åcient low-\nrank multimodal fusion with modality-speciÔ¨Åc fac-\ntors. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 2247‚Äì2256, Melbourne,\nAustralia. Association for Computational Linguis-\ntics.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated\ncorpus of English: The Penn Treebank. Computa-\ntional Linguistics, 19(2):313‚Äì330.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311‚Äì318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaid\nHarchaoui. 2021. Mauve: Measuring the gap be-\ntween neural text and human text using divergence\nfrontiers. In Neural Information Processing Sys-\ntems.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAli Razavi, Aaron van den Oord, Ben Poole, and Oriol\nVinyals. 2019. Preventing posterior collapse with\ndelta-vaes. In International Conference on Learning\nRepresentations.\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan\nWierstra. 2014. Stochastic backpropagation and\napproximate inference in deep generative models.\nIn International conference on machine learning ,\npages 1278‚Äì1286. PMLR.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073‚Äì\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nStanislau Semeniuta, Aliaksei Severyn, and Erhardt\nBarth. 2017. A hybrid convolutional variational au-\ntoencoder for text generation. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 627‚Äì637, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,\nLaurent Charlin, Joelle Pineau, Aaron Courville,\nand Yoshua Bengio. 2016. A hierarchical latent\nvariable encoder-decoder model for generating dia-\nlogues. arXiv: Computation and Language.\nCasper Kaae S√∏nderby, Tapani Raiko, Lars Maal√∏e,\nS√∏ren Kaae S√∏nderby, and Ole Winther. 2016. Lad-\nder variational autoencoders. In Neural Information\nProcessing Systems.\nArash Vahdat and Jan Kautz. 2020. Nvae: A deep hier-\narchical variational autoencoder. In Neural Informa-\ntion Processing Systems.\n707\nAaron van den Oord, Oriol Vinyals, and Koray\nKavukcuoglu. 2017. Neural discrete representation\nlearning. In Neural Information Processing Sys-\ntems.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998‚Äì6008.\nKe Wang and Xiaojun Wan. 2018. Sentigan: Gener-\nating sentimental texts via mixture adversarial net-\nworks. In International Joint Conference on ArtiÔ¨Å-\ncial Intelligence.\nLiwei Wang, Alexander G. Schwing, and Svetlana\nLazebnik. 2017. Diverse and accurate image de-\nscription using a variational auto-encoder with an\nadditive gaussian encoding space. In Neural Infor-\nmation Processing Systems.\nTianming Wang and Xiaojun Wan. 2019. T-cvae:\ntransformer-based conditioned variational autoen-\ncoder for story completion. In International Joint\nConference on ArtiÔ¨Åcial Intelligence.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38‚Äì45, Online. Asso-\nciation for Computational Linguistics.\nZichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and\nTaylor Berg-Kirkpatrick. 2017. Improved varia-\ntional autoencoders for text modeling using dilated\nconvolutions. In International conference on ma-\nchine learning, pages 3881‚Äì3890. PMLR.\nXiaoyuan Yi, Ruoyu Li, Cheng Yang, Wenhao Li, and\nMaosong Sun. 2020. Mixpoet: Diverse poetry gen-\neration via learning controllable mixed latent space.\nIn Proceedings of The Thirty-Fourth AAAI Confer-\nence on ArtiÔ¨Åcial Intelligence, New York, USA.\nMeng-Hsuan Yu, Juntao Li, Danyang Liu, Dongyan\nZhao, Rui Yan, Bo Tang, and Haisong Zhang. 2020.\nDraft and edit: Automatic storytelling through\nmulti-pass hierarchical conditional variational au-\ntoencoder. In National Conference on ArtiÔ¨Åcial In-\ntelligence.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi.\n2017. Learning discourse-level diversity for neural\ndialog models using conditional variational autoen-\ncoders. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 654‚Äì664, Vancou-\nver, Canada. Association for Computational Linguis-\ntics.\nQile Zhu, Wei Bi, Xiaojiang Liu, Xiyao Ma, Xiaolin\nLi, and Dapeng Wu. 2020. A batch normalized in-\nference network keeps the KL vanishing away. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 2636‚Äì\n2649, Online. Association for Computational Lin-\nguistics.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,\nWeinan Zhang, Jun Wang, and Yong Yu. 2018. Texy-\ngen: A benchmarking platform for text generation\nmodels. In International ACM SIGIR Conference on\nResearch and Development in Information Retrieval.\n708\nA Experiment Details\nA.1 Implementation Details\nWe load pretrained model GPT-2 (Radford et al.,\n2019) as initial parameters for unconditional gener-\nation and story generation, and pretrained BART-\nbase (Lewis et al., 2020) for summarization and\nparaphrasing generation tasks. For the summa-\nrization and paraphrasing generation, we keep\nthe encoder-decoder attention block. No encoder-\ndecoder attention is used in unconditional gener-\nation and story generation tasks. The number of\nlayers and dimensions of hidden states in DELLA\nis consistent with the conÔ¨Ågurations of correspond-\ning pretrained models (GPT-2 has 12 layers and\nBart-base has 6-layer encoder and 6-layer decoder.\nThe hidden size of both is 768). We use the state\nof a special token to obtain the representation in\nthe encoder. We utilize cyclical annealing tricks to\ntrain DELLA and other V AE baselines. SpeciÔ¨Åcally,\ntwo epochs are one annealing period. In one period,\nŒ≤(the weight of KL term in ELBO) keeps 1e-5 in\nthe Ô¨Årst half, then linearly increases to 1 in the next\nquarter, then keeps at 1 for the last quarter. We se-\nlect batch size over {16,32}and learning rate over\n{5e-5, 7e-5}. We use beam search for DELLA and\ntop-k sampling for compared baseline models for\nthe unconditional generation and story generation.\nFor the summarization and paraphrasing genera-\ntion, we use beam search in all the models.\nWe implement DELLA and other V AE baselines\nbased on Huggingface Transformers (Wolf et al.,\n2020) library of v4.10.0 and use NVIDIA GeForce\nRTX 3090 to train our model. The total number of\ntraining GPU hours on different datasets is in Table\n6. The number of parameters for our model is\n193,353,984 in the unconditional generation setting\nand 195,180,114 in the conditional generation one.\nAll experimental results are trained and tested in a\nsingle run.\nDataset Training Time\nYelp 20h\nYahoo 20h\nPTB 6h\nSNLI 12h\nCNN/DM 40h\nWP 170h\nQuora 5h\nTable 6: GPU hours of training DELLA with RTX3090\nDataset # Train # Dev # Test Avarage Length\nYelp 100k 10k 10k 96\nYahoo 100k 10k 10k 79\nPTB 42k 3k 3k 21\nSNLI 100k 10k 10k 10\nCNN/DM 287k 13k 11k S: 790 T: 61\nWP 272k 15k 15k S: 28 T: 674\nQuora 134k 5k 10k S: 10 T: 10\nTable 7: Statistics of datasets. We present the size of\ntrain/dev/test sets and the average length for 7 datasets.\nS means source text and T means target text.\nA.2 Datasets Details\nThe detailed dataset statistics are in Table 7. For\nthe licenses of the datasets we use, CNN/DM and\nWritingPrompts use MIT License, while SNLI uses\nCC BY-SA 4.0. Meanwhile, PTB, Quora, and Yelp\nuse their own license: LDC User Agreement, Yelp\nData Agreement, and Quora‚Äôs Terms of Service,\nrespectively. All of these licenses and agreements\nallow their data for academic use. Unfortunately,\nwe did not Ô¨Ånd the license for the Yahoo Dataset.\nA.3 Metrics Details\nHere we provide more details of the metrics used\nin our experiments.\nPerplexity (PPL). PPL = p(x)‚àí1/n is com-\nmonly used to evaluate the performance of lan-\nguage models, where nis number of tokens xcon-\ntains. For V AE-based model, we can only obtain\nthe lower bound of log p(x). We consider klatent\nvariables z1,z2,...,z k sampled from the posterior\ndistribution q(zi|x). Based on the fact that average\nimportance weights are an unbiased estimator of\nlog p(x) (Burda et al., 2016) and Jensen‚Äôs Inequal-\nity, we have:\nLk = E\n[\nlog 1\nk\nk‚àë\ni=1\np(x,zi)\nq(zi|x)\n]\n(13)\n‚â§log E\n[\n1\nk\nk‚àë\ni=1\np(x,zi)\nq(zi|x)\n]\n= logp(x).\nWe use Lk to estimate log p(x) and calculate PPL.\nELBO. The ELBO is the sum of reconstruction\nloss and KL divergence.\nKL. The KL divergence of the posterior and\nprior distribution.\nMutual Information(MI)(Alemi et al., 2016).\n709\nMutual Information I(x,z) is deÔ¨Åned as:\nIq(x,z) (14)\n=Ep(x)Eq(z|x) log q(z|x) ‚àíEq(z) log q(z)\nwhere q(z) =Ep(x)q(z|x) is called the aggregated\nposterior.\nActivate Units(AU)(Burda et al., 2016). AU\nis the active units in latent varibles, deÔ¨Åned as\nAz = Covx(Ez‚àºq(z|x)[z]) >Œ¥, where Œ¥is a thresh-\nold, commonly set as 0.01. However, we Ô¨Ånd that\nwith Œ¥= 0.01, all V AE models in our experiments\nhave full active unit. So we increase the threshold\nto 0.2 to distinguish the performance of different\nmodels on this metric. Please notethat DELLA in-\ncorporates latent variables in all layers, and hence\nwe calculate AU for the latent variable in each layer\nand then report the average.\nBLEU (Papineni et al., 2002). BLEU measures\nthe n-gram overlap of generated sequences and\nthe reference ones. For unconditional setting, we\nregard all samples in the test set as references to\neach generated example.\nCND (Li et al., 2020b). CND approximates the\ndivergence of the empirical reference distribution\nand generated text distribution in n-gram spaces.\nMAUVE (Pillutla et al., 2021). MAUVE mea-\nsures the gap between reference text and generated\ntext using divergence frontiers.\nSelf-BLEU (Zhu et al., 2018). Self-Bleu calcu-\nlates the BLEU score on the generated samples,\nwhich averages the BLEU score of each generated\nsequence calculated with other generated ones as\nreferences. This metric measures the diversity of\na set of generated sequences. Higher Self-BLEU\nmeans these generated sequences are more distin-\nguishable from each other.\nDist (Li et al., 2016). Dist measures the propor-\ntion of distinct n-grams on generated samples.\nJaccard Similarity(JS)(Wang and Wan, 2018).\nJS calculates the average n-gram Jaccard similarity\nbetween every two generated sequences.\nRouge (Lin and Hovy, 2002). Rouge computes\nn-gram overlap of generated examples with given\ntarget samples. We use rouge-score v0.0.4 to evalu-\nate the rouge score of our model and the baselines.\nBERTScore (Zhang et al., 2020). BERTScore\nuses pre-trained BERT (Devlin et al., 2019) to ob-\ntain the vector representations of generated and\nreference text and calculates their cosine similar-\nity. We use bert-score v0.3.10 to calculate the\nBERTScore of our model and the baselines.\nA.4 Human Evaluation Details\nDue to the relatively long length of generated text,\nwe randomly sample 30 examples in the test set\nof WP and CNN/DM as input to DELLA and other\ncompared baseline models to generate the target.\nWe invite Ô¨Åve graduate students proÔ¨Åcient in En-\nglish to score the generated text. The criteria for\nstory generation include Ô¨Çuency, coherence, and\nnovelty, and the criteria for summarization gen-\neration include informativeness, consistency, and\nnovelty. SpeciÔ¨Åcally, Ô¨Çuency measures whether\nthe generated sentences are syntactically Ô¨Çuent;\ncoherence measures whether the generated text is\nlogically structured and consistent with the input\ntext; novelty measures whether the content is novel\nand attractive; informativeness measures to what\nextent the generated summarization summarizes\nthe general idea of the article.\nWhen conducting the human evaluation, we in-\nformed the participants as follows:\n‚Ä¢ The following contents are generated by the\nautomatic models. Some of them may be of-\nfensive or contain improper arguments. Please\nbe conscious of these risks and evaluate these\ncontents equitably and adequately.\n‚Ä¢ The evaluation you provide will be used only\nfor academic use and will never be used com-\nmercially.\nEvery evaluator will sign their signature below\nthese warnings to conÔ¨Årm that they have read those\nwords. After Ô¨Ånishing the annotation, they will re-\nceive $25. This amount is determined by the time\nof the whole annotation process and the estimation\nof average hourly income. The ethics review board\nfor data collection protocol is not essential in our\ncountry, so we did not conduct this review for our\ndata collection protocol.\nA.5 Additional Experimental Result\nTable 8 and Table 9 report the results on PTB, SNLI\nand Quora dataset.\nA.6 Case Study Details\nWe take two sentences x1 and x2\nand sample two groups of latent vari-\nables z(1) = {z(1)\n1 ,z(1)\n2 ,..., z(1)\nL } and\nz(2) = {z(2)\n1 ,z(2)\n2 ,..., z(2)\nL }from posterior distri-\nbutions p(z(1)|x1) and p(z(2)|x2). We obtain the\nweighted latent variables ÀÜz= {ÀÜz1,ÀÜz2,..., ÀÜzL}by\n710\ntaking weighted sum at each corresponding ele-\nment in two groups, i.e. ÀÜzi = œÑ‚àóz(1)\ni +(1‚àíœÑ)‚àóz(2)\ni .\nThe mixed sentence ÀÜxis generated conditioned on\np(ÀÜx|ÀÜz) by the decoder.\nA.7 Potential Risks and Limitations of our\nwork\nDue to the unclean corpus (especially in the WP\ndataset) we use where slang repeatedly appears,\nthe model training on this corpus may also output\nsome rude expressions during generation. Also, the\ntext generated in the unconditional generation task\nis not controllable, which may contain some bias\nor politically sensitive expression. Besides, since\nour model signiÔ¨Åcantly improves the quality and di-\nversity of generated, it can produce more plausible\ntexts like news, which could be possibly utilized to\ncreate fake news or disinformation. However, on\nthe other hand, our model could beneÔ¨Åt fairness in\nlanguage generation. Previous text generation mod-\nels tend to produce biases like gender or nationality\nbiases, which means only the majority would be\nappropriately described while the minority may be\nignored. These biases are mainly caused by the\nbiased training corpus. With the same data, our\nmodel can improve the diversity of generated text,\nwhich is also potential for mitigating these biased.\nWe will try to develop debiased language gener-\nation systems in future work to avoid these risks\nharming society.\nWhile DELLA shows good performance on text\ngeneration, it has one limitation: training efÔ¨Åciency.\nDELLA brings more parameters compared with\nthree baseline methods. Training efÔ¨Åciency needs\nto be considered if we further explore the perfor-\nmance of DELLA on the large pretrained model.\n711\nModel Representation Learning Generation Quality Generation Diversity\nPPL‚Üì ELBO‚Üì KL‚Üë MI‚Üë AU‚Üë BLEU‚Üë CND‚Üì MAUVE‚Üë SB‚Üì Dist‚Üë JS ‚Üì\nDataset: PTB\nGPT-2 25.80 - - - - 27.91 1.12 0.73 41.55 37.79 0.30\nOptimus 22.79 344.10 15.09 7.67 - - - - - - -\nEmbed 19.98 327.28 4.77 4.14 6 28.04 1.38 0.69 41.32 34.46 0.33\nMemory 24.41 90.25 1.22 1.17 4 21.31 1.21 0.58 26.58 38.28 0.08\nSoftmax 24.04 90.63 2.13 1.89 21 28.59 1.39 0.72 42.15 33.91 0.30\nDELLA 10.28 58.43 12.46 12.35 22 28.15 0.63 0.68 24.87 41.84 0.17\nDataset: SNLI\nGPT-2 20.19 - - - - 63.57 1.95 0.71 75.34 19.11 0.58\nOptimus 16.67 38.50 16.35 8.89 - - - - - - -\nEmbed 13.79 32.97 3.24 3.16 20 59.26 0.98 0.72 65.59 20.89 0.44\nMemory 13.78 32.62 2.13 2.08 10 62.80 1.24 0.67 54.59 21.87 0.33\nSoftmax 14.21 33.18 2.70 2.65 16 60.51 1.94 0.71 71.84 18.59 0.57\nDELLA 5.13 10.23 5.86 16.58 23 62.94 0.85 0.69 36.85 32.61 0.21\nTable 8: Additional results for language model and unconditional generation task. The results of Optimus are\ncopied from original paper with Œª= 0.5.\nModel BLEU‚Üë Rouge-1‚Üë Rouge-2‚Üë Rouge-L‚Üë Bertscore‚Üë KL‚Üë\nBart-base 64.34 63.27 39.83 60.28 94.72 -\nEmbed 63.94 63.12 39.42 60.22 94.66 0.0\nMem 63.78 62.86 39.18 59.96 94.65 0.0\nSoftmax 64.30 63.25 39.92 60.39 94.71 0.0\nDELLA 64.40 63.80 40.58 61.03 94.84 3.88\nTable 9: Results on Quora dataset. Because the sentences in Quora are quite short and constrained, the results of\nthe three diversity metrics on all baselines are almost the same. So we omit them here.\n712\nB Additional Proof\nB.1 Derivation of KL Divergence of Layer-Wise Latent Variables\nKL divergence of layer-wise latent variables\nKL(q(z|x)||p(z))\n=\n‚à´\nq(z|x) logq(z|x)\np(z) dz\n=\n‚à´ L‚àè\nl=1\nq(zl|x,z<l) log\n‚àèL\nl=1 q(zl|x,z<l)‚àèL\nl=1 p(zl|z<l)\ndz1 dz2 ... dzL\n=\nL‚àë\ni=1\n‚à´ L‚àè\nl=1\nq(zl|x,z<l) logq(zl|x,z<l)\np(zl|z<l) dz1 dz2 ... dzL\n=\nL‚àë\nl=1\n‚à´\nq(z<l|x)q(zl|x,z<l) logq(zl|x,z<l)\np(zl|z<l) dz1 dz2 ... dzl\n=\nL‚àë\nl=1\nEq(z<l|x)KL(q(zl|x,z<l)||p(zl|z<l))\n(15)\nB.2 Proof of Theorem 1\nFirst, we consider on term in the summation and can obtain:\nEp(x)Eq(z<l|x)[KL(q(z|x,z<l)||p(zl|z<l))]\n=\n‚à´\nq(x)q(z<l|x)q(zl|x,z<l) logq(zl|x,zl)\np(zl|z<l) dxdzl dz<l\n=\n‚à´\nq(x,zl,z<l) logq(zl|x,zl)\np(zl|z<l) dxdzl dz<l\n=\n‚à´\nq(x,zl,z<l) log\n( q(z,x|z<l)\nq(x|z<l)q(zl|z<l)\nq(zl|z<l)\np(zl|z<l)\n)\ndxdzl dz<l\n=\n‚à´\nq(z<l)q(x,zl|z<l) log q(z,x|z<l)\nq(x|z<l)q(zl|z<l) dxdzl dz<l+\n‚à´\nq(x|zl,z<l)q(zl|z<l)q(z<l) logq(zl|z<l)\np(zl|z<l) dxdzl dz<l\n=\n‚à´\nq(x,zl|z<l) log q(z,x|z<l)\nq(x|z<l)q(zl|z<l) dxdzl+\n‚à´\nq(zl|z<l)q(z<l) logq(zl|z<l)\np(zl|z<l) dzl dz<l\n=H(zl|z<l) ‚àíH(zl|z<l,x) +Eq(z<l)KL(q(zl|z<l||p(zl|z<l))\n‚â•H(zl|z<l) ‚àíH(zl|z<l,x)\n(16)\nwhere H is the Shannon entropy. Then, the summation has a lower bound:\nL‚àë\ni=1\nEp(x)Eq(z<l|x)[KL(q(z|x,z<l)||p(zl|z<l))]\n‚â•\nL‚àë\ni=1\nH(zl|z<l) ‚àíH(zl|z<l,x)\n=H(z1,..., zL) ‚àíH(z1,..., zL|x)\n=I(x; z1,..., zL)\n(17)\n713\nwhere I is mutual information. Next, we prove the following inequality with induction:\nI(x; z1,..., zL) ‚â•I(x; z1; ... ; zL) (18)\nWhen L= 2, we proof I(x; z1,z2) ‚â•I(x; z1; z2). Actually, we have the following facts:\nI(x; z1,z2)\n=H(x) +H(z1,z2) ‚àíH(x,z1,z2) (19)\nI(x; z1; z2)\n=H(x) +H(z1) +H(z2) +H(x,z1,z2)\n‚àíH(z1,z2) ‚àíH(x,z1) ‚àíH(x,z2)\n(20)\nBased on the facts above, we have:\nI(x; z1,z2) ‚â•I(x; z1; z2) (21)\n‚áî2H(z1,z2) +H(x,z1) +H(x,z2) ‚â•H(z1) +H(z2) + 2H(x,z1,z2) (22)\nIt‚Äôs true because we have:\nH(z1,z2) +H(x,z1)\n=H(z2|z1) +H(x|z1) + 2H(z1)\n‚â•H(x,z2|z1) + 2H(z1)\n=H(x,z1,z2) +H(z1)\n(23)\nSimilarly, the following inequality also holds true:\nH(z1,z2) +H(x,z2) ‚â•H(x,z1,z2) +H(z2) (24)\nTherefore, making sum to Eq.(23) and Eq.(24), we conclude that I(x; z1,z2) ‚â•I(x; z1; z2). Hence, we\nÔ¨Ånish the proof of the L= 2case.\nWhen L = k, suppose I(x; z1,..., zk) ‚â•I(x; z1; ... ; zk), we consider L = k+ 1. In this case,\nbased on the inductive assumption, we have:\nI(x; z1,..., zk+1) ‚â•I(x; z1,..., zk) ‚â•I(x; z1; ... ; zk) ‚â•I(x; z1; ... ; zk+1) (25)\nHence, the case of L = k + 1 also holds true. Therefore, we conclude that I(x; z1,..., zL) ‚â•\nI(x; z1; ... ; zL).\nNow, we consider the interaction information and can obtain:\nI(x; z1; ... ; zL)\n=I(zL,zL‚àí1) ‚àí\nL‚àí1‚àë\ni=2\nI(zL; ... ; zi|zi‚àí1) ‚àíI(zL; ... ; z1|x)\n‚â•\nL‚àí1‚àë\ni=2\nI(zL; ... ; zi|zi‚àí1) ‚àíI(zL; ... ; z1|x)\n(26)\nFinally, based on Eq.(16), (17), (25), (26), we can conclude:\nEp(x)[LR] =\nL‚àë\ni=1\nEp(x)Eq(z<l|x)[KL(q(z|x,z<l)||p(zl|z<l))]\n‚â•I(x; z1,..., zL)\n‚â•I(x; z1; ... ; zL)\n‚â•\nL‚àí1‚àë\ni=2\nI(zL; ... ; zi|zi‚àí1) ‚àíI(zL; ... ; z1|x)\n(27)\n714\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 0, average\n0.0\n0.2\n0.4\n0.6\n0.8\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 0, max\n0.0\n0.2\n0.4\n0.6\n0.8\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 0, min\n0.0\n0.2\n0.4\n0.6\n0.8\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention  Layer: 1, average\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 1, max\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 1, min\n0.0\n0.2\n0.4\n0.6\n0.8\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention  Layer: 2, average\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention  Layer: 2, max\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 2, min\n0.0\n0.2\n0.4\n0.6\n0.8\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention  Layer: 3, average\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 3, max\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 3, min\n0.0\n0.2\n0.4\n0.6\n0.8\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 4, average\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention  Layer: 4, max\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 4, min\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 5, average\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 5, max\n0.0\n0.2\n0.4\n0.6\n0.8\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention  Layer: 5, min\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFigure 6: Attention weight of the Memory paradigm for layer 0 to layer 5. We plot three heatmaps in each layer.\nAverage means averaging weights throught all head. Max and min means we select the head with max and min\nattention weight on the memory token (latent variable).We can see the memory token tends to be ignored by most\nheads especially in lower layers.\n715\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 6, average\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 6, max\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention  Layer: 6, min\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 7, average\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention  Layer: 7, max\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention  Layer: 7, min\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 8, average\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention  Layer: 8, max\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention  Layer: 8, min\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 9, average\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention  Layer: 9, max\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention  Layer: 9, min\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 10, average\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention  Layer: 10, max\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 10, min\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention  Layer: 11, average\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention Layer: 11, max\n0.0\n0.2\n0.4\n0.6\n0.8\n<s> the woman is playing the piano\nz\n<s>\nthe\nwoman\nis\nplaying\nthe\npiano\nAttention  Layer: 11, min\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFigure 7: Attention weight of Memory paradigm for layer 6 to layer 11.\n716",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.6819568276405334
    },
    {
      "name": "Latent variable",
      "score": 0.6611167192459106
    },
    {
      "name": "Transformer",
      "score": 0.6544676423072815
    },
    {
      "name": "Variable (mathematics)",
      "score": 0.5070719718933105
    },
    {
      "name": "Fuse (electrical)",
      "score": 0.5005018711090088
    },
    {
      "name": "Computer science",
      "score": 0.49863386154174805
    },
    {
      "name": "Computational linguistics",
      "score": 0.43103331327438354
    },
    {
      "name": "Layer (electronics)",
      "score": 0.4282240569591522
    },
    {
      "name": "Natural language processing",
      "score": 0.41797566413879395
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40674254298210144
    },
    {
      "name": "Engineering",
      "score": 0.22905153036117554
    },
    {
      "name": "Electrical engineering",
      "score": 0.21565383672714233
    },
    {
      "name": "Mathematics",
      "score": 0.17292729020118713
    },
    {
      "name": "Materials science",
      "score": 0.12509921193122864
    },
    {
      "name": "Nanotechnology",
      "score": 0.09722906351089478
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I29955533",
      "name": "Center for Information Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    }
  ]
}