{
  "title": "Question Answering over Knowledge Base using Language Model Embeddings",
  "url": "https://openalex.org/W3090028958",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3100051643",
      "name": "Japa Sai Sharath",
      "affiliations": [
        "Southern Illinois University Carbondale"
      ]
    },
    {
      "id": "https://openalex.org/A2743047757",
      "name": "Rekabdar Banafsheh",
      "affiliations": [
        "Southern Illinois University Carbondale"
      ]
    },
    {
      "id": "https://openalex.org/A3100051643",
      "name": "Japa Sai Sharath",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743047757",
      "name": "Rekabdar Banafsheh",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6732742072",
    "https://openalex.org/W2471900581",
    "https://openalex.org/W2251079237",
    "https://openalex.org/W2251143283",
    "https://openalex.org/W6691476020",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2882319491",
    "https://openalex.org/W2964212344",
    "https://openalex.org/W2251287417",
    "https://openalex.org/W2766317792",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2022166150",
    "https://openalex.org/W1801721664",
    "https://openalex.org/W2739716023",
    "https://openalex.org/W2068470708",
    "https://openalex.org/W6633947590",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6684350123",
    "https://openalex.org/W2511149293",
    "https://openalex.org/W2142898321",
    "https://openalex.org/W6731910000",
    "https://openalex.org/W6629631241",
    "https://openalex.org/W2584356431",
    "https://openalex.org/W2148721079",
    "https://openalex.org/W1894439495",
    "https://openalex.org/W6617055948",
    "https://openalex.org/W2132839294",
    "https://openalex.org/W2121269107",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W1552847225",
    "https://openalex.org/W6606104288",
    "https://openalex.org/W6800569846",
    "https://openalex.org/W6638318767",
    "https://openalex.org/W1891272071",
    "https://openalex.org/W80422862",
    "https://openalex.org/W6751097180",
    "https://openalex.org/W6726253136",
    "https://openalex.org/W2251289180",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2929198159",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W6729654139",
    "https://openalex.org/W6729263887",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1496189301",
    "https://openalex.org/W2093605559",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2950482226",
    "https://openalex.org/W2293634267",
    "https://openalex.org/W3192290435",
    "https://openalex.org/W2951278025",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2167187514",
    "https://openalex.org/W2798858969",
    "https://openalex.org/W580074167",
    "https://openalex.org/W1615518286",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W2952792693",
    "https://openalex.org/W2552027021",
    "https://openalex.org/W2551396370",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W1566256432",
    "https://openalex.org/W2963871484",
    "https://openalex.org/W1990518628",
    "https://openalex.org/W4295253143",
    "https://openalex.org/W150307355",
    "https://openalex.org/W2516930406",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2573462507",
    "https://openalex.org/W3099235767"
  ],
  "abstract": "Knowledge Base, represents facts about the world, often in some form of\\nsubsumption ontology, rather than implicitly, embedded in procedural code, the\\nway a conventional computer program does. While there is a rapid growth in\\nknowledge bases, it poses a challenge of retrieving information from them.\\nKnowledge Base Question Answering is one of the promising approaches for\\nextracting substantial knowledge from Knowledge Bases. Unlike web search,\\nQuestion Answering over a knowledge base gives accurate and concise results,\\nprovided that natural language questions can be understood and mapped precisely\\nto an answer in the knowledge base. However, some of the existing\\nembedding-based methods for knowledge base question answering systems ignore\\nthe subtle correlation between the question and the Knowledge Base (e.g.,\\nentity types, relation paths, and context) and suffer from the Out Of\\nVocabulary problem. In this paper, we focused on using a pre-trained language\\nmodel for the Knowledge Base Question Answering task. Firstly, we used Bert\\nbase uncased for the initial experiments. We further fine-tuned these\\nembeddings with a two-way attention mechanism from the knowledge base to the\\nasked question and from the asked question to the knowledge base answer\\naspects. Our method is based on a simple Convolutional Neural Network\\narchitecture with a Multi-Head Attention mechanism to represent the asked\\nquestion dynamically in multiple aspects. Our experimental results show the\\neffectiveness and the superiority of the Bert pre-trained language model\\nembeddings for question answering systems on knowledge bases over other\\nwell-known embedding methods.\\n",
  "full_text": "Question Answering over Knowledge Base using\nLanguage Model Embeddings\nSai Sharath, Japa\nSchool of Computing\nSouthern Illinois University\nCarbondale, IL\nsharath.japa@siu.edu\nBanafsheh, Rekabdar\nSchool of Computing\nSouthern Illinois University\nCarbondale, IL\nbrekabdar@cs.siu.edu\nAbstract—Knowledge Base, represents facts about\nthe world, often in some form of subsumption ontology,\nrather than implicitly, embedded in procedural code,\nthe way a conventional computer program does. While\nthere is a rapid growth in knowledge bases, it poses a\nchallenge of retrieving information from them. Knowl-\nedge Base Question Answering is one of the promising\napproaches for extracting substantial knowledge from\nKnowledge Bases. Unlike web search, Question An-\nswering over a knowledge base gives accurate and con-\ncise results, provided that natural language questions\ncan be understood and mapped precisely to an answer\nin the knowledge base. However, some of the existing\nembedding-based methods for knowledge base ques-\ntion answering systems ignore the subtle correlation\nbetween the question and the Knowledge Base (e.g.,\nentity types, relation paths, and context) and suﬀer\nfrom the Out Of Vocabulary problem. In this paper,\nwe focused on using a pre-trained language model for\nthe Knowledge Base Question Answering task. Firstly,\nwe used Bert base uncased for the initial experiments.\nWe further ﬁne-tuned these embeddings with a two-\nway attention mechanism from the knowledge base to\nthe asked question and from the asked question to the\nknowledge base answer aspects. Our method is based\nonasimpleConvolutionalNeuralNetworkarchitecture\nwith a Multi-Head Attention mechanism to represent\nthe asked question dynamically in multiple aspects.\nOurexperimentalresultsshowtheeﬀectivenessandthe\nsuperiority of the Bert pre-trained language model em-\nbeddings for question answering systems on knowledge\nbases over other well-known embedding methods.\nIndex Terms—knowledge base question answering,\nBERT, Language Model, KBQA, Multi-Head Atten-\ntion\nI. Introduction\nQuestion Answering (QA) systems enable natural lan-\nguage platforms to interact with the Knowledge Bases.\nThese QA systems return direct and more speciﬁc answers\nto the asked questions. In recent years, with the increase in\nthe popularity and the use of knowledge bases (KB) such\nas Google’s Knowledge Graph [33], YAGO [34], DBPedia\n[25], and Freebase [4] people are more interested in seeking\neﬀective methods to access these knowledge bases. Most of\nsuch knowledge bases adopt Resource Description Frame-\nwork (RDF) as their data format and they also contain\nbillions of SPO (subject, predicate, object) triples [24].\nThere are several languages designed for querying such\nlarge KBs, including SPARQL [31], Xcerpt [8], RQL [9].\nHowever, learning these languages adds a limitation, as\none needs to be familiarized with the query language, it’s\nsyntax, it’s semantics, and the ontology of the knowledge\nbase.\nBy contrast, KB-QA which takes natural language as\nit’s query is a more user-friendly solution and became a\nresearch focus in recent years [36]. There are two main\nresearch streams for the task of QA: Semantic Parsing-\nbased systems (SP-based) [51], [11], [52], and Information\nRetrieval-based systems (IR-based) [17], [46], [5], [6]. SP-\nbased methods address the QA problem by constructing a\nsemantic parser that converts the natural language ques-\ntion into a conditionally structured expressions like the\nlogical forms [1] and then run the query on the Knowledge\nbase to obtain the answer. SP- based methods consists\nof three modules: (1) Entity linking, recognizes all entity\nmentions in a question and links each mention to an entity\nin KB; (2)Predicate mapping, ﬁnds candidate predicates\nin KB for the question;(3) Answer selection, converts the\ncandidate entity-predicate pairs into a query statement\nand queries the knowledge base to obtain the answer. IR-\nbased methods [17], [46], [5], [6] focus on mapping answers\nand questions into the same embedding space, where one\ncould query any KB, independent of its schema without\nrequiring any grammar or lexicon. IR-based methods are\nmore ﬂexible and require less supervision compared with\nthe SP-based approaches [51], [11], [52].\nWith the advancement in the embedding techniques\nwhich can capture the semantics, deep neural networks\nare applied in several areas of Natural Language Pro-\ncessing (NLP) [29] and Natural Language Understand-\ning (NLU) [40]. In the ﬁeld of KB-QA, under IR-based\numbrella, embedding-based approaches [7] [18]have been\nproposed. However, these approaches face two limitations.\nFirst, these models encode diﬀerent components sepa-\nrately without learning the representation of the whole\nKB. Hence, are not able to capture the compositional\nsemantics in a global perspective. Second, the performance\narXiv:2010.08883v1  [cs.CL]  17 Oct 2020\nof Long Short Term Memory [30](LSTM)s, Bi-directional\nLSTM [16](BiLSTM)s, and Convolutional Neural Network\n[15](CNN)s are heavily dependent on large training data\nsets which is often not available in practice. In recent\nyears, pre-trained Language Models [12] [28] [27] [26] on\nlarge-scale unsupervised corpus has shown its advantages\non mining prior linguistic knowledge automatically, it\nindicates a possible way to deal with above problems.\nIn comparison to previous embedding-based approaches,\nwe focus on exploiting pre-trained language models for\nIR based KB-QA task. We use BERT [12] pre-trained\nlanguage model embeddings to encode our question and\ncandidate answer contexts. We exploit a multi head at-\ntention encoder based on Convolution Neural Network\n(CNN) encoder [37] [22] to ﬁne tune Bert pre-trained\nembeddings in particular for the KB-QA task. Since we\nuse a Language Model trained on the English Wikipedia\nand Brown Corpus [12] for our question and KB repre-\nsentations, Out Of Vocabulary (OOV) problem can be\nnegated. The interrelationships between the questions and\nthe underlying KB are stored as the context for our model.\nThis context is used to ﬁlter out the candidate answers by\nimplementing cross attention between the question being\nasked and the KB.\nThe contribution of this paper is summarized as follows:\n(1) We propose a method called Language Model based\nKnowledge Base Question Answering(LM-KBQA). It ex-\nploits the BERT pre-trained language model embeddings\n[12] ( which eliminates the need for using Recurrent\nNeural Network(RNN) architectures (LSTM, Bi-LSTM)\nto capture the contextual representations); (2) it is based\non CNN encoder with self multi-head attention mechanism\nto ﬁne tune the Bert embedding for the KB-QA task; (3)\nour results demonstrate the eﬀectiveness of the proposed\napproach on the open Web-Questions data-set [3].\nII. Related Work\nOver the past few years we have seen a growing research\non KB-QA, shaping interactive paradigm. This enables\nuser to take advantage of the communicative power of\nlinguistic web knowledge while hiding their complexity\nbehind an easy-to-use interface. At the same time the\nabundance of information has led to a heterogeneous data\nlandscape where QA systems struggle to stay up with\nthe quantity, variety, and truthfulness of the underlying\nknowledge. In general, the most popular methods for KB-\nQA can be mainly divided into two classes: SP-based and\nIR-based.\nSemantic parsing(SP) based approaches focuses on con-\nstructingasemanticparsingtreeorequivalentquerystruc-\nture that represents the semantic meaning of the question.\nIn terms of logical representation of natural language\nquestions, many methods have been implemented, such as\nquery graph [47], [48] or RDF query language [11] [20].\nInformation retrieval(IR) based system try to obtain\ntarget answer directly from question information and\nKB knowledge without explicit considering interior query\nstructure. There are various methods [6], [13], [44], [46] to\nselect candidate answers and rank results.\n[7] Were the pioneers to use embedding-based models to\nresolve the KB-QA problem. The queries and KB triples\nwere represented as vectors in an exceedingly low dimen-\nsional vector space. Subsequently the cosine similarity is\napplied to ﬁnd the most accurate answer. Also the Bag Of\nWords (BOW) methodology [19] is used to generate one\nvector for all the query answers. The Pairwise training\nmethodology is utilized and the negative samples are\nrandomly extracted from the KB. Authors in [5] improved\ntheir work based on sub-graph embeddings which is ca-\npable of answering complex questions. Their main motive\nwas to include the maximum amount of information within\nthe answer context which encodes the surrounding sub-\ngraph of the KB(e.g., answer path and context). The pro-\nposed sub-graph embeddings accommodate all the entities\nand relations that are connected to the answer entity. The\nresulting vector was obtained using BOW [19] represen-\ntations. In follow-up work [6] [21], memory networks [39]\nwere used to store candidate set, and could be iteratively\naccessed to mimic multi-hop reasoning. Unlike the above\nmethods that mainly use a bag-of-words (BOW) approach\nto encode questions and KB contexts, [13], [18] apply\nmore advanced network modules (e.g.,CNNs and LSTMs).\nThe approach proposed in [48] is ﬁxated on single-relation\nqueries. The KB-QA task was sub-divided into 2 stages.\nFirst and foremost, the topic entity of the question was\nfound. Then, the later question was considered by CNNs\nand utilize to match relations. Authors in [13], thought\nof the diverse features of answers to represent questions\nrespectively using 3 columns of CNNs. Authors in [43],\n[44] proposed to use multi-channel CNNs to extract the\nrelations of KB triples while exploiting the free text of the\nWikipedia.\nMost embedding-based approaches encode question and\nKB contexts independently. In NLP, [2] was the ﬁrst to\napply attention model. By cooperatively learning to align\nand translate, they enhanced the encoder-decoder Neural\nMachine Translation (NMT) framework. They argued that\nhaving source sentence by a ﬁxed vector is illogical, and\nproposed a soft-align method, which could be understood\nas attention mechanism. [13] represented questions using\nthree CNNs with diﬀerent parameters when dealing with\ndiﬀerent answer contexts including answer path, answer\ncontext and answer type. With this inspiration [18] pro-\nposed a cross-attention mechanism to encode questions\naccording to various candidate answer aspects. Further\nmore [10] proposed bidirectional attention similar to those\napplied in machine reading comprehension [42] [32] [38]\nby modeling the interactions between questions and KB\ncontexts. We take a stride forward incorporating BERT\n[12] pre-trained language model embeddings to encode\nour question and candidate answer contexts from KB.\nWe ﬁne tuned BERT for the KB-QA problem with a\nMulti-Head Attention mechanism [37] which is based on\na Convolution Neural Network (CNN) [22] encoder. Our\nproposed architecture is also based on the bi-directional\ncross attention mechanism between the asked question and\nKB answer contexts [10].\nIII. Overview\nThe goal of the KB-QA task can be formalized as\nfollows: given a natural language question q, the model\nshould return an entity set A as answers. The general\narchitecture of a KB-QA system is illustrated in Figure\n1. First, the candidate entity of the question is identiﬁed,\nthen the candidate answers are generated from the Free-\nbase [4] knowledge base.\nThe questions are then encoded using pre-trained-Bert\nmodel [12] which is ﬁne tuned with a CNN encoder with\nMulti-Head attention [37] for relationship understanding.\nThen, cross attention neural networks [18] are employed to\nrepresent question under the inﬂuence of candidate answer\nsetaspects(likeentitytypes,relationpaths)andviceversa.\nFinally, the similarity score between the question and each\ncorresponding candidate answer set is calculated, and the\ncandidates with the highest score will be selected as the\nﬁnal answer.\nFig. 1: General Architecture of a KB-QA.\nWe refer to Freebase [4] as our knowledge base.\nIt has more than 3 billion facts, and is used as\nthe supporting knowledge base for many Question\nAnswering systems. In Freebase, the facts are\nrepresented as subject-predicate-object triples (s,p,o).\nFollowing is an example triple in Freebase: (/m/01428,\n/language/human_language/countries_spoken_in,\n/m/03_r3) which relates to the fact that the language\nspoken in the country of Jamaica is Jamaican English\nwhere \"/m/03_r3\" denotes the country Jamaica while\n\"/m/01428\" denotes the language Jamaican English, and\n\"/language/human_language/countries_spoken_in\" is a\nrelationship between the Freebase entities.\nIV. Our Approach\nA. Candidate Generation\nIdeally all entities in the Freebase should be considered\nfor candidate answers, but practically, this is computation-\nally expensive and can be avoided. We use Freebase API\n[4] to ﬁnd a named entity for each question q which acts\nas the primary entity for the given question. For general\nunderstanding, Freebase API methodology resolved 86%\nof questions in open Web-Questions data-set when top-1\naccuracy criteria [46] is used.\nFor example if the question \"what language Jamaican\npeople speak?\" is passed through the Freebase API, it\nreturns \"Jamaica\" as the main entity. After the named\nentity is identiﬁed with Freebase API, we gather all the\nother entities that are directly associated to the named\nentity with in 2-hop. These entities create the candidate\nset.\nB. Question Representation\nInitially, we need to collect the representations for each\nword in the question. These representations contain all the\ninformation of the question, and could serve as follows:\nFor example, the question Q is represented as Q = (x1,\nx2,..., xn) wherexi stands forith word. The input natural\nlanguage question Q = {qi}|Q|\ni=1 as a sequence of word\nembeddings qi ﬁne tuned from Bert pre-trained language\nmodel using an encoder layer.\nC. Context Representation\nFor each candidate set from the KB, we generate context\nfor each question focusing on three aspects: answer type,\nanswer path, and answer context from KB.\nAnswer type contains entity type information and helps\nin narrowing down entities while ranking the answers.\nIf a question uses the word \"who\", the candidates an-\nswers that are relevant to a person are more likely to\nbe correct. Answer path is a sequence of relations from\na candidate set to a named entity. For example \"/lan-\nguage/human_language/countries_spoken_in\" is a rela-\ntion path between Jamaica and Jamaican English which\nis stored as [human, language, countries, spoken, in] in the\nFreebase.\nAnswer context is deﬁned as the surrounding entities\n(e.g., neighbour nodes) of the candidate which help to\nanswer the asked questions with constraints. We only\nconsider the context nodes that have overlap with the\nasked questions. In particular, for each context node (i.e.\na sequence of words) of a candidate, we ﬁrst compute the\nlongest common sub-sequence between the neighbors and\nthe question. If there exists some common sub-sequence\nbetween question and neighbor entities we store them as\nAnswer context which will help in answering questions\nwith multiple entities. All this information is stored as the\ncontext and will be fed into the pre-trained Bert model to\ngenerate embeddings and are ﬁne tuned using an encoder\nlayer.\nD. BERT Language Model\nBERT is a multi-layer bidirectional transformer en-\ncoder. The given input is a token sequence at the\ncharacter-level. This means it is either a single sequence\nor a special token [SEP] separated sentence pair. Each\ntoken in the input sequences is the aggregate of the token\nembeddings, the segment embeddings, and the position\nembeddings. The initial token in every sequence is consis-\ntently a special classiﬁcation symbol ([CLS]), and for the\nclassiﬁcation tasks it uses the hidden state of the token.\nAfter ﬁne tuning, the pre-trained BERT [12] representa-\ntions are used in several natural language processing tasks.\n1) Notation: For an input sequence of word or sub-word\ntokens X = (x1, . . . ,xn ), BERT trains an encoder that\ngenerates a contextualized representations for each token:\nx1, . . . , xn = enc(x1, . . . , xn). Each token in the\nsequence, positional embeddingsp1, . . . ,pn are used to\nlabel the absolute position of input sequence because deep\ntransformer is used to implement the encoder.\n2) Masked Language Modeling (MLM): Masked Lan-\nguage Modeling (MLM) or \"Cloze test\", is used to predict\nthe missing tokens from their placeholders in a given\nsequence. When a subset of tokens Y⊆X sampled and\nsubstituted with placeholder set of tokens.In Bert’s MLM\nimplementation ,Y accounts for 15% of the tokens in\nX; of those, 80% are replaced with [MASK], 10% are\nreplaced with a random token (according to the unigram\ndistribution), and 10% are kept unchanged. The main idea\nis to predict the modiﬁed input from the original tokens\nin Y. BERT selects each token in Y independently by\nrandomly selecting a subset.\n3) Next Sentence Prediction (NSP):The Next Sentence\nPrediction (NSP) predicts whetherXB is immediate con-\ntinuation of XA when two sequences XA and XB are\ngiven as input. In BERT methodology, it initially takes\nXA from corpus. Then it either readsXB from whereXA\nhas terminated or it randomly samplesXB from diﬀerent\npoint in corpus. A special token [SEP] is used to separate\nthe two sequences. Also, a special token [CLS] is appended\nto XA and XB to express as an input. The main idea of\n[CLS]istotrulyﬁndwhether XB followsXA inthecorpus.\nE. Embedding Layer\nTo embed question and KB context we use pre-trained\nBERT [12] language model. Without the need of Recurrent\nArchitecture BERT uses positional embeddings to en-\ncode the word sequences. By using unsupervised learning\nbased on Masked Language Models (MLM) and Next Sen-\ntence Prediction (NSP), BERT embeddings are generated.\nThese embeddings contain bidirectional attention from\nMasked Language Model(MLM) section of BERT.\nBERT model takes sentences in one or two formats. If\nwe have a context of [location, language, human, language,\ncountries,spoken, in, Jamaican] and a question [what does\nJamaican people speak?], we can either encode it as a\nsingle input or two separate inputs to the BERT Model\nas shown below.\n<CLS>[question]<SEP> [context] Or\n<CLS>[question]\n<CLS>[context]\nWe provide Context and Query together with query ﬁrst\nfollowed by the context as shown in the ﬁgure above. Since\nBERT is trained on \"next sentence prediction\" outcome,\nwe believe that using this formulation will provide richer\nquery and context embeddings. The maximum length of\nthe context and the question are 96 and 18 respectively.\nWe use the concatenation of all 12 layers of BERT model\nas our embedding layer.\nF. Encoder Layer\nThe encoder layer is a stack of following layers:\n[CNN-layer + self-attention-layer + feed-forward-layer]\nSimilar to [32], the input of this layer at each position\nis [c, a, c⨀ a, c⨀ b], where a and b are respectively a\nrow of attention matrix A and B. For the self-attention-\nlayer, we adopted multi-head attention mechanism deﬁned\nin [37] which is the improvement of the basic attention\nmechanism.\nheadi = Attention(QWQ\ni , KWK\ni , V WV\ni ) (1)\nMulti −Head(Q, K, V) =Concat(head1,·····, headh) (2)\nSelfMulti −Head = Multi −head(X, X, X) (3)\nThese three equations are the formation process of the\nself multi-head attention mechanism. The matrix of W\nis the weight matrix. The Q, V, K should multiply its\ncorresponding weight matrix before getting into the at-\ntention function. Repeat this process h(number of heads)\ntimes and connect each result then we can get a new\nvector matrix that reﬂects the relationship between Q, V.\nEspecially, in the self multi-head attention mechanism, to\nlook for internal connections within words, the Q = V =\nK = X, and X represents the word vector matrix.\nThe multi-head attention mechanism helps the model\nlearn the words relevant information in diﬀerent presenta-\ntion sub-spaces. The self-attention mechanism can extract\nthe dependence in words. As the name shows, the self\nmulti-head attention mechanism integrates the beneﬁts of\nboth, creates a context vector for each word. Then we\ndon’t need to depend on additional information to get\na matrix that reﬂects the context relationship between\ncurrent word and other words in a sequence. Each of these\nbasic operations (cnn/self-attention/ﬀn) is placed inside a\nresidual block\nFig. 2: BERT Embedding Layer\nG. Attention Layer\nWe use a focused, Context-Query attention layer on\ntop of the pre-trained BERT embeddings identical to that\nof the QANet [50] model. Such attention modules were\nstandard in other KB-QA models such as [18] [10].\nC is used to denote context from KB and Q for natu-\nral language question. The context-to-query attention is\nconstructed as follows:(1) Compute similarities between\neach pair of context and query words that generates a\nsimilarity matrix S ∈Rn∗m. (2) Normalize each row of\nS by applying soft-max function. (3) Compute context-to-\nquery attention A = S·QT∈Rn∗d\nThe similarity function used here is the tri-linear func-\ntion as mentioned in [32] : f(q, c) =W0[q, c, q⨀ c]\nV. Experiments\nA. Dataset\nWe use the Web-Questions [3] dataset. Web-Questions\ndataset is built by using the Google Suggest API to obtain\nquestions that begin with a wh-word and contain exactly\none entity. Speciﬁcally, they queried the question exclud-\ning the entity, the phrase before the entity, or the phrase\nafter it. Each query generates 5 candidate questions, which\nare added to the queue. Further they iterated until\n1 million questions were visited; a random 100K were\nsubmitted to Amazon Mechanical Turk (AMT). The AMT\ntask requested that the distributed workforce should an-\nswer the question using only the Freebase page of the\nquestions’ entity, or otherwise mark it as unanswerable\nby Freebase. The answer was restricted to be one of the\npossible entities, values, or list of the entities on the page.\nThus, this combination of Web-Questions dataset with\nFreebase KB is used as the base line.\nB. Comparison with Sate of the art\nTo assess the proposed methodology, experiments were\nconducted on the Freebase KB and the Web-Questions [3]\ndataset. Freebase is large-scale KB [4] that has organised\ngeneral facts as subject-predicate-object triples. It has\n41M non-numeric entities with 19K unique properties and\n596M assertions.The Web-Questions [3] dataset has 3,778\nquestion-answer pairs for training and 2,032 for testing.\nThe questions are gathered from Google Suggest API,\nand the answers from Amazon Mechanical Turk which are\nlabelled manually. All of the answers are from the Freebase\nKB. we use 80% of the training data as training set and\n20% as validate set. The evaluation metric we use is F1\nscore, and the average result is calculated by [3] script.\n1) Settings: For KB-QA training, we use pre-trained\nBert embeddings base uncased version. During tokeniza-\ntion, BERT [12] code uses a word-piece algorithm to split\nwords into sub-words and all less frequent words will be\nsplit into two or more sub-words. The vocab size of Bert\nis 30522. We adopt delexicalization strategy as mentioned\nin [10]. For each question, the candidate entity mentions\nthose belonging to date, ordinal, or number are replaced\nwith their type. Same is applied on answer context from\nKB text, if the overlap belongs to above type. This assures\nthat the query matches up with answer context in the\nembedding space. The dropout rates for both question and\nanswer encoder side is set to 0.3. The bath size is set as 4\nand answer module threshold is set to 0.7 to allow multiple\nanswers for questions with list of answers. We use Adam\noptimizer [23] to train the model. The initial learning rate\nis set to 0.01. Further learning rate is reduced by a factor\nof 10 if no improvement is observed in validation process\nfor 3 successive epochs. The hyper-parameters are tuned\non validation-set.\n2) Results: In this section, we compare the perfor-\nmance of our method with other IR-based approaches.\nThe results are shown in Table 1. Based on this table\nour method (LMKB-QA) obtained an F1 score of 52.7\non Web-Questions using the topic entity predicted by\nFreebase API. As Table 1 shows our method achieves\nbetter results or even competes with state-of-the-arts.\nThis demonstrate the eﬀectiveness of our idea to use Bert\npre-trained language model embeddings in the Question\nAnswering on Knowledge Base problem.\nIt is important to note that our proposed approach is\nbased on a CNN architecture, and only depends on the\ntraining data (it does not depends on wiki text [43]).\n[7] applies BOW method to obtain a single vector for\nMethods Avg F1\nBordes 2014b [7] 29.7\nBordes 2014a [5] 39.2\nYang 2014 [45] 41.3\nDong 2015 [13] 40.8\nBordes 2015 [6] 42.2\nXu 2016 [44] 42.2\nHao 2017 [18] 42.2\nChen 2019 [10] 49.7\nChen 2019 [10] topic entity predictor 51.8\nOur Approach 52.7\nTable I: Evaluation results on Web-Questions\nboth questions and answers. [5] further improve their\nwork by proposing the concept of sub-graph embeddings.\nBesides the answer path, the sub-graph contains all the\nentities and relations connected to the answer entity. The\nﬁnal vector is also obtained by bag-of-words strategy. [45]\nfollows the SP-based manner, but uses embeddings to\nmap entities and relations into KB resources, then the\nquestion can be converted into logical forms. [13] use\nthree columns of Convolution Neural Networks (CNNs)\nto represent questions corresponding to three aspects of\nthe answers, namely the answer context, the answer path\nand the answer type. [6] put KB-QA into the memory\nnetworks framework [35], and achieves the state-of-the-art\nperformance of end-to-end methods. Our approach incor-\nporates pre-trained Bert language model embeddings ﬁne\ntuned for the KB-QA task using a Encoder architecture\nwith multi head attention. [5]–[7] all utilize BOW model\nto represent the questions, while ours takes advantage of\npre-trained language model embeddings. Also note that [6]\nuses additional training data such as Reverb [14] and their\noriginal data set Simple-Questions.\nThe proposed method in [13] employs three ﬁxed CNNs\nto represent questions. [18] implemented the mutual in-\nﬂuence between the representation of questions and the\ncorresponding answer aspects. [10] further enhanced the\nresult by using memory networks [39] to control the\nmutual inﬂuence between the representation of questions\nand the corresponding answer aspects. The approach in\n[10] achieved an F1 score of 0.518 using a custom topic\nentity predictor while an F1 score of 0.497 has achieved\nvia the Freebase Search API. The [47], [49] have higher\nF1 scores than other methods. Their approach was able to\naddress more questions with constraints and aggregations.\nBut, their methods apply number of manually designed\nrules and features, which come from the observations\non the training questions set. This is a manual process\nwhich reduces the versatility of their proposed approach.\nIntegrated systems like [43], [44] generates higher F1 score\nleveraging Wikipedia free text as external knowledge,\nwhich are based on semantic parsing. Therefore the sys-\ntems are not directly compared to ours.\nVI. Conclusion\nIn this paper, we focused on using a pre-trained lan-\nguage model for KB-QA task. Firstly, we usedBert base\nuncased for the initial experiments. We further ﬁne tuned\nthese embeddings with two way attention mechanism from\nthe knowledge base to the asked question and from the\nasked question to the knowledge base answer aspects. Our\nmethod is based on a simple CNN architecture with Multi-\nHeadAttentionmechanismtorepresenttheaskedquestion\ndynamically in the multiple aspects. Our experimental\nresults show the eﬀectiveness of the Bert pre-trained lan-\nguage model embeddings.\nIn the future, we would like to explore other language\nmodels including GPT-2, RoBERTa, Transformer-XL and\nXLNet. We will also further investigate to answer even\nmore complex questions on the knowledge bases.\nVII. Acknowledgements\nWe would like to thank Hugging Face [41] for their\nopen-source code to use pretrained BERT embeddings for\ndownstream tasks using PyTorch, BAMnet [10] for open-\nsource code to process Freebase knowledge base dump,\nand Web-Questions [3] for the train, test dataset, and F1\nmetric script for comparison.\nReferences\n[1] Hiyan Alshawi and Jan van Eijck. Logical forms in the core\nlanguage engine. In27th Annual Meeting of the Association for\nComputational Linguistics, pages 25–32, 1989.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neu-\nral machine translation by jointly learning to align and trans-\nlate. arXiv preprint arXiv:1409.0473, 2014.\n[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.\nSemantic parsing on Freebase from question-answer pairs. In\nProceedings of the 2013 Conference on Empirical Methods in\nNatural Language Processing, pages 1533–1544, Seattle, Wash-\nington, USA, October 2013. Association for Computational\nLinguistics.\n[4] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge,\nand Jamie Taylor. Freebase: a collaboratively created graph\ndatabaseforstructuringhumanknowledge. In Proceedingsofthe\n2008 ACM SIGMOD international conference on Management\nof data, pages 1247–1250. AcM, 2008.\n[5] Antoine Bordes, Sumit Chopra, and Jason Weston. Ques-\ntion answering with subgraph embeddings. arXiv preprint\narXiv:1406.3676, 2014.\n[6] Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason\nWeston. Large-scale simple question answering with memory\nnetworks. arXiv preprint arXiv:1506.02075, 2015.\n[7] AntoineBordes,JasonWeston,andNicolasUsunier. Openques-\ntion answering with weakly supervised embedding models. In\nJoint European conference on machine learning and knowledge\ndiscovery in databases, pages 165–180. Springer, 2014.\n[8] François Bry, Paula-Lavinia Pătrânjan, and Sebastian Schaﬀert.\nXcerptandxchange–logicprogramminglanguagesforquerying\nand evolution on the web. In Bart Demoen and Vladimir\nLifschitz, editors, Logic Programming, pages 450–451, Berlin,\nHeidelberg, 2004. Springer Berlin Heidelberg.\n[9] Brice Chardin, Emmanuel Coquery, Marie PAILLOUX, and\nJean-Marc Petit. Rql: A sql-like query language for discovering\nmeaningful rules. volume 2014, pages 1203–1206, 12 2014.\n[10] Yu Chen, Lingfei Wu, and Mohammed J Zaki. Bidirectional at-\ntentive memory networks for question answering over knowledge\nbases. arXiv preprint arXiv:1903.02188, 2019.\n[11] Wanyun Cui, Yanghua Xiao, Haixun Wang, Yangqiu Song,\nSeung-won Hwang, and Wei Wang. Kbqa: learning question\nanswering over qa corpora and knowledge bases.Proceedings of\nthe VLDB Endowment, 10(5):565–576, 2017.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert:Pre-trainingofdeepbidirectionaltransformers\nfor language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[13] Li Dong, Furu Wei, Ming Zhou, and Ke Xu. Question answering\nover Freebase with multi-column convolutional neural networks.\nIn Proceedings of the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Volume 1: Long\nPapers), pages 260–269, Beijing, China, July 2015. Association\nfor Computational Linguistics.\n[14] Anthony Fader, Stephen Soderland, and Oren Etzioni. Identify-\ning relations for open information extraction. InProceedings of\nthe 2011 Conference on Empirical Methods in Natural Language\nProcessing, pages 1535–1545, Edinburgh, Scotland, UK., July\n2011. Association for Computational Linguistics.\n[15] Kunihiko Fukushima. Neocognitron: A hierarchical neural net-\nwork capable of visual pattern recognition. Neural networks,\n1(2):119–130, 1988.\n[16] Alex Graves, Santiago Fernández, and Jürgen Schmidhuber.\nBidirectional lstm networks for improved phoneme classiﬁcation\nand recognition. InProceedings of the 15th International Con-\nference on Artiﬁcial Neural Networks: Formal Models and Their\nApplications-VolumePartII ,ICANN’05,page799–804,Berlin,\nHeidelberg, 2005. Springer-Verlag.\n[17] Yanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He, Zhanyi\nLiu, Hua Wu, and Jun Zhao. An end-to-end model for question\nanswering over knowledge base with cross-attention combining\nglobal knowledge. InProceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long\nPapers), pages 221–231, 2017.\n[18] Yanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He, Zhanyi\nLiu, Hua Wu, and Jun Zhao. An end-to-end model for question\nanswering over knowledge base with cross-attention combining\nglobal knowledge. In Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Volume 1:\nLong Papers), pages 221–231, Vancouver, Canada, July 2017.\nAssociation for Computational Linguistics.\n[19] ZelligSHarris. Distributionalstructure. Word,10(2-3):146–162,\n1954.\n[20] Sen Hu, Lei Zou, Jeﬀrey Xu Yu, Haixun Wang, and Dongyan\nZhao. Answering natural language questions by subgraph\nmatchingoverknowledgegraphs. IEEETransactionsonKnowl-\nedge and Data Engineering, 30(5):824–837, 2017.\n[21] Sarthak Jain. Question answering over knowledge base using\nfactualmemorynetworks. In ProceedingsoftheNAACLStudent\nResearch Workshop, pages 109–115, 2016.\n[22] Yoon Kim. Convolutional neural networks for sentence classiﬁ-\ncation. arXiv preprint arXiv:1408.5882, 2014.\n[23] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.arXiv preprint arXiv:1412.6980, 2014.\n[24] Graham Klyne, Jeremy J Carroll, and B McBride. Resource\ndescription framework (rdf): concepts and abstract syntax,\n2004. February. URL: http://www. w3. org/TR/2004/REC-\nrdf-concepts-20040210, 2009.\n[25] Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dim-\nitris Kontokostas, Pablo N Mendes, Sebastian Hellmann, Mo-\nhamed Morsey, Patrick Van Kleef, Sören Auer, et al. Dbpedia–\na large-scale, multilingual knowledge base extracted from\nwikipedia. Semantic Web, 6(2):167–195, 2015.\n[26] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gard-\nner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\nDeep contextualized word representations. arXiv preprint\narXiv:1802.05365, 2018.\n[27] Alec Radford, Karthik Narasimhan, Tim Sali-\nmans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf, 2018.\n[28] Alec Radford, Jeﬀrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsupervised\nmultitask learners.OpenAI Blog, 1(8):9, 2019.\n[29] Ivan A. Sag. Linguistic theory and natural language processing.\nIn Ewan Klein and Frank Veltman, editors,Natural Language\nand Speech, pages 69–83, Berlin, Heidelberg, 1991. Springer\nBerlin Heidelberg.\n[30] Hasim Sak, Andrew W Senior, and Françoise Beaufays. Long\nshort-term memory recurrent neural network architectures for\nlarge scale acoustic modeling. 2014.\n[31] Andy Seaborne, Geetha Manjunath, Chris Bizer, John Bres-\nlin, Souripriya Das, Ian Davis, Steve Harris, Kingsley Idehen,\nOlivier Corby, Kjetil Kjernsmo, et al. Sparql/update: A lan-\nguage for updating rdf graphs. W3c member submission, 15,\n2008.\n[32] Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh\nHajishirzi. Bidirectional attention ﬂow for machine comprehen-\nsion. arXiv preprint arXiv:1611.01603, 2016.\n[33] Amit Singhal. Introducing the knowledge graph: things, not\nstrings. Oﬃcial google blog, 16, 2012.\n[34] Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum.\nYago: A core of semantic knowledge. InProceedings of the 16th\nInternational Conference on World Wide Web, WWW ’07, page\n697–706, New York, NY, USA, 2007. Association for Computing\nMachinery.\n[35] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-\nto-end memory networks. In Advances in neural information\nprocessing systems, pages 2440–2448, 2015.\n[36] Christina Unger, André Freitas, and Philipp Cimiano. An intro-\nduction to question answering over linked data. InReasoning\nWeb International Summer School, pages 100–140. Springer,\n2014.\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\nLlion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polo-\nsukhin. Attention is all you need.CoRR, abs/1706.03762, 2017.\n[38] Shuohang Wang and Jing Jiang. Machine comprehen-\nsion using match-lstm and answer pointer. arXiv preprint\narXiv:1608.07905, 2016.\n[39] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory\nnetworks. arXiv preprint arXiv:1410.3916, 2014.\n[40] Terry Winograd. Language as a cognitive process: Volume 1:\nSyntax. 1983.\n[41] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,\nClement Delangue, Anthony Moi, Pierric Cistac, Tim Rault,\nR’emi Louf, Morgan Funtowicz, and Jamie Brew. Hugging-\nface’s transformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771, 2019.\n[42] Caiming Xiong, Victor Zhong, and Richard Socher. Dynamic\ncoattention networks for question answering. arXiv preprint\narXiv:1611.01604, 2016.\n[43] Kun Xu, Yansong Feng, Songfang Huang, and Dongyan Zhao.\nHybrid question answering over knowledge base and free text.\nIn Proceedings of COLING 2016, the 26th International Con-\nference on Computational Linguistics: Technical Papers, pages\n2397–2407, Osaka, Japan, December 2016. The COLING 2016\nOrganizing Committee.\n[44] Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang, and\nDongyan Zhao. Question answering on Freebase via relation\nextraction and textual evidence. InProceedings of the 54th An-\nnual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 2326–2336, Berlin, Germany,\nAugust 2016. Association for Computational Linguistics.\n[45] Min-Chul Yang, Nan Duan, Ming Zhou, and Hae-Chang Rim.\nJoint relational embeddings for knowledge-based question an-\nswering. In Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 645–\n650, Doha, Qatar, October 2014. Association for Computational\nLinguistics.\n[46] Xuchen Yao and Benjamin Van Durme. Information extraction\nover structured data: Question answering with freebase. In\nProceedings of the 52nd Annual Meeting of the Association for\nComputationalLinguistics(Volume1:LongPapers) ,pages956–\n966, 2014.\n[47] Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng\nGao. Semantic parsing via staged query graph generation:\nQuestion answering with knowledge base. In Proceedings of\nthe 53rd Annual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers), pages 1321–\n1331, Beijing, China, July 2015. Association for Computational\nLinguistics.\n[48] Wen-tau Yih, Xiaodong He, and Christopher Meek. Semantic\nparsing for single-relation question answering. InProceedings of\nthe 52nd Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 643–648, Balti-\nmore, Maryland, June 2014. Association for Computational\nLinguistics.\n[49] Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-Wei\nChang, and Jina Suh. The value of semantic parse labeling for\nknowledge base question answering. InProceedings of the 54th\nAnnual Meeting of the Association for Computational Linguis-\ntics (Volume 2: Short Papers), pages 201–206, Berlin, Germany,\nAugust 2016. Association for Computational Linguistics.\n[50] Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao,\nKai Chen, Mohammad Norouzi, and Quoc V Le. Qanet: Com-\nbining local convolution with global self-attention for reading\ncomprehension. arXiv preprint arXiv:1804.09541, 2018.\n[51] Luke S Zettlemoyer and Michael Collins. Learning context-\ndependent mappings from sentences to logical form. In Pro-\nceedings of the Joint Conference of the 47th Annual Meeting of\nthe ACL and the 4th International Joint Conference on Natural\nLanguage Processing of the AFNLP: Volume 2-Volume 2, pages\n976–984. Association for Computational Linguistics, 2009.\n[52] Luke S Zettlemoyer and Michael Collins. Learning to map\nsentences to logical form: Structured classiﬁcation with prob-\nabilistic categorial grammars.arXiv preprint arXiv:1207.1420,\n2012.",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.8859162330627441
    },
    {
      "name": "Knowledge base",
      "score": 0.8472548723220825
    },
    {
      "name": "Computer science",
      "score": 0.8370869159698486
    },
    {
      "name": "Natural language processing",
      "score": 0.5637398958206177
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5505700707435608
    },
    {
      "name": "Embedding",
      "score": 0.5382418632507324
    },
    {
      "name": "Knowledge-based systems",
      "score": 0.5335341691970825
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.5236206650733948
    },
    {
      "name": "Open Knowledge Base Connectivity",
      "score": 0.5225855112075806
    },
    {
      "name": "Vocabulary",
      "score": 0.5207371711730957
    },
    {
      "name": "Natural language",
      "score": 0.5093997120857239
    },
    {
      "name": "Domain knowledge",
      "score": 0.5088980197906494
    },
    {
      "name": "Entity linking",
      "score": 0.4577227532863617
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4406730532646179
    },
    {
      "name": "Information retrieval",
      "score": 0.40901607275009155
    },
    {
      "name": "Personal knowledge management",
      "score": 0.19100195169448853
    },
    {
      "name": "Knowledge management",
      "score": 0.16001832485198975
    },
    {
      "name": "Linguistics",
      "score": 0.13922268152236938
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Organizational learning",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I110378019",
      "name": "Southern Illinois University Carbondale",
      "country": "US"
    }
  ],
  "cited_by": 15
}