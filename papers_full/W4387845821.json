{
  "title": "A lightweight hybrid vision transformer network for radar-based human activity recognition",
  "url": "https://openalex.org/W4387845821",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2108904852",
      "name": "Sha Huan",
      "affiliations": [
        "Guangdong University of Education",
        "Guangzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2120598316",
      "name": "Zhaoyue Wang",
      "affiliations": [
        "Guangzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2105576756",
      "name": "Xiaoqiang Wang",
      "affiliations": [
        "Naval University of Engineering"
      ]
    },
    {
      "id": "https://openalex.org/A2130128548",
      "name": "Limei Wu",
      "affiliations": [
        "Guangzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2123093938",
      "name": "Xiaoxuan Yang",
      "affiliations": [
        "Guangzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2113318334",
      "name": "Hongming Huang",
      "affiliations": [
        "Guangzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A5101258511",
      "name": "Gan E. Dai",
      "affiliations": [
        "Foshan University"
      ]
    },
    {
      "id": "https://openalex.org/A2108904852",
      "name": "Sha Huan",
      "affiliations": [
        "Guangdong University of Education",
        "Guangzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2120598316",
      "name": "Zhaoyue Wang",
      "affiliations": [
        "Guangzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2105576756",
      "name": "Xiaoqiang Wang",
      "affiliations": [
        "Naval University of Engineering"
      ]
    },
    {
      "id": "https://openalex.org/A2130128548",
      "name": "Limei Wu",
      "affiliations": [
        "Guangzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2123093938",
      "name": "Xiaoxuan Yang",
      "affiliations": [
        "Guangzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2113318334",
      "name": "Hongming Huang",
      "affiliations": [
        "Guangzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A5101258511",
      "name": "Gan E. Dai",
      "affiliations": [
        "Foshan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2954377489",
    "https://openalex.org/W2566887294",
    "https://openalex.org/W2909218472",
    "https://openalex.org/W4320801601",
    "https://openalex.org/W3174770113",
    "https://openalex.org/W2118368846",
    "https://openalex.org/W2157770256",
    "https://openalex.org/W2165913693",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4211158710",
    "https://openalex.org/W3186583119",
    "https://openalex.org/W3153961130",
    "https://openalex.org/W3039610325",
    "https://openalex.org/W3129895333",
    "https://openalex.org/W4212831312",
    "https://openalex.org/W3134230466",
    "https://openalex.org/W3004500412",
    "https://openalex.org/W4312705335",
    "https://openalex.org/W2984834466",
    "https://openalex.org/W4394668313",
    "https://openalex.org/W3137278571",
    "https://openalex.org/W4286910290",
    "https://openalex.org/W4287025584",
    "https://openalex.org/W4292387633",
    "https://openalex.org/W4327714852",
    "https://openalex.org/W4297775537",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W2506886870",
    "https://openalex.org/W2973122072",
    "https://openalex.org/W2304648132",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W2400429454",
    "https://openalex.org/W3133954504",
    "https://openalex.org/W4378528421",
    "https://openalex.org/W4308336036",
    "https://openalex.org/W2010838375",
    "https://openalex.org/W2724359148",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2963608065",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W4362564496",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W2963504571"
  ],
  "abstract": "Abstract Radar-based human activity recognition (HAR) offers a non-contact technique with privacy protection and lighting robustness for many advanced applications. Complex deep neural networks demonstrate significant performance advantages when classifying the radar micro-Doppler signals that have unique correspondences with human behavior. However, in embedded applications, the demand for lightweight and low latency poses challenges to the radar-based HAR network construction. In this paper, an efficient network based on a lightweight hybrid Vision Transformer (LH-ViT) is proposed to address the HAR accuracy and network lightweight simultaneously. This network combines the efficient convolution operations with the strength of the self-attention mechanism in ViT. Feature Pyramid architecture is applied for the multi-scale feature extraction for the micro-Doppler map. Feature enhancement is executed by the stacked Radar-ViT subsequently, in which the fold and unfold operations are added to lower the computational load of the attention mechanism. The convolution operator in the LH-ViT is replaced by the RES-SE block, an efficient structure that combines the residual learning framework with the Squeeze-and-Excitation network. Experiments based on two human activity datasets indicate our method’s advantages in terms of expressiveness and computing efficiency over traditional methods.",
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2023) 13:17996  | https://doi.org/10.1038/s41598-023-45149-5\nwww.nature.com/scientificreports\nA lightweight hybrid vision \ntransformer network \nfor radar‑based human activity \nrecognition\nSha Huan 1,2, Zhaoyue Wang 1, Xiaoqiang Wang 3*, Limei Wu 1, Xiaoxuan Yang 1, \nHongming Huang 1 & Gan E. Dai 4\nRadar‑based human activity recognition (HAR) offers a non‑contact technique with privacy protection \nand lighting robustness for many advanced applications. Complex deep neural networks demonstrate \nsignificant performance advantages when classifying the radar micro‑Doppler signals that have \nunique correspondences with human behavior. However, in embedded applications, the demand for \nlightweight and low latency poses challenges to the radar‑based HAR network construction. In this \npaper, an efficient network based on a lightweight hybrid Vision Transformer (LH‑ViT) is proposed \nto address the HAR accuracy and network lightweight simultaneously. This network combines the \nefficient convolution operations with the strength of the self ‑attention mechanism in ViT. Feature \nPyramid architecture is applied for the multi‑scale feature extraction for the micro‑Doppler map. \nFeature enhancement is executed by the stacked Radar‑ViT subsequently, in which the fold and unfold \noperations are added to lower the computational load of the attention mechanism. The convolution \noperator in the LH‑ViT is replaced by the RES‑SE block, an efficient structure that combines the \nresidual learning framework with the Squeeze‑and‑Excitation network. Experiments based on two \nhuman activity datasets indicate our method’s advantages in terms of expressiveness and computing \nefficiency over traditional methods.\nHuman activity recognition (HAR) has huge potential for numerous applications, such as intelligent healthcare, \nsmart homes, intelligent security, and autonomous driving. In recent years, HAR data sources have been cat -\negorized into two groups: visual-based HAR and non-visual sensor-based  HAR1.Visual-based  HAR2 analyzes \nhuman motion using video or photos acquired by optical cameras, whereas non-visual sensor-based HAR collects \ndata using smart  sensors3 such as gyroscopes, accelerometers, and radars. Millimeter-wave radar can adapt to \ndifferent weather and lighting conditions with low power consumption and privacy protection. Considerable \nattention has been paid to HAR technology based on millimeter-wave  radar4,5.\nTime-varying kinematic information integrating human  motion6 can be investigated by analyzing and pro-\ncessing millimeter-wave radar echo signals, and activity recognition may be carried out utilizing the resulting \nkinematic information. Radar-based HAR is usually based on the micro-Doppler feature of target echoes. Micro-\nDoppler features from the time-Doppler graph can highlight the self-vibration and rotation of the human’s torso \nand limbs. Based on the clear and unique correspondence between the micro-Doppler features and human \nbehaviour, supervised learning methods are usually used for radar-based HAR. However, HAR methods with \nhigh accuracy and embeddable potential are facing challenges and it is worthwhile devoting much effort to this.\nTraditional classification techniques such as multi-layer perceptron, principal component analysis (PCA), \nsupport vector machines (SVM)7 and linear discriminant analysis are used in some research. Manually extracted \nmicro-Doppler characteristics are typically employed as classification  inputs8–10. Prior knowledge and the intri-\ncacy of the categorization task restrict the use of these characteristics. Deep learning has been steadily advanc-\ning in recent years, its excellent categorization performance has also garnered considerable attention. Radar-\nbased HAR research has gotten more intelligent due to the in-corporation of deep learning (DL) techniques. \nOPEN\n1School of Electronics and Communication Engineering, Guangzhou University, Guangzhou 510006, \nChina. 2Key Laboratory of On-Chip Communication and Sensor Chip of Guangdong Higher Education Institutes, \nGuangzhou 510006, China. 3College of Naval Architecture and Ocean Engineering, Naval University of Engineering, \nWuhan 430033, China. 4School of Electronic Information Engineering, Foshan University, Foshan 528225, \nChina. *email: wxq_nue@126.com\n2\nVol:.(1234567890)Scientific Reports |        (2023) 13:17996  | https://doi.org/10.1038/s41598-023-45149-5\nwww.nature.com/scientificreports/\nConvolutional neural networks (CNN) 11, re-current neural networks (RNN) 12,  transformers13, and hybrid \n networks14 are the four broad classifications of DL techniques. These methods use supervised learning to auto-\nmatically extract sample features, hence overcoming the limitations of conventional models for feature extraction. \nUsing recursive neural networks, time-series models can extract temporal correlation characteristics between \ndata sequences. Numerous studies have demonstrated that adding long short-term memory (LSTM) 15 and Bi-\ndirectional long short-term memory (BiLSTM)16 architectures to a network can effectively enhance HAR’s rec-\nognition performance. Furthermore, LSTM and BiLSTM was combined to achieve  HAR17. Multi-layer  BiLSTM18 \nwas used to classify human activities with an average accuracy of around 90%. However, the large number \nof parameters of the networks above will be a computational burden in embedded applications. Lightweight \n CNN19,20 was utilized to reduce the number of parameters and improve running performance substantially, but \nat the cost of missing some details, resulting in a decline in recognition accuracy.\nBy combining the strengths of the constituent networks, hybrid networks such as CNN-LSTM21 can outper-\nform single networks. The spatiotemporal features of the input data can be completely exploited by this hybrid \nstructure and improve recognition precision. Inspired by the attention process, researchers have combined \nattention modules with neural networks for various  purposes22,23. Typically, attention modules are not utilized \nalone, they are incorporated into various neural networks to increase network performance. Attention methods \nwere added into residual networks, convolutional auto-encoders and LSTM. Networks with attention mecha-\nnisms achieve faster convergence and greater recognition accuracy. Attention typically avoids the problem of \ndisappearing gradients because it provides direct links between all data time steps. In contrast to convolutional \nneural networks, which must preserve spatial locality in the input data, self-attention mechanisms can process \ndata at any place in the input sequence. This increases the generalizability of the network while processing radar \nimages of various sizes, shapes, and orientations.\nThe Transformer network drops the usual neural network calculation method in favour of self-attention \nmethods for network calculation. The vision transformer (ViT) 24 has performed extraordinarily well in the \nfield of vision because of its usage of attention mechanisms. However, the majority of ViT networks include a \nlarge number of parameters and are challenging to implement in embedded applications. In recent years, some \nlightweight  ViT25–27 structures were proposed to reduce the number of parameters while maintaining precision. \nHowever, more in-depth work on the lightweight of ViT is worth looking forward to.\nConsidering the embedded application background of radar-based HAR, some work has attempted to solve \nthe efficiency and performance  issues28,29, but new networks need to be developed to improve the recognition \nperformance on the lightweight structures more effectively. To achieve high-accuracy HAR, this paper devel-\noped a lightweight hybrid Vision Transformer (LH-ViT) network. The network uses the residual structure joint \nSqueeze-and-Excitation (SE) module (RES-SE) block to form a feature pyramid for HAR feature extraction at \ndifferent scales. The following stacked RadarViT networks are designed to enhance useful features through self-\nattention. The radar data in different bands verify that LH-ViT can achieve efficient HAR at different Doppler \nscales. Moreover, the LH-ViT employs depthwise separable convolution and lightweight attention models, which \ngreatly reduce the parameter count compared to conventional ViT while maintaining the same level of accuracy.\nThe contributions of our research are summarized as follows:\n(1) We developed a novel lightweight hybrid Vision Transformer (LH-ViT) in this paper. LH-ViT combines \na feature extraction network with a pyramid structure and a feature enhancement network consisting of \nstacked Radar-ViT components. The primary innovation of LH-ViT lies in its ability to enhance the repre-\nsentational power of radar-based HAR effectively by incorporating spatial attention into the micro-Doppler \nfeature hierarchy. We conducted an in-depth investigation to optimize the structure of this proposed net-\nwork. Furthermore, we conducted a comprehensive comparison of LH-ViT with several state-of-the-art \nHAR networks, using both our self-established dataset and a publicly available dataset” .\n(2) An efficient RES-SE block is designed to replace the traditional convolution operator. Operating within \na residual learning framework, the RES-SE module employs depthwise separable convolutions to extract \nmicro-Doppler features with reduced computational overhead. The lightweight SE module is inserted in \nthe RES-SE block, which adaptively adjusts feature channel weights for enhanced representation accuracy.\n(3) Radar-ViT is developed as a lightweight design of ViT, which enables embedded applications of trans-\nformer-based models. Radar-ViT simplifies the traditional class token module to a point-wise convolution. \nAdditionally, we introduce fold and unfold operations to reduce the computational demands of the multi-\nhead attention block, prioritizing essential micro-Doppler features. Stacked Radar-ViTs excel at capturing \nglobal features on the micro-Doppler map, resulting in superior HAR performance.\nThe remainder of the paper is organized as follows. Section “ Radar-based HAR with LH-ViT ”  introduces \nthe structure and key modules of the proposed LH-ViT network. Section “ Experimental results” provides the \nexperimental findings of two datasets to validate the proposed algorithm’s superiority. Finally, Section “Conclu-\nsion” presents the conclusions.\nRadar‑based HAR with LH‑ViT\nFigure 1 shows the framework of radar-based HAR with LH-ViT in this section. The millimeter wave radar \ncollects the echo from the moving human body and outputs multi-channel intermediate frequency signals after \ndechirp processing. The multi-channel intermediate frequency signals are first preprocessed with 2D FFT. 2D FFT \nprocessing compresses the signal energy at the corresponding position on the range-angle plane. A phase average \ncancellation  method29 is then utilized for the static clutter suppression, which will preserve the micro-Doppler \nsignal components. Two-dimension constant false alarm rate (2D-CFAR) is applied to detect the target against \n3\nVol.:(0123456789)Scientific Reports |        (2023) 13:17996  | https://doi.org/10.1038/s41598-023-45149-5\nwww.nature.com/scientificreports/\nthe noise background. After the target bin is locked, the target bins on the range-angle plane from each frame \nare combined into a slow-time vector. This vector is transformed by the short-time Fourier transform (STFT) \nto generate MDM that reflects the target’s motion in the time-frequency domain. The normalized MDM is fed \ninto the subsequent LH-ViT network for high-efficiency HAR. The LH-ViT is composed of a feature extraction \nnetwork, a feature enhancement network, and a classification module. Maximum pooling and linear layers are \nused in the classification module to output the prediction results. The specific implementation of the first two \nnetworks will be introduced in the following subsections.\nFeature extraction network\nA pyramid structure is adopted in the feature extraction network, as shown in Fig.  2. The feature pyramid can \ncapture the multi-scale micro-Doppler feature on the MDM. Especially when the Doppler range is relatively \nlarge and the micro-Doppler expressions are compressed, the network can still learn the activity features from \nthe MDM accurately and effectively. In terms of a specific implementation, each layer of the pyramid uses a pair \nof RES-SE modules to achieve efficient feature extraction. In each layer, the first RES-SE module is used for the \nmicro-Doppler feature extraction at the current scale, and the second RES-SE module realizes upsampling by \nadjusting the stride value.\nThe RES-SE module applies a residual network structure to achieve feature fusion at different levels. Two \nbranches are added, performing 1 × 1 convolution joint BN operation, and only BN operation respectively. The \nbackbone of RES-SE uses 1 × 1 convolution for dimension expansion and then uses 3 × 3 Depthwise separable \nconvolution (DSC)30 for first-level feature extraction. DSC is an effective approach for the lightweight design of \nstandard convolution operations. DSC improves on the standard convolution by decomposing it into depthwise \nconvolution and point convolution. As a representative of a lightweight network, DSC can achieve feature extrac-\ntion with lower parameter amounts and computational costs. Subsequently, an SE  Block31 based on a light-weight \nchannel attention mechanism is used to process the output of DSC, as shown in Fig. 3.\nThe output of DSC is the local spatial correlation obtained by the 2D spatial kernel. The channel dependencies \nare implicitly embedded in each channel of the DSC output, entangled with the spatial features. The SE block \nFeature\nPyramid\nFeature\nExtraction\nRadar\nPreprecessing\nClassifi-\ncation\nData\nAcquisition Simulation\nRES-SE\nBlock\n2D FFT Stacked\nRadar-ViT\nFeature\nEnhancement\nRadar-ViT\nFold-unFold\nIF Data\nCollection\nMicro-Doppler\nMap\n2D-CFAR\nSTFT\nClutter\nSuppression\nFigure 1.  The framework of radar-based HAR with LH-ViT.\nFigure 2.  Feature extraction network structure diagram.\n4\nVol:.(1234567890)Scientific Reports |        (2023) 13:17996  | https://doi.org/10.1038/s41598-023-45149-5\nwww.nature.com/scientificreports/\nachieves inter-channel attention in a lightweight structure by explicitly modeling the channel dependencies, \nthereby enhancing the feature sensitivity in the channel dimension.\nFirst, the squeeze module uses global average pooling to arrogate each 2D channel into a channel descriptor.\nxdsc is denoted as the DSC output with H× W spatial dimension. As the channel descriptor, X sq is a statistical \nparameter, which represents the aggregated feature of the current channel. All the X sq are processed through a \nbottleneck structure consisting of two fully connected layers and a sigmoid activation in the excitation module. \nThis bottleneck structure can capture the inter-channel dependencies flexibly. A channel dimensional adjusting \nrate of 4 is adopted in this work. After the excitation module obtains the weights of the different channels w sq \naccording to their importance, a weighting process is performed on the corresponding channels. The SE Block \nachieves channel adjustment with fewer parameters through refined model design, emphasizing the channels \nwith more separable information, and suppressing channels less useful.\nAfter the channel attention processing in the SE block, the backbone features are projected through a 1 × 1 \nconvolution and combined with the two branch results to obtain a more effective high-dimensional expression \nof micro-Doppler behaviour features.\nEach 1 × 1 convolution and DSC operation are followed by a Batch normalization (BN) layer and a non-\nlinear activation function ReLU. The BN layer implements normalization by calculating the mean and variance \nof the input. A Hardswish activation function is used to process the output of the SE block. The nonlinearity of \nthe Hardswish is defined as\nIt has been verified that it performs better in the deeper network. The Hardswish can reduce the filter number \nunder the same precision.\nFeature enhancement network\nThe feature extraction network focuses on the local micro-Doppler feature extraction at different scales. The \nfeature enhancement network can eliminate background noise interference  effectively32 and highlight the micro-\nDoppler features related to human behavior based on multi-scale feature extraction. In this paper, cross-stacked \nRadar-ViT and RES-SE modules are applied to achieve global feature enhancement. In the combination structure, \nthe RES-SE module learns the local representation of the micro-Doppler features with spatial inductive bias. The \nRadar-ViT processes the global information encoding of the HAR. This hybrid structure enables us to design a \nshallow and narrow lightweight network.\nConsidering the RES-SE modules at both ends, Radar-ViT further simplifies the local representation and \nfusion modules of MobileVit, as shown in Fig. 4. Two 1 × 1 convolutions are designed around the stacked global \nrepresentation modules for the channel adjustment, to keep the consistent scales of the input and output.\nAssuming that the size of the feature map is H× W , the feature map of each channel is divided into non-\noverlapping cells of size P , with the total number of HW /P . The unfold operation after the point-wise convolu-\ntion downsamples each feature map to form P non-overlapping flatten patches. The position information within \neach cell is retained between the P flatten patches, and the spatial relationship between the cells, that is, the \nglobal micro-Doppler features is preserved in each flatten patch. Therefore, although the subsequent multi-head \nattention modules act on the downsampled flat patch, the overall effective receptive field is H× W . Multi-head \nattention is the key module in ViT, which is a combination of multiple self-attention blocks. The input is linearly \nmapped through learnable matrixes into three variables with the same dimension, namely query Q , key K , and \nvalue V . The normalized similarity between Q and K is used as the weight of V . The self-attention model adopts \nthe short-cut structure from the residual network, which can effectively prevent the degradation problem. The \nfeature outputs from different attention heads are combined by a Concat. Multiple heads enable the network to \ncapture abundant feature information from different representation subspaces.\n(1)xsq = 1\nH × W\nH∑\ni=1\nW∑\nj=1\nxdsc(i, j).\n(2)xse = w sqxdsc\n(3)hardswith[x] = xReLU6(x + 3)\n6 .\nFigure 3.  Schematic of the Squeeze-Excitation (SE) block.\n5\nVol.:(0123456789)Scientific Reports |        (2023) 13:17996  | https://doi.org/10.1038/s41598-023-45149-5\nwww.nature.com/scientificreports/\nRadar-ViT obtains a global representation of the micro-Doppler feature within each flatten patch separately \nby L stacked normalization modules and multi-head attention modules. The global micro-Doppler feature can \nrestore its scale through the fold operation. After a point-wise convolution, the fold output is combined with \nthe Radar-ViT’s input via concatenation operation. The shortcut branch provides another direct path, allowing \nfaster information propagation. It can accelerate the training process, speed up model convergence, and enhance \nrecognition accuracy. These concatenated features are fused in the subsequent RES-SE modules.\nExperimental results\nExperiment dataset\nTwo datasets were used to validate the superiority of the LH-ViT. The public dataset is collected by a C-band \n radar33. The radar’s working bandwidth is 400 MHz. The chirp period is 1 ms. This dataset contains radar ech -\noes of 6 human activities. Among them, 5 human activities were collected with a duration of 5 s, namely sitting \nin a chair, standing up, bending to pick up an object, drinking from a cup or glass, and falling. The collection \ntime of the walking activity is 10 s. Due to the lack of data corresponding to the falling activity, the experiment \nonly uses the other five human activity data in this paper. The sketch images, MDMs, and quantities of different \nhuman activities in the public datasets are listed in Table 1. The experiments on the public dataset are measured \nat 656 × 656.\nThe self-established dataset developed by Guangzhou University is collected by a millimeter wave (mmWave) \nradar working at 79 GHz. The mmWave Radar’s working bandwidth is 3.68 GHz. The chirp period is 392 μ s . The \nexperiment was carried out in a laboratory. The radar platform was 1.5 m in height. The self-established dataset \ncontains the radar echo data of 5 human activities. They are walking, running, standing up after squatting down, \n(4)Attention(Q,K,V) = softmax\n(\nQKT\n√dk\n)\nV.\nGlobal\nRepresentations\nFold\nMulti-Head\nAttention\nLayerN orm\nQK V\nݴL\nUnfold\nShortcut\nConv1x1\nConv1x1\nH\nW\nP\nN\nH\nW\nP\nN\nK VQ\nAttention\nWeight\nFigure 4.  Radar-ViT diagram based on multi-head attention mechanism.\nTable 1.  The public dataset collected by a C-band radar.\nLabel (0) (1) (2) (3) (4) (5)\nSketch map\nActivity Walking back and forth Sitting in a chair Standing up Bending to pick up an \nobject\nDrinking from a cup \nof glass Falling\nMDM\nQuantity 312 312 311 311 310 198\n6\nVol:.(1234567890)Scientific Reports |        (2023) 13:17996  | https://doi.org/10.1038/s41598-023-45149-5\nwww.nature.com/scientificreports/\nbending, and turning respectively. The self-established dataset collects the human activities of 10 participants, \nincluding 7 males and 3 females. To increase the within-class diversity of this dataset, the participants varied in \nage, height, and weight. Radar data were recorded according to their respective behavior habits, with no special \nbehavior constraints attached. To expand the data amount, data augmentation was applied additionally only to \nthe self-established dataset. The sketch images, MDMs, and quantities of different human activities in the public \ndatasets are listed in Table 2. The experiments on the self-established dataset are measured at 224 × 224.\nBoth datasets were divided into 80% for training and 20% for testing at random. MATLAB is applied for the \nradar signal processing of MDM. PyTorch 11.3 is used to build a DL model. The adaptive moment estimation \n(Adam) optimizer is utilized for network training. The learning rate is set to 0.0001. A dropout with a probability \nof 0.5 is applied after each pyramid layer. All the experiments in this paper are based on a hardware platform \nwith an Intel i9 16-core CPU and one NVIDIA 3090 24G GPU.\nNetwork structure discussion\nThe LH-ViT network proposed in this work consists of a multi-layer pyramid and alternate stacked Radar-ViT \nand RES-SE models. The recognition performance and efficiency of the LH-ViT are closely related to the num-\nber of the pyramid layer, the alternate stacked Radar-ViT and RES-SE models. A trade is essential between the \nfeature representation and the computational efficiency of the LH-ViT. The feature representative capability can \nbe enhanced along with the deepening of the network for the raised nonlinear expression ability. Deep networks \nare capable of fitting more complex features. However, performance saturation, optimization difficulties, and \nshallow learning decline also occur as the network deepens. The test results on the network structure in this \nsection are all based on the self-built dataset.\nFirst, the optimal massive structure is determined by different combinations of the pyramid layers, the Radar-\nViT, and the RES-SE stacking number. L in each Radar-ViT is fixed as 2. The HAR average accuracy, the parameter \nquantity, the floating point operations (FLOPs), and the inference time are used as the indicators of the network \nperformance.\nBased on the self-established dataset, Table 3 discusses the optimal structure of the proposed LH-ViT network. \nThis table also includes the ablation experiment. For concise structure representation, i − j − k  is used to indicate \nthat the feature extraction part of the network structure contains i level pyramids, and the feature enhancement \npart contains j Radar-ViT and k auxiliary RES-SE modules. In general, the accuracy of the LH-ViT increased \nalong with the deepening of the network structure. But when the number of pyramid layers rises to more than 4 \nlayers, the deeper structure contributes little to the network performance. Taking the LH-ViT(4-2-1) as an exam-\nple, it is the smallest structure with an accuracy greater than 99%. This structure achieves 99.7% HAR accuracy \nwith a parameter amount of 769.32 K. When the pyramid layers number rises up to 5 with the rest of the structure \nunchanged, the parameter amount increases by 176.576 K, but the recognition accuracy rate decreases by 0.2%.\nIn terms of network efficiency, as the network structure deepens, the inference time shows a trend from \ndecline to rise. It shows that a reasonable combination of network modules can not only make the network more \npowerful but also more efficient. Specifically, compare the LH-ViT(4-0-0) and LH-ViT(4-1-1). LH-ViT(4-1-1) \nadds 1 Radar-ViT and 1 auxiliary RES-SE on the four-layer pyramid in LH-ViT(4-0-0). Both network parameters \nand FLOPs are doubled in LH-ViT(4-1-1). LH-ViT(4-1-1) has higher accuracy and less interference time. A \nsimilar pattern can also be found in the comparison of LH-ViT(3-0-0) and LH-ViT(3-1-1). It shows that Radar-\nViT can help the feature pyramid to make better use of the GPU, making it more efficient to implement a single \nMDM inference and thus faster.\nFinally, the LH-ViT(4-2-1) network, marked in bold in Table 3, is used as a reference structure for subsequent \ncomparison and discussion. The results of the LH-ViT(0-2-1) and LH-ViT(4-0-0) network in Table  3 can be \nregarded as ablation experiments. It shows the network performance that only includes the feature extractor or \nthe feature enhancement part. The HAR accuracy of the LH-ViT(0-2-1) network without the feature pyramid is \nonly 91.7% and requires 24.29 ms inference time. This shows that Radar-ViT based on the multi-head attention \nneeds MDM feature pre-extraction. Insufficient feature extraction can greatly degrade its performance. Radar-\nViT enables important feature attention among pre-extracted rich features. The importance of the attention \nmechanism has been generally accepted, which also accounts for the performance improvement in the inference \nefficiency and accuracy of the LH-ViT(4-2-1) network relative to the LH-ViT(4-0-0) network. The results show \nTable 2.  The self-established dataset collected by a mmWave radar.\nLabel (0) (1) (2) (3) (4)\nSketch map\nActivity Walking Running Standing up after squatting down Bending Turning\nMDM\nQuantity 990 990 990 990 990\n7\nVol.:(0123456789)Scientific Reports |        (2023) 13:17996  | https://doi.org/10.1038/s41598-023-45149-5\nwww.nature.com/scientificreports/\nthat the performance of the hybrid network including feature pyramid and Radar-ViT outperforms that of a single \nnetwork. Compared with LH-ViT(0-2-1)and LH-ViT(4-0-0), the LH-ViT(4-2-1) has improved the accuracy by \n8% and 5.1% respectively, and the inference time has been shortened by 22.43 ms and 0.97 ms respectively. It \nmeans the LH-ViT(4-2-1) network can achieve more accurate and efficient HAR from MDM.\nTable 4 compares the network performance with different L in the Radar-ViT module. Experimental results \nshow that increasing the transformer repetitions does not improve the network performance significantly. Con-\nversely, a bigger L leads to an increase in the parameters and FLOPs, which is not conducive to a lightweight \ndesign. At the same time, the inference time also increases. Therefore, a setting of L = 2 is adopted in the refer-\nence LH-ViT(4-2-1) network.\nTable 5 compares the network performance with different convolutional structures. The RES-SE module \nin the LH-ViT(4-2-1) network is replaced by conventional  convolution11, transposed  convolution34, dilated \n convolution35, and group  convolution36 respectively. The network using the RES-SE module achieves the highest \nmeasured parameters and FLOPs, but at the same time, it also has the shortest inference time and highest HAR \naccuracy. Compared with the best-performing group convolution in the comparison module, the recognition \naccuracy of the structure using the RES-SE module is improved by 0.9%, and the inference time is shortened by \n1.33 ms. It illustrates the superiority of the LH-ViT network based on the RES-SE module for micro-Doppler \nfeature extraction.\nIn the comparison of four different attention mechanisms in Table 6, the SE module demonstrates the highest \naccuracy and the shortest inference time in the micro-Doppler feature extraction.\nTable 3.  Discussion of the LH-ViT network structure based on the self-established dataset. Significant values \nare in bold.\nPyramid layer Radar-ViT number RE-SES number Parameters FLOPs Inference time Accuracy\n0 2 1 124.464 K 1.56 G 24.29 ms 91.7%\n1 1 1 274.888 K 0.87 G 6.86 ms 93.9%\n2 1 1 288.092 K 0.92 G 4.07 ms 95.7%\n3 0 0 101.200 K 0.34 G 3.82 ms 93.2%\n3 1 1 344.948 K 1.09 G 3.75 ms 97.9%\n3 2 1 299.224 K 0.96 G 5.06 ms 97.5%\n3 2 2 361.464 K 1.15 G 6.05 ms 98.2%\n4 0 0 146.488 K 0.48 G 2.83 ms 94.6%\n4 0 1 173.638 K 0.56 G 2.95 ms 98.5%\n4 1 1 379.912 K 1.19 G 1.58 ms 98.7%\n4 2 1 769.320 K 2.41 G 1.86 ms 99.7%\n4 2 2 901.656 K 2.81 G 2.02 ms 99.8%\n5 1 1 920.808 K 2.87 G 7.02 ms 99.5%\n5 2 1 945.896 K 2.95 G 7.87 ms 99.5%\n6 1 1 1.041 M 3.23 G 7.57 ms 99.6%\nTable 4.  Comparison of parameters for different numbers of transformers based on the self-established \ndataset.\nL Parameters FLOPs Inference time Accuracy\n2 769.320 K 2.41 G 1.86 ms 99.7%\n4 943.976 K 2.95 G 4.45 ms 99.8%\n6 1.118 M 3.49 G 5.04 ms 99.9%\nTable 5.  Performance comparison of different convolution structures based on the self-established dataset.\nConvolution structure Parameters FLOPs Inference time Accuracy\nConventional  convolution11 783.684 K 2.33 G 3.30 ms 95.7%\nTransposed  convolution34 783.437 K 2.33 G 3.27 ms 98.1%\nDilated  convolution35 783.354 K 2.34 G 3.32 ms 97.8%\nGroup  convolution36 743.960 K 2.12 G 3.19 ms 98.8%\nRES-SE module 769.320 K 2.41 G 1.86 ms 99.7%\n8\nVol:.(1234567890)Scientific Reports |        (2023) 13:17996  | https://doi.org/10.1038/s41598-023-45149-5\nwww.nature.com/scientificreports/\nTo better comprehend the role of the attention mechanism in the HAR task based on the radar signals, Fig. 5 \ndepicts the feature region in MDM that the last layer of the attention mechanism focuses on. Heatmaps high-\nlight the regions considered crucial for HAR by the LH-ViT network, facilitating the visual display. The first row \ndisplays five grayscale MDM images with activity labels. The second row displays the matching heatmaps for the \ngrayscale MDM image. The red regions on the heatmap indicate the regions that the network prioritizes. The \nmajority of red patches in the attention heatmap are dispersed near endpoints and the Doppler center, reflecting \nchanges in micro-Doppler distributions. It aligns with the Doppler distribution characteristics that can reflect \nhuman activities in MDM.\nLH‑ViT versus state‑of‑the‑art and literature networks\nTable 7 used the state-of-the-art DL networks and the literature networks which have been applied to solve the \nHAR problem based on radar signals for comparative discussion. The accuracy of these networks was tested on \nboth datasets. The public dataset has a larger input data size and less data volume. Due to the different Doppler \nscales, the MDMs in the public dataset have lower micro-Doppler features significance. This all increases the \ndifficulty of achieving accurate HAR on the public dataset.\nThe HAR accuracy of SVM and HMM is relatively low.  ShuffleNet41 and  EfficientNet42 are convolutional \nneural networks. Among them, Shufflenet has fewer parameters, but lower accuracy. The parameter amount \nof Efficientnet has reached about 4M, and its accuracy rate is high. The inference time of both networks above \nis within 1.5 ms.  LSTM15 and GRU 43 are sequential networks of RNN variants, in which GRU has a higher \naccuracy rate. The main problem with this type of network is the parameter quantity and interference efficiency \nintroduced by the network complexity.  DeiT24,  CrossViT44 and  MobileViT26 are three lightweight ViT network \nexamples with good performance. DeiT has a smaller number of parameters, Flops, and shorter inference time. \nThe accuracy of DeiT is higher on the self-established dataset, but lower on the public dataset. MobileViT has \nbetter performance but longer inference time.\nStack3-LSTM18 and LSTM-BiLSTM17 realize HAR in the form of a hybrid network considering the timing \ncorrelation characteristics of radar human motion signals. Both networks achieved over 95% HAR accuracy on \nthe self-established dataset. However, similar to LSTM and GRU, such networks’ accuracy comes at the expense \nof a huge number of network parameters and time overhead, and both networks’ performance shows a sharp \nTable 6.  Performance comparison of different attention module in RES-SE based on the self-established \ndataset.\nAttention modul Parameters FLOPs Inference time Accuracy\nCA37 766.296 K 2.39 G 3.03 ms 99.1%\nCBAM38 769.472 K 2.41 G 2.03 ms 99.5%\nECA39 740.318 K 2.32 G 1.72 ms 99.3%\nSE 769.320 K 2.41 G 1.86 ms 99.7%\nFigure 5.  Grayscale images of the five actions along with their heatmaps, (a) grayscale image of walking. \n(b) grayscale image of running. (c) grayscale image of standing up after squatting down. (d) grayscale image \nof bending. (e) grayscale image of turning. (f) heatmap of walking. (g) heatmap of running. (h) heatmap of \nstanding up after squatting down. (i) heatmap of bending. (j) heatmap of turning.\n9\nVol.:(0123456789)Scientific Reports |        (2023) 13:17996  | https://doi.org/10.1038/s41598-023-45149-5\nwww.nature.com/scientificreports/\ndecrease in the public dataset. Mobile-RadarNet 20 has the smallest FLOPs, but its accuracy is not competitive \namong the networks in Table 7.  CLA25 has the fewest parameters and the fastest inference time, and its accuracy \nis also at a good level in both datasets. Although Slice-VIT45 makes ViT better adaptable in solving radar-based \nHAR through slice preprocessing, the complexity and efficiency of this network are still key issues to be solved.\nThe LH-ViT proposed serves as a lightweight hybrid network of convolution and ViT. The highest accuracy \nis achieved on both datasets. Moreover, the amount of parameters is the smallest among the ViT-type networks, \nand the inference time is also at a relatively fast level. The above results illustrate the excellent performance of \nthe LH-ViT network as well as its good adaptability and robustness.\nThe confusion matrix illustrates the specific recognition results of four lightweight network models using 297 \nimages for each activity, as shown in Fig. 6. LH-ViT only had four images misrecognized in the turning category \nfor bending. Unlike vision-based HAR, radar-based HAR is achieved through the time-dependent variation \nin the micro-Doppler components introduced by limb movements, so the frequency characteristics of human \nmovements determine the degree of different activity similarity. Human activities which exhibit similar features \nin the Doppler domain along slow time will lead to recognition errors. Despite this, the performance of LH-ViT \nis the best among the four networks.\nSubject-independent split can reflect the individual differences sensitivity and the generalization performance \nof the proposed network. The public dataset contains 20 people’s radar data of activities, of which 16 individu -\nals are used for training and 4 for testing. For the self-established dataset, 8 people’s data are used for training \nand 2 for testing. Tables  8 and 9 show the results of the subject-independent split experiment under different \ndatasets respectively. The accuracy of LH-ViT(4-2-1) is only reduced by 0.4% and 0.2% respectively in the public \ndataset and the self- established dataset. These results are better than the MobileViT in both individual activity \naccuracy and comprehensive accuracy. It shows that the LH-ViT network proposed in this paper can well adapt \nto the individual differences and achieve high performance radar-based HAR through accurate Micro-Doppler \nfeature extraction.\nConclusion\nThis paper developed a lightweight hybrid Vision Transformer network for HAR based on radar’s micro-Doppler \nfeatures. After preprocessing, the network can obtain the recognition accuracy of 99.7% in the self-established \ndataset and 92.1% in the public dataset respectively. We investigated the performance of the proposed network \nunder various architectures and obtained the optimal structure. The optimal structure was compared with other \nwidely used networks as well as HAR networks in the literature and showed performance advantages. The \nproposed network satisfies the accuracy and real-time requirements for HAR and is promising for embedded \napplications. This work is only used for single-action recognition, and the collection scenario is relatively ideal. In \nthe future, we plan to improve and expand the number and type of data sets, develop the radar signal processing \nalgorithms, and optimize the deep learning network structure to improve radar-based HAR performance in the \nface of complex and continuous human activities.\nTable 7.  Comparison of state-of-the-art networks. Significant values are in bold.\nNetworks Parameters FLOPs Inference time Accuracy of the public dataset\nAccuracy of the self-established \ndataset\nSVM7 657.41 K 193.5 M 0.56 ms 59.1% 71.7%\nHMM40 732.5 K 274.6 G 0.72 ms 60.7% 75.4%\nShuffleNet41 346.917 K 426.3 M 1.38 ms 88.6% 95.5%\nEfficientNet42 4.01 M 398 M 1.48 ms 88.4% 98.8%\nLSTM15 11.6 M 7.88 G 38.48 ms 60.3% 75.3%\nGRU 43 8.76 M 5.91 G 36.43 ms 63.9% 96.9%\nDeiT24 5.679 M 1.08 G 1.40 ms 83.9% 98.7%\nCrossViT44 6.649 M 1.29 G 1.90 ms 87.5% 87.8%\nMobileViT26 1.27 M 1.44G 15.17 ms 91.3% 98.9%\nLSTM-BiLSTM17 282.285 K 10.6 G 32.30 ms 76.1% 96.3%\nStack3-LSTM18 3.08 M 446.47 M 5.17 ms 72.3% 95.4%\nMobile-RadarNet20 241.1 K 3.11 M 2.61 ms 85.7% 95.6%\nCLA25 97.57 K 12.57 M 0.38 ms 89.1% 97.1%\nSlice-VIT45 85 M 16.86 G 38.47 ms 86.4% 99.1%\nLH-ViT(4-2-1) 769.32 K 2.41 G 1.58 ms 92.1% 99.7%\n10\nVol:.(1234567890)Scientific Reports |        (2023) 13:17996  | https://doi.org/10.1038/s41598-023-45149-5\nwww.nature.com/scientificreports/\nFigure 6.  Comparison of confusion matrices in different networks, (a) ShuffleNet. (b) GRU. (c) DeiT. (d) \nLH-ViT(4-2-1).\nTable 8.  Subject-independent split experiment based on the public dataset. Significant values are in bold.\nNetworks\nAccuracy\nLabel (0) Label (1) Label (2) Label (3) Label (4) Label (5) ALL\nMobileViT 91.5% 91.3% 91.4% 91.5% 91.5% 90.2% 90.9%\nLH-ViT(4-2-1) 92.4% 92.5% 92.5% 92.4% 92.2% 91.1% 91.7%\nTable 9.  Subject-independent split experiment based on the self-established dataset. Significant values are in \nbold.\nNetworks\nAccuracy\nLabel (0) Label (1) Label (2) Label (3) Label (4) ALL\nMobileViT 98.4% 98.5% 98.4% 98.5% 98.7% 98.6%\nLH-ViT(4-2-1) 99.4% 99.6% 99.4% 99.5% 99.6% 99.5%\n11\nVol.:(0123456789)Scientific Reports |        (2023) 13:17996  | https://doi.org/10.1038/s41598-023-45149-5\nwww.nature.com/scientificreports/\nData availability\nThe datasets used and/or analyzed during the current study available from the first author on reasonable request.\nReceived: 10 July 2023; Accepted: 16 October 2023\nReferences\n 1. Gurbuz, S. Z. & Amin, M. G. Radar-based human-motion recognition with deep learning: Promising applications for indoor \nmonitoring. IEEE Signal Process. Mag. 36, 16–28. https:// doi. org/ 10. 1109/ MSP . 2018. 28901 28 (2019).\n 2. Kamal, S., Jalal, A. & Kim, D. Depth images-based human detection, tracking and activity recognition using spatiotemporal features \nand modified HMM. J. Electr. Eng. Technol. 11, 1857–1862. https:// doi. org/ 10. 5370/ JEET. 2016. 11.6. 1857 (2016).\n 3. Jalal, A., Quaid, M. A. K., & Hasan, A. S. Wearable sensor-based human behavior understanding and recognition in daily life for \nsmart environments. In 2018 International Conference on Frontiers of Information Technology (FIT), 105–110, https:// doi. org/ 10. \n1109/ FIT. 2018. 00026 (2018).\n 4. Waghumbare, A., Singh, U. & Singhal, N. DCNN based human activity recognition using micro-doppler signatures. In 2022 IEEE \nBombay Section Signature Conference (IBSSC), 1–6, https:// doi. org/ 10. 1109/ IBSSC 56953. 2022. 10037 310 (2022).\n 5. Wang, C. xi, Chen, Z. C., Chen, X., Tang, X. & Liang, F . T. Detection of MMW radar target based on Doppler characteristics and \ndeep learning. In 2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID, 266–271, https://  doi. \norg/ 10. 1109/ AIID5 1893. 2021. 94564 97 (2021).\n 6. Chen, V . C. Joint time-frequency analysis for radar signal and imaging. In 2007 IEEE International Geoscience and Remote Sensing \nSymposium, 5166–516, https:// doi. org/ 10. 1109/ IGARSS. 2007. 44240 25 (2007).\n 7. Kim, Y . & Ling, H. Human activity classification based on micro-Doppler signatures using a support vector machine. IEEE Trans. \nGeosci. Remote Sens. 47, 1328–1337. https:// doi. org/ 10. 1109/ TGRS. 2009. 20128 49 (2009).\n 8. Fairchild, D. P . & Narayanan, R. M. Classification of human motions using empirical mode decomposition of human micro-Doppler \nsignatures. IET Radar Sonar Navig. 8, 425–434. https:// doi. org/ 10. 1049/ iet- rsn. 2013. 0165 (2014).\n 9. Karabacak, C. et al. Knowledge exploitation for human micro-Doppler classification. IEEE Geosci. Remote Sens. Lett. 12, 2125–2129. \nhttps:// doi. org/ 10. 1049/ iet- rsn. 2013. 0165 (2015).\n 10. Kim, Y . & Moon, T. Human detection and activity classification based on micro-Doppler signatures using deep convolutional \nneural networks. IEEE Geosci. Remote Sens. Lett. 13, 8–12. https:// doi. org/ 10. 1049/ iet- rsn. 2013. 0165 (2016).\n 11. Krizhevsky, A. & Sutskever, I. Imagenet classification with deep convolutional neural networks. Adv. Neural Inf. Process. Syst.  \nhttps:// doi. org/ 10. 1145/ 30653 86 (2012).\n 12. Zaremba, W ., Sutskever, I. & Vinyals, O. Recurrent neural network regularization. Preprint at https:// arXiv. org/ arXiv: 1409. 2329, \nhttps:// doi. org/ 10. 48550/ arXiv. 1409. 2329 (2014).\n 13. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. Syst. https:// doi. org/ 10. 48550/ arXiv. 1706. 03762 (2017).\n 14. Zhu, J., Chen, H. & Y e, W . A hybrid CNN–LSTM network for the classification of human activities based on micro-doppler radar. \nIEEE Access 8, 24713–24720. https:// doi. org/ 10. 1109/ ACCESS. 2022. 31508 38 (2020).\n 15. Güneş, O. & Morgül, Ö. LSTM based classification of targets using FMCW radar signals. In 2021 29th Signal Processing and Com-\nmunications Applications Conference (SIU), 1–4, https:// doi. org/ 10. 1109/ SIU53 274. 2021. 94779 27 (2021).\n 16. Kwon, H. B. et al. Attention-based LSTM for non-contact sleep stage classification using IR-UWB radar. IEEE J. Biomed. Health \nInform. 25, 3844–3853. https:// doi. org/ 10. 1109/ JBHI. 2021. 30726 44 (2021).\n 17. Shrestha, A., Li, H., Le Kernec, J. & Fioranelli, F . Continuous human activity classification from FMCW radar with Bi-LSTM \nnetworks. IEEE Sens. J. 20, 13607–13619. https:// doi. org/ 10. 1109/ JSEN. 2020. 30063 86 (2020).\n 18. Pan, M. et al. Radar HRRP target recognition model based on a stacked CNN–Bi-RNN with attention mechanism. IEEE Trans. \nGeosci. Remote Sens. 60, 1–14. https:// doi. org/ 10. 1109/ TGRS. 2021. 30550 61 (2022).\n 19. Chakraborty, M., Kumawat, H. C., Dhavale, S. V . & Raj, A. A. B. DIAT-μ RadHAR (micro-doppler signature dataset) & μ RadNet \n(a lightweight DCNN)—For human suspicious activity recognition. IEEE Sens. J. 22, 6851–6858. https:// doi. org/ 10. 1109/ JSEN. \n2022. 31519 43 (2022).\n 20. Zhu, J., Lou, X. & Y e, W . Lightweight deep learning model in mobile-edge computing for radar-based human activity recognition. \nIEEE Internet Things J. 8, 12350–12359. https:// doi. org/ 10. 1109/ JIOT. 2021. 30635 04 (2021).\n 21. Khalid, H.-U.-R., Gorji, A., Bourdoux, A., Pollin, S. & Sahli, H. Multi-view CNN-LSTM architecture for radar-based human activity \nrecognition. IEEE Access 10, 24509–24519. https:// doi. org/ 10. 1109/ ACCESS. 2020. 29710 64 (2022).\n 22. Hua, W ., Wang, X., Zhang, C. & Jin, X. Attention-based deep sequential network for polsar image classification. 2000 IEEE Int. \nGeosci. Remote Sens. Symp. https:// doi. org/ 10. 1109/ IGARS S46834. 2022. 98836 34 (2022).\n 23. He, Y ., Li, X. & Jing, X. A mutiscale residual attention network for multitask learning of human activity using radar micro-doppler \nsignatures. Remote Sens. 11, 2584. https:// doi. org/ 10. 3390/ rs112 12584 (2019).\n 24. Touvron, H. et al. Training data-efficient image transformers & distillation through attention. In Proceedings of the 38th International \nConference on Machine Learning, Vol. 139, 10347–10357, https:// doi. org/ 10. 48550/ arXiv. 2012. 12877 (2021).\n 25. D’ Ascoli, S. et al. ConViT: Improving vision transformers with soft convolutional inductive biases. In Proceedings of the 38th \nInternational Conference on Machine Learning, vol. 139, 2286–2296, https:// doi. org/ 10. 1088/ 1742- 5468/ ac9830 (2021).\n 26. Mehta, S. & Rastegari, M. MobileViT: Light-weight, general-purpose, and mobile-friendly vision transformer. arXiv  https:// doi. \norg/ 10. 48550/ arXiv. 2110. 02178 (2021).\n 27. Chen, Y ., Dai, X., Chen, D. & Liu, M. Mobile-former: Bridging mobilenet and transformer. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR), 5270–5279, https:// doi. org/ 10. 48550/ arXiv. 2108. 05895 (2022).\n 28. Ahmed, W ., Naeem, U., Y ousaf, M. H., & Velastin, S. A. Lightweight CNN and GRU network for real-time action recognition. In \n2022 12th International Conference on Pattern Recognition Systems (ICPRS), 1–7, https:// doi. org/ 10. 1109/ ICPRS 54038. 2022. 98538 \n54 (2022).\n 29. Huan, S., Wu, L., Zhang, M., Wang, Z. & Y ang, C. Radar human activity recognition with an attention-based deep learning network. \nSensors 23, 3185. https:// doi. org/ 10. 3390/ s2306 3185 (2023).\n 30. Howard, A. G. et al. MobileNets: Efficient convolutional neural networks for mobile vision applications. arXiv https:// doi. org/ 10. \n48550/ arXiv. 1704. 04861 (2017).\n 31. Hu, J., Shen, L., Albanie, S., Sun, G. & Wu, E. Squeeze-and-excitation networks. IEEE Trans. Pattern Anal. Mach. Intell. 42, \n2011–2023. https:// doi. org/ 10. 1109/ CVPR. 2018. 00745 (2020).\n 32. Jalal, A., Kim, Y .-H., Kim, Y .-J., Kamal, S. & Kim, D. Robust human activity recognition from depth video using spatiotemporal \nmulti-fused features. Pattern Recognit. 61, 295–308. https:// doi. org/ 10. 1016/j. patcog. 2016. 08. 003 (2017).\n 33. Fioranelli, D. F . et al. Radar sensing for healthcare. Electron. Lett. 55, 1022–1024. https:// doi. org/ 10. 1049/ el. 2019. 2378 (2019).\n 34. Dumoulin, V ., & Visin, F . A guide to convolution arithmetic for deep learning. Preprint at https:// arXiv. org/ arXiv: 1603. 07285, \nhttps:// doi. org/ 10. 48550/ arXiv. 1603. 07285 (2016).\n 35. Yu, F ., & Koltun, V . Multi-scale context aggregation by dilated convolutions. Preprint at https:// arXiv. org/ arXiv: 1511. 07122, https:// \ndoi. org/ 10. 48550/ arXiv. 1511. 07122 (2015).\n12\nVol:.(1234567890)Scientific Reports |        (2023) 13:17996  | https://doi.org/10.1038/s41598-023-45149-5\nwww.nature.com/scientificreports/\n 36. Ioannou, Y ., Robertson, D., Cipolla, R. & Criminisi, A. Deep roots: Improving CNN efficiency with hierarchical filter groups. Proc. \nIEEE Conf. Comput. Vis. Pattern recogn. https:// doi. org/ 10. 1109/ CVPR. 2017. 633 (2017).\n 37. Hou, Q., Zhou, D. & Feng, J. Coordinate attention for efficient mobile network design. arXiv https:// doi. org/ 10. 48550/ arXiv. 2103. \n02907 (2021).\n 38. Agac, S. & Durmaz Incel, O. On the use of a convolutional block attention module in deep learning-based human activity recogni-\ntion with motion sensors. Diagnostics https:// doi. org/ 10. 3390/ diagn ostic s1311 1861 (2023).\n 39. Geng, F . et al. Light-efficient channel attention in convolutional neural networks for tic recognition in the children with tic disor-\nders. Front. Comput. Neurosci. 16, 1047954. https:// doi. org/ 10. 3389/ fncom. 2022. 10479 54 (2022).\n 40. Piyathilaka, L., & Kodagoda, S. Gaussian mixture based HMM for human daily activity recognition using 3D skeleton features. In \n2013 IEEE 8th Conference on Industrial Electronics and Applications (ICIEA), 567–572, https:// doi. org/ 10. 1109/ ICIEA. 2013. 65664 \n33 (2013).\n 41. Zhang, X., Zhou, X., Lin, M. & Sun, J. ShuffleNet: An extremely efficient convolutional neural network for mobile devices. arXiv  \nhttps:// doi. org/ 10. 48550/ arXiv. 1707. 01083 (2017).\n 42. Tan, M. & Le, Q. V . EfficientNet: Rethinking model scaling for convolutional neural networks. arXiv  https:// doi. org/ 10. 48550/ \narXiv. 1905. 11946 (2019).\n 43. Dey, R. & Salem, F . M. Gate-variants of gated recurrent unit (GRU) neural networks. In 2017 IEEE 60th International Midwest \nSymposium on Circuits and Systems (MWSCAS), 1597–1600, https:// doi. org/ 10. 1109/ MWSCAS. 2017. 80532 43 (2017).\n 44. Chen, C.-F . R., Fan, Q. & Panda, R. CrossViT: Cross-attention multi-scale vision transformer for image classification. In 2021 \nIEEE/CVF International Conference on Computer Vision (ICCV), 357–366, https:// doi. org/ 10. 1109/ iccv4 8922. 2021. 00041 (2021).\n 45. Wang, Z. et al. Attention-based vision transformer for human activity classification using mmwave radar. In Proc. of the 2022 4th \nInternational Conference on Video, Signal and Image Processing, 128–134, https:// doi. org/ 10. 1145/ 35771 64. 35771 84 (2023).\nAcknowledgements\nThis research was supported by the “Key Laboratory of On-Chip Communication and Sensor Chip of Guangdong \nHigher Education Institutes, Guangzhou University” , KLOCCSCGHEI (2023KSYS002).\nAuthor contributions\nS.H. and X.Q.W . conceived the experiments, Z.Y .W . and G.E.D. conducted the experiment, H.H.M. and X.X.Y . \nanalyzed the results. S.H. and L.M.W . prepared the original draft. All authors reviewed the manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to X.W .\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8045004606246948
    },
    {
      "name": "Radar",
      "score": 0.6120644211769104
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5813592672348022
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5290844440460205
    },
    {
      "name": "Transformer",
      "score": 0.4871325194835663
    },
    {
      "name": "Deep learning",
      "score": 0.4766700565814972
    },
    {
      "name": "Feature extraction",
      "score": 0.45320820808410645
    },
    {
      "name": "Artificial neural network",
      "score": 0.45074254274368286
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4402558505535126
    },
    {
      "name": "Real-time computing",
      "score": 0.37667375802993774
    },
    {
      "name": "Engineering",
      "score": 0.1104663610458374
    },
    {
      "name": "Telecommunications",
      "score": 0.10973390936851501
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}