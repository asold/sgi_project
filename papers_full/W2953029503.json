{
  "title": "Reducing Gender Bias in Word-Level Language Models with a Gender-Equalizing Loss Function",
  "url": "https://openalex.org/W2953029503",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5090922071",
      "name": "Yusu Qian",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A5004394596",
      "name": "Urwa Muaz",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A5100700612",
      "name": "Ben Zhang",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A5010090397",
      "name": "Jae Won Hyun",
      "affiliations": [
        "New York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2887768933",
    "https://openalex.org/W2483215953",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2889624842",
    "https://openalex.org/W2942370121",
    "https://openalex.org/W3037697022",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2962787423",
    "https://openalex.org/W3122810052"
  ],
  "abstract": "Gender bias exists in natural language datasets which neural language models tend to learn, resulting in biased text generation. In this research, we propose a debiasing approach based on the loss function modification. We introduce a new term to the loss function which attempts to equalize the probabilities of male and female words in the output. Using an array of bias evaluation metrics, we provide empirical evidence that our approach successfully mitigates gender bias in language models without increasing perplexity. In comparison to existing debiasing strategies, data augmentation, and word embedding debiasing, our method performs better in several aspects, especially in reducing gender bias in occupation words. Finally, we introduce a combination of data augmentation and our approach, and show that it outperforms existing strategies in all bias evaluation metrics.",
  "full_text": "arXiv:1905.12801v2  [cs.CL]  3 Jun 2019\nReducing Gender Bias in W ord-Level Language Models with a\nGender-Equalizing Loss Function\nY usu Qian∗\nT andon School\nof Engineering\nNew Y ork University\n6 MetroT ech Center\nBrooklyn, NY , 11201\nyq729@nyu.edu\nUrwa Muaz∗\nT andon School\nof Engineering\nNew Y ork University\n6 MetroT ech Center\nBrooklyn, NY , 11201\num367@nyu.edu\nBen Zhang\nCenter for\nData Science\nNew Y ork University\n60 Fifth A venue\nNew Y ork, NY , 10012\nbz957@nyu.edu\nJae W on Hyun\nDepartment of\nComputer Science\nNew Y ork University\n251 Mercer St\nNew Y ork, NY , 10012\njaewhyun@nyu.edu\nAbstract\nGender bias exists in natural language datasets\nwhich neural language models tend to learn,\nresulting in biased text generation. In this\nresearch, we propose a debiasing approach\nbased on the loss function modiﬁcation. W e\nintroduce a new term to the loss function\nwhich attempts to equalize the probabilities of\nmale and female words in the output. Using\nan array of bias evaluation metrics, we provide\nempirical evidence that our approach success-\nfully mitigates gender bias in language mod-\nels without increasing perplexity by much. In\ncomparison to existing debiasing strategies,\ndata augmentation, and word embedding de-\nbiasing, our method performs better in sev-\neral aspects, especially in reducing gender bias\nin occupation words. Finally, we introduce a\ncombination of data augmentation and our ap-\nproach, and show that it outperforms existing\nstrategies in all bias evaluation metrics.\n1 Introduction\nNatural Language Processing (NLP) models are\nshown to capture unwanted biases and stereotypes\nfound in the training data which raise concerns\nabout socioeconomic, ethnic and gender discrimi-\nnation when these models are deployed for public\nuse (\nLu et al. , 2018; Zhao et al. , 2018).\nThere are numerous studies that identify al-\ngorithmic bias in NLP applications. Lapowsky\n(2018) showed ethnic bias in Google autocom-\nplete suggestions whereas Lambrecht and Tucker\n(2018) found gender bias in advertisement de-\nlivery systems. Additionally , Zhao et al. (2018)\ndemonstrated that coreference resolution systems\nexhibit gender bias.\nLanguage modelling is a pivotal task in\nNLP with important downstream applica-\ntions such as text generation (\nSutskever et al. ,\n2011). Recent studies by Lu et al. (2018) and\n∗\nY usu Qian and Urwa Muaz contributed equally to the\npaper.\nBordia and Bowman (2019) have shown that this\ntask is vulnerable to gender bias in the training\ncorpus. T wo prior works focused on reducing\nbias in language modelling by data preprocessing\n(\nLu et al. , 2018) and word embedding debiasing\n(Bordia and Bowman , 2019). In this study , we\ninvestigate the efﬁcacy of bias reduction during\ntraining by introducing a new loss function which\nencourages the language model to equalize the\nprobabilities of predicting gendered word pairs\nlike he and she. Although we recognize that\ngender is non-binary , for the purpose of this study ,\nwe focus on female and male words.\nOur main contributions are summarized as fol-\nlows: i) to our best knowledge, this study is the\nﬁrst one to investigate bias alleviation in text gen-\neration by direct modiﬁcation of the loss func-\ntion; ii) our new loss function effectively reduces\ngender bias in the language models during train-\ning by equalizing the probabilities of male and\nfemale words in the output; iii) we show that\nend-to-end debiasing of the language model can\nachieve word embedding debiasing; iv) we pro-\nvide an interpretation of our results and draw a\ncomparison to other existing debiasing methods.\nW e show that our method, combined with an ex-\nisting method, counterfactual data augmentation,\nachieves the best result and outperforms all exist-\ning methods.\n2 Related W ork\nRecently , the study of bias in NLP applications\nhas received increasing attention from researchers.\nMost relevant work in this domain can be broadly\ndivided into two categories: word embedding de-\nbiasing and data debiasing by preprocessing.\nW ord Embedding Debiasing\nBolukbasi et al.\n(2016) introduced the idea of gender subspace\nas low dimensional space in an embedding that\ncaptures the gender information.\nBolukbasi et al.\n(2016) and Zhao et al. (2017) deﬁned gender bias\nas a projection of gender-neutral words on a gen-\nder subspace and removed bias by minimizing this\nprojection.\nGonen and Goldberg (2019) proved\nthat bias removal techniques based on minimiz-\ning projection onto the gender space are insufﬁ-\ncient. They showed that male and female stereo-\ntyped words cluster together even after such debi-\nasing treatments. Thus, gender bias still remains\nin the embeddings and is easily recoverable.\nBordia and Bowman (2019) introduced a co-\noccurrence based metric to measure gender bias\nin texts and showed that the standard datasets used\nfor language model training exhibit strong gender\nbias. They also showed that the models trained\non these datasets amplify bias measured on the\nmodel-generated texts. Using the same deﬁni-\ntion of embedding gender bias as\nBolukbasi et al.\n(2016), Bordia and Bowman (2019) introduced a\nregularization term that aims to minimize the pro-\njection of neutral words onto the gender subspace.\nThroughout this paper,we refer to this approach as\nREG. They found that REG reduces bias in the\ngenerated texts for some regularization coefﬁcient\nvalues. But, this bias deﬁnition is shown to be in-\ncomplete by\nGonen and Goldberg (2019). Instead\nof explicit geometric debiasing of the word em-\nbedding, we implement a loss function that mini-\nmizes bias in the output and thus adjust the whole\nnetwork accordingly . For each model, we analyze\nthe generated word embedding to understand how\nit is affected by output debiasing.\nData Debiasing\nLu et al. (2018) showed that\ngender bias in coreference resolution and language\nmodelling can be mitigated through a data aug-\nmentation technique that expands the corpus by\nswapping the gender pairs like he and she, or fa-\nther and mother. They called this Counterfactual\nData Augmentation (CDA) and concluded that it\noutperforms the word embedding debiasing strat-\negy proposed by\nBolukbasi et al. (2016). CDA\ndoubles the size of the training data and increases\ntime needed to train language models. In this\nstudy , we intend to reduce bias during training\nwithout requiring an additional data preprocessing\nstep.\n3 Methodology\n3.1 Dataset\nFor the training data, we use Daily Mail news\narticles released by\nHermann et al. (2015). This\ndataset is composed of 219,506 articles covering a\ndiverse range of topics including business, sports,\ntravel, etc., and is claimed to be biased and sen-\nsational (\nBordia and Bowman , 2019). For man-\nageability , we randomly subsample 5% of the text.\nThe subsample has around 8.25 million tokens in\ntotal.\n3.2 Language Model\nW e use a pre-trained 300-dimensional word em-\nbedding, GloV e, by\nPennington et al. (2014). W e\napply random search to the hyperparameter tuning\nof the LSTM language model. The best hyperpa-\nrameters are as follows: 2 hidden layers each with\n300 units, a sequence length of 35, a learning rate\nof 20 with an annealing schedule of decay start-\ning from 0.25 to 0.95, a dropout rate of 0.25 and\na gradient clip of 0.25. W e train our models for\n150 epochs, use a batch size of 48, and set early\nstopping with a patience of 5.\n3.3 Loss Function\nLanguage models are usually trained using cross-\nentropy loss. Cross-entropy loss at time step t is\nLCE (t) =−\n∑\nw∈ V\nyw,tlog (ˆyw,t) ,\nwhere V is the vocabulary , y is the one hot vector\nof ground truth and ˆy indicates the output softmax\nprobability of the model.\nW e introduce a loss term LB, which aims to\nequalize the predicted probabilities of gender pairs\nsuch as woman and man.\nLB(t) = 1\nG\nG∑\ni\n⏐\n⏐\n⏐\n⏐log ˆyfi,t\nˆymi,t\n⏐\n⏐\n⏐\n⏐\nf and m are a set of corresponding gender pairs, G\nis the size of the gender pairs set, and ˆy indicates\nthe output softmax probability . W e use gender\npairs provided by\nZhao et al. (2017). By consider-\ning only gender pairs we ensure that only gender\ninformation is neutralized and distribution over se-\nmantic concepts is not altered. For example, it\nwill try to equalize the probabilities of congress-\nman with congresswoman and actor with actress\nbut distribution of congressman, congresswoman\nversus actor, actress will not be affected. Overall\nloss can be written as\nL = 1\nT\nT∑\nt=1\nLCE (t) +λLB(t) ,\nwhere λ is a hyperparameter and T is the corpus\nsize. W e observe that among the similar minima\nof the loss function, LB encourages the model\nto converge towards a minimum that exhibits the\nlowest gender bias.\n3.4 Model Evaluation\nLanguage models are evaluated using perplexity ,\nwhich is a standard measure of performance for\nunseen data. For bias evaluation, we use an array\nof metrics to provide a holistic diagnosis of the\nmodel behavior under debiasing treatment. These\nmetrics are discussed in detail below . In all the\nevaluation metrics requiring gender pairs, we use\ngender pairs provided by\nZhao et al. (2017). This\nlist contains 223 pairs, all other words are consid-\nered gender-neutral.\n3.4.1 Co-occurrence Bias\nCo-occurrence bias is computed from the model-\ngenerated texts by comparing the occurrences of\nall gender-neutral words with female and male\nwords. A word is considered to be biased to-\nwards a certain gender if it occurs more frequently\nwith words of that gender. This deﬁnition was\nﬁrst used by\nZhao et al. (2017) and later adapted\nby Bordia and Bowman (2019). Using the def-\ninition of gender bias similar to the one used\nby\nBordia and Bowman (2019), we deﬁne gender\nbias as\nBN = 1\nN\n∑\nw∈ N\n⏐\n⏐\n⏐\n⏐log c(w, m )\nc(w, f )\n⏐\n⏐\n⏐\n⏐,\nwhere N is a set of gender-neutral words, and\nc(w, g ) is the occurrences of a word w with words\nof gender g in the same window . This score\nis designed to capture unequal co-occurrences of\nneutral words with male and female words. Co-\noccurrences are computed using a sliding window\nof size 10 extending equally in both directions.\nFurthermore, we only consider words that occur\nmore than 20 times with gendered words to ex-\nclude random effects.\nW e also evaluate a normalized version of BN\nwhich we denote by conditional co-occurrence\nbias, BN\nc . This is deﬁned as\nBN\nc = 1\nN\n∑\nw∈ N\n⏐\n⏐\n⏐\n⏐log P (w|m)\nP (w|f)\n⏐\n⏐\n⏐\n⏐,\nwhere\nP (w|g) = c(w, g )\nc(g) .\nBN\nc is less affected by the disparity in the general\ndistribution of male and female words in the text.\nThe disparity between the occurrences of the two\ngenders means that text is more inclined to men-\ntion one over the other, so it can also be considered\na form of bias. W e report the ratio of occurrence\nof male and female words in the model generated\ntext, GR, as\nGR = c(m)\nc(f) .\n3.4.2 Causal Bias\nAnother way of quantifying bias in NLP models is\nbased on the idea of causal testing. The model is\nexposed to paired samples which differ only in one\nattribute (e.g. gender) and the disparity in the out-\nput is interpreted as bias related to that attribute.\nZhao et al. (2018) and Lu et al. (2018) applied this\nmethod to measure bias in coreference resolution\nand\nLu et al. (2018) also used it for evaluating gen-\nder bias in language modelling.\nFollowing the approach similar to Lu et al.\n(2018), we limit this bias evaluation to a set of\ngender-neutral occupations. W e create a list of\nsentences based on a set of templates. There are\ntwo sets of templates used for evaluating causal\noccupation bias (T able\n1). The ﬁrst set of tem-\nplates is designed to measure how the probabilities\nof occupation words depend on the gender infor-\nmation in the seed. Below is an example of the\nﬁrst set of templates:\n[Gendered word] is a | [occupation] .\nHere, the vertical bar separates the seed sequence\nthat is fed into the language models from the target\noccupation, for which we observe the output soft-\nmax probability . W e measure causal occupation\nbias conditioned on gender as\nCB |g = 1\n|O|\n1\nG\n∑\no∈ O\nG∑\ni\n⏐\n⏐\n⏐\n⏐log p(o|fi)\np(o|mi)\n⏐\n⏐\n⏐\n⏐,\nwhere O is a set of gender-neutral occupations and\nG is the size of the gender pairs set. For exam-\nple, P (doctor|he) is the softmax probability of\nHe is a |\ndoctor log P (t|s1)\nP (t|s2)\nShe is a |\ns1\ns2\nt\n(a) Occupation bias conditioned on gendered words\nThe doctor is a |\nman\nlog P (t1|s)\nP (t2|s)\nwoman\ns t1\nt2\n(b) Occupation bias conditioned on occupations\nT able 1: Example templates of two types of occupation bias\nthe word doctor where the seed sequence is He\nis a. The second set of templates like below , aims\nto capture how the probabilities of gendered words\ndepend on the occupation words in the seed.\nT he [occupation] is a | [gendered word ] .\nCausal occupation bias conditioned on occupation\nis represented as\nCB |o = 1\n|O|\n1\nG\n∑\no∈ O\nG∑\ni\n⏐\n⏐\n⏐\n⏐log p(fi|o)\np(mi|o)\n⏐\n⏐\n⏐\n⏐,\nwhere O is a set of gender-neutral occupations and\nG is the size of the gender pairs set. For example,\nP (man|doctor) is the softmax probability of man\nwhere the seed sequence is The doctor is a.\nW e believe that both CB |g and CB |o contribute\nto gender bias in the model-generated texts. W e\nalso note that CB |o is more easily inﬂuenced by\nthe general disparity in male and female word\nprobabilities.\n3.4.3 W ord Embedding Bias\nOur debiasing approach does not explicitly ad-\ndress the bias in the embedding layer. Therefore,\nwe use gender-neutral occupations to measure the\nembedding bias to observe if debiasing the output\nlayer also decreases the bias in the embedding. W e\ndeﬁne the embedding bias, EBd, as the difference\nbetween the Euclidean distance of an occupation\nword to male words and the distance of the occu-\npation word to the female counterparts. This deﬁ-\nnition is equivalent to bias by projection described\nby\nBolukbasi et al. (2016). W e deﬁne EBd as\nEBd =\n∑\no∈ O\nG∑\ni\n|∥E(o) − E(mi)∥2\n−∥E(o) − E(fi)∥2| ,\nwhere O is a set of gender-neutral occupations,\nG is the size of the gender pairs set and E is the\nword-to-vector dictionary .\n3.5 Existing Approaches\nW e apply CDA where we swap all the gendered\nwords using a bidirectional dictionary of gender\npairs described by\nLu et al. (2018). This creates\na dataset twice the size of the original data, with\nexactly the same contextual distributions for both\ngenders and we use it to train the language models.\nW e also implement the bias regularization\nmethod of\nBordia and Bowman (2019) which\ndebiases the word embedding during language\nmodel training by minimizing the projection of\nneutral words on the gender axis. W e use hyper-\nparameter tuning to ﬁnd the best regularization co-\nefﬁcient and report results from the model trained\nwith this coefﬁcient. W e later refer to this strategy\nas REG.\n4 Experiments\nInitially , we measure the co-occurrence bias in the\ntraining data. After training the baseline model,\nwe implement our loss function and tune for the\nλ hyperparameter. W e test the existing debias-\ning approaches, CDA and REG, as well but since\nBordia and Bowman (2019) reported that results\nﬂuctuate substantially with different REG regu-\nlarization coefﬁcients, we perform hyperparame-\nter tuning and report the best results in T able\n2.\nAdditionally , we implement a combination of our\nloss function and CDA and tune for λ. Finally ,\nbias evaluation is performed for all the trained\nmodels. Causal occupation bias is measured di-\nrectly from the models using template datasets dis-\ncussed above and co-occurrence bias is measured\nfrom the model-generated texts, which consist of\n10,000 documents of 500 words each.\n4.1 Results\nResults for the experiments are listed in T able\n2.\nIt is interesting to observe that the baseline model\nampliﬁes the bias in the training data set as mea-\nsured by BN and BN\nc . From measurements us-\ning the described bias metrics, our method effec-\ntively mitigates bias in language modelling with-\nModel BN BN\nc GR P pl. CB |o CB |g EB d\nDataset 0.340 0.213 - - - -\nBaseline 0.531 0.282 1.415 117.845 1.447 97.762 0.528\nREG 0.381 0.329 1.028 114.438 1.861 108.740 0.373\nCDA 0.208 0.149 1.037 117.976 0.703 56.82 0.268\nλ0.01 0.492 0.245 1.445 118.585 0.111 9.306 0.077\nλ0.1 0.459 0.208 1.463 118.713 0.013 2.326 0.018\nλ0.5 0.312 0.173 1.252 120.344 0.000 1.159 0.006\nλ0.8 0.226 0.151 1.096 119.792 0.001 1.448 0.002\nλ1 0.218 0.153 1.049 120.973 0.000 0.999 0.002\nλ2 0.221 0.157 1.020 123.248 0.000 0.471 0.000\nλ 0.5 + CDA 0.205 0.145 1.012 117.971 0.000 0.153 0.000\nT able 2: Evaluation results for models trained on Daily Mail and their generated texts\nout a signiﬁcant increase in perplexity . At λ value\nof 1, it reduces BN by 58.95%, BN\nc by 45.74%,\nCB |o by 100%, CB |g by 98.52% and EBd by\n98.98%. Compared to the results of CDA and\nREG, it achieves the best results in both occupa-\ntion biases, CB |g and CB |o, and EBd. W e notice\nthat all methods result in GR around 1, indicat-\ning that there are near equal amounts of female\nand male words in the generated texts. In our ex-\nperiments we note that with increasing λ, the bias\nsteadily decreases and perplexity tends to slightly\nincrease. This indicates that there is a trade-off\nbetween bias and perplexity .\nREG is not very effective in mitigating bias\nwhen compared to other methods, and fails to\nachieve the best result in any of the bias metrics\nthat we used. But REG results in the best perplex-\nity and even does better than the baseline model in\nthis respect. This indicates that REG has a slight\nregularization effect. Additionally , it is interesting\nto note that our loss function outperforms REG\nin EBd even though REG explicitly aims to re-\nduce gender bias in the embeddings. Although\nour method does not explicitly attempt geomet-\nric debiasing of the word embedding, the results\nshow that it results in the most debiased embed-\nding as compared to other methods. Furthermore,\nGonen and Goldberg (2019) emphasizes that ge-\nometric gender bias in word embeddings is not\ncompletely understood and existing word embed-\nding debiasing strategies are insufﬁcient. Our ap-\nproach provides an appealing end-to-end solution\nfor model debiasing without relying on any mea-\nsure of bias in the word embedding. W e believe\nthis concept is generalizable to other NLP appli-\ncations.\nOur method outperforms CDA in CB |g, CB |o,\nand EBd. While CDA achieves slightly better re-\nsults for co-occurrence biases, BN and BN\nc , and\nresults in a better perplexity . With a marginal\ndifferences, our results are comparable to those\nof CDA and both models seem to have similar\nbias mitigation effects. However, our method does\nnot require a data augmentation step and allows\ntraining of an unbiased model directly from bi-\nased datasets. For this reason, it also requires less\ntime to train than CDA since its training data has\na smaller size without data augmentation. Fur-\nthermore, CDA fails to effectively mitigate occu-\npation bias when compared to our approach. Al-\nthough the training data for CDA does not con-\ntain gender bias, the model still exhibits some gen-\nder bias when measured with our causal occupa-\ntion bias metrics. This reinforces the concept that\nsome model-level constraints are essential to debi-\nasing a model and dataset debiasing alone cannot\nbe trusted.\nFinally , we note that the combination of CDA\nand our loss function outperforms all the methods\nin all measures of biases without compromising\nperplexity . Therefore, it can be argued that a cas-\ncade of these approaches can be used to optimally\ndebias the language models.\n5 Conclusion and Discussion\nIn this research, we propose a new approach for\nmitigating gender bias in neural language models\nand empirically show its effectiveness in reducing\nbias as measured with different evaluation metrics.\nOur research also highlights the fact that debias-\ning the model with bias penalties in the loss func-\ntion is an effective method. W e emphasize that\nloss function based debiasing is powerful and gen-\neralizable to other downstream NLP applications.\nThe research also reinforces the idea that geomet-\nric debiasing of the word embedding is not a com-\nplete solution for debiasing the downstream appli-\ncations but encourages end-to-end approaches to\ndebiasing.\nAll the debiasing techniques experimented in\nthis paper rely on a predeﬁned set of gender pairs\nin some way . CDA used gender pairs for ﬂipping,\nREG uses it for gender space deﬁnition and our\ntechnique uses them for computing loss. This re-\nliance on pre-deﬁned set of gender pairs can be\nconsidered a limitation of these methods. It also\nresults in another concern. There are gender asso-\nciated words which do not have pairs, like preg-\nnant. These words are not treated properly by\ntechniques relying on gender pairs.\nFuture work includes designing a context-aware\nversion of our loss function which can distinguish\nbetween the unbiased and biased mentions of the\ngendered words and only penalize the biased ver-\nsion. Another interesting direction is exploring the\napplication of this method in mitigating racial bias\nwhich brings more challenges.\n6 Acknowledgment\nW e are grateful to Sam Bowman for helpful ad-\nvice, Shikha Bordia, Cuiying Y ang, Gang Qian,\nXiyu Miao, Qianyi Fan, Tian Liu, and Stanislav\nSobolevsky for discussions, and reviewers for de-\ntailed feedback.\nReferences\nT olga Bolukbasi, Kai-W ei Chang, James Zou,\nV enkatesh Saligrama, and Adam Kalai. 2016.\nMan is to computer programmer as woman is to homemaker? debia sing word embeddings .\nIn NIPS’16 Proceedings of the 30th International\nConference on Neural Information Processing\nSystems, pages 4356–4364.\nShikha Bordia and Samuel R. Bowman. 2019.\nIdentifying and reducing gender bias in word-level languag e models .\nArXiv:1904.03035.\nHila Gonen and Y oav Goldberg. 2019.\nLipstick on a pig: Debiasing methods cover up systematic gen der biases in word embeddings but do not remove them .\nArXiv:1903.03862.\nKarl Hermann, T om Koisk, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa\nSuleyman, and Phil Blunsom. 2015.\nT eaching machines to read and comprehend . In\nNIPS’15 Proceedings of the 28th International\nConference on Neural Information Processing\nSystems, pages 1693–1701.\nAnja Lambrecht and Catherine E. Tucker. 2018.\nAlgorithmic bias? an empirical study into apparent gender- based discrimination in the display of stem career ads .\nIssie Lapowsky. 2018.\nGoogle autocomplete still makes vile suggestions .\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Pree-\ntam Amancharla, and Anupam Datta. 2018.\nGender bias in neural natural language processing .\nArXiv:1807.11714v1.\nJeffrey Pennington, Richard Socher,\nand Christopher Manning. 2014.\nGlove: Global vectors for word representation .\nIn Proceedings of the 2014 Conference on Em-\npirical Methods in Natural Language Processing ,\npage 15321543. Association for Computational\nLinguistics.\nIlya Sutskever, James Martens,\nand Geoffrey Hinton. 2011.\nGenerating text with recurrent neural networks .\nIn ICML’11 Proceedings of the 28th Interna-\ntional Conference on International Conference on\nMachine Learning, pages 1017–1024.\nJieyu Zhao, Tianlu W ang, Mark Y atskar, V i-\ncente Ordonez, and Kai-W ei Chag. 2017.\nMen also like shopping: Reducing gender bias ampliﬁcation u sing corpus-level constraints .\nIn Conference on Empirical Methods in Natural\nLanguage Processing.\nJieyu Zhao, Y ichao Zhou, Zeyu Li,\nW ei W ang, and Chang Kaiwei. 2018.\nLearning gender-neutral word embeddings . In\nProceedings of the 2018 Conference on Empir-\nical Methods in Natural Language Processing ,\npage 48474853. Association for Computational\nLinguistics.",
  "topic": "Debiasing",
  "concepts": [
    {
      "name": "Debiasing",
      "score": 0.988523006439209
    },
    {
      "name": "Perplexity",
      "score": 0.9131090641021729
    },
    {
      "name": "Computer science",
      "score": 0.7192729115486145
    },
    {
      "name": "Function (biology)",
      "score": 0.5875666737556458
    },
    {
      "name": "Word (group theory)",
      "score": 0.582979142665863
    },
    {
      "name": "Gender bias",
      "score": 0.5506479740142822
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5122236609458923
    },
    {
      "name": "Natural language processing",
      "score": 0.4397597908973694
    },
    {
      "name": "Language model",
      "score": 0.43113645911216736
    },
    {
      "name": "Machine learning",
      "score": 0.3823871314525604
    },
    {
      "name": "Psychology",
      "score": 0.2294434905052185
    },
    {
      "name": "Linguistics",
      "score": 0.12736064195632935
    },
    {
      "name": "Social psychology",
      "score": 0.10949793457984924
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}