{
  "title": "A Compare Aggregate Transformer for Understanding Document-grounded Dialogue",
  "url": "https://openalex.org/W3098595900",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5040643252",
      "name": "Longxuan Ma",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5102020195",
      "name": "Weinan Zhang",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5101094265",
      "name": "Runxin Sun",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100418171",
      "name": "Ting Liu",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963475460",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2551396370",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2997300509",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W2963945575",
    "https://openalex.org/W2962717182",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2972664115",
    "https://openalex.org/W2966404868",
    "https://openalex.org/W2891103209",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963790827",
    "https://openalex.org/W2586847566",
    "https://openalex.org/W2998083599",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2952592807",
    "https://openalex.org/W2922791555",
    "https://openalex.org/W2951508633",
    "https://openalex.org/W2799176105",
    "https://openalex.org/W2945525091",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W2995183464",
    "https://openalex.org/W2945052683",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2891826200",
    "https://openalex.org/W2951225599",
    "https://openalex.org/W2752047430",
    "https://openalex.org/W3022187094",
    "https://openalex.org/W3006065545",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2950902819",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2986867746",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2807873315",
    "https://openalex.org/W2963520511"
  ],
  "abstract": "Unstructured documents serving as external knowledge of the dialogues help to generate more informative responses. Previous research focused on knowledge selection (KS) in the document with dialogue. However, dialogue history that is not related to the current dialogue may introduce noise in the KS processing. In this paper, we propose a Compare Aggregate Transformer (CAT) to jointly denoise the dialogue context and aggregate the document information for response generation. We designed two different comparison mechanisms to reduce noise (before and during decoding). In addition, we propose two metrics for evaluating document utilization efficiency based on word overlap. Experimental results on the CMU_DoG dataset show that the proposed CAT model outperforms the state-of-the-art approach and strong baselines.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1358–1367\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n1358\nA Compare Aggregate Transformer for Understanding\nDocument-grounded Dialogue\nLongxuan Ma, Weinan Zhang, Runxin Sun, Ting Liu\nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology, Harbin, Heilongjiang, China\n{lxma,wnzhang,rxsun,tliu}@ir.hit.edu.cn\nAbstract\nUnstructured documents serving as external\nknowledge of the dialogues help to generate\nmore informative responses. Previous research\nfocused on knowledge selection (KS) in the\ndocument with dialogue. However, dialogue\nhistory that is not related to the current dia-\nlogue may introduce noise in the KS process-\ning. In this paper, we propose a Compare\nAggregate Transformer (CAT) to jointly de-\nnoise the dialogue context and aggregate the\ndocument information for response generation.\nWe designed two different comparison mecha-\nnisms to reduce noise (before and during de-\ncoding). In addition, we propose two met-\nrics for evaluating document utilization efﬁ-\nciency based on word overlap. Experimental\nresults on the CMU DoG dataset show that the\nproposed CAT model outperforms the state-of-\nthe-art approach and strong baselines.\n1 Introduction\nDialogue system (DS) attracts great attention from\nindustry and academia because of its wide appli-\ncation prospects. Sequence-to-sequence models\n(Seq2Seq) (Sutskever et al., 2014; Serban et al.,\n2016) are veriﬁed to be an effective framework for\nthe DS task. However, one problem of Seq2Seq\nmodels is that they tended to generate generic re-\nsponses that provids deﬁcient information Li et al.\n(2016); Ghazvininejad et al. (2018). Previous re-\nsearchers proposed different methods to alleviate\nthis issue. One way is to focus on models’ ability\nto extract information from conversations. Li et al.\n(2016) introduced Maximum Mutual Information\n(MMI) as the objective function for generating di-\nverse response. Serban et al. (2017) proposed a la-\ntent variable model to capture posterior information\nof golden response. Zhao et al. (2017) used condi-\ntional variational autoencoders to learn discourse-\nlevel diversity for neural dialogue models. The\nDocument:\n Movie Name: The Shape of Water. Year: 2017. Director:\n Guillermo del Toro. Genre: Fantasy, Drama.Cast: Sally \n Hawkins as Elisa Esposito, a mute cleaner who works at\n a secret government laboratory. ... Critical Response: one\n of del Toro's most stunningly successful works ... \nDialogue: \nS1: I thought The Shape of Water was one of Del Toro's \nbest works. What about you?\nS2: Yes, his style really extended the story.\nS1: I agree. He has a way with fantasy elements that real-\nly helped this story be truly beautiful. It has a very high r-\nating on rotten tomatoes, too.\nS2: Sally Hawkins acting was phenomenally expressiv-\ne. Didn't feel her character was mentally handicapped.\nS1: The characterization of her as such was definitely off \nthe mark. \nFigure 1: One DGD example in the CMUDoG dataset.\nS1/S2 means Speaker-1/Speaker-2, respectively.\nother way is introducing external knowledge, ei-\nther unstructured knowledge texts Ghazvininejad\net al. (2018); Ye et al. (2019); Dinan et al. (2019)\nor structured knowledge triples (Liu et al., 2018;\nYoung et al., 2018; Zhou et al., 2018a) to help\nopen-domain conversation generation by produc-\ning responses conditioned on selected knowledge.\nThe Document-grounded Dialogue (DGD)\n(Zhou et al., 2018b; Zhao et al., 2019; Li et al.,\n2019) is a new way to use external knowledge. It\nestablishes a conversation mode in which relevant\ninformation can be obtained from the given docu-\nment. One example of DGD is presented in Figure\n1. Two interlocutors talk about the given document\nand freely reference the text segment during the\nconversation.\nTo address this task, two main challenges need\nto be considered in a DGD model: 1) Determining\nwhich of the historical conversations are related\nto the current conversation, 2) Using current con-\nversation and the related conversation history to\nselect proper document information and to gener-\n1359\nate an informative response. Previous work Arora\net al. (2019); Zhao et al. (2019); Qin et al. (2019);\nTian et al. (2020); Ren et al. (2019) generally fo-\ncused on selecting knowledge with all the conversa-\ntions. However, the relationship between historical\nconversations and the current conversation has not\nbeen studied enough. For example, in Figure 1, the\nitalics utterance from user1, ”Yes, his style really\nextended the story.”, is related to dialogue history.\nWhile the black fold utterance from user1, ”Sally\nHawkins acting was phenomenally expressive.\nDidn’t feel her character was mentally handi-\ncapped.”, has no direct relationship with the his-\ntorical utterances. when employing this sentence\nas the last utterance, the dialogue history is not\nconducive to generate a response.\nIn this paper, we propose a novel Transformer-\nbased (Vaswani et al., 2017) model for under-\nstanding the dialogues and generate informative\nresponses in the DGD, named Compare Aggre-\ngate Transformer (CAT). Previous research (Sankar\net al., 2019) has shown that the last utterance is the\nmost important guidance for the response genera-\ntion in the multi-turn setting. Hence we divide the\ndialogue into the last utterance and the dialogue\nhistory, then measure the effectiveness of the dia-\nlogue history. If the last utterance and the dialogue\nhistory are related, we need to consider all the con-\nversations to ﬁlter the document information. Oth-\nerwise, the existence of dialogue history is equal to\nthe introduction of noise, and its impact should be\neliminated conditionally. For this purpose, on one\nside, the CAT ﬁlters the document information with\nthe last utterance; on the other side, the CAT uses\nthe last utterance to guide the dialogue history and\nemploys the guiding result to ﬁlter the given doc-\nument. We judge the importance of the dialogue\nhistory by comparing the two parts, then aggre-\ngate the ﬁltered document information to generate\nthe response. Experimental results show that our\nmodel can generate more relevant and informative\nresponses than competitive baselines. When the di-\nalogue history is less relevant to the last utterance,\nour model is veriﬁed to be even more effective. The\nmain contributions of this paper are:\n(1) We propose a compare aggregate method to\ndetermine the relationship between the historical di-\nalogues and the last utterance. Experiments showed\nthat our model outperformed strong baselines on\nthe CMU DoG dataset.\n(2) We propose two new metrics to evaluate the\ndocument knowledge utilization in the DGD. They\nare both based on N-gram overlap among generated\nresponse, the dialogue, and the document.\n2 Related Work\nThe DGD maintains a dialogue pattern where ex-\nternal knowledge can be obtained from the given\ndocument. Most recently, some DGD datasets\nZhou et al. (2018b); Moghe et al. (2018); Qin et al.\n(2019); Gopalakrishnan et al. (2019) have been\nreleased to exploiting unstructured document infor-\nmation in conversations.\nModels trying to address the DGD task can be\nclassiﬁed into two categories based on their en-\ncoding process with dialogues: one is parallel\nmodeling and the other is incremental modeling.\nFor the ﬁrst category, Moghe et al. (2018) used\na generation-based model that learns to copy in-\nformation from the background knowledge and a\nspan prediction model that predicts the appropriate\nresponse span in the background knowledge. Liu\net al. (2019) claimed the ﬁrst to unify knowledge\ntriples and long texts as a graph. Then employed\na reinforce learning process in the ﬂexible multi-\nhop knowledge graph reasoning process. To im-\nprove the process of using background knowledge,\n(Zhang et al., 2019) ﬁrstly adopted the encoder\nstate of the utterance history context as a query to\nselect the most relevant knowledge, then employed\na modiﬁed version of BiDAF (Seo et al., 2017) to\npoint out the most relevant token positions of the\nbackground sequence. Meng et al. (2019) used a\ndecoding switcher to predict the probabilities of\nexecuting the reference decoding or generation de-\ncoding. Some other researchers (Zhao et al., 2019;\nArora et al., 2019; Qin et al., 2019; Meng et al.,\n2019; Ren et al., 2019) also followed this parallel\nencoding method. For the second category, Kim\net al. (2020) proposed a sequential latent knowl-\nedge selection model for Knowledge-Grounded Di-\nalogue. Li et al. (2019) designed an incremental\ntransformer to encode multi-turn utterances along\nwith knowledge in the related document. Mean-\nwhile, a two-way deliberation decoder (Xia et al.,\n2017) was used for response generation. However,\nthe relationship between the dialogue history and\nthe last utterance is not well studied. In this pa-\nper, we propose a compare aggregate method to\ninvestigate this problem. It should be pointed out\nthat when the target response changes the topic, the\ntask is to detect whether the topic is ended and to\n1360\nself-attention\nself-attention\nutter-attention\n  doc-attention   doc-attention\nLast Utterance\nfeed forward feed forward\nDecoder\nDocument\nUtterance History\nLast Utterance\nResponse\nN×N×\nself-attention self-attention\n  doc-attention   doc-attention\nfeed forward\nDocument\nResponse WordResponse Word\nutter-attention\nfeed forward\nutter-attention\nD final\nR1\nR2\nself-attention\n  self-attention\nfeed forward\nResponse Word\nMerge attention\n  self-attention\n~Dn Dn\nUtterance attention\nR1\n    Last \nUtterance\n    Last \nUtterance\nN× N×\nN×\n(a) CAT Model (b) Deliberation Decoder (c) Transformer Enhanced Decoder\nDn ~Dn\nFigure 2: The architecture of the CAT model. ”utter” is short for utterance. ”doc” is short for document.\ninitiate a new topic (Akasaki and Kaji, 2019). We\ndo not study the conversation initiation problem in\nthis paper, although we may take it as future work.\n3 The Proposed CAT Model\n3.1 Problem Statement\nThe inputs of the CAT model are the given docu-\nment D = (D1, D2, ..., Dd) with dwords, dialogue\nhistory H = (H1, H2, ..., Hh) with h words and\nthe last utterance L = (L1, L2, ..., Ll) with lwords.\nThe task is to generate the response R = (R1, R2,\n..., Rr) with rtokens with probability:\nP(R|H,L,D; Θ) =\nr∏\ni=1\nP(Ri|H,L,D,R<i; Θ),\n(1)\nwhere R<i = ( R1, R2, ..., Ri−1), Θ is the\nmodel’s parameters.\n3.2 Encoder\nThe structure of the CAT model is shown in Figure\n2. The hidden dimension of the CAT model is ˆh.\nWe use the Transformer structure (Vaswani et al.,\n2017). The self-attention is calculated as follow:\nAttention(Q,K,V) =softmax(QKT\n√dk\n)V, (2)\nwhere Q, K, and V are the query, the key, and the\nvalue, respectively; dk is the dimension of Q and\nK. The encoder and the decoder stack N (N = 3\nin our work) identical layers of multihead attention\n(MAtt):\nMAtt(Q,K,V) =[A1,..., An]WO, (3)\nAi = Attention(QWQ\ni ,KWK\ni ,VWV\ni ), (4)\nwhere WQ\ni ,WK\ni ,WV\ni (i= 1,...,n ) and WO are\nlearnable parameters.\nThe encoder of CAT consists of two branches\nas ﬁgure 2 (a). The left branch learns the infor-\nmation selected by dialogue history H, the right\npart learns the information chosen by the last ut-\nterance L. After self-attention process, we get\nHs = MAtt(H,H,H) and Ls = MAtt(L,L,L).\nThen we employ Ls to guide the H. H1 =\nMAtt(Ls,H,H), where H1 is the hidden state\nat the ﬁrst layer. Then we adopt H1 to se-\nlect knowledge from the document D, D1 =\nFF(MAtt(H1,D,D)). FF is the feed-forward pro-\ncess. In the second layer, D1 is the input, D1\ns =\nMAtt(D1,D1,D1)), H2 = MAtt(D1\ns,H,H), D2 =\nFF(MAtt(H2,D,D)). After N layers, we obtain\nthe information Dn selected by H. In the right\nbranch, we use Ls to ﬁlter the D. ˜Dn is the in-\nformation selected by L.\n3.3 Comparison Aggregate\nAs demonstrated by (Sankar et al., 2019), the last\nutterance played an fundamental role in response\ngeneration. We need to preserve the document in-\nformation ﬁltered by L, and determine how much\ninformation selected by H is needed. We propose\n2 different compare aggregate methods: one is\nconcatenation before decoding and the other is at-\ntended comparison in the decoder.\n1361\n3.3.1 Concatenation\nWe use average pooling to Hs and Ls to get their\nvector representations Hsa and Lsa ∈ Rˆh∗1, re-\nspectively. The concatenation method calculates\nrelevance score αto determine the importance of\nDn as follow:\nα=tanh(HsaWH + LsaWL), (5)\nDfinal =[sigmoid(Wαα) ∗Dn; ˜D\nn\n], (6)\nwhere WH, WL ∈Rˆh∗ˆh, Wα ∈R1∗ˆh are learn-\nable parameters. [X; Y] is the concatenation of X\nand Y in sentence dimension. ∗is the element-wise\nmultiplication. Note that the Dn is guided by H,\nthe concatenation method performs a second level\ncomparison with H and L and then transfers the\ntopic-aware Dfinal to the two-pass Deliberation De-\ncoder (DD) (Xia et al., 2017). The structure of the\nDD is shown in Figure 2 (b). The ﬁrst-pass takes L\nand Dfinal as inputs and learns to generate a con-\ntextual coherently response R1. The second-pass\ntakes R1 and the document D as inputs and learns\nto inject document knowledge. The DD aggregates\ndocument, conversation, and topic information to\ngenerate the ﬁnal response R2. Loss is from both\nthe ﬁrst and the second layers:\nL= −\nM∑\nm=1\nr∑\ni=1\n(logP(R1\ni) + logP(R2\ni)), (7)\nwhere M is the total training example; R1\ni and\nR2\ni are the i-th word generated by the ﬁrst and\nsecond decoder layer, respectively.\n3.3.2 Attended Comparison\nWe employ an Enhanced Decoder (Zheng and\nZhou, 2019) to perform the attended comparing.\nThe structure of our Enhanced Decoder is illus-\ntrated in Figure 2 (c). It accepts Dn, ˜D\nn\nand the\nresponse R as inputs, applying a different way to\ncompare and aggregate. The merge attention com-\nputes weight across all inputs:\nP =[R; Dn; ˜D\nn\n]WP, (8)\nVmerge =PRR + PDDn + P˜D\n˜D\nn\n, (9)\nwhere WP is learnable parameters. The dimen-\nsion of P is 3. PR, PD and P˜D are the Softmax\nresults of P. Vmerge and L are used for next utter-\nance attention as shown in Figure 2 (c). The output\nof the Enhanced Decoder is connected to the sec-\nond layer of DD and we deﬁne this new structure as\nEnhanced Deliberation Decoder (EDD). The loss\nis the same as Eq. (7).\n4 Experiments\n4.1 Dataset\nWe evaluate our model with the CMU DoG (Zhou\net al., 2018b) dataset. There are 4112 dialogs based\non 120 documents in the dataset. One document\ncontains 4 sections, such as movie introduction\nand scenes. A related section is given for every\nseveral consequent utterances. However, the con-\nversations are not constrained to the given section.\nIn our setting, we use the full document (with 4\nsection) as external knowledge. The average length\nof documents is around 800 words. We concate-\nnate consequent utterances of the same person as\none utterance. When training, we remove the ﬁrst\ntwo or three rounds of greeting sentences. Each\nsample contains one document, two or more histor-\nical utterances, one last utterance, and one golden\nresponse. When testing, we use two different ver-\nsions of the test set. The ﬁrst follows the process\nof training data, we name it Reduced version. The\nsecond is constructed by comparing the original\ndocument section of the conversation based, we\npreserve the examples that the dialogue history and\nthe last utterance are based on different document\nsections. For example, dialogue history is based\non section 2, the last utterance and response are\nbased on section 3. We name it Sampled version\nand it is used for testing our models’ comprehend-\ning ability of the topic transfer in conversations.\nThe data statistics are shown in Table 1. Please\nrefer to Zhou et al. (2018b) for more details. It\nis worth noting that the sampled version does not\nrepresent the proportion of all conversation topic\ntransfers, but it demonstrates this problem better\nthan the Reduced version. We also test our method\non the Holl-E Moghe et al. (2018) dataset. Since\nthe processing of the dataset and the experimental\nconclusions obtained are similar to CMU DoG, we\ndid not present in this article.\n4.2 Baselines\nWe evaluated several competitive baselines.\n1362\nDataset U.Num(train / dev / test) W/Utter\nOriginal 72922 / 3626 / 11577 18.6\nReduced 66332 / 3269 / 10502 19.7\nSampled 66332 / 3269 / 1317 19.6\nTable 1: Statistics of the CMU DoG dataset. ”U.Num”\nmeans Utterances Numbers, ”W/Utter” means average\nwords per utterance.\n4.2.1 RNN-based models\nVHRED: A Hierarchical Latent Variable Encoder-\nDecoder Model (Serban et al., 2017), which intro-\nduces a global (semantic level) latent variable Z\nfor the problem that HRED (Serban et al., 2016) is\ndifﬁcult to generate meaningful and high-quality\nreplies. Z is calculated with the encoder RNN\noutputs and the context RNN outputs. The latent\nvariable Z contains some high-level semantic in-\nformation, which encourages the model to extract\nabstract semantic concepts. Please refer to Ser-\nban et al. (2017) for more details. We use Z to\ncapture the topic transfer in conversations and test\nthree different settings. For the ﬁrst setting, we\ndo not employ the document knowledge, only use\ndialogue as input to generate the response. It is\nrecorded as VHRED(-k). For the second one, we\nuse the same encoder RNN with shared parameters\nto learn the representation of the document and the\nutterance, then concatenate the ﬁnal hidden state\nof them as the input of the context RNN. It is de-\nnoted by VHRED(c). For the third one, we use\nword-level dot-attention (Luong et al., 2015) to get\nthe document-aware utterance representation and\nuse it as the input of context RNN. It is termed as\nVHRED(a).\n4.2.2 Transformer-based models\nT-DD/T-EDD: They both use the Transformer as\nthe encoder. The inputs are the concatenation of\ndialogues and the document. These two models\nparallel encode the dialogue without detecting topic\ntransfer. The T-DD uses a Deliberation Decoder\n(DD) as the decoder. The T-EDD uses an Enhanced\nDeliberation Decoder (EDD) as the decoder.\nITDD (Li et al., 2019): It uses Incremental\nTransformer Encoder (ITE) and two-pass Delib-\neration Decoder (DD). Incremental Transformer\nuses multi-head attention to incorporate document\nsections and context into each utterance’s encod-\ning process. ITDD incrementally models dialogues\nwithout detecting topic transitions.\n4.3 Evaluation Metrics\nAutomatic Evaluation: We employ perplexity\n(PPL) (Bengio et al., 2000), BLEU (Papineni et al.,\n2002) and ROUGE (Lin, 2004). The PPL of the\ngold response is measured, lower perplexity in-\ndicates better performance. BLEU measures the\nn-gram overlap between a generated response and\na gold response. Since there is only one reference\nfor each response, BLEU scores are extremely low.\nROUGE measures the n-gram overlap based on the\nrecall rate. Since the conversations are constrained\nby the background material, ROUGE is reliable.\nWe also introduce two metrics to automatically\nevaluate the Knowledge Utilization (KU), they\nare both based on N-grams overlaps. We deﬁne\none document, conversations and generated re-\nsponse in Test set as ( D,C,R). The N-grams set\nof each (D,C,R) are termed as GN\nd ,GN\nc and GN\nr ,\nrespectively. The number of overlapped N-grams\nof GN\nd and GN\nr is recorded as GN\ndr. Tuples which\nare in GN\ndr but not in GN\nc is named GN\ndr−c. Then\nKU = len(GN\ndr−c)/len(GN\ndr) reﬂects how many\nN-grams in the document are used in the generated\nreplies, len(G) is the tuple number inG. The larger\nthe KU is, the more N-grams of the document is\nutilized. Since low-frequency tuples may be more\nrepresentative of text features, we deﬁne the recip-\nrocal of the frequency of each tuple kin G as RG\nk,\nwhich represents the importance of a tuple. Then\nthe Quality of Knowledge Utilization (QKU)is\ncalculated as:\nQKU =\n∑\n(D,C,R)\n∑\nkRGr\nk∑\nkRGd\nk\n, k ∈Gdr−c. (10)\nIf RGr\nk is more important in response and RGd\nk is\nless important in document, the QKU will become\neven larger. So the smaller QKU means the higher\nquality of the used document knowledge.\nHuman Evaluation: We randomly sampled100\nconversations from the Sampled test set and ob-\ntained 800 responses from eight models. We have\n5 graduate students as judges. They score each\nresponse with access to previous dialogues and the\ndocument. We use three metrics: Fluency, Co-\nherence, and Informativeness. Fluency measures\nwhether the response is a human-like utterance. Co-\nherence measures if the response is coherent with\nthe dialogue context. Informativeness measures if\nthe response contains relevant and correct informa-\ntion from the document. They are scored from 1 to\n1363\nModel PPL BLEU (%) ROUGE-L KU-2/3 (%) QKU-2/3\nVHRED(-k) 97.3⋄ (99.3)* 0.49* (0.49)* 7.80* (7.82)* –/– (–/–) –/– (–/–)\nVHRED(c) 80.2⋄ (85.4)* 0.79* (0.77)* 8.64* (8.63)* 12.0/27.0⋄ (12.1/27.6)⋄ 3.36/2.82⋄ (3.35/2.80)⋄\nVHRED(a) 77.2⋄ (78.5)* 0.84* (0.80)* 8.98* (8.99)* 13.7/31.7⋄ (13.1/31.3)* 3.23/2.72* (3.23/2.72)*\nT-DD 18.2* (20.5)* 0.90* (0.89)* 9.23* (9.24)* 8.0/23.1* (8.0/23.0)* 2.55/1.94* (2.55/1.95)*\nT-EDD 18.2* (20.3)* 0.91* (0.90)* 9.35* (9.36)* 8.3/23.5* (8.1/23.4)* 2.45/1.91* (2.45/1.92)*\nITDD 16.2* (18.7)* 1.01* (0.99)* 10.12⋄ (10.10)* 9.0/24.5* (9.1/24.4)* 2.18/1.84* (2.15/1.82)*\nCAT-EDD 16.0* (18.2)* 1.14* (1.14)* 11.10* (11.12)* 9.5/24.8* (9.7/24.9)* 2.12/1.77* (2.11/1.76)*\nCAT-DD 15.2 (16.1) 1.22 (1.21) 11.22 (11.22) 11.0/26.5 (11.1/26.4) 2.08/1.64 (2.05/1.62)\nTable 2: Automatic evaluations on the CMU DoG Dataset. ·(·) means Reduced (Sampled) test data. We take the\nCAT-DD as the base model to do the signiﬁcant test,⋄and * stands p<0.05 and p<0.01, respectively.\n5 (1:very bad, 2:bad, 3:acceptable, 4:good, 5:very\ngood). Overall inter-rater agreement measured by\nFliess’ Kappa is0.32 (”fair”).\n4.4 Experimental Setup\nWe use OpenNMT-py (Klein et al., 2017) as the\ncode framework. For all models, the pre-trained\n300 dimension word embedding (Mikolov et al.,\n2013) is shared by dialogue, document, and gen-\nerated responses, the dimension of the hidden\nsize is 300. For the RNN-based models, 3-layer\nbidirectional GRU and 3-layer GRU are applied\nfor encoder and decoder, respectively. For the\nTransformer-based models, the layers of both en-\ncoder and decoder are set to 3, the number of heads\nin multi-head attention is 8 and the ﬁlter size is\n2048. We use Adam ( α = 0.001, β1 = 0.9, β2 =\n0.999, and ϵ= 10−8) (Kingma and Ba, 2015) for\noptimization. The beam size is set to 5 in the de-\ncoder. We truncate the words of the document to\n800 and the dialogue utterance to 40. All models\nare trained on a TITAN X (Pascal) GPU. The aver-\nage training time per epoch is around 40 minutes\nfor the Transformer-based models and around 20\nminutes for the RNN-based models.\n5 Analysis\n5.1 Experimental Results study\nTable 2 shows the automatic evaluations for all\nmodels on the Reduced (Sampled) dataset. The\ndialogue history is 2 rounds. We only present\nROUGE-L as ROUGE-1/2 show the same trend\nas ROUGE-L. Through experiments, we can see\nthat the change range of KU-2 ( 8.0-13.7) is less\nthan KU-3 (23.1-31.7) on the Reduced data, indi-\ncating that the KU-3 can better reﬂect the amount\nof knowledge used than KU-2.\nIn the RNN-based models, the VHRED(-k)\ngets the worst PPL/BLEU/ROUGE, which re-\nveals the importance of injecting document knowl-\nedge in the DGD task. We did not calculate the\nKU/QKU of the VHRED(-k) since the model did\nnot use document knowledge. The VHRED(a)\ngets better PPL/BLEU/ROUGE/KU/QKU than the\nVHRED(c) model, which means the smaller gran-\nular extraction of document information beneﬁts\nmore in generating responses.\nAmong the Transformer-based models, The\nITDD model gets better PPL/BLEU/ROUGE-\nL/KU/QKU than the T-DD model, which means\nthe incremental encoding method is stronger than\nparallel encoding. The CAT-EDD and the CAT-DD\nmodels achieve better performance than the T-DD\nand the T-EDD models, respectively. It indicates\nthat our Compare-Aggregate method is helpful to\nunderstand the dialogue. The CAT-EDD model\noutperforms the ITDD model on all metrics, which\nindicates that our CAT module automatically learns\nthe topic transfer between conversation history and\nthe last utterance as we expected. The CAT-EDD\ndoes not perform as good as the CAT-DD, which\nshows that it is necessary to set up an independent\nmechanism to learn topic transfer, rather than auto-\nmatic learning by attentions in the decoder.\nComparing with the RNN-based models, the\nTransformer-based models get better performance\non PPL/BLEU/ROUGE. It proves that the latter is\nbetter in the ability of convergence to the ground\ntruth. The VHRED(c) and the VHRED(a) get bet-\nter KU and worse QKU than the Transformer-based\nmodels. It means that the latent variable models\nincrease the diversity of replies and use more doc-\nument tuples, but their ability to extract unique tu-\nples is not as good as the Transformer-based ones.\nTable 3 shows the manual evaluations for all\nmodels on the Reduced(Sampled) dataset. The\nCAT-DD model gets the highest scores on Flu-\nency/Coherence/Informativeness. When experi-\nmenting with the Sampled test set, we can see that\nthe advantages of our models become greater than\n1364\nModel Flu. Coh. Inf.\nVHRED(-k) 3.71 (3.72) 2.82 (2.72) 3.01 (2.82)\nVHRED(c) 3.73 (3.82) 3.04 (3.11) 3.03 (3.05)\nVHRED(a) 3.84 (3.77) 3.11 (3.14) 3.22 (3.06)\nT-DD 3.84 (3.82) 3.03 (3.06) 3.03 (3.06)\nT-EDD 3.84 (3.83) 3.02 (3.08) 3.05 (3.05)\nITEDD 3.90 (3.91) 3.11 (3.12) 3.43 (3.42)\nCAT-EDD 4.02 (3.93) 3.12 (3.33) 3.33 (3.41)\nCAT-DD 4.09 (4.09) 3.39 (3.43) 3.44 (3.61)\nTable 3: Manual evaluations on the CMUDoG Dataset.\nFlu. /Coh. /Inf. / ·(·) mean Fluency /Coherence /Infor-\nmativeness /Reduced (Sampled) test data, respectively.\nModels PPL BLEU KU-2(%)/QKU-2\nCAT-DD 16.1 1.21 11.1 / 2.05\nw/o-left 19.8* 0.90* 8.2* / 2.56*\nw/o-(5,6) 18.7* 0.93* 9.1* / 2.48⋄\nw/o-(G) 18.2* 0.96* 9.2⋄ / 2.46*\nTable 4: Ablation Study on the Sampled test set. We\ntake the CAT-DD as the base model to do the signiﬁcant\ntest, ⋄and * stand for p<0.05 and p<0.01, respectively.\nw/o means without.\nthe results of the Reduced version in both automatic\nand manual evaluations. Our model shows more\nadvantages in datasets with more topic transfer.\n5.2 Ablation Study\nTable 4 illustrates the ablation study of the CAT-DD\nmodel. w/o-left means the left branch is removed\nand the model degenerates to T-DD which takes\nthe last utterance and document as inputs. We can\nsee that all the automatic evaluation indexes signif-\nicantly reduce, indicating the dialogue history can\nnot be simply ignored. w/o-(5,6) is a model with-\nout Eq. (5) and (6), which is equivalent to simply\nconnect the outputs of the left and the right encoder\nbranches. The results showed that the ability of the\nmodel to distinguish the conversation topic transfer\nis weakened. w/o-(G) is a model removing the utter-\nattention in the left branch, which means wedo not\nuse L to guide the H, the structure of left branch\nchanges to the right branch and the input is H. The\nperformance is declining, which indicates that the\nguiding process is useful. The signiﬁcant tests (two-\ntailed student t-test) on PPL/BLEU/KU-2/QKU-2\nreveal the effectiveness of each component.\n5.3 History Round Study\nWe use the CAT-DD model and the Sampled test\nset to study the inﬂuence of the historical dia-\nlogue rounds. For example, setting dialogue his-\ntory to 0 means we use only the last utterance,\nthe CAT-DD becomes the w/o-left model in the\n0\n0.8\n0.9\n1.0\n1.1\n1.2\nVHRED_BLEU ITEDD_BLEU CATDD_BLEU\n0\n25\n30\n35\n40\nVHRED_KU-3 ITEDD_KU-3 CATDD_KU-3\n0 1 2 3 4\n2.0\n2.5\nVHRED_QKU-3 ITEDD_QKU-3 CATDD_QKU-3\nFigure 3: The effect of dialogue history rounds on\nVHRED(a)/ITDD/CAT-DD models. The abscissa rep-\nresents the historical dialogue rounds. The ordinate rep-\nresents the BLEU/KU-3/QKU-3 values.\nablation study. Setting dialogue history to N\nmeans we use N rounds of dialogue history for\nthe input of the left branch. We set the conver-\nsation history to 0/1/2/3/4 to test the response\nof VHRED(a)/ITDD/CAT-DD models. Figure 3\nshows the trend of BLEU/KU-3/QKU-3. The top\nﬁgure shows the BLEU trend, the CAT-DD reaches\nthe maximum when the rounds are 2. The con-\ntinuous increase of rounds does not signiﬁcantly\nimprove the generation effect. In the middle pic-\nture, with the increase of historical dialogue from 0\nto 2, the VHRED(a) and the CAT-DD have a visible\nimprovement on the KU-3, which shows that the\ninformation contained in the historical dialogue can\nbe identiﬁed and affect the extraction of document\ninformation. The ITDD model is not as sensitive\nas the others on the KU-3, indicating that the incre-\nmental encoding structure pays more attention to\nthe information of the last utterance. The bottom\nﬁgure shows the trend of the QKU-3. When the\nhistory dialogue increases, the ITDD model keeps\nstable and the VHRED(a) and the CAT-DD models\nhave a declining trend, which again indicates that\nthe VHRED(a) and the CAT-DD are more sensitive\nto the historical dialogue.\n5.4 History Importance Study\nFigure 4 shows the average sigmoid(Wαα) value\nin the CAT-DD model over the Reduced/Sampled\ntest set and the Validation set. A higher value\nmeans a stronger correlation between the last utter-\nance and the historical dialogue. We can see that\n1365\n1 2 3 4 5 6\n0.20\n0.25\n0.30\n0.35\n0.40\n Reduced test set\nSampled test set\nValidation set\nFigure 4: The rating of dialogue history in the CAT-\nDD model with Reduced and Sampled test set. The ab-\nscissa represents the dialogue rounds and the ordinate\nrepresents the correlation score in the model.\nDocument:\n... sally hawkins as elisa esposito, a mute cleaner who wor-\nks at a secret government laboratory. michael shannon as \ncolonel richard strickland ... rating rotten tomatoes: 92% T-\nhe shape of water is a 2017 american fantasy film ... it stars\nsally hawkins, michael shannon, richard jenkins, Doug jon-\nes, michael stuhlbarg, and octavia spencer  ... \nDialogue history:\nS1: I wonder if it's a government creation or something ca-\nptured from the wild. i would assume the wild. \nS2: It was captured for governmental experiments.\nThe last Utterance:\nS1: Is it a big name cast?\nGroud truth:\nS2: Sally hawkins played the role of the mute cleaner, mic-\nhael shannon played the role of colonel richard strickland.\nGenerated response:\nVHRED(a): it has rating rotten tomatoes: 92%.\nTDD: i am not sure about it. \nITDD: yes, sally hawkins as elisa esposito. \nCAT-DD: sally hawkins, michael shannon, richard jenkins, \ndoug jones, michael stuhlbarg, and octavia spencer.\n(w/o-(5,6)): yes, sally hawkins works at a secret governme-\nnt laboratory.\n(w/o-(G)): it is a 2017 american fantasy film.\nFigure 5: Case study in the CMU DoG Sampled\nDataset. S1/S2 means Speaker-1/Speaker-2, respec-\ntively. (w/o-(5,6)) and (w/o-(G)) are models in the ab-\nlation study.\non the Reduced test set and the Validation set, the\nrelevance score is higher than that of the Sampled\ndata, which proves that the last utterance and the\nhistorical dialogue are more irrelevant in the lat-\nter. Our model captures this change and performs\nbetter on the Sampled data than the Reduced data.\nWhen the historical rounds increase from 1 to 2,\nthe relevance score reduces obviously for all data\nsets, which means the increase of dialogue history\nintroduces more unrelated information. When the\nhistorical conversations increases from 2 to 6, all\ndata have no signiﬁcant change, indicating that in-\ncreasing the dialogue rounds does not improve the\nrecognition ability of the model to the topic change.\n5.5 Case Study\nIn Figure 5, we randomly select an example in the\nSampled test set for a case study. The document,\nthe dialogue history, the last utterance, and the\nground truth are presented. We can observe that the\nlast utterance is irrelevant to the dialogue history.\nThe generated responses of different models are\nlisted below. The VHRED(a) and CAT-DD(w/o-\n(G)) models misunderstand the dialogue and use\nthe wrong document knowledge. The TDD gives a\ngeneric reply. The ITDD model answers correctly\nbut without enough document information. The\nCAT-DD(w/o-(5,6)) model gives a response that\nwas inﬂuenced by the irrelevant historical dialogue\nwhich we want to eliminate. Only the CAT-DD\nmodel generates a reasonable reply and uses the\ncorrect document knowledge, which means it cor-\nrectly understands the dialogues.\n6 Conclusion\nWe propose the Compare Aggregate method to\nunderstand Document-grounded Dialogue (DGD).\nThe dialogue is divided into the last utterance and\nthe dialogue history. The relationship between\nthe two parts is analyzed to denoise the dialogue\ncontext and aggregate the document information\nfor response generation. Experiments show that\nour model outperforms previous work in both au-\ntomatic and manual evaluations. Our model can\nbetter understand the dialogue context and select\nproper document information for response gener-\nation. We also propose Knowledge Utilization\n(KU) and Quality of Knowledge Utilization (QKU),\nwhich are used to measure the quantity and quality\nof the imported external knowledge, respectively.\nIn the future, we will further study the topic transi-\ntion problem and the knowledge injecting problem\nin the DGD.\nAcknowledgments\nThis paper is supported by the National Natu-\nral Science Foundation of China under Grant No.\n62076081, No.61772153 and No.61936010.\nReferences\nSatoshi Akasaki and Nobuhiro Kaji. 2019. Conversa-\ntion initiation by diverse news contents introduction.\nIn NAACL-HLT (1), pages 3988–3998. Association\nfor Computational Linguistics.\nSiddhartha Arora, Mitesh M. Khapra, and Harish G.\nRamaswamy. 2019. On knowledge distillation from\ncomplex networks for response prediction. In\nNAACL-HLT (1), pages 3813–3822. Association for\nComputational Linguistics.\n1366\nYoshua Bengio, R´ejean Ducharme, and Pascal Vincent.\n2000. A neural probabilistic language model. In\nNIPS, pages 932–938. MIT Press.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In ICLR (Poster). OpenReview.net.\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei\nChang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and\nMichel Galley. 2018. A knowledge-grounded neu-\nral conversation model. In AAAI, pages 5110–5117.\nAAAI Press.\nKarthik Gopalakrishnan, Behnam Hedayatnia, Qin-\nlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu\nVenkatesh, Raefer Gabriel, Dilek Hakkani-T ¨ur, and\nAmazon Alexa AI. 2019. Topical-chat: To-\nwards knowledge-grounded open-domain conversa-\ntions. Proc. Interspeech 2019, pages 1891–1895.\nByeongchang Kim, Jaewoo Ahn, and Gunhee\nKim. 2020. Sequential latent knowledge selec-\ntion for knowledge-grounded dialogue. CoRR,\nabs/2002.07510.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel-\nlart, and Alexander Rush. 2017. OpenNMT: Open-\nsource toolkit for neural machine translation. In\nProceedings of ACL 2017, System Demonstrations ,\npages 67–72, Vancouver, Canada. Association for\nComputational Linguistics.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nHLT-NAACL, pages 110–119. The Association for\nComputational Linguistics.\nZekang Li, Cheng Niu, Fandong Meng, Yang Feng,\nQian Li, and Jie Zhou. 2019. Incremental trans-\nformer with deliberation decoder for document\ngrounded conversations. In ACL (1), pages 12–21.\nAssociation for Computational Linguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nShuman Liu, Hongshen Chen, Zhaochun Ren, Yang\nFeng, Qun Liu, and Dawei Yin. 2018. Knowledge\ndiffusion for neural dialogue generation. In ACL (1),\npages 1489–1498. Association for Computational\nLinguistics.\nZhibin Liu, Zheng-Yu Niu, Hua Wu, and Haifeng\nWang. 2019. Knowledge aware conversation gen-\neration with reasoning on augmented graph. CoRR,\nabs/1903.10245.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2015, Lisbon, Portu-\ngal, September 17-21, 2015, pages 1412–1421. The\nAssociation for Computational Linguistics.\nChuan Meng, Pengjie Ren, Zhumin Chen, Christof\nMonz, Jun Ma, and Maarten de Rijke. 2019. Refnet:\nA reference-aware network for background based\nconversation. CoRR, abs/1908.06449.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013. Distributed rep-\nresentations of words and phrases and their com-\npositionality. In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013,\nLake Tahoe, Nevada, United States , pages 3111–\n3119.\nNikita Moghe, Siddhartha Arora, Suman Banerjee, and\nMitesh M. Khapra. 2018. Towards exploiting back-\nground knowledge for building conversation sys-\ntems. In EMNLP, pages 2322–2332. Association for\nComputational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In ACL, pages 311–\n318. ACL.\nLianhui Qin, Michel Galley, Chris Brockett, Xiaodong\nLiu, Xiang Gao, Bill Dolan, Yejin Choi, and Jian-\nfeng Gao. 2019. Conversing by reading: Contentful\nneural conversation with on-demand machine read-\ning. In ACL (1), pages 5427–5436. Association for\nComputational Linguistics.\nPengjie Ren, Zhumin Chen, Christof Monz, Jun Ma,\nand Maarten de Rijke. 2019. Thinking globally,\nacting locally: Distantly supervised global-to-local\nknowledge selection for background based conver-\nsation. CoRR, abs/1908.09528.\nChinnadhurai Sankar, Sandeep Subramanian, Chris Pal,\nSarath Chandar, and Yoshua Bengio. 2019. Do neu-\nral dialog systems use the conversation history ef-\nfectively? an empirical study. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers , pages\n32–37.\nMin Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2017. Bidirectional attention\nﬂow for machine comprehension. In ICLR (Poster).\nOpenReview.net.\nIulian Vlad Serban, Alessandro Sordoni, Yoshua Ben-\ngio, Aaron C. Courville, and Joelle Pineau. 2016.\n1367\nBuilding end-to-end dialogue systems using gener-\native hierarchical neural network models. In Pro-\nceedings of the Thirtieth AAAI Conference on Arti-\nﬁcial Intelligence, February 12-17, 2016, Phoenix,\nArizona, USA, pages 3776–3784.\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,\nLaurent Charlin, Joelle Pineau, Aaron C. Courville,\nand Yoshua Bengio. 2017. A hierarchical latent\nvariable encoder-decoder model for generating di-\nalogues. In Proceedings of the Thirty-First AAAI\nConference on Artiﬁcial Intelligence, February 4-9,\n2017, San Francisco, California, USA, pages 3295–\n3301.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to sequence learning with neural networks.\nIn NIPS, pages 3104–3112.\nZhiliang Tian, Wei Bi, Dongkyu Lee, Lanqing\nXue, Yiping Song, Xiaojiang Liu, and Nevin L.\nZhang. 2020. Response-anticipated memory for on-\ndemand knowledge integration in response genera-\ntion. CoRR, abs/2005.06128.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998–6008.\nYingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,\nNenghai Yu, and Tie-Yan Liu. 2017. Deliberation\nnetworks: Sequence generation beyond one-pass de-\ncoding. In NIPS, pages 1784–1794.\nHao-Tong Ye, Kai-Ling Lo, Shang-Yu Su, and Yun-\nNung Chen. 2019. Knowledge-grounded response\ngeneration with deep attentional latent-variable\nmodel. CoRR, abs/1903.09813.\nTom Young, Erik Cambria, Iti Chaturvedi, Hao Zhou,\nSubham Biswas, and Minlie Huang. 2018. Aug-\nmenting end-to-end dialogue systems with com-\nmonsense knowledge. In AAAI, pages 4970–4977.\nAAAI Press.\nYangjun Zhang, Pengjie Ren, and Maarten de Rijke.\n2019. Improving background based conversation\nwith context-aware knowledge pre-selection. CoRR,\nabs/1906.06685.\nTiancheng Zhao, Ran Zhao, and Maxine Esk ´enazi.\n2017. Learning discourse-level diversity for neural\ndialog models using conditional variational autoen-\ncoders. In ACL (1), pages 654–664. Association for\nComputational Linguistics.\nXueliang Zhao, Chongyang Tao, Wei Wu, Can Xu,\nDongyan Zhao, and Rui Yan. 2019. A document-\ngrounded matching network for response selection\nin retrieval-based chatbots. In IJCAI, pages 5443–\n5449. ijcai.org.\nWen Zheng and Ke Zhou. 2019. Enhancing conver-\nsational dialogue models with grounded knowledge.\nIn CIKM, pages 709–718. ACM.\nHao Zhou, Tom Young, Minlie Huang, Haizhou Zhao,\nJingfang Xu, and Xiaoyan Zhu. 2018a. Com-\nmonsense knowledge aware conversation generation\nwith graph attention. In IJCAI, pages 4623–4629. ij-\ncai.org.\nKangyan Zhou, Shrimai Prabhumoye, and Alan W.\nBlack. 2018b. A dataset for document grounded\nconversations. In EMNLP, pages 708–713. Associa-\ntion for Computational Linguistics.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8173038959503174
    },
    {
      "name": "Computer science",
      "score": 0.7933061718940735
    },
    {
      "name": "Decoding methods",
      "score": 0.6641489863395691
    },
    {
      "name": "Aggregate (composite)",
      "score": 0.5405044555664062
    },
    {
      "name": "Natural language processing",
      "score": 0.5025112628936768
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42976075410842896
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4268796145915985
    },
    {
      "name": "Information retrieval",
      "score": 0.336905837059021
    },
    {
      "name": "Speech recognition",
      "score": 0.3220067024230957
    },
    {
      "name": "Algorithm",
      "score": 0.11384031176567078
    },
    {
      "name": "Engineering",
      "score": 0.08016088604927063
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    }
  ],
  "cited_by": 8
}