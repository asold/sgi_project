{
    "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification",
    "url": "https://openalex.org/W3168124404",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3088555891",
            "name": "Rao, Yongming",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2350858634",
            "name": "Zhao, Wenliang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2529700725",
            "name": "Liu, Benlin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2609714807",
            "name": "Lu, Jiwen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1950278301",
            "name": "Zhou Jie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222521706",
            "name": "Hsieh, Cho-Jui",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3107016329",
        "https://openalex.org/W2982479999",
        "https://openalex.org/W2898170443",
        "https://openalex.org/W3128633047",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W3213165621",
        "https://openalex.org/W1724438581",
        "https://openalex.org/W3137963805",
        "https://openalex.org/W3151130473",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2547875792",
        "https://openalex.org/W2754084392",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W3204563069",
        "https://openalex.org/W3112516115",
        "https://openalex.org/W2963363373",
        "https://openalex.org/W3034429256",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3213601271",
        "https://openalex.org/W3133696297",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3203898101",
        "https://openalex.org/W3139049060",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3116489684"
    ],
    "abstract": "Attention is sparse in vision transformers. We observe the final prediction in vision transformers is only based on a subset of most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. To optimize the prediction module in an end-to-end manner, we propose an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens. Benefiting from the nature of self-attention, the unstructured sparse tokens are still hardware friendly, which makes our framework easy to achieve actual speed-up. By hierarchically pruning 66% of the input tokens, our method greatly reduces 31%~37% FLOPs and improves the throughput by over 40% while the drop of accuracy is within 0.5% for various vision transformers. Equipped with the dynamic token sparsification framework, DynamicViT models can achieve very competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs and vision transformers on ImageNet. Code is available at https://github.com/raoyongming/DynamicViT",
    "full_text": "DynamicViT: Efï¬cient Vision Transformers with\nDynamic Token Sparsiï¬cation\nYongming Rao1 Wenliang Zhao1 Benlin Liu2,3\nJiwen Lu1âˆ— Jie Zhou1 Cho-Jui Hsieh2\n1 Tsinghua University 2 UCLA 3 University of Washington\nAbstract\nAttention is sparse in vision transformers. We observe the ï¬nal prediction in\nvision transformers is only based on a subset of most informative tokens, which is\nsufï¬cient for accurate image recognition. Based on this observation, we propose a\ndynamic token sparsiï¬cation framework to prune redundant tokens progressively\nand dynamically based on the input. Speciï¬cally, we devise a lightweight prediction\nmodule to estimate the importance score of each token given the current features.\nThe module is added to different layers to prune redundant tokens hierarchically. To\noptimize the prediction module in an end-to-end manner, we propose an attention\nmasking strategy to differentiably prune a token by blocking its interactions with\nother tokens. Beneï¬ting from the nature of self-attention, the unstructured sparse\ntokens are still hardware friendly, which makes our framework easy to achieve\nactual speed-up. By hierarchically pruning 66% of the input tokens, our method\ngreatly reduces 31% âˆ¼37% FLOPs and improves the throughput by over 40%\nwhile the drop of accuracy is within 0.5% for various vision transformers. Equipped\nwith the dynamic token sparsiï¬cation framework, DynamicViT models can achieve\nvery competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs\nand vision transformers on ImageNet. Code is available athttps://github.com/\nraoyongming/DynamicViT.\n1 Introduction\nThese years have witnessed the great progress in computer vision brought by the evolution of CNN-\ntype architectures [12, 18]. Some recent works start to replace CNN by using transformer for many\nvision tasks, like object detection [36, 20] and classiï¬cation [25]. Just like what has been done to the\nCNN-type architectures in the past few years, it is also desirable to accelerate the transformer-like\nmodels to make them more suitable for real-time applications.\nOne common practice for the acceleration of CNN-type networks is to prune the ï¬lters that are of less\nimportance. The way input is processed by the vision transformer and its variants, i.e. splitting the\ninput image into multiple independent patches, provides us another orthogonal way to introduce the\nsparsity for the acceleration. That is, we can prune the tokens of less importance in the input instance,\ngiven the fact that many tokens contribute very little to the ï¬nal prediction. This is only possible for\nthe transformer-like models where the self-attention module can take the token sequence of variable\nlength as input, and the unstructured pruned input will not affect the self-attention module, while\ndropping a certain part of the pixels can not really accelerate the convolution operation since the\nunstructured neighborhood used by convolution would make it difï¬cult to accelerate through parallel\ncomputing. Since the hierarchical architecture of CNNs with structural downsampling has improved\nmodel efï¬ciency in various vision tasks, we hope to explore the unstructured and data-dependent\nâˆ—Corresponding author.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2106.02034v2  [cs.CV]  26 Oct 2021\n(a)StructuralDownsamplingconv\nconv\nself-attentionFFN\n(b)Dynamic Token Sparsification\n(c)AttentionVisualization\nFigure 1: Illustration of our main idea. CNN models usually leverage the structural downsam-\npling strategy to build hierarchical architectures as shown in (a). unstructured and data-dependent\ndownsampling method in (b) can better exploit the sparsity in the input data. Thanks to the nature\nof the self-attention operation, the unstructured token set is also easy to accelerate through parallel\ncomputing. (c) visualizes the impact of each spatial location on the ï¬nal prediction in the DeiT-S\nmodel [25] using the visualization method proposed in [ 3]. These results demonstrate the ï¬nal\nprediction in vision transformers is only based on a subset of most informative tokens, which suggests\na large proportion of tokens can be removed without hurting the performance.\ndownsampling strategy for vision transformers to further leverage the advantages of self-attention\n(our experiments also show unstructured sparsiï¬cation can lead to better performance for vision\ntransformers compared to structural downsampling). The basic idea of our method is illustrated in\nFigure 1.\nIn this work, we propose to employ a lightweight prediction module to determine which tokens to be\npruned in a dynamic way, dubbed as DynamicViT. In particular, for each input instance, the prediction\nmodule produces a customized binary decision mask to decide which tokens are uninformative and\nneed to be abandoned. This module is added to multiple layers of the vision transformer, such that\nthe sparsiï¬cation can be performed in a hierarchical way as we gradually increase the amount of\npruned tokens after each prediction module. Once a token is pruned after a certain layer, it will not\nbe ever used in the feed-forward procedure. The additional computational overhead introduced by\nthis lightweight module is quite small, especially considering the computational overhead saved by\neliminating the uninformative tokens.\nThis prediction module can be optimized jointly in an end-to-end manner together with the vision\ntransformer backbone. To this end, two specialized strategies are adopted. The ï¬rst one is to adopt\nGumbel-Softmax [15] to overcome the non-differentiable problem of sampling from a distribution so\nthat it is possible to perform the end-to-end training. The second one is about how to apply this learned\nbinary decision mask to prune the unnecessary tokens. Considering the number of zero elements\nin the binary decision mask is different for each instance, directly eliminating the uninformative\ntokens for each input instance during training will make parallel computing impossible. Moreover,\nthis would also hinder the back-propagation for the prediction module, which needs to calculate the\nprobability distribution of whether to keep the token even if it is ï¬nally eliminated. Besides, directly\nsetting the abandoned tokens as zero vectors is also not a wise idea since zero vectors will still affect\nthe calculation of the attention matrix. Therefore, we propose a strategy called attention masking\nwhere we drop the connection from abandoned tokens to all other tokens in the attention matrix based\non the binary decision mask. By doing so, we can overcome the difï¬culties described above. We\nalso modify the original training objective of the vision transformer by adding a term to constrain\nthe proportion of pruned tokens after a certain layer. During the inference phase, we can directly\nabandon a ï¬xed amount of tokens after certain layers for each input instance as we no longer need to\nconsider whether the operation is differentiable, and this will greatly accelerate the inference.\n2\nWe illustrate the effectiveness of our method on ImageNet using DeiT [ 25] and LV-ViT [16] as\nbackbone. The experimental results demonstrate the competitive trade-off between speed and\naccuracy. In particular, by hierarchically pruning 66% of the input tokens, we can greatly reduce 31%\nâˆ¼37% GFLOPs and improve the throughput by over 40% while the drop of accuracy is within 0.5%\nfor all different vision transformers. Our DynamicViT demonstrates the possibility of exploiting the\nsparsity in space for the acceleration of transformer-like model. We expect our attempt to open a new\npath for future work on the acceleration of transformer-like models.\n2 Related Work\nVision transformers. Transformer model is ï¬rst widely studied in NLP community [26]. It proves\nthe possibility to use self-attention to replace the recurrent neural networks and their variants. Recent\nprogress has demonstrated the variants of transformers can also be a competitive alternative to CNNs\nand achieve promising results on different vision tasks including image classiï¬cation [8, 25, 20, 35,\n23], object detection [2], semantic segmentation [34, 5] and 3D analysis [31, 33]. DETR [2] is the\nï¬rst work to apply the transformer model to vision tasks. It formulates the object detection task as\na set prediction problem and follows the encoder-decoder design in the transformer to generate a\nsequence of bounding boxes. ViT [8] is the ï¬rst work to directly apply transformer architecture on\nnon-overlapping image patches for the image classiï¬cation task, and the whole framework contains\nno convolution operation. Compared to CNN-type models, ViT can achieve better performance with\nlarge-scale pre-training. It is really preferred if the architecture can achieve the state-of-the-art without\nany pre-training. DeiT [25] proposes many training techniques so that we can train the convolution-\nfree transformer only on ImageNet1K [ 7] and achieve better performance than ViT. LV-ViT [16]\nfurther improves the performance by introducing a new training objective called token labeling. Both\nViT and its follow-ups split the input image into multiple independent image patches and transform\nthese image patches into tokens for further process. This makes it feasible to incorporate the sparsity\nin space dimension for these transformer-like models.\nModel acceleration. Model acceleration techniques are important for the deployment of deep\nmodels on edge devices. There are many techniques can be used to accelerate the inference speed of\ndeep model, including quantization [9, 27], pruning [13, 22], low-rank factorization [30], knowledge\ndistillation [14, 19] and so on. There are also many works aims at accelerating the inference speed of\ntransformer models. For example, TinyBERT [17] proposes a distillation method to accelerate the\ninference of transformer. Star-Transformer [10] reduces quadratic space and time complexity to linear\nby replacing the fully connected structure with a star-shaped topology. However, all these works\nfocus on NLP tasks, and few works explore the possibility of making use of the characteristic of\nvision tasks to accelerate vision transformer. Furthermore, the difference between the characteristics\nof Transformer and CNN also makes it possible to adopt another way for acceleration rather than the\nmethods used for CNN acceleration like ï¬lter pruning [13], which removes non-critical or redundant\nneurons from a deep model. Our method aims at pruning the tokens of less importance instead of the\nneurons by exploiting the sparsity of informative image patches.\n3 Dynamic Vision Transformers\n3.1 Overview\nThe overall framework of our DynamicViT is illustrated in Figure 2. Our DynamicViT consists of a\nnormal vision transformer as the backbone and several prediction modules. The backbone network\ncan be implemented as a wide range of vision transformer (e.g., ViT [8], DeiT [25], LV-ViT [16]).\nThe prediction modules are responsible for generating the probabilities of dropping/keeping the\ntokens. The token sparsiï¬cation is performed hierarchically through the whole network at certain\nlocations. For example, given a 12-layer transformer, we can conduct token sparsiï¬cation before the\n4th, 7th, and 10th blocks. During training, the prediction modules and the backbone network can be\noptimized in an end-to-end manner thanks to our newly devised attention masking strategy. During\ninference, we only need to select the most informative tokens according to a predeï¬ned pruning ratio\nand the scores computed by the prediction modules.\n3\nSelf-Attention\nFFN\nPatchEmbedding\nğ‘µğŸÃ— P\nÃ—\nfeaturesdecisions\nsparsify\npredictionmodule\nSelf-Attention\nFFN\nğ‘µğŸÃ—\nâ€¦\nclstoken\nFigure 2: The overall framework of the proposed approach. The proposed prediction module is\ninserted between the transformer blocks to selectively prune less informative token conditioned on\nfeatures produced by the previous layer. By doing so, less tokens are processed in the followed layers.\n3.2 Hierarchical Token Sparsiï¬cation with Prediction Modules\nAn important characteristic of our DynamicViT is that the token sparsiï¬cation is performed hierarchi-\ncally, i.e., we gradually drop the uninformative tokens as the computation proceeds. To achieve this,\nwe maintain a binary decision mask Ë†D âˆˆ{0,1}N to indicate whether to drop or keep each token,\nwhere N = HW is the number of patch embeddings 2. We initialize all elements in the decision\nmask to 1 and update the mask progressively. The prediction modules take the current decision Ë†D\nand the tokens x âˆˆRNÃ—C as input. We ï¬rst project the tokens using an MLP:\nzlocal = MLP(x) âˆˆRNÃ—Câ€²\n, (1)\nwhere Câ€²can be a smaller dimension and we use Câ€²= C/2 in our implementation. Similarly, we\ncan compute a global feature by:\nzglobal = Agg(MLP(x), Ë†D) âˆˆRCâ€²\n, (2)\nwhere Agg is the function which aggregate the information all the existing tokens and can be simply\nimplemented as an average pooling:\nAgg(u, Ë†D) =\nâˆ‘N\ni=1 Ë†Diui\nâˆ‘N\ni=1 Ë†Di\n, u âˆˆRNÃ—Câ€²\n. (3)\nThe local feature encodes the information of a certain token while the global feature contains the\ncontext of the whole image, thus both of them are informative. Therefore, we combine both the local\nand global features to obtain local-global embeddings and feed them to another MLP to predict the\nprobabilities to drop/keep the tokens:\nzi = [zlocal\ni ,zglobal\ni ], 1 â‰¤iâ‰¤N, (4)\nÏ€ = Softmax(MLP(z)) âˆˆRNÃ—2, (5)\nwhere Ï€i,0 denotes the probability of dropping the i-th token and Ï€i,1 is the probability of keeping it.\nWe can then generate current decision D by sampling from Ï€ and update Ë†D by\nË†D â†Ë†D âŠ™D, (6)\nwhere âŠ™is the Hadamard product, indicating that once a token is dropped, it will never be used.\n2We omit the class token for simplicity, while in practice we always keep the class token (i.e., the decision\nfor class token is always â€œ1â€).\n4\n3.3 End-to-end Optimization with Attention Masking\nAlthough our target is to perform token sparsiï¬cation, we ï¬nd it non-trivial to implement in practice\nduring training. First, the sampling from Ï€ to get binary decision mask D is is non-differentiable,\nwhich impedes the end-to-end training. To overcome this, we apply the Gumbel-Softmax tech-\nnique [15] to sample from the probabilities Ï€:\nD = Gumbel-Softmax(Ï€)âˆ—,1 âˆˆ{0,1}N , (7)\nwhere we use the index â€œ1â€ becauseD represents the mask of the kept tokens. The output of Gumbel-\nSoftmax is a one-hot tensor, of which the expectation equals Ï€ exactly. Meanwhile, Gumbel-Softmax\nis differentiable thus makes it possible for end-to-end training.\nThe second obstacle comes when we try to prune the tokens during training. The decision mask Ë†D is\nusually unstructured and the masks for different samples contain various numbers of 1â€™s. Therefore,\nsimply discarding the tokens where Ë†Di = 0 would result in a non-uniform number of tokens for\nsamples within a batch, which makes it hard to parallelize the computation. Thus, we must keep the\nnumber of tokens unchanged, while cut down the interactions between the pruned tokens and other\ntokens. We also ï¬nd that merely zero-out the tokens to be dropped using the binary mask Ë†D is not\nfeasible, because in the calculation of self-attention matrix [26]\nA = Softmax\n(QKT\nâˆš\nC\n)\n(8)\nthe zeroed tokens will still inï¬‚uence other tokens through the Softmax operation. To this end, we\ndevise a strategy called attention masking which can totally eliminate the effects of the dropped\ntokens. Speciï¬cally, we compute the attention matrix by:\nP = QKT /\nâˆš\nC âˆˆRNÃ—N , (9)\nGij =\n{1, i = j,\nË†Dj, i Ì¸= j. 1 â‰¤i,j â‰¤N, (10)\nËœAij = exp(Pij)Gij\nâˆ‘N\nk=1 exp(Pik)Gik\n, 1 â‰¤i,j â‰¤N. (11)\nBy Equation (10) we construct a graph where Gij = 1 means the j-th token will contribute to the\nupdate of the i-th token. Note that we explicitly add a self-loop to each token to improve numerically\nstability. It is also easy to show the self-loop does not inï¬‚uence the results: if Ë†Dj = 0, the j-th\ntoken will not contribute to any tokens other than itself. Equation (11) computes the masked attention\nmatrix ËœA, which is equivalent to the attention matrix calculated by considering only the kept tokens\nbut has a constant shape N Ã—N during training.\n3.4 Training and Inference\nWe now describe the training objectives of our DynamicViT. The training of DynamicViT includes\ntraining the prediction modules such that they can produce favorable decisions and ï¬ne-tuning the\nbackbone to make it adapt to token sparsiï¬cation. Assuming we are dealing with a minibatch of B\nsamples, we adopt the standard cross-entropy loss:\nLcls = CrossEntropy(y,Â¯y), (12)\nwhere y is the prediction of the DynamicViT (after softmax) and Â¯y is the ground truth.\nTo minimize the inï¬‚uence on performance caused by our token sparsiï¬cation, we use the original\nbackbone network as a teacher model and hope the behavior of our DynamicViT as close to the\nteacher model as possible. Speciï¬cally, we consider this constraint from two aspects. First, we make\nthe ï¬nally remaining tokens of the DynamicViT close to the ones of the teacher model, which can be\nviewed as a kind of self-distillation:\nLdistill = 1âˆ‘B\nb=1\nâˆ‘N\ni=1 Ë†Db,S\ni\nBâˆ‘\nb=1\nNâˆ‘\ni=1\nË†Db,S\ni (ti âˆ’tâ€²\ni)2, (13)\n5\nTable 1: Main results on ImageNet. We apply our method on three representative vision transform-\ners: DeiT-S, LV-ViT-S and LV-ViT-M. DeiT-S [25] is a widely used vision transformer with the\nsimple architecture. LV-ViT-S and LV-ViT-M [16] are the state-of-the-art vision transformers. We\nreport the top-1 classiï¬cation accuracy, theoretical complexity in FLOPs and throughput for different\nratio Ï. The throughput is measured on a single NVIDIA RTX 3090 GPU with batch size ï¬xed to 32.\nBase Model Metrics Keeping RatioÏat each stage\n1.0 0.9 0.8 0.7\nDeiT-S [25]\nImageNet Acc. (%) 79.8 79.8 (-0.0) 79.6 (-0.2) 79.3 (-0.5)\nGFLOPs 4.6 4.0 (-14%) 3.4 (-27%) 2.9 (-37%)\nThroughput (im/s) 1337.7 1524.8 (+14%) 1774.6 (+33%) 2062.1 (+54%)\nLV-ViT-S [16]\nImageNet Acc. (%) 83.3 83.3 (-0.0) 83.2 (-0.1) 83.0 (-0.3)\nGFLOPs 6.6 5.8 (-12%) 5.1 (-22%) 4.6 (-31%)\nThroughput (im/s) 993.3 1108.3 (+12%) 1255.6 (+26%) 1417.6 (+43%)\nLV-ViT-M [16]\nImageNet Acc. (%) 84.0 83.9 (-0.1) 83.9 (-0.1) 83.8 (-0.2)\nGFLOPs 12.7 11.1 (-13%) 9.6 (-24%) 8.5 (-33%)\nThroughput (im/s) 589.5 688.5 (+17%) 791.2 (+34%) 888.2 (+50%)\nwhere ti and tâ€²\ni denotes the i-th token after the last block of the DynamicViT and the teacher model,\nrespectively. Ë†Db,s is the decision mask for the b-th sample at the s-th sparsiï¬cation stage. Second,\nwe minimize the difference of the predictions between our DynamicViT and its teacher via the KL\ndivergence:\nLKL = KL (yâˆ¥yâ€²) , (14)\nwhere yâ€²is the prediction of the teacher model.\nFinally, we want to constrain the ratio of the kept tokens to a predeï¬ned value. Given a set of target\nratios for Sstages Ï = [Ï(1),...,Ï (S)], we utilize an MSE loss to supervise the prediction module:\nLratio = 1\nBS\nBâˆ‘\nb=1\nSâˆ‘\ns=1\n(\nÏ(s) âˆ’ 1\nN\nNâˆ‘\ni=1\nË†Db,s\ni\n)2\n. (15)\nThe full training objective is a combination of the above objectives:\nL= Lcls + Î»KLLKL + Î»distillLdistill + Î»ratioLratio, (16)\nwhere we set Î»KL = 0.5,Î»distill = 0.5,Î»ratio = 2 in all our experiments.\nDuring inference, given the target ratio Ï, we can directly discard the less informative tokens via the\nprobabilities produced by the prediction modules such that only exact ms = âŒŠÏsNâŒ‹tokens are kept\nat the s-th stage. Formally, for the s-th stage, let\nIs = argsort(Ï€âˆ—,1) (17)\nbe the indices sorted by the keeping probabilities Ï€âˆ—,1, we can then keep the tokens of which the\nindices lie in Is\n1:ms while discarding the others. In this way, our DynamicViT prunes less informative\ntokens dynamically at runtime, thus can reduce the computational costs during inference.\n4 Experimental Results\nIn this section, we will demonstrate the superiority of the proposed DynamicViT through extensive\nexperiments. In all of our experiments, we ï¬x the number of sparsiï¬cation stages S = 3 and apply\nthe target keeping ratio Ï as a geometric sequence [Ï,Ï2,Ï3] where Ïranges from (0,1). During\ntraining DynamicViT models, we follow most of the training techniques used in DeiT [ 25]. We\nuse the pre-trained vision transformer models to initialize the backbone models and jointly train the\nwhole model for 30 epochs. We set the learning rate of the prediction module to batch size\n1024 Ã—0.001 and\nuse 0.01Ã—learning rate for the backbone model. We ï¬x the weights of the backbone models in the\nï¬rst 5 epochs. All of our models are trained on a single machine with 8 GPUs. Other training setups\nand details can be found in the supplementary material.\n6\n4 6 8 10 12\nGFLOPs\n80\n81\n82\n83\n84ImageNet Top-1 Acc (%)\nDeiT-SPVT-Small\nCoaT Mini\nCrossViT-S\nSwin-T\nT2T-ViT-14CPVT-Small-GAP\nCoaT-Lite Small\nPVT-Medium\nRegNetY-8G\nT2T-ViT-19\nSwin-S\nEfficientNet-B5\nNFNet-F0LV-ViT-S\nLV-ViT-M\nDyViT-LV-S/0.5\nDyViT-LV-S/0.7\nDyViT-LV-S/0.8\nDyViT-LV-S/0.9\nDyViT-LV-M/0.8\nFigure 3: Model complexity (FLOPs) and top-1 ac-\ncuracy trade-offs on ImageNet. We compare Dynam-\nicViT with the state-of-the-art image classiï¬cation\nmodels. Our models achieve better trade-offs com-\npared to the various vision transformers as well as\ncarefully designed CNN models.\nFigure 4: Comparison of our dynamic token sparsi-\nï¬cation method with model width scaling. We train\nour DynamicViT based on DeiT models with embed-\nding dimension varying from 192 to 384 and ï¬x ratio\nÏ= 0.7. We see dynamic token sparsiï¬cation is more\nefï¬cient than commonly used model width scaling.\n4.1 Main results\nOne of the most advantages of the DynamicViT is that it can be applied to a wide range of vision\ntransformer architectures to reduce the computational complexity with minor loss of performance. In\nTable 1, we summarize the main results on ImageNet [7] where we evaluate our DynamicViT used\nthree base models (DeiT-S [25], LV-ViT-S [16] and LV-ViT-M [16]). We report the top-1 accuracy,\nFLOPs, and the throughput under different keeping ratios Ï. Note that our token sparsiï¬cation\nis performed hierarchically in three stages, there are only âŒŠNÏ3âŒ‹tokens left after the last stage.\nThe throughput is measured on a single NVIDIA RTX 3090 GPU with batch size ï¬xed to 32.\nWe demonstrate that our DynamicViT can reduce the computational costs by 31% âˆ¼37% and\naccelerate the inference at runtime by 43% âˆ¼54%, with the neglectable inï¬‚uence of performance\n(âˆ’0.2% âˆ¼âˆ’0.5%).\n4.2 Comparisons with the-state-of-the-arts\nIn Table 2, we compare the DynamicViT with the state-of-the-art models in image classiï¬cation,\nincluding convolutional networks and transformer-like architectures. We use the DynamicViT with\nLV-ViT [16] as the base model and use the â€œ /Ïâ€ to indicate the keeping ratio. We observe that\nour DynamicViT exhibits favorable complexity/accuracy trade-offs at all three complexity levels.\nNotably, we ï¬nd our DynamicViT-LV-M/0.7 beats the Efï¬cientNet-B5 [24] and NFNet-F0 [1], which\nare two of the current state-of-the-arts CNN architectures. This can also be shown clearer in Figure 3,\nwhere we plot the FLOPS-accuracy curve of DynamicViT series (where we use DyViT for short),\nalong with other state-of-the-art models. We can also observe that DynamicViT can achieve better\ntrade-offs than LV-ViT series, which strongly demonstrates the effectiveness of our method.\n4.3 Analysis\nDynamicViT for model scaling. The success of Efï¬cientNet [ 24] shows that we can obtain a\nmodel with better complexity/accuracy tradeoffs by scaling the model along different dimensions.\nWhile in vision transformers, the most commonly used method to scale the model is to change the\nnumber of channels, our DynamicViT provides another powerful tool to perform token sparsiï¬cation.\nWe analysis this nice property of DynamicViT in Figure 4. First, we train several DeiT [25] models\nwith the embedding dimension varying from 192 (DeiT-Ti) to 384 (DeiT-S). Second, we train our\nDynamicViT based on those models with the keeping ratioÏ= 0.7. We ï¬nd that after performing\ntoken sparsiï¬cation, the complexity of the model is reduced to be similar to its variant with a smaller\nembedding dimension. Speciï¬cally, we observe that by applying our DynamicViT to DeiT-256, we\n7\nTable 2: Comparisons with the state-of-the-arts on ImageNet. We compare our DynamicViT\nmodels with state-of-the-art image classifciation models with comparable FLOPs and number of\nparameters. We use the DynamicViT with LV-ViT [16] as the base model and use the â€œ/Ïâ€ to indicate\nthe keeping ratio. We also include the results of LV-ViT models as references.\nModel Params (M) GFLOPs Resolution Top-1 Acc (%)\nDeiT-S [25] 22.1 4.6 224 79.8\nPVT-Small [28] 24.5 3.8 224 79.8\nCoaT Mini [29] 10.0 6.8 224 80.8\nCrossViT-S [4] 26.7 5.6 224 81.0\nPVT-Medium [28] 44.2 6.7 224 81.2\nSwin-T [20] 29.0 4.5 766 81.3\nT2T-ViT-14 [32] 22.0 5.2 224 81.5\nCPVT-Small-GAP [6] 23.0 4.6 817 81.5\nCoaT-Lite Small [29] 20.0 4.0 224 81.9\nLV-ViT-S [16] 26.2 6.6 224 83.3\nDynamicViT-LV-S/0.5 26.9 3.7 224 82.0\nDynamicViT-LV-S/0.7 26.9 4.6 224 83.0\nRegNetY-8G [21] 39.0 8.0 224 81.7\nT2T-ViT-19 [32] 39.2 8.9 224 81.9\nSwin-S [20] 50.0 8.7 224 83.0\nEfï¬cientNet-B5 [24] 30.0 9.9 456 83.6\nNFNet-F0 [1] 72.0 12.4 256 83.6\nDynamicViT-LV-M/0.7 57.1 8.5 224 83.8\nViT-Base/16 [8] 86.6 17.6 224 77.9\nDeiT-Base/16 [25] 86.6 17.6 224 81.8\nCrossViT-B [4] 104.7 21.2 224 82.2\nT2T-ViT-24 [32] 64.1 14.1 224 82.3\nTNT-B [11] 66.0 14.1 224 82.8\nRegNetY-16G [21] 84.0 16.0 224 82.9\nSwin-B [20] 88.0 15.4 224 83.3\nLV-ViT-M [16] 55.8 12.7 224 84.0\nDynamicViT-LV-M/0.8 57.1 9.6 224 83.9\nobtain a model that has a comparable computational complexity to DeiT-Ti, but enjoys around4.3%\nhigher ImageNet top-1 accuracy.\nVisualizations. To further investigate the behavior of DynamicViT, we visualize the sparsiï¬cation\nprocedure in Figure 5. We show the original input image and the sparsiï¬cation results after the three\nstages, where the masks represent the corresponding tokens are discarded. We ï¬nd that through the\nhierarchically token sparsiï¬cation, our DynamicViT can gradually drop the uninformative tokens and\nï¬nally focus on the objects in the images. This phenomenon also suggests that the DynamicViT leads\nto better interpretability, i.e., it can locate the important parts in the image which contribute most to\nthe classiï¬cation step-by-step.\nBesides the sample-wise visualization we have shown above, we are also interested in the statistical\ncharacteristics of the sparsiï¬cation decisions, i.e., what kind of general patterns does the DynamicViT\nlearn from the dataset? We then use the DynamicViT to generate the decisions for all the images in\nthe ImageNet validation set and compute the keep probability of each token in all three stages, as\nshown in Figure 6. We average pool the probability maps into 7 Ã—7 such that they can be visualized\nmore easily. Unsurprisingly, we ï¬nd the tokens in the middle of the image tend to be kept, which is\nreasonable because in most images the objects are located in the center. We can also ï¬nd that the\nlater stage generally has lower probabilities to be kept, mainly because that the keeping ratio at the s\nstage is Ïs, which decreases exponentially as sincreases.\n8\ninputstage1stage2stage3 inputstage1stage2stage3 inputstage1stage2stage3\nFigure 5: Visualization of the progressively sparsiï¬ed tokens. We show the original input image\nand the sparsiï¬cation results after the three stages, where the masks represent the corresponding\ntokens are discarded. We see our method can gradually focus on the most representative regions in\nthe image. This phenomenon suggests that the DynamicViT has better interpretability.\nTable 3: Effects of different losses. We pro-\nvide the results after removing the distillation\nloss and the KL loss.\nBase Model DeiT-S LVViT-S\nDynamicViT 79.3(-0.5) 83.0(-0.3)\nw/o distill (Eq.13) 79.3(-0.5) 82.7(-0.6)\nw/o KL (Eq.14) 79.2(-0.6) 82.9(-0.4)\nw/o distill & KL 79.2(-0.6) 82.5(-0.8)\nstage1\nkeepprobability\nstage3stage2 Figure 6: The keep probabilities of the tokens at each stage.\nEffects of different losses. We show the effects of different losses in Table 3. We see the improve-\nment brought by the distillation loss and the KL loss is not very signiï¬cant, but it can consistently\nfurther boost the performance of various models.\nComparisons of different sparsiï¬cation strategies. As illustrated in Figure 2, the dynamic token\nsparsiï¬cation is unstructured. To discuss whether the dynamic sparsiï¬cation is better than other\nstrategies, we perform ablation experiments and the results are shown in Table 4. For the structural\ndownsampling, we perform an average pooling with kernel size 2 Ã—2 after the sixth block of\nthe baseline DeiT-S [25] model, which has similar FLOPs to our DynamicViT. The static token\nsparsiï¬cation means that the sparsiï¬cation decisions are not conditioned on the input tokens. We also\ncompare our method with other token removal methods like randomly removing tokens or removing\n9\nTable 4: Comparisons of different sparsiï¬cation strategies. We investigate different methods to select\nredundant tokens based on the DeiT-S model. We report the top-1 accuracy on ImageNet for different\nmethods. We ï¬x the complexity of the accelerated models to 2.9G FLOPs for fair comparisons.\n(a) Dynamic sparsiï¬cation vs.\nstatic/structural downsampling.\nModel Acc. (%)\nStructural 78.2 (-1.6)\nStatic 73.4 (-6.4)\nDynamic 79.3 (-0.5)\n(b) Different redundant token re-\nmoval methods.\nModel Acc. (%)\nRandom 77.5 (-2.3)\nAttention 78.1 (-1.7)\nPrediction 79.3 (-0.5)\n(c) Effects of number of sparsiï¬ca-\ntion stages.\nModel Acc. (%)\nSingle-stage 77.4 (-2.4)\nTwo-stage 79.2 (-0.6)\nThree-stage 79.3 (-0.5)\nTable 5: Results on larger models. We apply our method to the model with larger width (i.e., DeiT-B)\nand the model with larger input size (i.e., DeiT-S with 384 Ã—384 input).\n(a) Results on DeiT-B.\nModel GFLOPs Acc. (%)\nDeiT-B 17.5 81.8\nDynamicViT-B/0.7 11.2 (-36%) 81.3 (-0.5)\n(b) Results on the 384 Ã— 384 input.\nModel GFLOPs Acc. (%)\nDeiT-S 15.5 81.6\nDynamicViT-S/0.7 9.5 (-39%) 81.4 (-0.2)\nDynamicViT-S/0.5 7.0 (-55%) 80.3 (-1.3)\ntokens based the attention score of the class token. We ï¬nd through the experiments that although\nother strategies have similar computational complexities, the proposed dynamic token sparsiï¬cation\nmethod achieves the best accuracy. We also show that the progressive sparsiï¬cation method is\nsigniï¬cantly better than one-stage sparsiï¬cation.\nAccelerating larger models. To show the effectiveness of our method on larger models, we apply\nour method to the model with larger width ( i.e., DeiT-B) and models with larger input size ( i.e.,\nDeiT-S with 384 Ã—384 input). The results are presented in Table 5. We see our method also works\nwell on the larger DeiT model. The accuracy drop become less signiï¬cant when we apply our method\nto the model with larger feature maps. Notably, we can reduce the complexity of the DeiT-S model\nwith 384 Ã—384 input by over 50% with only 1.3% accuracy drop.\n5 Conclusion\nIn this work, we open a new path to accelerate vision transformer by exploiting the sparsity of\ninformative patches in the input image. For each input instance, our DynamicViT model prunes the\ntokens of less importance in a dynamic way according to the customized binary decision mask output\nfrom the lightweight prediction module, which fuses the local and global information containing in\nthe tokens. The prediction module is added to multiple layers such that the token pruning is performed\nin a hierarchical way. Gumbel-Softmax and attention masking techniques are also incorporated for\nthe end-to-end training of the transformer model together with the prediction module. During the\ninference phase, our approach can greatly improves the efï¬ciency by gradually pruning 66% of the\ninput tokens, while the drop of accuracy is less than 0.5% for different transformer backbone. In this\npaper, we focus on the image classiï¬cation task. Extending our method to other scenarios like video\nclassiï¬cation and dense prediction tasks can be interesting directions.\nAcknowledgment\nThis work was supported in part by the National Key Research and Development Program of China\nunder Grant 2017YFA0700802, in part by the National Natural Science Foundation of China under\nGrant 62125603, Grant 61822603, Grant U1813218, Grant U1713214, in part by Beijing Academy\nof Artiï¬cial Intelligence (BAAI), in part by National Science Foundation under grant IIS-1901527,\nIIS-2008173, IIS-2048280, and in part by a grant from the Institute for Guo Qiang, Tsinghua\nUniversity.\n10\nReferences\n[1] Andrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-\nscale image recognition without normalization. arXiv preprint arXiv:2102.06171, 2021. 7,\n8\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, pages 213â€“229,\n2020. 3\n[3] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization.\narXiv preprint arXiv:2012.09838, 2020. 2\n[4] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision\ntransformer for image classiï¬cation. arXiv preprint arXiv:2103.14899, 2021. 8\n[5] Bowen Cheng, Alexander G. Schwing, and Alexander Kirillov. Per-pixel classiï¬cation is not all\nyou need for semantic segmentation. NeurIPS, 2021. 3\n[6] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and\nChunhua Shen. Conditional positional encodings for vision transformers. arXiv preprint\narXiv:2102.10882, 2021. 8\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In CVPR, pages 248â€“255, 2009. 3, 7, 13\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3, 8\n[9] Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional\nnetworks using vector quantization. arXiv preprint arXiv:1412.6115, 2014. 3\n[10] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang.\nStar-transformer. arXiv preprint arXiv:1902.09113, 2019. 3\n[11] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in\ntransformer. arXiv preprint arXiv:2103.00112, 2021. 8\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In CVPR, pages 770â€“778, 2016. 1, 13\n[13] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural\nnetworks. In ICCV, pages 1389â€“1397, 2017. 3\n[14] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531, 2015. 3\n[15] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.\nIn ICLR, 2017. 2, 5\n[16] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie Jin, Anran Wang, and Jiashi Feng.\nToken labeling: Training a 85.5% top-1 accuracy vision transformer with 56m parameters on\nimagenet. arXiv preprint arXiv:2104.10858, 2021. 3, 6, 7, 8, 13, 14\n[17] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and\nQun Liu. Tinybert: Distilling bert for natural language understanding. arXiv preprint\narXiv:1909.10351, 2019. 3\n[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiï¬cation with deep\nconvolutional neural networks. NeurIPS, 25:1097â€“1105, 2012. 1\n[19] Benlin Liu, Yongming Rao, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Metadistiller: Network\nself-boosting via meta-learned top-down distillation. In European Conference on Computer\nVision, pages 694â€“709. Springer, 2020. 3\n11\n[20] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021. 1, 3, 8\n[21] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr DollÃ¡r. Design-\ning network design spaces. In CVPR, pages 10428â€“10436, 2020. 8\n[22] Yongming Rao, Jiwen Lu, Ji Lin, and Jie Zhou. Runtime network routing for efï¬cient image\nclassiï¬cation. IEEE transactions on pattern analysis and machine intelligence, 41(10):2291â€“\n2304, 2018. 3\n[23] Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and Jie Zhou. Global ï¬lter networks for\nimage classiï¬cation. In NeurIPS, 2021. 3\n[24] Mingxing Tan and Quoc Le. Efï¬cientnet: Rethinking model scaling for convolutional neural\nnetworks. In ICML, pages 6105â€“6114. PMLR, 2019. 7, 8\n[25] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHervÃ© JÃ©gou. Training data-efï¬cient image transformers & distillation through attention. arXiv\npreprint arXiv:2012.12877, 2020. 1, 2, 3, 6, 7, 8, 9, 13, 14\n[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need.arXiv preprint arXiv:1706.03762,\n2017. 3, 5\n[27] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated\nquantization with mixed precision. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8612â€“8620, 2019. 3\n[28] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction\nwithout convolutions. arXiv preprint arXiv:2102.12122, 2021. 8\n[29] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image\ntransformers. arXiv preprint arXiv:2104.06399, 2021. 8\n[30] Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. On compressing deep models by\nlow rank and sparse decomposition. In CVPR, pages 7370â€“7379, 2017. 3\n[31] Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu, and Jie Zhou. Pointr: Diverse\npoint cloud completion with geometry-aware transformers. In ICCV, 2021. 3\n[32] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay,\nJiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch\non imagenet. arXiv preprint arXiv:2101.11986, 2021. 8\n[33] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. Point transformer. In\nICCV, 2021. 3\n[34] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip H.S. Torr, and Li Zhang. Rethinking semantic segmentation\nfrom a sequence-to-sequence perspective with transformers. In CVPR, 2021. 3\n[35] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou, and Jiashi\nFeng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021. 3\n[36] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159,\n2020. 1\n12\nA Implementation Details\nWe conduct our experiments on the ImageNet (also known as ILSVRC2012) [7] dataset. ImageNet\nis a commonly used benchmark for image classiï¬cation. We train our models on the training set,\nwhich consists of 1.28M images. The top-1 accuracy is measured on the 50k validation images\nfollowing common practice [12, 25]. To fairly compare with previous methods, we report the single\ncrop results.\nWe ï¬x the number of sparsiï¬cation stages S = 3 in all of our experiments, since this setting can\nlead to a decent trade-off between complexity and performance. For the sake of simplicity, we set\nthe target keeping ratio Ï as a geometric sequence [Ï,Ï2,Ï3], where Ïis the keeping ratio after each\nsparsifcation ranging from (0,1). For the prediction module, we use the identical architecture for\ndifferent stages. We use two LayerNorm â†’Linear(C, C/2) â†’GELU block to produce zlocal and\nzglobal respectively. We employ a Linear(C, C/2) â†’GELU â†’Linear(C/2, C/4) â†’GELU â†’\nLinear(C/4, 2) â†’Softmax block to predict the probabilities.\nDuring training our DynamicViT models, we follow most of the training techniques used in DeiT [25].\nWe use the pre-trained vision transformer models to initialize the backbone models and jointly train\nthe backbone model as well as the prediction modules for 30 epochs. We set the learning rate of the\nprediction module to batch size\n1024 Ã—0.001 and use 0.01Ã—learning rate for the backbone model. The batch\nsize is adjusted adaptively for different models according to the GPU memory. We ï¬x the weights of\nthe backbone models in the ï¬rst 5 epochs. All of our models can be trained on a single machine with\n8 NVIDIA GTX 1080Ti GPUs.\nB More Analysis\nIn this section, we provide more analysis of our method. We investigate the effects of progressive\nsparsiï¬cation, distillation loss, ratio loss, and keeping ratio. We also include more visualization\nresults. The following describes the details of the experiments, results and analysis.\nProgressive sparsiï¬cation. To verify the effectiveness of the progressive sparsiï¬cation strategy,\nwe test different sparsiï¬cation methods that result in similar overall complexity. Here we provide\nmore detailed results and more analysis. We ï¬nd that progressive sparsiï¬cation is much better than\nsingle-shot sparsiï¬cation. Increasing the number of stages will lead to better performance. Since\nfurther increasing the number of stages (>3) will not lead to signiï¬cantly better performance but\nadd computation, we use a 3-stage progressive sparsiï¬cation strategy in our main experiments.\nTop-1 accuracy (%) GFLOPs\nDeiT-S [25] 79.8 4.6\nÏ= 0.25,[Ï] (single-stage) 77.4(-2.4) 2.9(-37%)\nÏ= 0.60,[Ï,Ï2] (two-stage) 79.2(-0.6) 2.9(-37%)\nÏ= 0.70,[Ï,Ï2,Ï3] (three-stage) 79.3(-0.5) 2.9(-37%)\nAblation on the distillation loss and ratio loss. The weights of the distillation losses and ratio\nloss are the key hyper-parameters in our method. Since the token-wise distillation loss and the KL\ndivergence loss play similar roles in our method, we set Î»KL = Î»distill in all of our experiments for\nthe sake of simplicity. In this experiment, we ï¬x the keeping ratio Ïto be 0.7. We ï¬nd our method\nis not sensitive to these hyper-parameters in general. The proposed ratio loss can encourage the\nmodel to reach the desired acceleration rate. Distillation losses can improve the performance after\nsparsiï¬cation. We directly apply the best hyper-parameters searched on DeiT-S for all models.\nSmaller keeping ratio. We have also tried applying a smaller keeping ratio (larger acceleration\nrate). The results based on DeiT-S [ 25] and LV-ViT-S [16] models are presented in the following\ntables. We see that using Ï <0.7 will lead to a signiï¬cant accuracy drop while reducing fewer\nFLOPs. Since only 22% and 13% tokens are remaining in the last stage when we set Ïto 0.6 and\n0.5 respectively, small Ïmay cause a signiï¬cant information loss. Therefore, we use Ïâ‰¥0.7 in our\nmain experiments. Jointly scaling Ïand the model width can be a better solution to achieve a large\nacceleration rate as shown in Figure 4 in the paper.\n13\nTop-1 accuracy (%)\nDeiT-S [25] 79.8\nÎ»KL = Î»distill = 0 79.17(-0.63)\nÎ»KL = Î»distill = 0.5 79.32(-0.48)\nÎ»KL = Î»distill = 1 79.23(-0.57)\nTop-1 accuracy (%)\nDeiT-S [25] 79.8\nÎ»ratio = 1 79.15(-0.65)\nÎ»ratio = 2 79.32(-0.48)\nÎ»ratio = 4 79.29(-0.51)\nTop-1 acc. (%) GFLOPs\nDeiT-S [25] 79.8 4.6\nÏ= 0.9 79.8(-0.0) 4.0(-14%)\nÏ= 0.8 79.6(-0.3) 3.4(-27%)\nÏ= 0.7 79.3(-0.5) 2.9(-37%)\nÏ= 0.6 78.5(-1.3) 2.5(-46%)\nÏ= 0.5 77.5(-2.3) 2.2(-52%)\nTop-1 acc. (%) GFLOPs\nLV-ViT-S [16] 83.3 6.6\nÏ= 0.9 83.3(-0.0) 5.8(-12%)\nÏ= 0.8 83.2(-0.1) 5.1(-22%)\nÏ= 0.7 83.0(-0.3) 4.6(-31%)\nÏ= 0.6 82.6(-0.7) 4.1(-38%)\nÏ= 0.5 82.0(-1.3) 3.7(-44%)\nMore visual results. We provide more visual results in Figure 7. The input images are randomly\nsampled from the validation set of ImageNet. We see our method works well for different images\nfrom various categories.\n14\ninput stage1stage2stage3input stage1stage2stage3input stage1stage2stage3\nFigure 7: More visual results. The input images are randomly sampled from the validation set of\nImageNet. We see our method works well for different images from various categories.\n15"
}