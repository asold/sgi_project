{
  "title": "Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text",
  "url": "https://openalex.org/W4385570924",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2291840625",
      "name": "Zhun Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2889029425",
      "name": "Adam Ishay",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A2101096475",
      "name": "JooHyung Lee",
      "affiliations": [
        "Arizona State University",
        "Samsung (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4224912544",
    "https://openalex.org/W3156012351",
    "https://openalex.org/W2163274265",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W2091138122",
    "https://openalex.org/W4226352076",
    "https://openalex.org/W2971107062",
    "https://openalex.org/W2094734546",
    "https://openalex.org/W1525961042",
    "https://openalex.org/W3128862637",
    "https://openalex.org/W3101554965",
    "https://openalex.org/W4385573087",
    "https://openalex.org/W4385574218",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3165543901",
    "https://openalex.org/W1496189301",
    "https://openalex.org/W4308233871",
    "https://openalex.org/W4224266081",
    "https://openalex.org/W2999524812",
    "https://openalex.org/W2995993311",
    "https://openalex.org/W1596557244",
    "https://openalex.org/W2121465811"
  ],
  "abstract": "While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question-answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLM’s adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state-of-the-art performance on several NLP benchmarks, including bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot planning tasks that an LLM alone fails to solve.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 5186–5219\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nCoupling Large Language Models with Logic Programming\nfor Robust and General Reasoning from Text\nZhun Yang, Adam Ishay 1\n1 Arizona State University\n{zyang90,aishay}@asu.edu\nJoohyung Lee 1,2\n2 Samsung Research\njoolee@asu.edu\nAbstract\nWhile large language models (LLMs), such as\nGPT-3, appear to be robust and general, their\nreasoning ability is not at a level to compete\nwith the best models trained for specific natural\nlanguage reasoning problems. In this study, we\nobserve that a large language model can serve\nas a highly effective few-shot semantic parser.\nIt can convert natural language sentences into\na logical form that serves as input for answer\nset programs, a logic-based declarative knowl-\nedge representation formalism. The combi-\nnation results in a robust and general system\nthat can handle multiple question-answering\ntasks without requiring retraining for each new\ntask. It only needs a few examples to guide\nthe LLM’s adaptation to a specific task, along\nwith reusable ASP knowledge modules that can\nbe applied to multiple tasks. We demonstrate\nthat this method achieves state-of-the-art perfor-\nmance on several NLP benchmarks, including\nbAbI, StepGame, CLUTRR, and gSCAN. Ad-\nditionally, it successfully tackles robot planning\ntasks that an LLM alone fails to solve.\n1 Introduction\nA typical way to handle a question-answering task\nis to train a neural network model on large train-\ning data and test it on similar data. Such models\nwork well with linguistic variability and ambiguity\nbut often learn statistical features and correlations\nrather than true reasoning (Ruder, 2021), which\nmakes them not robust, lack generalization, and\ndifficult to interpret.\nAlternatively, transformer-based large language\nmodels (LLMs) have recently shown wide success\non many downstream tasks, demonstrating general\nreasoning capability on diverse tasks without being\nretrained. However, when we restrict our atten-\ntion to individual NLP reasoning benchmarks, they\nusually do not perform as well as state-of-the-art\nmodels despite various efforts to improve accuracy\nthrough prompt engineering (Wei et al., 2022; Zhou\net al., 2022).\nSimilarly, LLMs gained attention for plan gener-\nation for robots due to the rich semantic knowledge\nthey acquired about the world (Ahn et al., 2022;\nHuang et al., 2022; Zeng et al., 2022). However,\nLLMs are known to perform shallow reasoning\nand cannot find complex plans (Valmeekam et al.,\n2022).\nIn another context, Nye et al. (2021) note that\nwhile LLMs are good at System-1 thinking, their\noutputs are often inconsistent and incoherent. This\nis because LLMs are trained to predict subsequent\nwords in a sequence and do not appear to have a\ndeep understanding of concepts such as cause and\neffect, logic, and probability, which are important\nfor reasoning.\nNevertheless, we note that the rich semantic\nknowledge that LLMs possess makes them effec-\ntive general-purpose few-shot semantic parsers that\ncan convert linguistically variable natural language\nsentences into atomic facts that serve as input to\nlogic programs. We also note that the fully declara-\ntive nature of answer set programs (Lifschitz, 2008;\nBrewka et al., 2011) makes them a good pair with\nthe LLM semantic parsers, providing interpretable\nand explainable reasoning on the parsed result of\nthe LLMs using background knowledge. Combin-\ning large language models and answer set programs\nleads to an attractive dual-process, neuro-symbolic\nreasoning that works across multiple QA tasks with-\nout retraining for individual tasks.\nWe tested this idea with several NLP bench-\nmarks, bAbI (Weston et al., 2016), StepGame (Shi\net al., 2022), CLUTRR (Sinha et al., 2019), and\ngSCAN (Ruis et al., 2020), by applying the same\ndual-system model and achieved state-of-the-art\nperformance in all of them. Furthermore, the high\naccuracy and transparency allow us to easily iden-\ntify the source of errors, making our system a useful\ndata set validation tool as well. In particular, we\nfound a significant amount of errors in the original\n5186\nCLUTRR dataset that are hard to detect manually.\nWhile the new version of GPT-3 (Brown et al.,\n2020) (text-davinci-003) shows improvement over\nits predecessors, we observe that it also retains\ncritical limitations. In the process, we develop\nprompt methods for semantic parsing to overcome\nsome of them.\nThe implementation of our method is publicly\navailable online at https://github.com/\nazreasoners/LLM-ASP.\n2 Preliminaries\n2.1 Semantic Parsing and LLMs\nSemantic parsing involves converting a natural lan-\nguage query or statement into a structured represen-\ntation that a computer can understand and manip-\nulate. Statistical methods have increased in popu-\nlarity (Zelle and Mooney, 1996; Miller et al., 1996;\nZettlemoyer and Collins, 2005; Wong and Mooney,\n2007), and encoder-decoder models in particular\nhave been widely used (Dong and Lapata, 2016; Jia\nand Liang, 2016; Koˇcisk`y et al., 2016). However,\nthese statistical methods require annotated input\nand output pairs. Furthermore, machine learning\nmodels often fail to compositionally generalize to\nunseen data (Lake and Baroni, 2018).\nMore recently, pre-trained language models have\nbeen applied to semantic parsing tasks (Liu et al.,\n2021), such as generating SQL queries, SPARQL\nqueries, logical forms, or programs, from natu-\nral language, together with fine-tuning or prompt-\ntuning on pre-trained models, such as BART,\nRoBERTa and GPT-2 (Chen et al., 2020a; Shin\net al., 2021; Schucher et al., 2022). With larger\npre-trained networks, such as GPT-3, prompting ap-\npears to yield a reasonable semantic parser without\nthe need for fine-tuning (Shin et al., 2021; Drozdov\net al., 2022).\nAnother line of related work is to apply pre-\ntrained language models to relation extraction, the\ntask of extracting semantic relationships from a text\ngiven two or more entities (Liu et al., 2021). Wang\net al. (2022) do zero-shot relation extraction with\npre-trained language models from the BERT family\nand GPT-2 variants. Zhou and Chen (2022) fine-\ntune BERT and RoBERTa models for the extraction\nof sentence-level relations. Chen et al. (2022) apply\nprompt-tuning to RoBERT_LARGE for relation ex-\ntraction. Similar to ours, Agrawal et al. (2022) use\na few-shot prompt with GPT-3 for the extraction of\nclinical relations.\n2.2 Dual-System Model\nThere is increasing interest in combining neural\nand symbolic systems (Marcus, 2018; Lamb et al.,\n2020; Sarker et al., 2021). Such dual-system mod-\nels achieved new state-of-the-art results in visual\nquestion answering (Goldman et al., 2018; Sampat\nand Lee, 2018; Yi et al., 2019; Chen et al., 2020b;\nDing et al., 2021). In the case of textual prob-\nlems, to improve LLMs to generate more consis-\ntent and coherent sentences, Nye et al. (2021) sug-\ngest that generation be decomposed into two parts:\ncandidate sentence generation by an LLM (sys-\ntem 1 thinking) and a logical pruning process (sys-\ntem 2 thinking) implemented via a separate sym-\nbolic module. They demonstrate that this neuro-\nsymbolic, dual-process model requires less training\ndata, achieves higher accuracy, and exhibits better\ngeneralization. However, the main limitation of\ntheir work is that the symbolic module is manu-\nally constructed in Python code for the specific\ntask at hand, requiring subtantial efforts. Addition-\nally, their Python symbolic module is not readily\nreusable or composable. Furthermore, their main\nresults primarily focus on the problem of consistent\ntext generation, rather than evaluating the method\non the datasets and comparing it with existing mod-\nels. This is because writing the world models in\nPython is not a scalable approach.\nIn our work, we follow the idea presented\nin (Nye et al., 2021) but adopt logic programming\nin place of the System 2 process. We argue that\nthis combination is much more appealing than the\napproach in (Nye et al., 2021), as it can achieve the\npromised results without the limitations mentioned\nabove.\n2.3 Answer Set Programming\nAnswer Set Programming (ASP) (Lifschitz, 2008;\nBrewka et al., 2011) is a declarative logic program-\nming paradigm that has been shown to be effective\nin knowledge-intensive applications. It is based\non the stable model (a.k.a. answer set) semantics\nof logic programs (Gelfond and Lifschitz, 1988),\nwhich could express causal reasoning, default rea-\nsoning, aggregates, and various other constraints.\nThere are several efficient solvers, such asCLINGO ,\nDLV, and WASP. We use CLINGO v5.6.0 as the\nanswer set solver. For the language of CLINGO , we\nrefer the reader to the textbook (Lifschitz, 2019) or\n5187\nthe CLINGO manual.1\nIt is also known that classical logic-based ac-\ntion formalisms, such as the situation calculus (Mc-\nCarthy and Hayes, 1969; Reiter, 2001) and the\nevent calculus (Shanahan, 1995), can be formu-\nlated as answer set programs. For example, the\nfollowing is one of the axioms in Discrete Event\nCalculus stating the commonsense law of inertia,\nsaying that fluent F holds at the next time if there\nis no action affecting it.\n% (DEC5)\nholds_at(F,T+1) :- timepoint(T), fluent(F),\nholds_at(F,T), -released_at(F,T+1),\nnot terminated(F,T).\nSuch a rule is universal and applies to almost all\nobjects.\nAnswer set programs are also known to be elab-\noration tolerant (McCarthy, 1998). There has been\nwork on modularizing knowledge bases in ASP,\nsuch as module theorem (Oikarinen and Janhunen,\n2006; Babb and Lee, 2012) and knowledge mod-\nules (Baral et al., 2006). While ASP has been\nwidely applied to many reasoning problems, it has\nnot been considered as much in reasoning with nat-\nural language text because its input is expected to\nbe strictly in a logical form, giving little flexibil-\nity in accepting diverse forms of natural language\ninput.\n3 Our Method\nWe refer to our framework as [LLM]+ASP where\n[LLM] denotes a large pre-trained network such as\nGPT-3, which we use as a semantic parser to gen-\nerate input to the ASP reasoner. Specifically, we\nassume data instances of the form ⟨S, q, a⟩, where\nS is a context story in natural language, q is a nat-\nural language query associated with S, and a is\nthe answer. We use an LLM to convert a prob-\nlem description (that is, context S and query q)\ninto atomic facts, which are then fed into the ASP\nsolver along with background knowledge encoded\nas ASP rules. The output of the ASP solver is inter-\npreted as the prediction for the given data instance.\nFigure 1 illustrates the inference flow in the context\nof StepGame. The pipeline is simple but general\nenough to bke applied to various tasks without the\nneed for retraining. It only requires replacing the\nfew-shot prompts to the LLM and the ASP back-\nground knowledge with those suitable for the new\n1https://github.com/potassco/guide/\nreleases.\ntasks.\nBy combining LLMs and ASP in this manner,\nwe enable robust symbolic reasoning that can han-\ndle diverse and unprocessed textual input. The ASP\nknowledge modules remain unaffected by the di-\nverse forms of input text that express the same facts.\nOur method does not rely on training datasets. In-\nstead, a few examples that turn natural language\nsentences into atomic facts are sufficient to build a\nsemantic parser due to the learned representations\nin LLMs. Furthermore, ASP knowledge modules\ncan be reused for different tasks.\n3.1 Prompts for Fact Extraction\nWe use GPT-3 to extract atomic facts from the\nstory and query. Most of the time, giving several\nexamples yields accurate semantic parsing. The\nfollowing is an example prompt for bAbI.\nPlease parse the following statements into facts\n. The available keywords are: pickup , drop ,\nand go.\nSentence: Max journeyed to the bathroom.\nSemantic parse: go(Max , bathroom).\nSentence: Mary grabbed the football there.\nSemantic parse: pickup(Mary , football).\n...\nWe find that GPT-3 is highly tolerable to linguis-\ntic variability. For example, in StepGame, GPT-3\ncan turn various sentences below into the same\natomic fact top_right(\"C\",\"D\").\nC is to the top right of D.\nC is to the right and above D at an angle of\nabout 45 degrees.\nC is at a 45 degree angle to D, in the upper\nrighthand corner.\nC is directly north east of D.\nC is above D at 2 o’clock.\nIn the experiments to follow, we find that the\nfollowing strategy works well for fact extraction.\n1. In general, we find that if the information in\na story (or query) can be extracted indepen-\ndently, parsing each sentence separately (us-\ning the same prompt multiple times) typically\nworks better than parsing the whole story.\n2. There is certain commonsense knowledge that\nGPT-3 is not able to leverage from the exam-\nples in the prompt. In this case, detailing the\nmissing knowledge in the prompt could work.\nFor example, in StepGame, clock numbers are\nused to denote cardinal directions, but GPT-3\ncouldn’t translate correctly even with a few\n5188\nFigure 1: The GPT-3+ASP pipeline for the StepGame dataset.\nexamples in the prompt. It works after enu-\nmerating all cases (“12 denotes top, 1 and 2\ndenote top_right, 3 denotes right, . . .”) in the\nprompt.\n3. Semantic parsing tends to work better if we\ninstruct GPT-3 to use a predicate name that\nbetter reflects the intended meaning of the\nsentence. For example, \"A is there and B\nis at the 5 position of a clock face\" is better\nto be turned into down_right(B,A) than\ntop_left(A,B) although, logically speak-\ning, the relations are symmetric.\nThe complete set of prompts for semantic parsing\nis given in Appendix C.\n3.2 Knowledge Modules\nInstead of constructing a minimal world model for\neach task in Python code (Nye et al., 2021), we\nuse ASP knowledge modules. While some knowl-\nedge could be lengthy to be described in English,\nit could be concisely expressed in ASP. For exam-\nple, the location module contains rules for spatial\nreasoning in a 2D grid space and is used for bAbI,\nStepGame, and gSCAN. Below is the main rule\nin the location module that computes the location\n(Xa,Ya) of object A from the location (Xb,Yb)\nof object B by adding the offsets(Dx,Dy) defined\nby the spatial relation R between A and B.\nlocation(A, Xa , Ya) :- location(B, Xb , Yb),\nis(A, R, B), offset(R, Dx , Dy),\nXa=Xb+Dx , Ya=Yb+Dy.\nThe location module also includes 9 predefined\noffsets, e.g., offset(left,-1,0), that can be\nused to model multi-hop spatial relations of objects\nor effects of a robot’s moving in a 2D space. For\nexample, queries in StepGame are about the spatial\nrelation R of object A to B. Using the location\nmodule, one can fix B’s location to be (0,0) and\ncompute the spatial relation R based on the location\nof A as follows.\nlocation(B, 0, 0) :- query(A, B).\nanswer(R) :- query(A, B), location(A, X, Y),\noffset(R, Dx , Dy),\nDx=-1: X<0; Dx=0: X=0; Dx=1: X>0;\nDy=-1: Y<0; Dy=0: Y=0; Dy=1: Y>0.\nThe second rule above contains six conditional\nliterals among which Dx=-1:X<0 says that “Dx\nmust be -1 if X<0.” For example, if A’s location\n(X,Y) is (-3,0), then (Dx,Dy) is (-1,0)\nand the answer R is left. Similar rules can also\nbe applied to bAbI task 17, which asks if A is R of\nB.\nIn the above rules, the relation R in, e.g.,\nis(A,R,B), is a variable and can be substituted\nby any binary relation. Such high-order representa-\ntion turns out to be quite general and applicable to\nmany tasks that query relation or its arguments.\nFigure 2: The knowledge modules at the bottom are\nused in each task on the top.\n5189\nFigure 2 shows the knowledge modules used in\nthis paper, where DEC denotes the Discrete Event\nCalculus axioms from (Mueller, 2006; Lee and\nPalla, 2012). In this section, we explained the main\nrules in the location module. The complete ASP\nknowledge modules are given in Appendix E.\n4 Experiments\nWe apply the method in the previous section to four\ndatasets.2 As a reminder, our approach involves\nfew-shot in-context learning and does not require\ntraining. We use the same pipeline as shown in\nFigure 1, but with different prompts and knowl-\nedge modules for each dataset. For more detailed\ninformation about the experimental settings, please\nrefer to the appendix.\n4.1 bAbI\nThe bAbI dataset (Weston et al., 2016) is a collec-\ntion of 20 QA tasks that have been widely applied\nto test various natural language reasoning problems,\nsuch as deduction, path-finding, spatial reasoning,\nand counting. State-of-the-art models, such as\nself-attentive associative-based two-memory model\n(STM) (Le et al., 2020) and Query-Reduction net-\nworks (QRN) (Seo et al., 2017) achieve close to\n100% accuracy after training with 10k instances\nwhile QRN’s accuracy drops to 90% with 1k train-\ning instances.\nWe first designed two GPT-3 baselines, one with\nfew shot prompts (containing a few example ques-\ntions and answers) and the other with Chain-of-\nThought (CoT) prompts (Wei et al., 2022), which\nstate the relevant information to derive the answer.\nWe also apply GPT-3+ASP. For example, we use\nGPT-3 to turn “the kitchen is south of the bathroom”\ninto an atomic fact is(kitchen, southOf,\nbathroom) by giving a few examples of the same\nkind. Regarding knowledge modules, Tasks 1–3, 6–\n9, 10–14, and 19 are about events over time and use\nthe DEC knowledge module. Tasks 4, 17, and 19\nrequire various domain knowledge modules such\nas location and action knowledge modules. The\nremaining tasks do not require domain knowledge\nand rely only on simple rules to extract answers\nfrom parsed facts.\nTable 1 compares our method with the two GPT-\n3 baselines, as well as two state-of-the-art methods\non bAbI datasets, STM and QRN. Interestingly, the\n2Due to space restriction, we put the experiments about\nPick&Place in Appendix A.\nnew GPT-3, text-davinci-003 (denoted GPT-3 (d3)),\nwith basic few-shot prompting achieves 80.34%\naccuracy, while CoT improves it to 86.18%. GPT-\n3(d3)+ASP achieves state-of-the-art performance\non bAbI with 99.99% average performance among\nall tasks, producing only two answers that disagree\nwith the labels in the dataset. It turns out that the\ntwo questions are malformed since the answers\nare ambiguous, and our model’s answers can be\nconsidered correct.3\n4.2 StepGame\nAlthough bAbI has been extensively tested, it has\nseveral problems. Shi et al. (2022) note data leak-\nage between the train and the test sets where named\nentities are fixed and only a small number of re-\nlations are used. Palm et al. (2018) point out that\nmodels do not need multi-hop reasoning to solve\nthe bAbI dataset. To address the issues, Shi et al.\n(2022) propose the StepGame dataset. It is a con-\ntextual QA dataset in which the system is required\nto interpret a story S about spatial relationships\namong several entities and answers a query q about\nthe relative position of two of those entities, as\nillustrated in Figure 1. Unlike the bAbI dataset,\nStepGame uses a large number of named entities,\nand requires multi-hop reasoning up to as many as\n10 reasoning steps.\nIn the basic form of the StepGame dataset, each\nstory consists of k sentences that describe k spatial\nrelationships between k + 1entities in a chain-like\nshape. In this paper, we evaluate the StepGame\ndataset with noise, where the original chain is ex-\ntended with noise statements by branching out with\nnew entities and relations.\nSimilarly to bAbI, we designed two GPT-3 base-\nlines and applied our method to the StepGame data\nset. More details on the prompts are available in\nAppendix C.2.\nFor each k ∈{1, . . . ,10}, the StepGame dataset\nwith noise consists of 30,000 training samples,\n1000 validation samples, and 10,000 test samples.\nTo save the API cost for GPT-3, we only evaluated\nthe two GPT-3 baselines on the first 100 test sam-\nples and evaluated our method on the first 1,000\ntest samples for each k ∈{1, . . . ,10}. Table 2\ncompares the accuracy of our method with the two\nbaselines of GPT-3 and the current methods, i.e.\nRN (Santoro et al., 2017), RRN (Palm et al., 2018),\nUT (Dehghani et al., 2018), STM (Le et al., 2020),\n3See Appendix F.1 for the examples.\n5190\nTask GPT-3(d3) GPT-3(d3) GPT-3(d3) STM(Le et al., 2020) QRN(Seo et al., 2017)\nFew-Shot CoT +ASP (10k train) (10k train) (1k train)\n1: Single supporting fact 98.4 97.3 100.0 100.0 ± 0.0 100.0 100.0\n2: Two supporting facts 60.8 72.2 100.0 99.79 ± 0.23 100.0 99.3\n3: Three supporting facts 39.6 54.1 100.0 97.87 ± 1.14 100.0 94.3\n4: Two arg relations 60.4 72.7 100.0 100.0 ± 0.0 100.0 100.0\n5: Three arg relations 88.2 89.1 99.8 99.43 ± 0.18 100.0 98.9\n6: Yes/no questions 97.4 97.3 100.0 100.0 ± 0.0 100.0 99.1\n7: Counting 90.6 88.6 100.0 99.19 ± 0.27 100.0 90.4\n8: Lists/sets 96.2 97.1 100.0 99.88 ± 0.07 99.6 94.4\n9 : Simple negation 98.4 98.2 100.0 100.0 ± 0.0 100.0 100.0\n10: Indefinite knowledge 93.6 92.4 100.0 99.97 ± 0.06 100.0 100.0\n11: Basic coreference 93.6 99.2 100.0 99.99 ± 0.03 100.0 100.0\n12: Conjunction 88.6 88.8 100.0 99.96 ± 0.05 100.0 100.0\n13: Compound coreference 98.4 97.3 100.0 99.99 ± 0.03 100.0 100.0\n14: Time reasoning 78.0 91.5 100.0 99.84 ± 0.17 99.9 99.2\n15: Basic deduction 57.0 95.0 100.0 100.0 ± 0.0 100.0 100.0\n16: Basic induction 90.8 97.5 100.0 99.71 ± 0.15 100.0 47.0\n17: Positional reasoning 66.0 70.8 100.0 98.82 ± 1.07 95.9 65.6\n18: Size reasoning 89.8 97.1 100.0 99.73 ± 0.28 99.3 92.1\n19: Path finding 21.0 28.7 100.0 97.94 ± 2.79 99.9 21.3\n20: Agents motivations 100.0 100.0 100.0 100.0 ± 0.0 100.0 99.8\nAverage 80.34 86.18 99.99 99.85 99.70 90.1\nTable 1: Test accuracy on 20 tasks in bAbI data\nMethod k=1 k=2 k=3 k=4 k=5\nRN 22.6 17.1 15.1 12.8 11.5\nRRN 24.1 20.0 16.0 13.2 12.3\nUT 45.1 28.4 17.4 14.1 13.5\nSTM 53.4 36.0 23.0 18.5 15.1\nTPR-RNN 70.3 46.0 36.1 26.8 24.8\nTP-MANN 85.8 60.3 50.2 37.5 31.3\nSynSup 98.6 95.0 92.0 79.1 70.3\nFew-Shot (d3) 55.0 37.0 25.0 30.0 32.0\nCoT (d3) 61.0 45.0 30.0 35.0 35.0\nGPT-3(c1)+ASP 44.7 38.8 40.5 58.8 62.4\nGPT-3(d2)+ASP 92.6 89.9 89.1 93.8 92.9\nMethod k=6 k=7 k=8 k=9 k=10\nRN 11.1 11.5 11.2 11.1 11.3\nRRN 11.6 11.4 11.8 11.2 11.7\nUT 12.7 12.1 11.4 11.4 11.7\nSTM 13.8 12.6 11.5 11.3 11.8\nTPR-RNN 22.3 19.9 15.5 13.0 12.7\nTP-MANN 28.5 26.5 23.7 22.5 21.5\nSynSup 63.4 58.7 52.1 48.4 45.7\nFew-Shot (d3) 29.0 21.0 22.0 34.0 31.0\nCoT (d3) 27.0 22.0 24.0 23.0 25.0\nGPT-3(c1)+ASP 57.4 56.2 58.0 56.5 54.1\nGPT-3(d2)+ASP 91.6 91.2 90.4 89.0 88.3\nTable 2: Test accuracy on the StepGame test dataset,\nwhere (c1), (d2), and (d3) denote text-curie-001, text-\ndavinci-002, and text-davinci-003 models, respectively\nTPR-RNN (Schlag and Schmidhuber, 2018), TP-\nMANN (Shi et al., 2022), and SynSup (with pre-\ntraining on the SPARTUN dataset) (Mirzaee and\nKordjamshidi, 2022). Surprisingly, the GPT-3 base-\nlines could achieve accuracy comparable to other\nmodels (except for SynSup) for largek values. CoT\ndoes not always help and decreases the accuracy\nwith big ks. This may be because there is a higher\nchance of making a mistake in a long chain of\nthought. GPT-3(d2)+ASP outperforms all state-\nof-the-art methods and the GPT-3 baselines by a\nlarge margin for k = 4, . . . ,10. Although SynSup\nachieves a higher accuracy for k = 1, 2, 3, this\nis misleading due to errors in the dataset. As we\nanalyze below, about 10.7% labels in the data are\nwrong. The SynSup training makes the model learn\nto make the same mistakes over the test dataset,\nwhich is why its performance looks better than\nours.\nThe modular design of GPT-3+ASP enables us\nto analyze the reasons behind its wrong predic-\ntions. We collected the first 100 data instances for\neach k ∈{1, . . . ,10}and manually analyzed the\npredictions on them. Among 1000 predictions of\nGPT-3(d2)+ASP, 108 of them disagree with the\ndataset labels, and we found that 107 of those have\nerrors in the labels. For example, given the story\nand question “J and Y are horizontal and J is to\nthe right of Y. What is the relation of the agent Y\nwith the agent J?”, the label in the dataset is “right”\nwhile the correct relation should be “left”.4 Recall\n4The remaining disagreeing case is due to text-davinci-\n002’s mistake. For the sentence, “if E is the center of a clock\nface, H is located between 2 and 3 .” text-davinci-002 turns\nit into “right(H, E)” whereas text-davinci-003 turns it into\n“top-right(H, E)” correctly. To save API cost for GPT-3, we\n5191\nthat our method is interpretable, so we could easily\nidentify the source of errors.\n4.3 CLUTRR\nCLUTRR (Sinha et al., 2019) is a contextual QA\ndataset that requires inferring family relationships\nfrom a story. Sentences in CLUTRR are generated\nusing 6k template narratives written by Amazon\nMechanical Turk crowd-workers, and thus are more\nrealistic and complex compared to those in bAbI\nand StepGame.\nCLUTRR consists of two subtasks, systematic\ngeneralization that evaluates stories containing un-\nseen combinations of logical rules (Minervini et al.,\n2020; Bergen et al., 2021) and robust reasoning\nthat evaluates stories with noisy descriptions (Tian\net al., 2021). Since we use ASP for logical rea-\nsoning, which easily works for any combination\nof logical rules, we focus on the robust reasoning\ntask.\nMethod CLU. clean supp. irre. disc.\nRN 1.0 49 68 50 45\nMAC 1.0 63 65 56 40\nBi-att 1.0 58 67 51 57\nGSM 1.0 68.5 48.6 62.9 52.8\nGPT-3(d3)+ASP 1.0 68.5 82.8 74.8 67.4\nGPT-3(d3)+ASP 1.3 97.0 84.0 92.0 90.0\nTable 3: Test accuracy on 4 categories in CLUTRR 1.0\nand CLUTRR 1.3 datasets\nTable 3 compares our method with RN (Santoro\net al., 2017), MAC (Hudson and Manning, 2018),\nBiLSTM-attention (Sinha et al., 2019), and GSM\n(Tian et al., 2021) on the original CLUTRR dataset,\nnamely CLUTRR 1.0, in four categories of data\ninstances: clean, supporting, irrelevant, and discon-\nnected (Sinha et al., 2019). Except for our method,\nall other models are trained on the corresponding\ncategory of CLUTRR training data. Although our\nmethod achieves similar or higher accuracies in\nall categories, they are still much lower than we\nexpected.\nWe found that such low accuracy is due to the\nclear errors in CLUTRR, originating mostly from\nerrors in the template narratives or the generated\nfamily graphs that violate common sense. The au-\nthors of CLUTRR recently published CLUTRR 1.3\ncodes to partially resolve this issue. 5 With the new\ncode, we created a new dataset, namely CLUTRR\ndid not re-run the whole experiments with text-davinci-003.\n5https://github.com/facebookresearch/\nclutrr/tree/develop\n1.3, consisting of 400 data instances with 100 for\neach of the four categories. The last row in Table 3\nshows that our method actually performs well on\nrealistic sentences in CLUTRR. Indeed, with our\nmethod (using text-davinci-003) on CLUTRR 1.3\ndataset, 363 out of 400 predictions are correct, 16\nare still wrong due to data mistakes (e.g., the la-\nbel says “Maryann has an uncle Bruno” while the\nnoise sentence added to the story is “Maryann told\nher son Bruno to give the dog a bath”), and 21 are\nwrong due to GPT-3’s parsing mistakes (e.g., GPT-\n3 turned the sentence “Watt and Celestine asked\ntheir mother, if they could go play in the pool” into\nmother(\"Watt\", \"Celestine\"). Since\nthe sentences in CLUTRR 1.3 are more realistic\nthan those in bAbI and StepGame, GPT-3 makes\nmore mistakes even after reasonable efforts of\nprompt engineering. More details on data errors\nand GPT-3 errors are available in Appendix F.2 and\nAppendix D.\nMethod clean supp. irre. disc.\nDeepProbLog 100 100 100 94\nGPT-3(d2)+ASP 100 100 97 97\nGPT-3(d3)+ASP 100 100 100 100\nTable 4: Test accuracy on CLUTRR-S dataset\nWe also evaluated our method on a simpler and\ncleaner variant of the CLUTRR data set, namely\nCLUTRR-S, that was used as a benchmark prob-\nlem for a state-of-the-art neuro-symbolic approach\nDeepProbLog (Manhaeve et al., 2021). Table 4\ncompares the accuracy of our method and Deep-\nProbLog in all 4 categories of test data. GPT-\n3(d3)+ASP achieves 100% accuracy, outperform-\ning DeepProbLog without the need for training.\nRemark: Due to the modular structure, our\nmethod could serve as a data set validation tool to\ndetect errors in a dataset. We detected 107 wrong\ndata instances in the first 1000 data in StepGame\nand 16 wrong data instances in the 400 data in\nCLUTRR 1.3.\n4.4 gSCAN\nThe gSCAN dataset (Ruis et al., 2020) poses a task\nin which an agent must execute action sequences\nto achieve a goal (specified by a command in a\nnatural language sentence) in a grid-based visual\nnavigation environment. The dataset consists of\ntwo tasks, and we evaluate our method on the data\nsplits from the compositional generalization task.\nThere is one shared training set, one test set (split\n5192\nA) randomly sampled from the same distribution\nof the training set, and seven test sets (splits B\nto H) with only held-out data instances (i.e., not\nappearing in the training set) in different ways.\nIn the gSCAN dataset, each data instance is a\ntuple ⟨G, q, a⟩where G is the grid configuration (in\nJSON format) describing the size of the gird, the\nlocation and direction of the agent, and the location\nand features of each object in the grid; q is a query\n(e.g., “pull a yellow small cylinder hesitantly”); and\na is the answer in the form of a sequence of actions\n(e.g., “turn right, walk, stay, pull, stay, pull, stay”).\nFor each data instance, we (i) use a Python script to\nextract atomic facts (e.g., pos(agent,(2,3)))\nfrom the grid configuration G; (ii) extract\natomic facts from query q into atomic facts\n(e.g., query(pull), queryDesc(yellow),\nwhile(hesitantly)) using GPT-3; and (iii)\npredict the sequence of actions for this query us-\ning ASP. The details of the prompts are given in\nAppendix C.4.\nMethod A B C D\nGECA 87.60 34.92 78.77 0.00\nDualSys 74.7 81.3 78.1 0.01\nVilbert+CMA 99.95 99.90 99.25 0.00\nGPT-3(c1)+ASP 98.30 100 100 100\nGPT-3(d2)+ASP 100 100 100 100\nMethod E F G H\nGECA 33.19 85.99 0.00 11.83\nDualSys 53.6 76.2 0.0 21.8\nVilbert+CMA 99.02 99.98 0.00 22.16\nGPT-3(c1)+ASP 100 100 100 100\nGPT-3(d2)+ASP 100 100 100 100\nTable 5: Test accuracy on the gSCAN dataset\nTable 5 compares the accuracy of our method\nand the state-of-the-art methods, i.e., GECA (Ruis\net al., 2020), DualSys (Nye et al., 2021) and Vil-\nbert+CMA (Qiu et al., 2021), on the gSCAN test\ndataset in eight splits. To save API cost for GPT-\n3, we only evaluated the first 1000 data instances\nof each split. With text-davinci-002, our method\nGPT-3+ASP achieves 100% accuracy. With text-\ncurie-001, the accuracy is slightly lower, making\n17 errors in split A. The errors are of two kinds.\nThe language model fails to extract adverbs in the\ncorrect format for 11 data instances (e.g., GPT-3 re-\nsponded queryDesc(while spinning) in-\nstead of while(spinning)) and didn’t ground\nthe last word in a query for 6 data instances (e.g.,\nfor query walk to a small square, GPT-\n3 missed an atomic fact queryDesc(square)).\nOnce the parsed results are correct, ASP does not\nmake a mistake in producing plans.\n4.5 Findings\nThe following summarizes the findings of the ex-\nperimental evaluation.\n• Our experiments confirm that LLMs like GPT-\n3 are still not good at multi-step reasoning\ndespite various prompts we tried. Chain-of-\nThought is less likely to improve accuracy\nwhen a long chain of thought is required.\n• On the other hand, LLMs are surprisingly\ngood at turning a variety of expressions into\na \"canonical form\" of information extraction.\nThis in turn allows ASP knowledge modules\nto be isolated from linguistic variability in the\ninput.\n• Even for generating simple atomic facts,\nlarger models tend to perform better. For ex-\nample, in StepGame and gSCAN, text-curie-\n001 performs significantly worse compared to\ntext-davinci-002 (Tables 2 and 5).\n• The total amount of knowledge that needs\nto be encoded for all of the above datasets\nis not too large. This is in part due to the\nfact that GPT-3 \"normalized\" various forms\nof input sentences for ASP to process and that\nknowledge modules could be reused across\ndifferent datasets.\n• The modular design of our approach makes it\npossible to locate the root cause of each failed\nprediction in the training data and improve\nupon it. There are three sources of errors: se-\nmantic parsing in LLMs, symbolic constraints,\nand the dataset itself, and we can resolve the\nfirst two issues by improving the prompts and\nupdating the constraints, respectively.\n• Our framework could serve as a few-shot\ndataset justifier and corrector. Among all pre-\ndictions by our method that do not align with\nthe labels, almost all of them (with only a few\nexceptions discussed in the paper) are due to\nerrors in the dataset.\n5 Conclusion\nSymbolic logic programming was previously con-\nsidered limited in its ability to reason from text due\n5193\nto its inability to handle various and ambiguous\nlinguistic expressions. However, combining it with\na large language model that has learned distributed\nrepresentations helps alleviate this problem. The\nmethod not only achieves high accuracy but also\nproduces interpretable results, as the source of the\nerrors can be identified. It is also general; by using\npre-trained networks with few-shot prompts and\nreusable knowledge modules, adapting to a new\ndomain does not require extensive training.\nThe knowledge modules used in our experi-\nments are reusable. For the above experiments,\nthe modules are relatively simple to write, as are\nthe prompts for parsing natural language for LLMs.\nHowever, acquiring this kind of knowledge on a\nmassive scale is also an important line of research\n(Liu and Singh, 2004; Bosselut et al., 2019; Hwang\net al., 2021) that needs to be combined. In addition,\nit is possible to use LLM’s code generation capa-\nbility (Chen et al., 2021) to generate logic program\nrules, which we leave for future work.\nOne may think that the logic rules are too rigid.\nHowever, there are many weighted or probabilistic\nrules that can be defeated (Richardson and Domin-\ngos, 2006; Fierens et al., 2013; Lee and Wang,\n2018). They could be used for more realistic set-\ntings, but for the benchmark problems above, they\nwere not needed.\nEthical Considerations\nAll datasets used in this paper are publicly avail-\nable. For CLUTRR dataset, the gender informa-\ntion is essential to tell if, e.g., A is B’s uncle or\nniece. We used GPT-3 to predict the genders of\npersons in each story. Since each story is systemati-\ncally generated using sampled common first names\nand sampled sentence templates, it does not reveal\nany identity. As mentioned, the original CLUTRR\ndataset had some errors, and we describe carefully\nthe codes and settings of the generated CLUTRR\n1.3 dataset in Appendix B.1.\nLimitations\nThe current work requires that knowledge modules\nbe written by hand. Commonly used axioms, such\nas general knowledge like the commonsense law of\ninertia expressed by event calculus, can be reused\neasily, but there are vast amounts of other common-\nsense knowledge that are not easy to obtain. LLMs\ncould be used to supply this information, but we\nhave not tried. Knowledge graphs, such as Con-\nceptNet (Liu and Singh, 2004), COMET (Bosselut\net al., 2019) and ATOMIC (Hwang et al., 2021),\ncan be utilized to populate ASP rules. Like code\nmodels, we expect that LLMs could generate ASP\ncode, which we leave for future work.\nAlso, when using large language models, despite\nvarious efforts, sometimes it is not understandable\nwhy they do not behave as expected.\nAcknowledgements\nThis work was partially supported by the National\nScience Foundation under Grant IIS-2006747.\nReferences\nMonica Agrawal, Stefan Hegselmann, Hunter Lang,\nYoon Kim, and David Sontag. 2022. Large language\nmodels are few-shot clinical information extractors.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, page\n1998–2022. Association for Computational Linguis-\ntics.\nMichael Ahn, Anthony Brohan, Yevgen Chebotar,\nChelsea Finn, Karol Hausman, Alexander Herzog,\nDaniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan\nJulian, et al. 2022. Do as I can, not as I say: Ground-\ning language in robotic affordances. In 6th Annual\nConference on Robot Learning.\nJoseph Babb and Joohyung Lee. 2012. Module theorem\nfor the general theory of stable models. Theory and\nPractice of Logic Programming, 12(4-5):719–735.\nChitta Baral, Juraj Dzifcak, and Hiro Takahashi. 2006.\nMacros, macro calls and use of ensembles in mod-\nular answer set programming. In International\nConference on Logic Programming, pages 376–390.\nSpringer.\nLeon Bergen, Timothy O’Donnell, and Dzmitry Bah-\ndanau. 2021. Systematic generalization with edge\ntransformers. Advances in Neural Information Pro-\ncessing Systems, 34:1390–1402.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. Comet: Commonsense transformers for knowl-\nedge graph construction. In Association for Compu-\ntational Linguistics (ACL).\nGerhard Brewka, Ilkka Niemelä, and Miroslaw\nTruszczynski. 2011. Answer set programming at\na glance. Communications of the ACM, 54(12):92–\n103.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\n5194\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nXiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng,\nYunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and\nHuajun Chen. 2022. Knowprompt: Knowledge-\naware prompt-tuning with synergistic optimization\nfor relation extraction. In Proceedings of the ACM\nWeb Conference 2022, pages 2778–2788.\nXilun Chen, Asish Ghoshal, Yashar Mehdad, Luke\nZettlemoyer, and Sonal Gupta. 2020a. Low-resource\ndomain adaptation for compositional task-oriented\nsemantic parsing. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 5090–5100.\nZhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-\nYee Kenneth Wong, Joshua B Tenenbaum, and\nChuang Gan. 2020b. Grounding physical concepts\nof objects and events through dynamic visual rea-\nsoning. In International Conference on Learning\nRepresentations.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2018. Universal\ntransformers. In International Conference on Learn-\ning Representations.\nMingyu Ding, Zhenfang Chen, Tao Du, Ping Luo, Josh\nTenenbaum, and Chuang Gan. 2021. Dynamic visual\nreasoning by learning differentiable physics models\nfrom video and language. Advances in Neural Infor-\nmation Processing Systems, 34.\nLi Dong and Mirella Lapata. 2016. Language to logical\nform with neural attention. In 54th Annual Meet-\ning of the Association for Computational Linguistics,\npages 33–43. Association for Computational Linguis-\ntics (ACL).\nAndrew Drozdov, Nathanael Schärli, Ekin Akyürek,\nNathan Scales, Xinying Song, Xinyun Chen, Olivier\nBousquet, and Denny Zhou. 2022. Compositional\nsemantic parsing with large language models. arXiv\npreprint arXiv:2209.15003.\nDaan Fierens, Guy Van den Broeck, Joris Renkens,\nDimitar Shterionov, Bernd Gutmann, Ingo Thon,\nGerda Janssens, and Luc De Raedt. 2013. Inference\nand learning in probabilistic logic programs using\nweighted boolean formulas. Theory and Practice of\nLogic Programming, pages 1–44.\nMichael Gelfond and Vladimir Lifschitz. 1988. The\nstable model semantics for logic programming. In\nProceedings of International Logic Programming\nConference and Symposium, pages 1070–1080. MIT\nPress.\nOmer Goldman, Veronica Latcinnik, Ehud Nave, Amir\nGloberson, and Jonathan Berant. 2018. Weakly su-\npervised semantic parsing with abstract examples. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1809–1819.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Pete Florence, Andy Zeng, Jonathan Tompson,\nIgor Mordatch, Yevgen Chebotar, Pierre Sermanet,\nTomas Jackson, Noah Brown, Linda Luu, Sergey\nLevine, Karol Hausman, and brian ichter. 2022. Inner\nmonologue: Embodied reasoning through planning\nwith language models. In 6th Annual Conference on\nRobot Learning.\nDrew A Hudson and Christopher D Manning. 2018.\nCompositional attention networks for machine rea-\nsoning. In International Conference on Learning\nRepresentations.\nJena D Hwang, Chandra Bhagavatula, Ronan Le Bras,\nJeff Da, Keisuke Sakaguchi, Antoine Bosselut, and\nYejin Choi. 2021. (comet-) atomic 2020: On sym-\nbolic and neural commonsense knowledge graphs.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pages 6384–6392.\nRobin Jia and Percy Liang. 2016. Data recombination\nfor neural semantic parsing. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n12–22.\nTomáš Koˇcisk`y, Gábor Melis, Edward Grefenstette,\nChris Dyer, Wang Ling, Phil Blunsom, and\nKarl Moritz Hermann. 2016. Semantic parsing with\nsemi-supervised sequential autoencoders. In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1078–\n1087.\nBrenden Lake and Marco Baroni. 2018. Generalization\nwithout systematicity: On the compositional skills\nof sequence-to-sequence recurrent networks. In In-\nternational conference on machine learning, pages\n2873–2882. PMLR.\nLuis C Lamb, Artur Garcez, Marco Gori, Marcelo\nPrates, Pedro Avelar, and Moshe Vardi. 2020. Graph\nneural networks meet neural-symbolic computing:\nA survey and perspective. In Proceedings of Inter-\nnational Joint Conference on Artificial Intelligence\n(IJCAI), pages 4877–4884.\nHung Le, Truyen Tran, and Svetha Venkatesh. 2020.\nSelf-attentive associative memory. In International\nConference on Machine Learning, pages 5682–5691.\nPMLR.\nJoohyung Lee and Ravi Palla. 2012. Reformulating\nthe situation calculus and the event calculus in the\ngeneral theory of stable models and in answer set pro-\ngramming. Journal of Artificial Inteligence Research\n(JAIR), 43:571–620.\n5195\nJoohyung Lee and Yi Wang. 2018. Weight learning\nin a probabilistic extension of answer set programs.\nIn Proceedings of International Conference on Prin-\nciples of Knowledge Representation and Reasoning\n(KR), pages 22–31.\nVladimir Lifschitz. 2008. What is answer set program-\nming? In Proceedings of the AAAI Conference on\nArtificial Intelligence, pages 1594–1597. MIT Press.\nVladimir Lifschitz. 2019. Answer set programming .\nSpringer Heidelberg.\nHugo Liu and Push Singh. 2004. Conceptnet—a practi-\ncal commonsense reasoning tool-kit. BT technology\njournal, 22(4):211–226.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys (CSUR).\nRobin Manhaeve, Sebastijan Dumanˇci´c, Angelika Kim-\nmig, Thomas Demeester, and Luc De Raedt. 2021.\nNeural probabilistic logic programming in deep-\nproblog. Artificial Intelligence, 298:103504.\nGary Marcus. 2018. Deep learning: A critical appraisal.\narXiv preprint arXiv:1801.00631.\nJohn McCarthy. 1998. Elaboration tolerance. In Work-\ning Papers of the Fourth Symposium on Logical For-\nmalizations of Commonsense Reasoning.\nJohn McCarthy and Patrick Hayes. 1969. Some philo-\nsophical problems from the standpoint of artificial\nintelligence. In B. Meltzer and D. Michie, editors,\nMachine Intelligence, volume 4, pages 463–502. Ed-\ninburgh University Press, Edinburgh.\nScott Miller, David Stallard, Robert Bobrow, and\nRichard Schwartz. 1996. A fully statistical approach\nto natural language interfaces. In 34th Annual Meet-\ning of the Association for Computational Linguistics,\npages 55–61.\nPasquale Minervini, Sebastian Riedel, Pontus Stenetorp,\nEdward Grefenstette, and Tim Rocktäschel. 2020.\nLearning reasoning strategies in end-to-end differ-\nentiable proving. In International Conference on\nMachine Learning, pages 6938–6949. PMLR.\nRoshanak Mirzaee and Parisa Kordjamshidi. 2022.\nTransfer learning with synthetic corpora for spatial\nrole labeling and reasoning. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing, page 6148–6165. Association\nfor Computational Linguistics.\nErik Mueller. 2006. Commonsense reasoning. Elsevier.\nMaxwell Nye, Michael Tessler, Josh Tenenbaum, and\nBrenden M Lake. 2021. Improving coherence and\nconsistency in neural sequence models with dual-\nsystem, neuro-symbolic reasoning. Advances in\nNeural Information Processing Systems, 34:25192–\n25204.\nEmilia Oikarinen and Tomi Janhunen. 2006. Modular\nequivalence for normal logic programs. In 17th Eu-\nropean Conference on Artificial Intelligence(ECAI),\npages 412–416.\nRasmus Palm, Ulrich Paquet, and Ole Winther. 2018.\nRecurrent relational networks. In Proceedings of\nAdvances in Neural Information Processing Systems,\npages 3368–3378.\nLinlu Qiu, Hexiang Hu, Bowen Zhang, Peter Shaw, and\nFei Sha. 2021. Systematic generalization on gscan:\nWhat is nearly solved and what is next? In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 2180–2188.\nRaymond Reiter. 2001. Knowledge in Action: Log-\nical Foundations for Specifying and Implementing\nDynamical Systems. MIT Press.\nMatthew Richardson and Pedro Domingos. 2006.\nMarkov logic networks. Machine Learning, 62(1-\n2):107–136.\nSebastian Ruder. 2021. Challenges and Opportuni-\nties in NLP Benchmarking. http://ruder.io/\nnlp-benchmarking.\nLaura Ruis, Jacob Andreas, Marco Baroni, Diane\nBouchacourt, and Brenden M Lake. 2020. A bench-\nmark for systematic generalization in grounded lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 33:19861–19872.\nShailaja Sampat and Joohyung Lee. 2018. A model-\nbased approach to visual reasoning on cnlvr dataset.\nIn Sixteenth International Conference on Principles\nof Knowledge Representation and Reasoning.\nAdam Santoro, David Raposo, David G Barrett, Ma-\nteusz Malinowski, Razvan Pascanu, Peter Battaglia,\nand Timothy Lillicrap. 2017. A simple neural net-\nwork module for relational reasoning. In Advances in\nneural information processing systems, pages 4967–\n4976.\nMd Kamruzzaman Sarker, Lu Zhou, Aaron Eberhart,\nand Pascal Hitzler. 2021. Neuro-symbolic artificial\nintelligence. AI Communications, pages 1–13.\nImanol Schlag and Jürgen Schmidhuber. 2018. Learn-\ning to reason with third order tensor products. Ad-\nvances in neural information processing systems, 31.\nNathan Schucher, Siva Reddy, and Harm de Vries. 2022.\nThe power of prompt tuning for low-resource seman-\ntic parsing. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 148–156.\nMin Joon Seo, Sewon Min, Ali Farhadi, and Hannaneh\nHajishirzi. 2017. Query-reduction networks for ques-\ntion answering. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Pro-\nceedings.\n5196\nMurray Shanahan. 1995. A circumscriptive calculus of\nevents. Artif. Intell., 77(2):249–284.\nZhengxiang Shi, Qiang Zhang, and Aldo Lipani. 2022.\nStepgame: A new benchmark for robust multi-hop\nspatial reasoning in texts. Association for the Ad-\nvancement of Artificial Intelligence.\nRichard Shin, Christopher Lin, Sam Thomson, Charles\nChen Jr, Subhro Roy, Emmanouil Antonios Platanios,\nAdam Pauls, Dan Klein, Jason Eisner, and Benjamin\nVan Durme. 2021. Constrained language models\nyield few-shot semantic parsers. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 7699–7715.\nKoustuv Sinha, Shagun Sodhani, Jin Dong, Joelle\nPineau, and William L Hamilton. 2019. Clutrr: A di-\nagnostic benchmark for inductive reasoning from text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 4506–4515.\nJidong Tian, Yitian Li, Wenqing Chen, HE Hao, and\nYaohui Jin. 2021. A generative-symbolic model for\nlogical reasoning in nlu. In Is Neuro-Symbolic SOTA\nstill a myth for Natural Language Inference? The\nfirst workshop.\nKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan,\nand Subbarao Kambhampati. 2022. Large language\nmodels still can’t plan (a benchmark for LLMs on\nplanning and reasoning about change). In NeurIPS\n2022 Foundation Models for Decision Making Work-\nshop.\nChenguang Wang, Xiao Liu, and Dawn Song. 2022.\nIelm: An open information extraction benchmark for\npre-trained language models. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing, page 8417–8437. Association\nfor Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nJason Weston, Antoine Bordes, Sumit Chopra, and\nTomás Mikolov. 2016. Towards ai-complete question\nanswering: A set of prerequisite toy tasks. In 4th In-\nternational Conference on Learning Representations,\nICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,\nConference Track Proceedings.\nYuk Wah Wong and Raymond Mooney. 2007. Learn-\ning synchronous grammars for semantic parsing with\nlambda calculus. In Proceedings of the 45th Annual\nMeeting of the Association of Computational Linguis-\ntics, pages 960–967.\nKexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli,\nJiajun Wu, Antonio Torralba, and Joshua B Tenen-\nbaum. 2019. CLEVRER: Collision events for video\nrepresentation and reasoning. In ICLR.\nJohn M Zelle and Raymond J Mooney. 1996. Learning\nto parse database queries using inductive logic pro-\ngramming. In Proceedings of the national conference\non artificial intelligence, pages 1050–1055.\nAndy Zeng, Adrian Wong, Stefan Welker, Krzysztof\nChoromanski, Federico Tombari, Aveek Purohit,\nMichael Ryoo, Vikas Sindhwani, Johnny Lee, Vin-\ncent Vanhoucke, et al. 2022. Socratic models: Com-\nposing zero-shot multimodal reasoning with lan-\nguage. arXiv preprint arXiv:2204.00598.\nLuke S Zettlemoyer and Michael Collins. 2005. Learn-\ning to map sentences to logical form: structured clas-\nsification with probabilistic categorial grammars. In\nProceedings of the Twenty-First Conference on Un-\ncertainty in Artificial Intelligence, pages 658–666.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. arXiv preprint\narXiv:2205.10625.\nWenxuan Zhou and Muhao Chen. 2022. An improved\nbaseline for sentence-level relation extraction. InPro-\nceedings of the 2nd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics and the 12th International Joint Conference\non Natural Language Processing (Volume 2: Short\nPapers), pages 161–168, Online only. Association for\nComputational Linguistics.\n5197\nAppendix\nSection A presents another experiment with\nrobot planning. Section B discusses more details\nabout how we generated CLUTRR dataset and the\nexperimental result on CLUTRR 1.0. Section C\npresents GPT-3 prompts for semantic parsing. Sec-\ntion D enumerates the errors with GPT-3 in seman-\ntic parsing. Section E presents ASP knowledge\nmodules we used for the experiments. Section F\nenumerates the errors in the datasets.\nFor bAbI, the prompts for the baseline few-shot\nprompting can be found in the directory bAbI_\nbaseline/example_prompts, while the\nprompts for chain-of-thought can be found in\nbAbI_baseline/COT_prompts_v3. For\nStepGame, the prompts for the baseline few-shot\nprompting and chain-of-thought can be found in\nthe directory stepGame/prompts. The follow-\ning table records the cost for GPT-3 queries used\nin GPT-3 baselines and our method, where Eng.\ndenotes the engine of GPT-3, c1, d2, d3 denote\ntext-curie-001, text-davinci-002, and text-davinci-\n003.\nDataset Method Eng. #Data Cost\nbAbI\nFew-Shot d3 20k $190\nCoT d3 20k $280\nGPT-3+ASP d3 20k $41\nStepGame\nFew-Shot d3 1k $21\nCoT d3 1k $26\nGPT-3+ASP c1 10k $89\nGPT-3+ASP d2 10k $886\nCLUTRR 1.0 GPT-3+ASP d3 879 $37\nCLUTRR 1.3 GPT-3+ASP d3 400 $17\nCLUTRR-S GPT-3+ASP d3 563 $19\ngSCAN GPT-3+ASP c1 8k $0.2\nPick&Place Few-Shot d3 40 $0.5\nGPT-3+ASP d3 40 $0.4\nAll experiments were conducted on Ubuntu\n18.04.2 LTS with two 10-core CPU Intel(R)\nXeon(R) CPU E5-2640 v4 @ 2.40GHz and four\nGP104 [GeForce GTX 1080] graphics cards.\nAll datasets used in this paper are publicly avail-\nable. The bAbI dataset is under BSD license. The\nCLUTRR dataset is released under “Attribution-\nNonCommercial 4.0 International” license. The\nStepGame dataset doesn’t have a specified license.\nThe gSCAN dataset is released under MIT license.\nA Robot Planning\nRecently, there has been increasing interest in us-\ning LLMs to find a sequence of executable actions\nfor robots, aiming to achieve high-level goals ex-\npressed in natural language, such as SayCan (Ahn\net al., 2022) and Innermonologue (Huang et al.,\n2022). However, it is worth noting that the actions\ngenerated by LLMs tend to be loosely connected\nand do not take into account the intermediate state\nchanges that occur during the execution of these\nactions.\nFigure 3: The GPT-3+ASP pipeline for Pick&Place\nWe based our work on SayCan’s open-source vir-\ntual tabletop environment6, where a robot is tasked\nwith achieving a goal, such as \"stack the blocks,\" on\na table with colored blocks and bowls. We noticed\nthat the successful plans demonstrated by SayCan\nare restricted to simple one-step look-ahead plans\nthat do not take into account intermediate state\nchanges.\nWe randomly sampled 40 data instances of the\nform ⟨Si, Sg, L⟩in the Pick&Place domain with\n4 to 7 blocks and 3 to 7 bowls, possibly stacked\ntogether and with 3 to 10 steps of pick_and_place\nactions required by the robot to change the initial\nstate Si to the goal state Sg. Here, the label L is\nthe set of instructions to achieve the goals (e.g., “1.\nMove the violet block onto the blue block. 2...”).\nAmong 40 data instances, 20 data instances contain\nonly blocks that can be placed on the table while\n20 data instances contain both blocks and bowls\nand assume all blocks must be on the bowls.\nThe baseline for this dataset follows the method\nin SayCan’s open-source virtual tabletop environ-\nment, where GPT-3 is used as the large language\nmodel to directly find the sequence of actions\nfrom Si to Sg. However, the baseline fails to\nfind successful plans for all 40 randomly sampled\ndata instances. This result confirms the claim by\n(Valmeekam et al., 2022) that large language mod-\nels are not suitable as planners.\nWe also applied our method to this task. We let\nGPT-3 turn the statesSi and Sg into atomic facts of\nthe form on(A, B,0) and on(A, B), respectively.\n6https://github.com/google-research/\ngoogle-research/tree/master/saycan\n5198\nThen, an ASP program for the Pick&Place domain\nis used to find an optimal plan. We found that\nwhile GPT-3 has only 0% accuracy in predicting the\nwhole plan, it has 100% accuracy in fact extraction\nunder the provided format. When we apply sym-\nbolic reasoning to these extracted atomic facts with\nan ASP program, we could achieve 100% accuracy\non the predicted plans. Details of the prompts are\navailable in Appendix C.5.\nMethod Blocks Blocks+Bowls\nGPT-3(d3) 0 0\nGPT-3(d3)+ASP 100 100\nTable 6: Test accuracy on the Pick&Place dataset. (d3)\ndenotes the text-davinci-003 model.\nFigure 4: A simple plan predicted by GPT-3+ASP in\nthe Pick&Place domain.\nB More about CLUTRR\nB.1 CLUTRR 1.3 Data Generation\nWe used CLUTRR 1.3 codes to generate 400 test\ndata instances. 7 Our generated CLUTRR 1.3\ndataset consists of 100 data for each of the four cat-\negories: (assuming that the query is asking about\nthe relation between persons A and D)\n• clean: each story describes 3 relations in a\nchain of four persons A −B −C −D;\n7We used the development branch of CLUTRR repository\nhttps://github.com/facebookresearch/clutrr/tree/develop.\n• supporting: each story describes 3 relations in\na chain of four persons A −B −C −D as\nwell as an additional relation X −Y such that\nX, Y∈{A, B, C, D}and X −Y is not the\nqueried pair;\n• irrelevant: each story describes 3 relations in\na chain of four persons A −B −C −D as\nwell as an additional relation X −Y such that\nX ∈{A, B, C, D}and Y ̸∈{A, B, C, D};\n• disconnected: each story describes 3 relations\nin a chain of four persons A −B −C −D as\nwell as an additional relation X −Y such that\nX, Y̸∈{A, B, C, D}.\nB.2 Evaluation on CLUTRR 1.0\nTraining Testing BA GSM d2 d3\nClean\nClean 58 69 63 68\nSupporting 76 66 62 62\nIrrelevant 70 77 66 71\nDisconnected 49 36 59 59\nSupporting Supporting 67 49 83 83\nIrrelevant Irrelevant 51 63 72 75\nDisconnected Disconnected 57 53 63 67\nTable 7: Test accuracy on the CLUTRR dataset. BA\ndenotes BiLSTM-Attention. d2 and d3 denote GPT-\n3+ASP with text-davinci-002 and text-davinci-003\nmodel.\nTable 7 compares the accuracy of our method\nand the state-of-the-art methods, i.e., BiLSTM-\nAttention (Sinha et al., 2019) and GSM (with a\nBiLSTM encoder) (Tian et al., 2021), on the (origi-\nnal) CLUTRR test dataset. Except for our method,\nall other models are trained on a specific split of\nthe CLUTRR training dataset.\nTraining Testing DP d2 d3\nClean\nClean 100 100 100\nSupporting 99 96 99\nIrrelevant 98 99 100\nDisconnected 99 98 100\nSupporting Supporting 100 100 100\nIrrelevant Irrelevant 100 97 100\nDisconnected Disconnected 94 97 100\nTable 8: Test accuracy on the CLUTRR-S dataset. DP\ndenotes DeepProbLog, d2 and d3 denote GPT-3+ASP\nwith the text-davinci-002 and text-davinci-003 model.\nTable 8 compares the accuracy of our method\nand the state-of-the-art method, DeepProbLog\n(Manhaeve et al., 2021) on the CLUTRR-S test\ndataset. With GPT-3(d2)+ASP on the CLUTRR-\nS dataset, 550 out of 563 predictions are correct,\n5199\nand 13 are wrong. All errors occur due to the en-\ntities in a relation being swapped. For example,\nwe use “ son(A,B)” to represent “A has a son\nB” while GPT-3 text-davinci-002 responded with\n“son(Robert,Ryan)” for the sentence “Robert\nis Ryan’s son.” On the other hand, text-davinci-003\nperformed better, with only a single error and 562\nout of 563 predictions being correct.\nC Prompts for Semantic Parsing\nBelow, we present the details of the general knowl-\nedge of the prompts that we summarized and ap-\nplied in this paper, followed by some examples.\n1. If the information in a story (or query) can\nbe extracted independently, parsing each sen-\ntence separately (using the same prompt multi-\nple times) typically works better than parsing\nthe whole story. Since people usually cache all\nGPT-3 responses to save cost by avoiding du-\nplicated GPT-3 requests for the same prompt,\nparsing each sentence separately also yields\nbetter usage of cached responses. Below are\nsome examples.\n• In most bAbI tasks (except for tasks 11\nand 13), the sentences in a story (includ-\ning the query sentence) are independent\nof each other. We parse each sentence\nseparately using GPT-3 as in the Ap-\npendix C.1.\n• In the stepGame dataset, each sentence\nin a story describes the spatial relation\nbetween 2 objects. There are 4 sentences\nin a story when k = 1and about 20 sen-\ntences when k = 10. If we ask GPT-3\nto extract all the atomic facts from the\nwhole story, it always misses some atoms\nor predicts wrong atoms. Since every\nsentence is independent of each other as\nshown in Figure 1, we use the follow-\ning (truncated) prompt multiple times\nfor each data instance where each time\n[INPUT] is replaced with one sentence\nin the story or the query. This yields a\nmuch higher accuracy as in Section 4.3.\nThe complete prompt is available in Ap-\npendix C.2.\nPlease parse each sentence into a fact\n. If the sentence is describing\nclock -wise information , then 12\ndenotes top , 1 and 2 denote\ntop_right , 3 denotes right , ... If\nthe sentence is describing\ncardinal directions , then north\ndenotes top , ...\nSentence: What is the relation of the\nagent X to the agent K?\nSemantic Parse: query (\"X\", \"K\").\nSentence: H is positioned in the front\nright corner of M.\nSemantic Parse: top_right (\"H\", \"M\").\n...\nSentence: [INPUT]\nSemantic Parse:\nHowever, if some sentences in a story are\ndependent, splitting them may lead to unex-\npected results in the GPT-3 response. Below\nare some examples.\n• In bAbI task #11 and #13, a story may\ncontain the two consecutive sentences\n“Mary went back to the bathroom. After\nthat she went to the bedroom.” There is\na dependency on the sentences to under-\nstand that “she” in the second sentence\nrefers to “Mary” in the first. For this\nreason, task #11 stories are parsed as a\nwhole. This is similar for task #13.\n• In the CLUTRR dataset, a story may\ncontain sentences with coreferences like\n“Shirley enjoys playing cards with her\nbrother. His name is Henry.” where\nthe latter sentence depends on the for-\nmer one, and a family relation can be\ncorrectly extracted only with both sen-\ntences. Thus for CLUTRR datasets\n(i.e., CLUTRR 1.0, CLUTRR 1.3, and\nCLUTRR-S), we extract the family rela-\ntions and gender relations from the whole\nstory.\n2. There is certain commonsense knowledge that\nGPT-3 is not aware of, and describing the\nmissing knowledge in the prompt works bet-\nter than adding examples only. This happens\nwhen GPT-3 cannot generalize such knowl-\nedge well with a few examples.\n• For example, in StepGame dataset, clock\nnumbers are used to denote cardinal di-\nrections, e.g., “H is below J at 4 o’clock”\nmeans “H is on the bottom-right of J”.\nSuch knowledge in the dataset is not\nwell captured by GPT-3 and enumerat-\ning examples in the prompt doesn’t work\n5200\nwell. On the other hand, describing\nsuch knowledge at the beginning of the\nprompt as shown in Appendix C.2 in-\ncreases the accuracy by a large margin.\nC.1 bAbI\nFor bAbI dataset, there are two prompts for each\ntask, corresponding to the context and query. Each\nprompt has a consistent set of basic instructions\nfollowed by example pairs of text and parsed\ntext. Below are the prompts used to parse the\ncontext and query facts from a story and query,\nwhere [Input] at the end is replaced with the\nstory in each test data instance. We only present\nthe prompts for Tasks 1,2, and 3. The rest of the\nprompts can be found in the repository in https:\n//github.com/azreasoners/LLM-ASP/\nblob/main/bAbI/GPT_prompts.py.\nTasks 1/2/3 (Context)\nPlease parse the following statements into facts\n. The available keywords are: pickup , drop ,\nand go.\nSentence: Max journeyed to the bathroom.\nSemantic parse: go(Max , bathroom).\nSentence: Mary grabbed the football there.\nSemantic parse: pickup(Mary , football).\nSentence: Bob picked up the apple.\nSemantic parse: pickup(Bob , apple).\nSentence: Susan dropped the milk.\nSemantic parse: drop(Susan , milk).\nSentence: Bob got the football there.\nSemantic parse: pickup(Bob , football).\nSentence: Max left the cup.\nSemantic parse: drop(Max , cup).\nSentence: Kevin put down the pie there.\nSemantic parse: drop(Kevin , pie).\nSentence: John took the football there.\nSemantic parse: pickup(John , football).\nSentence: [INPUT]\nSemantic parse:\nTask 1 (Query)\nPlease parse the following questions into query\nfacts. The available keywords are:\nwhereAgent.\nSentence: Where is Mary?\nSemantic parse: whereAgent(Mary).\nSentence: Where is Daniel?\nSemantic parse: whereAgent(Daniel).\nSentence: Where is Sandra?\nSemantic parse: whereAgent(Sandra).\nSentence: Where is John?\nSemantic parse: whereAgent(John).\nSentence: [INPUT]\nSemantic parse:\nTask 2 (Query)\nPlease parse the following questions into query\nfacts. The available keywords are: loc.\nSentence: Where is the toothbrush?\nSemantic parse: loc(toothbrush).\nSentence: Where is the milk?\nSemantic parse: loc(milk).\nSentence: Where is the apple?\nSemantic parse: loc(apple).\nSentence: Where is the football?\nSemantic parse: loc(football).\nSentence: [INPUT]\nSemantic parse:\nTask 3 (Query)\nPlease parse the following questions into query\nfacts. The available keywords are: loc.\nSentence: Where was the football before the\nbathroom?\nSemantic parse: before(football ,bathroom).\nSentence: Where was the apple before the garden?\nSemantic parse: before(apple ,garden).\nSentence: Where was the milk before the kitchen?\nSemantic parse: before(milk ,kitchen).\nSentence: Where was the apple before the bedroom\n?\nSemantic parse: before(apple ,bedroom).\nSentence: Where was the football before the\nhallway?\nSemantic parse: before(football ,hallway).\nSentence: [INPUT]\nSemantic parse:\nC.2 StepGame\nFor the StepGame dataset, there is only one prompt\nbelow to extract the location relations among ob-\njects. All example sentences are from the training\ndata in (the noise split of) the original StepGame\ndataset.8 The [Input] at the end of the prompt is\nreplaced with each sentence in a test data instance.\nPlease parse each sentence into a fact. If the\nsentence is describing clock -wise\ninformation , then 12 denotes top , 1 and 2\ndenote top_right , 3 denotes right , 4 and 5\n8https://github.com/ZhengxiangShi/\nStepGame/tree/main/Code/babi_format/\nnoise\n5201\ndenote down_right , 6 denotes down , 7 and 8\ndenote down_left , 9 denote left , 10 and 11\ndenote top_left. If the sentence is\ndescribing cardinal directions , then north\ndenotes top , east denotes right , south\ndenotes down , and west denotes left. If the\nsentence is a question , the fact starts with\nquery. Otherwise , the fact starts with one\nof top , down , left , right , top_left ,\ntop_right , down_left , and down_right.\nSentence: What is the relation of the agent X to\nthe agent K?\nSemantic Parse: query (\"X\", \"K\").\nSentence: H is positioned in the front right\ncorner of M.\nSemantic Parse: top_right (\"H\", \"M\").\nSentence: F is on the left side of and below Q.\nSemantic Parse: down_left (\"F\", \"Q\").\nSentence: Y and I are parallel , and Y is on top\nof I.\nSemantic Parse: top(\"Y\", \"I\").\nSentence: V is over there with T above.\nSemantic Parse: top(\"T\", \"V\").\nSentence: V is slightly off center to the top\nleft and G is slightly off center to the\nbottom right.\nSemantic Parse: top_left (\"V\", \"G\").\nSentence: The objects S and A are over there.\nThe object S is lower and slightly to the\nleft of the object A.\nSemantic Parse: down_left (\"S\", \"A\").\nSentence: D is diagonally below Z to the right\nat a 45 degree angle.\nSemantic Parse: down_right (\"D\", \"Z\").\nSentence: V is at A’s 9 o’clock.\nSemantic Parse: left (\"V\", \"A\").\nSentence: J is at O’s 6 o’clock.\nSemantic Parse: down (\"J\", \"O\").\nSentence: H is below J at 4 o’clock.\nSemantic Parse: down_right (\"H\", \"J\").\nSentence: O is there and C is at the 5 position\nof a clock face.\nSemantic Parse: down_right (\"C\", \"O\").\nSentence: If H is the center of a clock face , B\nis located between 10 and 11.\nSemantic Parse: top_left (\"B\", \"H\").\nSentence: [Input]\nSemantic Parse:\nC.3 CLUTRR\nFor CLUTRR dataset, there are two prompts\nto extract the family relations and genders\nfrom a story respectively. All example sto-\nries in both prompts are from the training data\n“data_06b8f2a1/2.2,2.3_train.csv” in the original\nCLUTRR dataset.9 Below is the prompt to extract\nfamily relations from a story where [Input] at\nthe end is replaced with the story in each test data\ninstance.\nGiven a story , extract atomic facts of the form\nrelation (\" Person\", \"Person \"). Example\nrelations are: father , mother , parent , son ,\ndaughter , child , grandfather , grandmother ,\ngrandson , granddaughter , wife , husband ,\nspouse , sibling , nephew , niece , uncle , aunt ,\nchild_in_law , and parent_in_law .\nStory: [Verdie] waved good bye to her dad [Henry\n] for the day and went next door with her\nsister [Amanda ]. [Henry]’s daughter , [Amanda\n], went to the city this weekend. She spent\nher time there visiting her grandfather , [\nKyle], and had a wonderful time with him.\nSemantic Parse: father (\" Verdie\", \"Henry \").\nsister (\" Verdie\", \"Amanda \"). daughter (\" Henry\n\", \"Amanda \"). grandfather (\" Amanda\", \"Kyle \").\nStory: [Michelle] was excited for today , its her\ndaughter ’s, [Theresa], spring break. She\nwill finally get to see her. [Michael] was\nbusy and sent his wife , [Marlene], instead.\n[Kristen] loved to care for her newborn\nchild [Ronald ]. [Eric]’s son is [Arthur ].\nSemantic Parse: daughter (\" Michelle\", \"Theresa \").\nwife (\" Michael\", \"Marlene \"). child (\" Kristen\n\", \"Ronald \"). son(\" Eric\", \"Arthur \").\nStory: [Vernon] was present in the delivery room\nwhen his daughter [Raquel] was born , but\nwhen his daughter [Constance] was born he\nwas too sick. [Vernon] and his daughter [\nMargaret] went to the movies. [Constance], [\nMargaret]’s sister , had to stay home as she\nwas sick.\nSemantic Parse: daughter (\" Vernon\", \"Raquel \").\ndaughter (\" Vernon\", \"Constance \"). daughter (\"\nVernon\", \"Margaret \"). sister (\" Margaret\", \"\nConstance \").\nStory: [Eric] who is [Carl]’s father grounded [\nCarl] after finding out what [Carl] had done\nat school. [Ronald] was busy planning a 90\nth birthday party for his aunt , [Theresa ]. [\nEric] and his son [Carl] went to the park\nand saw [Eric]’s father [Kyle] there with\nhis dog.\nSemantic Parse: father (\" Carl\", \"Eric \"). aunt (\"\nRonald\", \"Theresa \"). son(\" Eric\", \"Carl \").\nfather (\" Eric\", \"Kyle \").\nStory: [Shirley] and [Edward] are siblings and\nbest friends. They do everything together. [\nHenry] walked his daughters [Amanda] and [\nMichelle] to school. [Kyle] enjoys watching\nmovies with his son ’s daughter. Her name is\n[Amanda ].\nSemantic Parse: sibling (\" Shirley\", \"Edward \").\ndaughter (\" Henry\", \"Amanda \"). daughter (\" Henry\n9The original CLUTRR data is available in https://\ngithub.com/facebookresearch/clutrr.\n5202\n\", \"Michelle \"). granddaughter (\" Kyle\", \"\nAmanda \").\nStory: [Raquel] and her brother [Casey] took her\ngrandmother [Karen] to the store to buy a\nnew dress. [Karen] and her husband [Kyle]\njust celebrated 10 years of marriage. [Karen\n] loves her grandson , [Casey], and he loves\nher too.\nSemantic Parse: brother (\" Raquel\", \"Casey \").\ngrandmother (\" Raquel\", \"Karen \"). husband (\"\nKaren\", \"Kyle \"). grandson (\" Karen\", \"Casey \").\nStory: [Allen]’s father , [Eric], bought him some\nice cream. [Karen] was baking cookies for\nher grandson , [Allen ]. [Allen]’s brother [\nArthur] came home from school , so she baked\nsome extra for him , too. [Eric]’s son , [\nArthur], was ill and needed to be picked up\nat school. [Eric] hurried to his side.\nSemantic Parse: father (\" Allen\", \"Eric \").\ngrandson (\" Karen\", \"Allen \"). brother (\" Allen\",\n\"Arthur \"). son(\" Eric\", \"Arthur \").\nStory: [Karen] was spending the weekend with her\ngrandson , [Eddie ]. [Eddie]’s sister [\nMichelle] was supposed to come too , but she\nwas busy and could n’t make it. [Theresa]\ntook her daughter , [Michelle], out to High\nTea yesterday afternoon. [Eddie]’s mother [\nTheresa] baked brownies for dessert after\nthey had dinner.\nSemantic Parse: grandson (\" Karen\", \"Eddie \").\nsister (\" Eddie\", \"Michelle \"). daughter (\"\nTheresa\", \"Michelle \"). mother (\" Eddie\", \"\nTheresa \").\nStory: [Input]\nSemantic Parse:\nWe also use a variant of the above prompt to\nextract the gender of each person in a story. The\nprompt context is a bit simpler as there are only\ntwo genders. The examples are the same while the\nSemantic Parse result is simply replaced with\nthe atomic facts about gender information. Below\nis the prompt to extract the gender of each person\nin a story where [Input] is replaced with the\nstory in each test data instance.\nGiven a story , extract atomic facts of the form\nmale (\" Person \") or female (\" Person \") for every\nperson that appears in the sentences.\nStory: [Verdie] waved good bye to her dad [Henry\n] for the day and went next door with her\nsister [Amanda ]. [Henry]’s daughter , [Amanda\n], went to the city this weekend. She spent\nher time there visiting her grandfather , [\nKyle], and had a wonderful time with him.\nSemantic Parse: female (\" Verdie \"). male (\" Henry \").\nfemale (\" Amanda \"). male (\" Kyle \").\nStory: [Michelle] was excited for today , its her\ndaughter ’s, [Theresa], spring break. She\nwill finally get to see her. [Michael] was\nbusy and sent his wife , [Marlene], instead.\n[Kristen] loved to care for her newborn\nchild [Ronald ]. [Eric]’s son is [Arthur ].\nSemantic Parse: female (\" Michelle \"). female (\"\nTheresa \"). male (\" Michael \"). female (\" Marlene\n\"). female (\" Kristen \"). male (\" Ronald \"). male\n(\" Eric \"). male (\" Arthur \").\nStory: [Vernon] was present in the delivery room\nwhen his daughter [Raquel] was born , but\nwhen his daughter [Constance] was born he\nwas too sick. [Vernon] and his daughter [\nMargaret] went to the movies. [Constance], [\nMargaret]’s sister , had to stay home as she\nwas sick.\nSemantic Parse: male (\" Vernon \"). female (\" Raquel \")\n. female (\" Constance \"). female (\" Margaret \").\nStory: [Eric] who is [Carl]’s father grounded [\nCarl] after finding out what [Carl] had done\nat school. [Ronald] was busy planning a 90\nth birthday party for his aunt , [Theresa ]. [\nEric] and his son [Carl] went to the park\nand saw [Eric]’s father [Kyle] there with\nhis dog.\nSemantic Parse: male (\" Eric \"). male (\" Carl \"). male\n(\" Ronald \"). female (\" Theresa \"). male (\" Kyle \").\nStory: [Shirley] and [Edward] are siblings and\nbest friends. They do everything together. [\nHenry] walked his daughters [Amanda] and [\nMichelle] to school. [Kyle] enjoys watching\nmovies with his son ’s daughter. Her name is\n[Amanda ].\nSemantic Parse: female (\" Shirley \"). male (\" Edward\n\"). male (\" Henry \"). female (\" Amanda \"). female\n(\" Michelle \"). male (\" Kyle \").\nStory: [Raquel] and her brother [Casey] took her\ngrandmother [Karen] to the store to buy a\nnew dress. [Karen] and her husband [Kyle]\njust celebrated 10 years of marriage. [Karen\n] loves her grandson , [Casey], and he loves\nher too.\nSemantic Parse: female (\" Raquel \"). male (\" Casey \").\nfemale (\" Karen \"). male (\" Kyle \").\nStory: [Allen]’s father , [Eric], bought him some\nice cream. [Karen] was baking cookies for\nher grandson , [Allen ]. [Allen]’s brother [\nArthur] came home from school , so she baked\nsome extra for him , too. [Eric]’s son , [\nArthur], was ill and needed to be picked up\nat school. [Eric] hurried to his side.\nSemantic Parse: male (\" Allen \"). male (\" Eric \").\nfemale (\" Karen \"). male (\" Arthur \").\nStory: [Karen] was spending the weekend with her\ngrandson , [Eddie ]. [Eddie]’s sister [\nMichelle] was supposed to come too , but she\nwas busy and could n’t make it. [Theresa]\ntook her daughter , [Michelle], out to High\nTea yesterday afternoon. [Eddie]’s mother [\nTheresa] baked brownies for dessert after\nthey had dinner.\nSemantic Parse: female (\" Karen \"). male (\" Eddie \").\nfemale (\" Michelle \"). female (\" Theresa \").\nStory: [Input]\nSemantic Parse:\n5203\nFor CLUTRR-S dataset, i.e., the simpler version\nof the CLUTRR dataset from DeepProbLog (Man-\nhaeve et al., 2021) repository, there are also two\nprompts below to extract the family relations and\ngenders from a story respectively.10 All example\nstories in both prompts are from the training data\n“data_a7d9402e/2.2,2.3_train.csv”.\nGiven a story , extract atomic facts of the form\nrelation (\" Person\", \"Person \") about family\nrelationships that appear in the sentences.\nStory: [Mervin] is [Robert]’s father. [Robert]\nis the father of [Jim]. [Jon] is [Robert]’s\nbrother. [Mervin] is the father of [Jon].\nSemantic Parse: father (\" Robert\", \"Mervin \").\nfather (\"Jim\", \"Robert \"). brother (\" Robert\", \"\nJon\"). father (\"Jon\", \"Mervin \").\nStory: [Brooke] is [Cheryl]’s sister. [Jon] is\nthe father of [Brooke ]. [Melissa] is [Jon]’\ns mother. [Jon] is [Cheryl]’s father.\nSemantic Parse: sister (\" Cheryl\", \"Brooke \").\nfather (\" Brooke\", \"Jon\"). mother (\"Jon\", \"\nMelissa \"). father (\" Cheryl\", \"Jon\").\nStory: [Jon] is [Carol]’s brother. [Carol] is [\nJoyce]’s mother. [Helen] is [Carol]’s\nsister. [Helen] is a sister of [Jon].\nSemantic Parse: brother (\" Carol\", \"Jon\"). mother\n(\" Joyce\", \"Carol \"). sister (\" Carol\", \"Helen \")\n. sister (\"Jon\", \"Helen \").\nStory: [Melissa] is [Glenn]’s grandmother. [\nMelissa] is the mother of [Calvin ]. [Glenn]\nis a son of [Lila ]. [Calvin] is [Glenn]’s\nfather.\nSemantic Parse: grandmother (\" Glenn\", \"Melissa \").\nmother (\" Calvin\", \"Melissa \"). son(\" Lila\", \"\nGlenn \"). father (\" Glenn\", \"Calvin \").\nStory: [Margaret] has a brother named [William ].\n[William] is [Carol]’s son. [Margaret] is\n[Carol]’s daughter. [Lila] is the aunt of\n[William ].\nSemantic Parse: brother (\" Margaret\", \"William \").\nson(\" Carol\", \"William \"). daughter (\" Carol\", \"\nMargaret \"). aunt (\" William\", \"Lila \").\nStory: [Stephanie] is a sister of [Lois ]. [Lois\n] is [Theresa]’s sister. [Helen] is [Lois]’\ns mother. [Helen] is [Stephanie]’s mother.\nSemantic Parse: sister (\" Lois\", \"Stephanie \").\nsister (\" Theresa\", \"Lois \"). mother (\" Lois\", \"\nHelen \"). mother (\" Stephanie\", \"Helen \").\nStory: [Jon] is [Elias]’s brother. [Michael] is\na son of [Helen ]. [Jon] is the uncle of [\nMichael ]. [Elias] is the father of [Michael\n].\nSemantic Parse: brother (\" Elias\", \"Jon\"). son(\"\nHelen\", \"Michael \"). uncle (\" Michael\", \"Jon\").\nfather (\" Michael\", \"Elias \").\n10The CLUTRR-S dataset is from https://github.\ncom/ML-KULeuven/deepproblog/tree/master/\nsrc/deepproblog/examples/CLUTRR/data.\nStory: [Carol] has a son called [William ]. [\nMelissa] is the mother of [Jon]. [Jon] is\nthe uncle of [William ]. [Carol] has a\nbrother named [Jon].\nSemantic Parse: son(\" Carol\", \"William \"). mother\n(\"Jon\", \"Melissa \"). uncle (\" William\", \"Jon\").\nbrother (\" Carol\", \"Jon\").\nStory: [Robert] is the father of [Jim]. [Robert\n] has a daughter called [Ashley ]. [Elias]\nis [Robert]’s brother. [Elias] is the uncle\nof [Ashley ].\nSemantic Parse: father (\"Jim\", \"Robert \").\ndaughter (\" Robert\", \"Ashley \"). brother (\"\nRobert\", \"Elias \"). uncle (\" Ashley\", \"Elias \").\nStory: [Elias] is the father of [Carlos ]. [\nElias] is the father of [Andrew ]. [Andrew]\nis [Carlos]’s brother. [Jon] is a brother\nof [Elias ].\nSemantic Parse: father (\" Carlos\", \"Elias \").\nfather (\" Andrew\", \"Elias \"). brother (\" Carlos\",\n\"Andrew \"). brother (\" Elias\", \"Jon\").\nStory: [Jon] is the father of [Ben]. [James] is\n[Kevin]’s brother. [Ben] is a brother of [\nJames ]. [Jon] is [James]’s father.\nSemantic Parse: father (\"Ben\", \"Jon\"). brother (\"\nKevin\", \"James \"). brother (\" James\", \"Ben\").\nfather (\" James\", \"Jon\").\nStory: [Carol] has a sister named [Lila ]. [\nWilliam] is [Carol]’s son. [Helen] is [Lila\n]’s sister. [Lila] is [William]’s aunt.\nSemantic Parse: sister (\" Carol\", \"Lila \"). son(\"\nCarol\", \"William \"). sister (\" Lila\", \"Helen \").\naunt (\" William\", \"Lila \").\nStory: [Calvin] is [Bruce]’s father. [Elias] is\n[Calvin]’s brother. [Calvin] is [Kira]’s\nfather. [Kira] is [Bruce]’s sister.\nSemantic Parse: father (\" Bruce\", \"Calvin \").\nbrother (\" Calvin\", \"Elias \"). father (\" Kira\", \"\nCalvin \"). sister (\" Bruce\", \"Kira \").\nStory: [Carol] is a sister of [Helen ]. [Carol]\nis [Carlos]’s aunt. [Lila] is [Carol]’s\nsister. [Carlos] is [Helen]’s son.\nSemantic Parse: sister (\" Helen\", \"Carol \"). aunt (\"\nCarlos\", \"Carol \"). sister (\" Carol\", \"Lila \").\nson(\" Helen\", \"Carlos \").\nStory: [Input]\nSemantic Parse:\nNote that, although the sentences in the CLUTRR-\nS dataset is much simpler than those in CLUTRR\ndataset, we don’t achieve 100% accuracy in GPT-\n3 responses with the above long prompt. This\nis partially because the above prompt violates\nprompting strategy 3 in Section 3 as the order\nof names in a binary relation in sentences is\nmostly following “relationOf(A,B)” instead\nof “relation(B,A)”.\nGiven a story , extract atomic facts of the form\nmale (\" Person \") or female (\" Person \") for every\n5204\nperson that appears in the sentences.\nStory: [Jon] is [Carol]’s brother. [Mervin] has\na daughter called [Carol ]. [Chantell] is a\ndaughter of [Jon]. [Mervin] has a son\ncalled [Jon].\nSemantic Parse: male (\"Jon\"). female (\" Carol \").\nmale (\" Mervin \"). female (\" Chantell \").\nStory: [Melissa] is [Glenn]’s grandmother. [\nMelissa] is the mother of [Calvin ]. [Glenn]\nis a son of [Lila ]. [Calvin] is [Glenn]’s\nfather.\nSemantic Parse: female (\" Melissa \"). male (\" Glenn \")\n. male (\" Calvin \"). female (\" Lila \").\nStory: [Input]\nSemantic Parse:\nC.4 gSCAN\nFor gSCAN dataset, there is only one prompt below\nto extract the command in each data instance. All\nexample sequences are from the training data. 11\nThe [Input] at the end of the prompt is replaced\nwith the command in each test data instance.\nPlease parse each sequence of words into facts.\nSequence: pull a yellow small circle\nSemantic Parse: query(pull). queryDesc(yellow).\nqueryDesc(small). queryDesc(circle).\nSequence: push a big square\nSemantic Parse: query(push). queryDesc(big).\nqueryDesc(square).\nSequence: push a green small square cautiously\nSemantic Parse: query(push). queryDesc(green).\nqueryDesc(small). queryDesc(square). while(\ncautiously).\nSequence: pull a circle hesitantly\nSemantic Parse: query(pull). queryDesc(circle).\nwhile(hesitantly).\nSequence: walk to a yellow big cylinder while\nspinning\nSemantic Parse: query(walk). queryDesc(yellow).\nqueryDesc(big). queryDesc(cylinder). while(\nspinning).\nSequence: push a big square while zigzagging\nSemantic Parse: query(push). queryDesc(big).\nqueryDesc(square). while(zigzagging).\nSequence: push a cylinder hesitantly\nSemantic Parse: query(push). queryDesc(cylinder)\n. while(hesitantly).\nSequence: [Input]\nSemantic Parse:\n11https://github.com/LauraRuis/\ngroundedSCAN/tree/master/data/\ncompositional_splits.zip\nC.5 Pick&Place\nFor the Pick&Place dataset, there are two prompts\nbelow to extract the atomic facts from the initial\nstate and the goal state, respectively.\nTurn each sentence into an atomic fact of the\nform on(A, B, 0).\nSentence: The red block is on the yellow bowl.\nSemantic Parse: on(\"red block\", \"yellow bowl\",\n0).\nSentence: The violet block is on the blue block.\nSemantic Parse: on(\" violet block\", \"blue block\",\n0).\nSentence: [INPUT]\nSemantic Parse:\nTurn each sentence into an atomic fact of the\nform on(A, B).\nSentence: The red block is on the yellow bowl.\nSemantic Parse: on(\"red block\", \"yellow bowl \").\nSentence: The violet block is on the blue block.\nSemantic Parse: on(\" violet block\", \"blue block \")\n.\nSentence: [INPUT]\nSemantic Parse:\nFor each sentence in the initial or goal state, we\nreplace [INPUT] in the corresponding prompt\nabove with this sentence and request GPT-3 to ex-\ntract a single atomic fact. The union of these atomic\nfacts extracted from all sentences is then used in\nthe symbolic reasoner module to find an optimal\nplan.\nFor the GPT-3 baseline, we use the following\nprompt to let GPT-3 directly find a plan where\n[INPUT] at the end of the prompt is replaced\nwith the initial and goal state of the queried data\ninstance.\nFind a shortest plan to move blocks from an\ninitial state to a goal state. Note that you\ncannot move a block if anything is on it.\nYou cannot move a block onto a target block\nor bowl if there is anything is on the\ntarget block or bowl. At most two blocks can\nbe placed in the same bowl with one on top\nof the other.\n# Initial State:\nNothing is on the green bowl.\nThe violet block is on the blue bowl.\nThe blue block is on the violet bowl.\nThe green block is on the blue block.\n# Goal State:\nThe violet block is on the green bowl.\nThe green block is on the violet block.\nThe blue block is on the blue bowl.\n5205\nNothing is on the violet bowl.\nPlan:\n1. Move the violet block onto the green bowl.\n2. Move the green block onto the violet block.\n3. Move the blue block onto the blue bowl.\n# Initial State:\nNothing is on the blue bowl.\nThe yellow block is on the green bowl.\nThe green block is on the violet bowl.\nThe violet block is on the green block.\nThe blue block is on the yellow bowl.\nThe red block is on the blue block.\n# Goal State:\nThe yellow block is on the blue bowl.\nThe green block is on the yellow block.\nThe red block is on the green bowl.\nNothing is on the violet bowl.\nThe blue block is on the yellow bowl.\nThe violet block is on the blue block.\nPlan:\n1. Move the yellow block onto the blue bowl.\n2. Move the red block onto the green bowl.\n3. Move the violet block onto the blue block.\n4. Move the green block onto the yellow block.\n[INPUT]\nPlan:\nD GPT-3 Errors in Semantic Parsing\nIn this section, we group and record the errors in the\nGPT-3 responses in tables where each row records\na 3-tuple ⟨dataset, sentence(s), GPT-3 response ⟩.\nIn this section, we list the following.\n• all 21 errors for the CLUTRR 1.3 dataset with\ntext-davinci-003;\n• the single mistake in the first 100 data in-\nstances for every k ∈ {1, . . . ,10}in the\nStepGame dataset with text-davinci-002.\nD.1 Argument misorder\nA common mistake in the GPT-3 response is that\nthe relation and arguments for an atom are correctly\nextracted, but the order of the arguments is incor-\nrect. Such mistakes can be greatly alleviated by\nproper few-shot prompting where the orders of ar-\nguments in the example target atoms follow their\norders in the stories.\nCLUTRR\n1.3\n[Erasmus] bought to dress\nfor his father [Joeseph]\nfather(\"Joeseph\",\n\"Erasmus\")\nCLUTRR\n1.3\n[Baldwin] bought to dress\nfor his father [Sidney]\nfather(\"Sidney\",\n\"Baldwin\")\nCLUTRR\n1.3\n[Elizabeth] is the uncle of\n[Leopold].\nuncle(\"Elizabeth\",\n\"Leopold\")\nThere are only 3 errors in CLUTRR 1.3 due\nto argument misorder. The first 2 mistakes are\nindeed due to their missing periods at the end of\nthe sentences – if we simply add the periods back,\ntheir GPT-3 responses would become correct.\nD.2 Wrong relation\nSometimes the arguments are correct, but the rela-\ntions extracted by GPT-3 are incorrect or cannot be\nrecognized by the ASP program.\nCLUTRR\n1.3\n[Leila] and [Enoch] have\nbeen married for 35 years.\nmarried(\"Leila\",\n\"Enoch\")\nCLUTRR\n1.3\n[Lottie] and her grand-\nsons [Clinton] and\n[Enoch] picked berries\nfrom the garden.\ngrandsons(\n\"Lottie\", \"Clin-\nton\") grand-\nsons( \"Lottie\",\n\"Enoch\")\nCLUTRR\n1.3\n[Felix] loved going to\nthe store with his mom\n[Christena].\nmom(\"Felix\",\n\"Christena\")\nCLUTRR\n1.3\n[Archibald] asked his fa-\nther, [Abraham], to watch\nover his two sons, [Jus-\ntus] and [Merle], for the\nweekend.\nsons(\n\"Archibald\",\n\"Justus\",\n\"Merle\")\nStepGame\nIf E is the center of a\nclock face, H is located\nbetween 2 and 3.\nright(\"H\", \"E\")\nThese kinds of mistake may be resolved by re-\nstricting the space of possible relations. For ex-\nample, the mistakes in the first four rows can\nbe resolved by simply adding the sentence “Use\nspouse(\"Person\", \"Person\") if two persons are cou-\nples.” in the prompt.\nD.3 Ambiguious or incorrect co-reference\nCLUTRR\n1.3\n[Katherine] was eating\nlunch with his son\n[Daniel] and his wife\n[Adelle].\nson(\"Katherine\",\n\"Daniel\")\nwife(\"Daniel\",\n\"Adelle\")\nCLUTRR\n1.3\n[Sylvester] and his son\n[Rollie] went to his\nmother [Ottilia]’s home\nson(\"Sylvester\",\n\"Rollie\")\nmother(\"Rollie\",\n\"Ottilia\")\nCLUTRR\n1.3\n[Elsie] and [Lewis] did\nthe Father daughter dance\nat the recital and his wife\n[Dollie] was so happy she\ncried\nfather(\"Lewis\",\n\"Dollie\")\nwife(\"Lewis\",\n\"Dollie\")\nD.4 Anonymous argument\n5206\nCLUTRR\n1.3\n[Abe] is proud of his\ndaughter’s son. His name\nis [Johnie].\nson(\"Johnie\",\n\"Abe\")\nCLUTRR\n1.3\n[Watt] and [Celestine]\nasked their mother, if they\ncould go play in the pool.\nmother(\"Watt\",\n\"Celestine\")\nCLUTRR\n1.3\n[Davie] told his wife\n[Kitty] to pick up their\ndaughter.\nwife(\"Davie\",\n\"Kitty\") daugh-\nter(\"Davie\",\n\"Kitty\")\nCLUTRR\n1.3\n[Ethyl] just got married\nto her now husband, and\nin the process she got\nherself a daughter-in-law\nnamed [Henry].\nhusband(\"Ethyl\",\n\"Michael\").\ndaughter_in_law(\n\"Ethyl\", \"Henry\")\nCLUTRR\n1.3\n[Barnett] and [Manie]\nasked their mother, if they\ncould go play in the pool.\nmother(\"Barnett\",\n\"Manie\")\nCLUTRR\n1.3\n[Santiago] told his wife\n[Octavia] to pick up their\ndaughter.\nwife(\"Santiago\",\n\"Octavia\") daugh-\nter(\"Santiago\",\n\"Octavia\")\nD.5 Missed to generate some atoms\nCLUTRR\n1.3\n[Elizabeth] bought to\ndress for his father\n[Leopold] [Orson] took\nhis wife [Abbie] out for\ndinner.\nfather(\"Leopold\",\n\"Orson\")\nCLUTRR\n1.3\n[Asa] felt lonely when\nhis wife [Magdalena] was\ngone to see her mother\n[Josiephine].\nwife(\"Asa\", \"Mag-\ndalena\")\nCLUTRR\n1.3\n[Warner]’s father,\n[Johnny], and grand-\nfather, [Bryant], went\nhiking during the first\nweekend of spring.\nmale(\"Johnny\")\nmale(\"Bryant\")\nCLUTRR\n1.3\n[Hollie] and [Rosanna],\nthe happy couple, just got\nmarried last week.\n–\nCLUTRR\n1.3\n[Violet] took her brother\n[Travis] to the park, but\nleft her sister [Serena] at\nhome.\nbrother(\"Violet\",\n\"Travis\")\nE ASP Knowledge Modules\nE.1 Discrete Event Calculus (DEC) Axioms\nModule\n% (DEC1)\nstopped_in(T1 ,F,T2) :- timepoint(T),\ntimepoint(T1),\ntimepoint(T2),\nfluent(F),\nevent(E),\nhappens(E,T),\nT1 <T,\nT<T2 ,\nterminates(E,F,T).\n% (DEC2)\nstarted_in(T1 ,F,T2) :- timepoint(T),\ntimepoint(T1),\ntimepoint(T2),\nfluent(F),\nevent(E),\nhappens(E,T),\nT1 <T,\nT<T2 ,\ninitiates(E,F,T).\n% (DEC3)\nholds_at(F2 ,T1+T2) :- timepoint(T1),\ntimepoint(T2),\nfluent(F1),\nfluent(F2),\nevent(E),\nhappens(E,T1),\ninitiates(E,F1 ,T1),\n0<T2 ,\ntrajectory(F1 ,T1 ,F2 ,T2),\nnot stopped_in(T1 ,F1 ,T1+T2\n).\n% (DEC4)\nholds_at(F2 ,T1+T2) :- timepoint(T1),\ntimepoint(T2),\nfluent(F1),\nfluent(F2),\nevent(E),\nhappens(E,T1),\nterminates(E,F1 ,T1),\n0<T2 ,\nanti_trajectory (F1 ,T1 ,F2 ,\nT2),\nnot started_in(T1 ,F1 ,T1+T2\n).\ninitiated(F,T) :- timepoint(T),\nfluent(F),\nevent(E),\nhappens(E,T),\ninitiates(E,F,T).\nterminated(F,T) :- timepoint(T),\nfluent(F),\nevent(E),\nhappens(E,T),\nterminates(E,F,T).\nreleased(F,T) :- timepoint(T),\nfluent(F),\nevent(E),\nhappens(E,T),\nreleases(E,F,T).\n% (DEC5)\nholds_at(F,T+1) :- timepoint(T),\nfluent(F),\nholds_at(F,T),\n-released_at(F,T+1),\nnot terminated(F,T).\n% (DEC6)\n-holds_at(F,T+1) :- timepoint(T),\nfluent(F),\n-holds_at(F,T),\n-released_at(F,T+1),\nnot initiated(F,T).\n% (DEC7)\nreleased_at(F,T+1) :- timepoint(T),\nfluent(F),\nreleased_at(F,T),\nnot initiated(F,T),\nnot terminated(F,T).\n5207\nTask DEC Axioms Action Location Family Relation\n1: Single supporting fact ✓ ✓\n2: Two supporting facts ✓ ✓\n3: Three supporting facts ✓ ✓\n4: Two arg relations ✓\n5: Three arg relations ✓\n6: Yes/no questions ✓ ✓\n7: Counting ✓ ✓\n8: Lists/sets ✓ ✓\n9 : Simple negation ✓ ✓\n10: Indefinite knowledge ✓ ✓\n11: Basic coreference ✓ ✓\n12: Conjunction ✓ ✓\n13: Compound coreference ✓ ✓\n14: Time reasoning ✓ ✓\n15: Basic deduction\n16: Basic induction\n17: Positional reasoning ✓\n18: Size reasoning\n19: Path finding ✓ ✓ ✓\n20: Agents motivations\nStepGame ✓\ngSCAN ✓ ✓\nCLUTRR ✓\nPick&Place ✓ ✓\nTable 9: Knowledge modules used for each of the tasks. Note that DEC Axioms, action, and location modules are\nused in at least two datasets. Some domains aren’t listed as they are small and domain specific.\n% (DEC8)\n-released_at(F,T+1) :- timepoint(T),\nfluent(F),\n-released_at(F,T),\nnot released(F,T).\n% (DEC9)\nholds_at(F,T+1) :- timepoint(T),\nfluent(F),\nevent(E),\nhappens(E,T),\ninitiates(E,F,T).\n% (DEC10)\n-holds_at(F,T+1) :- timepoint(T),\nfluent(F),\nevent(E),\nhappens(E,T),\nterminates(E,F,T).\n% (DEC11)\nreleased_at(F,T+1) :- timepoint(T),\nfluent(F),\nevent(E),\nhappens(E,T),\nreleases(E,F,T).\n% (DEC12)\n-released_at(F,T+1) :- timepoint(T),\nfluent(F),\nevent(E),\nhappens(E,T),\ninitiates(E,F,T).\n-released_at(F,T+1) :- timepoint(T),\nfluent(F),\nevent(E),\nhappens(E,T),\nterminates(E,F,T).\nE.2 Action Module\n%********************\n* common interface\n* check: if location(unknown) is needed\n*********************%\n% what happened in the given story\nhappens(action(A, pickup , I), T) :- pickup(A, I,\nT).\nhappens(action(A, drop , I), T) :- drop(A, I, T).\nhappens(action(A1 , give , A2 , I), T) :- give(A1 ,\nI, A2 , T).\nhappens(action(A, goto , L), T) :- go(A, L, T).\nhappens(action(A, goto , L), T) :- isIn(A, L, T).\n%********************\n* basic atoms\n*********************%\ndirection(east; west; north; south).\nagent(A) :- happens(action(A, _, _), _).\nagent(A) :- happens(action(A, _, _, _), _).\nagent(A) :- happens(action(_, give , A, _), _).\nitem(I) :- happens(action(_, pickup , I), _).\nitem(I) :- happens(action(_, drop , I), _).\nitem(I) :- happens(action(_, give , _, I), _).\nlocation(L) :- happens(action(_, goto , L), _),\nnot direction(L).\n%********************\n* atoms in DEC_AXIOMS\n*********************%\n5208\n% event /1\nevent(action(A, pickup , I)) :- agent(A), item(I)\n.\nevent(action(A, drop , I)) :- agent(A), item(I).\nevent(action(A1 , give , A2 , I)) :- agent(A1),\nagent(A2), item(I), A1 != A2.\nevent(action(A, goto , L)) :- agent(A), location(\nL).\nevent(action(A, goto , D)) :- agent(A), direction\n(D).\nevent(action(robot , pick_and_place , Src , Dst))\n:- feature(Src , block), location(Dst), Src\n!= Dst.\n% timepoint /1\ntimepoint(T) :- happens(_, T). % the timepoint\nin story\ntimepoint(T) :- T=0..N, maxtime(N). % the\ntimepoint for planning without story\n% fluent /1\nfluent(at(A, L)) :- agent(A), location(L).\nfluent(at(I, L)) :- item(I), location(L).\nfluent(carry(A, I)) :- agent(A), item(I).\nfluent(on(B, L)) :- feature(B, block), location(\nL), B!=L.\n% -released_at /2\n% 1. -released_at(F, T) means commonsense law\nof inertia (CLI) can be applied to fluent F\nat T\n% 2. CLI is also applied to this literal\nitself\n-released_at(F, 0) :- fluent(F).\n% holds_at /2\n% initial states of fluents -- only location of\nitems needs to be guessed\n{holds_at(at(I, L), 0): location(L)} = 1 :- item\n(I).\nholds_at(on(B, L), 0) :- on(B, L, 0).\n% happens /2\n% for each timepoint , at most 1 event happens;\nand it happens as fewer as possible\n% {happens(E, T): event(E)}1 :- timepoint(T). %\nthis rule would slow down many tasks\n:∼ happens(E, T). [1@0 , E, T]\n% every action should have some effect\n%:- happens(E,T), not initiates(E,_,T).\n% precondition on actions -- pickup\n:- happens(action(A, pickup , I), T), holds_at(at\n(A, L1), T), holds_at(at(I, L2), T), L1 !=\nL2.\n% initiates /3 and terminates /3\n% effect of actions -- pickup\ninitiates(action(A, pickup , I), carry(A, I), T)\n:- agent(A), item(I), timepoint(T).\n% effect of actions -- drop\nterminates(action(A, drop , I), carry(A, I), T)\n:- agent(A), item(I), timepoint(T).\n% effect of actions -- give\ninitiates(action(A1 , give , A2 , I), carry(A2 , I),\nT) :- agent(A1), agent(A2), item(I),\ntimepoint(T), A1 != A2.\nterminates(action(A1 , give , A2 , I), carry(A1 , I)\n, T) :- agent(A1), agent(A2), item(I),\ntimepoint(T), A1 != A2.\n% effect of actions -- goto\ninitiates(action(A, goto , L), at(A, L), T) :-\nagent(A), location(L), timepoint(T).\ninitiates(action(A, goto , L), at(I, L), T) :-\nholds_at(carry(A, I), T), location(L).\ninitiates(action(A, goto , D), at(A, L2), T) :-\nagent(A), location(L1), location(L2),\ntimepoint(T),\nholds_at(at(A, L1), T), is(L2 , D, L1).\nterminates(action(A, goto , L1), at(A, L2), T) :-\nagent(A), location(L1), location(L2),\ntimepoint(T), L1 != L2.\nterminates(action(A, goto , L1), at(I, L2), T) :-\nholds_at(carry(A, I), T), location(L1),\nlocation(L2), L1 != L2.\nterminates(action(A, goto , Direction), at(A, L),\nT) :-\nhappens(action(A, goto , Direction), T),\nholds_at(at(A, L), T), Direction != L.\n% effect of actions -- pick_and_place\ninitiates(action(robot , pick_and_place , Src , Dst\n), on(Src , Dst), T) :-\nfeature(Src , block), location(Dst), Src !=\nDst , timepoint(T),\nnot holds_at(on(_, Src), T),\nnot holds_at(on(_, Dst), T): Dst !=\" table \".\nterminates(action(robot , pick_and_place , Src ,\nDst), on(Src , L), T) :-\nholds_at(on(Src , L), T), location(Dst), Dst\n!= L.\nE.3 Location Module\n% general format translation , which can also be\neasily done in python script\n% (this is not needed if we directly extract the\ngeneral form in the beginning as in bAbI\ntask4)\nis(A, top , B) :- top(A, B).\nis(A, top , B) :- up(A, B).\nis(A, down , B) :- down(A, B).\nis(A, left , B) :- left(A, B).\nis(A, right , B) :- right(A, B).\nis(A, top_left , B) :- top_left(A, B).\nis(A, top_right , B) :- top_right(A, B).\nis(A, down_left , B) :- down_left(A, B).\nis(A, down_right , B) :- down_right(A, B).\nis(A, east , B) :- east(A, B).\nis(A, west , B) :- west(A, B).\nis(A, south , B) :- south(A, B).\nis(A, north , B) :- north(A, B).\n% synonyms\nsynonyms(\nnorth , northOf; south , southOf; west , westOf\n; east , eastOf;\ntop , northOf; down , southOf; left , westOf;\nright , eastOf\n).\nsynonyms(A, B) :- synonyms(B, A).\nsynonyms(A, C) :- synonyms(A, B), synonyms(B, C)\n, A!=C.\n5209\n% define the offsets of 8 spacial relations\noffset(\noverlap ,0 ,0; top ,0 ,1; down ,0,-1; left ,-1,0;\nright ,1 ,0;\ntop_left ,-1,1; top_right ,1 ,1; down_left\n,-1,-1; down_right ,1,-1\n).\n% derive the kind of spacial relation from\nsynonyms and offset\nis(A, R1 , B) :- is(A, R2 , B), synonyms(R1 , R2).\nis(A, R1 , B) :- is(B, R2 , A), offset(R2 ,X,Y),\noffset(R1 ,-X,-Y).\n% derive the location of every object\n% the search space of X or Y coordinate is\nwithin -100 and 100\n% (to avoid infinite loop in clingo when data\nhas error)\nnums ( -100..100).\nlocation(A, Xa , Ya) :-\nlocation(B, Xb , Yb), nums(Xa), nums(Ya),\nis(A, Kind , B), offset(Kind , Dx , Dy),\nXa -Xb=Dx , Ya -Yb=Dy.\nlocation(B, Xb , Yb) :-\nlocation(A, Xa , Ya), nums(Xb), nums(Yb),\nis_on(A, Kind , B), offset(Kind , Dx , Dy),\nXa -Xb=Dx , Ya -Yb=Dy.\nE.4 Family Module\n% gender\nmale(B) :- grandson(A, B).\nmale(B) :- son(A, B).\nmale(B) :- nephew(A, B).\nmale(B) :- brother(A, B).\nmale(B) :- father(A, B).\nmale(B) :- uncle(A, B).\nmale(B) :- grandfather(A, B).\nfemale(B) :- granddaughter (A, B).\nfemale(B) :- daughter(A, B).\nfemale(B) :- niece(A, B).\nfemale(B) :- sister(A, B).\nfemale(B) :- mother(A, B).\nfemale(B) :- aunt(A, B).\nfemale(B) :- grandmother(A, B).\n% gender -irrelevant relationships\nsibling(A, B) :- siblings(A, B).\nsibling(A, B) :- brother(A, B).\nsibling(A, B) :- sister(A, B).\nsibling(A, B) :- parent(A, C), parent(B, C), A\n!= B.\nsibling(A, B) :- sibling(B, A).\nsibling(A, B) :- sibling(A, C), sibling(C, B), A\n!= B.\nsibling(A, B); sibling_in_law (A, B) :- child(A,\nC), uncle(C, B).\nsibling(A, B); sibling_in_law (A, B) :- child(A,\nC), aunt(C, B).\nsibling_in_law (A, B) :- sibling_in_law (B, A).\n:- spouse(A, B), sibling(A, B).\n:- spouse(A, B), sibling_in_law (A, B).\n:- sibling(A, B), sibling_in_law (A, B).\nspouse(A, B) :- wife(A, B).\nspouse(A, B) :- husband(A, B).\nspouse(A, B) :- spouse(B, A).\nparent(A, B) :- father(A, B).\nparent(A, B) :- mother(A, B).\nparent(A, B) :- parent(A, C), spouse(C, B).\nparent(A, B) :- sibling(A, C), parent(C, B).\nparent(A, B) :- child(B, A).\nchild(A, B) :- children(A, B).\nchild(A, B) :- son(A, B).\nchild(A, B) :- daughter(A, B).\nchild(A, B) :- spouse(A, C), child(C, B).\nchild(A, B) :- child(A, C), sibling(C, B).\nchild(A, B) :- parent(B, A).\ngrandparent(A, B) :- grandfather(A, B).\ngrandparent(A, B) :- grandmother(A, B).\ngrandparent(A, B) :- parent(A, C), parent(C, B).\ngrandparent(A, B) :- grandchild(B, A).\ngrandparent(A, B) :- sibling(A, C), grandparent(\nC, B).\ngrandparent(A, B) :- grandparent(A, C), spouse(C\n, B).\ngrandchild(A, B) :- grandson(A, B).\ngrandchild(A, B) :- granddaughter (A, B).\ngrandchild(A, B) :- grandparent(B, A).\ngreatgrandparent (A, B) :- grandparent(A, C),\nparent(C, B).\ngreatgrandchild (A, B) :- greatgrandparent (B, A).\nparent_in_law (A, B) :- spouse(A, C), parent(C, B\n).\nparent(A, B) :- spouse(A, C), parent_in_law (C, B\n).\nparent(A, B); parent_in_law (A, B) :- parent(C, A\n), grandparent(C, B).\n:- parent(A, B), parent(B, A).\n:- parent(A, B), parent_in_law (A, B).\nchild_in_law (A, B) :- parent_in_law (B, A).\n% gender -relevant relationships\ngreatgrandson (A, B) :- greatgrandchild (A, B),\nmale(B).\ngreatgranddaughter (A, B) :- greatgrandchild (A, B\n), female(B).\ngrandson(A, B) :- grandchild(A, B), male(B).\ngranddaughter (A, B) :- grandchild(A, B), female(\nB).\nson(A, B) :- child(A, B), male(B).\ndaughter(A, B) :- child(A, B), female(B).\nnephew(A, B) :- sibling(A, C), son(C, B).\nniece(A, B) :- sibling(A, C), daughter(C, B).\nhusband(A, B) :- spouse(A, B), male(B).\nwife(A, B) :- spouse(A, B), female(B).\nbrother(A, B) :- sibling(A, B), male(B).\nsister(A, B) :- sibling(A, B), female(B).\nfather(A, B) :- parent(A, B), male(B).\nmother(A, B) :- parent(A, B), female(B).\nuncle(A, B) :- parent(A, C), brother(C, B).\n5210\naunt(A, B) :- parent(A, C), sister(C, B).\ngrandfather(A, B) :- grandparent(A, B), male(B).\ngrandmother(A, B) :- grandparent(A, B), female(B\n).\ngreatgrandfather (A, B) :- greatgrandparent (A, B)\n, male(B).\ngreatgrandmother (A, B) :- greatgrandparent (A, B)\n, female(B).\nson_in_law(A, B) :- child_in_law(A, B), male(B).\ndaughter_in_law (A, B) :- child_in_law(A, B),\nfemale(B).\nfather_in_law (A, B) :- parent_in_law (A, B), male\n(B).\nmother_in_law (A, B) :- parent_in_law (A, B),\nfemale(B).\nE.5 Domain Specific Modules\nIn this section, we list all domain-specific rules for\neach task. Some rules serve as an interface to turn\nthe atoms in GPT-3 responses into a general format\nused in ASP modules. These rules are not neces-\nsary and can be removed if we let GPT-3 directly\nreturn the general atoms, e.g., “ query(at(A,\nwhere))” instead of “ whereAgent(A)”. To\nsave the cost for GPT-3 requests, we did not re-\nproduce the experiments using new GPT-3 prompts\nwith atoms in general formats.\nE.5.1 bAbI Tasks 1 and 11\n%%%% Interface -- these rules can be removed if\nwe let GPT3 return the heads directly\nquery(at(A, where)) :- whereAgent(A).\n% Find where last location of agent is\nanswer(L) :- query(at(A, where)), holds_at(at(A,\nL), T), T>=Tx: holds_at(at(A, _), Tx).\nE.5.2 bAbI Task 2\n%%%% Interface -- these rules can be removed if\nwe let GPT3 return the heads directly\nquery(at(I, where)) :- loc(I).\n% Find where last location of object is\nanswer(L) :- query(at(A, where)), holds_at(at(A,\nL), T), T>=Tx: holds_at(at(A, _), Tx).\nE.5.3 bAbI Tasks 3 and 14\n% the query before(O, L) is given , asking about\nthe location of O before moving to L\n% find all location changes of the queried\nobject\nlocation_change (L1 , L2 , T) :- before(O, _),\nholds_at(at(O, L1), T), holds_at(at(O, L2),\nT+1), L1 != L2.\n% find the last location change to queried\nlocation\nanswer(L1) :- before(_, L2), location_change (L1 ,\nL2 , T), T>=Tx: location_change (_, L2 , Tx).\nE.5.4 bAbI Task 4\nanswer(A) :- query(what , R1 , B), is(A, R1 , B).\nanswer(B) :- query(A, R1 , what), is(A, R1 , B).\nE.5.5 bAbI Task 5\ncandidate(A1 , T) :- query(action(who , give , A, I\n)), happens(action(A1 , give , A2 , I), T),\nA2=A: A!= anyone.\ncandidate(A2 , T) :- query(action(A, give , who , I\n)), happens(action(A1 , give , A2 , I), T),\nA1=A: A!= anyone.\ncandidate(I, T) :- query(action(A1 , give , A2 ,\nwhat)), happens(action(A1 , give , A2 , I), T).\nlocation(unknown).\n%%%% Interface -- these rules can be removed if\nwe let GPT -3 return the heads directly\ngive(A1 , A2 , I, T) :- gave(A1 , I, A2 , T).\nquery(action(A1 , give , A2 , what)) :-\nwhatWasGiven (A1 , A2).\nquery(action(anyone , give , who , I)) :- received(\nI).\nquery(action(A1 , give , who , I)) :- whoWasGiven(\nA1 , I).\nquery(action(who , give , anyone , I)) :- whoGave(I\n).\nquery(action(who , give , A2 , I)) :- whoGave(I, A2\n).\nanswer(A) :- candidate(A, T), Tx <=T: candidate(_\n, Tx).\nE.5.6 bAbI Tasks 6 and 9\nanswer(yes) :- query(at(A, L)), holds_at(at(A, L\n), T), Tx <=T: holds_at(at(A, _), Tx).\nanswer(no) :- not answer(yes).\n%%%% Interface -- these rules can be removed if\nwe let GPT -3 return the heads directly\nquery(at(A, L)) :- isIn(A, L).\nE.5.7 bAbI Task 7\n% find all items I that A is carrying at the\nlast moment; then count I\ncarry(A, I) :- query(carry(A, count)), holds_at(\ncarry(A,I),T),\nT>Tx: happens(E,Tx).\nlocation(unknown).\n%%%% Interface -- these rules can be removed if\nwe let GPT -3 return the heads directly\nquery(carry(A, count)) :- howMany(A).\nanswer(N) :- query(carry(A, count)), N=# count{I:\ncarry(A, I)}.\n5211\nE.5.8 bAbI Task 8\n%%%% Interface -- these rules can be removed if\nwe let GPT -3 return the heads directly\nquery(carry(A, what)) :- carrying(A).\nlocation(unknown).\n% find all items I that A is carrying at the\nlast moment\nanswer(I) :- query(carry(A, what)), holds_at(\ncarry(A,I),T),\nT>Tx: happens(E,Tx).\nE.5.9 bAbI Task 10\nreleased(F,T) :- fluent(F), timepoint(T).\nanswer(yes) :- query(at(A, L)), holds_at(at(A, L\n), T), Tx <=T: holds_at(at(A, _), Tx).\nanswer(maybe) :- query(at(A, L)), timepoint(T),\n1{ isEither(A, L, _, T); isEither(A, _, L, T)\n},\nTx <=T: holds_at(at(A, _), Tx);\nTx <=T: isEither(A, _, _, Tx).\nanswer(no) :- not answer(yes), not answer(maybe)\n.\n%%%% Interface -- these rules may be removed if\nwe let GPT -3 return the heads directly\nquery(at(A, L)) :- isInQ(A, L).\nholds_at(at(A, L), T) :- isIn(A, L, T).\ngo(A, L, T) :- move(A, L, T).\ntimepoint(T) :- isIn(_, _, T).\ntimepoint(T) :- isEither(_, _, _, T).\nE.5.10 bAbI Tasks 12 and 13\n%%%% Interface -- these rules can be removed if\nwe let GPT -3 return the heads directly\nquery(at(A, where)) :- whereAgent(A).\ngo(A1 , L, T) :- go(A1 , A2 , L, T).\ngo(A2 , L, T) :- go(A1 , A2 , L, T).\nE.5.11 bAbI Task 15\nquery(afraid(N, what)) :- agent_afraid (N).\nE.5.12 bAbI Task 16\nanimal(frog;lion;swan;rhino).\ncolor(green;white;yellow;gray).\nisColor(Agent2 ,Color):- isAnimal(Agent ,Animal),\nisColor(Agent ,Color),isAnimal(Agent2 ,Animal)\n.\nanswer(Color) :- isColor(Name), isColor(Name ,\nColor).\nE.5.13 bAbI Task 17\n% assume the 2nd queried object is at location\n(0 ,0)\nlocation(B, 0, 0) :- query(_, _, B).\n% the queried relation R is correct if its\noffset agrees with the location of A\nanswer(yes) :- query(A, R, B), offset(R, Dx , Dy)\n, location(A, X, Y),\nX>0: Dx=1; X<0: Dx=-1;\nY>0: Dy=1; Y<0: Dy=-1.\nanswer(no) :- not answer(yes).\n%%%% Interface -- these rules can be removed if\nwe let GPT -3 return the heads directly\nis(A, left , B) :- leftOf(A, B).\nis(A, right , B) :- rightOf(A, B).\nis(A, top , B) :- above(A, B).\nis(A, down , B) :- below(A, B).\nquery(A, left , B) :- leftOf_nondirect (A, B).\nquery(A, right , B) :- rightOf_nondirect (A, B).\nquery(A, top , B) :- above_nondirect (A, B).\nquery(A, down , B) :- below_nondirect (A, B).\nE.5.14 bAbI Task 18\nsmaller(A, B) :- bigger(B, A).\nsmaller(A, C) :- smaller(A, B), smaller(B, C).\nanswer(yes) :- query(smaller(A, B)), smaller(A,\nB).\nanswer(no) :- not answer(yes).\n%%%% Interface -- these rules can be removed if\nwe let GPT -3 return the heads directly\nquery(smaller(A, B)) :- doesFit(A, B).\nquery(smaller(A, B)) :- isBigger(B, A).\nE.5.15 bAbI Task 19\nagent(agent).\nmaxtime (10).\n% location\nlocation(L) :- is(L,_,_).\nlocation(L) :- is(_,_,L).\n% for each timestep , we take at most 1 action\n{happens(action(agent , goto , D), T): direction(D\n)}1 :- timepoint(T).\n% initial location\nholds_at(at(agent , L), 0) :- initial_loc(L).\n% goal\n:- goal(L), not holds_at(at(agent , L), _).\n% we aim to achieve the goal as early as\npossible\n:∼ goal(L), holds_at(at(agent , L), T). [-T@1 ,\ngoal]\nE.5.16 bAbI Task 20\nloc(kitchen). loc(bedroom). loc(kitchen). loc(\ngarden).\nobj(pajamas). obj(football). obj(milk). obj(\napple).\nanswer(Location) :- query(where , Agent , go), is(\nAgent , Quality), motivation(Quality ,Location\n), loc(Location).\n5212\nanswer(Quality) :- query(why , Agent , go ,\nLocation), is(Agent , Quality), motivation(\nQuality , Location), loc(Location).\nanswer(Quality) :- query(why ,Agent , get , Obj),is\n(Agent , Quality), motivation(Quality , Obj),\nobj(Obj).\nanswer(Location) :- query(where , Agent , go), is(\nAgent , Quality), motivation(Quality ,\nLocation), loc(Location).\nE.5.17 StepGame\n% assume the 2nd queried object is at location\n(0 ,0)\nlocation(Q2 , 0, 0) :- query(_, Q2).\n% extract answer relation R such that the offset\n(Ox ,Oy) of R is in the same direction of (X\n,Y)\nanswer(R) :- query(Q1 , _), location(Q1 , X, Y),\noffset(R, Ox , Oy),\nOx=-1: X<0; Ox=0: X=0; Ox=1: X>0;\nOy=-1: Y<0; Oy=0: Y=0; Oy=1: Y>0.\nE.5.18 gSCAN\n%********************\n* find the goal\n*********************%\n% features of objects\nfeature(O, shape , V) :- shape(O, V).\nfeature(O, color , V) :- color(O, V).\nfeature(O, size , V) :- size(O, V).\n% feature of destination\nfeature(destination , V) :- query(walk),\nqueryDesc(V).\nfeature(destination , V) :- query(push),\nqueryDesc(V).\nfeature(destination , V) :- query(pull),\nqueryDesc(V).\n% find the destination object and location\npos_same(destination , O) :- feature(O,_,_),\nfeature(O,_,V): feature(destination , V),\nfeature(_,_,V).\nsame(destination , O) :- pos_same(destination , O)\n, feature(O, size , V),\nVx <=V: feature(destination , big), pos_same(\ndestination , Ox), feature(Ox , size , Vx);\nVx >=V: feature(destination , small), pos_same\n(destination , Ox), feature(Ox , size , Vx)\n.\ngoal(at(agent ,L)) :- same(destination , O), pos(O\n,L).\n%********************\n* basic atoms\n*********************%\nagent(agent).\nitem(I) :- pos(I, L), I!= agent.\nlocation ((X,Y)) :- X=0..N-1, Y=0..N-1, gridSize(\nN).\nis((X1 ,Y1), east , (X2 ,Y2)) :- location ((X1 ,Y1)),\nlocation ((X2 ,Y2)), X1=X2 , Y1=Y2+1.\nis((X1 ,Y1), west , (X2 ,Y2)) :- location ((X1 ,Y1)),\nlocation ((X2 ,Y2)), X1=X2 , Y1=Y2 -1.\nis((X1 ,Y1), north , (X2 ,Y2)) :- location ((X1 ,Y1))\n, location ((X2 ,Y2)), X1=X2 -1, Y1=Y2.\nis((X1 ,Y1), south , (X2 ,Y2)) :- location ((X1 ,Y1))\n, location ((X2 ,Y2)), X1=X2+1, Y1=Y2.\npos_actions(walk; turn_left; turn_right; stay;\npush; pull).\nleft_dir(east , north; north , west; west , south;\nsouth , east).\n%********************\n* atoms in DEC_AXIOMS\n*********************%\n% fluent /1\nfluent(dir(A, L)) :- agent(A), direction(L).\nfluent(ready(A)) :- agent(A).\n% event /1\nevent(action(Agent , A)) :- agent(Agent),\npos_actions(A).\n% initial fluent values\nholds_at(at(O,L) ,0) :- pos(O, L).\nholds_at(dir(A,D) ,0) :- dir(A, D).\n% for each timestep , we take at most 1 action\n{happens(action(agent , A), T): pos_actions(A)}1\n:- timepoint(T).\n% initial location\nholds_at(at(agent , L), 0) :- initial_loc(L).\n%%%%%%%%%%%%%%%\n% action -- walk (to check simplification )\n%%%%%%%%%%%%%%%\n% initiates /3\ninitiates(action(A, walk), at(A, L2), T) :-\nagent(A), location(L), timepoint(T),\nholds_at(dir(A, D), T),\nholds_at(at(A, L1), T),\nis(L2 , D, L1).\n% terminates /3\nterminates(action(A, walk), at(A, L1), T) :-\nagent(A), location(L), timepoint(T),\nholds_at(dir(A, D), T),\nholds_at(at(A, L1), T),\nis(L2 , D, L1).\n% precondition\n% we don ’t walk in a deadend (i.e., the walk\nwill result in no location change)\n:- happens(action(agent , walk), T), not\ninitiates(action(agent , walk), _, T).\n%%%%%%%%%%%%%%%\n% action -- turn_left (to check simplification )\n%%%%%%%%%%%%%%%\n% initiates /3\ninitiates(action(A, turn_left), dir(A, D2), T)\n:- agent(A), timepoint(T),\nholds_at(dir(A, D1), T),\nleft_dir(D1 , D2).\n5213\n% terminates /3\nterminates(action(A, turn_left), dir(A, D1), T)\n:- agent(A), timepoint(T),\nholds_at(dir(A, D1), T).\n%%%%%%%%%%%%%%%\n% action -- turn_right (to check simplification )\n%%%%%%%%%%%%%%%\n% initiates /3\ninitiates(action(A, turn_right), dir(A, D2), T)\n:- agent(A), timepoint(T),\nholds_at(dir(A, D1), T),\nleft_dir(D2 , D1).\n% terminates /3\nterminates(action(A, turn_right), dir(A, D), T)\n:- agent(A), timepoint(T),\nholds_at(dir(A, D), T).\n%%%%%%%%%%%%%%%\n% action -- push/pull\n%%%%%%%%%%%%%%%\n% initiates /3 for objects with size <= 2\ninitiates(action(A, push), at(A, L2), T) :-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T),\nsame(destination , Target), holds_at(at(\nTarget , L1), T),\nis(L2 , D, L1), feature(Target , size , V), V\n<= 2.\ninitiates(action(A, push), at(Target , L2), T) :-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T),\nsame(destination , Target), holds_at(at(\nTarget , L1), T),\nis(L2 , D, L1), feature(Target , size , V), V\n<= 2.\ninitiates(action(A, pull), at(A, L2), T) :-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T),\nsame(destination , Target), holds_at(at(\nTarget , L1), T),\nis(L1 , D, L2), feature(Target , size , V), V\n<= 2.\ninitiates(action(A, pull), at(Target , L2), T) :-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T),\nsame(destination , Target), holds_at(at(\nTarget , L1), T),\nis(L1 , D, L2), feature(Target , size , V), V\n<= 2.\n% terminates /3 for objects with size <= 2\nterminates(action(A, push), at(A, L1), T) :-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T),\nsame(destination , Target), holds_at(at(\nTarget , L1), T),\nis(L2 , D, L1), feature(Target , size , V), V\n<= 2.\nterminates(action(A, push), at(Target , L1), T)\n:-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T),\nsame(destination , Target), holds_at(at(\nTarget , L1), T),\nis(L2 , D, L1), feature(Target , size , V), V\n<= 2.\nterminates(action(A, pull), at(A, L1), T) :-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T),\nsame(destination , Target), holds_at(at(\nTarget , L1), T),\nis(L1 , D, L2), feature(Target , size , V), V\n<= 2.\nterminates(action(A, pull), at(Target , L1), T)\n:-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T),\nsame(destination , Target), holds_at(at(\nTarget , L1), T),\nis(L1 , D, L2), feature(Target , size , V), V\n<= 2.\n% initiates /3 for objects with size >= 3\ninitiates(action(A, push), ready(A), T) :-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T), not holds_at(ready(A), T)\n,\nsame(destination , Target), holds_at(at(\nTarget , L1), T), feature(Target , size , V\n), V >= 3.\ninitiates(action(A, push), at(A, L2), T) :-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T), holds_at(ready(A), T),\nsame(destination , Target), holds_at(at(\nTarget , L1), T), feature(Target , size , V\n), V >= 3,\nis(L2 , D, L1).\ninitiates(action(A, push), at(Target , L2), T) :-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T), holds_at(ready(A), T),\nsame(destination , Target), holds_at(at(\nTarget , L1), T), feature(Target , size , V\n), V >= 3,\nis(L2 , D, L1).\ninitiates(action(A, pull), ready(A), T) :-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T), not holds_at(ready(A), T)\n,\nsame(destination , Target), holds_at(at(\nTarget , L1), T), feature(Target , size , V\n), V >= 3.\ninitiates(action(A, pull), at(A, L2), T) :-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T), holds_at(ready(A), T),\nsame(destination , Target), holds_at(at(\nTarget , L1), T), feature(Target , size , V\n), V >= 3,\nis(L1 , D, L2).\ninitiates(action(A, pull), at(Target , L2), T) :-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T), holds_at(ready(A), T),\n5214\nsame(destination , Target), holds_at(at(\nTarget , L1), T), feature(Target , size , V\n), V >= 3,\nis(L1 , D, L2).\n% terminates /3 for objects with size >= 3\n{ terminates(action(A, push), ready(A), T);\nterminates(action(A, push), at(A, L1), T);\nterminates(action(A, push), at(Target , L1), T)\n}=3 :-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T), holds_at(ready(A), T),\nsame(destination , Target), holds_at(at(\nTarget , L1), T), feature(Target , size , V\n), V >= 3,\nis(L2 , D, L1).\n{ terminates(action(A, pull), ready(A), T);\nterminates(action(A, pull), at(A, L1), T);\nterminates(action(A, pull), at(Target , L1), T)\n}=3 :-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T), holds_at(ready(A), T),\nsame(destination , Target), holds_at(at(\nTarget , L1), T), feature(Target , size , V\n), V >= 3,\nis(L1 , D, L2).\n% precondition\n% 1. we don ’t push/pull in a deadend (i.e., the\naction will result in no location change)\n:- happens(action(agent , push), T), not\ninitiates(action(agent , push), _, T).\n:- happens(action(agent , pull), T), not\ninitiates(action(agent , pull), _, T).\n% 2. the agent can push/pull only if it ’s\nqueried\n:- happens(action(agent , push), _), not query(\npush).\n:- happens(action(agent , pull), _), not query(\npull).\n% 2. it ’s not allowed to have 3 objects (agent +\n2 items) in the same cell\n% (I use holds_at(_, T) instead of timepoint(T)\nsince the latter doesn ’t cover the last T+1\ntimestamp)\n%:- holds_at(_, T), location(L), N = #count{O:\nholds_at(at(O, L), T)}, N>2.\n% 3. after push/pull , the agent cannot do a\ndifferent action in {walk , push , pull}\n:- happens(action(agent , A1), T1), happens(\naction(agent , A2), T2), A1!=A2 , T1 <T2 ,\n1{A1=push; A1=pull},\n1{A2=push; A2=pull; A2=walk }.\n% 4. the agent cannot change its direction to\npush/pull after reaching destination\nreach_destination (T) :- goal(at(agent ,L)),\nholds_at(at(agent , L), T),\nnot reach_destination (Tx): timepoint(Tx), Tx\n<T.\n:- reach_destination (T1), holds_at(dir(agent , D1\n), T1),\nholds_at(dir(agent , D2), T2), happens(action\n(agent , push), T2), T1 <T2 , D1!=D2.\n:- reach_destination (T1), holds_at(dir(agent , D1\n), T1),\nholds_at(dir(agent , D2), T2), happens(action\n(agent , pull), T2), T1 <T2 , D1!=D2.\n%%%%%%%%%%%%%%%\n% goal\n%%%%%%%%%%%%%%%\n% 1. (optional to speed up) we need to reach the\ndestination and as early as possible\n:- goal(at(agent ,L)), not reach_destination (_).\n:∼ goal(at(agent , L)), reach_destination (T). [\nT@10 , goal]\n% 2. we need to reach the goal and as early as\npossible\n% a. the direction when reaching goal must\nalign with the direction when reaching\ndestination\n% b. if it ’s not deadend , there must be\nsomething blocking the next push/pull\nreach_goal(T) :-\nagent(A), holds_at(at(A, L1), T), holds_at(\ndir(A, D), T),\nsame(destination , Target), holds_at(at(\nTarget , L1), T),\nreach_destination (Tr), holds_at(dir(A, D),\nTr),\nholds_at(at(_, L2), T): query(push), is(L2 ,\nD, L1);\nholds_at(at(_, L2), T): query(pull), is(L1 ,\nD, L2);\nnot reach_goal(Tx): timepoint(Tx), Tx <T.\n:- not reach_goal(_).\n:∼ reach_goal(T). [T@9 , goal]\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% additional requirements to achieve the goal\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n% the agent cannot move further before reaching\ndestination\n:- reach_destination (T), goal(at(agent , (Xg ,Yg))\n),\nholds_at(at(agent , (X1 ,Y1)), Tx), holds_at(\nat(agent , (X2 ,Y2)), Tx+1), Tx <T,\n|X1 -Xg| + |Y1 -Yg| < |X2 -Xg| + |Y2 -Yg|.\n% by default , walking all the way horizontally\nfirst and then vertically\nmove(horizontally , T) :- happens(action(agent ,\nwalk), T), holds_at(dir(agent , D), T), 1{D=\neast; D=west }.\nmove(vertically , T) :- happens(action(agent ,\nwalk), T), holds_at(dir(agent , D), T), 1{D=\nsouth; D=north }.\n:- not while(zigzagging), move(horizontally , T1)\n, move(vertically , T2), T1 >T2.\n% hesitantly: the agent must stay after every\naction in {walk , push , pull}\n:- while(hesitantly), happens(action(agent , A),\nT),\n1{A=walk; A=push; A=pull},\nnot happens(action(agent , stay), T+1).\n% cautiously\ncautious(T) :- happens(action(agent , turn_left),\nT),\n5215\nhappens(action(agent , turn_right), T+1),\nhappens(action(agent , turn_right), T+2),\nhappens(action(agent , turn_left), T+3).\n% the agent must be cautious before every action\nin {walk , push , pull}\n:- while(cautiously), happens(action(agent , A),\nT),\n1{A=walk; A=push; A=pull},\nnot cautious(T-4).\n% spinning\nspin(T) :- happens(action(agent , turn_left), T),\nhappens(action(agent , turn_left), T+1),\nhappens(action(agent , turn_left), T+2),\nhappens(action(agent , turn_left), T+3).\n% we always spin at the beginning if there is\nany action\n:- while(spinning), happens(_,_), not spin (0).\n% we always spin after every action in {walk ,\npush , pull} except for the last one\n:- while(spinning), happens(action(agent , A1),\nT1), happens(action(agent , A2), T2), T1 <T2 ,\n1{A1=walk; A1=push; A1=pull},\n1{A2=walk; A2=push; A2=pull},\nnot spin(T1+1).\n% zigzagging\n% if horizontal move is needed , the first move\nmust be horizontal\n:- while(zigzagging), move(horizontally , _),\nmove(D, Tmin), D!= horizontally ,\nTmin <=Tx: move(_,Tx).\n% if a different kind of move D2 is after D1 , D2\nmust be followed directly\n:- while(zigzagging), move(D1 , T1), move(D2 , T2)\n, D1!=D2 , T1 <T2 ,\nnot move(D2 , T1+2).\nE.5.19 Pick&Place\n%%%%%\n% Set up the environment\n%%%%%\n% Define the number of grippers for the robot\n#const grippers =1.\n% Define the maximum number of steps to consider\n{maxtime(M): M=0..10} = 1.\n:∼ maxtime(M). [M]\n%%%%%\n% Extract the features for all items in the\nintial and goal states\n% we assume these items form the complete set of\nitems in this example\n%%%%%\nfeature(I, F) :- on(I,_), F=@gen_feature (I).\nfeature(I, F) :- on(I,_,0), F=@gen_feature (I).\nfeature(I, F) :- on(_,I), I!=\" table\", F=\n@gen_feature (I).\nfeature(I, F) :- on(_,I,0), I!=\" table\", F=\n@gen_feature (I).\n% Define all locations\nlocation (\" table \").\nlocation(L) :- feature(L, block).\nlocation(L) :- feature(L, bowl).\n%********************\n* atoms in DEC_AXIOMS\n*********************%\n% happens /2\n{happens(E,T): event(E)}grippers :- timepoint(T)\n.\n% ****constraints****\n% the goal must be achieved in the end\n:- maxtime(M), on(A, B), not holds_at(on(A,B), M\n+1).\n% At any time T, for each block/bowl , there\ncannot be 2 items directly on it\n:- timepoint(T), feature(L, _), 2{ holds_at(on(I,\nL), T): feature(I,_)}.\n% if there are bowls on the table , a block can\nonly be on a block or a bowl;\n:- feature(_,bowl), feature(I,block), holds_at(\non(I,L),_), {feature(L, block); feature(L,\nbowl)} = 0.\n% there cannot be more than max_height -1 blocks\nstacked on a block\nup(A,B,T) :- holds_at(on(A, B), T).\nup(A,C,T) :- up(A,B,T), up(B,C,T).\n:- timepoint(T), feature(L, block), #count{I: up\n(I,L,T)} >= max_height.\nF Dataset Errors\nThis section enumerates the errors in the datasets\nwe found.\nF.1 bAbI\nIn task 5, the dataset has two errors with regard to\nthe labels.\nError #1. In the following example, the answer is\nambiguous since Bill gives Mary both the football\nand the apple.\nCONTEXT:\nMary journeyed to the kitchen.\nMary went to the bedroom.\nMary moved to the bathroom.\nMary grabbed the football there.\nMary moved to the garden.\nMary dropped the football.\nFred went back to the kitchen.\nJeff went back to the office.\nJeff went to the bathroom.\nBill took the apple there.\nMary picked up the milk there.\nMary picked up the football there.\nBill went back to the kitchen.\nBill went back to the hallway.\nFred journeyed to the office.\nBill discarded the apple.\nMary journeyed to the kitchen.\n5216\nFred journeyed to the garden.\nMary went to the hallway.\nMary gave the football to Bill.\nBill passed the football to Mary.\nBill took the apple there.\nBill gave the apple to Mary.\nJeff travelled to the kitchen.\nQUERY:\nWhat did Bill give to Mary?\nPREDICTION:\napple\nAnswer:\nfootball\nError #2. In the following example, the answer is\nambiguous since Fred gives Bill both the milk and\nthe apple.\nCONTEXT:\nMary journeyed to the bathroom.\nMary moved to the hallway.\nMary went to the kitchen.\nBill went back to the bedroom.\nBill grabbed the apple there.\nFred went back to the garden.\nMary went to the garden.\nFred took the milk there.\nJeff moved to the hallway.\nBill dropped the apple there.\nFred handed the milk to Mary.\nMary handed the milk to Fred.\nFred went back to the bedroom.\nFred passed the milk to Bill.\nFred took the apple there.\nFred gave the apple to Bill.\nJeff went to the kitchen.\nBill dropped the milk.\nQUERY:\nWhat did Fred give to Bill?\nPREDICTION:\napple\nAnswer:\nmilk\nF.2 CLUTRR\nWe detected 16 data errors in the CLUTRR 1.3\ndataset using our method. These errors can be\ngrouped into the following 4 categories.\n• 5 data instances are due to incorrect rela-\ntion graphs. For example, one relation graph\ncontains the main part “A-son-B-daughter-C-\naunt-D” and a noise (supporting) relation “B-\nspouse-D”. However, if B and D are couples,\nthen C should have mother D instead of aunt\nD.\n• 9 data instances have a correct relation graph\n(e.g., A-son-B-grandmother-C-brother-D with\na noise supporting relation B-mother-A) but\nthe noise relation is translated into a sen-\ntence with a wrong person name (e.g., \"D has\nmother A\" instead of \"B has mother A\").\n• 1 data instance has a correct relation graph\nand story, but has a wrong label (i.e., the label\nshould be mother_in_law instead of mother).\n• 1 data instance has a correct relation graph and\nstory, but the query cannot be answered due\nto the ambiguity of a sentence. It uses \"A has\ngrandsons B and C\" to represent brother(B,\nC), while B and C may have different parents.\n5217\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 7.\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSections 2, 3, 4.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSections 2, 3, 4.\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAt the beginning of the appendix.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 4 and Appendix A, B.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 4.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 4.\nC □\u0013 Did you run computational experiments?\nSection 4.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAt the beginning of the appendix.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n5218\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n5219",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8324002027511597
    },
    {
      "name": "Answer set programming",
      "score": 0.6093364357948303
    },
    {
      "name": "Natural language understanding",
      "score": 0.606117844581604
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6028224229812622
    },
    {
      "name": "Natural language",
      "score": 0.5647441744804382
    },
    {
      "name": "Parsing",
      "score": 0.534899115562439
    },
    {
      "name": "Natural language processing",
      "score": 0.5242753028869629
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.4887428283691406
    },
    {
      "name": "Question answering",
      "score": 0.4621458947658539
    },
    {
      "name": "Automated reasoning",
      "score": 0.45873796939849854
    },
    {
      "name": "Language model",
      "score": 0.44433966279029846
    },
    {
      "name": "Task (project management)",
      "score": 0.4393216669559479
    },
    {
      "name": "Formalism (music)",
      "score": 0.4311802089214325
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4167959690093994
    },
    {
      "name": "Programming language",
      "score": 0.414040207862854
    },
    {
      "name": "Logic programming",
      "score": 0.3973059356212616
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Musical",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I55732556",
      "name": "Arizona State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210101778",
      "name": "Samsung (United States)",
      "country": "US"
    }
  ],
  "cited_by": 24
}