{
    "title": "S2WAT: Image Style Transfer via Hierarchical Vision Transformer Using Strips Window Attention",
    "url": "https://openalex.org/W4393156269",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2259934708",
            "name": "Chiyu Zhang",
            "affiliations": [
                "Sichuan Normal University",
                "Nanjing University of Aeronautics and Astronautics"
            ]
        },
        {
            "id": "https://openalex.org/A2114042467",
            "name": "Xiaogang Xu",
            "affiliations": [
                "Zhejiang University",
                "Zhejiang Lab"
            ]
        },
        {
            "id": "https://openalex.org/A120759433",
            "name": "Lei Wang",
            "affiliations": [
                "Sichuan Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A4379862130",
            "name": "Zaiyan Dai",
            "affiliations": [
                "Sichuan Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2096416514",
            "name": "Jun Yang",
            "affiliations": [
                "Sichuan Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2259934708",
            "name": "Chiyu Zhang",
            "affiliations": [
                "Sichuan Normal University",
                "Nanjing University of Aeronautics and Astronautics"
            ]
        },
        {
            "id": "https://openalex.org/A2114042467",
            "name": "Xiaogang Xu",
            "affiliations": [
                "Zhejiang University",
                "Zhejiang Lab"
            ]
        },
        {
            "id": "https://openalex.org/A2096416514",
            "name": "Jun Yang",
            "affiliations": [
                "Sichuan Normal University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3149335081",
        "https://openalex.org/W2604737827",
        "https://openalex.org/W6693912633",
        "https://openalex.org/W3086415202",
        "https://openalex.org/W6840514303",
        "https://openalex.org/W3031118247",
        "https://openalex.org/W6650146654",
        "https://openalex.org/W6794053914",
        "https://openalex.org/W2161208721",
        "https://openalex.org/W6640174519",
        "https://openalex.org/W3146091044",
        "https://openalex.org/W4385014284",
        "https://openalex.org/W2603777577",
        "https://openalex.org/W2331128040",
        "https://openalex.org/W4316143309",
        "https://openalex.org/W6738438352",
        "https://openalex.org/W6639102338",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4319300559",
        "https://openalex.org/W2902904812",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W6791943378",
        "https://openalex.org/W3204777666",
        "https://openalex.org/W3170544306",
        "https://openalex.org/W6758142056",
        "https://openalex.org/W6846971355",
        "https://openalex.org/W4281253515",
        "https://openalex.org/W6847144867",
        "https://openalex.org/W2969803502",
        "https://openalex.org/W2617602381",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4214493665",
        "https://openalex.org/W4214588794",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W3211490618",
        "https://openalex.org/W3177457352",
        "https://openalex.org/W4385490110",
        "https://openalex.org/W4310998732",
        "https://openalex.org/W4287163461",
        "https://openalex.org/W2267186426",
        "https://openalex.org/W3213331968",
        "https://openalex.org/W4386071613",
        "https://openalex.org/W2964193438",
        "https://openalex.org/W2910836674",
        "https://openalex.org/W4225456588",
        "https://openalex.org/W4318348344",
        "https://openalex.org/W4287324101",
        "https://openalex.org/W4287203089",
        "https://openalex.org/W2125879936",
        "https://openalex.org/W2502312327",
        "https://openalex.org/W3020975476",
        "https://openalex.org/W3093370337"
    ],
    "abstract": "Transformer's recent integration into style transfer leverages its proficiency in establishing long-range dependencies, albeit at the expense of attenuated local modeling. This paper introduces Strips Window Attention Transformer (S2WAT), a novel hierarchical vision transformer designed for style transfer. S2WAT employs attention computation in diverse window shapes to capture both short- and long-range dependencies. The merged dependencies utilize the \"Attn Merge\" strategy, which adaptively determines spatial weights based on their relevance to the target. Extensive experiments on representative datasets show the proposed method's effectiveness compared to state-of-the-art (SOTA) transformer-based and other approaches. The code and pre-trained models are available at https://github.com/AlienZhang1996/S2WAT.",
    "full_text": "S2W AT: Image Style Transfer via Hierarchical Vision Transformer\nUsing Strips Window Attention\nChiyu Zhang1,2, Xiaogang Xu3,4,*, Lei Wang1, Zaiyan Dai1, Jun Yang1,5,*\n1Sichuan Normal University\n2Nanjing University of Aeronautics and Astronautics\n3Zhejiang Lab\n4Zhejiang University\n5Visual Computing and Virtual Reality Key Laboratory of Sichuan Provience\n{alienzhang19961005, xiaogangxu00, londmi9, zaiyan.dai}@gmail.com, jkxy yjun@sicnu.edu.cn\nAbstract\nTransformer’s recent integration into style transfer leverages\nits proficiency in establishing long-range dependencies, al-\nbeit at the expense of attenuated local modeling. This paper\nintroduces Strips Window Attention Transformer (S2W AT), a\nnovel hierarchical vision transformer designed for style trans-\nfer. S2W AT employs attention computation in diverse win-\ndow shapes to capture both short- and long-range dependen-\ncies. The merged dependencies utilize the “Attn Merge” strat-\negy, which adaptively determines spatial weights based on\ntheir relevance to the target. Extensive experiments on repre-\nsentative datasets show the proposed method’s effectiveness\ncompared to state-of-the-art (SOTA) transformer-based and\nother approaches. The code and pre-trained models are avail-\nable at https://github.com/AlienZhang1996/S2W AT.\nIntroduction\nBackground. Image style transfer imparts artistic character-\nistics from a style image to a content image, evolving from\ntraditional (Efros and Freeman 2001) to iterative (Gatys,\nEcker, and Bethge 2015, 2016) and feed-forward meth-\nods (Johnson, Alahi, and Fei-Fei 2016; Chen et al. 2017).\nHandling multiple styles concurrently remains a challenge,\naddressed by Universal Style Transfer (UST) (Park and Lee\n2019; Kong et al. 2023; Li et al. 2022). This sparks innova-\ntive approaches like attention mechanisms for feature styl-\nization (Yao et al. 2019; Deng et al. 2020; Chen et al. 2021),\nthe Flow-based method (An et al. 2021) for content leakage,\nand Stable Diffusion Models (SDM) for creative outcomes\n(Zhang et al. 2023). New neural architectures, notably the\ntransformer, show remarkable potential. (Deng et al. 2022)\nintroduces StyTr2, leveraging transformers for SOTA perfor-\nmance. However, StyTr2’s encoder risks losing information\ndue to one-time downsampling, impacting local details with\nglobal MSA (multi-head self-attention).\nChallenge. To enhance the transformer’s local modeling\ncapability, recent advancements propose the use of window-\nbased attention computation, exemplified by hierarchical\nstructures like Swin-Transformer (Liu et al. 2021). However,\napplying window-based transformers directly for feature ex-\n*Corresponding authors\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(a) Content (b) Style (c) Encoder - Swin (d) Encoder - S2WAT\nFigure 1: Illustration for locality problem. (c) Results of the\nSwin-based model. (d) Results from our S2W AT.\ntraction in style transfer can lead to grid-like patterns, as de-\npicted in Fig. 1 (c). This arises due to the localized nature of\nwindow attention, termed the locality problem. While win-\ndow shift can capture long-range dependencies (Liu et al.\n2021), it necessitates deep layer stacks, introducing substan-\ntial model complexity for style transfer, particularly with\nhigh-resolution samples.\nMotivation and Technical Novelty.Diverging from cur-\nrent transformer-based approaches, we introduce a novel\nhierarchical transformer framework for image style trans-\nfer, referred to as S2W AT (Strips Window Attention\nTransformer). This structure meticulously captures both lo-\ncal and global feature extraction, inheriting the efficiency\nof window-based attention. In detail, we introduce a dis-\ntinct attention mechanism (Strips Window Attention, SpW\nAttention) that amalgamates outputs from multiple win-\ndow attentions of varying shapes. These diverse window\nshapes enhance the equilibrium between modeling short-\nand long-range dependencies, and their integration is facili-\ntated through our devised “Attn Merge” technique.\nIn this paper, we formulate the SpW Attention in a simple\nwhile effective compound mode, which encompasses three\nwindow types: horizontal strip-like, vertical strip-like, and\nsquare windows. The attention computations derived from\nstrip windows emphasize long-range modeling for extract-\ning non-local features, while the square window attention fo-\ncuses on short-range modeling for capturing local features.\nFurthermore, the “Attn Merge” method combines atten-\ntion outputs from various windows by computing spatial\ncorrelations between them and the input. These calculated\ncorrelation scores serve as merging weights. In contrast to\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7024\nstatic merge strategies like summation and concatenation,\n“Attn Merge” dynamically determines the significance of\ndifferent window attentions, thus enhancing transfer effect.\nContributions. Extensive quantitative and qualitative ex-\nperiments are conducted to prove the effectiveness of the\nproposed framework, including a large-scale user study. The\nmain contributions of our work include:\n• We introduce a pioneering image style transfer frame-\nwork, S2W AT, founded on a hierarchical transformer.\nThis framework adeptly undertakes both short- and long-\nrange modeling concurrently, effectively mitigating the\nchallenge of locality issues.\n• We devise a novel attention computation within the trans-\nformer for style transfer, termed SpW Attention. This\nmechanism intelligently merges outputs from diverse\nwindow attentions using the “Attn Merge” approach.\n• We extensively evaluate our proposed S2W AT on well-\nestablished public datasets, demonstrating its state-of-\nthe-art performance for the style transfer task.\nRelated Work\nImage Style Transfer. Style transfer methods can fall\ninto single-style (Ulyanov, Vedaldi, and Lempitsky 2016),\nmultiple-style (Chen et al. 2017), and arbitrary-style\n(UST) (Zhang et al. 2022; Kong et al. 2023; Ma et al. 2023)\ncategories based on their generalization capabilities. Besides\nmodels based on CNNs, recent works include Flow-based\nArtFlow (An et al. 2021), transformer-based StyTr2 (Deng\net al. 2022), and SDM-based InST (Zhang et al. 2023).\nArtFlow, with Projection Flow Networks (PFN), achieves\ncontent-unbiased results, while IEST (Chen et al. 2021)\nand CAST (Zhang et al. 2022) use contrastive learning for\nappealing effects. InST achieves creativity through SDM.\nModels like (Wu et al. 2021b; Zhu et al. 2023; Hong et al.\n2023) use transformers to fuse image features, and (Liu et al.\n2022; Bai et al. 2023) encode text prompts for text-driven\nstyle transfer. StyTr2 leverages transformers as the backbone\nfor pleasing outcomes. Yet, hierarchical transformers remain\nunexplored in style transfer.\nHierarchical Vison Transformer. Lately, there has been a\nresurgence of interest in hierarchical architectures within the\nrealm of transformers. Examples include LeViT (Graham\net al. 2021) & CvT (Wu et al. 2021a), which employ global\nMSA; PVT (Wang et al. 2021) & MViT (Fan et al. 2021),\nwhich compress the resolution of K & V . However, in these\napproaches, local information is not adequately modeled.\nWhile Swin effectively captures local information through\nshifted windows, it still gives rise to the locality problem\nwhen applied to style transfer (see Fig. 1). Intuitive attempts,\nsuch as inserting global MSA (see Section Pre-Analysis) or\nintroducing Mix-FFN (Xie et al. 2021) by convolutions (see\nappendix), are powerless for locality problem. In the context\nof style transfer, a promising avenue involves advancing fur-\nther with a new transformer architecture that encompasses\nboth short- and long-range dependency awareness and pos-\nsesses the capability to mitigate the locality problem.\nDifferences with Other Methods While the attention\nmechanism in certain prior methods may share similarities\nContent Style 7-7-7 7-7-(7-224) 7-7-224 S2WAT\n(a) location of global MSA\n(b) window size\nContent Style 7-7-7 7-7-4 7-7-14 S2WAT\nFigure 2: Results of the Swin-based encoder experiments.\n7-7-7 means the Swin used has 3 stages (each stage with 2\nlayers) and 7 is the window size of each layer. 7-7-(7-224)\ndenotes the window size of the last layer in the last stage\nis 224 which represents global MSA. (a) Results of experi-\nments inserting global MSA in certain layers. (b) Results of\nexperiments changing the window size.\nwith the proposed SpW Attention, several key distinctions\nexist. 1) The fusion strategy stands out: our proposed “Attn\nMerge” demonstrates remarkable superiority in image style\ntransfer. 2) In our approach, all three window shapes shift\nbased on the computation point, and their sizes dynamically\nadapt to variations in input sizes. Detailed differentiations\nfrom previous methods, such as CSWin, Longformer, and\niLAT have been outlined in the Appendix.\nPre-Analysis\nOur preliminary analysis aims to unveil the underlying\ncauses of grid-like outcomes (locality problem) that arise\nwhen directly employing Swin for style transfer. Our hy-\npothesis points towards the localized nature of window at-\ntention as the primary factor. To validate this hypothesis,\nwe undertake experiments across four distinct facets as dis-\ncussed in this Section. The details of the models tested in\nthis part can be found in Appendix.\nGlobal MSA for Locality Problem\nThe locality problem should be relieved or excluded when\napplying global MSA instead of window or shifted window\nattention, if the locality of window attention is the culprit.\nIn the Swin-based encoder, we substitute the last one or two\nwindow attentions with global MSA, configuring the win-\ndow size for target layers at 224 (matching input resolution).\nFig. 2 (a) presents the experiment results, highlighting grid-\nlike textures at a window size of 7 (column 3) and block-like\nformations when the last window attention is swapped with\nglobal MSA (column 4). While replacing the last two win-\ndow attentions with global MSA effectively alleviates grid-\nlike textures, complete exclusion remains a challenge. This\nseries of experiments substantiates that the locality problem\nindeed stems from the characteristics of window attention.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7025\n(c) feature maps\nContent Style Feature Maps Stylized\nFigure 3: The last feature maps from Swin-based encoder.\n(d) attention maps\nContent Style\n Stylized\nh1 h2 h3 h4 h5 h6 h7 h8\nA.M.\nS.M.of p1 (L.T.)\nS.M.of p2 (R.T.)\nS.M.of p3 (L.D.)\nS.M.of p4 (R.D.)\nS.M.of p5 (C.)\nFigure 4: Attention maps (row 1) and similarity maps (rows\n2-6) of the five points. Attention and similarity maps differ in\nshape. They are scaled for easy observation. Andhi denotes\ni-th attention head. S/A.M. is short for “similarity/attention\nmaps” and L/R/T/D/C for “left/right/top/down/center”.\nInfluence of Window Size for Locality Problem\nThe window size in window attention, akin to the recep-\ntive field in CNN, delineates the computational scope. To\nexamine the impact of window size, assuming the locality\nof window attention causes the locality problem, we inves-\ntigate three scenarios: window sizes of 4, 7, and 14 for the\nlast stage. The outcomes of these experiments are depicted\nin Fig. 2 (b). Notably, relatively small blocks emerge with\na window size of 4 (column 4), while a shifted window’s\nrough outline materializes with a window size of 14 (col-\numn 5). This series of experiments underscores the pivotal\nrole of window size in the locality problem.\nLocality Phenomenon in Feature Maps\nIn previous parts, we discuss the changes in external factors,\nand we will give a shot at internal factors in the following\nparts. Since the basis of the transformer-based transfer mod-\nule is the similarity between content and style features, the\ncontent features should leave clues if the stylized images are\ngrid-like. For this reason, we check out all the feature maps\nfrom the last layer of the encoder and list some of them (see\nFig. 3), which are convincing evidence to prove that features\nfrom window attention have strongly localized.\nLocality Phenomenon in Attention Maps\nTo highlight the adverse impact of content feature local-\nity on stylization, we analyze attention maps from the first\ninter-attention (Cheng, Dong, and Lapata 2016) in the trans-\nfer module (see Fig. 4). Five points, representing corners\n(p1: top-left in red, p2: top-right in green, p3: bottom-left\nin blue, p4: bottom-right in black), and the central point (p5:\nwhite) are selected from style features to gauge their similar-\nity with content features. These points, extracted from spe-\ncific columns of attention maps and reshaped into squares,\nmirror content feature shapes. The similarity map of p1 re-\nveals pronounced responses aligned with red blocks in the\nstylized image. Conversely, p2, p3, and p5 exhibit robust re-\nsponses in areas devoid of red blocks. As for p4’s similarity\nmap, responses are distributed widely. These outcomes un-\nderline the propagation of window attention’s locality from\ncontent features within the encoder to the attention maps\nof the transfer module. This influence significantly disrupts\nthe stylization process, ultimately culminating in the locality\nproblem. To address this issue, we present the SpW Atten-\ntion and S2W AT solutions.\nMethod\nFig. 5 (c) presents the workflow of proposed S2W AT.\nStrips Window Attention\nAs illustrated in Fig. 5 (b), SpW Attention comprises two\ndistinct phases: a window attention phase and a fusion phase.\nWindow Attention. Assuming input features possess a\nshape of C × H × W and n denotes the strip width, the\nfirst phase involves three distinct window attentions: a hor-\nizontal strip-like window attention with a window size of\nn × W, a vertical strip-like window attention with a win-\ndow size of H × n, and a square window attention with a\nwindow size of M × M (where M = 2n). A single strip-\nlike window attention captures local information along one\naxis while accounting for long-range dependencies along\nthe other. In contrast, the square window attention focuses\non the surrounding information. Combining the outputs of\nthese window attentions results in outputs that consider both\nlocal information and long-range dependencies. Illustrated\nin Fig. 6, the merged approach gathers information from a\nbroader range of targets, striking a favorable balance be-\ntween computational complexity and the ability to sense\nglobal information.\nIn computing square window attention, we follow (Liu\net al. 2021) to include relative position bias B ∈ RM2×M2\nto each head in computing the attention map, as\nW-MSAM×M(Q, K, V) =Softmax( QKT\n√\nd\n+ B)V, (1)\nwhere Q, K, V∈ RM2×d are the query, key, and value ma-\ntrices; d is the dimension of query/key, M2 is the number\nof patches in the window, and W-MSAM×M denotes multi-\nhead self-attention using window in shape of M × M. We\nexclusively apply relative position bias to square window at-\ntention, as introducing it to strip-like window attention did\nnot yield discernible enhancements.\nAttn Merge. Following the completion of the window at-\ntention phase, a fusion module named “Attn Merge” is en-\ngaged to consolidate the outcomes with the inputs. Illus-\ntrated in Fig. 7, “Attn Merge” comprises three core steps:\nfirst, tensor stacking; second, similarity computation be-\ntween the first tensor and the rest at every spatial location;\nthird, weighted summation based on similarity. The compu-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7026\nPatch Embedding\nStage 1\nPadding\nSpW Attention\nBlock\nUn-padding\nx2\nPatch Merging\nStage 2\nPadding\nSpW Attention\nBlock\nUn-padding\nx2\nPatch Merging\nStage 3\nPadding\nSpW Attention\nBlock\nUn-padding\nx2\nImages\nPatch Partition\nWindow\nAttention\nn x W\nWindow\nAttention\nH x n\nWindow\nAttention\n2n x 2n\nAttn Merge\nLN\nSpW Attention Strips Window Attention\n(SpW Attention)\nLN\nMLP\nStrips Window Attention Block(a) Strips Window Attention Transformer Encoder\n(b) Strips Window Attention\nLayer 1\nLayer 2\nLayer 3\nSpW Transformer\nEncoder\nDecoder\nTransformer Decoder\nfc\nfs\nfcs\nIc\nIcs\nMirrored VGG\nPatch Reverse\nImages\n(c) Net Architecture of S2WAT (d) Decoder (e) Transformer Decoder Layer\nIs\n(SpW Transformer Encoder)\nLN\nMSA\nfc\nLN\nMHA\nLN\nMLP\nQK V\nfs\nFigure 5: Overall pipeline of the proposed S2W AT. Given a content image Ic and a style image Is, the encoder produces\ncorresponding features fc and fs. These features undergo style transfer from fs to fc within the transfer module, yielding\nstylized features fcs. Subsequently, stylized features are decoded in the decoder to generate the stylized image Ics.\nA patchA patch taken into attentionThe patch that is computing\nSquare Window Horizontal S.W. Vertical S.W.\n+ +\nMerge\nFigure 6: Receptive field of Strips Window Attention. Sin-\ngle strip-like window attention or square window attention\ncan only glean information from limited targets in the im-\nage, while the merged one enlarges the receptive region to\nmultiple directions. S.W. denotes “strip window”.\ntational efficiency of “Attn Merge” is noteworthy, as\nY = Stack(x, a, b, c), Y ∈ Rn×4×d,\nx′ = Unsqueeze(x), x ∈ Rn×1×d,\nZ = x′Y T Y, Z ∈ Rn×1×d,\nz = Squeeze(Z), z ∈ Rn×d,\n(2)\nwhere x, a, b, c∈ Rn×d are input tensors and z is the out-\nputs; Stack denotes the operation to collect tensors in a new\ndimension and Unsqueeze / Squeeze represents the opera-\ntion to add or subtract a dimension of tensor.\nStrips Window Attention Block. We now provide an\noverview of the comprehensive workflow of the SpW Atten-\ntion block. The structure of the SpW Attention block mirrors\nthat of a standard transformer block, except for the substitu-\ntion of MSA with a SpW Attention (SpW-MSA) module.\nAs depicted in Fig. 5 (b), a SpW Attention block comprises\na SpW-MSA module, succeeded by a two-layer MLP featur-\nStack Sequence\n...\nDot Product\na b c d\nSoftmax\na x b x+ c x+ b x +\nWeighted Sum\n...\n......\nInputs of SpW A.\nOutputs of W.A. (H x n)\nOutputs of W.A. (n x W)\nOutputs of W.A. (2n x 2n)\nOutput\nFigure 7: Workflow of “Attn Merge”. W./A. denotes “Win-\ndow/Attention”.\ning GLUE as the non-linear activation in between. Preceding\neach SpW-MSA module and MLP, a LayerNorm (LN) oper-\nation is applied, and a residual connection is integrated after\neach module. The computation process of a SpW Attention\nblock unfolds as follows:\nˆzl\nn×W = W-MSAn×W (LN(zl−1)),\nˆzl\nH×n = W-MSAH×n(LN(zl−1)),\nˆzl\n2n×2n = W-MSA2n×2n(LN(zl−1)),\n˜zl = A(LN(zl−1), ˆzl\nn×W , ˆzl\nH×n, ˆzl\n2n×2n) + zl−1,\nzl = MLP(LN(ˆzl)) + ˜zl,\n(3)\nwhere “A” means “Attn Merge”, zl , ˜zl and ˆzl denote the\noutputs of MLP, “Attn Merge”, and W-MSA for block l,\nrespectively; W-MSAn×m denotes multi-head self-attention\nusing window in shape of n × m. As shown in (3), the SpW\nAttention block primarily consists of two parts: SpW Atten-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7027\ntion (comprising W-MSA and “Attn Merge”) and an MLP.\nComputational Complexity. To make the cost of computa-\ntion in SpW Attention clear, we compare the computational\ncomplexity of MSA, W-MSA, and the proposed SpW-MSA.\nSupposing the window size of W-MSA and the strip width\nof SpW-MSA are equal to M and C is the dimension of\ninputs, the computational complexity of a global MSA, a\nsquare window based one, and a Strips Window based one\non an image of h × w patches are:\nΩ(MSA) = 2(wh)2C + 4whC2, (4)\nΩ(W - MSA) = 2M2whC + 4whC2, (5)\nΩ(SpW - MSA) = 2M(w2h + wh2 + 4Mwh)C+\n12whC2 + 8whC. (6)\nAs shown in Eqs. (4)-(6), MSA is quadratic to the patch\nnumber hw, and W-MSA is linear when M is fixed. And\nthe proposed SpW-MSA is something in the middle.\nOverall Architecture\nIn contrast to StyTr2 (Deng et al. 2022), which employs sep-\narate encoders for different input domains, we adhere to the\nconventional encoder-transfer-decoder design of UST. This\narchitecture encodes content and style images using a single\nencoder. An overview is depicted in Fig. 5.\nEncoder.Like Swin, S2W AT’s encoder initially divides con-\ntent and style images into non-overlapping patches using a\npatch partition module. These patches serve as “tokens” in\ntransformers. We configure the patch size as2 ×2, resulting\nin patch dimensions of2×2×3 = 12. Subsequently, a linear\nembedding layer transforms the patches into a user-defined\ndimension (C).\nAfter embedding, the patches proceed through a series of\nconsecutive SpW Attention blocks, nestled between padding\nand un-padding operations. Patches are padded to achieve\ndivisibility by twice the strip width and cropped (un-padded)\nafter SpW Attention blocks, preserving the patch count. No-\ntably, patch padding employs reflection to mitigate poten-\ntial light-edge artifacts that can arise when using constant\n0 padding. These SpW Attention blocks uphold the patch\ncount ( H\n2 × W\n2 ) and, in conjunction with the patch em-\nbedding layer and padding/un-padding operations, consti-\ntute “Stage 1”.\nTo achieve multi-scale features, gradual reduction of the\npatch count is necessary as the network deepens. Swin in-\ntroduces a patch merging layer as a down-sample module,\nextracting elements with a two-step interval along the hori-\nzontal and vertical axes. By concatenating 2 × 2 groups of\nthese features in the channel dimension and reducing chan-\nnels from 4C to 2C through linear projection, a 2x down-\nsampling result is obtained. Subsequent application of SpW\nAttention blocks, flanked by padding and un-padding oper-\nations, transforms the features while preserving a resolution\nof H\n4 × W\n4 . This combined process is designated as “Stage\n2”. This sequence is reiterated for “Stage 3”, yielding an out-\nput resolution of H\n8 × W\n8 . Consequently, the encoder’s hier-\narchical features in S2W AT can readily be employed with\ntechniques like FPN or U-Net.\nTransfer Module. A multi-layer transformer decoder re-\nplaces the transfer module, similar to StyTr2 (Deng et al.\n2022). In our implementation, we maintain a close resem-\nblance to the original transformer decoder (Vaswani et al.\n2017), with two key distinctions from StyTr2: a) The initial\nattention module of each transformer decoder layer is MSA,\nwhereas StyTr2 employs MHA (multi-head attention); b)\nLayerNorm precedes the attention module and MLP, rather\nthan following them. The structure is presented in Fig. 5 (e)\nand more details can be found in codes.\nDecoder. In line with prior research (Huang and Belongie\n2017; Park and Lee 2019; Deng et al. 2021), we utilize a\nmirrored VGG for decoding stylized features. Detailed im-\nplements are available in codes.\nNetwork Optimization\nSimilar to (Huang and Belongie 2017), we formulate two\ndistinct perceptual losses for gauging the content dissimilar-\nity between stylized imagesIcs and content imagesIc, along\nwith the style dissimilarity between stylized images Ics and\nstyle images Is. The content perceptual loss is defined as:\nLcontent =\nX\nl∈C\n∥\nϕl(Ics) − ϕl(Ic)∥2, (7)\nwhere the overline denotes mean-variance channel-wise nor-\nmalization; ϕl(·) represents extracting features of layer l\nfrom a pre-trained VGG19 model; C is a set consisting of\nrelu4\n1 and relu5 1 in the VGG19. The style perceptual\nloss is defined as:\nLstyle =\nX\nl∈S\n∥µ(ϕl(Ics)) − µ(ϕl(Is))∥2\n+ ∥σ(ϕl(Ics)) − σ(ϕl(Is))∥2,\n(8)\nwhere µ(·) and σ(·) denote mean and variance of features,\nrespectively; and S is a set consisting of relu2 1, relu3 1,\nrelu4 1 and relu5 1 in the VGG19.\nWe also adopt identity losses (Park and Lee 2019) to fur-\nther maintain the structure of the content image and the style\ncharacteristics of the style image. The two different identity\nlosses are defined as:\nLid1 = ∥Icc − Ic∥2 + ∥Iss − Is∥2, (9)\nLid2 = P\nl∈N ∥ϕl(Icc) − ϕl(Ic)∥2 + ∥ϕl(Iss) − ϕl(Is)∥2, (10)\nwhere Icc (or Iss) denotes\nthe output image stylized from\ntwo same content (or style) images andN is a set consisting\nof relu2 1, relu3 1, relu4 1 and relu5 1 in the VGG19. Fi-\nnally, our network is trained by minimizing the loss function\ndefined as:\nLtotal = λcLcontent + λsLstyl\ne + λid1Lid1 + λid2Lid2 , (11)\nwhere λc, λs, λid1, and λid2 are\nthe weights of different\nlosses. We set the weights to 2, 3, 50, and 1, relieving the\nimpact of magnitude differences.\nExperiments\nMS-COCO (Lin et al. 2014) and WikiArt (Phillips and\nMackintosh 2011) are used as the content dataset and the\nstyle dataset respectively. Other implementation details are\navailable in Appendix and codes.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7028\nContent/Style (a) Ours\n(a) Visual Comparison\n(b) Content Leak\n(b) InST (c) StyTr2 (d) CAST (e) IEST (f) A.F.+AdaIN (g) MCC (h) SANET (i) WCT (j) AdaIN\nContentStyle\nOurs StyTr2 A.F.+AdaIN MCC SANet AdaIN\nRound 1Round 20\nCASTInST IEST\nFigure 8: Visual comparison of the results from SOTA methods and visualization of content leak. A.F. denotes “ArtFlow”.\nStyle Transfer Results\nIn this Section, we compare the results between the proposed\nS2W AT and previous SOTAs, including AdaIN (Huang and\nBelongie 2017), WCT (Li et al. 2017), SANet (Park and Lee\n2019), MCC (Deng et al. 2021), ArtFlow (An et al. 2021),\nIEST (Chen et al. 2021), CAST (Zhang et al. 2022), StyTr2\n(Deng et al. 2022) and InST (Zhang et al. 2023).\nQualitative Comparison. In Fig. 8 (a), we present vi-\nsual outcomes of the compared algorithms. AdaIN, relying\non mean and variance alignment, fails to capture intricate\nstyle patterns. While WCT achieves multi-level stylization,\nit compromises content details. SANet, leveraging attention\nmechanisms, enhances style capture but may sacrifice con-\ntent details. MCC, lacking non-linear operations, faces over-\nflow issues. Flow-based ArtFlow produces content-unbiased\noutcomes but may exhibit undesired patterns at borders.\nCAST retains content structure through contrastive meth-\nods but may compromise style. InST’s diffusion models\nyield creative results but occasionally sacrifice consistency.\nStyTr2 and proposed S2W AT strike a superior balance, with\nS2W AT excelling in preserving content details (e.g., num-\nbers on the train, the woman’s glossy lips, and letters on\nbillboards), as highlighted in dashed boxes in Fig. 8 (a). Ad-\nditional results are available in the Appendix.\nQuantitative Comparison. In this section, we follow a\nmethodology akin to (Huang and Belongie 2017; An et al.\n2021; Deng et al. 2022) utilizing losses as indirect metrics.\nStyle, content, and identity losses serve as metrics, evalu-\nating style quality, content quality, and input information\nretention, respectively. Additionally, inspired by (An et al.\n2021), the Structural Similarity Index (SSIM) is included to\ngauge structure preservation. As shown in Table 1, S2W AT\nachieves the lowest content and identity losses, while SANet\nexhibits the lowest style loss. StyTr2 and S2W AT show com-\nparable loss performance, emphasizing style and content, re-\nspectively. Due to its content-unbiased nature, ArtFlow reg-\nisters identity losses of 0, signaling an unbiased approach.\nWhile ArtFlow is unbiased, S2W AT outperforms it in style\nand SSIM. S2W AT attains the highest SSIM, indicating su-\nperior content structure retention. It excels in preserving\nboth content input structures and artistic style characteris-\ntics simultaneously.\nContent Leak\nContent leak problem occur when applying the same style\nimage to a content image repeatedly, especially if the model\nstruggles to preserve content details impeccably. Following\n(An et al. 2021; Deng et al. 2022), We investigate content\nleakage in the stylization process, focusing on S2W AT and\ncomparing it to ArtFlow, StyTr2, CNN-based, and SDM-\nbased methods. Our experiments, detailed in Fig. 8 (b), re-\nveal S2W AT and StyTr2, both transformer-based, exhibit\nminimal content detail loss over 20 iterations, surpassing\nCNN and SDM methods known for noticeable blurriness.\nWhile CAST alleviates content leak partially, the stylized\neffect remains suboptimal. In summary, S2W AT effectively\nmitigates the content leak issue.\nInST occasionally underperforms, especially when con-\ntent and style input styles differ significantly, potentially due\nto overfitting in the Textual Inversion module during single-\nimage training. More details are available in the Appendix.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7029\nMethod Ours\nInST StyTr2 CAST IEST ArtFlow-AdaIN ArtFlow-WCT MCC SANet WCT AdaIN\nContent Loss↓ 1.661.661.66\n3.73 1.83 2.07 1.81 1.93 1.73 1.92 2.16 2.56 1.71\nStyle Loss↓ 1.74\n29.98 1.52 4.33 2.72\n1.90 1.89 1.70 1.11 1.111.11 2.23 3.50\nIdentity Loss 1↓ 0.160.160.16 0.71 0.26 1.94 0.91\n0.00 0.00 1.07 0.81 3.01 2.54\nIdentity Loss 2↓ 1.381.381.38 134.23 3.10 18.72 7.16\n0.00 0.00 7.72 6.03 21.88 17.97\nSSIM ↑ 0.6510.6510.651 0.401 0.605 0.619 0.551 0.578 0.612\n0.578 0.448 0.364 0.539\nTable 1: Quantitative evaluation results of different style transfer methods. The losses above are average values from 400 random\nsamples, while SSIMs are computed average from 100 pieces. For a fair comparison, we take relu1 1 into consideration in\ncomputing style loss and identity loss 2 while not in the training of S2W AT. The optimal results are highlighted in bold, the\nsecond-best results are underlined, and instances with a value of 0 are derived from unbiased methods.\nContent Style Attn Merge Sum Concat\nFigure 9: Visual comparisons when utilizing different fusion\nstrategies for attention outputs from multiple windows.\nAblation Study\nAttn Merge. In order to showcase the effectiveness and su-\nperiority of “Attn Merge”, we undertake experiments where\n“Attn Merge” is replaced by fusion strategies such as the\nconcatenation operation (as employed by CSWin) or the sum\noperation. The outcomes are depicted in Fig. 9. Stylized im-\nages generated using the sum operation are extensively cor-\nrupted, indicating a failure in model optimization. On the\nother hand, outputs obtained through concatenation relin-\nquish a substantial portion of information from input im-\nages, particularly the style images. An intuitive rationale for\nthis phenomenon lies in the optimization challenges posed\nby straightforward fusion operations. Comprehensive expla-\nnations are available in the Appendix. The proposed “Attn\nMerge”, however, facilitates smooth information transmis-\nsion, allowing the model to undergo normal training.\nStrips Window. To verify the demand to fuse outputs from\nwindow attention of various sizes, we carry out experiments\nemploying window attention with distinct window sizes in-\ndependently. As illustrated in Fig. 10, the utilization of hor-\nizontal or vertical strip-like windows in isolation yields cor-\nresponding patterns. Applying square windows alone results\nin grid-like patterns. However, the incorporation of “Attn\nMerge” to fuse outcomes leads to pleasing stylized images,\nsurpassing the results obtained solely from window atten-\ntion. Further details regarding the ablation study for Swin\nand Swin with Mix-FFN can be found in the Appendix.\nUser Study\nIn comparing virtual stylization effects between S2W AT\nand the aforementioned SOTAs like StyTr2, ArtFlow, MCC,\nand SANet, user studies were conducted. Using a widely-\nemployed online questionnaire platform, we created a\nContent Style Attn Merge Horizontal Vertical Square\nFigure 10: Visual comparisons for the ablation study when\nemploying different window attention mechanisms.\nMethod Ours\nStyTr2 ArtFlow MCC SANet\nPer\ncent(%) 25.425.425.4 23.6 13.3 19.4 18.3\nTable 2: Percentage of votes in the user study.\ndataset comprising 528 stylized images from 24 content im-\nages and 22 style images. Participants, briefed on image\nstyle transfer and provided with evaluation criteria, assessed\n31 randomly selected content and style combinations. Cri-\nteria emphasized preserving content details and embody-\ning artistic attributes. With 3002 valid votes from 72 par-\nticipants representing diverse backgrounds, including high\nschool students and professionals in computer science, art,\nand photography, our method achieved a marginal victory in\nthe user study, as reflected in Table 2. Additional details in-\ncluding an example questionnaire page can be found in the\nAppendix.\nConclusion\nIn this paper, we introduce S2W AT, a pioneering image style\ntransfer framework founded upon a hierarchical vision trans-\nformer architecture. S2W AT’s prowess lies in its capacity to\nsimultaneously capture local and global information through\nSpW Attention. The SpW Attention mechanism, featuring\ndiverse window attention shapes, ensures an optimal equi-\nlibrium between short- and long-range dependency model-\ning, further enhanced by our proprietary “Attn Merge”. This\nadaptive merging technique efficiently gauges the signifi-\ncance of various window attentions based on target similar-\nity. Furthermore, S2W AT mitigates the content leak predica-\nment, yielding stylized images endowed with vibrant style\nattributes and intricate content intricacies.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7030\nAcknowledgements\nThis work was supported by the National Key R&D Program\nof China (2021YFB3100700), the National Natural Science\nFoundation of China (62076125, U20B2049, U22B2029,\n62272228), and Shenzhen Science and Technology Program\n(JCYJ20210324134408023, JCYJ20210324134810028).\nReferences\nAn, J.; Huang, S.; Song, Y .; Dou, D.; Liu, W.; and Luo, J.\n2021. Artflow: Unbiased image style transfer via reversible\nneural flows. In CVPR.\nBai, Y .; Liu, J.; Dong, C.; and Yuan, C. 2023. ITstyler:\nImage-optimized Text-based Style Transfer. arXiv.\nChen, D.; Yuan, L.; Liao, J.; Yu, N.; and Hua, G. 2017.\nStylebank: An explicit representation for neural image style\ntransfer. In CVPR.\nChen, H.; Wang, Z.; Zhang, H.; Zuo, Z.; Li, A.; Xing, W.;\nLu, D.; et al. 2021. Artistic style transfer with internal-\nexternal learning and contrastive learning. In NeurIPS.\nCheng, J.; Dong, L.; and Lapata, M. 2016. Long short-term\nmemory-networks for machine reading. In EMNLP.\nDeng, Y .; Tang, F.; Dong, W.; Huang, H.; Ma, C.; and Xu,\nC. 2021. Arbitrary video style transfer via multi-channel\ncorrelation. In AAAI.\nDeng, Y .; Tang, F.; Dong, W.; Ma, C.; Pan, X.; Wang, L.;\nand Xu, C. 2022. StyTr2: Image Style Transfer with Trans-\nformers. In CVPR.\nDeng, Y .; Tang, F.; Dong, W.; Sun, W.; Huang, F.; and Xu, C.\n2020. Arbitrary style transfer via multi-adaptation network.\nIn ACM MM.\nEfros, A. A.; and Freeman, W. T. 2001. Image quilting for\ntexture synthesis and transfer. In Proceedings of the 28th\nAnnual Conference on Computer Graphics and Interactive\nTechniques.\nFan, H.; Xiong, B.; Mangalam, K.; Li, Y .; Yan, Z.; Malik, J.;\nand Feichtenhofer, C. 2021. Multiscale vision transformers.\nIn ICCV.\nGatys, L.; Ecker, A. S.; and Bethge, M. 2015. Texture syn-\nthesis using convolutional neural networks. In NeurIPS.\nGatys, L. A.; Ecker, A. S.; and Bethge, M. 2016. A neural\nalgorithm of artistic style. In Vision Sciences Society.\nGraham, B.; El-Nouby, A.; Touvron, H.; Stock, P.; Joulin,\nA.; J ´egou, H.; and Douze, M. 2021. Levit: a vision trans-\nformer in convnet’s clothing for faster inference. In ICCV.\nHong, K.; Jeon, S.; Lee, J.; Ahn, N.; Kim, K.; Lee, P.;\nKim, D.; Uh, Y .; and Byun, H. 2023. AesPA-Net: Aesthetic\nPattern-Aware Style Transfer Networks. In ICCV.\nHuang, X.; and Belongie, S. 2017. Arbitrary style transfer\nin real-time with adaptive instance normalization. In ICCV.\nJohnson, J.; Alahi, A.; and Fei-Fei, L. 2016. Perceptual\nlosses for real-time style transfer and super-resolution. In\nECCV.\nKong, X.; Deng, Y .; Tang, F.; Dong, W.; Ma, C.; Chen, Y .;\nHe, Z.; and Xu, C. 2023. Exploring the Temporal Consis-\ntency of Arbitrary Style Transfer: A Channelwise Perspec-\ntive. IEEE Transactions on Neural Networks and Learning\nSystems.\nLi, G.; Cheng, B.; Cheng, L.; Xu, C.; Sun, X.; Ren, P.; Yang,\nY .; and Chen, Q. 2022. Arbitrary Style Transfer with Seman-\ntic Content Enhancement. In The 18th ACM SIGGRAPH\nInternational Conference on Virtual-Reality Continuum and\nits Applications in Industry.\nLi, Y .; Fang, C.; Yang, J.; Wang, Z.; Lu, X.; and Yang, M.-\nH. 2017. Universal style transfer via feature transforms. In\nNeurIPS.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In ECCV.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In ICCV.\nLiu, Z.-S.; Wang, L.-W.; Siu, W.-C.; and Kalogeiton, V .\n2022. Name your style: An arbitrary artist-aware image style\ntransfer. arXiv.\nMa, Y .; Zhao, C.; Li, X.; and Basu, A. 2023. RAST:\nRestorable Arbitrary Style Transfer via Multi-Restoration.\nIn WACV.\nPark, D. Y .; and Lee, K. H. 2019. Arbitrary style transfer\nwith style-attentional networks. In CVPR.\nPhillips, F.; and Mackintosh, B. 2011. Wiki Art Gallery, Inc.:\nA case for critical thinking. Issues in Accounting Education.\nUlyanov, D.; Vedaldi, A.; and Lempitsky, V . 2016. Instance\nnormalization: The missing ingredient for fast stylization.\narXiv.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NeurIPS.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. In ICCV.\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.;\nand Zhang, L. 2021a. Cvt: Introducing convolutions to vi-\nsion transformers. In ICCV.\nWu, X.; Hu, Z.; Sheng, L.; and Xu, D. 2021b. Styleformer:\nReal-time arbitrary style transfer via parametric style com-\nposition. In ICCV.\nXie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez, J. M.;\nand Luo, P. 2021. SegFormer: Simple and efficient design\nfor semantic segmentation with transformers. Advances in\nNeural Information Processing Systems, 34: 12077–12090.\nYao, Y .; Ren, J.; Xie, X.; Liu, W.; Liu, Y .-J.; and Wang, J.\n2019. Attention-aware multi-stroke style transfer. In CVPR.\nZhang, Y .; Huang, N.; Tang, F.; Huang, H.; Ma, C.; Dong,\nW.; and Xu, C. 2023. Inversion-based style transfer with\ndiffusion models. In CVPR, 10146–10156.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7031\nZhang, Y .; Tang, F.; Dong, W.; Huang, H.; Ma, C.; Lee, T.-\nY .; and Xu, C. 2022. Domain Enhanced Arbitrary Image\nStyle Transfer via Contrastive Learning. In SIGGRAPH.\nZhu, M.; He, X.; Wang, N.; Wang, X.; and Gao, X. 2023.\nAll-to-key Attention for Arbitrary Style Transfer. In ICCV.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7032"
}