{
    "title": "Rethinking Skip Connection with Layer Normalization in Transformers and ResNets",
    "url": "https://openalex.org/W3162800503",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2269575529",
            "name": "Liu Feng-lin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221880362",
            "name": "Ren, Xuancheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1700595713",
            "name": "Zhang Zhi-yuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2106641529",
            "name": "Sun Xu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2115955093",
            "name": "Zou, Yuexian",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3010768098",
        "https://openalex.org/W2962931466",
        "https://openalex.org/W2989571009",
        "https://openalex.org/W3037932933",
        "https://openalex.org/W2302255633",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3103334733",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W3204406378",
        "https://openalex.org/W3037973456",
        "https://openalex.org/W2905927205",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2963418779",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2970157301",
        "https://openalex.org/W2912521296",
        "https://openalex.org/W2952355681",
        "https://openalex.org/W3034772996",
        "https://openalex.org/W3038058348",
        "https://openalex.org/W3024133716",
        "https://openalex.org/W3118608800",
        "https://openalex.org/W2962761235",
        "https://openalex.org/W2274287116",
        "https://openalex.org/W2194775991"
    ],
    "abstract": "Skip connection, is a widely-used technique to improve the performance and the convergence of deep neural networks, which is believed to relieve the difficulty in optimization due to non-linearity by propagating a linear component through the neural network layers. However, from another point of view, it can also be seen as a modulating mechanism between the input and the output, with the input scaled by a pre-defined value one. In this work, we investigate how the scale factors in the effectiveness of the skip connection and reveal that a trivial adjustment of the scale will lead to spurious gradient exploding or vanishing in line with the deepness of the models, which could be addressed by normalization, in particular, layer normalization, which induces consistent improvements over the plain skip connection. Inspired by the findings, we further propose to adaptively adjust the scale of the input by recursively applying skip connection with layer normalization, which promotes the performance substantially and generalizes well across diverse tasks including both machine translation and image classification datasets.",
    "full_text": "Rethinking Skip Connection with Layer Normalization\nin Transformers and ResNets\nFenglin Liu1∗, Xuancheng Ren2∗, Zhiyuan Zhang2, Xu Sun2, Yuexian Zou1\n1ADSPLAB, School of ECE, Peking University, China\n2MOE Key Laboratory of Computational Linguistics, School of EECS, Peking University\n{fenglinliu98, renxc, zzy1210, xusun, zouyx}@pku.edu.cn\nAbstract\nSkip connection, is a widely-used technique to improve the performance and the convergence of\ndeep neural networks, which is believed to relieve the difﬁculty in optimization due to non-linearity\nby propagating a linear component through the neural network layers. However, from another\npoint of view, it can also be seen as a modulating mechanism between the input and the output,\nwith the input scaled by a pre-deﬁned value one. In this work, we investigate how the scale\nfactors in the effectiveness of the skip connection and reveal that a trivial adjustment of the scale\nwill lead to spurious gradient exploding or vanishing in line with the deepness of the models,\nwhich could be addressed by normalization, in particular, layer normalization, which induces\nconsistent improvements over the plain skip connection. Inspired by the ﬁndings, we further\npropose to adaptively adjust the scale of the input by recursively applying skip connection with\nlayer normalization, which promotes the performance substantially and generalizes well across\ndiverse tasks including both machine translation and image classiﬁcation datasets.\n1 Introduction\nThere is a surge of research interest in solving the optimization problem of deep neural networks,\nwhich presents a considerable number of novel difﬁculties that are in need of practical solutions. Batch\nNormalization (Ioffe and Szegedy, 2015), Layer Normalization (Ba et al., 2016) and Skip Connection (He\net al., 2016a) are widely-used techniques to facilitate the optimization of deep neural networks, which\nprove to be effective in multiple contexts (Szegedy et al., 2016; Vaswani et al., 2017).\nAmong them, Batch Normalization (BN) and Layer Normalization (LN) make progress by normalizing\nand rescaling the intermediate hidden representations to deal with the so-called internal co-variate shift in\nmini-batch training and collective optimization of multiple correlated features. Skip Connection bypasses\nthe gradient exploding or vanishing problem and tries to solve the model optimization problem from the\nperspective of information transfer. It enables the delivery and integration of information by adding an\nidentity mapping from the input of the neural network to the output, which may ease the optimization and\nallow the error signal to pass through the non-linearities.\nRecently, several efforts have been made to deal with the optimization problem by further exploiting\nthe existing solutions, such as exploring the implementation of skip connection (Srivastava et al., 2015)\nand combining the skip connection with other normalization methods (He et al., 2016b; Vaswani et al.,\n2017). Some of the proposals are illustrated in Figure 1. Most of these variations could be formulated as\nor coarsely equivalent to\ny= G(λx+ F(x,W)), (1)\nwhere xdenotes the input of the block, i.e., the skip connection or the shortcut, Fdenotes the non-linear\ntransformation induced by the neural network parameterized by W, Gdenotes the normalization function,\n∗Equal contribution.\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/.\narXiv:2105.07205v1  [cs.LG]  15 May 2021\nFigure 1: Illustrations of commonly-used skip connections: (a) Conventional Residual Unit (He et al.,\n2016a); (b) Residual Unit in Highway Net (Srivastava et al., 2015); (c) Residual Unit in PreAct-ResNet\n(He et al., 2016b), where BN represents batch normalization (Ioffe and Szegedy, 2015); (d) Residual Unit\nin Transformer (Vaswani et al., 2017), where LN represents layer normalization (Ba et al., 2016).\nydenotes the output of the residual block, and λdenotes the modulating factor that controls the relative\nimportance of the skip connection or the shortcut.1 A residual model consists of a stack of such blocks.\nA missing piece from the existing work is how would the residual block perform if Gis realized as\nlayer normalization and λdoes not equal one. Although Transformer (Vaswani et al., 2017) has shown\nthe effectiveness of the combination of layer normalization and skip connection, it is intuitive that the\nmodulating factor λmay not always be one, especially when Fis not well trained and not sufﬁcient in\nrepresentation ability. In addition, the intuition is supported by Srivastava et al. (2015) who found that if\nusing a gate mechanism to determine the balance between the two components, the skip component wins\nin almost all situations. However, in the study of He et al. (2016b)2, which discussed the combination of\nbatch normalization, learned λ, and convolutional neural networks, they found empirically that the best\nperformance is achieved when λis ﬁxed as one, suggesting the shortcut and the transformed input should\nbe of equal importance.\nTherefore, in order to explore whether the optimal ratio of xto F(x,W) is one in the residual block,\nand to extend the understandings of the residual block formed as y= G(λx+ F(x,W)), we propose\nthree different implementations of the residual block in combination with layer normalization. The ﬁrst\nimplementation, which we name Expanded Skip Connection, simply expands the skip connection and\nsets λlarger than one, following the ﬁndings of Srivastava et al. (2015). The second variation, named as\nExpanded Skip Connection with Layer Normalization, includes the layer normalization after the expanded\nskip connection, since layer normalization is observed to be helpful in facilitating the optimization of\nskip connection as in Vaswani et al. (2017). The ﬁnal proposal, Recursive Skip Connection with Layer\nNormalization, is a novel combination that does not ﬁt in the general form of the residual block, which\ntakes the advantages of skip connection and layer normalization in a recursive manner, so that the input\nsignal is ampliﬁed yet with layer normalization stabilizing the optimization. The illustrations of those\ncombinations are shown in Figure 2. Our experiments on diverse tasks, including machine translation and\nimage classiﬁcation with strong baselines, i.e., Transformer and ResNet, conclude the following:\n• The expanded skip connection without any normalization indeed causes serve gradient malformation,\nleading to unsatisfactory learning of the neural networks. Layer normalization helps to deal with the\noptimization problem introduced by the expanded skip connection to a certain extent.\n• The proposed recursive skip connection with layer normalization further facilities the optimization\nby separating the expanded skip connection into multiple stages to better incorporate the effect of the\ntransformed input. It achieves comfortable margin on datasets from different domains in extensive\nexperiments to determine the proper granularity of the skip connection.\n1The terms skip connection and shortcut are used interchangeably in this work.\n2They move the batch normalization applied to F(x,W) forward and apply it to x instead. From another point of view, it is\nsimilar to applying the batch normalization to y of the previous block.\nFigure 2: Various combination of skip connection and layer normalization: (a) Original skip connection,\n(b) Expanded skip connection, (c) Expanded skip connection with layer normalization, and (d) Recursive\nskip connection with layer normalization (λ= 2as an example), where LN represents layer normalization.\n• The experimental results on the WMT-2014 EN-DE machine translation dataset using Transformer\nfurther proves the effectiveness and the efﬁciency of the recursive architecture, which helps a model\nto perform even better than the model three times larger.\n2 Related Work\nIn recent years, a large number of skip connection methods have been used for neural networks (He et\nal., 2016a; Srivastava et al., 2015; He et al., 2016b; Vaswani et al., 2017; Szegedy et al., 2017; Wang et\nal., 2019; Nguyen and Salazar, 2019; Liu et al., 2020a; Xu et al., 2019; Bachlechner et al., 2020; Liu et\nal., 2020b; Shen et al., 2020; Xiong et al., 2020). For the skip connection, there are two main types of\nproblems. The ﬁrst is how should the skip connection be incorporated into the existing neural network\nstructures so that the best improvements can be achieved. The second is how should the neural network\nwith skip connections be optimized so that its representation capability could be fully mined. Therefore,\nwe categorized the related work into On the Connection Problem and On the Optimization Problem.\nOn the connection problem Highway network (Srivastava et al., 2015) built a highway connection\nfrom the input to the output, similar to the skip connection. However, a transform gate was proposed to\ncontrol the balance of the input xand the transformed input F(x,W), instead of using identity mapping\nto combine the input and the transformed input. He et al. (2016b) designed ﬁve types of skip connections\nand discussed the possible skip connections in detail. Based on their theory and experiments, they argued\nthat it is likely for a model to perform the best when the skip connection is directly combined into the\noutput without any additional scaling such that the skip and the transformed input have equal contribution\nto the output. The reason is that with scaling, the gradient of the skip suffers from the gradient exploding\nor vanishing problem, which hinders the deep neural network from efﬁcient optimization.\nOn the optimization problem Since the early work has established that it is sufﬁcient to directly com-\nbine skip connection into forward propagation of neural networks without any scaling (He et al., 2016b),\nthe succeeding studies on the optimization problem mostly followed the simple yet effective architecture\nof the skip connection. In computer vision task, PreAct-ResNet (He et al., 2016b) demonstrated that it\nis helpful to move the batch normalization applied to F(x,W) forward and apply it to xinstead. From\nanother point of view, it is similar to applying the batch normalization to yof the previous block. In\nnatural language processing tasks, Transformer (Vaswani et al., 2017) made use of skip connection and\nlayer normalization extensively in its architecture, and from our experiments, the layer normalization\nseems essential to the overall performance of the model and can help the optimization of the non-linear\ntransformation to some extent.\nIn all, we propose to use the form y = G(λx+ F(x,W)) to summarize the combination of nor-\nmalization and skip connection in existing work. The form is in accordance with the ﬁndings that the\nnormalization Gshould be placed after the addition of the skip and it could be beneﬁcial to elevate the\nportion of the input information with λ. Moreover, this work distinguishes itself in that (a) we mainly\nfocus on the combination of layer normalization and skip connection, which is less investigated but\ndeems promising, (b) we rethink the common practice that the skip need not scale in consideration of\nthe help of layer normalization, (c) we conduct experiments on both representative computer vision and\nnatural language processing tasks, while existing work only provides result in a single modality, and most\nimportantly (d) we escape from the general form of the residual block that generalizes all previous work\nand propose a novel recursive construction of the residual block with layer normalization that outperforms\nall the variants of the general form that are examined in this work.\n3 Architecture\nIn this section, we describe three skip connection constructions, which are illustrated in Figure 2. They\nare based on the integration of more input information and application of the layer normalization.\nExpanded Skip Connection (xSkip) The expanded skip connection is deﬁned similar to the constant\nscaling method in He et al. (2016b) as\ny= λx+ F(x,W), (2)\nwhere xand yare the input and the output of the residual block, respectively. Fdenotes the weighted\nneural network layer, and λis a modulating scalar. Naturally, this construction adjusts the importance\nof the skip considering that the neural network layer may come with different representation ability and\noptimization difﬁculty. However, it should be noted that in this work λis ﬁxed to isolate the effect of\nscaling. While it is possible that a learned λmay better capture the balance between the two components,\nthe learning of λbecomes another variable. Following Srivastava et al. (2015), we mainly experiment\nwith λ> 1.\nExpanded Skip Connection with Layer Normalization (xSkip+LN) Motivated by Transformer,\nwhich combines skip connection with layer normalization, we further examine the effect of layer normal-\nization on expanded skip connection, which takes the form of\ny= LN(λx+ F(x,W)). (3)\nLayer normalization may help mitigate the gradient malformation in optimization caused by the modulating\nfactor. We use the parameterized layer normalization as suggested in Ba et al. (2016), because different\ninput features naturally have different importance. Different from batch normalization which acts in\n“sample space”, layer normalization acts in “feature space”. Considering the special case that F(x,W) is\nill-learned and outputs random valuesϵand the examples X = {xi}in the mini-batch all contain the same\ninput feature f of the same value v, batch normalization simply ignores the input feature f which results\nin uneventful optimization according to mini-batch statistics, while layer normalization still recognizes\nthe input feature f and its value can be properly adjusted according to the other input features. The same\nmay also apply to the situation where λxis dominant compared to F(x,W) and the transformed input\nfalls into the range comparable to random noise. In such situations, which may happen when the neural\nnetwork is hard to optimize, layer normalization could still help the learning of the shortcut, while batch\nnormalization might fail.\nRecursive Skip Connection with Layer Normalization (rSkip+LN) Another way to stabilize the\ngradient is to keep λ= 1each time but repeatedly add the shortcut with layer normalization, such that\nmore input information is also modeled. It is deﬁned recursively as\nyλ = LN(x+ yλ−1) and y1 = LN(x+ F(x,W)), (4)\nwhere λshould be an integer no smaller than one. For example, when λ= 1, it regresses to the block\nused in Transformer and conforms to the ﬁndings that the skip need not scale. A further example is\nwhen λ= 2, y2 = LN(x+ y1) =LN(x+ LN(x+ F(x,W))), which is shown in Figure 2. Through\nrecursive skip connection with layer normalization, the model is encouraged to use layer normalization\nmultiple times to improve optimization, and can incorporate more information ofxby the skip connection.\nMoreover, the model may gain more expressive power than simply incorporating the scaled skip at once,\nsince each recursive step essentially constructs a different feature distribution and the recursive structure\ncan learn the adaptive ratio of xto F(x,W). The ratio between the input xand the residual F(x,W) is\nx/F(x,W) = σ1/w1 + 1, where σ1 is the standard deviation of x+ F(x,W) and w1 is the gain parameters\nof the inner layer normalization, which penetrate through the normalization and effectively act as the\nlearnable ratio. It is also important to note that the ratio is also dependent on the current layer distribution,\nwhich could serve as a regularization component. For any λlarger than 2, the general form of the ratio\nis 1 +∑λ\ni=1\n∏i\nj=1 σj/wj . This derivation, which is detailed in Appendix A, also indicates that setting\nλ= 2should be sufﬁcient for learning the shortcut-residual ratio and adjusting the ratio based on instant\ndata distribution. Yet with larger λ, the distribution of the higher order combination of xand F(x) will\nbe considered. The recursive deﬁnition introduces gradual and smooth combination of the input and the\nresidual up to the speciﬁed order λwhich could deal with different distributions better.\n4 Empirical Study\nIn this section, we examine the effect of the discussed combinations on two representative baselines\nthrough empirical studies. First, we conduct exploratory experiments on datasets that are relatively small\nto gain insights of the combinations, such as the choice of λand the effectiveness of the constructions.\nThen, we discuss the observed results and conduct further analysis. Finally, we validate our ﬁndings on\nlarger and more difﬁcult datasets.\n4.1 Tasks and Settings\nFor ease of introduction, brief descriptions of the tasks and corresponding models are summarized together\nin the following. As the combinations to be examined only relates to the skip connection and we keep the\ninner structure of the residual block untouched, please refer to the cited work for more detailed description\nof the baselines.\n4.1.1 Image Classiﬁcation\nDatasets CIFAR-10 and CIFAR-100 (Krizhevsky, 2009) are two datasets for colored image classiﬁcation.\nBoth contain 50K images for training and 10K images for testing. The former has 10 classes, while\nthe latter has 100 classes and is considered more difﬁcult. We report the classiﬁcation error rates as the\nevaluation of the performance.\nBaselines and settings We conduct experiments using the ResNet with pre-activation (PreAct-ResNet)\nas proposed in He et al. (2016b). For experiments on CIFAR-10, we use ResNet-110 for exploration\nbecause of its deep structures. It contains 54 two-layer residual blocks, which makes it a challenging\nbaseline for optimization. We validate the proposal on CIFAR-100, using a variety of ResNet structures,\nsuch as ResNet-20, ResNet-32, ResNet-44, ResNet-56, and ResNet-110, to ensure the generality of the\nresults. We mostly adopt the hyperparameters from He et al. (2016b) to keep the number of variables\ncontrolled and comparable, except that we use a weight decay rate of 0.0002.\n4.1.2 Machine Translation\nDatasets IWSLT 2015 English-Vietnamese dataset (EN-VI) (Cettolo et al., 2015) consisting of 133K\nsentence pairs, WMT 2014 English-German dataset (EN-DE) consisting of 4.5M sentence pairs and\nWMT 2014 English-French dataset (EN-FR) consisting of 36M sentence pairs are for machine translation.\nFollowing previous work (Luong and Manning, 2015), we use tst2012 and tst2013 as development\nset and test set, respectively for EN-VI dataset. For EN-DE, we use newstest2013 for development and\nnewstest2014 for testing. For EN-FR, we validate on newstest2012+2013 and test on newstest2014. The\nevaluation metric is BLEU (Papineni et al., 2002) against the reference translations.\nBaselines and settings Transformer (Vaswani et al., 2017) is a representative model for machine\ntranslation, which includes skip connection and layer normalization in the model structure. Transformer\nfollows the encoder-decoder framework, except that it substitutes the conventional recurrent layers with\nMethod Architecture λ G Error (%)\nBaseline x + F(x,W) 1 - 6.37\n1xSkip x + F(x,W) 1 - 6.31\n2xSkip 2x + F(x,W) 2 - 8.41\n3xSkip 3x + F(x,W) 3 - 9.23\n1xSkip+LN LN (x + F(x,W)) 1 LN 7.72\n2xSkip+LN LN (2x + F(x,W)) 2 LN 6.29\n3xSkip+LN LN (3x + F(x,W)) 3 LN 6.07\n1rSkip+LN LN (x + F(x,W)) 1 LN 7.72\n2rSkip+LN LN (x + LN(x + F(x,W))) 2 LN 6.02\n3rSkip+LN LN (x + LN(x + LN(x + F(x,W)))) 3 LN 6.55\nTable 1: Results on CIFAR-10 using PreAct-ResNet-110. Average of 5 runs. The lower the better. The\nunderlined numbers denote the best results under the same comparable settings. λxSkip, λxSkip+LN, and\nλrSkip+LN stand for expanded skip connection, expanded skip connection with LN, and recursive skip\nconnection with LN, respectively. Please note that 1xSkip is equivalent to Baseline and 1rSkip+LN is\nequivalent to 1xSkip+LN. It is clear that simply expanding the skip connection hurts the performance and\nusing layer normalization can mitigate the problem.\nthe self-attention based blocks, i.e., F(x,W) consists of a dot-product based attention layer and two\nfeed-forward layers. Although Transformer is not as deep as ResNet-110, it is heavily parameterized and\nthe commonly-used six-layer Transformer-Base structure embodies 65M learnable parameters, which\nalso poses challenge for optimization. We base our experiments on the tensor2tensor (Vaswani et al.,\n2018) code. For experiments on IWSLT-2015, we set batch size to 4096 and train on single GPU, as it is\nrelatively small. For experiments on WMT-2014 that is much larger, due to the complication caused by\nthe special batching algorithm and the distributed learning, we ﬁnd setting batch size to 16,384 tokens in\ntotal on 3 GPUs can reproduce the reported results on 8 GPUs using batch size of 32,768 tokens in total\n(Vaswani et al., 2017). However, the training steps required are increased from 100,000 steps to 550,000\nsteps. The rest of hyperparameters are the same as Vaswani et al. (2017).\n4.2 Exploratory Results\nIn this section, we evaluate the effect of the proposed constructions and report the exploratory results on\nCIFAR-10 using ResNet and IWSLT-2015 using Transformer.\nThe main results on CIFAR-10 using PreAct-ResNet-110 are summarized in Table 1. We experiment\nwith different constructions and different λs. 1xSkip is our reimplementation of the ResNet-110 with\npre-activation and outperforms the reported result in He et al. (2016b) by 0.06. This margin could\napproximately serve as the boundary deciding whether the improvement is caused by random variations\nor experimental settings. As we can see, for skip connection without normalization, it is indeed the best to\nadd the input without scaling and expanding the skip results in signiﬁcant loss in accuracy. However, with\nthe application of layer normalization, the expanded skip can also be sufﬁciently optimized and surpass\nthe baseline, which is suggested by He et al. (2016b) by extensive structure search. Moreover, with the\nrecursive design, the performance can also be maintained and yet further improved, which reaches the\nbest result in our experiments when using λ= 2. It is also interesting to see that in combinations with\nlayer normalization, the λthat results in the best accuracy is also different. It is intuitive that the recursive\nstructure adjusts the effective ratio of the skip and the transformed input more sophisticatedly than the\nplain expanding structure so that different λmay be required.\nTable 2 shows the results on IWSLT-2015 using Transformer and the results about layer normalization\nare similar. With the increasing of the weight of the skip and layer normalization, the performance of\nthe model improves. Since the models incorporated expanded skip connection with layer normalization\nhave the same number of parameters, regardless of λ, thus embodying the same representation ability,\nthe improved performance should mainly be attributed to the enhanced shortcut information. Moreover,\nthe recursive version brings signiﬁcant improvements upon the simply expanding version, pushing the\nMethod Architecture λ G BLEU\nBaseline LN (x + F(x,W)) 1 - 30.31\n1xSkip x + F(x,W) 1 - 28.98\n1xSkip+LN LN (x + F(x,W)) 1 LN 30.31\n2xSkip+LN LN (2x + F(x,W)) 2 LN 30.99\n3xSkip+LN LN (3x + F(x,W)) 3 LN 30.92\n1rSkip+LN LN (x + F(x,W)) 1 LN 30.31\n2rSkip+LN LN (x + LN(x + F(x,W))) 2 LN 31.45\n3rSkip+LN LN (x + LN(x + LN(x + F(x,W)))) 3 LN 31.10\nTable 2: Results on EN-VI machine translation dataset using Transformer. The higher the better. λxSkip,\nλxSkip+LN, and λrSkip+LN are deﬁned similarly. Please note that Baseline, 1xSkip+LN, and 1rSkip+LN\nare equivalent. It is clear that without layer normalization, Transformer experiences signiﬁcantly perfor-\nmance degradation. By expanding the shortcut, substantially better results can be achieved. The best\nimprovements of over 1.1 BLEU scores are brought by the proposed recursive skip connection.\nBLEU score past 31 and reaching a margin of 1.1 BLEU scores at most, demonstrating the advantage\nof gradually adding the expanded skip to ease the optimization. A notable difference from the results\non PreAct-ResNet-110 is that the plain skip connection version does not perform better than the one\nwith layer normalization (30.31 BLEU vs. 28.98 BLEU on Transformer and 92.28 accuracy vs. 93.69\naccuracy on PreAct-ResNet-110). We ﬁnd that the reason is likely to be that in the residual block of\nPreAct-ResNet-110, there are already two instances of batch normalization before each convolutional\nlayer to facilitate its optimization, while in the Transformer block no other normalization technique exists.\nIn summary, our experiments show that feeding more input signals to the layer output should be\nhelpful to learning of the overall model. Previous concerns on the optimization difﬁculty introduced by\nthe expanded skip connection could be alleviated by the usage of layer normalization. Especially, the\nproposed recursive skip connection with layer normalization brings the most improvements by dividing\nthe expanded skip and iteratively adding the pieces to the output, which can further stabilize the gradient\non the input and help the training of the weighted layer.\n4.3 Discussions\nHowever, questions remain whether the improvements can also be achieved by contracting skip connection\nand whether batch normalization can achieve the same effect which is more commonly in convolutional\nneural networks. We conduct the following analysis to answer these questions. Furthermore, we demon-\nstrate that layer normalization adaptively adjusts the gradients of the scaled skip, solving the gradient\nmalformation caused by the modulating factor λ.\nContracting skip connection We experiment two different ways to relatively tune down the skip ratio,\ni.e., using smaller λand expanding F(x,W). The results are shown in Table 3. As we can see, if the\nshortcut is forced to play the minor role, the deep neural network fails to be optimized properly and shows\ncatastrophic performance. In contrast, the expanded skip causes less harm to the overall performance (cf.\nTable 1), indicating the importance of linear component in the optimization of deep neural networks. It is\nconsistent with the ﬁndings of Srivastava et al. (2015) and He et al. (2016b). Moreover, the results also\nshow that applying layer normalization under this situation further deteriorates the performance.\nUsing batch normalization Since batch normalization has been widely used in CNNs, we replace layer\nnormalization with batch normalization to see the difference. However, comparing Table 1 and Table 4, it\ncan be seen that batch normalization is less effective than layer normalization in combination with skip\nconnection. The proposed recursive structure can help batch normalization deal with the expanded skip as\neach time the ratio is still kept as one. As we explained in Section 3, batch normalization makes use of\nmini-batch statistics which may prevent the plainly expanded skip from coming into effect.\n1xSkip\n2xSkip\n2xSkip+BN\n0.5xSkip\n1xSkip+LN\n2xSkip+LN\n2rSkip+LN\n0.5xSkip+LN\nFigure 3: Gradient norm of the output of residual blocks of various constructions on CIFAR-10 using\nPreAct-ResNet-110. We show the gradient norm of the output yof the residual blocks of middle depth in\nthe model averaged over 10,000 random examples. By deﬁnition, yequals xof the following block.\nArchitecture Error (%)\nx + F(x,W) 6.31\n0.5x + F(x,W) 18.61\nLN(0.5x + F(x,W)) 29.88\nLN(x + 2∗F(x,W)) 24.98\nLN(x + 3∗F(x,W)) 61.02\nTable 3: Results on CIFAR-10\nwith contracted skip connection.\nLowering the skip ratio leads to\ncatastrophic results and LN fur-\nther aggravates the performance.\nResNet-110 λ G Error (%)\n1xSkip 1 - 6.31\n1xSkip+BN 1 BN 8.23\n2xSkip+BN 2 BN 6.43\n3xSkip+BN 3 BN 6.50\n1rSkip+BN 1 BN 8.23\n2rSkip+BN 2 BN 6.77\n3rSkip+BN 3 BN 6.68\nTable 4: Results on CIFAR-10\nwith BN. Compared to LN, BN\nis less effective dealing with the\nexpanded skip connection.\nTransformer # Blocks BLEU\n1xSkip+LN 6 30.31\n2rSkip+LN 6 31.45\n1xSkip+LN 2 30.04\n2rSkip+LN 2 30.49\nTable 5: Results on Transformer\nof fewer blocks. As we can\nsee, with the proposed combina-\ntion of recursive skip connection,\nthe same performance could be\nachieved by fewer parameters.\nParameter efﬁciency A byproduct of the proposed structure is that it could be possible to use fewer\nparameters to reach the same performance, especially for Transformer, which is heavily parameterized,\nas the expanded skip connection and layer normalization both helps to ease the optimization. Fewer\nparameter may translate into lower time cost. We conduct experiments on the IWSLT-2015 EN-VI\nmachine translation dataset using Transformer with two blocks instead of six blocks used in the main\nexperiments. The results are shown in Table 5. Surprisingly, a Transformer of two blocks does not\nperform substantially worse, and with the proposed rSkip+LN, the reduction could be easily made up. A\ninteresting phenomenon is that the proposed method gains larger margin on deeper models, which also\nattests that proposed combination helps the optimization of the model, so that the performance of deeper\nmodel could be further exploited.\nBetter Optimization To see how the proposed combinations help the optimization of the deep neural\nmodels, in Figure 3, we show the averaged gradient norm of the output of each residual block in pretrained\nResNet-110 on CIFAR-10 based on 10,000 random training examples. Due to limited space, we only show\nthe representative blocks at the “middle” of the model, meaning those blocks have the same ﬁlter size but\ndifferent from the blocks lower and the blocks higher. As we can see, the original construction 1xSkip\nshows slight gradient “explosion”, while simply scaling the skip causes severe gradient malformation (e.g.,\n2xSkip and 0.5xSkip). With normalization techniques such as layer normalization and batch normalization,\nthe gradient malformation is generally mitigated. However, the magnitude of the gradient is different,\nwith layer normalization more accurately reﬂecting the modulating factor λ(e.g. 1xSkip and 1xSkip+LN\nhave similar values, 2xSkip+LN is approximately twice 1xSkip, etc.), which is intuitive since batch\nnormalization is based on mini-batch statistics unaware of the feature space. The recursive structure\n/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000036/uni00000057/uni00000048/uni00000053\n/uni00000013/uni00000011/uni00000013/uni00000013\n/uni00000013/uni00000011/uni00000015/uni00000018\n/uni00000013/uni00000011/uni00000018/uni00000013\n/uni00000013/uni00000011/uni0000001a/uni00000018\n/uni00000014/uni00000011/uni00000013/uni00000013\n/uni00000014/uni00000011/uni00000015/uni00000018\n/uni00000014/uni00000011/uni00000018/uni00000013\n/uni00000014/uni00000011/uni0000001a/uni00000018\n/uni00000015/uni00000011/uni00000013/uni00000013/uni0000002f/uni00000052/uni00000056/uni00000056\n/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni0000004f/uni00000052/uni00000056/uni00000056/uni00000003/uni0000000b/uni00000015/uni00000055/uni00000036/uni0000004e/uni0000004c/uni00000053/uni0000000c\n/uni00000059/uni00000044/uni0000004f/uni0000004c/uni00000047/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000004f/uni00000052/uni00000056/uni00000056/uni00000003/uni0000000b/uni00000015/uni00000055/uni00000036/uni0000004e/uni0000004c/uni00000053/uni0000000c\n/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni0000004f/uni00000052/uni00000056/uni00000056/uni00000003/uni0000000bw/uni00000036/uni0000004e/uni0000004c/uni00000053/uni0000000c\n/uni00000059/uni00000044/uni0000004f/uni0000004c/uni00000047/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000004f/uni00000052/uni00000056/uni00000056/uni00000003/uni0000000bw/uni00000036/uni0000004e/uni0000004c/uni00000053/uni0000000c\n/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013\n/uni00000036/uni00000057/uni00000048/uni00000053\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000048/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000000b/uni00000015/uni00000055/uni00000036/uni0000004e/uni0000004c/uni00000053/uni0000000c\n/uni00000059/uni00000044/uni0000004f/uni0000004c/uni00000047/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000048/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000000b/uni00000015/uni00000055/uni00000036/uni0000004e/uni0000004c/uni00000053/uni0000000c\n/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000048/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000000bw/uni00000036/uni0000004e/uni0000004c/uni00000053/uni0000000c\n/uni00000059/uni00000044/uni0000004f/uni0000004c/uni00000047/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000048/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni0000000bw/uni00000036/uni0000004e/uni0000004c/uni00000053/uni0000000c\nFigure 4: Learning curve for 2rSkip+LN and wSkip initialized as 1 on CIFAR-10 using ResNet-110. It\ncan be observed that wSkip has difﬁculty in start training.\n2rSkip+LN is similar to 2xSkip+LN, except that its norm is larger. Especially, note the gradient at the\nBlock 36, which receives gradient from the higher block that have different ﬁlter size; xSkip+LN and\nxSkip+BN seem to fail normalize its gradient properly, while rSkip+LN successfully elevates its scale,\nsuggesting it could mitigate the difference between blocks of different nature as well. The last but not the\nleast, layer normalization does not adjust the effective scale of the skip connection in forward propagation\nsuch that the effective scale becomes 1 and regresses to the plain residual structure. The learned effective\nscale is 2.82 and 2.64 for 2xSkip+LN and 2rSkip+LN, respectively, averaged over the shown blocks.\nLearning λ To automatically adjust the ratio between the skip and the residual branch, a trivial extension\nis to replace the ﬁxed scalar λwith a learnable scalar or vector. We also try this method on CIFAR-10\nusing ResNet-110. Concretely, we adopt the form of y= LN(w·x+ F(x)) (wSkip+LN), as using\na parameter vector should be more general and a vector covers all the possible solutions of a scalar.\nHowever, the ﬁndings are not in favor of this approach over the ﬁxed scalar version. When the wis\ninitialized as 1, the model experiences signiﬁcantly performance degradation. The averaged error rate\nover 5 runs is 8.66 and the ﬁnal norm of a sampled w∈R32 at middle layers is 1.34, suggesting very\nsmall scale for each dimension of the skip connection (0.24 on average). The small scale of the skip\nconnection indicates that the model relies more on the residual branch that is parameterized and optimized\nfor the training data, entailing a higher overﬁtting risk. It can be conﬁrmed from Figure 4, where we show\nthe learning curve of wSkip compared to 2rSkip (+LN is omitted for conciseness). wSkip and 2rSkip\nboth achieve low training loss, but wSkip has difﬁculty in starting learning and has higher validation loss.\nWe also try different initialization strategy forwSkip, e.g., initializing was 2 since we ﬁnd 2 is a good\nempirical value for rSkip+LN. We observe a performance of 6.45 in terms of error rate, which is indeed\nbetter but still worse than rSkip+LN. Moreover, such strategy would make wSkip no better than using\npre-determined ﬁxed scalar values, as a proper initialization value still needs to be searched.\n4.4 Validation Results\nWe verify the proposed combination of recursive skip connection with layer normalization on more\ndifﬁcult datasets, i.e., CIFAR-100, WMT-2014 EN-DE and EN-FR datasets. Considering the training\nbudget, we only validate the performance of 2rSkip+LN based on the exploratory results.\nThe results are shown in Table 6 and Table 7, respectively, which demonstrate the effectiveness of\nthe proposed recursive combination that brings consistent improvements regardless of the model and\nthe dataset. The results are in accordance with our ﬁndings in exploratory experiments as well. For\nCIFAR-100, the best improvement of 1.02 absolute error rates is achieved upon the baseline that is the\ndeepest and should be the most difﬁcult to optimize. The relative improvements across models of different\ndepth also suggest that the representation ability of the deep models still needs to be developed and\ntransferred into real performance elevation, while the proposal shows potential. The results on EN-VI,\nEN-DE and EN-FR datasets are also encouraging. With approximately one third of the parameters, the\nTransformer-Base model could be improved to compete with the Transformer-Big model on EN-VI and\nDataset Method ResNet-20 ResNet-32 ResNet-44 ResNet-56 ResNet-110\nCIFAR-100 Baseline 33.38 32.00 30.50 29.81 28.71\n+ proposal 32.99 (+0.39) 31.39 (+0.61) 29.85 (+0.65) 28.98 (+0.83) 27.69 (+1.02)\nTable 6: Results on CIFAR-100 using ResNet and the proposed 2rSkip+LN skip connection construction.\nAverage of 5 runs. The numbers in parentheses denote the improvements. The proposal brings consistent\nimprovements. The improvements are more evident for deeper and complex models.\nMethod # Parameters EN-VI EN-DE EN-FR\nTransformer-Big (Vaswani et al., 2017) 213M - 28.4 41.0\nDeep Representations (Dou et al., 2018) 356M - 29.2 -\nEvolved Transformer (So et al., 2019) 218M - 29.8 41.3\nFixup (Zhang et al., 2019) - - 29.3 -\nAdaNorm (Xu et al., 2019) - 31.4 28.5 -\nScaleNorm (Nguyen and Salazar, 2019) - 32.8 27.6 -\nAdmin (Liu et al., 2020b) - - 29.0 43.8\nMUSE (Zhao et al., 2019) - 31.3 29.9 43.5\nTransformer-Base 65M 30.3 27.9 38.2\n+ proposal +0.03M 31.5 (+1.2) 28.6 (+0.7) 39.0 (+0.8)\nTransformer-Big 213M 31.4 28.5 41.0\n+ proposal +0.06M 32.2 (+0.8) 29.0 (+0.5) 41.6 (+0.6)\nTable 7: Results on EN-VI, EN-DE and EN-FR using Transformer and the proposed 2rSkip+LN skip\nconnection construction. The numbers of parameters are reported for the EN-DE models. As we can see,\nthe proposal brings consistent substantial improvements.\nEN-DE, with the help of the reﬁned skip connection architecture. It again demonstrates the potential of\nthe proposal in realizing the capability of the models by inducing more efﬁcient optimization.\n5 Conclusion and Future Work\nIn this work, we focus on exploring the combination of skip connection and layer normalization to build a\nnovel skip connection architecture able to incorporate reasonable skip information at output level and solve\nthe gradient explosion problem in practice. According to the empirical studies, we ﬁnd that expanding\nskip information should be helpful to the optimization of the model, which is hindered by the gradient\nmalformation in practice, and layer normalization does better than batch normalization in mitigating such\nproblem. The proposed Recursive Skip Connection with Layer Normalization guarantees the stability\nof the shorcut gradient by recursively dividing the expanded skip connection and adding it to the output\niteratively with layer normalization. The reﬁned skip connection architecture may enable more efﬁcient\noptimization. Validation on CIFAR-100, WMT-2014 EN-DE and EN-FR proves the ﬁndings and the\neffectiveness of the proposed recursive skip connection construction with best improvements over 1.0\nabsolute point, demonstrating its generalization ability to a wide range of existing systems.\nUnfortunately, a limitation of our investigation is that λis selected as integer and only a small number\nof values are used, which may not be of the best granularity for expanding the skip connection. A\nmore desirable solution would be to adaptively determine the value of λ, maybe in terms of the gradient\ndistribution during the learning so that more efﬁcient optimization could be realized. We will further seek\nto address this limitation in future work.\nAcknowledgments\nThis work is partly supported by AOTO-PKUSZ Joint Research Center for Artiﬁcial Intelligence on Scene\nCognition Technology Innovation, and Beijing Academy of Artiﬁcial Intelligence (BAAI). We thank all\nthe anonymous reviewers for their constructive comments and suggestions.\nReferences\nLei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization. CoRR, abs/1607.06450.\nThomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W. Cottrell, and Julian J.\nMcAuley. 2020. Rezero is all you need: Fast convergence at large depth. CoRR, abs/2003.04887.\nMauro Cettolo, Jan Niehues, Sebastian St ¨uker, Luisa Bentivogli, Roldano Cattoni, and Marcello Federico. 2015.\nThe iwslt 2015 evaluation campaign. In International Workshop on Spoken Language Translation 2015, IWSLT\n2015.\nZi-Yi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi, and Tong Zhang. 2018. Exploiting deep representations\nfor neural machine translation. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors,\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 4253–4262. Association for Computational Linguistics.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016a. Deep residual learning for image recognition.\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV , USA, June\n27-30, 2016, pages 770–778. IEEE Computer Society.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016b. Identity mappings in deep residual networks.\nIn Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision - ECCV 2016 - 14th\nEuropean Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, volume 9908\nof Lecture Notes in Computer Science, pages 630–645. Springer.\nSergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In Francis R. Bach and David M. Blei, editors, Proceedings of the 32nd International\nConference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 , volume 37 of JMLR Workshop\nand Conference Proceedings, pages 448–456. JMLR.org.\nAlex Krizhevsky. 2009. Learning multiple layers of features from tiny images. Technical report, Computer\nScience Department, University of Toronto.\nFenglin Liu, Xuancheng Ren, Guangxiang Zhao, and Xu Sun. 2020a. Layer-wise cross-view decoding for\nsequence-to-sequence learning. CoRR, abs/2005.08081.\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. 2020b. Understanding the difﬁculty of\ntraining transformers. CoRR, abs/2004.08249.\nMinh-Thang Luong and Christopher D. Manning. 2015. Stanford neural machine translation systems for spoken\nlanguage domain. In International Workshop on Spoken Language Translation 2015, IWSLT 2015.\nToan Q. Nguyen and Julian Salazar. 2019. Transformers without tears: Improving the normalization of self-\nattention. In International Workshop on Spoken Language Translation 2019, IWSLT 2019.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalua-\ntion of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational\nLinguistics, July 6-12, 2002, Philadelphia, PA, USA., pages 311–318. ACL.\nSheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. 2020. Powernorm: Rethinking\nbatch normalization in transformers. CoRR, abs/2003.07845.\nDavid R. So, Quoc V . Le, and Chen Liang. 2019. The evolved transformer. In Kamalika Chaudhuri and Ruslan\nSalakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-\n15 June 2019, Long Beach, California, USA , volume 97 of Proceedings of Machine Learning Research, pages\n5877–5886. PMLR.\nRupesh Kumar Srivastava, Klaus Greff, and J¨urgen Schmidhuber. 2015. Highway networks. In ICML 2015 Deep\nLearning Workshop.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2016, Las Vegas, NV , USA, June 27-30, 2016, pages 2818–2826. IEEE Computer Society.\nChristian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi. 2017. Inception-v4, inception-\nresnet and the impact of residual connections on learning. In Satinder P. Singh and Shaul Markovitch, editors,\nProceedings of the Thirty-First AAAI Conference on Artiﬁcial Intelligence, February 4-9, 2017, San Francisco,\nCalifornia, USA., pages 4278–4284. AAAI Press.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. 2017. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,\nHanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman Garnett, editors, Advances in Neural\nInformation Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9\nDecember 2017, Long Beach, CA, USA, pages 6000–6010.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Franc ¸ois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones,\nLukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. 2018.\nTensor2tensor for neural machine translation. In Colin Cherry and Graham Neubig, editors, Proceedings of the\n13th Conference of the Association for Machine Translation in the Americas, AMTA 2018, Boston, MA, USA,\nMarch 17-21, 2018 - Volume 1: Research Papers , pages 193–199. Association for Machine Translation in the\nAmericas.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019. Learning\ndeep transformer models for machine translation. In Anna Korhonen, David R. Traum, and Llu ´ıs M`arquez,\neditors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Flo-\nrence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 1810–1822. Association for Computational\nLinguistics.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei\nWang, and Tie-Yan Liu. 2020. On layer normalization in the transformer architecture. CoRR, abs/2002.04745.\nJingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. 2019. Understanding and improv-\ning layer normalization. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch ´e-Buc,\nEmily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual\nConference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver,\nBC, Canada, pages 4383–4393.\nHongyi Zhang, Yann N. Dauphin, and Tengyu Ma. 2019. Fixup initialization: Residual learning without normal-\nization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019. OpenReview.net.\nGuangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan Zhang, and Liangchen Luo. 2019. MUSE: parallel multi-scale\nattention for sequence to sequence learning. CoRR, abs/1911.09483.\nA On Adaptive Ratio of Recursive Skip Connection with Layer Normalization\nTo see how the recursive structure can learn the ratio, we takeλ= 2as an example and unroll the formula:\ny2 = LN (x+ LN (x+ F(x)))\n= w2 ·\n\n\nx+\n(\nw1 ·\n(\nx+(F(x))−µ1\nσ1\n)\n+ b1\n)\n−µ2\nσ2\n\n+ b2\n=\n(w2\nσ2\n+ w2 ·w1\nσ2σ1\n)\nx+ w2 ·w1\nσ2σ1\nF(x) +C, (5)\nwhere ·denotes entrywise-product, w1,w2,b1,b2 are the gain and the bias parameters of the afﬁne\ntransformation in layer normalization, and σ1,σ2,µ1,µ2 are the standard deviation and the mean of\nx+ F(x) and x+ y1, respectively. As we can see, the ratio between the input and the residual is\nx: F(x) := w2\nσ2\n+ w2 ·w1\nσ2σ1\n: w2 ·w1\nσ2σ1\n= σ1\nw1\n+ 1. (6)\nBecause the recursive deﬁnition, the inner gain parameters of layer normalization could penetrate through\nthe normalization and effectively act as the learnable ratio. It is also important to note that the ratio is also\ndependent on the current layer distribution, which could serve as a regularization component. For any λ\nlarger than 2, the general form of the ratio is 1 +∑λ\ni=1\n∏i\nj=1 σj/wj . It should be reminded that setting\nλ= 2is sufﬁcient for automatically learning the shortcut-residual ratio and dynamically adjusting the\nratio based on instant data distribution. Yet with larger λ, the distribution of the higher order combination\nof xand F(x) will be considered. The recursive deﬁnition introduces gradual and smooth combination\nof the input and the residual up to the speciﬁed order λwhich could deal with different distributions better.\nOur experimental results also demonstrate that the proposal performs better than the scaled shortcut."
}