{
    "title": "Explaining How Transformers Use Context to Build Predictions",
    "url": "https://openalex.org/W4385569886",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5110335380",
            "name": "Javier Ferrando",
            "affiliations": [
                "Universitat Politècnica de Catalunya"
            ]
        },
        {
            "id": "https://openalex.org/A5014771820",
            "name": "Gerard I. Gállego",
            "affiliations": [
                "Universitat Politècnica de Catalunya"
            ]
        },
        {
            "id": "https://openalex.org/A5076257265",
            "name": "Ioannis Tsiamas",
            "affiliations": [
                "Universitat Politècnica de Catalunya"
            ]
        },
        {
            "id": "https://openalex.org/A5074210163",
            "name": "Marta R. Costa‐jussà",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3035563045",
        "https://openalex.org/W4327811009",
        "https://openalex.org/W2346578521",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W2594633041",
        "https://openalex.org/W3101155149",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W4308023630",
        "https://openalex.org/W2996728628",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2977944219",
        "https://openalex.org/W4221152155",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2398041834",
        "https://openalex.org/W2912070261",
        "https://openalex.org/W3213645763",
        "https://openalex.org/W2971296520",
        "https://openalex.org/W1902674502",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2950768109",
        "https://openalex.org/W4297645476",
        "https://openalex.org/W4297412003",
        "https://openalex.org/W4385572790",
        "https://openalex.org/W4385570354",
        "https://openalex.org/W3035422918",
        "https://openalex.org/W2962851944",
        "https://openalex.org/W2970820321"
    ],
    "abstract": "Language Generation Models produce words based on the previous context. Although existing methods offer input attributions as explanations for a model’s prediction, it is still unclear how prior words affect the model’s decision throughout the layers. In this work, we leverage recent advances in explainability of the Transformer and present a procedure to analyze models for language generation. Using contrastive examples, we compare the alignment of our explanations with evidence of the linguistic phenomena, and show that our method consistently aligns better than gradient-based and perturbation-based baselines. Then, we investigate the role of MLPs inside the Transformer and show that they learn features that help the model predict words that are grammatically acceptable. Lastly, we apply our method to Neural Machine Translation models, and demonstrate that they generate human-like source-target alignments for building predictions.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 5486–5513\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nExplaining How Transformers Use Context to Build Predictions\nJavier Ferrando1, Gerard I. Gállego1, Ioannis Tsiamas1, Marta R. Costa-jussà2\n1TALP Research Center, Universitat Politècnica de Catalunya\n2Meta AI\n{javier.ferrando.monsonis,gerard.ion.gallego,ioannis.tsiamas}@upc.edu\ncostajussa@meta.com\nAbstract\nLanguage Generation Models produce words\nbased on the previous context. Although exist-\ning methods offer input attributions as explana-\ntions for a model’s prediction, it is still unclear\nhow prior words affect the model’s decision\nthroughout the layers. In this work, we lever-\nage recent advances in explainability of the\nTransformer and present a procedure to analyze\nmodels for language generation. Using con-\ntrastive examples, we compare the alignment\nof our explanations with evidence of the lin-\nguistic phenomena, and show that our method\nconsistently aligns better than gradient-based\nand perturbation-based baselines. Then, we\ninvestigate the role of MLPs inside the Trans-\nformer and show that they learn features that\nhelp the model predict words that are gram-\nmatically acceptable. Lastly, we apply our\nmethod to Neural Machine Translation models,\nand demonstrate that they generate human-like\nsource-target alignments for building predic-\ntions.\n1 Introduction\nLanguage Generation Models, like Transformer-\nbased Language Models (Brown et al., 2020; Zhang\net al., 2022a) have recently revolutionized the field\nof Natural Language Processing (NLP). Despite\nthis, there is still a gap in our understanding of\nhow they are able to produce language that closely\nresembles that of humans. This means that we\nare unable to determine the cause of a model’s\nfailure in specific instances, which can result in the\ngeneration of hallucinated content or toxic output.\nThe majority of previous work in explainability\nof NLP model predictions has focused on analyzing\nthem on downstream tasks, generally with a small\noutput space, such as text classification or Natural\nLanguage Inference (Atanasova et al., 2020; Bast-\nings et al., 2022; Zaman and Belinkov, 2022). This\nline of research includes a large body of work fo-\ncusing on the analysis of the attention mechanism\nLogits Difference: Increase Decrease\nModel Prediction: has (2.2%), have (0.1%)\nLogits Difference: logithas−have = 3.1\nL12 | A report about the Impressionists has\nL11 | A report about the Impressionists has\nL10 | A report about the Impressionists has\nL9 | A report about the Impressionists has\nL8 | A report about the Impressionists has\nL7 | A report about the Impressionists has\nL6 | A report about the Impressionists has\nL5 | A report about the Impressionists has\nL4 | A report about the Impressionists has\nL3 | A report about the Impressionists has\nL2 | A report about the Impressionists has\nL1 | A report about the Impressionists has\n∑ | A report about the Impressionists has\nTable 1: Updates to the (logits) prediction difference\nbetween has and have in different layers produced by\ninput tokens. Red indicates an increase in the difference\nin logits between both predictions. At the bottom, we\nshow the final logit contributions. The contrastive exten-\nsion of our proposed method, ALTI-Logit, shows that\nthe model relies on the head of the subject (report) to\ncorrectly solve the subject-verb agreement. See expla-\nnations from other methods in Table 3. GPT-2 Small\nshown here, see GPT-2 XL ALTI-Logit explanation in\nAppendix H.2.\n(Jain and Wallace, 2019; Serrano and Smith, 2019;\nPruthi et al., 2020), and on applying gradient-based\nmethods (Li et al., 2016a; Sundararajan et al., 2017)\nto obtain input attribution scores.\nRecently, several works have tackled the inter-\npretability of Transformers (Vaswani et al., 2017)\non the Language Modeling task. Elhage et al.\n(2021) studied the Transformer from the residual\nstream perspective, depicted in Figure 1, where dif-\nferent components (MLPs, attention heads...) read\nand write to subspaces of the residual stream. This\n5486\napproach has aided in explaining certain behaviours\nof language models, like induction heads (Olsson\net al., 2022), where attention heads search over the\ncontext for previous repetitions of the same token\nand copy the next token, or even specialized heads\nsolving the Indirect Object Identification (IOI) task\n(Wang et al., 2023). Similarly, MLPs inside the\nTransformer have also been studied as elements\nwriting into the residual stream. Geva et al. (2022)\nobserved that MLP blocks can act as key-value\nmemories, where values add to the residual, thus\npromoting the prediction of words that convey sim-\nilar semantic meaning.\nFurthermore, the attention mechanism in the\nTransformer, composed of attention heads, an out-\nput weight matrix, and a layer normalization, can\nbe decomposed into an interpretable operation\n(Kobayashi et al., 2020, 2021), providing layer-\nwise explanations which have proven to be highly\nfaithful (Ferrando et al., 2022b,a).\nIn this work, we propose explaining the pre-\ndictions of Transformers language generators by\ncombining the residual stream analysis perspective\nwith the attention decomposition. Our approach\nmeasures the amount of logit (pre-activation of the\nsoftmax) added or subtracted by each token rep-\nresentation at each layer. We then track the logit\ncontributions back to the model’s input by aggregat-\ning across layers (Logit explanation). Additionally,\nwe consider the mixing of information in interme-\ndiate layers by using ALTI (Ferrando et al., 2022b)\n(ALTI-Logit explanation).\nTo evaluate the proposed interpretability meth-\nods, we follow the recently introduced contrastive\nexplanations framework (Yin and Neubig, 2022),\nwhich aims to explain why the model predicted one\ntoken instead of a foil token, a prioriexplained by\nsome linguistic phenomena evidence. Then, we\nanalyze the role of MLPs and show that they aid\nthe model in determining predictions that follow\ngrammar rules. Finally, we demonstrate that NMT\nmodels generate human-like source-target align-\nments for building translations.1\n2 Approach\n2.1 Residual Stream\nGiven a language generation timestep t, the out-\nput of the last layer,2 xL\nt ∈Rd, is projected to the\n1The code accompanying the paper is available at\nhttps://github.com/mt-upc/logit-explanations.\n2We refer to it as a row vector.\nLN \nLN \nsoftmax \ncomputer laptop \nFigure 1: A Transformer Language Model, represented\nas modules writing into the residual stream.\ntoken embedding space by applying the unembed-\nding matrix U ∈Rd×|V |to get the logits of the\nnext token prediction. Then, a softmax function is\napplied to obtain a probability distribution over the\nvocabulary:\nP(xL\nt ) = softmax(xL\nt U) (1)\nThe residual connection in the Transformer can\nbe seen as an information stream (nostalgebraist,\n2020; Elhage et al., 2021; Mickus et al., 2022) that\ngets updated after each block. Let’s call ol\nt and\n˜xl\nt the output of the MLP and self-attention blocks\nat layer lrespectively, ‘writing’ into the residual\nstream at position t(Figure 1). The last state of the\nresidual stream can be represented as\nxL\nt =\nL∑\nl\nol\nt +\nL∑\nl\n˜xl\nt + x0\nt (2)\nThe final logit of a particular next token prediction\nwcan be computed by multiplying the last state of\nthe residual stream with the w-th column3 of U:\nlogitw = xL\nt Uw\n=\n( L∑\nl\nol\nt +\nL∑\nl\n˜xl\nt + x0\nt\n)\nUw\n(3)\nBy linearity:\nlogitw =\nL∑\nl\nol\ntUw +\nL∑\nl\n˜xl\ntUw + x0\nt Uw (4)\n3Note that we refer to the j-th column of a matrix B as\nBj, instead of B:,j.\n5487\nLN \nFigure 2: The output of the self-attention block at each\nlayer updates the logit of w (left). The logit’s update\ncan be decomposed per input token (right).\n2.2 Multi-head Attention as a Sum of Vectors\nInspired by the decomposition of the Post-LN self-\nattention block done by Kobayashi et al. (2021),\nwe apply a similar approach to the Pre-LN setting,\ncommon in current LMs (see full derivation in Ap-\npendix A). The output of the self-attention block at\neach generation step tcan be expressed as\n˜xl\nt =\nt∑\nj\nTl\nt,j(xl−1\nj ) + bl\nO (5)\nwhere Tl\nt,j : Rd ↦→Rd is an affine transformation\napplied to each layer’s input token representation\n(or residual stream) xl−1\nj ∈Rd:\nTl\nt,j(xl−1\nj ) =\nH∑\nh\n(\nxl−1\nj LlWl,h\nV Al,h\nt,jWl,h\nO +Al,h\nt,jθl,h\n)\n(6)\nwith Wl,h\nV ∈ Rd×dh the matrix generating the\nvalues, Wl,h\nO ∈Rdh×d the attention output ma-\ntrix (per head) and bl\nO ∈Rd its associated bias.\nAl,h ∈Rt×t is the attention weight matrix of each\nhead, θl,h ∈Rd remaining terms originated from\nbiases, and Ll ∈Rd×d combines centering, nor-\nmalizing, and scaling operations of the layer nor-\nmalization (see Appendix A).\n2.3 Layer-wise Contributions to the Logits\nCombining Equation (4) and Equation (5) we get4:\nlogitw =\nL∑\nl\nol\ntUw\n\n∆logitl\nw←MLPl\n+\nL∑\nl\nt∑\nj\nTl\nt,j(xl−1\nj )Uw\n  \n∆logitl\nw←Self-attnl\n+x0\nt Uw\n(7)\nThe logit’s update of each self-attention,\n∆logitl\nw←Self-attnl, can be expanded into individual\n4Biases are removed to save space.\nFigure 3: xl−1\n2 and xl−1\n3 contribute 10 and 4 logits\nrespectively to the next token predictionw= time. Due\nto the mixing of contextual information across layers,\nupon contributes 1\n2 to xl−1\n2 and 1\n4 to xl−1\n3 , which results\nin upon contributing 10 ·1\n2 + 4·1\n4 = 5 + 1 = +6logits.\nupdates by each xl−1\nj (Figure 2). Therefore, the\ncontribution of each layer’s input token represen-\ntation xl−1\nj to an output token wcan be defined as\nits update to the logit of w:\n∆logitl\nw←xl−1\nj\n= Tl\nt,j(xl−1\nj )Uw (8)\nSimilarly, logit updates can be computed at the\nhead level (∆logitl,h\nw←xl−1\nj\n) by multiplying the un-\nembedding matrix with the head-wise affine trans-\nformation in Equation (6).\n2.4 Tracking Logit Updates to the Input\nTokens\nIf we assume each residual stream preserves its\ntoken identity throughout the layers, the total logit\nupdate to wproduced by input token scan be com-\nputed as\n∆logitw←s =\nL∑\nl\n∆logitl\nw←xl−1\nj=s\n(9)\nthat is, the sum of the logit updates performed by\nthe s-th token intermediate representations at every\nlayer. Henceforth, we refer to this as the Logit\nexplanation.\nHowever, in intermediate layers, each residual\nstream represents a mixture of input tokens (Brun-\nner et al., 2020). Therefore, ∆logitl\nw←xl−1\nj\ncan’t\nbe directly interpreted as the logit update caused\nby the model’s input token s= j. We propose to\ntrack the logit update back to the model inputs by\nmeasuring the mixing of contextual information\nin the residual streams. For that purpose, we use\nALTI (Ferrando et al., 2022b). ALTI, as well as\nother methods relying on therollout method (Abnar\nand Zuidema, 2020; Mohebbi et al., 2023) assume\n5488\nthat token representations are formed by linearly\ncombining the representations from the preced-\ning layer, i.e. xl\ni = ∑\nj cl\ni,jxl−1\nj , with ∑\nj cl\ni,j = 1.\nEach cl\ni,j refers to the contribution of xl−1\nj to xl\ni.\nBy multiplying the layer-wise coefficient matrices,\nMl = Cl ·C2 ···C1, one can describe each inter-\nmediate layer representation as a linear combina-\ntion of the model input tokens, xl\ni = ∑\ns ml\ni,sx0\ns.\nColumn sof Ml−1 contains the proportion of\nthe s-th input token’s contribution encoded in each\ntoken representation entering layer l. We can ob-\ntain the update performed by each model input to-\nken (Figure 3, right) to the logit of a next prediction\ntoken was\n∆logitl\nw←s = ∆logitl\nw←xl−1 Ml−1\ns (10)\nWe refer to Appendix B for a more detailed ex-\nplanation. The final contribution of the s-th input\ntoken to the prediction of token wcan be obtained\nas the sum of its logit updates at each layer:\n∆logitw←s =\nL∑\nl\n∆logitl\nw←s (11)\nWe denote this method the ALTI-Logit explanation.\nNote that if we don’t consider mixing of contextual\ninformation, Ml−1 becomes the identity matrix,\nand we get the Logit explanation (Equation (9)).\n2.5 Contrastive Explanations\nContrastive explanations (Yin and Neubig, 2022)\naim to explain why the model predicted one target\ntoken winstead of another foil token f. We can ex-\nplain this decision by determining how much each\ntoken contributed to the final logit difference be-\ntween wand f: logit(w−f). Following Equation (9)\nand Equation (11), we can define the Contrastive\nLogit and Contrastive ALTI-Logit5 saliency scores\nof input tokens as their update to the logit differ-\nence:\n∆logit(w−f)←s = ∆logitw←s −∆logitf←s (12)\n3 Experimental Setup\nWe evaluate the quality of our proposed method\nthrough contrastive explanations. Following Yin\nand Neubig (2022) we use a subset of BLiMP\ndataset (Warstadt et al., 2020), which contains sen-\ntence pairs with small variations in grammatical\n5Throughout the paper we use Logit and ALTI-Logit to\nrefer also to their contrastive variant.\nPhenomena ID Example(Acceptable/Unacceptable)\nAnaphor AgreementagaKarla could listen to herself/himself.anaEva approached herself/themselves.\nArgument Structure asp Gerald ishated by the teachers/pie.\nDeterminer-Noun Agreement\ndna Eva has scaredthese children/child.dnai Tammy was observingthat man/men.dnaa The driver seesthat unlucky person/people.dnaai Phillip likedthat smooth horse/horses.\nNPI Licensing npiEven Danielle also/ever leaves.\nSubject-Verb Agreementdarn Thegrandfathers of Diana drink/drinks.ipsv Manypeople have/has hidden away.rpsv Mostassociations buy/buys those libraries.\nTable 2: Examples: in Table 8 of BLiMP phenomenons6\nused by Yin and Neubig (2022), with acceptable and\nunacceptable continuations in bold. Underlined words\nrepresent the linguistic evidence to resolve the phenom-\nena (extracted by the rules).\ncorrectness. The 11 subsets belong to 5 linguistic\nphenomena: anaphor agreement, argument struc-\nture, determiner-noun agreement, NPI licensing,\nand subject-verb agreement.\nFor each linguistic phenomena, we use spaCy\n(Honnibal and Montani, 2017) and follow Yin and\nNeubig (2022) rules to find the evidence (in previ-\nous tokens), that is enforcing grammatical accept-\nability (Table 2). For anaphor agreement, we obtain\nall context tokens that are coreferent with the target\ntoken. For argument structure, we extract the main\nverb of the sentence. Determiner-noun agreement’s\nevidence is found in the determiner of the target\nnoun. In NPI licensing, \"even\" word can appear in\nthe acceptable target, but not in the unacceptable.\nFinally, in the subject-verb agreement phenomenon,\nthe form of the verb has to agree in number with\nthe head of the subject, which we use as evidence.\nWe differ from Yin and Neubig (2022) in that we\ndiscard ipsv and rpsv subsets, due to the large\nfraction of sentences with a ‘quantifier + head of\nsubject + verb’ structure, where both the quantifier\n(many, most...) and the head of the subject could\nbe used by the model to solve the agreement.\nWe also add to the analysis SVA (subject-verb\nagreement) (Linzen et al., 2016) and the Indirect\nObject Identification (IOI) (Wang et al., 2023; Fa-\nhamu, 2022) datasets. The SVA dataset includes\nnouns with an opposite number to that of the main\n6BLiMP IDs. aga: anaphor_gender_agreement; ana:\nanaphor_number_agreement; asp: animate_subject_passive;\ndna: determiner_noun_agreement_1; dnai: deter-\nminer_noun_agreement_irregular_1; dnaa: deter-\nminer_noun_agreement_with_adj_1; dnaai: deter-\nminer_noun_agreement_with_adj_irregular_1; npi:\nnpi_present_1; darn: distractor_agreement_relational_noun;\nipsv: irregular_plural_subject_verb_agreement_1; rpsv:\nregular_plural_subject_verb_agreement_1\n5489\nsubject, which makes this dataset well-suited for\nevaluating saliency methods. Indirect object identi-\nfication (IOI) is a feature present in sentences that\nhave an initial dependent clause, like \"After Lee\nand Evelyn went to the lake\", followed by a main\nclause, like \"Lee gave a grape to Evelyn\". The\nindirect object \"Evelyn\" and the subject \"Lee\" are\nfound in the initial clause. In all examples of IOI\ndataset, the main clause refers to the subject again,\nwhich gives an object to the IO. The goal of theIOI\ntask is to predict the final word in the sentence to\nbe the IO. In IOI examples, the rule for predicting\nthe IO is the IO itself being in the first clause.\nWe use GPT-2 XL (1.5B) model (Radford et al.,\n2019), as in (Yin and Neubig, 2022), as well as\nother autoregressive Transformer language mod-\nels, such as GPT-2 Small (124M), and GPT-2\nLarge models (774M), OPT 125M (Zhang et al.,\n2022b), and BLOOM’s 560M and 1.1B variants\n(BigScience Workshop, 2022), through Hugging-\nFace library (Wolf et al., 2020).\nAlignment Metrics. Following Yin and Neubig\n(2022), we define the evidence as a binary vector\nb ∈Rt (with as many components as the number\nof previous tokens), with all zeros except in the\nposition of the tokens inside the evidence, i.e. the\ntokens which the prediction depends on, extracted\nby the rule. Explanations are vectors, also ∈Rt.\nTo measure the alignment between an explanation\nand the evidence we use MRR (Mean Reciprocal\nAnalysis). Sorting the tokens in descending order,\nMRR evaluates the average of the inverse of the\nrank of the first token that is part of b. Although\nYin and Neubig (2022) use also dot-product and\nProbes Needed metrics for measuring alignments,\ndot-product favors Grad Norm explanations since\nit gives positive scores only, and Probes Needed is\nclosely related to MRR, giving redundant results.\n4 Contrastive Methods\nYin and Neubig (2022) proposed extending dif-\nferent common input attribution methods to the\ncontrastive setting. In §5 we compare their expla-\nnations with the ones obtained with our proposed\ncontrastive methods (Equation (12)).\n4.1 Input Erasure\nErasure-based methods remove parts of the input\nand measure the change in the model’s prediction\n(Li et al., 2016b), where the higher the prediction\nFigure 4: Alignment (MRR ↑) of different explanation\nmethods of GPT-2 Small model predictions with BLiMP,\nSVA, and IOI datasets.\nchange, the higher the attribution of that partic-\nular token. Specifically, we take the difference\nbetween the model’s output with the entire input\nx, and after removing from x the s-th token, i.e.\nmw(x) −mw(x¬s). Yin and Neubig (2022) define\nthe Contrastive Input Erasure as\nce\n(w,¬f)←s = (mw(x)−mw(x¬s))−(mf(x)−mf(x¬s))\n(13)\nThis metric evaluates the extent to which removing\nxs from the input increases the likelihood of the\nfoil, and decreases the likelihood of the target in\nthe model’s output.\n4.2 Gradient Norm\nThe Transformer model can be approximated\nby the linear part of the Taylor-expansion\nat a baseline point (Simonyan et al., 2014),\nm(X0) ≈∇m(X0) ·X0, where X0 ∈ Rt×d\nis the sequence of input embeddings. There-\nfore, ∇mw(X0) represents the sensitivity of the\nmodel to each input dimension when predicting\nw. Following, saliency scores for each token can\nbe computed by taking the norm of the gradi-\nent vector corresponding to the token embedding,∇x0s m(X0)\n\n1.\nYin and Neubig (2022) extend this method to the\nContrastive Gradient Norm and define it as\ncg\n(w,¬f)←s =\n∇x0s\n(\nmw(X0) −mf (X0)\n)\n1\n(14)\n4.3 Gradient ×Input\nThe gradient ×input method (Shrikumar et al.,\n2016; Denil et al., 2014) calculates the dot product\nbetween the gradient and the input token embed-\nding. Yin and Neubig (2022) define the Contrastive\nGradient ×Input as\ncg×i\n(w,¬f)←s = ∇x0s\n(\nmw(X0) −mf (X0)\n)\n·x0\ns\n(15)\n5490\nLogit\nAreportabouttheImpressionistshas\nALTI-Logit\nAreportabouttheImpressionistshas\nErasure\nAreportabouttheImpressionistshas\nGrad Norm\nAreportabouttheImpressionistshas\nG×I\nAreportabouttheImpressionistshas\nTable 3: Comparison of different contrastive explanation\nmethods described in §4 and ALTI-Logit (has vs. have).\nSame example as in Table 1.\n5 Results\nIn the following sections we provide results on the\nalignment between the explanations of different\nmethods and linguistic evidence, as well as an anal-\nysis of observed model behaviours through the lens\nof ALTI-Logit.\n5.1 Alignment Results\nIn Figure 4 we present the MRR results of GPT-2\nSmall averaged across dataset categories, while the\nextended results for every subset can be found at\nAppendix C, Table 7. In Appendix C, Figure 11\nwe expand Figure 4 across different models. We\ncan observe that Logit and ALTI-Logit explana-\ntions consistently align better with the evidence\nof linguistic phenomena than common gradient-\nbased and erasure-based baselines. Note that for\nBLiMP the average we show in Figure 4 is across\n9 different subsets. In Table 3 we show an exam-\nple comparing different contrastive explanations,\nwhere Grad Norm, G×I and Erasure explanations\ndon’t align with the evidence to solve the subject-\nverb agreement (report), and disagree between each\nother.\nWe find similar alignment results for Logit and\nALTI-Logit methods. However, we observe that\nALTI-Logit aligns better at tasks where the to-\nkens of the linguistic evidence are far from the\nprediction. This is especially noticeable in Subject-\nverb agreement datasets (including SVA and darn),\nwhere ALTI-Logit shows higher alignments than\nany other method across all models. This might\nindicate that incorporating information about con-\ntextual mixing is advantageous for dealing with\nlarge contexts.\nDespite the generally accurate performance of\nthe models examined in this study (Figure 12 and\nFigure 13, Appendix D), there are cases where\nFigure 5: Update to the logit difference between the\nacceptable and the unacceptable predictions produced\nby the input tokens inside the linguistic evidence (GPT-\n2 XL).\nFigure 6: ALTI-Logit MRR alignment scores (line plots)\nand updates in logit difference by every input token\n(∆logitl\n(w−f)←Self-attnl ) between acceptable and unac-\nceptable predictions (box plots) per layer (GPT-2 Small).\nHorizontal dashed lines refer to random alignment.\nthe unacceptable token gets predicted with a higher\nprobability. In order to gain a deeper understanding\nof the variations in model behavior between correct\nand incorrect predictions, we analyze the logit up-\ndate generated by the input tokens associated with\nthe linguistic evidence. This analysis, conducted\nusing ALTI-Logit (Figure 5), reveals differences in\nthe distributions. These findings suggest that the\ntokens representing the linguistic evidence play a\ncrucial role in achieving accurate predictions, and\nif their contribution is only marginal, the likelihood\nof failure increases considerably.\n5491\nFigure 7: ALTI-Logit MRR alignment scores across\nlayers (GPT-2 XL). Horizontal dashed lines refer to\nrandom alignment.\n5.2 Layer-wise Analysis with ALTI-Logit\nIn the line plots in Figures 6 and 7 we provide\nthe MRR alignment results across layers of GPT2-\nSmall and GPT2-XL for two different linguistic\nphenomena. Models behave similarly across sub-\nsets inside the same phenomena, like in Subject-\nVerb Agreement ( SVA and darn), and Anaphor\nAgreement ( aga and ana) in Appendix E. The\nmodel’s alignment trend also stays similar, even\nthough the distance between the prediction and the\nevidence is different across subsets (SVA’s distance\nis 4 times darn’s).\nIn the boxplots in Figure 6, we show the distribu-\ntion of self-attention updates to the logit difference\nbetween the acceptable and the unacceptable pre-\ndictions, ∆logitl\n(w−f)←Self-attnl. As a general pat-\ntern, we observe that models tend to update more\nheavily on the layers where the alignment with\nlinguistic phenomena is higher. This conclusion\nholds for larger models too, see the darn exam-\nple in Appendix H.2, where large logit updates are\nfound in layers 28, 35, and 40, matching the layers\nwhere alignment peaks (Figure 7 Top). In IOI and\nSVA tasks both models align with the evidence and\nincrease their logit update towards the last layers.\nThis indicates that models solve these phenomena\nonce they have acquired sufficient contextual infor-\nmation.\nOur findings in the IOI task support those by\nWang et al. (2023). In GPT-2 Small we observe\nhigh logit difference updates coming from the In-\ndirect Object (IO) in layers 10 and 11. We further\nstudy the heads in those layers (Table 4), where\nWang et al. (2023) found ‘Name Mover Heads’ and\n‘Negative Mover Heads’. These heads rely on the\nIO to increase (Name Mover Heads) and decrease\nName Mover Head L10 H7Then,YvetteandAngiewereworkingatthemountain.YvettedecidedtogiveabananatoAngie\nName Mover Head L10 H10Then,YvetteandAngiewereworkingatthemountain.YvettedecidedtogiveabananatoAngie\nName Mover Head L11 H1Then,YvetteandAngiewereworkingatthemountain.YvettedecidedtogiveabananatoAngie\nNegative Name Mover Head L11 H8Then,YvetteandAngiewereworkingatthemountain.YvettedecidedtogiveabananatoAngie\nNegative Name Mover Head L12 H11Then,YvetteandAngiewereworkingatthemountain.YvettedecidedtogiveabananatoAngie\nTable 4: GPT-2 Small updates to the logit predic-\ntion difference between Angie and Yvette in different\nheads produced by layer input token representations\n(∆logitl,h\n(w−f)←xl−1\nj\n).\n(Negative Mover Heads) respectively the logit of\nthe correct prediction. In Appendix H.3 we provide\nan example of how every model solves the task\nacross layers.\n6 Analysis of MLPs\nThe MLP block in the Transformer contains two\nlearnable weight matrices 7: Wl\n1 ∈Rd×dmlp and\nWl\n2 ∈Rdmlp×d, and an element-wise non-linear\nactivation function α. It takes as input the state of\nthe residual stream at timestept(˜xl\nt) and computes:\nol\nt = α(LN(˜xl\nt)Wl\n1)Wl\n2 (16)\nFollowing, ol\nt is added back to the residual stream\n(Figure 1). Equation (16) can be seen as key-value\nmemories (Geva et al., 2021), where keys are stored\nin components of kl = α(LN(xl\nt)Wl\n1) ∈Rdmlp,\nand values (vl) are rows of W2. Following the key-\nvalue perspective, Equation (16) can be rewritten\nas\nol\nt =\ndmlp∑\ni\nkl\nivl\ni (17)\nwhere vl\ni represents the i-th row of W2. Recalling\nhow the final logit of a token w is decomposed\nby layer-wise updates in Equation (7), the MLPl\nupdates the logit of was follows:\n∆logitl\nw←MLPl = ol\ntU⊺\nw\n=\ndmlp∑\ni\nkl\nivl\niU⊺\nw\n=\ndmlp∑\ni\n∆logitl\nw←kl\nivl\ni\n(18)\nThus, the update of the MLP can be decomposed\ninto sub-updates (Geva et al., 2022) performed by\n7We omit bias terms.\n5492\n(a)\n (b)\n(c)\n (d)\nFigure 8: Average (across the dataset) of the updates\nto the logit difference caused by the weighted values\nin the MLP (each row iin Wl\n2), ∆logitl\n(w−f)←kl\nivl\ni\n. a)\ndna: dimension i=383 (L11) promotes singular nouns\n(increases the logit difference between singular and plu-\nral nouns) after this/that, b) dna: dimension i=3038\n(L11) promotes plural nouns afterthese/those. Dimen-\nsion i= 2187 (L12) pushes the prediction of singular\nverbs in different Subject-Verb Agreement datasets c)\ndarn and d) SVA.\neach kl\nivl\ni (weighted row in Wl\n2). The update in the\nlogit’s difference between the target and foil tokens\nby each value iis therefore:\n∆logitl\n(w−f)←kl\nivl\ni\n= ∆logitl\nw←kl\nivl\ni\n−∆logitl\nf←kl\nivl\ni\n(19)\nIn Figure 8, we show some examples of the\ncontribution of each weighted value kl\nivl\ni to the\nlogit difference between the acceptable target to-\nken and the unacceptable one, at different layers\nand datasets. We can observe that there is a small\nsubset of values that consistently increase the dif-\nference in logits helping to solve the linguistic task.\nSome of them include the value i=383 in layer 10\n(Figure 8 (a)), which increases the logit of singular\nnouns and reduces the plural ones when the deter-\nminer is this or that. For instance, in the sen-\ntence “William described this ___”, value i=383\nincreases the logit difference between movie and\nmovies. In dimension 3038 we find a value up-\nweighting the logits of the plural nouns over the sin-\ngular ones when the determiner is these or those\n(Figure 8 (b)). These values help solve the lin-\nguistic task at hand across different subsets, for\ninstance, the value in dimension i = 2187 is in\ncharge of promoting the singular form of the verb\nwhen the head of the subject is singular too. This\noccurs in both darn and SVA subsets.\nLN\nLN\nsource\ntarget prefix\ntarget prefixsource\nFigure 9: Cross-attention block in the Transformer’s\ndecoder (left) and its equivalent using vector transfor-\nmations (right). Depicted in green and red it’s shown the\ninformation coming from the encoder and the decoder\n(target prefix) respectively.\n7 Neural Machine Translation\nAn NMT system estimates the likelihood of a tar-\nget sequence of tokens, y = (y1,...,y t), given a\nsource sequence of tokens, x = (x1,...,x I):\nP(y|x) =\nt∏\ns\nP(ys|y<s,x) (20)\nwhere y<s = ( y0,...,y s−1) is the prefix of ys,\nand xI = y0 = </s> is a special token used\nto mark the start and end of the sentence. The\nencoder processes the source sentence and gener-\nates a sequence of contextualized representations,\ne = (e1,..., eI). At each decoding step t, the de-\ncoder uses the encoder outputs and the target prefix\nto compute a probability distribution over the target\nvocabulary.\nCross-attention. Similar to Equation (6), the out-\nput of the cross-attention ( ˜yc,l\nt ) and self-attention\n(˜ys,l\nt ) (Figure 9) of a decoder layer in an encoder-\ndecoder Transformer can be decomposed8 as\n˜yc,l\nt =\nI∑\ni\nTc,l\nt,i (ei), ˜ys,l\nt =\nt∑\nj\nTs,l\nt,j (yl−1\nj )\n(21)\nAs shown in Figure 9, each transformed vector\nupdates the logits of the token predictions by mul-\ntiplying it with the corresponding column of U, as\nin Equation (8):\n∆logitl\nw←ei = Tc,l\nt,i (ei)Uw (22)\n8Removing biases.\n5493\nAER(↓)\nMethod Bilingual M2M\nAttention weights 48.6 96.4\nSD-SmoothGrad (Ding et al., 2019)36.4 -\nVector Norms (Kobayashi et al., 2020)41.4 -\nDistance Vectors-Output (Ferrando et al., 2022a)38.8 36.4\nProposed alignment extraction 26.0 27.3\nTable 5: Mean AER of the cross-attention contributions\nin the best layer of the bilingual and M2M models. For\nthe bilingual model, we show the average on five differ-\nent seeds.\nAlignment. Source-target alignments derived\nfrom attention weights in NMT systems can be un-\nreliable (Zenkel et al., 2019; Li et al., 2019; Garg\net al., 2019), with upper layers producing better\nalignments. A limitation of using this method to\ninterpret model predictions is that the ground truth\ntarget word may not match the model’s actual pre-\ndiction. However, by measuring how the encoder\ntoken representations update the logits of the ref-\nerence words, ∆logitl\nw←ei, we can more precisely\nexplain which source word causes the final logit of\nthe reference word, even if it is not one of the top\npredictions.\nFollowing Kobayashi et al. (2020) and Ding\net al. (2019) setting, we train a 6-layer Transformer\nmodel for the German-English (De-En) transla-\ntion task using Europarl v7 corpus9 Koehn (2005).\nWe also evaluate on M2M, a 12 layer multilin-\ngual model (Fan et al., 2021). We use Vilar et al.\n(2006) dataset, consisting of 508 De-En human\nannotated sentence pairs with alignments, and com-\npare them with our extracted alignments using\nAlignment Error Rate (AER). We also show re-\nsults of other attention-based alignments extrac-\ntion methods. Vector Norms take the norm of\nthe transformed vectors in Equation (21), Distance\nVectors-Output measures the distance between the\ntransformed vectors and the attention block output\n˜yc,l\nt . SD-SmoothGrad relies on gradients to ex-\ntract alignments. In Table 5 we show that our pro-\nposed method achieves lower AER values, which\nindicates that NMT models generate human-like\nalignments for building model predictions.\n8 Related Work\nThe projection of LMs representations and model\nparameters to the vocabulary space has been a sub-\nject of previous research (Belrose et al., 2023; Din\net al., 2023). Geva et al. (2021, 2022) view feed-\n9http://www.statmt.org/europarl/v7\nFigure 10: Left: attention weights in the cross-attention\nin the penultimate layer. Right: contributions obtained\nas logit updates to token predictions in the penultimate\nlayer.\nforward layers as performing updates to the proba-\nbility distribution of the token predictions. Mickus\net al. (2022) study how the different Transformer\nmodules contribute to the hidden representations,\nand Dar et al. (2022) directly interpret Transformer\nstatic parameters in the embedding space. In this\nwork, our focus lies in interpreting the influence of\ninput tokens and its representations in the model\npredictions.\nFurthermore, work on mechanistic interpretabil-\nity (Olah, 2022) has discovered ‘circuits’ within\nLMs in charge of solving tasks (Wang et al., 2023;\nGeva et al., 2023). In contrast to their methods, our\napproach does not rely on causal interventions in\nthe computations of Transformers. More broadly,\nour work can be related to those explaining the pre-\ndiction process of LMs (Tenney et al., 2019; V oita\net al., 2019; Sarti et al., 2023).\n9 Conclusions\nIn this paper, we introduce a new procedure for an-\nalyzing language generation models by combining\nthe residual stream perspective with interpretable\nattention decomposition, and tested our approach\nusing contrastive examples in Transformer LMs.\nWe found that the explanations provided by our\nproposed methods, Logit and ALTI-Logit, align\nbetter with available linguistic evidence in the con-\ntext of the sentence, compared to common gradient-\nbased and erasure-based baselines. We also ana-\nlyzed the role of MLPs and showed that they assist\nthe model in determining predictions that conform\nto the grammar rules. Additionally, we applied\nour method to a Machine Translation model and\ndemonstrated that it generates human-like align-\nments for building predictions. Overall, our results\nsuggest that decomposing the logit scores is an ef-\nfective way to analyze language generation models.\n5494\n10 Limitations\nThe experimental methodology employed in this\nstudy for both contrastive explanations and NMT\nis not directly extensible to languages other than\nEnglish, due to the scarcity of resources such as\nmodels and annotations.\nThe datasets employed in this study to evaluate\ncontrastive explanations across various linguistic\nparadigms are restricted to sentences that possess a\nwell-defined structure. As a result, it is possible that\nthe conclusions drawn may not be generalizable to\nthe broader distribution of sentences.\nLastly, it should be noted that the method pro-\nposed in this study should not be used as a defini-\ntive explanation of model predictions in any other\ncontext. It is recommended to use the method as a\ndebugging tool and should be employed in conjunc-\ntion with other methods to gain a comprehensive\nunderstanding of model predictions.\n11 Ethics statement\nIt is acknowledged that the experiments reported in\nthis study are limited to high-resource languages.\nHowever, the methodology employed is language-\nindependent and may be applied to other languages\nin the future, provided that adequate annotated data\nbecomes available.\n12 Acknowledgements\nWe would like to thank the anonymous reviewers\nfor their useful comments. Javier Ferrando, Gerard\nI. Gállego and Ioannis Tsiamas are supported by\nthe Spanish Ministerio de Ciencia e Innovación\nthrough the project PID2019-107579RB-I00 / AEI\n/ 10.13039/501100011033.\nReferences\nSamira Abnar and Willem Zuidema. 2020. Quantify-\ning attention flow in transformers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4190–4197, On-\nline. Association for Computational Linguistics.\nPepa Atanasova, Jakob Grue Simonsen, Christina Li-\noma, and Isabelle Augenstein. 2020. A diagnostic\nstudy of explainability techniques for text classifi-\ncation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 3256–3274, Online. Association for\nComputational Linguistics.\nJasmijn Bastings, Sebastian Ebert, Polina Zablotskaia,\nAnders Sandholm, and Katja Filippova. 2022. “will\nyou find these shortcuts?” a protocol for evaluating\nthe faithfulness of input salience methods for text\nclassification. In Proceedings of the 2022 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 976–991, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nNora Belrose, Zach Furman, Logan Smith, Danny Ha-\nlawi, Igor Ostrovsky, Lev McKinney, Stella Bider-\nman, and Jacob Steinhardt. 2023. Eliciting latent\npredictions from transformers with the tuned lens.\nBigScience Workshop. 2022. BLOOM (revision\n4ab0472).\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nGino Brunner, Yang Liu, Damian Pascual, Oliver\nRichter, Massimiliano Ciaramita, and Roger Wat-\ntenhofer. 2020. On identifiability in transformers. In\nInternational Conference on Learning Representa-\ntions.\nGuy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant.\n2022. Analyzing transformers in embedding space.\nMisha Denil, Alban Demiraj, and Nando de Freitas.\n2014. Extraction of salient sentences from labelled\ndocuments. CoRR, abs/1412.6815.\nAlexander Yom Din, Taelin Karidi, Leshem Choshen,\nand Mor Geva. 2023. Jump to conclusions: Short-\ncutting transformers with linear transformations.\nShuoyang Ding, Hainan Xu, and Philipp Koehn. 2019.\nSaliency-driven word alignment interpretation for\nneural machine translation. In Proceedings of the\nFourth Conference on Machine Translation (Volume\n1: Research Papers), pages 1–12, Florence, Italy.\nAssociation for Computational Linguistics.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom\nHenighan, Nicholas Joseph, Ben Mann, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly,\nNova DasSarma, Dawn Drain, Deep Ganguli, Zac\nHatfield-Dodds, Danny Hernandez, Andy Jones,\nJackson Kernion, Liane Lovitt, Kamal Ndousse,\nDario Amodei, Tom Brown, Jack Clark, Jared Ka-\nplan, Sam McCandlish, and Chris Olah. 2021. A\nmathematical framework for transformer circuits.\nTransformer Circuits Thread.\nFahamu. 2022. ioi (revision 223da8b).\n5495\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\nChaudhary, Naman Goyal, Tom Birch, Vitaliy\nLiptchinsky, Sergey Edunov, Michael Auli, and Ar-\nmand Joulin. 2021. Beyond english-centric multilin-\ngual machine translation. Journal of Machine Learn-\ning Research, 22(107):1–48.\nJavier Ferrando, Gerard I. Gállego, Belen Alastruey,\nCarlos Escolano, and Marta R. Costa-jussà. 2022a.\nTowards opening the black box of neural machine\ntranslation: Source and target interpretations of the\ntransformer. Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 8756–8769.\nJavier Ferrando, Gerard I. Gállego, and Marta R. Costa-\njussà. 2022b. Measuring the mixing of contextual\ninformation in the transformer. Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing, pages 8698–8714.\nSarthak Garg, Stephan Peitz, Udhyakumar Nallasamy,\nand Matthias Paulik. 2019. Jointly Learning\nto Align and Translate with Transformer Models.\narXiv:1909.02074 [cs]. ArXiv: 1909.02074.\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir\nGloberson. 2023. Dissecting recall of factual associ-\nations in auto-regressive language models.\nMor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-\nberg. 2022. Transformer feed-forward layers build\npredictions by promoting concepts in the vocabulary\nspace. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 30–45, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 5484–5495, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremental\nparsing. To appear.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2020. Attention is not only a weight:\nAnalyzing transformers with vector norms. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7057–7075, Online. Association for Computa-\ntional Linguistics.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2021. Incorporating Residual and Nor-\nmalization Layers into Analysis of Masked Language\nModels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4547–4568, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of\nMachine Translation Summit X: Papers, pages 79–86,\nPhuket, Thailand.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016a. Visualizing and understanding neural models\nin NLP. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 681–691, San Diego, California.\nAssociation for Computational Linguistics.\nJiwei Li, Will Monroe, and Dan Jurafsky. 2016b. Un-\nderstanding neural networks through representation\nerasure.\nXintong Li, Guanlin Li, Lemao Liu, Max Meng, and\nShuming Shi. 2019. On the word alignment from\nneural machine translation. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 1293–1303, Florence, Italy.\nAssociation for Computational Linguistics.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of lstms to learn syntax-\nsensitive dependencies. Trans. Assoc. Comput. Lin-\nguistics, 4:521–535.\nTimothee Mickus, Denis Paperno, and Mathieu Con-\nstant. 2022. How to Dissect a Muppet: The Structure\nof Transformer Embedding Spaces. Transactions\nof the Association for Computational Linguistics,\n10:981–996.\nHosein Mohebbi, Willem Zuidema, Grzegorz Chrupała,\nand Afra Alishahi. 2023. Quantifying context mixing\nin transformers. In Proceedings of the 17th Confer-\nence of the European Chapter of the Association\nfor Computational Linguistics, pages 3378–3400,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nnostalgebraist. 2020. interpreting gpt: the\nlogit lens. https://www.lesswrong.\ncom/posts/AcKRB8wDpdaN6v6ru/\ninterpreting-gpt-the-logit-lens .\nChris Olah. 2022. Mechanistic interpretability,\nvariables, and the importance of interpretable\nbases. https://transformer-circuits.pub/\n2022/mech-interp-essay/index.html.\n5496\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas\nJoseph, Nova DasSarma, Tom Henighan, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Con-\nerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\nDanny Hernandez, Scott Johnston, Andy Jones, Jack-\nson Kernion, Liane Lovitt, Kamal Ndousse, Dario\nAmodei, Tom Brown, Jack Clark, Jared Kaplan,\nSam McCandlish, and Chris Olah. 2022. In-context\nlearning and induction heads. Transformer Circuits\nThread.\nDanish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham\nNeubig, and Zachary C. Lipton. 2020. Learning to\ndeceive with attention-based explanations. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4782–\n4793, Online. Association for Computational Lin-\nguistics.\nA. Radford, Jeffrey Wu, R. Child, David Luan, Dario\nAmodei, and Ilya Sutskever. 2019. Language models\nare unsupervised multitask learners. In OpenAI Blog.\nGabriele Sarti, Nils Feldhus, Ludwig Sickert, Oskar\nvan der Wal, Malvina Nissim, and Arianna Bisazza.\n2023. Inseq: An interpretability toolkit for sequence\ngeneration models. ArXiv, abs/2302.13942.\nSofia Serrano and Noah A. Smith. 2019. Is attention in-\nterpretable? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2931–2951, Florence, Italy. Association for\nComputational Linguistics.\nAvanti Shrikumar, Peyton Greenside, Anna Shcherbina,\nand Anshul Kundaje. 2016. Not just a black box:\nLearning important features through propagating ac-\ntivation differences. CoRR, abs/1605.01713.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zis-\nserman. 2014. Deep inside convolutional networks:\nVisualising image classification models and saliency\nmaps. In 2nd International Conference on Learn-\ning Representations, ICLR 2014, Banff, AB, Canada,\nApril 14-16, 2014, Workshop Track Proceedings.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Pro-\nceedings of the 34th International Conference on\nMachine Learning, volume 70 of Proceedings of Ma-\nchine Learning Research, pages 3319–3328, Interna-\ntional Convention Centre, Sydney, Australia. PMLR.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nDavid Vilar, Maja Popovic, and H. Ney. 2006. Aer: do\nwe need to \"improve\" our alignments? In IWSLT.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4396–4406, Hong Kong,\nChina. Association for Computational Linguistics.\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy,\nBuck Shlegeris, and Jacob Steinhardt. 2023. Inter-\npretability in the wild: a circuit for indirect object\nidentification in GPT-2 small. In The Eleventh Inter-\nnational Conference on Learning Representations.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020. BLiMP: The benchmark of linguis-\ntic minimal pairs for English. Transactions of the\nAssociation for Computational Linguistics, 8:377–\n392.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nKayo Yin and Graham Neubig. 2022. Interpreting lan-\nguage models with contrastive explanations. Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 184–198.\nKerem Zaman and Yonatan Belinkov. 2022. A multilin-\ngual perspective towards the evaluation of attribution\nmethods in natural language inference.\nThomas Zenkel, Joern Wuebker, and John DeNero.\n2019. Adding interpretable attention to neural trans-\nlation models improves word alignment. CoRR,\nabs/1901.11359.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022a. Opt: Open\npre-trained transformer language models.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\n5497\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022b. Opt: Open\npre-trained transformer language models.\n5498\nA Pre-LN Self-attention Decomposition\nxl−1\nj ∈Rd×dh Layer Input (Residual Stream position j)\nAl,h ∈Rt×t Attention Matrix\nWl,h\nV ∈Rd×dh Values Weight Matrix\nWl,h\nO ∈Rdh×d Output Weight Matrix (per head)\nbl,h\nV ∈Rdh Value bias\nbl\nO ∈Rd Output bias\nH ∈R Number of heads\nLNl : Rd ↦→Rd Layer Normalization\nTable 6: Components of the self-attention module.\nAt position t, each head of a Pre-LN self-attention mechanism computes:\nzl,h\nt =\nt∑\nj\n(\nLNl(xl−1\nj )Wl,h\nV + bl,h\nV  \nj-th value\n)\nAl,h\nt,j (23)\nBy representing attention heads as parallel independent components, we can express the output of the\nself-attention as\n˜xl\nt =\nH∑\nh\nzl,h\nt Wl,h\nO + bl\nO (24)\nleading to:\n˜xl\nt =\nt∑\nj\nH∑\nh\n(\nLNl(xl−1\nj )Wl,h\nV + bl,h\nV\n)\nAl,h\nt,j Wl,h\nO + bl\nO (25)\nThe layer normalization computes:\nLNl(xl−1\nj ) =\nxl−1\nj −µ(xl−1\nj )\nσ(xl−1\nj )\n⊙γl + βl (26)\nwith µand σ computing the mean and standard deviation, and γl ∈Rd and βl ∈Rd refer to learned\nelement-wise transformation and bias respectively. Considering σ(xl−1\nj ) as a constant, LN can be treated\nas a constant affine transformation:\nLN(xl−1\nj ) = xl−1\nj Ll + βl (27)\nwhere Ll ∈Rd×d represents a matrix that combines centering, normalizing, and scaling operations\ntogether.\nUsing Equation (27) in Equation (25):\n˜xl\nt =\nt∑\nj\nH∑\nh\n((\n(xl−1\nj Ll + βl)Wl,h\nV + bl,h\nV\n)\nAl,h\nt,j Wl,h\nO\n)\n+bl\nO\n=\nt∑\nj\nH∑\nh\n((\nxl−1\nj LlWl,h\nV + βlWl,h\nV + bl,h\nV\n)\nAl,h\nt,j Wl,h\nO\n)\n+bl\nO\n=\nt∑\nj\nH∑\nh\n(\nxl−1\nj LlWl,h\nV Al,h\nt,j Wl,h\nO + βlWl,h\nV Al,h\nt,j Wl,h\nO + bl,h\nV Al,h\nt,j Wl,h\nO\n)\n+bl\nO\n5499\n=\nt∑\nj\nH∑\nh\n(\nxl−1\nj LlWl,h\nV Al,h\nt,j Wl,h\nO + Al,h\nt,j\n(\nβlWl,h\nV Wl,h\nO + bl,h\nV Wl,h\nO\n))\n+bl\nO (28)\nConsidering θl,h =\n(\nβlWl,h\nV + bl,h\nV\n)\nWl,h\nO\n˜xl\nt =\nt∑\nj\nH∑\nh\n(\nxl−1\nj LlWl,h\nV Al,h\nt,j Wl,h\nO + Al,h\nt,j θl,h\n)\n+bl\nO (29)\nFor each j-th input term, Haffine transformations are applied to xj. Furthermore, all heads’ operations\ncan be further grouped into a single affine transformation:\n˜xl\nt =\nt∑\nj\n(\nxl−1\nj Ll\nH∑\nh\nWl,h\nV Al,h\nt,j Wl,h\nO +\nH∑\nh\nAl,h\nt,j θl,h\n)\n+bl\nO (30)\nSo, we can write ˜xl\nt as a sum of taffine transformations, and the output bias:\n˜xl\nt =\nt∑\nj\nTl\nt,j(xl−1\nj ) + bl\nO (31)\nB Tracking Logits to the Input with Rollout\nThe rollout method (Abnar and Zuidema, 2020) assumes any intermediate representation is a linear\ncombination of the model inputs, xl−1\nj = ∑\ns ml−1\nj,s x0\ns, where ml−1\nj,s is a score indicating the contribution\nof input token sto the l−1 representation (or residual path) of token j. By dividing the logit update\nperformed by xl−1\nj among the model inputs ( ∆logitl\nw,j←x0s\n) based on their contributions to xl−1\nj , we\nobtain:\n∆logitl\nw←xl−1\nj\n=\n∑\ns\n∆logitl\nw,j←x0s\n=\n∑\ns\nml−1\nj,s ∆logitl\nw←xl−1\nj\n(32)\nBased on the total logit update produced in layer l, we have that:\n∆logitl\nw←Self-attnl =\n∑\nj\n∆logitl\nw←xl−1\nj\n=\n∑\nj\n∑\ns\n∆logitl\nw,j←x0s\n=\n∑\nj\n∑\ns\nml−1\nj,s ∆logitl\nw←xl−1\nj\n=\n∑\ns\n∑\nj\nml−1\nj,s ∆logitl\nw←xl−1\nj\n=\n∑\ns\n∆logitl\nw←s (33)\nSo, we have obtained Equation (10):\n∆logitl\nw←s = ∆logitl\nw←xl−1 Ml−1\ns (34)\n5500\nC Results\nFigure 11: Alignment (MRR ↑) of different explanation methods of GPT-2 Small, Large, and XL, OPT 125M,\nBLOOM 560M, and BLOOM 1B1 model predictions with BLiMP, SV A, and IOI datasets.\n5501\nC.1 GPT-2 Small Results\nDataset Erasure Logit ALTI-Logit Grad Norm G×I Random Distance\naga 0.959 0.827 0.964 0.793 0.791 0.699 3.2ana 0.963 0.817 0.976 0.675 0.739 0.716 3.2asp 0.492 0.386 0.499 0.751 0.409 0.381 3.3dna 0.35 0.737 0.646 0.363 0.387 0.459 1dnai 0.374 0.711 0.637 0.408 0.432 0.466 1dnaa 0.61 0.951 0.807 0.263 0.321 0.397 2.1dnaai 0.659 0.9 0.757 0.263 0.339 0.406 2.1npi 0.663 0.445 0.417 0.785 0.495 0.599 3.2darn 0.557 0.802 0.949 0.617 0.363 0.488 3.9SV A 1 0.389 0.558 0.641 0.432 0.298 0.333 8SV A 2 0.425 0.57 0.606 0.421 0.303 0.292 11.6SV A 3 0.454 0.459 0.603 0.51 0.356 0.259 12.9SV A 4 0.371 0.454 0.566 0.433 0.222 0.249 16.4IOI 0.865 1.0 1.0 0.86 0.111 0.245 14.9\nTable 7: MRR Alignment of different explanation meth-\nods on GPT-2 Small predictions on every dataset. The\naverage distance to the linguistic evidence tokens is\nshown in the last column.\nC.2 GPT-2 XL Results\nDataset Erasure Logit ALTI-Logit Grad Norm G×I Random Distance\naga 0.974 0.79 0.974 0.778 0.713 0.681 3.2ana 0.945 0.777 0.964 0.721 0.655 0.71 3.2asp 0.506 0.368 0.514 0.721 0.44 0.369 3.3dna 0.326 0.655 0.539 0.255 0.486 0.465 1dnai 0.366 0.598 0.524 0.264 0.515 0.453 1dnaa 0.631 0.932 0.615 0.205 0.352 0.413 2.1dnaai 0.644 0.874 0.529 0.205 0.359 0.393 2.1npi 0.735 0.602 0.711 0.82 0.586 0.594 3.2darn 0.576 0.873 0.945 0.686 0.477 0.51 3.9SV A 1 0.416 0.564 0.638 0.467 0.365 0.352 8SV A 2 0.455 0.558 0.646 0.489 0.353 0.269 11.6SV A 3 0.424 0.455 0.678 0.535 0.343 0.31 12.9SV A 4 0.411 0.418 0.625 0.489 0.256 0.226 16.4IOI 0.643 1.0 1.0 0.829 0.131 0.239 14.9\nTable 8: MRR Alignment of different explanation meth-\nods on GPT-2 XL predictions on every dataset. The\naverage distance to the linguistic evidence tokens is\nshown in the last column.\nD Model Predictions\nFigure 12: Logit difference between the acceptable and\nthe unacceptable predictions of a GPT-2 Small on every\ndataset.\nFigure 13: Logit difference between the acceptable and\nthe unacceptable predictions of a GPT-2 XL on every\ndataset.\nE MRR Alignment across layers\nFigure 14: ALTI-Logit MRR alignment scores across\nlayers on Anaphor Agreement datasets (GPT-2 Small).\nFigure 15: ALTI-Logit MRR alignment scores across\nlayers on Anaphor Agreement datasets (GPT-2 XL).\nFigure 16: ALTI-Logit MRR alignment scores across\nlayers on Determiner-Noun Agreement datasets (GPT-2\nSmall).\n5502\nFigure 17: ALTI-Logit MRR alignment scores across\nlayers on Determiner-Noun Agreement datasets (GPT-2\nXL).\nF MLPs Logit Difference Update\nFigure 18: MLPs update to the logit difference\n∆logitl\n(w−f)←MLPl across layers (GPT-2 Small).\n5503\nG Self-attention Logit Difference Update\nFigure 19: Self-attention update to the logit difference\n∆logitl\n(w−f)←Self-attnl across layers (GPT-2 Small).\nH Qualitative Contrastive Exaplantions\nH.1 Explanations of Different Contrastive\nMethods\nLogit\na 2006guidetothechurchesofangleseysays\nALTI-Logit\na 2006guidetothechurchesofangleseysays\nErasure\na 2006guidetothechurchesofangleseysays\nGrad Norm\na 2006guidetothechurchesofangleseysays\nG×I\na 2006guidetothechurchesofangleseysays\nTable 9: Comparison of different contrastive explana-\ntions on a GPT-2 Small SVA example (why says instead\nof say).\nLogit\nDiane should complain about these unconvinced drivers\nALTI-Logit\nDiane should complain about these unconvinced drivers\nErasure\nDiane should complain about these unconvinced drivers\nGrad Norm\nDiane should complain about these unconvinced drivers\nG×I\nDiane should complain about these unconvinced drivers\nTable 10: Comparison of different contrastive explana-\ntions on a GPT-2 Small dnaa example (why drivers\ninstead of driver).\nLogit\nAmanda isn’t respected by the children\nALTI-Logit\nAmanda isn’t respected by the children\nErasure\nAmanda isn’t respected by the children\nGrad Norm\nAmanda isn’t respected by the children\nG×I\nAmanda isn’t respected by the children\nTable 11: Comparison of different contrastive explana-\ntions on a GPT-2 Small asp example (why children\ninstead of cups).\n5504\nH.2 GPT-2 XL ALTI-Logit across layers\nL48 |AreportabouttheImpressionistshas\nL47 |AreportabouttheImpressionistshas\nL46 |AreportabouttheImpressionistshas\nL45 |AreportabouttheImpressionistshas\nL44 |AreportabouttheImpressionistshas\nL43 |AreportabouttheImpressionistshas\nL42 |AreportabouttheImpressionistshas\nL41 |AreportabouttheImpressionistshas\nL40 |AreportabouttheImpressionistshas\nL39 |AreportabouttheImpressionistshas\nL38 |AreportabouttheImpressionistshas\nL37 |AreportabouttheImpressionistshas\nL36 |AreportabouttheImpressionistshas\nL35 |AreportabouttheImpressionistshas\nL34 |AreportabouttheImpressionistshas\nL33 |AreportabouttheImpressionistshas\nL32 |AreportabouttheImpressionistshas\nL31 |AreportabouttheImpressionistshas\nL30 |AreportabouttheImpressionistshas\nL29 |AreportabouttheImpressionistshas\nL28 |AreportabouttheImpressionistshas\nL27 |AreportabouttheImpressionistshas\nL26 |AreportabouttheImpressionistshas\nL25 |AreportabouttheImpressionistshas\nL24 |AreportabouttheImpressionistshas\nL23 |AreportabouttheImpressionistshas\nL22 |AreportabouttheImpressionistshas\nL21 |AreportabouttheImpressionistshas\nL20 |AreportabouttheImpressionistshas\nL19 |AreportabouttheImpressionistshas\nL18 |AreportabouttheImpressionistshas\nL17 |AreportabouttheImpressionistshas\nL16 |AreportabouttheImpressionistshas\nL15 |AreportabouttheImpressionistshas\nL14 |AreportabouttheImpressionistshas\nL13 |AreportabouttheImpressionistshas\nL12 |AreportabouttheImpressionistshas\nL11 |AreportabouttheImpressionistshas\nL10 |AreportabouttheImpressionistshas\nL9 | AreportabouttheImpressionistshas\nL8 | AreportabouttheImpressionistshas\nL7 | AreportabouttheImpressionistshas\nL6 | AreportabouttheImpressionistshas\nL5 | AreportabouttheImpressionistshas\nL4 | AreportabouttheImpressionistshas\nL3 | AreportabouttheImpressionistshas\nL2 | AreportabouttheImpressionistshas\nL1 | AreportabouttheImpressionistshas\nTable 12: GPT-2 XL darn (why has instead of have).\nL48 |Katherinecan’thelpherself\nL47 |Katherinecan’thelpherself\nL46 |Katherinecan’thelpherself\nL45 |Katherinecan’thelpherself\nL44 |Katherinecan’thelpherself\nL43 |Katherinecan’thelpherself\nL42 |Katherinecan’thelpherself\nL41 |Katherinecan’thelpherself\nL40 |Katherinecan’thelpherself\nL39 |Katherinecan’thelpherself\nL38 |Katherinecan’thelpherself\nL37 |Katherinecan’thelpherself\nL36 |Katherinecan’thelpherself\nL35 |Katherinecan’thelpherself\nL34 |Katherinecan’thelpherself\nL33 |Katherinecan’thelpherself\nL32 |Katherinecan’thelpherself\nL31 |Katherinecan’thelpherself\nL30 |Katherinecan’thelpherself\nL29 |Katherinecan’thelpherself\nL28 |Katherinecan’thelpherself\nL27 |Katherinecan’thelpherself\nL26 |Katherinecan’thelpherself\nL25 |Katherinecan’thelpherself\nL24 |Katherinecan’thelpherself\nL23 |Katherinecan’thelpherself\nL22 |Katherinecan’thelpherself\nL21 |Katherinecan’thelpherself\nL20 |Katherinecan’thelpherself\nL19 |Katherinecan’thelpherself\nL18 |Katherinecan’thelpherself\nL17 |Katherinecan’thelpherself\nL16 |Katherinecan’thelpherself\nL15 |Katherinecan’thelpherself\nL14 |Katherinecan’thelpherself\nL13 |Katherinecan’thelpherself\nL12 |Katherinecan’thelpherself\nL11 |Katherinecan’thelpherself\nL10 |Katherinecan’thelpherself\nL9 | Katherinecan’thelpherself\nL8 | Katherinecan’thelpherself\nL7 | Katherinecan’thelpherself\nL6 | Katherinecan’thelpherself\nL5 | Katherinecan’thelpherself\nL4 | Katherinecan’thelpherself\nL3 | Katherinecan’thelpherself\nL2 | Katherinecan’thelpherself\nL1 | Katherinecan’thelpherself\nTable 13: GPT-2 XL aga (why herself instead of him-\nself).\n5505\nH.3 ALTI-Logit (IOI) across Models\nL12 | </s> When Paulaand Marthagot a coconutat the zoo, Marthadecidedto give the coconutto Paula\nL11 | </s> When Paulaand Marthagot a coconutat the zoo, Marthadecidedto give the coconutto Paula\nL10 | </s> When Paulaand Marthagot a coconutat the zoo, Marthadecidedto give the coconutto Paula\nL9 | </s> When Paulaand Marthagot a coconutat the zoo, Marthadecidedto give the coconutto Paula\nL8 | </s> When Paulaand Marthagot a coconutat the zoo, Marthadecidedto give the coconutto Paula\nL7 | </s> When Paulaand Marthagot a coconutat the zoo, Marthadecidedto give the coconutto Paula\nL6 | </s> When Paulaand Marthagot a coconutat the zoo, Marthadecidedto give the coconutto Paula\nL5 | </s> When Paulaand Marthagot a coconutat the zoo, Marthadecidedto give the coconutto Paula\nL4 | </s> When Paulaand Marthagot a coconutat the zoo, Marthadecidedto give the coconutto Paula\nL3 | </s> When Paulaand Marthagot a coconutat the zoo, Marthadecidedto give the coconutto Paula\nL2 | </s> When Paulaand Marthagot a coconutat the zoo, Marthadecidedto give the coconutto Paula\nL1 | </s> When Paulaand Marthagot a coconutat the zoo, Marthadecidedto give the coconutto Paula\nFigure 20: OPT 125M IOI (why Paula instead of Martha).\n5506\nL24 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL23 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL22 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL21 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL20 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL19 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL18 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL17 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL16 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL15 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL14 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL13 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL12 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL11 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL10 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL9 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL8 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL7 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL6 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL5 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL4 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL3 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL2 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL1 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nTable 14: BLOOM 560M IOI (why Paula instead of Martha).\n5507\nL24 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL23 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL22 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL21 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL20 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL19 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL18 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL17 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL16 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL15 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL14 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL13 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL12 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL11 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL10 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL9 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL8 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL7 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL6 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL5 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL4 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL3 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL2 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL1 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nTable 15: BLOOM 1B1 IOI (why Paula instead of Martha).\n5508\nL12 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL11 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL10 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL9 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL8 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL7 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL6 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL5 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL4 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL3 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL2 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL1 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nTable 16: GPT-2 Small IOI (why Paula instead of Martha).\n5509\nL36 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL35 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL34 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL33 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL32 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL31 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL30 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL29 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL28 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL27 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL26 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL25 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL24 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL23 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL22 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL21 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL20 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL19 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL18 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL17 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL16 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL15 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL14 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL13 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL12 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL11 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL10 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL9 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL8 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL7 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL6 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL5 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL4 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL3 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL2 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nL1 | When Paula and Martha got a coconut at the zoo, Martha decided to give the coconut to Paula\nTable 17: GPT-2 LargeIOI (why Paula instead of Martha).\n5510\nL48 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL47 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL46 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL45 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL44 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL43 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL42 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL41 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL40 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL39 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL38 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL37 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL36 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL35 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL34 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL33 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL32 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL31 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL30 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL29 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL28 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL27 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL26 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL25 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL24 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL23 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL22 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL21 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL20 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL19 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL18 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL17 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL16 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL15 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL14 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL13 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL12 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL11 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL10 |WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL9 | WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL8 | WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL7 | WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL6 | WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL5 | WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL4 | WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL3 | WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL2 | WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nL1 | WhenPaulaandMarthagota coconutat thezoo,Marthadecidedto givethecoconutto Paula\nTable 18: GPT-2 XL IOI (why Paula instead of Martha).\n5511\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 9\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection 9\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 2\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 2\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 9\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 3\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 3\nC □\u0013 Did you run computational experiments?\nSection 3 and 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 3\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n5512\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNot applicable. Left blank.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 3\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 3\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n5513"
}