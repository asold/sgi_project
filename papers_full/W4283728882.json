{
    "title": "SatFormer: Saliency-Guided Abnormality-Aware Transformer for Retinal Disease Classification in Fundus Image",
    "url": "https://openalex.org/W4283728882",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3022266348",
            "name": "Yankai Jiang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2100549106",
            "name": "Ke Xu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2119606313",
            "name": "Xinyue Wang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2103884630",
            "name": "Yuan Li",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2123238028",
            "name": "Hongguang Cui",
            "affiliations": [
                "First Affiliated Hospital Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2158146757",
            "name": "Yubo Tao",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2102167785",
            "name": "Hai Lin",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4214709605",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W2115531174",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2983395335",
        "https://openalex.org/W3203111947",
        "https://openalex.org/W3175515048",
        "https://openalex.org/W4214493665",
        "https://openalex.org/W2741346289",
        "https://openalex.org/W3156811085",
        "https://openalex.org/W3204146347",
        "https://openalex.org/W3166225737",
        "https://openalex.org/W3128220181",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4389977645",
        "https://openalex.org/W3167044851",
        "https://openalex.org/W2889859380",
        "https://openalex.org/W3157528469",
        "https://openalex.org/W4293680532"
    ],
    "abstract": "Automatic and accurate retinal disease diagnosis is critical to guide proper therapy and prevent potential vision loss. Previous works simply exploit the most discriminative features while ignoring the pathological visual clues of scattered subtle lesions. Therefore, without a comprehensive understanding of features from different lesion regions, they are vulnerable to noise from complex backgrounds and suffer from misclassification failures. In this paper, we address these limitations with a novel saliency-guided abnormality-aware transformer which explicitly captures the correlation between different lesion features from a global perspective with enhanced pathological semantics. The model has several merits. First, we propose a saliency enhancement module (SEM) which adaptively integrates disease related semantics and highlights potentially salient lesion regions. Second, to the best of our knowledge, this is the first work to explore comprehensive lesion feature dependencies via a tailored efficient self-attention. Third, with the saliency enhancement module and abnormality-aware attention, we propose a new variant of Vision Transformer models, called SatFormer, which outperforms the state-of-the-art methods on two public retinal disease classification benchmarks. Ablation study shows that the proposed components can be easily embedded into any Vision Transformers via a plug-and-play manner and effectively boost the performance.",
    "full_text": "SatFormer: Saliency-Guided Abnormality-Aware\nTransformer for Retinal Disease Classiﬁcation in Fundus Image\nYankai Jiang1 , Ke Xu1 , Xinyue Wang1 , Yuan Li1 , Hongguang Cui2 ,\nYubo Tao1\u0003 and Hai Lin1\u0003\n1State Key Laboratory of CAD&CG, College of Computer Science and Technology,\nZhejiang University, Hangzhou, China\n2The First Afﬁliated Hospital, Zhejiang University School of Medicine, Hangzhou, China\n{jyk1996ver, 3180103434, xinyuewang, yuanli, 1189002}@zju.edu.cn,\n{taoyubo, lin}@cad.zju.edu.cn\nAbstract\nAutomatic and accurate retinal disease diagnosis\nis critical to guide proper therapy and prevent po-\ntential vision loss. Previous works simply exploit\nthe most discriminative features while ignoring the\npathological visual clues of scattered subtle lesions.\nTherefore, without a comprehensive understanding\nof features from different lesion regions, they are\nvulnerable to noise from complex backgrounds and\nsuffer from misclassiﬁcation failures. In this paper,\nwe address these limitations with a novel saliency-\nguided abnormality-aware transformer which ex-\nplicitly captures the correlation between different\nlesion features from a global perspective with en-\nhanced pathological semantics. The model has sev-\neral merits. First, we propose a saliency enhance-\nment module (SEM) which adaptively integrates\ndisease related semantics and highlights potentially\nsalient lesion regions. Second, to the best of our\nknowledge, this is the ﬁrst work to explore compre-\nhensive lesion feature dependencies via a tailored\nefﬁcient self-attention. Third, with the saliency en-\nhancement module and abnormality-aware atten-\ntion, we propose a new variant of Vision Trans-\nformer models, called SatFormer, which outper-\nforms the state-of-the-art methods on two public\nretinal disease classiﬁcation benchmarks. Ablation\nstudy shows that the proposed components can be\neasily embedded into any Vision Transformers via\na plug-and-play manner and effectively boost the\nperformance.\n1 Introduction\nRetinal diseases are the leading cause of vision impair-\nment and irreversible blindness. Several typical retinal dis-\neases, such as diabetic retinopathy (DR), glaucoma, age-\nrelated macular degeneration (AMD) and retinal vein occlu-\nsion (RVO), are becoming common in the working-age pop-\nulation worldwide [Bourne et al., 2017]. Early screening and\n\u0003The corresponding author.\ntimely proper therapy is crucial to prevent disease progres-\nsion and potential vision loss. In clinical routines, ophthal-\nmologists identify retinal diseases in fundus images based on\nthe type and number of associated lesion symptoms, such as\nmicroaneurysms, haemorrhages, soft exudates and hard ex-\nudates [Li et al., 2019; Sun et al., 2021]. However, these\npathological abnormal regions in fundus images are usually\nsmall in size and scattered over the entire retina, which makes\nthe diagnosis difﬁcult. Therefore, there is a strong demand\nfor fully automatic and accurate retinal disease recognition in\nfundus images.\nConvolutional neural networks (CNNs) based methods\nhave achieved signiﬁcant success in retinal disease diagno-\nsis tasks [Wang et al., 2017; Bi et al., 2021 ]. However,\nthe repeated combination of pooling and down-sampling lay-\ners in CNN models inherently results in the loss of detailed\ninformation of subtle lesions, which limits the representa-\ntion of diagnostic features in scattered small pathological re-\ngions. In the ophthalmology ﬁeld, the occurred lesions al-\nways determine the diagnostic results of the speciﬁc diseases.\nTherefore, the relatively weak ability of CNNs to retain ﬁne-\ngrained lesion semantics may impose more difﬁculties for\ndiscovering diagnostic visual clues related to early symptoms.\nTo alleviate this limitation, several recent methods [Li et\nal., 2019; Sun et al., 2021] resort to adopt attention mecha-\nnisms or even transformers to explore long-range dependen-\ncies and keep more detailed information. But most of them\nstill adopt convnets as main bodies and treat transformers as\nassisted modules to help encode global context into convo-\nlutional representations. As a result, these models still pre-\nfer large continuous areas and are difﬁcult to extract multiple\ndiversiﬁed discriminative small parts. In order to fully uti-\nlize the inherent encoding capabilities of transformers, Yu et\nal. [2021] explore the applicability of Vision Transformer for\nthe retinal disease classiﬁcation task. However, this method\ntakes all individual patches into consideration without high-\nlighting semantics of small lesions, which results in a strong\nbias on the most salient lesion regions while ignoring trivial\ndetail information contained in the subtle phenotypes. Un-\nfortunately, such characteristic may impair the performance\nof retinal diseases recognition. More speciﬁcally, the chal-\nlenge that limits the adoption of a computer-aided diagnosis\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n987\ntool by the ophthalmologist is, some disease related subtle\npathologies such as microaneurysms and exudates are usually\nignored. Thus, it is vital to enhance the feature representation\nof small lesions and gain a comprehensive understanding of\nfeatures from diverse lesion regions for retinal diseases recog-\nnition.\nBased on the above discussions, the limitations of existing\nmethods motivate us to design speciﬁc architectures to tackle\nthe above challenges. The main contributions of this paper\nare summarized as follows.\n•We are the ﬁrst to explore an efﬁcient Vision Trans-\nformer which adaptively exploits diverse lesion features\nfrom a global perspective with enhanced pathological\nsemantics for retinal disease classiﬁcation, where a com-\nprehensive understanding of different lesions is particu-\nlarly important.\n•We design a saliency enhancement module to mine dis-\ncriminative lesion semantics and enhance the feature\nrepresentation of small lesion regions.\n•We propose an abnormality-aware attention which ex-\nplicitly facilitates the dependencies between different le-\nsion regions to drive the model to distinguish the main\nabnormal regions as well as the small subtle lesions such\nas microaneurysms and exudates with remarkably re-\nduced computation complexity.\nWe conduct comprehensive experiments to demonstrate the\neffectiveness of each proposed components. Our method sig-\nniﬁcantly outperforms prior state-of-the-art methods on mul-\ntiple datasets.\n2 Related Works\nIn this section, we give a brief review of retinal disease as-\nsessment based on CNNs and Vision Transformers.\nCNN-based methods. In the past decade, convolutional\nneural networks serve as the main design paradigm for reti-\nnal disease screening tasks. For instance, Lin et al. [2018]\nand Li et al.[2019] utilized CNN architectures for DR grad-\ning. Phene et al.[2019] developed methods based on CNNs\nfor glaucoma diagnosis. Though these methods have made\nsome progress, they are still limited by insufﬁcient represen-\ntation for the tiny and subtle lesion regions in deep layers due\nto severe loss of spatial details caused by down-sampling op-\nerations (pooling). As a result, these methods ignore trivial\nlesion information. Moreover, CNN-based models inherently\nlack the ability to explicitly capture long-range feature de-\npendencies between different abnormal symptoms in a global\nscope, which would impair the performance of disease diag-\nnosis.\nTransformer-based methods. ViT [Dosovitskiy et al.,\n2020] ﬁrst proves that a pure transformer can achieve state-\nof-the-art performance in image classiﬁcation with sufﬁ-\ncient training data. For retinal disease classiﬁcation, Yu et\nal. [2021] introduce a multiple instance learning head on ViT\nto fully exploit the features extracted from individual patches.\nHowever, this method directly utilizes ViT without explicitly\nexploiting the complex dependencies among diverse lesions.\nDifferent from natural images, the diagnostic features in a\nfundus image, usually only occupy a small part of the whole\nimage. Thus modeling long-range dependencies across all\npatches is inefﬁcient and results in less discriminative feature\nrepresentations for subtle lesion regions. LAT [Sun et al.,\n2021] alleviates this limitation by introducing an encoder-\ndecoder transformer framework to learn lesion-aware ﬁlters\nfor DR grading. Nonetheless, this method still employs con-\nvnets as main feature extractors, on top of which a trans-\nformer is further applied to exploit pixel relation. Since spa-\ntial information and ﬁne-grained details may have been lost\nin deep convolution layers due to down-sampling, the advan-\ntages of a transformer is not fully exploited. So far, how to\nexploit pathological features of retinal diseases distributed at\ndifferent positions and build an optimal network structure still\nremains an open question.\n3 Methodology\nThe architecture of SatFormer is illustrated in Figure 1. Sim-\nilar to [Liu et al., 2021; Wang et al., 2021b ], SatFormer\nalso produces a hierarchical representation, which has four\nstages. Each stage consists of a saliency enhancement mod-\nule (SEM) and Li sequential SatFormer blocks. A SEM\ngradually aggregates lesion pixels and highlights potentially\nsalient lesion regions to generate expressive multi-scale em-\nbeddings. Then, several SatFormer blocks, each of which\ninvolves abnormality-aware attention, are set up after SEM.\nThe image classiﬁcation is performed by applying a global\naverage pooling layer on the output feature maps of the last\nstage, followed by a linear classiﬁer.\n3.1 Saliency Enhancement Module\nFor retinal diseases, it is challenging to distinguish the scat-\ntered subtle lesion regions since pure transformer architec-\ntures lack the inductive bias, such as locality and transla-\ntion equivariance. Moreover, the coarse splitting of patches\nlimits the ability to model details within each patch [Xu et\nal., 2021 ]. Such deteriorated local details compromise the\ndiscovery of inconspicuous lesion information. Some recent\nmethods [Li et al., 2021; Wu et al., 2021a] incorporate with\nconvolution operations to bring locality to Vision Transform-\ners. However, without highlighting potentially salient regions\nfrom a global perspective, these hybrid models tend to be bi-\nased towards the most discriminative lesion regions, while ig-\nnoring the diversity of lesion information. Unfortunately, the\ntrivial or less discriminative lesion regions contained in a fun-\ndus image may be important for disease recognition.\nTo overcome the above limitations, we propose a saliency\nenhancement module (SEM) to mine more explicit lesion\nsemantics and enhance feature activations corresponding to\nscattered small abnormal regions at each stage. In this\nprocess, SEM reduces the number of embeddings and in-\ncreases their dimensions. The generated feature maps have\nH\n2i+1 × W\n2i+1 patch tokens for the ithstage.\nLocality Enhanced Multi-Scale Embedding. Figure 2 il-\nlustrates how the SEM generates patch embeddings in Stage\n1. We ﬁrst use four convolution kernels with different sizes to\nsample patches in an input image. Considering that lesion re-\ngions usually exhibit signiﬁcant variations in shape, size, and\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n988\nFigure 1: The architecture of SatFormer. The input image size is H \u0002 W \u0002 3, and the size of feature maps in each stage is shown at the\nbottom. Stage i consists of a SEM and Li SatFormer blocks.\nstructure, distinguishing these abnormalities from a variety\nof different scales helps to enhance the ability for mining the\ndiscriminative visual clues related to each lesion. We keep the\nstride (4 ×4) of four kernels the same to make sure they gen-\nerate the same number of embeddings. Then the four patches\nwill be projected and concatenated as one embedding.\nTo control the total computational budget, we use a lower\ndimension dfor large kernels and successively doubles it for\nsmall kernels. This scheme saves a lot of computational cost\nwithout signiﬁcant performance drop. In Stage 2 −4, the\nstrides are set as 2×2 while kernels are set as3×3 and 5×5.\nSince each token embedding already learns information from\nregions of different scales, our strategy empowers the model\nwith enriched multi-scale local context modeling capabilities.\nSpatial Saliency Enhancement. After the embedding pro-\ncess, SEM performs the following procedures. First, given\npatch embeddings Xp\ne ∈RHi\u0002Wi\u0002Ci , a 1 ×1 convolution is\nconducted to expand the embeddings to a higher dimension\nof Xd\ne ∈RHi\u0002Wi\u0002(2\u0002\u000b\u0002Ci), where \u000b is the expand ratio,\nHi, Wi and Ci denote the height, width and channel dimen-\nsions of the input in stage i, respectively. A key ingredient\nin SEM is the spatial perception layer which adaptively en-\nhances the response of lesions and suppresses cluttered back-\nground features. Speciﬁcally, we split Xd\ne into two indepen-\ndent parts (Xd1\ne ;Xd2\ne ) along the channel dimension. A spa-\ntial saliency distribution map S ∈RHi\u0002Wi\u0002(\u000b\u0002Ci) is gen-\nerated through feeding Xd2\ne to a depthwise convolution layer\nfollowed by one GELU and Softmax. In experiments, we\nfound that adding GELU before Softmax, especially in shal-\nlow layers, helps stabilize the training process. Our purpose\nis to generate a spatial weight distribution matrix to high-\nlight the potential salient regions and suppress noisy back-\ngrounds. Therefore, the Softmax normalizes the feature maps\nalong the spatial dimension. For each small lesion, its feature\nactivation on the spatial saliency distribution map S is ex-\npected to be higher than that of its surrounding backgrounds.\nGuided by S, the model learns the context of pathological se-\nmantics from a global perspective and highlights potentially\nsalient lesion regions on the entire feature map scale. Fur-\nFigure 2: Illustration of the saliency enhancement module (SEM).\nThe input image is sampled by four different kernels with same\nstride. Each patch token is constructed by concatenating embed-\ndings of the four patches. Then we feed patch tokens into the spatial\nperception layer. S denotes the spatial saliency distribution map.\nthermore, Xd1\ne and S are combined by element-wise multi-\nplication, generating Xs\ne ∈RHi\u0002Wi\u0002(\u000b\u0002Ci), which encodes\nsalient semantics of abnormalities scattered over the entire\nretina. This procedure can be noted as:\nS = Softmax(GELU(DWConv(Xd2\ne )) (1)\nXs\ne = S⊙Xd1\ne : (2)\nFinally, we project Xs\ne to the initial dimension and then add\nthe output with Xp\ne through a residual connection to obtain\nthe output patch embeddings Xo\ne ∈ RHi\u0002Wi\u0002Ci . A Lay-\nerNorm (LN) layer and a GELU is applied following each\nlinear projection and convolution. In this way, SEM empow-\ners the model with enriched feature representations for small\nlesions and captures diverse pathological semantics.\n3.2 Abnormality-Aware Attention Mechanism\nSelf-attention [Vaswani et al., 2017] has advantages in cap-\nturing long-range dependencies, but it incurs huge memory\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n989\nFigure 3: Abnormality-aware attention module. We ﬁrst calculate\nthe group embedding for each patch token. Then we attach position\nembeddings to embedded patches and group them according to the\ngroup embeddings. Only the tokens in the lesion group are updated.\nAfter that, the token sequence is restored and the group embeddings\nare removed.\nand computation costs. Some methods, e.g., PVT [Wang et\nal., 2021a], PVTv2 [Wang et al., 2022] and P2T [Wu et al.,\n2021b] adopt spatial-reduction (pooling operation) to down-\nsample the feature maps when computing keys and values in\nthe multi-head self-attention (MHSA). With the pooled fea-\ntures, they reduce the number of tokens but at the cost of\nlosing ﬁne-grained details. Other methods [Liu et al., 2021;\nWang et al., 2021b] resort to computing self-attention within\ndilated partitions of feature maps but sacriﬁce the direct\nglobal relationship modeling.\nThough the aforementioned methods have made some\nprogress, they still suffer from two drawbacks. (1) They fail\nto adaptively perceive and group speciﬁc tokens that contain\nthe discriminative features. (2) They lose the global receptive\nﬁeld and fail to gather complementary information from the\nless discriminative groups of tokens. As a result, they sacri-\nﬁce accuracy as a trade-off with efﬁciency. Moreover, mod-\nels without comprehensive information on diverse scattered\nlesions and the dependencies between different pathological\nfeatures tend to be biased on the most salient lesion but ig-\nnore the less discriminating lesions that may be important for\ndiagnosis.\nMotivated by the above limitations, we propose a novel\nabnormality-aware attention mechanism. Speciﬁcally, apart\nfrom the position embedding, we attach an additional group\nembedding to each patch token. This group embedding indi-\ncates whether the corresponding token encodes useful lesion\nsemantics. Generating group embeddings from a global view\nis straightforward. As shown in Figure 3, the patch embed-\ndings Xo\ne ∈ RHi\u0002Wi\u0002Ci from SEM are fed into a 1 ×1\nconvolutional layer with single channel output. In this way, a\ntoken relative importance matrix Dis generated via:\nD= Sigmoid(GELU(\u0012·Xo\ne + b)); (3)\nwhere \u0012and brepresent the weights and bias of the convolu-\ntional layer, respectively. Dn;m helps to describe the proba-\nbility that the patch at row nand column mcontains lesion\ninformation or pathological semantics associated with abnor-\nmalities. The group embeddings G are obtained by setting\nan threshold \" for D. For each patch, if its corresponding\nDn:m ≤\", then this patch will be assigned to the background\ngroup, otherwise it will be assigned to the lesion group. In\nthis way, G enables the model to adaptively perceive and\ngroup speciﬁc patch tokens that contain the discriminative\nfeatures.\nInstead of focusing on all input tokens, we propose to com-\npute self-attention within the lesion group for efﬁcient model-\ning. However, the patches in the background group may con-\ntain important complementary information that contributes to\na thorough understanding of lesions and their surroundings.\nThus we propose to add an extra cross-group attention to\nenable cross-group information exchange. As shown in Fig-\nure 3, we ﬂatten all input patch tokens and compute self-\nattention within the lesion group. In the lesion group, patch\ntokens Xl ∈RN2\u0002Ci are linearly transformed to three parts,\ni.e., queries Qs ∈RN2\u0002Ck , keys Ks ∈RN2\u0002Ck and values\nVs ∈RN2\u0002Cv where N2 is the sequence length of the lesion\ngroup, C;Ck;Cv are the dimensions of inputs, queries (keys)\nand values, respectively. The scaled dot-product attention is\napplied via:\nATTl(Xl) =softmax(QsKT\ns\n√Ck\n)Vs: (4)\nAs for the cross-group attention, the queriesQc ∈RN2\u0002Ck\ncome from the lesion group while keys Kc ∈RN1\u0002Ck and\nvalues Vc ∈RN1\u0002Cv come from the background group. This\nallows each patch in the lesion group to focus on the fea-\ntures of all patches Xb ∈RN1\u0002Ci in the background group to\ngather complementary information that is highly relevant to\nlesion diagnosis. The cross-group attention is applied via:\nCrossATTlb(Xl;Xb) =softmax(QcKT\nc\n√Ck\n)Vc: (5)\nWe then add the results of the two applied attention through\nan element-wise sum operation and employ a linear layer to\nproduce the output ^Xl ∈RN2\u0002Ci of the lesion group. Fi-\nnally, patches in the background group are concatenated with\n^Xl through a residual connection. We remove the group em-\nbeddings and restore the order of the patches according to\ntheir original positions. Once we obtain the output of the\nabnormality-aware attention module, we send it to the MLP\nblock for proceeding computation as usual. At the end of\neach stage, we restore patch tokens to “images” in the spa-\ntial dimension based on the original positions. The compu-\ntational cost of the abnormality-aware attention is reduced\nfrom O(N2) to O(N2\n2 + N1N2), and N2 ≪ N in most\ncases. Multi-head version of abnormality-aware attention can\nbe easily derived. Just as MHSA, we split the queries, keys\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n990\nand values to hparts and perform the abnormality-aware at-\ntention mechanism in parallel. Then we concatenate the out-\nput values of each head and linearly project them to form the\nﬁnal output.\nNotably, each abnormality-aware attention module com-\nputes its own group embeddings. This gives the model an\nopportunity to rethink how to group and make full use of\npatches containing lesion semantics to gather as much infor-\nmation as possible. The learning process of group embed-\ndings also beneﬁts from SEM. SEM highlights potentially\nsalient regions of small lesions and provides sufﬁcient con-\ntext to help the model perceive diverse lesions, which makes\nit easier and more meaningful to predict whether a patch con-\ntains pathological semantics.\nOur abnormality-aware attention introduces the inductive\nbias that the spatial interactions should be dynamically pa-\nrameterized based on the pathological semantics and the cor-\nrelations between different lesion features. This characteristic\nis conducive to the diagnosis of diseases, especially for reti-\nnal diseases, where pathological regions are often small and\nscattered in distribution.\n4 Experiments\n4.1 Dataset and Implementation\nWe conduct experiments on a large dataset of fundus im-\nages collected from a regional hospital and two public bench-\nmarks including EyePACS[Cuadros and Bresnick, 2009] and\nRFMiD [Pachade et al., 2021]. The collected dataset contains\n28;360 DR images, 5;816 glaucoma images, 4;805 AMD\nimages, 25;748 RVO images and35;369 normal images. We\nrandomly split 70% of the dataset for training, 10% for val-\nidation and the rest 20% for testing. We adopt the ten-fold\ncross-validation method on our dataset. EyePACS [Cuadros\nand Bresnick, 2009 ] contains 35;126 training, 10;906 val-\nidation and 42;670 testing DR images. Each image is di-\nvided into one of ﬁve DR grades. Retinal Fundus Multi-\ndisease Image Dataset (RFMiD) [Pachade et al., 2021] con-\ntains 1;920 training, 640 validation and 640 testing images,\nwhich screens retinal images into normal and abnormal (com-\nprising of 45 different types of diseases) categories.\nAll experiments are performed on 8 V100 GPUs. We em-\nploy an AdamW optimizer for 500 epochs using a cosine de-\ncay learning rate scheduler and 20 epochs of linear warm-up.\nA mini-batch size of 128, an initial learning rate of 0:001,\nand a weight decay of 0:05 are used. We use gradient clip-\nping with a max norm of 1:0 to stabilize the training process.\nAugmentations such as rotation, scaling, gaussian blur, color\njitting and mirroring are utilized during the training process.\nFor the collected dataset and EyePACS, as multi-class clas-\nsiﬁcation problems, we utilize evaluation metrics of accu-\nracy, area under the curve (AUC), weighted F1 and weighted\nKappa following [Yu et al., 2021; Sunet al., 2021].\nFor RFMiD, which contains only two classes of either dis-\nease or normal, the AUC, accuracy and F1 metric are used.\nFor fair comparison, we reap the beneﬁt of pre-trained\nweights of MLP layers and QKV attention on ImageNet\npre-training. Concretely, we align channel numbers of Sat-\nFormer blocks to those of pre-trained models so that we\nload the weights of MLP layers and QKV attention. In\neach abnormality-aware attention layer, we reuse the pre-\ntrained weights of QsKsVs to initialize QcKcVc. Similar to\nViT [Dosovitskiy et al., 2020] and Swin [Liu et al., 2021],\nwe build two architecture variants. The hyper-parameters of\nthese model variants are:\n•SatFormer-S: C = 96, block numbers = {2;2;8;2},\nH = {3;6;12;12}, d = {16;64;128;256}, \" = 0:5,\n\u000b= {2;2;1;1}\n•SatFormer-B: C = 128, block numbers = {2;2;18;2},\nH = {3;6;12;24}, d = {16;64;128;256}, \" = 0:5,\n\u000b= {2;2;1;1}\nwhere Cand Hmean embedding dimensions and the number\nof heads in the multi-head self-attention, respectively. dand\n\u000bare the kernel dimension and expansion ratio in the SEM,\nrespectively. \" is the threshold for grouping. -S and -B for\nsmall and base, respectively.\n4.2 Comparisons with State-of-the-art Methods\nWe compare our method with the state-of-the-arts including\nrepresentative CNN-based models, transformer-based mod-\nels and hybrid architectures. The results are shown in Ta-\nble 1. SatFormer noticeably surpasses the other state-of-the-\nart structures.\nCompared against strong Transformer based baselines ViT-\nL/16, PVTv2-B5, P2T-Large, Swin-L, and Twins-SVT-L,\nSatFormer-B outperforms them at least absolute 2.3% in the\nweighted Kappa on EyePACS, and 4.2% in F1 on RFMiD.\nFurther, even our SatFormer-S achieves at least 2% improve-\nment of F1 relative to these large models with signiﬁcantly\nlower paramerters. Notably, as for RFMiD dataset, the\nAUC and F1 score of SatFormer-B surpass that of MIL-VT,\nwhich is pre-trained on a large fundus dataset, with much\nlower FLOPs (65.0G vs. 200.0G). This success has been at-\ntributed to the proposed abnormality-aware attention module,\nin which the weights are dynamically computed based on the\nsimilarity or afﬁnity between every pair of tokens in the lesion\ngroup. As a consequence, the captured global dependencies\nare potentially more efﬁcient and discriminative than interac-\ntions built on all patch tokens.\nCompared with the state-of-the-art ConvNets, i.e., Efﬁ-\ncientNet and ReXNet, the SatFormer-B achieves at least ab-\nsolute 1.8% improvement of Kappa on EyePACS and 2.5%\nimprovement of F1 on RFMiD. Moreover, our smallest model\nSatFormer-S with 27M parameters and 22.5G FLOPs sur-\npasses the CANet by 3.1% of Kappa on EyePACS and 4.5%\nof F1 score on RFMiD, while CANet has 3 times the FLOPs\nof SatFormer-S. This effectiveness can be explained that our\napproach is capable of both perceiving locally low contrast\nsmall-sized lesions and capturing dependencies between scat-\ntered pathologies globally, while CNN-based models suffer\nfrom spatial detail loss and lack the ability to model interac-\ntions between lesion features.\nCompared to other hybrid models, SatFormer signiﬁcantly\noutperforms the other SOTA models for all the three datasets.\nSatFormer-S achieves better performance compared with\nCvT-21 and CrossFormer-L with fewer parameters and simi-\nlar FLOPs. Our SatFormer-B surpasses other hybrid architec-\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n991\nMethod T\nype Network Image Size #param. (M) FLOPs (G) Collected Dataset EyePACS RFMiD\nAccuracy\nAUC F1-score Kappa Accuracy AUC F1-score Kappa Accuracy AUC F1-score\nConv\nolutional Networks\nAFN [Lin et al., 2018] 224×224 - - - - - - - - - 85.9 - - -\nCANet (ResNet50) [Li et al., 2019] 224×224 29 66.0 78.5 88.1 77.1 84.3 81.2 90.5 79.3 86.3 88.3 91.0 90.4\nEffNet-B7 [Tan and Le, 2019] 600×600 66 37.0 81.1 90.3 80.4 85.8 80.4 89.8 78.7 86.0 88.2 91.0 90.7\nReXNet (×3:0) [Han et al., 2021] 224×224 34 44.5 85.1 95.7 83.5 89.2 86.1 95.7 86.2 89.0 91.3 94.5 93.3\nPure T\nransformers\nViT-B/16[Dosovitskiy et al., 2020] 384×384 86 55.5 82.3 91.5 80.1 86.3 83.5 94.1 82.2 87.7 87.1 92.4 90.3\nViT-L/16[Dosovitskiy et al., 2020] 384×384 307 190.7 83.0 91.5 80.9 86.7 84.1 94.2 83.0 87.9 86.4 91.5 90.1\nSwin-B [Liu et al., 2021] 384×384 88 47.0 83.1 92.2 81.5 87.4 84.6 95.6 83.5 88.2 88.3 93.2 91.1\nSwin-L [Liu et al., 2021] 384×384 197 103.9 83.9 92.8 82.3 88.2 85.0 96.0 84.0 88.5 89.5 93.8 91.6\nTwins-SVT-B[Chu et al., 2021] 224×224 56 8.6 82.1 92.5 82.0 87.9 84.5 93.9 82.8 86.4 86.3 91.7 89.4\nTwins-SVT-L[Chu et al., 2021] 224×224 99 16.0 84.1 93.1 82.4 88.4 84.7 94.1 83.0 86.5 86.5 92.0 90.3\nPVTv2-B5 [Wang et al., 2022] 224×224 45 6.9 81.9 90.7 80.0 86.1 82.4 93.4 81.3 85.9 90.1 92.4 90.9\nP2T-Large [Wu et al., 2021b] 224×224 55 9.8 83.5 92.4 81.7 87.5 84.8 95.7 83.6 88.3 88.7 93.4 91.2\nMIL-VT [Yu et al., 2021] 384×384 98 200.0 83.0 91.0 80.6 86.7 84.2 94.7 83.3 87.8 91.1 95.9 94.4\nHybrid Architectures\nLA\nT [Sun et al., 2021] 512×512 - - - - - - - - - 88.4 - - -\nLocalViT-PVT[Li et al., 2021] 224×224 14 4.8 83.1 91.6 80.9 86.9 84.2 94.1 83.4 87.8 90.4 92.5 91.7\nCrossFormer-L [Wang et al., 2021b] 224×224 92 16.1 83.8 93.0 82.4 88.3 84.0 93.7 83.2 87.5 90.6 94.3 92.0\nCvT-21 [Wu et al., 2021a] 224×224 32 25.0 83.3 91.6 81.2 87.5 82.2 90.7 80.1 86.7 87.5 91.8 90.6\nOurs: SatFormer\n-S 224×224 27 22.5 85.2 96.2 83.8 89.4 86.7 96.2 86.0 89.4 92.2 95.4 94.9\nOurs: SatFormer-B 224×224 78 65.0 87.4 97.4 86.0 92.2 88.9 97.7 87.5 90.8 93.8 96.5 95.8\nTable 1: Comparison with state-of-the-art methods on our collected dataset and two benchmark datasets. Most models are initialized using\nthe ImageNet pre-trained weights, while MIL-VT uses pre-trained weights on a large fundus dataset. Results of AFN and LAT are drawn\nfrom original papers.\nModel #param.\n(M)\nFLOPs\n(G)\nEyePA\nCS\nKappa\nRFMiD\nF1-score\nSwin-B (\nw/. vs. w/o.) 104 vs. 88 62.6 vs. 47.0 88.7 vs. 88.2 92.7 vs. 91.1\nSwin-L (w/. vs. w/o.) 223 vs. 197 131.2 vs. 103.9 88.9 vs. 88.5 93.3 vs. 91.6\nCrossFormer-L (w/. vs. w/o.) 118 vs. 92 37.4 vs. 16.1 88.1 vs. 87.5 93.1 vs. 92.0\nSatFormer-S (w/. vs. w/o.) 27 vs. 25 22.5 vs. 19.1 89.4 vs. 88.8 94.9 vs. 93.7\nSatFormer-B (w/. vs. w/o.) 78 vs. 62 65.0 vs. 49.4 90.8 vs. 90.3 95.8 vs. 94.5\nTable 2: Ablation study of the SEM on the two public benchmarks.\nWe apply the SEM on different vision transformer architectures.\n“w/. vs. w/o.” denotes the comparison between using SEM or with-\nout using SEM.\ntures by a large margin in terms of all evaluation metrics with\nacceptable parameters and FLOPs. Different from previous\nhybrid models, which either simply treat transformers as as-\nsisted modules or compute self-attention within dilated par-\ntitions of feature maps, our SatFormer adaptively perceives\nand groups speciﬁc tokens to explicitly model the correlation\nbetween different lesions, resulting in a more comprehensive\nunderstanding of pathological clues.\n4.3 Ablation Study\nEffectiveness of the Saliency Enhancement Module. Ta-\nble 2 shows the ablation study on the effectiveness of the\nSEM. All models achieve more than 1.0% absolute perfor-\nmance gain on the RFMiD dataset by adopting our SEM. This\nshows applying our SEM yields signiﬁcant improvements on\nvarious architectures without too much additional computa-\ntional cost. Moreover, we compare the spatial perception\nlayer in SEM with several other close alternatives which also\ncompute a spatial weight distribution matrix. The results in\nTable 3 show that our spatial perception layer works better\nthan Non-local and CBAM. In practice, Non-local needs to\ncompute a matrix quadratic over the HW while our spa-\ntial saliency distribution map is linear over the input squence\nlength. CBAM adopts average pooling to compute spatial\nweight matrix, which may lose important clues about distinc-\ntive object features. These experiments verify the effective-\nness of our SEM and show that it generalizes well to various\ntransformer architectures.\nSigniﬁcance of the Abnormality-Aware Attention. Ta-\nble 4 shows the ablation study on the effectiveness of the\nModel Non-local\n[Wang et\nal., 2018]\nCBAM\n[Woo et al., 2018]\nSpatial\nPerception Layer\nEyePACS\nKappa\nRFMiD\nF1-score\nSatFormer\n-B\nX 89.7 94.7\nX 90.2 95.1\nX 90.8 95.8\nTable 3: Ablation study of the spatial perception layer. We compare\nit with Non-local and CBAM.\nabnormality-aware attention. Several self-attention mecha-\nnisms used in PVT, Swin, Twins and CrossFormer are com-\npared. Our SatFormer-B outperforms them at least absolute\n1.7% Kappa on the EyePACS dataset and achieves at least ab-\nsolute 2.8% performance gain on the RFMiD dataset. More-\nover, Table 2 shows that even without SEM, our SatFormer-B\n(w/o.) still outperforms other Vison Transformers. This indi-\ncates applying our abnormality-aware attention in other Vison\nTransformers (like Swin-B or Swin-L) also beneﬁts the over-\nall performance. In particular, PVT adopts spatial-reduction\nto reduce the sequence length and thus sacriﬁces the ﬁne-\ngrained features, while Swin restricts the self-attention in a\nlocal window, sacriﬁcing the long-distance attention. Twins\nand CrossFormer compute self-attention within dilated par-\ntitions of feature maps, but do not adaptively group speciﬁc\ntokens to explicitly capture long-range feature dependencies\nbetween lesion regions. Compared with simply splitting the\nfeature map into multiple windows in which tokens share the\nsame surroundings, our adaptive patch grouping strategy pre-\nvents tremendous background information from overwhelm-\ning the subtle visual clues related to pathological abnormali-\nties. In this way, high-level semantic information from differ-\nent pathological regions are combined to form a comprehen-\nsive lesion feature understanding. The results show that our\nabnormality-aware attention is most conducive to improving\nthe performance.\nImportance of the Cross-Group Attention. In Table 5,\nwe show that it is crucial to make use of the cross-group atten-\ntion, where removing it deteriorates the overall classiﬁcation\nperformance by over 1%. The underlying reason is that the\ncross-group information exchange helps the model to learn\nimportant supplementary information such as the structure of\nthe entire retina, which is beneﬁcial to a thorough understand-\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n992\nInput Image EfficientNet  ReXNet Swin CrossFormer CvT Ours\nFigure 4: Visualization results on the RFMiD testset. CNN-based models prefer large continuous areas, while Transformer-based models and\nother hybrid models prefer the most important lesion region and still suffer from irrelevant backgrounds. Only our SatFormer can perceive\ndiverse scattered lesions thanks to the explict modeling of dependencies between pathological features.\nModel PVT\nCrossFormer Twins Swin Abnormality-Aware\nAttention\nEyePACS\nKappa\nRFMiD\nF1-score\nSatFormer\n-B\nX 86.8 92.1\nX 88.3 93.0\nX 87.4 92.5\nX 89.1 92.9\nX 90.8 95.8\nTable 4: Ablation study of the abnormality-aware attention. The\nbase model is SatFormer-B.\nModels Cross\n-Group Attention #param.(M) FLOPs (G) EyePACS\nKappa\nRFMiD\nF1-score\nSatFormer\n-S × 16 13.4 88.3 91.8\nX 27 22.5 89.4 94.9\nSatFormer\n-B × 49 38.8 89.5 93.7\nX 78 65.0 90.8 95.8\nTable 5: Ablation study of the cross-group attention.\ning of lesions and their surroundings. Otherwise, the network\nwill not be able to learn useful representations from the to-\nkens in the background group.\n4.4 Visualization of Attention Maps\nSeveral representative visualizations of attention maps in Fig-\nure 4 clearly indicate that the scattered pathological regions\nare well captured and perceived by our SatFormer. On the\ncontrary, the existing methods show a strong bias on the most\nsalient lesion region and ignore trivial lesion information con-\ntained in the subtle regions. Moreover, it is interesting to\nobserve that the existing state-of-the-art Vision Transformers\nstill have high responses in irrelevant regions (e.g., Swin and\nCrossFormer). This phenomenon is consistent with the un-\nderlying mechanism of their self-attention functions, which\ncompute dependencies between patches from naive partitions\nof feature maps. Different from existing methods, we pro-\npose to adaptively perceive and group speciﬁc tokens contain-\ning discriminative features. Our abnormality-aware attention\nadaptively selects patch tokens and gather useful information\nto update the chosen tokens instead of all tokens. Only the vi-\nsualization results of our SatFormer show a thorough compre-\nhension of diverse subtle lesion regions. This phenomenon\nalso reveals that the overall accuracy improvement of Sat-\nFormer compared to other models is brought about by cap-\nturing trivial or less discriminative lesion regions contained\nin a fundus image.\n5 Conclusions and Future Work\nIn this paper, we propose a saliency-guided abnormality-\naware transformer for retinal disease classiﬁcation, which ex-\nplicitly captures long-range dependencies between scattered\nsubtle lesions from a global perspective. Particularly, we de-\nsign a saliency enhancement module to enhance the semantics\nof small pathological regions. Our abnormality-aware atten-\ntion with an adaptive patch grouping strategy helps the model\ngain a comprehensive understand of diverse lesions in an ef-\nﬁcient way. Experiments show our SatFormer signiﬁcantly\noutperforms prior state-of-the-art methods.\nSatFormer follows the hierarchical design of popular Vi-\nsion Transformers (e.g., Swin Transformer and PVT) which\ngenerate multi-scale feature maps. In future research, it is\nworth exploring the capability of SatFormer as a general-\npurpose backbone for dense prediction tasks (e.g., object de-\ntection and semantic segmentation).\nAcknowledgments\nThis research was partially supported by the Key Research\nand Development Program of Zhejiang Province under Grant\n2021C03032, the National Natural Science Foundation of\nChina under Grant 61972343, and the National Major Scien-\ntiﬁc Research Instrument Development Project under Grant\n81827804.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n993\nReferences\n[Bi et al., 2021] Qi Bi, Shuang Yu, Wei Ji, Cheng Bian, Li-\njun Gong, Hanruo Liu, Kai Ma, and Yefeng Zheng. Local-\nglobal dual perception based deep multiple instance learn-\ning for retinal disease classiﬁcation. In MICCAI, pages\n55–64. Springer, 2021.\n[Bourne et al., 2017] Rupert RA Bourne, Seth R Flaxman,\nTasanee Braithwaite, et al. Magnitude, temporal trends,\nand projections of the global prevalence of blindness and\ndistance and near vision impairment: a systematic review\nand meta-analysis. The Lancet Global Health, 5(9):e888–\ne897, 2017.\n[Chu et al., 2021] Xiangxiang Chu, Zhi Tian, Yuqing Wang,\nBo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and\nChunhua Shen. Twins: Revisiting the design of spatial\nattention in vision transformers. Advances in Neural In-\nformation Processing Systems, 34, 2021.\n[Cuadros and Bresnick, 2009] Jorge Cuadros and George\nBresnick. Eyepacs: an adaptable telemedicine system for\ndiabetic retinopathy screening. Journal of diabetes science\nand technology, 3(3):509–516, 2009.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\n[Han et al., 2021] Dongyoon Han, Sangdoo Yun, Byeongho\nHeo, and YoungJoon Yoo. Rethinking channel dimensions\nfor efﬁcient model design. In ICCV, pages 732–741, 2021.\n[Li et al., 2019] Xiaomeng Li, Xiaowei Hu, Lequan Yu, Lei\nZhu, Chi-Wing Fu, and Pheng-Ann Heng. Canet: cross-\ndisease attention network for joint diabetic retinopathy and\ndiabetic macular edema grading. IEEE transactions on\nmedical imaging, 39(5):1483–1493, 2019.\n[Li et al., 2021] Yawei Li, Kai Zhang, Jiezhang Cao, Radu\nTimofte, and Luc Van Gool. Localvit: Bringing locality\nto vision transformers. arXiv preprint arXiv:2104.05707,\n2021.\n[Lin et al., 2018] Zhiwen Lin, Ruoqian Guo, Yanjie Wang,\nBian Wu, Tingting Chen, Wenzhe Wang, Danny Z Chen,\nand Jian Wu. A framework for identifying diabetic\nretinopathy based on anti-noise detection and attention-\nbased fusion. In MICCAI, pages 74–82. Springer, 2018.\n[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nshifted windows. In ICCV, pages 10012–10022, 2021.\n[Pachade et al., 2021] Samiksha Pachade, Prasanna Porwal,\net al. Retinal fundus multi-disease image dataset (rfmid):\nA dataset for multi-disease detection research. Data,\n6(2):14, 2021.\n[Phene et al., 2019] Sonia Phene, R Carter Dunn, Naama\nHammel, Yun Liu, Jonathan Krause, Naho Kitade, Mike\nSchaekermann, Rory Sayres, Derek J Wu, Ashish Bora,\net al. Deep learning and glaucoma specialists: the rel-\native importance of optic disc features to predict glau-\ncoma referral in fundus photographs. Ophthalmology,\n126(12):1627–1639, 2019.\n[Sun et al., 2021] Rui Sun, Yihao Li, Tianzhu Zhang, Zhen-\ndong Mao, Feng Wu, and Yongdong Zhang. Lesion-aware\ntransformers for diabetic retinopathy grading. In CVPR,\npages 10938–10947, 2021.\n[Tan and Le, 2019] Mingxing Tan and Quoc Le. Efﬁcient-\nnet: Rethinking model scaling for convolutional neural\nnetworks. In ICML, pages 6105–6114. PMLR, 2019.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NIPS, pages 5998–6008, 2017.\n[Wang et al., 2017] Zhe Wang, Yanxin Yin, Jianping Shi,\nWei Fang, Hongsheng Li, and Xiaogang Wang. Zoom-\nin-net: Deep mining lesions for diabetic retinopathy de-\ntection. In MICCAI, pages 267–275. Springer, 2017.\n[Wang et al., 2018] Xiaolong Wang, Ross Girshick, Abhinav\nGupta, and Kaiming He. Non-local neural networks. In\nCVPR, pages 7794–7803, 2018.\n[Wang et al., 2021a] Wenhai Wang, Enze Xie, Xiang Li,\nDeng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A ver-\nsatile backbone for dense prediction without convolutions.\nIn ICCV, pages 568–578, 2021.\n[Wang et al., 2021b] Wenxiao Wang, Lu Yao, Long Chen,\nBinbin Lin, Deng Cai, Xiaofei He, and Wei Liu. Cross-\nformer: A versatile vision transformer hinging on cross-\nscale attention. arXiv preprint arXiv:2108.00154, 2021.\n[Wang et al., 2022] Wenhai Wang, Enze Xie, Xiang Li,\nDeng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pvt v2: Improved baselines with\npyramid vision transformer. Computational Visual Media,\npages 1–10, 2022.\n[Woo et al., 2018] Sanghyun Woo, Jongchan Park, Joon-\nYoung Lee, and In So Kweon. Cbam: Convolutional block\nattention module. In ECCV, pages 3–19, 2018.\n[Wu et al., 2021a] Haiping Wu, Bin Xiao, Noel Codella,\nMengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. In ICCV,\npages 22–31, 2021.\n[Wu et al., 2021b] Yu-Huan Wu, Yun Liu, Xin Zhan, and\nMing-Ming Cheng. P2t: Pyramid pooling transformer for\nscene understanding. arXiv preprint arXiv:2106.12011,\n2021.\n[Xu et al., 2021] Weijian Xu, Yifan Xu, Tyler Chang, and\nZhuowen Tu. Co-scale conv-attentional image transform-\ners. In ICCV, pages 9981–9990, 2021.\n[Yu et al., 2021] Shuang Yu, Kai Ma, Qi Bi, Cheng Bian,\nMunan Ning, Nanjun He, Yuexiang Li, Hanruo Liu, and\nYefeng Zheng. Mil-vt: Multiple instance learning en-\nhanced vision transformer for fundus image classiﬁcation.\nIn MICCAI, pages 45–54. Springer, 2021.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n994"
}