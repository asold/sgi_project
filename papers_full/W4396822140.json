{
  "title": "Investigating Interaction Modes and User Agency in Human-LLM Collaboration for Domain-Specific Data Analysis",
  "url": "https://openalex.org/W4396822140",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5029774979",
      "name": "Jiajing Guo",
      "affiliations": [
        "Robert Bosch (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5018864415",
      "name": "Vikram Mohanty",
      "affiliations": [
        "Robert Bosch (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5051400014",
      "name": "Jorge Piazentin Ono",
      "affiliations": [
        "Robert Bosch (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5004271027",
      "name": "Hongtao Hao",
      "affiliations": [
        "Robert Bosch (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5095084507",
      "name": "Liang Gou",
      "affiliations": [
        "Robert Bosch (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5073167969",
      "name": "Liu Ren",
      "affiliations": [
        "Robert Bosch (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4362659486",
    "https://openalex.org/W4291910525",
    "https://openalex.org/W4389520463",
    "https://openalex.org/W2906151105",
    "https://openalex.org/W2963963837",
    "https://openalex.org/W4393970768",
    "https://openalex.org/W2911396267",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W2064766209",
    "https://openalex.org/W2044102377",
    "https://openalex.org/W3201904098",
    "https://openalex.org/W3016099278",
    "https://openalex.org/W4366587430",
    "https://openalex.org/W4213145388",
    "https://openalex.org/W4225108562",
    "https://openalex.org/W4366590384"
  ],
  "abstract": "Despite demonstrating robust capabilities in performing tasks related to general-domain data-operation tasks, Large Language Models (LLMs) may exhibit shortcomings when applied to domain-specific tasks. We consider the design of domain-specific AI-powered data analysis tools from two dimensions: interaction and user agency. We implemented two design probes that fall on the two ends of the two dimensions: an open-ended high agency (OHA) prototype and a structured low agency (SLA) prototype. We conducted an interview study with nine data scientists to investigate (1) how users perceived the LLM outputs for data analysis assistance, and (2) how the two test design probes, OHA and SLA, affected user behavior, performance, and perceptions. Our study revealed insights regarding participants' interactions with LLMs, how they perceived the results, and their desire for explainability concerning LLM outputs, along with a noted need for collaboration with other users, and how they envisioned the utility of LLMs in their workflow.",
  "full_text": "Investigating Interaction Modes and User Agency in Human-LLM\nCollaboration for Domain-Specific Data Analysis\nJiajing Guo\njiajing.guo@us.bosch.com\nBosch Research North America\nSunnyvale, California, USA\nVikram Mohanty\nvikrammohanty@acm.org\nBosch Research North America\nSunnyvale, California, USA\nJorge Ono Piazentin\njorge.piazentinono@us.bosch.com\nBosch Research North America\nSunnyvale, California, USA\nHongtao Hao∗\nhongtao.hao@us.bosch.com\nBosch Research North America\nSunnyvale, California, USA\nLiang Gou\nliang.gou@us.bosch.com\nBosch Research North America\nSunnyvale, California, USA\nLiu Ren\nliu.ren@us.bosch.com\nBosch Research North America\nSunnyvale, California, USA\nABSTRACT\nDespite demonstrating robust capabilities in performing tasks re-\nlated to general-domain data-operation tasks, Large Language Mod-\nels (LLMs) may exhibit shortcomings when applied to domain-\nspecific tasks. We consider the design of domain-specific AI-powered\ndata analysis tools from two dimensions: interaction and user\nagency. We implemented two design probes that fall on the two\nends of the two dimensions: anopen-ended high agency (OHA) pro-\ntotype and a structured low agency (SLA) prototype. We conducted\nan interview study with nine data scientists to investigate (1) how\nusers perceived the LLM outputs for data analysis assistance, and (2)\nhow the two design probes, OHA and SLA, affected user behavior,\nperformance, and perceptions. Our study revealed insights regard-\ning participants’ interactions with LLMs, how they perceived the\nresults, and their desire for explainability concerning LLM outputs,\nalong with a noted need for collaboration with other users, and\nhow they envisioned the utility of LLMs in their workflow.\nCCS CONCEPTS\n• Human-centered computing →Empirical studies in HCI .\nKEYWORDS\nLarge language model, data analysis, domain knowledge, human-AI\ncollaboration, user agency\nACM Reference Format:\nJiajing Guo, Vikram Mohanty, Jorge Ono Piazentin, Hongtao Hao, Liang\nGou, and Liu Ren. 2024. Investigating Interaction Modes and User Agency in\nHuman-LLM Collaboration for Domain-Specific Data Analysis. In Extended\nAbstracts of the CHI Conference on Human Factors in Computing Systems\n(CHI EA ’24), May 11–16, 2024, Honolulu, HI, USA. ACM, New York, NY, USA,\n9 pages. https://doi.org/10.1145/3613905.3651042\n∗This work was completed during an internship at Bosch Research North America.\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nCHI EA ’24, May 11–16, 2024, Honolulu, HI, USA\n© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0331-7/24/05.\nhttps://doi.org/10.1145/3613905.3651042\n1 INTRODUCTION\nDomain-specific data operations encompass a range of activities,\nincluding data transformation, processing, and analysis across spe-\ncialized domains, such as medicine, manufacturing, finance, sports,\nand beyond. Conducting complex data operations requires a deep\nunderstanding of the specific data structures and thorough knowl-\nedge of the unique concepts and terminologies relevant to each\ndomain.\nIn recent years, pre-trained Large Language Models (LLMs) have\ndemonstrated significant capabilities in diverse data science and\nmachine learning tasks, such as visualization [26], junior-level data\nanalysis [5], classification [ 12], and model selection [ 22]. Many\nresearch communities and companies started to investigate Human-\nLLM collaboration as the future of programming, as studies showed\nthat LLMs can save developers’ searching efforts [ 24], improve\nproductivity and developer happiness [14]. Commercially available\ntools such as Github Copilot and ChatGPT Code Interpreter [19]\ngarnered significant attention upon their release.\nDespite demonstrating robust capabilities in general-domain\ndata operations, LLMs may exhibit shortcomings when applied to\ndomain-specific tasks. While the extensive training data enables\nLLMs to comprehend a wide range of concepts or terms, it may\nstill be insufficient for understanding meanings specific to certain\norganizations or nuanced contexts.\nWe consider the design of AI-powered domain-specific data anal-\nysis from two dimensions:interaction and user agency, as shown\nin Table 1. Many conversational LLM applications offer open-\nended interactions, requiring users to consider their prompts to\nLLMs, with the freedom to use code, natural language commands,\nor questions. Structured interaction typically features predefined\nstages and ensures that the user’s journey through a system fol-\nlows a logical path. Users are guided by the system to complete\ncertain steps to get results. User agency or human control is a\ncritical aspect in automated systems [11, 23]. Code assistants such\nas Github Copilot offer suggestions, leaving it up to the user to\ndecide which code snippet to use, depending on the user’s actions\nlike code execution to achieve the result. In contrast, LLM agents\nsuch as ChatGPT Code Interpreter execute the code on behalf of\nthe user and automatically present the results. AI-powered tools\nfor data analysis such as PandasAI [25] and VizGPT [1] provided\nautomatic workflow to convert user queries into visualization or\nanalysis results. Besides user agency on the code execution level,\narXiv:2405.05548v1  [cs.HC]  9 May 2024\nCHI EA ’24, May 11–16, 2024, Honolulu, HI, USA Guo and Mohanty, et al.\nInteraction User agency ApplicationExecution Planning\nOpen-ended high high Copilot, ChatGPT, *Design Probe OHA\nOpen-ended low high Code Interpreter, PandasAI, VizGPT\nStructured low low *Design Probe SLA\nTable 1: AI-powered coding tools on interaction and user agency dimension. We implemented two design probes on the ends\nof the two dimensions: an open-ended high agency (OHA) that has an open-ended workflow and gives users high execution\nand planning agency on LLM outputs; another is a structured low agency (SLA) application that guides users step-by-step and\nautomatically plans and executes code to complete tasks.\nplanning-level assistance is also a critical aspect in data analysis\nand has also been discussed [9]. However, little work has explored\nstructured interaction in AI-powered data analysis tools. In the\ncontext of domain-specific tasks, human-LLM collaboration is\nessential as LLMs often lack sufficient domain knowledge.\nIn this work, we explore the design space of domain-specific data\nanalysis by inviting users to work with two probes from the ends of\nthe two dimensions: one has an open-ended interaction workflow\nand gives users high agency on both execution and planning level;\nthe other has a structured interaction workflow that guides the\nuser step-by-step and gives lower execution and planning agency\ncompared with the first one. We conducted an interview study with\nnine participants and invited them to complete two domain-specific\nanalysis tasks on each prototype.\nWe aim to answer the following research questions:\n•RQ1: How do users perceive the outputs of LLMs in the\ncontext of domain-specific data analysis tasks?\n•RQ2: What is the optimal design for LLM-powered tools to\nassist in data analysis tasks?\nOur key findings are:\n•Participants expressed overall satisfaction with the LLMs\nin assisting them with data analysis tasks, while raising\nconcerns about the need for transparency and ability to\nverify results.\n•The different interaction modes (open-ended and structured\nworkflow) and the two degrees of agency (high and low)\nresulted in different user behaviors, each with its own set of\nadvantages and limitations.\nBased on these findings, we propose design implications for\ndesigning an LLM-enhanced collaborative system for supporting\ndata analysis tasks.\n2 INTERVIEW STUDY\n2.1 Design Probes\nWe implemented two interactive prototypes instead of using slide-\nbased prototypes [20] or Wizard-of-oz method [9] because we want\nthe users to experience the application as close to their daily job\nas possible. We opted not to employ existing tools such as Github\nCopilot or ChatGPT because we want the two prototypes to use\nthe same LLM to make the output consistent. Code Interpreter is\na commercially available tool but its data analysis requires users\nto upload data files, which is not applicable for enterprises that\nconcern data security and sensitivity.\nOHA (Figure 1-A) is an LLM-based chat web application inte-\ngrated with a rudimentary API that connects to Azure OpenAI\nGPT-4 service. As a vanilla chat interface, it is limited to basic mes-\nsaging functionalities. When used in data analysis tasks, users still\nneed a developer environment to execute code.\nSLA (Figure 1-B) is a structured, low-agency LLM-powered ap-\nplication. It guides users through a data analysis workflow, makes\nplans, and executes the code on behalf of the user. We designed the\nworkflow based on literature about data analysis [16, 28] and LLM\nprompt engineering [27, 29]. The workflow is as follows:\n(1) Describe data (User). Users can upload multiple CSV files,\nedit descriptions of each file, and provide context informa-\ntion about the analysis, such as data source and domain (see\nFigure 3 in Appendix). LLMs tend to perform better with\nmore contextual information. Dibia [7] examined that pro-\nviding enriched data summary leads to reduced error rate\nwhen generating visualizations.\n(2) Inject query and domain knowledge (User). Users can inject\na question they want to investigate and domain knowledge\nthat may be helpful for the LLM to complete the task.\n(3) Enhance prompt (System). The system generates a step-by-\nstep guide to complete the task based on the question and\ndomain knowledge the user provided (see Figure 4 in Ap-\npendix).\n(4) Select enhanced prompt (User). Users can select from three\nvariations, then the user can trigger execution.\n(5) Automatic data analysis (System). This step involves code\ngeneration, execution, debugging, and insight generation.\nThe backend is an LLM agent that adapts the ReAct frame-\nwork [10, 29], which orients the LLM to iterate through\nreasoning, action, and observation cycles.\n(6) Review output (User). Users can view the result of execution,\nincluding visualizations, code, and insights generated by SLA\n(see Figure 5 in Appendix).\n2.2 Tasks and Procedure\nFollowing a lab experiment study guideline, we conducted a within-\nsubject study to compare the interactive system built upon our\nframework and existing LLM applications.\nParticipants. We recruited nine data scientists from a prod-\nuct team in a multi-national enterprise (8M, 1F), aged from 25 to\n46). Their job content includes data analysis in manufacturing and\nsensor domains. All of them have at least one year of working ex-\nperience in data analysis. Participants self-identified job titles are\nInvestigating Interaction Modes and User Agency in Human-LLM Collaboration for Domain-Specific Data Analysis CHI EA ’24, May 11–16, 2024, Honolulu, HI, USA\nFigure 1: A: Interface of design probe OHA. B: Interface of design probe SLA.\ndata scientist (6), data engineer (1), and research scientist (2). Par-\nticipants have various degrees of familiarity with LLM applications\nfor data analysis. Some participants have experience of using LLMs\nto polish writing and summarize articles but have not used LLMs\nin data science job. Some express untrust towards LLMs, and use\nLLMs to generate code templates or overall structures only. Two\nuse LLMs frequently in their daily work. Each participant received\n$20 gift card as compensation.\nBefore the study, participants completed a pre-study survey ask-\ning for demographic information and their previous experience\nwith LLM-powered applications. The interview study can be bro-\nken down into three parts.\nPart 1: Pre-study interview . We first asked some questions\nabout participants’ job content that is relevant to data analysis.\nParticipants were cued to recall an experience when they needed\nto deliver data analysis reports to stakeholders, the volume of data\nto be analyzed, the negotiation procedure, and the format of the\nreport.\nPart 2: Two domain-specific data analysis tasks . Participants\nwere asked to complete two data analysis tasks on enterprise in-\nternal data with the help of LLM-powered tools, one is OHA, and\nthe other is SLA. Participants were encouraged to \"think aloud\"\nwhile they were doing the task. The order of tools was randomized.\nWe want to simulate participants’ daily work as much as possible,\ntherefore the data used in the study were extracted from a huge\ndata lake to which the authors have access. The two tasks used the\nsame data source but different requirements (see Figure 2).\nPart 3: Critiques and reflections . After two tasks, participants\ncritiqued and reflected on their experience by comparing the two\ntools, and discussing the perceived utility and limitations.\nInterviews were conducted by two researchers. One researcher\nled the interview and the other took notes. The two researchers\nhad retrospective meetings after each interview and discussed the\ntakeaways. All the interviews were video-recorded and with partic-\nipants’ permission and transcribed. Using an inductive open-coding\nanalysis approach [6, 21], the authors read through the transcripts\nand watched recordings multiple times to conduct the initial cod-\ning. Then, both authors read through and discussed the coded\ntranscripts together, iteratively highlighted excerpts, and identified\nkey insights and recurring patterns in the data.\n3 PRELIMINARY FINDINGS\nIn this section, we discuss the insights from our probe interviews,\nwhich we have organized according to our two main research ques-\ntions: (1) how users perceived the LLM outputs for data analysis\nassistance, and (2) how the two design probes, OHA and SLA, af-\nfected user behavior, performance, and perceptions.\nCHI EA ’24, May 11–16, 2024, Honolulu, HI, USA Guo and Mohanty, et al.\nFigure 2: Example tasks in the interview study. Participants\nreceived the example task as a prompt and were informed that\nthey could customize their own queries that fit their job.\n3.1 User Perception of LLM outputs for\nDomain-specific Data Analysis\nIn terms of how users perceived LLM outputs for their data analy-\nsis tasks across the two design probes, we identified three broad\nthemes:\n3.1.1 Output Quality. Participants appreciated OHA for its ability\nto assist in quickly generating initial versions of code, which was\nseen as a valuable feature for starting data analysis tasks, even\nthough executing the code threw errors at times. P5 said, \"It’s quite\neasy and intuitive, especially for easy tasks, they are just supposed\nto write out some code. \" They also expressed similar satisfaction\nwith the insights produced by SLA, acknowledging its utility in\nenhancing workflow efficiency, especially during the exploratory\nphase and reporting stage. The prototype’s ability to generate com-\nprehensive plots and explanations was particularly valued for its\npotential to aid communication with stakeholders.\nI think for the first time even for us to think what to tell\nto customer from this graph, it takes a little bit time to\nthink how to explain. But here, if we read these insights,\nit gives some tips what can make sense... To start with,\nit is really good. Maybe a little bit more articulation or\nadditional information might be required when we are\ntelling to the user. I think this insights are helpful. (P8)\nParticipants expressed overall satisfaction with the code and\ninsights produced by LLMs across both probes, even though some\nconclusions lacked in-depth findings or did not align with the prob-\nlem. Despite these limitations, participants were optimistic about\nhow the LLM outputs here can be a promising starting point for\nfuture iterations.\n3.1.2 Trust and Verifiability. Trust emerged as a crucial concern\namong participants using SLA, particularly due to challenges in\nverifying the tool’s results. Users expressed hesitation in relying on\nthe system without independent verification, underscoring the need\nfor trust-building measures. P2 encapsulated a similar sentiment\nfor OHA: \"I’m seeing how nice that was. It was really quick to get\nan answer. I don’t know how to trust it without actually redoing all\nthe work myself, which would be very time-consuming. \" The lack\nof domain-specific fine-tuning in LLMs further contributed to this\ntrust issue, with users like P1 and P4 voicing concerns about the\nsystem’s understanding of their intentions and domain knowledge.\nAdditionally, the inconsistency in the tool’s outputs, attributed to\nthe LLM’s temperature setting, was found to impact trust, especially\nfor users less familiar with data analysis methodologies.\n3.1.3 Need for Explanations. Participants emphasized the impor-\ntance of explanations and transparency when interacting with\nLLMs. They actively sought clarity on the LLM’s reasoning process,\nparticularly when outputs did not match expectations. P2 captured\nthis need: \"Cause at least it gives me an idea that it understands the\nconcept and what to do... I could edit this if I was doing a real thing\nand make sure that this is exactly the right steps. \" This desire for\nexplanations was especially pronounced in situations where the\nLLM suggested complex operations, like machine learning models,\nbut did not follow through, leading to dissatisfaction and confusion.\nP7’s experience underscores this frustration: \"It completely over\npromised the delivery of the machine learning model... I’m very unsat-\nisfied that it does not give me a model at all and also does not explain\nto me why. \" This finding highlights a need for more transparent\ncommunication of the LLM’s decision-making process to build user\ntrust and understanding.\n3.2 Impact of Design Modes on User Behavior\nHere, we discuss how the interaction modes (i.e., open-ended and\nstructured) and the user agency (i.e., high v/s low) in the two probes\naffected user behavior and perception of the LLM prototypes.\n3.2.1 Interaction Modes. We examine the impact of the two dis-\ntinct user workflows — open-ended (OHA) and structured (SLA) —\non user behaviors (see Table 2) and perceptions. In the open-ended\nOHA mode, participants with LLM experience often included sam-\nple data or key columns in their queries, seeking to provide context.\nP4 highlighted the cumbersome nature of this process: \"I wish we\nhave a better way to directly inject this code and meta informational\ndata into this system... I have to manually write down this prompt by\nmyself, which is somewhat not easy. \" Conversely, in the structured\nSLA mode, users typically did not engage in an interactive dialogue\nabout the data. Instead, they provided brief text descriptions or\nessential details about the dataset, relying on the system’s structure\nto guide the data analysis process.\nFor domain knowledge, OHA users often embedded this infor-\nmation within their questions or as side notes, while SLA users\nprovided varying levels of detail, intuitively gauging what the model\nmight need. P1’s experience illustrates this:\"With the domain knowl-\nedge, I can be more clear about what features and values I want... Like\ntraffic lights on the left, maybe the language model would interpret\nthis as high values for left, but instead, this is low values for left. \" This\nvaried approach to input in SLA sometimes led to users selecting\nthe first provided LLM-enhanced domain knowledge variation and\nproceeding without exploring other options, citing time constraints.\nThis structured approach in SLA helped guide users in providing\nnecessary information, sometimes filling in gaps that were more\nchallenging in the open-ended OHA mode.\n3.2.2 Agency. In the OHA setup, users demonstrated high agency\nby independently determining the course of their analysis, mak-\ning decisions without system guidance. In contrast, the SLA probe\nguided users through the process, providing detailed domain knowl-\nedge and outlining analysis steps, thus reducing the level of user\nInvestigating Interaction Modes and User Agency in Human-LLM Collaboration for Domain-Specific Data Analysis CHI EA ’24, May 11–16, 2024, Honolulu, HI, USA\nagency in planning. Error handling in OHA demonstrated high user\nagency, with participants actively diagnosing and communicating\nerrors to the LLM. Users copied the entire or partial error messages,\ndirectly engaging in the debugging process. In contrast, the SLA\nprobe exhibited low user agency in error handling; most errors\nwere automatically managed within the workflow, often remaining\nunseen by users.\nHowever, in cases of unresolved errors due to token limitations,\nuser agency was limited to rephrasing inputs without direct in-\nvolvement in error resolution. Participants across both setups un-\nderscored the need for iteration capabilities in LLM-powered tools:\n\"I think it does need some iteration with the model, and then after\nyou take the output of the model, you need to adapt it a little bit on\nyour own. \"\n4 DISCUSSION\nThe design probes used in the interview were designed and imple-\nmented to align with the two ends on the interaction and user\nagency dimensions. In this section, we discuss the findings and\ndesign considerations from these dimensions. Then we discuss\nthe trust and verifiability of LLM-powered data analysis tools and\ndesign recommendations.\n4.1 Design Considerations of AI Assistance for\nDomain-specific Data Analysis\nStructured Interaction. In HCI literature, structured workflows\nhave been widely adopted in interactive applications. This approach,\nwhich predates the advent of LLM applications such as ChatGPT, is\ndesigned to streamline user workflows and guide users to prevent\nerrors. Since the launch of ChatGPT, conversational and open-ended\ninteraction have grown in popularity, offering more flexible and\nnatural interaction paradigms. Many AI-powered tools for data anal-\nysis [1, 25] and research-oriented design prototypes have adopted\nthe open-ended interaction paradigm [9, 20]. Despite allowing users\nthe freedom to initiate conversations, open-ended interaction neces-\nsitates that users possess prompt engineering skills, such as choice\nof terminology, arrangement of word order, as well as repetition in\nkeywords [30]. This requirement may burden data analysts with\nthe additional task of mastering prompt engineering if their original\ngoal of using AI-powered tools is to increase efficiency [3, 20]. For\nusers inexperienced with prompt engineering, predefined steps aid\nin understanding how LLMs function. Intermediate steps elucidate\nhow the LLM interprets tasks and decides on actions, enhancing\ntransparency. The appropriateness of conversational interaction for\nLLM-powered data analysis remains an open question. Designers\nand researchers should avoid creating chatbots merely for their\nown sake. Instead, considering a system that incorporates both\nopen-ended and structured interactions may be beneficial. For in-\nstance, developers could implement a conversational assistant that\nguides users through predefined steps, or a structured interface\nthat allows for configurable steps.\nExecution-level User Agency . Participants expressed appreci-\nation for the automated processes in code generation, debugging,\nand interpreting results. This finding suggests that low levels of\ncontrol at the execution level do not result in dissatisfaction among\nour participants. It was suggested that increased agency, allow-\ning data analysts to modify the code before and after execution,\nwould be beneficial. We also found that depending on the job con-\ntent, the range of execution-level tasks may be expanded to data\nwrangling pipeline instead of only code generation and execution.\nParticipants expressed a need for automation in foundational\ndata pre-processing and wrangling tasks , such as normalizing\ndata, filling in missing values, extracting descriptive statistics, find-\ning outliers, and visualizing distribution. These tasks, despite their\nrepetitive nature, are crucial for thorough analysis [8]. Our obser-\nvations confirmed that participants consistently prioritize these\ninitial steps, underscoring the potential benefits of streamlining\nthem through automation [2]. The efficacy of LLMs in automating\nfundamental pre-processing tasks streamlines initial data analysis\nstages [4, 11, 15, 17], enabling human analysts to focus on complex,\ndomain-specific analytical challenges.\nPlanning-level User Agency. In contrast to their significant en-\nthusiasm for assistance at the execution level, participants showed\ninterest and caution regarding planning-level assistance. One rea-\nson for their caution is a lack of confidence in the quality of plans\ngenerated by LLMs. While using OHA, most participants opted to\ncreate execution commands rather than consult the LLM’s anal-\nysis plan. The planning mechanism in SLA, which generates a\nstep-by-step guide based on user questions and domain knowl-\nedge, demonstrated limited capabilities and may not suit complex\nreal-world tasks. Some participants were interested in whether\nthe LLM-powered tool could assist in brainstorming potential ana-\nlytical approaches and facilitating iterative exploration. However,\nthey emphasized that the tool should offer suggestions, while the\nfinal decision on which suggestion to adopt should rest with them\nas data scientists. This finding underscores the importance users\nplace on the planning-level agency. It is imperative that design\nconsiderations reflect this priority.\n4.2 Calibrating User Trust and Expectations\nwith AI Explainability in Data Analysis\nAssistance\nDuring our probe, participants exhibited a general caution\ntowards the results generated by SLA, mainly due to the the ab-\nsence of methods to verify these outcomes and an expectation\nfor consistent results . Additionally, they also raised questions\nconcerning the process by which the LLM reached specific con-\nclusions, impacting their trust in the outputs of LLM-based sys-\ntems collectively. While this behavior aligned with prior work on\ndata scientists seeking interpretable results [ 18], this skepticism\ncould be rooted in the well-documented hallucination tendencies of\nLLMs [13]. It could be beneficial to offer users a verification sand-\nbox playground, allowing them to manually conduct their analyses\nand compare results with those generated by the LLM, aiding in\ncalibrating expectation and understanding the capabilities\nof an LLM-based system .\nThe quest for transparency was evident when participants, with\nenhanced domain knowledge, showed a preference for step-by-step\nelucidation of the analyses, assuming that SLA followed these steps.\nThis hints at the potential benefit of visualizing the analytical\nCHI EA ’24, May 11–16, 2024, Honolulu, HI, USA Guo and Mohanty, et al.\nInteraction OHA Probe SLA Probe\nData Context\nEstablishment\nCopied sample data or columns from Jupyter notebok,\ne.g., “Write python code that takes a pandas dataframe\nwith the following fields <copied_text>. \"\nDrafted prompt in a notepad then paste to chat textbox.\nOnly described useful columns.\nDirectly asked questions without giving any information\nabout the data.\nSelected dataset or upload own csv files.\nManually typed column description in the data descrip-\ntion text box.\nDomain Knowledge\nInjection\nManually typed in the text box.\nInstead of giving a formal definition of domain concepts,\ndirectly gave concrete instructions of calculation or plot-\nting.\nManually provided input and generated variation(s).\nIdentified and corrected incorrect augmented domain\nknowledge.\nError Handling and\nDebugging\nCopied the partial or entire error message from Jupyter\nnotebook.\nTried to debug by themselves\nModified the question or domain knowledge and run\nanalysis again.\nOutput Interpretation Read the plot or transformed data, gave explanation\nbased on their own interpretation.\nRead through the \"insights\" generated by LLM, critiqued\nand revised what is wrong.\nTable 2: Summary of participants’ interactions with the two LLM-based tools used in the interview study.\nprocess through a flowchart or similar means, displaying inter-\nmediate steps, and even showcasing counterfactual results at each\njuncture for better user validation. Enhancing transparency — visu-\nalizing backend operations and ensuring step-by-step explainability\n— can demystify the process, nurturing trust and engagement with\nthe system.\n5 LIMITATIONS AND FUTURE WORK\nIn this study, we only interviewed data scientists with experience\nwith manufacturing and transportation data. As some participants\npointed out, the tool can be beneficial for users who have rich do-\nmain knowledge but little programming skills. Future work would\npropose new design ideas with the suggestions we list above. We\nalso would like to consider different design options for users with\ndifferent skills and needs.\n6 CONCLUSION\nWe explored the design of AI-powered, domain-specific data analy-\nsis tools through two distinct prototypes, differing in interaction\nstyles and user agency, and conducted interviews with data scien-\ntists to assess their effectiveness. Our findings showed that while\nusers appreciated the assistance of LLMs, they also emphasized the\nneed for transparency, result validation, and different interaction\nworkflows catered to varying levels of agency. This research opens\nthe door for future studies on the integration of LLMs into domain-\nspecific applications, focusing on enhancing user collaboration,\nexplainability, and adaptable interaction modes.\nREFERENCES\n[1] [n. d.]. VizGPT. https://vizgpt.ai/. Accessed: 2024-1-26.\n[2] Anaconda. 2020. The State of Data Science in 2020: Moving from Hype Toward\nMaturity. https://www.anaconda.com/state-of-data-science-2020 Accessed:\n2023-10-09.\n[3] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded\nCopilot: How Programmers Interact with Code-Generating Models. Proc. ACM\nProgram. Lang. 7, OOPSLA1 (6 April 2023), 85–111. https://doi.org/10.1145/\n3586030\n[4] Mehwish Bilal, Ghulam Ali, Muhammad Waseem Iqbal, Muhammad Anwar,\nMuhammad Sheraz Arshad Malik, and Rabiah Abdul Kadir. 2022. Auto-prep:\nefficient and automated data preprocessing pipeline. IEEE Access 10 (2022),\n107764–107784.\n[5] Liying Cheng, Xingxuan Li, and Lidong Bing. 2023. Is GPT-4 a Good Data\nAnalyst?. InFindings of the Association for Computational Linguistics: EMNLP 2023\n(Singapore), Bouamor, Houda and Pino, Juan and Bali, Kalika (Ed.). Association\nfor Computational Linguistics, 9496–9514.\n[6] Juliet Corbin and Anselm Strauss. 2012. Basics of Qualitative Research (3rd ed.):\nTechniques and Procedures for Developing Grounded Theory . https://doi.org/10.\n4135/9781452230153\n[7] Victor Dibia. 2023. LIDA: A Tool for Automatic Generation of Grammar-Agnostic\nVisualizations and Infographics using Large Language Models. In Proceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume\n3: System Demonstrations) , Danushka Bollegala, Ruihong Huang, and Alan Ritter\n(Eds.). Association for Computational Linguistics, Toronto, Canada, 113–126.\nhttps://doi.org/10.18653/v1/2023.acl-demo.11\n[8] Ebubeogu Amarachukwu Felix and Sai Peck Lee. 2019. Systematic literature\nreview of preprocessing techniques for imbalanced data.IET Software 13, 6 (2019),\n479–496.\n[9] Ken Gu, Madeleine Grunde-McLaughlin, Andrew M McNutt, Jeffrey Heer, and\nTim Althoff. 2023. How Do Data Analysts Respond to AI Assistance? A Wizard-\nof-Oz Study. (18 Sept. 2023). arXiv:2309.10108 [cs.HC]\n[10] Jiajing Guo, Vikram Mohanty, Hongtao Hao, Liang Gou, and Liu Ren. 2024. Can\nLLMs Infer Domain Knowledge from Code Exemplars? A Preliminary Study. In\nCompanion Proceedings of the 29th International Conference on Intelligent User\nInterfaces. ACM, ACM, New York, NY, USA, Greenville, SC, USA, 6. https:\n//doi.org/10.1145/3640544.3645228\n[11] Jeffrey Heer. 2019. Agency plus automation: Designing artificial intelligence into\ninteractive systems. Proceedings of the National Academy of Sciences 116, 6 (2019),\n1844–1850.\n[12] Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi\nJiang, and David Sontag. 2023. TabLLM: Few-shot Classification of Tabular Data\nwith Large Language Models. In Proceedings of The 26th International Conference\non Artificial Intelligence and Statistics (Proceedings of Machine Learning Research,\nVol. 206), Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent (Eds.). PMLR,\n5549–5581.\n[13] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,\nYe Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in\nnatural language generation. Comput. Surveys 55, 12 (2023), 1–38.\n[14] Eirini Kalliamvakou. 2022. Research: quantifying GitHub Copilot’s impact on\ndeveloper productivity and happiness . Technical Report. Github.\n[15] Sean Kandel, Andreas Paepcke, Joseph Hellerstein, and Jeffrey Heer. 2011. Wran-\ngler: Interactive visual specification of data transformation scripts. InProceedings\nof the sigchi conference on human factors in computing systems . 3363–3372.\n[16] S Kandel, A Paepcke, J M Hellerstein, and J Heer. 2012. Enterprise Data Analysis\nand Visualization: An Interview Study. IEEE transactions on visualization and\ncomputer graphics 18, 12 (Dec. 2012), 2917–2926. https://doi.org/10.1109/TVCG.\nInvestigating Interaction Modes and User Agency in Human-LLM Collaboration for Domain-Specific Data Analysis CHI EA ’24, May 11–16, 2024, Honolulu, HI, USA\n2012.219\n[17] Shubhra Kanti Karmaker, Md Mahadi Hassan, Micah J Smith, Lei Xu, Chengxiang\nZhai, and Kalyan Veeramachaneni. 2021. Automl to date and beyond: Challenges\nand opportunities. ACM Computing Surveys (CSUR) 54, 8 (2021), 1–36.\n[18] Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wal-\nlach, and Jennifer Wortman Vaughan. 2020. Interpreting interpretability: under-\nstanding data scientists’ use of interpretability tools for machine learning. In\nProceedings of the 2020 CHI conference on human factors in computing systems .\n1–14.\n[19] Yiwen Lu. 2023. What to Know About ChatGPT’s New Code Interpreter Feature.\nThe New York Times (11 July 2023).\n[20] Andrew M Mcnutt, Chenglong Wang, Robert A Deline, and Steven M Drucker.\n2023. On the Design of AI-powered Code Assistants for Notebooks. InProceedings\nof the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg,\nGermany) (CHI ’23, Article 434) . Association for Computing Machinery, New\nYork, NY, USA, 1–16. https://doi.org/10.1145/3544548.3580940\n[21] Johnny Saldaña. 2021. The coding manual for qualitative researchers . sage.\n[22] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting\nZhuang. 2023. HuggingGPT: Solving AI Tasks with ChatGPT and its Friends\nin Hugging Face. In Advances in Neural Information Processing Systems , A Oh,\nT Neumann, A Globerson, K Saenko, M Hardt, and S Levine (Eds.), Vol. 36. Curran\nAssociates, Inc., 38154–38180.\n[23] Ben Shneiderman. 2022. Human-centered AI . Oxford University Press.\n[24] Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. 2022. Expectation\nvs. Experience: Evaluating the Usability of Code Generation Tools Powered\nby Large Language Models. In Extended Abstracts of the 2022 CHI Conference\non Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA ’22,\nArticle 332). Association for Computing Machinery, New York, NY, USA, 1–7.\nhttps://doi.org/10.1145/3491101.3519665\n[25] Gabriele Venturi. 2022. PandasAI. https://github.com/gventuri/pandas-ai\n[26] Henrik Voigt, Nuno Carvalhais, Monique Meuschke, Markus Reichstein, Sina\nZarrie, and Kai Lawonn. 2023. VIST5: An Adaptive, Retrieval-Augmented Lan-\nguage Model for Visualization-oriented Dialog. In Proceedings of the 2023 Confer-\nence on Empirical Methods in Natural Language Processing: System Demonstrations ,\nYansong Feng and Els Lefever (Eds.). Association for Computational Linguistics,\nSingapore, 70–81. https://doi.org/10.18653/v1/2023.emnlp-demo.5\n[27] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and\nEe-Peng Lim. 2023. Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-\nThought Reasoning by Large Language Models. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for\nComputational Linguistics, Toronto, Canada, 2609–2634. https://doi.org/10.\n18653/v1/2023.acl-long.147\n[28] Kanit Wongsuphasawat, Yang Liu, and Jeffrey Heer. 2019. Goals, Process, and\nChallenges of Exploratory Data Analysis: An Interview Study. (1 Nov. 2019).\narXiv:1911.00568 [cs.HC]\n[29] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,\nand Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language\nModels. In The Eleventh International Conference on Learning Representations .\n[30] Haoquan Zhou and Jingbo Li. 2023. A Case Study on Scaffolding Exploratory\nData Analysis for AI Pair Programmers. In Extended Abstracts of the 2023 CHI\nConference on Human Factors in Computing Systems (Hamburg, Germany) (CHI\nEA ’23, Article 561) . Association for Computing Machinery, New York, NY, USA,\n1–7. https://doi.org/10.1145/3544549.3583943\nCHI EA ’24, May 11–16, 2024, Honolulu, HI, USA Guo and Mohanty, et al.\nFigure 3: Probe SLA: Adding Data Context\nFigure 4: Probe SLA: Providing Domain Knowledge and Enhancing it with the LLM\nInvestigating Interaction Modes and User Agency in Human-LLM Collaboration for Domain-Specific Data Analysis CHI EA ’24, May 11–16, 2024, Honolulu, HI, USA\nFigure 5: Probe SLA: Analysis results – visualization and code. Insights are not shown in this figure.",
  "topic": "Agency (philosophy)",
  "concepts": [
    {
      "name": "Agency (philosophy)",
      "score": 0.8469318151473999
    },
    {
      "name": "Workflow",
      "score": 0.793167769908905
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6777423620223999
    },
    {
      "name": "Computer science",
      "score": 0.633388102054596
    },
    {
      "name": "Perception",
      "score": 0.5377464890480042
    },
    {
      "name": "Human–computer interaction",
      "score": 0.47330060601234436
    },
    {
      "name": "Knowledge management",
      "score": 0.3535005748271942
    },
    {
      "name": "Data science",
      "score": 0.348651647567749
    },
    {
      "name": "Psychology",
      "score": 0.23567670583724976
    },
    {
      "name": "Database",
      "score": 0.21898677945137024
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    }
  ],
  "cited_by": 7
}