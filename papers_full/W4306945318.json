{
  "title": "A wheat spike detection method based on Transformer",
  "url": "https://openalex.org/W4306945318",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2169680355",
      "name": "Qiong Zhou",
      "affiliations": [
        "Hefei Institutes of Physical Science",
        "Anhui Agricultural University",
        "Chinese Academy of Sciences",
        "University of Science and Technology of China",
        "Institute of Intelligent Machines"
      ]
    },
    {
      "id": "https://openalex.org/A2896054121",
      "name": "Ziliang Huang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Intelligent Machines",
        "University of Science and Technology of China",
        "Hefei Institutes of Physical Science"
      ]
    },
    {
      "id": "https://openalex.org/A2521102149",
      "name": "Shijian Zheng",
      "affiliations": [
        "Hefei Institutes of Physical Science",
        "Institute of Intelligent Machines",
        "Chinese Academy of Sciences",
        "Southwest University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2119502692",
      "name": "Lin Jiao",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Anhui University",
        "Hefei Institutes of Physical Science",
        "Institute of Intelligent Machines"
      ]
    },
    {
      "id": "https://openalex.org/A2112173110",
      "name": "Liusan Wang",
      "affiliations": [
        "Institute of Intelligent Machines",
        "Chinese Academy of Sciences",
        "Hefei Institutes of Physical Science"
      ]
    },
    {
      "id": "https://openalex.org/A2237752352",
      "name": "Rujing Wang",
      "affiliations": [
        "Institute of Intelligent Machines",
        "University of Science and Technology of China",
        "Chinese Academy of Sciences",
        "Hefei Institutes of Physical Science"
      ]
    },
    {
      "id": "https://openalex.org/A2169680355",
      "name": "Qiong Zhou",
      "affiliations": [
        "Anhui Agricultural University",
        "Institute of Intelligent Machines",
        "Hefei Institutes of Physical Science"
      ]
    },
    {
      "id": "https://openalex.org/A2896054121",
      "name": "Ziliang Huang",
      "affiliations": [
        "Hefei Institutes of Physical Science",
        "Institute of Intelligent Machines"
      ]
    },
    {
      "id": "https://openalex.org/A2521102149",
      "name": "Shijian Zheng",
      "affiliations": [
        "Southwest University of Science and Technology",
        "Hefei Institutes of Physical Science",
        "Institute of Intelligent Machines"
      ]
    },
    {
      "id": "https://openalex.org/A2119502692",
      "name": "Lin Jiao",
      "affiliations": [
        "Hefei Institutes of Physical Science",
        "Anhui University",
        "Institute of Intelligent Machines"
      ]
    },
    {
      "id": "https://openalex.org/A2112173110",
      "name": "Liusan Wang",
      "affiliations": [
        "Institute of Intelligent Machines",
        "Hefei Institutes of Physical Science"
      ]
    },
    {
      "id": "https://openalex.org/A2237752352",
      "name": "Rujing Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3018757597",
    "https://openalex.org/W2930246464",
    "https://openalex.org/W6746085279",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2061563558",
    "https://openalex.org/W6797094956",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3090977627",
    "https://openalex.org/W2932062063",
    "https://openalex.org/W2579985080",
    "https://openalex.org/W6631782140",
    "https://openalex.org/W6675026286",
    "https://openalex.org/W3118028009",
    "https://openalex.org/W2901871634",
    "https://openalex.org/W6735463952",
    "https://openalex.org/W6725739302",
    "https://openalex.org/W2624387057",
    "https://openalex.org/W3173399583",
    "https://openalex.org/W6792155083",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2899128648",
    "https://openalex.org/W3012947883",
    "https://openalex.org/W6761040001",
    "https://openalex.org/W6745737171",
    "https://openalex.org/W6628973269",
    "https://openalex.org/W4293584584",
    "https://openalex.org/W639708223",
    "https://openalex.org/W6764024669",
    "https://openalex.org/W2971281060",
    "https://openalex.org/W6760947256",
    "https://openalex.org/W2995725277",
    "https://openalex.org/W6790555728",
    "https://openalex.org/W3136958399",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W3171891302",
    "https://openalex.org/W6770992763",
    "https://openalex.org/W3011886372",
    "https://openalex.org/W6788620109",
    "https://openalex.org/W4394657967",
    "https://openalex.org/W2806070179",
    "https://openalex.org/W3199936014"
  ],
  "abstract": "Wheat spike detection has important research significance for production estimation and crop field management. With the development of deep learning-based algorithms, researchers tend to solve the detection task by convolutional neural networks (CNNs). However, traditional CNNs equip with the inductive bias of locality and scale-invariance, which makes it hard to extract global and long-range dependency. In this paper, we propose a Transformer-based network named Multi-Window Swin Transformer (MW-Swin Transformer). Technically, MW-Swin Transformer introduces the ability of feature pyramid network to extract multi-scale features and inherits the characteristic of Swin Transformer that performs self-attention mechanism by window strategy. Moreover, bounding box regression is a crucial step in detection. We propose a Wheat Intersection over Union loss by incorporating the Euclidean distance, area overlapping, and aspect ratio, thereby leading to better detection accuracy. We merge the proposed network and regression loss into a popular detection architecture, fully convolutional one-stage object detection, and name the unified model WheatFormer. Finally, we construct a wheat spike detection dataset (WSD-2022) to evaluate the performance of the proposed methods. The experimental results show that the proposed network outperforms those state-of-the-art algorithms with 0.459 mAP (mean average precision) and 0.918 AP 50 . It has been proved that our Transformer-based method is effective to handle wheat spike detection under complex field conditions.",
  "full_text": "A wheat spike detection method\nbased on Transformer\nQiong Zhou1,2,3† , Ziliang Huang1,2† , Shijian Zheng1,4, Lin Jiao1,5*,\nLiusan Wang1* and Rujing Wang1,2*\n1Institute of Intelligent Machines, Hefei Institutes of Physical Science, Chinese Academy of\nSciences, Hefei, China,2Science Island Branch, University of Science and Technology of China,\nHefei, China,3College of Information and Computer, Anhui Agricultural University, Hefei, China,\n4Department of Information Engineering Southwest, University of Science and Technology,\nMianyang, China,5School of Internet, Anhui University, Hefei, China\nWheat spike detection has important research signiﬁcance for production\nestimation and crop ﬁeld management. With the development of deep\nlearning-based algorithms, researchers tend to solve the detection task by\nconvolutional neural networks (CNNs). However, traditional CNNs equip with\nthe inductive bias of locality and scale-invariance, which makes it hard to\nextract global and long-range dependency. In this paper, we propose a\nTransformer-based network named Multi-Window Swin Transformer (MW-\nSwin Transformer). Technically, MW-Swin Transformer introduces the ability of\nfeature pyramid network to extract mu lti-scale features and inherits the\ncharacteristic of Swin Transformer that performs self-attention mechanism\nby window strategy. Moreover, bounding box regression is a crucial step in\ndetection. We propose a Wheat Intersection over Union loss by incorporating\nthe Euclidean distance, area overlapping, and aspect ratio, thereby leading to\nbetter detection accuracy. We merge the proposed network and regression\nloss into a popular detection architecture, fully convolutional one-stage object\ndetection, and name the uniﬁed model WheatFormer. Finally, we construct a\nwheat spike detection dataset (WSD-2022) to evaluate the performance of the\nproposed methods. The experimental results show that the proposed network\noutperforms those state-of-the-art algorithms with 0.459 mAP (mean average\nprecision) and 0.918 AP\n50. It has been proved that our Transformer-based\nmethod is effective to handle whe at spike detection under complex\nﬁeld conditions.\nKEYWORDS\ndeep learning, IoU loss function, transformer, wheat spike detection, agriculture\nFrontiers inPlant Science frontiersin.org01\nOPEN ACCESS\nEDITED BY\nPo Yang,\nThe University of Shefﬁeld,\nUnited Kingdom\nREVIEWED BY\nLiu Liu,\nShanghai Jiao Tong University, China\nLe Zou,\nHefei University, China\n*CORRESPONDENCE\nRujing Wang\nrjwang@iim.ac.cn\nLiusan Wang\nlswang@iim.ac.cn\nLin Jiao\nljiao@ahu.edu.cn\n†These authors have contributed\nequally to this work and share\nﬁrst authorship\nSPECIALTY SECTION\nThis article was submitted to\nSustainable and Intelligent\nPhytoprotection,\na section of the journal\nFrontiers in Plant Science\nRECEIVED 20 August 2022\nACCEPTED 22 September 2022\nPUBLISHED 20 October 2022\nCITATION\nZhou Q,Huang Z,Zheng S,\nJiao L,Wang L andWang R\n(2022) A wheat spike detection\nmethod based on Transformer.\nFront. Plant Sci.13:1023924.\ndoi: 10.3389/fpls.2022.1023924\nCOPYRIGHT\n© 2022 Zhou, Huang, Zheng, Jiao,\nWang and Wang. This is an open-access\narticle distributed under the terms of\nthe Creative Commons Attribution\nLicense (CC BY).The use, distribution\nor reproduction in other forums is\npermitted, provided the original\nauthor(s) and the copyright owner(s)\nare credited and that the original\npublication in this journal is cited, in\naccordance with accepted academic\npractice. No use, distribution or\nreproduction is permitted which does\nnot comply with these terms.\nTYPE Original Research\nPUBLISHED 20 October 2022\nDOI 10.3389/fpls.2022.1023924\n1 Introduction\nWheat is one of the most important food crops in the world,\nwith an annual production of 730 million tons in around 215\nmillion ha (Catherine et al., 2014). As the global yield supports\napproximately 30% of the world population, wheat production\nestimation has become a focus of agricultural research. It could\nprovide key indicators for agricultural decision-making andﬁeld\nmanagement. Since wheat spike is a major factor that reﬂects the\ngrain number per unit area, it is signiﬁcant to accurately detect\nthe wheat spike for estimating crop yield.\nTraditional ﬁeld yield estimation methods are time-\nconsuming, inefﬁcient, and poorly representative, so they are\nnot suitable for current large-scale yield forecasting tasks. With\nthe development of computer vision, many researchers have\nconducted research through machine learning techniques.Fang\net al. (2020)proposed to estimate the wheat tiller density based\non terrestrial laser scanning data. Fernandez-Gallego et al.\n(2019) used zenithal/nadir thermal images to count the\nnumber of wheat spikes. Jin et al. (2017) adopted unmanned\naerial vehicles (UAVs) to obtain high-resolution imagery for\nestimating wheat plant density. In these traditional machine\nlearning studies, image texture, geometry, and color intensity are\nprimarily used to discriminate spikes. However, the process is\npartly manually designed to deﬁne the range and threshold in\nthe model. They are not robust enough for different situations\nwith dense distribution, complex structural environments, and\nsevere occlusion in theﬁeld (Zhang et al., 2020a). Convolutional\nneural networks (CNNs) have been introduced into the research\nof wheat spike detection in recent studies.Khoroshevsky et al.\n(2021) suggested that a network incorporates multiple targets in\na single deep model, and the results show that the method is\neffective as a yield estimator.Misra et al. (2020)combined digital\nimage analysis with CNN techniques to identify and count wheat\nspikes. CNNs are effective to extract local information, but they\nlack the ability to extract long-range features from global\ninformation. Due to the ﬁeld environment of wheat being\ncomplex, i.e., dense distribution, complex structural\nenvironment, and severe occlusion, it is hard for CNNs to\nperform well.\nThe evolution of Transformer (Vaswani et al., 2017)i n\nnatural language processing (N LP) provides an alternative\npath, and many researchers have subsequently transferred the\nNLP models to computer vis ion models. Compared with\nconventional CNN backbones, Transformers always produce\nglobal receptive ﬁelds rather than local receptiveﬁelds, which\nis more suitable for detecting objects in complex backgrounds.\nThe Transformer architecture avoids repetition and instead\nrelies entirely on the attention mechanism to map the global\ndependencies between inputs and outputs. The signi ﬁcant\nsuccess in the natural language processing domain motivates\nresearchers to investigate the application in classi ﬁcation\n(Dosovitskiy et al., 2021 ) and dense prediction tasks\n(Bochkovskiy et al., 2020; Carion et al., 2020; Xizhou et al.,\n2020). There are two main challenges in transferring the NLP\nTransformer to the visual domain Transformer. Firstly, unlike\nthe word tokens that are the basic elements of a linguistic\nTransformer, the vision elements can be very different from\nthe NLP in scale. Another is that Transformer has high\ncomputational and memory costs for prediction tasks.\nBounding box regression is a key operation to locate the\ntarget object in detection tasks. The loss function is to calculate\nthe difference between the regression result and the true value\nand ﬁnally minimize the regression error. The ln−norm loss\nfunction is widely adopted in bounding box regression, while the\ncommon ln−norm loss (e.g. l1−norm or l2−norm ) is used for\nmeasuring the distance between bounding boxes. However,\naccording to the research of Yu et al. ( Yu et al., 2016 ;\nRezatoﬁghi et al, 2019), it is not tailored to the Intersection\nover Union (IoU) metric. IoU loss ( Yu et al., 2016 )a n d\ngeneralized IoU (GIoU) loss ( Rezatoﬁghi et al., 2019)h a v e\nrecently been suggested to improve the IoU metric. IoU loss\ncan be effective only when the bounding boxes overlap, but it is\nuseless for non-overlapping cases. GIoU adds a penalty term that\nthe predicted bounding box will move to the target box without\noverlapping. Nevertheless, GIoU empirically has a lower\nconvergence speed, and it will degrade to IoU loss for\nenclosing boxes (Zheng et al., 2020). Therefore, it is important\nto design an effective loss function for bounding box regression.\nIn this work, we aim to explore a Transformer-based\nnetwork for wheat spike detection. To the best of our\nknowledge, this is the ﬁrst attempt using Transformer in the\nwheat detectionﬁeld. Inspired by the novel architecture of Swin\nTransformer (Liu et al., 2021) and exploring to overcome the\nabove-mentioned limitations, we propose a Transformer-based\nnetwork named MW-Swin Transformer. It has the following\nadvantages: Firstly, compared with the conventional\nTransformer, the proposed T ransformer occupies the\nhierarchical architecture that is essential for downstream tasks.\nSecondly, compared with Swin Transformer, we inherit the\nexcellent network and design of a multi-window Transformer\nblock to extract target features with different scales. Thirdly, our\nmethod has three variants according to the number of stacked\nlayers, which is ﬂexible to ﬁt the actual requirements.\nFurthermore, we propose a WIoU loss for bounding box\nregression. Speciﬁcally, we add a penalty term on IoU loss,\nconsidering the overlap area, Euclidean distance, and aspect\nratio. The three geometric indicators are important, e.g., the\nEuclidean distance is used to minimize the distance of central\npoints in two bounding boxes, and the consistency of aspect\nratios is also bringing about an impact on IoU loss. We\nincorporate the proposed methods into the FCOS and name\nthe new model WheatFormer, as illustrated in Figure 1 .\nWheatFormer contains two major parts: the multi-window\nSwin (MW-Swin) Transformer and the wheat detector. The\ninput image is split into non-overlapping patches, and each\nZhou et al. 10.3389/fpls.2022.1023924\nFrontiers inPlant Science frontiersin.org02\np a t c hi sr e g a r d e da sat o k e na n df e di n t ot h eM W - S w i n\nTransformer backbone to learn long-range features from\nglobal information. Then, the extracted feature maps are fed\ninto the one-stage detector to locate the wheat spike. Finally, we\nconstruct a wheat spike detection dataset named WSD-2022 to\nevaluate the performance of the proposed WheatFormer. The\ndataset contains 6,404 images from two data sources, theﬁrst\nwas from the Global Wheat Head Detection (GWHD) dataset\n(David et al., 2021) and the second was collected in theﬁeld\nenvironment by our collaborators. The major contributions of\nour work are as follows:\n● We propose the MW-Swin Transformer with multiple\nwindows for different scale objects, which inherits from\nthe shifted windows in Swin Transformer. This strategy\nbrings a much lower latency than those previous\nTransformer models, leading to strong performance\ndue to the global receptiveﬁeld.\n● A WIoU loss function is proposed for bounding box\nregression, considering th ree important geometric\nindicators. WIoU helps the network achieve a better\nperformance than normal IoU loss and other improved\nIoU loss functions.\n● We build the WSD-2022 dataset for detecting wheat\nspikes. This dataset contains wheat spike images from\ndifferent regions and different developmental stages.\nOur work provides a richer benchmark dataset for\nwheat spike detection tasks.\n2 Related work\n2.1 CNN-based methods in wheat\nspike detection\nCNNs have been widely used in computer vision tasks, such\nas image classiﬁcation (Huang et al., 2017), object detection (Ren\net al., 2017), and semantic segmentation (He et al., 2017), which\nhave achieved excellent achievements. Differently from\ntraditional machine lear ning methods, CNNs can\nautomatically abstract features without manual intervention.\nSadeghi-Tehran et al. (2019) proposed a low-computational-\ncost system to automatically detect the number of wheat spikes,\nwhich used simple linear iterative clustering with CNN.Hasan\net al. (2018)introduced a robust R-CNN model for the accurate\ndetection, counting, and analysis of wheat ears for yield\nestimation. Wang et al. (2019)provided a method based on a\nfully convolutional network and Harris corner detection, solving\nthe problem of counting wheat ears inﬁeld conditions. Madec\net al. (2019)used Faster R-CNN to provide accurate ear density\nusing RGB images taken from the UAV.Pound et al. (2017)\ninvestigated a deep learning method capable of accurately\nlocalizing wheat ears and spikelets. Gong et al. (2020)\nproposed a novel object method of wheat head detection based\non dual SPP networks to enhance the speed and accuracy of\ndetection. Yang et al. (2021)combined the convolutional neural\nnetwork and attention mechanism technology to propose a\nCBAM-YOLOv4 wheat ear detection and counting method.\nFIGURE 1\nThe main architecture of WheatFormer.\nZhou et al. 10.3389/fpls.2022.1023924\nFrontiers inPlant Science frontiersin.org03\n2.2 Object detection\nObject detection methods can be divided into two groups:\nwith two stages and with one-stage. For two-stage detectors, the\nﬁrst stage is to produce lots of high-quality region proposals by a\nproposal generator, and the second stage is classifying and\nreﬁning the proposals by region-wise subnetworks. R-CNN\n(Girshick et al., 2014) and Fast R-CNN (Girshick, 2015) are\nthe typical networks of two-stage detectors, which combined the\nregion proposals and CNN for object detection. Faster R-CNN\n(Ren et al., 2017) was proposed to speed up Fast R-CNN and\npromote detection accuracy by using region proposal network.\nOther two-stage detectors mainly include Mask R-CNN (He\net al., 2020), Libra R-CNN (Pang et al., 2019), and Cascade R-\nCNN ( Cai and Vasconcelos, 2018 ). However, two-stage\ndetectors show a weakness in detection ef ﬁciency (Redmon\net al., 2016). For one-stage detectors, they drop the process of\ngeneration region proposals, treating the object detection task as\na single shot problem, such as the YOLO series networks: YOLO\n(Redmon et al., 2016), YOLOv3 (Redmon and Farhadi, 2018),\nand YOLOv4 (Bochkovskiy et al., 2020). Tian et al. (2019)\nproposed a fully convolutional one-stage object detector. This\nmethod avoided the complex computation by eliminating the\npredeﬁned set of region proposals. SSD ( Fu et al., 2017 )\nintroduced additional cont ext into the popular general\nobject detection.\n2.3 Vision Transformer\nThe Transformer is proposed by Vaswani et al. (2017),\nwhich is widely used in NLP tasks. Recently, the pioneering\nwork of vision Transformer ViT ( Dosovitskiy et al., 2021 )\ndemonstrated that the pure Transformer-based model can also\nachieve competitive performance in vision tasks. Based on the\nsuccess of ViT, many studies have on designing more advanced\nTransformer base networks been published, including image\nprocessing (Wan et al., 2021), classiﬁcation (Wang et al., 2021),\nobject detection (Carion et al., 2020), and semantic segmentation\n(Zheng et al., 2021). However, the normal ViT-based models are\nnot compatible with many downstream tasks due to the high\ncomputational cost. To alleviate the limitations, an efﬁcient and\neffective hierarchical Transformer named Swin Transformer\n(Liu et al., 2021) was proposed as a uniﬁed vision backbone.\nSwin Transformer designed the shifted windows mechanism,\nachieving state-of-the-art performance in many downstream\ntasks. We introduce Swin Transformer due to its excellent\ncharacteristics, and the hierarchical architecture is designed to\nreduce the complex computation by progressively decreasing the\nshape of feature maps.\n3 Materials and methods\n3.1 Dataset\nWe built a wheat spike detection dataset named WSD-2022,\ncontaining a total of 6,404 images, of which 978 images we\ncollected ourselves in the ﬁeld environment. We conducted\nwheat image collection in four locations, including Dangtu\nCounty, Ma’anshan; Feidong County, Hefei; Guizhi District,\nChizhou; and Susong County, Anqing. The images were\ncollected from April 18 to May 10, 2021 from theﬂowering\nstage to the milk stage of maturity. We collected the wheat spikes\nof varieties with different colors, shapes, and densities, thus\nincreasing the diversity of the data. We shot the images using\ndifferent types of cameras at different shooting angles and\ndistances to collect image data under different lighting\nconditions to enhance the robustness of the model. About 80%\nof the images were captured at a resolution of over 3,000*4000\npixels. The captured images need to label each wheat spike, and we\nuse LabelImg software to annotate the bounding boxes around the\nwheat spikes. Each wheat spike is labeled with a bounding box, the\nannotation is represented as a vector (x,y,w,h) where (x,y) are the\ncoordinates of the upper left and (w,h) are the width/height of the\nbounding box.Figure 2shows some examples of WSD-2022. Due\nto the different shooting angles, different lighting conditions,\ndifferent wheat growth periods, different wheat distribution\ndensities, and different wheat spike sizes, we can ﬁnd the\ndiversity and complexity of the dataset. We randomly split the\nWSD-2022 into training and validation subsets at a ratio of 8:2.\nThe details of the two subsets are summarized inTable 1.\n3.2 MW-Swin Transformer\n3.2.1 Overall architecture\nThis section describes the design of MW-Swin Transformer.\nThe pyramid structure was introduced based on the Transformer\nmodel to generate hierarchical feature maps for downstream tasks.\nThe overall architecture of MW-Swin Transformer is similar to\nCNN networks. As shown in (Figure 1). For an input image with\nsize of H*W*3 , we follow Swin Transformer to split the image\ninto patches atﬁrst (we treat each patch as a“token”); the patch\nsize is 4*4. By such approach, the feature dimension of each patch\nbecomes 4*4*3 = 48. Then, a linear embedding layer is employed\nto project the feature dimension to arbitrary dimension (set asC ).\nTo produce hierarchical feature representation, the model\narchitecture consists of four stages; a patch merging layer is\nadded after each stage for down-sampling (reduce the number\nof tokens, which is similar to the pooling layer in CNN).\nIn the ﬁrst stage, we divide the input image intoHW/4\n2\npatches, with a size of 4*4*3 for each of them. Through the linear\nZhou et al. 10.3389/fpls.2022.1023924\nFrontiers inPlant Science frontiersin.org04\nembedding layer, we feed the ﬂattened patches to MW-Swin\nTransformer blocks (the number of blocks is represented byN ),\nand the output is reshaped to a feature map with a size ofH/\n4*W/4*C1 (represented asF1 ). The patch merging layer down-\nsampled each feature mapFi,i={1,2,3,4} with strides [4, 8, 16, 32]\nwith respect to the size of the input image. The output\ndimensions of Fi is set toCi,i={1,2,3,4} . Therefore, the output\nresolution of each stage isH/4*W/4*C1 , H/8*W/8*C2 , H/16*W/\n16*C3 , andH/32*W/32*C4 , respectively. With the hierarchical\nstructure, our model possesses the progressive shrinking strategy\nthat adjusts the output scale of each stage so that we can easily\napply the model to downstream tasks.\n3.2.2 MW-Swin Transformer block\nTransformer obtains the powerful ability of long-range\ncontext modeling, but the computation complexity of\nconventional Transformer is quadratic to feature map size. For\ndense prediction tasks with high-resolution images as input,\nusing conventional Transformer is expensive. Therefore, Swin\nTransformer is proposed to perform self-attention by non-\nTABLE 1 Number of images in the WSD-2022 dataset.\nWSD-2022 Train Validation Total\nOurs 782 196 978\nGWHD 4,309 1,117 5,426\nTotal 5,091 1,313 6,404\nFIGURE 2\nSamples of the WSD-2022 dataset. Theﬁrst and second rows of theﬁgure show the images that we acquired, while the third and fourth rows of\nthe ﬁgure come from GWHD.\nZhou et al. 10.3389/fpls.2022.1023924\nFrontiers inPlant Science frontiersin.org05\noverlapping local windows and shifted windows. However, the\nwindow size of Swin Transformer is ﬁxed, which is not\nconducive to detecting objects of different sizes. To enlarge the\nreceptive ﬁeld and obtain global self-attention moreﬂexibly, we\npropose the MW-Swin Transformer; the architecture is similar\nto the feature pyramid network, using different-sized windows to\ndetect objects across a large range of scales.\nAs shown in Figure 3 , two consecutive MW-Swin\nTransformer blocks are presented. Each block contains two\nLayerNorm ( Bosilj et al. 2020 ) layers, a multi-head self-\nattention (MSA), and a multilayer perceptron (MLP). The\nmulti-window MSA (MW-MSA) and the shifted multi-\nwindow MSA (SMW-MSA) are adopted in the consecutive\nTransformer blocks, respectively. With the MW-MSA module\nand the SMW-MSA module, consecutive MW-Swin\nTransformer blocks can be represented as:\n/C22zl = MW − SMA(LN(zl−1)) +zl−1\n/C22zl = SR(/C22zl)\nzl = MLP(LN(/C22zl)) +/C22zl (1)\n/C22zl+1 = SMW − SMA(LN(zl)) +zl\n/C22zl+1 = SR(/C22zl+1)\nzl+1 = MLP(LN(/C22zl+1)) +/C22zl+1\nwhere /C22zl and zl represent the outputs of (S)MW-SMA module\nand the MLP for the block, respectively. MW-MSA equalsConcat\n(W−MSA(zl−1)1,W−MSA(zl−1)2,W−MSA(zl−1)3) , whereW−MSA\n()i,i=1,2,3 indicates theith window with sizeX , and we setX=\n[7,9,11] in experiments. SR() denotes the spatial reduction\nmodule to reduce the spatial scale of /C22zl, which reduces the\nmemory and computational cost. Similar to the conventional\nTransformer (Dosovitskiy et al., 2021; Liu et al., 2021), the\nattention operation can be computed as follows:\nAttention(Q, K, V)= Soft max (QKT\nﬃﬃ ﬃ\nd\np + B)V (2)\nwhere Q,K,V represent the query, key, and value matrices;\nthe other parameters are in accordance with Swin Transformer.\nCompared with the previous MSA in vision Transformers,\nthe MW-MSA controls the computation area in multi-window\nas a unit. It reduces the complexity and computational cost,\nenhancing the ability to detect multi-scale features. MW-Swin\nTransformer block can serve as a plug-and-play block to replace\nthe raw Transformer block in Swin Transformer, with only\nminor modiﬁcations to the vanilla structure.\n3.2.3 Architecture variants\nWe named the base model WheatFormer-B, which is a\ntrade-off between efﬁciency and accuracy. Considering higher\nFIGURE 3\nMW-Swin Transformer block.\nZhou et al. 10.3389/fpls.2022.1023924\nFrontiers inPlant Science frontiersin.org06\nefﬁciency needs in some cases, we have introduced a small\nversion named WheatFormer-S. On the other hand, when\naccuracy needs to be considered more, we have introduced a\nlarge version named WheatFormer-L. The architectures of our\nbase model and variants are listed inTable 2.\n3.3 Wheat detector\n3.3.1 One-stage object detector\nFCOS is a one-stage anchor-free object detection algorithm\n(Tian et al., 2019 ) with higher accuracy and faster speed\ncompared with the representative model Faster R-CNN (Ren\net al., 2017) and other two-stage detectors. FCOS mainly consists\nof three parts: a feature extraction backbone, a feature pyramid\nnetwork (FPN), and a detection head. The backbone extracts\nmulti-level features of the input image. Then, low-level spatial\ninformation and high-level semantic information are fed into\nFPN, generating multi-scale feature maps. In previous research,\nlow-level information can obtain more detailed texture\ninformation, which leads to more ef ﬁcient detection. High-\nlevel information gets more semantic information and is more\nsuitable for classiﬁcation. FCOS is a pixel-based detector, which\nmeans that each pixel on the feature map is used for regression.\nFirst, each pixel map back to the original input image, and a pixel\nconsiders a positive sample if its location falls within any\nground-truth box with the correct class label. Otherwise, it is a\nnegative sample. As for regression, FCOS uses a vectort\n*=(l*,t*,\nr*,b*) , wherel*,t*,r*,b* denote the distances from the location\n(x,y) to the four sides of the bounding box, as shown inFigure 4.\nThe target regression process can be formulated as follows:\nl* = x − x0\n(i)\nt* = y − y0\n(i)\nr* = x1\n(i) − x\nb* = y1\n(i) − y\n(3)\nwhere (x0\n(i),y0\n(i)) and represent coordinates of the left-top\nand right-bottom corners of the bounding box.\n3.3.2 WIoU loss\nThe training loss function of the proposed WheatFormer\nmainly obtains three branch loss functions:\nLWheatFormer = 1\nNpos\nLcls + l1\nNpos\nLcenter−ness + l2\nNpos\nLreg (4)\nwhere Lcls and Lcenter−ness represent the classiﬁcation and\ncenter-ness loss function which are designed in FCOS. Npos\ndenotes the number of positive pixels.l1 and l2 are balance\nweights to adjust the proportions of three branch loss functions.\nThe parameters follow the settings inTian et al. (2019). FCOS\nuses IoU loss to calculate the regression loss, which can be\nformulated as follows:\nLreg = ox,y∈(Rp∪Rn)(1 − IoU(P rx,y, Gtx,y)) (5)\nwhere Rp represents the positive sample region and Rn\ndenotes the negative sample region.Gti,j indicates the ground\ntruth localization of the pixel (x,y) , while Pri,j denotes the\npredicted target of (x,y).\nThe IoU loss regresses all bound variables as a whole for\njoint regression and directly enforces the maximum overlap\nbetween the prediction bounding box and the ground truth. The\nIoU loss leads to faster convergence and more accurate\nlocalization compared with theln−norm loss used in previous\nstudies. However, the IoU loss cannot provide moving gradients\nfor non-overlapping cases,i.e., IoU loss is only valid when the\nbounding boxes overlap. Based on previous researches and the\nIoU loss, we consider three important geometric metrics, which\nare the overlap region, Euclidean distance, and aspect ratio of\nbounding boxes. In summary, we add a penalty term to the IoU\nloss, named WIoU loss. The ne wl o s sf u n c t i o nd i r e c t l y\nminimizes the Euclidean distance between the predicted box\nand the ground truth. At the same time, we take into account the\neffect of the consistency of aspect ratios. The WIoU loss function\nis deﬁned as follows:\nL\nreg = ox,y∈(Rp∪Rn)(1 − IoU(P rx,y, Gtx,y)+ y ║Prx,y, Gtx,y ║2 )\ny = 4\np2 ( arctanwGt\nx,y\nhGtx,y − arctan wPr\nx,y\nhPrx,y )2\n(6)\nwhere y measures the consistency of the aspect ratio and\nplays the role of regularization for the distance between the\npredicted bounding box and the target bounding box.wGt and\nhGt represent the width and height of the ground truth.wPr and\nhPr represent the width and height of the predicted bounding\nbox. The optimization of WIoU loss is the same as the IoU loss.\nTABLE 2 Detailed settings of WheatFormer variants.\nModels C1,C2,C3,C4 N1,N2,N3,N4 #Head #Expansion #Params (MB)\nWheatFormer-S [96, 192, 384, 768] [2, 2, 2, 2] 32 a=4 42.4\nWheatFormer-B [96, 192, 384, 768] [2, 2, 6, 2] 32 a=4 60.1\nWheatFormer-L [96, 192, 384, 768] [2, 2, 18, 2] 32 a=4 100.6\nCi , channel number of the hidden layers in each stage; Ni , layer numbers in each stage; #Head, query dimension of each head; #Expansion, expansion layer of each multilayer perceptron;\n#Params, amount of model parameters.\nZhou et al. 10.3389/fpls.2022.1023924\nFrontiers inPlant Science frontiersin.org07\n4 Experiments and discussionAP\n4.1 Experimental settings\nAll the experiments were performed using the Pytorch deep\nlearning frame, and the operation system was Ubuntu 18.04 with\nCUDA10.1. We use a piece of NVIDIA TITAN RTX GPU, Intel\nCore i9-9900k CPU with 128GB RAM. Furthermore, we train\nour model with the AdamW (Loshchilov and Hutter, 2017)\noptimizer for 24 epochs. The initial learning rate is 1e−4 , and\nthe weight decay is 0.05. The settings of comparison networks\nfollow the original settings.\n4.2 Evaluation metrics\nIn our experiments, we use the evaluation metrics as the\nmetric deﬁnition of the COCO dataset. Average precision (AP )i s\nthe area surrounded by the precision-recall curve. The deﬁnition\nof AP is deﬁned as Formula 7.AP@50 (AP50 ) means the value\nFIGURE 4\nRegression method of FCOS.l*, t*, r*, andb* represent the distances from the pixel to the left, top, right, and bottom, respectively, of the\nbounding box.\nZhou et al. 10.3389/fpls.2022.1023924\nFrontiers inPlant Science frontiersin.org08\nwhen IoU is equal to 0.5,AP@75 (AP75 )i st h eAP value when the\nIoU equals 0.75, and the meanAP ( mAP ) is the threshold of the\nIoU from 0.5 to 0.95 (AP@[0.5:0.05:0.95] ) with a step size of 0.05.\nprecision = TP\nTP+FP\nrecall = TP\nTP+FN\nAP =\nZ 1\n0\nprecision(recall)d(recall)\n(7)\nwhere TP (true positive), FP (false positive), and FN (false\nnegative) represent the number of correctly detected wheat\nspikes, false detected wheat spikes, and missing detected wheat\nspikes. At the same time, we useAPs , APm , APl deﬁned in the\nCOCO dataset in our experiments, which represent the detection\naccuracy for different target sizes. Considering that the wheat\nspike in the dataset occupies a larger proportion of the image, we\nonly applyAPm (for medium targets) andAPl (for large targets)\nas the evaluation metric. In the ﬁeld of object detection, AP\nmetric is widely adopted for evaluating the comprehensive\ndetection performance of the model.\n4.3 Model performance\nThe experiments in this section aim to demonstrate the\neffectiveness of the proposed method in terms of detection\nperformance. We compared seven state-of-the-art algorithms,\nincluding Faster R-CNN (Madec et al., 2019), Mask R-CNN (He\net al., 2020), FCOS (Tian et al., 2019), ATSS (Zhang et al.,\n2020b), SSD (Fu et al., 2017), Centernet (Zhou et al., 2019), and\nYOLOv3 (Redmon and Farhadi, 2018). Faster R-CNN and Mask\nR-CNN are two-stage networks, and the rest are one-stage\nnetworks. The experimental results are listed inTable 3, and\nwe can ﬁnd that the proposed WheatFormer outperforms the\nother models. To be speciﬁc, compared with the two-stage CNN-\nbased models, WheatFormer achieves about 10–20% higher in\nAP50 and 8–15% improvement in. Compared with the one-stage\nCNN models, our model increases theAP50 and mAP by 1.2–\n11.5 and 2.2–9.5%, respectively. In terms of Swin Transformer-\nbased models, the detection performance is generally better than\nthe CNN-based models. The FCOS-based Swin Transformer\nachieves a mAP of 0.452, while our model increasesmAP by\n0.7% and AP\n50 by 3.2%. The Mask R-CNN based on Swin\nTransformer achieves theAP50 of 0.914, which is comparable to\nthat of WheatFormer, but our model gets a highermAP of 3.3%.\nConsidering the model parameters, our model achieves a larger\nsize than most CNN models but is similar to Swin Transformer-\nbased models. We show some comparison examples inFigure 5\nand the detection results of WheatFormer inFigure 6. Figure 5\nshows that Faster R-CNN has too many overlapping prediction\nboxes, and YOLOv3 obtains too many missing boxes. At the\nsame time, WheatFormer obtains a higher accuracy than the\ncomparison models in classiﬁcation. InFigure 6, we canﬁnd that\nWheatFormer has excellent detection performance at different\nshooting angles, different light conditions, different wheat\ngrowth periods, different wheat distribution densities, and\ndifferent wheat spikes sizes. WheatFormer can accurately\nidentify most wheat spikes even at high density and high\nocclusion. This intuitively illustrates the excellent performance\nof WheatFormer.\n4.4 Ablation experiments\nAs mentioned, the major drawbacks of CNN models are the\nconsistently produced local receptiveﬁelds, which are unsuitable\nTABLE 3 Detection results on WSD-2022.\nMethod Backbone mAP AP 50 AP75 APm APl #Params (MB)\nFaster R-CNN ResNet50 0.301 0.709 0.215 0.284 0.339 39.4\nMask R-CNN 0.345 0.774 0.237 0.311 0.382 41.9\nFaster R-CNN ResNet101 0.304 0.750 0.208 0.306 0.352 57.6\nMask R-CNN 0.366 0.812 0.246 0.331 0.394 60.1\nFCOS ResNet50 0.368 0.825 0.250 0.355 0.409 30.6\nATSS 0.364 0.803 0.255 0.357 0.402 30.6\nSSD SSDVGG 0.428 0.890 0.362 0.382 0.488 22.7\nCenterNet ResNet18 0.414 0.876 0.318 0.345 0.487 13.8\nYOLOv3 DarkNet53 0.437 0.906 0.381 0.387 0.497 58.7\nFaster R-CNN Swin Transformer 0.397 0.881 0.276 0.352 0.450 65.6\nMask R-CNN 0.426 0.914 0.318 0.379 0.473 68.1\nFCOS 0.452 0.886 0.402 0.415 0.523 43.8\nWheatFormer MW-Swin Transformer 0.459 0.918 0.384 0.415 0.533 60.1\nFaster R-CNN and Mask R-CNN are the representative models of two stages. FCOS, ATSS, SSD, CenterNet, and YOLOv3 are the representative models of one stage.\nZhou et al. 10.3389/fpls.2022.1023924\nFrontiers inPlant Science frontiersin.org09\nfor detecting objects in complex backgrounds. There are\nrelatively few studies on Transformers-based backbone applied\nto wheat spike detection. We conduct ablation experiments to\nrepresent the effectiveness of our proposed methods.\n4.4.1 Effect of the MW-Swin Transformer\nIn this part, we describe the effectiveness of the proposed\nMW-Swin Transformer. The results are listed inTable 4, which\ncontains three backbones: the CNN backbone, the Swin\nTransformer backbone, an dt h eM W - S w i nT r a n s f o r m e r\nbackbone. Obviously, the Swin Transformer backbone-based\nmodels greatly improve the detection performance of the state-\nof-the-art algorithms. For a detailed representative comparison\nof different backbones, we show the precision–recall curve of\nWheatFormer inFigure 7. Speciﬁcally, compared with the CNN\nbackbone and the Swin Transformer backbone, the\nWheatFormer boosts the Loc, Sim, Oth, and BG to 0.964,\n0.964, 0.964, and 0.990. It obtains 9.1% improvements on\nmAP and 9.3% improvements on AP\n50 after replacing the\nbackbone with MW-Swin Transformer. This indicates that the\nproposed Transformer can effectively increase the detection\nability of the detectors.\n4.4.2 Effect of the WIoU loss\nThe loss function plays an important role in the deep\nlearning training process. To further validate the performance\nof the proposed WioU loss, we conduct experiments comparing\nIoU, GioU, and CioU (Zheng et al., 2020). We present the\ncomparison results inTable 5. We canﬁnd that GioU, CioU, and\nWioU make further detection improvements than the original\nIoU loss for most cases— for instance, the WheatFormer with\nWioU loss obtains 0.452mAP , which is 2.9% higher than the\nIoU-based model, 1% higher than the GioU-based model, and\n2.4% higher than the CioU-based model. Therefore, we can\nconclude that the WheatFormer can obtain better detection\nperformance when trained with WioU loss.\n4.4.3 Performance of the variant models\nAs mentioned, we constructed three different variants of\nWheatFormer, and the detection results are shown inTable 6.\nFIGURE 5\nVisualization of the comparative models. The left column represents the result of Faster R-CNN, the middle column represents the result of\nYOLOv3, and the right column represents the result of WheatFormer.\nZhou et al. 10.3389/fpls.2022.1023924\nFrontiers inPlant Science frontiersin.org10\nWheatFormer-S obtains 42.4 MB parameters, similar to the Swin\nTransformer-based FCOS (43. 8M B ) ,w h i l eW h e a t F o r m e r\nachieves 0.438 at mAP (1.4% lower than SSD) and 0.908 at\nAP50 (2.2% higher than Swin Transformer-based FCOS).\nWheatFormer-B obtains 60.1 MB parameters, the same as\nMask R-CNN. Nevertheless, our model achieves 0.459 atmAP\n(9.3% higher than Mask R-CNN) and 0.918 at AP50 (10.6%\nhigher than Mask R-CNN), which signiﬁcantly surpasses the\ndetection ability of Mask R-CNN. The large version obtains\nparameters of 100.6 MB, showing a better performance than the\nprevious versions.\n4.5 Limitations and future work\nIn this work, we conduct extensive experiments to evaluate the\neffectiveness of the proposed methods. The experimental results\nprove that the proposed methods can greatly improve the\nFIGURE 6\nVisualization of detected results by the WheatFormer.(A) Early maturity, 65 spikes per image, direct sunlight, and wheat ear group with 80°\nviewing angle of photographing,(B) ﬁlling stage, 75 spikes per image, diffuse light conditions, and wheat ear group with 45° viewing angle of\nphotographing, (C) ﬁlling stage, 45 spikes per image, diffuse light conditions, and wheat ear group with 45° viewing angle of photographing,(D)\nearly maturity, 25 spikes per image, diffuse light conditions, and wheat ear group with 90° viewing angle of photographing,(E) poplar blossom,\n23 spikes per image, direct sunlight, and wheat ear group with 45° viewing angle of photographing,(F) the milk stage of maturity, 30 spikes per\nimage, direct sunlight, and wheat ear group with 90° viewing angle of photographing,(G) poplar blossom, 27 spikes per image, direct sunlight,\nand wheat ear group with 30° viewing angle of photographing,(H) the milk stage of maturity, 22 spikes per image, diffuse light conditions, and\nwheat ear group with 90° viewing angle of photographing, and(I) the milk stage of maturity, 30 spikes per image, diffuse light conditions, and\nwheat ear group with 90° viewing angle of photographing.\nZhou et al. 10.3389/fpls.2022.1023924\nFrontiers inPlant Science frontiersin.org11\nA BC\nFIGURE 7\nPrecision– recall (PR) curves of WheatFormer with different backbones.(A) WheatFormer with convolutional neural network backbone.\n(B) WheatFormer with Swin Transformer backbone.(C) WheatFormer with MW-Swin Transformer backbone. C75: PR at threshold equals 0.75;\nC50: PR at threshold equals 0.50; Loc: PR at threshold equals 0.1, and location errors ignored without duplicate detections; Sim: PR after\nsupercategory false positives are removed; Oth: PR after all class confusions are removed; BG: PR after all background false positive are\nremoved; FN: PR after all remaining errors are removed.TABLE 5 Results of WheatFormer with different IoU loss functions.\nMethod IoU GioU CioU WioU mAP AP 50 AP75\nWheatFormer ✔ 0.423 0.894 0.322\n✔ 0.442 0.896 0.374\n✔ 0.428 0.900 0.326\n✔ 0.459 0.918 0.384\nBold values are the results of our experimental method.\nThe symbols \"✔\" means the method used in the model.\nTABLE 6 Comparison of variant models.\nMethod mAP AP 50 AP75 APm APl #Params (M)\nWheatFormer-S 0.438 0.908 0.366 0.402 0.516 42.4\nWheatFormer-B 0.459 0.918 0.384 0.415 0.533 60.1\nWheatFormer-L 0.466 0.927 0.400 0.422 0.524 100.6\nTABLE 4 Comparison of different backbones.\nMethod CNN backbone Swin Transformer MW-Swin Transformer AP50 AP75\nFaster R-CNN ✔ 0.301 0.709 0.215\n✔ 0.397 (9.6%↑) 0.881 (17.2% ↑) 0.276 (6.1% ↑)\n✔ 0.417 (2%↑) 0.893 (1.2% ↑) 0.315 (1.2% ↑)\nMask R-CNN ✔ 0.345 mAP 0.774 0.237\n✔ 0.426 (8.1%↑) 0.914 (14% ↑) 0.318 (8.1% ↑)\n✔ 0.433 (0.7%↑) 0.909 (0.5% ↓) 0.344 (2.6% ↑)\nCenternet ✔ 0.414 0.876 0.318\n✔ 0.436 (2.2%↑) 0.913 (3.7% ↑) 0.372 (5.4% ↑)\n✔ 0.448 (1.2%↑) 0.912 (0.1% ↑) 0.365 (0.7% ↓)\nWheatFormer ✔ 0. 368 0.825 0. 250\n✔ 0. 452 (8.4%↑) 0. 886 (6.1% ↑) 0. 402 (15.2 ↑)\n✔ 0. 459(0.7%↑) 0. 918(3.2%↑) 0. 384(1.8%↓)\nBold values are the results of our experimental method.\nThe symbols“↑” means the increase values compared to the previous method, \"↓\" means the decrease values compared to the previous method, and \"✔\" means the method used in the model.\nZhou et al. 10.3389/fpls.2022.1023924\nFrontiers inPlant Science frontiersin.org12\ndetection performance of wheat spike detection. Although\nWheatFormer has shown to be effective in wheat spike\ndetection tasks, there are still some limitations. It is worth\nnoting that the experiment is only perfomed on the WSD-2022\ndataset with a limited number of images. Moreover, our method\nattempts to improve the detection ability of the spike detector,\nwhile the parameters of our base model are relatively large. In\nfuture research, we will focus on solving the above-mentioned\nproblems. Firstly, we will collect more wheat spike images\ncontaining more regions and more growth cycles to validate our\nmethods. Secondly, we will continue to design more lightweight\nmodels to improve the capabilities for practical applications.\n5 Conclusions\nIn this paper, we explore a Transformer-based network for\nwheat spike detection within a newly constructed dataset. We are\nthe ﬁrst to introduce the Transformer for wheat spike detection.\nTo extract global and long-range semantic information, we\ndesign the MW-Swin Transformer as the backbone, and we\npropose the WioU loss function to improve positioning\naccuracy. Finally, we created a wheat spike dataset named\nWSD-2022 to verify the effectiveness of our model. The\nextensive experiments show that the method proposed in this\ns t u d yc a no b t a i na ne n c o u r a g i n gd e t e c t i o np e r f o r m a n c e\ncompared with those state-of-the-art algorithms. We hope that\nthis research will provide novel insights into the development of\nmore advanced detection methods in the agriculturalﬁeld.\nData availability statement\nThe original contributions presented in the study are\nincluded in the article/supplementary materials. Further\ninquiries can be directed to the corresponding authors.\nAuthor contributions\nQZ: conceptualization, methodology, software,\ninvestigation, formal analysis, and writing— original draft. ZH:\nconceptualization, methodology, software, investigation, formal\nanalysis, and writing — original draft. SZ: visualization and\ninvestigation. LJ, LW and RW: c onceptualization, funding\nacquisition, resources, supervision, and writing — review and\nediting. All authors contributed to the article and approved\nthe submitted version.\nFunding\nThis work was supported by the National Key R&D Program\nof China (2019YFE0125700) and the Natural Science\nFoundation of Anhui Province (2208085MC57).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could\nbe construed as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their afﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed\nor endorsed by the publisher.\nReferences\nBochkovskiy, A., Wang, C.-Y., and Liao, H.-Y. M. (2020). YOLOv4: Optimal\nspeed and accuracy of object detection.ArXiv abs 2004, 10934. doi: 10.48550/\narXiv.2004.10934\nBosilj, P., Aptoula, E., Duckett, T., and Cielniak, G. (2020). Transfer learning\nbetween crop types for semantic segmentation of crops versus weeds in precision\nagriculture. J. Field Robotics37, 7–19. doi:10.1002/rob.21869\nCai, Z., and Vasconcelos, N. (2018).“Cascade r-CNN: Delving into high quality\nobject detection,” in IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 6154–6162.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S.\n(2020). “End-to-End object detection with transformers,” in Computer vision –\nECCV 2020. Ed. A. Vedaldi, et al (Cham: Springer International Publishing),\n213–229.\nCatherine, F., Klaus, F., Mayer, X., Rogers, J., and Eversole, K. (2014). SLICING\nTHE WHEAT GENOME.Science 345, 285–285. doi:10.1126/science.1257983\nDavid, E., Serouart, M., Smith, D., Madec, S., Velumani, K., Liu, S., et al. (2021)\nGlobal wheat head dataset 2021: more diversity to improve the benchmarking of\nwheat head localization methods. arXiv preprint arXiv:2105.07660.\nDosovitskiy, A., Beyer, L., Kolesnik o v ,A . ,W e i s s e n b o r n ,D . ,Z h a i ,X . ,\nUnterthiner, T., et al. (2021). An image is worth 16x16 words: Transformers for\nimage recognition at scale.ArXiv abs2010, 11929. doi:10.48550/arXiv.2010.11929\nFang, Y., Qiu, X., Guo, T., Wang, Y., and Gui, L. (2020). An automatic method\nfor counting wheat tiller number in theﬁeld with terrestrial LiDAR.Plant Methods\n16, 132. doi:10.1186/s13007-020-00672-8\nFernandez-Gallego, J., Buchaillot, M., Gutié rrez, N. A., Nieto-Taladriz, M.,\nAraus, J., and Kefauver, S. (2019). Automatic wheat ear counting using thermal\nimagery. Remote Sens.11 (7), 751. doi:10.3390/rs11070751\nFu, C.-Y., Liu, W., Ranga, A., Tyagi, A., and Berg, A. C. (2017). DSSD :\nDeconvolutional single shot detector. ArXiv abs, 1701.06659. doi: 10.48550/\narXiv.1701.06659\nZhou et al. 10.3389/fpls.2022.1023924\nFrontiers inPlant Science frontiersin.org13\nGirshick, R. (2015). “Fast r-CNN, ” in IEEE International Conference on\nComputer Vision (ICCV). 1440–1448.\nGirshick, R., Donahue, J., Darrell, T., and Malik, J. (2014). “Rich feature\nhierarchies for accurate object detection and semantic segmentation,” in IEEE\nConference on Computer Vision and Pattern Recognition. 580–587.\nGong, B., Ergu, D., Cai, Y., and Ma, B. (2020). Real-time detection for wheat\nhead applying deep neural network.Sensors 21 (1), 191. doi:10.3390/s21010191\nHasan, M. M., Chopin, J. P., Laga, H., and Miklavcic, S. J. (2018). Detection and\nanalysis of wheat spikes using convolutional neural networks.Plant Methods 14,\n100. doi:10.1186/s13007-018-0366-8\nHe, K. M., Gkioxari, G., Dollá r, P., and Girshick, R (2017).“Mask r-cnn,” In\nProceedings of the IEEE international conference on computer vision,( p p .2 9 6 1–2969).\nHe, K. M., Gkioxari, G., Dollar, P., and Girshick, R. (2020).“Mask r-CNN,” in\nIeee Transactions on Pattern Analysis and Machine Intelligence, Vol. 42. 386–397.\ndoi: 10.1109/tpami.2018.2844175\nHuang, G., Liu, Z., Van der Maaten, L., and Weinberger, K. Q. (2017).“Densely\nconnected convolutional networks,” In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition. pp. 4700–4708.\nJin, X., Liu, S., Baret, F., Hemerlé , M., and Comar, A. (2017). Estimates of plant\ndensity of wheat crops at emergence from very low altitude UAV imagery.Remote\nSens. Environ.198, 105–114. doi:10.1016/j.rse.2017.06.007\nKhoroshevsky, F., Khoroshevsky, S., and Bar-Hillel, A. (2021). Parts-per-Object\ncount in agricultural images: Solving phenotyping problems via a single deep\nneural network.Remote Sens.13, 2496. doi:10.3390/rs13132496\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., et al. (2021).“Swin transformer:\nHierarchical vision transformer using shifted windows,” In Proceedings of the IEEE/\nCVF International Conference on Computer Vision.1 0 0 1 2–10022.\nLoshchilov, I., and Hutter, F. (2017). Fixing weight decay regularization in\nAdam. ArXiv abs, 1711.05101. doi:10.48550/arXiv.1711.05101\nMadec, S., Jin, X., Lu, H., De Solan, B., Liu, S., Duyme, F., et al. (2019). Ear\ndensity estimation from high resolution RGB imagery using deep learning\ntechnique. Agric. For. Meteorol.264, 225–234. doi:10.1016/j.agrformet.2018.10.013\nMisra, T., Arora, A., Marwaha, S., Chinnusamy, V., Rao, A. R., Jain, R., et al.\n(2020). SpikeSegNet-a deep learning approach utilizing encoder-decoder network\nwith hourglass for spike segmentation and counting in the wheat plant from visual\nimaging. Plant Methods16 (1), 1–20. doi:10.1186/s13007-020-00582-9\nPang, J., Chen, K., Shi, J., Feng, H., Ouyang, W., and Lin, D. (2019).“Libra R-\nCNN: Towards balanced learning for object detection,” in IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR). 821–830.\nPound, M. P., Atkinson, J. A., Wells, D. M., Pridmore, T. P., and French, A. P.\n(2017). “Deep learning for multi-task plant phenotyping,” in IEEE International\nConference on Computer Vision Workshops (ICCVW). 2055–2063.\nRedmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016).“You only look\nonce: Uniﬁed, real-time object detection,” in IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR). 779–788.\nRedmon, J., and Farhadi, A. (2018). YOLOv3: An incremental improvement.\nArXiv abs1804.02767. doi:10.48550/arXiv.1804.02767\nRen, S., He, K., Girshick, R., and Sun, J. (2017). Faster r-CNN: Towards real-time\nobject detection with region proposal networks.IEEE Trans. Pattern Anal. Mach.\nIntell. 39, 1137–1149. doi:10.1109/tpami.2016.2577031\nRezatoﬁghi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I., and Savarese, S. (2019).\n“Generalized intersection over union: A metric and a loss for bounding box\nregression, ” in IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR). 658–666.\nSadeghi-Tehran, P., Virlet, N., Ampe, E. M., Reyns, P., and Hawkesford, M. J.\n(2019). DeepCount: In-ﬁeld automatic quantiﬁcation of wheat spikes using simple\nlinear iterative clustering and deep convolutional neural networks.Front. Plant Sci.\n10. doi:10.3389/fpls.2019.01176\nTian, Z., Shen, C., Chen, H., and He, T. (2019).“FCOS: Fully convolutional one-\nstage object detection,” in IEEE/CVF International Conference on Computer Vision\n(ICCV). 9626–9635.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need, proceedings of the 31st international conference on\nneural information processing systems (Long Beach, California, USA: Curran\nAssociates Inc.), 6000–\n6010.\nWang, D., Fu, Y., Yang, G., Yang, X., Liang, D., Zhou, C., et al. (2019). Combined\nuse of FCN and Harris corner detection for counting wheat ears inﬁeld conditions.\nIEEE Access7, 178930–178941. doi:10.1109/ACCESS.2019.2958831\nWang, W., Xie, E., Li, X., Fan, D. P., Song, K., Liang, D., et al. (2021).“Pyramid\nvision transformer: A versatile backbone for dense prediction without\nconvolutions, ” in IEEE/CVF International Conference on Computer Vision\n(ICCV). 548–558.\nWan, Z., Zhang, J., Chen, D., and Liao, J. (2021).“High-ﬁdelity pluralistic image\ncompletion with transformers,” in 18th IEEE/CVF International Conference on\nComputer Vision (ICCV 2021), IEEE.\nXizhou, Z., Weijie, S., Lewei, L., Bin, L., Xiaogang, W., and Jifeng, D. (2020).\nDeformable DETR: Deformable transformers for end-to-End object detection.\narXiv preprintarXiv:2010.04159 doi:10.48550/arXiv.2010.04159\nYang, B., Gao, Z., Gao, Y., and Zhu, Y. (2021). Rapid detection and counting of\nwheat ears in theﬁeld using YOLOv4 with attention module.Agronomy 11 (6),\n1202. doi:10.3390/agronomy11061202\nYu, J., Jiang, Y., Wang, Z., Cao, Z., and Huang, T. (2016).UnitBox: An advanced\nobject detection network, proceedings of the 24th ACM international conference on\nmultimedia (Amsterdam, The Netherlands: Association for Computing\nMachinery), 516–520.\nZhang, S., Chi, C., Yao, Y., Lei, Z., and Li, S. Z. (2020b).“Bridging the gap\nbetween anchor-based and anchor-free detection via adaptive training sample\nselection,” in IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR). 9756–9765.\nZhang, Q., Liu, Y., Gong, C., Chen, Y., and Yu, H. (2020). Applications of deep\nlearning for dense scenes analysis in agriculture.A Review Sensors (Basel)20 (5),\n1520. doi:10.3390/s20051520\nZheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., et al. (2021).“Rethinking\nsemantic segmentation from a seque nce-to-Sequence perspective with\ntransformers, ” in IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR). 6877–6886.\nZheng, Z., Wang, P., Liu, W., Li, J., Ye, R., and Ren, D. (2020). Distance-IoU loss:\nFaster and better learning for bounding box regression. InProceedings of the AAAI\nconference on artiﬁcial intelligence(Vol. 34, No. 07, pp. 12993–13000).\nZhou, X., Wang, D., and Krähenbühl, P. (2019). Objects as points.ArXiv abs,\n1904.07850. doi:10.48550/arXiv.1904.07850\nZhou et al. 10.3389/fpls.2022.1023924\nFrontiers in\nPlant Science frontiersin.org14",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7008811831474304
    },
    {
      "name": "Transformer",
      "score": 0.6192365288734436
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5757154226303101
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5586555004119873
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5462328195571899
    },
    {
      "name": "Object detection",
      "score": 0.5263998508453369
    },
    {
      "name": "Minimum bounding box",
      "score": 0.4285739064216614
    },
    {
      "name": "Machine learning",
      "score": 0.3338138461112976
    },
    {
      "name": "Voltage",
      "score": 0.13649258017539978
    },
    {
      "name": "Engineering",
      "score": 0.08825835585594177
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ]
}