{
    "title": "Real-time and lightweight detection of grape diseases based on Fusion Transformer YOLO",
    "url": "https://openalex.org/W4392109678",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2102849999",
            "name": "Yifan Liu",
            "affiliations": [
                "Tianjin University of Technology and Education"
            ]
        },
        {
            "id": "https://openalex.org/A2112029730",
            "name": "Qiudong Yu",
            "affiliations": [
                "Tianjin University of Technology and Education"
            ]
        },
        {
            "id": "https://openalex.org/A2150156429",
            "name": "Shuze Geng",
            "affiliations": [
                "Tianjin University of Technology and Education"
            ]
        },
        {
            "id": "https://openalex.org/A2102849999",
            "name": "Yifan Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2112029730",
            "name": "Qiudong Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2150156429",
            "name": "Shuze Geng",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3024660143",
        "https://openalex.org/W4282974753",
        "https://openalex.org/W2918927384",
        "https://openalex.org/W4210834027",
        "https://openalex.org/W4303184554",
        "https://openalex.org/W2889543275",
        "https://openalex.org/W2983849081",
        "https://openalex.org/W3184439416",
        "https://openalex.org/W4384281913",
        "https://openalex.org/W2999419508",
        "https://openalex.org/W6804951342",
        "https://openalex.org/W6743731764",
        "https://openalex.org/W4200266411",
        "https://openalex.org/W2981513382",
        "https://openalex.org/W4250685322",
        "https://openalex.org/W2938366133",
        "https://openalex.org/W4384340819",
        "https://openalex.org/W4316843048",
        "https://openalex.org/W3176187859",
        "https://openalex.org/W4281389393",
        "https://openalex.org/W3035016091",
        "https://openalex.org/W3043944662",
        "https://openalex.org/W4220947986",
        "https://openalex.org/W4365509318",
        "https://openalex.org/W3165651119",
        "https://openalex.org/W4311949872",
        "https://openalex.org/W3143527385",
        "https://openalex.org/W3195963910",
        "https://openalex.org/W2548258044",
        "https://openalex.org/W4365451359",
        "https://openalex.org/W4210741358",
        "https://openalex.org/W4388823657",
        "https://openalex.org/W4220941693",
        "https://openalex.org/W6849520326",
        "https://openalex.org/W4221049914",
        "https://openalex.org/W6799002949",
        "https://openalex.org/W3033097971",
        "https://openalex.org/W4383533709",
        "https://openalex.org/W4285744245",
        "https://openalex.org/W6782744377",
        "https://openalex.org/W4306945316",
        "https://openalex.org/W4281651885",
        "https://openalex.org/W4285008339",
        "https://openalex.org/W3176139635",
        "https://openalex.org/W3122239467",
        "https://openalex.org/W3208826636",
        "https://openalex.org/W3203701986",
        "https://openalex.org/W2963420686",
        "https://openalex.org/W4386076325",
        "https://openalex.org/W3175751069",
        "https://openalex.org/W3172087149"
    ],
    "abstract": "Introduction Grapes are prone to various diseases throughout their growth cycle, and the failure to promptly control these diseases can result in reduced production and even complete crop failure. Therefore, effective disease control is essential for maximizing grape yield. Accurate disease identification plays a crucial role in this process. In this paper, we proposed a real-time and lightweight detection model called Fusion Transformer YOLO for 4 grape diseases detection. The primary source of the dataset comprises RGB images acquired from plantations situated in North China. Methods Firstly, we introduce a lightweight high-performance VoVNet, which utilizes ghost convolutions and learnable downsampling layer. This backbone is further improved by integrating effective squeeze and excitation blocks and residual connections to the OSA module. These enhancements contribute to improved detection accuracy while maintaining a lightweight network. Secondly, an improved dual-flow PAN+FPN structure with Real-time Transformer is adopted in the neck component, by incorporating 2D position embedding and a single-scale Transformer Encoder into the last feature map. This modification enables real-time performance and improved accuracy in detecting small targets. Finally, we adopt the Decoupled Head based on the improved Task Aligned Predictor in the head component, which balances accuracy and speed. Results Experimental results demonstrate that FTR-YOLO achieves the high performance across various evaluation metrics, with a mean Average Precision (mAP) of 90.67%, a Frames Per Second (FPS) of 44, and a parameter size of 24.5M. Conclusion The FTR-YOLO presented in this paper provides a real-time and lightweight solution for the detection of grape diseases. This model effectively assists farmers in detecting grape diseases.",
    "full_text": "Real-time and lightweight\ndetection of grape diseases\nbased on Fusion\nTransformer YOLO\nYifan Liu, Qiudong Yu* and ShuzeGeng\nCollege of Information Technology Engineering, Tianjin University of Technology and Education,\nTianjin, China\nIntroduction: Grapes are prone to various diseases throughout their growth\ncycle, and the failure to promptly control these diseases can result in reduced\nproduction and even complete crop failure. Therefore, effective disease control\nis essential for maximizing grape yield. Accurate disease identiﬁcation plays a\ncrucial role in this process. In this paper, we proposed a real-time and lightweight\ndetection model called Fusion Transformer YOLO for 4 grape diseases detection.\nThe primary source of the dataset co mprises RGB images acquired from\nplantations situated in North China.\nMethods: Firstly, we introduce a lightweight high-performance VoVNet, which\nutilizes ghost convolutions and learnable downsampling layer. This backbone is\nfurther improved by integrating effective squeeze and excitation blocks and\nresidual connections to the OSA module. These enhancements contribute to\nimproved detection accuracy while maintaining a lightweight network. Secondly,\nan improved dual- ﬂow PAN+FPN structure with Real-time Transformer is\nadopted in the neck component, by incorporating 2D position embedding and\na single-scale Transformer Encoder into the last feature map. This modiﬁcation\nenables real-time performance and improved accuracy in detecting small\ntargets. Finally, we adopt the Decoupled Head based on the improved Task\nAligned Predictor in the head component, which balances accuracy and speed.\nResults: Experimental results demonstrate that FTR-YOLO achieves the high\nperformance across various evaluation metrics, with a mean Average Precision\n(mAP) of 90.67%, a Frames Per Second (FPS) of 44, and a parameter size of 24.5M.\nConclusion: The FTR-YOLO presented in this paper provides a real-time and\nlightweight solution for the detection of grape diseases. This model effectively\nassists farmers in detecting grape diseases.\nKEYWORDS\ngrape diseases detection, YOLO, transformer, lightweight, real-time\nFrontiers inPlant Science frontiersin.org01\nOPEN ACCESS\nEDITED BY\nPo Yang,\nThe University of Shefﬁeld, United Kingdom\nREVIEWED BY\nJun Liu,\nWeifang University of Science and\nTechnology, China\nGuoxiong Zhou,\nCentral South University Forestry and\nTechnology, China\nJakub Nalepa,\nSilesian University of Technology, Poland\n*CORRESPONDENCE\nQiudong Yu\n624432360@qq.com\nRECEIVED 30 July 2023\nACCEPTED 07 February 2024\nPUBLISHED 23 February 2024\nCITATION\nLiu Y,Yu Q andGeng S (2024) Real-time and\nlightweight detection of grape diseases based\non Fusion Transformer YOLO.\nFront. Plant Sci.15:1269423.\ndoi: 10.3389/fpls.2024.1269423\nCOPYRIGHT\n© 2024 Liu, Yu and Geng. This is an open-\naccess article distributed under the terms of\nthe Creative Commons Attribution License\n(CC BY).The use, distribution or reproduction\nin other forums is permitted, provided the\noriginal author(s) and the copyright owner(s)\nare credited and that the original publication\nin this journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nTYPE Original Research\nPUBLISHED 23 February 2024\nDOI 10.3389/fpls.2024.1269423\n1 Introduction\nChina’s extensive agricultural heritage, spanning over 2000\nyears, encompasses grape cultivation. Not only it is a signiﬁcant\ngrape-producing nation but it also stands as the largest exporter of\ngrapes worldwide. Grapes are not only consumed directly but are\nalso processed into various products such as grape juice, raisins,\nwine, and other valuable commodities, thus holding substantial\ncommercial value (El-Saadony et al., 2022). However, during the\ngrape growth process, susceptibility to diseases can lead to reduced\ngrape yield and signiﬁcant economic losses (Elnahal et al., 2022).\nHence, the timely and effective detection of grape diseases is crucial\nfor ensuring healthy grape growth. Conventionally, the diagnosis of\ngrape diseases predominantly relies on ﬁeld inspections by\nagricultural experts (Liu et al., 2022; Ahmad et al., 2023). This\napproach incurs high costs, ha s a lengthy cycle, and lacks\noperational efﬁciency.\nThe development of computer vision and machine learning\ntechnology provides a new solu tion for real-time automatic\ndetection of crop diseases (Fuentes et al., 2018, 2019). Traditional\nmachine learning methods in crop diseases identi ﬁcation and\npositioning have made some valuable experience, such as image\nsegmentation [such as K-means clustering (Trivedi et al., 2022) and\nthreshold method (Singh and Misra, 2017)], feature detection [such\nas SURF (Hameed and Üstündağ , 2020), KAZE (Rathor, 2021), and\nMSER blob (Lee et al., 2023)], and pattern recognition [such as\nKNN (Balakrishna and Rao, 2019), SVM, and bp neural network\n(Hatuwal et al., 2021 ; Kaur and Singh, 2021 )]. Due to the\ncomplexity of image preprocessing and feature extraction, these\nmethods are still ineffective in detection.\nDeep learning can automatically learn the hierarchical features\nof different disease regions without manual design of feature\nextraction and classiﬁer, with excellent generalization ability and\nrobustness. The detection of crop diseases through CNN has\nbecome a new hotspot in intelligent agriculture research. Jiang\net al. (2019)proposed a novel network architecture invar-SSD based\non VGG-Net and inception to the detection of apple leaf diseases,\nmAP reached 78.8%. Yang et al. (2023) proposed a SE-VGG16\nmodel uses VGG16 as the basis and adds the SE attention, which\nclassiﬁed corn weeds with an average accuracy of 99.67%.Guan\net al. (2023)proposed a dise efﬁcient based on the EfﬁcientNetV2\nmodel, achieved an accuracy of 99.80% on the plant disease and pest\ndataset. The above three methods are merely applicable for simple\nclassiﬁcation tasks. However, when it comes to detection tasks, the\nprevailing approach currently in use is YOLO.Liu and Wang (2020)\nproposed an improved YOLOv3 algorithm to detect tomato\ndiseases and insect pests. Results show that the detection accuracy\nis 92.39%, and the detection time is 20.39 ms.Wang et al. (2022)\nproposed a lightweight model based on the improved YOLOv4 to\ndetect dense plums in orchards. Compared with YOLOv4 model,\nthe model size is compressed by 77.85%, the parameters are only\n17.92%, and the speed is accelerated by 112%.Kuznetsova et al.\n(2020) designed harvesting robots based on a YOLOv3 algorithm,\napple detection time averaged 19 ms with 90.8% recall, and 7.8%\nFalse Positive Rate (FPR).Qi et al. (2021)proposed a highly fused,\nlightweight detection model named the Fusion-YOLO model to\ndetect the earlyﬂowering stage of tea chrysanthemum.Huang et al.\n(2021) used the YOLOv5 algorithm to detect the citrus collected by\nUAV, the detection accuracy rate was 93.32%.Qiu et al. (2022)used\nYOLOv5 for detecting citrus greening disease. The F1 scores for\nrecognizing ﬁve symptoms achieved 85.19%. Zhou et al. (2022)\nproposed an improved YOLOX-s algorithm. Compared with the\noriginal YOLOX-s, the model improved the detection Average\nprecision (AP) of kiwifruit by 6.52%, reduced the number of\nparameters by 44.8% and upgraded the model detection speed by\n63.9%. Soeb et al. (2023)used YOLOv7 forﬁve tea leaf diseases in\nnatural scene, which validated by detection accuracy 97.3%,\nprecision 96.7%, recall 96.4%, mAP 98.2%, and F1-score\n0.965, respectively.\nThe application of machine learning and deep learning in crop\ndisease detection in recent years is summarized. Deep learning,\nespecially CNN, has also made some contributions to grape disease\ndetection. Ji et al. (2020)designed the United Model and selected\n1,619 images of healthy and three kinds of diseased grape leaves in\nPlant village, with detection accuracy up to 98.57%. However, it\nshould be noted that all the data were obtained from laboratory\nsamples, and no comparative experiments were conducted in a\nnatural environment. Sanath Rao et al. (2021)used a pre-trained\nAlexNet to classify grapes and mango leaf diseases, achieved\naccuracy of 99% and 89% for grape leaves and mango leaves,\nrespectively. Ji et al. (2020)proposed a united CNN architecture\nbased on InceptionV3 and ResNet50 and can be used to classify\ngrape images into four classes, achieved average validation accuracy\nof 99.17% and test accuracy of 98.57%.Adeel et al. (2022)proposed\na entropy-controlled CNN to identify grape leaf diseases at the early\nstages, achieved an accuracy of 99%.Lu et al. (2022)proposed a\nGhost-conv. and Transformer networks for diagnosing 11 classes\ngrape leaf and pest, reached 180 frames per second (FPS), 1.16 M\nweights and 98.14% accuracy. After adding Transformer and\nGhost-conv., the performance is improved signiﬁcantly, but only\nthe identiﬁcation work is done.Xie et al. (2020)presented a Faster\nDR-IACNN model with higher feature extraction capability,\nachieved a precision of 81.1% mAP, and the detection speed\nreaches 15.01 FPS. The above two methods only detect grape leaf\ndiseases. Sozzi et al. (2022) evaluated six versions of the YOLO\n(YOLOv3, YOLOv3-tiny, YOLOv4, YOLOv4-tiny, YOLOv5x, and\nYOLOv5s) for real-time bunch detection and counting in grapes.\nPinheiro et al. (2023)presented three pre-trained YOLO models\n(YOLOv5x6, YOLOv7-E6E, and YOLOR-CSP-X) to detect and\nclassify grape bunches as healthy or damaged by the number of\nberries with biophysical lesions, highlighting YOLOv7 with 77% of\nmAP and 94% of the F1-score. Both of the aforementioned methods\nsolely utilized YOLO for grape bunch detection and did not involve\ndisease detection.Zhu et al. (2021)proposed YOLOv3-SPP network\nfor detection of black rot on grape leaves, applied in ﬁeld\nenvironment with 86.69% precision and 82.27% recall.Zhang Z.\net al. (2022)proposed a YOLOv5-CA, which highlights the downy\nmildew disease –related visual features to achieve an mAP of\n89.55%. Both methods employed YOLO for the detection of a\nsingle disease in grapes. We have listed the advantages and\ndisadvantages of different methods for plant disease detection\nin Table 1.\nLiu et al. 10.3389/fpls.2024.1269423\nFrontiers inPlant Science frontiersin.org02\nThere are also several challenges in grape disease detection: (1)\ngrape fruits and inﬂorescence are small and dense, making it difﬁcult to\ndetect the incidence area, which can be very small. (2) Photos taken in\nnatural scenes are susceptible to external interference. (3) The model\nneeds to balance detection accuracy with lightweight requirements for\ndeployment and real-time performance. To address these challenges,\nthis paper proposes a real-time detection model based on Fusion\nTransformer YOLO (FTR-YOLO) for grape diseases. The main\ncontributions of this paper are summarized as follows:\n Regarding the issue of limited detection of disease types in\nother models and the detection under non-natural\nenvironments, we have collected four grape diseases\n(anthracnose, grapevine white rot, gray mold, and\npowdery mildew) datasets in natural environments,\ncovered different parts such as leaves, fruits, andﬂower\ninﬂorescence. The primary source of the dataset comprises\nR G Bi m a g e sa c q u i r e df r o mp l a n t a t i o n ss i t u a t e di n\nNorth China.\n In backbone, we integrate learnable downsampling layer\n(LDS), effective squeeze and excitation (eSE) blocks, and\nresidual connections based on VoVnet, effectively\nimproving the ability of network to extract feature\ninformation. In neck component, an improved real-time\nTransformer with two-dimensional (2D) position\nembedding and single-scale Transformer encoder (SSTE)\nare incorporated to the last feature map to accurate\ndetection of small targets. In head component, the\nDecoupled Head based on the improved Task-Aligned\nPredictor (ITAP) is adopted to optimize detection accuracy.\n To address the challenges with deploying application using\nmodels that have a large capacity and slow inference speed,\nwe replace the convolution with ghost module in the model,\nabandon Transformer decoder, and adopt more efﬁcient\nSSTE with VoVnet-39 of fewer layers to ensure the\nlightweight and detection speed.\nThe rest of the article is organized as follows: Section 2\nexplicates the datasets and experimental settings and the network\narchitecture and improvement of FTR-YOLO.Section 3 presents\nthe evaluation of the experimental performance and analyses.\nDiscussions of the performance are presented inSection 4. Last,\nSection 5 offers conclusions and suggestions for future work.\n2 Materials and methods\n2.1 Experimental dataset building\nIn the process of building grape diseases detection dataset,\nsmartphone is used to collect photos in the local orchard. The\nphotos are taken in different time periods, weather conditions, and\nscenes. The labeling tool is used to mark the images, the region of\ninterest by manually marking the rectangle, and then generated the\nconﬁguration ﬁle automatically.\nData augmentation is employed to expand the number of\nimages within the training dataset. The methods include random\nﬂipping, Gaussian blur, afﬁne transformation, image interception,\nﬁlling, and so forth. The network model is designed to enhance\nrandomly selected images by one or several operations.\nThe number of samples for each category is shown inTable 2.\nThrough data enhancement, the dataset is expanded to 4,800\nimages. The ratio of training set and test set is 8:2.\nThe overall structure of FTR-YOLO is shown inFigure 1. The\nprimary innovations of the model are represented by streamlined\nmodules. For comprehensive details, please consult the detailed\nillustrations provided inSections 2.2–2.4.\n2.2 Backbone of FTR-YOLO\nIn backbone component, a lightweight high-performance\nVoVnet (LH-VoVNet) (Zhao et al., 2022) network is used. The\nproposed net adds the LDS Layer, eSE attention module (Long et al.,\n2020 ) and residual connection on the basis of One-Shot\nAggregation (OSA) module. Also, the Conv. layer is replaced with\nGhost Module (Zhang B. et al., 2022) to further lightweight the\nnetwork. The LH-VoVNet has s horter computation time and\nhigher detection accuracy compared with other common\nbackbone, which is more suitable for grape disease detection tasks.\nTABLE 2 The number of samples for each disease type.\nDisease Sample\nsize\nNumber of\nlabeled samples\n(bounding box)\nPercent of\nbounding\nbox samples\nAnthracnose 1200 4587 20.53%\nWhite rot 1200 6025 26.97%\nGray moid 1200 5160 23.09%\nPowdery\nmildew\n1200 6571 29.41%\nTotal 4800 22343 100%\nTABLE 1 Comparison of the advantages and disadvantages of\ndifferent methods.\nMethod Advantage Disadvantage\nMachine\nlearning\n- Less data and computing\nresources.\n- High interpretability.\n- Difﬁcult to handle complex\nproblems.\n- Poor detection accuracy.\nDeep learning\n(classiﬁcation)\n- The model can\nautomatically learn image\nfeature representations.\n- High detection accuracy.\n- More data and computing\nresources.\n- The task is relatively simple;\nthe practical value is limited.\nDeep learning\n(detection)\n- Higher accuracy and\ngeneralization hold\nsigniﬁcant practical value.\n- End-to-end, one-stage\nmodels (YOLO) are easy\nto implement.\n- Additional data,\nannotations, and computing\nresources are necessary.\n- Balancing detection\naccuracy and speed is a\nchallenging task.\nLiu et al. 10.3389/fpls.2024.1269423\nFrontiers inPlant Science frontiersin.org03\n2.2.1 VoVNet\nOne of the challenges with DenseNet (Jianming et al., 2019)i s\nthat the dense connections can become overly cumbersome. Each\nlayer aggregates the features from the preceding layers, leading to\nfeature redundancy. Furthermore, based on the L1 norm of the\nmodel weights, it is evident that the middle layer has minimal\nimpact on the ﬁnal classiﬁcation layer, as shown in Figure 2A.\nInstead, this information redundancy is a direction that can be\noptimized, so the OSA module is adopted, as shown inFigure 2B.\nSimply put, the OSA aggregates all the layers up to theﬁnal one,\neffectively addressing the prior issue encountered with DenseNet.\nSince the number of input channels per layer isﬁxed, the number of\noutput channels can be consistent with the input to achieve the\nminimum MAC, and the 1 × 1 Conv. layer is no longer required to\ncompress features, the OSA module is computationally efﬁcient.\n2.2.2 LDS layer\nAt present, in common networks, the steps of downsampling\nfeature maps are usually completed at theﬁrst Conv. of each stage.\nFigure 3A shows the general Residual block. In Path A, once the\ninput data are received, it undergoes a 1 × 1 Conv. with a stride of 2.\nThis operation leads to a loss of 3/4 of the information in the input\nfeature maps.\nTo solve this problem, the LDS layer is adopted. The\ndownsampling is moved to the following 3 × 3 Conv. in Path A,\nand the identity part (Path B) downsampling is done by the added\nFIGURE 1\nThe architecture of FTR-YOLO.\nA\nB\nFIGURE 2\nThe architecture of DenseNet and VoVNet.(A) Dense aggregation (DenseNet) and(B) One-shot aggregation (VoVNet).\nLiu et al. 10.3389/fpls.2024.1269423\nFrontiers inPlant Science frontiersin.org04\navg-pool, so as to avoid the loss of information caused by the\nsimultaneous appearance of 1 × 1 Conv. and stride. Details are\nshown inFigure 3B.\n2.2.3 RE-OSA module\nThe pivotal element of the VoVnet lies in the OSA module as\ndescribed in Section 2.2.1. While the performance of the OSA\nmodule is not enhanced, it offers lower MAC and improved\ncomputational efﬁciency. Therefore, this paper adds eSE block\nand residual connection in OSA module to further enhance\nfeatures and improve detection accuracy, called RE-OSA module.\nT h ec o r ei d e ao fS EB l o c ki st ol e a r nt h ef e a t u r ew e i g h t\naccording to loss through the network (Hu et al., 2018), so that\nthe effective feature map has a larger weight and the rest of the\nfeature map has a smaller weight to train the model to achieve better\nresults. The SE module squeezes the entire spatial features on a\nchannel into a global feature by global average pooling, then two\nfully connected (FC) layers are used to concat the feature map\ninformation of each channel. Assume that the input feature map\nXi ∈ RC/C2 W/C2 H , the channel attention map Ach(Xi) ∈ RC/C2 1/C2 1 is\ncomputed inEquations 1, 2.\nAch(Xi)= sðWc(dðWc=r(F gap(Xi ÞÞÞÞÞ (1)\nFgap(X)= 1\nWH o\nW,H\ni,j=1 Xi,j (2)\nWhere Fgap is channel-wise global average pooling,Wc=r, Wc ∈\nRC/C2 1/C2 1 are weights of two FC layers,s denotes ReLU activation\nfunction, d denotes sigmoid activation function.\nIn SE block, to avoid the computational burden of such a large\nmodel, reduction ratior is used in theﬁrst FC layer to reduce the\ninput feature channels fromc to c/r. The second FC layer needs to\nexpand the reduced number of channels to the original channelc.I n\nthis process, the reduction of channel dimensions leads to the loss of\nchannel information.\nTherefore, we adopt eSE that uses only one FC layer withc\nchannels instead of two FC layers without channel dimension\nreduction, which rather maintains channel information and in\nturn improves performance. In this paper, the ReLU/sigmoid\nactivation function in the module is replaced by the SiLU\nfunction with better performance in YOLOv7 (Wang et al., 2023).\nThe eSE is computed inEquations 3, 4:\nAeSE = ϑ(Wc(Fgap(Xi))) (3)\nXrefine = AeSE ⊗Xi (4)\nwhere ϑ denotes SiLU activation function. As a channel\nattentive feature descriptor, the AeSE ∈ RC/C2 1/C2 1 is applied to the\ndiversiﬁed feature map Xi to make the diversiﬁed feature more\ninformative. Finally, the reﬁned feature mapXreﬁne is obtained by\nchannel-wise multiplicationAeSE and Xi.\n2.2.4 Lightweight with ghost convolution\nIt can be seen fromSection 2.2.3that Conv. layer appears most\nfrequently in VoVNet. As a result, the whole network has a large\namount of computation and parameter volume, which is not\nconducive to lightweight deployment.\nTo solve this problem, this paper adopts a structure— Ghost\nModule, which can generate a large number of feature graphs with\ncheap operations. This method can reduce the amount of\ncomputation and parameter volume on the basis of ensuring the\nperformance ability of the algorithm.\nIn the feature map extracted by the mainstream deep neural\nnetworks, the rich and even redundant information usually ensures\na comprehensive understanding of the input data. These\nredundancies are called ghost maps.\nA B\nFIGURE 3\nTwo different methods of downsampling.(A) Conv. downsampling and(B) LDS downsampling.\nLiu et al. 10.3389/fpls.2024.1269423\nFrontiers inPlant Science frontiersin.org05\nThe ghost module consists of two parts. One part is the feature\nmap generated by the ordinary Conv. The other part is the ghost\nmaps generated by simple linear operationF. It is assumed that the\ninput feature map of sizeh×w×c is convolved with n sets of kernels\nof sizek×k, and the output feature map of sizeh'×w'×n. In the ghost\nmodel, m groups of k×k kernels are convolved with input to\ngenerate the identity maps of size h'×w'×m,a f t e rw h i c ht h e\nidentity maps are linearly transformed by depth-wise convolution\n(k=5) to produce ghost maps. Finally, identity maps are concated\nwith ghost maps to generate g host convolution. The ghost\nconvolution acceleration ratio rs and compression ratio rc are\ncalculated compared with ordinary convolution, as shown in\nEquations 5, 6.\nrs = n·h 0 ·w 0 ·c·k·k\nn\ns ·h 0 ·w 0 ·c·k·k +( s − 1) · n\ns ·h 0 ·w 0 ·c·d·d ≈ s (5)\nrc = n·c·k·k\nn\ns ·c·k·k +( s − 1) · n\ns ·c·d·d ≈ s (6)\nwhere the numerator is the complexity of ordinary convolution.\nThe denominator is the complexity of ghost module.s is the total\nmapping generated by each channel (one identity map and s-1\nghost maps),c is the number of input feature maps, generallys ≪ c;\nn/s refers to the identity map output by general convolution;d×d is\nthe average kernel size of depth-wise Conv. and has a similar size\nto k×k.\nEquations 5, 6 show that, compared with ordinary Conv.,\nGhost-conv. greatly reduces the amount of computation and the\nnumber of parameters.\nFinally, GC-RE-OSA module replaced 3 × 3 Conv. in RE-OSA\nmodule (Section 2.2.3) with Ghost-conv. The structure of GC-RE-\nOSA is shown inFigure 4.\nThe speciﬁc structure of LH-VoVnet can be found inTable 3.\nLH-VoVNet comprises a stem block that consists of three 3 × 3\nConv. layers, followed by GC-RE-OSA modules implemented in\nfour stages. At the start of each stage, an LDS with a stride of 2 is\nutilized (Section 2.2.2). The model achieves aﬁnal output stride of\n32. For more details, please refer toSections 2.2.3 and 2.2.4.\n2.3 Neck of FTR-YOLO\nIndeed, the Transformer model relies on a global attention\nmechanism that requires substantial computational resources for\noptimal performance ( Carion et al., 2020 ). Consequently, it\nbecomes crucial to address this issue effectively. To mitigate this\nconcern, we eschew the initial image or multi-layer feature maps as\ninput and instead incorporate only theﬁnal feature map obtained\nfrom the backbone. This is then directly connected to the neck.\nAdditionally, we select only two improved modules of Position\nEmbedding and Encoder.\nWithin the neck component, we utilize the current optimal\ndual-ﬂow PAN + FPN structure and enhance it through integration\nwith the GC-RE-OSA module introduced in this paper.\n2.3.1 Real-time transformer\nTo enhance the detection accuracy, an enhanced global\nattention mechanism based on the Vision Transformer (ViT) is\nintroduced. This modiﬁcation takes into consideration that some\ngrape diseases may share similarities, while others have limited\noccurrence areas. By incorporating this improved global attention\nmechanism, the detection accuracy can be further improved in\ndetecting different grape diseases.\nThe current common detection transformer (DETR) algorithms\nextract the last three layers of feature maps (C3, C4, and C5) from\nthe backbone network as the input. However, this approach usually\nhas two problems:\n1. Previous DETRs, such as deformable DETR (Zhu et al.,\n2020), ﬂatten multi-scale features, and concatenate them\ninto a single long-sequence vector. This approach not only\nenables effective interaction between the different scale\nFIGURE 4\nThe structure of GC-RE-OSA module.\nLiu et al. 10.3389/fpls.2024.1269423\nFrontiers inPlant Science frontiersin.org06\nfeatures but it also introduces signiﬁcant computational\ncomplexity and increases the time required for processing.\n2. Compared to the shallower C3 and C4 features, the deepest\nlayer C5 feature has deeper, higher level, and richer\nsemantic features. These semantic features are more\nuseful for distinguishing different objects and are more\ndesirable for Transformer. Shallow features do not play\nmuch of a role due to the lack of better semantic features.\nTo address these issues, we only select the C5 feature map output\nby the backbone network as the input for the Transformer. To retain\nkey feature information as much as possible, we replaced the simple\nﬂattening of feature maps into a vector with a 2D encoding in the\nPosition Embedding module (Wu et al., 2021). Additionally, a\nlightweight single-scale Transformer encoder is adopted.\nThe Multi-Head Self-Attention (MHSA) aggregation in\nTransformer combines input elements without differentiating their\npositions; thus, Transformer possess permutation invariance. To\nalleviate this issue, we need to embed spatial information into the\nfeature map, which requires adding 2D position encoding to theﬁnal\nlayer feature map. Speciﬁcally, the original sine and cosine positional\nencodings in Position Embedding are respectively extended to column\nand row positional encodings, and concatenated with themﬁnally.\nAfter the feature map is processed by 2D position embedding,\nwe use a single-scale Transformer Encoder, which only contains one\nEncoder layer (MHSA + Feed Forward network) to process the\noutput of Q, K, and V at three scales. Note that the three scales share\none SSTE and, through this shared operation, the information of the\nthree scales can interact to some extent. Finally, the processing\nresults are concatenated together to form a vector, which is then\nadjusted back to a 2D feature map, denoted as F5. In the neck part,\nC3, C4, and F5 are sent to dual-ﬂow PAN + FPN for multi-scale\nfeature fusion. SeeFigure 1for details.\n2.3.2 Dual-ﬂow PAN + FPN\nIn order to achieve better information fusion of the three-layer\nfeature maps (C3, C4, and F5), our enhanced neck implements a\ndual-stream PAN + FPN architecture, which is featured in the latest\nYOLO series. In addition to this, we have introduced GC-RE-OSA\nmodule to ensure faster detection speed while preserving accuracy.\nA comparison between YOLOv5 (Jocher et al., 2021)( Figure 5A)\nand our enhanced neck structure (Figure 5B) is provided. Our\nimproved architecture substitutes the C3 module with the GC-RE-\nOSA module and eliminates the Conv. prior to upsampling. This\nenables direct utilization of features output from diverse stages of\nthe backbone.\n2.4 Head of FTR-YOLO\nFor the Head component, we have employed Decoupled Head\nto perform separate classiﬁcation and regression tasks via two\nA B\nFIGURE 5\nTwo different neck structures.(A) YOLOv5 neck and(B) ours.\nTABLE 3 The speciﬁc structure of LH-VoVnet.\nType Output\nstride\nStage Output\nchannel\nStem\n2\n2\n2\n3×3 Ghost-conv., 64,\nStride = 2\n3×3 Ghost-conv., 64,\nStride = 1\n3×3 Ghost-conv., 128,\nStride = 1\n64\nStage 1 4 LDS Layer ×1, GC-\nRE-OSA×1\n128\nStage 2 8 LDS Layer ×1, GC-\nRE-OSA×1\n256\nStage 3 12 LDS Layer ×1, GC-\nRE-OSA×2\n512\nStage 4 32 LDS Layer ×1, GC-\nRE-OSA×2\n1024\nLiu et al. 10.3389/fpls.2024.1269423\nFrontiers inPlant Science frontiersin.org07\ndistinct convolutional channel s. Furthermore, our architecture\nincludes the ITAP within each branch, which enhances the\ninteraction between the two tasks.\nObject detection commonly faces a task con ﬂict between\nclassi ﬁcation and localization. While decoupled head is\nsuccessfully applied to SOTA YOLO model in YOLOX (Ge et al.,\n2021), v6 (Li et al., 2023), v7 (Wang et al., 2023), and v8 (Terven and\nCordova-Esparza, 2023), drawing lessons from most of the one-\nstage and two-stage detectors, si ngle-stage detectors perform\nclassi ﬁcation and localization tasks in parallel using two\nindependently functioning branches. However, this dual-branch\napproach may lack interaction , resulting in inconsistent\npredictions during execution.\nTo address this issue, we drew inspiration from the TAP in\nTOOD (Feng et al., 2021 ) and made some improvements to\nmaintain accuracy while improving speed. As shown inFigure 6,\nthe ITAP uses eSE to replace the layer attention in TOOD. To\nfurther enhance ef ﬁciency, we incorporated a more ef ﬁcient\nConvolution+BN layer+Silu (CBS) module before the shortcut.\nMoreover, during the training phase, we utilized different loss for\nthe two branches.\n2.5 Label assignment and loss\nThe loss calculation in our study employed the label assignment\nstrategy. SimOTA is employed in YOLOX, v6 and v7 to enhance\ntheir performance. Task alignment learning (TAL) proposed in\nTOOD is used in YOLOv8. This strategy entails selecting positive\nsamples based on the weighted scores of the classiﬁcation and\nregression branches within the loss function. For the classiﬁcation\nbranch, we utilize the varifocal loss (VFL) (Zhang et al., 2021), while\nfor the regression branch, the distribution focal loss (DFL) (Li et al.,\n2020) is employed. Furthermore, we incorporate the Complete-IoU\n(CIoU) Loss. The combination of these three losses is achieved\nthrough weighted proportions.\nVFL utilizes the target score to assign weight to the loss of\npositive samples. This implementation signiﬁcantly ampliﬁes the\nimpact of positive samples with high IoU on the loss function.\nConsequently, the model prioritizes high-quality samples during\nthe training phase while de-emphasizing the low-quality ones.\nSimilarly, both approaches utilize IoU-aware classiﬁcation score\n(IACS) as the target for prediction. This enables effective learning of\na combined representation that includes both classiﬁcation score\nand localization quality estimation. By employing DFL to tackle the\nuncertainty associated with bounding boxes, the network gains the\nability to swiftly concentrate on the distribution of neighboring\nregions surrounding the target location. SeeEquation 7for details.\nLoss = a · lossVFL + b · lossCIoU + g lossDFL\no\nNpos\ni ^t\n(7)\nwhere ^t denotes the normalized score used in TOOD,a, b, and\ng represent different weights.\n3 Experimental results\nThe experimental hardware environment is conﬁgured with\nINTEL I7-13700 CPU, 32GB RAM, and GEFORCE RTX3090\ngraphics. The operating syste m is Windows10 professional\nedition, the programming language is Python 3.8, and the\nacceleration environment is CUDA 11.1 and CUDNN 8.2.0. The\ntraining parameters of the training process used in the experiment\nare shown inTable 4.\n3.1 Ablation study on backbone\nThe improved network is composed of backbone, neck, and\nhead, so the inﬂuence of the improvement of each part on the model\nperformance should be veriﬁed respectively.\nIn this paper, the LH-VoVNet is veriﬁed through experiment.\nThe improvements include (1) the LDS layer is used for\ndownsampling. (2) By adding eSE block and RE-OSA module. (3)\nThe Conv. is replaced with Ghost Module to further lightweight the\nnetwork. The results of the ablation study are shown inTable 5.\nFIGURE 6\nITAP decoupled head structures.\nLiu et al. 10.3389/fpls.2024.1269423\nFrontiers inPlant Science frontiersin.org08\nOn the basis of VoVnet, compared by adding LDS layer/RE-\nOSA module improves accuracy by 1.06%/1.42% mAP. By replacing\nGhost-conv., the number of parameters in the network is greatly\nreduced (−62.7%), the FPS is signiﬁcantly improved (+78.9%), and\nthe detection performance is also slightly improved (+0.31%).\nFinally, the integration of these three components shows that\nmAP 86.79% (+2.17%) is optimal, Params 24.7MB (−50.1%) and\nFPS 56 (+47.4%), achieve lightweight and real-time in backbone.\n3.2 Ablation study on neck\nTo verify the effectiveness of the proposed neck, we evaluate the\nindicators of the set of variants designed inSection 2.3, including\nmAP, number of parameters, latency and FPS. The backbone used\nin the ablation experiment is LH-VoVNet. The improvements\ninclude the following: (1) Only the C5 feature map output by the\nbackbone as the input is selected for the Transformer. (2) The real-\ntime Transformer only includes 2D position embedding and SSTE\nto further lightweight the network. (3) The C3 module is replaced\nwith GC-RE-OSA module. The parameters for the Transformer\nEncoder are as follows: num of head = 8, num of encoder layers = 1,\nhidden dim = 256, dropout = 0.1, activation = relu.\nThe experimental results are shown inTable 6. On the basis of\nYOLOv5 neck, by adding real-time Transformer delivers 1.41% AP\nimprovement, while increasing the number of parameters by 4.5%,\nthe latency by 47.2%, decreasing the FPS by 17.9%. This\ndemonstrates the effective enhancement of detection accuracy by\nTransformer while maintaining a high degree of lightweight and\nreal-time performance. By adding GC-RE-OSA module delivers\n0.45% AP improvement, the number of parameters experienced a\nslight increase of 4.5%, the latency decreases by 25.0%, and the FPS\nincrease by 8.9%. This shows that the module not only enables\nlightweight networking but also enhances performance. Finally, the\nintegration of these two components shows that mAP 88.85%\n(+2.06%) is optimal, Params 22.5MB (−8.9%), Latency 56.3ms\n(+7.2%), and FPS 49 ( −12.5%). The improved neck further\nenhances network detection performance and lightweight, albeit\nwith a slight ﬂuctuation in FPS and Latency that has negligible\nimpact on real-time detection.\n3.3 Ablation study on head and loss\nTo verify the effectiveness of the proposed head, we evaluate the\nindicators of the set of variants designed inSections 2.4 and 2.5,\nincluding mAP, number of parameters, latency, and FPS. We\nconduct this experiment on above-modiﬁed model, which uses\nLH-VoVNet, improved neck, and YOLOv5 head as the baseline.\nThe parameters for the TAL are as follows: topk = 13, alpha = 1.0,\nand beta = 6.0. Similarly, for the SimOTA Assigner, the parameters\nare center_radius = 2.5 and topk = 10. InEquation 7, the weights\nassigned to the three losses are as follows: VFL (a = 1.0), CIoU (b =\n2.5), and DFL (g = 0.5). The experimental results are shown\nin Table 7.\nOn the basis of YOLOv5 head, by adding ITAP Decoupled\nHead delivers 0.61% AP improvement, while increasing the number\nof parameters by 6.2%, the latency by 6.6%, decreasing the FPS by\n8.2%. This indicates that the improved head has minimal impact on\nparameter and computational speed, while simultaneously\nenhancing detection accuracy. By adding SimOTA delivers 0.27%\nAP improvement, the number o f parameters/Latency/FPS\nexperience a slightﬂuctuation by +2.2%/+1.4%/−2.0%. By adding\nTAL delivers 1.06% AP improvement, the number of parameters/\nLatency/FPS experience a slightﬂuctuation by +2.7%/+1.8%/−2.0%.\nTABLE 5 The results of the ablation study of backbone components.\nMethods mAP@0.5 Params(M) FPS\nVoVnet 84.62 49.0 38\n+LDS layer 85.68 49.4 37\n+RE-OSA module 86.04 53.5 24\n+Ghost-conv. 84.93 18.3 68\nLH-VoVNet 86.79 24.7 56\nBold values represents the optimal values.\nTABLE 4 The implementation details of training parameters.\nParameter Value Parameter Value\nOptimizer AdamW Weight decay 0.0005\nLearning rate 0.001 Momentum 0.937\nBatch size 8 warmup steps 300\nImage size 640*640 Epochs 200\nNMS threshold 0.7 EMA decay 0.9998\nTABLE 6 The results of the ablation study of neck components.\nMethods mAP@0.5 Params\n(M)\nLatency\n(ms) FPS\nYOLOv5 neck 86.79 24.7 52.5 56\n+Real\ntime Transformer 88.20 25.8 77.3 46\n+GC-RE-\nOSA module 87.22 20.3 39.4 61\nOurs neck 88.85 22.5 56.3 49\nBold values represents the optimal values.\nTABLE 7 The results of the ablation study of head & loss components.\nMethods mAP@0.5 Params\n(M)\nLatency\n(ms) FPS\nYOLOv5 head 88.85 22.5 56.3 49\n+ITAP\ndecoupled head 89.46 23.9 60.0 45\n+SimOTA 89.12 23.0 57.1 48\n+TAL 89.91 23.1 57.3 48\nOurs head 90.67 24.5 61.5 44\nBold values represents the optimal values.\nLiu et al. 10.3389/fpls.2024.1269423\nFrontiers inPlant Science frontiersin.org09\nAfter comparing the label assignments of SimOTA and TAL, it was\nfound that TAL exhibited superior performance, thus making it the\npreferred choice for our paper. Finally, we adopted a hybrid\nmethodology comprising ITAP Decoupled Head+TAL, resulting\nin an optimized mAP of 90.67% (+1.82%). Additionally, there was\nan augmentation in the model’s parameters and Latency to 24.5MB\n(+8.9%) and 61.5 (+9.2%), respectively, while the FPS decreased to\n44 (−10.2%).\n3.4 Comparison with other detectors\nTable 8 compares FTR-YOLO with other real-time detectors\n(YOLOv5, YOLOv6, YOLOv7, YOLOv8, and PP-YOLOE) and\nVision Transformer detector (DINO-DETR).\nCompared to real-time detectors YOLOv5/YOLOv6/YOLOv7/\nYOLOv8/PP-YOLOE, FTR-YOLO signiﬁcantly improves accuracy\nby 6.44%/3.13%/1.81%/1.75%/4.92% mAP, increases FPS by 10.0%/\n18.9%/7.3%/0.0%/7.3%, and reduces the number of parameters by\n47.1%/58.5%/33.1%/43.4%/53.1%. Even among the AP metrics for the\nfour categories, the FTR-YOLO algorithm consistently demonstrates\nthe best performance. Additionally, the differences in AP values among\nthe four disease categories are relatively small, indicating that the FTR-\nYOLO algorithm exhibits good robustness. This demonstrates the\nsuperior performance of FTR-YOLO compared to the state-of-the-\nart YOLO detectors in terms of accuracy, speed, and lightweight.\nIn order to determine the statistical signi ﬁcance of the\ndifferences between various algorithms, we performed four\nindependent repeated experiments for each algorithm. A t test\nwas employed, and the p-values for mAP among different\nalgorithms were computed. Due to substantial variations in\nparameters, including image input size and training epochs,\nbetween the DINO-DETR algo rithm and other detection\nalgorithms, it was excluded from the statistical analysis. The\nexperimental results reveal that thep-values comparing different\nalgorithms are considerably small, all well below 0.01, signifying\nnoteworthy variances between the algorithms.\nCompared to DINO-DETR, the number of parameters/mAP/\nFPS experience a ﬂuctuation by −48.3%/−0.45%/+2100.0%. This\nobservation highlights that, while DINO achieves a slightly higher\nmAP of 0.45% compared to FTR-YOLO, it fails to meet real-time\nrequirements due to its signiﬁcantly lower FPS (2). Furthermore,\nthere is no discernible advantage in terms of model lightweight.\n3.5 Object size sensitivity analysis\nDifferent disease types, periods, and locations result in different\ncharacteristics and sizes. The improved network proposed in this\npaper effectively enhances the detection accuracy in small object\nscenario. In order to verify the detection effect of the small object\ndetection performance, the test dataset is divided intoﬁve groups\nbased on the size of the disease area. The, 0%–10%, 10%–20%, 20%–\n40%, 40%–60%, and 60%–90%, ﬁve groups are named with different\nlabels: XS, S, M, L, and XL, which represent the size of different\nobjects. The comparison of the detection accuracy of six common\nalgorithms with FTR-YOLO forﬁve different sizes.\nAs shown inFigure 7, The YOLOv5 and PP-YOLOE perform\nwell in large target region (XL and L), but the detection effect of\nsmall target decreases sharply (XS and S). YOLOv6, v7, and v8 have\nshown slight improvements in detection accuracy compared to\nYOLOv5. Among them, v8 performs better on smaller scales (XS\nand S) while demonstrating similar detection effectiveness on M, L,\nand XL scales. The DINO-DETR is optimal in the detection\naccuracy on smaller scales (XS and S). FTR-YOLO demonstrates\nsuperior performance on M (90.43%), L (95.30%), and XL (98.73%)\nscales. The mAP values show a signiﬁcant improvement when\ncompared to the otherﬁve YOLO algorithms on both XS and S\nscales. Speciﬁcally, it shows improvements of 7.81%/6.71%/4.69%/\n3.31%/6.58% on XS scale, and improvements of 10.65%/8.01%/\n5.67%/4.82%/7.24% on S scale. These improvements highlight the\neffectiveness of the system in achieving higher mAP values\ncompared to its counterparts. While it may have slightly lower\nperformance than DINO-DETR, FTR-YOLO is still the optimal\nchoice due to its lightweight and real-time capabilities.\n3.6 Image size sensitivity analysis\nThe Batch Random Resize is applied to a batch of images, which\nhelps increase the diversity and randomness of the data. By\nintroducing such variations during training, the model becomes\nTABLE 8 The comparison results of different methods.\nMethod Size Params(M)\nAP for each category*\nmAP@0.5 FPS p-value\n1234\nYolo V5 640*640 46.3 87.42 76.03 85.29 88.18 84.23 40 < 0.01\nYolo V6 640*640 59.0 90.70 88.59 80.37 90.14 87.54 37 < 0.01\nYolo V7 640*640 36.6 89.51 90.44 84.23 91.26 88.86 41 < 0.01\nYolo V8 640*640 43.3 89.83 88.47 85.30 92.08 88.92 44 < 0.01\nPP-YOLOE 640*640 52.2 88.15 78.59 84.42 91.84 85.75 41 < 0.01\nDINO-DETR 800*1333 47.4 91.79 90.58 88.76 93.35 91.12 2 ——\nFTR-YOLO 640*640 24.5 90.73 90.67 88.54 92.74 90.67 44 < 0.01\n*In Table 8, 1, 2, 3, and 4 represent the four types of detected diseases: 1, anthracnose; 2, grapevine white rot; 3, gray mold; 4, powdery mildew.\nBold values represents the optimal values.\nLiu et al. 10.3389/fpls.2024.1269423\nFrontiers inPlant Science frontiersin.org10\nmore robust and better able to generalize to unseen examples. This\ntechnique can contribute to improving the overall performance and\ngeneralization ability of the model in tasks such as object detection\nor image classiﬁcation. In our experiment, the data were randomly\nresized into the following 10 different sizes: [320, 384, 448, 480, 512,\n544, 576, 640, 672, 704, 736, and 768].\nTo further validate the detection performance on images of\nvarying sizes, we categorized the dataset into three groups based on\ndifferent sizes: (1) small size, less than or equal to 480; (2) medium\nsize, ranging from 480 to 768; (3) large size, greater than 768.Figure 8\nshows the detection performance of seven different algorithms.\nThe detection accuracy among samples of different sizes does\nnot show signiﬁcant variation, as illustrated inFigure 8. However, it\nshould be noted that the detection accuracy is affected by the\ndistortion introduced when resizing small-sized images to 640.\nAmong the various algorithms, the DINO-DETR algorithm is\nparticularly sensitive to this impact. On the other hand, FTR-\nYOLO demonstrates superior performance on small-sized images\n(87.34%) and medium-sized images (90.80%). Additionally, FTR-\nYOLO signiﬁcantly improves mAP values compared to the other\nﬁve YOLO algorithms on small-sized images by 4.73%, 3.31%,\n2.02%, 1.49%, and 3.85%. It also improves mAP values on\nmedium-sized images by 5.07%, 3.55%, 2.20%, 2.24%, and 4.78%.\nFurthermore, it improves mAP values on large-sized images by\n5.41%, 2.92%, 1.46%, 1.34%, and 4.85%. Although FTR-YOLO may\nhave slightly lower performance than DINO-DETR on large-sized\nimages, it is still considered the optimal choice due to its lightweight\ndesign and real-time capabilities.\nBased on the comparative evaluation inSections 3.4–3.6, LH-\nVoVnet-39 outperforms YOLO ’s backbone CSPDarknet-53 or\nCSPResnet-50, which replaced th e convolution downsampling\noperation with LDS, enabling the model to better preserve\nimportant features. Additionally, the GC-RE-OSA module, along\nwith residual connections and eSE attention mechanism, further\nimproves feature extraction. Furthermore, we have made\nimprovements to the TAP and loss selection based on YOLOv7\nFIGURE 7\nObject size sensitivity analysis.\nFIGURE 8\nImage size sensitivity analysis.\nLiu et al. 10.3389/fpls.2024.1269423\nFrontiers inPlant Science frontiersin.org11\nand v8 decoupled heads. As a result, FTR-YOLO demonstrates\nsuperior performance in terms of mAP and AP values for each\ncategory, with minimal numerical differences and strong\ngeneralization capabilities (Table 8).\nDue to VoVnet-39 having fewer layers and the utilization of\nlightweight ghost modules instead of convolutions, in addition to a\nreal-time transformer that consists of 2D position embedding and a\nsingle-scale Transformer encoder, but does not include decoder,\nFTR-YOLO achieves comparable FPS performance to YOLOv8\nwhile delivering optimal results (Table 8).\nOn the other hand, DINO-DETR, with its multi-scale\nTransformer encoder and decoder, possesses more input feature\nmaps and layers, resulting in better performance for object\ndetection. It outperforms FTR-YOLO in speciﬁc metrics such as\nmAP inTable 8,m A Pf o rX Sa n dSo b j e c ts c a l e si nFigure 7, and mAP\nfor large-sized inputs inFigure 8. However, this improvement comes\nat the cost of signiﬁcantly increased computational complexity,\nleading to an FPS of only 2, which limits its practical applications.\nFIGURE 9\nThe p–r curve of FTR-YOLO.\nA B\nDE F\nGH\nC\nFIGURE 10\nThe detection results of FTR-YOLO.(A) diseased leaves of anthracnose,(B) diseased leaves of grapevine white rot,(C) diseased leaves of gray mold,\n(D) diseased leaves of powdery mildew,(E) diseased fruits of gray mold,(F) diseased fruits of grapevine white rot,(G) diseased fruits of anthracnose,\nand (H) diseased inﬂorescence of gray mold.\nLiu et al. 10.3389/fpls.2024.1269423\nFrontiers inPlant Science frontiersin.org12\n3.7 Performance visualization on\nFTR-YOLO\nThe precision–recall curves of each disease are provided in\nFigure 9, which intuitively shows the detailed relationship between\nprecision and recall. It has been observed that as recall increases, the\nrate of change in precision also increases. When the graph’s curve is\ncloser to the upper right corner, it indicates that the drop in\nprecision as recall increases is less noticeable, indicating improved\noverall performance of FTR-YOLO.\nThe detection results of four diseases of grape are shown in\nFigure 10. Figures 10A–D show the detection results of diseased\nleaves of anthracnose, grapevine white rot, gray mold, powdery\nmildew, respectively, while Figures 10E–G show the detection\nresults of diseased fruits of gray mold, grapevine white rot and\nanthracnose respectively. Figure 10H shows the diseased\ninﬂorescence of gray mold. The results indicate that the FTR-\nYOLO model exhibits precise detection of diverse symptoms in\ndifferent parts of the vine within natural scenes. This underscores\nthe model’s remarkable generalization and robustness. It is evident\nthat the majority of detection boxes have scores exceeding 0.8.\nAdditionally, a substantial portion of the diseased areas have been\naccurately detected, highlighting the exceptional precision and\nprecise localization capabilities of the proposed model. We also\ncompared the detection performance of different algorithms. For\ndetails, please seeFigure 11.\nThe experimental results in Figure 11 show that YOLOv5\nmissed some small objects, while the PPYOLOE and DINO-\nDETR algorithms detected additional object areas. There are\nslight differences in the detected bounding boxes and conﬁdence\nlevels among the different algorithms, which overall align with the\nexperimental results obtained in the paper. The proposed FTR-\nYOLO (Figure 10A) performs well in terms of detection accuracy\nand conﬁdence levels.\n4 Discussions\nBased on the information provided, the FTR-YOLO model is\nproposed in this paper to achieve accurate, real-time, and\nlightweight intelligent detection of four common grape diseases in\nnatural environments. The model incorporates several\nimprovements in its components. In backbone, the LH-VoVNet\nis introduced, which includ es LDS layer and Ghost-conv.\nAdditionally, eSE blocks and residual connections are added to\nthe OSA module (GC-RE-OSA module). Experimental results\npresented in Table 5 demonstrate that the LH-VoVNet achieves\noptimal performance in terms of detection (mAP 86.79%),\nlightweight design (Params 24.7MB), and real-time capabilities\n(FPS 56). The neck component also undergoes improvements.\nOnly the C5 feature map output by the backbone is selected as\nthe input for the real-time Transformer, includes 2D position\nembedding and SSTE. Additionally, the C3 module is replaced\nwith the GC-RE-OSA module in PAN + FPN. Experimental results\npresented inTable 6show that the improved neck further enhances\nperformance in detection (mAP 88.85%) and lightweight design\n(Params 22.5MB). In the head and loss component, the ITAP is\nproposed, and TAL is used with VFL and DFL. Experimental results\npresented inTable 7demonstrate that the ITAP Decoupled Head +\nTAL achieves an optimized mAP of 90.67%. Moreover,Table 8;\nFigures 7 , 8 show the superior performance of FTR-YOLO\ncompared to state-of-the-art YOLO detectors in terms of accuracy\n(mAP 90.67%), speed (FPS 44), and lightweight design (Params\n24.5MB), particularly improved accuracy on smaller scales (XS and\nS) and different sample sizes.\nAB\nDEF\nC\nFIGURE 11\nThe detection results of different methods.(A) YOLOv5, (B) YOLOv6, (C) YOLOv7, (D) YOLOv8, (E) PPYOLO-E, and(F) DINO-DETR.\nLiu et al. 10.3389/fpls.2024.1269423\nFrontiers inPlant Science frontiersin.org13\n5 Conclusion and future works\nIn this paper, we propose a real-time and lightweight detection\nmodel, called Fusion Transformer YOLO, for grape disease detection.\nIn backbone, we integrate GC-RE-OSA module based on VoVnet,\neffectively improving the ability of network to extract feature\ninformation while keeping the network lightweight. In neck\ncomponent, an improved Real-Time Transformer with 2D position\nembedding and SSTE are incorporated to the last feature map to\naccurate detection of small targets in natural environments. In head\ncomponent, the Decoupled Head based on the ITAP is adopted to\noptimize detection strategy. Our proposed FTR-YOLO achieved\n24.5MB Params, 90.67% mAP@0.5 with 44 FPS, which\noutperformed YOLOv5-v8 and PP-YOLOE. Although FTR-YOLO\nuses a real-time Transformer to improve model performance, it still\nfalls behind DETR in terms of performance due to DETR’s multi-\nscale and multi-layer global transformer architecture.\nFuture studies plan to explore the fusion of CNN and transformer\nmodels, as well as the integration of multimodal features, to further\nenhance the model’s performance. Additionally, this paper focuses on\ndisease detection in grapes, theoretically, the FTR-YOLO algorithm\nhas the potential to achieve good performance when retrained on\nother datasets. It can be applied to tasks such as the detection of plant\ntraits and pest diseases in other plants.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material. Further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nYL: Conceptualization, Methodology, Software, Writing –\noriginal draft, Writing – review & editing. QY: Data curation,\nFunding acquisition, Writing– review & editing. SG: Supervision,\nValidation, Visualization, Writing– review & editing.\nFunding\nThe author(s) declare ﬁnancial support was received for the\nresearch, authorship, and/or publication of this article. This\nresearch is supported by the National Natural Science Foundation\nof China under Grant No. ZZG0011806; Scientiﬁc research Project\nof Tianjin Science and Technology Commission under Grant No.\n2022KJ108 and 2022KJ110; Tianjin University of Technology and\nEducation Key Talent Project under Grant No. KYQD202104\nand KYQD202106.\nAcknowledgments\nWe are grateful for the reviewers’hard work and constructive\ncomments, which allowed us to improve the quality of\nthis manuscript.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their af ﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAdeel, A., Khan, M. A., Akram, T., Sharif, A., Yasmin, M., Saba, T., et al. (2022).\nEntropy-controlled deep features selec tion framework for grape leaf diseases\nrecognition. Expert Syst. 39, e12569. doi:10.1111/exsy.12569\nAhmad, A., Saraswat, D., and El Gamal, A. (2023). A survey on using deep learning\ntechniques for plant disease diagnosis and recommendations for development of\nappropriate tools.Smart Agric. Technol.3, 100083. doi:10.1016/j.atech.2022.100083\nBalakrishna, K., and Rao, M. (2019). Tomato plant leaves disease classiﬁcation using\nKNN and PNN.Int. J. Comput. Vision Image Process. (IJCVIP)9, 51–63. doi:10.4018/\nIJCVIP.2019010104\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S.\n(2020). End-to-end object detection with tra nsformers. European conference on\ncomputer vision (Cham. Springer International Publishing), 213–229.\nElnahal, A., El-Saadony, M., Saad, A., Desoky, E.-S., Eltahan, A., Rady, M., et al.\n(2022). Correction: The use of microbial inoculants for biological control, plant growth\npromotion, and sustainable agriculture: A review. Eur. J. Plant Pathol. 162, 1–1.\ndoi: 10.1007/s10658-022-02472-3\nEl-Saadony, M. T., Saad, A. M., Soliman, S. M., Salem, H. M., Ahmed, A. I.,\nMahmood, M., et al. (2022). Plant growth-promoting microorganisms as biocontrol\nagents of plant diseases: Mechanisms, challenges and future perspectives.Front. Plant\nSci. 13. doi:10.3389/fpls.2022.923880\nFeng, C., Zhong, Y., Gao, Y., Scott, M. R., and Huang, W. (2021).Tood: Task-aligned\none-stage object detection. 2021 IEEE/CVF International Conference on Computer\nVision (ICCV) (IEEE Computer Society, Montreal, QC, Canada), 3490 –3499.\ndoi: 10.1109/ICCV48922.2021.00349\nFuentes, A. F., Yoon, S., Lee, J., and Park, D. S. (2018). High-performance deep neural\nnetwork-based tomato plant diseases and pests diagnosis system with reﬁnement ﬁlter\nbank. Front. Plant Sci.9. doi:10.3389/fpls.2018.01162.\nFuentes, A., Yoon, S., and Park, D. S. (2019). Deep learning-based phenotyping\nsystem with glocal description of plant anomalies and symptoms.Front. Plant Sci.10.\ndoi: 10.3389/fpls.2019.01321\nGe, Z., Liu, S., Wang, F., Li, Z., and Sun, J. (2021). Yolox: Exceeding yolo series in\n2021. arXiv preprint arXiv:2107.08430. doi:10.48550/arXiv.2107.08430\nLiu et al. 10.3389/fpls.2024.1269423\nFrontiers inPlant Science frontiersin.org14\nGuan, H., Fu, C., Zhang, G., Li, K., Wang, P., and Zhu, Z. (2023). A lightweight model\nfor efﬁcient identiﬁcation of plant diseases and pests based on deep learning.Front.\nPlant Sci. 14. doi:10.3389/fpls.2023.1227011\nHameed, J., and Üstündağ , B. (2020). Evolutionary feature optimization for plant leaf\ndisease detection by deep neural networks. Int. J. Comput. Intell. Syst. 13, 12.\ndoi: 10.2991/ijcis.d.200108.001\nHatuwal, B., Shakya, A., and Joshi, B. (2021). Plant leaf disease recognition using\nrandom forest, KNN, SVM and CNN.Polibits 62, 13–19. doi:10.17562/PB-62-2\nH u ,J . ,S h e n ,L . ,a n dS u n ,G .( 2 0 1 8 ) .“Squeeze-and-excitation networks, ” in\nProceedings of the IEEE conference on computer vision and pattern recognition. Salt\nLake City, UT, USA 7132–7141. doi:10.1109/CVPR.2018.00745\nHuang, H., Huang, T., Li, Z., Lyu, S., and Hong, T. (2021). Design of citrus fruit\ndetection system based on mobile platform and edge computer device.Sensors 22, 59.\ndoi: 10.3390/s22010059\nJi, M., Zhang, L., and Wu, Q. (2020). Automatic grape leaf diseases identiﬁcation via\nUnitedModel based on multiple convolutional neural networks.Inf. Process. Agric.7,\n418–426. doi:10.1016/j.inpa.2019.10.003\nJiang, P., Chen, Y., Liu, B., He, D., and Liang, C. (2019). Real-time detection of apple\nleaf diseases using deep learning approach based on improved convolutional neural\nnetworks. IEEE Access 7, 59069–59080. doi:10.1109/Access.6287639.\nJianming, Z., Chaoquan, L., Xudong, L., Hye-Jin, K., and Jin, W. (2019). A full\nconvolutional network based on DenseNet for remote sensing scene classiﬁcation.\nMath. Biosci. Eng.16, 3345–3367. doi:10.3934/mbe.2019167\nJocher, G., Stoken, A., Borovec, J., Chaurasia, A., Changyu, L., Hogan, A., et al.\n(2021). ultralytics/yolov5: v5. 0-YOLOv5-P6 1280 models, AWS, Supervise. ly and\nYouTube integrations (Zenodo).\nKaur, P. P., and Singh, S. (2021).Classiﬁcation of Herbal Plant and Comparative\nAnalysis of SVM and KNN Classiﬁer Models on the Leaf Features Using Machine\nLearning (Singapore: Springer Singapore), 227–239.\nKuznetsova, A., Maleva, T., and Soloviev, V. (2020).Detecting apples in orchards\nusing YOLOv3 and YOLOv5 in general and close-up images. Advances in Neural\nNetworks–ISNN 2020: 17th International Symposium on Neural Networks, ISNN 2020,\nCairo, Egypt, December 4–6, 2020, Proceedings 17, 2020(Springer, Cham), 233–243.\ndoi: 10.1007/978-3-030-64221-1_20\nLee, C. P., Lim, K. M., Song, Y. X., and Alqahtani, A. (2023). Plant-CNN-ViT: plant\nclassiﬁ\ncation with ensemble of convolutional neural networks and vision transformer.\nPlants 12, 2642. doi:10.3390/plants12142642\nLi, C., Li, L., Geng, Y., Jiang, H., Cheng, M., Zhang, B., et al. (2023). Yolov6 v3. 0: A\nfull-scale reloading.arXiv preprint arXiv:2301.05586. doi:10.48550/arXiv.2301.05586\nLi, X., Wang, W., Wu, L., Chen, S., Hu, X., Li, J., et al. (2020). Generalized focal loss:\nLearning qualiﬁed and distributed bounding boxes for dense object detection.Adv.\nNeural Inf. Process. Syst.33, 21002–21012. doi:10.1109/CVPR46437.2021.01146\nLiu, H., Jiao, L., Wang, R., Xie, C., Du, J., Chen, H., et al. (2022). WSRD-Net: A\nconvolutional neural network-based arbitrary-oriented wheat stripe rust detection\nmethod. Front. Plant Sci.13. doi:10.3389/fpls.2022.876069\nL i u ,J . ,a n dW a n g ,X .( 2 0 2 0 ) .T o m a t od i s e a s es and pests detection based on improved Yolo\nV3 convolutional neural network.Front. Plant Sci.11. doi:10.3389/fpls.2020.00898\nLong, X., Deng, K., Wang, G., Zhang, Y., Dang, Q., Gao, Y., et al. (2020). PP-YOLO:\nAn effective and ef ﬁcient implementation of object detector. arXiv preprint\narXiv:2007.12099. doi:10.48550/arXiv.2007.12099\nLu, X., Yang, R., Zhou, J., Jiao, J., Liu, F., Liu, Y., et al. (2022). A hybrid model of\nghost-convolution enlightened transformer for effective diagnosis of grape leaf disease\nand pest. J. King Saud University-Computer Inf. Sci. 34, 1755–1767. doi: 10.1016/\nj.jksuci.2022.03.006\nPinheiro, I., Moreira, G., Queiró s da Silva, D., Magalhães, S., Valente, A., Moura\nOliveira, P., et al. (2023). Deep learning YOLO-based solution for grape bunch\ndetection and assessment of biophysical lesions.Agronomy 13, 1120. doi: 10.3390/\nagronomy13041120\nQi, C., Nyalala, I., and Chen, K. (2021). Detecting the earlyﬂowering stage of tea\nchrysanthemum using the F-YOLO model. Agronomy 11, 834. doi: 10.3390/\nagronomy11050834\nQiu, R.-Z., Chen, S.-P., Chi, M.-X., Wang, R.-B., Huang, T., Fan, G.-C., et al. (2022).\nAn automatic identiﬁcation system for citrus greening disease (Huanglongbing) using a\nYOLO convolutional neural network. Front. Plant Sci. 13. doi: 10.3389/\nfpls.2022.1002606\nRathor, S. (2021).“Ensemble based plant species recognition system using fusion of\nhog and kaze approach, ” in Futuristic Trends in Network and Communication\nTechnologies. Eds. P. K. Singh, G. Veselov, V. Vyatkin, A. Pljonkin, J. M. Dodero\nand Y. Kumar (Springer Singapore, Singapore), 536–545.\nSanath Rao, U., Swathi, R., Sanjana, V., Arpitha, L., Chandrasekhar, K., and Naik, P.\nK. (2021). Deep learning precision farming: grapes and mango leaf disease detection by\ntransfer learning.Global Transitions Proc.2, 535–544. doi:10.1016/j.gltp.2021.08.002\nSingh, V., and Misra, A. K. (2017). Detection of plant leaf diseases using image\nsegmentation and soft computing techniques.Inf. Process. Agric.4, 41–49. doi:10.1016/\nj.inpa.2016.10.005\nSoeb, M. J. A., Jubayer, M. F., Tarin, T. A., Al Mamun, M. R., Ruhad, F. M., Parven,\nA., et al. (2023). Tea leaf disease detection and identiﬁcation based on YOLOv7\n(YOLO-T). Sci. Rep. 13, 6078. doi:10.1038/s41598-023-33270-4\nSozzi, M., Cantalamessa, S., Cogato, A., Kayad, A., and Marinello, F. (2022).\nAutomatic bunch detection in white grape varieties using YOLOv3, YOLOv4, and\nYOLOv5 deep learning algorithms. Agronomy 12, 319. doi: 10.3390/\nagronomy12020319\nTerven, J., and Cordova-Esparza, D. (2023). A Comprehensive Review of YOLO\nArchitectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS.Mach.\nLearn. Knowl. Extr.5, 1680–1716. doi:10.3390/make5040083\nTrivedi, V. K., Shukla, P. K., and Pandey, A. (2022). Automatic segmentation of plant\nleaves disease using min-max hue histogram and k-mean clustering.Multimedia Tools\nAppl. 81, 20201–20228. doi:10.1007/s11042-022-12518-7\nWang, C.-Y., Bochkovskiy, A., and Liao, H.-Y. M. (2023).“YOLOv7: Trainable bag-\nof-freebies sets new state-of-the-art for real-time object detectors,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition .( C V P R ) ,\nVancouver, BC, Canada 7464–7475. doi:10.1109/CVPR52729.2023.00721\nWang, L., Zhao, Y., Liu, S., Li, Y., Chen, S., and Lan, Y. (2022). Precision detection of\ndense plums in orchards using the improved YOLOv4 model.Front. Plant Sci. 13.\ndoi: 10.3389/fpls.2022.839269\nWu, K., Peng, H., Chen, M., Fu, J., and Chao, H. (2021).“Rethinking and improving\nrelative position encoding for vision transformer,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. (ICCV): Montreal, QC, Canada 10033–\n10041. doi:10.1109/ICCV48922.2021.00988\nXie, X., Ma, Y., Liu, B., He, J., Li, S., and Wang, H. (2020). A deep-learning-based\nreal-time detector for grape leaf diseases using improved convolutional neural\nnetworks. Front. Plant Sci.11. doi:10.3389/fpls.2020.00751\nYang, L., Xu, S., Yu, X., Long, H., Zhang, H., and Zhu, Y. (2023). A new model based\non improved VGG16 for corn weed identiﬁcation. Front. Plant Sci.14. doi: 10.3389/\nfpls.2023.1205151\nZhang, Z., Qiao, Y., Guo, Y., and He, D. (2022). Deep learning based automatic grape\ndowny mildew detection.Front. Plant Sci.13. doi:10.3389/fpls.2022.872107\nZhang, H., Wang, Y., Dayoub, F., and Sunderhauf, N. (2021).“Varifocalnet: An iou-\naware dense object detector,” in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition. (CVPR): Nashville, TN, USA 8514–8523. doi:10.1109/\nCVPR46437.2021.00841\nZhang, B., Wang, R., Zhang, H., Yin, C., Xia, Y., Fu, M., et al. (2022). Dragon fruit\ndetection in natural orchard environment by integrating lightweight network and\nattention mechanism.Front. Plant Sci.13. doi:10.3389/fpls.2022.1040923.\nZhao, L., Niu, R., Li, B., Chen, T., and Wang, Y. (2022). Application of improved\ninstance segmentation algorithm based on VoVNet-v2 in open-pit mines remote\nsensing pre-survey.Remote Sens.\n14, 2626. doi:10.3390/rs14112626\nZhou, J., Hu, W., Zou, A., Zhai, S., Liu, T., Yang, W., et al. (2022). Lightweight\ndetection algorithm of kiwifruit based on improved YOLOX-S.Agriculture 12, 993.\ndoi: 10.3390/agriculture12070993\nZhu, J., Cheng, M., Wang, Q., Yuan, H., and Cai, Z. (2021). Grape leaf black rot\ndetection based on super-resolution image enhancement and deep learning.Front.\nPlant Sci.12. doi:10.3389/fpls.2021.695749\nZhu, X., Su, W., Lu, L., Li, B., Wang, X., and Dai, J. (2020). Deformable detr:\nDeformable transformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159. doi:10.48550/arXiv.2010.04159\nLiu et al. 10.3389/fpls.2024.1269423\nFrontiers inPlant Science frontiersin.org15"
}