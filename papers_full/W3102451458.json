{
  "title": "s-Transformer: Segment-Transformer for Robust Neural Speech Synthesis",
  "url": "https://openalex.org/W3102451458",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100442237",
      "name": "Xi Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5110320609",
      "name": "Huaiping Ming",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5061724331",
      "name": "Lei He",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5065394791",
      "name": "Frank K. Soong",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963609956",
    "https://openalex.org/W2972702018",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W3015194534",
    "https://openalex.org/W2970730223",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2936123380",
    "https://openalex.org/W3015974384",
    "https://openalex.org/W2903739847",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W1525783482",
    "https://openalex.org/W2949382160",
    "https://openalex.org/W2963091184",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2964272710",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963729263",
    "https://openalex.org/W2970006822",
    "https://openalex.org/W349236604"
  ],
  "abstract": "Neural end-to-end text-to-speech (TTS) , which adopts either a recurrent model, e.g. Tacotron, or an attention one, e.g. Transformer, to characterize a speech utterance, has achieved significant improvement of speech synthesis. However, it is still very challenging to deal with different sentence lengths, particularly, for long sentences where sequence model has limitation of the effective context length. We propose a novel segment-Transformer (s-Transformer), which models speech at segment level where recurrence is reused via cached memories for both the encoder and decoder. Long-range contexts can be captured by the extended memory, meanwhile, the encoder-decoder attention on segment which is much easier to handle. In addition, we employ a modified relative positional self attention to generalize sequence length beyond a period possibly unseen in the training data. By comparing the proposed s-Transformer with the standard Transformer, on short sentences, both achieve the same MOS scores of 4.29, which is very close to 4.32 by the recordings; similar scores of 4.22 vs 4.2 on long sentences, and significantly better for extra-long sentences with a gain of 0.2 in MOS. Since the cached memory is updated with time, the s-Transformer generates rather natural and coherent speech for a long period of time.",
  "full_text": "s-Transformer: Segment-Transformer for Robust Neural Speech Synthesis\nXi Wang, Huaiping Ming, Lei He, Frank K. Soong\nMicrosoft (China) Corporation\n{xwang, huming, helei, frankkps}@microsoft.com\nAbstract\nNeural end-to-end text-to-speech (TTS) , which adopts either a\nrecurrent model, e.g. Tacotron, or an attention one, e.g. Trans-\nformer, to characterize a speech utterance, has achieved sig-\nniÔ¨Åcant improvement of speech synthesis. However, it is still\nvery challenging to deal with different sentence lengths, partic-\nularly, for long sentences where sequence model has limitation\nof the effective context length. We propose a novel segment-\nTransformer (s-Transformer), which models speech at segment\nlevel where recurrence is reused via cached memories for both\nthe encoder and decoder. Long-range contexts can be captured\nby the extended memory, meanwhile, the encoder-decoder at-\ntention on segment which is much easier to handle. In addition,\nwe employ a modiÔ¨Åed relative positional self attention to gen-\neralize sequence length beyond a period possibly unseen in the\ntraining data. By comparing the proposed s-Transformer with\nthe standard Transformer, on short sentences, both achieve the\nsame MOS scores of 4.29, which is very close to 4.32 by the\nrecordings; similar scores of 4.22 vs 4.2 on long sentences, and\nsigniÔ¨Åcantly better for extra-long sentences with a gain of 0.2\nin MOS. Since the cached memory is updated with time, the s-\nTransformer generates rather natural and coherent speech for a\nlong period of time.\nIndex Terms: speech synthesis, TTS, Transformer, segment re-\ncurrence, cached memory\n1. Introduction\nEnd-to-end sequence-based TTS models such as Tacotron [1]\n[2] and Transformer TTS [3] [4] have successfully applied to\nmodel ‚ü®text,speech‚ü©pairs. With the help of high quality neu-\nral vocoder like WaveNet[5], LPCNet [6], GAN based vocoder\n[7], and etc, the generated speech quality could be very natural.\nAs a uniÔ¨Åed framework, the seq2seq TTS model has largely im-\nproved the model capability, taking advantages of direct mod-\neling from text input to speech and whole utterance modeling\nwith either RNN attention or Transformer multi-head attention.\nHowever, it is still challenging for the seq2seq TTS model\nto generate extra long sentences, as the ability of modeling com-\nplex and long-term dependency remains a research problem.\nRNNs are difÔ¨Åcult to optimize due to gradient vanishing and ex-\nplosion [8] while the sequence length increasing. Transformer\n[9], based solely on attention mechanisms, with global depen-\ndencies between input and output, owing efÔ¨Åcient computation\nand high parallelization, requires large memory growth of se-\nquence length and is difÔ¨Åcult to optimize during training on the\nvarious length speech corpus, especially when the acoustic fea-\nture number arrives to thousands of frames.\nHow to capture the long-term dependency has been a\ngeneric challenge for the sequence transduction model. Vari-\nous attention mechanisms are tried in the encoder-decoder at-\ntention model, like stepwise monotonic attention [10], forward-\nSamples are available at https://vancycici.github.io/sTransformer/\nattention[11], monotonic chunkwise attention [12], and etc.\nHowever, these attention mechanisms are usually based on pre-\nvious state calculation, and not easy to apply into the high par-\nallelized attention calculation of Transformer model. Sparse-\nTransformer [13], introducing sparse factorization on the atten-\ntion matrix, gains the ability to produce thousands of timesteps\non images, audio and texts, nevertheless it only works for the\nself-attention, which is not direct suitable for encoder-decoder\nattention as different length pair as ‚ü®text,speech‚ü©. In the\nspeech recognition tasks, there are many works on the streaming\nend-to-end model [14], using CTC plus Transformer [15][16]\nor RNN-Transducer [17] plus LAS [18] [19]. These two-stage\nÔ¨Çows need to be jointly decoded or rescored later. Transformer-\nXL [20], incorporating a segment-level recurrence mechanism\nand a novel positional encoding schema, achieves excellent per-\nformance in language model task for extra long sequence. It\nreuse the hidden states obtained from previous segments and\nbuilds up a recurrent connection between segments. Thus, mod-\neling very long-term dependency becomes possible because in-\nformation can be propagated through the recurrent connections.\nInspired by the segment streaming model, we think it would\nbe feasible to model speech utterances into segments without\ndisrupting the temporal coherence. We have tried segment\nbased Tacotron model using partial conditioning with incre-\nmental prediction[14], but due to the size limitation of previous\nRNN state, the experiment reports inferior performance. Ac-\ncordingly we turn to Transformer-XL like model which is Ô¨Çex-\nible and high efÔ¨Åcient, and propose s-Transformer for speech\nsynthesis. This recurrence mechanism is incorporated in both\nTransformer encoder and decoder self-attention, as text input\nand spectrogram output segments, with previous segments com-\nputed as hidden states cached as extended context, while the\nencoder-decoder attention aligns in segment-level. The cached\nmemory lengths for encoder and decoder are setting differently\nregarding the assumptions that text sequence usually has a long-\nterm dependence while the speech spectrogram is more local\nrelated. We set a large cached memory size for encoder, and a\nsmaller one for decoder. With the decoder cached memory, it\nallows the adjacent acoustic features between segments to par-\nticipate in the computation as segment overlap, to relax the strict\nalignment of segment and maintain the coarticulation in contin-\nuous speech. As the cached memory size is Ô¨Åxed, the mem-\nory will update while segment passing through and ‚Äúforget‚Äù the\n‚Äútoo far‚Äù segment when it exceeds the memory capacity. What‚Äôs\nmore, with the relative positional self attention, it gains gener-\nalized attribute regardless of various speech utterance lengths.\nThe s-Transformer can generate extra long utterance which is\neven unseen in the corpus and obtains the appealing modeling\ncapability beyond sentences like paragraphs.\n2. s-Transformer\nThe proposed s-Transformer based speech synthesis system\nis shown in Figure 1. Given an input text sequence em-\narXiv:2011.08480v1  [eess.AS]  17 Nov 2020\nEncoder-decoder \nattention\nSelf attention \nwith recurrence\nùëå = {ùë¶1,ùë¶2,‚Ä¶ùë¶ùëá}Mel spectrogram\nInput text ùëã = {ùë•1,ùë•2,‚Ä¶ùë•ùëô}\nMasked self \nattention with \nrecurrence\nTransformer \nDecoder\nRelative \npositional \nencoding\nRelative \npositional \nencoding\nTransformer \nEncoder\nMasked self \nattention with \nrecurrence\nMasked self \nattention with \nrecurrence\nSelf attention \nwith recurrence\nSelf attention \nwith recurrence\nEncoder Pre-net\nDecoder Pre-net\nDecoder Pos-Net Stop token\nFigure 1: An overview of proposed s-Transformer based speech\nsynthesis. The green and red modules are different as standard\nTransformer TTS. Dot line modules represent previous and next\nsegments.\nbedding x = {x1,x2,...x l} and output mel-spectrogram\ny = {y1,y2,...y T}, with alignment, it approximately splits\naccording to a predeÔ¨Åned input chunk length as xœÑ =\n{xœÑ,1,xœÑ,2,...x œÑ,l}and corresponding output chunk yœÑ =\n{yœÑ,1,yœÑ,2,...y œÑ,l‚Ä≤ }. Different from Transformer TTS mod-\neling on the whole utterance level, it applies the s-Transformer\nmodel on the speech segment, keeping the segments internal or-\nder in the utterance. In the Transformer encoder and decoder\nblocks, the self-attention modules are calculated with previous\nsegment recurrence as extended context with the help of cached\nencoder and decoder memories. The Transformer encoder-\ndecoder attention remains the same to produce variable output\nlengths by using the query vectors and decoder states to gen-\nerate the attention matrix to a sequence of encoder state. Al-\nthough the gradients are calculated in the segment, the addi-\ntional extended context allows the network to exploit the long-\nrange context of previous history information. In other words,\nthe s-Transformer essentially works similar as the monotonic\nchunk-wise attention with segment recurrence, to ease the op-\ntimization for s-Transformer architecture while learning long-\nterm dependency.\n2.1. Segment level recurrence\nWith the recurrence mechanism to Transformer architecture for\nconsecutive segments, the hidden state sequence computed for\nCurrent segment ùë•ùúèPrevious segment ùë•ùúè‚àí1 Next segment ùë•ùúè+1\nFigure 2: Illustration of segment level recurrence for self atten-\ntion, with an example of segment length 4.\nthe previous segment is cached to be reused as an extended con-\ntext when the model processes the current segment. Let the\ntwo consecutive segments be as xœÑ = {xœÑ,1,xœÑ,2,...,x œÑ,l}and\nxœÑ+1 = {xœÑ+1,1,xœÑ+1,2,...,x œÑ+1,l‚Ä≤ }respectively, denoting\nthe n-th layer hidden state sequence produced for the œÑ ‚àíth\nsegment sœÑ. Then, the n-th layer hidden state for segment sœÑ+1\nis calculated as followings:\nÀúh= [SG(hn‚àí1\nœÑ ) ‚ó¶hn‚àí1\nœÑ+1 ] (1)\nqn\nœÑ+1 = hn‚àí1\nœÑ+1 WT\nq (2)\nkn\nœÑ+1,vn\nœÑ+1 = Àúhn‚àí1\nœÑ+1 WT\nk,Àúhn‚àí1\nœÑ+1 WT\nv (3)\nhn\nœÑ+1 = Transformer-layer(qn\nœÑ+1,kn\nœÑ+1,vn\nœÑ+1) (4)\nWhere the function SG(¬∑) stands for stop gradient, the no-\ntion [hv ‚ó¶hu] indicates the concatenation operation for the\ntwo sequences along the length dimension as extended con-\ntext, and W denote the Transformer model matrix parameters.\nThe critical difference is the hidden state Àúhn‚àí1\nœÑ+1 which is ex-\ntended with previous segment. Correspondingly, the current\nsegment based key kn\nœÑ+1 and value vn\nœÑ+1are derived from this\nextended hidden state, while queryqn\nœÑ+1 remains the same com-\nputation from standard hidden state as hn‚àí1\nœÑ+1 . The cached mem-\nory could contain multiple previous segments, which extends\nthe context information beyond just two consecutive segments.\nIt‚Äôs noticed that the recurrent dependency between connected\nlayers shifts one layer downwards per-segments, which is close\nto global dependency as Transformer self-attention, using lo-\ncal query to compute the attention with extended hidden state\nderived key and value. Consequently, the largest dependence\nlength grows linearly with number of layers, segment length\nand cached memory length.\n2.2. Relative positional self attention\nIn order for the Transformer model to make use of the sequence\norder, the relative or absolute positional embedding is injected\nwith the input embeddings at the bottoms of the encoder and de-\ncoder stacks. In Transformer TTS, we use scaled positional en-\ncoding [3] instead of Ô¨Åxed positional embeddings, to adaptively\nÔ¨Åt the scale of both encoder and decoder, regarding the range of\ntext and speech sequence are quite different. The scaled weights\nÔ¨Åt for both encoder and decoder positional embedding are train-\nable. When we reuse the extended context into hidden state, we\nshould also handle the coherence of position information. In\nthe segment positional embedding, information of relative po-\nsition in the internal segment sequence is injected by triangle\npositional embeddings. Meanwhile, similar as [21][22] intro-\nducing relative position representations to allow attention to be\ninformed by how far two positions are apart in the sequence,\nwe propose to add a trainable bias matrix to learn the relative\nposition embedding for the extended context between a query\nand key respectively. In the standard Transformer TTS, encoder\nself-attention has a masking bias matrix to mask the padding in-\nput embeddings, while decoder self-attention has a casual mask-\ning bias matrix to prevent the usage of future mel-spectrograms.\nLet the masking bias matrix denoted as Bcur which is based on\ncurrent segment input embeddings, the trainable bias matrix de-\nnoted as Brel for the extended context, anL√ó(L+M) dimen-\nsional logits matrix multiplied by Q and K, which modulates\nthe attention probabilities for each head as:\nRelativeAttention = Softmax(Q(K)T + [Brel‚ó¶Bcur])V (5)\n[Brel ‚ó¶Bcur] indicates the concatenation of the relative\nextended bias matrix and corresponding bias matrix along the\ntime length dimension.\n2.3. Segment and sentence feature\n2.3.1. Chunk stop token\nSimilar as Tacotron2, Transformer-TTS model also employs an\nutterance stop token to help autoregressive generation stop emit-\nting output while synthesis. As in the s-Transformer, it should\nhave the capability to know when to stop generating output for\nthis segment. We propose to add a chunk stop token network\nwhich works together with utterance stop token. The chunk stop\nnetwork is a Linear network similar as stop network predicted\nfrom decoder output. The Ô¨Ånal stop logit is weighted summa-\nrized by both stop token and chunk stop token jointly to decide\nwhen to stop for this segment no matter it‚Äôs beginning, middle\nor ending part in the utterance.\nStopLogit = Lchunk(D)√ó(1‚àíuttend)+Lutt(D)√ó(1‚àíuttend)\n(6)\nWhere Lchunk and Lutt represent the Linear network of chunk\nstop and utterance stop token, D denotes the decoder output,\nand uttend is the 0/1 indicator as 1 when current segment is the\nending of utterances.\n2.3.2. Chunk speaking rate\nTo improve the various speaking rate for multiple speech seg-\nments, we propose to add a chunk speaking rate feature into\ntraining. The chunk speaking rate is calculated as:\nChunkSpeakingRate = Lchunk/Tchunk (7)\nLchunkand Tchunkrepresent the chunk input embedding length\nand corresponding mel-spectrogram frames. The chunk speak-\ning rate is calculated as a feature during the training, adding\nwith encoder output. Additionally, there is another chunk\nspeaking rate network as Linear network trained from encoder\noutput. During inference, the chunk speaking rate could be es-\ntimated from encoder output and averaged according to the en-\ncoder length dimension axis.\n2.3.3. Sentence feature\nThere are some global sentence features, such as the sentence\ntype which is implied in the ‚Äúfuture‚Äù segment, missing in the s-\nTransformer while model the current segment. To address this\nlimitation, we collect the sentence feature as additional global\nembedding input, pass through another Pre-net, add with the\nsegment input embedding as encoder input.\n3. Experiments\nWe apply Transformer TTS as baseline and s-Transformer on\na professional enUS female speaker, 45.8 hours, 16kHz, 16bit\nspeech corpus to evaluation the capability on Text-to-Speech\nvoice quality, include short, long and extra long sentences. Be-\nsides acoustic model, we train a 20 layer 16kHz 16bit WaveNet\nneural vocoder conditional by mel-spectrogram to generate the\nwaveform. All the subjective tests are evaluated via Microsoft\ncrowdsourcing UHRS (Universal Human Relevance System)\nplatform, with at least 9 native judges for Comparsion MOS\n(as CMOS)[23] and 15 judges for MOS test for each test case.\n3.1. Baseline\nThe baseline model have the similar architectures and hyper pa-\nrameters as Transformer TTS[3]. Both encoder and decoder are\ncomposed of 6 layers with hidden size as 512 and each multi-\nhead attention has 8 heads, including encoder-decoder attention.\nWe use phoneme as input and 80-channel mel-spectrogram as\noutput, the speech frame shift is 12.5ms. Dynamic batch size\nis used to support for efÔ¨Åcient parallel computing. The model\nis trained on NVIDIA V100 4xGPU 16GB, with maximum\nbatch size 64 on each GPU, using Adam optimizer Œ≤1 = 0.9,\nŒ≤2 = 0.999, œµ = 10‚àí8 and scheduled learning rate exponen-\ntially decay. At Ô¨Årst, we remove the long utterances ( >800\nframes as 10 seconds, about 20% in the corpus) to get a sta-\nble baseline model, for some long utterances in the corpus will\nlead the encoder-decoder attention hard to learn. Then, we use\nall utterances to reÔ¨Åne the model. To help the baseline model\non extra long sentence, we augment the corpus about 40% utter-\nances to increase the long sentences percentage by merging two\nsentences into one, from 1200 to 2400 frames length utterances\nincrease about 20% for further model reÔ¨Ånement.\n3.2. s-Transformer\nWe employ the same parameters as Transformer TTS, with 6\nlayers for both encoder and decoder with hidden size as 512, self\nmulti-head as 8, but the encoder-decoder multi-head attention\nnumber is reduced to 4 considering the segment alignment task\nwould be easier. The encoder and decoder Pre-net and output\nPos-net are the same as Baseline.\n3.2.1. Order chunk reader\nThere are some additional parameters for s-Transformer: chunk\nsize for input segment, cached encoder and decoder memory\nlength. The speech corpus is aligned by a speech recognition\nmodel on phoneme duration. Chunk size for input segment is\ncalculated for the input phonemes including punctuation, de-\nlimiter and etc., and corresponding me-spectrogram segment is\nobtained according to the phoneme alignment. The chunk size\nis not exactly segmention point, but to Ô¨Ånd the center phone.\nTo preserve the speech coarticulation for phonemes in a word,\na search window from center phone left to right is set as 20 to\nÔ¨Ånd an appropriate segmentation point, preferring punctuation\nor word boundary. The data reader supports to fetch the seg-\nments in utterance order within a Ô¨Åxed batch size.\n3.2.2. Training and synthesis\ns-Transformer performs training iteration on each segment\nbatch with fetched cached encoder and decoder memories. The\ncached memory will be zero initialized at each utterance begin-\nning. We conduct the s-Transformer training on NVIDIA V100\nTable 1: CMOS test for different domains. # calculate the average value. Input number include phoneme, punctuation, delimiter and\netc. 800, all and aug. means different training corpus, while aug. represent enlarged augmented extra-long sentences.\nDomain #input #word # ‚âàspeech Model Baseline Baseline Baseline\n(num) (num) (frames) (params enc mem & corpus) ( <800) (all) (aug.)\nShort 91 11.5 330 s-Transformer(120 & all) 0.033 -0.126 -\nLong 281 31.3 950 0.304 -0.012 -\nShort 91 11.5 330 s-Transformer(180 & all) - 0.048 -\nLong 281 31.3 950 - -0.033 -\nExtra-long 426 48.5 1500 s-Transformer(180 & aug.) - - 0.415\n8xGPU 32GB with Ô¨Åxed batch size 16 for each GPU. There is\nno need to prune the training corpus for s-Transformer is robust\nto various speech utterance lengths. We explore the following\nparameter settings for the experiments: chunk size=60, decoder\nmemory=4, encoder memory lengths as 120 and 180 respec-\ntively. During synthesis, the phoneme input is also followed\nthe same strategy for the segmentation. The whole utterance\nis generated segment by segment and merged in order as one\nutterance.\n3.3. Evaluation\nIn order to test the general voice quality of TTS model and the\ncapability of long-term dependence, we prepare 3 different do-\nmains of test set, each set contains 30 sentences, content from\nnews and audiobooks, but different on the text length (total word\nnumber) and rough estimated speech length according to syn-\nthesized speech.\nFirstly, we conduct a group of CMOS tests to compare\nthe voice quality between different Baseline and s-Transformer\nmodels. According to Table1 CMOS results, we observe that\ncomparing with Baseline( <800), the s-Transformer with en-\ncoder memory size 120 could achieve equal naturalness on short\nsentences, obvious better voice quality on long sentences, due to\nBaseline model is trained on a tailored corpus without over 800\nframes long utterances. After reÔ¨Ånement with all corpus, the\nBaseline model is improved on both short and long sentences.\nThe s-Transformer with encoder memory size 180 improve the\nvoice quality on both short and long sentence comparing with\nBaseline(all), as expected that longer context would help to im-\nprove the overall prosody and quality. To further test the capa-\nbility on extra-long sentence and impact of training corpus, we\nperform model reÔ¨Ånement on the augmented corpus included a\ncertain degree of augmented extra-long training utterances for\nboth Baseline and s-Transformer. We still observes many at-\ntention errors occurs when the input text is too long for Base-\nline model, especially the intelligibility drops a lot for Baseline\nmodel synthesis beyond 1500 frames, while s-Transfomer per-\nforms not much change. After removing some serious attention\nerror samples in the extra-long test set, the CMOS=0.415 in-\ndicates that s-Transformer has obvious advantage on the extra-\nlong sentences.\nThen, we select the Baseline (all) and s-Transformer with\nencoder memory 180 (all) for short and long sentences, and\nBaseline(aug.) and s-Transformer(aug.) for extra-long sen-\ntences to perform MOS test, to evaluate the general naturalness\nwith the recording in similar domain. For the short and long do-\nmains, Baseline and s-Transformer gain equal 4.29 MOS score,\nvery close to the recording as gap=0.03 on short sentences, big-\nger gap=0.2 on long sentences. SpeciÔ¨Åcally, for extra-long test\nset, s-Transformer signiÔ¨Åcant outperform Baseline with MOS\nscore 3.99 versus 3.79. It performs very stable on the extra-\nlong sentences, and in theory, it even be able to generate speech\nas long as you want. However, due to extra-long sentence like\naudiobook content is very challenging to obtain rich prosody,\ns-Transformer still has MOS gap 0.6 with recording, suggesting\nthat even longer-term context by enlarging the cached memory\nand extracting higher level history and future information are\npotential beneÔ¨Åcial.\nTable 2: MOS test for different domains\nDomain Recording Baseline s-Transformer\nShort 4.32 4.29 4.29\nLong 4.46 4.22 4.2\nExtra-long 4.63 3.79 3.99\nAt last, we select a extra long sentence sample as case study\nto observe the encoder-decode attention which represent the\nalignments for baseline and concatenate segment alignments for\ns-Transformer. In the Figure 3, comparing with Baseline, in par-\nticular the alignment over 1000 decoding steps, s-Transformer\nalignment is much clearer and smoother, which is essentially\nlike monotonic chunk-wise attention.\nFigure 3: Baseline vs. s-Transformer attention alignment, base-\nline from layer 0 head 1, s-Transfomer concatenates from 5 seg-\nments, layer 1 head 0\n4. Conclusions\nIn this paper, we propose a novel architecture as s-Transformer\nmodel for speech synthesis, which utilizes segment recur-\nrence to capture long-term dependence and performs segment-\nlevel encoder-decoder attention to handle the difÔ¨Åculty of long\npair ‚ü®text,speech‚ü©. Our experiments show the proposed s-\nTransformer can achieve equal naturalness as standard Trans-\nformer TTS on general test set, in particular stable and obvious\nbetter quality on extra-long sentences, even unseen long speech\nas couple minutes as thousands frames. With this segment re-\ncurrence mechanism, it builds up a framework for long speech\nsynthesis even beyond sentences.\n5. References\n[1] Y . Wang, R. Skerry-Ryan, D. Stanton, Y . Wu, R. J. Weiss,\nN. Jaitly, Z. Yang, Y . Xiao, Z. Chen, and S. B. et al., ‚ÄúTacotron:\nTowards end-to-end speech synthesis,‚Äù inProc Interspeech, 2017,\npp. 4006‚Äì4010.\n[2] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang,\nZ. Chen, Y . Zhang, Y . Wang, and R. S.-R. et al., ‚ÄúNatural tts syn-\nhtesis by conditioning wavenet on mel spectrogram predictions,‚Äù\nin IEEE International Conference on Acoustics, Speech and Sig-\nnal Processing(ICASSP), 2018, pp. 4779‚Äì4783.\n[3] N. Li, S. Liu, Y . Liu, S. Zhao, M. Liu, and M. Zhou, ‚ÄúNeural\nspeech synthesis with transformer network,‚Äù in AAAI Conference\non ArtiÔ¨Åcial Intelligence (AAAI), 2019.\n[4] Y . Ren, Y . Ruan, X. Tan, Q. Tao, S. Zhao, Z. Zhao, and T. Liu,\n‚ÄúFastspeech: Fast, robust and controllable text to speech,‚Äù inNeu-\nral Information Processing Systems, 2019, pp. 3165‚Äì4174.\n[5] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,\nA. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu,\n‚ÄúWavenet: A generative model for raw audio,‚Äù in ArXiv preprint\narXiv:1609.03499, 2016.\n[6] J. Valin and J. Skoglund, ‚ÄúLpcnet: Improving neural speech syn-\nthesis through linear prediction,‚Äù inProc. ICASSP 2019, Brighton,\nUnited Kingdom, 2019, pp. 5891‚Äì5895.\n[7] K. Kumar, R. Kumar, T. de Boissiere, L. Gestin, W. Z. Teoh,\nA. d. B. J. Sotelo, Y . Bengio, and A. Courville, ‚ÄúMelgan: Gen-\nerative adversarial networks for conditional waveform synthesis,‚Äù\nin Neural Information Processing Systems, 2019, pp. 5891‚Äì5895.\n[8] S. Hochreiter, Y . Bengio, P. Frasconi, and J. Schmidhuber, ‚ÄúGradi-\nent Ô¨Çow in recurrent nets: the difÔ¨Åculty of learning long-term de-\npendencies,‚Äù in Advances in Neural Information Processing Sys-\ntems, 2001.\n[9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù\nin Neural Information Processing Systems, 2017, pp. 6000‚Äì6010.\n[10] M. He, Y . Deng, and L. He, ‚ÄúRobust sequence-to-sequence acous-\ntic modeling with stepwise monotonic attention for neural tts,‚Äù in\nProc. Interspeech, 2019, pp. 1293‚Äì1297.\n[11] J.-X. Zhang, Z.-H. Ling, and L.-R. Dai, ‚ÄúForward attention in\nsequenceto-sequence acoustic modeling for speech synthesis,‚Äù in\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing(ICASSP), 2018, pp. 4789‚Äì4793.\n[12] C.-C. Chiu and C. Raffel, ‚ÄúMonotonic chunkwise attention,‚Äù in\narXiv preprint arXiv:1712.05382, 2017.\n[13] R. Child, S. Gray, A. Radford, and I. Sutskever, ‚ÄúGenerat-\ning long sequences with sparse transformers,‚Äù in arXiv preprint\narXiv:1904.10509, 2019.\n[14] N. Jaitly, D. Sussillo, Q. V . Le, O. Vinyals, I. Sutskever, and\nS. Bengio, ‚ÄúAn online sequence-to-sequence model using partial\nconditioning,‚Äù inNeural Information Processing Systems, 2016.\n[15] N. Moritz, T. Hori, and J. L. Roux, ‚ÄúTriggered attention for\nend-to-end speech recognition,‚Äù inProceedings of ICASSP . IEEE,\n2019, pp. 5666‚Äì5670.\n[16] ‚Äî‚Äî, ‚ÄúStreaming automatic speech recognition with the trans-\nformer model,‚Äù inProceedings of ICASSP . IEEE, 2020.\n[17] A. Graves, ‚ÄúSequence transduction with recurrent neural net-\nworks,‚Äù inarXiv preprint arXiv:1211.3711, 2012.\n[18] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, ‚ÄúListen, attend\nand spell: A neural network for large vocabulary conversational\nspeech recognition,‚Äù in Proceedings of ICASSP . IEEE, 2016, pp.\n4960‚Äì4964.\n[19] T. N. Sainath, Y . He, B. Li, A. Narayanan, R. Pang, A. Bruguier,\nS. Chang, and et al., ‚ÄúA streaming on-device end-to-end model\nsurpassing server-side conventional model quality and latency,‚Äù\nin Proceedings of ICASSP . IEEE, 2020.\n[20] Z. Dai, Z. Yang, Y . Yang, W. W. Cohen, J. Carbonell, Q. V .\nLe, and R. Salakhutdinov, ‚ÄúTransformer-xl: Attentive lan-\nguage models beyond a Ô¨Åxed-length context,,‚Äù in arXiv preprint\narXiv:1901.02860, 2019.\n[21] P. Shaw, J. Uszkoreit, and A. Vaswani, ‚ÄúSelf-attention\nwith relative position representations,‚Äù in arXiv preprint\narXiv:1803.02155, 2018.\n[22] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, I. Simon,\nC. Hawthorne, N. Shazeer, A. M. Dai, M. D. Hoffman, M. Din-\nculescu, and D. Eck, ‚ÄúMusic transformer: Generating music\nwith long-term structure,‚Äù inSeventh International Conference on\nLearning Representations, 2018.\n[23] P. C. Loizou, ‚ÄúSpeech quality assessment,‚Äù Multimedia analysis,\nprocessing and communications, pp. 623‚Äì654, 2011.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6815261840820312
    },
    {
      "name": "Computer science",
      "score": 0.492021769285202
    },
    {
      "name": "Speech recognition",
      "score": 0.4495290517807007
    },
    {
      "name": "Electrical engineering",
      "score": 0.21344056725502014
    },
    {
      "name": "Engineering",
      "score": 0.20974934101104736
    },
    {
      "name": "Voltage",
      "score": 0.11416149139404297
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ]
}