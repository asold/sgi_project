{
    "title": "Evaluating the Potential of Large Language Models for Vestibular Rehabilitation Education: A Comparison of ChatGPT, Google Gemini, and Clinicians",
    "url": "https://openalex.org/W4391213634",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2036613820",
            "name": "Yael Arbel",
            "affiliations": [
                "Bar-Ilan University"
            ]
        },
        {
            "id": "https://openalex.org/A2041319985",
            "name": "Yoav Gimmon",
            "affiliations": [
                "Sheba Medical Center",
                "University of Haifa"
            ]
        },
        {
            "id": "https://openalex.org/A301484997",
            "name": "Liora Shmueli",
            "affiliations": [
                "Bar-Ilan University"
            ]
        },
        {
            "id": "https://openalex.org/A2036613820",
            "name": "Yael Arbel",
            "affiliations": [
                "Bar-Ilan University"
            ]
        },
        {
            "id": "https://openalex.org/A2041319985",
            "name": "Yoav Gimmon",
            "affiliations": [
                "Sheba Medical Center",
                "University of Haifa"
            ]
        },
        {
            "id": "https://openalex.org/A301484997",
            "name": "Liora Shmueli",
            "affiliations": [
                "Bar-Ilan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4251534361",
        "https://openalex.org/W4200603351",
        "https://openalex.org/W2761907689",
        "https://openalex.org/W2580661229",
        "https://openalex.org/W2257747823",
        "https://openalex.org/W1548611712",
        "https://openalex.org/W4293769421",
        "https://openalex.org/W2890973584",
        "https://openalex.org/W3091799942",
        "https://openalex.org/W4366999665",
        "https://openalex.org/W4368340908",
        "https://openalex.org/W4319868628",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4391996564",
        "https://openalex.org/W4392505993",
        "https://openalex.org/W4367175507",
        "https://openalex.org/W4319455199",
        "https://openalex.org/W4319460874",
        "https://openalex.org/W4361017283",
        "https://openalex.org/W4379599010",
        "https://openalex.org/W4394791257",
        "https://openalex.org/W2099714048",
        "https://openalex.org/W2981296841",
        "https://openalex.org/W4319460381",
        "https://openalex.org/W4327946446",
        "https://openalex.org/W2162821268"
    ],
    "abstract": "Abstract Objective We aimed to evaluate the performance of two publicly available large language models, ChatGPT and Google Gemini in response to multiple-choice questions related to vestibular rehabilitation. Methods The study was conducted among 30 physical therapist professionals experienced with VR (vestibular rehabilitation) and 30 physical therapy students. They were asked to complete a Vestibular Knowledge Test (VKT) consisting of 20 multiple-choice questions that were divided into three categories: (1) Clinical Knowledge, (2) Basic Clinical Practice, and (3) Clinical Reasoning. ChatGPT and Google Gemini were tasked with answering the same 20 VKT questions. Three board-certified otoneurologists independently evaluated the accuracy of each response using a 4-level scale, ranging from comprehensive to completely incorrect. Results ChatGPT outperformed Google Gemini with a 70% score on the VKT test, while Gemini scored 60%. Both excelled in Clinical Knowledge with a perfect score of 100% but struggled in Clinical Reasoning with ChatGPT scoring 50% and Gemini scoring 25%. According to three otoneurologic experts, ChatGPT’s accuracy was considered comprehensive in 45% of the 20 questions, while 25% were found to be completely incorrect. ChatGPT provided comprehensive responses in 50% of Clinical Knowledge and Basic Clinical Practice questions, but only 25% in Clinical Reasoning. Conclusion Caution is advised when using ChatGPT and Google Gemini due to their limited accuracy in clinical reasoning. While they provide accurate responses concerning Clinical Knowledge, their reliance on web information may lead to inconsistencies. ChatGPT performed better than Gemini. Healthcare professionals should carefully formulate questions and be aware of the potential influence of the online prevalence of information on ChatGPT’s and Google Gemini’s responses. Combining clinical expertise and clinical guidelines with ChatGPT and Google Gemini can maximize benefits while mitigating limitations. Impact Statement This study highlights the potential utility of large language models like ChatGPT in supplementing clinical knowledge for physical therapists, while underscoring the need for caution in domains requiring complex clinical reasoning. The findings emphasize the importance of integrating technological tools carefully with human expertise to enhance patient care and rehabilitation outcomes.",
    "full_text": " \n \n 1 \nUnveiling the Potential: ChatGPT's Impact on Vestibular Rehabilitation Education 1 \n- Trust, Learning, and Value 2 \n 3 \nYael Arbel P.T. 1, Yoav Gimmon P.T., Ph.D 2, 3, Liora Shmueli, Ph.D.1  4 \n1 Department of Management, Bar-Ilan University, Ramat-Gan, 52900, Israel.  5 \n2 Department of Physical Therapy, Faculty of Social Welfare & Health Studies, 6 \nUniversity of Haifa, Haifa, Israel 7 \n3 Department of Otolaryngology-Head and Neck Surgery, Sheba Medical Center, Tel-8 \nHashomer, Israel 9 \n 10 \nCorresponding Author: Liora Shmueli, Ph.D., Department of Management,  11 \nBar-Ilan University, Ramat-Gan, 52900, Israel. Email: liora.shmueli@biu.ac.il 12 \n 13 \nAbstract  14 \n 15 \nObjective:  To evaluate the accuracy, completeness, and explanations provided by 16 \nChatGPT in response to multiple-choice questions related to vestibular rehabilitation. 17 \nStudy Design:  The study was conducted among 30 physical therapists professionals 18 \nexperienced with vestibular rehabilitation and 30 physical therapy students. They were 19 \nasked to complete a Vestibular Knowledge Test consisting of 20 multiple-choice 20 \nquestions categorized into three groups: (1) Clinical Knowledge, (2) Basic Clinical 21 \nPractice, and (3) Clinical Reasoning. Additionally, in May 2023, ChatGPT was tasked 22 \nwith answering the same 20 VKT questions and providing rationales for its answers. 23 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n \n \n 2 \nThree expert board-certified otoneurologists evaluated independently the accuracy of 24 \neach ChatGPT response on a 4-level scale. 25 \nResults: ChatGPT correctly answered 14 of the 20 multiple-choice questions (70%). It 26 \nexcelled in Clinical Knowledge (100%) but struggled in Clinical Reasoning (50%). 27 \nAccording to three otoneurologic experts, ChatGPT's accuracy was \"comprehensive\" for 28 \n9 of the 20 questions (45%), while 5 (25%) were \"completely incorrect\". ChatGPT 29 \nprovided \"comprehensive\" responses in 50% of Clinical Knowledge and Basic Clinical 30 \nPractice questions, but only 25% in Clinical Reasoning. 31 \nConclusion: Caution is advised when using the current version of ChatGPT due to its 32 \nlimited accuracy in clinical reasoning. While it provides accurate responses concerning 33 \nClinical Knowledge, its reliance on web information may lead to inconsistencies. 34 \nHealthcare professionals should carefully formulate questions and be aware of the 35 \npotential influence of the online prevalence of information on ChatGPT's responses. 36 \nCombining clinical expertise and guidelines with ChatGPT can maximize benefits while 37 \nmitigating limitations. 38 \n 39 \nKeywords: ChatGPT, Accuracy, Vestibular Rehabilitation, Education  40 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 3 \nIntroduction 41 \nThe integration of artificial intelligence (AI) technologies in healthcare has 42 \ncreated a new era in patient care, marked by the exploration of the potential integration 43 \nof AI chatbots, including ChatGPT (Generative Pre-trained Transformer), in medicine as 44 \na means to assist healthcare professionals. However, AI’s use in vestibular 45 \nrehabilitation (VR), which requires precise knowledge and expertise, has not yet been 46 \nthoroughly investigated. Understanding the capabilities of ChatGPT and its like and their 47 \npotential applications in VR is vital for advancing evidence-based practices and 48 \ndelivering accurate knowledge to patients. This study, therefore, sought to assess the 49 \naccuracy and grading of the information provided by ChatGPT concerning specialized 50 \nphysical therapists (PTs) in vestibular rehabilitation.  51 \nVR is a specialized form of treatment for patients suffering from dizziness. This 52 \nexercise-based treatment program is designed to promote vestibular adaptation and 53 \nsubstitution. Since its advent in the 1940s1, VR has undergone significant 54 \nadvancements and is now the recommended therapy for dizziness, vestibular 55 \ndysfunction, and benign paroxysmal positional vertigo (BPPV).2 The Barany Society, 56 \nhas amended its definitions for diagnosing vestibular disorders several times since 57 \n2009.3–6 In addition, updated guidelines provide evidence-based recommendations on 58 \nhow to improve diagnostic accuracy and treatment efficiency.2,7 For many clinicians, 59 \nimplementing these guidelines is challenging and requires an active effort of continuous 60 \nongoing learning to stay up-to-date. Research has shown that constant learning is 61 \nessential to ensure consistent, evidence-based treatment and standards in VR.8,9 62 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 4 \n Effective use of AI technologies can assist in making knowledge accessible to 63 \nVR therapists. ChatGPT, an interactive chatbot powered by the GPT3.5 architecture 64 \ndeveloped by OpenAI, is a new advanced Large Language Model (LLM) that has 65 \ndemonstrated tremendous potential in accelerating learning and knowledge acquisition 66 \nin diverse medical fields.10 Trained on a vast dataset from the Internet, ChatGPT excels 67 \nat generating human-like responses in conversations and prompts across multiple 68 \nlanguages and subject domains, making it a valuable tool for various applications.11–13 69 \nRecent studies have evaluated the accuracy level of the answers provided by 70 \nvarious version of ChatGPT to knowledge-based questions in the field of medicine, both 71 \nwith respect to multiple-choice questions and to open-ended questions. When 72 \nanswering multiple-choice questions related to well-defined, established medical 73 \nquestionnaires, ChatGPT's accuracy rate was found to vary across different domains, 74 \nranging from 42% in the field of ophthalmology, as measured by the OKAP 75 \n(Ophthalmic Knowledge Assessment Program) test, to 76% in cardiology, specifically in 76 \nthe American Heart Association (AHA) and Advanced Cardiovascular Life Support 77 \n(ACLS) exams.11,12  78 \nThe studies that assessed the accuracy and reproducibility of ChatGPT 79 \nresponses to open-ended questions involved grading the responses by several clinical 80 \ndomain experts using a scale that included four categories: (1) comprehensive, (2) 81 \ncorrect but inadequate, (3) some correct and some incorrect, and (4) completely 82 \nincorrect. For instance, a study on Bariatric Surgery14 found that the model provided 83 \n\"comprehensive\" responses to 131 out of 151 questions, resulting in an accuracy rate of 84 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 5 \n86.8%. In a similar study regarding cirrhosis and hepatocellular carcinoma, an accuracy 85 \nrate of 76.9% was determined.15  86 \nAn alternative approach implemented by Gilson to determine the degree of 87 \naccuracy on the AMBOSS Student Medical exam entailed evaluating the text output of 88 \neach ChatGPT response across 3 qualitative metrics: logical justification of the answer 89 \nselected, presence of information internal to the question, and presence of information 90 \nexternal to the question.16 91 \nIn this study, we compared the performance of ChatGPT on these multiple-92 \nchoice questions with that of PTs who received training in VR and that of PT students. 93 \nAdditionally, we assessed the accuracy and completeness of ChatGPT’s answers and 94 \nexplanations to a multiple-choice questionnaire we developed about VR.  95 \n 96 \n 97 \n 98 \n 99 \n 100 \n 101 \n 102 \n 103 \n 104 \n 105 \n 106 \n 107 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 6 \nMaterials and Methods 108 \nData source  109 \nVKT multiple choice Questionnaire  110 \nIn May 2023, a Vestibular Knowledge Test (VKT) consisting of 20 multiple-choice 111 \nquestions (one correct answer and three incorrect options (distractors)) was developed 112 \n(See supplemental Appendix 1). It was categorized into three groups: Clinical 113 \nKnowledge, Basic Clinical Practice, and Clinical Reasoning. The test was validated 114 \nthrough an anonymous online survey using Qualtrics.17 The survey link was distributed 115 \nvia WhatsApp, targeting two distinct groups of participants: 1) PTs who had previously 116 \nreceived training in VR, and 2) PT students from all years of the degree. In the survey, 117 \nbefore each question, the prompt \"Please select the correct answer.” appeared. 118 \nOut of the 60 respondents (30 PTs and 30 PT students) who completed the survey, 119 \n53% (n = 32) identified as female. The ages of the participants ranged from 21 to 51 120 \nyears (M = 31.65, SD = 7.92). With respect to the 30 PTs, the median duration of work 121 \nas a PTs was 11.08 years (range <2–23 years), and the median time since their last 122 \nvestibular training was 2.44 years (range <0–12 years). 123 \nTo assess the internal consistency of the questionnaire, reliability statistics were 124 \ncalculated using Cronbach's alpha. For the PT students, the Cronbach's alpha value 125 \nwas α = .37, indicating relatively low internal consistency. In contrast, for the PT group, 126 \nthe Cronbach's alpha value was α = .68, suggesting a higher level of internal 127 \nconsistency. 128 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 7 \nChatGPT Response Generation 129 \nIn order to generate responses, each VKT question was prompted to the 130 \nChatGPT (May 16th version) AI language model based on the GPT-3.5 architecture. 131 \nThe model is trained on data last updated in September 2021. The questions were 132 \nentered separately using the same chat.18  133 \nGrading  134 \nTwo grading methods were implemented: (1) Multiple-choice questionnaire grading: A 135 \nscore of 1 was assigned to each multiple-choice question answered correctly, and a 136 \ngrade of 0 to an incorrect answer, resulting in a maximum total score of 20 points. We 137 \nchose this technique to simulate human test-taking. In order to assess the knowledge 138 \nbase of ChatGPT, we compared its performance on multiple-choice questions with that 139 \nof PTs who had received training in VR and PT students. 140 \n(2) Question-response grading: A score was assigned to each explanation provided by 141 \nChatGPT, based on an independent assessment of its accuracy and compatibility with 142 \nthe answer provided. The review and grading of each response were independently 143 \nperformed by three board-certified otoneurologists who are experts in the vestibular 144 \nfield. The reviews were conducted based on evidence-based knowledge to determine 145 \nthe accuracy of the ChatGPT responses. The accuracy of each ChatGPT response was 146 \nrated using the following scale.14 1. “Comprehensive”: The answer is correct, and the 147 \nexplanation provided is accurate and comprehensive. 2. “Correct but inadequate”: Both 148 \nthe answer and the explanation are correct but not satisfactory in terms of 149 \ncompleteness. 3. “Mixed with correct and incorrect/outdated data”: The answer is 150 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 8 \nincorrect, and the explanation is partially accurate. 4. “Completely incorrect”: Both the 151 \nanswer and the explanation are incorrect. The accuracy of each answer was 152 \ndetermined based on the median of the experts' answers.  153 \nThe accuracy of the ChatGPT responses is displayed in Figure 2 according to three 154 \nVKT knowledge categories.  155 \n 156 \nStatistical analyses 157 \nThe grading was determined by comparing ChatGPT’s answer to the correct 158 \nanswer key. The model's grading was determined from a single run. The means and 159 \nstandard deviation obtained on the 20 multiple-choice single-answer questions by the 160 \nPTs experienced in VR and PT students with no prior knowledge of vestibular 161 \nrehabilitation were compared to the values of the ChatGPT model using One Sample T-162 \nTest (test of means) to determine if there is a significant difference between the 163 \nproportions of response points earned were calculated as a total grade for each domain 164 \nand reported as percentages.  165 \nThe accuracy of the ChatGPT answers was rated by three experts and the score 166 \nwas determined according to the median of the three given scores. A correlation test 167 \nwas performed between each two experts using Spearman analysis.  168 \n 169 \n 170 \n 171 \n 172 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 9 \nResults 173 \n 174 \nOverall Performance  175 \nA VKT consisting of 20 multiple-choice single-answer questions was completed by 30 176 \nPTs experienced in VR, 30 PT students and ChatGPT-3.5 (See supplemental Appendix 177 \n1). ChatGPT answered 70% of the questions correctly (14 out of 20), while the PTs 178 \nachieved a score of 76.3%, and the PT students a score of 40.5% (Figure 1). A 179 \nstatistical analysis using t-tests revealed significant differences between the groups. 180 \nThe PTs’ performance was significantly better than that of ChatGPT (t = 2.46, p < .05), 181 \nwith a large effect size (Cohen's d = 2.89). The PT students performed significantly 182 \nworse than ChatGPT (t = -13.24, p < .001), with a large effect size (Cohen's d = 2.44). 183 \n 184 \nFigure 1: Performance of ChatGPT, the PTs and the PT Students on the VKT, stratified 185 \nby question type and topic. 186 \n 187 \n 188 \n70\n50\n66.66\n100\n76.3 75.83 74.62\n82.5\n40.5\n20\n42.5\n55\n0\n20\n40\n60\n80\n100\n120\nTotal grade Clinical Reasoning Basic clinical practice Clinical knowledge\nResponses generated by ChatGPT Vestibular PT Answers (N30) PT students (N30)\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 10 \n 189 \nPerformance by Question Type  190 \nIn terms of performance, ChatGPT excelled in Clinical Knowledge questions, 191 \nanswering all 4 correctly (100%). However, in the category of Clinical Reasoning, it 192 \nmanaged to answer only 1 out of 4 correctly (25%), and in the Basic Clinical Practice 193 \ncategory, it provided the correct answer to only 8 of the 12 questions (66.66%).  194 \nAccuracy of the ChatGPT-generated explanations  195 \nThe accuracy of ChatGPT's response was assessed and categorized as either 196 \ncomprehensive, correct but inadequate, mixed (correct and incorrect/outdated data), 197 \nand completely wrong Based on the experts’ evaluation of the ChatGPT answers, 9 of 198 \nthe 20 answers were \"comprehensive\" (45%), 5 were “correct but inadequate” (25%), 1 199 \nwas deemed “mixed with correct and incorrect responses” (5%) and the remaining 5 200 \nwere considered \"completely incorrect\" (25%). When analyzed according to knowledge 201 \ncategory, it was found that ChatGPT provides \"comprehensive\" responses to 2 of the 4 202 \n(50%) question related to Clinical Knowledge, to 6 of the 12 (50%) questions concerning 203 \n\"Basic Clinical Practice, and to only 1 of the 4 (25%) questions in the Clinical Reasoning 204 \ncategory (Figure 2). Figure 3 illustrates further examples of ChatGPT prompts by 205 \ncategory. 206 \n  207 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 11 \nFigure 2: Accuracy of the responses generated by ChatGPT-3.5 to questions related to 208 \nVR, categorized by knowledge category 209 \n 210 \n 211 \n 212 \n 213 \nFigure 3:  Examples of ChatGPT prompts for multiple-choice questions, and expert-214 \ngraded ChatGPT responses 215 \n 216 \n 217 \n 218 \nIntercorrelation among experts 219 \nA Spearman correlation test was conducted to assess the intercorrelation among 220 \nthe judges by examining the grading of each pair of experts. The correlation coefficients 221 \nranged from 0.68 to 0.76, indicating a moderate to strong positive association between 222 \nthe judgments of the experts (rs = 0.68 to 0.76, p < 0.05).  223 \n 224 \n225 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 12 \nDiscussion 226 \nIn this study, we present a novel analysis of ChatGPT’s performance in the 227 \ndomain of vestibular rehabilitation. We found that when answering multiple-choice 228 \nquestions, PTs perform significantly better than ChatGPT, whereas PT students fare 229 \nmuch worse than ChatGPT. ChatGPT is accurate in answering Clinical Knowledge 230 \nquestions and in providing accurate explanations for the answers, but their accuracy 231 \nstarkly declines when it comes to Clinical Reasoning. Therefore, the way in which the 232 \nquestions for the chatGPT are formulated is crucial. 233 \nOur finding that ChatGPT achieved an accuracy of 70% on multiple-choice 234 \nquestions specifically focused on VR aligns with previous studies where ChatGPT’s 235 \nperformance was near or passed the threshold of 60% accuracy on the United States 236 \nMedical Licensing Exam (USMLE),13 an accuracy of 76.3% (29/38) on two 38-question 237 \nAHA ACLS exams,12 and successfully cleared the Medical Physiology Examination of 238 \nPhase I MBBS with distinction (>75% of marks)19. Likewise, in another such study 239 \ninvolving quiz-style questions from the platform of the German Society of Oto-Rhino-240 \nLaryngology that subspecialties 2576 questions into 15 distinct otolaryngology, in the 241 \nvestibular system category ChatGPT responded correctly to 63% of the questions (n=95 242 \ncorrect vs. n=57 false).20 243 \nThe frequency of errors in ChatGPT's explanations in our study is particularly 244 \nnotable in the Best Clinical Practice topics. Indeed, errors often occur when addressing 245 \nquestions that involve clinical reasoning and descriptions, which require a deep 246 \nunderstanding of medical contexts rather than solely relying on clinical knowledge. For 247 \nexample, when providing a description of a patient who exhibits a down-beating 248 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 13 \nnystagmus with a rightward rotation during Dix-Hallpike tests on both sides, the 249 \nChatGPT response concluded that the \"most appropriate treatment choice would be: A 250 \npositioning treatment for the right posterior canal by performing a Gufoni maneuver.\" 251 \nWhile ChatGPT did accurately diagnose the right ear, it was wrong when it came to the 252 \nidentification of the canal and the appropriate treatment.  253 \nAn interesting finding was revealed from ChatGPT's responses to post-treatment 254 \nrestrictions regarding prohibitions and restrictions. In this context, ChatGPT's answers 255 \nwere found to be incorrect and based on outdated approaches and previously published 256 \narticles21 available on the Internet. For example, when asked what recommendations 257 \nshould be given after a patient is successfully treated with BPPV concerning what 258 \nshould he or she should avoid doing at home, The Chat incorrectly responded: \"Avoid 259 \nlying on the side that was treated\". ChatGPT's responses were influenced by the 260 \nabundance of information available online rather than by recent and reliable sources.22 261 \nChatGPT, however, did provide accurate and current answers when the question was 262 \nrephrased: In response to, “After you successfully treat a patient diagnosed with BPPV, 263 \nwhat will be your post-treatment recommendations?”, ChatGPT’s response was, 264 \n\"Return to normal activity; you must not refrain from movement or restrict the sleeping 265 \nposition but should avoid climbing ladders or stools.\" Thus, careful consideration should 266 \nbe given to the impact of the wording of the question and presentation scenarios, as it 267 \ncould affect the quality of the ChatGPT response. Additionally, it is crucial to formulate 268 \nquestions carefully and be aware that ChatGPT's responses may be influenced by the 269 \nprevalence of information on the web rather than being based on updated, current and 270 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 14 \nreliable sources. Of note, we ensured that no prompting or training was provided to the 271 \nAI, by entering each question separately using the same chat. 272 \nIt is evident that the utilization of AI for enhancing clinical decision-making will 273 \ncontinue to expand. This growing trend highlights the necessity for effective 274 \ncollaboration between medical professionals and technology developers. With the rapid 275 \ngrowth of medical knowledge, the integration of technologies like AI becomes crucial in 276 \nenabling healthcare professionals to effectively apply this knowledge in their practice. 277 \nHealth care education emerges as a captivating domain to explore, given the vast 278 \namount of information and diverse concepts that healthcare students are expected to 279 \ncomprehend. 23 For example, in a recent editorial by Moons and Van Bulck (2023), they 280 \nhighlight the potential of ChatGPT in cardiovascular nursing practice and research. 281 \nThey further emphasize its ability to summarize large texts, facilitate the work of 282 \nresearchers, and assist in data collection, making ChatGPT a potentially valuable tool in 283 \nhealth care practice. 24 284 \nOur study, together with previous research, leads us to formulate the following 285 \nrecommendations regarding the specific cases where ChatGPT can be utilized to 286 \nexpedite the learning process in clinical knowledge. Instead of investing time on reading 287 \nand memorizing scientific literature updates and guidelines, ChatGPT can provide a 288 \nviable alternative. Indeed, a recent systematic review examined the potential 289 \napplications of Language Model models (LLMs) in healthcare education. The review 290 \nhighlighted several benefits of ChatGPT in healthcare education, such as enhanced 291 \npersonalized learning experiences and an emphasis on critical thinking and problem-292 \nbased learning.25 293 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 15 \nTo determine to what extent ChatGPT could provide accurate responses and 294 \nreliable information, further research is needed to validate the efficacy of ChatGPT in 295 \nproviding accurate answers in the medical domain, specifically in the context of VR. 296 \nFurthermore, studies should be conducted to determine if ChatGPT’s performance can 297 \nbe enhanced through techniques such as question repetition and integration of reliable 298 \nmedical literature to ChatGPT. As emphasized by ChatGPT itself, the evaluation of its 299 \noutput by professionals remains crucial in ensuring the accuracy and completeness of 300 \nthe information provided. In our opinion, clinicians who wish to “consult” with ChatGPT 301 \nshould simplify their questions and avoid clinical reasoning questions. 302 \nLimitations 303 \nWith respect to the study’s limitations, there is no common questionnaire that tests 304 \nknowledge and clinical reasoning in the field of vestibular rehabilitation. We compiled a 305 \nquestionnaire and validated it. In this VKT questionnaire, there are only 20 multiple-306 \nchoice questions, which may have limited participants' ability to express nuanced 307 \nperspectives, potentially impeding the assessment of higher-order thinking skills. 308 \nAdditionally, the survey focuses on three VR categories, which may overlook other 309 \nrelevant knowledge categories. Furthermore, the survey was conducted among Hebrew 310 \nspeakers and we translated into English, but as professional terminology, which is 311 \nprimarily in English, was used in the Hebrew questionnaire, it may be that English 312 \nspeakers might have performed better. Due to this discrepancy in linguistic proficiency, 313 \nChatGPT may be biased and potentially discriminatory against participants. 314 \n 315 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 16 \nConclusions 316 \nThe ChatGPT platform is an effective tool that can be used to obtain information and 317 \nanswer questions related to a variety of fields, including VR. In spite of this, it is 318 \nimportant to recognize that ChatGPT has a number of limitations such as the potential 319 \nprovision of false and inaccurate answers to questions in specific VR categories, 320 \nespecially in clinical reasoning questions. The model is trained on an updated version of 321 \nChatGPT from 2021 and is influenced by the abundance of information available online 322 \nrather than by recent and reliable sources. Medical professionals should therefore use 323 \nthis tool carefully and be aware of its limitations.  324 \n 325 \nAcknowledgments: We would like to express our gratitude to Prof. Avi Shupak, Dr. 326 \nYahav Oron, Dr. Amit Wolfovitz, as well as the physical therapists and physiotherapy 327 \nstudents who participated in the study. 328 \n 329 \nFunding: The authors received no financial support for the research, authorship, and/or 330 \npublication of this article. 331 \nDeclaration of conflicting interests: The Authors declares that there is no conflict of 332 \ninterest. 333 \n 334 \nEthics approval and consent to participate 335 \nThe study was approved by the Ethics Committee for Non-clinical Studies at Bar Ilan-336 \nUniversity in Israel.  The ethics form was signed by the committee head and the date of 337 \napproval was 21 May 2023. 338 \n 339 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 17 \nCRediT authorship contribution statement 340 \nYael Arbel, P.T: conceptualization, methodology, formal analysis resources, and writing 341 \nthe original draft. 342 \nYoav Gimmon, P.T., Ph.D: methodology, formal analysis resources, writing - review & 343 \nediting, supervision. 344 \nLiora Shmueli, Ph.D.: conceptualization, methodology, formal analysis resources, 345 \nwriting the original draft, writing - review & editing, supervision and project 346 \nadministration. 347 \nAll authors approved the final draft.  348 \nAdditional Contributions 349 \nAdditional Information 350 \nThis data was collected using the AI ChatGPT developed by OpenAI. The data was 351 \nanalyzed using SPSS version 28 352 \n 353 \nData Sharing Statement All the data generated or analyzed during this study is 354 \nincluded in this published article [and its supplementary information files]. 355 \n 356 \nConsent for publication 357 \nNot applicable 358 \n 359 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 18 \nAbbreviations 360 \nChatGPT - Generative Pre-trained Transformer 361 \nVKT - Vestibular Knowledge Test  362 \nAI - artificial intelligence  363 \nVR - vestibular rehabilitation  364 \nPTs - Physical therapists 365 \nBPPV - benign paroxysmal positional vertigo  366 \nLLM - Large Language Model  367 \nAHA – merican Heart Association   368 \nACLS – Advanced Cardiovascular Life Support  369 \n 370 \n 371 \n  372 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 19 \nReferences  373 \n1. Cawthorne T. Vestibular Injuries. Proceedings of the Royal Society of Medicine. 374 \n1946;39(5):270-273. doi:10.1177/003591574603900522 375 \n2. Hall CD, Herdman SJ, Whitney SL, et al. Vestibular Rehabilitation for Peripheral 376 \nVestibular Hypofunction: An Updated Clinical Practice Guideline From the Academy 377 \nof Neurologic Physical Therapy of the American Physical Therapy Association. 378 \nJournal of Neurologic Physical Therapy. 2022;46(2):118-177. 379 \ndoi:10.1097/NPT.0000000000000382 380 \n3. Staab JP, Eckhardt-Henn A, Horii A, et al. Diagnostic criteria for persistent postural-381 \nperceptual dizziness (PPPD): Consensus document of the committee for the 382 \nClassification of Vestibular Disorders of the Bárány Society. VES. 2017;27(4):191-383 \n208. doi:10.3233/VES-170622 384 \n4. Strupp M, Lopez-Escamez JA, Kim JS, et al. Vestibular paroxysmia: Diagnostic 385 \ncriteria. VES. 2017;26(5-6):409-415. doi:10.3233/VES-160589 386 \n5. Von Brevern M, Bertholon P, Brandt T, et al. Benign paroxysmal positional vertigo: 387 \nDiagnostic criteria: Consensus document of the Committee for the Classification of 388 \nVestibular Disorders of the Bárány Society. VES. 2015;25(3,4):105-117. 389 \ndoi:10.3233/VES-150553 390 \n6. Bisdorff A, Von Brevern M, Lempert T, Newman-Toker DE. Classification of 391 \nvestibular symptoms: Towards an international classification of vestibular disorders. 392 \nVES. 2009;19(1-2):1-13. doi:10.3233/VES-2009-0343 393 \n7. Bhattacharyya N, Gubbels SP, Schwartz SR, et al. Clinical Practice Guideline: 394 \nBenign Paroxysmal Positional Vertigo (Update). Otolaryngol--head neck surg. 395 \n2017;156(S3). doi:10.1177/0194599816689667 396 \n8. Male AJ, Ramdharry GM, Grant R, Davies RA, Beith ID. A survey of current 397 \nmanagement of Benign Paroxysmal Positional Vertigo (BPPV) by physiotherapists’ 398 \ninterested in vestibular rehabilitation in the UK. Physiotherapy. 2019;105(3):307-399 \n314. doi:10.1016/j.physio.2018.08.007 400 \n9. Meldrum D, Burrows L, Cakrt O, et al. Vestibular rehabilitation in Europe: a survey of 401 \nclinical and research practice. J Neurol. 2020;267(S1):24-35. doi:10.1007/s00415-402 \n020-10228-4 403 \n10. Liao W, Liu Z, Dai H, et al. Differentiate ChatGPT-generated and Human-written 404 \nMedical Texts. Published online April 23, 2023. doi:10.48550/arXiv.2304.11567 405 \n11. Antaki F, Touma S, Milad D, El-Khoury J, Duval R. Evaluating the Performance of 406 \nChatGPT in Ophthalmology: An Analysis of its Successes and Shortcomings. 407 \nOphthalmology Science. Published online May 5, 2023:100324. 408 \ndoi:10.1016/j.xops.2023.100324 409 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 20 \n12. Fijačko N, Gosak L, Štiglic G, Picard CT, Douma MJ. Can ChatGPT pass the life 410 \nsupport exams without entering the American heart association course? 411 \nResuscitation. 2023;185. doi:10.1016/j.resuscitation.2023.109732 412 \n13. Kung TH, Cheatham M, Medenilla A, et al. Performance of ChatGPT on USMLE: 413 \nPotential for AI-assisted medical education using large language models. PLOS 414 \nDigital Health. 2023;2(2):e0000198. doi:10.1371/journal.pdig.0000198 415 \n14. Samaan JS, Yeo YH, Rajeev N, et al. Assessing the Accuracy of Responses by the 416 \nLanguage Model ChatGPT to Questions Regarding Bariatric Surgery. OBES SURG. 417 \nPublished online April 27, 2023. doi:10.1007/s11695-023-06603-5 418 \n15. Yeo YH, Samaan JS, Ng WH, et al. Assessing the performance of ChatGPT in 419 \nanswering questions regarding cirrhosis and hepatocellular carcinoma. Published 420 \nonline February 8, 2023:2023.02.06.23285449. doi:10.1101/2023.02.06.23285449 421 \n16. Gilson A et al. JMIR Medical Education - How Does ChatGPT Perform on the United 422 \nStates Medical Licensing Examination? The Implications of Large Language Models 423 \nfor Medical Education and Knowledge Assessment. Published 2023. Accessed May 424 \n24, 2023. https://mededu.jmir.org/2023/1/e45312/ 425 \n17. Qualtrics Survey Software. Published online 2023. https://www.qualtrics.com 426 \n18. OpenAI. ChatGPT (Version GPT-3.5). Published online 2021. https://openai.com 427 \n19. Subramani M, Jaleel I, Krishna Mohan S. Evaluating the performance of ChatGPT in 428 \nmedical physiology university examination of phase I MBBS. Advances in 429 \nPhysiology Education. 2023;47(2):270-271. doi:10.1152/advan.00036.2023 430 \n20. Hoch CC, Wollenberg B, Lüers JC, et al. ChatGPT’s quiz skills in different 431 \notolaryngology subspecialties: an analysis of 2576 single-choice and multiple-choice 432 \nboard certification preparation questions. Eur Arch Otorhinolaryngol. Published 433 \nonline June 7, 2023. doi:10.1007/s00405-023-08051-4 434 \n21. Herdman SJ. Treatment of Benign Paroxysmal Positional Vertigo. Physical Therapy. 435 \n1990;70(6):381-388. doi:10.1093/ptj/70.6.381 436 \n22. Bhattacharyya N, Gubbels SP, Schwartz SR, et al. Clinical Practice Guideline: 437 \nBenign Paroxysmal Positional Vertigo (Update). Otolaryngol--head neck surg. 438 \n2017;156(S3). doi:10.1177/0194599816689667 439 \n23. Paranjape K, Schinkel M, Panday RN, Car J, Nanayakkara P. Introducing Artificial 440 \nIntelligence Training in Medical Education. JMIR Medical Education. 441 \n2019;5(2):e16048. doi:10.2196/16048 442 \n24. Moons P, Van Bulck L. ChatGPT: can artificial intelligence language models be of 443 \nvalue for cardiovascular nurses and allied health professionals. European Journal of 444 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint \n \n \n 21 \nCardiovascular Nursing. Published online February 8, 2023:zvad022. 445 \ndoi:10.1093/eurjcn/zvad022 446 \n25. Sallam M. ChatGPT Utility in Healthcare Education, Research, and Practice: 447 \nSystematic Review on the Promising Perspectives and Valid Concerns. Healthcare. 448 \n2023;11(6):887. doi:10.3390/healthcare11060887 449 \n 450 \n 451 \n 452 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted January 25, 2024. ; https://doi.org/10.1101/2024.01.24.24301737doi: medRxiv preprint "
}