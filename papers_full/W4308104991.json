{
  "title": "Ensembles of data-efficient vision transformers as a new paradigm for automated classification in ecology",
  "url": "https://openalex.org/W4308104991",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5052827424",
      "name": "Sreenath P. Kyathanahally",
      "affiliations": [
        "Swiss Federal Institute of Aquatic Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5072274414",
      "name": "Thomas Hardeman",
      "affiliations": [
        "Swiss Federal Institute of Aquatic Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5048068115",
      "name": "Marta Reyes",
      "affiliations": [
        "Swiss Federal Institute of Aquatic Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5009566217",
      "name": "Ewa Merz",
      "affiliations": [
        "Swiss Federal Institute of Aquatic Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5112876138",
      "name": "Thea Bulas",
      "affiliations": [
        "Swiss Federal Institute of Aquatic Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5027048566",
      "name": "Philipp Brun",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5012686763",
      "name": "Francesco Pomati",
      "affiliations": [
        "Swiss Federal Institute of Aquatic Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5075251136",
      "name": "Marco Baity‐Jesi",
      "affiliations": [
        "Swiss Federal Institute of Aquatic Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2004475311",
    "https://openalex.org/W2922176752",
    "https://openalex.org/W3093727647",
    "https://openalex.org/W2144812320",
    "https://openalex.org/W2299764710",
    "https://openalex.org/W2737340643",
    "https://openalex.org/W4210883322",
    "https://openalex.org/W2168887716",
    "https://openalex.org/W2886137878",
    "https://openalex.org/W2055032654",
    "https://openalex.org/W2049409009",
    "https://openalex.org/W2563540059",
    "https://openalex.org/W3092144848",
    "https://openalex.org/W3154706941",
    "https://openalex.org/W2883471235",
    "https://openalex.org/W2250600206",
    "https://openalex.org/W2919994468",
    "https://openalex.org/W2769210209",
    "https://openalex.org/W2895082331",
    "https://openalex.org/W2952113774",
    "https://openalex.org/W3129142304",
    "https://openalex.org/W3212185757",
    "https://openalex.org/W2507414358",
    "https://openalex.org/W2604278999",
    "https://openalex.org/W2513863019",
    "https://openalex.org/W2896018215",
    "https://openalex.org/W3010914008",
    "https://openalex.org/W2999689051",
    "https://openalex.org/W3009074285",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W2028138594",
    "https://openalex.org/W3170360335",
    "https://openalex.org/W6795475546",
    "https://openalex.org/W3161120562",
    "https://openalex.org/W2781072509",
    "https://openalex.org/W2984313420",
    "https://openalex.org/W2963400281",
    "https://openalex.org/W6964694417",
    "https://openalex.org/W1530098540",
    "https://openalex.org/W1604680558",
    "https://openalex.org/W2049489886",
    "https://openalex.org/W1954152232",
    "https://openalex.org/W3139434170",
    "https://openalex.org/W3149894872",
    "https://openalex.org/W2887280559",
    "https://openalex.org/W6754002923",
    "https://openalex.org/W2936503027",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2271840356",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W4302596785",
    "https://openalex.org/W2498672755",
    "https://openalex.org/W2029458478",
    "https://openalex.org/W3153564903",
    "https://openalex.org/W3115465645",
    "https://openalex.org/W1979328769"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2022) 12:18590  | https://doi.org/10.1038/s41598-022-21910-0\nwww.nature.com/scientificreports\nEnsembles of data‑efficient vision \ntransformers as a new paradigm \nfor automated classification \nin ecology\nS. P . Kyathanahally 1*, T. Hardeman 1, M. Reyes 1, E. Merz 1, T. Bulas 1, P . Brun 2, F. Pomati 1 & \nM. Baity‑Jesi 1*\nMonitoring biodiversity is paramount to manage and protect natural resources. Collecting images of \norganisms over large temporal or spatial scales is a promising practice to monitor the biodiversity of \nnatural ecosystems, providing large amounts of data with minimal interference with the environment. \nDeep learning models are currently used to automate classification of organisms into taxonomic units. \nHowever, imprecision in these classifiers introduces a measurement noise that is difficult to control \nand can significantly hinder the analysis and interpretation of data. We overcome this limitation \nthrough ensembles of Data‑efficient image Transformers (DeiTs), which we show can reach state‑of‑\nthe‑art (SOTA) performances without hyperparameter tuning, if one follows a simple fixed training \nschedule. We validate our results on ten ecological imaging datasets of diverse origin, ranging from \nplankton to birds. The performances of our EDeiTs are always comparable with the previous SOTA, \neven beating it in four out of ten cases. We argue that these ensemble of DeiTs perform better not \nbecause of superior single‑model performances but rather due to smaller overlaps in the predictions \nby independent models and lower top‑1 probabilities, which increases the benefit of ensembling.\nBiodiversity monitoring is critical because it serves as a foundation for assessing ecosystem integrity, disturbance \nresponses, and the effectiveness of conservation and recovery  efforts1–3. Traditionally, biodiversity monitoring \nrelied on empirical data collected  manually4. This is time-consuming, labor-intensive, and costly. Moreover, \nsuch data can contain sampling biases as a result of difficulties controlling for observer subjectivity and ani-\nmals’ responses to observer  presence5. These constraints severely limit our ability to estimate the abundance of \nnatural populations and community diversity, reducing our ability to interpret their dynamics and interactions. \nCounting wildlife by humans has a tendency to greatly underestimate the number of individuals  present6,7. \nFurthermore, population estimates based on extrapolation from a small number of point counts are subject to \nsubstantial uncertainties and may fail to represent the spatio-temporal variation in ecological interactions (e.g. \npredator-prey), leading to incorrect predictions or  extrapolations7,8. While human-based data collection has a \nlong history in providing the foundation for much of our knowledge of where and why animals dwell and how \nthey interact, present difficulties in wildlife ecology and conservation are revealing the limitations of traditional \nmonitoring  methods7.\nRecent improvements in imaging technology have dramatically increased the data-gathering capacity by low-\nering costs and widening the scope and coverage compared to traditional approaches, opening up new paths for \nlarge-scale ecological  studies7. Many formerly inaccessible places of conservation interest may now be examined \nby using high-resolution remote  sensing9, and digital technologies such as camera  traps10–12 are collecting vast \nvolumes of data non-invasively. Camera traps are low-cost, simple to set up, and provide high-resolution image \nsequences of the species that set them off, allowing researchers to identify the animal species, their behavior, \nand interactions including predator-prey, competition and facilitation. Several cameras have already been used \nto monitor biodiversity around the world, including underwater  systems13,14, making camera traps one of the \nmost widely-used  sensors12. In biodiversity conservation initiatives, camera trap imaging is quickly becoming \nthe gold  standard10,11, as it enables for unparalleled precision monitoring across enormous expanses of land.\nHowever, people find it challenging to analyze the massive amounts of data provided by these devices. The \nenormous volume of image data generated by modern gathering technologies for ecological studies is too large to \nOPEN\n1Eawag, Überlandstrasse 133, 8600 Dübendorf, Switzerland. 2WSL, Zürcherstrasse 111, 8903 Birmensdorf, \nSwitzerland. *email: sreenath.kyathanahally@eawag.ch; marco.baityjesi@eawag.ch\n2\nVol:.(1234567890)Scientific Reports |        (2022) 12:18590  | https://doi.org/10.1038/s41598-022-21910-0\nwww.nature.com/scientificreports/\nbe processed and analyzed at scale to derive compelling ecological  conclusions15. Although online crowd-sourcing \nplatforms could be used to annotate  images16, such systems are unsustainable due to the exponential expansion \nin data acquisition and to the insufficient expert knowledge that is most often required for the annotation. In \nother words, we need tools that can automatically extract relevant information from the data and help to reliably \nunderstand how ecological processes act across space and time.\nMachine learning has proven to be a suitable methodology to unravel the ecological insights from massive \namounts of  data17. Detection and counting pipelines have evolved from imprecise extrapolations from manual \ncounts to machine learning-based systems with high detection  rates18–20. Using deep learning (DL) to detect and \nclassify species for the purpose of population estimation is becoming increasingly  common18–27. DL models, most \noften with convolutional neural network (CNN) like  architectures18–20,22,24,26, have been the standard thus far in \nbiodiversity monitoring. Although these models have an acceptable performance, they often unreliably detect \nminority  classes22, require a very well-tailored model selection and training, large amounts of  data20, and have \na non-negligible error rate that negatively influences the modeling and interpretation of the outcoming data. \nThereupon, it is argued that many DL-based monitoring systems cannot be deployed in a fully autonomous way \nif one wants to ensure a reliable-enough  classification28,29.\nRecently, following their success in natural language processing  applications30, transformer architectures were \nadapted to computer vision applications. The resulting structures, known as vision transformers (ViTs)31, differ \nfrom CNN-based models, that use image pixels as units of information, in using image patches, and employing an \nattention mechanism to weigh the importance of each part of the input data differently. Vision transformers have \ndemonstrated encouraging results in several computer vision tasks, outperforming the state of the art (SOTA) \nin several paradigmatic datasets, and paving the way for new research areas within the branch of deep learning.\nIn this article, we use a specific kind of ViTs, Data efficient image Transformers (DeiTs)32, for the classification \nof biodiversity images such as plankton, coral reefs, insects, birds and large animals (though our approach can \nalso be applied in different domains). We show that while the single-model performance of DeiTs matches that \nof alternative approaches, ensembles of DeiTs (EDeiTs) achieve very good performances without requiring any \nhyperparameter tuning. We see that this mainly happens because of a higher disagreement in the predictions, \nwith respect to other model classes, between independent DeiT models.\nResults\nA new state of the art. We trained EDeiTs on several ecological datasets, spanning from microorganisms \nto large animals, including images in color as well as in black-and-white, with and without background; and \nincluding datasets of diverse sizes and with varying numbers of classes, both balanced and unbalanced. Details \non the datasets are provided in section  \"Data \". As shown in Fig.  1, the error rates of EDeiTs are sometimes \nclose to or even smaller than those of previous SOTA. In the SI, we provide a detailed comparison between our \nmodels’ accuracy and F1-score and that of the previous SOTA. Details on models and training are provided in \nsections \"Models\", \"Implementation\" and \"Ensemble learning\".\nIndividual models comparison. We now show that the better performance of EDeiTs is not a property of \nthe single models, but that it rather stems from the ensembling step. To do this, we focus on the ZooLake dataset \nwhere the previous state of the art is an ensemble of CNN models 22 that consisted of EfficientNet, MobileNet and \nDenseNet architectures. In Table 1, we show the single-model performances of these architectures, and those of \nFigure 1.  Comparing EDeiTs to the previous SOTA. For each dataset, we show the error, which is the fraction \nof misclassified test images ( 1 − accuracy ). The error of the existing SOTA model is shown in orange. For the \nensembles of DeiTs, we show two ways of combining the individual learnings: through arithmetic (blue) and \ngeometric (purple) averaging.\n3\nVol.:(0123456789)Scientific Reports |        (2022) 12:18590  | https://doi.org/10.1038/s41598-022-21910-0\nwww.nature.com/scientificreports/\nthe DeiT-Base model (\"Implementation\"  section), which is the one we used for the results in Fig. 1. The accura-\ncies and (macro-averaged) F1-scores of the two families of models (CNN and DeiT) when compared individu-\nally are in a similar range: the accuracies are between 0.96 and 0.97, and the F1-scores between 0.86 and 0.90.\nEnsemble comparison. We train each of the CNNs in Table 1 four times (as described in Ref.22), with dif-\nferent realisations of the initial conditions, and show their arithmetic average ensemble and geometric average \nensemble (\"Ensemble learning\" section) in the last two columns. We also show the performance of the ensemble \nmodel developed in Ref.22, which ensembles over the six shown CNN architectures. We compare those with the \nensembled DeiT-Base model, obtained through arithmetic average ensemble and geometric average ensemble \nover three different initial conditions of the model weights.\nAs can be expected, upon ensembling the individual model performance improves sensibly. However, the \nimprovement is not the same across all models. The CNN family reaches a maximum F1-score ≤ 0.920 for \nensemble of Efficient-B7 network across initial conditions. When the best CNNs are picked and ensembled the \nensemble performance (Best_6_avg) reaches F1-score ≤ 0.924 . In the case of DeiT models, the ensemble was \ncarried out without picking the best model across different DeiTs but still reaches similar classification accuracy \n(with the F1-score reaching 0.924) with no hyperparameter tuning.\nWhy DeiT models ensemble better. To understand the better performance of DeiTs upon ensembling, \nwe compare CNNs with DeiTs when ensembling over three models. For CNNs, we take the best EfficientNet-B7, \nMobileNet and Dense121 models from Ref. 22 (each had the best validation performance from 4 independent \nruns). For DeiTs, we train a DeiT-Base model three times (with different initial weight configurations) and \nensemble over those three.\nSince the only thing that average ensembling takes into account is the confidence vectors of the models, we \nidentify two possible reasons why EDeiTs perform better, despite the single-model performance being equal to \nCNNs: \n(a) Different CNN models tend to agree on the same wrong answer more often than DeiTs.\n(b) The confidence profile of the DeiT predictions is better suited for average ensembling than the other models.\nWe will see that both (a) and (b) are true, though the dominant contribution comes from (a). In Fig. 2a we show \na histogram of how many models gave a right (R) or wrong (W) prediction (e.g. RRR denotes three correct \npredictions within the individual models, RRW denotes one mistake, and so on).\nOn Fig. 2b and c, we show the same quantity, but restricted to the examples that were correctly classified by \nthe arithmetic and geometric averaged ensemble models. The CNN ensemble has more RRR cases (2523) than the \nEDeiT (2515), but when the three models have some disagreement, the EDeiTs catch up with the CNN ensembles \nIn particular:\nThe correct RWW cases are 2.0x more common in the geometric average and arithmetic average EDeiT \n(Geometric CNN: 8, Geometric EDeiT: 15; Arithmetic CNN: 8, Arithmetic EDeiT: 16). In the SI (See Footnote \n1) we show that the probability that a RWW ensembling results in a correct prediction depends on the ratio \nbetween the second and third component of the ensembled confidence vector, and that the better performance \nof DeiT ensembles in this situation is justified by the shape of the confidence vector.We thus measure the mutual \nagreement between different models. To do so, we take the confidence vectors, ⃗c0 , ⃗c1 and ⃗c2 of the three models, \nand calculate the similarity\n(1)S = 1\n3 (�c0 ·�c1 +�c0 ·�c2 +�c1 ·�c2 ) ,\nTable 1.  Summary of the performance of the individual models on the ZooLake dataset. The ensemble score \non the rightmost column is obtained by averaging across either 3 or 4 different initial conditions. The Best_6_\navg model is an ensemble of DenseNet121, EfficientNet-B2, EfficientNet-B5, EfficientNet-B6, EfficientNet-B7 \nand MobileNet (combining learners through an arithmetic mean)  models22. The numbers in parentheses are \nthe standard errors, referred to the last significant digit.\nModel\nNo. of params for each \nmodel Accuracy mean F1-score mean\nArithmetic ensemble \n(accuracy/F1-score)\nGeometric ensemble \n(accuracy/F1-score)\nDense121 8.1M 0.965 (3) 0.86 (1) 0.976/0.916 0.977/0.917\nEfficient-B2 9.2M 0.9670 (4) 0.894 (2) 0.975/0.915 0.975/0.914\nEfficient-B5 30.6M 0.964 (2) 0.87 (1) 0.971/0.891 0.971/0.898\nEfficient-B6 43.3M 0.965 (1) 0.880 (7) 0.971/0.904 0.972/0.906\nEfficient-B7 66.0M 0.968 (1) 0.893 (4) 0.974/0.913 0.974/0.920\nMobile-V2 3.5M 0.961 (2) 0.881 (5) 0.971/0.907 0.973/0.909\nBest_6_avg – – – 0.978/0.924 0.977/0.923\nDeiT-Base 85.8M 0.962 (3) 0.899 (2) 0.973/0.924 0.972/0.922\n4\nVol:.(1234567890)Scientific Reports |        (2022) 12:18590  | https://doi.org/10.1038/s41598-022-21910-0\nwww.nature.com/scientificreports/\naveraged over the full test set. For DeiTs, we have S = 0.799 ± 0.004 , while for CNNs the similarity is much \nhigher, S = 0.945 ± 0.003 . This is independent of which CNN models we use. If we ensemble Eff2, Eff5 and \nEff6, we obtain S = 0.948 ± 0.003 . Note that the lower correlation between predictions from different DeiT \nlearners is even more striking given that we are comparing the same  DeiT model trained three times, with dif -\nferent CNN architectures. This suggests that the CNN predictions focus on similar sets of characteristics of the \nimage, so when they fail, all models fail similarly. On the contrary, the predictions of separate DeiTs are more \nindependent. Given a fixed budget of single-model correct answers, RWW combinations result more likely in a \ncorrect answer when the two wrong answers are different (see SI (See Footnote 1)). The situation is analogous \nfor geometric averaging (Fig. 2c).\nComparison to vanilla ViTs : For completeness, in the SI (See Footnote 1) we also provide a comparison \nbetween  DeiTs32 and vanilla  ViTs31. Also here, we find analogous results: despite the single-model performance \nbeing similar, DeiTs ensemble better, and this can be again attributed to the lower similarity between predic-\ntions coming from independent models. This suggests that the better performance of DeiT ensembles is not \nrelated to the attention mechanism of ViTs, but rather of the distillation process which is characteristic of DeiTs \n(\"Models\" section).\nDiscussion\nWe presented Ensembles of Data Efficient Image Transformers (EDeiTs) as a standard go-to method for image \nclassification. Though the method we presented is valid for any kind of images, we provided a proof of concept \nof its validity with biodiversity images. Besides being of simple training and deployment (we performed no \nspecific tuning for any of the datasets), EDeiTs achieve results comparable to those of earlier carefully tuned \nstate-of the-art methods, and even outperform them in classifying biodiversity images in four of the ten datasets.\nFocusing on a single dataset, we compared DeiT with CNN models (analogous results stem from a compari-\nson with vanilla ViTs). Despite the similar performance of individual CNN and DeiT models, ensembling benefits \nDeiTs to a larger extent. We attributed this to two mechanisms. To a minor extent, the confidence vectors of DeiTs \nare less peaked on the highest value, which has a slight benefit on ensembling. To a major extent, independently \nof the architecture, the predictions of CNN models are very similar to each other (independently of whether the \nprediction is wrong or right), whereas different DeiTs have a lower degree of mutual agreement, which turns \nout beneficial towards ensembling. This greater independence between DeiT learners also suggests that the loss \nlandscape of DeiTs is qualitatively different from that of CNNs, and that DeiTs might be particularly suitable for \nalgorithms that average the model weights throughout learning, such as stochastic weighted  averaging33, since \ndifferent weight configurations seem to interpret the image in a different way.\nUnlike many kinds of ViTs, the DeiT models we used have a similar number of parameters compared to \nCNNs, and the computational power required to train them is similar. In addition to their deployment requir -\ning similar efforts, with higher performances, DeiTs have the additional advantage of being more straightfor -\nwardly interpretable than CNNs by ecologists, because of the attention map that characterizes transformers. The \nattention mechanism allows to effortlessly identify where in the image the model focused its attention (Fig.  3), \nrendering DeiTs more transparent and controllable by end users.\nAll these observations pose EDeiTs as a solid go-to method for the classification of ecology monitoring \nimages. Though EDeiTs are likely to be an equally solid method also in different domains, we do not expect \nEDeiTs to beat the state of the art in mainstream datasets such as  CIFAR34 or  ImageNet35. In fact, for such data-\nsets, immense efforts were made to achieve the state of the art, the top architectures are heavily tailored to these \n datasets36, and their training required huge numerical efforts. Even reusing those same top architectures, it is \nhard to achieve high single-model performances with simple training protocols and moderate computational \nresources. In addition, while ensembling provides  benefits37, well-tailored architectural choices can provide the \nFigure 2.  Comparison between three-model ensemble models based on CNNs and on DeiTs on the ZooLake \ntest set. The bar heights indicate how often each combination (RRR, RRW , RWW , WWW) appeared. RRR \nindicates that all the models gave the right answer, RRW means that one model gave a wrong answer, and so on. \nThe numbers below each bar indicate explicitly the height of the bar. On panel (a) we consider the whole test set, \non panel (b) we only consider the examples which were correctly classified by the arithmetic ensemble average, \nand on panel (c) those correctly classified through geometric ensemble average.\n5\nVol.:(0123456789)Scientific Reports |        (2022) 12:18590  | https://doi.org/10.1038/s41598-022-21910-0\nwww.nature.com/scientificreports/\nsame  benefits38. Therefore, it is expected that the SOTA models trained on these datasets will benefit less from \nensembling.\nFinally, we note that the nominal test performance of machine learning models is often subject to a decrease \nwhen the models are deployed on real world data. This phenomenon, called data shift, can be essentially attrib-\nuted to the fact that the data sets often do not adequately represent the distribution of images that is sampled at \nthe moment of  deployment39. This can be due to various reasons (sampling method, instrument degradation, \nseasonal effects, an so on) and is hard to harness. However, it was recently shown that Vision Transformer models \n(here, ViT and DeiT) are more robust to data  shift40–42 and to other kinds of perturbations such as  occlusions41, \nwhich is a further reason for the deployment of EDeiTs in ecological monitoring.\nMethods\nData. We tested our models on ten publicly available datasets. In Fig.  4 we show examples of images from \neach of the datasets. When applicable, the training and test splits were kept the same as in the original dataset. \nFor example, the ZooScan, Kaggle, EILAT, and RSMAS datasets lack a specific training and test set; in these \ncases, benchmarks come from k-fold cross-validation43,44, and we followed the exact same procedures in order \nto allow for a fair comparison.\nRSMAS This is a small coral dataset of 766 RGB image patches with a size of 256 × 256 pixels  each45. The \npatches were cropped out of bigger images obtained by the University of Miami’s Rosenstiel School of Marine \nFigure 3.  Examples of DeiTs identifying images from different datasets: (a) Stanford Dogs, (b) SriLankan tiger \nbeetles, (c) Florida wild-trap, and (d) NA-Birds datasets are visualized. The original image is shown on the left in \neach panel, while the right reveals where our model is paying attention while classifying the species in the image.\nFigure 4.  Examples of images from each of the datasets.(a) RSMAS (b) EILAT (c) ZooLake (d) WHOI (e) \nKaggle (f) ZooScan (g) NA-Birds (h) Stanford dogs (i) SriLankan Beetles (j) Florida Wildtrap.\n6\nVol:.(1234567890)Scientific Reports |        (2022) 12:18590  | https://doi.org/10.1038/s41598-022-21910-0\nwww.nature.com/scientificreports/\nand Atmospheric Sciences. These images were captured using various cameras in various locations. The data is \nseparated into 14 unbalanced groups and whose labels correspond to the names of the coral species in Latin. \nThe current SOTA for the classification of this dataset is  by44. They use the ensemble of best performing 11 CNN \nmodels. The best models were chosen based on sequential forward feature selection (SFFS) approach. Since an \nindependent test is not available, they make use of 5-fold cross-validation for benchmarking the performances.\nEILAT This is a coral dataset of 1123 64-pixel RGB image  patches45 that were created from larger images that \nwere taken from coral reefs near Eilat in the Red sea. The image dataset is partitioned into eight classes, with an \nunequal distribution of data. The names of the classes correspond to the shorter version of the scientific names \nof the coral species. The current  SOTA44 for the classification of this dataset uses the ensemble of best perform-\ning 11 CNN models similar to RSMAS dataset and 5-fold cross-validation for benchmarking the performances.\nZooLake This dataset consists of 17943 images of lake plankton from 35 classes, acquired using a Dual-\nmagnification Scripps Plankton Camera (DSPC) in Lake Greifensee (Switzerland) between 2018 and 2020  14,46. \nThe images are colored, with a black background and an uneven class distribution. The current  SOTA22 on this \ndataset is based on a stacking ensemble of 6 CNN models on an independent test set.\nWHOI This dataset 47 contains images of marine plankton acquired by Image  FlowCytobot48, from Woods \nHole Harbor water. The sampling was done between late fall and early spring in 2004 and 2005. It contains \n6600 greyscale images of different sizes, from 22 manually categorized plankton classes with an equal number \nof samples for each class. The majority of the classes belonging to phytoplankton at genus level. This dataset \nwas later extended to include 3.4M images and 103 classes. The WHOI subset that we use was previously used \nfor benchmarking plankton classification  models43,44. The current  SOTA22 on this dataset is based on average \nensemble of 6 CNN models on an independent test set.\nKaggle-plankton The original Kaggle-plankton dataset consists of plankton images that were acquired by \nIn-situ Ichthyoplankton Imaging System (ISIIS) technology from May to June 2014 in the Straits of Florida. The \ndataset was published on Kaggle ( https:// www. kaggle. com/c/ datas cienc ebowl) with images originating from \nthe Hatfield Marine Science Center at Oregon State University. A subset of the original Kaggle-plankton dataset \nwas published  by43 to benchmark the plankton classification tasks. This subset comprises of 14,374 greyscale \nimages from 38 classes, and the distribution among classes is not uniform, but each class has at least 100 samples. \nThe current  SOTA22 uses average ensemble of 6 CNN models and benchmarks the performance using 5-fold \ncross-validation.\nZooScan The ZooScan dataset consists of 3771 greyscale plankton images acquired using the Zooscan technol-\nogy from the Bay of Villefranche-sur-mer49. This dataset was used for benchmarking the classification models in \nprevious plankton recognition  papers43,44. The dataset consists of 20 classes with a variable number of samples for \neach class ranging from 28 to 427. The current  SOTA22 uses average ensemble of 6 CNN models and benchmarks \nthe performance using 2-fold cross-validation.\nNA-Birds NA-Birds50 is a collection of 48,000 captioned pictures of North America’s 400 most often seen bird \nspecies. For each species, there are over 100 images accessible, with distinct annotations for males, females, and \njuveniles, totaling 555 visual categories. The current  SOTA51 called TransFG modifies the pure ViT model by \nadding contrastive feature learning and part selection module that replaces the original input sequence to the \ntransformer layer with tokens corresponding to informative regions such that the distance of representations \nbetween confusing subcategories can be enlarged. They make use of an independent test set for benchmarking \nthe model performances.\nStanford Dogs The Stanford Dogs dataset comprises 20,580 color images of 120 different dog breeds from all \naround the globe, separated into 12,000 training images and 8,580 testing  images52. The current  SOTA51 makes \nuse of modified ViT model called TransFG as explained above in NA-Birds dataset. They make use of an inde-\npendent test set for benchmarking the model performances.\nSri Lankan Beetles The arboreal tiger beetle  data53 consists of 380 images that were taken between August \n2017 and September 2020 from 22 places in Sri Lanka, including all climatic zones and provinces, as well as 14 \ndistricts. Tricondyla (3 species), Derocrania (5 species), and Neocollyris (1 species) were among the nine species \ndiscovered, with six of them being endemic . The current  SOTA53 makes use of CNN-based SqueezeNet archi -\ntecture and was trained using pre-trained weights of ImageNet. The benchmarking of the model performances \nwas done on an independent test set.\nFlorida Wild Traps The wildlife camera  trap54 classification dataset comprises 104,495 images with visually \nsimilar species, varied lighting conditions, skewed class distribution, and samples of endangered species, such as \nFlorida panthers. These were collected from two locations in Southwestern Florida. These images are categorized \nin to 22 classes. The current  SOTA54 makes use of CNN-based ResNet-50 architecture and the performance of \nthe model was benchmarked on an independent test set.\nModels. Vision transformers (ViTs)31 are an adaptation to computer vision of the Transformers, which were \noriginally developed for natural language  processing30. Their distinguishing feature is that, instead of exploiting \ntranslational symmetry, as CNNs do, they have an attention mechanism which identifies the most relevant part \nof an image. ViTs have recently outperformed CNNs in image classification tasks where vast amounts of train-\ning data and processing resources are  available30,55. However, for the vast majority of use cases and consumers, \nwhere data and/or computational resources are limiting, ViTs are essentially untrainable, even when the network \narchitecture is defined and no architectural optimization is required. To settle this issue, Data-efficient Image \nTransformers (DeiTs) were  proposed32. These are transformer models that are designed to be trained with much \nless data and with far less computing  resources32. In DeiTs, the transformer architecture has been modified to \nallow native  distillation56, in which a student neural network learns from the results of a teacher model. Here, a \n7\nVol.:(0123456789)Scientific Reports |        (2022) 12:18590  | https://doi.org/10.1038/s41598-022-21910-0\nwww.nature.com/scientificreports/\nCNN is used as the teacher model, and the pure vision transformer is used as the student network. All the DeiT \nmodels we report on here are DeiT-Base  models32. The ViTs are ViT-B16, ViT-B32, and ViT-L32  models31.\nImplementation. To train our models, we used transfer  learning57: we took a model that was already pre-\ntrained on the  ImageNet35 dataset, changed the last layers depending on the number of classes, and then fine-\ntuned the whole network with a very low learning rate. All the models were trained with two Nvidia GTX 2080Ti \nGPUs.\nDeiTs We used DeiT-Base32 architecture, using the Python package  TIMM58, which includes many of the well-\nknown deep learning architectures, along with their pre-trained weights computed from the ImageNet  dataset35. \nWe resized the input images to 224 x 224 pixels and then, to prevent the model from overfitting at the pixel level \nand help it generalize better, we employed typical image augmentations during training such as horizontal and \nvertical flips, rotations up to 180 degrees, small zoom up’s to 20%, a small Gaussian blur, and shearing up to 10%. \nTo handle class imbalance, we used class reweighting, which reweights errors on each example by how present \nthat class is in the  dataset59. We used sklearn  utilities60 to calculate the class weights which we employed during \nthe training phase.\nThe training phase started with a default  pytorch61 initial conditions (Kaiming uniform initializer), an \nAdamW optimizer with cosine  annealing62, with a base learning rate of 10−4 , and a weight decay value of 0.03, \nbatch size of 32 and was supervised using cross-entropy loss. We trained with early stopping, interrupting training \nif the validation F1-score did not improve for 5 epochs. The learning rate was then dropped by a factor of 10. We \niterated until the learning rate reached its final value of 10−6 . This procedure amounted to around 100 epochs \nin total, independent of the dataset. The training time varied depending on the size of the datasets. It ranged \nbetween 20min (SriLankan Beetles) to 9h (Florida Wildtrap). We used the same procedure for all the datasets: \nno extra time was needed for hyperparameter tuning.\nViTs We implemented the ViT-B16, ViT-B32 and ViT-L32 models using the Python package vit-keras (https:// \ngithub. com/ faust omora les/ vit- keras), which includes pre-trained weights computed from the  ImageNet35 dataset \nand the Tensorflow  library63.\nFirst, we resized input images to 128 × 128 and employed typical image augmentations during training such as \nhorizontal and vertical flips, rotations up to 180 degrees, small zooms up to 20%, small Gaussian blur, and shear-\ning up to 10%. To handle class imbalance, we calculated the class weights and use them during the training phase.\nUsing transfer learning, we imported the pre-trained model and froze all of the layers to train the model. We \nremoved the last layer, and in its place we added a dense layer with nc outputs (being nc the number of classes), \nwas preceded and followed by a dropout layer. We used the Keras-tuner 64 with Bayesian optimization  search65 \nto determine the best set of hyperparameters, which included the dropout rate, learning-rate, and dense layer \nparameters (10 trials and 100 epochs). After that, the model with the best hyperparameters was trained with \na default  tensorflow63 initial condition (Glorot uniform initializer) for 150 epochs using early stopping, which \ninvolved halting the training if the validation loss did not decrease after 50 epochs and retaining the model \nparameters that had the lowest validation loss.\nCNNs CNNs included  DenseNet66,  MobileNet67, EfficientNet-B268, EfficientNet-B568, EfficientNet-B668, and \nEfficientNet-B7 68 architectures. We followed the training procedure described in Ref. 22, and carried out the \ntraining in tensorflow.\nEnsemble learning. We adopted average ensembling, which takes the confidence vectors of different learn-\ners, and produces a prediction based on the average among the confidence vectors. With this procedure, all \nthe individual models contribute equally to the final prediction, irrespective of their validation performance. \nEnsembling usually results in superior overall classification metrics and model  robustness69,70.\nGiven a set of n models, with prediction vectors �ci (i = 1, ... ,n) , these are typically aggregated through an \narithmetic average. The components of the ensembled confidence vector ⃗cAA  , related to each class α are then\nAnother option is to use a geometric average,\nWe can normalize the vector ⃗cg , but this is not relevant, since we are interested in its largest component, \nmaxα (cGA,α ) , and normalization affects all the components in the same way. As a matter of fact, also the nth root \ndoes not change the relative magnitude of the components, so instead of ⃗cGA  we can use a product rule: \nmaxα (cGA,α ) = maxα (cPROD,α ) , with cPROD ,α =\nn∏\ni=1\nci,α.\nWhile these two kinds of averaging are equivalent in the case of two models and two classes, they are gener-\nally different in any other  case71. For example, it can easily be seen that the geometric average penalizes more \nstrongly the classes for which at least one learner has a very low confidence value, a property that was termed \nveto  mechanism72 (note that, while in Ref.72 the term veto is used when the confidence value is exactly zero, here \nwe use this term in a slightly looser way).\n(2)cAA, α = 1\nn\nn∑\ni=1\nci,α .\n(3)cGA, α = n\n√\nn∏\ni=1\nci,α .\n8\nVol:.(1234567890)Scientific Reports |        (2022) 12:18590  | https://doi.org/10.1038/s41598-022-21910-0\nwww.nature.com/scientificreports/\nData availability\nAll the data we used is open access. The datasets analysed during the current study are available in the reposi-\ntories, that we indicate in \"Data\" section.\nCode availability\nThe code for the reproduction of our results is available at https:// github. com/ kspru thvir aj/ Plank iform er.\nReceived: 22 April 2022; Accepted: 5 October 2022\nReferences\n 1. Kremen, C., Merenlender, A. M. & Murphy, D. D. Ecological monitoring: A vital need for integrated conservation and development \nprograms in the tropics. Conserv. Biol. 8, 388–397 (1994).\n 2. Jetz, W . et al. Essential biodiversity variables for mapping and monitoring species populations. Nat. Ecol. Evol. 3, 539–551 (2019).\n 3. Kühl, H. S. et al. Effective biodiversity monitoring needs a culture of integration. One Earth 3, 462–474. https:// doi. org/ 10. 1016/j. \noneear. 2020. 09. 010 (2020).\n 4. Witmer, G. Wildlife population monitoring: Some practical considerations. Wildl. Res.https:// doi. org/ 10. 1071/ WR040 03 (2005).\n 5. McEvoy, J. F ., Hall, G. P . & McDonald, P . G. Evaluation of unmanned aerial vehicle shape, flight path and camera type for waterfowl \nsurveys: Disturbance effects and species recognition. Peer J. 4, e1831–e1831. https:// doi. org/ 10. 7717/ peerj. 1831 (2016).\n 6. Hodgson, J. C. et al. Drones count wildlife more accurately and precisely than humans. Methods Ecol. Evol. 9, 1160–1167. https:// \ndoi. org/ 10. 1111/ 2041- 210X. 12974 (2018).\n 7. Tuia, D. et al. Perspectives in machine learning for wildlife conservation. Nat. Commun. 13, 792. https:// doi. org/ 10. 1038/ s41467- \n022- 27980-y (2022).\n 8. Soranno, P . A. et al. Cross-scale interactions: Quantifying multi-scaled cause-effect relationships in macrosystems. Front. Ecol. \nEnviron. 12, 65–73. https:// doi. org/ 10. 1890/ 120366 (2014).\n 9. Luque, S., Pettorelli, N., Vihervaara, P . & Wegmann, M. Improving biodiversity monitoring using satellite remote sensing to \nprovide solutions towards the 2020 conservation targets. Methods Ecol. Evol. 9, 1784–1786. https:// doi. org/ 10. 1111/ 2041- 210X. \n13057 (2018).\n 10. Burton, A. C. et al. Review: Wildlife camera trapping: A review and recommendations for linking surveys to ecological processes. \nJ. Appl. Ecol. 52, 675–685. https:// doi. org/ 10. 1111/ 1365- 2664. 12432 (2015).\n 11. Rowcliffe, J. M. & Carbone, C. Surveys using camera traps: Are we looking to a brighter future? Anim. Conserv. 11, 185–186. https:// \ndoi. org/ 10. 1111/j. 1469- 1795. 2008. 00180.x (2008).\n 12. Steenweg, R. et al. Scaling-up camera traps: Monitoring the planet’s biodiversity with networks of remote sensors. Front. Ecol. \nEnviron. 15, 26–34. https:// doi. org/ 10. 1002/ fee. 1448 (2017).\n 13. Orenstein, E. C. et al. The scripps plankton camera system: A framework and platform for in situ microscopy. Limnol. Oceanogr. \nMethods 18, 681–695. https:// doi. org/ 10. 1002/ lom3. 10394 (2020).\n 14. Merz, E. et al. Underwater dual-magnification imaging for automated lake plankton monitoring. Water Res. 203, 117524. https:// \ndoi. org/ 10. 1101/ 2021. 04. 14. 439767 (2021).\n 15. Farley, S. S., Dawson, A., Goring, S. J. & Williams, J. W . Situating ecology as a big-data science: Current advances, challenges, and \nsolutions. Bioscience 68, 563–576. https:// doi. org/ 10. 1093/ biosci/ biy068 (2018).\n 16. Jamison, E. & Gurevych, I. Noise or additional information? Leveraging crowdsource annotation item agreement for natural \nlanguage tasks. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing pp 291–297 (2015).\n 17. Kwok, R. Ai empowers conservation biology. Nature 567, 133–134. https:// doi. org/ 10. 1038/ d41586- 019- 00746-1 (2019).\n 18. Norouzzadeh, M. S. et al. Automatically identifying, counting, and describing wild animals in camera-trap images with deep \nlearning. Proc. Natl. Acad. Sci. 115, E5716–E5725. https:// doi. org/ 10. 1073/ pnas. 17193 67115 (2018).\n 19. Willi, M. et al. Identifying animal species in camera trap images using deep learning and citizen science. Methods Ecol. Evol.  10, \n80–91. https:// doi. org/ 10. 1111/ 2041- 210X. 13099 (2019).\n 20. Tabak, M. A. et al. Machine learning to classify animal species in camera trap images: Applications in ecology. Methods Ecol. Evol. \n10, 585–590. https:// doi. org/ 10. 1111/ 2041- 210X. 13120 (2019).\n 21. Henrichs, D. W ., Anglès, S., Gaonkar, C. C. & Campbell, L. Application of a convolutional neural network to improve automated \nearly warning of harmful algal blooms. Environ. Sci. Pollut. Res. pp 1–12 (2021).\n 22. Kyathanahally, S. P . et al. Deep learning classification of lake zooplankton. Front. Microbiol.https:// doi. org/ 10. 3389/ fmicb. 2021. \n746297 (2021).\n 23. Py, O., Hong, H., & Zhongzhi, S. Plankton classification with deep convolutional neural networks. In 2016 IEEE Information \nTechnology, Networking, Electronic and Automation Control Conference pp 132–136. https:// doi. org/ 10. 1109/ ITNEC. 2016. 75603 \n34 (2016).\n 24. Dai, J., Yu, Z., Zheng, H., Zheng, B. & Wang, N. A hybrid convolutional neural network for plankton classification. In Chen, C.-S., \nLu, J. & Ma, K.-K. (eds.) Computer Vision – ACCV 2016 Workshops, 102–114 (Springer International Publishing, Cham, 2017).\n 25. Lee, H., Park, M. & Kim, J. Plankton classification on imbalanced large scale database via convolutional neural networks with \ntransfer learning. In 2016 IEEE International Conference on Image Processing (ICIP), pp 3713–3717. https:// doi. org/ 10. 1109/ ICIP . \n2016. 75330 53 (2016).\n 26. Luo, J. Y . et al. Automated plankton image analysis using convolutional neural networks. Limnol. Oceanogr. Methods 16, 814–827. \nhttps:// doi. org/ 10. 1002/ lom3. 10285 (2018).\n 27. Islam, S. B. & Valles, D. Identification of wild species in texas from camera-trap images using deep neural network for conservation \nmonitoring. In 2020 10th Annual Computing and Communication Workshop and Conference (CCWC) pp 0537–0542, https:// doi. \norg/ 10. 1109/ CCWC4 7524. 2020. 90311 90 (2020).\n 28. Green, S. E., Rees, J. P ., Stephens, P . A., Hill, R. A. & Giordano, A. J. Innovations in camera trapping technology and approaches: \nThe integration of citizen science and artificial intelligence. Animalshttps:// doi. org/ 10. 3390/ ani10 010132 (2020).\n 29. Schneider, S., Greenberg, S., Taylor, G. W . & Kremer, S. C. Three critical factors affecting automated image species recognition \nperformance for camera traps. Ecol. Evol. 10, 3503–3517. https:// doi. org/ 10. 1002/ ece3. 6147 (2020).\n 30. Vaswani, A. et al. Attention is all you need. CoRR arXiv: 1706. 03762 (2017).\n 31. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR arXiv: 2010. 11929 (2020).\n 32. Touvron, H. et al. Training data-efficient image transformers & distillation through attention. CoRR arXiv: 2012. 12877 (2020).\n 33. Izmailov, P ., Podoprikhin, D., Garipov, T., Vetrov, D. & Wilson, A. G. Averaging weights leads to wider optima and better gener-\nalization (2018).\n 34. Krizhevsky, A. Learning multiple layers of features from tiny images. (2009). https:// www. cs. toron  to. edu/ ~kriz/ learn ing- featu \nres- 2009- TR. pdf.\n9\nVol.:(0123456789)Scientific Reports |        (2022) 12:18590  | https://doi.org/10.1038/s41598-022-21910-0\nwww.nature.com/scientificreports/\n 35. Deng, J. et al. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recogni-\ntion pp 248–255 (Ieee, 2009).\n 36. Recht, B., Roelofs, R., Schmidt, L. & Shankar, V . Do imagenet classifiers generalize to imagenet?. In International Conference on \nMachine Learning pp 5389–5400 (PMLR, 2019).\n 37. d’ Ascoli, S., Refinetti, M., Biroli, G. & Krzakala, F . Double trouble in double descent: Bias and variance (s) in the lazy regime. In \nInternational Conference on Machine Learning pp 2280–2290 (PMLR, 2020).\n 38. Nakkiran, P ., Venkat, P ., Kakade, S. & Ma, T. Optimal regularization can mitigate double descent. arXiv preprint arXiv: 2003. 01897 \n(2020).\n 39. Moreno-Torres, J. G., Raeder, T., Alaiz-Rodríguez, R., Chawla, N. V . & Herrera, F . A unifying view on dataset shift in classification. \nPattern Recogn. 45, 521–530. https:// doi. org/ 10. 1016/j. patcog. 2011. 06. 019 (2012).\n 40. Minderer, M. et al. Revisiting the calibration of modern neural networks. Adv. Neural. Inf. Process. Syst. 34, 15682–15694 (2021).\n 41. Naseer, M. M. et al. Intriguing properties of vision transformers. Adv. Neural. Inf. Process. Syst. 34, 23296–23308 (2021).\n 42. Paul, S. & Chen, P .-Y . Vision transformers are robust learners. In Proceedings of the AAAI Conference on Artificial Intelligence 36, \npp 2071–2081 (2022).\n 43. Zheng, H. et al. Automatic plankton image classification combining multiple view features via multiple kernel learning. BMC \nBioinf. 18, 570. https:// doi. org/ 10. 1186/ s12859- 017- 1954-8 (2017).\n 44. Lumini, A., Nanni, L. & Maguolo, G. Deep learning for plankton and coral classification. Appl. Comput. Inform.  https:// doi. org/ \n10. 1016/j. aci. 2019. 11. 004 (2020).\n 45. Gómez-Ríos, A. et al. Towards highly accurate coral texture images classification using deep convolutional neural networks and \ndata augmentation. Expert Syst. Appl. 118, 315–328. https:// doi. org/ 10. 1016/j. eswa. 2018. 10. 010 (2019).\n 46. Kyathanahally, S. et al. Data for: Deep learning classification of lake zooplankton. Front. Microbiol.https:// doi. org/ 10. 25678/ 0004DY \n(2021).\n 47. Sosik, H. & Olson, R. Automated taxonomic classification of phytoplankton sampled with imaging-in-flow cytometry. Limnol. \nOceanogr. Methods 5, 204–216 (2007).\n 48. Olson, R. J. & Sosik, H. M. A submersible imaging-in-flow instrument to analyze nano-and microplankton: Imaging flowcytobot. \nLimnol. Oceanogr. Methods 5, 195–203. https:// doi. org/ 10. 4319/ lom. 2007.5. 195 (2007).\n 49. Gorsky, G. et al. Digital zooplankton image analysis using the ZooScan integrated system. J. Plankton Res. 32, 285–303. https:// \ndoi. org/ 10. 1093/ plankt/ fbp124 (2010).\n 50. Van Horn, G. et al. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained \ndataset collection. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp 595–604. https:// doi. org/ 10. \n1109/ CVPR. 2015. 72986 58 (2015).\n 51. He, J., et al. Transfg: A transformer architecture for fine-grained recognition. CoRR arXiv: 2103. 07976 (2021).\n 52. Khosla, A., Jayadevaprakash, N., Y ao, B. & Fei-Fei, L. Novel dataset for fine-grained image categorization. In First Workshop on \nFine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition (Colorado Springs, CO, 2011).\n 53. Abeywardhana, D., Dangalle, C., Nugaliyadde, A. & Mallawarachchi, Y . Deep learning approach to classify tiger beetles of Sri \nLanka. Eco. Inform. 62, 101286. https:// doi. org/ 10. 1016/j. ecoinf. 2021. 101286 (2021).\n 54. Gagne, C., Kini, J., Smith, D. & Shah, M. Florida wildlife camera trap dataset. CoRR arXiv:  2106. 12628 (2021).\n 55. Xu, Y ., Zhang, Q., Zhang, J. & Tao, D. Vitae: Vision transformer advanced by exploring intrinsic inductive bias. CoRR arXiv: 2106. \n03348 (2021).\n 56. Allen-Zhu, Z. & Li, Y . Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. CoRR arXiv: \n2012. 09816 (2020).\n 57. Tan, C. et al. A survey on deep transfer learning. In International conference on artificial neural networks pp 270–279 (Springer, \n2018).\n 58. Torch image models (2022). Available at https:// fastai. github. io/ timmd ocs/.\n 59. Johnson, J. M. & Khoshgoftaar, T. M. Survey on deep learning with class imbalance. J. Big Data 6, 1–54 (2019).\n 60. Pedregosa, F . et al. Scikit-learn: Machine learning in Python. J. Mach. Learn. Res. 12, 2825–2830 (2011).\n 61. Paszke, A. et al. Pytorch: An imperative style, high-performance deep learning library. Adv. Neural Inf. Process. Syst. 32, 8024–8035 \n(2019).\n 62. Loshchilov, I. & Hutter, F . Fixing weight decay regularization in adam. CoRR abs/1711.05101 (2017).\n 63. Abadi, M. et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. CoRR abs/1603.04467 (2016).\n 64. O’Malley, T. et al. Keras Tuner. https:// github. com/ keras- team/ keras- tuner (2019).\n 65. Mockus, J. Bayesian Approach to Global Optimization: Theory and Applications Vol. 37 (Springer Science & Business Media, 2012).\n 66. Huang, G., Liu, Z., van der Maaten, L. & Weinberger, K. Q. Densely connected convolutional networks (2018).\n 67. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., & Chen, L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks. In 2018 \nIEEE/CVF Conference on Computer Vision and Pattern Recognition. pp 4510–4520. https:// doi. org/ 10. 1109/ CVPR. 2018. 00474 \n(2018).\n 68. Tan, M. & Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine \nLearning pp 6105–6114 (PMLR, 2019).\n 69. Seni, G. & Elder, J. Ensemble Methods in Data Mining: Improving Accuracy Through Combining Predictions Vol. 2 (Morgan & \nClaypool Publishers, 2010).\n 70. Zhang, C. & Ma, Y . Ensemble Machine Learning: Methods and Applications (Springer, 2012).\n 71. Alexandre, L. A., Campilho, A. C. & Kamel, M. On combining classifiers using sum and product rules. Pattern Recogn. Lett.  22, \n1283–1289 (2001).\n 72. Tax, D. M., Duin, R. P . & Breukelen, M. V . Comparison between product and mean classifier combination rules. In In Proc. Work-\nshop on Statistical Pattern Recognition, 165–170 (1997).\nAcknowledgements\nThis project was funded by the Eawag DF project Big-Data Workflow (#5221.00492.999.01), the Swiss Fed -\neral Office for the Environment (contract Nr Q392-1149) and the Swiss National Science Foundation (project \n182124).\nAuthor contributions\nM.B.J. designed the study, S.K. mined the data, S.K. built the models, S.K., T.H., E.M., T.B., M.R., P .B., F .P . and \nM.B.J. were actively involved in the discussion while building and improving the models and data, S.K. and M.B.J. \nwrote the paper. All the authors contributed to the manuscript.\nCompeting interests \nThe authors declare no competing interests.\n10\nVol:.(1234567890)Scientific Reports |        (2022) 12:18590  | https://doi.org/10.1038/s41598-022-21910-0\nwww.nature.com/scientificreports/\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 022- 21910-0.\nCorrespondence and requests for materials should be addressed to S.P .K. or M.B.-J.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2022, corrected publication 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7246952652931213
    },
    {
      "name": "Ranging",
      "score": 0.6122704744338989
    },
    {
      "name": "Biodiversity",
      "score": 0.548276960849762
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5443708896636963
    },
    {
      "name": "Machine learning",
      "score": 0.5258080363273621
    },
    {
      "name": "Data mining",
      "score": 0.4230743646621704
    },
    {
      "name": "Transformer",
      "score": 0.415130615234375
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32287347316741943
    },
    {
      "name": "Ecology",
      "score": 0.24771720170974731
    },
    {
      "name": "Biology",
      "score": 0.09828633069992065
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I63664421",
      "name": "Swiss Federal Institute of Aquatic Science and Technology",
      "country": "CH"
    }
  ]
}