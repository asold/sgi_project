{
  "title": "Retrieval-Augmented Conversational Recommendation with Prompt-based Semi-Structured Natural Language State Tracking",
  "url": "https://openalex.org/W4399353928",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Kemper, Sara",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A4309797206",
      "name": "Cui, Justin",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": null,
      "name": "Dicarlantonio, Kai",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": null,
      "name": "Lin, Kathy",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": null,
      "name": "Tang, Danjie",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": null,
      "name": "Korikov, Anton",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A3160008830",
      "name": "Sanner, Scott",
      "affiliations": [
        "University of Toronto"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3014901735",
    "https://openalex.org/W2746689340",
    "https://openalex.org/W3155116159",
    "https://openalex.org/W2990003990",
    "https://openalex.org/W2468710617",
    "https://openalex.org/W2438667436",
    "https://openalex.org/W4384636969",
    "https://openalex.org/W2604620497",
    "https://openalex.org/W3098520853",
    "https://openalex.org/W2069065514",
    "https://openalex.org/W1582147469"
  ],
  "abstract": "Conversational recommendation (ConvRec) systems must understand rich and diverse natural language (NL) expressions of user preferences and intents, often communicated in an indirect manner (e.g., \"I'm watching my weight\"). Such complex utterances make retrieving relevant items challenging, especially if only using often incomplete or out-of-date metadata. Fortunately, many domains feature rich item reviews that cover standard metadata categories and offer complex opinions that might match a user's interests (e.g., \"classy joint for a date\"). However, only recently have large language models (LLMs) let us unlock the commonsense connections between user preference utterances and complex language in user-generated reviews. Further, LLMs enable novel paradigms for semi-structured dialogue state tracking, complex intent and preference understanding, and generating recommendations, explanations, and question answers. We thus introduce a novel technology RA-Rec, a Retrieval-Augmented, LLM-driven dialogue state tracking system for ConvRec, showcased with a video, open source GitHub repository, and interactive Google Colab notebook.",
  "full_text": "Retrieval-Augmented Conversational Recommendation with\nPrompt-based Semi-Structured Natural Language State Tracking\nSara Kemper‚àó\nUniversity of Waterloo\nWaterloo, Ontario, Canada\nJustin Cui‚àó\nKai Dicarlantonio‚àó\nKathy Lin‚àó\nDanjie Tang‚àó\nUniversity of Toronto\nToronto, Ontario, Canada\nAnton Korikov\nScott Sanner\nanton.korikov@mail.utoronto.ca\nUniversity of Toronto\nToronto, Ontario, Canada\nABSTRACT\nConversational recommendation (ConvRec) systems must under-\nstand rich and diverse natural language (NL) expressions of user\npreferences and intents, often communicated in an indirect manner\n(e.g., ‚ÄúI‚Äôm watching my weight ‚Äù). Such complex utterances make\nretrieving relevant items challenging, especially if only using often\nincomplete or out-of-date metadata. Fortunately, many domains\nfeature rich item reviews that cover standard metadata categories\nand offer complex opinions that might match a user‚Äôs interests\n(e.g., ‚Äúclassy joint for a date ‚Äù). However, only recently have large\nlanguage models (LLMs) let us unlock the commonsense connec-\ntions between user preference utterances and complex language in\nuser-generated reviews. Further, LLMs enable novel paradigms for\nsemi-structured dialogue state tracking, complex intent and pref-\nerence understanding, and generating recommendations, explana-\ntions, and question answers. We thus introduce a novel technology\nRA-Rec, a Retrieval-Augmented, LLM-driven dialogue state track-\ning system for ConvRec, showcased with a video, 1 open source\nGitHub repository,2 and interactive Google Colab notebook.3\nCCS CONCEPTS\n‚Ä¢ Information systems ‚ÜíRecommender systems; Personal-\nization; Language models .\nKEYWORDS\nConversational Recommendation, LLM, Dialogue State Tracking\nACM Reference Format:\nSara Kemper, Justin Cui, Kai Dicarlantonio, Kathy Lin, Danjie Tang, An-\nton Korikov, and Scott Sanner. 2024. Retrieval-Augmented Conversational\nRecommendation with Prompt-based Semi-Structured Natural Language\nState Tracking. In Proceedings of the 47th International ACM SIGIR Con-\nference on Research and Development in Information Retrieval (SIGIR ‚Äô24),\nJuly 14‚Äì18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 5 pages.\nhttps://doi.org/10.1145/3626772.3657670\n‚àóThese authors contributed equally to this research.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR ‚Äô24, July 14‚Äì18, 2024, Washington, DC, USA\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0431-4/24/07.\nhttps://doi.org/10.1145/3626772.3657670\n1 INTRODUCTION\nEffective conversational recommendation (ConvRec) systems need\nto understand rich and diverse natural language (NL) expressions of\nuser preferences and intents, often communicated in an indirect or\nsubtle manner [12, 14, 17, 18]. For instance, a user who asks ‚ÄúDo they\nhave parking? ‚Äù is both inquiring and providing a preference for avail-\nable parking. Similarly, a user looking for a restaurant who states\n‚ÄúI‚Äôm watching my weight ‚Äù is expressing a complex preference that\nrequires commonsense reasoning and may not match any prede-\nfined restaurant metadata fields. Metadata is also often incomplete\nor out-of-date, making it challenging to connect NL requests to\nrelevant item recommendations. This creates major limitations for\ntraditional NL ConvRec systems that rely on mapping user intents\nand preferences to predefined metadata taxonomies [13, 19, 23, 26].\nFortunately, many recommendation domains have an abundance\nof rich NL item reviews that not only refer to standard metadata\ncategories but also offer more complex opinions and narratives that\nmight match a user‚Äôs interests, e.g. ‚ÄúThe menu had lots of low-cal\nveggie options!‚Äù. However, what we have lacked until recently with\nthe advent of large language models (LLMs) [5, 6, 20] is the ability to\nunlock the commonsense reasoning connections between rich user\npreference utterances and expressive language in user-generated\ncontent such as NL reviews. In addition to bridging this language ex-\npression and reasoning gap, LLMs also provide novel opportunities\nto control and facilitate a range of interactions in ConvRec dialogue,\nsuch as understanding user intents and preferences,and generating\nrecommendations, explanations, and answers to questions [8].\nWe thus introduce a novel open source demonstration technol-\nogy RA-Rec, a Retrieval-Augmented, LLM-driven dialogue state\ntracking system for ConvRec, making the following contributions:\n‚Ä¢We introduce prompt-driven ConvRec intent classification\nand state updating that captures nuanced NL expressions\nwhile maintaining domain-specific preference structure via\na semi-structured NL dialogue state (Sec. 3.2).\n‚Ä¢We extend recent work onreviewed-item retrieval [1] to Conv-\nRec dialogue, generating state-based queries, recommenda-\ntions, explanations, and question answers (Figure 2).\n‚Ä¢We demonstrate RA-Rec for restaurant recommendation, in-\ncluding a video,1 a well-documented open source GitHub\nrepository under a permissive MIT License,2 and an interac-\ntive Google Colab notebook that can run the system.3\n1https://www.youtube.com/watch?v=W8Y56UW2LTU\n2https://github.com/D3Mlab/llm-convrec\n3https://apoj.short.gy/d3m-llm-convrec-demo\narXiv:2406.00033v1  [cs.CL]  25 May 2024\nSIGIR ‚Äô24, July 14‚Äì18, 2024, Washington, DC, USA Sara Kemper et al.\nFigure 1: The RA-Rec prompt-driven dialogue state tracking loop. LLM prompting is used for multi-label intent classification\nand for updating a JSON semi-structured NL state which tracks user preferences and other key dialogue elements. The state\nkeys provide an easily configurable structure, while LLM-generated state values can capture nuanced NL expressions.\nTable 1: User intents and system actions in RA-Rec, which are a subset of the recommendation taxonomy of Lyu et al. [18].\nUser Intents\nIntent Description Examples\nProvide Preference Provide or refine preference for their desired item ‚ÄúI want a place with a very good scenic view. ‚Äù\nInquire Ask for more information about the recommended item(s) ‚ÄúWhat kind of menu do they offer?‚Äù, ‚ÄúHow do these options compare for price?‚Äù\nReject Recommendation Reject a recommended item, either explicitly or implicitly ‚ÄúProbably too expensive, what else is there?‚Äù\nAccept Recommendation Accept a recommended item, either explicitly or implicitly ‚ÄúThe first place looks good!‚Äù\nSystem Actions\nAction Description Examples\nRequest Information Request the user‚Äôs preferences towards item aspect(s) ‚ÄúWhere are you located?‚Äù, ‚ÄúWhat kind of cuisine are you looking for?‚Äù\nRecommend and Explain Recommend item(s) and explain how they match user preferences‚ÄúHow about trying Washoku Bistro for a comfortable and laid-back vibe while\nenjoying some delicious Japanese sushi?‚Äù\nAnswer Respond to user inquiry about recommended item(s) ‚ÄúYes, Tokyo Express has a parking lot. ‚Äù\nRespond to Rejection Respond to user‚Äôs rejection of recommended item(s) ‚ÄúI‚Äôm sorry that you did not like the recommendation. Is there anything else I\ncan assist you with?‚Äù\nRespond to Acceptance Respond to user‚Äôs acceptance of recommended item(s) ‚ÄúGreat! If you need any more assistance, feel free to ask. ‚Äù\nGreeting Greet the user. ‚ÄúHello there! I am an Edmonton restaurant recommender. How can I help you?‚Äù\n2 BACKGROUND AND RELATED WORK\n2.1 Dialogue State Tracking\nA standard Dialogue State Tracking (DST) loop [24] has four steps:\n(1) intent understanding, (2) dialogue state updating, (3) action\nselection, and (4) response generation. A traditional state consists\nof keys and values, typically from a predefined set of labels such\nas ‚Äúfood: italian‚Äù, ‚Äúprice: cheap‚Äù, ‚Äúarea: east‚Äù , that represent a most\nlikely estimate of the participants‚Äô shared intentions and beliefs at a\ngiven turn [7, 24]. State tracking techniques generally map features\nextracted from user utterances to state labels, and include hand-\ncrafted rules [ 3, 15], discriminative classifiers [ 4] and Bayesian\nnetworks [21, 25]. While following the DST loop steps for modular\ndialogue control, our RA-Rec system (Sec. 3) extends traditional\nDST methods with LLM-driven state tracking to capture complex,\nNL expressions of preference and to facilitate state-based retrieval-\naugmented recommendation and question answering (QA).\n2.2 Reviewed Item Retrieval\nAiming to unlock the expressive NL data available in reviews, Abdol-\nlah Pour et al. [1] recently extended Neural IR [22] to an approach\nthey call Reviewed Item Retrieval (RIR), where the key challenge lies\nin fusing low-level information from multiple reviews to a higher\nitem level [28]. They demonstrate it is more effective to first score\nindividual reviews against a query and then aggregate these scores\nto an item level ( late fusion ), instead of summarizing reviews at\nan item level before query-scoring (early fusion ), since late fusion\nretains critical nuanced review information lost by early fusion.\nIn late fusion RIR, given a query and a set of reviews, a neural en-\ncoder maps each review and the query to respective embeddings. A\nsimilarity function, such as the dot product, then computes a query-\nreview similarity score. For each item, scores from the reviews with\nthe highest query-review similarities are then averaged (fused) to\ngive a query-item similarity score, and the top-scoring items are\nreturned. As we will discuss in the next section, our RA-Rec sys-\ntem adapts late fusion RIR to ConvRec by generating queries from\na NL dialogue state and using review-based retrieval-augmented\ngeneration for recommendation and QA, as illustrated in Figure 2.\n3 RETRIEVAL AUGMENTED\nCONVERSATIONAL RECOMMENDATION\nTo leverage both the modular structure of a traditional DST loop and\nthe NL reasoning abilities of LLMs, we propose RA-Rec, a modular,\nretrieval-augmented ConvRec system, illustrated in Figure 1. We\nRetrieval-Augmented ConvRec with Semi-Structured NL State Tracking SIGIR ‚Äô24, July 14‚Äì18, 2024, Washington, DC, USA\nFigure 2: Retrieval-augmented recommendation and explanation using late fusion RIR. First, preferences in the dialogue state\nare used to generate a NL query. Then, query and review embeddings are scored using dot product similarity, and the top review\nscores for each item are averaged (fused) into an item score. Finally, the top items‚Äô most relevant reviews and metadata are used\nto generate a recommendation and explanation of how the item satisfies the preferences in the state.\nTable 2: JSON keys in the RA-Rec semi-structured NL dialogue state. Subkeys can be modified to easily facilate new domains.\nBold subkeys indicate mandatory preferences the system will request information about if these preferences are not provided.\nState Key Description Subkeys\nhard_constraints User preferences that must be satisfied location, cuisine_type, dish_type, price_range, atmosphere,\ndietary_restrictions, wait_times, type_of_meal, otherssoft_constraints User preferences that are not required\nrecommended_items Previously recommended items -\nrejected_items Previously rejected items -\naccepted_items Previously accepted items -\nemploy a prompt-driven approach for intent classification and state\nupdating, with the latter relying on a JSON format NL state that can\nbe configured with domain-specific keys while capturing nuance\nthrough LLM-generated NL values. We then use this NL state to\nfacilitate personalized, retrieval-augmented recommendation and\nQA utilizing item reviews and metadata.\n3.1 Prompt-Driven Intent Classification\nAfter the user makes an utterance, LLM-prompting is used to deter-\nmine whether the user expresses any of the four intents in Table 1,\nwhich are a subset of the recommendation dialogue intent taxon-\nomy of Lyu et al. [18]. Table 3 outlines the prompts used in RA-Rec,\nwith full prompt templates available in the GitHub repository (see\nSec. 1). We take a multi-label intent classification approach to cap-\nture multiple intents that might be expressed in a single utterance\n‚Äî for example, the utterance ‚ÄúDoes Washoku Bistro have parking?‚Äù\nshould be classified using both the intents ‚ÄúInquire‚Äù and ‚ÄúProvide\npreference‚Äù because it expresses a preference towards available park-\ning. A larger set of user intents can be facilitated by updating the\nsystem‚Äôs prompts and initial state keys.\n3.2 Semi-Structured NL Dialogue State Tracking\nWe store descriptions of user preferences and other important con-\nversational elements such as rejected recommendations in a JSON\nstate using the keys shown in Table 2 ‚Äî two state examples are in\nFigures 1 and 2. While the keys provide structure, the state values\nare typically LLM-generated from the latest utterances, allowing\nthe state to represent complex NL expressions of preference such\nas ‚ÄúI‚Äôm watching my weight‚Äù at a level of nuance and expressivity\nthat would be impossible with predefined value sets. We thus refer\nto this state representation as a semi-structured NL dialogue state.\n3.2.1 State Elements. Since the goal of RA-Rec is recommendation,\nthe most important components of the state maintain an up-to-date\nbelief about user preferences, represented through hard (required)\nand soft (not required) constraints. In our restaurant recommenda-\ntion demo, these constraints are represented with several domain-\nspecific subkeys listed in Table 2, as well as an ‚Äúothers‚Äù subkey to\ncapture any unspecified preference types. To adaptRA-Rec to a new\ndomain, these restaurant-specific subkeys can be replaced with\ndomain-specific subkeys with little effort from a system designer.\nOther state elements include previously recommended, rejected,\nor accepted restaurants ‚Äì more elements could be easily added\nto handle a wider set of (domain-specific) user intents and sys-\ntem actions. Most state values are LLM-generated (prompts are\nsummarized in Table 3) and used downstream for action selection,\nrecommendation, explanation, and QA, discussed next.\n3.3 Action Selection\nThe main system actions, summarized in Table 1 are Request Infor-\nmation, Recommend and Explain , and Answer. To understand our\nSIGIR ‚Äô24, July 14‚Äì18, 2024, Washington, DC, USA Sara Kemper et al.\nTable 3: The main prompts used in RA-Rec ‚Äì full templates can be found in the repository documentation (see Sec. 1 link).\nComponent Prompt Description\nIntent Classification Classify Intent Given a user utterance and description of an intent (e.g. inquire), identify whether the utterance expresses the intent.\nState Update Update Constraints Given a user utterance and the previous hard and soft constraints, update the constraints.\nUpdate Accepted/Rejected\nItem\nGiven a user utterance with intent \"Accept/Reject Recommendation\", identify which item was accepted/rejected.\nRecommendation\nand Explanation\nGenerate Recommendation\nQuery\nGiven hard/soft constraints in the state, generate a NL query.\nExplain Recommendations Given the top retrieved items, their metadata, and their top reviews, explain how these recommended items match\nthe hard/soft constraints.\nQA\nDetermine QA Knowledge\nSource\nGiven a user inquiry about recommended items and those items‚Äô metadata, identify which fields should be used to\nanswer the inquiry, if any. If none, reviews will be used as the QA knowledge source.\nAnswer Using Metadata Given an inquiry and relevant metadata entries, generate an answer.\nGenerate QA Query Given a user inquiry utterance, generate a NL query.\nAnswer Using Reviews Given an inquiry and retrieved reviews, generate an answer.\nRequest Information implementation, consider a user asking for a\nrestaurant recommendation without giving a location preference\n‚Äî a recommendation may yield a restaurant in the wrong city! To\navoid such premature recommendations with insufficient context,\nwe identify mandatory preferences that the system must ask before\nrecommending if not already provided by the user. In our demo,\nmandatory preferences are location and cuisine_type as shown in\nTable 2, but this selection is easily customized. Once mandatory\npreferences have been provided, the system will Answer if the user\nhas made an inquiry and Recommend and Explain otherwise.\n3.4 Retrieval-Augmented Recommendation and\nExplanation\nTo leverage expressive user review content inRA-Rec, we provide\na novel adaptation of retrieval-augmented generation [16] for late\nfusion recommendation and explanation. To do this, we first gener-\nate a query based on semi-structured preferences in the dialogue\nstate and then retrieve relevant items using RIR (Sec. 2.2) over both\nthe item reviews and known metadata. This process is illustrated\nin Figure 2 with relevant prompts summarized in Table 3.\nSpecifically, after a NL query is generated from the hard and soft\nconstraints in the state, we implement late fusion RIR to retrieve a\nlist of top-ùëò scoring items. Our implementation of late fusion RIR\nuses a TAS-B dense encoder [11] (a variant of BERT [9] fine-tuned\nfor retrieval), dot product similarity, and approximate maximum-\ninner product search (MIPS) via FAISS [10] to enable scalability to\nlarge review corpora. After the top-ùëò items (ùëò = 2 in our demo) are\nretrieved, we use the metadata and top-scoring reviews for each\nitem in a prompt to generate a recommendation and explanation\nof how these items match the dialogue state preferences.\n3.5 Retrieval-Augmented Question Answering\nAs observed by Lyu et al. [18], the later stages of a recommendation\nconversational often involve a number of inquiries about the rec-\nommended item to confirm that it meets the user‚Äôs requirements.\nTo address such QA, RA-Rec retrieves relevant reviews or metadata\nfor each of the items in question and uses this retrieved informa-\ntion to generate an answer ‚Äì with Table 3 outlining the prompts\nused in our QA approach. Our framework is capable of addressing\nboth individual item questions such as ‚ÄúWhat kind of menu do they\noffer?‚Äù as well as comparative questions such as ‚ÄúHow do their prices\ncompare?‚Äù as demonstrated in the video (see Sec. 1).\nIn more detail, the first step of QA uses prompting to determine\nwhether an inquiry can be answered using available metadata,\nwhich is typically the best knowledge source for simple questions\nabout common properties. In our restaurant recommendation demo,\nsuch common metadata fields include price, delivery availability,\nand parking information. If the inquiry cannot be answered with\nmetadata, a NL query is generated from the user utterance and used\nto retrieve several reviews for each item in question. As discussed\nabove, reviews are an expressive knowledge source, especially when\ninquiries and preferences are stated in complex ways. Finally, the\nretrieved reviews and metadata for each item are used to generate\nan answer to the question, which may include item comparisons.\n3.6 RA-Rec System Summary\nIn summary, RA-Rec employs an LLM-driven, modular DST struc-\nture to facilitate a controllable recommendation dialogue that can\nconnect complex NL user preferences to matching items using\ntheir reviews and metadata. Its JSON semi-structured NL state\nfeatures configurable keys for domain-specific control while the\nLLM-updated state values are able to express NL nuance. This state\nsupports novel retrieval-augmented recommendation, explanation,\nand QA, using scalable retrieval methods such as late fusion RIR\nand leveraging item reviews and metadata to generate responses.\n4 DEMONSTRATION DETAILS\nOur system is designed for easy adaptation to various domains,\nand as a demonstration, we present RA-Rec for restaurant recom-\nmendation ‚Äî see Sec. 1 for demo links. Specifically, we use the\nYelp Academic Dataset4 to obtain metadata and over 46K reviews\nfor 1298 restaurants in Edmonton, Alberta.5 GPT-3.5-turbo is the\nLLM used for all prompting steps, but the RA-Rec framework is\nLLM-agnostic and will work with any prompt-based LLM model.\n5 FUTURE WORK\nRA-Rec is a flexible LLM prompt-driven architecture and thus opens\nmany new directions for ConvRec systems to support natural\nuser workflows [13, 18]. Key extensions include support for (1) ac-\ntive preference elicitation to narrow down large item spaces [ 2],\n(2) structured reasoning over multi-aspect NL preferences [27], and\n(3) trade-off negotiation between multiple recommendations.\n4https://www.yelp.com/dataset/download\n5The median number of reviews per restaurant was 21.\nRetrieval-Augmented ConvRec with Semi-Structured NL State Tracking SIGIR ‚Äô24, July 14‚Äì18, 2024, Washington, DC, USA\nREFERENCES\n[1] Mohammad Mahdi Abdollah Pour, Parsa Farinneya, Armin Toroghi, Anton Ko-\nrikov, Ali Pesaranghader, Touqir Sajed, Manasa Bharadwaj, Borislav Mavrin, and\nScott Sanner. 2023. Self-supervised Contrastive BERT Fine-tuning for Fusion-\nBased Reviewed-Item Retrieval. In European Conference on Information Retrieval .\nSpringer, 3‚Äì17.\n[2] David Eric Austin, Anton Korikov, Armin Toroghi, and Scott Sanner. 2024.\nBayesian Optimization with LLM-Based Acquisition Functions for Natural Lan-\nguage Preference Elicitation. arXiv preprint arXiv:xxxx.xxxxx (2024).\n[3] Dan Bohus and Alexander Rudnicky. 2003. Ravenclaw: Dialog management using\nhierarchical task decomposition and an expectation agenda. (2003).\n[4] Dan Bohus and Alexander Rudnicky. 2006. A ‚Äúk hypotheses+ other‚Äù belief\nupdating model. (2006).\n[5] S√©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha\nNori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial\nGeneral Intelligence: Early experiments with GPT-4. arXiv:2303.12712 [cs.CL]\n[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. 2022. PALM: Scaling language modeling with pathways.\n2022. arXiv preprint arXiv:2204.02311 (2022).\n[7] Philip R Cohen, Hector J Levesque, et al. 1987. Rational interaction as the basis\nfor communication . CSLI Stanford.\n[8] Yashar Deldjoo, Zhankui He, Julian McAuley, Anton Korikov, Scott Sanner, Arnau\nRamisa, Ren√© Vidal, Maheswaran Sathiamoorthy, Atoosa Kasirzadeh, and Silvia\nMilano. 2024. A Review of Modern Recommender Systems Using Generative\nModels (Gen-RecSys). arXiv preprint arXiv:2404.00579 (2024).\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of deep bidirectional transformers for language understanding. (Jun\n2019), 4171‚Äì4186. https://doi.org/10.18653/v1/N19-1423\n[10] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy,\nPierre-Emmanuel Mazar√©, Maria Lomeli, Lucas Hosseini, and Herv√© J√©gou. 2024.\nThe FAISS library. (2024). arXiv:2401.08281 [cs.LG]\n[11] Sebastian Hofst√§tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan\nHanbury. 2021. Efficiently teaching an effective dense retriever with balanced\ntopic aware sampling. In Proceedings of the 44th International ACM SIGIR Confer-\nence on Research and Development in Information Retrieval . 113‚Äì122.\n[12] Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis. 2023.\nResolving Indirect Referring Expressions for Entity Selection. In Proceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers) , Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.).\nAssociation for Computational Linguistics, Toronto, Canada, 12313‚Äì12335. https:\n//doi.org/10.18653/v1/2023.acl-long.688\n[13] Dietmar Jannach, Ahtsham Manzoor, Wanling Cai, and Li Chen. 2021. A survey\non conversational recommender systems. ACM Computing Surveys (CSUR) 54, 5\n(2021), 1‚Äì36.\n[14] Jie Kang, Kyle Condiff, Shuo Chang, Joseph A. Konstan, Loren Terveen, and\nF. Maxwell Harper. 2017. Understanding How People Use Natural Language to\nAsk for Recommendations. In Proceedings of the Eleventh ACM Conference on\nRecommender Systems (Como, Italy) (RecSys ‚Äô17) . Association for Computing Ma-\nchinery, New York, NY, USA, 229‚Äì237. https://doi.org/10.1145/3109859.3109873\n[15] Staffan Larsson and David R Traum. 2000. Information state and dialogue manage-\nment in the TRINDI dialogue move engine toolkit. Natural language engineering\n6, 3-4 (2000), 323‚Äì340.\n[16] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel,\nSebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. InProceedings of the 34th International Conference\non Neural Information Processing Systems (Vancouver, BC, Canada) (NIPS‚Äô20).\nCurran Associates Inc., Red Hook, NY, USA, Article 793, 16 pages.\n[17] Annie Louis, Dan Roth, and Filip Radlinski. 2020. ‚ÄúI‚Äôd rather just go to bed‚Äù: Un-\nderstanding Indirect Answers. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) , Bonnie Webber, Trevor Cohn,\nYulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online,\n7411‚Äì7425. https://doi.org/10.18653/v1/2020.emnlp-main.601\n[18] Shengnan Lyu, Arpit Rana, Scott Sanner, and Mohamed Reda Bouadjenek. 2021.\nA workflow analysis of context-driven conversational recommendation. In Pro-\nceedings of the Web Conference 2021 . 866‚Äì877.\n[19] Fedelucio Narducci, Pierpaolo Basile, Marco de Gemmis, Pasquale Lops, and\nGiovanni Semeraro. 2020. An investigation on the user interaction modes of\nconversational recommender systems for the music domain. User Modeling and\nUser-Adapted Interaction 30 (2020), 251‚Äì284.\n[20] TB OpenAI. 2022. ChatGPT: Optimizing language models for dialogue. OpenAI\n(2022).\n[21] Tim Paek and Eric J Horvitz. 2013. Conversation as action under uncertainty.\narXiv preprint arXiv:1301.3883 (2013).\n[22] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings\nusing siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019).\n[23] Yueming Sun and Yi Zhang. 2018. Conversational recommender system. In The\n41st international acm sigir conference on research & development in information\nretrieval. 235‚Äì244.\n[24] Jason D Williams, Antoine Raux, and Matthew Henderson. 2016. The dialog state\ntracking challenge series: A review. Dialogue & Discourse 7, 3 (2016), 4‚Äì33.\n[25] Jason D Williams and Steve Young. 2007. Partially observable Markov decision\nprocesses for spoken dialog systems. Computer Speech & Language 21, 2 (2007),\n393‚Äì422.\n[26] Zhao Yan, Nan Duan, Peng Chen, Ming Zhou, Jianshe Zhou, and Zhoujun Li. 2017.\nBuilding task-oriented dialogue systems for online shopping. InProceedings of the\nThirty-First AAAI Conference on Artificial Intelligence (San Francisco, California,\nUSA) (AAAI‚Äô17). AAAI Press, 4618‚Äì4625.\n[27] Haochen Zhang, Anton Korikov, Parsa Farinneya, Mohammad Mahdi Abdol-\nlah Pour, Manasa Bharadwaj, Ali Pesaranghader, Xi Yu Huang, Yi Xin Lok, Zhaoqi\nWang, Nathan Jones, et al. 2023. Recipe-MPR: A Test Collection for Evaluating\nMulti-aspect Preference-based Natural Language Retrieval. In Proceedings of\nthe 46th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval . 2744‚Äì2753.\n[28] Shuo Zhang and Krisztian Balog. 2017. Design patterns for fusion-based object\nretrieval. In European Conference on Information Retrieval . Springer, 684‚Äì690.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8152322769165039
    },
    {
      "name": "Natural language",
      "score": 0.6358011960983276
    },
    {
      "name": "Tracking (education)",
      "score": 0.5964639186859131
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.5952979326248169
    },
    {
      "name": "Natural language processing",
      "score": 0.5515923500061035
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5232733488082886
    },
    {
      "name": "State (computer science)",
      "score": 0.49303051829338074
    },
    {
      "name": "Information retrieval",
      "score": 0.37459853291511536
    },
    {
      "name": "Programming language",
      "score": 0.13188642263412476
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I185261750",
      "name": "University of Toronto",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I151746483",
      "name": "University of Waterloo",
      "country": "CA"
    }
  ],
  "cited_by": 8
}