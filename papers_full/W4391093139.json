{
  "title": "Towards Automated Regulatory Compliance Verification in Financial Auditing with Large Language Models",
  "url": "https://openalex.org/W4391093139",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2101023729",
      "name": "Armin Berger",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    },
    {
      "id": "https://openalex.org/A2046223250",
      "name": "Lars Hillebrand",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    },
    {
      "id": "https://openalex.org/A5091957484",
      "name": "David Leonhard",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    },
    {
      "id": "https://openalex.org/A4305839394",
      "name": "Tobias Deußer",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    },
    {
      "id": "https://openalex.org/A2742308682",
      "name": "Thiago Bell Felix de Oliveira",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    },
    {
      "id": "https://openalex.org/A4314257588",
      "name": "Tim Dilmaghani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2136404602",
      "name": "Mohamed Khaled",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2066569305",
      "name": "Bernd Kliem",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A610571192",
      "name": "Rüdiger Loitz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A830440161",
      "name": "Christian Bauckhage",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    },
    {
      "id": "https://openalex.org/A2072919878",
      "name": "Rafet Sifa",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4317829538",
    "https://openalex.org/W6767182473",
    "https://openalex.org/W3169554260",
    "https://openalex.org/W4360765003",
    "https://openalex.org/W6857871097",
    "https://openalex.org/W4376503911",
    "https://openalex.org/W2797923083",
    "https://openalex.org/W4360764576",
    "https://openalex.org/W4317905944",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W4385595254",
    "https://openalex.org/W4313007903",
    "https://openalex.org/W4318147798",
    "https://openalex.org/W4298110867",
    "https://openalex.org/W4372219079",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3035101152",
    "https://openalex.org/W6852712537",
    "https://openalex.org/W4376958440",
    "https://openalex.org/W3195248505",
    "https://openalex.org/W2973794125",
    "https://openalex.org/W2986193304",
    "https://openalex.org/W6854866820",
    "https://openalex.org/W6850820320",
    "https://openalex.org/W6780227653",
    "https://openalex.org/W6853465110",
    "https://openalex.org/W6852748123",
    "https://openalex.org/W4214919977",
    "https://openalex.org/W4376653732",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4378711582",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W4300485781",
    "https://openalex.org/W4387686993",
    "https://openalex.org/W3037252472"
  ],
  "abstract": "The auditing of financial documents, historically a labor-intensive process, stands on the precipice of transformation. AI-driven solutions have made inroads into streamlining this process by recommending pertinent text passages from financial reports to align with the legal requirements of accounting standards. However, a glaring limitation remains: these systems commonly fall short in verifying if the recommended excerpts indeed comply with the specific legal mandates. Hence, in this paper, we probe the efficiency of publicly available Large Language Models (LLMs) in the realm of regulatory compliance across different model configurations. We place particular emphasis on comparing cutting-edge open-source LLMs, such as Llama-2, with their proprietary counterparts like OpenAI's GPT models. This comparative analysis leverages two custom datasets provided by our partner PricewaterhouseCoopers (PwC) Germany. We find that the open-source Llama-2 70 billion model demonstrates outstanding performance in detecting non-compliance or true negative occurrences, beating all their proprietary counterparts. Nevertheless, proprietary models such as GPT-4 perform the best in a broad variety of scenarios, particularly in non-English contexts.",
  "full_text": "Towards Automated Regulatory Compliance\nVerification in Financial Auditing with\nLarge Language Models\nArmin Berger∗†, Lars Hillebrand ∗†, David Leonhard †‡, Tobias Deußer†‡, Thiago Bell Felix de Oliveira †‡\nTim Dilmaghani§, Mohamed Khaled §, Bernd Kliem § R¨udiger Loitz§,\nChristian Bauckhage†‡, Rafet Sifa †‡\n†Fraunhofer IAIS, Sankt Augustin, Germany\n‡University of Bonn , Bonn, Germany\n§PricewaterhouseCoopers GmbH, D ¨usseldorf, Germany\nAbstract—The auditing of financial documents, historically a\nlabor-intensive process, stands on the precipice of transformation.\nAI-driven solutions have made inroads into streamlining this\nprocess by recommending pertinent text passages from financial\nreports to align with the legal requirements of accounting\nstandards. However, a glaring limitation remains: these systems\ncommonly fall short in verifying if the recommended excerpts\nindeed comply with the specific legal mandates. Hence, in this pa-\nper, we probe the efficiency of publicly available Large Language\nModels (LLMs) in the realm of regulatory compliance across\ndifferent model configurations. We place particular emphasis\non comparing cutting-edge open-source LLMs, such as Llama-2,\nwith their proprietary counterparts like OpenAI’s GPT models.\nThis comparative analysis leverages two custom datasets provided\nby our partner PricewaterhouseCoopers (PwC) Germany. We\nfind that the open-source Llama-2 70 billion model demonstrates\noutstanding performance in detecting non-compliance or true\nnegative occurrences, beating all their proprietary counterparts.\nNevertheless, proprietary models such as GPT-4 perform the\nbest in a broad variety of scenarios, particularly in non-English\ncontexts.\nIndex Terms—Large Language Models, Text Matching, Finan-\ncial Auditing, Compliance Check\nI. I NTRODUCTION\nCorporate financial disclosures in the form of financial\nstatements provide critical insights into a firm’s economic\nhealth and future trajectory. These documents provide the\npublic with detailed information on the financial stability, pro-\nductivity, and profitability of a company, thus having a major\ninfluence on investment decisions made by external investors.\nFinancial statements are documents that contain financial\ninformation of organizations such as assets, liabilities, and\nrevenues. These documents are examined annually to check\nconformity with the relevant financial reporting framework,\nsuch as the International Financial Reports Standards (IFRS)\nand Germany’s Handelsgesetzbuch (HGB). The examination\nprocess requires a lot of expert knowledge and manual analysis\n* Both authors contributed equally to this research.\nof lengthy financial texts. It includes tasks such as verifying the\ncompleteness, accuracy, valuation, consistency, classification,\nand readability of the reported information. The intricate\nnature of the IFRS and similar accounting standards exacerbate\nthis challenge. Typically structured as an exhaustive list of\nchecklist items, auditors are tasked with the responsibility of\ncomparing relevant text passages from the financial document\nwith each specific regulatory mandate. This necessitates the\ncareful identification and correlation of text segments in the\nfinancial disclosure to the myriad stipulations in the accounting\nstandard. With the advent of advanced Large Language Mod-\nels (LLMs) like GPT-3.5-Turbo and GPT-4 [21] showcasing\nimpressive reasoning and text comprehension skills on various\ndownstream tasks, we seek to explore their role in reshaping\nthe auditing paradigm.\nThis paper builds upon our prior introduction of the Auto-\nmated List Inspection (ALI) [23] and the ZeroShotALI [11]\nsystem to streamline the mapping between legal requirements\nand financial report segments. ZeroShotALI is a novel rec-\nommender system that leverages a state-of-the-art LLM in\nconjunction with a domain-specifically optimized transformer-\nbased text-matching solution.\nIn this paper, we extend the capabilities of our current\nsystems by investigating the potential of “out-of-the-box”\nLanguage Models in evaluating the compliance of a legal\nrequirement with a specified number of pertinent text passages\nextracted from financial documents. Our primary objectives\nencompass two key aspects: firstly, to evaluate the performance\nof open-source models in comparison to prominent proprietary\nmodels like GPT-4; and secondly, to analyze the impact of\nframing the problem through the utilization of prompts.\nOur motivation for exploring open-source models primarily\nstems from considerations related to cost-effectiveness and\ndata privacy, both of which are pivotal concerns in the contem-\nporary landscape of accounting and machine learning research.\nIn the following, we first review related work, before\ndescribing our modeling approach in Section III. In Section IV,\nwe outline our datasets, present our experiments, and discuss\n©2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists,\nor reuse of any copyrighted component of this work in other works.\narXiv:2507.16642v1  [cs.CL]  22 Jul 2025\nthe results. Section V then draws a conclusion and provides\nan outlook into conceivable future work.\nII. R ELATED WORK\nThe use of natural language processing (NLP) in the finan-\ncial domain is an increasingly important field of research. To\noutline the contributions of other research that are related to\nthis work, three areas are selected for distinctive comparison.\nFirst, the general field of financial NLP will be explored\nshortly. Then, research regarding the use of OpenAI’s GPT\nline or the Llama-2 model, [25], for financial tasks will be\ncaptured. Last, general research on completeness or compli-\nance checks, using NLP, will be presented.\nWithin the extensive field of financial NLP, automated\nauditing is a sub-field that has been of interest to us for\nseveral years. The Automated List Inspection (ALI) tool\n[23], a supervised recommender system that ranks textual\ncomponents of financial documents according to the require-\nments of established regulatory frameworks, such as IFRS,\nwas introduced in 2019. Traditional NLP techniques like Tf-\nIdf, latent semantic indexing, neural networks, and logistic\nregression were employed to accomplish the ranking task.\nCombining the first and last methods resulted in the best\nperformance. In [22], we enhanced ALI by utilization of a\npre-trained BERT (developed by [10]) language model to\nencode text segments. A more general framework for this task\nwas introduced by [4]. Further, to automatically verify the\nconsistency of financial disclosures, a more detailed method\nfor information extraction was needed. For this, we introduced\nKPI-Check, [13], also presented by [12], a BERT-based system\nthat utilizes a customized model for named entity and relation\nextraction. This tool automatically identifies and validates\nsemantically equivalent key performance indicators in financial\nreports. The KPI extraction task, while previously focused on\nGerman documents, was also studied on an English dataset in\n[8], which was released together with the results. Combining\ntext and table consistency checks in [1], we investigated\nfinancial reports using several pre-trained tabular models. The\nmost important related work, which can be understood as a\npredecessor, is published in [11]. There, we presented a novel\nrecommender system for financial documents by employing a\ncustom BERT-based model in conjunction with an LLM. In\nthis work, we extend this approach by LLM-based complete-\nness check given the recommended text segments. Works by\nother researchers specializing in financial NLP include [26]\nwho introduced B LOOMBERG GPT, employing Bloomberg’s\nextensive data sources and evaluating the model’s performance\non financial tasks and general LLM benchmarks. FinBERT,\nintroduced by [2] and further employed and researched by\n[27], [18], [3], and [14] is another LLM intended for tasks\nspecializing in financial language.\nThe GPT line of OpenAI is still relatively novel. Thus,\nthere is limited research exploring its capabilities and restraints\nregarding the distinct use case of automated auditing. Addi-\ntionally, as new versions are continuously being published,\nresults obtained at different points in tine are comparable only\nIFRS Legal Checklist\nRequirement j \nFinancial Report\nSegment i \nZeroShotALI\nALI Compliance Check\nRequirement j Top 5 segments\nis complied: yes / no / unclear / not applicable\nFig. 1. Schematic visualization of the complete auditing pipeline combing\nour previous work, ZeroShotALI, an auditing-specific textual recommender\nsystem and the ALI compliance check system introduced in this work. While\nZeroShotALI focuses on retrieving the top 5 relevant text passages per legal\nrequirement, the compliance check system evaluates whether the retrieved\npassages comply with the provided requirement.\nto a limited degree. In the financial domain, there is work\nby [6], who qualitatively demonstrated GPT-4’s effectiveness\nregarding the tasks of sentiment analysis, ESG analysis, cor-\nporate culture analysis, and Federal Reserve opinion analysis.\n[20] conducted a quantitative analysis using ChatGPT to gen-\nerate financial recommendations for the Australian financial\nsector. The analysis revealed that ChatGPT was not effective\nin handling complex financial advice and needed additional\nprofessional guidance. Using GPT-3.5 and GPT-4 for vali-\ndation, [29] proposed the Automated Financial Information\nExtraction framework to enhance the general ability of LLMs\nto extract KPIs from financial reports, finding significant\naverage accuracy increases over a naive method. Last, [5]\ninvestigated the general capabilities of GPT-3.5 and GPT-4 in\nfinancial analysis, considering Zero-Shot, Chain-of-Thought,\nand Few-Shot scenarios, by prompting the models with mock\nexam questions from the Chartered Financial Analyst (CFA)\nprogram.\nWhen it comes to the assurance of compliance and com-\npletion, which extends to the detection of contradictory state-\nments, several works are noteworthy. The task of automated\ncontradiction detection, using a transformer-based model, was\nstudied by us in [9]. Other work includes research on fraudu-\nlent statement detection by [24], who implemented a Deep\nDense Multilayer Perceptron and compared it with other\nmodels such as Decision Trees, k-Nearest Neighbours, Support\nVector Machines, and Logistic Regression. Additionally, [30]\nput forth a novel capsule network to detect fraudulent activities\nin accounting reports. [7] employed a combined entity and\nrelation extraction method to verify formulas in Chinese finan-\ncial documents. Taking a look at the susceptibility of GPT-3 to\nmanipulation of financial texts given the task of financial senti-\nment analysis, [16] demonstrated the vulnerability of keyword-\nbased approaches to adversarial attacks and with it arising the\nimportance of robust models such as context-aware LLMs. Our\nresearch uniquely integrates a proven recommender system\nwith an LLM to comprehensively verify the completeness of\nfinancial reports across varied reporting standards.\nIII. M ETHODOLOGY\nIn this section, we briefly formulate the problem and mo-\ntivate our modeling approach before turning to the in-depth\nanalysis of our proposed architecture, which is visualized in\nFigure 1 and builds upon our prior research, ZeroShotALI.\nIn ZeroShotALI, we primarily focused on matching relevant\ntext passages to legal mandates. However, the auditing work-\nflow extends beyond this preliminary stage. Currently, auditors\nat PwC Germany employ tools that align each legal require-\nment within a financial reporting framework to its correspond-\ning text passages. Subsequently, auditors must ascertain that\nthese suggested passages not only pertain but also conform to\nthe associated legal criterion. To streamline this process and\nenhance the efficacy of the auditing protocol, we propose the\nintegration of LLMs to automatically validate the relevancy\nand compliance of these recommended text sections with their\nrespective legal prerequisites. Further optimizing this process\nand alleviating the auditors’ workload becomes increasingly\ncritical in light of the expanding audit demands, driven by\nemerging regulatory frameworks such as the European Union’s\nCorporate Sustainability Reporting Directive (CSRD).\nIV. E XPERIMENTS\nIn the following sections, we, first, introduce our two\ncustom datasets, which are based on the International Financial\nReports Standards (IFRS) and Germany’s Handelsgesetzbuch\n(HGB). Secondly, we showcase how we evaluate the perfor-\nmance of different configurations. Following that we elaborate\non what model and prompt configurations we deployed for\neach configuration. Finally, we present our results and discuss\nthe implications of our findings on the auditing workflow.\nA. Data\nOur dataset1 is based on 50 IFRS- and 50 HGB-compliant\nfinancial reports. In 2019 we introduced the Automated List\nInspection (ALI) tool [23], a supervised recommender system\nthat ranks text passages of financial documents according to\nthe requirements of an auditing framework, such as IFRS or\nHGB. These machine-processed reports were then assessed by\nPwC auditors, who ensured that the relevant text segments in\neach report were correctly mapped to the corresponding IFRS\nor HGB accounting requirements. The task of annotating these\n1We are currently unable to publish the dataset and the accompanying\nPython code because both are developed and used in the context of an ongoing\nindustrial project.\nTABLE I\nCLASS DISTRIBUTION OF GROUND TRUTH VALUES FOR IFRS AND HGB.\nLabel IFRS HGB\nYes 17 43\nNo 82 28\nUnclear 1 23\nNot Applicable - 26\nTotal 100 120\nNote: HGB Compliance Data was annotated by auditors at PwC Germany\nwith ‘yes’, ‘no’, ‘unclear’, or ‘not applicable’, while IFRS Data was\nannotated with ‘yes’, ‘no’, and ‘unclear’.\nmachine-processed reports was evenly divided among three au-\nditors, who were overseen by a senior auditor. Through several\nrounds of review, the generated labels for requirements were\nverified and fine-tuned. This was achieved by re-examining\nrandomly chosen annotated samples and qualitatively assess-\ning both false positives and negatives generated by the model.\nDue to the size of the original datasets, we randomly\nsampled from both datasets. For the IFRS dataset, we sampled\n10 financial reports, out of which we sub-sampled 100 require-\nments for our evaluation. For the HGB dataset, we randomly\nselected 3 reports, out of which we selected requirements\nfor which at least two annotations existed. Since this paper\nassesses the ability of publicly available LLMs and does not\ntrain any models on domain-specific data, no splitting of the\ndata was required.\nDue to the nature of the IFRS standard, the auditors anno-\ntated whether text passages complied with a requirement as\neither ‘yes’ or ‘no’. For HGB all pairs of requirements and\ntext passages were annotated as either ‘yes’, ‘no’, ‘unclear’\nor ‘not applicable’. Table I shows the distribution of ground\ntruth annotations for both datasets.\nB. Evaluation Metrics\nWe quantitatively evaluate our system’s performance for\neach of the predicted classes by calculating precision, recall,\nand F1 scores. The three metrics are defined as\nPrecision(c) = |TP(c)|\n|TP(c)| + |FP(c)|\nRecall(c) = |TP(c)|\n|TP(c)| + |FN(c)|\nwhere c ∈ Crepresents the class (either “yes”, “no”, “un-\nclear”, or “not applicable”), TP (c) represents the true positive\nobservations, FP(c) represents the false positive observations,\nand FN(c) represents the false negative observations for class\nc.\nF1(c) = 2· Precision(c) · Recall(c)\nPrecision(c) +Recall(c) . (1)\nAll scores are grouped by the prompt used to query a model\nand then averaged across the dataset to determine the overall\nperformance per class. Additionally, we calculate the averages\nacross all classes, ‘yes’, ‘no’, ‘unclear’, or ‘not applicable’,\nusing a macro average, averaging the unweighted mean per\nclass, and a micro average, averaging the support-weighted\nmean per class. Let |C| denote the number of classes present\nper dataset. The two metrics are defined as\nRecallmacro =\nP|C|\ni=1 Recall(ci)\n|C|\nRecallmicro =\nP|C|\ni=1 |TP(ci)|\nP|C|\ni=1(|TP(ci)| + |FN(ci)|)\nC. Baselines\nIn this study, we evaluate six state-of-the-art Large Lan-\nguage Models, comprising both open-source and proprietary\narchitectures. Specifically, we include three variants of the\nopen-source Llama-2 model 2 [25], with sizes denoted as 7b,\n13b, and 70b parameters, and three versions of the closed-\nsource GPT architecture: GPT-3.5-Turbo, GPT-3.5-Turbo-16K,\nand GPT-4. Our rationale for juxtaposing open-source and\nproprietary models is twofold: economic considerations and\ndata privacy issues. Additionally, the open-source nature of\ncertain models offers the potential for fine-tuning, facilitating\nadaptability for specialized applications.\nTo ensure a controlled environment for model inference, we\ndeployed a dedicated server equipped with an NVIDIA A100\n80 GB GPU. We implemented an inference API analogous\nto the OpenAI API to facilitate on-demand access to the\nLlama-2 models. For experimental consistency, all models\nwere subjected to identical prompts and parameters during the\ninference phase.\nD. Prompt Design\nSince all systems involve the querying of an LLM, we\nevaluate the impact of prompt design on model performance.\nThe term prompt design encompasses how a task is presented\nto LLMs. Building on insights by [17] into the impact of\nprompt phrasing on LLM performance, our evaluation centers\non two key factors: (1) task phrasing and (2) the structure of\npermitted model responses.\nIn our evaluation methodology, we have devised a specific\ntask for all tested LLMs, involving the assessment of text\npassages from financial reports against regulatory accounting\nstandards like IFRS or the German HGB. Through a process\nof trial and error and qualitative assessment, we have selected\na total of eight prompts aimed at solving the above-stated\ntask. Prompt performance was then quantified using metrics\nincluding Precision, Recall and F 1-Score per predicted class,\nas well as Micro and Macro F 1-Score averages across all\nclasses (detailed in the Evaluation Metric section). Below we\nhave added one exemplary prompt.\nExemplary prompt:\n2The concrete model IDs from Huggingface are: meta-llama/Llama-2-7b-\nchat-hf, meta-llama/Llama-2-13b-chat-hf, meta-llama/Llama-2-70b-chat-hf.\nSystem: You are an expert auditor with perfect knowl-\nedge of the IFRS accounting standard. You always answer\ntruthfully whether a given regulatory requirement is fully\ncomplied with in the following line ids.\nAnswer with “yes”, if all sub-requirements are fully\ncomplied. Answer with “no”, if at least one of the sub-\nrequirements is not fully complied.\nFormat your output complying to the following json\nschema:\n{{\"answer\": <\"yes\"|\"no\">}}\nrequirement: “requirement”\ndocument: “document”\nThe main two factors we discuss, are (1) task phrasing and\n(2) the structure of permitted model responses.\nIn terms of task phrasing, we used a variety of techniques\nsuch as chain of thought prompting, providing the model\nwith an example prompt answer combination (also referred to\nas a one-shot prompt), asking the model for an explanation\nfor their answer, and Tree-of-Thought prompting. Tree of\nThought prompting as introduced by [28] and [19] (https:\n//www.promptingguide.ai/techniques/tot) refers to the idea of\nenhancing LLM’s ability to solve more elaborate problems\nthrough tree search via a multi-round conversation. Since\nthis technique traditionally requires multiple LLM calls, the\ntechnique is costly and compute-intensive, thus not scaling\nwell for commercial applications like ours. To overcome this\nissue, we have employed the prompting method from [15]\nthat adapts key elements from Tree-of-Thought frameworks\nto create a singular prompt that enables LLMs to assess\nintermediate ideas.\nWhen examining the response structure, a noteworthy ef-\nfect was observed. Outputs were categorized into two main\nformats: “open-ended,” allowing the model to provide explana-\ntions, and “closed,” constraining the model to return only ‘yes’,\n‘no’, ‘unclear’ or ‘not applicable’. Intriguingly, the “closed”\nformat yielded superior performance compared to the “open-\nended” format.\nDue to the lengthy nature of each prompt, we summarize\nthe differences between each evaluated prompt. The complete\nprompt definitions can be found in Appendix VII-B.\nI. In-Out-Sub-Template:\n• Simple Yes/No/Unclear/Not applicable answer.\n• Short, point-by-point answers without explanation.\n• Formatting in JSON schema.\nII. Cot-Sub-Template:\n• Chain of thought response leveraging intermediate explanations\nbefore the final answer.\n• Specification of relevant line numbers from the document.\n• No formatting in the JSON schema.\nIII. In-Out-Template:\n• Simplified version of In-Out-Sub.\n• Direct Yes/No/Unclear/Not applicable response.\n• Formatting in JSON Schema.\nIV . Cot-Template:\n• Step-by-step response and explanation for each sub-request.\n• Detailed explanation for each sub-request.\n• No formatting in JSON schema.\nV . In-Out-Tot-Template:\n• Same as In-Out, with the addition of tree-of-thought Prompting.\n• Three experts give their opinion on the issue and come to a\nconclusion through majority voting.\nVI. In-Out-Tot-One-Shot-Template:\n• Same as In-Out-Tot, with the addition of a one-shot example.\n• In the one-shot example chosen, the text passage complies with the\nrequirement.\nVII. In-Out-One-Shot-Template:\n• Same as In-Out with the addition of a one-shot example.\n• In the one-shot example chosen, the text passage complies with the\nrequirement.\nVIII. In-Out-One-Shot-No-Template:\n• Same as In-Out with the addition of a one-shot example.\n• In the one-shot example chosen, the text passage does not comply\nwith the requirement.\nE. Evaluation and Results\nThe goal of this paper is to assess whether current state-of-\nthe-art Large Language Models can be deployed in an auditing\nsetting to validate the compliance of financial report passages\nwith regulatory standards. To evaluate this, we selected six\nLLMs and deployed each on two custom financial auditing\ndata sets. For each of these configurations, we ran the same\neight selected prompts across all six LLMs, with the only\nexception being an answer adaptation for the open-source\nLlama-2 models. In the case of the LLM answering the task\noutside the scope of the predefined answer choices, the answer\nwas cast as ‘invalid’. The three main questions we sought to\nanswer by deploying a variety of different configurations are:\n1) Performance Across Configurations: Amongst all selected\nsystems, which LLM and prompt configuration performed\nthe best per class and across all classes?\n2) Prompt Consistency Across Models: Is prompt perfor-\nmance consistent across models? If yes, which prompts\nperform the best across all models and datasets?\n3) Deploying LLM for Compliance: How can we deploy an\nLLM-based compliance check system in a manner that\nsaves auditors time while minimizing false negatives?\n1) Performance Across Configurations: Evaluating the\noverall micro F1-Score performance across both datasets, HGB\nand IFRS, the generally best-performing model is GPT-4. As\ncan be seen in Table II, the model achieves a micro F1 score of\n59.31% on HGB and 71.65% on IFRS data averaged across all\nprompts. A similar picture can be seen in the detailed analysis\nof comparing all prompt configurations for all models and\ndatasets in Table V in Appendix VII-A. Overall, we observed\nsignificantly worse performance on HGB data than on IFRS\ndata across all models. This is likely attributed to the fact\nthat most state-of-the-art LLMs are almost exclusively trained\nTABLE II\nMICRO F1-SCORES PER MODEL AND DATASET (HGB AND IFRS) FOR THE\nBEST PERFORMING PROMPT AND THE AVERAGE OVER ALL PROMPTS .\nin % HGB IFRS\nModel Average Best Prompt Average Best Prompt\nGPT-3.5-Turbo 41.83 46.15 66.68 77.56\nGPT-3.5-Turbo-16k 40.48 46.15 49.92 77.58\nGPT-4 59.31 75.60 71.65 77.05\nLlama2-7b 28.49 49.23 47.56 68.02\nLlama2-13b 24.81 47.34 48.01 65.58\nLlama2-70b 18.04 49.23 53.02 70.04\nTABLE III\nBEST-PERFORMING PROMPT PER MODEL AND DATASET BASED ON THE\nMICRO F1-SCORE .\nModel Prompt HGB Prompt IFRS\nGPT-3.5-Turbo I VI\nGPT-3.5-Turbo-16k I I\nGPT-4 VII II\nLlama-2-7b II I\nLlama-2-13b VI III\nLlama-2-70b VI III\non an English text corpus, thus neglecting text understanding\nin other languages. The Llama-2 models for reference were\ntrained on a 98% English text corpus as stated in Meta’s\ntechnical report [25].\nA further notable finding is that the increasing parameter\nsize in the open-source Llama-2 models did not translate into\nsuperior performance across all prompt types. Llama-2-70b\nfor reference performed worse overall, considering micro F 1-\nScore performance across both the HGB and the IFRS dataset,\nthan its significantly smaller counterpart Llama-2-7b.\n2) Prompt Consistency Across Models: Prompt perfor-\nmance does not appear to be consistent across models for our\ntask. Despite the varying performance across models, we have\nnoticed that prompt I. ‘In-Out-Sub-Template’ achieved the best\nscore in 4 out of 12 cases, when considering micro F 1-scores\n(see table III. These results indicate that a combination of\nkeeping the prompt instructions brief and providing the model\nwith examples to follow can improve the response quality.\nIt is interesting to note that the more advanced prompting\ntechniques such as chain of thought (II. and IV .)or trees of\nthought (VI. and VII.) did not induce a significant performance\nincrease.\n3) Deploying LLMs for Compliance: In collaboration with\nour partners at PwC Germany, we have determined that a com-\npliance check system should avoid false positive predictions\nat all costs. Assuming that a text passage complying with a\ngiven auditing context is considered positive, a false negative\nprediction, in this case, would imply a financial report text\npassage falsely being classified as complying with a regulatory\nrequirement. To avoid false positives, we want a model that\nhas very high precision and recall for the class ‘No’. In our\nevaluations, we found that the best model for the ‘No’ class\nin terms of F 1-Score is the open-source Llama-2-70b with a\nFig. 2. Grouped bar plot of F 1-Scores by model and answer choices on IFRS data.\nFig. 3. Grouped bar plot of F 1-Scores by model and answer choices on HGB data.\nNote: Due to poor model performance in the German language, some LLMs were incapable of generating any machine-readable consistent outputs that are\ninterpretable with a heuristic for some prompt formats, leading to some F 1-Scores being 0.\nTABLE IV\nRESULTS FOR LLAMA -2-70 B IN % IFRS D ATA - CLASS ‘NO’.\nNo\nPrecision Recall F 1\nI 75.00 41.25 53.23\nII 80.22 91.25 85.38\nIII 82.22 46.25 59.20\nIV 82.86 72.50 77.33\nV 81.58 77.50 79.49\nVI 80.21 96.25 87.50\nVII 82.35 17.50 28.87\nVIII 85.00 21.25 34.00\nPrecision of 80.21%, Recall of 96.25% and an F 1-Score of\n87.50% on IFRS data (see Table IV). This performance did\nnot translate into a similar performance on HGB data, which\nis likely due to the German language.\nEven though the ‘No’ class is most important in practice, we\nalso report the models’ micro F 1 scores for the other classes\nin Figures 2 for IFRS and 3 for HGB. It can be seen that for\nboth datasets no model picked up on the ‘Unclear‘ class which\nmight be explained by the models being overly confident in\ntheir ‘Yes’ (is complied) and ‘No’ (is not complied) answers.\nAlso, the figures reveal that surprisingly for HGB the models\nperform best on the ‘Yes’ class while performing significantly\nbetter on the ‘No’ class for IFRS.\nV. C ONCLUSION AND FUTURE WORK\nIn our exploration to determine the suitability of publicly\navailable Large Language Models (LLMs) in auditing settings\nfor regulatory compliance verification, we have garnered sev-\neral noteworthy insights.\nFirstly, on the question of performance across configura-\ntions, the GPT-4 model demonstrated the most robust perfor-\nmance with an impressive micro F 1 score across the IFRS\ndataset given the complexity of the task. It is also crucial to\nunderscore the challenge faced by LLMs when confronted\nwith languages other than English. Our findings strongly\nsuggest that the majority of LLMs are likely to exhibit sub-\noptimal performance on non-English datasets, like the German\nHGB, due to their primary training on English text corpora.\nThis phenomenon was especially pronounced in the Llama-2\nmodels.\nSecondly, concerning prompt consistency across models, it\nbecame evident that there is no one-size-fits-all prompt. Differ-\nent LLMs responded optimally to varied prompts, and more\nadvanced prompting techniques did not consistently outper-\nform their simpler counterparts. This underscores the necessity\nto custom-tailor prompts for different models and tasks to\nensure peak performance. The success of brief prompts paired\nwith exemplary one-shot instructions suggests the potential\nbenefits of this approach.\nFinally, in the context of deploying LLMs for compliance\nvalidation in financial reports, precision is of paramount im-\nportance. False positives can have significant repercussions in\nthe auditing domain. Our collaboration with PwC Germany\nelucidated the importance of achieving high precision, espe-\ncially for detecting non-compliance or ‘No’ answers. Inter-\nestingly, the open-source Llama-2-70b model demonstrated\noutstanding performance in this regard on the IFRS dataset. To\nconclude, while LLMs hold immense promise in revolution-\nizing the auditing landscape, it is imperative to approach their\ndeployment judiciously. Selecting the right model, tailoring the\nprompts, and acknowledging their shortcomings is imperative\nto ensuring they can be integrated as reliable components in\nan auditing pipeline. Given the results of our evaluation, we\nare currently not confident in the ability of “out-of-the-box”\nLanguage Models to reliably assess the compliance of legal\nrequirements.\nIn subsequent research, given the commendable perfor-\nmance of the open-source Llama-2-70b model in accurately\npredicting true negatives, it warrants additional investigation\nto explore the potential improvements achievable through\nfurther model training. Specifically, fine-tuning the model\non comprehensive accounting compliance data may enhance\nits effectiveness and enable a reliable assessment of regula-\ntory compliance. Opting for a dedicated open-source model\nalso offers advantages in terms of data privacy, especially\nwhen hosted locally. Moreover, such an approach could be\neconomically advantageous in the long run, considering the\nsubstantial costs associated with commercial LLM APIs, such\nas OpenAI’s GPT-4.\nVI. A CKNOWLEDGMENT\nThis research has been funded by the Federal Ministry of\nEducation and Research of Germany and the state of North-\nRhine Westphalia as part of the Lamarr-Institute for Machine\nLearning and Artificial Intelligence, LAMARR22B.\nREFERENCES\n[1] S. M. Ali, T. Deußer, S. Houben, L. Hillebrand, T. Metzler, and R.\nSifa, “Automatic consistency checking of table and text in financial\ndocuments,” in Proc. NLDL, 2023.\n[2] D. Araci, “Finbert: Financial sentiment analysis with pre-trained\nlanguage models,” arXiv:1908.10063, 2019.\n[3] Y . Arslan, K. Allix, L. Veiber, et al. , “A comparison of pre-trained\nlanguage models for multi-class text classification in the financial\ndomain,” in Proc. WWW, 2021.\n[4] D. Biesner, M. Pielka, R. Ramamurthy, et al., “Zero-shot text match-\ning for automated auditing using sentence transformers,” in Proc.\nICML-A, 2022.\n[5] E. Callanan, A. Mbakwe, A. Papadimitriou, et al., “Can gpt models\nbe financial analysts? an evaluation of chatgpt and gpt-4 on mock cfa\nexams,” arXiv:2310.08678, 2023.\n[6] Y . Cao and J. Zhai, “Bridging the gap–the impact of chatgpt on fi-\nnancial research,” Journal of Chinese Economic and Business Studies,\n2023.\n[7] Y . Cao, H. Li, P. Luo, and J. Yao, “Towards automatic numerical\ncross-checking: Extracting formulas from text,” in Proc. WWW, 2018.\n[8] T. Deußer, S. M. Ali, L. Hillebrand, et al. , “KPI-EDGAR: A novel\ndataset and accompanying metric for relation extraction from financial\ndocuments,” in Proc. ICMLA, 2022.\n[9] T. Deußer, M. Pielka, L. Pucknat, et al., “Contradiction detection in\nfinancial reports,” in Proc. NLDL, 2023.\n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language understand-\ning,” in Proc. NAACL, 2019.\n[11] L. Hillebrand, A. Berger, T. Deußer, et al., “Improving zero-shot text\nmatching for financial auditing with large language models,” in Proc.\nDocEng, 2023, pp. 1–4.\n[12] L. Hillebrand, T. Deußer, T. Dilmaghani, et al. , “Kpi-bert: A joint\nnamed entity recognition and relation extraction model for financial\nreports,” in Proc. ICPR, 2022.\n[13] L. Hillebrand, T. Deußer, T. Dilmaghani, et al., “Towards automating\nnumerical consistency checks in financial reports,” in Proc. BigData,\n2022.\n[14] A. H. Huang, H. Wang, and Y . Yang, “Finbert: A large language\nmodel for extracting information from financial text,” Contemporary\nAccounting Research, 2022.\n[15] D. Hulbert, Tree of knowledge: Tok aka tree of knowledge dataset for\nlarge language models llm , https://github.com/dave1010/tree- of-\nthought-prompting, 2023.\n[16] M. Leippold, “Sentiment spin: Attacking financial sentiment with gpt-\n3,” Finance Research Letters, 2023.\n[17] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-\ntrain, prompt, and predict: A systematic survey of prompting methods\nin natural language processing,” ACM Computing Surveys , pp. 1–35,\n2023.\n[18] Z. Liu, D. Huang, K. Huang, Z. Li, and J. Zhao, “Finbert: A pre-\ntrained financial language representation model for financial text\nmining,” in Proc. IJCAI, 2021.\n[19] J. Long, “Large language model guided tree-of-thought,”\narXiv:2305.08291, 2023.\n[20] B. Neilson, “Artificial intelligence authoring financial recommenda-\ntions: Comparative australian evidence,” Journal of Financial Regu-\nlation, 2023.\n[21] OpenAI, “Gpt-4 technical report,” 2023.\n[22] R. Ramamurthy, M. Pielka, R. Stenzel, et al. , “Alibert: Improved\nautomated list inspection (ali) with bert,” in Proc. DocEng, 2021.\n[23] R. Sifa, A. Ladi, M. Pielka, et al., “Towards automated auditing with\nmachine learning,” in Proc. DocEng, 2019.\n[24] G. S. Temponeras, S.-A. N. Alexandropoulos, S. B. Kotsiantis, and\nM. N. Vrahatis, “Financial fraudulent statements detection through a\ndeep dense artificial neural network,” in Proc. IISA, 2019.\n[25] H. Touvron, L. Martin, K. Stone, et al., “Llama 2: Open foundation\nand fine-tuned chat models,” arXiv:2307.09288, 2023.\n[26] S. Wu, O. Irsoy, S. Lu, et al., “Bloomberggpt: A large language model\nfor finance,” arXiv:2303.17564, 2023.\n[27] Y . Yang, M. C. S. Uy, and A. Huang, “Finbert: A pretrained language\nmodel for financial communications,” arXiv:2006.08097, 2020.\n[28] S. Yao, D. Yu, J. Zhao, et al., “Tree of thoughts: Deliberate problem\nsolving with large language models,” arXiv:2305.10601, 2023.\n[29] C. Yue, X. Xu, X. Ma, et al., “Leveraging llms for kpis retrieval from\nhybrid long-document: A comprehensive framework and dataset,”\narXiv:2305.16344, 2023.\n[30] F. Zhu, D. Ning, Y . Wang, and S. Liu, “A novel cost-sensitive capsule\nnetwork for audit fraud detection,” in Proc. IUCC, 2021.\nVII. A PPENDIX\nA. Prompt evaluation\nTable V shows the detailed evaluation of all prompt config-\nurations per dataset and model based on the micro F 1 score.\nB. Prompt configurations\nThe following features the list of evaluated prompts as\noutlined in Section IV-D. First, the prompts used for reports\nwritten under the IFRS are shown. Second, prompts used for\nGerman reports written for the HGB are listed.\nI In-Out-Sub-Template\nTABLE V\nMICRO F1-SCORES FOR ALL MODELS PER DATASET AND PROMPT CONFIGURATION .\nPrompt Dataset GPT-3.5 GPT-3.5-16K GPT-4 Llama-2-7b Llama-2-13b Llama-2-70b\nHGB 45.88 45.88 66.67 49.23 0.00 0.00I IFRS 76.42 77.21 73.60 58.58 24.57 42.58\nHGB 35.49 35.10 40.07 46.89 36.20 13.68II IFRS 69.69 10.32 77.05 33.13 52.83 70.04\nHGB 43.44 43.44 66.67 36.20 0.00 24.62III IFRS 58.22 57.22 71.73 29.74 22.59 50.69\nHGB 37.87 37.87 41.03 43.44 43.44 49.23IV IFRS 74.27 11.03 75.23 35.83 56.49 66.57\nHGB 43.96 33.57 53.85 38.46 0.00 13.68V IFRS 72.00 60.77 71.37 35.68 65.41 69.65\nHGB 37.87 37.87 75.60 0.00 46.89 43.08VI IFRS 77.56 77.58 66.38 68.02 65.58 70.00\nHGB 43.96 43.96 63.95 13.68 24.62 0.00VII IFRS 70.04 70.04 70.13 65.82 61.62 24.76\nHGB 46.15 46.15 66.67 0.00 47.34 0.00VIII IFRS 35.21 35.21 67.74 53.69 35.02 29.87\nNote: Due to the verbose nature of Llama-2 Models and their poor performance in the German language, Llama-2 was incapable of generating any\nmachine-readable consistent outputs that are interpretable with a heuristic for some prompt formats. This leads to some Micro F 1-Scores being 0.\n“System: You are an expert auditor with perfect knowledge of the IFRS\naccounting standard. You always answer truthfully whether a given regulatory\nrequirement is fully complied in the following line ids.\nIs the following IFRS sub-requirement fully complied in the following input\ndocument? Answer with ‘yes’, if the sub-requirement is fully complied.\nAnswer with ‘no’, if it is not fully complied.\nFormat your output complying to the following json schema: {{‘answer’:\n<‘yes’|‘no’>}}\nrequirement: ‘{requirement}’ document: ‘ {document}’\n{{‘answer’:”\nII Cot-Sub-Template\n“System: You are an expert auditor with perfect knowledge of the IFRS\naccounting standard. You always answer truthfully whether a given regulatory\nrequirement is fully complied in the following line ids.\nIs the following IFRS sub-requirement fully complied in the following input\ndocument? Think step by step: Explain whether it is fully complied and ref-\nerence relevant line ids from the input document. Based on your explanation,\ndetermine whether the requirement is fully complied by answering with ‘yes’\nor ‘no’.\nrequirement: ‘{requirement}’ document: ‘ {document}’\n{{‘answer’:”\nIII In-Out-Template\n“System: You are an expert auditor with perfect knowledge of the IFRS\naccounting standard. You always answer truthfully whether a given regulatory\nrequirement is fully complied in the following line ids.\nAnswer with ‘yes’, if all sub-requirements are fully complied. Answer with\n‘no’, if at least one of the sub-requirements is not fully complied.\nFormat your output complying to the following json schema: {{‘answer’:\n<‘yes’|‘no’>}}\nrequirement: ‘{requirement}’ document: ‘ {document}’\n{{‘answer’:”\nIV Cot-Template\n“System: You are an expert auditor with perfect knowledge of the IFRS\naccounting standard. You always answer truthfully whether a given regulatory\nrequirement is fully complied in the following line ids.\nAnswer with ‘yes’, if all sub-requirements are fully complied. Answer with\n‘no’, if at least one of the sub-requirements is not fully complied.\nFormat your output complying to the following json schema: {{‘answer’:\n<‘yes’|‘no’>}}\nrequirement: ‘{requirement}’ document: ‘ {document}’\n{{‘answer’:”\nV In-Out-Tot-Template\n“System: Imagine three different experts in the field of auditing answering\nthis question. Each expert has perfect knowledge of the IFRS accounting\nstandard. All experts always answer truthfully whether a particular regulatory\nrequirement in the following line numbers is is completely fulfilled.\nEach expert writes down 1 step of their thought process and shares it with the\ngroup. Then all experts move to the next step, and so on. Show each step and\neach expert’s thinking process. If at any time an expert realizes he is wrong,\nhe is eliminated.\nAnswer with ‘yes’, if all sub-requirements are fully complied. Answer with\n‘no’, if at least one of the sub-requirements is not fully complied.\nFormat your output complying to the following json schema: {{‘answer’:\n<‘yes’|‘no’>}}\nrequirement: ‘{requirement}’ document: ‘ {document}’\n{{‘answer’:”\nVI In-Out-Tot-One-Shot-Template\n“System: Imagine three different experts in the field of auditing answering\nthis question. Each expert has perfect knowledge of the IFRS accounting\nstandard. All experts always answer truthfully whether a particular regulatory\nrequirement in the following line numbers is is completely fulfilled.\nEach expert writes down 1 step of their thought process and shares it with the\ngroup. Then all experts move to the next step, and so on. Show each step and\neach expert’s thinking process. If at any time an expert realizes he is wrong,\nhe is eliminated.\nAnswer with ‘yes’, if all sub-requirements are fully complied. Answer with\n‘no’, if at least one of the sub-requirements is not fully complied.\nFormat your output complying to the following json schema: {{‘answer’:\n<‘yes’|‘no’>}}\nRequirement:‘{requirement}’ Document:‘{document}‘\nExample:\nRequirement: ‘Disclose the amount of receivables with a remaining maturity\nof more than one year; separately for each item reported.’ Document: ‘544:\nAll receivables have a remaining maturity of less than one year.\nExpert 1: ‘The requirement asks for the amount of receivables with a\nremaining maturity of more than one year. However, line 544 states that all\nreceivables have a remaining maturity of less than one year. This seems to\nimply that there are no receivables with a remaining maturity of more than\none year and the requirement has been met.’\nExpert 2: ‘I see Expert 1’s point. Since the document indicates that all\nreceivables have a remaining term of less than one year, we can assume\nthat the requirement has been met.’\nExpert 3: ‘I agree with my colleagues. Although the document does not\nprovide any other specific information, we can conclude from the context\nthat the requirement has been met.’\n{{‘answer’: ‘yes’}}\n{{‘answer’:”\nVII In-Out-One-Shot-Template\n“System: You are an expert auditor with perfect knowledge of the IFRS\naccounting standard. You always answer truthfully whether a given regulatory\nrequirement is fully complied in the following line ids.\nAnswer with ‘yes’, if all sub-requirements are fully complied. Answer with\n‘no’, if at least one of the sub-requirements is not fully complied.\nFormat your output complying to the following json schema: {{‘answer’:\n<‘yes’|‘no’>}}\nRequirement: ‘{requirement}’ Document: ‘ {document}’\nExample:\nRequirement: ‘Disclose the amount of receivables due in more than one year;\nseparately for each item shown.’ Document: ‘544: All receivables have a\nremaining maturity of less than one year.\n{{‘answer’: ‘yes’}}\n{{‘answer’:”\nVIII In-Out-Tot-One-Shot-Template\n“System: You are an expert auditor with perfect knowledge of the IFRS\naccounting standard. You always answer truthfully whether a given regulatory\nrequirement is fully complied in the following line ids.\nAnswer with ‘yes’, if all sub-requirements are fully complied. Answer with\n‘no’, if at least one of the sub-requirements is not fully complied.\nFormat your output complying to the following json schema: {{‘answer’:\n<‘yes’|‘no’>}}\nRequirement: ‘{requirement}’ Document: ‘ {document}’\nExample:\nRequirement: ‘Disclose the amount of receivables due in more than one year;\nseparately for each item shown.’ Document: ‘544: All receivables have a\nremaining maturity of less than one year.\n{{‘answer’: ‘no’}}\n{{‘answer’:”\nI In-Out-Sub-Template (German)\n“System: Sie sind ein Experte im Bereich Wirtschaftspr ¨ufung und haben\nperfekte Kenntnisse des HGB-Bilanzierungstandards. Sie antworten immer\nwahrheitsgem¨aß, ob eine bestimmte beh ¨ordliche Anforderung in den folgen-\nden Zeilennummern vollst ¨andig erf ¨ullt ist.\nIst die folgende HGB-Anforderung im unten genannten Dokument vollst¨andig\nerf¨ullt? Antworten Sie mit: - ‘yes’, wenn die Anforderung erf ¨ullt ist, - ‘no’,\nwenn die Anforderung nicht erf ¨ullt ist, - ‘unclear’, wenn die Erf ¨ullung der\nAnforderung nicht beantwortet werden kann, da Kontextinformationen fehlen\nund - ‘not applicable’, wenn die Anforderung f ¨ur das Dokument nicht relevant\nist.\nFormatieren Sie Ihre Ausgabe gem ¨aß dem folgenden JSON-Schema:\n{{‘answer’: <‘yes’|‘no’|‘unclear’|‘not applicable’>}}\nAnforderung: ‘{requirement}’ Dokument: ‘ {document}’\n{{‘answer’:”\nII Cot-Sub-Template (German)\n“System: Sie sind ein Experte im Bereich Wirtschaftspr ¨ufung und haben\nperfekte Kenntnisse des HGB-Bilanzierungstandards. Sie antworten immer\nwahrheitsgem¨aß, ob eine bestimmte beh ¨ordliche Anforderung in den folgen-\nden Zeilennummern vollst ¨andig erf ¨ullt ist.\nIst die folgende HGB-Teilanforderung im gegebenen Eingangsdokument\nvollst¨andig erf ¨ullt? Denke Schritt f ¨ur Schritt: Erkl ¨aren Sie, ob sie vollst ¨andig\nerf¨ullt ist, und geben Sie die relevanten Zeilennummern aus dem vorhanden\nDokument an. Basierend auf Ihrer Erkl ¨arung bestimmen Sie, ob die An-\nforderung vollst¨andig erf ¨ullt ist. Beenden Sie ihre Antwort mit - ‘yes’, wenn\ndie Anforderung erf ¨ullt ist, - ‘no’, wenn die Anforderung nicht erf ¨ullt ist,\n- ‘unclear’, wenn die Erf ¨ullung der Anforderung nicht beantwortet werden\nkann, da Kontextinformationen fehlen und - ‘not applicable’, wenn die\nAnforderung f ¨ur das Dokument nicht relevant ist.\nAnforderung: ‘{requirement}’ Dokument: ‘ {document}’\n{{‘answer’:”\nIII In-Out-Template (German)\n“System: Sie sind ein Experte im Bereich Wirtschaftspr ¨ufung und haben\nperfekte Kenntnisse des HGB-Bilanzierungstandards. Sie antworten immer\nwahrheitsgem¨aß, ob eine bestimmte beh ¨ordliche Anforderung in den folgen-\nden Zeilennummern vollst ¨andig erf ¨ullt ist.\nAntworten Sie mit: - ‘yes’, wenn die Anforderung erf ¨ullt ist, - ‘no’, wenn die\nAnforderung nicht erf ¨ullt ist, - ‘unclear’, wenn die Erf ¨ullung der Anforderung\nnicht beantwortet werden kann, da Kontextinformationen fehlen und - ‘not\napplicable’, wenn die Anforderung f ¨ur das Dokument nicht relevant ist.\nFormatieren Sie Ihre Ausgabe gem ¨aß dem folgenden JSON-Schema:\n{{‘answer’: <‘yes’|‘no’|‘unclear’|‘not applicable’>}}\nAnforderung: ‘{requirement}’ Dokument: ‘ {document}’\n{{‘answer’:”\nIV Cot-Template (German)\n“System: Sie sind ein Experte im Bereich Wirtschaftspr ¨ufung und haben\nperfekte Kenntnisse des HGB-Bilanzierungstandards. Sie antworten immer\nwahrheitsgem¨aß, ob eine bestimmte beh ¨ordliche Anforderung in den folgen-\nden Zeilennummern vollst ¨andig erf ¨ullt ist.\nDenke Schritt f ¨ur Schritt: Erkl ¨are f ¨ur jede Teilanforderung, ob sie vollst ¨andig\nerf¨ullt ist, und verweise in jeder Erkl ¨arung auf die relevanten Zeilennummern\naus dem gegebenen Dokument. Basierend auf deinen Erkl ¨arungen entscheide,\nob jede Teilanforderung vollst ¨andig erf ¨ullt ist. Beenden Sie ihre Antwort mit\n- ‘yes’, wenn die Anforderung erf ¨ullt ist, - ‘no’, wenn die Anforderung nicht\nerf¨ullt ist, - ‘unclear’, wenn die Erf ¨ullung der Anforderung nicht beantwortet\nwerden kann, da Kontextinformationen fehlen und - ‘not applicable’, wenn\ndie Anforderung f ¨ur das Dokument nicht relevant ist.\nAnforderung: ‘{requirement}’ Dokument: ‘ {document}’\n{{‘answer’:”\nV In-Out-Tot-Template (German)\n“System: Stellen Sie sich vor, drei verschiedene Experten im Bereich\nWirtschaftspr¨ufung beantworten diese Frage. Jeder Experte hat perfekte\nKenntnisse des HGB-Bilanzierungstandards. Alle Experten antworten immer\nwahrheitsgem¨aß, ob eine bestimmte beh ¨ordliche Anforderung in den folgen-\nden Zeilennummern vollst ¨andig erf ¨ullt ist.\nJeder Experte schreibt 1 Schritt seines Denkprozesses nieder und teilt ihn\nmit der Gruppe. Dann gehen alle Experten zum n ¨achsten Schritt ¨uber, usw.\n.Zeige jeden Schritt und den Denkprocess jedes Expereten. Wenn ein Experte\nzu irgendeinem Zeitpunkt feststellt, dass er falsch liegt, scheidet er aus.\nGeben Sie bei einer Mehrheitsabstimmung unter den Experten nur eine\nAntwort in diesem Format zur ¨uck: - ‘yes’, wenn die Anforderung erf ¨ullt ist,\n- ‘no’, wenn die Anforderung nicht erf ¨ullt ist, - ‘unclear’, wenn die Erf ¨ullung\nder Anforderung nicht beantwortet werden kann, da Kontextinformationen\nfehlen und - ‘not applicable’, wenn die Anforderung f ¨ur das Dokument nicht\nrelevant ist.\nFormatieren Sie Ihre Ausgabe gem ¨aß dem folgenden JSON-Schema:\n{{‘answer’: <‘yes’|‘no’|‘unclear’|‘not applicable’>}}\nAnforderung: ‘{requirement}’ Dokument: ‘ {document}’\n{{‘answer’:”\nVI In-Out-Tot-One-Shot-Template (German)\n“System: Stellen Sie sich vor, drei verschiedene Experten im Bereich\nWirtschaftspr¨ufung beantworten diese Frage. Jeder Experte hat perfekte\nKenntnisse des HGB-Bilanzierungstandards. Alle Experten antworten immer\nwahrheitsgem¨aß, ob eine bestimmte beh ¨ordliche Anforderung in den folgen-\nden Zeilennummern vollst ¨andig erf ¨ullt ist.\nJeder Experte schreibt 1 Schritt seines Denkprozesses nieder und teilt ihn\nmit der Gruppe. Dann gehen alle Experten zum n ¨achsten Schritt ¨uber, usw.\n.Zeige jeden Schritt und den Denkprocess jedes Expereten. Wenn ein Experte\nzu irgendeinem Zeitpunkt feststellt, dass er falsch liegt, scheidet er aus.\nGeben Sie bei einer Mehrheitsabstimmung unter den Experten nur eine\nAntwort in diesem Format zur ¨uck: - ‘yes’, wenn die Anforderung erf ¨ullt ist,\n- ‘no’, wenn die Anforderung nicht erf ¨ullt ist, - ‘unclear’, wenn die Erf ¨ullung\nder Anforderung nicht beantwortet werden kann, da Kontextinformationen\nfehlen und - ‘not applicable’, wenn die Anforderung f ¨ur das Dokument nicht\nrelevant ist.\nFormatieren Sie Ihre Ausgabe gem ¨aß dem folgenden JSON-Schema:\n{{‘answer’: <‘yes’|‘no’|‘unclear’|‘not applicable’>}}\nAnforderung: ‘{requirement}’ Dokument: ‘ {document}’\nExample:\nAnforderung: ‘Angabe des Betrags der Forderungen mit einer Restlaufzeit von\nmehr als einem Jahr; gesondert f ¨ur jeden ausgewiesenen Posten’ Dokument:\n‘544: S ¨amtliche Forderungen haben eine Restlaufzeit von weniger als einem\nJahr.\nExperte 1: ‘Die Anforderung verlangt die Angabe des Betrags der Forderun-\ngen mit einer Restlaufzeit von mehr als einem Jahr. In Zeile 544 wird jedoch\nangegeben, dass alle Forderungen eine Restlaufzeit von weniger als einem\nJahr haben. Das scheint zu implizieren, dass es keine Forderungen mit einer\nRestlaufzeit von mehr als einem Jahr gibt und die Anforderung erf ¨ullt worden\nist.’\nExperte 2: ‘Ich sehe den Punkt von Experte 1. Da das Dokument angibt, dass\nalle Forderungen eine Restlaufzeit von weniger als einem Jahr haben, k ¨onnen\nwir davon ausgehen, dass die Anforderung erf ¨ullt worden ist.’\nExperte 3: ‘Ich stimme meinen Kollegen zu. Obwohl das Dokument keine\nweiteren spezifischen Informationen enth ¨alt, k ¨onnen wir aus dem Kontext\nschließen, dass die Anforderung erf ¨ullt worden ist.’\n{{‘answer’: ‘yes’}}\n{{‘answer’:”\nVII In-Out-One-Shot-Template (German)\n“System: Sie sind ein Experte im Bereich Wirtschaftspr ¨ufung und haben\nperfekte Kenntnisse des HGB-Bilanzierungstandards. Sie antworten immer\nwahrheitsgem¨aß, ob eine bestimmte beh ¨ordliche Anforderung in den folgen-\nden Zeilennummern vollst ¨andig erf ¨ullt ist.\nAntworten Sie mit: - ‘yes’, wenn die Anforderung erf ¨ullt ist, - ‘no’, wenn die\nAnforderung nicht erf ¨ullt ist, - ‘unclear’, wenn die Erf ¨ullung der Anforderung\nnicht beantwortet werden kann, da Kontextinformationen fehlen und - ‘not\napplicable’, wenn die Anforderung f ¨ur das Dokument nicht relevant ist.\nFormatieren Sie Ihre Ausgabe gem ¨aß dem folgenden JSON-Schema:\n{{‘answer’: <‘yes’|‘no’|‘unclear’|‘not applicable’>}}\nAnforderung: ‘{requirement}’ Dokument: ‘ {document}’\nExample:\nAnforderung: ”Angabe des Betrags der Forderungen mit einer Restlaufzeit von\nmehr als einem Jahr; gesondert f ¨ur jeden ausgewiesenen Posten” Dokument:\n”544: S ¨amtliche Forderungen haben eine Restlaufzeit von weniger als einem\nJahr.\n{{‘answer’: ‘yes’}}\n{{‘answer’:”\nVIII In-Out-Tot-One-Shot-Template (German)\n“System: Sie sind ein Experte im Bereich Wirtschaftspr ¨ufung und haben\nperfekte Kenntnisse des HGB-Bilanzierungstandards. Sie antworten immer\nwahrheitsgem¨aß, ob eine bestimmte beh ¨ordliche Anforderung in den folgen-\nden Zeilennummern vollst ¨andig erf ¨ullt ist.\nAntworten Sie mit: - ‘yes’, wenn die Anforderung erf ¨ullt ist, - ‘no’, wenn die\nAnforderung nicht erf ¨ullt ist, - ‘unclear’, wenn die Erf ¨ullung der Anforderung\nnicht beantwortet werden kann, da Kontextinformationen fehlen und - ‘not\napplicable’, wenn die Anforderung f ¨ur das Dokument nicht relevant ist.\nFormatieren Sie Ihre Ausgabe gem ¨aß dem folgenden JSON-Schema:\n{{‘answer’: <‘yes’|‘no’|‘unclear’|‘not applicable’>}}\nAnforderung: ‘{requirement}’ Dokument: ‘ {document}’\nExample:\nAnforderung: ‘Angabe des Betrags der Forderungen mit einer Restlaufzeit von\nmehr als einem Jahr; gesondert f ¨ur jeden ausgewiesenen Posten’ Dokument:\n‘544: S ¨amtliche Forderungen haben eine Restlaufzeit von weniger als einem\nJahr.\n{{‘answer’: ‘yes’}}\n{{‘answer’:”",
  "topic": "Audit",
  "concepts": [
    {
      "name": "Audit",
      "score": 0.7694653272628784
    },
    {
      "name": "Compliance (psychology)",
      "score": 0.7376187443733215
    },
    {
      "name": "Computer science",
      "score": 0.5078585743904114
    },
    {
      "name": "Accounting",
      "score": 0.40457338094711304
    },
    {
      "name": "Business",
      "score": 0.3380843997001648
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210144576",
      "name": "Fraunhofer Institute for Intelligent Analysis and Information Systems",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I135140700",
      "name": "University of Bonn",
      "country": "DE"
    }
  ]
}