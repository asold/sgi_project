{
  "title": "Algebraic graph-assisted bidirectional transformers for molecular property prediction",
  "url": "https://openalex.org/W3166272013",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A1985267153",
      "name": "Dong Chen",
      "affiliations": [
        "Peking University",
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2973360518",
      "name": "Kaifu Gao",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2095985200",
      "name": "Duc Duy Nguyen",
      "affiliations": [
        "University of Kentucky"
      ]
    },
    {
      "id": "https://openalex.org/A2100185308",
      "name": "Chen Xin",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2014579076",
      "name": "Yi Jiang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2629267498",
      "name": "Guo-Wei Wei",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A1969492474",
      "name": "Feng Pan",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1985267153",
      "name": "Dong Chen",
      "affiliations": [
        "Peking University Shenzhen Hospital",
        "Peking University",
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2973360518",
      "name": "Kaifu Gao",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2095985200",
      "name": "Duc Duy Nguyen",
      "affiliations": [
        "University of Kentucky"
      ]
    },
    {
      "id": "https://openalex.org/A2100185308",
      "name": "Chen Xin",
      "affiliations": [
        "Peking University",
        "Peking University Shenzhen Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2014579076",
      "name": "Yi Jiang",
      "affiliations": [
        "Peking University",
        "Peking University Shenzhen Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2629267498",
      "name": "Guo-Wei Wei",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A1969492474",
      "name": "Feng Pan",
      "affiliations": [
        "Peking University Shenzhen Hospital",
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2773405722",
    "https://openalex.org/W2017398555",
    "https://openalex.org/W2963028280",
    "https://openalex.org/W2963017945",
    "https://openalex.org/W2741292700",
    "https://openalex.org/W2749279690",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W2901476322",
    "https://openalex.org/W2793945656",
    "https://openalex.org/W2773987374",
    "https://openalex.org/W3002565078",
    "https://openalex.org/W3001282080",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W3012107310",
    "https://openalex.org/W2200017991",
    "https://openalex.org/W2036298287",
    "https://openalex.org/W2148950790",
    "https://openalex.org/W3004227146",
    "https://openalex.org/W2963969719",
    "https://openalex.org/W2951676304",
    "https://openalex.org/W3005417975",
    "https://openalex.org/W2984249386",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2973114758",
    "https://openalex.org/W1757990252",
    "https://openalex.org/W2558999090",
    "https://openalex.org/W2082142179",
    "https://openalex.org/W2046228060",
    "https://openalex.org/W2093216528",
    "https://openalex.org/W2157851318",
    "https://openalex.org/W16352594",
    "https://openalex.org/W3175318380",
    "https://openalex.org/W2911612351",
    "https://openalex.org/W1977573154",
    "https://openalex.org/W2964007201",
    "https://openalex.org/W2061800084",
    "https://openalex.org/W2020116600",
    "https://openalex.org/W2966357564",
    "https://openalex.org/W3134146005",
    "https://openalex.org/W2406943157",
    "https://openalex.org/W2073201685",
    "https://openalex.org/W2786477584",
    "https://openalex.org/W1560202902",
    "https://openalex.org/W3208580134",
    "https://openalex.org/W3004917954",
    "https://openalex.org/W3100751385",
    "https://openalex.org/W1545231783",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3033056555",
    "https://openalex.org/W2784055555",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W197967138",
    "https://openalex.org/W1950993160"
  ],
  "abstract": "Abstract The ability of molecular property prediction is of great significance to drug discovery, human health, and environmental protection. Despite considerable efforts, quantitative prediction of various molecular properties remains a challenge. Although some machine learning models, such as bidirectional encoder from transformer, can incorporate massive unlabeled molecular data into molecular representations via a self-supervised learning strategy, it neglects three-dimensional (3D) stereochemical information. Algebraic graph, specifically, element-specific multiscale weighted colored algebraic graph, embeds complementary 3D molecular information into graph invariants. We propose an algebraic graph-assisted bidirectional transformer (AGBT) framework by fusing representations generated by algebraic graph and bidirectional transformer, as well as a variety of machine learning algorithms, including decision trees, multitask learning, and deep neural networks. We validate the proposed AGBT framework on eight molecular datasets, involving quantitative toxicity, physical chemistry, and physiology datasets. Extensive numerical experiments have shown that AGBT is a state-of-the-art framework for molecular property prediction.",
  "full_text": "ARTICLE\nAlgebraic graph-assisted bidirectional transformers\nfor molecular property prediction\nDong Chen 1,2, Kaifu Gao2, Duc Duy Nguyen3, Xin Chen1, Yi Jiang1, Guo-Wei Wei 2,4,5✉ & Feng Pan 1✉\nThe ability of molecular property prediction is of great signiﬁcance to drug discovery, human\nhealth, and environmental protection. Despite considerable efforts, quantitative prediction of\nvarious molecular properties remains a challenge. Although some machine learning models,\nsuch as bidirectional encoder from transformer, can incorporate massive unlabeled molecular\ndata into molecular representations via a self-supervised learning strategy, it neglects three-\ndimensional (3D) stereochemical information. Algebraic graph, speciﬁcally, element-speciﬁc\nmultiscale weighted colored algebraic graph, embeds complementary 3D molecular infor-\nmation into graph invariants. We propose an algebraic graph-assisted bidirectional trans-\nformer (AGBT) framework by fusing representations generated by algebraic graph and\nbidirectional transformer, as well as a variety of machine learning algorithms, including\ndecision trees, multitask learning, and deep neural networks. We validate the proposed AGBT\nframework on eight molecular datasets, involving quantitative toxicity, physical chemistry,\nand physiology datasets. Extensive numerical experiments have shown that AGBT is a state-\nof-the-art framework for molecular property prediction.\nhttps://doi.org/10.1038/s41467-021-23720-w OPEN\n1 School of Advanced Materials, Peking University, Shenzhen Graduate School, Shenzhen, China.2 Department of Mathematics, Michigan State University,\nEast Lansing, MI, USA.3 Department of Mathematics, University of Kentucky, Lexington, KY, USA.4 Department of Electrical and Computer Engineering,\nMichigan State University, East Lansing, MI, USA.5 Department of Biochemistry and Molecular Biology, Michigan State University, East Lansing, MI, USA.\n✉email: weig@msu.edu; panfeng@pkusz.edu.cn\nNATURE COMMUNICATIONS|         (2021) 12:3521 | https://doi.org/10.1038/s41467-021-23720-w | www.nature.com/naturecommunications 1\n1234567890():,;\nT\nhe fact that there is no speciﬁc and effective drug for\ncoronavirus disease 2019 (COVID-19) 1 year after the\noutbreak reminds us that drug discovery remains a grand\nchallenge. Rational drug discovery involves a long list of mole-\ncular properties, including binding af ﬁnity, toxicity, partition\ncoefﬁcient, solubility, pharmacokinetics, pharmacodynamics, etc\n1.\nExperimental determination of molecular properties is very time-\nconsuming and expensive. In addition, experimental testing\ninvolving animals or humans is subject to serious ethical con-\ncerns. Therefore, various computer-aided or in silico approaches\nhave become highly attractive because they can produce quick\nresults without seriously sacriﬁcing accuracy in many cases2. One\nof the most popular approaches is the quantitative structure-\nactivity relationship analysis. It assumes that similar molecules\nhave similar bioactivities and physicochemical properties3.\nRecently, machine learning (ML), including deep learning\n(DL), has emerged as a powerful approach for data-driven dis-\ncovery in molecular science. For example, graph convolutional\nnetworks (GCNs)4–6, convolutional neural networks (CNNs)7,\nand recurrent neural networks (RNNs)8, have become popular for\ndrug discovery and molecular analysis8–10. Generative adversarial\nnetworks (GANs) 11 combined with some machine learning\nstrategies, such as supervised learning and reinforcement learn-\ning, have also been applied to the generation of novel molecules\nand drug design12. However, DL methods require large datasets\nto determine their large number of weights and might not be\ncompetitive for small datasets13.\nAlthough DL methods, particularly CNN and GANs, can\nautomatically extract features from simple data, such as images\nand/or texts, the performance of ML and DL methods for\nmolecules, particularly macromolecules, crucially depends on the\nmolecular descriptors or molecular representations due to their\nintricate structural complexity14. Earlier molecular descriptors are\ndesigned as the proﬁles or ﬁngerprints of interpretable physical\nproperties in a bit string format15. Variousﬁngerprints have been\ndeveloped in the past few decades 16,17. There are four main\ncategories of two-dimensional (2D) ﬁngerprints17, namely sub-\nstructure key-based ﬁngerprints18, topological or path-based\nﬁngerprints19, circular ﬁngerprints16, and pharmacophore\nﬁngerprints20. However, 2D ﬁngerprints lack three-dimensional\n(3D) structural information of molecules, especially stereo-\nchemical descriptions.\nTo deal with the aforementioned problems, 3D-structure-based\nﬁngerprints have been developed to capture 3D patterns of\nmolecules21. However, the molecular structural complexity and\nhigh dimensionality are the major obstacles in designing efﬁcient\n3D ﬁngerprints14. Recently, a variety of 3D molecular repre-\nsentations based on advanced mathematics, including algebraic\ntopology7,22, differential geometry23, and algebraic graph24 have\nbeen proposed to simplify the structural complexity and reduce\nthe dimensionality of molecules and biomolecules 14,25. These\nmethods have had tremendous success in protein classiﬁcation,\nand the predictions of solubility, solvation-free energies, toxicity,\npartition coef ﬁcients, protein folding stability changes upon\nmutation, and Drug Design Data Resource (D3R) Grand\nChallenges14,26, a worldwide competition series in computer-\naided drug design. However, this approach depends on the\navailability of reliable 3D molecular structures.\nAlternatively, a self-supervised learning (SSL) strategy can be\nused to pre-train an encoder model that can produce latent space\nvectors as molecular representations without 3D molecular\nstructures. The unlabeled data is used in the SSL strategy, but\nunlike unsupervised learning, the data input to the model is\npartially masked, and then the model is trained to predict the\nmasked part in the training process, where the originally masked\ndata can be used as labels. This strategy allows a large amount of\nunlabeled data to be utilized. The initial development of SSL was\ndue to the need for natural language processing (NLP)27,28. For\nexample, bidirectional encoder representations from transformers\n(BERTs) are designed to pre-train deep bidirectional transformer\nrepresentations from unlabeled texts\n27. The techniques developed\nin understanding sequential words and sentences in NLP have\nbeen used for understanding the fundamental constitutional\nprinciples of molecules expressed as a simpliﬁed molecular-input\nline-entry system (SMILES)29. Unlabeled SMILES strings can be\nconsidered as text-based chemical sentences and are used as\ninputs for SSL pre-training 28,30. It is worth noting that the\navailability of large public chemical databases such as ZINC31 and\nChEMBL32 makes SSL a viable option for molecular representa-\ntion generation. However, latent-space representations ignore\nmuch stereochemical information, such as the dihedral angle33\nand chirality34. In addition, latent-space representations lack\nspeciﬁc physical and chemical knowledge about task-speci ﬁc\nproperties. For example, van der Waals interactions can play a\ngreater role than the covalent interactions in many drug-related\nproperties35, and need to be considered in the description of these\nproperties.\nIn this work, we introduce algebraic graph-assisted bidirec-\ntional transformer (AGBT) to construct molecular representa-\ntions via combining the advantages of 3D element-speci ﬁc\nweighted colored algebraic graphs and deep bidirectional trans-\nformers. The element-speciﬁc weighted colored algebraic graphs\ngenerate intrinsically low-dimensional molecular representations,\ncalled algebraic graph-based ﬁngerprints (AG-FPs), that sig-\nniﬁcantly reduce the molecular structural complexity while\nretaining essentially physical/chemical information and physical\ninsight24. Deep bidirectional transformer (DBT) utilizes an SSL-\nbased pre-training process to learn fundamental constitutional\nprinciples from massive unlabeled SMILES data and aﬁne-tuning\nprocedure to further train the model with task-speciﬁc data. The\nresulting molecularﬁngerprints, called bidirectional transformer-\nbased ﬁngerprints (BT-FPs), are latent-space vectors of the DBT.\nThe proposed AGBT model is applied to eight benchmark\nmolecular datasets involving quantitative toxicity and partition\ncoefﬁcient\n2,13,36,37. Extensive validation and comparison suggest\nthat the proposed AGBT model gives rise to some of the best\npredictions of molecular properties.\nResults\nIn this section, we present the proposed AGBT model and its\nresults for molecular prediction on eight datasets, i.e., LD50,\nIGC50, LC50, LC50DM, partition coefﬁcient, FreeSolv, Lipophi-\nlicity, and BBBP datasets. Supplementary Table 1 lists the basic\ninformation of these datasets and the CheMBL32 dataset was used\nin the pre-training. More descriptions of the datasets can be\nfound in Supplementary Note 1.\nAlgebraic graph-assisted deep bidirectional transformer\n(AGBT). As shown in Fig.1, the proposed AGBT consists of four\nmajor modules: AG-FP generator (i.e., the blue rectangles), BT-\nFP generator (i.e., the orange rectangles), random forest (RF)-\nbased feature-fusion module (i.e., the green rectangle), and\ndownstream machine learning module (i.e., the pink rectangle).\nFor the graph ﬁngerprint generation, we use element-speciﬁc\nmultiscale weighted colored algebraic graphs to encode the che-\nmical and physical interactions into graph invariants and capture\n3D molecular structural information. The BT-FPs have created in\ntwo steps: an SSL-based pre-training step with massive unlabeled\ninput data and a task-speciﬁc ﬁne-tuning step. The task-speciﬁc\nﬁne-tuning step can be executed in two ways. Theﬁrst way is\nmerely to adopt the same SSL procedure toﬁne-tune the model\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-23720-w\n2 NATURE COMMUNICATIONS|         (2021) 12:3521 | https://doi.org/10.1038/s41467-021-23720-w | www.nature.com/naturecommunications\nwith task-speciﬁc data and generate their BT-FPs. The other way\nis to utilize labels in task-speciﬁc data via a SL procedure toﬁne-\ntuning model and generate latent-space vectors of task-speciﬁc\ndata, denoted as BTs-FPs (i.e, the orange vector). The random\nforest algorithm is used to rank the importance of fused AG-FP\nand BT-PF features and select an optimal set of AGBT-FPs of a\nﬁxed number of components. The downstream machine learning\nalgorithms are fed with optimal features to achieve the best\nperformance on four benchmark toxicity datasets.\nWe carry out our ﬁnal predictions by using some standard\nmachine learning algorithms, namely, gradient boosted decision\ntree (GBDT), random forest (RF), and deep neural networks\n(DNNs), including single-task DNN (ST-DNN, Supplementary\nFig. 10a) and multitask DNN (MT-DNN, Supplementary\nFig. 10b). Our training follows the traditional pipeline 38.T o\neliminate systematic errors in the machine learning models, for\neach machine learning algorithm, the consensus of the predicted\nvalues from 20 different models (generated with different random\nseeds) was taken for each molecule. Note that the consensus value\nhere refers to the average of the predicted results from different\nmodels for each molecule of each speciﬁc training-test splitting.\nIn this work, the squared Pearson correlation coefﬁcient (R2),\nroot-mean-square error (RMSE), and mean absolute error (MAE)\nare used to assess the performance of the regression task, while\nthe classi ﬁcation accuracy and the area under the receiver\noperating characteristic convex hull (AUC-ROC) are used to\nevaluate the performance of classiﬁcation model. All deﬁnitions\nare given in Supplementary Note 2. Further details on our AGBT\nmodel are given in the“Method” Section and the parameters set is\nprovided in Supplementary Note 3.\nToxicity prediction. Toxicity, a critical issue to consider in drug\nlead optimization, measures the degree to which a chemical\ncompound can affect an organism adversely2. Indeed, toxicity and\nside effects are responsible for more than half of drug candidate\nfailures on their path to the market39. The LC50DM set refers to\nthe concentration of test chemicals in the water in milligrams per\nliter that cause 50% Daphnia Magna to die after 48 h. Its size is\nthe smallest among the four datasets. Among its 353 molecules,\n283 are used as a training set and the rest 70 as a test set2. The\nsmall size leads to difﬁculties in training a good prediction model.\nThe overﬁtting issue poses a challenge to traditional machine\nlearning methods if a large number of descriptors is used. In this\nwork, the MT-DNN is applied to extract information from data\nsets that share certain statistical distributions, which can effec-\ntively improve the predictive ability of models and avoided\noverﬁtting on the small datasets\n2,13.\nBased on the AGBT framework, we fuse AG-FPs and BTs-FPs,\ni.e., BT-FPs with a supervised ﬁne-tuning procedure for task-\nspeciﬁc data. The best performance is obtained by the MT-DNN\nmodel, whichR2 = 0.830 and RMSE= 0.743. As shown in Fig.2b,\nour model yields the best result, which is over 13% better than the\nprevious best score ofR2 = 0.733.\nThe IGC50 set is the second-largest toxicity set and its toxicity\nvalues range from 0.334−log10 mol/L to 6.36−log10 mol/L2.A s\nshown in Fig.2a, the R2s from different methodsﬂuctuate from\n0.274 to 0.810 Karim et al.40 also studied IGC50 dataset, but their\ntraining set and test set are different from those of others2 and\nthus their results cannot be included in the present comparison.\nFor our method, the R2 of MT-DNN with AGBT-FP is 0.842,\nwhich exceeds that of all existing methods on the dataset IGC50.\nThe oral rat LD50 set measures the number of chemicals that\ncan kill half of the rats when orally ingested36,37,41. This dataset is\nthe largest set among the four sets with as many as 7413\ncompounds. However, a large range of values in this set makes it\nrelatively difﬁcult to predict42. Gao et al.17 studied this problem\nusing many 2D molecular ﬁngerprints and various machine\nlearning methods, which include GBDT, ST-DNN, and MT-\nDNN. However, the prediction accuracy of the LD50 data set was\nnot improved much. As shown in Table 1 (the complete\ncomparison in Supplementary Table 6), the R2 values for all\nexisting methods range from 0.392 to 0.643. In our case, our\nmethod can achieveR2 0.671 and RMSE 0.554 log(mol/L), which\nare better than those methods.\nLC50 dataset reports the concentration of test chemicals in\nwater by milligrams per liter that cause 50% of fathead minnows\\\nto die after 96 h\n41. Wu et al.2 used physical information including\nenergy, surface energy, electric charge, and so on to construct\nmolecular descriptors. These physical properties are related to\nmolecular toxicity, achieving the prediction accuracy ofR2 0.771.\nIn this work, our AGBT-FPs with MT-DNN deliver the bestR2 of\n0.776. We also test the performance of our BT-FPs, which achieve\nR2 0.783 with MT-DNN. As listed in Table 1, our model\noutperforms all other existing methods.\nPartition coefﬁcient prediction. Partition coefﬁcient denoted P,\nderived from the ratio of the concentration of a mixture of two\nmutually insoluble solvents (octanol and water in these data) at\nequilibrium, measures the drug relevance of the compound as\nwell as its hydrophobicity to the human bodies. The logarithm of\nthis coefﬁcient is denoted as logP43. The training set used for logP\nprediction includes 8199 molecules44. A set of 406 molecules\napproved by the Food and Drug Administration (FDA) is used as\norganic drugs were used as the test set44 and its logP values range\nfrom −3.1 to 7.57. The comparison of different prediction\nmethods for FDA molecular data set is listed in Table 1 and\nSupplementary Table 6. It should be mentioned that the ALOGPS\nmodel established by Tetko et al.45 can also be used in logP\nprediction, however, there is no guarantee that the training set of\nALOGPS are independent of the test set and thus its result is not\nincluded in the comparison. As we can see from Table1, our\nAGBTs-FPs with STDNN model produce the bestR2 of 0.905.\nThe predicted result of AGBTs-FPs with STDNN model for FDA\ndata set are shown in Supplementary Fig. 11c.\nFig. 1 Illustration of AGBT model.For a given molecular structure and its\nSMILES strings, AG-FPs are generated from element-speciﬁc algebraic\nsubgraphs module and BT-FPs are generated from a deep bidirectional\ntransformer module, as shown inside the dashed rectangle, which contains\nthe pre-training andﬁne-tuning processes, and thenﬁnally completes the\nfeature extraction using task-speciﬁc SMILES as input. Then the random\nforest algorithm is used to fuse, rank, and select optimalﬁngerprints\n(AGBT-FPs) for machine learning.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-23720-w ARTICLE\nNATURE COMMUNICATIONS|         (2021) 12:3521 | https://doi.org/10.1038/s41467-021-23720-w | www.nature.com/naturecommunications 3\nFreeSolv and lipophilicity prediction. Solvation-free energy and\nlipophilicity are basic physical and chemical properties for\nunderstanding how molecules interact with solvents. In this work,\nFreeSolv and Lipophilicity datasets are derived from\nMoleculeNet9, which is a benchmark for molecular property\nprediction. There are 643 samples and 4200 samples for FreeSolv\nand Lipophilicity datasets, respectively. For comparison, the\ndatasets were split into train, validation, test sets with the ratio of\n8:1:1, which follows the same procedure as MoleculeNet9. We set\ndifferent random seeds and follow the same procedure ten times\nto obtain ten different data splitting. To eliminate systematic\nerrors in downstream machine learning models and to better\ncompare molecular descriptors, for each machine learning algo-\nrithm, the consensus of the predicted values from 20 models of\ndifferent random seeds for each data-splitting was taken for each\nmolecule. And theﬁnal score for the dataset is the average score\nover ten different data-splittings. As shown in Table1, using the\nSTDNN algorithm, our method obtains the best RMSE of 0.994\non FreeSolv, which is better than the best results reported by\nChemprop46 (RMSE = 1.075), MMNB47 (RMSE = 1.155), and\nMoleculeNet9 (RMSEGraphConv = 1.15). For the Lipophilicity\ndataset, using the RF algorithm, the RF method, our descriptors\nobtains the best RMSE of 0.573 ± 0.026, a result that is close to the\nbest result reported by Chemprop46 (RMSEGraphCov = 0.555). A\ncomplete result with multiple evaluation metrics is available in\nSupplementary Table 7.\nClassiﬁcation task for binary labels of blood – brain barrier\npenetration (BBBP). The BBBP dataset contains 2042 small\nmolecules and original from a study on the modeling and pre-\ndiction of barrier permeability48. The binary labels for compound\npermeability properties are used in this study. For a better com-\nparison, we follow the same scaffold splitting method described in\nMoleculeNet9. The dataset was split into training, validation, and\nthe test set follow the ratio of 8/1/1. Table1 reports the best\nachieved AUC-ROC of our method is 0.763, which is better than\nresults reported in Chemprop46 (AUC-ROC = 0.738), MMNB47\n(AUC-ROC =0.739), and MoleculeNet 9 (AUC-ROCECFP =\n0.671).\nIt is worth noting that to validate the performance of our\nAGBT framework, for downstream machine learning models, we\nFig. 2 Results from AGBT framework and feature analysis. a, b Illustrate the comparison of theR2 by various methods for the IGC50 set and the LC50DM\nset, respectively.AGBTs-FP means a supervisedﬁne-tuning process is applied to AGBT-FP. The other results were taken from refs.2,13,17,23,40,41. c The bar\ncharts illustrate theR2 of AGBT-FPs and BT-FPs with three machine learning algorithms for the IGC50 dataset.d The bar charts illustrate the consensusR2\nof AGBT-FPs and AGBTs-FPs with three machine learning algorithms for the LC50DM dataset.e Visualization of the LD50 set. The axes are the top three\nimportant features of AGBT-FPs.f Predicted results of AGBT-FPs with MT-DNN model for the IGC50 set(left) and the LC50DM set(right), respectively.\nThe box plots statisticR2 values forn = 358 (left), and 70 (right) independent samples examined over 20 independent machine learning experiments, and\nthe detailed statistic values are listed in Supplementary Table 5.g The AGBT-FPs of the IGC50 and LC50DM datasets were ranked by their feature\nimportance. For both datasets, 188/512 of the AGBT features are from AG-FPs and the remaining 348/512 are from BT-FPs.h Based on AGBT-FPs and\nAGBTs-FPs, the variance ratios in theﬁrst two components from the principal component analysis (PCA) are used to visualize the IGC50 and LC50DM\ndatasets.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-23720-w\n4 NATURE COMMUNICATIONS|         (2021) 12:3521 | https://doi.org/10.1038/s41467-021-23720-w | www.nature.com/naturecommunications\ndid not over-tune the parameters to optimize our results in this\nstudy. For all the above-mentioned datasets, the same set of\nparameters is taken for each type of descriptors in our model. The\ndetails of the parameter settings can be found in Supplementary\nNote 3. Our method exhibits state-of-the-art results in seven out\nof eight above-mentioned datasets. This illustrates the stable and\nrobust performance of our AGBT framework and its applicability\nto a wide range of molecular prediction tasks.\nDiscussion\nIn this section, we discuss how the AGBT model brings insights\nto molecular property predictions, as well as the enhancement\nthat algebraic graph-based ﬁngerprints and deep bidirectional\ntransformers-based ﬁngerprints give rise to our proposed AGBT\nmethod.\nImpact of algebraic graph descriptor. Pre-trained on a large\nnumber of molecules, deep SSL-based molecular ﬁngerprints\ncould achieve high accuracy. Many deep learning-based mole-\ncular ﬁngerprints have shown a better performance than con-\nventional ﬁngerprints. However, deep learning ﬁngerprints,\nincluding our BT-FPs, are prone to the loss of molecular ste-\nreochemical information. This lack of information often makes\n\"activity cliff” a nuisance. An example in Supplementary Fig. 16\ndemonstrates this drawback. Therefore, we propose the use of\nalgebraic graph theory in association with our AGBT framework\nto retain stereochemical and physical information and enhance\nthe performance of original BT-FPs. Moreover, in this work, we\nset the total dimension of molecular ﬁngerprints after feature\nfusion to 512, and thus we only need to optimize one neural\nnetwork architecture. Our AGBT model is an efﬁcient framework\nfor molecular property predictions.\nFigure 2f shows the best prediction performance on the IGC50\nand LC50DM datasets using the AGBT framework, namely,R2 =\n0.842 on IGC50 andR2 = 0.830 on LC50DM. The orange bar at\neach point is the deviation of predicted toxicity with 20 models\n(with different random seed). For each model,R2 was calculated\nand the distribution of R2 is shown in sub ﬁgures. The\nperformance on the LD50 and LC50 datasets is shown in\nSupplementary Fig. 11b. For LD50, IGC50, and LC50DM\ndatasets, the best prediction results are obtained by the algebraic\ngraph-assisted with MTDNN. For the IGC50 dataset, theR2 of\nthe toxicity predictions from the three machine learning\nalgorithms, i.e., GBDT, ST-DNN, and MT-DNN, are shown in\nthe bar plot of Fig.2c. It is obvious that, for the IGC50 dataset,\nAGBT-FP performs better than BT-FP with GBDT and MT-\nDNN, while it shows the opposite result on the STDNN. It is\nmainly because that AG-FPs and BT-FPs are produced from two\ndifferent molecular ﬁngerprint generators and have the dimen-\nsions of 1800 and 512, respectively. The fused molecular\nﬁngerprints, AGBT-FPs, return 512 components with hetero-\ngeneous information from AG-FPs and BT-FPs and tend to cause\nsome anomalies in the STDNN method.\nFor the IGC50 dataset, 1434 molecular structures were used to\ntrain the AGBT model, leading toﬂuctuation in prediction Fig.2f.\nSimilar situations are found in the LD50 dataset and the LC50DM\ndataset, as shown in Supplementary Fig. 11. For the LC50 dataset,\nthe best result is obtained with BT-FPs, but the result of AGBT-\nFPs also reachesR\n2 0.776, exceeding the other reported methods.\nAs shown in Table2 and Supplementary Table 8, for FreeSolv\nand Lipophilicity datasets, the best results are all generated by\nusing fused descriptors, which illustrate that algebraic graph does\nhave an important impact on the molecular property prediction.\nIn Supplementary Table 8, the standard deviation onR2, RMSE,\nand MAE of FreeSolv and Lipophilicity’s prediction also show\nthat AGBTs-FP can obtain the most stable performance in most\ncases (5/6). Therefore, the fusion of AG-FPs and BT-FPs\nimproves the accuracy and stability of predictions for most\ndatasets. Mathematically based molecular descriptors can com-\nplement data-driven potential spatial descriptors.\nPredictive power of ﬁne-tuning strategies. In this work, we\ndevelop two strategies in theﬁne-tuning stage: SSL and SL with\ntask-speciﬁc data. It is found that SSL strategy (See Supplemen-\ntary Fig. 3 performs better on LD50, IGC50, and LC50 data sets,\nas shown in Fig.2f and Supplementary Fig. 11, while SL strategy\nwith task-speciﬁc data (See Supplementary Fig. 4 is the best for\nLC50DM dataset. The LC50DM dataset is the smallest set with\nonly 283 molecules in its training set. Conventional methods\ncannot capture enough information from such a small dataset to\nachieve satisfactory results. In the AGBT model, the pre-training\nstrategy with a bidirectional transformer enables the model to\nacquire a general knowledge of molecules. During theﬁne-tuning\nphase, we further feed the model with four toxicity datasets with\nlabels, and the labeled data guide the model to speciﬁcally extract\ntoxin-related information from all the training data. Then we\ncomplement ﬁne-tuning ﬁngerprints with algebraic graph\ndescriptors to ultimately enhance the robustness of the AGBT\nmodel and improve the performance on the LC50DM set (R2 =\n0.830, RMSE= 0.743).\nFigure 2d shows the performance of AGBT-FPs and AGBTs-\nFPs on the LC50DM dataset using three advanced machine\nlearning methods. The bar charts show the R2 of prediction\nresults with three machine learning algorithms. Thisﬁgure shows\nthat AGBTs-FPs have an excellent performance with all three\nmachine learning algorithms, withR2 values being 0.822 (GBDT),\n0.815 (ST-DNN), and 0.830 (MT-DNN), respectively. This\nindicates that AGBT s-FPs can capture general toxin-related\ninformation during the sequential ﬁne-tuning process. There is\nno signiﬁcant difference among the three predictions based on\nGBDT, ST-DNN, and MT-DNN. In contrast, AGBT-FPs are\nderived from the model after self-supervised training. Their pre-\ntraining and ﬁne-tuning processes do not involve any labeled\ndata. The resulting prediction accuracies with GBDT and ST-\nDNN are quite low withR2 being 0.587 and 0.659, respectively.\nThrough the MT-DNN model, the performance of AGBT-FPs\ncan be improved fromR2 0.587 to 0.781.\nTable 1 Comparison of the best-achieved performance with\nthe reported score on six datasets.\nLD50 LC50 FDA\nMethod R2 Method R2 Method R2\nOurs 0.671 Ours 0.783 Ours 0.905\nMACCS17 0.643 BTAMDL2 13 0.750 ESTD-1 43 0.893\nFP217 0.631 ESTDS 2 0.745 Estate2 17 0.893\nHybridModel40 0.629 Daylight-\nMTDNN17\n0.724 XLOGP3 44 0.872\nFreeSolv Lipophilicity BBBP\nMethod RMSE Method RMSE Method AUC-\nROC\nOurs 0.994 Ours 0.570 Ours 0.763\nMMNB47 1.155 MMNB 47 0.625 MMNB 47 0.739\nChemprop46 1.075 Chemprop 46 0.555 Chemprop46 0.738\nGraphConv9 1.15 GraphConv 9 0.715 ECFP 9 0.671\nThe best result for each data set has been bolded. Comparison of theR2 for the rest datasets,\nIGC50, and LC50DM, are shown in Fig.2 and Supplementary Table 6. A complete table with\nmultiple evaluation metrics for all eight datasets is available in Supplementary Table 7.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-23720-w ARTICLE\nNATURE COMMUNICATIONS|         (2021) 12:3521 | https://doi.org/10.1038/s41467-021-23720-w | www.nature.com/naturecommunications 5\nThe above discussion indicates that SSL can acquire general\nmolecular information and universal molecular descriptors\nwithout the guidance of labels. In downstream tasks, the MT-\nDNN model can also help to extract the task-speciﬁc information\nfrom related data. As for small datasets, such as the LC50DM\ndataset (300 samples), the subsequent ﬁne-tuning with an SL\nstrategy is much more promising.\nThe results of all eight datasets using AG-FP, BT-FP/BTs-FP,\nand AGBT-FP/AGBTs-FP are shown in Table 2. The fused\ndescriptors (AGBT-FP/AGBT s-FP) achieved the best perfor-\nmance in 5/8 of the tasks. For the LC50 dataset, the AGBT-FP\nprediction of 0.776 is very close to the best performance of 0.783\nobtained by BT-FPs. For Lipophilicity dataset, the performance of\nAGBT\ns-FP is RMSE= 0.579. It is close to the best RMSE (0.57).\nAnd for the BBBP dataset, the classi ﬁcation performance of\nAGBTs-FP is AUC-ROC= 0.761, which is almost the same as the\nbest 0.763. The complete results for all 8 datasets with multiple\nevaluation metrics are shown in Supplementary Table 7.\nMolecular representations and structural genes. In chemistry,\nthe properties of molecules, such as toxicity, are often determined\nby some speciﬁc functional groups or fragments. Similar to bio-\nlogical genes, molecules have some determinants of their prop-\nerties, which are called structural genes in this work. For some\npath-based ﬁngerprints, such as FP2, a molecule is represented by\n256 length vectors, each corresponding to a speciﬁc fragment.\nHowever, it is difﬁcult to achieve the best results from such a\nﬁngerprint, as shown in Fig.2a, b. The proposed AGBT-FP is a\n512-dimensional ﬁngerprint, with each dimension being a pro-\njection of various physical information about the molecule. In this\nsection, we hope to characterize the key dimensions of AGBT-FPs\nto identify the structural genes.\nUsing a random forest algorithm, we performed a feature\nimportance analysis of AGBT-FPs. As shown in Supplementary\nFig. 13, for the LD50, IGC50, and LC50 datasets, the top three\nfeatures in the feature importance ranking are all from algebraic\ngraph-based descriptors. For the toxicity datasets, the ratio of\ncomponents from AG-FPs and BT-FPs in the AGBT-FPs is 188:\n324, as shown in Fig. 2g and Supplementary Fig. 13. For the\nLC50DM dataset, the most important feature is from BT-FPs and\nthe 2nd and 3rd important features are from AG-FPs. This\nimplies that the multiscale weighted colored algebraic graph-\nbased molecular descriptors contribute the critical molecular\nfeatures, which are derived from embedding speciﬁc physical and\nchemical information into graph invariants. The top three\nimportant features of the LD50 set are illustrated in Fig. 2e,\nwhere each point represents a molecule and the toxicity is\nrepresented by the color. It is easy to ﬁnd that the top three\nimportant dimensions in AGBT-FP, denoted as Feature 1, Feature\n2, and Feature 3, divide the molecules into two groups: one can be\ndistinguished by Feature 3 and the other is a linear combination\nof Feature 1 and Feature 2. This means the molecule can be\nclassiﬁed by just three key dimensions (features), indicating that\nthese three features, or structural genes, dominate the intrinsic\ncharacteristics of molecules. However, since predicting molecular\ntoxicity is complex, it is difﬁcult to directly distinguish the toxicity\nof each molecule in AGBT-FPs through theﬁrst three dimen-\nsions. Similarly, the visualizations for the IGC50, LC50, and\nLC50DM datasets can be seen in Supplementary Fig. 14.\nWe projected both AGBT-FPs and AGBT s-FPs into an\northogonal subspace by principal component analysis. As shown\nin Fig. 2h, the ﬁrst two principal components of AGBT-FPs can\nroughly divide the data into two clusters and the molecules in the\nsame cluster have similar toxicity. Similarly, the top two\ncomponents of AGBT s-FPs are given in Fig. 2h. Along the\ndirection of the ﬁrst principal components, the molecular data\ncan be well clustered according to the toxicity, with low toxic\nmolecules on the left (green) and higher toxic molecules on the\nright (red). It indicates these two molecularﬁngerprints contain\nvery different information. As shown in Supplementary Fig. 15,\nfor AGBT-FPs we need 112 components to explain 90% of the\nvariance, while for AGBTs-FPs we only need 48 components. The\ntop two principal components of AGBT-FPs are just explaining\n9% and 8% of the variance, which indicates that, since there is no\nlabeled data to train the model, the generated AGBT-FPs\nrepresent general information about the molecular constitution\nrather than speciﬁc molecular properties. Theﬁrst two compo-\nnents for AGBTs-FPs can explain 40% and 13% of the variance\nrespectively, which indicates that by using SL-basedﬁne-tuning\ntraining, the model can effectively capture task-speci ﬁc\ninformation.\nThe AGBTs-FP model performs better in predicting speciﬁc\nproperties because the labeled data are used to train the model\nduring ﬁne-tuning. It should be noted that some molecular\ninformation irrelevant to that particular property might be lost in\nthis way. This strategy leads to better results for some datasets\nwith minimal data, such as LC50DM, whose small amount of data\nis not enough to effectively obtain property-speciﬁc information\nin downstream tasks. However, if more downstream data are\navailable, such as LD50, IGC50, and LC50, downstream machine\nlearning methods can also derive property-speciﬁc information\nfrom general molecular information. For example, AGBT-FPs\nperform better on LD50, IGC50, and LC50 datasets.\nDespite many efforts in the past decade, accurate and reliable\nprediction of numerous molecular properties remains a challenge.\nRecently, deep bidirectional transformers have become a popular\napproach in molecular science for their ability to extract\nfundamental constitutional information of molecules from\nmassive SSL. However, they neglect crucial stereochemical\ninformation. The algebraic graph is effective in simplifying\nmolecular structural complexity but relies on the availability of 3-\nD structures. We propose an AGBT framework for molecular\nproperty prediction. Speci ﬁcally, element-speci ﬁc multiscale\nweighted colored algebraic subgraphs are introduced to char-\nacterize crucial physical/chemical interactions. Moreover, for\nsmall datasets, we introduce a supervisedﬁne-tuning procedure\nTable 2 Performance of descriptors generated with the AGBT framework on eight datasets.\nDatasets LD50 IGC50 LC50 LC50DM LogP FreeSolv Lipophilicity BBBP\nMetric R2 R2 R2 R2 R2 RMSE RMSE AUC-ROC\nAG-FP 0.647 0.788 0.713 0.75 0.838 1.018 0.664 0.677\nBT-FP 0.667 0.839 0.783d 0.763 0.895 1.125 0.626 0.736\nBTs-FP 0.617 0.798 0.75 0.829 0.903 1.036 0.57a 0.763b\nAGBT-FP 0.671d 0.842d 0.776 0.781 0.885 0.994c 0.663 0.738\nAGBTs-FP 0.612 0.805 0.75 0.83d 0.905d 1.039 0.579 0.761\nA complete result with multiple evaluation metrics is available in Supplementary Table 7; Best performances are produced onaGBDT, bRF, cSTDNN, and dMTDNN, and are bolded.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-23720-w\n6 NATURE COMMUNICATIONS|         (2021) 12:3521 | https://doi.org/10.1038/s41467-021-23720-w | www.nature.com/naturecommunications\nto the standard pre-trained SSL to focus on task-speci ﬁc\ninformation. These approaches are paired with random forest,\ngradient boosted decision trees, multitask deep learning, and deep\nneural network algorithms in AGBT. We demonstrate that the\nproposed AGBT framework achieves R2 values of 0.671, 0.842,\n0.783, 0.830, and 0.905 on LD50, IGC50, LC50, LC50DM, and\nFDA logP dataset, respectively. In the datasets FreeSolv and\nLipophilicity, we obtained RMSE scores of 0.994 and 0.579,\nrespectively, and in the classiﬁcation dataset BBBP, we obtained\nan AUC-ROC score of 0.763. Our model can be easily extended to\nthe prediction of other molecular properties. Our results show\nthat the proposed AGBT is a robust, and powerful framework for\nstudying various properties of small molecules in drug discovery\nand environmental sciences.\nMethods\nAlgebraic graph-based molecularﬁngerprints (AG-FPs). Graph theory can\nencode the molecular structures from a high-dimensional space into a low-\ndimensional representation. The connections between atoms in a molecule can be\nrepresented by graph theory, as shown in Fig.3a, b. However, ignoring the\nquantitative distances between atoms and the different atomic types in traditional\ngraphs will result in the loss of critical chemical and physical information about the\nmolecule. Element-speciﬁc multiscale weighted colored graph representations can\nquantitatively capture the patterns of different chemical aspects, such as van der\nWaals interactions and hydrogen bonds between different atoms\n24. Figure 3c\nillustrates a colored graph representation, which captures the element information\nby using colored vertices and different edges are corresponding to different pair-\nwise interactions in the molecule. Moreover, the algebraic graph features are easily\nobtained from the statistics of the eigenvalues of appropriated graph Laplacians\nand/or adjacency matrices\n24.\nAs shown in Fig.3d, for a given molecule, weﬁrst construct element-speciﬁc\ncolored subgraphs using selected subsets of atomic coordinates as vertices,\nV ¼f ð ri; αiÞjri 2 R3; αi 2 E; i ¼ 1; 2; :::;Ng ð1Þ\nwhere E ¼f H; C; N; O; S; P; F; Cl; Br; ::: g is a set of commonly occurring element\ntypes for a given dataset. Andith atom in aN-atom subset is labeled both by its\nelement typeαi and its positionri. We denote all the pairwise interactions between\nelement types Ek1\nand Ek2\nin a molecule by fast-decay radial basis functions\nW ¼ Ψðkri /C0 rjk; ηk1 k2\nÞjαi ¼ Ek1\n; αj ¼ Ek2\n; i; j ¼ 1; 2; :::; N;\nn\n´ kri /C0 rjk >r i þ rj þ σ\no ð2Þ\nwhere kri /C0 rjk is the Euclidean distance betweenith andjth atoms in a molecule,\nri and rj are the atomic radii ofith and jth atoms, respectively, andσ is the mean\nstandard deviation ofri and rj in the dataset. Figure3e gives the illustration of\nLaplace and adjacency matrices based on the weighted colored subgraph. For the\nprediction of toxicity, van der Waals interactions are much more critical than\ncovalent interactions and thus the distance constraint (kri /C0 rjk >r i þ rj þ σ)i s\nused to exclude covalent interactions. In biomolecules, we usually choose\ngeneralized exponential functions or generalized Lorentz functions asΨ, which are\nweights between graph edges49. Here, ηk1 k2\nin the function is a characteristic\ndistance between the atoms and thus is a scale parameter. Therefore, we generate a\nweighted colored subgraphGðV; WÞ. In order to construct element-speciﬁc\nmolecular descriptors, the multiscale weighted colored subgraph rigidity is deﬁned\nas\nRIGðηk1 k2\nÞ¼ ∑\ni\nμG\ni ðηk1 k2\nÞ¼ ∑\ni\n∑\nj\nΨðkri /C0 rjk; ηk1 k2\nÞ;\nαi ¼ Ek1\n; αj ¼ Ek2\n; kri /C0 rjk >r i þ rj þ σ\nð3Þ\nwhere μG\ni ðηk1 k2\nÞ is a geometric subgraph centrality for theith atom50. TheμG\ni ðηk1 k2\nÞ\nhere is a weight subgraph generalization of Gaussian network model or a subgraph\ngeneralization of the multiscaleﬂexibility-rigidity index. The summation over\n∑jμG\ni ðηk1 k2\nÞ represents the total interaction strength for the selected pair of element\ntypes Ek1\nand Ek2\n, which provide the element-speciﬁc coarse-grained description of\nmolecular properties. By choosing appropriate element combinationsk1 and k2, the\ncharacteristic distance ηk1 k2\n, and subgraph weightΨ,w e ﬁnally construct a family\nof element-speciﬁc, scalable (i.e., molecular size independent), multiscale geometric\ngraph-based molecular descriptors24.\nTo generate associated algebraic graphﬁngerprints, we construct corresponding\ngraph Laplacians and/or adjacency matrices. For a given subgraph, its matrix\nrepresentation can provide a straightforward description of the interaction between\nsubgraph elements. To construct a Laplacian matrix, we consider a subgraphG\nk1 k2\nfor each pair of element typesEk1\nand Ek2\nand deﬁne an element-speciﬁc weighted\ncolored Laplacian matrixLðηk1 k2\nÞ as24\nLijðηk1 k2\nÞ¼\n/C0 Ψðkri /C0 rjkÞ if i ≠ j; αi ¼ Ek1\n; αj ¼ Ek2\nand kri /C0 rjk >r i þ rj þ σ;\n/C0 ∑jLij if i ¼ j\n(\nð4Þ\nMathematically, the element-speciﬁc weighted Laplacian matrix is symmetric,\ndiagonally dominant, and positive semi-deﬁnite, and thus all the eigenvalues are\nnon-negative. The ﬁrst eigenvalue of the Laplacian matrix is zero because the\nsummation of every row or every column of the matrix is zero. Theﬁrst non-zero\neigenvalue of Lijðηk1 k2\nÞ is the algebraic connectivity (i.e., Fiedler value).\nFurthermore, the rank of the zero-dimensional topological invariant, which\nrepresents the number of the connected components in the graph, is equal to the\nnumber of zero eigenvalues ofLijðηk1 k2\nÞ. A certain connection between geometric\ngraph formulation and algebraic graph matrix can be deﬁned by:\nRIg ðηk1 k2\nÞ¼ Tr Lðηk1 k2\nÞ; ð5Þ\nwhere Tr is the trace. Therefore, we can directly construct a set of element-speciﬁc\nweighted colored Laplacian matrix-based molecular descriptors by the statistics of\nnontrivial eigenvalues fλL\ni gi¼1;2;3;:: , i.e., summation, minimum, maximum, average,\nFig. 3 Illustration of weighted colored element-speciﬁc algebraic graphs. aThe molecular structure of 2-Triﬂuoroacetyl. b, c Represent a traditional graph\nrepresentation and a colored graph representation, respectively.d Illustration of the process of decomposing a colored graph into element-speciﬁc CC, FO,\nand CH subgroups, where element refers to the chemical element in this study, e.g., H, C, N.e Illustration of weighted colored element-speciﬁc subgraph\nGSH, its adjacency matrix, and Laplacian matrix, whereΨ refers to the weight of the edge in subgraph.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-23720-w ARTICLE\nNATURE COMMUNICATIONS|         (2021) 12:3521 | https://doi.org/10.1038/s41467-021-23720-w | www.nature.com/naturecommunications 7\nand standard deviation of nontrivial eigenvalues. Note that the Fiedler value is\nincluded as the minimum.\nSimilarly, an element-speciﬁc weighted adjacency matrix can be deﬁned by\nAijðηk1 k2\nÞ¼ Ψðkri /C0 rjkÞ if i ≠ j; αi ¼ Ek1\n; αj ¼ Ek2\nand kri /C0 rjk >r i þ rj þ σ;\n0i f i ¼ j\n/C26\nð6Þ\nMathematically, adjacency matrixAijðηk1 k2\nÞ is symmetrical non-negative matrix.\nThe spectrum of the proposed element-speciﬁc weighted colored adjacency matrix\nis real. A set of element-speciﬁc weighted labeled adjacency matrix-based molecular\ndescriptors can be obtained by the statistics offλA\ni gi¼1;2;3;:: , i.e., summation,\nminimum, maximum, average, and standard deviation of all positive eigenvalues.\nTo predict the properties of a molecule, graph invariants, such as the eigenvalue\nstatistics of the above matrix, can capture topological and physical information\nabout the molecules, which is named AG-FPs. Detailed parameters of the proposed\nalgebraic graph model can be found in Supplementary Note 3.\nBidirectional transformer ﬁngerprints (BT-FPs). Unlike RNN-based models,\nDBT is based on the attention mechanism and it is more parallelable to reduce the\ntraining time with massive data\n28. Based on the DBT architecture, Devlin et al.27\nintroduced a representation model called BERT for natural language processing.\nThere are two tasks involving BERT, masked language learning, and consecutive\nsentences classiﬁcation. Masked language learning uses a partially masked sentence\n(i.e., words) as input and employs other words to predict the masked words. The\nconsecutive sentences classiﬁcation is to classify if two sentences are consecutive. In\nthe present work, the inputs of the deep bidirectional transformer are molecular\nSMILES strings. Unlike the sentences in traditional BERT for natural language\nprocessing, the SMILES strings of different molecules are not logically connected.\nHowever, we train the bidirectional encoder from the transformer to recover the\nmasked atoms or functional groups.\nBecause a molecule could have multiple SMILES representations, weﬁrst convert\nall the input data into canonical SMILES strings, which provide a unique\nrepresentation of each molecular structure\n51. Then, a SMILES string is split into\nsymbols, e.g., C, H, N, O,=, Br, etc., which generally represent the atoms, chemical\nbonds, and connectivity, see Supplementary Table 2 for more detail. In the pre-\ntraining stage, weﬁrst select a certain percentage of the input symbols randomly for\nthree types of operations: mask, random changing, and no changing. The purpose of\nthe pre-training is to learn fundamental constitutional principles of molecules in a SSL\nmanner with massive unlabeled data. A loss function is built to improve the rate of\ncorrectly predicted masked symbols during the training. For each SMILES string, we\nadd two special symbols, <s>a n d< \\s>. Here, <s> means the beginning of a SMILES\nstring and <\\s> is a special terminating symbol. All symbols are embedded into input\ndata of aﬁxed length. A position embedding is added to every symbol to indicate the\norder of the symbol. The embedded SMILES strings are fed into the BERT framework\nfor further operation. Supplementary Fig. 2 shows the detailed process of pre-training\nprocedure. In our work, more than 1.9 million unlabeled SMILES data from\nCheMBL\n32 are used for the pre-training so that the model learns basic \"syntactic\ninformation” about SMILES strings and captures global information of molecules.\nBoth BT-FPs and BTs-FPs are created in theﬁne-tuning training step, which\nfurther learns the characteristics of task-speciﬁc data. Two types ofﬁne-tuning\nprocedures are used in our task-speciﬁc ﬁne-tuning. Theﬁrst type is still based on\nthe SSL strategy, where the task-speciﬁc SMILES strings are used as the training\ninputs, as shown in Supplementary Fig. 3. To accurately identify these task-speciﬁc\ndata, only the“mask” and “no changing” operations are allowed in thisﬁne-tuning.\nThe resulting latent-space representations are called BT-FPs.\nThe secondﬁne-tuning procedure is based on an SL strategy with labeled task-\nspeciﬁc data. As shown in Supplementary Fig. 4, when dealing with multiple\ndatasets with cross-dataset correlations, such as four toxicity datasets in the present\nstudy (Supplementary Table 4), we make use of all the labels of four datasets to\ntune the model weights via supervised learning before generating the latent-space\nrepresentations (i.e., BT\ns-FPs), which signiﬁcantly strengthens the predictive power\nof the model on the smallest dataset.\nIn our DBT, an input SMILES string has a maximal allowed length of\n256 symbols. During the training, each of the 256 symbols is embedded into a 512-\ndimensional vector that contains the information of the whole SMILES string. In\nthis extended 256 × 512 representation, one can, in principle, select one or multiple\n512-dimensional vectors to represent the original molecule. In our work, we choose\nthe corresponding vector of the leading symbol <s> of a molecular SMILES string\nas the bidirectional transformerﬁngerprints (BT-FPs or BT\ns-FPs) of the molecule.\nIn the downstream tasks, BT-FPs or BTs-FPs are used for molecular property\nprediction. Detailed model parameters can be found in Supplementary Note 3.\nReporting summary. Further information on research design is available in the Nature\nResearch Reporting Summary linked to this article.\nData availability\nThe pre-training dataset used in this work is CheMBL26, which is available athttps://ftp.\nebi.ac.uk/pub/databases/chembl/ChEMBLdb/releases/chembl_26/. To ensure the\nreproducibility of this work, the eight datasets used in this work, including four\nquantitative toxicity datasets (LD50, IGC50, LC50, and LC50DM), partition coefﬁcient\ndataset, FreeSolv dataset, Lipophilicity dataset, and BBBP dataset, are available athttps://\nweilab.math.msu.edu/Database/.\nCode availability\nThe overall models and related code have been released as an open-source code and is\nalso available in the Github repository:https://github.com/ChenDdon/AGBTcode52.\nReceived: 21 January 2021; Accepted: 6 May 2021;\nReferences\n1. Di, L. & Kerns, E. H. Drug-Like Properties: Concepts, Structure Design and\nMethods from ADME to Toxicity Optimization(Academic Press, 2015).\n2. Wu, K. & Wei, G.-W. Quantitative toxicity prediction using topology-based\nmultitask deep neural networks.J. Chem. Inform. modeling58, 520–531\n(2018).\n3. Hansch, C., Maloney, P. P., Fujita, T. & Muir, R. M. Correlation of biological\nactivity of phenoxyacetic acids with hammett substituent constants and\npartition coefﬁcients. Nature 194, 178–180 (1962).\n4. De Cao, N. & Kipf, T. Molgan: an implicit generative model for small\nmolecular graphs, arXiv preprint arXiv:1805.11973(2018).\n5. Li, Y., Zhang, L. & Liu, Z. Multi-objective de novo drug design with\nconditional graph generative model.J. Cheminform. 10, 33 (2018a).\n6. Li, R., Wang, S., Zhu, F. and Huang, J. Adaptive graph convolutional neural\nnetworks. In Proc. of the Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence (2018b).\n7. Cang, Z. & Wei, G.-W. Topologynet: topology based deep convolutional and\nmulti-task neural networks for biomolecular property predictions.PLoS\nComput. Biol. 13, e1005690 (2017).\n8. Xu, Z., Wang, S., Zhu, F. & Huang, J. Seq2seqﬁngerprint: an unsupervised\ndeep molecular embedding for drug discovery. InProc. 8th ACM International\nConference on Bioinformatics, Computational Biology, and Health Informatics,\n285–294 (2017).\n9. Wu, Z. et al. Moleculenet: a benchmark for molecular machine learning.\nChem. Sci. 9, 513–530 (2018a).\n10. Winter, R., Montanari, F., Noé, F. & Clevert, D.-A. Learning continuous and\ndata-driven molecular descriptors by translating equivalent chemical\nrepresentations. Chem. Sci. 10, 1692–1701 (2019).\n11. Putin, E. et al. Adversarial threshold neural computer for molecular de novo\ndesign. Mol. Pharmaceutics 15, 4386–4397 (2018).\n12. Popova, M., Isayev, O. & Tropsha, A. Deep reinforcement learning for de\nnovo drug design.Sci. Adv. 4, eaap7885 (2018).\n13. Jiang, J. et al. Boosting tree-assisted multitask deep learning for small scientiﬁc\ndatasets. J. Chem. Inform. Model.60, 1235–1244 (2020).\n14. Nguyen, D. D., Cang, Z. & Wei, G.-W. A review of mathematical representations\nof biomolecular data.Phys. Chem. Chem. Phys.22, 4343–4367 (2020a).\n15. Todeschini, R. & Consonni, V.Handbook of Molecular Descriptors, volume 11\n(John Wiley, Sons, 2008).\n16. Rogers, D. & Hahn, M. Extended-connectivityﬁ\nngerprints. J. Chem. Inform.\nModel. 50, 742–754 (2010).\n17. Gao, K. et al. Are 2Dﬁngerprints still valuable for drug discovery?Phys. Chem.\nChem. Phys. 22, 8373–8390 (2020).\n18. Durant, J. L., Leland, B. A., Henry, D. R. & Nourse, J. G. Reoptimization of\nMDL keys for use in drug discovery.J. Chem. Inform. Comput Sci.42,\n1273–1280 (2002).\n19. James, C.A., Weininger, D. and Delany, J. Daylight Theory Manual. Daylight.\n(Chemical Information Systems Inc., Irvine, CA, 1995).\n20. Mason, J. S. & Cheney, D. L. Library design and virtual screening using\nmultiple 4-point pharmacophoreﬁngerprints. Biocomputing 2000, 576–587\n(1999).\n21. Verma, J., Khedkar, V. M. & Coutinho, E. C. 3d-qsar in drug design-a review.\nCurr. Top. Med. Chem.10,9 5–115 (2010).\n22. Meng, Z., Anand, D. V., Lu, Y., Wu, J. & Xia, K. Weighted persistent\nhomology for biomolecular data analysis.Sci. Rep. 10,1 –15 (2020).\n23. Nguyen, D. D. & Wei, G.-W. Dg-gl: Differential geometry-based geometric\nlearning of molecular datasets.Int. J. Numer. Methods Biomed. Eng.35, e3179\n(2019a).\n24. Nguyen, D. D. & Wei, G.-W. Agl-score: Algebraic graph learning score for\nprotein–ligand binding scoring, ranking, docking, and screening.J. Chem.\nInform. Model. 59, 3291–3304 (2019).\n25. Li, H., Sze, K.-H., Lu, G. & Ballester, P. J. Machine-learning scoring functions\nfor structure-based drug lead optimization.Wiley Interdiscip. Rev. Comput.\nMol. Sci. 10, e1465 (2020).\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-23720-w\n8 NATURE COMMUNICATIONS|         (2021) 12:3521 | https://doi.org/10.1038/s41467-021-23720-w | www.nature.com/naturecommunications\n26. Nguyen, D. D., Gao, K., Wang, M. & Wei, G.-W. Mathdl: mathematical deep\nlearning for D3R grand challenge 4.J. Comput.-Aided Mol. Des.34, 131–147\n(2020b).\n27. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of deep\nbidirectional transformers for language understanding. InProceedings of the\n2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), 4171–4186. (Association for Computational\nLinguistics, 2019).\n28. Vaswani, A. et al. Attention is all you need. Advances in Neural Information\nProcessing Systems, 5998–6008 (2017).\n29. Weininger, D. Smiles, a chemical language and information system. 1.\nintroduction to methodology and encoding rules.J. Chem. Inform. Comput.\nSci. 28,3 1–36 (1988).\n30. Wang, S., Guo, Y., Wang, Y., Sun, H. & Huang, J. Smiles-BERT: large scale\nunsupervised pre-training for molecular property prediction. InProceedings of\nthe 10th ACM International Conference on Bioinformatics, Computational\nBiology and Health Informatics, 429–436 (2019).\n31. Sterling, T. & Irwin, J. J. Zinc 15–ligand discovery for everyone.J. Chem.\nInform. Model. 55, 2324–2337 (2015).\n32. Gaulton, A. et al. The chembl database in 2017.Nucleic Acids Res.45,\nD945–D954 (2017).\n33. Blondel, A. & Karplus, M. New formulation for derivatives of torsion angles\nand improper torsion angles in molecular mechanics: elimination of\nsingularities. J. Comput. Chem.17, 1132–1141 (1996).\n34. Bruice, P. Y. Organic Chemistry: Pearson New International Edition(Pearson\nHigher Ed, 2013).\n35. Chi, Z., Liu, R., Yang, B. & Zhang, H. Toxic interaction mechanism between\noxytetracycline and bovine hemoglobin.J. Hazard. Mater.180, 741–747\n(2010).\n36. Akers, K. S., Sinks, G. D. & Schultz, T. W. Structure–toxicity relationships for\nselected halogenated aliphatic chemicals.Environ. Toxicol. Pharmacol.7,\n33–39 (1999).\n37. Zhu, H. et al. Combinatorial qsar modeling of chemical toxicants tested\nagainst tetrahymena pyriformis.J. Chem. Inform. Model.48, 766–784 (2008).\n38. Anu Grover, Manish Grover, and Komal Sharma. A practical overview of\nquantitative structure-activity relationship.World J. Pharm. Pharm. Sci. 5,\n427–437 (2016).\n39. Van De Waterbeemd, H. & Gifford, E. Admet in silico modelling: towards\nprediction paradise? Nat. Rev. Drug Discov.2, 192–204 (2003).\n40. Karim, A., Mishra, A., Newton, M. A. H. & Sattar, A. Efﬁcient toxicity\nprediction via simple features using shallow neural networks and decision\ntrees. ACS Omega 4, 1874–1888 (2019).\n41. Martin, T. et al. User’s Guide for Test (version 4.2)(Toxicity Estimation\nSoftware Tool): A Program to Estimate Toxicity from Molecular Structure.\n(Washington (USA): US-EPA, 2016).\n42. Zhu, H. et al. Quantitative structure— activity relationship modeling of rat\nacute toxicity by oral exposure.Chem. Res. Toxicol.22, 1913–1921 (2009).\n43. Wu, K., Zhao, Z., Wang, R. & Wei, G.-W. Topp–s: persistent homology-based\nmulti-task deep neural networks for simultaneous predictions of partition\ncoefﬁcient and aqueous solubility.J. Comput. Chem.39, 1444–1454 (2018b).\n44. Cheng, T. et al. Computation of octanol-water partition coefﬁcients by guiding\nan additive model with knowledge.J. Chem. Inform. Model.47, 2140–2148\n(2007).\n45. Tetko, I. V. & Bruneau, P. Application of alog ps to predict 1-octanol/water\ndistribution coefﬁcients, log p, and log d, of astrazeneca in-house database.J.\nPharm. Sci. 93, 3103–3110 (2004).\n46. Yang, K. et al. Analyzing learned molecular representations for property\nprediction. J. Chem. Inform. Model.59, 3370–3388 (2019).\n47. Shen, W. X. et al. Out-of-the-box deep learning prediction of pharmaceutical\nproperties by broadly learned knowledge-based molecular representations,\nNat. Mach. Intell.1 –10 (2021).\n48. Martins, I. F., Teixeira, A. L., Pinheiro, L. & Falcao, A. O. A bayesian approach\nto in silico blood–brain barrier penetration modeling.J. Chem. Inform. Model.\n52, 1686–1697 (2012).\n49. Opron, K., Xia, K. & Wei, G.-W. Fast and anisotropicﬂexibility-rigidity index\nfor protein ﬂexibility and ﬂuctuation analysis. J. Chem. Phys.140, 06B617_1\n(2014).\n50. Bramer, D. & Wei, G.-W. Multiscale weighted colored graphs for protein\nﬂexibility and rigidity analysis.J. Chem. Phys.148\n, 054103 (2018).\n51. Neglur, G., Grossman, R. L. & Liu, B. Assigning unique keys to chemical\ncompounds for data integration: some interesting counter examples. In\nProceedings of the International Workshop on Data Integration in the Life\nSciences, 145–157 (Springer, 2005).\n52. Chen, D. ChenDdon/AGBTcode: AGBT source code. Zenodo, May 2021.\nhttps://doi.org/10.5281/ZENODO.4732328. https://zenodo.org/record/\n4732328.\nAcknowledgements\nThework of Dong Chen, Xin Chen, Yi Jiang and Feng Pan was supported in part by the\nNational Key R&D Program of China (2016YFB0700600). The work of Kaifu Gao and\nGuo-Wei Wei was supported in part by NSF grants DMS-2052983, DMS1761320,\nIIS1900473, NIH grants GM126189, and GM129004, Bristol-Myers Squibb, and Pﬁ-\nzer. The work of Duc Nguyen was supported in part by NSF grant DMS-2053284 and\nUniversity of Kentucky start-up fund. The work of Dong Chen was also partly supported\nby Michigan State University.\nAuthor contributions\nDong Chen designed the project, performed computational studies, analyzed data, wrote\nthe ﬁrst draft, and revised the manuscript. Kaifu Gao, Xin Chen, and Yi Jiang analyzed\ndata, Duc Duy Nguyen analyzed data and revised the manuscript. Guo-Wei Wei con-\nceptualized and supervised the project, revised the manuscript, and acquired funding.\nFeng Pan conceptualized and supervised the project and acquired funding.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41467-021-23720-w.\nCorrespondence and requests for materials should be addressed to G.-W.W. or F.P.\nPeer review informationNature Communications thanks Pedro Ballester, Alex\nZhavoronkov, and Hao Zhu for their contribution to the peer review of this work. Peer\nreviewer reports are available.\nReprints and permission informationis available athttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2021\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-23720-w ARTICLE\nNATURE COMMUNICATIONS|         (2021) 12:3521 | https://doi.org/10.1038/s41467-021-23720-w | www.nature.com/naturecommunications 9",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7165642976760864
    },
    {
      "name": "Molecular graph",
      "score": 0.7008236050605774
    },
    {
      "name": "Transformer",
      "score": 0.5951421856880188
    },
    {
      "name": "Graph",
      "score": 0.5609160661697388
    },
    {
      "name": "Algebraic number",
      "score": 0.5562352538108826
    },
    {
      "name": "Encoder",
      "score": 0.5036181807518005
    },
    {
      "name": "Deep learning",
      "score": 0.46778973937034607
    },
    {
      "name": "Machine learning",
      "score": 0.46369728446006775
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4588589668273926
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45163872838020325
    },
    {
      "name": "Mathematics",
      "score": 0.15154072642326355
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I87216513",
      "name": "Michigan State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I143302722",
      "name": "University of Kentucky",
      "country": "US"
    }
  ],
  "cited_by": 157
}