{
  "title": "Intracerebral hemorrhage CT scan image segmentation with HarDNet based transformer",
  "url": "https://openalex.org/W4367857116",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2555316191",
      "name": "Zhegao Piao",
      "affiliations": [
        "Sejong University"
      ]
    },
    {
      "id": "https://openalex.org/A2223208628",
      "name": "Yeong Hyeon Gu",
      "affiliations": [
        "Sejong University"
      ]
    },
    {
      "id": "https://openalex.org/A2139630916",
      "name": "Hailin Jin",
      "affiliations": [
        "Sejong University"
      ]
    },
    {
      "id": "https://openalex.org/A2127025874",
      "name": "Seong Joon Yoo",
      "affiliations": [
        "Sejong University"
      ]
    },
    {
      "id": "https://openalex.org/A2555316191",
      "name": "Zhegao Piao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2223208628",
      "name": "Yeong Hyeon Gu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2139630916",
      "name": "Hailin Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127025874",
      "name": "Seong Joon Yoo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2592374523",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2996290406",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2987175876",
    "https://openalex.org/W2464708700",
    "https://openalex.org/W3102335403",
    "https://openalex.org/W3113958627",
    "https://openalex.org/W2928165649",
    "https://openalex.org/W3153303671",
    "https://openalex.org/W3138895753",
    "https://openalex.org/W3153621404",
    "https://openalex.org/W1794121648",
    "https://openalex.org/W2962807789",
    "https://openalex.org/W3203841574",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W3158094492"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2023) 13:7208  | https://doi.org/10.1038/s41598-023-33775-y\nwww.nature.com/scientificreports\nIntracerebral hemorrhage CT scan \nimage segmentation with HarDNet \nbased transformer\nZhegao Piao , Yeong Hyeon Gu *, Hailin Jin  & Seong Joon Yoo *\nAlthough previous studies conducted on the segmentation of hemorrhage images were based on the \nU-Net model, which comprises an encoder-decoder architecture, these models exhibit low parameter \npassing efficiency between the encoder and decoder, large model size, and slow speed. Therefore, to \novercome these drawbacks, this study proposes TransHarDNet, an image segmentation model for the \ndiagnosis of intracerebral hemorrhage in CT scan images of the brain. In this model, the HarDNet block \nis applied to the U-Net architecture, and the encoder and decoder are connected using a transformer \nblock. As a result, the network complexity was reduced and the inference speed improved while \nmaintaining the high performance compared to conventional models. Furthermore, the superiority \nof the proposed model was verified by using 82,636 CT scan images showing five different types of \nhemorrhages to train and test the model. Experimental results showed that the proposed model \nexhibited a Dice coefficient and IoU of 0.712 and 0.597, respectively, in a test set comprising 1200 \nimages of hemorrhage, indicating better performance compared to typical segmentation models such \nas U-Net, U-Net++, SegNet, PSPNet, and HarDNet. Moreover, the inference time was 30.78 frames \nper second (FPS), which was faster than all en-coder-decoder-based models except HarDNet.\nIntracerebral hemorrhage (ICH) is the condition caused by bleeding in the ventricles of the brain when blood \nvessels rupture spontaneously due to reasons other than external injury. ICH occurs primarily in middle-aged \nadults and is the sub stay of stroke, exhibiting the second highest occurrence rate after ischemic  stroke1 owing to \nthe high incidence, mortality, and disability rates. ICH can be categorized into five types based on the bleeding \nlocation within the brain: epidural hemorrhage (EDH), subdural hemorrhage (SDH), subarachnoid hemor -\nrhage (SAH), intraventricular hemorrhage (IVH), and intraparenchymal hemorrhage (IPH). Given that ICH \nhas become a life threatening disease and causes a burden on the families of those suffering from the disease, it \nis essential to develop accurate and rapid diagnosis and treatment methods for ICH.\nA computed tomography (CT) scan is a fast diagnostic imaging technique having good resolution used for \naccurately determining the location of hematoma, amount of bleeding, the mass effect, presence or absence \nof bleeding in the ventricles, and the amount of damage to the subarachnoid and surrounding brain tissues. \nTherefore, it is considered ideal for the diagnosis and treatment of  ICH2. Generally, experts first confirm the \npresence of hemorrhage through CT scans followed by detecting the type and location of the bleeding. However, \na diagnosis as such requires extensive time from a radiology specialist for the examination, especially when it \nentails the possibility of a missed diagnosis.\nMedical image segmentation is the process of identifying areas affected by the disease using medical diag-\nnosis technologies such as computed tomography (CT) or magnetic resonance imaging (MRI). While existing \ndeep learning-based ICH image segmentation (hereinafter referred to as “ICH segmentation”) methods using \nthe U-shaped encoder-decoder architecture acquired adequate results, two problems still persisted. First, these \nnetworks require much time for inference and training owing to a large number of parameters. The inference \ntime increases for high resolution input images. Second, when low-resolution features extracted from the encoder \nare transformed into high-resolution features in the decoder, it results in the significant loss of sensitivity to the \nsensitivity of the final segmentation.\nBecause ICH must be definitively diagnosed and treated within 1h of its occurrence, the speed of the diagnosis \nmodel is critical when diagnosing ICH, in addition to the performance. Therefore, this study proposes a Tran-\nsHarDNet ICH segmentation network to overcome such drawbacks for the effective diagnosis and treatment of \nICH. TransHarDNet comprises a U-shaped encoder-decoder architecture and has the following characteristics: \nThe existing convolution calculation is replaced with a transformer block with a self-attention mechanism for \nOPEN\nDepartment of Computer Science and Engineering, Sejong University, Seoul, South Korea. *email: yhgu@sejong.\nac.kr; sjyoo@sejong.ac.kr\n2\nVol:.(1234567890)Scientific Reports |         (2023) 13:7208  | https://doi.org/10.1038/s41598-023-33775-y\nwww.nature.com/scientificreports/\nthe effective exchange of information between the encoder and the  decoder3. Long-distance dependency can \nbe modeled, and global information is analyzed to extract various context features and produce more detailed \nsegmentation results.\nIn this study, we used 82,636 CT scan images of ICH as datasets from five different institutions, including \nthe Catholic University of Korea Seoul St. Mary’s Hospital. Furthermore, we compared the inference speed \nand segmentation performance of the TransHarDNet model with that of other segmentation models, such as \nthe U-Net4, U-Net++5,  SegNet6,  PSPNet7, and  HarDNet8. Experimental results showed that the TransHarDNet \nmodel exhibited an inference speed of 30.78 FPS, IoU of 0.597, and a Dice coefficient of 0.712, which makes it \nsuperior to other conventional models.\nRelated works\nIn an ICH image analysis, segmentation accurately detects the bleeding location amount in the initial step of \nidentifying the occurrence of bleeding, which is why it has more clinical applicability than classification. Seg-\nmentation techniques are also used to analyze medical diagnostic images except that of ICH. The most frequently \nused segmentation models include those having an encoder-decoder architecture that has been transformed \nbased on U-Net.\nU-Net4, a segmentation model proposed in 2015, uses a symmetric encoder-decoder architecture with a \nskip connection, and exhibits outstanding performance in the seg-mentation of medical images by converging \nmultiscale features. Other U-Net shaped models based on U-Net, such as U-Net++5, 3D U-Net9, and Attention \nU-Net10, have been widely used owing to their excellent performance in the analysis of medical images. Fur -\nthermore, models such as  SegNet9 and  PSPNet10 having an encoder-decoder architecture have also been widely \nadopted. Zhang et al.11 proposed a technology that used a generator net to generate an ICH image, which was \nfurther synthesized along with a normal ICH image having an insufficient amount of training data using the \nU-Net-based network. Results showed that the performance could be improved if the ICH detection model was \ntrained on the synthesized and actual data simultaneously. This however was a new case wherein U-Net was \napplied to a medical image synthesis in addition to segmentation. Kushnure and  Talbar12 conducted a study and \naccurately extracted the global and local feature information from CT scan images by replacing the CNN block \nof the U-Net and combining  Res2Net13 and a squeeze-and-excitation (SE) network. Abramova et al.14 proposed \na segmentation model using 3D U-Net, where the SE network was applied to U-Net. Y ou et al.15 proposed a 3D \nDissimilar-Siamese-U-Net comprising two U-Nets connected to the encoder by a distance block. The brain CT \nscan images were analyzed in the 3D Dissimilar-Siamese-U-Net by receiving two inputs: left and right. Mizusawa \net al.16 conducted a study wherein U-Net was applied for the reconstruction of an X-ray image.\nRecurrent neural networks (RNN), a model architecture for processing sequence data, have been widely used \nin natural language processing (NLP). Because CT scan or MRI images are established as continuous slices in \nmedical image analyses they can be analyzed using RNNs. Stollenga et al.17 conducted a study for the segmenta-\ntion of brain MRI images using the 3D PyraMiDLSTM model. The network was constructed to enable GPU-based \nparallel processing to significantly improve the efficiency of model training, which produced good segmentation \nresults in the MRBrainS  challenge18. Koutnìk et al.19 constructed a spatial clockwork recurrent neural network \n(CW-RNN) using fewer parameters than RNNs for the segmentation of muscular disease images. As a result, \nthe average accuracy of CW-RNN was 5% higher than that of U-Net, and the execution speed was 100 times \nshorter than that of the CNN models. Poudel et al. 20 constructed recurrent fully convolutional networks based \non FCN and RNNs for the real-time computing of heart segmentation.\nChen et al.21 performed CT image segmentation using TransUNet, developed by combining U-Net with 12 \ntransformer layers and obtained outstanding results. Wang et al. 22 built a 3D MRI brain tumor segmentation \nmodel with a transformer architecture based on U-Net. Chen et al. 21 and Wang et al.22 proved that the overall \nperformance of the segmentation model can be improved by combining a CNN model having an encoder-\ndecoder architecture with a transformer used for the analysis of sequence data. However, Wang et al. 22 used a \n3D CNN layer with a large number of parameters and exhibited a slow processing time considering the existing \nU-Net model architecture was applied.\nDataset. In this study, we used 82,636 CT scan images of ICH as datasets, collected from the Catholic Uni-\nversity of Korea Seoul St. Mary’s Hospital, Chung-Ang University, Inje University, Inje University Pusan Paik \nHospital, and Konkuk University Medical Center(The dataset published on  AIHub23). For the data, experts man-\nually found the disease area and marked the ground truth. All images were high-resolution (512 ± 512) and were \ncategorized as EDH, IPH, IVH, SAH, or SDH depending on the location of bleeding. Figure 1a–f show examples \nof the CT scan images from each category, that is, intraparenchymal hemorrhage (IPH), intraventricular hemor-\nrhage (IVH), subarachnoid hemorrhage (SAH), subdural hemorrhage (SDH), epidural hemorrhage (EDH), and \nat least one type of hemorrhage (multiple), respectively.\nThe 200 ICH images were selected from the EDH, IPH, IVH, SAH, SDH, and multiple categories to acquire \n1200 images for the test data. The remaining 81,436 images were divided in a ratio of 8:2 at the unit of disease \ncategory to form training and validation datasets comprising 65,151 and 16,285 images, respectively. Table  1 \nshows the statistics of the data used for the training and testing of a model in each category.\nThe proposed model: TransHarDNet\nThis study proposed the TransHarDNet segmentation model to accurately and quickly generate segmentation \nresults for the CT scan images of ICH. Figure  5 shows a schematic of TransHarDNet. The model comprises an \nencoder-decoder-based U-Net architecture. HarDNet was used as the backbone of the encoder-decoder owing \n3\nVol.:(0123456789)Scientific Reports |         (2023) 13:7208  | https://doi.org/10.1038/s41598-023-33775-y\nwww.nature.com/scientificreports/\nto its light-weight architecture. The simple convolution calculation was replaced with a transformer block that \nconnected the encoder and the decoder. Table 2 is the details of the model architecture.\nHarDNet block. HarDNet is a densely connected network architecture built to maintain high accuracy \nwhile reducing memory usage. Compared to methods such as DenseNet block or ResNet block, HarDNet block \ncan shorten the inference time by approximately 30% at a similar performance level in applications such as image \nclassification, object detection, and image  segmentation8.\nHarDNet comprises harmonic dense blocks (HDBs), which are connected when the k-th layer is connected \nto the k − 2n-th layer, when k − 2n is greater than 0 and 2n\nk  is a natural number, as seen in ( 1). k is the location \nof a layer in the HDB, n  is the layer connected to k in the HDB, and N  is a natural number. And Fig.  2 is an \nillustration of HarDNet.\nIn HarDNet, HDBs are connected by a depth wise-separable convolution layer (DWConv), which reduces \nthe convolutional input/output (CIO) by 50% when compared to the 1 × 1 convolution layer. Therefore, a 2 × 2 \naverage pooling layer is used in DenseNet[3]. Figure 3 is a comparison of the transition layers of DenseNet and \nHarDNet.\n(1)C k = k − 2n ,if2n\nk ∈N ,k − 2n ≥ 0\nFigure 1.  Data examples; (a) example of intraparenchymal hemorrhage (IPH); (b) example of intraventricular \nhemorrhage (IVH); (c) example of subarachnoid hemorrhage (SAH); (d) example of subdural hemorrhage \n(SDH); (e) example of epidural hemorrhage (EDH); (f) CT images with one or more cerebral hemorrhagic \nlesions (the image include IPH, SDH, and EDH).\nTable 1.  Our ICH dataset.\nEDH IVH SDH SAH IPH Multiple Sum\nTrain 2286 5352 27,413 13,421 17,605 15,359 81,436\nTest 200 200 200 200 200 200 1200\nSum 2486 5552 27,613 13,621 17,805 15,559 82,636\n4\nVol:.(1234567890)Scientific Reports |         (2023) 13:7208  | https://doi.org/10.1038/s41598-023-33775-y\nwww.nature.com/scientificreports/\nTable 2.  Model architecture.\nStage Block name Details Output size\nInput – – 512 × 512 × 1\nEncoder\nConv block 4 × convolution 128 × 128 × 48\nHarDNet block 4 × convolution 128 × 128 × 48\nDown sampling block Convolution, AvgPool2d 64 × 64 × 64\nHarDNet block 4 × convolution 64 × 64 × 78\nDown sampling block Convolution, AvgPool2d 32 × 32 × 96\nHarDNet block 8 × convolution 32 × 32 × 160\nDown sampling block Convolution, AvgPool2d 16 × 16 × 160\nHarDNet block 8 × convolution 16 × 16 × 214\nDown sampling block Convolution, AvgPool2d 8 × 8 × 224\nHarDNet block 8 × convolution 8 × 8 × 286\nTransformer (bottle neck)\n– 1 × convolution 8 × 8 × 320\nLinear projection Reshape 512 × 64\nTransformer block 4 × transformer layer 512 × 64\n– 1 × convolution 8 × 8 × 512\n– 1 × convolution 8 × 8 × 320\nDecoder\nUp sampling block Upsample, convolution 16 × 16 × 320\nHarDNet block 8 × convolution 16 × 16 × 214\nUp sampling block Upsample, convolution 32 × 32 × 214\nHarDNet block 8 × convolution 32 × 32 × 160\nUp sampling block Upsample, convolution 64 × 64 × 160\nHarDNet block 4 × convolution 64 × 64 × 78\nUp sampling block Upsample, convolution 128 × 128 × 78\nHarDNet block 4 × convolution 128 × 128 × 48\nUp sampling block Upsample, convolution 512 × 512 × 6\nOutput Conv block 1 × convolution 512 × 512 × 6\nFigure 2.  Example of HarDNet connections.\nFigure 3.  Comparison of the transition layers of DenseNet and HarDNet.\n5\nVol.:(0123456789)Scientific Reports |         (2023) 13:7208  | https://doi.org/10.1038/s41598-023-33775-y\nwww.nature.com/scientificreports/\nTransformer block. In the transformer, the sequence data processing method used in NLP analysis was suc-\ncessfully applied to computer vision. Currently, Owing to their outstanding performance, transformers are gain-\ning wide attention in computer vision for applications such as  detection24,  segmentation25, and  classification26. \nIn this study, the existing CNN connection between the U-NET encoder and decoder was replaced with a seg-\nmentation transformer (SETR). The SETR architecture is shown in Fig. 4.\nA feature map extracted from the encoder using HarDNet as a backbone was input to the transformer. The \nfeature map is transformed into sequence data in the linear projection block using the position-embedding \ntechnique. The sequence data comprising information on the location were first normalized through layer nor-\nmalization (LN), which resolved the internal covariate shift (ICS) occurring during the training of small batches. \nFurthermore, the normalized data is input to a multi-head attention block to extract various features by inferring \nthe relationship between the location information in the sequence data.\nThe output of the multi-head attention block and the first sequence data delivered through the skip connection \nare combined and passed through the LN and feed-forward network (FFN). The FFN comprises two activation \nfunctions: the first layer is the ReLU activation function, and the second layer is a linear activation function that \nfacilitates inference. One transformer layer was configured as such. In this study, we constructed a module with \nfour transformer layers, and the feature map passing through these layers is decoded to the same size as the input.\nModel architecture. TransHarDNet comprises an encoder, a decoder, and a bottleneck layer.\nThe encoder extracts the feature map and reduces the image size through down sampling. The encoder com-\nprises a convolution block and the HarDNet block. W , H, and C represent the width, height, and channel of the \npreprocessed image (W , H, C). The shape of the feature map inferred with a convolution block was W/4, H/4, \nC*48. The convolution block consists of a convolution layer where filter = 16, kernel size = 3, stride = 2, a convolu-\ntion layer where filter = 24, kernel size = 3, stride = 1, a convolution layer where filter = 32, kernel size = 3, stride \n= 2, and a convolution layer where filter = 48, kernel size = 3, and stride = 1. A down-sampling block consists of \na convolution layer with kernel size = 1 and an AvgPoll2d layer with kernel size = 2 and stride = 2. Subsequently, \nthe feature map undergoes down sampling through the HarDNet block and results in W/32, H/32, C*320.\nThe transformation section extracts valid information from the feature map and delivers it to the decoder. \nThe feature map is encoded into sequence data through a linear projection layer and passed through four trans-\nformer layers. The sequence data is decoded by two convolution layers where kernel size = 1, stride = 1 again to \nthe dimensions of W/32, H/32, C*320 and delivered to the TransHarDNet decoder.\nThe feature map from the transformer block is up-sampled by the decoder to the same size as the Tran-\nsHarDNet input, while the ICH region of the feature map is marked in the output image. The decoder outputs \nthe final (W , H) image size by passing through four HarDNet blocks, five up-sampling blocks, and the last \nconvolution layer with kernel size = 1. An up-sampling block consists of an interpolate function that uses the \n“bilinear” mode and a convolution layer with kernel size = 1. Figure  5 and Table 2 are detailed descriptions of \nthe HarDNet structure.\nExperimentations\nPerformance evaluation indicators. We evaluated the performance of the model based on four indica-\ntors commonly used to evaluate the performance of a model in medical image segmentation: the Dice similarity \ncoefficient (DSC), intersection over union (IoU), Jaccard index, precision, and recall. IoU is calculated as the \nratio of the intersection value of the predicted and actual values to the union value and is used for object detec-\ntion and semantic segmentation. The Dice coefficient, IoU, precision, and recall can be inferred using true posi-\ntive (TP), true negative (TN), false positive (FP), and false negative (FN) indicators of a confusion matrix. The \nDice coefficient, IoU, precision, and recall can be calculated as shown in (2)–(5).\nFigure 4.  Architecture of transformer.\n6\nVol:.(1234567890)Scientific Reports |         (2023) 13:7208  | https://doi.org/10.1038/s41598-023-33775-y\nwww.nature.com/scientificreports/\nExperimental environment and parameter setting. Table 3 presents the experimental environment \nused for this study. The input image size for all models was 515 ± 512 for training, and the batch size was 8. Adap-\ntive moment estimation (Adam) was used as the optimization algorithm for training the model, and the initial \nlearning rate was set to 0.01. The learning rate was reduced by 0.5 when the training loss value did not decrease \nfor five epochs, and early stopping was applied when the value did not decrease for 10 epochs.\nSelection of a loss function. We conducted an experiment to determine an appropriate loss function for \nthe model by combining the Dice loss, cross entropy (CE), and focal  loss27.\nThe Dice loss, which stems from the Dice coefficient, was first proposed in a study by Milletari et al.28 and is \nwidely used in medical image segmentation. In this study, the Dice loss was used to indicate similarities between \nthe two samples. The Dice loss value ranges between 0 and 1, and a smaller value indicates a higher level of \nsimilarity between the two samples. It can be calculated using (6):\n(2)Dice= 2TP\n(2TP + FP + FN )\n(3)IoU = TP\n(TP + FP + FN )\n(4)Precision= TP\n(TP + FP )\n(5)Recall= TP\n(TP + FN )\n(6)LDice(X ,Y ) = 1 − 2 |X⋂Y |\n|X|+| Y |\nFigure 5.  Overall architecture of the proposed TransHarDNet.\nTable 3.  Experimental environment.\nDevice Specifications\nOS Windows 10\nCPU Intel Core i9-9900KF 3.6 GHz\nGPU NVIDIA GeForce RTX 2080Ti * 1\nRAM (memory) 96 GB\nStorage 1TB SSD + 4TB HDD\nLanguage Python 3.7, PyTorch = 1.5\n7\nVol.:(0123456789)Scientific Reports |         (2023) 13:7208  | https://doi.org/10.1038/s41598-023-33775-y\nwww.nature.com/scientificreports/\nThe concept of CE originated from information theory, an expanded concept of binary cross entropy frequently \nused in multinomial classification. As seen from (7 ), the CE loss function infers the difference in the quantity \nof information between the predicted and actual values of the sample, where M is the number of categories, yic \nis the dummy variable having a value of 1 with identical predicted and actual values, and 0 otherwise, and pic is \nthe probability of category c for input i.\nFocal loss is a loss function first used for object detection, and since, has been used to solve category imbalance \nissues and differences in category difficulty of classification problems. As shown in (8 ), the focal loss adds a \nmodulating factor based on weight cross entropy (WCE) to reduce the weight of samples that can be classified \neasily during training to focus on the samples difficult to classify.\nThe losses in (9 ) and (10), referred to as DiceCE and DiceFocal, respectively, were combined to perform the \nexperiment in this study. Two loss functions were applied to the model for training, and the performance was \nmeasured using the test dataset. The results are provided in Table  4. Compared to DiceFocal, the model with \nthe DiceCE loss function applied produced improved results for all four performance indicators. Therefore, the \nDiceCE loss function was used in this study.\nTwo loss functions were applied to TransHarDNet for training, and the performance of the model was meas-\nured using the test dataset. The results are provided in Table  4. With an average Dice coefficient of 0.712, IoU \nof 0.597, precision of 0.777, and recall of 0.708, TransHarDNet exhibited better performance when DiceCE was \napplied compared to when DiceFocal was applied. Furthermore, DiceCE produced better results for each ICH \ncategory compared to DiceFocal. Therefore, the DiceCE loss function was used in TransHarDNet for the fol-\nlowing experiments.\nComparative analysis for the model performance. We conducted a comparative analysis by measur-\ning the segmentation performance for the TransHarDNet model proposed in this study and the conventional \nsegmentation models such as U-Net4, U-Net++5,  SegNet18,  PSPNet19, and  HarDNet8 to verify the effectiveness \nof the proposed model. Furthermore, we used identical hyper-parameters and DiceCE loss function to ensure \nconsistency in the training process.\nThe experimental results are listed in Table  5. The proposed TransHarDNet exhibited better performance \nthan the four conventional semantic methods, with Dice coefficients, IoU, and HD95 of 0.712, 0.597, and 27.733, \nrespectively. Furthermore, the TransHarDNet, wherein a transformer module was introduced to HarDNet, \nimproved the model accuracy by 1.6% compared to HarDNet alone by applying simple convolution calculation.\nFigure 6 shows the prediction results, which intuitively represent the results of the semantic segmentation \nmethods used in the experiment. The results showed that the TransHarDNet can segment the bleeding location \nmore accurately compared to other segmentation methods.\nComparative analysis for the model speed. Owing to a large number of parameters, existing segmen-\ntation models have limitations in terms of the complicated model architecture and slow inference speed. Addi-\ntionally, while most segmentation analysis models require a 3-channel RGB image as input, the brain CT scan \nimages are grayscale and exhibit a simple image type. Therefore, using models with complicated architecture \nand a large number of parameters to acquire brain CT scan images could reduce model efficiency and result in \noverfitting.\n(7)LCE = 1\nN\nm∑\nc=1\nyiclog(pic)\n(8)L F =− (1 − pt)rlog(pt)\n(9)L1 =LDice + LCE\n(10)L2 =LDice + LFocal\nTable 4.  Comparison of performance by category of the proposed models.\nDice IoU Precision Recall\nDiceCE DiceFocal DiceCE DiceFocal DiceCE DiceFocal DiceCE DiceFocal\nEDH 0.777 0.709 0.681 0.614 0.809 0.786 0.772 0.684\nIPH 0.809 0.770 0.714 0.676 0.845 0.832 0.821 0.752\nIVH 0.742 0.675 0.625 0.566 0.810 0.761 0.734 0.656\nSAH 0.545 0.471 0.414 0.353 0.643 0.615 0.554 0.454\nSDH 0.709 0.618 0.591 0.505 0.766 0.742 0.712 0.586\nMulticategory 0.686 0.657 0.557 0.528 0.783 0.785 0.653 0.609\nAverage 0.712 0.650 0.597 0.540 0.777 0.754 0.708 0.623\n8\nVol:.(1234567890)Scientific Reports |         (2023) 13:7208  | https://doi.org/10.1038/s41598-023-33775-y\nwww.nature.com/scientificreports/\nThe model proposed in this study possessed the characteristics of HarDNet, which enables fast and accurate \nICH segmentation. The total inference time, FPS, and the number of parameters for each semantic segmentation \nnetwork were identified using the test dataset comprising 1200 images. As shown in Table 6, the inference time \nof TransHarDNet is 30.78, which is faster than most encoder-decoder-based segmentation networks, except \nfor HarDNet. Furthermore, the inference speed improved by 44.64% compared to PSPNet, which is the second \nmost outstanding segmentation model in terms of performance. With respect to the model size, TransHarDNet \nis lighter compared to other semantic segmentation models, except for HarDNet.\nTable 5.  Comparative analysis for the models.\nModel Dice IoU HD95\nU-Net 0.684 0.569 30.693\nU-Net++ 0.676 0.561 32.005\nSegNet 0.588 0.480 33.391\nPSPNet 0.709 0.593 27.886\nHarDNet 0.708 0.591 28.609\nSwinTransformer 0.710 0.593 28.614\nTransUNet 0.651 0.532 38.253\nTransHarDNet (our) 0.712 0.597 27.733\nFigure 6.  Example of segmentation results.\nTable 6.  Comparative analysis of the models speed.\nModel Inference times (s) FPS Model size (MB)\nU-Net 49.0 24.49/s 69.07\nU-Net++ 51.5 23.30/s 36.65\nSegNet 46.9 25.58/s 117.78\nPSPNet 56.4 21.28/s 186.83\nHarDNet 38.6 31.09/s 16.47\nSwinTransformer 158.8 7.56/s 19.53\nTransUNet 58.2 20.62/s 207.21\nTransHarDNet (our) 39.0 30.78/s 108.09\n9\nVol.:(0123456789)Scientific Reports |         (2023) 13:7208  | https://doi.org/10.1038/s41598-023-33775-y\nwww.nature.com/scientificreports/\nDiscussion. In this study, we focused on improving the performance of ICH segmentation by connecting \nthe HarDNet block and the transformer block. Also, the proposed model showed good performance in many \ncategories except SAH in ICH segmentation. When SAH and multi-class are excluded, the proposed model \nexhibits more desirable performance with Dice coefficient of 0.759, IoU of 0.653, precision of 0.808, and recall of \n0.760. But in this study, the reason for the low Dice and IoU performance in SAH was not confirmed. This will \nbe addressed in future research.\nResults\nIn this study, we proposed a TransHarDNet model with a U-Net-based encoder-decoder architecture for the \nsegmentation of ICH regions in the CT scan images of the brain. The conventional CNN block between the \nencoder and decoder was replaced with the HarDNet backbone. Furthermore, the part between the encoder and \ndecoder connected through CNN calculation was replaced by a transformer block. By combining the HarDNet \nand transformer blocks, the TransHarDNet network complexity was reduced, which improved the inference \nspeed while maintaining the high performance of the model.\nThrough the self-attention mechanism of the transformer, the proposed model can effectively analyze and \nmodel the feature map by learning the context in high-level semantics, thereby overcoming the drawbacks of \nextensive calculation and insufficient understanding of the context existing in conventional methods.\nWe used 82,636 CT scan images of five different types of ICH provided by the Catholic University of Korea \nSeoul St. Mary’s Hospital to verify the proposed model. Compared to conventional segmentation models such as \nU-Net, U-Net++, SegNet, PSPNet, and HarDNet, the TransHarDNet exhibited the best performance in all per-\nformance evaluation indicators with a Dice coefficient, IoU, and HD95 of 0.712, 0.597, and 27.733, respectively.\nMoreover, the TransHarDNet has fewer parameters and maintains a high speed when using a HarDNet block. \nThe inference speed of TransHarDNet was calculated as 30.78 FPS, which was 25.68% faster than U-Net, and \nthe performance improved by 3%. Although the inference speed was 1.0% slower than that of the conventional \nHarDNet, the segmentation performance improved by 2%. Based on the acquired results, the effectiveness of \nthe proposed TransHarDNet model proposed has been sufficiently proven.\nData availibility\nThe datasets analysed during the current study are available in the  [AIHub23] repository, [https:// aihub. or. kr/ \naidata/ 34101], or available from the corresponding author on reasonable request.\nReceived: 1 September 2022; Accepted: 18 April 2023\nReferences\n 1. Yang, K. et al. The presence of previous cerebral microbleeds has a negative effect on hypertensive intracerebral hemorrhage \nrecovery. Front. Aging Neurosci. 9, 49 (2017).\n 2. Bahdanau, D., Cho, K. & Bengio, Y . Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:  \n1409. 0473 (2014).\n 3. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. Syst. 30 (2017).\n 4. Ronneberger, O., Fischer, P . & Brox, T. U-net: Convolutional networks for biomedical image segmentation. In International Confer-\nence on Medical image computing and computer-assisted intervention, 234–241 (Springer, 2015).\n 5. Zhou, Z., Siddiquee, M. M. R., Tajbakhsh, N. & Liang, J. Unet++: Redesigning skip connections to exploit multiscale features in \nimage segmentation. IEEE Trans. Med. Imaging 39, 1856–1867 (2019).\n 6. Badrinarayanan, V ., Kendall, A. & Cipolla, R. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. \nIEEE Trans. Pattern Anal. Mach. Intell. 39, 2481–2495 (2017).\n 7. Zhao, H., Shi, J., Qi, X., Wang, X. & Jia, J. Pyramid scene parsing network. In Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition, 2881–2890 (2017).\n 8. Chao, P ., Kao, C.-Y ., Ruan, Y .-S., Huang, C.-H. & Lin, Y .-L. Hardnet: A low memory traffic network. In Proceedings of the IEEE/\nCVF International Conference on Computer Vision, 3552–3561 (2019).\n 9. Çiçek, Ö., Abdulkadir, A., Lienkamp, S. S., Brox, T. & Ronneberger, O. 3d u-net: learning dense volumetric segmentation from \nsparse annotation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, 424–432 (Springer, \n2016).\n 10. Oktay, O. et al. Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv: 1804. 03999 (2018).\n 11. Zhang, H. et al. Intra-domain task-adaptive transfer learning to determine acute ischemic stroke onset time. Comput. Med. Imaging \nGraph. 90, 101926 (2021).\n 12. Xu, G., Cao, H., Udupa, J. K., Tong, Y . & Torigian, D. A. DiSegNet: A deep dilated convolutional encoder-decoder architecture for \nlymph node segmentation on PET/CT images. Comput. Med. Imaging Graph. 88, 101851 (2021).\n 13. Gao, S.-H. et al. Res2net: A new multi-scale backbone architecture. IEEE Trans. Pattern Anal. Mach. Intell. 43, 652–662 (2019).\n 14. Abramova, V . et al. Hemorrhagic stroke lesion segmentation using a 3d u-net with squeeze-and-excitation blocks. Comput. Med. \nImaging Graph. 90, 101908 (2021).\n 15. Y ou, J. et al. 3D dissimilar-siamese-u-net for hyperdense middle cerebral artery sign segmentation. Comput. Med. Imaging Graph. \n90, 101898 (2021).\n 16. Mizusawa, S., Sei, Y ., Orihara, R. & Ohsuga, A. Computed tomography image reconstruction using stacked u-net. Comput. Med. \nImaging Graph. 90, 101920 (2021).\n 17. Stollenga, M. F ., Byeon, W ., Liwicki, M. & Schmidhuber, J. Parallel multi-dimensional lstm, with application to fast biomedical \nvolumetric image segmentation. Adv. Neural Inf. Process. Syst. 28 (2015).\n 18. Mendrik, A. M. et al. MRBrainS challenge: Online evaluation framework for brain image segmentation in 3T MRI scans. Comput. \nIntell. Neurosci. 2015 (2015).\n 19. Koutnik, J., Greff, K., Gomez, F . & Schmidhuber, J. A clockwork rnn. In International Conference on Machine Learning, 1863–1871 \n(PMLR, 2014).\n 20. Poudel, R. P ., Lamata, P . & Montana, G. Recurrent fully convolutional neural networks for multi-slice mri cardiac segmentation. \nIn Reconstruction, Segmentation, and Analysis of Medical Images, 83–94 (Springer, 2016).\n10\nVol:.(1234567890)Scientific Reports |         (2023) 13:7208  | https://doi.org/10.1038/s41598-023-33775-y\nwww.nature.com/scientificreports/\n 21. Chen, J. et al. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv: 2102. 04306 \n(2021).\n 22. Wang, W . et al. Transbts: Multimodal brain tumor segmentation using transformer. In International Conference on Medical Image \nComputing and Computer-Assisted Intervention, 109–119 (Springer, 2021).\n 23. AIHub. Dataset provider site. https:// aihub. or. kr/ aidata/ 34101 (2021) (Accessed 10 Aug 2021).\n 24. Carion, N. et al. End-to-end object detection with transformers. In European Conference on Computer Vision, 213–229 (Springer, \n2020).\n 25. Zheng, S. et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proceedings of \nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6881–6890 (2021).\n 26. Dosovitskiy, A. et al. An image is worth 16×16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010.  \n11929 (2020).\n 27. Lin, T.-Y ., Goyal, P ., Girshick, R., He, K. & Dollár, P . Focal loss for dense object detection. In Proceedings of the IEEE International \nConference on Computer Vision, 2980–2988 (2017).\n 28. Milletari, F ., Navab, N. & Ahmadi, S.-A. V-net: Fully convolutional neural networks for volumetric medical image segmentation. \nIn 2016 Fourth International Conference on 3D Vision (3DV), 565–571 (IEEE, 2016).\nAcknowledgements\nThis work was supported by the Institute of Information & Communications Technology Planning & Evalua-\ntion (IITP) grant funded by the Korean government (MSIT) (No. 2021-0-00755/20210007550012002, Dark data \nanalysis technology for data scale and accuracy improvement).\nAuthor contributions\nConceptualization, Z.P . and H.J.; methodology, Z.P . and H.J.; software, Z.P . and H.J.; validation, Z.P ., Y .G. and \nS.J.Y .; formal analysis, Y .G. and S.J.Y .; investigation, Y .G.; resources, Y .G.; data curation, Z.P .; writing-original \ndraft preparation, Z.P . and H.J.; writing-review and editing, Z.P . and Y .G.; visualization, Z.P .; supervision, S.J.Y .; \nproject administration, Y .G.; funding acquisition, Y .G. All authors have read and agreed to the published version \nof the manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to Y .H.G. or S.J.Y .\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Intracerebral hemorrhage",
  "concepts": [
    {
      "name": "Intracerebral hemorrhage",
      "score": 0.6316590905189514
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5216922163963318
    },
    {
      "name": "Computed tomography",
      "score": 0.495079904794693
    },
    {
      "name": "Computer science",
      "score": 0.43868327140808105
    },
    {
      "name": "Segmentation",
      "score": 0.43188416957855225
    },
    {
      "name": "Medicine",
      "score": 0.4233779311180115
    },
    {
      "name": "Computer vision",
      "score": 0.38995665311813354
    },
    {
      "name": "Radiology",
      "score": 0.38252928853034973
    },
    {
      "name": "Subarachnoid hemorrhage",
      "score": 0.2012740969657898
    },
    {
      "name": "Surgery",
      "score": 0.16089385747909546
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I28777354",
      "name": "Sejong University",
      "country": "KR"
    }
  ],
  "cited_by": 17
}