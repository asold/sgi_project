{
  "title": "Revolutionizing Education: The Era of Large Language Models",
  "url": "https://openalex.org/W4394824858",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5032417058",
      "name": "Ho-Woong Choi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5095695700",
      "name": "Sardor Abdirayimov",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4362472309",
    "https://openalex.org/W4381546750",
    "https://openalex.org/W4317910584",
    "https://openalex.org/W4386053840",
    "https://openalex.org/W4390437716",
    "https://openalex.org/W6860549370",
    "https://openalex.org/W2922296866",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W4378223984",
    "https://openalex.org/W4391921616"
  ],
  "abstract": "This paper examines the rapid evolution of Large Language Models (LLMs) and their profound impact on the education sector since the emergence of ChatGPT in 2022. It explores the integration of LLMs into pedagogical practices, addressing both the enhancement of the personalized learning and the potential dependency issues. The paper also discusses the fast development of offline LLMs, emphasizing their benefits for promising intelligent tutoring systems (ITS). In addition, it high-lights current challenges of running offline LLMs. To sum up, the paper shows future of Artificial Intelligence in Education (AIEd) driven by advancements in LLM technologies.",
  "full_text": "Journal of Multimedia Information System VOL. 11, NO. 1, March 2024 (pp. 97-100): ISSN 2383-7632 (Online) \nhttp://doi.org/10.33851/JMIS.2024.11.1.97 \n97         \n \nAbstract: This paper examines the rapid evolution of Large Lan-\nguage Models (LLMs) and their profound impact on the education \nsector since the emergence of ChatGPT in 2022. It explores the \nintegration of LLMs into pedagogical practices, addressing both  \nthe enhancement of the personalized learning and the potential de-\npendency issues. The paper also discusses the fast development of \noffline LLMs, emphasizing their benefits for promising intelligent \ntutoring systems (ITS). In additi on, it highlights current chal -\nlenges of running offline LLMs. To sum up, the paper shows fu-\nture of Artificial Intelligence in Education (AIEd) driven by a d-\nvancements in LLM technologies. \nKey Words: LLM, AIEd, ITS, ChatGPT, Language Models.\n \n   \nI. INTRODUCTION  \nFrom the boom of ChatGPT in late 2022, the domain of \nLLM starts expanding dramatically. Our society was \namazed by the practical capability of such ai agent to create \nhuman-like sentences. ChatGPT was dominating in the \nmarket until new model were introduced like Bard, LLaMA, \nand other language models from tech giants. ChatGPT \nmodel is one application of powerful Large Language Mod-\nels paradigm. ChatGPT enabled new capabilities for each \nperson to deal with texts. McMurtrie (2022) compared the \nwriting tools like ChatGPT to the calculator and computers, \nwhich were part of math and science [1]. Indeed, ChatGPT \nand similar chatbot technol ogies are dynamically being \nadopt to our daily life. Over 2,000 papers were published in \narxiv.org regarding the ChatGPT and LLM domain [2].  \nMany organizations including Microsoft, Notion etc. im-\nplemented the chatbot functionality to their products [3-4]. \nThis work will explain the cu rrent stage of AIEd and the \nimportance of private language models. \n  \nII. EDUCATIONAL SHIFT BROUGHT \nBY LLM \n2.1. ChatGPT Influence \nFirst appearance of ChatGPT has sent an earthquake \nthrough the higher educator sector for its capability of gen-\nerating human-like texts in very short time. They were ini-\ntially used as quick solution to complete writing assign-\nments. Big concern of educators with ChatGPT is that it \nmight addict young learners to refer to AI agent instead of \ndeveloping own critical thinking skills. Practicing \"writing \nskill\" is very crucial to develop mature logical argumenta-\ntion and strong reasoning approach. According to Milano at \nel., 2023, foreign-language students or students who are ed-\nucationally disadvantaged are going to have strong depend-\nance to chatbots rather than crafting own sentences [5]. \nMany academies have begun to explore pedagogical us-\nage of language models. For example, Professor Ethan \nMollick from Wharton Business School has written blog \nabout his trials of using ai agents in his courses [6]. Other \ngroups re-arranged their assignments format from essay \nformat to oral or another format which was challenging task \nto do for that ChatGPT. At last, education institutes started \nto integrate the chatbot technologies and train their faculty \nto use it for creativity point of view.  \nRudolph et al. argued that a major benefit of ChatGPT is \nallowing students to learn through experimentation and ex-\nperience [7]. By using ChatGPT, students can evaluate dif-\nferent strategies and approaches to solving problems and \nachieving goals through game-based learning (Sutton and \nAllen, 2019) [8]. \n \n2.2. The Emergence of Llama, Bard, and GPT-4 \nAs time goes, language models have become safer and \nmore concrete to generate answers. Unlike previous version \nof chatbots, new version of LLMs is capable of surfing the \nweb, providing the references of their response. According \nto David et al., 2023, chatbots in education are being used \nto clarify subject concepts, problem-solving, data analysis \n[9]. Chatbots offer interactive practice sessions and quizzes \nto assess student’s understanding of academic concepts. \nChatGPT is one example of LLMs. Meta company intro-\nduced own LLaMA family of open-source models to make \nthe field of LLM more accessible and fairer [10]. Recently, \nBrief Paper: \nRevolutionizing Education: The Era of Large Language Models \n \nHo-Woong Choi1*, Sardor Abdirayimov2 \nManuscript received February 28, 2024; Revised March 08; Accepted March 18, 2024. (ID No. JMIS-24M-02-005)  \nCorresponding Author: Ho-Woong Choi, +82-10-8972-0517, techimpress@naver.com \n1Department of Media Software, Sungkyul University, Anyang, Korea, techimpress@naver.com \n2Department of AI and Big Data, Woosong University, Daejeon, Korea, 202112112@live.wsu.ac.kr \nRevolutionizing Education: The Era of Large Language Models \n98 \n \ngoogle also introduced own Gemma models family as \nopen-source project [11]. Such actions forward to think \nabout fine-tuning own language model for own use case. \n \nIII. CONSTRASTING PARADIGMS: \nOPEN-SOURCE AND PROPRIETARY \nLANGUAGE MODELS \nProprietary language models, while providing advanced \nfeatures and consistent updates, can cause data privacy is-\nsues. The use of these models might often involve the shar-\ning of users’ input data with the company. It becomes very \ntrivial when students and educator’s interaction with mod-\nels could have sensitive information. On the other hand, \nopen-source language models offer open flexibility and em-\nphasizes transparency. They are available to everyone and \nallow for modifications to suit specific need without think-\ning about privacy concerns. Thanks to open-source model \ndevelopments, new paradigm of SLMs is being dramati-\ncally emerged. SLMs differ from LLMs on compact num-\nber of parameters. SLMs are inference-optimal models. \nSLMs like TinyGPT-V, TinyLlama are first steps on build-\ning cost-effective, efficient, and high-performing language \nmodels [12-13]. \nFramework called LangChain gained more than 77,600 \nstarts in February 2024 [14]. LangChain provides latest \ntools and APIs to manipulate the output of any the language \nmodels for certain tasks. Moreover, the Hugging Face hub \nhas more than 800,000 public accessible AI models availa-\nble for developers and researchers [15]. The increasing abil-\nity to run sophisticated LLMs offline, as seen with open-\nsource projects like Ollama and Llama.cpp, present numer-\nous advantages [16-17]. It pr ovides a more secure interac-\ntion with AI agents, retaining user data privacy and advanc-\ning the development of intelligent tutoring systems, thereby \nenrichening the education experience. \n   \nIV . FUTURE OF LLM IN EDUCATION \nAccording to (Rudolph et al., 2023), intelligent tutoring \nsystems (ITS) are the best solution of transforming educa-\ntion with AI technologies [7]. With such AI-powered algo-\nrithms, ITSs can simulate the assistance provided by a tutor, \nsuch as by providing personalized assistance in solving \nproblems. The benefit of ITS technologies will be the abil-\nity of adopting to the characteristics of students and their \nemotional state in every aspect of their learning in real-time, \nresulting in personalized adaptive learning (PAL) (Peng et \nal., 2019) [18]. \nLLMs are at the heart of the personalized learning revo-\nlution. Instead of traditional, static, one-size-fits-all educa- \ntional materials and contexts, LLMs can provide personal-\nized, adaptive, and dynamic insight and feedback — signal-\ning a paradigm shift in individualized learning more reflec-\ntive of the power of AI itself. T hey are able to adjust their \nprogress and style to suit each and every learner, with lead-\ning platforms such as Hugging Face offering such an exten-\nsive range of models and datasets that it is enabling ad-\nvanced educational tools that go far beyond traditional con-\nsumer programming language frameworks.  \n  \n4.1. Benefits of Running Offline Models \nRunning language models offline offers substantial ben-\nefit for organizations aiming for secure and private AI in-\nteractions. Processing queries locally without transmitting \ninformation over the internet, ensures data privacy.  \nOne of the big benefits of the offline chatbots is their spe-\ncial customization to organization’s specific needs. Lastly, \nbut not least, offline models are a step towards realizing so-\nphisticated intelligent tutorial systems (ITS), which is the \nmost promising innovation in Artificial Intelligence in Ed-\nucation (AIEd). \n  \n4.2. Challenges of Running Offline Models \nOffline language models face significant challenges, \nwhile they offer advantages in privacy and security. Firstly, \nmaintaining offline language models poses distinct chal-\nlenges compared to their cloud-based counterparts. In other \nwords, without the pipeline for automatic updates, these \nmodels can quickly become outdated. Secondly, the perfor-\nmance of offline models often lags without the robust com-\nputational support of the cloud, potentially leading to \nslower responses that unpleasant the users. Additionally, in-\nterfacing with these models may also prove less efficient \ncausing use frustration due to increased wait times. To sum \nup, integration of current offline models demands careful \nplanning and resources to enable swift interaction with \nchatbots in educational environments. \n   \n4.3. Prompt Engineering as New Subject \nIn chatbot dimension, prompt is set of guidance for lan-\nguage model to follow in response generation process. Lan-\nguage models are quite sensitive to given prompt. While \nwell-written prompt unlocks full potential of chatbot, poor \nprompt might lead to non-sequitur response. Leveraging the \npotential of LLMs in productive and ethical ways requires \na systematic focus on prompt engineering. In this study [19] \nauthors implemented flipped interaction pattern, which \nforces the chatbot to ask the user questions until it obtains \nenough information to achieve a particular goal. Educating \nnuances of prompt engineering to young learners would en-\nhance their digital literacy.  \n \nJournal of Multimedia Information System VOL. 11, NO. 1, March 2024 (pp. 97-100): ISSN 2383-7632 (Online) \nhttp://doi.org/10.33851/JMIS.2024.11.1.97 \n99           \n \nV . CONCLUSION \nTo conclude, the paper has discussed the impact of Large \nLanguage Models like ChatGPT on the education, high-\nlighting both their transformative potential and the chal-\nlenges they present. While these AI tools have revolution-\nized personal learning and brought new opportunity for \nITS. In addition, the paper highlighted the differences be-\ntween company models and open-source models as innova-\ntion of offline available models. Two chatbots were pre-\nsented as an example of offline chatbots in devices. How-\never, maintaining these offline systems poses its own sets \nof challenges, in terms of ensuring responsive interactions. \nAs AI continues to evolve, it would be possible to overcome \nchallenges occurred with offline chatbots. The future of ed-\nucation with AIEd appears promising, where students will \nobtain personalized and inclusive education. \n \nREFERENCES \n[1] B. McMurtrie, \"ChatGPT is everywhere: Love it or hate \nit, academics can't ignore the already pervasive technol-\nogy,\" The Chronicle of Higher Education , vol. 69, no. \n15, pp. 32-38, Mar. 2023. \n[2] arXiv, curated research-sharing platform, https://arxiv.o \nrg/search/advanced?advanced=&terms-0-operator=AN \nD&terms-0-term=ChatGPT&terms-0-field=all&classif \nication-physics_archives=all&classification-include_cr \noss_list=include&date-year=&date-filter_by=date_ran \nge&date-from_date=2022&date-to_date=2024&date-d \nate_type=submitted_date&ab-\nstracts=show&size=50&order=-announced_date_first. \n[3] All Things How Blog, https://allthings.how/how-to-\nuse-notion-ai/, 2023. \n[4] Microsoft Blog, Copilot, https://www.microsoft.com/ \nen-us/microsoft-365/blog/2023/03/16/introducing-mi-\ncrosoft-365-copilot-a-whole-new-way-to-work/, 2023. \n[5] S. Milano, J. A. McGrane, and S. Leonelli, \"Large lan-\nguage models challenge the future of higher education,\" \nNature Machine Intelligence, vol. 5, no. 4, pp. 333-334. \n2023. \n[6] E. Mollick and L. Mollick, \"Assigning AI: Seven ap-\nproaches for students, with prompts,\" arXiv.2306.1 \n0052, 2023. \n[7] J. Rudolph, S. Tan, and S. Tan, \"ChatGPT: Bullshit \nspewer or the end of traditional assessments in higher \neducation?\" Journal of Applied Learning and Teaching, \nvol. 6, no. 1, 2003. \n[8] R. Bina, M. J. Sutton, and K. Allen, \"Emotify! The \npower of the human element in game-based learning, \nserious games and experiential education,\" EI Games \nLLC, 2019. \n[9] D. S. Calonge, L. Smail, and F. Kamalov, \"Enough of \nthe chit-chat: A comparative analysis of four AI chat-\nbots for calculus and statistics,\"  Journal of Applied \nLearning and Teaching, vol. 6, no. 2, 2023. \n[10] LLaMA, Meta language Model, https://llama.meta.com/ \n[11] Gemma, Open-Source Model by Google, https://blog. \ngo ogle/technology/developers/gemma-open-models/, \n2024. \n[12] Z. Yuan, L. Zhaoxu, and S. Lichao \"Tinygpt-v: Effi-\ncient multimodal large language model via small back-\nbones,\" arXiv preprint arXiv:2312.16862, 2023. \n[13] P. Zhang, Z. Guangtao, W. Tianduo, and L. Wei, \"Ti-\nnyllama: An open-source small language model,\" \narXiv preprint arXiv:2401.02385, 2024. \n[14] LangChain GitHub repository, https://github.com/lan \ngchain-ai/langchain. \n[15] Hugging Face Hub, https://huggingface.co/models \n[16] Ollama Language Model website, https://ollama.com/ \n[17] LLM inference in C/C++, https://github.com/gger-\nganov/llama.cpp \n[18] H. Peng, S. Ma, and J. M. Spector, \"Personalized adap-\ntive learning: An emerging pedagogical approach en-\nabled by a smart learning environment,\" Smart Learn-\ning Environments, vol. 6, no. 1, pp. 1-14, 2019. \n[19] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, and \nH. Gilbert, et al., \"A prompt pattern catalog to enhance \nprompt engineering with chatgpt,\" arXiv preprint \narXiv:2302.11382, 2023. \n[ 2 0 ]  P .  L i m n a ,  T .  K r a i w a n i t ,  K .  J a n g j a r a t ,  P .  K l a y k l u n g ,  \nand P. Chocksathaporn, \"The use of ChatGPT in the \ndigital era: Perspectives on chatbot implementation,\" \nJournal of Applied Learning and Teaching, vol. 6, no. \n1, 2023. \n[ 2 1 ]  M .  M .  V a n  W y k ,  \" I s  C h a t G P T  a n  o p p o r t u n i t y  o r  a  \nthreat? Preventive strategies employed by academics \nrelated to a GenAI-based LLM at a faculty of educa-\ntion,\" Journal of Applied Learning and Teaching, vol. \n7, no. 1, 2024. \n[22] J. Rudolph, S. Tan, and S. Tan, \"ChatGPT: Bullshit \nspewer or the end of traditional assessments in higher \neducation?,\" Journal of Applied Learning and Teach-\ning, vol. 6, no. 1, pp. 342-363, 2023. \n  \nRevolutionizing Education: The Era of Large Language Models \n100 \n \n \n \n ",
  "topic": "Dependency (UML)",
  "concepts": [
    {
      "name": "Dependency (UML)",
      "score": 0.5551537871360779
    },
    {
      "name": "Computer science",
      "score": 0.3514527976512909
    },
    {
      "name": "Political science",
      "score": 0.3313073515892029
    },
    {
      "name": "Artificial intelligence",
      "score": 0.25545012950897217
    }
  ]
}