{
  "title": "VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena",
  "url": "https://openalex.org/W4281633937",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2952608021",
      "name": "Letitia Parcalabescu",
      "affiliations": [
        "Heidelberg University",
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A2562337787",
      "name": "Michele Cafagna",
      "affiliations": [
        "University of Malta"
      ]
    },
    {
      "id": "https://openalex.org/A4282353333",
      "name": "Lilitta Muradjan",
      "affiliations": [
        "Heidelberg University",
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A2113562383",
      "name": "Anette Frank",
      "affiliations": [
        "Heidelberg University",
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A2250213072",
      "name": "Iacer Calixto",
      "affiliations": [
        "New York University",
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A2265295789",
      "name": "Albert Gatt",
      "affiliations": [
        "Utrecht University",
        "University of Malta"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3166893724",
    "https://openalex.org/W2962707719",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2912371042",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2952215760",
    "https://openalex.org/W3016211260",
    "https://openalex.org/W3106784008",
    "https://openalex.org/W3177487519",
    "https://openalex.org/W3175910413",
    "https://openalex.org/W3093871960",
    "https://openalex.org/W2786470004",
    "https://openalex.org/W2423576022",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W3134095442",
    "https://openalex.org/W3099954305",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W3098415518",
    "https://openalex.org/W2970078867",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2962749469",
    "https://openalex.org/W3104379732",
    "https://openalex.org/W3167150513",
    "https://openalex.org/W2939888942",
    "https://openalex.org/W3034727271",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W3214105842",
    "https://openalex.org/W3034381157",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W3096682293",
    "https://openalex.org/W2945087694",
    "https://openalex.org/W3105928338",
    "https://openalex.org/W2964068236",
    "https://openalex.org/W3104788521",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3174340530",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3035512383",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W3170976857",
    "https://openalex.org/W2803125506",
    "https://openalex.org/W3011101596",
    "https://openalex.org/W2962833164",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W2038721957",
    "https://openalex.org/W3101638716",
    "https://openalex.org/W3172863300",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3163542683",
    "https://openalex.org/W2968124245"
  ],
  "abstract": "We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&amp;L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground linguistic phenomena in the visual modality, allowing more fine-grained evaluations than hitherto possible. We build VALSE using methods that support the construction of valid foils, and report results from evaluating five widely-used V&amp;L models. Our experiments suggest that current models have considerable difficulty addressing most phenomena. Hence, we expect VALSE to serve as an important benchmark to measure future progress of pretrained V&amp;L models from a linguistic perspective, complementing the canonical task-centred V&amp;L evaluations.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 8253 - 8280\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nV ALSE\n : A Task-Independent Benchmark for Vision and Language\nModels Centered on Linguistic Phenomena\nLetitia Parcalabescu1∗ Michele Cafagna2 Lilitta Muradjan1\nAnette Frank1 Iacer Calixto3,4 Albert Gatt5,2\n1Heidelberg University, Department of Computational Linguistics\n2University of Malta, Institute of Linguistics and Language Technology\n3New York University 4ILLC, University of Amsterdam\n5Utrecht University, Department of Information and Computing Sciences\nAbstract\nWe propose V ALSE (Vision And Language\nStructured Evaluation), a novel benchmark de-\nsigned for testing general-purpose pretrained\nvision and language (V&L) models for their\nvisio-linguistic grounding capabilities on spe-\nciﬁc linguistic phenomena . V ALSE offers\na suite of six tests covering various linguis-\ntic constructs. Solving these requires models\nto ground linguistic phenomena in the visual\nmodality, allowing more ﬁne-grained evalua-\ntions than hitherto possible. We build V ALSE\nusing methods that support the construction of\nvalid foils, and report results from evaluating\nﬁve widely-used V&L models. Our experi-\nments suggest that current models have consid-\nerable difﬁculty addressing most phenomena.\nHence, we expect V ALSE to serve as an impor-\ntant benchmark to measure future progress of\npretrained V&L models from a linguistic per-\nspective, complementing the canonical task-\ncentred V&L evaluations.\n1 Introduction\nGeneral-purpose pretrained vision and language\n(V&L) models have gained notable performance on\nmany V&L tasks (Lu et al., 2019; Tan and Bansal,\n2019; Li et al., 2019; Chen et al., 2020; Li et al.,\n2020a; Su et al., 2020). As a result, V&L research\nhas changed its focus from task-speciﬁc architec-\ntures to ﬁne-tuning large V&L models.\nCurrent benchmarks give a good perspective\non model performance on a wide range of V&L\ntasks (Cao et al., 2020; Lourie et al., 2021; Li\net al., 2021), but the ﬁeld is only starting to assess\nwhy models perform so well and whether models\nlearn speciﬁc capabilities that span multiple V&L\ntasks. Speciﬁcally, we lack detailed understand-\ning of the extent to which such models are able to\nground linguistic phenomena—from morphosyn-\ntax to semantics—in the visual modality (Bernardi\n∗ Corresponding author parcalabescu@cl.\nuni-heidelberg.de.\nand Pezzelle, 2021). For example, recent evidence\nsuggests that models are insensitive to linguistic\ndistinctions of verb-argument structure (Hendricks\nand Nematzadeh, 2021) and word order (Cirik et al.,\n2018; Akula et al., 2020).\nOur work addresses this gap with V ALSE (Vi-\nsion And Language Structured Evaluation), a\nbenchmark for V&L model evaluation compris-\ning six tasks, or ‘pieces’, where each piece has the\nsame structure: given a visual input, a model is\nasked to distinguish real captions from foils, where\na foil is constructed from a caption by altering a\nword or phrase that realizes a speciﬁc linguistic\nphenomenon, e.g., semantic number of nouns, verb\nargument structure, or coreference. V ALSE uses a\nresource-lean diagnostic setup that dispenses with\nlarge-scale annotation (e.g., of bounding boxes),\nand builds on existing high-quality image caption-\ning and VQA data. V ALSE is designed to lever-\nage the existing prediction heads in pretrained (or\nﬁnetuned) V&L models; for that reason, our bench-\nmark does not include any re-training and can be\ninterpreted as a zero-shot evaluation. We build test\ndata for each piece so as to safeguard against the\npossibility of models exploiting artefacts or statis-\ntical biases in the data, a well-known issue with\nhighly parameterised neural models pretrained on\nlarge amounts of data (Goyal et al., 2017; Mad-\nhyastha et al., 2018; Kaﬂe et al., 2019). With this\nin view, we propose novel methods to guard against\nthe emergence of artefacts during foiling.\nOur main contributions are:\ni) We introduce V ALSE, a novel benchmark\naimed at gauging the sensitivity of pre-trained\nV&L models to foiled instances.\nii) We cover a wide spectrum of basic linguistic\nphenomena affecting the linguistic and visual\nmodalities: existence, plurality, counting, spa-\ntial relations, actions, and entity coreference.\niii) We investigate novel strategies to buildvalid\nfoils that include automatic and human valida-\n8253\ntion. We balance word frequency distributions\nbetween captions and foils, and test against\npretrained models solving the benchmark uni-\nmodally by relying only on text. We employ\nmasked language modeling (MLM) in foil cre-\nation and semantic inference for validating\nfoils, and ﬁnally collect human annotations\nfor the entire benchmark.\niv) We establish initial experimental results for\npretrained V&L models of diverse architec-\ntures on V ALSE. The overall weak perfor-\nmance of these models indicates that the time\nis ripe for a novel, reliable foiling dataset tar-\ngeting the visual grounding capabilities of\nV&L models through the lens of linguistic\nconstructs.1\n2 Background and Related work\nPretrained V&L models learn to combine vision\nand language through self-supervised multitask\nlearning. Tasks include multimodal masked model-\ning—where words in the text and object labels or re-\ngions in the image are masked out, then predicted—\nand image-sentence alignment, whereby a model\nlearns to predict whether an image and a text corre-\nspond. Major architectures are single- and dual-\nstream multimodal transformers: single-stream\nmodels concatenate word and image features, and\nencode the resulting sequence with a single trans-\nformer stack; dual-stream models use distinct trans-\nformer stacks to handle visual and textual inputs,\nand additional layers (e.g. co-attention) to fuse\nthese into multimodal features.\nBenchmarking V&L models V&L models (Li\net al., 2019; Lu et al., 2019; Tan and Bansal, 2019;\nLu et al., 2020; Li et al., 2020b; Kim et al., 2021)\nare commonly evaluated on V&L tasks such as\nVQA (Goyal et al., 2017), visual reasoning (Suhr\net al., 2019), or image retrieval (Lin et al., 2014;\nPlummer et al., 2015).\nGiven how well transformer-based models per-\nform across unimodal and multimodal tasks, re-\nsearch efforts have recently started to address what\nmakes them so effective, and to what extent they\nlearn generalisable representations. Techniques\nto address these questions in unimodal and multi-\nmodal V&L contexts include: adversarial examples\n(Jia and Liang, 2017; Jia et al., 2019); investigation\n1We release our dataset containing all annotators’ votes\n(Prabhakaran et al., 2021) at https://github.com/\nHeidelberg-NLP/VALSE.\nof the impact of bias, be it linguistic (Gururan-\ngan et al., 2018), visual semantic (Agarwal et al.,\n2020), or socio-economic (Garg et al., 2019); and\nthe use of linguistically-informed counterfactual\nand minimally-edited examples (Levesque et al.,\n2012; Gardner et al., 2020). A trend within the\nlatter research line that is speciﬁc to V&L mod-\nels is vision-and-language foiling (Shekhar et al.,\n2017b; Gokhale et al., 2020; Bitton et al., 2021;\nParcalabescu et al., 2021; Rosenberg et al., 2021),\nwhere the idea is to create counterfactual (i.e.,\nfoiled) and/or minimally edited examples by per-\nforming data augmentation on captions (Shekhar\net al., 2017b,a) or images (Rosenberg et al., 2021).\nSince most V&L models are pretrained on some\nversion of the image-text alignment task, it is pos-\nsible to test their ability to distinguish correct from\nfoiled captions (in relation to an image) in a zero-\nshot setting. The construction of foils can serve\nmany investigation purposes. With V ALSE, we\ntarget the linguistic grounding capabilities of V&L\nmodels, focusing on pervasive linguistic phenom-\nena that span multiple tokens, described in §3.1–\n§3.6. At the same time, we ensure that our data\nis robust to perturbations and artefacts by i) con-\ntrolling for word frequency biases between cap-\ntions and foils, and ii) testing against unimodal\ncollapse, a known issue of V&L models (Goyal\net al., 2017; Madhyastha et al., 2018), thereby pre-\nventing models from solving the task using a single\ninput modality. The issue of neural models exploit-\ning data artefacts is well-known (Gururangan et al.,\n2018; Jia et al., 2019; Wang et al., 2020b; He et al.,\n2021) and methods have been proposed to uncover\nsuch effects, including gradient-based, adversar-\nial perturbations or input reduction techniques (cf.\nWallace et al., 2020). Yet, these methods are still\nnot fully understood (He et al., 2021) and can be\nunreliable (Wang et al., 2020b).\nOur work is related to Gardner et al. (2020),\nwho construct task-speciﬁc contrast sets for NLU.\nHowever, our focus is on modelling linguistic phe-\nnomena instead of tasks, and we construct carefully\ncurated, balanced, single foils from valid instances\nthat we select from multiple multimodal datasets.\n3 Constructing the V ALSE benchmark\nWe resort to a musical analogy to describe V ALSE:\nVision And Language Structured Evaluation is\ncomposed of 6 pieces, each corresponding to a\nspeciﬁc linguistic phenomenon (see Table 1 for an\n8254\nData collection & metadata\npieces existence plurality counting relations actions coreference\ninstruments existential\nquantiﬁers\nsemantic number balanced, adver-\nsarial, small numbers\nprepositions replacement,\nactant swap\nstandard, clean\n#examples† 505 851 2 ,459 535 1 ,633 812\nfoil\ngeneration\nmethod\nnothing↔\nsomething\nNP replacement\n(sg2pl; pl2sg) &\nquantiﬁer insertion\nnumeral re-\nplacement\nSpanBERT pre-\ndiction\naction replace-\nment, actant\nswap\nyes↔no\nMLM \u0017 \u0017 \u0017 \u0013 \u0013 \u0017\nGRUEN \u0017 \u0013 \u0017 \u0013 \u0017 \u0017\nNLI \u0017 \u0013 \u0017 \u0013 \u0017 \u0017\nsrc. dataset Visual7W MSCOCO Visual7W MSCOCO SWiG VisDial v1.0\nimage src. MSCOCO MSCOCO MSCOCO MSCOCO SituNet MSCOCO\nExample data\ncaption\n(blue) / foil\n(orange)\nThere are\nno animals\n/ animals\nshown.\nA small copper vase\nwith some ﬂowers /\nexactly one ﬂower in\nit.\nThere are four / six ze-\nbras.\nA cat plays with\na pocket knife on\n/ underneath a\ntable.\nA man / woman\nshouts at a\nwoman / man.\nBuffalos walk\nalong grass.\nAre they in a\nzoo? No / Yes.\nimage\nTable 1: Overview of pieces and instruments in V ALSE, with number of examples per piece; the foil generation\nmethod used; whether masked language modelling (MLM), GRUEN, and NLI ﬁltering are used; dataset and image\nsources; and image-caption-foil examples. †The number of examples is the sum of the examples available for each\ninstrument in the piece. In Table 5 (in the Appendix) we list the number of examples in each individual instrument.\noverview). Each piece consists of one or more in-\nstruments designed to evaluate a model’s ability to\nground that speciﬁc linguistic phenomenon.\nAll instruments are built by applyingfoiling func-\ntions (FFs) speciﬁc to the linguistic phenomenon\nunder study. FFs take acorrect caption as input and\nchange a speciﬁc part to produce a foiled caption\n(or foil). We design FFs such that the sentences\nthey produce fail to describe the image, while still\nbeing grammatical and otherwise valid sentences.\nOf course, a foiled caption may be less likely\nthan the original caption from which it was pro-\nduced, and such unwarranted biases can be eas-\nily picked up by overparameterised V&L models.\nMoreover, an automatic FF may fail to produce a\nfoil that contradicts the image, for example by alter-\ning the original caption to yield a near-synonymous\none, or one that is entailed by the original caption.\nFor phenomena that make it difﬁcult to control\nthese crucial properties of foils, we apply addi-\ntional ﬁlters: i) some FFs make use of strong LMs\nto propose changes to captions, so that the gener-\nated foils are still high-probability sentences; ii)\nwe use state-of-the-art natural language inference\n(NLI) methods to detect cases where there is an\nentailment between caption and foil, and ﬁlter out\nsuch foils from the dataset (see §4 for discussion).\nAs a ﬁnal measure, we employ human annotators\nto validate all generated testing data in V ALSE.\nV ALSE data is sourced from existing V&L\ndatasets. Below, we describe each piece and its\ninstruments, and the corresponding task setup in\nV ALSE. For each instrument, we follow the same\nprocedure: i) we identify captions that contain in-\nstances of the targeted linguistic phenomenon; ii)\nwe apply a FF that automatically replaces the ex-\npression with a variant that contradicts the original\nexpression’s visual content, thereby constructing\none or more foils from each target instance in the\noriginal caption, as discussed in §4; we then iii)\nsubject the obtained foils to various ﬁlters, with the\naim of distilling a subset of valid and reliable foils\nthat cannot be easily tricked by a new generation\nof highly parameterised pretrained V&L models.\n3.1 Existence\nThe existence piece has a single instrument and tar-\ngets instances with existential quantiﬁers. Mod-\nels need to differentiate between examples i) where\nthere is no entity of a certain type or ii) where one\nor more of these entities are visible in an image.\nWe use the Visual7W visual question answering\ndataset (Zhu et al., 2016) and source its ‘how many’\nexamples, building a pool of those whose answers\nare numerals (0, 1, 2, etc.). We use templates to\ntransform question and answer ﬁelds into a declara-\ntive statement that correctly describes what can be\nseen in the image, e.g. ‘Q: How many animals are\nshown? A: 0’→‘There are 0 animals shown’. We\nthen transform these statements into an existential\n8255\nstatement. In the example above, we replace the nu-\nmeral by the word ‘no’ to create a correct caption\n(‘There are no animals shown’) and remove the\nnumeral altogether to create a foil (‘There are ani-\nmals shown’). The existence piece has 505 image–\ncaption–foil tuples after manual validation, out of\n534 candidates (cf. §4), and captions/foils are bal-\nanced: 50% of the (correct) captions originally\nhave answer 0, and the remaining have answer 1 or\ngreater. Full details are provided in A.1.\n3.2 Plurality\nThe plurality piece has a single instrument, con-\ncerned with semantic number. It is intended to\ntest whether a model is able to distinguish between\nnoun phrases denoting a single entity in an im-\nage (‘exactly one ﬂower’), versus multiple entities\n(‘some ﬂowers’). The dataset consists of 851 vali-\ndated instances out of 1000 generated candidates\n(cf. §4), evenly divided between cases where the\ncaption contains a plural NP, foiled by replacing it\nwith a singular (pl2sg: ‘some ﬂowers’→‘exactly\none ﬂower’), or conversely, the caption contains a\nsingular which is foiled by replacing it with a plural\n(sg2pl). Foil candidates were generated from the\nCOCO 2017 validation set (Chen et al., 2015). Full\ndetails about the foil construction and our measures\nagainst introducing biases with quantiﬁers such as\n‘exactly one’, are provided in A.2.\n3.3 Counting\nThe counting piece has three instruments: bal-\nanced, adversarial and small numbers. All in-\nstances are statements about the number of entities\nvisible in an image. The model needs to differenti-\nate between examples where the speciﬁc number of\nentities in the associated image is correct or incor-\nrect, given the statement. Similarly to the existence\npiece, we use the Visual7W VQA dataset (Zhu\net al., 2016) and source its ‘how many’ examples\nwhose answers are numerals (0, 1, 2, etc.). We use\ntemplates to transform question and answer ﬁelds\ninto a declarative statement describing the image\nand create foils by replacing the numeral in the\ncorrect statement by another numeral.\nAll three instruments are designed to show\nwhether models learn strategies that generalize be-\nyond the training distribution, and to what extent\na model exploits class frequency bias.2 In count-\ning balanced we cap the number of examples to\n2We take the original answer in Visual7W as the example\nclass: e.g., in ‘There are 0 animals shown’, the class is 0.\na maximum per class and make sure correct and\nfoil classes are balanced, so that models that ex-\nploit class frequency bias are penalized. In count-\ning adversarial we ensure that all foils take class\nn∈{0,1,2,3}, whereas all correct captions take\nclass m ∈{m|m ≥4}. Biased models are ex-\npected to favour more frequent classes. Since small\nnumbers are naturally the most frequent, models\nthat resort to such biases should perform poorly on\nthis adversarial test set. Counting small numbers\nis a sanity check where all correct captions and\nfoils have class n ∈{0,1,2,3}, and caption/foil\nclasses are balanced. Since models likely have\nbeen exposed to many examples in this class set\nand all such classes are high-frequency, with this in-\nstrument we disentangle model performance from\nclass exposure. Counting balanced, adversarial,\nand small numbers have 868 (1000), 691 (756),\nand 900 (1000) instances after (before) manual val-\nidation, respectively (cf. §4). For details, see A.3.\n3.4 Spatial relations\nThe relations piece has a single instrument and\nfocuses on the ability of models to distinguish be-\ntween different spatial relations. Foils differ from\nthe original caption only by the replacement of\na spatial preposition. As with plurals, the data\nwas sourced from the COCO 2017 validation split.\nTo create foils, we ﬁrst identiﬁed all preposition\nsequences in captions (e.g., ‘in’, ‘out of’). Foils\nwere created by masking the prepositions and using\nSpanBERT (Joshi et al., 2020) to generate candi-\ndates of between 1–3 words in length. We keep\nSpanBERT candidates, which are spans whose\nlengths vary from 1 to 3, if they differ from the orig-\ninal preposition sequence, but exist in the dataset.\nThere are 535 instances after manual validation out\nof 614 proposed instances (cf. §4), and we ensure\nthat prepositions are similarly distributed among\ncaptions and foils. Full details are provided in A.4.\n3.5 Actions\nThe actions piece has two instruments: i) action\nreplacement and ii) actant swap . They test a\nV&L model’s capability to i) identify whether an\naction mentioned in the text matches the action\nseen in the image (e.g., ‘a manshouts / smiles at a\nwoman’), and ii) correctly identify theparticipants\nof an action and the roles they play (e.g., is it the\nman who is shouting or is it the woman, given the\npicture in Table 1?).\nThe SWiG dataset (Pratt et al., 2020) contains\n8256\n504 action verbs, and we generate captions and\nfoils from SWiG annotations of semantic roles and\ntheir ﬁllers. For the action replacement piece, we\nexchange action verbs with other verbs from SWiG\nthat ﬁt the linguistic context as suggested by BERT.\nFor the actant swap, we swap role ﬁllers in the role\nannotations, hence generating action descriptions\nwith inverted roles. Action replacement and actant\nswap have 648 (779) and 949 (1042) instances after\n(before) manual validation, respectively (cf. §4).\nSee A.5 for full details.\n3.6 Coreference\nThe coreference piece aims to uncover whether\nV&L models are able to perform pronominal coref-\nerence resolution. It encompasses cases where i)\nthe pronoun has a noun (phrase) antecedent and\npronoun and (noun) phrase are both grounded in\nthe visual modality (‘A woman is driving a motor-\ncycle. Is she wearing a helmet?’), and cases where\nii) the pronoun refers to a region in the image or\neven to the entire image (‘Is thisoutside?’).\nWe create foils based on VisDial v1.0 (Das et al.,\n2017) with images from MSCOCO (Lin et al.,\n2014). VisDial captions and dialogues are Q&A se-\nquences. We select image descriptions of the form\n[Caption. Question? Yes/No. ] where the ques-\ntion contains at least one pronoun. When foiling,\nwe exchange the answer from yes to no and vice-\nversa (see Table 1). We ensure a 50-50% balance\nbetween yes / no answers.\nThe coreference piece consists of two instru-\nments: coreference standard originating from the\nVisDial train set and a small coreference clean set\nfrom the validation set, containing 708 (916) and\n104 (141) examples after (before) manual valida-\ntion, respectively (cf. §4).3 See A.6 for full details.\n4 Reliable construction of valid foils\nIn V ALSE, an instance consisting of an image-\ncaption-foil triple is considered valid if: the foil\nminimally differs from the original caption; the foil\ndoes not accurately describe the image; and inde-\npendent judges agree that the caption, but not the\nfoil, is an accurate description of the image. We\nconsider a foiling method to be more reliable the\nmore it ensures that a generated foil does not sub-\nstantially differ from a human caption regarding\ndistributional and plausibility bias, and cannot be\neasily solved unimodally.\n3VisDial annotations are not available for the test set.\nIn this section, we discuss automatic and man-\nual means to reliably construct valid foils. In this\ncontext, two types of bias are especially worthy of\nnote: distributional bias (§4.1) and plausibility bias\n(§4.2). In §4.3 we discuss how we apply a natu-\nral language inference model to ﬁlter examples in\nour data pipeline, and §4.4 show how we manually\nvalidate all examples in our benchmark. Random\nsamples from the ﬁnal version of each instrument\nare shown in Tab. 6–11.\n4.1 Mitigating distributional bias\nA ﬁrst form of bias is related to distributional imbal-\nance between captions and foils (e.g., certain words\nor phrases having a high probability only in foils).\nPrevious foiling datasets exhibit such imbalance,\nenabling models to solve the task disregarding the\nimage (Madhyastha et al., 2019). To mitigate this\nproblem, for each phenomenon and throughout our\ndata creation process, we ensure that the token fre-\nquency distributions in correct and foiled captions\nare approximately the same (cf. App. A and E).\n4.2 Countering plausibility bias\nA second form of bias may arise from automatic\nprocedures yielding foils that are implausible or un-\nnatural, which can facilitate their detection. Often,\nV ALSE pieces can be safely foiled by simple rules\n(e.g., switching from existence to non-existence,\nor from singular to plural or vice versa). However,\nwith spatial relations and actions, a foil could be\ndeemed unlikely given only the textual modality\nand independently of the image, e.g., ‘a man stands\nunder / on a chair’. Such plausibility biases may\nbe detected by large language models that incorpo-\nrate commonsense knowledge (Petroni et al., 2019;\nWang et al., 2020a), and we expect future V&L\nmodels to exhibit similar capabilities.\nTo ensure that foiled and correct captions are\nsimilarly plausible, we use language models such\nas BERT (Devlin et al., 2019) and SpanBERT\n(Joshi et al., 2020) to suggest replacements in our\nfoiling functions. Additionally, in the case of spa-\ntial relations and plurals, we also apply a grammat-\nicality ﬁlter using GRUEN (Zhu and Bhat, 2020).\nGRUEN was originally proposed to automatically\nscore generated sentences based on discourse-level\nand grammatical properties. We use only the gram-\nmaticality component of GRUEN, and retain only\nfoil candidates with a grammaticality score ≥0.8.\nFurthermore, we evaluate unimodal, language-\nonly models on V ALSE to verify whether our\n8257\nbenchmark could be solved by a multimodal model\nwith strong linguistic capacities in unimodal col-\nlapse, whereby a model silently relies on a single\nmodality within which biases are easier to exploit\n(Goyal et al., 2017; Shekhar et al., 2019a). By eval-\nuating V ALSE with unimodal models, we establish\na baseline that V&L models should exceed if we\nare to expect true multimodal integration.\n4.3 Filtering foils with NL Inference\nWhen constructing foils, we need to ensure that\nthey fail to describe the image. To test this au-\ntomatically, we apply natural language inference\n(NLI) with the following rationale: We consider an\nimage and its caption as a premise and its entailed\nhypothesis, respectively (a similar rationale is ap-\nplied in the visual entailment task; Xie et al., 2019).\nIn addition, we consider the caption as premise and\nthe foil as its hypothesis. If a NLI model predicts\nthe foil to be entailed (E) by the caption, it cannot\nbe a good foil since by transitivity it will give a\ntruthful description of the image. By contrast, if\nthe foil is predicted to contradict (C) or to be neu-\ntral (N) with respect to the caption, we take this as\nan indicator of a valid (C) or a plausible (N) foil.4\nWe use the NLI model ALBERT (Lan et al.,\n2020) ﬁnetuned on the task (see Appendix C for\ndetails). Filtering with NLI was initially applied\nto relations, plurals and actions, on the grounds\nthat foils in these pieces may induce substantive\nchanges to lexical content.5 Following automatic\nlabelling of caption-foil pairs, we manually vali-\ndated a sample labelled as E, C or N. For relations\n(N = 30), labels were found to be near 100% accu-\nrate with only 2 (0.06%) errors overall. For plurals\n(N = 60, 50% sg2pl and 50% pl2sg), the er-\nror rate was also low, with 0 errors for C, 33%\nerrors for E and 11% errors for N. Here, a number\nof entailment errors were due to odd formulations\narising from the automatic foiling process, whereas\nno such oddities were observed for C. We therefore\ninclude only foils labelled C in the ﬁnal relations\nand plurals pieces. For actions, the model labelled\n4See the following examples from action replacement:\nP: A mother scolds her son.\nH1: A mother encourages her son. (C; good foil);\nH2: A mother camps with her son. (N; needs image control);\nH3: A mother talks to her son. (E; not a suitable foil)\nIf the NLI prediction is N, we still need to check the image,\nsince the description might happen to ﬁt the image content.\n5By contrast, existence and counting foils involve a more\nstraightforward swap (e.g., between numerical quantities);\nsimilarly, coreference foils simply involve the replacement of\na positive with a negative answer.\ncontradictions very accurately (0% error) but was\nerroneous up to 97.1% for E, meaning that a large\nnumber of valid foils would be spuriously excluded.\nTo avoid reducing the dataset too much, we did not\nuse NLI ﬁltering for actions, but relied on human\nannotation as a ﬁnal validity check.\n4.4 Manual evaluation of generated foils\nAs a ﬁnal step, the data for each instrument was\nsubmitted to a manual validation. For each instance,\nannotators were shown the image, the caption and\nthe foil. Caption and foil were numbered and dis-\nplayed above each other to make differences more\napparent, with differing elements highlighted in\nboldface (Fig. 2, App. E). Annotators were not in-\nformed which text was the caption and which was\nthe foil, and captions appeared ﬁrst (numbered 1)\n50% of the time. The task was to determine which\nof the two texts accurately described what could be\nseen in the image. In each case, annotators had a\nforced choice between ﬁve options: a) the ﬁrst, but\nnot the second; b) the second, but not the ﬁrst; c)\nboth of them; d) neither of the two; and e) I cannot\ntell.\nEach item was annotated by three individuals.\nThe validation was conducted on Amazon Mechan-\nical Turk with a ﬁxed set of annotators who had\nqualiﬁed for the task. For details see App. E. For\nthe ﬁnal version of V ALSE, we include instances\nwhich passed the following validation test: at least\ntwo out of three annotators identiﬁed the caption,\nbut not the foil, as the text which accurately de-\nscribes the image. Across all instruments, 87.7%\nof the instances satisﬁed this criterion (min 77.3%;\nmax 94.6%), with 73.6% of instances overall hav-\ning a unanimous (3/3) decision that the caption,\nbut not the foil, was an accurate description. We\nconsider these ﬁgures high, suggesting that the au-\ntomatic construction and ﬁltering procedures yield\nfoils which are likely to be valid, in the sense dis-\ncussed in §4 above.\nWe compute inter-annotator agreement for each\ninstrument (Tab. 5). On the valid subset, agreement\nis low to medium (Krippendorff’s α: min=0.23,\nmax=0.64, mean=0.42, sd=0.12). We note that\nthere is considerable variation in the number of an-\nnotations made by individuals, and αis computed\nover 5 categories. Hence, this result cannot be\nstraightforwardly interpreted as a ceiling of human\nperformance for V ALSE. However,αis higher for\npieces on which models also perform better (e.g.\n8258\nexistence, Foil-It!; cf. §5).\n5 Benchmarking with V ALSE\nWe propose V ALSE as a task-independent, zero-\nshot benchmark to assess the extent to which mod-\nels learn to ground speciﬁc linguistic phenomena as\na consequence of their pretraining (or ﬁne-tuning).\nV ALSE is built in the spirit of approaches such\nas Checklist (Ribeiro et al., 2020), including pairs\nconsisting of captions and minimally edited foils.\nThe only requirement to evaluate a model on\nour benchmark is: i) to have a binary classiﬁcation\nhead to predict whether an image-sentence pair is\nfoiled, or ii) to predict an image-sentence matching\nscore between the image and the caption vs. the foil,\nreturning the pair with the highest score. Systems\nreporting results on V ALSE are expected to report\nany data used in model training prior to testing on\nV ALSE, for comparability.\n5.1 Benchmark Metrics\nWe employ ﬁve metrics 6 for evaluation: over-\nall accuracy (acc) on all classes (foil and cor-\nrect); precision (pc) measuring how well mod-\nels identify the correct examples; foil precision\n(pf ) measuring how well foiled cases are identi-\nﬁed; pairwise ranking accuracy (accr), which\nmeasures whether the image-sentence alignment\nscore is greater for a correct image-text pair than\nfor its foiled pair; and area under the receiver\noperating characteristic curve (AUROC), which\nmeasures how well models distinguish correct vs.\nfoiled examples across different prediction thresh-\nolds. accr is more permissive than accas it accepts\nmodel predictions if the score for a foil is lower\nthan the caption’s score. Our main metrics are AU-\nROC and accr. accr gives results for a pair ⟨image,\ncaption⟩and ⟨image, foil⟩. Both AUROC and accr\nare well suited to evaluate minimally-edited pairs\nas neither uses a classiﬁcation threshold. As for pc\nand pf , since these are competing metrics where\nnaively increasing one can decrease the other, we\nreport the smaller of the two as an indicator of\nhow informed model predictions are. Since all in-\nstruments are implemented as a balanced binary\nclassiﬁcation, the random baseline is always 50%.\n5.2 V&L models\nWe benchmark ﬁve V&L models on V ALSE: CLIP\n(Radford et al., 2021), LXMERT (Tan and Bansal,\n6All metrics are deﬁned in Appendix B.\n2019), ViLBERT (Lu et al., 2019), ViLBERT 12-\nin-1 (Lu et al., 2020), and VisualBERT (Li et al.,\n2019). These models have different architectures\nand are pretrained on a variety of tasks with differ-\nent training data. We also benchmark two unimodal\ntext-only models, GPT1 (Radford et al., 2018) and\nGPT2 (Radford et al., 2019). See Appendix D for\ndetails on all these models used in our evaluation.\nUnimodal models GPT1 and GPT2 are autore-\ngressive language models pretrained on English\ntext. We test whether V ALSE is solvable by these\nunimodal models by computing the perplexity of\nthe correct and foiled caption and predicting the\nentry with the lowest perplexity. If the perplexity\nis higher for the foil, we take this as an indication\nthat the foiled caption may suffer fromplausibility\nbias or other linguistic biases (cf. §4.2).\n5.3 Experiments and Results\nWe test V&L and unimodal models on V ALSE in a\nzero-shot setting, and also evaluate on a number of\ncorrect captions and foils from the FOIL it! dataset\n(Shekhar et al., 2017b) (cf. App. A.7 for details).\nAll results are listed in Table 2.\nUnimodal results For most instruments, uni-\nmodal results are close to random and hence do\nnot signal strong linguistic or plausibility biases.\nOne exception is the original FOIL it! dataset, in\nline with Madhyastha et al. (2019)’s ﬁndings. Also\nthe spatial relations (77.2%), action replacement\n(66.8%) and actant swap (76.9%) instruments sug-\ngest plausibility biases in foils. Such biases are\nhard to avoid in automatic foil generation for ac-\ntions due to the verb arguments’ selectional restric-\ntions, which are easily violated when ﬂipping role\nﬁllers, or replacing the verb. Similar considerations\nhold for relations: though SpanBERT proposals are\nintended to aid selection of likely replacements for\nprepositions, plausibility issues arise with relatively\nrare argument-preposition combinations.\nWhile these might be the ﬁrst instruments in\nV ALSE to be solved in the future, current V&L\nmodels struggle to detect even blatant mismatches\nof actant swap, e.g., ‘A ball throws a tennis player.’\nFor V ALSE, the unimodal scores will serve as a\nbaseline for the pairwise accuracy of V&L models.\nMultimodal results The best zero-shot results\nare achieved by ViLBERT 12-in-1 with the high-\nest scores across the board, followed by ViLBERT,\n8259\nMetric Model Existence Plurality Counting Sp.rel.‡ Action Coreference Foil-it! Avg.quantiﬁers number balanced sns.† adv.† relations repl.† actant swapstandard clean\nRandom 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\naccr\nGPT1∗ 61.8 53.1 51.2 48.7 69.5 77.2 65.4 72.2 45.6 45.2 77.5 60.7\nGPT2∗ 58.0 51.9 51.6 49.8 45.3 75.0 66.8 76.9 54.5 50.0 80.7 60.1\nCLIP 66.9 56.2 62.1 62.5 57.5 64.3 75.6 68.6 52.1 49.7 88.8 64.0\nLXMERT 78.6 64.4 62.2 69.2 42.6 60.2 54.8 45.8 46.8 44.2 87.1 59.6\nViLBERT 65.5 61.2 58.6 62.9 73.7 57.2 70.7 68.3 47.2 48.1 86.9 63.7\n12-in-1 95.6 72.4 76.7 80.2 77.3 67.7 65.9 58.9 75.7 69.2 86.9 75.1\nVisualBERT 39.7 45.7 48.2 48.2 50.0 39.7 49.2 44.4 49.5 47.6 48.5 46.4\nacc\nLXMERT 55.8 55.1 52.0 55.4 49.9 50.8 51.1 48.5 49.8 49.0 70.8 53.5\nViLBERT 2.4 50.3 50.7 50.6 51.8 49.9 52.6 50.4 50.0 50.0 55.9 51.3\n12-in-1 89.0 62.0 64.9 69.2 66.7 53.4 57.3 52.2 54.4 54.3 71.5 63.2\nVisualBERT 49.3 46.5 48.3 47.8 50.0 49.3 48.8 49.7 50.0 50.0 46.6 48.8\nmin(pc, pf)\nLXMERT 41.6 42.2 50.9 50.0 37.3 28.4 35.8 36.8 18.4 17.3 69.3 38.9\nViLBERT 47.9 2.1 24.4 24.7 17.5 1.5 11.9 7.1 1.3 1.9 12.9 13.9\n12-in-1 85.0 33.4 64.3 61.7 59.5 13.3 47.8 37.6 15.8 13.5 48.8 43.7\nVisualBERT 1.3 0.3 0.0 0.0 0.0 1.3 0.0 0.0 0.0 0.0 0.2 0.3\nAUROC\n×100\nLXMERT 60.5 57.3 53.8 57.7 50.5 51.9 52.1 47.6 49.8 49.5 76.9 55.2\nViLBERT 52.5 54.1 50.8 51.6 53.5 51.2 57.2 57.8 49.9 49.9 75.2 54.9\n12-in-1 96.3 67.4 72.0 77.8 75.1 55.8 61.3 55.0 59.8 59.6 81.0 69.2\nVisualBERT 28.9 29.0 24.5 16.5 20.9 45.2 17.7 36.3 45.3 46.3 28.5 30.8\nTable 2: Performance of unimodal and multimodal models on the V ALSE benchmark according to different metrics.\nWe bold-face the best overall result per metric, and underscore all results below (or at) the random baseline.accr is\na pairwise ranking accuracy where a prediction is considered correct ifp(caption,img) >p(foil,img ). Precision\npc and foil precision pf are competing metrics where naïvely increasing one can decrease the other: therefore\nlooking at the smaller number among the two gives a good intuition of how informed is a model prediction . †sns.\nCounting small numbers. adv. Counting adversarial. repl. Action replacement. ‡Sp.rel. Spatial relations.\n∗Unimodal text-only models that do not use images as input. CLIP is only tested in pairwise ranking mode (fn. 6).\nLXMERT, CLIP,7 and ﬁnally VisualBERT. The\nlatter obtains high pf but very low pc values—\nreﬂected in the min(pc,pf ) scores—indicating that\nVisualBERT learned a heuristic that does not gen-\neralise (see Hendricks and Nematzadeh, 2021, for\nsimilar observations with other models). We hy-\npothesise that this is due to the way image-sentence\nalignment is framed in VisualBERT’s pretraining:\nthe model expects an image and a correct sen-\ntence c1, and predicts whether a second sentence\nc2 is a match.8 During pretraining c1 and c2 are\nlikely to differ in many ways, whereas in our set-\nting, they are nearly identical. This may bias the\nmodel against predicting foils, which would raise\nthe value pf .\nInstruments centered on individual objects like\nexistence and theFOIL it! dataset are almost solved\nby ViLBERT 12-in-1, highlighting that models are\ncapable of identifying named objects and their pres-\nence in images. However, none of the remaining\npieces can be reliably solved in our adversarial\nfoiling settings: i) distinguishing references to sin-\ngle vs. multiple objects or counting them in an\n7CLIP works in a contrastive fashion, therefore we report\nonly accr (cf. Appendix D for details).\n8c1 is one of the 5 captions describing the relevant image\nin MSCOCO. During VisualBERT’s pretraining,c2 can be an\nalternative caption out of these 5, or a randomly drawn caption\nwhich does not describe the image. The pretraining task is to\ndetermine if c2 correctly describes the image or not.\nimage (plurality and counting); ii) correctly classi-\nfying a named spatial relation between objects in\nan image (relations); iii) distinguishing actions and\nidentifying their participants, even if supported by\npreference biases (actions); or, iv) tracing multiple\nreferences to the same object in an image through\nthe use of pronouns (coreference).\nCorrect vs. foil precision pc and pf show that\nV&L models struggle to solve the phenomena in\nV ALSE. When a model achieves high precision on\ncorrect captions pc this is often at the expense of\nvery low precision on foiled captions pf (cf. ViL-\nBERT), or vice-versa (cf. VisualBERT). This sug-\ngests that such models are insensitive to V ALSE’s\ninputs: models that almost always predict a match\nwill inﬂate pf at the expense of pc. min(pc,pf )\nreveals that VisualBERT and ViLBERT perform\npoorly and below random baseline, and LXMERT\nclose to or below it. ViLBERT 12-in-1 performs\nstrongly on existence, well on counting, but strug-\ngles on plurality, spatial relations, coreference, and\nactions. These tendencies we see reﬂected in our\nmain metrics, accr and AUROC.\n6 Conclusions and Future Work\nWe present the V ALSE benchmark to help the com-\nmunity improve V&L models by hard-testing their\nvisual grounding capabilities through the lens of lin-\n8260\nguistic constructs. Our experiments show that V&L\nmodels identify named objects and their presence\nin images well (as shown by the existence piece),\nbut struggle to ground their interdependence and re-\nlationships in visual scenes when forced to respect\nlinguistic indicators. We encourage the commu-\nnity to use V ALSE for measuring progress towards\nV&L models capable of true language grounding.\nFurthermore, V ALSE could be used as an indirect\nassessment of datasets, as models could be evalu-\nated before and after training or ﬁne-tuning to see\nif a dataset helps models improve on any of the\naspects tested by V ALSE.\nV ALSE is designed as a living benchmark. As\nfuture work we plan to extend it to further linguistic\nphenomena, and to source data from diverse V&L\ndatasets to cover more linguistic variability and\nimage distributions.\nAcknowledgments\nIC has received funding from the European Union’s\nHorizon 2020 research and innovation programme\nunder the Marie Skłodowska-Curie grant agree-\nment No 838188. AG and MC are supported by the\nEuropean Union’s Horizon 2020 research and in-\nnovation Programme under the Marie Skłodowska-\nCurie grant agreement No 860621. This collabora-\ntion was facilitated by the Multi3Generation COST\nAction CA18231.\nReferences\nVedika Agarwal, Rakshith Shetty, and Mario Fritz.\n2020. Towards causal vqa: Revealing and reduc-\ning spurious correlations by invariant and covariant\nsemantic editing. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 9690–9698.\nArjun Akula, Spandana Gella, Yaser Al-Onaizan, Song-\nChun Zhu, and Siva Reddy. 2020. Words aren’t\nenough, their order matters: On the robustness of\ngrounding visual referring expressions. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 6555–6565,\nOnline. Association for Computational Linguistics.\nRaffaella Bernardi and Sandro Pezzelle. 2021. Linguis-\ntic issues behind visual question answering. Lan-\nguage and Linguistics Compass, 15(6):1–25.\nYonatan Bitton, Gabriel Stanovsky, Roy Schwartz, and\nMichael Elhadad. 2021. Automatic generation of\ncontrast sets from scene graphs: Probing the compo-\nsitional consistency of GQA. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 94–105, Online.\nAssociation for Computational Linguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nJize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun\nChen, and Jingjing Liu. 2020. Behind the scene:\nRevealing the secrets of pre-trained vision-and-\nlanguage models. arXiv preprint arXiv:2005.07310.\nXinlei Chen, Hao Fang, Tsung-yi Lin, Ramakrishna\nVedantam, C Lawrence Zitnick, Saurabh Gupta,\nand Piotr Doll. 2015. Microsoft COCO Captions\n: Data Collection and Evaluation Server. arXiv,\n1504.00325:1–7.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In ECCV.\nV olkan Cirik, Louis-Philippe Morency, and Taylor\nBerg-Kirkpatrick. 2018. Visual referring expression\nrecognition: What do systems actually learn? In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 2 (Short Papers), pages 781–787, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nAbhishek Das, Satwik Kottur, Khushi Gupta, Avi\nSingh, Deshraj Yadav, José M.F. Moura, Devi\nParikh, and Dhruv Batra. 2017. Visual Dialog. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nChristiane Fellbaum. 1998. WordNet: An Electronic\nLexical Database. Bradford Books.\nMatt Gardner, Yoav Artzi, Victoria Basmov, Jonathan\nBerant, Ben Bogin, Sihao Chen, Pradeep Dasigi,\nDheeru Dua, Yanai Elazar, Ananth Gottumukkala,\nNitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco,\nDaniel Khashabi, Kevin Lin, Jiangming Liu, Nel-\nson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer\nSingh, Noah A. Smith, Sanjay Subramanian, Reut\nTsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou.\n8261\n2020. Evaluating models’ local decision boundaries\nvia contrast sets. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1307–1323, Online. Association for Computational\nLinguistics.\nSahaj Garg, Vincent Perot, Nicole Limtiaco, Ankur\nTaly, Ed H. Chi, and Alex Beutel. 2019. Counterfac-\ntual fairness in text classiﬁcation through robustness.\nIn Proceedings of the 2019 AAAI/ACM Conference\non AI, Ethics, and Society, AIES ’19, page 219–226,\nNew York, NY , USA. Association for Computing\nMachinery.\nAlbert Gatt and Ehud Reiter. 2009. SimpleNLG: A re-\nalisation engine for practical applications. In Pro-\nceedings of the 12th European Workshop on Natural\nLanguage Generation (ENLG 2009) , pages 90–93,\nAthens, Greece. Association for Computational Lin-\nguistics.\nGabriel Goh, Nick Cammarata †, Chelsea V oss †,\nShan Carter, Michael Petrov, Ludwig Schubert,\nAlec Radford, and Chris Olah. 2021. Multi-\nmodal neurons in artiﬁcial neural networks. Distill.\nHttps://distill.pub/2021/multimodal-neurons.\nTejas Gokhale, Pratyay Banerjee, Chitta Baral, and\nYezhou Yang. 2020. MUTANT: A training\nparadigm for out-of-distribution generalization in vi-\nsual question answering. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 878–892, Online.\nAssociation for Computational Linguistics.\nYash Goyal, Tejas Khot, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. 2017. Making the\nv in vqa matter: Elevating the role of image under-\nstanding in visual question answering. In Proceed-\nings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 6904–6913.\nSuchin Gururangan, Swabha Swayamdipta, Omer\nLevy, Roy Schwartz, Samuel Bowman, and Noah A.\nSmith. 2018. Annotation artifacts in natural lan-\nguage inference data. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 107–112, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nFeijuan He, Yaxian Wang, Xianglin Miao, and Xia Sun.\n2021. Interpretable visual reasoning: A survey. Im-\nage and Vision Computing, 112:104194.\nLisa Anne Hendricks and Aida Nematzadeh. 2021.\nProbing Image-Language Transformers for Verb Un-\nderstanding. arXiv, 2106.09141.\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2021–2031, Copenhagen, Denmark. Association for\nComputational Linguistics.\nRobin Jia, Aditi Raghunathan, Kerem Göksel, and\nPercy Liang. 2019. Certiﬁed robustness to adver-\nsarial word substitutions. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4129–4142, Hong Kong,\nChina. Association for Computational Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64–77.\nKushal Kaﬂe, Robik Shrestha, and Christopher Kanan.\n2019. Challenges and prospects in vision and lan-\nguage research. Frontiers in Artiﬁcial Intelligence ,\n2:28.\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021.\nVilt: Vision-and-language transformer without con-\nvolution or region supervision. arXiv preprint\narXiv:2102.03334.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning.\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, and\nDaxin Jiang. 2020a. Unicoder-vl: A universal en-\ncoder for vision and language by cross-modal pre-\ntraining. In The Thirty-Fourth AAAI Conference\non Artiﬁcial Intelligence, AAAI 2020, The Thirty-\nSecond Innovative Applications of Artiﬁcial Intelli-\ngence Conference, IAAI 2020, The Tenth AAAI Sym-\nposium on Educational Advances in Artiﬁcial Intel-\nligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 11336–11344. AAAI Press.\nLinjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-\nChun Chen, Rohit Pillai, Yu Cheng, Luowei Zhou,\nXin Eric Wang, William Yang Wang, et al. 2021.\nValue: A multi-task benchmark for video-and-\nlanguage understanding evaluation. arXiv preprint\narXiv:2106.04632.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A\nsimple and performant baseline for vision and lan-\nguage. In Arxiv.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, et al. 2020b. Oscar: Object-\nsemantics aligned pre-training for vision-language\ntasks. In European Conference on Computer Vision,\npages 121–137. Springer.\n8262\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C. Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In Computer Vision –\nECCV 2014, pages 740–755, Cham. Springer Inter-\nnational Publishing.\nNicholas Lourie, Ronan Le Bras, Chandra Bhagavatula,\nand Yejin Choi. 2021. Unicorn on rainbow: A uni-\nversal commonsense reasoning model on a new mul-\ntitask benchmark. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence , 15, pages 13480–\n13488.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems, pages 13–23.\nJiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi\nParikh, and Stefan Lee. 2020. 12-in-1: Multi-task\nvision and language representation learning. In The\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR).\nPranava Madhyastha, Josiah Wang, and Lucia Specia.\n2019. VIFIDEL: Evaluating the visual ﬁdelity of\nimage descriptions. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 6539–6550, Florence, Italy. Asso-\nciation for Computational Linguistics.\nPranava Swaroop Madhyastha, Josiah Wang, and Lu-\ncia Specia. 2018. Defoiling foiled image captions.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 2 (Short Papers), pages 433–438, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nYixin Nie, Haonan Chen, and Mohit Bansal. 2019.\nCombining fact extraction and veriﬁcation with neu-\nral semantic matching networks. In Association for\nthe Advancement of Artiﬁcial Intelligence (AAAI).\nYixin Nie, Adina Williams, Emily Dinan, Mohit\nBansal, Jason Weston, and Douwe Kiela. 2020. Ad-\nversarial NLI: A new benchmark for natural lan-\nguage understanding. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 4885–4901, Online. Association\nfor Computational Linguistics.\nLetitia Parcalabescu, Albert Gatt, Anette Frank, and\nIacer Calixto. 2021. Seeing past words: Testing\nthe cross-modal capabilities of pretrained v&l mod-\nels on counting tasks. In Proceedings of the 1st\nWorkshop on Multimodal Semantic Representations\n(MMSR), pages 32–44, Groningen, Netherlands (On-\nline). Association for Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nBryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana\nLazebnik. 2015. Flickr30k entities: Collecting\nregion-to-phrase correspondences for richer image-\nto-sentence models. In Proceedings of the IEEE\ninternational conference on computer vision , pages\n2641–2649.\nVinodkumar Prabhakaran, Aida Mostafazadeh Davani,\nand Mark Díaz. 2021. On releasing annotator-level\nlabels and information in datasets.\nSarah Pratt, Mark Yatskar, Luca Weihs, Ali Farhadi,\nand Aniruddha Kembhavi. 2020. Grounded situa-\ntion recognition. In Computer Vision - ECCV 2020 -\n16th European Conference, pages 314–332.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. arXiv preprint\narXiv:2103.00020.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Be-\nhavioral testing of NLP models with CheckList. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4902–\n4912, Online. Association for Computational Lin-\nguistics.\nDaniel Rosenberg, Itai Gat, Amir Feder, and Roi Re-\nichart. 2021. Are VQA systems RAD? Measuring\nrobustness to augmented data with focused interven-\ntions. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and\nthe 11th International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers),\npages 61–70, Online. Association for Computational\nLinguistics.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for au-\ntomatic image captioning. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8263\n2556–2565, Melbourne, Australia. Association for\nComputational Linguistics.\nRavi Shekhar, Sandro Pezzelle, Aurélie Herbelot, Moin\nNabi, Enver Sangineto, and Raffaella Bernardi.\n2017a. Vision and language integration: Moving be-\nyond objects. In IWCS 2017 — 12th International\nConference on Computational Semantics — Short\npapers.\nRavi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Au-\nrélie Herbelot, Moin Nabi, Enver Sangineto, and\nRaffaella Bernardi. 2017b. FOIL it! ﬁnd one mis-\nmatch between image and language caption. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 255–265, Vancouver, Canada. Asso-\nciation for Computational Linguistics.\nRavi Shekhar, Ece Takmaz, Raquel Fernández, and\nRaffaella Bernardi. 2019a. Evaluating the represen-\ntational hub of language and vision models. In Pro-\nceedings of the 13th International Conference on\nComputational Semantics - Long Papers, pages 211–\n222, Gothenburg, Sweden. Association for Compu-\ntational Linguistics.\nRavi Shekhar, Aashish Venkatesh, Tim Baumgärtner,\nElia Bruni, Barbara Plank, Raffaella Bernardi, and\nRaquel Fernández. 2019b. Beyond task success: A\ncloser look at jointly learning to see, ask, and Guess-\nWhat. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n2578–2587, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. Vl-bert: Pre-\ntraining of generic visual-linguistic representations.\nIn International Conference on Learning Represen-\ntations.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019. A corpus for\nreasoning about natural language grounded in pho-\ntographs. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6418–6428, Florence, Italy. Association for\nComputational Linguistics.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100–5111, Hong Kong, China. Association for\nComputational Linguistics.\nEric Wallace, Matt Gardner, and Sameer Singh. 2020.\nInterpreting predictions of NLP models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: Tutorial Abstracts,\npages 20–23, Online. Association for Computational\nLinguistics.\nChenguang Wang, Xiao Liu, and Dawn Song. 2020a.\nLanguage models are open knowledge graphs.\narXiv preprint arXiv:2010.11967.\nJunlin Wang, Jens Tuyls, Eric Wallace, and Sameer\nSingh. 2020b. Gradient-based analysis of NLP mod-\nels is manipulable. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020 ,\npages 247–258, Online. Association for Computa-\ntional Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Julien Chaumond, Lysandre Debut, Vic-\ntor Sanh, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Morgan Funtowicz, Joe Davison, Sam\nShleifer, et al. 2020. Transformers: State-of-the-\nart natural language processing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 38–45.\nNing Xie, Farley Lai, Derek Doran, and Asim Kadav.\n2019. Visual Entailment: A Novel Task for Fine-\nGrained Image Understanding. arXiv, 1901.06706.\nMark Yatskar, Luke Zettlemoyer, and Ali Farhadi.\n2016. Situation recognition: Visual semantic role\nlabeling for image understanding. In Proceedings of\nthe IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR).\nWanzheng Zhu and Suma Bhat. 2020. GRUEN for\nevaluating linguistic quality of generated text. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020 , pages 94–108, Online. As-\nsociation for Computational Linguistics.\nYuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-\nFei. 2016. Visual7W: Grounded Question Answer-\ning in Images. In IEEE Conference on Computer\nVision and Pattern Recognition.\n8264\nA Benchmark creation\nA.1 Existence\nThe existence piece has a single instrument and tar-\ngets instances with existential quantiﬁers. Mod-\nels need to differentiate between examples i) where\nthere is no entity of a certain type or ii) where there\nis one or more of these entities visible in an image.\nData sources We use the Visual7W visual ques-\ntion answering dataset (Zhu et al., 2016) to source\nexamples, starting with the ‘how many’ questions\nin Visual7W and building a pool of those whose\nanswers are numerals (e.g., 0, 1, 2, etc.). We use\nthe templates from Parcalabescu et al. (2021) to\ntransform question and answer ﬁelds into a declara-\ntive statement that correctly describes what can be\nseen in the image, e.g., ‘Q: How many animals are\nshown? A: 0’→‘There are 0 animals shown’.\nFoiling method Let us use x = ‘There are N\nanimals shown’ as a running example for a cor-\nrect caption, where N is a number. If N >0, we\nsimply remove N from the sentence, effectively\ncreating the statement ∃x or ‘There are animals\nshown’. IfN = 0, we replace N by ‘no’, creating\nthe statement ¬∃xor ‘There are no animals shown’.\nIf necessary, we ﬁx singular–plural agreement. To\ncreate data with balanced correct and foil classes,\nwe select 50% of our examples from those where\nthe correct answer is originally 0, and the remain-\ning 50% from those where the correct answer is\nany other number (e.g., 1, 2, etc.). To create foils,\nwe then simply convert the statement from ∃xto\n¬∃x, and vice-versa.\nA.2 Plurality\nThe plurality piece has a single instrument, con-\ncerned with semantic number, that is, the distinc-\ntion between single entities in an image (‘exactly\none ﬂower’) and multiple instances of the same\ntype (‘some ﬂowers’). In this piece, foil candidates\nare created either by converting a singular NP and\nits coreferents to a plural, or vice versa.\nData sources The data was sourced from the val-\nidation split of the COCO 2017 dataset (Chen et al.,\n2015). Captions are only foiled if their length after\ntokenization with the pretrained BERT tokenizer9\nis of 80 tokens or less. This is done to minimise\nthe risk that captions and foils need to be truncated\n9We use the bert-large-cased pretrained tokenizer\ndistributed as part of the transformers python library.\nto accommodate the input speciﬁcations of current\npretrained V&L models.\nFoiling method Foiling is done in two directions:\nsingular-to-plural ( sg2pl) or plural-to-singular\n(pl2sg). Given a caption, NP chunking is applied\nto identify all non-pronominal NPs. In the sg2pl\ncase, a foiled version of a caption containing a sin-\ngular NP is created by pluralising the head noun.\nWe automatically identify anaphoric expressions\ncoreferring to the singular NP within the caption\nand pluralise them in the same way. For NPs which\nare subjects of copular VPs or VPs with an auxil-\niary requiring subject-verb number agreement (e.g.\n‘N is V’), we also pluralise the verb. Note that\nthis procedure creates a potential foil for every sin-\ngular NP in the caption; thus, more than one foil\ncandidate can be created for each instance in the\nsource dataset.10 In the pl2sg case, the same pro-\ncedure is carried out, but turning a plural NP, as\nwell as its coreferents, into a singular. We generate\nall foil candidates using the Checklist framework\n(Ribeiro et al., 2020), within which we implement\nour procedures for data perturbation.\nAn important consideration, especially in the\npl2sg case, is that singularising an NP in a foil\ncan still be truth-preserving. Speciﬁcally, a caption\nwith a plural NP, such as ‘A small copper vase with\nsome ﬂowers in it’, arguably still entails the ver-\nsion with the singular ‘(. . . )a ﬂower’. As a result,\nthe singular version may still correctly be judged\nto match the image. One way around this problem\nis to insert a quantiﬁer in the singular NP which\nmakes it explicit that exactly one instance and no\nmore is intended (e.g. ‘exactly one ﬂower’). This\nmay however result in a biased dataset, with such\nsingular quantiﬁers acting as signals for singular\nfoils and enabling models to solve the task with\nno grounding in the visual information. We avoid\nthis by adopting a uniform strategy for bothsg2pl\nand pl2sg. We determine two singular quantiﬁers\n(‘exactly one N’ and ‘a single N’) and two plural\nquantiﬁers (‘some N’, ‘a number of N’). When a\nfoil candidate is generated, we alter theoriginal NP\nby inserting one of the two quantiﬁers matching\nits semantic number, and generate a foil with one\n10NP chunking is performed using the Spacy v.3 pipeline\nfor English using the en_core_web_md pretrained mod-\nels. Coreference chains are detected using the pretrained En-\nglish model for Coreferee (github.com/msg-systems/\ncoreferee). Pluralisation of head nouns is carried\nout using the inflect engine (github.com/jaraco/\ninflect/).\n8265\nof the two quantiﬁers for the other number. In the\nforegoing example, we end up with ‘A small copper\nvase with some ﬂowers / exactly one ﬂower in it.’\nAfter generating all candidate foils, in both direc-\ntions, we use the GRUEN pretrained model (Zhu\nand Bhat, 2020) to score the foils for grammat-\nicality. We only keep foils with a score ≥ 0.8,\nand run each foil-caption pair through the NLI\nmodel described in Section 4.3, keeping only pairs\nwhose predicted label is contradiction, for an ini-\ntial candidate set of 1000 cases (500 sg2pl and\n500 pl2sg), of which 851 (85.1%) are considered\nvalid following manual validation (see §4.4). Fig-\nure 4 shows the distribution of nouns in captions\nand foils, before and after the validation. Note that\nthe validation process does not result in signiﬁcant\nchange to the distributions.\nA.3 Counting\nThe counting piece comes in three instruments:\nbalanced, adversarial and small numbers. All\nthree instruments include instances with statements\nabout the number of entities visible in an image .\nThe model needs to differentiate between exam-\nples where the speciﬁc number of entities in the\nassociated image is correct or incorrect, given the\nstatement.\nAll three instruments are designed to show\nwhether models learn strategies that generalize be-\nyond the training distribution, and to what extent\na model exploits class frequency bias.11 In count-\ning balanced we cap the number of examples to\na maximum per class and make sure correct/foil\nclasses are balanced, so that models that exploit\nclass frequency bias are penalized. In counting\nadversarial we make sure that all foils take class\nn∈{0,1,2,3}, whereas all correct captions take\nclass n ∈ {n|n ≥4}. Biased models are ex-\npected to favour more frequent classes and these\ncorrespond to smaller numbers, therefore models\nthat resort to such biases should perform poorly on\nthis adversarially built test. Instrument counting\nsmall numbers is a sanity check where all correct\ncaptions and foils have class n∈{0,1,2,3}, and\ncaption/foil classes are balanced. Models likely\nhave been exposed to many examples in this class\nset, so with this instrument we assess model per-\nformance certain it does not suffer from (class)\nexposure bias.\n11We take the original answer in Visual7W as the example\nclass. E.g., in There are four zebras, the class is 4.\nData sources We use the Visual7W visual ques-\ntion answering dataset (Zhu et al., 2016) and source\nits ‘how many’ examples, building a pool of those\nwhose answers are numerals (e.g., 0, 1, 2, etc.). We\nuse the templates from Parcalabescu et al. (2021) to\ntransform question and answer ﬁelds into a declara-\ntive statement that correctly describes what can be\nseen in the image.\nFoiling method We create foils by directly re-\nplacing the numeral in the correct caption by an-\nother numeral. When creating foils we make sure\nthat the class distribution for correct and foiled cap-\ntions are approximately the same, i.e., there are a\nsimilar number of correct and foiled examples in\neach class in each instrument. The only exception\nis the counting adversarial instrument, where the\nclasses used in correct and foiled captions are dis-\njoint, i.e., n ∈{0,1,2,3}and n ∈{n|n ≥4},\nrespectively. See Figure 3 for a visualisation of\nthese distributions.\nA.4 Spatial relations\nThe relations piece has one instrument and focuses\non the ability of models to distinguish between dif-\nferent spatial relations, as expressed by preposi-\ntions. Foils therefore consist of captions identical\nto the original except for the replacement of a spa-\ntial preposition.\nData sources Data was sourced from the COCO\n2017 validation split (Chen et al., 2015). To gen-\nerate foil candidates, we ﬁrst extracted from the\noriginal COCO captions all the sequences consist-\ning of one or more consecutive prepositions (e.g.,\n‘on’ or ‘out of’). Foils are generated by detecting\nthese preposition spans, and replacing them with\nanother preposition span attested in the list.\nFoiling method To generate foils, we mask the\npreposition span in an original caption, and use\nSpanBERT (Joshi et al., 2020), a pretraining\nmethod based on BERT (Devlin et al., 2019). 12\nThe advantage of SpanBERT over BERT is that in\na masked language modelling context, with masks\nspanning more than a single word, SpanBERT pre-\ndicts sequences and takes into account their joint\nprobability, whereas BERT trained with standard\nMasked Language Modelling can only predict sin-\ngle tokens independently. With SpanBERT, we\n12We use SpanBERT with the pretrained\nbert-large-cased model distributed as part of\nthe transformers Python library.\n8266\ngenerate replacements of between 1 and 3 tokens\nin length, in each case retaining only the best pre-\ndiction out of the top kwhich matches one of the\npreposition sequences in the pre-extracted list.\nAfter all candidates are generated, we apply\nGRUEN (Zhu and Bhat, 2020) to score the foils for\ngrammaticality, and further apply the NLI model\ndescibed in Section 4.3 to label the entailment rela-\ntionship between caption and foil pairs. From the\nresulting data, we sample as follows: i) we keep\nonly caption-foil pairs labelled as contradiction,\nwhere the GRUEN grammaticality score is ≥0.8;\nii) for every caption-foil pair sampled where pis\nreplaced with q, we search for another caption-foil\npair where q is replaced with p, if present. This\nstrategy yields a roughly balanced dataset, where\nno single preposition or preposition sequence is\nover-represented in captions or foils.\nThese processes result in an initial set of 614\ncases, of which 535 (87.1%) are selected following\nmanual validation described in §4.4.\nFigure 3 shows proportions in captions and foils\nof the prepositions. E.g.: ‘A cat plays with a pocket\nknife on / underneath a table.’\nAs with plurals, we implement procedures\nfor foil candidate generation by extending the\nperturb functionality in Checklist (Ribeiro et al.,\n2020).\nA.5 Actions\nThe action piece consists of two instruments: i) ac-\ntion replacement and ii) actant swap. They are\ntesting a V&L model’s capability of i) identifying\nwhether an action mentioned in the textual modal-\nity matches the action seen in the image or not\n(e.g. ‘a man shouts / smiles at a woman’) and ii)\ncorrectly identifying the participants of an action\nand the roles they are playing in it (e.g., given the\npicture in Table 1: is it the man or the woman who\nshouts?).\nData source For creating interesting foils withdi-\nverse actions, we focus on the SWiG dataset (Pratt\net al., 2020) that comprises 504 action verbs anno-\ntated with semantic roles and their ﬁllers, which are\ngrounded in images of the imSitu dataset (Yatskar\net al., 2016). We generate English captions for\nthe images using SimpleNLG (Gatt and Reiter,\n2009)13. For generation we use the speciﬁed ac-\n13SimpleNLG is a surface realization engine that – given\nsome content and crucial syntactic speciﬁcations – performs\nsurface generation including morphological adjustments.\ntion verb, the realized FrameNet semantic roles\nand their annotated ﬁller categories (see Table 1\nfor shout: AGENT : man, ADDRESSEE : woman),\nand generate short captions, with realization of two\nroles in active form. We apply various ﬁlters to\nensure high quality of the generated captions using\ndiverse metrics14 and manual checks through AMT\ncrowdsourcing.\nFoiling method When creating the action re-\nplacement instrument, we need to make sure that\nthe action replacement suits the context. We pro-\npose action replacements with BERT (Devlin et al.,\n2019) that need to satisfy three conditions: 1) the\nproposed action verbs originate from the SWiG\ndataset – otherwise new verbs are introduced on\nthe foil side only, which may induce biases; 2) the\nfrequency distribution of action verbs on the cap-\ntion and on the foil side is approximately the same\n(cf. Figure 4); 3) we constrain the replacement\nverbs to be either antonyms of the original verb\nor at least not synonyms, hyponyms or hypernyms\nto the original, according to WordNet (Fellbaum,\n1998) in order to avoid situations where replace-\nments are almost synonymous to the original action.\nThe actant swap instrument is based on the origi-\nnal image annotations, but swaps the two role ﬁllers\n(e.g., ‘A woman shouts at the man.’ for the image\nin Table 1). To avoid agreement mistakes, wegen-\nerate these foils using the inverted role ﬁllers as\ninput.\nWe plot caption and foil word frequency distribu-\ntions for action replacement in Figure 4. We do not\nplot statistics for the actant swap instrument since\nby construction it cannot suffer from distributional\nbias since caption and foil contain the same words\nup to a permutation.\nA.6 Coreference\nThe coreference piece consists of two pieces:\ncoreference standard and coreference clean. It\naims to uncover whether V&L models are able to\nperform pronoun coreference resolution. The coref-\nerence phenomenon encompasses both cases where\ni) the pronoun refers to a noun (phrase) and both\nthe pronoun and the (noun) phrase are grounded\n14We use the GRUEN metric (Zhu and Bhat, 2020) that\nscores grammaticality, naturalness and coherence of genera-\ntions and compute perplexity with GPT-2 to rank alternative\noutputs. We determined appropriate thresholds based on man-\nual judgements of acceptability and chose the highest-ranked\ncandidates. The ﬁnal data quality is controlled by crowd-\nsourced annotation with AMT.\n8267\nin the visual modality (e.g. ‘A woman is driving a\nmotorcycle. Is she wearing a helmet?’), and cases\nwhere ii) the pronoun refers directly to a region in\nthe image or even to the whole image (e.g. ‘A man\nis sitting on a bench. Is this outside?’).\nData source We source the data from VisDial\nv1.0 (Das et al., 2017), which contains images\nfrom MSCOCO (Lin et al., 2014), their captions\nand dialogues about the images in form of Q&A\nsequences. To ensure that the coreference phe-\nnomenon is present in the [ Caption. Question?\nYes/No.] formulations, we check whether pronouns\nare present in the question. The list of pronouns\nand their frequencies in our train-val-test splits are\nrepresented in Figure 1.\nThe coreference standard instrument contains\n916 data samples (708 are valid 15) from the Vis-\nDial’s training set. The data ofcoreference clean\ninstrument consisting of 141 samples (104 are\nvalid), originates from VisDial’s validation set.\nWith models that have been trained on VisDial,\nwe would be in the situation where models are\ntested on their training data. Therefore we also\nhave the coreference clean instrument based on\nthe validation set of VisDial to test models safely.\nUnfortunately, we cannot use VisDial’s test set be-\ncause the required question-answers annotations\nnecessary for foiling are withheld.\nFoiling method When foiling, we take the im-\nage description of the form [ Caption. Question?\nYes/No.] and exchange the answer: yes →no and\nvice-versa (see example in Table 1). This way, we\nkeep the full textual description including pronoun\nand noun (phrase) intact, hence ensuring that the\ncoreference phenomenon is present and valid in the\nfoil too, and rely on the model to interpret afﬁr-\nmation and negation correctly. Note that we rely\non the capability of models to correctly interpret\nnegation also in the existence piece (cf. §3.1).\nArguably, coreference is the most difﬁcult phe-\nnomenon to foil in V ALSE. Especially in cases\nwhere pronouns refer to a noun (phrase) (e.g.,\n‘A woman is driving a motorcycle. Is she wear-\ning a helmet? Yes.’), exchanging the pronoun with\nanother pronoun would generate incoherent and un-\nlikely sequences16 (e.g., ‘A woman is driving a mo-\n15The majority of manual annotators validated that the cap-\ntion describes the image but the foil does not.\n16Even more, the possibilities of exchanging pronouns with\npronouns in grammatical ways are very limited: she – he but\nnot she – they / her / their.\nFigure 1: Normalized pronoun frequencies in the coref-\nerence subset.\ntorcycle. Is he wearing a helmet?’), and exchanging\nit with a noun phrase would furthermore break the\npronoun coreference phenomenon because there\nwould be no pronoun anymore (e.g., ‘A woman is\ndriving a motorcycle. Is the man wearing a hel-\nmet?’). Therefore when foiling the coreference\npiece, we aim to keep the original description in-\ntact for ensuring the preservation of the coreference\nphenomenon. Hence we rely on the answers con-\ntaining yes or no17 and exchange afﬁrmative to\nnegative answers and vice-versa.\nA.7 FOIL it! data\nWe include an additional piece in V ALSE consist-\ning of 1000 randomly sampled entries from the\nFOIL it! dataset (Shekhar et al., 2017b). Each\nentry in FOIL it! consists of an MSCOCO (Lin\net al., 2014) image and a foiled caption where a\nnoun phrase depicting an object visible in the im-\nage was replaced by a semantically related noun\nphrase. Since examples in the FOIL it! dataset are\nlinked to MSCOCO, we use these links to retrieve\none correct caption from the ﬁve captions available\nfor the image, and create an image–caption–foil\ntriple. From the original 1000 entries, 943 have\nbeen validated by our manual annotation proce-\ndure (in Appendix E). Please refer to Shekhar et al.\n(2017b) for more details.\nB Evaluation metrics\nWe evaluate pretrained V&L models on V ALSE\nusing accuracy (acc), the overall accuracy on all\nclasses; precision or positive predictive value (pc),\nwhich measures the proportion of correctly identi-\nﬁed correct captions; and foil precisionor negative\npredictive value (pf ), which measures the propor-\ntion of correctly identiﬁed foiled examples; pair-\nwise ranking accuracy accr, computed using the\nimage-sentence alignment score φthat the model\nassigns to correct and foiled image-text pairs; and\n17If the answer is longer than just yes/no (e.g., ‘Yes, she is’)\nwe shorten it to yes/no.\n8268\narea under the receiver operating characteris-\ntic curve (AUROC)—a classic metric used in ma-\nchine learning classiﬁcation problems—which in\nour case measures how well models distinguish\ncorrect vs. foiled examples across different predic-\ntion thresholds. The AUROC has a probabilistic\ninterpretation and can be understood as the prob-\nability that a model will assign a higher score to\na randomly chosen correct example relative to a\nrandomly chosen foil.\nWith accr, a prediction is considered successful,\nif given an image (i) paired with a correct (c) versus\na foil (f) text, the score of the positive/correct pair\nis greater than that of the foiled pair.\naccr =\n∑\n(i,c)∈C\n∑\nf∈F s(i,c,f )\n|C|+ |F| ,\ns(i,c,f ) =\n{\n1, if φ(i,f) ≤φ(i,c),\n0, otherwise,\nwhere C is the set of correct image-caption pairs\n(i,c), and F is the set of foils for the pair (i,c).\nThe pairwise accuracy accr is important for\ntwo reasons: First, it enables V&L models to be\nevaluated on V ALSE without a binary classiﬁcation\nhead for classifying image-sentence pairs as correct\nor foiled. For example, CLIP (Radford et al., 2021)\nis a model that computes a score given an image-\nsentence pair. This score can be used to compare\nthe scores of a correct image-sentence pair and the\ncorresponding foiled pair. By contrast, a model\nlike LXMERT (Tan and Bansal, 2019) has a binary\nimage-sentence classiﬁcation head and can predict\na correct pair independently of the foiled pair (and\nvice-versa). Second, accr enables the evaluation of\nunimodal models on V ALSE, as motivated in §4.2.\nIn Table 4, we show results for all models investi-\ngated according to all above-mentioned metrics.\nC Filtering methods\nNLI ﬁltering For NLI ﬁltering we make use of\nthe HuggingFace (Wolf et al., 2020) implementa-\ntion of ALBERT (xxlarge-v2) that was already ﬁne-\ntuned on the concatenation of SNLI (Bowman et al.,\n2015), MultiNLI (Williams et al., 2018), FEVER-\nNLI (Nie et al., 2019) and ANLI datasets (Nie\net al., 2020). The model is the best performing on\nthe ANLI benchmark leaderboard18 and it achieves\n90% accuracy on MultiNLI devset.\n18github.com/facebookresearch/anli\nD Vision & Language and Unimodal\nModels\nIn Table 3 we summarise the ﬁve V&L models used\nin our experiments, their architecture, pretraining\ntasks and data, and ﬁne-tuning tasks (if any).\nCLIP CLIP (Radford et al., 2021) is composed\nof two transformer-based text and an image en-\ncoders. These are jointly trained on 400M image-\ntext pairs through contrastive learning for predict-\ning high scores for paired image-text examples and\nlow scores when image-text samples are not paired\nin the dataset. CLIP has shown zero-shot capa-\nbilities in e.g. object classiﬁcation, OCR, activity\nrecognition (Radford et al., 2021). Goh et al. (2021)\nhave shown the existence of multimodal neurons\nin CLIP, responding to the same topic regardless of\nwhether it is represented in an image, drawing or\nhandwritten text. We use CLIP’s image-text align-\nment scores for benchmarking on V ALSE: Given\nan image, we compare whether CLIP 19 predicts\nhigher image-text similarity for the correct or for\nthe foiled caption.\nLXMERT LXMERT (Tan and Bansal, 2019) is\na dual-stream transformer model combining V&L\nthrough cross-modal layers. It is pretrained on\nMSCOCO (Lin et al., 2014) and on multiple VQA\ndatasets for (i) multimodal masked word and object\nprediction, (ii) image-sentence alignment, i.e., de-\ntermining whether a text corresponds to an image\nor not, and (iii) question-answering. For bench-\nmarking on V ALSE, we use LXMERT’s20 image-\nsentence alignment head.\nViLBERT and ViLBERT 12-in-1 ViLBERT\n(Lu et al., 2019) is a BERT-based transformer archi-\ntecture that combines V&L on two separate streams\nby co-attention layers. It is pretrained on Google\nConceptual Captions (Sharma et al., 2018) on (i)\nmultimodal masked word and object prediction;\nand (ii) image-sentence alignment. ViLBERT 12-\nin-1 (Lu et al., 2020) further ﬁnetuned a ViLBERT\nmodel checkpoint on 12 different tasks including\nVQA, image retrieval, phrase grounding and oth-\ners.21 We use the image-sentence alignment head\nof the publicly available model checkpoints for\n19github.com/openai/CLIP\n20github.com/huggingface/transformers\n21github.com/facebookresearch/\nvilbert-multi-task\n8269\nCLIP LXMERT ViLBERT ViLBERT 12-in-1 VisualBERT\n(Radford et al., 2021) (Tan and Bansal, 2019) (Lu et al., 2019) (Lu et al., 2020) (Li et al., 2019)\nmodel type separate image and\ntext encoders dual stream dual stream dual stream single stream\npretraining\ndata\n400M image-text\npairs MSCOCO Conceptual Captions Conceptual Captions MSCOCO\npretraining\ntasks ISA ISA, MLM, MOP, VQA ISA, MLM, MOP ISA, MLM, MOP ISA, MLM, MOP\nﬁnetuning – VQA – 12 V&L tasks –\nTable 3: V&L models evaluated with V ALSE in our experiments.ISA: image-sentence alignment; MLM: masked\nlanguage modelling; MOP: masked object prediction; VQA: visual question answering.\nMetric Model ExistencePlurality Counting Sp.rel.‡ Action Coreference Foil-it! Avg.quantiﬁers number balanced sns.† adv.† relations repl.† actant swapstandard clean\nRandom 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0\naccr\nGPT1∗ 61.8 53.1 51.2 48.7 69.5 77.2 65.4 72.2 45.6 45.2 77.5 60.7\nGPT2∗ 58.0 51.9 51.6 49.8 45.3 75.0 66.8 76.9 54.5 50.0 80.7 60.1\nCLIP 66.9 56.2 62.1 62.5 57.5 64.3 75.6 68.6 52.1 49.7 88.8 64.0\nLXMERT 78.6 64.4 62.2 69.2 42.6 60.2 54.8 45.8 46.8 44.2 87.1 59.6\nViLBERT 65.5 61.2 58.6 62.9 73.7 57.2 70.7 68.3 47.2 48.1 86.9 63.7\n12-in-1 95.6 72.4 76.7 80.2 77.3 67.7 65.9 58.9 75.7 69.2 86.9 75.1\nVisualBERT 39.7 45.7 48.2 48.2 50.0 39.7 49.2 44.4 49.5 47.6 48.5 46.4\nacc\nLXMERT 55.8 55.1 52.0 55.4 49.9 50.8 51.1 48.5 49.8 49.0 70.8 53.5\nViLBERT 2.4 50.3 50.7 50.6 51.8 49.9 52.6 50.4 50.0 50.0 55.9 51.3\n12-in-1 89.0 62.0 64.9 69.2 66.7 53.4 57.3 52.2 54.4 54.3 71.5 63.2\nVisualBERT 49.3 46.5 48.3 47.8 50.0 49.3 48.8 49.7 50.0 50.0 46.6 48.8\npc\nLXMERT 41.6 68.0 50.9 50.0 61.5 73.1 35.8 36.8 81.2 80.8 72.3 59.3\nViLBERT 56.8 98.5 77.0 76.6 86.1 98.3 93.2 93.7 98.7 98.1 98.8 88.7\n12-in-1 85.0 90.7 64.3 76.7 59.5 93.5 66.7 66.8 92.9 95.2 94.3 80.5\nVisualBERT 1.3 0.3 0.0 0.0 0.0 1.3 0.0 0.0 0.0 0.0 0.2 0.3\npf\nLXMERT 70.1 42.2 53.0 60.8 37.3 28.4 66.4 60.2 18.4 17.3 69.3 47.6\nViLBERT 47.9 2.1 24.4 24.7 17.5 1.5 11.9 7.1 1.3 1.9 12.9 13.9\n12-in-1 93.1 33.4 65.6 61.7 74.0 13.3 47.8 37.6 15.8 13.5 48.8 45.9\nVisualBERT 97.3 92.8 96.7 95.7 100.0 97.3 97.6 99.4 100.0 100.0 93.0 97.3\nmin(pc,pf)\nLXMERT 41.6 42.2 50.9 50.0 37.3 28.4 35.8 36.8 18.4 17.3 69.3 38.9\nViLBERT 47.9 2.1 24.4 24.7 17.5 1.5 11.9 7.1 1.3 1.9 12.9 13.9\n12-in-1 85.0 33.4 64.3 61.7 59.5 13.3 47.8 37.6 15.8 13.5 48.8 43.7\nVisualBERT 1.3 0.3 0.0 0.0 0.0 1.3 0.0 0.0 0.0 0.0 0.2 0.3\nAUROC\n×100\nLXMERT 60.5 57.3 53.8 57.7 50.5 51.9 52.1 47.6 49.8 49.5 76.9 55.2\nViLBERT 52.5 54.1 50.8 51.6 53.5 51.2 57.2 57.8 49.9 49.9 75.2 54.9\n12-in-1 96.3 67.4 72.0 77.8 75.1 55.8 61.3 55.0 59.8 59.6 81.0 69.2\nVisualBERT 28.9 29.0 24.5 16.5 20.9 45.2 17.7 36.3 45.3 46.3 28.5 30.8\nTable 4: Performance of unimodal and multimodal models on the V ALSE benchmark according to different metrics.\nWe bold-face the best overall result per metric, and underscore all results below (or at) the random baseline.accr is\na pairwise ranking accuracy where a prediction is considered correct ifp(caption,img) >p(foil,img ). Precision\npc and foil precision pf are competing metrics where naïvely increasing one can decrease the other: therefore\nlooking at the smaller number among the two gives a good intuition of how informed is a model prediction . †sns.\nCounting small numbers. adv. Counting adversarial. repl. Action replacement. ‡Sp.rel. Spatial relations.\n∗Unimodal text-only models that do not use images as input. CLIP is only tested in pairwise ranking mode (fn. 6).\nViLBERT22 and ViLBERT 12-in-123.\nVisualBERT VisualBERT (Li et al., 2019) is\nalso a BERT-based transformer. Its single-stream\narchitecture encodes image regions and linguis-\ntic features via a transformer stack, using self-\nattention to discover the alignments between the\ntwo modalities. VisualBERT is pretrained on\nMSCOCO captions (Chen et al., 2015) on two\n22https://dl.fbaipublicfiles.com/\nvilbert-multi-task/pretrained_model.bin\n23https://dl.fbaipublicfiles.com/\nvilbert-multi-task/multi_task_model.bin\ntasks: (i) masked language modelling, and (ii)\nsentence-image prediction. The latter is framed\nas an extension of the next sentence prediction task\nused with BERT. Inputs consist of an image and\na caption, with a second caption which has a 50%\nprobability of being random. The goal is to deter-\nmine if the second caption is also aligned to the\nimage. In our experiments, we use the publicly\navailable implementation of VisualBERT24.\nGPT-1 and GPT-2 – Unimodal models GPT1\n(Radford et al., 2018) and GPT2 (Radford et al.,\n24github.com/uclanlp/visualbert\n8270\nPiece Instrument #Inst. #Valid (%) #Unan. (%) #Lex.it. JS JS Val. α α Valid\nExistence Existential quantiﬁers 534 505 (94.6) 410 (76.8) 25 0.628 0.629 0.607 0.644\nPlurality Semantic Number 1000 851 (85.1) 617 (61.7) 704 0.742 0.766 0.303 0.359\nCounting\nBalanced 1000 868 (86.8) 598 (59.8) 25 0.070 0.082 0.361 0.423\nSmall numbers 1000 900 (90.0) 637 (63.7) 4 0.059 0.071 0.417 0.473\nAdversarial 756 691 (91.4) 522 (69.0) 27 1.000 1.000 0.387 0.441\nRelations Prepositions 614 535 (87.1) 321 (52.3) 38 0.083 0.114 0.210 0.229\nActions Replacement 779 648 (83.2) 428 (54.9) 262 0.437 0.471 0.229 0.318\nActant swap 1042 949 (91.1) 756 (72.6) 467 0.000 0.000 0.386 0.427\nCoreference standard: VisDial train 916 708 (77.3) 499 (54.5) 2 0.053 0.084 0.291 0.360\nclean: VisDial val 141 104 (73.8) 69 (48.9) 2 0.126 0.081 0.248 0.375\nFoil-It! noun replacement 1000 943 (94.3) 811 (81.1) 73 0.426 0.425 0.532 0.588\nOverall 8782 7702 (87.7) 5668 (73.6)\nTable 5: Manual validation results for each piece in V ALSE, as well as for the Foil-it dataset. #Inst.: number\nof instances for linguistic phenomenon. #Valid (%): number (percent) of cases for which at least 2 out of 3\nannotators chose the caption; #Unan. (%) : number (percent) of cases for which all annotators chose the caption;\n#Lex.It.: number of phrases or lexical items in the vocabulary that differ between foils and captions; JS: Jensen-\nShannon divergence between foil-caption distributions for all instances in the whole instrument; JS Val.: Jensen-\nShannon divergence between foil-caption distribution for the valid subset of the instrument, after sub-sampling;α:\nKrippendorff’sαcoefﬁcient computed over all the instances; αvalid: Krippendorff’sαcoefﬁcient computed over\nthe Valid instances.\n2019) are transformer-based autoregressive lan-\nguage models pretrained on English data through\nself-supervision. We test whether our benchmark is\nsolvable by these unimodal models by computing\nthe perplexity of the correct sentence and compare\nit to the perplexity of the foiled sentence. In case\nthe computed perplexity is higher for the foil than\nfor the correct sentence, we assume that the cor-\nrectly detected foiled caption may possibly suffer\nfrom a plausibility bias (as described in section\n4.2) or from other biases (e.g. a model’s preference\ntowards afﬁrmative or negative sentences).\nE Mechanical Turk Annotation and\nEvaluation\nSetup The validation study was conducted on all\nthe data for each instrument in V ALSE, as well\nas for the FOIL it! data (Shekhar et al., 2019b).\nEach instance consisted of an image, a caption and\na foiled version of the caption, as shown in Fig-\nure 2. Annotators received the following general\ninstructions:\nYou will see a series of images, each\naccompanied by two short texts. Your\ntask is to judge which of the two texts\naccurately describes what can be seen in\nthe image.\nEach instance was accompanied by the caption\nand the foil, with the ordering balanced so that the\ncaption appeared ﬁrst 50% of the time. In each\ninstance, the caption and foil were placed above\neach other, with the differing parts highlighted in\nbold. Annotators were asked to determine which\nof the two sentences accurately describes what can\nbe seen in the image? In each case, they had to\nchoose between ﬁve options: (a) the ﬁrst, but not\nthe second; (b) the second, but not the ﬁrst; (c) both\nof them; (d) neither of the two; and (e) I cannot tell.\nWe collected three annotations for each instance,\nfrom three independent workers.\nAnnotator selection We recruited annotators\nwho had an approval rating of 90% or higher on\nAmazon Mechanical Turk. We ran an initial, pre-\nselection study with 10 batches of 100 instances\neach, in order to identify annotators who under-\nstood the instructions and performed the task ade-\nquately. The pre-selection batches were ﬁrst man-\nually annotated by the authors, and we identiﬁed\n‘good’ annotators based on the criterion that they\npreferred the caption to the foil at least 70% of\nthe time. Based on this, we selected a total of 63\nannotators. Annotators were paid $0.05 per item\n(i.e. per HIT on Mechanical Turk).\nResults Table 5 shows, for each instrument, the\nnumber of instances in total, as well as the pro-\nportion of instances which we consider valid, that\nis, those for which at least two out of three anno-\ntators chose the caption, but not the foil , as the\n8271\nFigure 2: Example of an instance from the validation study. The example is from the counting piece, adversarial\ninstrument (see Section 3.3).\ntext which accurately describes the image. We also\nshow the number of instances for which annotators\nunanimously (3/3) chose the caption.\nAnnotator agreement As shown in Table 5, the\nproportion of valid instances in each instrument\nwas high, ranging from 73.8% to 94.6%, with most\ninstruments having annotators choose the caption\nwell over 80% of the time. The table also shows\ntwo inter-annotator agreement statistics, both com-\nputed using Krippendorff’s α: over all the data\nin a given instrument, and over the valid subset\nonly. On the valid subset, agreement is higher, and\nranges from 0.3 to 0.6 (mean = 0.42; sd=0.12).\nThere is a signiﬁcant positive correlation between\nthe percentage of valid instances per instrument\nand the αvalue (Spearman’sρ = 0.75; p < .05).\nThe low to medium agreement suggested by the α\nrange is due to two factors: ﬁrst, the statistic is com-\nputed over the entire pool of annotators, of whom\nthere were signiﬁcant diversions in the amount of\nannotations they computed (e.g. some workers an-\nnotated fewer than 5 HITs); furthermore, the agree-\nment is computed over 5 categories (see above).\nGiven these factors, the inter-annotator agreement\nresults should be treated with caution, and are not\nstraightforwardly interpretable as an index of hu-\nman performance on V ALSE - in particular, the\nvalidation task (with 5 categories) was framed dif-\nferently from the benchmark (which is binary).\nBias check While measures were taken to con-\ntrol for distributional bias between captions and\nfoils in the different pieces of V ALSE (cf. §4.1), it\nis possible that sub-sampling after manual valida-\ntion could reintroduce such biases. To check that\nthis is not the case, we compare theword frequency\ndistributions between captions and foils in the orig-\ninal pieces, and the word frequency distribution of\nthe manually validated set. We report the Jensen-\nShannon divergence and the number of words that\ndiffer between caption and foil in Table 5. The\nfoil-caption word frequency distributions can be\ninspected in Figures 3 and 4. The Jensen-Shannon\n(JS) divergence is deﬁned as:\nJS(f ∥c) =\n√\nKL(f ∥m) +KL(c∥m)\n2\nwhere f is the normalized word frequency for foils,\ncthe normalized word frequency for captions, m\nis the point-wise mean of f and c, and KL is the\nKullback-Leibler divergence.\nAs Table 5 shows, the JS-divergence between\ncaption and foil distributions remains the same, or\nchanges only marginally (compare columns JS-div\nand Js-div valid, where #Lexical Items indicates the\nnumber of lexical/phrasal categories in the relevant\ndistributions). This indicates that no signiﬁcant\nbias was introduced as a result of subsampling after\nmanual validation.\n8272\nFigure 3: Word frequency distributions for captions and foils before and after the manual validation for existence,\ncounting and relations.\n8273\nFigure 4: Word frequency distributions for captions and foils before and after the manual validation for plurality,\naction replacement and FOIL it. The actant swap instrument is not visualised here: By construction, actant swap\ncannot suffer from distributional bias since caption and foil contain the same words up to a permutation.\n8274\npiece image caption (blue) foil (orange)\nexistence\nThere are no people in the pic-\nture. There are people in the picture.\nThere is a truck pictured. There is no truck pictured.\nThere are no clouds in the sky. There are clouds in the sky.\nThere are no people riding on\nelephants.\nThere are people riding on ele-\nphants.\nThere is a kite. There is no kite.\nTable 6: Randomly selected data examples for existence.\n8275\npiece image caption (blue) foil (orange)\nplurality\nTwo young men playing frisbee\nat night on exactly one sports\nﬁeld.\nTwo young men playing frisbee\nat night on a number of sports\nﬁelds.\nExactly one row of motorcycles\nparked together on a grass yard\narea with a house in the back-\nground.\nA number of rows of motorcy-\ncles parked together on a grass\nyard area with a house in the\nbackground.\nTwo men are looking inside of a\nsingle giant barbecue.\nTwo men are looking inside of a\nnumber of giant barbecues.\nSome children are playing base-\nball outside in a ﬁeld.\nA single child is playing base-\nball outside in a ﬁeld.\nA number of people riding some\nmotorbikes on the road.\nA single person riding some mo-\ntorbikes on the road.\nTable 7: Randomly selected data examples for plurality.\n8276\npiece image caption (blue) foil (orange)\ncounting\nThere are exactly 8 horses. There are exactly 5 horses.\nThere is exactly 1 person snow-\nboarding.\nThere are exactly 4 people snow-\nboarding.\nThere are exactly 6 motorcycles\nin this photo altogether.\nThere are exactly 7 motorcycles\nin this photo altogether.\nThere are exactly 2 banana\nstalks.\nThere are exactly 4 banana\nstalks.\nThere are exactly 12 roman nu-\nmerals on the clock.\nThere are exactly 9 roman nu-\nmerals on the clock.\nTable 8: Randomly selected data examples for counting.\n8277\npiece image caption (blue) foil (orange)\nrelations\nA baby elephant is walking un-\nder a larger elephant.\nA baby elephant is walking on a\nlarger elephant.\nFruits and vegetables are being\nsold in a market.\nFruits and vegetables are being\nsold outside a market.\nAn airplane is letting off white\nsmoke against a blue sky.\nAn airplane is letting in white\nsmoke against a blue sky.\nA cow stands on a sidewalk out-\nside a building.\nA cow stands on a sidewalk in a\nbuilding.\nThree giraffes banding down to\ndrink water with trees in the\nbackground.\nThree giraffes banding up to\ndrink water with trees in the\nbackground.\nTable 9: Randomly selected data examples for relations.\n8278\npiece image caption (blue) foil (orange)\nactions\nA ﬁgure climbs the stairs. A ﬁgure descends the stairs.\nA woman skips a jump rope. A woman releases a jump rope.\nAn old man coaches people. An old man bothers people.\nThe people unveil the prize. A prize unveils people.\nA baby drools over clothing. A clothing drools over the baby.\nTable 10: Randomly selected data examples for actions.\n8279\npiece image caption (blue) foil (orange)\ncoreference\nA close up of a hot dog with\nonions. Is it a big hot dog? Yes.\nA close up of a hot dog with\nonions. Is it a big hot dog? No.\nA skateboarding man is on a half\npipe. Does he wear a helmet?\nNo.\nA skateboarding man is on a half\npipe. Does he wear a helmet?\nYes.\n2 women who have painted on\nmustaches petting a horse. Are\nthey wearing hats? No.\n2 women who have painted on\nmustaches petting a horse. Are\nthey wearing hats? Yes.\nYellow sunﬂowers are in a blue\nand white giraffe styled vase. Is\nit inside? Yes.\nYellow sunﬂowers are in a blue\nand white giraffe styled vase. Is\nit inside? No.\nAn adult giraffe and a child\ngiraffe standing near a fence.\nDoes this look like zoo? Yes.\nAn adult giraffe and a child\ngiraffe standing near a fence.\nDoes this look like zoo? No.\nTable 11: Randomly selected data examples for coreference.\n8280",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.7637794613838196
    },
    {
      "name": "Task (project management)",
      "score": 0.63836270570755
    },
    {
      "name": "Computer science",
      "score": 0.6141327619552612
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.6105466485023499
    },
    {
      "name": "Linguistics",
      "score": 0.6002746224403381
    },
    {
      "name": "Artificial intelligence",
      "score": 0.470213919878006
    },
    {
      "name": "Association (psychology)",
      "score": 0.4481401741504669
    },
    {
      "name": "Natural language processing",
      "score": 0.4435025155544281
    },
    {
      "name": "Computational linguistics",
      "score": 0.4174359142780304
    },
    {
      "name": "Cognitive science",
      "score": 0.3305683135986328
    },
    {
      "name": "Psychology",
      "score": 0.21387559175491333
    },
    {
      "name": "Philosophy",
      "score": 0.17430126667022705
    },
    {
      "name": "Engineering",
      "score": 0.12840747833251953
    },
    {
      "name": "Geography",
      "score": 0.10126304626464844
    },
    {
      "name": "Physics",
      "score": 0.06690287590026855
    },
    {
      "name": "Epistemology",
      "score": 0.057923972606658936
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I223822909",
      "name": "Heidelberg University",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I85384741",
      "name": "Heidelberg University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I197854408",
      "name": "University of Malta",
      "country": "MT"
    },
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I193662353",
      "name": "Utrecht University",
      "country": "NL"
    }
  ]
}