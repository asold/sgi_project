{
    "title": "Effects of sub-word segmentation on performance of transformer language models",
    "url": "https://openalex.org/W4389520677",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2104212582",
            "name": "Jue Hou",
            "affiliations": [
                "University of Helsinki"
            ]
        },
        {
            "id": "https://openalex.org/A2625321718",
            "name": "Anisia Katinskaia",
            "affiliations": [
                "University of Helsinki"
            ]
        },
        {
            "id": "https://openalex.org/A3159853359",
            "name": "Anh Duc Vu",
            "affiliations": [
                "University of Helsinki"
            ]
        },
        {
            "id": "https://openalex.org/A705197487",
            "name": "Roman Yangarber",
            "affiliations": [
                "University of Helsinki"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W46679369",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W4385573195",
        "https://openalex.org/W4389519397",
        "https://openalex.org/W4360951492",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4287855085",
        "https://openalex.org/W4281609260",
        "https://openalex.org/W3158607076",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W4287993739",
        "https://openalex.org/W2969601108",
        "https://openalex.org/W3176893837",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2152561112",
        "https://openalex.org/W3018647120",
        "https://openalex.org/W4287887845",
        "https://openalex.org/W3123545922",
        "https://openalex.org/W2758950307",
        "https://openalex.org/W2757976180",
        "https://openalex.org/W309335912",
        "https://openalex.org/W3137243147",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W1533169541",
        "https://openalex.org/W2100664567",
        "https://openalex.org/W2250342921",
        "https://openalex.org/W3101140821",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2963979492",
        "https://openalex.org/W2117621558",
        "https://openalex.org/W3207091856",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W3031412205"
    ],
    "abstract": "Language modeling is a fundamental task in natural language processing, which has been thoroughly explored with various architectures and hyperparameters. However, few studies focus on the effect of sub-word segmentation on the performance of language models (LMs). In this paper, we compare GPT and BERT models trained with the statistical segmentation algorithm BPE vs. two unsupervised algorithms for morphological segmentation — Morfessor and StateMorph. We train the models for several languages — including ones with very rich morphology — and compare their performance with different segmentation algorithms, vocabulary sizes, and model sizes. The results show that training with morphological segmentation allows the LMs to: (1) achieve lower perplexity, (2) converge more efficiently in terms of training time, and (3) achieve equivalent or better evaluation scores on downstream tasks. Lastly, we show that (4) LMs of smaller size using morphological segmentation can perform comparably to models of larger size trained with BPE — both in terms of (1) perplexity and (3) scores on downstream tasks. Points (2) and (4) impact on sustainability, since they reduce the model cost; and while 2 reduces cost only in the training phase, 4 does so also in the inference phase.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7413–7425\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nEffects of sub-word segmentation\non performance of transformer language models\nJue Hou,*† Anisia Katinskaia,*† Anh-Duc Vu,*† Roman Yangarber†\n* Department of Computer Science\n†Department of Digital Humanities\nUniversity of Helsinki, Finland\nfirst.last@helsinki.fi\nAbstract\nLanguage modeling is a fundamental task in\nnatural language processing, which has been\nthoroughly explored with various architectures\nand hyperparameters. However, few studies fo-\ncus on the effect of sub-word segmentation on\nthe performance of language models (LMs). In\nthis paper, we compare GPT and BERT models\ntrained with the statistical segmentation algo-\nrithm BPE vs. two unsupervised algorithms\nfor morphological segmentation—Morfessor\nand StateMorph. We train the models for sev-\neral languages—including ones with very rich\nmorphology—and compare their performance\nwith different segmentation algorithms, vocab-\nulary sizes, and model sizes. The results show\nthat training with morphological segmentation\nallows the LMs to: 1. achieve lower perplexity,\n2. converge more efficiently in terms of training\ntime, and 3. achieve equivalent or better evalu-\nation scores on downstream tasks. Lastly, we\nshow 4. that LMs of smaller size using mor-\nphological segmentation can perform compara-\nbly to models of larger size trained with BPE—\nboth in terms of (1) perplexity and (3) scores\non downstream tasks. Points 2 and 4 impact\non sustainability, since they reduce the model\ncost; and while 2 reduces cost only in the train-\ning phase, 4 does so also in the inference phase.\n1 Introduction\nA key component required to train language mod-\nels is the tokenizer. One “basic” naïve approach is\nword-based tokenization—splitting the input into\nwords, and assembling a training sequence as the\nlist of word tokens. The problem with this ap-\nproach is out-of-vocabulary tokens (OOV) during\ninference—i.e., tokens not seen during training.\nOne solution is to pick a set of most frequent words,\nand replace all other words with a special [UNK] to-\nken (Bahdanau et al., 2015; Sutskever et al., 2014).\nThis works well only when the unknown vocab-\nulary is small (Cho et al., 2014; Bahdanau et al.,\n2015). Another solution is to use an extensive dic-\ntionary (Jean et al., 2015; Luong et al., 2015). This\napproach is problematic for morphologically-rich\nlanguages, such as Finnish or Russian. The word-\nbased approach results in a huge vocabulary, and\nstill very many OOV tokens, due to the possibly\nrare morphological variants (Sennrich et al., 2016).\nAt the other extreme is character-based tokeni-\nzation—treating each character as a separate to-\nken; but this leads to long sequences, and non-\nmeaningful tokens, which inevitably increase the\ncomputational complexity (Libovický et al., 2022).\nHence, the most common approach is in the\nmiddle—“sub-word” tokenization. A segmenter is\nan algorithm that splits words into sub-word units,\nbetween word- and character-based tokenization.\nThe most common choice for a segmenter is\nthe “Byte-Pair Encoding” (BPE) algorithm, origi-\nnally designed for simple data compression (Gage,\n1994). Currently most state-of-the-art neural lan-\nguage models, such as GPT (Radford et al., 2019)\nand BERT (Devlin et al., 2018), use it as the seg-\nmenter. The idea behind BPE and its variants is\nto compute the frequencies of all segments, to it-\neratively merge the most frequent consecutive pair\nof segments into one segment, and add it to the\nvocabulary of tokens. This yields a compression\nof the data, and also a way to represent the data\nusing a finite vocabulary of segment tokens—much\nsmaller than by using entire word tokens.\nBPE works well in training language models,\nas it reduces the size of the vocabulary while still\npreserving some frequent words in the language\nas tokens. However, BPE has several problems. It\nis a greedy algorithm, and gives a crude approxi-\nmation to the “true” linguistic structure of words,\nthat is, morphemes. A morpheme is a linguistically\nmeaningful segment of a word—it carries either\nsemantic or syntactic content, and cannot be split\nfurther into smaller segments. Also, morpheme\nretains its meaning in different contexts.\n7413\nThe goal of this work is to determine whether we\ncan improve the performance of language models\nby using segmenters that are more sophisticated\nthan BPE—linguistically motivated and morpho-\nlogically informed. For example, BPE may de-\ncide to segment the English word baking into two\nfrequent segments: ba·king. However, we may\nconsider bak·ing a “better” segmentation: root mor-\npheme bak and suffix morpheme ing. It stands to\nreason that “knowing” about morphology—having\nmore meaningful segments in its vocabulary—\nshould help the LM predict data better. To be fair,\nthe LM might be able to “recover” from the naïve\nsegmentation, and still achieve excellent perfor-\nmance on various tasks, but it will have to do so at\na higher cost!—in terms of more parameters and\nlonger training—which degrades sustainability.\nWhich segmentation algorithm yields a better\nlanguage model? We explore the “goodness” of a\nLM through four research questions:\n· RQ1: compared to BPE, does morphological\nsegmentation help LMs achieve lower perplexity?\n· RQ2: compared to BPE, does morphological\nsegmentation help LMs converge faster?\n· RQ3: compared to BPE, does morphological seg-\nmentation allow LMs to achieve similar (or better)\nperformance on downstream tasks?\n· RQ4: compared to BPE, does morphological\nsegmentation allow us to reduce model size without\ncompromizing model performance?\nThe paper is organized as follows: Section 2\nbriefly reviews prior work on language modeling\nwith morphological features, and the segmentation\nmethods. Section 3 introduces the language mod-\nels and the downstream tasks used for evaluation.\nSection 4 presents the results of our experiments.\nSection 5 concludes and discusses future work.\n2 Prior Work\nSub-word tokenization is a well-studied topic in\nnatural language processing. Several approaches\nare proposed to segment words into sub-word units,\n(Batsuren et al., 2022; Peters and Martins, 2022;\nMinixhofer et al., 2023). However, few papers have\nstudied the effect of sub-word segmentation on the\nperformance of neural language models.\nHofmann et al. (2021) discuss how sub-word\nsegmentation affects BERT’s interpretation of com-\nplex words. They conduct a series of semantic\nprobing tasks and show that applying morphologi-\ncal segmentation is beneficial for language models.\nPark et al. (2021) trained models with several\nsegmentation algorithms, including BPE and Mor-\nfessor (Creutz and Lagus, 2002), on a corpus\nof Bible verses in 92 languages. They evaluate\nthe models according to surprisal (negative log-\nlikelihood) per verse. Their results show that Mor-\nfessor segmentation yields much lower surprisal\nper verse than character or BPE segmentation for\nthe overwhelming majority of the tested languages.\nBostrom and Durrett (2020) compare the output\nof BPE and Unigram tokenization (Kudo, 2018)\non English and Japanese. Comparing with gold-\nstandard morpheme boundaries, they found that Un-\nigram tokenization segments words more closely\nto morphological references, while BPE greedily\nmerges sub-words according to their frequency,\neven if the sub-word pair is not semantically mean-\ningful. They experimented with evaluating lan-\nguage models on downstream tasks, and pre-trained\nmodels from scratch with various segmentation\nmethods. They found that Unigram tokenization\noutperforms BPE on both English and Japanese.\nToraman et al. (2022) conduct a comprehensive\nstudy to evaluate how tokenization affects the per-\nformance of Turkish LMs. Besides BPE and its\nvariant WordPiece (Wu et al., 2016), they apply a\nmorphological analysis tool for Turkish, and use\nit as sub-word segmenter. Similarly to (Bostrom\nand Durrett, 2020), they conducted experiments by\nevaluating six downstream LM tasks. Although\nBPE and WordPiece achieve the best performance,\nLM with morphology-based segmentation achieve\ncomparable results. They also point out that LM\nwith a bigger vocabulary size can generally per-\nform better. However, a large vocabulary increases\nthe computational complexity.\n2.1 Sub-word segmentation\nWe briefly introduce the three segmentation algo-\nrithms we use in this work. We leave out the math-\nematical details, and refer the reader to the original\npapers for further information.\nBPE: is one of most popular sub-word segmen-\ntation algorithms (Gage, 1994). It is a greedy algo-\nrithm, that starts from a character-based “segmen-\ntation” (tokenization) of each word, then iteratively\nsorts all adjacent pairs of segments by frequency\nof co-occurrence, and merges the most frequent ad-\njacent pair. The merged pair is added to the vocab-\nulary, until the vocabulary reaches a pre-selected\nmaximum size. At this point, BPE stops merging.\n7414\nWe use Google’sSentencePiece implementation of\nBPE.\nMorfessor: is an unsupervised algorithm for\nmorphology induction (Creutz and Lagus, 2002).\nBased on the Minimum Description Length (MDL)\nprinciple (Rissanen, 1998; Grünwald, 2007), it fa-\nvors lexicons with fewer and shorter sub-words.\nIt learns the model parameters with MAP estima-\ntion using a two-part cost function: a prior cost\nand a corpus cost. The corpus cost encodes the\nprobability of the corpus given the lexicon and seg-\nmentation.\nThe original Morfessor has been extended in sev-\neral ways (Grönroos et al., 2020, 2014); however,\nthe later implementations are not included in the of-\nficial Python library distribution. Therefore, we use\nthe Morfessor 2.0 implementation (Virpioja et al.,\n2013) of the baseline method. We emphasize that\nin the Morfessor baseline that we use, vocabulary\nsize fixed, determined by the training (not customiz-\nable). Exploring other variants with configurable\nvocabulary sizes is left for future work.\nStateMorph: is based on the MDL principle\nsimilarly to Morfessor, but is different in several\nrespects (Nouri and Yangarber, 2017). It tries to\nmodel the morphological structure of the language\nusing states in a finite-state network. Each state\nlearns to emit segments (“morphs”) of a certain\nkind, e.g., verb stems, noun suffixes, etc. It also\nuses a MDL-based two-part objective: the cost of\nthe complete data—i.e., the segmented corpus—is\nthe cost of the model, plus the cost of the data given\nthe model. The cost of the model consists of three\ncomponents: the cost of the morph lexicon, the\ncost of transitions between states, and the cost of\nemitting morphs from states. Unlike Morfessor,\nStateMorph uses simulated annealing to optimize\nthe search, which makes learning much slower. We\nuse the baseline implementation released with the\noriginal paper.1 It decides on its own optimal lexi-\ncon size, according to the optimized objective.\nIn our experiments, we also use a variant of State-\nMorph segmentation, where we configure the de-\nsired lexicon size. Similarly to Morfessor baseline,\nwe cannot control the size of StateMorph’s lexicon\nduring training; however, we can prune the lexicon\nafter training, by simply dropping the least frequent\nmorphs. The resulting segmenter will tend to seg-\nment a word with more frequent sub-words (even if\nthe overall cost may be higher). But this allows us\n1http://nlp.cs.helsinki.fi/morpho\nto compare language models under similar condi-\ntions: with same lexicon size. In the following, we\ndenote normal StateMorph bySM, and StateMorph\nwith a pruned lexicon by SMp.\n3 Methods\nTo evaluate the impact of the segmentation on the\nlanguage models, we conduct several experiments.\nFor each LM—BERT and GPT, we perform four\nevaluations, corresponding to the four research\nquestions. Each model is trained with the sub-\nword lexicon resulting from the three segmentation\nalgorithms: BPE, Morfessor, and StateMorph.\n3.1 Training the language models\nRQ1 asks: which of the segmentation algorithms\nyields a better language model—in terms of per-\nplexity? In the information-theoretic sense, this is\nthe definitive measure of the model’s “goodness”:\nperplexity tells how well the model is able to pre-\ndict the data. Thus, in theoretical terms, a model\nwith lower perplexity is a better model.\nWe also keep track of how many steps each LM\ntakes to converge, to answer RQ2: does a smarter\nsegmentation help the LM learn faster.\nHyperparameters used in training follow the\nsame settings as for BERTbase in (Turc et al., 2019),\nexcept we use a smaller feed-forward/filter size to\nreduce the model size. We describe all settings and\nhyperparameters in detail in Appendix A.\nWe used a smaller instance size—256, half of\nwhat is typically used for BERT. This is due to\nlimited computational resources and exceptionally\nlarge vocabulary size for some LMs. We were\nnot able to train with a conventional batch size.\nAs the instance size is smaller than regular and\nresources are limited, we would like to focus only\non experimenting with the effect of segmentation\nand avoid spare resources on harder side-tasks such\nas next-sentence prediction. Therefore, we did not\ninclude it as a part of BERT pre-training. We aim\nfor sentence-level LMs (rather than larger context).\nIn future work, we can repeat this on bigger models\nproperly, with more computational resources.\nData: the corpora used to train the language\nmodels are as follows. The Finnish corpus con-\ntains data from two major Finnish news agencies:\nHelsingin Sanomat (HS) and YLE. 2 The corpus\ncontains around 17M instances. For Russian, we\nuse the Taiga corpus (Shavrina and Shapovalova,\n2MetaShare: Yle Finnish News Archive\n7415\n2017). The corpus contains 66M instances. The\nEnglish corpus comes mainly from the English\nWikipedia dump from 2020-05-01.3 We add our\nown news data, privately crawled from the Internet.\nThe corpus contains 61M instances. For Turkish,\nwe use the OSCAR corpus (Abadji et al., 2022).\nThe corpus contains 20M instances. Each training\ninstance is composed of 3 sentences.\nPre-processing: For each language, we train\nall segmentation algorithms with the same word\nlist, extracted from the training corpus. We lower-\ncase all words to assure that the segmentation of a\ngiven word is the same regardless of its case. How-\never, the language models should handle mixed-\ncase input. Additional technical details about the\npre-training phase are given in Appendix A.1.\nTo make the comparison fair, we make sure that\nthe experiments use the same vocabulary size. As\nmentioned above, we are not able to customize the\nlexicon size for Morfessor directly. To customize\nthe lexicon size for StateMorph, we prune the lex-\nicon by frequency of the morph. Therefore, we\nset the lexicon size for BPE manually—to match\nthe lexicon size produced by Morfessor and State-\nMorph. We leave the exploration of the effect of\nStateMorph variants and Morfessor variants with\nadjustable lexicon size for future work.\nWe should clarify that we do not aim for optimal\nlexicon size in terms of LM pre-training or any\nof the downstream tasks; it is only to make the\ncomparison fair. Searching for the optimal lexicon\nsize would require exorbitant compute resources,\nand is likely highly language-specific. This is not\nour goal here. The sizes of the resulting lexicons\nare shown in Appendix A.\n3.2 Model size\nRQ4 asks whether a smarter segmentation will al-\nlow us to build a language model that has equiv-\nalent or better performance with smaller model\nsize—measured in terms of the number of learned\nmodel parameters. This is a crucial question for\nsustainability, since the large language models con-\nsume vast amounts of computing resources—both\nin the training and in the inference phases.\nWe configure our small models similarly to\nBERTmedium in (Turc et al., 2019); more details\nabout hyperparameters are in Appendix A.\n3HuggingFace Wikipedia datasets\n3.3 Downstream tasks\nRQ3 asks whether we can confirm that language\nmodels with lower perplexity yield comparable (or\nbetter) performance on downstream tasks. Thus\nwe also evaluate performance of the models on\npractical applications. We consider two types of\ntasks: classification vs. generative tasks.\nFinnish: we use two topic classification tasks,\nand Part-of-Speech (PoS) tagging, which is a\nsequence-labeling task. Topic classification is\nbased on two corpora: In-domain and Out-of-\ndomain. For the In-domain task, we use the YLE\ncorpus, same as for training the segmentation algo-\nrithms and LMs. We classify documents into four\ntopics: Sport, Politics, Science, and Culture. For\nthe Out-of-domain task, we use the dataset from\nYlilauta4 with topics: Sport, Politics, Science, and\nFood&Drink. We use the same instance size as in\npre-training. For each corpus, we use 10k, 1k, and\n1k for training, validation, and testing, respectively.\nThe PoS tagging task is based onFinnish-TDT from\nUD: Universal Dependency (Nivre et al., 2020).\nFor the generative task, we use a paraphrase\ndataset from (Kanerva et al., 2021). We use 21k for\ntraining, 2.6k for validation, and 2.6k for testing.\nRussian: For classification, we use topic clas-\nsification based on the Lenta.ru corpus; part-of-\nspeech tagging, based on Russian-SynTagRus from\nUD; and a linguistic acceptability (LA) task, us-\ning RuCoLa (Mikhailov et al., 2022), with GPT\nand BERT models. For the generative task, we use\nparaphrase generation with ru-paraphrase-NMT-\nLeipzig dataset.5\nWe explore PoS tagging only with BERT, as GPT\nis a left-to-right LM, which prevents the model’s\naccess to the “future” during training, while PoS\ntagging may require inference from the entire sur-\nrounding context, not only the left side. Therefore,\nGPT may not be suitable for PoS tagging.\nWe explore the paraphrase generation task only\nwith GPT models, as we did not include next-\nsentence prediction in pre-training our BERT—and\ntherefore it may not be suitable for generative tasks.\nWe leave this for future work.\n4 Experiments and results\nThis section presents a series of experiments that\nwe conduct to address the research questions.\n4MetaShare: Ylilauta Corpus\n5HuggingFace: ru-paraphrase-NMT-Leipzig\n7416\n(a) Finnish GPT\n (b) Finnish BERT\n(c) Russian GPT\n (d) Russian BERT\nFigure 1: Learning curves for pre-training Finnish and Russian LMs with different segmentation methods.\n(a) English GPT\n (b) Turkish GPT\nFigure 2: Learning curves for pre-training English and Turkish GPT with different segmentation methods.\n4.1 Pre-Training with different segmentations\nTo explore the behavior of the different segmenta-\ntion methods, we pretrain an array of LMs, GPT\nand BERT. We visualize the learning curves of\nFinnish and Russian models in Figure 1. Solid\nlines indicate BPE, lines with dashes indicate mor-\nphological segmentation; line colors code the vo-\ncabulary sizes. The same experiments for English\nand Turkish, GPT only, are shown in Figure 2.\nWe show the final perplexity, and the number of\nsteps to reach convergence—Finnish and Russian\nin Table 1, English and Turkish in Table 2.\nThe curves and the Tables show that—with the\nsame vocabulary size—for almost all models, mor-\nphological segmentation yields lower perplexity.\nThere are only a few exceptions: BERT with Mor-\nfessor (FI and RU) and GPT with Morfessor (EN\nand TR); but perplexity is very near to the BPE\ncounterpart. This suggests models with morpho-\nlogical segmentation yield overwhelmingly better\nperplexity, compared to using BPE, with the same\nvocabulary size (RQ1).\nRegarding the impact of segmentation on the\nlearning progress: the number of steps to reach\nconvergence (first column in both tables) is higher\nfor most models with BPE, several times higher\nfor some. This suggests that the models with mor-\nphological segmentation are also more efficient in\n7417\nGPT BERT\nSegmenter steps (k) PPL steps (k) PPL\nFinnish\nbpe (233k) 593 38.17 1426 14.18\nMorfessor 578 38.09 1339 15.20\nbpe (149k) 664 30.16 1690 9.75\nSM 419 19.71 521 7.23\nbpe (78k) 757 23.69 1522 6.89\nSMp 724 16.65 584 5.92\nRussian\nbpe (488k) 762 35.01 888 12.96\nMorfessor 609 30.43 591 13.43\nbpe (516k) 843 34.96 840 13.40\nSM 363 29.66 579 11.47\nbpe (170k) 660 24.97 1663 6.97\nSMp 816 19.08 615 6.47\nTable 1: Pre-training results for Finnish and Russian\nLMs with different segmentation methods. V ocabulary\nsize (in parentheses) applies to all models in each box.\nGPT\nSegmenter steps (k) PPL\nEnglish\nbpe (151k) 624 19.82\nMorfessor 594 20.05\nbpe (177k) 579 20.33\nSM 465 18.93\nbpe (107k) 739 17.92\nSMp 531 17.55\nTurkish\nbpe (92k) 402 13.53\nMorfessor 393 14.39\nbpe (85k) 474 13.04\nSM 426 11.07\nbpe (60k) 381 12.93\nSMp 432 10.67\nTable 2: Pre-training results for English and Turkish\nGPTs with different segmentation methods. V ocabulary\nsize (in parentheses) applies to all models in each box.\nterms of training time (RQ2).\n4.2 Pre-Training with different model sizes\nWe next explore RQ4: model size. We train\ntwo more GPT models as above for Finnish and\nRussian—with morphological segmentation (SMp)\nand with BPE, but in smaller size, i.e., with a\nsmaller number of parameters. In this experiment,\nwe use models with the smallest vocabulary: 78k\nfor Finnish and 170k for Russian.\nFigure 3 shows the learning curves, and Table 3\nshows the final perplexity at convergence for mod-\nels of different sizes. We see thatsmaller LMs with\nmorphological segmentation yield better perplex-\nity compared to larger LMs with BPE segmenta-\ntion. The smaller LMs have 132M parameters in\nFinnish, and 276M parameters in Russian, whereas\nSegmenter Size steps (k) PPL\nFinnish\nbpe (78k) Base 757 23.69\nSMp Small 542 21.22\nbpe Small 833 29.23\nSMp Base 724 16.65\nRussian\nbpe (170k) Base 660 24.97\nSMp Small 651 23.39\nbpe Small 1029 28.99\nSMp Base 816 19.08\nTable 3: Pre-training results for GPT language models\nof different sizes with BPE vs. SMp, compared in blue.\nBase models are the same as in Table 1.\nthe base LMs have 189M parameters in Finnish,\n354M parameters in Russian—which is 43% and\n28% bigger respectively.6 This further confirms\nRQ4: morphological segmentation can help reduce\nthe model size, and improve the sustainability of\nthe models in the training and inference phases.\n4.3 Fine-tuning for downstream tasks\nWe do not try to optimize the fine-tuning settings\nfor the specific downstream tasks, since we do not\naim for state-of-the-art performance. Rather our\ngoal is to explore the impact of segmentation on\nthe performance of LMs on downstream tasks. We\nrun each downstream task experiment three times,\nand report the mean and standard deviation (σ) of\nthe resulting scores. We evaluate classification task\nperformance with 3 metrics: Accuracy, F1, and\nMatthews Correlation Coefficient (MCC).\nTopic classification (Finnish): Table 4 shows\nin-domain and out-of-domain topic classification\nfor both LM types. The performance of the LMs\nis overall quite close. The best performing model\non the In-domain task is BERT with SMp segmen-\ntation, with an average accuracy of 92.6%. This\nis around 1% higher than its corresponding BPE-\nsegmented LM. The best model on the Out-of-\ndomain task is GPT with SM segmentation, with av-\nerage accuracy of 79.3%. This is about 3% higher\nthan its corresponding BPE-segmented LM.\nAll models achieve very good scores on In-\ndomain data, and relatively reasonable scores on\nOut-of-domain. We apply the t-test to compare the\nmorphologically-segmented LMs with their corre-\nsponding BPE-segmented LMs. Only BERT with\nSM shows a significantly worse performance than\n6Details about the numbers of parameters for all models\nare given in Appendix A, Tables 10 and 11.\n7418\n(a) Finnish GPT\n (b) Russian GPT\nFigure 3: Learning curves for pre-training language models with different model sizes.\nIn-domain (YLE corpus) Out-of-domain (Ylilauta corpus)\nAccuracy F1-measure MCC Accuracy F1-measure MCC\nAvg. σ Avg. σ Avg. σ Avg. σ Avg. σ Avg. σ\nGPT\nbpe (233k) 90.0 1.4 85.8 2.0 86.8 1.8 75.8 0.3 70.3 2.0 68.1 0.6\nmorfessor 89.4 1.0 85.0 1.4 85.7 1.6 78.4 2.4 72.2 3.4 71.2 3.3\nbpe (149k) 87.9 1.2 84.2 1.2 84.1 1.6 77.2 2.2 70.8 3.3 69.7 3.5\nSM 90.1 2.1 86.2 3.1 87.0 2.7 79.3 3.6 74.4 4.9 72.9 4.8\nbpe (78k) 89.1 2.8 84.1 2.5 85.4 1.5 77.5 2.9 71.1 3.6 70.1 3.9\nSMp 90.1 1.5 86.5 1.8 86.7 2.4 77.6 1.5 71.9 1.8 70.2 2.3\nSMp-sm 88.4 2.8 84.8 3.5 84.9 3.6 75.5 2.5 69.4 4.3 67.3 3.4\nbpe-sm 88.9 2.5 83.7 2.8 85.0 3.5 75.0 2.3 68.5 2.9 66.4 3.3\nBERT\nbpe (233k) 90.5 2.5 89.7 1.0 87.4 1.6 72.5 3.7 67.1 4.7 64.1 4.9\nmorfessor 90.2 1.1 89.6 2.8 87.0 3.5 75.4 1.5 69.5 1.8 67.5 2.2\nbpe (149k) 91.8 1.3 91.3 1.0 89.3 1.6 75.4 3.7 70.7 4.3 67.5 4.8\nSM 89.9 0.2 89.2 0.3 86.6 0.3 76.2 3.2 71.2 2.7 68.5 3.6\nbpe (78k) 91.4 1.5 90.7 1.4 88.6 2.1 77.0 3.3 71.3 2.5 69.7 4.0\nSMp 92.6 0.2 91.9 0.2 90.1 0.3 74.1 4.3 69.3 4.4 66.0 6.1\nTable 4: Fine-tuning for Finnish topic classification.\nBERT with BPE (149k), with p-value of 0.03. All\nother t-tests for all comparison pairs in all metrics\ndo not show a significant difference in performance,\nwith p-values all greater than 0.05. This suggests\nthat the Finnish LMs with different segmentations\nperform comparably on topic classification after\nfine-tuning.7 This includes smaller-size models\n(SMp-sm and BPE-sm). GPT with SMp-sm, which\nis smaller in size, is comparable with GPT with\nBPE (78k), which is of regular size. The minimum\np-value of their t-test for all metrics is 0.2, while\nthe maximum p-value is 0.41. This further suggests\npotential benefits for sustainability.\nFor reference, Virtanen et al. (2019) reach 90.6%\naccuracy on classifying YLE data, and 79.8% on\n7We use an unpaired t-test with unequal variance assump-\ntion. The null hypothesis is: a morphologically-segmented\nLM performs comparably to its BPE-segmented counterpart;\nthe alternative hypothesis is: one LM outperforms the other.\nYlilauta, with similar fine-tuning settings.\nTopic classification (Russian): Table 5 shows\nthe performance of each LM after fine-tuning.\nOverall, all models achieve very good performance\non this task. The GPT model with SMp achieves\nthe best overall performance, with an average ac-\ncuracy of 91.4%, about 1% higher than the corre-\nsponding model with SM. As in Finnish, the small\nGPT model with SMp-sm is slightly better (1%)\nthan base GPT with BPE (170k).\nAmong the BERT models, BERT with SM\nachieves the best performance overall with 87.3%\naccuracy. This is relatively 8% higher in accu-\nracy and F1 than the corresponding model with\nBPE (516k), and about 11% higher in MCC. Com-\npared to Finnish, the difference between models\nwith morphological segmentation and BPE is larger\n(though not significantly) in accuracy and F1. To\n7419\nAccuracy F1 MCC\nAvg. σ Avg. σ Avg. σ\nGPT\nbpe (488k) 89.8 2.3 85.5 3.7 86.2 3.1\nMorfessor 89.3 0.9 84.6 0.4 85.8 1.1\nbpe (516k) 91.4 0.5 88.1 1.2 88.7 0.5\nSM 90.5 0.5 86.1 0.4 87.4 0.7\nbpe (170k) 88.5 1.8 83.5 2.2 84.5 2.5\nSMp 91.1 0.7 87.1 1.9 88.2 0.9\nSMp-sm 89.4 2.2 84.4 2.0 85.7 3.0\nbpe-sm 88.6 1.7 83.6 1.7 84.8 2.2\nBERT\nbpe (488k) 87.0 2.8 82.1 3.3 82.7 3.7\nMorfessor 82.7 4.3 77.6 4.3 77.0 5.9\nbpe (516k) 81.2 3.9 76.9 4.1 75.2 4.8\nSM 87.3 2.0 83.2 2.5 83.1 2.7\nbpe (170k) 87.1 1.9 82.5 1.9 82.8 2.4\nSMp 83.5 4.4 77.9 4.3 77.8 5.6\nTable 5: Fine-tuning for Russian topic classification.\nAccuracy MCC\nAvg. σ Avg. σ\nFinnish\nbpe (233k) 95.0 0.1 94.0 0.1\nMorfessor 94.0 0.1 92.9 0.0\nbpe (149k) 95.0 0.0 94.0 0.0\nSM 95.4 0.0 94.5 0.0\nbpe (78k) 95.3 0.1 94.5 0.1\nSMp 95.4 0.0 94.5 0.0\nRussian\nbpe (488k) 97.6 0.0 97.2 0.0\nMorfessor 97.5 0.0 97.1 0.0\nbpe (516k) 97.7 0.0 97.3 0.0\nSM 97.6 0.0 97.2 0.0\nbpe (170k) 98.0 0.0 97.7 0.0\nSMp 97.9 0.0 97.5 0.0\nTable 6: Fine-tuning for Finnish PoS tagging.\nconfirm this, we apply the t-test, comparing the\nmorphological BERTs with their corresponding\nBPE-segmented BERTs, for accuracy and F1.\nThe average t-test p-values on morphological\nBERT vs. BPE BERT are 0.1 and 0.09, for accuracy\nand F1, respectively; the p-values for the t-tests in\nFinnish are 0.45 and 0.42. This indicates more sig-\nnificant difference between morphological BERT\nvs. BPE BERT in Russian than in Finnish. But\nthe performance of morphological BERTs is still\ncomparable with BPE BERTs in terms of accuracy\nand F1, as their p-values are over 0.05.\nPublic leaderboards show state-of-the-art accu-\nracy of about 96% on this task.8\nPoS tagging: Table 6 shows the results for\nFinnish and Russian. All models achieve very\ngood accuracy and MCC. The BERT models with\n8Kaggle leaderboard, text classification (RU).\nAccuracy MCC\nAvg. σ Avg. σ\nGPT\nbpe (488k) 55.0 1.9 17.7 6.9\nMorfessor 52.5 1.9 15.1 5.5\nbpe (516k) 48.8 2.9 11.0 6.6\nSM 52.2 1.7 15.9 3.6\nbpe (170k) 52.7 6.5 6.2 8.4\nSMp 56.0 8.3 2.4 0.5\nSMp-sm 49.0 3.2 8.2 7.7\nbpe-sm 56.6 5.6 6.8 9.7\nBERT\nbpe (488k) 57.1 2.9 15.8 3.3\nMorfessor 60.5 6.8 10.6 9.3\nbpe (516k) 55.7 2.2 14.6 8.3\nSM 53.7 2.4 16.4 3.1\nbpe (170k) 54.8 5.0 11.7 2.1\nSMp 52.1 4.1 14.8 4.6\nTable 7: Fine-tuning for Russian linguistic acceptability\nSM and SMp achieve the best performance, but\nthis is only slightly better than the performance\nof BERT with BPE (149k) and BPE (78k), while\nBERT with Morfessor is worse by 1%. In Rus-\nsian, LMs with morphological segmentation have\ngenerally the same performance as LMs with BPE\n(less than 0.1% difference). Overall, performance\nis very close on this downstream task.\nFor comparison, Virtanen et al. (2019) achieve\n98.23% accuracy for Finnish with fine-tuning on\nthe same dataset; others achieve 97.8% accuracy\nfor Russian on this task.9\nLinguistic acceptability: Table 7 shows the re-\nsults on Russian. This is a very difficult task, with\naccuracy only around 50–60%. Overall, the BERT\nmodel with Morfessor segmentation achieves the\nbest accuracy, not significantly better than the cor-\nresponding BPE, which p-value of their t-test is\n0.25. BERT and GPT achieve a relatively close\nperformance. The small GPT model with SMp seg-\nmentation performs worse than regular GPT with\nBPE (170k), in relative terms by 7% on accuracy,\nbut better on MCC, by 32%.\nFor comparison, GPT-3 in (Mikhailov et al.,\n2022) reaches 55.8% accuracy on this task, while\nBERT reaches 75.9% accuracy.\nParaphrasing: Table 8 shows the results of ex-\nperiments on paraphrase generation, another ex-\ntremely challenging task, even for the human. The\nevaluation metric chrF++, from machine transla-\ntion (Popovi´c, 2017, 2015), uses the F-score statis-\ntic for character n-gram and word n-gram matches.\n9XLM-RoBERTa base, UD POS tagging: Russian\n7420\naveragechrF++ σ\nFinnish\nbpe (233k) 28.9 0.5\nMorfessor 29.6 0.5\nbpe (149k) 30.8 1.2\nSM 31.5 0.5\nbpe (78k) 32.8 0.6\nSMp 31.9 1.4\nSMp-sm 27.2 0.3\nbpe-sm 28.2 2.1\nRussian\nbpe (488k) 24.0 0.1\nMorfessor 25.8 0.7\nbpe (516k) 26.2 0.2\nSM 26.0 0.6\nbpe (170k) 22.6 0.8\nSMp 28.9 0.0\nSMp-sm 22.2 0.8\nbpe-sm 22.6 0.3\nTable 8: Fine-tuning for paraphrase generation\nThe Finnish models and most of the Russian\nmodels show fairly similar performance. The per-\nformance of the Russian models with SMp segmen-\ntation is relatively better than the models with BPE\nsegmentation by 28%. The small Russian models\nwith SMp segmentation is 2% below the regular\nmodel with BPE (170k), while the small Finnish\nmodel with SMp-sm segmentation is 21% below\nthe regular model with BPE (78k), in relative terms.\n5 Conclusions and future work\nWe have explored the impact of intelligent seg-\nmentation algorithms on the performance of lan-\nguage models. We experiment with four languages:\nFinnish, Russian, Turkish, and English, with an\nin-depth investigation of the first two.10 We train\ntwo language models—GPT and BERT—and com-\npare the statistical segmentation algorithm, BPE,\nwith two morphological segmentation algorithms:\nMorfessor and StateMorph.\nWe aim to show that LMs trained on a vocabulary\nbased on morphological information are better than\nLMs trained on “naïve” sub-word segments pro-\nduced by BPE. Although BPE does not explicitly\nmodel morphology, it will inevitably stumble into\ndiscovering some morphemes as well—because\nmany morphemes also happen to be frequent sub-\nwords. This makes BPE a tough baseline to beat.\n10The languages are chosen as representatives of their re-\nspective sub-families—Finno-Ugric and Slavic—which have\nvery rich morphology, both verbal and nominal, certainly\namong the richest among the European languages.\nWe explore four research questions: does mor-\nphological segmentation help LMs— 1: reach\nlower perplexity; 2: learn and converge faster; 3:\nperform at least as well on downstream tasks; 4:\nperform at least as well with smaller model size.\nWe show that LMs trained with morphological\nsegmentation reach much lower perplexity (except\nMorfessor, which has the largest vocabulary) than\nLMs trained with BPE (RQ1). We also show that\nLMs trained with morphological segmentation con-\nverge faster than LMs trained with BPE (RQ2).\nWe evaluate the performance of language mod-\nels on several downstream tasks (RQ3). The results\nshow that the performance of LMs with morpholog-\nical segmentation (including smaller LMs) is com-\nparable to models with BPE. While performance\non the topic classification tasks is quite convincing,\nwe meet several challeges with other downstream\ntasks. The tasks are so high-level and so difficult,\nand the baseline performance is so low, that the\ngains from “smarter” segmentation may not be\neasy to demonstrate directly. The languages we\nwork with have a paucity of “standard” datasets for\ndownstream tasks with sufficient labeled data. In\nthe future, we will investigate more languages and\nmore downstream tasks. However, the theoretical\nresults from RQ1, 2 and 4 are convincing.\nTo investigate the impact of segmentation on\nmodel size (RQ4), we pre-train a smaller version\nof each LM with StateMorph, for each language.\nWe show that—for a fixed vocabulary size—small\nLMs with StateMorph segmentation have lower\nperplexity than regular-sized LMs with BPE seg-\nmentation.\nThis suggests that morphological segmentation\ncan reduce the size of language models, and im-\nprove the sustainability of LMs (RQ4). Sustain-\nability is impacted by RQ2, but even more so by\nRQ4, since RQ2 affects only the training, whereas\nRQ4 affects training and inference.\nIn future work, we plan to experiment with more\nmorphological segmentation algorithms, a broader\nrange of languages, and more types of language\nmodels, such as Transformer-XL (Dai et al., 2019)\nand XLNet (Yang et al., 2019). As a final point,\nthe smarter segmentations that we have tested have\nmuch room for improvement—e.g., we can expect\nthat supervised or rule-based morphological seg-\nmentation will be still better than the unsupervised\nsegmentation that we have tested so far.\n7421\nAcknowledgements\nThis research was supported in part by BusinessFin-\nland (Grant 42560/31/2020), and by a grant from\nthe Helsinki Institute for Information Technology\n(HIIT). We are grateful for assistance from Javad\nNouri.\nLimitations\nWe acknowledge that this work has several limi-\ntations. First, we use only two language model\narchitectures, GPT and BERT. Second, we use only\ntwo languages, Finnish and Russian, for each ar-\nchitecture. Third, we acknowledge our selection of\nsegmentation algorithms is limited; other segmen-\ntation algorithms exist, both supervised or unsuper-\nvised. We plan to investigate other languages and\nthe impact of different segmentation algorithms as\nwell as different LM architectures and settings in\nfuture work.\nWe also acknowledge that the size of the lexicon\nis a factor which impacts the performance of the\nlanguage model. Due to limited computational\nresources, we experimented with a limited choice\nof lexicon sizes, where some of them may not be\noptimal. We plan to investigate further the effects\nof lexicon size. Lastly, we explored only a limited\nnumber of downstream tasks, which may not reveal\nthe complete picture about the performance of a\nlanguage model.\nEthics Statement\nThis data used in this work is mostly open data,\nor data used with explicit permission from its pub-\nlisher. We do not use any private data, nor any data\nthat is not allowed to be used for research purposes.\nReferences\nJulien Abadji, Pedro Ortiz Suarez, Laurent Romary, and\nBenoît Sagot. 2022. Towards a Cleaner Document-\nOriented Multilingual Crawled Corpus. arXiv e-\nprints, page arXiv:2201.06642.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd International\nConference on Learning Representations, ICLR, San\nDiego, CA, USA.\nKhuyagbaatar Batsuren, Gábor Bella, Aryaman Arora,\nViktor Martinovic, Kyle Gorman, Zdenˇek Žabokrt-\nský, Amarsanaa Ganbold, Šárka Dohnalová, Magda\nŠevˇcíková, Kateˇrina Pelegrinová, Fausto Giunchiglia,\nRyan Cotterell, and Ekaterina Vylomova. 2022. The\nSIGMORPHON 2022 shared task on morpheme seg-\nmentation. In Proceedings of the 19th SIGMOR-\nPHON Workshop on Computational Research in Pho-\nnetics, Phonology, and Morphology, pages 103–116,\nSeattle, Washington.\nKaj Bostrom and Greg Durrett. 2020. Byte pair encod-\ning is suboptimal for language model pretraining. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 4617–4624, Online.\nKyunghyun Cho, Bart van Merriënboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder-decoder\nfor statistical machine translation. In Proceedings\nof the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 1724–\n1734, Doha, Qatar.\nMathias Creutz and Krista Lagus. 2002. Unsupervised\ndiscovery of morphemes. arXiv preprint cs/0205057.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a fixed-length context. arXiv preprint\narXiv:1901.02860.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nPhilip Gage. 1994. A new algorithm for data compres-\nsion. C Users Journal, 12(2):23–38.\nStig-Arne Grönroos, Sami Virpioja, and Mikko Ku-\nrimo. 2020. Morfessor EM+ Prune: Improved sub-\nword segmentation with expectation maximization\nand pruning. arXiv preprint arXiv:2003.03131.\nStig-Arne Grönroos, Sami Virpioja, Peter Smit, and\nMikko Kurimo. 2014. Morfessor FlatCat: An HMM-\nbased method for unsupervised and semi-supervised\nlearning of morphology. In Proceedings of COLING,\nthe 25th International Conference on Computational\nLinguistics, pages 1177–1185.\nPeter Grünwald. 2007. The Minimum Description\nLength Principle. MIT Press.\nValentin Hofmann, Janet Pierrehumbert, and Hinrich\nSchütze. 2021. Superbizarre is not superb: Deriva-\ntional morphology improves BERT’s interpretation\nof complex words. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing, pages 3594–3608,\nOnline.\nSébastien Jean, Kyunghyun Cho, Roland Memisevic,\nand Yoshua Bengio. 2015. On using very large tar-\nget vocabulary for neural machine translation. In\n7422\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing, pages 1–10, Beijing, China.\nJenna Kanerva, Filip Ginter, Li-Hsin Chang, Iiro Ras-\ntas, Valtteri Skantsi, Jemina Kilpeläinen, Hanna-Mari\nKupari, Jenna Saarni, Maija Sevón, and Otto Tarkka.\n2021. Finnish paraphrase corpus. In Proceedings\nof the 23rd Nordic Conference on Computational\nLinguistics (NoDaLiDa), pages 288–298, Reykjavik,\nIceland.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 66–75, Melbourne, Australia.\nJindˇrich Libovický, Helmut Schmid, and Alexander\nFraser. 2022. Why don’t people use character-level\nmachine translation? In Findings of the Association\nfor Computational Linguistics: ACL 2022 , pages\n2470–2485, Dublin, Ireland.\nThang Luong, Hieu Pham, and Christopher D. Manning.\n2015. Effective approaches to attention-based neural\nmachine translation. In Proceedings of the 2015 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1412–1421, Lisbon, Portugal.\nVladislav Mikhailov, Tatiana Shamardina, Max\nRyabinin, Alena Pestova, Ivan Smurov, and Ekate-\nrina Artemova. 2022. RuCoLA: Russian corpus of\nlinguistic acceptability. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 5207–5227, Abu Dhabi,\nUnited Arab Emirates.\nBenjamin Minixhofer, Jonas Pfeiffer, and Ivan Vuli ´c.\n2023. CompoundPiece: Evaluating and improving\ndecompounding performance of language models.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Haji ˇc, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal Dependencies v2:\nAn evergrowing multilingual treebank collection. In\nProceedings of the Twelfth Language Resources and\nEvaluation Conference, pages 4034–4043, Marseille,\nFrance. European Language Resources Association.\nJavad Nouri and Roman Yangarber. 2017. Learning\nmorphology of natural language as a finite-state gram-\nmar. In SLSP: the 5th International Conference on\nStatistical Language and Speech Processing, pages\n44–57. Springer.\nHyunji Hayley Park, Katherine J. Zhang, Coleman Ha-\nley, Kenneth Steimel, Han Liu, and Lane Schwartz.\n2021. Morphology Matters: A multilingual language\nmodeling analysis. Transactions of the Association\nfor Computational Linguistics, 9:261–276.\nBen Peters and Andre F. T. Martins. 2022. Beyond char-\nacters: Subword-level morpheme segmentation. In\nProceedings of the 19th SIGMORPHON Workshop\non Computational Research in Phonetics, Phonology,\nand Morphology, pages 131–138, Seattle, Washing-\nton.\nMaja Popovi´c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\ntenth workshop on statistical machine translation ,\npages 392–395.\nMaja Popovi´c. 2017. chrF++: words helping character\nn-grams. In Proceedings of the second conference on\nmachine translation, pages 612–618.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJorma Rissanen. 1998. Stochastic complexity in statisti-\ncal inquiry, volume 15. World scientific.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1715–1725, Berlin, Germany.\nTatiana Shavrina and Olga Shapovalova. 2017. To\nthe methodology of corpus construction for machine\nlearning: «Taiga» syntax tree corpus and parser. Pro-\nceedings of \"Corpora-2017\" Conference, pages 78–\n84.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nAdvances in neural information processing systems,\n27.\nCagri Toraman, Eyup Halit Yilmaz, Furkan ¸ Sahinuç,\nand Oguzhan Ozcelik. 2022. Impact of tokenization\non language models: An analysis for Turkish. arXiv\npreprint arXiv:2204.08832.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nThe impact of student initialization on knowledge\ndistillation. CoRR, abs/1908.08962.\nSami Virpioja, Peter Smit, Stig-Arne Grönroos, Mikko\nKurimo, et al. 2013. Morfessor 2.0: Python imple-\nmentation and extensions for Morfessor Baseline.\nTechnical report, Aalto University.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough:\nBERT for Finnish. CoRR, abs/1912.07076.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff\nKlingner, Apurva Shah, Melvin Johnson, Xiaobing\nLiu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,\n7423\nTaku Kudo, Hideto Kazawa, Keith Stevens, George\nKurian, Nishant Patil, Wei Wang, Cliff Young, Jason\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals,\nGreg Corrado, Macduff Hughes, and Jeffrey Dean.\n2016. Google’s neural machine translation system:\nBridging the gap between human and machine trans-\nlation. CoRR, abs/1609.08144.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\nA Pre-Training details\nBase Small\nTransformer blocks (L) 12 8\nSelf-attention heads (A) 12 8\nHidden size (H) 768 512\nFeed-forward/filter 1024 512\nTable 9: Hyperparameters for GPT/BERT models.\nWe pre-train all language models with the same\nsettings for the optimizer. We apply AdamW op-\ntimizer, mostly with the default parameters from\nTorch, except we use (0.9, 0.998) for the beta val-\nues. We use batch size of 100 for most Russian and\nFinnish models, except Russian GPT with Morfes-\nsor, SM, and BPE with the corresponding vocabu-\nlary size. We are not able to fit Russian models and\ndata into the GPU memory during training, there-\nfore we compensate by training with batch size 50,\nand accumulate gradient for 2 steps to achieve the\nsame effect as batch size of 100. We use batch size\n300 for all English and Turkish GPTs to speed up\nthe pre-training process. We use the same learning\nrate (5e-5) for all models, except for the Finnish\nGPT models, which uses a larger learning rate (1e-\n4). We use a different number of validation steps\ndepending on model type: 300 for the GPT mod-\nels, and 1200 for BERT models. We use a larger\nnumber of validation steps, because the random\nmasking mechanism decrease the actual tokens for\nvalidation, as compared to GPT models. We use\nEarlyStop with a patience of 10 and δ= 10−5.\nTable 9 shows the hyperparameters used for pre-\ntraining GPT and BERT models. Tables 10 and 11\nshow the vocabulary size of different segmenta-\ntions, and their corresponding overall number of\nparameters, when pre-training Finnish and Russian\nlanguage models, respectively.\nWe pre-train in two stages for all of the Finnish\nand Russian GPT models. We first pre-train each\nSegmenter Size V oc (K) #Param (M)\nMorfessor Base 233 467\nbpe Base 233 473\nSM Base 149 315\nbpe Base 149 315\nSMp Base 78 188\nSMp Small 78 132\nbpe Base 78 189\nbpe Small 78 134\nTable 10: Segmentation of Finnish data. Corresponding\nvocabulary sizes (thousands of tokens), and model size\n(millions of parameters)\nSegmenter Size V oc (K) #Param (M)\nMorfessor Base 487 921\nbpe Base 488 923\nSM Base 518 979\nbpe Base 516 974\nSMp Base 171 355\nSMp Small 171 276\nbpe Base 170 354\nbpe Small 170 275\nTable 11: Segmentation of Russian data. Corresponding\nvocabulary sizes (thousands of tokens), and model size\n(millions of parameters)\nmodel on a GPU cluster for the first 36 hours. Each\nnode in the cluster is equipped with 4 Nvidia A100-\n40G GPUs and two AMD Rome 7H12 CPUs with\n64 cores each. We use all GPUs and 64 cores for\neach job. We then continue the pre-training on a\nbigger cluster, until the models converge, where\neach cluster node is equipped with 4 AMD MI250x\nGPU modules. Each GPU module has a AMD\nEPYC \"Trento\" CPU and two GPU dies with 64GB\nof HBM2 memory, which makes 8 GPUs overall\nin one node. We request the same number of GPUs\nand CPU cores for each model, as in the jobs we\nrun on the first cluster. For BERT models as well\nas English and Turkish GPT models, we pre-train\nonly on the second cluster, with the same settings\nas for the GPT models.\nA.1 Segmentation and capitalization\nA technical point of difference between BPE and\nthe other segmenters: BPE distinguishes tokens ap-\npearing word-initially vs. elsewhere, with a special\nsymbol “_”, to designate whitespace, for example,\n“_ba·king”. Morfessor and StateMorph do not distin-\nguish in their output whether a morpheme appears\n7424\ninitially or medially.11\nTherefore we perform segmentation as follows:\npre-segment all words in lower case, then convert\nthe tokens back to the original case, mark all tokens\nappearing word-initially with “_” (as BPE does),\nand collect the exact vocabulary for pre-training\nlanguage models.\nB Fine-tuning details\nWe follow a similar training process as pre-training.\nWe conduct all fine-tuning tasks on the second clus-\nter, which is also used in pre-training, with the same\nresources as in pre-training. We use the same opti-\nmizer (AdamW), and same optimizer parameters\nas in pre-training. We apply the same learning rate\n(5e-5), and the same batch size of 50; we use 50\nrather than 100 in pre-training, so that all models\ncan be fine-tuned uniformly. We uniformly accu-\nmulate gradient for 5 steps.\n11They do model the beginning or end of a word when\nlearning to segment.\n7425"
}