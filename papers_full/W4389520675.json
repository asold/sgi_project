{
  "title": "Beat LLMs at Their Own Game: Zero-Shot LLM-Generated Text Detection via Querying ChatGPT",
  "url": "https://openalex.org/W4389520675",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5093457297",
      "name": "Biru Zhu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5104303696",
      "name": "Lifan Yuan",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5102542015",
      "name": "Ganqu Cui",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5029135114",
      "name": "Yangyi Chen",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A5102590191",
      "name": "Chong Fu",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A5113257067",
      "name": "Bingxiang He",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5093457298",
      "name": "Yangdong Deng",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5100320723",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5046448314",
      "name": "Maosong Sun",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5110272283",
      "name": "Ming Gu",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4288334893",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W3101600240",
    "https://openalex.org/W3101891351",
    "https://openalex.org/W2951080837",
    "https://openalex.org/W4323927417",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2963204221",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3170490008",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3114326827",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4318351452",
    "https://openalex.org/W4361193535",
    "https://openalex.org/W3016164449",
    "https://openalex.org/W4317553041",
    "https://openalex.org/W2950681488",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3176270593",
    "https://openalex.org/W2969958763",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2949615363"
  ],
  "abstract": "Biru Zhu, Lifan Yuan, Ganqu Cui, Yangyi Chen, Chong Fu, Bingxiang He, Yangdong Deng, Zhiyuan Liu, Maosong Sun, Ming Gu. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7470–7483\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nBeat LLMs at Their Own Game: Zero-Shot LLM-Generated\nText Detection via Querying ChatGPT\nBiru Zhu1, Lifan Yuan2∗, Ganqu Cui2, Yangyi Chen3, Chong Fu4, Bingxiang He2,\nYangdong Deng1†, Zhiyuan Liu2†, Maosong Sun2, Ming Gu1\n1 School of Software, Tsinghua University, China\n2 Department of Computer Science and Technology, Tsinghua University, China\n3 University of Illinois Urbana-Champaign, USA 4 Zhejiang University, China\nzbr19@mails.tsinghua.edu.cn, lievanyuan173@gmail.com\n{dengyd, liuzy}@tsinghua.edu.cn\nAbstract\nLarge language models (LLMs), e.g., ChatGPT,\nhave revolutionized the domain of natural lan-\nguage processing because of their excellent per-\nformance on various tasks. Despite their great\npotential, LLMs also incur serious concerns\nas they are likely to be misused. There are al-\nready reported cases of academic cheating by\nusing LLMs. Thus, it is a pressing problem\nto identify LLM-generated texts. In this work,\nwe design a zero-shot black-box method for\ndetecting LLM-generated texts. The key idea\nis to revise the text to be detected using the\nChatGPT model. Our method is based on the\nintuition that the ChatGPT model will make\nfewer revisions to LLM-generated texts than it\ndoes to human-written texts, because the texts\ngenerated by LLMs are more in accord with the\ngeneration logic and statistical patterns learned\nby LLMs like ChatGPT. Thus, if the text to\nbe detected and its ChatGPT-revised version\nhave a higher degree of similarity, the text is\nmore likely to be LLM-generated. Extensive\nexperiments on various datasets and tasks show\nthat our method can effectively detect LLM-\ngenerated texts. Moreover, compared with\nother detection methods, our method has better\ngeneralization ability and is more stable across\nvarious datasets. The codes are publicly avail-\nable at https://github.com/thunlp/LLM-gene\nrated-text-detection.\n1 Introduction\nBenefiting from the emergent ability, large lan-\nguage models (LLMs) have demonstrated excellent\nperformance on a large number of natural language\nprocessing tasks (Brown et al., 2020; Sanh et al.,\n2022). Recently, LLMs (e.g., ChatGPT) trained to\nfollow instructions (Ouyang et al., 2022; Wei et al.,\n2022) can generate high-quality responses when\ngiven specific instructions and input data by a user.\n∗Lifan Yuan has graduated from Huazhong University\nof Science and Technology and he is doing an internship in\nTHUNLP group now.\n†Corresponding author.\nThe strong emergent abilities, however, also trig-\nger social concerns for the misuse of LLMs. The\nacademic cheating by using LLMs to write essays\nand homework has been repeatedly reported. The\nfabricated news generated by LLMs also causes\na significant problem to our society. Thus, it is\nan important problem to detect whether a piece\nof text is generated by the LLM. Following pre-\nvious works (Jawahar et al., 2020; Mitchell et al.,\n2023), we formulate the problem of detecting LLM-\ngenerated texts as a binary classification task, i.e.,\nclassifying whether a piece of text is generated by\nthe LLM or written by human.\nPrevious methods for detecting LLM-generated\ntexts can be classified into two categories. The\nfirst category is the zero-shot detection method (So-\nlaiman et al., 2019; Gehrmann et al., 2019; Mitchell\net al., 2023), which needs to access the model’s out-\nput logits or losses for detection. However, many\nLLM services provided by commercial companies\ndo not expose the model’s output logits or losses\nat the inference time. Thus, these methods have\nto rely on local proxy models to access the output\ninformation. However, the inconsistency between\nthe online model and the local proxy model may\nlead to poor detection performance. The second\ncategory is the supervised fine-tuning method (Guo\net al., 2023), which trains a Deep Neural Network\n(DNN)-based classifier with the labeled training\ndata. It is prone to overfitting the training data and\nthus may have poor generalization ability.\nTo address the abovementioned problems, we\npropose a zero-shot black-box detection method\nleveraging the ChatGPT model. Our method is\nbased on the following finding. If we use the Chat-\nGPT model to revise texts, it will make fewer revi-\nsions to LLM-generated texts than it does to human-\nwritten texts. The underlying reason may be that\nthe LLM-generated texts conform to the genera-\ntion logic and statistical patterns learned by the\nChatGPT model. The overall framework of our\n7470\nChatGPT\nYes No\n  written by human generated by LLM\n(3) Score\n(4) Compare\nGiven the text\nRevised text \nRevise (2) Revise\n\"Revise the following text: \" +(1) Add the prompt\nFigure 1: The overall framework of our approach.\nFirstly, the text to be detected xi is revised by Chat-\nGPT. Then our method calculates the similarity score si\nbetween the text xi and its corresponding revised text˜xi.\nIf the similarity score si is no smaller than the threshold\nε, the text xi is likely to be generated by LLM.\nmethod is shown in Figure 1. Firstly, we leverage\nthe ChatGPT model to revise the text to be detected.\nThen we measure the similarity between the text to\nbe detected and its ChatGPT-revised version using\nthe similarity metric. The applied similarity met-\nric could be BLEU score (Papineni et al., 2002),\nROUGE score (Lin, 2004), BERTScore (Zhang*\net al., 2020) or BARTScore (Yuan et al., 2021). The\nLLM-generated texts are more likely to achieve\nhigher similarity scores compared with human-\nwritten texts.\nWe perform experiments on six datasets includ-\ning question answering and summarization genera-\ntion tasks. The experimental results prove that our\nmethod can effectively detect LLM-generated texts.\nBesides, we compare our method with other zero-\nshot detection methods and the supervised fine-\ntuning method. The experimental results show that\nour detection method is more universal across var-\nious datasets and has better generalization ability.\nMoreover, our method is robust to dataset biases.\n2 Related Work\nLarge Language Models. Large language mod-\nels (LLMs) have achieved fantastic performance\non a huge number of natural language processing\ntasks (Brown et al., 2020). Some recent works\nshow that LLMs can be trained to follow instruc-\ntions with instruction-tuning (Ouyang et al., 2022;\nWei et al., 2022; Chung et al., 2022). LLMs pro-\nvided by AI companies like OpenAI can generate\nresponses to users’ inputs and solve different prob-\nlems. Many applications are built upon them, e.g.,\nchatbots. Though LLMs act as useful tools, mis-\nusing them may cause bad influence, e.g., students\nuse LLMs to do their homework.\nLLM-Generated Text Detection. Previous\nLLM-generated text detection methods can\ngenerally be classified into two categories. The\nfirst category is the zero-shot detection method (So-\nlaiman et al., 2019; Gehrmann et al., 2019;\nMitchell et al., 2023). However, these methods\nrely on the model’s output logits or losses. If there\nis no access to the output logits or losses of the\nsource model that generates the text to be detected,\nthese methods can use a proxy model to get the\nproxy model’s output logits or losses for detection.\nHowever, the difference between the source model\nand the proxy model may cause poor detection\nperformance. Thus, it is essential to design a\ndetection method without accessing the model’s\noutput logits or losses.\nThe second category is to train a DNN-\nbased classifier using some labeled human-written\nand LLM-generated samples (Guo et al., 2023;\nUchendu et al., 2020). However, it is hard to col-\nlect sufficient labeled training samples to train a\ngeneralizable DNN-based classifier (Bakhtin et al.,\n2019; Uchendu et al., 2020). Moreover, training\nDNN-based classifiers are vulnerable to backdoor\nattacks (Qi et al., 2021) and the DNN-based detec-\ntor is lack of interpretability. Thus, it is important\nto build an explainable detector without training\nDNN-based classifiers.\nJawahar et al. (2020) survey some automatic de-\ntection methods and identify some future directions\nfor building useful detectors. He et al. (2023) de-\nvelop a benchmark for evaluating existing detection\nmethods and call for more robust detection meth-\nods. In this paper, we aim to design a robust and\nexplainable black-box detection method without\ntraining DNN-based classifiers.\n3 Method\nIn this section, we illustrate our method step by step.\n(1) Given the texts {x1,x2,...,x n}to be detected,\nwe first revise the texts using the ChatGPT model.\nSpecifically, we add the prompt, i.e., “Revise the\nfollowing text: ”, before the texts to be detected.\nThen we feed the modified texts into the ChatGPT\nmodel Mand use its responses {˜x1,˜x2,..., ˜xn}as\nthe revised texts.\n7471\n˜xi = M(“Revise the following text: ” +xi),\ni∈{1,2,...n}\n(1)\n(2) Then we use an unsupervised similarity metric\nSim to calculate the similarity score si between\nthe original text xi and its revised text ˜xi.\nsi = Sim(xi,˜xi) (2)\nWe take the BARTScore as an example. Given\na sequence-to-sequence pre-trained model like\nBART model (Lewis et al., 2020) which is pa-\nrameterized by θ, the BARTScore is calculated\nusing the log probability of the target text xi given\nthe revised text ˜xi as the source text. The target\ntext is tokenized into a sequence of tokens: xi =\n{xi1,xi2,...,x ik}.\nBARTScore =\nk∑\nt=1\nlog p\n(\nxit|xi(j<t),˜xi,θ\n)\n(3)\nThe BARTScore can measure the semantic cov-\nerage between the source text and the target\ntext (Yuan et al., 2021). For the calculation of\nBARTScore in this paper, we use the BARTScore-\nCNN, which uses a BART model that is fine-tuned\non the CNNDM dataset (Hermann et al., 2015). For\nmore details of other evaluated similarity metrics,\nplease refer to appendix B. The LLM-generated\ntext and its revised text are more similar compared\nwith the human-written text and its revised text.\nThus, a higher similarity score si indicates that the\ntext xi is more likely to be LLM-generated.\n4 Experiments\nIn this section, we evaluate the detection perfor-\nmance of our method and compare our method\nwith other detection methods.\n4.1 Experimental Setting\nDatasets. For the summarization generation task,\nwe perform experiments on MultiNews (Fabbri\net al., 2019), GovReport (Huang et al., 2021) and\nBillSum (Kornilova and Eidelman, 2019) datasets.\nWe use the representative LLM, i.e., ChatGPT (gpt-\n3.5-turbo), as the source model to generate sum-\nmaries to be detected. For the question answer-\ning task, we consider three datasets including Fi-\nnance (Maia et al., 2018), Medicine (Chen et al.,\n2020), and Reddit Eli5 (Fan et al., 2019). In main\nexperiments, we use the ChatGPT-generated texts\ncollected by Guo et al. (2023). For more details of\nthe datasets, please refer to appendix D. We also\ndemonstrate that our method can detect texts that\nare generated by other source models in section 4.3.\n4.2 Main Experiments\n4.2.1 Comparisons in Zero-Shot Setting\nZero-Shot Methods for Comparison.We com-\npare our method with other zero-shot detec-\ntion methods including Log-Likelihood (Solaiman\net al., 2019), Rank (Gehrmann et al., 2019), Log-\nRank (Mitchell et al., 2023), Entropy (Gehrmann\net al., 2019) and DetectGPT (Mitchell et al., 2023).\nThese zero-shot detection methods are based on dif-\nferences between output losses/logits of the model\non human-written texts and LLM-generated texts.\nThey consider that the model will be more famil-\niar with LLM-generated texts. For example, the\nLog-Likelihood method takes the negative loss\nof the model on the text to be detected as the\nLog-Likelihood score. A higher Log-Likelihood\nscore indicates the text is more likely to be LLM-\ngenerated. The details of these detection methods\nare shown in appendix C. Since the logits or losses\nof ChatGPT can not be accessed at the inference\ntime, we use GPT-2-medium (Radford et al., 2019)\nas the proxy model for deriving the logits or losses\nfor these methods, following He et al. (2023).\nMetrics. Following Mitchell et al. (2023), we\nuse the AUROC as the evaluation metric. For our\nmethod, we use the BARTScore-CNN (Yuan et al.,\n2021) as the similarity metric. We also prove that\nour method is effective under different similarity\nmetrics. For more details of our method’s detec-\ntion performance when using different similarity\nmetrics including BLEU, ROUGE and BERTScore,\nplease refer to appendix A.\nResults. As shown in Table 1, our method achieves\ngood detection performance across various datasets.\nSpecifically, the AUROC is consistently higher\nthan 70% for our method on all datasets. The av-\nerage AUROC on all datasets is 90.05% for our\nmethod. However, for other zero-shot detection\nmethods, the AUROC is low on MultiNews, Gov-\nReport and BillSum datasets. The reason may be\nthat the training data of ChatGPT is different from\nthat of GPT-2-medium. Thus, the GPT-2-medium\nmodel may be unfamiliar with some ChatGPT-\ngenerated texts, reflected in the high losses on some\nChatGPT-generated texts. As a result, the detec-\n7472\nMethod Finance Medicine Reddit Eli5 MultiNews GovReport BillSum Average Value\nLog-Likelihood 98.61 98.85 99.08 55.07 36.17 34.15 70.32\nRank 92.24 97.62 78.81 56.61 42.44 37.03 67.46\nLog-Rank 98.64 98.91 99.26 56.12 36.93 35.79 70.94\nEntropy 97.07 98.79 97.78 52.31 31.54 24.50 67.0\nDetectGPT 88.56 96.43 83.66 40.36 43.63 36.57 64.87\nOur Method (BARTScore-CNN) 97.40 95.06 99.15 86.28 75.64 86.77 90.05\nTable 1: Comparisons with other zero-shot detection methods. The evaluation metric is AUROC (%). The \"Average\nValue\" is the average performance on all datasets.\nSource Dataset Fine-Tuning Our Method\nFinance 81.77 80.05\nMedicine 67.35 80.62\nReddit Eli5 52.68 69.93\nMultiNews 51.21 83.44\nGovReport 83.62 82.67\nBillSum 65.03 81.39\nTable 2: The average accuracy (%) on all target datasets\nof the fine-tuning method and our method in the OOD\nsetting when using original datasets as source datasets.\nSource Dataset Fine-Tuning Our Method\nBiased Finance 54.89 80.36\nBiased Medicine 56.31 80.48\nBiased GovReport 54.34 85.74\nTable 3: The average accuracy (%) on all target datasets\nof the fine-tuning method and our method in the OOD\nsetting when using biased datasets as source datasets.\ntion performance of the methods that rely on the\noutput losses of the proxy model will degrade. For\nthe statistics of BARTScores and Log-Likelihood\nscores, please refer to appendix A.\n4.2.2 Comparisons with Fine-Tuning\nComparisons in Vanilla Setting.We compare\nthe accuracy of our method with the fine-tuning\nmethod. To calculate the accuracy of our method,\nwe need to determine a threshold. If the test sam-\nple’s similarity score between it and its revised text\nis no smaller than the threshold, it is predicted as\nthe LLM-generated text. The detailed steps to find\nthe optimal threshold on the source dataset are as\nfollows1. Firstly, we get the similarity scores for all\nsamples in the source dataset and their ChatGPT-\nrevised versions. We take the minimum similarity\nscore and the maximum similarity score. Then,\nwe split the range between the minimum similar-\n1For the fine-tuning method, the source dataset is the train-\ning dataset, which is used to train a classifier. Our method\ndoes not need to train a classifier. For our method, we find the\noptimal threshold on the source dataset.\nity score and the maximum similarity score into\n10,000 uniform intervals. We get 10,000 interpola-\ntion values between the minimum similarity score\nand the maximum similarity score. Finally, we\nuse the best interpolation value with the highest\nclassification accuracy on the source dataset as the\noptimal threshold.\nWe find that the fine-tuning method performs\nwell in the situation where the distributions of the\ntraining and testing data are identical, as shown in\nappendix A. However, in practice, it is difficult to\nguarantee that the distributions of the training and\ntesting data are identical. Thus, we also test the out-\nof-distribution (OOD) robustness. Specifically, we\nuse one dataset as the training (source) dataset and\nthe other datasets as the testing (target) datasets.\nAs shown in Table 2, the OOD performance of\nour method is better than the fine-tuning method\non Medicine, Reddit Eli5, MultiNews and BillSum\ndatasets. The fine-tuning method may overfit the\ntraining datasets and thus has poor OOD perfor-\nmance. Overall, our method is more robust under\nthe OOD setting. The reason is that the difference\nbetween degrees of revisions for the human-written\ntexts and LLM-generated texts is the universal fea-\nture among various datasets.\nComparisons on Biased Datasets. The fine-\ntuning method is proven to easily overfit the dataset\nbiases (McCoy et al., 2019). The dataset biases\nare some spurious correlations that are not shared\namong all datasets of a task (Lynch et al., 2023).\nWe construct the biased datasets by adding the pre-\nfix “Answer: ” to the human-written answers of Fi-\nnance and Medicine datasets and adding the prefix\n“Summarization: ” to the human-written summaries\nof the GovReport dataset.\nAs shown in Table 3, for the fine-tuning method,\nthe classification models that are fine-tuned on bi-\nased datasets have very poor OOD performance,\nwith all average accuracy falling below 60%. The\nreason is that the trained classification model with\n7473\nSource Model\nBERTScore BARTScore-CNN\nFinance Medicine Finance Medicine\nText-davinci-003 90.18 92.94 83.38 91.41\nText-davinci-002 86.83 86.21 88.90 90.81\nVicuna 78.68 88.40 95.43 97.88\nTable 4: Performance of our method when detecting\ntexts that are generated by various source models. The\nevaluation metric is AUROC (%).\nthe fine-tuning method overfits dataset biases, i.e.,\nthe mapping from the prefix to the label “human-\nwritten”. However, these dataset biases are not\nuniversal features among all datasets. Also, from\nthe results in Table 3, we can see that our method is\nrobust to the dataset bias. The average accuracy on\nall target datasets of our method is above 80% with\nwhichever biased dataset as the source dataset. The\nreason is that our method leverages the similarity\nbetween a pair of texts and is less influenced by\nthe inserted prefix biases. For more details, please\nrefer to appendix A.\n4.3 Detecting Texts Generated by Various\nSource Models\nWe demonstrate that our method can be applied\nto detect texts generated by various source mod-\nels such as text-davinci-003, text-davinci-002 and\nVicuna (Chiang et al., 2023). Firstly, we use the\nsource model to generate answers of the Finance\nand Medicine datasets. Then we leverage ChatGPT\nto revise texts. As shown in Table 4, our method\ncan detect texts generated by various source mod-\nels. The AUROC is higher than 78% under all the\nevaluated settings. The revisions for other source\nmodels’ generated texts are minor compared with\nthe revisions for human-written texts when using\nChatGPT to revise the texts. The texts generated by\nChatGPT and other source models may share some\nsimilar characteristics. The deeper reason may be\nthat the training data of ChatGPT and other source\nmodels may have similar or overlapping parts.\n4.4 Case Study\nWe conduct the case study on a human-written\nsummary and the summary generated by Chat-\nGPT for the same bill from the BillSum dataset.\nFrom Figure 3 in the appendix, we can see that for\nthe ChatGPT-generated summary, the revisions are\nmostly about synonym replacements or adding a\nword. However, from Figure 2 in the appendix, we\ncan see that the revision is relatively large for the\nhuman-written summary, which may change the\nsentences’ structures. For example, a long human-\nwritten sentence with complex logic is revised into\na relatively shorter sentence by ChatGPT. For more\ndetails, please refer to appendix A.\n5 Conclusion\nIn this paper, we design a simple and effective\nbaseline method for detecting LLM-generated texts.\nOur proposed method is based on the intuition that\nthe ChatGPT model revises less for LLM-generated\ntexts than it does for human-written texts. Com-\npared with previous methods, our method neither\nneeds to train a DNN-based classifier nor requires\naccess to the source model’s output logits or losses.\nThus, our method has better generalization ability\nand is more practical. We perform experiments on\nsix datasets of question answering and summariza-\ntion generation tasks. The experimental results on\nvarious datasets show that our method can effec-\ntively detect LLM-generated texts. Moreover, our\nmethod is robust to dataset biases.\n6 Limitations\nAlthough our method is simple and effective, it still\nhas some limitations. Firstly, our method needs\nto query the ChatGPT model and thus may cost\nmoney. Secondly, if the text to be detected is ex-\ntremely short, the difference of revisions for human-\nwritten and LLM-generated texts may not be evi-\ndent, which may lower the detection performance\nof our method.\nAcknowledgments\nThis work is supported by the National Key\nR&D Program of China (No.2022ZD0116312), Na-\ntional Natural Science Foundation of China (No.\n62236004) and Institute Guo Qiang at Tsinghua\nUniversity.\nBiru Zhu, Lifan Yuan and Chong Fu designed\nthe methods. Biru Zhu, Lifan Yuan, Ganqu Cui,\nYangyi Chen and Bingxiang He designed the exper-\niments. Yangdong Deng, Zhiyuan Liu, Maosong\nSun and Ming Gu advised the project and partici-\npated in the discussion.\nReferences\nAnton Bakhtin, Sam Gross, Myle Ott, Yuntian\nDeng, Marc’Aurelio Ranzato, and Arthur Szlam.\n7474\n2019. Real or fake? learning to discriminate ma-\nchine from human generated text. arXiv preprint\narXiv:1906.03351.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nShu Chen, Zeqian Ju, Xiangyu Dong, Hongchao Fang,\nSicheng Wang, Yue Yang, Jiaqi Zeng, Ruisi Zhang,\nRuoyu Zhang, Meng Zhou, et al. 2020. Meddialog: a\nlarge-scale medical dialogue dataset. arXiv preprint\narXiv:2004.03329.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nAlexander Fabbri, Irene Li, Tianwei She, Suyi Li, and\nDragomir Radev. 2019. Multi-news: A large-scale\nmulti-document summarization dataset and abstrac-\ntive hierarchical model. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1074–1084, Florence, Italy. Asso-\nciation for Computational Linguistics.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. Eli5:\nLong form question answering. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3558–3567.\nSebastian Gehrmann, Hendrik Strobelt, and Alexander\nRush. 2019. GLTR: Statistical detection and visual-\nization of generated text. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics: System Demonstrations, pages 111–116,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,\nJinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng\nWu. 2023. How close is chatgpt to human experts?\ncomparison corpus, evaluation, and detection. arXiv\npreprint arXiv:2301.07597.\nXinlei He, Xinyue Shen, Zeyuan Chen, Michael Backes,\nand Yang Zhang. 2023. Mgtbench: Benchmarking\nmachine-generated text detection. arXiv preprint\narXiv:2303.14822.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. Advances in neural information\nprocessing systems, 28.\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng\nJi, and Lu Wang. 2021. Efficient attentions for long\ndocument summarization. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1419–1436, Online.\nAssociation for Computational Linguistics.\nGanesh Jawahar, Muhammad Abdul-Mageed, and\nVS Laks Lakshmanan. 2020. Automatic detection\nof machine generated text: A critical survey. In Pro-\nceedings of the 28th International Conference on\nComputational Linguistics, pages 2296–2309.\nAnastassia Kornilova and Vladimir Eidelman. 2019.\nBillSum: A corpus for automatic summarization of\nUS legislation. In Proceedings of the 2nd Workshop\non New Frontiers in Summarization, pages 48–56,\nHong Kong, China. Association for Computational\nLinguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and comprehen-\nsion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7871–7880.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAengus Lynch, Gbètondji JS Dovonon, Jean Kaddour,\nand Ricardo Silva. 2023. Spawrious: A benchmark\nfor fine control of spurious correlation biases. arXiv\npreprint arXiv:2303.05470.\nMacedo Maia, Siegfried Handschuh, André Freitas,\nBrian Davis, Ross McDermott, Manel Zarrouk, and\nAlexandra Balahur. 2018. Www’18 open challenge:\nfinancial opinion mining and question answering. In\nCompanion proceedings of the the web conference\n2018, pages 1941–1942.\n7475\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the wrong reasons: Diagnosing syntactic heuris-\ntics in natural language inference. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3428–3448, Florence,\nItaly. Association for Computational Linguistics.\nEric Mitchell, Yoonho Lee, Alexander Khazatsky,\nChristopher D Manning, and Chelsea Finn. 2023.\nDetectgpt: Zero-shot machine-generated text detec-\ntion using probability curvature. arXiv preprint\narXiv:2301.11305.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nFanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang,\nZhiyuan Liu, Yasheng Wang, and Maosong Sun.\n2021. Hidden killer: Invisible textual backdoor at-\ntacks with syntactic trigger. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 443–453, Online. Asso-\nciation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda\nAskell, Ariel Herbert-V oss, Jeff Wu, Alec Rad-\nford, Gretchen Krueger, Jong Wook Kim, Sarah\nKreps, et al. 2019. Release strategies and the so-\ncial impacts of language models. arXiv preprint\narXiv:1908.09203.\nAdaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee.\n2020. Authorship attribution for neural text gener-\nation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 8384–8395, Online. Association for\nComputational Linguistics.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text genera-\ntion. In Advances in Neural Information Processing\nSystems, volume 34, pages 27263–27277. Curran As-\nsociates, Inc.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\n7476\nAppendix\nA Additional Experimental Results\nResults of Different Similarity Metrics. We\nevaluate the effectiveness of our method with dif-\nferent similarity metrics, including BLEU score,\nROUGE score, BERTScore and BARTScore-CNN.\nWe use the similarity score as the prediction score\nand the labels of LLM-generated texts are set as\n1. The experimental results are shown in Table 5.\nFrom the experimental results, we can see that the\nAUROC is above 60% no matter which similar-\nity metric is used on all datasets, which demon-\nstrates the effectiveness of our method. The results\nverify our intuition, i.e., compared with human-\nwritten texts and their corresponding revised texts,\nLLM-generated texts and their corresponding re-\nvised texts are more similar.\nBesides, we can find that the performance of\nour detection method varies with different sim-\nilarity metrics. For example, when the similar-\nity metric is BARTScore-CNN, the average AU-\nROC on all datasets is 90.05%. When the simi-\nlarity metric is BLEU score, the average AUROC\non all datasets is 77.29%. The BLEU score and\nROUGE score are based on n-gram matches. The\nBERTScore and BARTScore-CNN are DNN-based\nsimilarity metrics. To sum up, the DNN-based\nsimilarity metrics perform better than those based\non n-gram matches. The reason may be that the\nDNN-based similarity metrics can capture the se-\nmantic similarity better than those based on n-gram\nmatches. The BERTScore performs better than\nBARTScore-CNN on MultiNews, GovReport and\nBillSum datasets. However, the BARTScore-CNN\nachieves the highest average performance among\nall similarity metrics.\nStatistics of BARTScores. For both human-\nwritten texts and ChatGPT-generated texts, we\nrecord the mean value and variance of BARTScores\nbetween the original texts and their corresponding\nrevised texts. We use BARTScore-CNN as the sim-\nilarity metric to calculate the BARTScores. The\nhigher BARTScore indicates the higher similarity\nbetween the text and its corresponding revised text.\nAs shown in Table 6, the average BARTScore of\nChatGPT-generated texts and their revised texts\nis higher than that of human-written texts for\neach dataset. The ChatGPT model revises more\nfor human-written texts compared with ChatGPT-\ngenerated texts. Also, the variance of BARTScores\nof ChatGPT-generated texts and their revised texts\nis lower than that of human-written texts for each\ndataset. This means that the extents of revisions\nvary largely among human-written texts while vary-\ning little among ChatGPT-generated texts.\nLog-Likelihood Scores on Different Datasets.\nThe Log-Likelihood method takes the negative loss\nof the model on the text to be detected as the Log-\nLikelihood score. A higher Log-Likelihood score\nmeans this piece of text is more likely to be LLM-\ngenerated. Since the output losses of ChatGPT\nare not available at the inference time, we use the\nGPT-2-medium model as the proxy model for the\nLog-Likelihood method. We record the mean value\nof negative losses of the GPT-2-medium model (i.e.,\nthe Log-Likelihood scores) on human-written texts\nand that on ChatGPT-generated texts. As shown in\nTable 7, the losses derived from the GPT-2-medium\nmodel are high on the ChatGPT-generated texts of\nMultiNews, GovReport and BillSum.\nResults of Comparisons with Fine-Tuning in\nVanilla Setting. We compare the accuracy (ACC)\nof our method with the fine-tuning method. Besides\nconsidering the setting that the training (source)\nand testing (target) data have the same distribu-\ntion, we also test OOD robustness. For the ex-\nperiments where the training (source) and testing\n(target) datasets are of the same distribution, we\nevaluate the detection performance in the classic\ncross-validation manner. For the experiments of\ntesting OOD robustness, we use one dataset as\nthe training dataset (source dataset) and a different\ndataset as the testing dataset (target dataset).\nThe experimental results are shown in Table 8.\nWe can see that the fine-tuning method achieves\ngood performance when the training and testing\ndata have the same distribution. However, as shown\nin Table 8, there are 14 results whose accuracy are\nbelow 60% for the fine-tuning method. Also, we\ncan see that there are 2 results whose accuracy are\nbelow 60% for our method. From this point of\nview, our method has better generalization ability\ncompared with the fine-tuning method. The trained\nclassification model is easy to overfit the training\ndata for the fine-tuning method. Thus, the gener-\nalization ability of the fine-tuning method is poor.\nOur method is more robust under the OOD setting.\nResults of Comparisons with Fine-Tuning on\nBiased Datasets. The dataset biases are some\nspurious correlations that are not shared among\n7477\nSimilarity Metric Finance Medicine Reddit Eli5 MultiNews GovReport BillSum Average Value\nBLEU Score 83.93 90.18 83.80 70.73 69.55 65.54 77.29\nROUGE-1 F1 Score 85.43 90.28 82.34 73.16 68.69 70.25 78.36\nROUGE-2 F1 Score 87.60 90.18 81.17 69.85 65.18 63.40 76.23\nROUGE-L F1 Score 87.23 87.48 80.43 72.10 64.72 66.13 76.35\nBERTScore 84.41 89.47 89.87 86.73 83.55 91.62 87.61\nBARTScore-CNN 97.40 95.06 99.15 86.28 75.64 86.77 90.05\nTable 5: Performance of our method using different similarity metrics. The evaluation metric is AUROC (%). The\n\"Average Value\" is the average performance on all datasets.\nDataset H-Mean H-Var C-Mean C-Var\nFinance -2.504 0.271 -1.373 0.088\nMedicine -2.780 0.493 -1.630 0.339\nReddit Eli5 -3.373 0.331 -1.651 0.161\nMultiNews -2.250 0.209 -1.614 0.127\nGovReport -1.826 0.218 -1.411 0.125\nBillSum -2.004 0.200 -1.401 0.099\nTable 6: Statistics of BARTScores computed on human-\nwritten/ChatGPT-generated texts and their correspond-\ning revised texts. The prefix “H” and “C” repre-\nsent “human-written” and “ChatGPT-generated”, re-\nspectively. “Mean” and “Var” represent mean value\nand variance, respectively.\nDataset H-Score C-Score\nFinance -3.355 -1.864\nMedicine -3.888 -1.893\nReddit Eli5 -3.689 -2.062\nMultiNews -2.994 -2.941\nGovReport -2.634 -2.788\nBillSum -2.526 -2.694\nTable 7: The mean value of negative losses (Log-\nLikelihood scores) on human-written texts and that on\nChatGPT-generated texts. “H-Score”/“C-Score” rep-\nresents the mean value of negative losses on human-\nwritten texts/ChatGPT-generated texts.\nall datasets of a specific task (McCoy et al., 2019;\nLynch et al., 2023). We select 3 datasets as the\nsource datasets that the classification model is fine-\ntuned on, including Finance, Medicine, and Gov-\nReport. The classification models trained on these\nthree original datasets have relatively good OOD\nperformance, as shown in Table 8. We construct\nthe biased datasets by adding the prefix “Answer:\n” to the human-written answers of Finance and\nMedicine datasets and adding the prefix “Summa-\nrization: ” to the human-written summaries of the\nGovReport dataset. We do not perform any addi-\ntional operations on the LLM-generated texts.\nWe evaluate the fine-tuning method and our\nmethod in the scenario where the source datasets\nare biased datasets. As shown in Table 9, for the\nfine-tuning method, the OOD performance of the\nclassification models that are fine-tuned on biased\ndatasets is very poor, with the accuracy below 60%\nin most evaluated situations. The reason is that\nthe trained classification model with supervised\nfine-tuning overfits dataset biases, i.e., the mapping\nfrom the prefix “Answer: ” or “Summarization:\n” to the label “human-written”. However, these\ndataset biases are not the universal useful features\namong all datasets and they can not be used for\nclassifying the human-written and LLM-generated\ntexts of other datasets without these biases. Thus,\nthe OOD performance of the fine-tuning method\ndeclines much when the training dataset contains\nbiases. Besides, as shown in Table 9, our method\nis robust to the dataset bias. Under all evaluated\nsituations, the accuracy of our method is above\n65%, which demonstrates that our method is still\neffective when the source dataset contains biases.\nResults of the Case Study.We show examples\nof a human-written summary and the summary\ngenerated by ChatGPT for the same bill from the\nBillSum dataset (Kornilova and Eidelman, 2019).\nSpecifically, we show the human-written summary\nand its ChatGPT-revised version in Figure 2. The\nChatGPT-generated summary and its ChatGPT-\nrevised version are shown in Figure 3. Some re-\nvised parts in the revised texts are highlighted in\nred color. From Figure 2 and Figure 3, we can see\nthat the revisions are more obvious for the human-\nwritten summary. For example, the human-written\nsentence “Prescribes procedural guidelines ... for\nNonperformance of Transportation” is revised into\nthe sentence “It also establishes guidelines ... for\nNonperformance of Transportation offers the ser-\nvice”. The ChatGPT model revises the long human-\nwritten sentence into a relatively shorter sentence.\nWe show the similarity scores between the\nhuman-written summary and its corresponding re-\nvised text, and the similarity scores between the\n7478\nSource/Target Dataset Finance Medicine Reddit Eli5 MultiNews GovReport BillSum\nFine-Tuning\nFinance 99.66 87.32 92.61 75.2 70.75 82.95\nMedicine 88.92 99.92 95.48 44.11 53.07 55.16\nReddit Eli5 52.15 60.26 99.99 49.98 50.02 50.98\nMultiNews 51.50 48.89 51.15 100.0 51.12 53.38\nGovReport 75.58 85.81 72.77 90.19 96.38 93.74\nBillSum 72.63 66.37 78.21 50.13 57.83 99.97\nOur Method\nFinance 93.72 88.92 87.60 77.35 68.26 78.14\nMedicine 91.97 90.25 93.41 77.85 65.44 74.41\nReddit Eli5 79.95 84.39 97.51 68.85 56.71 59.74\nMultiNews 92.72 90.65 92.01 77.9 66.35 75.46\nGovReport 92.91 84.95 81.51 75.18 69.22 78.79\nBillSum 93.10 86.32 83.18 75.65 68.69 78.59\nTable 8: Results of the fine-tuning method and our method when using original datasets as source datasets. The\nevaluation metric is ACC (%).\nSource/Target Dataset Finance Medicine Reddit Eli5 MultiNews GovReport BillSum\nFine-Tuning\nBiased Finance 100.0 53.44 58.55 53.43 52.57 56.46\nBiased Medicine 59.07 100.0 72.60 49.78 50.05 50.05\nBiased GovReport 53.44 54.96 53.17 57.40 100.0 52.73\nOur Method\nBiased Finance 94.93 89.65 88.57 77.85 67.94 77.79\nBiased Medicine 91.76 92.30 93.64 77.8 65.02 74.16\nBiased GovReport 93.01 90.37 90.78 78.03 88.76 76.49\nTable 9: Results of the fine-tuning method and our method when using biased datasets as source datasets. The\nevaluation metric is ACC (%). For the experiments where the training (source) and testing (target) datasets are of\nthe same distribution, the training (source) and testing (target) data are both with dataset biases. For the experiments\nof testing OOD robustness, the source dataset has the dataset bias while the target dataset (different from the source\ndataset) does not have the dataset bias.\nSimilarity Metric Human-Written ChatGPT-Generated\nBLEU Score 0.272 0.794\nROUGE-1 F1 Score 0.589 0.904\nROUGE-2 F1 Score 0.377 0.823\nROUGE-L F1 Score 0.529 0.865\nBERTScore 0.538 0.909\nBARTScore-CNN -2.67 -0.711\nTable 10: Similarity scores between the human-\nwritten/ChatGPT-generated summary and its revised\ntext in the case study, measured by different similarity\nmetrics.\nChatGPT-generated summary and its correspond-\ning revised text under different similarity metrics\nin Table 10. From the results in Table 10, we can\nsee that the similarity score between the ChatGPT-\ngenerated summary and its corresponding revised\ntext is higher than that between the human-written\nsummary and its corresponding revised text no mat-\nter which similarity metric is used.\nB Details of Similarity Metrics\nThe details of the similarity metrics are as follows.\n• BLEU score (Papineni et al., 2002): The\nBLEU score is calculated based on the over-\nlap between the hypothesis’s n-grams and the\nreference’s n-grams. In our scenario, the ref-\nerence is the text to be detected and the hy-\npothesis is its corresponding revised text. The\nhigher BLEU score indicates the higher simi-\nlarity between the text and its revised text.\n• ROUGE score (Lin, 2004): Similar to the\nBLEU score, the ROUGE score is also based\non the overlap between the hypothesis’s n-\ngrams and the reference’s n-grams. The\nhigher ROUGE score indicates the higher sim-\nilarity between two pieces of texts.\n• BERTScore (Zhang* et al., 2020): The\nBERTScore is calculated based on the co-\n7479\nAuthorizes passenger transportation in foreign-flag \ncruise vessels between Alaska ports, and between \nAlaska ports and those on the west coast of the \ncontiguous States. \nPrescribes procedural guidelines under which the \nSecretary of Transportation shall notify the owner or \noperator of one or more foreign-flag vessels that he \nshall terminate the authorization for the foreign-flag \nvessel to provide passenger transportation upon a \nshowing by the owner or charterer of a U.S. cruise \nvessel that the U.S. vessel is offering such passenger \nservice pursuant to a Certificate of Financial \nResponsibility for Indemnification of Passengers for \nNonperformance of Transportation. \nStates that coastwise privileges granted a foreign-flag \ncruise vessel under this Act shall expire on the \n365th day following receipt of the Secretary's \ntermination notification. \nThis legislation permits travel for passengers on \nforeign-flag cruise vessels between Alaskan ports \nand between Alaskan ports and those on the west \ncoast of the contiguous United States. \nIt also \nestablishes guidelines for the Secretary of \nTransportation to inform the owner or operator of a \nforeign-flag vessel that it must cease passenger \ntransportation if a US vessel with a Certificate of \nFinancial Responsibility for Indemnification of \nPassengers for Nonperformance of Transportation \noffers the service. These privileges for the foreign-\nflag vessel will expire 365 days after receiving the \nSecretary's notification of termination . \nHuman-Written Text\nRevised Human-Written Text\nFigure 2: The human-written text and its ChatGPT-revised version.\nThe text discusses a congressional finding regarding \nthe growing cruise ship industry in Alaska and the \nneed to encourage the use of US facilities, labor, \nand services. It notes the importance of protecting \nexisting US employment and economic activity and \nhighlights the significant economic benefits received \nby Vancouver through the industry. The text also \npermits the transportation of passengers in foreign-\nflag cruise vessels between ports in Alaska and on \nthe west coast of the contiguous states, provided \ncertain conditions are met. It allows coastwise \nprivileges for US cruise vessels and outlines a \nprocess for terminating foreign-flag vessels' service \nafter a notification period. The text provides \ndefinitions and a disclaimer regarding the \ntransportation of passengers and merchandise in \nCanadian vessels between ports in Alaska and the \nUS. \nThe text discusses a congressional finding regarding \nthe growing cruise ship industry in Alaska and \nemphasizes the need to prioritize US facilities, labor, \nand services. It stresses the importance of protecting \nexisting US employment and economic activity while \nrecognizing the significant economic benefits received \nby Vancouver through the industry. The text also \npermits the transportation of passengers in foreign-flag \ncruise vessels between ports in Alaska and on the west \ncoast of the contiguous states, \nsubject to specific \nconditions. It grants coastwise privileges for US cruise \nvessels and outlines a process for terminating foreign-\nflag vessels' service after a \nnotice period. Additionally, \nthe text provides definitions and a disclaimer \nregarding the transportation of passengers and \nmerchandise in Canadian vessels between ports in \nAlaska and the US. \nChatGPT-Generated Text Revised ChatGPT-Generated Text\nFigure 3: The ChatGPT-generated text and its ChatGPT-revised version.\nsine similarities of pairwise tokens’ represen-\ntations of two sentences. The tokens’ rep-\nresentations are encoded by the pre-trained\nlanguage model. The higher BERTScore indi-\ncates the higher similarity between two pieces\nof texts.\n• BARTScore (Yuan et al., 2021): For the calcu-\nlation of BARTScore in this paper, we use the\nBARTScore-CNN, which uses a BART model\nthat is fine-tuned on the CNNDM dataset (Her-\nmann et al., 2015). The revised text is input\nto the BART model and the original text to be\ndetected is the target text. The BARTScore\nis the negative loss between the output logits\nof the BART model and the target text. The\nhigher BARTScore indicates the higher simi-\nlarity between the text to be detected and its\ncorresponding revised text.\nC Details of the Compared Detection\nMethods\nThe details of the detection methods we compare\nour method with are as follows.\n(1) Log-Likelihood (Solaiman et al., 2019): The\ntext that needs to be detected is fed into the GPT-2-\nmedium model. The Log-Likelihood score is the\nnegative output loss of the model. A higher Log-\nLikelihood score means the text is more likely to\n7480\nStatistics Finance Medicine Reddit Eli5 MultiNews GovReport BillSum\nNumber of Samples 7866 2492 29190 4000 1878 3994\nAverage Length of H-Texts 175.6 82.2 147.9 202.1 303.1 182.1\nAverage Length of Revised H-Texts 136.3 97.6 102.0 158.8 223.8 156.3\nAverage Length of C-Texts 205.2 187.4 173.6 102.3 151.9 129.1\nAverage Length of Revised C-Texts 168.7 161.1 140.8 103.8 147.6 126.8\nTable 11: Dataset statistics. “H-Texts” and “C-Texts” represent the human-written texts and ChatGPT-generated\ntexts, respectively. The number of samples for each dataset takes both human-written samples and ChatGPT-\ngenerated samples into account.\nbe LLM-generated.\n(2) Rank (Gehrmann et al., 2019): The text that\nneeds to be detected is fed into the GPT-2-medium\nmodel. Then this method sorts the output logits in\ndescending order for each token. After sorting, the\nmethod gets the rank of each label token. Then the\nmethod gets the final rank score by averaging the\nrank scores of all label tokens. A lower rank score\nmeans the text is more likely to be generated by the\nLLM.\n(3) Log-Rank (Mitchell et al., 2023): A little\ndifferent from the Rank method, the Log-Rank\nmethod just adds an additional operation by apply-\ning a log function on the rank score of each token.\nSimilar to the rank score, a lower log-rank score\nmeans the text is more likely to be LLM-generated.\n(4) Entropy (Gehrmann et al., 2019): The text\nthat needs to be detected is fed into the GPT-2-\nmedium model. The Entropy method calculates the\nentropy of the softmax logits derived from the GPT-\n2-medium model and then averages the entropy of\neach token to get the final entropy score. A lower\nentropy score means the text is more likely to be\nLLM-generated.\n(5) DetectGPT (Mitchell et al., 2023): For the\nDetectGPT method, it perturbs the text to be de-\ntected using the T5-large (Raffel et al., 2020) model\nand gets the perturbed text. Then it applies a log\nfunction on the ratio of the original text’s proba-\nbility to the perturbed text’s probability to get the\nfinal ratio. If this ratio is high, it means the text is\nlikely to be LLM-generated.\n(6) Supervised Fine-Tuning (Guo et al., 2023):\nFor the implementation of the supervised fine-\ntuning method, we fine-tune theRoBERTaBASE (Liu\net al., 2019) model on the labeled training samples\nto get a classification model.\nD Implementation Details\nD.1 Experimental Setting\nDatasets. For the summarization generation task,\nwe perform experiments on the MultiNews (Fab-\nbri et al., 2019), GovReport (Huang et al., 2021)\nand BillSum (Kornilova and Eidelman, 2019)\ndatasets. For human-written texts, we directly\nuse the human-written summaries in the original\ndatasets. In the main experiments, we use the rep-\nresentative LLM, i.e., ChatGPT (gpt-3.5-turbo), as\nthe source model to generate the LLM-generated\ntexts to be detected. We use the prompt “Please\nsummarize the following text: ” to make the Chat-\nGPT model generate the summaries and get the\nChatGPT-generated summaries. For the question\nanswering task, the datasets we consider are Fi-\nnance (Maia et al., 2018), Medicine (Chen et al.,\n2020), and Reddit Eli5 (Fan et al., 2019). In the\nmain experiments, we use the ChatGPT-generated\ntexts collected by Guo et al. (2023) for these three\ndatasets. Specifically, we use the first human-\nwritten answer in the human-written answer list\nand the first ChatGPT-generated answer in the\nChatGPT-generated answer list for each question.\nFor some datasets, we sample some samples from\nthe original datasets. The detailed statistics of the\nnumber of samples and the average length of sam-\nples for each dataset are shown in Table 11.\nWhen we revise texts with ChatGPT, we use the\ngpt-3.5-turbo API provided by OpenAI. When gen-\nerating ChatGPT-generated summaries for Multi-\nNews, GovReport and BillSum datasets, we use the\ngpt-3.5-turbo API provided by OpenAI.\nAll experiments that call gpt-3.5-turbo API in\nthis paper are done before June 2023, with the gpt-\n3.5-turbo API being gpt-3.5-turbo-0301.\nSimilarity Metrics. For the calculation of\nBARTScore in all experiments in this paper, we use\nthe BARTScore-CNN, which uses a BART model\nthat is fine-tuned on the CNNDM dataset (Her-\n7481\nmann et al., 2015). For the ROUGE score and\nBERTScore, we use the F1 score.\nD.2 Main Experiments\nComparisons in Zero-Shot Setting.When cal-\nculating the AUROC performance of other zero-\nshot detection methods, we use the Log-Likelihood\nscore, negative rank score, negative log-rank score,\nnegative entropy score and log probability ra-\ntio as the prediction score, respectively, for Log-\nLikelihood, Rank, Log-Rank, Entropy and Detect-\nGPT methods. The labels of ChatGPT-generated\ntexts are set as 1.\nFor the Medicine dataset, we drop a pair of\nhuman-written and ChatGPT-generated answers of\none question for other zero-shot detection methods,\ndue to the NaN value of the Log-Likelihood score.\nFor the implementation of DetectGPT, we use the\nT5-large as the mask filling model and the number\nof perturbations is set as 10. The perturbation mode\nis set as “z” for DetectGPT.\nComparisons with Fine-Tuning in Vanilla Set-\nting. For the experiments where the training and\ntesting datasets are of the same distribution, we\nevaluate the detection performance in the classic\ncross-validation manner. Specifically, we first split\nthe whole dataset into 4 parts. We train the model\nand test the performance for 4 turns. In each turn,\nwe use 3 parts as the training part and the remaining\n1 part as the testing part. In each turn, we use a part\nthat is different from the testing parts in other turns\nas the testing part for this turn. Finally, we use\nthe total number of correctly predicted testing sam-\nples in 4 turns to divide the total number of testing\nsamples in 4 turns and get the final accuracy.\nFor the experiments of testing the OOD robust-\nness, we fine-tune the classification model with\nthree different seeds and get the average OOD per-\nformance as the final OOD performance for the\nfine-tuning method. The number of training epochs\nis set as 10 and the learning rate is set as 2 ×10−5\nfor the fine-tuning method.\nFor our method, we use the BARTScore-CNN as\nthe similarity metric to calculate similarity scores.\nComparisons with Fine-Tuning on Biased\nDatasets. For the experiments where the train-\ning and testing datasets are of the same distribution,\nwe evaluate the detection performance in the clas-\nsic cross-validation manner. Specifically, we first\nsplit the whole dataset into 4 parts. We train the\nmodel and test the performance for 4 turns. In\neach turn, we use 3 parts as the training part and\nthe remaining 1 part as the testing part. In each\nturn, we use a part that is different from the test-\ning parts in other turns as the testing part for this\nturn. Finally, we use the total number of correctly\npredicted testing samples in 4 turns to divide the\ntotal number of testing samples in 4 turns and get\nthe final accuracy.\nThe source dataset on which the classification\nmodel is fine-tuned is the biased dataset. For the\nexperiments where the training (source) and testing\n(target) datasets are of the same distribution, the\ntraining (source) and testing (target) data are both\nwith dataset biases. For the experiments of testing\nOOD robustness, the source dataset has the dataset\nbias while the target dataset (different from the\nsource dataset) does not have the dataset bias.\nFor the experiments of testing the OOD robust-\nness, we fine-tune the classification model with\nthree different seeds and get the average OOD per-\nformance as the final OOD performance for the\nfine-tuning method.\nFor our method, we use the BARTScore-CNN\nas the similarity metric.\nD.3 Detecting Texts Generated by Various\nSource Models\nFor generating LLM-generated texts with Vicuna,\nwe first obtain a Vicuna model using the LLaMA-\n7B model2 and the delta weight 3, following the\nofficial implementation4. Then we use the Vicuna\nmodel to generate texts.\nSome other details of the experiments are as fol-\nlows. For generating LLM-generated texts with\nthe text-davinci-0025 and Vicuna models, we use\nthe instruction “Please generate a long answer for\nthe following question: ”. For generating LLM-\ngenerated texts with the text-davinci-003 model6,\nwe do not add any additional instruction and just\ninput the questions of the Finance and Medicine\ndatasets into the model. For experiments of detect-\ning the text-davinci-002 model’s generated texts for\nthe Medicine dataset, we drop 15 pairs of human-\nwritten and LLM-generated texts that correspond\n2https://huggingface.co/decapoda-research/lla\nma-7b-hf\n3https://huggingface.co/lmsys/vicuna-7b-delta\n-v1.1\n4https://github.com/lm-sys/FastChat\n5https://openai.com/\n6https://openai.com/\n7482\nto 15 questions due to the blank answers returned\nby the text-davinci-002 model. We evaluate the\ndetection performance of our method using the\nBERTScore and BARTScore-CNN as the similar-\nity metric, respectively.\n7483",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.4932400584220886
    },
    {
      "name": "Shot (pellet)",
      "score": 0.49112269282341003
    },
    {
      "name": "Chemistry",
      "score": 0.21353664994239807
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ]
}