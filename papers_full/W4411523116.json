{
    "title": "Fairness Mediator: Neutralize Stereotype Associations to Mitigate Bias in Large Language Models",
    "url": "https://openalex.org/W4411523116",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5076884053",
            "name": "Yisong Xiao",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A5014870180",
            "name": "Aishan Liu",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A5081392445",
            "name": "Siyuan Liang",
            "affiliations": [
                "National University of Singapore"
            ]
        },
        {
            "id": "https://openalex.org/A5024067284",
            "name": "Xianglong Liu",
            "affiliations": [
                null,
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A5074103823",
            "name": "Dacheng Tao",
            "affiliations": [
                "Nanyang Technological University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3184144760",
        "https://openalex.org/W3128630643",
        "https://openalex.org/W2109308568",
        "https://openalex.org/W4308641598",
        "https://openalex.org/W4284968624",
        "https://openalex.org/W3123340107",
        "https://openalex.org/W4385562710",
        "https://openalex.org/W4404780679",
        "https://openalex.org/W4284697101",
        "https://openalex.org/W2730550703",
        "https://openalex.org/W4284681038",
        "https://openalex.org/W3016970897",
        "https://openalex.org/W4205460703",
        "https://openalex.org/W3034115845",
        "https://openalex.org/W2751749598",
        "https://openalex.org/W4388229648",
        "https://openalex.org/W4387068389",
        "https://openalex.org/W2129217160",
        "https://openalex.org/W4391579668",
        "https://openalex.org/W4386136237",
        "https://openalex.org/W3047533667",
        "https://openalex.org/W3206487987",
        "https://openalex.org/W6839328737",
        "https://openalex.org/W4411551627",
        "https://openalex.org/W4384304645",
        "https://openalex.org/W3176477796",
        "https://openalex.org/W3105882417",
        "https://openalex.org/W2186046013",
        "https://openalex.org/W3168398407",
        "https://openalex.org/W3035241006",
        "https://openalex.org/W3134354193",
        "https://openalex.org/W6784167499",
        "https://openalex.org/W4224249875",
        "https://openalex.org/W4396762170",
        "https://openalex.org/W4308731725",
        "https://openalex.org/W4220659214",
        "https://openalex.org/W4389209181",
        "https://openalex.org/W3185095134",
        "https://openalex.org/W4403780588",
        "https://openalex.org/W4323891867",
        "https://openalex.org/W4384154513",
        "https://openalex.org/W4389794056",
        "https://openalex.org/W4400267113",
        "https://openalex.org/W4308643061",
        "https://openalex.org/W2963526187",
        "https://openalex.org/W4284709622",
        "https://openalex.org/W3185212449",
        "https://openalex.org/W4281657280",
        "https://openalex.org/W4401943272",
        "https://openalex.org/W3135734416",
        "https://openalex.org/W4386242329",
        "https://openalex.org/W4391558518",
        "https://openalex.org/W3103934428",
        "https://openalex.org/W2792317186",
        "https://openalex.org/W4398239361",
        "https://openalex.org/W2623399293",
        "https://openalex.org/W4402443087",
        "https://openalex.org/W4205221547",
        "https://openalex.org/W4319736227",
        "https://openalex.org/W4287333591"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse applications, yet they inadvertently absorb spurious correlations from training data, leading to stereotype associations between biased concepts and specific social groups. These associations perpetuate and even amplify harmful social biases, raising significant concerns about fairness, which is a crucial issue in software engineering. To mitigate such biases, prior studies have attempted to project model embeddings into unbiased spaces during inference. However, these approaches have shown limited effectiveness due to their weak alignment with downstream social biases. Inspired by the observation that concept cognition in LLMs is primarily represented through a linear associative memory mechanism, where key-value mapping occurs in the MLP layers, we posited that biased concepts and social groups are similarly encoded as entity (key) and information (value) pairs, which can be manipulated to promote fairer associations. To this end, we propose Fairness Mediator (FairMed), an effective and efficient bias mitigation framework that neutralizes stereotype associations. Our framework comprises two main components: a stereotype association prober and an adversarial debiasing neutralizer. The prober captures stereotype associations encoded within MLP layer activations by employing prompts centered around biased concepts (keys) to detect the emission probabilities for social groups (values). Subsequently, the adversarial debiasing neutralizer intervenes in MLP activations during inference to equalize the association probabilities among different social groups. Extensive experiments across nine protected attributes show that our FairMed significantly outperforms state-of-the-art methods in effectiveness, achieving average bias reductions of up to 84.42",
    "full_text": null
}