{
  "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences",
  "url": "https://openalex.org/W4225727438",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2885677207",
      "name": "Mandy Guo",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3016646846",
      "name": "Joshua Ainslie",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2322051720",
      "name": "David Uthus",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2124070775",
      "name": "Santiago Ontañón",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2773709065",
      "name": "Jianmo Ni",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4224699826",
      "name": "Yun-Hsuan Sung",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2155460496",
      "name": "Yinfei Yang",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2470673105",
    "https://openalex.org/W2963204221",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2734330123",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W2786148476",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3098960752",
    "https://openalex.org/W2891534142",
    "https://openalex.org/W3171494313",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W4301633306",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3206557162",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2953280096",
    "https://openalex.org/W3169012807",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2970392338",
    "https://openalex.org/W3006983028",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W4205897796",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3169942382",
    "https://openalex.org/W3155147984",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W4287019748",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present LongT5, a new model that explores the effects of scaling both the input length and model size at the same time. Specifically, we integrate attention ideas from long-input transformers (ETC), and adopt pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call Transient Global (TGlobal), which mimics ETC’s local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization and question answering tasks, as well as outperform the original T5 models on these tasks. We have open sourced our architecture and training code, as well as our pre-trained model checkpoints.",
  "full_text": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 724 - 736\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nLongT5: Efﬁcient Text-To-Text Transformer for Long Sequences\nMandy Guo∗†, Joshua Ainslie ∗†, David Uthus ∗, Santiago Ontañón ∗\nJianmo Ni, Yun-Hsuan Sung, Yinfei Yang\nGoogle Research\n{xyguo, jainslie, duthus, santiontanon, jianmon, yhsung, yinfeiy}@google.com\nAbstract\nRecent work has shown that either (1) in-\ncreasing the input length or (2) increasing\nmodel size can improve the performance of\nTransformer-based neural models. In this pa-\nper, we present LongT5, a new model that\nexplores the effects of scaling both the in-\nput length and model size at the same time.\nSpeciﬁcally, we integrate attention ideas from\nlong-input transformers (ETC), and adopt pre-\ntraining strategies from summarization pre-\ntraining (PEGASUS) into the scalable T5 ar-\nchitecture. The result is a new attention mech-\nanism we call Transient Global (TGlobal),\nwhich mimics ETC’s local/global attention\nmechanism, but without requiring additional\nside-inputs. We are able to achieve state-of-\nthe-art results on several summarization and\nquestion answering tasks, as well as outper-\nform the original T5 models on these tasks.\nWe have open sourced our architecture and\ntraining code, as well as our pre-trained model\ncheckpoints.\n1 Introduction\nTransformer models such as BERT (Devlin et al.,\n2019), and other variants (Liu et al., 2019; Radford\net al., 2019; Raffel et al., 2019a; Lewis et al., 2020)\nhave achieved state-of-the-art results on many chal-\nlenging NLP tasks. Moreover, recent work in long-\ninput transformers (Ainslie et al., 2020; Zaheer\net al., 2020b; Beltagy et al., 2020; Tay et al., 2021)\nhas shown that increasing the input length a Trans-\nformer is able to process results in further perfor-\nmance gains. Additionally, it is also known that\nincreasing model size also leads to performance\ngains in many tasks (Kaplan et al., 2020).\nIn this paper, we present a new model, called\nLongT5, with which we explore the effects of scal-\ning both the input length and model size at the\nsame time. To achieve this, we integrate long-input\n∗Equal contributions.\n† Corresponding authors.\n0 4k 8k 16k33\n34\n35\n36\n37\n38\n39\n40Average ROUGE Score\nLongT5-XL\nHAT-BART\nBigBird-Pegasus\nPRIMER\nLED-large\nArxiv\n0 4k 8k 16k35\n36\n37\n38\n39\n40\n41\n42\nLongT5-XL\nHAT-BART\nBigBird-Pegasus\nPubMed\nInput Sequence Length\nFigure 1: The average ROUGE score ( (R-1 +R-2 +\nR-L)/3) of LongT5 and baseline models on arXiv and\nPubMed summarization tasks (Cohan et al., 2018) with\ndifferent input length (x axis). Baseline models: HAT-\nBART (Rohde et al., 2021), BigBird-PEGASUS (Za-\nheer et al., 2020b), PRIMER (Xiao et al., 2021),\nLED (Beltagy et al., 2020). The size of circle roughly\nindicates the # of parameters for each model.\ntransformer attention and pre-training ideas into\nthe scalable T5 (Raffel et al., 2019a) model archi-\ntecture. The resulting model, as shown in Figure 1,\nachieves state-of-the-art performance on several\ntasks which require handling long sequence inputs.\nRegarding attention, we design a new atten-\ntion mechanism, which we call Transient Global\n(TGlobal), that mimics ETC’s local/global mecha-\nnism (Ainslie et al., 2020). Importantly, TGlobal\nattention removes the need for the additional side\ninputs in ETC, in order to ﬁt within the T5 archi-\ntecture. The main idea of ETC’s local/global mech-\nanism is to introduce local sparsity in the attention\nmechanism to reduce the quadratic cost when scal-\ning to long inputs. Speciﬁcally, ETC only allows\ntokens in the input (called the long input) to attend\nto a local neighborhood, and adds a secondary input\ncalled the global memory, through which tokens in\nthe long input can attend to each other indirectly.\nOne disadvantage of this mechanism is that it re-\nquires designing this secondary global input for\neach new problem. In order to adapt it to T5, our\nnew TGlobal mechanism synthesizes these global\ntokens on the ﬂy (as aggregations of groups of\n724\ntokens in the input), at each attention layer. Our ex-\nperiments show that this mechanism results in only\na small degradation in performance with respect to\nfull attention in the same input length but allows\nthe model to scale to much larger input lengths,\nresulting in signiﬁcant performance gains.\nRegarding pre-training, we adopt the pre-\ntraining strategy in the PEGASUS (Zhang et al.,\n2019a) model. This pre-training strategy was origi-\nnally designed for abstractive summarization, but\nin our experiments, we found it also improves\nmodel performance for other tasks, such as ques-\ntion answering, and hence we adopted it in LongT5.\nThe key idea is to mask out key (principle) sen-\ntences from a document and ask the model to repro-\nduce them as a single string, as if it was a summary.\nWe evaluate LongT5 on several summariza-\ntion and question answering tasks (see Sections\n4.2.1 and 4.3.1 for detailed descriptions of these\ndatasets). Thanks to the scaling of both input length\nand model size, we achieve state-of-the-art results\non many of them.\nThe main contributions of this work are:\n•A new Transformer architecture, LongT5, that\nallows for scalingboth input length and model\nscale at the same time.\n•A new attention mechanism (TGlobal), which\nmimics ETC’s local/global mechanism but is\na drop-in replacement to regular attention for\nexisting Transformer architectures like T5.\n•An analysis of model performance when vary-\ning both input length and model size of vanilla\nT5 and LongT5 models (pushing both models\nup to the maximum lengths they can handle\nbefore encountering memory issues), to un-\nderstand the trade-offs in both performance\nand computation cost.\n•State-of-the-art results on the arXiv, PubMed,\nBigPatent, MediaSum, and TriviaQA datasets.\nFor Natural Questions, we used a slightly dif-\nferent formulation than the original tasks, and\nhence we do not make state-of-the-art claims.\n•We open source our model architecture1 and\ntraining code, as well as pre-trained model\ncheckpoints on GitHub2.\n1Published under the Flaxformer GitHub https:\n//github.com/google/flaxformer/tree/\nmain/flaxformer/architectures/longt5\n2https://github.com/google-research/\nlongt5\n2 T5\nT5 (Raffel et al., 2019a) is a transformer based text-\nto-text pre-trained language model that is gaining\npopularity for its uniﬁed framework that converts\nall text-based language problems into a text-to-text\nformat, and its ease to scale up in number of param-\neters (from 60M to 11B parameters) with model\nparallelism. With full attention transformer, T5 has\nbeen successfully applied to many NLP tasks, but\nthe tasks only require shorter input sequences. This\nis due to the limitation of quadratic computation\ngrowth with respect to input sequence length, re-\nsulting in larger memory consumption and longer\ntraining time. Recently, Press et al. (2021) explored\nscaling up T5 style models at inference time to\nlonger sequences than seen during training, but how\nto scale up T5 style models in the input sequence\nlength during training remains underexplored.\n3 LongT5\n3.1 Architecture\nWe extend the original T5 encoder with global-\nlocal attention sparsity patterns (Ainslie et al.,\n2020; Zaheer et al., 2020a) to handle long inputs.\nFor the work reported in this paper, we used a stan-\ndard T5 decoder since all of the tasks we considered\nrequire relatively short output sequence lengths.\nArchitecturally, the main difference between T5\nand LongT5 lies in the attention mechanism. We\nexperiment with two attention mechanism varia-\ntions for LongT5, illustrated in Figure 2: (1) Lo-\ncal Attentionand (2) Transient Global Attention\n(TGlobal). Both variations preserve several prop-\nerties of T5: relative position representations, sup-\nport for example packing, and compatibility with\nT5 checkpoints.\n3.1.1 Local Attention\nFor Local Attention, we simply replace the encoder\nself-attention operation in T5 with a sparse sliding-\nwindow local attention operation following the im-\nplementation in ETC (Ainslie et al., 2020). Specif-\nically, for a given local radius r, this formulation\nonly allows each token to attend r tokens to the left\nand right of it (see Figure 2.a). We found r = 127\nto be sufﬁcient in practice, where r is the number\nof neighboring tokens to the left and to the right.\nLocal Attention does not introduce any new pa-\nrameters and easily accommodates the attention\nmasking required for example packing 3. For a\n3Example packing refers to packing more than one short\n725\nAttention keys\nAttention queries\nAttention keys\nAttention queries\n \n \n \n \n \n \n \n \nAttention keys\nAttention queries\n \n \n \n \n \n \n \n \nx1 … xk xk+1 … xl g1 … gm\n+\n+\n…\nInput Tokens Global Tokens\nEach global token is the result of \naveraging k input tokens.\n \n \n \n \n \n \n \n \nEach input token can \nattend to its \nneighborhood (like in \nlocal attention), plus \nto all global tokens.\nEach input token can \nattend to its \nneighborhood: r \ntokens to the left, and \nr tokens to the right.\nrr\na) LongT5 Local Attention b) LongT5 Transient Global (TGlobal) Attention\nLayerNorm\n…\nFigure 2: Illustration of the two attention mechanisms we experimented with in LongT5.\ngiven choice of r, complexity is linear in input\nsequence length l: O(l ×r).\n3.1.2 Transient Global Attention (TGlobal)\nTo allow input tokens to interact with each other in\neach layer of the encoder at a longer range than Lo-\ncal Attention’s local radius, we introduceTransient\nGlobal Attentionas a modiﬁcation of ETC’s global-\nlocal attention in a “ﬁxed blocks” pattern. Namely,\nwe divide the input sequence into blocks of k to-\nkens, and for each block we compute a global token\nby summing (and then normalizing) the embed-\ndings of every token in the block (see Figure 2.b).\nNow when computing attention, we allow each\ninput token to attend not only to nearby tokens\nlike in Local Attention, but also to every global\ntoken. We call these global tokens transient be-\ncause in contrast to ETC-like global-local attention\npatterns, these tokens are dynamically constructed\n(and subsequently discarded) within each attention\noperation, removing any requirement for deciding\nwhich input tokens should be treated as “global”.\nTGlobal attention only introduces a couple new\nparameters4: (1) T5-style relative position biases\nrepresenting the distance from an input token’s\nblock to the block of each global token it’s attend-\ning to, and (2) T5-style layer normalization parame-\nters for normalizing each global token’s embedding.\nThe rest of the parameters are identical to T5, and\nwe accommodate sequence packing by addition-\nexample in the same input sequence to increase training efﬁ-\nciency. This is specially useful in LongT5, since with the large\ninput lengths used in our model, if many examples are short,\nmost of the input sequence would be dedicated to padding,\nwasting signiﬁcant computation.\n4For base models, we introduced 10k additional parame-\nters, 25k for large, and 50k for xl.\nally masking attention from input tokens to global\ntokens of other examples. We found block size\nk = 16 to be sufﬁcient in practice. Notice thus,\nthat TGlobal attention introduces a block of l ∗l/k\nadditional attention key-value pairs to calculate on\ntop of Local Attention ( l input tokens, attending\nto l/k global tokens; represented by the right most\nrectangle in Figure 2.b), hence for input sequence\nlength l, complexity is O(l(r + l/k)).\n3.2 PEGASUS Principle Sentences\nGeneration Pre-training\nT5 is pre-trained with a span corruption objective,\nwhere spans of consecutive input tokens are re-\nplaced with a mask token and the model is trained\nto reconstruct the masked-out tokens. While it is\neffective, recent work on masked language model-\ning (MLM) (Liu et al., 2019; Zhang et al., 2019b)\nshows that carefully selecting the prediction objec-\ntive could lead to signiﬁcantly better performance.\nOne argument is that predicting more informative\ntokens from the text could force the model to learn\nbetter semantics of the text. Motivated by that,\nwe explore masking and generating the principle\nsentences from the text. In particular, we adopt\nthe Gap Sentences Generation with Principle Ind-\nUniq strategy from Zhang et al. (2019a), which\nwas used for summarization pre-training.\nFollowing Zhang et al. (2019a), we select\ntop-m scored ( Principle) sentences based on\nROUGE-F1 score (Lin, 2004) using si =\nrouge(xi, D\\{xi}, ∀i), where i is the sentence\nindex, D is the collection of sentences in the docu-\nment. Each sentence is scored independently (Ind),\nand each n-gram is only counted once (Uniq).\n726\n4 Experiments\n4.1 Conﬁgurations\nLongT5 is implemented using JAX5 and the Flax-\nformer6 library. Following the same setup as\nT5.1.17, we consider models of 3 sizes: base\n(∼220M), large (∼770M), and xl (∼3B), and use\nthe same cased English SentencePiece vocab model\nused by T5.1.1, which contains 32000 sentence\npieces. We use batch size of 128 and Adafactor\nas the optimizer in all experiments. We decide to\nuse greedy decoding instead of beam search for\nall our experiments even with the test sets, there-\nfore, our results reported below could potentially\nbe improved further by using beam search, but we\nwould like to make the setup consistent with our\ndev setup.\n4.1.1 Pre-training\nWe pre-train LongT5 models for 1M steps on\n4096 input sequence length and 910 output se-\nquence length. We use the same inverse square-\nroot learning rate schedule as T5, with learn-\ning rate set to 1/\n√\nmax(step, warm_up steps),\nwhere warm_up steps is set to 10000. The same as\nT5.1.1, we pre-train LongT5 only on the C4 dataset\n(Raffel et al., 2019b), and we do not apply dropout\nduring pre-training. As described in section 3.2,\nwe use the PEGASUS Principle Sentences Gener-\nation objective as our pre-training objective. The\nconﬁguration is similar to what was described by\nZhang et al. (2019a) for their larger models, ex-\ncept for the masked sentence ratio in which we\nuse a value of 0.2 instead of 0.458. In section 5.3,\nwe will show our ablation study between Principle\nSentences Generation and Span Corruption.\n4.1.2 Fine-tuning\nFor ﬁne-tuning, we use a constant learning rate of\n0.001 and dropout rate of 0.1 for all tasks. For\nsummarization tasks, we experiment with values of\n4096, 8192, and 16384 for input lengths and 512\nfor output lengths. For QA tasks, we experiment\nwith values starting at 512 and scale up to 36864\nfor input lengths and 128 for output lengths.\n5https://github.com/google/jax\n6https://github.com/google/ﬂaxformer\n7https://github.com/google-research/text-to-text-transfer-\ntransformer/blob/main/released_checkpoints.md#t511\n8We brieﬂy experimented with other values, but found 0.2\nto work best with the downstream tasks of interest.\n4.2 Evaluation on Summarization Tasks\nWe choose to benchmark our models on summa-\nrization tasks that cover various context lengths,\nbecause of their long context understanding and\ngenerative nature.\n4.2.1 Datasets\nLongT5 was benchmarked on the following six\ndatasets.\nCNN / Daily Mail (Nallapati et al., 2016) News\nfrom CNN and Daily Mail are used as input and the\narticle’s summary bullets are the target summary.\nPubMed (Cohan et al., 2018) Scientiﬁc docu-\nments were collected from PubMed, with a docu-\nment’s content used as input and its corresponding\nabstract as the target summary.\narXiv (Cohan et al., 2018) Similar to PubMed,\nbut with documents taken from arXiv.\nBigPatent (Sharma et al., 2019) U.S. patent doc-\numents, with the patent’s details used as input and\nthe patent’s abstract as the target summary.\nMediaSum (Zhu et al., 2021) Interview tran-\nscripts from CNN and NPR were used as input\nand their corresponding topic and overviews used\nas the target summary.\nMulti-News (Fabbri et al., 2019) The task in-\nvolves summarizing multiple news documents\nabout a topic into a human-written summary.\nTable 1 provides statistics for the number of ex-\namples in train, validation, and test splits, and the\naverage, median, max, and 90th percentile input\nsequence length. As can be seen, these datasets are\nlong in input length, and would beneﬁt from mod-\nels that can model lengthier inputs. We included\nthe CNN / Daily Mail dataset to benchmark on a\ncommon task, especially to see how using TGlobal\nattention impacts the model, despite the length of\nthe inputs being smaller than the other datasets.\n4.2.2 Results\nWe compare LongT5 with various top approaches:\nBigBird-PEGASUS (Zaheer et al., 2020b), HAT-\nBART (Rohde et al., 2021), DANCER PEGASUS\n(Gidiotis and Tsoumakas, 2020), PRIMER (Xiao\net al., 2021), TG-MultiSum (Cui and Hu, 2021),\nLED (Beltagy et al., 2020), and an application of\nBART by Zhu et al. (2021). For these comparisons,\nwe use common evaluation metrics of ROUGE-1,\nROUGE-2, and ROUGE-L.\n727\nDataset Example Count Input Length\nTrain Validation Test Average Median Max 90th percentile\nCNN / Daily Mail 287,113 13,368 11,490 982.39 894 5268 1659\narXiv 203,037 6,436 6,440 10,720.18 8,519 378,825 20,170\nPubMed 119,924 6,633 6,658 4,747.97 3,883 452,915 8,883\nBigPatent 1,207,222 67,068 67,072 6,537.32 5,236 294,004 11,328\nMediaSum 443,596 10,000 10,000 2,302.02 1,748 125,974 4,128\nMulti-News 44,972 5,622 5,622 2,593.81 1,902.5 683,544 4,853\nTable 1: Statistics for the summarization datasets. Input length measured in tokens using a SentencePiece Model.\narXiv\nApproach R-1 R-2 R-L\nDANCER PEGASUS 45.01 17.6 40.56\nBigBird-PEGASUS (large) 46.63 19.02 41.77\nHAT-BART 46.68 19.07 42.17\nLED (large) 46.63 19.62 41.83\nPRIMER 47.6 20.8 42.6\nLongT5 (large - 16k input) 48.28 21.63 44.11\nLongT5 (xl - 16k input) 48.35 21.92 44.27\nPubMed\nApproach R-1 R-2 R-L\nDANCER PEGASUS 46.34 19.97 42.42\nBigBird-PEGASUS (large) 46.32 20.65 42.33\nHAT-BART 48.36 21.43 37.00\nLongT5 (large - 16k input) 49.98 24.69 46.46\nLongT5 (xl - 16k input) 50.23 24.76 46.67\nBigPatent\nApproach R-1 R-2 R-L\nBigBird-PEGASUS (large) 60.64 42.46 50.01\nLongT5 (large - 16k input) 70.38 56.81 62.73\nLongT5 (xl - 16k input) 76.87 66.06 70.76\nMultiNews\nApproach R-1 R-2 R-L\nTG-MultiSum 47.10 17.55 20.73\nPRIMER 49.9 21.1 25.9\nLongT5 (large - 8k input) 47.18 18.44 24.18\nLongT5 (xl - 8k input) 48.17 19.43 24.94\nMediaSum\nApproach R-1 R-2 R-L\nBART (large) 35.09 18.05 31.44\nLongT5 (large - 4k input) 35.54 19.04 32.20\nLongT5 (xl - 4k input) 36.15 19.66 32.80\nCNN / Daily Mail\nApproach R-1 R-2 R-L\nHAT-BART 44.48 21.31 41.52\nLongT5 (large - 4k input) 42.49 20.51 40.18\nLongT5 (xl - 4k input) 43.94 21.40 41.28\nTable 2: Summarization results comparing LongT5\nwith best known approaches. LongT5 scores are with\nmodels using TGlobal attention. For each task, we\nscale up the input length depending on the inputs’ statis-\ntics, thus not all are scaled to 16k. For more results,\nplease see Section A in the Appendix.\nAs can be seen in Table 2, LongT5 is able\nto achieve state-of-the-art rouge scores for arXiv,\nPubMed, BigPatent, and MediaSum. For arXiv\nand PubMed, which are composed of longer inputs,\nbeing able to scale up to 16k input length helps\nLongT5 achieve strong results.\nOne dataset where LongT5 is not able to achieve\nstate-of-the-art results is with Multi-News. LongT5\nis the 2nd best model, slightly worth than PRIMER.\nThis is understandable as the PRIMER model was\npre-trained on a large corpus of documents related\nto news events, thus exposing the model to a similar\ncorpus as that seen in Multi-News.\nWhen looking at CNN / Daily Mail, we can\nsee that LongT5 was comparable with HAT-BART,\ndespite not having full attention. LongT5 did at\nleast get stronger scores in the ROUGE-2 metric.\n4.3 Evaluation on QA Tasks\nFor the evaluation on QA tasks, we choose two pop-\nular benchmarks, Natural Questions and TriviaQA,\nthat require long context understanding.\n4.3.1 Datasets\nNaturalQuestions (NQ) Questions are real\nqueries issued by multiple users to Google search\nthat retrieve a Wikipedia page in the top ﬁve search\nresults. Answer text is drawn from the search re-\nsults (Kwiatkowski et al., 2019).\nThe original NQ dataset asks models to predict a\nshort answer (including no-answer or yes/no) and\na long answer. We framed the task as a seq2seq\ntask and ignored the long answer. Hence, our re-\nsults focus only on short answer. Moreover, since\nour models predict answer texts instead of answer\nspans, our evaluation method differs slightly from\nthe leader boards, and our results are not directly\ncomparable to other existing approaches: (1) Since\nonly the train and dev sets are publicly available,\nwe use 90% of the ofﬁcial train set for training\nwhile using 10% as hold-out dev set to ﬁne-tune\nthe hyperparameters and training epoch, and use\n728\nDataset Example Count Input Length\nTrain Validation Test Average Median Max 90th percentile\nNQ 307,373 7,830 6,695.92 4,486 151,519 15,290.8\nTriviaQA 87,622 11,313 10,832 69,082.51 45,011 1,174,918 150,643\nTable 3: Statistics for the QA datasets. Input length measured in tokens using a SentencePiece Model.\nNQ\nApproach EM F1\nT5.1.1 (base - 512 input) 50.93 52.54\nT5.1.1 (base - 6k input) 56.73 56.73\nT5.1.1 (large - 512 input) 57.29 60.68\nT5.1.1 (large - 3k input) 60.09 64.17\nT5.1.1 (xl - 4k input) 60.75 64.07\nLocal:\nLongT5 (base - 512 input) 54.39 58.24\nLongT5 (base - 36k input) 55.77 59.66\nLongT5 (large - 512 input) 55.19 58.00\nLongT5 (large - 10k input) 60.01 64.40\nTGlobal:\nLongT5 (base - 512 input) 55.73 59.06\nLongT5 (base - 12k input) 58.12 62.44\nLongT5 (large - 512 input) 57.55 61.53\nLongT5 (large - 4k input) 60.77 65.38\nLongT5 (large - 6k input) 59.17 63.38\nLongT5 (xl - 8k input) 62.66 66.61\nTriviaQA\nApproach EM F1\nBigBird-ETC (random attn) 80.86 84.5\nFusion-in-Decoder 80.09 84.35\nReadTwice 76.86 80.85\nTGlobal:\nLongT5 (base - 16k input) 74.67 78.9\nLongT5 (large - 16k input) 78.38 82.45\nLongT5 (xl - 16k input) 81.00 84.83\nTable 4: QA results: (1) NQ results comparing T5.1.1\nand LongT5. Base/large models are trained on 4x8\nTPUv3 with no model partitioning. Xl models are\ntrained on 8x16 TPUv3 with 8 partitions. (2) Trivi-\naQA results compared to top models on leader board.\nLongT5 scores using Local and TGlobal attention. Full\nresults in Appendix B.\nthe ofﬁcial dev set as our test set. (2) We benchmark\nLongT5 against the corresponding T5.1.1 models\ninstead of directly comparing to the leader boards.\nTriviaQA Trivia enthusiasts authored question-\nanswer pairs. Answers are drawn from Wikipedia\nand Bing web search results, excluding trivia web-\nsites (Joshi et al., 2017).\nWe use the ofﬁcial train/validation splits for\ntraining and ﬁne-tuning the hyperparameters and\ntraining epoch, then re-train that model combining\nboth train and validation sets to evaluate on the\nWikipedia domain on the leader board 9.\nTable 3 shows the dataset statistics for the num-\nber of examples in train and validation splits, and\nthe average, median, max, and 90th percentile input\nsequence length.\n4.3.2 Results\nTable 4 shows a summary of the results for the NQ\nand TriviaQA datasets (see Appendix B for full\nresults). For each dataset, we show two metrics:\nEM (Exact Match) and F1 score (evaluating preci-\nsion and recall of individual words in the answer\ncompared to the ground truth, ignoring stop words).\nFor NQ, we compare T5.1.1, LongT5 with Local\nAttention, and LongT5 with TGlobal attention. We\ndecided to run T5.1.1 (1) with the default 512 input\nsequence length10 and (2) with the largest input\nsequence length that can ﬁt into device memory11,\nand use those as baselines. Since we are comparing\nagainst T5.1.1, for LongT5 experiments we report\nresults at 512 input length for base and large, and\nthe largest input length allowed by each model be-\nfore running out of memory on the same hardware\nconﬁguration used in our T5.1.1 experiments.\nAs the table shows, increasing input length gen-\nerally results in signiﬁcant beneﬁts in NQ, with\nmodels with larger input lengths signiﬁcantly out-\nperforming those with smaller input lengths in most\ncases. Some times, models with the largest input\n9https://competitions.codalab.org/competitions/17208\n10For base and large models.\n11For base and large models, we used 4x8 TPUv3 and no\nmodel partitioning; for xl model, we used 8x16 TPUv3 and 8\npartitions.\n729\nInput Length\nSequences per second\n60\n80\n100\n200\n400\n600\n800\n1000\n400\n600\n800\n1000\n2000\n4000\n6000\n8000\n10000\n20000\n40000\nT5.1.1 base LongT5 base Local LongT5 base TGlobal\nT5.1.1 large LongT5 large Local LongT5 large TGlobal\nFigure 3: Sequences per second as a function of input\nlength for T5.1.1, LongT5 with Local Attention and\nLongT5 with TGlobal attention. Input lengths start at\n512, and go as far as possible before running out of\nmemory. Measurements taken with batch size 128, on\n4x8 TPUv3 slices. base and large model sizes shown.\nlengths underperform those with 4k length, but we\nbelieve those to be due to noise in the experiments,\nas results are the output of just one repetition of\neach experiment due to resource constraints. More-\nover, while LongT5 with Local Attention often\nunderperforms T5.1.1, LongT5 with TGlobal at-\ntention signiﬁcantly outperforms T5.1.1. For ex-\nample, considering the large size models, T5.1.1\nwas able only to scale up to an input length of 3k\ntokens, while the TGlobal model was able to reach\n6k tokens, outperforming T5.1.1 at 4k token length\n(there was a dip at 6k token length, but we hypothe-\nsize this is just due to variance, as we only did one\nrun for each conﬁguration).\nFor TriviaQA, we compare LongT5 with various\ntop approaches on the leader board: BigBird-ETC\n(Zaheer et al., 2020a), Fusion-in-Decoder (Izacard\nand Grave, 2021), and ReadTwice (Zemlyanskiy\net al., 2021). As shown in Table 3, TriviaQA inputs\nare quite long, therefore being able to scale up both\nin model size and to 16k input length helps LongT5\nachieve state-of-the-art.\n5 Analysis\n5.1 Input Length vs Speed\nIn order to evaluate the training speed and mem-\nory consumption of LongT5, compared to T5.1.1,\nwe performed a series of training runs in the NQ\ndata set starting at input length 512, and increasing\nSequences per second\nF1 (short answer)\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n0 100 200 300 400 500 600\nT5.1.1 large LongT5 large Local LongT5 large TGlobal \n512\n1k\n2k\n4k\n3k10k\n6k\n2k\n8k\n1k\n1k\n4k\n512\n512\nFigure 4: Speed versus Performance on NQ (short-\nanswer F1), for T5, LongT5 with Local Attention and\nLongT5 with TGlobal attention, for different input se-\nquence lengths. Input lengths start at 512, and go as far\nas possible before running out of memory. Measure-\nments taken with batch size 128, on 4x8 TPUv3 slices.\nthe input length steadily until models ran out of\nmemory on a 4x8 TPUv3 slice. Results are shown\nin Figure 3, which compares 6 different model\nconﬁgurations: T5.1.1 base, T5.1.1 large, LongT5\n(base Local), LongT5 (large Local), LongT5 (base\nTGlobal), and LongT5 (large TGlobal). For each\nmodel conﬁguration, we show a curve plotting the\nnumber of sequences per second processed during\ntraining (speed, in the vertical axis) for each input\nlength (horizontal axis). Both axes are shown in\nlogarithmic scale.\nWe can see that at shorter lengths (512), T5.1.1,\nLongT5 Local, LongT5 TGlobal have similar\nspeeds, but as we increase the sequence length,\nLongT5 becomes signiﬁcantly faster. For exam-\nple at sequence length 2048, T5.1.1 base can only\nprocess 479 sequences per second, while LongT5\n(base TGlobal) can process 765 and LongT5 (base\nLocal) can process 860. The differences grow even\nlarger as sequence length increases.\nAnother important fact that Figure 3 shows is\nthat T5.1.1 models reach their out of memory point\nmuch earlier. For example, we could only scale\nup to 6k tokens for T5.1.1 base. On the other\nhand, LongT5 (base Local) can go up to 36k tokens\nin length, and LongT5 (base TGlobal) up to 12k.\nLarge models show a similar picture with T5.1.1\nlarge going only up to 3k, but the LongT5 variants\ngoing to 10k (large Local) and 6k (large TGlobal).\n5.2 Input Length vs Performance\nThis section presents a similar analysis, but where\nwe plotted model speed versus performance in NQ\n730\n(F1 score). Results are shown in Figure 4 for mod-\nels with large size. Each point in the curves is\nannotated with the corresponding sequence length.\nAs Figure 4 shows, performance increases sig-\nniﬁcantly as input length increases, highlighting\nthe beneﬁts of LongT5. Moreover, input length by\nitself is not enough to achieve good performance\nin all datasets, and in particular, in the NQ dataset\n(used in this ﬁgure), using Local Attention signif-\nicantly hurts performance when compared with\nTGlobal or with T5.1.1. So, even at very long\ninput lengths, LongT5 with Local Attention just\nmatches T5.1.1 with input length of 3k in NQ. How-\never, LongT5 with TGlobal attention outperforms\nT5.1.1. Moreover, note that although the plot shows\na few irregularities (such as 8k length for LongT5\nwith Local Attention, or 6k length with TGlobal\nAttention), that is because the plot shows only the\nresults of a single run, and hence there is some\nnoise. However, trends can clearly be seen.\n5.3 Principle Sentences Generation vs. Span\nCorruption\nAs mentioned in section 3.2, we use PEGASUS\nPrinciple Sentences Generation instead of default\nSpan Corruption used in T5 as our pre-training\nobjective. Table 5 shows our ablation study for\nﬁne-tuning on NQ and arXiv from a model pre-\ntrained using the default Span Corruption objec-\ntive, a model pre-trained with Principle Sentences\nGeneration, and a model pre-trained with both ob-\njectives. The comparison is done on the dev set of\nthe tasks, and with TGlobal base models. Both pre-\ntraining and ﬁne-tuning on the models mentioned\nabove are done with input sequence length 4096.\nThe table shows, even though Principle Sentences\nGeneration was developed by Zhang et al. (2019a)\nas a pre-training strategy for summarization, it ben-\neﬁts both summarization and QA tasks, but using\nboth objectives together perform worse than just\nusing PSG.\nTable 6 shows an additional ablation study with\narXiv and PubMed, where we compare using reg-\nular T5.1.1 with Span Corruption compared to\nT5.1.1 pretrained with Principle Sentences Gen-\neration while using the same pre-training input se-\nquence length of 512 (as was done in the original\nT5.1.1 pre-training task). As expected, Principle\nSentences Generation helped the model achieve\nbetter results compared to Span Corruption when\nseeing the same amount of pre-training data. We\nNQ arXiv\nObjective EM F1 R-1 R-2 R-3\nPSG 62.21 66.94 44.95 18.74 40.99\nSC 58.65 63.05 43.49 18.12 39.71\nSC + PSG 59.74 64.54 44.85 18.79 40.90\nTable 5: Ablation study on dev set for different pre-\ntraining strategies using span corruption (SC) vs. prin-\nciple sentences generation (PSG) and the effects on\nNQ and arXiv ﬁne-tuning tasks. The models are\nTGlobal base, and ﬁne-tuning is done with input se-\nquence length 4096.\narXiv\nObjective R-1 R-2 R-3\nSC 44.59 18.34 40.65\nPSG 45.78 18.94 41.53\nLongT5 (4k) 45.66 19.22 41.49\nLongT5 (16k) 48.21 21.7 44.03\nPubMed\nObjective R-1 R-2 R-3\nSC 47.86 22.14 44.39\nPSG 48.74 23.42 45.24\nLongT5 (4k) 48.47 23.38 45.01\nLongT5 (16k) 50.12 24.78 46.56\nTable 6: Ablation study on arXiv and PubMed for\ndifferent pre-training strategies using span corruption\n(SC) vs. principle sentences generation (PSG) with\nT5.1.1 model along with LongT5 with TGlobal atten-\ntion. Fine-tuning was done on large model size, with\ninput sequence length of 4096 except where otherwise\nnoted.\nalso compare this with dev scores from LongT5\nwith TGlobal attention at 4k and 16k input lengths,\nsuch that we can see having full attention will allow\nfor better results, but being able to scale to longer\ninput sequence lengths allows LongT5 to achieve\nits stronger results.\n6 Related Work\nLanguage model pre-training followed by task\nspeciﬁc ﬁne-tuning has proven to be a powerful\ntool for numerous NLP tasks (Devlin et al., 2019;\nLiu et al., 2019; Zhang et al., 2019b; Radford et al.,\n2019; Raffel et al., 2019a; Lewis et al., 2020; Joshi\net al., 2020). BERT (Devlin et al., 2019) intro-\nduced Mask Language Model (MLM), where a\nmodel predicts masked tokens given a sequence of\ntext input. Fine-tuning a pre-trained BERT model\nhas led to improved performance on various NLP\ntasks. However, MLM predictions are not made\nauto-regressively, which limits the capability of the\n731\nBERT family for generation tasks. Raffel et al.\n(2019a) introduced the span corruption task in T5\nas the pre-training objective, where a model pre-\ndicts the masked token span using an autoregressive\nmodel. It can handle the generation tasks as the pre-\ntraining is done in a generative way. BART (Lewis\net al., 2020) is similar to T5 but used a slightly\ndifferent pre-training objective, in which spans are\nmasked from the input but the complete output is\npredicted. However, none of these works tried to\ninvestigate pre-training for very long sequence in-\nputs. They often use a transformer (Vaswani et al.,\n2017) architecture as backbone, the complexity of\nwhich is quadratic to the input length, making them\nimpractical to model very long sequence input.\nLong text modeling An extensive amount of\nwork has also been done for modeling long text like\ndocuments. The work from Roy et al. (2016); Chen\n(2017); Wu et al. (2018) obtained document embed-\ndings from word-level embeddings. Another line\nof research tries to model long documents through\nhierarchical training. The work from Yang et al.\n(2016); Miculicich et al. (2018) employed Hier-\narchical Attention Networks for document classi-\nﬁcation and neural machine translation, and Guo\net al. (2019) proposed using a hierarchy network\nto build document embeddings on top of sentence\nembeddings for parallel document mining.\nMore recent research has been focusing on im-\nproving the memory and computation efﬁciency\nof transformer models (Tay et al., 2020b, 2021)\nfor handling long input. One type of such ap-\nproaches is using non-full attention patterns to re-\nstrict the attention ﬁeld range, so that it reduces the\nattention complexity from O(n2) to O(nlogn) or\nO(n), including Sinkhorn (Tay et al., 2020a), Long-\nformer (Beltagy et al., 2020), ETC (Ainslie et al.,\n2020), and BigBird (Zaheer et al., 2020a). An-\nother type of approaches is leveraging the low-rank\napproximation of the attention matrix, such as Lin-\nformer (Wang et al., 2020), Performer (Choroman-\nski et al., 2021), Random Feature Attention (Peng\net al., 2021), and LUNA (Ma et al., 2021).\n7 Conclusion\nThis paper presented a new Transformer-based neu-\nral model called LongT5, with which we have ex-\nplored the effects of scaling both input length and\nmodel size at the same time. Speciﬁcally, the main\ndifferences of LongT5 with respect to T5.1.1 are\n(1) a new scalable attention mechanism calledTran-\nsient Globalattention, which is a drop-in replace-\nment to the standard T5 attention mechanism, and\nhence can be used without needing additional side-\ninputs to the model or modiﬁcations to the model\ninputs; and (2) using a PEGASUS-style Principle\nSentences Generation pre-training objective.\nVia experimentation in several challenging sum-\nmarization and question answering datasets, we\nhave explored the performance gains that can be\nachieved by scaling both input length and model\nsize, resulting in state-of-the-art results on several\ndatasets: arXiv, PubMed, BigPatent, MediaSum,\nand TriviaQA.\nAs part of our future work, we would like to pur-\nsue several directions such as studying efﬁcient at-\ntention mechanisms in the decoder and decoder-to-\nencoder attention pieces of the model (both Local\nAttention and TGlobal attention are only applied\nto the encoder in LongT5 for now). Additionally,\nwe would like to incorporate additional long-input\ntransformer ideas into the LongT5 architecture, that\ncould further improve model efﬁciency.\nReferences\nJoshua Ainslie, Santiago Ontañón, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. Etc: Encoding long and structured data in\ntransformers. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP 2020).\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nMinmin Chen. 2017. Efﬁcient vector representation\nfor documents through corruption. 5th International\nConference on Learning Representations.\nKrzysztof Marcin Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Quincy Davis, Afroz\nMohiuddin, Lukasz Kaiser, David Benjamin Be-\nlanger, Lucy J Colwell, and Adrian Weller. 2021.\nRethinking attention with performers. In Interna-\ntional Conference on Learning Representations.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Na-\nzli Goharian. 2018. A discourse-aware attention\nmodel for abstractive summarization of long docu-\nments. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 615–621,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\n732\nPeng Cui and Le Hu. 2021. Topic-guided abstractive\nmulti-document summarization.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAlexander Fabbri, Irene Li, Tianwei She, Suyi Li, and\nDragomir Radev. 2019. Multi-News: A large-scale\nmulti-document summarization dataset and abstrac-\ntive hierarchical model. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 1074–1084, Florence, Italy.\nAssociation for Computational Linguistics.\nAlexios Gidiotis and Grigorios Tsoumakas. 2020. A\ndivide-and-conquer approach to the summarization\nof long documents. IEEE/ACM Transactions on Au-\ndio, Speech, and Language Processing, 28:3029–\n3040.\nMandy Guo, Yinfei Yang, Keith Stevens, Daniel Cer,\nHeming Ge, Yun-hsuan Sung, Brian Strope, and Ray\nKurzweil. 2019. Hierarchical document encoder for\nparallel corpus mining. In Proceedings of the Fourth\nConference on Machine Translation (Volume 1: Re-\nsearch Papers), pages 64–72, Florence, Italy. Asso-\nciation for Computational Linguistics.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64–77.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Transactions of the Association of Compu-\ntational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nXuezhe Ma, Xiang Kong, Sinong Wang, Chunting\nZhou, Jonathan May, Hao Ma, and Luke Zettle-\nmoyer. 2021. Luna: Linear uniﬁed nested attention.\nIn Thirty-Fifth Conference on Neural Information\nProcessing Systems.\nLesly Miculicich, Dhananjay Ram, Nikolaos Pappas,\nand James Henderson. 2018. Document-level neu-\nral machine translation with hierarchical attention\nnetworks. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 2947–2954, Brussels, Belgium. Associa-\ntion for Computational Linguistics.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar G ˙ulçehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of The 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah Smith, and Lingpeng Kong. 2021.\nRandom feature attention. In International Confer-\nence on Learning Representations.\nOﬁr Press, Noah A. Smith, and Mike Lewis. 2021.\nTrain short, test long: Attention with linear biases\nenables input length extrapolation.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019a. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. CoRR, abs/1910.10683.\n733\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019b. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. CoRR, abs/1910.10683.\nTobias Rohde, Xiaoxia Wu, and Yinhan Liu. 2021. Hi-\nerarchical learning for generation with long source\nsequences.\nDwaipayan Roy, Debasis Ganguly, Mandar Mitra, and\nGareth J. F. Jones. 2016. Representing documents\nand queries as sets of word embedded vectors for\ninformation retrieval. CoRR, abs/1606.07869.\nEva Sharma, Chen Li, and Lu Wang. 2019. BIG-\nPATENT: A large-scale dataset for abstractive and\ncoherent summarization. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2204–2213, Florence, Italy.\nAssociation for Computational Linguistics.\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-\nCheng Juan. 2020a. Sparse sinkhorn attention. In\nICML.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang\nShen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu\nYang, Sebastian Ruder, and Donald Metzler. 2021.\nLong range arena : A benchmark for efﬁcient trans-\nformers. In International Conference on Learning\nRepresentations.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020b. Efﬁcient transformers: A survey.\nArXiv, abs/2009.06732.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-attention\nwith linear complexity.\nLingfei Wu, Ian En-Hsu Yen, Kun Xu, Fangli\nXu, Avinash Balakrishnan, Pin-Yu Chen, Pradeep\nRavikumar, and Michael J. Witbrock. 2018. Word\nmover’s embedding: From word2vec to document\nembedding. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 4524–4534. Association for Com-\nputational Linguistics.\nWen Xiao, Iz Beltagy, Giuseppe Carenini, and Arman\nCohan. 2021. PRIMER: Pyramid-based masked sen-\ntence pre-training for multi-document summariza-\ntion.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy. 2016. Hierarchical\nattention networks for document classiﬁcation. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1480–1489, San Diego, California. Associa-\ntion for Computational Linguistics.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontañón,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\nand Amr Ahmed. 2020a. Big bird: Transformers for\nlonger sequences. CoRR, abs/2007.14062.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Ahmed. 2020b. Big Bird: Trans-\nformers for longer sequences. In Advances in\nNeural Information Processing Systems, volume 33,\npages 17283–17297. Curran Associates, Inc.\nYury Zemlyanskiy, Joshua Ainslie, Michiel de Jong,\nPhilip Pham, Ilya Eckstein, and Fei Sha. 2021.\nReadtwice: Reading very large documents with\nmemories.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2019a. PEGASUS: pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nCoRR, abs/1912.08777.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019b. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1441–1451, Florence, Italy. Association\nfor Computational Linguistics.\nChenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng.\n2021. MediaSum: A large-scale media interview\ndataset for dialogue summarization. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5927–5934,\nOnline. Association for Computational Linguistics.\n734\nA Summarization Results\nTable 8 shows the full set of results on the summa-\nrization datasets used in this paper. This includes\nboth standard T5 model (using version T5.1.1), T5\nwith PEGASUS Principle Sentences Generation\npre-training, and LongT5 model.\nAs can be seen, scaling up the input size for the\nmodels helps achieve better performance metrics.\nT5 models though struggle when scaling up to 4k\nfor input, as the ﬁne-tuning task can take many\ndays even when using a large topology of TPUv3.\nWhen comparing regular T5.1.1 model with\na T5.1.1 model using PEGASUS Principle Sen-\ntences Generation pre-training, the latter was able\nto achieve better results, with the results also im-\nproving as the input size scaled up. This helps\nshow that both using the latter pre-training objec-\ntive along with scaling up allows us to get the best\nresults from these models.\nLongT5, despite having a reduced attention from\nusing TGlobal attention, is able to get strong per-\nformance results due to both scaling up to larger\ninputs and leveraging the Gap Sentences Genera-\ntion pre-training strategy.\nB QA Results\nTable 7 shows the full set of results comparing\nT5.1.1 and LongT5 models on the QA datasets\nused in this paper. For both NQ and TriviaQA in\nthis comparison study, we use 90% of the ofﬁcial\ntraining set for training while using 10% as hold-\nout dev set to ﬁne-tune the hyperparameters and\ntraining epoch, and use the ofﬁcial dev set to report\nthe numbers in this table. We run each model to\nthe largest input length allowed before running out\nof memory on speciﬁc hardware conﬁguration -\nbase/large models on 4x8 TPUv3 with no model\npartitioning, and xl models on 8x16 TPUv3 with 8\npartitions.\nNQ TriviaQA\nApproach EM F1 EM F1\nbase:\nT5.1.1 (512) 50.93 52.54 48.91 52.89\nT5.1.1 (6k) 56.73 56.73 59.09 63.31\nlarge:\nT5.1.1 (512) 57.29 60.68 53.26 57.01\nT5.1.1 (3k) 60.09 64.17 60.15 64.15\nxl:\nT5.1.1 (4k) 60.75 64.07 65.33 69.43\nbase Local:\nLongT5 (512) 54.39 58.24 - -\nLongT5 (1k) 54.60 57.88 - -\nLongT5 (2k) 56.48 60.56 - -\nLongT5 (4k) 56.10 60.52 - -\nLongT5 (8k) 55.90 59.98 - -\nLongT5 (16k) 56.41 60.46 - -\nLongT5 (32k) 55.84 59.59 - -\nLongT5 (36k) 55.77 59.66 - -\nbase TGlobal:\nLongT5 (512) 55.73 59.06 - -\nLongT5 (1k) 57.41 61.25 - -\nLongT5 (2k) 56.96 60.25 - -\nLongT5 (4k) 58.97 63.03 - -\nLongT5 (8k) 58.07 62.67 - -\nLongT5 (12k) 58.12 62.44 63.27 67.42\nlarge Local:\nLongT5 (512) 55.19 58.00 - -\nLongT5 (1k) 57.47 60.79 - -\nLongT5 (2k) 58.49 62.12 - -\nLongT5 (4k) 59.44 63.72 - -\nLongT5 (8k) 58.66 62.28 - -\nLongT5 (10k) 60.01 64.40 - -\nlarge TGlobal:\nLongT5 (512) 57.55 61.53 - -\nLongT5 (1k) 59.69 63.91 - -\nLongT5 (4k) 60.77 65.38 - -\nLongT5 (6k) 59.17 63.38 63.76 67.82\nxl TGlobal:\nLongT5 (4k) 62.38 66.39 - -\nLongT5 (8k) 62.66 66.61 67.89 71.71\nTable 7: QA results comparing T5.1.1 and LongT5 at\ndifferent sequence lengths. Base and large models are\ntrained on 4x8 TPUv3 with no model partitioning, and\nxl models are trained on 8x16 TPUv3 with 8 partitions.\n735\narXiv PubMed\nApproach R-1 R-2 R-L R-1 R-2 R-L\nDANCER PEGASUS 45.01 17.6 40.56 46.34 19.97 42.42\nBigBird-PEGASUS (large) 46.63 19.02 41.77 46.32 20.65 42.33\nHAT-BART 46.68 19.07 42.17 48.36 21.43 37.00\nLED (large) 46.63 19.62 41.83 - - -\nPRIMER 47.6 20.8 42.6 - - -\nT5.1.1 (large - 1k input) 39.79 14.02 36.23 42.18 16.60 38.96\nT5.1.1 (large - 2k input) 42.84 16.62 39.01 45.51 19.55 42.10\nT5.1.1 (large - 4k input) 44.51 18.20 40.62 47.90 22.08 44.36\nT5.1.1 + PSG (large - 1k input) 38.53 13.61 35.08 43.34 17.55 40.10\nT5.1.1 + PSG (large - 2k input) 42.85 16.50 38.99 46.51 20.37 43.00\nT5.1.1 + PSG (large - 4k input) 45.86 18.40 41.62 48.94 22.92 45.4\nLongT5 (base - 4k input) 44.87 18.54 40.97 47.77 22.58 44.38\nLongT5 (large - 4k input) 45.64 18.6 41.51 48.38 23.32 44.93\nLongT5 (large - 8k input) 46.61 19.67 42.44 49.81 24.3 46.26\nLongT5 (large - 16k input) 48.28 21.63 44.11 49.98 24.69 46.46\nLongT5 (xl - 4k input) 45.99 19.51 42.04 48.99 23.48 45.51\nLongT5 (xl - 8k input) 47.44 20.84 43.34 50.04 24.45 46.42\nLongT5 (xl - 16k input) 48.35 21.92 44.27 50.23 24.76 46.67\nBigPatent MultiNews\nApproach R-1 R-2 R-L R-1 R-2 R-L\nBigBird-PEGASUS (large) 60.64 42.46 50.01 - - -\nTG-MultiSum - - - 47.10 17.55 20.73\nPRIMER - - - 49.9 21.1 25.9\nT5.1.1 (large - 1k input) 55.07 37.49 45.90 43.69 16.26 23.03\nT5.1.1 (large - 2k input) 60.07 43.49 50.90 44.95 17.26 23.74\nT5.1.1 (large - 4k input) 62.14 45.85 52.95 45.67 17.88 24.15\nT5.1.1 + PSG (large - 1k input) 58.58 41.80 49.74 44.43 15.85 22.41\nT5.1.1 + PSG (large - 2k input) 64.51 49.15 56.01 46.65 17.74 23.74\nT5.1.1 + PSG (large - 4k input) 67.05 52.24 58.70 47.48 18.60 24.31\nLongT5 (base - 4k input) 60.95 44.22 51.52 46.01 17.37 23.5\nLongT5 (large - 4k input) 66.17 51.10 57.70 46.99 18.21 24.08\nLongT5 (large - 8k input) 67.42 52.62 59.04 47.18 18.44 24.18\nLongT5 (large - 16k input) 70.38 56.81 62.73 - - -\nLongT5 (xl - 4k input) 75.82 64.64 69.54 48.15 19.30 24.76\nLongT5 (xl - 8k input) 76.39 65.37 70.16 48.17 19.43 24.94\nLongT5 (xl - 16k input) 76.87 66.06 70.76 - - -\nMediaSum CNN / Daily Mail\nApproach R-1 R-2 R-L R-1 R-2 R-L\nHAT-BART - - - 44.48 21.31 41.52\nBART (large) 35.09 18.05 31.44 - - -\nT5.1.1 (large - 1k input) 30.68 14.88 27.88 42.60 20.41 40.03\nT5.1.1 (large - 2k input) 32.83 16.75 29.79 42.55 20.25 39.99\nT5.1.1 (large - 4k input) 34.37 18.09 31.12 42.27 19.93 39.72\nT5.1.1 + PSG (large - 1k input) 32.02 16.15 28.89 42.62 20.46 40.02\nT5.1.1 + PSG (large - 2k input) 34.04 17.87 30.77 42.69 20.40 40.06\nT5.1.1 + PSG (large - 4k input) 36.11 19.48 32.67 43.41 20.99 40.77\nLongT5 (base - 4k input) 35.09 18.35 31.87 42.15 20.11 39.6\nLongT5 (large - 4k input) 35.54 19.04 32.20 42.49 20.51 40.18\nLongT5 (xl - 4k input) 36.15 19.66 32.80 43.94 21.40 41.28\nTable 8: Summarization results comparing T5, T5 with PEGASUS-style Principle Sentences Generation (PSG)\npre-training, and LongT5 with best known approaches for the various datasets. All T5 scores are with standard\nT5.1.1 model. All LongT5 scores are with models using TGlobal attention. For each task, we scale up the input\nlength depending on the statistics of the inputs, thus not all of the tasks were scaled to 16k. We do not include input\nlength of other models because each model uses the input differently, and hence, direct comparison is not possible.\n736",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9284830093383789
    },
    {
      "name": "Computer science",
      "score": 0.8147361278533936
    },
    {
      "name": "Transformer",
      "score": 0.7684248685836792
    },
    {
      "name": "Scalability",
      "score": 0.7323483228683472
    },
    {
      "name": "Architecture",
      "score": 0.621179461479187
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4995849132537842
    },
    {
      "name": "Question answering",
      "score": 0.4368782341480255
    },
    {
      "name": "Machine learning",
      "score": 0.36599624156951904
    },
    {
      "name": "Natural language processing",
      "score": 0.35352760553359985
    },
    {
      "name": "Database",
      "score": 0.09438613057136536
    },
    {
      "name": "Engineering",
      "score": 0.09324902296066284
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}