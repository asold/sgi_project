{
  "title": "Optimising and comparing source-extraction tools using objective segmentation quality criteria",
  "url": "https://openalex.org/W3106348855",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3023419717",
      "name": "Caroline Haigh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2996334934",
      "name": "Nushkia Chamba",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284141327",
      "name": "Aku Venhola",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2590575495",
      "name": "Reynier Peletier",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3074217573",
      "name": "Lars Doorenbos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2047182514",
      "name": "Matthew Watkins",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3188675235",
      "name": "Michael H. F. Wilkinson",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1624197873",
    "https://openalex.org/W2135625048",
    "https://openalex.org/W3104608022",
    "https://openalex.org/W2066202006",
    "https://openalex.org/W4289217525",
    "https://openalex.org/W1983192361",
    "https://openalex.org/W2955923615",
    "https://openalex.org/W2788595045",
    "https://openalex.org/W1966828720",
    "https://openalex.org/W2966827606",
    "https://openalex.org/W2322601319",
    "https://openalex.org/W2298287474",
    "https://openalex.org/W3106460758",
    "https://openalex.org/W2139370468",
    "https://openalex.org/W2751754953",
    "https://openalex.org/W1969296052",
    "https://openalex.org/W4234842379",
    "https://openalex.org/W2798304462",
    "https://openalex.org/W2916452334",
    "https://openalex.org/W2129217551",
    "https://openalex.org/W2142238829",
    "https://openalex.org/W3099747210",
    "https://openalex.org/W3103472417",
    "https://openalex.org/W2888532099",
    "https://openalex.org/W1965508489",
    "https://openalex.org/W2897914751",
    "https://openalex.org/W2127193339",
    "https://openalex.org/W1998468445",
    "https://openalex.org/W1972544340",
    "https://openalex.org/W1510052597",
    "https://openalex.org/W3106224859",
    "https://openalex.org/W2257344440",
    "https://openalex.org/W3106293547",
    "https://openalex.org/W2146103050",
    "https://openalex.org/W2067741302",
    "https://openalex.org/W4288360392",
    "https://openalex.org/W2953475150",
    "https://openalex.org/W4288562380",
    "https://openalex.org/W3105041955",
    "https://openalex.org/W2885073701",
    "https://openalex.org/W2146842130",
    "https://openalex.org/W2763961411",
    "https://openalex.org/W2892647939",
    "https://openalex.org/W3105618524",
    "https://openalex.org/W3098151555",
    "https://openalex.org/W2903006681",
    "https://openalex.org/W3106426025",
    "https://openalex.org/W1572126587",
    "https://openalex.org/W2087496273",
    "https://openalex.org/W4295973587",
    "https://openalex.org/W2088682428",
    "https://openalex.org/W2541426243",
    "https://openalex.org/W3103774384",
    "https://openalex.org/W3102230680",
    "https://openalex.org/W2293501768",
    "https://openalex.org/W2131172512",
    "https://openalex.org/W2094181926",
    "https://openalex.org/W1652775531",
    "https://openalex.org/W3125820053",
    "https://openalex.org/W3095880909",
    "https://openalex.org/W2786879317",
    "https://openalex.org/W2601359817",
    "https://openalex.org/W3104062568"
  ],
  "abstract": "\\nContext. With the growth of the scale, depth, and resolution of astronomical imaging surveys, there is increased need for highly accurate automated detection and extraction of astronomical sources from images. This also means there is a need for objective quality criteria, and automated methods to optimise parameter settings for these software tools.\\nAims. We present a comparison of several tools developed to perform this task: namely SExtractor, ProFound, NoiseChisel, and MTObjects. In particular, we focus on evaluating performance in situations that present challenges for detection. For example, faint and diffuse galaxies; extended structures, such as streams; and objects close to bright sources. Furthermore, we develop an automated method to optimise the parameters for the above tools.\\nMethods. We present four different objective segmentation quality measures, based on precision, recall, and a new measure for the correctly identified area of sources. Bayesian optimisation is used to find optimal parameter settings for each of the four tools when applied to simulated data, for which a ground truth is known. After training, the tools are tested on similar simulated data in order to provide a performance baseline. We then qualitatively assess tool performance on real astronomical images from two different surveys.\\nResults. We determine that when area is disregarded, all four tools are capable of broadly similar levels of detection completeness, while only NoiseChisel and MTObjects are capable of locating the faint outskirts of objects. MTObjects achieves the highest scores on all tests for all four quality measures, whilst SExtractor obtains the highest speeds. No tool has sufficient speed and accuracy to be well suited to large-scale automated segmentation in its current form.\\n",
  "full_text": "A&A 645, A107 (2021)\nhttps://doi.org/10.1051/0004-6361/201936561\nc⃝ESO 2021\nAstronomy&Astrophysics\nOptimising and comparing source-extraction tools using objective\nsegmentation quality criteria\nCaroline Haigh1, Nushkia Chamba2,3,4 , Aku Venhola5,6 , Reynier Peletier5, Lars Doorenbos1,\nMatthew Watkins1, and Michael H. F. Wilkinson1\n1 Bernoulli Institute for Mathematics, Computer Science and Artiﬁcial Intelligence, University of Groningen, PO Box 407,\n9700 AK Groningen, The Netherlands\ne-mail: c.haigh@rug.nl, m.h.f.wilkinson@rug.nl\n2 Instituto de Astrofísica de Canarias, Calle Vía Láctea, s/n, 38205 San Cristóbal de La Laguna, Santa Cruz de Tenerife, Spain\ne-mail: chamba@iac.es\n3 Departamento de Astrofísica, Universidad de La Laguna, 38205 La Laguna, Tenerife, Spain\n4 Department of Astronomy and Oskar Klein Centre for Cosmoparticle Physics, Stockholm University, AlbaNova University Centre,\n10691 Stockholm, Sweden\n5 Kapteyn Astronomical Institute, University of Groningen, PO Box 800, 9700 A V Groningen, The Netherlands\n6 Space Physics and Astronomy Research Unit, University of Oulu, Pentti Kaiteran katu 1, 90014 Oulu, Finland\nReceived 23 August 2019 / Accepted 11 September 2020\nABSTRACT\nContext. With the growth of the scale, depth, and resolution of astronomical imaging surveys, there is increased need for highly\naccurate automated detection and extraction of astronomical sources from images. This also means there is a need for objective\nquality criteria, and automated methods to optimise parameter settings for these software tools.\nAims. We present a comparison of several tools developed to perform this task: namely SExtractor, ProFound, NoiseChisel, and\nMTObjects. In particular, we focus on evaluating performance in situations that present challenges for detection. For example, faint\nand diﬀuse galaxies; extended structures, such as streams; and objects close to bright sources. Furthermore, we develop an automated\nmethod to optimise the parameters for the above tools.\nMethods. We present four diﬀerent objective segmentation quality measures, based on precision, recall, and a new measure for the\ncorrectly identiﬁed area of sources. Bayesian optimisation is used to ﬁnd optimal parameter settings for each of the four tools when\napplied to simulated data, for which a ground truth is known. After training, the tools are tested on similar simulated data in order\nto provide a performance baseline. We then qualitatively assess tool performance on real astronomical images from two di ﬀerent\nsurveys.\nResults. We determine that when area is disregarded, all four tools are capable of broadly similar levels of detection completeness,\nwhile only NoiseChisel and MTObjects are capable of locating the faint outskirts of objects. MTObjects achieves the highest scores\non all tests for all four quality measures, whilst SExtractor obtains the highest speeds. No tool has suﬃcient speed and accuracy to be\nwell suited to large-scale automated segmentation in its current form.\nKey words. techniques: image processing – surveys – methods: data analysis\n1. Introduction\nSegmentation maps, which are images that match speciﬁc pix-\nels of an image to a particular source or sources, are used\nextensively to preprocess observational data for analysis. They\nare used for masking sources, estimating sky backgrounds, and\ncreating catalogues, amongst other applications. It is therefore\nessential that the tools used to create these maps are accurate\nand reliable. Otherwise, the subsequent scientiﬁc process may\nbe invalidated by errors in the measurements of sources.\nUnfortunately, astronomical images have many properties\nthat cause problems for traditional image-segmentation algo-\nrithms. Images may be highly noisy and have an extremely large\ndynamic range. Objects generally have no clear boundaries, and\ntheir outer regions may extend below the level of background\nnoise (see Fig. 1). As many generic segmentation algorithms are\nedge-based (Pal & Pal 1993; Wilkinson 1998), they are unable\nto accurately process these images.\nIn addition, with growth of the scale of astronomical sur-\nveys, there is increased need for a fast and accurate tool for\nsegmentation. This is illustrated by current projects such as the\nLegacy Survey of Space and Time (LSST), which aims to pro-\nduce around 15 TB of raw data per night (Ivezi ´c et al. 2019).\nWith surveys of this scale, human intervention will no longer be\nfeasible, meaning that the tools should ideally be robust to vari-\nations in images without manual tuning.\nBecause of these unique challenges, a number of tools have\nbeen developed for the sole purpose of accurately detecting\nsources in astronomical images. The most well-known of these\nfor optical data is SExtractor (Bertin & Arnouts 1996). How-\never, in recent years, a number of alternatives have been pro-\nposed, including ProFound (Robotham et al. 2018), NoiseChisel\n(Akhlaghi & Ichikawa 2015), and MTObjects (Teeninga et al.\n2013, 2016).\nIn this paper, we evaluate and compare these segmenta-\ntion tools in order to study their strengths and weaknesses.\nArticle published by EDP Sciences A107, page 1 of 30\nA&A 645, A107 (2021)\nA thorough comparison provides a means for astronomers to\nchoose the algorithm that is best suited for their scientiﬁc\ngoals. In addition, several of these tools are still under active\ndevelopment, and such an analysis can help to direct future\nadvancements.\nFor this comparison, we developed numerical measures for\nsegmentation quality (Sect. 3.3), and propose a method for auto-\nmatic conﬁguration of tool parameters (Sect. 3.2). This approach\nto evaluating segmentation maps is designed to provide an objec-\ntive measure of quality. To test this, we use simulated images\nwith a known ground truth (Sect. 3.1) to provide evaluations that\nare not dependent on visual bias and preconceptions. We supple-\nment our results by demonstrating the performance of our auto-\nmatically conﬁgured parameters on real survey images (Sect. 5).\nThroughout this paper we use the terms ‘segmentation’,\n‘source detection’, and ‘source extraction’ interchangeably to\nrefer to the process of identifying unique sources in astronom-\nical images and marking the pixels of the image in which each\nsource is the dominant contributor.\n2. Source-extraction methods\n2.1. Previous methods\nFor as long as astronomical images have been produced, it has\nbeen necessary for their contents to be catalogued and measured\nin order that they may be used for scientiﬁc applications. As\nmanually locating and outlining objects is a slow and subjec-\ntive process, particularly when considering the faint outskirts\nof objects, many attempts have been made at automating this\nprocess.\nEarly automatic tools directly scanned photographic plates to\nlocate sources and produce measurements. A notable example is\nCOSMOS (Pratt 1977), which used a process of repeated thresh-\nolding to produce ‘coarse measurements’ of images, essentially\nquantising the image over an estimated local background level.\nIt then used ‘ﬁne measurements’ to produce more accurate\nmeasurements of the object proﬁles. Later additions included\nimproved deblending of adjacent sources (Beard et al. 1990).\nWhilst modern tools no longer use digitised photographic\nplates, instead working directly with data captured by CCDs, the\noverall process used in recent tools is fundamentally very simi-\nlar to that used in their predecessors. Almost all tools follow the\nsame four main steps:\n1. Identify and measure the background level.\n2. Threshold the image relative to the background.\n3. Locate (and deblend) sources appearing above the threshold.\n4. Produce a catalogue of sources and their measured\nproperties.\nSExtractor, described in more detail below, uses a very similar\nmethod to COSMOS, namely repeated thresholding. In contrast,\nseveral other tools make use of dendrograms, that is, hierar-\nchical representations of images, in which nodes representing\nlocal maxima are connected at the highest brightness level where\nthresholding would show a single, unbroken object. Users may\nsubsequently ‘prune’ the dendrogram by removing nodes con-\nnecting very small or faint regions, and may automatically or\nmanually mark objects meeting some criteria. Dendrograms\nhave been used to visualise and analyse hierarchical structure\nin both infrared images (Houlahan & Scalo 1992) and radio data\ncubes (Rosolowsky et al. 2008; Goodman et al. 2009).\nOther tools have deviated from a thresholding-based\napproach. Many of these tools and their methods are described\nin Masias et al. (2012).\n2.2. Deblending\nDeblending, the process of separating overlapping or nested\nsources, is closely linked to source extraction; all of the tools we\ndiscuss in this paper make some attempt at deblending. How-\never, for some scientiﬁc purposes, the tools do not produce suﬃ-\nciently accurate separation of sources, leading to problems such\nas poor photometry (Abbott et al. 2018; Huang et al. 2018), and\nsystematic measurements of physical properties such as redshift\n(Boucaud et al. 2020) and cluster mass (Simet & Mandelbaum\n2015). Consequently, several tools also exist to perform deblend-\ning as a separate process. As these tools are predominantly either\ndesigned to use the results of another source extraction tool\n(such as SCARLET Melchior et al. 2018, which uses SExtractor\nfor initial source detection), or are predominantly designed for\nsmaller images with only a few galaxies (such as the machine-\nlearning-based methods proposed in Reiman & Göhre 2019), we\nchose not to include them in the comparisons in this paper. How-\never, the evaluation process we deﬁne in Sect. 3 could equally be\nused to compare deblending-speciﬁc tools.\n2.3. Compared tools\nWe chose to focus our comparison on four tools: SExtrac-\ntor (Bertin & Arnouts 1996), which is in common use, and\nthree recent alternatives: ProFound (Robotham et al. 2018),\nNoiseChisel + Segment (Akhlaghi & Ichikawa 2015), and MTO-\nbjects (Teeninga et al. 2013, 2016). We chose to exclude several\nother source-extraction tools from this comparison for various\nreasons, notably DeepScan (Prole et al. 2018), which is depen-\ndent on the use of another tool (such as SExtractor) to produce\nan initial mask; and AstroDendro (Robitaille et al. 2013), which\nwas prohibitively slow to run on large images.\n2.3.1. SExtractor\nSExtractor (Bertin & Arnouts 1996) is a widely used tool for the\ncreation of segmentation maps. It was developed with the goal of\nproducing catalogues of astronomical sources from large-scale\nsky surveys.\nThe ﬁrst step in the SExtractor pipeline is the estimation\nand subtraction of the background. The image is divided into\ntiles, and a histogram is produced for each. Values more than\nthree standard deviations from the median are removed. Tiles\nare then classiﬁed into crowded and uncrowded ﬁelds based on\nthe change in histogram distribution, and a background value is\nestimated based on the median and mode of each tile.\nThe image is then thresholded at a ﬁxed number of expo-\nnentially spaced levels above a user-deﬁned threshold. This\nconverts the light in the image into trees, with branches rep-\nresenting bright areas within larger, fainter objects. Pixels in\nbranches that contain at least a given proportion of the light\nof their parent objects are marked as individual objects, whilst\nbranches containing a lower amount of light are regarded as\npart of the parent object. Pixels in the outskirts of objects are\nallocated labels based on the probability that a pixel of that\nvalue is present at that point, using proﬁles ﬁtted to the detected\nsources.\nIn practice, SExtractor may be used in multiple passes,\nparticularly when detecting extended sources. For example, a\nhot/cold method may be used, wherein a sensitive pass captures\nthe outskirts of objects, and a less sensitive pass identiﬁes which\nobjects are not false-positive detections (Rix et al. 2004). It may\nA107, page 2 of 30\nC. Haigh et al.: Optimising and comparing source-extraction tools using objective segmentation quality criteria\nalso be used to identify candidate objects, which are then manu-\nally veriﬁed.\nSExtractor version 2.19.5 was used for this comparison,\nusing the default ﬁlter: a convolution with a 3 ×3 pyramidal\nfunction that approximates Gaussian smoothing. We found in\nsubsequent testing, described in Appendix A, that using a 9 ×9\nGaussian PSF with a full width at half maximum of 5 pixels pro-\nduced marginally better results, although this di ﬀerence is not\nsigniﬁcant, and does not a ﬀect the general conclusions of this\npaper.\n2.3.2. ProFound\nProFound (Robotham et al. 2018), like SExtractor, was designed\nas a general-purpose package for detecting and extracting astro-\nnomical sources; however, it is designed to produce a more accu-\nrate segmentation, which may be used for galaxy proﬁling.\nInstead of using multiple thresholds, ProFound uses a sin-\ngle threshold after the background estimation stage in order\nto demarcate pixels containing sources. These pixels are then\nprocessed in descending order of brightness, with a watershed\nprocess being used to allocate less bright pixels (within some\ntolerance) neighbouring the object of the brightest pixel in a\nregion, until all pixels bordering the object are either allocated\nto other objects, are marked as background, or have higher ﬂux\nthan neighbouring pixels within the object.\nFollowing this process, the background is re-estimated, and\nan iterative process of calculating photometric properties of the\nsegments and repeatedly dilating them is performed, to produce\na ﬁnal segmentation map. ProFound version 1.1.0 was used for\nthis comparison.\n2.3.3. NoiseChisel + Segment\nNoiseChisel (Akhlaghi & Ichikawa 2015) was designed with\nthe goal of ﬁnding ‘nebulous objects’, such as irregular or faint\ngalaxies, accurately. NoiseChisel is intended to be hand-tuned\nfor individual images; the tutorial states that conﬁgurations are\n‘not generic’ (GNU Astronomy Utilities 2019).\nNoiseChisel separates the image into areas containing light\nfrom objects, and areas containing only background. To do this,\nit uses a threshold below the estimated background level, and\nperforms a series of binary morphological operations to create\nan initial detection map. Further morphological operations are\nthen performed on the ‘objects’ and ‘background’ separately,\nand area and signal-to-noise thresholds are used to remove false\ndetections. Segment then produces a map of ‘clumps’ by locat-\ning connected regions around local maxima in the image with a\nwatershed-like process. It then discards those that do not meet\na signal-to-noise threshold, and grows the remaining clumps to\ncreate a ﬁnal segmentation map.\nSince the publication of the original paper, the program has\nbeen split into two separate tools within the GNU Astronomy\nUtilities package: NoiseChisel, and Segment. For the purposes\nof this comparison, the tools are treated as a single pipeline, and\nevaluated together, and we examine only this ﬁnal ‘objects’ out-\nput. We used the latest version at the start of our comparison,\nversion 0.7.42a. Several new versions have since been released,\nwhich may contain di ﬀerent parameters and produce di ﬀerent\nresults.\n2.3.4. MTObjects\nMTObjects (Teeninga et al. 2013, 2016) takes a similar approach\nto SExtractor; both operate on the principle that after a\nbackground subtraction step, objects can be detected by a thresh-\nolding process. However, where SExtractor uses a small number\nof ﬁxed thresholds, MTObjects uses tree-based morphological\noperators.\nA max-tree (Salembier et al. 1998) is constructed from the\nsmoothed and background-subtracted image. The max-tree is a\ntree of the image: the leaves represent local maximum pixels,\nnodes represent increasingly large connected areas of the image,\nwith decreasing minimum pixel values, and the root represents\nthe entire image. This tree is then ﬁltered, using tests to deter-\nmine which nodes of the tree – or areas of the image – contain\nan amount of ﬂux, given their area, that is statistically signiﬁcant\nrelative to their background. If a node has no signiﬁcant ‘parent’,\nor its parent has another ‘child’ with greater ﬂux, it is marked\nas an object. Despite representing all connected components at\nall grey levels in the image, building the max-tree is typically\nvery eﬃcient (O(N log N) for ﬂoating-point images Carlinet &\nGéraud 2014).\nThe max-tree structure used in MTObjects is very similar\nto the dendrogram used in several astronomical applications as\ndescribed above (Houlahan & Scalo 1992; Rosolowsky et al.\n2008). There are two main di ﬀerences. Firstly, the dendrogram\nonly contains nodes where areas connect, whereas the max-tree\ncontains a node for every di ﬀerence in brightness value. Sec-\nondly, MTObjects uses a single statistical signiﬁcance test to\ndetect objects, combining multiple attributes of the node, whilst\nthe dendrogram methods frequently ﬁlter small and faint objects\nat ﬁxed thresholds.\nThere have been no oﬃcial software releases of MTObjects.\nWe used a Python and C implementation 1, which we adapted\nfrom the software used in the original paper. We used signiﬁ-\ncance test 4 as recommended by Teeninga et al. (2016).\n3. Methodology\n3.1. Data\nIn this section we describe the data with which we tested the\ntools. Simulated data (Sects. 3.1.1 and 3.1.2) allow us to accu-\nrately quantify performance on a simpliﬁed version of the prob-\nlem, whilst survey images (Sect. 3.1.3) allow us to qualitatively\nexplore behaviour in a range of diﬀerent real situations.\n3.1.1. Simulated data\nTesting source-detection algorithms on real observational data\nhas several limitations. Firstly, the ground truth is not known;\neven if objects have been manually labelled, it is possible that\nobjects have been missed, or incorrectly measured. In particu-\nlar, it is di ﬃcult to establish the true extent of objects at a low\nbrightness level, as their outer regions may not be clearly visu-\nally distinguishable from background.\nSecondly, the abundance of many features of interest –such\nas ultra-diﬀuse galaxies (Van Dokkum et al. 2015)– is not yet\nfully understood. This means that it is diﬃcult to establish a sta-\ntistical measure of how accurately they can be detected. As these\nobjects are also more di ﬃcult for algorithms to detect, a larger\nsample is required to determine the accuracy of the algorithms.\nBy using simulated data, we gain the ability to test the algo-\nrithms on large datasets with a known ground truth. This means\nthat we can make accurate measures of precision and accuracy\nfor faint features, while taking into account the true extent of\n1 https://github.com/CarolineHaigh/mtobjects\nA107, page 3 of 30\nA&A 645, A107 (2021)\nFig. 1. A gri-composite image of IAC Stripe82 ﬁeld f0363_\ng.rec.fits showing a large structure of Galactic cirri. Such complex,\noverlapping structures are challenging for source-detection tools.\nobjects. We can also measure the accuracy of algorithms in\ndiﬀerent controlled conditions, such as with high noise, back-\nground variation, and overlapping sources (see Fig. 2).\nWe created ten frames of data emulating images in the r′-\nband of data in the Fornax Deep Survey (FDS). This is a deep,\nmedium-sized ground-based survey of the nearby Fornax clus-\nter, which is located at a distance of 20 Mpc (Iodice et al. 2016;\nVenhola et al. 2018). Each simulated image contains approxi-\nmately 1500 ‘stars’, 4000 ‘cluster galaxies’, and 50 ‘background\ngalaxies’. Stars were simulated as point sources and galaxies as\nSérsic models (Sersic 1968). The number and structural parame-\nters of the stars and galaxies were drawn from distributions sim-\nilar to those found in the FDS. In the simulated images, stars\nhave magnitudes between 10 and 23 mag, and galaxies have\nmean e ﬀective surface brightnesses between 21 mag arcsec −2\nand 31 mag arcsec−2. Background galaxies have e ﬀective radii\nbetween 0.5 and 3.5 arcsec and Sérsic indices between 2 and 4.\nCluster galaxies have eﬀective radii between 2.5 and 40 arcsec,\nand Sérsic indices between 0.5 and 2. Axis ratios varied from\n0.3 to 1.0. To replicate observation conditions, images were con-\nvolved with ther-band point spread function of the OmegaCAM,\nand Poissonian and Gaussian noise were added (Venhola et al.\n2018). For further details of the process, see Venhola (2019,\nChapter 5).\n3.1.2. Choosing a ground truth\nAstronomical sources have no clear boundary; their light merely\nbecomes insigniﬁcant in relation to noise and background light\nat some point in their outskirts. This means that when we create\na ground truth for simulated images –a ‘correct’ segmentation\nmap– we need to choose a threshold, t, below which we judge\nlight from sources to be undetectable. Assuming a ﬂat back-\nground, this threshold can be expressed as a sum of the back-\nground level, bg, and some multiple, n, of the standard deviation\nFig. 2.Simulated survey images.\nof the noise, σ:\nt = bg+ (n ∗σ). (1)\nSources may also overlap, meaning that each pixel contains\nlight from multiple sources. In segmentation maps, each pixel is\nallocated to a single source; therefore, it is necessary to deter-\nmine the source that has the strongest relationship with a given\npixel. It should be noted that whilst segmentation maps are the\ntraditional method of demarcating sources within an image, they\nare limited by their inability to represent the reality that pix-\nels contain light belonging to multiple sources 2. Consequently,\ntree-based methods, which inherently model nested objects, are\nunable to capture this structure within segmentation maps. As\nsuch, information contained in the models is lost, and not mea-\nsured in the evaluation.\nWe initially considered allocating each pixel to the source\nwhich contributed the most ﬂux to it. However, this meant that\nfainter sources in the vicinity of bright sources were entirely\nerased, as they had a lower raw ﬂux contribution.\nInstead, we chose to allocate labels based on a combination\nof the importance of the pixel to the source and the importance\nof the source to the pixel. For a source with total ﬂux Fs, con-\ntributing a ﬂux fs,p to a pixel with total ﬂuxFp, the pixel contains\nfs,p\nFs\nof the ﬂux of the source. Conversely, the source contributes\nfs,p\nFp\nof the light contained within the pixel.\nThese measures may be combined to give a single measure:\nfs,p\nFs\n× fs,p\nFp\n· (2)\nWhen allocating a pixel to a source, Fp will be constant for all\nsources contributing to the pixel. Therefore, the pixel may be\nallocated to the source with the highest value for\n( fs,p)2\nFs\n(3)\nand fs,p ≥t.\n2 A new data format would be required to clearly represent this nested\ndata. This could prove to be a challenging problem because of the com-\nplexity of allocating multiple labels and proportional brightnesses to\neach pixel.\nA107, page 4 of 30\nC. Haigh et al.: Optimising and comparing source-extraction tools using objective segmentation quality criteria\n(a)\n (b)\n(c)\n (d)\nFig. 3.Ground-truth segmentations of a\nsimulated image, with a varying thresh-\nold (n ∗σ). The coloured regions label\ndistinct objects, and the black regions\nmake up the background. ( a) Simu-\nlated image. ( b) Ground truth (1.0 σ).\n(c) Ground truth (0.5 σ). ( d) Ground\ntruth (0.1σ).\nThe value of n has a substantial eﬀect on the areas allocated\nto objects, as shown in Fig. 3. Consequently, it has a large impact\non the evaluation of the segmentation maps produced by the\ntools.\nAs we aim to evaluate the performance of the tools at levels\nof low surface brightness, we chose to use a value of n = 0.1 for\nour ground truths. This pushes the tools to optimise their param-\neters to capture and correctly allocate as much of the light in the\nimages as possible.\n3.1.3. Real-world data\nWhilst testing algorithms on real-world data is subject to several\nlimitations, as discussed above, it is nevertheless essential, as it\nallows us to subjectively evaluate performance on structures and\nconditions which cannot be easily simulated, such as streams,\nspiral galaxies, and unusual artefacts.\nWith this in mind, we selected a number of images in the\noptical which contained examples of these features. We chose\nimages from the Fornax Deep Survey (FDS; Iodice et al. 2016;\nVenhola et al. 2018), IAC Stripe 82 Legacy Project 3 (hereafter\nIAC Stripe82; Fliri & Trujillo 2016; Román & Trujillo 2018)\nwhich is a 2.5 degree stripe (−50◦<RA <60◦,−1.25◦<Dec <\n1.25◦) with a total area of 275 square degrees in all the ﬁve Sloan\nDigital Sky Survey (SDSS) bands and the Hubble Ultra Deep\nField (HUDF; Beckwith et al. 2006), a 11 arcmin2 region in the\n3 http://research.iac.es/proyecto/stripe82/\nSouthern Sky. As the simulated images were designed to mimic\nthe FDS, using real images from this survey allowed us to test\nthe optimised parameters with similar imaging conditions, where\nthey would be expected to perform well. The additional use of\nIAC Stripe82 and HUDF images allows us to examine the con-\nsistency of parameters on images with very di ﬀerent imaging\nconditions.\nWhile the FDS and IAC Stripe82 are deep surveys using\nground-based telescopes, namely the VLT Survey Telescope\n(VST) and the SDSS Telescope, respectively, the well-studied\nHUDF extends our analysis to the higher resolution, space-\nbased data from the Hubble Space Telescope. In terms of depth,\nthe HUDF is the deepest with a 5 σ point source depth of\n∼29 mag computed over 0.6 ′′ apertures (see Bouwens et al.\n2009, Table 1), which corresponds to a surface brightness\nlimit of µV606 ∼ 32.5 mag arcsec−2 in the V606-band, com-\nputed as a 3 σ ﬂuctuation with respect to the background of\nthe image in 10 ×10 arcsec2 boxes (3σ; 10 ×10 arcsec2). The\nFDS images in the SDSS r-band have a limiting depth of µr ∼\n29.8 mag arcsec−2 (3σ; 10 ×10 arcsec2) and the IAC Stripe82\nsurvey is ∼1 mag shallower than FDS with a limiting surface\nbrightness depth of µr ∼28.6 mag arcsec−2 (3σ; 10 ×10 arcsec2)\nand µg ∼29.1 mag arcsec−2 (3σ; 10 ×10 arcsec2). In order to\nselect the deepest imaging from all these surveys in the optical\nregime, we use theV606-band images in the HUDF and the SDSS\nr- and g-band images from FDS and IAC Stripe82 respectively.\nAdditionally, the FDS, IAC Stripe82 and HUDF datasets col-\nlectively represent deep data with di ﬀerent surface brightness\nA107, page 5 of 30\nA&A 645, A107 (2021)\nTable 1.Summary of qualitative evaluation.\nMTObjects NoiseChisel ProFound SExtractor\nOptimised parameters 2 20 8 6\nLanguage Python /C C R C\nClean edges of detected objects – ✓ ✓ Sometimes\nDetects elongated galaxy (FDS – Fig. 17) ✓ Fragmented – Fragmented\nDetects galaxy close to star (FDS – Fig. 18) ✓ Fragmented – Fragmented\nDetects cirrus (Stripe82 – Fig. 19) ✓ ✓ – Sometimes\nIsolates spiral substructures (HUDF – Fig. 24) ✓ – – –\ndepths and spatial resolutions: FDS is >1 mag deeper and\ntwo times higher in spatial resolution than IAC Stripe82 (0.2\narcsec pixel−1 resolution in FDS (rebinned from the 0.21\narcsec pixel−1 of the VST) compared to 0.396 arcsec pixel −1 in\nSDSS) and the HUDF is >2 mag deeper than FDS, with the best\nresolution currently possible from space ∼0.05 arcsec pixel−1.\nTherefore, the optimised parameters of each algorithm are tested\non real images with varying depth and resolution. However, in\nthis work we speciﬁcally chose images in the optical wave-\nlengths to test the limits of current detection algorithms for\nupcoming deeper and wider surveys such as LSST. In future\nwork, a similar analysis to that performed here could readily be\nextended to other wavelengths.\n3.2. Parameter optimisation\nTo produce a fair comparison of the capabilities of the algo-\nrithms, they should be tested with parameters that are as close\nto optimal as possible. Due to the extremely large parame-\nter spaces of some of the tools, it was not feasible to manu-\nally optimise the tools, or to test every possible combination of\nparameters.\nWe therefore chose to use an automatic method to select\ngood parameters for each tool. We initially considered using\na genetic algorithm for this purpose; however, this proved to\nbe prohibitively slow, as a high number of time-consuming\nruns of each tool was required. Instead, we used Bayesian\noptimisation.\nBayesian optimisation is a method of black-box optimisation\nwell-suited for functions that take a long time to evaluate (Jones\net al. 1998). It operates by creating a model of how the function\nbehaves, identifying the regions in parameter space where it may\nperform well or where it may not be well-ﬁtted, and choosing\npoints in these regions to evaluate, in order to improve the model.\nIn the context of source-extraction tools, the input takes the\nform of a set of relevant parameters, as dictated by each tool’s\ndocumentation. The parameters are evaluated by running the tool\non a training image, comparing the output to a known ground\ntruth, and choosing a metric (as detailed in Sect. 3.3) as the out-\nput score to optimise.\nWe used the GPyOpt optimisation library (The GPyOpt\nauthors 2016) to perform the optimisations. For each metric,\neach tool was optimised on every image individually, and the\nfound parameters were then applied to all of the remaining\nimages to assess their performance. The tools’ default param-\neters were used as a starting point. Using the local penalisa-\ntion method, 120 evaluations were performed on each image in\nbatches of four, and the best set of parameters was chosen.\n3.3. Metrics\nThe quality of a segmentation can be measured both in terms of\nthe presence and absence of ground-truth objects and the simi-\nlarity between the true objects and segmented shapes.\n3.3.1. Matching detections\nWhen measuring detection rates, it is necessary to match\ndetected objects with ground-truth objects. It may be the case\nthat a detected object covers the area of multiple true objects,\nor conversely that multiple detected objects are found within the\narea of a single true object. Therefore, a one-to-one mapping is\nrequired in order to prevent algorithms from being rewarded for\nfailing to correctly distinguish between sources.\nWe chose to use the brightest pixel in each object as an iden-\ntiﬁer, and the detected object containing the brightest pixel in a\nground-truth object was matched to this identiﬁer. In the event\nthat a detected object contained the brightest pixel of multiple\nground-truth objects, the object containing the pixel with the\nhighest ﬂux was chosen as a unique match.\nThree measures made use of this matching procedure:\n– Detection recall (completeness): the proportion of objects\nthat are detected.\n– Detection precision (purity): the proportion of segments that\ncan be matched to real objects.\n– F-score: the harmonic mean of precision and recall:\nF-score = 2 ×precision ×recall\nprecision + recall· (4)\n3.3.2. Evaluating areas\nIn order to quantify the accuracy of the areas of segmented\nobjects, we used a modiﬁed version of over-merging and under-\nmerging scores (Levine & Nazif 1981). The under-merging score\nmeasures the extent to which objects that should be a single seg-\nment are broken into multiple pieces by the segmentation tool.\nThe over-merging score measures the opposite, that is, the extent\nto which multiple objects are incorrectly combined into a single\nsegment by the tool. Combining these scores gives a measure of\nthe overall quality of the segmentation.\nIn the original method, the ground-truth segmentation is\ndivided into N segments, R1 ... RN , with areas A1 ... AN , and the\ntest segmentation is divided into M segments, T1 ... TM, with\nareas a1 ... aM. The original metrics are calculated by ﬁnding Rk\nto maximise T j ∩Rk,for each test segment, T j:\n– Under-merging error (UM):\nUM =\nM∑\nj=1\n(Ak −(T j ∩Rk))(T j ∩Rk)\nAk\n· (5)\nA107, page 6 of 30\nC. Haigh et al.: Optimising and comparing source-extraction tools using objective segmentation quality criteria\n– Original over-merging error (OM 0):\nOM0 =\nM∑\nj=1\n(aj −(T j ∩Rk)). (6)\nIn these original deﬁnitions, we found that the over-merging\nscore did not penalise segmentations, which divided large\nobjects into many small pieces. This meant that tools could ﬁnd\nenormous numbers of false positives, fragmenting the ‘back-\nground’ segment, without penalty. Consequently, we chose to\nredeﬁne the over-merging score to become symmetric to the\nunder-merging score, which better takes into account the num-\nber and size of segments. We also deﬁned an Area score, which\ncombined the two measures to give an overall score.\n– Over-merging error (OM): for each reference segment, Rk,\nﬁnd T j to maximise T j ∩Rk\nOM =\nN∑\nk=1\n(aj −(T j ∩Rk))(T j ∩Rk)\naj\n· (7)\n– Area score:\nArea score = 1 −\n√\nOM2 + UM2. (8)\nAs the Area score alone does not take into account precision and\nrecall, we also deﬁned two combined scores. These give us the\nability to optimise for a balanced F-score and Area score.\n– Combined score A:\n√\nArea score2 + F-score2. (9)\n– Combined score B:\n3√\n(1 −OM) ×(1 −UM) ×F-score. (10)\nWe additionally measure speed, that is, the rate at which images\ncan be processed, in megapixels per second.\n4. Results\nWhilst the original intent was to compare all four programs on\nall metrics, ProFound proved to be very slow to optimise and\nrun, making it impractical for use on large images and surveys.\nAs such, it was optimised only on F-score and Area score. Pro-\ncessing speeds are discussed in more detail in Sect. 4.6.\n4.1. Detection accuracy\nFigure 4 shows the range of F-scores produced when each tool is\noptimised for F-score. Two plots are shown for each tool: one in\nwhich the scores are grouped by the image being evaluated, and\none in which the scores are grouped by the training image used\nto optimise the parameters. The scores of the training image are\nexcluded from both graphs.\nFor both MTObjects and SExtractor, it is notable that the\nscores have smaller interquartile ranges and more varied medi-\nans when grouped by test image. This suggests that for these\ntools, the factor limiting the performance is the structure of each\nindividual test image, rather than the particular parameter set\nchosen. In contrast, ProFound has a smaller interquartile range\nwhen scores are grouped by optimisation image, suggesting that\nin this case, performance is limited by the image used in the opti-\nmisation process.\n1 2 3 4 5 6 7 8 9 10\nTest ID\n0.75\n0.76\n0.77\n0.78\n0.79\n0.80\n0.81F-score\nMT\n1 2 3 4 5 6 7 8 9 10\nTest ID\nNC\n1 2 3 4 5 6 7 8 9 10\nTest ID\n0.75\n0.76\n0.77\n0.78\n0.79\n0.80\n0.81F-score\nPF\n1 2 3 4 5 6 7 8 9 10\nTest ID\nSE\n(a)\n1 2 3 4 5 6 7 8 9 10\nTrain ID\n0.75\n0.76\n0.77\n0.78\n0.79\n0.80\n0.81F-score\nMT\n1 2 3 4 5 6 7 8 9 10\nTrain ID\nNC\n1 2 3 4 5 6 7 8 9 10\nTrain ID\n0.75\n0.76\n0.77\n0.78\n0.79\n0.80\n0.81F-score\nPF\n1 2 3 4 5 6 7 8 9 10\nTrain ID\nSE\n(b)\nFig. 4. F-score test distributions. The parameters of each tool were\noptimised for F-score on each of the ten images, and evaluated on the\nremaining nine images. Boxes extend from ﬁrst (Q1) to third (Q3) quar-\ntiles of the results, with median values marked; whiskers extend to the\nfurthest F-score less than 1 .5 ∗(Q3−Q1) from each end of the box.\n(a) F-scores grouped by image evaluated. ( b) F-scores grouped by\nimage used to optimise parameters.\nOverall, we see the strongest performance from MTObjects,\nwith median scores of over 0.80 for the majority of images. The\nweakest performance was produced by SExtractor, with scores\nof under 0.78 in most cases.\nA107, page 7 of 30\nA&A 645, A107 (2021)\n0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 1.00\nPrecision\n0.62\n0.64\n0.66\n0.68Recall\nSE\nNC\nMT\nPF\nFig. 5.Precision vs. recall. The parameters of each tool were optimised\nfor F-score on each of the ten images, and evaluated on the remaining\nnine images.\nExamining the precision and recall scores that make up the\nF-scores shows that all programs are capable of broadly simi-\nlar performance, with recall between 0.61 and 0.7 and precision\ngreater than 0.93. Whilst the recall scores appear low, many of\nthe faintest objects in the image are not even visible to the human\neye, and may in fact be impossible to detect with any tool; these\nobjects are included in order to fully explore the limits of the\ntools’ capabilities. It is therefore useful to regard recall scores\nprimarily as a relative measure, to compare the tools’ perfor-\nmances.\nDiﬀerences between the programs become apparent when\nthe scores are plotted against each other, as shown in Fig. 5.\nAll the tools have a moderate spread of recall scores, which\nmay be caused by diﬀerences in diﬃculty between the individual\nimages.\nMTObjects and NoiseChisel both produce generally higher\nlevels of precision than SExtractor; with MTObjects giving a\nslightly higher maximum value, and a lower spread. ProFound\nachieves the greatest values for both precision and recall, but has\na very wide spread.\nWhen optimised for Area score, SExtractor showed a sub-\nstantially lower precision; it found an enormous number of false\npositives, as shown in Fig. 6. Here, we clearly see that optimising\nfor Area score is detrimental to the F-score results. This appears\nto be the result of a very low threshold being selected in order to\nmaximise the area of large shapes, meaning that a large number\nof small areas of noise are incorrectly marked as objects.\nIn contrast, NoiseChisel and MTObjects were capable of\nincreasing their Area scores without substantially compromis-\ning their F-scores. ProFound performed inconsistently, covering\nthe full range of precision scores across the ten optimisations.\n4.2. Area measures\nUnsurprisingly, all tools were capable of reaching higher Area\nscores when optimised for Area score rather than F-score, as can\nbe seen in Fig. 9.\n0.0 0.2 0.4 0.6 0.8 1.0\nPrecision\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80Recall\nSE\nNC\nMT\nPF\nFig. 6.Precision vs. recall. The parameters of each tool were optimised\nfor Area score on each of the ten images, and evaluated on the remaining\nnine images.\nWhen optimised for Area score, NoiseChisel and MTObjects\nboth performed well, showing Area scores substantially higher\nthan the other two tools, with MTObjects performing slightly\nbetter than NoiseChisel. Both tools also showed lower variation\nwhen scores were grouped by test image, as shown in Fig. 7,\nsuggesting that the performance of these tools is being limited by\nthe content of the test images, rather than the parameters found\nin the optimisation.\nIn contrast, ProFound showed much greater variability in\nArea scores when grouped by test image, and indeed, substan-\ntial variation between the parameter sets. It also produced the\nweakest Area scores overall. SExtractor was capable of produc-\ning higher Area scores than ProFound, but at substantial cost to\nprecision, as discussed above.\n4.3. Combined scores\nThe two combined metrics oﬀered a way of optimising for both\nArea and F-score, diﬀering in the balance between the two mea-\nsures. As such, optimising for these metrics gives an indication\nof the overall peak performance of the tools.\nIn practice, both metrics produced broadly similar results in\nterms of both Area and F-score, as shown in Fig. 8. MTObjects\nproduced the highest values for both F-score and Area score,\nwith NoiseChisel producing slightly lower values in both met-\nrics. SExtractor produced lower F-scores, with a large degree\nof variability, and substantially lower Area scores, as would\nbe expected from its limited success when optimising purely\nfor area. These results indicate that optimisation for combined\nscores prevents a large number of spurious detections being\nfound by SExtractor, when compared to Area score alone.\n4.4. Overview of optimisation metrics\nFigure 9 shows an overview of the results of the optimisation\nin the form of scatter plots of F-score and Area score. Points\nrepresent the result of evaluating the performance of the four\nA107, page 8 of 30\nC. Haigh et al.: Optimising and comparing source-extraction tools using objective segmentation quality criteria\n1 2 3 4 5 6 7 8 9 10\nTest ID\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55Area score\nMT\n1 2 3 4 5 6 7 8 9 10\nTest ID\nNC\n1 2 3 4 5 6 7 8 9 10\nTest ID\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55Area score\nPF\n1 2 3 4 5 6 7 8 9 10\nTest ID\nSE\n(a)\n1 2 3 4 5 6 7 8 9 10\nTrain ID\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55Area score\nMT\n1 2 3 4 5 6 7 8 9 10\nTrain ID\nNC\n1 2 3 4 5 6 7 8 9 10\nTrain ID\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55Area score\nPF\n1 2 3 4 5 6 7 8 9 10\nTrain ID\nSE\n(b)\nFig. 7.Area score test distributions. The parameters of each tool were\noptimised for Area score on each of the ten images, and evaluated on\nthe remaining nine images. (a) Area scores grouped by image evaluated.\n(b) Area scores grouped by image used to optimise parameters.\ntools when applied to each image using the parameters found\nby optimising for each metric on every other image individually.\nFrom this, we can make several observations about the tools’\nperformance.\nFirstly, the tools designed speciﬁcally for locating low-\nsurface-brightness structures (NoiseChisel and MTObjects) are\nunsurprisingly capable of achieving higher Area scores than\nthe general-purpose tools. Secondly, all the tools must to some\n0.6 0.7 0.8\nF-score\n0.1\n0.2\n0.3\n0.4\n0.5\nArea-score \nCombined A\nSE\nNC\nMT\n(a)\n0.6 0.7 0.8\nF-score\n0.1\n0.2\n0.3\n0.4\n0.5\nArea-score \nCombined B\nSE\nNC\nMT\n(b)\nFig. 8.F-score vs. Area score. The parameters of each tool were opti-\nmised for the combined measures on each of the ten images, and eval-\nuated on the remaining nine images. ( a) Optimised for Combined A.\n(b) Optimised for Combined B.\ndegree compromise F-score to obtain a higher Area score, but\nthis trade-o ﬀ is much greater for the general-purpose tools.\nThirdly, MTObjects has less spread than the other tools; indeed,\nit ﬁnds identical parameters and consequently produces identical\nresults for nearly all optimisations over area or combined scores.\nExamining Figs. 10 and 11 provides further insight into the\nbehaviour leading to these scores. We see that both NoiseChisel\nand MTObjects capture regions of light with visually similar\nA107, page 9 of 30\nA&A 645, A107 (2021)\n0.0 0.2 0.4 0.6 0.8\nF-score\n0.1\n0.2\n0.3\n0.4\n0.5Area-score \nF-score\n0.0 0.2 0.4 0.6 0.8\nF-score\n0.1\n0.2\n0.3\n0.4\n0.5Area-score \nArea\n0.0 0.2 0.4 0.6 0.8\nF-score\n0.1\n0.2\n0.3\n0.4\n0.5Area-score \nComb. A\n0.0 0.2 0.4 0.6 0.8\nF-score\n0.1\n0.2\n0.3\n0.4\n0.5Area-score \nComb. B\nSE\nNC\nMT\nPF\nFig. 9. A summary of test scores for\neach program using each optimisation\nmethod. Each point represents the eval-\nuation of the segmentation of one image\nusing parameters found by optimising\non a di ﬀerent image. Each plot shows\nresults for a di ﬀerent optimisation met-\nric. We note that ProFound was only\noptimised on F-score and Area score.\nboundaries, but that MTObjects marks many small, frac-\ntured sections in the outer regions as background. Meanwhile,\nNoiseChisel captures an area of light with fewer holes, but seg-\nments it into objects rather arbitrarily. In contrast, SExtractor and\nProFound, which both have generally lower Area scores, cap-\nture the compact centres of objects and only limited areas of the\noutskirts.\n4.5. Background values\nEach program makes internal estimations of background, which\nmay be global or localised. We may also examine the pixels in\nthe image which are not allocated to any segment in the ﬁnal\nmap. As the simulated images have a ﬂat background with a\nmean of zero, we can use the mean value of these unallocated\npixels as an indication of whether pixels containing no source\nlight are being incorrectly allocated to sources or, conversely,\npixels are incorrectly regarded as belonging to sources.\nProFound and SExtractor both consistently overestimated\nthe background, giving values on the order of 10−1σ, where σis\nthe standard deviation of the background noise (1 .1 ×10−12 for\nthe simulated images). This suggests that they are not detecting\nsome parts of the sources; visual inspection of Figs. 10 and 11\nconﬁrms that this is the case. There was one exception to this\nbehaviour: SExtractor generally underestimated the background\nwhen optimised for area, with values on the order of −10−2σ.\nThis corresponds to the large number of small false-positive\ndetections made under this optimisation thanks to the low back-\nground threshold used (see Table B.1).\nMTObjects also underestimated the background, with values\nof around −10−1σ when optimised for metrics including area\nmeasures; it underestimated to a lesser degree ( −10−2σ) when\noptimised for F-score. This behaviour may be a consequence of\nthe holes in the outskirts of objects causing the optimisation pro-\ncess to select parameters that overestimate the size of objects,\nthereby increasing the solid area within objects but also the num-\nber of incorrectly labelled background pixels.\nThe strongest background estimation performance was pro-\nduced by NoiseChisel. Whilst optimising for F-score lead to an\noverestimation in a similar range to SExtractor and ProFound,\nit produced mean backgrounds in the order ±10−3σwhen opti-\nmised for a metric including area measures. Not only were the\nvalues closer to the goal of zero, but there was also no evidence\nof systematic over- or underestimation.\n4.6. Speed\nThe speed at which an image can be processed is very important\nwhen we consider the size and quantity of images produced by\nmodern surveys.\nAt its best, SExtractor was the fastest of all the tools by\na considerable margin, as shown in Fig. 12. When optimised\nfor area, this advantage vanished completely, potentially due\nto the vast increase in the number of false positives and large\nobjects. When optimised for combined metrics, processing speed\ndepended heavily on the individual set of parameters, producing\na wide spread of speeds.\nA107, page 10 of 30\nC. Haigh et al.: Optimising and comparing source-extraction tools using objective segmentation quality criteria\n(a)\n (b)\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(c)\nFig. 10.Segmentations of a full simulated image using the parameters which gave the highest median score for each combination of optimisation\nmeasure and tool on the simulated images: SExtractor (SE), NoiseChisel + Segment (NC), MTObjects (MT) and ProFound (PF). The coloured\nregions label distinct objects, and the black regions make up the background. In the interest of speed, PF was not optimised for Combined A and\nB. (a) Original simulated image. (b) Ground truth (0.1σ). (c) Segmentation maps.\nMTObjects had the most consistent speed across optimi-\nsations. Neither SExtractor nor MTObjects used parallel pro-\ncessing, which potentially reduced their speed. It should be\nnoted that the original C implementation of MTObjects is faster\nthan our current Python and C implementation. As reported by\nTeeninga et al. (2016), SExtractor was only 2.5 times faster than\nthe C version of MTObjects in terms of median performance,\nand only 1.3 times faster on average. Some code optimisation\nand using a parallel max-tree algorithm Moschini et al. (2018)\nshould be able to improve the performance in terms of speed.\nNoiseChisel showed fast performance when optimised for\nF-Score alone, but was much slower when Area score was\nA107, page 11 of 30\nA&A 645, A107 (2021)\n(a)\n (b)\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(c)\nFig. 11.Segmentations of a section of a simulated image using the parameters which gave the highest median score for each combination of\noptimisation measure and tool. For more information, see Fig. 10. (a) Original simulated image. (b) Ground truth (0.1σ). (c) Segmentation maps.\nincluded in the optimisation criterion. This appears to be due to\na combination of factors; predominantly a lower value for ‘det-\ngrowquant’, which aﬀects the extent to which objects are grown\nafter detection4.\n4 We ﬁnd that some non-optimal parameter combinations also cause\nsubstantial slowdown, which is due to the program requiring large\namounts of memory and consequently writing some data structures to\ndisk.\nAs mentioned previously, ProFound consistently had a very\nlong processing time, which greatly reduced its viability as a\ntool for processing large images from surveys with many images.\nThis is due in part to it writing temporary data to disk, which is\ndiscussed in the original ProFound publication (Robotham et al.\n2018): ProFound oﬀers a low-memory mode which reduces the\namount of data stored, allowing the processing of larger images\nwithout a drastic slowdown; however, as noted, the method is\nfundamentally rather slow. The use of R as the implementation\nA107, page 12 of 30\nC. Haigh et al.: Optimising and comparing source-extraction tools using objective segmentation quality criteria\nMT NC PF SE\nTool\n1.5\n1.0\n0.5\n0.0\n0.5\nF-score\nMT NC PF SE\nTool\nArea\nMT NC SE\nTool\n1.5\n1.0\n0.5\n0.0\n0.5\nComb. A\nMT NC SE\nTool\nComb. B\nlog10(Speed (Mpx/second))\nFig. 12. Distributions of processing speed across all combinations of\nimages and optimised parameters for each tool and optimisation metric.\nlanguage may further reduce the potential speed of the tool. The\nauthors of ProFound are rewriting parts of the code in C ++,\nwhich should signiﬁcantly improve its performance.\n4.7. Parameter consistency\nMTObjects was by far the most consistent of the tools; having\nonly two relevant parameters, it had a much smaller parameter\nspace to explore. While its optimised parameters varied slightly\nwhen optimising only over F-score, all other metrics gave the\nsame optimal parameters for all cases but one, as shown in\nTable B.4.\nSExtractor and NoiseChisel, optimised over 6 and 20 param-\neters respectively, displayed far less consistency in the param-\neters that were found (Tables B.1–B.3). This could potentially\nhave been reduced by increasing the number of iterations of\nthe optimisation process. However, the similar scores produced\nusing very di ﬀerent parameters suggest that there is no single\nbest choice, and many combinations of settings perform equally\nwell overall, but are better or worse in certain contexts.\n4.8. Inserted galaxies\nAs a ﬁnal step, we evaluated the performance of the tools on\na sample of real galaxies, inserted into a frame of the Fornax\nDeep Survey (FDS), which the simulated data was designed to\nemulate. Testing the tools on real galaxies allows us to verify\nthat the behaviour of the tools generalises to galaxies which are\nnot perfect ellipticals.\nWe selected a sample of 22 galaxies from the EFIGI cat-\nalogue (Baillard et al. 2011), which contains images from the\nfourth data release of the Sloan Digital Sky Survey (SDSS;\nAdelman-McCarthy et al. 2006). Galaxies were selected with\nD25 (diameter measured at the 25.0 mag arcsec −2 isophote, in\nunits of log 0.1 arcmin) between 1.7 and 1.999, a heliocen-\ntric velocity <2000 km s−1, and a galactic latitude of between\n60◦ and 70◦. This is a representative sample of galaxies in the\nnearby Universe, with high-quality SDSS images and detailed\n(a)\n (b)\n(c)\n (d)\nFig. 13. Galaxy from the EFIGI sample inserted into the FDS frame\nat the four given brightness scalings. ( a) 10 −10. ( b) 10 −11. ( c) 10 −12.\n(d) 10−13.\nmorphological types. We isolated the galaxy at the centre of each\nimage using k-ﬂat ﬁltering (Ouzounis & Wilkinson 2010), which\nremoved areas of light not connected to the central pixel, whilst\npreserving the galaxy’s internal detail. We then convolved each\ngalaxy with ther-band point spread function of the OmegaCAM,\nand added Poissonian noise.\nIn order to examine the performance of the algorithms on\ngalaxies of diﬀerent brightnesses, we scaled the images to four\ndiﬀerent brightness levels, as shown in Fig. 13. At the brightest\nlevel, the brightest pixel in each galaxy had a value on the same\norder as the brightest pixels in the image, (around 10 −10, corre-\nsponding to a surface brightness of 21.5 mag arcsec −2). At the\nfaintest, the brightest pixels were barely visible to the human\neye (around 10 −13, corresponding to a surface brightness of\n29 mag arcsec−2). We selected 22 locations in the FDS frame\nwhere there were very few objects present in order to minimise\ninterference with the inserted galaxies. We then created four\nimages, with the 22 galaxies inserted into the same locations\nin the FDS frame using a di ﬀerent brightness scaling for each\nimage. We then ran all four tools on each image with the four\nsets of optimised parameters obtained on the simulated images.\nWhilst using inserted galaxies meant that we had a ground\ntruth for those galaxies, there may still have been other objects\npresent around them in the FDS frame, which would also be\ndetected by the tools. This means that we are unable to rely on\nthe previously deﬁned metrics, as other detected objects would\nbe marked as false positives and raise the under-merging error.\nInstead, we use a modiﬁed process to determine whether\nan inserted galaxy has been detected. If the brightest pixel in\nan object is contained within a non-background segment of the\nsegmentation map and is also the brightest pixel in that segment,\nwe determine that the object has been detected.\nAdditionally, we classify detections into two types: those\nwhere the galaxy has been mostly detected as a single object,\nand those where the algorithm has substantially fragmented the\ngalaxy. To do this, we check for other detected segments whose\nbrightest pixel is contained within the area of the inserted galaxy,\nA107, page 13 of 30\nA&A 645, A107 (2021)\n(a)\n (b)\nFig. 14.Segmentation maps showing the two deﬁned types of detection.\n(a) A ‘whole’ detected galaxy. (b) A fragmented galaxy.\nsuggesting that they are not primarily detecting some other back-\nground object. If there are multiple segments which meet this\ncriteria, we check that the segment containing the most light\nfrom the inserted galaxy has at least ten times the amount of\nlight contained in the segment containing the second-most light.\nIf it does, we mark the detection as ‘whole’; otherwise as frag-\nmented. Whilst lacking the numerical accuracy of the previously\ndeﬁned Area score, this provides an indication of the quality of\ndetections. Examples of the two types of detection are shown in\nFig. 14.\nThe results of this process are summarised in Fig. 15.\nAt higher brightnesses, most tools perform well, with only\nNoiseChisel failing to detect any objects at the two highest\nbrightness levels.\nAt fainter levels, the tools show more variation. At the 10−12\nbrightness level, ProFound shows the strongest performance,\nfully detecting nearly over 90% of the objects under an Area\nscore optimisation. SExtractor shows high levels of fragmenta-\ntion at this level, consistent with its low Area score found on\nthe simulated data. NoiseChisel maintains a roughly consistent\nrate of fragmented detections, but with fewer detections over-\nall, whilst MTObjects begins to show some fragmentation and a\nlower detection rate at this level.\nAt the faintest brightness, very few of the inserted galax-\nies are visible to the human eye, and this is reﬂected in the\nresults. Again, ProFound has a stronger performance than the\nother tools, with up to 40% of galaxies detected, but a higher\nrate of fragmentation than at higher brightness levels. SExtrac-\ntor reaches a similar detection rate under an Area score optimi-\nsation, but only produces fragmented detections; visual inspec-\ntion shows that this is due to the tool ﬁnding many tiny objects,\nas with the simulated images. Both NoiseChisel and MTObjects\nﬁnd very few objects at this low brightness level.\nThese results are generally consistent with the results shown\nin the preceding sections: all tools were capable of similar\nF-scores, and this is reﬂected in the similar detection rates found\non the inserted galaxies. Similarly, variations in Area score\nroughly correspond to the fraction of the inserted galaxies with\nsubstantial fragmentation for each tool, particularly at the 10 −12\nbrightness level.\nIt is notable that when the inserted galaxies are fainter, opti-\nmisations for F-score appear to be less e ﬀective than optimisa-\ntions for Area score. This may be due to the higher sensitivity to\nnoise and lower thresholds generally found in area-based opti-\nmisations causing the fainter objects to be detected, whilst the\nF-score-based optimisations ignore these objects in order to min-\nimise false detections.\n5. Qualitative evaluation\nIn this section, we evaluate how the optimised parameters for\neach tool transfer to di ﬀerent surveys and instruments. We\nselected three surveys for application of the tools, using the\nparameters with the highest median test score following the opti-\nmisation process: the Fornax Deep Survey (FDS; Iodice et al.\n2016; Venhola et al. 2018); the IAC Stripe82 Legacy Project\n(Fliri & Trujillo 2016; Román & Trujillo 2018); and the HUDF\n(Beckwith et al. 2006). All of these datasets are deep surveys,\nwith surface brightness limits fainter than µ ∼28 mag arcsec−2,\nand have been used in several studies of galaxies of low sur-\nface brightness; for example, Venhola et al. (2017, 2019) and\nIodice et al. (2019) for FDS, Román & Trujillo (2017a,b) for\nIAC Stripe82, and Oesch et al. (2009) and Bouwens et al. (2008)\nfor HUDF. As far as we are aware, all these works used SExtrac-\ntor for masking sources of light and processing observational\ndata. Therefore, evaluating the quality of segmentation for these\ndeep datasets using the other available source-extraction tools\nis an added value to ongoing research on faint structures of\ngalaxies.\nMoreover, using the source-extraction tools to derive seg-\nmentation maps of a completely new dataset with the ‘best’ opti-\nmised parameters allows us to assess whether or not the param-\neters perform in a consistent manner across di ﬀerent datasets\nacquired in very diﬀerent conditions. It is also a test of the practi-\ncal applicability of each tool to large astronomical surveys of the\nfuture, such as those produced by Euclid (Amiaux et al. 2012)\nand the LSST (Ivezi´c et al. 2019).\nThe ‘best’ parameters derived from our optimisation scheme\nfor each test score that are used for the tools are highlighted with\nan asterisk in Appendix B.\n5.1. Fornax Deep Survey\nAs the simulated images were created using the characteristics of\nthe Fornax Deep Survey (FDS), using images from the real sur-\nvey allows us to check that the parameters found on simulated\ndata perform similarly on data that contain more unusual struc-\ntures. The limiting surface brightness for r-band images of FDS\nis 29.8 mag arcsec−2 (3σ; 10 ×10 arcsec2; Venhola et al. 2017).\nHere we show a complete frame of the survey, and two\nsmaller areas of the same frame, containing faint and challeng-\ning objects. For each combination of training image and optimi-\nsation method, the parameters with the highest median test score\non the simulated dataset were used.\nIt is clear from Fig. 16 that the parameters lead to a very\nsimilar performance with the real images to with the simulated\nimages. MTObjects and NoiseChisel both capture similar areas\nof light, but segment them very diﬀerently; whilst ProFound and\nSExtractor capture only the centres of objects.\nExamining smaller details of the images gives more insight\ninto behaviour on challenging sources. Figure 17 shows the seg-\nmentation of a faint, elongated galaxy. SExtractor only detects a\nsmall area of the galaxy when optimised for area, and incorrectly\nmerges it with other surrounding objects; in all other optimisa-\ntions it fails to detect the galaxy at all, perhaps because of an\noverly high detection threshold. ProFound detects small blobs\ncovering the area of the galaxy, but does not identify an under-\nlying structure. Similarly, NoiseChisel, whilst locating a larger\narea of light, breaks it into chunks appearing to correspond to\nsmaller objects, losing the large structure. MTObjects was the\nonly tool to capture the entire structure as one object, but incor-\nrectly labelled it as the same object as the bright source in the\nA107, page 14 of 30\nC. Haigh et al.: Optimising and comparing source-extraction tools using objective segmentation quality criteria\n10 10\n 10 11\n 10 12\n 10 13\nOrder of maximum pixel brightness\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100Percentage of galaxies detected (%)\nOptimisation metric\nF-score\nArea score\nCombined A\nCombined B\n(a)\n10 10\n 10 11\n 10 12\n 10 13\nOrder of maximum pixel brightness\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100Percentage of galaxies detected (%)\nOptimisation metric\nF-score\nArea score\nCombined A\nCombined B (b)\n10 10\n 10 11\n 10 12\n 10 13\nOrder of maximum pixel brightness\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100Percentage of galaxies detected (%)\nOptimisation metric\nF-score\nArea score\n(c)\n10 10\n 10 11\n 10 12\n 10 13\nOrder of maximum pixel brightness\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100Percentage of galaxies detected (%)\nOptimisation metric\nF-score\nArea score\nCombined A\nCombined B (d)\nFig. 15. Percentage of inserted objects found, grouped by tool, brightness scaling, and optimisation metric, using the parameters which gave\nthe highest median score for each combination of optimisation measure and tool. Lighter, stacked bars represent galaxies that are detected but\nsubstantially fragmented. (a) MTObjects. (b) NoiseChisel. (c) ProFound. (d) SExtractor.\nbottom right corner, and also connected it to the outskirts of the\nobject in the bottom left.\nFigure 18 contains another faint structure located near a\nbright star, which is extremely di ﬃcult to visually detect. All\nfour tools struggle to produce ideal results in this situation. As\nbefore, ProFound and SExtractor do not detect the faintest parts\nof objects, which here gives the advantage of allowing both tools\nto distinguish between smaller sources. SExtractor again pro-\nduces a high number of false positives when optimised for area,\nbut does begin to detect areas of structure in Combined A and\nB. In contrast, ProFound produces a blobby segmentation, with\nless visual similarity to the input image, but again covering a\ngood deal of the smaller structures. NoiseChisel and MTOb-\njects mark almost all of the image section as containing sources,\nbut with a very di ﬀerent segmentation. The area optimisation\nof NoiseChisel fails to detect any substructures in this part of\nthe image, marking all objects as a single large structure. In the\nother optimisations, it shows very little visual similarity to the\ninput image. MTObjects correctly detects many of the sources\nin the area, although it again joins the outskirts of some objects,\nand produces a ragged appearance.\n5.2. IAC Stripe 82 Legacy Project\nAs an added layer of generalisation, we test the parameters\nwith the highest median test score found for each combination\nof training image and optimisation method on deep g-band\nIAC Stripe82 images. The limiting surface brightness is 29 .1\nmag arcsec−2 (3σ, 10 ×10 arcsec2; Román & Trujillo 2018).\nThese images consist of faint and di ﬀuse structures such as\nGalactic cirri, tidal streams, interacting galaxies, and include\nscattered light from point sources.\nSimilarly to the segmentation of the simulated images seen\nFigs. 10 and 11, we ﬁnd that SExtractor detects the least amount\nof light compared to the other tools. In particular, it misses large\nportions of the Galactic cirrus structure in Fig. 19, even when\noptimised for the Area score. As in the case of the simulated\nimages, the fact that many smaller objects (including many false\npositives) are detected in the background when optimised for the\nArea score is most likely a consequence of the very low threshold\nused to ﬁnd larger areas. However, the Galactic cirrus structure is\nhighly extended and diﬀuse with low- and high-density regions,\nso the tool is unable to segment the structure as a single object,\nand fragments it into several pieces. However, this ‘failure of\ndetection’ may be taken advantage of (with some manual inter-\nvention) for studying the properties of Galactic cirrus (Román\net al. 2020).\nRemarkably, the performance of SExtractor on the image of\nthe interacting galaxies connected with a tidal stream in Fig. 20\nis much better on the main objects with parameters optimised for\nArea score and both combined scores, whilst performing poorly\nin the background. Similar observations can be made in the case\nof the elliptical galaxy with a large stream in Fig. 21, but this\nstream is much fainter than in the interacting galaxies case, and\nA107, page 15 of 30\nA&A 645, A107 (2021)\n(a)\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b)\nFig. 16.Segmentations of a complete FDS ﬁeld (ﬁeld 11). (a) Input image – the r-band of ﬁeld 11 of the FDS. (b) Segmentations of the ﬁeld using\nthe parameters that gave the highest median score for each combination of optimisation measure and tool. For more information, see Fig. 10.\nSExtractor detects the stream in fragments (similar to the Galac-\ntic cirrus).\nIn contrast, for all the IAC Stripe82 images, both Noise\nChisel and MTObjects detect the largest amount of light as dis-\ntinct objects or di ﬀuse regions (reﬂected in the highest optimi-\nsation scores). Visually, the performance of NoiseChisel seems\nbetter when optimised for F-Score compared to the other scores,\nbut there is still di ﬀuse light around the objects which have\ngone undetected. When optimised for Area score or the com-\nbined scores, this missing light is recovered, but as mentioned\npreviously, the algorithm seems to segment structures within\nlarger objects rather arbitrarily. When comparing the outputs\nA107, page 16 of 30\nC. Haigh et al.: Optimising and comparing source-extraction tools using objective segmentation quality criteria\n(a)\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b)\nFig. 17. Segmentations of a section\nof an FDS ﬁeld (ﬁeld 11), showing a\nlow-surface brightness galaxy. ( a) Input\nimage – the r-band of ﬁeld 11 of the\nFDS. ( b) Segmentations of the ﬁeld\nsection using the parameters that gave\nthe highest median score for each combi-\nnation of optimisation measure and tool.\nFor more information, see Fig. 10.\nfrom each optimisation method, we can see that the substruc-\nture is segmented quite diﬀerently in all the IAC Stripe82 exam-\nples. This is probably a consequence of growing the ‘clumps’\n(as detected in the CLUMPS output of Segment) to cover the full\ndetected area; if the detected area is di ﬀerent, then the growth\nof the clumps seems to vary. This e ﬀect is visible in compar-\ning NoiseChisel’s output when optimised for all four measures\nin all the IAC Stripe 82 examples. The fact that the substructure\nover the detected regions seems visually arbitrary may not be an\nissue in some cases, such as when segmentation maps are used\nfor reducing datasets where all pixels with a signiﬁcant amount\nof signal above the background needs to be masked for process-\ning (see e.g. Borlaﬀ et al. 2019), or when the user is simply not\nconcerned with the substructure of astronomical sources5.\nHowever, for studies where more accurate segmentation of\ntidal streams and nested objects (or substructure) is required\nfor photometric calculations, it is not possible to automatically\n5 The NoiseChisel manual ( https://www.gnu.org/software/\ngnuastro/manual/html_node/NoiseChisel.html) states that the\nuser may choose to run Segment after NoiseChisel depending on\nwhether they want to analyse the substructure of sources.\nA107, page 17 of 30\nA&A 645, A107 (2021)\n(a)\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b)\nFig. 18.Segmentations of a section of a FDS ﬁeld (ﬁeld 11), showing a very faint structure in the lower centre. ( a) Input image – the r-band of\nﬁeld 11 of the FDS. ( b) Segmentations of the ﬁeld using the parameters that gave the highest median score for each combination of optimisation\nmeasure and tool.\nallocate these fragmented regions to their host structure, and the\nuser may need to manually select regions of interest. This is\nespecially visible for the large Galactic cirrus in Fig. 19 and the\nfaint stream in Fig. 21 where the structures are segmented into\nseparate objects of all kinds of shapes.\nFor the same IAC Stripe82 examples, a similar observa-\ntion can be made for MTObjects, but the partitioning better\nfollows the visual shape of all objects (background and nested).\nThis behaviour means that the user is able to make a visual\nmapping between the input image and segmentation map much\nmore easily, if they need to manually select regions of interest.\nIn comparison, the outputs of the tool when optimised for the\ndiﬀerent scores are very similar; the outputs for the area and\ncombined scores are the same, and the only visible di ﬀerence\nA107, page 18 of 30\nC. Haigh et al.: Optimising and comparing source-extraction tools using objective segmentation quality criteria\n(a)\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b)\nFig. 19.Results for IAC Stripe82 ﬁeldf0363_g.rec.fits showing a large structure of Galactic cirri. (a) Left: gri-composite image. Right: g-band\ninput image in log scale. ( b) Segmentation maps using the parameters that gave the highest median score for each combination of optimisation\nmeasure and tool. For more information, see Fig. 10.\nwith F-Score is the extent to which the edges are fractured out-\nwards. Compared to the other tools, the existence of these highly\nfractured edges of the segmented regions in MTObjects may not\nbe an appealing characteristic for the user if smoother edges are\nrequired; such as for instance to make photometric calculations,\nsuch as the total magnitude of objects6.\nAnother characteristic of MTObjects can be seen in the ﬁeld\ncontaminated by a cluster of bright stars to the right of an ellip-\ntical galaxy in Fig. 21. MTObjects allocates the di ﬀuse stream\n6 Of the tools, this e ﬀect in the segmentation maps can only be\ncontrolled in NoiseChisel without compromising the extent to which\nobjects are detected.\nand faint halo around the core of the galaxy to the cluster of\nstars. This is clearly a problem with how the detected regions\nare represented. MTObjects is ﬁnding the di ﬀuse regions in the\nimage (at least those that could be visually identiﬁed in this\nexample), but allocating them to the wrong object. This means\nthat the user will need to once again manually select the regions\nthat belong to the galaxy, and this may not always be possible\nto identify in advance when dealing with deep datasets. Apart\nfrom these exceptions, MTObjects performs fairly similarly and\nconsistently across the IAC Stripe82 images tested in this work.\nDue to speed, at the time of writing we are only able to\ncomplete the optimisation of parameters for ProFound using the\nA107, page 19 of 30\nA&A 645, A107 (2021)\n(a)\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b)\nFig. 20. Results for IAC Stripe82 ﬁeld cropped to show two interacting galaxies (SDSS J031943.04+003355.64 and\nSDSS J031947.01+003504.44). ( a) Left: gri-composite image. Right: g-band input image in log scale. ( b) Segmentation maps using the\nparameters that gave the highest median score for each combination of optimisation measure and tool. For more information, see Fig. 10.\nA107, page 20 of 30\nC. Haigh et al.: Optimising and comparing source-extraction tools using objective segmentation quality criteria\n(a)\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b)\nFig. 21.Results for IAC Stripe82 ﬁeld zoomed in on an elliptical galaxy with an extended, very faint tidal stream (SDSSJ235618.80-001820.17)\nand a bright collection of stars with a signiﬁcant amount of scattered light contaminating the galaxy from the right. (a) Left: gri-composite image.\nRight: g-band input image in log scale. ( b) Segmentation maps using the parameters that gave the highest median score for each combination of\noptimisation measure and tool. For more information, see Fig. 10.\nF-Score and Area score. In this work, we ﬁnd that when opti-\nmised for these scores, only in the merging galaxies case in\nFig. 20 does the tool segment the galaxy shape and its compan-\nion (though also fragmented into several arbitrary pieces, as in\nNoiseChisel’s segmentation). In all of the other images, the large\ngalaxies or structures are barely visible, and only because our\neye is able to connect the smaller fragments into one connected\nregion.\n5.3. The Hubble Ultra Deep Field\nIn order to examine the behaviour of the tools on space-based\nobservations, we ran the tools on the V606-band of the HUDF.\nAs mentioned in Sect. 3.1, the HUDF is the deepest data used in\nthis work, with a point source depth of 29.3 mag (Beckwith et al.\n2006) which is equivalent to a limiting surface brightness depth\nof µV606 ∼32.5 mag arcsec−2 (3σ; 10 ×10 arcsec2).\nAs the original drizzled image contained wide, zero-valued\nborders, we rotated and cropped it to contain as much of the ﬁeld\nas possible, while excluding the borders. We then ran the tools\non the image, using the same optimised parameters as in the pre-\nvious sections. We show here the complete image (see Fig. 22)\nand two smaller areas of interest containing a type of feature not\ncommon in the other surveys: face-on spiral galaxies with visible\nsubstructures.\nBesides these artefacts, the behaviour of all four tools on the\nHUDF image appears to be generally similar to their behaviour\non the images from other surveys, despite the higher depth and\nthe diﬀerent telescope type.\nThis is further corroborated by the results shown in\nFig. 23, which shows a face-on spiral galaxy, as well as\nA107, page 21 of 30\nA&A 645, A107 (2021)\n(a)\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b)\nFig. 22.Segmentations of the rotated and cropped Hubble Ultra Deep Field. (a) Input image – the V606-band of the ﬁeld. (b) Segmentations of the\nﬁeld, using the parameters that gave the highest median score for each combination of optimisation measure and tool. For more information, see\nFig. 10.\nseveral smaller elliptical galaxies. As before, SExtractor ﬁnds\nonly the bright centres of objects, except when optimised\nfor area; however, it noticeably divides the spiral galaxy into\nchunks where there is substructure. Somewhat arbitrary divi-\nsion of the galaxy is also visible in the results of NoiseChisel\nand ProFound; with NoiseChisel capturing more of the out-\nskirts, as before. MTObjects appears to be the most success-\nful at segmenting the spiral, with the majority of the galaxy\ncaptured as a single object, with smaller structures nested\nwithin it; although, as in previous instances, the outskirts are\nfractured.\nIn Fig. 24, which shows a larger spiral galaxy displayed at\nthe same scale, the tools have even greater diﬃculty segmenting\nthe galaxy in a meaningful way. As before, MTObjects has the\nmost success in separating nested structures without fragmenting\nthe overall structure of the object. NoiseChisel is also consistent\nwith previous behaviour. In contrast, ProFound produces quite\ndiﬀerent segmentations, with a far less blobby appearance. SEx-\ntractor produces quite poor segmentations when area is included\nin the optimisation; with elongated ovals being found in both of\nthe combined score images.\nIt must be borne in mind that the parameters were optimised\nfor images in quite di ﬀerent conditions, and so it is di ﬃcult to\nquantify the extent to which these inaccurate segmentations are\ncaused by parameters ill-suited to this context. However, as the\nbehaviour is very similar to that shown in the images from dif-\nferent surveys, it is reasonable to expect that it is largely caused\nby inherent limitations of the tools.\nA107, page 22 of 30\nC. Haigh et al.: Optimising and comparing source-extraction tools using objective segmentation quality criteria\n(a)\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b)\nFig. 23.Segmentations of a section of the Hubble Ultra Deep Field. ( a) Input image – the V606-band of the ﬁeld. ( b) Segmentations of the ﬁeld\nusing the parameters that gave the highest median score for each combination of optimisation measure and tool. For more information, see Fig. 10.\n5.4. Usability\nAs shown in the parameter tables in Appendix B, only MTO-\nbjects reliably found the same set of ‘optimal’ parameters over\nmultiple optimisations. All of the other tools appeared to have\nmultiple locally optimum parameter combinations. This has a\nnegative impact on ease of use; users manually conﬁguring a tool\nthrough trial and error may fail to ﬁnd globally optimum param-\neters, and be unaware of this fact. The best parameters may also\nbe dependent on the image used for optimisation, that is, the\nparameters found for one image or survey may not produce opti-\nmal results when applied to others.\nAll four programs deﬁne parameters in terms of the individual\nsteps of the method (e.g. usen thresholds), rather than in terms of\nhow they aﬀect the overall detection (e.g. detect objects to a given\ndegree of certainty). Without using an optimisation framework,\nusers have no choice but to manually select settings that visu-\nally produce a good result, but which do not necessarily have any\nscientiﬁc justiﬁcation for being chosen. This is further exacer-\nbated by large parameter spaces in the cases of NoiseChisel and\nProFound allowing the user to inﬁnitely adjust the behaviour of\nthe tools without the implications of their choices being clear. The\nability to deﬁne performance in terms of the result rather than the\nprocess would greatly improve the ease of use of the tools, and\nwould reduce the opacity of their behaviour.\nThese are no major problems in the case of a user process-\ning a small number of images, but problems arise when large\nsurveys requiring automatic segmentation for many images are\nA107, page 23 of 30\nA&A 645, A107 (2021)\n(a)\nF-Score Area Combined A Combined B\nSE\nNC\nMT\nPF\n(b)\nFig. 24.Segmentations of a face-on spiral in the Hubble Ultra Deep Field. (a) Input image – the V606-band of the ﬁeld. ( b) Segmentations of the\nﬁeld, using the parameters that gave the highest median score for each combination of optimisation measure and tool. For more information, see\nFig. 10.\nconsidered. The user must select a set of parameters that pro-\nduces good results for all images in their survey, an impossible\ntask if the tool requires manual tuning on individual images.\n6. Conclusions\nAll the compared tools were capable of a reasonable level\nof object detection, as measured by F-score. However, Pro-\nFound and SExtractor were incapable of detecting the out-\nskirts of objects with any degree of accuracy. NoiseChisel and\nMTObjects were both much more e ﬃcient at ﬁnding these\nfainter regions, but both had other di ﬃculties: the ‘Segment’\ntool used in NoiseChisel divided detected light into apparently\narbitrary regions, whilst MTObjects produced extremely ragged\nedges and had a tendency to over-allocate faint regions to the\nbrightest objects. NoiseChisel also produced the most accurate\nbackground values.\nWe found that there appears to be a trade-oﬀ between speed\nand accurate detection of the outskirts of objects. SExtractor\nwas capable of the highest speeds by a substantial margin, but\nwas unable to accurately detect faint regions. MTObjects and\nNoiseChisel were both able to detect these regions but at the cost\nof processing speed. There may potentially be improvements to\nbe made on both tools by increased parallelisation and optimisa-\ntion of the code.\nA107, page 24 of 30\nC. Haigh et al.: Optimising and comparing source-extraction tools using objective segmentation quality criteria\nA common weakness in the tools was in accurately deblend-\ning nested objects. The MTObjects approach, using tree-based\nconnected morphological ﬁlters (Salembier & Wilkinson 2009)\ndeals relatively well when small, faint objects are nested within\nlarger, brighter ones, but performs poorly when more similar\nobjects overlap. In the latter case, the other methods, which are\ngenerally based on a form of watershed segmentation (Beucher\n1982; Roerdink & Meijster 2000) might give a better result. This\nis a non-trivial problem, which merits further investigation.\nMTObjects was the only tool to ﬁnd stable parameters\nacross multiple optimisations, suggesting that it requires the least\nadjustment for individual images, and may be the best-suited for\nuse in automatic pipelines. Furthermore, in the test on simulated\ndata, it consistently outperformed the other methods, regardless\nof the quality measure used. The likelihood of MTObjects rank-\ning in ﬁrst place out of four in the case of F-score and Area score\nin ten tests is about 10 −6 under the null hypothesis that all tools\nhave equal performance. Despite the modest performance mar-\ngin with respect to the others, the result is statistically signiﬁcant.\nWe ﬁnd that the optimisation criteria must be chosen care-\nfully in order to produce useful parameters. In particular, we ﬁnd\nthat optimising for area alone causes a substantial drop in accu-\nracy for SExtractor and ProFound, whereas combining multiple\ncriteria yields more meaningful results.\nAs discussed in the introduction, the growth of the scale\nof modern surveys means that there is a need for segmenta-\ntion tools which are fast, automatic, and accurate. We ﬁnd that\nof the tools tested, MTObjects is capable of the highest scores\non both area and detection measures, and has the most consis-\ntent parameters, whilst SExtractor obtains the highest speeds,\nbut with much lower accuracy. As noted earlier, a faster imple-\nmentation of MTObjects already exists, and the developers of\nProFound are rewriting parts of their tool to improve its speed.\nIn addition, we present a framework for automated parame-\nter setting and evaluation of astronomical source-detection tools,\nwhich is generic, and can be used with any other quality measure\nor model ground truth. This procedure could be used to analyse\nimprovements to existing tools, as well as to evaluate the capa-\nbilities of future techniques.\nAcknowledgements. We acknowledge ﬁnancial support from the European\nUnion’s Horizon 2020 research and innovation programme under Marie\nSkłodowska-Curie grant agreement No 721463 to the SUNDIAL ITN network.\nNC acknowledges support from the State Research Agency (AEI) of the Spanish\nMinistry of Science and Innovation and the European Regional Development\nFund (FEDER) under the grant with reference PID2019-105602GB-I00, and\nfrom IAC project P/300724, ﬁnanced by the Ministry of Science and Innovation,\nthrough the State Budget and by the Canary Islands Department of Economy,\nKnowledge and Employment, through the Regional Budget of the Autonomous\nCommunity. A V acknowledges ﬁnancial support from the Emil Aaltonen\nFoundation. The Dell R815 Opteron server was obtained through funding from\nthe Netherlands Organisation for Scientiﬁc Research (NWO) under project num-\nber 612.001.110. This research made use of Astropy, ( http://www.astropy.\norg) a community-developed core Python package for Astronomy (Astropy\nCollaboration 2013, 2018). This work was partly done using GNU Astronomy\nUtilities (Gnuastro, ascl.net /1801.009) version 0.7.42-22d2. Gnuastro is a\ngeneric package for astronomical data manipulation and analysis which was\ninitially created and developed for research funded by the Monbukagakusho\n(Japanese government) scholarship and European Research Council (ERC)\nadvanced grant 339659-MUSICOS.\nReferences\nAbbott, T., Abdalla, F., Allam, S., et al. 2018, ApJS, 239, 18\nAdelman-McCarthy, J. K., Agüeros, M. A., Allam, S. S., et al. 2006, ApJS, 162,\n38\nAkhlaghi, M., & Ichikawa, T. 2015, ApJS, 220, 1\nAmiaux, J., Scaramella, R., Mellier, Y ., et al. 2012, in Space Telescopes and\nInstrumentation 2012: Optical, Infrared, and Millimeter Wave, Int. Soc. Opt.\nPhoton., 8442, 84420Z\nAstropy Collaboration (Robitaille, T. P., et al.) 2013, A&A, 558, A33\nAstropy Collaboration (Price-Whelan, A. M., et al.) 2018, AJ, 156, 123\nBaillard, A., Bertin, E., De Lapparent, V ., et al. 2011, A&A, 532, A74\nBeard, S., MacGillivray, H., & Thanisch, P. 1990, MNRAS, 247, 311\nBeckwith, S. V ., Stiavelli, M., Koekemoer, A. M., et al. 2006, AJ, 132, 1729\nBertin, E. 2006, Automatic Astrometric and Photometric Calibration with\nSCAMP (San Francisco: Astronomical Society of the Paciﬁc)\nBertin, E., & Arnouts, S. 1996, A&AS, 117, 393\nBeucher, S. 1982, ICASSP ’82. IEEE International Conference on Acoustics,\nSpeech, and Signal Processing, 7\nBorlaﬀ, A., Trujillo, I., Román, J., et al. 2019, A&A, 621, A133\nBoucaud, A., Heneka, C., Ishida, E. E., et al. 2020, MNRAS, 491, 2481\nBouwens, R. J., Illingworth, G. D., Franx, M., & Ford, H. 2008, ApJ, 686, 230\nBouwens, R., Illingworth, G., Franx, M., et al. 2009, ApJ, 705, 936\nCarlinet, E., & Géraud, T. 2014, IEEE Trans. Image Process., 23, 3885\nFliri, J., & Trujillo, I. 2016, MNRAS, 456, 1359\nGNU Astronomy Utilities 2019, NoiseChisel Optimization, https:\n//www.gnu.org/software/gnuastro/manual/html_node/\nNoiseChisel-optimization.html\nGoodman, A. A., Rosolowsky, E. W., Borkin, M. A., et al. 2009, Nature, 457,\n63\nHoulahan, P., & Scalo, J. 1992, ApJ, 393, 172\nHuang, S., Leauthaud, A., Murata, R., et al. 2018, PASP, 70, S6\nIodice, E., Capaccioli, M., Grado, A., et al. 2016, ApJ, 820, 42\nIodice, E., Spavone, M., Capaccioli, M., et al. 2019, A&A, 623, A1\nIvezi´c, Z., Tyson, J., Abel, B., et al. 2019, ApJ, 873, 111\nJones, D. R., Schonlau, M., & Welch, W. J. 1998, J. Global Optim., 13, 455\nLevine, M. D., & Nazif, A. 1981, An Experimental Rule-based System for\nTesting Low Level Segmentation Strategies (McGill University)\nMasias, M., Freixenet, J., Lladó, X., & Peracaula, M. 2012, MNRAS, 422,\n1674\nMelchior, P., Moolekamp, F., Jerdee, M., et al. 2018, Astron. Comput., 24, 129\nMoschini, U., Meijster, A., & Wilkinson, M. H. F. 2018, IEEE Trans. Pattern\nAnal. Mach. Intell., 40, 513\nOesch, P., Bouwens, R. J., Carollo, C. M., et al. 2009, ApJ, 709, L21\nOuzounis, G. K., & Wilkinson, M. H. 2010, IEEE Trans. Pattern Anal. Mach.\nIntell., 33, 224\nPal, N. R., & Pal, S. K. 1993, Pattern Recognit., 26, 1277\nPratt, N. 1977, Vistas Astron., 21, 1\nProle, D. J., Davies, J. I., Keenan, O. C., & Davies, L. J. 2018, MNRAS, 478,\n667\nReiman, D. M., & Göhre, B. E. 2019, MNRAS, 485, 2617\nRix, H.-W., Barden, M., Beckwith, S. V ., et al. 2004, ApJS, 152, 163\nRobitaille, T., Beaumont, C., McDonald, B., & Rosolowsky, E. 2013,\nAstrodendro, A Python Package to Compute Dendrograms of Astronomical\nData, http://www.dendrograms.org\nRobotham, A., Davies, L., Driver, S., et al. 2018, MNRAS, 476, 3137\nRoerdink, J. B. T. M., & Meijster, A. 2000, Fundam. Inf., 41, 187\nRomán, J., & Trujillo, I. 2017a, MNRAS, 468, 703\nRomán, J., & Trujillo, I. 2017b, MNRAS, 468, 4039\nRomán, J., & Trujillo, I. 2018, Res. Notes Am. Astron. Soc., 2, 144\nRomán, J., Trujillo, I., & Montes, M. 2020, A&A, 644, A42\nRosolowsky, E., Pineda, J., Kauﬀmann, J., & Goodman, A. 2008, ApJ, 679, 1338\nSalembier, P., & Wilkinson, M. H. F. 2009, IEEE Signal Process. Mag., 26,\n136\nSalembier, P., Oliveras, A., & Garrido, L. 1998, IEEE Trans. Image Process., 7,\n555\nSersic, J. L. 1968, Atlas de Galaxias Australes (Cordoba, Argentina:\nObservatorio Astronomico)\nSimet, M., & Mandelbaum, R. 2015, MNRAS, 449, 1259\nTeeninga, P., Moschini, U., Trager, S. C., & Wilkinson, M. H. F. 2013, 11th\nInternational Conference “Pattern Recognition and Image Analysis: New\nInformation Technologies” (PRIA-11-2013), IPSI RAS, 746\nTeeninga, P., Moschini, U., Trager, S. C., & Wilkinson, M. H. F. 2016,\nMathematical Morphology – Theory and Applications, 1, 100\nThe GPyOpt authors 1968, GPyOpt: A Bayesian Optimization Framework in\nPython, http://github.com/SheffieldML/GPyOpt\nVan Dokkum, P. G., Abraham, R., Merritt, A., et al. 2015, ApJ, 798, L45\nVenhola, A. 2019, PhD Thesis, University of Groningen\nVenhola, A., Peletier, R., Laurikainen, E., et al. 2017, A&A, 608, A142\nVenhola, A., Peletier, R., Laurikainen, E., et al. 2018, A&A, 620, A165\nVenhola, A., Peletier, R., Laurikainen, E., et al. 2019, A&A, 625, A143\nWilkinson, M. H. F. 1998, in Digital Image Analysis of Microbes, eds. M. H. F.\nWilkinson, & F. Schut (Chichester, UK: John Wiley and Sons, Ltd), 135\nA107, page 25 of 30\nA&A 645, A107 (2021)\nAppendix A: SExtractor ﬁlters\n1 2 3 4 5 6 7 8 9 10\nTrain ID\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775F-score\nDefault\n1 2 3 4 5 6 7 8 9 10\nTrain ID\nGaussian\n1 2 3 4 5 6 7 8 9 10\nTrain ID\n0.600\n0.625\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775F-score\nPSF\n1 2 3 4 5 6 7 8 9 10\nTrain ID\nTop-hat\n(a)\n1 2 3 4 5 6 7 8 9 10\nTrain ID\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24Area score\nDefault\n1 2 3 4 5 6 7 8 9 10\nTrain ID\nGaussian\n1 2 3 4 5 6 7 8 9 10\nTrain ID\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24Area score\nPSF\n1 2 3 4 5 6 7 8 9 10\nTrain ID\nTop-hat\n(b)\nFig. A.1. Optimised test distributions. The parameters for each tool\nwere optimised for Combined A score on each of the ten images, and\nevaluated on the remaining nine images. Boxes extend from ﬁrst (Q1) to\nthird (Q3) quartiles of the results, with median values marked; whiskers\nextend to the furthest F-score less than 1 .5 ∗(Q3−Q1) from each end\nof the box. (a) F-scores grouped by image used to optimise parameters.\n(b) Area scores grouped by image used to optimise parameters.\nSExtractor uses a ﬁlter to pre-process the input image. A num-\nber of ﬁlters are provided with the tool, but custom ﬁlters\nmay also be used. The SExtractor manual suggests that the\n0.6 0.7 0.8\nF-score\n0.1\n0.2\n0.3\n0.4\n0.5Area-score \nCombined A\nSE+Default\nSE+Gaussian\nSE+PSF\nSE+Top-hat\nNC\nMT\nFig. A.2.F-score vs. Area score. The parameters for each tool were opti-\nmised for Combined A score on each of the ten images, and evaluated\non the remaining nine images.\nsymmetrical PSF of the data is an optimal ﬁlter for detecting\nstars (Bertin 2006), whilst documentation provided with the ﬁl-\nters suggests that Gaussian or top-hat ﬁlters are e ﬀective in\ndetecting extended, low-surface-brightness objects.\nAs the range of valid ﬁlters is inﬁnite, it would not be feasible\nto optimise the ﬁlter in addition to the other parameters. Accord-\ningly, we used the default ﬁlter throughout the main experiments\nof the paper. We subsequently tested a subset of the available ﬁl-\nters to determine whether or not they had a signiﬁcant e ﬀect on\nthe tool’s performance:\n– Default – 3 ×3 pyramidal function (approximating gaussian\nsmoothing).\n– Gaussian – 9 ×9 gaussian PSF with a full width at half max-\nimum of 5 pixels.\n– PSF – 9 ×9 symmetrical window of the PSF of the simulated\nimages.\n– Top-hat – 5 ×5 top-hat PSF.\nWe optimised SExtractor’s parameters for Combined A score\nas described in Sect. 3.2. Figure A.1 shows the distribution of\nF-scores and Area scores for each of the four ﬁlters.\nWe ﬁnd that the di ﬀerent ﬁlters have very little e ﬀect on F-\nscore, but that there is a slightly higher Area score on average\nwhen using the Gaussian ﬁlter as compared to the default. Whilst\nthe Gaussian ﬁlter could therefore be recommended in this situa-\ntion, the choice of ﬁlter has no eﬀect on the overall conclusions.\nAs shown in Fig. 8, both MTObjects and NoiseChisel achieved\nsubstantially higher Area scores of 0.4 −0.6 compared to SEx-\ntractor’s scores of 0.1−0.25. Plotting the four SExtractor ﬁlters\non the same axes as Fig. 8 shows the relative similarity of the\nscores, as in Fig. A.2.\nA107, page 26 of 30\nC. Haigh et al.: Optimising and comparing source-extraction tools using objective segmentation quality criteria\nAppendix B: Optimised parameter tables\nParameter sets marked in bold and with an asterisk produce the\nhighest median test score for their optimisation metric and tool.\nTable B.1.Optimised parameters – Source Extractor.\nMode Image BACK BACK DEBLEND DEBLEND DETECT DETECT\nSIZE FILTERSIZE MINCONT NTHRESH MINAREA THRESH\n0 1 65 2 1.00E −03 33 6 1.45E +00\n0 2 64 2 1.00E −03 34 9 1.35E +00\n0 3 65 2 1.00E −03 35 5 1.59E +00\n0 4 64 3 1.00E −03 33 5 1.48E +00\n0 5 64 2 1.00E −03 31 5 1.51E +00\n0 6 37 5 1.00E −03 23 7 1.46E +00\n0 7 65 2 1.00E −03 33 5 1.78E +00\n0* 8 63 3 1.00E −03 30 5 1.43E +00\n0 9 63 3 1.00E −03 32 6 1.69E +00\n0 10 17 7 1.73E −02 62 8 1.28E +00\n1 1 88 5 1.00E −03 21 8 1.00E −01\n1 2 36 9 1.00E −03 8 17 1.00E −01\n1 3 63 5 9.82E −02 32 4 1.00E −01\n1 4 22 7 9.81E −02 3 6 1.00E −01\n1* 5 107 5 1.00E −03 41 30 1.00E −01\n1 6 24 11 1.00E −01 8 49 1.00E −01\n1 7 111 6 2.12E −02 6 14 1.05E −01\n1 8 108 3 1.00E −03 26 34 1.00E −01\n1 9 35 5 4.40E −02 48 28 1.13E −01\n1 10 80 4 3.67E −02 23 2 1.06E −01\n2 1 84 7 6.96E −02 44 36 6.56E −01\n2 2 29 7 1.82E −02 3 49 5.96E −01\n2 3 105 6 1.00E −03 63 37 6.03E −01\n2 4 110 11 1.00E −03 60 32 7.28E −01\n2 5 98 9 1.00E −03 40 28 7.07E −01\n2 6 33 8 1.00E −03 47 29 6.19E −01\n2 7 124 10 1.00E −03 50 29 1.24E +00\n2* 8 98 7 1.00E −03 29 46 5.70E −01\n2 9 85 4 1.00E −03 44 22 8.94E −01\n2 10 118 3 6.87E −03 41 9 1.07E +00\n3 1 123 7 1.00E −03 60 43 3.93E −01\n3 2 71 5 2.53E −02 23 41 4.91E −01\n3 3 119 2 2.33E −02 23 44 4.94E −01\n3 4 72 11 1.00E −03 55 47 5.23E −01\n3 5 61 7 9.62E −02 53 43 5.35E −01\n3 6 101 3 1.00E −03 38 41 5.30E −01\n3* 7 119 6 1.00E −03 20 49 4.37E −01\n3 8 104 7 1.00E −03 49 37 5.11E −01\n3 9 25 8 1.07E −02 39 25 6.56E −01\n3 10 88 4 1.00E −03 49 16 8.23E −01\nA107, page 27 of 30\nA&A 645, A107 (2021)\nTable B.2.Optimised parameters – Noise Chisel (Noise Chisel).\nMode Image tilesize qthresh snquant detgrow- dthresh erode opening detgrow- meanmed- erode- opening- minsky- noerode-\nquant maxholesize qdi ﬀ ngb ngb frac quant\n0 1 27 4.99E −01 9.99E −01 9.99E −01 0.00E +00 3 4 100 5.00E −03 4 4 8.00E −01 7.00E −01\n0 2 90 4.99E −01 9.71E −01 9.99E −01 6.34E −01 4 3 52 2.00E −02 4 4 7.54E −01 1.00E +00\n0 3 50 4.00E −01 9.27E −01 9.98E −01 8.26E −01 5 4 19 2.00E −02 4 4 6.62E −01 9.73E −01\n0* 4 26 4.99E −01 9.99E −01 9.86E −01 4.03E −01 3 1 59 2.00E −02 4 8 8.00E −01 1.00E +00\n0 5 49 4.99E −01 6.12E −01 9.46E −01 6.87E −01 1 5 77 1.98E −02 8 8 8.00E −01 7.02E −01\n0 6 40 4.99E −01 9.99E −01 9.99E −01 3.19E −01 10 1 86 2.00E −02 4 4 8.00E −01 1.00E +00\n0 7 87 4.33E −01 9.71E −01 9.99E −01 5.20E −01 6 2 14 6.30E −03 8 8 8.00E −01 9.79E −01\n0 8 26 2.00E −01 9.99E −01 9.99E −01 0.00E +00 2 5 48 2.00E −02 4 4 8.00E −01 9.48E −01\n0 9 98 4.99E −01 6.00E −01 9.99E −01 5.39E −01 8 4 88 5.00E −03 8 8 4.00E −01 7.00E −01\n0 10 87 4.99E −01 9.99E −01 8.28E −01 1.67E −01 5 2 26 2.00E −02 4 8 6.03E −01 7.00E −01\n1* 1 75 3.69E −01 9.99E −01 6.00E −01 1.00E +00 6 5 79 5.00E −03 4 8 4.00E −01 7.75E −01\n1 2 40 4.99E −01 9.99E −01 6.00E −01 1.00E +00 1 5 25 2.00E −02 4 8 4.00E −01 1.00E +00\n1 3 27 4.99E −01 6.00E −01 6.00E −01 0.00E +00 1 5 85 1.60E −02 8 8 4.56E −01 1.00E +00\n1 4 86 4.99E −01 9.99E −01 6.00E −01 1.00E +00 6 3 32 2.00E −02 4 8 8.00E −01 1.00E +00\n1 5 34 4.99E −01 6.00E −01 6.00E −01 0.00E +00 5 2 97 8.07E −03 4 4 4.00E −01 1.00E +00\n1 6 28 2.00E −01 6.00E −01 6.19E −01 6.86E −01 2 1 94 1.97E −02 4 8 4.00E −01 1.00E +00\n1 7 57 2.00E −01 9.99E −01 6.00E −01 0.00E +00 10 5 58 5.00E −03 8 4 4.00E −01 7.00E −01\n1 8 33 4.99E −01 6.15E −01 6.15E −01 1.69E-13 5 1 65 2.00E −02 8 4 8.00E −01 1.00E +00\n1 9 96 3.64E −01 9.28E −01 6.03E −01 7.15E −01 1 4 63 1.03E −02 8 8 6.87E −01 9.27E −01\n1 10 43 3.74E −01 8.28E −01 6.54E −01 8.98E −01 1 5 16 1.14E −02 4 8 4.98E −01 1.00E +00\n2* 1 68 3.26E −01 6.00E −01 6.11E −01 6.85E −01 8 5 68 2.00E −02 4 8 5.16E −01 8.05E −01\n2 2 30 4.99E −01 6.00E −01 6.00E −01 6.36E −01 2 2 67 1.06E −02 8 8 8.00E −01 1.00E +00\n2 3 27 3.46E −01 9.99E −01 6.09E −01 0.00E +00 9 2 55 1.98E −02 4 4 4.94E −01 8.84E −01\n2 4 28 4.99E −01 6.00E −01 6.16E −01 0.00E +00 4 3 80 2.00E −02 8 4 4.00E −01 9.03E −01\n2 5 89 2.00E −01 6.58E −01 6.18E −01 7.90E −01 5 2 8 2.00E −02 8 8 8.00E −01 7.09E −01\n2 6 29 2.91E −01 9.99E −01 6.10E −01 1.19E −02 3 1 68 2.00E −02 4 4 5.01E −01 1.00E +00\n2 7 30 3.50E −01 6.00E −01 6.00E −01 0.00E +00 8 5 28 7.24E −03 8 8 6.97E −01 7.00E −01\n2 8 26 2.00E −01 6.00E −01 6.00E −01 2.73E −02 9 5 23 2.00E −02 8 8 5.20E −01 7.00E −01\n2 9 45 2.00E −01 8.07E −01 7.21E −01 7.51E −01 7 2 76 8.42E −03 8 8 7.46E −01 7.88E −01\n2 10 82 4.42E −01 6.87E −01 6.82E −01 5.27E −01 6 2 38 2.00E −02 4 8 7.44E −01 9.14E −01\n3 1 27 2.04E −01 9.08E −01 6.03E −01 0.00E +00 4 4 23 2.00E −02 8 4 7.20E −01 7.68E −01\n3 2 64 4.99E −01 6.00E −01 6.00E −01 7.87E −01 4 2 41 7.97E −03 8 8 4.00E −01 7.00E −01\n3 3 22 2.00E −01 9.98E −01 6.02E −01 1.57E −03 1 5 48 2.00E −02 8 8 4.02E −01 7.01E −01\n3 4 20 2.00E −01 9.99E −01 6.00E −01 0.00E +00 1 5 76 5.00E −03 8 8 4.00E −01 7.00E −01\n3 5 41 4.99E −01 9.50E −01 6.00E −01 2.16E −01 8 1 90 5.00E −03 4 8 4.00E −01 1.00E +00\n3 6 82 4.67E −01 9.96E −01 6.37E −01 1.07E −02 8 1 38 1.09E −02 8 4 8.00E −01 9.56E −01\n3 7 63 4.82E −01 6.93E −01 6.38E −01 5.90E −01 7 2 85 1.89E −02 8 8 4.80E −01 8.70E −01\n3 8 31 4.99E −01 7.88E −01 6.29E −01 5.51E −01 9 5 29 2.00E −02 4 4 5.33E −01 9.86E −01\n3* 9 71 2.00E −01 9.99E −01 6.00E −01 0.00E +00 1 5 11 2.00E −02 4 8 4.00E −01 7.00E −01\n3 10 100 3.44E −01 9.99E −01 6.00E −01 0.00E +00 9 5 5 2.00E −02 4 4 4.00E −01 7.28E −01\nA107, page 28 of 30\nC. Haigh et al.: Optimising and comparing source-extraction tools using objective segmentation quality criteria\nTable B.3.Optimised parameters – Noise Chisel (Segment).\nMode Image tilesize snquant gthresh snminarea minriver- objbordersn minskyfrac\nlength\n0 1 72 9.99E −01 1.00E +00 25 23 1.28E +01 4.00E −01\n0 2 90 9.99E −01 1.00E +00 18 12 1.86E +01 5.75E −01\n0 3 93 9.94E −01 8.65E −01 23 20 3.18E +01 6.67E −01\n0* 4 60 9.99E −01 6.69E −01 25 9 2.56E +01 7.52E −01\n0 5 20 9.99E −01 2.44E −01 20 40 2.00E +01 8.00E −01\n0 6 20 9.99E −01 1.00E +00 19 5 1.12E +01 8.00E −01\n0 7 29 9.99E −01 1.00E +00 21 37 1.49E +01 7.28E −01\n0 8 72 9.99E −01 1.00E +00 20 40 3.16E +01 8.00E −01\n0 9 45 9.99E −01 0.00E +00 20 14 2.26E +01 8.00E −01\n0 10 91 9.99E −01 3.77E −01 25 25 1.51E +01 4.00E −01\n1* 1 65 9.99E −01 5.41E −01 10 34 5.00E −01 4.00E −01\n1 2 85 9.99E −01 1.00E +00 25 35 1.73E +01 8.00E −01\n1 3 35 9.99E −01 0.00E +00 15 21 1.88E +01 4.00E −01\n1 4 65 9.99E −01 1.00E +00 23 8 1.80E +01 7.43E −01\n1 5 32 9.99E −01 6.34E −01 15 14 1.34E +00 8.00E −01\n1 6 41 9.99E −01 4.41E −01 21 36 1.15E +01 5.99E −01\n1 7 52 9.99E −01 0.00E +00 25 40 1.81E +01 4.00E −01\n1 8 46 9.99E −01 2.85E −02 25 40 2.12E +01 8.00E −01\n1 9 82 9.97E −01 5.72E −01 20 23 3.45E +01 7.81E −01\n1 10 61 9.99E −01 2.14E −01 17 31 8.88E +00 5.64E −01\n2* 1 25 9.99E −01 3.28E −01 25 24 3.11E +01 4.90E −01\n2 2 45 9.99E −01 4.89E −01 24 21 1.77E +01 4.00E −01\n2 3 80 9.99E −01 6.60E −03 22 30 5.88E +00 4.09E −01\n2 4 81 9.99E −01 0.00E +00 22 8 1.03E +01 8.00E −01\n2 5 31 9.99E −01 2.48E −01 22 16 7.32E +00 4.00E −01\n2 6 74 9.99E −01 9.37E −03 25 22 6.71E +00 4.10E −01\n2 7 38 9.99E −01 2.11E −01 20 31 1.29E +01 4.00E −01\n2 8 20 9.99E −01 0.00E +00 25 33 7.35E +00 4.00E −01\n2 9 90 9.99E −01 8.11E −01 14 27 3.96E +01 4.00E −01\n2 10 48 9.99E −01 6.99E −01 18 15 2.60E +01 6.55E −01\n3 1 72 9.99E −01 5.36E-16 24 13 1.29E +01 4.03E −01\n3 2 94 9.99E −01 8.28E −01 25 30 2.42E +01 4.00E −01\n3 3 31 9.97E −01 3.17E −02 25 5 2.21E +01 4.02E −01\n3 4 20 9.99E −01 1.00E +00 25 11 1.63E +01 4.00E −01\n3 5 32 9.99E −01 0.00E +00 25 25 1.81E +01 8.00E −01\n3 6 20 9.99E −01 3.93E −01 22 26 9.46E +00 6.79E −01\n3 7 78 9.99E −01 5.50E −02 24 21 2.89E +01 4.52E −01\n3 8 47 9.99E −01 6.88E −01 16 16 3.86E +01 8.00E −01\n3* 9 74 9.99E −01 0.00E +00 25 11 2.38E +01 8.00E −01\n3 10 32 9.99E −01 2.18E −01 23 18 1.11E +01 5.88E −01\nA107, page 29 of 30\nA&A 645, A107 (2021)\nTable B.4.Optimised parameters – MTObjects.\nMode Image Move_factor Min_dist\n0 1 4.96E −02 1.49E −01\n0 2 6.14E −02 1.15E −01\n0 3 5.73E −02 1.57E −01\n0 4 8.24E −03 1.32E −01\n0 5 1.21E −01 1.45E −01\n0 6 0 1.50E −01\n0* 7 0 1.13E −01\n0 8 9.34E −02 1.55E −01\n0 9 1.13E −01 1.33E −01\n0 10 3.07E −02 1.17E −01\n1* 1 0 0\n1 2 0 0\n1 3 0 0\n1 4 0 0\n1 5 0 0\n1 6 0 0\n1 7 0 0\n1 8 0 0\n1 9 0 0\n1 10 0 0\nTable B.4.continued.\nMode Image Move_factor Min_dist\n2* 1 0 0\n2 2 0 0\n2 3 0 0\n2 4 0 0\n2 5 0 0\n2 6 0 0\n2 7 0 0\n2 8 0 0\n2 9 0 0\n2 10 0 0\n3* 1 0 0\n3 2 0 0\n3 3 0 0\n3 4 0 0\n3 5 0 0\n3 6 0 0\n3 7 0 0\n3 8 9.36E −03 0\n3 9 0 0\n3 10 0 0\nTable B.5.Optimised parameters – ProFound.\nMode Image Skycut Tolerance Ext Sigma Pixcut Size Iters Threshold\n0 1 7.22E −01 3.71E +00 1.92E +00 2.22E +00 4 5 7 7.50E −01\n0 2 9.34E −01 2.06E +00 2.29E +00 1.35E +00 10 7 2 8.26E −01\n0 3 1.00E +00 1.00E +00 7.03E +00 1.19E +00 7 7 4 7.50E −01\n0 4 8.23E −01 1.58E +00 8.03E +00 2.23E +00 6 7 3 1.20E +00\n0 5 1.22E +00 3.52E +00 3.89E +00 9.17E −01 6 5 6 1.78E +00\n0 6 6.54E −01 1.00E +00 3.78E +00 3.00E +00 15 7 9 7.50E −01\n0 7 9.34E −01 1.00E +00 4.55E +00 1.43E +00 16 5 0 1.96E +00\n0* 8 5.56E −01 2.03E +00 3.28E +00 2.11E +00 15 7 6 9.49E −01\n0 9 1.79E +00 3.90E +00 3.29E +00 8.76E −01 4 5 7 1.35E +00\n0 10 6.25E −01 2.93E +00 4.33E +00 1.36E +00 14 9 8 2.00E +00\n1 1 5.26E −01 5.29E +00 2.22E +00 1.98E +00 7 9 7 8.68E −01\n1 2 2.38E −01 2.62E +00 9.01E +00 2.21E +00 11 7 6 1.12E +00\n1 3 7.65E −01 5.71E +00 2.70E +00 1.45E +00 5 5 8 8.42E −01\n1 4 6.13E −01 4.44E +00 4.77E +00 2.83E +00 5 7 7 8.00E −01\n1 5 1.57E −01 3.72E +00 4.23E +00 2.74E +00 10 9 6 1.24E +00\n1 6 2.77E −01 2.49E +00 8.57E +00 2.26E +00 15 7 9 8.31E −01\n1 7 6.23E −01 3.68E +00 5.18E +00 1.01E +00 1 5 4 1.04E +00\n1* 8 6.45E −01 6.00E +00 8.97E +00 1.13E +00 14 9 9 9.10E −01\n1 9 2.82E −01 4.48E +00 6.28E +00 2.07E +00 3 7 4 7.50E −01\n1 10 1.33E +00 3.23E +00 3.63E +00 1.82E +00 9 7 9 8.23E −01\nA107, page 30 of 30",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.751095175743103
    },
    {
      "name": "Ground truth",
      "score": 0.6886878609657288
    },
    {
      "name": "Segmentation",
      "score": 0.6286356449127197
    },
    {
      "name": "Data mining",
      "score": 0.5310630798339844
    },
    {
      "name": "Software",
      "score": 0.5228067636489868
    },
    {
      "name": "Precision and recall",
      "score": 0.5021891593933105
    },
    {
      "name": "Task (project management)",
      "score": 0.4923053979873657
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47473153471946716
    },
    {
      "name": "Completeness (order theory)",
      "score": 0.4424769878387451
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.43849095702171326
    },
    {
      "name": "Measure (data warehouse)",
      "score": 0.42415690422058105
    },
    {
      "name": "Focus (optics)",
      "score": 0.4113161563873291
    },
    {
      "name": "Machine learning",
      "score": 0.37130939960479736
    },
    {
      "name": "Mathematics",
      "score": 0.09634479880332947
    },
    {
      "name": "Engineering",
      "score": 0.08091449737548828
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}