{
  "title": "Lattice Transformer for Speech Translation",
  "url": "https://openalex.org/W2951289103",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100415705",
      "name": "Pei Zhang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5019715118",
      "name": "Boxing Chen",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5016546383",
      "name": "Niyu Ge",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5055225377",
      "name": "Kai Fan",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963201387",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W2798858969",
    "https://openalex.org/W2513832136",
    "https://openalex.org/W2605202026",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2794365787",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2884519246",
    "https://openalex.org/W2125838338",
    "https://openalex.org/W2950874967",
    "https://openalex.org/W2962904552",
    "https://openalex.org/W2963206679",
    "https://openalex.org/W2902169904",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2963831883",
    "https://openalex.org/W3012492057",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2798702047",
    "https://openalex.org/W2955541912",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963979492"
  ],
  "abstract": "Recent advances in sequence modeling have highlighted the strengths of the transformer architecture, especially in achieving state-of-the-art machine translation results. However, depending on the up-stream systems, e.g., speech recognition, or word segmentation, the input to translation system can vary greatly. The goal of this work is to extend the attention mechanism of the transformer to naturally consume the lattice in addition to the traditional sequential input. We first propose a general lattice transformer for speech translation where the input is the output of the automatic speech recognition (ASR) which contains multiple paths and posterior scores. To leverage the extra information from the lattice structure, we develop a novel controllable lattice attention mechanism to obtain latent representations. On the LDC Spanish-English speech translation corpus, our experiments show that lattice transformer generalizes significantly better and outperforms both a transformer baseline and a lattice LSTM. Additionally, we validate our approach on the WMT 2017 Chinese-English translation task with lattice inputs from different BPE segmentations. In this task, we also observe the improvements over strong baselines.",
  "full_text": "Lattice Transformer for Speech Translation\nPei Zhang∗, Boxing Chen∗, Niyu Ge∗, Kai Fan∗†\nAlibaba Group Inc.\n{xiaoyi.zp,boxing.cbx,niyu.ge,k.fan}@alibaba-inc.com\nAbstract\nRecent advances in sequence modeling have\nhighlighted the strengths of the transformer ar-\nchitecture, especially in achieving state-of-the-\nart machine translation results. However, de-\npending on the up-stream systems, e.g., speech\nrecognition, or word segmentation, the input\nto translation system can vary greatly. The\ngoal of this work is to extend the attention\nmechanism of the transformer to naturally\nconsume the lattice in addition to the tradi-\ntional sequential input. We ﬁrst propose a\ngeneral lattice transformer for speech transla-\ntion where the input is the output of the au-\ntomatic speech recognition (ASR) which con-\ntains multiple paths and posterior scores. To\nleverage the extra information from the lat-\ntice structure, we develop a novel control-\nlable lattice attention mechanism to obtain la-\ntent representations. On the LDC Spanish-\nEnglish speech translation corpus, our exper-\niments show that lattice transformer general-\nizes signiﬁcantly better and outperforms both\na transformer baseline and a lattice LSTM.\nAdditionally, we validate our approach on the\nWMT 2017 Chinese-English translation task\nwith lattice inputs from different BPE segmen-\ntations. In this task, we also observe the im-\nprovements over strong baselines.\n1 Introduction\nTransformer based encoder-decoder framework\n(Vaswani et al., 2017) for Neural Machine Trans-\nlation (NMT) has currently become the state-of-\nthe-art in many translation tasks, signiﬁcantly im-\nproving translation quality in text (Bojar et al.,\n2018; Fan et al., 2018) as well as in speech (Jan\net al., 2018). Most NMT systems fall into the cat-\negory of Sequence-to-Sequence (Seq2Seq) model\n(Sutskever et al., 2014), because both the input and\n∗indicates equal contribution.\n†corresponding author.\n0: <s>1: iban\n2: ivan6: esquinas\n5: así\n4: esquinas\n3: espinas\n7: así8:entonces9: </s>0.870.871\n0.130.131\n0.130.111\n0.870.760.87\n1 0.110.11\n1 0.130.131 0.890.89111 11 11 11\nx0 x1\nx2 x6\nx5\nx4\nx3\nx7 x8 x9\n0: <s>1: iban2: esquinas3: así4:entonces5: </s>\nx0 x1 x2 x3 x4 x5\nAttention in Standard Transformer Encoder\nAttention in Lattice Transformer Encoder\nFigure 1: Illustration of our proposed attention mech-\nanism (best viewed in color). Our attention depends\non the tokens of common paths and forward (blue) /\nmarginal (grey) / backward (orange) probability scores.\noutput consist of sequential tokens. Therefore, in\nmost neural speech translation, such as that of (Bo-\njar et al., 2018), the input to the translation sys-\ntem is usually the 1-best hypothesis from the ASR\ninstead of the word lattice output with its corre-\nsponding probability scores.\nHow to consume word lattice rather than se-\nquential input has been substantially researched in\nseveral natural language processing (NLP) tasks,\nsuch as language modeling (Buckman and Neu-\nbig, 2018), Chinese Named Entity Recognition\n(NER) (Zhang and Yang, 2018), and NMT (Su\net al., 2017). Additionally, some pioneering works\n(Adams et al., 2016; Sperber et al., 2017; Osamura\net al., 2018) demonstrated the potential improve-\nments in speech translation by leveraging the ad-\nditional information and uncertainty of the packed\nlattice structure produced by ASR acoustic model.\nEfforts have since continued to push the\nboundaries of long short-term memory (LSTM)\n(Hochreiter and Schmidhuber, 1997) models.\nMore precisely, most previous works are in line\nwith the existing method Tree-LSTMs (Tai et al.,\narXiv:1906.05551v1  [cs.CL]  13 Jun 2019\n2015), adapting to task-speciﬁc variant Lattice-\nLSTMs that can successfully handle lattices and\nrobustly establish better performance than the\noriginal models. However, the inherently sequen-\ntial nature still remains in Lattice-LSTMs due to\nthe topological representation of the lattice graph,\nprecluding long-path dependencies (Khandelwal\net al., 2018) and parallelization within training\nexamples that are the fundamental constraint of\nLSTMs.\nIn this work, we introduce a generalization of\nthe standard transformer architecture to accept\nlattice-structured network topologies. The stan-\ndard transformer is a transduction model relying\nentirely on attention modules to compute latent\nrepresentations, e.g., the self-attention requires to\ncalculate the intra-attention of every two tokens\nfor each sequence example. Latest works such as\n(Yu et al., 2018; Devlin et al., 2018; Lample et al.,\n2018; Su et al., 2018) empirically ﬁnd that trans-\nformer can outperform LSTMs by a large mar-\ngin, and the success is mainly attributed to self-\nattention. In our lattice transformer, we propose\na lattice relative positional attention mechanism\nthat can incorporate the probability scores of ASR\nword lattices. The major difference with the self-\nattention in transformer encoder is illustrated in\nFigure 1.\nWe ﬁrst borrow the idea from the relative po-\nsitional embedding (Shaw et al., 2018) to maxi-\nmally encode the information of the lattice graph\ninto its corresponding relative positional matrix.\nThis design essentially does not allow a token to\npay attention to any token that has not appeared\nin a shared path. Secondly, the attention weights\ndepend not only on the query and key represen-\ntations in the standard attention module, but also\non the marginal / forward / backward probabil-\nity scores (Rabiner, 1989; Post et al., 2013) de-\nrived from the upstream systems (such as ASR).\nInstead of 1-best hypothesis alone (though it is\nbased on forward scores), the additional probabil-\nity scores have rich information about the distri-\nbution of each path (Sperber et al., 2017). It is in\nprinciple possible to use them, for example in at-\ntention weights reweighing, to increase the uncer-\ntainty of the attention for other alternative tokens.\nOur lattice attention is controllable and ﬂexi-\nble enough for the utilization of each score. The\nlattice transformer can readily consume the lat-\ntice input alone if the scores are unavailable. A\ncommon application is found in the Chinese NER\ntask, in which a Chinese sentence could possi-\nbly have multiple word segmentation possibilities\n(Zhang and Yang, 2018). Furthermore, different\nBPE operations (Sennrich et al., 2016) or proba-\nbilistic subwords (Kudo, 2018) can also bring sim-\nilar uncertainty to subword candidates and form a\ncompact lattice structure.\nIn summary, this paper makes the following\nmain contributions. i) To our best knowledge, we\nare the ﬁrst to propose a novel attention mecha-\nnism that consumes a word lattice and the proba-\nbility scores from the ASR system. ii) The pro-\nposed approach is naturally applied to both the\nencoder self-attention and encoder-decoder atten-\ntion. iii) Another appealing feature is that the lat-\ntice transformer can be reduced to standard lattice-\nto-sequence model without probability scores, ﬁt-\nting the text translation task. iv) Extensive ex-\nperiments on speech translation datasets demon-\nstrate that our method outperforms the previous\ntransformer and Lattice-LSTMs. The experiment\non the WMT 2017 Chinese-English translation\ntask shows the reduced model can improve many\nstrong baselines such as the transformer.\n2 Background\nWe ﬁrst brieﬂy describe the standard transformer\nthat our model is built upon, and then elaborate on\nour proposed approach in the next section.\n2.1 Transformer\nThe Transformer follows the typical encoder-\ndecoder architecture using stacked self-attention,\npoint-wise fully connected layers, and the\nencoder-decoder attention layers. Each layer is\nin principle wrapped by a residual connection (He\net al., 2016) and a postprocessing layer normal-\nization (Ba et al., 2016). Although in principle,\nit is not necessary to mask for self-attentions in\nthe encoder, in practical implementation it is re-\nquired to mask the padding positions. However,\nself-attention in the decoder only allows positions\nup to the current one to be attended to, preventing\ninformation ﬂow from the left and preserving the\nauto-regressive property. The illegal connections\nwill be masked out by setting as −109 before the\nsoftmax operation.\n2.2 Dot-product Attention\nSuppose that for each attention layer in the trans-\nformer encoder and decoder, we have two input\nsequences that can be presented as two matrices\nX ∈ Rn×d and Y ∈ Rm×d, where n, mare\nthe lengths of source and target sentences respec-\ntively, and d is the hidden size (usually equal to\nembedding size), the output is h new sequences\nZi ∈Rn×d/h or ∈Rm×d/h, where h is the num-\nber of heads in attention. In general, the result of\nmulti-head attention is calculated according to the\nfollowing procedure.\nQ = XW Q or Y WQ or Y WQ (1)\nK = XW K or Y WK or XW K (2)\nV = XW V or Y WV or XW V (3)\nZi = Softmax\n(\nQK⊤\n√\nd/h\n+ IdM\n)\nV (4)\nZ = Concat(Z1, ..., Zh)WO (5)\nwhere the matrices WQ, WK, WV ∈Rd×d/h and\nWO ∈ Rd×d represent the learnable projection\nparameters, and the masking matrix M ∈Rm×m\nis an upper triangular matrix with zero on the di-\nagonal and non-zero (−109) everywhere else.\nNote that i) the three columns in the right-side\nof Eq (1,2,3) are used to compute the encoder\nself-attention, the decoder self-attention, and the\nencoder-decoder attention respectively,ii) Id is the\nindicator function that returns 1 if it computes de-\ncoder self-attention and 0 otherwise, iii) the pro-\njection parameters are unique per layer and head,\niv) the Softmax in Eq (4) means a row-wise ma-\ntrix operation, computing the attention weights by\nscaled dot product and resulting in a simplex ∆n\nfor each row.\n3 Lattice Transformer\nAs motivated in the introduction, our goal is to en-\nhance the standard transformer architecture, which\nis limited to sequential inputs, to consume lattice\ninputs with additional information from the up-\nstream ASR systems.\n3.1 Lattice Representation\nWithout loss of generality, we assume a word lat-\ntice from ASR system to be a directed, connected\nand acyclic graph following a topological ordering\nsuch that a child node comes after its parent nodes.\nx0 x1\nx2 x6\nx5\nx4\nx3\nx7\nx8 x9\nx0x1x2x3x4x5x6x7x8x9x00112232345x1-10-inf112X234x2-1X0-inf-infX1234x3-2-1-inf0-inf1-inf-inf23x4-2-1-inf-inf0-inf-inf123x5-3-2-inf-1-inf0-inf-inf12x6-2-inf-1-infinf-inf0123x7-3-2-2-inf-1-inf-1012x8-4-3-3-2-2-1-2-101x9-5-4-4-3-3-2-3-2-10\n0\n2\n-1\n-1-3\n1\n-2\n-2 -inf -inf\nFigure 2: An example of the lattice relative position\nmatrix, where “-inf” in the matrix is a special num-\nber denoting that no relative position exists between the\ncorresponding two tokens.\nWe add two special tokens to each path of the lat-\ntice, which represent the start of sentence and the\nend of sentence (e.g., Figure 1), so that the graph\nhas a single source node and a single end node,\nwhere each node is assigned a token.\nGiven the deﬁnition and property described\nabove, we propose to use a relative positional lat-\ntice matrixL ∈Nn×n to encode the graph infor-\nmation, where n is number of nodes in the graph.\nFor any two nodes i, jin the lattice graph, the ma-\ntrix entry Lij is the minimum relative distance be-\ntween them. In other words, if the nodes i, jshare\nat least one path, then we have\nLij = min\np∈common paths for i, j\nLp\ni0 −Lp\nj0, (6)\nwhere Lp\n·0 is the distance to the source node in path\np. If no common path exists for two nodes, we\ndenote the relative distance as −∞(−109 in prac-\ntice) for subsequent masking in the lattice atten-\ntion. The reason for choosing the “min” in Eq (6)\nis that in our dataset we ﬁnd about 70% of Lijs\ncomputed by “min” and “max” are identical, and\nabout 20% entries just differ by 1. Empirically, our\nexperiments also show no signiﬁcant difference in\nthe performance of either one.\nAn illustration to compute the lattice matrix for\nthe example in the introduction is shown in Fig-\nure 2. Since we can deterministically reconstruct\nthe lattice graph from those matrix elements that\nare equal to 1, it indicates the relation information\nbetween the parent and child nodes.\n3.2 Controllable Lattice Attention\nBesides the lattice graph representation, the pos-\nterior probability scores can be simultaneously\nproduced from the acoustic model and language\nmodel in most ASR systems. We deliberately de-\nsign a controllable lattice attention mechanism to\nincorporate such information to make the attention\nencode more uncertainties.\nIn general, we denote the posterior probability\nof a node i as the forward scorefi, where the sum-\nmation of the forward scores for its child nodes is\n1. Following the recursion rule in (Rabiner, 1989),\nwe can further derive another two useful probabil-\nities, the marginal score mi = fi\n∑\nj∈Pa(i) mj and\nthe backward score bi = mi/ ∑\nk∈Ch(i) mk, where\nPa(i) or Ch(i) denotes node i’s predecessor or suc-\ncessor set, respectively. Intuitively, the marginal\nscore measures the global importance of the cur-\nrent token compared with its substitutes given all\npredecessors; the backward score is analogous to\nthe forward score, which is only locally associ-\nated with the importance of different parents to\ntheir children, where the summation of its parent\nnodes’ scores is 1. Therefore, our controllable at-\ntention aims to employ marginal scores and for-\nward / backward scores.\n3.2.1 Lattice Embedding\nWe ﬁrst construct the latent representations of the\nrelative positional lattice matrix L. The matrix L\ncan be straightforwardly decomposed into two ma-\ntrices: one is the mask LM with only 0 and −∞\nvalues, and the other is the matrix with regular val-\nues i.e., LR = L −LM . Given a 2D embedding\nmatrix WL, the embedded vector of LR\nij can be\nwritten as WL[LR\nij, :] with the NumPy style index-\ning. In order to prevent the the lattice embedding\nfrom dynamically changing, we have to clip ev-\nery entry of LR with a positive integer c1, such\nthat WL ∈R(2c+1)×d/h has a ﬁxed dimensional-\nity and becomes learnable parameters.\n3.2.2 Attention with Probability Scores\nOur proposed controllable lattice attention is de-\npicted in the left panel of Figure 3. It shows the\ncomputational graph with detailed network mod-\nules. More concretely, we ﬁrst denote the lattice\nembedding for LR as a 3D array E ∈Rn×n×d/h.\nThen, the attention weights adapted from tradi-\ntional transformer are integrated with marginal\n1clip(l, c) = max(−c, min(l, c))\nscores that capture the distribution of each path in\nthe lattice. The logits in Eq (4) will become the ad-\ndition of three individual terms (if we temporarily\nomit the mask matrix),\nQK⊤+ einsum(’ik,ijk->ij’, Q, E)√\nd/h\n+ wmm . (7)\nThe original QK⊤will remain since the word em-\nbeddings have the majority of signiﬁcant seman-\ntic information. The difﬁcult part in Eq (7) is the\nnew dot product term involving the lattice embed-\nding by einsum2 operation, where einsum is a\nmulti-dimensional linear algebraic array operation\nin Einstein summation convention. In our case, it\ntries to sum out the dimension of the hidden size,\nresulting in a new 2D array ∈Rn×n, which is fur-\nther be scaled by 1√\nd/h as well. In addition, we ag-\ngregate the scaled marginal score vector m ∈Rn\ntogether to obtain the logits. With the new param-\neterization, each term has an intuitive meaning:\nterm i) represents semantic information, term ii)\ngoverns the lattice-dependent positional relation,\nterm iii) encodes the global uncertainty of the ASR\noutput.\nThe attention logits associated with the for-\nward or backward scores are much different from\nmarginal scores, since they govern the local in-\nformation between the parent and child nodes.\nThey are represented as a matrix rather than a vec-\ntor, where the matrix has only non-zero values if\nnodes i, jhave a parent-child relation in the lattice\ngraph. First, an upper or lower triangular mask\nmatrix is used to enforce every token’s attention to\nthe forward scores of its successors or the back-\nward scores of its predecessors. It seems counter-\nintuitive but the reason is that the summation of\nthe forward scores for each token’s child nodes is\n1. So is the backward scores of each token’s par-\nent nodes. Secondly, before applying the softmax\noperation, the lattice mask matrix LM is added\nto each logits to prevent attention from crossing\npaths. Eventually, the ﬁnal attention vector used to\nmultiply the value representation V is a weighed\naveraging of the three proposed attention vectors\n2This op is available in NumPy, TensorFlow, or PyTorch.\nIn our example, Q and E are 2D and 3D arrays, and the re-\nsult of this op is a 2D array, with the element in ith row, jth\ncolumn is ∑\nk QikEijk .\nMatMulQK MatMulQLattice Embedding\nAddLattice MaskSoftMax\nMarginal Scores\nScale\nAdd\nForward / Backward Scores\nMatMul\nVUpper triangularLower triangular\nLattice MaskLattice Mask\nSoftMaxSoftMax\n!\" !# !$\nScale\nScale\nInput Embedding\nLattice Word Inputs\nOutput Embedding\nOutputsLattice Matrix\nLattice Embedding\nControllable Lattice MaskedMulti-Head AttentionScale\nmarginal scores\nAdd & NormFeed FrowardAdd & Norm\nF/B scores\nSplit & Mask\nMulti-Head Attention\nMasked Multi-Head Attention\nAdd & Norm\nPositional Encoding\nAdd & NormFeed FrowardAdd & NormLinearSoftmaxOutput Probabilities\nScale\nN x\nN x\nFigure 3: Left panel: the controllable lattice attention, wheresm, sf , sb are learnable scalars andsm +sf +sb = 1.\nRight panel: the overall model architecture of lattice transformer.\nA·with different probability scores s·,\nAfinal =smAm + sf Af + sbAb, (8)\ns.t. sm + sf + sb = 1 .\nIn summary, the overall architecture of lattice\ntransformer is illustrated in the right of Figure 3.\n3.2.3 Discussion\nA critical point for the lattice transformer is\nwhether the model can generalize to other com-\nmon lattice-based inputs. More speciﬁcally, how\ndoes the model apply to the lattice input without\nprobability scores? And to what extent can we\ntrain the lattice model on a regular sequential in-\nput? If probability scores are unavailable, we can\nuse the lattice graph representations alone by set-\nting the scalar wm = 0 in Eq (7) and sf = sb = 0,\nsm = 1 in Eq (8) as non-trainable constants.\nWe validate this viewpoint on the Chinese-English\ntranslation task, where the Chinese input is a pure\nlattice structure derived from different tokeniza-\ntions. As to sequential inputs, it is just a special\ncase of the lattice graph with only one path.\nAn interesting point to mention is that our\nencoder-decoder attention also takes the key and\nvalue representations from the lattice input and ag-\ngregates the marginal scores, though the sequen-\ntial target forbids us to use lattice self-attention in\nthe decoder. However, we can still visualize how\nthe sequential target attends to the lattice input.\nA practical point for the lattice transformer is\nwhether the training or inference time for such\na seemingly complicated architecture is accept-\nable. In our implementation, we ﬁrst preprocess\nthe lattice input to obtain the position matrix for\nthe whole dataset, thus the one-time preprocess-\ning will bring almost no over-head to our training\nand inference. In addition, the extra enisum oper-\nation in controllable lattice attention is the most\ntime-consuming computation, but remaining the\nsame computational complexity as QK⊤. Em-\npirically, in the ASR experiments, we found that\nthe training and inference of the most complicated\nlattice transformer (last row in the ablation study)\ntake about 100% and 40% more time than standard\ntransformer; in the text translation task, our algo-\nrithm takes about 30% and 20% more time during\ntraining and inference.\n4 Experiments\nWe mainly validate our model in two scenarios,\nspeech translation with word lattices and poste-\nrior scores, and Chinese to English text translation\nwith different BPE operations on the source side.\n4.1 Speech Translation\nFor the speech translation experiment, we use\nthe Fisher and Callhome Spanish-English Speech\nTranslation Corpus from LDC (Post et al., 2013),\nwhich is produced from telephone conversations.\nOur baseline models are the vanilla Transformer\nwith relative positional embeddings (Vaswani\net al., 2017; Shaw et al., 2018), and Lattice-\nLSTMs (Sperber et al., 2017).\n4.1.1 Datasets\nThe Fisher corpus includes the contents between\nstrangers, while the Callhome corpus is primarily\nbetween friends and family members. The num-\nbers of sentence pairs of the two datasets are re-\nspectively 138,819 and 15,080. The source side\nSpanish corpus consists of four data types: ref-\nerence (human transcripts), oracle of ASR lat-\ntices (the optimal path with the lowest word error\nrate (WER)), ASR 1-best hypothesis, and ASR\nlattice. For the data processing, we make case-\ninsensitive tokenization with the standard moses 3\ntokenizer for both the source and target transcripts,\nand remove the punctuation in source sides. The\nsentences of the other three types have been al-\nready been lowercased and punctuation-removed.\nTo keep consistent with the lattices, we add a to-\nken “<s>” at the beginning for all cases.\nSetting Description\nR baseline, trained with human transcripts only\nR+1 ﬁne-tuned on 1-best hypothesis\nR+Lﬁne-tuned on lattices without probability scores\nR+L+Sﬁne-tuned on lattices with probability scores\nTable 1: 4 systems for comparison\n4.1.2 Training and Cross-Evaluation\n4 systems in Table 1 are trained for both Lattice-\nLSTMs and Lattice Transformer. For fair and\ncomprehensive comparison, we also evaluate all\nalgorithms on the inputs of four types. We ini-\ntially train the baseline of our lattice transformer\nwith the human transcripts on Fisher/Train data\nalone, which is equivalent to the modiﬁed trans-\nformer (Shaw et al., 2018). Then we ﬁne-tune the\npre-trained model with 1-best hypothesis or word\nlattices (and probability scores) for either Fisher\nor Callhome dataset.\nThe source and target vocabularies are built\nrespectively from the transcripts of Fisher/Train\nand Callhome/Train corpus, with vocabulary sizes\n32000 and 20391. The hyper-parameters of our\nmodel are the same as Transformer-base with\n512 hidden size, 6 attention layers, 8 atten-\ntion heads and beam size 4. We use the same\noptimization strategy as (Vaswani et al., 2017)\nfor pre-training with 4 GPU cards, and apply\nSGD with constant learning rate 0.15 for ﬁne-\ntuning. We select the best performed model\nbased on Fisher/Dev or Callhome/Dev, and test on\nFisher/Dev2, Fisher/Test or Callhome/Test.\nTo better analyze the performance of our ap-\nproach, we use an intensive cross-evaluation\n3https://github.com/moses-smt/mosesdecoder\nmethod, i.e., we feed 4 possible inputs to test dif-\nferent models. The cross-evaluation results are put\ninto several 4 ×4 blocks in Table 2 and 3. As the\naforementioned discussion, if the input is not ASR\nlattice, the evaluation on the model R+L+S needs\nto set wm = sf = sb = 0, sm = 1. If the input is\nan ASR lattice but fed into the other three models,\nthe probability scores are in fact discarded.\n4.1.3 Results on Fisher and Callhome\nWe mainly compare our architecture with the pre-\nvious Lattice-LSTMs (Sperber et al., 2017) and\nthe transformer (Shaw et al., 2018) in Table 2.\nSince the transformer itself is a powerful architec-\nture for sequence modeling, the BLEU scores of\nthe baseline (R) have signiﬁcant improvement on\ntest sets. In addition, ﬁne-tuning without scores\nhasn’t outperformed the 1-best hypothesis ﬁne-\ntuning, but has about 0.5 BLEU improvement on\noracle and transcript inputs. We suspect this may\nbe due to the high ASR WER and if the ASR sys-\ntem has a lower WER, the lattice without score\nﬁne-tuning may get a better translation. We will\nleave this as a future research direction on other\ndatasets from better ASR systems. For now, we\njust validate this argument in the BPE lattice ex-\nperiments, and detailed discussion sees next sec-\ntion. As to ﬁne-tuning with both lattices and\nprobability scores, it increases the BLEU with a\nrelatively large margin of 0.9/1.0/0.7 on Fisher\nDev/Dev2/Test sets. Besides, for ASR 1-best in-\nputs, it is still comparable with the R+1 systems,\nwhile for oracle and transcript inputs, there are\nabout 0.5-0.9 BLEU score improvements.\nThe results of Callhome dataset are all ﬁne-\ntuned from the pre-trained model based on\nFisher/Train corpus, since the data size of Call-\nhome is too small to train a large deep learning\nmodel. This is the reason why we adopt the strat-\negy for domain adaption. We use the same method\nfor model selection and test. The detailed results\nin Table 3 show the consistent performance im-\nprovement.\n4.1.4 Inference Analysis\nOn the test datasets of Fisher and Callhome, we\nmake an inference for predicting the translations,\nand some examples are shown in Table 4. We\nalso visualize the alignment for both encoder self-\nattention and encoder-decoder attention for the in-\nput and predicted translation. Two examples are\nillustrated in Figure 4 and 5. As expected, the to-\nArchitecture Inference Inputs Fisher dev Fisher dev2 Fisher test\nR R+1 R+L R+L+S R R+1 R+L R+L+S R R+1 R+L R+L+S\nLattice LSTM\nreference - - - - 53.9 53.8 53.7 54 52.2 51.8 52.2 52.7\noracle - - - - 44.9 45.6 45.2 45.2 44.4 44.6 44.6 44.8\nASR 1-best - - - - 35.8 37.1 36.2 36.2 35.9 36.6 36.2 36.4\nASR Lattice - - - - 25.9 25.8 36.9 38.5 26.2 25.8 36.1 38\nLattice Transformer\nreference 57.1 55.0 55.5 55.5 58.0 56.1 56.4 56.6 56.0 53.7 54.1 54.2\noracle 46.3 46.2 46.8 46.7 47.1 47.0 47.5 47.9 46.8 46.4 46.9 46.9\nASR 1-best 36.5 37.4 37.6 37.4 37.4 38.4 38.3 38.6 37.7 38.5 38.2 38.4\nASR Lattice 32.9 33.8 37.7 38.3 33.4 34.0 38.6 39.4 33.5 33.7 37.9 39.2\nTable 2: Cross-Evaluation of BLEU on Fisher. Note that for the lattice transformer architecture with R or R+1 set-\nting, the resulted model is equivalent to a standard transformer with relative positional embeddings. The evaluation\nof oracle inputs is similar to ASR 1-best, but it can indicate an upper bound of the performance. The evaluation\nresults of Lattice LSTM on Fisher dev are not reported in (Sperber et al., 2017).\nArchitecture Inference Inputs Callhome devtest\nR R+1 R+LR+L+S\nLattice Transformer\nreference 28.3 29.6 30.0 30.4\noracle 17.7 19.7 19.5 19.6\nASR 1-best13.4 15.2 14.8 15.1\nASR Lattice13.4 13.4 15.6 15.7\nCallhome evltest\nR R+1 R+LR+L+S\nLattice LSTM\nreference 24.7 24.3 24.8 24.4\noracle 15.8 16.8 16.3 15.9\nASR 1-best11.8 13.3 12.4 12.0\nASR Lattice9.3 7.1 13.7 14.1\nLattice Transformer\nreference 27.1 28.6 28.9 29.1\noracle 16.5 18.1 17.7 18.0\nASR 1-best12.7 14.5 13.6 14.1\nASR Lattice12.7 13.0 14.2 14.9\nTable 3: Cross-Evaluation of BLEU on Callhome.\nkens from different paths will not attend to each\nother, e.g., “pero” and “perd ´on” in Figure 4 or\n“hoy” and “y” in Figure 5. In Figure 4, we ob-\nserve that the 1-best hypothesis can even result in\nerroneous translation “sorry, sorry”, which is sup-\nposed to be “but in peru”. In Figure 5, the transla-\ntion from 1-best hypothesis obviously misses the\nimportant information “i heard it”. We primar-\nily attribute such errors to the insufﬁcient infor-\nmation within 1-best hypothesis, but if the lattice\ntransformer is appropriately trained, the transla-\ntions from lattice inputs can possibly correct them.\nDue to limited space, more visualization examples\ncan be found in supplementary material.\n4.1.5 Model Ablation Study\nWe conduct an ablation study to examine the ef-\nfectiveness of every module in the lattice trans-\nformer. We gradually add one module from a\nstandard transformer model to our most compli-\ncated lattice transformer. From the results in Ta-\nble 5, we can see that the application of marginal\nscores in encoder or decoder has the most inﬂu-\nential impact on the lattice ﬁne-tuning. Further-\nmore, the superimposed application of marginal\nscores in both encoder and decoder can gain an\nadditional promotion, compared to individual ap-\nplications. However, the use of forward and back-\nward scores has no signiﬁcantly extra rewards in\nthis situation. Perhaps due to overﬁtting, the most\ncomplicated lattice transformer on the Callhome\nof smaller data size cannot achieve better BLEUs\nthan simpler models.\n4.2 Chinese English Text Translation\nIn this experiment, we demonstrate the perfor-\nmance of our lattice transformer when the prob-\nability scores are unavailable. The compari-\nson baseline method is the vanilla transformer\n(Vaswani et al., 2017) in both base and big set-\ntings.\n4.2.1 Datasets and Settings\nThe Chinese to English parallel corpus for WMT\n2017 news task contains about 20 million sen-\ntences after deduplication. For Chinese word seg-\nmentation, we use Jieba 4 as the baseline (Zhang\net al., 2018; Hassan et al., 2018), while the English\nsentences are tokenized by moses tokenizer. Some\ndata ﬁltering tricks have been applied, such as the\nratio within [1/3, 3] of lengths between source and\ntarget sentence pairs and the count of tokens in\nboth sides (≤200).\nThen for the Chinese source corpus, we learn\nthe BPE tokenization with 16K / 32K / 48K op-\nerations, while for the English target corpus, we\nonly learn the BPE tokenization with 32K opera-\ntions. In this way, each Chinese input can be rep-\nresented as three different tokenized results, thus\nbeing ready to construct a word lattice.\nThe hyper-parameters of our model are the\n4https://github.com/fxsjy/jieba\nsrc transcriptqu´e tal , eh , yo soy guillermo , ¿ c´omo est´as ?porque como esto tiene que ir avanzando ¿ no ?pues , ¿ y llevas muchos a˜nos aqu`ı en atlanta ?quererlo y tener fe .tgt referencehow are you , eh i ’m guillermo , how are you ?because like this has to be moving forward , no ?well . and you ’ve been many years here in atlanta ?to love him and have faith .ASR 1-bestquedar eh yo soy guillermo c´omo est´as porque como esto tiene que ir avanzando nopas lleva muchos aos aqu en atlantaquieren lo y tener femt from R+1stay . eh , i ’m guillermo . how are you ?why do you have to move forward or not ?country has been many years here in atlantathey want to have faith\nASR latticequedar que qu´e eh yo soy dar eh yo tal eh yo soyguillermo cmo comprar con como est´a est´as porque como esto tiene que ir avanzando nopa´ıs well lleva lleva muchos a˜nos aqu´ı en atlantaquieren quererlo lo y tenertenerse fe y tener tenerse femt from R+L+Show are you ? i ’m guillermo . how are you ?because since this has to move forward , right ?well , you ’ve been here many years in atlantaloving him and having faith\nTable 4: Translation examples on test sets. Note that the presented ASR lattice does not include lattice information.\nFigure 4: Visualization of Lattice Transformer encoder\nself-attention and encoder-decoder attention for infer-\nence. Top panel: ASR 1-best. Bottom panel: ASR\nlattice. Target reference: “But in Peru, I’ve heard there\nare parts where it really gets cold.”\nsame as the setting with the speech translation in\nprevious experiments. We follow the optimization\nconvention in (Vaswani et al., 2017) to use ADAM\noptimizer with Noam invert squared decay. All\nof our lattice transformers are trained on 4 P-100\nGPU cards. Similar to our comparison method,\ndetokenized cased-sensitive BLEU is reported in\nour experiment.\n4.2.2 Results\nFor our lattice transformer, we have three models\ntrained for comparison. First we use the 32K BPE\nChinese corpus alone to train our lattice trans-\nformer, which is equivalent to the standard trans-\nFigure 5: Left panel: ASR 1-best. Right panel: ASR\nlattice. Target reference: “Yes, yes, I heard it.”\nModel Fisher dev2 Fisher test Callhome evltestLSTM (1-best input) 37.1 36.6 13.3Lattice LSTM (lattice input) 36.9 36.1 13.7+lattice prob scores 38.5 38 14.1Transformer (1-best input) 38.4 38.5 14.5Lattice Transformer (lattice input) 38.6 37.9 14.2+ marginal scores in decoder 39.0 38.7 14.4+ marginal scores in encoder 38.8 38.2 14.7+ marginal scores in encoder and decoder 39.5 39.0 14.8+ marginal scores in encoder and decoder,and forward / backward scores onlyin encoder self-attention layer 0 and layer 139.6 39.1 14.9\n+ marginal scores in encoder and decoder,and forward / backward scoresin all encoder self-attention layers39.4 39.2 14.7\nTable 5: Ablation Experiment BLEU Results. The\nrows of the Lattice LSTM and the Lattice Transformer\nrepresent the 1-best hypothesis ﬁne-tuning, and the\nBLEUs are evaluated on 1-best inputs and on lattice\ninputs for the others. The colored BLEU values come\nfrom Table 2 and 3.\nformer with relative positional embeddings. Sec-\nondly, we train another lattice transformer with\nthe word lattice corpus from scratch. In addition,\nwe follow the convention of the speech transla-\ntion task in previous experiments by ﬁne-tuning\nthe ﬁrst model with word lattice corpus. For each\nsetting, the model evaluated on test 2017 dataset\nis selected from the best model performed on the\ndev2017 data. The ﬁne-tuning of Lattice Model 3\nstarts from the best checkpoint of Lattice Model\n1. The BLEU evaluation is shown in Table 6, and\ntwo examples of attention visualization are shown\nin Figure 6. Notice that the ﬁrst two results of\ntransformer-base and -big are directly copied from\nthe relevant references. From the result, we can\nFigure 6: Attention visualization for Chinese English\ntranslation task.\nsee that our Model 1 can be comparable with the\nvanilla transformer-big model in a base setting,\nand signiﬁcantly better than the transformer-base\nmodel. We also validate the argument that train-\ning from scratch can also achieve a better result\nthan most baselines. Empirically, we ﬁnd an inter-\nesting phenomena that training from scratch con-\nverges faster than other settings.\n5 Conclusions\nIn this paper, we propose a novel lattice trans-\nformer architecture with a controllable lattice at-\ntention mechanism that can consume a word lat-\ntice and probability scores from the ASR system.\nThe proposed approach is naturally applied to both\nArchitecture Inference Inputstest2017\nTransformer (Zhang et al., 2018)BPE 32K 23.01\nTransformer-big (Hassan et al., 2018)BPE 32K 24.20\n1. Transformer with BPE 32K BPE 32K 24.26\n2. Lattice Transformer from scratchlattice 24.71\n3. Lattice Transformer with ﬁne-tuninglattice 24.81\nTable 6: BLEU on WMT 2017 Chinese English\nthe encoder self-attention and encoder-decoder at-\ntention. We mainly validate our lattice trans-\nformer on speech translation task, and addition-\nally demonstrate its generalization to text transla-\ntion on the WMT 2017 Chinese-English transla-\ntion task. In general, the lattice transformer can\nincrease the metric BLEU for translation tasks by\na signiﬁcant margin over many baselines.\nAcknowledgments\nWe thank Nguyen Bach to provide the script for\nattention visualization.\nReferences\nOliver Adams, Graham Neubig, Trevor Cohn, and\nSteven Bird. 2016. Learning a translation model\nfrom word lattices. In Interspeech.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nOndˇrej Bojar, Christian Federmann, Mark Fishel,\nYvette Graham, Barry Haddow, Philipp Koehn, and\nChristof Monz. 2018. Findings of the 2018 confer-\nence on machine translation (wmt18). In Proceed-\nings of the Third Conference on Machine Transla-\ntion: Shared Task Papers, pages 272–303.\nJacob Buckman and Graham Neubig. 2018. Neural lat-\ntice language models. Transactions of the Associa-\ntion for Computational Linguistics, 6:529–541.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nKai Fan, Bo Li, Fengming Zhou, and Jiayi Wang. 2018.\n” bilingual expert” can ﬁnd translation errors. arXiv\npreprint arXiv:1807.09433.\nHany Hassan, Anthony Aue, Chang Chen, Vishal\nChowdhary, Jonathan Clark, Christian Feder-\nmann, Xuedong Huang, Marcin Junczys-Dowmunt,\nWilliam Lewis, Mu Li, et al. 2018. Achieving hu-\nman parity on automatic chinese to english news\ntranslation. arXiv preprint arXiv:1803.05567.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nNiehues Jan, Roldano Cattoni, St ¨uker Sebastian,\nMauro Cettolo, Marco Turchi, and Marcello Fed-\nerico. 2018. The iwslt 2018 evaluation campaign. In\nInternational Workshop on Spoken Language Trans-\nlation, pages 2–6.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Ju-\nrafsky. 2018. Sharp nearby, fuzzy far away: How\nneural language models use context. arXiv preprint\narXiv:1805.04623.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , volume 1,\npages 66–75.\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, et al. 2018. Phrase-based & neu-\nral unsupervised machine translation. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 5039–5049.\nKaho Osamura, Takatomo Kano, Sakriani Sakti, Kat-\nsuhito Sudoh, and Satoshi Nakamura. 2018. Using\nspoken word posterior features in neural machine\ntranslation. architecture, 21:22.\nMatt Post, Gaurav Kumar, Adam Lopez, Damianos\nKarakos, Chris Callison-Burch, and Sanjeev Khu-\ndanpur. 2013. Improved speech-to-text translation\nwith the ﬁsher and callhome spanish–english speech\ntranslation corpus. In International Workshop on\nSpoken Language Translation.\nLawrence R Rabiner. 1989. A tutorial on hidden\nmarkov models and selected applications in speech\nrecognition. Proceedings of the IEEE , 77(2):257–\n286.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), volume 1, pages\n1715–1725.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), volume 2, pages\n464–468.\nMatthias Sperber, Graham Neubig, Jan Niehues, and\nAlex Waibel. 2017. Neural lattice-to-sequence mod-\nels for uncertain inputs. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1380–1389.\nJinsong Su, Zhixing Tan, Deyi Xiong, Rongrong Ji, Xi-\naodong Shi, and Yang Liu. 2017. Lattice-based re-\ncurrent neural network encoders for neural machine\ntranslation. In Thirty-First AAAI Conference on Ar-\ntiﬁcial Intelligence.\nYuanhang Su, Kai Fan, Nguyen Bach, C-C Jay\nKuo, and Fei Huang. 2018. Unsupervised multi-\nmodal neural machine translation. arXiv preprint\narXiv:1811.11365.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104–3112.\nKai Sheng Tai, Richard Socher, and Christopher D\nManning. 2015. Improved semantic representations\nfrom tree-structured long short-term memory net-\nworks. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\nvolume 1, pages 1556–1566.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\nLe. 2018. Qanet: Combining local convolution\nwith global self-attention for reading comprehen-\nsion. arXiv preprint arXiv:1804.09541.\nYue Zhang and Jie Yang. 2018. Chinese ner using lat-\ntice lstm. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , volume 1, pages 1554–\n1564.\nZhirui Zhang, Shuangzhi Wu, Shujie Liu, Mu Li, Ming\nZhou, and Enhong Chen. 2018. Regularizing neu-\nral machine translation by target-bidirectional agree-\nment. arXiv preprint arXiv:1808.04064.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6929754614830017
    },
    {
      "name": "Transformer",
      "score": 0.640657365322113
    },
    {
      "name": "Speech recognition",
      "score": 0.5191961526870728
    },
    {
      "name": "Segmentation",
      "score": 0.5113481879234314
    },
    {
      "name": "Machine translation",
      "score": 0.48407426476478577
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.4511227011680603
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4333752989768982
    },
    {
      "name": "Natural language processing",
      "score": 0.43068087100982666
    },
    {
      "name": "Lattice (music)",
      "score": 0.42108726501464844
    },
    {
      "name": "Voltage",
      "score": 0.151228666305542
    },
    {
      "name": "Acoustics",
      "score": 0.12203222513198853
    },
    {
      "name": "Engineering",
      "score": 0.1119997501373291
    },
    {
      "name": "Physics",
      "score": 0.10885870456695557
    },
    {
      "name": "Electrical engineering",
      "score": 0.09658321738243103
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    }
  ],
  "cited_by": 8
}