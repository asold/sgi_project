{
  "title": "A-Prot: protein structure modeling using MSA transformer",
  "url": "https://openalex.org/W4220840971",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5027399963",
      "name": "Yiyu Hong",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5066026979",
      "name": "Juyong Lee",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5105988820",
      "name": "Junsu Ko",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3186243460",
    "https://openalex.org/W3178087467",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W2132022258",
    "https://openalex.org/W1979762151",
    "https://openalex.org/W136234207",
    "https://openalex.org/W6691548924",
    "https://openalex.org/W2065921821",
    "https://openalex.org/W2114340287",
    "https://openalex.org/W2136799255",
    "https://openalex.org/W2120836664",
    "https://openalex.org/W2999044305",
    "https://openalex.org/W2971003495",
    "https://openalex.org/W2967606876",
    "https://openalex.org/W3166142427",
    "https://openalex.org/W2999481648",
    "https://openalex.org/W3037888463",
    "https://openalex.org/W3010387158",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W3111174583",
    "https://openalex.org/W3133458480",
    "https://openalex.org/W3193271391",
    "https://openalex.org/W2957531855",
    "https://openalex.org/W2983571096",
    "https://openalex.org/W2138122982",
    "https://openalex.org/W2972411752",
    "https://openalex.org/W2557595285",
    "https://openalex.org/W2949342052",
    "https://openalex.org/W2950954328",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W6767164110",
    "https://openalex.org/W3124030253",
    "https://openalex.org/W2987090428",
    "https://openalex.org/W3175014854",
    "https://openalex.org/W3185358186",
    "https://openalex.org/W3183921815",
    "https://openalex.org/W1982583124",
    "https://openalex.org/W2107867854",
    "https://openalex.org/W2997234557"
  ],
  "abstract": null,
  "full_text": "A‑Prot: protein structure modeling using \nMSA transformer\nYiyu Hong1, Juyong Lee1,2* and Junsu Ko1 \nIntroduction\nModeling the 3D structure of a protein from its sequence has been one of the most criti -\ncal problems in biophysics and biochemistry [1, 2]. The knowledge of the 3D structure \nof a protein facilitates the discovery of novel ligands, function annotation, and protein \nengineering. Due to its importance, the community of computational protein scientists \nhave been developing various prediction methods and assessed their performance in \nthe large-scale blind tests, CASPs, which have continued for over two decades [2]. In \nCASP14, Deepmind demonstrated that their deep learning-based model, AlphaFold2 \n(AF2), predicts the 3D structures of proteins from their sequences with extremely high \nAbstract \nBackground: The accuracy of protein 3D structure prediction has been dramatically \nimproved with the help of advances in deep learning. In the recent CASP14, Deepmind \ndemonstrated that their new version of AlphaFold (AF) produces highly accurate 3D \nmodels almost close to experimental structures. The success of AF shows that the \nmultiple sequence alignment of a sequence contains rich evolutionary information, \nleading to accurate 3D models. Despite the success of AF, only the prediction code is \nopen, and training a similar model requires a vast amount of computational resources. \nThus, developing a lighter prediction model is still necessary.\nResults: In this study, we propose a new protein 3D structure modeling method, \nA-Prot, using MSA Transformer, one of the state-of-the-art protein language models. \nAn MSA feature tensor and row attention maps are extracted and converted into 2D \nresidue-residue distance and dihedral angle predictions for a given MSA. We dem-\nonstrated that A-Prot predicts long-range contacts better than the existing methods. \nAdditionally, we modeled the 3D structures of the free modeling and hard template-\nbased modeling targets of CASP14. The assessment shows that the A-Prot models are \nmore accurate than most top server groups of CASP14.\nConclusion: These results imply that A-Prot accurately captures the evolutionary and \nstructural information of proteins with relatively low computational cost. Thus, A-Prot \ncan provide a clue for the development of other protein property prediction methods.\nKeywords: Protein structure prediction, Multiple sequence alignment, Protein \nlanguage model, Deep learning\nOpen Access\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nHong et al. BMC Bioinformatics           (2022) 23:93  \nhttps://doi.org/10.1186/s12859‑022‑04628‑8 BMC Bioinformatics\n*Correspondence:   \njuyong.lee@kangwon.ac.kr \n1 Arontier Co, Seoul, Republic \nof Korea\nFull list of author information \nis available at the end of the \narticle\nPage 2 of 11Hong et al. BMC Bioinformatics           (2022) 23:93 \naccuracy, comparable to experimental accuracy [3]. The source code of AF2 became \npublicly available recently, and many model structures of genes of biologically important \norganisms have been released [3].\nThe success of AF2 can be attributed to the accurate extraction of coevolutionary \ninformation from multiple sequence alignments (MSA). The idea of using coevolutionary \ninformation for contact prediction or structure modeling has been widely used [4–12]. \nHowever, previous attempts were not successful enough because coevolutionary signals \nin MSAs were not strong enough or too noisy. Various statistical mechanics-based mod-\nels were proposed, but their accuracy, discriminating actual contacts from false contacts, \nwas limited. In CASP13, AlphaFold and RaptorX used coevolutionary information from \nMSAs as input and predicted inter-residue distances [13–15]. They achieved significant \nimprovements than previous CASPs, but their predictions were not accurate enough to \nobtain model structures comparable to experiments. Finally, in CASP14, with the help of \nthe attention algorithm, truly accurate extraction of actual residue-residue contact sig -\nnals from MSAs becomes available [3].\nDespite this remarkable achievement of AF2, certain limitations remain for this tech -\nnology to be widely accessible and used for further development. First, the source code \nfor training AF2 is not open-sourced yet. In the first release of AF2, only the production, \nmodel generation, part of AF2, and the model parameters are open. Second, training the \nAF2 architecture is computationally expensive. It is reported that Deepmind used 128 \nTPUv3 cores for approximately one week and four more days to train AF2. This amount \nof computational resources are not readily accessible for most academic groups. Thus, \ndeveloping lighter and general models is still necessary.\nTo extract evolutionary information from MSAs, various protein language models \nhave been proposed [16–22]. Rao et al. proposed the MSA transformer model, an unsu -\npervised protein language model using the MSA of a query sequence instead of a single \nquery sequence. The model uses row and column attention of input MSAs and masked \nlanguage modeling objectives. It is demonstrated that the model successfully predicted \nlong-range contacts between residues. In addition, the model predicted other proper -\nties of proteins, such as secondary structure prediction and mutational effects, with high \naccuracy [20]. These results indicate that the MSA Transformer model extracts the char-\nacteristics of proteins from their MSA profiles efficiently.\nThis study developed a new protein 3D structure prediction method, A-Prot, using \nMSA Transformer [22]. For a given MSA, we extracted evolutionary information with \nMSA Transformer. The extracted row attention map and input features were converted \nto a 2D residue-residue distance map and dihedral angle predictions. We benchmarked \nthe 3D protein structure modeling performance using the FM/TBM-hard targets of \nCASP13 and 14 [23, 24]. The results show that A-Prot outperforms most top server \ngroups of CASP13 and 14 in terms of long-range contact predictions and 3D protein \nstructure modeling.\nMethods\nOverview\nThe overall pipeline of the proposed protein 3D structure prediction method is shown \nin Fig. 1. We mainly combine the works of the MSA Transformer [22] and the trRosetta \nPage 3 of 11\nHong et al. BMC Bioinformatics           (2022) 23:93 \n \nFig. 1 Pipeline of the proposed protein 3D structure prediction method. Input an MSA to the MSA \nTransformer to extract MSA Features and row attention maps. Then, the MSA features corresponding to the \nquery sequence and the row attention maps are combined to a 2D feature maps by a set of transformations. \nNext, the 2D feature maps are input to a Dilated ResNet after dimension reduction to output inter-residue \ngeometries, which further input to the trRosetta Protein Structure Modeling to output a predicted protein 3D \nstructure\nPage 4 of 11Hong et al. BMC Bioinformatics           (2022) 23:93 \n[25]. Given an MSA, it will input to the MSA Transformer to output MSA features and \nrow attention maps. After a series of transformations like dimension reduction and con -\ncatenation etc., these two kinds of features will be transformed and combined into 2D \nfeature maps, which is suitable for input to the trRosetta to finally output protein 3D \nstructure.\nThe MSA Transformer used in this paper was an already pre-trained version that was \nlearned from 26 million MSAs. It plays a role as a feature extractor that inputs an MSA \nand outputs its relative features.\ntrRosetta consists of a deep neural network part and protein structure modeling part. \nThe deep neural network is modified to receive the MSA features from the MSA Trans -\nformer instead of the original manually engineered features generated by the statistic \napproach. Nevertheless, the protein structure modeling part remains the same.\nDataset and MSA generation\nWe used the procedure with MSA transformer to generate the MSAs of target sequences \n(Rao, Liu, et  al., 2021). The MSA of a query sequence was generated using Jackham -\nmer [26], HHblits ver. 3.3.0, and HHsearch [27] together with the unicluster30_2017_10 \n[28] and BFD [29] databases. If not specified, all training and test MSAs are used BFD. \nIf the number of detected homologous sequences exceeds 256, up to 256 sequences \nwere selected through diversity minimization [22]. The upper limit of the number \nof sequences was determined by the maximum size of GPU memory of the NVIDIA \nQuadro RTX 8000 GPU (48 GB) card.\nTo perform contact predictions, a customized non-redundant protein structure data -\nbase is compiled, and we named it PDB30. The PDB30 dataset consists of protein struc -\ntures deposited in PDB before Apr-30–2018 whose resolution is higher than 2.5 A and \nsequence length is longer than 40 amino acids. Using 102,300 sequences that satisfy the \ncondition, clustering analysis was performed using MMSeq2 [30] with a threshold of \nsequence identity of 30%, leading to 16,612 non-redundant sequences.\nNetwork architecture\nLet us define an input MSA as an r × c character matrix, where r and c are rows (num -\nber of sequences) and columns (sequence length) in the MSA, respectively. Through \nthe token and position embedding of the MSA Transformer, the input MSA is embed -\nded into a r × c × 768 tensor which is the input and output of each attention block. The \nattention block is composed in the order of row attention layer, column attention layer, \nand feed-forward layer. A layer normalization operation is followed by each layer. And \neach attention layer has 12 attention heads. The MSA Transformer is a stack of 12 such \nattention blocks.\nTwo kinds of features were extracted from the MSA Transformer to construct 2D fea -\nture maps by a series of transformations. (1) One is the last attention block’s output, \na r × c × 768 tensor; we named it MSA features (Fig.  1). Only features corresponding \nto the query sequence are selected, which is a 1 × c × 768 tensor. Then, the dimension \nof this feature is reduced to 128 by an MLP (multi-layer perceptron) consist of 3 lin -\near layers with neuron sizes 384, 192, 128. The dimension reduced 1D feature is then \nouter concatenated (redundantly expanding horizontally and vertically and then stacked \nPage 5 of 11\nHong et al. BMC Bioinformatics           (2022) 23:93 \n \ntogether) to form a query sequence feature of a c × c × 256 tensor. (2) The other one \nis row attention maps that are derived from each attention head of each row attention \nlayer, totally 12 × 12 = 144 attention maps stacked to shape in c × c × 144 . Then it is \nsymmetrized by adding it to its transposed tensor to yield symmetrized row attention \nmaps. The query sequence feature and the symmetrized row attention maps are con -\ncatenated in the feature map dimension to form a 2D feature map that is a c × c × 400 \ntensor.\nThe dimension of the 2D feature maps afterward reduced to 64 by three convolu -\ntional layers that consist of 256, 128, 64 kernels of size 1 × 1 [31], where each convo -\nlutional layer is followed by an instance normalization and a ReLU activation. Then \nthis c × c × 64 tensor is input to the Dilated ResNet consist of 28 residual blocks [32], \neach having one instance normalization, two ReLU activations, and two convolutional \nlayers each with 64 kernels of size 3 × 3 . Dilation is applied to the convolutional lay -\ners cycle through the residual blocks with rates of 1, 2, 4. After the last residual block, \nthere are four independent convolutional layers with 25, 13, 25, 37 kernels of size 1 × 1 , \neach for predicting inter-residue geometries of θ,ϕ,ω,d  , respectively. Please refer to the \nreference [25] for more details about the inter-residue geometries as we used the same \nsettings. Finally, the trRosetta Protein Structure Modeling module will predict and mod-\neling the protein 3D structure based on the inter-residue geometries information.\nTraining and inference\nAt the training stage, we fixed the parameters in the MSA Transformer. In contrast, \nparameters in the other deep neural networks were trained with a batch size of 16 with \ngradient accumulation steps, a learning rate of 1e − 3, using the RAdam optimizer [33], \nthe categorical cross-entropy loss was calculated with equal weight for the four inter-res-\nidue geometry objectives [25]. The ground truth values for the inter-residue geometries \nare all discretized into bins which have same number as corresponding convolutional \nkernels ( θ for 25 bins, ϕ for 13 bins, ω for 25 bins, d for 37 bins); each bin is treated as a \nclassification label. The total model was trained end-to-end on an NVIDIA Quadro RTX \n8000 GPU (48 GB) for around 40 epochs which took about five days.\nAn MSA subsampling strategy is applied during training, not only for regarding as \ndata augmentation to train a robust model, but also for preventing the GPU from run -\nning out of memory when filled with large MSA. We randomly select MSA rows, up to \na maximum of 214/c , and down to a minimum of 16, though always including the query \nsequences in the first row. Large proteins of more than 1023 residues long were dis -\ncarded during training. We subsampled MSA with 256 sequences at the inference stage \nby adding the sequence with the lowest average hamming distance. We performed trRo -\nsetta protein structure modeling five times with the same input and selected the struc -\nture with the lowest energy for protein structure similarity measurement.\nResults and discussion\nBenchmarking contact prediction\nFirst, we benchmarked the long-range contact prediction performance of A-Prot \nusing the FM and FM/TBM targets of CASP13 [24]. The benchmark results show \nthat the performance of our model outperforms that of the existing methods \nPage 6 of 11Hong et al. BMC Bioinformatics           (2022) 23:93 \n(Table 1). We compared the precision of our model’s top L/5, L/2, long-range contact \npredictions (long-range: sequence separation of the residue pair ≥ 24) and the other \nexisting methods. The performance measures of the other methods are adopted \nfrom the reference [34]. The top L/5, Top L/2, and L contact precisions of our model \nare 0.812, 0.710, and 0.562, which are higher than those of the other methods. For \nexample, DeepDist, one of the state-of-the-art methods, predicted the top L/5, L/2, \nand L with precisions of 0.793, 0.661, and 0.517. Also, compared with AlphaFold \npredictions of CASP13, A-Prot predicted more accurately in all three measures by \n7–9%.\nWe also compared the contact prediction accuracy of A-Prot with MSA-Trans -\nformer, which is the baseline of A-Prot. The performance of long-range Top L and \nTop L/5 for supervised contact prediction on CASP13 FM targets were 0.546 and \n0.775 respectively (Rao, Meier, et al., 2021). The contact precision of Top L and Top \nL/5 for A-prot were 0.539 and 0.785, which is comparable to those of MSA Trans -\nformer (Additional file  1: Table S2). We also investigated the effect of a MSA to con -\ntact prediction by predicting structures with the MSAs obtained without BFD and \nobtained with DeepMSA [35]. The assessment shows that not using BFD deteriorates \nthe quality of models. The accuracies of Top L and Top L/5 contacts decreased by \n2.5% and 4.0%, respectively. When DeepMSA was used, the Top L and Top L/5 accu -\nracies dropped by 3.9% and 2.0%, respectively. These results show that considering \na gigantic meta-genome DB helps improve prediction quality, but not significantly.\nTo investigate how the number of residual blocks affects the prediction quality, \nablation tests were performed by changing the number of the residual blocks (Addi -\ntional file  1: Table S4). We performed contact and structure prediction with 4, 16, \n28, and 40 residual blocks. The contact prediction results show that A-Prot calcu -\nlations with 28 blocks resulted in the best contact prediction. Interestingly, using \nmore blocks, 40, decreased contact prediction accuracy. Fewer residual blocks, 4 and \n16, led to significantly worse contact prediction results. In terms of model accuracy, \nA-Prot with 40 blocks resulted in the highest TMscore although contact predictions \nwere most accurate with 28 blocks. These results show that A-Prot with 28 blocks is \nclose to the optimal model considering both the prediction accuracy and computa -\ntional cost.\nTable 1 Contact Precision on CASP13 FM and FM/TBM targets corresponding to 43 domains (results \nare  adapted from DeepDist [34])\nThe highest score of each column is highlighted in bold\nGroup Top L Top L/2 Top L/5\nTripletRes 0.451 0.587 0.700\nAlphaFold 0.497 0.629 0.742\nRaptorX-Contact 0.481 0.612 0.744\ntrRosetta 0.506 0.652 0.751\nDeepDist 0.517 0.661 0.793\nA-Prot (w BFD) 0.562 0.710 0.812\nA-Prot (w/o BFD) 0.540 0.681 0.780\nPage 7 of 11\nHong et al. BMC Bioinformatics           (2022) 23:93 \n \nBenchmark on model accuracy\nIn addition to contact prediction, we also compared the quality of protein models \npredicted by A-Prot with those submitted by the top-performing server groups of \nCASP14 (Table  2). The highest score of each column is highlighted in bold.First, we \nmodeled the structures of 25 FM/TBM and TBM-hard targets of CASP14. The aver -\nage TM-score and lDDT score of the models were compared with those of the fol -\nlowing server groups: FEIG-S [36], BAKER-ROSETTASERVER [ 37], Zhang-Server, \nand QUARK [38]. The model structures of the other groups were downloaded from \nthe archive of the CASP14 website, and TM-score and lDDT scores were recalculated \nwith the crystal structures and domain information for a fair comparison.\nA-Prot outperforms the other top server groups in terms of lDDT. Compared with \nBAKER- ROSETTASERVER and FEIG-S, A-Prot models are consistently more accu -\nrate in both measures. The P-values show that A-Prot outperforms ROSETTASERVER \nand FEIG-S statistically significantly. These results show that the accuracy of A-Prot \nis comparable to or better than the top-performing server groups that participated in \nCASP14 [23]. In terms of TM-score, A-Prot is showing slightly worse results, 0.576, \nthan Zhang-Server and QUARK, whose TM-scores are 0.595 and 0.588. However, the \nP-values show that the A-Prot results are not meaningfully different.\nWe also compared the performance of A-Prot with trRosetta [25] by modeling \nstructures using trRosetta with the identical MSAs that we used for A-Prot (Table  3). \nThe results show that A-Prot significantly outperforms trRosetta using the identical \nMSAs. In terms of dihedral angle predictions, A-Prot improved the correlation coef -\nficient between ground truth and predictions by 0.071 in average. In addition, A-Prot \ngenerated better models with higher TMscore and lDDT values than the trRosetta \nresults. The average TMscore and lDDT enhanced by 0.048 and 0.042 respectively.\nIn contrast to MSA-Transformer or ROSETTA, A-Prot used the subsampling strat -\negy that minimizes the diversity of sequences to reduce the sizes of MSAs. For protein \nTable 2 The average TM-score and lDDT of the model structures of 25 CASP14 FM, FM/TBM, and \nTBM-hard domains\nServer group TM-score P-value (TM-score) lDDT P-value (lDDT)\nFEIG-S 0.461 0.0007 0.413 0.0172\nBAKER-ROSETTASERVER 0.517 0.0391 0.455 0.2636\nZhang-Server 0.595 0.6684 0.489 0.8510\nQUARK 0.588 0.6208 0.484 0.7408\nA-Prot 0.576 – 0.499 –\nTable 3 Performance of A-Prot and trRosetta using same MSA of ours on 25 CASP14 FM, FM/TBM, \nTBM-Hard domains. (Inter-residue distance and angles are measured using Pearson correlation \nbetween predicted bin indexes of max probability and ground truth, Top L for long range contact \nprecision)\nMethod θ ϕ ω d Top L TMS lDDT\ntrRosetta 0.539 0.498 0.459 0.362 0.363 0.524 0.457\nA-Prot 0.604 0.578 0.528 0.464 0.424 0.576 0.499\nPage 8 of 11Hong et al. BMC Bioinformatics           (2022) 23:93 \nlanguage models, four subsampling strategies have been tested: random, diversity \nmaximization, diversity minimization, and HHFilter to reduce the size of input MSAs \nwhile preserving prediction accuracies. It was shown that diversity maximizing per -\nformed best for the supervised contact prediction [22]. On the contrary, trRosetta \ngenerates a MSA in a similar manner to the diversity minimizing approach [25]. Dur -\ning the development of A-Prot, we tried both diversity minimization and maximiza -\ntion approaches. The prediction results show that diversity minimizing yields more \naccurate models than diversity maximization (Additional file  1: Table  S3). We pre -\ndicted the structures of 43 domains of CASP13 FM, FM/TBM and 25 domains of \nCASP14 FM, FM/TBM and TBM-hard targets using MSAs subsampled with diver -\nsity minimization and maximization. In average, the models generated with the MSAs \nobtained with diversity minimization have higher TM-scores, 0.658 and 0.555 for \nthe CASP13 and CASP14 targets, than those with diversity maximization, 0.603 and \n0.532. Thus, we employed the diversity minimization approach for A-Prot.\nA head-to-head comparison with ROSETTASERVER\nBecause A-Prot uses trRosetta for modeling structure at the final stage, we performed \na head-to-head comparison of A-Prot models with the ROSETTASERVER mod -\nels to identify improvement in residue-residue distance predictions (Fig.  2). In terms \nof TM-score, many predictions made by A-Prot are significantly better than BAKER-\nROSETTASERVER. For instance, the model qualities of five targets that were predicted \nto have TM-score less than 0.4 by ROSETTASERVER were improved higher than 0.4, \ncorresponding to a correct fold prediction. Four highly accurate models are depicted in \nFig. 3.\nSimilarly, in terms of the lDDT measure, prediction results of eight targets were \nimproved significantly. On the contrary, only two targets deteriorated more than 0.05. \nTherefore, A-Prot predicts residue-residue distances and dihedral angles better than \nmost server groups participated in CASP14 [2].\nFig. 2 TM-score and lDDT on CASP14 FM, FM/TBM and TBM-hard 25 domains compared with \nBAKER-ROSETTASERVER\nPage 9 of 11\nHong et al. BMC Bioinformatics           (2022) 23:93 \n \nOn the other hand, nearly all worse predictions by A-Prot deteriorated only by a small \nmargin except the T1026-D1 target. For T1026-D1, A-Prot did not predict its correct \nfold. This incorrect prediction is attributed to an incomplete MSA. T1026-D1 is a pro -\ntein consisting of a virus capsid (PDB ID: 6S44). Our sequence search procedure found \nonly less than 30 sequences, which appear to be not enough to extract correct evolu -\ntionary information. When T1026-D1 is modelled with the MSA of trRosetta containing \nmore than 100 sequences, T1026-D1, a model with a similar TM-score is obtained. Thus, \nthe failure of T1026-D1 suggests that having enough homologous sequences is critical in \naccurate 3D structure modeling using MSA-Transformer. In other words, a more exten-\nsive sequence search may improve the model accuracy of A-Pro. Except for T1026-D1, \nthe deviations of all worse predictions than ROSETTASERVER were less than 0.1 TM-\nscore, much smaller than the improvements.\nConclusion\nIn this study, we introduced a new protein structure prediction method, A-Prot, using \nMSA Transformer. Our benchmark results on the CASP13 TBM/FM and FM targets \nshow that A-Prot predicts long-range residue-residue contacts more accurately than the \nexisting methods. We also assessed the quality of protein structure models based on the \npredicted residue-residue distance information. The model generated by A-Prot is more \naccurate than most of the server groups that participated in CASP14. The average lDDT \nof A-Prot models is higher than that of all server group models. In terms of TM-score, \nour model is slightly worse than QUARK and Zhang-Server. These results show that our \napproach yields highly accurate residue-residue distance predictions.\nFig. 3 Model comparison of four high-quality CASP14 models generated from our method versus their \nnative structures. Brown: native structure; Blue: model\nPage 10 of 11Hong et al. BMC Bioinformatics           (2022) 23:93 \nA-Prot requires less computational resources than the other state-of-the-art protein \nstructure prediction models [2]. The source code of AlphaFold2 is only partially open \n[3]. Its model parameters are fixed, and only the structure modeling part is open. Thus, it \nis hard to tune AlphaFold2 for bespoken purposes. In addition, training the AlphaFold2 \narchitecture requires a significant amount of computational resources, which is not \nassessable for most academic groups. On the other hand, A-Prot can be trained with a \nsingle GPU card. In summary, A-Prot will open new possibilities for training novel deep-\nlearning-based models to predict various properties of proteins only using sequence \ninformation.\nAbbreviations\nMSA: Multiple sequence alignment; FM: Free modeling; TBM: Template-based modeling; RMSD: Root mean square \ndeviation.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. org/ 10. 1186/ s12859- 022- 04628-8.\nAdditional file 1. The additional analysis on the modeling results are provided.\nAcknowledgements\nWe wish to thank CASP organizers and predictors for sharing the data used in this work.\nAuthors’ contributions\nYH, JL and JK conceived the research. YH wrote the code, performed simulations. YH, JL, and JK analyzed the results and \nwrote the manuscript. All authors read and approved the final manuscript.\nFunding\nJL was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) \n(Nos. 2018R1C1B600543513, 2019M3E5D4066898 and 2020M3A9G710393).\nAvailability of data and materials\nAll predicted model structures and the multiple sequences alignments used as the inputs for the A-Prot models are \navailable at https:// github. com/ aront ier/A_ Prot_ Paper.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no conflict of interests.\nAuthor details\n1 Arontier Co, Seoul, Republic of Korea. 2 Department of Chemistry, Division of Chemistry and Biochemistry, Kangwon \nNational University, Chuncheon, Republic of Korea. \nReceived: 11 September 2021   Accepted: 3 March 2022\nReferences\n 1. Kwon S, Won J, Kryshtafovych A, Seok C. Assessment of protein model structure accuracy estimation in CASP14: old \nand new challenges. Proteins Struct Funct Bioinform. 2021;89:1940–8. https:// doi. org/ 10. 1002/ prot. 26192.\n 2. Pereira J, Simpkin AJ, Hartmann MD, Rigden DJ, Keegan RM, Lupas AN. High-accuracy protein structure prediction in \nCASP14. Proteins Struct Funct Bioinform. 2021;89:1687–99. https:// doi. org/ 10. 1002/ prot. 26171.\n 3. Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, et al. Highly accurate protein structure prediction \nwith AlphaFold. Nature. 2021. https:// doi. org/ 10. 1038/ s41586- 021- 03819-2.\n 4. Tillier ERM, Charlebois RL. The human protein coevolution network. Genome Res. 2009;19:1861–71.\nPage 11 of 11\nHong et al. BMC Bioinformatics           (2022) 23:93 \n \n 5. Weigt M, White RA, Szurmant H, Hoch JA, Hwa T. Identification of direct residue contacts in protein–protein interac-\ntion by message passing. Proc Natl Acad Sci. 2009;106:67–72.\n 6. Lunt B, Szurmant H, Procaccini A, Hoch JA, Hwa T, Weigt M. Inference of direct residue contacts in two-component \nsignaling. In: Methods in enzymology. 2010. pp. 17–41.\n 7. Ovchinnikov S, Kamisetty H, Baker D. Robust and accurate prediction of residue-residue interactions across protein \ninterfaces using evolutionary information. eLife. 2014;2014:1–21.\n 8. de Juan D, Pazos F, Valencia A. Emerging methods in protein co-evolution. Nat Rev Genet. 2013;14:249–61.\n 9. Marks DS, Hopf TA, Sander C. Protein structure prediction from sequence variation. Nat Publ Group. 2012. https:// \ndoi. org/ 10. 1038/ 2419.\n 10. Seemayer S, Gruber M, Söding J. CCMpred: fast and precise prediction of protein residue-residue contacts from cor-\nrelated mutations. Bioinformatics. 2014;30:3128–30.\n 11. Jones DT, Singh T, Kosciolek T, Tetchner S. MetaPSICOV: combining coevolution methods for accurate prediction of \ncontacts and long range hydrogen bonding in proteins. Bioinformatics. 2015;31:999–1006.\n 12. Hopf TA, Schärfe CPI, Rodrigues JPGLM, Green AG, Kohlbacher O, Sander C, et al. Sequence co-evolution gives 3D \ncontacts and structures of protein complexes. eLife. 2014; 3.\n 13. Senior AW, Evans R, Jumper J, Kirkpatrick J, Sifre L, Green T, et al. Improved protein structure prediction using poten-\ntials from deep learning. Nature. 2020;577:706–10.\n 14. Xu J, Wang S. Analysis of distance-based protein structure prediction by deep learning in CASP13. Proteins Struct \nFunct Bioinform. 2019;87:1069–81.\n 15. Xu J. Distance-based protein folding powered by deep learning. Proc Natl Acad Sci USA. 2019;116:16856–65.\n 16. Bepler T, Berger B. Learning the protein language: evolution, structure, and function. Cell Syst. 2021;12:654-669.e3.\n 17. Strodthoff N, Wagner P , Wenzel M, Samek W. UDSMProt: Universal deep sequence models for protein classification. \nBioinformatics. 2020;36:2401–9.\n 18. Vig J, Madani A, Varshney LR, Xiong C, Socher R, Rajani NF. BERTology meets biology: interpreting attention in pro-\ntein language models. 2020.\n 19. Madani A, McCann B, Naik N, Keskar NS, Anand N, Eguchi RR, et al. ProGen: language modeling for protein genera-\ntion. 2020. https:// doi. org/ 10. 1101/ 2020. 03. 07. 982272.\n 20. Rives A, Meier J, Sercu T, Goyal S, Lin Z, Liu J, et al. Biological structure and function emerge from scaling unsuper-\nvised learning to 250 million protein sequences. Proc Natl Acad Sci. 2021;118:e2016239118.\n 21. Rao R, Meier J, Sercu T, Ovchinnikov S, Rives A. Transformer protein language models are unsupervised structure \nlearners. In: ICLR 2021 conference. 2021.\n 22. Rao R, Liu J, Verkuil R, Meier J, Canny JF, Abbeel P , et al. MSA transformer. 2021.\n 23. Kinch LN, Schaeffer RD, Kryshtafovych A, Grishin NV. Target classification in the 14th round of the critical assessment \nof protein structure prediction (CASP14). Proteins Struct Funct Bioinform. 2021;89:1618–32. https:// doi. org/ 10. 1002/ \nprot. 26202.\n 24. Kinch LN, Kryshtafovych A, Monastyrskyy B, Grishin NV. CASP13 target classification into tertiary structure prediction \ncategories. Proteins Struct Funct Bioinform. 2019;87:1021–36.\n 25. Yang J, Anishchenko I, Park H, Peng Z, Ovchinnikov S, Baker D. Improved protein structure prediction using pre-\ndicted interresidue orientations. Proc Natl Acad Sci USA 2020;117.\n 26. Eddy SR. Accelerated profile HMM searches. PLOS Comput Biol. 2011;7:1002195.\n 27. Steinegger M, Meier M, Mirdita M, Vöhringer H, Haunsberger SJ, Söding J. HH-suite3 for fast remote homology \ndetection and deep protein annotation. BMC Bioinform. 2019; 20.\n 28. Mirdita M, von den Driesch L, Galiez C, Martin MJ, Söding J, Steinegger M. Uniclust databases of clustered and \ndeeply annotated protein sequences and alignments. Nucleic Acids Res. 2017;45:D170–6.\n 29. Steinegger M, Mirdita M, Söding J. Protein-level assembly increases protein sequence recovery from metagenomic \nsamples manyfold. Nat Methods. 2019;16:603–6.\n 30. Steinegger M, Söding J. MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets. \nNat Biotechnol. 2017;35:1026–8.\n 31. Lin M, Chen Q, Yan S. Network in network. 2013; arXiv:1312.4400.\n 32. He K, Zhang X, Ren S, Sun J. Identity mappings in deep residual networks. In: Leibe Bastian and Matas J and SN and \nWM, editor. Computer vision—ECCV 2016. Cham: Springer International Publishing; 2016. p. 630–45.\n 33. Liu L, Jiang H, He P , Chen W, Liu X, Gao J, et al. On the variance of the adaptive learning rate and beyond. In: Eighth \ninternational conference on learning representations (ICLR). 2020.\n 34. Wu T, Guo Z, Hou J, Cheng J. DeepDist: real-value inter-residue distance prediction with deep residual convolutional \nnetwork. BMC Bioinform. 2021; 22.\n 35. Zhang C, Zheng W, Mortuza SM, Li Y, Zhang Y. DeepMSA: Constructing deep multiple sequence alignment to \nimprove contact prediction and fold-recognition for distant-homology proteins. Bioinformatics. 2020;36:2105–12.\n 36. Heo L, Janson G, Feig M. Physics-based protein structure refinement in the era of artificial intelligence. Proteins \nStruct Funct Bioinform. 2021;89:1870–87. https:// doi. org/ 10. 1002/ prot. 26161.\n 37. Anishchenko I, Baek M, Park H, Hiranuma N, Kim DE, Dauparas J, et al. Protein tertiary structure prediction and refine-\nment using deep learning and Rosetta in CASP14. Proteins Struct Funct Bioinform. 2021;89:1722–33. https:// doi. org/ \n10. 1002/ prot. 26194.\n 38. Zheng W, Li Y, Zhang C, Zhou X, Pearce R, Bell EW, et al. Protein structure prediction using deep learning distance \nand hydrogen-bonding restraints in CASP14. Proteins Struct Funct Bioinform. 2021;89:1734–51. https:// doi. org/ 10. \n1002/ prot. 26193.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7162339091300964
    },
    {
      "name": "Protein structure prediction",
      "score": 0.6576919555664062
    },
    {
      "name": "CASP",
      "score": 0.5594888925552368
    },
    {
      "name": "Dihedral angle",
      "score": 0.5339948534965515
    },
    {
      "name": "Protein structure",
      "score": 0.4726352095603943
    },
    {
      "name": "Data mining",
      "score": 0.41297537088394165
    },
    {
      "name": "Transformer",
      "score": 0.4114657938480377
    },
    {
      "name": "Machine learning",
      "score": 0.39537400007247925
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3825054168701172
    },
    {
      "name": "Computational biology",
      "score": 0.33927714824676514
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3233131170272827
    },
    {
      "name": "Biology",
      "score": 0.15071699023246765
    },
    {
      "name": "Chemistry",
      "score": 0.0950116217136383
    },
    {
      "name": "Engineering",
      "score": 0.08986112475395203
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Hydrogen bond",
      "score": 0.0
    },
    {
      "name": "Molecule",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 18
}