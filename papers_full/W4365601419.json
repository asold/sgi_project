{
  "title": "â€œWhat It Wants Me To Sayâ€: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models",
  "url": "https://openalex.org/W4365601419",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2898523786",
      "name": "Michael Xieyang Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2810769762",
      "name": "Advait Sarkar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3029463518",
      "name": "Carina Negreanu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2497362432",
      "name": "Benjamin Zorn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098065861",
      "name": "Jack Williams",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1991514586",
      "name": "Neil Toronto",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101841271",
      "name": "Andrew D. Gordon",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2942367614",
    "https://openalex.org/W4246144686",
    "https://openalex.org/W6828070779",
    "https://openalex.org/W1981425990",
    "https://openalex.org/W4224994457",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W1481305879",
    "https://openalex.org/W2914121031",
    "https://openalex.org/W2166050198",
    "https://openalex.org/W3169611685",
    "https://openalex.org/W2037133710",
    "https://openalex.org/W2239306219",
    "https://openalex.org/W4293795850",
    "https://openalex.org/W4385572831",
    "https://openalex.org/W2064189581",
    "https://openalex.org/W2132525863",
    "https://openalex.org/W2001962162",
    "https://openalex.org/W2197966456",
    "https://openalex.org/W3212201636",
    "https://openalex.org/W2898534553",
    "https://openalex.org/W4284676027",
    "https://openalex.org/W4307472488",
    "https://openalex.org/W3022461789",
    "https://openalex.org/W4294960805",
    "https://openalex.org/W2093495548",
    "https://openalex.org/W1540823594",
    "https://openalex.org/W2918497321",
    "https://openalex.org/W2053075547",
    "https://openalex.org/W4385567208",
    "https://openalex.org/W4385574286",
    "https://openalex.org/W1694295455",
    "https://openalex.org/W2795274753",
    "https://openalex.org/W2610474269",
    "https://openalex.org/W2898125335",
    "https://openalex.org/W2972018816",
    "https://openalex.org/W4226287673",
    "https://openalex.org/W2131251708",
    "https://openalex.org/W1999680491",
    "https://openalex.org/W2980877729",
    "https://openalex.org/W4288076021",
    "https://openalex.org/W4220955183",
    "https://openalex.org/W2405187948",
    "https://openalex.org/W4291476001",
    "https://openalex.org/W3017863658",
    "https://openalex.org/W1577581905",
    "https://openalex.org/W3198963017",
    "https://openalex.org/W2943537571",
    "https://openalex.org/W3029828555",
    "https://openalex.org/W4287890934",
    "https://openalex.org/W4385572980",
    "https://openalex.org/W2084944215",
    "https://openalex.org/W2135676535",
    "https://openalex.org/W2013091051",
    "https://openalex.org/W2012519870",
    "https://openalex.org/W918203734",
    "https://openalex.org/W3023825030",
    "https://openalex.org/W4302797994",
    "https://openalex.org/W2199134679",
    "https://openalex.org/W4292261888",
    "https://openalex.org/W2555903409",
    "https://openalex.org/W1808011207",
    "https://openalex.org/W2916721057",
    "https://openalex.org/W4221000625",
    "https://openalex.org/W3111869873",
    "https://openalex.org/W67011371",
    "https://openalex.org/W2023531915",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W1924618820",
    "https://openalex.org/W4225108562",
    "https://openalex.org/W2962727507",
    "https://openalex.org/W4386576744",
    "https://openalex.org/W3148399464",
    "https://openalex.org/W4225080353",
    "https://openalex.org/W3203321135",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W3123221944",
    "https://openalex.org/W2970393840",
    "https://openalex.org/W4289494028",
    "https://openalex.org/W2766697724",
    "https://openalex.org/W4235278727",
    "https://openalex.org/W3154248444",
    "https://openalex.org/W4205140928",
    "https://openalex.org/W2478911292",
    "https://openalex.org/W2516678343"
  ],
  "abstract": "Code-generating large language models translate natural language into code.\\nHowever, only a small portion of the infinite space of naturalistic utterances\\nis effective at guiding code generation. For non-expert end-user programmers,\\nlearning this is the challenge of abstraction matching. We examine this\\nchallenge in the specific context of data analysis in spreadsheets, in a system\\nthat maps the users natural language query to Python code using the Codex\\ngenerator, executes the code, and shows the result. We propose grounded\\nabstraction matching, which bridges the abstraction gap by translating the code\\nback into a systematic and predictable naturalistic utterance. In a\\nbetween-subjects, think-aloud study (n=24), we compare grounded abstraction\\nmatching to an ungrounded alternative based on previously established query\\nframing principles. We find that the grounded approach improves end-users'\\nunderstanding of the scope and capabilities of the code-generating model, and\\nthe kind of language needed to use it effectively.\\n",
  "full_text": "â€œWhat It Wants Me To Sayâ€: Bridging the Abstraction Gap\nBetween End-User Programmers and Code-Generating Large\nLanguage Models\nMichael Xieyang Liuâˆ—\nMicrosoft Research,\nCarnegie Mellon University\nUSA\nAdvait Sarkarâˆ—\nMicrosoft Research,\nUniversity of Cambridge,\nUniversity College London\nUK\nCarina Negreanu\nMicrosoft Research\nUK\nBenjamin Zorn\nMicrosoft Research\nUSA\nJack Williams\nMicrosoft Research\nUK\nNeil Toronto\nMicrosoft Research\nUK\nAndrew D. Gordon\nMicrosoft Research\nUK\nABSTRACT\nCode-generating large language models map natural language to\ncode. However, only a small portion of the infinite space of nat-\nuralistic utterances is effective at guiding code generation. For\nnon-expert end-user programmers, learning this is the challenge\nof abstraction matching . We examine this challenge in the specific\ncontext of data analysis in spreadsheets, in a system that maps\nthe userâ€™s natural language query to Python code using the Codex\ngenerator, executes the code, and shows the result. We propose\ngrounded abstraction matching, which bridges the abstraction gap\nby translating the code back into a systematic and predictable natu-\nralistic utterance. In a between-subjects, think-aloud study (n=24),\nwe compare grounded abstraction matching to an ungrounded al-\nternative based on previously established query framing principles.\nWe find that the grounded approach improves end-usersâ€™ under-\nstanding of the scope and capabilities of the code-generating model,\nand the kind of language needed to use it effectively.\nCCS CONCEPTS\nâ€¢ Human-centered computing â†’Natural language interfaces;\nInteractive systems and tools ; Empirical studies in HCI .\nKEYWORDS\nNatural Language Programming, Spreadsheets, Human-AI Interac-\ntion, Large Language Models\nâˆ—Equal contribution.\nCHI â€™23, April 23â€“28, 2023, Hamburg, Germany\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-9421-5/23/04. . . $15.00\nhttps://doi.org/10.1145/3544548.3580817\nACM Reference Format:\nMichael Xieyang Liu, Advait Sarkar, Carina Negreanu, Benjamin Zorn, Jack\nWilliams, Neil Toronto, and Andrew D. Gordon. 2023. â€œWhat It Wants Me\nTo Sayâ€: Bridging the Abstraction Gap Between End-User Programmers\nand Code-Generating Large Language Models. In Proceedings of the 2023\nCHI Conference on Human Factors in Computing Systems (CHI â€™23), April\n23â€“28, 2023, Hamburg, Germany. ACM, New York, NY, USA, 31 pages. https:\n//doi.org/10.1145/3544548.3580817\n1 INTRODUCTION\nProgramming languages are an extremely powerful form of user\ninterface. They also happen to be extremely difficult to learn, espe-\ncially for non-expert end-user programmers who lack training in\ncomputing [48]. What if end-user programmers could instead use\na natural language such as English? Natural language is already\nknown to the user, and ostensibly requires little conscious invest-\nment of effort or learning. This prospect can be realized through\nlarge language models: deep neural networks using the transformer\narchitecture [113], trained on large corpora, fine-tuned to gener-\nate code from natural language. For brevity, we use LLM to mean\ncode-generating large language models. Despite impressive bench-\nmark performance, LLMs are beset with issues in practical use. Lab\nand field studies have shown that the mapping between natural\nlanguage and code is poorly understood, that generated code can\ncontain subtle bugs, and that generated code can be difficult to\nverify [95, 112, 124].\nIn this paper, we consider the specific problem of abstraction\nmatching [95]: when the user has a well-formed intent, how do\nthey select an utterance from the near infinite space of naturalistic\nutterances that they believe the system will reliably map to a sat-\nisfactory solution? This involves â€œmatchingâ€ the utterance to the\nright level of â€œabstractionâ€, by specifying the utterance at a level of\ngranularity and detail that matches the set of actions the system\ncan take, and selecting suitable words and grammar.\nThe abstraction matching problem affects practically every nat-\nural language interface. Solutions (detailed in Section 2) include\nshowing example commands, teaching users techniques such as\n                 \n              \n              \n               \n              \n               \n           \n \n                  \n               \n                \n               \n       \n                  \n               \n                 \n               \n       \nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nCHI â€™23, April 23â€“28, 2023, Hamburg, Germany Liu, et al.\nNaturalistic utterancesSystem actionsâ€œCreate a new column with the yearâ€\nGrounded utterancesâ€œCreate column year, select column date, select characters until character 4â€\n1\n 2\n3\n4 df['yearâ€™]=df['date'].str[:4]\nFigure 1: A summary of the user interaction loop in grounded abstraction matching. The user wishes to extract the year from a\ncolumn of date strings. (1) The user chooses a naturalistic utterance to express their intent. (2) The utterance is mapped to a\npoint in the space of system actions, in this case, a piece of Python code. (3) A grounded utterance is generated, which reflects\nthe system action back to the user. (4) The user observes this utterance, interacts with it, and develops their mental model for\nfuture utterances.\nbreaking down their problem, operating with a restricted vocabu-\nlary and grammar [77], and incorporating other interface elements\n(e.g., graphical menus) to help users formulate their query. Each has\ndrawbacks: examples are not necessarily reflective of user interests\nand do not help the user generalize to a wider range of utterances,\ntutorials take time, and restricted grammar reduces user flexibility.\nWe propose an alternative solution to the abstraction matching\nproblem, which we call grounded abstraction matching (Section 3):\nAn interface supports grounded abstraction match-\ning if the userâ€™s naturalistic utterance is mapped to\na system action, and then mapped back to a natu-\nralistic utterance that is an editable example of how\nto consistently invoke the same action. This gives a\ngrounded example of the level of abstraction at which\nthe system expresses its solutions.\nThe setting of our study. Figure 1 is a sketch of the user interac-\ntion loop we propose. It depicts a concrete example of grounded\nabstraction matching in an interface for doing data analysis tasks in\nspreadsheets. The user selects an utterance (1) to express their intent\nfrom the large space of naturalistic utterances, such asâ€œCreate a new\ncolumn with the yearâ€ . This is translated into the space of system\nactions, which in the example is the space of data analysis code (2).\nThe code is then translatedback into a smaller subspace of grounded\nutterances (3), which are in a consistent format, can reliably be inter-\npreted by the system, and which we wish the user to learn. Finally\n(4), the user observes this grounded utterance (which influences\ntheir future use of the system) and can edit it to refine their query.\nWe investigate grounded abstraction matching in the context of\nend-user programmers solving data analysis tasks in spreadsheets.\nEnd-user programmers stand to benefit greatly from natural lan-\nguage programming, as they often do not have formal training\nin programming [ 48, 94]. On the other hand, their lack of pro-\ngramming expertise exacerbates the abstraction matching problem.\nWithout knowledge of the underlying code generator and avail-\nable APIs, it is much harder to formulate oneâ€™s intent in terms that\ncan be reliably translated to code. The combination of the high\nreal-world value, as well as the acute interaction design challenge,\nmakes this an ideal setting to study the problem. We make the\nfollowing contributions in this paper:\nâ€¢A description of the problem of abstraction matching situated\nin prior work, and a solution: grounded abstraction matching\n(Section 3). We show an example of how this solution can be\ninstantiated in the context of end-user programmers solving\ndata analysis tasks in spreadsheets (Section 4).\nâ€¢We present a user study (ğ‘›= 24) comparing grounded and un-\ngrounded techniques for abstraction matching, showing that\nthe grounded approach improves usersâ€™ abilities to recover\nfrom system failures, and that users gain greater confidence\nand sense of control in using the system (Section 5-6).\nâ€¢Our discussion throws new light on issues of end-user in-\nteraction with large language models. We find design ten-\nsions between prompt language and explanation language,\nand suggest that confusion of subtly different â€œdialectsâ€ in\ndifferent natural language systems may be a future design\nchallenge (Section 8).\n2 BACKGROUND AND RELATED WORK\nNatural language interaction faces many challenges. One of these\nchallenges is abstraction matching: selecting an utterance that will\ntranslate into the desired system action. Various solutions have been\nproposed, but none appears to be targeted towards end-user pro-\ngrammers working with a large language model, or with the goal of\nhelping users develop a mental model of the level of abstraction at\nwhich the LLM operates. Here, we review related problems and solu-\ntions identified in interaction design and machine learning research.\n2.1 Natural language interfaces\nMany usability issues of natural language interfaces can be seen\nas variations, or consequences, of Normanâ€™s â€œgulf of executionâ€\n[40]: the problem of getting the computer to do what you want it\nto do. The issues begin with forming an intent: a user intent may\nbe beyond the capabilities of the system. Luger and Sellen [ 71]\nfind a gulf between user expectations and the practical experience\nof conversational agents, suggesting that they should set realistic\nexpectations to scaffold the learning process, and should reveal the\nsystem capabilities through interaction.\nHowever, a well-formed intent is not enough; the user now faces\nthe problem of targeting a set of system actions that might solve\ntheir problem (termed the selection barrier by Ko et al. [49]). For\nexample, the user of a mobile phone voice assistant, while cooking,\nmay form an intent not to let the egg overcook. This intent can\nbe solved through the system action of setting a 2-minute timer.\nDesign approaches to address the selection barrier include improv-\ning the visibility of the available actions, as in blocks programming\nlanguages [85], where actions are visualized and reified in a virtual\nBridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models CHI â€™23, April 23â€“28, 2023, Hamburg, Germany\ntoolbox. Similarly, â€œmenu-drivenâ€ interfaces for natural language\nallow the user to construct naturalistic queries by pointing [108].\nEven when a tool is selected, there are use barriers and coor-\ndination barriers (in Ko et al. â€™s terms), because the user needs to\nfigure out how to operate the tool. In a programming language, this\namounts to using syntax, APIs, data structures, etc. in the correct\nway [38, 39, 67â€“69], and has been termed thematch-mismatch prob-\nlem [28]. Programming languages, which use a highly constrained\nsyntax, are at a relatively fixed level of abstraction (Green and Petre\n[29] give a working definition of abstraction). But with natural lan-\nguage interfaces, the grammatical possibilities are vast. Consider\nthe variety of ways in which one can express even the relatively\nsimple intent to set a 2-minute timer and the small number of ut-\nterances that typically produce the correct result in contemporary\nvoice assistants. Choosing the right level of abstraction, the right\nlevel of granularity in a command, is called theabstraction matching\nproblem [95].\nIn this work, we focus on abstraction matching, and propose\nan approach to improve the user experience of this problem. We\nacknowledge the other problems that are prior to, but distinct from,\nthe abstraction matching problem, such as the problem of creating\na well-formed intent (i.e., knowing what the system can do), and\nselecting tools that can solve that intent (e.g., knowing that a timer\nwill help you avoid overcooking your egg). Our approach does not\ntarget these issues explicitly , although as we will see, in some in-\nstances it can have indirect benefits for these â€œupstreamâ€ challenges.\nWhen conceptualized as a longer dialog, issues such as conver-\nsational breakdowns, turn-taking, and self-repair emerge [3, 59â€“61,\n102]. We note these issues and intentionally leave them out of scope.\n2.2 Natural language programming\nNatural language (NL) has been seen as an attractive mode of pro-\ngramming due to its (perceived) lower learning requirements. In the\n1960s, there were debates about the suitability of natural language\nas a programming notation [19, 32, 88]. At that time, the focus was\nnot on translating arbitrary intents expressed in natural language\ninto program code, but rather about adopting naturalistic keywords\nand grammars in programming languages, as in AppleScript [17],\nwhere naturalistic statements such as set word to \"Apple\" are\nused instead of more conventional algebraic notation (e.g., let\nword = \"Apple\" in JavaScript).\nWith developments in natural language processing (NLP), the\npossibility emerged of more free-form NL utterances being trans-\nlated into program code (e.g., [66]). The growth of the population\nof non-expert computer users and end-user programmers [48, 100]\ngave added motivation. But NLP technology still had significant\nlimitations, which led to unpredictable user experiences [ 71]. In\nresponse, researchers proposed techniques such as context-limiting,\nand using a reduced vocabulary (i.e., a well-specified subset of nat-\nural language in a tightly defined application context) [77]. Åzcan\net al. [ 82] survey challenges for natural language interfaces for\nquerying data.\nAbstraction matching in large language models. Advances\nin LLMs have led to the ability to solve previously intractable prob-\nlems. For example, generating workflow scripts in mobile appli-\ncations [2], helping non-experts design web pages by translating\nNL requests into CSS properties [47], summarizing code [104], and\nwrangling data [78].\nSarkar et al. [95] review studies of the usability of natural lan-\nguage programming with LLMs in particular (e.g., [43, 105, 112, 123,\n124]) and articulate how LLMs have made the abstraction matching\nproblem â€œfuzzyâ€ (termed â€œfuzzy abstraction matchingâ€ ): while LLMs\ncan interpret a much wider variety of naturalistic utterances than\nearlier models, as a consequence, the space of utterances that may\nbe effective at controlling the model is even more difficult and un-\npredictable for an end-user to learn. While a previous model might\nsimply fail to interpret an utterance, an LLM may interpret it in a\nmanner that is opaque and difficult for the user to generalize from.\nTechnical strategies for improving code generation from\nLLMs. Notable LLMs include GPT-3 [10] and LaMDA [109]. One\napproach to adapt a general LLM for a â€œdownstreamâ€ task such as\ncode generation is to fine-tune a pre-trained LLM by updating part\nof the modelâ€™s weights. For example, Codex [ 16] is a fine-tuned\nversion of GPT-3 for code-related tasks, which we use in our system.\nText generation by an LLM is seeded by a prompt: a sequence of\ntext that somehow describes the desired output.Prompt engineering,\nthe templatized design of such prompts, can improve performance\non the downstream task [70], e.g., code generation. According to\nLiu et al. [70], approaches for prompting include appending rele-\nvant examples [41], appending a table schema [ 110], generating\nmutations of the prompt [64], summarizing complicated prompts\n[52], adding explanations [54], or various manipulations in the em-\nbedding space of the model [1, 46, 114]. Prompting guidelines for\nCodex1 include specifying the language (e.g. Python), libraries (e.g.\nPandas), exploiting comment style (e.g. Python doc strings and the\n# symbol), providing examples for the format or style, or organiz-\ning tasks into functions. Our approach (described in Section 4.1)\ninvolves combining the user query and data into a fragment of\nPython suitable for seeding the model.\nImportantly, prompt engineering is a class of technical solutions\nfor improving model performance; it positions itself as an inter-\nmediate step that augments and modifies any text written by the\nend-user of a natural language interface. Prompt engineering, as\nthe term is used in the machine learning literature, is not therefore\nintended as a user-facing or interaction design concern.\nDesign strategies for improving code generation from\nLLMs. LLMs are capable of learning and generating instructions,\nand breaking down or decomposing a large task into smaller sub-\ntasks has consistently been found to improve the performance of\nLLMs [26, 37, 75, 83, 84, 115, 119, 121, 122]. Jayagopal et al. [ 42]\nexamine the usability of program synthesizers, including GitHub\nCopilot, by novices, finding problems arising from the difficulty of\ntask decomposition, and recommending that designers offer scaf-\nfolding for decomposition. Unlike prior work, which asks the user\nto decompose their request but does not consider how the user\nshould be guided to do so, our approach can be viewed as providing\nan example decomposition of the generated code that the user can\nupdate and resend to the LLM as a set of instructions.\nThe broader area of interactive parsing frames the process of\nconverting natural language to code as a dialogue with the user.\n1https://beta.openai.com/docs/guides/code/best-practices\nCHI â€™23, April 23â€“28, 2023, Hamburg, Germany Liu, et al.\nPrevious work has explored various interaction design possibili-\nties for this dialogue, such as generating clarification questions in\nnatural language [125] or multiple-choice [62], helping users cor-\nrect errors through natural language feedback [22, 76], formalizing\nuser intent as tests [53], interactively â€œnaturalizingâ€ a commanding\nlanguage [116], and guided step-by-step solution generation [101].\nWeisz et al. [120] interviewed programmers about their perceptions\nof neural code translation tools, i.e., models that translate from\none programming language into another, finding that confidence\nhighlighting and alternative translations can improve their utility\nand comprehensibility. Other human-in-the-loop approaches at the\nlevel of the LLM support conversational code generation [81]. Our\napproach differs from these in the important respect that we focus\non non-expert end-users, who have low expertise in programming\n(or none), and in our system users never view generated code, only\nthe output of its execution.\n3 GROUNDED ABSTRACTION MATCHING\nThe key idea of grounded abstraction matching is to systematically\ntranslate the system action (e.g., code) generated from a user query\nback into a naturalistic utterance that is an editable example of how\nto consistently invoke the same action. The resultant utterance is\ngrounded in two ways: it has a direct, systematic correspondence\nto the actual code that the system generated, and it is based on an\nintent the user actually has.\nContrast the grounded approach with tutorial examples typically\ngiven for natural language interfaces, which are ungrounded in\nboth senses. Take for instance an instructional example displayed\nby Microsoftâ€™s Cortana assistant2: â€œWhatâ€™s on my calendar tomor-\nrow?â€. From this example, the user cannot infer a general principle\nfor using the system in different ways. The system cannot answer\nâ€œWhatâ€™s on my bossâ€™s calendar tomorrow?â€, or â€œWhatâ€™s on the tele-\nvision tomorrow?â€. This example is thus ungrounded; it bears no\nstructured resemblance to the code generated by the system. The\nuser experience of such systems is piecemeal; a set of disjoint com-\nmands to invoke distinct capabilities of the system, with little or no\noverarching grammatical structure that can be learned or used to\ninfer the capabilities of the system. The example is also ungrounded\nin the userâ€™s intent: it is only useful to people who use calendars,\nand who often need reminding of tomorrowâ€™s schedule.\nThe grounded abstraction matching approach carves out a space\nof naturalistic utterances, which have a predictable mapping with\nthe space of system actions. We do not force the user to only gen-\nerate utterances in this space. In fact, most utterances, due to the\nsuperior performance of LLMs, have a high likelihood of being\nmapped to something in the action space. However, an utterance\nthat matches the capabilities of the system is more likely to gen-\nerate the desired result (Section 2). By recasting the userâ€™s own\nquery in terms of this subspace, the user is exposed to grounded\nexamples of the kind of utterances, grammar, vocabulary, and level\nof specificity, that is effective at generating the desired system ac-\ntion. Our hypothesis is that exposure to such grounded examples\nleads to more effective and confident use of a natural language\nprogramming interface.\n2https://www.microsoft.com/en-us/cortana/\nThere are several related problems we do not address. First, we\nare not directly addressing the problem of helping the user develop\na well-formed intent in the first place (i.e., understanding what the\nsystem can/cannot do). In Section 6, we see that our grounded ap-\nproach can help, albeit indirectly. Nor are we explicitly guiding the\nuser to decompose a problem into smaller units. Nor are we claiming\nthat natural language programming is more effective for spread-\nsheet users than alternatives (e.g., formulas, charts, pivot tables).\nNor are we attempting toexplain the output of the model [90], al-\nthough again, our results show that the grounded approach can per-\nform some of the functions of explanation. Unlike an explanation,\na grounded utterance must also be an equivalent command for the\nsystem. If explanation was the objective, it does not follow that lan-\nguage is the best form of explanation (we could have used visualiza-\ntion), or that an operation-by-operation restatement of the code is\nappropriate (we could summarize, or explain in greater detail). Both\nHead et al. â€™s Tutorons [34] and Guoâ€™s Python Tutor [31] articulate\nthe aims of code explanation and instantiate particular solutions in\nparticular contexts. Those systems draw on graphical and diagram-\nmatic elements, documentation, etc., to produce an explanation.\nIn other words, our interface does not directly address â€œthis is\nhow the system understood your queryâ€ (input interpretation), nor\nâ€œthis is what the generated code doesâ€ (explanation), but rather â€œthis\nis how you should ask the system to do what the system thinks you\njust asked it to doâ€ (grounded abstraction matching). This differen-\ntiates our design from ostensibly similar designs, such as the query\nparsing (input interpretation) visualizations of Wolfram Alpha [80].\nFinally, our design does not address many aspects of explanation\nand transparency typically considered in explainable AI research\n[36, 65], such as why did the model generate the output it did (and\nwhy not some other output), how did it generate it (what data and\nprocess were used), how confident is the model, how trustworthy\nis the prediction, how biased is the prediction, etc. The tensions\nbetween explanation and abstraction matching led to interesting\ndesign problems, which we discuss in Section 4.2.4.\n4 SYSTEM DESIGN AND IMPLEMENTATION\nWe built two systems, implemented as Microsoft Excel spreadsheet\nadd-ins.3\nâ€¢Both systems share a common code generation, execution,\nand output display pipeline (Section 4.1).\nâ€¢System I implements grounded abstraction matching (Sec-\ntion 4.2).\nâ€¢System II implements an ungrounded yet viable alternative\nsolution to the abstraction matching problem based on pre-\nviously established guidelines for effectively writing queries\nfor large language models (Section 4.3).\n4.1 Shared code generation and output pipeline\nBoth systems share five technical components, summarised in Fig-\nure 2. This figure follows an example where the user has a dataset\nlisting astronauts, the total time they spent in space, and a comma-\nseparated list of missions they participated in. The task is to calcu-\nlate each astronautâ€™s average mission duration in hours. To begin,\n3https://docs.microsoft.com/en-us/office/dev/add-ins/reference/overview/excel-add-\nins-reference-overview\nBridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models CHI â€™23, April 23â€“28, 2023, Hamburg, Germany\nCalculate average mission lengthGo\nâ€¦ â€¦â€¦\nInputA\n # Python 3import pandas as pddf = pd.DataFrame()df['Name'] = ['Joseph Acaba', 'Buzz Aldrin', 'Andrew Allen', 'Neil Armstrong', 'Richard Arnold']df['Space Flight (hr)'] = [3307, 289, 906, 205, 307]df['Missions'] = ['STS-119 (Discovery), ISS-31/32 (Soyuz)', 'Gemini 12, Apollo 11', 'STS-46 (Atlantis), STS-62 (Columbia), STS-75 (Columbia)', 'Gemini 8, Apollo 11', 'STS-119 (Discovery)']# Calculate average mission length\nPromptB B1B2\nB3\nB4\n1\nUser queryA1\nUser tableA2\n df['Mission Length'] =df['Space Flight (hr)'] /df['Missions'].str.count('STS')\nGenerated Python codeC\n2\nUpdated tableE1\nOutputE\nUpdating query via:D System I: Grounded abstracting matching\nSystem II: Guided reframingorD1\n4\n 3\n5\nExcel add-inCodex\nD2\n D3\nFigure 2: System architecture. (1) The user query and table are combined into a prompt. (2) The prompt is passed to Codex to\ngenerate a Python completion. (3) The user table is updated with extra columns or rows or a new table unless a value is returned,\nin which case the value is directly displayed to the user. (4) The user may update their query (in different ways depending on\nwhich system they use, i.e., (D1) or (D2)). (5) The new user query is transformed into a prompt.\nthe user enters the query â€œcalculate average mission length. â€ Here is\nhow that query is processed:\n(1) Conversion of user query to prompt. Figure 2-A&B shows\nhow the textual prompt is generated. Per best practice, we spec-\nify the target language (B1) and libraries (B2). We choosePython\nand Pandas both because of Codexâ€™s high performance on\nPython generation, as well as the large set of operations that are\ntailored to common data analysis tasks, which simplifies much\ndata processing code into the chaining of Pandas API calls. The\nExcel table is converted to a Pandas dataframe (B3). The sys-\ntem assumes a normalized relational table. Finally we append\nthe user query as a comment (B4). Using docstrings instead of\n#-style comments did not affect the performance in our setting.\n(2) Code generation. We call the OpenAI Codex API with our\nprompt and hyperparameters. In particular, we set the tempera-\nture to 0 (to minimize variability), and we set the stop sequence\nto â€œ\\n#â€ (i.e., at the start of a Python comment, as Codex tends\nto delimit self-contained solutions using Python comments).\nFigure 2-C shows a generated Python snippet.\n(3) Code execution and output display. Snippets are then run in\na JavaScript web service sandbox using Pyodide4. This approach\nimproves security and ensures a consistent Python runtime en-\nvironment, since the user may not have an up-to-date version of\nPython (with appropriate libraries) installed. Figure 2-E1 shows\nan extra column added by writing the output from the snippet\nin Step 2 to the spreadsheet grid. If the completionâ€™s output is\na new column or row, we append it to the userâ€™s table. If the\noutput is a single value or a new table, we show it in a side-pane\nonly. The user is not shown the Python snippet.\n(4) User interaction. This manifests in two options, which differ\nbetween System I and System II, as shown in Figure 2-D. In\nSystem I (D1), we generate a grounded utterance for thePython\nsnippet displayed as steps, and in System II (D2), we provide a\n4https://pyodide.org. Pyodide is a Python distribution for WebAssembly.\nsimilar â€œstep-stagingâ€ area where users can do their own prob-\nlem decomposition, or provide additional hints to the system.\nRegardless of the option, the user can edit, add, or delete steps\n(D3). We expand on System I in Section 4.2 and System II in\nSection 4.3.\n(5) Preparation of a new prompt. When the user presses â€œUp-\ndate & Goâ€ (in Figure 2-D3), the steps are concatenated into a\nnew query, loaded into the query box, with which we proceed\nas per Step 1.\n4.2 System I: grounded abstraction matching\n4.2.1 Example usage scenario. Interaction with System I is summa-\nrized in Figure 3.\nSherry is a journalist working on an article about NASA astro-\nnauts, and has gathered data about twenty astronauts in an Excel\nspreadsheet, including columns such as â€œNameâ€, â€œStatusâ€ (whether\nan astronaut is active or retired), â€œSpace Flight (hr)â€, and â€œMissionsâ€.\nâ€œMissionsâ€ is a comma-delimited list of space missions that an as-\ntronaut has participated in, e.g., â€œApollo 11â€, â€œSTS-132 (Atlantis)â€,\nand â€œISS-19/20 (Soyuz)â€.\nSherry wants to calculate an average mission length for ev-\nery astronaut. For example, astronaut Joseph Acaba has a â€œSpace\nFlight (hr)â€ of 3307 and has been on â€œSTS-119 (Discovery), ISS-31/32\n(Soyuz)â€, so his average mission length is (3307 Ã·2 =) 1653.5 hours.\nSherry opens the add-in to the right (Figure 3-A), and types\nâ€œcalculate average mission lengthâ€ into the query box (Figure 3-B1)\nand clicks â€œGoâ€. This results in the generation and execution of a\nPython snippet (via the process in Section 4.1). The generated code\n(Figure 3-G1), which is not visible to Sherry, is:\ndf['Mission Length'] = df['Space Flight (hr)'] /\ndf['Missions'].str.count('STS')â†©â†’\n(1)\nA new â€œMission Lengthâ€ column along with the calculated data\nappears in a green background (Figure 3-F1), which is also shown in\nthe â€œResultsâ€ panel (Figure 3-C) in the sidebar. Sherry notices some\nCHI â€™23, April 23â€“28, 2023, Hamburg, Germany Liu, et al.\nB1 B2\nC\nD1 D2\nF1 F2E df['Mission Length'] =df['Space Flight (hr)'] /(df['Missions'].str.count(',') +1)G2\ndf['Mission Length'] =df['Space Flight (hr)'] /df['Missions'].str.count('STS')G1\nA\nFigure 3: System I interface. Left: the user has entered a query (B1), which has been converted to code (G1) and executed by\nSystem I. The result is visible as a new column in the sheet (F1), and a grounded utterance representing the code is visible (D1).\nRight: the user has edited the grounded utterance via the text boxes (D1 â‡’D2) and submitted it as a new query (B2) by clicking\nthe â€œUpdate & Goâ€ button (E), and new results are visible (F2).\nempty cells, so she suspects there has been an error in interpreting\nor executing her query. She turns to the grounded utterance panel\n(Figure 3-D1).\nThese grounded utterances (generated according to the method\nexplained later in Section 4.2.2) are presented as a series of editable\nsteps (Figure 3-D1). Sherry notices in step 2 (â€œcolumn Space Flight\n(hr) divided by count â€˜STSâ€™ from column Missionsâ€) the system\ninvented a faulty heuristic for counting the number of missions\n(the denominator) by counting the number of occurrences of the\nword â€œSTSâ€ from the â€œMissionsâ€ column. Sherry observes that some\nrows do not contain â€œSTSâ€, which leads to a zero denominator\nand ultimately an empty cell, which also guides her to devise a\ncorrect way of calculating the mission count, namely by counting\nthe number of commas in the â€œMissionsâ€ column and adding 1. After\nediting the second step with the new logic (Figure 3-D2), Sherry\nclicks the â€œUpdate & Goâ€ button (Figure 3-E) to re-run the task,\nwith the query being a concatenation of the updated steps (Figure\n3-B2).\nAfter reading the new result column (Figure 3-F2) and the new\ngrounded utterances (based on the newly generated Python code,\nsee Figure 3-G2), Sherry is convinced that the calculation is correct.\n4.2.2 Systematic grounded utterance generation. We generate grou-\nnded utterances using program analysis. Our algorithm takes as\ninput a Python program that is assumed to use the Pandas library,\nand outputs a sequence of utterances. We structure the algorithm\nin two parts: first, the construction of a task-centric program repre-\nsentation (TCR); and second, the generation of grounded utterances\nusing this representation.\nThe TCR is designed to retain the algorithmic detail of the code\nwhilst reducing ambiguity introduced by the concrete Python rep-\nresentation. Consider the code df['Missions'].str.count('STS') ,\nwhich counts the number of occurrences of'STS' in the 'Missions'\ncolumn. The presence of attribute .str is an artifact of thePandas\nlibrary, meaningless to a user with no Python expertise. Further,\nconsider df['Missions'].str[0] , which extracts the first charac-\nter from the 'Missions' column. There are two syntactic access\nexpressions of the form expr1[expr2] with different meanings;\nthe first represents column projection and the second represents\nstring indexing. A purely syntax-driven utterance will fail to reflect\nthese different meanings.\nWe construct the TCR using a type-directed translation from the\nPython abstract syntax tree. Types are required to resolve identi-\nfiers, such as count , to symbols. We can then associate utterance\ntemplates to each symbol. Types are also used to enrich utterances:\nthe phrases first letter or first word , rather than first element , can\nbe selected depending on the type of expr1 in expr1[0] . At its\ncore, the TCR is a domain-specific language for dataframes which\nincludes row selection, column projection, column extension, vari-\nable binding, and a series of methods that operate on dataframes\nand columns.\nWe derive the utterances through a traversal of the TCR; a\nprocess we refer to as layout. When a TCR operation has a sin-\ngle subject, we present the operation as an instruction and lay-\nout the subject as additional instructions. For example, the code\ndf['Missions'].str.count('STS') is a linear chain of operations,\neach with a single subject, and is therefore presented as a sequence\nof instructions: (1) select column â€œMissionsâ€, (2) calculate count\nâ€œSTSâ€. In contrast, the code\ndf['Space Flight (hr)'] / df['Missions'].str.count('STS') is\nrooted with a binary operator, and is therefore presented as a single\ndescriptive instruction: (1) column â€œSpace Flight (hr)â€ divided by\ncount â€œSTSâ€ from column â€œMissionsâ€. The layout algorithm can\nBridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models CHI â€™23, April 23â€“28, 2023, Hamburg, Germany\ncombine the instructional and descriptive styles, and therefore, the\nresulting utterance for example code (1) mentioned in Section 4.2.1\nis: (1) create column â€œMission Lengthâ€, (2) column â€œSpace Flight\n(hr)â€ divided by count â€œSTSâ€ from column â€œMissionsâ€ .\nOur algorithm only supports a subset of Python constructs and\nthe Pandas library (details in Appendix B); the set is selected based\non their frequency in a benchmark problem set discussed in the\nnext section.\n4.2.3 Round-trip stability. An important validity criterion for a\ngrounded utterance is that running it through the code generation\npipeline generates the same system action in which it is grounded.\nTo test whether our heuristics do in fact generate utterances that\nhave this property, we curated a benchmark dataset of questions\non Stack Overflow5 that requested help solving problems in spread-\nsheets. Each question and answer was distilled into an input table,\na natural language query, and an expected output.\nWe generate a code snippet ğ¶1 from the query, then generate\na grounded utterance for that snippet. We then generate a new\ncode snippet ğ¶2 from the grounded utterance. We test whetherğ¶1\nmatches ğ¶2 (code generation equality) and whether executing ğ¶1\nand ğ¶2 results in the same output (output equivalence). In practice,\ncode generation equality is unnecessarily conservative, as Codex\nrandomly injects statements such as â€œprint(df) â€ that do not affect\nthe output but will lead to trivial code inequality. Output equiva-\nlence, therefore, has more bearing on the user experience.\nWe calculated equivalences in both our synthetic dataset (126\nqueries) and the actual queries submitted by participants during\nour user study tasks (191 queries where a grounded utterance could\nbe generated). In both datasets, the outputs were equivalent approx-\nimately 85% of the time, suggesting that the grounded utterances\nwere sufficiently stable for our study of the principle of ground-\ning. Clearly, these results are not perfect and there is room for\nimprovement, which would be suitable for future work.\n4.2.4 Design tensions in the grounded utterance language. In the\ndesign of the utterances, we faced a tension between using the\nlanguage as an explanation, versus as a querying language. This\ntension manifested as many individual design trade-offs between\noptimizing for user understanding versus tactics for effectively\nguiding the model. For example, the keyword â€œstringâ€ is highly\neffective at specifying the type of textual content for the model,\nyet is meaningless to non-expert end-users. We chose â€œtextâ€ after\nempirically verifying that the model was performant enough to\ninterpret this consistently. We made similar decisions between\nwords for operations such as â€œaverageâ€ (more user-friendly) and\nâ€œmeanâ€ (aligned with thePython function name). An interesting and\ntricky case is array indexing. As Python arrays are zero-indexed,\nthe model can appear inconsistent to non-programmers: asking it\nto produce â€œthe first itemâ€ in an array yields array[0] , but asking\nfor â€œthe item in position 1â€ yields array[1] , which is in fact the\nsecond item in the array. Here we introduced ad-hoc intermediate\nheuristics that identified such references to indices and added 1\nbefore displaying it to the user but subtracted 1 before sending it\nback to the system.\n5https://stackoverflow.com/questions/tagged/excel-formula\nA\nBC\nFigure 4: System II interface. The portion shown replaces the\nsection where grounded utterances are displayed in System I,\notherwise the two systems are identical. The ungrounded sys-\ntem encourages known best practices for prompting LLMs,\nincluding (A) problem decomposition, (B) specifying output\nconstraints, and (C) avoiding reliance on background infor-\nmation.\nThe design of these utterances is a rich space, and while we\nmade some effort to optimize the language for our prototype, we\nemphasize that there is still much work to be done in exploring and\narticulating the various possibilities. Our solution, whilst sufficient\nfor evaluating the concept of grounded abstraction matching, has\nmuch room for improvement, some of which we will discuss in\nSection 8.\n4.3 System II: guidance reframing\nWe could have compared System I to one without any form of user\nsupport for learning how to use the system effectively. However, this\nwould have been a straw man comparison, and a lost opportunity to\nstudy how different solutions to the abstraction matching problem\ncompare. Previous work by Sarkar et al. [95] and Srinivasa Ragavan\net al. [105] has already established the key problems that arise when\nusers are given a system without support, namely: it is difficult for\nusers to recover from errors, to develop a mental model of the\ncapabilities of the system, and to trust the results. We were able\nto verify that these problems also apply to our scenario in a small\nformative study (Appendix C).\nWe thus designed an alternative system that supports users\naccording to established best practices, but where the support is\nnot grounded in the space of system actions or user intent. This\nungrounded version of the system (Figure 4) interprets recently\nestablished techniques for effectively writing queries for LLMs, in\nthe context of end-user programming in spreadsheets with LLMs.\nWe draw upon the following practices:\n(1) Decomposing tasks. LLMs are effective at mapping simple\nproblems to code, e.g. those which can be expressed as a few\nCHI â€™23, April 23â€“28, 2023, Hamburg, Germany Liu, et al.\nAPI or function calls, but less effective in multi-step reasoning,\nwhich requires correctly decomposing a problem [5, 75, 122].\nThus, we prompt users to â€œDescribe the task in stepsâ€ using the\nsame step-staging UI as System I (Figure 4-A). Users decompose\nthe task using their own logic (contrasted with System I, where\nthe steps can be bootstrapped through the grounded utterances).\n(2) Specifying output constraints. Prior work [63, 75, 79, 103,\n110] has found that specifying explicit textual statements of\noutput constraints, such as the intended shape (a single value, a\ncolumn, or a table, etc.) and data type (number, boolean, string,\netc.) can improve performance, as the code corpora used for fine-\ntuning these LLMs contain comments (likely documentation\nstrings) that specify the output type.\nThus, users are asked to specify the shape (from â€œa single valueâ€\nto â€œnew columnsâ€ or â€œnew tablesâ€) and type (â€œnumber(s)â€, â€œchar-\nacter(s) or word(s)â€, or â€œtrue/falseâ€) of the system output via the\ncontrols under the prompt â€œI expect the result to beâ€ (Figure 4-B).\n(3) Avoiding reliance on background information. While there\nis evidence showing that code-generating LLMs can use knowl-\nedge about the world learned from natural language corpora\nwhen solving natural language tasks, it is unreliable and can\nlead to unanticipated side effects [75].\nThus, a â€œFinal checkâ€ toggle asks users to confirm that their\nquery (given the data in the spreadsheet) does not use terms\nand concepts that require additional knowledge to interpret\n(Figure 4-C). This toggleâ€™s purpose is to ensure that the user\nconsciously designs their query to avoid requiring background\ninformation, and does not affect the prompt.\nThe information in these sections is concatenated and passed as\na prompt to the model, as in System I.\n5 USER STUDY: HOW DOES GROUNDING\nAFFECT ABSTRACTION MATCHING?\nWe designed a study to explore the effect of grounding on the prob-\nlem of abstraction matching. Participants completed tasks modeled\nafter real-world spreadsheet problems. We examined the queries\nthey submitted, think-aloud data, and questionnaire responses, and\naddress the following questions:\nâ€¢In what ways does our system fail to correctly interpret the\nuser intent?\nâ€¢How do participants rewrite their queries in response to\nfailures, to make progress on the tasks?\nâ€¢How does grounding affect user perceptions of the utility of\nsuch systems, their confidence and trust in the system, and\ntheir mental models?\n5.1 Participants\nWe recruited a purposive sample [24] through emails and social\nmedia. We selected for a diversity of backgrounds, including prior\nspreadsheet experience, formula writing experience, and program-\nming experience. Experience was measured using a previously\ndeveloped spreadsheet expertise questionnaire [93]. Participants\nwere required to be over 18 years of age and fluent in English.\nWe recruited 24 participants (1 non-binary, 9 women, 14 men)\nacross 11 industries. Fourteen participants were 25-34 years old,\nfour aged 35-44, four aged 18-24, and two aged 45-54. Of these, half\nself-reported having some experience with basic spreadsheet usage\nwhile the other half reported having a lot of experience and having\nused at least some advanced features. Eight participants reported\nknowing a few basic functions (such as SUM and AVERAGE) in their\nspreadsheet formulas, nine reported having knowledge of advanced\nfunctions but rarely used them and preferred basic functions when\nwriting formulas, and the remaining seven reported having built\na wide variety of formulas or VBA functions. Three participants\nreported having little or no programming experience, seven re-\nported having limited knowledge of programming to use it for\nsmall infrequent tasks (this is common for spreadsheet users [93],\nand falls within the level of expertise typical of non-expert end-user\nprogrammers [48]), nine reported being moderately experienced\nin programming and wrote code regularly, and the remaining five\nreported being highly experienced in programming.\n5.2 Study protocol\nWe chose a between-subjects design, where participants were strat-\nified and then randomly assigned to either the grounded or the\nungrounded condition (the obvious learning effect of the interfaces\nrules out a within-subjects approach). Groups were balanced in\nterms of gender and prior experience. Concretely, Table 1 shows the\nsimilar gender distribution, median spreadsheet expertise, spread-\nsheet formula expertise, and programming expertise (as per our\nscreening questionnaire and its integer coding scheme, taken from\nprior work [93]) between the two conditions.\nParticipants first signed a consent form and completed the demo-\ngraphic survey. Participants then spent 5 minutes discussing their\nexperience with, and typical use of, spreadsheets. They were shown\na brief tutorial explaining the user interface elements of the system\nused in their respective conditions. Participants then completed\nan example task, to increase their familiarity with the system and\nmitigate order and learning effects in the remainder of the tasks.\nWe designed three tasks based on real-world spreadsheet ques-\ntions on Stack Overflow and other similar question-answering fo-\nrums. For each task, participants were presented with a textual\ndescription of the task and a data table (of roughly 25 rows), which\nthey read before starting the task. They were asked to think aloud\n[27] while completing the task. The experimenter answered ques-\ntions about the task objectives and intervened to help participants\nrecover from bugs in the system implementation, but did not inter-\nvene otherwise.\nThe first task was to identify how many times the city of New\nOrleans had won Super Bowl games (an American sporting event)\ngiven a dataset of Super Bowl records. The second was to calculate\na column of average mission duration given a dataset of astronaut\nspace flights (similar to the example in Section 4.2.1). The third was\nto calculate a column that checked whether a house satisfied three\ncriteria, given a dataset of houses.\nParticipants were given 15 minutes per task, progressing if they\nfinished early. Task completion was determined by the partici-\npantâ€™s own judgement, and notifying the experimenter. In rare\ncases, participants believed they had succeeded but had slightly\nmisinterpreted the task (e.g., not counting a house built in 1970 as\npart of the class of houses built after 1970). We considered such\ncases as successes and did not intervene, since the participant had\na well-formed intent but had simply misread the question.\nBridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models CHI â€™23, April 23â€“28, 2023, Hamburg, Germany\nTable 1: Group characteristics in experimental conditions: groups are evenly matched.\nCondition Gender Programming expertiseSpreadsheet expertiseFormula expertise\n(non-binary/woman/man)(median) (median) (median)\nGrounded 1/4/7 3.5 3.5 4\nUngrounded 0/5/7 4 3.5 4\nAvoiding priming. We needed to avoid framing the task objec-\ntives in a way that strongly influenced user queries. While some\namount of priming is inevitable, we considered multiple options\nthat might reduce the influence of the task instructions. Presenting\nthe task objective pictorially, by showing a screenshot of the de-\nsired output column, made it too difficult to infer the task objective.\nPhrasing the task differently for each participant would eliminate\nan overall bias, but might still bias each participant in idiosyncratic\nways. Our solution was verbosity and circumlocution: we described\nthe problem indirectly and in a long-winded manner, which we\nexpected would encourage participants to formulate their initial\nquery in their own terms. This strategy was effective in practice\n(detailed in Section 6). Details of the task descriptions and datasets\nare given in Appendix D.\nAfter the tasks, participants completed the NASA TLX [33] and\nSystem Usability Scale [58] questionnaires, and engaged in a semi-\nstructured interview probing the perceived effectiveness of the\nsystem, their practices around formulating and refining queries, de-\nbugging and verifying the system-generated results, and scenarios\nwhere they thought the system would be useful and not useful.\nEach session took approximately 65 minutes, conducted via Mi-\ncrosoft Teams video conferencing software, with the participants\nremotely controlling the experimenterâ€™s computer, a designated\nThinkpad laptop with Microsoft Excel and the prototype systems\ninstalled. Sessions were screen and audio recorded. Participants\nwere compensated USD $25 or local currency equivalent. The study\nwas approved by our institutionâ€™s ethics review board.\n5.3 Data segmentation and analysis\nWe transcribed think-aloud remarks and the post-study interviews\nfor all participants. Each participantâ€™s transcript was segmented\ninto remarks made: 1) before the tutorial task, 2) during the tutorial\ntask, 3) during the first task, 4) the second task, 5) the third task,\n6) post-study interview. The transcripts for the tasks were further\nsegmented into â€œquery episodesâ€, each starting when the user sub-\nmits a query and ending when the user submits the next query (or\nthe task is complete). Thus each query episode is a single loop of\nthe user submitting a query, observing and checking the systemâ€™s\noutput, and figuring out how to proceed if the task is still unsolved.\nThese segments and query episodes were augmented with teleme-\ntry, consisting of the query the user entered, whether the user sub-\nmitted the query from the query box or from the step-staging area,\nthe generated code (invisible to the user), the generated output (vis-\nible to the user), the generated grounded utterance (in the grounded\ncondition), and any errors.\nEach augmented episode was analyzed using iterative open cod-\ning [111] in accordance with Braun and Clarkeâ€™s thematic analysis\n[9]. Think-aloud and post-experiment interview data were further\nanalyzed to identify comments relating to the use of language, trust,\nand confidence in the model.\nTask episodes were coded intofailure modes and rewriting strate-\ngies. To generate a codebook, two researchers began by indepen-\ndently open-coding the same set of 52 query episodes (data from six\nparticipants). Together they generated 22 and 25 proto-codes for\nfailure modes and rewrite strategies, which, after discussion and\nnegotiations, led to an initial codebook of 14 failure mode codes and\n17 rewrite strategies. The researchers then independently re-coded\nthe same sample with the codebook. Manual inspection showed\npoor inter-rater agreement, consistent with initial open coding\nrounds in prior studies [11, 14, 106]. The two researchers then dis-\ncussed disagreements and ambiguities and revised the codebook.\nThe final codebook (Appendix A) consisted of 12 failure modes and\n16 rewrite strategies; 14 code definitions were updated and three\nwere merged from the initial codebook following two rounds of\nnegotiations between researchers.\nWith this final codebook, the two researchers independently\ncoded the entire set. The agreement on failure modes was 98% (re-\nsearchers disagreed in 7 out of 293 query episodes), and on rewrite\nstrategies was 57.3%. To achieve a high level of negotiated agree-\nment [73, 87], the two researchers manually negotiated each dis-\nagreement until 100% agreement was reached on the coded set. Our\nuse of negotiated agreement was with the intent to make quan-\ntitative comparisons between code frequencies in the grounded\nand ungrounded conditions. While more work may be needed to\nestablish the reusability of the codebook as an independent analysis\ndevice, the shared agreement established is sufficient for us to draw\nreliable conclusions about our specific dataset. This aligns with Mc-\nDonald et al. â€™s guidelines for reliability in CSCW and HCI research\n[73]. To connect our findings to our codebook, direct references\nto codes are presented in bold . Finally, the researchers grouped\ncodes into larger themes, discussed overall findings, and selected\nrepresentative query episodes and quotes.\nTable 2: Number of queries issued until first solution.\nTask Grounded(meanÂ±standard deviation) Ungrounded\n1 3.17 Â±2.15 2.33 Â±1.31\n2 3.50 Â±1.61 3.33 Â±2.75\n3 4.50 Â±3.07 2.42 Â±1.80\n6 RESULTS\n6.1 Task completion and queries\nAll participants completed every task successfully. Participants com-\npleted the three tasks in similar amounts of time in both grounded\n(mean = 32 minutes 38 seconds, ğœ = 7 minutes 35 seconds) and un-\ngrounded (mean = 27 minutes 51 seconds, ğœ = 7 minutes 8 seconds)\nconditions. The difference was not statistically significant using a\nt-test (ğ‘¡(ğ‘‹)= 1.592,ğ‘ = 0.1256). We include this analysis of time\ntaken only as an additional description of the difficulty of our tasks;\nowing to the variable effects of a think-aloud protocol on timing,\nCHI â€™23, April 23â€“28, 2023, Hamburg, Germany Liu, et al.\nTable 3: Response to NASA TLX items. Format: median (mean Â±standard deviation)\nCondition Mental demand Physical demand Temporal demand Performance Effort Frustration\nGrounded 4.0 (4.17Â±1.46) 0.5 (1.08Â±1.44) 2.0 (2.25Â±1.59) 8.5 (8.50Â±0.96) 3.5 (4.08Â±1.80) 1.0 (1.50Â±1.71)\nUngrounded 3.0 (3.75Â±2.71) 1.0 (1.75Â±2.24) 1.5 (1.58Â±1.61) 8.5 (8.42Â±1.55) 4.0 (3.92Â±2.66) 0.5 (1.83Â±2.58)\nTable 4: Response to System Usability Scale items. Format: median (mean Â±standard deviation)\nQuestion categoryStatement Grounded Ungrounded\nComprehensibilityI would consider my interactions with the tool to be understandable and clear. 2 (1.25Â±1.36) 2 (1.25Â±1.30)\nLearnability I would consider it easy for me to learn how to use this tool. 2 (1.50Â±1.38) 2 (1.83Â±0.37)\nEnjoyability I enjoyed the features provided by the tool. 2 (1.42Â±1.38) 2 (1.67Â±0.62)\nApplicability Using this tool would make solving spreadsheet problems at work more efficient and effective.2 (1.08Â±1.66) 2 (1.42Â±0.86)\nRecommendabilityIf possible, I would recommend the tool to my friends and colleagues. 2 (1.58Â±1.38) 2 (1.58Â±0.64)\nwe cannot draw conclusions on the direct effect of condition on\ntask completion time.\nParticipants required similar numbers of attempts (i.e., issued\nsimilar numbers of queries) to solve each task in both grounded and\nungrounded conditions. This is shown in Table 2. The difference\nbetween conditions is not statistically significant. As in the case\nof task 3, the grounded approach can even increase the number of\nqueries. Our qualitative analysis will show why this is the case, and\nwhy it is not necessarily the disadvantage it may appear.\nOur strategy to avoid priming the usersâ€™ initial queries was to\nmake the task description circumlocutory, as detailed in Section 5.\nTo validate whether this strategy worked, we measure the homo-\ngeneity of the initial queries submitted by users for each task. The\nprinciple is that the stronger the priming effect, the more similar\nusersâ€™ initial queries will be, having been biased towards certain\nwords or phrases by the task description. We chose a simple metric,\nthe Levenshtein distance [ 57], computed between every pair of\ninitial queries for a given task (i.e., (24 Ã—23)Ã· 2 = 276 unique\ninitial query pairs per task). The median Levenshtein distance for\nthe tasks are 44, 61, and 88.5 respectively. These are 66%, 72%, and\n86% of the median length in characters of the initial queries to each\ntask. This can be interpreted roughly as follows: the typical pair\nof initial queries share only 14-34% of their textual content; they\nare mostly distinct and contain unique content. We consider this as\nhaving successfully avoided priming.\n6.2 Usability and cognitive load questionnaires\nParticipants filled the NASA TLX [33] cognitive load scale and the\nSystem Usability Scale (SUS) [58] questionnaires after completing\nthe tasks. SUS Likert items were integer-coded on a scale from -2\n(strongly disagree) to +2 (strongly agree). The median response\nvalues are presented in Tables 3 and 4. Across both systems, partic-\nipants reported a low to moderate perceived cognitive load, high\nperceived performance, and high usability.\nThe distributions of responses for each item in the TLX and\nSUS in the grounded and ungrounded conditions were compared\nusing the Mann-Whitney U test [74]. Similarly, we compared the\ndistributions of responses between participants with low and high\nprogramming experience (defined as those who responded in cat-\negories 1-3 and 4-5 respectively, to the programming experience\nitem in our questionnaire). We compared the distributions of re-\nsponses between gender groups. In all cases, we found no significant\ndifferences.\nThe fact that we did not detect a significant quantitative dif-\nference in cognitive load or standardized usability between the\nconditions is not surprising, given the high success rate in both\nconditions and the similar number of attempts needed for success.\nIt speaks to the strength of the reframing principles embodied in the\nungrounded condition, and shows that at least according to these\nmetrics, the ungrounded condition is a strong, viable alternative\ncondition and not just a straw man. However, this apparent non-\nresult obscures significant qualitative differences between the two\nconditions revealed both by the strategies adopted by participants\nto overcome model failures, as well as in the development of their\nmental model and sense of agency, which we shall report in the\nfollowing sections.\n6.3 Failure modes\nOur analysis revealed twelve different types of model failures, i.e.,\nreasons why the output did not satisfy the userâ€™s intent. These could\nbe (loosely) organized into the following four themes: technical\nfailures, input failures, output failures, and logic failures. These\nwere not mutually exclusive; a given query episode may result in\nmultiple failures simultaneously. We classify these failure modes\nwith full visibility of the generated code, which was not visible to\nthe user, and as such the failure mode is not necessarily apparent\nto the user. The frequency of each failure mode is presented in\nFigure 5. There was no statistically significant difference between\nthe failure mode distributions in the grounded and ungrounded\nconditions. Extended examples of each type of failure are given in\nAppendix E.\n6.3.1 Technical failures. Technical failures are limitations of our\ncode generation pipeline. In generation failure, no code is gen-\nerated. In execution failure , code is generated but cannot be\nexecuted by the prototype. The user experience of each of these\ntypes of error is largely the same; the user gets no feedback except\nfor a generic error message. Technical failures were uncommon,\noccurring in 55 query episodes (18.8% of the total).\n6.3.2 Input failures. In input-related failures, the generated code\noperated on the wrong columns as input. This could either be\na wrong input column selected when the correct column was\nexplicitly specified in the query, or an instance of soft wrong\ninput column, when the correct column was not specified and the\nmodel failed to infer the correct one.\nBridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models CHI â€™23, April 23â€“28, 2023, Hamburg, Germany\nFigure 5: Relative frequency of failure modes by condition (blue: grounded, striped green: ungrounded).\n6.3.3 Output failures. Similarly, in output-related failures, the gen-\nerated code produced errors in the output format. The model might\nattempt to overwrite existing columns (though overwriting original\ndata is forbidden in our current implementation) when specifically\nasked to generate new columns, or attempt to make a soft over-\nwrite failure, when the request for a new column is implicit or\nambiguous. It may generate extra columns which were not re-\nquested, or there may be missing columns which were requested.\n6.3.4 Logic failures. Logic failures are errors in the algorithm cho-\nsen by the model to solve the problem. Arguably input and output\nfailures, being part of the generated code, are also logic failures,\nbut here we loosely apply the term â€œlogicâ€ to mean the method\nof computing the result. A logic failure may be a partial answer,\nwhich solves a portion of what the user requested, and which the\nuser might potentially build upon to solve the remainder of the\ntask in a subsequent query. Or, it may beraw data output , directly\nhallucinating data values to return (which may or may not be cor-\nrect), rather than generating code to calculate those values. It may\nbe the wrong heuristic for calculating an intermediate step when\na method for computing it has not been specified or suggested by\nthe user. Failures that do not fall into any of the above categories\nare coded as other incorrect.\n6.4 Query rewriting strategies\nHow did users cope with model failures? According to decision\nsupport theories of interactive AI system design [ 50, 89], users\nare constantly deciding what to do next when interacting with an\nimperfect AI system, in particular when the modelâ€™s results are\nunsatisfactory. In this view, the role of system design is to provide\nthe information required to help the user decide what to do next.\nWe, therefore, focus on the differences between consecutive\nquery attempts, and characterize the rewriting strategies that par-\nticipants applied in the expectation that the changes made would\nsolve the model failure, or at least help them make progress towards\nachieving their goal.\nOur analysis revealed sixteen distinct rewriting strategies, which\ncan be organized into four themes: scoping, elaboration, language\nrestructuring, and intent shaping. These rewriting strategies were\nnot mutually exclusive; the difference from one version of a query\nto the next might involve the application of multiple strategies.\n6.4.1 Scoping. In scoping changes, participants either chose to\nstrictly add steps to their previous query, asking the system to do\nan additional prior, intermediate, or subsequent step; or conversely,\nthey chose to reduce scope , strictly removing a step that was\nimplicitly or explicitly present in the previous query. Some examples\nare given in Table 5.\n6.4.2 Elaboration. In elaboration changes, participants added more\ndetail to their queries. They could either elaborate how to per-\nform a certain calculation or computation by specifying a method.\nOr they could elaborate what they wanted in more detail with-\nout necessarily specifying how to compute it. Common ways of\nadding more detail about what was required were tospecify input\ncolumns that the system should use, or specify output type such\nas new column or number, or toname output columns . Examples\nare in Table 6.\n6.4.3 Language restructuring. Language restructuring consisted\nof changes to the grammar (syntax) and vocabulary used in the\nquery. Users might break down a query into individual clauses that\nspecify more clearly a computational step. This could have either\nbeen a self-breakdown, using the utterance area of the proto-\ntype to stage a sequence of steps, or in the grounded condition it\ncould involve partial or full reuse (of the) system breakdown .\nIn the grounded condition, participants exposed to the grounded\nutterances may adopt system-like language in subsequent queries\n(i.e., manually entering a query with grammar or vocabulary that\nmimics the grounded utterances). Or in both conditions, partici-\npants may include code-like syntax, such as quotes, parentheses,\nor even keywords from other programming languages. These are\nillustrated in Table 7.\nCode-like syntax and system-like language were effective in\nimproving the model output. This shows that the space of utterances\nthat effectively controls code-generating models is not merely a\nsubset of natural language, it is a naturalistic space that may contain\nelements of natural language and code. This was not lost on our\nparticipants. One participant (P1) remarked, while entering the\nquery â€œrows where basement not zeroâ€ (i.e., find houses which have\na basement) that it would not make sense spoken aloud: â€œthat one\nactually may be a bit less actual language, but [...] I feel like the\nsystem will understand kind of what I mean [...] I donâ€™t have to think\ntoo much about how I would actually say that in a way that would\nmake sense [to someone else]. â€\n6.4.4 Intent shaping. The final theme consists of rewriting strate-\ngies that reflect a shift in the userâ€™s overall intent (i.e., what they\nwant the system to do), or which help the user evaluate their intent\nwith respect to the capabilities of the system. They may discard a\nCHI â€™23, April 23â€“28, 2023, Hamburg, Germany Liu, et al.\nTable 5: Examples of scoping changes. Orange and green highlights indicate differences ( removals and additions, respectively)\nbetween a query and its follow-up query (the same color scheme applies to Table 6-8 as well).\nRewrite StrategyPrevious Query (Participant-Task) Follow-up Query\nAdd steps Define mission_count by splitting Missions by â€˜, â€™(P6-2) â‡’ Define mission_count by splitting Missions by â€˜, â€™, thendivide\nSpaceFlightbymission_count\nReduce scope Create column good where year built is greater than or equal to 1970AND\nsquarefootbasementis not0 andyearrenovatedis not0 (P9-3)\nâ‡’ Create column good where year built is greater than or equal\nto 1970\nTable 6: Examples of elaboration changes.\nRewrite StrategyPrevious Query (Participant-Task) Follow-up Query\nElaborate how\nAdd a column oftheaverageflighthourof eachmissionforeach\nastronaut(P5-2)\nâ‡’\nAdd a column ofthevalueoftotalspaceflighthourdividedbythe\nnumberofmissionsforeachastronaut\ndf['Average Flight Hour'] = [1653.5, 190, 167, 407, 289,\n302, 313, 309, 147, 1001.25, 297, 289.5, 423.5, 319.5,\n205, 307, 327.5, 168.5, 343, 482.5, 2537.5, 366.5, 190]\nâ†©â†’\nâ†©â†’\ndf['Average Mission Time'] = df['Space Flight (hr)'] /\ndf['Missions'].str.count('\\(')â†©â†’\nElaborate what\nhow many superbowls has thecityofNewOrleans won(P27-1)\nâ‡’\nhow many superbowls hasNewOrleansSaints won\ndf[df['Host City'] == 'New Orleans'].shape[0] df[df['Winner'] == 'New Orleans Saints'].shape[0]\nName output\ncolumns\nReturn true if year built >=1970 AND basement >0 and renovated\nTRUE(P11-3)\nâ‡’ Returnreviewcolumn where if year built >=1970 AND basement >0\nAND renovated >0\nSpecify input\ncolumns\nHow many super bowls has New Orleans won(P9-1) â‡’ Selectcolumnâ€œwinnerâ€ where text includes new orleans\ndf[df['Host City'] == 'New Orleans']['Winner'].count() df[df['Winner'].str.contains('New Orleans')]\nSpecify output\ntype\nSelect rows where column yr_built greater than 1970 and column\nyr_renovated NptEq 0 and column sqft_basement NotEq 0(P16-3)\nâ‡’ Createa column where column yr_built greater than 1970 and col-\numn yr_renovated NotEq 0 and column sqft_basement NotEq 0\nTable 7: Examples of language restructuringchanges.\nRewrite StrategyPrevious Query (Participant-Task) Follow-up Query\nSelf breakdown Countthenumberofrowswithâ€˜NewOrleansâ€™inthewinnercolumn\n(P12-1)\nâ‡’ (1)Createanewcolumncalledcitythatdropsthelastwordinthewinner\ncolumn,(2)Countthenumberofrowsincitythatsayâ€œNewOrleansâ€\nReuse system\nbreakdown\nThe number of superbowls the city of New Orleans has won(P5-1)\nâ‡’\n(1) select rows where columnWinner is New Orleans Saints, (2) count\ndf[df['Host City'] =='New Orleans']['Winner'].count() df[df['Winner'] =='New Orleans Saints'].count()\n(1) select rows where columnHostCity is New Orleans,\n(2) selectcolumnWinner,\n(3) count.\n(1) select rows where column Winner is New Orleans Saints,\n(2) count.\nSystem-like how many super bowls has New Orleans won(P9-1) â‡’ selectcolumn winnerwhere textincludes new orleans\nCode-like syntaxCreate a column that shows the value of Space Flight (hr) divided by\nthe number of items of the Missions column(P26-2)\nâ‡’ (1) create column Space Flight (hr) per Mission, (2) column Space Flight\n(hr) divided by(count, fromcolumnMissions+ 1)\nTable 8: Examples of intent shapingchanges.\nRewrite StrategyPrevious Query (Participant-Task) Follow-up Query\nNew intent Select rows where basement > 0 and yr_built >= 1970 and\nyr_renovated > 0(P21-3)\nâ‡’ Createacolumncalledconsider?wherethevalueistrueifbasement>0 and\nyr_built>=1970andyr_renovated>0\nStart over Usewinnercolumntosubtractthewinnerteamcolumntogetthe\nwinnercity(P22-1)\nâ‡’ Createa newwinnercitythatremovethewinnerteamfromthewinner\ncolumn\nNext step Count the number of mission in column I delimited by comma(P24-2) â‡’ Foreachnamecalculatethehoursofspaceflightdividedbythemissioncount\nTesting Createanewcolumnthatcountshowmanystringsareseparatedby\nâ€œ, â€intheâ€œMissionsâ€column.Createanothercolumnthatcalculates\nâ€œSpaceFlight(hr)â€dividedbyâ€œMissions_Countâ€(P25-2)\nâ‡’ newcolumn:#ofstringsseparatedbyâ€œ, â€inâ€œMissionsâ€.Createanothercolumn\nthatcalculatesâ€œSpaceFlight(hr)â€dividedbyâ€œMissions_Countâ€\nstrategy for solving the problem or change their interpretation of\nthe problem and form an entirely new intent, or they may start\nover with the same intent but with a completely fresh expression\nof that intent. They may choose to partition the problem into a\nseries of tasks, and when one task is solved, they may move on to\nthe next step . Finally, they may write testing queries to probe\nthe systemâ€™s capabilities and improve their own understanding.\nExamples are in Table 8. Rewrite strategies that did not fit any of\nthe above categories were marked as other rephrase.\n6.4.5 Rewriting strategies differ between grounded and ungrounded\nconditions. The frequency of each rewrite strategy code is presented\nin Figure 6. There are some differences between the frequencies in\nthe grounded and ungrounded conditions. One trivial difference\nBridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models CHI â€™23, April 23â€“28, 2023, Hamburg, Germany\nFigure 6: Frequency of rewrite strategies per task and overall.\nis that the grounded condition enables the reuse system break-\ndown and system-like language restructuring strategies. These\nare naturally absent from the ungrounded condition.\nNot including the trivially different codes, the overall distribution\nof code frequencies in the grounded condition is different from\nthe ungrounded condition with statistical significance ( ğœ’2 (13)=\n70.3,ğ‘ = 7.1 Â·10âˆ’7). In the following paragraphs, we will focus on\ndifferences observed in the frequencies of the next step, reduce\nscope, and start over strategies, which, combined with qualitative\nanalysis of the think-aloud data, explain some of the key advantages\nof the grounded strategy.\nWe see greater use of the next step strategy in the grounded\ncondition and particularly in task 3. This is because participants\nwere able to recognize a partial answer much more effectively using\nthe grounded utterance, and they were more likely to choose to\nbuild upon the partial answer in later steps. In task 3 (finding houses\nto satisfy 3 criteria), the model would often give a partial answer by\nonly satisfying 1 or 2 criteria, or by giving 3 separate columns and\nnot combining them. When faced with this scenario, participants\nin the grounded condition were more likely to recognize this as\na partial answer due to the grounded utterance making it explicit\nwhat the system had done. In contrast, in the ungrounded condition,\nparticipants were more likely to disregard such apparently partial\nanswers and attempt a self-breakdown or introduce clarity via\ncode-like syntax.\nThe reduce scope strategy, though rare in absolute terms (prob-\nably because the tasks were of small scope to begin with), oc-\ncurred relatively much more often in the grounded condition. The\ngrounded language, as hypothesized, served as a reference point for\nthe granularity of command that is achievable by the system, i.e.,\nthe complexity that can be expressed in a typicalPython statement.\nThe start over strategy is particularly interesting, because it\nwas more common in the grounded condition in task 1, but in the\nungrounded condition in tasks 2 and 3. This is because the decision\nto start over is a cost-benefit tradeoff: the cost of starting over is\nhigh in longer and more complex tasks (such as tasks 2 and 3) but\nis lower in simple tasks (such as task 1). The grounded utterances\nhelped participants evaluate this tradeoff more effectively; in task\n1, it was easier for participants to understand the system failure\nand start over, but in tasks 2 and 3, it was easier for participants to\nunderstand the system failure and adapt their query, thus avoiding\nthe need to start over. Conversely, without the feedback of the\ngrounded utterances, participants in task 1 were stuck with ineffec-\ntive approaches longer than necessary, and in tasks 2 and 3 were\nlikely to abandon results that were partially correct because they\ncould not recognize them as such.\n6.5 Perceived utility of grounded utterances,\nand their effects on trust and mental models\nFrom think-aloud comments and semi-structured interviews, we\nfound that grounded utterances facilitated explanation and debug-\nging, increased usersâ€™ trust and confidence, and shaped usersâ€™ men-\ntal models of system capabilities.\n6.5.1 Grounded utterances facilitated explanation and debugging.\nDespite the fact that explanation was not our main objective, grounded\nutterances gave participants a way of comprehending the systemâ€™s\nbehavior, by manifesting the systemâ€™s problem-solving logic and\nkey information. P3 remarked that â€œthe breakdown would help me\njust be able to check that what Iâ€™ve typed in did actually make sense\nto the system and that it did actually do what I was hoping. â€ P1 ex-\nplained that â€œ[the breakdown] helps me understand whatâ€™s going on,\nand therefore whether the results is going to be accurate or not, â€ and\nparticipants imagined it being increasingly valuable with larger ta-\nbles, where it is infeasible to manually check all rows and columns.\nThe presentation of the grounded utterances as isolated steps,\neach with reduced scope and standardized explanation language,\nexposed multiple entry points for users to identify and repair bugs.\nIn general, participants thought that the grounded utterancesâ€œmade\nit easy to check your workâ€ (P4), highly â€œprogrammableâ€ (P5), and\nâ€œproviding opportunities for you to modify and iterate over itâ€ (P26).\nGrounding for debugging was especially useful for participants with\nvery little or no programming experience. For example, P24 reflected\nthat â€œin the New Orleans [task], immediately, I saw that it picked from\nthe â€˜Host Cityâ€™, and I knew this is why youâ€™re giving me [an incorrect\nanswer] and then I had to redirect it to go look at the â€˜Winnerâ€™ col-\numn. â€Even a few exposures to the grounded utterances influenced\nCHI â€™23, April 23â€“28, 2023, Hamburg, Germany Liu, et al.\nparticipantsâ€™ rewrite strategies (discussed in Section 6.4), as opposed\nto having to employ guesswork to debug in the ungrounded case.\n6.5.2 Grounded utterances increased usersâ€™ trust and confidence. In\naddition to the ease of comprehension and debugging discussed in\nSection 6.5.1, participants commented on the fact that the grounded\nutterances aligned with their â€œintuitions on how to solve a problemâ€\n(P22). For example, P13, who reported having little prior program-\nming experience, recalled thatâ€œthe step-by-step approach felt natural\nand very much mirrored what I usually do, which is to create these\ntemporary columns as I go along. â€ Meanwhile, participants with\nprogramming experience also thought the grounded utterances felt\nfamiliar, e.g., â€œitâ€™s like a more natural language version of SQLâ€ (P16).\nParticipants, especially non-programmers, also felt that having\naccess to the grounded utterances made it easier for them to trust\nthe output, for example,â€œthe more you use [grounded utterances], the\nmore confident youâ€™ll get to the values, and you get less worriedâ€ (P24).\nFurthermore, despite having some prior programming experience,\nP5 imagined a future in which she wouldâ€œrely more and more on the\nadd-in to do my work. â€ In general, the informative signals from the\ngrounded utterances contributed to comprehension, debugging, and\na sense of intuitiveness and familiarity, which in turn contributed\nto participantsâ€™ trust in the systemâ€™s behavior and confidence in\ntheir ability to steer the system toward a desirable result.\n6.5.3 Grounded utterances shaped usersâ€™ mental models of system\ncapabilities. Interacting with the grounded utterances enabled par-\nticipants to develop mental models of the systemâ€™s capabilities and\nlimitations. To some (8/12), being specific about what is being asked\nfor helped with successful LLM generations, reflected in rewrite\nstrategies such as elaborate what, name output columns , spec-\nify input columns , and specify output type . P6 explained his\nchoice of mentioning the column name in his queries for task 3: â€œI\nwent to the exact column name because thatâ€™s a useful reference [for\nthe system], and I donâ€™t super trust the system to be able to semanti-\ncally determined that a renovation is connected to â€˜year_renovatedâ€™. â€\nParticipants picked up vocabularies and styles of utterance from\nthe grounded utterances, which would reliably get the system to\nwork according to their intent (7/12). This was particularly helpful\nfor non-programmers, for example, P13 recalled that through inter-\nacting with the breakdowns, â€œI can see what itâ€™s working with, what\nwords and language and vocabulary itâ€™s working with, and then I can\nkind of shift my understanding of what it wants me to say or what it\nunderstands the best. â€ Meanwhile, some participants even attempted\nto map the breakdown texts to programming or script languages\nthat they were already familiar with, for example, â€œwhen I read and\n[subsequently] wrote words like â€˜selectâ€™, I was very much thinking\nabout SQLâ€ (P9) and â€œthis [grounded utterance] would be very similar\nto how I would tackle it in R. â€ Participants understood such styles\nof utterance to be â€œreusable and transferableâ€ (P16) across different\ntasks, i.e., they could generalize from grounded utterances to form\na predictable notion of a commanding language.\nMuch like re-purposing spreadsheet formulas [44], 4/12 partic-\nipants saw the value of keeping track of the grounded utterances\nof successful LLM generations so that they can act as â€œinformal\ndocumentationâ€ (P26) for future selves and collaborators to better\nunderstand the original intent and the systemâ€™s calculation process.\n7 LIMITATIONS\nWe only generate grounded utterances for a subset of the Pandas/\nPython APIs, selected by frequency in our benchmark dataset. This\nwas sufficient for our study: an explanation failure occurred only\n13 times out of 159 queries in the grounded condition, on average\n1.08 times per user. We do not claim that our algorithm is the\nmost effective, and future work could explore alternative ways of\nproducing grounded utterances, such as leveraging the LLM itself\n[45, 72]. Our system is limited in assuming a single well-defined\nrelational data table. Future work may investigate handling multiple\ntables [126] or automatic table detection [20].\nWe chose the reframing principles from previous work as a viable\nalternative to our grounded approach, but there are other options,\nsuch as examples or tutorials. We chose an interface that does not\nrequire the users to encounter any information over and above the\ntext in the user interface, on the basis that in a realistic commercial\nspreadsheet feature, the user is not interested in learning and prefers\nto develop skills through usage (the â€œparadox of the active userâ€\n[12]). However, this is not a concern for every natural language\ninterface, and future work should explore alternatives.\nA decision support loop restricted to rewriting queries is not the\nonly possibility; there is a large design space which may increase the\ndecision surface to the user, including data-oriented (â€œobservation-\nlevelâ€) interactions [23, 98] (such as indicating incorrect outputs\n[91, 96], or manually giving correct examples [ 30]), or access to\nadjust parameters of the model [51, 92] such as temperature. In our\nprototype, the usability of naturalistic utterances as a method for\ncontrolling a large language model is the central concern. Thus\nrewriting and resubmitting a query is the only available response\nto a model failure in our study. This allowed us to inspect the use\nof language closely, but future work may explore the interaction of\nlanguage with these alternatives.\nThe data tables in the study consisted of 20-30 rows. In practice,\nspreadsheet data analysis can contain much fewer or more rows.\nWith large datasets, it becomes impossible to verify the modelâ€™s\noutput for each row. A large dataset would have unnecessarily\nincreased the complexity of the debugging task, which would con-\nfound our investigation of grounding, and which we leave for future\nwork.\nA lab study cannot capture long-term effects, which become\napparent with days, weeks, or months of use. What appears to be\na clear advantage of one approach over another may erode with\nuser practice and learning. What appears to be an insignificant\ndifference may compound over time to create a marked gap. To\nlongitudinally validate our findings, future work may conduct diary\nstudies [86] or experience sampling [18].\nMany of our participants had prior expertise in formulas and pro-\ngramming. It is likely that most users donot regularly use formulas,\nbased on corpus estimates [97]. Our sample reflects the important\nsegment of spreadsheet users who do write formulas, and because\nthe interaction design of our system avoids any direct inspection\nor authoring of code, we have reason to believe that some of our\nfindings might generalize to non-programmers as well. Future work\nmay explore specifically how grounded abstraction matching might\nhelp users without any prior exposure to formulas or programming.\nBridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models CHI â€™23, April 23â€“28, 2023, Hamburg, Germany\n8 DISCUSSION\n8.1 Comparison with related work\nOur findings throw additional light on prior work. Setlur and Tory\n[102] evaluated the interaction design of a chatbot for data analyt-\nics. We further explore how a non-expert end-user, working with a\nnatural language interface in a data analytics setting, can be guided\nto query the system more effectively. Our rewrite strategies expand\nupon their classification of follow-up utterances into simplification\nand clarification categories, and we also expand their analysis of fail-\nure cases. They observed the need to â€œsupport query expressibilityâ€ ,\nwhich we directly address with grounded utterances. Our findings\nsupport their observation that â€œpredictability [...] for handling dif-\nferent types of analytical questions further enhanced peopleâ€™s trustâ€ .\nOur findings also support certain conclusions of Jayagopal et al.\n[42], in particular, their analysis of Lauâ€™s design guidelines for PBD\nsystems [56] in the context of LLM-based code generation. Our\nfindings support guidelines 2: â€œMake it easy to correct the systemâ€ , 3:\nâ€œEncourage trust by presenting a model users can understandâ€ : as we\nobserved in Section 6.5. Our use case shows a new facet of guideline\n5: â€œConsider the perceived value of automationâ€ , which considers the\ncost-benefit trade-off of the system as a whole. We found that even\nwithin each episode of use, there are smaller cost-benefit trade-offs,\nsuch as whether to continue refining a particular query or start over.\nRagavan et al. â€™s study of natural language formulas in the spread-\nsheet grid [105] is a close precedent to our work, as it shares the\napplication domain (data analysis in spreadsheets), target end-user,\nand natural language interface. We find significant commonalities.\nFor instance, our participants also clearly stated the ease of use of\nnatural language as an advantage compared to spreadsheet formulas\nor programming language. Their analysis of failure cases identified\ncauses such as using incorrect or ambiguous words for concepts, or\nphrases requiring background knowledge to understand: our data\nshows how both grounding and established reframing principles\ncan help users avoid, detect, and recover from such issues. We also\nextend their work. They acknowledge the limitations of their sys-\ntem that â€œIn cases of [...] errors, a user needs to understand how the\nintelligence apparatus has interpreted their utterance, and how they\ncan fix itâ€ , and that â€œusers need a way to [...] provide an alternative\nphrasing for the task, when the intent is misinterpretedâ€ ; our findings\nshow what happens when these are provided, namely: that the abil-\nity to rewrite the query manifests in several rewriting behaviors,\nand an intelligibility mechanism (such as grounded utterances) can\nshape these behaviors.\n8.2 The tutorial value of grounded utterances\nA recurring theme from participant feedback was that grounded\nutterances could not only be directly reused on future tasks with\nthe same goal, but also serve as examples of canonical ways of\nexpressing intent, much like example code for calling an API in\ndocumentation [68]. Some participants who are not expert spread-\nsheet or formula users felt it much more straightforward to â€œwork\nwith natural language than trying to recall or search for that exact\nformula for formula combinationâ€ to perform data analysis tasks\n(P5), observing that â€œeven if I canâ€™t figure out what Excel functions to\nuse, at least now I know how to sort of just say what I want in English\nand make sure to hit those keywords like â€˜create a columnâ€™, â€˜split byâ€™,\nâ€˜select rows where â€â€™ (P26).\nThe grounded utterances can serve as an â€œeducational toolâ€ (P22)\nfor learning logical thinking and problem decomposition skills. This\nhas two advantages: 1) the step-by-step utterances are grounded in\na sequence of API calls in the generated code that are by themselves\nprimitive building blocks for computation; 2) LLMs are trained on\nlarge corpora of data and are usually able to â€œtranslateâ€ a reason-\nably specified initial intent by a user into some logical code, as\ndocumented in Section 2 and evidenced by our system.\nThe way that an LLM solves a task may be suboptimal (e.g., being\ntime and space inefficient, reflecting poor practices in coding, etc.),\nand could lead to users being misguided downstream. However,\nin our study, grounded utterances were presented as a series of\neditable steps exposing the abstractions of the LLM-generated code,\nwhich enabled them to reason about model failures (Section 6.3)\nand edit the utterances to fix model mistakes (Section 6.4). Helping\nlearners build mental models for chained functions is a topic of in-\nterest in educational tools, and future work may explore alternative\nvisualizations, such as those in DS.js [128], or Pandas Tutor [55].\n8.3 Genres of naturalistic commanding\nThe notional language of queries that users form is surprisingly easy\nto influence. We observed that even a single exposure to a grounded\nutterance can change the grammar and vocabulary of subsequent\nqueries. Participants looked for cues in the task question (although\nthey were largely thwarted by our strategy of circumlocution), in\nthe language of the interface, and in the language used by the ex-\nperimenter. Moreover, they draw upon their previous experiences\nof querying search engines, their expertise (if available) in formula\nprogramming and other programming languages, and syntax from\nalgebraic notation. The process by which users form a coherent\nquery language from these disparate, fragmented influences re-\nsembles the process of creolization as articulated by Bickerton [6].\nLanguage bioprogramming theory [7], which assumes an innate\ncapacity for grammar, may be a partial explanation for how users\ntransform their priming into a structured language.\nHowever, the similarity is superficial: creolization typically oc-\ncurs at a generational timescale, whereas users of these systems\nresolve their influences into a querying style within minutes. Unlike\na creole, the user is the only speaker of their particular query lan-\nguage. We observed substantial individual differences in querying\nstyles (without substantial differences in success rate). Thus each in-\ndividual appears to develop their own â€œspeech genresâ€, per Bakhtin\n[4]. This flexibility can be an advantage, but also a disadvantage\nwhen it comes to collaboration and communication; as several par-\nticipants noted, the query forms a part of the documentation of\nthe spreadsheet to be passed to collaborators for comprehension.\nBut this is another manifestation of the common fallacy committed\nby spreadsheet authors [106]: that what is intelligible to them is\ntherefore also intelligible to a different reader.\nNonetheless, it is possible that through collaboration, certain\nnorms, standards, and best practices might emerge, which will lead\ngroups of individuals to a shared style of naturalistic commanding.\nHowever, these will be slightly different for every system and its\nmethod for generating grounded utterances, which may be vendor-\nspecific. The user might learn one style of naturalistic commanding\nCHI â€™23, April 23â€“28, 2023, Hamburg, Germany Liu, et al.\nfor a spreadsheet software made by one company, but another for\ndatabase software made by another company. This confusion of\nâ€œdialects, â€ if we may call them such, is a potential interaction design\nchallenge for the future of these interfaces.\n8.4 Applications of grounded abstraction\nmatching\nGrounded abstraction matching is a general technique for familiar-\nizing users with the space of utterances effective for commanding\nany particular language model. Here we have applied it to a system\nwhere the space of system actions is short Python data analysis\nprograms using the Pandas library.\nEven within spreadsheets, there are other applications. For exam-\nple, a query seeking a particular presentation style in a spreadsheet,\nsuch as coloring alternate rows in red, could be solved using spread-\nsheet presentation scripting APIs, and grounded utterances could\nhelp the user learn how to express their preferred styles. Similarly,\na query such as â€œgenerate a scatterplot for each data series and\nmark all negative ğ‘¥ values on the plots in redâ€, could result in the\ngeneration of a Vega-Lite [99] visualization, and grounded utter-\nances could be used to allow the user to learn the space of effective\nutterances and grammar of visualizations.\nOutside of spreadsheets, grounded utterances could be integrated\ninto the feedback loop for commercial voice assistants, chatbots, or\nsoftware that rely on naturalistic queries, such as search engines.\nWhile most LLM applications currently use naturalistic utter-\nances as their input space, there may be applications where the\ninput space is not a linguistic notation. For example, as part of\nan accessibility device, an LLM may be used to â€œtranslateâ€ from a\nspace of non-lingual speech sounds, or a space of body movements,\nor gestures, into a space of system actions. Here again, grounded\nâ€œutterancesâ€ can orient the user to an effective use of the LLM.\n8.5 Continued applicability of grounded\nabstraction matching as LLMs evolve\nAt the time of writing, LLMs are improving at a rapid pace, with\nyearly dataset and parameter scaling consistently achieving emer-\ngent properties [ 118]. Based on current research trajectories, it\nis not unreasonable to forecast that LLMs will improve in two\ndirections: they will be more frequently able to offer â€œzero-shotâ€\nsolutions (i.e., without bespoke fine-tuning) in current scenarios,\nand they will be able to tackle new scenarios.\nLLMs can now provide support in many scenarios that were\npreviously intractable [8]. The technical capability has gone from\nnot being able to offer any assistance to being able to offer a wide\nvariety of compelling and viable, yet poorly-understood and un-\npredictable assistance. The key challenge for interaction design,\ntherefore, is finding appropriate application domains, and helping\nusers make the best use of LLMs while accounting for their limita-\ntions. The specific problem of abstraction matching, along with the\nrelated problems of explanation and trust, are unlikely to disappear\nwith better performance; rather as LLMs become more performant\nthey are likely to be applied in increasingly complex and high-risk\napplications. The interaction design principle of grounded abstrac-\ntion matching is generic, yet prescriptive enough to be helpful to\nsystem designers in many of these situations.\n8.6 Implications for design\nOur work is an early exploration of grounded utterances as a solu-\ntion to the abstraction matching problem. It is not straightforward\nnor entirely appropriate to directly prescribe implications for design\n[21, 107]. Nonetheless, some of our findings may help design.\nAbstraction matching becomes a serious challenge when the\nspace of system actions is large (as with Python code). Fuzzy ab-\nstraction matching becomes a serious challenge when the language\nunderstanding model is highly performant but still highly unpre-\ndictable (as is Codex). In such situations, grounded abstraction\nmatching can be a systematic and effective way of getting users\nfamiliar with a naturalistic commanding language.\nDesigners should be aware of language priming cues in the user\ninterface and also other cues that the user base is exposed to, e.g.,\nfrom other software, programming languages, search engines, etc.\nthat they commonly use. These priming cues can be built upon,\ne.g., by borrowing naturalistic keywords from languages such as\nSQL. Conversely, a mismatch between environmental cues and the\nlanguage needed to effectively command your system can interfere\nwith the development of mental models.\nGrounded utterances have the potential to serve multiple func-\ntions simultaneously: as tutorial examples for the user, as input\ninterpretation, and as system behavior explanations. Ideally, a nat-\nural language interface would treat each of these separately, but in\ncommercial tools such as spreadsheets, the user will not expect to\nattend to many different categories of feedback just to get their data\nanalysis done, and could get overwhelmed. Thus a consideration for\nfeedback from any natural language interfaces ought to be whether\nit can serve â€œdouble dutyâ€ as grounded examples (testable through\nround-trip experiments) as well as explanations.\n9 CONCLUSION\nAbstraction matching, selecting a natural language utterance that\nis likely to be understood correctly by the system, is a core problem\nfacing users of almost all natural language interfaces. We propose\ngrounded abstraction matching , in which grounded examples of such\neffective utterances are systematically generated and shown to the\nuser. We present a concrete instantiation of grounded abstraction\nmatching in a system that helps non-expert end-user programmers\nperform data analysis in spreadsheets.\nIn a study comparing this approach to an ungrounded alternative,\nwe find that the grounded approach has many positive effects on\nthe strategies available to end-users to cope with model failures. We\nfind that over time, exposure to grounded examples leads to a more\nconsistent mental model, greater confidence, and perception of trust\nin the system. There are many avenues for future work, including\nstudying the effect of such grounding in dialogue, in other contexts\nbesides spreadsheets, studying the effects over longitudinal usage,\nand how different â€œdialectsâ€ of naturalistic language that arise due\nto differences between users and between systems might interact.\nACKNOWLEDGMENTS\nWe would like to thank our study participants for their kind partici-\npation. We sincerely thank Sruti Srinivasa Ragavan, Ian Drosos, and\nSherry Tongshuang Wu for their insightful feedback and constant\nsupport.\nBridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models CHI â€™23, April 23â€“28, 2023, Hamburg, Germany\nREFERENCES\n[1] Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen,\nNanning Zheng, and Jian-Guang Lou. 2022. Input-Tuning: Adapting Unfamiliar\nInputs to Frozen Pretrained Models. ArXiv abs/2203.03131 (2022).\n[2] Deniz Arsan, Ali Zaidi, Aravind Sagar, and Ranjitha Kumar. 2021. App-Based\nTask Shortcuts for Virtual Assistants. In The 34th Annual ACM Symposium on\nUser Interface Software and Technology . 1089â€“1099.\n[3] Zahra Ashktorab, Mohit Jain, Q Vera Liao, and Justin D Weisz. 2019. Resilient\nchatbots: Repair strategy preferences for conversational breakdowns. In Pro-\nceedings of the 2019 CHI conference on human factors in computing systems .\n1â€“12.\n[4] Mikhail Bakhtin and Ghodrat GhÂ¯asemipour. 2011. The problem of speech genres.\nLiterary Criticism 4, 15 (2011), 114â€“136.\n[5] Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On\nMeaning, Form, and Understanding in the Age of Data. InProceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics . Association\nfor Computational Linguistics, Online, 5185â€“5198. https://doi.org/10.18653/v1/\n2020.acl-main.463\n[6] Derek Bickerton. 1983. Creole languages. Scientific American 249, 1 (1983),\n116â€“123.\n[7] Derek Bickerton. 1984. The language bioprogram hypothesis. Behavioral and\nbrain sciences 7, 2 (1984), 173â€“188.\n[8] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,\nSydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, et al. 2021. On the opportunities and risks of foundation models.\narXiv preprint arXiv:2108.07258 (2021).\n[9] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology.\nQualitative research in psychology 3, 2 (2006), 77â€“101.\n[10] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\nArXiv abs/2005.14165 (2020).\n[11] John L Campbell, Charles Quincy, Jordan Osserman, and Ove K Pedersen. 2013.\nCoding in-depth semistructured interviews: Problems of unitization and inter-\ncoder reliability and agreement. Sociological methods & research 42, 3 (2013),\n294â€“320.\n[12] John M Carroll and Mary Beth Rosson. 1987. Paradox of the active user. In\nInterfacing thought: Cognitive aspects of human-computer interaction . 80â€“111.\n[13] Federico Cassano, John Gouwar, Daniel Nguyen, Sy Duy Nguyen, Luna Phipps-\nCostin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson,\nMolly Q. Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. 2022.\nA Scalable and Extensible Approach to Benchmarking NL2Code for 18 Program-\nming Languages. ArXiv abs/2208.08227 (2022).\n[14] George Chalhoub and Advait Sarkar. 2022. â€œItâ€™s Freedom to Put Things Where\nMy Mind Wantsâ€: Understanding and Improving the User Experience of Struc-\nturing Data in Spreadsheets. InProceedings of the 2022 CHI Conference on Human\nFactors in Computing Systems (CHI â€™22) . Association for Computing Machinery,\nNew York, NY, USA, 1â€“24. https://doi.org/10.1145/3491102.3501833\n[15] Kathy Charmaz. 2006. Constructing Grounded Theory: A Practical Guide through\nQualitative Analysis. SAGE. Google-Books-ID: 2ThdBAAAQBAJ.\n[16] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde\nde Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy\nKhlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,\nMikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens\nWinter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plap-\npert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin,\nSuchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford,\nMatthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welin-\nder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wo-\njciech Zaremba. 2021. Evaluating Large Language Models Trained on Code.\nhttps://doi.org/10.48550/ARXIV.2107.03374\n[17] William R Cook. 2007. Applescript. In Proceedings of the third ACM SIGPLAN\nconference on History of programming languages . 1â€“1.\n[18] Mihaly Csikszentmihalyi and Reed Larson. 2014. Validity and reliability of the\nexperience-sampling method. In Flow and the foundations of positive psychology .\nSpringer, 35â€“54.\n[19] Edsger W Dijkstra. 1979. On the foolishness of\" natural language programming\".\nProgram construction (1979), 51â€“53.\n[20] Haoyu Dong, Shijie Liu, Shi Han, Zhouyu Fu, and Dongmei Zhang. 2019. Ta-\nbleSense: Spreadsheet Table Detection with Convolutional Neural Networks.\nProceedings of the AAAI Conference on Artificial Intelligence 33, 01 (July 2019),\n69â€“76. https://doi.org/10.1609/aaai.v33i01.330169 Number: 01.\n[21] Paul Dourish. 2006. Implications for design. In Proceedings of the SIGCHI confer-\nence on Human Factors in computing systems . 541â€“550.\n[22] Ahmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney,\nGonzalo A. Ramos, and Ahmed Hassan Awadallah. 2021. NL-EDIT: Correcting\nSemantic Parse Errors through Natural Language Interaction. In NAACL.\n[23] Alex Endert, Chao Han, Dipayan Maiti, Leanna House, and Chris North. 2011.\nObservation-level interaction with statistical models for visual analytics. In\n2011 IEEE conference on visual analytics science and technology (VAST) . IEEE,\n121â€“130.\n[24] Ilker Etikan, Sulaiman Abubakar Musa, Rukayya Sunusi Alkassim, et al. 2016.\nComparison of convenience sampling and purposive sampling.American journal\nof theoretical and applied statistics 5, 1 (2016), 1â€“4.\n[25] Mikhail Evtikhiev, Egor Bogomolov, Yaroslav Sokolov, and Timofey Bryksin.\n2022. Out of the BLEU: how should we assess quality of the Code Generation\nmodels? ArXiv abs/2208.03133 (2022).\n[26] Matthew Finlayson, Kyle Richardson, Ashish Sabharwal, and Peter Clark. 2022.\nWhat Makes Instruction Learning Hard? An Investigation and a New Challenge\nin a Synthetic Environment. ArXiv abs/2204.09148 (2022).\n[27] Marsha E Fonteyn, Benjamin Kuipers, and Susan J Grobe. 1993. A description\nof think aloud method and protocol analysis. Qualitative health research 3, 4\n(1993), 430â€“441.\n[28] TRG Green, M Petre, and RKE Bellamy. 1991. Comprehensibility of Vi-\nsual and Textual Programs: A Test of Superlativism Against theâ€™Match-\nMismatchâ€™Conjecture. Empirical Studies of Programming: Fourth Workshop. J.\nKoenemann-Belliveau, TG Moher and SP Robertson. New Brunswick.\n[29] Thomas R. G. Green and Marian Petre. 1996. Usability analysis of visual pro-\ngramming environments: a â€˜cognitive dimensionsâ€™ framework. Journal of Visual\nLanguages & Computing 7, 2 (1996), 131â€“174.\n[30] Sumit Gulwani. 2011. Automating string processing in spreadsheets using\ninput-output examples. ACM Sigplan Notices 46, 1 (2011), 317â€“330.\n[31] Philip J Guo. 2013. Online python tutor: embeddable web-based program\nvisualization for cs education. InProceeding of the 44th ACM technical symposium\non Computer science education . 579â€“584.\n[32] Mark Halpern. 1966. Foundations of the Case for Natural-Language Program-\nming. In Proceedings of the November 7-10, 1966, Fall Joint Computer Conference\n(San Francisco, California) (AFIPS â€™66 (Fall)) . Association for Computing Ma-\nchinery, New York, NY, USA, 639â€“649. https://doi.org/10.1145/1464291.1464360\n[33] Sandra G Hart and Lowell E Staveland. 1988. Development of NASA-TLX\n(Task Load Index): Results of empirical and theoretical research. In Advances in\npsychology. Vol. 52. Elsevier, 139â€“183.\n[34] Andrew Head, Codanda Appachu, Marti A Hearst, and BjÃ¶rn Hartmann. 2015.\nTutorons: Generating context-relevant, on-demand explanations and demon-\nstrations of online code. In 2015 IEEE Symposium on Visual Languages and\nHuman-Centric Computing (VL/HCC) . IEEE, 3â€“12.\n[35] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora,\nEthan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Xiaodong Song, and\nJacob Steinhardt. 2021. Measuring Coding Challenge Competence With APPS.\nArXiv abs/2105.09938 (2021).\n[36] Andreas Holzinger. 2018. From machine learning to explainable AI. In 2018\nworld symposium on digital intelligence for systems and machines (DISA) . IEEE,\n55â€“66.\n[37] Or Honovich, Uri Shaham, Samuel R. Bowman, and Omer Levy. 2022. Instruction\nInduction: From Few Examples to Natural Language Task Descriptions. ArXiv\nabs/2205.10782 (2022).\n[38] Amber Horvath, Michael Xieyang Liu, River Hendriksen, Connor Shannon,\nEmma Paterson, Kazi Jawad, Andrew Macvean, and Brad A Myers. 2022. Un-\nderstanding How Programmers Can Use Annotations on Documentation. In\nProceedings of the 2022 CHI Conference on Human Factors in Computing Systems\n(CHI â€™22) . Association for Computing Machinery, New York, NY, USA, 1â€“16.\nhttps://doi.org/10.1145/3491102.3502095\n[39] Jane Hsieh, Michael Xieyang Liu, Brad A. Myers, and Aniket Kittur. 2018. An\nExploratory Study of Web Foraging to Understand and Support Programming\nDecisions. In 2018 IEEE Symposium on Visual Languages and Human-Centric\nComputing (VL/HCC) . 305â€“306. https://doi.org/10.1109/VLHCC.2018.8506517\nISSN: 1943-6092.\n[40] Edwin L Hutchins, James D Hollan, and Donald A Norman. 1985. Direct manip-\nulation interfaces. Humanâ€“computer interaction 1, 4 (1985), 311â€“338.\n[41] Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh\nParthasarathy, Sriram Rajamani, and Rahul Sharma. 2022. Jigsaw: Large Lan-\nguage Models meet Program Synthesis. In International Conference on Software\nEngineering (ICSE) . https://www.microsoft.com/en-us/research/publication/\njigsaw-large-language-models-meet-program-synthesis/\n[42] Dhanya Jayagopal, Justin Lubin, and Sarah E Chasins. 2022. Exploring the\nLearnability of Program Synthesizers by Novice Programmers. In User Interface\nSoftware and Technology (UISTâ€™22) .\nCHI â€™23, April 23â€“28, 2023, Hamburg, Germany Liu, et al.\n[43] Ellen Jiang, Edwin Toh, Alejandra Molina, Kristen Olson, Claire Kayacik, Aaron\nDonsbach, Carrie J Cai, and Michael Terry. 2022. Discovering the Syntax and\nStrategies of Natural Language Programming with Generative Language Models.\nIn CHI Conference on Human Factors in Computing Systems . 1â€“19.\n[44] Nima Joharizadeh, Advait Sarkar, Andrew D. Gordon, and Jack Williams. 2020.\nGridlets: Reusing Spreadsheet Grids. In Extended Abstracts of the 2020 CHI\nConference on Human Factors in Computing Systems (CHI EA â€™20) . Association\nfor Computing Machinery, New York, NY, USA, 1â€“7. https://doi.org/10.1145/\n3334480.3382806\n[45] Junaed Younus Khan and Gias Uddin. 2022. Automatic Code Documenta-\ntion Generation Using GPT-3. https://doi.org/10.48550/arXiv.2209.02235\narXiv:2209.02235 [cs].\n[46] Daniel Khashabi, Shan Lyu, Sewon Min, Lianhui Qin, Kyle Richardson, Sameer\nSingh, Sean Welleck, Hannaneh Hajishirzi, Tushar Khot, Ashish Sabharwal,\nand Yejin Choi. 2022. Prompt Waywardness: The Curious Case of Discretized\nInterpretation of Continuous Prompts. In NAACL.\n[47] Tae Soo Kim, DaEun Choi, Yoonseo Choi, and Juho Kim. 2022. Stylette: Styling\nthe Web with Natural Language. In CHI Conference on Human Factors in Com-\nputing Systems . 1â€“17.\n[48] Amy J. Ko, Robin Abraham, Laura Beckwith, Alan Blackwell, Margaret Burnett,\nMartin Erwig, Chris Scaffidi, Joseph Lawrance, Henry Lieberman, Brad Myers,\nMary Beth Rosson, Gregg Rothermel, Mary Shaw, and Susan Wiedenbeck. 2011.\nThe State of the Art in End-user Software Engineering. ACM Comput. Surv. 43,\n3 (April 2011), 21:1â€“21:44. https://doi.org/10.1145/1922649.1922658\n[49] Amy J. Ko, Brad A. Myers, and Htet Htet Aung. 2004. Six Learning Barriers in\nEnd-User Programming Systems. In Proceedings of the 2004 IEEE Symposium\non Visual Languages - Human Centric Computing (VLHCC â€™04) . IEEE Computer\nSociety, Washington, DC, USA, 199â€“206. https://doi.org/10.1109/VLHCC.2004.\n47\n[50] Rafal Kocielnik, Saleema Amershi, and Paul N Bennett. 2019. Will you accept an\nimperfect ai? exploring designs for adjusting end-user expectations of ai systems.\nIn Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems .\n1â€“14.\n[51] Todd Kulesza, Margaret Burnett, Weng-Keen Wong, and Simone Stumpf. 2015.\nPrinciples of explanatory debugging to personalize interactive machine learning.\nIn Proceedings of the 20th international conference on intelligent user interfaces .\n126â€“137.\n[52] Kirby Kuznia, Swaroop Mishra, Mihir Parmar, and Chitta Baral. 2022. Less is\nMore: Summary of Long Instructions is Better for Program Synthesis. ArXiv\nabs/2203.08597 (2022).\n[53] Shuvendu K. Lahiri, Aaditya Naik, Georgios Sakkas, Piali Choudhury, Curtis von\nVeh, Madan Musuvathi, Jeevana Priya Inala, Chenglong Wang, and Jianfeng Gao.\n2022. Interactive Code Generation via Test-Driven User-Intent Formalization.\nArXiv abs/2208.05950 (2022).\n[54] Andrew Kyle Lampinen, Ishita Dasgupta, Stephanie C. Y. Chan, Kory Matthew-\nson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane X.\nWang, and Felix Hill. 2022. Can language models learn from explanations in\ncontext? ArXiv abs/2204.02329 (2022).\n[55] Sam Lau and Philip Guo. 2022. Pandas tutor visualizes how python code trans-\nforms dataframes. https://pandastutor.com/\n[56] Tessa Lau. 2009. Why programming-by-demonstration systems fail: Lessons\nlearned for usable ai. AI Magazine 30, 4 (2009), 65â€“65.\n[57] Vladimir I Levenshtein et al. 1966. Binary codes capable of correcting deletions,\ninsertions, and reversals. In Soviet physics doklady , Vol. 10. Soviet Union, 707â€“\n710.\n[58] James R. Lewis. 2018. The System Usability Scale: Past, Present, and Future.\nInternational Journal of Humanâ€“Computer Interaction 34, 7 (July 2018), 577â€“\n590. https://doi.org/10.1080/10447318.2018.1455307 Publisher: Taylor & Francis\n_eprint: https://doi.org/10.1080/10447318.2018.1455307.\n[59] Toby Jia-Jun Li, Amos Azaria, and Brad A. Myers. 2017. SUGILITE: Creat-\ning Multimodal Smartphone Automation by Demonstration. In Proceedings of\nthe 2017 CHI Conference on Human Factors in Computing Systems (CHI â€™17) .\nAssociation for Computing Machinery, Denver, Colorado, USA, 6038â€“6049.\nhttps://doi.org/10.1145/3025453.3025483\n[60] Toby Jia-Jun Li, Igor Labutov, Xiaohan Nancy Li, Xiaoyi Zhang, Wenze Shi,\nWanling Ding, Tom M. Mitchell, and Brad A. Myers. 2018. APPINITE: A\nMulti-Modal Interface for Specifying Data Descriptions in Programming by\nDemonstration Using Natural Language Instructions. In 2018 IEEE Sympo-\nsium on Visual Languages and Human-Centric Computing (VL/HCC) . 105â€“114.\nhttps://doi.org/10.1109/VLHCC.2018.8506506 ISSN: 1943-6106.\n[61] Toby Jia-Jun Li, Marissa Radensky, Justin Jia, Kirielle Singarajah, Tom M.\nMitchell, and Brad A. Myers. 2019. PUMICE: A Multi-Modal Agent that Learns\nConcepts and Conditionals from Natural Language and Demonstrations. In\nProceedings of the 32nd Annual ACM Symposium on User Interface Software and\nTechnology (UIST â€™19) . Association for Computing Machinery, New Orleans, LA,\nUSA, 577â€“589. https://doi.org/10.1145/3332165.3347899\n[62] Yuntao Li, Bei Chen, Qian Liu, Yan Gao, Jian-Guang Lou, Yan Zhang, and\nDongmei Zhang. 2020. â€œWhat Do You Mean by That?â€ - a Parser-Independent\nInteractive Approach for Enhancing Text-to-SQL. In EMNLP.\n[63] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,\nRÃ©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago,\nThomas Hubert, Peter Choy, Cyprien de Masson dâ€™Autume, Igor Babuschkin,\nXinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov,\nJames Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli,\nNando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-\nLevel Code Generation with AlphaCode. https://doi.org/10.48550/ARXIV.2203.\n07814\n[64] Zongjie Li, Chaozheng Wang, Zhibo Liu, Haoxuan Wang, Shuai Wang, and\nCuiyun Gao. 2022. CCTEST: Testing and Repairing Code Completion Systems.\nArXiv abs/2208.08289 (2022).\n[65] Brian Y Lim and Anind K Dey. 2009. Assessing demand for intelligibility in\ncontext-aware applications. In Proceedings of the 11th international conference\non Ubiquitous computing . 195â€“204.\n[66] Hugo Liu and Henry Lieberman. 2005. Programmatic Semantics for Natural Lan-\nguage Interfaces. In CHI â€™05 Extended Abstracts on Human Factors in Computing\nSystems (Portland, OR, USA)(CHI EA â€™05). Association for Computing Machinery,\nNew York, NY, USA, 1597â€“1600. https://doi.org/10.1145/1056808.1056975\n[67] Michael Xieyang Liu, Jane Hsieh, Nathan Hahn, Angelina Zhou, Emily Deng,\nShaun Burley, Cynthia Taylor, Aniket Kittur, and Brad A. Myers. 2019. Unakite:\nScaffolding Developersâ€™ Decision-Making Using the Web. In Proceedings of the\n32Nd Annual ACM Symposium on User Interface Software and Technology (UIST\nâ€™19). ACM, New Orleans, LA, USA, 67â€“80. https://doi.org/10.1145/3332165.\n3347908 event-place: New Orleans, LA, USA.\n[68] Michael Xieyang Liu, Aniket Kittur, and Brad A. Myers. 2021. To Reuse or Not\nTo Reuse? A Framework and System for Evaluating Summarized Knowledge.\nProceedings of the ACM on Human-Computer Interaction 5, CSCW1 (April 2021),\n166:1â€“166:35. https://doi.org/10.1145/3449240\n[69] Michael Xieyang Liu, Aniket Kittur, and Brad A. Myers. 2022. Crystalline:\nLowering the Cost for Developers to Collect and Organize Information for\nDecision Making. In Proceedings of the 2022 CHI Conference on Human Factors in\nComputing Systems (CHI â€™22) . Association for Computing Machinery, New York,\nNY, USA. https://doi.org/10.1145/3491102.3501968 event-place: New Orleans,\nLA, USA.\n[70] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and\nGraham Neubig. 2021. Pre-train, Prompt, and Predict: A Systematic Survey of\nPrompting Methods in Natural Language Processing. ArXiv abs/2107.13586\n(2021).\n[71] Ewa Luger and Abigail Sellen. 2016. \"Like Having a Really Bad PA\" The Gulf\nbetween User Expectation and Experience of Conversational Agents. In Pro-\nceedings of the 2016 CHI conference on human factors in computing systems .\n5286â€“5297.\n[72] Stephen MacNeil, Andrew Tran, Dan Mogil, Seth Bernstein, Erin Ross, and Zi-\nheng Huang. 2022. Generating Diverse Code Explanations using the GPT-3 Large\nLanguage Model. In Proceedings of the 2022 ACM Conference on International\nComputing Education Research - Volume 2 (ICER â€™22) . Association for Computing\nMachinery, New York, NY, USA, 37â€“39. https://doi.org/10.1145/3501709.3544280\n[73] Nora McDonald, Sarita Schoenebeck, and Andrea Forte. 2019. Reliability and\ninter-rater reliability in qualitative research: Norms and guidelines for CSCW\nand HCI practice. Proceedings of the ACM on Human-Computer Interaction 3,\nCSCW (2019), 1â€“23.\n[74] Patrick E McKnight and Julius Najab. 2010. Mann-Whitney U Test. The Corsini\nencyclopedia of psychology (2010), 1â€“1.\n[75] Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh\nHajishirzi. 2021. Reframing Instructional Prompts to GPTkâ€™s Language. arXiv\npreprint arXiv:2109.07830 (2021).\n[76] Lingbo Mo, Ashley Glen Lewis, Huan Sun, and Michael White. 2022. Towards\nTransparent Interactive Semantic Parsing via Step-by-Step Correction. ArXiv\nabs/2110.08345 (2022).\n[77] Jesse Mu and Advait Sarkar. 2019. Do we need natural language? Exploring\nrestricted language interfaces for complex domains. In Extended Abstracts of the\n2019 CHI Conference on Human Factors in Computing Systems . 1â€“6.\n[78] Avanika Narayan, Ines Chami, Laurel Orr, and Christopher Râ€™e. 2022. Can\nFoundation Models Wrangle Your Data? ArXiv abs/2205.09911 (2022).\n[79] Avanika Narayan, Ines Chami, Laurel Orr, and Christopher Râ€™e. 2022. Can\nFoundation Models Wrangle Your Data? ArXiv abs/2205.09911 (2022).\n[80] Petr Necesal and Jan PospÄ±Å¡il. 2012. Experience with teaching mathematics for\nengineers with the aid of Wolfram Alpha. In Proceedings of the World Congress\non Engineering and Computer Science , Vol. 1. 271â€“274.\n[81] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Haiquan Wang, Yingbo Zhou,\nSilvio Savarese, and Caiming Xiong. 2022. A Conversational Paradigm for\nProgram Synthesis. ArXiv abs/2203.13474 (2022).\n[82] Fatma Åzcan, Abdul Quamar, Jaydeep Sen, Chuan Lei, and Vasilis Efthymiou.\n2020. State of the art and open challenges in natural language interfaces to data.\nIn Proceedings of the 2020 ACM SIGMOD International Conference on Management\nof Data . 2629â€“2636.\nBridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models CHI â€™23, April 23â€“28, 2023, Hamburg, Germany\n[83] Mihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, M. Hassan Murad, and\nChitta Baral. 2022. In-BoXBART: Get Instructions into Biomedical Multi-Task\nLearning. In NAACL-HLT.\n[84] Pruthvi Patel, Swaroop Mishra, Mihir Parmar, and Chitta Baral. 2022. Is a\nQuestion Decomposition Unit All We Need? ArXiv abs/2205.12538 (2022).\n[85] Mitchel Resnick, John Maloney, AndrÃ©s Monroy-HernÃ¡ndez, Natalie Rusk, Eve-\nlyn Eastmond, Karen Brennan, Amon Millner, Eric Rosenbaum, Jay Silver, Brian\nSilverman, et al. 2009. Scratch: programming for all. Commun. ACM 52, 11\n(2009), 60â€“67.\n[86] John Rieman. 1993. The diary study: a workplace-oriented research tool to guide\nlaboratory efforts. In Proceedings of the INTERACTâ€™93 and CHIâ€™93 conference on\nHuman factors in computing systems . 321â€“326.\n[87] Johnny SaldaÃ±a. 2021. The coding manual for qualitative researchers . sage.\n[88] Jean E. Sammet. 1966. The Use of English as a Programming Language.Commun.\nACM 9, 3 (mar 1966), 228â€“230. https://doi.org/10.1145/365230.365274\n[89] Advait Sarkar. 2018. Interactive analytical modelling . Technical Report. Univer-\nsity of Cambridge, Computer Laboratory.\n[90] Advait Sarkar. 2022. Is explainable AI a race against model complexity?. InWork-\nshop on Transparency and Explanations in Smart Systems (TeXSS), in conjunction\nwith ACM Intelligent User Interfaces (IUI 2022) (CEUR Workshop Proceedings,\n3124). 192â€“199. http://ceur-ws.org/Vol-3124/paper22.pdf\n[91] Advait Sarkar, Alan F Blackwell, Mateia Jamnik, and Martin Spott. 2014. Teach\nand try: A simple interaction technique for exploratory data modelling by\nend users. In 2014 IEEE Symposium on Visual Languages and Human-Centric\nComputing (VL/HCC) . IEEE, 53â€“56.\n[92] Advait Sarkar, Alan F. Blackwell, Mateja Jamnik, and Martin Spott. 2015. In-\nteraction with Uncertainty in Visualisations. (2015). https://doi.org/10.2312/\neurovisshort.20151138 Accepted: 2015-05-24T19:43:20Z Publisher: The Euro-\ngraphics Association.\n[93] Advait Sarkar, Judith W. Borghouts, Anusha Iyer, Sneha Khullar, Christian Can-\nton, Felienne Hermans, Andrew D. Gordon, and Jack Williams. 2020. Spread-\nsheet Use and Programming Experience: An Exploratory Survey. In Extended\nAbstracts of the 2020 CHI Conference on Human Factors in Computing Systems\n(CHI EA â€™20) . Association for Computing Machinery, New York, NY, USA, 1â€“9.\nhttps://doi.org/10.1145/3334480.3382807\n[94] Advait Sarkar and Andrew D. Gordon. 2018. How do people learn to use\nspreadsheets? (Work in progress). In Proceedings of the 29th Annual Conference\nof the Psychology of Programming Interest Group (PPIG 2018) . 28â€“35.\n[95] Advait Sarkar, Andrew D. Gordon, Carina Negreanu, Christian Poelitz,\nSruti Srinivasa Ragavan, and Ben Zorn. 2022. What is it like to program with ar-\ntificial intelligence? https://doi.org/10.48550/arXiv.2208.06213 arXiv:2208.06213\n[cs].\n[96] Advait Sarkar, Mateja Jamnik, Alan F. Blackwell, and Martin Spott. 2015. In-\nteractive visual machine learning in spreadsheets. In 2015 IEEE Symposium on\nVisual Languages and Human-Centric Computing (VL/HCC) . 159â€“163. https:\n//doi.org/10.1109/VLHCC.2015.7357211\n[97] Advait Sarkar, Sruti Srinivasa Ragavan, Jack Williams, and Andrew D Gordon.\n2022. End-user encounters with lambda abstraction in spreadsheets: Apolloâ€™s\nbow or Achillesâ€™ heel?. In2022 IEEE Symposium on Visual Languages and Human-\nCentric Computing (VL/HCC) . IEEE Computer Society, 1â€“11.\n[98] Advait Sarkar, Martin Spott, Alan F Blackwell, and Mateja Jamnik. 2016. Visual\ndiscovery and model-driven explanation of time series patterns. In 2016 IEEE\nSymposium on Visual Languages and Human-Centric Computing (VL/HCC) . IEEE,\n78â€“86.\n[99] Arvind Satyanarayan, Dominik Moritz, Kanit Wongsuphasawat, and Jeffrey\nHeer. 2016. Vega-lite: A grammar of interactive graphics. IEEE transactions on\nvisualization and computer graphics 23, 1 (2016), 341â€“350.\n[100] Christopher Scaffidi, Mary Shaw, and Brad Myers. 2005. Estimating the numbers\nof end users and end user programmers. In 2005 IEEE Symposium on Visual\nLanguages and Human-Centric Computing (VL/HCCâ€™05) . IEEE, 207â€“214.\n[101] Viktor Schlegel, Benedikt Lang, Siegfried Handschuh, and AndrÃ© Freitas. 2019.\nVajra: step-by-step programming with natural language. In Proceedings of the\n24th International Conference on Intelligent User Interfaces . 30â€“39.\n[102] Vidya Setlur and Melanie Tory. 2022. How do you Converse with an Analytical\nChatbot? Revisiting Gricean Maxims for Designing Analytical Conversational\nBehavior. In CHI Conference on Human Factors in Computing Systems . 1â€“17.\n[103] Disha Shrivastava, H. Larochelle, and Daniel Tarlow. 2022. Repository-Level\nPrompt Generation for Large Language Models of Code. ArXiv abs/2206.12839\n(2022).\n[104] Ankita Nandkishor Sontakke, Manasi Patwardhan, Lovekesh Vig, Raveendra Ku-\nmar Medicherla, Ravindra Naik, and Gautam Shroff. 2022. Code Summarization:\nDo Transformers Really Understand Code?. InDeep Learning for Code Workshop .\n[105] Sruti Srinivasa Ragavan, Zhitao Hou, Yun Wang, Andrew D Gordon, Haidong\nZhang, and Dongmei Zhang. 2022. GridBook: Natural Language Formulas\nfor the Spreadsheet Grid. In 27th International Conference on Intelligent User\nInterfaces (IUI â€™22) . Association for Computing Machinery, New York, NY, USA,\n345â€“368. https://doi.org/10.1145/3490099.3511161\n[106] Sruti Srinivasa Ragavan, Advait Sarkar, and Andrew D Gordon. 2021. Spread-\nsheet Comprehension: Guesswork, Giving Up and Going Back to the Author. In\nProceedings of the 2021 CHI Conference on Human Factors in Computing Systems\n(CHI â€™21) . Association for Computing Machinery, New York, NY, USA, 1â€“21.\nhttps://doi.org/10.1145/3411764.3445634\n[107] Erik Stolterman. 2008. The nature of design practice and implications for\ninteraction design research. International Journal of Design 2, 1 (2008).\n[108] Harry R. Tennant, Kenneth M. Ross, and Craig W. Thompson. 1983. Usable Natu-\nral Language Interfaces through Menu-Based Natural Language Understanding.\nIn Proceedings of the SIGCHI Conference on Human Factors in Computing Systems\n(Boston, Massachusetts, USA) (CHI â€™83) . Association for Computing Machinery,\nNew York, NY, USA, 154â€“160. https://doi.org/10.1145/800045.801601\n[109] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kul-\nshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang\nLi, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali,\nYanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen,\nYuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao,\nYanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett,\nPranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel\nMorris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben\nZevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kris-\nten Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi\nRajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton,\nAaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui,\nMarian Croak, Ed Chi, and Quoc Le. 2022. LaMDA: Language Models for Dialog\nApplications. (Jan. 2022). https://doi.org/10.48550/arXiv.2201.08239\n[110] Immanuel Trummer. 2022. CodexDB: Generating Code for Processing SQL\nQueries using GPT-3 Codex. ArXiv abs/2204.08941 (2022).\n[111] Mojtaba Vaismoradi, Hannele Turunen, and Terese Bondas. 2013. Content anal-\nysis and thematic analysis: Implications for conducting a qualitative descriptive\nstudy. Nursing & Health Sciences 15, 3 (2013), 398â€“405. https://doi.org/10.1111/\nnhs.12048 _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/nhs.12048.\n[112] Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. 2022. Expectation\nvs. Experience: Evaluating the Usability of Code Generation Tools Powered by\nLarge Language Models. In CHI Conference on Human Factors in Computing\nSystems Extended Abstracts . 1â€“7.\n[113] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you\nneed. Advances in neural information processing systems 30 (2017).\n[114] Chaozheng Wang, Yuan-Hong Yang, Cuiyun Gao, Yun Peng, Hongyu Zhang,\nand Michael R. Lyu. 2022. No More Fine-Tuning? An Experimental Evaluation\nof Prompt Tuning in Code Intelligence. ArXiv abs/2207.11680 (2022).\n[115] Liwen Wang, Rumei Li, Yang Yan, Yuanmeng Yan, Sirui Wang, Wei Yu Wu, and\nWeiran Xu. 2022. InstructionNER: A Multi-Task Instruction-Based Generative\nFramework for Few-shot NER. ArXiv abs/2203.03903 (2022).\n[116] Sida I Wang, Samuel Ginn, Percy Liang, and Christoper D Manning. 2017.\nNaturalizing a programming language via interactive learning. arXiv preprint\narXiv:1704.06956 (2017).\n[117] Zhiruo Wang, Grace Cuenca, Shuyan Zhou, Frank F. Xu, and Graham Neubig.\n2022. MCoNaLa: A Benchmark for Code Generation from Multiple Natural\nLanguages. ArXiv abs/2203.08388 (2022).\n[118] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian\nBorgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,\net al . 2022. Emergent abilities of large language models. arXiv preprint\narXiv:2206.07682 (2022).\n[119] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le,\nand Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in Large\nLanguage Models. ArXiv abs/2201.11903 (2022).\n[120] Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross,\nFernando Martinez, Mayank Agarwal, and Kartik Talamadupula. 2021. Perfec-\ntion not required? Human-AI partnerships in code translation. In 26th Interna-\ntional Conference on Intelligent User Interfaces . 402â€“412.\n[121] Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina,\nMichael Terry, and Carrie J Cai. 2022. PromptChainer: Chaining Large Lan-\nguage Model Prompts through Visual Programming. In Extended Abstracts\nof the 2022 CHI Conference on Human Factors in Computing Systems (CHI\nEA â€™22) . Association for Computing Machinery, New York, NY, USA, 1â€“10.\nhttps://doi.org/10.1145/3491101.3519729\n[122] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. AI Chains: Trans-\nparent and Controllable Human-AI Interaction by Chaining Large Language\nModel Prompts. In CHI Conference on Human Factors in Computing Systems\n(CHI â€™22) . Association for Computing Machinery, New York, NY, USA, 1â€“22.\nhttps://doi.org/10.1145/3491102.3517582\n[123] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022. A\nsystematic evaluation of large language models of code. In Proceedings of the\n6th ACM SIGPLAN International Symposium on Machine Programming . 1â€“10.\n[124] Frank F Xu, Bogdan Vasilescu, and Graham Neubig. 2022. In-ide code generation\nfrom natural language: Promise and challenges. ACM Transactions on Software\nCHI â€™23, April 23â€“28, 2023, Hamburg, Germany Liu, et al.\nEngineering and Methodology (TOSEM) 31, 2 (2022), 1â€“47.\n[125] Ziyu Yao, Yu Su, Huan Sun, and Wen tau Yih. 2019. Model-based Interactive\nSemantic Parsing: A Unified Framework and A Text-to-SQL Case Study. In\nEMNLP.\n[126] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li,\nJames Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir\nRadev. 2019. Spider: A Large-Scale Human-Labeled Dataset for Complex and\nCross-Domain Semantic Parsing and Text-to-SQL Task. https://doi.org/10.\n48550/arXiv.1809.08887 arXiv:1809.08887 [cs].\n[127] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li,\nJames Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R.\nRadev. 2018. Spider: A Large-Scale Human-Labeled Dataset for Complex and\nCross-Domain Semantic Parsing and Text-to-SQL Task. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing, Brussels,\nBelgium, October 31 - November 4, 2018 , Ellen Riloff, David Chiang, Julia Hock-\nenmaier, and Junâ€™ichi Tsujii (Eds.). Association for Computational Linguistics,\n3911â€“3921. https://doi.org/10.18653/v1/d18-1425\n[128] Xiong Zhang and Philip J Guo. 2017. Ds. js: Turn any webpage into an example-\ncentric live programming environment for learning data science. In Proceedings\nof the 30th Annual ACM Symposium on User Interface Software and Technology .\n691â€“702.\n[129] Ming Zhu, Aneesh Jain, Karthik Suresh, Roshan Ravindran, Sindhu Tipirneni,\nand Chandan K. Reddy. 2022. XLCoST: A Benchmark Dataset for Cross-lingual\nCode Intelligence. ArXiv abs/2206.08474 (2022).\nBridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models CHI â€™23, April 23â€“28, 2023, Hamburg, Germany\nA CODES AND DESCRIPTIONS\nA.1 Failure modes\nTable 9: Failure modes and their descriptions\nFailure mode Description\nGeneration failure No completion from Codex.\nExecution failure Code could not be executed.\nOutput type failure Our prototype could not parse the output type (e.g., the model generate a column, dataframe, or value, but the format cannot be displayed in the\nspreadsheet).\nWrong input Model generates code that calculates answer using wrong columns (even if user instruction does not specify which input columns to use, but researcher\ncan infer it from context).\nSoft wrong input Like wrong input, but it is difficult for researcher to infer correct columns from user query.\nOverwrite Model attempts to overwrite existing dataframe column when specifically asked to create a column.\nSoft overwrite Model attempts to overwrite existing dataframe column, but not asked by user to specifically create a new column.\nOther incorrect Model generates code that is incorrect, but none of the other more specific codes applies.\nN/A Model generated code matches user intent (success, either intermediate or overall). Or user playing around e.g., â€œhi. â€\nRaw data output Model hallucinates data output (array of values) directly, not a computation. Considered a failure regardless of whether values are correct or not,\nbecause it is not reproducible on other data sets.\nMissing columns Model generates wrong number of output columns, fewer than the user asked.\nExtra columns Model generates wrong number of output columns, more than the user asked.\nWrong heuristic Model invents a plausible way of calculating a quantity which is asked for but the method to calculate it is unspecified, but the method is wrong.\nPartial answer Model generates an intermediate step that the user can build on to complete task, OR model only correctly answers a clearly delineated subpart (e.g., in\nthe breakdown style, or separated by AND or comma) of the user query.\nA.2 Rewrite strategies\nTable 10: Rewrite strategies and their descriptions\nRewrite strategy Description\nAdd steps User is asking the model to do strictly more than in the previous query, e.g., adding a new follow-on step, or a new intermediate column.\nCode-like syntax User borrows syntax from coding languages, e.g., â€œ()â€ to group statements, use of quotes, use of â€œif/then/elseâ€, use of symbols like â€œ>=â€, use of\nSQL-like keywords. But not specifically the style of the system-generated breakdowns.\nNext step Marks when the user moves on to a new query intent that is a following step to their overall task solution.\nNew intent User decides to try asking the system a different thing (i.e., different code is needed to solve the new intent - not just a rephrasing to ask the system to\ndo the same thing).\nName output columns Specify a name for the desired output column\nSpecify output type Add information about the number of columns, type of columns, or type of values expected. (Not if already accounted for by â€œname output columnsâ€).\nSpecify input columns Add information about which columns the code should use to calculate the answer name output columns Specify a name for the desired output column.\nReuse system breakdown Partial or full reuse of system-generated breakdown.\nSelf-breakdown Expresses query in clearly delineated steps, but text is not system-generated.\nElaborate how Give more detailed logic or computation steps for something that user was already asking for previously (e.g.,â€œcalculate averageâ€-> â€œcalculate average\nby dividing x by yâ€).\nElaborate what More detailed description of the required output (but not including new details of how to compute it). E.g.â€œcountâ€-> â€œcount the rowsâ€.\nTesting User experimenting, playing, rolling back etc.\nReduce scope User asks the system to do less, or a subset, of previous query.\nSystem-like User borrows style/syntax/vocabulary from the system-generated explanations, but not a reuse (i.e. the user types it manually).\nStart over User appears to abandon previous query text entirely; new query text contains little or no character sequences in common with previous query (and the\nintent is the same, i.e. not new intent).\nOther rephrase User rewrites part of the query, but none of the other codes applies to that specific part. (other parts may be rewritten in a different manner which we\ncan code).\nCHI â€™23, April 23â€“28, 2023, Hamburg, Germany Liu, et al.\nB SYSTEM IMPLEMENTATION DETAILS\nB.1 Further details of grounded utterance generation\nThe implementation of utterance generation is specific to a programming language or API, in this case Pandas, but the design of the\nalgorithm is generic. The algorithm is split into three components:\n(1) Translation of python code into the task-centric representation (TCR).\n(2) Translation of TCR into an explanation representation (ER), a tree of natural language fragments, preserving hierarchical structure.\n(3) The layout algorithm that converts an ER tree into an utterance. The algorithm is parameterised such that it can generate a single\nsentence or a sequence of utterances.\nAs the algorithm consumes and produces different trees, starting with a Python abstract syntax tree, the algorithm works with single or\nmulti-line programs. The features that determine what is supported by the algorithm are the language constructs and API methods used,\nrather than how the code is written.\nFor this work we focus on defining the algorithm over a subset of Pandas that covers basic data analysis and wrangling tasks. This\nincludes:\nâ€¢Dataframe indexing operations such as loc and iloc, supporting patterns that includes element access, slice access, and masking\nwith boolean-valued series.\nâ€¢Vectorised operators applied to both dataframes and series.\nâ€¢Variable declarations bound to pandas expression.\nâ€¢Dataframe methods such as groupby, size, and transpose.\nâ€¢Aggregation methods such as sum, min, and max.\nâ€¢Series methods such as idxmax, sum, and mean.\nâ€¢Series string methods such as split, replace, lower, and strip.\nâ€¢Series date and time methods such as ceil and year.\nNotable Python features not currently supported include function declarations, list comprehensions, and control flow.\nTo improve the generated utterances we add special handling for certain patterns. Examples include:\nâ€¢Labels for tuple types. Certain Pandas operations return tuples with a defined meaning, such as .shape which returns the dimensions\nof a dataframe. Our algorithm understands that .shape returns a tuple type, and additionally, we allow the type to label each element\nof the tuple, such as rows and columns. When generating an utterance for a subscript operation, such as e1[e2], if e1 is inferred to\nhave a labeled tuple type and e2 is a known constant, we use the label in the utterance rather than a generic â€œelement of\" snippet.\nâ€¢Strings. When accessing elements of a string column which contains either a single string or a list of strings, we use the terms\nâ€œcharacter\" or â€œword\" instead of â€œelement\".\nâ€¢Array indexing. As Python arrays are zero-indexed, we add 1 to indices before displaying to the user (e.g., array[1] is rendered as\nâ€œelement 2 of the arrayâ€), and subtract 1 from user queries, detecting such indices using templates, before sending them to the system.\nB.2 Formal description of round-trip benchmark\nThe benchmark simulates the following steps for each input table ğ¼ and natural language utterance ğ‘„:\n(1) The user selects table ğ¼, types ğ‘„ into the query box, and clicks â€œGoâ€.\n(2) The system generates code ğ¶1 and output ğ‘‚1 (which can be a single value, new column(s), or new table(s)).\n(3) If possible, the system generates a grounded utterance ğº1 from ğ¶1; otherwise the interaction ends.\n(4) The user clicks â€œUpdate & Goâ€ without editing ğº1.\n(5) The system generates code ğ¶2 and output ğ‘‚2.\nTo measure the equality of results, the benchmark calculates two metrics:\nâ€¢Code generation equality : how frequently ğ¶2 = ğ¶1 given that ğº1 can be generated.\nâ€¢Output equivalence: how frequently ğ‘‚2 = ğ‘‚1 (modulo column names) given that ğº1 can be generated.\nWe calculated equivalences in both our synthetic dataset and the actual queries submitted by participants during our user study tasks\n(Section 5). Table 11 summarizes the results.\nTable 11: Results of the round-trip stability experiments on both the Stack Overflow dataset and actual participantsâ€™ queries\nfrom the study. ğ‘ is the number of data points that successfully passed the aforementioned 5 simulated interactions. Both\nequality measurements are presented as the percentage of data points where equality was reached.\nDataset ğ‘ Code generation equalityOutput equivalence\nStack Overflow dataset 126 58.7% 85.7%\nStudy participant dataset191 72.8% 84.8%\nBridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models CHI â€™23, April 23â€“28, 2023, Hamburg, Germany\nB.3 Evaluating LLM performance\nIn order to assess a LLMâ€™s performance for code generation, the research community has developed a wide range of benchmarks and\nquantitative metrics. Primarily there are two types of evaluations that are conducted: output matching and code equivalence. For output\nmatching, we mark a generation as correct if it passes all I/O examples or tests (as the generation process is not deterministic we use pass@k\nto estimate the chance that we obtain a correct generation [16]). For code equivalence we use code similarity metrics [25] where we mark a\ngeneration as correct if it is similar enough to the expected code snippet. The most used benchmarks target Python generation (e.g. APPS\n[35], HumanEval [16]) and SQL generation (e.g. Spider [127]), but recently benchmarks for cross-lingual generations have emerged (e.g. [13],\nXLCosT [129]) as well as benchmarks with cross-lingual asks (e.g. MCoNaLa [117]). In our work we evaluate over a new dataset of problems\nextracted from StackOverflow under the tag Excel-Formulas, where the users believe their ask can be solved by an Excel formula. We use\noutput matching (pass@k) as our metric for correctness.\nC FORMATIVE USABILITY STUDY\nWe conducted an informal, exploratory study to gain an initial understanding of the challenges users might face when interacting with an\nLLM for data analysis in spreadsheets, and to check whether and to what extent the abstraction matching challenges observed by Sarkar\net al. [95] and Srinivasa Ragavan et al. [105] were present in our application.\nC.1 Prototype\nThe prototype used in this formative study uses the code generation pipeline described in Section 4.1, specifically steps 1-3. These steps take\na user query, generate executable Python code using Codex, run the code, and insert the results into the spreadsheet. However, the user\ninterface of this prototype was minimal. Users could only enter queries using a query box (similar to Figure 3-B1/B2) and subsequently view\nthe execution results in the spreadsheet (similar to Figure 3-F1/F2). There was no guidance of any kind for writing effective queries. This\nuser experience resembles commercially available spreadsheet querying features, such as the â€œAnalyze Dataâ€ feature in Microsoft Excel,6 or\nthe â€œExploreâ€ feature of Google Sheets.7\nC.2 Method\nWe recruited 5 participants (FP1 to FP5) through UserTesting.com. Participants were end-user programmers who all self-reported writing\nformulas in spreadsheets for work. One of the participants had some prior programming experience using C++, while the rest had no\nprogramming experience. There were no participants in common between this sample, and the sample used in the main study. While we\ndo not claim that this sample is representative of all users, the study sessions were informative and helped motivate the idea of grounded\nabstraction matching, the development of both System I and System II, and the design of the formal study in Section 5.\nWe began by asking participants about their experience with, and typical use of spreadsheets. Participants then attempted data manipulation\nand analysis tasks (early versions of the tasks used in the main study). The sessions concluded with a semi-structured interview discussing\ntheir experience with the prototype, what they liked, and what to improve upon. Sessions were conducted remotely through Microsoft\nTeams and lasted around 30 minutes each. They were recorded and transcribed. One of the authors analysed the transcripts with an informal\nopen-coding approach [15], which included discussions with the research team. Our key findings are presented below.\nC.3 Results\nParticipants appreciated the ability to perform spreadsheet tasks with natural language queries rather than developing the correct formula,\nwhether through personal experience or complex web searching and sensemaking, claiming that it was much more efficient (when it worked)\nand reduced the need for â€œgooglingâ€ (FP2). However, they encountered several significant challenges in the process:\nC.3.1 It was hard for participants to recover from system errors. Sometimes when the system output was obviously incorrect or the system\ndid something unexpected, participants felt directionless about their options for fixing the errors. On one hand, there often existed a gap\nbetween the natural language intent (for example, â€œadd space flight and space walksâ€ ) and the system output manifested in the spreadsheet\n(in this case, two new columns, one calculating â€œspace flight = space flights * space flight (hr)â€, the other calculating â€œspace\nwalk = space walks * space walk (hr))â€, and it was difficult for participants to form hypotheses of what the root cause was without any\nknowledge of the logic generated by Codex. Even if participants had some intuitions on potential fixes, those intuitions were not systematic,\nand were often random guesses, such as â€œshould I be careful about pluralization or casesâ€ (FP5).\nWe observed that when an error occurred, participants only tried to reformulate their natural language intent twice on average before\nabandoning the system. When asked about what they would like to do next to solve the tasks, participants reported wanting to â€œjust figure\nout how to write a formulaâ€ (FP4) or consult â€œgoogleâ€ (FP1).\nC.3.2 It was difficult for participants to develop mental models of the capabilities of the system. Even after repeated use, participants still had\npoor notions about what the system could do, and how to phrase their queries to get the system to work reliably. Instead, their queries (and\n6https://support.microsoft.com/en-us/office/analyze-data-in-excel-3223aab8-f543-4fda-85ed-76bb0295ffc4\n7https://support.google.com/docs/answer/6280499\nCHI â€™23, April 23â€“28, 2023, Hamburg, Germany Liu, et al.\nsubsequent fixes, if applicable) were biased by their personal beliefs, speculations, and haphazard experimentation, such as the example\ndiscussed in section C.3.1. Participants wished to be able to better grasp the kinds of utterances, grammar, vocabulary, and level of specificity\nthat is effective at generating the desired output. For example, FP4 wondered â€œwhat does it [the system] understand? Do I have to use the same\ncolumn names [as the ones in the table]?â€\nMoreover, participants found it difficult to generalise from a successful solution, to replicate the same or similar computation on different\nproblems, since they could not identify why a query had been successful (e.g., was it their word choice, level of abstraction or problem\ndecomposition, specific operations, or mentioning exact elements in the spreadsheet?). They wished that the system could â€œtalk to me\nin English and say what itâ€™s doingâ€ (FP5), and felt that there was a missed opportunity of â€œlearningâ€ to â€œmake it [the same type of data\nmanipulation] faster elsewhereâ€ (FP2).\nC.3.3 Participants expressed a lack of trust towards system results. As opposed to spreadsheet formulas, which many spreadsheet users\nare familiar with, participants expressed frustration and a sense of distrust towards the instability of the results produced by their natural\nlanguage queries. For example, FP2 complained that â€œnon-determinism is a painâ€ when comparing the system behaviors with regular\nspreadsheet formulas that he could understand and obtain anticipated results from. In addition, when system produced some partial instead\nof expected results, participants were not sure if they were â€œon the right trackâ€ (FP3). Last but not least, participants thought that, unlike\nformulas, the current natural language interface lacked a way for them to effectively verify the correctness of the system output. Though it\nwas relatively easy to â€œsanity checkâ€ (FP4) a few â€œinstances or examples [of the system output]â€ during the study, participants felt such spot\nchecking approach was insufficient and intractable in reality, especially for large data tables with â€œperhaps thousands or tens of thousands of\nrowsâ€ (FP1).\nIn summary, our exploratory study gave evidence in our specific application context both of the abstraction matching problem as well as\nthe issues around error recovery, mental models, and trust identified by Sarkar et al. [95] and Srinivasa Ragavan et al. [105]. This motivated\nthe development of the grounded abstraction matching approach.\nBridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models CHI â€™23, April 23â€“28, 2023, Hamburg, Germany\nD STUDY TASKS\nD.1 Task descriptions shown to participants\nâ€¢Task 1: As you could probably tell from the spreadsheet, each city has its own home team, for example the city of Denver has Broncos and\nBaltimore has Ravens. Your task is: Use the sidebar to find out the number of superbowls the city of New Orleans has won?\nâ€¢Task 2: This is a worksheet containing the details of some of the NASA astronauts who have been into space, such as their name, birth place,\nhours of space flight, as well as the space missions (separated by â€œ, â€) that they have flown before. Depending on the primary objectives, the\nduration of each space mission can vary a lot. Now, please imagine that you would like to find out on average how long each mission is for\nevery astronaut. Your task is to create a new column that calculates that.\nâ€¢Task 3: This is a worksheet listing the prices of some houses for sale as well as their details, such as the number of bedrooms, bathrooms,\nwhere it is located, etc. Now, imagine that you would like to check if a house if relatively new. In addition, you would like to see if it has a\nbasement that you can use as storage, and if is has been renovated before. For the sake of this task, new houses are those built in or after\n1970. Your task is to use the sidebar to create a column checking if a house satisfies all three of the aforementioned criteria.\nD.2 Task data\nTable 12: Full data table for task 1 (Super Bowl) in the study as described in section 5.\nDate Winner Winner\nPts\nLoser Loser\nPts\nMVP Stadium Host City Host State\nFeb 2 2020 Kansas City Chiefs 31 San Francisco 49ers20 Patrick MahomesHard Rock Stadium Miami Florida\nFeb 3 2019 New England Patriots13 Los Angeles Rams 3 Julian Edelman Mercedes-Benz Stadium Atlanta Georgia\nFeb 4 2018 Philadelphia Eagles 41 New England Patriots33 Nick Foles U.S. Bank Stadium Minneapolis Minnesota\nFeb 5 2017 New England Patriots34 Atlanta Falcons 28 Tom Brady NRG Stadium Houston Texas\nFeb 7 2016 Denver Broncos 24 Carolina Panthers 10 Von Miller Leviâ€™s Stadium Santa Clara California\nFeb 1 2015 New England Patriots28 Seattle Seahawks 24 Tom Brady University of Phoenix StadiumGlendale Arizona\nFeb 2 2014 Seattle Seahawks 43 Denver Broncos 8 Malcolm Smith MetLife Stadium East RutherfordNew Jersey\nFeb 3 2013 Baltimore Ravens 34 San Francisco 49ers31 Joe Flacco Mercedes-Benz SuperdomeNew Orleans Louisiana\nFeb 5 2012 New York Giants 21 New England Patriots17 Eli Manning Lucas Oil Stadium Indianapolis Indiana\nFeb 6 2011 Green Bay Packers 31 Pittsburgh Steelers 25 Aaron Rodgers Cowboys Stadium Arlington Texas\nFeb 7 2010 New Orleans Saints 31 Indianapolis Colts 17 Drew Brees Sun Life Stadium Miami Florida\nFeb 1 2009 Pittsburgh Steelers 27 Arizona Cardinals 23 Santonio HolmesRaymond James Stadium Tampa Florida\nFeb 3 2008 New York Giants 17 New England Patriots14 Eli Manning University of Phoenix StadiumGlendale Arizona\nFeb 4 2007 Indianapolis Colts 29 Chicago Bears 17 Peyton ManningDolphin Stadium Miami Florida\nFeb 5 2006 Pittsburgh Steelers 21 Seattle Seahawks 10 Hines Ward Ford Field Detroit Michigan\nFeb 6 2005 New England Patriots24 Philadelphia Eagles21 Deion Branch Alltel Stadium Jacksonville Florida\nFeb 1 2004 New England Patriots32 Carolina Panthers 29 Tom Brady Reliant Stadium Houston Texas\nJan 26 2003Tampa Bay Buccaneers48 Oakland Raiders 21 Dexter Jackson Qualcomm Stadium San Diego California\nFeb 3 2002 New England Patriots20 St. Louis Rams 17 Tom Brady Louisiana Superdome New Orleans Louisiana\nJan 28 2001Baltimore Ravens 34 New York Giants 7 Ray Lewis Raymond James Stadium Tampa Florida\nJan 30 2000St. Louis Rams 23 Tennessee Titans 16 Kurt Warner Georgia Dome Atlanta Georgia\nJan 31 1999Denver Broncos 34 Atlanta Falcons 19 John Elway Pro Player Stadium Miami Florida\nJan 25 1998Denver Broncos 31 Green Bay Packers 24 Terrell Davis Qualcomm Stadium San Diego California\nJan 26 1997Green Bay Packers 35 New England Patriots21 Desmond HowardLouisiana Superdome New Orleans Louisiana\nCHI â€™23, April 23â€“28, 2023, Hamburg, Germany Liu, et al.\nTable 13: Full data table for task 2 (astronauts) in the study as described in section 5.\nName Status Birth\nDate\nBirth Place Gender Space\nFlight\n(hr)\nSpace\nWalks\nSpace\nWalks\n(hr)\nMissions\nJoseph M. Acaba Active 5/17/67 Inglewood, CA Male 3307 2 13 STS-119 (Discovery), ISS-31/32 (Soyuz)\nLoren W. Acton Retired 7/3/36 Lewiston, MT Male 190 0 0 STS 51-F (Challenger)\nJames C. AdamsonRetired 3/3/46 Warsaw, NY Male 334 0 0 STS-28 (Columbia), STS-43 (Atlantis)\nThomas D. Akers Retired 5/20/51 St. Louis, MO Male 814 4 29 STS-41 (Discovery), STS-49 (Endeavor), STS-61 (Endeavor),\nSTS-79 (Atlantis)\nBuzz Aldrin Retired 1/20/30 Montclair, NJ Male 289 2 8 Gemini 12, Apollo 11\nAndrew M. Allen Retired 4/8/55 Philadelphia, PA Male 906 0 0 STS-46 (Atlantis), STS-62 (Columbia), STS-75 (Columbia)\nJoseph P. Allen Retired 6/27/37 Crawsfordsville, INMale 313 2 12 ST-5 (Columbia), STS 51-A (Discovery)\nScott D. Altman Retired 8/15/59 Lincoln, IL Male 1236 0 0 STS-90 (Columbia), STS-106 (Atlantis), STS-109 (Columbia),\nSTS-125 (Atlantis)\nWilliam A. AndersRetired 10/17/33 Hong Kong Male 147 0 0 Apollo 8\nClayton C. AndersonRetired 2/23/59 Omaha, NE Male 4005 6 38 STS-117/120 (Atlantis/Discovery), STS-131 (Discovery)\nMichael P. AndersonDeceased 12/25/59 Plattsburgh, NY Male 594 0 0 STS-89 (Endeavor), STS-107 (Columbia)\nDominic A. AntonelliActive 8/23/67 Detroit, MI Male 579 0 0 STS-119 (Discovery), STS-132 (Atlantis)\nJerome Apt III Retired 4/18/49 Springfield, MA Male 847 2 11 STS-37 (Atlantis), STS-47 (Endeavor), STS-59 (Endeavor),\nSTS-79 (Atlantis)\nLee J. ArchambaultRetired 8/25/60 Oak Park, IL Male 639 0 0 STS-117 (Atlantis), STS-119 (Discovery)\nNeil A. Armstrong Deceased 5/8/30 Wapakoneta, OHMale 205 1 2 Gemini 8, Apollo 11\nRichard R. Arnold IIActive 11/26/63 Cheverly, MD Male 307 2 12 STS-119 (Discovery)\nJeffrey S. Ashby Retired 1/6/54 Dallas, TX Male 655 0 0 STS-93 (Columbia), STS-100 (Endeavor), STS-112 (Atlantis)\nJames P. Bagian Retired 2/22/52 Philadelphia, PA Male 337 0 0 STS-29 (Discovery), STS-40 (Columbia)\nEllen S. Baker Retired 4/27/53 Fayettesville, NCFemale 686 0 0 STS-34 (Atlantis), STS-50 (Columbia), STS-71 (Atlantis)\nMichael A. Baker Management10/27/53 Memphis, TN Male 965 0 0 STS-43 (Atlantis), STS-52 (Columbia), STS-68 (Endeavor),\nSTS-81 (Atlantis)\nMichael R. Barratt Active 4/16/59 Vancouver, WA Male 5075 1 5 ISS-19/20 (Soyuz), STS-133 (Discovery)\nDaniel T. Barry Retired 12/30/53 Norwalk, CT Male 733 4 26 STS-72 (Endeavor), STS-96 (Discovery), STS-105 (Discovery)\nJohn-David F. BartoeRetired 11/17/44 Abington, PA Male 190 0 0 STS 51-F (Challenger)\nTable 14: Full data table for task 3 (houses) in the study as described in section 5.\nprice bedrooms bathrooms floors sqft_above sqft_basementyr_built yr_renovatedzipcode latitude longitude\n221900 3 1 1 1180 0 1955 0 98178 47.5112 -122.257\n538000 3 2.25 2 2170 400 1951 1991 98125 47.721 -122.319\n180000 2 1 1 770 0 1933 0 98028 47.7379 -122.233\n604000 4 3 1 1050 910 1965 1998 98136 47.5208 -122.393\n510000 3 2 1 1680 0 1987 0 98074 47.6168 -122.045\n1230000 4 4.5 1 3890 1530 2001 0 98053 47.6561 -122.005\n257500 3 2.25 2 1715 0 1995 0 98003 47.3097 -122.327\n291850 3 1.5 1 1060 0 1963 0 98198 47.4095 -122.315\n229500 3 1 1 1050 730 1960 2010 98146 47.5123 -122.337\n323000 3 2.5 2 1890 0 2003 0 98038 47.3684 -122.031\n662500 3 2.5 1 1860 1700 1965 1998 98007 47.6007 -122.145\n468000 2 1 1 860 300 1942 0 98115 47.69 -122.292\n310000 3 1 1.5 1430 0 1927 0 98028 47.7558 -122.229\n400000 3 1.75 1 1370 400 1977 2001 98074 47.6127 -122.045\n530000 5 2 1.5 1810 0 1900 0 98107 47.67 -122.394\n650000 4 3 2 1980 970 1979 0 98126 47.5714 -122.375\n395000 3 2 2 1890 0 1994 0 98019 47.7277 -121.962\n485000 4 1 1.5 1600 0 1916 0 98103 47.6648 -122.343\n189000 2 1 1 1200 0 1921 0 98002 47.3089 -122.21\n230000 3 1 1 1250 296 1970 2019 98003 47.3343 -122.306\n385000 4 1.75 1 860 760 1947 0 98133 47.7025 -122.341\n2000000 3 2.75 1 2330 720 1968 0 98040 47.5316 -122.233\n285000 5 2.5 2 2270 200 1995 2015 98092 47.3266 -122.169\n252700 2 1.5 1 1070 0 1985 0 98030 47.3533 -122.166\nBridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models CHI â€™23, April 23â€“28, 2023, Hamburg, Germany\nE EXTENDED QUERY EPISODE EXAMPLES\nE.1 Extended examples for failure modes\nTable 15: Examples of input failures encountered by participants\nFailure Mode Example Query (Participant-Task) GeneratedPythonCode Explanation of Failure\nWrong input How many times has the city of New Or-\nleans appear in the winners column(P24-1)\ndf[df['Host City'] == 'New Orleans'].shape[0] The model should have used the â€œWinnerâ€\ncolumn instead of the â€œHost Cityâ€ column.\nSoft wrong\ninput\nhow many superbowls has the city of New\nOrleans won(P27-1)\ndf[df['Host City'] == 'New Orleans'].shape[0] The model should have used the â€œWinnerâ€\ncolumn instead of the â€œHost Cityâ€ column.\nTable 16: Examples of output failures encountered by participants\nFailure Mode Example Query (Participant-Task)GeneratedPythonCode Explanation of Failure\nOverwrite Create col showing how many missions(P1-\n2)\ndf['Missions'] = df['Missions'].str.split(',')\ndf['Missions'] = df['Missions'].str.len()\nThe model should not have overwritten the\nâ€œMissionsâ€ column in both lines.\nSoft overwrite Split the Missions column and count the\nnumber of splits(P28-2)\ndf['Missions'] = df['Missions'].str.split(', ')\ndf['Mission Count'] = df['Missions'].str.len()\nThe model should not have overwritten the\nâ€œMissionsâ€ column in the first line of code.\nExtra\ncolumns\nAdd a column to see if the house has base-\nment, been renovated and was built later\nthan 1970(P5-3)\ndf['has_basement'] = df['sqft_basement'] > 0\ndf['has_renovated'] = df['yr_renovated'] > 0\ndf['built_after_1970'] = df['yr_built'] > 1970\nThe model should not have generated\nthe extra columns of â€œhas_basementâ€,\nâ€œhas_renovatedâ€, or â€œbuilt_after_1970â€,\nsince the participant specifically asked for\nonly one column.\nMissing\ncolumns\n(1) Add new column with count number of\nmissions, each separated by a â€˜, â€™, (2) Add\nnew column and calculate average space\nflight (hr) per name per mission. Return\ntype: new columns of number(P7-2)\ndf['Mission Count'] =\ndf['Missions'].str.split(',').str.len()â†©â†’\nThe model failed to create a second column\nthat the participant asked for that should\nâ€œcalculate average space flight (hr) per name\nper mission. â€\nTable 17: Examples of logic failures encountered by participants\nFailure Mode Example Query (Participant-Task)GeneratedPythonCode Explanation of Failure\nPartial answer Returning the time of â€œNew Orleansâ€ ap-\npears in the winner column(P16-1)\ndf[df['Winner'].str.contains('New Orleans')] The model only created a new table that se-\nlected the rows where â€œNew Orleansâ€ ap-\npears in the â€œWinnerâ€ column. However, the\nuser can easily achieve the final goal by\nadding a new step to â€œreturn the number\nof rowsâ€ from the new table.\nRaw data outputNew column number of missions(P20-2) df['Number of Missions'] = [2, 1, 2, 4, 2, 3, 2,\n4, 1, 3, 2, 2, 2, 4, 2, 1, 3, 2, 3, 4, 2, 2,\n1]\nâ†©â†’\nâ†©â†’\nThe model hallucinated data values in the\nâ€œNumber of Missionsâ€ column rather than\ngenerating code to calculate those values.\nWrong heuristicList all the teams that have lost to New\nEngland Patriots(P21-1)\ndf[df['Loser'] =='New England Patriots'] The model should have used the â€œWinnerâ€\ncolumn instead of the â€œLoserâ€ column to per-\nform the filtering.\nOther incorrect Return 3 columns for if year built >=1970,\nbasement >0, renovated >0(P11-3)\ndef get_features(df):\ndf['built_after_1970'] = df['yr_built'] >= 1970\ndf['has_basement'] = df['sqft_basement'] > 0\ndf['has_renovated'] = df['yr_renovated'] > 0\nreturndf[['built_after_1970', 'has_basement',\n'has_renovated']]â†©â†’\nThe model generated some code defining a\nget_featuresfunction, but failed to include\nany code that runs it, nor was the logic of\nthe function syntactically correct.\nCHI â€™23, April 23â€“28, 2023, Hamburg, Germany Liu, et al.\nE.2 Extended examples for rewrite strategies\nTable 18: Examples of scoping changes. Orange and green highlights indicate differences ( removals and additions, respectively)\nbetween a query and its follow-up query (the same color scheme applies to Table 19-21 as well).\nRewrite StrategyPrevious Query (Participant-Task)\nGeneratedPythonCode\nGrounded Utterance\nFollow-up Query\nGeneratedPythonCode\nGrounded Utterance\nAdd steps\nDefine mission_count by splitting Missions by â€˜, â€™(P6-2)\nâ‡’\nDefine mission_count by splitting Missions by â€˜, â€™, thendivideSpace\nFlightbymission_count\ndf['mission_count'] =\ndf['Missions'].str.split(',').str.len()â†©â†’\ndf['mission_count'] =\ndf['Missions'].str.split(',').str.len()â†©â†’\ndf['Space Flight (hr)'] = df['Space Flight (hr)'] /\ndf['mission_count']â†©â†’\n(1) create column mission_count,\n(2) select column Missions,\n(3) split the text on â€˜, â€™,\n(4) len\n(1) create column mission_count from len from the text split on\nâ€˜, â€™ from column Missions,\n(2) create column Space Flight (hr) from column Space Flight\n(hr) divided by column mission_count.\nReduce scope\nCreate column good where year built is greater than or equal to 1970\nANDsquarefootbasementis not0 andyearrenovatedis not0 (P9-3)\nâ‡’\nCreate column good where year built is greater than or equal to 1970\ndf['good'] = ((df['yr_built'] >= 1970) &\n(df['sqft_basement'] != 0) & (df['yr_renovated'] != 0))â†©â†’\ndf['good'] = df['yr_built'] >= 1970\n(1) create column good,\n(2) column yr_built greater than or equal to 1970 and column\nsqft_basement NotEq 0 and column yr_renovated NotEq 0.\n(1) create column good,\n(2) column yr_built greater than or equal to 1970.\nBridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models CHI â€™23, April 23â€“28, 2023, Hamburg, Germany\nTable 19: Examples of elaboration changes.\nRewrite StrategyPrevious Query (Participant-Task)\nGeneratedPythonCode\nGrounded Utterance\nFollow-up Query\nGeneratedPythonCode\nGrounded Utterance\nElaborate how\nAdd a column oftheaverageflighthourofeachmissionforeachastro-\nnaut(P5-2)\nâ‡’\nAdd a column ofthevalueoftotalspaceflighthourdividedbythe\nnumberofmissionsforeachastronaut\ndf['Average Flight Hour'] = [1653.5, 190, 167, 407, 289, 302,\n313, 309, 147, 1001.25, 297, 289.5, 423.5, 319.5, 205,\n307, 327.5, 168.5, 343, 482.5, 2537.5, 366.5, 190]\nâ†©â†’\nâ†©â†’\ndf['Average Mission Time'] = df['Space Flight (hr)'] /\ndf['Missions'].str.count('\\(')â†©â†’\nN/A\n(1) create column Average Mission Time,\n(2) column Space Flight (hr) divided by count\\( from column\nMissions.\nElaborate what\nhow many superbowls has thecityofNewOrleans won(P27-1)\nâ‡’\nhow many superbowls hasNewOrleansSaints won\ndf[df['Host City'] =='New Orleans'].shape[0] df[df['Winner'] =='New Orleans Saints'].shape[0]\n(1) select rows where column Host City is New Orleans,\n(2) return number of rows.\n(1) select rows where column Winner is New Orleans Saints,\n(2) return number of rows.\nName output\ncolumns\nReturn true if year built >=1970 AND basement >0 and renovated TRUE\n(P11-3)\nâ‡’\nReturnreviewcolumn where if year built >=1970 AND basement\n>0 AND renovated >0\ndef is_renovated(row):\nif row['yr_built'] >= 1970and row['sqft_basement'] > 0\nand row['yr_renovated'] > 0:â†©â†’\nreturn True\nelse:\nreturn False\ndf['renovated'] = df.apply(is_renovated, axis=1)\nprint(df)\ndef review(row):\nif row['yr_built'] >= 1970and row['sqft_basement']\n> 0 and row['yr_renovated'] > 0:â†©â†’\nreturn1\nelse:\nreturn0\ndf['review'] = df.apply(review, axis=1)\nprint(df)\nN/A N/A\nSpecify input\ncolumns\nHow many super bowls has New Orleans won(P9-1)\nâ‡’\nSelectcolumnâ€œwinnerâ€ where text includes new orleans\ndf[df['Host City'] =='New Orleans']['Winner'].count() df[df['Winner'].str.contains('New Orleans')]\n(1) select rows where column Host City is New Orleans,\n(2) select column Winner,\n(3) count.\n(1) select rows where contains â€˜New Orleansâ€™ from column\nWinner.\nSpecify output\ntype\nSelect rows where column yr_built greater than 1970 and column\nyr_renovated NptEq 0 and column sqft_basement NotEq 0(P16-3)\nâ‡’\nCreatea column where column yr_built greater than 1970 and\ncolumn yr_renovated NotEq 0 and column sqft_basement NotEq 0\ndf[(df['yr_built'] > 1970) & (df['yr_renovated'] != 0) &\n(df['sqft_basement'] != 0)]â†©â†’\ndf['new_house'] = df.apply(lambdarow: 1if\nrow['yr_built'] > 1970and row['yr_renovated'] != 0\nand row['sqft_basement'] != 0else0, axis=1)\nâ†©â†’\nâ†©â†’\n(1) select rows where column yr_built greater than 1970 and column\nyr_renovated NotEq 0 and column sqft_basement NotEq 0. N/A\nCHI â€™23, April 23â€“28, 2023, Hamburg, Germany Liu, et al.\nTable 20: Examples of language restructuring changes.\nRewrite StrategyPrevious Query (Participant-Task)\nGeneratedPythonCode\nGrounded Utterance\nFollow-up Query\nGeneratedPythonCode\nGrounded Utterance\nSelf breakdown\nCountthenumberofrowswithâ€˜NewOrleansâ€™inthewinnercolumn\n(P12-1)\nâ‡’\n(1)Createanewcolumncalledcitythatdropsthelastwordinthewinner\ncolumn,(2)Countthenumberofrowsincitythatsayâ€œNewOrleansâ€\ndf[df['Winner'] =='New Orleans'].count() df['city'] = df['Winner'].str.split().str[-1]\ndf['city'].value_counts()\nN/A N/A\nReuse of the\nsystem\nbreakdown\nThe number of superbowls the city of New Orleans has won(P5-1)\nâ‡’\n(1) select rows where columnWinner is New Orleans Saints, (2) count\ndf[df['Host City'] =='New Orleans']['Winner'].count() df[df['Winner'] =='New Orleans Saints'].count()\n(1) select rows where columnHostCity is New Orleans,\n(2) selectcolumnWinner,\n(3) count.\n(1) select rows where column Winner is New Orleans Saints,\n(2) count.\nSystem-like\nhow many super bowls has New Orleans won(P9-1)\nâ‡’\nselectcolumn winnerwhere textincludes new orleans\ndf[df['Host City'] =='New Orleans']['Winner'].count() df[df['Winner'].str.contains('New Orleans')]\n(1) select rows where column Host City is New Orleans\n(2) select column Winner\n(3) count\n(1) select rows where contains New Orleans from column Winner\nCode-like syntax\nCreate a column that shows the value of Space Flight (hr) divided by\nthe number of items of the Missions column(P26-2)\nâ‡’\n(1) create column Space Flight (hr) per Mission, (2) column Space Flight\n(hr) divided by(count , from column Missions + 1)\ndf['Space Flight (hr) per Mission'] = df['Space Flight\n(hr)'] / df['Missions'].str.count(',') + 1â†©â†’\ndf['Space Flight (hr) per Mission'] = df['Space Flight (hr)']\n/ (df['Missions'].str.count(',') + 1)â†©â†’\n(1) create column Space Flight (hr) per Mission,\n(2) column Space Flight (hr) divided by count â€œ, â€ from column\nMissions + 1.\n(1) create column Space Flight (hr) per Mission,\n(2) column Space Flight (hr) divided by count â€œ, â€ from column Mis-\nsions + 1.\nBridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models CHI â€™23, April 23â€“28, 2023, Hamburg, Germany\nTable 21: Examples of intent shaping changes.\nRewrite\nStrategy\nPrevious Query (Participant-Task)\nGeneratedPythonCode\nGrounded Utterances\nFollow-up Query\nGeneratedPythonCode\nGrounded Utterances\nNew intent\nSelect rows where basement > 0 and yr_built >= 1970 and\nyr_renovated > 0(P21-3)\nâ‡’\nCreatea columncalledconsider?wherethevalueis trueif basement> 0\nandyr_built>=1970andyr_renovated> 0\ndf[(df['sqft_basement'] > 0) & (df['yr_built'] >= 1970) &\n(df['yr_renovated'] > 0)]â†©â†’\n\"df['consider?'] = df['sqft_basement'] > 0\ndf['consider?'] = df['consider?'] & (df['yr_built'] >= 1970)\ndf['consider?'] = df['consider?'] & (df['yr_renovated'] > 0)\"\nN/A N/A\nStart over\nUsewinnercolumnto subtractthewinnerteamcolumnto getthe\nwinnercity(22-1)\nâ‡’\nCreatea newwinnercitythatremovethewinnerteamfromthewinner\ncolumn\ndf['Winner City'] = df['Winner'] - df['Winner team'] df['Winner City'] = df['Winner'].str.replace(r'\\b\\w+\\b', '')\n(1) create column Winner City,\n(2) column Winner - column Winner team.\n(1) create column Winner City,\n(2) select column Winner,\n(3) replaceâ€˜\\b\\w+\\bâ€™withâ€˜â€™.\nNext step\nCount the number of mission in column I delimited by comma(P24-2)\nâ‡’\nForeachnamecalculatethehoursof spaceflightdividedbythemission\ncount\ndf['Mission Count'] = df['Missions'].str.count(',') + 1 df['Hours per Mission'] = df['Space Flight (hr)'] /\ndf['Mission Count']â†©â†’\n(1) create column Mission Count,\n(2) count â€˜, â€™ from column Missions + 1.\n(1) create column Hours per Mission,\n(2) column Space Flight (hr) divided by column Mission Count.\nTesting\nCreatea newcolumnthatcountshowmanystringsareseparatedby\nâ€œ, â€intheâ€œMissionsâ€column.Createanothercolumnthatcalculates\nâ€œSpaceFlight(hr)â€dividedbyâ€œMissions_Countâ€(P25-2)\nâ‡’\nnewcolumn:# ofstringsseparatedbyâ€œ, â€inâ€œMissionsâ€.Createanother\ncolumnthatcalculatesâ€œSpaceFlight(hr)â€dividedbyâ€œMissions_Countâ€\ndf['Missions_Count'] = df['Missions'].str.count(',') + 1\ndf['Space Flight (hr) per Mission'] = df['Space Flight\n(hr)'] / df['Missions_Count']â†©â†’\ndf['Missions_Count'] = df['Missions'].str.count(',') + 1\ndf['Avg_Mission_Length'] = df['Space Flight (hr)'] /\ndf['Missions_Count']â†©â†’\nN/A N/A",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.86677485704422
    },
    {
      "name": "Programming language",
      "score": 0.6522015929222107
    },
    {
      "name": "Code generation",
      "score": 0.4816688299179077
    },
    {
      "name": "Abstraction",
      "score": 0.47668886184692383
    },
    {
      "name": "Natural language",
      "score": 0.42471325397491455
    },
    {
      "name": "Python (programming language)",
      "score": 0.42081156373023987
    },
    {
      "name": "Source code",
      "score": 0.42070603370666504
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2804015874862671
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I45129253",
      "name": "University College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1290206253",
      "name": "Microsoft (United States)",
      "country": "US"
    }
  ]
}