{
    "title": "BioVAE: a pre-trained latent variable language model for biomedical text mining",
    "url": "https://openalex.org/W3205613331",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5062861266",
            "name": "Hai-Long Trieu",
            "affiliations": [
                "University of Manchester",
                "National Institute of Advanced Industrial Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5101412540",
            "name": "Makoto Miwa",
            "affiliations": [
                "National Institute of Advanced Industrial Science and Technology",
                "Toyota Technological Institute"
            ]
        },
        {
            "id": "https://openalex.org/A5077976343",
            "name": "Sophia Ananiadou",
            "affiliations": [
                "Turing Institute",
                "University of Manchester"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2169099542",
        "https://openalex.org/W2948909602",
        "https://openalex.org/W2279376656",
        "https://openalex.org/W6758293299",
        "https://openalex.org/W2346452181",
        "https://openalex.org/W6780237715",
        "https://openalex.org/W6762287338",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2995435108",
        "https://openalex.org/W3035763680"
    ],
    "abstract": "Abstract Summary Large-scale pre-trained language models (PLMs) have advanced state-of-the-art (SOTA) performance on various biomedical text mining tasks. The power of such PLMs can be combined with the advantages of deep generative models. These are examples of these combinations. However, they are trained only on general domain text, and biomedical models are still missing. In this work, we describe BioVAE, the first large-scale pre-trained latent variable language model for the biomedical domain, which uses the OPTIMUS framework to train on large volumes of biomedical text. The model shows SOTA performance on several biomedical text mining tasks when compared to existing publicly available biomedical PLMs. In addition, our model can generate more accurate biomedical sentences than the original OPTIMUS output. Availability and implementation Our source code and pre-trained models are freely available: https://github.com/aistairc/BioVAE. Supplementary information Supplementary data are available at Bioinformatics online.",
    "full_text": null
}