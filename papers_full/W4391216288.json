{
    "title": "Deep Transformer-Based Asset Price and Direction Prediction",
    "url": "https://openalex.org/W4391216288",
    "year": 2024,
    "authors": [
        {
            "id": null,
            "name": "Gezici, Abdul Haluk Batur",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287160563",
            "name": "Sefer, Emre",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3172523270",
        "https://openalex.org/W4242322941",
        "https://openalex.org/W4205137258",
        "https://openalex.org/W2284153934",
        "https://openalex.org/W4388994448",
        "https://openalex.org/W3022746105",
        "https://openalex.org/W6637373629",
        "https://openalex.org/W1849277567",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W6763701032",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W6852499189",
        "https://openalex.org/W4317033517",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W6811014117",
        "https://openalex.org/W3209249686",
        "https://openalex.org/W4281848794",
        "https://openalex.org/W2800569739",
        "https://openalex.org/W2944069985",
        "https://openalex.org/W2897244933",
        "https://openalex.org/W2624385633",
        "https://openalex.org/W2021938316",
        "https://openalex.org/W2129413312",
        "https://openalex.org/W3125462345",
        "https://openalex.org/W2059852492",
        "https://openalex.org/W2612454343",
        "https://openalex.org/W2118067958",
        "https://openalex.org/W130570386",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W6684191040",
        "https://openalex.org/W2016053056",
        "https://openalex.org/W2120615054",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W4206706211",
        "https://openalex.org/W6697136110",
        "https://openalex.org/W2026430219",
        "https://openalex.org/W2342352817",
        "https://openalex.org/W19790595",
        "https://openalex.org/W2762958083",
        "https://openalex.org/W2889201272",
        "https://openalex.org/W2802496958",
        "https://openalex.org/W2893230400",
        "https://openalex.org/W6750995883",
        "https://openalex.org/W4224220755",
        "https://openalex.org/W3035414307",
        "https://openalex.org/W6783992249",
        "https://openalex.org/W6766007741",
        "https://openalex.org/W4306962664",
        "https://openalex.org/W2195027038",
        "https://openalex.org/W6888629825",
        "https://openalex.org/W3124979809",
        "https://openalex.org/W2945020349",
        "https://openalex.org/W6763509872",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W4241584625",
        "https://openalex.org/W6768817161",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4312729969",
        "https://openalex.org/W3105448877",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W4285661751",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3203509268",
        "https://openalex.org/W3123329971"
    ],
    "abstract": "The field of algorithmic trading, driven by deep learning methodologies, has garnered substantial attention in recent times. Within this domain, transformers, convolutional neural networks, and patch embedding-based techniques have emerged as popular choices within the computer vision community. Here, inspired by the latest cutting-edge computer vision methodologies and the existing work showing the capability of image-like conversion for time-series datasets, we apply more advanced transformer-based and patch-based approaches for predicting asset prices and directional price movements. The employed transformer models include Vision Transformer (ViT), Data Efficient Image Transformers (DeiT), and Swin. We use ConvMixer for a patch embedding-based convolutional neural network architecture without a transformer. Our tested transformer-based and patch-based methodologies aim to predict asset prices and directional movements using historical price data by leveraging the inherent image-like properties within the historical time-series dataset. Before the implementation of attention-based architectures, the historical time series price dataset is transformed into two-dimensional images. This transformation is facilitated through the incorporation of various common technical financial indicators, each contributing to the data for a fixed number of consecutive days. Consequently, a diverse set of two-dimensional images is constructed, reflecting various dimensions of the dataset. Subsequently, the original images depicting market valleys and peaks are annotated with labels such as Hold, Buy, or Sell. According to the experiments, trained attention-based models consistently outperform the baseline convolutional architectures, particularly when applied to a subset of frequently traded Exchange-Traded Funds (ETFs). This better performance of attention-based architectures, especially ViT, is evident in terms of both accuracy and other financial evaluation metrics, particularly during extended testing and holding periods. These findings underscore the potential of transformer-based approaches to enhance predictive capabilities in asset price and directional forecasting. Our code and processed datasets are available at https://github.com/seferlab/price_transformer.",
    "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\nDeep Transformer-based Asset Price and\nDirection Prediction\nBatur Gezici1, (Member, IEEE), and Emre Sefer1(Member, IEEE)\n1Computer Science Department, Ozyegin University, Istanbul, Turkey\nCorresponding author: Emre Sefer (e-mail: emre.sefer@ozyegin.edu.tr).\nABSTRACT\nThe field of algorithmic trading, driven by deep learning methodologies, has garnered substantial attention\nin recent times. Within this domain, transformers, convolutional neural networks, and patch embedding-\nbased techniques have emerged as popular choices within the computer vision community. Here, inspired\nby the latest cutting-edge computer vision methodologies and the existing work showing the capability\nof image-like conversion for time-series datasets, we apply more advanced transformer-based and patch-\nbased approaches for predicting asset prices and directional price movements. The employed transformer\nmodels include Vision Transformer (ViT), Data Efficient Image Transformers (DeiT), and Swin. We use\nConvMixer for a patch embedding-based convolutional neural network architecture without a transformer.\nOur tested transformer-based and patch-based methodologies aim to predict asset prices and directional\nmovements using historical price data by leveraging the inherent image-like properties within the historical\ntime-series dataset. Before the implementation of attention-based architectures, the historical time series\nprice dataset is transformed into two-dimensional images. This transformation is facilitated through the\nincorporation of various common technical financial indicators, each contributing to the data for a fixed\nnumber of consecutive days. Consequently, a diverse set of two-dimensional images is constructed, reflecting\nvarious dimensions of the dataset. Subsequently, the original images depicting market valleys and peaks\nare annotated with labels such as Hold, Buy, or Sell. According to the experiments, trained attention-\nbased models consistently outperform the baseline convolutional architectures, particularly when applied\nto a subset of frequently traded Exchange-Traded Funds (ETFs). This better performance of attention-\nbased architectures, especially ViT, is evident in terms of both accuracy and other financial evaluation\nmetrics, particularly during extended testing and holding periods. These findings underscore the potential of\ntransformer-based approaches to enhance predictive capabilities in asset price and directional forecasting.\nOur code and processed datasets are available at https://github.com/seferlab/price_transformer.\nINDEX TERMS Asset Price Prediction, Deep Learning, Attention, Vision Transformers, Convolutional\nNeural Network\nI. INTRODUCTION\nThe prediction of asset prices, including stocks, through\nartificial intelligence systems has been a subject of study\nfor nearly three decades. In contemporary financial markets,\nthe scope of tradeable instruments extends beyond stocks\nto include options [1], Exchange-Traded Funds (ETFs) [2],\ncryptocurrencies [3], commodities [4], etc. Correspondingly,\nthe role of artificial intelligence-based trading systems has\ngrown in significance and functionality across diverse global\nmarkets [4].\nDeep learning methods have recently exhibited superior\nperformance in various classification and prediction tasks\ncompared to more traditional machine learning models like\nSupport Vector Machines (SVMs). Notably, deep learning ex-\ncels in image processing tasks such as segmentation and clas-\nsification, marking a significant departure from conventional\nmethodologies [5]. A parallel trend has emerged in finan-\ncial prediction and classification tasks, encompassing asset\nprice and directional prediction. While traditional models like\nLong Short-Term Memory (LSTM), Convolutional Neural\nNetworks (CNN), and Recurrent Neural Networks (RNN)\nhave been employed, the application of these techniques in\nfinancial prediction lags behind their prevalence in other\nartificial intelligence areas such as computer vision [6]–[8]\nand Natural Language Processing [9]–[11]. Among these\ntechniques, CNNs have demonstrated notable performance\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3358452\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nGezici and Sefer: Deep Transformer-based Asset Price and Direction Prediction\nin financial asset price prediction, even though their primary\napplication has been in computer vision tasks like image\nclassification [5].\nTransformers [12], [13], introduced primarily for se-\nquence analysis in natural language processing, have re-\ncently surpassed CNNs in multiple vision tasks. Vision Trans-\nformer (ViT) [14], for example, has shown promising results\nand surpassed state-of-the-art convolutional neural networks\nwhile requiring significantly less training cost. This suc-\ncess has led to the development of various advanced vision\ntransformers for computer vision tasks. Transformers lever-\nage attention and self-attention mechanisms, facilitating the\nestablishment of connections within sequences or images.\nSelf-attention, a mechanism that relates different parts of an\ninput while computing a representation of the same input,\nhas proven particularly effective in addressing longer-range\ndependencies.\nIn this study, we evaluate three commonly used vision\ntransformer-based approaches and a recently proposed patch\nembedding-based approach within our algorithmic trading\nframework, aiming to predict both asset prices and directional\nmovements. Among these three transformer techniques, ViT,\na vision transformer-based algorithm relies on attention and\nself-attention mechanisms. ViT can be seen as the first adapta-\ntion of the transformer which is proposed for natural language\nprocessing tasks to the vision tasks. ViT combines those\nattention mechanisms with patch-based embeddings. Another\ntransformer, DeIT (Data-Efficient Image Transformers) [15]\nis similar to ViT except for the distillation mechanism which\nincorporates the soft and hard predictions from both the stu-\ndent and teacher sides respectively. More recently proposed\nSwin [16] is the last transformer we analyze. Swin trans-\nformer is a hierarchical Vision Transformer that uses shifted\nwindows to efficiently calculate the attention. Lastly, we also\napply a patch-embedding convolutional neural network-based\napproach ConvMixer [17] which combines multiple convolu-\ntional architectures with patch embeddings. ConvMixer ar-\nchitecture is similar to ViT, except it completely removes the\ntransformer blocks. So, its performance will be important in\nanalyzing the importance of transformer blocks.\nIn our financial prediction problem, patch-embedding\nconvolutional neural networks and transformers repre-\nsent two distinct methodologies employed in transform-\ning one-dimensional financial time series datasets into two-\ndimensional representations akin to images. We transform\nour time-series datasets into an image by using 65 different\ntechnical indicators, each with various parameter combina-\ntions over a defined period. Some common examples of these\ntechnical indicators are MACD [18], RSI [19], Bollinger\nBands [20], Fibonacci Retractment, etc. Once turned into\na matrix, the resulting 2D representation organizes rows to\ncluster indicators with similar patterns, ensuring smooth tran-\nsitions along the y-axis, and capturing consecutive patterns\nfor deep learning techniques in asset price and direction\npredictions.\nIn our experiments, we generate images of varying di-\nmensions through technical indicators, feeding them into Vi-\nsion Transformer and patch embedding-based convolutional\nneural networks, respectively. The application of algorith-\nmic trading approaches involving the transformation of time\nseries datasets into 2D representations is relatively limited\n[21]. CNN-TA, proposed in [21], utilizes convolutional neu-\nral networks for understanding, yet as our results indicate,\nsuch networks do not surpass more recent architectures like\ntransformers or patch embedding-based CNNs. To the best\nof our knowledge, the utilization of algorithmic trading by\ntransforming time series datasets into 2D representations and\nsubsequently processing them through transformers or patch\nembedding-based CNNs, analogous to 2D image classifica-\ntion tasks, is unprecedented, even in other financial prediction\ndomains.\nOur detailed experiments demonstrate that transformer-\nbased approaches generally outperform the well-known base-\nline methods and similar methods utilizing simpler CNNs\nwithout the transformer architecture across extended periods.\nAcross both longer and shorter testing periods, transformer-\nbased architectures are still more accurate than all baselines\nand the competing CNN-based method CNN-TA. Moreover,\ntransformer-based models outperform a simple buy-and-hold\nstrategy [22], common technical indicator-based models [23],\nMultilayer Perception (MLP) [24], and a common deep learn-\ning time series forecasting model LSTM [25]. While the\nperformance of transformer-based approaches is promising,\nfurther enhancement is achievable through more detailed hy-\nperparameter optimization and fine-tuning.\nA. RELATED WORK\nFinance can be seen as one of the most focused machine\nlearning application areas during the last 30 years. Until\nnow, many research papers have been published. Traditional\nmachine learning methodologies have been extensively em-\nployed for stock market forecasting, with studies focusing\non applying time-series prediction techniques directly to fi-\nnancial datasets. Some have utilized fundamental or technical\nanalysis techniques for accurate forecasting, as demonstrated\nin surveys such as [4]. Previous works, like [26]–[30], have\napplied Artificial Neural Networks (ANNs) to predict stock\nindex values or forecast stock prices, integrating technical\nanalysis indicators. [31] has compared the existing common\ntechniques for text mining which are also adapted to stock\nmarket prediction problems such as Rule-based systems, Ge-\nnetic algorithms (GAs), and neural networks. Among other\nrelated research, [32] used kernel techniques to forecast stock\nmarket changes. In general, machine learning techniques per-\nformance is limited since they cannot infer deeper attributes\nin the financial dataset which is a key factor in closing the gap\nbetween machine and human trading performance.\nIn recent years, the surge in computational capacity has led\nto the emergence of newer deep learning-based approaches.\nDeep learning, as a specialized case of ANN with multi-\nple layers, has demonstrated enhanced performance com-\npared to shallower networks [33]. Various deep learning\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3358452\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nGezici and Sefer: Deep Transformer-based Asset Price and Direction Prediction\nmodels, including Convolutional Neural Networks (CNNs),\nLong Short-Term Memory (LSTM), Recurrent Neural Net-\nworks (RNNs), and Transformers, have found applications\nacross diverse domains. CNNs, for instance, are widely used\nin image classification [34], [35] and video processing, as\nwell as in natural language processing tasks such as sentence\ncategorization [36]. LSTMs and RNNs are predominantly\nemployed for sequential data analysis in speech processing,\nnatural language processing, and time-series-related tasks.\nTransformers, initially applied to Natural Language Process-\ning (NLP) tasks [37], have more recently demonstrated suc-\ncess in computer vision tasks [14], [38].\nWhile deep CNNs and transformer-based architectures\nhave become prevalent in the last decade, their application in\nfinancial tasks remains limited. Existing studies, such as [21],\n[25], [39]–[43], have explored deep learning approaches for\nforecasting stock markets, leveraging events data, financial\ntime-series datasets, and news integration. Relevant studies\nin [21], [43] incorporate technical analysis indicators into pre-\ndiction models, employing feedforward neural networks and\nCNNs with two-dimensional matrix characterizations of tech-\nnical analysis datasets. Notably, deep learning demonstrates\naccurate learning and generalization across buy and sell time\npoints over extended testing horizons in these studies.\nOther studies enhance stock market prediction perfor-\nmance by incorporating extra strategies that can be utilized\nto assist in market movement prediction. Such methods are\ngenerally called hybrid methods, which are deep learning\ntechniques integrated with extra algorithms. Those hybrid\ntechniques have shown great promise in terms of their pre-\ndictive abilities [44], [45]. For instance, the method proposed\nin [46] uses adversarial training to enhance neural networks\ngeneralizability by preventing overfitting while predicting\nstock price rise or fall. Similarly, [47] came up with a novel\ndeep generative architecture that combines price and textual\ndatasets to better model the stock prediction problems com-\nplexity. Different than traditional discriminative topic models,\nthe model proposed in [47] handles the stochasticity better by\nintroducing recursive variables where they used Variational\nAutoencoders (V AEs) for detailed reasoning.\nOn the other hand, transformer-based approaches for fi-\nnancial market prediction are notably limited. Studies such\nas [48]–[50] propose novel transformer architectures or meth-\nods to predict stock movements, incorporating feature engi-\nneering and multi-scale Gaussian priors. These studies also\nintegrate textual attributes from the Twitter dataset into the\nprediction. The existing studies frequently utilize CNNs for\nimage analysis and classification tasks, while deep learn-\ning methods, particularly LSTM and RNN, are commonly\nemployed for financial time-series prediction. However, the\nintegration of technical analysis datasets with deep learning\ntechniques, especially the combination of CNNs with two-\ndimensional matrix characterizations of these datasets, is\nrelatively rare in algorithmic trading studies [5], [21], [30],\n[51], [52]. These studies typically convert the financial time-\nseries prediction problem into an image classification task\nby generating two-dimensional images from price indicators.\nRecent work [52] applies a single type of vision transformer\nto such image-like data for the first time.\nII. DATA PREPROCESSING\nA. DATASET PREPARATION\nFinancial data is generally analyzed by either technical analy-\nsis (TA), fundamental analysis, or quantitative analysis [4]. In\nfundamental analysis, investors focus on predicting an asset’s\nprice by examining the corresponding company’s reported\nmetrics such as long-term debt, short-term debt, cash flow,\nreturn on equity, return on asset, earning per share, etc [53].\nOn the contrary, technical analysis is based on analyzing the\ntemporal price and volume characteristics of a company via\npredefined mathematical models. A huge number of technical\nanalysis indicators have been proposed to help with financial\nasset price prediction over time.\nHere, we have collected Open, High, Low, Close, Vol-\nume (OHLCV) time-series datasets for nine of the frequently-\ntraded Exchange-Traded Funds (ETFs) by using the yfinance\nlibrary [54] over 20 years period from 1/1/2002 to 1/1/2022\nfor training and testing our approaches. This 20-year duration\nconsists of both bull and major bear markets such as the\n2008 financial crisis and 2020 coronavirus crises. These 9\nETFs are summarized in Table 1. We applied 65 commonly-\nused technical indicators [55] which focus on different time\nhorizons on distinct categories such as volatility, momentum,\nvolume, overlap studies, price transformation, and statistics.\nWe use TA-Lib (Technical Analysis Library) in Python to\ncalculate these indicator values over time. All used indicators\nare summarized in Table 2.\nSymbol Description Inception Date\nXLF Financial Select Sector SPDR ETF 12/16/1998\nXLU Utilities Select Sector SPDR ETF 12/16/1998\nQQQ PowerShares QQQ ETF 10/03/1999\nSPY SPDR S&P 500 ETF 1/22/1993\nXLP Consumer Staples Select Sector SPDR ETF 12/16/1998\nEWZ iShares MSCI Brazil Capped ETF 7/10/2000\nEWH iShares MSCI Hong Kong ETF 3/12/1996\nXLY Consumer Discret Sel Sect SPDR ETF 12/16/1998\nXLE Energy Select Sector SPDR ETF 12/16/1998\nTABLE 1: ETFs used in our analysis and their attributes.\nB. GENERATING LABELS\nWe labeled the time-series dataset for our supervised learning\nproblem, after preprocessing and partitioning the dataset. In\nthis case, we assigned one of Buy, Hold, or Sell classes ac-\ncording to a predetermined threshold value. Data for periods\nabove the threshold are tagged as Buy, below the threshold\nare tagged as Sell, and the remaining periods are tagged\nas Hold. The threshold value has a significant impact on\nthe label distribution, as the generated dataset frequency is\ndependent on the threshold. In our analysis, we focused on\n2 threshold values 0.0038 and 0.01, which correspond to\nbalanced and imbalanced datasets respectively. Evaluating\nthe performance on both balanced and imbalanced datasets is\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3358452\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nGezici and Sefer: Deep Transformer-based Asset Price and Direction Prediction\nTABLE 2: Technical Analysis Indicators Used in our study\nCategory Indicators\nStatistics\nBETA, CORREL, LINEARREG,\nLINEARREG_ANGLE,\nLINEARREG_INTERCEPT,\nLINEARREG_SLOPE, STD, TSF, V AR\nPrice Transforma-\ntion\nA VGPRICE, MEDPRICE, TYPPRICE, WCL-\nPRICE\nVolatility TRANGE\nVolume AD, ADOSC, OBV\nMomentum\nADX, ADXR, APO, AROONUP,\nAROONDOWN, AROONOSC, BOP, CCI,\nCMO, DX, FASTD, FASTDRSI, FASTK,\nFASTKRSI, MACD, MACDEXT, MACDFIX,\nMFI, MINUS_DI, MINUS_DM, MOM,\nPLUS_DI, PLUS_DM, PPO, ROC, ROCP,\nROCR, ROCR100, RSI, SLOWD, SLOWK,\nTRIX, ULTOSC, WILLR\nOverlap Studies\nBBANDSL, BBANDSM, BBANDSU, DEMA,\nEMA, HT_TRENDLINE, KAMA, MA,\nMIDPOINT, MIDPRICE, SMA, TEMA,\nTRIMA, WMA\ncrucial for quality and robustness. Intuitively, 0.01 threshold\nmeans buying the asset for a price increase greater than 1%\nand selling the asset for a price decrease greater than 1% in a\nday which is a reasonable assumption even though it results\nin an imbalanced dataset. On the other hand, 0.0038 threshold\ngenerates a dataset of almost equal distribution among these\nthree classes.\nC. MAPPING TECHNICAL INDICATORS TO IMAGES\nTo apply vision transformer-based and patch-based ap-\nproaches, we transform our one-dimensional time-series\ndataset into 2D images. While converting time-series data to\nimage dataset, we calculate MACD, CMO, PPO, WILLR,\nEMA, WMA, SMA, ROC, CCI, TEMA, RSI, and 54 more\ntechnical indicator signals across different horizons. These\nindicator ranges are between one week to three weeks. At\na higher level, those technical analysis indicators might also\nbe considered as filters applied to financial time series. For\ninstance, the Fast Fourier Transform will also be an indica-\ntor, but we do not consider it in our application since it is\nnot finance-specific. Those indicators are frequently utilized\nand ideal for medium-horizon trading, which is the focus of\nthis research. If one is interested in high-frequency trading\nor longer-term trading, these technical analysis indicators\nshould be adjusted accordingly.\nAfter applying these technical indicators to a one-\ndimensional time-series dataset of 65 historical days, we gen-\nerate a 65 × 65 image of each training sample. In this image\nmatrix, each column represents a distinct day in these 65 days\nwhereas each row corresponds to a different technical indi-\ncator. Even though columns are already ordered temporally,\nthere is no such apparent ordering for technical indicators.\nThe indicator ordering is a key factor in our results since we\nwill obtain distinct images for distinct orderings. In our case,\nwe sort the resulting set of indicators by their categories as\nshown in Table 2. Moreover, once we generate the images\nfor a given ordering, we apply standardization to make the\ndataset more robust and consistent [56]. For instance, we\nplot examples of 65 × 65 pixel images generated during our\npreprocessing in Figure 1.\nFIGURE 1: 65 × 65 Labeled Generated Images\nIII. METHODS\nWe test the transformer-based and patch embedding-based\nalgorithmic trading approaches performance and compare it\nwith baselines including convolutional neural networks.\nA. VIT (VISION TRANSFORMER)\nTransformers have frequently resulted in state-of-the-art out-\ncomes for many natural language processing tasks. However,\nthey cannot be directly applied to image-like data. A natural\nadaptation is to apply attention together with convolutional\nnetworks, or to modify only certain portions of convolutional\nnetworks without modifying the overall architecture. Direct\nself-attention application on image-like data will result in\na very high complexity since each pixel should attend to\nevery other pixel. Different solutions have been proposed to\ndecrease this complexity [57], [58] which apply self-attention\nto local neighborhoods in a scalable way.\nFollowing such adaptations, researchers came up with\nViT (Vision Transformer) [14] which can be seen as a pure\ntransformer for image-like data. ViT applies self-attention\ndirectly to sequenced image patches, where it completely\nremoves the dependence on CNNs. Since its proposal, ViT\nhas been integrated into many computer vision tasks includ-\ning image segmentation, and image classification. Its perfor-\nmance has been state-of-the-art in those domains. In ViT,\nwe have a patch and position-based embedding layer that\nfirst partitions the image into sub-images. Once images are\npartitioned, it is flattened and a transformer encoder which\nhas multi-layer attention is applied as in Figure 2.\nIn more detail, given an image of W ×H and a patch size B,\nViT first transforms the image into W\nB × H\nB size components.\nOnce these W\nB × H\nB are flattened, a multi-layer encoder is\napplied. In each encoder layers, as defined in [14], the forward\nequations are follows:\nz0 = [xcl ass; x1\np E; x2\np E; . . .; xN\np E] +Epos, (1)\nE ∈ R(P2C)×D, Epos ∈ R(N+1)×D\n(2)\nz′\nl = MSA(LN(z − 1)) +zl−1, l = 1. . .L (3)\nzl = MLP(LN(z)) +z′\nl , l = 1. . .L (4)\ny = LN(zL 0) (5)\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3358452\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nGezici and Sefer: Deep Transformer-based Asset Price and Direction Prediction\nFIGURE 2: Overview of ViT architecture in our problem.\nwhere MSA represents multi-headed self-attention, MLP rep-\nresents multilayer perception, and LN represents layer nor-\nmalization in the Transformer Encoder part. ViT adds posi-\ntional embeddings to patch embeddings to keep positional\nknowledge.\nB. CONVMIXER\nWhile CNNs are pyramid architectures with decreasing res-\nolution by using convolution, ConvMixer [17] is an isotropic\narchitecture that is integrated with patch representation and\nconvolution. As summarized in Figure 3, ConvMixer archi-\ntecture is very simple and preserves locality using tensor\npatch embeddings. Afterwards, d copies of a simple Fully\nConvolutional block are applied where each block includes\nlarge-kernel depthwise convolution and pointwise convolu-\ntion. Lastly, global average pooling is applied which is fol-\nlowed by a simple fully connected layer.\nIn our case, ConvMixer [17] has been applied to discuss\nthe effectiveness of transformers for asset price prediction\nproblems. Even though ConvMixer has no attention mech-\nanism, it trains CNN units integrated with Gaussian Linear\nUnit (GeLU). More formally, ConvMixer implements patch\nembeddings with a patch size p and embedding dimension h\nas a convolution with cin input channels, h output channels,\nkernel size p, and stride p:\nz0 = BN (σ{Convcin→h(X, stride=p, kernel_size=p)})\n(6)\nwhere BN represents batch normalization. Patch embedding\nsummarizes a p ×p patch into an embedded vector of dimen-\nsions e. The authors implement this by a single convolution\nwith kernel size p, stride p, and h output channels, followed\nby a non-linearity. This surprising trick will convert the n ×n\nimage into features of shape h× n\np × n\np . The ConvMixer block\nis composed of depthwise convolution (grouped convolution\nwith groups equal to the number of channels, h) followed\nby pointwise convolution (kernel size 1 × 1). Each of these\nconvolutions is followed by an activation and post-activation\nbatch normalization as in:\nz′\nl = BN (σ{ConvDepthwise(zl−1)}) +zl−1 (7)\nzl+1 = BN (σ{ConvPointwise(z′\nl )}) (8)\nC. DEIT (DATA-EFFICIENT IMAGE TRANSFORMERS)\nDeiT [15] is another type of recent vision transformer that\nis based on Knowledge Distillation (KD). Knowledge dis-\ntillation [59] represents a training paradigm where a student\nmodel uses soft labels suggested by a strong teacher model.\nIn this case, soft labels are the output vector of the teacher’s\nsoftmax function instead of maximum scores which can be\nconsidered as a hard label. This type of training will enhance\nthe student model’s performance since it will transfer the\nknowledge from a larger model to a smaller one.\nDeiT studies the distillation of a transformer student\nby either a convolutional neural network or a transformer\nteacher. DeiT came up with a new distillation process that\nis transformer-specific. DeiT assumes to have a powerful\nclassifier as a teacher model which can be a convolutional\nneural network or a mixture of classifiers. It mainly tries to\nlearn a transformer by using the knowledge from the teacher.\nDeiT incorporates a new distillation token that interacts with\nthe class token and other patch tokens via the following self-\nattention layers. The architecture uses the distillation token\nsimilar to the class token. One main difference between them\nis that the distillation token focuses on reproducing the hard\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3358452\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nGezici and Sefer: Deep Transformer-based Asset Price and Direction Prediction\nResidual connection\n+\nDepthwiseConvolution\nGELU\nBatchNorm\nGELU\nBatchNorm\nPointwiseConvolution\nConvMixer Layer\ndepth\nGlobal Average Pooling\nFully-Connected\nClass\nGELU\nBatchNorm\nPatch Embedding\nFIGURE 3: Overview of ConvMixer architecture in our problem.\nlabel inferred by the teacher, whereas the class token’s objec-\ntive is to predict the true label.\nDeiT includes both soft and hard distillation. Soft dis-\ntillation focuses on minimizing the Kullback-Leibler (KL)\ndivergence between the softmax of the teacher model and\nthe softmax of the student model. More formally, let Zt and\nZs represent the logits of the teacher and student models\nrespectively.τ represents distillation temperature, λ balances\nthe KL divergence loss (KL) and the cross-entropy loss ( LCE)\non ground truth labels y, and ψ defines the softmax function.\nIn this case, the soft distillation objective can be defined as:\nLglobal = (1−λ)LCE(ψ(Zs), y)+λτ2KL(ψ(Zs/τ), ψ(Zt/τ)).\n(9)\nOn the other hand, in hard distillation, the teacher’s\nhard decision is treated as a true label. Assuming yt =\nargmaxcZt(c) be the hard decision of the teacher, the hard\ndistillation objective can be defined as:\nLhardDistill\nglobal = 1\n2LCE(ψ(Zs), y) +1\n2LCE(ψ(Zs), yt). (10)\nOverall, a novel distillation token is added to the initial\nembeddings which consist of the class token and patches.\nThe distillation token is similar to the class token in the\nway it interacts with other embeddings via self-attention and\noutput is generated by the neural network following the last\nlayer. While testing, both distillation and class embeddings\ngenerated by the transformer will be used by the following\nlinear classifiers.\nD. SWIN TRANSFORMER\nSwin transformer [16] is one of the most recent vision trans-\nformers which can be used as a general-purpose architecture\nfor almost all vision problems. Even though transformers\nhave been mainly proposed across language tasks, their adap-\ntation to the vision domain requires two main changes: They\nshould be adapted to large changes in visual entity scale, and\nimage pixel resolution is higher than the words in a document.\nSwin transformer addresses these challenges by proposing\na Hierarchical Transformer whose representation is obtained\nvia Shifted windows. In this case, shifted window-based mod-\neling is quite efficient since it restricts self-attention compu-\ntation to only local windows without an overlap.\nMore formally, Swin Transformer architecture is sum-\nmarized in Figure 4. Image-like input is split into non-\nintersecting patches similar to ViT. It treats each patch as a\n\"token\" and each patch attribute is composed of raw values\nconcatenation. Then, the Swin transformer applies a linear\nembedding layer to this feature for arbitrary dimension pro-\njection, denoted as C.\nSwin applies many Transformer blocks with modified\nself-attention computation on patch tokens. The Transformer\nblocks keep the number of tokens as ( H\n4 × W\n4 ). Over-\nall, Swin transformer replaces the standard multi-head self-\nattention (MSA) module with a shifted windows-based mod-\nule. As shown in Figure 4(b), a Swin Transformer block\nincludes a shifted window-based MSA module which is fol-\nlowed by a 2-layer Multilayer Perceptron (MLP) with GELU\nnon-linearity in between. Swin Transformer block applies a\nlayer normalization (LN) before each MSA module and MLP,\nwhere a residual connection is applied after each module.\nQuadratic complexity is the main challenge for both stan-\ndard Transformer architecture [12] and its vision-based ver-\nsion [14] since both approaches compute the relationships\nbetween all token pairs. To efficiently model self-attention\nin image-like data, Swin transformer calculates self-attention\nwithin local windows where it arranges windows for nonover-\nlapping partition of the image-like data. Assuming each win-\ndow contains M × M patches, Swin uses the shifted window\npartitioning technique to efficiently compute non-intersecting\nwindows. In this case, we alternate between different 2 parti-\ntion configurations in successive Swin Transformer blocks.\nThe initial block utilizes an orderly window partitioning\napproach where it starts from the left-upper pixel, and the\n8 × 8 feature map is evenly split into 2 × 2 windows of\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3358452\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nGezici and Sefer: Deep Transformer-based Asset Price and Direction Prediction\nFIGURE 4: (a) Overview of Swin Transformer architecture in our problem. (b) Consecutive Swin transformer blocks where W-\nMSA and SW-MSA represent multi-head self-attention modules with regular and shifted window configurations, respectively.\nsize 4 × 4 (M = 4 ). However, the latter block uses a\nconfiguration that is shifted from that of the preceding layer,\nby displacing the windows by (⌊M\n2 ⌋, ⌊M\n2 ⌋) pixels from the\nregularly partitioned windows. With the shifted window par-\ntitioning approach, consecutive Swin Transformer blocks are\ncomputed as:\nˆzl = W-MSA\n\u0000\nLN\n\u0000\nzl−1\u0001\u0001\n+ zl−1 (11)\nzl = MLP\n\u0000\nLN\n\u0000ˆzl \u0001\u0001\n+ ˆzl (12)\nˆzl+1 = SW-MSA\n\u0000\nLN\n\u0000\nzl \u0001\u0001\n+ zl (13)\nzl+1 = MLP\n\u0000\nLN\n\u0000ˆzl+1\u0001\u0001\n+ ˆzl+1 (14)\nwhere ˆzl and zl represent the output features of the (S)W-\nMSA and the MLP modules for block l, respectively. Ad-\nditionally, W-MSA and SW-MSA represent window-based\nmulti-head self-attention utilizing regular and shifted window\npartitioning configurations, respectively.\nIV. EXPERIMENTS\nA. DATA PREPARATION\nSince we are applying an evaluation on a time-series dataset,\nwe chose the train and test datasets in continuous intervals.\nETF prices between 1/1/2002 to 1/1/2022 are used in our\nanalysis. We apply a sliding window-based with a retraining\napproach, where a consecutive 5-year interval is selected as\nthe training period and the following year is selected as the\ntesting period. Following such partition, we shift train and test\ndataset periods one more year forward and repeat the training\nprocess above. As a result, each year between 2007 and 2021\nis tested once by such repeated retraining technique. Once\nthe images are generated and standardization is applied, we\nobtain 45090 images, or 5010 images for each ETF in total.\nWe combine all distinct ETF images into a single dataset since\na joint model is trained for all ETFs.\nB. BASELINE TECHNIQUES\nWe focused on 6 baselines while comparing the performance\nof distinct transformers: Buy & Hold, RSI (Relative Strength\nIndex) (14 days, 70–30), SMA (Simple Moving Average) (50\ndays), LSTM [25], MLP regression [60], and Enhanced CNN-\nTA. We have implemented each of these techniques and\nanalyzed associated financial metrics. Among them, Buy &\nHold Strategy (BaH) baseline simply longs the assets at the\nbeginning of the test period and it unrolls the position by\nselling it once the test period finishes. In the RSI model,\nwe trade by only using the RSI indicator. In this case, we\ncalculate RSI for each testing day and we buy if RSI is\nless than 30. On the other hand, if RSI is greater than 70,\nwe trigger a sell signal. Similarly, in the SMA model, we\ncalculate a 50-day SMA for each testing day. We generate\na buy signal if the associated test data is greater than a 50-\nday SMA, and a sell signal if the test data is less than 50 days\nSMA. LSTM and MLP regression techniques are utilized as\nbaselines in financial time series data analysis. Our LSTM\nbaseline consists of 25 neurons (1 neuron in the input layer,\n25 neurons in the latent layer, 1 neuron in the output layer).\nIn this case, dropout is selected as 0.5 and we run LSTM for\n1000 epochs. On the other hand, our MLP baseline consists of\nfour layers which have 1, 10, 5, and 1 neurons consecutively.\nFor MLP, dropout is again selected as 0.5 and we run MLP\nfor 200 epochs.\nLastly, we have enhanced CNN-TA [21] and used it as\nour last baseline. CNN-TA architecture consists of deep\nconvolutional neural networks, making predictions on two-\ndimensional data. Once it gets 2D input, it has 2 convolutional\nlayers where each applies 3×3 kernel. After these convolution\nlayers, max pooling is applied. It has 2 dropouts with 0, 25\nand 0.5 probabilities. Lastly, the output of these convolu-\ntional steps is connected to a fully connected layer which\ngenerates the output. Even though CNN-TA is also a neural\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3358452\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nGezici and Sefer: Deep Transformer-based Asset Price and Direction Prediction\nnetwork-based baseline, its fully convolutional structure does\nnot incorporate an attention mechanism or patch embeddings.\nIt also uses only 15 technical indicators. To make a fair\ncomparison between CNN-TA and vision transformers, we\nincrease the number of technical indicators from 15 to 65\nsince all vision transformers use 65 indicators. In this case,\nthis enhanced approach is called Enhanced CNN-TA or CNN-\nTA++ in short and we use 65 × 65 images instead of 15 × 15\nimage as input.\nC. PERFORMANCE EVALUATION\nWe assess and compare the performance of multiple vision\ntransformer-based approaches, and the proposed 6 baselines\nvia traditional machine learning metrics and financial metrics.\nIn terms of traditional metrics, we use Accuracy, Recall,\nPrecision, and F1 score for classification performance eval-\nuation. These metrics are defined below:\nAccuracy = TP + TN\nTP + TN + FP + FN (15)\nPrecision = TP\nTP + FP (16)\nRecall = TP\nTP + FN (17)\nF1 Score =2 × Precision × Recall\nPrecision + Recall (18)\nwhere TP, TN, FP, and FN represent true positive, true neg-\native, false positive, and false negative sample counts, re-\nspectively. Let TPR = TP\nTP+FN be the true positive rate and\nFPR = FP\nFP+TN be the false positive rate.\nIn addition to the traditional machine learning methods,\nwe also evaluate the financial performance of the proposed\ntransactions. In this case, we simulate the daily trades by\nutilizing the model predictions over the test dataset. We can\neither buy, sell, or hold an asset. When we predict the label as\n\"Buy\", we buy the asset by using all our available capital if\nit has not been already bought. When we predict the label as\n\"Sell\", we sell the asset completely if it has been previously\nbought. Lastly, when we predict the label as \"Hold\", we do\nnot take any action. If we predict the same label consecutively\nwhile executing a trade, we only focus on the first label and\nwe perform the associated transaction. If a label is repeated,\nwe ignore the label until the label changes. We assume that\nwe start with $10000 cash at the beginning and the trading\ncommission is assumed to be $1 per transaction since we\nare trading only liquid high-volume EFTs. We summarize the\nfinancial evaluation scenario in Eq. 19 where \"S\" indicates\nthe financial evaluation scenario, \"Money\" indicates the total\namount of cash, and \"#OfStocks\" represents the number of\nstocks.\nS =\n\n\n\n#OfStocks = tMoney\nprice if label = ’Buy’\ntMoney = price ∗ #OfStocks if label = ’Sell’\nno action if label = ’Hold’\n(19)\nOne common financial evaluation metric for a strategy is\nSharpe Ratio [61] which divides the method’s annualized re-\nturn relative to risk-free rate by annualized standard deviation\nwhere a 3-month US treasury bill is used to model the risk-\nfree rate. Our remaining evaluation metrics are defined by the\nfollowing equations:\nAR =\n \u0012totalMoney\nstartMoney\n\u0013 1\nnumberOfYears\n− 1\n!\n∗ 100 (20)\nAnT = transactionCount\nnumberOfYears (21)\nPoS = successTransactionCount\ntransactionCount ∗ 100 (22)\nApT = totalPercentProfit\ntransactionCount ∗ 100 (23)\nL = totalTransactionLength\ntransactionCount ∗ 100 (24)\nIdleR = data.length − totalTransLength\ndata.length ∗ 100 (25)\nwhere AR represents the annualized return, AnT represents\nthe annualized number of transactions, PoS represents the\npercent of success, ApT represents the average percent profit\nper transaction, L represents the average transaction length in\ndays, and IdleR represents the idle ratio. Additionally, MpT\nand MlT represent the maximum profit/loss percentage in\ntransaction respectively. We have implemented transformer\ntechniques and baseline approaches mainly by using Hug-\ngingFace [62] and Pytorch [63]. We have also employed\nvarious Java libraries, including Spark, and Hadoop among\nothers, to facilitate the implementation and execution of the\nexperiments. Our code and processed datasets are available\nat https://github.com/seferlab/price_transformer.\nV. RESULTS\nA. COMPARISON VIA TRADITIONAL METRICS\nAs indicated in Table 3, the ConvMixer algorithm exhibits\nsuperior performance in terms of test accuracy. Among the\ntested vision transformers, the ViT architecture closely trails\nthe ConvMixer, with the most notable divergence found in test\naccuracy, where there exists a substantial margin exceeding\n8%. Among the other vision transformers, the hierarchical\ntransformer Swin performs better than DeiT. Among the all\nconsidered baselines, the best-performing baseline is CNN-\nTA++, and it performs worse than all transformers except\nDeiT in terms of accuracy. We retrieved the performance of\nall baselines other than CNN-TA++ from a similar analysis\napplied in [21].\nHowever, in terms of F1 score, vision transformer ViT\noutperforms all other methods. Even though the patch\nembedding-based non-transformer approach ConvMixer per-\nforms quite well in terms of accuracy, this is not correct in\nterms of F1 score. This can be explained by the fact that\nthere are more \"Hold\" signals in our datasets. Such imbalance\namong the classes causes problems for ConvMixer and all\nother transformers except ViT. These \"Hold\" signals are more\nlearnable for ViT. Our dataset can be considered as limited\nfor deeper and more powerful architectures, so it is easier for\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3358452\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nGezici and Sefer: Deep Transformer-based Asset Price and Direction Prediction\nViT to learn the True Positives of \"Hold\", \"Buy\" and \"Sell\"\nsignals.\nAlgorithm Train Loss Train Accuracy Test Accuracy Test F1\nViT 5,27% 98,31% 79,90% 0.71\nConvMixer 1,58% 99,57% 87,94% 0.32\nDeiT 14,83 60,32% 44,24% 0.12\nSwin 34,15% 89,57% 77,49% 0.47\nCNN-TA++ 34,00% 86,26% 62,06% 0.65\nBaH - - 58,04% 0.61\nLSTM 37,12% 83,21% 61,11% 0.49\nMLP 39,15% 83,12% 60,55% 0.45\nRSI - - 58,34% 0.46\nSMA - - 56,01% 0.47\nTABLE 3: Train and test performance of the tested algorithms\nand baselines.\nB. CONFUSION MATRICES\nThe examination of the confusion matrices for tested architec-\ntures elucidates notable strengths and challenges inherent in\ntheir classification performance. We report the performance\nof tested approaches as shown in Tables 4-7. For instance,\nfor ConvMixer in Table 4, the model demonstrates profi-\nciency in accurately identifying certain instances within the\n\"Hold\" class; however, a considerable number of false neg-\natives (744) is observed. Conversely, the model encounters\nmore pronounced challenges in classifying instances within\nthe \"Buy\" and \"Sell\" classes. Notably, the \"Buy\" class is\ncharacterized by a complete absence of true positives, indi-\ncating a deficiency in the model’s ability to correctly identify\ninstances of this class. In the case of the \"Sell\" class, the model\nexhibits a noteworthy occurrence of false positives (782),\nsignifying instances where the model incorrectly predicts the\npresence of \"Sell\" orders. These detailed findings underscore\nthe imperative of addressing and refining the ConvMixer\narchitecture, particularly in enhancing its sensitivity to \"Buy\"\nclass instances and reducing false positives in the prediction\nof \"Sell\" orders.\nIn terms of Swin architecture as seen in Table 5, the ex-\namination of its confusion matrix f illuminates its proficiency\nin accurately identifying instances within the \"Hold\" class,\nunderscored by a notable count of true positives (898) and\na minimal occurrence of false negatives (196). However, the\nmodel’s performance encounters challenges in both the \"Buy\"\nand \"Sell\" classes. Specifically, the \"Buy\" class exhibits a\ndiminished number of true positives (2) and a comparatively\nelevated count of false positives (40), implying difficulties in\nthe accurate identification of instances within this category.\nSimilarly, in the \"Sell\" class, the model demonstrates limited\nsuccess, with only 3 true positives and 167 false positives.\nThis implies a notable struggle in effectively distinguishing\nbetween instances belonging to the \"Sell\" and \"Hold\" classes.\nIn terms of ViT architecture as seen in Table 6, its con-\nfusion matrix unveils the model’s proficiency in accurately\nidentifying instances within the \"Hold\" class, as underscored\nby a notable count of true positives (799) and comparatively\nlow occurrences of false positives (64) and false negatives\n(295). However, its performance encounters challenges in\nthe \"Buy\" class, characterized by 33 true positives, 42 false\nnegatives, and 101 false positives. This suggests a difficulty\nin the accurate discrimination of instances between the \"Buy\"\nand \"Hold\" categories. Notably, the model demonstrates a\ncontrasting strength in the \"Sell\" class, exhibiting 53 true\npositives and 194 false negatives, indicative of adeptness in\nidentifying instances of the \"Sell\" category. This intriguing\nobservation points to a particular challenge in distinguishing\nbetween instances of the \"Sell\" and \"Hold\" categories.\nLastly, the evaluation of the confusion matrix for the DeiT\ntransformer as in Table 7 reveals a suboptimal performance,\ncharacterized by a meager count of correct predictions (151)\nagainst a significantly elevated number of false predictions\n(1093). This disparity results in a notably low F1 score,\nindicative of the model’s limited ability to accurately identify\ninstances within any given category. The discerned inability\nof the model to effectively distinguish between classes is a\nprominent factor contributing to the overarching poor results\nobserved. Consequently, the implemented trading strategy\nyields unfavorable outcomes, underscoring the imperative of\naddressing and refining the DeiT architecture.\nC. ALGORITHMIC TRADING PORTFOLIO ANALYSIS\nWe also evaluated the portfolio analysis of the financial\nstrategies by using the results of our tested algorithms pre-\ndictions. Table 8 presents a comprehensive overview of the\naggregated outcomes of vision transformer approaches such\nas ViT, DeiT, Swin, non-transformer patch embedding-based\napproach ConvMixer, and the best-performing baseline CNN-\nTA++. Similar to more traditional machine learning metrics,\nViT outperforms all remaining methods.\nA detailed comparison between ViT and the best-\nperforming enhanced baseline CNN-TA++, focusing on the\nAnnualized Number of Transactions and Average Transaction\nLength, reveals distinct trading strategy approaches. Specif-\nically, CNN-TA++ executed 80 transactions, nearly dou-\nbling ViT’s 43 transactions. However, a counterintuitive re-\nlationship emerges when examining transaction lengths, with\nCNN-TA++ exhibiting an average length of 6 transactions\ncompared to ViT’s 10 transactions. Both algorithms remained\nactive in their respective strategies, accumulating 480 and 430\ntransaction points for CNN-TA++ and ViT, respectively. This\ndetailed portfolio analysis provides valuable insights into the\ndivergent trading strategies employed by these algorithms.\nD. FINANCIAL EVALUATION BY WEALTH CURVE OVER\nTIME\nFigure 5 shows wealth curves of all considered approaches\nand the best-performing baseline CNN-TA++. In the context\nof strategy performance, our evaluation identifies the ViT ar-\nchitecture as the optimal choice, with a narrow gap compared\nto the Enhanced CNN-TA architecture. Both these algorithms\nconsistently yield returns exceeding 120%. Intriguingly, de-\nspite ConvMixer’s great performance in terms of traditional\nmachine learning metrics, it ranks second to the last when\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3358452\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nGezici and Sefer: Deep Transformer-based Asset Price and Direction Prediction\nHold Buy Sell\nHold 350 0 744\nBuy 37 0 38\nSell 30 0 45\nTABLE 4: ConvMixer Confusion Matrix\nHold Buy Sell\nHold 898 36 160\nBuy 66 2 7\nSell 68 4 3\nTABLE 5: Swin Confusion Matrix\nHold Buy Sell\nHold 799 101 194\nBuy 42 33 0\nSell 22 0 53\nTABLE 6: ViT Confusion Matrix\nHold Buy Sell\nHold 82 111 901\nBuy 6 9 60\nSell 5 10 60\nTABLE 7: DeiT Confusion Matrix\nConvMixer ViT CNN-TA++ DeiT Swin\nSharpe Ratio (Daily) 0.13 0.15 0.12 -0.02 -0.12\nEnding Capital 15778.9 22098.77 22070.29 8466.28 8331.54\nAnnualized return 9.55% 17.19% 17.16% -3.28% -3.58%\nAnnualized number of transaction 38 43 80 71 28\nPercent success of transaction 52.63% 72.09% 71.25% 56.34% 53.57%\nAverage percent profit per transaction 1.42% 2.14% 1.13% -0.17% -0.46%\nAverage transaction length 13 10 6 1 11\nMaximum profit percent in transaction 14.55% 12.84% 8.24% 4.14% 8.85%\nMaximum loss percent in transaction -16.11% -27.97% -18.63% -8.98% -19.36%\nMaximum capital value 15778.9 22098.77 22070.29 10810.12 12590.97\nMinimum capital value 9334.11 9365.84 9554.23 7267.91 7639.49\nIdle Ratio 60.21% 65.19% 55.63% 89.79% 73.55%\nTABLE 8: General comparison of ConvMixer, ViT, CNN-TA++, DeiT, and Swin architectures.\nevaluated from a more financial, portfolio evaluation per-\nspective. Consequently, the most favorable algorithm in terms\nof strategy profitability is the ViT architecture. Nonetheless,\nit is essential to acknowledge that various considerations,\nincluding average transaction length, transaction frequency,\nand fluctuation in capital values, can influence the optimal\nchoice of algorithm.\nAccording to Figure 5, DeiT algorithm demonstrates a no-\ntable frequency of transactions; however, it frequently falters\nin effectively identifying optimal entry and exit points. As a\nconsequence, following an initial phase of profitability, the\nalgorithm undergoes a gradual decrement, culminating in an\nultimate negative balance relative to the initial capital. This\nobserved outcome underscores the algorithm’s limited capac-\nity to apprehend the inherent strategic logic, as elucidated\nby the diminished accuracy metrics in previous tables. This\ndiscernment not only accentuates the algorithm’s suboptimal\nperformance but also suggests a deficiency in its ability to\nalign with the fundamental principles of the underlying trad-\ning strategy. Further exploration and analysis are required\nto elucidate the specific aspects contributing to the observed\ninadequacies and to inform potential enhancements for algo-\nrithmic optimization.\nSimilarly, Swin algorithm exhibits a low volume of trans-\nactions and fails to generate a profit with more than half of\nthe transaction actions, thus resulting in a negative balance.\nThis outcome contradicts the relatively high accuracy and\nF1 score metrics observed in Table 3. This discrepancy can\nbe attributed to the algorithm’s inability to identify the ideal\nentry and exit points, as reflected in the low precision and\nrecall metrics for \"Sell\" and \"Buy\" orders seen in its confusion\nmatrix. This result is further supported by the high number\nof false positives and false negatives in the identification\nof \"Buy\" and \"Sell\" orders, respectively. From a strategic\nstandpoint, these findings indicate the need to address and\nreduce the prevalence of incorrect \"Buy\" and \"Sell\" orders,\nas these errors are recurrent and contribute significantly to\ncapital loss.\nLastly, as seen in Figure 5, ConvMixer has a certain ten-\ndency towards \"Hold\" orders. This strategic inclination is\nindicative of a discerning recognition that financial losses\noften emanate from the execution of \"Buy\" and \"Sell\" or-\nders. Consequently, ConvMixer algorithm partakes in a lower\nfrequency of transactions relative to alternative algorithms.\nWhile this conservative approach mitigates exposure to po-\ntential losses, it simultaneously constrains the algorithm’s\nprofit-generation capacity. These empirical observations col-\nlectively show that, although the Convmixer strategy is char-\nacterized by a risk-averse approach, it may not optimize\nprofitability. Our comprehensive analysis of the ConvMixer\nalgorithm’s outcomes underscores the pivotal importance of\nstriking a balance between risk mitigation and profit maxi-\nmization in the quest for efficacious trading strategies.\nIn summary, each algorithm employs a unique approach to\nprocessing both financial data and executing trading strate-\ngies. Distinct algorithmic priorities are observed, with some\nalgorithms placing a paramount emphasis on precision, while\nothers prioritize profitability, sometimes at the cost of achiev-\ning the utmost precision in their predictions. Such obser-\nvation underscores the inherent trade-offs and strategic de-\ncisions that deep learning algorithms must navigate when\napplied within the domain of financial time series analysis.\nThe detailed interplay between precision and profitability\nunderscores the complexity of algorithmic decision-making\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3358452\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nGezici and Sefer: Deep Transformer-based Asset Price and Direction Prediction\n(a) DeiT\n (b) ConvMixer\n(c) ViT\n (d) Swin\nFIGURE 5: Wealth curves for tested deep learning approaches.\nin financial markets, necessitating a thoughtful consideration\nof the strategic objectives and inherent risks associated with\neach algorithmic approach. These observations contribute to\na deeper understanding of the intricate dynamics involved\nin optimizing learning algorithms for effective and strategic\nfinancial market operations.\nE. STATISTICAL SIGNIFICANCE OF RESULTS\nAccording to the statistical significance tests shown in Ta-\nble 9, the ViT-based asset price prediction method’s behavior\nis not impacted by distinct periods and changing market states\nsuch as 2002–2012, and 2012-2022 time intervals. For many\nevaluation criteria, ViT operates quite robustly and stably. Al-\nthough the performance difference between ViT-based price\nprediction methods under various distinct market conditions\nis not statistically significant, it outperforms the competing\nbaselines like MLP, LSTM, SMA, RSI, and Buy & Hold\nespecially during non-bull market conditions.\nIn addition to the summary of our strategy-based automated\ntrading in Table 9, we also provide statistical significance\nresults by comparing the performance of various transformers\nand baseline approaches as in Table 10. According to these\nresults, ViT performs significantly better than simpler base-\nlines such as BaH, LSTM, and MLP across different periods.\nHowever, the statistical significance of its outperformance\nconcerning other methods depends on the considered period.\nVI. CONCLUSIONS\nIn this study, we have undertaken a comprehensive in-\nvestigation into the performance of various deep vision\ntransformer-based algorithms and two-dimensional deep\npatch embedding-based convolutional neural networks in fi-\nnancial asset price and direction prediction. Our techniques\nintegrate several well-known technical analysis indicators\ninto the algorithmic trading prediction framework. We ana-\nlyze temporally changing ETF datasets by first forming two-\ndimensional images out of the original price data via technical\nindicators. We designed meaningful and profit-making trades\nby inferring entry and exit points via 3 categories such as Buy,\nSell, and Hold. According to our experiments, transformer-\nbased methods consistently outperform the Buy and Hold\nbaseline, LSTM, and the enhanced version of the only con-\nvolutional CNN-TA architecture in terms of both traditional\nmachine learning metrics and financial portfolio analysis\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3358452\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nGezici and Sefer: Deep Transformer-based Asset Price and Direction Prediction\nt-test 2002-2012 2012-2022\nSharpe Ratio (Daily) - 0.13 0.17\nAnnualized return 0.108 15.31% 18.65%\nAnnualized number of transaction 0.317 41 45\nPercent success of transaction 0.076 70.65% 73.98 %\nAverage percent profit per transaction 0.465 2.15% 2.25%\nAverage transaction length 0.547 11 10\nMaximum profit percent in transaction 0.653 12.92% 12.65%\nMaximum loss percent in transaction 0.667 -27.62% -28.35%\nIdle Ratio 0.074 64.02% 66.35%\nTABLE 9: t-test results and average results of ViT model across different periods.\nTime Period Comparison t-test\n2002–2012\nViT-Swin 0.001\nViT-ConvMixer 0.095\nViT-DeiT 0.032\nViT-BaH 0.001\nViT-LSTM 0.009\nViT-MLP 0.013\n2012–2022\nViT-Swin 0.002\nViT-ConvMixer 0.023\nViT-DeiT 0.064\nViT-BaH 0.001\nViT-LSTM 0.007\nViT-MLP 0.031\nTABLE 10: t-test for annualized ETF returns.\nmetrics. In addition to transformers, patch-embedding-based\nconvolutional architecture is also effective in asset price and\ndirection performance.\nThe principal objective of this research has been to gain\na deeper understanding of the inherent strengths and weak-\nnesses of each distinct transformer-based algorithm and how\nthese attributes may be harnessed to enhance the perfor-\nmance of algorithmic trading strategies. Although we ob-\ntained reasonably well performance by transformer-based\narchitectures, some more enhancements could be integrated\ninto our framework. In future work, current analysis on ETFs\ncan be enhanced to cryptocurrencies which are more volatile\nthan ETFs. Such extension to multiple cryptocurrencies will\nalso result in a massive amount of training data for our\ntransformer-based deep learning methods, where the per-\nformance of transformers is known to be impacted by the\ntraining size. Moreover, the ordering of technical indicators\nwhile forming the image-like data could be enhanced as well.\nAnother enhancement could be to focus more on different\ntrading strategies instead of just long-only strategies. Lastly,\nwe can integrate Explainable AI into our framework where\nExplainable AI methods for convolutional methods have been\npreviously studied. However, similar explainable AI meth-\nods for vision transformer-based architectures have not been\nstudied as much. Overall, the compelling prospects for future\nresearch underscore the continued importance of exploring\nand advancing the application of AI in the domain of financial\ntime series analysis.\nACKNOWLEDGMENT\nThe authors would like to thank the editors and anonymous\nreviewers for providing insightful suggestions and comments\nto improve the quality of the research paper.\nREFERENCES\n[1] M. Ge, S. Zhou, S. Luo, and B. Tian, ‘‘3d tensor-based deep learning\nmodels for predicting option price,’’ 2021.\n[2] W. Zheng, ‘‘Exchange-traded fund price prediction based on the deep\nlearning model,’’ in 2021 China Automation Congress (CAC) , 2021, pp.\n7429–7434.\n[3] S. Freeda, T. C. E. Selvan, and I. Hemanandhini, ‘‘Prediction of bitcoin\nprice using deep learning model,’’ in 2021 5th International Conference on\nElectronics, Communication and Aerospace Technology (ICECA) , 2021,\npp. 1702–1706.\n[4] R. C. Cavalcante, R. C. Brasileiro, V. L. Souza, J. P. Nobrega, and A. L.\nOliveira, ‘‘Computational intelligence and financial markets: A survey and\nfuture directions,’’Expert Systems with Applications , vol. 55, pp. 194–211,\n2016.\n[5] A. M. Ozbayoglu, M. U. Gudelek, and O. B. Sezer, ‘‘Deep learning for\nfinancial applications : A survey,’’ Applied Soft Computing , vol. 93, p.\n106384, 2020.\n[6] K. Simonyan and A. Zisserman, ‘‘Very deep convolutional networks for\nlarge-scale image recognition,’’ arXiv preprint arXiv:1409.1556 , 2014.\n[7] M. D. Zeiler and R. Fergus, ‘‘Visualizing and understanding convolu-\ntional networks,’’ in Computer Vision–ECCV 2014: 13th European Con-\nference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I\n13. Springer, 2014, pp. 818–833.\n[8] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\nrecognition,’’ CoRR, vol. abs/1512.03385, 2015. [Online]. Available:\nhttp://arxiv.org/abs/1512.03385\n[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘Bert: Pre-training of\ndeep bidirectional transformers for language understanding,’’ 2019.\n[10] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V. Stoyanov, ‘‘Roberta: A robustly optimized bert\npretraining approach,’’ 2019.\n[11] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le,\n‘‘Xlnet: Generalized autoregressive pretraining for language understand-\ning,’’ 2020.\n[12] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. u. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Advances\nin Neural Information Processing Systems , I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds.,\nvol. 30. Curran Associates, Inc., 2017.\n[13] R. E. Turner, ‘‘An introduction to transformers,’’ 2023.\n[14] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\nterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,\nand N. Houlsby, ‘‘An image is worth 16x16 words: Transformers for image\nrecognition at scale,’’ CoRR, vol. abs/2010.11929, 2020.\n[15] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,\n‘‘Training data-efficient image transformers & distillation through atten-\ntion,’’ 2021.\n[16] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, ‘‘Swin\ntransformer: Hierarchical vision transformer using shifted windows,’’ in\nProceedings of the IEEE/CVF international conference on computer vi-\nsion, 2021, pp. 10 012–10 022.\n[17] A. Trockman and J. Z. Kolter, ‘‘Patches are all you need?’’ CoRR, vol.\nabs/2201.09792, 2022.\n[18] Y. SANTUR, ‘‘Deep learning based regression approach for algorithmic\nstock trading: A case study of the bist30,’’ Gümüşhane Üniversitesi Fen\nBilimleri Dergisi, vol. 10, no. 4, p. 1195–1211, 2020.\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3358452\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nGezici and Sefer: Deep Transformer-based Asset Price and Direction Prediction\n[19] Y. K. Pardeshi and P. P. Kale, ‘‘Technical analysis indicators in stock\nmarket using machine learning: A comparative analysis,’’ in 2021 12th\nInternational Conference on Computing Communication and Networking\nTechnologies (ICCCNT), 2021, pp. 1–6.\n[20] E. Hossain, M. S. Hossain, P.-O. Zander, and K. Andersson, ‘‘Machine\nlearning with belief rule-based expert systems to predict stock price\nmovements,’’ Expert Systems with Applications , vol. 206, p. 117706,\n2022. [Online]. Available: https://www.sciencedirect.com/science/article/\npii/S0957417422009940\n[21] O. B. Sezer and A. M. Ozbayoglu, ‘‘Algorithmic financial trading with deep\nconvolutional neural networks: Time series to image conversion approach,’’\nApplied Soft Computing , vol. 70, pp. 525–538, 2018.\n[22] E. C. Hui and K. K. K. Chan, ‘‘Alternative trading strategies to beat\n“buy-and-hold”,’’ Physica A: Statistical Mechanics and its Applications ,\nvol. 534, p. 120800, 2019. [Online]. Available: https://www.sciencedirect.\ncom/science/article/pii/S0378437119304108\n[23] M. R. Vargas, C. E. M. dos Anjos, G. L. G. Bichara, and A. G. Evsukoff,\n‘‘Deep leaming for stock market prediction using technical indicators and\nfinancial news articles,’’ in 2018 International Joint Conference on Neural\nNetworks (IJCNN), 2018, pp. 1–8.\n[24] I. J. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. Cambridge,\nMA, USA: MIT Press, 2016, http://www.deeplearningbook.org.\n[25] T. Fischer and C. Krauss, ‘‘Deep learning with long short-term memory\nnetworks for financial market predictions,’’ European Journal of Opera-\ntional Research, vol. 270, no. 2, pp. 654–669, 2018.\n[26] J.-Z. Wang, J.-J. Wang, Z.-G. Zhang, and S.-P. Guo, ‘‘Forecasting stock\nindices with back propagation neural network,’’ Expert Systems with Ap-\nplications, vol. 38, no. 11, pp. 14 346–14 355, 2011.\n[27] Z. Liao and J. Wang, ‘‘Forecasting model of global stock index by stochas-\ntic time effective neural network,’’ Expert Systems with Applications ,\nvol. 37, no. 1, pp. 834–841, 2010.\n[28] A.-S. Chen, M. T. Leung, and H. Daouk, ‘‘Application of neural networks\nto an emerging financial market: forecasting and trading the taiwan stock\nindex,’’ Computers & Operations Research , vol. 30, no. 6, pp. 901–923,\n2003, operation Research in Emerging Economics.\n[29] E. Guresen, G. Kayakutlu, and T. U. Daim, ‘‘Using artificial neural network\nmodels in stock market index prediction,’’ Expert Systems with Applica-\ntions, vol. 38, no. 8, pp. 10 389–10 397, 2011.\n[30] O. B. Sezer, A. M. Ozbayoglu, and E. Dogdu, ‘‘An artificial neural\nnetwork-based stock trading system using technical analysis and big data\nframework,’’ in Proceedings of the SouthEast Conference , ser. ACM SE\n’17. New York, NY, USA: Association for Computing Machinery, 2017,\np. 223–226.\n[31] D. Zhang and L. Zhou, ‘‘Discovering golden nuggets: data mining in finan-\ncial application,’’ IEEE Transactions on Systems, Man, and Cybernetics,\nPart C (Applications and Reviews) , vol. 34, no. 4, pp. 513–522, 2004.\n[32] S. K. Chalup and A. Mitschele, Kernel Methods in Finance . Berlin,\nHeidelberg: Springer Berlin Heidelberg, 2008, pp. 655–687. [Online].\nAvailable: https://doi.org/10.1007/978-3-540-49487-4_27\n[33] Y. LeCun, Y. Bengio, and G. Hinton, ‘‘Deep learning,’’ Nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\n[34] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‘‘Imagenet classification\nwith deep convolutional neural networks,’’ in Advances in Neural Infor-\nmation Processing Systems , F. Pereira, C. Burges, L. Bottou, and K. Wein-\nberger, Eds., vol. 25. Curran Associates, Inc., 2012.\n[35] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and\nL. Fei-Fei, ‘‘Large-scale video classification with convolutional neural\nnetworks,’’ in 2014 IEEE Conference on Computer Vision and Pattern\nRecognition, 2014, pp. 1725–1732.\n[36] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, ‘‘A convolutional neural\nnetwork for modelling sentences,’’ in Proceedings of the 52nd Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long\nPapers). Baltimore, Maryland: Association for Computational Linguis-\ntics, Jun. 2014, pp. 655–665.\n[37] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,\nT. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen,\nC. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame,\nQ. Lhoest, and A. Rush, ‘‘Transformers: State-of-the-art natural language\nprocessing,’’ in Proceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing: System Demonstrations . Online:\nAssociation for Computational Linguistics, Oct. 2020, pp. 38–45.\n[38] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,\n‘‘Transformers in vision: A survey,’’ ACM Comput. Surv. , dec 2021.\n[39] X. Ding, Y. Zhang, T. Liu, and J. Duan, ‘‘Deep learning for event-driven\nstock prediction,’’ in Proceedings of the 24th International Conference on\nArtificial Intelligence, ser. IJCAI’15. AAAI Press, 2015, p. 2327–2333.\n[40] M. Längkvist, L. Karlsson, and A. Loutfi, ‘‘A review of unsupervised\nfeature learning and deep learning for time-series modeling,’’ Pattern\nRecognition Letters, vol. 42, pp. 11–24, 2014.\n[41] C. Krauss, X. A. Do, and N. Huck, ‘‘Deep neural networks, gradient-\nboosted trees, random forests: Statistical arbitrage on the s&p 500,’’ Euro-\npean Journal of Operational Research , vol. 259, no. 2, pp. 689–702, 2017.\n[42] A. Yoshihara, K. Fujikawa, K. Seki, and K. Uehara, ‘‘Predicting stock\nmarket trends by recurrent deep neural networks,’’ in PRICAI 2014: Trends\nin Artificial Intelligence, D.-N. Pham and S.-B. Park, Eds. Cham: Springer\nInternational Publishing, 2014, pp. 759–769.\n[43] O. B. Sezer, M. Ozbayoglu, and E. Dogdu, ‘‘A deep neural-network based\nstock trading system based on evolutionary optimized technical analysis\nparameters,’’ Procedia Computer Science , vol. 114, pp. 473–480, 2017,\ncomplex Adaptive Systems Conference with Theme: Engineering Cyber\nPhysical Systems, CAS October 30 – November 1, 2017, Chicago, Illinois,\nUSA.\n[44] W. Chen, C. K. Yeo, C. T. Lau, and B. S. Lee, ‘‘Leveraging social\nmedia news to predict stock index movement using rnn-boost,’’ Data &\nKnowledge Engineering , vol. 118, pp. 14–24, 2018. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S0169023X17305839\n[45] C.-Y. Lee and V.-W. Soo, ‘‘Predict stock price with financial news based\non recurrent convolutional neural networks,’’ in 2017 Conference on Tech-\nnologies and Applications of Artificial Intelligence (TAAI) , 2017, pp. 160–\n165.\n[46] F. Feng, X. He, X. Wang, C. Luo, Y. Liu, and T.-S. Chua, ‘‘Temporal\nrelational ranking for stock prediction,’’ ACM Trans. Inf. Syst. , vol. 37,\nno. 2, mar 2019. [Online]. Available: https://doi.org/10.1145/3309547\n[47] Y. Xu and S. B. Cohen, ‘‘Stock movement prediction from tweets and\nhistorical prices,’’ in Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\nI. Gurevych and Y. Miyao, Eds. Melbourne, Australia: Association for\nComputational Linguistics, Jul. 2018, pp. 1970–1979. [Online]. Available:\nhttps://aclanthology.org/P18-1183\n[48] Q. Zhang, C. Qin, Y. Zhang, F. Bao, C. Zhang, and P. Liu, ‘‘Transformer-\nbased attention network for stock movement prediction,’’ Expert Systems\nwith Applications, vol. 202, p. 117239, 2022.\n[49] Q. Ding, S. Wu, H. Sun, J. Guo, and J. Guo, ‘‘Hierarchical multi-scale\ngaussian transformer for stock movement prediction,’’ in Proceedings of\nthe Twenty-Ninth International Joint Conference on Artificial Intelligence,\nIJCAI-20, C. Bessiere, Ed. International Joint Conferences on Artificial\nIntelligence Organization, 7 2020, pp. 4640–4646, special Track on AI in\nFinTech.\n[50] J. Liu, H. Lin, X. Liu, B. Xu, Y. Ren, Y. Diao, and L. Yang, ‘‘Transformer-\nbased capsule network for stock movement prediction,’’ in Proceedings\nof the First Workshop on Financial Technology and Natural Language\nProcessing, Macao, China, Aug. 2019, pp. 66–73.\n[51] N. Cohen, T. Balch, and M. Veloso, ‘‘Trading via image classification,’’\nCoRR, vol. abs/1907.10046, 2019. [Online]. Available: http://arxiv.org/\nabs/1907.10046\n[52] T. Tuncer, U. Kaya, E. Sefer, O. Alacam, and T. Hoser, ‘‘Asset price\nand direction prediction via deep 2d transformer and convolutional\nneural networks,’’ in Proceedings of the Third ACM International\nConference on AI in Finance , ser. ICAIF ’22. New York, NY,\nUSA: Association for Computing Machinery, 2022, p. 79–86. [Online].\nAvailable: https://doi.org/10.1145/3533271.3561738\n[53] A. S. Wafi, H. Hassan, and A. Mabrouk, ‘‘Fundamental analysis models\nin financial markets – review study,’’ Procedia Economics and Finance ,\nvol. 30, pp. 939–947, 2015, iISES 3rd and 4th Economics and Finance\nConference. [Online]. Available: https://www.sciencedirect.com/science/\narticle/pii/S2212567115013441\n[54] B. Enkhsaikhan, ‘‘Yahoo finance asset price dataset,’’ 2023. [Online].\nAvailable: https://dx.doi.org/10.21227/87yd-jk77\n[55] A. W. Lo, H. Mamaysky, and J. Wang, ‘‘Foundations of technical analysis:\nComputational algorithms, statistical inference, and empirical implemen-\ntation,’’The Journal of Finance , vol. 55, no. 4, pp. 1705–1765, 2000.\n[56] D. Singh and B. Singh, ‘‘Investigating the impact of data normalization on\nclassification performance,’’ Applied Soft Computing , vol. 97, p. 105524,\n2020. [Online]. Available: https://www.sciencedirect.com/science/article/\npii/S1568494619302947\n[57] N. Parmar, A. Vaswani, J. Uszkoreit, Łukasz Kaiser, N. Shazeer, A. Ku,\nand D. Tran, ‘‘Image transformer,’’ 2018.\nVOLUME 11, 2023 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3358452\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nGezici and Sefer: Deep Transformer-based Asset Price and Direction Prediction\n[58] H. Hu, Z. Zhang, Z. Xie, and S. Lin, ‘‘Local relation networks for image\nrecognition,’’ 2019.\n[59] G. Hinton, O. Vinyals, and J. Dean, ‘‘Distilling the knowledge in a neural\nnetwork,’’ 2015.\n[60] E. Guresen, G. Kayakutlu, and T. U. Daim, ‘‘Using artificial neural\nnetwork models in stock market index prediction,’’ Expert Systems with\nApplications, vol. 38, no. 8, pp. 10 389–10 397, 2011. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S0957417411002740\n[61] W. F. Sharpe, ‘‘Mutual fund performance,’’ The Journal of Business ,\nvol. 39, no. 1, pp. 119–138, 1966.\n[62] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,\nT. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen,\nC. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest,\nand A. M. Rush, ‘‘Huggingface’s transformers: State-of-the-art natural\nlanguage processing,’’ 2020.\n[63] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison,\nA. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,\nB. Steiner, L. Fang, J. Bai, and S. Chintala, ‘‘Pytorch: An imperative\nstyle, high-performance deep learning library,’’ in Advances in\nNeural Information Processing Systems 32 . Curran Associates, Inc.,\n2019, pp. 8024–8035. [Online]. Available: http://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf\nABDUL HALUK BATUR GEZICIgraduated from\nthe Computer Engineering Department of Istanbul\nAydın University in 2021 as the second runner-\nup of the engineering faculty. Immediately after\ngraduation, due to his interest in artificial intelli-\ngence and finance, he joined Özyeğin University\nArtificial Intelligence Master’s program.\nEMRE SEFER obtained his B.Eng from Bogazici\nUniversity, Department of Computer Engineering\nin 2008, M.S. in Computer Science from Univer-\nsity of Maryland College Park in 2011, and Ph.D.\nin Computational Biology from Carnegie Mellon\nUniversity in 2015. After completing his Ph.D.\nDr. Sefer had a short post-doc at CMU Machine\nLearning Department with Ziv-Bar Joseph. He is\ncurrently an assistant professor in the Computer\nScience Department, Ozyegin University. His aca-\ndemic research has focused on machine learning on graph and time-series\ndatasets, where he focuses especially on Finance and bioinformatics appli-\ncation domains. He has published in several journals and conferences during\nhis PhD.\n14 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3358452\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}