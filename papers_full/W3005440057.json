{
  "title": "Consistency of a Recurrent Language Model With Respect to Incomplete Decoding",
  "url": "https://openalex.org/W3005440057",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222670212",
      "name": "Welleck, Sean",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226362637",
      "name": "Kulikov, Ilia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746454236",
      "name": "Kim Jaedeok",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227072491",
      "name": "Pang, Richard Yuanzhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753220617",
      "name": "Cho, Kyunghyun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963748792",
    "https://openalex.org/W2963382396",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2964103964",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W183625566",
    "https://openalex.org/W2798664956",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W2770579863",
    "https://openalex.org/W2970692082",
    "https://openalex.org/W2487501366",
    "https://openalex.org/W2963572611",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2963248296",
    "https://openalex.org/W2111041233",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2571859396",
    "https://openalex.org/W2147880316"
  ],
  "abstract": "Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms. To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model. We prove that commonly used incomplete decoding algorithms - greedy search, beam search, top-k sampling, and nucleus sampling - are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length. Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model. Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency.",
  "full_text": "Consistency of a Recurrent Language Model With Respect to\nIncomplete Decoding\nSean Welleck1∗ Ilia Kulikov1∗ Jaedeok Kim2†\nRichard Yuanzhe Pang1 Kyunghyun Cho1,3\n1 New York University 2 Samsung Research 3 CIFAR Associate Fellow\nAbstract\nDespite strong performance on a variety of\ntasks, neural sequence models trained with\nmaximum likelihood have been shown to ex-\nhibit issues such as length bias and degener-\nate repetition. We study the related issue of\nreceiving inﬁnite-length sequences from a re-\ncurrent language model when using common\ndecoding algorithms. To analyze this issue, we\nﬁrst deﬁne inconsistency of a decoding algo-\nrithm, meaning that the algorithm can yield an\ninﬁnite-length sequence that has zero probabil-\nity under the model. We prove that commonly\nused incomplete decoding algorithms – greedy\nsearch, beam search, top- k sampling, and nu-\ncleus sampling – are inconsistent, despite the\nfact that recurrent language models are trained\nto produce sequences of ﬁnite length. Based\non these insights, we propose two remedies\nwhich address inconsistency: consistent vari-\nants of top-kand nucleus sampling, and a self-\nterminating recurrent language model. Empir-\nical results show that inconsistency occurs in\npractice, and that the proposed methods pre-\nvent inconsistency.\n1 Introduction\nNeural sequence models trained with maximum\nlikelihood estimation (MLE) have become a stan-\ndard approach to modeling sequences in a variety\nof natural language applications such as machine\ntranslation (Bahdanau et al., 2015), dialogue mod-\neling (Vinyals et al., 2015), and language modeling\n(Radford et al., 2019). Despite this success, MLE-\ntrained neural sequence models have been shown\nto exhibit issues such as length bias (Sountsov\nand Sarawagi, 2016; Stahlberg and Byrne, 2019)\nand degenerate repetition (Holtzman et al., 2019).\n∗Equal contribution. Correspondence to: Sean Welleck\nwellecks@nyu.edu.\n†Work done at New York University.\nThese issues are suspected to be related to the max-\nimum likelihood objective’s local normalization,\nwhich results in a discrepancy between the learned\nmodel’s distribution and the distribution induced by\nthe decoding algorithm used to generate sequences\n(Lafferty et al., 2001; Andor et al., 2016). This has\nprompted the development of alternative decoding\nmethods (Wu et al., 2016; Holtzman et al., 2019)\nand training objectives (Murray and Chiang, 2018;\nWelleck et al., 2019). In this paper, we formalize\nand study this discrepancy between the model and\nthe decoding algorithm.\nWe begin by formally deﬁning recurrent neu-\nral language models, a family that encompasses\nneural models used in practice, such as recurrent\nneural networks (Elman, 1990; Cho et al., 2014;\nHochreiter and Schmidhuber, 1997), and transform-\ners (Vaswani et al., 2017). Next, we formally deﬁne\na decoding algorithm – a function that induces a\ndistribution over sequences given a recurrent lan-\nguage model and a context distribution – which is\nused to obtain probable sequences from a model. In\nthis paper, we show that the distribution induced by\na decoding algorithm can contradict this intended\nuse; instead, the decoding algorithm may return\nimprobable, inﬁnite-length sequences.\nOur main ﬁnding is that a sequence which re-\nceives zero probability under a recurrent language\nmodel’s distribution can receive nonzero probabil-\nity under the distribution induced by a decoding\nalgorithm. This occurs when the recurrent language\nmodel always ranks the sequence termination token\noutside of the set of tokens considered at each de-\ncoding step, yielding an inﬁnite-length, zero proba-\nbility sequence. This holds whenever the decoding\nalgorithm is incomplete, in the sense that the algo-\nrithm excludes tokens from consideration at each\nstep of decoding, which is the case for common\nmethods such as greedy search, beam search, top-k\nsampling (Fan et al., 2018), and nucleus sampling\narXiv:2002.02492v2  [cs.LG]  2 Oct 2020\n(Holtzman et al., 2019). We formalize our main\nﬁnding using the notion of consistency (Chen et al.,\n2017) – whether a distribution assigns probability\nmass only to ﬁnite sequences – and prove that a\nconsistent recurrent language model paired with\nan incomplete decoding algorithm can induce an\ninconsistent sequence distribution.\nBased on the insight that inconsistency occurs\ndue to the behavior of the termination token un-\nder incomplete decoding, we develop two meth-\nods for addressing inconsistency. First, we pro-\npose consistent sampling methods which guarantee\nthat the termination token is not excluded from se-\nlection during decoding. Second, we introduce a\nself-terminating recurrent language model which\nensures that the termination token is eventually\nranked above all others, guaranteeing consistency\nunder incomplete decoding.\nTo empirically measure inconsistency, we de-\ncode sequences from trained recurrent language\nmodels and measure the proportion of sequences\nwith lengths far exceeding the maximum training\nsequence length. Our experiments on the Wikitext2\ndataset (Merity et al., 2016) suggest that inconsis-\ntency occurs in practice when using incomplete\ndecoding methods, while the proposed consistent\nsampling methods and self-terminating model pa-\nrameterization prevent inconsistency and maintain\nlanguage modeling quality.\nThe theoretical analysis reveals defects of ex-\nisting decoding algorithms, providing a way to\ndevelop future models, inference procedures, and\nlearning algorithms. We present methods related to\nsampling and model parameterization, but there are\nmore directions for future investigation; we close\nwith directions related to sequence-level learning.\n2 Background\nWe begin our discussion by establishing back-\nground deﬁnitions. First, we deﬁne a sequence\nwhich is the main object of our investigation.\nDeﬁnition 2.1 (Sequence). A sequence Y is an\nordered collection of items from a predeﬁned ﬁnite\nvocabulary V. A sequence of ﬁnite length always\nends with a special token ⟨eos⟩ ∈V that only\nappears at the end of a sequence.\nEach model we consider generates a sequence\nconditioned on context information, such as a preﬁx\nin sentence completion. To consider this, we deﬁne\na context distribution.\nDeﬁnition 2.2 (Context distribution). A context dis-\ntribution p(C) is a probability distribution deﬁned\nover a set C. An element C ∈C is called a context.\n2.1 Recurrent Language Models\nA recurrent language model is an autoregressive\nmodel of a sequence distribution, where each con-\nditional probability is parameterized with a neural\nnetwork. Importantly, we assume that all tokens\nin a sequence are dependent on each other under a\nrecurrent language model. This allows us to avoid\ncases in which the model degenerates to a Marko-\nvian language model, such as an n-gram model\nwith a ﬁnite n.\nDeﬁnition 2.3 (Recurrent language model). A re-\ncurrent language model pθ is a neural network that\ncomputes the following at each time step:\npθ(yt = v|y<t,C) = exp(u⊤\nv ht + cv)∑\nv′∈V exp(u⊤\nv′ht + cv′),\nwhere ht = fθ(yt,ht−1) and h0 = gθ(C), and\nu,c,θ are parameters. A recurrent language model\nthereby computes the probability of a sequence\nY = (y1,...,y T) by\npθ(Y |C) =\nT∏\nt=1\npθ(yt|y<t,C),\nwhere y<t = (y1,...,y t−1). This distribution sat-\nisﬁes yi ̸⊥ ⊥yj |C, ∀i<j .\nPractical variants of the recurrent language\nmodel differ by the choice of transition function fθ\n(Elman, 1990; Hochreiter and Schmidhuber, 1997;\nCho et al., 2014; Vaswani et al., 2017). The use of\nsoftmax (Bridle, 1990) implies that every unique\ntoken in the vocabulary is considered at every loca-\ntion of a sequence.\nRemark 2.1. Under the conditional distribution\nof a recurrent LM, every token v ∈ V is as-\nsigned a positive probability, implying that 0 <\npθ(v|y<t,C) < 1.Any ﬁnite sequence is proba-\nble under a recurrent LM under any context, i.e.,\npθ(Y |C) >0 for any sequence Y of ﬁnite length.\n2.2 Decoding Algorithms\nBecause it is intractable to decode the most proba-\nble sequence, it is necessary in practice to use an\napproximate decoding algorithm.\nDeﬁnition 2.4 (Decoding algorithm). A decoding\nalgorithm F(pθ,C) is a function that generates\na sequence ˜Y given a recurrent language model\npθ and context C. Let qF denote the distribution\ninduced by the decoding algorithm F.\nWe consider two families of decoding algo-\nrithms. In our analysis we only consider algorithms\nthat decode in a single pass, forward in time, with-\nout modifying previously selected tokens.\nStochastic decoding. The ﬁrst family consists of\nstochastic algorithms. Among them, ancestral sam-\npling is asymptotically unbiased and can be used\nfor ﬁnding the most probable sequence, although\nwith high variance.\nDeﬁnition 2.5 (Ancestral sampling) . Ancestral\nsampling Fanc generates a sequence from a re-\ncurrent language model pθ given context Cby re-\ncursively sampling from pθ(yt|˜y<t,C) until ˜yt =\n⟨eos⟩: ˜yt ∼pθ(yt|˜y<t,C).\nTo avoid the high variance, two approximate\nstochastic decoding algorithms have recently been\nproposed and tested with recurrent language mod-\nels. Top-ksampling considers only a subset of the\nkmost probable tokens from the vocabulary at a\ntime, while nucleus sampling considers only the\nminimal subset of most probable tokens whose total\nprobability is higher than a predeﬁned threshold.\nDeﬁnition 2.6 (Top-ksampling (Fan et al., 2018)).\nTop-ksampling Ftop-k generates a sequence from\na recurrent language model pθ given context Cby\nrecursively sampling from:\nq(v) ∝\n{\npθ(v|y<t,C), if v∈Vk,\n0, otherwise.\nwhere Vk = arg top-k\nv′\npθ(v′|y<t,C).\nDeﬁnition 2.7 (Nucleus sampling (Holtzman et al.,\n2019)). Nucleus sampling Fnuc-µ generates a se-\nquence from a recurrent language modelpθ given\ncontext C by recursively sampling from the fol-\nlowing proposal distribution. Let v1,...,v |V|\ndenote tokens in V such that pθ(vi|y<t,C) ≥\npθ(vj |y<t,C) for all i<j , and deﬁne\nq(v) ∝\n{\npθ(v|y<t,C), if v∈Vµ,\n0, otherwise,\nwhere Vµ =\n{\nv1,··· ,vkµ\n}\nwith\nkµ = min\n{\nk\n⏐⏐⏐⏐⏐\nk∑\ni=1\npθ(vi|y<t,C) >µ\n}\n.\nDeterministic decoding. The other family con-\nsists of deterministic decoding algorithms, where\na token is selected deterministically according to\na rule at each decoding step. The most naive al-\ngorithm, called greedy decoding, simply takes the\nmost probable token at each step.\nDeﬁnition 2.8 (Greedy decoding). Greedy decod-\ning Fgreedy generates a sequence from a recurrent\nlanguage model pθ given context Cby recursively\nselecting the most likely token from pθ(yt|˜y<t,C)\nuntil ˜yt = ⟨eos⟩:\n˜yt = arg max\nv∈V\nlog pθ(yt = v|˜y<t,C).\nIn contrast to greedy decoding,beam searchwith\nwidth k, Fbeam-k, operates on the level of partial se-\nquences or preﬁxes. Starting from a set of empty\npreﬁxes, at each iteration a new preﬁx set is formed\nby expanding each preﬁx with each possible token,\nthen choosing the khighest scoring expanded pre-\nﬁxes; refer to Appendix A for a formal deﬁnition.\nIncompleteness. Other than ancestral sampling,\nthe decoding algorithms above are incomplete in\nthat they only consider a strict subset of the full\nvocabulary V at each time step, aside from the\ntrivial case of k= |V|.1\nDeﬁnition 2.9 (Incomplete Decoding). A decoding\nalgorithm Fis incomplete when for each context\nC and preﬁx y<t, there is a strict subset V′\nt ⊊ V\nsuch that\n∑\nv∈V′\nt\nqF(yt = v|y<t,C) = 1.\n3 Consistency of a Decoding Algorithm\nDeﬁnition of consistency. A recurrent language\nmodel pθ may assign a positive probability to an\ninﬁnitely long sequence, in which case we call the\nmodel inconsistent. This notion of consistency was\nraised and analyzed earlier, for instance by Booth\nand Thompson (1973) and Chen et al. (2017), in\nterms of whether the distribution induced by pθ is\nconcentrated on ﬁnite sequences. We extend their\ndeﬁnition to account for the context C.\nDeﬁnition 3.1 (Consistency of a recurrent lan-\nguage model) . A recurrent language model is\nconsistent under a context distribution p(C) if\npθ(|Y|= ∞) = 0 . Otherwise, the recurrent lan-\nguage model is said to be inconsistent.\n1Nucleus sampling is incomplete when for every context\nCand preﬁx y<t, minv∈V pθ(v|y<t,C) <1 −µ.\nAny sequence decoded from a consistent model\nfor a given context is guaranteed to terminate.\nLemma 3.1. If a recurrent LM pθ is consistent,\npθ(|Y|= ∞|C) = 0 for any probable context C.2\nNext, we establish a practical condition under\nwhich a recurrent language model is consistent.\nLemma 3.2. A recurrent LM pθ is consistent if\n∥ht∥p is uniformly bounded for some p≥1.\nProof sketch. If ∥ht∥p is bounded, then each u⊤\nv ht\nis bounded, hence pθ(⟨eos⟩|y<t,C) > ξ >0 for\na constant ξ. Thus pθ(|Y|= ∞) ≤limt→∞(1 −\nξ)t = 0, meaning that pθ is consistent.\nAlthough this condition is practical because\nlayer normalization or bounded activation func-\ntions (Elman, 1990; Cho et al., 2014; Vaswani et al.,\n2017) result in bounded ht, we show that even if a\nrecurrent language model is consistent, a decoding\nalgorithm may produce an inﬁnite-length sequence.\nWe formalize this discrepancy using the consis-\ntency of a decoding algorithm.\nDeﬁnition 3.2 (Consistency of a decoding algo-\nrithm). A decoding algorithm Fis consistent with\nrespect to a consistent recurrent language modelpθ\nunder a context distribution p(C) if the decoding\nalgorithm Fpreserves the consistency of the model\npθ, that is, qF(|Y|= ∞) = 0.\nWhen a consistent recurrent language model pθ\nand a decoding algorithm Finduce a consistent\ndistribution qF, we say that pθ paired with Fis\nconsistent. For instance, any consistent recurrent\nlanguage model paired with ancestral sampling is\nconsistent, because the induced distribution qFanc is\nthe same as the distribution of the original model.\nWe also have an analogue of Lemma 3.1.\nLemma 3.3. A consistent decoding algorithm with\nrespect to a consistent recurrent LM decodes only\nprobable sequences. That is, if qF(Y |C) > 0,\nthen pθ(Y |C) >0 for any probable context C.\nInconsistency of incomplete decoding. Any in-\ncomplete decoding algorithm (Deﬁnition 2.9) can\nbe inconsistent regardless of the context distribu-\ntion, because there is a recurrent LM that places\n⟨eos⟩outside of V′\nt at every step of decoding. To\nshow this, we construct a consistent recurrent lan-\nguage model whose distribution induced by an in-\ncomplete decoding algorithm is inconsistent.\n2Proofs of Lemmas 3.1-3.3 are in Appendix B.\nFigure 1: A depiction of the model’s sequence distribu-\ntion (light grey, solid border) and the decoder’s induced\nsequence distribution (dark grey, dotted border). The\nwhite and black rectangles depict the set of all ﬁnite\nand inﬁnite sequences, respectively. We prove that un-\nder practical conditions, any incomplete decoding algo-\nrithm may be inconsistent with respect to a consistent\nmodel, as depicted.\nTheorem 3.4 (Inconsistency of an incomplete de-\ncoding algorithm). There exists a consistent recur-\nrent LM pθ from which an incomplete decoding\nalgorithm F, that considers only up to (|V|−1)-\nmost likely tokens according to pθ(yt|y<t,C) at\neach step t, ﬁnds an inﬁnite-length sequence ˜Y\nwith probability 1, i.e., qF(|Y|= ∞) = 1.\nProof. We prove this theorem by constructing a\ntanh recurrent network. We deﬁne the recurrent\nfunction fθ as\nht = fθ(yt,ht−1)\n= tanh\n([ Wh 0\n0 I\n]\nht−1 +\n[ 0\ne(yt)\n])\n,\nwhere e(yt) ∈R|V|is a one-hot representation of\nyt, Wh ∈Rd×d where every entry is positive, and\nI is an identity matrix of size |V|×| V|. h0 =\ngθ(C) is constructed to consist of positive values\nonly. Because each element of |ht|is bounded by\n1, the constructed recurrent language model pθ is\nconsistent by Lemma 3.2.\nWe set uv (see Deﬁnition 2.3) to\nuv =\n[ ¯uv\ne(v)\n]\n, u ⟨eos⟩=\n[ ¯u⟨eos⟩\ne(⟨eos⟩)\n]\n,\nwhere v ̸= ⟨eos⟩, all elements of ¯uv are positive,\nall elements of ¯u⟨eos⟩are negative, and e(v) is a\none-hot representation of v. cv is set to zero.\nThis deﬁnes a valid recurrent language model\n(Deﬁnition 2.3), since the conditional distribution\nat each time t is inﬂuenced by all the previous\ntokens. More speciﬁcally, the logit of a token v\ndepends on ∑t\nt′=1 1(yt′ = v), where 1 is an indi-\ncator function.\nThis recurrent language model always outputs\npositive logits for non- ⟨eos⟩tokens, and outputs\nnegative logits for the ⟨eos⟩ token. This im-\nplies p(⟨eos⟩|y<t,C) < p(v|y<t,C) for all\nv ∈ V\\{⟨eos⟩}. This means that ⟨eos⟩is al-\nways ranked last at each time step, so an incom-\nplete decoding algorithm that considers at most\n(|V|−1) most probable tokens at each time step\nfrom pθ(yt|y<t,C) cannot decode ⟨eos⟩and thus\nalways decodes an inﬁnitely long sequence ˆY, i.e.,\nqF(|Y|= ∞|C) = 1 for any context C. It yields\nqF(|Y|= ∞) = 1, while pθ(|Y|= ∞) = 0 due\nto consistency of the model pθ.\nGreedy decoding, beam search, top-ksampling,\nand nucleus sampling are all inconsistent according\nto this theorem.\n4 Fixing the inconsistency\nIn this section, we consider two ways to prevent\ninconsistency arising from incomplete decoding\nalgorithms. First, we introduce consistent versions\nof top-k and nucleus sampling. Second, we in-\ntroduce the self-terminating recurrent language\nmodel, which is consistent when paired with any of\nthe decoding algorithms considered in this paper.\n4.1 Consistent Sampling Algorithms\nThe proof of Theorem 3.4 suggests that the incon-\nsistency of incomplete decoding algorithms arises\nfrom the fact that ⟨eos⟩may be excluded indeﬁ-\nnitely from the set of top-ranked tokens. We pro-\npose a simple modiﬁcation to top- k and nucleus\nsampling that forces ⟨eos⟩to be included at each\nstep of decoding. First, we give a condition for\nwhen a particular model pθ paired with a decoding\nalgorithm Fis consistent.\nTheorem 4.1. Suppose a recurrent LM pθ has uni-\nformly bounded ∥ht∥p for some p ≥1. If a de-\ncoding algorithm Fsatisﬁes qF(⟨eos⟩|y<t,C) ≥\npθ(⟨eos⟩|y<t,C) for every preﬁx y<t and context\nC, then the decoding algorithm Fis consistent\nwith respect to the model pθ.3\nWe deﬁne consistent variants of top- k and nu-\ncleus sampling which satisfy this condition.\nDeﬁnition 4.1 (Consistent top-ksampling). Con-\nsistent top-ksampling is top-ksampling with the\n3See Appendix C for the proof.\nFigure 2: The self-terminating recurrent LM uses the\nlayer shown in grey instead of the standard softmax\nlayer. The layer takes logits ( u⊤\n· ht), the previous\nstep’s⟨eos⟩probability (p⟨eos⟩\nt−1 ), and a hyper-parameter\nϵ ∈(0,1). The layer computes αusing Deﬁnition 4.3,\nwhich determines the ⟨eos⟩probability (p⟨eos⟩\nt ∈(ϵ,1)),\nand guarantees that p⟨eos⟩\nt > p⟨eos⟩\nt−1 . The remaining\nprobability mass is allocated to the non-⟨eos⟩tokens.\nfollowing modiﬁed proposal distribution:\nq(v) ∝\n{\npθ(v|y<t,C), if v∈V′,\n0, otherwise,\nwhere V′= {⟨eos⟩}∪arg top-k\nv′\npθ(v′|y<t,C).\nDeﬁnition 4.2 (Consistent nucleus sampling) .\nConsistent nucleus sampling is nucleus sampling\nwith the following modiﬁed proposal distribution:\nq(v) ∝\n{\npθ(v|y<t,C), if v∈Vµ ∪{⟨eos⟩},\n0, otherwise.\nThe induced probability of ⟨eos⟩under these\ntwo algorithms is always equal to or larger than the\nmodel’s probability. By Theorem 4.1, these algo-\nrithms are consistent with respect to any consistent\nrecurrent language model.\n4.2 Self-Terminating Recurrent LM\nAlthough these consistent sampling algorithms can\nbe used with any recurrent language model, their\nstochastic nature may not be suitable for ﬁnding a\nsingle, highly probable sequence. To avoid this lim-\nitation, we propose the self-terminating recurrent\nlanguage model (STRLM).\nDeﬁnition 4.3 (Self-terminating recurrent lan-\nguage model). A self-terminating recurrent lan-\nguage model computes the following conditional\nprobability at each time step:\npθ(v|y<t,C) =\n\n\n\n1 −α(ht), v = ⟨eos⟩,\nα(ht) exp(u⊤\nv ht+cv)∑\nv′∈V′exp(u⊤\nv′ht+cv′) ,\nα(h0) = σ(u⊤\n⟨eos⟩h0),\nα(ht) = σ(u⊤\n⟨eos⟩ht) [1−pθ(⟨eos⟩|y<t−1,C)] ,\nwith σ : R →[0,1 −ε] and ε ∈(0,1). ht is\ncomputed as in the original recurrent LM.\nThe underlying idea is that the probability of\n⟨eos⟩increases monotonically, since\np⟨eos⟩\nt = 1 −\nt∏\nt′=0\nσ(u⊤\n⟨eos⟩ht′).\nConsequently, the STRLM is consistent when\npaired with greedy decoding or beam search; see\nAppendix C for formal statements and proofs.\n5 Empirical Validation\nThe theoretical results rely on the existence of a\nmodel that results in inconsistency; it remains to\nbe shown that inconsistency with respect to incom-\nplete decoding occurs with recurrent language mod-\nels encountered in practice. Moreover, while the\nproposed methods carry theoretical guarantees in\nterms of consistency, we must check whether they\nretain language modeling quality. To do so, we\nperform experiments using a sequence completion\ntask. In each experiment, we use the beginning of\na sequence as context, then decode continuations\nfrom a trained recurrent LM and measure the pro-\nportion of non-terminated sequences in order to\napproximately measure inconsistency. The ﬁrst ex-\nperiment (§5.1) shows that inconsistency occurs in\npractice, and the second experiment (§5.2) shows\nthe effectiveness of the proposed approaches. Our\nthird experiment (§5.3) shows that inconsistency\nalso occurs frequently in GPT-2, a large-scale trans-\nformer language model.4\nSequence completion. We evaluate recurrent\nlanguage models on a sequence completion task,\nwhich has previously been used to evaluate the\neffectiveness of sequence models, e.g., Sutskever\net al. (2011); Graves (2013); Radford et al. (2019);\nHoltzman et al. (2019); Welleck et al. (2019). Se-\nquence completion is a general setting for studying\n4Code available at https://github.com/uralik/\nconsistency-lm.\nthe behavior of language models, encompassing\nmachine translation (Bahdanau et al., 2015), story\ngeneration (Fan et al., 2018), and dialogue mod-\neling (Vinyals et al., 2015). The task consists of\ndecoding a continuation ˆY ∼F (pθ,C) given a\nlength-k preﬁx C = ( c1,...,c k), resulting in a\ncompletion (c1,...,c k,ˆy1 ..., ˆyT).\nDataset. Our ﬁrst two experiments use Wikitext2\n(Merity et al., 2016), which consists of paragraphs\nfrom English Wikipedia, since it has frequently\nbeen used to evaluate language models (Grave et al.,\n2017; Melis et al., 2018; Merity et al., 2018). We\nconsider both word and BPE 5 tokenization. We\nsplit each paragraph into sentences using Spacy6.\nWe split each sequence, using the ﬁrst k tokens\nas a context and the remaining tokens as a con-\ntinuation. To ensure that each sequence contains a\npreﬁx, we prepend padding tokens to make it length\nk. Special ⟨bos⟩and ⟨eos⟩tokens are inserted at\nthe beginning and end of each sequence. We use\nk= 10. Table 7 contains dataset statistics.\nContext distribution. We deﬁne empirical con-\ntext distributions with preﬁxes from the train, valid,\nand test sets: p(C; D) = 1\n|D|\n∑|D|\nn=1 1(C = C(n)),\nwhere D= {(C(n),Y (n))}N\nn=1 is a dataset split.\nEvaluation metrics. We use ﬁnite sequences to\napproximately measure the consistency of a model\npaired with a decoding algorithm, since decoding\nan inﬁnite-length sequence is impossible. We use\nthe proportion of decoded continuations that are\nlonger than a predeﬁned limit,\nrL = 1\n|D|\n|D|∑\nn=1\n1(|ˆY(n)|≥ L),\nwhere ˆY(n) ∼F(pθ,C(n)) for each context C(n)\nin D. We call rL the non-termination ratio of the\ndecoding algorithm Ffor an underlying model and\ncontext distribution. A value of rL greater than\nzero means that some sequences did not terminate\nwithin L steps. When L is inﬁnity, this implies\nthat the model paired with the decoding algorithm\nis inconsistent. In practice, we use a ﬁnite Lthat\nis substantially larger than the maximum training\nsequence length, and we interpret a non-zero rL as\nevidence that the model paired with the decoding\nalgorithm is inconsistent. We use L= 1500, more\nthan 10 times the max training sequence length.\n5github.com/huggingface/tokenizers\n6https://spacy.io/\nIn each experiment, we report the mean and stan-\ndard deviation of metrics across 10 independent ini-\ntializations. Unless speciﬁed otherwise, we report\nmetrics using the test context distribution, since\nthe train, valid, and randomly generated context\ndistributions had similar results.\nTraining. We train recurrent language\nmodels for sequence completion with\nmaximum likelihood, using the loss\nL(pθ,Y ) = −∑T\nt=1 log pθ(yt|y<t,c1,...,c k),\nwhere Y = (c1,...,c k,y1,...,y T). This amounts\nto running the full training sequence through a\nrecurrent model and zeroing the loss for the ﬁrst\nk tokens, so that the ﬁrst k steps correspond to\nlearning a gθ that encodes the context.\nModels. We consider recurrent neural networks\nwith hyperbolic tangent activations (tanh-RNN; El-\nman, 1990) and LSTM units (LSTM-RNN; Hochre-\niter and Schmidhuber, 1997). We perform an ini-\ntial hyper-parameter sweep and select the best set\nof hyper-parameters for each of tanh-RNN and\nLSTM-RNN based on the validation perplexities.7\nWith this best set of hyperparameters, we train each\nof these models with 10 different initializations.\nThe choice of tanh and LSTM RNNs implies that\nall of the recurrent language models that we train\nare consistent according to Lemma 3.2. Our LSTM\nmodels achieve similar test perplexity (91.86±0.4,\nword tokenization) to those reported in previous\nwork (Merity et al., 2018); see Appendix D.\nAdditionally, we train self-terminating tanh-\nRNN and LSTM-RNN variants (Deﬁnition 4.3) at\nvarious values of ε, which controls a lower bound\non the termination probability at each step. We use\nσ(x) = (1 −ε) ·sigmoid(x). We use the hyper-\nparameters selected in the preceding grid search.\nBelow, we consider BPE tokenization; similar con-\nclusions held for word tokenization.8\n5.1 Inconsistency of Recurrent LMs\nIn this experiment, we demonstrate evidence of\ninconsistency with incomplete decoding methods.\nTable 1 shows non-termination ratios for the re-\ncurrent language models using the decoding algo-\nrithms considered in this work. Decoding with an-\ncestral sampling always resulted in sequences that\nterminated within Lsteps, since the induced distri-\nbution is the same as that of the consistent model.\n7Refer to Appendix D for the hyper-parameter ranges.\n8Refer to Appendix for results with word tokenization.\ntanh-RNN LSTM-RNN\nancestral 0.00 ±0.0 0.00 ±0.0\ngreedy 12.35 ±5.18 1.53 ±1.41\nbeam-2 1.38 ±0.95 0.07 ±0.06\nbeam-4 0.25 ±0.19 0.00 ±0.01\ntopk-2 0.01 ±0.01 0.01 ±0.01\ntopk-4 0.00 ±0.0 0.00 ±0.01\nnucleus-0.2 0.06 ±0.02 0.13 ±0.15\nnucleus-0.4 0.04 ±0.02 0.02 ±0.01\nconsistent topk-2 0.00 ±0.0 0.00 ±0.01\nconsistent topk-4 0.00 ±0.0 0.00 ±0.0\nconsistent nucleus-0.2 0.04 ±0.02 0.01 ±0.01\nconsistent nucleus-0.4 0.02 ±0.02 0.01 ±0.01\nTable 1: Non-termination ratio (rL (%)) of decoded se-\nquences using ancestral sampling, incomplete, and con-\nsistent decoding methods.\nOn the other hand, the non-zero non-termination\nratios for the incomplete decoding algorithms sug-\ngest inconsistency with respect to each algorithm,\nproviding evidence for Theorem 3.4.\nUsing greedy decoding, roughly 12% of all\ncontexts resulted in a non-terminating continua-\ntion with the tanh-RNN, and roughly 1% with\nthe LSTM-RNN. Nucleus sampling also produced\nnon-terminating sequences with the tanh-RNN\n(0.06%, nuc-0.2) and LSTM-RNN (0.13%, nuc-\n0.2). Top- k sampling yielded a small number\nof non-terminating samples. In general, non-\ntermination approaches zero as kand µincrease,\nsince ⟨eos⟩has a lower chance of being excluded.\nBeam search produced non-terminating se-\nquences with both the tanh-RNN and LSTM-RNN\nmodels. This means that ⟨eos⟩was outside of the\ntop tokens (determined by the beam width) con-\nsidered at each step, since in our experiments we\nterminated the beam search when a single beam\npreﬁx contained ⟨eos⟩. Larger beam widths reduce\nnon-termination, similar to increasing kor µ.\n5.2 Consistency of the Proposed Methods\nConsistent sampling. Table 1 shows that consis-\ntent nucleus and top- k sampling (§4.1) resulted\nin only terminating sequences, except for a few\ncases that we attribute to the ﬁnite limit L used\nto measure the non-termination ratio. Consistent\nnucleus paired with tanh-RNN did not reduce rL\nas much as when it was paired with LSTM-RNN.\nExample continuations are shown in Table 2. On\npreﬁxes that led to non-termination with the base-\nline method, the quality tends to improve with the\nconsistent variant since the continuation now termi-\nPreﬁx One Direction delivered a performance of “ Kiss You\nnucleus ” , and the album ’s second album , “ The X @-@ Files ” , “ The A. ” , “ The Preder ” , “ We ’ve Have You ” , “\nI ’ve You Wanna Stay ” , “ The Dream ” , “ The Bide ” , “ My Achievement ”, “ The B. B. ” , “ A Life ”...\nc-nucleus ” , and “ My Boo ” was released on September 29 , 2010 .⟨eos⟩\nPreﬁx Boulter starred in two ﬁlms in 2008 ,\nnucleus and the band ’s music , and “ The Rise of Monkey ” , “ The One With the Way ” , “ The “ Always ” , ” “ Always\nYour ” , “ The Wift ” , “ The Baste ” , “ The Special With ” , “ The Way ” , “ The Special With You ”...\nc-nucleus and the latter was released in the United States .⟨eos⟩\nPreﬁx This period of unhappiness was the making of\nBaseline the “ most important ” of the “ mad ” , and the “ “ most important ” of the ” “ ” , “ the most important ” , and\nthe “ devil ” , “ The ” , “ The One ” , “ The One ” , “ The One ” , “ The One ” , “ The One ” , “ The One ” , “\nThe One ” , “ The One ” , “ The One ” , “ The One ” , “ The One ” , “ The One ” , “ The One ” , “ The One ”...\nSTRLM the ﬁrst commandment of the poem .⟨eos⟩\nPreﬁx Du Fu ’s mother died shortly after he was\nBaseline a member of the Order of the Order of the Order of the Order of the Order of the Order of the Order of the\nOrder of the Order of the Republic of the Republic of the Republic of the Republic of the Republic of...\nSTRLM a member of the Order of the British Empire .⟨eos⟩\nTable 2: Continuations with consistent nucleus sampling (µ= 0.2) and self-terminating LSTM (ϵ= 10−3).\nnates. Note that since the model’s non-⟨eos⟩token\nprobabilities at each step are only modiﬁed by a\nmultiplicative constant, the sampling process can\nstill enter a repetitive cycle (e.g., when the constant\nis close to 1), though it is guaranteed to terminate.\nSelf-terminating RLM. As seen in Table 3, the\nself-terminating recurrent language models are con-\nsistent with respect to greedy decoding, at the ex-\npense of perplexity compared to the vanilla model.\nThe value of εfrom Deﬁnition 4.3, which controls\na lower-bound on termination probability at each\nstep, inﬂuences both rL and perplexity. When εis\ntoo large (ε= 10−2), perplexity degrades. When\nεis too small (ε= 10−4), the lower-bound grows\nslowly, so ⟨eos⟩is not guaranteed to be top-ranked\nwithin L steps, resulting in a positive rL. An ε\nof 10−3 balanced consistency and language model-\ning quality, with a zero non-termination ratio and\nperplexity within 8 points of the baseline.\nAs shown in Figure 3, the self-terminating model\nmatches the data length distribution better than\nthe baseline. Example decoded sequences are\nshown in Table 2. For preﬁxes that led to non-\ntermination with the baseline, the self-terminating\nmodels yields ﬁnite sequences with reasonable\nquality. The examples suggest that some cases\nof degenerate repetition (Holtzman et al., 2019;\nWelleck et al., 2019) are attributed to inconsistency.\n5.3 Inconsistency of GPT-2\nWe perform a ﬁnal experiment with GPT-2 117M,\na transformer language model pre-trained with\nmaximum likelihood on WebText, a collection of\nST ϵ r L (%) perplexity\ntanh-RNN\n! 10−2 00.00 ±0.0 229.09 ±9.2\n! 10−3 00.00 ±0.0 191.63 ±1.4\n! 10−4 00.02 ±0.02 188.36 ±2.2\n\u0017 – 12.35 ±5.2 186.44 ±1.4\nLSTM\n! 10−2 0.00 ±0.0 219.71 ±9.2\n! 10−3 0.00 ±0.0 186.04 ±1.6\n! 10−4 0.18 ±0.35 183.57 ±2.3\n\u0017 – 1.48 ±1.43 178.19 ±1.3\nTable 3: Non-termination ratio (rL (%)) of greedy-\ndecoded sequences and test perplexity for STRLMs.\nFigure 3: Lengths of generated sequences using greedy\ndecoding from vanilla and self-terminating LSTMs.\nscraped web pages (see Radford et al. (2019)).\nGPT-2 has been observed to produce repetitive\ntext with greedy and beam search (Holtzman et al.,\n2019).\nExperimental setup. We use the Wikitext-103\ndataset (Merity et al., 2016), a large-scale collec-\ntion of Wikipedia articles with over 100 million\nwords and 260 thousand unique tokens. We split\nthe dataset into sequences according to the dataset’s\nnewline boundaries, then split each sequence into a\ncontext Cand continuation Y, resulting in a dataset\nof (C,Y ) pairs. Each continuation ends in a spe-\ncial ⟨eos⟩token. We use a context size of k = 10\ntokens, and discard sequences that are length k\nor shorter. The resulting dataset contains 874,556\ntraining, 1,896 validation, and 2,162 test pairs.\nWe ﬁne-tune the pre-trained GPT-2 model using\nmaximum likelihood for 400k steps, and select the\nmodel state with the lowest validation perplexity\n(evaluated every 5k steps). Each training batch con-\ntains a maximum of 1024 total tokens. We use the\nimplementation and default hyper-parameters from\nthe transformers library (Wolf et al., 2019).\nWe ﬁne-tune the self-terminating GPT-2 models in\na similar manner, starting from the pre-trained GPT-\n2 model and using the same hyper-parameters.\nEach model is evaluated using greedy decoding\nwith a maximum sequence length of 500, which\nwas selected so that each decoded validation batch\ncould ﬁt in GPU memory. We deﬁne the non-\ntermination ratio (rL) using L = 500; this limit\nis more strict than the limit used in the preced-\ning experiments (1500), yet still allows us to see\nlarge differences in generation behavior between\nthe model and the ground truth (e.g. see Figure 4).\nResults. Table 4 shows the non-termination ratio\nand perplexity of the baseline and self-terminating\nGPT-2 models. The self-terminating variant pre-\nvents non-termination, at the cost of perplexity. The\nmodel here uses ϵ = 2 .5 ×10−3, which we se-\nlected after observing that at higher values of ϵ, e.g.\n1.0 ×10−3, the self-terminating model generated\nsequences longer than the limit used to determine\ntermination (500). Figure 4 shows the length dis-\ntributions of the baseline GPT-2 continuations and\nthose of the self-terminating GPT-2. The GPT-\n2 117M model generates many sequences at or\nnear the maximum sequence length (500), unlike\nthe ground-truth data. Introducing self-termination\nshifts the mass towards shorter sequences, whose\nlengths are also present in the ground-truth data.\n6 Future Directions\nThe methods we proposed in this paper resolve in-\nconsistency by changing the decoding algorithm\nor model parameterization. Another approach is to\naddress inconsistency in the learning phase. One\nrL (%) perplexity\nGPT2-117M 37.91 20.92\nGPT2-117M ST 00.00 27.25\nTable 4: Non-termination ratio (rL (%)) of greedy-\ndecoded sequences and perplexity for GPT2-117M and\nthe self-terminating variant (ST) on Wikitext-103.\ninteresting direction is to investigate whether the\nlack of decoding in maximum likelihood learning\nis a cause of inconsistency. Maximum likelihood\nlearning ﬁts the model pθ using the data distribu-\ntion, whereas a decoded sequence from the trained\nmodel follows the distribution qF induced by a\ndecoding algorithm. Sequence-level learning, how-\never, uses a decoding algorithm during training\n(e.g., Ranzato et al. (2016)), which we hypothe-\nsize can result in a good sequence generator that is\nconsistent with respect to incomplete decoding.\n7 Conclusion\nWe extended the notion of consistency of a recur-\nrent language model put forward by Chen et al.\n(2017) to incorporate a decoding algorithm, and\nused it to analyze the discrepancy between a model\nand the distribution induced by a decoding algo-\nrithm. We proved that incomplete decoding is in-\nconsistent, and proposed two methods to prevent\nthis: consistent decoding and the self-terminating\nrecurrent language model. Using a sequence com-\npletion task, we conﬁrmed that empirical incon-\nsistency occurs in practice, and that each method\nprevents inconsistency while maintaining the qual-\nity of generated sequences. We suspect the absence\nof decoding in maximum likelihood estimation as a\ncause behind this inconsistency, and suggest inves-\ntigating sequence-level learning as an alternative.\nAcknowledgements\nWe thank Chris Dyer, Noah Smith and Kevin\nKnight for valuable discussions. This work was\nsupported by NSF Award 1922658 NRT-HDR: FU-\nTURE Foundations, Translation, and Responsibil-\nity for Data Science; Samsung Advanced Institute\nof Technology (Next Generation Deep Learning:\nfrom pattern recognition to AI); and Samsung Re-\nsearch (Improving Deep Learning using Latent\nStructure). KC thanks eBay and NVIDIA for their\nsupport.\nReferences\nDaniel Andor, Chris Alberti, David Weiss, Aliaksei\nSeveryn, Alessandro Presta, Kuzman Ganchev, Slav\nPetrov, and Michael Collins. 2016. Globally normal-\nized transition-based neural networks. In 54th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2016 - Long Papers , volume 4,\npages 2442–2452.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nT. L. Booth and R. A. Thompson. 1973. Applying prob-\nability measures to abstract languages. IEEE Trans-\nactions on Computers, C-22(5):442–450.\nJohn S Bridle. 1990. Probabilistic interpretation of\nfeedforward classiﬁcation network outputs, with re-\nlationships to statistical pattern recognition. In Neu-\nrocomputing, pages 227–236. Springer.\nYining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan\nMay, and Kevin Knight. 2017. Recurrent neural\nnetworks as weighted language recognizers. arXiv\npreprint arXiv:1711.05408.\nKyunghyun Cho, Bart van Merri ¨enboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. 2014. On the properties\nof neural machine translation: Encoder–decoder ap-\nproaches. In Proceedings of SSST-8, Eighth Work-\nshop on Syntax, Semantics and Structure in Statisti-\ncal Translation, pages 103–111, Doha, Qatar. Asso-\nciation for Computational Linguistics.\nJeffrey L Elman. 1990. Finding structure in time. Cog-\nnitive science, 14(2):179–211.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. arXiv preprint\narXiv:1805.04833.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017. Improving neural language models with a con-\ntinuous cache. In 5th International Conference on\nLearning Representations, ICLR 2017 - Conference\nTrack Proceedings.\nAlex Graves. 2013. Generating sequences with\nrecurrent neural networks. arXiv preprint\narXiv:1308.0850.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735–1780.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin\nChoi. 2019. The curious case of neural text degener-\nation. arXiv preprint arXiv:1904.09751.\nJohn Lafferty, Andrew McCallum, and Fernando C N\nPereira. 2001. Conditional random ﬁelds: Prob-\nabilistic models for segmenting and labeling se-\nquence data. ICML ’01 Proceedings of the Eigh-\nteenth International Conference on Machine Learn-\ning.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2018. On\nthe state of the art of evaluation in neural language\nmodels. In 6th International Conference on Learn-\ning Representations, ICLR 2018 - Conference Track\nProceedings.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and optimizing LSTM\nlanguage models. In 6th International Conference\non Learning Representations, ICLR 2018 - Confer-\nence Track Proceedings.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. ArXiv, abs/1609.07843.\nKenton Murray and David Chiang. 2018. Correct-\ning length bias in neural machine translation. In\nProceedings of the Third Conference on Machine\nTranslation: Research Papers, pages 212–223, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2016. Sequence level train-\ning with recurrent neural networks. In 4th Inter-\nnational Conference on Learning Representations,\nICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,\nConference Track Proceedings.\nPavel Sountsov and Sunita Sarawagi. 2016. Length\nbias in encoder decoder models and a case for global\nconditioning. In Proceedings of the 2016 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 1516–1525, Austin, Texas. Asso-\nciation for Computational Linguistics.\nFelix Stahlberg and Bill Byrne. 2019. On NMT search\nerrors and model errors: Cat got your tongue? In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3354–\n3360, Hong Kong, China. Association for Computa-\ntional Linguistics.\nIlya Sutskever, James Martens, and Geoffrey Hinton.\n2011. Generating text with recurrent neural net-\nworks. In Proceedings of the 28th International\nConference on Machine Learning, ICML 2011.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems.\nOriol Vinyals, Google Quoc, and V Le. 2015. A Neu-\nral Conversational Model. In ICML Deep Learning\nWorkshop.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2019. Neu-\nral text generation with unlikelihood training. arXiv\npreprint arXiv:1908.04319.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Fun-\ntowicz, et al. 2019. Transformers: State-of-the-\nart natural language processing. arXiv preprint\narXiv:1910.03771.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nA Additional Deﬁnitions\nIn contrast to greedy decoding, beam search with\nwidth k, Fbeam-k, operates on the level of partial\nsequences or preﬁxes.\nDeﬁnition A.1 (Preﬁx). A preﬁx ρt is an ordered\ncollection of items from V. The score of a preﬁx is\ns(ρt) =\nt∑\nτ=1\nlog pθ(yτ = ρt[τ] |ρt[<τ ],C),\nwhere ρt[τ] is a token at time τ from ρt.\nStarting from a set of empty preﬁxes, at each\niteration a new preﬁx set is formed by expanding\neach preﬁx, then choosing the k highest scoring\nexpanded preﬁxes.\nDeﬁnition A.2 (Beam search). Beam search with\nwidth k, Fbeam−k, generates a sequence from a\nrecurrent language modelpθ by maintaining a size-\nk preﬁx set Ptop\nt . Starting with Ptop\n0 = ∅, at\neach iteration t ∈{1,2,... }beam search forms\na new preﬁx set Ptop\nt by expanding the current set,\nPt = ⋃\nρ∈Ptop\nt−1\n{ρ◦v|v∈V}(where ρ◦vis con-\ncatenation), then choosing the k highest scoring\nelements: Ptop\nt = arg top-k\nρ∈Pt\ns(ρ).Any ρ∈Ptop\nt end-\ning with ⟨eos⟩is restricted from being expanded\nfurther, and is added to a set S. Beam search ends\nwhen Scontains ksequences, and returns the high-\nest scoring sequence in S.\nB Proof of Lemmas in Section 3\nLemma 3.1. If a recurrent language model pθ is\nconsistent, pθ(|Y|= ∞|C) = 0 for any probable\ncontext C.\nProof. Suppose there exists a probable context ˜C\nsuch that pθ(|Y|= ∞| ˜C) >0. Then\npθ(|Y|= ∞) = E[pθ(|Y|= ∞|C)]\n≥p( ˜C)pθ(|Y|= ∞| ˜C) >0,\nwhich contradicts the consistency of the model pθ.\nLemma 3.2. A recurrent language model pθ is\nconsistent if ∥ht∥p is uniformly bounded for some\np≥1.\nProof. Let B > 0 be an upper bound such that\n∥ht∥p < Bfor all t. Let q be the conjugate of\npsatisfying 1/p+ 1/q = 1. Then we have from\nH¨older’s inequality, for allv∈V and t,\nu⊤\nv ht ≤∥u⊤\nv ht∥1 ≤∥ht∥p∥uv∥q <Bu +,\nwhere u+ = maxv∈V ∥uv∥q. Note that\nlog\n∑\nv∈V\neu⊤\nv ht+cv ≤log\n(\nmax\nv∈V\neu⊤\nv ht+cv ×|V|\n)\n≤max\nv∈V\n{u⊤\nv ht + cv}+ log|V|\n<Bu + + c+ + log|V|,\nwhere c+ = maxv∈V cv. For a given y<t and con-\ntext C,\nlog pθ(⟨eos⟩|y<t,C)\n=(u⊤\n⟨eos⟩ht + c⟨eos⟩) −log\n∑\nv∈V\neu⊤\nv ht+cv\n>(−Bu+ + c⟨eos⟩) −(Bu+ + c+ + log|V|) >−∞,\nand it follows that pθ(⟨eos⟩|y<t,C) > ξ >0 for\nsome strictly positive constant ξ. Then\npθ(|Y|= ∞) = lim\nt→∞\npθ(|Y|>t)\n= lim\nt→∞\nE[pθ(|Y|>t |C)]\n= E\n[\nlim\nt→∞\npθ(|Y|>t |C)\n]\n≤E\n[\nlim\nt→∞\n(1 −ξ)t\n]\n= 0,\nand hence pθ is consistent.\nLemma 3.3. A consistent decoding algorithm with\nrespect to a consistent recurrent language model\ndecodes only probable sequences. That is, if\nqF(Y |C) >0, then pθ(Y |C) >0 for any proba-\nble context C.\nProof. Suppose there exists a decoded sequence\n˜Y by F and probable context ˜C such that\nqF( ˜Y |˜C) > 0 but pθ( ˜Y |˜C) = 0 . By Remark\n2.1, the sequence ˜Y is of inﬁnite length and thus\nqF(|Y|= ∞| ˜C) ≥qF( ˜Y |˜C) >0, which contra-\ndicts the consistency of qFby Lemma 3.1.\nC Proofs for Section 4\nTheorem 4.1. Suppose a recurrent LM pθ has uni-\nformly bounded ∥ht∥p for some p ≥1. If a de-\ncoding algorithm Fsatisﬁes qF(⟨eos⟩|y<t,C) ≥\npθ(⟨eos⟩|y<t,C) for every preﬁx y<t and context\nC, then the decoding algorithm Fis consistent\nwith respect to the model pθ.\nProof. By Lemma 3.2 the model pθ is con-\nsistent and pθ(⟨eos⟩|y<t,C) > ξ for some\npositive value ξ. Thus, qF(⟨eos⟩|y<t,C) ≥\npθ(⟨eos⟩|y<t,C) >ξ. For t≥1,\nqF(|Y|>t |C)\n= qF(y1 ̸= ⟨eos⟩,··· ,yt ̸= ⟨eos⟩|C)\n≤(1 −ξ)t.\nTaking the limit t →∞ and expectation over C,\nwe have\nqF(|Y|= ∞) = EC\n[\nlim\nt→∞\nqF(|Y|>t |C)\n]\n≤lim\nt→∞\n(1 −ξ)t = 0,\nfrom which the decoding algorithm is consistent.\nTheorem 4.2. Greedy decoding is consistent with\nrespect to any self-terminating recurrent LM.\nProof. Let p⟨eos⟩\nt denote pθ(⟨eos⟩|y<t,C) and\na⟨eos⟩\nt denote u⊤\n⟨eos⟩ht + c⟨eos⟩. By Deﬁnition 4.3\nwe have\np⟨eos⟩\nt = 1 −σ(a⟨eos⟩\nt )(1 −p⟨eos⟩\nt−1 )\n= 1 −\nt∏\nt′=0\nσ(a⟨eos⟩\nt′ ) ≥1 −(1 −ϵ)t+1.\nTake B = −log 2/log(1 −ϵ). We then have\np⟨eos⟩\nt > 1/2 for all t > B, which implies that\n⟨eos⟩is always the most probable token after time\nstep B. Hence, the sequence length is less than B\nwith probability 1.\nTheorem 4.3. Beam search with widthk, Fbeam−k,\nis consistent with respect to any STRLM.\nProof. Let S(ρ) be the size-kset of sequences kept\nby Fbeam−k that start with a preﬁx ρ.\nTake B = −log 2/log(1 −ϵ) as in the proof of\nTheorem 4.2. Suppose that there exists at least one\npreﬁx ˆρ∈Ptop\nB which does not end with ⟨eos⟩.\nWe ﬁrst want to show that ˆρinduces at most k\nmore steps in beam search with width k, that is,\nY ∈S(ˆρ) implies |Y|≤ B+ k.\nWe know from the proof of Theorem 4.2 that\nan STRLM pθ satisﬁes: for any context C and\nv∈V \\{⟨eos⟩},\npθ(⟨eos⟩|ˆρ,C) >pθ(v|ˆρ,C).\nFor any subsequence y = (y1,...,y l) with y1 ̸=\n⟨eos⟩,\npθ(ˆρ◦y|ˆρ,C) =\nl∏\ni=1\npθ(yi|ˆρ◦y<i,C)\n≤pθ(y1 |ˆρ,C)\n<pθ(⟨eos⟩|ˆρ,C).\nThus, ˆρ ◦⟨eos⟩is the most probable sequence\namong sequences starting with the preﬁx ˆρ, and\nit follows that ˆρ◦⟨eos⟩∈ S(ˆρ).\nThus, in S(ˆρ), there are (k−1) sequences start-\ning with ˆρ◦vfor v ∈V \\{⟨eos⟩}. By the same\nargument, at each step at least one sequence ending\nwith ⟨eos⟩is added to S(ˆρ), and therefore at time\nstep (B+ k), ksequences ending with ⟨eos⟩are in\nS(ˆρ).\nNote that the result set S by Fbeam−k (Deﬁni-\ntion 2.11) satisﬁes\nS ⊆\n⋃\nρ∈Ptop\nB\nS(ρ).\nSince each ρ∈Ptop\nB induces sequences of length\nat most B+ k, we have\npθ(|Y|>B + k|C) = 0.\nTaking the expectation over C yields the consis-\ntency of the model pθ.\nParameter Values\nHidden Size {256,512,1024}\nDropout {0.1,0.3,0.5}\nEmbedding Weight Tying {True,False}\nTable 5: Grid search speciﬁcation. The values selected\nfor the LSTM-RNN and tanh-RNN models are shown\nin bold and italics, respectively (word tokenization).\nD Additional Results and Experiment\nDetails\nTraining. Each model is trained on a single\nNvidia P40 GPU for up to 100 epochs, stopping\nwhen validation perplexity does not decrease for\n10 consecutive epochs.\nHyper-parameters. Tables 5,6 show the grid\nsearch speciﬁcations. All models were 2 layers\nand were trained with the Adam optimizer.\nModel perplexities. Tables 10, 11 shows train\nand test perplexities for the tanh-RNN and LSTM-\nRNN models using word and BPE tokenization,\nrespectively.\nAdditional example continuations. Table 12\nshows additional greedy-decoded continuations us-\ning a self-terminating LSTM-RNN and the baseline\nLSTM-RNN with BPE tokenization.\nGPT-2 length distributions. Figure 4 shows the\nlength distributions of ground-truth continuations,\ncontinuations from GPT-2 117M, and continuations\nfrom the self-terminating GPT-2 117M.\nFigure 4: Lengths of ground-truth and greedy-decoded\ncontinuations from the baseline GPT-2 117M and self-\nterminating GPT-2 117M models (ϵ= 0.0025).\nParameter Values\nHidden Size {256,512,1024}\nDropout {0.1,0.3,0.5}\nEmbedding Weight Tying {True,False}\nTable 6: Grid search speciﬁcation. The values selected\nfor the LSTM-RNN and tanh-RNN models are shown\nin bold and italics, respectively (BPE tokenization).\nType # Train # Valid # Test |V| Avg. len\nWord 78274 8464 9708 33182 24\nBPE 83344 8721 10156 19483 28\nTable 7: Wikitext2 statistics.\ntanh-RNN LSTM-RNN\nancestral 0.00 ±0.0 0.00 ±0.0\ngreedy 6.07 ±5.6 1.03 ±0.3\nbeam-2 1.21 ±0.3 0.07 ±0.1\nbeam-4 0.29 ±0.1 0.00 ±0.0\ntopk-2 0.84 ±0.8 0.00 ±0.0\ntopk-4 0.02 ±0.0 0.00 ±0.0\nnucleus-0.2 2.49 ±0.2 0.76 ±0.3\nnucleus-0.4 0.32 ±0.1 0.22 ±0.1\nTable 8: Non-termination ratio ( rL (%)) of decoded\nsequences using ancestral sampling and incomplete de-\ncoding methods (word tokenization).\nST ϵ r L (%) perplexity\ntanh-RNN\n! 10−2 0.00 ±0.0 150.07 ±2.7\n! 10−3 0.00 ±0.0 138.01 ±0.6\n! 10−4 1.04 ±0.6 138.67 ±1.8\n\u0017 – 6.07 ±5.6 136.57 ±1.8\nLSTM\n! 10−2 0.00 ±0.0 101.24 ±0.3\n! 10−3 0.00 ±0.0 94.33 ±0.6\n! 10−4 0.94 ±0.5 94.15 ±0.8\n\u0017 – 1.03 ±0.3 91.86 ±0.4\nTable 9: Non-termination ratio (rL (%)) of greedy-\ndecoded sequences and test perplexity for self-\nterminating recurrent models (word tokenization).\nmodel context perplexity\ntanh-RNN train 91.54 ±7.9\ntanh-RNN test 136.57 ±1.8\nLSTM-RNN train 45.80 ±2.5\nLSTM-RNN test 91.86 ±0.4\nTable 10: Perplexities of trained recurrent language\nmodels (word tokenization).\nmodel context perplexity\ntanh-RNN train 61.20 ±1.2\ntanh-RNN test 186.44 ±1.4\nLSTM-RNN train 72.72 ±2.4\nLSTM-RNN test 178.39 ±1.2\nTable 11: Perplexities of trained recurrent language\nmodels (BPE tokenization).\nPreﬁx Payne was quoted as saying : “ With the album\nnucleus ’s ” album , “ The Predious ” , “ The One With the Wind ” , “ I ’ve Ever ’t Have You ’s My Way ” , “ I ’ve\nEver It ’s Johnny ” , “ The Predes ” , “ “ Always ” , “ The Predatory Was ” , “ The Dream ” , “ The Baste ” , ” “\nAlways Boy ” , “ My Drum ” , “ The Simpsons ” , “ “ Always Man ”, “ The “ Sweet Night ” ,...\nc-nucleus ’s ” album , “ The Predious ” , “ The One With the Wind ” , “ I ’ve Ever ’t Have You ’s My Way ”⟨eos⟩\nPreﬁx In a 2006 interview , fellow actor Ben Whish\nnucleus ’s father , a young actor , and a romantic relationship with the show , “ The One Where the The Simpsons ” , “\nThe Pape ” , “ The Next Generation ” , “ The Sixth Extinction ” , “ We ’t You Wanna Stay ” , “ The Dream ” , “\nThe Predator ” , “ The Collection ” , “ The Big Lear ” , “ The Predor ” , “ The Predation ” , “ My Blue ” , “\nThe Simpsons ” , “ The Sixth Extinction ” , “ My Love ” , “ The Rise of the Year ” , “ The Simpsons ” , “ The\nPredator ” , “ My Dream ” ,...\nc-nucleus was the ﬁrst time in the ﬁlm , and was published in the same episode of the season .⟨eos⟩\nPreﬁx Most of what is known of Du Fu ’s\nBaseline “ the ” , the ” “ great ” , the ” “ ” , “ the most important ” , “ the most important ” , “ Ode to the Nightingale ” , “\nOde to the Nightingale ” , “ Ode to the Nightingale ” , “ Ode to the Nightingale ” , “ Ode to the Nightingale ” , “\nOde to the Nightingale ” , “ Ode to the Nightingale ” , “ Ode to the Nightingale ” ,...\nSTRLM Coty , was a “ one of the most important ” of the American science ﬁction .⟨eos⟩\nPreﬁx He was relieved by Yan Wu , a friend and\nBaseline the ﬁrst wife of the Order of the Order of the Order of the Order of the Order of the Republic of the Republic of\nthe Republic of the Republic of the Republic of the Republic of the Republic of the Republic of the Republic\nof the Republic of the Republic of the Republic of the Republic...\nSTRLM the wife of the Royal Navy .⟨eos⟩\nTable 12: More continuations with consistent nucleus sampling (µ= 0.2) and self-terminating LSTM (ϵ= 10−3)\nwith BPE tokenization.",
  "topic": "Decoding methods",
  "concepts": [
    {
      "name": "Decoding methods",
      "score": 0.8030815124511719
    },
    {
      "name": "Sequence (biology)",
      "score": 0.6897311210632324
    },
    {
      "name": "Language model",
      "score": 0.6079605221748352
    },
    {
      "name": "Computer science",
      "score": 0.5899327993392944
    },
    {
      "name": "Sampling (signal processing)",
      "score": 0.5879846811294556
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.581102728843689
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5587124228477478
    },
    {
      "name": "Algorithm",
      "score": 0.4988081455230713
    },
    {
      "name": "Beam search",
      "score": 0.4580309987068176
    },
    {
      "name": "Repetition (rhetorical device)",
      "score": 0.4572784900665283
    },
    {
      "name": "Meaning (existential)",
      "score": 0.43154090642929077
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36474817991256714
    },
    {
      "name": "Theoretical computer science",
      "score": 0.35356399416923523
    },
    {
      "name": "Search algorithm",
      "score": 0.1919821798801422
    },
    {
      "name": "Psychology",
      "score": 0.15665605664253235
    },
    {
      "name": "Linguistics",
      "score": 0.10912391543388367
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    }
  ]
}