{
  "title": "How Large Language Models Perform on the United States Medical Licensing Examination: A Systematic Review",
  "url": "https://openalex.org/W4386399156",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5092734868",
      "name": "Dana Brin",
      "affiliations": [
        "Tel Aviv University",
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2809724045",
      "name": "Vera Sorin",
      "affiliations": [
        "Tel Aviv University",
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2155772135",
      "name": "Eli Konen",
      "affiliations": [
        "Tel Aviv University",
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2419298921",
      "name": "Girish Nadkarni",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A1498187152",
      "name": "Benjamin S Glicksberg",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A1945837108",
      "name": "Eyal Klang",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai",
        "Tel Aviv University",
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5092734868",
      "name": "Dana Brin",
      "affiliations": [
        "Sheba Medical Center",
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A2809724045",
      "name": "Vera Sorin",
      "affiliations": [
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2155772135",
      "name": "Eli Konen",
      "affiliations": [
        "Sheba Medical Center",
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A2419298921",
      "name": "Girish Nadkarni",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A1498187152",
      "name": "Benjamin S Glicksberg",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A1945837108",
      "name": "Eyal Klang",
      "affiliations": [
        "Sheba Medical Center",
        "Icahn School of Medicine at Mount Sinai"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385416747",
    "https://openalex.org/W4313650047",
    "https://openalex.org/W3010446820",
    "https://openalex.org/W4282916733",
    "https://openalex.org/W4367311208",
    "https://openalex.org/W4385290206",
    "https://openalex.org/W4379769651",
    "https://openalex.org/W4383346782",
    "https://openalex.org/W3118615836",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4385623654",
    "https://openalex.org/W4313197536"
  ],
  "abstract": "ABSTRACT Objective The United States Medical Licensing Examination (USMLE) assesses physicians’ competency and passing is a requirement to practice medicine in the U.S. With the emergence of large language models (LLMs) like ChatGPT and GPT-4, understanding their performance on these exams illuminates their potential in medical education and healthcare. Materials and Methods A literature search following the 2020 PRISMA guidelines was conducted, focusing on studies using official USMLE questions and publicly available LLMs. Results Three relevant studies were found, with GPT-4 showcasing the highest accuracy rates of 80-90% on the USMLE. Open-ended prompts typically outperformed multiple-choice ones, with 5-shot prompting slightly edging out zero-shot. Conclusion LLMs, especially GPT-4, display proficiency in tackling USMLE-standard questions. While the USMLE is a structured evaluation tool, it may not fully capture the expansive capabilities and limitations of LLMs in medical scenarios. As AI integrates further into healthcare, ongoing assessments against trusted benchmarks are essential.",
  "full_text": "How Large La ng ua ge Models Perform on t he United States Med ica l Licensing \nExaminatio n: A Systematic Review \n \nDana Brin, MD1,2; Vera Sorin, MD1-3; Eli Konen, MD1,2; Girish Nadkarni, MD4-5; Benjamin S \nGlicksberg , MD6 ; Eyal Klang, MD1-5  \n1 Department of Diagnostic Imaging, Chaim Sheba Medical Center, Tel Hashomer, Israel  \n2 The Faculty of Medicine, Tel-Aviv University, Israel  \n3 DeepVision Lab, Chaim Sheba Medical Center, Tel Hashomer, Israel  \n4 Division of Data-Driven and Digital Medicine (D3M), Icahn School of Medicine at Mount Sinai, \nNew York, New York, USA  \n5 The Charles Bronfman Institute of Personalized Medicine, Icahn School of Medicine at Mount Sinai, \nNew York, New York, USA.  \n6 Hasso Plattner Institute for Digital Health, Icahn School of Medicine at Mount Sinai, New York, \nNew York, USA. \n \nCorresponding Author: \nDana Brin, MD \nDepartment of Diagnostic Imaging, Chaim Sheba Medical Center  \nAddress: Emek Haela St. 1, Ramat Gan, Israel, 52621.  \nTel: +972-3-5302530, Fax: +972-3-5357315, Email: dannabrin@gmail.com \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nABSTRACT \nO bjective : The United States Medi cal Licensing Examination (USMLE)  assesses \nphysicians' com petency  and passing is a requirement to  practice m edicine in the U.S.  \nWith the emergence of large langu age models (LLMs) like ChatGPT a nd GPT-4, \nunderstanding their perf ormance on these e xams illuminates their potential in medical \neducation and healthcare.  \nMaterials and Methods : A literatur e search f ollowing the 2020 PRIS M A guidelines was \nconducte d, fo cusing on  studies us ing official USMLE questions  and p ublicly avai lable \nLLMs.  \nResults : Three relev ant studies were foun d, with GPT-4 sh owcasing the highest \naccuracy rates o f 80-90% on the U SMLE. Open-en ded prom pts typic ally outperforme d \nmultiple-choice ones , with 5-shot  prom pting slightly edging out zer o-shot .  \nC onclusion : LLMs, especially GPT- 4, display pr oficiency in tackling U SMLE-standard \nquestions. While the USMLE is a st ructured evaluation tool, it may n ot fully capture the \nexpansive capabilities and limitati ons o f LLMs in medical scenarios . As AI integrates \nfurther into healthcare, ong oing a ssessments against trusted bench marks are \nessential.\n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nINTRODUCTION \nThe United States Medical Licen sing Exa minatio n (USM LE) is a multi-p art \nprofessional exam t hat is man d atory to practice medicine in th e United States. \nIt is a rigorous assessment o f p hysicians' kno wledge a nd skills, providing a \nstandardized meas ure of comp etence for both do mestic and i nternatio nal \nmedical graduates. (1–4) As s uc h, the USM LE ha s become a critical benchmark \nfor medical education a nd is in creasingly used in research as a standard for \ntesting the capa bilities of various healt hcare-foc used artificial intelligence \ntools. \nAdvance ments i n na tural lan gu age processing ( NLP ), ha ve led to the \ndevelopment o f large lan gua ge  models (L LMs) like GPT-3 a nd GPT-4. These \nmodels can ge nerate h u man -like text and perform various co mplex N LP tasks. \nLL Ms are increasingly stu died in healt hcare for different ap plications, includin g \naiding diag nosis, streamlinin g administrative tas ks, and e nha ncing medical \neducation.(5 –8) It is critical to a ssess the performa nce of t hese models in a \nstandardized and rigorous ma n ner, especially in speciali zed fi elds like \nmedicine.(6,9)  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nGiven the imp ortant role of t he USMLE in assessin g medical competence, \nundersta ndin g LL Ms f are on thi s test offers valu able insights i nto their clinical \nreasoning, pote ntial applications, and limita tions in healthcar e. Thus, the ai m of \nour study was to syste matically  review the literature on the pe rforma nce of \npublicly available LLMs o n o ffic ial USMLE questions, a nalyze th eir clinical \nreasoning capa bilities, and determine the im pact of various pr omptin g \nmethodolo gies on o utcomes.  \n \nMETHODS \nLiterature sear ch  \nA systematic literature search was cond ucted for st udies on L LM’s perfor mance \non the US MLE.  \nWe searched for articles publis hed up to J uly 2023. PubMed a nd Google Scholar \nwere used as databases. Search  keywords included “USM LE”, “U nited Stated \nMedical License Exams”, “ LL M” and “L arge La ng uage Models”. We also searched \nthe references lists of relevant studies for any additional relevant st udies. \nFigure 1  presents a flow dia gram of t he screening a nd incl usion process.  \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nEl igibil ity Crite ria \nWe included studies that e valua ted the performa nce of p ublicly available LLM’s \non of ficial USMLE questio ns. We excluded papers that evalu at e LLMs usin g \nuno fficial sources of US MLE-lik e questions (e.g., MedQA) a nd non -En glish \npapers.   \n \nScr eenin g and Sy nt hesis \nThis review was reported according to the 2020 Preferred Reporting Items f or \nSystematic Reviews a nd Meta- Analysis (PRISMA ) g uidelines.(10) \n \nRESULTS \nData were extracted from t hree publications tha t evalu ated the  performance o f \npublicly available LLMs o n USM LE questions. T he parameters e valuated in eac h \npublication are described in Table  1 .  \n \nData sets \nThere are two official sources f or USMLE question s – US MLE Sa mple exam, \nwhich is freely available,(11) an d the N MBE Self -Assess ment, a vailable for \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \npurchase at t he N M BE website.( 12) Both i nclude questio ns for Step 1, 2CK and \n3.  \n \nLarge La ng uage Models  \nThe large lang uage models included in this st udy were all developed by \nOpenAI.(13) GPT-3 is an a utore gressive model kno wn for its ability to handle a \nvariety of lan gua ge tasks witho ut extensive fine-t u ning. GPT - 3.5, a subsequent \nversion, serves as the fo u ndati on for bot h ChatGPT a nd Instr u ctGPT. ChatGPT is \ntailored to generate dialogic re sponses across diverse topics, while InstructGPT \nis designed to provide detailed answers to specific user promp ts. Both models, \nalthou gh s harin g a fo un dationa l architecture, have been fine-t uned usin g \ndifferent methodolo gies and da tasets to cater to their respecti ve purposes. \nGPT-4, thou gh specifics are no t fully disclosed, is recognized to have a larger \nscale than its predecessor GPT-3.5, indicating impro veme nts in model \nparameters and trai ning da ta s cope.(14–16) \nWhen ev aluated o n USM LE ques tions, GPT-4 outperfor med all other models \nwith acc uracy rates of 80-86%, 81-89% and 81-90% i n Step1, Step2 and Step3, \nrespectively. ChatGPT also had relatively good results, outperf orming GPT -3, \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nInstructGPT and GPT -3.5 with accuracy rates of 41-75%, 49- 61% and 55-68% in \nStep1, Step2 and Step3, respectively ( Tables 2-4 ).  \n \nQuestions wit h Media Elements  \nSome of t he USM LE questions u se media elements s uch as gra phs, ima ges, and \ncharts (14.4% and 13% o f q uestions in the Self -assess ment a n d Sample exa m, \naccordingly).(17) W hile Glison et al.(14) and K u ng et al.(18) exc luded these \nquestions beca use these eleme nts do not get passed to the m odel, Nori et \nal.(17) included the m in t he ev aluation a nd fo u nd th at w hile GPT-4 performs \nbest on text-o nly questio ns, it still performs well on questions  with media \nelements, wit h 68-79% accurac y, despite not being able to see the relevant \nimages. GPT-3.5 was fo und to have a 41-53% accuracy in t he media elements \ncontaini ng q uestions.  \n \nPrompting methods  \nDifferent promp ting met hods were used to test the perfor ma nce of the LL Ms. \nExamples of pro mpts are sho w n in Tabl e 5 .  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nKu ng et al. (18) tested three pr omptin g options. I n the Ope n- Ended form at, \nanswer ch oices were eliminate d, and variable lead-in in terrogative phrases were \nincorporated to mirror natural user queries. In the M ultiple-Choice Single \nAnswer wit ho ut Forced Justifica tion form at, USMLE q uestions were reproduced \nexactly. The third forma t, M ultiple Choice Single Answer wit h Forced \nJustification, req uired ChatGPT to provide rationales for each a nswer choice.  \nGlison et al.(14) used only mult iple choice prompting.   \nNori et al. (17) also used m ultiple choice questions but tested  both zero-shot \nand fe w-s hot prom pting. Zero- shot prompti ng requires a mod el to complete \ntasks witho ut a ny prior exampl es, utilizing only the k nowled g e gained fro m \npre-training. O n t he other ha n d, few-s hot pro mpting pro vides the model wit h \nlimited examples of t he task at  hand be fore execution. The m odel is expected \nto generalize from these exa m ples and accurately perform t h e task. \nOverall, the performa nce of t he models was slig htly af fected by the prompti ng \nmethod, with ope n-e nded promptin g sho win g better results than m ultiple \nchoice, and 5-s hot prompti ng giving better results t han zero- shot.  \n \nPerformance assess ment  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nAll papers assessed accuracy of the models in a ns wering USM L E questions. Two \nstudies also included qu alitativ e assessment o f the a ns wers and explanatio ns \nprovided by the LL Ms. Glison et  al.(14) assessed each ans wer f or logical \nreasoning (identification o f the logic in the ans wer selection), use of \ninform ation inter nal to the q ue stion, and use of in form ation e xternal to the \nquestion. Ch atGPT was reporte d to use infor matio n internal to  the question i n \n97% of questio ns. The use o f in formatio n external to the q uest ion was hig her in \ncorrect (90-93%) answers t han incorrect answers (48-63%). I n addition, every \nincorrect answer was labeled fo r the reason of t he error: logica l error (the \nresponse uses the pertine nt inf ormation b ut does not tra nslat e it to the correct \nanswer), i nfor mation error (did not identify t he key in formatio n needed) or \nstatistical error (an arithmetic mistake). Lo gical errors were the most com mo n, \nfou nd in 42% o f incorrect answ ers. (14) Kun g et al.(18) eval uat ed each outp ut \nfor concordance a nd insig ht, b y two ph ysician reviewers. A hi gh co ncordance of \n94.6% was fo und across ChatG PT’s answers. Cha tGPT produce d at least one \nsignifican t insig ht in 88.9% of questions. The de nsity of insi g ht contai ned \nwithin t he explan ations provide d by ChatGPT was si gnifica ntly higher in \nquestions a nswered accurately tha n in incorrect ans wers.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \n \nDISCUSS ION  \nThis review provides a compara tive analysis of LL Ms performa nce on USM LE \nquestions. W hile GPT-4 secured accuracy rates withi n the 80- 90% range, \nChatGPT demo nstrated com petent results, outpaci ng t he capa bilities of \nprevious models, GPT-3, Instr u ctGPT, and GPT-3.5. \nThe results of this review u nderscore that the mai n factor t hat  affects \nperforma nce, are the inherent capabilities of the LL M. Other f actors, including \nvarious prompti ng met hods, in corporation of questio ns that i nclude media, a nd \nvariability in question sets h ad secondary roles. This observation emp hasizes \nthe priority of adva ncing core A I model developmen t to ens ure better accuracy \nand utility in co mplex sectors li ke healthcare. As we consider t he integratio n of \nLL Ms into medical domains, it's  crucial to recognize that w hile optimizing \nexternal parameters can co ntribute to performa nce tweak s, th e most profo und \nimproveme nts lie in the refine ment o f the model's core capabilities.  \n \nThe proficiency of these LL Ms on a rigorous a nd fo u ndation al examinatio n suc h \nas the USM LE provides a compe lling indication of t hese models ’ potential role in \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nthe medical dom ain. GPT-4's a bility to achieve high accuracy levels signifies the \nprogression and mat uration o f AI's capabilities in deciphering complex medical \nknowledge. S uch a dva nceme nt s could be pivotal in assisting h ealthcare \nprofessionals, improvi ng diag n ostic accuracy, and facilitatin g medical \neducation. Ho wever, w hile the results are promising, it is crucial to approach \nthe integratio n of L LM s in medi cal practices with caution. The USMLE's textual \nnat ure mig ht not e ncomp ass th e entire scope of clinical expert ise, where skills \nlike patient interaction,(19) ha n ds-on procedures, a nd ethical considerations \nplay an importa nt role. \n \nPromptin g is considered to hold a significa nt role in shapin g t he performa nce \nof LL Ms whe n ans wering q ueries.(20,21) This review demo nstrates that t he way \nquestions are struct ured can su btly influe nce the responses ge nerated by these \nmodels. Notably, ope n-e nded promptin g has a slig ht edge ov er the standard \nmultiple-c hoice format. T his sug gests that LL Ms mi ght ha ve a nu anced \npreference whe n processing in f ormation based o n t he context they're provided. \nMoreover, the m arginally better  outcomes with 5-s hot pro mpti ng co mpared to \nzero-shot hi nt at the LL Ms' capacity to adjust and prod uce inf ormed ans wers \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nwhe n give n a fe w g uiding exa mples. Thou gh t hese dif ferences are subtle and \nmay not dram atically chan ge th e overall performance, they pro vide valuable \ninsights i nto the opti mization o f interactions with L LMs.  \n \nTwo sets of for mal USM LE ques tions were utilized in the studie s reviewed. Both \nGPT-3.5 and GPT-4 exhibited s uperior performance o n the Sa mple exam \ncompared to the self -assessme nt. While the Sa mple exam is p ublicly accessible, \nthe self-assess ment ca n o nly b e obtained throu g h purch ase. This raises the \npossibility that the hig her accu racy is derived from previous e ncou nters of t he \nmodels with q uestions from t h e Sample exam. Nori et al.(17) developed an \nalgorithm to detect pote ntial signs o f data leakage or memori zation effects. \nThis algorithm is desig ned to a scertain if specific data was likely incorporated \ninto a model's trainin g set. Not ably, this met hod did not detec t any evide nce of \ntraining dat a me morization in t he official USM LE datasets, w hi ch include bot h \nthe self-assess ment and t he Sa mple exam. Ho wever, it is important to note t hat \nwhile the algorit hm de mo nstrat es high precision, its recall remains \nundeter mined. Therefore, the e xtent to w hich t hese models might have bee n \nexposed to the question s durin g their training re mains i nconcl usive.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \n \nIn surveyi ng t he expansi ve liter ature on t he application of LL M s in healthcare, it \nis noteworth y that o nly t hree studies are directly comparable i n their use of \nformal q uestion sets. This obse rvation hi ghlig hts the pote ntial disparity in \nevaluatio n met hods a nd emp ha sizes the need for standardized  bench marks in \nassessing L LMs' medical proficiency. The USM LE stands as a primary metric for \nevaluati ng medical students a n d residents in the U.S., and its role as a \nbench mark for L LMs warrants c areful consideration. W hile it offers a structured \nand recognized platfor m, it is crucial to contemplate whet her such sta ndardized \ntests can f ully encapsulate t he depth and breadt h of LL Ms' cap abilities in \nmedical knowled ge. Lookin g ah ead there is a pressing need fo r research that \ndelves into alternative testin g mecha nisms, e nsuri ng a co mprehensive a nd \nmultidime nsion al evaluatio n of LL Ms in the realm o f he althcare . \n \nThis systematic review has sev eral limitations. First, the studi es reviewed \nprimarily focused on m ultiple choice questions, which, alt ho u gh a prevale nt \nformat i n the USM LE a nd an acc epted method for assessing me dical knowledge \namo ng st udents a nd clinicians,  may not f ully capture the co m plexity of real-\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nworld medical scenarios. Actual  clinical cases often present wit h complexities \nand sub tleties that mi ght not st rictly align with textbook descr iptions. Hence, \nwhe n conte mplatin g the applic ability of LL Ms in a clinical setting, it is vital to \nrecognize and accou nt for t his disparity. Secondly, our review intention ally \nexcluded studies that exa mi ned other datasets used for USML E preparation, \nsuch as MedQA. This decision was made to m aintai n consiste ncy in t he \ncomparison o f questio n sets. However, there is a wide array of  research that \nevaluates v arious other L LM s u sing diverse question sets t hat mimic USM LE \nquestions a nd measure medical proficiency. The exclusion of t hese studies \npotentially limits the compre hensiveness o f our insi ghts i nto t he capabilities of \nLL Ms in the medical domai n.  \n \nTo conclude, this syste matic review presents a detailed analysi s of the \nperforma nce capabilities of LL Ms on t he USM LE. The primary influe nce of t he \ncore model's capabilities over e xternal factors, as hig hlig hted in our fi ndin gs, \nunderscores the importa nce of contin ual adva nceme nts in AI model \ndevelopment t argeted to the m edical field. While the USMLE is a reputable \nmetric for assessing medical knowledge, it might not wholly c apture the diverse \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nchallenges of clinical practice. Fut ure research should strive f or a broader range \nof bench marks, tra nscendin g tr aditional testing syste ms, to f ul ly understa nd \nthe potential a nd bou ndaries of  LLMs i n healt hcare. \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nTable  1 . P ublications reporting on performa nce of LL Ms in US MLE q uestions.  \nStudy \n(Ref)  \nMont h, \nYear \nJournal  Model(s)  \ntested \nUSMLE \nstep(s) \nevaluate\nd \nQuestions \nsets used \nPrompting  \nMethodology  \nImages \nand \ngraphs  \nPerformance  \nmetric(s) used  \nKu ng et \nal.(18) \nFebruary \n2023  \nPLOS \ndigital \nhealth  \nChatGPT  Step \n1,2,3  \nSample \nexam*  \nOpen-\nended, \nMultiple \nchoice, \nMultiple \nchoice with \nforced \nExcluded Accuracy, concordance, \nand insig ht  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \njustification  \nGlison \net \nal.(14) \nAug ust \n2023  \nJMIR \nmedical \neducation  \nChatGPT, \nGPT-3, \nInstructGP\nT \nStep 1,2 Sample \nexam*  \nMultiple \nchoice \nExcluded Logical justification, \npresence of infor mation \ninternal to the q uestion, \npresence of infor mation \nexternal to the questio n  \nNori et \nal.(17) \nApril 2023  arXiv GPT-4, \nGPT-3.5 \nStep \n1,2,3  \nSample \nexam*, \nSelf-\nassessme\nnt** \nOpen ended \ntemplate: \nzero-shot, \n5-shot  \nIncluded  -\n  \n* Sourced from t he official USM LE website  \n** Sourced from t he official N M BE website\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nTable  2 . L L M’s performa nce (% )  on USMLE Step1  \nRef. Data Set Promptin g  GPT-3 InstructGP\nT \nGPT-\n3.5 \nChatGPT  GPT-4 \n(18) Sample \nexam  \nOpen-e nded  \nMultiple choice \nMultiple choice with \nforced justification  \n   75/45.4*  \n55.8/36.\n1* \n64.5/41.\n2* \n \n(14) Sample \nexam  \nMultiple choice 25.3  51.7   64.4   \n(17) Self-\nassessmen\nt \nZero-sho t  \n5-shot  \n  49.62  \n54.22  \n 83.46  \n85.21  \nSample \nexam  \nZero-sho t  \n5-shot  \n  51.26  \n52.10  \n 80.67  \n85.71  \n* With indeterminate respo nses included \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nTable  3 . L L M’s performa nce (% )  on USMLE Step2  \nRef. Data Set Promptin g  GPT-3 InstructGP\nT \nGPT-\n3.5  \nChatGPT  GPT-4 \n(18) Sample \nexam  \nOpen-e nded  \nMultiple choice \nMultiple choice with \nforced justification  \n   \n  \n \n61.5/54.1\n* \n59.1/56.9\n* \n52.4/49.5\n* \n \n(14) Sample \nexam  \nMultiple choice 18.6  52.9   57.8   \n(17) Self-\nassessmen\nt \nZero-sho t  \n5-shot  \n  48.12  \n52.75  \n 84.75  \n89.50  \nSample \nexam  \nZero-sho t  \n5-shot  \n  60.83  \n58.33  \n 81.67  \n83.33  \n* With indeterminate respo nses included \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \n \nTable  4 . L L M’s performa nce (% )  on USMLE Step3  \nRef. Data Set Promptin g  GPT-\n3.5 \n \nChatGPT  GPT-4 \n(18) Sample \nexam  \nOpen-e nded  \nMultiple choice \nMultiple choice with \nforced justification  \n 68.8/61.5\n* \n61.3/55.7\n* \n65.2/59.8\n* \n \n(17) Self-\nassessmen\nt \nZero-sho t  \n5-shot  \n50.00  \n53.41  \n 81.25  \n83.52  \nSample \nexam  \nZero-sho t  \n5-shot  \n58.39  \n64.96  \n 89.78  \n90.71  \n* With indeterminate respo nses included \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nTable  5 .  Pro mpt templates use d to assess USMLE questio ns. E lements betwee n \n<> are replaced with question -specific data. \nPromptin g  Example  \nOpen ended  <question witho ut m ultiple cho ice answers> \n“What would be the patie nt’s diagnosis based o n \nthe infor matio n provided?” \nMultiple choice <question with multiple choice answers>  \nMultiple choice \nwith f orced \njustification  \n<question with multiple choice answers>  \n“Explain your rationale for eac h  choice”/ “Why are \nthe other choices incorrect?” \nZero-sho t  “The followin g are m ultiple cho ice questions (wit h \nanswers) about medical kno wledge.” \n“**Question:**” <questio n wit h  multiple choice \nanswers>  \n“**Answer:**”  \n5-shot  “The followin g are m ultiple cho ice questions (wit h \nanswers) about medical kno wledge.” \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \n<few s hot exa mples> \n“**Question:**” <questio n wit h  multiple choice \nanswers>  \n“**Answer:**”  \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nFigure 1.  Flo w diagra m of t he search and incl usion process in the study. T he \nstudy follo wed the Preferred Reporting Items for Systematic Re view an d Meta -\nAnalyses (PRISMA ) g uidelines. USMLE = United States Medical Licensing \nExamin ation. L LMs = Large La n gua ge Models.  \n  \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \n \n \nREFER ENCES  \n1. About the US MLE | USM LE [In ternet]. [cited 2023 Aug 2]. Av ailable from: \nhttps://w w w.us mle.org/abo ut- usmle  \n2. Lomb ardi CV, Chidiac NT, Record BC, Laukk a JJ. USM LE step 1 and step 2 CK \nas indicators of resident performa nce. BMC Med Educ. 2023 Ju l \n31;23(1):543.  \n3. Ozair A, Bhat V, Detc hou D KE. The US Residency Selection Process After the \nUnited States Medical Licensin g  Examinatio n Step 1 Pass/Fail C han ge: \nOverview f or Applicants and Ed ucators. JMIR Med Ed uc. 2023 Jan \n6;9:e37069.   \n4. Chaud hry HJ, K atsu frakis PJ, Ta llia AF. The USMLE Step 1 Decision: An \nOpportunity for Medical Educat ion and Traini ng. JA MA. 2020 May \n26;323(20):2017–8.  \n5. Grunh ut J, Marques O, Wy att A TM. Needs, Challen ges, and Ap plications of \nArtificial Intelligence in Medical Education Curriculu m. J MIR Me d Educ. 2022 \nJu n 7;8(2):e35587.  \n6. Li R, Ku mar A, Chen JH. How C hatbots a nd Lar ge La ng uage M odel Artificial \nIntelligence Systems Will Resha pe Modern Medicine: Fo unt ain of Creativity or \nPandora’s B ox? JAMA I ntern Me d. 2023 Jun 1;183(6):596.  \n7. Sah ni NR, Carrus B. Artificial Int elligence in U.S. Health Care De livery. Drazen \nJM, Ko ha ne IS, Leon g TY, editors. N Engl J Med. 2023 Jul 27;389(4):348–58.  \n8. Jiang LY, Li u XC, Nejatia n N P, N asir-Moin M, Wan g D, Abidin A , et al. Health \nsystem-scale lan g uage models are all-purpose prediction engines. Nat ure. \n2023 Jul 13;619(7969):357–62.   \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \n9. Meskó B, Topol EJ. The imperat ive for regulatory oversig ht of l arge lang uage \nmodels (or generative AI) i n he althcare. Npj Digit Med. 2023 Jul 6;6(1):1–6.  \n10. Page MJ, McKe nzie JE, Bossuy t PM, Bo utron I, Hof f ma nn TC, M ulrow CD, et \nal. The PRISMA 2020 statement : an upda ted guideline for repo rting \nsystematic reviews. BMJ. 2021 Mar 29;372:n71.  \n11. Prepare for Your Exam | US MLE  [Internet]. [cited 2023 Aug 7]. Available \nfrom: h ttps://w w w.us mle.org/ prepare-your-exa m  \n12. Taking a Self -Assessme nt | N B ME [Internet ]. [cited 2023 Aug 7]. Available \nfrom: h ttps://w w w.nb me.org/e xaminees/self -assess ments  \n13. OpenAI Platfor m [Inter net]. [cit ed 2023 Aug 2]. Available from : \nhttps://platfor m.open ai.com  \n14. Gilson A, Safranek CW, Hua ng T, Socrates V, Chi L, Taylor RA, et al. How \nDoes ChatGPT Perform o n the United States Medical Licensin g  Examinatio n? \nThe Implications of Large La n g uage Models for Medical Educa tion and \nKno wledge Assessme nt. J MIR M ed Educ. 2023 Feb 8;9:e45 312.   \n15. Introducin g ChatGPT [I nternet].  [cited 2023 Aug 7]. Available from: \nhttps://open ai.com/blog/c hat gpt  \n16. OpenAI. GPT-4 Technical Report [Internet]. arXiv; 2023 [cited 2023 Sep 5]. \nAvailable from: ht tp://arxiv.org /abs/2303.08774  \n17. Nori H, King N, Mc Kin ney SM, C arigna n D, Horvitz E. Capabiliti es of gpt-4 o n \nmedical challenge problems. Ar Xiv Prepr ArXiv230313375. 20 23;  \n18. Ku ng TH, Cheat ha m M, Medenil la A, Sillos C, De  Leon L, Elepañ o C, et al. \nPerforma nce of ChatGPT o n US MLE: Pote ntial for AI-assisted medical \neducation using large la ng uage  models. PLOS Digit Healt h. 2023 \nFeb;2(2):e0000198.  \n19. Sorin V, Brin D, Barash Y, Ko ne n E, Charney A, Nadkar ni G, et al. Large \nLan gu age Models ( LL Ms) a nd E mpat hy – A S ystema tic Review [ Internet]. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint \nmedRxiv; 2023 [cited 2023 Aug 10]. p. 2023.08.07. 2329376 9. Available \nfrom: h ttps://w w w.medrxiv.or g/conte nt/10.1101/2023.08.0 7.23293769v1  \n20. Sing hal K, Azizi S, Tu T, Mahda vi SS, Wei J, Chu ng HW, et al. La rge lang uage \nmodels encode clinical knowle dge. Nat ure. 2023;1–9.  \n21. Wei J, Wang X, Sch u urma ns D, Bos ma M, Ichter B, Xia F, et al. Chain- of-\nThou ght Pro mptin g Elicits Reasoning i n Large La ng uage Models [Internet]. \narXiv; 2023 [cited 2023 Aug 7]. Available from: \nhttp://arxiv.org/ab s/2201.119 03 \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 7, 2023. ; https://doi.org/10.1101/2023.09.03.23294842doi: medRxiv preprint ",
  "topic": "United States Medical Licensing Examination",
  "concepts": [
    {
      "name": "United States Medical Licensing Examination",
      "score": 0.6857408881187439
    },
    {
      "name": "Expansive",
      "score": 0.6736708879470825
    },
    {
      "name": "Licensure",
      "score": 0.5431919693946838
    },
    {
      "name": "Medical education",
      "score": 0.45919787883758545
    },
    {
      "name": "English language",
      "score": 0.41840192675590515
    },
    {
      "name": "Health care",
      "score": 0.41673892736434937
    },
    {
      "name": "Medicine",
      "score": 0.39661669731140137
    },
    {
      "name": "Psychology",
      "score": 0.358151376247406
    },
    {
      "name": "Political science",
      "score": 0.27483657002449036
    },
    {
      "name": "Medical school",
      "score": 0.23353081941604614
    },
    {
      "name": "Mathematics education",
      "score": 0.13757532835006714
    },
    {
      "name": "Law",
      "score": 0.08779141306877136
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Compressive strength",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2799810450",
      "name": "Sheba Medical Center",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I16391192",
      "name": "Tel Aviv University",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I98704320",
      "name": "Icahn School of Medicine at Mount Sinai",
      "country": "US"
    }
  ]
}