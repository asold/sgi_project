{
    "title": "Prompting and Tuning: A Two-Stage Unsupervised Domain Adaptive Person Re-identification Method on Vision Transformer Backbone",
    "url": "https://openalex.org/W4313654972",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2097423409",
            "name": "Shengming (余盛明) Yu",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A3008871218",
            "name": "Zhaopeng Dou",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2132399716",
            "name": "Shengjin Wang",
            "affiliations": [
                "Tsinghua University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6784333009",
        "https://openalex.org/W2204750386",
        "https://openalex.org/W4213019189",
        "https://openalex.org/W3203755984",
        "https://openalex.org/W6790690058",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W6762509386",
        "https://openalex.org/W3035402405",
        "https://openalex.org/W4306955484",
        "https://openalex.org/W4205914576",
        "https://openalex.org/W2963854696",
        "https://openalex.org/W3108349366",
        "https://openalex.org/W6796526935",
        "https://openalex.org/W6794345597",
        "https://openalex.org/W6791705549",
        "https://openalex.org/W6797360341",
        "https://openalex.org/W2996988779",
        "https://openalex.org/W3009761962",
        "https://openalex.org/W2963893396",
        "https://openalex.org/W2927956561",
        "https://openalex.org/W3007744269",
        "https://openalex.org/W6756367734",
        "https://openalex.org/W4312480274",
        "https://openalex.org/W3198377975",
        "https://openalex.org/W6779510507",
        "https://openalex.org/W4312651322",
        "https://openalex.org/W3009238880",
        "https://openalex.org/W6811219898",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W3186033197",
        "https://openalex.org/W4214736485",
        "https://openalex.org/W3034417718",
        "https://openalex.org/W6790019176",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W3189423081",
        "https://openalex.org/W3166986030",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W3174505228",
        "https://openalex.org/W6786337132",
        "https://openalex.org/W6781000555",
        "https://openalex.org/W3035070480",
        "https://openalex.org/W2998792609",
        "https://openalex.org/W6791353385",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W2963047834",
        "https://openalex.org/W2963000559",
        "https://openalex.org/W3035014997",
        "https://openalex.org/W2962859295",
        "https://openalex.org/W6792695861",
        "https://openalex.org/W6793592898",
        "https://openalex.org/W6792496768",
        "https://openalex.org/W2988852559",
        "https://openalex.org/W2953214814",
        "https://openalex.org/W6771395016",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W4388543952",
        "https://openalex.org/W3166942762",
        "https://openalex.org/W3133696297",
        "https://openalex.org/W3106858249",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W3141972123",
        "https://openalex.org/W3034140121",
        "https://openalex.org/W3157528469",
        "https://openalex.org/W3125736290",
        "https://openalex.org/W2995852213",
        "https://openalex.org/W3139049060",
        "https://openalex.org/W3126337491",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2946310364",
        "https://openalex.org/W4313007769",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2904427185",
        "https://openalex.org/W3109976102",
        "https://openalex.org/W3137963805",
        "https://openalex.org/W3150226983"
    ],
    "abstract": "This paper explores the Vision Transformer (ViT) backbone for Unsupervised Domain Adaptive (UDA) person Re-Identification (Re-ID). While some recent studies have validated ViT for supervised Re-ID, no study has yet to use ViT for UDA Re-ID. We observe that the ViT structure provides a unique advantage for UDA Re-ID, i.e., it has a prompt (the learnable class token) at its bottom layer, that can be used to efficiently condition the deep model for the underlying domain. To utilize this advantage, we propose a novel two-stage UDA pipeline named Prompting And Tuning (PAT) which consists of a prompt learning stage and a subsequent fine-tuning stage. In the first stage, PAT roughly adapts the model from source to target domain by learning the prompts for two domains, while in the second stage, PAT fine-tunes the entire backbone for further adaption to increase the accuracy. Although these two stages both adopt the pseudo labels for training, we show that they have different data preferences. With these two preferences, prompt learning and fine-tuning integrated well with each other and jointly facilitated a competitive PAT method for UDA Re-ID.",
    "full_text": "TSINGHU\nA SCIENCE AND TECHNOLOGY\nI\nSSN ll1007-0214 15/16 pp799–810\nDOI: 1 0 . 2 6 5 9 9 / T S T . 2 0 2 2 . 9 0 1 0 0 4 4\nVolume 28, Number 4, August 2023\n\rC The\nauthor(s) 2023. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\nPrompting and Tuning: A Two-Stage Unsupervised Domain Adaptive\nPerson Re-identiﬁcation Method on Vision Transformer Backbone\nShengming Yu, Zhaopeng Dou, and Shengjin Wang\u0003\nAbstract: This paper explores the Vision Transformer (ViT) backbone for Unsupervised Domain Adaptive (UDA)\nperson Re-Identiﬁcation (Re-ID). While some recent studies have validated ViT for supervised Re-ID, no study has\nyet to use ViT for UDA Re-ID. We observe that the ViT structure provides a unique advantage for UDA Re-ID, i.e., it\nhas a prompt (the learnable class token) at its bottom layer, that can be used to efﬁciently condition the deep model\nfor the underlying domain. To utilize this advantage, we propose a novel two-stage UDA pipeline named Prompting\nAnd Tuning (PAT) which consists of a prompt learning stage and a subsequent ﬁne-tuning stage. In the ﬁrst stage,\nPAT roughly adapts the model from source to target domain by learning the prompts for two domains, while in the\nsecond stage, PAT ﬁne-tunes the entire backbone for further adaption to increase the accuracy. Although these two\nstages both adopt the pseudo labels for training, we show that they have different data preferences. With these two\npreferences, prompt learning and ﬁne-tuning integrated well with each other and jointly facilitated a competitive PAT\nmethod for UDA Re-ID.\nKey words: unsupervised domain adaption;person re-identiﬁcation; transformer; prompt learning; uncertainty\n1 Introduction\nPerson Re-Identiﬁcation (Re-ID) [1] is targeted at\nsearching the same person across multiple cameras with\nnon-overlapped views and distributed among different\nlocations such as airports, station, supermarkets, among\nothers. Apart from the well-investigated fully supervised\nRe-ID[2, 3], Unsupervised Domain Adaptive (UDA) Re-\nID is another topic that has drawn great interest from\nthe Re-ID research community. UDA Re-ID considers\nthat under realistic scenarios, there usually exists a\ndomain gap between the training and the testing data,\nwhich signiﬁcantly compromises the Re-ID accuracy. By\nadapting the already-learned deep model on the source\ndomain to the novel target domain without additional\n\u000fShengming Yu, Zhaopeng Dou, and Shengjin Wang are\nwith the Department of Electroic Engineering, Tsinghua\nUniversity, Beijing 100084, China. E-mail: yushengming1993\n0628@gmail.com; dcp19@mails.tsinghua.edu.cn; wgsgj@\ntsinghua.edu.cn.\n\u0003To whom correspondence should be addressed.\nManuscript received: 2022-07-20; revised: 2022-10-02;\naccepted: 2022-10-07\nannotations, the UDA Re-ID efﬁciently improve the\ncross-domain Re-ID.\nThis paper explores the Vision Transformer (ViT)\nas the backbone for UDA Re-ID. It is common\nsense that the backbone model is critical for Re-ID\nmethods based on deep learning, as well as for many\nother deep-learning-based methods\n[4, 5]. Recently, He\net al.[6] proposed the use of ViT as the backbone for\nsupervised Re-ID and demonstrated its superiority over\nConvolutional Neural Networks (CNN) backbones. Thus\nfar, however, no study has investigated the use of ViT\nfor UDA Re-ID, so far as we know. Given that ViT has\nalready reformed many aspects of computer vision, we\nthink it has a great potential in solving the UDA Re-ID\nproblem and deserves further investigation.\nIn this paper, we observe that the ViT structure\nprovides a unique advantage for UDA Re-ID, because it\nhas a prompt[7] (the learnable class token) at its bottom\nlayer, that can be used to efﬁciently condition the deep\nmodel to the underlying domain. For example, in Ref.\n[8], Ge et al. proposed Domain Adaptation via Prompt\nLearning (DAPL) to embed domain information into\n800 Tsinghua Science and Technology, August 2023, 28(4): 799–810\nprompts, a form of representation generated from natural\nlanguage, which is then used to perform classiﬁcation. In\ncontrast, we utilized the class token to condition the deep\nmodel for the source domain and the target domain, by\nfreezing the local backbone parameters and ﬁne-tuning\nthe class token parameters, which are denoted as the\nprompt tokens, thereby capturing more domain adaption\ninformation.\nTo utilize the above advantage, this paper proposes\na novel two-stage UDA method named Prompting\nand Tuning (PAT) for UDA Re-ID. PAT consists of\ntwo major adaption stages, i.e., a rough adaption\nthrough prompt learning and a more accurate adaption\nthrough sub-sequential ﬁne-tuning. These two stages\nboth adopt the popular pseudo-label pipeline, i.e.,\nassigning pseudo labels to the target-domain samples\nand then using these pseudo-labeled samples for adaptive\ntraining. Speciﬁcally, the prompt learning stage only\nlearns the prompt and freezes all the other backbone\nparameters, while the ﬁne-tuning stage trains the whole\nViT backbone as a whole. Given that the prompt learning\nstages only learn the prompt, which has very limited\nlearnable parameters, it has high resistance against\npseudo label noises, thus allows for the use of all the\ntarget domain samples (including some potentially noisy\nones). In contrast, the ﬁne-tuning stage learns all the\nparameters and is prone to overﬁtting the potential noises.\nTherefore, PAT ﬁlters out the potentially noisy samples\nand chooses the samples with high-conﬁdent pseudo\nlabels for the ﬁne-tuning stage.\nWith their complementary characteristics (e.g.,\nunderﬁtting v.s. overﬁtting, different preferences on\ndata), the prompt learning and ﬁne-tuning stages\nare well cooperated with each other and jointly\nfacilitate a competitive PAT method for UDA Re-\nID. In this paper we experiment on the four UDA\nRe-ID tasks, i.e., DukeMTMC-reID\n!Market1501,\nMarket1501!DukeMTMC-reID, DukeMTMC-reID!\nMSMT17, and Market1501!MSMT17. Through the\npre-training, prompt learning, and ﬁne-tuning stages, our\nmethod achieved signiﬁcant improvements on original\nCNN-based UDA state of the art\n[9, 10] and achieved\nslightly better performance of 2:4% Mean Average\nPrecision (mAP) and 0:8% R1 for DukeMTMC-reID\n! Market1501 and 1:7% mAP and 0:5% R 1 for\nMarket1501 !DukeMTMC-reID.\nOur contributions to the literature can be summarized\nas follows:\n(1) We propose a novel two-stage UDA Re-ID named\nPrompting And Tuning (PAT), which explores the unique\nadvantage of taking the vision Transformer (ViT) as\nthe backbone for UDA Re-ID. Furthermore, we apply\na novel two-stage training processing by taking the\nlearnable class token at the bottom layer as the prompt\nto indicate the domain.\n(2) We give two kinds of prompting stages for different\nUDA methods in order to train the source and target\nthe prompt. For UDA with pre-training, we trained the\ntwo prompts in the same place at different periods of\ntime. For UDA without pre-training, we train the two\nprompting in different places at the same time.\n(3) We apply different data preferences for the two\nstages and compare two ways of data ﬁltering: the frozen\nthreshold and uncertainty. In the ﬁrst prompt learning\nstage, PAT’s target is to absorb the target domain feature\nas much as possible so that it can tolerate more pseudo\nlabel noise. In the second ﬁne-tuning stage, PAT ﬁlters\nout those potentially noisy samples as it needs more\naccurate data. Our method outperforms other state-of-\nthe-art methods by a large margin for all common UDA\nRe-ID tasks.\n2 Related Work\n2.1 Vision transformer\nTransformer-based methods that can capture the\nglobal context information by self-attention and the\nspatial position relationship [11, 12], have been applied\nto a wide range of visual tasks thereby realizing\nsigniﬁcant performance improvements, including\nimage classiﬁcation, objection detection, and instance\nsegmentation, to name a few. Similar to BERT’s\nŒclass token, Transformer-based methods can obtain\na learnable embedding that is applied to the sequence\nof embedding image patches and deﬁned as a visual\ntoken, and typical research in this ﬁeld has been\nproposed in Ref. [\n13]. With a similar focus on local\ninformation, various approaches have been proposed,\nsuch as DeiT\n[14], TNT [15], Twins [16], RegionViT [17],\nand so on. Especially, Swin Transformers[18] achieved\nlocal attention within a window and introduced a\nshifted window partitioning approach for cross-window\nconnections. In addition, many researchers [19, 20] have\nfocused on the key components (self-attention layer,\nmulti-head attention, feed-forward network, etc.) of\nthe Transformer. At present, numerous studies have\nattempted to utilize Transformer-based approaches to\nShengming Yu et al.: Prompting and Tuning: A Two-Stage Unsupervised Domain Adaptive Person Re-identiﬁcation: : :801\nsolve Re-ID tasks[21, 22], such as TransReID[6].\n2.2 Unsupervised domain adaptatiion\nThere are three categories of UDA Re-ID methods,\nnamely style-transferring [23, 24], memory-based [25–27]\nand clustering-based[28–33]. The ﬁrst group of methods\npreserves the personal identities of source images that\ndiscard source domain styles or transfer to target domain\nstyles[23, 24]. The second category of memory-based\nmethods[25–27] stores features using a memory bank\nand then calculates the contrastive loss by mining\npositive and negative sample pairs from the features\nin the memory bank. The third category trains the\nmodels with unlabeled samples’ pseudo labels which are\nassigned through clustering. Among these, clustering-\nbased methods have been proven effective. SSG [28]\njointly extracts local and global features for clustering.\nMMT\n[29] establishes two models to generate pseudo\nlabels to supervise each other. The pseudo label noise\nwill damage the ﬁne-tuning process for clustering-\nbased UDA Re-ID methods. Some studies [29, 31, 34]\nhave focused on the noisy pseudo labels in clustering-\nbased UDA Re-ID methods. UNRN\n[31] takes the KL-\ndivergence between the output of the student and teacher\nmodel as the uncertainty and then uses the uncertainty\nto leverage the contribution of different samples for the\ntraining objective.\n2.3 Prompt learning\nPrompt learning ﬁrst introduced in Natural Language\nProcessing (NLP)[7] problems has been widely used in\nrecent years. It prepends language instructions to the\ninput text so that the pre-trained model can understand\nthe downstream task and increase its performance. GPT-\n3[35], whose prompts are chosen manually, transfers to\nfew-shot downstream tasks with powerful generalization.\nTo avoid inappropriate instructions that can mislead\nthe model, automatic prompt exploration has been\nproposed in several methods [36–38]. Prompt learning\nhas been applied to the vision-language model more\nrecently[8, 39–42]. For instance, CLIP [4] pre-trained on\n400 million image-text pairs achieved state-of-the-art\nperformance. Few methods[43] work on pure vision tasks\ndue to the huge difference between vision and language\narchitecture.\n3 Method\n3.1 Preliminaries\nUnsupervised domain adaption. The task of UDA\nis to adapt the trained model from a given source\ndomain DS D f.xSi ; ySi /ji D1; 2; : : : ;NSgand a\ntarget domain DT D fxTi ji D1; 2; : : : ;NTg. There\nare NS labeled samples such as xSi with label ySi\nbelonging to CS identities in the source domain, along\nwith NT unlabeled samples xTi in the target domain.\nAssigning pseudo labels to the target-domain samples\nis one of the most popular frameworks for UDA Re-ID.\nFollowing this, we use Density-Based Spatial Clustering\nof Applications with Noise (DBSCAN) clustering at\nthe beginning of every training epoch for target domain\nfeatures and obtain CT clusters on the target domain,\nresulting in CS training identities. We can obtain the\npseudo label Qyi of each unlabeled sample xTi in the\ntarget domain through clustering. Then, we learn and\nobtain the target domain model with the pseudo label\nand optimize it through several rounds of iteration.\nVision transformer . We use ViT encoder as the\nbackbone network to extract the features of every\nimage as shown in Fig. 1. Differently, for each input\nimage x 2RH\u0002W \u0002C containing H \u0002W pixels with C\nchannels, we use sliding windows and obtain a sequence\nof K ﬂattened 2D patches fxi\npji D1; 2; : : : ; Kg. For\nthe original image with resolution H \u0002W slid by patch\nsize P \u0002P and step size S which is set S < P, the\nresulting number of patches K can be calculated using\nEq. (1)\nK D\n\u0016H CS \u0000P\nS\n\u0017\n\u0002\n\u0016W CS \u0000P\nS\n\u0017\n(1)\nIn Eq. (1) b\u0001cis the ﬂoor function. For the smaller\nS, we will obtain the larger overlap of patches and the\nlonger sequence. Compared with splitting the image\ninto non-overlapping patches, this is a kind of data\naugmentation. Similar to ViT’s backbone, for the raw\npatch sequence fxi\npji D1; 2; : : : ; Kg, we give a linear\nprojection L.\u0001/ to map each patch from P \u0002P to D\ndimensions. Then, a learnable embedding xcls 2RD is\nprepended. After the position embedding P is added,\nwe obtain the input sequence z0 of Transformer layers,\nwhich is formed using Eq. (2) below:\nz0 DŒxclsIL.x1\np/IL.x2\np/I: : :IL.xK\np / CP (2)\nThe input sequence z0 and the position embedding P\nhave the same size, such that z0; P2R.KC1/\u0002D. For\nthe learnable P , we introduce a bilinear 2D interpolation.\nFor the Transformer encoder with depth L, the input\nand output sequences of the l-th layer can be denoted\nas zl\u00001 and zl, respectively. Each Transformer layer\ncontains a Multiheaded Self-Attention (MSA) block and\n802 Tsinghua Science and Technology, August 2023, 28(4): 799–810\nFig. 1 Pipeline of Transformer-based UDA baseline. We devide the image into patch sequences and produce a linear projection.\nThen we concatenate a class token into it, and add a position embedding to obtain the input sequence. Next, we feed it to the\nTransformer block and choose the class token of the ﬁnal layer output as the image feature, which is used to generate UDA loss\nand perform clustering at the beginning of every epoch.\na Multi-Layer Perceptron (MLP) block both followed by\na Layernorm (LN) layer and uses residual connections.\nThus, for l D1; 2; : : : ; L, the l-th Transformer layer\noperation obtained as in Eq. (3) below.\nz0\nl DMSA.LN.zl\u00001// Czl\u00001/\nzl DMLP.LN.z l// Cz0\nl\n(3)\nFor the ﬁnal layer output, we take the ﬁnal layer class\ntoken as the whole image representation f DzLŒ0.\nThe output f is used as the feature for our UDA\ntask. First, we perform supervised training in the source\ndomain. For each UDA epoch, we perform clustering\nthrough f . Then we assign the pseudo labels to the target\ndataset and train it as supervised tasks. This serves as\nour baseline.\n3.2 Two stages pseudo label training\nWe note that a unique parameter class token does\nnot appear in the CNN backbone. In any Transformer\nsequence zl, zlŒ0 can represent the global feature,\nwhich is why we take the zLŒ0 as the output of the\nwhole Transformer. In the input sequence, according\nto Eq. (2) z0Œ0 Dxcls CP Œ0where xcls and P are\nboth learnable parameters. Then we ﬁnd that the class\ntoken plays the same role as the linear projection of an\nimage patch. This means that the trained class token\ndoes the work of adding one more image patch to\ncalculate self-attention, and this patch shared by every\ninput sample does not depend on speciﬁc input image\ncontent. Therefore the class token, which plays the role\nof public patch projection, contains the information of\nall image patches for training, while other parameters\nin the Transformer layers control extracting the feature\nfrom image patches. In supervised Transformer tasks,\nthe token is the representation of all training images,\nwhich is also prior knowledge of the training dataset.\nFor our unsupervised domain adaption tasks, the key is\nto ﬁnd and mitigate the domain gap between the source\nand target domains. To do so, we simply replace the\nconvolutional neural network backbone with the state-of-\nthe-art supervised model Transformer as our baseline for\nUDA. Then we ﬁnd that the learnable z0 can represent\nthe domain feature. Therefore, we can get our two\nstages pseudo label training after the source domain\npre-training by taking the class token z0 as prompt.\n3.2.1 First prompting stage\nWe freeze the other parameters of the source domain\ntrained model, except the prompt xcls during pseudo\nlabel training.\nGiven that the Transformer backbone has got better\nperformance than CNN on supervised tasks, the pre-\ntrained model’s ability to extract image features is\nsatisfying. After the pre-training, the prompt token\ncan indicate the source domain. The target of the ﬁrst\nstage is to learn the data distribution of the target\ndataset. Therefore we only unfreeze\nxcls (equivalent\nto only unfreezing z0Œ0) for gradient descent and freeze\nShengming Yu et al.: Prompting and Tuning: A Two-Stage Unsupervised Domain Adaptive Person Re-identiﬁcation: : :803\nthe other parameters. The prompt token will transfer\nfrom the source domain indicator to the target domain\nindicator during the training at this stage.\nIn the ﬁrst stage, the key is to absorb the target\ndomain feature as much as possible. Upon freezing all\nthe other parameters, the learnable parameters become\nvery limited. Thus, the model will not learned to overﬁt\nand can tolerate high pseudo label noise.\nSome of the main pseudo label based UDA methods\nchoose to train by mixing the source and the target\ndomains (e.g., SPCL[44]) without pre-training. For these\nkinds of methods, the model is adapted to both domains\nsimultaneously. We apply our ViT prompting stage by\nassigning two different prompts for the source domain\nand target domain samples. In this situation, we train\ntwo prompts simultaneously instead of transferring one\ntoken during two periods of time. Both are shown in the\nFig. 2.\n3.2.2 Second tuning stage\nWe unfreeze all the parameters for the gradient descent.\nAfter the former stage, the prompt xcls is adapted\nto the target domain. The target of the second stage\nis to improve the accuracy by ﬁne-tuning the model.\nTherefore, the whole model is unfrozen. In the second\nstage, the model is sensitive to pseudo label noise, and\nmore accurate data should be fed.\nThe naive way to feed more accurate data is manual.\nAfter DBSCAN clustering, we throw out all the\nunclustered outliers. For the clustered samples, we throw\nout all the samples that are too far away from the\ncluster centroid. To do this, we choose a ratio threshold\nR 2.0; 1/, after which the top R ratio far samples are\ndropped.\n3.3 Uncertainty\nNotably, when we ﬁnish the pre-training, we obtain the\nsource domain prompt xS\ncls. After the ﬁrst prompting\nstage, the prompt is learned as the target domain\nindicator xT\ncls without changing any of the other\nparameters. We can utilize the difference between the\nsame models with different domain indicator prompts to\nmeasure the reliability of the pseudo label for the second\nﬁn-tuning stage. Speciﬁcally, in the second tuning stage,\nthe prompt of the Transformer encoder is xsec\ncls , while\nthose of other parameters are Pa . Then, for the input\ntarget domain image patch sequence qTip Dfxi\nTip ; iD\n1; 2; : : : ; Kg, the pseudo label uncertainty is calculated\nfollowing Eq. (4) below.\nf S\nTi DT r.qTip ; xS\ncls; Pa/; fsec\nTi DT r.qTip ; xsec\ncls ; Pa/;\nuTip DDKL.f sec\nTi kf S\nTi/ D\nCSCCT\nX\nkD1\nf sec\nTi logf sec\nTi\nf S\nTi\n(4)\nIn Eq. (4) T r.input; xcls; Pa/ is the Transformer\nencoder. In particular, we use the reliability of the image\npatch sequence fxi\nTip ji D1; 2; : : : ; Kgas shown in Eq.\n(5)\nW.f sec\nTi ; fS\nTi/ Dexp.\u0000uTip / (5)\nFig. 2 Prompting stage. As the prompt token can indicate the domain, this stage is about training two prompts for the source\nand target domains, respectively. The left sub-ﬁgure (a) shows the prompting stage for state of the art with pre-training. We\ntrain the prompt of the two domains in the same place for different periods. At the ﬁrst of this stage, the token is a pre-trained\nsource domain prompt. We freeze all other parameters in the model and only train the prompt by feeding the target domain\nsamples. This makes the domain indicated by the prompt transfer from source to target. The right sub-ﬁgure (b) shows the\nprompting stage for state of the art without pre-training which uses source and target domain data simultaneously. For this\nsituation, we train two prompts at the same time for different places. We initialize two prompts for the source and target domain\ndata. The other part of the prompting stage is the same as in the vanilla Transformer training shown in Fig 1.\n804 Tsinghua Science and Technology, August 2023, 28(4): 799–810\nThe image patch sequence fxi\nSip ji D 1; 2; : : : ; Kg\nfrom the source domain is considered reliable, so Eq. (6)\nis deﬁned as\nW.f sec\nSi ; fS\nSi / D1 (6)\nImages from the source dataset have ground truth\nlabels. We only calculate the target dataset’s reliability,\nand take it as the weight of loss function to leverage the\ntarget domain data. The whole PAT algorithm is shown\nin Algorithm 1.\n4 Experiment\n4.1 Implementation details\nWe implement our model with PyTorch and train it\non four V100 GPUs. Similar to TransREID, SGD is\nadopted as the optimizer for the models with the weight\ndecay parameter of 1 \u000210\u00004. In each mini-batch, we\nrandomly sample 64 images for the target dataset and\nresize all images to the standard pixel size of 256 \u0002128.\nThen we use the sliding window for stride size Œ16; 16\nto devide the sample image into patch sequence. We\nutilize DBSCAN on features extracted by the backbone\nto create pseudo labels at the beginning of each epoch.\nThe minimum number of neighbors in DBSCAN is set\nto 4.\n4.2 Datasets and protocols\nThe proposed PAT method is evaluated on the\nfour UDA Re-ID tasks, namely, i.e. DukeMTMC-\nreID!Market1501, Market1501 !DukeMTMC-\nreID, DukeMTMC-reID !MSMT17, and\nMarket1501!MSMT17. The Market1501 dataset is\nproposed in Ref. [ 57]. It has 12 936training images,\n3368 query images, and 15 913gallery images. The\nDukeMTMC-reID[58] dataset contains 16 522training\nimages, 2228 query images, and 17 661 gallery\nimages. Meanwhile, to the best of our knowledge,\nthe MSMT17\n[24] is the most challenging dataset. It\ncontains multiple scenes and multiple time person\nimages, and has\n126 441images of 4101 identities\nin MSMT17. The evaluation protocols adopted are\nmAP and Cumulated Matching Characteristic (CMC)\nrank-1/5/10 (R1/R5/R10).\n4.3 Comparison with the state-of-the-art methods\nThe proposed PAT is compared with state-of-the-art\nmethods on the four tasksm and the result are shown\nin Table 1. The IDM\n[56] which adds an intermediate\ndomain module in ResNet-50 and IBN-ResNet-50 has\nthe best performance. Our method follows this pipeline,\nAlgorithm 1 Prompting and Tuning\nInput: Source domain labeled dataset DS D\nf.xSi ; ySi /ji D1; 2; : : : ; NSgand Target domain unlabeled\ndataset DT Dfx Ti ji D1; 2; : : : ; NTg\nstate-of-the-art CNN backbone UDA method\nInitialize the backbone Transformer encoderT rwith ImageNet-\npre-trained model\n(1) Pre-training on the source domain Supervised training by\nthe loss function of the state-of-the-art UDA method with\nTransformer replaced\nwhile pre \u0000train epoch do\nwhile iter do\nfS DT r:forward.input batch/\nL DLoss.fS; y/\nT r:backward./\nend while\nend while\nGet xS\ncls from T r\n(2) Prompt adaption training\n(a) For state-of-the-art with pre-training\nFreeze all parameters except xcls when tarining the target\ndomain samples\nwhile adaption epoch do\nAssign pseudo labels Qy\nwhile iter do\nfT DT r:forward.input batch/\nL DLoss.fT; Qy/\nT r:backward./\nend while\nend while\n(b) For state-of-the-art without pre-training\nCreate the source prompt for the source doamin samples and\nthe target prompt for the target domain samples\nwhile adaption\nepoch do\nAssign pseudo labels Qy\nwhile iter do\nfT DT r:forward.input batch/\nL DLoss.fT ; Qy/\nT r:backward./\nend while\nend while\n(3) Fine-tuning\nUnfreeze all parameters with the target domain prompt, Filter\nsamples by uncertainty\nwhile ﬁne-tune epoch do\nAssign pseudo labels Qy\nwhile iter do\nf S\nTi DT r.qTip ; xS\ncls; Pa/; fsec\nTi DT r.qTip ; xsec\ncls ; Pa/\nuTip DDKL.f sec\nTi kf S\nTi/ D\nCSCCT\nP\nkD1\nf sec\nTi log f sec\nTi\nf S\nTi\nW Dexp.\u0000uTip / or 1 for source domain sample\nL DW \u0002Loss.fT; Qy/\nT r:backward./\nend while\nend while\nShengming Yu et al.: Prompting and Tuning: A Two-Stage Unsupervised Domain Adaptive Person Re-identiﬁcation: : :805\nTable 1 Performance comparison with the state-of-the-art methods for UDA Re-ID on the datasets of DukeMTMC-reID,\nMarket1501, and MSMT17. The best results are marked by bold text.\nMethod DukeMTMC-reID!Market1501 Market1501!DukeMTMC-reID\nmAP R1 R5 R10 mAP R1 R5 R10\nATNet[45] (CVPR’19) 25.6 55.7 73.2 79.4 24.9 45.1 59.5 64.2\nSPGAN+LMP[23] (CVPR’18) 26.7 57.7 75.8 82.4 26.2 46.4 62.3 68.0\nCFSM[46] (AAAI’19) 28.3 61.2 – – 27.3 49.8 – –\nBUC[47] (AAAI’19) 38.3 66.2 79.6 84.5 27.5 47.4 62.6 68.4\nECN[25] (CVPR’19) 43.0 75.1 87.6 91.6 40.4 63.3 75.8 80.4\nUCDA[48] (ICCV’19) 30.9 60.4 – – 31.0 47.7 – –\nPDA-Net[49] (ICCV’19) 47.6 75.2 86.3 90.2 45.1 63.2 77.0 82.5\nPCB-PAST[50] (ICCV’19) 54.6 78.4 – – 54.3 72.4 – –\nSSG[28] (ICCV’19) 58.3 80.0 90.0 92.4 53.4 73.0 80.6 83.2\nACT[51] (AAAI’20) 60.6 80.5 – – 54.5 72.4 – –\nMPLP[52] (CVPR’20) 60.4 84.4 92.8 95.0 51.4 72.4 82.9 85.0\nDAAM[53] (AAAI’20) 67.8 86.4 – – 63.9 77.6 – –\nMMT[29] (ICLR’20) 71.2 87.7 94.9 96.9 65.1 78.0 88.8 92.5\nNRMT[30] (ECCV’20) 71.7 87.8 94.6 96.5 62.2 77.8 86.9 89.5\nB-SNR+GDS-H[54] (ECCV’20) 72.5 89.3 – – 59.7 76.7 – –\nSpCL[44] (NeurIPS’20) 76.7 90.3 96.2 97.7 68.8 82.9 90.1 92.5\nUNRN[31] (AAAI’21) 78.1 91.9 96.1 97.8 69.1 82.0 90.7 93.5\nGLT[34] (CVPR’21) 79.5 92.2 96.5 97.8 69.2 82.0 90.2 92.8\nAWB[55] (TIP’22) 81.0 93.5 97.4 98.3 70.9 83.8 92.3 94.0\nIDM[56] (ICCV’21) 82.8 93.2 97.5 98.1 70.5 83.6 91.5 93.7\nIDM (IBN-ResNet-50)[56] 83.9 93.4 – – 71.1 83.9 – –\nPAT (Ours) 86.3 94.2 97.8 98.5 72.8 84.4 92.5 94.0\nMethod Marke1501!MSMT17 DukeMTMC-reID!MSMT17\nmAP R1 R5 R10 mAP R1 R5 R10\nATNet[45] (CVPR’19) – – – – – – – –\nSPGAN+LMP[23] (CVPR’18) – – – – – – – –\nCFSM[46] (AAAI’19) – – – – – – – –\nBUC[47] (AAAI’19) – – – – – – – –\nECN[25] (CVPR’19) 8.5 25.3 36.3 42.1 10.2 30.2 41.5 46.8\nUCDA[48] (ICCV’19) – – – – – – – –\nPDA-Net[49] (ICCV’19) – – – – – – – –\nPCB-PAST[50] (ICCV’19) – – – – – – – –\nSSG[28] (ICCV’19) 13.2 31.6 – 49.6 13.3 32.2 – 51.2\nACT[51] (AAAI’20) – – – – – – – –\nMPLP[52] (CVPR’20) – – – – – – – –\nDAAM[53] (AAAI’20) 20.8 44.5 – – 21.6 46.7 – –\nMMT[29] (ICLR’20) 22.9 49.2 63.1 68.8 23.3 50.1 63.9 69.8\nNRMT[30] (ECCV’20) 19.8 43.7 56.5 62.2 20.6 45.2 57.8 63.3\nB-SNR+GDS-H[54] (ECCV’20) – – – – – – – –\nSpCL[44] (NeurIPS’20) – – – – – – – –\nUNRN[31] (AAAI’21) 25.3 52.4 64.7 69.7 26.2 54.9 67.3 70.6\nGLT[34] (CVPR’21) 26.5 56.6 67.5 72.0 27.7 59.5 70.1 74.2\nAWB[55] (TIP’22) 29.0 57.3 70.7 75.9 29.5 61.0 73.5 77.9\nIDM[56] (ICCV’21) 33.5 61.3 73.9 78.4 35.4 63.6 75.5 80.2\nIDM (IBN-ResNet-50)[56] (ICCV’21) 39.3 67.2 – – 40.8 69.6 – –\nPAT (Ours) 41.5 67.8 74.5 78.9 45.3 70.1 80.0 83.6\nand achieves slightly better performance, in which 2:4%\nmAP and 0:8% R1 DukeMTMC-reID !Market1501\nand 1:7% mAP and 0:5% R 1 for Market1501 !\nDukeMTMC-reID. Other cluster-based methods, such\nas SSG[28], also show promising results. We attribute\nthe superiority of the proposed PAT to two aspects. On\n806 Tsinghua Science and Technology, August 2023, 28(4): 799–810\nthe one hand, the stronger backbone, ViT, extracts more\ndiscriminative features than commonly-used CNNs. On\nthe other hand, the proposed PAT training method based\non ViT learns a better prompt, which is critical for person\nRe-ID.\n4.4 Consistent improvements in different state-of-\nthe-art methods\nApart from using IDM[56], we also select Six other CNN-\nbased state-of-the-art methods to demonstrate that PAT\nbrings general improvement, as illustrated in Table 2.\nThe selected state-of-the-art methods are ACT[51], PCB-\nPAST[50], SSG [28], MMT [29], SpCL [44], and AWB [55].\nAs shown in Table 2, the consistent improvement\ndemonstrates the effectiveness and generalizability of\nthe proposed PAT method.\n4.5 Ablation studies\nThis section carries extensive experiments to prove the\neffectiveness of each component in the proposed PAT.\nWe select two tasks, namely, DukeMTMC-reID !\nMarket1501 and Market1501 !DukeMTMC-reID. The\nexperimental results are shown in the following.\n4.5.1 Effectiveness of the vanilla Transformer\nThis section investigates the effectiveness of the vanilla\nTransformer, which we use to substitute for the IBN-\nResNet-50 used in IDM [56]. They are denoted as “V-\nTransformer” and “the IBN-ResNet-50” in Table 3.\nCompared with the commonly-used CNN, this simple\nsubstitution generates improvements of 0:8% mAP\nand 0:3% R 1 for DukeMTMC-reID !Market1501\nand 0:9% mAP and 0:2% R 1 for Market1501 !\nDukeMTMC-reID. With a stronger backbone, the\nTransformer extracts more discriminative features and\nproduces improved performance on UDA Re-ID. This\nphenomenon is also observed in TransReID[6].\n4.5.2 Effectiveness of two-stage training\nThis section compares the one-stage training with\nthe two-stage training by training the prompt and\nall other parameters of the network together. The\nexperimental results are denoted as “One-stage” and\n“Two-stage” in Table 3, respectively. Compared with\nthe one-stage training, we observe the following\nperformance improvements: 0:5% mAP and0:1% R1 for\nDukeMTMC-reID !Market1501 and 0:3% mAP and\n0:1% for Market1501 !DukeMTMC-reID R1. Such\nimprovements prove the effectiveness of the sole training\nprocess of the prompt, which can disentangle domain-\nspeciﬁc information to help discover and mitigate the\ndomain gap. In the two-stage training, the prompt is\ntrained to be an indicator of one domain, thus making it\ncritical for UDA tasks.\n4.5.3 Comparison between data preference and\nuncertainty\nThis section compares three choices for selecting\ntarget data for training. The baseline choice uses\nall the target data without discarding any images,\nsuch as outliers. Two advanced choices are data\npreference and uncertainty, respectively, and the ratio\nthreshold\nR is set as 0:3. Compared with the process\nof keeping all the images in the target domain, our\nproposed data preference generates improvements\nof 0:4% mAP and 0:2% R 1 for DukeMTMC-reID\n! Market1501 and 0:2% mAP and 0:1% R 1 for\nMarket1501 !DukeMTMC-reID. The data preference\nselects reliable data to train the network and reduces the\ninterference from noisy labels. In fact, the discarding\nprocess of data preference is not an optimal choice\nas it brings information loss to the target domain.\nInstead of discarding, uncertainty assigns different\nweights to images with varying reliabilities. Therefore,\nfurther improvements of 0:7% mAP and 0:3% R 1\nfor DukeMTMC-reID !Market1501 and 0:3% mAP\nand 0:1% R1 for Market1501 !DukeMTMC-reID is\nbrought by the uncertainty. In conclusion, suppressing\nthe noise while keeping the information intact plays an\nimportant role in UDA Re-ID.\n4.5.4 Training time consuming analysis\nThis section compares the training time consuming for\neach experiment. The CNN-based method takes about\nhalf a day for training. Then we replace the backbone\nwith ViT, which contains more parameters. The training\ntime increases to about 4 days. After that, we apply our\ntwo-stage training procedure. In the prompting stage,\nmost of the model parameters are frozen, so the training\ntime increases to 5 days. Then we ﬁlter the data in two\nstages. It takes about the same time when we discard the\ndata through a given ratio. Finally, we add an uncertainty\nblock to ﬁlter the data. This step gives each sample an\nuncertainty score as the weight factor to soft discard data.\nOur PAT takes5:5 days to complete the training, which\nis slightly longer than the vanilla ViT baseline.\n5 Conclusion\nThis paper proposes a two-stage unsupervised domain\nadaption method named Prompting And Tuning (PAT)\nfor person re-identiﬁcation. PAT is based on the ViT\nShengming Yu et al.: Prompting and Tuning: A Two-Stage Unsupervised Domain Adaptive Person Re-identiﬁcation: : :807\nTable 2 Consistent improvement in different state-of-the-art methods. We apply our PAT on different CNN-based UDA\nmethods, some of which contain/do not contain pre-training. The consistent improvement in these methods proves the\neffectiveness and generalizability of the proposed PAT.\nMethod DukeMTMC-reID!Market1501 Market1501!DukeMTMC-reID\nOriginal-mAP mAP Original-mAP mAP\nPCB-PAST[50] 54.6 65.9 54.3 63.6\nSSG[28] 58.3 68.9 53.4 63.4\nACT[51] 60.6 70.2 54.5 60.1\nMMT[29] 71.2 77.5 65.1 68.0\nSpCL[44] 76.7 80.1 68.8 70.2\nAWB[55] 81.0 83.6 70.9 71.6\nIDM (IBN-ResNet-50)[56] 83.9 86.3 71.1 72.8\nTable 3 Ablation studies on the effectiveness of components in our proposed PAT on Market1501 and Duke. IBN-ResNet-50:\nIDM. V-Transformer: Vanilla Transformer-substituted. One-stage: One stage training. Two-stage: Prompt learning in the\nﬁrst stage, model ﬁne-tuning in the second stage. Data-discarding: Discarding data through ratio R in the ﬁne-tuning stage\nUncertainty: Discarding data through uncertainty.\nMethod Training time (day) DukeMTMC-reID!Market1501 Market1501!DukeMTMC-reID\nmAP R1 mAP R1\nIBN-ResNet-50 0.5 83.9 93.4 71.1 83.9\nV-Transformer 4 84.7 93.7 72.0 84.1\nOne-stage 4 84.7 93.7 72.0 84.1\ntwo-stage 5 85.2 93.8 72.3 84.2\nData-discarding 5 85.6 93.9 72.5 84.3\nUncertainty 5.5 86.3 94.2 72.8 84.4\nbackbone and utilizes the latter’s unique advantage of\nhaving a prompt token that can efﬁciently condition the\nmodel for different domains.\nFirst, PAT roughly adapts the deep model to the target\ndomain using a prompt learning approach, which has\nhigh resistance against pseudo label noises but is prone\nto underﬁtting. After the rough prompt-based adaption,\nPAT further selects partial target samples with high-\nconﬁdence pseudo labels to ﬁne-tune the whole deep\nmodel. Therefore, the prompting and ﬁne-tuning steps\njointly adopt the deep model from source to target in a\ncoarse to ﬁne manner. Experimental results conﬁrm\nthat PAT allows ViT-backbones to compete with the\nCNN backbones for UDA re-ID. The achieved results\noutperform the state-of-the-art UDA methods.\nAcknowledgment\nThis work was supported by the National Key Research\nand Development Program of China in the 13th Five-Year\n(No. 2016YFB0801301) and in the 14th Five-Year (Nos.\n2021YFFO602103, 2021YFF0602102, and 20210Y1702).\nReferences\n[1] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian,\nScalable person re-identiﬁcation: A benchmark, in Proc.\n2015 IEEE Int. Conf. on Computer Vision, Santiago, Chile,\n2015, pp. 1116–1124.\n[2] M. Ye, J. Shen, G. Lin, T. Xiang, L. Shao, and S. C. H. Hoi,\nDeep learning for person re-identiﬁcation: A survey and\noutlook, IEEE Trans. Pattern Anal. Mach. Intell., vol. 44,\nno. 6, pp. 2872-2893, 2022.\n[3] L. Zheng, Y . Yang, and A. G. Hauptmann, Person re-\nidentiﬁcation: Past, present and future, arXiv preprint arXiv:\n1610.02984, 2016.\n[4] J. Chen, X. Wang, Z. Guo, X. Zhang, and Sun J, Dynamic\nregion-aware convolution, in Proc. of 2021 IEEE/CVF Conf.\non Computer Vision and Pattern Recognition, Nashville, TN,\nUSA, 2021, pp. 8060–8069.\n[5] Z. Zhang, C. Lan, W. Zeng, and Z. Chen, Multi-granularity\nreference-aided attentive feature aggregation for video-based\nperson re-identiﬁcation, in Proc. of 2020 IEEE/CVF Conf.\non Computer Vision and Pattern Recognition, Seattle, W A,\nUSA, 2020, pp. 10404–10413.\n[6] S. He, H. Luo, P. Wang, F. Wang, H. Li, and W. Jiang,\nTransReID: Transformer-based object re-identiﬁcation, in\nProc. of 2021 IEEE/CVF Int. Conf. on Computer Vision,\nMontreal, Canada, 2021, pp. 14993–15002.\n[7] F. Petroni, T. Rockt¨aschel, S. Riedel, P. Lewis, A. Bakhtin,\nY . Wu, and A. H. Miller, Language models as knowledge\nbases? in Proc. 2019 Conf. on Empirical Methods in Natural\nLanguage Processing and the 9th Int. Joint Conf. on Natural\nLanguage Processing, Hong Kong, China, 2019, pp. 2463–\n2473.\n[8] C. Ge, R. Huang, M. Xie, Z. Lai, S. Song, S. Li, and G.\n808 Tsinghua Science and Technology, August 2023, 28(4): 799–810\nHuang, Domain adaptation via prompt learning, arXiv\npreprint arXiv: 2202.06687, 2022.\n[9] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian,\nScalable person re-identiﬁcation: A benchmark, in Proc.\n2015 IEEE Int. Conf. on Computer Vision, Santiago, Chile,\n2015, pp. 1116–1124.\n[10] S. Karanam, M. Gou, Z. Wu, A. Rates-Borras, O. Camps,\nand R. J. Radke, A systematic evaluation and benchmark\nfor person re-identiﬁcation: Features, metrics, and datasets,\nIEEE Trans. Pattern Anal. Mach. Intell., vol. 41, no. 3, pp.\n523-536, 2019.\n[11] T. Lin, Y . Wang, X. Liu, and X. Qiu, A survey of\ntransformers, AI Open, vol. 3, pp. 111–132, 2022.\n[12] K. Han, Y . Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y .\nTang, A. Xiao, C. Xu, Y . Xu, et al., A survey on vision\ntransformer, IEEE Trans. Pattern Anal. Mach. Intell., doi:\n10.1109/TPAMI.2022.3152247.\n[13] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G.\nHeigold, S. Gelly, et al., An image is worth 16x16 words:\nTransformers for image recognition at scale, arXiv preprint\narXiv: 2010.11929, 2020.\n[14] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles,\nand H. Jegou, Training data-efﬁcient image transformers &\ndistillation through attention, in Proc. 38th Int. Conf. on\nMachine Learning, 2021, pp. 10347–10357.\n[15] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang,\nTransformer in transformer, in Proc. 35thConf. on Neural\nInformation Processing Systems, 2021, p. 34.\n[16] X. Chu, Z. Tian, Y . Wang, B. Zhang, H. Ren, X. Wei, H.\nXia, and C. Shen, Twins: Revisiting the design of spatial\nattention in vision transformers. in Proc. 35th Conf. on\nNeural Information Processing Systems, 2021, pp. 9355–\n9366.\n[17] C. F. Chen, R. Panda, and Q. Fan, RegionViT: Regional-\nto-local attention for vision transformers, presented at the\nTenth Int. Conf. on Learning Representations, 2021.\n[18] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D.\nChen, and B. Guo, CSWIN transformer: A general vision\ntransformer backbone with cross-shaped windows. in Proc.\n2022 IEEE/CVF Conf. on Computer Vision and Pattern\nRecognition, New Orleans, LA, USA, 2021, pp. 12114–\n12124.\n[19] D. Zhou, B. Kang, X. Jin, L. Yang, X. Lian, Z. Jiang, Q. Hou,\nand J. Feng, DeepViT: Towards deeper vision transformer,\narXiv preprint arXiv: 2103.11886, 2021.\n[20] X. Chu, Z. Tian, B. Zhang, X. Wang, X. Wei, H. Xia,\nand C. Shen, Conditional positional encodings for vision\ntransformers, arXiv preprint arXiv: 2102.10882, 2021.\n[21] T. Zhang, L. Wei, L. Xie, Z. Zhuang, Y . Zhang, B. Li, and\nQ. Tian, Spatiotemporal transformer for video-based person\nre-identiﬁcation, arXiv preprint arXiv: 2103.16469, 2021.\n[22] X. Liu, P. Zhang, C. Yu, H. Lu, X. Qian, and X. Yang, A\nvideo is worth three views: Trigeminal transformers for\nvideo-based person re-identiﬁcation, arXiv preprint arXiv:\n2104.01745, 2021.\n[23] W. Deng, L. Zheng, Q. Ye, G. Kang, Y . Yang, and\nJ. Jiao, Image-image domain adaptation with preserved\nCVFself-similarity and domain-dissimilarity for person\nre-identiﬁcation, in Proc. of 2018 IEEE/CVF Conf. on\nComputer Vision and Pattern Recognition, Salt Lake City,\nUT, USA, 2018, pp. 994–1003.\n[24] L. Wei, S. Zhang, W. Gao, and Q. Tian, Person transfer\nGAN to bridge domain gap for person re-identiﬁcation, in\nProc. 2018 IEEE/CVF Conf. on Computer Vision and Pattern\nRecognition, Salt Lake City, UT, USA, 2018, pp. 79–88.\n[25] Z. Zhong, L. Zheng, Z. Luo, S. Li, and Y . Yang, Invariance\nmatters: Exemplar memory for domain adaptive person re-\nidentiﬁcation, in Proc. 2019 IEEE/CVF Conf. on Computer\nVision and Pattern Recognition, Long Beach, CA, USA,\n2019, pp. 598–607.\n[26] X. Wang, H. Zhang, W. Huang, and M. R. Scott, Cross-batch\nmemory for embedding learning, in Proc. 2020 IEEE/CVF\nConf. on Computer Vision and Pattern Recognition, Seattle,\nW A, USA, 2020, pp. 6387–6396.\n[27] H. X. Yu, W. S. Zheng, A. Wu, X. Guo, S. Gong, and J. H.\nLai, Unsupervised person re-identiﬁcation by soft multilabel\nlearning, in Proc. 2019 IEEE/CVF Conf. on Computer Vision\nand Pattern Recognition, Long Beach, CA, USA, pp. 2143–\n2152.\n[28] Y . Fu, Y . Wei, G. Wang, Y . Zhou, H. Shi, U.\nUiuc, and T. Huang, Self-similarity grouping: A simple\nunsupervised cross domain adaptation approach for person\nre-identiﬁcation, in Proc. 2019 IEEE/CVF Int. Conf. on\nComputer Vision, Seoul, Republic of Korea, 2019, pp. 6111–\n6120.\n[29] Y . Ge, D. Chen, and H. Li, Mutual mean-teaching: Pseudo\nlabel reﬁnery for unsupervised domain adaptation on person\nre-identiﬁcation, in Proc. 8th Int. Conf. on Learning\nRepresentations, Addis Ababa, Ethiopia, 2020.\n[30] F. Zhao, S. Liao, G. S. Xie, J. Zhao, K. Zhang, and L.\nShao, Unsupervised domain adaptation with noise resistible\nmutual-training for person re-identiﬁcation, in Proc. 16th\nEuropean Conf. on Computer Vision, Glasgow, UK, 2020,\npp. 526–544.\n[31] K. Zheng, C. Lan, W. Zeng, Z. Zhang, and Z. J. Zha,\nExploiting sample uncertainty for domain adaptive person\nre-identiﬁcation, Proc. AAAI Conf. Artif. Intell., vol. 35, no.\n4, pp. 3538–3546, 2021.\n[32] Y . Zhai, S. Lu, Q. Ye, X. Shan, J. Chen, R. Ji, and Y .\nTian, AD-Cluster: Augmented discriminative clustering\nfor domain adaptive person re-identiﬁcation, in Proc.\n2020 IEEE/CVF Conf. on Computer Vision and Pattern\nRecognition, Seattle, W A, USA, 2020, pp. 9018–9027.\n[33] Y . Zhai, Q. Ye, S. Lu, M. Jia, R. Ji, and Y . Tian,\nMultiple expert brainstorming for domain adaptive person\nre-identiﬁcation, In Proc. 16th European Conf. on Computer\nVision, Glasgow, UK, 2020, pp. 594–611.\n[34] K. Zheng, W. Liu, L. He, T. Mei, J. Luo, and Z. J. Zha,\nGroup-aware label transfer for domain adaptive person re-\nidentiﬁcation, in Proc. 2021 IEEE/CVF Conf. on Computer\nVision and Pattern Recognition, Nashville, TN, USA, 2021,\npp. 5306–5315.\nShengming Yu et al.: Prompting and Tuning: A Two-Stage Unsupervised Domain Adaptive Person Re-identiﬁcation: : :809\n[35] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P.\nDhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et\nal., Language models are few-shot learners. inProc. 34th Int.\nConf. on Neural Information Processing Systems, Vancouver,\nCanada, 2020, pp. 1877–1901.\n[36] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, How can we\nknow what language models know? Trans. Assoc. Comput.\nLinguist, vol. 8, pp. 423–438, 2020.\n[37] Z. Zhong, D. Friedman, and D. Chen, Factual probing\nis [mask]: Learning vs. learning to recall, in Proc. 2021\nConf. of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\n2021, pp. 5017–5033.\n[38] T. Shin, Y . Razeghi, R. L. Logan IV , E. Wallace, and S.\nSingh, AUTOPROMPT: Eliciting knowledge from language\nmodels with automatically generated prompts, in Proc.\n2020 Conf. on Empirical Methods in Natural Language\nProcessing, 2020, pp. 4222–4235.\n[39] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S.\nAgarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al.,\nLearning transferable visual models from natural language\nsupervision, in Proc. 38th Int. Conf. on Machine Learning,\n2021, pp. 8748–8763.\n[40] C. Jia, Y . Yang, Y . Xia, Y . T. Chen, Z. Parekh, H. Pham,\nQ. Le, Y . H. Sung, Z. Li, and T. Duerig, Scaling up visual\nand vision-language representation learning with noisy text\nsupervision, in Proc. 38th Int. Conf. on Machine Learning,\n2021, pp. 4904–4916.\n[41] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, Learning to prompt\nfor vision-language models, Int. J. Comput. Vis., vol. 130,\nno. 9, pp. 2337–2348, 2022.\n[42] C. Ju, T. Han, K. Zheng, Y . Zhang, and W. Xie, Prompting\nvisual-language models for efﬁcient video understanding, in\nProc. 17th European Conf. on Computer Vision, Tel Aviv,\nIsrael, 2022, pp. 105–124.\n[43] M. Jia, L. Tang, B. C. Chen, C. Cardie, S. Belongie, B.\nHariharan, and S. N. Lim, Visual prompt tuning, in Proc.\n17th European Conf. on Computer Vision, Tel Aviv, Israel,\n2022, pp. 709–727.\n[44] Y . Ge, F. Zhu, D. Chen, R. Zhao, and H. Li, Self-\npaced contrastive learning with hybrid memory for domain\nadaptive object re-ID, in Proc. 34thInt. Conf. on Neural\nInformation Processing Systems, Vancouver, Canada, 2020,\np. 949.\n[45] C. Fu, Y . Li, and Y . Zhang, ATNet: Answering cloze-style\nquestions via intra-attention and inter-attention, in Proc.\n23rd Paciﬁc-Asia Conf. on Knowledge Discovery and Data\nMining, Macau, China, 2019, pp. 242–252.\n[46] X. Chang, Y . Yang, T. Xiang, and T. M. Hospedales, Disjoint\nlabel space transfer learning with common factorised\nspace, in Proc. Thirty-Third AAAI Conf. on Artiﬁcial\nIntelligence and Thirty-First Innovative Applications of\nArtiﬁcial Intelligence Conf. and Ninth AAAI Symp. on\nEducational Advances in Artiﬁcial Intelligence, Honolulu,\nHA, USA, 2019, p. 404.\n[47] Y . Lin, X. Dong, L. Zheng, Y . Yan, and Y . Yang, A\nbottom-up clustering approach to unsupervised person re-\nidentiﬁcation, Proc. AAAI Conf. Artif. Intell., vol. 33, no. 1,\npp. 8738–8745, 2019.\n[48] L. Qi, L. Wang, J. Huo, L. Zhou, Y . Shi, and Y . Gao, A novel\nunsupervised camera-aware domain adaptation framework\nfor person re-identiﬁcation, in Proc. 2019 IEEE/CVF Int.\nConf. on Computer Vision, Seoul, Republic of Korea, 2019,\npp. 8079–8088.\n[49] Y . J. Li, C. S. Lin, Y . B. Lin, and Y . C. F. Wang,\nCross-dataset person re-identiﬁcation via unsupervised pose\ndisentanglement and adaptation, in Proc. 2019 IEEE/CVF\nInt. Conf. on Computer Vision, Seoul, Republic of Korea,\n2019, pp. 7918–7928.\n[50] X. Zhang, J. Cao, C. Shen, and M. You, Self-training with\nprogressive augmentation for unsupervised cross-domain\nperson re-identiﬁcation, in Proc. 2019 IEEE/CVF Int. Conf.\non Computer Vision, Seoul, Republic of Korea, 2019, pp.\n8221–8230.\n[51] F. Yang, K. Li, Z. Zhong, Z. Luo, X. Sun, H. Cheng, X.\nGuo, F. Huang, R. Ji, S. and Li, Asymmetric co-teaching for\nunsupervised cross-domain person re-identiﬁcation, Proc.\nAAAI Conf. Artif. Intell., vol. 34, no. 7, pp. 12597–12604,\n2020.\n[52] D. Wang and S. Zhang, Unsupervised person re-\nidentiﬁcation via multi-label classiﬁcation, in Proc.\n2020 EEE/CVF Conf. on Computer Vision and Pattern\nRecognition, Seattle, W A, USA, 2020, pp. 10978–10987.\n[53] Y . Huang, P. Peng, Y . Jin, J. Xing, C. Lang, and S.\nFeng, Domain adaptive attention model for unsupervised\ncross-domain person re-identiﬁcation, arXiv preprint arXiv:\n1905.10529, 2019.\n[54] X. Jin, C. Lan, W. Zeng, and Z. Chen, Global\ndistance-distributions separation for unsupervised person re-\nidentiﬁcation, in Proc. 16th European Conf. on Computer\nVision, Glasgow, UK, 2020, pp. 735–751.\n[55] W. Wang, F. Zhao, S. Liao, and L. Shao, Attentive\nWaveBlock: Complementarity-enhanced mutual networks\nfor unsupervised domain adaptation in person re-\nidentiﬁcation and beyond, IEEE Trans. Image Process., vol.\n31, pp. 1532–1544, 2022.\n[56] Y . Dai, J. Liu, Y . Sun, Z. Tong, C. Zhang, and L. Y .\nDuan, IDM: An intermediate domain module for domain\nadaptive person re-ID, in Proc. 2021 IEEE/CVF Int. Conf.\non Computer Vision, Montreal, Canada, 2021, pp. 11844–\n11854.\n[57] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian,\nScalable person re-identiﬁcation: A benchmark, in Proc.\n2015 IEEE Int. Conf. on Computer Vision, Santiago, Chile,\n2015, pp. 1116–1124.\n[58] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi,\nPerformance measures and a data set for multi-target, multi-\ncamera tracking, In Proc. 14th European Conf. on Computer\nVision, Amsterdam, The Netherlands, 2016, pp. 17–35.\n810 Tsinghua Science and Technology, August 2023, 28(4): 799–810\nShengming Yu received the BE degree in\nthe Department of Electronic Engineering,\nTsinghua University, China, in 2015. He\nis pursuing the PhD degree with the\nDepartment of Electronic Engineering,\nTsinghua University, since 2015. His\nresearch interests are computer vision,\npattern recognition, and person re-\nidentiﬁcation\nZhaopeng Dou received the BE degree in\nthe Department of Electronic Engineering,\nTsinghua University, China, in 2019.\nHe is pursuing the PhD degree with\nthe Department of Electronic Engineering,\nTsinghua University, since 2019. His research\ninterests include image processing, pattern\nrecognition, and person re-identiﬁcation.\nShengjin Wang received the BE degree in\n1985 from the Department of Electronics\nEngineering, Tsinghua University, China,\nand the PhD from the Precision and\nIntelligence Laboratory, Tokyo Institute of\nTechnology, Japan in 1997. From May 1997\nto August 2003, he was a researcher at\nthe Internet System Research Laboratories\nof NEC Corp. in Japan. From September 2003, he becomes\nprofessor of the Department of Electronics Engineering, Tsinghua\nUniversity. He is currently a director of the Research Institute\nof Image and Graphics (RIIG) in Tsinghua University. He has\npublished more than 100 journal and conference papers on image\nprocessing, ten patents, and won the National Scientiﬁc and\nTechnical Progress Awards 2008. His current research interests\ninclude computer vision, machine learning, intelligent video\nanalysis, person re-identiﬁcation and multimodal cooperative\nrobotics. He is a senior member of the IEEE."
}