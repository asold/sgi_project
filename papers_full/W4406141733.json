{
    "title": "Expert evaluation of large language models for clinical dialogue summarization",
    "url": "https://openalex.org/W4406141733",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A3015423722",
            "name": "David Fraile Navarro",
            "affiliations": [
                "Macquarie University"
            ]
        },
        {
            "id": "https://openalex.org/A2224665237",
            "name": "Enrico Coiera",
            "affiliations": [
                "Macquarie University"
            ]
        },
        {
            "id": "https://openalex.org/A2952184167",
            "name": "Thomas W. Hambly",
            "affiliations": [
                "University of Technology Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A5115799698",
            "name": "Zoe Triplett",
            "affiliations": [
                "Macquarie University"
            ]
        },
        {
            "id": "https://openalex.org/A5115799699",
            "name": "Nahyan Asif",
            "affiliations": [
                "Macquarie University"
            ]
        },
        {
            "id": null,
            "name": "Anindya Susanto",
            "affiliations": [
                "Macquarie University",
                "University of Indonesia"
            ]
        },
        {
            "id": "https://openalex.org/A2715586441",
            "name": "Anamika Chowdhury",
            "affiliations": [
                "Canberra Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2432738276",
            "name": "Amaya Azcoaga-Lorenzo",
            "affiliations": [
                "Madrid Health Service",
                "University of St Andrews",
                "Hospital Universitario Fundación Jiménez Díaz"
            ]
        },
        {
            "id": "https://openalex.org/A2198576706",
            "name": "Mark Dras",
            "affiliations": [
                "Macquarie University"
            ]
        },
        {
            "id": "https://openalex.org/A2703876371",
            "name": "Shlomo Berkovsky",
            "affiliations": [
                "Macquarie University"
            ]
        },
        {
            "id": "https://openalex.org/A3015423722",
            "name": "David Fraile Navarro",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2224665237",
            "name": "Enrico Coiera",
            "affiliations": [
                "Macquarie University"
            ]
        },
        {
            "id": "https://openalex.org/A2952184167",
            "name": "Thomas W. Hambly",
            "affiliations": [
                "University of Technology Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A5115799698",
            "name": "Zoe Triplett",
            "affiliations": [
                "Macquarie University"
            ]
        },
        {
            "id": "https://openalex.org/A5115799699",
            "name": "Nahyan Asif",
            "affiliations": [
                "Macquarie University"
            ]
        },
        {
            "id": null,
            "name": "Anindya Susanto",
            "affiliations": [
                "Macquarie University",
                "University of Indonesia"
            ]
        },
        {
            "id": "https://openalex.org/A2715586441",
            "name": "Anamika Chowdhury",
            "affiliations": [
                "Canberra Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2432738276",
            "name": "Amaya Azcoaga-Lorenzo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2198576706",
            "name": "Mark Dras",
            "affiliations": [
                "Macquarie University"
            ]
        },
        {
            "id": "https://openalex.org/A2703876371",
            "name": "Shlomo Berkovsky",
            "affiliations": [
                "Macquarie University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3113519334",
        "https://openalex.org/W3188578695",
        "https://openalex.org/W2408054057",
        "https://openalex.org/W2052045476",
        "https://openalex.org/W3134273711",
        "https://openalex.org/W2897608176",
        "https://openalex.org/W4285239851",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4390041933",
        "https://openalex.org/W4287887977",
        "https://openalex.org/W4307537353",
        "https://openalex.org/W4287815380",
        "https://openalex.org/W3199259836",
        "https://openalex.org/W4225727438",
        "https://openalex.org/W4310918653",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W3081603953",
        "https://openalex.org/W1979290264",
        "https://openalex.org/W2942340548",
        "https://openalex.org/W3169068430",
        "https://openalex.org/W6854636774",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W4386566731",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4401042496",
        "https://openalex.org/W4392193048",
        "https://openalex.org/W4394585199",
        "https://openalex.org/W7005521895",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4367623495",
        "https://openalex.org/W6600686112",
        "https://openalex.org/W4401422245",
        "https://openalex.org/W3197078525",
        "https://openalex.org/W4307444957"
    ],
    "abstract": null,
    "full_text": "Expert evaluation of large language \nmodels for clinical dialogue \nsummarization\nDavid Fraile Navarro1, Enrico Coiera1, Thomas W. Hambly2, Zoe Triplett3, Nahyan Asif4, \nAnindya Susanto1,5, Anamika Chowdhury6, Amaya Azcoaga Lorenzo7,8,9, Mark Dras10 & \nShlomo Berkovsky1\nWe assessed the performance of large language models’ summarizing clinical dialogues using \ncomputational metrics and human evaluations. The comparison was done between automatically \ngenerated and human-produced summaries. We conducted an exploratory evaluation of five language \nmodels: one general summarisation model, one fine-tuned for general dialogues, two fine-tuned \nwith anonymized clinical dialogues, and one Large Language Model (ChatGPT). These models \nwere assessed using ROUGE, UniEval metrics, and expert human evaluation was done by clinicians \ncomparing the generated summaries against a clinician generated summary (gold standard). The fine-\ntuned transformer model scored the highest when evaluated with ROUGE, while ChatGPT scored the \nlowest overall. However, using UniEval, ChatGPT scored the highest across all the evaluated domains \n(coherence 0.957, consistency 0.7583, fluency 0.947, and relevance 0.947 and overall score 0.9891). \nSimilar results were obtained when the systems were evaluated by clinicians, with ChatGPT scoring \nthe highest in four domains (coherency 0.573, consistency 0.908, fluency 0.96 and overall clinical \nuse 0.862). Statistical analyses showed differences between ChatGPT and human summaries vs. all \nother models. These exploratory results indicate that ChatGPT’s performance in summarizing clinical \ndialogues approached the quality of human summaries. The study also found that the ROUGE metrics \nmay not be reliable for evaluating clinical summary generation, whereas UniEval correlated well with \nhuman ratings. Large language models may provide a successful path for automating clinical dialogue \nsummarization. Privacy concerns and the restricted nature of health records remain challenges for its \nintegration. Further evaluations using diverse clinical dialogues and multiple initialization seeds are \nneeded to verify the reliability and generalizability of automatically generated summaries.\nKeywords Natural language processing, Electronic health records, Primary care, Artificial intelligence\nClinical history taking is one of the pillars of medical practice. This is especially true in the context of primary \ncare where clinical history taking and examination take a central role. Clinicians have traditionally kept a \nrecord of their patient consultations. With the advent of the informatics era, this record has evolved into an \nElectronic Health Record (EHR) where, among others, a synthesized summary of a clinical conversation is kept, \nand produced either during or after each clinical visit. EHR usage has become a major burden for clinicians \nworldwide1,2. Several reasons explain this situation, such as the ever-increasing complexity of records and \npoor implementation of EHR systems 3 as well as a growing shortage of clinicians, especially in primary care 4. \nEffectively, EHRs consume a considerable amount of clinicians’ time 5, and are considered one of the causes \ninfluencing their burnout 6. The use of an automated approach to record keeping 7 may prove to be a viable \nalternative.\nAbstractive summarization involves generating lengthy summaries by rephrasing or using new words. This \napproach differs from extractive summarization, which combines important text from the source. However, \n1Centre for Health Informatics, Australian Institute of Health Innovation, Macquarie University, Level 6, 75 Talavera \nRoad, North Ryde, Sydney, NSW 2113, Australia. 2Faculty of Engineering and Information Technology, University \nof Technology Sydney, Sydney, Australia. 3School of Medicine, Faculty of Human and Health Sciences, Macquarie \nUniversity, Sydney, Australia. 4Macquarie University Hospital, Sydney, Australia. 5Faculty of Medicine, Universitas \nIndonesia, Jakarta, Indonesia. 6Cowra District Hospital, Cowra, Australia. 7Health Centre Los Pintores, Madrid \nHealth Services, Madrid, Spain. 8Health Research Institute, Fundación Jimenez Díaz, Madrid, Spain. 9University of \nSt Andrews, St Andrews, Scotland, UK. 10School of Computing, Macquarie University, Sydney, Australia. email: \ndavid.frailenavarro@mq.edu.au\nOPEN\nScientific Reports |         (2025) 15:1195 1| https://doi.org/10.1038/s41598-024-84850-x\nwww.nature.com/scientificreports\n\nabstractive summarization comes with its own set of challenges. These challenges must be addressed to ensure \nthe summaries are of high quality. The resulting summaries should be coherent and accurately represent the \nsource text. To produce effective abstractive summaries, several key challenges need to be overcome: First, as the \nlength of the source text increases, the risk of introducing irrelevant or repetitive content in the summary also \nrises, which can negatively impact the summary’s coherence and usefulness. Second, longer summaries require \na deeper understanding of the relationships between the source text concepts and ideas, making it difficult \nfor language models to maintain consistency and avoid contradictions. Third, computational complexity and \nmemory requirements for generating longer summaries are significantly higher, which can hinder the training \nand deployment of large-scale models, especially in resource-constrained environments. Fourth, evaluating the \nquality of lengthy summaries becomes challenging, as traditional metrics such as ROUGE 8,9 and BLEU10 may \nnot fully capture the nuances and subtleties of longer, more complex texts. Fifth, ensuring that lengthy summaries \nremain faithful to the original text while maintaining a high level of abstraction is a critical challenge, as the \nmodels need to strike a delicate balance between comprehensiveness and conciseness. Therefore, developing \nadvanced abstractive summarization models capable of addressing these challenges will be crucial for unlocking \ntheir full potential in various domains, including clinical settings, scientific literature, and legal texts.\nNatural Language Processing (NLP) has been developing steadily, especially over the last decade, with the \nadvent of word-vectorization11, transformer models12, and their derivatives such as BART13 and T514. Previously \ndeemed as challenging, the possibility of abstractive summarization has become increasingly feasible with \nnewer language models. This is a result of their flexible architectures, ability to fine-tune pre-trained models \nand improvements in the long text processing capability. Previously, one of the main barriers to perform clinical \ndialogue summarization was the ability to process full clinical conversations with deep learning architectures. \nThanks to recent developments in transformer models, processing medical conversations in full is now feasible. \nThe progress has been exponential the last years, scaling from a few hundred words for older transformers14 up \nto more than 1 million tokens (several books) for latest Google’s Gemini model15, which would suffice to process \nentirely a full patient record.\nThe aim of this exploratory study is to perform and compare a computational and human expert evaluation \nof abstractive summarization of clinical dialogues. This evaluation assesses the suitability of various NLP models \nfor clinical use. The study compares the performance of the models using diverse evaluation metrics. Through \nthis comparison, we seek to surface indicative empirical evidence regarding the most effective and reliable \napproaches for generating accurate, coherent, and contextually relevant summaries that can assist healthcare \nprofessionals in their daily practice. By evaluating state-of-the-art transformer-based architectures and Large \nLanguage Models (LLMs), we aim to gain insights into their strengths and weaknesses when applied to the \ndomain-specific language and nuanced content of clinical dialogues. We assess the quality of the generated \nsummaries using two approaches. First, we employ computational metrics that focus on common characteristics \nsuch as sentence similarity and semantic quality. Second, we conduct human expert evaluations. This dual \napproach ensures a comprehensive understanding of each model’s performance. We specifically focus on \nhow well the models capture and present the critical aspects of clinical dialogues. This approach allowed us to \nevaluate the quality of the generated summaries. It shows that one system (ChatGPT) outperforms the rest both \nby human judgment and with most comprehensive computational metrics. Further studies, using larger datasets \nand more diverse initialization seeds, may be needed to replicate and re-affirm our findings.\nMaterials and methods\nExtending previous research16, we evaluated fine-tuned and off-the-shelf language models deployed for a clinical \ndialogue summarization task. The summarized outputs were compared with a human-generated gold standard, \nand the quality of the output was further evaluated by clinician evaluators.\nExperimental setup\nWe included five models pre-trained for the summarization task: two models were fine-tuned with clinical \ndialogues, one was trained only with general dialogues, one was trained for long summarization in books, and \none was a general-purpose LLM. The main characteristics of the included models are as follows:\n• BART-LSG-conv – This model is based on BART 13 pre-trained with snippets from clinical dialogues 16. We \nutilized the LSG attention mechanism17 to modify the model and increase the maximum allowed number of \ntokens to 8,000.\n• BART-DnC (Divide-and-conquer) – Following the divide-and-conquer approach 18, we utilized the pre-\ntrained model of 16 to produce summaries of snippets. Subsequently, a second summarization step was de -\nployed to generate the final summary.\n• LongDialSumm –  Long Dialogue Summarization mode l 19 based on BART 13 This model was not further \nfine-tuned.\n• T5-Booksum - LongT520 model pre-trained for long summarization using the BookSum21 dataset.\n• OpenAI’s ChatGPT (text-davinci-003 mode)l22.\nThe two fine-tuned models ( BART-LSG-Conv and BART-DnC) using the clinical dialogue dataset were based \non the best performing model in our previous study 16 that are fine-tuned versions of BART (Available at  h t t    \np s : / /  h  u g g i  n g f  a c e  .   c o / d a f r a i l e / C l i n i - d i a l o g - s u m - B A R T     ) . The other models were deployed in a domain-agnostic \nmanner, without any further training. The fine-tuned models where trained using the Huggingface’s transformer \nlibrary with a Pytorch backend on Amazon Web Services (p2.xlarge) instances with an Nvidia Tesla K80 \nGPU with 12GB or VRAM or a p3.2xlarge with an NVIDIA V100 GPU and 16GB of VRAM. Both instances \nwere configured with 4-core CPU and 61GB of conventional RAM. Given that no additional fine-tuning was \nScientific Reports |         (2025) 15:1195 2| https://doi.org/10.1038/s41598-024-84850-x\nwww.nature.com/scientificreports/\nperformed on longer dialogues, we did not utilise different initialization seeds beyond the fine-tuning already \nreported in 16. Training scripts are available in the Supplementary File 1 including model configuration and \ntraining configuration details.\nDataset\nThe dataset exploited in this study was sourced from 27 anonymized clinical dialogues recorded from primary \ncare consultations, a subset of data reported by Quiroz et al.23. This data was further processed for summarization \ntasks as described by Navarro et al. 16. For the models that were fine-tuned ( BART-LSG-conv and BART-DnC), \nas the dialogue transcripts exceeded the token limit (512 or 1024 tokens) set by these language models, a pre-\nprocessing phase was necessary. To preserve the structural integrity of the conversations, patients were assigned \ngeneric identities (Joe and Jane) while the clinician was generically named and referred to as “Doctor” . The \ntranscripts were divided into 400-word segments, which were further refined to maintain semantically consistent \npairs of clinician-patient interaction, i.e., a doctor’s question followed by a patient’s answer. A small portion \nof the segmented snippets ( < 5%) was discarded due to the lack of relevant clinical content. These snippets \ncontained non-relevant clinical text (such as administrative discussions), or were part of the introductory or \nclosing parts of the conversations, without any medical relevance (e.g. discussing holidays).\nThe dataset was subsequently split in an 80 − 20 ratio for training and evaluation purposes. Given the data \nsplit ratio, 22 dialogues were used for training and the remaining 5 dialogues were used for evaluation purposes. \nIn total the dataset contained 56,158 tokens for the clinical dialogues (41,501 for training + 14,657 for testing) \nand 9,784 in the summaries (7,040 for training and 2,744 for testing). The average dialogue length was 391.52 \ntokens (median 418, standard deviation 85.28) and the average summary length was 66.42 tokens (median 67.5, \nstandard deviation 21.42).\nEvaluation\nWe kept the same evaluation subset used in our previous evaluation16. Given that the original snippet annotations \ndid not generate full dialogue summaries, new expert-generated summaries were created by a clinician (DFN) \nand reviewed by a second clinician (AAL). To generate semantically self-contained summaries that were \nsemantically contained in the dialogues, the human summaries conformed to two rules. First, they only used the \nvocabulary that was present in the dialogue (e.g. if a patient or doctor described a symptom as “out of breath” \nit would not use the more technical term dyspnea). Second, the human-produced summaries were created as \na recollection of descriptive sentences, with minimal stylistic features (e.g. not splitting it into paragraphs or \nadding headings) and containing only a recollection of the clinically relevant information (e.g. past medical \nhistory, drug use, family history). An example of a generated summary by one of the language models and the \nhuman-generated summary (gold standard) is shown in Box 1 (the complete original dialogue is available in \nSupplementary File 1).\nEvaluation metrics\nSummaries were evaluated with quantitative and qualitative methods, using both automated and manual \nscores. For the automated scores, the ROUGE 24 family metrics were used. Additionally, we used UniEval 25 to \nautomatically compute 4 quality dimensions (or, domains) of the produced summaries. UniEval, a unified multi-\ndimensional evaluator for Natural Language Generation (NLG), reframes evaluation as a Boolean question-\nanswering task. By incorporating external knowledge and guiding the model with different questions, UniEval \nwas shown to achieve a higher correlation with human judgements and demonstrated strong zero-shot learning \ncapabilities in unseen evaluation dimensions and tasks25.\nQualitative evaluation\nFive medically trained clinicians performed the human evaluation (TH, ZT, NA, AC, AS) assessing the summaries \ngenerated by the five systems and the gold-standard human-generated summaries. They were blinded regarding \nwhich system they were evaluating including the human summaries. To avoid priming, systems were presented \nin a random order, allowing the evaluator to access them in a non-consecutive way.\nHuman evaluators were explained the task and the scoring with respect to the different evaluation domains. \nThe human evaluators followed the same 4-domain structure proposed in UniEval 25: coherence, consistency, \nfluency and relevance. The evaluators were encouraged to resolve with the investigators any difficulties arising \nupon completing the first dialogue to ensure consistency in the evaluation.\nLongT5-booksum summary: The Doctor meets with Joe, a student at the university who has been complaining of a feverish cough and sore throat for two weeks. He also complains \nabout having pressure in his ear as well as a cold. The Doctor asks Joe if he is on any regular medicine, and Joe replies that he’s an international student studying business and finance \nfrom the United States. They discuss his family history, which includes his mother and his siblings. His father has heart disease, and his sister has surgery just two days ago. When the \nDoctor looks at Joe’s ear, he notices nodes of redness but doesn’t know whether it’s caused by a disease or a infection. The doctor decides to give Joe a penicillin to prevent him from \ngetting sick again.\nHuman summary: Joe had a cough and a sore throat for the last couple of days. Today he wasn’t feverish when waking up, but he was the last two days and he has been taking ibuprofen. \nHe has been sick like this a couple of times. A year and half ago, he had five tonsillitis episodes within the year. This time is different because he also has cough. He has no allergies. \nOccasionally he has difficulties with breathing when exercising but he has not been diagnosed of asthma. Joe is an international student from USA studying international business and \nfinance. He doesn’t smoke and he drinks occasionally. His dad has heart issues due to high blood pressure. His sister has a heart condition too, she was born with and is having surgery. He \ndoesn’t have a fever now. Tonsils are enlarged and lymph nodes are a bit sore, which is consistent with a viral infection. Doctor listened to Joe’s chest and is fine. Doctor takes a throat swab \nto rule out a particular type of strep, in case that he’s positive he would have to take penicillin for 10 days. Doctor orders penicillin one tablet twice a day. Joe will start the treatment until \nthe results come back on Monday. He can take Ibuprofen, honey, lemon or Strepsils or Lozenges from the chemist. Joe will come back on Monday to check the results.\nBox 1. Generated summary by LongT5-booksum model (top) and human-generated summary (bottom).\n \nScientific Reports |         (2025) 15:1195 3| https://doi.org/10.1038/s41598-024-84850-x\nwww.nature.com/scientificreports/\nThe decimal scoring of UniEval was converted into a 5-point Likert scale to facilitate human evaluation. \nInstead of producing an overall score as in the UniEval metric, human evaluators were asked an additional \nquestion that reflected on the overall potential for clinical use: “To what extent do you think the summary could \nbe used in a clinical setting? (e.g. to produce a progress, note in primary care)” , which was also evaluated on a \n5-point Likert scale. Following each question, a free-text box allowed the evaluators to justify their scores and \nprovide examples. The obtained free-text feedback was evaluated by extracting the commonalities across the \nanswers using a bottom-up thematic analysis26.\nStatistical analysis\nWe calculated inter-rater reliability using intra-class correlation coefficients (ICC) between human evaluators \nthemselves and between humans and the automated (UniEval) metrics. For this calculation, we used the average \nscore across the evaluated dialogues for each evaluator, for each system and each dimension (one intra-class \ncorrelation coefficient for each dimension and system). Additionally, individual dialogue scores were calculated \nand are also reported in Supplementary File 2.\nFor the comparison between human evaluations and UniEval, an average score (converted to a decimal scale) \nfor the human scoring was compared with the automated UniEval score. To compare the performance of the \nstudied systems across the evaluated domains, we used repeated measures ANOV A, with a significance level \nset at p = .05, and a post hoc analysis using a t-test with the Bonferroni correction. We established comparisons \nbetween the model-generated and gold-standard human summaries using the scorings obtained from the \nhuman evaluation. Additionally, we repeated the analysis using the scores obtained with UniEval.\nAll the study methods were carried out in accordance with Macquarie University research policies. \nExperimental protocol was deemed exempt from requiring additional ethics approval by the Macquarie \nUniversity, School of Computing Ethics liaison. Original data collection Ethics Approval available at 27 where \ninformed consent was obtained from all subjects and/or their legal guardian(s).\nResults\nIn this exploratory study, when summaries were evaluated using ROUGE, the highest ROUGE-1, ROUGE-2 \nand ROUGE-L-SUM scores were obtained by the BART-LSG-conv model pre-trained with clinical dialogue \nsnippets, while ChatGPT scored the lowest for ROUGE-1 and ROUGE-L-Sum. For the ROUGE-L metric, \nBART-LSG-conv and LongDialSumm scored similarly, while T5-Booksum was the lowest for ROUGE-2 and \nROUGE-L. Figure 1 represents the ROUGE scores across the different systems and evaluation metrics.\nUniEval Scoring\nApplying the UniEval scoring, ChatGPT scored the highest for coherence, consistency, fluency, relevance and \noverall. LongDialSumm achieved the lowest scores in all the domains except for consistency, where the lowest \nscore was obtained by T5-BookSum (Table 1). Note that human-generated summaries were excluded from the \nautomated UniEval evaluation, as they were used by UniEval as the gold standard to calculate the scores of the \nautomated summarization systems.\nHuman evaluation\nWhen clinicians evaluated the summaries concerning the four UniEval domains and an additional item judging \nthe overall potential for use in a clinical setting, ChatGPT scored the highest in coherency, consistency fluency \nand clinical use, while human-generated summaries scored the highest in the relevance domain (Table 1).\nThe intra-class correlation coefficient showed generally excellent reliability among the human evaluators \nwhen comparing the averaged ratings across the evaluated dialogues. When comparing the averaged human \nevaluation with UniEval, excellent reliability was found for coherency, relevance and overall, while consistency \nFig. 1. ROUGE scores by system.\n \nScientific Reports |         (2025) 15:1195 4| https://doi.org/10.1038/s41598-024-84850-x\nwww.nature.com/scientificreports/\nand fluency reached moderate reliability (Table 2). Supplementary file 2 contains the individual scores for each \ndialogue.\nThe repeated measures ANOV A test indicated a significant difference in the scores between the systems across \nall domains: F(5,20) = 28.18, p < .001, for the coherency domain, F(2.23,8.93) = 7.58, p = .011 for consistency, \nF(2.26,9.05) = 27.6, p < .001 for fluency, F(5,20) = 22.47, p < .001 for relevance and F(1.99,7.94) = 20.41, p < .001 \nfor the overall clinical use (Table  3 and supplementary file 3). The post hoc paired t-test using a Bonferroni \ncorrection showed that the means of several pairs were significantly different for human-generated and \nChatGPT-generated summaries compared to all the other systems (Table  3 and supplementary file 3). When \nanalysing UniEval automated scores, repeated measures ANOV A found significant differences for fluency \n(p = .008), relevance (p < .001) and overall (p = .003) (supplementary file 3).\nQualitative evaluation\nHuman evaluators provided comprehensive justifications for the scores they assigned across various domains, \nalong with the overall clinical usefulness. These explanations offered valuable insight into the distinct types of \nerrors and limitations that surfaced, as well as the main factors influencing their scoring decisions. We analyzed \nthe feedback received for the potential clinical use and categorized it into three scoring categories. For scores 1 \nand 2 (Poor Quality / Limited Usefulness), for score 3 (Adequate but Needs Improvement) and for summaries \nscores of 4 and 5 (Good to Excellent Quality).\nFor the category of coherency, evaluators frequently cited non-sequential sentence ordering and inconsistent \nverb tense usage as factors contributing to lower scores. Medium scores were given in cases where the system \nunexpectedly incorporated elements of the original dialogue. Higher scores were attributed to the presence of \nlogically ordered sentences.\nRegarding consistency, evaluators noted that low scores were primarily due to the usage of grammatically \nincorrect sentences, inaccuracies, factual errors, and the inclusion of hallucinatory content. Medium scores \nwere triggered by the absence of certain clinical information, while high scores were reserved for summaries \nDomain ICC F df1 df2 p-value CI95%\nAverage fixed raters ICC human evaluation\n Coherency 0.971 34.201 5 20 < 0.001 [0.9, 1.0]\n Consistency 0.961 25.818 5 20 < 0.001 [0.87, 0.99]\n Fluency 0.959 24.691 5 20 < 0.001 [0.87, 0.99]\n Relevance 0.947 18.748 5 20 < 0.001 [0.82, 0.99]\n Overall (clinical use) 0.979 47.759 5 20 < 0.001 [0.93, 1.0]\nAverage fixed raters ICC human evaluation vs. UniEval\n Coherency 0.974 37.816 4 4 0.002 [0.75, 1.0]\n Consistency 0.658 2.925 4 4 0.162 [-2.28, 0.96]\n Fluency 0.689 3.213 4 4 0.142 [-1.99, 0.97]\n Relevance 0.901 10.105 4 4 0.023 [0.05, 0.99]\n Overall 0.89 9.132 4 4 0.027 [-0.05, 0.99]\nTable 2. ICC coefficients for averaged scores by domain for all systems. Significant values are in bold.\n \nUNIEV AL scores Coherence Consistency Fluency Relevance Overall     \nLongDialsumm 0.53153 0.61116 0.6438 0.2597 0.5115\nChatGPT 0.9547 0.73634 0.9576 0.9437 0.8981\nT5booksum 0.68484 0.56454 0.9341 0.6486 0.708\nBART-DnC 0.59903 0.61694 0.7925 0.4575 0.6165\nBART-LSG-conv 0.52529 0.69167 0.8451 0.4632 0.6313\nHuman \nevaluation \nscores Coherence Consistency Fluency Relevance\nClinical \nuse\nHuman \nsummaries 0.904 0.872 0.832 0.896 0.848\nBART-LSG-conv 0.48 0.488 0.544 0.496 0.408\nLongDialSumm 0.452 0.562 0.474 0.432 0.414\nT5-BookSum 0.544 0.456 0.536 0.472 0.392\nBart-DnC 0.504 0.616 0.496 0.52 0.456\nChatGPT 0.973 0.908 0.96 0.886 0.862\nTable 1. Automated UniEval and human evaluation scores for each system (average scores across 5 dialogues). \nTop result values are in bold.\n \nScientific Reports |         (2025) 15:1195 5| https://doi.org/10.1038/s41598-024-84850-x\nwww.nature.com/scientificreports/\nthat demonstrated consistent information flow, even if the subject matter order was lacking. When evaluating \nfluency, evaluators determined low scores by the presence of nonsensical sentences and repetitive content. \nSummaries containing non-technical words received medium scores. High scores were assigned to summaries \nthat were well-articulated and exhibited correct grammar. In terms of relevance, summaries with missing factual \ncontent typically resulted in low scores, whereas summaries with incorrect minor facts led to medium scores. \nHigh scores were given to summaries that captured key information, even if they omitted minor details.\nIn relation to the overall potential for clinical use, summaries falling under low scores were characterized by \nfactual inaccuracies, deficiencies in crucial clinical information, incoherent sentence construction, disjointed \ninformation presentation, and poor language or grammar usage. These summaries are considered to have limited \nuse or are deemed unusable in a clinical setting without substantial improvement. Summaries that garnered \nmedium scores generally encapsulated the main points of the consultation. However, they could potentially \nbe lacking in key details or contain inaccuracies. However, with a thorough review and necessary corrections \nor additions, these summaries could still be employed in a clinical setting. Finally, summaries awarded high \nscores exhibited high levels of accuracy, excellent structure, and effectively captured the central topics of the \nconsultation. While they may contain minor errors, omissions, or areas necessitating improvement, they largely \nsatisfy the requirements for clinical use, requiring little to no revisions.\nTable 4and Supplementary File 4 provides a detailed breakdown of these factors, accompanied by specific \nexamples that illustrate the feedback from the evaluators.\nDiscussion\nThe reported expert evaluation of clinical dialogue summarization suggests that while fine-tuned with clinical \ndialogues models ( BART-LSG-conv) outperform those that were not fine-tuned with respect to classical \nevaluation scores (ROUGE metrics), this did not translate to improved quality or usefulness when more granular \nmetrics such as UniEval or human evaluation were applied. Strikingly, the worst performing model with the \nROUGE metrics (ChatGPT) consistently performed best when evaluated both with UniEval and human \nevaluation, outperforming all other models. ChatGPT results show significant improvements in the quality of \nthe summaries compared to previously developed BART and T5 transformer models. Additionally, our findings \nsuggest that the quality of the ChatGPT summaries may be comparable to the quality of human-generated \nsummaries when assessed by clinicians. We have also shown a strong correlation between human evaluation and \nthe automated UniEval metrics, validating the usefulness of this metric in the clinical summarization scenario.\nWhile the existence of clinical dialogue datasets for summarization remains a major obstacle to the practical \nimplementation of these approaches, thanks to the above developments, especially LLMs, the potential for \nproducing, accurate, clinical dialogue summaries is within reach. Ultimately, they have the potential to advance \nSum of square Mean square\nF Statistic      \n(df1, df2)     p-value\nCoherency 32.276 6.4552 28.1845 (5,20) < 0.0001\nConsistency 20.8323 4.1665 7,5815 (5,20) < 0.001\nFluency 245,524 49,105 27,6025 (5,20) < 0.0001\nRelevance 35.0293 7.0059 22.4653 (5,20) < 0.0001\nOverall 33.9617 6.7923 20.4076 (5,20) < 0.0001\nBART-LSG-\nConv BART-DnC LongDialSumm T5-Booksum\nCoherency\n Human * *\n ChatGPT * * * *\nConsistency\n Human * *\n ChatGPT\nFluency\n Human *\n ChatGPT * * * *\nRelevance\n Human *\n ChatGPT * * * *\nOverall (clinical use)\n Human *\n ChatGPT * * * *\nTable 3. Repeated measurements ANOV A scores for between systems differences and post hoc t-tests \nfor model differences by domain using human evaluation scoring. *Denotes a significant difference (with \nBonferroni correction applied for a baseline alpha = 0.05). Significant values are in bold.\n \nScientific Reports |         (2025) 15:1195 6| https://doi.org/10.1038/s41598-024-84850-x\nwww.nature.com/scientificreports/\nthe development of tailored abstractive summarization tools for the healthcare domain, enhance communication \namong medical professionals, improve documentation accuracy, and consequently facilitate better patient care.\nStrength and limitations\nThis exploratory study has several strengths. First, it produces an empirical comparison of pre-trained models \ndeployed for clinical dialogue summarization tasks evaluated by expert users (clinicians) which provided \na thoroughly evaluation of the models’ outputs, both quantitative and qualitative. Our study adds value to \nprevious ones as it is able to produce full-dialogue summarization, compared to previous research 16,28 that \nonly produced summaries of clinical dialogue snippets, as models were unable to capture the entire clinical \nconversations, given the lack of powerful models with enough context length. It additionally compares various \nautomated summarization evaluation methods, the established ROUGE family of metrics, and the newer, more \ncomprehensive UniEval. One of the key limitations of our study concerns the lack of multiple initialization seeds \nfor the fine-tuned models (BART-LSG-Conv and BART-DnC). Given that transformer models like BART are \nsensitive to their initial weights, using only a single initialization might not fully capture the models’ performance \nvariability. While the pronounced performance gap between ChatGPT and other models suggests that different \ninitialization parameters would unlikely affect our main conclusions, this limitation should be addressed in \nfuture work.\nOur findings also align with previous findings regarding the advantages of using UniEval 25, which are also \nsupported by our work, demonstrating its applicability to new data with context-specific domain knowledge \n(clinical medicine) and a distinct textual structure (dialogues). Moreover, we also produced an original \nLow (1–2) Medium (3) High (4–5)\nCoherency\nNon-sequential order: Each sentence is not in sequential order. (BART-\nDnC, Evaluator 5, Dialogue 1)\nMix of verb tenses: The mix of past and present tense makes this \ndifficult to read and understand. (BART-DnC, Evaluator 5, Dialogue 1)\nIncorporating dialogue: Starts somewhat \ncoherent, but degenerates about \nhalfway through when it incorporates \nsome dialogue (“Doctor:“) and begins \ndiscussing 22q11. (T5-Booksum, \nEvaluator 1, Dialogue 5)\nAdequate sequential order: The text \ncoherently documents the key findings from \nthe conversation in a sequential matter from \nthe patient’s symptoms to their allergies and \nfamily history and then to the examination \nand plan. (ChatGPT, Evaluator 5, Dialogue 1)\nConsistency\nIncorrect sentences: “Sometimes it is just the right side. Sometimes it \nis just the click. ” isn’t correct - it is only the right side, and the clicking is \nconsistent. (BART-LSG-conv, Evaluator 1, Dialogue 2)\nLack of accuracy: Sometimes it is a combination of pain relief, time, \ntime and possibly some specialized physiotherapy. ” appears to be saying \nthat he has used these treatments before, which isn’t accurate. (BART-\nLSG-conv, Evaluator 1, Dialogue 2)\nFactual errors: He is allergic and takes Diazepam five times a week. ” this \nis wrong - he is not allergic to anything, and only takes the diazepam \n1–2 times per week. The name is incorrect (listed as John, when the \ndocument says Joe). (BART-LSG-conv, Evaluator 1, Dialogue 2)\nHallucination: […]makes up multiple surgeries that the patient didn’t \nhave. ” (LongDialogSumm, Evaluator 1, Dialogue 4)\nOmitting (some) clinical information: \nThe majority of facts listed are correct, \nalthough some modest errors above. \nOmits significant amounts about \ndiagnosis, treatment and history/exams. \n(BART-DnC, Evaluator 1, Dialogue 3)\nConsistent but lacking subject matter \norder: Consistent, however, misses a lot \nof important clinical detail compared to \nother models. And does not arrange subject \nmatter within sentences in a sequence as \none would expect for a clinical summary. \n(LongDialogSumm, Evaluator 2, Dialogue 1)\nFluency\nLack of sense: Very poor-quality sentences, at times using phrases that \nare themselves non-sensical such as “feverish cough” . (T5-Booksum, \nEvaluator 3, Dialogue 1)\nRepetition: Not fluent. Long sentences, repetition in sentences. (BART-\nLSG-conv, Evaluator 5, Dialogue 4)\nUse of non-technical words: […] Text \nalso uses words such as ‘a whole bunch \nof colds’ and ‘bugs’ which are not of very \nhigh quality for medical documentation \npurposes: ‘He has head airiness and \npressure in his ears as well as a couple \nof times. ’ (BART-LSG-conv, Evaluator 5, \nDialogue 1)\nWell-written, correct grammar: All the \nsentences make sense and are well-written. \n(ChatGPT, Evaluator 1, Dialogue 1)\nWell-composed sentences make sense, and \ngrammar is good. (ChatGPT, Evaluator 1, \nDialogue 1)\nRelevance\nMissing facts: The summary is missing the majority of the relevant \nfacts including examination findings, diagnosis, medication \nprescribed, follow-up plan and previous tonsillitis episodes […]. \n(LongDialogSumm, Evaluator 5, Dialogue 1)\nIncorrect facts: Most facts are relevant, if \nsomewhat butchered. More problematic \nis that the facts are often wrong. Missing \nmuch of the content it should have. \n[…] Interprets examination finding \nincorrectly (“little lymph nodes consistent \nwith a virus or infection”) (T5-Booksum, \nEvaluator 1, Dialogue 2)\nImportant items are present: Summarizes \nmany of the important features of the case. \nCould mention the past medical history and \ncould also mention the plan for follow-up \nas well. Could also include exam findings. \n(BART-DnC, Evaluator 1, Dialogue 1)\nMissing detail: Contains most components \nof a good history […]. However, it did not \ncontain information including lack of regular \nmedications, lack of past medical history and \nsocial history[…] It also did not document \nthe examination findings or the time frame \nof his symptoms (2 weeks). (CHATGPT, \nEvaluator 5, Dialogue 1)\nClinical use\nUnsuitable: I do not think this summary could be used in a clinical \nsetting[…]. Its largest downfall is the lack of discussion regarding the \nmanagement and follow-up plan for this patient and in documenting an \nimpression or diagnosis. (BART-LSG-conv, Evaluator 5, Dialogue 1)\nIrrelevant information: It also documents a lot of irrelevant \ninformation and misinterprets some information discussed pertaining \nto pathogens and the symptoms experienced currently versus in \nprevious similar episodes of illness. (BART-LSG-conv, Evaluator 5, \nDialogue 1)\nMisinterpretation and lack of clinical information: Not usable in a \nreal-world clinical scenario due to misinterpretation of examination \nfindings and paucity of other clinical information required for a medical \nsummary. (BART-DnC, Evaluator 2, Dialogue 1)\nMissing important findings: This doesn’t \ninclude key exam findings and treatment \nplans. The facts are mostly correct. \nDiscusses x-ray results at too much \nlength. (LongDialogSumm, Evaluator 1, \nDialogue 3)\nReasonable summary, however, some \nobvious mistakes. Missing some content \nabout examination findings and the \nwhole treatment plan/advice. (T5-\nBooksum, Evaluator 1, Dialogue 4)\nConcise enough: Can be used to provide a \nconcise summary of the consultation, that \nthe physician can then corroborate/explore \nfurther with the original medical progress \nnote/consult notes. (CHATGPT, Evaluator 2, \nDialogue 3)\nUseful: Good note overall, but several \nminor mistakes as mentioned previously. \nThis could actually be useful for a clinician. \n(CHATGPT, Evaluator 1, Dialogue 1)\nTable 4. Human evaluation: examples by domain and reasons provided for each scoring.\n \nScientific Reports |         (2025) 15:1195 7| https://doi.org/10.1038/s41598-024-84850-x\nwww.nature.com/scientificreports/\nevaluation setting, comparing automated systems with human-generated summaries demonstrating that \nChatGPT produces summaries comparable to the human ones, with important implications for the clinical \ndocumentation applications.\nSeveral other limitations need to be highlighted. First, given the small number of dialogues in the dataset, \nwe could not extensively fine-tune models for the summarization task, dismissing potential improvements \nachievable if a larger dataset was used. Likewise, our small number of clinical dialogues samples for evaluation, \nlimits the generalizability of these findings and points to further evaluation with larger sets and across different \nspecialties and medical domains. However, in this exploratory study, we have detected a strong signal favouring \nsummaries generated by LLMs, consistently with the emerging literature exploring the capabilities of those \nmodels. Additionally, we have not explored in-depth parameter tuning, evaluated performance in a k-fold \nvalidation manner, possibly limiting the generalizability of results. In relation to reproducibility aspects and \nChatGPT, while additional configurations or prompting strategies could have been explored, after initial testing \nand given the onerous task of manual evaluation by human experts we decided to focus on the best performing \nprompt and default configuration (reflecting the most likely scenario for regular non-technical users). Lastly, it \nis important to note the subjective nature of human evaluation scores. We mitigated this by pooling estimates \nof 5 clinical expert evaluators, presenting the responses in a random order to avoid priming, and masking the \nautomatically generated summaries as well as the human-produced ones, to minimize potential interference \nwith the scoring.\nThe strength of our ratings is confirmed by the standardized metrics of reliability (ICC), which is also \nmaintained when compared with the automated metric UniEval. An additional limitation may have emerged \nfrom the quality of the human-generated summaries. As a single clinician produced those summaries, they \nmay not be representative of the summaries produced in the clinical setting, as well as they may vary from one \nclinician to another. Considering that these summaries were produced also with some stylistic constraints, this \nmight have penalized the ROUGE scoring of ChatGPT while also picturing the human-generated summaries as \nless stylistically adequate than expected. While not removing completely this limitation, we mitigated it by using \nanother clinician to review and suggest corrections with the human-generated summaries.\nAlthough the results of ChatGPT are encouraging an important limitation to its use surrounds the potential \ndata privacy concerns that may arise from using it to process private medical information. In the light of this, it \nis important to note that open-source alternative and smaller-scale models that can run on premises or on the \ndevice may pose better alternatives and will require further evaluation, which we plan to conduct in the future.\nRelation to previous research\nPrevious studies focusing on clinical dialogue summarization 16,29 have not produced conclusive results. First, \nthe length of the clinical conversations was unsuitable for earlier language models to process into a full, coherent \nsummary and were limited to summarize snippets of conversations. Second, given the paucity of clinical data, \nit remained challenging to produce models that were reliable and performed the task consistently 28, while \nsummarization of other types of documents such as news30 or law texts31 had larger corpora available for training. \nRecent advancements can be seen in the creation of a contest of synthetic clinical notes and summarization \nevaluation32.\nOur study also confirms the strong capabilities of the newer LLMs, illustrated by the GPT-derived models33. \nThese models have outperformed previous approaches in several tasks 34 including summarization, while \nnot requiring special fine-tuning or retraining of a custom model and being able to respond to a wide range \nof questions and use cases. More recently, studies applied to the context of clinical summarization had also \nappeared35,36, showcasing the benefits of this approach in different types of medical text such as radiology \nreports37 and clinical dialogues.\nThe results of our study are also in line with previous summarization metric evaluations that showed the \nsuboptimal quality of the ROUGE metrics 9 especially when applying to the medical domain 38 and increasing \ncontext length for multi-document summarization 39. Among the limitations that ROUGE encounters when \nprocessing long text, is its reliance on overlapping n-grams to calculate its scores. This may explain partially how \nmodels using a zero-shot approach (such as ChatGPT) may underperform when measured with ROUGE, as \nthey diverge more broadly from the expected summaries, particularly for long texts such as clinical dialogues. \nFurther exploration of ROUGE as a metric is needed, especially for evaluating when the generated summaries \ndiffer from the gold standard while maintaining their quality. Likewise, this study confirms previous findings \nproposing new, automated, unified summarization UniEval metrics25. When deployed in a different evaluation \nscenario of summarizing clinical dialogues, these metrics still produced similar results to human evaluators.\nFollow up research\nGiven the exploratory nature of our research, evaluating with a wider variety of clinical settings (primary \nvs. secondary care), different clinical specialties (with specific medical vocabulary and acronyms), as well as \nsite- and context-specific variations (different hospitals using different vocabulary) are vital to ensuring the \ngeneralizability of our findings. Additional research needs to explore whether LLMs consistently summarize \nclinical facts present in clinical dialogues, ensuring accuracy, completeness and clinical usefulness. Recently, \nopen-source LLMs have been released, including LLaMA39 and its derivatives40, and models trained on medical \ntexts41. The use of open-source LLMs may provide similar performance advantages in summarization, while \nmaintaining control over the model ownership and data governance. Moreover, the potential to be deployed \nlocally, or be fine-tuned for specific tasks is particularly important in a highly regulated environment such \nas healthcare. In terms of metrics and evaluation, future research may include additional evaluation metrics \nsuited for transformer-based architectures such as BERTScore42 or BARTScore43. Further metrics proposed for \nLLMs may include a more comprehensive approach such as an “ecosystemic” evaluation44. Additional benefits \nScientific Reports |         (2025) 15:1195 8| https://doi.org/10.1038/s41598-024-84850-x\nwww.nature.com/scientificreports/\nmay be ripped from an ensemble approach that combines a pretrained transformer to perform Named Entity \nRecognition, with an LLM such as a GPT or LLAMA tasked with the summarization and natural language \ngeneration parts.\nWhile not the focus of this study, an important area of research refers to integrating these text automation tools \nin day-to-day clinical practice. As previous findings have shown, there is sometimes little correlation between \nmodel development, its reported performance, and its implementation in practice 45. Exploring clinicians’ \nneeds, the fit of automated summarization tools into clinical pipelines, and safe implementation in a high-stake \nscenario such as medicine remain open questions. In relation to these findings, integration with current or future \nEHRs and exploring user experience and interaction aspects of clinical dialogue summarization is a crucial step \ntowards their swift and meaningful adoption, to ensure safety and usefulness, while complying with medico-\nlegal issues and potential clinicians’ resistance to change46.\nConclusions\nOur exploratory findings suggest that LLMs such as ChatGPT can effectively perform clinical dialogue \nsummarization tasks, consistently producing summaries not differing in quality from human-generated ones \nand outperforming previous approaches, while not requiring fine-tuning. In the LLM era, and especially when \nperforming long-text summarization, the performance of ROUGE-based metrics may not reflect the real \nperformance of the models, unfairly penalizing the models that have not seen domain-specific training data, \nwhile being more capable than their pre-trained counterparts. These findings question the usefulness of such \nmetrics, pointing at potentially replacing them with more comprehensive metrics, such as UniEval. Lastly, our \nresults indicate that clinical dialogue summarization is a feasible task in the era of LLMs. Exploring how to bring \nsummarization into practice, especially considering privacy concerns and the restricted nature of health records, \nremains an open question.\nData availability\nThe data that support the findings of this study are not openly available due to reasons of sensitivity and are \navailable from the corresponding author upon reasonable request. Data are located in controlled access data \nstorage at Macquarie University.\nReceived: 16 July 2024; Accepted: 27 December 2024\nReferences\n 1. Dymek, C. et al. Building the evidence-base to reduce electronic health record–related clinician burden. J. Am. Med. Inform. Assoc. \n00, 1–5 (2020).\n 2. Frintner, M. P . et al. The effect of electronic health record burden on pediatricians’ work–life balance and career satisfaction. Appl. \nClin. Inf. 12, 697–707 (2021).\n 3. Fortune, F . S. & Fry, E. Death by 1,000 clicks: Where electronic health records went wrong. Kaiser Health News (2019).  h t t p s : / / f o r t \nu n e . c o m / l o n g f o r m / m e d i c a l - r e c o r d s /     Accesed 4 November 2024.\n 4. Cerny, T., Rosemann, T., Tandjung, R. & Chmiel, C. Reasons for general practitioner shortage: A comparison between France and \nSwitzerland. Praxis (Bern 1994). 105, 619–636 (2016).\n 5. Farber, J., Siu, A. & Bloom, P . How much time do physicians spend providing care outside of office visits? Ann. Intern. Med. 147, \n693–698 (2007).\n 6. Y an, Q., Jiang, Z., Harbin, Z., Tolbert, P . H. & Davies, M. G. Exploring the relationship between electronic health records and \nprovider burnout: A systematic review. J. Am. Med. Inform. Assoc. 28, 1009–1021 (2021).\n 7. Coiera, E., Kocaballi, B., Halamka, J. & Laranjo, L. The digital scribe. NPJ Digit. Med. 1, 58 (2018).\n 8. Lin, C-Y . ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out 74–81 (Association for \nComputational Linguistics, Barcelona, Spain, 2004).\n 9. Akter, M., Bansal, N. & Karmaker, S. K. Revisiting automatic evaluation of extractive summarization task: Can we do better than \n{ROUGE}? In Findings of the Association for Computational Linguistics: ACL 2022  1547–1560 (Association for Computational \nLinguistics, Dublin, Ireland, 2022).\n 10. Papineni, K., Roukos, S., Ward, T. & Zhu, W-J. Bleu: A method for automatic evaluation of machine translation 311–318 (2002).\n 11. Pennington, J., Socher, R. & Manning, C. D. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference \non Empirical Methods in Natural Language Processing (EMNLP) 1532–1543 (2014).\n 12. Vaswani, A. Attention is all you need (Advances in Neural Information Processing Systems, 2017).\n 13. Lewis, M. et al. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. \nhttps://arxiv.org/abs/1910.13461 (2019).\n 14. Raffel, C. et al. Exploring the limits of transfer learning with a unified text-to-text transformer https://arxiv.org/abs/1910.10683(2019).\n 15. Gemini Team, Anil R, Borgeaud S, et al. Gemini: A family of highly capable multimodal models.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / a r X i v . 2 \n3 1 2 . 1 1 8 0 5     (2023).\n 16. Navarro, D. F ., Dras, M. & Berkovsky, S. Few-shot fine-tuning SOTA summarization models for medical dialogues. In Proceedings of \nthe 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: \nStudent Research Workshop 254–266 (Association for Computational Linguistics, Hybrid: Seattle, Washington, 2022).\n 17. Condevaux, C. & Harispe, S. LSG Attention: Extrapolation of pretrained transformers to long sequences.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 \n/ a r X i v . 2 2 1 0 . 1 5 4 9 7     (2022).\n 18. Gidiotis, A. & Tsoumakas, G. A Divide-and-conquer approach to the summarization of long documents.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 \n/ a r X i v . 2 0 0 4 . 0 6 1 9 0     (2020).\n 19. Zhang, Y . et al. An Exploratory Study on Long Dialogue Summarization (What Works and What’s Next, 2021).\n 20. Guo, M. et al. LongT5: Efficient text-to-text transformer for long sequences (2022).\n 21. Kryściński, W ., Rajani, N., Agarwal, D., Xiong, C. & Radev, D. BookSum: A collection of datasets for long-form narrative \nsummarization. https://doi.org/10.48550/arXiv.2105.08209 (2022).\n 22. Brown, T. et al. Language models are few-shot learners. Adv. Neural. Inf. Process. Syst. 33, 1877–1901 (2020).\n 23. Quiroz, J. C. et al. Identifying relevant information in medical conversations to summarize a clinician-patient encounter. Health \nInf. J. 26, 2906–2914 (2020).\n 24. Lin, C-Y . Rouge: A package for automatic evaluation of summaries 74–81 (2004).\nScientific Reports |         (2025) 15:1195 9| https://doi.org/10.1038/s41598-024-84850-x\nwww.nature.com/scientificreports/\n 25. Zhong, M. et al. Towards a unified multi-dimensional evaluator for text generation (2022). https://arxiv.org/abs/2210.07197v1. \nAccessed 18 Mar 2023.\n 26. Braun, V . & Clarke, V . Using thematic analysis in psychology. Qual. Res. Psychol. 3, 77–101 (2006).\n 27. Kocaballi, A. B. et al. A network model of activities in primary care consultations. J. Am. Med. Inform. Assoc. 26, 1074–1082 (2019).\n 28. Chintagunta, B., Katariya, N., Amatriain, X. & Kannan, A. Medically aware GPT-3 as a data generator for medical dialogue \nsummarization. In Proceedings of the 6th Machine Learning for Healthcare Conference 354–372 (PMLR, 2021).\n 29. Xie, Q., Luo, Z., Wang, B. & Ananiadou, S. A survey for biomedical text summarization: From pre-trained to large language \nmodels. https://arxiv.org/abs/2304.08763 (2023).\n 30. See, A., Liu, P . J. & Manning, C. D. Get to the point: summarization with pointer-generator networks.  h t t p s : / / a r x i v . o r g / a b s / 1 7 0 4 . 0 \n4 3 6 8     (2017).\n 31. Ben Abacha, A., Yim, W ., Fan, Y . & Lin, T. An empirical study of clinical note generation from doctor-patient encounters. In  \nProceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics  (eds Vlachos, A. & \nAugenstein, I.) 2291–2302 (Association for Computational Linguistics, Dubrovnik, Croatia, 2023).\n 32. Abacha, A. B., Yim, W ., Fan, Y . & Lin, T. An empirical study of clinical note generation from doctor-patient encounters 2291–2302 \n(2023).\n 33. OpenAI. GPT-4 Technical Report. https://doi.org/10.48550/arXiv.2303.08774 (2023)\n 34. Bubeck, S. et al. Sparks of artificial general intelligence: Early experiments with gpt-4. https://arxiv.org/abs/2303.12712 (2023).\n 35. Chen, Y-W . & Hirschberg, J. Exploring robustness in doctor-patient conversation summarization: An analysis of out-of-domain \nSOAP notes. In Proceedings of the 6th Clinical Natural Language Processing Workshop (eds Naumann, T., Ben Abacha, A., Bethard, \nS., Roberts, K., Bitterman, D.). 1–9 (Association for Computational Linguistics, Mexico City, Mexico, 2024).\n 36. Van Veen, D. et al. Adapted large language models can outperform medical experts in clinical text summarization. Nat. Med. 30, \n1134–1142 (2024).\n 37. López-Úbeda, P ., Martín-Noguerol, T., Díaz-Angulo, C. & Luna, A. Evaluation of large language models performance against \nhumans for summarizing MRI knee radiology reports: A feasibility study. Int. J. Med. Inform.. 187, 105443 (2024).\n 38. Campillos-Llanos, L. et al. Replace, Paraphrase or fine-tune? Evaluating automatic simplification for medical texts in Spanish. In \nProceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-\nCOLING 2024) (eds Calzolari N, Kan M-Y , Hoste V , Lenci A, Sakti S, Xue N) 13929–13945 (ELRA and ICCL, Torino, Italia, 2024).\n 39. Touvron, H. et al. LLaMA: Open and efficient foundation language models. https://doi.org/10.48550/arXiv.2302.13971 (2023). \nAccesed 4 November 2024.\n 40. Taori Rohan, G. et al. Hashimoto tatsunori B alpaca: A strong, replicable instruction-following model.  h t t p s : / / c r f m . s t a n f o r d . e d u / \n2 0 2 3 / 0 3 / 1 3 / a l p a c a . h t m l     . Accessed 10 May 2023.\n 41. Wu, C., Zhang, X., Zhang, Y ., Wang, Y . & Xie, W . PMC-LLaMA: Further finetuning LLaMA on medical papers.  h t t p s : / / d o i . o r g / 1 0 \n. 4 8 5 5 0 / a r X i v . 2 3 0 4 . 1 4 4 5 4     (2023). Accesed 4 November 2024.\n 42. Zhang, T., Kishore, V ., Wu, F ., Weinberger, K. Q. & Artzi, Y . Bertscore: Evaluating text generation with bert.  h t t p s : / / a r x i v . o r g / a b s / 1 \n9 0 4 . 0 9 6 7 5     (2019).\n 43. Yuan, W ., Neubig, G. & Liu, P . Bartscore: Evaluating generated text as text generation. Adv. Neural. Inf. Process. Syst. 34, 27263–\n27277 (2021).\n 44. Coiera, E. & Fraile-Navarro, D. AI as an Ecosystem—Ensuring Generative AI Is Safe and Effective (NEJM AI AIp2400611, 2024).\n 45. Verma, A. A. et al. Implementing machine learning in medicine. Cmaj 193, E1351–E1357 (2021).\n 46. Navarro, D. F ., Kocaballi, A. B., Dras, M. & Berkovsky, S. Collaboration, not confrontation: Understanding general practitioners’ \nattitudes towards natural language and text automation in clinical practice. ACM Trans. Comput-Hum Interact.  h t t p s : / / d o i . o r g / 1 0 . \n1 1 4 5 / 3 5 6 9 8 9 3     (2022).\nAuthor contributions\nD.F .N. conducted the conceptualization, investigation, data curation, formal analysis, methodology, project \nadministration, writing of the original draft, and writing review and editing. E.C. performed formal analysis, \nmethodology, and writing review and editing. T.W ., Z.T., N.A., A.S., A.C., and A.A.L. handled investigation, data \ncuration, and writing review and editing. M.D. contributed to conceptualization and writing review and editing. \nS.B. managed investigation, supervision, methodology, conceptualization, formal analysis, and writing review \nand editing. All authors reviewed the manuscript.\nFunding\nThis study was not funded. David Fraile Navarro was supported by an iMQRES scholarship.\nDeclarations\nEthics approval and consent to participate\nHuman Ethics and Consent to Participate declarations: not applicable.\nCompeting interests\nThe authors declare no competing interests.\nOriginal data collection Ethics Approval available at: Kocaballi AB, Coiera E, Tong HL, White SJ, Quiroz JC, \nRezazadegan F , Willcock S, Laranjo L (2019) A network model of activities in primary care consultations. \nJournal of the American Medical Informatics Association 26:1074–1082.\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 4 - 8 4 8 5 0 - x     .  \nCorrespondence and requests for materials should be addressed to D.F .N.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nScientific Reports |         (2025) 15:1195 10| https://doi.org/10.1038/s41598-024-84850-x\nwww.nature.com/scientificreports/\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |         (2025) 15:1195 11| https://doi.org/10.1038/s41598-024-84850-x\nwww.nature.com/scientificreports/"
}