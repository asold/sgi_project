{
    "title": "AST: Audio Spectrogram Transformer",
    "url": "https://openalex.org/W3146639881",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2117679203",
            "name": "Gong Yuan",
            "affiliations": [
                "Artificial Intelligence in Medicine (Canada)"
            ]
        },
        {
            "id": "https://openalex.org/A4286961604",
            "name": "Chung, Yu-An",
            "affiliations": [
                "Artificial Intelligence in Medicine (Canada)"
            ]
        },
        {
            "id": "https://openalex.org/A2745591341",
            "name": "Glass, James",
            "affiliations": [
                "Artificial Intelligence in Medicine (Canada)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3164279099",
        "https://openalex.org/W2889374687",
        "https://openalex.org/W2052666245",
        "https://openalex.org/W2132037657",
        "https://openalex.org/W2745357708",
        "https://openalex.org/W3025581723",
        "https://openalex.org/W3049446265",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3126565544",
        "https://openalex.org/W2912934387",
        "https://openalex.org/W2399733683",
        "https://openalex.org/W3015399080",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W3017216675",
        "https://openalex.org/W2059652044",
        "https://openalex.org/W2797583228",
        "https://openalex.org/W2936774411",
        "https://openalex.org/W2012296273",
        "https://openalex.org/W2964328535",
        "https://openalex.org/W3094550259",
        "https://openalex.org/W2963173418",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3158553615",
        "https://openalex.org/W3097777922",
        "https://openalex.org/W2144005487",
        "https://openalex.org/W3045062880",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W2090777335",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W1538131130",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W2593116425"
    ],
    "abstract": "In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the Audio Spectrogram Transformer (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech Commands V2.",
    "full_text": "AST: Audio Spectrogram Transformer\nYuan Gong, Yu-An Chung, James Glass\nMIT Computer Science and Artiﬁcial Intelligence Laboratory, Cambridge, MA 02139, USA\n{yuangong, andyyuan, glass}@mit.edu\nAbstract\nIn the past decade, convolutional neural networks (CNNs)\nhave been widely adopted as the main building block for end-\nto-end audio classiﬁcation models, which aim to learn a direct\nmapping from audio spectrograms to corresponding labels. To\nbetter capture long-range global context, a recent trend is to\nadd a self-attention mechanism on top of the CNN, forming a\nCNN-attention hybrid model. However, it is unclear whether\nthe reliance on a CNN is necessary, and if neural networks\npurely based on attention are sufﬁcient to obtain good perfor-\nmance in audio classiﬁcation. In this paper, we answer the ques-\ntion by introducing the Audio Spectrogram Transformer(AST),\nthe ﬁrst convolution-free, purely attention-based model for au-\ndio classiﬁcation. We evaluate AST on various audio classiﬁ-\ncation benchmarks, where it achieves new state-of-the-art re-\nsults of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50,\nand 98.1% accuracy on Speech Commands V2.\nIndex Terms: audio classiﬁcation, self-attention, Transformer\n1. Introduction\nWith the advent of deep neural networks, over the last decade\naudio classiﬁcation research has moved from models based\non hand-crafted features [1, 2] to end-to-end models that di-\nrectly map audio spectrograms to corresponding labels [3, 4, 5].\nSpeciﬁcally, convolutional neural networks (CNNs) [6] have\nbeen widely used to learn representations from raw spectro-\ngrams for end-to-end modeling, as the inductive biases inherent\nto CNNs such as spatial locality and translation equivariance\nare believed to be helpful. In order to better capture long-range\nglobal context, a recent trend is to add a self-attention mech-\nanism on top of the CNN. Such CNN-attention hybrid mod-\nels have achieved state-of-the-art (SOTA) results for many au-\ndio classiﬁcation tasks such as audio event classiﬁcation [7, 8],\nspeech command recognition [9], and emotion recognition [10].\nHowever, motivated by the success of purely attention-based\nmodels in the vision domain [11, 12, 13], it is reasonable to ask\nwhether a CNN is still essential for audio classiﬁcation.\nTo answer the question, we introduce the Audio Spectro-\ngram Transformer (AST), a convolution-free, purely attention-\nbased model that is directly applied to an audio spectrogram\nand can capture long-range global context even in the lowest\nlayers. Additionally, we propose an approach for transferring\nknowledge from the Vision Transformer (ViT) [12] pretrained\non ImageNet [14] to AST, which can signiﬁcantly improve the\nperformance. The advantages of AST are threefold. First, AST\nhas superior performance: we evaluate AST on a variety of au-\ndio classiﬁcation tasks and datasets including AudioSet [15],\nESC-50 [16] and Speech Commands [17]. AST outperforms\nstate-of-the-art systems on all these datasets. Second, AST nat-\nurally supports variable-length inputs and can be applied to dif-\nferent tasks without any change of architecture. Speciﬁcally, the\nCode at https://github.com/YuanGongND/ast.\nTransformer Encoder\nLinear ProjectionE[CLS]\nP[0]\nE[1]E[2]E[3]E[4]E[5]E[6]E[7]E[8]\nP[1]P[2]P[3]P[4]P[5]P[6]P[7]P[8]\nLinearOutput\nPatch Split with Overlap\nPatchEmbedding\nPositional Embedding\nPatch Embedding\nInput Spectrogram\n1 2 3 456 78\n1 2 3 4 5 6 7 8\nFigure 1: The proposed audio spectrogram transformer (AST)\narchitecture. The 2D audio spectrogram is split into a sequence\nof 16×16 patches with overlap, and then linearly projected to\na sequence of 1-D patch embeddings. Each patch embedding\nis added with a learnable positional embedding. An additional\nclassiﬁcation token is prepended to the sequence. The output\nembedding is input to a Transformer, and the output of the clas-\nsiﬁcation token is used for classiﬁcation with a linear layer.\nmodels we use for all aforementioned tasks have the same archi-\ntecture while the input lengths vary from 1 sec. (Speech Com-\nmands) to 10 sec. (AudioSet). In contrast, CNN-based models\ntypically require architecture tuning to obtain optimal perfor-\nmance for different tasks. Third, comparing with SOTA CNN-\nattention hybrid models, AST features a simpler architecture\nwith fewer parameters, and converges faster during training. To\nthe best of our knowledge, AST is the ﬁrst purely attention-\nbased audio classiﬁcation model.\nRelated Work The proposed Audio Spectrogram Trans-\nformer, as the name suggests, is based on the Transformer ar-\nchitecture [18], which was originally proposed for natural lan-\nguage processing tasks. Recently, the Transformer has also\nbeen adapted for audio processing, but is typically used in\nconjunction with a CNN [19, 20, 21]. In [19, 20], the au-\nthors stack a Transformer on top of a CNN, while in [21],\nthe authors combine a Transformer and a CNN in each model\nblock. Other efforts combine CNNs with simpler attention\nmodules [8, 7, 9]. The proposed AST differs from these stud-\nies in that it is convolution-free and purely based on attention\nmechanisms. The closest work to ours is the Vision Trans-\nformer (ViT) [11, 12, 13], which is a Transformer architecture\nfor vision tasks. AST and ViT have similar architectures but\nViT has only been applied to ﬁxed-dimensional inputs (images)\nwhile AST can process variable-length audio inputs. In addi-\ntion, we propose an approach to transfer knowledge from Ima-\ngeNet pretrained ViT to AST. We also conduct extensive exper-\niments to show the design choice of AST on audio tasks.\narXiv:2104.01778v3  [cs.SD]  8 Jul 2021\n2. Audio Spectrogram Transformer\n2.1. Model Architecture\nFigure 1 illustrates the proposed Audio Spectrogram Trans-\nformer (AST) architecture. First, the input audio waveform of t\nseconds is converted into a sequence of 128-dimensional log\nMel ﬁlterbank (fbank) features computed with a 25ms Ham-\nming window every 10ms. This results in a128 ×100t spectro-\ngram as input to the AST. We then split the spectrogram into a\nsequence of N 16×16 patches with an overlap of 6 in both time\nand frequency dimension, where N = 12⌈(100t −16)/10⌉is\nthe number of patches and the effective input sequence length\nfor the Transformer. We ﬂatten each16×16 patch to a 1D patch\nembedding of size 768 using a linear projection layer. We re-\nfer to this linear projection layer as the patch embedding layer.\nSince the Transformer architecture does not capture the input\norder information and the patch sequence is also not in tem-\nporal order, we add a trainable positional embedding (also of\nsize 768) to each patch embedding to allow the model to cap-\nture the spatial structure of the 2D audio spectrogram.\nSimilar to [22], we append a [CLS] token at the beginning\nof the sequence. The resulting sequence is then input to the\nTransformer. A Transformer consists of several encoder and\ndecoder layers. Since AST is designed for classiﬁcation tasks,\nwe only use the encoder of the Transformer. Intentionally, we\nuse the original Transformer encoder [18] architecture without\nmodiﬁcation. The advantages of this simple setup are 1) the\nstandard Transformer architecture is easy to implement and re-\nproduce as it is off-the-shelf in TensorFlow and PyTorch, and\n2) we intend to apply transfer learning for AST, and a stan-\ndard architecture makes transfer learning easier. Speciﬁcally,\nthe Transformer encoder we use has an embedding dimension\nof 768, 12 layers, and 12 heads, which are the same as those\nin [12, 11]. The Transformer encoder’s output of the [CLS]\ntoken serves as the audio spectrogram representation. A linear\nlayer with sigmoid activation maps the audio spectrogram rep-\nresentation to labels for classiﬁcation.\nStrictly speaking, the patch embedding layer can be viewed\nas a single convolution layer with a large kernel and stride size,\nand the projection layer in each Transformer block is equivalent\nto 1×1 convolution. However, the design is different from con-\nventional CNNs that have multiple layers and small kernel and\nstride sizes. These Transformer models are usually referred to\nas convolution-free to distinguish them from CNNs [11, 12].\n2.2. ImageNet Pretraining\nOne disadvantage of the Transformer compared with CNNs is\nthat the Transformer needs more data to train [11]. In [11],\nthe authors point out that the Transformer only starts to out-\nperform CNNs when the amount of data is over 14 million for\nimage classiﬁcation tasks. However, audio datasets typically\ndo not have such large amounts of data, which motivates us\nto apply cross-modality transfer learning to AST since images\nand audio spectrograms have similar formats. Transfer learn-\ning from vision tasks to audio tasks has been previously stud-\nied in [23, 24, 25, 8], but only for CNN-based models, where\nImageNet-pretrained CNN weights are used as initial CNN\nweights for audio classiﬁcation training. In practice, it is com-\nputationally expensive to train a state-of-the-art vision model,\nbut many commonly used architectures (e.g., ResNet [26], Ef-\nﬁcientNet [27]) have off-the-shelf ImageNet-pretrained mod-\nels for both TensorFlow and PyTorch, making transfer learning\nmuch easier. We also follow this regime by adapting an off-the-\nshelf pretrained Vision Transformer (ViT) to AST.\nWhile ViT and AST have similar architectures (e.g., both\nuse a standard Transformer, same patch size, same embedding\nsize), they are not same. Therefore, a few modiﬁcations need to\nmake for the adaptation. First, the input of ViT is a 3-channel\nimage while the input to the AST is a single-channel spectro-\ngram, we average the weights corresponding to each of the\nthree input channels of the ViT patch embedding layer and use\nthem as the weights of the AST patch embedding layer. This\nis equivalent to expanding a single-channel spectrogram to 3-\nchannels with the same content, but is computationally more\nefﬁcient. We also normalize the input audio spectrogram so that\nthe dataset mean and standard deviation are 0 and 0.5, respec-\ntively. Second, the input shape of ViT is ﬁxed (either224 ×224\nor 384 ×384), which is different from a typical audio spectro-\ngram. In addition, the length of an audio spectrogram can be\nvariable. While the Transformer naturally supports variable in-\nput length and can be directly transferred from ViT to AST, the\npositional embedding needs to be carefully processed because\nit learns to encode the spatial information during the ImageNet\ntraining. We propose a cut and bi-linear interpolate method for\npositional embedding adaptation. For example, for a ViT that\ntakes 384 ×384 image input and uses a patch size of 16 ×16,\nthe number of patches and corresponding positional embedding\nis 24 ×24 = 576 (ViT splits patches without overlap). An\nAST that takes 10-second audio input has 12 ×100 patches,\neach patch needs a positional embedding. We therefore cut\nthe ﬁrst dimension and interpolate the second dimension of the\n24 ×24 ViT positional embedding to 12 ×100 and use it as\nthe positional embedding for the AST. We directly reuse the\npositional embedding for the [CLS] token. By doing this we\nare able to transfer the 2D spatial knowledge from a pretrained\nViT to the AST even when the input shapes are different. Fi-\nnally, since the classiﬁcation task is essentially different, we\nabandon the last classiﬁcation layer of the ViT and reinitialize\na new one for AST. With this adaptation framework, the AST\ncan use various pretrained ViT weights for initialization. In this\nwork, we use pretrained weights of a data-efﬁcient image Trans-\nformer (DeiT) [12], which is trained with CNN knowledge dis-\ntillation, 384 ×384 images, has 87M parameters, and achieves\n85.2% top-1 accuracy on ImageNet 2012. During ImageNet\ntraining, DeiT has two [CLS] tokens; we average them as a\nsingle [CLS] token for audio training.\n3. Experiments\nIn this section, we focus on evaluating the AST on AudioSet\n(Section 3.1) as weakly-labeled audio event classiﬁcation is one\nof the most challenging audio classiﬁcation tasks. We present\nour primary AudioSet results and ablation study in Section 3.1.2\nand Section 3.1.3, respectively. We then present our experi-\nments on ESC-50 and Speech Commands V2 in Section 3.2.\n3.1. AudioSet Experiments\n3.1.1. Dataset and Training Details\nAudioSet [15] is a collection of over 2 million 10-second au-\ndio clips excised from YouTube videos and labeled with the\nsounds that the clip contains from a set of 527 labels. The bal-\nanced training, full training, and evaluation set contains 22k,\n2M, and 20k samples, respectively. For AudioSet experiments,\nwe use the exact same training pipeline with [8]. Speciﬁcally,\nwe use ImageNet pretraining (as described in Section 2.2), bal-\nanced sampling (for full set experiments only), data augmenta-\nTable 1: Performance comparison of AST and previous methods\non AudioSet.\nModel\nArchitecture\nBalanced\nmAP\nFull\nmAP\nBaseline [15] CNN+MLP - 0.314\nPANNs [7] CNN+Attention 0.278 0.439\nPSLA [8] (Single) CNN+Attention 0.319 0.444\nPSLA (Ensemble-S) CNN+Attention 0.345 0.464\nPSLA (Ensemble-M) CNN+Attention 0.362 0.474\nAST (Single) Pure Attention 0.347\n± 0.001\n0.459\n± 0.000\nAST (Ensemble-S) Pure Attention 0.363 0.475\nAST (Ensemble-M) Pure Attention 0.378 0.485\ntion (including mixup [28] with mixup ratio =0.5 and spectro-\ngram masking [29] with max time mask length of 192 frames\nand max frequency mask length of 48 bins), and model aggrega-\ntion (including weight averaging [30] and ensemble [31]). We\ntrain the model with a batch size of 12, the Adam optimizer [32],\nand use binary cross-entropy loss. We conduct experiments on\nthe ofﬁcial balanced and full training set and evaluate on the Au-\ndioSet evaluation set. For balanced set experiments, we use an\ninitial learning rate of 5e-5 and train the model for 25 epochs,\nthe learning rate is cut into half every 5 epoch after the 10th\nepoch. For full set experiments, we use an initial learning rate\nof 1e-5 and train the model for 5 epochs, the learning rate is\ncut into half every epoch after the 2nd epoch. We use the mean\naverage precision (mAP) as our main evaluation metric.\n3.1.2. AudioSet Results\nWe repeat each experiment three times with the same setup but\ndifferent random seeds and report the mean and standard devi-\nation. When AST is trained with the full AudioSet, the mAP at\nthe last epoch is 0.448 ±0.001. As in [8], we also use weight\naveraging [30] and ensemble [31] strategies to further improve\nthe performance of AST. Speciﬁcally, for weight averaging, we\naverage all weights of the model checkpoints from the ﬁrst to\nthe last epoch. The weight-averaged model achieves an mAP of\n0.459±0.000, which is our best single model (weight averag-\ning does not increase the model size). For ensemble, we eval-\nuate two settings: 1) Ensemble-S: we run the experiment three\ntimes with the exact same setting, but with a different random\nseed. We then average the output of the last checkpoint model of\neach run. In this setting, the ensemble model achieves an mAP\nof 0.475; 2) Ensemble-M: we ensemble models trained with\ndifferent settings, speciﬁcally, we ensemble the three models\nin Ensemble-S together with another three models trained with\ndifferent patch split strategies (described in Section 3.1.3 and\nshown in Table 5). In this setting, the ensemble model achieves\nan mAP of 0.485, this is our best full model on AudioSet. As\nshown in Table 1, the proposed AST outperforms the previous\nbest system in [8] in all settings. Note that we use the same\ntraining pipeline with [8] and [8] also use ImageNet pretrain-\ning, so it is a fair comparison. In addition, we use fewer models\n(6) for our best ensemble models than [8] (10). Finally, it is\nworth mentioning that AST training converges quickly; AST\nonly needs 5 training epochs, while in [8], the CNN-attention\nhybrid model is trained for 30 epochs.\nWe also conduct experiments with the balanced AudioSet\n(about 1% of the full set) to evaluate the performance of AST\nwhen the training data volume is smaller. For weight averag-\ning, we average all weights of the model checkpoints of the\nTable 2: Performance impact due to ImageNet pretraining.\n“Used” denotes the setting used by our optimal AST model.\nBalanced Set Full Set\nNo Pretrain 0.148 0.366\nImageNet Pretrain (Used) 0.347 0.459\nTable 3: Performance of AST models initialized with different\nViT weights on balanced AudioSet and corresponding ViT mod-\nels’ top-1 accuracy on ImageNet 2012. (* Model is trained\nwithout patch split overlap due to memory limitation.)\n# Params ImageNet AudioSet\nViT Base [11] 86M 0.846 0.320\nViT Large [11]* 307M 0.851 0.330\nDeiT w/o Distill [12] 86M 0.829 0.330\nDeiT w/ Distill (Used) 87M 0.852 0.347\nlast 20 epochs. For Ensemble-S, we follow the same setting\nused for the full AudioSet experiment; for Ensemble-M, we in-\nclude 11 models trained with different random seeds (Table 1),\ndifferent pretrained weights (Table 3), different positional em-\nbedding interpolation (Table 4), and different patch split strate-\ngies (Table 5). The single, Ensemble-S, and Ensemble-M model\nachieve 0.347±0.001, 0.363, and 0.378, respectively, all outper-\nform the previous best system. This demonstrates that AST can\nwork better than CNN-attention hybrid models even when the\ntraining set is relatively small.\n3.1.3. Ablation Study\nWe conduct a series of ablation studies to illustrate the design\nchoices for the AST. To save compute, we mainly conduct ab-\nlation studies with the balanced AudioSet. For all experiments,\nwe use weight averaging but do not use ensembles.\nImpact of ImageNet Pretraining.We compare ImageNet pre-\ntrained AST and randomly initialized AST. As shown in Ta-\nble 2, ImageNet pretrained AST noticeably outperforms ran-\ndomly initialized AST for both balanced and full AudioSet ex-\nperiments. The performance improvement of ImageNet pre-\ntraining is more signiﬁcant when the training data volume is\nsmaller, demonstrating that ImageNet pretraining can greatly\nreduce the demand for in-domain audio data for AST. We fur-\nther study the impact of pretrained weights used. As shown in\nTable 3, we compare the performance of AST models initialized\nwith pretrained weights of ViT-Base, ViT-Large, and DeiT mod-\nels. These models have similar architectures but are trained with\ndifferent settings. We made the necessary architecture modiﬁ-\ncations for AST to reuse the weights. We ﬁnd that AST using\nthe weights of the DeiT model with distillation that performs\nbest on ImageNet2012 also performs best on AudioSet.\nImpact of Positional Embedding Adaptation.As mentioned\nin Section 2.2, we use a cut and bi-linear interpolation approach\nfor positional embedding adaptation when transferring knowl-\nedge from the Vision Transformer to the AST. We compare it\nwith a pretrained AST model with a randomly initialized posi-\ntional embedding. As shown in Table 4, we ﬁnd reinitializing\nthe positional embedding does not completely break the pre-\ntrained model as the model still performs better than a fully\nrandomly reinitialized model, but it does lead to a noticeable\nperformance drop compared with the proposed adaptation ap-\nproach. This demonstrates the importance of transferring spatial\nTable 4: Performance impact due to various positional embed-\nding adaptation settings.\nBalanced Set\nReinitialize 0.305\nNearest Neighbor Interpolation 0.346\nBilinear Interpolation (Used) 0.347\nTable 5: Performance impact due to various patch overlap size.\n# Patches Balanced Set Full Set\nNo Overlap 512 0.336 0.451\nOverlap-2 657 0.342 0.456\nOverlap-4 850 0.344 0.455\nOverlap-6 (Used) 1212 0.347 0.459\nTable 6: Performance impact due to various patch shape and\nsize. All models are trained with no patch split overlap.\n# Patches w/o Pretrain w/ Pretrain\n128×2 512 0.154 -\n16×16 (Used) 512 0.143 0.336\n32×32 128 0.139 -\nknowledge. Bi-linear interpolation and nearest-neighbor inter-\npolation do not result in a big difference.\nImpact of Patch Split Overlap.We compare the performance\nof models trained with different patch split overlap [13]. As\nshown in Table 5, the performance improves with the overlap\nsize for both balanced and full set experiments. However, in-\ncreasing the overlap also leads to longer patch sequence inputs\nto the Transformer, which will quadratically increase the com-\nputational overhead. Even with no patch split overlap, AST can\nstill outperform the previous best system in [8].\nImpact of Patch Shape and Size. As mentioned in Sec-\ntion 2.1, we split the audio spectrogram into 16 ×16 square\npatches, so the input sequence to the Transformer cannot be in\ntemporal order. We hope the positional embedding can learn\nto encode the 2D spatial information. An alternative way to\nsplit the patch is slicing the audio spectrogram into rectangu-\nlar patches in the temporal order. We compare both methods\nin Table 6, when the area of the patch is the same (256), using\n128 ×2 rectangle patches leads to better performance than us-\ning 16 ×16 square patches when both models are trained from\nscratch. However, considering there is no 128 ×2 patch based\nImageNet pretrained models, using 16 ×16 patches is still the\ncurrent optimal solution. We also compare using patches with\ndifferent sizes, smaller size patches lead to better performance.\n3.2. Results on ESC-50 and Speech Commands\nThe ESC-50 [16] dataset consists of 2,000 5-second environ-\nmental audio recordings organized into 50 classes. The cur-\nrent best results on ESC-50 are 86.5% accuracy (trained from\nscratch, SOTA-S) [33] and 94.7% accuracy (with AudioSet pre-\ntraining, SOTA-P) [7]. We compare AST with the SOTA mod-\nels in these two settings, speciﬁcally, we train an AST model\nwith only ImageNet pretraining (AST-S) and an AST model\nwith ImageNet and AudioSet pretraining (AST-P). We train\nboth models with frequency/time masking [29] data augmen-\ntation, a batch size of 48, and the Adam optimizer [32] for 20\nTable 7: Comparing AST and SOTA models on ESC-50 and\nSpeech Commands. “-S” and “-P” denotes model trained with-\nout and with additional audio data, respectively.\nESC-50 Speech Commands V2 (35 classes)\nSOTA-S 86.5 [33] 97.4 [34]\nSOTA-P 94.7 [7] 97.7 [35]\nAST-S 88.7 ±0.7 98.11±0.05\nAST-P 95.6±0.4 97.88±0.03\nepochs. We use an initial learning rate of 1e-4 and 1e-5 for AST-\nS and AST-P, respectively, and decrease the learning rate with\na factor of 0.85 every epoch after the 5th epoch. We follow the\nstandard 5-fold cross-validation to evaluate our model, repeat\neach experiment three times, and report the mean and standard\ndeviation. As shown in Table 7, AST-S achieves 88.7±0.7 and\nAST-P achieves 95.6 ±0.4, both outperform SOTA models in\nthe same setting. Of note, although ESC-50 has 1,600 training\nsamples for each fold, AST still works well with such a small\namount of data even without AudioSet pretraining.\nSpeech Commands V2 [17] is a dataset consists of 105,829\n1-second recordings of 35 common speech commands. The\ntraining, validation, and test set contains 84,843, 9,981, and\n11,005 samples, respectively. We focus on the 35-class clas-\nsiﬁcation task, the SOTA model on Speech Commands V2 (35-\nclass classiﬁcation) without additional audio data pretraining is\nthe time-channel separable convolutional neural network [34],\nwhich achieves 97.4% on the test set. In [35], a CNN model\npretrained with additional 200 million YouTube audio achieves\n97.7% on the test set. We also evaluate AST in these two set-\ntings. Speciﬁcally, we train an AST model with only ImageNet\npretraining (AST-S) and an AST model with ImageNet and\nAudioSet pretraining (AST-P). We train both models with fre-\nquency and time masking [29], random noise, and mixup [28]\naugmentation, a batch size of 128, and the Adam optimizer [32].\nWe use an initial learning rate of 2.5e-4 and decrease the learn-\ning rate with a factor of 0.85 every epoch after the 5th epoch.\nWe train the model for up to 20 epochs, and select the best\nmodel using the validation set, and report the accuracy on\nthe test set. We repeat each experiment three times and re-\nport the mean and standard deviation. AST-S model achieves\n98.11±0.05, outperforms the SOTA model in [9]. In addition,\nwe ﬁnd AudioSet pretraining unnecessary for the speech com-\nmand classiﬁcation task as AST-S outperforms AST-P. To sum-\nmarize, while the input audio length varies from 1 sec. (Speech\nCommands), 5 sec. (ESC-50) to 10 sec. (AudioSet) and content\nvaries from speech (Speech Commands) to non-speech (Au-\ndioSet and ESC-50), we use a ﬁxed AST architecture for all\nthree benchmarks and achieve SOTA results on all of them. This\nindicates the potential for AST use as a generic audio classiﬁer.\n4. Conclusions\nOver the last decade, CNNs have become a common model\ncomponent for audio classiﬁcation. In this work, we ﬁnd CNNs\nare not indispensable, and introduce the Audio Spectrogram\nTransformer (AST), a convolution-free, purely attention-based\nmodel for audio classiﬁcation which features a simple architec-\nture and superior performance.\n5. Acknowledgements\nThis work is partly supported by Signify.\n6. References\n[1] F. Eyben, F. Weninger, F. Gross, and B. Schuller, “Recent de-\nvelopments in openSMILE, the Munich open-source multimedia\nfeature extractor,” inMultimedia, 2013.\n[2] B. Schuller, S. Steidl, A. Batliner, A. Vinciarelli, K. Scherer,\nF. Ringeval, M. Chetouani, F. Weninger, F. Eyben, E. Marchi,\nM. Mortillaro, H. Salamin, A. Polychroniou, F. Valente, and S. K.\nKim, “The Interspeech 2013 computational paralinguistics chal-\nlenge: Social signals, conﬂict, emotion, autism,” in Interspeech,\n2013.\n[3] N. Jaitly and G. Hinton, “Learning a better representation of\nspeech soundwaves using restricted boltzmann machines,” in\nICASSP, 2011.\n[4] S. Dieleman and B. Schrauwen, “End-to-end learning for music\naudio,” in ICASSP, 2014.\n[5] G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nico-\nlaou, B. Schuller, and S. Zafeiriou, “Adieu features? end-to-end\nspeech emotion recognition using a deep convolutional recurrent\nnetwork,” inICASSP, 2016.\n[6] Y . LeCun and Y . Bengio, “Convolutional networks for images,\nspeech, and time series,”The Handbook of Brain Theory and Neu-\nral Networks, vol. 3361, no. 10, p. 1995, 1995.\n[7] Q. Kong, Y . Cao, T. Iqbal, Y . Wang, W. Wang, and M. D. Plumb-\nley, “PANNs: Large-scale pretrained audio neural networks for\naudio pattern recognition,”IEEE/ACM TASLP, vol. 28, pp. 2880–\n2894, 2020.\n[8] Y . Gong, Y .-A. Chung, and J. Glass, “PSLA: Improving audio\nevent classiﬁcation with pretraining, sampling, labeling, and ag-\ngregation,”arXiv preprint arXiv:2102.01243, 2021.\n[9] O. Rybakov, N. Kononenko, N. Subrahmanya, M. Visontai, and\nS. Laurenzo, “Streaming keyword spotting on mobile devices,” in\nInterspeech, 2020.\n[10] P. Li, Y . Song, I. V . McLoughlin, W. Guo, and L.-R. Dai, “An\nattention pooling based representation learning method for speech\nemotion recognition,” in Interspeech, 2018.\n[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,\nS. Gelly, J. Uszkoreit, and N. Houlsby, “An image is worth 16x16\nwords: Transformers for image recognition at scale,” in ICLR,\n2021.\n[12] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J´egou, “Training data-efﬁcient image transformers & distilla-\ntion through attention,” arXiv preprint arXiv:2012.12877, 2020.\n[13] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, F. E. Tay, J. Feng, and\nS. Yan, “Tokens-to-token ViT: Training vision transformers from\nscratch on ImageNet,” arXiv preprint arXiv:2101.11986, 2021.\n[14] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,\n“ImageNet: A large-scale hierarchical image database,” inCVPR,\n2009.\n[15] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence,\nR. C. Moore, M. Plakal, and M. Ritter, “Audio Set: An ontology\nand human-labeled dataset for audio events,” inICASSP, 2017.\n[16] K. J. Piczak, “ESC: Dataset for environmental sound classiﬁca-\ntion,” in Multimedia, 2015.\n[17] P. Warden, “Speech commands: A dataset for limited-vocabulary\nspeech recognition,” arXiv preprint arXiv:1804.03209, 2018.\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin NIPS, 2017.\n[19] K. Miyazaki, T. Komatsu, T. Hayashi, S. Watanabe, T. Toda,\nand K. Takeda, “Convolution augmented transformer for semi-\nsupervised sound event detection,” inDCASE, 2020.\n[20] Q. Kong, Y . Xu, W. Wang, and M. D. Plumbley, “Sound event\ndetection of weakly labelled data with CNN-transformer and au-\ntomatic threshold optimization,” IEEE/ACM TASLP, vol. 28, pp.\n2450–2460, 2020.\n[21] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, J. Yu,\nW. Han, S. Wang, Z. Zhang, Y . Wu, and R. Pang, “Conformer:\nConvolution-augmented transformer for speech recognition,” in\nInterspeech, 2020.\n[22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” in NAACL-HLT, 2019.\n[23] G. Gwardys and D. M. Grzywczak, “Deep image features in mu-\nsic information retrieval,”IJET, vol. 60, no. 4, pp. 321–326, 2014.\n[24] A. Guzhov, F. Raue, J. Hees, and A. Dengel, “ESResNet: Envi-\nronmental sound classiﬁcation based on visual domain models,”\nin ICPR, 2020.\n[25] K. Palanisamy, D. Singhania, and A. Yao, “Rethinking CNN mod-\nels for audio classiﬁcation,” arXiv preprint arXiv:2007.11154 ,\n2020.\n[26] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in CVPR, 2016.\n[27] M. Tan and Q. V . Le, “EfﬁcientNet: Rethinking model scaling for\nconvolutional neural networks,” inICML, 2019.\n[28] Y . Tokozume, Y . Ushiku, and T. Harada, “Learning from between-\nclass examples for deep sound recognition,” inICLR, 2018.\n[29] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D.\nCubuk, and Q. V . Le, “SpecAugment: A simple data augmen-\ntation method for automatic speech recognition,” in Interspeech,\n2019.\n[30] P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. G.\nWilson, “Averaging weights leads to wider optima and better gen-\neralization,” in UAI, 2018.\n[31] L. Breiman, “Bagging predictors,” Machine Learning , vol. 24,\nno. 2, pp. 123–140, 1996.\n[32] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-\nmization,” in ICLR, 2015.\n[33] H. B. Sailor, D. M. Agrawal, and H. A. Patil, “Unsupervised ﬁlter-\nbank learning using convolutional restricted boltzmann machine\nfor environmental sound classiﬁcation.” in Interspeech, 2017.\n[34] S. Majumdar and B. Ginsburg, “Matchboxnet–1d time-channel\nseparable convolutional neural network architecture for speech\ncommands recognition,” arXiv preprint arXiv:2004.08531, 2020.\n[35] J. Lin, K. Kilgour, D. Roblek, and M. Shariﬁ, “Training keyword\nspotters with limited and synthesized speech data,” in ICASSP,\n2020."
}