{
  "title": "Contextual Bidirectional Long Short-Term Memory Recurrent Neural Network Language Models: A Generative Approach to Sentiment Analysis",
  "url": "https://openalex.org/W2740586471",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2112123579",
      "name": "Amr Mousa",
      "affiliations": [
        "University of Passau"
      ]
    },
    {
      "id": "https://openalex.org/A2079380910",
      "name": "Bj√∂rn Schuller",
      "affiliations": [
        "Imperial College London",
        "University of Passau"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1591450302",
    "https://openalex.org/W1761709430",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2395693197",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W1711224405",
    "https://openalex.org/W2252215182",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W2166706824",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2960930698",
    "https://openalex.org/W2114524997",
    "https://openalex.org/W2293185259",
    "https://openalex.org/W2949380545",
    "https://openalex.org/W2275025625",
    "https://openalex.org/W2084046180",
    "https://openalex.org/W2107223151",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2533523411",
    "https://openalex.org/W233613663",
    "https://openalex.org/W2028467872",
    "https://openalex.org/W1964613733",
    "https://openalex.org/W2949547296",
    "https://openalex.org/W2154359981",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2131744502",
    "https://openalex.org/W2123925574",
    "https://openalex.org/W2265846598",
    "https://openalex.org/W179707045",
    "https://openalex.org/W2140430379",
    "https://openalex.org/W2950500591",
    "https://openalex.org/W2120615054",
    "https://openalex.org/W14230040",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W1966812932",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2951278869",
    "https://openalex.org/W2160451571",
    "https://openalex.org/W304834817"
  ],
  "abstract": "Traditional learning-based approaches to sentiment analysis of written text use the concept of bag-of-words or bag-of-n-grams, where a document is viewed as a set of terms or short combinations of terms disregarding grammar rules or word order. Novel approaches de-emphasize this concept and view the problem as a sequence classification problem. In this context, recurrent neural networks (RNNs) have achieved significant success. The idea is to use RNNs as discriminative binary classifiers to predict a positive or negative sentiment label at every word position then perform a type of pooling to get a sentence-level polarity. Here, we investigate a novel generative approach in which a separate probability distribution is estimated for every sentiment using language models (LMs) based on long short-term memory (LSTM) RNNs. We introduce a novel type of LM using a modified version of bidirectional LSTM (BLSTM) called contextual BLSTM (cBLSTM), where the probability of a word is estimated based on its full left and right contexts. Our approach is compared with a BLSTM binary classifier. Significant improvements are observed in classifying the IMDB movie review dataset. Further improvements are achieved via model combination.",
  "full_text": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1023‚Äì1032,\nValencia, Spain, April 3-7, 2017.c‚Éù2017 Association for Computational Linguistics\nContextual Bidirectional Long Short-Term Memory Recurrent Neural\nNetwork Language Models:\nA Generative Approach to Sentiment Analysis\nAmr El-Desoky Mousa1 and Bj¬®orn Schuller1,2\n1Chair of Complex & Intelligent Systems, University of Passau, Passau, Germany\n2Department of Computing, Imperial College London, London, UK\namr.mousa@uni-passau.de\nschuller@ieee.org\nAbstract\nTraditional learning-based approaches to\nsentiment analysis of written text use\nthe concept of bag-of-words or bag-of- n-\ngrams, where a document is viewed as\na set of terms or short combinations of\nterms disregarding grammar rules or word\norder. Novel approaches de-emphasize\nthis concept and view the problem as a\nsequence classiÔ¨Åcation problem. In this\ncontext, recurrent neural networks (RNNs)\nhave achieved signiÔ¨Åcant success. The\nidea is to use RNNs as discriminative bi-\nnary classiÔ¨Åers to predict a positive or neg-\native sentiment label at every word posi-\ntion then perform a type of pooling to get\na sentence-level polarity. Here, we investi-\ngate a novel generative approach in which\na separate probability distribution is esti-\nmated for every sentiment using language\nmodels (LMs) based on long short-term\nmemory (LSTM) RNNs. We introduce a\nnovel type of LM using a modiÔ¨Åed version\nof bidirectional LSTM (BLSTM) called\ncontextual BLSTM (cBLSTM), where the\nprobability of a word is estimated based\non its full left and right contexts. Our ap-\nproach is compared with a BLSTM binary\nclassiÔ¨Åer. SigniÔ¨Åcant improvements are\nobserved in classifying the IMDB movie\nreview dataset. Further improvements are\nachieved via model combination.\n1 Introduction\nSentiment analysis of text (also known as opinion\nmining) is the process of computationally identify-\ning and categorizing opinions expressed in a piece\nof text in order to determine the writer‚Äôs attitude\ntowards a particular topic. Due to the tremendous\nincrease in web content, many organizations be-\ncame increasingly interested in analyzing this big\ndata in order to monitor the public opinion and as-\nsist decision making. Therefore, sentiment analy-\nsis attracted the interest of many researchers.\nThe task of sentiment analysis can be seen as a\ntext classiÔ¨Åcation problem. Depending on the tar-\nget of the analysis, the classes can be described by\ncontinuous primitives such as valence, a polarity\nstate (positive or negative attitude), or a subjectiv-\nity state (objective or subjective). In this work, we\nare interested in the binary classiÔ¨Åcation of doc-\numents into a positive or negative attitude. Such\ndetection of polarity is a non-trivial problem due\nto the existence of noise, comparisons, vocabulary\nchanges, and the use of idioms, irony, and domain\nspeciÔ¨Åc terminology (Schuller et al., 2015).\nTraditional approaches to sentiment analysis\nrely on the concept of bag-of-words or bag-of- n-\ngrams, where a document is viewed as a set of\nterms or short combinations of terms disregarding\ngrammar rules or word order. In this case, usually,\nthe analysis involves: tokenization and parsing of\ntext documents, careful selection of important fea-\ntures (terms), dimensionality reduction, and clas-\nsiÔ¨Åcation of the documents into categories. For ex-\nample, Pang et al. (2002) have considered different\nclassiÔ¨Åers, such as Naive Bayes (NB), maximum\nentropy (MaxEnt), and support vector machines\n(SVM) to detect the polarity of movie reviews.\nPang and Lee (2004) have combined polarity and\nsubjectivity analysis and proposed a technique to\nÔ¨Ålter out objective sentences of movie reviews\nbased on Ô¨Ånding minimum cuts in graphs. In\n(Taboada et al., 2011; Ding et al., 2008), lexicon-\nbased techniques are examined, where word-level\nsentimental orientation scores are used to evaluate\nthe polarity of product reviews. More advanced\napproaches utilize word or n-gram vectors, like in\n(Maas et al., 2011; Dahl et al., 2012).\n1023\nNovel approaches are mainly based on artiÔ¨Å-\ncial neural networks (ANNs). These approaches\nde-emphasize the concept of bag-of-words or bag-\nof-n-grams. A document is viewed as a set of\nsentences, each sentence is a sequence of words.\nThe sentiment problem is rather considered as a\nsequence classiÔ¨Åcation problem. For example, in\n(Dong et al., 2014; Dong et al., 2016), RNN clas-\nsiÔ¨Åers are used with an adaptive method to select\nrelevant semantic composition functions for ob-\ntaining vector representations of sentences. This\nis found to improve sentiment classiÔ¨Åcation on the\nStanford Sentiment Treebank (SST). Rong et al.\n(2014) have used a RNN model to learn word rep-\nresentation simultaneously with the sentiment dis-\ntribution. Santos and Gatti (2014) have proposed a\nconvolutional neural network (CNN) that exploits\nfrom character- to sentence-level information to\nperform sentiment analysis on the Stanford Twit-\nter Sentiment (STS) corpus. Kalchbrenner et al.\n(2014) have used a dynamic convolutional neu-\nral network (DCNN) with a dynamic k-max pool-\ning to perform sentiment analysis on the SST and\nTwitter sentiment datasets. Lai et al. (2015) have\nutilized a combination of RNNs and CNNs called\nrecurrent convolutional neural network (RCNN) to\nperform text classiÔ¨Åcation on multiple datasets in-\ncluding sentiment analysis on the SST dataset.\nOther novel approaches use tree structured neu-\nral models instead of sequential models (like\nRNNs) in order to capture complex semantic re-\nlationships that relate words to phrases. Despite\ntheir good performance, these models rely on\nexisting parse trees of the underlying sentences\nwhich are, in most cases, not readily available or\nnot trivial to generate. For example, Socher et\nal. (2013) have introduced a recursive neural ten-\nsor network (RNTN) to predict the compositional\nsemantic effects in the SST dataset. In (Tai et\nal., 2015; Le and Zuidema, 2015), tree-structured\nLSTMs are used to improve the earlier models.\nAnother perspective to the sentiment problem\nis to assume that each sentence with a positive or\nnegative class is drawn from a particular proba-\nbility distribution related to that class. Then, in-\nstead of estimating a discriminative model that\nlearns how to separate sentiment classes in sen-\ntence space, we estimate a generative model that\ntells us how these sentences are generated. This\ngenerative approach can be better or complemen-\ntary in some sense to the discriminative approach.\nThe probability distributions over word se-\nquences are well known as language models\n(LMs). They have also been used for sentiment\nanalysis. However, no trial is made to go beyond\nsimple bigram models. For example, Hu et al.\n(2007b) have estimated two separate positive and\nnegative LMs from training collections. Tests are\nperformed by computing the Kullback-Leibler di-\nvergence between the LM estimated from the test\ndocument and the sentiment LMs. Therein, uni-\nand bigram models are shown to outperform SVM\nmodels in classifying a movie review dataset. In\n(Hu et al., 2007a), a batch of terms in a domain\nare identiÔ¨Åed. Then, two different unigram LMs\nrepresenting classifying knowledge for every term\nare built up from subjective sentences. A classi-\nfying function based on the generation of a test\ndocument is deÔ¨Åned for the sentiment classiÔ¨Åca-\ntion. This approach has outperformed SVM on a\nChinese digital product review dataset. Liu et al.\n(2012) have employed an emoticon smoothed un-\nigram LM to perform sentiment classiÔ¨Åcation.\nIn this paper, we compare the generative LM\napproach with the discriminative binary classiÔ¨Å-\ncation approach. We estimate a separate proba-\nbility distribution for each sentiment using long-\nspan LMs based on unidirectional LSTMs (Sun-\ndermeyer et al., 2012) trained to predict a word\ndepending on its full left context. The probability\nscores from positive and negative LMs are used to\nclassify unseen sentences. In addition, we intro-\nduce a novel type of LM using a modiÔ¨Åed version\nof the standard bidirectional LSTM called contex-\ntual bidirectional LSTM (cBLSTM). In contrast to\nthe unidirectional model, this model is trained to\npredict a word depending on its full left and right\ncontexts. Moreover, we combine the LM approach\nwith the binary classiÔ¨Åcation approach using lin-\near interpolation of probabilities. We observe that\nthe cBLSTM LM outperforms both the LSTM\nLM and the BLSTM binary classiÔ¨Åer. Combining\napproaches together yields further improvements.\nModels are evaluated on the IMDB large movie\nreview dataset1 (Maas et al., 2011).\n2 Language Models\nA statistical LM is a probability distribution over\nword sequences that assigns a probability p(wM\n1 )\nto any word sequence wM\n1 of length M. Thus, it\nprovides a way to estimate the relative likelihood\n1http://ai.stanford.edu/‚àºamaas/data/sentiment/\n1024\nof different phrases. It is a widely used model in\nmany natural language processing tasks, like au-\ntomatic speech recognition, machine translation,\nand information retrieval. Usually, to estimate a\nLM, the assumption of the(n‚àí1)th order Markov\nprocess is used (Bahl et al., 1983), in which a cur-\nrent word wm is assumed conditionally dependent\non the preceding (n‚àí1) history words, such that:\np(wM\n1 ) ‚âà\nM‚àè\nm=1\np(wm|wm‚àí1\nm‚àín+1). (1)\nThis is called an n-gram LM. A conventional ap-\nproach to estimate these probabilities is the back-\noff LM which is based on count statistics col-\nlected from the training text. In addition to the\ninitial n-gram approximation, a major drawback\nof this model is that it backs-off to a shorter his-\ntory whenever insufÔ¨Åcient statistics are observed\nfor a givenn-gram. Novel state-of-the-art LMs are\nbased on ANNs like RNNs that provide long-span\nprobabilities conditioned on all predecessor words\n(Mikolov et al., 2010; Kombrink et al., 2011).\n3 Unidirectional RNN Models\n3.1 Standard RNN\nA RNN maps from a sequence of input observa-\ntions to a sequence of output labels. The mapping\nis deÔ¨Åned by a set of activation weights and a non-\nlinear activation function. Recurrent connections\nallow to access activations from past time steps.\nFor an input sequence xT\n1 , a RNN computes the\nhidden sequence hT\n1 and the output sequence yT\n1\nby performing the following operations for time\nsteps t= 1to T (Graves et al., 2013):\nht = H(Wxhxt + Whhht‚àí1 + bh) (2)\nyt = Whyht + by, (3)\nwhere His the hidden layer activation function,\nWxh is the weight matrix between the input and\nhidden layer, Whh is the recurrent weight ma-\ntrix between the hidden layer and itself, Why is\nthe weight matrix between the hidden and output\nlayer, bh and by are the hidden and output layer\nbias vectors respectively. His usually an element-\nwise application of the sigmoid function.\n3.2 LSTM RNN\nIn (Hochreiter and Schmidhuber, 1997), an al-\nternative RNN called Long Short-Term Memory\n(LSTM) is introduced where the conventional neu-\nron is replaced with a so-called memory cell con-\ntrolled by input, output and forget gates in order to\novercome the vanishing gradient problem of tradi-\ntional RNNs. In this case, Hcan be described by\nthe following composite function:\nit =œÉ(Wxixt + Whiht‚àí1 + Wcict‚àí1 + bi) (4)\nft =œÉ(Wxf xt + Whf ht‚àí1 + Wcf ct‚àí1 + bf )(5)\nct =ftct‚àí1 +it tanh(Wxcxt+Whcht‚àí1 +bc)(6)\not =œÉ(Wxoxt + Whoht‚àí1 + Wcoct + bo) (7)\nht =ot tanh(ct), (8)\nwhere œÉ is the sigmoid function, i,f,o, and c are\nrespectively the input, forget, output gates, and\ncell activation vectors (Graves et al., 2013).\n3.3 LSTM LM\nIn a LSTM LM, the time steps correspond to the\nword positions in a training sentence. At every\ntime step, the network takes as input the word at\nthe current position encoded as a 1-hot binary vec-\ntor. The input vector is then passed to one or more\nrecurrent hidden layers with self connections that\nimplicitly take into account all the previous his-\ntory words presented to the network. The output of\nthe Ô¨Ånal hidden layer is passed to an output layer\nwith a softmax activation function to produce a\ncorrectly normalized probability distribution. The\ntarget output at each word position is the next word\nin the sentence. A cross-entropy loss function is\nused which is equivalent to maximizing the likeli-\nhood of the training data. At the end, the network\ncan predict the long-span conditional probability\np(wm|wm‚àí1\n1 ) for any word wm ‚ààV and a given\nhistory wm‚àí1\n1 , where V is the vocabulary. Fig. 1\nshows an unfolded example of a LSTM LM over a\nsentence <s> w1 w2 w3 </s>, where <s>and\n</s>are the sentence start and end symbols.\n \n \n</ùë† > \n ùë§2 \n ùë§3 \n \nùë§3   \n‚Ñé1 \n ‚Ñé2 \n \n‚Ñé3 \n \nùë§1 \n \nùë§2 \n \nTarget Words \nRecurrent Layer \nInput Words \nùë§1 \n‚Ñé0 \n< ùë† > \n  \nFigure 1: Architecture of a LSTM LM predicting\na word given its full previous history.\n4 Bidirectional RNN Models\n4.1 BLSTM RNN\nA BLSTM processes input sequences in both di-\nrections with two sub-layers in order to account\nfor the full input context. These two sub-layers\n1025\ncompute forward and backward hidden sequences‚àí ‚Üíh, ‚Üê ‚àíh respectively, which are then combined to\ncompute the output sequence y(see Fig. 2), thus:\n‚àí ‚Üíht = H(Wx‚àí ‚Üíh xt + W‚àí ‚Üíh ‚àí ‚Üíh\n‚àí ‚Üíht‚àí1 + b‚àí ‚Üíh ) (9)\n‚Üê ‚àíht = H(Wx‚Üê ‚àíh xt + W‚Üê ‚àíh ‚Üê ‚àíh\n‚Üê ‚àíht+1 + b‚Üê ‚àíh )(10)\nyt = W‚àí ‚Üíh y\n‚àí ‚Üíht + W‚Üê ‚àíh y\n‚Üê ‚àíht + by (11)\n \n \nùë¶ùë°+1    ‚Ä¶ \n \n‚Ä¶    ùë¶ùë°‚àí1 \n ùë¶ùë° \n \nùë•ùë°+1   ‚Ä¶ \n‚Ñé  ùë°‚àí1 \n‚Ñé  ùë°‚àí1 \n‚Ñé  ùë° \n \n‚Ñé  ùë° \n \n‚Ñé  ùë°+1 \n \n‚Ñé  ùë°+1 \n \n‚Ä¶    ùë•ùë°‚àí1 \n \nùë•ùë° \n \nOutputs \nBackward Layer \nForward Layer \nInputs \nFigure 2: Architecture of a BLSTM, every output\ndepends on the whole input sequence.\n4.2 Contextual BLSTM LM\nThe standard BLSTM described in Section 4.1 is\nnot suitable for estimating LMs. This is because\nit predicts every output symbol depending on the\nwhole input sequence. Since a LM indeed uses the\nsame word sequence in both input and target sides\nof the network, it would be incorrect to predict a\nword given the whole input sentence. Rather, it is\nrequired to predict a word given the full left and\nright context while excluding the predicted word\nitself from the conditional dependence. To allow\nfor this, we modify the architecture of the standard\nBLSTM such that it accounts for a contextual de-\npendence rather than a full sequence dependence.\nThe new model is called a contextual BLSTM or\ncBLSTM in short. The architecture of this model\nis illustrated in Fig. 3.\n \n \nùë§2   \n‚Ñé  0 \n‚Ñé  0 \n‚Ñé  1 \n \n‚Ñé  1 \n \n‚Ñé  2 \n \n‚Ñé  2 \n \n< ùë† > \n \nùë§1 \n \nTarget Words \nBackward Layer \nForward Layer \nInputs Words \nùë§2   \n< ùë† > \n  \nùë§1 \n  \nùë§3  \n‚Ñé  3 \n  \n‚Ñé  3 \n  \nùë§3   \n</ùë† > \n</ùë† > \nFigure 3: Architecture of a cBLSTM LM predict-\ning a word given its full left and right contexts.\nThe model consists of a forward and a backward\nsub-layer. The forward sub-layer receives the en-\ncoded input words staring from the sentence start\nsymbol up to the last word before the sentence\nend symbol (sequence <s> w 1 w2 w3 in Fig.\n3). The forward hidden states are used to predict\nwords starting from the Ô¨Årst word after the sen-\ntence start symbol up to the sentence end symbol\n(sequence w1 w2 w3 </s>in Fig. 3). The back-\nward sub-layer does exactly the reverse operation.\nThe two sub-layers are interleaved together in or-\nder to adjust the conditional dependence such that\nthe prediction of any target word depends on the\nfull left and right contexts. Note that the hidden\nstate at the Ô¨Årst as well as the last time step needs\nto be padded by zeros so that the size of the hid-\nden vector is consistent across all time steps. At\nthe end, the model can effectively predict the con-\nditional probability p(wm|wm‚àí1\n1 ,wM\nm+1) for any\nword wm ‚ààV, left context wm‚àí1\n1 and right con-\ntext wM\nm+1, where V is the vocabulary and M is\nthe length of the sentence. Table 1 shows the pre-\ndicted probability at each time step of Fig. 3. Note\nthat one direction dependence is maintained at the\nstart and end of sentence (time steps 1 and 5).\ntime step predicted conditional prob.\n1 p(<s> |w1 w2 w3 </s>)\n2 p(w1 |<s>, w2 w3 </s>)\n3 p(w2 |<s> w1 , w3 </s>)\n4 p(w3 |<s> w1 w2 , </s>)\n5 p(</s>|<s> w1 w2 w3)\nTable 1: Predicted conditional probabilities at ev-\nery time step of the cBLSTM shown in Fig. 3.\nOur implementation of the novel cBLSTM\nRNN model is integrated into our publicly avail-\nable CURRENNT2 toolkit initially introduced by\nWeninger et al. (2014). A new version of the\ntoolkit with the novel implementations is planned\nto be available by the date of publication.\nHere, it is worth noting that deep cBLSTM\nmodels can not be easily constructed by stacking\nmultiple hidden bidirectional layers together. The\nreason is that the hidden states obtained after the\nÔ¨Årst bidirectional layer are dependent on the full\nleft and right contexts. If these states are utilized\nas inputs to a second bidirectional layer that identi-\ncally repeats the same operation again, then the de-\nsired conditional dependence will not be correctly\n2http://sourceforge.net/p/currennt\n1026\nmaintained. One method to solve this problem is\nto create deeper models by stacking multiple for-\nward and backward sub-layers independently. The\nfusion of both sub-layers takes place and the end\nof the deep stack. The implementation of such a\ndeep cBLSTM model is not yet available.\n5 Dataset\nOur experiments on sentiment analysis are per-\nformed on the IMDB large movie review dataset\n(v1.0) introduced by Maas et al. (2011). The la-\nbeled partition of the dataset consists of 50k bal-\nanced full-length movie reviews with 25k positive\nand 25k negative reviews extracted from the Inter-\nnet Movie Database (IMDB)3.\nSince the reviews are in a form of long para-\ngraphs which are difÔ¨Åcult to handle directly with\nRNNs, we break down these paragraphs into rela-\ntively short sentences based on punctuation clues.\nAfter breaking down the paragraphs, the average\nnumber of sentences per review is around 13 sen-\ntences. We randomly selected 1000 positive and\n1000 negative reviews as our test set. A similar\nnumber of random reviews are selected as a devel-\nopment set. The remaining reviews are used as a\ntraining set. Note that this is not the ofÔ¨Åcial dataset\ndivision provided by Maas et al. (2011), where 25k\nbalanced reviews are dedicated for training and the\nother 25k balanced reviews are dedicated for test-\ning. The reasons not to follow the ofÔ¨Åcial divi-\nsion are Ô¨Årstly that, it does not provide a devel-\nopment set; secondly, our proposed models need\nmuch data to train well as revealed by initial exper-\niments; thirdly, it would be very time consuming\nto use the whole data as one partition and perform\nmulti-fold cross validation as usually adopted with\nlarge sentiment datasets (Schuller et al., 2015). A\npreprocessed version of the IMDB dataset with the\nmodiÔ¨Åed partitioning is planned to be available for\ndownload by the date of publication.\nA word list of the 10k most frequent words is se-\nlected as our vocabulary. This covers around 95%\nof the words in our development and test sets. Any\nout-of-vocabulary word is mapped to a specialunk\nsymbol. We use the classiÔ¨Åcation accuracy as our\nevaluation measure. The unweighted average F1\nscores over positive and negative classes are also\ncalculated. However, their values are found almost\nsimilar to the classiÔ¨Åcation accuracies. Therefore,\nonly classiÔ¨Åcation accuracies are reported.\n3http://www.imdb.com\n6 Related Work\nThe work of this paper is closely related to several\nprevious publications that report sentiment classi-\nÔ¨Åcation accuracy on the same dataset. For exam-\nple, in (Maas et al., 2011), the IMDB dataset is\nintroduced and a semi-supervised word vector in-\nduction framework is used, where an unsupervised\nprobabilistic model similar to latent Dirichlet allo-\ncation (LDA) is proposed to learn word vectors.\nAnother supervised model is utilized to constrain\nwords expressing similar sentiment to have sim-\nilar representations in vector space. In (Dahl et\nal., 2012), documents are treated as bags of n-\ngrams. Restricted Boltzmann machines (RBMs)\nare used to extract vector representations for n-\ngrams. Then, a linear SVM model is utilized to\nclassify documents based on the resulting feature\nvectors. Wang and Manning (2012) have used a\nvariant of SVM with Naive Bayes log-count ratios\nas well as word bigrams as features. This modi-\nÔ¨Åed SVM model is referred to as NBSVM. In our\nprevious publication (Schuller et al., 2015), LSTM\nLMs trained on 40% of the whole IMDB dataset\nare used for performing sentiment analysis. How-\never, a carefully tuned MaxEnt classiÔ¨Åer is found\nto perform better. Le and Mikolov (2014) have\nused a paragraph vector methodology with an un-\nsupervised algorithm based on feed-forward neu-\nral networks that learns Ô¨Åxed-length vector rep-\nresentations from variable-length texts. All these\npublications use the ofÔ¨Åcial IMDB dataset division\nexcept for (Schuller et al., 2015), where a similar\ndivision as in this paper is used. To give a compre-\nhensive idea about the aforementioned techniques,\nwe show in Table 2 the classiÔ¨Åcation results as re-\nported in the related publications. Note that only\nthe results of (Schuller et al., 2015) are directly\ncomparable to our results.\nexperiment Accuracy [%]\nMaas et al. (2011) 88.89\nDahl et al. (2012) 89.23\nWang and Manning (2012) 91.22\nSchuller et al. (2015)‚àó 91.55\nLe and Mikolov (2014) 92.58\nTable 2: Sentiment classiÔ¨Åcation accuracies from\nprevious publications on the IMDB dataset.\nIn relation to our novel cBLSTM LM, previ-\nous trials have been made to estimate bidirectional\n1027\nLMs. For example, in (Frinken et al., 2012), dis-\ntinct forward and backward LMs are estimated\nfor handwriting recognition. However, no trial is\nmade to go beyond 4-gram models. In (Xiong et\nal., 2016), standard forward and backward RNN\nLMs are separately estimated for a conversational\nspeech recognition task. The log probabilities\nfrom both models are added. In (Arisoy et al.,\n2015), bidirectional RNNs and LSTMs are used\nto estimate LMs for an English speech recogni-\ntion task. Therein, the standard bidirectional ar-\nchitecture (as in Fig. 2) is used without modiÔ¨Åca-\ntions. This causes circular dependencies to arise\nwhen combining probabilities from multiple time\nsteps. Therefore, pseudo-likelihoods are utilized\nrather than true likelihoods which is not perfect\nfrom the mathematical point of view. Not sur-\nprisingly, the BLSTM LMs do not yield any gain\nover the LSTM LMs. In addition, the perplexity\nof such a model becomes invalid. More impor-\ntantly, in (Peris and Casacuberta, 2015), bidirec-\ntional RNN LMs are used for a statistical machine\ntranslation task. However, only standard RNNs\nbut not LSTMs are utilized. Furthermore, no de-\ntails are provided about how the model is exactly\nmodiÔ¨Åed and how the left and right dependencies\nare maintained over time steps.\n7 Sentiment ClassiÔ¨Åcation\n7.1 Generative LM-based classiÔ¨Åer\nOur Ô¨Årst approach to sentiment classiÔ¨Åcation is the\ngenerative approach based on LMs. We either use\nLSTM LMs described in Section 3.3 or cBLSTM\nLMs described Section 4.2. Two separate LMs\nare estimated from positive and negative training\ndata. We use networks with a single hidden layer\nthat consists of 300 memory cells followed by a\nsoftmax layer with a dimension of 10k+ 3. This\nis equal to the full vocabulary size in addition to\n<s>, </s>, and unk symbols representing sen-\ntence start, sentence end, and unknown word sym-\nbols respectively. In case of using cBLSTM net-\nworks, a single hidden layer of 600 memory cells\nis used (300 cells for each forward and backward\nsub-layer). A cross-entropy loss function is used\nwith a momentum of 0.9. We use sentence-level\nmini-batches of size 100 sentences computed in\nparallel. The learning rate is set initially to 10‚àí3\nand then decreased gradually to 10‚àí6. The train-\ning process is controlled by monitoring the cross-\nentropy error over the development set.\nIn addition, we use a data sub-sampling\nmethodology during training. For this purpose,\na traditional 5-gram backoff LM is created out of\nthe development data, we call this a ranking LM.\nThen, all training sentences are ranked according\nto their perplexities with the ranking LM. Using\nthese ranks, we divide our training sentences into\nthree partitions that reÔ¨Çect the relative importance\nof the data, such that the Ô¨Årst partition contains\nthe 100k sentences with the lowest perplexities,\nthe second partition contains the 100k sentences\nwith next lowest perplexities. The third partition\ncontains all the other sentences. Instead of using\nthe whole training data in each epoch, we use a\nrandom sample with more sentences from the Ô¨Årst\ntwo partitions than the third one. After a sufÔ¨Å-\ncient number of epochs, the whole training data\nis covered. The sub-sampling approach speeds up\nthe training and makes it feasible with any size of\ntraining data. At the same time, the training is fo-\ncused on the relatively more important examples.\nIn addition, it adds a useful regularization to the\ntraining process. Yet, it leads to a less smoother\nconvergence. To show the efÔ¨Åciency of our sen-\ntence ranking methodology, Table 3 shows exam-\nples of the highest and lowest ranked sentences\nfrom positive and negative training data.\nmost +ve this is one of the best Ô¨Ålms\never made.\nleast +ve cheap laughs but great value.\nmost -ve this is one of the worst movies\ni have ever seen.\nleast -ve life‚Äôs too short.\nTable 3: Examples of the highest/lowest ranked\nsentences from positive/negative training data.\nAfter training the neural networks, each of the\npositive and negative sentiment LM estimates a\nprobability distribution for the corresponding sen-\ntiment, we call these probability distributions p+\nand p‚àí. To evaluate the sentiment of some test\nreview, we calculate the perplexity of each model\np+ and p‚àíwith respect to the whole review. Thus,\ngiven a probability distributionp, and a review text\nS composed of K sentences S = s1,...,s K, each\nsentence sk : 1 ‚â§k ‚â§K is composed of a se-\nquence of Mk words sk = wk\n1 ,wk\n2 ,...,w k\nMk ; we\ncalculate the perplexity PPp(S) of a model pwith\nrespect to text S. It is a very common measure-\n1028\nment of how well a probability distribution pre-\ndicts a sample. A low perplexity indicates that the\nprobability distribution is good at predicting the\nsample. Perplexity is deÔ¨Åned as the exponentiated\nnegative average log-likelihood, or in other words,\nthe inverse of the geometric average probability\nassigned by the model to each word in the sam-\nple. We calculate the Perplexity using Equation\n12 if the model p is based on LSTM, and using\nEquation 13 if the model is based on cBLSTM:\nPPp(S)=\n[K‚àè\nk=1\nMk‚àè\nm=1\np(wk\nm|wk\n1,wk\n2,...,wk\nm‚àí1)\n]‚àí1\nN\n(12)\nPPp(S) =\n[K‚àè\nk=1\nMk‚àè\nm=1\np(wk\nm|wk\n1,wk\n2,...,wk\nm‚àí1;\nwk\nm+1,wk\nm+2,...,w k\nMk )\n]‚àí1\nN\n,(13)\nwhere N = ‚àëK\nk=1 Mk is the total number of\nwords in text S. Then, a sentiment polarity P ‚àà\n{‚àí1,+1}is assigned toSaccording to the follow-\ning decision rule:\nP(S) =\n{ +1 if PPp+ (S) <PP p‚àí(S)\n‚àí1 otherwise .\n(14)\n7.2 Discriminative BLSTM-based Binary\nClassiÔ¨Åer\nOur second approach to sentiment classiÔ¨Åcation\nis the discriminative approach based on BLSTM\nRNNs described in Section 4.1. We use BLSTM\nnetworks with a single hidden layer that consists of\n600 memory cells (300 cells for each forward and\nbackward sub-layer). Since the BLSTM performs\na binary classiÔ¨Åcation task, only a single output\nneuron is used with a sigmoid activation function.\nA cross-entropy loss function is used with a mo-\nmentum of 0.9. The same training settings like\nthe case of LSTM/cBLSTM LMs are used includ-\ning sub-sampling with the same partitioning of the\ntraining data. However, a single training dataset\nwith all positive and negative reviews is used. For\na sentence with a positive sentiment, the target out-\nputs are set toones at all time steps. For a sentence\nwith a negative sentiment, the target outputs are set\nto zeros at all time steps. Since the sigmoid func-\ntion provides output values in the interval [0,1],\nthe network is trained to produce the probability\nof the positive class at every time step. Although\nthe output of the BLSTM network at a given time\nstep is dependent on the whole input sequence, it\nis widely known that every output is more affected\nby the inputs at closer time steps in both direc-\ntions. Therefore, a sentence-level sentiment can\nbe deduced by comparing the average probability\nmass assigned to the positive class over all time\nsteps with the average probability mass assigned\nto the negative class. Thus, similar to Section 7.1,\ngiven a review text S composed of K sentences,\neach sentence is a sequence of Mk words, we cal-\nculate two probabilities p+(S) and p‚àí(S) that the\nreview S has a positive or negative sentiment us-\ning Equations 15 and 16 respectively:\np+(S) = 1\nN\nK‚àë\nk=1\nMk‚àë\nm=1\np+(wk\nm) (15)\np‚àí(S) = 1\nN\nK‚àë\nk=1\nMk‚àë\nm=1\n(1 ‚àíp+(wk\nm)), (16)\nwhere N is the total number of words in text S,\nand p+(wk\nm) is the probability that a positive class\nis assigned to the word at position m of the kth\nsentence of the review S. Then, a sentiment po-\nlarity P‚àà{‚àí 1,+1}is assigned to Saccording to\nthe following decision rule:\nP(S) =\n{ +1 if p+(S) >p‚àí(S)\n‚àí1 otherwise . (17)\n7.3 Model Combination\nThe probability scores of the generative LM-based\nclassiÔ¨Åer and the discriminative BLSTM-based bi-\nnary classiÔ¨Åer discussed in Sections 7.1 and 7.2\ncan be combined together via linear interpolation.\nThis is achieved by Ô¨Årst normalizing the probabil-\nities from the LMs such that the probabilities of\npositive and negative classes for a given review are\nsummed up to 1.0. Note that this normalization\nproperty holds by default for the BLSTM-based\nbinary classiÔ¨Åer. Then, the probabilities of both\nmodels are linearly interpolated to obtain a single\nprobability score. The interpolation weights are\noptimized on the development data.\n8 Experimental Results\nTable 4 shows the results of our experiments.\nAll the neural networks in this work are trained\n1029\nand optimized using our own CURRENNT toolkit\n(Weninger et al., 2014). Both the LSTM and\ncBLSTM LMs are linearly interpolated with two\nadditional LMs, namely a 5-gram backoff LM\nsmoothed with modiÔ¨Åed Kneser-Ney smoothing\n(Kneser and Ney, 1995), and another 5-gram Max-\nEnt LM (Alum ¬®ae and Kurimo, 2010). These two\nmodels are estimated using the SRILM language\nmodeling toolkit (Stolcke, 2002).\nclassiÔ¨Åcation model Acc.\n[%]\nLSTM LM 89.58\n+ 5-grm backoff LM 91.05\n+ 5-grm MaxEnt LM 91.23\ncBLSTM LM 89.88\n+ 5-grm backoff LM 91.38\n+ 5-grm MaxEnt LM 91.48\nBLSTM binary classiÔ¨Åer 90.15\nLSTM LM + BLSTM binary classiÔ¨Åer 92.35\ncBLSTM LM + BLSTM binary classiÔ¨Åer 92.83\nSchuller et al. (2015) LSTM + 5-grm LM 90.50\nSchuller et al. (2015) MaxEnt classiÔ¨Åer 91.55\nTable 4: Sentiment classiÔ¨Åcation accuracies mea-\nsured on the IMDB dataset.\nWe observe that the use of cBLSTM LM\nas a generative sentiment classiÔ¨Åer signiÔ¨Åcantly\noutperforms the use of both LSTM LM and\nBLSTM discriminative binary classiÔ¨Åers. The\nstatistical signiÔ¨Åcance is veriÔ¨Åed using a boot-\nstrap method of signiÔ¨Åcance analysis described by\nBisani and Ney (2004). The probability of im-\nprovement (POIboot) is around 95%. Combining\nLM-based classiÔ¨Åers with BLSTM-based binary\nclassiÔ¨Åers via linear interpolation of probabilities\nachieves further improvements. Our best accuracy\n(92.83%) is obtained by combining the cBLSTM\nLM classiÔ¨Åer with the BLSTM binary classiÔ¨Åer.\nThese results reveal that both the generative and\ndiscriminative approaches are complementary in\nsolving the sentiment classiÔ¨Åcation problem.\nFinally, our best result is better than the best\npreviously published result in (Schuller et al.,\n2015) on the same IMDB dataset with the same\ndataset partitioning. Even though they are not di-\nrectly comparable, our results are better than other\npreviously published results reported in Table 2\nwhere a different dataset partitioning is used.\nFor illustration, Table 5 shows two examples\nof positive and negative reviews that could not be\ncorrectly classiÔ¨Åed by the discriminative BLSTM\nbinary classiÔ¨Åer, however they are correctly clas-\nsiÔ¨Åed by the cBLSTM LM classiÔ¨Åer. We can ob-\nserve the implicit indication of the writer‚Äôs attitude\ntowards the movie which can not be easily cap-\ntured by simple approaches. In this case, learning\na separate long-span bidirectional probability dis-\ntribution for each sentiment seems to help.\n+ve low budget mostly no name actors.\nthis is what a campy horror Ô¨Çick\nis supposed to be all about. these\nare the types of movies that kept\nme on the edge of my seat as a kid\nstaying up too late to watch cable.\nif you liked the eighties horror\nscene this is the movie for you.\n-ve i and a friend rented this movie.\nwe both found the movie soundtrack\nand production techniques to be\nlagging. the movie‚Äôs plot appeared\nto drag on throughout with little\nsurprise in the ending. we both\nagreed that the movie could have\nbeen compressed into roughly an\nhour giving it more suspense and\nmoving plot.\nTable 5: Examples of reviews correctly classiÔ¨Åed\nby the cBLSTM LM classiÔ¨Åer.\n9 Conclusions\nWe have introduced a generative approach to senti-\nment analysis in which a novel contextual BLSTM\n(cBLSTM) LM is used as a sentiment classiÔ¨Åer.\nSeparate LM probability distributions are esti-\nmated for positive and negative sentiment from the\ntraining data. Then, probability scores from these\nLMs are utilized to classify test data. Results have\nbeen compared with a discriminative sentiment\nclassiÔ¨Åcation approach that uses a BLSTM-based\nbinary classiÔ¨Åer. We have observed that the gener-\native cBLSTM LM approach signiÔ¨Åcantly outper-\nforms other approaches. Models have been eval-\nuated on the IMDB large movie review dataset.\nThe proposed models have achieved better results\nthan the previously published results on the same\ndataset with the same partitioning. In addition,\nindicative comparisons have been made with the\npreviously published results on the same dataset\nwith different partitioning. Using model combi-\n1030\nnation, we could achieve further performance im-\nprovement indicating that both the generative and\ndiscriminative approaches are complementary in\nsolving the sentiment analysis problem. More-\nover, we have introduced an efÔ¨Åcient methodol-\nogy based on perplexity calculation to partition\nthe training data according to relative importance\nto the learning task. This partitioning methodol-\nogy has been combined with a sub-sampling tech-\nnique to efÔ¨Åciently train the neural networks on\nlarge data. As a future work, we plan to investi-\ngate deeper cBLSTM as well as hybrid recurrent\nand convolutional models. Another direction is to\nexperiment with pre-trained word vectors.\nAcknowledgments\nThe research leading to these results has received\nfunding from the European Unions Horizon 2020\nProgramme through the Research and Innovation\nAction #645378 (ARIA-V ALUSPA), the Innova-\ntion Action #644632 (MixedEmotions), as well as\nthe German Federal Ministry of Education, Sci-\nence, Research and Technology (BMBF) under\ngrant agreement #16SV7213 (EmotAsS). We fur-\nther thank the NVIDIA Corporation for their sup-\nport of this research by Tesla K40 GPU donation.\nReferences\nTanel Alum ¬®ae and Mikko Kurimo. 2010. EfÔ¨Åcient\nestimation of maximum entropy language models\nwith N-gram features: an SRILM extension. In\nProc. Interspeech Conference of the International\nSpeech Communication Association , pages 1820‚Äì\n1823, Makuhari, Chiba, Japan, September.\nEbru Arisoy, Abhinav Sethy, Bhuvana Ramabhad-\nran, and Stanely Chen. 2015. Bidirectional re-\ncurrent neural network language models for auto-\nmatic speech recognition. In Proc. IEEE Interna-\ntional Conference on Acoustics, Speech, and Signal\nProcessing, pages 5421‚Äì5425, Brisbane, Australia,\nApril.\nLalit R. Bahl, Frederick Jelinek, and Robert L. Mer-\ncer. 1983. A maximum likelihood approach to con-\ntinuous speech recognition. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 5:179 ‚Äì\n190, March.\nMaximilian Bisani and Hermann Ney. 2004. Bootstrap\nestimates for conÔ¨Ådence intervals in ASR perfor-\nmance evaluation. In Proc. IEEE International Con-\nference on Acoustics, Speech, and Signal Process-\ning, volume 1, pages 409 ‚Äì 412, Montreal, Canada,\nMay.\nGeorge E. Dahl, Ryan Prescott Adams, and Hugo\nLarochelle. 2012. Training restricted boltzmann\nmachines on word observations. In Proc. Interna-\ntional Conference on Machine Learning, pages 679‚Äì\n686, Edinburgh, Scotland, UK, June.\nXiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A\nholistic lexicon-based approach to opinion mining.\nIn Proc. International Conference on Web Search\nand Data Mining , pages 231‚Äì240, Palo Alto, Cali-\nfornia, USA, February.\nLi Dong, Furu Wei, Ming Zhou, and Ke Xu. 2014.\nAdaptive multi-compositionality for recursive neu-\nral models with applications to sentiment analysis.\nIn Proc. AAAI Conference on ArtiÔ¨Åcial Intelligence,\npages 1537‚Äì1543, Qu¬¥ebec, Qu¬¥ebec, Canada, July.\nLi Dong, Furu Wei, Ke Xu, Shixia Liu, and Ming Zhou.\n2016. Adaptive multi-compositionality for recursive\nneural network models. IEEE/ACM Transactions on\nAudio, Speech & Language Processing, 24(3):422‚Äì\n431.\nV olkmar Frinken, Alicia Forn ¬¥es, Josep Llad ¬¥os, and\nJean-Marc Ogier, 2012. Bidirectional Language\nModel for Handwriting Recognition , pages 611‚Äì\n619. Springer Berlin Heidelberg, Berlin, Heidel-\nberg.\nAlex Graves, Abdel rahman Mohamed, and Geoffrey\nHinton. 2013. Speech recognition with deep re-\ncurrent neural networks. In Proc. IEEE Interna-\ntional Conference on Acoustics, Speech, and Sig-\nnal Processing, pages 6645 ‚Äì 6649, Vancouver, BC,\nCanada, May.\nSepp Hochreiter and J ¬®urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735 ‚Äì 1780, November.\nYi Hu, Ruzhan Lu, Yuquan Chen, and Jianyong\nDuan. 2007a. Using a generative model for senti-\nment analysis. International Journal of Computa-\ntional Linguistics & Chinese Language Processing ,\n12(2):107‚Äì126, June.\nYi Hu, Ruzhan Lu, Xuening Li, Yuquan Chen, and\nJianyong Duan. 2007b. A language modeling ap-\nproach to sentiment analysis. In Proc. International\nConference on Computational Science, pages 1186‚Äì\n1193, Beijing, China, May.\nNal Kalchbrenner, Edward Grefenstette, and Phil Blun-\nsom. 2014. A convolutional neural network for\nmodelling sentences. In Proc. Annual Meeting of\nthe Association for Computational Linguistics , vol-\nume 1, pages 655‚Äì665, Baltimore, MD, USA, June.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for M-gram language modeling. In\nProc. IEEE International Conference on Acoustics,\nSpeech, and Signal Processing, volume 1, pages 181\n‚Äì 184, Detroit, Michigan, USA, May.\n1031\nStefan Kombrink, Tom ¬¥aÀás Mikolov, Martin KaraÔ¨Å ¬¥at,\nand Luk¬¥aÀás Burget. 2011. Recurrent neural network\nbased language modeling in meeting recognition. In\nProc. Interspeech Conference of the International\nSpeech Communication Association , pages 2877 ‚Äì\n2880, Florence, Italy, August.\nSiwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.\nRecurrent convolutional neural networks for text\nclassiÔ¨Åcation. In Proc. AAAI Conference on ArtiÔ¨Å-\ncial Intelligence, pages 2267‚Äì2273, Austin, Texas,\nUSA, January.\nQuoc V . Le and Tom¬¥aÀás Mikolov. 2014. Distributed\nrepresentations of sentences and documents. In\nProc. International Conference on Machine Learn-\ning, pages 1188‚Äì1196, Beijing, China, June.\nPhong Le and Willem Zuidema. 2015. Compositional\ndistributional semantics with long short term mem-\nory. In Proc. Joint Conference on Lexical and Com-\nputational Semantics , pages 10‚Äì19, Denver, CO,\nUSA, June.\nKun-Lin Liu, Wu-Jun Li, and Minyi Guo. 2012.\nEmoticon smoothed language models for twitter\nsentiment analysis. In Proc. AAAI Conference on\nArtiÔ¨Åcial Intelligence , pages 1678‚Äì1684, Toronto,\nOntario, Canada, July.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment anal-\nysis. In Proc. Annual Meeting of the Association\nfor Computational Linguistics, pages 142‚Äì150, Port-\nland, Oregon, USA, June.\nTom¬¥aÀás Mikolov, Martin KaraÔ¨Å¬¥at, Luk¬¥aÀás Burget, Jan H.\nÀáCernock¬¥y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In Proc.\nInterspeech Conference of the International Speech\nCommunication Association , pages 1045 ‚Äì 1048,\nMakuhari, Chiba, Japan, September.\nBo Pang and Lillian Lee. 2004. A sentimental educa-\ntion: Sentiment analysis using subjectivity summa-\nrization based on minimum cuts. In Proc. Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 271 ‚Äì 278, Barcelona, Spain, July.\nBo Pang, Lillian Lee, and Shivakumar Vaithyanathan.\n2002. Thumbs up?: Sentiment classiÔ¨Åcation using\nmachine learning techniques. In Proc. Conference\non Empirical Methods in NLP , volume 10, pages\n79‚Äì86, Philadelphia, PA, USA, July.\n¬¥Alvaro Peris and Francisco Casacuberta. 2015. A bidi-\nrectional recurrent neural language model for ma-\nchine translation. Procesamiento del Lenguaje Nat-\nural, 55:109‚Äì116, September.\nWenge Rong, Baolin Peng, Yuanxin Ouyang, Chao Li,\nand Zhang Xiong. 2014. Structural information\naware deep semi-supervised recurrent neural net-\nwork for sentiment analysis. Frontiers of Computer\nScience, 9(2):171‚Äì184.\nC¬¥ƒ±cero Nogueira dos Santos and Maira Gatti. 2014.\nDeep convolutional neural networks for sentiment\nanalysis of short texts. In Proc. International Con-\nference on Computational Linguistics, pages 69‚Äì78,\nDublin, Ireland, August.\nBj¬®orn Schuller, Amr E. Mousa, and Vryniotis\nVasileios. 2015. Sentiment analysis and opin-\nion mining: On optimal parameters and perfor-\nmances. WIREs Data Mining and Knowledge Dis-\ncovery, 5:255‚Äì263, September/October.\nRichard Socher, Alex Perelygin, Jean Y . Wu, Jason\nChuang, Christopher D. Manning, Andrew Y . Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proc. Conference on Empirical Meth-\nods in NLP , pages 1631‚Äì1642, Seattle, W A, USA,\nOctober.\nAndreas Stolcke. 2002. SRILM - an extensible lan-\nguage modeling toolkit. In Proc. International Con-\nference on Spoken Language Processing, volume 2,\npages 901 ‚Äì 904, Denver, Colorado, USA, Septem-\nber.\nMartin Sundermeyer, Ralf Schl¬®uter, and Hermann Ney.\n2012. LSTM neural networks for language model-\ning. In Proc. Interspeech Conference of the Inter-\nnational Speech Communication Association , Port-\nland, OR, USA, September.\nMaite Taboada, Julian Brooke, Milan ToÔ¨Åloski, Kim-\nberly V oll, and Manfred Stede. 2011. Lexicon-\nbased methods for sentiment analysis. Computa-\ntional linguistics, 37(2):267‚Äì307, June.\nKai Sheng Tai, Richard Socher, and Christopher D.\nManning. 2015. Improved semantic representa-\ntions from tree-structured long short-term memory\nnetworks. In Proc. Annual Meeting of the Associ-\nation for Computational Linguistics , pages 1556‚Äì\n1566, Beijing, China, July.\nSida I. Wang and Christopher D. Manning. 2012.\nBaselines and bigrams: Simple, good sentiment and\ntopic classiÔ¨Åcation. In Proc. Annual Meeting of the\nAssociation for Computational Linguistics, pages 90\n‚Äì 94, Jeju Island, Korea, July.\nFelix Weninger, Johannes Bergmann, and Bj ¬®orn\nSchuller. 2014. Introducing CURRENNT ‚Äì the\nMunich open-source CUDA RecurREnt Neural Net-\nwork Toolkit. Journal of Machine Learning Re-\nsearch, 15(99), October.\nWayne Xiong, Jasha Droppo, Xuedong Huang, Frank\nSeide, Mike Seltzer, Andreas Stolcke, Dong Yu, and\nGeoffrey Zweig. 2016. Achieving human parity\nin conversational speech recognition. Technical Re-\nport MSR-TR-2016-71, Microsoft Research, Octo-\nber.\n1032",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8265909552574158
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7215253710746765
    },
    {
      "name": "Recurrent neural network",
      "score": 0.6879556179046631
    },
    {
      "name": "Pooling",
      "score": 0.6481066942214966
    },
    {
      "name": "Discriminative model",
      "score": 0.6279984712600708
    },
    {
      "name": "Sentence",
      "score": 0.5755032300949097
    },
    {
      "name": "Natural language processing",
      "score": 0.561978816986084
    },
    {
      "name": "Generative grammar",
      "score": 0.5553753972053528
    },
    {
      "name": "Binary classification",
      "score": 0.4838096499443054
    },
    {
      "name": "Sentiment analysis",
      "score": 0.47089889645576477
    },
    {
      "name": "Word (group theory)",
      "score": 0.46877020597457886
    },
    {
      "name": "Language model",
      "score": 0.4566243886947632
    },
    {
      "name": "Generative model",
      "score": 0.4495689272880554
    },
    {
      "name": "Artificial neural network",
      "score": 0.39890119433403015
    },
    {
      "name": "Support vector machine",
      "score": 0.21400871872901917
    },
    {
      "name": "Linguistics",
      "score": 0.08419498801231384
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I186354981",
      "name": "University of Passau",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I47508984",
      "name": "Imperial College London",
      "country": "GB"
    }
  ]
}