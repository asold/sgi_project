{
  "title": "Enhancing Text-to-SQL Capabilities of Large Language Models: A Study on Prompt Design Strategies",
  "url": "https://openalex.org/W4389523875",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2901790570",
      "name": "Linyong Nan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2224021027",
      "name": "Yilun Zhao",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A5007156413",
      "name": "Weijin Zou",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A5112921371",
      "name": "Narutatsu Ri",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A3171351878",
      "name": "Jaesung Tae",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2639143393",
      "name": "Ellen Zhang",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A1983754593",
      "name": "Arman Cohan",
      "affiliations": [
        "Yale University",
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2913816252",
      "name": "Dragomir Radev",
      "affiliations": [
        "Yale University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385572953",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W4382202531",
    "https://openalex.org/W4226053975",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W4361019453",
    "https://openalex.org/W3035231859",
    "https://openalex.org/W3199077625",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W3034835156",
    "https://openalex.org/W4309953112",
    "https://openalex.org/W4302011807",
    "https://openalex.org/W2546950329",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W4389520255",
    "https://openalex.org/W4297633153",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4303649020",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W3175818566",
    "https://openalex.org/W2970442801",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2751448157",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2890431379",
    "https://openalex.org/W2148721079",
    "https://openalex.org/W2890867094",
    "https://openalex.org/W4385573504",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W3170721718",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2884709258",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4321277276"
  ],
  "abstract": "In-context learning (ICL) has emerged as a new approach to various natural language processing tasks, utilizing large language models (LLMs) to make predictions based on context that has been supplemented with a few examples or task-specific instructions. In this paper, we aim to extend this method to question answering tasks that utilize structured knowledge sources, and improve Text-to-SQL systems by exploring various prompt design strategies for employing LLMs. We conduct a systematic investigation into different demonstration selection methods and optimal instruction formats for prompting LLMs in the Text-to-SQL task. Our approach involves leveraging the syntactic structure of an example's SQL query to retrieve demonstrations, and we demonstrate that pursuing both diversity and similarity in demonstration selection leads to enhanced performance. Furthermore, we show that LLMs benefit from database-related knowledge augmentations. Our most effective strategy outperforms the state-of-the-art system by 2.5 points (Execution Accuracy) and the best fine-tuned system by 5.1 points on the Spider dataset. These results highlight the effectiveness of our approach in adapting LLMs to the Text-to-SQL task, and we present an analysis of the factors contributing to the success of our strategy.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14935–14956\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nEnhancing Text-to-SQL Capabilities of Large Language Models:\nA Study on Prompt Design Strategies\nLinyong Nan1 Yilun Zhao1 Weijin Zou1 Narutatsu Ri2 Jaesung Tae1\nEllen Zhang1 Arman Cohan1,3 Dragomir Radev1\n1Yale University 2Columbia University 3Allen Institute for AI\n{linyong.nan, yilun.zhao}@yale.edu\nAbstract\nIn-context learning (ICL) has emerged as a\nnew approach to various natural language pro-\ncessing tasks, utilizing large language models\n(LLMs) to make predictions based on context\nthat has been supplemented with a few exam-\nples or task-specific instructions. In this paper,\nwe aim to extend this method to question an-\nswering tasks that utilize structured knowledge\nsources, and improve Text-to-SQL systems by\nexploring various prompt design strategies for\nemploying LLMs. We conduct a systematic\ninvestigation into different demonstration se-\nlection methods and optimal instruction for-\nmats for prompting LLMs in the Text-to-SQL\ntask. Our approach involves leveraging the syn-\ntactic structure of an example’s SQL query to\nretrieve demonstrations, and we demonstrate\nthat pursuing both diversity and similarity in\ndemonstration selection leads to enhanced per-\nformance. Furthermore, we show that LLMs\nbenefit from database-related knowledge aug-\nmentations. Our most effective strategy outper-\nforms the state-of-the-art system by 2.5 points\n(Execution Accuracy) and the best fine-tuned\nsystem by 5.1 points on the Spider dataset.\nThese results highlight the effectiveness of our\napproach in adapting LLMs to the Text-to-SQL\ntask, and we present an analysis of the factors\ncontributing to the success of our strategy.\n1 Introduction\nQuestion answering using structured knowledge\nsource is a critical function of information retrieval\nsystems that act as an interface between humans\nand vast structured data repositories. Extracting\nand aggregating information accurately is a funda-\nmental requirement of these systems and is thus\na primary goal in their design. In recent years,\nthe neural symbolic design approach (Berant et al.,\n2013; Yao and Van Durme, 2014; Liang et al., 2017;\nGardner et al., 2018; Yu et al., 2018; Cheng et al.,\n2023) has become the preferred choice for such\nsystems for two main reasons. First, neural mod-\nels have inherent limitations, including a limited\nworking memory that is costly to access during in-\nference and a long-term memory that is unreliable\nto read from or write to, making it impractical to\nhave them directly read from large-scale knowl-\nedge sources. Second, understanding how a system\ndecides which information to retrieve and how to\naggregate it is crucial for assessing its reliability\nand robustness.\nRecent investigations have demonstrated the ef-\nfectiveness of the neural symbolic approach in pro-\nducing transparent reasoning process in formal lan-\nguage sequence (such as Text-to-SQL) for question\nanswering tasks based on databases or knowledge\ngraphs (Berant et al., 2013; Zhong et al., 2017; Yu\net al., 2018; Yin and Neubig, 2018; Yu et al., 2019;\nRen et al., 2021; Cheng et al., 2023). A typical sys-\ntem comprises a neural semantic parsing module\nthat translates user queries in natural language to\nformal language sequences (e.g., logical forms or\nexecutable code) and a symbolic reasoner module,\nsuch as database management system (DBMS), that\nexecutes the code on structured knowledge sources\nto extract the result. The primary objective of this\nwork is to improve the semantic parsing module, as\nit is essential in extracting answers from relational\ndatabases using SQL as the formal language.\nCurrent semantic parsing modules can be\nbroadly categorized based on their learning strate-\ngies. State-of-the-art systems involve fine-tuning\na pretrained language models on a large corpus\nof {question, SQL} pairs, enabling the model to\ngenerate code (Wang et al., 2020; Yin et al., 2020;\nScholak et al., 2021; Xie et al., 2022; Li et al.,\n2023). Alternatively, the in-context learning (ICL)\napproach exploits the inherent capabilities of large\nlanguage models (LLMs) to directly produce SQL\ncode by providing a well-defined task prompt (Xie\net al., 2022; Chen et al., 2022; Rajkumar et al.,\n2022; Ni et al., 2023). Existing research indicates\n14935\nthat LLMs using prompt-based semantic parsing\nunderperform their fine-tuned counterparts (Liu\net al., 2023), while recent studies also suggest that\nperformance of ICL-trained LLMs is significantly\naffected by the structure of the prompt (Liu et al.,\n2022; Rubin et al., 2022; Lu et al., 2022; Wei et al.,\n2022; Fu et al., 2023; Ye et al., 2023). This moti-\nvates us to examine various prompt configurations\nfor semantic parsing tasks, taking advantage of the\nlatest advancements of LLMs pertaining to our do-\nmain of interest.\nOur study focused on exploring various prompt\ndesign strategies for semantic parsing tasks in the\nText-to-SQL domain. We conducted a systematic\ninvestigation into different demonstration example\nselection criteria and instruction formats on Text-to-\nSQL datasets. Specifically, we propose to employ\nan example’s SQL syntactic structure as the basis\nfor retrieving demonstrations, thereby facilitating a\nmore accurate representation of the problem struc-\nture. Our approach revealed that selecting demon-\nstration examples with a dual emphasis on diversity\nand similarity objectives yields maximized gain in\nperformance. Our study also showed that LLMs\nbenefit from database-related knowledge augmen-\ntation in certain circumstances. Through experi-\nments, we identified the most effective strategy,\nwhich resulted in an Execution Accuracy score of\n84.4 on the Spider dataset (Yu et al., 2018). This\nscore is 2.5 points higher than the state-of-the-art\nsystem (Ni et al., 2023) and 5.1 points higher than\nthe best fine-tuned system (Scholak et al., 2021)\nat the time of writing.1 These results demonstrate\nthe effectiveness of our in-context learning scheme\nin adapting LLMs to our target task. Furthermore,\nwe present the empirical findings and analysis on\nthe factors that contributed to the success of our\nstrategy.2\n2 Methods\nTo design prompts for in-context learning in zero-\nshot or few-shot settings, it is important to find an\noptimal way to represent, augment, and arrange all\nresources in the input-output mapping. Addition-\nally, the task instructions should be formulated to\nalign with these resources. When few-shot learning\nis employed, the selection of a subset of demon-\n1Our comparison focuses on fine-tuning studies that em-\nployed the standard Transformer architecture without any layer\nmodifications or the inclusion of additional modules.\n2We will open source our code for experiments: https:\n//anonymous.url\nstrations from a pool of annotated examples for\neach test instance is another critical design choice\nthat can impact the ICL performance. We proposed\nenhancements for each of these components and\nevaluated them against existing methods.\n2.1 Demonstration Selection\nThe goal is to select a subset of annotated examples\nfrom a pool that offers the best context for solving\nthe test problem. While random selection from\nthe pool is one option, Liu et al. (2022) proposed\nkNN-augmented example selection (KATE), which\nretrieves k nearest neighbors from the pool based\non the input of the compared instances. To achieve\nthis, all the pool instances are first transformed\ninto continuous vectors using a sentence encoder.\nDuring inference, the input of a test instance is\nprojected into a latent space using the same encoder\nand then compared to the pool of vectors using\na similarity measure, such as negative Euclidean\ndistance or cosine similarity. Finally, the topk most\nsimilar annotated examples are selected from the\npool.\nStructured Prediction as Basis for Retrieval\nWe propose utilizing the output SQL queries to\nselect the demonstration examples, rather than us-\ning the input questions. This is because, unlike\nmany tasks where the output is a classification la-\nbel or extracted entity with little information about\nthe problem structure, Text-to-SQL demands struc-\ntured prediction which contains more explicit in-\nformation about the problem structure than that\nprovided in the input question. Furthermore, un-\nlike natural language questions that can only be\nconverted into continuous semantic vectors, SQL\nqueries can be easily transformed into discrete fea-\nture vectors based on their syntax, making their\ncomparison more efficient and transparent. To im-\nplement our proposal, we begin by converting the\nSQL queries of all pool instances into discrete syn-\ntax vectors. This is done by parsing the queries and\nidentifying their syntactic elements, including key-\nwords, operators, and identifiers. Each SQL query\nis then mapped to a \"Bag-of-Syntactic-Elements\"\nfeature vector, each entry of which indicates the\npresence of a syntactic element in the query, i.e.,\nwe assign 1 (instead of the count) if an element\nis present in the SQL query. During inference,\nwe first generate a draft of the SQL query using\na preliminary predictor. We then apply the same\nprocess to convert this draft query into a discrete\n14936\nvector, which is used to represent the test instance\nfor retrieving demonstration examples.\nBalancing Diversity and SimilarityWe propose\na new demonstration selection strategy that differs\nfrom Liu et al. (2022), which retrieves the most\nsimilar examples with continuous-valued measure-\nments for each test instance. In contrast, our strat-\negy seeks to balance similarity and diversity of\nthe demonstrations. This is achieved by chang-\ning the representation of the given example from\na continuous-valued vector denoting the question\nsemantics to a discrete-valued vector that captures\nthe SQL syntax. To obtain demonstration examples\nthat are similar to the given example, we first split\nthe pool of annotated examples into disjoint par-\ntitions that represent different categories. Specifi-\ncally, we use the difficulty-level based categoriza-\ntion derived from the Spider dataset (Yu et al.,\n2018), because it is developed strictly based on\nsyntactic coverage and structure of a SQL query,\nensuring that queries satisfying the same condi-\ntions are grouped into the same category. While\nalternative categorization options may exist, we\nleave this for exploration in future work. Given\na test instance, we use a preliminary predictor to\ngenerate a draft SQL query and, based on its cat-\negory, retrieve candidate examples that belong to\nthe relevant partition. Next, to select diverse exam-\nples from the candidate partitions, we implement\nk-means clustering on the discrete vectors of exam-\nples, selecting k diverse examples that are closest\nto each centroid of the cluster. The resulting ex-\namples exhibit similarity to the test example by\nsharing the same category, yet maintain diversity\nin problem structures. These demonstrations are\nthen used to construct the prompt. The procedure\nfor our demonstration selection strategy is outlined\nin Algorithm 1 of the appendix.\n2.2 Schema Representation in Instruction\nInstructions are crucial to designing prompts, as\nthey define the task by clarifying how provided re-\nsources can aid the inference process (Dong et al.,\n2023). Our primary focus lies in determining the\noptimal way to represent a structured knowledge\nsource within the instruction and identifying sup-\nplementary resources that can enhance the infer-\nence process.\nLinearization of Structured KnowledgeWe be-\ngin by altering the linearization of structured knowl-\nedge. In prior research (Xie et al., 2022), structured\nknowledge sources such as databases or tables have\nbeen linearized into a “text” sequence. Building on\nprevious methods (Rajkumar et al., 2022), we adopt\na representation of the database using a “code” se-\nquence, specifically the CREATE query employed to\nconstruct the table initially, as illustrated in listing 1\nand 2 of the Appendix. This linearization approach\nprovides data type information for each column\nand encompasses all foreign key constraint details\nwithin the database. Moreover, we modify other\nresources in the instructions, such as the question\nand example entries in the database, to conform\nto the code sequence style by appending them as\ncomments.\nSchema-related Knowledge Augmentation\nThe ontology of a database delineates the structure\nand semantics of the database by offering defini-\ntions for a set of classes (tables), their attributes\n(columns), and the relationships among them. We\ninitially enhance the semantics of each class and\nattribute by elaborating on their meanings within\nthe context of the entire database. Specifically,\nwe employ OpenAI’s gpt-3.5-turbo engine3 to\ngenerate a natural language definition for each\ncolumn in every table, considering all its values\nand other columns. We then incorporate these\ndefinitions into the input either by appending\nthem as a block comment or inserting them\nwithin the CREATE query as inline comments.\nFurthermore, we suggest augmenting the repre-\nsentation of the database structure by providing\nan Entity-Relationship summary that outlines the\nconnections between tables and specifies how\nthey can be joined. As depicted in Figure 9 of\nthe Appendix, an Entity-Relationship diagram of\na database is utilized to enumerate all possible\npaths between distinct tables. These paths are\nsubsequently arranged in descending order based\non their respective lengths. The resulting summary\nhas shown to be useful in our experiments for\ntest instances where multiple tables need to be\ncombined. Listing 5 further demonstrates our\naugmentations and how we arrange them to\nconstruct the prompt.\n2.3 Integrated Strategy for Text-to-SQL\nUpon examination, we found that models trained\nwith ICL exhibit sensitivity to the number of\ndemonstration examples, resulting in noticeable\nvariance in performance across models provided\n3Public API available at https://openai.com/api/.\n14937\nwith various numbers of demonstrations. To estab-\nlish substantial conclusions when comparing dis-\ntinct prompting approaches, we present the mean\nand standard deviation for models sharing identi-\ncal configurations except for the varying number\nof demonstrations. In addition, we employ a ma-\njority vote on these models exhibiting diverse per-\nformances. Specifically, we obtain the execution\nresults of different models’ greedy decoding predic-\ntions, eliminate those with execution errors by de-\nterministic database management system (DBMS),\nand choose the prediction that receives the major-\nity vote. Alternative integration methods, such as\nthe self-consistency sampling (Wang et al., 2023),\nare also available, but we reserve their exploration\nfor future research. The comprehensive results are\navailable in Figures 10, 11, 12 of the Appendix for\nreader’s perusal.\nWe propose the following procedure for con-\nstructing prompts for the Text-to-SQL task. Given\na set Aof annotated examples, we first establish\na categorization that divides the pool into disjoint\npartitions Aα,Aβ,..., , with each partition contain-\ning examples whose SQL queries share a relatively\nsimilar syntax structure. Next, we apply the k-\nMeans strategy detailed in Section 2.1 to obtain\ndiverse demonstration examples Dj for partition\nAj. For each example, the demonstration is con-\nstructed by transforming the database into multi-\nple CREATE queries and augmenting with schema-\nrelated knowledge. During inference, we employ a\npreliminary model to generate a draft SQL query,\nwhich is used to determine the problem category\nand thus the corresponding Dj for building the\nprompt. We obtain multiple predictions using vari-\nous numbers of shots in Dj and perform majority\nvoting to arrive at the final prediction. Details of\nthis approach are shown in Algorithm 2 of the ap-\npendix.\n3 Experiments\n3.1 Experimental Settings\nDataset We conduct comprehensive experiments\non the following four semantic parsing datasets:\n• Spider (Yu et al., 2018) is a cross-domain se-\nmantic parsing dataset that contains complex\nText-to-SQL problems. The data originates\nfrom 200 databases covering 138 different do-\nmains. We use the 7,000 training data as our\npool of annotated examples.\n• Spider-Syn (Gan et al., 2021a) replaced\nschema-related words in the questions of Spi-\nder examples with manually selected syn-\nonyms that reflect real-world question para-\nphrases to evaluate the robustness of systems.\n• Spider-DK (Gan et al., 2021b) defined five\ntypes of domain knowledge and modified\nsome Spider examples by adding domain\nknowledge to evaluate the cross-domain gen-\neralization capability of a given system.\n• Spider-Realistic (Deng et al., 2021) removed\nexplicit mentions of column names from Spi-\nder examples to reflect more realistic text-\ntable alignment settings, and selected eight ex-\nisting Text-to-SQL datasets for cross-domain\nevaluation.\nModel We evaluate different ICL strategies with\nCodex (Chen et al., 2021), a GPT-3 variant that was\nfinetuned on code data on the web and has demon-\nstrated state-of-the-art performance as the time of\nwriting (Ni et al., 2023). Specifically, we use the\ncode-davinci-002 engine and present the results\nof systems with prompts ranging from 1 to 10-shot.\nAdditionally, we report the few-shot results utiliz-\ning the ChatGPT (gpt-3.5-turbo) model. How-\never, due to its maximum context length limitation\nof 4096, we only obtain results for systems pro-\nvided with prompts ranging from 1 to 5-shot.4\nEvaluation Metric We use execution accuracy\nas the evaluation metric for all experiments, which\nmeasures the percentage of system predictions lead-\ning to the gold execution result.\nBaselines We compare the following prompting\nstrategies for generating SQL queries in few-shot\nand zero-shot settings.\nFew-shot\n• Random sampling (R): Select demonstration ex-\namples randomly from the pool.\n• Similarity sampling (S)\n• Diversity sampling (D): Select diverse examples\nfrom k-Means clusters of the pool.\n• Similarity-Diversity sampling (SD): Select ex-\namples based on Algorithm 1.\n• SD + schema augmentation (SA): Enhance in-\nstructions with schema knowledge (semantic aug-\nmentation or structure augmentation).\n4Public API available at https://openai.com/api/.\n14938\n(a) Few-shot results\n(b) Zero-shot results\nFigure 1: Few-shot and zero-shot results of Codex for all datasets. In the few-shot setting, error bars indicate\nmeans and standard deviations over performances of systems provided with prompts ranging from 4-shot to 10-shot.\nTo obtain the error bars for the random sampling approach, we conducted 3 independent runs using different\nrandom seeds. Schema augmentation utilized for the reported results in (a) is structure augmentation - add ontology\nsummary. In the zero-shot setting, the error bars indicate means and standard deviations over 3 independent runs.\nOur results suggest that 1) using similarity and diversity objectives in the sampling process, 2) including schema\nrepresentation in instructions, and 3) employing model voting with different shot outcomes both contribute to the\nimprovement of ICL performance.\n• SD + SA + Voting: Integrated strategy described\nin Algorithm 2.\nZero-shot\n• Baseline - DB as text-seq: Standard prompt for\nText-to-SQL task, where structured knowledge is\nlinearized as text sequence.\n• Baseline - DB as code-seq: Improve instructions\nby linearizing structured knowledge source as\nmultiple SQL CREATE queries.\n• Baseline - DB as code-seq + SA: Enhance in-\nstructions with schema knowledge.\n3.2 Main Results\nIn this section, we present a comprehensive analy-\nsis of various prompting strategies, assessing their\nefficacy across multiple datasets. The evaluation\nof demonstration sampling strategies in a few-\nshot setting testing on code-davinci-002 is illus-\ntrated in Figure 1a, and more few-shot results of\ngpt-3.5-turbo are shown in Figure 2. We com-\npared different demonstration selection strategies,\nincluding random selection, k-nearest neighbors\nselection (similarity sampling) 5, k-means selec-\ntion (diversity sampling), and our proposed ap-\nproach, which combines both similarity and di-\nversity. Moreover, we examined the impact of\naugmenting schema representation within the task\ninstructions and assessed the performance of our\n5Due to the deprecation of the Codex API in March 2023,\nsimilarity sampling experiments were only conducted on the\nSpider dataset.\n14939\nintegrated strategy. Our findings indicate that em-\nploying similarity and diversity objectives in the\nsampling process leads to better performance on\naverage across all datasets. Furthermore, incorpo-\nrating schema representation within the instructions\nenhances performance, and the implementation of\nvoting of models with different shot results in a\nmarked improvement in overall performance.\nFigure 2: Few-shot results of gpt-3.5-turbo for Spi-\nder. Error bars indicate means and standard deviations\nover performances of systems provided with 1-shot to\n5-shot prompts. Schema augmentation utilized for the\nreported results is semantic augmentation - add column\nsummary as block-comment.\nThe efficacy of schema augmentation is fur-\nther supported by experiments in a zero-shot set-\nting, as illustrated in Figure 1b. We compared\nsystems using different linearization methods for\nprompts: one that transforms the database into\na text sequence, and another that uses multiple\nCREATE queries to represent the database. The\nlatter method shows noticeable improvement in\nperformance. We also contrasted two separate\ntechniques for augmenting schema representation:\none that adds semantic information to each col-\numn within each table, and another that incorpo-\nrates entity-relationship knowledge into the schema.\nThe results suggest that structural augmentation\n(adding ontology summary) brings a slight greater\nimprovement in the few-shot setting for Codex\n(shown in Figure 5), while semantic augmenta-\ntion (adding column summary as block comments)\nproves more beneficial in the zero-shot setting for\nCodex and also the few-shot setting for ChatGPT\n(gpt-3.5-turbo). We hypothesize that this dif-\nference may arise from the less descriptive nature\nof structural augmentation, which calls for more\ndemonstrations in order to effectively understand\nand utilize the provided information. In future\nstudy, we will explore better structural schema aug-\nmentation that aligns to the zero-shot setting.\n4 Analysis\n4.1 Prediction-Syntax based Retrieval\nThe existing method for selecting demonstrations\nrelies on the semantic representations of the ques-\ntion and the database. We propose an alterna-\ntive method specifically for code generation tasks,\nwhich focuses on the syntax of the solution code.\nWe examined syntax coverage and syntax similarity\nof the prompts produced with different strategies.\nSyntax coverage is computed by counting the occur-\nrence of syntactic elements (keywords, operators,\nand identifiers), and dividing it by the total number\nof all syntactic elements. Syntax similarity, on the\nother hand, is measured by the mean Euclidean\ndistance between the discrete vector representation\nof the predicted SQL and vectors that represent\nthe gold SQLs of the demonstrations selected. As\nindicated in Table 1 of the appendix, both of these\nmetrics contribute to the quality of the examples\nselected. Furthermore, a simple summation of the\ntwo measurements suggests a correlation with the\nsystem’s performance, as illustrated in Figure 6 of\nthe appendix. We argue the efficacy of our strat-\negy through the following rationale: (1) in cases\nwhere the pool of annotated examples is limited\nin diversity of the problem structures, certain test\nproblems may lack similar examples available for\nretrieval; and (2) neither the semantic represen-\ntation of the question/database nor the distance\nmetric inherently support encapsulation and com-\nparison of different problem structures, whereas\nSQL syntax provides direct measurement of the\nproblem structures. Given these constraints, the\noptimal strategy is to select similar examples while\nensuring the coverage of as many syntax demon-\nstrations as feasible to mitigate potential failures in\nsimilarity-based retrieval.\n4.2 Comparative Analysis of Retrieval\nMethods\nWe conducted an examination of various similarity-\nbased retrieval methods and presented a compar-\native analysis of their performance in Figure 3.\nThe primary variable in this investigation was the\nrepresentation extracted for each example, with a\nfocus on extracting and comparing the following\n14940\nFigure 3: Comparison between various similarity based demonstration selection methods. Q indicates the embedding\nmodel employed to extract representation for the question; D stands for database, and S stands for SQL query.\nFigure 4: Comparison between various diversity based\ndemonstration selection methods.\nFigure 5: Comparison between various schema augmen-\ntations in few-shot and zero-shot settings.\nembedding types: (1) question embeddings gener-\nated by Sentence-BERT (Reimers and Gurevych,\n2019)6 , RoBERTa-base (Liu et al., 2020), and Ope-\nnAI’s text-embedding-ada-002; (2) combined\nquestion and database embeddings obtained by\n(i) employing a single model (i.e., T5-base (Raf-\nfel et al., 2020) finetuned on the Spider train-\ning split and text-embedding-ada-002) with the\ndatabase linearized as a text sequence or CREATE\nqueries, and (ii) utilizing separate models, specif-\nically RoBERTa-base for encoding questions and\n6HuggingFace model name: all-MiniLM-L6-V2\nCodeT5-base (Wang et al., 2021) or CodeBERT-\nbase (Feng et al., 2020) for encoding databases; (3)\nsyntactic embeddings of predicted SQL, generated\nby either binary coding to indicate the presence\nof SQL syntactic elements or by quantifying their\noccurrences; and finally, (4) embeddings that en-\ncode questions, databases and predicted SQL using\ntext-embedding-ada-002.\nThe following conclusions can be drawn\nabout the similarity-based retrieval methods\nfor Text-to-SQL task: (1) questions alone ef-\nfectively represent distinct examples for re-\ntrieval purposes; (2) RoBERTa-base provides\nbetter embeddings for comparisons relative to\ntext-embedding-ada-002; (3) it is feasible to\nemploy models that have not been fine-tuned\non Text-to-SQL examples for similarity-based re-\ntrieval, while still achieving comparable perfor-\nmance to fine-tuned models; (4) the linearization of\ndatabases as SQL queries facilitates the extraction\nof enhanced embeddings.\nAdditionally, we conducted a comparison be-\ntween multiple embeddings utilized for diversity-\nbased demonstration selection, encompassing em-\nbeddings that encode the semantics of questions,\ndatabases and predicted SQL, as well as embed-\ndings that capture the syntactic features of pre-\ndicted SQL. As depicted in Figure 4, the syntactic\nembeddings of predicted SQL serve as the most\neffective basis for contrasting different examples\nfor diversity-based retrieval purposes.\n4.3 Schema Augmentation\nFigure 5 presents the outcomes of various schema\naugmentations applied to the instruction. It is ob-\nserved that improvement is not apparent in the few-\nshot setting; however, in the zero-shot setting, the\n14941\nsemantic augmentation incorporating descriptions\nof all table columns proves to be beneficial.\n4.4 Effectiveness Analysis\nIn order to determine the problem types that ben-\nefit most or least from our proposed methods, we\nalso evaluate the performance of different mod-\nels across various problem categories within the\nSpider dataset. As indicated in Figure 7 of the\nappendix, our similarity-diversity strategy proves\nbeneficial for most problem types, with the excep-\ntion of the medium split, which includes the most\ndiverse problems. This is the case where similarity-\nbased retrieval fails and syntax coverage becomes\nmore crucial. Furthermore, we observe that aug-\nmenting schema semantics is more effective for\nthe easy and medium splits (albeit with high vari-\nance), while augmenting schema structure is more\neffective for more complex problems. This ob-\nvervation leads us to hypothesize that challenging\nproblems necessitate addressing a higher number\nof tables, thus requiring a more comprehensive un-\nderstanding of the entire database structure. Lastly,\nthe integrated approach is effective across all ex-\namples, offering increased benefits especially for\nthose difficult problems.\n4.5 Preliminary Models\nTo assess the impact of the choice of preliminary\nmodel used to generate the draft SQL on our ap-\nproach, we conducted tests involving our methods\nfor preliminary models with varying performance\nlevels. Figure 8 of the appendix reveals that the\npreliminary models have a relatively minor effect\non the performance of the similarity-diversity or\nintegrated approaches, exhibiting gradual improve-\nments as better preliminary models are utilized.\n5 Related Work\nExisting literature indicates the ability of large lan-\nguage models to adapt to new tasks at inference\ntime by learning from a few example demonstra-\ntions (Brown et al., 2020; Radford et al., 2019).\nThis new capability has been referred to as in-\ncontext learning. In this paper, we expand on pre-\nvious works that investigate the optimal representa-\ntions for prompt inputs.\n5.1 Prompt Organization\nPrompt organization investigates the task of select-\ning and organizing in-context examples, a critical\naspect of enhancing model performance. Several\nstudies (Sorensen et al., 2022; Gonen et al., 2022;\nWu et al., 2022; Hu et al., 2022; Lu et al., 2022)\nhave proposed metrics to measure the suitability of\nexamples with respect to the target objective and to\ndetermine the optimal ordering of them. Liu et al.\n(2022) suggest selecting examples that are seman-\ntically similar to the test example by employing\na k-NN approach in the embedding space. Rubin\net al. (2022) train a prompt retriever based on con-\ntrastive learning, wherein examples are classified as\neither positive or negative if they are ranked among\nthe top-k or bottom-k probabilities of a language\nmodel generating the target output, conditioned on\nthe retrieved example and the input. Zhang et al.\n(2022) suggests to actively select demonstrations\nusing Q-Learning. Su et al. (2023) introduces the\nV ote-k approach to selectively annotate diverse and\nrepresentative examples for pool construction, then\nretrieve based on the similarity. In contrast, our\napproach retrieve a diverse set of examples given a\npre-established pool. As the authors demonstrate\nthat having a diverse and representative pool is im-\nportant for the success of ICL, we posit that a sim-\nilar characteristic is equally important when com-\nposing the prompt, as this approach increases the\nlikelihood of including various syntactical usages\nor similar problem structures within the prompt.\n5.2 Prompt Formatting\nPrompt engineering is concerned with investigat-\ning the impact of prompt structure on downstream\ntask performance. For tasks that involve multi-step\nreasoning and higher complexity,Chain-of-thought\nprompting has been developed (Wei et al., 2023;\nKojima et al., 2023). This approach involves lay-\ning out the generation process over multiple steps\nand using the model’s own intermediate process as\ninput. Wang et al. (2023) proposes to sample mul-\ntiple different chain-of-thoughts then selects the\nmost consistent answer through marginalization of\nall possible reasoning paths. Press et al. (2023)\nsuggests that prompting LLMs to ask follow-up\nquestions is an effective way to construct the chain-\nof-thoughts process. Zhou et al. (2023) proposes an\nautomatic approach to identify the optimal prompt\nby searching over a pool of model generated in-\nstructions, assigning scores to them, and selecting\nthe prompt with the highest score.\n14942\n6 Conclusions\nIn this study, we investigated various prompt de-\nsign approaches for semantic parsing tasks in the\nText-to-SQL domain. We proposed an approach\nthat leverages an example’s SQL syntactic structure\nfor demonstration examples selection, emphasising\nboth diversity and similarity as the sampling ob-\njectives. Additionally, We found that LLMs gain\nbenefits from database-related knowledge augmen-\ntations. Future research can build upon our findings\nto examine the transferability of our approach to\nother domains. Through ongoing improvement of\nLLMs’ capabilities in semantic parsing, we aim to\ncontribute to the development of QA systems that\nare more accurate, robust and comprehensible.\nLimitations\nOne of the main limitations of this study is the re-\nproducibility problem. The experiments presented\nin this paper relied on the use of OpenAI APIs,\nwhich were available at the time of our research\nbut have since been or will be deprecated. This\nmeans that the results of our experiments cannot\nbe replicated using the same APIs, which hinders\nthe reproducibility of our findings. To address this\nlimitation, we will focus on providing experiments\nresults that are based on open-sourced LLMs (Tou-\nvron et al., 2023; Taori et al., 2023; Chiang et al.,\n2023) for greater transparency and reproducibility.\nAnother limitation is that it is not clear how our\napproach will benefit LLMs given smaller or more\nconstrained pools of annotated examples. Although\nwe postulate that our approach offers the advantage\nof providing a prompt with maximal coverage of\nsimilar problem structures when identically struc-\ntured problems cannot be found in the pool, we\ncould not substantiate this due to our limited bud-\nget and access to the OpenAI APIs.\nReferences\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1533–1544, Seattle, Wash-\nington, USA. Association for Computational Linguis-\ntics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluating\nlarge language models trained on code.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W. Cohen. 2022. Program of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588.\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu\nLi, Rahul Nadkarni, Yushi Hu, Caiming Xiong,\nDragomir Radev, Mari Ostendorf, Luke Zettlemoyer,\nNoah A. Smith, and Tao Yu. 2023. Binding language\nmodels in symbolic languages. In The Eleventh Inter-\nnational Conference on Learning Representations.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nXiang Deng, Ahmed Hassan Awadallah, Christopher\nMeek, Oleksandr Polozov, Huan Sun, and Matthew\nRichardson. 2021. Structure-grounded pretraining\nfor text-to-SQL. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1337–1350, Online. As-\nsociation for Computational Linguistics.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong\nWu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and\nZhifang Sui. 2023. A survey on in-context learning.\n14943\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nBERT: A pre-trained model for programming and\nnatural languages. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1536–1547, Online. Association for Computational\nLinguistics.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and\nTushar Khot. 2023. Complexity-based prompting for\nmulti-step reasoning. In The Eleventh International\nConference on Learning Representations.\nYujian Gan, Xinyun Chen, Qiuping Huang, Matthew\nPurver, John R. Woodward, Jinxia Xie, and Peng-\nsheng Huang. 2021a. Towards robustness of text-\nto-SQL models against synonym substitution. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 2505–\n2515, Online. Association for Computational Lin-\nguistics.\nYujian Gan, Xinyun Chen, and Matthew Purver. 2021b.\nExploring underexplored limitations of cross-domain\ntext-to-SQL generalization. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 8926–8931, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nMatt Gardner, Pradeep Dasigi, Srinivasan Iyer, Alane\nSuhr, and Luke Zettlemoyer. 2018. Neural semantic\nparsing. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics: Tu-\ntorial Abstracts, pages 17–18, Melbourne, Australia.\nAssociation for Computational Linguistics.\nHila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith,\nand Luke Zettlemoyer. 2022. Demystifying prompts\nin language models via perplexity estimation.\nYushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu,\nNoah A. Smith, and Mari Ostendorf. 2022. In-\ncontext learning for few-shot dialogue state tracking.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 2627–2643, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2023. Large lan-\nguage models are zero-shot reasoners.\nHaoyang Li, Jing Zhang, Cuiping Li, and Hong Chen.\n2023. Resdsql: Decoupling schema linking and\nskeleton parsing for text-to-sql. In AAAI.\nChen Liang, Jonathan Berant, Quoc Le, Kenneth D. For-\nbus, and Ni Lao. 2017. Neural symbolic machines:\nLearning semantic parsers on Freebase with weak\nsupervision. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 23–33, Vancouver,\nCanada. Association for Computational Linguistics.\nAiwei Liu, Xuming Hu, Lijie Wen, and Philip S. Yu.\n2023. A comprehensive evaluation of chatgpt’s zero-\nshot text-to-sql capability.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nRo{bert}a: A robustly optimized {bert} pretraining\napproach.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nAnsong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov,\nWen tau Yih, Sida I. Wang, and Xi Victoria Lin. 2023.\nLever: Learning to verify language-to-code genera-\ntion with execution.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2023. Measuring\nand narrowing the compositionality gap in language\nmodels.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nNitarshan Rajkumar, Raymond Li, and Dzmitry Bah-\ndanau. 2022. Evaluating the text-to-sql capabilities\nof large language models.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\n14944\nHongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Michi-\nhiro Yasunaga, Haitian Sun, Dale Schuurmans, Jure\nLeskovec, and Denny Zhou. 2021. Lego: Latent\nexecution-guided reasoning for multi-hop question\nanswering on knowledge graphs. In Proceedings of\nthe 38th International Conference on Machine Learn-\ning, volume 139 ofProceedings of Machine Learning\nResearch, pages 8959–8970. PMLR.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2655–2671, Seattle, United States.\nAssociation for Computational Linguistics.\nTorsten Scholak, Nathan Schucher, and Dzmitry Bah-\ndanau. 2021. PICARD: Parsing incrementally for\nconstrained auto-regressive decoding from language\nmodels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9895–9901, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nTaylor Sorensen, Joshua Robinson, Christopher Ryt-\nting, Alexander Shaw, Kyle Rogers, Alexia Delorey,\nMahmoud Khalil, Nancy Fulda, and David Wingate.\n2022. An information-theoretic approach to prompt\nengineering without ground truth labels. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 819–862, Dublin, Ireland. Association\nfor Computational Linguistics.\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,\nTianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A. Smith, and Tao Yu. 2023.\nSelective annotation makes language models better\nfew-shot learners. In The Eleventh International Con-\nference on Learning Representations.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr\nPolozov, and Matthew Richardson. 2020. RAT-SQL:\nRelation-aware schema encoding and linking for text-\nto-SQL parsers. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7567–7578, Online. Association for\nComputational Linguistics.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023. Self-consistency improves\nchain of thought reasoning in language models. In\nThe Eleventh International Conference on Learning\nRepresentations.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H.\nHoi. 2021. CodeT5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understand-\ning and generation. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 8696–8708, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nZhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Ling-\npeng Kong. 2022. Self-adaptive in-context learning.\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Vic-\ntor Zhong, Bailin Wang, Chengzu Li, Connor Boyle,\nAnsong Ni, Ziyu Yao, Dragomir Radev, Caiming\nXiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,\nLuke Zettlemoyer, and Tao Yu. 2022. UnifiedSKG:\nUnifying and multi-tasking structured knowledge\ngrounding with text-to-text language models. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 602–631,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nXuchen Yao and Benjamin Van Durme. 2014. Infor-\nmation extraction over structured data: Question an-\nswering with Freebase. In Proceedings of the 52nd\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 956–966,\nBaltimore, Maryland. Association for Computational\nLinguistics.\nSeonghyeon Ye, Hyeonbin Hwang, Sohee Yang,\nHyeongu Yun, Yireun Kim, and Minjoon Seo. 2023.\nIn-context instruction learning.\nPengcheng Yin and Graham Neubig. 2018. TRANX:\nA transition-based neural abstract syntax parser for\nsemantic parsing and code generation. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing: System Demonstra-\ntions, pages 7–12, Brussels, Belgium. Association\nfor Computational Linguistics.\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\nbastian Riedel. 2020. TaBERT: Pretraining for joint\nunderstanding of textual and tabular data. InProceed-\nings of the 58th Annual Meeting of the Association\n14945\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\nSungrok Shim, Tao Chen, Alexander Fabbri, Zifan\nLi, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, Walter\nLasecki, and Dragomir Radev. 2019. CoSQL: A\nconversational text-to-SQL challenge towards cross-\ndomain natural language interfaces to databases. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1962–\n1979, Hong Kong, China. Association for Computa-\ntional Linguistics.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, Zilin Zhang, and Dragomir\nRadev. 2018. Spider: A large-scale human-labeled\ndataset for complex and cross-domain semantic pars-\ning and text-to-SQL task. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3911–3921, Brussels, Bel-\ngium. Association for Computational Linguistics.\nYiming Zhang, Shi Feng, and Chenhao Tan. 2022. Ac-\ntive example selection for in-context learning. InPro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 9134–\n9148, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2017. Seq2sql: Generating structured queries\nfrom natural language using reinforcement learning.\nCoRR, abs/1709.00103.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2023. Large language models are human-level\nprompt engineers. In The Eleventh International\nConference on Learning Representations.\n14946\nAppendix\nAlgorithm 1:Similarity-Diversity Demonstration Selection\nInput: Set of annotated examples A, test examples T, # demonstrations k, categorization\n{α,β,... }\nResult: Set of prompts P, where Pi is the prompt for test example Ti\n/* Split A into disjoint partitions Aα,β,... */\nfor Ai in annotated setAdo\nci = get_category(Ai.SQL);\nAci .append(Ai);\nVi = get_syntax_vectors(Ai.SQL);\nend\n/* Prepare demonstrations Dj for each partition Aj */\nfor partition Aj in Aα,β,... do\nM = k-Means_clustering(Vj, k);\n/* Vj is set of discrete vectors for examples in Aj, M has k centroids\nµ1,...,µ k */\nfor µi in M do\nDj.append(get_nearest(A,µi));\nend\nend\n/* Build test prompts */\nfor Ti in test setT do\nTi.SQL = initial_predictor(Ti);\nci = get_category(Ti.SQL);\nPi = build_prompt(Dci ,Ti);\nend\nreturn P\n14947\nAlgorithm 2:Integrated Strategy\nInput: Set of annotated examples A, test examples T, # demonstrations k, categorization\n{α,β,... }, and from Algorithm 1: disjoint partitions {Aα,Aβ,...} and corresponding\ndemonstrations {Dα,Dβ,...}\nResult: Set of SQL predictions SP, where SPi is the final prediction for test example Ti\nfor Ti in test setT do\nTi.SQL = initial_predictor(Ti);\nci = get_category(Ti.SQL);\nfor n= 4to kdo\nPn\ni = build_prompt(Dci [: n],Ti);\nPn∗\ni = augment_schema(Pn\ni );\nSPn\ni = Model(Pn∗\ni );\nERn\ni = DBMS(SPn\ni);\nend\nER∗\ni = Remove_Exec_Errors(ERi);\nSPi = Majority_Vote(ER∗\ni);\nend\nreturn SP\nCoverage Similarity Execution Accuracy\nRandom 0.38 0.24 76.03\nSimilarity 0.35 0.30 78.33\nDiversity 0.43 0.23 78.64\nSimilarity-Diversity 0.50 0.26 80.32\nTable 1: Average syntax coverage and similarity measures of the prompt for different demonstration selection\nstrategies and the corresponding execution accuracies.\n14948\nFigure 6: Correlation between syntax coverage and similarity measures of prompts and execution accuracy.\nFigure 7: Effects of various prompting strategies on Text-to-SQL problems of different difficulty levels.\nFigure 8: Effects of preliminary model on proposed strategies.\n14949\n1 Given the following database schema :\n2 gymnast : gymnast_id , floor_exercise_points , pommel_horse_points , rings_points ,\nvault_points , parallel_bars_points , horizontal_bar_points , total_points |\npeople : people_id , name , age , height , hometown\n3\n4 Answer the following : Return the total points of the gymnast with the lowest\nage .\n5\n6 select t1. total_points from gymnast as t1 join people as t2 on t1. gymnast_id =\nt2. people_id order by t2. age asc limit 1\nListing 1: Baseline prompt with text representation of the database.\n1 /* Given the following database schema : */\n2 CREATE TABLE IF NOT EXISTS \" gymnast \" (\n3 \" Gymnast_ID \" int ,\n4 \" Floor_Exercise_Points \" real ,\n5 \" Pommel_Horse_Points \" real ,\n6 \" Rings_Points \" real ,\n7 \" Vault_Points \" real ,\n8 \" Parallel_Bars_Points \" real ,\n9 \" Horizontal_Bar_Points \" real ,\n10 \" Total_Points \" real ,\n11 PRIMARY KEY (\" Gymnast_ID \"),\n12 FOREIGN KEY (\" Gymnast_ID \") REFERENCES \" people \"(\" People_ID \")\n13 );\n14 CREATE TABLE IF NOT EXISTS \" people \" (\n15 \" People_ID \" int ,\n16 \" Name \" text ,\n17 \" Age \" real ,\n18 \" Height \" real ,\n19 \" Hometown \" text ,\n20 PRIMARY KEY (\" People_ID \")\n21 );\n22\n23 /* Answer the following : Return the total points of the gymnast with the lowest\nage . */\n24\n25 select t1. total_points from gymnast as t1 join people as t2 on t1. gymnast_id =\nt2. people_id order by t2. age asc limit 1\nListing 2: Baseline prompt with code representation of the database.\n14950\n1 /* Given the following database schema : */\n2 CREATE TABLE IF NOT EXISTS \" department \" (\n3 \" Department_ID \" int , -- a unique identifier for a department\n4 \" Name \" text , -- the name of the department\n5 \" Creation \" text , -- the date the department was created\n6 \" Ranking \" int , -- the ranking of the department within the organization\n7 \" Budget_in_Billions \" real , -- the department 's budget in billions of\ndollars\n8 \" Num_Employees \" real , -- the number of employees in the department\n9 PRIMARY KEY (\" Department_ID \")\n10 );\n11 CREATE TABLE IF NOT EXISTS \" head \" (\n12 \" head_ID \" int , -- a unique identifier for the head of a department\n13 \" name \" text , -- the name of the head of the department\n14 \" born_state \" text , -- the state where the head of the department was born\n15 \" age \" real , -- the age of the head of the department\n16 PRIMARY KEY (\" head_ID \")\n17 );\n18 CREATE TABLE IF NOT EXISTS \" management \" (\n19 \" department_ID \" int , -- the unique identifier for the department being\nmanaged\n20 \" head_ID \" int , -- the unique identifier for the head of the department\n21 \" temporary_acting \" text , -- whether the head of the department is serving\nin a temporary or acting capacity\n22 PRIMARY KEY (\" Department_ID \", \" head_ID \")\n23 FOREIGN KEY (\" Department_ID \") REFERENCES `department `(\" Department_ID \")\n24 FOREIGN KEY (\" head_ID \") REFERENCES `head `(\" head_ID \")\n25 );\n26\n27 /* Answer the following : What are the distinct creation years of the\ndepartments managed by a secretary born in state 'Alabama '? */\n28\n29 select distinct t1. creation from department as t1 join management as t2 on t1.\ndepartment_id = t2. department_id join head as t3 on t2. head_id = t3. head_id\nwhere t3. born_state = 'Alabama '\nListing 3: Prompt with semantic augmentation of the schema as inline comment.\n14951\n1 /* Given the following database schema : */\n2 CREATE TABLE IF NOT EXISTS \" department \" (\n3 \" Department_ID \" int ,\n4 \" Name \" text ,\n5 \" Creation \" text ,\n6 \" Ranking \" int ,\n7 \" Budget_in_Billions \" real ,\n8 \" Num_Employees \" real ,\n9 PRIMARY KEY (\" Department_ID \")\n10 );\n11 CREATE TABLE IF NOT EXISTS \" head \" (\n12 \" head_ID \" int ,\n13 \" name \" text ,\n14 \" born_state \" text ,\n15 \" age \" real ,\n16 PRIMARY KEY (\" head_ID \")\n17 );\n18 CREATE TABLE IF NOT EXISTS \" management \" (\n19 \" department_ID \" int ,\n20 \" head_ID \" int ,\n21 \" temporary_acting \" text ,\n22 PRIMARY KEY (\" Department_ID \",\" head_ID \"),\n23 FOREIGN KEY (\" Department_ID \") REFERENCES `department `(\" Department_ID \"),\n24 FOREIGN KEY (\" head_ID \") REFERENCES `head `(\" head_ID \")\n25 );\n26\n27 /* Table column descriptions :\n28 {' department ': { ' Department_ID ': 'a unique identifier for a department ', 'Name\n': 'the name of the department ', 'Creation ': 'the date the department was\ncreated ', 'Ranking ': 'the ranking of the department within the organization\n', 'Budget_in_Billions ': \" the department 's budget in billions of dollars \",\n'Num_Employees ': 'the number of employees in the department '} , 'head ': { '\nhead_ID ': 'a unique identifier for the head of a department ', 'name ': 'the\nname of the head of the department ', 'born_state ': 'the state where the\nhead of the department was born ', 'age ': 'the age of the head of the\ndepartment '} , 'management ': { ' department_ID ': 'the unique identifier for\nthe department being managed ', 'head_ID ': 'the unique identifier for the\nhead of the department ', 'temporary_acting ': 'whether the head of the\ndepartment is serving in a temporary or acting capacity '}} */\n29 /* Answer the following : What are the distinct creation years of the\ndepartments managed by a secretary born in state 'Alabama '? */\n30\n31 select distinct t1. creation from department as t1 join management as t2 on t1.\ndepartment_id = t2. department_id join head as t3 on t2. head_id = t3. head_id\nwhere t3. born_state = 'Alabama '\nListing 4: Prompt with semantic augmentation of the schema as block comment.\n14952\ncontinents.contid -> countries.continent, countries.countryid -> \ncar_makers.country, car_makers.id -> model_list.maker, model_list.model -> \ncar_names.model, car_names.makeid -> cars_data.id\nemployee.emp_num -> department.emp_num, department.dept_code -> course.dept_code, \ncourse.crs_code -> class.crs_code, class.class_code -> enroll.class_code\ndepartment.dept_code -> student.dept_code, student.stu_num -> enroll.stu_num\nemployee.emp_num -> class.prof_num\nemployee.emp_num -> professor.emp_num\ndepartment.dept_code -> professor.dept_code\nFigure 9: Examples of schema structure representation construction.\n14953\n1 /* Given the following database schema : */\n2 CREATE TABLE IF NOT EXISTS \" continents \" (\n3 \" ContId \" INTEGER PRIMARY KEY ,\n4 \" Continent \" TEXT\n5 );\n6 CREATE TABLE IF NOT EXISTS \" countries \" (\n7 \" CountryId \" INTEGER PRIMARY KEY ,\n8 \" CountryName \" TEXT ,\n9 \" Continent \" INTEGER ,\n10 FOREIGN KEY ( Continent ) REFERENCES continents ( ContId )\n11 );\n12 CREATE TABLE IF NOT EXISTS \" car_makers \" (\n13 \"Id\" INTEGER PRIMARY KEY ,\n14 \" Maker \" TEXT ,\n15 \" FullName \" TEXT ,\n16 \" Country \" TEXT ,\n17 FOREIGN KEY ( Country ) REFERENCES countries ( CountryId )\n18 );\n19 CREATE TABLE IF NOT EXISTS \" model_list \" (\n20 \" ModelId \" INTEGER PRIMARY KEY ,\n21 \" Maker \" INTEGER ,\n22 \" Model \" TEXT UNIQUE ,\n23 FOREIGN KEY ( Maker ) REFERENCES car_makers (Id)\n24\n25 );\n26 CREATE TABLE IF NOT EXISTS \" car_names \" (\n27 \" MakeId \" INTEGER PRIMARY KEY ,\n28 \" Model \" TEXT ,\n29 \" Make \" TEXT ,\n30 FOREIGN KEY ( Model ) REFERENCES model_list ( Model )\n31 );\n32 CREATE TABLE IF NOT EXISTS \" cars_data \" (\n33 \"Id\" INTEGER PRIMARY KEY ,\n34 \" MPG \" TEXT ,\n35 \" Cylinders \" INTEGER ,\n36 \" Edispl \" REAL ,\n37 \" Horsepower \" TEXT ,\n38 \" Weight \" INTEGER ,\n39 \" Accelerate \" REAL ,\n40 \" Year \" INTEGER ,\n41 FOREIGN KEY (Id) REFERENCES car_names ( MakeId )\n42 );\n43\n44 /*\n45 Database ontology :\n46 continents . contid -> countries . continent , countries . countryid -> car_makers .\ncountry , car_makers .id -> model_list . maker , model_list . model -> car_names .\nmodel , car_names . makeid -> cars_data .id\n47 */\n48 /* Answer the following : How many continents are there ? */\n49\n50 select count (*) from continents ;\nListing 5: Prompt with structure augmentation of the schema.\n14954\nFigure 10: Few-shot results for comparing different sampling strategies with different number of demonstra-\ntion examples selected for the prompt.\nFigure 11: Few-shot results for comparing different schema representation augmentation methods with\ndifferent number of demonstration examples selected for the prompt.\n14955\nFigure 12: Few-shot results for comparing different sampling strategies on Text-to-SQL problems of different\ndifficulty levels, with different number of demonstration examples selected for the prompt.\n14956",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8519445061683655
    },
    {
      "name": "SQL",
      "score": 0.7915689945220947
    },
    {
      "name": "Task (project management)",
      "score": 0.6273509860038757
    },
    {
      "name": "Context (archaeology)",
      "score": 0.612139880657196
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.5066186189651489
    },
    {
      "name": "Natural language processing",
      "score": 0.43135330080986023
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40588629245758057
    },
    {
      "name": "Programming language",
      "score": 0.24090644717216492
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}