{
  "title": "Can Large Language Models Fix Data Annotation Errors? An Empirical Study Using Debatepedia for Query-Focused Text Summarization",
  "url": "https://openalex.org/W4389520290",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3031002218",
      "name": "Md Tahmid Rahman Laskar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2075277251",
      "name": "Mizanur Rahman",
      "affiliations": [
        "Royal Bank of Canada"
      ]
    },
    {
      "id": "https://openalex.org/A2053237779",
      "name": "Israt Jahan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2146337708",
      "name": "Enamul Hoque",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2186921142",
      "name": "Jimmy Huang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3033187248",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4321276803",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3196820561",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W4225958595",
    "https://openalex.org/W3111401020",
    "https://openalex.org/W4385570371",
    "https://openalex.org/W2742009068",
    "https://openalex.org/W3118109963",
    "https://openalex.org/W3029927342",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4385571411",
    "https://openalex.org/W3022773562",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W4376167348",
    "https://openalex.org/W4361806892",
    "https://openalex.org/W4389519239",
    "https://openalex.org/W4317553041",
    "https://openalex.org/W4391973028",
    "https://openalex.org/W2963898730",
    "https://openalex.org/W3094041811",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W4293583643",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3171639395",
    "https://openalex.org/W3114312194",
    "https://openalex.org/W3166614014",
    "https://openalex.org/W4220732108",
    "https://openalex.org/W4288088047",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963443217",
    "https://openalex.org/W3105310509",
    "https://openalex.org/W4379259169",
    "https://openalex.org/W3110414860",
    "https://openalex.org/W2598569220"
  ],
  "abstract": "Debatepedia is a publicly available dataset consisting of arguments and counter-arguments on controversial topics that has been widely used for the single-document query-focused abstractive summarization task in recent years. However, it has been recently found that this dataset is limited by noise and even most queries in this dataset do not have any relevance to the respective document. In this paper, we study whether large language models (LLMs) can be utilized to clean the Debatepedia dataset to make it suitable for query-focused abstractive summarization. More specifically, we harness the language generation capabilities of two LLMs, namely, ChatGPT and PaLM to regenerate its queries. Based on our experiments, we find that solely depending on large language models for query correction may not be very useful for data cleaning. However, we observe that leveraging a rule-based approach for data sampling followed by query regeneration using LLMs (especially ChatGPT) for the sampled instances may ensure a higher quality version of this dataset suitable for the development of more generalized query-focused text summarization models.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10245–10255\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCan Large Language Models Fix Data Annotation Errors? An Empirical\nStudy Using Debatepedia for Query-Focused Text Summarization\nMd Tahmid Rahman Laskar1,3, Mizanur Rahman 2,3, Israt Jahan 3,\nEnamul Hoque3, Jimmy Xiangji Huang 3,∗\n1Dialpad Canada Inc., 2Royal Bank of Canada, 3York University†\nToronto, Ontario, Canada\n{tahmid20,mizanurr,israt18,enamulh,jhuang}@yorku.ca\nAbstract\nDebatepedia is a publicly available dataset con-\nsisting of arguments and counter-arguments on\ncontroversial topics that has been widely used\nfor the single-document query-focused abstrac-\ntive summarization task in recent years. How-\never, it has been recently found that this dataset\nis limited by noise and even most queries in this\ndataset do not have any relevance to the respec-\ntive document. To this end, this paper aims to\nstudy whether large language models (LLMs)\ncan be utilized to clean the Debatepedia dataset\nto make it suitable for query-focused abstrac-\ntive summarization. More specifically, we har-\nness the language generation capabilities of two\nLLMs, namely, ChatGPT1, and PaLM2 to re-\ngenerate its queries. Based on our experiments,\nwe find that only fixing the queries in Debate-\npedia via LLMs may not be useful. However,\nleveraging a rule-based approach via filtering\nout noisy instances followed by query regenera-\ntion using LLMs for the sampled instances may\nensure a higher quality version of this dataset\nsuitable for the development of more general-\nized query-focused text summarization models.\n1 Introduction\nText summarization is a natural language process-\ning technique that involves generating a concise\nand coherent summary of a longer piece of text\nwhile preserving its most important information\n(Yao et al., 2017). Query-focused text summariza-\ntion is a specific type of summarization that gener-\nates a summary of the given text that is focused on\nanswering a specific question (Laskar et al., 2020c)\nor addressing a particular topic, rather than provid-\ning a general overview of the text. (Baumel et al.,\n2018; Goodwin et al., 2020; Su et al., 2020; Xu\nand Lapata, 2021; Laskar et al., 2020a,b, 2022).\n∗Contact Author.\n†All work being done at York University.\n1https://openai.com/blog/chatgpt/\n2https://ai.google/discover/palm2\nOne widely used dataset for this task is the De-\nbatepedia dataset that consists of arguments and\ncounter-arguments on conversational topics (Nema\net al., 2017). The query-focused summarization of\nargumentative text is a challenging task that has\ngained increasing attention in recent years due to\nits potential applications in various domains, such\nas policy-making, journalism, and legal reasoning.\nHowever, it has been recently found that the\nquality of the Debatepedia dataset that is widely\nused for this task is limited by noise, with many\nof the queries in this dataset having no relevance\nwith the source document (Laskar et al., 2022).\nSince Debatepedia is a rich source of argumen-\ntative text on controversial topics that can serve\nas a valuable resource for the development and\nevaluation of summarization models, in this paper,\nwe present a novel methodology to clean the De-\nbatepedia dataset via re-annotation of its queries\nto make it a useful resource for query-focused ab-\nstractive summarization. Our data annotation ap-\nproach leverages large pre-trained language models\n(Devlin et al., 2018; Brown et al., 2020; Ouyang\net al., 2022), such as ChatGPT (OpenAI, 2023)\nand PaLM-2 (Anil et al., 2023), that have demon-\nstrated impressive capability of generating fluent\nand coherent text (Laskar et al., 2023a). Using\nthese LLMs, we regenerate the queries in the De-\nbatepedia dataset that are more likely to have no\nrelevance to the document and the summary. More\nspecifically, this paper aims to investigate whether\nLLMs can be utilized to fix the existing issues in the\nDebatepedia dataset. Our extensive experiments\nshow that utilizing rule-based filtering to eradicate\nnoisy instances alongside leveraging the generative\npower of LLMs to regenerate the irrelevant queries\nleads to performance improvement in terms of both\nquery relevance and summary generation quality.\nWe will make this LLM-annotated cleaned version\nof Debatepedia publicly available3.\n3https://github.com/tahmedge/CQSUMDP\n10245\n2 Related Work\nQuery-focused text summarization using neural\nmodels has gained increasing attention in recent\nyears (Baumel et al., 2018; Laskar et al., 2022).\nThe recent success of transformer-based models\n(Vaswani et al., 2017; Lewis et al., 2019; Raffel\net al., 2019; Zhang et al., 2019) on generic abstrac-\ntive summarization has also inspired researchers\nto utilize such models for query-based abstractive\nsummarization (Goodwin et al., 2020; Vig et al.,\n2021; Laskar et al., 2020a,b, 2022), leading to state-\nof-the-art performance in benchmark query-based\nsummarization datasets, such as DUC (Feigenblat\net al., 2017; Xu and Lapata, 2020), AQuaMuSe\n(Kulkarni et al., 2020), QMSum (Zhong et al.,\n2021), Debatepedia (Nema et al., 2017), etc.\nAmong the datasets mentioned above, one no-\ntable exception is the Debatepedia dataset since\nit requires generating summaries from a docu-\nment containing argumentative text (i.e., arguments\nand counter-arguments). However, it has been\nfound recently that many samples in the Debate-\npedia dataset are not actually query oriented while\nmodels that are trained without considering the\nquery relevance could achieve almost similar per-\nformance as the query-focused summarization mod-\nels (Laskar et al., 2022). Thus, there remains a\nscarcity of datasets specifically tailored to generate\nquery-focused summaries of argumentative texts.\nThough some studies (Abdullah and Chali, 2020)\nhave attempted to generate the queries in generic\nsummarization datasets (e.g., CNNDM (Nallapati\net al., 2016)), we find that these queries are gen-\nerated by directly extracting words from the ref-\nerence summaries, leading to unexpected access\nto the keywords in the reference summaries for\nthe summarization models. LLMs have received\na lot of attention recently due to their impressive\nlanguage generation capability – ensuring high flu-\nency, coherence, and grammatical correctness on\nthe generated texts (Laskar et al., 2023a; Qin et al.,\n2023; Bang et al., 2023; Yang et al., 2023; Wang\net al., 2023; Koco´n et al., 2023). More importantly,\nChatGPT like LLMs also demonstrated impressive\ncapability for data annotation (Wang et al., 2021;\nDing et al., 2022; Gilardi et al., 2023). To this end,\nin this paper, we study how to fix the queries in\nDebatepedia using LLMs to construct a cleaned\nversion of the dataset to make it suitable for query-\nfocused summarization of argumentative texts.\n3 Our Annotation Methodology\nDebatepedia is a publicly available dataset of ar-\nguments and counter-arguments on debate topics,\nproposed by Nema et al. (Nema et al., 2017). It con-\ntains about 13K query-document-summary pairs.\nThe average number of words per document, sum-\nmary, and query in the Debatepedia dataset is 66.4,\n11.16, and 9.97, respectively. The dataset covers a\nwide range of topics, such as politics, sports, and\ntechnology, and has been extensively used in recent\nyears to build query-based summarization models\nfor argumentative text (Laskar et al., 2022). How-\never, the quality of Debatepedia as a dataset for\nquery-based summarization has lots of limitations\n(see Table 5 in Appendix A.1 for some examples),\nas it has been found recently that many queries\nin this dataset are not relevant to the document\n(Laskar et al., 2022). To address these limitations,\nwe propose a methodology for cleaning the Debate-\npedia dataset via leveraging two popular LLMs:\nChatGPT (Laskar et al., 2023a) and PaLM-2 (Anil\net al., 2023), as annotators. In this regard, we ini-\ntially explored various techniques to identify how\nto effectively sample the noisy instances, and subse-\nquently, we regenerated the queries for the sampled\ninstances. We denote our ChatGPT and PaLM an-\nnotated versions of Debatepedia (DP) for Query\nFocused Abstractive Summarization as the CQ-\nSumDP and the PQSumDP, respectively.\n3.1 Data Sampling\nWe explore two approaches for data sampling. In\none approach, we study whether only fixing the\nqueries in the Debatepedia dataset via leveraging\nLLMs for query regeneration could address the is-\nsues in the Debatepedia dataset or not. For this\npurpose, we ask LLMs to identify the instances in\nthe Debatepedia dataset where the queries seemed\nirrelevant. In our other approach, we first sam-\nple data instances based on some filtering rules by\nexcluding instances that are less relevant for query-\nfocused summarization, and then we ask LLMs\nto re-generate the queries from these sampled in-\nstances where the queries looked irrelevant. Our\nprompt for data sampling using LLMs is shown in\nTable 1(a). Below, we describe these approaches.\n(i) LLM-based Data Sampling without Filter-\ning: In this approach, we use the full Debatepedia\ndataset to find the irrelevant queries using LLMs.\nFor this purpose, we provide each instance of De-\nbatepedia to the LLMs to determine if the query is\n10246\n(a) Prompt: Data Sampling for Query Regeneration (b) Prompt: Regenerating the Sampled Queries\nBelow, we provide a query, a document, and the query-focused\nsummary of the given document. Identify whether the query is\nrelevant to the summary? Answer as either yes or no.\nQuery: [QUERY]\nDocument: [DOCUMENT]\nSummary: [SUMMARY]\nA document along with its summary are given below.\nWrite down the most reasonable query relevant to\nthis document-summary pair?\nDocument: [DOCUMENT]\nSummary: [SUMMARY]\nTable 1: Prompts for LLMs: (a) data sampling for query regeneration, and (b) regenerating the sampled queries.\nrelevant to the document/summary. However, we\nfind a significant difference between LLMs in this\ntask. While PaLM-2 only identifies 659 queries\nas irrelevant (612/19/28 in train/valid/test sets, re-\nspectively), ChatGPT identifies 6435 queries as\nirrelevant (5697/316/422 in train/valid/test sets, re-\nspectively), out of 13719 samples.\n(ii) LLM-based Data Sampling via Filtering:\nIn this approach, instead of cleaning the Debatepe-\ndia dataset by only fixing the queries, we also fol-\nlow some rules to first filter out some irrelevant in-\nstances from the dataset to address the existing lim-\nitations in Debatepedia (Laskar et al., 2022), such\nas smaller-sized documents, close-ended questions,\netc. Since for the smaller-sized documents, the ref-\nerence summaries are mainly the overall generic\nsummary of the document where the additional\nquery does not help, we aim to exclude smaller-\nsized documents to ensure that the reference sum-\nmaries are more query-focused. This also helps us\nto address the noisy scenario in the dataset when\nthe reference summary length is longer than the\ndocument length. Based on manual analysis, we\nfind that a minimum length of 75 words for each se-\nlected document at least ensures a document where\nthe query could play a role in the summary genera-\ntion. To also address the issue of short summaries\nthat looked like answers to closed-ended ques-\ntions,we exclude instances where the length of the\nsummary is shorter than 5 words. This helps us to\nclean the dataset in a way such that instead of hav-\ning a dataset with close-ended questions and short\nanswers, we propose a dataset consisting of concise\nbut coherent summaries. This results in a filtered\nversion of the dataset which is smaller in size, con-\nsisting of 5291/309/405 instances, in comparison to\nthe original dataset4 containing 12000/719/1000 in-\nstances, in train/valid/test sets, respectively. We\nalso find that ChatGPT and PaLM-2 identified\n2171/120/145 and 218/6/6 queries as irrelevant in\n4https://github.com/PrekshaNema25/\nDiverstiyBasedAttentionMechanism/tree/master\nthe training, validation, and test sets, respectively.\nBelow, we demonstrate how we utilize LLMs\nfor query regeneration.\n3.2 Using LLM for Query Regeneration\nWe concatenate the document and the reference\nsummary together and give as input to the LLMs\nfor query regeneration. Our sample prompt for this\ntask can be found in Table 1(b). While we could\nask LLMs to generate both the query and the query-\nbased summary by only giving the document in the\ninput prompt, we did not do so since it is found\nthat LLMs like ChatGPT tend to generate longer\nsummaries (Laskar et al., 2023a; Qin et al., 2023)\nwhile the resulting dataset could become a fully\nsynthetic dataset. Thus, we use both the document\nand the summary as input and only regenerate the\nqueries while keeping the original reference sum-\nmaries intact. We find that the regenerated queries\nusing ChatGPT and PaLM-2 only have 15.2% and\n11.4% word overlaps, respectively, with the gold\nsummaries, in comparison to the 10.6% word over-\nlaps in the original Debatepedia dataset.\n4 Experimental Results\nIn this section, we present our experimental find-\nings. We denote the version of our dataset where\nwe did not apply any filtering as the unfiltered\nversion, whereas we denote the version of our\ndataset where we also utilize filtering while sam-\npling data instances based on applying some rules\nas the filtered version. For ChatGPT, we use the\ngpt-3.5-turbo-03015 model; while for PaLM-\n2, we use the text-bison@0016 model. We fine-\ntune the following models to benchmark the per-\nformance in our re-annotated versions of Debatepe-\ndia since these models achieved impressive perfor-\nmance in query-focused abstractive summarization\n5https://platform.openai.com/docs/models/\ngpt-3-5\n6https://cloud.google.com/vertex-ai/docs/\ngenerative-ai/learn/models\n10247\nModel Training Evaluation ROUGE-1 ROUGE-2 ROUGE-L BERTScore\nBART-Base Original Debatepedia MS-MARCO 37.8 20.0 34.3 68.8\nPegasus-Base Original Debatepedia MS-MARCO 31.2 14.9 27.7 59.9\nT5-Base Original Debatepedia MS-MARCO 45.1 26.9 40.9 71.9\nBART-Base CQSumDP (filtered) MS-MARCO 42.3 24.2 38.2 70.5\nPegasus-Base CQSumDP (filtered) MS-MARCO 47.1 31.4 43.3 69.0\nT5-Base CQSumDP (filtered) MS-MARCO 47.6 29.4 43.0 73.1\nBART-Base PQSumDP (filtered) MS-MARCO 41.4 23.3 37.5 70.6\nPegasus-Base PQSumDP (filtered) MS-MARCO 44.7 28.2 40.7 68.7\nT5-Base PQSumDP (filtered) MS-MARCO 47.3 29.0 42.8 73.0\nBART-Base CQSumDP (unfiltered) MS-MARCO 38.4 20.6 34.8 68.4\nPegasus-Base CQSumDP (unfiltered) MS-MARCO 43.7 27.3 40.0 67.7\nT5-Base CQSumDP (unfiltered) MS-MARCO 44.7 26.5 40.4 71.9\nBART-Base PQSumDP (unfiltered) MS-MARCO 39.1 21.4 35.5 69.1\nPegasus-Base PQSumDP (unfiltered) MS-MARCO 40.1 23.1 36.3 66.2\nT5-Base PQSumDP (unfiltered) MS-MARCO 44.4 25.6 40.0 72.3\nTable 2: Performance of different models on MS-MARCO when trained on respective versions of Debatepedia (DP).\nModel Dataset ROUGE-1 ROUGE-2 ROUGE-L BERTScore\nBART-Base CQSumDP (filtered) 39.6 22.1 36.6 70.8\nPegasus-Base CQSumDP (filtered) 31.5 13.9 28.4 66.8\nT5-Base CQSumDP (filtered) 31.3 13.2 28.6 67.1\nBART-Base PQSumDP (filtered) 37.8 21.,2 35.6 70.3\nPegasus-Base PQSumDP (filtered) 27.1 10.8 24.9 64.9\nT5-Base PQSumDP (filtered) 28.0 10.6 25.4 65.6\nBART-Base CQSumDP (unfiltered) 41.6 23.4 39.1 72.3\nPegasus-Base CQSumDP (unfiltered) 33.4 15.8 30.6 68.3\nT5-Base CQSumDP (unfiltered) 33.9 15.1 31.0 68.6\nBART-Base PQSumDP (unfiltered) 39.8 22.2 37.2 71.7\nPegasus-Base PQSumDP (unfiltered) 29.2 12.5 26.6 66.3\nT5-Base PQSumDP (unfiltered) 30.2 12.6 27.6 66.9\nTable 3: Performance of different models on various versions of Debatepedia.\nin recent years (Laskar et al., 2022; Goodwin et al.,\n2020): (i) BART (Lewis et al., 2019), (ii) T5 (Raf-\nfel et al., 2019), and (iii) Pegasus (Zhang et al.,\n2019). Similar to prior work on query-focused text\nsummarization (Laskar et al., 2022), we concate-\nnate the query with the document and give as input\nto these models to generate the query-focused sum-\nmaries. For all models, we use their respective Base\nversions from HuggingFace (Wolf et al., 2019) (see\nAppendix A.3 for more details). For all models, the\nresults are evaluated using ROUGE-1, ROUGE-2,\nROUGE-L, and BERTScore. For BERTScore, we\nuse the DeBERTa-xlarge-mnli (He et al.) model.\n4.1 Effectiveness of LLMs for Data Cleaning\nIn this section, to investigate the effectiveness of\nusing LLMs for data cleaning, we evaluate the\nperformance of models trained on different LLM-\nannotated versions of the Debatepedia dataset in\nan out-of-domain dataset for the query-focused ab-\nstractive summarization task. This is done to ensure\nthat all models are evaluated on the same evaluation\nset. In this regard, we use the development set of\nthe QA-NLG version of the MS-MARCO (Wang\net al., 2018) dataset (12467 samples). We follow\nthe similar settings of Laskar et al. (2022) by only\nconsidering the gold passage as the source docu-\nment, and after combining the passage with the\nquery we give the concatenated text as input to the\nmodels. The results of all three models (BART, T5,\nPegasus) on MS-MARCO that are fine-tuned on\nthe respective versions of Debatepedia are shown\nin Table 2. We observe based on our experimental\nresults that the domain generalization performance\nis much better when the CQSumDP/PQSumDP ver-\nsions of the Debatepedia dataset are used in compar-\nison using the Original Debatepedia dataset. While\ncomparing ChatGPT and PaLM as data annotators,\nwe observe that models trained on CQSumDP per-\nform better than PQSumDP. Moreover, we find that\nmodels trained on the filtered version obtain bet-\nter performance (with T5-Base achieving the best\nresult), indicating the importance of cleaning the\nDebatepedia dataset by excluding noisy instances,\nalongside utilizing LLM-generated queries.\nQualitative Evaluation of Model Generated\nSummaries: We sample 10 summaries generated\nby each model (BART, T5, Pegasus) on the MS-\nMARCO dataset to conduct human evaluations for\nour best-performing approach, the CQSumDP (fil-\ntered version), and the baseline Original Debate-\npedia. In our human evaluation, we ask humans\n10248\nto score between 1 to 5 for the factual consistency\nand the coherence of the summaries generated by\ndifferent models for the given queries. The average\ncoherence and factual consistency scores for mod-\nels trained on CQSumDP (filtered) are 3.4 and 3.3,\nrespectively; in comparison to the average coher-\nence and factual consistency scores of 3 and 2.6,\nrespectively, for the Original Debatepedia. This\nfurther establishes the effectiveness of using LLMs\nas annotators to make a more suitable dataset for\nquery-focused text summarization.\nQualitative Evaluation of LLM Generated\nQueries: We sample 100 instances and ask three\nhuman evaluators to choose between the ChatGPT\nand PaLM-generated (see Appendix A.4 for some\nexamples) queries that they prefer based on the\nconciseness and the relevancy of the query. We\nfind that in 66% cases (via majority voting), the\nChatGPT-generated queries were preferred.\nLLM as Query Relevancy Classifier: To mea-\nsure the capability of LLMs in classifying whether\nthe query is relevant to the document/summary, we\nsample 100 instances and evaluate using three hu-\nman evaluators to find if they also agree with the\nclassification done by LLMs. We find based on\nmajority voting that the precision for the classifica-\ntion task for PaLM-2 is 75%, while for ChatGPT\nis 63%. This trend of PaLM-2 outperforming Chat-\nGPT in discriminative tasks (e.g., classification)\nwhile being underperformed in generative tasks is\nalso observed in recent studies (Jahan et al., 2023).\nAblation Studies: To further investigate the use-\nfulness of LLM-generated queries, we conduct the\nfollowing ablation tests using the best-performing\nmodel, T5-base, on MS-MARCO (see Table 4).\n(i) Remove LLM-generated Query: Here, we\nevaluate the performance of the T5 model by fine-\ntuning it on the filtered version of Debatepedia\nwithout incorporating any query relevance. We\nfind based on the average score across different\nmetrics that the performance for T5 is dropped by\n9.53% on average, in comparison to the T5 model\nfine-tuned on the CQSumDP (filtered) dataset.\n(ii) Replace LLM-generated Query: Here, we\nevaluate the performance by fine-tuning T5 using\nthe original query instead of the LLM-generated\nquery in the filtered version of Debatepedia. Based\non the average scores achieved by the T5 model,\nthe performance is dropped by 3.57% on average,\ncompared to T5 fine-tuned on CQSumDP (filtered).\nType ROUGE-1 ROUGE-2 ROUGE-L BERTScore\nChatGPT Generated Query 47.6 29.4 43.0 73.1\nWithout Query Relevance 42.2 23.7 38.0 70.8\nOriginal Query 45.7 29.2 41.6 69.7\nTable 4: Ablation test result for the T5-Base model fine-tuned\non Debatepedia (filtered) and evaluated on MS-MARCO.\nCost and Time Efficiency: Recently, it was\nfound that LLMs could significantly reduce the\nlabeling cost without sacrificing the model’s per-\nformance much, making it possible to train models\non larger datasets without the need for human la-\nbeling (Wang et al., 2021; Ding et al., 2022; Liu\net al., 2023). In this work, we observe that Chat-\nGPT/PaLM APIs could generate about 15 queries\non average per minute, which should be much faster\nthan using human annotators, since humans may\nneed some time to come up with the most effective\nquery for the given document-summary pairs. This\nmakes LLMs to be more effective for annotation.\n4.2 Performance Benchmarking on Different\nVersions of Debatepedia\nIn this section, we benchmark the performance of\nvarious LLM-annotated versions of the Debatepe-\ndia dataset. We present our results in Table 3 to\nfind that all three models perform better in the CQ-\nSumDP dataset in comparison to their performance\non the PQSumDP. This gives further indication that\nthe queries generated by ChatGPT are more help-\nful in improving the model performance. While\ncomparing between different models, we find that\nin both the filtered and the unfiltered versions, the\nbest performance is achieved by the BART model.\n5 Conclusions and Future Work\nIn this paper, we study how to effectively leverage\nLLMs to construct a cleaned version of the Debate-\npedia dataset to address the existing limitations in\nthis dataset in order to make it suitable for query-\nfocused text summarization. Based on extensive ex-\nperiments and evaluation, we demonstrate that our\nproposed data re-annotation approach using LLMs\n(especially ChatGPT) results in a cleaner version of\nDebatepedia that is found to be more effective for\nthe query-focused summarization task in compari-\nson to the original dataset. In the future, we will ex-\nplore whether few-shot examples with LLMs lead\nto better performance. Our re-annotated versions of\nDebatepedia will also be made publicly available\nhere: https://github.com/tahmedge/CQSUMDP.\n10249\n6 Limitations\nChatGPT (GPT-3.5) and PaLM models are contin-\nuously upgraded by OpenAI and Google. Thus, it\nmay not be possible to reproduce the same queries\nusing these models. However, this also mimics\nthe real-world scenario as different human annota-\ntors may write different queries (e.g., in many text\nsummarization datasets, there can be multiple gold\nreference summaries written by different human\nannotators). However, similar to the work of Guo\net al. (2023), we also notice that this difference is\nvery small. Therefore, we also generate only one\nquery for each example. Though a new version of\nChatGPT called GPT-47 has been recently released\nwhich may generate more powerful queries, in this\nwork, we did not utilize GPT-4 as it is quite expen-\nsive to use than the original ChatGPT (i.e., GPT-\n3.5) while being significantly slower. Nonetheless,\nfuture work may compare with other more power-\nful LLMs (including GPT-4) for data annotation.\n7 Ethics Statement\nSince this paper only utilizes LLMs to generate\nthe queries for the given document-summary pairs,\nit does not lead to any unwanted biases or ethical\nconcerns. However, all the responses generated\nby ChatGPT and PaLM are still manually checked\nby the authors to ensure that the LLM-generated\nqueries in the cleaned version of the dataset do not\npose any ethical concerns or unwanted biases. Only\na publicly available academic dataset was used that\ndid not require any licensing. Thus, no personally\nidentifiable information has been used while uti-\nlizing LLMs to fix the queries in the Debatepedia\ndataset. All the human evaluators were also paid\nabove the minimum wage.\nAcknowledgements\nWe would like to thank all the anonymous review-\ners and the area chairs for their excellent review\ncomments. This research is supported in part by the\nNatural Sciences and Engineering Research Coun-\ncil (NSERC) of Canada, the York Research Chairs\n(YRC) program, and the Generic Research Fund\nof York University. We also acknowledge Com-\npute Canada for providing us with the computing\nresources. Jimmy Huang (jhuang@yorku.ca) is the\ncontact author of this paper.\n7https://openai.com/research/gpt-4\nReferences\nDeen Mohammad Abdullah and Yllias Chali. 2020. To-\nwards generating query to perform query focused\nabstractive summarization using pre-trained model.\nIn Proceedings of the 13th International Conference\non Natural Language Generation, pages 80–85.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nTal Baumel, Matan Eyal, and Michael Elhadad. 2018.\nQuery focused abstractive summarization: Incorpo-\nrating query relevance, multi-document coverage,\nand summary length constraints into seq2seq models.\narXiv preprint arXiv:1801.07704.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. Proceedings of the Annual Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, 4171-4186.\nBosheng Ding, Chengwei Qin, Linlin Liu, Lidong Bing,\nShafiq Joty, and Boyang Li. 2022. Is gpt-3 a good\ndata annotator? arXiv preprint arXiv:2212.10450.\nGuy Feigenblat, Haggai Roitman, Odellia Boni, and\nDavid Konopnicki. 2017. Unsupervised query-\nfocused multi-document summarization using the\ncross entropy method. In Proceedings of the 40th\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval , pages\n961–964.\nFabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.\n2023. Chatgpt outperforms crowd-workers for text-\nannotation tasks. arXiv preprint arXiv:2303.15056.\nTravis Goodwin, Max Savery, and Dina Demner-\nFushman. 2020. Flight of the pegasus? compar-\ning transformers on few-shot and zero-shot multi-\ndocument abstractive summarization. In Proceedings\nof the 28th International Conference on Computa-\ntional Linguistics, pages 5640–5646.\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,\nJinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng\n10250\nWu. 2023. How close is chatgpt to human experts?\ncomparison corpus, evaluation, and detection. arXiv\npreprint arXiv:2301.07597.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. Deberta: Decoding-enhanced bert\nwith disentangled attention. In International Confer-\nence on Learning Representations.\nIsrat Jahan, Md Tahmid Rahman Laskar, Chun Peng,\nand Jimmy Huang. 2023. A comprehensive eval-\nuation of large language models on benchmark\nbiomedical text processing tasks. arXiv preprint\narXiv:2310.04270.\nJan Koco´n, Igor Cichecki, Oliwier Kaszyca, Mateusz\nKochanek, Dominika Szydło, Joanna Baran, Julita\nBielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil\nKanclerz, et al. 2023. Chatgpt: Jack of all trades,\nmaster of none. arXiv preprint arXiv:2302.10724.\nSayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha,\nand Eugene Ie. 2020. Aquamuse: Automatically\ngenerating datasets for query-based multi-document\nsummarization. arXiv preprint arXiv:2010.12694.\nMd Tahmid Rahman Laskar, M Saiful Bari, Mizanur\nRahman, Md Amran Hossen Bhuiyan, Shafiq Joty,\nand Jimmy Huang. 2023a. A systematic study and\ncomprehensive evaluation of ChatGPT on benchmark\ndatasets. In Findings of the Association for Com-\nputational Linguistics: ACL 2023 , pages 431–469,\nToronto, Canada. Association for Computational Lin-\nguistics.\nMd Tahmid Rahman Laskar, Enamul Hoque, and Jimmy\nHuang. 2020a. Query focused abstractive summa-\nrization via incorporating query relevance and trans-\nfer learning with transformer models. In Canadian\nConference on Artificial Intelligence, pages 342–348.\nSpringer.\nMd Tahmid Rahman Laskar, Enamul Hoque, and\nJimmy Xiangji Huang. 2022. Domain adaptation\nwith pre-trained transformers for query-focused ab-\nstractive text summarization. Computational Linguis-\ntics, 48(2):279–320.\nMd Tahmid Rahman Laskar, Enamul Hoque, and Xi-\nangji Huang. 2020b. WSL-DS: Weakly supervised\nlearning with distant supervision for query focused\nmulti-document abstractive summarization. In Pro-\nceedings of the 28th International Conference on\nComputational Linguistics, pages 5647–5654.\nMd Tahmid Rahman Laskar, Xiangji Huang, and Ena-\nmul Hoque. 2020c. Contextualized embeddings\nbased transformer encoder for sentence similarity\nmodeling in answer selection task. In Proceedings of\nthe 12th Language Resources and Evaluation Con-\nference, pages 5505–5514.\nMd Tahmid Rahman Laskar, Mizanur Rahman, Israt\nJahan, Enamul Hoque, and Jimmy Huang. 2023b.\nCqsumdp: A chatgpt-annotated resource for query-\nfocused abstractive summarization based on debate-\npedia. arXiv preprint arXiv:2305.06147.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. BART:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and compre-\nhension. arXiv preprint arXiv:1910.13461.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text summariza-\ntion branches out, pages 74–81.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. Gpteval:\nNlg evaluation using gpt-4 with better human align-\nment. arXiv preprint arXiv:2303.16634.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nCaglar Gulcehre, and Bing Xiang. 2016. Abstractive\ntext summarization using sequence-to-sequence rnns\nand beyond. In Proceedings of The 20th SIGNLL\nConference on Computational Natural Language\nLearning, pages 280–290.\nPreksha Nema, Mitesh M Khapra, Anirban Laha, and\nBalaraman Ravindran. 2017. Diversity driven atten-\ntion model for query-based abstractive summariza-\ntion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1063–1072.\nR OpenAI. 2023. Gpt-4 technical report. arXiv, pages\n2303–08774.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver? arXiv preprint arXiv:2302.06476.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nDan Su, Yan Xu, Tiezheng Yu, Farhad Bin Siddique,\nElham J Barezi, and Pascale Fung. 2020. Caire-\ncovid: a question answering and query-focused\nmulti-document summarization system for covid-19\nscholarly information management. arXiv preprint\narXiv:2005.03975.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\n10251\nJesse Vig, Alexander R Fabbri, Wojciech Kry ´sci´nski,\nChien-Sheng Wu, and Wenhao Liu. 2021. Explor-\ning neural models for query-focused summarization.\narXiv preprint arXiv:2112.07637.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021. Want to reduce\nlabeling cost? gpt-3 can help. arXiv preprint\narXiv:2108.13487.\nYizhong Wang, Kai Liu, Jing Liu, Wei He, Yajuan Lyu,\nHua Wu, Sujian Li, and Haifeng Wang. 2018. Multi-\npassage machine reading comprehension with cross-\npassage answer verification. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1918–1927.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Fun-\ntowicz, et al. 2019. HuggingFace’s Transformers:\nState-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nYumo Xu and Mirella Lapata. 2020. Coarse-to-fine\nquery focused multi-document summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3632–3645.\nYumo Xu and Mirella Lapata. 2021. Text sum-\nmarization with latent queries. arXiv preprint\narXiv:2106.00104.\nXianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and\nWei Cheng. 2023. Exploring the limits of chatgpt\nfor query or aspect-based text summarization. arXiv\npreprint arXiv:2302.08081.\nJin-Ge Yao, Xiaojun Wan, and Jianguo Xiao. 2017. Re-\ncent advances in document summarization. Knowl-\nedge and Information Systems, 53(2):297–336.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J Liu. 2019. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization. arXiv\npreprint arXiv:1912.08777.\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia\nMutuma, Rahul Jha, Ahmed Hassan, et al. 2021.\nQmsum: A new benchmark for query-based multi-\ndomain meeting summarization. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5905–5921.\nA Appendix\nA.1 Debatepedia Dataset Limitations\nBased on a randomly sampled 100 instances, it has\nbeen found in a recent study (Laskar et al., 2022,\n2023b) that:\n• 52% of the queries in this dataset have no\nrelevance to the documents or the summaries,\nas demonstrated in Table 5.\n• 70% of the queries are close-ended (i.e.,\nYes/No type) questions (see Example 4 in Ta-\nble 5).\n• Though, many queries in this dataset are rel-\nevant to the documents but the summaries\nare more of generic due to shorter document\nlength. The average length of the document\nin this dataset is only 66.4 words.\nIn addition, many instances in this dataset only\ncontain one word summary (see Example 2 in Table\n5) for a given query that appears both in the train-\ning and evaluation sets, which may also help the\nmodel to memorize such words for similar queries\nduring the training phase. These issues may lead to\nan unexpected increase in the ROUGE score when\nthe model starts learning to reproduce those words\nin the summary during the evaluation phase. Fur-\nthermore, we also find some instances where the\nlength of the summary is longer than the document\nlength, which usually happens in short documents\n(see Example 3 in Table 5).\nA.2 Example Prompt for Query Generation\nOne example prompt to re-generate the query using\nLLMs is shown in Figure 1.\nA.3 Experimental Details\nA.3.1 Models\nTo evaluate the effectiveness of our ChatGPT anno-\ntated CQSumDP and PaLM annotated PQSumDP\ndatasets, we fine-tune some state-of-the-art pre-\ntrained sequence to sequence models (Lewis et al.,\n2019; Raffel et al., 2019; Zhang et al., 2019; Good-\nwin et al., 2020). For this purpose, we concatenate\nthe query with the document and give as input to\nthese models to generate the query-focused abstrac-\ntive summaries as this approach has shown impres-\nsive performance in the query-focused abstractive\nsummarization task recently (Laskar et al., 2022).\nWe describe these models below:\n10252\nExample 1:Query having no relevance with the document and the summary.\nQuery: Does an MBA enhance leadership skills?\nDocument: Business schools might improve your quantitative presentation and communication skills. It might but get\nyou thinking about ethical and strategy. But two years of case studies aren’t go to turn you into a leader if you weren’t\ndied one. There’s no learning charisma persuasiveness elegance or gut instinct.\nReference Summary: PhD will not improve cm factors of leaders.\nExample 2:One word summary having no relevance with the query or document.\nQuery: Education : do child benefit from watching tv?\nDocument: by watching news child can learn about geography politics advances in science – everything simply and\nlater explained . furthermore child learn about real-life situation that happens on everyday basis which will benefit them\nin the future.\nReference Summary: News.\nExample 3:The length of the summary is longer than the document with the query being irrelevant.\nQuery: activists : where do the keys activists and organizations stand ?\nDocument: see an analyses of the article ...\nReference Summary: philip martin of berkeley davis and michael teitelbaum the mirage of mexican guest workers\nnov/dec # foreign affairs .\nExample 4:More of a close-ended question.\nQuery: friendships : does twitter harms relationships ?\nDocument: twitter helps those stay in touches no matter how far they may be from each other .\nReference Summary: long-distance friendships .\nTable 5: Some examples demonstrating the limitations in the Debatepedia dataset.\nBART (Bidirectional and Auto-Regressive\nTransformer): BART (Lewis et al., 2019) is\na pre-trained sequence-to-sequence model based\non the encoder-decoder architecture that was pre-\ntrained on a large amount of diverse text data using\nthe denoising auto-encoding technique to recover\nthe original form of a corrupted document. The\npre-training involved various objectives such as ro-\ntating the document, permuting sentences, infilling\ntext, masking tokens, and deleting tokens. We use\nthe pre-trained BART model since fine-tuning this\nmodel was found to be very effective in abstractive\nsummarization (Laskar et al., 2022).\nT5 (Text-to-Text Transfer Transformer): The\nT5 model (Raffel et al., 2019) is a transformer-\nbased model based on the BERT architecture. How-\never, unlike traditional BERT models that classify\ninput text into a specific category, the T5 model\ntreats all tasks such as text classification, question\nanswering, neural machine translation, and text\nsummarization as a sequence-to-sequence problem\nusing various pre-training objectives. After pre-\ntraining, the model is fine-tuned on many down-\nstream tasks, achieving impressive performance\nacross various datasets including summarization.\nPegasus (Pre-training with Extracted Gap-\nsentences for Abstractive Summarization): Pe-\ngasus (Zhang et al., 2019) is a transformer-based\npre-trained encoder-decoder model for abstractive\nsummarization. Its pre-training objective involves\ngenerating summary like text from an input docu-\nment. To achieve this, the PEGASUS model first\nselects and masks some sentences from the input\ndocument(s). It then concatenates these selected\nsentences to create a pseudo-summary. The model\nuses different approaches to select these sentences,\nsuch as randomly selecting a certain number of\nsentences, selecting the first few sentences, or com-\nputing the ROUGE-1 score between each sentence\nand the rest of the document to choose the top-\nscoring sentences. This pseudo-summary is then\nused for self-supervised learning. By pre-training\non large datasets using this approach, the model\nachieves impressive fine-tuning performance on\ndownstream summarization datasets.\nA.3.2 Implementation\nWe use the HuggingFace8 (Wolf et al., 2019)library\nto implement the baseline models for performance\nevaluation. Similar to the prior work, we concate-\n8https://huggingface.co/\n10253\nFigure 1: Example Input to LLMs for Query Generation.\nnated the query with the document to give as input\nto the pre-trained baselines (i.e., BART, Pegasus,\nT5). The pre-trained model is then fine-tuned using\n4 NVIDIA V100 GPUs. The training batch size\nfor BART was set to 16, while it was set to 4 for\nPegasus and T5. The other hyperparameters were\nsimilar for all models, with the learning rate being\nset to 2e − 3 and the maximum input (i.e., the con-\ncatenated query and document) sequence length\nbeing 150 tokens. The minimum and the maxi-\nmum target (i.e., the generated summary) sequence\nlengths were 5 and 25, respectively. A total of 10\nepochs were run to fine-tune the pre-trained summa-\nrization models. We computed the ROUGE (Lin,\n2004) scores in terms of ROUGE-1, ROUGE-2,\nand ROUGE-L using the Evaluate9 library to com-\npare the performance of different models on the\nrespective test set. As noted earlier, for ChatGPT,\nwe use the gpt-3.5-turbo-0301 model; while for\nPaLM, we use the text-bison@001model.\nA.4 Qualitative Analysis of the Annotated\nData\nIn this section, we do some qualitative analyses\nbetween the queries in the Original Debatepedia\ndataset as well as the queries generated using LLMs\nin our proposed CQSumDP and PQSumDP ver-\nsions of the Debatepedia dataset. For our analysis,\nwe collect a set of 3 samples from this dataset and\npresent them in Table 6. While comparing between\nthe queries in the first example in the table, we\nfind that the original query is just one word length\nand very ambiguous, while the ChatGPT generated\nquery is more descriptive and more relevant to both\nthe document and the summary. For the second ex-\n9https://huggingface.co/spaces/\nevaluate-metric/rouge\nample, we find that even though the original query\nis descriptive, it does not have any relevance to the\ngenerated summary. Whereas both the ChatGPT\nand PaLM generated queries are very relevant to\nboth the document and the summary (in this ex-\nample, PaLM generated query is more descriptive).\nFor the third example, we find that the original\nquery is related to “entrepreneurs”. However, the\ndocument is about “product managers”, not “en-\ntrepreneurs”. Meanwhile, the ChatGPT and PaLM\ngenerated queries are also very relevant to the doc-\nument and both LLM-generated queries are the\nsame. This analysis further demonstrates the rele-\nvance of our LLM-generated query in comparison\nto the original query in Debatepedia.\n10254\n# Original\nQuery\nChatGPT\nQuery\nPaLM\nQuery\nSource Document Gold\nSummary\n1. military : What actions\ndid the gov-\nernment take\nto improve the\nsituation for\nU.S. troops and\nveterans?\nWhat did obama\ndo for veterans\n?\nprovided better body armor to our troops\n. provided the department of veterans af-\nfairs ( va ) with more than $ # . # billion\nto improve services to america s veter-\nans . ended media blackout on war ca-\nsualties and the return of fallen soldiers\nto dover afb . announced creation of a\njoint virtual lifetime electronic record\nfor members of the u.s. armed forces to\nimprove quality of medical care . ended\nthe previous stop-loss policy that kept\nsoldiers in iraq/afghanistan longer than\ntheir enlistment date . signed the veter-\nans health care budget reform and trans-\nparency act authorizing advance appro-\npriations for the department of veterans\naffairs by providing two-fiscal year bud-\nget authority thus enabling better med-\nical care for veterans . endorsed by\nthe american legion american veterans\nblinded veter ... ans association\nimproved ser-\nvices benefits\nand respect for\ntroops .\n2. we economy\n: has wto\nbenefited the\neconomy of the\nunited states ?\nHas NAFTA\ncaused job\nlosses in the\nU.S?\nWhat does\nthe author say\nabout the effect\nof NAFTA\non American\nwages?\n“ nafta and job losses ” . cyril morong (\nPhD ) the wall street journal may # # - “\ndid nafta cause the u.s. to lose so many\njobs [ citing figures provided in the range\nof # million and # # ] especially high-\npaying manufacture jobs ? probably not\n. i say probably since causality in any\nsocial science ( economics included ) is\ndifficult to prove since so many factors\nchange so quickly in the real world . but\nif many high-paying manufacture jobs\nwere lost it took many years until after\nnafta went into effect before they were ...\nbut what about manufacture jobs ? we\nhad just about # million in # . it actually\nrose to # . # million in # and was at # .\n# in #.\nnafta has de-\ncreased the\nnumber of\namerican job\n3. entrepreneurs:\ndoes an mba\nhelp en-\ntrepreneurs\n?\nIs an MBA nec-\nessary for prod-\nuct managers?\nIs an MBA nec-\nessary for prod-\nuct managers?\nchristopher cummings . “ is an mba nec-\nessary for product managers ? ” product\nmanagement meet pop culture . febru-\nary # # : “ hindsight . looking back\nthe brass tacks of my mba experience\nwere about the basics of management\neconomics and business strategy . could\nthat have been picked up on the job ?\nmaybe . [ ... ] however the more impor-\ntant throughline of the experience relates\nto critical thinking perspective and learn-\ning when to lead and when to follow . [\n... ] on the job especially as a young\npm it can be easy to lose perspective to\nmiss the forest for the trees . at the time\ni was definitely into the plate-spinning\nthe go-go-go the tactics and day-to-day .\nno time to think ; just keep moving . [ ...\n] the mba experience\nmba teach\nstrategy plan\nnot just tactics\nTable 6: Comparisons between the original queries and the LLM-generated queries in some samples of the Debatepedia dataset.\nNote that the personally identifiable information in this dataset is anonymized with the # token.\n10255",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9234280586242676
    },
    {
      "name": "Computer science",
      "score": 0.8693544268608093
    },
    {
      "name": "Information retrieval",
      "score": 0.6843375563621521
    },
    {
      "name": "Relevance (law)",
      "score": 0.6483869552612305
    },
    {
      "name": "Query expansion",
      "score": 0.5392670631408691
    },
    {
      "name": "RDF query language",
      "score": 0.5307300686836243
    },
    {
      "name": "Web search query",
      "score": 0.5157166123390198
    },
    {
      "name": "Task (project management)",
      "score": 0.4924488961696625
    },
    {
      "name": "Query language",
      "score": 0.4813045561313629
    },
    {
      "name": "Multi-document summarization",
      "score": 0.43684375286102295
    },
    {
      "name": "Sampling (signal processing)",
      "score": 0.4364606738090515
    },
    {
      "name": "Natural language processing",
      "score": 0.42481064796447754
    },
    {
      "name": "Web query classification",
      "score": 0.41321954131126404
    },
    {
      "name": "Language model",
      "score": 0.4108012914657593
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3626568019390106
    },
    {
      "name": "Search engine",
      "score": 0.16567638516426086
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I192455969",
      "name": "York University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I125133608",
      "name": "Royal Bank of Canada",
      "country": "CA"
    }
  ],
  "cited_by": 3
}