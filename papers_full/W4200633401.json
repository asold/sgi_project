{
  "title": "TransZero: Attribute-Guided Transformer for Zero-Shot Learning",
  "url": "https://openalex.org/W4200633401",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2123122862",
      "name": "Shiming Chen",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2771499845",
      "name": "Ziming Hong",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1983143503",
      "name": "Yang Liu",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4220546467",
      "name": "Guo-Sen Xie",
      "affiliations": [
        "Inception Institute of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2804925800",
      "name": "Baigui Sun",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2097763185",
      "name": "Hao Li",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2307589976",
      "name": "Qinmu Peng",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2132266551",
      "name": "Ke Lu",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2113952954",
      "name": "Xinge You",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1983143503",
      "name": "Yang Liu",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A4220546467",
      "name": "Guo-Sen Xie",
      "affiliations": [
        "Inception Institute of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2804925800",
      "name": "Baigui Sun",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2097763185",
      "name": "Hao Li",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2132266551",
      "name": "Ke Lu",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Peng Cheng Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2772366472",
    "https://openalex.org/W3037198159",
    "https://openalex.org/W3002728158",
    "https://openalex.org/W2772217839",
    "https://openalex.org/W3186453003",
    "https://openalex.org/W3204333469",
    "https://openalex.org/W3088026569",
    "https://openalex.org/W6789548710",
    "https://openalex.org/W6771429369",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6780924892",
    "https://openalex.org/W3142382410",
    "https://openalex.org/W2950626540",
    "https://openalex.org/W6766818547",
    "https://openalex.org/W3035655772",
    "https://openalex.org/W3102601130",
    "https://openalex.org/W2968074593",
    "https://openalex.org/W3014124957",
    "https://openalex.org/W2134270519",
    "https://openalex.org/W2128532956",
    "https://openalex.org/W6601146891",
    "https://openalex.org/W2948678175",
    "https://openalex.org/W2791906491",
    "https://openalex.org/W2991813857",
    "https://openalex.org/W6790753860",
    "https://openalex.org/W3013483068",
    "https://openalex.org/W6752166675",
    "https://openalex.org/W6682222085",
    "https://openalex.org/W6775772702",
    "https://openalex.org/W2070148066",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W2398118205",
    "https://openalex.org/W2902002028",
    "https://openalex.org/W6781201046",
    "https://openalex.org/W2794925779",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6600609147",
    "https://openalex.org/W6746524570",
    "https://openalex.org/W6734394377",
    "https://openalex.org/W2924476266",
    "https://openalex.org/W2982407353",
    "https://openalex.org/W6784486608",
    "https://openalex.org/W6781630272",
    "https://openalex.org/W2962677366",
    "https://openalex.org/W3014242421",
    "https://openalex.org/W3134712266",
    "https://openalex.org/W3167939936",
    "https://openalex.org/W2970732459",
    "https://openalex.org/W2963545832",
    "https://openalex.org/W3203055845",
    "https://openalex.org/W2963955422",
    "https://openalex.org/W27961112",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W3034359780",
    "https://openalex.org/W4226179686",
    "https://openalex.org/W3096741441",
    "https://openalex.org/W2963283377",
    "https://openalex.org/W3034730995",
    "https://openalex.org/W2150295085",
    "https://openalex.org/W3143107425",
    "https://openalex.org/W4288329833",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963960318",
    "https://openalex.org/W3124953418",
    "https://openalex.org/W3171926364",
    "https://openalex.org/W3182605419",
    "https://openalex.org/W3034379915",
    "https://openalex.org/W3097309192",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2963846885",
    "https://openalex.org/W3098404559",
    "https://openalex.org/W2989779801",
    "https://openalex.org/W3176716813",
    "https://openalex.org/W2990947836",
    "https://openalex.org/W3034655362",
    "https://openalex.org/W2962716320",
    "https://openalex.org/W2596142952",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3035284526",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3043840704"
  ],
  "abstract": "Zero-shot learning (ZSL) aims to recognize novel classes by transferring semantic knowledge from seen classes to unseen ones. Semantic knowledge is learned from attribute descriptions shared between different classes, which are strong prior for localization of object attribute for representing discriminative region features enabling significant visual-semantic interaction. Although few attention-based models have attempted to learn such region features in a single image, the transferability and discriminative attribute localization of visual features are typically neglected. In this paper, we propose an attribute-guided Transformer network to learn the attribute localization for discriminative visual-semantic embedding representations in ZSL, termed TransZero. Specifically, TransZero takes a feature augmentation encoder to alleviate the cross-dataset bias between ImageNet and ZSL benchmarks and improve the transferability of visual features by reducing the entangled relative geometry relationships among region features. To learn locality-augmented visual features, TransZero employs a visual-semantic decoder to localize the most relevant image regions to each attributes from a given image under the guidance of attribute semantic information. Then, the locality-augmented visual features and semantic vectors are used for conducting effective visual-semantic interaction in a visual-semantic embedding network. Extensive experiments show that TransZero achieves a new state-of-the-art on three ZSL benchmarks. The codes are available at: https://github.com/shiming-chen/TransZero.",
  "full_text": "TransZero: Attribute-Guided Transformer for Zero-Shot Learning\nShiming Chen1*, Ziming Hong1*, Yang Liu2, Guo-Sen Xie3, Baigui Sun2,\nHao Li2, Qinmu Peng1, Ke Lu4,5, Xinge You1†\n1Huazhong University of Science and Technology 2Alibaba Group, Hangzhou, China\n3Inception Institute of Artiﬁcial Intelligence 4 University of Chinese Academy of Sciences 5 Peng Cheng Laboratory, China\n{shimingchen, pengqinmu, youxg}@hust.edu.cn {hoongzm, gsxiehm}@gmail.com luk@ucas.ac.cn\nAbstract\nZero-shot learning (ZSL) aims to recognize novel classes by\ntransferring semantic knowledge from seen classes to unseen\nones. Semantic knowledge is learned from attribute descrip-\ntions shared between different classes, which act as strong\npriors for localizing object attributes that represent discrim-\ninative region features, enabling signiﬁcant visual-semantic\ninteraction. Although some attention-based models have at-\ntempted to learn such region features in a single image, the\ntransferability and discriminative attribute localization of vi-\nsual features are typically neglected. In this paper, we pro-\npose an attribute-guided Transformer network, termed Tran-\nsZero, to reﬁne visual features and learn attribute localization\nfor discriminative visual embedding representations in ZSL.\nSpeciﬁcally, TransZero takes a feature augmentation encoder\nto alleviate the cross-dataset bias between ImageNet and ZSL\nbenchmarks, and improves the transferability of visual fea-\ntures by reducing the entangled relative geometry relation-\nships among region features. To learn locality-augmented vi-\nsual features, TransZero employs a visual-semantic decoder\nto localize the image regions most relevant to each attribute in\na given image, under the guidance of semantic attribute infor-\nmation. Then, the locality-augmented visual features and se-\nmantic vectors are used to conduct effective visual-semantic\ninteraction in a visual-semantic embedding network. Exten-\nsive experiments show that TransZero achieves the new state\nof the art on three ZSL benchmarks. The codes are available\nat: https://github.com/shiming-chen/TransZero.\nIntroduction\nInspired by human cognitive competence, zero-shot learning\n(ZSL) was proposed to recognize new classes during learn-\ning by exploiting the intrinsic semantic relatedness between\nseen and unseen classes (Larochelle, Erhan, and Bengio\n2008; Palatucci et al. 2009; Lampert, Nickisch, and Harmel-\ning 2009). In ZSL, there are no training samples available\nfor unseen classes in the test set, and the label spaces for the\ntraining set and test set are disjoint from each other. Thus,\nthe key task for ZSL is to learn discriminative visual fea-\ntures for conducting effective visual-semantic interactions\n*These authors contributed equally.\n†Corresponding author\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nExisting Attention-based Method(a)\nCNN \nBackbone\nAttention \nMechanism\nVisual-Semantic\nInteraction\nCNN \nBackbone\nAttribute-Guided \nT ransformer\nVisual-Semantic\nInteraction\nOur TransZero(b)\nFigure 1: Motivation illustration. (a) Existing attention-\nbased ZSL methods simply learn region embeddings (e.g.,\nthe whole bird body), neglecting the transferability and dis-\ncriminative attribute localization (e.g., the distinctive bird\nbody parts) of visual features; (b) Our TransZero reduces\nthe entangled relationships among region features to im-\nprove their transferability and localizes the object attributes\nto represent discriminative region features, enabling signiﬁ-\ncant visual-semantic interaction.\nbased on the semantic information (e.g., sentence embed-\ndings (Reed et al. 2016), and attribute vectors (Lampert,\nNickisch, and Harmeling 2014)), which are shared between\nthe seen and unseen classes employed to support the knowl-\nedge transfer. According to their classiﬁcation range, ZSL\nmethods can be categorized into conventional ZSL (CZSL),\nwhich aims to predict unseen classes, and generalized ZSL\n(GZSL), which can predict both seen and unseen classes\n(Xian, Schiele, and Akata 2017).\nTo enable visual-semantic interactions, early ZSL meth-\nods attempt to build an embedding between seen classes and\ntheir class semantic vectors, and then classify unseen classes\nby nearest neighbor search in the embedding space. Since\nthe embedding is only learned by seen class samples, these\nembedding-based methods usually overﬁt to seen classes un-\nder the GZSL setting (known as the bias problem). To tackle\nthis problem, many generative ZSL methods have been pro-\nposed to generate samples of unseen classes by leveraging\ngenerative models (e.g., variational autoencoders (V AEs)\n(Arora et al. 2018; Sch¨onfeld et al. 2019; Chen et al. 2021b),\ngenerative adversarial nets (GANs) (Xian et al. 2018, 2019;\nChen et al. 2021a), and generative ﬂows (Shen, Qin, and\nHuang 2020)) for data augmentation. Thus the ZSL task is\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n330\nconverted into a supervised classiﬁcation problem.\nAlthough these methods have achieved progressive im-\nprovement, they rely on global visual features which are\ninsufﬁcient for representing the ﬁne-grained information of\nclasses (e.g., red-legged of Kittiwake), since the discrimina-\ntive information is contained in a few regions corresponding\nto a few attributes. Thus, the visual feature representations\nare limited, resulting in poor visual-semantic interactions.\nMore recently, some attention-based models (Xie et al. 2019,\n2020; Zhu et al. 2019; Xu et al. 2020; Yu et al. 2018; Liu\net al. 2019) have attempted to learn more discriminative re-\ngion features with the guidance of the semantic information,\nas shown in Fig. 1(a). However, these methods are limited\nin: i) They directly take the entangled region (grid) features\nfor ZSL classiﬁcation, which hinders the transferability of\nvisual features from seen to unseen classes; ii) They sim-\nply learn region embeddings (e.g., the whole bird body),\nneglecting the importance of discriminative attribute local-\nization (e.g., the distinctive bird body parts). Thus, properly\nimproving the transferability and localizing the object at-\ntributes for enabling signiﬁcant visual-semantic interaction\nin ZSL has become very necessary.\nTo tackle the above challenges, in this paper, we propose\nan attribute-guided Transformer, termed TransZero, which\nreduces the entangled relationships among region features\nto improve their transferability and localizes the object at-\ntributes to represent discriminative region features in ZSL,\nas shown in Fig. 1(b). Speciﬁcally, TransZero consists of\nan attributed-guided Transformer (AGT) that learns locality-\naugmented visual features and a visual-semantic embedding\nnetwork (VSEN) that conducts visual-semantic interactions.\nIn AGT, we ﬁrst take a feature augmentation encoder to i)\nalleviate the cross-dataset bias between ImageNet and ZSL\nbenchmarks, and ii) reduce the entangled relative geome-\ntry relationships between different regions for improving the\ntransferability from seen to unseen classes. They are ignored\nby existing ZSL methods. To learn locality-augmented vi-\nsual features, we employ a visual-semantic decoder in AGT\nto localize the image regions most relevant to each attribute\nin a given image, under the guidance of semantic attribute in-\nformation. Then, the locality-augmented visual features and\nsemantic vectors are used to enable visual-semantic interac-\ntion in VSEN. Extensive experiments show that TransZero\nachieves the new state of the art on three ZSL benchmarks.\nThe qualitative results also demonstrate that TransZero re-\nﬁnes visual features and provides attribute-level localization.\nThe main contributions of this paper are summarized as:i)\nWe introduce a novel ZSL method, termed TransZero, which\nemploys an attribute-guided Transformer to reﬁne the visual\nfeatures and learn the attribute localization for discrimina-\ntive visual embedding representations. To the best of our\nknowledge, TransZero is the ﬁrst work extending the Trans-\nformer to the ZSL task. ii) We propose a feature augmen-\ntation encoder to i) alleviate the cross-dataset bias between\nImageNet and ZSL benchmarks, and ii) reduce the entangled\nrelative geometry relationships between different regions\nto improve the transferability from seen to unseen classes.\nThey are ignored by existing ZSL methods. iii) Extensive\nexperiments demonstrate that TransZero achieves the new\nstate of the art on three ZSL benchmarks. We further qual-\nitatively show that our TransZero reﬁnes the visual features\nand accurately localizes ﬁne-grained parts for discriminative\nfeature representations.\nRelated Work\nZero-Shot Learning. Early ZSL methods (Song et al.\n2018; Li et al. 2018; Xian et al. 2018, 2019; Yu et al. 2020;\nMin et al. 2020; Han et al. 2021; Chen et al. 2021a; Chou,\nLin, and Liu 2021; Han et al. 2021) focus on learning a\nmapping between the visual and semantic domains to trans-\nfer semantic knowledge from seen to unseen classes. They\ntypically extract global visual features from pre-trained or\nend-to-end trainable networks. Typically, end-to-end mod-\nels achieve better performance than pre-trained ones because\nthey ﬁne-tune the visual features, thus alleviating the cross-\ndataset bias between ImageNet and ZSL benchmarks (Chen\net al. 2021a; Xian et al. 2019). However, these methods still\nusually yield relatively undesirable results, since they can-\nnot efﬁciently capture the subtle differences between seen\nand unseen classes. More relevant to this work are the re-\ncent attention-based ZSL methods (Xie et al. 2019, 2020;\nZhu et al. 2019; Xu et al. 2020; Liu et al. 2021) that uti-\nlize attribute descriptions as guidance to discover the more\ndiscriminative region (or part) features. Unfortunately, They\nsimply learn region embeddings (e.g., the whole bird body)\nneglecting the importance of discriminative attribute local-\nization (e.g., the distinctive bird body parts). Furthermore,\nthe end-to-end attention models are also time-consuming\nwhen it comes to ﬁne-tuning the CNN backbone. In contrast,\nwe propose an attribute-guided Transformer to learn the at-\ntribute localization for discriminative region feature repre-\nsentations under non end-to-end ZSL model.\nTransformer Model. Transformer models (Vaswani et al.\n2017) have recently demonstrated excellent performance on\na broad range of language and computer vision tasks, e.g.,\nmachine translation (Ott et al. 2018), image recognition\n(Dosovitskiy et al. 2021), video understanding (Gabeur et al.\n2020), visual question answering (Zhang et al. 2021), etc.\nThe success of Transformers can be mainly attributed to self-\nsupervision and self-attention (Khan et al. 2021). The self-\nsupervision allows complex models to be trained without the\nhigh cost of manual annotation, which in turn enables gener-\nalizable representations that encode useful relationships be-\ntween the entities presented in a given dataset to be learned.\nThe self-attention layers take the broad context of a given\nsequence into account by learning the relationships between\nthe elements in the token set (e.g., words in language or\npatches in an image). Some methods (Gabeur et al. 2020;\nCornia et al. 2020; Huang et al. 2019; Pan et al. 2020) have\nalso shown that the transformer architecture can better cap-\nture the relationship between visual features and process se-\nquences in parallel during training. Motivated by these, we\ndesign an attribute-guided Transformer that reduces the re-\nlationships among different regions to improve the transfer-\nability of visual features and learns the attribute localization\nfor representing discriminative region features.\n331\nCNN Backbone\nAttribute-Guided Transformer (AGT)\nVisual-Semantic Decoder\nRed\nlong\nSmall\n…\nVisual-Semantic Embedding Network (VSEN)\n…\nEncoder\nLayer 1\nEncoder\nLayer 2\n…\n Encoder\nLayer L\nDecoder\nLayer 1\nDecoder\nLayer 2\n…\n Decoder\nLayer L\nFeature Augmentation Encoder\n一\nVisual Features\nRegion Geometry Features\n0.8\n0.6\n0.9 …\n…\n…\nLanguage \nModel\nFigure 2: The architecture of the proposed TransZero model. TransZero consists of an attribute-guided Transformer (AGT) and\na visual-semantic embedding network (VSEN). AGT includes a feature augmentation encoder that alleviates the cross-dataset\nbias between ImageNet and ZSL benchmarks and reduces the entangled geometry relationships between different regions for\nimproving the transferability from seen to unseen classes, and a visual-semantic decoder that learns locality-augmented visual\nfeatures based on the semantic attribute information. VSEN is used to enable signiﬁcant visual-semantic interaction.\nProposed Method\nWe ﬁrst introduce some notations and the problem deﬁni-\ntion. Assume that we have Ds = {(xs\ni ;ys\ni )}as training data\nwith Cs seen classes, where xs\ni ∈X denotes the image i,\nand ys\ni ∈Ys is the corresponding class label. Another set of\nunseen classes Cu has unlabeled samples Du = {(xu\ni ;yu\ni )},\nwhere xu\ni ∈X are the unseen class images, and yu\ni ∈Yu\nare the corresponding labels. A set of class semantic vec-\ntors of the class c ∈ Cs ∪Cu = C with A attributes\nzc = [zc\n1;:::;z c\nA]⊤ = \u001e(y) helps knowledge transfer from\nseen to unseen classes. Note that we also use the semantic\nattribute vectors of each attribute VA = {va}A\na=1 learned\nby a language model (i.e., GloVe (Pennington, Socher, and\nManning 2014)) according to each word in attribute names.\nZSL aims to predict the class labels yu ∈Yu and y ∈Y =\nYs∪Yu in the CZSL and GZSL settings, respectively, where\nYs ∩Yu = ∅.\nIn this paper, we propose an attribute-guided Transformer\nnetwork (termed TransZero) to reﬁne the visual features and\nlocalize the object attributes for representing the discrimi-\nnative region features under a non end-to-end model. This\nenables signiﬁcant visual-semantic interaction in ZSL. As\nillustrated in Fig. 2, our TransZero includes an attribute-\nguided Transformer (AGT) and visual-semantic embedding\nnetwork (VSEN). AGT reﬁnes the visual feature using a fea-\nture augmentation encoder, and learns locality-augmented\nvisual features using a visual-semantic decoder. VSEN en-\nables visual-semantic interaction for ZSL classiﬁcation.\nAttribute-Guided Transformer\nFeature Augmentation Encoder. Since there is a cross-\ndataset bias between ImageNet and ZSL benchmarks (Chen\net al. 2021a), we introduce a feature augmentation encoder\nto reﬁne the visual features of ZSL benchmarks. Addition-\nally, previous ZSL methods typically ﬂatten the grid fea-\ntures (extracted from a CNN backbone) into a feature vector,\nwhich is further used for generative models or embedding\nlearning. However, such a feature vector implicitly entangles\nthe feature representations among various regions in an im-\nage, which hinders their transferability from one domain to\nother domains (e.g., from seen to unseen classes) (Xu et al.\n2020; Atzmon et al. 2020; Chen et al. 2021c). As such, we\npropose a feature-augmented scaled dot-product attention to\nfurther enhance the encoder layer by reducing the relative\ngeometry relationships among the grid features.\nTo learn relative geometry features (Herdade et al. 2019;\nZhang et al. 2021), we ﬁrst calculate the relative center coor-\ndinates (vcen\ni , tcen\ni ) based on the pair of 2D relative positions\nof the i-th grid\n{(\nvmin\ni ;tmin\ni\n)\n;(vmax\ni ;tmax\ni )\n}\n:\n(vcen\ni ;tcen\ni ) =\n(vmin\ni + vmax\ni\n2 ;tmin\ni + tmax\ni\n2\n)\n; (1)\nwi =\n(\nvmax\ni −vmin\ni\n)\n+ 1; (2)\nhi =\n(\ntmax\ni −tmin\ni\n)\n+ 1; (3)\nwhere (vmin\ni ;tmin\ni ) and (vmax\ni ;tmax\ni ) are the relative position\ncoordinates of the top left corner and bottom right corner of\nthe grid i, respectively.\nThen, we construct region geometry featuresGij between\ngrid iand grid j:\nGij = ReLU\n(\nwT\ng gij\n)\n; (4)\nwhere\ngij = FC (rij) ; r ij =\n\n\nlog\n(\n|vcen\ni −vcen\nj |\nwi\n)\nlog\n(\n|tcen\ni −tcen\nj |\nhi\n)\n\n; (5)\nwhere rij is the relative geometry relationship between grids\niand j, FC is a fully connected layer followed by a ReLU\nactivation, and wT\ng is a set of learnable weight parameters.\n332\nFinally, we substract the region geometry features from\nthe visual features in the feature-augmented scaled dot-\nproduct attention to provide a more accurate attention map,\nformally deﬁned as:\nQe = UWe\nq ;Ke = UWe\nk ;V e = UWe\nv ; (6)\nZaug = softmax\n(\nQeKe>\n√\nde −G\n)\nVe; (7)\nU ← U + Zaug; (8)\nwhere Q, K, V are the query, key and value matrices,\nWe\nq , We\nk , We\nv are the learnable matrices of weights, de is\na scaling factor, and Zaug is the augmented features. U ∈\nRHW ×C are the packed visual features, which are learned\nfrom the ﬂattened features embedded by a fully connected\nlayer followed by a ReLU and a Dropout layer.\nVisual-Semantic Decoder. Following the standard Trans-\nformer (Vaswani et al. 2017), our visual-semantic decoder\ntakes a multi-head self-attention layer and feed-forward net-\nwork (FFN) to build the decoder layer. The decoding pro-\ncess continuously incorporates visual information under the\nguidance of semantic attribute featuresVA. Thus, our visual-\nsemantic decoder can effectively localize the image regions\nmost relevant to each attribute in a given image. The multi-\nhead self-attention layer uses the outputs of the encoder U\nas keys (Kd\nt ) and values (Vd\nt ) and a set of learnable semantic\nembeddings VA as queries (Qd\nt ). It is deﬁned as:\nQd\nt = VAWd\nqt;Kd\nt = UWd\nkt;V d\nt = UWd\nvt; (9)\nheadt = softmax\n(\nQd\nt Kd>\nt\n√\ndd\n)\nVd\nt ; (10)\n^F = ∥T\nt=1(headt)Wo; (11)\nwhere Wd\nqt, Wd\nkt, Wd\nvt are the learnable weights,dd is a scal-\ning factor, and ∥is a concatenation operation. Then, an FFN\nwith two linear transformations followed a ReLU activation\nin between is applied to the attended features ^F:\nF = ReLu\n(\n^FW1 + b1\n)\nW2 + b2; (12)\nwhere W1, W2, b1 and b2 are the weights and biases of the\nlinear layers respectively, and F are the locality-augmented\nvisual features.\nVisual-Semantic Embedding Network\nAfter generating locality-augmented visual features, we fur-\nther map them into the semantic embedding space. To en-\ncourage the mapping to be more accurate, we take the se-\nmantic attribute vectors VA = {va}A\na=1 as support, based\non a mapping function (M). Speciﬁcally, Mmatches the\nlocality-augmented visual features F with the semantic at-\ntribute information vA:\n (xi) =M(F) =V⊤\nA WF; (13)\nwhere W is an embedding matrix that embeds F into the\nsemantic attribute space. In essence,  (xi)[a] is an attribute\nscore that represents the conﬁdence of having the a-th at-\ntribute in the imagexi. Given a set of semantic attribute vec-\ntors VA = {va}A\na=1, TransZero attains a mapped semantic\nembedding  (xi).\nModel Optimization\nTo achieve effective optimization, we employ the attribute\nregression loss, attribute-based cross-entropy loss and self-\ncalibration loss to train TransZero.\nAttribute Regression Loss. To encourage VSEN to accu-\nrately map visual features into their corresponding seman-\ntic embeddings, we introduce an attribute regression loss to\nconstrain TransZero. Here, we regard visual-semantic map-\nping as a regression problem and minimize the mean square\nerror between the ground truth attribute zc and the embed-\nded attribute score  (xs\ni ) of a set of sample {xs\ni }nb\ni=1:\nLAR = −1\nnb\nnb∑\ni=1\n∥ (xs\ni ) −zc∥2\n2: (14)\nAttribute-Based Cross-Entropy Loss. Since the associ-\nated image embedding is projected near its class semantic\nvector zc when an attribute is visually present in an image,\nwe take the attribute-based cross-entropy loss LACE to op-\ntimize the parameters of the TransZero model, i.e., the dot\nproduct between the visual embedding and each class se-\nmantic vector is calculated to produce class logits. This en-\ncourages the image to have the highest compatibility score\nwith its corresponding class semantic vector. Given a batch\nof nb training images {xs\ni }nb\ni=1 with their corresponding class\nsemantic vectors zc, LACE is deﬁned as:\nLACE = −1\nnb\nnb∑\ni=1\nlog exp ( (xs\ni ) ×zc)∑\n^c∈Cs exp ( (xs\ni ) ×z^c): (15)\nSelf-Calibration Loss. Since LAR and LACE optimize the\nmodel on seen classes, TransZero inevitably overﬁts to these\nclasses, as also observed in (Zhu et al. 2019; Huynh and El-\nhamifar 2020a; Xu et al. 2020). To tackle this challenge,\nwe further introduce a self-calibration loss LSC to explic-\nitly shift some of the prediction probabilities from seen to\nunseen classes. LSC is thus formulated as:\nLSC = −1\nnb\nnb∑\ni=1\nCu\n∑\nc0=1\nlog\nexp\n(\n (xs\ni ) ×zc0\n+ I[c0∈Cu]\n)\n∑\n^c∈Cexp\n(\n (xs\ni ) ×z^c + I[^c∈Cu]\n);\n(16)\nwhere I[c∈Cu] is an indicator function (i.e., it is 1 when\nc ∈ Cu, otherwise -1). Intuitively, LACE encourages non-\nzero probabilities to be assigned to the unseen classes during\ntraining, which allows TransZero to produce a (large) non-\nzero probability for the true unseen class when given test\nsamples from unseen classes.\nFinally, we formulate the overall loss function of Tran-\nsZero:\nLtotal = LACE + \u0015ARLAR + \u0015SC LSC ; (17)\nwhere \u0015AR and \u0015SC are the weights to controle their corre-\nsponding loss terms.\n333\nMethods\nCUB SUN A\nWA2\nCZSL GZSL\nCZSL GZSL CZSL GZSL\nacc U\nS H acc U S H acc U S H\nEnd-to-End\nQFSL (Song\net al. 2018) 58.8 33.3 48.1 39.4 56.2 30.9 18.5 23.1 63.5 52.1 72.8 60.7\nLDF (Li et al. 2018) 67.5 26.4 81.6 39.9 – – – – 65.5 9.8 87.4 17.6\nSGMA∗(Zhu et al. 2019) 71.0 36.7 71.3 48.5 – – – – 68.8 37.6 87.1 52.5\nAREN∗(Xie et al. 2019) 71.8 38.9 78.7 52.1 60.6 19.0 38.8 25.5 67.9 15.6 92.9 26.7\nLFGAA∗(Liu et al. 2019) 67.6 36.2 80.9 50.0 61.5 18.5 40.0 25.3 68.1 27.0 93.4 41.9\nAPN∗(Xu et al. 2020) 72.0 65.3 69.3 67.2 61.6 41.9 34.0 37.6 68.4 57.1 72.4 63.9\nNon End-to-End\nGenerativ\ne Methods\nf-CLSWGAN (Xian et al. 2018) 57.3 43.7 57.7 49.7 60.8 42.6 36.6 39.4 68.2 57.9 61.4 59.6\nf-V AEGAN-D2 (Xian et al. 2019) 61.0 48.4 60.1 53.6 64.7 45.1 38.0 41.3 71.1 57.6 70.6 63.5\nOCD-CV AE (Keshari, Singh, and Vatsa 2020) – 44.8 59.9 51.3 – 44.8 42.9 43.8 – 59.5 73.4 65.7\nE-PGN (Yu et al. 2020) 72.4 52.0 61.1 56.2 – – – – 73.4 52.6 83.5 64.6\nComposer (Huynh and Elhamifar 2020b) 69.4 56.4 63.8 59.9 62.6 55.1 22.0 31.4 71.5 62.1 77.3 68.8\nGCM-CF (Yue et al. 2021) – 61.0 59.7 60.3 – 47.9 37.8 42.2 – 60.4 75.1 67.0\nFREE (Chen et al. 2021a) – 55.7 59.9 57.7 – 47.4 37.2 41.7 – 60.4 75.4 67.1\nHSV A (Chen et al. 2021b) 62.8 52.7 58.3 55.3 63.8 48.6 39.0 43.3 – 59.3 76.6 66.8\nNon-Generative\nMethods\nSP-AEN (Chen et al. 2018) 55.4 34.7 70.6 46.6 59.2 24.9 38.6 30.3 58.5 23.3 90.9 37.1\nPQZSL (Li et al. 2019) – 43.2 51.4 46.9 – 35.1 35.3 35.2 – 31.7 70.9 43.8\nIIR (Cacheux, Borgne, and Crucianu 2019) 63.8 30.4 65.8 41.2 63.5 22.0 34.1 26.7 67.9 17.6 87.0 28.9\nTCN (Jiang et al. 2019) 59.5 52.6 52.0 52.3 61.5 31.2 37.3 34.0 71.2 61.2 65.8 63.4\nDVBE (Min et al. 2020) – 53.2 60.2 56.5 – 45.0 37.2 40.7 – 63.6 70.8 67.0\nDAZLE∗(Huynh and Elhamifar 2020a) 66.0 56.7 59.6 58.1 59.4 52.3 24.3 33.2 67.9 60.3 75.7 67.1\nTransZer\no (Ours) 76.8 69.3 68.3 68.8 65.6 52.6 33.4 40.8 70.1 61.3 82.3 70.2\nTable 1: Results (%) of the state-of-the-art CZSL and GZSL modes on CUB, SUN and AW A2, including end-to-end and non\nend-to-end methods (generative and non-generative methods). The Symbol “–” indicates no results. The Symbol “*” denotes\nattention-based methods.\nZero-Shot Prediction\nAfter training TransZero, we ﬁrst obtain the embedding fea-\ntures of a test instance xi in the semantic space i.e.,  (xi).\nThen, we take an explicit calibration to predict the test label\nof xi, which is formulated as:\nc∗= arg max\nc∈Cu=C\n (xi) ×zc + I[c∈Cu]: (18)\nHere, Cu=Ccorresponds to the CZSL/GZSL setting respec-\ntively.\nExperiments\nDataset Our extensive experiments are conducted on\nthree popular ZSL benchmarks, including two ﬁne-grained\ndatasets (e.g., CUB (Welinder et al. 2010) and SUN (Pat-\nterson and Hays 2012)) and a coarse-grained dataset (e.g.,\nAW A2 (Xian, Schiele, and Akata 2017)). CUB has 11,788\nimages of 200 bird classes (seen/unseen classes = 150/50)\ndepicted with 312 attributes. SUN includes 14,340 images\nfrom 717 scene classes (seen/unseen classes = 645/72) de-\npicted with 102 attributes. AW A2 consists of 37,322 images\nfrom 50 animal classes (seen/unseen classes = 40/10) de-\npicted with 85 attributes.\nEvaluation Protocols Following (Xian, Schiele, and\nAkata 2017), we measure the top-1 accuracy both in the\nCZSL and GZSL settings. In the CZSL setting, we simply\npredict the unseen classes to compute the accuracy of test\nsamples, i.e., acc. In the GZSL setting, we compute the ac-\ncuracy of the test samples from both the seen classes (de-\nnoted as S) and unseen classes (denoted as U). Meanwhile,\ntheir harmonic mean (deﬁned as H = (2\u0002S \u0002U)=(S +\nU)) is also employed for evaluation in the GZSL setting.\nImplementation Details We use the training splits pro-\nposed in (Xian et al. 2018). We take a ResNet101 pre-trained\non ImageNet as the CNN backbone for extracting the feature\nmap without ﬁne-tuning. We use the SGD optimizer with\nhyperparameters (momentum = 0.9, weight decay = 0.0001)\nto optimize our model. The learning rate and batch size are\nset to 0.0001 and 50, respectively. We empirically set \u0015SC\nto 0.3 and \u0015AR to 0.005 for all datasets. The encoder and\ndecoder layers are set to 1 with one attention head.\nComparison with State of the Art\nConventional Zero-Shot Learning. Here, we ﬁrst com-\npare our TransZero with the state-of-the-art methods in\nthe CZSL setting. As shown in Table 1, our TransZero\nachieves the best accuracies of 76.8% and 65.6% on CUB\nand SUN, respectively. This shows that TransZero effec-\ntively learns the attribute-augmented region feature repre-\nsentations for distinguishing various ﬁne-grained classes. As\nfor the coarse-grained dataset, TransZero still obtains com-\npetitive performance, with a top-1 accuracy of 70.1%. Com-\npared with other attention-based methods (e.g., SGMA (Zhu\net al. 2019), AREN (Xie et al. 2019), APN (Xu et al. 2020)),\nTransZero obtains signiﬁcant gains of over 4.8% and 4.0%\non CUB and SUN, respectively. This demonstrates that the\nattribute localization representations learned by our Tran-\nsZero are more discriminative than the region embeddings\nlearned by the existing attention-based methods.\n334\n(a) AREN\n(b) T\nransZero\nFigure 3: Visualization of attention maps for the attention-based method (i.e, AREN (Xie et al. 2019)) and our TransZero.\nMethod CUB SUN\nacc H\nacc H\nTransZero\nw/o FAE 67.3 56.8 61.2 32.1\nTransZero w/o FA 74.0 66.5 63.8 38.5\nTransZero w/o DEC 62.3 53.7 58.3 31.6\nTransZero w/o LSC 74.8 58.1 64.2 37.4\nTransZero w/o LAR 74.5 67.3 64.1 39.1\nTransZero (full) 76.8 68.8 65.6 40.8\nTable 2: Ablation studies for different components of Tran-\nsZero on the CUB and SUN datasets. “FAE” is the feature\naugmentation encoder, “FA” means feature augmentation,\nand “DEC” denotes visual-semantic decoder.\nGeneralized Zero-Shot Learning. Table 1 shows the re-\nsults of different methods in the GZSL setting. The results\nshow that the unseen accuracy (U) of all methods is usually\nlower than the seen accuracy (S ) on the CUB and AW A2\ndatasets, i.e., U < S. Meanwhile, U > S on the SUN\ndataset since the number of seen classes is much larger than\nthe number of unseen classes.\nWe can see that most state-of-the-art methods achieve\ngood results on seen classes but fail on unseen classes,\nwhile our method generalizes better to unseen classes with\nhigh unseen and seen accuracies. For example, TransZero\nachieves the best performance with harmonic mean of\n68.8% and 70.2% on CUB and AW A2, respectively. We ar-\ngue that the beneﬁts of TransZero come from the fact that\ni) the feature augmentation encoder in AGT improves the\ndiscriminability and transferability of visual features, and\nii) the self-calibration mechanism alleviates the bias prob-\nlem. Finally, our TransZero also outperforms the attention-\nbased methods by harmonic mean improvements of at least\n1.6%, 3.2% and 3.1% on CUB, SUN and AW A2, respec-\ntively. This demonstrates the superiority and great potential\nof our attribute-guided Transformer for the ZSL task.\nAblation Study\nTo provide the further insight into TransZero, we conduct\nablation studies to evaluate the effects of the feature aug-\nmentation encoder (denoted as FAE), feature augmenta-\ntion in FAE (denoted as FA), visual-semantic decoder (de-\nnoted as DEC), self-calibration loss (i.e., LSC ) and at-\ntribute regression loss (i.e., LAR). Our results are shown\nin Table 2. TransZero performs signiﬁcantly worse than\nits full model when no feature augmentation encoder is\nused, i.e., the acc/harmonic mean drops by 9.5%/12.0%\non CUB and 4.4%/8.7% on SUN. If we incorporate the\nencoder of the standard Transformer without feature aug-\nmentation, TransZero again achieves poor results compared\nto its full model, i.e., the acc/harmonic mean drops by\n2.8%/2.3% and 1.8%/2.3% on CUB and SUN, respectively.\nWhen TransZero without visual-semantic decoder, its per-\nformances decreases dramatically on all datasets. Moreover,\nthe self-calibration mechanism can effectively alleviate the\nbias problem, resulting in improvements in the harmonic\nmean of 10.7% and 3.4% on CUB and SUN, respectively.\nThe attribute regression constraint further improves the per-\nformance of TransZero by directing VSEN to conduct effec-\ntive visual-semantic mapping.\nQualitative Results\nVisualization of Attention Maps. To intuitively show\nthe effectiveness of our TransZero at learning locality-\naugmented visual features, we visualize the attention maps\nlearned by the existing attention-based methods (e.g., AREN\n(Xie et al. 2019)) and TransZero. As shown in Fig. 3, AREN\nsimply learns region embeddings for visual representations,\ne.g., the whole bird body, neglecting the ﬁne-grained seman-\ntic attribute information. In contrast, our Transzero learns\ndiscriminative attribute localization for visual features by as-\nsigning high positive scores to key attributes (e.g., the ‘bill\nshape all purpose’ of the Acadian Flycatcher in Fig. 3).\nThus, TransZero achieves signiﬁcant performance both in\nseen and unseen classes.\nt-SNE Visualizations. As shown in Fig. 4, we present the\nt-SNE visualization (Maaten and Hinton 2008) of visual fea-\ntures for (a) seen classes and (b) unseen classes on CUB,\nlearned by the CNN backbone, TransZero encoder w/o FA,\nTransZero encoder, and TransZero decoder. When we incor-\nporate the standard encoder into our TransZero, the visual\nfeatures learned by the encoder are signiﬁcantly improved\ncompared to the original visual features extracted from the\n335\n(a) Seen\nClasses\n(b) Unseen\nClasses\nFigure 4: t-SNE visualizations of visual features for (a) seen classes and (b) unseen classes, learned by the CNN backbone,\nTransZero encoder w/o FA, TransZero encoder, and TransZero decoder. The 10 colors denote 10 different seen/unseen classes\nrandomly selected from CUB.\n(a) CUB (b) SUN\nFigure 5: The effects of \u0015AR.\nCNN Backbone (e.g., ResNet101). When we use the feature\naugmentation encoder to reﬁne the original visual features,\nthe quality of the unseen features is further enhanced. These\nresults demonstrate that the encoder of TransZero effectively\nalleviates the cross-dataset bias problem and reduces the en-\ntangled relative geometry relationships among different re-\ngions, improving the transferability. Moreover, the visual-\nsemantic decoder learns locality-augmented visual features\nfor improving visual feature representations.\nHyperparameter Analysis\nEffects of Loss Weight. \u0015AR is employed to weigh the\nimportance of the attribute regression loss, which directs\nthe VSEN to conduct effective visual-semantic interaction.\nWe try a wide range of \u0015AR evaluated on CUB and SUN,\ni.e., \u0015AR = {0:001;0:005;0:01;0:05;0:1;0:5}. Results are\nshown in Fig. 5. When \u0015AR is set to a large value, all eval-\nuation protocols tend to drop. This is because the loss value\nof the attribute regression loss is too large, and thus the con-\ntributions of other losses are mitigated. When \u0015AR is set to\n0.005, our TransZero achieves the best performance.\n(a) CUB (b) SUN\nFigure 6: The effects of \u0015SC .\nEffects of Loss Weight. \u0015SC adjusts the weight of the\nself-calibration loss, which effectively alleviates the bias\nproblem. As shown in Fig. 6, the accuracy on seen classes in-\ncreases and the accuracy on unseen classes decreases when\nwe increase \u0015SC . Meanwhile, TransZero is insensitive to the\nself-calibration loss in the CZSL setting. We investigate a\nwide range of \u0015SC on CUB and SUN to ﬁnd an appropriate\nsetting for \u0015SC . Based on the results, we set \u0015SC to 0.3 for\nall datasets.\nConclusion\nIn this paper, we propose a novel attribute-guided Trans-\nformer network for ZSL (termed TransZero). First, our Tran-\nsZero employs a feature augmentation encoder to improve\nthe discriminability and transferability of visual features by\nalleviating the cross-dataset problem and reducing the en-\ntangled region feature relationships. Meanwhile, a visual-\nsemantic decoder is introduced to learn the attribute localiza-\ntion for locality-augmented visual feature representations.\nSecondly, a visual-semantic embedding network is used\nto enable effective visual-semantic interaction between the\nlearned locality-augmented visual features and class seman-\ntic vectors. Extensive experiments on three popular bench-\nmark datasets demonstrate the superiority of our approach.\n336\nWe believe that our work also facilitates the development\nof other visual-and-language learning systems, e.g., natural\nlanguage for visual reasoning.\nAcknowledgements\nThis work is partially supported by NSFC (61772220),\nSpecial projects for technological innovation in Hubei\nProvince (2018ACA135) and Key R&D Plan of Hubei\nProvince (2020BAB027).\nReferences\nArora, G.; Verma, V .; Mishra, A.; and Rai, P. 2018. Gen-\neralized Zero-Shot Learning via Synthesized Examples. In\nCVPR, 4281–4289.\nAtzmon, Y .; Kreuk, F.; Shalit, U.; and Chechik, G. 2020.\nA causal view of compositional zero-shot recognition. In\nNeurIPS.\nCacheux, Y . L.; Borgne, H.; and Crucianu, M. 2019. Mod-\neling Inter and Intra-Class Relations in the Triplet Loss for\nZero-Shot Learning. In ICCV, 10332–10341.\nChen, L.; Zhang, H.; Xiao, J.; Liu, W.; and Chang, S. 2018.\nZero-Shot Visual Recognition Using Semantics-Preserving\nAdversarial Embedding Networks. In CVPR, 1043–1052.\nChen, S.; Wang, W.; Xia, B.; Peng, Q.; You, X.; Zheng, F.;\nand Shao, L. 2021a. FREE: Feature Reﬁnement for Gener-\nalized Zero-Shot Learning. In ICCV.\nChen, S.; Xie, G.-S.; Yang Liu, Y .; Peng, Q.; Sun, B.; Li, H.;\nYou, X.; and Shao, L. 2021b. HSV A: Hierarchical Semantic-\nVisual Adaptation for Zero-Shot Learning. In NeurIPS.\nChen, T.; Pu, T.; Xie, Y .; Wu, H.; Liu, L.; and Lin, L. 2021c.\nCross-Domain Facial Expression Recognition: A Uniﬁed\nEvaluation Benchmark and Adversarial Graph Learning.\nIEEE transactions on pattern analysis and machine intel-\nligence.\nChou, Y .-Y .; Lin, H.-T.; and Liu, T.-L. 2021. Adaptive and\nGenerative Zero-Shot Learning. In ICLR.\nCornia, M.; Stefanini, M.; Baraldi, L.; and Cucchiara, R.\n2020. Meshed-Memory Transformer for Image Captioning.\nIn CVPR, 10575–10584.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In ICLR.\nGabeur, V .; Sun, C.; Karteek, A.; and Schmid, C. 2020.\nMulti-modal Transformer for Video Retrieval. In ECCV.\nHan, Z.; Fu, Z.; Chen, S.; and Yang, J. 2021. Contrastive\nEmbedding for Generalized Zero-Shot Learning. In CVPR.\nHerdade, S.; Kappeler, A.; Boakye, K.; and Soares, J. 2019.\nImage Captioning: Transforming Objects into Words. In\nNeurIPS.\nHuang, L.; Wang, W.; Chen, J.; and Wei, X.-Y . 2019. At-\ntention on Attention for Image Captioning. In ICCV, 4633–\n4642.\nHuynh, D.; and Elhamifar, E. 2020a. Fine-Grained General-\nized Zero-Shot Learning via Dense Attribute-Based Atten-\ntion. In CVPR, 4482–4492.\nHuynh, D. T.; and Elhamifar, E. 2020b. Compositional Zero-\nShot Learning via Fine-Grained Dense Feature Composi-\ntion. In NeurIPS.\nJiang, H.; Wang, R.; Shan, S.; and Chen, X. 2019. Transfer-\nable Contrastive Network for Generalized Zero-Shot Learn-\ning. In ICCV, 9764–9773.\nKeshari, R.; Singh, R.; and Vatsa, M. 2020. Generalized\nZero-Shot Learning via Over-Complete Distribution. In\nCVPR, 13297–13305.\nKhan, S.; Naseer, M.; Hayat, M.; Zamir, S. W.; Khan, F.; and\nShah, M. 2021. Transformers in Vision: A Survey. arXiv\npreprint arXiv:2101.01169.\nLampert, C. H.; Nickisch, H.; and Harmeling, S. 2009.\nLearning to detect unseen object classes by between-class\nattribute transfer. In CVPR, 951–958.\nLampert, C. H.; Nickisch, H.; and Harmeling, S. 2014.\nAttribute-Based Classiﬁcation for Zero-Shot Visual Object\nCategorization. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 36: 453–465.\nLarochelle, H.; Erhan, D.; and Bengio, Y . 2008. Zero-data\nLearning of New Tasks. In AAAI, 646–651.\nLi, J.; Lan, X.; Liu, Y .; Wang, L.; and Zheng, N. 2019. Com-\npressing Unknown Images With Product Quantizer for Efﬁ-\ncient Zero-Shot Classiﬁcation. In CVPR, 5458–5467.\nLi, Y .; Zhang, J.; Zhang, J.; and Huang, K. 2018. Discrim-\ninative Learning of Latent Features for Zero-Shot Recogni-\ntion. In CVPR, 7463–7471.\nLiu, Y .; Guo, J.; Cai, D.; and He, X. 2019. Attribute Atten-\ntion for Semantic Disambiguation in Zero-Shot Learning. In\nICCV, 6697–6706.\nLiu, Y .; Zhou, L.; Bai, X.; Huang, Y .; Gu, L.; Zhou, J.; and\nHarada, T. 2021. Goal-Oriented Gaze Estimation for Zero-\nShot Learning. In CVPR.\nMaaten, L. V . D.; and Hinton, G. E. 2008. Visualizing Data\nusing t-SNE. Journal of Machine Learning Research, 9:\n2579–2605.\nMin, S.; Yao, H.; Xie, H.; Wang, C.; Zha, Z.; and Zhang, Y .\n2020. Domain-Aware Visual Bias Eliminating for General-\nized Zero-Shot Learning. In CVPR, 12661–12670.\nOtt, M.; Edunov, S.; Grangier, D.; and Auli, M. 2018. Scal-\ning Neural Machine Translation. In WMT.\nPalatucci, M.; Pomerleau, D.; Hinton, G. E.; and Mitchell,\nT. M. 2009. Zero-shot Learning with Semantic Output\nCodes. In NeurIPS, 1410–1418.\nPan, Y .; Yao, T.; Li, Y .; and Mei, T. 2020. X-Linear Attention\nNetworks for Image Captioning. In CVPR, 10968–10977.\nPatterson, G.; and Hays, J. 2012. SUN attribute database:\nDiscovering, annotating, and recognizing scene attributes. In\nCVPR, 2751–2758.\nPennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:\nGlobal Vectors for Word Representation. In EMNLP.\n337\nReed, S.; Akata, Z.; Lee, H.; and Schiele, B. 2016. Learning\nDeep Representations of Fine-Grained Visual Descriptions.\nIn CVPR, 49–58.\nSch¨onfeld, E.; Ebrahimi, S.; Sinha, S.; Darrell, T.; and\nAkata, Z. 2019. Generalized Zero- and Few-Shot Learn-\ning via Aligned Variational Autoencoders. In CVPR, 8239–\n8247.\nShen, Y .; Qin, J.; and Huang, L. 2020. Invertible Zero-Shot\nRecognition Flows. In ECCV.\nSong, J.; Shen, C.; Yang, Y .; Liu, Y .; and Song, M. 2018.\nTransductive Unbiased Embedding for Zero-Shot Learning.\nCVPR, 1024–1033.\nVaswani, A.; Shazeer, N. M.; Parmar, N.; Uszkoreit, J.;\nJones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017.\nAttention is All you Need. In NeurIPS.\nWelinder, P.; Branson, S.; Mita, T.; Wah, C.; Schroff, F.; Be-\nlongie, S. J.; and Perona, P. 2010. Caltech-UCSD Birds 200.\nTechnical Report CNS-TR-2010-001, Caltech,.\nXian, Y .; Lorenz, T.; Schiele, B.; and Akata, Z. 2018. Fea-\nture Generating Networks for Zero-Shot Learning. InCVPR,\n5542–5551.\nXian, Y .; Schiele, B.; and Akata, Z. 2017. Zero-Shot Learn-\ning — The Good, the Bad and the Ugly. In CVPR, 3077–\n3086.\nXian, Y .; Sharma, S.; Schiele, B.; and Akata, Z. 2019. F-\nV AEGAN-D2: A Feature Generating Framework for Any-\nShot Learning. In CVPR, 10267–10276.\nXie, G.-S.; Liu, L.; Jin, X.; Zhu, F.; Zhang, Z.; Qin, J.; Yao,\nY .; and Shao, L. 2019. Attentive Region Embedding Net-\nwork for Zero-Shot Learning. In CVPR, 9376–9385.\nXie, G.-S.; Liu, L.; Jin, X.; Zhu, F.; Zhang, Z.; Yao, Y .; Qin,\nJ.; and Shao, L. 2020. Region Graph Embedding Network\nfor Zero-Shot Learning. In ECCV.\nXu, W.; Xian, Y .; Wang, J.; Schiele, B.; and Akata, Z. 2020.\nAttribute Prototype Network for Zero-Shot Learning. In\nNeurIPS.\nYu, Y .; Ji, Z.; Fu, Y .; Guo, J.; Pang, Y .; and Zhang, Z.\n2018. Stacked Semantics-Guided Attention Model for Fine-\nGrained Zero-Shot Learning. In NeurIPS.\nYu, Y .; Ji, Z.; Han, J.; and Zhang, Z. 2020. Episode-Based\nPrototype Generating Network for Zero-Shot Learning. In\nCVPR, 14032–14041.\nYue, Z.; Wang, T.; Zhang, H.; Sun, Q.; and Hua, X. 2021.\nCounterfactual Zero-Shot and Open-Set Visual Recognition.\nIn CVPR.\nZhang, X.; Sun, X.; Luo, Y .; Ji, J.; Zhou, Y .; Wu, Y .; Huang,\nF.; and Ji, R. 2021. RSTNet: Captioning with Adaptive At-\ntention on Visual and Non-Visual Words. In CVPR.\nZhu, Y .; Xie, J.; Tang, Z.; Peng, X.; and Elgammal, A.\n2019. Semantic-Guided Multi-Attention Localization for\nZero-Shot Learning. In NeurIPS.\n338",
  "topic": "Discriminative model",
  "concepts": [
    {
      "name": "Discriminative model",
      "score": 0.8046953678131104
    },
    {
      "name": "Computer science",
      "score": 0.7577447891235352
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6831825375556946
    },
    {
      "name": "Embedding",
      "score": 0.6734790205955505
    },
    {
      "name": "Encoder",
      "score": 0.563020646572113
    },
    {
      "name": "Locality",
      "score": 0.5481536388397217
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5201671123504639
    },
    {
      "name": "Semantic feature",
      "score": 0.5097343325614929
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.48327523469924927
    },
    {
      "name": "Transferability",
      "score": 0.44019922614097595
    },
    {
      "name": "Transformer",
      "score": 0.4333896040916443
    },
    {
      "name": "Natural language processing",
      "score": 0.42575234174728394
    },
    {
      "name": "Machine learning",
      "score": 0.329847127199173
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Logit",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I47720641",
      "name": "Huazhong University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210116052",
      "name": "Inception Institute of Artificial Intelligence",
      "country": "AE"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    }
  ]
}