{
  "title": "Class-Based N-Gram Language Model for New Words Using Out-of-Vocabulary to In-Vocabulary Similarity",
  "url": "https://openalex.org/W1983032739",
  "year": 2012,
  "authors": [
    {
      "id": "https://openalex.org/A5067201583",
      "name": "Welly Naptali",
      "affiliations": [
        "Toyohashi University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5047336359",
      "name": "Masatoshi Tsuchiya",
      "affiliations": [
        "Toyohashi University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5018428974",
      "name": "Seiichi Nakagawa",
      "affiliations": [
        "Toyohashi University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4301749008",
    "https://openalex.org/W2162965720",
    "https://openalex.org/W4302086270",
    "https://openalex.org/W2118172714",
    "https://openalex.org/W1497026191",
    "https://openalex.org/W2105830342",
    "https://openalex.org/W2401075988",
    "https://openalex.org/W2134939182",
    "https://openalex.org/W30564313",
    "https://openalex.org/W2595741664",
    "https://openalex.org/W298351679",
    "https://openalex.org/W2142222482",
    "https://openalex.org/W2217717732",
    "https://openalex.org/W76928536",
    "https://openalex.org/W2084531783",
    "https://openalex.org/W119112377",
    "https://openalex.org/W2302164184",
    "https://openalex.org/W1501139663",
    "https://openalex.org/W2143969840",
    "https://openalex.org/W1537482007",
    "https://openalex.org/W49282310",
    "https://openalex.org/W32751375",
    "https://openalex.org/W2135535719"
  ],
  "abstract": "Out-of-vocabulary (OOV) words create serious problems for automatic speech recognition (ASR) systems. Not only are they miss-recognized as in-vocabulary (IV) words with similar phonetics, but the error also causes further errors in nearby words. Language models (LMs) for most open vocabulary ASR systems treat OOV words as a single entity, ignoring the linguistic information. In this paper we present a class-based n-gram LM that is able to deal with OOV words by treating each of them individually without retraining all the LM parameters. OOV words are assigned to IV classes consisting of similar semantic meanings for IV words. The World Wide Web is used to acquire additional data for finding the relation between the OOV and IV words. An evaluation based on adjusted perplexity and word-error-rate was carried out on the Wall Street Journal corpus. The result suggests the preference of the use of multiple classes for OOV words, instead of one unknown class.",
  "full_text": "2308\nIEICE TRANS. INF. & SYST., VOL.E95–D, NO.9 SEPTEMBER 2012\nPAPER\nClass-Based N-Gram Language Model for New Words Using\nOut-of-Vocabulary to In-Vocabulary Similarity\nWelly NAPTALI†∗a), Nonmember, Masatoshi TSUCHIYA††, Member, and Seiichi NAKAGAWA†, Fellow\nSUMMARY Out-of-vocabulary (OOV) words create serious problems\nfor automatic speech recognition (ASR) systems. Not only are they miss-\nrecognized as in-vocabulary (IV) words with similar phonetics, but the er-\nror also causes further errors in nearby words. Language models (LMs)\nfor most open vocabulary ASR systems treat OOV words as a single entity,\nignoring the linguistic information. In this paper we present a class-based\nn-gram LM that is able to deal with OOV words by treating each of them\nindividually without retraining all the LM parameters. OOV words are as-\nsigned to IV classes consisting of similar semantic meanings for IV words.\nThe World Wide Web is used to acquire additional data for ﬁnding the re-\nlation between the OOV and IV words. An evaluation based on adjusted\nperplexity and word-error-rate was carried out on the Wall Street Journal\ncorpus. The result suggests the preference of the use of multiple classes for\nOOV words, instead of one unknown class.\nkey words: out-of-vocabulary, class-based n-gram, language model, ad-\njusted perplexity, speech recognition\n1. Introduction\nAn ASR system’s users tend to speak natural sentences that\noften contain out-of-vocabulary (OOV) words. OOV words\npresent a serious problem for automatic speech recognition\n(ASR) in that they introduce two error types into the sys-\ntem. First, OOV words are substituted with in-vocabulary\n(IV) words and second, the error a ﬀects other words nearby.\nTypically three steps are needed to handle OOV words in an\nASR system [1]. The ﬁrst step determines whether an utter-\nance contains an OOV word, while the second involves rec-\nognizing sub-word units contained in the OOV word. The ﬁ-\nnal step involves the recovery, converting the sub-word unit\ninto the corresponding OOV word. There is another ap-\nproach, that is, registering the OOV words into the ASR’s\nvocabulary. In this paper, we focus on estimating the prob-\nability of an OOV word after registering it in the ASR’s vo-\ncabulary. We simplify the problem as follows: given an\nOOV word, how do we assign a probability to that word\nwithout retraining the whole language model (LM).\nCalculating the probability of OOV words is not an\neasy task, especially in cases where no data or insu ﬃcient\nManuscript received November 14, 2011.\nManuscript revised April 17, 2012.\n†The authors are with the Department of Computer Science\nand Engineering, Toyohashi University of Technology, Toyohashi-\nshi, 441–8580 Japan.\n††The author is with the Information and Media Center,\nToyohashi University of Technology, Toyohashi-shi, 441–8580\nJapan.\n∗Presently, with Academic Center for Computing and Media\nStudies, Kyoto University.\na) E-mail: naptali@slp.cs.tut.ac.jp\nDOI: 10.1587/transinf.E95.D.2308\nrelevant training data are available. There is always a signif-\nicant amount of OOV words even when the vocabulary size\nis very large. A common approach to handling OOV words\nis to assume a special token <UNK>to represent all un-\nknown words. This unknown word token is treated the same\nas any other IV word in the probability estimation. This\nstraightforward approach has two shortcomings [7]. First,\nthere is a mismatch in the frequency of OOV words in the\ntraining and the test data. Second, the approach ignores their\nsyntactic types and other linguistic information.\nJelinek et al. [8] studied the problem of OOV words by\nassigning the word into statistically synonymous classes us-\ning maximum mutual information and maximum likelihood\napproach. When encountering a new word during evalua-\ntion, the word is added to the vocabulary and the probability\nis updated according to class-based n-gram LM. Suhm et\nal. [19] built an OOV-aware language model by mapping all\nwords outside a given vocabulary into the new word class.\nThe proposed LM is no other than a word-based n-gram with\nthe OOV words mapped into classes. Gallwitz et al. [7] pro-\nposed the similar approach to Jelinek’s but without the need\nto update the new word into the vocabulary. They assign\nmanually the OOV words into a word class. The work is\nthen followed by Bazzi and Glass [2], suggested multiple\nclasses for OOV words. They used a small number of OOV\nclasses (8 −32 classes) by utilizing parts-of-speech (POS)\nand agglomerative clustering. Martins et al. [10] proposed\na method to handle OOV words without the need for ad-\nditional data or LM retraining by using POS information.\nThen, recenly, Lecorve et al. [9] proposed a method utiliz-\ning POS and an n-gram similarity to obtain the probability\nof OOV words.\nTo handle the OOV words on ASR system, ﬁrst we\nhave to register the OOV words themselves in the ASR sys-\ntem’s vocabulary. The registration could be performed au-\ntomatically from the related web data of test data domain,\nor manually added by the user. After registering the OOV\nwords, one may re-train the LM and then perform the ASR,\nor by making a correction to the ASR’s results. Recovering\nOOV words using web data is not something new. Several\napproaches have been proposed by some researchers. Meth-\nods that use web data to increase the training data for re-\ntraining/adapting the LM and registering the vocabulary in\nthe ASR system have been discussed in [14] and [16]. In\n[15], web data was used to recover the OOV words, but the\nLM probabilities were obtained from their POS. However,\nPOS has a rather limited number of categories. Furthermore,\nCopyright c⃝2012 The Institute of Electronics, Information and Communication Engineers\nNAPTALI et al.: CLASS-BASED N-GRAM LANGUAGE MODEL FOR NEW WORDS USING OUT-OF-VOCABULARY TO IN-VOCABULARY SIMILARITY\n2309\nmost OOV words are proper nouns.\nThis paper proposes a framework to estimate the prob-\nability of OOV words without retraining the LM using addi-\ntional data (e.g., data from the World Wide Web (WWW)).\nAdopting a class-based n-gram LM, we categorize OOV\nwords as IV word classes with similar meanings, then each\nof the IV words is treated as a singleton class. In this way,\nfor example, a new person’s name (OOV word) can be asso-\nciated with an existing person’s name in the IV words that\nis related to a word class for OOV words. First, documents\nmust be collected to ﬁnd the class of the OOV word. Then,\nwe ﬁnd the relation of this OOV word to the IV words using\na deﬁned similarity measure. To verify the proposed model,\nwe compare it with the conventional models, in which all\nOOV words are treated as a single special token <UNK>,\nthe increasing IV size method based on retraining, and some\ncomparable baselines in terms of adjusted perplexity and\nWER.\nIn the remainder of this paper, we ﬁrst give our basic\nconsideration in Sect. 2. Then in Sect. 3, we describe details\nof the proposed method about how to ﬁnd the similar IV\nwords to the OOV on observation using WWW data. Sec-\ntion 4 gives the evaluation result on adjusted perplexity, and\nSect. 5 gives the evaluation result on WER for automatic\nspeech recognition. The paper ends with our conclusions\nand future works.\n2. Basic Consideration\nOOV words can be categorized into two types. The ﬁrst\ntype occurs in the training data (and in the test data). In this\ncase, the new words can be added and the LM is retrained,\nalthough it takes much computation time to recalculate all\nthe probability distributions. Moreover, the probability of\nthis new word will likely be unreliable owing to its low fre-\nquency of occurrence in the training data. The second type\nincludes OOV words that do not occur in the training data,\nbut only in the test data. For this type of OOV word, ad-\nditional data are needed to provide information about the\nword. Without any information, there is no way of assign-\ning an appropriate probability to the OOV word. See Fig. 1\nfor the illustration.\nOOV words of the ﬁrst type are relatively easier to deal\nwith than those of the second type, since the number of OOV\nwords is known, whereas the number of OOV words and the\nOOV rate in the second type are unknown. Without knowing\nthe total number of OOV words and OOV rate, it is hard to\nFig. 1 Two kinds of OOV words.\nconduct an evaluation. Therefore, we made an assumption\nthat the second type of OOV words would follow the ﬁrst\ntype in terms of evaluation, i.e., the kinds of OOV and OOV\nrate. Hence, we investigate the ﬁrst type of OOV words, and\nthen do the same for the second type.\n2.1 Perplexity and Adjusted Perplexity\nTo evaluate whether an LM is better or worse, we can per-\nform an ASR experiment and compare its word-error-rate\n(WER). However, this involves the entire ASR system pro-\ncess and consumes computation time. A simpler and more\nwidely used approach is to calculate its perplexity (PP) [13],\nPP =(P(W))\n−1\nN , (1)\nwhere W is a word sequence W =w1,w2,...,w N that in-\ncludes OOV words (mapped into a unique unknown symbol\n<UNK>), i.e., P(wOOV ) →P(<UNK>), where wOOV is one\nof unknown words. Therefore, the perplexity changes ac-\ncording to the vocabulary size. Usually perplexity decreases\nwhen the vocabulary size is smaller because of an increased\nthe number of OOV words or OOV ratio. Thus, the per-\nplexity measure should be reported together with the OOV\nrate. To compare two LMs with di ﬀerent vocabulary sizes,\nanother measure, called adjusted perplexity (APP) , is used.\nIn the APP, P(w\nOOV ) →P(<UNK>) 1\n|wOOV |. In this paper, we\nuse the APP metric introduced in [20] and [11], deﬁned as\nfollows:\nAPP =(P(W)m\n−o)−1\nN , (2)\nwhere o is the total number of OOV words in the test data,\nm is the number of di ﬀerent OOV words in the training\ndata ( m = |wOOV |), and N is the total number of words\n(that is, including both IV and OOV words) in the test data.\nTherefore, adjusted perplexity normalizes the e ﬀect of a re-\nduction in OOV rate on the perplexity. In other words,\nP(w\nOOV | <UNK>) = 1\nm . For our model, P(wOOVj ) →\nP(COOVj ) 1\n|wOOV ∈COOV j |, where COOVj means the word class for\nwOOVj .\n2.2 Study Case\nHandling OOV words using multiple classes is better than\nmapping it only to one class. Suppose we have a data set\nconsisting of 5 ,000 OOV words with OOV rate 5%. Then\nlet us map αof the OOV words (with βoccurence) to UNK 1\nclass, and the rest to UNK 2 class.\n# Words Occurence ratio\nUNK 1 α×5000 β\nUNK 2 (1 −α) ×5000 1 −β\nThen the probability of each OOV word can be calculated\nas follows:\nP(UNK 1) = 5\n100 ·β· 1\nα·5000,\n2310\nIEICE TRANS. INF. & SYST., VOL.E95–D, NO.9 SEPTEMBER 2012\nP(UNK 2) = 5\n100 ·(1 −β) · 1\n(1 −α) ·5000,\nand the expected total probability of OOV words is given by,\nP(OOV) =P(UNK 1)β·P(UNK 2)(1−β).\nIf we map the OOV words into one class, then α=β,r e -\nsulting P(OOV) =10−5. Note that mapping the OOV words\nrandomly will also cause α=β. In the other side, mapping\nthe OOV words (non-random) into multiple classes makes\nα/nequalβ, and P(OOV) >10\n−5. The larger or smaller the ra-\ntio of αand βis, the larger P(OOV) becomes. To prove\nthis statement, let γbe an OOV rate, M is the number of\nOOV words, N is the number of clusters, pi (i =1,2,..., N)\nis the kind rate of OOV words in each cluster, and qi\n(i =1,2,..., N) is the occurance rate of OOV words in each\ncluster. The probability of OOV words is given by:\nP(OOVi) =γqi\n1\nM ·pi\n.\nThe expected value of P(OOVi)i sg i v e nb y\nlogP(OOV) =logγ\nM +\n∑\nqilogqi −\n∑\nqilogpi.\nNote that the following expression of inequality is satisﬁed:\n−\n∑\nqilogpi ⩾ −\n∑\nqilogqi.\nTherefore, P(OOV for multi-classes) ⩾ P(OOV for a single\nclass). This leads to decrease APP.\n3. The Proposed Method\nA word-based n-gram language model is not adequate for\nmodeling OOV words that are very rare. A class-based n-\ngram language model is more suitable, since the rare words\ncan rely on other frequent words in the same class. For a\ngiven history h\ni =wi−n+1 ...w i−1, a class-based n-gram lan-\nguage model is deﬁned as follows:\nP(wi|hi) =P(Ci|Ci−n+1,..., Ci−1)P(wi|Ci), (3)\nwhere Ci is the class of word wi. Since the IV words are\nfrequent words, we map these words into singleton classes.\nThen each of the OOV words is mapped into the correspond-\ning class of IV classes. There are two problems to be solved.\nThe ﬁrst problem is how to ﬁnd the class of an OOV word\n(C\nOOV ). We solve this problem by mapping a word into a\nword vector and using a similarity measure to ﬁnd the clos-\nest word in IV words. The second problem is how to cal-\nculate P(wi|Ci) for OOV words. We assume that all OOV\nclasses are known, and then we calculate P(wOOV |COOV )\nbased on a uniform model:\nP(wOOV |COOV ) = 1\n|# K i n do fO O Vw o r d si nCOOV |. (4)\nNote that since IV words use singleton classes, the value of\nP(wi|Ci) for IV words is 1 .0.\nFig. 2 Training procedure of Proposed Method.\nFig. 3 Word to class mapping process.\nThe overall training procedure is illustrated by the chart\nin Fig. 2. In this phase, a web data is required to make OOV\nclasses. The mapping process of IV words and OOV words\nare illustrated by Fig. 3. Where n is the number of IV words,\nm +k is the total number of OOV words, m is the num-\nber of registered OOV words that have additional data, k is\nthe total number of OOV words that is not registered to the\nrecognition vocabulary (without any additional information,\nthere is no way to assign an appropriate probability to the\nOOV word. Therefore, for such OOVs, we use the unknown\nclass.), and corr{}means “corresponds to one of the classes\nof”.\n3.1 Clustering of IV Words\nBefore we ﬁnd the classes of OOV words, ﬁrst we need to\nmake classes of IV words. Note that these classes will not\nbe used to map the IV words in Eq. (3), but to be used to map\nthe OOV words through the similar IV words. In this paper,\nwe classify IV words based on the semantic similarity using\nlatent semantic analysis (LSA) [3]. LSA extracts semantic\nrelations from a corpus, and maps them on a low dimen-\nsion vector space. The discrete indexed words are projected\ninto an LSA space by applying singular value decomposi-\ntion (SVD) to a matrix that representing a corpus (represen-\ntation matrix). In the LSA space, any familiar clustering\nmethod could be applied to make semantic classes. In this\npaper, we used vector quantization with cosine distance.\n3.2 Clustering of OOV Words\nEach word w\ni can be represented by the following word vec-\ntor:\nwi =AT ci, (5)\nwhere A is the matrix representation and ci is the discrete\nvector of word wi, where the i-th element of the vector is\nNAPTALI et al.: CLASS-BASED N-GRAM LANGUAGE MODEL FOR NEW WORDS USING OUT-OF-VOCABULARY TO IN-VOCABULARY SIMILARITY\n2311\n(a) Term-document matrix.\n(b) Bigram matrix.\nFig. 4 Representation matrix A.\nset to 1 and all other elements are set to 0. We use a term-\ndocument matrix and a word co-occurrence matrix such as\nbigram or 1-4 distant bigram matrix [12] as matrix represen-\ntation to model the relation between words. Term-document\nmatrix A is a matrix where its cell a\nij contains frequency of\nword wi in the document dj. The observed OOV word wOOV\nis inserted in the last row of this matrix (see Fig. 4 (a)). Word\nco-occurrence matrix cell aij records how many times word\nwi occurs after wj. In other words, its row corresponds with\nthe current words and its column with the preceding words.\nUnlike the term-document matrix, the OOV word wOOV is\nalso inserted in the last column of the matrix (see Fig. 4 (b)).\nWe normalize the matrix according to a term frequency ( tf ):\ntf (i,j) = a\nij\n∑\nk akj\n. (6)\nAfter each word has been represented by a word vector, a\nsimilarity measure between an OOV word and any IV word\nis obtained to ﬁnd out which IV word can represent the ob-\nserved OOV word. Here, we used cosine similarity:\ncos(w\ni,wOOV) = wi.wOOV\n|wi||wOOV|. (7)\nBy sorting these scores in descending order for all wi ∈IV ,\nwe can determine the class to represent wOOV . However, we\nfound that several functional words appeared as the clos-\nest matching words to these OOV words. To avoid this, an\ninverse document frequency ( id f) weight is applied to (7).\nThe id f weight used in this work is deﬁned as follows:\nid f(w\ni) =log\n(dimension(wi)\n#Nonzero cell\n)\n. (8)\nThis id fweight is calculated from the training corpus (using\nthe same type of matrix representation), not from the corpus\nretrieved from the WWW. A small web corpus will be only\nresulting unreliable id f weight, and will not be e ﬀective on\neliminating the functional words. Thus, the similarity be-\ntween an IV word w\ni and an OOV word wOOVj is deﬁned\nby:\nSi m(wi,wOOVj ) =cosW (wi,wOOVj ) ∗id fT (wi), (9)\nwhere subscripts W and T indicate that the values are cal-\nculated using data collected from the WWW and from the\navailable training set, respectively. Finally, the class of OOV\nword wOOVj can be decided using the following criterion:\nCOOVj =Carg maxwi∈IV Si m(wi,wOOVj ), (10)\nwhere Cwi denotes the word class of wi to classify OOV.\nAfter obtaining the OOV word classes for all OOV words in\nthe training data, we mapped all the words to their classes\nand build a class-based n-gram model. Note that each of IV\nwords is mapped to its singleton class.\n4. Experiments on Adjusted Perplexity\nThe experiment was conducted on WSJ corpus from year\n1987 to 1989. The training corpus contains 39 ,962,779\nwords from 85 ,445 documents ( WSJTRAIN) and the\ntest corpus contains 365 ,730 words from 809 documents\n(WSJTEST). We made a reasonal assumption that any\nOOV or the same context appears at any additional web data.\nTherefore, the web data used is not necessarily the same.\nThe main point is to get an additional information about the\nOOV and its relation to the IV words from resources other\nthan training data. First, we conducted a preliminary exper-\niment to validate our proposed method using WWW to ﬁnd\nthe similar OOV words from IV words. Then we perform\nevaluation on OOV words that occur in the training data.\n4.1 World Wide Web as Additional Data\nUsing the most frequent 19 ,981 words as vocabulary, the\nOOV rate for training and testing corpus are 2 .47% and\n2.57%, respectively. There are 144 ,599 OOV kinds in the\ntraining set, and 6 ,102 OOV kinds in the test set. We col-\nlected at most 100 web page addresses for each OOV (us-\ning the OOV word itself as a keyword) from search engine\nGoogle\n† collected from December 2008 to February 2009.\nNote that not all OOV words exist in the WWW. We re-\nquire at least one occurrence for the class estimation of OOV\nword. A common stemming procedure to remove a ﬃxm a y\nsolve some of the problem.\nFrom the collected addresses, we retrieve the web page\nwith the depth of 1. Some web pages could not be retrieved\nfor some reasons (e.g., request time out), and some web\npages are not parseable (e.g., ﬂash, AJAX). However, these\nproblems could easily be solved using a programming tech-\n†http://www.google.com\n2312\nIEICE TRANS. INF. & SYST., VOL.E95–D, NO.9 SEPTEMBER 2012\nTable 1 Similarity results. The bold type face means semantically related IV word to the correspond-\ning OOV word.\n``````````OOV\nRepresentation matrix\nterm-doc bigram 1-4dbigram\nfogelman (proper name) cantor, philanthropy, armenians,\ndiablo, lifestyle, yu, accredited, po-\nlar, ne, partisans\nmcdonough, hello, stimulation,\nlyrics, awesome, undertake, hi, do-\nnation, transforming, whoever\ninterestingly, stimulation, believer,\nfatigue, strangers, meridian, im,\ngentle, oaks, mcdonough\ntroll (proper name) denim, russ, dragon, caves, wow,\nsailor, im, ne, coloring, brad\nstatue, blade, hassle, virtual, pot,\nsexes, caretaker, wreckage, breath,\nchair\ncave, tribes, ﬁnnish, edit, creature,\nim, capitals, preacher, thread, ah\nkurokawa (proper name) pavilion, empires, earthquakes, ﬂats,\nashes, organisms, gogh, ecology,\nprix, classmate\nretrospect, alexandria, acutely, gen-\neralized, molded, sorted, catherine,\nroses, purposely, villain\nbrowns, comics, chapel, acutely,\nroses, picasso, retrospect, oracle,\nrefreshing, residency\nsharer (proper name) softball, retarded, preacher, sprayed,\nquincy, raped, motorcycles, unan-\nnounced, starving, gravel\npartnership, stranger, agent, greene,\nmidst, telescope, ensuing, safest,\nnarrator, steward\nsteward, narrator, shave, mates,\nﬂashed, telescope, sails, tug, ladder,\nessay\nTable 2 Baseline PP and APP on WSJTRAIN for various vocabulary sizes.\nIV PP APP APP #OOVs #Kind OOV\nvocab size (OOV only) OOVs rate (%)\n1k 25.0 488.4 533,707 9,502,909 163,580 23.78\n5k 37.8 110.9 1,139,373 3,444,977 159,580 8.62\n20k 38.9 51.5 2,371,657 903,782 144,580 2.26\n21k 38.9 50.6 2,419,397 854,171 143,580 2.14\n25k 38.8 48.0 2,616,946 693,520 139,580 1.74\n30k 38.7 45.8 2,822,796 550,975 134,580 1.38\n40k 38.5 43.1 3,163,710 372,152 124,581 0.93\n60k 38.3 40.7 3,601,489 198,550 104,580 0.50\n100k 38.2 39.0 3,733,217 71,972 64,580 0.18\n165k 38.1 38.1 - 0 0 0\nnique. The retrieved web page can not be used for a lan-\nguage model directly. Several steps are necessary; clean-\ning HTML tags, removing boilerplate (e.g., canned text, in-\ncludes navigation bars, page headers, link list, disclaimers\nand copyright statements, and advertisement) and other un-\nwanted material. NCleaner toolkit [4] does this task auto-\nmatically. After cleaning the web data, we can build ma-\ntrix representation. The resulting matrix will be very sparse,\nsince it was built only based on a web corpus. Thus, the\nsimilarity measure calculation will not be a burden for com-\nputational resources. Table 1 shows the 10-closest IV words\nfor 4 OOV words using similarity measure deﬁned in Eq. (9)\nwith a term-document matrix, bigram matrix, and 1-4 dis-\ntance bigram matrix, respectively.\nWe can assume that the class of the 1\nst closest word is\nnot always suitable to represent the class of the OOV word.\nIn this case, we take the N-closest words and vote for the\nclasses of these words based on their occurrence to repre-\nsent the class of the OOV word. Another approach is to use\nthe averaged word vectors to represent each class, and then\nto calculate the similarity between each of these classes (in-\nstead of the IV words) and the OOV word. In this paper, we\nwill use the 1\nst candidate or 1 st closest word as the suitable\nclosest word.\n4.2 OOV Words that Occurs in the Training Data\nIn this section, we experimented with the training data us-\ning a limited vocabulary size and made an evaluation on the\nsame data. An ideal source for additional data is to use\nWWW. However, we used an English Wikipedia † dump\ndata on the 30 th of January 2010, because the number of\nOOV words is larger, and since there is a day limitation\nwhen requesting search results from a public search en-\ngine. The data is 5 .6 GB ﬁle consisting of 9 ,541,307 pages\nof articles or documents. For the rest of this section, this\nWikipedia data will be referred as web data.\n4.2.1 WSJTRAIN\nWe begin the experiment using the same training and test\ndata, i.e., WSJTRAIN, to avoid the e ﬀect of backo ﬀfor un-\nseen event. The baseline perplexity and adjusted perplexity\non the training data for various vocabulary sizes are given\nin Table 2. The baseline is a class-based 3-gram LM where\neach of IV words is mapped into a singleton class and OOV\nwords are mapped into <UNK>. The proposed model is a\nclass-based 3-gram where each IV word is mapped into a\nsingleton class, and OOV word is mapped into a similar IV\nword class, while the rest of words (UNK words) is mapped\nto <UNK>class.\nIn this research, we only take into consideration using\n20,000 IV words and 1 k,5 k,1 0k, and 20 k OOV words (ﬁrst\ntype). While the rest follows the traditional approach, i.e.,\nby mapping into a special token <UNK>, and we will refer\nto these as UNK words. The numbers of classes for OOV\n†http://en.wikipedia.com\nNAPTALI et al.: CLASS-BASED N-GRAM LANGUAGE MODEL FOR NEW WORDS USING OUT-OF-VOCABULARY TO IN-VOCABULARY SIMILARITY\n2313\nTable 3 Statistics of OOV , and UNK on WSJTRAIN for the Proposed\nMethod (+20k IV).\nOOV vocab size #OOVs #Kind OOVs #UNKs #Kind UNKs\n1k 47,622 960 856,160 143,620\n5k 199,342 4,736 704,440 139,844\n10k 333,206 9,431 570,576 135,149\n20k 488,152 18,008 415,630 126,572\nwords are 100 and 200. Note that these numbers should be\noptimized in the future. Table 3 shows the statistics for this\nmodel. Note that there is the di ﬀerence between OOV vocab\nsize and the number of kind OOVs, it is caused by the OOV\nwords without additional data (i.e., it does not appear on the\nweb) and it is mapped to the unknown class.\nTable 4 (a) gives the adjusted perplexity of the pro-\nposed model for each parameter. Compared to the base-\nline with 20 ,000 vocabulary, the adjusted perplexity is get-\nting better when we treat OOV words in multiple classes,\ninstead of one class. A small di ﬀerences on adjusted per-\nplexity is caused by the large number of IV words in the\ncalculation, compared to the number of OOV words. There-\nfore, we also calculated the adjusted perplexity where the\nsamples are only when the OOV words appear as the cur-\nrent word in a word sequence (APP (OOV only) in Table 4).\nTo further validate our proposed model, we compare it\nwith four other baselines; they are:\n•LSA baseline: A similar approach with the proposed\nmodel, diﬀerent similarity and without using web data.\nThe model uses the available LSA projection matrix to\nmap OOV words into IV classes (described below in\ndetail).\n•TRAIN baseline: A similar approach with the proposed\nmodel, the same similarity but without using web data.\nInstead, the model used training data split in docu-\nments, and used a simple indexing to retrieve docu-\nments that consisted of OOV words on observation.\nNote that TRAIN database contains all OOV words in\nthis evaluation.\n•RANDOMCLUSTER baseline: A similar approach\nwith the proposed model, di ﬀerent similarity and with-\nout using web data. Instead, the model used a random\napproach to map OOV words into IV classes during\ntraining, then are used by the deﬁned classes during\ntesting. Note that this random cluster is consistent with\nthe training data and test data (see Table 5 (a)). There-\nfore, this clustering method is impractical for new OOV\nwords in test corpus, because we have no means for\nclassifying the new word into one of pre-deﬁned clus-\nters.\n•RANDOM baseline: A similar approach with the pro-\nposed model, without using similarity and web data.\nInstead, this model used a completely random approach\nto map OOV words into IV classes during training and\ntesting. Di ﬀerent to RANDOMCLUSTER baseline,\nthe approach used no information taken from the OOV\nclasses during training, in other words, the cluster is not\nTable 4 Adjusted perplexity on WSJTRAIN ( +20k IV).\n(a) Proposed method\nOOV APP APP (OOV only)\nvocab 100 IV 200 IV 100 IV 200 IV\nsize Classes Classes Classes Classes\n0k 51.5 51.5 2,371,657 2,371,657\n1k 50.9 50.9 1,787,706 1,748,026\n5k 49.6 49.4 954,245 870,700\n10k 48.7 48.4 612,479 526,917\n20k 47.9 47.5 409,558 328,474\n(b) LSA baseline method\nOOV APP APP (OOV only)\nvocab 100 IV 200 IV 100 IV 200 IV\nsize Classes Classes Classes Classes\n1k 50.9 50.8 1,784,771 1,742,419\n5k 49.6 49.4 939,074 850,345\n10k 48.6 48.3 594,578 507,993\n20k 47.8 47.4 394,205 315,471\n(c) TRAIN baseline method\nOOV APP APP (OOV only)\nvocab 100 IV 200 IV 100 IV 200 IV\nsize Classes Classes Classes Classes\n1k 50.9 50.9 1,792,445 1,753,818\n5k 49.6 49.4 946,755 865,875\n10k 48.7 48.4 604,244 523,758\n20k 47.9 47.5 400,608 324,894\n(d) RANDOMCLUSTER baseline method\nOOV APP APP (OOV only)\nvocab 100 IV 200 IV 100 IV 200 IV\nsize Classes Classes Classes Classes\n1k 50.9 50.8 1,760,705 1,707,698\n5k 49.5 49.2 899,359 795,909\n10k 48.5 48.1 558,831 458,433\n20k 47.6 47.1 360,581 271,127\n(e) RANDOM baseline method\nOOV APP APP (OOV only)\nvocab 100 IV 200 IV 100 IV 200 IV\nsize Classes Classes Classes Classes\n1k 51.7 51.7 2,453,470 2,501,994\n5k 52.3 52.3 2,475,521 2,594,257\n10k 52.7 52.9 2,504,126 2,682,534\n20k 53.3 53.5 2,651,975 2,910,058\nTable 5 Example of word class mapping.\n(a) RANDOMCLUSTER method\nTraining Phase Test Phase\nw1 →CLASS 1 w1 →CLASS 1\nw2 →CLASS 2 w2 →CLASS 2\nw3 →CLASS 2 w3 →CLASS 2\nw4 →CLASS 1 w4 →CLASS 1\nw5 →CLASS 2 w5 →CLASS 2\n(b) RANDOM method\nTraining Phase Test Phase\nw1 →CLASS 1 w1 →CLASS 2\nw2 →CLASS 2 w2 →CLASS 2\nw3 →CLASS 2 w3 →CLASS 1\nw4 →CLASS 1 w4 →CLASS 2\nw5 →CLASS 2 w5 →CLASS 1\n2314\nIEICE TRANS. INF. & SYST., VOL.E95–D, NO.9 SEPTEMBER 2012\nconsistent with the training and test (see Table 5 (b)).\nThis leads to use wrong class in the test phase.\nFirst, let us perform the LSA baseline. From IV cluster-\ning, we obtained matrix projection that was used to project\nIV words into a vector space. Then, we used this projec-\ntion matrix to map OOV words into the same vector space\naccording to\nw\nOOV =wOOV VS−1, (11)\nwhere wOOV is an OOV word matrix corresponding to the\ndocuments in the training data and V and S are document\nand singular matrices, respectively, obtained from LSA. Af-\nter projecting OOV words into a vector space, for each OOV\nword, the distance against class centroids was calculated and\nassign the OOV word to an IV class with the nearest cen-\ntroid. Table 4 (b) gives the adjusted perplexity. Compared\nto the proposed model performance in Table 4 (a), there is\nno signiﬁcant di ﬀerences on adjusted perplexity. These re-\nsults suggest that obtaining OOV information from the web\ndata is as reliable as obtaining OOV information from the\ntraining data.This is a good sign for modeling the second\ntype of OOV words. The conclusion is also supported by\nTRAIN baseline, where its adjusted perplexity is given by\nTable 4 (c).\nFinally, RANDOMCLUSTER and RANDOM base-\nlines are performed using Fisher-Yates shu ﬄe [5]. Fisher-\nYates shuﬄe is an algorithm for generating a random per-\nmutation of a ﬁnite set. The results are given by Tables 4 (d),\nand 4 (e), respectively. RANDOMCLUSTER performs the\nbest amongs other baseline. It also performs better than our\nproposed method. The reason is because RANDOMCLUS-\nTER is actually a partial random method, i.e., it uses the\nOOV class relation in the training and test phases (see Ta-\nble 5 (a). For instance, OOV word hitchcock is mapped to\nclass C\n39 randomly during training, and during the test the\nword hitchcock will also be mapped into class C39.H o w -\never, when applied this to handle the OOV words that do not\noccur in the training data, the assignment of OOV classes\nwill become completely random. Thus, the performance of\nRANDOMCLUSTER will most likely perform as worse as\nRANDOM baseline. While using our proposed model to\nhandling the OOV words that do not occur in the training\ndata is expected to give results as good as handling the OOV\nwords that occur in the training data (Table 4 (a)).\nNote that LSA baseline, TRAIN baseline, and RAN-\nDOMCLUSTER except for the proposed method are not\navailable for the second type of OOV .\n4.2.2 WSJTEST\nThe experiment is ended by making an evaluation on\nWSJTEST data (open test data). Using web data, all OOV\nwords in the training data are mapped to word classes. For\n145k model (144,580 in Table 2), only 60 k OOV words have\nadditional data in the web, therefore we trained OOV classes\nusing only 60 k OOV words and the rest 85 k OOV words\nTable 6 Adjusted perplexity on WSJTEST for the proposed method\n(+20k IV).\nOOV APP APP (OOV only) OOV\nvocab 100 IV 200 IV 100 IV 200 IV rate\nsize Classes Classes Classes Classes (%)\n1k (1k) 149.3 149.3 3,119,956 3,089,913 2.39\n5k (5k) 148.4 148.2 2,499,368 2,446,090 1.99\n10k (9k) 147.8 147.6 2,193,484 2,168,025 1.66\n20k (18k) 147.5 147.3 2,024,062 1,991,909 1.27\n145k (60k) 148.6 148.4 2,652,141 2,607,767 0.75\nBaseline 149.8 3,426.387 2.52\nwere treated as <UNK>. The adjusted perplexity is given\nby Table 6. It shows that modeling the OOV words using\nmultiple classes lead to better adjusted perplexity than the\nbaseline with a single OOV class.\n5. Experiments on ASR\n5.1 Setup\nUsing the HTK toolkit [22], we trained acoustic models for\nAmerican English using 49 ,190 utterances from the Wall\nStreet Journal (WSJ) corpus for the years 1987-1989. The\nresulting feature vector is 39-dimensional, and consists of\n12 MFCCs plus the 0\nth ceptral, together with their ﬁrst\nand second deviation coe ﬃcients, normalized using ceptral\nmean subtraction. We also used the CMU pronunciation dic-\ntionary\n† and LOGIOS lexicon ††, containing 39 phonemes\nwithout lexical stress. The HMM models are initialized\nbased on TIMIT phonetic transcriptions. Cross-word tri-\nphones are learned by tied-state triphones based on a de-\ncision tree. There are 16 Gaussians for non-silent states\nand 32 for the silent state. For more details, please refer to\n[21]. The language model is similar to the previous section\n(Sect. 5.2). For the decoder, we used the in-house large vo-\ncabulary continuous speech recognition system, SPOJUS ++\n(SPOken Japanese Understanding System) [6].\nWe selected test data from the WSJ collection, consist-\ning of 553 sentences (10 ,957 words). Our used test dataset\nis relatively di ﬃcult set, because we selected the dataset\nin such a way that each sentence or utterance contained at\nleast one OOV word from a vocabulary of 20 ,000 words.\nThe OOV rate and APP are 6 .26% and 359 .6f o ra2 0 k vo-\ncabulary size, respectively. We denote this test set as WS-\nJASR. The APP, correct (CORR), and WER are given in\nTable 7 for various vocabulary sizes. The WER and OOV\nrate for this test dataset using a word-based trigram and ig-\nnoring the OOV words are also given in the table. Note\nthat in this experiment we focused on a vocabulary size\nof 20 ,000 (IV words) with a WER of 27 .3%. Registering\nOOV words in the ASR system vocabulary improves the\nWER, as can be seen in Table 8 by adding 1 k,5 k,1 0 k,\nand 20 k words, respectively. Note that this baseline is for\na class-based 3-gram, where all the IV words are mapped\n†http://www.speech.cs.cmu.edu/cgi-bin/cmudict\n††http://www.speech.cs.cmu.edu/tools/lextool.html\nNAPTALI et al.: CLASS-BASED N-GRAM LANGUAGE MODEL FOR NEW WORDS USING OUT-OF-VOCABULARY TO IN-VOCABULARY SIMILARITY\n2315\nTable 7 Adjusted perplexity and WER of baseline on WSJASR (%).\nV ocab size OOV rate (%) # OOVs in test # Kind OOVs in test PP APP Del Ins Sub CORR WER\n20k 6.26 686 595 164.3 359.6 3.2 4.1 20.0 76.8 27.3\n21k 5.89 645 559 169.6 354.0 3.2 3.9 19.5 77.2 26.7\n25k 4.52 495 424 191.1 335.8 3.3 3.2 18.1 78.6 24.6\n30k 3.26 357 302 212.1 318.1 3.4 2.8 17.2 79.4 23.4\n40k 2.04 212 178 239.5 304.2 3.4 2.1 16.2 80.4 21.7\n60k 0.64 67 49 274.2 295.4 3.7 1.6 15.2 81.2 20.4\n100k 0.19 20 15 289.8 296.0 3.8 1.4 15.1 81.1 20.4\n165k 0.11 12 9 297.8 297.8 3.9 1.3 14.8 81.3 20.0\nTable 8 WER of baseline by registering OOV words with 20k IV words\non WSJASR (%) (1 class).\nOOV V ocab Size APP Del Ins Sub CORR WER\nfor registration\n1k 359.6 3.3 3.8 19.6 77.1 26.7\n5k 359.6 3.4 3.3 18.5 78.1 25.2\n10k 359.6 3.4 2.9 17.8 78.8 24.2\n20k 359.6 3.6 2.3 16.9 79.6 22.7\nTable 9 Adjusted perplexity of the proposed method on 100 and 200 IV\nclasses on WSJASR ( +20k IV words).\nOOV PP APP\nvocab 100 IV 200 IV 100 IV 200 IV\nsize classes classes classes classes\n1k 170.6 170.3 356.7 356.1\n5k 195.7 195.5 346.3 345.9\n10k 221.5 220.0 339.2 336.8\n20k 253.6 251.0 337.0 333.6\nBaseline (1 class) 164.3 359.6\nTable 10 WER of the proposed method with 20k IV words on WSJASR\n(%).\n(a) 100 IV classes\nOOV V ocab Size Del Ins Sub CORR WER\nfor registration\n1k 3.3 3.8 19.6 77.1 26.7\n5k 3.4 3.1 18.3 78.3 24.8\n10k 3.5 2.8 17.5 79.0 23.9\n20k 3.7 2.2 16.8 79.5 22.7\n(b) 200 IV classes\nOOV V ocab Size Del Ins Sub CORR WER\nfor registration\n1k 3.3 3.8 19.6 77.1 26.7\n5k 3.1 3.1 18.3 78.3 24.9\n10k 2.8 2.8 17.5 79.0 23.8\n20k 3.6 2.1 16.6 79.8 22.4\nto a singleton class, and all the OOV words are mapped to\nan <UNK>class, where each OOV’s probability is given by\nP(<UNK>) × 1\n|#OOV|.\n5.2 First Type of OOVs\nFor the proposed method, the adjusted perplexity is given\nin Table 9. Similar to the previous section, the proposed\nmethod achieves better adjusted perplexity compared with\nthe baseline. The adjusted perplexity also improves with\nan increase in the number of OOV words registered on ob-\nservation. The WER for the proposed method can be seen\nFig. 5 WER of the Proposed Method on WSJASR. Baseline (word-\nbased) is a word-based 3-gram with OOV words mapped to a single class.\nin Tables 10 (a) and 10 (b). Similar to the adjusted per-\nplexity, the proposed method also performs better than the\nbaseline in Table 8 in terms of WER. Incorporating OOV\nwords yields an improvement of at most 18 .0% relative to\nthe WER against a model without OOV words. The pro-\nposed method also yields at most 1 .7% relative improve-\nment on WER against a model with a single <UNK>class\n(see Fig. 5), where the row “40k” in Table 7 corresponds to\nthe mark “ ×” at the right corner in Fig. 5.\nAlthough the proposed method performs worse than a\nword-based LM, which must retrain the LM by using all\ntraining data including OOV words, the proposed method\ndoes not require any retraining, and is therefore, more e ﬀec-\ntive for the second type of OOV words, but we can not use\nthe word-based method.\n5.3 Second Type of OOVs\nIn this subsection, we add the second type of OOV words\ninto the experiment. As we can see in Table 7, there are\n9 kinds of OOV words (12 occurrences) that does not ap-\npear on the training data. Using the same o ﬄine version of\nWikipedia that was explained in Sect. 5.2, we are only able\nto get the relation of 3 OOV words to the IV word classes.\nTherefore, we used di ﬀerent web data in recognition phase\nto obtain information about the OOV . We used the online\nversion of Wikipedia on April 2011. Using at most 100 ar-\nticles, we got the relation of 6 OOV words (8 occurrences).\nThe rest 3 OOV words (4 occurrences) of the second type\nwere mapped to <UNK>class. We performed the experi-\nments by taking into account of these OOV words (we will\n2316\nIEICE TRANS. INF. & SYST., VOL.E95–D, NO.9 SEPTEMBER 2012\nTable 11 WER of the proposed method by incorporating the second type\nof OOV words on WSJASR (%) ( +20k IV words).\nModel Del Ins Sub CORR WER\n1k OOV1 +9 OOV2 (1 class) 3.3 3.8 19.6 77.1 26.7\n1k OOV1 +6 OOV2 (100 classes) 3.3 3.8 19.5 77.2 26.6\n1k OOV1 +6 OOV2 (200 classes) 3.3 3.8 19.5 77.2 26.6\n20k OOV1 +9 OOV2 (1 class) 3.6 2.2 16.8 79.6 22.6\n20k OOV1 +6 OOV2 (100 classes) 3.7 2.2 16.7 79.6 22.6\n20k OOV1 +6 OOV2 (200 classes) 3.6 2.1 16.6 79.8 22.3\nrefer these as OOV2 words, and OOV1 for the ﬁrst type of\nOOV words) to some of our proposed model in the previ-\nous subsection, Sect. 5.2, where the IV size is 20 k,t h eO O V\nsizes (the ﬁrst type of OOV words) are 1 k and 20 k, and the\nnumber of classes are 100 and 200. The results can be seen\nin Table 11. Adding the OOV2 words to the system’s vo-\ncabulary and mapping the words to the <UNK>class did\nnot change the results (see the ﬁrst rows in Tables 10 and\n11). Finding the relation between the OOV2 words with the\nIV words lower the WER 0 .1% absolute for 1 k model and\n0.3% absolute for 20 k model. Statistical signiﬁcance was\ninvestigated on the latter experiment according to Strik et\nal. [17], [18] by using a combination of the Number of Er-\nrors per Sentence (NES) metric and the Wilcoxon Signed\nRank (WSR) test. The improvement is statistically signiﬁ-\ncant at the 0 .14% level ( p 2-tailed). Note that the proposed\nmethod also recovered the errors in the surrounding words\nby the correct recognition of unknown words. Among 12\noccurrences of OOV2, 5 occurrences were able to recov-\nered. Overall, from 426 OOV words (486 occurrences), the\n20k model baseline recovered 234 OOV words (268 occur-\nrences), and the our 20 k model recovered 268 OOV words\n(301 occurrences).\n6. Conclusions and Future Works\nWe have shown that the proposed method without retrain-\ning the LM is better than the baseline methods. The results\nsuggest the advantages of using multiple classes for OOV\nwords, instead of one unknown class. In this paper, we ex-\nperimented only on a certain vocabulary size, the number of\nobserved OOV words, and the number of OOV classes. For\nthe class estimation of OOV words, we only required at least\none occurrence of OOV word in the additional data, which\ncould result inadequate similarity. For future work, we aim\nto relax these limitations and optimize all three parameters\nto achieve a better OOV model.\nAcknowledgments\nThis research was supported in part by the Global COE Pro-\ngram “Frontiers of Intelligent Sensing” from the Ministry of\nEducation, Culture, Sports, Science and Technology, Japan.\nReferences\n[1] I. Bazzi and J.R. Glass, “Modeling out-of-vocabulary words for ro-\nbust speech recognition,” ICSLP 2000, pp.401–404, 2000.\n[2] I. Bazzi and J.R. Glass, “A multi-class approach for modelling out-\nof-vocabulary words,” ICSLP 2002, pp.1613–1616, 2002.\n[3] J.R. Bellegarda, J.W. Butzberger, Yen-Lu Chow, N.B. Coccaro, and\nD. Naik, “A novel word clustering algorithm based on latent seman-\ntic analysis,” Proc. Acoustics, Speech, and Signal Processing, vol.1,\npp.172–175, May 1996.\n[4] S. Evert, “A lightweight and e ﬃcient tool for cleaning web pages,”\n6th International Conference on Language Resources and Evalua-\ntion, pp.3489–3493, 2008.\n[5] R. Fisher and F. Yates, Statistical tables for biological, agricultural\nand medical research, 3rd ed. Oliver and Boyd, London, 1948.\n[6] Y . Fujii, K. Yamamoto, and S. Nakagawa, “Large vocabulary speech\nrecognition system: SPOJUS ++,” 11th WSEAS International Con-\nference on Multimedia Systems and Signal Processing (MUSP ’11),\npp.110–118, March 2011.\n[7] F. Gallwitz, E. Noeth, and H. Niemann, “A category based approach\nfor recognition of out-of-vocabulary words,” ICSLP 1996, vol.1,\npp.228–231, 1996.\n[8] F. Jelinek, R. Mercer, and S. Roukos, “Classifying words for im-\nproved statistical language models,” ICASSP 1990, pp.621–624,\n1990.\n[9] G. Lecorve, G. Gravier, and P. Sebillot, “Automatically ﬁnding se-\nmantically consistent n-grams to add new words in LVCSR sys-\ntems,” Proc. ICASSP, pp.4676–4679, May 2011.\n[10] C. Martins, A. Teixeira, and J. Neto, “Automatic estimation of lan-\nguage model parameters for unseen words using morpho-syntactic\ncontextual information,” Interspeech 2008, pp.1602–1605, 2008.\n[11] S. Nakagawa and H. Akamatsu, “A new computation method of per-\nplexity for text corpus including unknown words – new adjusted per-\nplexity,” Acoustic Society of Japan Autumn Meeting 1998, no.2-1-\n13, pp.63–64, Sept. 1998. (in Japanese).\n[12] W. Naptali, M. Tsuchiya, and S. Nakagawa, “Word co-occurrence\nmatrix and context dependent class in lsa based language model for\nspeech recognition,” North Atlantic University Union (NAUN) In-\nternational Journal of Computers, vol.3, no.1, pp.85–95, 2009.\n[13] H. Ney, S. Martin, and F. Wessel, “Discriminative training on lan-\nguage model,” Corpus-Based Methods in Language and Speech Pro-\ncessing, pp.174–207, Kluwer, The Netherlands, 1997.\n[14] T. Ngyz, M. Ostendorfy, M.-Y . Hwangy, M. Siuz, I. Bulykoy, and\nX. Leiy, “Web-data augmented language models for mandarin con-\nversational speech recognition,” IEEE International Conference on\nAcoustics, Speech and Signal Processing, pp.589–592, March 2005.\n[15] S. Oger, V . Popescu, and G. Linares, “Using the world wide web\nfor learning new words in continuous speech. recognition tasks:\nTwo case studies,” Proc. Speech and Computer 2009, pp.76–81, June\n2009.\n[16] C. Parada, A. Sethy, M. Dredze, and F. Jelinek, “A spoken term\ndetection framework for recovering out-of-vocabularywords using\nthe web,” Interspeech 2010, pp.1269–1272, Sept. 2010.\n[17] H. Strik, C. Cucchiarini, and J.M. Kessens, “Comparing the recog-\nnition performance of csrs: In search of an adequate metric and sta-\ntistical signiﬁcance test,” Proc. ICSLP 2000, pp.740–744, Beijing,\nChina, 2000.\n[18] H. Strik, C. Cucchiarini, and J.M. Kessens, “Comparing the perfor-\nmance of two CSRs: How to determine the signiﬁcance level of the\ndiﬀerences,” Proc. Eurospeech, vol.3, pp.2091–2094, Aalborg, Den-\nmark, 2001.\n[19] B. Suhm, M. Woszczyna, and A. Waibel, “Detection and transcrip-\ntion of new words,” EUROSPEECH 1993, pp.2179–2182, 1993.\n[20] J. Ueberla, “Analysing a simple language model: Some general con-\nclusions for language models for speech recognition,” Comput.\nSpeech Lang., vol.8, no.2, pp.153–176, 1994.\n[21] K. Vertanen, “Baseline WSJ acoustic models for HTK and Sphinx:\nTraining recipes and recognition experiments,” Technical report,\nCavendish Laboratory, University of Cambridge, 2006.\n[22] S. Young, G. Evermann, M. Gales, T. Hain, D. Kershaw, G. Moore,\nJ. Odell, D. Ollason, D. Povey, V . Valtchev, and P. Woodland, The\nHTK book (for HTK version 3.3). Cambridge, 2005.\nNAPTALI et al.: CLASS-BASED N-GRAM LANGUAGE MODEL FOR NEW WORDS USING OUT-OF-VOCABULARY TO IN-VOCABULARY SIMILARITY\n2317\nWelly Naptali received his B.Sc. of Math-\nematics degree (with honors) from Bandung In-\nstitute of Technology, Indonesia, in 2004. Re-\nceived his M.E. and Dr. of Informatics de-\ngree from Toyohashi University of Technology,\nJapan, in 2008 and 2011, respectively. He is now\na research fellow at Kyoto University. From July\n2004 to September 2005, he worked at the Re-\nsearch Consortium of Optimization on Gas and\nOil Transmission and Distribution Pipeline Net-\nwork (RC-OPPINET), Indonesia. His research\ninterests is in automatic speech recognition and natural language process-\ning.\nMasatoshi Tsuchiya received his B.E.,\nM.E., and Dr. of Informatics degrees from\nKyoto University in 1998, 2000, and 2007 re-\nspectively. He joined Computer Center of\nToyohashi University of Technology, which was\nreconstructed to Information Media Center in\n2005, as an assistant professor in 2004. His ma-\njor interest in research is natural language pro-\ncessing.\nSeiichi Nakagawa received Dr. of Eng. de-\ngree from Kyoto University in 1977. He joined\nthe faculty of Kyoto University in 1976 as a\nResearch Associate in the Department of Infor-\nmation Sciences. From 1980 to 1983, he was\nan Assistant Professor, from 1983 to 1990, he\nwas an Associate Professor, and since 1990, he\nhas been a Professor in the Department of Infor-\nmation and Computer Sciences, Toyohashi Uni-\nversity of Technology, Toyohashi. From 1985\nto 1986, he was a Visiting Scientist in the De-\npartment of Computer Sciences, Carnegie-Mellon University, Pittsburgh,\nUSA. He received the 1997 /2001 Paper Award from the IEICE and the\n1988 JC Bose Memorial Award from the Institution of Electro, Telecomm.\nEngrs. His major interests in research include automatic speech recogni-\ntion/speech processing, natural language processing, human interface and\nartiﬁcial intelligence. He is a Fellow of IPSJ.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9083164930343628
    },
    {
      "name": "Computer science",
      "score": 0.8382596969604492
    },
    {
      "name": "Vocabulary",
      "score": 0.6709117293357849
    },
    {
      "name": "Natural language processing",
      "score": 0.6670510768890381
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6319220066070557
    },
    {
      "name": "Word (group theory)",
      "score": 0.568729043006897
    },
    {
      "name": "Class (philosophy)",
      "score": 0.5653324723243713
    },
    {
      "name": "Language model",
      "score": 0.5520628690719604
    },
    {
      "name": "n-gram",
      "score": 0.5205860733985901
    },
    {
      "name": "Speech recognition",
      "score": 0.5033509135246277
    },
    {
      "name": "Word error rate",
      "score": 0.45436882972717285
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.42751646041870117
    },
    {
      "name": "Part of speech",
      "score": 0.41370242834091187
    },
    {
      "name": "Linguistics",
      "score": 0.25360023975372314
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}