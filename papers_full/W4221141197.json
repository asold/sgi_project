{
  "title": "A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models",
  "url": "https://openalex.org/W4221141197",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5046085212",
      "name": "Deming Ye",
      "affiliations": [
        "Tsinghua University",
        "Center for Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5043098453",
      "name": "Yankai Lin",
      "affiliations": [
        "Group Image (Poland)"
      ]
    },
    {
      "id": "https://openalex.org/A5100663450",
      "name": "Peng Li",
      "affiliations": [
        "Tsinghua University",
        "Group Image (Poland)"
      ]
    },
    {
      "id": "https://openalex.org/A5046448314",
      "name": "Maosong Sun",
      "affiliations": [
        "South China Institute of Collaborative Innovation",
        "Tsinghua University",
        "Center for Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100320723",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Tsinghua University",
        "Center for Information Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2252849902",
    "https://openalex.org/W2963777632",
    "https://openalex.org/W2963855739",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W3176793246",
    "https://openalex.org/W2971136144",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W3114916066",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W3102844651",
    "https://openalex.org/W2966610483",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W2980360762"
  ],
  "abstract": "Pre-trained language models (PLMs) cannot well recall rich factual knowledge of entities exhibited in large-scale corpora, especially those rare entities. In this paper, we propose to build a simple but effective Pluggable Entity Lookup Table (PELT) on demand by aggregating the entity’s output representations of multiple occurrences in the corpora. PELT can be compatibly plugged as inputs to infuse supplemental entity knowledge into PLMs. Compared to previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation with capability of acquiring knowledge from out-of-domain corpora for domain adaptation scenario. The experiments on knowledge-related tasks demonstrate that our method, PELT, can flexibly and effectively transfer entity knowledge from related corpora into PLMs with different architectures. Our code and models are publicly available at https://github.com/thunlp/PELT",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 523 - 529\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nA Simple but Effective Pluggable Entity Lookup Table\nfor Pre-trained Language Models\nDeming Ye1,2, Yankai Lin6, Peng Li6,7, Maosong Sun1,2,3,4,5∗, Zhiyuan Liu1,2,3,5\n1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China\n2Beijing National Research Center for Information Science and Technology\n3International Innovation Center of Tsinghua University, Shanghai, China\n4Jiangsu Collaborative Innovation Center for Language Ability, Xuzhou, China\n5Institute Guo Qiang, Tsinghua University 6Pattern Recognition Center, WeChat AI\n7Institute for AI Industry Research (AIR), Tsinghua University\nyedeming001@163.com\nAbstract\nPre-trained language models (PLMs) cannot\nwell recall rich factual knowledge of enti-\nties exhibited in large-scale corpora, espe-\ncially those rare entities. In this paper,\nwe propose to build a simple but effective\nPluggable Entity Lookup Table (PELT) on de-\nmand by aggregating the entity’s output repre-\nsentations of multiple occurrences in the cor-\npora. PELT can be compatibly plugged as\ninputs to infuse supplemental entity knowl-\nedge into PLMs. Compared to previous\nknowledge-enhanced PLMs, PELT only re-\nquires 0.2%∼5% pre-computation with capa-\nbility of acquiring knowledge from out-of-\ndomain corpora for domain adaptation sce-\nnario. The experiments on knowledge-related\ntasks demonstrate that our method, PELT, can\nﬂexibly and effectively transfer entity knowl-\nedge from related corpora into PLMs with dif-\nferent architectures. Our code and models are\npublicly available at https://github.com/\nthunlp/PELT.\n1 Introduction\nRecent advance in pre-trained language models\n(PLMs) has achieved promising improvements in\nvarious downstream tasks (Devlin et al., 2019; Liu\net al., 2019). Some latest works reveal that PLMs\ncan automatically acquire knowledge from large-\nscale corpora via self-supervised pre-training and\nthen encode the learned knowledge into their model\nparameters (Tenney et al., 2019; Petroni et al.,\n2019; Roberts et al., 2020). However, due to the\nlimited capacity of vocabulary, existing PLMs face\nthe challenge of recalling the factual knowledge\nfrom their parameters, especially for those rare en-\ntities (Gao et al., 2019a; Wang et al., 2021a).\nTo improve PLMs’ capability of entity under-\nstanding, a straightforward solution is to exploit\n∗Corresponding author: M. Sun (sms@tsinghua.edu.cn)\nModel #Ent Pre-Comp. D-Adapt\nZhang et al. (2019) 5.0M ∼160h No\nWang et al. (2021b) 4.6M ∼3,400h No\nYamada et al. (2020) 0.5M ∼3,800h No\nPELT (our model) 4.6M 7h Yes\nTable 1: Comparison of recent knowledge-enhanced\nPLMs. We report the pre-computation of BASE mod-\nels on Wikipedia entities on a V100 GPU. Pre-Comp.:\nPre-computation; D-Adapt: Domain Adaptation.\nan external entity embedding acquired from the\nknowledge graph (KG) (Zhang et al., 2019; Liu\net al., 2020; Wang et al., 2020), the entity descrip-\ntion (Peters et al., 2019), or the corpora (Pörner\net al., 2020). In order to make use of the ex-\nternal knowledge, these models usually learn to\nalign the external entity embedding (Bordes et al.,\n2013; Yamada et al., 2016) to the their original\nword embedding. However, previous works ignore\nto explore entity embedding from the PLM itself,\nwhich makes their learned embedding mapping is\nnot available in the domain-adaptation. Other re-\ncent works attempt to infuse knowledge into PLMs’\nparameters by extra pre-training, such as learning\nto build an additional entity vocabulary from the\ncorpora (Yamada et al., 2020; Févry et al., 2020), or\nadopting entity-related pre-training tasks to inten-\nsify the entity representation (Xiong et al., 2020;\nSun et al., 2020; Wang et al., 2021b). However,\ntheir huge pre-computation increases the cost of\nextending or updating the customized vocabulary\nfor various downstream tasks.\nIn this paper, we introduce a simple but effec-\ntive Pluggable Entity Lookup Table (PELT) to in-\nfuse knowledge into PLMs. To be speciﬁc, we\nﬁrst revisit the connection between PLMs’ input\nfeatures and output representations for masked lan-\nguage modeling. Based on this, given a new corpus,\nwe aggregate the output representations of masked\ntokens from the entity’s occurrences, to recover\n523\nan elaborate entity embedding from a well-trained\nPLM. Beneﬁting from the compatibility and ﬂex-\nibility of the constructed embedding, we can di-\nrectly insert them into the corresponding positions\nof the input sequence to provide supplemental en-\ntity knowledge. As shown in Table 1, our method\nmerely consumes 0.2%∼5% pre-computation com-\npared with previous works, and it also supports the\nvocabulary from different domains simultaneously.\nWe conduct experiments on two knowledge-\nrelated tasks, including knowledge probe and rela-\ntion classiﬁcation, across two domains (Wikipedia\nand biomedical publication). Experimental results\nshow that PLMs with PELT can consistently and\nsigniﬁcantly outperform the corresponding vanilla\nmodels. In addition, the entity embedding obtained\nfrom multiple domains are compatible with the\noriginal word embedding and can be applied and\ntransferred swiftly.\n2 Methodology\nIn this section, we ﬁrst revisit the masked language\nmodeling pre-training objective. After that, we\nintroduce the pluggable entity lookup table and\nexplain how to apply it to incorporate knowledge\ninto PLMs.\n2.1 Revisit Masked Language Modeling\nPLMs conduct self-supervised pre-training tasks,\nsuch as masked language modeling (MLM) (De-\nvlin et al., 2019), to learn the semantic and syntac-\ntic knowledge from the large-scale unlabeled cor-\npora (Rogers et al., 2020). MLM can be regarded\nas a kind of cloze task, which requires the model to\npredict the missing tokens based on its contextual\nrepresentation. Formally, given a sequence of to-\nkens X = (x1,x2,...,x n), with xi substituted by\n[MASK], PLMs, such as BERT, ﬁrst take tokens’\nword embedding and position embedding as input\nand obtain the contextual representation:\nH = Enc(LayerNorm(E(X) +P)), (1)\nwhere Enc(·) denotes a deep bidirectional Trans-\nformer encoder, LayerNorm(·) denotes layer nor-\nmalization (Ba et al., 2016), E ∈R|V |×D is the\nword embedding matrix, V is the word vocabu-\nlary, P is the absolute position embedding and\nH = (h1,h2,..., hn) is the contextual represen-\ntation. After that, BERT applies a feed-forward\nnetwork (FFN) and layer normalization on the con-\nWTO regards [MASK] has \nbecome a global epidemic.\n[MASK] is the disease caused \nby severe acute respiratory.\nPLM \nEncoding\nMasked Token’s\nOutput Rep.\nEntity\nEmbedding\nCOVID-19\nCOVID-19 \nOccurring Sentence\nFigure 1: An illustration of the our PELT.\ntextual representation to compute the output repre-\nsentation of xi:\nrxi = LayerNorm(FFN(hi)). (2)\nSince the weights in the softmax layer and word\nembeddings are tied in BERT, the model calculate\nthe product of rxi and the input word embedding\nmatrix to further compute xi’s cross-entropy loss\namong all the words:\nL= −\n∑\nlog Pr(xi|rxi)\n= −\n∑\nlog exp(E(xi)T rxi)∑\nwj∈V exp(E(wj)T rxi)\n.\n(3)\n2.2 Construct Pluggable Entity Embedding\nDue to the training efﬁciency, the vocabulary sizes\nin existing PLMs typically range from 30K to 60K\nsubword units, and thus PLMs have to disperse the\ninformation of massive entities into their subword\nembeddings. Through revisiting the MLM loss in\nEq. 3, we could intuitively observe that the word\nembedding and the output representation of BERT\nare located in the same vector space. Hence, we are\nable to recover the entity embedding from BERT’s\noutput representations to infuse their contextual-\nized knowledge to the model.\nTo be speciﬁc, given a general or domain-\nspeciﬁc corpus, we design to build the lookup table\nfor entities that occurs in the downstream tasks on\ndemand. For an entity e, such as a Wikidata entity\nor a proper noun entity, we construct its embedding\nE(e) as follows:\nDirection A feasible method to add entity eto\nthe vocabulary of PLM is to optimize its embed-\nding E(e) for the MLM loss with other parameters\nfrozen. We collect the sentences Se that contain\nentity eand substitute it with [MASK]. The total\ninﬂuence of E(e) to the MLM loss in Se can be\nformulated as:\nL(e) =−\n∑\nxi∈Se\nlog Pr(e|rxi)\n=\n∑\nxi∈Se\nlog Zxi −E(e)T ∑\nxi∈Se\nrxi,\n(4)\n524\nwhere Zxi = ∑\nwj∈V ∪{e}exp(E(wj)T rxi), xi\nis the replaced masked token for entity eand rxi is\nthe PLM’s output representation ofxi.\nCompared with the total impact of the entire\nvocabulary onZxi, E(e) has a much smaller impact.\nIf we ignore the minor effect of E(e) on Zxi, the\noptimal solution of E(e) for L(e) is proportional\nto ∑\nxi∈Se rxi. Hence, we set E(e) as:\nE(e) =C·\n∑\nxi∈Se\nrxi, (5)\nwhere Cdenotes the scaling factor.\nPractically, E(e) also serves as the negative log-\nlikelihood of other words’ MLM loss (Kong et al.,\n2020). However, Gao et al. (2019a) indicates that\nthe gradient from such negative log-likelihood will\npush all words to a uniformly negative direction,\nwhich weakens the quality of rare words’ represen-\ntation. Here, we ignore this negative term and ob-\ntain the informative entity embedding from Eq. 5.\nNorm We deﬁne p(e) as the position embedding\nfor entity e. Since the layer normalization in Eq. 1\nmakes the norm |E(e) +p(e)|to D\n1\n2 , we ﬁnd that\nthe norm |E(e)|has little effect on the input feature\nof the encoder in use. Therefore, we set the norm\nof all the entity embeddings as a constant L. Then,\nwe evaluate the model with different Lon the un-\nsupervised knowledge probe task and choose the\nbest Lfor those ﬁne-tuning tasks.\n2.3 Infuse Entity Knowledge into PLMs\nSince the entity embedding we obtained and the\noriginal word embedding are both obtained from\nthe masked language modeling objective, the entity\ncan be regarded as a special input token. To infuse\nentity knowledge into PLMs, we apply a pair of\nbracket to enclose the constructed entity embed-\nding and then insert it after the original entity’s\nsubwords. For example, the original input,\nMost people with COVID-19 have a dry\n[MASK] they can feel in their chest.\nbecomes\nMost people with COVID-19 (COVID-19) have\na dry [MASK] they can feel in their chest.\nHere, the entity COVID-19 adopts our constructed\nentity embedding and other words use their original\nembedding. We simply convey the modiﬁed input\nto the PLM for encoding without any additional\nstructures or parameters, to help the model predict\n[MASK] as cough.\nA note on entity links In previous section, we\nhypothesize that we know the entity linking annota-\ntions for the involved string name. In practice, we\ncan obtain the gold entity links provided by some\ndatasets like FewRel 1.0. For the datasets where the\nlinking annotations are not available, we employ a\nheuristic string matching for entity linking1.\n3 Experiment\n3.1 Implementation Details\nWe choose RoBERTaBase (Liu et al., 2019), a well-\noptimized PLM, as our baseline model and we\nequip it with our constructed entity embedding to\nobtain the PELT model. For the knowledge probe\ntask, we further experiment with another encoder-\narchitecture model, uncased BERT Base (Devlin\net al., 2019), and an encoder-decoder-architecture\nmodel, BARTBase (Lewis et al., 2020).\nWe adopt Wikipedia and biomedical S2ORC (Lo\net al., 2020) as the domain-speciﬁc corpora and\nsplit them into sentences with NLTK (Xue, 2011).\nFor Wikipedia, we adopt a heuristic entity link-\ning strategy with the help of hyperlink annota-\ntions. For the used FewRel 1.0 and Wiki80 datasets,\nwe directly use the annotated linking informa-\ntion. For other datasets, we link the given entity\nname through a simple string match. For each\nnecessary entity, we ﬁrst extract up to 256 sen-\ntences containing the entity from the corpora. We\nadopt Wikipedia as the domain-speciﬁc corpus for\nFewRel 1.0, Wiki80 and LAMA, and we adopt\nS2ORC as the domain-speciﬁc corpus for FewRel\n2.0. After that, we construct the entity embedding\naccording to Section 2.2.\nWe search the norm of entity embedding L\namong 1-10 on the knowledge probe task. We ﬁnd\nL = 7,10,3 performs a bit better for RoBERTa,\nBERT and BART respectively. In the ﬁne-tuning\nprocess, we freeze the constructed embeddings as\nan lookup table with the corresponding norm. After\nthat, we run all the ﬁne-tuning experiments with 5\ndifferent seeds and report the average score.\n3.2 Baselines\nWe select three of the most representative entity-\naware baselines, which adopt an external entity\nembedding, an entity-related pre-training task, or\na trainable entity embedding: (1) ERNIE (Zhang\net al., 2019) involves the entity embedding learned\nfrom Wikidata relation (Bordes et al., 2013). We\n1Details are shown in the Appendix.\n525\nModel Ext. Pretrain FewRel 1.0 FewRel 2.0\n5-1 5-5 10-1 10-5 5-1 5-5 10-1 10-5\nERNIE† ✓ 92.7±0.2 97.9±0.0 87.7±0.4 96.1±0.1 66.4±1.6 88.2±0.5 51.2±0.7 80.1±1.0\nKEPLER ✓ 90.8±0.1 96.9±0.1 85.1±0.1 94.2±0.1 74.0±1.0 89.2±0.2 61.7±0.1 82.1±0.1\nLUKE ✓ 91.8±0.4 97.5±0.1 85.3±0.4 95.3±0.1 64.8±1.4 89.2±0.2 46.6±0.8 80.5±0.5\nRoBERTa - 90.4±0.3 96.2±0.0 84.2±0.5 93.9±0.1 71.2±2.1 89.4±0.2 53.3±0.8 83.1±0.4\nPELT - 92.7±0.3 97.5±0.0 87.5±0.3 95.4±0.1 75.0±1.3 92.1±0.2 60.4±1.1 85.6±0.2\nTable 2: The accuracy on the FewRel dataset.N-Kindicates the N-way K-shot conﬁguration. Both of FewRel 1.0\nand FewRel 2.0 are trained on the Wikipedia domain, and FewRel 2.0 is tested on the biomedical domain. ERNIE†\nhas seen facts in the FewRel 1.0 test set during pre-training. We report standard deviations as subscripts.\nModel 1% 10% 100%\nERNIE 66.4±0.4 87.7±0.2 93.4±0.1\nKEPLER 62.3±1.0 85.4±0.2 91.7±0.1\nLUKE 63.1±1.0 86.9±0.4 92.9±0.1\nRoBERTa 59.8±1.7 85.7±0.2 91.7±0.1\nPELT 65.6±1.0 88.3±0.3 93.4±0.1\nTable 3: The accuracy on the test set of Wiki80.\n1%/ 10% indicate using 1%/ 10% supervised training\ndata respectively.\nadopt the RoBERTa version of ERNIE provided\nby Wang et al. (2021b); (2) KEPLER (Wang\net al., 2021b) encodes textual entity description\ninto entity embedding and learns fact triples and\nlanguage modeling simultaneously; (3)LUKE (Ya-\nmada et al., 2020) learns a trainable entity embed-\nding to help the model predict masked tokens and\nmasked entities in the sentences.\n3.3 Relation Classiﬁcation\nRelation Classiﬁcation (RC) aims to predict the\nrelationship between two entities in a given text.\nWe evaluate the models on two scenarios, the few-\nshot setting and the full-data setting.\nThe few-shot setting focuses on long-tail rela-\ntions without sufﬁcient training instances. We eval-\nuate models on FewRel 1.0 (Han et al., 2018) and\nFewRel 2.0 (Gao et al., 2019b). FewRel 1.0 con-\ntains instances with Wikidata facts and FewRel\n2.0 involves a biomedical-domain test set to ex-\namine the ability of domain adaptation. In the\nN-way K-shot setting, models are required to cat-\negorize the query as one of the existing N rela-\ntions, each of which contains K supporting sam-\nples. We choose the state-of-the-art few-shot frame-\nwork Proto (Snell et al., 2017) with different PLM\nencoders for evaluation. For the full-data setting,\nwe evaluate models on the Wiki80, which contains\n80 relation types from Wikidata. We also add 1%\nand 10% settings, meaning using only 1% / 10%\nModel LAMA LAMA-UHN\nG-RE T-REx G-RE T-REx\nERNIE 10.0 24.9 5.9 19.4\nKEPLER 5.5 23.4 2.5 15.4\nLUKE 3.8 32.0 2.0 25.3\nRoBERTa 5.4 24.7 2.2 17.0\nPELT 6.4 27.5 2.8 19.3\nBERT 13.9 34.9 8.8 26.8\nBERT-PELT 13.3 40.7 8.9 34.5\nBART 5.1 15.9 1.3 12.0\nBART-PELT 6.9 24.4 2.1 14.9\nTable 4: Mean P@1 on the knowledge probe bench-\nmark. G-RE: Google-RE.\ndata of the training sets.\nAs shown in Table 2 and Table 3, on FewRel\n1.0 and Wiki80 in Wikipedia domain, RoBERTa\nwith PELT beats the RoBERTa model by a large\nmargin (e.g. +3.3% on 10way-1shot), and it even\nachieves comparable performance with ERNIE,\nwhich has access to the knowledge graph. Our\nmodel also gains huge improvements on FewRel\n2.0 in the biomedical domain (e.g. + 7.1% on\n10way-1shot), while the entity-aware baselines\nhave little advance in most settings. Compared with\nmost existing entity-aware PLMs which merely ob-\ntain domain-speciﬁc knowledge in the pre-training\nphase, our proposed pluggable entity lookup table\ncan dynamically update the models’ knowledge\nfrom the out-of-domain corpus on demand.\n3.4 Knowledge Probe\nWe conduct experiments on a widely-used knowl-\nedge probe dataset, LAMA (Petroni et al., 2019).\nIt applies cloze-style questions to examine PLMs’\nability on recalling facts from their parameters. For\nexample, given a question templateParis is the cap-\nital of [MASK], PLMs are required to predict the\nmasked token properly. In this paper, we not only\n526\nModel [0,10) [10,50) [50,100) [100,+)\nRoBERTa 18.1 21.1 25.8 26.1\nPELT 21.9 24.8 29.0 28.7\nTable 5: Mean P@1 on T-Rex with respect to the sub-\nject entity’s frequency in Wikipedia.\nuse Gooogle-RE and T-REx (ElSahar et al., 2018)\nwhich focus on factual knowledge, but also evalu-\nate models on LAMA-UHN (Pörner et al., 2020)\nwhich ﬁlters out the easy questionable templates.\nAs shown in Table 4, without any pre-training,\nthe PELT model can directly absorb the entity\nknowledge from the extended input sequence to\nrecall more factual knowledge, which demonstrates\nthat the entity embeddings we constructed are com-\npatible with original word embeddings. We also\nﬁnd that our method can also bring huge improve-\nments to both BERT and BART in the knowledge\nprobe task, which proves our method’s generaliza-\ntion on different-architecture PLMs.\nEffect of Entity Frequency Table 5 shows the\nP@1 results with respect to the entity frequency.\nWhile RoBERTa performs worse on rare entities\nthan frequent entities, PELT brings a substantial\nimprovement on rare entities, i.e., near 3.8 mean\nP@1 gains on entities that occur less than 50 times.\n4 Conclusion\nIn this paper, we propose PELT, a ﬂexible entity\nlookup table, to incorporate up-to-date knowledge\ninto PLMs. By constructing entity embeddings on\ndemand, PLMs with PELT can recall rich factual\nknowledge to help downstream tasks.\nAcknowledgement\nThis work is supported by the National Key R&D\nProgram of China (No. 2020AAA0106502), Insti-\ntute Guo Qiang at Tsinghua University, and Inter-\nnational Innovation Center of Tsinghua University,\nShanghai, China. We thank Zhengyan Zhang and\nother members of THUNLP for their helpful dis-\ncussion and feedback. Deming Ye conducted the\nexperiments. Deming Ye, Yankai Lin, Xiaojun Xie\nand Peng Li wrote the paper. Maosong Sun and\nZhiyuan Liu provided valuable advices to the re-\nsearch.\nReferences\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E.\nHinton. 2016. Layer normalization. CoRR,\nabs/1607.06450.\nAntoine Bordes, Nicolas Usunier, Alberto García-\nDurán, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013,\nLake Tahoe, Nevada, United States , pages 2787–\n2795.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nHady ElSahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon S. Hare, Frédérique\nLaforest, and Elena Simperl. 2018. T-rex: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Inter-\nnational Conference on Language Resources and\nEvaluation, LREC 2018, Miyazaki, Japan, May 7-\n12, 2018 . European Language Resources Associa-\ntion (ELRA).\nThibault Févry, Livio Baldini Soares, Nicholas FitzGer-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as experts: Sparse memory access with entity\nsupervision. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP) , pages 4937–4951, Online. Associa-\ntion for Computational Linguistics.\nJun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-\nYan Liu. 2019a. Representation degeneration prob-\nlem in training natural language generation models.\nIn 7th International Conference on Learning Repre-\nsentations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019. OpenReview.net.\nTianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng Li,\nMaosong Sun, and Jie Zhou. 2019b. FewRel 2.0:\nTowards more challenging few-shot relation classiﬁ-\ncation. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n6250–6255, Hong Kong, China. Association for\nComputational Linguistics.\nXu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan\nYao, Zhiyuan Liu, and Maosong Sun. 2018. FewRel:\n527\nA large-scale supervised few-shot relation classiﬁca-\ntion dataset with state-of-the-art evaluation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 4803–\n4809, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nLingpeng Kong, Cyprien de Masson d’Autume, Lei\nYu, Wang Ling, Zihang Dai, and Dani Yogatama.\n2020. A mutual information maximization perspec-\ntive of language representation learning. In 8th\nInternational Conference on Learning Representa-\ntions, ICLR 2020, Addis Ababa, Ethiopia, April 26-\n30, 2020. OpenReview.net.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2020, Online, July 5-10, 2020 ,\npages 7871–7880. Association for Computational\nLinguistics.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang,\nQi Ju, Haotang Deng, and Ping Wang. 2020. K-\nBERT: enabling language representation with knowl-\nedge graph. In The Thirty-Fourth AAAI Conference\non Artiﬁcial Intelligence, AAAI 2020, The Thirty-\nSecond Innovative Applications of Artiﬁcial Intelli-\ngence Conference, IAAI 2020, The Tenth AAAI Sym-\nposium on Educational Advances in Artiﬁcial Intel-\nligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 2901–2908. AAAI Press.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel Weld. 2020. S2ORC: The semantic\nscholar open research corpus. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4969–4983, Online. As-\nsociation for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A.\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 43–54, Hong Kong, China. Associ-\nation for Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nNina Pörner, Ulli Waltinger, and Hinrich Schütze. 2020.\nE-BERT: efﬁcient-yet-effective entity embeddings\nfor BERT. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, Online Event,\n16-20 November 2020 , volume EMNLP 2020 of\nFindings of ACL , pages 803–818. Association for\nComputational Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the pa-\nrameters of a language model? In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020 , pages 5418–5426. Associ-\nation for Computational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow BERT works. Trans. Assoc. Comput. Linguis-\ntics, 8:842–866.\nJake Snell, Kevin Swersky, and Richard S. Zemel.\n2017. Prototypical networks for few-shot learning.\nIn Advances in Neural Information Processing Sys-\ntems 30: Annual Conference on Neural Informa-\ntion Processing Systems 2017, December 4-9, 2017,\nLong Beach, CA, USA, pages 4077–4087.\nTianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo,\nYaru Hu, Xuanjing Huang, and Zheng Zhang. 2020.\nCoLAKE: Contextualized language and knowledge\nembedding. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics ,\npages 3660–3670, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019. What do you\nlearn from context? probing for sentence structure\nin contextualized word representations. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net.\nCunxiang Wang, Pai Liu, and Yue Zhang. 2021a. Can\ngenerative pre-trained language models serve as\nknowledge bases for closed-book qa? In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, ACL/IJCNLP 2021, (Volume 1: Long Pa-\npers), Virtual Event, August 1-6, 2021, pages 3241–\n3251. Association for Computational Linguistics.\n528\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xu-\nanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang,\nand Ming Zhou. 2020. K-adapter: Infusing knowl-\nedge into pre-trained models with adapters. CoRR,\nabs/2002.01808.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang.\n2021b. KEPLER: A uniﬁed model for knowledge\nembedding and pre-trained language representation.\nTrans. Assoc. Comput. Linguistics, 9:176–194.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nNianwen Xue. 2011. Steven bird, evan klein and\nedward loper. Natural Language Processing with\nPython. o’reilly media, inc 2009. ISBN: 978-0-596-\n51649-9. Nat. Lang. Eng., 17(3):419–424.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. LUKE: Deep\ncontextualized entity representations with entity-\naware self-attention. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 6442–6454, On-\nline. Association for Computational Linguistics.\nIkuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and\nYoshiyasu Takefuji. 2016. Joint learning of the em-\nbedding of words and entities for named entity dis-\nambiguation. In Proceedings of the 20th SIGNLL\nConference on Computational Natural Language\nLearning, CoNLL 2016, Berlin, Germany, August\n11-12, 2016, pages 250–259. ACL.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1441–1451, Florence, Italy. Association\nfor Computational Linguistics.\nA Heuristic String Matching for Entity\nLinking\nFor the Wikipedia, we ﬁrst create a mapping from\nthe anchor texts with hyperlinks to their referent\nWikipedia pages. After that, We employ a heuristic\nstring matching to link other potential entities to\ntheir pages.\nFor preparation, we collect the aliases of the\nentity from the redirect page of Wikipedia and the\nrelation between entities from the hyperlink. Then,\nwe apply spaCy 2 to recognize the entity name in\nthe text. An entity name in the text may refer to\n2https://spacy.io/\nmultiple entities of the same alias. We utilize the\nrelation of the linked entity page to maintain an\navailable entity page set for entity disambiguation .\nAlgorithm 1 Heuristic string matching for entity\ndisambiguation\nS ⇐{ the linked entity page in anchor text}\nE ⇐{ potential entity name in text}\nrepeat\nS′ ⇐{ the neighbor entity pages that have\nhyperlink or Wikidata relation with pages in\nS}\nE′⇐{e|e∈Eand ecan be uniquely linked\nto entity page in S′by string matching }\nE ⇐E−E′\nS ⇐E′\nuntil S = φ\nDetails of the heuristic string matching are\nshown in Algorithm 1, we match the entity name to\nsurrounding entity page of the current page as close\nas possible. e will release all the source code and\nmodels with the pre-processed Wikipedia dataset.\nFor other datases, we adopt a simple string\nmatching for entity linking.\nB Training Conﬁguration\nWe train all the models with Adam opti-\nmizer (Kingma and Ba, 2015), 10% warming up\nsteps and maximum 128 input tokens. Detailed\ntraining hyper-parameters are shown in Table 6.\nWe run all the experiments with 5 different seeds\n(42, 43, 44, 45, 46) and report the average score\nwith the standard deviation. In the 1% and 10% set-\ntings’ experiments for Wiki80, we train the model\nwith 10-25 times epochs as that of the 100% set-\nting’s experiment.\nFor FewRel, we search the batch size among\n[4,8,32] and search the training step in [1500, 2000,\n2500]. We evaluate models every 250 on validation\nand save the model with best performance for test-\ning. With our hyper-parameter tuning, the results\nof baselines in FewRel signiﬁcantly outperforms\nthat reported by KEPLER (Wang et al., 2021b).\nDataset Epoch Train Step BSZ LR\nWiki80 5 - 32 3e-5\nFewRel 1.0 - 1500 32 2e-5\nFewRel 2.0 - 1500 32 2e-5\nTable 6: Training Hyper-parameters. BSZ: Batch size;\nLR: Learning rate.\n529",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8566528558731079
    },
    {
      "name": "Table (database)",
      "score": 0.6949944496154785
    },
    {
      "name": "Natural language processing",
      "score": 0.5624293088912964
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5280754566192627
    },
    {
      "name": "Domain knowledge",
      "score": 0.5150980353355408
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.5005810260772705
    },
    {
      "name": "Code (set theory)",
      "score": 0.48467567563056946
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.47834083437919617
    },
    {
      "name": "Domain adaptation",
      "score": 0.4775746166706085
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4741477370262146
    },
    {
      "name": "Language model",
      "score": 0.46730470657348633
    },
    {
      "name": "Encoding (memory)",
      "score": 0.44660714268684387
    },
    {
      "name": "Recall",
      "score": 0.436808317899704
    },
    {
      "name": "Lookup table",
      "score": 0.4354058504104614
    },
    {
      "name": "Programming language",
      "score": 0.25616562366485596
    },
    {
      "name": "Database",
      "score": 0.2233646810054779
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I29955533",
      "name": "Center for Information Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210101332",
      "name": "Group Image (Poland)",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I4210157323",
      "name": "South China Institute of Collaborative Innovation",
      "country": "CN"
    }
  ]
}