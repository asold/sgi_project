{
  "title": "Language models and Automated Essay Scoring",
  "url": "https://openalex.org/W2974340959",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4302756725",
      "name": "Rodriguez, Pedro Uria",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227069002",
      "name": "Jafari, Amir",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287522929",
      "name": "Ormerod, Christopher M.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3213767167",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W1590881479",
    "https://openalex.org/W2293665254",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2001744998",
    "https://openalex.org/W2567547739",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2798812533",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W3098654368",
    "https://openalex.org/W2149933564",
    "https://openalex.org/W1670421514",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W1847088711",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2250653455",
    "https://openalex.org/W3034730645",
    "https://openalex.org/W2053154970",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2155482699",
    "https://openalex.org/W2025719462",
    "https://openalex.org/W613277099",
    "https://openalex.org/W2945824677",
    "https://openalex.org/W87708496",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2883720816",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2964110616"
  ],
  "abstract": "In this paper, we present a new comparative study on automatic essay scoring (AES). The current state-of-the-art natural language processing (NLP) neural network architectures are used in this work to achieve above human-level accuracy on the publicly available Kaggle AES dataset. We compare two powerful language models, BERT and XLNet, and describe all the layers and network architectures in these models. We elucidate the network architectures of BERT and XLNet using clear notation and diagrams and explain the advantages of transformer architectures over traditional recurrent neural network architectures. Linear algebra notation is used to clarify the functions of transformers and attention mechanisms. We compare the results with more traditional methods, such as bag of words (BOW) and long short term memory (LSTM) networks.",
  "full_text": "arXiv:1909.09482v1  [cs.CL]  18 Sep 2019\nLanguage models and Automated Essay Scoring\nPedro Uria Rodriguez, Amir Jafari and Christopher M. Ormero d\nAbstract. In this paper, we present a new comparative study on automati c essay scoring (AES). The\ncurrent state-of-the-art natural language processing (NL P) neural network architectures are used in this\nwork to achieve above human-level accuracy on the publicly available Kaggle AES dataset. We compare two\npowerful language models, BERT and XLNet, and describe all t he layers and network architectures in these\nmodels. We elucidate the network architectures of BERT and XLNet using clear notation and diagrams and\nexplain the advantages of transformer architectures over t raditional recurrent neural network architectures.\nLinear algebra notation is used to clarify the functions of t ransformers and attention mechanisms. We\ncompare the results with more traditional methods, such as b ag of words (BOW) and long short term\nmemory (LSTM) networks.\n1. Introduction\nAutomated essay scoring (AES) is the use of some statistical mode l to assign grades to essays in an\neducational setting. These engines were initially used to reduce the cost of essay scoring [ 1, 2]. Aside from\ncost eﬀectiveness, AES is considered to be inherently more consist ent and less biased than human raters.\nWe can compare the performance of an AES engine with the perform ance of human raters using inter-rater\nreliability (IRR) statistics [ 3]. Recently, an AES engine with above human performance was prese nted in\n[4] based on an engine in which experts carefully engineered a set of fe atures. AES has been a subject of a\nnumber of recent works by Sakaguchi [ 5], Shermis and Hammer [ 6], and Yannakoudakis [ 7].\nThe function of AES is essentially one of classiﬁcation, where neural networks are associated with almost\nall the current state-of-the-art results. Feedforward (stat ic) neural networks are a class of powerful nonlinear\nstatistical models capable of modelling complex relationships between the input space and a set targets [ 8].\nMany of these Feedforward neural networks are known as Convo lutional Neural Networks (CNN)’s which\nare ubiquitously used in image classiﬁcation tasks [ 9]. These nonlinear models are ﬁt to a set of training\ndata using backpropagation and a variety of optimization algorithms . Recently very eﬃcient deep neural\nnet model architectures have been used to compute the vector r epresentation of words and/or subwords\ncalled embeddings [ 10]. These models are used heavily in Natural Language Processing (NL P) tasks to\nconvert words and/or subwords to vectors in a meaningful manne r that has been shown to preserve semantic\ninformation.\nWe also consider AES to be an area of NLP in which another type of dyn amic network is ubiquitously\nused. These dynamic networks are mostly called Recurrent Neural Networks (RNN)’s and are powerful tools\nused to model and classify data that is sequential in nature. These types of networks have been used in\nengineering and science in the identiﬁcation and modeling of complex sy stems [ 11]. Using an embedding we\nmay convert a sequence of words into a sequence of vectors that has preserved the semantic information.\nRNN’s, in combination with embeddings, have many applications in NLP ta sks like sentiment analysis, topic\nlabeling, language detection and machine translation [ 12]. In recent years, researchers have applied RNNs\nand Deep Neural Nets to AES.\nIn cases where there are a very large number of student essays, grading can be a very expensive and\ntime consuming process. Since scoring essays is a part of the stude nt assessment process that is conducted\n2010 Mathematics Subject Classiﬁcation. Primary .\nKey words and phrases. Neural Network, Natural Language Processing, RNN, Deep Lea rning, Transformers, Feature\nEngineering.\n1\n2 PEDRO URIA RODRIGUEZ, AMIR JAFARI AND CHRISTOPHER M. ORMER OD\nby almost all educational testing agencies, there are many AES eng ines being used in large-scale formative\nand summative assessment [ 6]. The core idea of essay scoring is to evaluate an essay with respect to a rubric\nwhich may depend on traits such as the use of grammar, the organiz ation of the essay in addition to topic\nspeciﬁc information. An AES engine seeks to extract measurable fe atures which may be used to approximate\nthese traits, hence, deduce a probable score based on statistica l inference. A comprehensive review of AES\nengines in production featured in the work of Shermis et al. [ 6].\nIn 2012, Kaggle released a Hewlett Foundation sponsored competit ion under the name “Automated\nStudent Assessment Prize” (ASAP). Competitors designed and de veloped statistical AES engines based on\ntechniques like Bag of Words (BOW) in combination with standard Mach ine Learning (ML) algorithms to\nextract important features of student responses that correla ted well with scores. Subsequent works applied\nRNN-based engines in combination with word embeddings to the Kaggle AES dataset [ 13]. This dataset and\nthese results provide us with a benchmark for AES engines and a way of comparing current state-of-the-art\nneural network architectures against previous results.\nSince there exists an abundance of unlabeled text data available, re searchers have started training very\ndeep language models, which are networks designed to predict some part of the text (usually words) based on\nthe other parts. These networks eventually learn contextual inf ormation. By adapting these language models\nto predict labels instead of words or sentences, state-of-the-a rt results have been achieved in many NLP tasks.\nMany of these models (see [ 14, 15, 16, 17]) are built from layers of Transformers which utilize attention to\nﬁnd the most relevant features to perform a particular task [ 17]. We concentrate on two such models; the\nBidirectional Encoder Representations from Transformers (BER T), introduced in [ 15], and XLNet, which is\na variation of the BERT model [ 16].\n2. Automated Essay Scoring\nIn this section, we discuss the task of producing an AES engine. This includes the data collection, how\nwe train the models and how we evaluate an AES engine. We include a brie f description of some of the\nstandard IRR statistics in the literature used in the context of eva luating models [ 3, 18].\nThe ﬁrst step in producing an AES engine is data collection. Typically, a large sample of essays is\ncollected for the task and scored by expert raters. The raters a re trained using a holistic rubric specifying the\ncriteria each essay is required to satisfy to be awarded each score . Exemplar essays are used to demonstrate\nhow the criteria is to be applied. A holistic rubric may take into account a number of factors such as grammar,\nspelling, organization, clarity and cohesion [ 19]. Since these essays are the result of speciﬁc prompts shown\nto students, the rubric may include prompt speciﬁc information. Th e training material for the Kaggle AES\ndataset was made publicly available. To evaluate the eﬃcacy of an AES engine, we require that every essay is\nscored by (at least) two diﬀerent raters. Other quality control m echanisms like resolution reads and targeted\nbackreads help improve the quality of the data [ 18].\nOnce the collection of essays is scored, we divide the essays into thr ee diﬀerent sets; a training set, a test\nset and a validation set. From a classiﬁcation standpoint, the input s pace is the set of raw text essays while\nthe targets for this problem are the human assigned labels. The goa l of an AES engine use and evaluate a set\nof features of the training set, either implicitly or explicitly, in a manne r that the labels of the test set may\nbe deduced as accurately as possible using statistical inference. U ltimately, if the features are appropriate\nand the statistical inference is valid, the AES engine assigns grades to essays statistically similarly to how a\nhuman would on the test set. Once the hyperparameters are optim ized for the test set, the engine is applied\non the validation set.\nIn the case of the ASAP data, two raters were used to evaluate th e essays. We call the scores of one\nreader the initial scores and the scores of the second reader the reliability scores. There are two main metrics\nused to evaluate the agreement between two sets of scores; the exact agreement (accuracy), which measures\nwhen two scores agree, and the quadratic weighted kappa (QWK) s tatistic [ 20] or Cohen’s Kappa Score.\nThe QWK of two sets of scores is deﬁned as follows:\nLANGUAGE MODELS AND AUTOMATED ESSAY SCORING 3\nK = 1 −\n∑\ni,j wi,jxi,j\n∑\ni,j mi,jxi,j\n(1)\nmi,j = xi,j(1 − xi,j)(2)\nwi,j = 1 − (i − j)2\n(k − 1)2(3)\nwhere k is the number of classes and xi,j is the probability of score i receiving score j. The original Cohen’s\nKappa Score is deﬁned as\n(4) K = po − pe\n1 − pe\nwhere po is the relative observed exact agreement among raters (i.e., accur acy), and pe is the hypothetical\nprobability of chance agreement, using the observed data to calcu late the probabilities of each observer\nrandomly seeing each category [ 20]. The QWK has the property that K = 1 if the raters are in complete\nagreement. The QWK captures the level of agreement above and b eyond what would be obtained by chance\nand weighted by the extent of disagreement. Furthermore, in con trast to the accuracy, QWK is statistically\na better measurement for detecting disagreements between rat ers since it depends on the entire confusion\nmatrix, not just the diagonal entries. Typically, the QWK between t wo raters is also used to measure the\nquality or subjectivity of the data used in training.\nWe may evaluate an AES engine on the ASAP dataset and compare the engine with a human rater by\ntraining an engine on the initial scores and showing that the scores p redicted by the engine are in greater\nagreement with the intial scores than the reliability scores on the va lidation set. We used the same 5-fold\ncross validation splits found in [ 13] where each of the ﬁve splits used 60 percent of the data as trainin g data,\n20 percent as a test set and 20 percent as a validation set. We also c onsidered hyperparameter tuning at a\nlevel in which the very structure of the network was altered.\nAutomated Essay Scoring is one of the more challenging tasks in NLP. The challenges that are somewhat\ndistinct to essay scoring relate to the length of essays, the quality of the language/spelling and typical\ntraining sample sizes. Essays can be long relative to the texts found in sentiment analysis, short answer\nscoring, language detection and machine translation. Furthermor e, while many tasks in NLP can be done\nsentence by sentence, the length and structure of essays ofte n introduces longer time dependencies which\nrequires more data than typically available. The amount of data is oft en restricted due to the expense of\nhand-scoring. The longer the essay, the more diﬃcult for Neural N etwork models to keep the information\nfrom beginning of the essay in the network. This results in converge nce issues or low performance. These\nare in addition to typical challenges of NLP such as the choice of embe dding, diﬀerent contextual meanings\nof words and the choice of ML algorithms.\nA variety of models have been introduced over the last 50 years in es say scoring [ 21]. These models\nstarted with statistical models using the Bag of Words (BOW) metho d with logistic regression or other\nclassiﬁers, SVD methods for feature selection and probabilistic mod els like Naive Bayes or Gaussian models.\nIn using Neural Network models, we are required to choose an appr opriate embedding [ 22]. An embedding\nmay be between characters, words, subwords or sentences into some real n-dimensional space that some-\nhow preserves the usage/semantics [ 22]. This converts a text into a sequence of vectors in n-dimensional\nspace which may be modeled using RNNs like LSTM Networks[ 23] and GRU Networks [ 12] or with CNNs.\nThe gating mechanisms, such as those found in GRUs and LSTM units, mitigate the issue of long term\ndependencies to some degree, however, it has been shown that lon g term dependencies are more eﬀectively\naccommodated for by attention mechanisms [ 15]. Recently people have started to combine these algorithms\nwith each other in order to improve the results.\nAt a word level, if a word is misrepresented or misspelled the embedding of that token results in an\ninconsistent input that is being used to train the NN models leading to p oor extrapolation. Standard\nalgorithms for correcting words may suggest words that do not ﬁt into the context. The language models\nin question are masked word models [ 14, 15, 16, 17] which seeks to guess a selection of missing words\nbetter than standard algorithms by incorporating context in thre e diﬀerent ways. These models use three\ndiﬀerent embeddings; a word/subword embedding, a sentence emb edding and a positional embedding that\nencodes the position of each word. The probably masked words are calculated by using context at a word\n4 PEDRO URIA RODRIGUEZ, AMIR JAFARI AND CHRISTOPHER M. ORMER OD\nand sentence level. By modelling sentences, these models possess m uch more information than typically\navailable using typical word embeddings.\nNeural networks are inherently non-linear and continuous models, however, to approximate a discrete\nscoring rubric, a series of boundaries is introduced in the output sp ace that distinguish the various scores.\nWhen the output lies close to the boundaries between scores it is diﬃc ult for the models to pick a score\ncorrectly. Ideas of committee (or ensemble) of networks by takin g a majority vote or the mean will be\ndiscussed in later sections.\n3. Models\nIn this section we will go into some detail regarding some of the major methods used develop AES\nengines [ 6]. We start with the BOW method in which the features are explicitly deﬁ ned. We then go on\nto describe RNN approaches. In particular, we will review how the ga ting mechanism in layers of LSTM\nunits allow for long term dependencies. The Multi Layer Perceptron and its variations are classiﬁed as static\nnetwork and networks that have delays are also considered RNNs. Lastly, we elucidate the structure and\nfunction of the language models featured in this paper.\n3.1. BOW. For ML algorithms, we mostly prefer to have well deﬁned ﬁxed input a nd targets. An issue\nwith modeling text data is that it is usually very messy and some techniq ues are required to pre-process it\ninto useful inputs and targets to feed to ML algorithms. Texts nee ds to be converted to numbers that we\ncan use in machine learning as proper input and labels. Converting tex tual data to vectors is called feature\nextraction or feature encoding. A bag of words (BOW) model is a te chnique to extract features from text\nand use them for modeling. The method is very simple:\n(1) Find all occurrence of words within a document.\n(2) Find a unique vocabulary of words.\n(3) Then form the vector that represents the frequency of eac h word.\n(4) Each dimension of the vector represents the number of count s (occurrence).\n(5) Remove dimensions associated with very high frequency words.\n(6) We use term frequency (TF) (take the raw frequency and divid e to max frequency).\n(7) We use inverse document frequency (IDF) (log of documents c ounts to the length of all the docu-\nments has has the term)\n(8) By multiplying the TF and IDF, we get (TF-IDF) to reduce the mos t important words.\n(9) Normalize the TF-IDF vectors.\nThe BOW model is completed and each essay is associated with a single v ector and the set of vectors\nwith a particular label may be classiﬁed by some traditional classiﬁer. We should note that the BOW model\nwill not consider the order of the words and that in each bag it ﬁnds t he words that have the most textual\ninformation.\n3.2. LSTM. The output of an RNN is a sequence that depends on the current inp ut to the network\nbut also on the previous inputs and outputs. In other words, the in put and output can be delayed and we\ncan also use the state of the network as input. Since these networ ks have delays, they operate on a sequence\nof inputs in which the order is important.\nAn RNN can be a purely Feed Forward network with delays in the inputs or they can have feedback\nconnections with the output of the network and/or the state of t he network. A variety of recurrent units,\nwhich are used to build RNNs, are available like LSTM [ 23], GRU[ 25], ELMAN, NARX [ 26] and Focused\nDelay Networks. In this section we are going to discuss networks of LSTM units. In order to do so, let\nus introduce a general notation by describing the most basic unit th at makes up all ANNs, that is, the\n(artiﬁcial) neuron, shown in Figure 1(a).\nA scalar input p is multiplied by a parameter w, called weight, and the result is added to another\nparameter b called bias. Their sum ( n, the net input ) goes into a (usually) nonlinear activation function\nf(x) to get the neuron output a. By updating the values of w and b through an iterative optimization\nalgorithm called Gradient Descent , this single neuron can ﬁnd the best parameters that ﬁt the neuro n\nequation (with a set transfer function) to any two-dimensional da ta. In other words, this single modular\nunit can map input data to the target and approximate the underlyin g function. By assigning a diﬀerent\nLANGUAGE MODELS AND AUTOMATED ESSAY SCORING 5\n Neuron Model and Network Architectures \n2-16 \nSummary of Results \nSingle-Input Neuron \nMultiple-Input Neuron \nD\u0003 \u0003I\u0003\u000bZS \u0003\u000e\u0003E\f\n*HQHUDO\u00031HXURQ \nDQ\n,QSXWV \n/LiteDiagLines/LiteDiagLines\n/LiteDiagLines/LiteDiagLines\nE\nS Z\n\u0014\n/LiteDiagLines/LiteDiagLines\n/LiteDiagLines/LiteDiagLines Σ I\n0XOWLSOH\u0010,QSXW\u00031HXURQ \nS\u0014\nDQ\n,QSXWV \nE\nS\u0015\nS\u0016\nS5\nZ\u0014\u000f\u00035\nZ\u0014\u000f\u0003\u0014\n\u0014\n/LiteDiagLines/LiteDiagLines\n/LiteDiagLines/LiteDiagLines\nD\u0003 \u0003I\u0003\u000b:S \u0003\u000e\u0003E\f\n/LiteDiagLines/LiteDiagLines\n/LiteDiagLines/LiteDiagLines I\n/LiteDiagLines/LiteDiagLines\n/LiteDiagLines/LiteDiagLines\n/LiteDiagLines/LiteDiagLines\n/LiteDiagLines/LiteDiagLines\nI\n0XOWLSOH\u0010,QSXW\u00031HXURQ \nD\u0003 \u0003I\u0003\u000b:S \u0003\u000e\u0003E\f\nS D\n\u0014\nQ/LiteDiagLines/LiteDiagLines\n/LiteDiagLines/LiteDiagLines\n:\n/LiteDiagLines/LiteDiagLines\n/LiteDiagLines/LiteDiagLines E\n5\u0003[\u0003\u0014\n\u0014\u0003[\u00035\n\u0014\u0003[\u0003\u0014\n\u0014\u0003[\u0003\u0014\n\u0014\u0003[\u0003\u0014\n,QSXW \n5 \u0014\n(a) Basic unit\nNetwork Architectures \n2-11 \nthese layers. We will use superscripts to identify the layers. Specifically, we \nappend the number of the layer as a superscript  to the names for each of \nthese variables. Thus, the weight matrix for the first layer is written as , \nand the weight matrix for the second layer is written as . This notation \nis used in the three-layer network shown in Figure 2.9\nFigure 2.9  Three-Layer Network \nAs shown, there are  inputs,  neurons in the first layer,  neurons in \nthe second layer, etc. Asnoted, different layers can have different numbers \nof neurons. \nThe outputs of layers one and two are the inputs for layers two and three. \nlayer 2 can be viewed as a one-layer network with  =  inputs, \n neurons, and an  weight matrix . The input to laye r 2 is \n, and the output is . \nA layer whose output is the network output is called an output layer . The \nother layers are called hi dden layers . The network shown above has an out\nput layer (layer 3) and two hidden layers (layers 1 and 2). \nThe same three-layer network discussed previously also can be drawn us\ning our abbreviated notation, as shown in Figure 2.10\nLayer Superscript \n)LUVW\u0003/D\\HU \nD\u0014\u0003 \u0003I\u0003 \u0014\u0003\u000b:\u0014S\u0003\u000e\u0003E\u0014\f D\u0015\u0003 \u0003I\u0003 \u0015\u0003\u000b:\u0015D\u0014\u0003\u000e\u0003E\u0015\f D\u0016\u0003 \u0003I\u0003 \u0016\u0003\u000b:\u0016D\u0015\u0003\u000e\u0003E\u0016\f\n/LiteDiagLines\n/LiteDiagLines I\u0003 \u0014\n/LiteDiagLines\n/LiteDiagLines I\u0003 \u0015\n/LiteDiagLines\n/LiteDiagLines I\u0003 \u0016\n,QSXWV \nD\u0016\u0015Q\u0016\u0015\nZ\u0003 \u0016\n6\u0003 \n\u0016\n\u000f\u00036\u0003\n\u0015\nZ\u0003 \u0016\u0014\u000f\u0014\nE\u0016\n\u0015\nE\u0016\n\u0014\nE\u00166\u0003 \n\u0016\nD\u0016\n6\u0003 \n\u0016Q\u0016\n6\u0003 \n\u0016\nD\u0016\u0014Q\u0016\u0014\n\u0014\n\u0014\n\u0014\n\u0014\n\u0014\n\u0014\n\u0014\n\u0014\n\u0014\nS\u0014\nD\u0014\u0015Q\u0014\u0015\nS\u0015\nS\u0016\nS5\nZ\u0003 \u00146\u0003 \u0014\u000f\u00035\nZ\u0003 \u0014\u0014\u000f\u0014\nD\u0014\n6\u0003 \n\u0014Q\u0014\n6\u0003 \n\u0014\nD\u0014\u0014Q\u0014\u0014\nD\u0015\u0015Q\u0015\u0015\nZ\u0003 \u0015\n6\u0003 \n\u0015\n\u000f\u00036\u0003\n\u0014\nZ\u0003 \u0015\u0014\u000f\u0014\nE\u0014\n\u0015\nE\u0014\n\u0014\nE\u00146\u0003 \n\u0014\nE\u0015\n\u0015\nE\u0015\n\u0014\nE\u00156\u0003 \n\u0015\nD\u0015\n6\u0003 \n\u0015Q\u0015\n6\u0003 \n\u0015\nD\u0015\u0014Q\u0015\u0014\n/LiteDiagLines/LiteDiagLines \n/LiteDiagLines/LiteDiagLines Σ\n/LiteDiagLines/LiteDiagLines \n/LiteDiagLines/LiteDiagLines Σ\n/LiteDiagLines/LiteDiagLines \n/LiteDiagLines/LiteDiagLines Σ\n/LiteDiagLines/LiteDiagLines \n/LiteDiagLines/LiteDiagLines Σ\n/LiteDiagLines/LiteDiagLines \n/LiteDiagLines/LiteDiagLines Σ\n/LiteDiagLines/LiteDiagLines \n/LiteDiagLines/LiteDiagLines Σ\n/LiteDiagLines/LiteDiagLines \n/LiteDiagLines/LiteDiagLines Σ\n/LiteDiagLines/LiteDiagLines \n/LiteDiagLines/LiteDiagLines Σ\n/LiteDiagLines/LiteDiagLines \n/LiteDiagLines/LiteDiagLines Σ\n/LiteDiagLines\n/LiteDiagLines I\u0003 \u0014\n/LiteDiagLines\n/LiteDiagLines I\u0003 \u0014\n/LiteDiagLines\n/LiteDiagLines I\u0003 \u0015\n/LiteDiagLines\n/LiteDiagLines I\u0003 \u0015\n/LiteDiagLines\n/LiteDiagLines I\u0003 \u0016\n/LiteDiagLines\n/LiteDiagLines I\u0003 \u0016\nD\u0016\u0003 \u0003I\u0003 \u0016\u0003\u000b:\u0016I\u0003 \u0015\u0003\u000b:\u0015I\u0003 \u0014\u0003\u000b:\u0014S\u0003\u000e\u0003E\u0014\f\u0003\u000e\u0003E\u0015\f\u0003\u000e\u0003E\u0016\f\n7KLUG\u0003/D\\HU 6HFRQG\u0003/D\\HU \nR\nR\nS S u\nOutput Layer \nHidden Layers \n(b) MLP\nFigure 1. Artiﬁcial Neural Networks Structure [ 24]\nweight to each input dimension, a single neuron can be extended to mo del N-dimensional data. In this case,\nboth p and w are N-dimensional vectors, and the neuron output equation is\n(5) a = f(W · p + b).\nBy combining multiple neurons together, and stacking multiple layers o f these neurons, a Multi-Layer\nPerceptron (MLP) is formed 1(b). The super script number shows the layer numbers. For example, the\nforward calculation of the three layers shown in the ﬁgure is\na1 = f1(W1p + b1),(6)\na2 = f2(W2a1 + b2),\na3 = f3(W3a2 + b3).\nWe want to introduce the neural network framework that we will us e to represent general recurrent\nnetworks. We added new notation that we have used to represent MLP, therefore we can conveniently\nrepresent networks with feedback connections and tapped delay lines.\nThe net input nm(k) for layer m of an RNN can be computed as follows:\nnm(k) =\n∑\nl∈Lf\nm\n∑\nd∈DLm,l\nL Wm,l(d)al(k − d)\n(7) +\n∑\nl∈Im\n∑\nd∈DIm,l\nIWm,l(d)pl(k − d) + bm,\nwhere pl(k) is the l-th input to the network at time k, IWm,l is the input weight between input l and layer\nm, L Wm,l is the layer weight between layer l and layer m, bm is the bias vector for layer m, DLm,l is the\nset of all delays in the tapped delay line between layer l and layer m, Im is the set of indices of input vectors\nthat connect to layer m, and Lf\nm is the set of indices of layers that connect directly forward to layer m. The\noutput of layer m is\n(8) am(k) = fm(nm(k)),\nfor m = 1 , 2, · · · , M , where fm is the transfer function at layer m. The set of M paired equations (7) and\n(8) describes the general RNN. RNN can have any number of layers , any number of neurons in any layer,\nand arbitrary connections between layers (as long as there are no zero-delay loops) [ 24].\nTraining RNN networks can be very complex and diﬃcult. The key issue s that may arise are Vanishing\nGradients [ 23], Exploding Gradient and instability [ 27]. Many architectures are proposed to deal with\nthese issues. Long Short Term Memory (LSTM) network is one of th ese network architectures [ 23] that\n6 PEDRO URIA RODRIGUEZ, AMIR JAFARI AND CHRISTOPHER M. ORMER OD\nD\n1\nA A A\nA A ALW5,1\nA A\nA A\nA A\nb1\nA A A\nA A A\nInput\nA A \nA A \nA A \nA A \n1 b1\nA A A\nA A A\nA A A\nA A A\nA A A\nA A A\n1 b1 1\nA A\nA A\nA A\nb1\nA A A\nA A A\nA A A\nA A A\nA A A\nA A A\nA A A\nA A A\nLW5,2\nLW5,3\nLW5,5\nInput Gate Feedback Gate\nConstant Error Carousel\nOutput Gate\nD\nD\nD D D\nA A \nA A \nA A \nA A \nOutput Gating Layer\nLW6,5\na7(t)a6(t)a5(t)\na4(t)a3(t)a2(t)\na1(t) n7(t)n6(t)n5(t)\nn4(t)n3(t)n2(t)\nn1(t)p1(t)\np1(t) p1(t) p1(t)\na7(t) a7(t) a7(t)\na7(t)\nIW1,1\nLW1,7\nLW2,7 LW3,7 LW4,7\nLW7,6\nIW2,1 IW3,1 IW4,1\nLW7,4\nFigure 2. Long Short Term Memory\nhas recently become very popular. They key concept in LSTM is we wo uld like to predict responses that\nmay be signiﬁcantly delayed from the corresponding stimulus. For ex ample, words in a previous paragraph\ncan provide context for a translation, therefore the network mu st enable this possibility to have long term\nmemory.\nLong term memories are the network weights and short term memor ies are the layer outputs. We need a\nnetwork which has long and short term memory combined. In RNNs, a s the weights change during training,\nthe length of the short term memory will change. It will be very diﬃcu lt to increase the length if the initial\nweight does not produce a long short term memory. Unfortunately , if the initial weight produces a long short\nterm memory, the network can easily have unstable outputs. To ma intain a long term memory, we need to\nhave a layer called Constant Error Carousel (CEC). This layer has a feed back matrix L W1,1 to have some\neigenvalues very close to one shown in Figure 2. This has to be maintain ed during training or the gradients\nwill vanish. In addition to ensure long memories, the derivative of the transfer function should be constant.\nTherefore, we need to set L W1,1 = I and use a linear transfer function.\nNow, we do not want to indiscriminately remember everything. Thus, we need to create a system that\nselectively picks what information to remember. The solution, outline d in [ 23] is a gating mechanism in\nwhich gates act like switches that operates on input, the CEC layer a nd the output layer. The input gate\nwill allow selective inputs into CEC, a feedback or forget gate will clear CEC, and the output gate will\nallow selective outputs from CEC. Each gate will be a layer with inputs f rom gated outputs and the network\ninputs. The network results in the LSTM, with CEC short term memor ies that last longer. The key details\nare:\n• The ◦ operator is the Hadamard product, which is an element by element mu ltiplication.\n• The weights in the CEC are all ﬁxed to the identity matrix and they are not trained.\n• The output and the gating layer weights are also ﬁxed to the identity matrix.\n• It has been shown that the best results are obtained when initializing the feedback or forget gate,\nbias b3, to all ones or larger values.\n• Other weight and biases are randomly initialized to small numbers.\n• The output of the gating layer generally connects to another layer or ML network with softmax\ntransfer function.\n• Multiple LSTM can be cascaded into each other.\nLANGUAGE MODELS AND AUTOMATED ESSAY SCORING 7\nDr\nBNDr BNDr\nDropout Batch Norm\nLayer 1\na1\n1\nInputs\nIW\n1,1\nb1\nS1(768x1)\n(512x768)\n(768x768)\n1\nIW2,1\nb2\n1\nIW3,1\nb3\n(768x768)\n(768x768)\n(768x1)\n(768x1)\n(512x768)\nLayer 2\nLayer 3 \na2\na3\nKey\nQuery\nValue\nSegmentation\n(12x512x64)\n(512x768)\n(512x768)\n(12x512x64)T\n(12x512x64)\n(12x512x512)\nAttention Mask\n(12x512x512)\nAttention Score\nAttention Probabilities\n(12x512x512)\n(0,-1000)-->0 for actual word and -1000 for padded zero\n(12x512x512)\n(12x512x64)\nAttention Mechanisim (Dot Product (Key and Query Transpose) and Softmax)\nDropout\n(512x768)\nCollapse\n1\nL W4,(-)\nb4,(-)\n(768x768)\n(768x1)\n(512x768)\na4\nDropout Batch Norm\n1\nL W5,4\nb5,4\n(3072,768)\n(3072x1)\n(512x3072)\na5\n1\nL W6,5\nb6,5\n(768,3072)\n(768x1)\n(512x768)\na6\nContext Matrix\nLayer 4 Layer 5 Layer 6Batch Norm\n(512x768)\nBatch Norm Dropout\nFigure 3. One layer of base BERT\nWe need to note that Deep Learning frameworks unroll these netw orks with delays and for each time\nstep they create a physical layer and then use static backpropag ation algorithm to calculate the gradients\n[27]. Then they roll the networks back and average the derivatives wit h respect to the weight and biases\nover the physical layers. The unrolling and rolling eﬀect is only an appr oximation of the true gradient with\nrespect to the weights.\n3.3. BERT. BERT, which stands for Bidirectional Encoder Representations from Transformers , is a\nLanguage Model released by the Google AI Language team at the en d of the year 2018 [ 15]. It has become\nthe state-of-the-art model for many diﬀerent Natural Language Undestanding tasks, including sequence and\ndocument classiﬁcation. This is best reﬂected by the fact that one can only see BERT-like models on the\nGLUE benchmark leaderboard, with the sole exception of XLNet [ 16], which then again is not so diﬀerent\nfrom BERT. The success of BERT can be explained in part by its novel language modeling approach, but also\nby the use of the Transformer [28], a Neural Network architecture based solely on Attention mechanisms,\nwhich was introduced one year prior, replacing Recurrent Neural Networks (RNNs) as the state-of-the-\nart Natural Language Understanding (NLU) techniques. We will giv e an overview of how Attention and\nTransformers work, and then explain BERT’s architecture and its p re-training tasks.\nSelf-Attention, the kind of Attention used on the Transformer, is essentially a mec hanism that allows a\nNeural Network to learn representations of some text sequence inﬂuenced by all the words on the sequence.\nIn a RNN context, this is achieved by using the hidden states of the p revious tokens as inputs to the next\ntime step. However, as the Transformer is purely feed-forward, it must ﬁnd some other way of combining all\nthe words together to map any kind of function in an a NLU task. It d oes so with the following equation\nSelf-Attention = Softmax\n(\nQKT\n√dk\n)\nV(9)\nHere, Q, K and V (query, key and value) are matrices which are obtained by taking the dot product\nof some trainable weight matrices W Q, W K and W V with the embedding matrix of our input sequence X.\nThat is, Q = W QXT , K = W K XT and V = W V XT . Basically, each row on these matrices corresponds to\none word, meaning that each word is mapped to three diﬀerent proj ections of its embedding space. These\nprojections serve as abstractions to compute the self-attentio n function for each word. The dot product\nbetween the query for word 1 and all the keys for words 1, 2, ..., n tells us how “similar” each word is\nto word 1, a measure that is normalized by the softmax function across all the words. The output of the\nsoftmax weights how much each word should contribute to the repr esentation of the sequence that is drawn\nfrom word 1. Thus, the output of the self-attention transfer fu nction for each word is a weighted sum\nof the values of all the words (including, and mainly, itself), by some p arameters that are learnt to get\nthe best representation that ﬁts the problem at hand. dk is the dimension of the query vectors (512 for\n8 PEDRO URIA RODRIGUEZ, AMIR JAFARI AND CHRISTOPHER M. ORMER OD\nBN Dr\nDropoutBatch Norm\nInput\nEmbeddings Size\n(30522x768)\n(512x768)\nWord Embedding\n(512x768)\nPositional Embedding\n(2x768)\nSegment Embedding\n(512x768)\n(512x768)\n(512x768)\nEmbedding Batch Norm Dropout\n(a) Embedding Layer\nL W6,5\nb6,5\n(768,3072)\n(768x1)\n(512x768)\na6\nLayer 6\nL W7,6\nb7,6\n(768,768)\n(768x1)\n(768x1)\na7\nPooler Layer\n(1x768)\nFirst Row\n(b) Pooler Layer\nFigure 4. BERT Embedding and Pooler Layer\nthe Transformer, and 768 for base BERT and XLNet), and diving by its square root leads to more stable\ngradients.\nThe Transformer model goes one step further than simply comput ing a Self-Attention function, by im-\nplementing what is called Multi-Head Attention . This is basically a set of L Self-Attention computations,\neach on diﬀerent sub-vectors, obtained from the original Q, K and V by breaking them up into L diﬀerent\nQl, Kl and Vl made up of R/L components each, where R is the embedding dimension (768 for the base\nBERT and XLNet) and L the number of Attention Heads (12 for base BERT and XLNet). This is illustrated\nin Figure 3 under “Segmentation”. After the Self-Attention is comp uted for each ( Ql, K l, V l), the original\ndimension is obtained by a simple concatenation (“Context Matrix” in F igure 3).\nAlthough up until this point we have only described the Encoder part of the Transformer, which is actu-\nally an Encoder-Decoder architecture, both BERT and XLNet use o nly an Encoder Transformer, so this is\nmainly all the architecture these Language Models are made of, with some key changes in the case of XLNet.\nNow we proceed to describe BERT’s architecture from input to outp ut, and also how it is pre-trained to learn\na natural language. First, the actual words in the text are proje cted into an embedding dimension, which\nwill be explained later in the context of Language Modeling. Once we ha ve the embedding representation\nof each word, we input them into the ﬁrst layer of BERT. Such layer, shown in Figure 3, consists mainly\nof a Multi-Head Attention Layer, which is identical to that of the Tra nsformer, except for the fact that an\nattention mask is added to the softmax input. This is done in order to avoid paying att ention to padded 0s\n(which are necessary if one wants to do vectorized mini-batching). The attention mask is vector made up of\n0s for the words we want the model to attend to (the actual word s in the sequence), and of very small values\n(like -10,000) for the padded 0s. The sums of the keys and queries d ot products with this mask will go into\nthe softmax, making the attention scores for the masked padded 0s become practically 0. The output of\nthis layer goes into a linear layer of size RxR, in order to learn a local linear combination of the Multi-Head\nAttention output. Batch Normalization is performed on the sum of t he output of this layer (after a Dropout)\nand the input to the BERT layer. This is fed into yet another linear laye r of size RxR′, where R′ = 3072 for\nthe base BERT, followed by a GeLu (Gaussian Error Linear Units) transfer function and another linea r layer\n(R′xR) that maps the higher dimensions back to the embedding dimensions, with also Dropout and Batch\nNorm. This constitutes one BERT Layer, of which the base model ha s 12. The outputs of the ﬁrst layer\nare treated as the hidden embeddings for each word, which the sec ond layer takes as inputs and does the\nsame kind of operations on them. Once we have gone through all the layers, the output for the ﬁrst token\n(a special token “[CLS]” that remains the same for all input sequenc es) is passed onto another linear layer\n(RxR) with a tanh transfer function. This layer (Figure 4(b)) acts as a pooler and its output is used as the\nrepresentation of the whole sequence, which can ﬁnally allow learning multiple types of tasks by using other\nspeciﬁc-purpose layers or even treating it as the sequence featu res to input into another kind of Machine\nLearning model.\nNow that we have described BERT’s architecture in detail, we will focu s on the other main aspect that\nmakes BERT so successful: BERT is, ﬁrst and foremost, a Languag e Model. This means that the model is\ndesigned to learn useful knowledge about natural language from la rge amounts of unlabeled text, but also to\nLANGUAGE MODELS AND AUTOMATED ESSAY SCORING 9\nBN\nBatch Norm\n1\nL W1,1\nb1,1\n(768,768)\n(768x1)\n(512x768)\na1\n1\nL W2,1\nb2,1\n(30522,768)\n(30522x1)\n(512x30522)\na2\nLayer 1 Layer 2Inputs\nLast Layer\n(512x768)\n(512x30522)\n1\nL W3,1\nb3,1\n(2,768)\n(2x1)\nLayer 3Inputs\nOutput Pooler Layer\n(1x768)\n(1x2)\na3\nWord embedding\nMask Language Model\nNext Sentence Prediciton\nCross-Entropy Loss\nCross-Entropy Loss\nFigure 5. BERT Pre-Training Heads\nretain and use this knowledge for supervised downstream tasks. T he way Language Modeling usually works\nin a RNN scenario is just using the n previous words as inputs to predict the next word n + 1. The model\ncannot take as input the word n + 1 or any words after it (although there are some bidirectional var iants), so\nthere is no need for special preprocessing of the text. However, as BERT is a feed-forward architecture that\nuses attention on all the words in some ﬁxed-length sequence, if no thing is done, the model would be able\nto attend mainly to the very same word it is trying to predict. One solu tion would be cutting the attention\non all the words after, and including, the target word. However, n atural language is not so simple. More\noften than one would think, words within a sequence only make sense when taking the words after them\nas context. Thankfully, the attention mechanism can allow to captu re both previous and future context,\nand one can stop the model from attending to the target word by masking it (not to be confused with the\nattention mask used for the padded zeros). In particular, for ea ch input sequence, 15 % of the tokens are\nrandomly masked, and then the model is trained to predict these to kens. The way this is done is taking the\noutput of BERT, before the pooler, and mapping the vectors corr esponding to each word to the vocabulary\nsize with a linear layer, whose weights are the same as the ones from t he input word embedding layer,\nalthough an additional bias is included, and then passing this to a soft max function in order to minimize a\nCategorical Cross-Entropy performance index that is computed with the predicted labels and the true labels\n(the ids on the token vocabulary, but only making the masked words contribute to the loss). Masking words\nis really straightforward: just replace them with the special token “[MASK]”. This way, the network cannot\nuse information from this word or any other masked words, aside fr om their position in the text. BERT was\nalso pre-trained to predict whether a sentence B follows another s entence A (both randomly sampled from\nthe text 50% of the time, while the rest of the time sentence B is actu ally the sentence that comes after\nsentence A). Although recent research [ 29] has shown that the same or even better results can be obtained\nwithout this second task, in the original implementation the model is o ptimized to minimize the sum of the\nlosses from each task at the same time. The additional architectur e just described is shown in Figure 5.\nIn addition to the usual word embeddings, positional embeddings ar e used to give the model information\nabout the position of each word on the sequence (this is also done in t he Transformer, although with some\ndiﬀerences), and due to the next sentence prediction task and als o for easy adaptation to downstream tasks\nsuch as question-answering, a segment embedding to represent e ach of the sentences is also utilized. The word\nembeddings used by BERT are WordPiece embeddings [ 30], which consist in a tokenization technique in\nwhich the words are split into sub-word units. This helps handling out-of-vocabulary words while keeping the\nactual vocabulary size small (30,522 unique word-pieces for BERT uncased). The positional embeddings are\nlook-up tables of size 512x R, which assign a diﬀerent embedding vector to each token based on it s position\n10 PEDRO URIA RODRIGUEZ, AMIR JAFARI AND CHRISTOPHER M. ORME ROD\nwithin the sequence. 512 was also chosen on the original Transform er as the maximum sequence length,\nmainly because Self-Attention’s complexity is quadratic to the seque nce length, due to the fact that it needs\nto compute the attentions of every word to every other word and also to themselves. While the ﬁrst token in\nevery input sequence will have the same positional embedding, the s ame applies to all the tokens belonging\nto the ﬁrst sentence in the pair of sentences A and B, i.e, the segment embeddings are look-up tables of size\n2xR. All of these embeddings have the same dimensions, so they can be s imply added up element-wise to\ncombine them together and obtain the input to the ﬁrst Multi-Head A ttention Layer, as shown in Figure\n4(a). Notice that these embeddings are learnable, so although pre -trained WordPiece are being used at the\nbeginning for the word embeddings, these are being updated to rep resent the words in a better way during\nBERT’s pre-training and ﬁne-tuning tasks. This becomes even more crucial in the case of the positional\nand segment embeddings, which need to be learned from scratch. I t is also worth noting that, although\nthe embedding layers are technically look-up tables, which work with in puts of dimension 512x1 (containing\none unique token vocabulary id for each word), mathematically this is equivalent to having a linear layer\n(without bias) of size 512x R, and one-hot-encoded inputs of dimension 512xVocabularySize. T he projection\nand weight update will be the same, but the ﬁrst method is much fast er because there is no matrix product\ninvolved, just a look-up (indexing) operation.\n3.4. XLNET. XLNet is a language model introduced very recently [ 16] that makes use of the Trans-\nformerXL [ 31] to incorporate information from previous sequence/s in order to process the current sequence,\nachieving a regressive eﬀect at the sequence level. To do so, it emplo ys a relative positional encoding and a\npermutation language modeling approach. Although BERT and XLNet share a lot of similarities, there are\nsome key diﬀerences that need to be explained.\nFirstly, XLNet’s Multi-Head Attention’s core operation is diﬀerent th an the one implemented in BERT\nand in the Transformer. In this case, instead of just breaking up t he original Q, K and V into L diﬀerent Ql,\nKl and Vl; L linear layers (for each) are used to map the input to the Multi-Head A ttention layer into these\ndiﬀerent Ql, Kl and Vl, and thus no intermediate Q, K and V are computed. This results in the three linear\nlayers of RxR being replaced by 3 L linear layers of R/L x R, which map the input into smaller subspaces\n(with the same number of dimensions which add up to the original dimen sion). These several (and parallel)\ncomputations on diﬀerent dimensions produce more variability, allowin g each word to attend more to other\nwords and not only to itself, which results in a ﬁnal richer represent ation of each word, calculated by adding\nup the results of mapping back each of the sub-representations t o the original embedding dimension R with\nagain 12 linear layers. This is expressed with the following equation, wh ere X is the input. Note that the\nactual implementation is a bit more complex, as shown in Figure 6, but t he heart of the operation is indeed\nin equation (10).\nMultiHead(X) =\n12∑\nl=1\n[\nSoftmax\n(\nQl\n\nXW Q\nl (\nKl\n \nXW K\nl )T\n√dk\n) Vl\n\nXW V\nl\n]\nW O\nl(10)\nSecondly, apart from this, XLNet’s Attention is diﬀerent from BERT ’s in two ways: 1. The keys and\nvalues (but not the queries) of the current sequence and for eac h layer depend on the hidden states of the\nprevious sequence/s, based on a memory length hyper-paramete r. That is, let the hidden state (output) of\nlayer m for the previous sequence be a matrix hm\nt−1 of dimensions 512 x 768, then if we choose a memory\nlength of mem len = 10 tokens, the key and value of the current sequence for layer m + 1 will be computed\nby concatenating to hm\nt the last ten vectors of hm\nt−1 and then projecting the result using the W K,m+1\nl and\nW V,m+1\nl matrices. This recurrence mechanism at the sequence level is illustr ated in Figure 7. If the memory\nlength is greater than 512, we can even reuse information from the two last sequences, although this becomes\nquadratically expensive. 2. The operation just described is only app lied to the word embeddings, and not\nto the sum of the three kinds of embeddings ( hm are just the word embeddings). The other two are used in\na diﬀerent way. The (relative) positional embeddings (encodings is a more suitable name) are computed by\nthe following equation\nLANGUAGE MODELS AND AUTOMATED ESSAY SCORING 11\nWord Embedding\n(512x768)\nIW1,1\nb1\n(64x768)\n(64)\n1\nIW2,1\nb2\n(64x768)\n(64)\n1\nIW12,1\nb12\n(64x768)\n(64)\n1\nL W13,1\n(64x768)\nL W14,2\n(64x768)\n(64x768)\nL W25,13\n(64x768)\nL W26,14\n(64x768)\nL W36,24\n(64x768)\nPositional Emebdding\nb13\n(64)\n1\nIW13,1\nRL\nb14\n(64)\n1\nIW14,1\nRL\nb24\n(64)\n1\nIW24,1\nRL\nSegment Embedding\nb25\n(64)\n1\nb-\n(64)\n1\nIW26,1\nb36\n(64)\n1\nIW25,1\nQ1\nQ2\nQ12\nQ1\nQ2\nQ12\natt-po-1\natt-po-2\natt-po-12\nM-K-1\nM-K-2\nM-K-12\nM- V -1\nM- V -2\nM- V -12\nQ1\nQ2\nQ12\natt-s-1\natt-s-2\natt-s-12\nM-K-1\nM-K-2\nM-K-12\natt-po-1\natt-po-2\natt-po-12\natt-s-1\natt-s-2\natt-s-12\nM- V -1\nM- V -1\nM- V -1\nL W37,25\nL W38,26\nL W48,36\n(512x64)\n(512x64)\n(512x64)\n(512x768)\n(512+memory x 64)\n(512+memory x 64)\n(512+memory x 64)\n(512+memory x 64)\n(512+memory x 64)\n(512+memory x 64)\n(64x768)\n(512x64)\n(512x64)\n(512x64)\n(64x768)\n(64x768)\n(512*2+memoryx64)\n(512x512+memory)\n(512x512+memory)\n(512x512+memory)\n(512+memoryx512)\nT\n(512x64)\n(512x64)\n(512x64)\n(64x2)\n(64x2)\n(64x2)\n(512x2x512+memory)\n(512x512+memory)\n(512x512+memory)\n(512x512+memory)\n(512x512+memory)\n(512x512+memory)\n(512x512+memory)\n(512x64)\n(512x64)\n(512x64)\n(768x64)\n(512x768)\n(512x768)\n(512x768)\n(512x768)\nIW36,1\nL W24,12\nAttention Mask\nAttention Mask\nAttention Mask\n(768x64)\n(768x64)\n(512*2+memoryx64)\n(512*2+memoryx64)\n(512*2+memoryx768)\n(512x512*2+memory)\n(512x512*2+memory)\n(512x512*2+memory)\nMemory\nConcat\n(memoryx768)\n(512+memory x 512x2)\n(512+memoryx512)T\n(512+memoryx512)T\n(512x2)T\n(512x2)T\n(512x2)T\nWord Embedding\nOverwrite Memory\nMemory\nFigure 6. One layer of base XLNet (content stream)\nppos = concat\n[\nsin\n(\neinv-inds ⊗ pinds\n)\n, cos\n(\neinv-inds ⊗ pinds\n) ]\n(11)\nwhere pinds = [mem len + seqlen, memlen + seqlen − 1, memlen + seqlen − 2, ..., 0, −1, −2, ..., −seqlen + 1] and\neinv-indsi =\n(\n10000eindsi /784\n) −1\n, with einds = [0 , 2, 4, ..., 766]. Note that these are also diﬀerent from BERT’s\nin the sense that they are not being learnt. ppos, of shape (2seq len + mem len) x 768, is projected into L\npositional keys Kl,p by learnable matrices W K,p\nl . The dot products between these and L positional queries\nobtained from the original queries by adding them up with learnable bia ses bQ,p\nl are performed, and then the\nsecond to the seq len + 1 elements are obtained from the memory dimension after perform ing a relative shift\nbetween this dimension and the current sequence dimension, result ing in positional attention scores which\nare added up to the regular attention scores before going into the softmax. This way, XLNet can perform\na smarter attention to both the words on the previous sequence/ s and the current sequence, by using this\ninformation that is being learnt based on the relative position of each word with respect to each other word\nof each sequence. To distinguish between current and previous se quence/s, a segment embedding is also\nutilized, which consists simply of a one-hot-encoded matrix of (seq len + mem len)xseqlenx2: we have a 1 if\nword i and word j belong to the same sequence, and 0 otherwise. Before attending t o this segment encoding\n(which acts as a unique segment key), the original queries are again added up with biases bQ,s\nl and then\nprojected by L weight matrices into L Q l,s, of shape 512x2. The result is also added to the attention scores\nbefore the softmax, and the operation described on equation (10 ) with the values is performed. The rest\nis almost identical to BERT (layers 5 and 6 on Figure 3, layer 4 and the b atch norm after it are omitted),\ntaking into account that the output of the ﬁrst XLNet layer acts a s hidden word embeddings that go into to\n12 PEDRO URIA RODRIGUEZ, AMIR JAFARI AND CHRISTOPHER M. ORME ROD\nDr\nPositonal Embedding\nSegment Embedding\nWord Embedding\nL1 Dr L2 L12\nMemory 1 Memory 2\nSegment 0\nConcatDr\nPositonal Embedding\nSegment Embedding\nWord Embedding\nMemory 1\nL1\nMemory 1\nConcatDr\nMemory 2 Memory 2\nL2 L12\nSegment 1\nFigure 7. 12 Layer XLNet (content stream)\nthe second layer, while the positional and segment encodings inpute d to this layer remain the same as the\nones inputed to the ﬁrst layer, as shown in Figure 7. The detailed arc hitecture is shown in Figure 6.\nWhile the architecture diﬀerences have been listed above, XLNet als o diﬀers from BERT in their pre-\ntraining tasks. XLNet is pre-trained by a permutation language mod eling approach. This means that, for any\nsequence, there are sequence length! permutations of the factorization order, and an AR langua ge modeling\ncan be performed by maximizing the likelihood under the forward auto regressive factorization. Note that\nthe order of the sequence remains unchanged: the permutation o nly aﬀects which words are attended to, by\nchanging the attention mask before the softmax: to predict word k, the attention mask is set to very small\nnumbers for words with i > k , so that only the words before and including k on the current factorization\norder are used to compute the attention. The trick here is that th e words that come before k change with\neach permutation, but their positions are kept constant within the sequence, allowing XLNet to capture\nbidirectional context. Additionally, due to the fact that utilizing per mutations causes slow convergence,\nXLNet is pre-trained to predict only the last 16.67 % of tokens in each factorization.\nIn order to use the position of the token k that is going to be predicted, but not its content, XLNet\nintroduces a new type of query. The same kind of Multi-Head attent ion is performed, starting from a\nrandomly initialized vector (or vectors if we are predicting more than one token at the same time). This\nvector is projected by the same L linear layers as the normal query to obtain the new type of query, w hich\nattends to the same keys and values as the regular query, but with the new attention mask explained before\n(with the diﬀerence that the element corresponding to word k is also set to a very small value). So basically,\nin the pre-training task, this new Multi-Head Attention (named quer y stream) and the one from Figure 6\n(named content stream) are performed at the same time layer by la yer, because the query stream needs\nthe outputs of each layer from the content stream to get the con tent keys and the values to perform the\nattention on the next layer. The content stream can see the cont ent of the words that come before k in the\nfactorization order, and also k, while the query stream can only see the content of the words that come before\nk. After going through the 12 XLNet layers and projecting the outp ut of this new query with a linear layer\nof VocabularySize x target tokens, the Cross-Entropy loss with the indexes of the real toke ns is computed\nand the model’s parameters are updated to minimize this loss. The met hod just described allows XLNet to\nbe pre-trained without the need to replace the target tokens with the special token “[MASK]”, which is not\npresent at all during ﬁne-tuning.\n4. Fine Tuning and Experiments\nIn this section we provide an overview of how neural language model ﬁne-tuning is done for a downstream\nclassiﬁcation task such as essay scoring, as well as explain the expe riments we did in order to improve\nperformance. The output layer/s that were used for the pre-tr aining task/s are replaced with a single\nclassiﬁcation layer. This layer has the same number of neurons as lab els (possible scores for the essays),\nwith a softmax activation function, which is then used, together with the target, to compute a cross-entropy\nperformance index as a loss function. In the case of BERT, the last hidden state of the ﬁrst (and special)\nLANGUAGE MODELS AND AUTOMATED ESSAY SCORING 13\ntoken “[CLS]” is used as the representation of the whole essay. Bec ause this representation needs to be\nadjusted to the particular problem at hand, the whole model is train ed. This diﬀers from the way in which\ntransfer learning is done on images, where, if the model was pretra ined using at least some images similar\nto the task at hand, updating all the parameters does not usually p rovide a boost in performance that is\njustiﬁably by the much longer training time. Regarding XLNet, the sa me method is applied but now the\n“[CLS]” token is located at the end of the essay.\nIn theory, the model should retain most of the knowledge it learnt a bout the English language during the\npre-training tasks. This would provide not only a much better initializa tion, which drastically reduces the\ndownstream training time, but also an increase in performance when compared with other Neural Networks\nthat need to learn natural language from random initial conditions f rom a much smaller corpus. However,\nin practice, various problems can arise such as catastrophic forgetting, which means the model forgets very\nquickly what it had learnt previously, rendering the main point of tran sfer learning almost useless. There are\nvarious ways of dealing with this: we try gradual unfreezing, discriminative ﬁne-tuning and a combination of\nboth as proposed in [ 32]. Gradual unfreezing consists of only training the last layer on the ﬁ rst epoch, which\ncontains the least general information about the language, and th en unfreezing one more layer per epoch,\nfrom last to ﬁrst. On the other hand, discriminative ﬁne-tuning con sists on using diﬀerent learning rates for\ndiﬀerent layers, as they capture diﬀerent kinds of features on De ep Networks [ 33]. In particular, BERT has\nbeen shown to attend to diﬀerent kinds of words and capture diver se linguistic notions on diﬀerent attention\nheads [ 34]. The learning rate across layers m follows the formula α m = ξαm+1, where ξ is a decay factor\nusually set close to 1 [ 35]. Another closely related problem is overﬁtting. To mitigate this, we try using the\nmodel’s hidden states at diﬀerent layers and also some data-prepro cessing to force the model to focus more\non other kinds of words, as well as diﬀerent dropout values. Lastly , we also use an ensemble of diﬀerent\nmodels trained with the diﬀerent approaches.\nWe run our experiments using pytorch-transformers implementations of BERT and XLNet. We\nchoose Adam as the optimizer, as in the original papers, and try diﬀe rent learning rates, narrowing the best\nvalues to either e−5 or 5 e−6. We also try diﬀerent warmup schedules , and ﬁnd that they make no signiﬁcant\ndiﬀerence. Regarding BERT, there are currently two main versions : “cased” and “uncased”. We ﬁnd that\noverall “uncased” works slightly better, although for some items t he “cased” version is superior. However,\nthe diﬀerence is still very small. For XLNet, the only available version is “cased”. We also compare the base\nand large versions and ﬁnd that they perform very similarly, so using the large versions is not worth it, given\nthat they are much more expensive to ﬁne-tune. Thus, all the res ults shown are for the base versions. The\nsame applies to the batch size, so we end up using the largest we could ﬁt in a 12GB GPU, i.e, 9 for BERT\nand 8 for XLNet.\nDue to the fact that BERT and XLNet were pre-trained with sequen ces of 512 tokens (510 when taking\ninto account “[CLS]” and “[SEP]”), and some of our essays are quite lo nger than that, we use a sliding-\nwindow approach in which longer essays are split into two or more sequ ences of 510 tokens. We force an\noverlapping of the last of these sequences with the second-to-las t, in order to avoid meaningless padding on\nthe last split. For prediction, we just round the average of the sco res on each of these splits. Although we\nalso experimented imputing only the ﬁrst 510 tokens, or the ﬁrst 12 8 and last 382, as proposed in [ 35], this\ndid not make any signiﬁcant diﬀerence, and even if it did, it should be av oided because in the context of\nessay scoring it could be argued to be unethical.\n5. Results\nIn this section we evaluate more in-depth each of the things we tried on the development set, and then\nprovide the results on the test set by picking the best model on the development set. Table 1 shows the\ndev qwk percentage diﬀerence between each combination and the base try , which is just using BERT/XLNet\nas they are. It can be seen that, overall, the methods to avoid cat astrophic forgetting do not work very\nwell, although for particular items they can give a small boost in perfo rmance. Their combination (1+2)\nalso performs poorly on all the items except for BERT on item 8. Incr easing the dropout probability is\nneither a good idea, and when it helps, it is only slightly. However, in the case of BERT, decreasing the\ninput complexity (removing stop-words), the model complexity (us ing only the three ﬁrst layers) and a\ncombination of both seems to actually reduce overﬁtting and works the best overall, which is good news\nbecause it is much more inexpensive than running a combination of the previous methods, even more so\nwhen trying diﬀerent learning rates and warm-up schedules for eac h of them. On the other hand, XLNet\n14 PEDRO URIA RODRIGUEZ, AMIR JAFARI AND CHRISTOPHER M. ORME ROD\nTable 1. BERT and XLNet Results of various experiments for each item\nBERT Experiments / Item ∆ 1 (%) ∆ 2 (%) ∆ 3 (%) ∆ 4 (%) ∆ 5 (%) ∆ 6 (%) ∆ 7 (%) ∆ 8 (%) Mean ∆ (%)\n(1) Gradual Unfreezing -4.58 -5.17 +0.04 -1.49 -5.76 -4.19 -18.87 -6.41 -5.80\n(2) Discriminative Finetuning ( ξ = 0 . 95) -3.59 -0.04 +0.70 +1.17 -1.61 -1.08 -0.14 -10.20 -1.85\n1+2 -2.69 -3.71 -0.54 -3.58 -6.07 -7.85 -10.8 +1.38 -4.23\n(3) Dropout (0.2) -3.77 +0.49 +0.93 -0.12 -2.11 -0.54 -3.57 -31.83 -5.07\n1+3 -5.29 -7.29 +0.32 -0.99 -4.27 -2.10 -8.49 -7.18 -4.41\n2+3 +0.57 +0.43 +0.23 -0.45 -3.47 -0.42 -8.22 -5.00 -2.04\n1+2+3 -3.65 -7.72 -0.36 -2.13 -7.15 -13.52 -19.06 -17.93 -8.94\n(4) Remove Stop-Words +2.60 -2.43 -0.66 -0.92 -4.03 -2.18 +0.69 -0.17 -0.89\n(5) 3 Layers -0.41 +0.23 -0.55 -0.52 +0.12 -1.14 +0.37 -2.71 -0.58\n4 + 5 +1.82 +1.35 +2.04 -0.05 -1.90 -2.20 +0.77 -0.65 +0.15\nXLNet Experiments / Item ∆ 1 (%) ∆ 2 (%) ∆ 3 (%) ∆ 4 (%) ∆ 5 (%) ∆ 6 (%) ∆ 7 (%) ∆ 8 (%) Avg ∆ (%)\n(1) Gradual Unfreezing -4.46 -6.00 -2.86 +1.38 -3.31 -1.34 -1.95 -0.26 -2.35\n(2) Discriminative Finetuning ( ξ = 0 . 95) +2.41 -2.02 -1.05 -1.02 -1.75 -0.41 -1.26 +4.50 -0.08\n1+2 -4.58 -6.38 -1.28 -1.93 -3.47 -2.46 -0.83 -5.51 -3.31\n(3) Dropout (0.2) -0.59 -2.28 -3.16 -2.65 -2.53 -6.88 -6.89 -1.87 -3.36\n1+3 -7.57 -19.41 -7.91 -9.56 -5.61 -10.29 -3.42 -9.07 -9.11\n2+3 -3.97 -6.80 -1.04 -1.02 -2.21 -4.60 -5.51 -18.97 -5.52\n1+2+3 -8.89 -23.34 -11.09 -5.89 -7.31 -16.66 -16.32 -37.86 -15.92\n(4) Remove Stop-Words +2.66 -4.19 -1.72 -0.79 -1.59 -2.36 +2.04 -3.19 -1.14\n(5) 3 Layers -0.96 -0.29 -4.07 -1.54 +0.06 -3.70 -7.95 -20.06 -4.81\n4 + 5 +0.94 -1.26 -2.49 -2.89 -0.12 -1.32 -8.58 -20.44 -4.52\nTable 2. qwks for the diﬀerent items\nItem 1 qwk (%) 2 qwk (%) 3 qwk (%) 4 qwk (%) 5 qwk (%) 6 qwk (%) 7 qwk (%) 8 qwk (%) Avg qwk(%)\nBERT 79.20 67.99 71.52 80.08 80.59 80.53 78.51 59.58 74.75\nXLNet 77.69 68.06 69.29 80.62 78.33 79.37 78.67 62.68 74.34\nLSTM [ 13] 77.50 68.70 68.30 79.50 81.80 81.30 80.50 59.40 74.63\nBERT Ensemble 80.21 67.21 70.82 81.56 80.63 81.47 80.42 59.74 75.26\nXLNet Ensemble 80.49 68.59 70.09 79.56 79.94 80.54 80.02 59.76 74.87\nBERT + XLNet Ensemble 80.78 69.67 70.31 81.90 80.82 81.45 80.67 60.46 75.76\nLSTM (+CNN) Ensemble [ 13] 82.10 68.80 69.40 80.50 80.70 81.90 80.80 64.40 76.08\nEASE (Bag of Words) [ 13] 78.10 62.10 63.00 74.90 78.20 77.10 72.70 53.40 69.90\nH1-H2 Agreement 72.08 81.23 76.90 85.10 75.27 77.59 72.09 62.03 75.29\nonly sees an increase in performance for two items when removing st op-words, and using three layers does\nnot help either. These ﬁndings suggests that BERT is more ﬂexible th an XLNet, or at least that it can adapt\nbetter to extreme changes in the architecture and input levels. Re garding catastrophic forgetting, it looks\nlike XLNet does witness more improvement than BERT for the items in w hich either gradual unfreezing,\ndiscriminative ﬁnetuning or their combination boost performance.\nTable 2 shows the ﬁnal results on each item for BERT, XLNet, a BERT ensemble, an XLNet ensemble\nand a BERT + XLNet ensemble. The ﬁrst two ensembles consist of 6 mo dels obtained using the diﬀerent\nexperiments from above. We tried taking a majority vote (using the best model out of these 6 to decide\nwhen there is a tie), and rounding the mean of the scores predicted by each model. Both methods performed\nsimilarly on items 1 to 6, but the majority vote performed signiﬁcantly poorer on items 7 and 8. The BERT\n+ XLNet ensemble consists of 12 models, i.e, it combines the models fro m the two other ensembles together.\nWe also show the results for the LSTM from [ 13] and their ensemble, which consists on 10 LSTMs and\n10 LSTMs with a convolutional layer before them, and which also arriv es at the ﬁnal prediction by taking\nthe mean of the scores. The last two rows correspond to the Bag o f Words model and the inter-human\nagreement.\nRegarding the individual models, BERT, XLNet and the LSTM obtain ve ry similar average qwk across\nall the items. This suggests that the essay scoring problem has rea ched its ceiling in terms of modeling, at\nleast for now. When compared to their individual versions, the ense mble boost performance by 0.51 % for\nLANGUAGE MODELS AND AUTOMATED ESSAY SCORING 15\nBERT, 0.53 % for XLNet, and 1.45 % for the LSTM. The main diﬀerence b etween the Language Models\nensembles and the LSTM ensemble that may account for an almost 3x bigger delta in favor of the LSTM\nis the amount of models (6 vs 20), although it is possible that the conv olution layer on the LSTM produces\nmore variability than using diﬀerent number of layers and altering the inputs to the Language Models. The\nBERT + XLNet ensemble (12 models) does better (+1.01 % from BERT a nd +1.42 % from XLNet), which\npoints to the ﬁrst reason being more likely.\nWhen compared with the Bag of Words method, Neural Networks sh ow a signiﬁcant superiority, with\na 6 % higher qwk on average. What is more, there is no single item for wh ich the Bag of Words performs\nbetter. And although individual networks are still a bit below the inte r-human agreement, the ensemble of\nthese models are actually beating humans by 0.79 % in the case of the L STM and by 0.47 % in the case of\nBERT + XLNet on average. Item by item, Neural Networks achieve h igher-than-human qwk on 5 out of 8.\n6. Conclusions\nTransfer learning and language models enhanced the performance of analyzing texts in natural language\nprocessing. In this paper, we demonstrated the two major trans former based neural network models which\nimproved the result of essay scoring on the Kaggle dataset. BERT a nd XLNet are discussed in a very\ndetailed manner to researchers for further improvements. The r esults of BERT and XLNet are compared\nwith other traditional methods and human standards. Overall, we g ot better results that human and rule\nbased techniques. Our major contribution is explaining the network architectures and generalizing it with\nsimple notation, and implementing a classiﬁcation technique using thes e models on the essay scoring problem\nto get an automated engine. This engine tends to be more reliable tha n humans and save a lot of time and\nmoney for grading essays in a large scale.\n7. Acknowledgements\nI would like to thank Balaji Kodeswaran, and Paul van Wamelen for th eir support and discussions.\n16 PEDRO URIA RODRIGUEZ, AMIR JAFARI AND CHRISTOPHER M. ORME ROD\nReferences\n[1] E. B. Page, “Grading essays by computer: progress report .” in Proceedings of the Invitational Conference on Testing\nProblems, 1967, pp. 87–100.\n[2] ——, “The use of the computer in analyzing student essays. ” in International Review of Education , 1968, pp. 210–225.\n[3] K. L. Gwet, Handbook of inter-rater reliability: The deﬁnitive guide t o measuring the extent of agreement among raters .\nAdvanced Analytics, LLC, 2014.\n[4] D. Alikaniotis, H. Yannakoudakis, and M. Rei, “Automati c text scoring using neural networks,” arXiv preprint\narXiv:1606.04289, 2016.\n[5] K. Sakaguchi, M. Heilman, and N. Madnani, “Eﬀective feat ure integration for automated short answer scoring,” in Pro-\nceedings of the 2015 conference of the North American Chapte r of the association for computational linguistics: Human\nlanguage technologies , 2015, pp. 1049–1054.\n[6] M. D. Shermis, “Contrasting state-of-the-art in the mac hine scoring of short-form constructed responses,” Educational\nAssessment, vol. 20, no. 1, pp. 46–65, 2015. [Online]. Available: https ://doi.org/10.1080/10627197.2015.997617\n[7] H. Yannakoudakis and R. Cummins, “Evaluating the perfor mance of automated text scoring systems,” in Proceedings of\nthe Tenth Workshop on Innovative Use of NLP for Building Educ ational Applications , 2015, pp. 213–223.\n[8] M. T. Hagan and M. B. Menhaj, “Training feedforward netwo rks with the marquardt algorithm,” IEEE transactions on\nNeural Networks , vol. 5, no. 6, pp. 989–993, 1994.\n[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation with deep convolutional neural networks,” i n\nAdvances in neural information processing systems , 2012, pp. 1097–1105.\n[10] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Eﬃcient es timation of word representations in vector space,” arXiv\npreprint arXiv:1301.3781 , 2013.\n[11] A. H. Jafari and M. T. Hagan, “Application of new trainin g methods for neural model reference con-\ntrol,” Engineering Applications of Artiﬁcial Intelligence , vol. 74, pp. 312 – 321, 2018. [Online]. Available:\nhttp://www.sciencedirect.com/science/article/pii/S0952197618301490\n[12] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine tran slation by jointly learning to align and translate,” arXiv\npreprint arXiv:1409.0473 , 2014.\n[13] K. Taghipour and H. T. Ng, “A neural approach to automated essay scoring,” Conference on Empirical Methods in Natural\nLanguage Processing, no. 1905.05583, p. 1882–1891, November 2016.\n[14] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sut skever, “Language models are unsupervised multitask\nlearners,” OpenAI Blog , vol. 1, no. 8, 2019.\n[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for langu age\nunderstanding,” arXiv, no. 1810.04805, Oct 2018.\n[16] Z. Yang, Z. Dai, Y. Yang, J. Carbonel, R. Salakhutdinov, and Q. V. Le, “Xlnet: Generalized autoregressive pretraini ng\nfor language understanding,” arXiv, no. 1906.08237, Jun 2019.\n[17] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, “Deep contextualized word\nrepresentations,” in Proc. of NAACL , 2018.\n[18] D. M. Williamson, X. Xi, and F. J. Breyer, “A framework fo r evaluation and use of automated scoring,” Educational\nmeasurement: issues and practice , vol. 31, no. 1, pp. 2–13, 2012.\n[19] J. Arter, “Rubrics, scoring guides, and performance criteria: Classroom tools for assessing and improving student learning.”\n2000.\n[20] J. Cohen, “A coeﬃcient of agreement for nominal scales, ” Educational and psychological measurement , vol. 20, no. 1, pp.\n37–46, 1960.\n[21] E. B. Page, “The imminence of... grading essays by compu ter,” The Phi Delta Kappan , vol. 47, no. 5, pp. 238–243, 1966.\n[22] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. D ean, “Distributed representations of words and phrases and\ntheir compositionality,” in Advances in neural information processing systems , 2013, pp. 3111–3119.\n[23] S. Hochreiter and J. Schmidhuber, “Long short-term mem ory,” Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[24] M. T. Hagan, H. B. Demuth, M. H. Beale, and O. D. Jes´ us, Neural Network Design, 2nd Edition . PWS Publishing,\nBoston.\n[25] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Gated feed back recurrent neural networks,” in International Conference\non Machine Learning , 2015, pp. 2067–2075.\n[26] A. H. Jafari and M. T. Hagan, “Enhanced recurrent networ k training,” in 2015 International Joint Conference on Neural\nNetworks (IJCNN) . IEEE, 2015, pp. 1–8.\n[27] R. Pascanu, T. Mikolov, and Y. Bengio, “On the diﬃculty o f training recurrent neural networks,” in International confer-\nence on machine learning , 2013, pp. 1310–1318.\n[28] A. Vaswani, N. Shazee, N. Parmar, J. Uszkorei, L. Jone, A . N. Gomez, /suppress Lukasz Kaiser, and I. Polosukhin, “Attention isall\nyou need,” arXiv, no. 1706.03762, Jun 2017.\n[29] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A\nrobustly optimized bert pretraining approach,” arXiv, no. 1907.11692, Jul 2019.\n[30] M. Schuster and K. Nakajima, “Japanese and korean voice search,” in 2012 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) . IEEE, 2012, pp. 5149–5152.\n[31] Z. Dai, Z. Yang, Y. Yang, W. W. Cohen, J. Carbonell, Q. V. L e, and R. Salakhutdinov, “Transformer-xl: Attentive\nlanguage models beyond a ﬁxed-length context,” arXiv, no. 1901.02860, Jan 2019.\n[32] Howard and Ruder, “Universal language model ﬁne-tunin g for text classiﬁcation,” arXiv, no. 1801.06146, 2018.\nLANGUAGE MODELS AND AUTOMATED ESSAY SCORING 17\n[33] Y. B. Jason Yosinski, Jeﬀ Clune and H. Lipson, “How trans ferable are features in deep neural networks?” Advances in\nneural information processing systems , pp. 3320–3328, 2014.\n[34] O. L. C. D. M. Kevin Clark, Urvashi Khandelwal, “What doe s bert look at? an analysis of bert’s attention,” arXiv, no.\n1906.0434, Jun 2019.\n[35] Y. X. X. H. Chi Sun, Xipeng Qiu, “How to ﬁne-tune bert for t ext classiﬁcation?” arXiv, no. 1905.05583, May 2019.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8267041444778442
    },
    {
      "name": "Transformer",
      "score": 0.7504979372024536
    },
    {
      "name": "Notation",
      "score": 0.6805794835090637
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5764998197555542
    },
    {
      "name": "Artificial neural network",
      "score": 0.5381247997283936
    },
    {
      "name": "Natural language processing",
      "score": 0.4868835508823395
    },
    {
      "name": "Language model",
      "score": 0.4433681070804596
    },
    {
      "name": "Recurrent neural network",
      "score": 0.41296151280403137
    },
    {
      "name": "Machine learning",
      "score": 0.37114953994750977
    },
    {
      "name": "Linguistics",
      "score": 0.07911673188209534
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 17
}