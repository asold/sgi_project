{
  "title": "Vision-language model-based human-robot collaboration for smart manufacturing: A state-of-the-art survey",
  "url": "https://openalex.org/W4406711794",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5028611772",
      "name": "Junming Fan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102567131",
      "name": "Yue Yin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101274426",
      "name": "Tian Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5021935675",
      "name": "Wenhang Dong",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5079101040",
      "name": "Pai Zheng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100434965",
      "name": "Lihui Wang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3016120632",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4391136507",
    "https://openalex.org/W6860529987",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W6847076894",
    "https://openalex.org/W4363675964",
    "https://openalex.org/W4283009931",
    "https://openalex.org/W6850503672",
    "https://openalex.org/W4398793540",
    "https://openalex.org/W4206654130",
    "https://openalex.org/W4387885937",
    "https://openalex.org/W4383047510",
    "https://openalex.org/W3212977909",
    "https://openalex.org/W4401414715",
    "https://openalex.org/W4301396999",
    "https://openalex.org/W4388340774",
    "https://openalex.org/W2987283559",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W4390872665",
    "https://openalex.org/W4385822471",
    "https://openalex.org/W4383108296",
    "https://openalex.org/W4384264726",
    "https://openalex.org/W3205423339",
    "https://openalex.org/W4320561714",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3211462570",
    "https://openalex.org/W4384264709",
    "https://openalex.org/W4389519446",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W4384615642",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4393147080",
    "https://openalex.org/W4399198074",
    "https://openalex.org/W4395053967",
    "https://openalex.org/W4285212811",
    "https://openalex.org/W2993247505",
    "https://openalex.org/W4401687172",
    "https://openalex.org/W4393148636",
    "https://openalex.org/W4221159977",
    "https://openalex.org/W4403842408",
    "https://openalex.org/W4392633655",
    "https://openalex.org/W4390872429",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4223537666",
    "https://openalex.org/W4384268338",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4393160097",
    "https://openalex.org/W4285069854",
    "https://openalex.org/W3207832698",
    "https://openalex.org/W4322716427",
    "https://openalex.org/W4388623944",
    "https://openalex.org/W4225750637",
    "https://openalex.org/W4310998175",
    "https://openalex.org/W4405022833",
    "https://openalex.org/W4387634951",
    "https://openalex.org/W4323066621",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W6845921028",
    "https://openalex.org/W4285231577",
    "https://openalex.org/W4386072185",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4396941373",
    "https://openalex.org/W2955808192",
    "https://openalex.org/W4396620506",
    "https://openalex.org/W4389266910",
    "https://openalex.org/W3162737204",
    "https://openalex.org/W4389665189",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4400944635",
    "https://openalex.org/W4296605949",
    "https://openalex.org/W4366290711",
    "https://openalex.org/W4396716480",
    "https://openalex.org/W4389611232",
    "https://openalex.org/W4386384908",
    "https://openalex.org/W4392172801",
    "https://openalex.org/W4402353981",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W4327667441",
    "https://openalex.org/W4394880521",
    "https://openalex.org/W4393154152",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W4385473486",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W3142041998",
    "https://openalex.org/W4399206636",
    "https://openalex.org/W2912083425",
    "https://openalex.org/W4206759694",
    "https://openalex.org/W2914304175"
  ],
  "abstract": "Abstract human-robot collaboration (HRC) is set to transform the manufacturing paradigm by leveraging the strengths of human flexibility and robot precision. The recent breakthrough of Large Language Models (LLMs) and Vision-Language Models (VLMs) has motivated the preliminary explorations and adoptions of these models in the smart manufacturing field. However, despite the considerable amount of effort, existing research mainly focused on individual components without a comprehensive perspective to address the full potential of VLMs, especially for HRC in smart manufacturing scenarios. To fill the gap, this work offers a systematic review of the latest advancements and applications of VLMs in HRC for smart manufacturing, which covers the fundamental architectures and pretraining methodologies of LLMs and VLMs, their applications in robotic task planning, navigation, and manipulation, and role in enhancing human–robot skill transfer through multimodal data integration. Lastly, the paper discusses current limitations and future research directions in VLM-based HRC, highlighting the trend in fully realizing the potential of these technologies for smart manufacturing.",
  "full_text": "Junming FAN, Yue YIN, Tian WANG, Wenhang DONG, Pai ZHENG, Lihui WANG\nV\nision-language model-based human-robot collaboration for\nsmart manufacturing: A state-of-the-art survey\n© The Author(s) 2024. This article is published with open access at link.springer.com and journal. hep.com.cn\n \nAbstract    human–robot  collaboration  (HRC)  is  set  to\ntransform  the  manufacturing  paradigm  by  leveraging  the\nstrengths  of  human  flexibility  and  robot  precision.  The\nrecent  breakthrough  of  Large  Language  Models  (LLMs)\nand  Vision-Language  Models  (VLMs)  has  motivated  the\npreliminary explorations and adoptions of these models in\nthe  smart  manufacturing  field.  However,  despite  the\nconsiderable  amount  of  effort,  existing  research  mainly\nfocused on individual components without a comprehensive\nperspective to address the full potential of VLMs, especially\nfor HRC in smart manufacturing scenarios. To fill the gap,\nthis work offers a systematic review of the latest advance-\nments and applications of VLMs in HRC for smart manu-\nfacturing, which covers the fundamental architectures and\npretraining  methodologies  of  LLMs  and  VLMs,  their\napplications  in  robotic  task  planning,  navigation,  and\nmanipulation,  and  role  in  enhancing  human–robot  skill\ntransfer  through  multimodal  data  integration.  Lastly,  the\npaper  discusses  current  limitations  and  future  research\ndirections  in  VLM-based  HRC,  highlighting  the  trend  in\nfully realizing the potential of these technologies for smart\nmanufacturing.\nKeywords      vision-language  models,  large  language\nmodels, human–robot collaboration, smart manufacturing \n1    Introduction\nhum\nan–robot collaboration (HRC) has been regarded as a\npromising  pathway  to  revolutionise  the  manufacturing\nsector  by  leveraging  the  complementary  strengths  of\nhumans and robots (Matheson et al., 2019). This synergy\na\nims  to  enhance  productivity,  adaptability,  and  effi-\nciency,  marking  a  significant  paradigm  shift  in  smart\nmanufacturing (Wang et al., 2019). The recent astonishing\nbre\nakthroughs  in  the  Artificial  Intelligence  (AI)  field,\nincluding computer vision and natural language process-\ning, have exhibited huge potential to drive this transfor-\nmation  by  endowing  robots  with  multimodal  perception\nand  understanding  capabilities,  enabling  more  sophisti-\ncated  and  seamless  collaborations  between  humans  and\nrobots (Fan et al., 2022; Wang et al., 2024c).\nAs\n the  most  recent  advancement  of  AI,  Large\nLanguage  Models,  such  as  GPT-3  (Brown  et  al.,  2020)\na\nnd  GPT-4  (Achiam  et  al.,  2023),  have  demonstrated\ne\nxceptional  capabilities  in  natural  language  processing,\nenabling them to exhibit human-like comprehension and\nconversational  abilities.  However,  standard  LLMs  are\ninherently  limited  to  processing  textual  information,\nwhich  restricts  their  applicability  in  scenarios  such  as\nHRC that require visual context. In response to this limi-\ntation,  Vision-Language  Models  (VLMs)  have  been\ndeveloped  to  integrate  visual  and  textual  data  (Zhou  et\na\nl., 2022), thereby enhancing the robot's ability to interpret\na\nnd interact with its environment. Prevalent examples of\n \nReceived Jul. 27, 2024; revised Nov. 13, 2024; accepted Nov. 15, 2024\nJunming FAN, Yue YIN, Tian WANG, Wenhang DONG,\nPa\ni ZHENG (✉)\nDepartment  of  Industrial  and  Systems  Engineering,  The  Hong  Kong\nPolytechnic University, Hong Kong 999077, China\nE-mail: pai.zheng@polyu.edu.hk\nLihui WANG (✉)\nDepartment of Production Engineering, KTH Royal Institute of Tech-\nnology, Stockholm, Sweden\nE-mail: lihuiw@kth.se\nThis  work  was  mainly  supported  by  the  funding  support  from  the\nR\nesearch Institute for Advanced Manufacturing (RIAM) of The Hong\nKong  Polytechnic  University  (1-CDJT);  the  Intra-Faculty  Interdisci-\nplinary Project 2023/24 (1-WZ4N), by the Research Committee of The\nHong Kong Polytechnic University; the State Key Laboratory of Intel-\nligent Manufacturing Equipment and Technology, Huazhong University\nof  Science  and  Technology  (IMETKF2024010);  Guangdong–Hong\nKong  Technology  Cooperation  Funding  Scheme  (GHX/075/22GD);\nInnovation and Technology Commission (ITC); the COMAC Interna-\ntional  Collaborative  Research  Project  (COMAC-SFGS-2023-3148);\nand the General Research Fund from the Research Grants Council of\nthe  Hong  Kong  Special  Administrative  Region,  China  (Project  Nos.\nPolyU15210222 and PolyU15206723); Open access funding provided\nby the Hong Kong Polytechnic University.\nFront. Eng. Manag. 2025, 12(1): 177–200\nhttps://doi.org/10.1007/s42524-025-4136-9\nREVIEW ARTICLE\nVLMs  include  CLIP  (Radford  et  al.,  2021)  and  ALIGN\n(Jia  et  al.,  2021),  which  have  shown  promise  in  tasks\nsuc\nh as image captioning, visual question answering, and\nmultimodal  reasoning.  The  significant  advancements  of\nVLMs have inspired the initial adoptions in HRC scenarios\nto enhance robotic intelligence and human–robot commu-\nnication  flexibility  (Fan  and  Zheng,  2024; Park  et  al.,\n2024).  However,  existing  research  endeavors  are  rather\nsc\nattered  in  different  applications  and  perspectives,\nresulting in a lack of a comprehensive investigation of the\npotential of VLMs in HRC scenarios.\nThis  paper  aims  to  bridge  this  gap  by  providing  a\nsystematic review of the latest advancements and applica-\ntions of VLMs in HRC. The overview of the structure of\nthis survey is illustrated in  Fig. 1. The exploration begins\nwi\nth  the  fundamental  architectures  and  pretraining\nmethodologies  of  LLMs  and  VLMs  in  Section  3,  in\nwhich we briefly introduce the intricacies of transformer\narchitectures,  the  mechanisms  of  pretraining  on  large-\nscale data sets, and the subsequent fine-tuning processes\nthat tailor these models to specific applications. Next, the\npractical applications of VLMs in robotic task planning,\nnavigation, and manipulation are examined in Section 4.\nThese  capabilities  are  essential  for  robots  to  function\neffectively  in  dynamic  manufacturing  settings  where\ntasks are varied and environments are constantly chang-\ning. On top of the basic functionalities, it is imperative to\nequip  robots  with  advanced  skill  acquisition  ability  in\norder to better adapt to the futuristic flexible manufacturing\nenvironments. Therefore, the role of VLMs in enhancing\nhuman–robot skill transfer is also explored in Section 5.\nCompared  to  traditional  robot  skill  acquisition  methods\nthat  often  involve  extensive  programming,  VLMs  can\nsignificantly  streamline  this  process  by  using  visual  and\nlinguistic inputs to facilitate more intuitive and effective\nhuman–robot  teaching.  Lastly,  the  current  challenges,\nthat  prevent  the  immediate  deployment  of  VLMs  in\nmanufacturing  scenes,  and  potentid  future  directions\ntoward  unlocking  the  full  potential  of  VLMs  in  smart\nmanufacturing are discussed in Section 6. \n2    Literature review process\nT\nhe  literature  review  process  for  this  paper  follows  a\nsystematic manner to ensure a comprehensive and unbiased\noverview of the latest advancements and applications of\nVLMs  in  HRC.  As  depicted  in Fig.  2,  an  initial  search\nwa\ns employed to identify relevant literature from various\nacademic  databases,  including  Web  of  Science,  Scopus,\nand  IEEE  Xplore.  The  search  was  conducted  using  a\ncombination  of  keywords  related  to  the  core  topics:\n“human–robot” and “vision language,” covering the time\nspan 2020–2024. The search yielded 63 items from Web\nof Science, 113 related documents from Scopus, and 89\nfrom IEEE Xplore (July 15, 2024).\nThe  literature  selection  process  was  conducted  in  two\nphases: initial screening and detailed reviewing, to ensure\nthat  the  most  relevant  and  high-quality  studies  were\nincluded. First, an initial filtering process was conducted,\nduring  which  only  journal  and  conference  papers  in\nEnglish  were  included.  Papers  that  were  obviously\nbeyond the scope of this survey were excluded based on\ntheir  titles,  keywords,  and  abstracts,  resulting  in  59\npapers.  Subsequently,  an  in-depth  review  process  was\nimplemented  to  further  identify  inadequate  items  and\ncategorise  suitable  papers.  Considering  the  results  from\nthe  aforementioned  databases  are  quite  limited,  supple-\nmentary relevant papers were added from other less rigor-\nous  search  engines  such  as  Google  Scholar  and  arXiv.\nNotably, although arXiv papers are not normally accepted\nas  trustworthy  paper  sources,  a  considerable  amount  of\n \nFig. 1    Ove rview of the survey of VLM-based HRC for smart manufacturing.\n178 Front. Eng. Manag. 2025, 12(1): 177–200\nstate-of-the-art  works  related  to  LLMs  and  VLMs  have\nnot\n yet  made  it  to  formal  publications  and  can  only  be\nfound on arXiv, thereby leading to the frequent inclusion\nof arXiv papers in this review. Finally, 109 papers have\nbeen  selected  as  the  basis  of  this  survey,  which  are\nfurther described in Section 3.5. \n3    Revisiting LLMs and VLMs\nIn\n recent years, significant advancements have been made\nin  the  development  of  large  language  models  (LLMs)\n(Zhao  et  al.,  2023a;  Chang  et  al.,  2024).  By  scaling  the\nsi\nze  of  data  and  models,  these  LLMs  have  exhibited\nexceptional  emergent  abilities,  including  instruction\nfollowing  (Peng  et  al.,  2023),  in-context  learning  (ICL)\n(Brown et al., 2020), and chain of thought (CoT) reasoning\n(Wei et al., 2022). Despite their impressive zero-shot and\nfe\nw-shot  performance  on  various  natural  language\nprocessing (NLP) tasks, LLMs are intrinsically limited in\ntheir  ability  to  interpret  visual  information,  as  they  can\nonly  process  discrete  text.  To  overcome  this  limitation,\nresearchers  have  developed  vision-language  models\n(VLMs) (Zhang et al., 2024a ). VLM is designed to learn\nri\nch  vision-language  correlation  from  large-scale  image-\ntext pairs, thereby enhancing its capacity for comprehen-\nsive and accurate understanding and reasoning. The typical\narchitectures of LLMs and VLMs are depicted in  Fig. 3.\nT\nhis  section  provides  a  brief  introduction  to  the  funda-\nmental  concepts  and  development  status  of  LLMs  and\nVLMs, respectively. \n3.1    Fundamentals of LLMs\nL\nLMs are normally constructed on the transformer archi-\ntecture (Vaswani et al., 2017), which uses a specifically\nde\nsigned  neural  network  and  the  multi-head  attention\nmechanism  to  understand  context  and  meaning  in  text.\nThe  core  mathematical  operation  in  the  self-attention\nmechanism is the computation of attention scores, which\nare  derived  from  the  query  (Q),  key  (K),  and  value  (V)\nvectors, as formulated in the following equation:\n \nAttention(Q,K,V ) =softmax\n(Q KT\n√dk\n)\nV, (1)\ndkwhere   represents the dimensionality of the key vectors.\nT\nhe  softmax  function  normalizes  the  attention  scores.\nThe  multi-head  attention  mechanism  is  then  built  upon\nseveral parallel self-attention heads:\n \nMultiHead(Q,K,V ) =Concat(head1 , ..., headh ) WO , (2)\n \nheadi =Attention(Q WQi ,KW Ki ,V WVi\n), (3)\nWQi WKi WVi\nWO\nwhere  ,  ,   are learned projection matrices for\ne\nach  head,  and   is  the  output  projection  matrix.  The\ntwo main components of Transformers are encoders and\ndecoders.  Encoders  extract  and  comprehend  relevant\ninformation  from  input  text  using  self-attention,  while\ndecoders  generate  translated  text  using  the  embeddings\nfrom  the  encoder  (Fu  et  al.,  2023).  The  encoder  block\nc\nombines  multi-head  self-attention  and  feed-forward\nneural networks. The output of each encoder layer can be\n \nFig. 2    Syst ematic literature review process and the trend of related publications.\n \nFig. 3    T ypical  architectures  of  LLMs  and  VLMs:  (a)  Three\ntypes  of  LLMs;  (b)  The  pretraining  architectures  of  VLMs.\n(Zhang et al., 2024a).\nJunm\ning FAN et al. Vision-language model-based human-robot collaboration in smart manufacturing 179\nrepresented as:\n \nEncoderOutput =LayerNorm(X +FFN(MultiHead(X))) ,\n(4)\nX FFN\nLayerNorm\nwhere   is the input,   is the feed-forward network,\na\nnd    is  the  layer  normalization  operation.  In\ncontrast, the decoder block has an additional cross-atten-\ntion mechanism that attends to the encoder’s output:\n \nDecoderOutput =LayerNorm(X +FFN\n(MultiHead(X,EncoderOutput))) . (5)\nBased  on  the  variation  of  the  encoder-decoder  struc-\nt\nure, LLMs can be categorised into three types: Encoder-\nonly, Decoder-only, and Encoder-Decoder.\n•  Encoder-only  models:  These  models  consist  solely\nof  encoders.  While  focusing  on  feature  encoding  for  a\nbetter understanding of the text information, they cannot\ndirectly generate textual output, making them suitable for\ntasks  like  text  categorisation  (Kenton  and  Toutanova,\n2019).\n• E\nncoder-decoder models: This type of model incor-\nporates  both  encoder  and  decoder  components.  They\nencode  the  input  into  feature  information  and  pass  it  to\nthe decoder, which then generates output according to the\nsequence. This structure effectively manages the connec-\ntion between input and output sequences, making it suitable\nfor translation and text summarizing tasks ( Lewis et al.,\n2020).\n•  De\ncoder-only  models:  These  models  only  have\ndecoder components. They use the encoder to generate a\ncorresponding sequence from the input encoding, focusing\non generating or predicting output from a series of inputs\n(Wang et al., 2022a). They specialize in generation tasks,\nsuc\nh  as  question  answering,  and  represent  the  dominant\narchitecture today.\nTable  1  summarizes  some  well-known  LLMs  that  fall\ni\nnto  these  categories.  The  following  content  of  this\nsection will briefly cover the pretraining, fine-tuning, and\nprompting technologies of LLMs. \n3.1.1    Pretraining\nPre\ntraining serves as the initial phase in which an LLM is\nsubjected to training on an extensive collection of textual\ndata in an unsupervised manner. This critical phase facili-\ntates the development of fundamental linguistic capabili-\nties and representational skills within the model. Utilizing\nthe  scalable  features  of  the  Transformer  architecture,\nwhich incorporates self-attention mechanisms, the BERT\nmodel  (Kenton  and  Toutanova,  2019)  was  developed.\nT\nhis  framework  advances  the  training  of  bidirectional\nlanguage  models  using  meticulously  crafted  tasks  on\nexpansive,  unlabeled  corpora.  The  word  representations\ngenerated  through  this  pretraining  process  are  context-\nsensitive  and  highly  effective  with  versatile  semantic\nfeatures.  This  innovative  approach  has  motivated  a\nconsiderable body of subsequent research, giving rise to\nthe  “pretraining  and  fine-tuning”  paradigm.  This\nparadigm  has  guided  extensive  investigations  such  as\n \nTable 1    Mainstream LLMs\nType Name Year Company Open Source\nE\nncoder-Only BERT (Kenton et al., 2019) 2018 Google Open\nR\noBERTa (Liu et al., 2019) 2019 Meta Open\nE\nRNIE (Zhang et al., 2019) 2019 Baidu Open\nDe\nBERTa (He et al., 2020) 2020 Microsoft Open\nE\nncoder-Decoder BART (Lewis et al., 2020) 2019 Meta Open\nT\n5 (Raffel et al., 2020) 2019 Google Open\nC\nhatGLM (GLM et al., 2024) 2023 Tsinghua University Open\nFl\nanUL2 (Tay et al., 2023) 2023 Google Open\nDe\ncoder-Only GPT-1 (Radford et al., 2018) 2018 OpenAI Open\nGPT\n-2 (Radford et al., 2019) 2019 OpenAI Open\nGPT\n-3 (Brown et al., 2020) 2020 OpenAI Close\nE\nRNIE3.0 (Sun et al., 2021) 2021 Baidu Close\nL\naMDA (Thoppilan et al., 2022) 2021 Google Close\nPa\nLM (Chowdhery et al., 2023) 2022 Google Close\nL\nLaMA (Touvron et al., 2023) 2023 Meta Open\nGe\nmini (Team et al., 2023) 2023 Google Open\nGPT\n-4 (Achiam et al., 2023) 2023 OpenAI Close\nC\nlaude (Anthropic 2023) 2023 Anthropic Close\n180 Front\n. Eng. Manag. 2025, 12(1): 177–200\nGPT-2  (Radford  et  al.,  2019)  and  BART  (Lewis  et  al.,\n2020),  and  the  implementation  of  refined  pretraining\nm\nethodologies  (Sanh  et  al.,  2022).  In  practice,  this\npa\nradigm typically involves adjusting the pretrained LLM\nthrough  fine-tuning  to  meet  the  demands  of  specific\ndownstream applications. \n3.1.2    Fine-tuning and alignment\nAft\ner  pretraining,  LLMs  are  fine-tuned  to  specialize  in\nspecific tasks or to align with human values and intents.\nFine-tuning  involves  training  the  pretrained  model  on  a\nsmaller,  task-specific  data  set,  often  using  supervised\nlearning  techniques.  Typical  fine-tuning  approaches  are\nsummarized as follows:\n•  Transfer  Learning:  Pretrained  LLMs  exhibit  com-\nmendable  performance  across  a  spectrum  of  tasks.\nNonetheless,  for  enhanced  task-specific  performance,\nthese  models  need  to  undergo  fine-tuning  with  task-\nspecific  data,  a  process  referred  to  as  transfer  learning\n(Raffel et al., 2020).\n• Instr\nuction-tuning: To ensure models respond effec-\ntively  to  user  queries,  pretrained  models  are  fine-tuned\nusing  instruction-formatted  data,  which  includes  natural\nlanguage  directives  coupled  with  relevant  input-output\npairs. This approach not only enhances the model’s ability\nto  generalize  across  new  scenarios,  but  also  boosts  its\nperformance  on  specific  tasks.  Detailed  methodologies\nfor creating and varying instructional data are outlined in\nthe literature (Chung et al., 2024).\n•  Al\nignment-tuning:  LLMs  can  sometimes  produce\ninaccurate or harmful content. To address this, alignment-\ntuning  is  conducted  using  human  feedback  to  adjust\noutputs  and  discourage  undesirable  responses.  This\nprocess ensures models align with ethical standards, char-\nacterized  by  the  “HHH”  criteria:  helpful,  honest,  and\nharmless  (Askell  et  al.,  2021).  Techniques  such  as  rein-\nforc\nement  learning  with  human  feedback  (RLHF)\n(Ziegler  et  al.,  2019)  are  employed,  where  models  are\nre\nfined through reward modeling (RM) and reinforcement\nlearning (RL) to meet these criteria. \n3.1.3    Prompting\nAft\ner  an  LLM  has  been  thoroughly  trained  and  fine-\ntuned,  the  prompting  technique  is  employed  to  elicit\nresponses from the LLM. LLMs can be prompted in various\nconfigurations; some setups allow the model to adapt to\ninstructions  without  further  fine-tuning,  while  others\nrequire  fine-tuning  on  data  sets  that  incorporate  diverse\nprompting  styles  (Kim  et  al.,  2023b).  Below  is  a  brief\nove\nrview of several commonly used prompt setups:\n•  Zero-shot  prompting:  The  model  is  given  a  task\nwithout  any  examples,  relying  solely  on  its  pretrained\nknowledge to generate a response (Kojima et al., 2022).\n• In-c\nontext learning: Also known as few-shot learn-\ning, this method provides the model with a few examples\nwithin  the  prompt  to  guide  its  response  (Dong  et  al.,\n2022).\n•  Chai\nn-of-Thought:  A  prompting  technique  where\nthe model is guided to generate step-by-step reasoning or\nexplanations  for  its  answers,  improving  performance  on\ncomplex reasoning tasks (Wei et al., 2022). \n3.2    Fundamentals of VLMs\nT\nhe  success  of  large-scale  models  in  the  NLP  field  has\ninspired  the  computer  vision  community  to  borrow  text\ninformation to enhance visual recognition, leading to the\nthriving  of  VLMs.  A  VLM  typically  consists  of  two\nparallel encoders: one for processing visual data (such as\nimages) and one for textual data (such as descriptions or\ninstructions).  These  encoders  transform  the  inputs  into\nhigh-dimensional embeddings, which are then aligned or\nfused  in  a  shared  feature  space,  allowing  the  model  to\njointly  interpret  and  reason  about  both  visual  and\nlanguage inputs. The development of VLMs has attracted\nconsiderable  attention  as  vision  and  language  are  the\ntwo  most  semantic-rich  information  sources.  VLM  is\ncrucial for cross-modal applications like image captioning\nand  visual  question  answering  and  plays  an  even  more\nsignificant  role  in  human–robot  collaboration  in  smart\nmanufacturing environments, where multimodal commu-\nnications between human operators and robots are indis-\npensable.\nThe  most  essential  step  of  VLMs  is  pretraining\n(Radford et al., 2021), which allows models to learn the\nc\norrelation  between  images  and  texts  by  employing  a\ncombination  of  visual  and  textual  encoders.  Typically,\nVLMs  use  separate  encoders  for  each  modality  (vision\nand text), enabling the extraction of meaningful features\nfrom both input types. Subsequently, pretraining objectives\nguide  the  model  in  learning  the  relationships  between\nthese visual and textual elements. The rest of this section\nwill  introduce  the  three  aspects  of  VLMs:  vision-\nlanguage  encoding,  vision-language  correlation,  and\npretraining, of which the related works are summarized in\nTable 2. \n3.2.1    Vision and language encoding\nVL\nMs  employ  deep  neural  networks  to  extract  features\nfrom  the  image-text  pairs  in  a  data  set.  The  network\nnormally consists of an image encoder and a text encoder,\nwhich respectively encode the image and text of an input\ndata pair into visual and textual embeddings. This section\noutlines  the  architectures  of  the  deep  neural  networks\ncommonly used in VLM for image and text encoding.\n•  Visual  Encoder:  1)  CNN-based:  Various  Convolu-\ntional  Networks,  such  as  ResNet  (He  et  al.,  2016)  and\nE\nfficientNet (Tan and Le, 2019), have been developed to\nJunm\ning FAN et al. Vision-language model-based human-robot collaboration in smart manufacturing 181\nenhance  image  feature  learning  (Radford  et  al.,  2021).\nR\nesNet, which is particularly prevalent in VLMs, incor-\nporates  skip  connections  across  convolutional  blocks  to\nprevent gradient issues and support deeper network archi-\ntectures.  2)  Transformer-based:  Recent  studies  have\nextensively  explored  the  application  of  Transformers  in\nVLMs,  especially  ViT  (Dosovitskiy  et  al.,  2020),  which\ni\ns  a  prototypical  Transformer  architecture  for  image\nfeature  learning.  It  processes  images  by  dividing  them\ninto  fixed-size patches,  which  are linearly  projected  and\nposition-embedded before encoding.\n•  Textual  Encoder:  Transformers  and  their  variants\nhave become fundamental for text feature encoding. The\nstandard Transformer features an encoder-decoder frame-\nwork.  Studies  in  VLMs,  such  as  CLIP  (Radford  et  al.,\n2021),  typically  employ  this  standard  architecture  with\nsl\night adaptations. \n3.2.2    Vision and language correlation\nT\nhe core of VLMs is the understanding of the correlation\nbetween  paired  vision-language  data.  Regarding  this\nissue,  various  pretraining  objectives  have  been  designed\nto enhance the learning of vision-language features:\n•  Contrastive  Objectives:  Contrastive  objectives\nenable VLMs to acquire discriminative representations by\nattracting  paired  samples  while  pushing  away  unpaired\nones  within  the  feature  space.  In  the  case  of  VLMs,\nimage-text  contrastive  learning  is  typically  leveraged,\nwhich  is  achieved  by  minimising  the  symmetric  image-\ntext infoNCE loss (Chen et al., 2020).\n• G\nenerative Objectives: This type of objective learns\nthe  vision-language  correlation  feature  by  training\nnetworks to generate image/text data via image generation\n(He et al., 2021), language generation (Yu et al., 2022),\nor c\nross-modal generation (Singh et al., 2022).\n•  Al\nignment  Objectives:  Alignment  objectives  are\ndesigned to align the image-text features via global image-\ntext matching (Dou et al., 2022) or region-word matching\n(Yao et al., 2022) on embedding space. \n3.2.3    Pretraining architecture\nT\nhe architecture for VLM pretraining mainly symbolises\nhow  the  vision  and  language  processing  branches  and\nembeddings  are  interconnected  and  communicated.  The\nmost widely adopted frameworks in VLMs include two-\ntower, two-leg and one-tower pretraining frameworks.\nSpecifically,  the  two-tower  framework  is  commonly\nutilized in VLMs ( Radford et al., 2021;  Jia et al., 2021)\ne\nmploying separate encoders to process input images and\ntexts.  In  a  variation,  the  two-leg  framework  (Yu  et  al.,\n2022;  Singh  et  al.,  2022)  includes  extra  multimodal\nfusi\non  layers,  facilitating  interaction  between  image  and\ntext  features.  By  contrast,  one-tower  VLMs  (Tschannen\ne\nt  al.,  2022;  Jang  et  al.,  2023)  integrate  vision  and\nl\nanguage  processing  into  a  single  encoder,  promoting\nmore  efficient  communication  across  different  data\nmodalities.\nWhile  the  pretraining  and  deployment  of  VLMs  have\nwitnessed significant progress in recent years, the appli-\ncation of current VLMs in real-life manufacturing scenar-\nios  still  faces  challenges  such  as  high  computational\ndemands,  the  scarcity  of  high-quality  data  sets,  and\n \nTable 2    Summarization of VLM pretraining works\nPretraining Aspect Category Name Reference\nVi\nsion-Language Encoding Visual Encoder ResNet He et al. (2016)\nVisual Encoder EfficientNet Tan and Le (2019)\nTextual Encoder ViT Dosovitskiy et al. (2020)\nVi\nsual-Textual Encoder CLIP Radford et al. (2021)\nVi\nsion-Language Correlation Contrastive Objectives SimCLR Chen et al. (2020)\nGenerative Objectives MoCo He et al. (2020)\nGenerative Objectives Coca Yu et al. (2022)\nGe\nnerative Objectives Flava Singh et al. (2022)\nAlignment Objectives FIBER Dou et al. (2022)\nAl\nignment Objectives DetCLIP Yao et al. (2022)\nPre\ntraining Architecture Two-Tower CLIP Radford et al. (2021)\nT\nwo-Tower ALIGN Jia et al. (2021)\nT\nwo-Leg Coca Yu et al. (2022)\nT\nwo-Leg Flava Singh et al. (2022)\nOne\n-Tower CLIPPO Tschannen et al. (2022)\nOne-Tower OneR Jang et al. (2023)\n182 Front\n. Eng. Manag. 2025, 12(1): 177–200\nlatency  issues,  which  comprise the robustness  and  relia-\nbi\nlity  of  VLMs  in  handling  variations  in  real-world\nproduction (Challenge 6.1). \n4    VLM-based human–robot interactive\nt\nask planning and execution\nThe  remarkable  capabilities  of  LLMs  and  VLMs  have\nattracted  the  attention  of  the  scientific  community,\nprompting extensive exploration into their potential appli-\ncations  in  robotic  interactive  task  planning,  navigation,\nand manipulation, as shown in  Fig. 4. The integration of\nL\nLMs  and  VLMs  in  these  areas  has  shown  promising\nresults, demonstrating their potential to revolutionise how\nrobots perform complex tasks. Moreover, their application\nextends to industrial collaborative robots and other intricate\nscenarios, showcasing their versatility and adaptability in\nenhancing  robotic  functions  and  efficiency.  These\nadvancements are paving the way for more sophisticated\nand  intelligent  robotic  systems  capable  of  operating  in\ndiverse  and  dynamic  environments.  This  section  sum-\nmarizes  the  most  representative  work  of  VLMs  and\nLLMs  in  task  planning,  navigation  and  manipulation,\nincluding the foundation models used, success rates and\napplications. \n4.1    Vision and language task planning\nT\nraditional task-planning methods often rely on predefined\nrules and logical reasoning. However, these methods are\npowerless when faced with a dynamic and complex real\nworld.  The effectiveness  and  capability  of  recent VLMs\nhave  attracted  the  attention  of  relevant  researchers  to\nexplore vision and language task planning. As shown in\nFig.  5,  vision  and  language  task  planning  refers  to  the\na\nbility of a robot to complete task planning based on its\nvisual perception of the environment and its understanding\nof  the  target  task.  LLMs  and  VLMs  have  strong  logical\nreasoning  and  visual  perception  capabilities,  which\nenable robots to complete task planning with a full under-\nstanding of the task scenario and human instructions, and\n \nFig. 4    VL M-based human–robot interactive task planning, navigation, and manipulation.\n \nFig. 5    Vi sion and language task planning adapted with permission from Hu et al. (2023).\nJunming FAN et al. Vision-language model-based human-robot collaboration in smart manufacturing 183\npromote  effective  communication  and  collaboration\nbe\ntween humans and robots. Table 3 lists the representative\nre\nsults  of  the  recent  application  of  VLMs  or  LLMs  to\nrobot task planning. \n4.1.1    Task understanding and decomposition\nT\nask understanding and decomposition is the first step in\ntask  planning,  which  involves  extracting  task  objectives\nfrom  natural  language  descriptions  and  decomposing\ncomplex  tasks  into  a  series  of  manageable  subtasks.\nVLMs  play  a  crucial  role  in  this  process  as  they  can\nextract  rich  semantic  information  from  text  and  images.\nFor example, Song et al. ( 2023) combined the user’s na-\nt\nural language instructions with the environment informa-\ntion, and realized the understanding of tasks such as navi-\ngation  and  manipulation  based  on  LLMs  and  object\ndetectors. Zheng et al. ( 2024) adopted BERT and ResNet\nt\no parse real scenes, and prompt LLM to decompose the\noverall task into subtasks. Zhao et al. (2023b) studied the\nuse\n of  ViLD  to  perceive  visual  information  to  generate\nscene descriptions, and used text descriptions as clues to\ninform  LLM  to  achieve  adaptive  robot  grasping  task\nplanning. The shortcomings of these studies lie in the fact\nthat spatial-based task understanding has not been achiev-\ned. Future research could focus on enhancing the process-\ning  of  environmental  spatial  information  to  achieve  a\nmore comprehensive task understanding and planning. \n4.1.2    Multimodal task information fusion\nT\nhe fusion and alignment of the multimodal task informa-\ntion is essential for the VLM to successfully comprehend\nand break down the overall human–robot interactive task.\nThe core of multimodal information fusion lies in how to\ncapture the associations between different modalities in a\nunified  representation  space,  and  leverage  the  comple-\nmentary  information  to  provide  a  more  comprehensive\nsemantic understanding for subsequent task planning. Fan\nand Zheng ( 2024) aimed at a human–robot collaborative\na\nssembly task, in which they leveraged the CLIP model\nto  parse  visual  information  and  LLM  to  comprehend\nlanguage  instructions  and  generated  a  feasible  robot\naction  sequence  accordingly.  Similarly,  Song  et  al.\n(2024)  adopted  YOLO  and  VLM  to  analyze  and  reason\na\nbout current social interactions, and generated immediate\noptimal  robot  actions  to  guide  the  motion  planner.  In\naddition,  Rana  et  al.  (2023)  generated  semantic  graphs\nba\nsed on three-dimensional scene graphs, retrieved task-\nrelated  semantic  subgraphs  through  LLM,  and  then\nperformed  subtask  planning.  Gu  et  al.  (2023)  converted\nt\nhe real scene graph into a 3D conceptual scene structure\ngraph,  and  then  converted  it  into  a  text  description  and\nprovided  it  to  LLM,  and  used  LLM  to  complete  task\nplanning. The above studies focus on utilizing visual and\nsemantic interactions. For robots, the tactile modality can\nmake  a  contribution  to  understanding  task  requirements\nbetter, facilitating task planning. \n4.1.3    Action sequence generation\nAft\ner the task decomposition and multimodal information\nfusion, corresponding action sequences should be gener-\nated  to  fulfil  the  designated  task.  In  this  process,  the\nvision  and  language  model  can  generate  reasonable\naction sequences directly from vision and language inputs\nthrough end-to-end VLMs ( Hu et al., 2023;  Zhang et al.,\n2024;  Skreta  et  al.,  2024).  Action  sequence  generation\n \nTable 3    VLMs/LLMs in task planning\nMethod Model Success rate Application Year Ref.\nMa\ntcha ViLD, GPT-3 90.57% (custom data set) Table-top manipulation task 2023 Zhao et al. ( 2023b)\nT\naPA GPT-3.5, CLIP Mask RCNN 61.11% (custom data set) Indoor embodied task 2023 Wu et al. ( 2023)\nSayPlan GPT-4 86.6% (Gibson) Indoor navigation 2023 Rana et al. (2023)\nVIL\nA GPT-4V 84.4% (custom task) Table-top manipulation 2023 Hu et al. ( 2023)\nConceptGraphs GPT-4, CLIP, DINO 97% (Replica) Indoor navigation 2023 Gu et al. ( 2023)\nL\nLM-Planner GPT-3, object detector 51% (ALFRED) Indoor embodied task 2023 Song et al. (2023 )\n\\ CLIP, GPT-4 93.30% (custom data set) human–robot collaboration assembly 2024 Fan and Zheng ( 2024)\nNaVid BERT, EVA-CLIP, Vicuna-7B 92% (VLN-CE R2R) Indoor navigation 2024 Zhang et al. ( 2024b)\nGa\nmeVLM YOLOWorld 83.30% (custom data set) Table-top manipulation 2024 Mei et al. ( 2024)\n\\ PaLM-E, LAVA 92% (RT-1) Table-top manipulation 2024 Du et al. ( 2023)\nVLM-SocialNav YOLO, GPT-4V 100% (SCAND) Indoor navigation 2024 Song et al. (2024)\nR\nEPLAN GPT-4V 86.25% (RC) Table-top manipulation 2024 Skreta et al. ( 2024)\nPa\nLM-E PaLM 82.5% (OK-VQA) Embodied task 2023 Driess et al. (2023 )\nRT-2 PaLM-E\\PaLI-X 62% (custom data set) Embodied task 2023 Zitkovich et al. (2023)\n\\\nGPT-3.5, resnet-50, BERT 85% (custom data set) Assembly task, pick and place. 2024 Zheng et al. (2024)\n184 Front\n. Eng. Manag. 2025, 12(1): 177–200\nneeds  to  consider  the  context  of  the  task,  the  dynamic\nc\nhanges of the environment, and the path to achieving the\ntask  goals.  In  this  way,  the  system  is  able  to  generate\ncoherent and executable action steps, improving the effi-\nciency  and  reliability  of  task  execution.  In  addition,\nresearchers from Google have conducted a series of work\nto explore the perception-action end-to-end robot model,\nincluding  the  development  of  a  general  and  transferable\nmulti-decision  agent  by  mixing  specific  data  into  the\ninput  of  the  multimodal  LLM  (Driess  et  al.,  2023),  and\nre\nformulating  the  LLM  to  directly  output  robot  action\nparameters  in  language  format  (Zitkovich  et  al.,  2023),\nwhi\nch requires a lot of computing resources because the\nnovel LLM structure needs to be fully trained. \n4.1.4    Long-horizon task planning\nL\nong-horizon task planning refers to task planning over a\nlonger time span. This type of planning involves multiple\nsteps and stages, and requires consideration of the long-\nhorizon goals of the task and the coordination of interme-\ndiate steps. Unlike short-term task planning, long-horizon\ntask  planning  needs  to  deal  with  more  uncertainty  and\ncomplexity, and usually requires more advanced strategies\nand a more comprehensive environmental understanding.\nWu et al. ( 2023) fine-tuned the LLaMA network through\na\n triple of visual scenes, instructions, and corresponding\nplans, and used it as a fine-tuned task planner to achieve\nlong-horizon  planning  for  complex  tasks.  Mei  et  al.\n(2024)  proposed  a  decision  and  expert  agent  system\nba\nsed on VLM, in which the decision agent is responsible\nfor  planning  tasks,  and  the  expert  agent  evaluates  these\ntask plans and resolves inconsistencies between different\nagents by introducing zero-sum game theory to determine\nthe optimal solution. Du et al. ( 2023) took long-horizon\nt\nask instructions and current image observations as input\nand  output  detailed  multimodal  (video  and  language)\nlong-horizon  video  plans.  The  program  scales  with\nincreasing computational budgets and can be synthesized\nacross  different  robotics  domains,  from  multi-object\nrearrangement  to  multi-camera  dual-arm  dexterous\nmanipulation, providing better task planning results.\nA  key  challenge  for  VLM-based  task  planning  is  its\napplicability  in  dynamic  environments.  Models  such  as\nTaPA and GameVLM have shown high success rates in\nlong-term  task  planning  by  accurately  encoding  the\nrecognition features of static scenes. Du et al. generated\nreal-time  long-term  planning  results  based  on  current\nimage observations, which reflects the trend of combining\nreal-time  scene  parsing  with  dynamic  planning.  This\ncapability is key to realizing industrial scene applications,\nbecause in industrial scenes, as production tasks proceed\ndynamically,  robot  planning  needs  to  be  constantly  up-\ndated in real-time according to the scene. (Challenge 6.2). \n4.2    Vision and language navigation\nVi\nsion and language navigation (VLN) refers to a robot's\nability  to  complete  navigation  tasks  based  on  visual\nperception  of  the  environment  and  comprehension  of\nhuman  natural  language  commands.  In  this  task,  the\nagent  is  typically  provided  with  the  following  informa-\ntion:  1)  Environmental  representation:  the  agent's  visual\nperception in a 3D space, including images, point clouds,\nand other forms of environmental representation; 2) Natu-\nral  language  instructions:  a  text  description  of  how  the\nagent should move to reach the target location; 3) Initial\nposition:  the  agent's  starting  point  in  the  environment.\nThe agent’s task is to navigate the environment and reach\nthe target location based on the natural language instruc-\ntions.  This  process  requires  the  agent  to  parse  and\ncomprehend  each  step  of  the  instructions,  match  them\nwith  the  visual  perception  of  the  environment,  plan  a\npath, and take actions to complete the navigation task as\ninstructed. VLMs and LLMs play a crucial role in VLN\ntasks as they can comprehend vision and language cues to\nsignificantly enhance the agent's navigation performance\nin complex environments. Based on different navigation\nscenarios, we categorise these works into indoor naviga-\ntion,  outdoor  navigation,  and  web  navigation.  The\ngeneral approach of VLM-based VLN is shown in  Fig. 6.\nVL\nMs are often employed as visual encoders to capture\nsemantic  information  from  visual  observations,  while\n \nFig. 6    Ge neral framework of vision and language navigation.\nJunming FAN et al. Vision-language model-based human-robot collaboration in smart manufacturing 185\nLLMs are frequently utilized to understand human langu-\na\nge commands and subsequent reasoning processes. The\nrecent  representative  works  using  VLMs  or  LLMs  in\nnavigation tasks are summarized in Table 4. \n4.2.1    VLM-based navigation in an indoor environment\nIndoor\n navigation is widely applied in the field of domestic\nrobotics,  with  some  work  also  involving  human–robot\ncollaboration in industrial settings. Indoor VLN can assist\nwith  assembly,  help  humans  retrieve  and  place  tools,\nmanage logistics distribution within the factory premises,\nand inspect and maintain equipment.\nIn VLN, the most common tasks are simple instruction-\nfollowing tasks, in which a robot or virtual agent receives\ndetailed,  step-by-step  natural  language  instructions  and\nnavigates  through  the  environment  to  a  specified  target\nlocation  based  on  these  instructions.  These  instructions\nare  typically  clear  and  specific,  describing  each  step  of\nthe action path. Khandelwal et al. (2022) explored CLIP's\nc\napabilities  in  embodied  AI,  discovering  that  CLIP's\nfeature  representations  encode  these  primitives  more\neffectively  than  ImageNet-pretrained  backbones.  They\nproposed  the  EmbCLIP  model,  which  achieved  a  47%\ntask success rate in the RoboTHOR OBJECTNAV Chal-\nlenge  2021.  Korekata  et  al.  (2023)  proposed  a  model\nc\nalled  SheFU,  consisting  of  switching  image  embedder\nand  funnel  transformer.  This  model  could  predict  the\ntarget object and destination separately and achieve a task\nsuccess  rate  of  83%  in  the  ALFRED-fc  data  set.  Hong\net  al.  (2023)  presented  Ego2-Map  based  on  the  ViT-B/\n16\n model  from  CLIP  for  VLN  in  continuous  environm-\nents and could reach a 47% success rate in the R2R-CE\ndata set.\nApart  from  simple  instruction-following  tasks,  some\nworks  focus  on  remote  embodied  referring  expression\ntasks,  which  means  the  instructor  only  gives  high-level\ninstructions,  and  the  robot  needs  to  figure  out  the  clear\ndestination by continuously asking questions according to\nthe current state. Huang et al. ( 2023a) presented VLMaps\nba\nsed on LSeg and GPT-3.5, in which they utilized GPT-\n3.5 to generate Python code for robot navigation, and the\nmethod reached a 62% success rate in their custom data\nset. Qiao et al. ( 2023) utilized CLIP as a scene perceiver\na\nnd  GPT-2  as  an  in-context  learning  model  to  build  the\nMiC  model  for  remote  embodied  referring  expression\ntask, which achieved a task success rate of 55.74% in the\nREVERIE data set. Similarly, Gao et al. (2024) introduced\n \nTable 4    VLMs/LLMs in navigation\nNavigation\ne\nnvironment Method VLM/LLM Success rate Application Year Ref.\nIndoor EmbCLIP CLIP 47% (RoboTHOR\nOBJECTNAV\nChallenge 2021)\nDomestic service robot 2022 Khandelwal et al. (2022)\nSHe\nFU Switching Image\nEmbedder, Funnel\nTransformer\n83.1% (ALFRED-fc) Domestic service robot 2023 Korekata et al. ( 2023)\nEgo2-Map CLIP 47% (R2R-CE) Domestic service robot 2023 Hong et al. (2023)\nVL\nMaps LSeg, GPT-3.5 62% (custom dataset) Domestic service robot 2023 Huang et al. (2023a )\nMi\nC GPT-2, CLIP 55.74% (REVERIE) Remote embodied referring\nexpression, domestic\nservice robot\n2023 Qiao et al. ( 2023)\nCKR+ CLIP 23.13% (REVERIE) Remote embodied referring\nexpression, domestic\nservice robot\n2024 Gao et al. ( 2024)\nNavGPT GPT-3.5, BLIP-2 34% (R2R) Domestic service robot 2024 Zhou et al. ( 2024)\nLANA+ CLIP 70.1% (R2R) Instruction following and\ngeneration, domestic\nservice robot\n2024 Wang et al. (2024d )\nAC\nK CLIP 59.1% (REVERIE) Remote embodied referring\nexpression, domestic\nservice robot\n2024 Mohammadi et al. ( 2024)\nC\nONSOLE ChatGPT, CLIP 72% (R2R) Domestic service robot 2024 Lin et al. ( 2024)\nDISH CLIP 44.3% (R2R) Domestic service robot 2024 Wang et al. (2024a )\nA vi\nsion AI-based HRC\nassembly approach\nGPT-4 \\ Human–robot collaboration 2024 Liu et al. ( 2024)\nA vision and language\ncobot navigation approach\nGPT-3.5 \\ Human–robot collaboration 2024 Wang et al. ( 2024b)\nOut\ndoor LM-Nav CLIP, GPT-3 80% (custom dataset) Urban VLN 2022 Shah et al. ( 2022)\nVELMA CLIP, GPT-4 23.1% (Map2seq) Urban VLN 2024 Schumann et al. (2024)\nVL\nN-VIDEO GPT-2 31.7% (Touchdown) Urban VLN 2024 Li et al. (2024)\nW\nebsite WebVLN GPT-3.5, BLIP-2 34.76% (WebVLN-v1) Web Navigation and\nQuestion–Answering\n2024 Chen et al. (2024)\n186 Front\n. Eng. Manag. 2025, 12(1): 177–200\nCKR  +  based  on  CLIP,  which  could  achieve  a  23.13%\nsuc\ncess  rate  in  the  REVERIE  data  set.  Wang  et  al.\n(2024d)  devised  a  CLIP-based  LANA  +  model  which\nc\nould mimic the human process of finding a path through\niterative  question-and-answer  interactions  and  reached  a\nsuccess  rate  of  70.1%  in  the  R2R  data  set.  Mohammadi\net  al.  (2024)  introduced  the  ACK  framework,  which\nut\nilizes  commonsense  information  structure  as  a  spatio-\ntemporal  knowledge  graph  to  enhance  agent  navigation.\nWithin this framework, the CLIP model was employed to\ngather  and  prioritise  the  most  relevant  knowledge  conc-\nerning the scene and identified objects. In the REVERIE\ndata set, ACK obtained a 59.1% task success rate.\nMany works also aim to explore the reasoning capabili-\nties  of  LLMs  and  apply  them  to  task  planning  in  VLN.\nThese  research  efforts  aim  to  harness  the  sophisticated\nnatural  language  understanding  and  generation  abilities\nof  LLMs  to  improve  the  decision-making  processes\ninvolved  in  navigating  complex  environments.  By  inte-\ngrating  LLMs  with  visual  information,  researchers  seek\nto develop advanced navigation systems capable of inter-\npreting and responding to dynamic scenarios. Zhou et al.\n(2024)  developed  a  purely  LLM-based  VLN  system\nba\nsed  on  GPT-3.5  and  BLIP-2.  This  work  innovatively\nexplored  GPT's  zero-shot  reasoning  capabilities  in\ncomplex environments and achieved a 34% task success\nrate on the R2R data set without any training. Similarly,\nLin et al. (2024) proposed CONSOLE based on ChatGPT\na\nnd  CLIP,  and  this  model  gained  a  72%  success  rate  in\nthe R2R data set.\nFurthermore,  VLMs  can  be  leveraged  to  formulate\nstrategies for robot learning in VLN tasks. This involves\nusing  VLMs  to  generate  and  optimise  action  plans  that\nguide robots through various navigation challenges. The\nintegration of VLMs in robot learning not only enhances\nthe  robots'  ability  to  understand  and  process  visual  and\nlinguistic  cues  but  also  improves  their  adaptability  and\nperformance  in  real-world  applications.  Wang  et  al.\n(2024a)  introduced  an  RL  framework  for  discovering\ni\nntrinsic  subgoals  via  hierarchical  (DISH)  RL  in  which\nCLIP’s  image  encoder  was  applied  for  visual  feature\nextraction.  This  method  overcame  the  label  annotation\nproblem in reinforcement learning and achieved a 44.3%\ntask success rate in the R2R data set.\nWhile  models  like  EmbCLIP  and  SheFU  demonstrate\nhigh  task  success  rates  by  effectively  encoding  visual\nfeatures and predicting target locations, approaches such\nas  VLMaps  and  MiC  emphasize  interactive  question-\nasking  to  improve  task  completion  in  more  ambiguous\nscenarios.  This  highlights  a  common  trend  toward  inte-\ngrating visual perception with dynamic linguistic interac-\ntion,  although  the  varying  success  rates  underline  the\nongoing challenge of achieving consistency across differ-\nent data sets and environments.\nIn the industrial sector, some works have also introdu-\nced  LLM-based  VLN  for  HRC  tasks.  Liu  et  al.  (2024)\nut\nilized  GPT-4  along  with  3D  object  reconstruction  and\npose  estimation  methods  in  human–robot  collaborative\nassembly tasks, while Wang et al. ( 2024b) built a cobot\nna\nvigation  framework  using  GPT-3.5  combined  with\nother  visual  methods  to  assist  operators  in  tool  retrieval\nand  placement  in  HRC.  However,  while  these  advance-\nments  showcase  promising  progress,  the  variability  in\nsuccess rates across different models and data sets high-\nlights a critical challenge in performance consistency, ind-\nicating a need for improved generalisation and adaptability\nin dynamic real-world environments (Challenge 6.3). \n4.2.2    VLM-based navigation in outdoor environment\nC\nompared to indoor VLN, the outdoor VLN environment\nis  typically  more  open  and  expansive,  with  fewer\nconstraints  on  movement.  Outdoor  landscapes  can\ninclude  a  variety  of  terrains,  weather  conditions,  and\nlighting  variations.  The  presence  of  dynamic  elements\nsuch as vehicles and pedestrians adds to the complexity.\nThe  applications  of  outdoor  VLN  include  autonomous\ndriving, robotic delivery, smart city management, rescue\noperations, and environmental monitoring. In the industrial\nsector,  outdoor  VLN  can  also  be  employed  for  factory\nsecurity patrols and logistics distribution between factory\nsites.  Shah  et  al.  (2022)  presented  a  system  called  LM-\nNa\nv based on CLIP and GPT-3 for long-horizon navigation\nthrough outdoor and complex environments and achieved\nan  80%  success  rate  in  their  custom  data  set.  Similarly,\nSchumann  et  al.  (2024)  proposed  VELMA,  a  model\nba\nsed on CLIP and GPT-4, which uses verbal descriptions\nof  trajectories  and  visual  environment  observations  as\ncontextual prompts for the next action, and they achieved\na  task  success  rate  of  23.10%  in  the  Map2seq  data  set.\nLi et al. (2024) introduced VLN-VIDEO, utilizing urban\ndri\nving  videos  combined  with  automatically  generated\nnavigation  instructions  and  actions  to  enhance  outdoor\nVLN  performance.  GPT-2  was  used  to  filter  out\ntemplates with low generation probability. They gained a\n31.7%  success  rate  in  the  Touchdown  data  set.  Despite\nthese  advancements,  the  significant  variation  in  success\nrates  indicates  that  outdoor  VLN  systems  must  address\nthe  challenges  of  diverse  environmental  conditions  and\ndynamic  elements  more  effectively  to  achieve  reliable\nperformance in real-world applications (Challenge 6.3). \n4.2.3    VLM-based navigation in web environment\nApa\nrt from VLN in real-world environments, Chen et al.\n(2024)  proposed  a  VLN  model  for  web  environments.\nT\nhey  utilized  GPT-3.5  and  BLIP-2  to  construct  a\nWebVLN  model  capable  of  answering  questions  based\non  web  content  and  automatically  navigating  to  the\nrequired  web  pages.  They  also  created  a  WebVLN-v1\ndata  set,  where  the  WebVLN  model  achieved  a  task\ncompletion rate of 34.7%. This is a newly proposed task\nJunming FAN et al. Vision-language model-based human-robot collaboration in smart manufacturing 187\nthat has not yet been explored in the industrial sector, but\ni\nt has potential applications in industrial QA systems. \n4.3    Vision and language manipulation\nVL\nMs  have  also  been  frequently  leveraged  in  robotic\nmanipulation  tasks  to  enable  robots  to  perform  physical\ntasks by parsing and understanding visual inputs (such as\nimages or videos) and language inputs (such as instructions\nor descriptions). This type of interaction enables robots to\nexecute more flexible and adaptive operations in complex\nenvironments. The methodology in manipulation is similar\nto  navigation;  it  also  requires  environmental  representa-\ntion, natural language instructions, and the robot's initial\nstate.  VLMs  and  LLMs  can  accomplish  joint  reasoning\nfor visual perception and language comprehension, while\nalso facilitating further trajectory planning. Recent works\nabout VLMs/LLMs in robotic manipulation are provided\nin Table 5.\nFor\n vision  and  language  manipulation  algorithms,\ncurrent  approaches  primarily  utilize  VLMs  for  scene\nunderstanding and LLMs for natural language command\ncomprehension,  combined  with  planning  algorithms  to\ngenerate trajectory key points or dense waypoints for the\nend-effector,  as  illustrated  in Fig.  7.  These  methods  can\nbe\n categorised  into  two  types:  one  involves  designing\ncomplex prompts for VLMs and LLMs to directly generate\ntrajectories,  referred  to  as planning-based  vision  and\nlanguage  manipulation,  and  the  other  employs  VLMs\nand  LLMs  to  assist  in  policy  generation  within  robot\nlearning  to  accomplish  manipulation  tasks,  known  as\nlearning-based  vision  and  language  manipulation.\nRegarding  application  scenarios,  most  work  applying\nVLMs/LLMs to robotic manipulation focuses on tabletop\nhousehold tasks, with some studies extending their use to\nindustrial settings. \n4.3.1    Planning-based vision and language manipulation\nfor t\nabletop household tasks\nPlanning-based  vision  and  language  manipulation  is  an\napproach  in  robotic  manipulation  where  VLMs  and\n \nTable 5    VLMs/LLMs in manipulation\nMethod VLM/LLM Success rate Application Year Ref.\nC\nLIP-SemFeat CLIP 58.8% (LVIS) Scene rearrangement, tabletop\nhousehold tasks 2022 Goodwin et al. ( 2022)\nAct3D CLIP ResNet50 83% (RLBENCH) Tabletop household tasks 2023 Gervet et al. ( 2023)\nCALAMARI CLIP 84% (RLBENCH) Contact-rich manipulation,\ntabletop household tasks 2023 Wi et al. ( 2023)\nGNFactor Stable diffusion, CLIP 31.7% (RLBENCH) Tabletop household tasks 2023 Ze et al. ( 2023)\nGVCCI Grounding entity transformer (Get),\nSetting entity transformer (Set) 50.98% (VGPI) Pick and place task, tabletop\nhousehold tasks 2023 Kim et al. ( 2023a)\nMOO\nOwl-ViT ~50% (RT-1) Tabletop household tasks 2023 Stone et al. ( 2023)\nPa\nLM-E PaLM-E 66.1% (OK-VQA) Tabletop household tasks 2023 Driess et al. ( 2023)\nSMS ViLD, BLIP-2, SAM 41.7% (custom dataset) Mechanical search, tabletop\nhousehold tasks 2023 Sharma et al. ( 2023)\nVoxposer GPT-4, OWL-ViT, SAM 88% (RLBENCH) Tabletop household tasks 2023 Huang et al. ( 2023b)\nVi\nsion-language guided\nrobotic planning CLIP, GPT-4 93.3% (custom dataset) Human–robot collaboration 2024 Fan and Zheng ( 2024)\n \nFig. 7    Ge neral approach of vision and language manipulation adapted with permission from Huang et al. ( 2023b).\n188 Front\n. Eng. Manag. 2025, 12(1): 177–200\nLLMs are utilized to generate precise movement trajecto-\nri\nes for robotic tasks. This method involves the design of\ncomplex natural language prompts that instruct LLMs to\nproduce detailed paths that the robot's end-effector should\nfollow. Goodwin et al. ( 2022) proposed a novel method\nc\nalled CLIP-SemFeat for scene rearrangement in Tabletop\nhousehold  tasks  by  leveraging  CLIP  to  solve  cross-\ninstance  matching  problems,  and  have  reached  a  58.5%\ntask  success  rate  in  the  LVIS  data  set.  Driess  et  al.\n(2023)  introduced  a  VLM  for  robot  manipulation  called\nPa\nLM-E, which can translate knowledge from the vision-\nlanguage domain into concrete reasoning, enabling robots\nto  plan  in  environments  with  complex  dynamics  and\nphysical  constraints  and  to  answer  questions  about  the\nobservable  world.  PaLM-E  could  achieve  a  66.1%  task\nsuccess  rate  in  the  OK-VQA  data  set.  Sharma  et  al.\n(2023)  presented  SME  consisting  of  ViLD,  BLIP-2  and\nSAM\n for  mechanic  searching  for  tabletop  household\ntasks,  and  could  reach  a  41.7%  task  success  rate  in  the\ncustom  data  set.  Huang  et  al.  (2023b)  proposed  a  novel\nfra\nmework  named  Voxposer  for  model-based  planning.\nThis framework utilized ViLD for object detection, SAM\nfor semantic segmentation, and GPT-4 for reasoning and\nplanning.  VoxPoser's  planning  precision  reached  the\nwaypoint  level  of  robotic  arm  movement  trajectories,\nmaking manipulation tasks highly flexible. It achieved an\n88%  task  success  rate  in  the  RLBENCH  tasks.  While\nthese  approaches  demonstrate  significant  progress  in\nhousehold  settings,  the  current  methods  often  lack  the\nprecision  required  for  complex  industrial  tasks,  under-\nscoring  the  need  for  further  research  to  enhance  motion\nplanning precision and extend their applicability to indus-\ntrial environments (Challenge 6.4). \n4.3.2    Learning-based vision and language manipulation\nfor t\nabletop household tasks\nLearning-based  vision  and  language  manipulation  is  an\napproach  where  VLMs  and  LLMs  are  used  to  assist  in\ngenerating policies for robot learning. Instead of directly\ngenerating trajectories, this method focuses on developing\npolicies that enable robots to learn and perform manipula-\ntion  tasks  through  experience  and  interaction  with  the\nenvironment. Gervet et al. (2023) proposed Act3D, utiliz-\ni\nng  CLIP  ResNet50  and  leveraging  their  shared  vision-\nlanguage feature space to interpret instructions and foun-\ndational  references  for  learning-from-demonstration\ntasks.  They  achieved  an  average  success  rate  of  83%  in\nRLBENCH tasks. Wi et al. (2023) introduced CALAMARI\nfor\n contact  manipulation  in  household  environments\nbased on CLIP and obtained a success rate of 90% in the\nwipe_desk  task,  84%  in  the sweep_to_dustpan  task,  and\n60% in the  push_putton task in the RLBENCH environ-\nment. Ze et al. (2023) presented a GNFactor model based\non\n stable  diffusion  and  CLIP  for  imitation  learning  in\nrobot  manipulation  tasks  and  gained  an  average  success\nrate  of  31.7%  in  10  tasks  in  RLBENCH.  Kim  et  al.\n(2023a) proposed GVCCI consisting of Grounding Entity\nT\nransformer  (Get)  and  Setting  Entity  Transformer  (Set)\nfor  pick  and  place  actions,  and  obtained  a  task  success\nrate  of  50.98%  in  Visual  Grounding  on  the  Pick-and-\nplace  Instruction  (VGPI)  data  set.  Stone  et  al.  (2023)\ni\nntroduced MOO in which Owl-ViT is applied to object-\nidentifying information from the language command and\nimage. They have reached about a 50% task success rate\nin the RT-1 data set. This section focuses on the task of\nmanipulation, with more detailed discussions on learning-\nbased  methods  to  be  presented  in  Section  5.  Despite\nnotable advancements, the variability in task success rates\nacross  different  models  and  tasks  indicates  that  these\nlearning-based  approaches  need  further  refinement  to\nenhance adaptability and precision, particularly for more\ncomplex and varied manipulation tasks beyond household\nenvironments (Challenge 6.4). \n4.3.3    Vision and language manipulation for industrial\nt\nasks\nIt  is  found  that  most  works  regarding  vision-language\nrobotic  manipulation  are  based  on  RLBENCH  and  thus\nonly  consider  household  tasks.  The  investigations  of\nVLM-based manipulation are still quite rare in the indus-\ntrial sector, and current applications have been limited to\nsimple  planning-based  methods.  Fan  and  Zheng  (2024)\ni\nnnovatively  introduced  a  VLM-based  manipulation\nmethod in HRC tasks. They proposed a vision-language-\nguided robotic planning approach for robotic action plan-\nning  for  HRC  assembly,  and  have  reached  a  93.3%\nsuccess  rate  in  their  custom  industrial  data  set.  In  the\nindustrial sector, VLM-based manipulation, in addition to\ncompleting  HRC  assembly  tasks,  has  potential  value  in\nwarehouse management, equipment maintenance, flexible\nproduction  line  adjustments,  and  item  sorting,  and  is\nworth further exploration.\nIt is important to note that while planning-based methods\nlike  CLIP-SemFeat  and  Voxposer  excel  in  task  success\nrates  for  household  tasks,  they  often  struggle  with  the\nprecision required for industrial applications. In contrast,\nlearning-based  approaches  such  as  Act3D  and  CALA-\nMARI  show  promise  in  adaptability  through  learning\nfrom  interactions  but  display  variability  in  success  rates\nacross  different  tasks.  Identifying  these  trends  and\ncommon  challenges  suggests  a  potential  for  integrating\nthe strengths of both approaches to improve precision and\nadaptability, particularly in transitioning from household\nto industrial applications. This synthesis highlights a crit-\nical  trend  toward  developing  hybrid  models  that  can\nbridge  current  gaps,  offering  a  more  robust  solution\nacross  varied  environments.  The  adaptation  of  VLM-\nbased manipulation from household to industrial applica-\ntions faces challenges such as the need for higher preci-\nsion,  robustness  in  diverse  conditions,  and  integration\nJunming FAN et al. Vision-language model-based human-robot collaboration in smart manufacturing 189\nwith  existing  industrial  systems,  indicating  significant\nopport\nunities  for  future  research  to  bridge  these  gaps\nand fully realize its potential in industrial settings (Chal-\nlenge 6.4). \n5    VLM-based human-guided skill transfer\na\nnd robot learning\nAnother  crucial  challenge  in  human–robot  collaborative\nmanufacturing is to enable robots to effectively learn new\nskills and actions, especially in unstructured and dynamic\nenvironments where pre-programmed movements are in-\nsufficient. One potential solution is to allow robots to ac-\nquire new skills from human demonstrations ( Yin et al.,\n2024).  Learning  from  demonstration,  or  imitation  learn-\ni\nng, aims to enable robots to extract human behaviors and\ndecision-making processes. This encompasses a range of\ncompetencies,  including  operational  skills,  sequence\nplanning, adaptability to different scenarios, and even the\nability to handle uncertainty and weigh pros and cons.\nCompared  to  traditional  algorithms,  VLM-based  skill\ntransfer has attracted increasing attention due to its power-\nful  in-context  learning  capabilities.  VLM  not  only  inte-\ngrates  the  efficient  feature  representation  capabilities  of\nboth vision and language, but also, due to its pre-training\non large-scale data, possesses strong generalisation abil-\nity.  This  allows  it  to  perform  exceptionally  well  in  new\ntasks and environments, reducing the need for extensive\ndemonstration  data.  Hence,  the  advancement  of  VLMs\nhas enabled robots to exhibit enhanced logical reasoning\nand compositional generalisation abilities in manufactur-\ning tasks.\nIn  this  section,  the  latest  progress  in  VLM-based\nhuman-guided skill transfer and robot learning are elabo-\nrated,  as  illustrated  in Fig.  8.  Our  focus  is  primarily  on\nl\niterature that leverages both vision and language modali-\nties  to  collect  human  demonstrations  and  facilitate  skill\nlearning.  The  main  content  is  divided  into  two  parts:  1)\nhigh-quality  human  demonstration  gathering  approaches\nand  2)  VLM-based  robot  learning  algorithms  and  archi-\ntectures. \n5.1    VLM-based human demonstration collection\nObt\naining human demonstrations has long been a crucial\nresearch focus in enabling robots to acquire human skills.\nThe integration of multiple modalities, such as vision and\nlanguage,  cannot  only  help  robots  better  comprehend\nhuman  demonstrations  but  also  enable  them  to  learn\nskills more robustly in partially observable environments.\nFigure 9 showcases the primary demonstration methods,\nhi\nghlighting the innovative approaches to human teaching\nenabled  by  vision  language  integration.  As  shown  in\nTable  6,  the  main  benefits  of  including  language  in  the\nde\nmonstration  process  have  been  summarized  through\nrelated  works.  Specifically,  compared  to  passive  video\nobservation, integrating the language modality can align\nmore closely with human cognition, improve comprehen-\nsion and generalisation capabilities, extract explicit rules,\nenhance human–robot interaction, and support the learning\nof more complex manipulation skills. \n5.1.1    Align with human cognition\nHum\nans typically learn new skills by combining the ob-\nservation of demonstrated actions with listening to related\nverbal explanations. Thus, integrating video with textual\nor  spoken  information  can  more  closely  mimic  human\nlearning  processes.  Azagra  et  al.  (2020)  designed  an\ni\nncremental learning pipeline. Through natural user inte-\nractions  (such  as  pointing,  showing,  and  verbal  descrip-\ntions), the robot utilizes an RGB camera to capture image\ndata and combines skeleton detection and speech recogni-\ntion technologies to incrementally learn and update object\nmodels. This enables robots to gradually learn to accurately\nidentify and understand different objects in dynamic and\ndiverse interactive environments, just like humans. Ding\net al. (2023) proposed a system called Embodied Concept\nL\nearner (ECL), enabling robots to emulate human capa-\nbilities  in  learning  visual  concepts  and  understanding\ngeometric  mapping  within  an  interactive  3D  environ-\nment. The robotic agent, akin to a baby, can acquire long-\nterm,  interpretable,  unsupervised  semantic  and  depth\nlearning through interactions with humans. \n \nFig. 8    VL M-based human-guided skill transfer and robot learning.\n190 Front. Eng. Manag. 2025, 12(1): 177–200\n5.1.2    Improve generalisation capability\nIn\n learning  from  human  demonstration,  purely  video-\nbased passive observation may lead robots to struggle to\ncapture  the  underlying  intentions  and  semantics  of\nactions.  In  contrast,  language  information  can  provide\ncrucial  contextual  details,  aiding  robots  in  better  under-\nstanding the purpose and logic of demonstrations, thereby\nenhancing learning accuracy and generalisation capabili-\nties.  For  instance,  Yin  and  Zhang  (2023)  presented  a\nfra\nmework  that  integrates  key  frame  extraction  with\nmultimodal information (text caption) fusion, significantly\nimproving  the  accuracy  of  robot  command  generation.\nTherefore, when integrated with an affordance detection\nnetwork  and  a  motion  planner,  this  framework  enables\nrobots to effectively reproduce the tasks demonstrated. \n5.1.3    Extract implicit rules\nW\nhen robots learn from human demonstrations, they can\nacquire implicit rules and subtle habits that are difficult to\nexplicitly  abstract  into  clear  guidelines,  unlike  auton-\nomous  learning  methods  such  as  reinforcement  learning\n(RL).  By  integrating  visual  and  linguistic  inputs,  robots\nare  better  equipped  to  uncover  these  latent  policy\npatterns. Ramrakhya et al. (2022) discovered that imitation\n \nFig. 9    VL M-based human demonstration collection: (a) Human-like teaching methods ( Azagra et al., 2020), including point, show, and\nspe\nak; (b) Vision-language fusion and goal parsing ( Ding et al., 2023), to assist robot in learning concepts through rich information and\nse\nlf-supervised  learning;  (c)  Gesture-based  teaching  and  teleoperation  methods  (Halim  et  al.,  2022),  to  provide  natural  interactions  for\ndi\nrect programming of the robot; (d) human demonstration at scale (Ramrakhya et al., 2022), to learn the skill preferences of object navigation\nfrom\n different human agents.\n \nTable 6    VLM-based human demonstration collection\nCategory Description Demonstration method Tasks/Application Ref.\nAl\nign more closely with\nhuman cognition\nIncremental Learning of Object Models\nfrom interactive human demonstration\nRGB camera, microphone Recognise and understand\nobject\nAzagra et al. (2020)\nSelf-supervised learning of concepts and\nmapping through instruction following\nin an interactive 3D environment\nRGB observation, Language\ncommand\nRoutine manipulation tasks Ding et al. ( 2023)\nImprove generalisation\ncapability\nImprove robot command\ngeneration accuracy\nVideo, Key frame caption Routine manipulation tasks Yin and Zhang ( 2023)\nExtract implicit rules A virtual teleoperation infrastructure to\ncollect large-scale demonstrations\nRGBD camera, GPS+compass\nsensor, Task instruction\nNavigation, pick and\nplace tasks\nRamrakhya et al. (2022)\nBehaviour-informed state abstractions\nvia language model queries to capture\nhuman task-relevant preferences\nVideo, language specification Tabletop manipulation tasks Peng et al. ( 2024)\nEnhance human–robot\ninteraction\nMultimodal no-code robotic\nprogramming\nRGB-D camera, Microphone Line movement, zigzag\nmovement, contour\nmovement, etc.\nHalim et al. (2022)\nMul\ntimodal demonstration interface 3D camera, speech, hand\nguiding\nPick and place tasks Lu et al. ( 2022)\nSupport more complex\nmanipulation skill learning\nMultimodal human demonstration\ncollection system\nMotion capture, depth camera,\nvideo recording, speech\nComplex tool\nmanipulation skills\nShukla et al. (2023\n)\nJunming FAN et al. Vision-language model-based human-robot collaboration in smart manufacturing 191\nlearning (IL) enables robots to learn effective object navi-\nga\ntion skills from humans—such as peeking into rooms,\nchecking corners for small objects, and turning around to\nget  a  panoramic  view.  This  surpasses  the  limitations  of\ntraditional  reinforcement  learning  (RL)  methods,  which\nrequire  intricate  reward  engineering  to  induce  such\nbehaviors.  Peng  et  al.  (2024)  proposed  a  Preference-\nC\nonditioned  Language-Guided  Abstraction  (PLGA)\nmethod,  using  language  model  (LM)  queries  to  capture\nimplicit human preferences in tasks. For instance, observ-\ning  behavior  changes  like  avoiding  electronics  during\n“throwing away a jar” allows the robot to infer and train\nstrategies based on these abstracted preferences. Notably,\nrobots  may  inadvertently  replicate  undesirable  human\nbiases when learning implicit rules, an issue that warrants\ncareful consideration in future research. \n5.1.4    Enhance human–robot interaction\nC\nompared to passive video observation, leveraging robot\nteleoperation to gather demonstration data is a more accu-\nrate and efficient approach. This natural teaching method,\nwhich combines action demonstration and language inter-\npretation,  leads  to  more  intuitive  and  effective\nhuman–robot  interaction.  By  grounding  the  learning\nprocess  in  multimodal  interactions,  Halim  et  al.  (2022)\ni\nntroduced  a  no-code  robotic  programming  system\ndesigned  for  beginners.  This  approach  utilizes  a  visual\nsystem  that  enables  users  to  convey  spatial  information,\nincluding 3D points, lines, and trajectories, through hand\nand  finger  gestures.  Additionally,  a  speech  recognition\nsystem aids users in setting robot parameters and engaging\nwith  the  robot's  state  machine.  Lu  et  al.  (2022)  also\npropose\nd a multimodal demonstration system integrating\nnatural language instruction, visual observation, and hand\nguiding to let robot learn task comprising goal concepts,\ntask  plans,  and  basic  actions,  which  can  be  applied  to\npick-and-place  tasks.  However,  some  interaction\napproaches  may  introduce  additional  implementation\ncosts  and  system  complexity.  It  is  essential  to  consider\nthe  scalability  and  practicality  of  interaction  methods  to\nensure the applicability across broader real-world scenar-\nios. \n5.1.5    Support more complex manipulation skill learning\nAc\nquiring complex manipulation skills often necessitates\nthe  use  of  multimodal  information.  Humans  not  only\ngrasp the actions required for a skill, but also understand\nthe various states, state transitions, and constraints associ-\nated with the task. Shukla et al. ( 2023) introduced a mul-\nt\nimodal framework supporting data collection from various\nmodalities, including speech, gestures, motion, video, and\n3D  depth  data.  This  framework  integrates  visual  and\nlinguistic  data  to  gather  rich  human  demonstration  data\nfor learning intricate tool operation skills, such as granular\nmedia  transport  tasks.  It  is  evident  that,  although  visual\nand linguistic cues have been widely utilized to facilitate\nhuman–robot  skill  transfer,  the  integration  of  additional\nmultimodal  sensory  data  remains  underexplored  (Chal-\nlenge 6.5). \n5.2    VLM-based robot learning from human\nde\nmonstrations\nIn  this  section,  we  delve  into  the  details  of  the  learning\nmechanisms  employed  by  researchers  utilizing  the  rich\ninformation  from  vision-language  integrated  human\ndemonstration  data.  This  exploration  encompasses  a\nspectrum of encoding, decoding, and learning strategies.\nThe learning process is systematically divided into three\nkey  areas:  vision-language  fusion,  the  combination  of\nimitation  and  reinforcement  learning,  and  enhancements\nin adaptability.  Figure 10 illustrates a typical application\nfor\n each  of  these  three  subdomains. Table  7  provides  a\nde\ntailed  account  of  the  learning  methods,  training  data\nsets,  and  evaluation  metrics  utilized  in  the  reviewed\narticles. \n5.2.1    Vision-language fusion and multimodal learning\nVi\nsion-language  fusion  and  multimodal  learning  offer\nsignificant advantages in the field of robotic manipulation\nand  human–robot  interaction.  By  integrating  visual  and\nlinguistic information, these approaches enable robots to\nunderstand and execute complex tasks with greater accu-\nracy  and  flexibility.  The  fusion  of  multiple  modalities\nallows for a richer and more comprehensive understanding\nof  the  environment,  facilitating  more  robust  decision-\nmaking processes.\nFor example, Shao et al. (2021) presented a new imitation\nl\nearning framework combining natural language instruc-\ntions and visual inputs for enhanced robot manipulation,\noffering  improved  generalisation,  efficient  learning,  and\nhandling  of  complex  manipulations.  Evaluation  results\nshow that the multi-task strategy performs well in terms\nof success rate and handling complex tasks. Wang et al.\n(2022b) put forward a novel Teaching-Learning-Prediction\n(T\nLP)  framework  that  enables  robots  to  predict  human\nintentions  in  hand-over  tasks  using  multimodal  data,\nincluding  natural  language  and  wearable  sensor  inputs.\nLeveraging  the  extreme  learning  machine  (ELM)  algo-\nrithm, the TLP framework significantly improves predic-\ntion accuracy and stability compared to traditional meth-\nods,  facilitating  more  efficient  human–robot  collabora-\ntion. Hori et al. (2023) put forward a multimodal imitation\nl\nearning  approach  that  leverages  videos,  audio,  and  text\nto generate robot action sequences using an audio-visual\nTransformer  (AVTransformer).  This  method  integrates\ndynamic  motion  primitives  (DMPs)  and  style  transfer\n192 Front. Eng. Manag. 2025, 12(1): 177–200\nlearning  to  enhance  performance.  CSATO  algorithm\n(Han  et  al.,  2024)  employed  a  visual-language  fusion\nne\ntwork,  a  token  reduction  network,  and  a  Transformer\ndecoder  to  model  correlations  among  instructions,\ncurrent,  and  historial  visual  observations,  generating\nautoregressive action predictions.\n \n \nFig. 10    VL M-based robot learning from human demonstrations, which is primarily divided into three sections: Vision language fusion\nand  multimodal  learning  (Wang  et  al.,  2022b),  IL  and  RL  combination  (Trick  et  al.,  2022),  and  task  and  environmental  adaptability\ne\nnhancement (Nair et al., 2022).\n \nTable 7    VLM-based robot learning from human demonstrations\nDescription Method Training data Evaluation Ref.\nMul\ntimodal\ndata fusion\nLearn manipulation\nconcept\nBERT, ResNet-18,\nDDPG, Batch RL\n78 tasks from “Something-\nSomething” dataset\n(video, task description)\nMulti-task success rate 76.3% Shao et al. (2021)\nPredicting human\nintentions\nTLP model, Extreme\nLearning Machine\n25000 Multimodal data from\nfive participants\nPrediction accuracy 98.5%,\ntime efficiency improved\nWang et al. (2022b)\nAc\ntion sequence\nacquisition\nAVTransformer, CLIP Epic-Kitchen-100, YouCookII,\nQuerYD, and in-house\ninstruction video datasets\nSuccess rate 32 %, improves\nDMP sequence quality\nby 2.3 times\nHori et al. (2023)\nC\nSATO multi-task\nskill learning\nTransformer\narchitecture\nRLBench including RGB image,\ndepth map, instruction,\naction sequence\nSuccess rates of 90.7% and 83.9%\nin single-task and multi-task\nscenarios\nHan et al. (2024)\nIL and RL\ncombination\nZero-shot task\ngeneralisation\nBC-Z VR teleoperation data, human\nvideo, task language strings\n24 unseen manipulation tasks\nwith 44% success rate\nJang et al. (2022)\nInt\neractive\nreinforcement learning\nMIA-IRL Train the classifier with collected\nspeech and gesture\nConvergence time, success rate,\nand robustness are\nsignificantly higher.\nTrick et al. (2022)\nR\noboCLIP: one demo\nto learn policies\nS3D VLM, RL (PPO) HowTo100M to pretrain VLM,\nMetaWorld for simulation\n2–3 times higher zero-shot\nperformance than\ncompeting IL methods\nSontakke et al. (2024)\nAda\nptability\nenhancement\nR3M: pretrained visual\nrepresentation\nResNet-18, ResNet-34 Egocentric 4D Average success rate 56% Nair et al. (2022)\nL\nearning Object Spatial\nRelationship\nSpatial probability\nmodels, SVM, GPT3\nNUSCENES dataset and\ntabletop scenes\nAccuracy 91.3% Yu et al. (2023)\nGNFactor multi-Task\nskill learning\nStable Diffusion, CLIP 10 tasks in RLBench Success rate 31.7% Ze et al. (2023)\nUser directed\nhierarchical learning\nCLIP, PerAct,\nGPT-4, Bard\nRLBench including RGB-D, text\ninstruction, voxel representation\n13% improvement in task\nsuccess rates\nWinge et al. (2024)\nSHOWTELL GPT-4,ViLD, BLIP-v2 Text instruction, RGB data, hand\ndetection, hand-object interaction\nSuccess rate over 85%,\nout-perform GPT4-V\nMurray et al. (2024)\nJunming FAN et al. Vision-language model-based human-robot collaboration in smart manufacturing 193\n5.2.2    Combination of IL and RL\nT\nhe combination of imitation learning and reinforcement\nlearning offers powerful synergy, significantly enhancing\ntask  generalisation  and  zero-shot  learning  capabilities  in\nrobotic  systems.  Imitation  learning  provides  a  strong\nfoundation  by  allowing  robots  to  quickly  acquire\ncomplex  behaviors  from  human  demonstrations,  effec-\ntively  reducing  the  initial  learning  curve  and  enabling\nrapid  adaptation  to  new  tasks.  Reinforcement  learning\ncomplements this by refining and optimising the learned\nbehaviors  through  trial  and  error,  guided  by  reward\nsignals. By integrating the strengths of both approaches,\nrobots  can  achieve  robust  task  generalisation,  applying\nlearned strategies to a wide array of scenarios with minimal\nretraining.\nJang  et  al.  (2022)  combined  a  multilingual  sentence\ne\nncoder for vision-based robotic manipulation. Using the\nBC-Z  model,  it  trains  on  a  large  data  set  of  human\ndemonstrations,  achieving  success  in  zero-shot  task\ngeneralisation  and  unseen  task  performance.  Trick  et  al.\n(2022)  introduced  an  interactive  reinforcement  learning\n(IR\nL)  approach  that  leverages  the  Bayesian  fusion  of\nmultimodal  advice.  The  proposed  method,  MIA-IRL,\nenables  a  robot  to  learn  the  pancake-making  task  from\nvarious  initial  states  by  incorporating  human-provided\nmultimodal  guidance,  such  as  speech  and  gestures.\nExperimental  results  demonstrate  that  the  MIA-IRL\napproach achieves faster convergence and greater robust-\nness in this task compared to existing methods. Sontakke\net al. (2024) presented RoboCLIP, an innovative imitation\nl\nearning  method  that  uses  pretrained  VLMs  to  generate\nreward  functions  from  a  single  demonstration  (video  or\ntext). It reduces the reliance on expert demonstrations and\ncomplex  reward  engineering,  leading  to  superior  zero-\nshot performance and efficient fine-tuning. However, the\nchallenges  of  integrating  these  methods  remain  signifi-\ncant, including the complexity of designing effective rew-\nard functions for various tasks in reinforcement learning,\nas well as ensuring data and training environment consis-\ntency between reinforcement and imitation learning. \n5.2.3    Task and environmental adaptability enhancement\nOt\nher applications also explore the potential of VLM in\nlearning spatial relationships to further improve task and\nenvironmental  adaptability  of  robots.  Specifically,  Nair\net  al.  (2022)  presented  R3M,  a  visual  representation\nm\nodel pretrained using a combination of time-contrastive\nlearning,  video-language  alignment,  and  L1  sparsity\npenalties,  which  significantly  enhances  data-efficient\nimitation  learning  for  robotic  manipulation  tasks.  By\nleveraging the diverse human video data set Ego4D, R3M\noutperforms  state-of-the-art  visual  representations  like\nCLIP and MoCo, demonstrating superior performance in\nunseen environments and tasks. Yu et al. (2023) proposed\na\n method  enabling  robots  to  learn  and  recognize  3D\nspatial  relationships  between  objects.  By  leveraging\nimage  and  language  demonstrations,  the  method  const-\nructs  new  spatial  relationship  probability  distribution\nmodels,  thereby  allowing  robots  to  execute  tasks  more\naccurately  in  complex  environments.  Regarding  visual\nrepresentation and encoding, Ze et al. ( 2023) introduced\nGNFa\nctor, a visual BC agent for multi-task robotic manip-\nulation  concentrating  on  improved  generalisation  abili-\nties.  It  leverages  the  Stable  Diffusion  model  to  encode\nsemantic  information  into  3D  voxel  representations,\nenabling  visual  reconstruction  and  language-conditioned\naction prediction. Winge et al. ( 2024) proposed a hierar-\nc\nhical robot learning framework, in which end users can\nprovide  text  instructions  related  to  observation  (spatial\nscenario).  Combined  with  the  VLM  Bard's  capability  to\nautomatically decompose complex tasks into intermediate\nskills,  robots  can  effectively  learn  to  execute  high-level\ntasks  by  understanding  the  environment  and  building\nfrom  basic  actions.  A  highly  modular  neural-symbolic\nframework is also introduced by Murray et al. ( 2024), for\nsynt\nhesizing  robotic  skills  from  visual  demonstrations\nand natural language instructions. Current methodologies\noften  assume  an  idealised  training  environment,  where\nobjects are expected to maintain reasonable initial poses\nand training steps are standardised. However, when these\nalgorithms are applied in real-world scenarios, they face a\nmuch  broader  spectrum  of  uncertainties,  rendering  them\ninsufficient  without  human  oversight.  This  reliance  on\ncontinuous  human  intervention  for  monitoring  and\nadjustment severely limits their practical implementation\nin real-world applications (Challenge 6.6). \n6    Challenges and future perspectives\nAl\nthough  preliminary  explorations  of  the  application  of\nVLMs  have  demonstrated  impressive  capabilities,  espe-\ncially within the human–robot collaborative manufacturing\narea, many technical issues have not yet been well cons-\nidered or addressed, which largely constrains the applica-\nbility of VLMs in practical real-life environments. In this\nsection,  some  key  challenges  and  corresponding  future\ndirections are discussed in the hope of motivating further\nresearch endeavors for VLM-based HRC systems. \n6.1    Data and computation-efficient training and\nde\nployment of VLMs\nThe  pretraining  and  deployment  of  VLMs  in  practical\nmanufacturing scenes face significant challenges, primar-\nily due to the high computational demands and extensive\ndata  requirements  for  training  these  models.  Obtaining\nhigh-quality, annotated data sets in diverse manufacturing\n194 Front. Eng. Manag. 2025, 12(1): 177–200\nenvironments  is  costly  and  time-consuming.  Addition-\na\nlly,  effective  HRC  applications  require  real-time  proc-\nessing, but VLMs often face latency issues. Meanwhile,\nin  real-life  production  scenarios,  VLMs  must  also  be\nrobust  and  reliable  to  handle  variations  and  ambiguities\neffectively.  Enhancing  robustness  through  rigorous  tes-\nting  and  incorporating  fail-safe  mechanisms  is  essential\nfor  reliable  deployment.  Potential  pathways  to  addres-\nsing  these  issues  include  efficient  training  strategies,\nmodel optimisation techniques such as pruning and quan-\ntisation,  and  robust  data  collection  methods.  These\napproaches  are  essential  for  the  practical  and  effective\ntraining and deployment of VLMs in HRC manufacturing\napplications. \n6.2    Vision and language task planning in dynamic\ne\nnvironments\nOne  of  the  significant  challenges  in  VLM-based  task\nplanning  is  the  current  focus  on  static  scenes,  which\nlimits the applicability of these models in dynamic envi-\nronments.  Real-time  task  planning  in  dynamic  scenes\nremains  an  unresolved  issue.  To  address  this,  future\nresearch  could  explore  the  integration  of  Simultaneous\nLocalization  and  Mapping  (SLAM)  technology.  SLAM\ncan  provide  real-time  updates  of  the  environment,\nenabling VLMs to adapt and plan tasks dynamically. This\nintegration  would  allow  robots  to  navigate  and  perform\ntasks in ever-changing environments, significantly enhan-\ncing their utility and robustness. Additionally, improving\nthe computational efficiency of VLMs to handle real-time\ndata  and  developing  more  sophisticated  algorithms  for\ndynamic task planning are crucial areas for future explo-\nration. \n6.3    Real-time 3D scene reconstruction and segmentation\nfor vi\nsion and language navigation\nDespite  the  significant  advancements  in  VLN  brought\nabout by VLMs and LLMs, their application in industrial\nmanufacturing remains limited. Current navigation plan-\nning  relies  on  pre-established  static  maps,  but  in  real-\nworld  scenarios,  both  humans  and  machines  can  be\nmobile.  Therefore,  real-time  updates  of  3D  scene  maps\nare  crucial.  However,  most  3D  reconstruction  and\nsegmentation techniques rely on RGB-D video frames for\npoint  cloud  reconstruction  and  color  rendering,  which\nresults  in  long  inference  times  and  significant  delays  in\nreal-time  reconstruction.  This  limitation  restricts  their\napplication in real industrial settings. Achieving fast, low-\nlatency real-time 3D reconstruction and segmentation is a\ncritical research direction for the future. A potential solu-\ntion  involves  combining  large  models  with  lightweight\nnetworks  and  dynamic  tracking  by  human  operators,\nenabling  efficient,  low-latency  3D  scene  updates  and\nadaptability in industrial environments. \n6.4    Motion planning with high precision for vision and\nl\nanguage manipulation\nVLM  and  LLM-based  robotic  manipulation  is  a  highly\nactive  research  topic  in  the  fields  of  AI  and  robotics.\nHowever, the focus has predominantly been on household\ntasks,  with  limited  work  in  industrial  applications.  One\nreason is that the planning precision of LLMs is not yet\nsufficient  for  industrial  requirements.  Currently,\nVLM/LLM-based robotic manipulation can only perform\ntasks such as “picking up a cup and pouring water,” but\nassembly tasks are much more complex and precise, such\nas aligning gears or installing screws. These tasks require\nmotion  planning  with  millimeter-level  or  even  finer\nprecision. VLMs and LLMs are known for their flexibility\nand  generalisation,  which  inevitably  leads  to  a  decrease\nin  precision  for  specific  tasks.  Enhancing  the  motion\nplanning  precision  of  VLM/LLM-based  manipulation\nto  achieve  a  balance  of  flexibility,  generalisation,  and\naccuracy is a promising research direction. To address the\nchallenge of achieving high precision in motion planning\nfor vision and language manipulation in industrial appli-\ncations,  integrating  advanced  sensor  technologies  and\nfeedback control systems may offer a viable solution. \n6.5    Additional modalities and complex instruction\nunde\nrstanding\nIntegrating  additional  modalities  into  VLM-guided\nhuman–robot  skill  transfer  could  enhance  a  robot's\ncontextual  understanding  and  skill  acquisition.  While\nvisual  and  linguistic  cues  furnish  robots  with  spatial\nsemantics,  the  incorporation  of  haptic  feedback  through\nadvanced sensors is pivotal. This addition enables robots\nwith precise force information, thereby augmenting their\ncapacity to execute complex tasks that necessitate delicate\nforce control. The multimodal approach not only enriches\nthe sensory feedback loop for real-time action adjustment\nbut also broadens the robot's interaction repertoire.\nFurthermore, existing research often confines language\ninstructions  to  a  simplistic  format,  typically  a  “(verb)\n(noun)”  structure.  However,  advancing  toward  more\ncomplex  linguistic  instructions  is  crucial  for  enhancing\nreal-world applicability. The current limitations of VLMs\nin  handling  intricate  or  contextually  dependent  instruc-\ntions, especially in scenarios with dynamic task or action\nsequences, require improvement. A key focus should be\nenhancing  VLMs  to  comprehend  multi-step  logical\ninstructions and clarify non-standard linguistic cues. For\nexample, integrating advanced natural language processing\ntechniques, such as context-aware transformers and hier-\narchical  task  analysis,  can  significantly  improve  the\ncomprehension of complex instructions. This evolution is\nvital for enabling robots with the sophistication required\nto  navigate  and  adapt  effectively  in  diverse  real-world\nscenarios. \nJunming FAN et al. Vi\n sion-language model-based human-robot collaboration in smart manufacturing 195\n6.6    Dynamic task adaptation and unsupervised\ne\nvaluation\nTo  facilitate  the  effective  transfer  of  skills  acquired  in\nsimulations to real-world applications, learning algorithms\nmust possess robustness to real-world variability, encom-\npassing factors such as fluctuating lighting and discrepan-\ncies in physical engine accuracy. However, current meth-\nods  always  rely  on  an  assumption  of  an  ideal  training\nenvironment, with expectations of reasonable object orig-\ninal poses or a standardised training step. However, real-\nworld  deployment  confronts  these  algorithms  with  a  far\ngreater  range  of  unpredictability,  which  necessitates\nhuman operators for continuous monitoring and interven-\ntion. This significantly hinders real applications.\nTo overcome this, robots must evolve to autonomously\nadapt  to  dynamic  environments  and  tasks,  incorporating\nmultimodal  feedback  for  self-adjustment.  This  may\nrequire  an  advanced  approach  that  integrates  domain\nrandomization and augmented reality for training, inten-\ntionally introducing a broad spectrum of real-world noise\nand variability within simulated environments. By doing\nso, skill transfer models can be equipped with enhanced\ngeneralisation abilities. Moreover, the implementation of\nunsupervised  evaluation  mechanisms  can  establish  a\nrobust self-assessment framework. Leveraging the obser-\nvation  of  robot  behavior  or  state  changes  could  reduce\nconstant human supervision when faced with crucial situ-\nations.  Specifically,  implementing  a  continuous  learning\nmechanism  allows  robots  to  adapt  and  improve  their\nperformance autonomously over time based on new expe-\nriences and feedback from the environment. \n7    Conclusions\nT\nhis survey investigated recent advancements and appli-\ncations of VLMs in HRC for smart manufacturing, high-\nlighting  their  potentials  and  current  limitations.  Starting\nwith  an  overview  of  the  fundamental  knowledge  of\nLLMs and VLMs, it detailed how integrating visual and\ntextual  data  enhances  robot  planning,  execution,  and\nlearning  capabilities.  One  can  observe  that  initial  explo-\nrations of VLMs in robotic task planning, navigation, and\nmanipulation  have  shown  promising  improvements  in\nflexibility  and  efficiency,  playing  a  vital  role  for  HRC\nin  dynamic  manufacturing  environments.  Meanwhile,\nVLMs  have  also  demonstrated  the  ability  to  streamline\nrobot  skill  learning  by  leveraging  multimodal  data  inte-\ngration.  Despite  these  advancements,  challenges  such  as\nreal-time  processing,  computational  demands,  and  han-\ndling dynamic environments are yet to be explored urg-\nently and timely. To fully unlock the potential of VLMs\nin  human-centric  smart  manufacturing,  the  following\nkey  perspectives  can  be  considered  in  future  research:\n1)  exploring  the  scalability  of  VLMs  in  highly  variable\nand real-time HRC scenarios to improve their robustness\nand applicability, 2) developing more natural and intuitive\nhuman–robot  interaction  mechanisms  that  can  enhance\nthe collaboration efficiency and smoothness, particularly\nwith  advancements  in  large  multimodal  models,  and\n3) exploring methods to reduce the data and computational\nrequirements of VLMs to make them more practical for\nlarge-scale  industrial  deployment.  By  addressing  these\nareas, future research can build on the progress made so\nfar and push the boundaries of VLM integration into real-\nworld HRC applications. \nCompeting  Interests    T\n he  authors  declare  that  they  have  no  competing\ninterests. \nOpen Access    T his article is licensed under a Creative Commons Attribution\n4.0 International License, which permits use, sharing, adaptation, distribution\nand reproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if changes were made. The images or other\nthird  party  material  in  this  article  are  included  in  the  article’s  Creative\nCommons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence\nand  your  intended  use  is  not  permitted  by  statutory  regulation  or  exceeds\nthe  permitted  use,  you  will  need  to  obtain  permission  directly  from  the\ncopyright  holder.  To  view  a  copy  of  this  licence,  visit  http://creativecom-\nmons.org/licenses/by/4.0/.\nReferences \n Achiam  J,  Adler  S,  Agarwal  S,  Ahmad  L,  Akkaya  I,  Aleman  F  L,\nAlmeida  D,  Altenschmidt  J,  Altman  S,  Anadkat  S  others  (2023).\nGpt-4 technical report. arXiv preprint arXiv:230308774\n Anthropic (2023). The Claude 3 Model Family: Opus, Sonnet, Haiku.\n Askell A, Bai Y, Chen A, Drain D, Ganguli D, Henighan T, Jones A,\nJoseph N, Mann B, DasSarma N others (2021). A general language\nassistant  as  a  laboratory  for  alignment.  arXiv  preprint\narXiv:211200861\n Azagra P, Civera J, Murillo A C (2020). Incremental learning of object\nmodels from natural human–robot interactions. IEEE Transactions\non Automation Science and Engineering, 17(4): 1883–1900\n Brown  T,  Mann  B,  Ryder  N,  Subbiah  M,  Kaplan  J  D,  Dhariwal  P,\nNeelakantan  A,  Shyam  P,  Sastry  G,  Askell  A  others  (2020).\nLanguage models are few-shot learners. Advances in Neural Infor-\nmation Processing Systems 33:1877–1901\n Chang  Y,  Wang  X,  Wang  J,  Wu  Y,  Yang  L,  Zhu  K,  Chen  H,  Yi  X,\nWang C, Wang Y, Ye W, Zhang Y, Chang Y, Yu PS, Yang Q, Xie\nX (2024). A survey on evaluation of large language models. ACM\nTransactions on Intelligent Systems and Technology, 15(3): 1–45\n Chen  Q,  Pitawela  D,  Zhao  C,  Zhou  G,  Chen  H  T,  Wu  Q  (2024).\nWebVLN:  Vision-and-language  navigation  on  websites.  In:\nProceedings  of  the  AAAI  Conference  on  Artificial  Intelligence,\n38(2): 1165–1173\n Chen T, Kornblith S, Norouzi M, Hinton G (2020). A simple framework\nfor  contrastive  learning  of  visual  representations.  In:  International\nConference on Machine Learning. PMLR, 1597–1607\n196 Front. Eng. Manag. 2025, 12(1): 177–200\n Chowdhery  A,  Narang  S,  Devlin  J,  Bosma  M,  Mishra  G,  Roberts  A\nothers  (2023).  Palm:  Scaling  language  modeling  with  pathways.\nJournal of Machine Learning Research, 24(240): 1–113\n Chung HW, Hou L, Longpre S, Zoph B, Tay Y, Fedus W, Li Y, Wang\nX, Dehghani M, Brahma S others (2024). Scaling instruction-fine-\ntuned  language  models.  Journal  of  Machine  Learning  Research,\n25(70): 1–53\n Ding  M,  Xu  Y,  Chen  Z,  Cox  D  D,  Luo  P,  Tenenbaum  J  B,  Gan  C\n(2023).  Embodied  concept  learner:  Self-supervised  learning  of\nconcepts and mapping through Instruction Following. In: Conference\non Robot Learning. PMLR, 1743–1754\n Dong Q, Li L, Dai D, Zheng C, Wu Z, Chang B, Sun X, Xu J, Sui Z\n(2022).  A  survey  on  in-context  learning.  arXiv  preprint\narXiv:230100234\n Dosovitskiy  A,  Beyer  L,  Kolesnikov  A,  Weissenborn  D,  Zhai  X,\nUnterthiner  T,  Dehghani  M,  Minderer  M,  Heigold  G,  Gelly  S\nothers  (2020).  An  image  is  worth  16x16  words:  Transformers  for\nimage recognition at scale. In: International Conference on Learning\nRepresentations\n Dou  Z  Y,  Kamath  A,  Gan  Z,  Zhang  P,  Wang  J,  Li  L,  Liu  Z,  Liu  C,\nLeCun Y, Peng N others (2022). Coarse-to-fine vision-language pre-\ntraining with fusion in the backbone. Advances in Neural Information\nProcessing Systems 35: 32942–32956\n Driess D, Xia F, Sajjadi M S, Lynch C, Chowdhery A, Ichter B, Wahid\nA, Tompson J, Vuong Q, Yu T others (2023). PaLM-E: An embodied\nmultimodal  language  model.  In:  International  Conference  on\nMachine Learning. PMLR, 8469–8488\n Du Y, Yang M, Florence P, Xia F, Wahid A, Ichter B, Sermanet P, Yu\nT, Abbeel P, Tenenbaum J B others (2023). Video language plan-\nning. arXiv preprint arXiv:231010625\n Fan J, Zheng P (2024). A vision-language-guided robotic action planning\napproach  for  ambiguity  mitigation  in  human–robot  collaborative\nmanufacturing. Journal of Manufacturing Systems, 74: 1009–1018\n Fan J, Zheng P, Li S (2022). Vision-based holistic scene understanding\ntowards  proactive  human–robot  collaboration.  Robotics  and\nComputer-integrated Manufacturing, 75: 102304\n Fu  Z,  Lam  W,  Yu  Q,  So  A  M  C,  Hu  S,  Liu  Z,  Collier  N  (2023).\nDecoder-only or encoder-decoder? interpreting language model as a\nregularized encoder-decoder. arXiv preprint arXiv:230404052\n Gao C, Liu S, Chen J, Wang L, Wu Q, Li B, Tian Q (2024). Room-\nobject  entity  prompting  and  reasoning  for  embodied  referring\nexpression.  IEEE  Transactions  on  Pattern  Analysis  and  Machine\nIntelligence, 46(2): 994–1010\n Gervet  T,  Xian  Z,  Gkanatsios  N,  Fragkiadaki  K  (2023).  Act3D:  3D\nfeature  field  transformers  for  multi-task  robotic  manipulation.  In:\nConference on Robot Learning. PMLR, 3949–3965\n GLM T Zeng A, Xu B, Wang B, Zhang C, Yin D, Rojas D, Feng G,\nZhao  H,  Lai  H  (2024).  ChatGLM:  A  family  of  large  language\nmodels  from  GLM-130B  to  GLM-4  all  tools.  arXiv  preprint\narXiv:240612793\n Goodwin  W,  Vaze  S,  Havoutis  I,  Posner  I  (2022).  Semantically\ngrounded object matching for robust robotic scene rearrangement.\nIn:  2022  International  Conference  on  Robotics  and  Automation\n(ICRA). IEEE, Philadelphia, PA, USA, 11138–11144\n Gu Q, Kuwajerwala A, Morin S, Jatavallabhula K M, Sen B, Agarwal\nA, Rivera C, Paul W, Ellis K, Chellappa R others (2023). Concept-\ngraphs: Open-vocabulary 3d scene graphs for perception and plan-\nning. arXiv preprint arXiv:230916650\n Halim J, Eichler P, Krusche S, Bdiwi M, Ihlenfeldt S (2022). No-code\nrobotic  programming  for  agile  production:  A  new  markerless-\napproach  for  multimodal  natural  interaction  in  a  human-robot\ncollaboration context. Frontiers in Robotics and AI, 9: 1001955\n Han R, Liu N, Liu C, Gou T, Sun F (2024). Enhancing robot manipulation\nskill  learning  with  multi-task  capability  based  on  transformer  and\ntoken reduction. In: Cognitive Systems and Information Processing.\nSpringer Nature Singapore, Singapore, 121–135\n He K, Fan H, Wu Y, Xie S, Girshick R (2020). Momentum contrast for\nunsupervised visual representation learning. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR). pp 9729–9738\n He K, Zhang X, Ren S, Sun J (2016). Deep residual learning for image\nrecognition. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR). pp 770–778\n He P, Liu X, Gao J, Chen W (2021). Deberta: Decoding-enhanced bert\nwith disentangled attention. arXiv preprint arXiv: 200603654\n Hong  Y,  Zhou  Y,  Zhang  R,  Dernoncourt  F,  Bui  T,  Gould  S,  Tan  H\n(2023). Learning navigational visual representations with semantic\nmap supervision. In: 2023 IEEE/CVF International Conference on\nComputer Vision (ICCV). IEEE, Paris, France, pp 3032–3044\n Hori C, Peng P, Harwath D, Liu X, Ota K, Jain S, Corcodel R, Jha D,\nRomeres  D,  Le  Roux  J  (2023).  Style-transfer  based  speech  and\naudio-visual scene understanding for robot action sequence acquisi-\ntion from videos. arXiv preprint arXiv: 230615644\n Hu  Y,  Lin  F,  Zhang  T,  Yi  L,  Gao  Y  (2023)  Look  before  you  leap:\nUnveiling the power of gpt-4v in robotic vision-language planning.\narXiv preprint arXiv: 231117842\n Huang C, Mees O, Zeng A, Burgard W (2023a). Visual language maps\nfor  robot  navigation.  In:  2023  IEEE  International  Conference  on\nRobotics and Automation (ICRA). IEEE, 10608–10615\n Huang  W,  Wang  C,  Zhang  R,  Li  Y,  Wu  J,  Fei-Fei  L  (2023b).\nVoxposer:  Composable  3D  value  maps  for  robotic  manipulation\nwith language models. In: Conference on Robot Learning. PMLR,\n540–562\n Jang E, Irpan A, Khansari M, Kappler D, Ebert F, Lynch C, Levine S,\nFinn  C  (2022).  Bc-z:  Zero-shot  task  generalization  with  robotic\nimitation  learning.  In:  Conference  on  Robot  Learning.  PMLR,\n991–1002\n Jang  J,  Kong  C,  Jeon  D,  Kim  S,  Kwak  N  (2023).  Unifying  vision-\nlanguage  representation  space  with  single-tower  transformer.  In:\nProceedings  of  the  AAAI  Conference  on  Artificial  Intelligence.\n980–988\n Jia C, Yang Y, Xia Y, Chen Y T, Parekh Z, Pham H, Le Q, Sung Y H,\nLi Z, Duerig T (2021). Scaling up visual and vision-language repre-\nsentation  learning  with  noisy  text  supervision.  In:  International\nConference on Machine Learning. PMLR, 4904–4916\n Kenton J D M W C, Toutanova L K (2019). Bert: Pre-training of deep\nbidirectional transformers for language understanding. In: Proceed-\nings of NAACL-HLT. pp 4171–4186\n Khandelwal A, Weihs L, Mottaghi R, Kembhavi A (2022). Simple but\neffective:  Clip  embeddings  for  embodied  AI.  In:  2022  IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR).\nIEEE, New Orleans, LA, USA, 14809–14818\nJunming FAN et al. Vision-language model-based human-robot collaboration in smart manufacturing 197\n Kim J, Kang G C, Kim J, Shin S, Zhang B T (2023a). GVCCI: Lifelong\nlearning of visual grounding for language-guided robotic manipula-\ntion.  In:  2023  IEEE/RSJ  International  Conference  on  Intelligent\nRobots and Systems. IEEE, Detroit, MI, USA, 952–959\n Kim S, Joo S J, Kim D, Jang J, Ye S, Shin J, Seo M (2023b). The cot\ncollection: Improving zero-shot and few-shot learning of language\nmodels via chain-of-thought fine-tuning. In: The 2023 Conference\non  Empirical  Methods  in  Natural  Language  Processing.\n12685–12708\n Kojima  T,  Gu  S,  Reid  M,  Matsuo  Y,  Iwasawa  Y  (2022).  Large\nlanguage models are zero-shot reasoners. Advances in Neural Infor-\nmation Processing Systems. 22199–22213\n Korekata R, Kambara M, Yoshida Y, Ishikawa S, Kawasaki Y, Takahashi\nM, Sugiura K (2023). Switching head-tail funnel UNITER for dual\nreferring expression comprehension with fetch-and-carry tasks. In:\n2023 IEEE/RSJ International Conference on Intelligent Robots and\nSystems. IEEE, Detroit, MI, USA, 3865–3872\n Lewis  M,  Liu  Y,  Goyal  N,  Ghazvininejad  M,  Mohamed  A,  Levy  O,\nStoyanov  V,  Zettlemoyer  L  (2020).  Bart:  Denoising  sequence-to-\nsequence  pre-training  for  natural  language  generation,  translation,\nand comprehension. In: Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics. 7871–7880\n Li  J,  Padmakumar  A,  Sukhatme  G,  Bansal  M  (2024).  Vln-video:\nUtilizing  driving  videos  for  outdoor  vision-and-language  naviga-\ntion.  Proceedings  of  the  AAAI  Conference  on  Artificial  Intelli-\ngence, 38(17): 18517–18526\n Lin  B,  Nie  Y,  Wei  Z,  Zhu  Y,  Xu  H,  Ma  S,  Liu  J,  Liang  X  (2024).\nCorrectable  landmark  discovery  via  large  models  for  vision-\nlanguage  navigation.  IEEE  Transactions  on  Pattern  Analysis  and\nMachine Intelligence, 46(12): 1–14\n Liu  S,  Zhang  J,  Wang  L,  Gao  R  X  (2024).  Vision  AI-based  human-\nrobot  collaborative  assembly  driven  by  autonomous  robots.  CIRP\nAnnals, 73(1): 13–16\n Liu  Y,  Ott  M,  Goyal  N,  Du  J,  Joshi  M,  Chen  D,  Levy  O,  Lewis  M,\nZettlemoyer L, Stoyanov V (2019). Roberta: A robustly optimized\nbert pretraining approach. arXiv preprint arXiv:190711692\n Lu S, Berger J, Schilp J (2022). System of robot learning from multi-\nmodal  demonstration  and  natural  language  instruction.  Procedia\nCIRP, 107: 914–919\n Matheson  E,  Minto  R,  Zampieri  E  G,  Faccio  M,  Rosati  G  (2019).\nHuman–robot  collaboration  in  manufacturing  applications:  A\nreview. Robotics, 8(4): 100\n Mei  A,  Wang  J,  Zhu  G  N,  Gan  Z  (2024).  GameVLM:  A  decision-\nmaking  framework  for  robotic  task  planning  based  on  visual\nlanguage  models  and  zero-sum  games.  arXiv  preprint\narXiv:24051375\n Mohammadi  B,  Hong  Y,  Qi  Y,  Wu  Q,  Pan  S,  Shi  J  Q  (2024).\nAugmented commonsense knowledge for remote object grounding.\nProceedings  of  the  AAAI  Conference  on  Artificial  Intelligence,\n38(5): 4269–4277\n Murray  M,  Gupta  A,  Cakmak  M  (2024).  Teaching  robots  with  show\nand tell: Using foundation models to synthesize robot policies from\nlanguage and visual demonstration. In: 8th Annual Conference on\nRobot Learning\n Nair  S,  Rajeswaran  A,  Kumar  V,  Finn  C,  Gupta  A  (2022).  R3M:  A\nuniversal visual representation for robot manipulation. In: Confer-\nence on Robot Learning. PMLR, 892–909\n Park  S,  Menassa  C  C,  Kamat  V  R  (2024).  Integrating  large  langu-\nage  models  with  multimodal  virtual  reality  interfaces  to  support\ncollaborative  human-robot  construction  work.  arXiv  preprint\narXiv:240403498\n Peng A, Bobu A, Li B Z, Sumers T R, Sucholutsky I, Kumar N, Griffiths\nT  L,  Shah  J  A  (2024).  Preference-conditioned  language-guided\nabstraction.  In:  Proceedings  of  the  2024  ACM/IEEE  International\nConference on Human-Robot Interaction. ACM, Boulder CO USA,\n572–581\n Peng B, Li C, He P, Galley M, Gao J (2023). Instruction tuning with\ngpt-4. arXiv preprint arXiv:230403277\n Qiao Y, Qi Y, Yu Z, Liu J, Wu Q (2023). March in chat: interactive\nprompting  for  remote  embodied  referring  expression.  In:  2023\nIEEE/CVF International Conference on Computer Vision (ICCV).\nIEEE, Paris, France, 15712–15721\n Radford A, Kim J W, Hallacy C, Ramesh A, Goh G, Agarwal S, Sastry\nG, Askell A, Mishkin P, Clark J others (2021). Learning transferable\nvisual models from natural language supervision. In: International\nConference on Machine Learning. PMLR, 8748–8763\n Radford A, Narasimhan K, Salimans T, Sutskever I (2018). Improving\nlanguage understanding by generative pre-training. OpenAI blog\n Radford  A,  Wu  J,  Child  R,  Luan  D,  Amodei  D,  Sutskever  I,  others\n(2019).  Language  models  are  unsupervised  multitask  learners.\nOpenAI blog 1(8):9\n Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, Zhou Y,\nLi W, Liu P J (2020). Exploring the limits of transfer learning with\na  unified  text-to-text  transformer.  Journal  of  Machine  Learning\nResearch, 21(140): 1–67\n Ramrakhya  R,  Undersander  E,  Batra  D,  Das  A  (2022).  Habitat-web:\nLearning  embodied  object-search  strategies  from  human  demon-\nstrations at scale. In: Proceedings of the IEEE/CVF Conference on\nComputer  Vision  and  Pattern  Recognition  (CVPR).  IEEE,\n5173–5183\n Rana  K,  Haviland  J,  Garg  S,  Abou-Chakra  J,  Reid  I,  Suenderhauf  N\n(2023). Sayplan: Grounding large language models using 3d scene\ngraphs for scalable robot task planning. In: 7th Annual Conference\non Robot Learning. pp 23–72\n Sanh V, Webson A, Raffel C, Bach S H, Sutawika L, Alyafeai Z, Chaffin\nA, Stiegler A, Le Scao T, Raja A others (2022). Multitask prompted\ntraining  enables  zero-shot  task  generalization.  In:  International\nConference on Learning Representations\n Schumann R, Zhu W, Feng W, Fu T J, Riezler S, Wang W Y (2024).\nVELMA: Verbalization embodiment of LLM agents for vision and\nlanguage  navigation  in  street  view.  Proceedings  of  the  AAAI\nConference on Artificial Intelligence, 38(17): 18924–18933\n Shah D, Osinski B, Ichter B, Levine S (2022). Lm-nav: Robotic navi-\ngation  with  large  pre-trained  models  of  language,  vision,  and\naction. In: Conference on Robot Learning. PMLR, 492–504\n Shao  L,  Migimatsu  T,  Zhang  Q,  Yang  K,  Bohg  J  (2021).\nConcept2Robot: Learning manipulation concepts from instructions\nand  human  demonstrations.  International  Journal  of  Robotics\nResearch, 40(12-14): 1419–1434\n Sharma  S,  Huang  H,  Shivakumar  K,  Chen  L  Y,  Hoque  R,  Ichter  B,\nGoldberg K (2023). Semantic mechanical search with large vision\nand  language  models.  In:  Conference  on  Robot  Learning.  PMLR,\n198 Front. Eng. Manag. 2025, 12(1): 177–200\n971–1005\n Shukla R, Manyar O M, Ranparia D, Gupta S K (2023). A framework\nfor  improving  information  content  of  human  demonstrations  for\nenabling  robots  to  acquire  complex  tool  manipulation  skills.  In:\n2023  32nd  IEEE  International  Conference  on  Robot  and  Human\nInteractive  Communication.  IEEE,  Busan,  Korea,  Republic  of,\n2273–2280\n Singh  A,  Hu  R,  Goswami  V,  Couairon  G,  Galuba  W,  Rohrbach  M,\nKiela D (2022). Flava: A foundational language and vision alignment\nmodel. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. 15638–15650\n Skreta  M,  Zhou  Z,  Yuan  J  L,  Darvish  K,  Aspuru-Guzik  A,  Garg  A\n(2024).  Replan:  Robotic  replanning  with  perception  and  language\nmodels. arXiv preprint arXiv:240104157\n Song C H, Wu J, Washington C, Sadler B M, Chao W L, Su Y (2023).\nLlm-planner:  Few-shot  grounded  planning  for  embodied  agents\nwith  large  language  models.  In:  Proceedings  of  the  IEEE/CVF\nInternational Conference on Computer Vision. 2998–3009\n Song  D,  Liang  J,  Payandeh  A,  Xiao  X,  Manocha  D  (2024).  Socially\naware  robot  navigation  through  scoring  using  vision-language\nmodels. arXiv preprint arXiv:240400210\n Sontakke S A, Zhang J, Arnold S M R, Pertsch K, Bıyık E, Sadigh D,\nFinn  C,  Itti  L  (2024).  Roboclip:  One  demonstration  is  enough  to\nlearn  robot  policies.  In:  Proceedings  of  the  37th  International\nConference  on  Neural  Information  Processing  Systems,  55681-\n55693\n Stone  A,  Xiao  T,  Lu  Y,  Gopalakrishnan  K,  Lee  K  H,  Vuong  Q,\nWohlhart  P,  Kirmani  S,  Zitkovich  B,  Xia  F,  Finn  C,  Hausman  K\n(2023).  Open-world  object  manipulation  using  pre-trained  vision-\nlanguage  models.  In:  Conference  on  Robot  Learning.  PMLR,\n3397–3417\n Sun Y, Wang S, Feng S, Ding S, Pang C, Shang J others (2021). Ernie\n3.0:  Large-scale  knowledge  enhanced  pre-training  for  language\nunderstanding and generation. arXiv preprint arXiv:210702137\n Tan M, Le Q (2019). Efficientnet: Rethinking model scaling for convo-\nlutional neural networks. In: International Conference on Machine\nLearning. PMLR, 6105–6114\n Tay Y, Dehghani M, Tran V, Garcia X, Wei J, Wang X, Chung H W,\nBahri  D,  Schuster  T,  Zheng  S,  Zhou  D,  Houlsby  N,  Metzler  D\n(2023).  UL2:  Unifying  Language  Learning  Paradigms.  In:  The\nEleventh International Conference on Learning Representations\n Team G, Anil R, Borgeaud S, Wu Y, Alayrac J B, Yu J others (2023).\nGemini:  A  family  of  highly  capable  multimodal  models.  arXiv\npreprint arXiv:231211805\n Thoppilan R, De Freitas D, Hall J, Shazeer N, Kulshreshtha A, Cheng\nH  T  others  (2022).  Lamda:  Language  models  for  dialog  applica-\ntions. arXiv preprint arXiv:220108239\n Touvron H, Lavril T, Izacard G, Martinet X, Lachaux M A, Lacroix T\nothers  (2023).  Llama:  Open  and  efficient  foundation  language\nmodels. arXiv preprint arXiv:230213971\n Trick  S,  Herbert  F,  Rothkopf  C  A,  Koert  D  (2022).  Interactive  rein-\nforcement  learning  with  Bayesian  fusion  of  multimodal  advice.\nIEEE Robotics and Automation Letters, 7(3): 7558–7565\n Tschannen  M,  Mustafa  B,  Houlsby  N  (2022).  Image-and-language\nunderstanding from pixels only. arXiv preprint arXiv:221208045\n Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez A N\nKaiser  Lukasz,  Polosukhin  I  (2017).  Attention  is  all  you  need.\nProceedings of the 31st International Conference on Neural Infor-\nmation Processing Systems. 6000–6010\n Wang  J,  Wang  T,  Xu  L,  He  Z,  Sun  C  (2024a).  Discovering  intrinsic\nsubgoals  for  vision-and-language  navigation  via  hierarchical  rein-\nforcement  learning.  IEEE  Transactions  on  Neural  Networks  and\nLearning Systems, 1–13\n Wang L, Gao R, Váncza J, Krüger J, Wang X V, Makris S, Chryssolouris\nG  (2019).  Symbiotic  human-robot  collaborative  assembly.  CIRP\nAnnals, 68(2): 701–726\n Wang T, Fan J, Zheng P (2024b). An LLM-based vision and language\ncobot  navigation  approach  for  Human-centric  Smart  Manufactur-\ning. Journal of Manufacturing Systems, 75: 299–305\n Wang T, Roberts A, Hesslow D, Le Scao T, Chung H W, Beltagy I,\nLaunay J, Raffel C (2022a). What language model architecture and\npretraining  objective  works  best  for  zero-shot  generalization?  In:\nInternational  Conference  on  Machine  Learning.  PMLR,\n22964–22984\n Wang  T,  Zheng  P,  Li  S,  Wang  L  (2024c).  Multimodal  human–robot\ninteraction  for  human-centric  smart  manufacturing:  A  survey.\nAdvanced Intelligent Systems, 6(3): 2300359\n Wang  W,  Li  R,  Chen  Y,  Sun  Y,  Jia  Y  (2022b).  Predicting  human\nintentions  in  human–robot  hand-over  tasks  through  multimodal\nlearning. IEEE Transactions on Automation Science and Engineer-\ning, 19(3): 2339–2353\n Wang X, Wang W, Shao J, Yang Y (2024d). Learning to follow and\ngenerate instructions for language-capable navigation. IEEE Trans-\nactions  on  Pattern  Analysis  and  Machine  Intelligence,  46(5):\n3334–3350\n Wei  J,  Wang  X,  Schuurmans  D,  Bosma  M,  Xia  F,  Chi  E,  Le  Q  V,\nZhou D others (2022). Chain-of-thought prompting elicits reasoning\nin large language models. Advances in Neural Information Process-\ning Systems 35: 24824–24837\n Wi Y, Mark V der M, Pete F, Zeng A, Fazeli N (2023). CALAMARI:\nContact-aware and language conditioned spatial action mapping for\ncontact-rich  manipulation.  In:  Conference  on  Robot  Learning.\nPMLR, 2753–2771\n Winge  C,  Imdieke  A,  Aldeeb  B,  Kang  D,  Desingh  A  (2024).  Talk\nthrough it: End user directed manipulation learning. IEEE Robotics\nand Automation Letters, 9(9): 8051–8058\n Wu Z, Wang Z, Xu X, Lu J, Yan H (2023). Embodied task planning\nwith large language models. arXiv preprint arXiv:230701848\n Yao L, Han J, Wen Y, Liang X, Xu D, Zhang W, Li Z, Xu C, Xu H\n(2022). Detclip: Dictionary-enriched visual-concept paralleled pre-\ntraining for open-world detection. Advances in Neural Information\nProcessing Systems, 35: 9125–9138\n Yin C, Zhang Q (2023). A multi-modal framework for robots to learn\nmanipulation tasks from human demonstrations. Journal of Intelligent\n& Robotic Systems, 107(4): 56\n Yin  Y,  Zheng  P,  Li  C,  Wan  K  (2024).  Enhancing  human-guided\nrobotic  assembly:  AR-assisted  DT  for  skill-based  and  low-code\nprogramming. Journal of Manufacturing Systems, 74: 676–689\n Yu  J,  Wang  Z,  Vasudevan  V,  Yeung  L,  Seyedhosseini  M,  Wu  Y\n(2022).  Coca:  Contrastive  captioners  are  image-text  foundation\nmodels. arXiv preprint arXiv:220501917\n Yu T, Zhou Z, Chen Y, Xiong R (2023). Learning object spatial rela-\nJunming FAN et al. Vision-language model-based human-robot collaboration in smart manufacturing 199\ntionship from demonstration. In: 2023 2nd International Conference\non\n Machine  Learning,  Cloud  Computing  and  Intelligent  Mining.\n370–376\n Ze Y, Yan G, Wu Y H, Macaluso A, Ge Y, Ye J, Hansen N, Li L E,\nWang  X  (2023).  GNFactor:  Multi-task  real  robot  learning  with\ngeneralizable neural feature fields. In: Conference on Robot Learn-\ning. PMLR, 284–301\n Zhang  J,  Huang  J,  Jin  S,  Lu  S  (2024a).  Vision-language  models  for\nvision tasks: A survey. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 46(8): 5625–5644\n Zhang J, Wang K, Xu R, Zhou G, Hong Y, Fang X, Wu Q, Zhang Z,\nWang H (2024b). NaVid: Video-based VLM plans the next step for\nvision-and-language navigation. arXiv preprint arXiv:240215852\n Zhang  Z,  Han  X,  Liu  Z,  Jiang  X,  Sun  M,  Liu  Q  (2019).  ERNIE:\nEnhanced  Language  Representation  with  Informative  Entities.  In:\nProceedings  of  the  57th  Annual  Meeting  of  the  Association  for\nComputational Linguistics. 1441–1451\n Zhao W X, Zhou K, Li J, Tang T, Wang X, Hou Y, Min Y, Zhang B,\nZhang  J,  Dong  Z  others  (2023a).  A  survey  of  large  language\nmodels. arXiv preprint arXiv:230318223\n Zhao X, Li M, Weber C, Hafez M B, Wermter S (2023b). Chat with\nthe  environment:  Interactive  multimodal  perception  using  large\nlanguage  models.  In:  2023  IEEE/RSJ  International  Conference  on\nIntelligent Robots and Systems. IEEE, 3590–3596\n Zheng P, Li C, Fan J, Wang L (2024). A vision-language-guided and\ndeep  reinforcement  learning-enabled  approach  for  unstructured\nhuman-robot  collaborative  manufacturing  task  fulfilment.  CIRP\nAnnals, 73(1): 341–344\n Zhou G, Hong Y, Wu Q (2024). Navgpt: Explicit reasoning in vision-\nand-language  navigation  with  large  language  models.  Proceedings\nof  the  AAAI  Conference  on  Artificial  Intelligence,  38(7):\n7641–7649\n Zhou K, Yang J, Loy C C, Liu Z (2022). Learning to prompt for vision-\nlanguage models. International Journal of Computer Vision, 130(9):\n2337–2348\n Ziegler D M, Stiennon N, Wu J, Brown T B, Radford A, Amodei D,\nChristiano  P,  Irving  G  (2019).  Fine-tuning  language  models  from\nhuman preferences. arXiv preprint arXiv:190908593\n Zitkovich  B,  Yu  T,  Xu  S,  Xu  P,  Xiao  T,  Xia  F,  Wu  J,  Wohlhart  P,\nWelker  S,  Wahid  A  others  (2023).  Rt-2:  Vision-language-action\nmodels transfer web knowledge to robotic control. In: Conference\non Robot Learning. PMLR, 2165–2183\n200 Front. Eng. Manag. 2025, 12(1): 177–200",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6525958776473999
    },
    {
      "name": "Robot",
      "score": 0.5769749879837036
    },
    {
      "name": "Human–computer interaction",
      "score": 0.5544030666351318
    },
    {
      "name": "State (computer science)",
      "score": 0.5525259971618652
    },
    {
      "name": "Finite-state machine",
      "score": 0.4290356934070587
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38209664821624756
    },
    {
      "name": "Computer vision",
      "score": 0.32898950576782227
    },
    {
      "name": "Programming language",
      "score": 0.16610288619995117
    }
  ],
  "institutions": []
}