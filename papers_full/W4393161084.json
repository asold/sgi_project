{
  "title": "T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering",
  "url": "https://openalex.org/W4393161084",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A120759433",
      "name": "Lei Wang",
      "affiliations": [
        "Beijing Forestry University",
        "Singapore Management University"
      ]
    },
    {
      "id": "https://openalex.org/A275320758",
      "name": "Yi Hu",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A4382476686",
      "name": "Jiabang He",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2111849503",
      "name": "Xing Xu",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2039430831",
      "name": "Ning Liu",
      "affiliations": [
        "Beijing Forestry University"
      ]
    },
    {
      "id": "https://openalex.org/A2073720603",
      "name": "Hui Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116772828",
      "name": "Heng Tao Shen",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A120759433",
      "name": "Lei Wang",
      "affiliations": [
        "Singapore Management University",
        "Beijing Forestry University"
      ]
    },
    {
      "id": "https://openalex.org/A275320758",
      "name": "Yi Hu",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A4382476686",
      "name": "Jiabang He",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2111849503",
      "name": "Xing Xu",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2039430831",
      "name": "Ning Liu",
      "affiliations": [
        "Beijing Forestry University"
      ]
    },
    {
      "id": "https://openalex.org/A2116772828",
      "name": "Heng Tao Shen",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2745461083",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3036982689",
    "https://openalex.org/W6794134432",
    "https://openalex.org/W2964067226",
    "https://openalex.org/W3120376221",
    "https://openalex.org/W4323922316",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W2746097825",
    "https://openalex.org/W6752083267",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W6810484225",
    "https://openalex.org/W4296605665",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2954861308"
  ],
  "abstract": "Large Language Models (LLMs) have recently demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. They have also shown the ability to perform chain-of-thought (CoT) reasoning to solve complex problems. Recent studies have explored CoT reasoning in complex multimodal scenarios, such as the science question answering task, by fine-tuning multimodal models with high-quality human-annotated CoT rationales. However, collecting high-quality COT rationales is usually time-consuming and costly. Besides, the annotated rationales are hardly accurate due to the external essential information missed. To address these issues, we propose a novel method termed T-SciQ that aims at teaching science question answering with LLM signals. The T-SciQ approach generates high-quality CoT rationales as teaching signals and is advanced to train much smaller models to perform CoT reasoning in complex modalities. Additionally, we introduce a novel data mixing strategy to produce more effective teaching data samples for simple and complex science question answer problems. Extensive experimental results show that our T-SciQ method achieves a new state-of-the-art performance on the ScienceQA benchmark, with an accuracy of 96.18%. Moreover, our approach outperforms the most powerful fine-tuned baseline by 4.5%. The code is publicly available at https://github.com/T-SciQ/T-SciQ.",
  "full_text": "T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language\nModel Signals for Science Question Answering\nLei Wang1,2, Yi Hu3, Jiabang He3, Xing Xu3, Ning Liu1*, Hui Liu4, Heng Tao Shen3\n1 School of Information Science and Technology, Beijing Forestry University, China\n2 Singapore Management University, Singapore\n3 School of Computer Science and Engineering, University of Electronic Science and Technology of China\n4 Beijing Rongda Technology Co., Ltd., China\ndemolwang@gmail.com, yihu0118@gmail.com, JiaBangH@outlook.com\nxing.xu@uestc.edu.cn, liuning0928@bjfu.edu.cn, ryuki122382@gmail.com, shenhengtao@hotmail.com\nAbstract\nLarge Language Models (LLMs) have recently demonstrated\nexceptional performance in various Natural Language Pro-\ncessing (NLP) tasks. They have also shown the ability to\nperform chain-of-thought (CoT) reasoning to solve complex\nproblems. Recent studies have explored CoT reasoning in\ncomplex multimodal scenarios, such as the science question\nanswering task, by ﬁne-tuning multimodal models with high-\nquality human-annotated CoT rationales. However, collecting\nhigh-quality COT rationales is usually time-consuming and\ncostly. Besides, the annotated rationales are hardly accurate\ndue to the external essential information missed. To address\nthese issues, we propose a novel method termed T-SciQ that\naims at teaching science question answering with LLM signals.\nThe T-SciQ approach generates high-quality CoT rationales as\nteaching signals and is advanced to train much smaller models\nto perform CoT reasoning in complex modalities. Addition-\nally, we introduce a novel data mixing strategy to produce\nmore effective teaching data samples for simple and complex\nscience question answer problems. Extensive experimental\nresults show that our T-SciQ method achieves a new state-\nof-the-art performance on the ScienceQA benchmark, with\nan accuracy of 96.18%. Moreover, our approach outperforms\nthe most powerful ﬁne-tuned baseline by 4.5%. The code is\npublicly available at https://github.com/T-SciQ/T-SciQ.\nIntroduction\nScientiﬁc problem solving has recently been employed to\nevaluate the multi-hop reasoning capability and interpretabil-\nity of AI systems (Kembhavi et al. 2017; Sampat, Yang, and\nBaral 2020; Dalvi et al. 2021). However, these datasets (Kem-\nbhavi et al. 2017; Jansen et al. 2018) suffer from limited scale.\nTo address this issue, Lu et al. (2022a) introduces a large-\nscale science question-answering dataset across broad topics\nand skills called ScienceQA. This dataset consists of 21,208\nmultimodal data examples associated with questions, context,\nimages, options, lectures, and explanations. An example is\nshown in Figure 1, illustrating that a model must comprehend\n*Corresponding Author\nCopyright © 2024, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nProblem\nSample of ScienceQA\nVision\nLecture: Maps have four cardinal directions, or main directions. Those \ndirections are north, south, east, and west. A compass rose is a set of \narrows that point to the cardinal directions. A compass rose usually \nshows only the first letter of each cardinal direction. The north arrow \npoints to the North Pole. On most maps, north is at the top of the map.\nSolution: To find the answer, look at the compass rose. Look at which \nway the north arrow is pointing. West Virginia is farthest north.\nQuestion: Which of these states is \nfarthest north?\nContext: N/A\nSkill: Read a map: cardinal directions\nOptions: (A) West Virginia (B) \nLouisiana (C) Arizona (D) Oklahoma\nAnswer: The answer is (A)\nHuman-Annotated CoT Signal \nGenerated CoT Signal \nLecture: N/A\nSolution: West Virginia is the farthest north of the four states listed. \nWest Virginia is located in the Appalachian region of the United States, \nwhich is in the northeastern part of the country. Louisiana, Arizona, and \nOklahoma are all located in the southern and southwestern parts of the \nUnited States. West Virginia is the northernmost of the four states, \nmaking it the farthest north.\nFigure 1. The input of a ScienceQA data example includes a\nquestion, context, image, skill, and options. Annotations in-\nclude the ground truth answer and CoT rationale (lecture and\nsolution). Compared to annotated CoT, LLM-generated CoT\nincludes greater amounts of essential external knowledge.\nmultimodal inputs and incorporate external knowledge to\nanswer scientiﬁc questions.\nRecently, Large Language Models (LLMs) have shown\nexceptional performance in various Natural Language Pro-\ncessing (NLP) tasks (Brown et al. 2020; Thoppilan et al.\n2022). Speciﬁcally, they have demonstrated the chain-of-\nthought (CoT) ability to solve complex reasoning problems\nby using a few demonstration examples without additional\ntraining (Wei et al. 2022a; Kojima et al. 2022; Zhang et al.\n2022). However, the existing research on CoT reasoning is\nmainly limited to the language modality (Wang et al. 2022a;\nZhou et al. 2022; Lu et al. 2022b; Fu et al. 2022), with little\nattention paid to multimodal scenarios, such as science ques-\ntion answering. To address this issue, a common approach\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19162\nis to use caption models to translate visual information into\nthe language modality and prompt LLMs to perform CoT\nreasoning (Lu et al. 2022a). However, the use of caption gen-\neration models in scientiﬁc problems may result in signiﬁcant\ninformation loss when meeting highly complex images. To\novercome this issue, Zhang et al. (2023b) proposed a frame-\nwork called Multimodal-CoT that models both language and\nvisual modalities into a two-stage ﬁne-tuning process, which\nseparates rationale generation and answer inference.\nThe Multimodal-CoT method has a signiﬁcant disadvan-\ntage because it relies on the human-annotated CoT rationale\nto ﬁne-tune the model. While incorporating human-annotated\nCoT signals is helpful for training models to facilitate CoT\nreasoning ability, it has two fundamental limitations. First, the\nhuman annotation of CoT reasoning is time-consuming (Nye\net al. 2021; Cobbe et al. 2021), particularly for complex tasks\nlike ScienceQA, which necessitates extensive expert knowl-\nedge to create a reasoning process for the answer. Second, as\nshown in Figure 1, the annotated rationale may lack essential\nexternal information to derive the ﬁnal answer due to the\nlimited expertise of human annotators.\nTo address these issues, we propose a novel approach\nnamed T-SciQ to solve the ScienceQA task. The proposed\nT-SciQ framework in Figure 2 consists of three stages: gen-\nerating teaching data, mixing teaching data, and ﬁne-tuning.\nFor teaching data generation, we use a simple zero-shot in-\nstruction and a hint of the correct answer to generate a CoT\nrationale for a QA data example to obtain a QA-CoT sample.\nAlthough the model taught by QA-CoT samples excels at\ntackling simple problems, it still struggles with highly com-\nplicated problems. To overcome this challenge, we follow\nthe zero-shot plan-and-solve prompting (Wang et al. 2023)\nto generate plan-based CoT (PCoT) rationales, which decom-\npose complex problems into simpler subproblems to solve,\nto obtain QA-PCoT teaching samples.\nTo this end, we construct a new teaching dataset called T-\nSciQ by mixing QA-CoT and QA-PCoT datasets to combine\nthe strengths of both teaching signals. Speciﬁcally, we use the\nvalidation set to determine whether the PCoT teaching signal\nor CoT teaching signal is more appropriate for each data\nexample in a given skill. Then, we ﬁne-tune the student model\nwith teaching data. We follow the Multimodal-CoT (Zhang\net al. 2023b) to build our student model, which consists of\ntwo-stage: rationale generation teaching and answer inference\nteaching. During inference, the model trained in the ﬁrst stage\ngenerates rationales for the test data. The generated rationales\nare subsequently used in the second stage to infer answers.\nExperiment results on the ScienceQA benchmark show that\nour method surpasses the previous state-of-the-art approaches\nby a large margin.\nOur main contributions are summarized as follows: 1)\nWe propose a novel framework for generating high-quality\nCoT rationale and training student models to perform CoT\nreasoning for the ScienceQA task; 2) We introduce a data\nmixing strategy to produce effective teaching data samples for\nsimple and complex problems; 3) Our method achieves a new\nstate-of-the-art performance on the ScienceQA benchmark,\nsurpassing all previous models by a large margin.\nRelated Work\nChain-of-Thought Prompting. Recently, to solve com-\nplex reasoning tasks, Wei et al. (2022b) propose CoT prompt-\ning by prompting large language models to generate interme-\ndiate reasoning processes before reaching the ﬁnal answer.\nSubsequently, a lot of work has been proposed to further\nimprove CoT prompting from different aspects, including\nimproving the quality of demonstrations (Rubin, Herzig, and\nBerant 2021; Zhang et al. 2022; Fu et al. 2022; Lu et al.\n2022b; He et al. 2023) and improving the quality of reason-\ning chains (Zhou et al. 2022; Khot et al. 2022; Chen et al.\n2022; Wang et al. 2022b,a; Li et al. 2022b; Tian et al. 2023).\nZero-shot CoT (Kojima et al. 2022) elicited reasoning step\nby appending a prompt like “Let’s think step by step” to\nthe test question. Chameleon (Lu et al. 2023) proposed a\nplug-and-play compositional reasoning framework to utilize\nmultiple modules to obtain high quality prompting. Our work\nmainly focuses on mixing different teaching CoT rationales\nfor different problems.\nLLMs as Teachers. In recent studies, CoT reasoning is\nelicited in small models using ﬁne-tuned language models.\nMagister et al. (2022) beneﬁt smaller models through CoT\ndistillation. Huang et al. (2022) show that LLMs can en-\nhance reasoning using self-generated solutions from unla-\nbeled data. Ho, Schmid, and Yun (2022) propose Fine-tune-\nCoT to leverage the capabilities of LLMs to generate rea-\nsoning samples and teach smaller models via ﬁne-tuning.\nDistilling step-by-step (Hsieh et al. 2023) improves small\nmodel performance using LLM rationales with less data.\nMultimodal-CoT (Zhang et al. 2023b) uses two-stage ﬁne-\ntuning with annotated CoT rationales and visual features to\nachieve state-of-the-art results on the ScienceQA benchmark.\nOur work exploits generating two types of teaching data from\nLLMs and mixing teaching data. We discover that this simple\nmethod highly improves student performance in complex\nmulti-modality tasks, which has not yet been recognized in\nprevious studies on ﬁne-tuning with CoT reasoning (Hsieh\net al. 2023; Ho, Schmid, and Yun 2022; Huang et al. 2022;\nMagister et al. 2022; Fu et al. 2023; Hu et al. 2023).\nOur T-SciQ Approach\nOverview\nThis section presents the proposed ﬁne-tuning strategy T-\nSciQ, which utilizes a LLM named SciTeacher to generate\nteaching data and improve the performance of a smaller stu-\ndent model (SciStudent) by generated teaching data. The\nproposed T-SciQ strategy comprises three components: gener-\nating teaching data, mixing teaching data, and ﬁne-tuning, as\ndepicted in Figure 2. To generate the teaching data, we lever-\nage SciTeacher to produce CoT rationales to obtain Question-\nAnswer-CoT (QA-CoT) samples, and planning-based CoT ra-\ntionale (PCoT) to obtain Question-Answer-PCoT (QA-PCoT)\nsamples. To combine the strengths of both datasets, we create\na new teaching dataset called T-SciQ by mixing QA-CoT\nand QA-PCoT datasets. Speciﬁcally, we use the validation\nset to determine whether the PCoT teaching signal or CoT\nteaching signal is more appropriate for each data example in a\ngiven skill. We then use T-SciQ teaching samples to ﬁne-tune\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19163\nFigure 2. Key steps of our T-SciQ approach. T-SciQ consists of three stages: (i) generating teaching data; (ii) mixing teaching\ndata; and (iii) ﬁne-tuning.\nthe smaller student models. In the following, we provide a\ndetailed description of these three components.\nGenerating Teaching Data\nWe produce two types of data samples for teaching: QA-CoT\nsample with a generated CoT rationale and QA-PCoT sample\nequipped with a generated PCoT rationale.\nQA-CoT Sample Generation. Although using human-\nannotated CoT signals is valuable for training models to\nelicit CoT reasoning ability, it has two inherent limitations:\ntime-consuming and lack of external essential information\ndue to human annotators’ restricted expertise.\nTo address these issues, we introduce a zero-shot prompt-\ning to generate high-quality CoT rationales from LLMs. We\nachieve this by converting the input training data example X\ninto a prompt, utilizing a straightforward template that reads\nas follows: “Question:[Xq]. Context: [Xc]. Options: [Xo].\nCorrect Answer: [A]. [Instruct]”. Here, the [Xq] slot is\nfor the input question, the [Xc] slot is for the input context,\nthe [Xo] slot contains the possible options, the [A] slot is\nfor the correct answer that can work as a hint to guide LLMs\nto generate a more reliable rationale, and the [Instruct]\nslot contains instructions, i.e., “Please give me a detailed\nexplanation.”, to guide LLMs to perform the task. Note that\nthe context may not be included for some data examples,\nin which case the context slot is replaced with “N/A”. Sub-\nsequently, we feed the ﬁlled prompt to LLMs to output a\nreasoning process for a given training data example to obtain\nQA-CoT data DQA-CoT.\nQA-PCoT Sample Generation. Although using QA-CoT\nsamples can address issues of human-annotated CoT, ad-\ndressing highly complex problems remains a challenge. To\novercome this challenge and obtain appropriate teaching CoT\nrationale, we introduce a 3-step zero-shot prompting to de-\ncompose complex problems into simpler subproblems.\nStep 1: Lecture Generation. The lecture template used to\ngenerate a lecture for a particular skill is formulated as fol-\nlows: “Skill: [S]. QA pairs: [Xq;A] ... [Instruct].” In\nthis prompt, [Instruct] is as follows: “based on the prob-\nlems above, please give a general lecture on the[S] type\nof question in one sentence.”. Note that many QA examples\nneed the same skill to be solved.\nStep 2: Plan Generation. The template used to generate\na plan for a speciﬁc skill based on the generated lecture\nis formulated as follows: “Skill: [S]. Lecture: [L]. QA\npairs: [Xq;A] ... [Instruct].”. In this prompt,[Instruct]\nis written as follows: “Based on the lecture above and these\nproblems, let’s understand these problems and devise a gen-\neral and brief plan step by step to solve these problems (begin\nwith 1, 2, 3...)”.\nStep 3: Rationale Generation. The lecture and plan gener-\nated by the ﬁrst two prompts are used to generate a plan-based\nCoT rationale for each training example. The rationale gener-\nation template is formulated as follows: “Skill:\n[S]. Lecture:\n[L]. Plan: [P]. QA pair: [Xq;A] . [Instruct].”. In this\nprompt, [Instruct] is written as follows: “Based on the\nlecture, the plan and the problem, please carry out the plan\nand solve the problem step by step (begin with 1, 2, 3...)”.\nExamples of this three-step prompting can be found in the\nsupplementary material.\nMixing Teaching Data\nThe QA-PCoT dataset is effective for teaching problem-\nsolving skills for complex problems, while simpler prob-\nlems don’t require decomposition. In contrast, the QA-CoT\ndataset is suitable for teaching problem-solving skills for sim-\nple problems. To combine the strengths of both datasets, we\ncreate a new teaching dataset called T-SciQ by mixing QA-\nCoT and QA-PCoT datasets. We introduce a new approach\nthat uses the validation set to determine whether the PCoT\nteaching signal or CoT teaching signal is more appropriate\nfor a data example in a given skill.\nGiven a ScienceQA problem Pi with the language input\nXi;la and the visual input Xi;v, our objective is to let an an-\nswer generation model Fs\na help identify the optimal teaching\nsignal Ti;k from the possible choices Ti, i.e., CoT teaching\nsignal Ti;cot or PCoT teaching signal Ti;pcot, thereby maxi-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19164\nmizing the answer accuracy of the validation set. The an-\nswer generation module Fs\na is similar to the one described in\nMultimodal-CoT (Zhang et al. 2023b). The generated answer\n^Ai is produced by Fs\na\u0000Xi;la;Xi;v;Ti;k\u0006, and the number of\nerrors is obtained by comparing the generated answer ^Ai and\nthe label Ai. If the number of errors for validation samples\nwith PCoT in a skill is lower than that of validation samples\nwith CoT in a skill, we select PCoT rationale as the teaching\nrationale for all training data examples in this skill. Otherwise,\nwe select CoT rationale. The obtained teaching samples are\nthen used to ﬁne-tune the student model. To train the answer\ngeneration module, we utilize a subset of training data exam-\nples, each of which is associated with the human-annotated\nteaching signal from the original ScienceQA dataset.\nFine-Tuning\nOur teaching follows the Multimodal-CoT (Zhang et al.\n2023b) two-stage ﬁne-tuning framework: rationale gener-\nation teaching and answer inference teaching.\nRationale Generation Teaching. In this stage, the ratio-\nnale generation model Fr\u0000Pi\u0006 is trained to predict the teach-\ning signal Ti for a given problem Pi, where Ti either be CoT\nrationale or PCoT rationale. The input of Fr\u0000Pi\u0006 consists of\nX1\ni;la and Xi;v, where X1\ni;la represents the language input and\nXi;v represents the visual input. Formally, the probability of\ngenerating rationale Ti can be formulated as follows:\np\u0000Ti¶Xi;la1 ;Xi;v\u0006 \u0000\nNTi\n5\nj\u00001\np\u0012r \u0002Ti;j ¶ X1\ni;la;Xi;v;Ti;$j\b;\n(1)\nwhere \u0012r represents learnable parameters of the rationale\ngeneration model Fr and NTi is the length of Ti.\nAnswer Inference Teaching. In the second stage, we con-\nstruct the language input X2\ni;la by appending the teaching\nrationale Ti to the original language input X1\ni;la. The new\ninput X¬\ni is then fed to the answer inference model to infer\nthe ﬁnal answer Ai \u0000 Fa\u0000X¬\ni\u0006, where X¬\ni \u0000 rX2\ni;la;Xi;vx.\nFormally, the probability of generating answer Ai can be\nformulated as follows:\np\u0000Ai¶X2\ni;la;Xi;v\u0006 \u0000\nNAi\n5\nj\u00001\np\u0012a \u0002Ai ¶ X2\ni;la;Xi;v;Ai;$j\b; (2)\nwhere \u0012a represents learnable parameters in the answer infer-\nence teaching stage.\nModel Architecture\nWe utilize the Multimodal-\nCoT (Zhang et al. 2023b) model architecture as our\ndefault, which employs a Transformer model (Vaswani et al.\n2017) for encoding language and a vision Transformer for\nencoding visual information. The gated fusion mechanism,\nproposed in (Li et al. 2022a), is used to effectively integrate\nthe language and vision representations. Finally, a Trans-\nformer decoder is used to generate the target output. Note\nthat rationale generation and answer inference share the\nsame model but differ in the input and output.\nExperiment\nExperimental Setup\nDataset. We evaluate our proposed method on the Sci-\nenceQA (Lu et al. 2022a) dataset, a latest multimodal\nmultiple-choice science question dataset comprising 21,208\nexamples. ScienceQA encompasses a wide range of topics\nacross three distinct subjects: natural science, social science,\nand language science. The dataset comprises 26 topics, 127\ncategories, and 379 skills that are relevant to these three sub-\njects. We employ the ofﬁcial split provided by ScienceQA,\nwhich divides the dataset into training, validation, and test\nsets with a ratio of 3:1:1, i.e., 12;726, 4;241, and 4;241\nexamples, respectively. The dataset includes annotated rea-\nsoning chains for each data example. In this work, we extract\nour training signals from large language models instead of\nusing human annotated signals.\nBaselines. We provide a comparison of our proposed method\nwith extensive baseline methods. Speciﬁcally, we have sev-\neral early VQA models, including MCAN (Yu et al. 2019),\nTop-Down (Anderson et al. 2018), BAN (Kim, Jun, and Zhang\n2018), DFAF (Gao et al. 2019). These VQA baselines use\nthe question, context, and answer choices as textual input\nand the image as the visual input. They predict a score dis-\ntribution over the answer candidates using a linear classiﬁer.\nIn addition, we include pre-trained text-to-text and multi-\nmodal models such as\nViLT (Kim, Son, and Kim 2021),\nPatch-TRM (Lu et al. 2021), and VisualBERT (Li et al.\n2019), UnifiedQA (Khashabi et al. 2020), MM-COT (Zhang\net al. 2023b). These methods use pre-trained models as\nbackbone models and incorporate additional modules to\nhandle multimodal signals if necessary. We also include\nrecent LLM-based multimodal ﬁne-tuned baselines such\nas\nLLaMa-Adapter (Zhang et al. 2023a) and LLaVA (Liu\net al. 2023). They use a strong open-access LLM such as\nLLaMa (Touvron et al. 2023) as the base model and incor-\nporate a vision module to model visual information. We\nalso include widely-used in-context learning baselines: the\nchain of thought (CoT) prompting (Wei et al. 2022a), where\neach in-context demonstration example comprises the input\nquestion and output annotated reasoning process. We com-\npare to the CoT baselines over different API-based OpenAI\nLLMs (OpenAI 2022, 2023), such as GPT-3.5 ( GPT-3.5\nw/ COT), ChatGPT (ChatGPT w/ COT), GPT-4 (GPT-4 w/\nCOT), and Chameleon (Lu et al. 2023). Additionally, we also\ncompare to the standard few-shot prompting approach using\nGPT-3.5 (GPT-3.5).\nEvaluation Metrics. As ScienceQA is a benchmark for\nmultiple-choice question answering, the accuracy of the an-\nswer is evaluated by comparing the ground truth option with\nthe ﬁnal prediction generated by the evaluated model.\nImplementation Details. By default, we utilize the GPT-\n3.5 of text-davinci-003 version as the teacher model for our\napproach unless otherwise speciﬁed. To validate the gen-\neralizability of our method, we experiment with three dis-\ntinct student models, namely UniﬁedQA\nBase w/ CoT (Lu\net al. 2022a), Mutimodal-CoT Base(Lu et al. 2022a), and\nMutimodal-CoTLarge(Zhang et al. 2023b). These models are\nchosen due to their strong performances achieved by ﬁne-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19165\nModel Size NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg\nHuman - 90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40\nMCAN (Yu et al. 2019) 95M 56.08 46.23 58.09 59.43 51.17 55.40 51.65 59.72 54.54\nTop-Down (Anderson et al. 2018) 70M 59.50 54.33 61.82 62.90 54.88 59.79 57.27 62.16 59.02\nBAN (Kim, Jun, and Zhang 2018) 112M 60.88 46.57 66.64 62.61 52.60 65.51 56.83 63.94 59.37\nDFAF (Gao et al. 2019) 74M 64.03 48.82 63.55 65.88 54.49 64.11 57.12 67.17 60.72\nViLT (Kim, Son, and Kim 2021) 113M 60.48 63.89 60.27 63.20 61.38 57.00 60.72 61.90 61.14\nPatch-TRM (Lu et al. 2021) 90M 65.19 46.79 65.55 66.96 55.28 64.95 58.04 67.50 61.42\nVisualBERT (Li et al. 2019) 111M 59.33 69.18 61.18 62.71 62.17 58.54 62.96 59.92 61.87\nUniﬁedQABase (Khashabi et al. 2020) 223M 68.16 69.18 74.91 63.78 61.38 77.84 72.98 65.00 70.12\nLLaMa-Adapter (Zhang et al. 2023a) %7B 84.37 88.30 84.36 83.72 80.32 86.90 85.83 84.05 85.19\nLLaV A (Liu et al. 2023) %7B 90.36 95.95 88.00 89.49 88.00 90.66 90.93 90.90 90.92\nGPT-3.5 (Chen et al. 2020) %175B 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97\nGPT-3.5 w/ CoT (Lu et al. 2022a) %175B 75.44 70.87 78.09 74.68 67.43 79.93 78.23 69.68 75.17\nChatGPT w/ CoT (Lu et al. 2023) %175B 78.82 70.98 83.18 77.37 67.92 86.13 80.72 74.03 78.31\nGPT-4 w/ CoT (Lu et al. 2023) %175B 84.06 73.45 87.36 81.87 70.75 90.73 84.69 79.10 82.69\nChameleon (Lu et al. 2023) %175B 89.83 74.13 89.82 88.27 77.64 92.13 88.03 83.72 86.54\nUniﬁedQA-CoTBase (Lu et al. 2022a) 223M 71.00 76.04 78.91 66.42 66.53 81.81 77.06 68.82 74.11\nUniﬁedQA-T-SciQBase (Ours) 223M 76.56 88.99 80.45 72.90 73.84 83.47 81.09 75.19 79.41\nImprovement - +5.56 +12.95 +1.54 +6.48 +7.31 +1.66 +4.03 +6.37 +5.30\nMutimodal-CoTBase (Zhang et al. 2023b) 223M 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\nMutimodal-T-SciQBase (Ours) 223M 91.52 91.45 92.45 91.94 90.33 92.26 92.11 91.10 91.75\nImprovement - +4.00 +14.28 +6.63 +4.06 +7.43 +5.43 +7.46 +5.73 +6.84\nMutimodal-CoTLarge (Zhang et al. 2023b) 738M 95.91 82.00 90.82 95.26 88.80 92.89 92.44 90.31 91.68\nMutimodal-T-SciQLarge (Ours) 738M 96.89 95.16 95.55 96.53 94.70 96.79 96.44 95.72 96.18\nImprovement - +0.98 +13.16 +4.73 +1.27 +5.90 +3.90 +4.00 +5.41 +4.50\nTable 1. Main results (%) on the test set of ScienceQA. There are totally 8 classes of questions, namely natural science (NAT),\nsocial science (SOC), language science (LAN), text context (TXT), image context (IMG), no context (NO), grades 1-6 (G1-6),\nand grades 7-12 (G7-12). The best results are boldfaced.\nModel Avg\nMultimodal-T-SciQBase (Mixing) 91.75\nMultimodal-T-SciQBase only w/ QA-CoT 85.99\nMultimodal-T-SciQBase only w/ QA-PCoT 88.56\nMutimodal-CoTBase 84.91\nMultimodal-T-SciQLarge (Mixing) 96.18\nMultimodal-T-SciQLarge only w/ QA-CoT 93.44\nMultimodal-T-SciQLarge only w/ QA-PCoT 94.11\nMutimodal-CoTLarge 91.68\nTable 2. Ablation study of the impact of different signals\nprovided by LLMs across all topics.\ntuning with annotated reasoning signals. To ensure fairness\nof comparison and effectiveness of our proposed method, we\nonly replace the training signals generated by our approach\nwith annotated signals while maintaining the same settings\nas the original paper. These student models are 200\u0012smaller\nthan their teacher models.\nMain Results\nT-SciQ v.s. Baselines. Table 1 details the performance ac-\ncuracy of baselines and student models trained using the\nproposed T-SciQ signals. Mutimodal-T-SciQLarge, which is\nthe model architecture of Mutimodal-CoT Large ﬁne-tuned\nwith mixed teacher signals, attains an accuracy of 96.18%\nand consistently outperforms all state-of-the-art methods\nby a large margin for all topics across all subjects. Speciﬁ-\ncally, Mutimodal-T-SciQLarge outperforms the most powerful\nﬁne-tuning baseline, Mutimodal-CoTLarge, which is trained\nby annotated chain-of-thought signals, by 4.5% (91.68%\n\u0000 96.18%), the strongest instruction-tuning based multi-\nmodal baseline, LLaVa, by 5.26% (90.92% \u0000 96.18%), the\nbest GPT-4 based few-shot baseline, Chameleon, by 9.64%\n(86.54% \u0000 96.18%), and human performance by 7.78%\n(88.40% \u0000 96.18%). This signiﬁcant improvement of our\nproposed method suggests that higher-quality teaching sig-\nnals of planning and reasoning provided by LLMs elicit better\nplanning and chain-of-thought reasoning ability in student\nmodels smaller than 1B.\nT-SciQ with Different Base Student Models. Instead of\nonly using the model architecture of Mutimodal-CoT Large\nas the base student model, we evaluate different base stu-\ndent models ﬁne-tuned with mixed teaching signals: the\nvariant UniﬁedQA-T-SciQBase and Mutimodal-T-SciQBase.\nThe relative performance ranking between the base student\nmodel with annotated CoT signals and the one with mixing\nteacher signals remains unchanged. Speciﬁcally, UniﬁedQA-\nT-SciQBase outperforms UniﬁedQA Base w/ CoT by 5.3%\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19166\nMethod T-SciQ\nQA-CoT QA-PCoT\nT-SciQ\nLanguage Only 84.44 85.38\n87.24\nw/ CLIP 86.18 87.41\n90.90\nw/ DETR 85.99 88.56\n91.75\nw/ ResNet 86.06 87.69\n91.44\nTable 3. Accuracy (%) of Mutimodal-T-SciQBase using dif-\nferent visual features.\n(a)\n (b)\nFigure 3. Further analysis on (a) the effect of Mutimodal-T-\nSciQBase trained with different proportion of generated data\nand (b) accuracy curve of the baseline Mutimodal-CoTBase\nand our Mutimodal-T-SciQBase across epochs.\n(74.11% \u0000 79.41%), and Mutimodal-T-SciQ Base outper-\nforms Mutimodal-CoTBase by 6.84% (84.91% \u0000 91.75%).\nT-SciQ still achieves the best performance with different\nbase student models. These encouraging results indicate the\ngeneralizability of the proposed teaching signals.\nFurther Analysis\nEffect of Different Signals of T-SciQ. Our approach in-\ncorporates two distinct components for teaching signals: QA-\nCoT and QA-PCoT. We early show that combining these\ntwo signals (i.e., Mutimodal-T-SciQ) yields signiﬁcantly bet-\nter results than using only human-annotated CoT signals\n(i.e., Mutimodal-CoT) when teaching student models. In this\nsection, we aim to evaluate the impact of each teaching sig-\nnals by testing the performance of Mutimodal-T-SciQ Base\nand Mutimodal-T-SciQLarge when either QA-CoT or QA-\nPCoT signal is removed. As demonstrated in Table 2, we can\nobserve a signiﬁcant decrease in answering accuracy when\neither teaching signal was removed. These ﬁndings indicate\nthe effectiveness of both proposed teaching signals. This is\nbecause 1) student models taught by QA-CoT signals can\nincorporate a more extensive range of knowledge from the\nopen world rather than solely relying on the knowledge of\nannotators and 2) student models taught by QA-PCoT sig-\nnals can decompose complex problems into several simpler\nsub-problems.\nImpact of visual Features. The choice of visual features\ncan signiﬁcantly affect the performance of models on Sci-\nenceQA. Thus, we conduct an evaluation of three widely-\nused visual features, which are CLIP (Radford et al. 2021),\nDETR (Carion et al. 2020), and ResNet (He et al. 2016). Both\n(a)\n (b)\nFigure 4. Further analysis on (a) accuracy (%) of Mutimodal-\nT-SciQBase with teaching signals provided by different base\nLLMs and (b) error analysis of prediction for speciﬁc skills.\nCLIP and DETR can provide patch-level features, and DETR\nis designed for object detection. As for ResNet features, we\nuse ResNet-50 to derive visual features. Table 3 shows the\nresults of comparing these three visual features. Our ﬁnd-\nings suggest that incorporating visual features yields superior\nperformance than relying on language-only baselines. No-\ntably, DETR consistently outperforms the other two features\nin most cases, and hence, we adopt it as the default visual\nfeature in our main experiments.\nProportion of Generated Data in Training Data. To fur-\nther compare the T-SciQ signals produced by LLMs and the\nannotated CoT signals, we experiment with manipulating\nthe proportion of these two signals within the training data.\nWe vary the proportion of T-SciQ signals from 0% to 100%.\nAs demonstrated in Figure 3a, the increasing proportion of\ntraining data with T-SciQ signals increases performance.\nPerformance Change with Epoch. Figure 3b shows the\nperformance trends of the baseline Mutimodal-CoTBase and\nour proposed Mutimodal-T-SciQBase across different training\nepochs. Notably, our method consistently outperforms the\nbaseline across all epochs. We adopt a two-stage training\napproach similar to the baseline Mutimodal-CoTBase, where\nwe ﬁrst train the explanation generation module and then train\nthe answer prediction. It indicates that our method exhibits\nrelatively higher accuracy at the initial training stages.\nEffect of Teaching Signals Provided by Different Base\nLLMs. We use the GPT-3.5 model by default, speciﬁcally\nthe text-davinci-003 version, to generate teaching signals in\nthe main experiment. However, other powerful LLMs can\nalso provide useful signals, such as the earlier version of\nGPT-3.5, text-davinci-002, and the recently popular ChatGPT\nmodel. This study explores the effectiveness of a mixture of\nQA-CoT signals from text-davinci-002, text-davinci-003, or\nChatGPT, and QA-PCoT signals from the above API-based\nmodels. We conduct this experiment using the Multimodal-\nT-SciQBase. Figure 4a shows the comparison of the perfor-\nmance of nine different mixture strategies. Our results show\nthat even the worst strategy, which involves a mixture of\nQA-CoT signal from text-davinci-003 and QA-PCoT signal\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19167\nRationale: 1. ... 2. ... 3. ... Louisiana is to the west of the other states, \nOhio is to the north, Maine is to the east, and Tennessee is to the \nsouth. Therefore, Maine is farthest east. 4. Select the correct answer \nfrom the given options. The correct answer is (C) Maine.\nAnswer: The answer is (C)\nVision\nProblem\nQuestion: Which of these states is \nfarthest east?\nContext: N/A\nOptions: (A) Louisiana (B) Ohio (C) \nMaine (D) Tennessee\nRationale: ... To find the answer, look at the compass rose. Look at \nwhich way the east arrow is pointing. Ohio is farthest east.\nAnswer: The answer is (B)\nMM-CoT\nT-SciQ\n(a)\nRationale: 1. Read the question carefully … 2. Determine if the word \ngiven is between the two guide words 3. If the word is between the \ntwo guide words … 4. If the question asks for a word between the \ntwo guide words … so the answer is \"stockade\".\nAnswer: The answer is (B)\nVision\nProblem\nQuestion: Which word would you \nfind on a dictionary page with the \nfollowing guide words? shot - suit\nContext: N/A\nOptions: (A) service (B) stockade\nRationale: ... Since serviceade is between the guide words shot - suit, \nit would be found on that page.\nAnswer: The answer is (A)\nMM-CoT\nT-SciQ\nN/A (b)\nFigure 5. Examples of MM-CoT (baseline) and the model trained with T-SciQ (ours) signals for generating rationales and\npredicting answers. To solve these examples, commonsense knowledge such as geographic knowledge (a) and multi-step\nreasoning (b) are required.\nfrom text-davince-002, outperforms annotated CoT signal\nby a signiﬁcant margin. It indicates that regardless of the\nmixture strategy used, LLMs can provide signals with more\nuseful knowledge from the open world.\nError Analysis. To better understand the model’s behavior\ntrained using our proposed T-SciQ signals, we analyze six\nselected skills shown in Figure 4b. It shows the error analysis\nof prediction for six speciﬁc skills (A-F), i.e., “Using guide\nwords”, “Comparing properties of objects”, “Reading a map:\ncardinal directions”, “Identifying oceans and continents”,\n“How is temperature related to thermal energy?”, and “Iden-\ntifying the Thirteen Colonies”, respectively. We can observe\nthat training with T-SciQ signals can signiﬁcantly reduce the\nnumber of errors. Examples of skills such as “Identifying\noceans and continents” require multi-step complex reasoning\nthat T-SciQ teaching signals can teach. On the other hand, ex-\namples of skills such as “Reading a map: cardinal directions”\nrequire common sense and factual knowledge from the open\nworld, which T-SciQ signals can also provide.\nCase Study. The case study compares T-SciQ and\nMultimodal-CoT on the ScienceQA benchmark (Figure 5).\nFigure 5a shows cases needing geographic knowledge.\nHuman-annotated CoT may lack open-world information,\nwhile T-SciQ includes it. Figure 5b shows a multi-step rea-\nsoning case without image input. Multimodal-CoT errors\nwhile our model decomposes and answers correctly. These\nhighlight that T-SciQ is well-suited to handle problems that\nrequire open knowledge and decomposition.\nComparison on Other NLP Reasoning Datasets. To ver-\nify the versatility of our teaching approach, we additionally\nassess our approach on six reasoning tasks, following Reason-\nTeacher (Ho, Schmid, and Yun 2022): arithmetic (Aqua (Ling\net al. 2017)), symbolic (Coin Flip (Wei et al. 2022b)), com-\nMethod Aqua Date\nShufﬂed Coin CS Strategy\nObjects Flip\nQA QA\nReason-Teacher 24.02 60.36\n64.44 98.67 56.76 55.02\nT-SciQ 74.80 89.29\n70.28 98.67 70.76 76.74\nTable 4. Accuracy (%) on other six reasoning datasets.\nmonsense (CommonSenseQA (CSQA) (Talmor et al. 2018),\nStrategyQA (Geva et al. 2021)) reasoning, and logic (Date\nUnderstanding, Tracking Shufﬂed Objects) (Geva et al. 2021).\nIn Table 4, we compare T-SciQ to diverse reasoning teaching\nsignals introduced by Reason-Teacher. The results show that\nour T-SciQ surpasses Reason-Teacher by a large margin in 5\nout of 6 datasets. It performs equally well in the remaining\ndataset, Coin Flip. These results indicate that higher-quality\nteaching signals of planning and reasoning can lead to a\nremarkable improvement in small student models across dif-\nferent scenarios.\nConclusion\nThis paper introduces a new approach named T-SciQ that\nutilizes large language models’ chain-of-thought (CoT) rea-\nsoning capabilities to teach small multimodal models for\ncomplex science question answering tasks. Our zero-shot\nprompting method generates QA-CoT samples as teaching\ndata. We also present a 3-step zero-shot prompting approach\nusing plan-based CoT for highly complex problems. Fur-\nthermore, our data mixture strategy combines CoT and plan-\nbased CoT to create a new T-SciQ teaching dataset. Our\nmethod overcomes the limitations of human-annotated CoT,\nproviding a promising approach for complex science question\nanswering. Future work includes exploring extensive LLMs\nand parameter-efﬁcient ﬁne-tuning with LLM teachers.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19168\nAcknowledgments\nThis work was supported in part by Central University\nBasic Research Business Fee Special Fund Project No.\nBLX202139, the National Natural Science Foundation of\nChina under Grant No. 62222203, the New Cornerstone Sci-\nence Foundation through the XPLORER PRIZE, and the\nScience and Technology Innovation Committee of Shenzhen\nMunicipality Foundation (No. JCYJ20210324132203007).\nReferences\nAnderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.;\nGould, S.; and Zhang, L. 2018. Bottom-up and top-down\nattention for image captioning and visual question answering.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, 6077–6086.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\nAdvances in neural information processing systems, 33: 1877–\n1901.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-End Object Detection\nwith Transformers. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part I, 213–229.\nChen, T.; Kornblith, S.; Swersky, K.; Norouzi, M.; and Hin-\nton, G. E. 2020. Big self-supervised models are strong semi-\nsupervised learners. Advances in neural information process-\ning systems, 33: 22243–22255.\nChen, W.; Ma, X.; Wang, X.; and Cohen, W. W. 2022. Pro-\ngram of thoughts prompting: Disentangling computation\nfrom reasoning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588.\nCobbe, K.; Kosaraju, V .; Bavarian, M.; Chen, M.; Jun, H.;\nKaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.;\net al. 2021. Training veriﬁers to solve math word problems.\narXiv preprint arXiv:2110.14168.\nDalvi, B.; Jansen, P.; Tafjord, O.; Xie, Z.; Smith, H.; Pi-\npatanangkura, L.; and Clark, P. 2021. Explaining answers\nwith entailment trees. Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Processing\n(EMNLP).\nFu, Y .; Peng, H.; Ou, L.; Sabharwal, A.; and Khot, T. 2023.\nSpecializing Smaller Language Models towards Multi-Step\nReasoning. arXiv preprint arXiv:2301.12726.\nFu, Y .; Peng, H.; Sabharwal, A.; Clark, P.; and Khot, T. 2022.\nComplexity-based prompting for multi-step reasoning. arXiv\npreprint arXiv:2210.00720.\nGao, P.; Jiang, Z.; You, H.; Lu, P.; Hoi, S. C.; Wang, X.; and\nLi, H. 2019. Dynamic fusion with intra-and inter-modality\nattention ﬂow for visual question answering. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, 6639–6648.\nGeva, M.; Khashabi, D.; Segal, E.; Khot, T.; Roth, D.; and Be-\nrant, J. 2021. Did aristotle use a laptop? a question answering\nbenchmark with implicit reasoning strategies. Transactions\nof the Association for Computational Linguistics, 9: 346–361.\nHe, J.; Wang, L.; Hu, Y .; Liu, N.; Liu, H.; Xu, X.; and Shen,\nH. T. 2023. ICL-D3IE: In-Context Learning with Diverse\nDemonstrations Updating for Document Information Extrac-\ntion. In IEEE/CVF International Conference on Computer\nVision, 19428–19437.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual\nLearning for Image Recognition. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2016,\nLas Vegas, NV , USA, June 27-30, 2016, 770–778. IEEE Com-\nputer Society.\nHo, N.; Schmid, L.; and Yun, S.-Y . 2022. Large Lan-\nguage Models Are Reasoning Teachers. arXiv preprint\narXiv:2212.10071.\nHsieh, C.-Y .; Li, C.-L.; Yeh, C.-K.; Nakhost, H.; Fujii, Y .;\nRatner, A.; Krishna, R.; Lee, C.-Y .; and Pﬁster, T. 2023. Dis-\ntilling step-by-step! outperforming larger language models\nwith less training data and smaller model sizes.arXiv preprint\narXiv:2305.02301.\nHu, Z.; Lan, Y .; Wang, L.; Xu, W.; Lim, E.-P.; Lee, R. K.-W.;\nBing, L.; and Poria, S. 2023. LLM-Adapters: An Adapter\nFamily for Parameter-Efﬁcient Fine-Tuning of Large Lan-\nguage Models. arXiv preprint arXiv:2304.01933.\nHuang, J.; Gu, S. S.; Hou, L.; Wu, Y .; Wang, X.; Yu, H.; and\nHan, J. 2022. Large language models can self-improve.arXiv\npreprint arXiv:2210.11610.\nJansen, P. A.; Wainwright, E.; Marmorstein, S.; and Morrison,\nC. T. 2018. Worldtree: A corpus of explanation graphs for\nelementary science questions supporting multi-hop inference.\narXiv preprint arXiv:1802.03052.\nKembhavi, A.; Seo, M.; Schwenk, D.; Choi, J.; Farhadi, A.;\nand Hajishirzi, H. 2017. Are you smarter than a sixth grader?\ntextbook question answering for multimodal machine com-\nprehension. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 4999–5007.\nKhashabi, D.; Min, S.; Khot, T.; Sabharwal, A.; Tafjord,\nO.; Clark, P.; and Hajishirzi, H. 2020. Uniﬁedqa: Crossing\nformat boundaries with a single qa system. arXiv preprint\narXiv:2005.00700.\nKhot, T.; Trivedi, H.; Finlayson, M.; Fu, Y .; Richardson, K.;\nClark, P.; and Sabharwal, A. 2022. Decomposed prompt-\ning: A modular approach for solving complex tasks. arXiv\npreprint arXiv:2210.02406.\nKim, J.-H.; Jun, J.; and Zhang, B.-T. 2018. Bilinear atten-\ntion networks. Advances in neural information processing\nsystems, 31.\nKim, W.; Son, B.; and Kim, I. 2021. Vilt: Vision-and-\nlanguage transformer without convolution or region super-\nvision. In International Conference on Machine Learning,\n5583–5594. PMLR.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa, Y .\n2022. Large language models are zero-shot reasoners. arXiv\npreprint arXiv:2205.11916.\nLi, B.; Lv, C.; Zhou, Z.; Zhou, T.; Xiao, T.; Ma, A.; and\nZhu, J. 2022a. On Vision Features in Multimodal Machine\nTranslation. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1:\nLong Papers), 6327–6337.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19169\nLi, L. H.; Yatskar, M.; Yin, D.; Hsieh, C.-J.; and Chang, K.-\nW. 2019. Visualbert: A simple and performant baseline for\nvision and language. arXiv preprint arXiv:1908.03557.\nLi, Y .; Lin, Z.; Zhang, S.; Fu, Q.; Chen, B.; Lou, J.-G.; and\nChen, W. 2022b. On the advance of making language models\nbetter reasoners. arXiv preprint arXiv:2206.02336.\nLing, W.; Yogatama, D.; Dyer, C.; and Blunsom, P. 2017.\nProgram induction by rationale generation: Learning to\nsolve and explain algebraic word problems. arXiv preprint\narXiv:1705.04146.\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023. Visual instruction\ntuning. arXiv preprint arXiv:2304.08485.\nLu, P.; Mishra, S.; Xia, T.; Qiu, L.; Chang, K.-W.; Zhu, S.-C.;\nTafjord, O.; Clark, P.; and Kalyan, A. 2022a. Learn to explain:\nMultimodal reasoning via thought chains for science ques-\ntion answering. Advances in Neural Information Processing\nSystems, 35: 2507–2521.\nLu, P.; Peng, B.; Cheng, H.; Galley, M.; Chang, K.-W.; Wu,\nY . N.; Zhu, S.-C.; and Gao, J. 2023. Chameleon: Plug-and-\nPlay Compositional Reasoning with Large Language Models.\narXiv preprint arXiv:2304.09842.\nLu, P.; Qiu, L.; Chang, K.-W.; Wu, Y . N.; Zhu, S.-C.; Rajpuro-\nhit, T.; Clark, P.; and Kalyan, A. 2022b. Dynamic prompt\nlearning via policy gradient for semi-structured mathematical\nreasoning. arXiv preprint arXiv:2209.14610.\nLu, P.; Qiu, L.; Chen, J.; Xia, T.; Zhao, Y .; Zhang, W.; Yu,\nZ.; Liang, X.; and Zhu, S.-C. 2021. Iconqa: A new bench-\nmark for abstract diagram understanding and visual language\nreasoning. arXiv preprint arXiv:2110.13214.\nMagister, L. C.; Mallinson, J.; Adamek, J.; Malmi, E.; and\nSeveryn, A. 2022. Teaching small language models to reason.\narXiv preprint arXiv:2212.08410.\nNye, M.; Andreassen, A. J.; Gur-Ari, G.; Michalewski, H.;\nAustin, J.; Bieber, D.; Dohan, D.; Lewkowycz, A.; Bosma,\nM.; Luan, D.; et al. 2021. Show your work: Scratchpads\nfor intermediate computation with language models. arXiv\npreprint arXiv:2112.00114.\nOpenAI. 2022. Introducing chatgpt. https://openai.com/blog/\nchatgpt. Accessed: 2022-11-30.\nOpenAI. 2023. GPT-4 Technical Report. arXiv preprint\narXiv:2303.08774.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International Conference on\nMachine Learning, 8748–8763. PMLR.\nRubin, O.; Herzig, J.; and Berant, J. 2021. Learning to\nretrieve prompts for in-context learning. arXiv preprint\narXiv:2112.08633.\nSampat, S. K.; Yang, Y .; and Baral, C. 2020. Visuo-Lingustic\nQuestion Answering (VLQA) Challenge. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: Findings (EMNLP), 4606–4616.\nTalmor, A.; Herzig, J.; Lourie, N.; and Berant, J. 2018. Com-\nmonsenseqa: A question answering challenge targeting com-\nmonsense knowledge. arXiv preprint arXiv:1811.00937.\nThoppilan, R.; De Freitas, D.; Hall, J.; Shazeer, N.; Kul-\nshreshtha, A.; Cheng, H.-T.; Jin, A.; Bos, T.; Baker, L.; Du,\nY .; et al. 2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nTian, Q.; Zhu, H.; Wang, L.; Li, Y .; and Lan, Y . 2023. R3\nPrompting: Review, Rephrase and Resolve for Chain-of-\nThought Reasoning in Large Language Models under Noisy\nContext. arXiv preprint arXiv:2310.16535.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efﬁcient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All you Need. In Advances in Neural Information\nProcessing Systems 30, 5998–6008.\nWang, L.; Xu, W.; Lan, Y .; Hu, Z.; Lan, Y .; Lee, R. K.-W.; and\nLim, E.-P. 2023. Plan-and-solve prompting: Improving zero-\nshot chain-of-thought reasoning by large language models.\narXiv preprint arXiv:2305.04091.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; and\nZhou, D. 2022a. Rationale-augmented ensembles in language\nmodels. arXiv preprint arXiv:2207.00747.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; and Zhou,\nD. 2022b. Self-consistency improves chain of thought reason-\ning in language models. arXiv preprint arXiv:2203.11171.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.;\nLe, Q.; and Zhou, D. 2022a. Chain of Thought Prompting\nElicits Reasoning in Large Language Models. ArXiv preprint,\nabs/2201.11903.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.;\nLe, Q.; and Zhou, D. 2022b. Chain of thought prompting\nelicits reasoning in large language models. arXiv preprint\narXiv:2201.11903.\nYu, Z.; Yu, J.; Cui, Y .; Tao, D.; and Tian, Q. 2019. Deep\nmodular co-attention networks for visual question answering.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 6281–6290.\nZhang, R.; Han, J.; Zhou, A.; Hu, X.; Yan, S.; Lu, P.; Li,\nH.; Gao, P.; and Qiao, Y . 2023a. Llama-adapter: Efﬁcient\nﬁne-tuning of language models with zero-init attention.arXiv\npreprint arXiv:2303.16199.\nZhang, Z.; Zhang, A.; Li, M.; and Smola, A. 2022. Automatic\nchain of thought prompting in large language models. arXiv\npreprint arXiv:2210.03493.\nZhang, Z.; Zhang, A.; Li, M.; Zhao, H.; Karypis, G.; and\nSmola, A. 2023b. Multimodal chain-of-thought reasoning in\nlanguage models. arXiv preprint arXiv:2302.00923.\nZhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Bousquet, O.; Le, Q.; and Chi, E. 2022.\nLeast-to-most prompting enables complex reasoning in large\nlanguage models. arXiv preprint arXiv:2205.10625.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19170",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.7155142426490784
    },
    {
      "name": "Computer science",
      "score": 0.589741051197052
    },
    {
      "name": "Natural language processing",
      "score": 0.4351470470428467
    },
    {
      "name": "Language model",
      "score": 0.4232429265975952
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4055388569831848
    },
    {
      "name": "Linguistics",
      "score": 0.3290560245513916
    },
    {
      "name": "Psychology",
      "score": 0.32634568214416504
    },
    {
      "name": "Cognitive science",
      "score": 0.3249506950378418
    },
    {
      "name": "Philosophy",
      "score": 0.13718834519386292
    }
  ]
}