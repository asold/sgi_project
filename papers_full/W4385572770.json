{
  "title": "RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder",
  "url": "https://openalex.org/W4385572770",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3132230269",
      "name": "Shitao Xiao",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2014768895",
      "name": "Zheng Liu",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2122925794",
      "name": "Yingxia Shao",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2120850591",
      "name": "Zhao Cao",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3098708719",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3006057906",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3153414861",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4324016655",
    "https://openalex.org/W4226315795",
    "https://openalex.org/W4225104598",
    "https://openalex.org/W4224313754",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W4281259526",
    "https://openalex.org/W3134665270",
    "https://openalex.org/W3168875417",
    "https://openalex.org/W4290945693",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W4206121183",
    "https://openalex.org/W3157758108",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W4287207977",
    "https://openalex.org/W3196655982",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3212725701",
    "https://openalex.org/W3203288040",
    "https://openalex.org/W4281393357",
    "https://openalex.org/W2963469388",
    "https://openalex.org/W3217305727",
    "https://openalex.org/W4385573970",
    "https://openalex.org/W3206455169",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W4290877239",
    "https://openalex.org/W2124509324",
    "https://openalex.org/W3118668786",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3005296017"
  ],
  "abstract": "Despite pre-training‚Äôs progress in many important NLP tasks, it remains to explore effective pre-training strategies for dense retrieval. In this paper, we propose RetroMAE, a new retrieval oriented pre-training paradigm based on Masked Auto-Encoder (MAE). RetroMAE is highlighted by three critical designs. 1) A novel MAE workflow, where the input sentence is polluted for encoder and decoder with different masks. The sentence embedding is generated from the encoder‚Äôs masked input; then, the original sentence is recovered based on the sentence embedding and the decoder‚Äôs masked input via masked language modeling. 2) Asymmetric model structure, with a full-scale BERT like transformer as encoder, and a one-layer transformer as decoder. 3) Asymmetric masking ratios, with a moderate ratio for encoder: 15 30%, and an aggressive ratio for decoder: 50 70%. Our framework is simple to realize and empirically competitive: the pre-trained models dramatically improve the SOTA performances on a wide range of dense retrieval benchmarks, like BEIR and MS MARCO. The source code and pre-trained models are made publicly available at https://github.com/staoxiao/RetroMAE so as to inspire more interesting research.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 538‚Äì548\nDecember 7-11, 2022 ¬©2022 Association for Computational Linguistics\nRetroMAE: Pre-Training Retrieval-oriented Language Models Via\nMasked Auto-Encoder\nShitao Xiao1‚Ä†, Zheng Liu2‚Ä†, Yingxia Shao1‚àó, Zhao Cao2\n1: Beijing University of Posts and Telecommunications, Beijing, China\n2: Huawei Technologies Ltd. Co., Shenzhen, China\n{stxiao,shaoyx}@bupt.edu.cn, {liuzheng107,caozhao1}@huawei.com\nAbstract\nDespite pre-training‚Äôs progress in many impor-\ntant NLP tasks, it remains to explore effec-\ntive pre-training strategies for dense retrieval.\nIn this paper, we propose RetroMAE, a new\nretrieval oriented pre-training paradigm based\non Masked Auto-Encoder (MAE). RetroMAE\nis highlighted by three critical designs. 1)\nA novel MAE workÔ¨Çow , where the input\nsentence is polluted for encoder and decoder\nwith different masks. The sentence embed-\nding is generated from the encoder‚Äôs masked\ninput; then, the original sentence is recovered\nbased on the sentence embedding and the de-\ncoder‚Äôs masked input via masked language\nmodeling. 2) Asymmetric model structure ,\nwith a full-scale BERT like transformer as en-\ncoder, and a one-layer transformer as decoder.\n3) Asymmetric masking ratios , with a mod-\nerate ratio for encoder: 15 ‚àº30%, and an ag-\ngressive ratio for decoder: 50 ‚àº70%. Our\nframework is simple to realize and empirically\ncompetitive: the pre-trained models dramati-\ncally improve the SOTA performances on a\nwide range of dense retrieval benchmarks, like\nBEIR and MS MARCO. The source code and\npre-trained models are made publicly avail-\nable at https://github.com/staoxiao/RetroMAE\nso as to inspire more interesting research.\n1 Introduction\nDense retrieval is important to many web appli-\ncations. By letting semantically correlated query\nand document represented as spatially close em-\nbeddings, dense retrieval can be efÔ¨Åciently con-\nducted via approximate nearest neighbour search,\nsuch as PQ (Jegou et al., 2010; Xiao et al., 2021,\n2022a) and HNSW (Malkov and Yashunin, 2018).\nRecently, large-scale language models have been\nwidely used as the encoding networks for dense\nretrieval (Karpukhin et al., 2020; Xiong et al.,\n‚Ä†. The two researchers make equal contributions to this\nwork and are designated as co-Ô¨Årst authors.\n‚àó. Corresponding author.\n2020; Luan et al., 2021). The mainstream mod-\nels, e.g., BERT (Devlin et al., 2019), RoBERTa\n(Liu et al., 2019), T5 (Raffel et al., 2019), are usu-\nally pre-trained by token-level tasks, like MLM and\nSeq2Seq. However, the sentence-level representa-\ntion capability is not fully developed in these tasks,\nwhich restricts their potential for dense retrieval.\nGiven the above defect, there have been in-\ncreasing interests to develop retrieval oriented pre-\ntrained models. One popular strategy is to lever-\nage self-contrastive learning (Chang et al., 2020;\nGuu et al., 2020), where the model is trained to\ndiscriminate positive samples from data augmenta-\ntion. However, the self-contrastive learning can be\nseverely limited by the data augmentation‚Äôs quality;\nbesides, it usually calls for massive amounts of neg-\native samples (He et al., 2020a; Chen et al., 2020).\nAnother strategy relies on auto-encoding (Gao and\nCallan, 2021; Lu et al., 2021; Wang et al., 2021),\nwhich is free from the restrictions on data augmen-\ntation and negative sampling. The current works\nare differentiated in how the encoding-decoding\nworkÔ¨Çow is designed, and it remains an open prob-\nlem to explore more effective auto-encoding frame-\nwork for retrieval oriented pre-training.\nWe argue that two factors are critical for the auto-\nencoding based pre-training: 1) the reconstruction\ntask must be demanding enough on encoding qual-\nity, 2) the pre-training data needs to be fully uti-\nlized. We propose RetroMAE (Figure 1), which\noptimizes both aspects with the following designs.\n‚Ä¢A novel MAE workÔ¨Çow. The pre-training fol-\nlows a novel masked auto-encoding workÔ¨Çow. The\ninput sentence is polluted twice with two differ-\nent masks. One masked input is used by encoder,\nwhere the sentence embedding is generated. The\nother one is used by decoder: joined with the sen-\ntence embedding, the original sentence is recovered\nvia masked language modeling (MLM).\n‚Ä¢Asymmetric structure. RetroMAE adopts\nan asymmetric model structure. The encoder is a\n538\nNorwegian forest cat is a breed of dom-estic cat originating in northern Europe\n[M] forest cat is a breed of [M] cat originating in [M] Europe[M] [M] cat is [M] [M] of dom-estic [M] [M] in northern [M]\nEncoder Decoder\nMask (EN)Mask (DE)\nNorwegian forest cat is abreedof domestic catoriginating in northern EuropeSentenceembedding\nFigure 1: RetroMAE. The encoder utilizes a full-scale\nBERT, whose input is moderately masked. The decoder\nis a one-layer transformer, whose input is aggressively\nmasked. The original input is recovered based on the\nsentence embedding and the decoder‚Äôs input via MLM.\nfull-scale BERT, which is able to generate discrim-\ninative embedding for the input sentence. In con-\ntrast, the decoder follows an extremely simpliÔ¨Åed\nstructure, i.e., a single-layer transformer, which is\nlearned to reconstruct the input sentence.\n‚Ä¢Asymmetric masking ratios. The encoder‚Äôs\ninput is masked at a moderate ratio: 15 ‚àº30%,\nwhich is slightly above its traditional value in\nMLM. However, the decoder‚Äôs input is masked\nat a much more aggressive ratio: 50‚àº70%.\nThe above designs of RetroMAE are favorable\nto the pre-training effectiveness thanks to the fol-\nlowing reasons. Firstly, the auto-encoding is made\nmore demanding on encoding quality. The con-\nventional auto-regression may attend to the preÔ¨Åx\nduring the decoding process; and the conventional\nMLM only masks a small portion (15%) of the\ninput tokens. By comparison, RetroMAE aggres-\nsively masks most of the input for decoding. As\nsuch, the reconstruction will be not enough to lever-\nage the decoder‚Äôs input alone, but heavily depend\non the sentence embedding. Thus, it will force\nthe encoder to capture in-depth semantics of the\ninput. Secondly, it ensures training signals to be\nfully generated from the input sentence. For con-\nventional MLM-style methods, the training signals\nmay only be generated from 15% of the input to-\nkens. Whereas for RetroMAE, the training signals\ncan be derived from the majority of the input. Be-\nsides, knowing that the decoder only contains one-\nsingle layer, we further propose the enhanced de-\ncoding on top of two-stream attention (Yang et al.,\n2019) and position-speciÔ¨Åc attention mask (Dong\net al., 2019). As such, 100% of the tokens can\nbe used for reconstruction, and each token may\nsample a unique context for its reconstruction.\nRetroMAE is simple to realize and empirically\ncompetitive. We merely use a moderate-amount of\ndata (Wikipedia, BookCorpus, MS MARCO cor-\npus) for pre-training, where a BERT base scale\nencoder is learned. For the zero-shot setting, it pro-\nduces an average score of 45.2 on BEIR (Thakur\net al., 2021); and for the supervised setting, it may\neasily reach an MRR@10 of41.6 on MS MARCO\npassage retrieval (Nguyen et al., 2016) following\nstandard knowledge distillation procedures. Both\nvalues are unprecedented for dense retrievers with\nthe same model size and pre-training conditions.\nWe also carefully evaluate the impact introduced\nfrom each of the components, whose results may\nbring interesting insights to the future research.\n2 Related works\nDense retrieval is widely applied to web applica-\ntions, like search engines (Karpukhin et al., 2020),\nadvertising (Lu et al., 2020; Zhang et al., 2022)\nand recommender systems (Xiao et al., 2022b). It\nencodes query and document within the same la-\ntent space, where relevant documents to the query\ncan be efÔ¨Åciently retrieved via ANN search. The\nencoding model is critical for the retrieval quality.\nThanks to the recent development of large-scale\nlanguage models, e.g., BERT (Devlin et al., 2019),\nRoBERTa (Liu et al., 2019), and T5 (Raffel et al.,\n2019), there has been a major leap-forward for\ndense retrieval‚Äôs performance (Karpukhin et al.,\n2020; Luan et al., 2021; Lin et al., 2021).\nThe large-scale language models are highly dif-\nferentiated in terms of pre-training tasks. One com-\nmon task is the masked language modeling (MLM),\nas adopted by BERT (Devlin et al., 2019) and\nRoBERTa (Liu et al., 2019), in which the masked\ntokens are predicted based on their context. The\nbasic MLM is extended in many ways. For ex-\nample, tasks like entity masking, phrase masking\nand span masking (Sun et al., 2019; Joshi et al.,\n2020) may help the pre-trained models to better\nsupport the sequence labeling applications, such\nas entity resolution and question answering. Be-\nsides, tasks like auto-regression (Radford et al.,\n2018; Yang et al., 2019) and Seq2Seq (Raffel et al.,\n2019; Lewis et al., 2019) are also utilized, where\nthe pre-trained models are enabled to serve NLG\nrelated scenarios. However, most of the generic\npre-trained models are based on token-level tasks,\n539\nwhere the sentence representation capability is not\neffectively developed (Chang et al., 2020). Thus, it\nmay call for a great deal of labeled data (Nguyen\net al., 2016; Kwiatkowski et al., 2019) and sophis-\nticated Ô¨Åne-tuning methods (Xiong et al., 2020;\nQu et al., 2020) to ensure the pre-trained models‚Äô\nperformance for dense retrieval.\nTo mitigate the above problem, recent works\npropose retrieval oriented pre-trained models. The\nexisting methods can be divided as the ones based\non self-contrastive learning (SCL) and the ones\nbased on auto-encoding (AE). The SCL based\nmethods (Chang et al., 2020; Guu et al., 2020;\nXu et al., 2022) rely on data augmentation, e.g.,\ninverse cloze task (ICT), where positive samples\nare generated for each anchor sentence. Then, the\nlanguage model is learned to discriminate the posi-\ntive samples from the negative ones via contrastive\nlearning. However, the self-contrastive learning\nusually calls for huge amounts of negative sam-\nples, which is computationally expensive. Besides,\nthe pre-training effect can be severely restricted by\nthe quality of data augmentation. The AE based\nmethods are free from these restrictions, where\nthe language models are learned to reconstruct the\ninput sentence based on the sentence embedding.\nThe existing methods utilize various reconstruction\ntasks, such as MLM (Gao and Callan, 2021) and\nauto-regression (Lu et al., 2021; Wang et al., 2021;\nLi et al., 2020), which are highly differentiated in\nterms of how the original sentence is recovered and\nhow the training loss is formulated. For example,\nthe auto-regression relies on the sentence embed-\nding and preÔ¨Åx for reconstruction; while MLM\nutilizes the sentence embedding and masked con-\ntext. The auto-regression derives its training loss\nfrom the entire input tokens; however, the conven-\ntional MLM only learns from the masked positions,\nwhich accounts for 15% of the input tokens. Ideally,\nwe expect the decoding operation to be demanding\nenough, as it will force the encoder to fully capture\nthe semantics about the input so as to ensure the\nreconstruction quality. Besides, we also look for-\nward to high data efÔ¨Åciency, which means the input\ndata can be fully utilized for the pre-training task.\n3 Methodology\nWe develop a novel masked auto-encoder for re-\ntrieval oriented pre-training. The model contains\ntwo modules: a BERT-like encoder Œ¶enc(¬∑) to gen-\nerate sentence embedding, and a one-layer trans-\nformer based decoder Œ¶dec(¬∑) for sentence recon-\nstruction. The original sentence X is masked as\nÀúXenc and encoded as the sentence embedding h ÀúX.\nThe sentence is masked again (with a different\nmask) as ÀúXdec; together with h ÀúX, the original sen-\ntence X is reconstructed. Detailed elaborations\nabout RetroMAE are made as follows.\n3.1 Encoding\nThe input sentence X is polluted as ÀúXenc for the\nencoding stage, where a small fraction of its tokens\nare randomly replaced by the special token [M]\n(Figure 2. A). We apply a moderate masking ratio\n(15‚àº30%), which means the majority of informa-\ntion about the input will be preserved. Then, the\nencoder Œ¶enc(¬∑) is used to transform the polluted\ninput as the sentence embedding h ÀúX:\nh ÀúX ‚ÜêŒ¶enc( ÀúXenc). (1)\nWe apply a BERT like encoder with 12 layers and\n768 hidden-dimensions, which helps to capture the\nin-depth semantics of the sentence. Following the\ncommon practice, we select the [CLS] token‚Äôs Ô¨Ånal\nhidden state as the sentence embedding.\n3.2 Decoding\nThe input sentence X is polluted as ÀúXdec for the\ndecoding stage (Figure 2. B). The masking ra-\ntio is more aggressive than the one used by the\nencoder, where 50‚àº70% of the input tokens will\nbe masked. The masked input is joined with the\nsentence embedding, based on which the original\nsentence is reconstructed by the decoder. Particu-\nlarly, the sentence embedding and the masked input\nare combined into the following sequence:\nH ÀúXdec\n‚Üê[h ÀúX, ex1 + p1, ...,exN + pN ]. (2)\nIn the above equation, exi denotes the embedding\nof xi, to which an extra position embedding pi\nis added. Finally, the decoder Œ¶dec is learned to\nreconstruct the original sentence X by optimizing\nthe following objective:\nLdec =\n‚àë\nxi‚ààmasked\nCE(xi|Œ¶dec(H ÀúXdec\n)), (3)\nwhere CE is the cross-entropy loss. As men-\ntioned, we use a one-layer transformer based de-\ncoder. Given the aggressively masked input and\nthe extremely simpliÔ¨Åed network, the decoding be-\ncomes challenging, which forces the generation of\nhigh-quality sentence embedding so that the origi-\nnal input can be recovered with good Ô¨Ådelity.\n540\nùë•![M] [M]ùë•\"P! P\" P#P$\nùë•# ùë•$\nùëØ%&%&'(B) Decoding(A) Encoding\nEncoder\nCLSùë•! ùë•$\nùíâ%ùëø\n)ùëã()*\n[M]ùë•#\nùë•\"MLM\nP+ P! P\" P#P$\nùëØ! ùëØ\"\nùë•! ùë•\"P! P\" P#P$ùë•# ùë•$\nùë•! ùë•\"ùë•# ùë•$ mask1234\n1234\n00\n‚äï\n(C) Enhanced decoding\nFigure 2: RetroMAE pre-training workÔ¨Çow. (A) Encoding: the input is moderately masked and encoded as the\nsentence embedding (the green rectangle). (B) Decoding: the input is aggressively masked, and joined with the\nsentence embedding to reconstruct the masked tokens (the shadowed tokens). (C) Enhanced encoding: all input\ntokens are reconstructed based on the sentence embedding and the visible context in each row (deÔ¨Åned in Eq. 7);\nthe main diagonal positions are Ô¨Ålled with ‚àí‚àû(grey), and positions for the visible context are Ô¨Ålled with 0 (blue).\n3.3 Enhanced Decoding\nOne limitation about the decoding process is that\nthe training signals, i.e., the cross-entropy loss, can\nonly be derived from the masked tokens. Besides,\nevery masked token is always reconstructed based\non the same context, i.e., H ÀúXdec\n. We argue that the\npre-training effect can be further enhanced provid-\ning that 1) more training signals can be derived\nfrom the input sentence, and 2) the reconstruction\ntask can be performed based on diversiÔ¨Åed con-\ntexts. To this end, we propose the enhanced de-\ncoding inspired by two-stream self-attention (Yang\net al., 2019) and position-speciÔ¨Åc attention mask\n(Dong et al., 2019). Particularly, we generate two\ninput streams: H1 (query) and H2 (context), for\nthe decoding operation (Figure 2. C):\nH1 ‚Üê[h ÀúX + p0, ...,h ÀúX + pN ],\nH2 ‚Üê[h ÀúX, ex1 + p1, ...,exN + pN ]. (4)\nwhere h ÀúX is the sentence embedding, exi is the\ntoken embedding (no token is masked in this place),\nand pi is the position embedding. We introduce\nthe position-speciÔ¨Åc attention mask M ‚ààRL√óL,\nwhere the self-attention is performed as:\nQ = H1WQ, K = H2WK, V = H2WV ;\nMij =\n{\n0, can be attended,\n‚àí‚àû, masked;\nA = softmax(QT K‚àö\nd\n+ M)V.\n(5)\nThe output A, together with H1 (because of the\nresidual connection) are used to reconstruct the\noriginal input (other operations, like layer-norm\nand FFN, are omitted from our discussion). Finally,\nthe following objective will be optimized:\nLdec =\n‚àë\nxi‚ààX\nCE(xi|A, H1). (6)\nKnowing that the decoder only consists of one\nsingle transformer layer, each token xi is recon-\nstructed based on the context which are visible to\nthe i-th row of matrix M. In this place, the fol-\nlowing rules are applied to generate the position\nspeciÔ¨Åc attention mask matrix M:\nMij =\n{\n0, xj ‚ààs(XÃ∏=i), or j|iÃ∏=0 = 0\n‚àí‚àû, otherwise. (7)\nThe sampled tokens, s(XÃ∏=i), and the 1st position\n(except for the 1st row) will be visible when recon-\nstructing xi. The diagonal elements, i.e., xi for the\ni-th row, will always be excluded, which means\nthey will always be masked; as a result, each token\ncannot attend to itself during the reconstruction.\nWe summarize the pre-training workÔ¨Çow with\nthe enhanced decoding as Algorithm 1. Note that\nthe original masked language modeling task in\nBERT is kept in encoder. The corresponding loss,\ndenoted as Lenc, is added with the decoder‚Äôs loss,\nwhich formulates the Ô¨Ånal training loss. The fol-\nlowing features need to be emphasized for our pre-\ntraining workÔ¨Çow. Firstly, the reconstruction task\nis demanding given the aggressive masking ratio\nand the extremely simpliÔ¨Åed network of decoder.\nSecondly, we may derive abundant pre-training\nsignals from the unsupervised corpus since all to-\nkens within each input sentence can be used for the\nreconstruction. Finally, the pre-training is simple\n541\nAlgorithm 1: RetroMAE\n1 begin\n2 ÀúXenc ‚Üêmask(X);\n3 h ÀúX ‚ÜêŒ¶enc( ÀúXenc);\n4 H1, H2 ‚ÜêEq. 4;\n5 M ‚ÜêEq. 7;\n6 A ‚Üêbased on H1, H2, M as Eq. 5;\n7 Ldec ‚ÜêEq. 6 ;\n8 model update w.r.t. Lenc + Ldec;\nto realize: 1) there are no requirements on sophis-\nticated data augmentation and negative sampling,\nand 2) the computation cost is maintained to be sim-\nilar with the conventional BERT/RoBERTa style\npre-training given the simplicity of decoder.\n4 Experimental Studies\nWe evaluate the retrieval performance of the sen-\ntence embedding generated by RetroMAE‚Äôs pre-\ntrained encoder, where two major issues are ex-\nplored. 1) RetroMAE‚Äôs impact on zero-shot and\nsupervised dense retrieval, in comparison with the\ngeneric pre-trained models and the retrieval ori-\nented pre-trained models. 2) The impact from the\nfour technical factors in RetroMAE: the enhanced\ndecoding, the decoder size, the decoder‚Äôs masking\nratio, and the encoder‚Äôs masking ratio.\n4.1 Experiment Settings\nThe following datasets are utilized for the pre-\ntraining and evaluation of RetroMAE.\n‚Ä¢Pre-training. We reuse the same pre-training\ncorpora as the ones utilized by BERT (Devlin et al.,\n2019): English Wikipediaand BookCorpus. Cor-\nresponding datasets are also frequently used by the\nprevious works on retrieval oriented pre-training,\nsuch as SEED (Lu et al., 2021) and Condenser (Gao\nand Callan, 2021). Following coCondenser (Gao\nand Callan, 2022), we also use MS MARCO cor-\npus to analyze the effect of in-domain pre-training\nfor dense retrieval (which we Ô¨Ånd critical to the\nperformance on MS MARCO but unnecessary to\nthe performances on other datasets).\n‚Ä¢Evaluation. We use two datasets to evalu-\nate the retrieval performance after supervision. 1)\nMS MARCO (Nguyen et al., 2016), which con-\ntains queries from Bing Search. We use its pas-\nsage retrieval task, which contains 502,939 train-\ning queries and 6,980 evaluation queries (Dev).\nThe answer needs to be retrieved from 8.8 mil-\nlion candidate passages. 2) Natural Questions\n(Kwiatkowski et al., 2019), which consists of\nqueries from Google. There are 79,168 train-\ning queries, 8,757 dev queries, and 3,610 testing\nqueries. The answer is retrieved from 21,015,324\nWikipedia passages. We evaluates the zero-shot re-\ntrieval performance on BEIR benchmark (Thakur\net al., 2021). It Ô¨Åne-tunes the pre-trained model\nwith MS MARCO queries, and evaluate the zero-\nshot transferability on the other 18 datasets. The\nevaluation data covers dense retrieval tasks across\ndifferent domains, such as question answering, fact\nchecking, bio-medical retrieval, news retrieval, and\nsocial media retrieval, etc.\nWe consider three types of baseline methods in\nour experimental studies1.\n‚Ä¢Generic models. The following generic pre-\ntrained language models are used in our experi-\nments. 1) BERT (Devlin et al., 2019), which is\nthe most popular pre-trained language model in\npractice. It is also frequently Ô¨Åne-tuned as the en-\ncoding network for dense retrievers (Karpukhin\net al., 2020; Xiong et al., 2020). 2) RoBERTa\n(Liu et al., 2019), which is an enhanced replication\nof BERT with substantially enlarged training data\nand improved training settings. 3) DeBERTa(He\net al., 2020b), which further improves BERT and\nRoBERTa with disentangled attention mechanism\nand an enhanced mask decoder; it is one of the\nstrongest pre-trained models on NLU benchmarks,\nsuch as GLUE and SuperGLUE.\n‚Ä¢Retrieval oriented models. We consider two\ntypes of retrieval oriented pre-trained models in our\nexperiments. One is based on self-contrastive learn-\ning, where the following methods are included. 1)\nSimCSE (Gao et al., 2021), in which the language\nmodel is learned to discriminate different views\nof the anchor sentence augmented by drop-out. 2)\nLaPraDoR (Xu et al., 2022), an enhancement of\nconventional ICT pre-training (Guu et al., 2020;\nChang et al., 2020); it proposes to train the query\nand document encoder iteratively so that the scale\nof negative samples can be expanded. 3) DiffCSE\n(Chuang et al., 2022), which enhances SimCSE\nwith the jointly utilization of self-contrastive learn-\ning and conditional difference prediction. The other\none is based on auto-encoding, in which the follow-\ning methods are included. 4) Condenser (Gao and\n1. For all baseline methods, we use their ofÔ¨Åcially released\npre-trained models for our experiments.\n542\nDatasets BERT RoBERTa DeBERTa LaPraDoR SimCSE DiffCSE SEED Condenser RetroMAE\nTREC-COVID 0.615 0.649 0.688 0.478 0.460 0.492 0.627 0.750 0.772\nBioASQ 0.253 0.279 0.290 0.252 0.263 0.258 0.308 0.322 0.421\nNFCorpus 0.260 0.243 0.238 0.310 0.260 0.259 0.278 0.277 0.308\nNQ 0.467 0.413 0.452 0.454 0.435 0.412 0.446 0.486 0.518\nHotpotQA 0.488 0.448 0.474 0.513 0.502 0.499 0.541 0.538 0.635\nFiQA-2018 0.252 0.291 0.299 0.288 0.250 0.229 0.259 0.259 0.316\nSignal-1M(RT) 0.204 0.229 0.243 0.241 0.262 0.260 0.256 0.261 0.265\nTREC-NEWS 0.362 0.385 0.378 0.286 0.356 0.363 0.358 0.376 0.428\nRobust04 0.351 0.384 0.378 0.299 0.330 0.343 0.365 0.349 0.447\nArguAna 0.265 0.395 0.297 0.499 0.413 0.468 0.389 0.298 0.433\nTouche-2020 0.259 0.299 0.271 0.137 0.159 0.168 0.225 0.248 0.237\nCQADupStack 0.282 0.278 0.279 0.309 0.290 0.305 0.290 0.347 0.317\nQuora 0.787 0.509 0.846 0.837 0.844 0.850 0.852 0.853 0.847\nDBPedia 0.314 0.275 0.271 0.334 0.314 0.303 0.330 0.339 0.390\nSCIDOCS 0.113 0.111 0.106 0.150 0.124 0.125 0.124 0.133 0.150\nFEVER 0.682 0.683 0.594 0.511 0.623 0.641 0.641 0.691 0.774\nClimate-FEVER 0.187 0.222 0.160 0.173 0.211 0.200 0.176 0.211 0.232\nSciFact 0.533 0.539 0.543 0.531 0.554 0.523 0.575 0.593 0.653\nA VERAGE 0.371 0.368 0.378 0.367 0.369 0.372 0.391 0.407 0.452\nTable 1: Zero-shot dense retrieval performances on BEIR benchmark (measured by NDCG@10).\nCallan, 2021), where the sentence embedding is\njoined with the intermediate hidden-states from en-\ncoder for MLM. 5) SEED (Lu et al., 2021), where\nthe sentence embedding is used to recover the orig-\ninal input via auto-regression.\n‚Ä¢Implementation details. RetroMAE utilizes\nbi-directional transformers as its encoder, with 12\nlayers, 768 hidden-dim, and a 30522-token vocabu-\nlary (same as BERT base). The decoder is a one-\nlayer transformer. The default masking ratios are\n0.3 for encoder and 0.5 for decoder. The model\nis trained for 8 epochs, with AdamW optimizer,\nbatch-size 32 (per device), learning rate 1e-4. The\ntraining is on a machine with 8 √óNvidia A100\n(40GB) GPUs. The models are implemented with\nPyTorch 1.8 and HuggingFace transformers 4.16.\nWe adopt the ofÔ¨Åcial script 2 from BEIR to prepare\nthe models for their zero-shot evaluation. For super-\nvised evaluations, we use DPR (Karpukhin et al.,\n2020) and ANCE (Xiong et al., 2020) to Ô¨Åne-tune\nthe pre-trained models. We also evaluate the mod-\nels‚Äô performance on MS MARCO with standard\nknowledge distillation. Particularly, we train one\nBERT base scale cross-encoder over hard negatives\nreturned by the ANCE-Ô¨Ånetuned bi-encoder; then,\nwe further Ô¨Ånetune the bi-encoder by minimizing\nthe KL-divergence with the cross-encoder.\n2. https://github.com/beir-cellar/beir/blob/\nmain/examples/retrieval/training/train_\nmsmarco_v3.py\n4.2 Main Results\nWe analyze the zero-shot performances in Table 1,\nwhere RetroMAE achieves remarkable advantages:\nit produces the best empirical results on most of\nthe datasets, and it surpasses the strongest baseline\nby +4.5% in total average. The leap-forward of\nzero-shot performance is much larger than any of\nthe improvements made by the baseline models.\nThe improvement is indeed thrilling knowing that\nit is not from the increasing of model scale or the\nenrichment of pre-training data, but purely from\nthe upgrade of pre-training algorithm. We further\ndemonstrate the supervised evaluations in Table\n2 and 3, where the pre-trained models are Ô¨Åne-\ntuned with DPR and ANCE. The baselines are par-\ntitioned into two groups according to whether they\nare generic pre-trained models or retrieval oriented\nones. It can be observed that RetroMAE maintains\nnotable advantages over the baselines. With DPR\nÔ¨Åne-tuning, it outperforms the strongest baselines\nby +1.96% (MRR@10) and +1.48% (Recall@10)\non the two datasets; with ANCE Ô¨Åne-tuning, corre-\nsponding advantages become +1.42% (MRR@10)\nand +1.41% (Recall@10).\nAs for RetroMAE‚Äôs pre-trained models over\nMS MARCO corpus. With ANCE Ô¨Åne-tuning (Ta-\nble 4), our method outperforms its peer approach\ncoCondenser (Gao and Callan, 2022) by +1.1% on\nMRR@10 (which is of the same model size and\nsame pre-training data). While under the knowl-\n543\nMS MARCO Natural Question\nMethods MRR@10 MRR@100 R@10 R@100 R@1000 R@10 R@20 R@30 R@50 R@100\nBERT 0.3170 0.3278 0.5801 0.8570 0.9598 0.7399 0.7925 0.8136 0.8396 0.8668\nRoBERTa 0.3136 0.3258 0.5638 0.8478 0.9579 0.7150 0.7676 0.7939 0.8211 0.8476\nDeBERTa 0.3186 0.3304 0.5824 0.8625 0.9654 0.7152 0.7778 0.8022 0.8269 0.8510\nSimCSE 0.3193 0.3307 0.5907 0.8653 0.9699 0.7291 0.7864 0.8125 0.8391 0.8670\nLaPraDoR 0.3191 0.3307 0.5833 0.8537 0.9602 0.7377 0.7920 0.8155 0.8399 0.8677\nDiffCSE 0.3202 0.3311 0.5832 0.8561 0.9607 0.7393 0.7934 0.8155 0.8407 0.8673\nSEED 0.3264 0.3374 0.5913 0.8535 0.9539 0.7454 0.7958 0.8208 0.8432 0.8701\nCondenser 0.3357 0.3471 0.6082 0.8770 0.9683 0.7562 0.8053 0.8269 0.8501 0.8711\nRetroMAE 0.3553 0.3665 0.6356 0.8922 0.9763 0.7704 0.8172 0.8399 0.8604 0.8812\nTable 2: Supervised evaluation results based on DPR Ô¨Åne-tuning.\nMS MARCO Natural Question\nMethods MRR@10 MRR@100 R@10 R@100 R@1000 R@10 R@20 R@30 R@50 R@100\nBERT 0.3460 0.3569 0.6220 0.8734 0.9642 0.7875 0.8227 0.8460 0.8601 0.8776\nRoBERTa 0.3433 0.3543 0.6130 0.8705 0.9637 0.7629 0.8053 0.8277 0.8449 0.8698\nDeBERTa 0.3396 0.3512 0.6016 0.8719 0.9670 0.7654 0.8097 0.8288 0.8479 0.8698\nSimCSE 0.3520 0.3623 0.6276 0.8849 0.9738 0.7742 0.8194 0.8418 0.8626 0.8864\nLaPraDoR 0.3456 0.3564 0.6129 0.8755 0.9640 0.7801 0.8247 0.8424 0.8590 0.8773\nDiffCSE 0.3462 0.3571 0.6217 0.8748 0.9654 0.7853 0.8252 0.8410 0.8620 0.8784\nSEED 0.3544 0.3653 0.6263 0.8812 0.9687 0.7803 0.8258 0.8449 0.8684 0.8870\nCondenser 0.3635 0.3742 0.6388 0.8912 0.9722 0.7903 0.8325 0.8524 0.8668 0.8834\nRetroMAE 0.3822 0.3928 0.6677 0.9074 0.9807 0.8044 0.8443 0.8632 0.8776 0.8942\nTable 3: Supervised evaluation results based on ANCE Ô¨Åne-tuning.\nMS MARCO\nMethods MRR@10 R@10 R@100 R@1000\ncoCondenser 0.382 - - 0.984\nRetroMAE 0.393 0.675 0.918 0.985\nTable 4: RetroMAE vs. coCondenser on MS MARCO.\nBoth models are Ô¨Åne-tuned by ANCE.\nMS MARCO\nMethods MRR@10 R@10 R@100 R@1000\nAR2 0.395 - - 0.986\nColBERTv2 0.397 - - 0.984\nRocketQAv2 0.388 - - 0.981\nERNIE-Search 0.401 - - 0.982\nRetroMAE 0.416 0.709 0.927 0.988\nTable 5: RetroMAE vs. the recent dense retrievers; all\nmodels are Ô¨Åne-tuned by knowledge distillation.\nedge distillation scenario (Table 5), RetroMAE sur-\npasses a series of strong baselines proposed in the\nrecent period, including the models with highly so-\nphisticated distillation methods: AR2 (Zhang et al.,\n2021) by +2.1%, RocketQAv2 (Ren et al., 2021) by\n+2.8%, ERNIE-search (Lu et al., 2022) by +1.5%,\nand the late-interaction model ColBERTv2 (San-\nthanam et al., 2021) by +1.9%.\nTo summarize, the above results verify Retro-\nMAE‚Äôs effectiveness from two aspects. 1) It sub-\nstantially improves the pre-trained model‚Äôstrans-\nferability, which helps to result in superior zero-\nshot performances on out-of-domain datasets. 2)\nIt provides a strong initialization of dense re-\ntriever; after Ô¨Åne-tuned with in-domain data, it\ngives rise to a high-quality supervised retrieval per-\nformance in the corresponding scenario. Besides\nthe primary results, we may also have the following\ninteresting observations.\nFirstly, the advanced pre-training methods in\ngeneric areas do not necessarily contribute to the\ndense retrieval performances. Particularly, both\nRoBERTa and DeBERTa are major improvements\nof BERT; however, none of them is able to achieve\nbetter performances than BERT as they did on gen-\neral NLU benchmarks. This observation further\nsupports the motivation to develop retrieval ori-\nented pre-trained models.\nSecondly, the auto-encoding style pre-training\n(adopted by SEED, Condenser, and RetroMAE)\nis empirically proved to be much more favorable\nfor dense retrieval, given its dominance over the\ngeneric and self-contrastive learning based pre-\n544\nMS MARCO Natural Questions\nFactor Setting MRR@10 MRR@100 R@100 R@1000 MRR@10 MRR@100 R@100 R@1000\nDecoding w. enhance 0.3553 0.6356 0.8922 0.9763 0.7704 0.8399 0.8604 0.8812\nw.o. enhance 0.3462 0.6218 0.8813 0.9725 0.7562 0.8291 0.8540 0.8759\nDecoder\nsize (w.o.)\n#layer = 1 0.3462 0.6218 0.8813 0.9725 0.7562 0.8291 0.8540 0.8759\n#layer = 2 0.3446 0.6217 0.8828 0.9729 0.7561 0.8289 0.8538 0.8759\n#layer = 3 0.3439 0.6223 0.8829 0.9730 0.7563 0.8290 0.8537 0.8760\nMask ratio\n(decoder)\n0.15 (w.) 0.3496 0.6297 0.8905 0.9734 0.7608 0.8309 0.8554 0.8750\n0.5 (w.) 0.3553 0.6356 0.8922 0.9763 0.7704 0.8399 0.8604 0.8812\n0.9 (w.) 0.3514 0.6285 0.8905 0.9740 0.7609 0.8343 0.8562 0.8756\n0.15 (w.o.) 0.3440 0.6177 0.8802 0.9700 0.7519 0.8253 0.8523 0.8758\n0.7 (w.o.) 0.3508 0.6262 0.8850 0.9738 0.7593 0.8327 0.8551 0.8760\n0.9 (w.o.) 0.3441 0.6198 0.8803 0.9725 0.7576 0.8307 0.8551 0.8745\nMask ratio\n(encoder)\n0.15 (w.) 0.3501 0.6306 0.8890 0.9757 0.7703 0.8404 0.8604 0.8795\n0.3 (w.) 0.3553 0.6356 0.8922 0.9763 0.7704 0.8399 0.8604 0.8812\n0.9 (w.) 0.3365 0.6143 0.8750 0.9701 0.7599 0.8296 0.8508 0.8692\nTable 6: Ablation studies. (‚Äúw.‚Äù/‚Äúw.o.‚Äù indicates ‚Äúwith‚Äù/‚Äúwithout‚Äù using the enhanced decoding.)\ntrained models in both zero-shot evaluation and\nsupervised evaluation.\nThirdly, the self-contrastive learning based pre-\ntrained models bring very little improvements over\nthe generic ones when Ô¨Åne-tuning is made avail-\nable, as their performances are close to each other\nin both supervised and zero-shot evaluations. In\nfact, there is no supervise about this observation\nconsidering the similar results reported by recent\nstudies on dense retrieval (Gao and Callan, 2021)\n(BERT against ICT) and image processing (El-\nNouby et al., 2021) (BEiT (Bao et al., 2021) against\nMoCo/DINO). That is the pre-trained models from\nself-contrastive learning tend to have comparatively\nlimited potential for Ô¨Åne-tuning. Therefore, we\nargue that the retrieval-oriented pre-training algo-\nrithm should be properly designed according to the\ncondition of Ô¨Åne-tuning in speciÔ¨Åc scenarios.\n4.3 Ablation Studies\nWe ablate RetroMAE with its supervised perfor-\nmance (DPR Ô¨Åne-tuning) in Table 6, where the fol-\nlowing factors are analyzed: 1) decoding method,\n2) decoder‚Äôs size, 3) decoder‚Äôs masking ratio, 4) en-\ncoder‚Äôs masking ratio. We may have the following\nobservations from our results.\nFirstly of all, we analyze the impact from the\ndecoding method, i.e., whether the enhanced de-\ncoding is used or not. It can be observed that the\nenhanced decoding (w.) notably outperforms the\nbasic decoding (w.o.). Such an empirical advantage\ncan be attributed to the improved data efÔ¨Åciency of\nthe enhanced decoding. Under the default masking\nratio, the basic decoding merely samples 50% of\nthe input tokens for reconstruction, all of which\nare predicted based on the same context. With the\nenhanced decoding (Section 3.3), all of the input\ntokens can be utilized for reconstruction, and each\nof the tokens is reconstructed based on a unique\ncontext sampled as Eq. 7. As such, the enhanced\ndecoding may obtain more sufÔ¨Åcient and diversi-\nÔ¨Åed training signals from the input data.\nSecondly, we analyze the impact from decoder\nsize, with the number of transformer layers in-\ncreased from 1 to 3. Knowing that the enhanced\ndecoding is only applicable for single-layer trans-\nformers, it is disabled for these experiments. De-\nspite higher computation costs, the enlarged de-\ncoders do not bring any empirical gains. Besides,\nconsidering that the enhanced decoding has to be\ndisabled for large decoders (#layer>1), which will\nseverely harm the retrieval performances, the one-\nlayer decoder is proved to be the best option.\nThirdly, we make an analysis for differentmask-\ning ratios of decoder , whose value is increased\nfrom 0.15 to 0.9. We introduce two sets of ex-\nperiments: one with the enhanced decoding (w.),\nand the other one without using enhanced decoding\n(w.o.). For both experiments, we observe substan-\ntial improvements of retrieval quality resulted from\nthe aggressive masking ratios. For enhanced de-\ncoding (w.), the optimal performance is achieved\nat 0.5; without using enhanced decoding (w.o.), the\noptimal performance is reached at 0.7. This minor\ndifference is probably because: unlike ‚Äúw.‚Äù where\nall input tokens can be reconstructed, ‚Äúw.o.‚Äù only\nreconstructs the masked tokens; thus, it may trade\na larger ratio for the increasing of training signals.\n545\nLastly, we study the encoder‚Äôs masking ratio.\nIt is quite interesting that a slightly improved mask-\ning ratio of 0.3 also improves the empirical perfor-\nmances, compared with the commonly used value\n0.15. For both encoder and decoder, the increased\nreconstruction difÔ¨Åculty can be the common rea-\nson why the increased masking ratios beneÔ¨Åt the\nretrieval quality. However, different from decoder,\na too aggressive ratio of encoder, e.g., 0.9, will\nseverely harm the retrieval performance. This is be-\ncause a too large masking ratio will prevent the gen-\neration of high-quality sentence embedding, con-\nsidering that most of the useful information of the\ninput sentence will be discarded.\nThe ablation studies are concluded as follows:\n1) RetroMAE‚Äôs performance can be notably im-\nproved by the enhanced decoding; 2) the one-layer\ntransformer is the best for decoder; 3) the retrieval\nquality can be improved from an aggressive mask-\ning ratio of decoder, and a moderately improved\nmasking ratio of encoder.\n5 Conclusion\nWe propose RetroMAE, a novel masked auto-\nencoding framework to pre-train retrieval oriented\nlanguage models: the input sentence is randomly\nmasked for encoder and decoder, and the sentence\nembedding is joined with the decoder‚Äôs masked in-\nput to reconstruct the original input. We introduce\nan asymmetric model structure (full-scale encoder\nand single-layer decoder) and asymmetric masking\nratios (a moderate ratio for encoder and an aggres-\nsive one for decoder), which makes the reconstruc-\ntion sufÔ¨Åciently demanding. We also introduce the\nenhanced decoding, which makes the full utiliza-\ntion of the pre-training data. Our experiments on\nBEIR, MS MARCO, and Natural Question validate\nRetroMAE‚Äôs effectiveness, as signiÔ¨Åcant improve-\nments on both zero-shot and supervised evaluations\ncan be achieved over the existing methods.\n6 Limitations\nSo far, our empirical studies are performed based\non BERT base scale transformers; meanwhile,\nmerely a moderate amount of pre-training data is\nused (mainly due to the limitations on computation\nresources). Despite the demonstrated effectiveness,\nit‚Äôs still necessary to explore the impact from en-\nlarged networks and increased pre-training data, as\nboth factors were found to be important in recent\nworks (Ni et al., 2021).\nAcknowledgements\nYingxia Shao‚Äôs work is supported by the Na-\ntional Natural Science Foundation of China (Nos.\nU1936104, 61902037, 62272054, 62192784), and\nCCF-Tencent Open Fund.\nReferences\nHangbo Bao, Li Dong, and Furu Wei. 2021. Beit: Bert\npre-training of image transformers. arXiv preprint\narXiv:2106.08254.\nWei-Cheng Chang, Felix X Yu, Yin-Wen Chang,\nYiming Yang, and Sanjiv Kumar. 2020. Pre-\ntraining tasks for embedding-based large-scale re-\ntrieval. arXiv preprint arXiv:2002.03932.\nTing Chen, Simon Kornblith, Mohammad Norouzi,\nand Geoffrey Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. In In-\nternational conference on machine learning, pages\n1597‚Äì1607. PMLR.\nYung-Sung Chuang, Rumen Dangovski, Hongyin Luo,\nYang Zhang, Shiyu Chang, Marin Soljacic, Shang-\nWen Li, Wen-tau Yih, Yoon Kim, and James R.\nGlass. 2022. Diffcse: Difference-based con-\ntrastive learning for sentence embeddings. CoRR,\nabs/2204.10298.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies,, pages 4171‚Äì4186. Association for Com-\nputational Linguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. UniÔ¨Åed language\nmodel pre-training for natural language understand-\ning and generation. Advances in Neural Information\nProcessing Systems, 32.\nAlaaeldin El-Nouby, Gautier Izacard, Hugo Tou-\nvron, Ivan Laptev, Herv ¬¥e Jegou, and Edouard\nGrave. 2021. Are large-scale datasets necessary\nfor self-supervised pre-training? arXiv preprint\narXiv:2112.10740.\nLuyu Gao and Jamie Callan. 2021. Condenser: a pre-\ntraining architecture for dense retrieval. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 981‚Äì993.\nLuyu Gao and Jamie Callan. 2022. Unsupervised cor-\npus aware language model pre-training for dense\npassage retrieval. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2843‚Äì2853,\nDublin, Ireland.\n546\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. arXiv preprint arXiv:2104.08821.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. arXiv\npreprint arXiv:2002.08909.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss Girshick. 2020a. Momentum contrast for unsu-\npervised visual representation learning. In Proceed-\nings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 9729‚Äì9738.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020b. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\nHerve Jegou, Matthijs Douze, and Cordelia Schmid.\n2010. Product quantization for nearest neighbor\nsearch. IEEE transactions on pattern analysis and\nmachine intelligence, 33(1):117‚Äì128.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64‚Äì77.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing, pages 6769‚Äì6781.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nÔ¨Åeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\nKenton Lee, et al. 2019. Natural questions: a bench-\nmark for question answering research. Transactions\nof the Association for Computational Linguistics,\n7:453‚Äì466.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nChunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiu-\njun Li, Yizhe Zhang, and Jianfeng Gao. 2020. Opti-\nmus: Organizing sentences via pre-trained modeling\nof a latent space. arXiv preprint arXiv:2004.04092.\nJimmy Lin, Rodrigo Nogueira, and Andrew Yates.\n2021. Pretrained transformers for text ranking: Bert\nand beyond. Synthesis Lectures on Human Lan-\nguage Technologies, 14(4):1‚Äì325.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nShuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed\nMalik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu,\nand Arnold Overwijk. 2021. Less is more: Pretrain\na strong Siamese encoder for dense text retrieval us-\ning a weak decoder. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 2780‚Äì2791.\nWenhao Lu, Jian Jiao, and Ruofei Zhang. 2020.\nTwinbert: Distilling knowledge to twin-structured\nbert models for efÔ¨Åcient retrieval. arXiv preprint\narXiv:2002.06275.\nYuxiang Lu, Yiding Liu, Jiaxiang Liu, Yunsheng Shi,\nZhengjie Huang, Shikun Feng Yu Sun, Hao Tian,\nHua Wu, Shuaiqiang Wang, Dawei Yin, et al. 2022.\nErnie-search: Bridging cross-encoder with dual-\nencoder via self on-the-Ô¨Çy distillation for dense pas-\nsage retrieval. arXiv preprint arXiv:2205.09153.\nYi Luan, Jacob Eisenstein, Kristina Toutanova, and\nMichael Collins. 2021. Sparse, dense, and atten-\ntional representations for text retrieval. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:329‚Äì345.\nYu A Malkov and Dmitry A Yashunin. 2018. EfÔ¨Åcient\nand robust approximate nearest neighbor search us-\ning hierarchical navigable small world graphs.IEEE\ntransactions on pattern analysis and machine intelli-\ngence, 42(4):824‚Äì836.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. Ms marco: A human generated machine read-\ning comprehension dataset. In CoCo@ NIPS.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\ntavo Hern ¬¥andez ¬¥Abrego, Ji Ma, Vincent Y Zhao,\nYi Luan, Keith B Hall, Ming-Wei Chang, et al.\n2021. Large dual encoders are generalizable retriev-\ners. arXiv preprint arXiv:2112.07899.\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\nand Haifeng Wang. 2020. Rocketqa: An opti-\nmized training approach to dense passage retrieval\nfor open-domain question answering. arXiv preprint\narXiv:2010.08191.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniÔ¨Åed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,\nQiaoqiao She, Hua Wu, Haifeng Wang, and Ji-Rong\nWen. 2021. Rocketqav2: A joint training method\n547\nfor dense passage retrieval and passage re-ranking.\narXiv preprint arXiv:2110.07367.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2021. Col-\nbertv2: Effective and efÔ¨Åcient retrieval via\nlightweight late interaction. arXiv preprint\narXiv:2112.01488.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nNandan Thakur, Nils Reimers, Andreas R ¬®uckl¬¥e, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. Beir:\nA heterogenous benchmark for zero-shot evaluation\nof information retrieval models. arXiv preprint\narXiv:2104.08663.\nKexin Wang, Nils Reimers, and Iryna Gurevych. 2021.\nTsdae: Using transformer-based sequential denois-\ning auto-encoder for unsupervised sentence embed-\nding learning. arXiv preprint arXiv:2104.06979.\nShitao Xiao, Zheng Liu, Weihao Han, Jianjin Zhang,\nDefu Lian, Yeyun Gong, Qi Chen, Fan Yang, Hao\nSun, Yingxia Shao, et al. 2022a. Distill-vq: Learn-\ning retrieval oriented vector quantization by dis-\ntilling knowledge from dense embeddings. arXiv\npreprint arXiv:2204.00185.\nShitao Xiao, Zheng Liu, Yingxia Shao, Tao Di, Bhuvan\nMiddha, Fangzhao Wu, and Xing Xie. 2022b. Train-\ning large-scale news recommenders with pretrained\nlanguage models in the loop. In Proceedings of the\n28th ACM SIGKDD Conference on Knowledge Dis-\ncovery and Data Mining, pages 4215‚Äì4225.\nShitao Xiao, Zheng Liu, Yingxia Shao, Defu Lian,\nand Xing Xie. 2021. Matching-oriented product\nquantization for ad-hoc retrieval. arXiv preprint\narXiv:2104.07858.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\nOverwijk. 2020. Approximate nearest neighbor neg-\native contrastive learning for dense text retrieval.\narXiv preprint arXiv:2007.00808.\nCanwen Xu, Daya Guo, Nan Duan, and Julian\nMcAuley. 2022. Laprador: Unsupervised pretrained\ndense retriever for zero-shot text retrieval. arXiv\npreprint arXiv:2203.06169.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. Advances in neural infor-\nmation processing systems, 32.\nHang Zhang, Yeyun Gong, Yelong Shen, Jiancheng\nLv, Nan Duan, and Weizhu Chen. 2021. Adversar-\nial retriever-ranker for dense text retrieval. arXiv\npreprint arXiv:2110.03611.\nJianjin Zhang, Zheng Liu, Weihao Han, Shitao Xiao,\nRuicheng Zheng, Yingxia Shao, Hao Sun, Hanqing\nZhu, Premkumar Srinivasan, Weiwei Deng, et al.\n2022. Uni-retriever: Towards learning the uniÔ¨Åed\nembedding based retriever in bing sponsored search.\nIn Proceedings of the 28th ACM SIGKDD Confer-\nence on Knowledge Discovery and Data Mining,\npages 4493‚Äì4501.\n548",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8396366834640503
    },
    {
      "name": "Encoder",
      "score": 0.81647127866745
    },
    {
      "name": "Transformer",
      "score": 0.7177601456642151
    },
    {
      "name": "Sentence",
      "score": 0.6886279582977295
    },
    {
      "name": "Language model",
      "score": 0.6529248952865601
    },
    {
      "name": "Embedding",
      "score": 0.6412251591682434
    },
    {
      "name": "Speech recognition",
      "score": 0.4812462031841278
    },
    {
      "name": "Natural language processing",
      "score": 0.45443952083587646
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44992318749427795
    },
    {
      "name": "Workflow",
      "score": 0.4123799502849579
    },
    {
      "name": "Database",
      "score": 0.11262300610542297
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139759216",
      "name": "Beijing University of Posts and Telecommunications",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2250955327",
      "name": "Huawei Technologies (China)",
      "country": "CN"
    }
  ]
}