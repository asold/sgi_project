{
    "title": "ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning",
    "url": "https://openalex.org/W4389519817",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2134104766",
            "name": "Viet Lai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287855901",
            "name": "Nghia Ngo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2767340308",
            "name": "Amir Pouran Ben Veyseh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4284265765",
            "name": "Hieu Man",
            "affiliations": [
                "University of Oregon"
            ]
        },
        {
            "id": "https://openalex.org/A15025084",
            "name": "Franck Dernoncourt",
            "affiliations": [
                "Adobe Systems (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2441312621",
            "name": "Trung Bui",
            "affiliations": [
                "Adobe Systems (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2134526459",
            "name": "Thien Nguyen",
            "affiliations": [
                "University of Oregon"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4312107301",
        "https://openalex.org/W3093517588",
        "https://openalex.org/W4317553041",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W4317463334",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W2004763266",
        "https://openalex.org/W4323570543",
        "https://openalex.org/W3205783417",
        "https://openalex.org/W4389520124",
        "https://openalex.org/W4323650985",
        "https://openalex.org/W2952087486",
        "https://openalex.org/W4362655849",
        "https://openalex.org/W4320009668",
        "https://openalex.org/W4287854446",
        "https://openalex.org/W3034469191",
        "https://openalex.org/W3099919888",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4385573112",
        "https://openalex.org/W4379259169",
        "https://openalex.org/W2963995027",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W4362511131",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W4321276803",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4321524280",
        "https://openalex.org/W4319793302",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W4385572845",
        "https://openalex.org/W4225513420",
        "https://openalex.org/W3156772763",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W4318069287",
        "https://openalex.org/W3174234060",
        "https://openalex.org/W2952230511",
        "https://openalex.org/W4389523957",
        "https://openalex.org/W4313451803",
        "https://openalex.org/W4226146865",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4320167623",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W4321524373",
        "https://openalex.org/W4327525855",
        "https://openalex.org/W4320854854",
        "https://openalex.org/W2915977242",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W4322759717",
        "https://openalex.org/W4387561119",
        "https://openalex.org/W4386875581",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4321472057",
        "https://openalex.org/W2760505947",
        "https://openalex.org/W3102483398",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W4312051216",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W4385521210",
        "https://openalex.org/W4313447114",
        "https://openalex.org/W4322760121",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W4319773014",
        "https://openalex.org/W4321854923",
        "https://openalex.org/W4386081764",
        "https://openalex.org/W4321392130",
        "https://openalex.org/W3173360659",
        "https://openalex.org/W2140679639",
        "https://openalex.org/W4323697401",
        "https://openalex.org/W4389524039"
    ],
    "abstract": "Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field. ChatGPT represents one of the most exciting LLM systems developed recently to showcase impressive skills for language generation and highly attract public attention. Among various exciting applications discovered for ChatGPT in English, the model can process and generate texts for multiple languages due to its multilingual training data. Given the broad adoption of ChatGPT for English in different problems and areas, a natural question is whether ChatGPT can also be applied effectively for other languages or it is necessary to develop more language-specific technologies. The answer to this question requires a thorough evaluation of ChatGPT over multiple tasks with diverse languages and large datasets (i.e., beyond reported anecdotes), which is still missing or limited in current research. Our work aims to fill this gap for the evaluation of ChatGPT and similar LLMs to provide more comprehensive information for multilingual NLP applications. In particular, we evaluate ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium, low, and extremely low resources. Compared to the performance of previous models, our extensive experiments demonstrate the worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 13171–13189\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nChatGPT Beyond English: Towards a Comprehensive Evaluation\nof Large Language Models in Multilingual Learning\nViet Dac Lai1∗, Nghia Trung Ngo1∗, Amir Pouran Ben Veyseh1∗,\nHieu Man1, Franck Dernoncourt2, Trung Bui2, Thien Huu Nguyen1\n1Dept. of Computer Science, University of Oregon, OR, USA\n2Adobe Research, USA\n{vietl@cs,nghian,apouranb@cs,hieum,thien@cs}@uoregon.edu\n{franck.dernoncourt,bui}@adobe.com\nAbstract\nOver the last few years, large language models\n(LLMs) have emerged as the most important\nbreakthroughs in natural language processing\n(NLP) that fundamentally transform research\nand developments in the field. ChatGPT repre-\nsents one of the most exciting LLM systems de-\nveloped recently to showcase impressive skills\nfor language generation and highly attract pub-\nlic attention. Among various exciting applica-\ntions discovered for ChatGPT in English, the\nmodel can process and generate texts for mul-\ntiple languages due to its multilingual training\ndata. Given the broad adoption of ChatGPT\nfor English in different problems and areas,\na natural question is whether ChatGPT can\nalso be applied effectively for other languages\nor it is necessary to develop more language-\nspecific technologies. The answer to this ques-\ntion requires a thorough evaluation of ChatGPT\nover multiple tasks with diverse languages and\nlarge datasets (i.e., beyond reported anecdotes),\nwhich is still missing or limited in current re-\nsearch. Our work aims to fill this gap for the\nevaluation of ChatGPT and similar LLMs to\nprovide more comprehensive information for\nmultilingual NLP applications. In particular,\nwe evaluate ChatGPT on 7 different tasks, cov-\nering 37 diverse languages with high, medium,\nlow, and extremely low resources. Compared\nto the performance of previous models, our ex-\ntensive experiments demonstrate the worse per-\nformance of ChatGPT for different NLP tasks\nand languages, calling for further research to\ndevelop better models and understanding for\nmultilingual learning.\n1 Introduction\nSince the introduction of word embeddings (Ben-\ngio et al., 2000) and deep learning architectures\n(Collobert et al., 2011), Natural Language Process-\ning (NLP) has witnessed significant breakthroughs\n∗ The first three authors contributed equally to this work.\nthat fundamentally transform research and appli-\ncations in various areas. Starting with the cre-\nation of word2vec (Mikolov et al., 2013), the major\nmilestones in NLP involve the presentation of the\nseq2seq or encoder-decoder framework (Cho et al.,\n2014; Sutskever et al., 2014), the proposal of the\nattention mechanism (Bahdanau et al., 2015), the\ndevelopment of Transformer architecture (Vaswani\net al., 2017), the notion of uncontextualized word\nembeddings from language models in ELMo (Pe-\nters et al., 2018), and the pre-trained transformer-\nbased language models, e.g., BERT (Devlin et al.,\n2019), GPT (Radford et al., 2018, 2019), T5 (Raffel\net al., 2020), and BART (Lewis et al., 2020).\nThe recent advances in NLP feature large lan-\nguage models (LLMs) that have parameter sizes\nover a hundred billion and are pre-trained on mas-\nsive data, e.g., GPT-3 (Rae et al., 2021), Megatron\n(Shoeybi et al., 2019), GPT-Jurassic (Lieber et al.,\n2021), OPT-175B (Zhang et al., 2022b), and mul-\ntilingual BLOOM (Scao et al., 2022). Although\nstill relying on the Transformer architecture, the un-\nprecedented scales of model size and training data\nhave allowed new emergent abilities to change the\nlandscape and practices in NLP (Wei et al., 2022).\nAn important emergent skill involves prompt-based\nlearning that facilities the probing of information\nfrom LLMs with prompts by sampling the learned\nlanguage distributions (Brown et al., 2020). In this\nway, the models demonstrate strong generalization\nin few-shot and zero-shot learning while avoiding\nparameter updates for the underlying architectures.\nTo this end, ChatGPT 1 is one of the latest de-\nvelopments in NLP. In the first two months of\nits launch, ChatGPT has attracted 100 million\nusers (Milmo, 2023). As the next iteration of\nInstructGPT (Ouyang et al., 2022), ChatGPT is\noptimized on top of a GPT-3.5 series model us-\ning reinforcement learning from human feedback\n(RLHF) (Christiano et al., 2017). In contrast to pre-\n1https://openai.com/blog/chatgpt/\n13171\nvious LLMs, ChatGPT and InstructGPT leverage\nhuman demonstrations of desired outputs for input\nprompts to train supervised models, while human\nrankings of generated outputs are obtained to train\na reward model to further optimize the LLMs with\nreinforcement learning. Compared to InstructGPT,\nChatGPT is trained with conversational data to al-\nlow follow-up questions. In this way, ChatGPT is\nable to interact with humans in multi-turn conversa-\ntions to generate more aligned outputs with human\ninterests, thus being more natural and accessible\nto users. In addition, due to the deployment of\npublic APIs to facilitate general users, there have\nbeen multiple reports on the successes of ChatGPT\nin solving challenging tasks in various areas, e.g.,\npassing the United States Medical Licensing Ex-\namination (Kung et al., 2022) and real exams in\na law school (Choi et al., 2023), performing com-\npetitively with commercial translation services for\nsome high-resource languages (Jiao et al., 2023),\nand even producing code from natural language\ninstructions. Nonetheless, the communities also\nexpress concerns about long-term implications of\nChatGPT and LLMs for society, citing issues on\nplagiarism, privacy, misinformation, and security\n(Bang et al., 2023).\nSimilar to other LLMs, ChatGPT is trained on a\nmix of training data from multiple languages. Al-\nthough English is the majority, the combination of\nmultilingual data contributes to ChatGPT’s abili-\nties to accept inputs and generate responses in dif-\nferent languages, making it accessible and widely\nadopted by people around the world. However,\ngiven the recency of the technology, ChatGPT has\nbeen mainly evaluated over English data. The com-\nmunity is lacking a comprehensive, public, and\nindependent evaluation of ChatGPT over various\nnon-English languages for diverse NLP tasks to\nprovide proper perspectives for future research and\napplications. Given ChatGPT’s transformative po-\ntentials, associated long-term risks, huge cost for\ntraining, and limited transparency, a fundamental\nquestion is whether multilingual LLMs such as\nChatGPT can also be reliably adopted for different\nlanguages or it is necessary to develop language-\nspecific LLMs/other technologies to solve NLP\nproblems for non-English languages.\nTo address the multilingual concerns for Chat-\nGPT, a few recent studies have investigated\nChatGPT’s performance and responses for non-\nEnglish languages. However, the considered\ntasks/languages/settings and scale of evaluation\ndata in existing multilingual evaluations are still\nlimited, which is unable to show a comprehen-\nsive picture of the potentials/performance of the\ntechnology on a diversity of other languages. For\ninstance, (Bang et al., 2023) evaluates the multi-\nlingual performance of ChatGPT on three tasks of\nlanguage identification, sentiment analysis, and ma-\nchine translation; however, only a few languages\nare selected for each task and the number of eval-\nuation samples for each language does not exceed\n50. Beyond English, the analysis of ChatGPT’s re-\nsponses for input questions in (Guo et al., 2023) is\nonly done for Chinese, while the results of the med-\nical licensing examinations for ChatGPT are only\nshown for Japanese in (Kasai et al., 2023). In ad-\ndition, (Fang et al., 2023) and (Wang et al., 2023a)\nexplores ChatGPT in three languages English, Chi-\nnese, and German; however, the studies only focus\non grammatical error correction or cross-lingual\nsummarization.\nTo this end, our paper aims to perform a more\nthorough evaluation of ChatGPT for its perfor-\nmance on multiple languages over different NLP\ntasks. Our experiments consider 37 diverse lan-\nguages, characterizing high-, medium-, low-, and\nextremely low-resource languages, to better high-\nlight ChatGPT’s potentials and limitations. To our\nknowledge, this is one of the largest sets of lan-\nguages evaluated for ChatGPT in a public study to\ndate. In addition to Natural Language Inference\n(NLI), Question Answering, and Common Sense\nReasoning, our current work will examine the tasks\nof Part-of-Speech (POS) Tagging, Named Entity\nRecognition (NER), Relation Extraction, and Sum-\nmarization, which are not covered in previous mul-\ntilingual evaluations for ChatGPT. To improve the\nreproducibility of the evaluations and better reflect\nthe approach of general users, our current work will\nfocus on the zero-shot learning setting for ChatGPT\nwhere no human-provided examples are presented\nto the model. Importantly, due to the scale of avail-\nable languages/tasks/datasets/models and the grow-\ning nature of multilingual learning research in NLP,\nwe will use this work as an ongoing and public\neffort to evaluate ChatGPT and other LLMs for\nmultiple languages, emphasizing on understudied\nlanguages to measure robustness and democratize\nimpacts of the technologies. Despite some poten-\ntial updates with future experiments, our current\nexperiments suggest the following tendencies:\n13172\n• ChatGPT’s zero-shot learning performance is\ngenerally worse than the state-of-the-art per-\nformance of the supervised learning models\nfor a majority of the considered tasks across\ndifferent languages, including high-, medium-\n, low-, and extremely-low resource languages.\nThe performance gaps are usually very large,\ndemonstrating the unfit of ChatGPT as a gen-\neral solver for different NLP problems. It\nthus highlights the importance of task-specific\nmodels for the development of NLP applica-\ntions.\n• ChatGPT’s performance is generally better\nfor English than for other languages, espe-\ncially for higher-level tasks that require more\ncomplex reasoning abilities (e.g., named en-\ntity recognition, question answering, common\nsense reasoning, and summarization). The\nperformance differences can be substantial\nfor some tasks and lower-resource languages,\nwhich justifies the biases of ChatGPT for En-\nglish and suggests the necessity to develop\nlanguage-specific models/LLMs for different\nlanguages and groups.\n• ChatGPT can perform better with English\nprompts even though the task and input texts\nare intended for other languages, further con-\nfirming the biases toward English of ChatGPT.\n2 Related Work\nSince the release of ChatGPT in November 2022\nwith impressive language abilities, there has been a\ngrowing interest in evaluating ChatGPT for differ-\nent aspects of natural language understanding. The\nfirst line of work concerns the performance com-\nparison of ChatGPT and state-of-the-art systems\nfor important tasks in NLP such as text summa-\nrization (Wang et al., 2023a; Yang et al., 2023),\nmachine translation (Hendy et al., 2023; Jiao et al.,\n2023; Kocmi and Federmann, 2023), question an-\nswering (Tan et al., 2023; Omar et al., 2023; Lai\net al., 2023), information extraction (Wei et al.,\n2023; Gao et al., 2023), text classification (Kuz-\nman et al., 2023; Amin et al., 2023), grammatical\nerror detection (Fang et al., 2023), and stance detec-\ntion (Zhang et al., 2022a). Along this line, several\nrecent studies have attempted to examine the perfor-\nmance of ChatGPT more comprehensively on mul-\ntiple datasets (Bang et al., 2023; Qin et al., 2023;\nKoco’n et al., 2023; Zhong et al., 2023). The sec-\nond direction for ChatGPT evaluation focuses on\nthe robustness/reliability of the model against pos-\nsible variants of input texts. For example, (Wang\net al., 2023b) explores the robustness of ChatGPT\nunder the adversarial and out-of-domain learning\nsettings while (Jang and Lukasiewicz, 2023) exam-\nines the logical prediction consistency of ChatGPT\nfor inputs with semantic equivalence, logical nega-\ntion, or symmetricity. Finally, the third dimension\nfor ChatGPT evaluation discusses the potential im-\npacts and risks of the technology for the broader\nsociety, e.g., in education (Susnjak, 2022; Khalil\nand Er, 2023), law (Choi et al., 2023), medical\n(Kung et al., 2022), ethnics (Shen et al., 2023),\nhuman-computer collaboration (Lanzi and Loia-\ncono, 2023), and cognition (Mahowald et al., 2023).\nHowever, to our knowledge, none of existing work\nhas conducted large-scale evaluations of ChatGPT\nfor multiple and diverse languages/tasks as we do.\n3 Methodology\nThe goal of our research is to evaluate the per-\nformance of ChatGPT and LLMs for NLP tasks\nin different languages. Given the large numbers\nof NLP datasets/tasks/languages and the growing\ndevelopments of LLMs, our work will be an ongo-\ning effort to include additional experiments to be\nmore comprehensive along the way. In the current\nversion of the paper, we will evaluate ChatGPT\non seven diverse NLP tasks, i.e., Part-of-Speech\n(POS) Tagging, Named Entity Recognition (NER),\nRelation Classification, Natural Language Infer-\nence (NLI), Question Answering (QA), Common\nSense Reasoning (CSR), and Summarization. Over\ndifferent tasks, our experiments will cover 34 di-\nverse languages, characterizing high-, medium-,\nlow-, and extremely low-resource languages to pro-\nvide broader perspectives. Following (Bang et al.,\n2023), we employ the ratio of the data for each lan-\nguage in the CommonCrawl corpus2, i.e., the main\ndata to pre-train GPT-3, to classify the resource lev-\nels. In particular, a language will be considered as\nhigh-, medium-, low-, and extremely low-resource\nif its data ratio is greater than 1% (> 1%), between\n0.1% and 1% (> 0.1%), between 0.01% and 0.1%\n(> 0.01%), and smaller than 0.01% (< 0.01%)\nrespectively. Table 1 presents information and cat-\negories for the languages considered in our work.\nAs the scale of ChatGPT precludes the ability\nto fine-tune the model on downstream task data\nfor most general users, we focus on the zero-shot\n2http://commoncrawl.org\n13173\nLanguage Code Pop. CC Size\n(M) (%) Cat.\nEnglish en 1,452 45.8786 H\nRussian ru 258 5.9692 H\nGerman de 134 5.8811 H\nChinese zh 1,118 4.8747 H\nJapanese jp 125 4.7884 H\nFrench fr 274 4.7254 H\nSpanish es 548 4.4690 H\nItalian it 68 2.5712 H\nDutch nl 30 2.0585 H\nPolish pl 45 1.6636 H\nPortuguese pt 257 1.1505 H\nVietnamese vi 85 1.0299 H\nTurkish tr 88 0.8439 M\nIndonesian id 199 0.7991 M\nSwedish sv 13 0.6969 M\nArabic ar 274 0.6658 M\nPersian fa 130 0.6582 M\nKorean ko 81 0.6498 M\nGreek el 13 0.5870 M\nThai th 60 0.4143 M\nUkrainian uk 33 0.3304 M\nBulgarian bg 8 0.2900 M\nHindi hi 602 0.1588 M\nBengali bn 272 0.0930 L\nTamil ta 86 0.0446 L\nUrdu ur 231 0.0274 L\nMalayalam ml 36 0.0222 L\nMarathi mr 99 0.0213 L\nTelugu te 95 0.0183 L\nGujarati gu 62 0.0126 L\nBurmese my 33 0.0126 L\nKannada kn 64 0.0122 L\nSwahili sw 71 0.0077 X\nPunjabi pa 113 0.0061 X\nKyrgyz ky 5 0.0049 X\nOdia or 39 0.0044 X\nAssamesese as 15 0.0025 X\nTable 1: List of languages, language codes, numbers\nof first and second speakers, data ratios in the Com-\nmonCrawl corpus, and language categories. The lan-\nguages are grouped into categories based on their data\nratios in the CommomCrawl corpus: High Resource\n(H, > 1%), Medium Resource (M, > 0.1%), and Low\nResource (L, > 0.01%), and Extremely-Low Resource\n(X, < 0.01%).\nlearning setting for ChatGPT. We also report the\nstate-of-the-art performance of the supervised mod-\nels for a task in each language as a reference for\nresearch progress. In zero-shot learning, an NLP\ntask T is specified by a natural-language task de-\nscription D. Given a new data sample with input\ntext X for the task T, the concatenation of D and\nX will then be sent into the ChatGPT model G\nas the input prompt to generate a natural-language\nresponse R = G([D; X]). Afterward, the response\nR will be parsed using pre-defined task-specific\nrules P to obtain an output Y = P(R(G([D; X]))\nin the required format for T (e.g., a pre-defined\nlabel for classification problems). Finally, the out-\nputs Y for examples in an evaluation dataset will be\nscored to return ChatGPT’s performance for task\nT.\nDifferent from some previous work that exploits\ntwo-stage prompting to adopt a zero-shot chain of\nthoughts (Kojima et al., 2022; Qin et al., 2023),\nwe directly utilize single-stage prompting that only\nadds the task description D into each input X to\nsimulate the common approach of general users\nfor ChatGPT. Other prompting strategies can be\nexplored in future work. As such, in the current\nversion, we aim to design simple task descriptions\nD while ensuring necessary information to indicate\nthe task and facilitate the parsing of responses to\nproduce accurate outputs Y . In addition, for tasks\nin a non-English target language, we will evaluate\ntask descriptions in both English and target-specific\nlanguages to shed light on the best approach to\nprompt ChatGPT in multilingual settings. To facili-\ntate the experiments, all non-English task descrip-\ntions are obtained using the automatic translation\ntool Google Translate 3 to translate the designed\nEnglish descriptions for each task. Finally, all of\nthe responses from ChatGPT in this work are ob-\ntained between March 1 and April 5. This is right\nafter ChatGPT is made available in OpenAI APIs to\nenable large-scale requests from the public for com-\nprehensive evaluations. To improve reproducibility,\nwe clear the conversations in ChatGPT for each\nquery to remove any previous context. In the fol-\nlowing, due to the space constraint, we will only\ndescribe the tasks, datasets, and ChatGPT’s perfor-\nmance. The designed prompts for each task will be\nprovided in the Appendix.\n4 Part-of-Speech Tagging\nPart-of-Speech (POS) Tagging is a coarse-grained\nword classification task whose goal is to label the\nsyntactic information of the words in a sentence.\n3https://translate.google.com\n13174\nWe evaluate ChatGPT for its multilingual POS tag-\nging abilities over the XGLUE-POS dataset (Liang\net al., 2020), which covers 18 languages and in-\ncludes labels derived from the Universal Dependen-\ncies (UD) Treebanks (v2.5) (Zeman et al., 2020).\nIn the experiments, we utilize the XGLUE-POS\ndataset from Huggingface Datasets4 that only in-\ncludes 17 languages (e.g., excluding Portuguese).\nAs such, we use the test sets of XGLUE-POS with\nmore than 15K samples for the selected languages\nin the evaluation. Appendix A provides details for\nour POS Tagging prompt for ChatGPT.\nLanguage Code Cat. XLM-R ChatGPT\n(en) (spc)\nEnglish en H 96.2 88.5 89.6\nRussian ru H 86.9 91.6 59.1\nGerman de H 92.2 90.2 89.9\nChinese zh H 60.4 76.5 75.3\nFrench fr H 89.9 93.2 93.5\nSpanish es H 89.0 92.2 91.9\nItalian it H 92.6 92.6 93.4\nDutch nl H 88.5 88.1 88.3\nPolish pl H 85.4 90.4 64.5\nVietnamese vi H 55.2 64.8 65.9\nTurkish tr M 72.7 78.6 69.6\nArabic ar M 67.3 81.0 80.9\nGreek el M 88.2 87.1 79.8\nThai th M 57.9 68.5 69.1\nBulgarian bg M 88.8 91.2 92.3\nHindi hi M 74.5 83.1 72.8\nUrdu ur L 62.1 78.4 80.7\nAverage 79.3 84.5 79.8\nTable 2: Accuracy of ChatGPT (zero-shot learning)\nand XLM-R (supervised learning) on the test sets of\nXGLUE-POS. ChatGPT is evaluated with both English\n(en) and language-specific (spc) task descriptions.\nResults: Table 2 presents the performance of\nChatGPT (zero-shot learning with both English\nand language-specific task descriptions) and the\nfully supervised XLM-R model (based on XLM-\nRoBERTa base) (Liang et al., 2020). Here, per-\nformance is measured via the accuracy of the pre-\ndicted POS tags. As can be seen, ChatGPT out-\nperforms XLM-R over 13 out 17 languages for\nmultilingual POS tagging. Different from XLM-\nR where English has the best POS tagging per-\nformance, ChatGPT seems to have better accu-\nracy than English with some other languages (e.g.,\nFrench, Spanish). Finally, we observe that English\nprompts tend to perform better or at lest competi-\n4https://huggingface.co/datasets/xglue\ntively with language-specific prompts for ChatGPT\nacross different languages for POS tagging.\n5 Named Entity Recognition\nNamed Entity Recognition (NER) is an important\ntask in NLP (Sang and Meulder, 2002), aiming to\nidentify spans and semantic types of names (e.g.,\nperson, organization) in text. NER is usually for-\nmulated as a sequence tagging problem where a\nlabel is assigned to each word in a sentence to indi-\ncate names. The BIO annotation schema is often\nleveraged to form the labels to capture both span\nand type information (Ratinov and Roth, 2009).\nFor multilingual NER evaluation of ChatGPT, we\nemploy the datasets from the recent shared task\nMultiCoNER (Malmasi et al., 2022) that seeks to\nbuild NER systems for 11 languages following the\nWNUT 2017 taxonomy for entity types (Derczyn-\nski et al., 2017). There are 6 entity types in Multi-\nCoNER, i.e., PER (person), LOC (location), CORP\n(corporation), CW (creative work), GRP (group of\npeople), and PROD (product). We utilize the test\nsets of the language in MultiCoNER for evalua-\ntion. Our prompt for the NER task is described in\nAppendix B.\nLanguage Code Cat. DAMO ChatGPT\n(en) (spc)\nEnglish en H 91.2 37.2 37.2\nRussian ru H 91.5 27.4 22.0\nGerman de H 90.7 37.1 32.8\nChinese zh H 81.7 18.8 19.8\nSpanish es H 89.9 34.7 33.2\nDutch nl H 90.5 35.7 37.5\nTurkish tr M 88.7 31.9 29.1\nPersian fa M 89.7 25.9 21.9\nKorean ko M 88.6 30.0 32.2\nHindi hi M 86.2 27.3 26.1\nBengali bn L 84.2 23.3 16.4\nAverage 88.4 29.9 28.0\nTable 3: Performance (F1 scores) of ChatGPT (zero-\nshot learning) and DAMO (supervised learning) on the\ntest sets of MultiCoNER. ChatGPT is evaluated with\nboth English (en) and language-specific (spc) task de-\nscriptions.\nResults: Table 3 evaluates the performance of Chat-\nGPT (zero-shot learning with both English and\nlanguage-specific task descriptions) and DAMO\n(Wang et al., 2022a), the model with current best-\nreported performance on MultiCoNER. The latter\nretrieves relevant context from Wikipeida for each\n13175\ninput sentence that are then fed into the XLMR-\nRoBERTa model (large version) for NER. DAMO\nalso employ a conditional random fields (CRF)\nlayer for the modeling. Our results for NER are\nevaluated using macro-averaged F1 scores (Mal-\nmasi et al., 2022). The most important observation\nfrom the table is that ChatGPT significantly un-\nderperforms DAMO on MultiCoNER across all 11\nlanguages. In fact, the performance of ChatGPT\nis less than 40% for all languages, which suggests\nless suitability of ChatGPT to solve NER in this\ndomain. Finally, we provide more analysis for the\nperformance of ChatGPT for NER in Appendix C\n6 Relation Extraction\nRelation Extraction (RE) is a crucial task in in-\nformation extraction (IE), aiming to identify and\nclassify semantic relations between two entity men-\ntions in an input text. To facilitate multilingual\nexperiments for RE, we conduct our evaluation\nover the SMiLER dataset (Seganti et al., 2021).\nSMiLER provides relation annotation for texts in\n14 languages with 36 relation types (including “no-\nrelation”). The test sets of the languages (with\nmore than 12K samples) are employed for evalu-\nation. We present our ChatGPT prompt for RE in\nAppendix D.\nLanguage Code Cat. mT5- ChatGPT\nIL (en) (spc)\nEnglish en H 96.0 61.9 61.8\nRussian ru H 83.3 78.8 77.5\nGerman de H 94.0 71.1 71.8\nFrench fr H 97.2 72.4 73.9\nSpanish es H 70.5 67.5 65.8\nItalian it H 97.0 74.4 74.6\nDutch nl H 93.5 66.8 66.6\nPolish pl H 93.0 63.4 65.8\nPortuguese pt H 85.2 64.8 66.3\nArabic ar M 94.1 84.9 90.1\nPersian fa M 73.1 58.9 63.8\nKorean ko M 83.2 65.3 70.1\nSwedish sv M 58.7 64.2 65.4\nUkrainian uk M 71.8 76.5 68.8\nAverage 85.0 69.4 70.2\nTable 4: Performance (F1 scores) of ChatGPT (zero-\nshot learning) and mT5-IL (supervised learning) on the\ntest sets of SMiLER. ChatGPT is evaluated with both\nEnglish (en) and language-specific (spc) task descrip-\ntions.\nResults: Table 4 shows the performance of Chat-\nGPT (zero-shot learning with both English and\nlanguage-specific task descriptions) and mT5-IL\n(Chen et al., 2022), a state-of-the-art supervised\nin-language prompting model for SMiLER. mT5-\nIL is based on the base version of mT5. Micro\nF1 scores are used as the performance metric for\nRE. From Table 4, the results suggest that mT5-\nIL significantly outperforms ChatGPT over differ-\nent languages no matter if we ask ChatGPT with\nEnglish or language-specific prompts (except for\nSwedish and Ukranian). The performance gap is up\nto 15% over F1 score on average for the languages.\nLanguage-specific prompts seem to yield better or\ncomparable performance as English prompts for\nChatGPT with RE. Ukrainian is an exception when\nEnglish prompts return better F1 score for Chat-\nGPT. Interestingly, ChatGPT performs the worst\nin English for RC with SMiLER, potentially due\nto the much larger size of English test data with\ngreater diversity and challenges (5,461 samples for\nEnglish vs. 1,243 samples for the second large test\nset for French).\n7 Natural Language Inference\nNatural Language Inference (NLI) aims to predict\nthe entailment/contradiction relations between two\ninput sentences, i.e., a premise and a hypothesis.\nTo evaluate ChatGPT for multilingual NLI, we uti-\nlize the XNLI dataset (Conneau et al., 2018) that\nprovides annotated data for English and 14 other\nlanguages with three categories, i.e., Entailment,\nContradiction, and Neutral. As such, the data in\nnon-English languages is obtained by translating\nEnglish data for XNLI. XNLI provides develop-\nment and test data to facilitate development and\nevaluation. However, as the labels for the test data\nare not publicly available, we utilize the develop-\nment data of XNLI in this experiment. The Chat-\nGPT prompt for NLI is described in Appendix E.\nResults: Table 5 reports the performance (accu-\nracy) of ChatGPT and the multilingual model mT5-\nXXL (Xue et al., 2021). Here, for each non-\nEnglish target language, we present ChatGPT’s\nperformance on two zero-shot learning settings de-\npending on whether the task descriptions are in En-\nglish or target language. For mT5-XXL, the model\nis fine-tuned on English training data and trans-\nlations in the target language to achieve the best\nreported performance on XNLI. It is clear from the\ntable that ChatGPT performs significantly poorer\nthan mT5-XXL across different languages by large\nmargins. The performance gaps between ChatGPT\n13176\nLanguage Code Cat. mT5- ChatGPT\nXXL (en) (spc)\nEnglish en H 92.4 70.2 70.2\nRussian ru H 86.4 60.8 45.4\nGerman de H 89.2 64.5 51.1\nChinese zh H 86.2 58.2 35.5\nFrench fr H 88.7 64.8 42.2\nSpanish es H 89.4 65.8 47.4\nVietnamese vi H 86.6 55.4 44.8\nTurkish tr M 86.4 57.1 37.1\nArabic ar M 87.1 55.3 22.3\nGreek el M 88.7 55.9 54.5\nThai th M 84.5 44.7 11.5\nBulgarian bg M 88.7 59.7 44.6\nHindi hi M 85.3 48.8 5.6\nUrdu ur L 82.9 43.7 6.3\nSwahili sw X 83.4 50.3 40.8\nAverage 87.1 57.0 37.3\nTable 5: Accuracy of ChatGPT (zero-shot learning) and\nmT5-XXL (supervised learning with English and trans-\nlated data) on the development set of XNLI. ChatGPT is\nevaluated with both English (en) and language-specific\n(spc) task descriptions.\nand mT5-XXL also seem smaller for high-resource\nlanguages. Finally, ChatGPT with target-language\ntask descriptions produces significantly lower ac-\ncuracy than those with English task descriptions\nacross all considered languages, suggesting the ben-\nefits of English descriptions for multilingual NLI\nwith ChatGPT.\n8 Question Answering\nGiven a context passage and a question, a Question\nAnswering (QA) model needs to return the answer\nfor the question, which should be a span of text\nin the input passage. To this end, we utilize the\nXQuAD dataset (Artetxe et al., 2020) to evaluate\nChatGPT in multiple languages for QA. XQuAD\ninvolves 240 paragraphs and 1190 question-answer\npairs in English and their translations into ten other\nlanguages for evaluation. We describe our Chat-\nGPT prompt for QA in Appendix F.\nGiven the responses from ChatGPT for our QA\nprompts for the examples, we remove the period\ncharacters in the end and directly evaluate remain-\ning responses using the SQuAD’s scorer5, which is\nsuggested by the original paper of XQuAD (Artetxe\net al., 2020).\n5https://raw.githubusercontent.com/allenai/\nbi-att-flow/master/squad/evaluate-v1.1.py\nLanguage Code Cat.mT5-XXL ChatGPT(en) ChatGPT(spc)EM F1 EM F1 EM F1\nEnglish en H 80.3 91.3 56.0 74.9 56.0 74.9\nRussian ru H 70.4 85.2 30.2 49.1 22.4 52.6German de H 68.2 85.0 45.9 65.8 44.7 65.8Chinese zh H 80.0 85.7 37.1 42.3 20.5 20.8Spanish es H 70.8 87.4 41.8 65.8 40.5 69.1Vietnamese vi H 67.1 85.3 36.1 57.3 26.8 60.8\nTurkish tr M 67.7 84.4 34.5 56.4 18.3 52.8Arabic ar M 68.2 83.4 32.0 50.3 24.1 49.9Greek el M 68.9 85.9 29.7 45.0 17.7 39.1Thai th M 74.5 80.2 31.2 43.4 1.5 13.1Hindi hi M 68.2 83.7 17.5 37.8 0.6 22.9\nAverage 71.3 85.2 35.6 53.5 21.7 47.4\nTable 6: Performance of ChatGPT (zero-shot learn-\ning) and mT5-XXL (supervised learning with trans-\nlated data) on the XQuAD dataset. en and spc indi-\ncate whether ChatGPT uses English or target language\nprompts. The performance is computed using exact\nmatch (EM) and F1 scores.\nResults: Table 6 shows the performance of Chat-\nGPT (zero-shot learning) and mT5-XXL (Xue\net al., 2021), a state-of-the-art supervised learn-\ning model for XQuAD. As such, for each language,\nmT5-XXL is trained over the combination of En-\nglish training data and the translations to the target\nlanguage to achieve optimal performance. We re-\nport the performance using both the exact match\n(EM) and F1 scores. Table 6 illustrates that Chat-\nGPT’s zero-shot performance is significantly worse\nthan the supervised model mT5-XXL for all the\nlanguages. Across different models and prompts,\nthe QA performance for English is significantly bet-\nter than those for other languages, demonstrating\nthe clear bias for English of current multilingual\nlanguage models. Finally, we find that prompting\nChatGPT with English tends to produce better per-\nformance for multilingual QA than using target\nlanguages.\n9 Common Sense Reasoning\nCommon Sense Reasoning (CSR) evaluates the\nreasoning of the models via multiple-choice ques-\ntions. The inputs for the models involve a question\nand a few choices for the answer, and the mod-\nels need to select one of the choices. To evalu-\nate ChatGPT’s multilingual abilities for CSR, we\nleverage two datasets: (i) X-CSQA (Talmor et al.,\n2019; Lin et al., 2021), which involves English\ndata and its translations to 15 other languages,\nand (ii) Wikipedia Cloze QA from IndicNLPSuite\n(Kakwani et al., 2020), which covers 11 low- and\nextremely-low-resource Indian languages. We eval-\nuate the models on the dev set of X-CSQA with\n1,000 samples for each language, while the Wiki\n13177\nLanguage Code Cat. TRT ChatGPT\n(en) (tgt)\nEnglish en H 70.0 75.0 75.0\nRussian ru H 59.8 50.2 53.5\nGerman de H 61.7 52.6 61.0\nChinese zh H 59.6 50.2 42.5\nJapanese jp H 54.3 41.9 43.0\nFrench fr H 60.9 50.5 61.7\nSpanish es H 61.1 53.3 62.5\nItaly it H 61.2 50.6 55.9\nDutch nl H 59.8 52.9 60.4\nPolish pl H 59.7 35.2 51.1\nPortugese pt H 60.5 49.5 59.2\nVietnamese vi H 59.3 42.3 47.9\nArabic ar M 58.1 49.4 47.3\nHindi hi M 53.8 41.1 38.6\nUrdu ur L 52.8 34.7 24.5\nSwahili sw X 51.8 35.6 46.6\nAverage 59.0 47.8 51.9\nTable 7: Accuracy of ChatGPT (zero-shot learning) and\nTRT (supervised learning) on the dev set of X-CSQA\ndataset. en and spc indicate whether ChatGPT uses\nEnglish or language-specific prompts.\nCloze QA dataset from IndicNLPSuite contains\n62,314 samples for all languages. Appendix G\npresents our ChatGPT prompt for CSR.\nResults: Table 7 reports the accuracy of Chat-\nGPT (zero-shot learning for both English and\nlanguage-specific prompts) and the state-of-the-\nart supervised model TRT (Fang et al., 2022)\non the X-CSQA dataset. TRT is based on the\nXLM-RoBERTa large model (Conneau et al.,\n2020) where commonsense knowledge in differ-\nent sources is retrieved to enrich input questions\nand answers. Except for English, the table illus-\ntrates the poorer performance of ChatGPT than\nTRT across all other languages for CSR on X-\nCSQA when the English task description is used.\nInterestingly, in contrast to other tasks, we find that\nlanguage-specific prompts tend to perform better\nthan English prompts for ChatGPT in CSR for high-\nresource languages (except for Chinese), leading to\nsome improvement over supervised learning (e.g.\nfor French, Spanish, and Dutch).\nFor IndicNLPSuite, Table 8 demonstrates the ac-\ncuracy of ChatGPT and IndicBERT (Kakwani et al.,\n2020), a pre-trained encoder-only model using the\nALBERT architecture over an Indian language cor-\npora. IndicBERT is fine-tuned on training data to\ndeliver state-of-the-art performance for IndicNLP-\nSuite in the original paper (Kakwani et al., 2020).\nOur experiment results for IndicNLPSuite confirm\nthe general tendency that supervised learning mod-\nels still perform better than ChatGPT over different\nlanguages. However, there are two exceptions with\nHindi and Kannada where ChatGPT can produce\nbetter accuracy over IndicNLPSuite. Finally, Table\n8 suggests that English prompts are a better way to\nprompt ChatGPT for Indian languages than these\nlanguages themselves (except for Marathi and Gu-\njarati).\nLanguage Code Cat. Indic- ChatGPT\nBERT (en) (tgt)\nHindi hi M 41.6 45.7 45.2\nBengali bn L 39.4 35.2 22.0\nTamil ta L 31.8 27.9 22.3\nMalayalam ml L 35.4 32.2 14.3\nMarathi mr L 44.9 36.2 36.9\nTelugu te L 32.6 32.5 22.2\nGujarati gu L 70.8 15.2 25.8\nKannada kn L 39.6 42.0 12.9\nPunjabi pa X 44.7 38.1 27.9\nOdia or X 39.3 34.7 32.9\nAssamese as X 40.5 35.2 24.8\nAverage 41.1 34.1 26.1\nTable 8: Accuracy of ChatGPT (zero-shot learning)\nand IndicBERT (supervised learning) on the Wikipedia\nCloze QA dataset (IndicNLPSuite). en and spc indicate\nwhether ChatGPT uses English or language-specific\nprompts.\nFinally, our ChatGPT evaluation for multilingual\nsummarization is included in Appendix H.\n10 Discussion\nThe most important findings from our experiment\nresults is that ChatGPT exhibits significantly worse\nperformance than state-of-the-art supervised mod-\nels for most of considered NLP tasks in different\nlanguages. Given the huge costs to train ChatGPT\nand similar LLMs as well as the necessity of paid\nAPIs to run large amounts of requests with Ope-\nnAI, it seems more reasonable to build smaller\ntask-specific models for NLP problems (or at least\nfor the considered tasks) in different languages that\ncan be hosted locally to serve at lower costs.\nIn addition, we notice an exception for the POS\ntagging task where ChatGPT can achieve com-\npetitive or even better performance than the su-\npervised learning models (especially with English\nprompts) over different languages. For instance,\n13178\nChatGPT has significantly better POS tagging ac-\ncuracy for Thai, Vietnamese, Bulgarian, Hindi, and\nUrdu, which are medium- and low-resource lan-\nguages. As such, in contrast to other considered\ntasks which require some level of semantic reason-\ning, POS tagging focuses on low-level syntactic\nanalysis. We thus hypothesize that ChatGPT pos-\nsesses high-level skills in grammar and low-level\nabilities of semantic reasoning to generate seem-\ningly fluent texts for multiple languages. However,\nfor more complicated semantic analysis, ChatGPT\nmight find it more challenging to perform accurate\npredictions and generations.\nRegarding the classification of high-, medium-,\nlow-, and extremely low-resource languages, our\nwork currently relies on data ratios for the lan-\nguages in the CommonCrawl corpus. According\nto our experiments, it is interesting that the perfor-\nmance of ChatGPT for low- and extremely-low-\nresource languages in some tasks is better or com-\nparable to those for high- or medium-resource lan-\nguages. For instance, for POS tagging in Table 2,\nChatGPT’s performance for Urdu (a low-resource\nlanguage) is better than the performance for Viet-\nnamese and Thai (high- and medium-resource lan-\nguages). In NER, ChatGPT achieves better per-\nformance for the low-resource language Bengali\nthan for Chinese (using English prompts in Table\n3). For the common sense reasoning task in Table\n7, ChatGPT’s performance for the extremely-low-\nresource language Swahili is comparable to those\nfor Polish (with English prompts). To this end, it\nseems evident that data size might not be the only\nfactor that dictates the resource level and perfor-\nmance for a task of a language with ChatGPT and\nLLMs.\nCompared to language-specific prompts, the su-\nperior performance of ChatGPT with English task\ndescriptions over a majority of problems and lan-\nguages suggests that ChatGPT might better under-\nstand/analyze the tasks with English prompts to\nlead to improved abilities to generate responses\nwith accurate outputs. In addition, the inclusion\nof English task descriptions for non-English inputs\ncan be seen as an approach to shift the representa-\ntions of language-specific inputs toward the English\nspace that can be better processed by ChatGPT due\nto the domination of English in its training data.\nHowever, we also note some recent work that re-\nveals a rather different findings, suggesting that\nChatGPT can perform competitively or even better\nwith language-specific prompts for NLP tasks in\ntarget languages (Hasan et al., 2023; Deng et al.,\n2023). A reason for those different findings might\ncome from potentially different versions of Chat-\nGPT at different times that are used to conduct the\nstudies. It thus highlights the importance of better\ntransparency for LLMs, e.g., with respect to train-\ning data (Nguyen et al., 2023), to allow accurate\nand deeper investigation of the models. Finally,\nthe better performance with English prompts also\nraises an interesting question on whether English\nis the optimal language to prompt ChatGPT or it is\nbetter to employ other languages for this purpose\nfor different target languages.\n11 Conclusion\nToward a more comprehensive understanding of\nChatGPT and LLMs on their multilingual learning\nabilities for NLP, our work conducts an evalua-\ntion for ChatGPT on 7 different tasks, i.e., Part-of-\nSpeech Tagging, Named Entity Recognition, Rela-\ntion Extraction, Natural Language Inference, Ques-\ntion Answering, Common Sense Reasoning, and\nSummarization. Using 37 diverse languages with\nhigh-, medium-, low-, and extremely low resources\nfor the experiments, our results reveal the less opti-\nmal performance of ChatGPT in the zero-shot learn-\ning setting for NLP tasks in different languages,\nadvocating for task-specific models to secure best\nperformance. As an ongoing research, we plan to\nextend the experiments to include more languages,\ntasks, models, criteria, and settings in future work\nto obtain broader and deeper insights.\nAcknowledgement\nThis research has been supported by the Army Re-\nsearch Office (ARO) grant W911NF-21-1-0112,\nthe NSF grant CNS-1747798 to the IUCRC Center\nfor Big Learning, and the NSF grant # 2239570.\nThis research is also supported in part by the Office\nof the Director of National Intelligence (ODNI),\nIntelligence Advanced Research Projects Activity\n(IARPA), via the HIATUS Program contract 2022-\n22072200003. The views and conclusions con-\ntained herein are those of the authors and should\nnot be interpreted as necessarily representing the\nofficial policies, either expressed or implied, of\nODNI, IARPA, or the U.S. Government. The U.S.\nGovernment is authorized to reproduce and dis-\ntribute reprints for governmental purposes notwith-\nstanding any copyright annotation therein.\n13179\nLimitations\nAs an ongoing work to evaluate ChatGPT and\nLLMs on multilingual learning tasks, our current\nwork observes several limitations that can be ad-\ndressed in future studies. First, although our exper-\niments have covered 37 languages, including low-\nand extremely low-languages, there are still many\nother languages that are not explored in the current\nwork. Some tasks/datasets in our work have not\ncovered lower-resource languages. The future work\ncan expand the language set with greater focuses\non lower-resource languages to better understand\nLLMs’ performance in this important direction.\nSecond, many other tasks, including those with\navailable multilingual datasets, have not been con-\nsidered in the current work. Examining more tasks\nand datasets will enable a more comprehensive un-\nderstanding of ChatGPT and LLMs in multilingual\nsettings. Third, our current work only evaluates\nChatGPT in the zero-shot learning setting, thus\nunable to show comparisons with other recent mul-\ntilingual LLMs, e.g., BLOOM (Scao et al., 2022),\nGPT-4, and BARD, in various learning scenarios.\nWhile some of these models are currently less ac-\ncessible for large-scale evaluations, our plan is to\nfurther include more models and learning settings\nalong the way to strengthen our evaluations and\ncomparisons when possible. Finally, the current\nwork only evaluates ChatGPT in terms of perfor-\nmance over NLP tasks in different languages. To\nbetter characterize ChatGPT and LLMs, other eval-\nuation metrics should also be investigated to report\nmore complete perspectives for multilingual learn-\ning, including but not limited to adversarial robust-\nness, biases, toxic/harmful content, hallucination,\naccessibility, development costs, and interpretabil-\nity.\nReferences\nRoee Aharoni, Shashi Narayan, Joshua Maynez,\nJonathan Herzig, Elizabeth Clark, and Mirella Lapata.\n2022. mface: Multilingual summarization with fac-\ntual consistency evaluation. ArXiv, abs/2212.10622.\nMostafa Mirzaie Amin, E. Cambria, and Björn Schuller.\n2023. Will affective computing emerge from foun-\ndation models and general ai? a first evaluation on\nchatgpt. ArXiv, abs/2303.03186.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4623–4637, Online. Association\nfor Computational Linguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of the\nInternational Conference on Learning Representa-\ntions ICLR.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan Xu,\nand Pascale Fung. 2023. A multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. ArXiv, abs/2302.04023.\nYoshua Bengio, Réjean Ducharme, and Pascal Vincent.\n2000. A neural probabilistic language model. In\nProceedings of the Annual Conference on Neural\nInformation Processing Systems.\nTom Brown, Benjamin Mann, and et al. 2020.\nLanguage models are few-shot learners. ArXiv,\nabs/2005.14165.\nYuxuan Chen, David Harbecke, and Leonhard Hennig.\n2022. Multilingual relation classification via effi-\ncient and effective prompting. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1059–1075, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nKyunghyun Cho, Bart van Merriënboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder–decoder\nfor statistical machine translation. In Proceedings\nof the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 1724–\n1734, Doha, Qatar. Association for Computational\nLinguistics.\nJonathan Choi, Kristin Hickman, Amy Monahan, and\nDaniel Schwarcz. 2023. Chatgpt goes to law school.\nAvailable at SSRN.\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan\nMartic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. In\nProceedings of the Annual Conference on Neural\nInformation Processing Systems (NeurIPS).\nRonan Collobert, Jason Weston, Léon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.\nNatural language processing (almost) from scratch.\nJournal of Machine Learning Research.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\n13180\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina\nWilliams, Samuel Bowman, Holger Schwenk, and\nVeselin Stoyanov. 2018. XNLI: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2475–2485, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nYue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Li-\ndong Bing. 2023. Multilingual jailbreak challenges\nin large language models. ArXiv.\nLeon Derczynski, Eric Nichols, Marieke van Erp, and\nNut Limsopatham. 2017. Results of the WNUT2017\nshared task on novel and emerging entity recogni-\ntion. In Proceedings of the 3rd Workshop on Noisy\nUser-generated Text, pages 140–147, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nTao Fang, Shu Yang, Kaixin Lan, Derek F. Wong, Jin-\npeng Hu, Lidia S. Chao, and Yue Zhang. 2023. Is\nchatgpt a highly fluent grammatical error correc-\ntion system? a comprehensive evaluation. ArXiv,\nabs/2304.01746.\nYuwei Fang, Shuohang Wang, Yichong Xu, Ruochen\nXu, Siqi Sun, Chenguang Zhu, and Michael Zeng.\n2022. Leveraging knowledge in multilingual com-\nmonsense reasoning. In Findings of the Association\nfor Computational Linguistics: ACL 2022 , pages\n3237–3246, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nJunfeng Gao, Huan Zhao, Changlong Yu, and Ruifeng\nXu. 2023. Exploring the feasibility of chatgpt for\nevent extraction. ArXiv, abs/2303.03836.\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,\nJinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng\nWu. 2023. How close is chatgpt to human experts?\ncomparison corpus, evaluation, and detection. ArXiv,\nabs/2301.07597.\nMd. Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj\nAlam, Anika Anjum, Avijit Sarker, and Sheak\nRashed Haider Noori. 2023. Zero- and few-shot\nprompting with llms: A comparative study with fine-\ntuned models for bangla sentiment analysis. ArXiv,\nabs/2308.10783.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\nM. Sohel Rahman, and Rifat Shahriyar. 2021. XL-\nsum: Large-scale multilingual abstractive summariza-\ntion for 44 languages. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4693–4703, Online. Association for Computa-\ntional Linguistics.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Has-\nsan Awadalla. 2023. How good are gpt models at\nmachine translation? a comprehensive evaluation.\nArXiv, 2302.09210.\nMyeongjun Jang and Thomas Lukasiewicz. 2023. Con-\nsistency analysis of chatgpt. ArXiv, abs/2303.06273.\nWenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing\nWang, and Zhaopeng Tu. 2023. Is chatgpt a good\ntranslator? yes with gpt-4 as the engine. ArXiv,\n2301.08745.\nDivyanshu Kakwani, Anoop Kunchukuttan, Satish\nGolla, Gokul N.C., Avik Bhattacharyya, Mitesh M.\nKhapra, and Pratyush Kumar. 2020. IndicNLPSuite:\nMonolingual corpora, evaluation benchmarks and\npre-trained multilingual language models for Indian\nlanguages. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 4948–\n4961, Online. Association for Computational Lin-\nguistics.\nJungo Kasai, Yuhei Kasai, Keisuke Sakaguchi, Yutaro\nYamada, and Dragomir Radev. 2023. Evaluating\ngpt-4 and chatgpt on japanese medical licensing ex-\naminations. ArXiv, abs/2303.18027.\nM. Khalil and Erkan Er. 2023. Will chatgpt get you\ncaught? rethinking of plagiarism detection. ArXiv,\nabs/2302.04335.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality. ArXiv, 2302.14520.\nJan Koco’n, Igor Cichecki, Oliwier Kaszyca, Ma-\nteusz Kochanek, Dominika Szydlo, Joanna Baran,\nJulita Bielaniewicz, Marcin Gruza, Arkadiusz Janz,\nKamil Kanclerz, Anna Koco’n, Bartlomiej Koptyra,\nWiktoria Mieleszczenko-Kowszewicz, P. Milkowski,\nMarcin Oleksy, Maciej Piasecki, Lukasz Radli’nski,\nKonrad Wojtasik, Stanislaw Wo’zniak, and Przemys-\nlaw Kazienko. 2023. Chatgpt: Jack of all trades,\nmaster of none. ArXiv, abs/2302.10724.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large\nlanguage models are zero-shot reasoners. In Pro-\nceedings of the Conference on Neural Information\nProcessing Systems (NeurIPS).\nTiffany H Kung, Morgan Cheatham, Arielle Medenilla,\nCzarina Sillos, Lorie De Leon, Camille Elepaño,\nMaria Madriaga, Rimel Aggabao, Giezel Diaz-\nCandido, James Maningo, et al. 2022. Performance\nof chatgpt on usmle: Potential for ai-assisted medical\neducation using large language models. medRxiv.\n13181\nTaja Kuzman, Igor Mozetic, and Nikola Ljubesic. 2023.\nChatgpt: Beginning of an end of manual linguistic\ndata annotation? use case of automatic genre identifi-\ncation. ArXiv, abs/2303.03953.\nViet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo,\nThuat Nguyen, Franck Dernoncourt, Ryan A. Rossi,\nand Thien Huu Nguyen. 2023. Okapi: Instruction-\ntuned large language models in multiple languages\nwith reinforcement learning from human feedback.\nArXiv, abs/2307.16039.\nPier Luca Lanzi and Daniele Loiacono. 2023. Chatgpt\nand other large language models as evolutionary en-\ngines for online interactive collaborative game design.\nArXiv, abs/2303.02155.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei\nGuo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin\nJiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang,\nRahul Agrawal, Edward Cui, Sining Wei, Taroon\nBharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu,\nShuguang Liu, Fan Yang, Daniel Campos, Rangan\nMajumder, and Ming Zhou. 2020. XGLUE: A new\nbenchmark dataset for cross-lingual pre-training, un-\nderstanding and generation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6008–6018,\nOnline. Association for Computational Linguistics.\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham.\n2021. Jurassic-1: Technical details and evaluation.\nWhite Paper. AI21 Labs.\nBill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, and Xi-\nang Ren. 2021. Common sense beyond English:\nEvaluating and improving multilingual language\nmodels for commonsense reasoning. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1274–1287, Online.\nAssociation for Computational Linguistics.\nKyle Mahowald, Anna A. Ivanova, Idan Asher Blank,\nNancy G. Kanwisher, Joshua B. Tenenbaum, and\nEvelina Fedorenko. 2023. Dissociating language\nand thought in large language models: a cognitive\nperspective. ArXiv, abs/2301.06627.\nShervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta\nKar, and Oleg Rokhlenko. 2022. SemEval-2022 task\n11: Multilingual complex named entity recognition\n(MultiCoNER). In Proceedings of the 16th Interna-\ntional Workshop on Semantic Evaluation (SemEval-\n2022), pages 1412–1437, Seattle, United States. As-\nsociation for Computational Linguistics.\nTomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013. Distributed repre-\nsentations of words and phrases and their composi-\ntionality. In Proceedings of the Annual Conference\non Neural Information Processing Systems.\nDan Milmo. 2023. Chatgpt reaches 100 million users.\nThe Guardian.\nThuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu\nMan, Nghia Trung Ngo, Franck Dernoncourt, Ryan\nRossi, and Thien Huu Nguyen. 2023. Culturax:\nA cleaned, enormous, and multilingual dataset for\nlarge language models in 167 languages. ArXiv,\nabs/2309.09400.\nReham Omar, Omij Mangukiya, Panos Kalnis, and Es-\nsam Mansour. 2023. Chatgpt versus traditional ques-\ntion answering for knowledge graphs: Current status\nand future directions towards knowledge graph chat-\nbots. ArXiv, abs/2302.06466.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke E. Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Francis Christiano, Jan Leike, and\nRyan J. Lowe. 2022. Training language models to\nfollow instructions with human feedback. ArXiv,\nabs/2203.02155.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver? ArXiv, abs/2302.06476.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. OpenAI blog.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nJack Rae, Sebastian Borgeaud, and et al. 2021. Scaling\nlanguage models: Methods, analysis & insights from\ntraining gopher. ArXiv, abs/2112.11446.\n13182\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. In Journal of Machine Learning Research.\nLev Ratinov and Dan Roth. 2009. Design challenges\nand misconceptions in named entity recognition. In\nProceedings of the Thirteenth Conference on Compu-\ntational Natural Language Learning (CoNLL-2009),\npages 147–155, Boulder, Colorado. Association for\nComputational Linguistics.\nErik F. Tjong Kim Sang and Fien De Meulder. 2002.\nIntroduction to the conll-2002 shared task: Language-\nindependent named entity recognition. In Proceed-\nings of the Conference on Computational Natural\nLanguage Learning (CoNLL).\nTeven Scao, Angela Fan, and et al. 2022. Bloom: A\n176b-parameter open-access multilingual language\nmodel. ArXiv, abs/2211.05100.\nAlessandro Seganti, Klaudia Firl ˛ ag, Helena Skowron-\nska, Michał Satława, and Piotr Andruszkiewicz. 2021.\nMultilingual entity and relation extraction dataset and\nmodel. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Main Volume, pages 1946–1955,\nOnline. Association for Computational Linguistics.\nYiqiu Shen, Laura Heacock, Jonathan Elias, Keith D\nHentel, Beatriu Reig, George Shih, and Linda Moy.\n2023. Chatgpt and other large language models are\ndouble-edged swords. Radiology.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using model parallelism.\nArXiv, abs/1909.08053.\nTeo Susnjak. 2022. Chatgpt: The end of online exam\nintegrity? ArXiv, abs/2212.09292.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Proceedings of the Annual Conference on Neural\nInformation Processing Systems.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYiming Tan, Dehai Min, Y . Li, Wenbo Li, Na Hu, Yon-\ngrui Chen, and Guilin Qi. 2023. Evaluation of chat-\ngpt as a question answering system for answering\ncomplex questions. ArXiv, abs/2303.07992.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems.\nJiaan Wang, Yunlong Liang, Fandong Meng, Zhixu Li,\nJianfeng Qu, and Jie Zhou. 2023a. Cross-lingual\nsummarization via chatgpt. ArXiv, abs/2302.14229.\nJindong Wang, Xixu Hu, Wenxin Hou, Hao Chen,\nRunkai Zheng, Yidong Wang, Linyi Yang, Haojun\nHuang, Wei Ye, Xiubo Geng, Binxin Jiao, Yue Zhang,\nand Xing Xie. 2023b. On the robustness of chatgpt:\nAn adversarial and out-of-distribution perspective.\nArXiv, abs/2302.12095.\nXinyu Wang, Yongliang Shen, Jiong Cai, Tao Wang, Xi-\naobin Wang, Pengjun Xie, Fei Huang, Weiming Lu,\nYueting Zhuang, Kewei Tu, Wei Lu, and Yong Jiang.\n2022a. DAMO-NLP at SemEval-2022 task 11: A\nknowledge-based system for multilingual named en-\ntity recognition. In Proceedings of the 16th Interna-\ntional Workshop on Semantic Evaluation (SemEval-\n2022), pages 1457–1468, Seattle, United States. As-\nsociation for Computational Linguistics.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran,\nAnjana Arunkumar, David Stap, Eshaan Pathak,\nGiannis Karamanolakis, Haizhi Lai, Ishan Puro-\nhit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,\nKrima Doshi, Kuntal Kumar Pal, Maitreya Patel,\nMehrad Moradshahi, Mihir Parmar, Mirali Purohit,\nNeeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,\nRavsehaj Singh Puri, Rushang Karia, Savan Doshi,\nShailaja Keyur Sampat, Siddhartha Mishra, Sujan\nReddy A, Sumanta Patro, Tanay Dixit, and Xudong\nShen. 2022b. Super-NaturalInstructions: General-\nization via declarative instructions on 1600+ NLP\ntasks. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 5085–5109, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol\nVinyals, Percy Liang, Jeff Dean, and William Fedus.\n2022. Emergent abilities of large language models.\nTransactions on Machine Learning Research.\nXiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang,\nXin Zhang, Shen Huang, Pengjun Xie, Jinan Xu,\nYufeng Chen, Meishan Zhang, Yong Jiang, and Wen-\njuan Han. 2023. Zero-shot information extraction via\nchatting with chatgpt. ArXiv, abs/2302.10205.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\n13183\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nXianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and\nWei Cheng. 2023. Exploring the limits of chatgpt\nfor query or aspect-based text summarization. ArXiv,\nabs/2302.08081.\nDaniel Zeman, Joakim Nivre, Mitchell Abrams, Elia\nAckermann, Noëmi Aepli, Hamid Aghaei, and\nR Ziane. 2020. Universal dependencies 2.5. LIN-\nDAT/CLARIAHCZ digital library at the Institute of\nFormal and Applied Linguistics (UFAL), Faculty of\nMathematics and Physics, Charles University. url:\nhttp://hdl. handle. net/11234/1-3226.\nBowen Zhang, Daijun Ding, and Liwen Jing. 2022a.\nHow would stance detection techniques evolve after\nthe launch of chatgpt? ArXiv, abs/2212.14548.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022b. Opt: Open\npre-trained transformer language models. ArXiv,\nabs/2205.01068.\nQihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and\nDacheng Tao. 2023. Can chatgpt understand too?\na comparative study on chatgpt and fine-tuned bert.\nArXiv, abs/2302.10198.\n13184\nTask Description:Please provide the POS tags\nfor each word in the input sentence. The input\nwill be a list of words in the sentence. The\noutput format should be a list of tuples, where\neach tuple consists of a word from the input text\nand its corresponding POS tag label from the\ntag label set: [“ADJ”, “ADP”, “ADV”, “AUX”,\n“CCONJ”, “DET”, “INTJ”, “NOUN”, “NUM”,\n“PART”, “PRON”, “PROPN”, “PUNCT”,\n“SCONJ”, “SYM”, “VERB”, “X”].\nNote: Your response should include only a list\nof tuples, in the order that the words appear in\nthe input sentence, with each tuple containing\nthe corresponding POS tag label for a word.\nInput: [“What”, “if”, “Google”, “Morphed”,\n“Into”, “GoogleOS”, “?”]\n⇒ [(“What”, “PRON”), (“if”, “SCONJ”),\n(“Google”, “PROPN”), (“Morphed”,\n“VERB”), (“Into”, “ADP”), (“GoogleOS”,\n“PROPN”), (“?”, “PUNCT”)].\nFigure 1: Input prompt and output of ChatGPT for the\nXGLUE-POS dataset.\nA Part-of-Speech Tagging Prompt\nOur prompt for POS tagging for ChatGPT con-\nsists of a task description, a note for output format,\nand an input sentence, concatenated in that order,\ni.e., PromptPOS = [task description; output format\nnote; input sentence]. Notably, instead of directly\nusing the text of input sentence, we feed ChatGPT\nwith the list of words in the sentence to facilitate\nthe word-label alignment and parsing of ChatGPT\nresponses for POS tagging. Our task description\nand output format note then emphasize on the ex-\npected format for the ChatGPT’s responses to fol-\nlow the tuple structure with pairs of words and their\ncorresponding POS tags. In the experiments, this\napproach has led to better performance for Chat-\nGPT than the direct input sentence. We illustrate an\nexample for the English POS prompts for ChatGPT\nin Figure 1.\nB Named Entity Recognition Prompt\nOur prompt structure for ChatGPT with Named\nEntity Recogntion (NER) follows the prompts for\nPOS Tagging, i.e.,PromptNER = [task description;\noutput format note; input sentence], which involve\na task description to explain the task and list en-\ntity type/labels of interest. We also have a note to\nspecify the expected output format with tuples of\nwords and predicted tags for names. However, a\nkey difference for NER is that we explicitly ask\nChatGPT to produce tags for each work in the BIO\nformat. Although this approach seems to make the\ntask more challenging for ChatGPT, we find that\nit actually improves the performance for ChatGPT.\nOur hypothesis is that the BIO tag requirement en-\ncourages ChatGPT to solve NER as a sequence la-\nbeling problem, thus forcing it to comprehensively\nannotate names in input sentences. In contrast, the\nsimpler approach to prompt ChatGPT for names\nwithout BIO specification might suggest reading\ncomprehension formulation that does not tag all\nnames with exact spans for NER. The responses\nfrom ChatGPT are also harder (i.e., more ambigu-\nous and unpredictable) to parse for NER outputs\nwithout the BIO requirement. We provide an En-\nglish prompt example for NER for ChatGPT in\nFigure 2.\nTask Description: You are working as a\nnamed entity recognition expert and your\ntask is to label a given text with named entity\nlabels. Your task is to identify and label any\nnamed entities present in the text. The named\nentity labels that you will be using are PER\n(person), LOC (location), CORP (corporation),\nCW (creative work), GRP (group of people),\nand PROD (product). You may encounter\nmulti-word entities, so make sure to label each\nword of the entity with the appropriate prefix\n(“B” for the first word of the entity, “I” for\nany non-initial word of the entity). For words\nwhich are not part of any named entity, you\nshould return “O”.\nNote: Your output format should be a list of\ntuples, where each tuple consists of a word\nfrom the input text and its corresponding named\nentity label.\nInput: [“john”, “is”, “first”, “mentioned”, “in”,\n“a”, “charter”, “from”, “1247”, “.”]\n⇒[(“john”, “B-PER”), (“is”, “O”), (“first”,\n“O”), (“mentioned”, “O”), (“in”, “O”), (“a”,\n“O”), (“charter”, “B-CW”), (“from”, “O”),\n(“1247”, “B-PROD”), (“. ”, “O”)].\nFigure 2: Input prompt and output of ChatGPT for the\nMultiCoNER dataset.\n13185\nC Analysis of ChatGPT’s Performance\nfor Named Entity Recognition\nLabel Precision Recall F1 Spurious\n(%)\nCORP 31.0 33.8 32.1 39\nCW 12.0 17.2 14.1 57\nGRP 6.7 5.7 6.2 26\nLOC 33.2 37.7 34.9 44\nPER 51.2 66.1 57.5 32\nPROD 20.3 22.3 21.1 55\nTable 9: ChatGPT label-wise scores on MultiCoNer\nIn order to better understand the performance\nof ChatGPT for MultiCoNER, we use the scoring\nscript nervaluate6 to compute detailed scores for\neach entity types for ChatGPT. Table 9 shows label-\nwise precision, recall, and F1 scores of ChatGPT\n(with English prompts). We also include spurious\npercentages (over total numbers of predictions),\nwhich are the percentages of ChatGPT’s predic-\ntions that do not exist in the annotated data for each\ntype. As can be seen, ChatGPT’s extraction perfor-\nmance is very poor for GRP (group of people) and\nCW (creative work), which have F1 scores of less\nthan 15%. Also, the spurious percentages of Chat-\nGPT are generally high for all entity types, which\nsuggests ChatGPT’s verbosity and confusion for\nNER.\nD Relation Extraction Prompt\nAn input example for RE involves an input text\nand two entity mentions in the text for classifica-\ntion. To probe ChatGPT for RE for an example, we\ndesign the prompt via the concatenation of a task\ndescription, input text, and two entity mentions, i.e.,\nPromptRE = [task description; output format note;\ninput text; entity 1; entity 2]. In the task description\nfor RE, we explicitly include all the relation types\nto inform ChatGPT. We also introduce an output\nformat note to specify the expected format for the\nresponses from ChatGPT for RE, thus facilitating\nresponse parsing for relation labels. To illustrate\nthe RE prompts for ChatGPT, we present an ex-\nample with the English prompt and corresponding\nresponse in Figure 3.\n6https://github.com/MantisAI/nervaluate\nTask Description: Given a input text de-\nscribing the relationship between two entities,\nextracts the relationship between them. The\nrelation has to be of the type: “birth-place”,\n“eats”, “event-year”, “first-product”, “from-\ncountry”, “has-author”, “has-child”, “has-edu”,\n“has-genre”, “has-height”, “has-highest-\nmountain”, “has-length”, “has-lifespan”,\n“has-nationality”, “has-occupation”, “has-\nparent”, “has-population”, “has-sibling”,\n“has-spouse”, “has-tourist-attraction”, “has-\ntype”, “has-weight”, “headquarters”, “invented-\nby”, “invented-when”, “is-member-of”,\n“is-where”, “loc-leader”, “movie-has-director”,\n“no-relation”, “org-has-founder”, “org-has-\nmember”, “org-leader”, “post-code”, “starring”,\n“won-award”.\nNote: Your output must only be the relation\nof the two given entities and must follow the\nformat: “Relation: <One of the above listed\nrelations>”.\nInput: North West Coastal Highway is a gener-\nally north-south Western Australian highway\nwhich links the coastal city of Geraldton with\nthe town of Port Hedland.\nEntity 1:North West Coastal Highway\nEntity 2:highway\n⇒ Relation: has-type.\nFigure 3: Input and output of ChatGPT for the SMiLER\ndataset.\nE Natural Language Inference Prompt\nTo construct the prompt for ChatGPT for each ex-\nample in XNLI, we directly concatenate the task\ndescription, the premise, the hypothesis, and a mul-\ntiple choice question (of entailment, contradiction,\nand neural) in this order, i.e., PromptNLI = [task\ndescription; premise; hypothesis; question ]. An\nexample of English input prompts and responses\nfrom ChatGPT is shown in Figure 4.\nF Question Answering Prompt\nWe collect the English task description for QA from\nthe NaturalInstructions repository (Wang et al.,\n2022b) for ChatGPT. In addition, as ChatGPT tends\nto generate long responses, we introduce a note to\nremind the model that the answers for our dataset\nshould be short and directly extracted from the in-\nput passage. This approach has helped ChatGPT\n13186\nTask Description:Please identify whether the\npremise entails or contradicts the hypothesis\nin the following premise and hypothesis.\nThe answer should be exact “entailment”,\n“contradiction”, or “neutral”.\nPremise: And he said, Mama, I’m home.\nHypothesis: He called his mom as soon as the\nschool bus dropped him off.\nIs it entailment, contradiction, or neutral?\n⇒ Neutral. The premise doesn’t confirm or\ndeny the hypothesis....\nFigure 4: Input prompt and output of ChatGPT for the\nXNLI dataset.\nto provide more direct answers in our experiments.\nTo this end, for an example with an input passage\nand question, our prompt for ChatGPT is formed\nvia: PromptQA = [task description; passage; ques-\ntion; note]. We demonstrate an example of the QA\nprompts in Figure 5.\nG Common Sense Reasoning Prompt\nIn the CSR prompts for ChatGPT, we combine\nthe task description, the question, and the multiple\nchoices for each sample, i.e., PromptCSR = [task\ndescription; question; multiple choices]. Here, for\nthe task description, we also indicate the language\nof the input question and multiple choices. Two\nexamples of prompts for CSR inputs are presented\nin Figure 6 for the X-CSQA dataset and in Fig-\nure 7 for the Wikipedia Cloze QA dataset from\nIndicNLPSuite.\nH Summarization\nIn summarization, systems need to provide key\nand concise information for a longer input text,\nwhich can be helpful for different downstream ap-\nplications such as news analysis, marketing, ques-\ntion answering, and scientific document processing.\nTo study the performance of ChatGPT for sum-\nmarization in multiple languages, we choose the\nXL-Sum dataset (Hasan et al., 2021) that provides\nsummaries of news articles in 44 languages. In\ncontrast to extractive summarization that select im-\nportant sentences in the input text to a summary,\nXL-Sum addresses abstractive summarization to\nallow text generation with more creative writing in\nthe summary (the sentences in the summary might\nnot necessarily appear in the input text). Despite\nTask Description:Answer the question from\nthe given passage. Your answer should be\ndirectly extracted from the passage, and it\nshould be a single entity, name, or number, not\na sentence.\nPassage: Peyton Manning became the first\nquarterback ever to lead two different teams to\nmultiple Super Bowls. He is also the oldest\nquarterback ever to play in a Super Bowl at\nage 39. The past record was held by John\nElway, who led the Broncos to victory in\nSuper Bowl XXXIII at age 38 and is currently\nDenver’s Executive Vice President of Football\nOperations and General Manager.\nQuestion: How old was Peyton Manning when\nhe played in Super Bowl 50?\nNote: Your answer should be directly extracted\nfrom the passage and be a single entity, name,\nor number, not a sentence.\n⇒ 39.\nFigure 5: Input prompt and output of ChatGPT for\nXQUAD dataset.\nTask description: In this task, you will be\npresented with a question that has multiple\npossible answers in English. You should choose\nthe most suitable option out of “A”, “B”, “C”,\n“D”, and “E”, based on your commonsense\nknowledge.\nQuestion: When you return to work you will\nlikely need what to get in the door if you are\nthe first to arrive?\nOptions:\nA earn money\nB key\nC need money\nD badge\nE get out of bed\n⇒ Option B is the most suitable answer: key.\nFigure 6: Input prompt and output of ChatGPT for X-\nCSQA dataset.\ngreater challenges, abstractive summarization can\nproduce more natural texts to better serve down-\nstream applications.\nTo facilitate the experiments, we select 12 lan-\nguages in XL-Sum, covering high-, medium-, low-,\nand extremely low-resource languages, and eval-\n13187\nFigure 7: Input prompt and output of ChatGPT for\nWikipedia Cloze QA dataset (IndicNLPSuite). Transla-\ntion of the statement and options by Google Translate:\nRatan Devasi was born on 25 September 1975 at Mount\nAbu in the Sirohi district of <MASK>. His father’s\nname is Shankarlal Devasi and wife’s name is Viraj\nDevasi. Devasi has been a brilliant student since child-\nhood. He is a Diploma in Hotel Management degree\nholder. Devasi is quick-tempered and soft-spoken since\nhis student life. Option A: Congress; Option B: NSUI;\nOption C: Rajasthan; Option D: Lok Sabha.\nuate ChatGPT’s performance on the test datasets\nof the languages. Table 10 shows the sizes of test\ndata (i.e., the numbers of samples) in XL-Sum for\nthe selected languages. In the experiments, we\nutilize the ROUGE-1, ROUGE-2, and ROUGE-L\nscores as performance measures for summarization.\nNote that for the non-English languages, the scorer\nscript in the original paper of XL-Sum (Hasan et al.,\n2021) is used for performance computation.\nAs a summary in XL-Sum is expected to be writ-\nten in the same language as the input text, given an\ninput text, our summarization prompt for ChatGPT\nis constructed via the concatenation: PromptSUM\n= [task description; output language specification:\ninput text]. Accordingly, the task description is sim-\nply: “Summarize this <lang> text.” while the out-\nput langauge specification is expressed via: “The\noutput should be in <lang>”. Here, <lang> in-\ndicates the the same language that is presented\nin the input text and expected in the summary\nresponse. <lang> can be translated into appro-\npriate languages as required by the language of\nthe prompts. For instance, using English for the\nprompts, the summarization prompt for a French\ninput is “Summarize this French text. The output\nshould be in French: . . .”. In the experiments, we\nfind that ChatGPT might generate responses in En-\nglish even for non-English inputs and including\noutput language specifications in the prompts is im-\nportant to instruct the same language in the inputs\nand outputs for ChatGPT.\nResults: Tables 10 and 11 presents the summariza-\ntion performance of ChatGPT (zero-shot learning)\nfor the selected languages in XL-Sum using En-\nglish and language-specific prompts respectively.\nIn the tables, we also include the performance of\nthe mT5-XXL model that is trained over training\ndata of specific languages in XL-Sum. mT5-XXL\nhas achieved state-of-the-art performance for XL-\nSum as reported in (Aharoni et al., 2022). It is ob-\nvious from the tables that ChatGPT’s performance\nis consistently inferior to mT5-XXL’s with large\nperformance gaps in different languages. To better\nunderstand the poor performance of ChatGPT, Ta-\nbles 10 and 11 also report the average lengths of\nthe human-provided summaries and the summaries\ngenerated by ChatGPT (in terms of the numbers of\ncharacters). It is clear from the tables that ChatGPT\ntends to generate lengthy summaries, potentially\nleading to its poorer performance. In addition, the\ntables show the success rates of ChatGPT for each\nlanguage, which is defined as the ratios of requests\nsent to the ChatGPT server and received non-empty\nresponses/summaries. As can be seen, the success\nrates of ChatGPT for lower-resource languages are\nalso lower that can further explain ChatGPT’s per-\nformance and reliability for such languages.\n13188\nLanguage Code Cat. Size ChatGPT Avg. Gold Avg. Model Success mT5-XXL\n1 2 L Length Length (%) L\nEnglish en H 11,535 19.71 5.52 13.38 125.84 612.38 99 32.51\nRussian ru H 7,780 18.65 5.13 12.83 182.11 523.03 96 28.48\nChinese zh H 4,670 21.14 5.31 15.27 420.10 191.46 98 33.54\nFrench fr H 1,086 20.76 7.09 14.12 147.38 601.17 99 34.12\nSpanish es H 4,763 17.81 4.44 11.97 163.39 719.52 99 27.40\nTurkish tr M 3,397 14.52 4.54 10.87 164.83 610.75 99 30.80\nArabic ar M 4,689 19.37 5.36 13.64 142.95 396.86 95 32.00\nThai th M 826 17.55 5.35 11.51 218.13 275.89 59 30.59\nHindi hi M 8,847 21.06 5.63 14.21 137.24 294.93 82 36.88\nBengali bn L 1,012 6.34 1.63 4.65 148.19 176.08 42 34.19\nBurmese my L 570 14.49 6.32 8.85 201.99 118.78 43 41.40\nKyrgyz ky X 500 5.10 1.47 4.16 188.14 437.51 87 26.48\nTable 10: Performance of ChatGPT (zero-shot learning) (ROUGE-1/2/L) and mT5-XXL (supervised learning)\n(ROUGE-L) for summarization over XL-Sum using English prompts.\nLanguage Code Cat. Size ChatGPT Avg. Gold Avg. Model Success mT5-XXL\n1 2 L Length Length (%) L\nEnglish en H 11,535 21.38 5.97 14.48 125.84 524.15 99 32.51\nRussian ru H 7,780 15.60 4.17 10.83 182.11 483.45 96 28.48\nChinese zh H 4,670 11.65 2.61 8.96 55.31 420.10 98 33.54\nFrench fr H 1,086 21.11 7.21 14.49 147.38 512.17 100 34.12\nSpanish es H 4,763 19.73 4.85 13.15 163.39 601.05 99 27.40\nTurkish tr M 3,397 15.58 4.91 11.79 164.83 468.64 99 30.80\nArabic ar M 4,689 16.95 4.74 12.04 142.95 383.71 94 32.00\nThai th M 826 14.39 4.11 9.71 218.13 257.40 58 30.59\nHindi hi M 8,847 4.28 1.13 2.94 137.24 423.58 82 36.88\nBengali bn L 1,012 1.88 0.43 1.39 148.19 198.60 40 34.19\nBurmese my L 570 0.45 0.34 0.44 201.99 152.27 40 41.40\nKyrgyz ky X 500 8.40 2.23 6.42 188.14 458.71 86 26.48\nTable 11: Performance of ChatGPT (zero-shot learning) (ROUGE-1/2/L) and mT5-XXL (supervised learning)\n(ROUGE-L) for summarization over XL-Sum using language-specific prompts.\n13189"
}