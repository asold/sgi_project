{
  "title": "Alzheimer's Disease Detection from Spontaneous Speech through Combining Linguistic Complexity and (Dis)Fluency Features with Pretrained Language Models",
  "url": "https://openalex.org/W3167985707",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2052563777",
      "name": "Qiao Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2222807914",
      "name": "Yin Xue-feng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222425969",
      "name": "Wiechmann, Daniel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222425971",
      "name": "Kerz, Elma",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3096757102",
    "https://openalex.org/W3016249629",
    "https://openalex.org/W3037696347",
    "https://openalex.org/W2740306476",
    "https://openalex.org/W2945976633",
    "https://openalex.org/W2107620532",
    "https://openalex.org/W2097606805",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W3101650868",
    "https://openalex.org/W3094848124",
    "https://openalex.org/W2067278056",
    "https://openalex.org/W2809621904",
    "https://openalex.org/W2997535983",
    "https://openalex.org/W3143075381",
    "https://openalex.org/W3116032906",
    "https://openalex.org/W2981836045",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2949541494",
    "https://openalex.org/W3047016605",
    "https://openalex.org/W3099475240",
    "https://openalex.org/W3120090883",
    "https://openalex.org/W3131831556",
    "https://openalex.org/W2120615054",
    "https://openalex.org/W3119676436",
    "https://openalex.org/W2889320648",
    "https://openalex.org/W1981399499",
    "https://openalex.org/W3097533615",
    "https://openalex.org/W2965210982",
    "https://openalex.org/W1994871153",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2770649413",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2080274663"
  ],
  "abstract": "In this paper, we combined linguistic complexity and (dis)fluency features with pretrained language models for the task of Alzheimer's disease detection of the 2021 ADReSSo (Alzheimer's Dementia Recognition through Spontaneous Speech) challenge. An accuracy of 83.1% was achieved on the test set, which amounts to an improvement of 4.23% over the baseline model. Our best-performing model that integrated component models using a stacking ensemble technique performed equally well on cross-validation and test data, indicating that it is robust against overfitting.",
  "full_text": "Alzheimer’s Disease Detection from Spontaneous Speech through Combining\nLinguistic Complexity and (Dis)Fluency Features with Pretrained Language\nModels\nYu Qiao1, Xuefeng Yin1, Daniel Wiechmann2, Elma Kerz1\n1RWTH Aachen University\n2University of Amsterdam\nyu.qiao@rwth-aachen.de, xuefeng.yin@rwth-aachen.de, d.wiechmann@uva.nl,\nelma.kerz@ifaar.rwth-aachen.de\nAbstract\nIn this paper, we combined linguistic complexity and\n(dis)ﬂuency features with pretrained language models for the\ntask of Alzheimer’s disease detection of the 2021 ADReSSo\n(Alzheimer’s Dementia Recognition through Spontaneous\nSpeech) challenge. An accuracy of 83.1% was achieved on\nthe test set, which amounts to an improvement of 4.23% over\nthe baseline model. Our best-performing model that integrated\ncomponent models using a stacking ensemble technique per-\nformed equally well on cross-validation and test data, indicating\nthat it is robust against overﬁtting.\nIndex Terms: Alzheimer’s disease, disﬂuency, pretrained lan-\nguage models, automated Alzheimer’s disease detection, lin-\nguistic complexity\n1. Introduction\nAlzheimer’s disease (AD) is a gradual and progressive neu-\nrodegenerative disease caused by neuronal cell death [1]. The\nnumber of people diagnosed with AD is rapidly increasing 1.\nThe high prevalence of the disease and the high costs associated\nwith traditional approaches to detection make research on auto-\nmatic detection of AD critical [2]. A growing body of research\nhas demonstrated that quantiﬁable indicators of cognitive de-\ncline associated with AD are detectable in spontaneous speech\n(see [3] for a recent review). These indicators encompass acous-\ntic features, such as vocalisation features (i.e. speech-silence\npatterns) [4], paralinguistic features, such as ﬂuency features\n[5] and speech pause distributions [6], as well as syntactic and\nlexical features extracted from speech transcripts [7].\nThis area of research has beneﬁted from recent advances\nin natural language processing (NLP) and machine learn-\ning, as well as an increasing number of interdisciplinary\nresearch collaborations. A prime example of this is the\nADReSS(o) (Alzheimer’s Dementia Recognition through Spon-\ntaneous Speech) Challenge, aimed at generating systematic ev-\nidence for the use of such indicators in automated AD detec-\ntion systems and towards their clinical implementation. This\nchallenge has made signiﬁcant contributions to research on AD\ndetection by enabling the research community to test their exist-\ning methods, develop novel approaches and to benchmark their\nAD detection systems on a shared dataset. The ADReSSo Chal-\nlenge at INTERSPEECH 2021 [8] is geared towards automatic\nrecognition of AD from spontaneous speech and involved three\nsubtasks. Here in this paper, we focus on the AD classiﬁca-\ntion subtask, for which research teams were asked to build a\nmodel to predict the label (AD or non-AD) for a short speech\n1https://www.alz.org/alzheimers-dementia/facts-ﬁgures\nsession. Participating teams could use the speech signal di-\nrectly and extract acoustic features or automatically convert the\nspeech to text (ASR) and extract linguistic features from this\nASR-generated transcript.\n1.1. Related work\nIn this section, we provide a concise review of research on\nautomatic AD detection through speech, with particular atten-\ntion to previous studies conducted as part of the 2020 ADReSS\nChallenge. The AD classiﬁcation approaches in this challenge\nrelied on a wide range of acoustic, paralinguistic, and linguistic\nfeatures or their combination. Classiﬁcation accuracy scores of\nthe proposed models ranged between 68% and 89.6%. While\nsome approaches either focused on acoustic or linguistic fea-\ntures, the best performing contributions in the 2020 challenge\nembraced a multi-modal approach combining several types of\nfeatures (e.g. [9][10][11]). Furthermore, building on earlier\nwork reporting on the effectiveness of the use of word embed-\ndings in AD detection ([12][13]), several approaches success-\nfully employed pretrained language models (e.g. [9][10][11]).\nAnother important issue addressed in several studies concerned\nhow to deal with variance in the predictive performance of pre-\ntrained models resulting from ﬁne-tuning for downstream tasks\nwith a small data set. In response to this issue, the authors of\nthe best performing model [9] introduced an ensemble method\nto increase the robustness of their approach. In response to this\nissue, the best performing paper of the 2020 challenge [9] intro-\nduced an ensemble approach to increase the robustness of their\nmodels. Finally, it is important to note that some of the high-\nperforming models in last year’s challenge – including the best\nmodel described in [9] – used rich manual transcription that in-\ncluded pause and disﬂuency annotation. Such transcripts were\nnot provided in the 2021 challenge, making it more demanding\ncompared to last year’s challenge.\n1.2. Modeling approach\nThe modeling approach presented in this paper builds on\nkey insights reported in the studies reviewed above and ex-\ntends on these (1) by integrating linguistic indicators of lin-\nguistic complexity and sophistication, features of (dis)ﬂuency\nand transformer-based pretrained language models and (2) by\nutilizing ensembling methods to combine the information from\nthese feature groups and to reduce the variance in model predic-\ntions. Speciﬁcally, we perform experiments with classiﬁcation\nbased on three ensembling techniques: Ensembling by bagging\nvia majority vote, ensembling by bagging using feature fusion,\nand ensembling by stacking.\narXiv:2106.08689v1  [cs.CL]  16 Jun 2021\n2. Data and analysis\n2.1. Data\nThe Alzheimer’s Disease Detection dataset provided by the\norganizers of the ADReSSo Challenge 2021 consists of speech\nrecordings of picture descriptions from the Boston Diagnostic\nAphasia Exam produced by 87 individuals with an AD diag-\nnosis and 79 cognitively normal subjects (control group). The\nrecordings were acoustically enhanced (noise reduction through\nspectral subtraction) and normalised. The data were also bal-\nanced with respect to age and gender. Besides the audio ﬁles,\nthe organizers provided segmentations of the recordings into\nvocalisation sequences with speaker identiﬁers. No transcripts\nwere provided.\n2.2. Speech Recognition\nWe used AppTek’s Automatic Speech Recognition technol-\nogy via a cloud API service2 for automatically transcribing the\naudio ﬁles. The transcripts were converted from XML into raw\ntext formats with full stops being added at the end of each ut-\nterance based on the segmentations provided by the organizers.\nThese ﬁles served as the input for the automated text analysis\n(see Section 2.4).\n2.3. (Dis)ﬂuency\nTo model the speakers’ articulatory (in particular\n(dis)ﬂuency-related) characteristics, we derived several fea-\ntures from the ASR system that fall into four classes. (1) Silent\npauses - The ASR output contained the start- and end-times as\nwell as conﬁdence scored for each recognized word. Durations\nof pauses were calculated from forced alignment and binned by\nduration into short pauses (< 2sec) and long pauses (> 2sec).\nIn addition, we calculated the total pause duration per sentence\n(in seconds). (2) Speed of articulation - We enriched the\noutput of the ASR with syllable counts from the Carnegie\nMellon University Pronouncing Dictionary 3. Based on this\ninformation we assessed the mean syllable duration as well as\nsyllables per minute for each utterance in the speech data. (3)\nFilled pauses - Next to the number and total duration of silent\npauses, we derived frequency counts per sentence for two ﬁlled\npause type, uh and um, that had been shown to discriminate\nbetween AD patients and controls in previous studies [9].\n(4) Pronunciation - As the known symptoms of AD patients\ninclude mispronunciation [14], we calculated average word\nlevel conﬁdence scores as a proxy of pronunciation quality,\nwhich have been employed for the speech pattern detection\nin the context of detection of Alzheimer’s Disease [15]. All\nmeasures were calculated at utterance level. An overview of\nthese measures with descriptive statistics for AD patients and\ncontrol subjects is presented in Table 1.\n2.4. Automated Text Analysis (ATA)\nThe speech transcripts were automatically analyzed using\nCoCoGen (short for: Complexity Contour Generator), a com-\nputational tool that implements a sliding window technique to\ncalculate within-text distributions of scores for a given language\nfeature (for current applications of the tool in the context of text\nclassiﬁcation, see [16, 17, 18]). In this paper, we employed a\ntotal of 293 features derived from interdisciplinary, integrated\napproaches to language [19] that fall into four categories: (1)\nmeasures of syntactic complexity, (2) measures of lexical rich-\nness, (3) register-based n-gram frequency measures, and (4)\ninformation-theoretic measures. In contrast to the standard ap-\n2https://www.apptek.com/\n3http://www.speech.cs.cmu.edu/cgi-bin/cmudict\nTable 1: Descriptive statistics of (dis)ﬂuency measures\nAD patients Control\n(Dis)Fluency measure M SD M SD\nSpeed of articulation\nMean syllable duration 0.28 0.05 0.26 0.03\nSyllables per minute 205 45.7 224 35.7\nSilent pauses\nPause time per sentence (in sec) 0.92 0.89 0.63 0.49\nN long pauses (> 2sec) 1.28 2.22 0.473 0.71\nN short pauses (< 2sec) 13.2 9.28 15.4 11.7\nFilled pauses\nN uh 0.29 0.88 0.24 0.54\nN um 0.07 0.37 0.31 0.74\nPronunciation\nMean ASR conﬁdence 0.83 0.09 0.86 0.08\nproach implemented in other software for automated text analy-\nsis that relies on aggregate scores representing the average value\nof a feature in a text, the sliding-window approach employed in\nCoCoGen tracks the distribution of the feature scores within a\ntext. A sliding window can be conceived of as a window of\nsize ws, which is deﬁned by the number of sentences it con-\ntains. The window is moved across a text sentence-by-sentence,\ncomputing one value per window for a given indicator. In the\npresent study, the ws was set to 1. The series of measurements\ngenerated by CoCoGen captures the progression of language\nperformance within a text for a given indicator and is referred\nhere to as a ‘complexity contour’ (see Figure 1 for illustration).\nCoCoGen uses the Stanford CoreNLP suite [20] for performing\ntokenization, sentence splitting, part-of-speech tagging, lemma-\ntization and syntactic parsing (Probabilistic Context Free Gram-\nmar Parser [21]).\n2.5. Pretrained Language Models\nSince their inception, transformer-based pretrained lan-\nguage models such as BERT [22] and ERNIE [23] have\nachieved state- of-the-art performance in various classiﬁcation\ntasks.The results of previous research demonstrate that the lan-\nguage characteristics of AD too can be captured by pretrained\nlanguage models ﬁne-tuned to the task of AD classiﬁcation (see\nabove). In this paper, pretrained BERT and ERNIE models were\nﬁne-tuned for the AD classiﬁcation task and combined with\nclassiﬁers trained on complexity and (dis)ﬂuency features (see\nSection 3). Each of the 161 speakers in the training data is con-\nsidered as a data point. The input of the model consists of all\nthe text sequences of each speaker obtained by the ASR system,\nand the output is the class of the corresponding speaker, 0 for\nControl and 1 for AD.\nFigure 1: Schematic representation of ‘complexity contours’ for\ntwo out of 293 complexity measures (CM) investigated: CTTR\n(Corrected Type Token Ratio) and Dependent Clauses per TU-\nnit). Centering/scaling was applied here only for purposes of\nillustration.\n3. Experimental Setup\nIn this section we describe the component models used in\nour approach and how they were combined. To assess the per-\nformance of each model, 5-fold cross validation was used.\n3.1. CNN Complexity + (Dis)Fluency Models\nIn order to make optimal use of the complexity and\n(dis)ﬂuency features, which are sequential in nature, we built\nconvolutional neural network (CNN) models. Originally pro-\nposed in computer vision, CNNs have been successfully\nadapted to various NLP tasks [24] and sentence classiﬁcation\ntasks [25][26][27]. The CNN model has the advantage over\nmodels that rely on aggregated features, e.g. mean feature val-\nues, in that it is capable of capturing patterns in a feature se-\nquence. We followed the approach proposed by [26], but re-\nplaced the word embedding with the concatenation of com-\nplexity and (dis)ﬂuency features. Due to the small size of the\ndataset, we set the size of ﬁlters to be 2 ×d, 3 ×d, 4 ×d where\nd is the input feature dimension. Eight ﬁlters were used for each\nof the three ﬁlter types.\n3.2. Fine-tuned BERT and ERNIE Models\nThe Huggingface Transformers library [28] was adopted\nfor ﬁne tuning pretrained language models. Bert-for-Sequence-\nClassiﬁcation was used and initialized with ‘bert-base-uncased’\nand ‘nghuyong/ernie-2.0-en’ as our pretrained BERT and\nERNIE model, respectively. In both cases, the base model was\nused rather than the large one, as preliminary experiments re-\nvealed no reliable differences in terms of classiﬁcation accuracy\nbetween the two models on our dataset. Both models consists\nof 12 Transformer layers with hidden size 768 and 12 atten-\ntion heads. The following hyperparameters were used for ﬁne-\ntuning: the learning rate was set to 2 × 10−5 with 50 warmup\nsteps and l2 regularization set to 0.1. The maximum sequence\nlength for both models was set to 256. For both models, default\ntokenizers were used.\n3.3. Use of Ensembling Methods\nPrevious research on predicting AD using pretrained lan-\nguage models has demonstrated that their predictions based on\nﬁne-tuning for downstream tasks with a small dataset tend to\nbe brittle and subject to high variance. To reduce this variance,\nwe used an adapted version of the ensembling approach pro-\nposed in [9]: Each of the models described above was trained\n50 times (N = 50). During the prediction phase, each model\ninstance independently generated a prediction. The ﬁnal classi-\nﬁcation decision was then determined by hard-voting, i.e. each\nmodel contributed its class prediction as a vote and the class that\nreceives the majority of the votes was returned by the ensemble\nmodel. Besides using ensemble methods so as to reduce the\nvariance in the prediction of a model, we also employed them\nto integrate information from different models. To this end, we\nperformed experiments with two types of ensemble based meth-\nods, which are referred to here as ensembling by bagging and\nensembling by stacking. Bagging involves ﬁtting several inde-\npendent models and pooling their predictions in order to obtain\na model with a lower variance, while stacking involves combin-\ning the models by training a meta-model to output a prediction\nbased on the different models predictions (see below). In each\nof the combined models, we used the same hyperparameter set-\ntings as stated above.\n3.3.1. Model A: Ensembling by bagging via majority vote\nEnsembling by bagging via majority vote has been shown\nto be a simple yet effective method to increase the performance\nof classiﬁcation models [29][30]. The ﬁrst classiﬁcation model\nFigure 2: Structure diagram of Model A. During training, we\ntrain each of the k models N times. During inference, jth in-\nstance of model i gives prediction ˆyij independently. The ﬁnal\noutput of the ensembled model ˆY is the label, which the major-\nity of the k × N model instances agree upon.\n(Model A) employed majority voting among 50 CNNs that used\ncomplexity and (dis)ﬂuency features and 50 ERNIE models (see\nFigure 2). That is, as speciﬁed above, in this approach, each\nmodel was ﬁrst trained/ﬁne tuned 50 times, meaning that the\nﬁnal classiﬁcation was based on 100 model instances. The clas-\nsiﬁcation in the Model A approach was then determined by\ncounting the votes for each class (AD and controls (CN)) and\nchoosing the more frequent class as the predicted one.\nComplexity +\n(Dis)fluency Features Word Embeddings\nCNN ERNIE\nGlobal Max Pooling ERNIE Pooled Layer\nDropout Layer\nFeedforward\nClassifier\nFigure 3: Structure diagram of Model B.\n3.3.2. Model B: Ensembling by bagging using feature fusion\nThe second model (Model B) combined a CNN and a BERT\nmodel, which has previously been shown to perform better than\neither model alone [31]. Following the approach taken by [31],\nwe built a model in which complexity and (dis)ﬂuency informa-\ntion was ﬁrst concatenated at the feature-level and subsequently\nfed into a CNN (see Figure 3. The hidden vector coming from\nthis CNN is then concatenated with ERNIE. More speciﬁcally,\nthe pooled output vector of the [CLS] 4 token for Ernie model.\nThis concatenated hidden vector will serve as the input of a feed\nforward classiﬁer on top of CNN and Ernie. To train this model,\nwe ﬁrst ﬁne-tune ERNIE model. Then we freeze the parameters\nof the ﬁne-tuned ERNIE model and jointly train the CNN model\nand combined feedforward classiﬁer.\n3.3.3. Model C: Ensembling by stacking\nThe ﬁnal model, Model C, used in our experiments em-\nployed a stacking approach to ensemble all models [32], which\nhas been shown to effectively increase the accuracy of the en-\nsembled individual models. Speciﬁcally, we employed model\n4[CLS], stands for classiﬁcation, is a special token added in front of\nevery input samples of BERT/ERNIE model to represent sample-level\nclassiﬁcation [22].\nTable 2: Mean accuracy (with standard deviations), precision, recall and F1 scores over a 5 fold cross-validation\nPrecision Recall F1\nModel Acc CN AD CN AD CN AD\nCNN Comp M (SD) M (SD) M (SD) M (SD) M (SD) M (SD) M (SD)\nCNN[Comp+DisFl] 0.80(0.06) 0.79(0.06) 0.81(0.08) 0.78(0.07) 0.83(0.06) 0.78(0.05) 0.82(0.07)\nBert-Base 0.79(0.06) 0.77(0.09) 0.84(0.08) 0.81(0.11) 0.78(0.12) 0.78(0.06) 0.80(0.07)\nErnie-Base 0.80(0.04) 0.80(0.08) 0.81(0.04) 0.77(0.07) 0.83(0.09) 0.78(0.04) 0.82(0.05)\nModel A: CNN[Comp+DisFl]+[Ernie]\n(sep mod, bagging) 0.76(0.07) 0.61(0.13) 0.88(0.05) 0.79(0.08) 0.74(0.08) 0.68(0.10) 0.80(0.06)\nModel B: CNN[Comp+DisFl]+[Ernie]\n(fusion, bagging) 0.83(0.06) 0.75(0.11) 0.89(0.04) 0.83(0.09) 0.82(0.06) 0.78(0.09) 0.85(0.04)\nModel C: LR[Comp]+LR[DisFl]+[Ernie]+[Bert]\n(stacking) 0.83(0.07) 0.82(0.10) 0.85(0.09) 0.83(0.10) 0.84(0.09) 0.82(0.08) 0.84(0.07)\nTable 3: Performance of the three ensemble models on test set\nPrecision Recall F1\nModel Acc CN AD CN AD CN AD\nModel A: CNN[Comp+DisFl]+Ernie(sep mod, bagging) 0.79 0.77 0.81 0.83 0.74 0.80 0.78\nModel B: CNN[Comp+DisFl]+Ernie (fusion, bagging) 0.75 0.73 0.77 0.81 0.69 0.76 0.72\nModel C: LR[Comp]+LR[DisFl]+Ernie+Bert (stacking) 0.83 0.82 0.85 0.86 0.80 0.84 0.82\nFigure 4: Schematic representation of ensembling by stacking.\nstacking to combine two logistic regression models (LR) using\ncomplexity and (dis)ﬂuency features respectively, and the two\npretrained language models, i.e. BERT and ERNIE. The train-\ning procedure consists of two stages (see Figure 4). First, in\nstage one, each of the four models is trained/ﬁne-tuned inde-\npendently using 5-fold cross-validation (CV). For each sample\nin the test fold, we obtain one prediction vector from each of\nthe four models (Models 1 to 4). These predictions vectors are\nthen concatenated and constitute the input data in a subsequent\nstage (stage 2). The ﬁnal predictions of Model C are derived\nfrom another logistic regression model trained on the concate-\nnated prediction vectors from stage 1. To perform inference\non the test set, we take the predictions from all model instances\ntrained in stage 1 and average them by model, which will served\nas input of stage 2 after concatenation. All hyperparameters for\nthe training/ﬁne-tuning of each of the ensembled models were\nselected as speciﬁed above.\n4. Evaluation\nIn this section, we present our results on the AD detection\ntask. The evaluation metrics for detection (accuracy, precision,\nrecall, and F1 score) on the cross-validation (CV) set are pre-\nsented in Table 2. The results on the evaluation set are shown in\nTable 3. As indicated by boldface numbers, the best perform-\ning model in both cross-validation (mean accuracy = 83.16%)\nand testing (accuracy = 83.10%) was Model C, i.e. the model\nthat combined complexity and (dis)ﬂuency features with both\npretrained language models using stacking. Model B, which\ncombined a CNN trained on utterance-level complexity and\n(dis)ﬂuency features with the best performing ﬁne-tuned pre-\ntrained language model (ERNIE) using late fusion and ensem-\nbling by bagging, fell close behind reaching 82.7% accuracy in\nCV . Model A, which combined the same features using majority\nvoting with separate classiﬁers, performed below the accuracy\nlevels of its component models, reaching 75.69% accuracy in\nCV . On the test set, the accuracy score of 83.1% of the best per-\nforming model, Model C, constitutes an improvement by 4.23%\nover the baseline model, which was based on fusion of linguis-\ntic and acoustic features [8]. Surprisingly, the relative perfor-\nmances of Model A and Model B were reversed on the test set,\nwith Model A matching the performance of the baseline exactly\n(accuracy = 78.87%) and Model B falling just short of that (ac-\ncuracy = 74.65%). The considerable discrepancies between the\nCV and test set classiﬁcation accuracy for these models suggest\nthat they suffer from overﬁtting. In contrast, Model C, which\nemployed the stacking technique, performed equally well on\nCV and test data, indicating that it is robust against overﬁtting.\n5. Discussion and Conclusion\nThe work presented here combined linguistic complexity\nand (dis)ﬂuency features with pretrained language models for\nthe task of Alzheimer’s disease detection. An accuracy of\n83.1% was achieved on the test set, which amounts to an im-\nprovement of 4.23 % over the baseline model, which was based\non fusion of linguistic and acoustic features. Our best perform-\ning model combined component models using a stacking en-\nsemble technique. A key ﬁnding of this study is that incor-\nporating information on linguistic complexity and (dis)ﬂuency\nimproved the performance of ﬁne-tuned pretrained language\nmodels in AD classiﬁcation by 3%, suggesting that different\ncomponent models encode complementary information regard-\ning the characteristic language patterns of AD. Another impor-\ntant aspect of our results is that the ensemble model trained\non ‘complexity contours’, i.e. utterance-level measurements of\nhuman-interpretable complexity and ﬂuency features, was able\nto match the performance of both ﬁne-tuned pretrained BERT-\nlike language models: Using 5-fold cross-validation with en-\nsembling of 50 models in each fold, we obtained robust perfor-\nmance scores ( ≈ 80%) for both types of models. This ﬁnd-\ning has important implications in light of increasing calls for\nmoving away from black-box models towards white-box (inter-\npretable) models for critical industries such as healthcare, ﬁ-\nnances and news industry [33, 34].\n6. References\n[1] M. P. Mattson, “Pathways towards and away from alzheimer’s dis-\nease,” Nature, vol. 430, no. 7000, pp. 631–639, 2004.\n[2] J. Zeisel, K. Bennett, R. Fleming et al., “World alzheimer report\n2020: Design, dignity, dementia: Dementia-related design and the\nbuilt environment,” 2020.\n[3] S. de la Fuente Garcia, C. Ritchie, and S. Luz, “Artiﬁcial intelli-\ngence, speech, and language processing approaches to monitoring\nalzheimer’s disease: A systematic review,”Journal of Alzheimer’s\nDisease, no. Preprint, pp. 1–27, 2020.\n[4] S. Luz, “Longitudinal monitoring and detection of alzheimer’s\ntype dementia from spontaneous speech data,” in2017 IEEE 30th\nInternational Symposium on Computer-Based Medical Systems\n(CBMS). IEEE, 2017, pp. 45–46.\n[5] E. L. Campbell, R. Y . Mes ´ıa, L. Doc ´ıo-Fern´andez, and\nC. Garc´ıa-Mateo, “Paralinguistic and linguistic ﬂuency features\nfor alzheimer’s disease detection,”Computer Speech & Language,\nvol. 68, p. 101198, 2021.\n[6] P. Pastoriza-Dominguez, I. G. Torre, F. Dieguez-Vide, I. Gomez-\nRuiz, S. Gelado, J. Bello-Lopez, A. Avila-Rivera, J. Matias-Guiu,\nV . Pytel, and A. Hernandez-Fernandez, “Speech pause distribu-\ntion as an early marker for alzheimer’s disease,” medRxiv, pp.\n2020–12, 2021.\n[7] R. S. Bucks, S. Singh, J. M. Cuerden, and G. K. Wilcock,\n“Analysis of spontaneous, conversational speech in dementia of\nalzheimer type: Evaluation of an objective technique for analysing\nlexical performance,” Aphasiology, vol. 14, no. 1, pp. 71–91,\n2000.\n[8] S. Luz, F. Haider, S. de la Fuente, D. Fromm, and B. MacWhin-\nney, “Detecting cognitive decline using speech only: The adresso\nchallenge,” medRxiv, 2021.\n[9] J. Yuan, Y . Bian, X. Cai, J. Huang, Z. Ye, and K. Church, “Disﬂu-\nencies and ﬁne-tuning pre-trained language models for detection\nof alzheimer’s disease,”Proc. Interspeech 2020, pp. 2162–2166,\n2020.\n[10] M. S. S. Syed, Z. S. Syed, M. Lech, and E. Pirogova, “Automated\nscreening for alzheimer’s dementia through spontaneous speech,”\nINTERSPEECH (to appear), pp. 1–5, 2020.\n[11] A. Balagopalan, B. Eyre, F. Rudzicz, and J. Novikova, “To bert or\nnot to bert: Comparing speech and language-based approaches for\nalzheimer’s disease detection,”arXiv preprint arXiv:2008.01551,\n2020.\n[12] J. S. Guerrero-Cristancho, J. C. V ´asquez-Correa, and J. R.\nOrozco-Arroyave, “Word-embeddings and grammar features to\ndetect language disorders in alzheimer’s disease patients,” Tec-\nnoL´ogicas, vol. 23, no. 47, pp. 63–75, 2020.\n[13] B. Mirheidari, D. Blackburn, T. Walker, A. Venneri, M. Reuber,\nand H. Christensen, “Detecting signs of dementia using word vec-\ntor representations.” in INTERSPEECH, 2018, pp. 1893–1897.\n[14] J. B. Orange, R. B. Lubinski, and D. J. Higginbotham, “Conver-\nsational repair by individuals with dementia of the alzheimer’s\ntype,” Journal of Speech, Language, and Hearing Research ,\nvol. 39, no. 4, pp. 881–895, 1996.\n[15] Y . Pan, B. Mirheidari, M. Reuber, A. Venneri, D. Blackburn, and\nH. Christensen, “Improving detection of alzheimer’s disease using\nautomatic speech recognition to identify high-quality segments\nfor more robust feature extraction,” Proc. Interspeech 2020, pp.\n4961–4965, 2020.\n[16] E. Kerz, Y . Qiao, D. Wiechmann, and M. Str¨obel, “Becoming lin-\nguistically mature: Modeling english and german children’s writ-\ning development across school grades,” inProceedings of the Fif-\nteenth Workshop on Innovative Use of NLP for Building Educa-\ntional Applications, 2020, pp. 65–74.\n[17] Y . Qiao, D. Wiechmann, and E. Kerz, “A language-based ap-\nproach to fake news detection through interpretable features and\nbrnn,” in Proceedings of the 3rd International Workshop on Ru-\nmours and Deception in Social Media (RDSM), 2020, pp. 14–31.\n[18] M. Str ¨obel, E. Kerz, and D. Wiechmann, “The relationship be-\ntween ﬁrst and second language writing: Investigating the effects\nof ﬁrst language complexity on second language complexity in\nadvanced stages of learning,” Language Learning, vol. 70, no. 3,\npp. 732–767, 2020.\n[19] M. H. Christiansen and N. Chater, “Towards an integrated science\nof language,” Nature Human Behaviour, vol. 1, no. 8, Jul. 2017.\n[20] C. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. Bethard, and\nD. McClosky, “The stanford corenlp natural language processing\ntoolkit,” inProceedings of 52nd annual meeting of the association\nfor computational linguistics: system demonstrations , 2014, pp.\n55–60.\n[21] D. Klein and C. D. Manning, “Accurate unlexicalized parsing,” in\nProceedings of the 41st Annual Meeting on Association for Com-\nputational Linguistics-Volume 1. Association for Computational\nLinguistics, 2003, pp. 423–430.\n[22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” arXiv preprint arXiv:1810.04805, 2018.\n[23] Y . Sun, S. Wang, Y . Li, S. Feng, H. Tian, H. Wu, and H. Wang,\n“Ernie 2.0: A continual pre-training framework for language un-\nderstanding,” arXiv preprint arXiv:1907.12412, 2019.\n[24] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu,\nand P. Kuksa, “Natural language processing (almost) from\nscratch,” Journal of machine learning research, vol. 12, no. AR-\nTICLE, pp. 2493–2537, 2011.\n[25] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A convolu-\ntional neural network for modelling sentences,” arXiv preprint\narXiv:1404.2188, 2014.\n[26] Y . Kim, “Convolutional neural networks for sentence classiﬁca-\ntion,” 2014.\n[27] M. Ma, L. Huang, B. Xiang, and B. Zhou, “Dependency-based\nconvolutional neural networks for sentence embedding,” arXiv\npreprint arXiv:1507.01839, 2015.\n[28] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue,\nA. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz,\nJ. Davison, S. Shleifer, P. von Platen, C. Ma, Y . Jernite,\nJ. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest,\nand A. M. Rush, “Transformers: State-of-the-art natural\nlanguage processing,” in Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing: System\nDemonstrations. Online: Association for Computational\nLinguistics, Oct. 2020, pp. 38–45. [Online]. Available: https:\n//www.aclweb.org/anthology/2020.emnlp-demos.6\n[29] C. Costello, R. Lin, V . Mruthyunjaya, B. Bolla, and C. Jankowski,\n“Multi-layer ensembling techniques for multilingual intent classi-\nﬁcation,” 2018.\n[30] N. C. Oza and K. Tumer, “Classiﬁer ensembles: Select real-world\napplications,” Information fusion, vol. 9, no. 1, pp. 4–20, 2008.\n[31] I. Alghanmi, L. Espinosa-Anke, and S. Schockaert, “Combining\nbert with static word embeddings for categorizing social media,”\n2020.\n[32] D. H. Wolpert, “Stacked generalization,” Neural networks, vol. 5,\nno. 2, pp. 241–259, 1992.\n[33] C. Rudin, “Stop explaining black box machine learning models\nfor high stakes decisions and use interpretable models instead,”\nNature Machine Intelligence, vol. 1, no. 5, pp. 206–215, 2019.\n[34] O. Loyola-Gonzalez, “Black-box vs. white-box: Understanding\ntheir advantages and weaknesses from a practical point of view,”\nIEEE Access, vol. 7, pp. 154 096–154 113, 2019.",
  "topic": "Overfitting",
  "concepts": [
    {
      "name": "Overfitting",
      "score": 0.8186173439025879
    },
    {
      "name": "Fluency",
      "score": 0.6789816617965698
    },
    {
      "name": "Computer science",
      "score": 0.6067585945129395
    },
    {
      "name": "Task (project management)",
      "score": 0.5235664248466492
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4958077371120453
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4888598322868347
    },
    {
      "name": "Dementia",
      "score": 0.4615267813205719
    },
    {
      "name": "Component (thermodynamics)",
      "score": 0.4554665684700012
    },
    {
      "name": "Natural language processing",
      "score": 0.4322195053100586
    },
    {
      "name": "Language model",
      "score": 0.41805803775787354
    },
    {
      "name": "Speech recognition",
      "score": 0.40255749225616455
    },
    {
      "name": "Psychology",
      "score": 0.30044683814048767
    },
    {
      "name": "Disease",
      "score": 0.27125322818756104
    },
    {
      "name": "Medicine",
      "score": 0.12091696262359619
    },
    {
      "name": "Artificial neural network",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematics education",
      "score": 0.0
    }
  ]
}