{
  "title": "Summary Cycles: Exploring the Impact of Prompt Engineering on Large Language Models’ Interaction with Interaction Log Information",
  "url": "https://openalex.org/W4392669885",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5064127720",
      "name": "Jeremy E. Block",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5100742726",
      "name": "Yupeng Chen",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5092899296",
      "name": "Abhilash Budharapu",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5036443161",
      "name": "Lisa Anthony",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5054060679",
      "name": "Bonnie J. Dorr",
      "affiliations": [
        "University of Florida"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4365512576",
    "https://openalex.org/W1959365993",
    "https://openalex.org/W4237289619",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W3098096010",
    "https://openalex.org/W4297094725",
    "https://openalex.org/W4319165821",
    "https://openalex.org/W4287887766",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W4287113019",
    "https://openalex.org/W2807008868",
    "https://openalex.org/W3123811550",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W3207553988",
    "https://openalex.org/W4385572815",
    "https://openalex.org/W3174073137",
    "https://openalex.org/W4385573484",
    "https://openalex.org/W4285254250",
    "https://openalex.org/W4283396845",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3170432046",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3213679486",
    "https://openalex.org/W3199174176",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4385573357",
    "https://openalex.org/W4303649108",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W2536393303",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2904790185",
    "https://openalex.org/W3035461111",
    "https://openalex.org/W2782931675"
  ],
  "abstract": "With the aim of improving work efficiency, we examine how Large Language Models (LLMs) can better support the handoff of information by summarizing user interactions in collaborative intelligence analysis communication. We experiment with interaction logs, or a record of user interactions with a system. Inspired by chain-of-thought prompting, we describe a technique to avoid API token limits with recursive summarization requests. We then apply ChatGPT over multiple iterations to extract named entities, topics, and summaries, combined with interaction sequence sentences, to generate summaries of critical events and results of analysis sessions. We quantitatively evaluate the generated summaries against human-generated ones using common accuracy metrics (e.g., ROUGE-L, BLEU, BLEURT, and TER). We also report qualitative trends and the factuality of the output. We find that manipulating the audience feature or providing single-shot examples minimally influences the model's accuracy. While our methodology successfully summarizes interaction logs, the lack of significant results raises questions about prompt engineering and summarization effectiveness generally. We call on explainable artificial intelligence research to better understand how terms and their placement may change LLM outputs, striving for more consistent prompt engineering guidelines.",
  "full_text": "Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems, pages 85–99\nNovember 1, 2023 ©2023 Association for Computational Linguistics\nSummary Cycles: Exploring the Impact of Prompt Engineering on Large\nLanguage Models’ Interaction with Interaction Log Information\nJeremy E. Block Yu-Peng Chen Abhilash Budharapu\nLisa Anthony Bonnie Dorr\n{j.block, yupengchen, budharapu.ab}@ufl.edu, lanthony@cise.ufl.edu, bonniejdorr@ufl.edu\nUniversity of Florida\nAbstract\nWith the aim of improving work efficiency, we\nexamine how Large Language Models (LLMs)\ncan better support the handoff of information\nby summarizing user interactions in collabora-\ntive intelligence analysis communication. We\nexperiment with interaction logs, or a record\nof user interactions with a system. Inspired\nby chain-of-thought prompting, we describe a\ntechnique to avoid API token limits with re-\ncursive summarization requests. We then ap-\nply ChatGPT over multiple iterations to ex-\ntract named entities, topics, and summaries,\ncombined with interaction sequence sentences,\nto generate summaries of critical events and\nresults of analysis sessions. We quantita-\ntively evaluate the generated summaries against\nhuman-generated ones using common accuracy\nmetrics (e.g., ROUGE-L, BLEU, BLEURT,\nand TER). We also report qualitative trends\nand the factuality of the output. We find that\nmanipulating the audience feature or provid-\ning single-shot examples minimally influences\nthe model’s accuracy. While our methodol-\nogy successfully summarizes interaction logs,\nthe lack of significant results raises questions\nabout prompt engineering and summarization\neffectiveness generally. We call on explain-\nable artificial intelligence research to better un-\nderstand how terms and their placement may\nchange LLM outputs, striving for more consis-\ntent prompt engineering guidelines.\n1 Introduction\nMark M. Lowenthal describes intelligence in three\nways: (1) the process of preparing collected intel-\nligence for (often) government consumers; (2) a\nproduct of such a process, e.g., a report, database,\nor “Intellipedia;” (3) the community of people and\ninstitutions involved in the preparation, and prod-\nucts, of the intelligence cycle (Lowenthal, 2018).\nWhile there is some debate about what is consid-\nered intelligence work (Andrew et al., 2019), this\ndomain is characterized by multiple, nonlinear data\nprocessing steps in collaboration with multiple de-\npartments and people. Tools that could support\nthe distribution of what is known and how the in-\nformation was derived could be beneficial. Yet, it\nis challenging to prepare written communication\nabout the precise event sequences that led to a par-\nticular outcome from users’ memory alone. To\naddress this, analytic provenance has emerged as a\npromising solution.\nProvenance, in this context, refers to the docu-\nmentation and representation of the process and\ncontext underlying an analysis, capturing the steps,\ndata sources, algorithms, and decisions made by\nan analyst. The promise of provenance is to en-\nable transparency and reproducibility, but listing\nall the steps leads to a verbose record that may not\nsupport these goals. Instead, we apply analytic\ntechniques to illicit patterns automatically or visu-\nally represent application states over time (Ragan\net al., 2015; Xu et al., 2020). When applied to the\nfield of intelligence, often that means capturing in-\nteraction logs (i.e., recorded steps taken by a user\nto complete their task) to distill key aspects, facili-\ntating a more comprehensive understanding of the\nproblem-solving process.\nThe goal of analytic provenance research is there-\nfor focused on illuminating the reasoning behind\nsteps taken and how conclusions are reached. Of-\nten, techniques can make steps clear or visualize\nhow often data is examined (Block et al., 2023), but\nunderstanding why a step was taken is often more\ndifficult to elucidate from system processes. This is\nwhere analytic provenance research seeks to push\nboundaries, providing more semantically meaning-\nful explanations by looking for patterns among the\nseries of interactions. By incorporating analytic\nprovenance, researchers can effectively communi-\ncate the methodology employed, supporting peer\nreview, knowledge exchange, and collaboration.\nResources such as Papers with Code, GitHub,\nand the Open Science Framework emphasize the\n85\nopen-source nature of research and the need to cen-\ntralize provenance information. However, we have\nnot seen evidence of efficiently processing interac-\ntion log information to provide textual summaries\nwith the goal of enabling transparency. By consid-\nering interaction logs to describe the steps taken\nto complete a task, LLMs are uniquely suited to\nexamine patterns in this language and might serve\nas a general-purpose analysis tool in the analytic\nprovenance toolkit.\nThis study aims to gain a better understanding of\nhow large language models (LLMs) can expand the\npossibilities of interaction log information, focus-\ning on a specific set of prompt engineering features.\nWe observe that the LLMs can extract features from\nan interaction history. We further evaluate the im-\npacts of different prompting effects on the output,\nengineering prompts to vary the addition of ex-\namples and audience description for the LLM. By\nmanipulating these prompts, we aim to investigate\nhow they impact the output generated by the model\nwhen presented with interaction log information.\nThis research seeks to shed light on the intri-\ncate relationship between large language models\nand interaction log data. By examining the effects\nof prompt engineering features on the model’s re-\nsponse, we can gain insights into how to effectively\nleverage these models for enhancing analytic prove-\nnance and, ultimately, the efficient communication\nof problem-solving in complex domains. The find-\nings from this study will contribute to advancing\nthe field of NLP and inform the development of\nmore sophisticated tools for capturing, summariz-\ning, and leveraging interaction log data in analytic\nprovenance research. Our contributions include the\nfollowing:\n1. A method of recursive prompt reduction with\nthe same LLM.\n2. A demonstration of our method on the relevant\nintelligence and analytic provenance domain.\n3. A quantitative analysis of accuracy and factu-\nality among output summaries.\n4. A qualitative comparison of output summaries\nand prompt engineering guidelines.\n5. A commentary on the ethical use of large lan-\nguage models for workplace cohesion tasks.\nBased on the research contributions completed, we\nbelieve that our work will benefit the intelligence\nfield by:\n• demonstrating that large language models can\nbe applied to the context of provenance infor-\nmation as a tool for describing how people\ncreate intelligence products,\n• reporting on the factuality and accuracy of the\nproducts to serve as a baseline for future work,\n• discussing some concerns about the use of\nlarge language models for the production of\nwork reports.\n2 Related Work\nThe NLP field has seen public attention this year\nfrom the widespread adoption and use of genera-\ntive pre-trained models (Zhao et al., 2023). In this\nwork, we explore how LLMs can support analytic\nprovenance research, especially when paired with\nprompt engineering approaches.\n2.1 NLP for Analysing Interaction Logs\nInteraction logs come in many forms and can be an-\nalyzed in different ways to extract insights. Marin-\nCastro and Tello-Leal (2021) consider user inter-\naction logs to better understand organizational pro-\ncesses, Hamooni et al. (2016), generate insights\nfrom internet-connected devices, and Guo, Yuan,\nand Wu (2021) identify anomalous activity among\nnetwork system log messages with a pre-trained\nencoder model like BERT. In all of these contexts,\nanalytic provenance techniques are applied to make\nsense of interaction logs and deliver insights in the\nform of interrelated and hierarchical system dia-\ngrams or notifications. This is helpful, especially\nwhen examining logs across large organizations or\namong corpora of captured event messages from\nheterogeneous sources. But at a smaller day-to-\nday scale, there are communications among team\nmembers and managers that communicate work\ncompleted that could use support from analytic\nprovenance techniques.\nHowever, common business communications are\nnot typically communicated with graphs or charts.\nTo match familiar styles and minimize a need for\nvisualization literacy, there is a need to present\ninsights as text. Liu et al. (2021) generate summa-\nrizations from code snippets to make code easier\nto interpret and maintain, but they rely heavily on\ngraphs as a transition language to map from lines of\ncode to text. Similarly, converting interaction histo-\nries into a textual summary is its own challenge. In\nour case, we explore a technique to automatically\n86\ncombine contextual information with interaction\ninformation to distill a comprehensive textual sum-\nmary of a user’s analysis session.\n2.2 Prompt Engineering\nPrompt engineering (Beltagy et al., 2022) has\nemerged as a viable technique for improving the\nperformance of summarization models. By pro-\nviding explicit instructions to the model, prompt\nengineering can help facilitate the generation of\nmore accurate summaries.\nFirstly, there are few-shot methods (Tsim-\npoukelli et al., 2021) that recommend providing\na task-specific example to improve the accuracy\nof the expected result. This approach leverages a\nlarge pre-trained language model and fine-tunes it\non a small example case for effective summariza-\ntion. For example, Liu et al. (2022) extend this\nconcept by providing unstructured information in-\nstead of a single example. Regardless, they show\nhow providing contextual information can support\nlarge language reasoning tasks.\nAlternatively, Reynolds and McDonell (2021)\nshow how the lack of task-specific examples can\nalso be effective. Several studies have explored\nthe zero-shot paradigm (Ye et al., 2023; Wei et al.,\n2022), where models are trained to generate sum-\nmaries without any specific fine-tuning on sum-\nmarization datasets. Often these approaches rely\non prefix-tuning (Zhou et al., 2023) or perturbing\nthe training data with noise (Lewis et al., 2019).\nregardless, these approaches have shown promise,\nespecially working with generalized pre-trained\nmodels (Reynolds and McDonell, 2021)\nFinally, Chain-of-thought methods have also\ngained attention, where an LLM is given a list\nof steps to complete in addition to the specified\ncontent (Wei et al., 2023). Zhang et al. (2023) pro-\npose a method to generate summaries by explicitly\ndescribing the chain of steps to the model and pro-\nviding a rationale. This encourages the model to\nreason more about the prompt and provide more\naccurate replies. Overall, prompt engineering tech-\nniques, including zero-shot, few-shot, and chain-\nof-thought methods, have shown promise in en-\nhancing summarization performance by providing\nexplicit guidance and controlling the generation\nprocess. These approaches influence the methodol-\nogy presented in this paper.\n3 Experimental Procedure\nTo better understand the expectations and effects\nof using large language models for summarization\nof interaction logs, we conduct a handful of experi-\nments, starting with the collection of user feedback\nfrom a qualitative study. From this pilot study, we\nthen conduct a series of NLP prompting experi-\nments to compare differences in how the addition\nof examples and audience types influence model\noutput summaries. Throughout these experiments,\nwe use the OpenAI Chat Completions API with\nthe “gpt-3.5-turbo 1” model as the LLM for our\napproach (Brown et al., 2020).\n3.1 Pilot Study\nMany summarization approaches score summaries\nbased on their coherence, fluency, informativeness,\nand relevancy (Wu et al., 2020), yet no applicable\nframework existed for summarizing intelligence\nwork for hand-off communication. We conduct a\nuser study with the primary objective of better un-\nderstanding which features are preferred by human\nusers in work summaries for different types of au-\ndiences. While the details of this study are beyond\nthe scope of this paper, we provide an overview of\nthe methodology used to derive our prompting fea-\ntures. We create an online questionnaire to gather\ninsights from anonymous participants and identify\nthe qualities of summarization that human evalua-\ntors find beneficial for peer collaborators and team\nmanagers. The study was approved by our insti-\ntutional review board and aims to understand user\npreferences for work summaries.\nTo help participants understand the context, they\nare asked to review LLM-generated work sum-\nmaries and rank them according to their commu-\nnicative support for peer collaborators or team man-\nagers. The summaries vary in their generated con-\ntent and lengths, and participants are asked to quote\nspecific features and textually describe how they\nare valuable and invaluable. Finally, we also ask\nparticipants to classify a set of adjectives (e.g., ac-\ncuracy, conciseness, clarity, etc.) as core compo-\nnents or non-essential adjectives used to describe\npeer or manager summaries.\nTwenty graduate students pass the attention\nchecks and complete the questionnaire, but due\nto limited statistical power and the fact that no sum-\nmary was consistently ranked higher than any other,\n1Available at https://platform.openai.com/docs/\nmodel-index-for-researchers/\n87\nwe focus on the adjective classifications to draw our\nconclusions. The study results indicate that most\nparticipants believe our eight adjectives are core\ncomponents of good summaries. However, a small\npreference exists for certain words and contexts.\nThe results suggest that participants consider objec-\ntivity, relevance, conciseness, and clarity slightly\nmore essential for a manager’s summary but not for\ntheir peers. Instead, participants prefer that sum-\nmaries for peers be engaging and accurate. Both\nrelevance and properly citedscore the same by con-\nditions. Qualitatively, participants highlight how\nsummaries should strike a balance between provid-\ning enough detail without being too vague or overly\ndetailed and tailoring the level of information to the\nuser’s needs. The findings have guided us in adapt-\ning our prompt engineering experiment to identify\nkey features and terms for effective prompting.\n3.2 Dataset\nWe use a set of interaction logs2 from users com-\npleting a 90-minute textual sensemaking task. Orig-\ninally captured from 24 university students (non-\nanalysis experts), it consists of thousands of user\ninteraction events (e.g., mouseover, click, search,\netc.) as they review 103 fictional bank transactions,\nemail intercepts, and other facsimile intelligence\nreports from the V AST Challenge dataset (Mohseni\net al., 2018). To conduct our analysis, we experi-\nment with the interaction logs of the first three users\nsolving the V AST 2010 mini-challenge 1. The size\nof the chosen context is intentionally not large. We\nconduct our work on data at a reasonable size for\nhuman comprehension to better evaluate and act as\na demonstration of our pipeline. This limited size\nmakes it possible for one author to manually write\ngold-standard summarizations of user analysis pro-\ncesses.\n3.3 Documents to Context Sources (A)\nBefore engaging with the interaction logs for con-\ntext, we need a fairly complete source of reliable\ncontextual information for each of the documents\nusers could interact with in the original analysis\ndatabase. However, including document content\nfor each interaction would be excessive. Entity\nextraction has been shown to detect factual incon-\nsistencies (Lee et al., 2022). Also, the inclusion of\nknowledge before prompting for a specific answer\n2Available for download from https://www.cise.ufl.\nedu/~eragan/provenance-datasets.html\nFigure 1: A depiction of our proposed pipeline for mak-\ning interaction logs into work summaries. We prepro-\ncess the document space to A) extract information and\nB) generate interaction sentences by combining this in-\nformation with interactions. The generated sentences\nare C) segmented and summarized to prepare our D) ex-\nperiments. Finally, we examine the E) output summary.\ncan also improve model performance at reasoning\ntasks (Liu et al., 2022). Therefore, we prompt Chat-\nGPT to infer topics, identify entities, and summa-\nrize each document in the underlying document\ndataset as a pre-processing phase (Figure 1A).\nThis allows us to include additional context when\nan interaction occurs on a document and supports\nshorter prompt lengths because we can provide\ndocument topics instead of an entire document as\ncontext. When prompting for this contextual infor-\nmation, we provide precise instructions in terms\nof output lengths and formatting preferences (i.e.,\n100 words; JSON format). For a comprehensive\noverview of the full prompts, please refer to Table 2\nin the Appendix or our open-source code.3\n3.4 Interaction Logs to Sentences (B)\nAlthough ChatGPT is able to handle structured data\nformats like the ones used for interaction logs (e.g.,\nJSON), directly including the raw interaction logs\nin an API request will significantly increase the\nnumber of tokens. Therefore, we use a sentence-\ntemplating approach to preprocess the interaction\nlogs into sentences. Each logged interaction is sys-\ntematically transformed into a sentence by applying\na manually designed template for each interaction\ntype. For example, a search interaction would be\nconverted into the sentence: “The user searched\nfor <term>,” where ‘<term>’ would be substituted\nwith the relevant information from the interaction.\nWe apply this process for all 11 interaction types in\n3A version of our approach can be found at https://\ngithub.com/jeremy-block/spygest\n88\nthe dataset4 to mimic the naive conversion of inter-\nactions into sentences. Although this approach cre-\nates many similar-sounding sentences, it maintains\nthe original interaction sequence and generates a\ncomprehensive corpus of sentences that preserves\nthe context of user interactions. This process (Fig-\nure 1B) allows for subsequent segmentation and\nprompting processes as described next.\n3.5 Segmentation and Token Management (C)\nAt the time of writing, the OpenAI Chat Comple-\ntions API has a token limit of 4096. 5 In our use\ncase, a significant challenge arises as the entire\ninteraction session comprises hundreds of interac-\ntions, resulting in an average length of 13,788.33\ntokens, excluding tokens needed for prompts and\nresponses. To help reduce the number of tokens\nsent to the API, we draw inspiration from the step-\nby-step zero-shot chain-of-thought prompting tech-\nnique (Wang et al., 2023). Our recursive approach\n(depicted as Figure 1C) involves requesting sum-\nmaries for smaller segments of the interaction sen-\ntences and linking the input of each request with\nthe response from previous requests. By doing so,\nwe not only prompt ChatGPT with “let’s think step\nby step” but also establish distinct steps for the\nagent to follow.\nFigure 2: An illustration of our prompting process.\nThe text corpus describing the entire interaction\nsession is divided into ten segments, determined\nthrough a trial-and-error process where test runs\nare conducted to ensure that the number of tokens\nremains within the specified limit. The API takes\nmessages as input, where each message is assigned\na specific role (i.e., system, user, or assistant). As\nshown in Figure 2, the entire prompting process is\nconducted as a conversation that follows a format\n4We do not use any “think aloud” interaction types because\nthese were manually added by the dataset creators to augment\nthe data and provide some semantic ground truth within the\ncaptured logs. Verbal utterances like this are not commonly\ncaptured in standard interaction logs, so we choose to exclude\nthem.\n5https://platform.openai.com/docs/models/\ngpt-3-5\nbeginning with a system message, followed by a se-\nquence of alternating user and assistant messages.\nA total of 11 requests are sent to the API for each\ninteraction log summary, including one request for\neach segment and a final request for an overall\nsummary. To address the model’s \"memoryless\"\nnature, all messages are added to a growing list,\nserving as memory for ChatGPT, with the entire\nlist consistently sent in each request.\n3.6 Prompt Design (D)\nPrior work has shown that an effective prompt\nshould include clear and specific instructions (Wei\net al., 2023; Liu et al., 2022). Our prompting pro-\ncess follows this principle consistently. We use\ndelimiters (e.g., triple backticks) to indicate dis-\ntinct parts of the input. To construct our prompts,\nwe provide the content in three different message\ntypes. The system message is the first and explicitly\ninstructs ChatGPT about the task to be executed\nand the expected behavior. We include the core\nfeatures from the pilot study here to help the model\ndefine its persona. Next, we use alternating user\nand assistant messages to provide additional con-\ntext and our final prompt.\nAs shown in Figure 2, user messages either in-\nclude the segment to be summarized or the final\nprompt. On the other hand, assistant messages are\nused as a pseudo-memory, only containing sum-\nmarized segment text returned from earlier API re-\nquests. In the final user message, detailed persona-\nspecific instructions are included to explore the\npotential of tailoring the agent’s response to spe-\ncific user needs and expectations. It is here that\nwe specify the different types of audiences and the\ninclusion of different examples.\n3.7 Ground Truth Development (E)\nTo evaluate the measures described above, we lever-\nage a set of reliable summaries as the gold standard.\nOften, summarization accuracy is based on human-\ngenerated ground truth corpora against which gen-\nerated summaries are compared (Dernoncourt et al.,\n2018). Therefore, we create three types of ground\ntruth summaries for each of the three interaction\nlog sessions to use in the evaluation.\nFirst, a set of summaries were crafted by one au-\nthor for the three interaction log sessions, referred\nto as the manual summary. This was prepared by\ncarefully reviewing each interaction log, paying at-\ntention to the think-aloud events, and writing about\nthe major events from the sessions. Additionally, a\n89\nbaseline summary is generated with ChatGPT by\nfollowing our recursive prompting procedure (Fig-\nure 2). In the later prompt engineering experiments,\nwe include example summaries and different adjec-\ntives for audience types, but these generated sum-\nmaries show what ChatGPT does when recursively\nasked to summarize interaction logs as a baseline.\nBy happenstance, when testing, we noticed that\nby repeating the pseudo-memory with the final\nprompt, the resulting summary was consistently\nshorter. Because automated accuracy measures are\nsensitive to summary length (Koh et al., 2022; Pap-\nineni et al., 2002; Sellam et al., 2020; Snover et al.,\n2005) we include the summaries with additional\npseudo-memory context for our evaluation.\nBy incorporating these three types of ground\ntruth summaries, we can compare how recursively\nasking large language models to generate work\nsummaries compares to manually written reports\nfrom interaction logs.\n4 Results\nOur goal with this work is to demonstrate the sim-\nplicity of a recursive summarization technique for\ncommunicating user interaction logs. Overall, the\ngenerated summaries are promising and may offer\na realistic possibility for generating sufficient sup-\nport for report generation with human refinement.\nIn this section, we offer a handful of observations.\n4.1 Quantifiable Objective Metrics\nOur work examines the impact of various prompt\ndesigns on the two quantifiable measures of interest\n(i.e., our dependent variables), namely factuality\nand accuracy. In our experiment, we manipulate\ntwo independent variables: the target audiences\nand the prompt engineering strategies, each of\nwhich has four different levels. The target audi-\nences are characterized by the core features iden-\ntified in our pilot study. The four levels include\nno audience (none), self, peer, and manager. The\nprompt engineering strategies are manipulated by\nhow examples were provided to the LLM. The four\nlevels include no examples (Zero-Shot), providing\na manual summary (One-Shot), providing a masked\nmanual summary (One-Shot + Hint), and providing\na masked template (Hint). We examine interaction\nlogs from three participants, resulting in the analy-\nsis of 48 summaries (i.e., 3 (participants) x 4 (types\nof audiences) x 4 (types of provided examples)).\nFactuality A known challenge with abstractive\nsummarization is the chance of the model generat-\ning inaccurate information (i.e., hallucinations (Ji\net al., 2023; Gabriel et al., 2021)). For this rea-\nson, we evaluate the factuality of the base sum-\nmaries. Some techniques try to calculate factual-\nity automatically but are either not trained on our\nspecific use case (Ribeiro et al., 2022) or strug-\ngle to decompose summaries into reliable chunks\nfor comparison (Glover et al., 2022). Instead, we\nuse the FRANK framework defined by Pagnoni\net al. (2021) to manually determine the percent of\nfactual phrases in our generated summaries.\nUsing the same entity definitions presented in\nthe FRANK framework, the three baseline sum-\nmaries (i.e., the None x None condition) for each\nof the three participants are coded. Semantic Frame\nErrors occur when predicates, entity mentions, or\ncircumstance details are inaccurate. Discourse Er-\nrors describe when pronouns or entailments are in-\ncorrect. Content Verifiability Errors describe when\nthe content is essentially hallucinated or dramati-\ncally inconsistent. Finally, we choose to also count\nthe frequency of repeated phrasing as an additional\nerror type. One author manually applies this code\nto individual phrases of a summary and counts the\noccurrence of different types of errors. These er-\nror counts are then divided by the total number of\nphrases in a summary to calculate the factuality\npercentage.\nSession 3\nSession 2\nSession 1\n0% 25% 50% 75% 100%\nPercentage of Factuality Errors Present in Baseline Summary\nError Type\nRepeated Phrase\nContent Verifiability Errors\nDiscourse Errors\nSemantic Frame Errors\nFactuality Errors by Participant Baseline Summary\nFigure 3: a representation of the relative percentage of\ndifferent error types for the baseline summaries for each\nof the three participant interaction logs\nIn Figure 3, we see very few factual errors among\nthe three participants examined. As (Pagnoni\net al., 2021) discuss, transformer models have been\nshown to have fewer semantic frame errors than\nLSTM (Hochreiter and Schmidhuber, 1997) mod-\nels, but, as we see with our results, there are still\ndiscourse errors. We also observe more repeti-\ntion of sequences of words. This may be due to\nthe fundamental functionality of transformer mod-\n90\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●●●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●●\n●\nPrompting\nROUGE\nPrompting\nBLEU\nPrompting\nBLEURT\nPrompting\nTER\nAudience\nROUGE\nAudience\nBLEU\nAudience\nBLEURT\nAudience\nTER\nManual Baseline Additional Manual Baseline Additional Manual Baseline Additional Manual Baseline Additional\nManual Baseline Additional Manual Baseline Additional Manual Baseline Additional Manual Baseline Additional\n0\n100\n200\n300\n0\n100\n200\n300\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0\n0.1\n0.2\n0.3\n0.4\n−1.2\n−0.8\n−0.4\n0.0\n−1.2\n−0.8\n−0.4\n0.0\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0\n0.1\n0.2\n0.3\n0.4\nGround Truth Comparison\nScore\nAudience Level\nNo Audience\nSelf\nPeer\nManager\nPrompting Level\n0−Shot (none)\n1−Shot (manual)\nMasked Manual\nMaskedTemplate\nAccuracy Measures by Audience Type and Prompt Engineering Approach\nFigure 4: We show the distribution of each automated measure (i.e., ROUGE-L, BLEU, BLEURT, and TER)\ncompared across the three ground-truth summaries: One manually generated by a human, one generated by the\nbaseline model without any prompt engineering, and one generated in the same way but with the prompt provided\ntwice. Notice that these scores show very little variation among each group showing that the independent variables\nof the Audience type or Prompt Engineering approach have little influence on the accuracy of the measure.\nels (Vaswani et al., 2017), where each word is gen-\nerated with a certain probability. Given this context,\nsome words like “arms dealing, fraud, and illegal\npossession of arms, as well as events related to\nsickness, health issues, and business success” may\nbe repeated by the model because it frequently saw\nthem appear together or were defined in the initial\nsystem message. Regardless, we see high factual-\nity scores across the baseline summaries for each\nparticipant, leading us to consider other dependent\nfeatures.\nAccuracy to ground truths Determining the\naccuracy of a summary can be a challenge, and var-\nious factors must be considered, such as cohesion,\nreadability, conciseness, information-richness, pre-\ncision, quality of input text and summarization al-\ngorithm used, length of the summary and human\nevaluation (Gupta and Gupta, 2019). Instead, we\napply an ensemble of summary accuracy measures\nto help determine a general sense of accuracy. The\nset of accuracy criteria selected requires the gen-\nerated summaries to be compared to some ground\ntruth. As described in Section 3.7, we designed\nthree types of ground truth. One summary is writ-\nten by an author (i.e., manual), our LLM pipeline\ngenerates another (i.e., baseline), and another gen-\nerated version where the pseudo-memory is re-\npeated in the final prompt (i.e., additional). Koh\net al. (2022), suggest that Rouge-L aligns with hu-\nman expectations, but BLEU (Papineni et al., 2002)\nand BLEURT (Sellam et al., 2020) are also popu-\nlar for abstractive text summarization evaluations.\nTER can also describe accuracy, by converting one\nstring of text to another and counting the number\nof changes (e.g., insertions, deletions, etc.) (Snover\net al., 2005). We choose a handful of techniques to\nget a general sense of the accuracy of our various\nground truth summaries (i.e., Manual, Baseline,\nand Additional) given different audience types and\nprompt engineering strategies.\nIn Figure 4, we see a variety of ranges for each\naccuracy measure (i.e., each facet of the chat).\nLooking at the Audience levels (i.e., green hues)\nand Prompt Engineering (i.e., magenta hues), we\nsee little variation among these levels too. Alter-\nnatively, we see more differentiation based on the\nground truth summary comparison (i.e., the hori-\nzontal grouping), signaling that the summary we\nused to compare the accuracy may have more influ-\nence on the score than either of our experimental\nfactors (i.e., Audience and Prompt Engineering).\n4.2 Qualitative Observations\nThe evaluation of the system’s performance reveals\nseveral notable qualities. Firstly, providing context\nand requesting summarization recursively proves\nto be a viable technique for this context. LLMs,\nlike ChatGPT, identify key phrases and reinforce\nthem in their summary. The system incorporates\nentities and topics from the dataset into the gen-\n91\nerated summaries, showcasing its proficiency in\nidentifying relevant concepts.\nHowever, certain aspects remain ambiguous and\nraise intriguing points for discussion. One notable\naspect is the pipeline’s goal-oriented focus on gen-\nerating final summaries. The phrasing used in the\nsummaries strongly implies that all the informa-\ntion provided is intricately connected to the given\ngoal. Consequently, every detail recorded in the\ninteraction log is considered relevant to the process\nof solving the puzzle at hand. This behavior is\nlikely a direct reflection of the task outlined in our\nprompt. In the initial system message to ChatGPT,\nwe explicitly mention that the interaction logs de-\npict someone “trying to investigate an event in the\nintelligence domain.”\nIt is from this perspective that the model oper-\nates, and as a result, the generated summary nat-\nurally strives to establish connections between all\navailable information (i.e., provenance sentences)\nand the specified goal (i.e., summarize the steps\ntaken). The absence of unrelated or misleading\ninformation in the underlying dataset further re-\ninforces the challenge of disambiguating between\nintentional deductions and serendipitous insights.\nWithin the dataset, there are few instances of red\nherrings or other relevant fallacies designed to di-\nvert the analyst’s attention heavily. Consequently,\nwhen reading the generated summaries, it is not\neasy to distinguish between insights that the model\nintentionally identified as relevant behaviors to-\nward the goal and those that were stumbled upon\nserendipitously.\nAnother intriguing observation is the system’s\ntendency to adopt phrasing from prompt engineer-\ning examples, even if it struggles to calculate the\ndescribed pattern accurately. Looking at the out-\nput of summaries where an example is provided\nshows that 9/48 summaries include percentages\nof topics covered. In the 0 Shot (i.e., Baseline)\nsummaries, the inclusion of percentages was never\ngenerated by default and only appears after seeing\nthe structure demonstrated in one of the masked\nprompts. This suggests that the system draws in-\nspiration from provided examples and incorporates\ntheir phrasing into the output, potentially refining\nthe final structure.\nStill, despite the seeming agency to control the\noutput’s phrasing, the percentages and values are\nincorrect. Even when the percentages provided\nby the manually generated example are accurate,\nthe returned output generates its own (incorrect)\nvalue for these phrases. Since transformer models\nare optimized to predict the next word in a phrase,\nthe system appears to rely on identifying relevant\nterms and phrases from the corpus rather than more\npreferred behaviors, like performing deeper statisti-\ncal analysis or ranking different behaviors as more\nrelevant than others.\nIncorporating a statistical determination layer\ninto the preprocessing pipeline could enhance the\nability to identify patterns beyond linear descrip-\ntions. On the other hand, while there are common\nevaluation measures for evaluating summarization,\nwe are unaware of benchmarks that evaluate the\nability of language models to group and consoli-\ndate information by examining the relative seman-\ntic meaning of concepts. Optimization in this direc-\ntion may improve LLMs in the analytic provenance\ncontext and likely many more.\n5 Discussions and Future Work\nIn this work, we explore the factors of audience and\nexample inclusion as a demonstration of applying\nprompt engineering to generate work summaries in\nthe intelligence domain. While we have not found\nother evidence of a methodology where the pro-\nposed pipeline consults a large language model,\nthe pre-processing steps taken on the dataset docu-\nments are inspired by the chain-of-thought prompt-\ning strategies. We use a series of prompts to extract\ninformation from documents and segment an inter-\naction log to build up a complete summary prompt\nand discuss the results.\nOur independent variables are derived from our\npilot study, where users identify essential elements\nof a work summary. Yet, we do not see strong ef-\nfects on baseline summary factuality or accuracy\nwhen adjusting the audience or the inclusion of\nexamples. Instead, in our testing, we observe dif-\nferent important factors. We observe differences\nin summary lengths when we included contextual\ninformation twice. Therefore we use two different\nkinds of ground truth (i.e., baseline and additional)\nto account for this. This leads us to think about\nhow specific wording in the prompt messages may\nnoticeably impact the focus of the output.\nNovel methods may emerge that afford the direct\nmanipulation of prompt wording. For example, it\nwould be interesting to investigate how opposite\nterms, antonymic to the adjectives used in our study,\nmay impact the model’s attention. Additionally, ab-\n92\nlation studies that target the specific adjectives we\nuse may offer fascinating insights into which terms\nmake the biggest difference. Regardless of the tech-\nnique employed, studies exploring the influence of\nindividual terms do not, to our knowledge, have\nconsistent summarization evaluation criteria, thus\ncalling attention to a need for more established\nevaluation methods.\nFinally, corresponding to the chain-of-thought\nnature of the work presented, there are obvious fu-\nture directions that could consider how the prompt-\ning process could involve human users to adjust\nand modify the prompt in real time. It would be\nhelpful to have domain experts rank the summaries\nand use these rankings to fine-tune the prompting\nprocess. Additionally, giving users interface con-\ntrols that manipulate the generated prompt by using\nprompt engineering guidelines could be imagined\nfor future exploration into model behaviors. It is\nalso interesting to consider the downstream tasks\nfrom a work summary and how different generation\nmethods are perceived and may influence future\nwork by human users.\nUltimately, in this work, we observe the feasi-\nbility of generating human-sounding summaries of\nwork from user interaction logs, but they tend to\nlist steps completed without a hierarchical struc-\nture that captures the concepts that are most impor-\ntant or structures the content to flow like a story.\nPerhaps future work could explore how additional\nanalysis layers, prompt engineering interfaces, or\nhuman feedback may help summaries acquire a\nsense of structured storytelling.\n6 Conclusion\nBy harnessing LLMs, researchers can enhance\ntransparency, reproducibility, and collaboration, im-\nproving problem-solving communication. In this\nwork, we showcase the potential of ChatGPT to\ngenerate work summaries from data analysis inter-\naction logs and the associated document contexts.\nBy manipulating prompt engineering features, we\ninvestigate the impact of different prompts on the\nLLM’s output in the intelligence domain. We de-\nvelop a recursive prompt reduction method to han-\ndle token limitations and evaluated prompt exam-\nples and audience types, both quantitatively and\nqualitatively. While we show the potential LLMs\nhave for automating work summaries from prove-\nnance information, we find few consistent impacts\nof these factors on summary accuracy. Instead, we\nrecognize that more reliable prompt engineering\nguidelines will be helpful when developing more\nsophisticated tools to analyze provenance informa-\ntion and control generated output.\nAs has become a common discussion within\nthe research community (Ray, 2023; Maslej et al.,\n2023), the need to better understand these mod-\nels and their impact on society is critical. While\nwhat we demonstrate shows promise for produc-\ntivity increases, there are tradeoffs that come from\nautomation that will impact how we individually\nengage with society. Therefore, we complete this\nwork with a discussion of the various limitations of\nwhat we proposed and the ethical considerations of\nLLM usage in workplace cohesion tasks.\nLimitations\nWe conduct our testing on a single dataset and\namong three users’ interaction histories to examine\nif large language models can be used to make work\nsummaries. The 103 textual documents included in\nthe V AST dataset are small enough that we can con-\nduct and test our summarization pipeline. Since the\ndata context is at a human-comprehensible scale,\nwe can ask for summaries, entity extraction, and\ntopic modeling while also writing gold-standard\nsummaries and verifying the content.\nThe results of the demonstrated technique are\npromising, but additional complications are likely\nto be introduced when applied to larger scales of\ndata. For example, challenges exist where the un-\nderlying document dataset is restricted due to pri-\nvacy concerns (e.g., healthcare records or govern-\nment intelligence) or its temporal dynamism (e.g.,\nsocial media posts or stock market movements).\nCapturing static, secure snapshots of the data an an-\nalyst is working with to conduct our approach will\nrequire additional consideration by the research\ncommunity.\nAlso, while the data context we demonstrate\ncontains some typos and misspellings of names,\nit would be beneficial to explore how this approach\napplies in multilingual contexts. Often intelli-\ngence work deals with content in foreign languages,\nand applying an approach that introduces machine\ntranslation or additional lingual morphologies, will\nsupport the promise of our proposed technique.\nEthics Statement on Broader Impact\nThe emergence of LLMs shows promise for enhanc-\ning bureaucratic activities and enhancing efficiency.\n93\nAs AI technologies advance, we are witnessing\nsignificant shifts in how individuals refer to and\ndiscuss the concepts of artificial intelligence. How-\never, the use of LLMs to automate processes that\ninvolve generating human-like text raises important\nethical considerations pertaining to human work\nand the creation of knowledge. LLMs will funda-\nmentally change how people work, necessitating\nnew skills in editing and engineering results. There\nare unexplored possibilities for extending LLMs’\nimpact on workplace activities and beyond. The\neffort to achieve explainability in LLMs is chal-\nlenging, but the ambition to identify weaknesses,\nbiases, and boundaries is encouraging (Agarwal\net al., 2022).\nUnfortunately, this work does little to mitigate\nthe potential drawbacks of large language models,\nbut we hope to demonstrate a methodology for\nelucidating underlying system behaviors for system\ndesigners who can then improve the models. The\ndata we used in our demonstration was collected\nfor research purposes with individuals’ informed\nconsent that their interactions would be interpreted\nin the future (Mohseni et al., 2018). In our work,\nwe have demonstrated how LLMs can serve as\nan essential lynchpin for novel applications and\nevaluation methodologies.\nIn a broader way, concerns still exist regarding\nthe detection and propagation of harmful and in-\naccurate information by generative models. Our\nexperiments demonstrate the model’s hyperfixation\non the terms provided in the system prompt, which\nleads to assumptions about the goal of the interac-\ntion log’s content and purpose. Behaviors like this\ncompromise the accuracy of reports and ultimately\ncould dissolve user trust.\nApart from improving model accuracy, empha-\nsizing AI literacy is crucial to recognizing technol-\nogy faults and differences. While it is delusional\nto assume that the public will ever deeply under-\nstand the workings of AI tools, the effort by design-\ners to encode best practices into tools and ensure\nsocietally-aligned responsible usage is a necessary\nfirst step. We call attention to these ethical consid-\nerations and promote the responsible use of LLMs\nin generating summaries of individual work.\nReferences\nChirag Agarwal, Eshika Saxena, Satyapriya Krishna,\nMartin Pawelczyk, Nari Johnson, Isha Puri, Marinka\nZitnik, and Himabindu Lakkaraju. 2022. Openxai:\nTowards a transparent evaluation of model explana-\ntions. arXiv preprint arXiv:2206.11104.\nChristopher M. Andrew, Richard J. Aldrich, and Wes-\nley K. Wark, editors. 2019. Secret intelligence: a\nreader, second edition. Routledge.\nIz Beltagy, Arman Cohan, Robert Logan IV , Sewon Min,\nand Sameer Singh. 2022. Zero- and few-shot NLP\nwith pretrained language models. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics: Tutorial Abstracts, pages\n32–37, Dublin, Ireland. Association for Computa-\ntional Linguistics.\nJeremy E. Block, Shaghayegh Esmaeili, Eric D. Ragan,\nJohn R. Goodall, and G. David Richardson. 2023.\nThe influence of visual provenance representations\non strategies in a collaborative hand-off data analysis\nscenario. IEEE Transactions on Visualization and\nComputer Graphics, 29(1):1113–1123.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nFranck Dernoncourt, Mohammad Ghassemi, and Walter\nChang. 2018. A repository of corpora for summariza-\ntion. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nSaadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi,\nand Jianfeng Gao. 2021. Go figure: A meta evalua-\ntion of factuality in summarization. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, page 478–487. Association for Com-\nputational Linguistics.\nJohn Glover, Federico Fancellu, Vasudevan Jagan-\nnathan, Matthew R. Gormley, and Thomas Schaaf.\n2022. Revisiting text decomposition methods for\nnli-based factuality scoring of summaries. In Pro-\nceedings of the 2nd Workshop on Natural Language\nGeneration, Evaluation, and Metrics (GEM) , page\n97–105. Association for Computational Linguistics.\nHaixuan Guo, Shuhan Yuan, and Xintao Wu. 2021. Log-\nbert: Log anomaly detection via bert. In 2021 In-\nternational Joint Conference on Neural Networks\n(IJCNN), pages 1–8.\nSom Gupta and S. K Gupta. 2019. Abstractive summa-\nrization: An overview of the state of the art. Expert\nSystems with Applications, 121:49–65.\n94\nHossein Hamooni, Biplob Debnath, Jianwu Xu, Hui\nZhang, Guofei Jiang, and Abdullah Mueen. 2016.\nLogmine: Fast pattern recognition for log analytics.\nIn Proceedings of the 25th ACM International on\nConference on Information and Knowledge Manage-\nment, CIKM ’16, page 1573–1582, New York, NY ,\nUSA. Association for Computing Machinery.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural Comput. ,\n9(8):1735–1780.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):248:1–248:38.\nHuan Yee Koh, Jiaxin Ju, He Zhang, Ming Liu, and\nShirui Pan. 2022. How far are we from robust long\nabstractive summarization? In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing, page 2682–2698. Association\nfor Computational Linguistics.\nHwanhee Lee, Cheoneum Park, Seunghyun Yoon,\nTrung Bui, Franck Dernoncourt, Juae Kim, and Ky-\nomin Jung. 2022. Factual error correction for abstrac-\ntive summaries using entity retrieval. In Proceedings\nof the 2nd Workshop on Natural Language Genera-\ntion, Evaluation, and Metrics (GEM), page 439–444.\nAssociation for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-\nter West, Ronan Le Bras, Yejin Choi, and Hannaneh\nHajishirzi. 2022. Generated knowledge prompting\nfor commonsense reasoning. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3154–3169, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nShangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow,\nand Yang Liu. 2021. Retrieval-augmented generation\nfor code summarization via hybrid gnn.\nMark M. Lowenthal. 2018. The future of intelligence.\nPolity.\nHeidy M. Marin-Castro and Edgar Tello-Leal. 2021.\nEvent log preprocessing for process mining: A re-\nview. Applied Sciences, 11(22).\nNestor Maslej, Loredana Fattorini, Erik Brynjolfs-\nson, John Etchemendy, Katrina Ligett, Terah Lyons,\nJames Manyika, Helen Ngo, Juan Carlos Niebles,\nVanessa Parli, and et al. 2023. The AI Index 2023\nAnnual Report. AI Index Steering Committee.\nSina Mohseni, Andrew Pachuilo, Ehsanul Haque Nir-\njhar, Rhema Linder, Alyssa M. Pena, and Eric D.\nRagan. 2018. Analytic provenance datasets: A data\nrepository of human analysis activity and interaction\nlogs. CoRR, abs/1801.05076.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with FRANK: A benchmark for\nfactuality metrics. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 4812–4829, Online. As-\nsociation for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting on Association for Computa-\ntional Linguistics, ACL ’02, page 311–318, USA.\nAssociation for Computational Linguistics.\nEric D Ragan, Alex Endert, Jibonananda Sanyal, and\nJian Chen. 2015. Characterizing provenance in\nvisualization and data analysis: an organizational\nframework of provenance types and purposes. IEEE\ntransactions on visualization and computer graphics,\n22(1):31–40.\nPartha Pratim Ray. 2023. Chatgpt: A comprehensive\nreview on background, applications, key challenges,\nbias, ethics, limitations and future scope. Internet of\nThings and Cyber-Physical Systems.\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\ngramming for large language models: Beyond the\nfew-shot paradigm. In Extended Abstracts of the\n2021 CHI Conference on Human Factors in Com-\nputing Systems, CHI EA ’21, New York, NY , USA.\nAssociation for Computing Machinery.\nLeonardo F. R. Ribeiro, Mengwen Liu, Iryna Gurevych,\nMarkus Dreyer, and Mohit Bansal. 2022. FactGraph:\nEvaluating factuality in summarization with semantic\ngraph representations. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3238–3253, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nThibault Sellam, Dipanjan Das, and Ankur P. Parikh.\n2020. Bleurt: Learning robust metrics for text gener-\nation.\nMathew Snover, Bonnie Dorr, Richard Schwartz, John\nMakhoul, Linnea Micciulla, and Ralph Weischedel.\n2005. A study of translation error rate with tar-\ngeted human annotation. Technical report, Technical\nReport LAMP-TR-126, CS-TR-4755, UMIACS-TR-\n2005-58, University of . . . .\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi,\nS. M. Ali Eslami, Oriol Vinyals, and Felix Hill. 2021.\nMultimodal few-shot learning with frozen language\n95\nmodels. In Advances in Neural Information Pro-\ncessing Systems, volume 34, pages 200–212. Curran\nAssociates, Inc.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st International\nConference on Neural Information Processing Sys-\ntems, NIPS’17, page 6000–6010, Red Hook, NY ,\nUSA. Curran Associates Inc.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi\nLan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-\nand-solve prompting: Improving zero-shot chain-of-\nthought reasoning by large language models.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022. Finetuned\nlanguage models are zero-shot learners.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nHanlu Wu, Tengfei Ma, Lingfei Wu, Tariro Manyumwa,\nand Shouling Ji. 2020. Unsupervised reference-free\nsummary quality evaluation via contrastive learning.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3612–3621, Online. Association for Computa-\ntional Linguistics.\nKai Xu, Alvitta Ottley, Conny Walchshofer, Marc Streit,\nRemco Chang, and John Wenskovitch. 2020. Sur-\nvey on the analysis of user interactions and visu-\nalization provenance. Computer Graphics Forum,\n39(3):757–783.\nSeonghyeon Ye, Doyoung Kim, Joel Jang, Joongbo\nShin, and Minjoon Seo. 2023. Guess the instruction!\nflipped learning makes language models stronger\nzero-shot learners. In The Eleventh International\nConference on Learning Representations.\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,\nGeorge Karypis, and Alex Smola. 2023. Multimodal\nchain-of-thought reasoning in language models.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A\nsurvey of large language models.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2023. Large language models are human-level\nprompt engineers.\nA Document Context Creation\nHere we provide the specific preprocessing\nprompts sent to ChatGPT to get the Topics, Entities,\nand summary for each document in the dataset. We\nspecify the length of topics, types of entities, and\nthe number of words to generate short document\ncontexts that include essential information.\nTopics prompt: Act as an intelligence\nanalyst, your task is to determine topics\nthat are being discussed in classified\ndocuments. Determine up to 5 topics in the\ndocument delimited by triple backticks.\nMake each item one to 2 words long.\nFormat your response as “a list of items\nseparated by commas”. Document: <content>\nEntities prompt: Act as an intelligence\nanalyst, your task is to identify\nnamed entities in classified documents.\nThere are 4 entities, which are\n“person, organization, location, and\nmiscellaneous” from CoNLL-2003. Identify\nthe entities in the document delimited by\ntriple backticks. Format your response\nin a JSON format. Document: <content>\nSummary prompt: Act as an intelligence\nanalyst, your task is to generate a\nshort summary of classified documents.\nSummarize the document delimited by\ntriple backticks in at most 100 words.\nDocument:ˋˋˋ<content>ˋˋˋ\nB Segment Summarization\nsegmentation: because there is a token limitation\nfor a single request, we ask the LLM to summa-\nrize the previous interactions and use this shorter\ninteraction history as its memory. This process is\nsimilar to the use case of a chatbot in which an\nLLM summarizes previous conversation and uses\nthat summary as its memory, instead of using the\nentire raw conversation log as the memory\nSystem Prompt: ˋˋAct as an intelligence\nanalyst, your task is to generate a\nsummary of the interaction logs of a\nuser who was trying to investigate an\nevent in the intelligence domain. The\nlogs are written in sentences. The\nentire interaction is divided into 10\nsegments. You will be summarizing the\nentire interaction session step by step\nby summarizing one segment at a time.\nWhen you are summarizing a segment, make\n96\nsure you take into account summaries of\nprevious segments. Please summarize a\nsegment in at most 100 words. The goal is\nto communicate findings and progress in\na collaborative investigation scenario.\nPlease focus on these core features\ndelimited by triple backticks when you\nsummarize:ˋˋˋ<terms for Audience level. See Ap-\npendix Table 1>ˋˋˋ”\nUser Prompt for each segment: “Summarize\nthe sentences describing the interactions\nof segment 1 delimited by triple backticks\nin at most 100 words. Make sure you\ntake into account summaries of previous\nsegments. Description: ˋˋˋ<segment N from\ninteraction sentences generated in preprocessing\nstage>ˋˋˋ”\nC Independent Variables\nBased on the findings of the pilot study, we exam-\nined how an audience may influence summariza-\ntion techniques. Similarly, we wanted to examine\nhow various prompt engineering approaches like\nzero-shot and few-shot may impact summaries in\nour chain-of-thought-inspired approach.\nC.1 Audience\nWe direct the ChatGPT prompt with the terms (see\nAppendix Table 1) derived from the pilot study.\nThese terms appear in the process of generated\nsegments (see Appendix B) and the final prompt\nconstruction (see Appendix Table 2).\nTable 1: Pilot Study Core Features/Terms As iden-\ntified by the user study described in Section 3.1, we\nexplicitly list the terms suggested as core features for\nsummarization. In the pilot study, a discussion about\nsummaries for an individual is not included, so we com-\nbined all the terms for this case.\nAudience Level Suggested Terms\nNone N/A\nSelf\nobjectivity, relevance,\nconciseness, clarity,\nengaging, accuracy,\nproper citation,\ncoherence.\nPeer\nCollaboration engaging, accuracy.\nTeam\nManager\nobjectivity, relevance,\nconciseness, clarity.\nC.2 Examples\nWe also systematically vary the inclusion of an\nexample in the Final User message . Below are\nexamples of the content sent to ChatGPT for the\nfirst interaction log. These examples would be\ncustomized for each user session.\nNone: N/A; like a zero-shot approach.\nManual example : “Please provide the\noverall summary based on the example\ndelimited by triple backticks. Example:\nˋˋˋThis session began by searching for\nthe word \"Nigeria\" and looking at the\ndocuments returned. They noted that\nDr. George and Mikhail emailed and then\ntransitioned to searches about \"Kenya\"\nand the Middle East. At this time,\nthey were reviewing people like Leonid\nMinsky and Anna Nicole Smith. By the\nend of the session, they had transitioned\nto exploring documents from Russia and\nmiddle eastern countries. They searched\nfor \"death,\" \"kasem\" and \"dubai.\" In the\nend, they returned to some of the same\ndocuments they had opened at the beginning\nbut also opened many different documents\nfor the first time. Out of the 46 topics\nand 102 documents, they reviewed 39\ntopics, opened 45% of the total documents\nat least once, and spent an average of 30\nseconds with each document. The people\nthey returned to most frequently were\nLeonid Minsky, Mikhail Dombrovski, and\nDr. George.ˋˋˋ”\nMasked manual example: “Please provide\nthe overall summary based on the\nexample delimited by triple backticks.\nExample: ˋˋˋThis session began by\nsearching for [KEYWORD1] and looking\nat the documents returned. They\nnoted that [KEYWORD2] and [KEYWORD3]\nemailed and then transitioned to searches\nabout [KEYWORD4] and [KEYWORD5]. At\nthis time, they were reviewing people\nlike [KEYWORD6] and [KEYWORD7]. By\nthe end of the session, they had\ntransitioned to exploring documents\nfrom [KEYWORD8] and [KEYWORD9]. They\nsearched for [[KEYWORD10], [[KEYWORD11]\nand [KEYWORD12]. In the end, they\nreturned to some of the same documents\nthey had opened at the beginning but also\n97\nopened many different documents for the\nfirst time. Out of the [NUMBER] topics\nand [NUMBER] documents, they reviewed\n[NUMBER] topics, opened [NUMBER]% of the\ntotal documents at least once, and spent\nan average of [NUMBER]\"ˋˋˋ\nMasked template : “Please provide the\noverall summary using the template\ndelimited by triple backticks. Example:\nˋˋˋThey focused on [NUMBER] main topics\nin this analysis session, exploring\n[PERCENTAGE] of the documents. The\ntopics that received the most attention\nwere [TOPICS]. They started searching\nfor [KEYWORD1], before transitioning\nto [KEYWORD2] and finally looking\nfor [KEYWORD3]. They conducted\nNUMBER searches throughout their session.\n[CONCLUSION]ˋˋˋ”\nD Ground Truth Descriptions\nWe used three different ground truths as an evalu-\nation standard and tweaked the process based on\ntwo different independent variables. The first is the\nManual summary seen in Appendix C.2. This is\ncustom for each user’s session and contains accu-\nrate and factual information written by one author.\nThe Baseline summary was generated by Chat-\nGPT without any additional prompting. This means\nthere were no specifications about an audience or\nexample provided.\nThe Additional summary was also generated by\nChatGPT but simply had the segment messages\nrepeated in the final prompt. By repeating the user\nand system messages in the final prompt, we no-\nticed the summary was shorter, which could influ-\nence accuracy calculations.\n98\nTable 2: Final Prompt Construction The final prompt to ChatGPT is generated from the variations shown in this\ntable. Each accuracy experiment designates some vertical combination of the following strings of text, choosing one\naudience level and one example level (4 x 4). This final prompt combines with all the prepended messages that\ncontain the initial system message as well as the pairs of user and assistant segmentation summaries.\nPlease provide a comprehensive summary of the entire interaction based on the summaries of\nuser.numSegmentssegments in at most finalLength.\nAudience\nNone Self Peer Manager\nN/A Please avoid\nbeing too\nvague and\noverly de-\ntailed.\nYour audience will be a peer who is\nmore comfortable working with team\nmembers’ uncertainty and hedged\nstatements. More specifically, you\nshould follow a list of instructions\ndelimited by triple backticks. Instruc-\ntions:\n1. Provide the context of the analysis\nby offering starting points and pro-\nviding more details later.\n2. Being entirely objective is less im-\nportant for peer collaboration than be-\ning accurate or relevant to their peers.\n3. Including the opinions of the au-\nthor in their summary can provide\ncontextual data (e.g., hedge state-\nments or other personal theories)\nabout the state of the investigation.\n4. Please avoid being too vague and\noverly detailed.\nYour audience will be a manager who\nexpects to see summaries with a high\ninformation density in each sentence\nand still provide context for the in-\nvestigation without offering too many\ndetails to invite the manager to do the\ntask themselves. More specifically,\nyou should follow a list of instruc-\ntions delimited by triple backticks.\nInstructions:\n1. Should not focus on the specific\nstatistics but focus on the general be-\nhaviors.\n2. Please provide a sense of how\nmuch work was completed.\n3. Please use more descriptive lan-\nguage.\n4. Please avoid being too vague and\noverly detailed.\nExample\nNone Manual Masked Template\nN/A Human-\nGenerated\nGround\nTruth\nHuman-Generated Ground Truth but\nnouns replaced with masks (e.g.,\n[number], [topic], [percentage], etc.)\nGeneric summary template for any\nsummary. All values are masked.\n99",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9283519983291626
    },
    {
      "name": "Computer science",
      "score": 0.8443554639816284
    },
    {
      "name": "Security token",
      "score": 0.5971594452857971
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48899972438812256
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.44995173811912537
    },
    {
      "name": "Natural language processing",
      "score": 0.4491368532180786
    },
    {
      "name": "Sequence (biology)",
      "score": 0.42920178174972534
    },
    {
      "name": "Feature engineering",
      "score": 0.42886143922805786
    },
    {
      "name": "Information retrieval",
      "score": 0.34784752130508423
    },
    {
      "name": "Human–computer interaction",
      "score": 0.33456823229789734
    },
    {
      "name": "Deep learning",
      "score": 0.17172574996948242
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ]
}