{
  "title": "Crowd Transformer Network",
  "url": "https://openalex.org/W2929151611",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4281801300",
      "name": "Ranjan, Viresh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743042772",
      "name": "Shah, Mubarak",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753011071",
      "name": "Nguyen Minh Hoai",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2097073572",
    "https://openalex.org/W2798781811",
    "https://openalex.org/W2950826633",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W1976959044",
    "https://openalex.org/W2511730936",
    "https://openalex.org/W2145983039",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W2541389513",
    "https://openalex.org/W2072232009",
    "https://openalex.org/W2952888378",
    "https://openalex.org/W2949162858"
  ],
  "abstract": "In this paper, we tackle the problem of Crowd Counting, and present a crowd density estimation based approach for obtaining the crowd count. Most of the existing crowd counting approaches rely on local features for estimating the crowd density map. In this work, we investigate the usefulness of combining local with non-local features for crowd counting. We use convolution layers for extracting local features, and a type of self-attention mechanism for extracting non-local features. We combine the local and the non-local features, and use it for estimating crowd density map. We conduct experiments on three publicly available Crowd Counting datasets, and achieve significant improvement over the previous approaches.",
  "full_text": "Crowd Transformer Network\nViresh Ranjan\nDept. of Computer Science\nStony Brook University\nMubarak Shah\nDept. of Computer Science\nUniversity of Central Florida\nMinh Hoai Nguyen\nDept. of Computer Science\nStony Brook University\nAbstract\nIn this paper, we tackle the problem of Crowd Counting, and present a crowd density estimation based approach for\nobtaining the crowd count. Most of the existing crowd counting approaches rely on local features for estimating the crowd\ndensity map. In this work, we investigate the usefulness of combining local with non-local features for crowd counting. We\nuse convolution layers for extracting local features, and a type of self-attention mechanism for extracting non-local features.\nWe combine the local and the non-local features, and use it for estimating crowd density map. We conduct experiments on\nthree publicly available Crowd Counting datasets, and achieve signiﬁcant improvement over the previous approaches.\n1. Introduction\nGiven an image of a crowd, Crowd Counting, as the name suggests, aims at estimating the total number of people in the\nimage. Crowd Counting has applications in various domains such as journalism, trafﬁc monitoring, video surveillance, and\npublic safety. In recent years, annotated datasets with extremely dense crowd images have been collected [13, 30], and a\nnumber of techniques have been proposed for estimating crowd counts on these challenging datasets. Most recent Crowd\nCounting approaches [22, 23, 25, 26, 30] estimate a crowd density map, and sum across all the pixels in the crowd density\nmap to obtain the crowd count. A typical crowd counting approach, shown in 1, uses a fully convolutional network to map\nthe crowd image into its corresponding crowd density map.\nHowever, the existing convolutional neural network architectures have some limitations for estimating a crowd density\nmap. The convolution operations performed by a neuron in a convolution layer are local, and cannot capture long range\ndependencies outside the receptive ﬁeld of the neuron. Most neural networks designed for classiﬁcation tasks such as [9, 14,\n24] have a few fully connected layers at the end, and each neuron in a fully connected layer has access to the convolutional\nfeature map from the entire image, and hence, can effectively extract non-local information from the convolutional feature\nmaps. Such non-local information is important for making global decisions pertaining to the image, such as predicting the\nimage label. However, the fully convolutional networks typically used for Crowd Counting do not have any fully connected\nlayer to extract non-local features. One possible strategy to extract non-local information in such fully convolutional networks\nwould be to make the networks very deep, so that the convolution ﬁlters in the higher layers have a large receptive ﬁeld.\nHowever, training such very deep networks could pose optimization difﬁculties [28]. One can also use fully connected\nlayers, but fully connected layers do not preserve locality, while locality is very important for crowd density estimation at\nevery location.\nOur work in this paper is inspired by the Non-local Neural Networks [27, 28] that use an attention strategy for capturing\nlong range dependencies without resorting to using very deep convolution layers or fully connected layers. Using the self-\nattention mechanism for extracting non-local information, we propose an end to end approach which combines local and\nnon-local information for estimating crowd density maps. Note that a few Crowd Counting approaches [26] do make use\nof the non-local information for Crowd Counting. However, they present a multi-stage approach which does not use self-\nattention, and is very different from our proposed approach. Sindagi and Patel [26] trained a classiﬁer to classify image\npatches into different density levels, and used the classiﬁer for extracting non-local information. However, their approach\nis multi-stage and also requires one to set the different density levels which introduces multiple hyper-parameters into the\npipeline.\nIn this paper, we present an efﬁcient strategy for incorporating non-local information to estimate better quality crowd\n1\narXiv:1904.02774v1  [cs.CV]  4 Apr 2019\nFigure 1. CNN based crowd density estimation:Most recent CNN based approaches used a fully convolutional CNN architecture to\nmap a crowd image to the density map, and the crowd count is obtained by summing across all the pixels in the predicted map. These\napproaches take into account the local information while estimating the crowd density map.\ndensity maps. Inspired by the Transformer [27] and Non-local Neural Network [28], we present Crowd Transformer Net-\nwork which maps a crowd image to its corresponding density map while taking into account the local as well as non-local\ninformation.\nOur key contributions are as follows:\n1. We present the ﬁrst end-to-end approach for crowd counting which combines local and non-local information for crowd\ndensity estimation.\n2. Our proposed approach achieves state-of-art performance on three challenging crowd counting datasets and reduces\nthe mean absolute error by 20 % and 15 % on UCF QNRF [13] and UCF CC [12] datasets.\n3. We propose a novel self-attention mechanism, Contextual Multi-Head Attention, which leads to improvement in the\nquality of the crowd density maps in comparison to the existing self-attention mechanism [27].\n2. Related Work\nIn this section, we ﬁrst describe different types of crowd counting approaches, and then discuss the recent CNN based\ndensity estimation approaches in more detail. We also describe some of the relevant self-attention mechanisms.\nDetection and Regression Based Crowd Counting Approaches.Some of the earliest crowd counting approaches were\ndetection based [16, 18] where the approaches used classiﬁer based detectors on top of hand crafted feature representation.\nLin et al. [18] trained SVM classiﬁer based detectors on top of Haar features for detecting head. Li et al. [16] proposed to\nﬁrst segment the image into crowd and non-crowd regions, and then use a head detector to identify individuals in the crowd\nregion. However, these detection-based approaches are severely affected by occlusion, and perform poorly in case of dense\ncrowd images. Some approaches [7, 8] proposed to circumvent the harder detection problem, and obtain the crowd count by\nlearning a regression function to map image patches to the count in that patch. These regression based approaches also relied\non hand crafted features.\nDensity Estimation Based Crowd Counting Approaches.Lempitsky and Zisserman [15] proposed a density estimating\nbased approach for crowd counting where they learned a linear mapping between crowd patches and the crowd density map.\nPham et al. [21] extended the density estimation approach by using a random decision forest for learning the mapping.\nHowever, these approaches also use hand crafted features, and do not perform well on the more recent crowd counting\ndatasets [30]. More recent approaches of the deep learning era [6, 13, 17, 20, 22, 23, 26, 30] extended the earlier density\nestimation approaches, and used CNNs for learning the mapping between a crowd image and the corresponding density map.\nZhang et al. [30] pointed out the large variation in crowd densities across different crowd images, and proposed a multi-\ncolumn architecture where the different columns had ﬁlters of varying sizes. The branch with larger kernels could help in\nextracting features relevant for low density crowd while branch with ﬁner kernel could focus on high density crowd regions.\n2\nThe features from the parallel branches were combined and passed through a 1 ×1 convolution layer to predict the crowd\ndensity map. One shortcoming of the multi-column approach was that each image, irrespective of its underlying density,\nwas processed by all the columns. Sam et al. [23] proposed to train separate CNN-based density regressors for different\ndensity types. They also trained a switch classiﬁer which routed an image patch to the appropriate regressor. Sindagi and\nPatel [26] presented an approach which incorporated non-local information for crowd density estimation. They trained an\nimage/patch level classiﬁer for classifying the image/patch into ﬁve different density categories. The classiﬁer prediction was\nused to create context maps which was used along with convolutional features for estimating the crowd density map. Ranjan\net al. [22] presented an iterative strategy for crowd density estimation, where a high resolution density map was predicted\nin two stages. In the ﬁrst stage, a low resolution density map was estimated, which was used to guide the second stage\nhigh resolution prediction. The authors also present a multi-stage generalization of their two stage approach. Lu et al. [19]\npresented a class agnostic counting approach, where counting was posed as a matching problem. The authors showed their\napproach worked not just for human crowd counting, but also for counting objects such as cells and cars.\nSelf-attention. Self-attention is an attention mechanism which transforms an input sequence of vectors into another se-\nquence of vectors, where each vector in the output sequence is obtained by relating the corresponding vector in the input\nsequence with the entire input sequence [27]. For handling sequence-to-sequence tasks, Vaswani et al. [27] presented the\nTransformer architecture which was based entirely on parametric attention modules, and not the usual recurrent neural net-\nwork architecture. Transformer architecture was initially proposed for handling sequence-to-sequence tasks in the Natural\nLanguage Processing domain. Wang et al. [28] proposed Non-local Neural Networks, which also used self-attention, sim-\nilar to the Transformer architecture, to extract non-local feature representations. The authors showed that these non-local\nneural features helped in capturing long range dependencies between features at different spatial/temporal locations which\nled to improved results for various static image and video classiﬁcation tasks. The authors also showed how the non-local\noperations related to the classic non-local means approach [5].\n3. Proposed Approach\n3.1. Background\nTransformer architecture [27] was proposed as an alternative to Recurrent Neural Networks for solving various sequence\nto sequence tasks in the NLP domain. The architecture consists of an encoder and a decoder where the encoder maps\nthe input sequence into an intermediate representation, which in turn is mapped by the decoder into the output sequence.\nThe transformer uses three types of attention layers: encoder self-attention, encoder-decoder attention , and decoder self-\nattention. For the proposed crowd counting approach in this paper, only the ﬁrst one is relevant which we describe brieﬂy\nnext. Henceforth we will use self-attention to refer to the self-attention of the encoder.\nEncoder Self-Attention: Given a query sequence along with a key-and-value sequence, the self-attention layer outputs a\nsequence where the i-th element in the output sequence is obtained as a weighted average of the value sequence, and the\nweights are decided based on the similarity between the i-th query element and key sequence. Let X ∈Rn×d be a matrix\nrepresentation of a sequence consisting of nvectors of ddimensions. The self-attention layer ﬁrst transforms X into query\n(XQ), key (XK) and value (XV ) matrices by multiplying X with matrices WQ, WK, and WV :\nXQ = XWQ, (1)\nXK = XWK, (2)\nXV = XWV . (3)\nThe output sequence Zis computed efﬁciently by doing two matrix multiplications:\nZ = softmax(XQXT\nK)XV (4)\nThe encoder consists of multiple self-attention layers arranged in a sequential order so that the output of one self-attention\nlayer is fed as input to the next self-attention layer.\nMulti-Head Attention: Instead of a using a single attention head as described above, Vaswani et al. [27] suggest to use h\nparallel heads, each with its own set of projection matrices. Each projection matrix is of size d×dk and hdk = d. The\noutputs from the hheads are concatenated, and the resulting matrix is transformed by multiplying with another matrix of size\nd×d.\n3\nFigure 2. CTN architecturecombines local and non-local features for crowd density estimation. The local features are computed by the\nconvolution layers in the local feature block. The resulting feature map(shown in green) is passed to the non-local feature block. Non-local\nfeature block uses a novel self-attention mechanism to compute the non-local features(shown in black). Density Prediction Head combines\nthe local and non-local features, and predicts the crowd density map.\n3.2. Crowd Transformer Network\nWe present the block diagram of Crowd Transformer Network (CTN) in Figure 2. CTN uses both local and non-local\nfeatures for estimating the crowd density map. Next, we will describe the Local Feature Block that is used for extracting\nlocal features. The Non-local Feature Block, which uses self-attention for extracting non-local features, will be described in\nSection 3.2.2. In Section 3.2.3, we will describe the density prediction head, which combines the local and non-local features\nand predicts a high resolution crowd density map. For non-local feature extraction, we propose a novel attention layer called\nContextual Multi-Head Attention, which is described in Section 3.3.\n3.2.1 Local Feature Block\nGiven an input image X of size H ×W, we pass it through the local feature block to obtain the convolutional feature\nmaps. The local feature block consists of ﬁve convolution layers with kernels of size 3 ×3, and the number of ﬁlters in\nthe convolution layers are 64, 64, 128, 128 and 256. We use the VGG-16 network [24] trained on the Imagenet dataset to\ninitialize the convolution layers in local feature block. The local feature block has two max pooling layers, after the second\nand fourth convolution layer. The resulting feature map is a tensor of size H\n4 ×W\n4 ×256. The feature map is passed to\nnon-local block as well as the density prediction head.\n3.2.2 Non-local Feature Block\nThe non-local feature block takes as input the feature map from the local feature block, and passes it through 3 convolution\nlayers of kernel size 3×3 and a max pooling layer, which results in a feature map of sizeH\n8 ×W\n8 ×512. We reduce the depth\nof the feature map by passing it through a 1 ×1 convolution layer, which yield a feature map of size H\n8 ×W\n8 ×240. The\nresulting feature map is ﬂattened into a matrix of sizeM×240, where M = H\n8 ×W\n8 . Each row in this matrix corresponds to\nsome location in the convolution feature map. The ﬂattened matrix is passed through three self-attention layers. The output\nfrom ﬁnal transformer layer is reshaped back into a tensor of size H\n8 ×W\n8 ×240.\nThe multi-head attention layer, while transforming the input sequence into the query, key and value sequences(as de-\nscribed in Section 3.1), does not utilize any context information. We present a novel multi-head attention mechanism, called\nContextual Multi-Head Attention, that uses context information while transforming an input sequence. We will describe the\nContextual Multi-Head Attention layer in Section 3.3\n3.2.3 Density Prediction Head\nBoth local and non-local features are important for estimating an accurate crowd density map. Hence, the Density Prediction\nHead uses a skip connection to obtain the convolutional features from the local feature block, and combines it with the\nfeatures from the non-local feature block. The non-local features are upsampled to the same spatial size as local features,\n4\nwhich results in a tensor of size H\n4 ×W\n4 ×240. The local and non-local features are concatenated and passed through 4\nconvolution layers (with 196, 128, 48, and 1 ﬁlters), where the last layer is a 1 ×1 convolution layer. We add a ReLU\nnon-linearity after the 1 ×1 convolution layer to prevent the network from predicting negative density. We use two bilinear\ninterpolation layers, after the second and third convolution layers in the prediction head. Each of the interpolation layer\nupsamples its input to twice its size. The input to the ﬁnal 1 ×1 convolution layer is a feature map of size H ×W ×48,\nwhich is transformed into a 2D map by the last convolution layer.\n3.3. Contextual Multi-Head Attention\nAs described in Section 3.1, the self-attention layer in the Transformer architecture [27] ﬁrst transforms the input sequence\ninto query, key, and value sequences by multiplying each vector in the input sequence with a weight matrix before applying\nthe attention mechanism. Let xi the i-th vector in matrix X. X here is the input to the self-attention layer. We also look at\nthe immediate neighborhood of xi, i.e. xi−1 and xi+1, while transforming xi into xQ\ni , xK\ni , xV\ni :\nxQ\ni = concat(xi−1,xi,xi+1)WQ, (5)\nxK\ni = concat(xi−1,xi,xi+1)WK, (6)\nxV\ni = concat(xi−1,xi,xi+1)WV , (7)\nwhere concat(·,·,·) is a function that output the concatenated vector for the input vectors. The above equation can be\nimplemented efﬁciently by using a 1D convolution layer:\nXQ = f1D−Conv (X,θQ), (8)\nXK = f1D−Conv (X,θK), (9)\nXV = f1D−Conv (X,θV ), (10)\nwhere f1D−Conv (X,θ refers to the 1D convolution layer with parameter θ and X is the input sequence. We pad the input\nsequence with zero vectors to handle the boundary cases. The query, key, and value sequences are transformed into the output\nsequence: Z = softmax(XQXT\nK)XV . Similar to the Transformer architecture [27], we use hparallel attention heads and\nconcatenate the h resulting sequences into a single sequence, which is transformed by another 1D convolution layer into\nthe output sequence. For our experiments, we use 3 such contextual multi-head attention layers for extracting the non-local\nfeatures.\nInstead of depending on just the immediate neighbors of xi, we can use a larger context. Below we describe the equation\nto transform xi using 2mcontext vectors:\nxQ\ni = concat(xi−m,...,x i−1,xi,xi+1,...,x i+m)WQ, (11)\nxK\ni = concat(xi−m,...,x i−1,xi,xi+1,...,x i+m)WK, (12)\nxV\ni = concat(xi−m,...,x i−1,xi,xi+1,...,x i+m)WV , (13)\nwhere concat(·,·,·) is a function that output the concatenated vector for the input vectors. Note that the size of the transfor-\nmation matrices depend upon the context, the input dimensionality and the dimensionality of the resulting vector. The above\nequation can be implemented efﬁciently by using a 1D convolution layer:\nXQ = f1D−Conv (X,θQ), (14)\nXK = f1D−Conv (X,θK), (15)\nXV = f1D−Conv (X,θV ), (16)\nwhere f1D−Conv (X,θ) refers to the 1D convolution layer with parameters θand X is the input sequence.\nFor our experiments, we set the number of parallel attention heads hto 12.\n4. Experiments\nWe conduct experiments on three challenging datasets: UCF-QNRF [13], Shanghaitech [30], and UCF Crowd Count-\ning [12].\n5\nMethod MAE RMSE\nIdrees et al. [12] 315 508\nMCNN [30] 277 426\nEncoder-Decoder [3] 270 478\nCMTL [25] 252 514\nSwitch CNN [1] 228 445\nResnet101 [10] 190 277\nDensenet201 [11] 163 226\nComposition Loss-CNN [13] 132 191\nCTN (Proposed) 102.6 177.7\nTable 1.Performance of various methods on the UCF-QNRF dataset. We compare our proposed approach with the previous approaches\nin terms of MAE and RMSE metrics. Training is done on random squared crops of size 384. The proposed approach outperforms all other\napproaches by a large margin in terms of both the MAE and the RMSE metrics. Note that we run the experiment twice, and report the\naverage MAE and RMSE values for CTN. Numbers corresponding to CTN are highlighted, and the numbers from the best approach are\nunderlined.\n4.1. Evaluation Metrics\nFollowing the previous Crowd Counting works, we compare the proposed approach to the existing approaches in terms of\nMean Absolute Error (MAE) and Root Mean Squared Error (RMSE). If the predicted count for an image is ˆyand the ground\ntruth count is y, the MAE and RMSE can be computed as:\nMAE = 1\nn\n∑n\ni=1|yi −ˆyi|, (17)\nRMSE =\n√\n1\nn\n∑n\ni=1(yi −ˆyi)2, (18)\nwhere the summation is done over the test set.\n4.2. Results on the UCF-QNRF dataset\nUCF-QNRF [13] is the largest annotated crowd counting dataset with 1535 crowd images and 1.2 million annotations.\nImages in the dataset were collected by doing image search on Google and Flickr search engines and from Hajj footage. The\ndataset is divided into 12,01 training and 334 test images. Crowd count across the images varies between 49 and 12,865.\nEach person in the dataset is annotated with a single dot annotation, and this dot annotation map is convolved with a Gaussian\nto obtain the target density map for training the crowd density estimation network.\nWe ﬁrst re-scale those images in the dataset where the larger dimension is greater than 1920. We preserve the original\naspect ratio while scaling the images. For training, we take 100 random crops of size 384 ×384 from each of the training\nimages. We use a batchsize of 3 and train our model for 10 epochs. We use a learning rate of 10−4 and Adam optimizer\nfor training our model. We normalize the training and test images using the mean and variance values computed on the\nImageNet dataset. We initialize the ﬁve convolutional layers in the local feature block by using the ﬁrst 5 layers from a Vgg\nNet [24] trained on the ImageNet dataset. We initialize the ﬁrst convolution layers in the non-local block (except the 1 ×1\nconvolution layer and the 1D convolution layer in the Contextual Multi-Head Attention layer) using the pretrained layers\nfrom Vgg16. The remaining learnable parameters are initialized randomly using a Gaussian distribution with zero mean and\nstandard deviation of 0.001. Since we perform the evaluation on a single GPU, we run into memory issue for larger test\nimages. For such images, we divide the image into non-overlapping crops, obtain the density map for each crop, and sum the\ncount across all the crops to obtain the overall count. In Table 1, we compare CTN with the previous state-of-art methods in\nterms of MAE and RMSE metrics. CTN outperforms all previous methods by a large margin in terms of both the MAE and\nthe RMSE metrics.\n4.3. Ablation Study on UCF QNRF\nIn Table 2, we show the results for ablation study conducted on the UCF-QNRF dataset [13]. We analyze the importance\nof the key components of the proposed CTN architecture: 1) local features, 2) non-local features, and 3) Contextual Multi-\nHead Attention. We observe that removing either the local or the non-local feature results in a drastic drop in performance.\nThis observation shows that both the local and the non-local features are needed for predicting accurate density map. We also\n6\nMethod MAE RMSE\nLocal features 120.2 218.4\nNon-local features 123.5 206.7\nCTN, but with Multi-Head Attention 108.3 190.8\nCTN (proposed) 102.6 177.7\nTable 2. Ablation study on the UCF-QNRF dataset [13]. The Local features approach does not use the non-local feature block and\nuses only the local features for estimating the crowd count. The Non-local features approach removes the skip connection from the CTN\narchitecture, i.e., the prediction head uses only the non-local features. The third method isCTN, but with Multi-Head Attention mechanism\nproposed by Vaswani et al. [27]; CTN refers to the proposed architecture. Local features and Non-local features approaches perform\npoorly in comparison to other two approaches. This suggests that both local and non-local features are important for accurate crowd\ndensity estimation. CTN outperforms CTN + Multi-Head Attention which shows the beneﬁts of using the contextual attention mechanism.\nContext MAE RMSE\n0 108.3 190.8\n2 105.7 184.5\n4 104.0 183.0\n6 102.6 177.7\n10 103.0 176.0\nTable 3. Effect of varying Context in the CTN arhictecture on UCF-QNRF dataset [13]:In this table, we vary the context information\nused in our proposed Contextual Multi-Head Attention layer. Context refers to the 2m context vectors in Equations 1 - 3 . Context of\n0 corresponds to the attention mechanism proposed by Vaswani et al. [27] for which only the vector at current location is taken into\nconsideration while transforming the sequence. Context of 2 are the CTN results presented in the main paper. In terms of MAE metric,\nperformance keeps improving as we increase the context from 0 to 6.\nobserve that using Multi-Head Attention [27], instead of using the proposed Contextual Multi-Head Attention mechanism,\nleads to worse results.\n4.4. Effect of Varying Context on UCF-QNRF dataset\nTo analyze the effects of varying the context information, we conduct experiments on the UCF-QNRF dataset [13]. We\nvary the context,i.e. the number of adjacent vectors in Equations1-3 in the supplementary submission, and analyze the impact\non the quality of the estimated crowd density map. In Table 3, we present the results on UCF-QNRF [13]. The context 0\nrefers to the original Multi-Head Attention mechanism proposed by Vaswaniet al. [27]. Context 2 are Contextual Multi-Head\nAttention results presented in the main paper. From the table, we can see that using more context helps. We see that using a\ncontext of 6 signiﬁcantly outperforms the attention mechanism presented in the Transformer architecture [27]. Furthermore\nusing a context of 6 also leads to improvements over using a context of 2 from the main paper. We do not experiment with\ncontext larger than 10 since the performance plateaus for context larger than 6.\n4.5. Results on UCF-CC dataset\nThe UCF Crowd Counting dataset [12] contains 50 crowd images with widely varying crowd count. The images are\ncollected via web search. Each person in the dataset is annotated with a single dot annotation, and this dot annotation map\nis convolved with a Gaussian to obtain the target density map for training. The maximum, minimum and average counts\nare 94, 4545 and 1280 respectively. Since the dataset is small in comparison to the UCF-QNRF dataset, we use the CTN\nnetwork trained on UCF-QNRF to initialize the model. We train the network using random crops of sizesH\n3 ×W\n3 . Following\nthe evaluation protocol in earlier works [30], we perform ﬁve-fold cross validation and report the average MAE and RMSE\nresults in Table 4. The proposed approach outperforms all other approaches by a large margin in terms of MAE metric. In\nterms of RMSE metric, CTN outperforms all the methods except CP-CNN [26].\n4.6. Results on Shanghaitech\nThe Shanghaitech crowd counting dataset contains of two parts. Part-A consists of 482 images collected from the web,\nand Part B consists of 716 images collected on the streets of Shanghai. Images for Part A are typically denser than Part B\n7\nMethod MAE RMSE\nLempitsky & Zisserman [15] 493.4 487.1\nIdrees et. al [12] 419.5 487.1\nCrowd CNN [29] 467.0 498.5\nCrowdnet [4] 452.5 -\nMCNN [30] 377.6 509.1\nHydra2s [20] 333.7 425.6\nSwitch CNN [23] 318.1 439.2\nCP-CNN [26] 295.8 320.9\nIG-CNN [2] 291.4 349.4\nic-CNN [22] 260.9 365.5\nSANet [6] 258.4 334.9\nCSR Net [17] 266.1 397.5\nCTN(Proposed) 219.3 331.0\nTable 4. Performance of various methods on the UCF Crowd Counting dataset.We compare our proposed approach with the previous\napproaches in terms of MAE and RMSE metrics. We perform ﬁve fold cross validation and report the average across the ﬁve runs. Training\nis done on random crops of size H\n3 × W\n3 . The proposed CTN outperforms all other approaches by a large margin in terms of MAE metric.\nIn terms of RMSE metric, our proposed approach outperforms all the methods except CP-CNN [26]. Numbers corresponding to CTN are\nhighlighted, and the numbers from the best approach are underlined.\nPart A Part B\nMAE RMSE MAE RMSE\nCrowd CNN [29] 181.8 277.7 32.0 49.8\nMCNN [30] 110.2 173.2 26.4 41.3\nSwitching CNN [23] 90.4 135.0 21.6 33.4\nCP-CNN [26] 73.6 106.4 20.1 30.1\nIG-CNN [2] 72.5 118.2 13.6 21.1\nic-CNN [22] 68.5 116.2 10.7 16.0\nSANet [6] 67.0 104.5 8.4 13.6\nCSR Net [17] 68.2 115.0 10.6 16.0\nCTN(Proposed) 64.3 107.0 8.6 14.6\nTable 5. Count errors of different methods on the Shanghaitech dataset.This dataset has two parts: A and B. Images in Part A are\ncollected from the web, while those in Part B are collected on the streets of Shanghai. Part A images are typically more dense than Part B\nimages. The average error on Part B is less compared to that in Part A . We compare our proposed approach with the previous approaches\nin terms of MAE and RMSE metrics. Training is done on random crops of size H\n3 × W\n3 . Numbers corresponding to CTN are highlighted,\nand the numbers from the best approach are underlined.\nimages. Each person in the dataset is annotated with a single dot annotation, and this dot annotation map is convolved with\na Gaussian to obtain the target density map for training. Since the dataset is small in comparison to the UCF-QNRF dataset,\nwe use the CTN network trained on UCF-QNRF for initializing the model. We train the network using random crops of sizes\nH\n3 ×W\n3 .\nIn Table 5, we compare the proposed approach with the previous approaches in terms of MAE and RMSE metrics. CTN\noutperforms all the approaches approaches except SANet [6] on both Part-A and Part-B datasets. On Part A data, CTN\noutperforms SANet in terms of MAE metric.\n4.7. Qualitative Results\nIn Figure 3, we show a few qualitative results obtained using our proposed approach. The three columns show the input\nimage, ground truth annotation map, and the predicted density map. The ﬁrst three rows are success cases for CTN, while the\nlast two are failure cases. Among the results shown, the error is the largest for the last crowd image which is the most dense\n8\nimage among the ones shown.\n4.8. Local Vs Non-local Features\nIn this section, we do a qualitative analysis of the beneﬁts of combining non-local features with local features for crowd\ndensity estimation. We compare the proposed CTN approach, which combines local and non-local features, with a local\napproach, which uses only the convolutional features for estimating the crowd density map. For the local baseline, we\nremove the non-local block from the CTN architecture. We show some qualitative results on UCF-QNRF [13] dataset in\nFigure 4.\nWe compare the two approaches and present few success and failure cases. We show the crowd image, corresponding\nground truth density map and the predicted crowd density map obtained via both of the approaches. The ﬁrst three examples\nare success cases for CTN, while the last two are failure cases. For majority of examples shown in the ﬁgure, predictions\nobtained using the proposed CTN approach are more accurate than those obtained by the local approach. For the ﬁrst\nexample, the density map from the local approach contains false positives towards the top while the CTN prediction contains\nsigniﬁcantly less false positives. The second and third examples comprise of dense crowd, where the people in the crowd\noccupy a small number of pixels. CTN signiﬁcantly outperforms the local approach in this case. This suggests that the\nnon-local features help in better handling very dense crowd images. The last two examples are failure cases for both of the\napproaches.\n5. Conclusion\nWe have presented an end-to-end approach for Crowd Counting which combines local and non-local features for esti-\nmating crowd density maps. We made use of self-attention mechanism to compute the non-local features. We proposed a\nnovel self-attention approach, which helped in obtaining more accurate crowd density maps. We showed the usefulness of\ncombining local with non-local information for Crowd Counting by surpassing the performance of previous state-of-the-art\napproaches.\nReferences\n[1] D. Babu Sam, S. Surya, and R. Venkatesh Babu. Switching convolutional neural network for crowd counting. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2017. 6\n[2] D. Babu Sam, N. N. Sajjan, R. Venkatesh Babu, and M. Srinivasan. Divide and grow: capturing huge diversity in crowd images with\nincrementally growing cnn. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3618–3626,\n2018. 8\n[3] V . Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation.\nIEEE transactions on pattern analysis and machine intelligence, 39(12):2481–2495, 2017. 6\n[4] L. Boominathan, S. S. Kruthiventi, and R. V . Babu. Crowdnet: A deep convolutional network for dense crowd counting. 2016. 8\n[5] A. Buades, B. Coll, and J.-M. Morel. A non-local algorithm for image denoising. In 2005 IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR’05), volume 2, pages 60–65. IEEE, 2005. 3\n[6] X. Cao, Z. Wang, Y . Zhao, and F. Su. Scale aggregation network for accurate and efﬁcient crowd counting. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV), pages 734–750, 2018. 2, 8\n[7] A. B. Chan and N. Vasconcelos. Bayesian poisson regression for crowd counting. 2009. 2\n[8] K. Chen, C. C. Loy, S. Gong, and T. Xiang. Feature mining for localised crowd counting. 2012. 2\n[9] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation.\nIn Proceedings of the International Conference on Computer Vision, 2015. 1\n[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–778, 2016. 6\n[11] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. InProceedings of the IEEE\nconference on computer vision and pattern recognition, pages 4700–4708, 2017. 6\n[12] H. Idrees, I. Saleemi, C. Seibert, and M. Shah. Multi-source multi-scale counting in extremely dense crowd images. 2013. 2, 5, 6, 7,\n8\n[13] H. Idrees, M. Tayyab, K. Athrey, D. Zhang, S. Al-Maadeed, N. Rajpoot, and M. Shah. Composition loss for counting, density\nmap estimation and localization in dense crowds. In Proceedings of the European Conference on Computer Vision (ECCV) , pages\n532–546, 2018. 1, 2, 5, 6, 7, 9\n[14] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classiﬁcation with deep convolutional neural networks. In Advances in Neural\nInformation Processing Systems, 2012. 1\n[15] V . Lempitsky and A. Zisserman. Learning to count objects in images. 2010. 2, 8\n9\nImage Ground truth Output\n645 647\n349 328\n347 354\n841 792\n1703 1586\nFigure 3. Qualitative results, some success and failure cases.The three columns show the input image, ground truth annotation map,\nand the prediction. The total counts are shown below each density map. The ﬁrst three rows are success cases for CTN, while the last two\nare failure cases. 10\nImage Ground truth CTN(Local + Non-local) Local\n172 207 291\n954 921 849\n438 470 505\n405 548 581\n208 276 222\nFigure 4. Qualitative results, some success and failure cases.The four columns show the input image, ground truth annotation map, the\nCTN prediction and the Local prediction. The total counts are shown below each density map. The ﬁrst three rows are success cases for\nCTN, while the last two are failure cases for CTN. For the last row, the local approach performs better than CTN.\n11\n[16] M. Li, Z. Zhang, K. Huang, and T. Tan. Estimating the number of people in crowded scenes by mid based foreground segmentation\nand head-shoulder detection. 2008. 2\n[17] Y . Li, X. Zhang, and D. Chen. Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 1091–1100, 2018. 2, 8\n[18] S.-F. Lin, J.-Y . Chen, and H.-X. Chao. Estimation of number of people in crowded scenes using perspective transformation. IEEE\nTransactions on Systems, Man, and Cybernetics-Part A: Systems and Humans, 31(6):645–654, 2001. 2\n[19] E. Lu, W. Xie, and A. Zisserman. Class-agnostic counting. arXiv preprint arXiv:1811.00472, 2018. 3\n[20] D. Onoro-Rubio and R. J. L ´opez-Sastre. Towards perspective-free object counting with deep learning. 2016. 2, 8\n[21] V .-Q. Pham, T. Kozakaya, O. Yamaguchi, and R. Okada. Count forest: Co-voting uncertain number of targets using random forest\nfor crowd density estimation. 2015. 2\n[22] V . Ranjan, H. Le, and M. Hoai. Iterative crowd counting. In Proceedings of the European Conference on Computer Vision (ECCV),\npages 270–285, 2018. 1, 2, 3, 8\n[23] D. B. Sam, S. Surya, and R. V . Babu. Switching convolutional neural network for crowd counting. 2017. 1, 2, 3, 8\n[24] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.arXiv preprint arXiv:1409.1556,\n2014. 1, 4, 6\n[25] V . A. Sindagi and V . M. Patel. Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting.\nIn 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), pages 1–6. IEEE, 2017. 1, 6\n[26] V . A. Sindagi and V . M. Patel. Generating high-quality crowd density maps using contextual pyramid cnns. In Proceedings of the\nIEEE International Conference on Computer Vision, pages 1861–1870, 2017. 1, 2, 3, 7, 8\n[27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In\nAdvances in Neural Information Processing Systems, pages 5998–6008, 2017. 1, 2, 3, 5, 7\n[28] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 7794–7803, 2018. 1, 2, 3\n[29] C. Zhang, H. Li, X. Wang, and X. Yang. Cross-scene crowd counting via deep convolutional neural networks. 2015. 8\n[30] Y . Zhang, D. Zhou, S. Chen, S. Gao, and Y . Ma. Single-image crowd counting via multi-column convolutional neural network. 2016.\n1, 2, 5, 6, 7, 8\n12",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7119427919387817
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5448103547096252
    },
    {
      "name": "Density estimation",
      "score": 0.48589175939559937
    },
    {
      "name": "Data mining",
      "score": 0.4027157425880432
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.36398857831954956
    },
    {
      "name": "Machine learning",
      "score": 0.33575165271759033
    },
    {
      "name": "Mathematics",
      "score": 0.11408290266990662
    },
    {
      "name": "Statistics",
      "score": 0.10321074724197388
    },
    {
      "name": "Estimator",
      "score": 0.0
    }
  ],
  "institutions": []
}