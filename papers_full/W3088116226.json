{
  "title": "Probabilistic Parsing Using Left Corner Language Models",
  "url": "https://openalex.org/W3088116226",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227134332",
      "name": "Manning, Christopher D.",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A4278384867",
      "name": "Carpenter Bob",
      "affiliations": [
        "Alcatel Lucent (Germany)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1541301615",
    "https://openalex.org/W1859173823",
    "https://openalex.org/W1994851566",
    "https://openalex.org/W1577362947",
    "https://openalex.org/W2164854335",
    "https://openalex.org/W1534503951",
    "https://openalex.org/W2087165009",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2110882317",
    "https://openalex.org/W2161204834",
    "https://openalex.org/W2047706513",
    "https://openalex.org/W2159955944",
    "https://openalex.org/W1969810644",
    "https://openalex.org/W2011725768",
    "https://openalex.org/W2950455042",
    "https://openalex.org/W1978470410"
  ],
  "abstract": "We introduce a novel parser based on a probabilistic version of a left-corner parser. The left-corner strategy is attractive because rule probabilities can be conditioned on both top-down goals and bottom-up derivations. We develop the underlying theory and explain how a grammar can be induced from analyzed data. We show that the left-corner approach provides an advantage over simple top-down probabilistic context-free grammars in parsing the Wall Street Journal using a grammar induced from the Penn Treebank. We also conclude that the Penn Treebank provides a fairly weak testbed due to the flatness of its bracketings and to the obvious overgeneration and undergeneration of its induced grammar.",
  "full_text": "arXiv:cmp-lg/9711003v1  17 Nov 1997\nPROBABILISTIC PARSING\nUSING LEFT CORNER LANGUAGE MODELS\nChristopher D. Manning\nLinguistics F12\nUniversity of Sydney NSW 2006\nAustralia\ncmanning@mail.usyd.edu.au\nBob Carpenter\nLucent Technologies Bell Labs\n600 Mountain Avenue, Room 2D-329\nMurray Hill NJ 07974\ncarp@research.bell-labs.com\nAbstract\nWe introduce a novel parser based on a probabilistic version of a left-corner parser. The left-corner strategy is\nattractive because rule probabilities can be conditioned o n both top-down goals and bottom-up derivations. We\ndevelop the underlying theory and explain how a grammar can b e induced from analyzed data. We show that the\nleft-corner approach provides an advantage over simple top -down probabilistic context-free grammars in parsing\nthe Wall Street Journal using a grammar induced from the Penn Treebank. We also concl ude that the Penn\nTreebank provides a fairly weak testbed due to the ﬂatness of its bracketings and to the obvious overgeneration and\nundergeneration of its induced grammar.\n1 Introduction\nFor context-free grammars (CFGs), there is a well-known standa rd probabilistic version, Probabilistic Context-\nFree Grammars (PCFGs), which have been thoroughly investigated [Suppes, 1970, Sankoﬀ, 1971, Baker, 1979,\nLari and Young, 1990, Kupiec, 1991, Jelinek et al., 1992, Charniak, 1993].\nUnder this model, one assigns probabilities for diﬀerent rewrites of a non-terminal. Or in other words, one\nis giving the probability of a local subtree given the mother node. So, for example, we might have:\nP (NP → Det N |NP mother) = 0 .2 P (NP → P ron|NP mother) = 0 .1\nwhere in general, ∀ nonterminals A, ∑\nγ P (A → γ) = 1.\nBut standard PCFGs are only one way to make a probabilistic version o f CFGs. If we think in parsing\nterms, a PCFG corresponds to a probabilistic version of top down pa rsing, since at each stage we are trying\nto predict the child nodes given knowledge only of the parent node. O ther parsing methods lend themselves to\ndiﬀerent models of probabilistic conditioning. Usually, such conditionin g is a mixture of top-down and bottom-\nup information. This paper discusses some initial results from anoth er point in this parameter space where the\nconditioning reﬂects a left-corner parsing strategy, yielding what we will call probabilistic left-corner grammars\n(PLCGs).1 Left-corner parsers simultaneously work top-down from a goal c ategory and bottom-up from the\n0We thank Edward Stabler and Mark Johnson for getting us inter ested in left corner parsing, and Mark more particularly for\nvaluable discussion of some of the work in this paper.\n1This name may appear strange since the symbolic part of the gr ammar is unchanged and still context-free. But if we regard\nthe probabilistic conditioning as part of the grammar, then we do have a diﬀerent kind of grammar. We should then perhaps c all\nthe result a LCPG, but we place the P in initial position for re asons of tradition.\nExpansion % as Subj % as Obj\nNP → PRP 13.7% 2.1%\nNP → NNP 3.5% 0.9%\nNP → DT NN 5.6% 4.6%\nNP → NN 1.4% 2.8%\nNP → NP SBAR 0.5% 2.6%\nNP → NP PP 5.6% 14.1%\nTable 1: Selected common expansions of NP as Subject vs. Object\nleft corner of a particular rule. For instance, a rule such as S → NP V P has the left-corner NP and will be\nﬁred whenever an NP has been derived and an S would help toward the eventual goal category. In this paper,\nwe present algorithms for PLCG parsing, present some results com paring PLCG parsing with PCFG parsing,\nand discuss some mechanisms for improving results.\nWhy might one want to employ PLCGs? While the main perceived weaknes s of PCFGs is their lack\nof lexicalization, they are also deﬁcient on purely structural groun ds [Briscoe and Carroll, 1993]. Inherent to\nthe idea of a PCFG is that probabilities are context-free: for instan ce, that the probability of a noun phrase\nexpanding in a certain way is independent of where the NP is in the tree . Even if we in some way lexicalize\nPCFGs to remove the other deﬁciency, this assumption of structu ral context-freeness remains. But this context-\nfree assumption is actually quite wrong. For example, Table 1 shows h ow the probabilities of expanding an NP\nnode (in the Penn Treebank) diﬀer wildly between subject position an d object position. Pronouns, proper names\nand deﬁnite NPs appear more commonly in subject position while NPs co ntaining post-head modiﬁers and bare\nnouns occur more commonly in object position (this reﬂects the fac t that the subject normally expresses the\nsentence-internal topic [Manning, 1996]).\nAnother advantage of PLCGs is that parse probabilities are straigh tforwardly calculated from left to right,\nwhich is convenient for online processing and integration with other lin ear probabilistic models. 2\n2 Probabilistic Left Corner Grammars\nLeft corner parsers [Rosenkrantz and Lewis II, 1970, Demers, 1 977] work by a combination of bottom-up and\ntop-down processing. One begins with a goal category (the root o f what is currently being constructed), and\nthen looks at the left corner of the string (i.e., one shifts the next t erminal). If the left corner is the same\ncategory as the goal category, then one can stop. Otherwise, o ne projects a possible local tree from the left\ncorner. The remaining children of this projected local tree then be come goal categories and one recursively does\nleft corner parsing of each. When this local tree is ﬁnished, one aga in recursively does left-corner parsing with\nthis subtree as the left corner, and the same goal category. To m ake this description more precise, a Prolog\nversion of a simple left corner recognizer is shown in Figure 1. This par ticular parser assumes a rule format for\nrule/2 that allows lexical material to appear on the right-hand side of a r ule.3\nA common formulation of left corner parsers is in terms of a stack of found and sought constituents, the\nlatter being represented as minus categories on the stack (and re presented as m(Cat) in the Prolog code). A left\ncorner parser that uses a stack is shown in Figure 2. Shifting is now a n explicit option on a par with projecting\nand attaching, but note that when to shift remains deterministic. I f the thing on top of the stack is a predicted\nm(Cat), then one must shift, and one can never successfully shift at othe r times. This second version of the\nparser more transparently corresponds to the probabilistic langu age model we employ.\nTo produce a language model that reﬂects the operation of a left c orner parser, we have to provide prob-\nabilities for the diﬀerent operations (the clauses of process in Figure 2). For each step, we need to decide the\nprobabilities of deciding to shift, attach, or project. The only inter esting choice here is deciding whether to\nattach in cases where the left corner category and the goal cate gory are the same. For the other two operations\nof the parser, we need to model the probability of shifting diﬀerent terminals, and the probability of building a\n2Note however, that while the obvious way of calculating PCFG probabilities does not allow incremental processing, incr emental\ncalculation is possible, as discussed by [Jelinek et al., 19 92].\n3In general, empty categories can be accommodated by allowin g a category to be introduced for completion without popping a\nword oﬀ the input stack.\n% lc(List_of_words_to_parse)\nlc(Ws) :- start(C),\ncomplete_list([C], Ws, []).\ncomplete(C, C, Ws, Ws). % attach\ncomplete(W, C, Ws, NewWs) :- % project lc\nrule(LHS, [W|Rest]), % lex / phrase\ncomplete_list(Rest, Ws, Ws2),\ncomplete(LHS, C, Ws2, NewWs).\ncomplete_list([], Ws, Ws).\ncomplete_list([C|Cs], [W|Ws], NewWs) :-\ncomplete(W, C, Ws, Ws2), % shift\ncomplete_list(Cs, Ws2, NewWs).\nFigure 1: A Prolog LC parser\nslc(Ws) :- start(C),\nslc(Ws, [m(C)]).\nslc([], []).\nslc(L0, Stack0) :-\nprocess(Stack0, Stack, L0, L),\nslc(L, Stack).\nprocess([A, m(A)|Stack], Stack, L, L). % attach\nprocess([Item|Items], Stack, L, L) :- % project LC\nrule(LHS, [Item|Rest]),\npredict(Rest, [LHS|Items], Stack).\nprocess(Stack, [L|Stack], [L|Ls], Ls). % shift\npredict([], L, L).\npredict([L|Ls], L2, [m(L)|NewLs]) :-\npredict(Ls, L2, NewLs).\nFigure 2: A Prolog LC stack parser\ncertain local tree given the left corner ( lc) and the goal category ( gc). Under this model, we have probabilities\nfor this last operation like this:\nP (SBar → P S |lc = P, gc = S) = 0 .25\nP (P P → P NP |lc = P, gc = S) = 0 .55\nHow to make probabilities out of the above choices is made precise in th e next section.\n2.1 The LC probability of a parse\nIn this section, we provide probabilities for left-corner derivations . These form the basis for a language model\nthat assigns probabilities to sentences. For a sentence s, we have that the probability of a sentence according\nto a grammar G is:\nP (s|G) =\n∑\nt\nP (s, t|G), t a parse tree of s\n=\n∑\n{t: yield(t)=s}\nP (t|G)\nThe last line follows since the parse tree determines the terminal yield . It is therefore suﬃcient to be able to\ncalculate the probability of a (parse) tree. Below we suppress the c onditioning of the probability according to\nthe grammar.\nNow following the intuition of our model having been inspired by left cor ner parsing, we can express the\nprobability of a parse tree in terms of the probabilities of left corner derivations of that parse tree:\nP (t) =\n∑\nd a LC derivation of t\nP (d)\nBut under left corner parsing, each parse tree has a unique deriva tion and so the summation sign can be dropped\nfrom this equation.\nNow, without any assumptions, the probability of a derivation can be expressed as a product in terms of the\nprobabilities of each of the individual operations in the derivation. Su ppose that ( C1, . . . , C m) is the sequence\nof operations in the LC parse derivation d of t. Then, by the chain rule, we have:\nP (t) = P (d) =\n∏\nC1,...,C m\nP (Ci|C1, . . . , C i−1)\nIn practice, we cannot condition the probability of each parse decis ion on the entire history. The simplest\nmodel, which we will explore for the rest of this section, is to assume t hat the probability of each parse decision\nis largely independent of the parse history, and just depends on th e state of the parser. In particular, we\nwill assume that it depends simply on the left corner and top goal cat egories of the parse stack. This drastic\nassumption nevertheless gives us a slightly richer probabilistic model than a PCFG, because elementary left-\ncorner parsing actions are conditioned by the goal category, rat her than simply being the probability of a local\ntree. For instance, the probability of a certain expansion of NP may be diﬀerent in subject position and object\nposition, because the goal category is diﬀerent.\nEach elementary operation of a left corner parser is either a shift, an attach or a left corner projection.\nUnder the independence assumptions mentioned above, the proba bility of a shift will simply be the probability\nof a certain left corner daughter ( lc) being shifted given the current goal category ( gc), which we will model\nby Pshift . Note that when to shift is deterministic. If a goal (i.e., minus) categ ory is on top of the stack (and\nhence there is no left corner category), then one must shift. Oth erwise one cannot. If one is not shifting, one\nmust choose to attach or project, which we model by Patt. Attaching only has a non-zero probability if the\nleft corner and the goal category are the same, but we deﬁne it fo r all pairs. If we do not attach, we project a\nconstituent based on the left corner with probability Plc. Thus the probability of each elementary operation Ci\ncan be expressed in terms of probability distributions Pshift , Patt, and Plc as follows:\nP (Ci = shift lc) =\n{ Pshift (lc|gc) if top of the stack is gc\n0 otherwise\nP (Ci = attach) =\n{\nPatt(lc, gc) if top of the stack is not gc\n0 otherwise\nP (Ci = project A → γ) =\n{ (1 − Patt(lc, gc))Plc(A → γ|lc, gc) if top of the stack is not gc\n0 otherwise\nWhere these operations obey the following:\n∑\nlc\nPshift (lc|gc) = 1\nIf lc ̸= gc, Patt(lc, gc) = 0∑\n{A→γ :γ =lc,... }\nP (A → γ|lc, gc) = 1\nFrom the above we note that the probabilities of the choice of proje ctions sums to one, and hence, since\nother probabilities are complements of each other, the probabilities of the actions available for each elementary\noperation sum to one. There are also no dead ends in a derivation, be cause unless A is a possible left corner\nconstituent of gc, P (A → γ|lc, gc) = 0. Thus we have shown that these probabilities deﬁne a language m odel.4\nThat is, ∑\ns P (s|G) = 1. It is possible to extend the PLCG model in various ways to include more probabilistic\nconditioning, as we discuss brieﬂy later, but our current results re ﬂect this model.\n4Subject to showing that the probability mass accumulates in ﬁnite trees.\nRule Freq. PCFG Prob. Rule Freq. PCFG Prob.\nPP → IN NP 76617 0.81 NP → PRP 17323 0.06\nNP → NP PP 34965 0.11 ADVP → RB 14228 0.72\nNP → DT NN 29351 0.09 NP → NN 13586 0.04\nS → NP VP 28292 0.30 NP → NNS 13318 0.04\nS → VP 23559 0.25 VP → TO VP 12900 0.09\nS → NP VP . 17703 0.19 NP → NNP 12575 0.04\nTable 2: Highest frequency CFG rules in Penn Treebank\n2–12 word sentences this paper Charniak\nGrammar (rules) 14 971 10 605\n% sent. length < cutoﬀ 16.6%\nTest set size (sentences) 401\nAverage Length (words) 8.3 8.7\nPrecision 89.8% 88.6\nRecall 90.7% 91.7\nLabelled Precision 83.5%\nLabelled Recall 82.9%\nLabelled Precision +1 87.1%\nLabelled Recall +1 85.2%\nAverage CBs 0.27\nNon-crossing accuracy 95.8% 97.9%\nSentences with 0 CBs 84.5%\nTable 3: PCFG results\n3 Parsing experiments\n3.1 PCFG Experiment\nTraining and testing were done on Release 2 of the Penn Treebank [Ma rcus et al., 1993], published in 1995. As\nin other recent work [Magerman, 1995, Collins, 1996], training was do ne on sections 02–21 of the Wall Street\nJournal portion of the treebank (approximately 40,000 sentence s, 780,153 local trees) and ﬁnal testing was done\non section 23, which contains 2416 sentences. Counts of how ofte n each local tree occurred in the treebank were\nmade, and these were used directly to give probabilities for rewriting each nonterminal. The highest frequency\nrules are given in Table 2. Local trees were considered down to the le vel of preterminals (i.e., part of speech\ntags); lexical information was ignored. 5 Every tree was given a new root symbol ‘ROOT’, attached by a unary\nbranch to the root in the treebank. Empty nodes (of which there a re several kinds in the treebank) were ignored,\nand nonterminals above them that dominated no pronounced words were also deleted. 6 No attempt was made to\ndo any smoothing. While in a few cases this would clearly be useful (e.g., the training data allows a compound\nnoun to be modiﬁed by four adjectives, but not a simple noun), in pra ctice the induced treebank grammar is\nhugely ambiguous, and greatly overgenerates. Thus, while the lack of smoothing in principle disallows some\ncorrect parses from being generated, the treebank grammar ca n always produce some parse for a sentence\n[Charniak, 1996] and adding unseen rules with low probabilities is unlikely to improve bottom line performance,\nbecause these added rules are unlikely to appear in the maximum prob ability parse. A better solution would\nbe to use a covering grammar with fewer rules and a more deeply nest ed structure.\nTesting was done by chart-parsing the part of speech tags of the sentences (i.e., ambiguities in part of\nspeech assignment were assumed to be successfully resolved). An exhaustive chartparse was done and the\nhighest probability (Viterbi) parse was selected, in the standard wa y [Charniak, 1993]. Results from such\nparsing are shown in Table 3 together with results from [Charniak, 19 96]. The measures shown have been used\n5Of course, we could easily integrate our model with a tagging model.\n6Simply eliminating empties in the treebank is dangerous bec ause they are the only trace of unbounded dependency constru ctions.\nThis leads to ridiculous rules like S → VP (with 23559 appearances in the treebank) stemming from S → NP VP where there is a\ntrace subject NP. A purely context-free solution would be to introduce slash percolation.\n2/4–40 word sentences PCFG Magerman Collins\n% sent. length < cutoﬀ 92.9%\nTest set size (sentences) 1759 2416\nAverage Length (words) 21.9 22.3\nPrecision 78.8% 86.3%\nRecall 80.4% 85.8%\nLabelled Precision 84.5% 86.3%\nLabelled Recall 84.0% 85.8%\nAverage CBs 1.33 1.14\nNon-crossing accuracy 87.7%\nSentences with 0 CBs 55.4% 57.2%\nTable 4: PCFG [Charniak, 1996] vs. [Magerman, 1995]/[Collins, 1996] comparison\nin various earlier works, and generally draw from the PARSEVAL meas ures [Black et al., 1991]. Precision is how\nmany brackets in the parse match those in the correct tree (perh aps also examining labels), recall measures how\nmany of the brackets in the correct tree are in the parse. The unm arked measures ignore unary constituents, the\nones marked +1 include unary constituents. 7 Crossing brackets (CBs) measures record how many brackets in\nthe parse cross bracketings in the correct tree, with the non-cr ossing accuracy measuring the percent of brackets\nthat are not CBs. The ‘% sent. length < cutoﬀ’ says what percentage of sentences within the 2416 sente nce\ntest section were shorter than the cutoﬀ and thus used in the tes t set for the current experiment. Our results\nare not directly comparable to Charniak’s since he was using an earlier release of the Penn Treebank. Further,\nhe used two strategies that aimed at increasing performance: rec oding auxiliaries from their Penn tag (which is\nundistinguished from other verbs) to special auxiliary tags, and ad ding a (crude) correction factor to the PCFG\nmodel so that it uniformly favored right-branching trees rather t han being context free. Whether the former\nchange was shown to be beneﬁcial is not discussed, but the later co rrection factor improved results by about\ntwo percent. The fact that the results are mixed between the two systems suggests that the quality of the Penn\nTreebank has improved in the second release, and these gains roug hly match the gains from these factors. It is\nuseful that the results are roughly comparable since we can then u se Charniak’s results as a rough benchmark\nfor PCFG performance on longer sentences, which we have not obt ained.8\nCharniak’s central contention is that purely structural parsing lik e this using treebank grammars works\nmuch better than community lore would have you believe. Indeed, as the comparison in Table 4 suggests, it\ndoes not work much worse than [Collins, 1996], a leading recent parse r that includes lexical content. That is,\nit seems one can score well in the PARSEVAL measures using purely st ructural factors, and that the use of\nlexical factors in other models is at present only adding a little to their performance.9 This is in part because\nthe Penn treebank does not represent certain semantic “attach ment” decisions, and the structure of the trees\nminimizes the penalty for other “attachment” errors, as we discus s in the last section of this paper.\n3.2 LC parsing results\nThe probabilistic left corner parser is implemented as a beam parser in C. As a k-best beam parser, it is not\nguaranteed to ﬁnd the best parse, unless the beam is eﬀectively inﬁ nite. Space requirements depend on the\nsize of the beam, and the length of the sentence, but are consider ably more reasonable than those for the chart\nparser used above. Nevertheless, the branching factor in the se arch space is very high because there are many\n7The original parsevalmeasures (because they were designed for comparing diﬀeren t people’s parsers that used diﬀerent theories\nof grammar) ignored node labels entirely, discarded unary b rackets, and performed other forms of tree normalization to give special\ntreatment to certain cases such as verbal auxiliaries. Whil e such peculiarities made some sense in terms of the purpose f or which\nthe measures were originally developed, it is not clear they are appropriate for the use of these measures within the stat istical NLP\ncommunity. Thus people often report labelled measures, but it is often unclear whether the other rules and transformati ons of the\nstandard have been employed (but this aﬀects the results rep orted). In this paper, unary nodes are deleted in measures ex cept\nthose marked +1, but none of the special case tree transforma tions in the parseval standard are applied. All punctuation and all\nconstituent labels (but not functional tags) are also retai ned.\n8Our PCFG parser builds a complete chart, which leads to unvia ble space requirements for long sentences.\n9Again, results are not strictly comparable. The comparison is unfair to Magerman and Collins’ systems since they are als o\ndoing part of speech tagging, whereas the PCFG is not. But on t he other hand, Magerman and Collins’ parsers conﬂate the ADV P\nand PRT labels and ignore all punctuation, which improves th eir reported results.\nParser results 2–12 Error\nSentence Lengths: 2–12 2–16 2–25 Reduction\nBeam size 50 000 50 000 40 000\n% sent. length < cutoﬀ 16.6% 28.1% 60.2%\nTest set (sentences) 401 680 1454\nAverage length (words) 8.3 10.9 16.3\nPrecision 92.0% 90.1% 84.6% 21.6%\nRecall 92.3% 89.5% 83.2% 17.2%\nLabelled Precision 87.1% 86.0% 81.1% 21.8%\nLabelled Recall 86.7% 84.9% 79.6% 22.2%\nLabelled Precision +1 88.6% 87.7% 83.5% 11.6%\nLabelled Recall +1 88.3% 86.3% 81.5% 20.9%\nAverage CBs 0.21 0.43 1.25 22.2%\nNon-crossing accuracy 96.8% 94.7% 89.6% 23.8%\nSentences with 0 CBs 87.5% 76.0% 52.0% 19.4%\nTable 5: Left Corner Parser results fromn-ary grammar.\nrules possible with a certain left corner and goal category (especia lly for a grammar induced from the Penn\nTreebank in the manner we have described). Therefore, a huge be am is needed for good results to be obtained.\nA way of addressing this problem by binarizing the grammar is discusse d in the next section.\nThe parser maintains two beams, one containing partial LC parses o f the ﬁrst i words, and another in\nwhich is built a beam of partial LC parses of the ﬁrst i + 1 words. The partial parses are maintained as pointers\nto positions in trie data structures that represent the list of pars er moves to this point and the current parse\nstack. At the end of parsing, the lists of parser moves can be easily turned into parse trees for the n-best parses\nin the beam.\nResults are shown in Table 5. They reﬂect the same training and test ing data as described above. The\nresults show a small increase in performance for PLCGs. This is show n more dramatically in the right-hand\ncolumn of Table 5, which shows that the extra information provided b y the Left Corner goal category reduces\nparsing errors by about 20% over our PCFG results on 2–12 word se ntences.\n4 Binarization\nFor the parser above, use of a beam search is quite ineﬃcient becau se, for a given left corner and goal category,\nthere are often hundreds of possible local trees that could be pro jected, and little information is available at\nthe time this decision is made, since the decision mainly depends on word s that have not yet been shifted.\nTherefore the beam must be large for good results to be obtained, and, at any rate, the branching factor of the\nsearch space is extremely high, which slows parsing. One could imagine using various heuristics to improve the\nsearch, but the way we have investigated combatting this problem is by binarizing the grammar.\nThe necessary step for binarization is to eliminate productions with t hree or more daughters. We carried\nthis out by merging the tails, so that a rule such as NP → Det JJ NN is replaced by two rules, NP → Det NP Det\nand NPDet → JJ NN . This is carried out recursively until only binary rules remain. As a res ult of this choice of\nbinarization, n-ary rules that share the same mother and left corner category a ll reduce to a single rule. This\ngreatly cuts the branching factor of the search space and allows d ecisions to be put oﬀ during parsing, until\nmore of the input has been seen, at which point alternative continua tions can be better evaluated. Furthermore,\nthe weights for such rules all combine into a larger weight for the com bined rule. 10\nIt is important to note that the resulting model is not equivalent to our original model. While the straight-\nforward way of binarizing a PCFG yields the same probability estimates for trees as the n-ary grammar, this\nis not true for our PLCG model since we are now introducing new estim ates for shifting terminals for each of\nour newly created non-terminals. Slightly diﬀerent probability estima tes result, and further work is needed to\ninvestigate what relationship exists between them and the probabilit y estimates of the original grammar.\n10If we were to do this even for rules that start out binary, and e liminate unary rules downward rather than upward, then for\nevery left corner C and mother G there will be a unique rule G → C GC .\nParser Results 2–25 Error\nSentence Lengths: 2–12 2–16 2–25 2-40 Reduction\nBeam size 40 000 40 000 40 000 40 000\n% sent. length < cutoﬀ 16.3% 28.1% 60.2% 91.8%\nTest set (sentences) 680 1454 2216\nAverage length (words) 8.3 10.9 16.3 21.6\nPrecision 93.5% 91.4% 86.6% 83.0% 13.0%\nRecall 92.8% 89.9% 84.3% 80.7% 6.5%\nLabelled Precision 89.8% 88.1% 83.4% 79.9% 12.2%\nLabelled Recall 88.4% 86.3% 81.0% 77.6% 6.9%\nLabelled Precision +1 90.0% 89.0% 85.1% 81.9% 9.7%\nLabelled Recall +1 89.5% 87.2% 82.6% 79.5% 5.9%\nAverage CBs 0.17 0.39 1.09 1.99 12.8%\nNon-crossing accuracy 97.2% 95.2% 90.9% 87.6% 12.5%\nSentences with 0 CBs 89.8% 78.2% 55.8% 41.5% 7.9%\nTable 6: Left Corner Parser results with binarized grammar\nPrior to the above binarization step, one might also wish to eliminate un ary productions, much as we earlier\neliminated empty categories. This can be done in two ways. One way is t o fold them upwards. This preserves\nlexical tagging. That is, if there is a category A dominating only a tree rooted at B, then the category A is\neliminated and the tree rooted at B moved upwards. This may cause the number of rules to increase, be cause a\nlocal tree that started with a daughter A will now show up with daughter B in the same place. The alternative\nis to eliminate B and replace it with A. This can also create a new local tree instance because the daught ers\nof B now show up with a new mother A. In this way, lexical tags can be changed. For instance, consider a\nrule NP → NNP for a noun phrase rewriting as a proper noun. In the context of elim inating empty categories\nupward, we get a new rule S → NNP V P , whereas by eliminating empty categories downward we would have\nproduced a new lexical entry NP → Jones. Our current results do not reﬂect the elimination of unary rules,\nbut we hypothesize that doing this would further improve the measu res that do not consider unary nodes, while\nprobably harming the results on measures that do include unary nod es.\nTable 6 shows that binarization brings a further modest improvemen t in results. The righthand column\nshows the percent error reduction on 2–25 word sentences betw een the n-ary grammar and the binary grammar.\n5 Extended Left Corner Models\nMore sophisticated PLCG parsing models will naturally provide greate r conditioning of the probability of an\nelementary operation based on the parse history. There are a num ber of ways that one could then proceed.\nFrom the background of work on LC parsing, a natural factor to c onsider is the size of the parse stack, and we\nwill brieﬂy investigate incorporating this factor.\nFor left corner parsing, the stack size is particularly interesting. [S tabler, 1994] notes that in contrast to\nbottom-up and top-down parsing methods, left-corner parsers can handle arbitrary left-branching and right-\nbranching structures with a ﬁnitely bounded stack size. Furtherm ore, left-corner parses of center embedded\nconstructions have stack lengths proportional to the amount of embedding. This is not actually true for the stack-\nbased LC parser presented earlier which has the stack growing with out bound for rightward-branching trees.\nTo gain the desirable property that Stabler notes, one needs to do stack composition by deciding immediately\nwhether to attach whenever one projects a category that matc hes the current goal category, rather than delaying\nthe attachment until after the left corner constituent is complet e. If one decides to attach, the goal minus\ncategory and the mother category are immediately removed from t he stack. This is implemented by replacing\nthe predicate process in Figure 2 by the code in Figure 3, where composition is done by the pre dicate compose.\nAll else being equal, this change makes no diﬀerence to the probabilist ic model presented earlier. In practice\nthough, this formulation makes a beam search less eﬀective since we are bringing forward the decision of whether\nto attach or not, and often both alternatives must be tried which ﬁ lls out the beam unnecessarily.\nGiven human intolerance for center embeddings and ease of parsing left and right branching structures, we\nwould expect the stack sizes to stay low. The general prediction (a nd empirical fact) is that the probability\nprocess([Item|Items], Stack, L, L) :- % lc\nrule(LHS, [Item|Rest]),\ncompose(LHS, Items, Stack1),\npredict(Rest, Stack1, Stack).\nprocess(Stack, [L|Stack], [L|Ls], Ls). % shift\ncompose(A, [m(A)|L], L). % attach\ncompose(A, L, [A|L]). % don’t\nFigure 3: Altered code for a Prolog LC parser that does stack composition\nStack Stack size change\nSize Total −1 0 +1\n8 23 20 (87%) 3 (13%) 0\n7 160 86 (54%) 54 (34%) 20 (13%)\n6 1291 987 (76%) 218 (17%) 86 ( 7%)\n5 3745 1393 (37%) 1365 (36%) 987 (26%)\n4 17000 12116 (71%) 3491 (21%) 1393 (8%)\n3 39105 12241 (31%) 14748 (38%) 12116 (31%)\n2 108544 63160 (58%) 33143 (31%) 12241 (11%)\n1 71681 8521 (12%) 0 63160 (88%)\nTable 7: Changes in stack size for Penn Treebank\nof not attaching (composing) decreases slightly with the size of the stack. The counts for the Penn Treebank,\nafter binarization (including removal of unary rules), are given in Ta ble 7. Once one looks beyond the odd-even\neﬀect in the data, the decreasing probability of not attaching can b e clearly seen.\nTo incorporate stack length into the model, we wish to more accurat ely predict: P (Ci|C1, . . . , C i−1).\nPreviously, histories of parse steps were equivalenced according t o just the goal category and the left corner (if\npresent). Now, we are going to additionally diﬀerentiate parse histo ries depending on ℓ, the length of the stack\nafter Ci−1. As before, if the top of the stack is a predicted category, we will s hift; otherwise we cannot shift. In\nthe latter case, we will predict Pδ (δ|ℓ, gc, lc ), the probability of various changes in the stack size, based on the\nstack size, the goal category, and the left corner.\nLet us assume that rules are binary, as discussed above. Then the possible change in the stack length from\na single elementary operation is between −2 and +1. Given that we are not shifting, we have the following:\nStack Delta Rule Type\n−2 unary left corner and attach\n−1 binary left corner and attach\n0 unary left corner (no attach)\n+1 binary left corner (no attach)\nThe probability of each elementary operation will then be the probab ility of a certain stack delta (given the\nstack size, left corner and goal category) times the probability of a certain rule, given the left corner, the goal\ncategory, and the stack delta. Whether we attach (compose) or not is deterministic given the stack delta, and\nso we no longer need to model the Patt distribution. Under this model, the probability of diﬀerent projectio n\noperations (given that we are not in a position to shift) becomes:\nP (Ci = project A → lc, attach) = Pδ (−2|ℓ, lc, gc )Plc′ (A → lc|lc, gc, ℓ, δ )\nP (Ci = project A → lc, do not attach) = Pδ (0|ℓ, lc, gc )Plc′ (A → lc|lc, gc, ℓ, δ )\nP (Ci = project A → lc c 2, attach) = Pδ (−1|ℓ, lc, gc )Plc′ (A → lc c 2|lc, gc, ℓ, δ )\nP (Ci = project A → lc c 2, do not attach) = Pδ (+1|ℓ, lc, gc )Plc′ (A → lc c 2|lc, gc, ℓ, δ )\nA variety of other extended models are possible. Note that the mod el does not need to be uniform, and that\nwe can estimate diﬀerent classes of elementary operations using diﬀ erent probabilistic submodels. In particular,\nat the level of preterminals, we can incorporate a tagging model. Giv en the structure of the Penn Treebank,\na terminal wj is always dominated by a unary rule giving the terminal’s part of speech pj . In line with our\nbasic model above, the choice of part of speech for a word (where the word now counts as the left corner)\nwill certainly depend on the current goal category. However, we c an also condition it on other preceding parse\ndecisions, in particular on the part of speech of the preceding two w ords, or, perhaps, in certain circumstances,\non the particular word that preceded. Taking the former possibility , we can say, for cases where Ci involves\npredicting a preterminal (through a left corner projection step) ,\nP (Ci|C1, . . . , C i−1) = P (pj |wj , gc, C 1, . . . , C i−1)\n≈ P (pj |wj , gc, p j−2, pj−1)\nAssuming – perhaps rashly – independence between the conditioning variables that we have been using before\n(lc, gc) and the new ones ( pj−1, pj−2), then we have that:\nP (pj |wj , gc, p j−2, pj−1) ≈ P (pj |wj , gc)P (pj |pj−2, pj−1)\nP (pj )\nThis is nice in part because we can calculate it using just the statistics previously gathered and a simple trigram\nmodel over POS tags. (An incidental nice property is that the prob ability is for the POS given the word, and\nnot the other way round, as in the ‘confusing’ P (w|t) term of standard Markov model POS taggers.) 11\n6 Comparison with previous work\nPrevious work on non-lexicalized parsing of Penn Treebank data inclu des [Schabes et al., 1993] and\n[Charniak, 1996], but the work perhaps most relevant to our own is t hat of [Briscoe and Carroll, 1993] and\n[Carroll and Briscoe, 1996], which also seeks to address the contex t-freeness assumption of PCFGs. They ap-\nproach the problem by using a probabilistic model based on LR parse t ables. Unfortunately, many diﬀerences of\napproach make meaningful comparisons diﬃcult, and a comparable s tudy of using PLCGs versus Probabilistic\nLR parsing remains to be done. Briscoe and Carroll use their Probab ilistic LR grammar to guide the actions of\na uniﬁcation-based parser which uses a hand-built grammar. While we are sympathetic with their desire to use\na more knowledge-based approach, this means that: their languag e model is deﬁcient, since probability mass\nis given to derivations which are ruled out because of uniﬁcation failur es; the coverage of their parser is quite\nlimited because of limitations of the grammar used; and much time need s to be expended in developing the\ngrammar, whereas our grammar is acquired automatically (and quick ly) from the treebank. Moreover, while\nresults are not directly comparable, our parsers seem to do rathe r better on precision and recall than the parser\ndescribed in [Carroll and Briscoe, 1996], while performing somewhat w orse on crossing brackets measures. How-\never, Carroll and Briscoe’s inferior results probably reﬂect the fa ct that the parse trees of their grammar do not\nmatch those of their treebank more than anything else.\n7 Observations on why parsing the Penn Treebank is easy\nHow is it that the purely structural – and context free even in stru ctural terms – PCFG parser manages\nto perform so well? An important observation is that the measures o f precision and recall (labelled or not)\nand crossing brackets are actually quite easy measures to do well o n. It is important to notice that they are\nmeasuring success at the level of individual decisions – and normally w hat makes NLP hard is that you have\nto make many consecutive decisions correctly to succeed. The ove rall success rate is then the nth power of the\nindividual decision success rate – a number that easily becomes small.\nBut beyond this, there are a number of features particular to the structure of the Penn Treebank that make\nthese measures particularly easy. Success on crossing brackets is helped by the fact that Penn Treebank trees\n11Below is a derivation of this equation, written in a simpliﬁe d form with three variables. We assume b and c are independent,\nand the result then follows by using Bayes’ rule followed by t he deﬁnition of conditional probability:\nP (a|b, c) = P (a)P (b, c|a)\nP (b, c) = P (a)P (b|a)P (c|a, b)\nP (b)P (c|b) = P (a)P (b|a)P (c|a)\nP (b)P (c) = P (a)P (b, a)P (c, a)\nP (a)2P (b)P (c) = P (a|b)P (a|c)\nP (a)\nPenn VP attach (VP saw (NP the man) (PP with (NP a telescope)))\nPenn NP attach (VP saw (NP (NP the man) (PP with (NP a telescope)) ))\nAnother VP attach (VP saw (NP the (N ′ man)) (PP with (NP a (N ′ telescope)))))\nAnother NP attach (VP saw (NP the (N ′ man (PP with (NP a (N ′ telescope))))))\nTable 8: Penn trees versus other trees\nare quite ﬂat. To the extent that sentences have very few brack ets in them, the number of crossing brackets\nis likely to be small. Identifying troublesome brackets that would lower precision and recall measures is also\navoided. As a concrete instance of this, one diﬃculty in parsing is dec iding the structure of noun compounds\n[Lauer, 1995]. Noun compounds of three or more words in length can display any combination of left- or right-\nbranching structure, as in [[cable modem] manufacturer] vs. [comp uter [power supply]]. But such ﬁne points\nare ﬁnessed by the Penn Treebank, which gives a completely ﬂat str ucture to a noun compound (and any other\npre-head modiﬁers) as shown below (note that the ﬁrst example als o illustrates the rather questionable Penn\nTreebank practice of tagging hyphenated non-ﬁnal portions of n oun compounds as adjectives!).\n(NP a/ dt stock-index/jj arbitrage/nn sell/nn program/nn )\n(NP a/ dt joint/jj venture/nn advertising/nn agency/nn )\nAnother case where peculiarities of the Penn Treebank help is the (s omewhat nonstandard) adjunction\nstructures given to post noun-head modiﬁers, of the general fo rm (NP (NP the man) (PP in (NP the moon))).\nA well-known parsing ambiguity is whether PPs attach to a preceding N P or VP – or even to a higher preceding\nnode – and this is one for which lexical or contextual information is cle arly much more important than structural\nfactors [Hindle and Rooth, 1993]. Note now that the use of the abov e adjunction structure reduces the penalty\nfor making this decision wrongly. Compare Penn Treebank style stru ctures, and another common structure in\nthe examples in Table 8. Note the diﬀerence in the results:\nError Errors assessed\nPrec. Rec. CBs\nPenn VP instead of NP 0 1 0\nNP instead of VP 1 0 0\nAnother VP instead of NP 1 2 1\nNP instead of VP 2 1 1\nThe forgivingness of the Penn Treebank scheme is manifest. One ca n get the attachment wrong and not have\nany crossing brackets. 12\n8 Conclusions\nThis paper explores a new class of probabilistic parsing algorithms for context-free grammars, probabilistic left\ncorner grammars. The ability of left corner parsers to support lef t-to-right online parsing makes them initially\npromising for many tasks. The diﬀerent conditioning model is slightly r icher than that of standard PCFGs, and\nthis was shown to bring worthwhile performance improvements over a standard PCFG when used to parse Penn\nTreebank sentences. Beyond this, the model can be extended in v arious ways, an avenue that we have only just\nbegun exploring. Because the left-corner component of the gram mar is purely structural, it can be combined\nwith other models that include lexical attachment preferences and preferences for basic phrasal chunking (both\nincorporated into Collins’ parser).\nReferences\n[Baker, 1979] Baker, J. K. (1979). Trainable grammars for speec h recognition. In Klatt, D. H. and Wolf,\nJ. J., editors, Speech Communication Papers for the 97th Meeting of the Acou stical Society of America , pages\n547–550.\n12If one includes unary brackets (recall footnote 7), then the contrast becomes even more marked, since there would be 2 pre cision\nand recall errors each under the alternative parse trees.\n[Black et al., 1991] Black, E., Abney, S., Flickinger, D., Gdaniec, C., Grish man, R., Harrison, P., Hindle, D.,\nIngria, R., Jelinek, F., Klavans, J., Liberman, M., Marcus, M., Roukos, S., Santorini, B., and Strzalkowski, T.\n(1991). A procedure for quantitatively comparing the syntactic c overage of English grammars. In Proceedings,\nSpeech and Natural Language Workshop, Paciﬁc Grove, CA , pages 306–311. DARPA.\n[Briscoe and Carroll, 1993] Briscoe, T. and Carroll, J. (1993). Gene ralized probabilistic LR parsing of natural\nlanguage (corpora) with uniﬁcation-based methods. Computational Linguistics , 19:25–59.\n[Carroll and Briscoe, 1996] Carroll, J. and Briscoe, T. (1996). App ortioning development eﬀort in a probabilistic\nLR parsing system through evaluation. In Proceedings of the Conference on Empirical Methods in Natur al\nLanguage Processing (EMNLP-96) , pages 92–100, University of Pennsylvania.\n[Charniak, 1993] Charniak, E. (1993). Statistical Language Learning . MIT Press, Cambridge, MA.\n[Charniak, 1996] Charniak, E. (1996). Tree-bank grammars. Tec hnical Report Technical Report CS-96-02,\nDept of Computer Science, Brown University.\n[Collins, 1996] Collins, M. J. (1996). A new statistical parser based o n bigram lexical dependencies. In Pro-\nceedings of the 34th Annual Meeting of the Association for Co mputational Linguistics , pages 184–191.\n[Demers, 1977] Demers, A. (1977). Generalized left corner parsin g. In Proceedings of the Fourth Annual ACM\nSymposium on Principles of Programming Languages , pages 170–181.\n[Hindle and Rooth, 1993] Hindle, D. and Rooth, M. (1993). Structur al ambiguity and lexical relations. Com-\nputational Linguistics , 19:103–120.\n[Jelinek et al., 1992] Jelinek, F., Laﬀerty, J. D., and Mercer, R. L. (19 92). Basic methods of probabilistic\ncontext free grammars. In Laface, P. and De Mori, R., editors, Speech Recognition and Understanding:\nRecent Advances, Trends, and Applications , volume 75 of Series F: Computer and Systems Sciences . Springer\nVerlag.\n[Kupiec, 1991] Kupiec, J. (1991). A trellis-based algorithm for estim ating the parameters of a hidden stochastic\ncontext-free grammar. In Proceedings of the Speech and Natural Language Workshop , pages 241–246. DARPA.\n[Lari and Young, 1990] Lari, K. and Young, S. J. (1990). The estim ation of stochastic context-free grammars\nusing the inside-outside algorithm. Computer Speech and Language , 4:35–56.\n[Lauer, 1995] Lauer, M. (1995). Designing Statistical Language Learners: Experiments on N oun Compounds .\nPhD thesis, Macquarie University, Sydney, Australia.\n[Magerman, 1995] Magerman, D. M. (1995). Statistical decision-t ree models for parsing. In Proceedings of the\n33st Annual Meeting of the Association for Computational Li nguistics, pages 276–283.\n[Manning, 1996] Manning, C. D. (1996). Ergativity: Argument Structure and Grammatical Relations . CSLI.\n[Marcus et al., 1993] Marcus, M. P., Santorini, B., and Marcinkiewicz, M . A. (1993). Building a large annotated\ncorpus of English: The Penn treebank. Computational Linguistics , 19:313–330.\n[Rosenkrantz and Lewis II, 1970] Rosenkrantz, S. J. and Lewis II , P. M. (1970). Deterministic left corner parser.\nIn IEEE Conference Record of the 11th Annual Syposium on Switch ing and Automata , pages 139–152.\n[Sankoﬀ, 1971] Sankoﬀ, D. (1971). Branching processes with ter minal types: applications to context-free gram-\nmars. Journal of Applied Probability , 8:233–240.\n[Schabes et al., 1993] Schabes, Y., Roth, M., and Osborne, R. (1993 ). Parsing the Wall Street Journal with the\nInside-Outside algorithm. In Proceedings of the Sixth Conference of the European Chapter of the Association\nfor Computational Linguistics , pages 341–347, University of Utrecht.\n[Stabler, 1994] Stabler, E. P. (1994). The ﬁnite connectivity of ling uistic structure. In Clifton, C., Frazier, L.,\nand Rayner, K., editors, Perspectives on Sentence Processing , pages 303–336. Lawrence Erlbaum, Hillsdale,\nNJ.\n[Suppes, 1970] Suppes, P. (1970). Probabilistic grammars for nat ural languages. Synthese, 22:95–116.",
  "topic": "Treebank",
  "concepts": [
    {
      "name": "Treebank",
      "score": 0.893584132194519
    },
    {
      "name": "Parsing",
      "score": 0.8026217222213745
    },
    {
      "name": "Computer science",
      "score": 0.7602314352989197
    },
    {
      "name": "Probabilistic logic",
      "score": 0.7202585339546204
    },
    {
      "name": "Natural language processing",
      "score": 0.6418521404266357
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6126351356506348
    },
    {
      "name": "Rule-based machine translation",
      "score": 0.5848230719566345
    },
    {
      "name": "Grammar",
      "score": 0.5794654488563538
    },
    {
      "name": "Context (archaeology)",
      "score": 0.415131539106369
    },
    {
      "name": "Programming language",
      "score": 0.35979437828063965
    },
    {
      "name": "Linguistics",
      "score": 0.19097408652305603
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}