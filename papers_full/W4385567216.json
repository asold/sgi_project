{
  "title": "Language Models of Code are Few-Shot Commonsense Learners",
  "url": "https://openalex.org/W4385567216",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2701979523",
      "name": "Aman Madaan",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2107456652",
      "name": "Shuyan Zhou",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2046927040",
      "name": "Uri Alon",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2098495947",
      "name": "Yi-Ming Yang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A277131583",
      "name": "Graham Neubig",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3214026168",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W2970243238",
    "https://openalex.org/W3154186000",
    "https://openalex.org/W135031542",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4206185536",
    "https://openalex.org/W2799002257",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W3150468745",
    "https://openalex.org/W4298149550",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W3200325732",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2963983586",
    "https://openalex.org/W3174524849",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4224060952",
    "https://openalex.org/W1560720302",
    "https://openalex.org/W4226242393",
    "https://openalex.org/W4293791046",
    "https://openalex.org/W4229868159",
    "https://openalex.org/W4282045675",
    "https://openalex.org/W2573974208",
    "https://openalex.org/W4383108457",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3170057763"
  ],
  "abstract": "We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph.To employ large language models (LMs) for this task, existing approaches 'serialize' the output graph as a flat list of nodes and edges.Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all.We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (codex) outperforms natural-LMs that are fine-tuned on the target task (T5) and other strong LMs such as GPT-3 in the few-shot setting.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1384–1403\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nLanguage Models of Code are Few-Shot Commonsense Learners\nAman Madaan♠, Shuyan Zhou♠, Uri Alon♠,\nYiming Yang♠, Graham Neubig♠ †\n♠ Language Technologies Institute, Carnegie Mellon University, USA\n† Inspired Cognition, USA\n{amadaan,shuyanzh,ualon,yiming,gneubig}@cs.cmu.edu\nAbstract\nWe address the general task of structured com-\nmonsense reasoning: given a natural language\ninput, the goal is to generate a graph such as\nan event or a reasoning-graph. To employ large\nlanguage models (LMs) for this task, existing\napproaches “serialize” the output graph as a\nflat list of nodes and edges. Although feasi-\nble, these serialized graphs strongly deviate\nfrom the natural language corpora that LMs\nwere pre-trained on, hindering LMs from gen-\nerating them correctly. In this paper, we show\nthat when we instead frame structured common-\nsense reasoning tasks as code generation tasks,\npre-trained LMs of code are better structured\ncommonsense reasoners than LMs of natural\nlanguage, even when the downstream task does\nnot involve source code at all. We demonstrate\nour approach across three diverse structured\ncommonsense reasoning tasks. In all these\nnatural language tasks, we show that using\nour approach, a code generation LM (CODEX )\noutperforms natural-LMs that are fine-tuned\non the target task ( e.g., T5) and other strong\nLMs such as GPT-3 in the few-shot setting.\nOur code and data are available at https:\n//github.com/madaan/CoCoGen.\n1 Introduction\nThe growing capabilities of large pre-trained lan-\nguage models (LLMs) for generating text have en-\nabled their successful application in a variety of\ntasks, including summarization, translation, and\nquestion-answering (Wang et al., 2019; Raffel et al.,\n2019; Brown et al., 2020; Chowdhery et al., 2022).\nNevertheless, while employing LLMs for natu-\nral language (NL) tasks is straightforward, a ma-\njor remaining challenge is how to leverage LLMs\nfor structured commonsense reasoning, including\ntasks such as generating event graphs (Tandon et al.,\n2019), reasoning graphs (Madaan et al., 2021a),\nscripts (Sakaguchi et al., 2021), and argument ex-\nplanation graphs (Saha et al., 2021). Unlike tradi-\ntional commonsense reasoning tasks such as read-\ning comprehension or question answering, struc-\ntured commonsense aims to generate structured\noutput given a natural language input. This family\nof tasks relies on the natural language knowledge\nlearned by the LLM, but it also requires complex\nstructured prediction and generation.\nTo leverage LLMs, existing structured common-\nsense generation models modify the output format\nof a problem. Specifically, the structure to be gen-\nerated (e.g., a graph or a table) is converted, or\n“serialized”, into text. Such conversions include\n“flattening” the graph into a list of node pairs (Fig-\nure 1d), or into a specification language such as\nDOT (Figure 1c; Gansner et al., 2006).\nWhile converting the structured output into text\nhas shown promising results (Rajagopal et al.,\n2021; Madaan and Yang, 2021), LLMs struggle\nto generate these “unnatural” outputs: LMs are\nprimarily pre-trained on free-form text, and these\nserialized structured outputs strongly diverge from\nthe majority of the pre-training data. Further, for\nnatural language, semantically relevant words are\ntypically found within a small span, whereas neigh-\nboring nodes in a graph might be pushed farther\napart when representing a graph as a flat string.\nThus, a language model which was trained on\nnatural language text is likely to fail to capture\nthe topology of the graph. Consequently, using\nLLMs for graph generation typically requires a\nlarge amount of task-specific training data, and\ntheir generated outputs show structural errors and\nsemantic inconsistencies, which need to be fur-\nther fixed either manually or by using a secondary\ndownstream model (Madaan et al., 2021b).\nDespite these struggles, the recent success of\nlarge-language models ofcode (Code-LLMs; Chen\net al., 2021b; Xu et al., 2022) for tasks such as\ncode generation from natural language (Austin\net al., 2021; Nijkamp et al., 2022), code comple-\ntion (Fried et al., 2022), and code translation (Wang\n1384\nTake the pies out to cool Open cabinet drawer\nTake out several plates\nBegin putting\npies on plate\nFill pies onto\nplates evenly\nServe the potpies on a plate\n(a) The script G\nclass Tree:\ngoal = \"serve the potpies on a plate\"\ndef __init__(self):\n# nodes\ntake_pies_out_to_cool = Node()\nopen_cabinet_drawer = Node()\ntake_out_several_plates = Node()\n...\n# edges\ntake_pies_out_to_cool.children =\n[take_out_several_plates]\nopen_cabinet_drawer.children =\n[take_out_several_plates]\n...\n(b) G converted to Python code Gc using our approach\ndigraph G {\nbegin -> take_pies_out_to_cool;\nbegin -> open_cabinet_drawer;\ntake_pies_out_to_cool ->\ntake_out_several_plates;\nopen_cabinet_drawer ->\ntake_out_several_plates;\ntake_out_several_plates ->\nbegin_putting_pies_on_plates;\nbegin_putting_pies_on_plates ->\nserve_potpies_on_plate;\nfill_pies_onto_plates_evenly ->\nserve_potpies_on_plate;\nserve_potpies_on_plate -> end;\n}\n(c) Straightforward encodings of the graph using the “DOT”\n[\n(take_pies_out_to_cool,\ntake_out_several_plates),\n(open_cabinet_drawer,\ntake_out_several_plates),\n(take_out_several_plates,\nbegin_putting_pies_on_plates),\n(take_out_several_plates,\nfill_pies_onto_plates_evenly),\n(begin_putting_pies_on_plates,\nserve_potpies_on_plate),\n(fill_pies_onto_plates_evenly,\nserve_potpies_on_plate),\n(serve_potpies_on_plate, end)\n]\n(d) Text format, or as a list of edges (node pairs)\nFigure 1: An illustration of COCOGEN for the task of script generation. An input graph (1a) is typically represented\nusing the DOT format (1c) or as a list of edges (1d), which allows modeling the graph using standard language\nmodels. These popular choices are sufficient in principle; however, these formats are loosely structured, verbose, and\nnot common in text corpora, precluding language models from effectively generating them. In contrast, COCOGEN\nconverts structures into Python code (1b), allowing to model them using large-scale language models of code.\net al., 2021), show that Code-LLMs are able to per-\nform complex reasoning on structured data such\nas programs. Thus, instead of forcing LLMs of\nnatural language (NL-LLMs) to be fine-tuned on\nstructured commonsense data, an easier way to\nclose the discrepancy between the pre-training data\n(free-form text) and the task-specific data (com-\nmonsense reasoning graphs) is to adapt LLMs that\nwere pre-trained on code to structured common-\nsense reasoning in natural language.\nThus, our main insight is that large language\nmodels of code are good structured commonsense\nreasoners. Further, we show that Code-LLMs can\nbe even better structured reasoners than NL-LLMs,\nwhen converting the desired output graph into a for-\nmat similar to that observed in the code pre-training\ndata. We call our method COCOGEN: models\nof Code for Commonsense Generation, and it is\ndemonstrated in Figure 1.\nOur contributions are as follows:\n1. We highlight the insight that Code-LLMs\nare better structured commonsense reasoners\nthan NL-LLMs, when representing the desired\ngraph prediction as code.\n2. We propose COCOGEN: a method for\nleveraging LLMs of code for structured\ncommonsense generation.\n3. We perform an extensive evaluation across\nthree structured commonsense generation\ntasks and demonstrate that COCOGEN vastly\noutperforms NL-LLMs, either fine-tuned or\nfew-shot tested, while controlling for the num-\nber of downstream task examples.\n4. We perform a thorough ablation study, which\n1385\nshows the role of data formatting, model size,\nand the number of few-shot examples.\n2 C OCOGEN: Representing\nCommonsense structures with code\nWe focus on tasks of structured commonsense gen-\neration. Each training example for such tasks is\nin the form (T , G), where T is a text input, and G\nis the structure to be generated (typically a graph).\nThe key idea of COCOGEN is transforming an out-\nput graph G into a semantically equivalent program\nGc written in a general-purpose programming lan-\nguage. In this work, we chose Python due to its\npopularity in the training data of modern Code-\nLLMs (Xu et al., 2022), but our approach is ag-\nnostic to the programming language. The code-\ntransformed graphs are similar in their format to\nthe pre-training data of Code-LLMs, and thus serve\nas easier to generalize training or few-shot exam-\nples than the original raw graph. COCOGEN uses\nCode-LLMs to generate Gc given T , which we\neventually convert back into the graph G.\nWe use the task of script generation (PROSCRIPT ,\nFigure 1) as a running example to motivate our\nmethod: script generation aims to create a script (G)\nto achieve a given high-level goal (T ).\n2.1 Converting (T , G) into Python code\nWe convert a (T , G) pair into a Python class or\nfunction. The general procedure involves adding\nthe input text T in the beginning of the code as\na class attribute or descriptive comment, and en-\ncoding the structure G using standard constructs\nfor representing structure in code (e.g., hashmaps,\nobject attributes) or function calls. The goal here is\nto compose Python code that represents a (T , G)\npair, but retains the syntax and code conventions of\ntypical Python code.\nFor example, for the script generation task, we\nconvert the (T , G) pair into a Tree class (Fig-\nure 1b). The goal T is added as class attribute\n(goal), and the script G is added by listing\nthe nodes and edges separately. We first in-\nstantiate the list of nodes as objects of class\nNode. Then, the edges are added as an attribute\nchildren for each node (Figure 1b). For ex-\nample, we instantiate the node “ Take out sev-\neral plates” as take_out_several_plates\n= Node(), and add it as a child of the node\ntake_pies_out_to_cool.\nWhile there are multiple ways of representing\na training example as a Python class, we found\nempirically that this relatively simple format is the\nmost effective, especially with larger models. We\nanalyze the choice of format and its connection\nwith the model size in Section 4.\n2.2 Few-shot prompting for generatingG\nWe focus on large-language models of the scale\nof CODEX (Chen et al., 2021a). Due to their pro-\nhibitively expensive cost to fine-tune, these large\nmodels are typically used in a few-shot prompting\nmode. Few-shot prompting uses k input-output ex-\namples {(xi, yi)}k\ni=1 to create an in-context prompt:\np =x1 ⊕y1 ⋅ x2 ⊕y2 ⋅ . . .⋅ xk ⊕yk, where ⊕\nis a symbol that separates an input from its output,\nand ⋅separates different examples.\nA new (test) input x is appended to the prompt\np (that is: p ⋅ x), and p ⋅ x ⊕is fed to the model\nfor completion. As found by Brown et al. (2020),\nlarge language models show impressive few-shot\ncapabilities in generating a completion ˆy given the\ninput p ⋅ x ⊕. The main question is how to\nconstruct the prompt?\nIn all experiments in this work, the prompt p\nconsists of k Python classes, each representing a\n(T , Gc) pair. For example, for script generation,\neach Python class represents a goal T and a script\nGc from the training set. Given a new goal T for\ninference, a partial Python class (i.e., only specify-\ning the goal) is created and appended to the prompt.\nFigure 2 shows such a partial class. Here, the code\ngeneration model is expected to complete the class\nby generating the definition for Nodeobjects and\ntheir dependencies for the goal make hot green tea.\nclass Tree:\ngoal = \"make hot green tea.\"\ndef __init__(self):\n# generate\nFigure 2: COCOGEN uses a prompt consisting of k (5-\n10) Python classes. During inference, the test input is\nconverted to a partial class, as shown above, appended to\nthe prompt, and completed by a code generation model\nsuch as CODEX .\nIn our experiments, we usedCODEX (Chen et al.,\n2021a) and found that it nearly always generates\nsyntactically valid Python. Thus, the generated\ncode can be easily converted back into a graph\nand evaluated using the dataset’s standard, original,\n1386\nmetrics. Appendix F lists sample prompts for each\nof the tasks we experimented with.\n3 Evaluation\nWe experiment with three diverse structured com-\nmonsense generation tasks: (i) script genera-\ntion ( PROSCRIPT , Section 3.2), (ii) entity state\ntracking (PROPARA , Section 3.3), and (iii) explana-\ntion graph generation (EXPLAGRAPHS , Section 3.4)\nDataset details are included in Appendix D. Despite\nsharing the general goal of structured common-\nsense generation, the three tasks are quite diverse\nin terms of the generated output and the kind of\nrequired reasoning.\n3.1 Experimental setup\nModel As our main Code-LLM for COCOGEN,\nwe experiment with the latest version of CODEX\ncode-davinci-002from OpenAI1 in few-shot\nprompting mode.\nBaselines We experimented with the following\ntypes of baselines:\n1. Text few-shot:Our hypothesis is that code-\ngeneration models can be repurposed to gen-\nerate structured output better. Thus, natural\nbaselines for our approach are NL-LLMs –\nlanguage models trained on natural language\ncorpus. We experiment with the latest ver-\nsions of CURIE (text-curie-001) and\nDAVINCI (text-davinci-002), the two\nGPT-3 based models by OpenAI (Brown\net al., 2020). For both these models, the\nprompt consists of (T , G) examples, where\nG is simply flattened into a string (as in Fig-\nure 1c). DAVINCI is estimated to be much\nlarger in size than CURIE , as our experiments\nalso reveal (Appendix A). DAVINCI , popu-\nlarly known as GPT-3, is the strongest text-\ngeneration model available through OpenAI\nAPIs.2\n2. Fine-tuning: we fine-tune a T5-large model\nfor EXPLAGRAPHS , and use the results from\nSakaguchi et al. (2021) on T5-xxl for PRO -\nSCRIPT tasks. In contrast to the few-shot setup\nwhere the model only has access to a few ex-\namples, fine-tuned models observe the entire\ntraining data of the downstream task.\n1As of June 2022\n2https://beta.openai.com/docs/models/\ngpt-3\nChoice of prompt We created the prompt p by\nrandomly sampling k examples from the training\nset. As all models have a bounded input size (e.g.,\n4096 tokens for CODEX code-davinci-002\nand 4000 for GPT-3 text-davinci-002), the\nexact value of k is task dependent: more examples\ncan fit in a prompt in tasks where (T , G) is short.\nIn our experiments, k varies between 5 and 30, and\nthe GPT-3 baseline is always fairly given the same\nprompts as CODEX . To control for the variance\ncaused by the specific examples selected into p,\nwe repeat each experiment with at least 3 different\nprompts, and report the average. We report the\nmean and standard deviations in Appendix I.\nCOCOGEN: We use COCOGEN to refer to se-\ntups where a CODEX is used with a Python prompt.\nIn Section 4, we also experiment with dynamically\ncreating a prompt for each input example, using\na NL-LLMs with code prompts, and using Code-\nLLMs with textual prompts.\n3.2 Script generation: PROSCRIPT\nGiven a high-level goal ( e.g., bake a cake ), the\ngoal of script generation is to generate a graph\nwhere each node is an action, and edges cap-\nture dependency between the actions (Figure 1a).\nWe use the PROSCRIPT (Sakaguchi et al., 2021)\ndataset, where the scripts are directed acyclic\ngraphs, which were collected from a diverse range\nof sources including ROCStories (Mostafazadeh\net al., 2016), Descript (Wanzare et al., 2016), and\nVirtual home (Puig et al., 2018).\nLet G(V, E) be a script for a high-level goal\nT with node and edge sets V and E, respectively.\nFollowing Sakaguchi et al. (2021), we experiment\nwith two sub-tasks: (i) script generation: gen-\nerating the entire script G(V, E) given a goal T ,\nand (ii) edge prediction:predicting the edge set E\ngiven the nodes V and the goal T .\nFigure 1 shows an input-output example from\nPROSCRIPT , and our conversion of the graph into\nPython code: we convert each node v ∈V into an\ninstance of a Nodeclass; we create the edges by\nadding childrenattribute for each of the nodes.\nAdditional examples are present in Figure 6\nTo represent a sample for edge prediction, we\nlist the nodes in a random order (specified after the\ncomment # nodes in Figure 1b). The model then\ncompletes the class by generating the code below\nthe comment # edges.\n1387\nBLEU ROUGE -L BLEURT ISO GED Avg(d) Avg( ∣V ∣) Avg( ∣E∣)\nG (reference graph) - - 1.00 0.00 1.84 7.41 6.80\nT5 (fine-tuned) 23.80 35.50 -0.31 0.51 1.89 1.79 7.46 6.70\nCURIE (15) 11.40 27.00 -0.41 0.15 3.92 1.47 8.09 6.16\nDAVINCI (15) 23.11 36.51 -0.27 0.64 1.44 1.74 7.58 6.59\nCOCOGEN (15) 25.24 38.28 -0.26 0.53 2.10 1.79 7.44 6.70\nTable 1: Semantic and structural metrics for the script generation task on PROSCRIPT . T5 is fine-tuned on the entire\ndataset, while the few-shot models (CURIE , DAVINCI , CODEX ) use 15 examples in the prompt.\nMethod prec rec F 1\nfine-tuned\nT5 (100) 52.26 52.91 51.89\nT5 (1k) 60.55 61.24 60.15\nT5 (4k) 75.71 75.93 75.72\nfew-shot\nCURIE (15) 10.19 11.61 10.62\nDAVINCI (15) 50.62 49.30 48.92\nCOCOGEN (15) 57.34 55.44 56.24\nTable 2: Precision, recall, and F1 for PROSCRIPT edge-\nprediction task. COCOGEN with 15 samples outper-\nforms strong few-shot models, and T5 trained on 1k\nsamples.\nScript Generation metrics We denote the script\nthat was generated by the model as ˆG, and evaluate\nˆG vs. G for both semantic and structural similar-\nity. To evaluate semantic similarity, we useBLEU ,\nROUGE -L, and the learned metric BLEURT to de-\ntermine the content overlap. Following Sakaguchi\net al. (2021), we use the following metrics for struc-\ntural evaluation of generated scripts:\n• Graph edit distance ( GED ): the number of\nrequired edits (node/edge removal/additions)\nto transform ˆG to G (Abu-Aisheh et al., 2015);\n• Graph isomorphism ( ISO ; Cordella et al.,\n2001): determines whether ˆG and G are iso-\nmorphic based on their structure, disregarding\nthe textual content of nodes;\n• Graph size: average number of nodes and\nedges, (∣G(V )∣, ∣G(E)∣, ∣ˆG(V )∣, ∣ˆG(V ))\nand the average degree ( d(G(V ))), where\nthe high-level goal is for ˆG to have as close\nmeasures to G as possible.\nEdge Prediction metrics For the edge prediction\ntask, the set of nodes is given, and the goal is to pre-\ndict the edges between them. Following Sakaguchi\net al. (2021), we measure precision, recall, and F1\ncomparing the true and predicted edges. Specifi-\ncally, p = ∣E∩ˆE∣\n∣ ˆE∣ , r = ∣E∪ˆE∣\n∣E∣ , and F1 = 2pr\np+r.\nResults Table 1 shows the results for script gener-\nation. The main results are that COCOGEN (based\non CODEX ), with just 15 prompt examples, outper-\nforms the fine-tuned model T5 which has been fine-\ntuned on all 3500 samples. Further, COCOGEN\noutperforms the few-shot NL-LM CURIE across\nall semantic metrics and structural metrics. COCO-\nGEN outperforms DAVINCI across all semantic met-\nrics, while DAVINCI performs slightly better in two\nstructural metrics.\nTable 2 shows the results for edge predic-\ntion: COCOGEN significantly outperforms the NL-\nLLMs CURIE and DAVINCI . When comparing with\nT5, which was fine-tuned, COCOGEN with only 15\nexamples outperforms the fine-tuned T5 which was\nfine-tuned on 100 examples. The impressive per-\nformance in the edge-generation task allows us to\nhighlight the better ability of COCOGEN in captur-\ning structure, while factoring out all models’ ability\nto generate the NL content.\n3.3 Entity state tracking: PROPARA\nThe text inputs T of entity state tracking are a\nsequence of actions in natural language about a par-\nticular topic (e.g., photosynthesis) and a collection\nof entities (e.g., water). The goal is to predict the\nstate of each entity after the executions of an action.\nWe use the PROPARA dataset (Dalvi et al., 2018) as\nthe test-bed for this task.\nWe construct the Python code Gc as follows, and\nan example is shown in Figure 3. First, we de-\nfine the main function and list all n actions as\ncomments inside the mainfunction. Second, we\ncreate k variables named as state_kwhere k is\nthe number of participants of the topic. The seman-\ntics of each variable is described in the comments\nas well. Finally, to represent the state change af-\nter each step, we define n functions where each\nfunction corresponds to an action. We additionally\ndefine an init function to represent the initial-\n1388\nAction Entity\nwater light CO2\nInitial states soil sun -\nRoots absorb\nwater from soil roots sun ?\nThe water flows\nto the leaf leaf sun ?\ndef main():\n# init\n# roots absorb water from soil\n# the water flows to the leaf\n# state_0 tracks the location/state water\n# state_1 tracks the location/state light\n# state_2 tracks the location/state CO2\ndef init():\nstate_0 = \"soil\"\nstate_1 = \"sun\"\nstate_2 = None\ndef roots_absorb_water_from_soil():\nstate_0 = \"roots\"\nstate_1 = \"sun\"\nstate_2 = \"UNK\"\ndef water_flows_to_leaf():\nstate_0 = \"leaf\"\nstate_1 = \"sun\"\nstate_2 = \"UNK\"\nFigure 3: A PROPARA example (left) and its corresponding Python code (right). We use a string to represent a\nconcrete location (e.g., soil), UNKto represent an unknown location, and Noneto represent non-existence.\nModel prec rec F 1\nCURIE 95.1 22.3 36.1\nDAVINCI 75.5 47.1 58.0\nCOCOGEN 80.0 53.6 63.0\nTable 3: 3-shots results on PROPARA . All numbers\nare averaged among five runs with different randomly\nsampled prompts. COCOGEN significantly outperforms\nCURIE and DAVINCI .\nization of entity states. Inside each function, the\nvalue of each variable tells the state of the corre-\nsponding entity after the execution of that action.\nGiven a new test example where only the actions\nand the entities are give, we construct the input\nstring until the initfunction, and we append it to\nthe few-shot prompts for predictions.\nMetrics We follow Dalvi et al. (2018) and mea-\nsure precision, recall and F1 score of the predicted\nentity states. We randomly sampled three examples\nfrom the training set as the few-shot prompt.\nResults As shown in Table 3, COCOGEN\nachieves a significantly better F1 score than\nDAVINCI . Across the five prompts, COCOGEN\nachieves 5.0 higher F1 than DAVINCI on aver-\nage. In addition, COCOGEN yields stronger perfor-\nmance than CURIE , achieving F1 of 63.0, which is\n74% higher than CURIE (36.1).3\nIn PROPARA , COCOGEN will be ranked 6th on\n3CURIE often failed to produce output with the desired\nformat, and thus its high precision and low recall.\nthe leaderboard.4 However, all the methods above\nCOCOGEN require fine-tuning on the entire train-\ning corpus. In contrast, COCOGEN uses only 3\nexamples in the prompt and has a gap of less than\n10 F1 points vs. the current state-of-the-art (Ma\net al., 2022). In the few-shot settings, COCOGEN\nis state-of-the-art in PROPARA .\n3.4 Argument graph generation:\nEXPLAGRAPHS\nGiven a belief (e.g., factory farming should not be\nbanned) and an argument ( e.g., factory farming\nfeeds millions), the goal of this task is to generate\na graph that uses the argument to either support\nor counter the belief (Saha et al., 2021). The text\ninput to the task is thus a tuple of (belief, argument,\n“supports”/“counters”), and the structured output is\nan explanation graph (Figure 4).\nWe use the EXPLAGRAPHS dataset for this\ntask (Saha et al., 2021). Since we focus on generat-\ning the argument graph, we take the stance as given\nand use the stance that was predicted by a stance\nprediction model released by Saha et al..\nTo convert an EXPLAGRAPHS to Python, the\nbelief, argument, and stance are instantiated as\nstring variables. Next, we define the graph struc-\nture by specifying the edges. Unlike PROSCRIPT ,\nthe edges in EXPLAGRAPHS are typed. Thus,\neach edge is added as an add_edge(source,\nedge_type, destination) function call.\nWe also list the starting nodes in a list instantiated\n4As of 10/11/2022, https://leaderboard.\nallenai.org/propara/submissions/public\n1389\nFactory\nFarming Millions\nFoodNecessary\nBanned\ncauses\nhas context desires\nhas context\nnot desires\nclass ExplanationDAG:\ndef __init__(self):\nbelief = \"factory farming should not be\nbanned.\"\nargument = \"Factory farming feeds millions.\"\nstance = \"support\"\n# Edges\nbegin = [\"factory farming\", \"millions\"]\nadd_edge(\"factory farming\", \"causes\", \"food\")\nadd_edge(\"factory farming\", \"has context\",\n\"necessary\")\nadd_edge(\"food\", \"has context\", \"necessary\")\nadd_edge(\"necessary\", \"not desires\", \"banned\")\nadd_edge(\"millions\", \"desires\", \"food\")\nFigure 4: An explanation graph (left) and the corresponding Python code (right)\nStCA (↑) SeCA ( ↑) G-BS ( ↑) GED (↓) EA ( ↑)\nfine-tuned T5 (150) 12.56 6.03 9.54 91.06 7.77\nT5 (1500) 38.19 21.86 29.37 73.09 23.41\nT5 (2500) 43.22 29.65 33.71 69.14 26.38\nfew-shot\nCURIE (30) 5.03 1.26 3.95 96.74 2.60\nDAVINCI (30) 23.62 10.80 18.46 83.83 11.84\nCOCOGEN (30) 45.20 23.74 34.68 68.76 23.58\nTable 4: Results for EXPLAGRAPHS (eval split). COCOGEN with only 30 examples outperforms the T5 model\nwhich was fine-tuned on 1500 examples, across all metrics.\nwith a begin variable (Figure 4). Given a test\nexample, we construct the input until the line of #\nEdgesand let a model complete the remaining.\nMetrics We use the metrics defined by Saha et al.\n(2021) (see Section 6 of Saha et al. (2021) for\na detailed description of the mechanisms used to\ncalculate these metrics):\n• Structural accuracy (StCA): fraction of graphs\nthat are connected DAGs with two concepts\neach from belief and the argument.\n• Semantic correctness (SeCA): a learned metric\nthat evaluates if the correct stance is inferred\nfrom a (belief, graph) pair.\n• G-BERTScore (G-BS): measures BERTscore-\n(Zhang et al., 2020) based overlap between gen-\nerated and reference edges .\n• GED (GED ): avg. edits required to transform\nthe generated graph to the reference graph.\n• Edge importance accuracy (EA): measures the\nimportance of each edge in predicting the target\nstance. A high EA implies that each edge in\nthe generated output contains unique semantic\ninformation, and removing any edge will hurt.\nResults Table 4 shows that COCOGEN with only\n30 examples outperforms the T5 model that was\nfine-tuned using 1500 examples, across all metrics.\nFurther, COCOGEN outperforms the NL-LLMs\nDAVINCI and CURIE with a text-prompt across all\nmetrics by about 50%-100%.\n4 Analysis\nIn this section, we analyze the effect of three im-\nportant components of COCOGEN: (i) the con-\ntributions of Code-LLMs and structured prompt\nGc; (ii) the selection of examples in the in-context\nprompt; and (iii) the design of the Python class.\nStructured Prompts vs. Code-LLMs Which\ncomponent is more important, using a Code-LLMs\nor the structured formatting of the input as code?\nTo answer this, we experimented with a text prompt\nwith a Code-LLM CODEX , and a code prompt with\nan NL-LLM, DAVINCI . Table 5 shows that both\ncontributions are indeed important: performance\nimproves for the NL-LLM DAVINCI both when we\nuse a code prompt, and when we use a Code-LLM.\nHowever when using both a Code-LLM and a code\nprompt – the improvement is greater than the sum\n1390\nEXPLAGRAPHS PROSCRIPT (edge-prediction)\nStCA (↑) SeCA ( ↑) G-BS ( ↑) GED (↓) EA ( ↑) p r F 1\nDAVINCI + text 33.16 7.14 25.91 77.45 15.9 43.06 41.52 43.06\nDAVINCI + code 33.00 15.37 26.15 76.91 16.68 50.62 48.27 49.3\nCODEX + text 38.02 18.23 29.46 73.68 19.54 45.31 43.95 44.47\nCOCOGEN (CODEX + code) 45.20 23.74 34.68 68.76 23.58 57.34 55.44 56.52\nTable 5: Teasing apart the contributions of a code generation model and a structured prompt. The experiments show\nthat both are helpful. DAVINCI , a text generation model, shows marginal improvements with a code prompt (top two\nrows). Similarly, CODEX , a code generation model, significantly benefits from a code prompt. Overall, CODEX\nwith code prompt performs better than the alternatives, across all metrics.\nof each of these solely.\nDynamic prompt selection The prompts for all\nexperiments in Section 3 were created by random\nsampling of examples from the training set. Specif-\nically, a set of k (T , G) pairs are sampled and con-\ncatenated into a prompt p, which we used for infer-\nence over all examples xtest in the test set. As an\nalternative to creating prompts, there is now a grow-\ning interest in customizing the in-context examples\neach example xtest. Popular techniques typically\ntrain a retriever, which is used to fetch the closest\nexamples (Liu et al., 2021; Rubin et al., 2021; Poe-\nsia et al., 2021). We also experimented with such\ndynamic creation of the prompt, that depends on\nthe particular test example. Specifically, following\nPoesia et al. (2021), we performed knowledge sim-\nilarity tuning (KST ): we trained a retriever model\nto retrieve the k closest examples for a given input.\nSetup p r F1\nCOCOGEN 57.34 55.44 56.52\nCOCOGEN + KST 67.11 64.57 65.71\nTable 6: Our retrieval mechanism is highly effective for\nedge prediction: the closest examples are from similar\ndomains and the model is able to leverage the informa-\ntion for better performance.\nThe results indicate that the efficacy of dynamic\nprompts depends on both the training data and task.\nIn the edge-prediction sub-task of PROSCRIPT ,\nedges between events in similar scripts are help-\nful, and Table 6 shows that the model was able to\neffectively leverage this information. In the script\ngeneration sub-task of PROSCRIPT , Table 8 shows\nthat KST provides gains as well (Appendix B).\nIn EXPLAGRAPHS , we observed that the train-\ning data had multiple examples which were nearly\nidentical, and thus dynamically created prompts\noften included such duplicate examples, effectively\nreducing diversity and prompt size (Table 9).\nPython Formatting We performed an extensive\nstudy of the effect of the Python format on the\ndownstream task performance in Appendix G. We\nfind that: (i) there are no clear task-agnostic Python\nclass designs that work uniformly well; and that\n(ii) larger models are less sensitive to prompt\n(Python class) design. In general, our approach\nbenefits the most from code formats that as similar\nas possible to the conventions of typical code.\nHuman evaluation We conduct human evalua-\ntion of the graphs generated by COCOGEN and\nDAVINCI to supplement automated metrics. The re-\nsults (Appendix C) indicate that human evaluation\nis closely correlated with the automated metrics:\nfor EXPLAGRAPHS , graphs generated by COCO-\nGEN are found to be more relevant and correct. For\nPROSCRIPT generation, both DAVINCI and COCO-\nGEN have complementary strengths, but COCO-\nGEN is generally better in terms of relevance.\n5 Related work\nStructured commonsense reasoning using LLMs\nExisting methods for structured commonsense\ngeneration typically flatten the output graphs as\nstrings (Madaan and Yang, 2021; Madaan et al.,\n2021a; Sakaguchi et al., 2021). Consequently,\nthese methods struggle with generation of well-\nformed outputs (Sakaguchi et al., 2021; Madaan\net al., 2021b). In contrast, we address the problem\nof structured generation by (1) translating the task\ninto Python code, and (2) generating code using\nlarge-code generation models.\nCode representation for procedural knowledge\nreasoning Programs inherently encode rich struc-\ntures, and they can efficiently represent task proce-\ndures. Existing works leverage the control-flows,\nnested functions and API calls of a programming\nlanguage such as Python to control the situated\nagents in the embodied environment (Sun et al.,\n1391\n2019; Zhou et al., 2022; Singh et al., 2022). In\nthis work, we go beyond these procedural tasks\nand show the effectiveness of using Code-LLMs\non broader structured commonsense tasks.\nAdapting Code-LLMs for reasoningAs code-\ngeneration models (Code-LLMs) are getting in-\ncreasingly popular, there is a growing interest in\nadapting them for a wide range reasoning tasks.\nWu et al. (2022) use CODEX and PaLM (Chowdh-\nery et al., 2022) for converting mathematical state-\nments written in natural language into a formal\nstructure that can be used for theorem provers, with\nmoderate success. The task is challenging, as it\ninvolves understanding the concepts used in the\ntheorem (e.g., set of real numbers) and the complex\nrelationship between them. Our work is similar in\nspirit to Wu et al. (2022), and seeks to leverage\nthe dual abilities of Code-LLMs for text and sym-\nbolic reasoning. However, differently from their\nwork, we close the gap between the pre-training\ndata and our tasks by translating our output into\nPython code. As our experiments show, this step is\ncrucial in outperforming text-only and fine-tuned\nmodels. To the best of our knowledge, our work is\nthe first to transform a natural-language reasoning\nproblem into code to successfully leverage code\ngeneration methods.\nSymbolic reasoning using LLMs The use of\nprogramming languages like LISP (Tanimoto,\n1987) and Prolog (Colmerauer and Roussel, 1996)\nto process natural language has a long history in\nAI. However, the recent progress in large language\nmodels has obviated the need for specialized meth-\nods for symbolic processing. Cobbe et al. (2021)\nand Chowdhery et al. (2022) address middle-school\nlevel algebra problem solving using large-language\nmodels in a few-shot setup. These problems require\na model to understand the order in which a set of\noperations should be performed over symbols (typ-\nically small integers). In contrast, structured com-\nmonsense reasoning requires broader information\nthan supplied in the prompt, while utilizing the\nmodels’ structural generation capabilities for gen-\nerating output effectively. Thus, the tasks in our\nwork push a model to use both its reasoning and\nsymbolic manipulation capabilities.\n6 Conclusion\nWe present the first work to employ large language\nmodels of code for structured commonsense gen-\neration. By converting the output commonsense\nstructures to Python code, COCOGEN provides\na simple and effective method for leveraging the\ncode-generation abilities of Code-LLMs for struc-\ntured generation. These results open a promising\ndirection for structural commonsense reasoning.\nWe believe that the principles and the methods pre-\nsented in this paper are applicable to additional\nNLP tasks that require “language understanding”\nand structured prediction.\nAcknowledgments\nWe thank Kaixin Ma, Keisuke Sakaguchi and Niket\nTandon for thoughtful discussion and helping with\nPROSCRIPT datasets and the anonymous review-\ners for valuable feedback. This material is partly\nbased on research sponsored in part by the Air\nForce Research Laboratory under agreement num-\nber FA8750-19-2-0200. The U.S. Government is\nauthorized to reproduce and distribute reprints for\nGovernmental purposes notwithstanding any copy-\nright notation thereon. The views and conclusions\ncontained herein are those of the authors and should\nnot be interpreted as necessarily representing the\nofficial policies or endorsements, either expressed\nor implied, of the Air Force Research Laboratory\nor the U.S. Government. This project was also\npartially supported by a gift from AWS AI.\nLimitations\nSome experiments in this work are performed with\nlanguage models that are not open-sourced, namely\nDAVINCI , CURIE , and CODEX . Existing documen-\ntation (Brown et al., 2020; Chen et al., 2021b) does\nnot fully describe the details of these models, such\nas the pretraining corpus, model size, and model\nbiases. Therefore, we can only provide educational\nguesses on these details (analysis in Appendix A).\nIn addition, even though CODEX is free to use for\nresearch as of June 2022, we are unsure whether the\nresearch community will continue to have free ac-\ncess in the future. Nonetheless, we release our code\nand model outputs to ensure the reproducibility of\nour work. Furthermore, in cases where the models\nwe experiment with reveal any issue, the publicly\navailable code will allow future investigations.\nAnother limitation of our work is that we exclu-\nsively experiment with datasets in English. Explor-\ning the efficacy of structured generation methods\nin cross-lingual settings is an interesting and im-\nportant future work.\n1392\nReferences\nZeina Abu-Aisheh, Romain Raveaux, Jean-Yves Ramel,\nand Patrick Martineau. 2015. An exact graph edit\ndistance algorithm for solving pattern recognition\nproblems. In An exact graph edit distance algorithm\nfor solving pattern recognition problems.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welin-\nder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. 2021a. Eval-\nuating Large Language Models Trained on Code.\narXiv:2107.03374 [cs]. ArXiv: 2107.03374.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde, Jared Kaplan, Harri Edwards, Yura\nBurda, Nicholas Joseph, Greg Brockman, et al.\n2021b. Evaluating large language models trained\non code. arXiv preprint arXiv:2107.03374.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar-\nian, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. 2021. Training veri-\nfiers to solve math word problems. arXiv preprint\narXiv:2110.14168.\nAlain Colmerauer and Philippe Roussel. 1996. The\nbirth of prolog. In History of programming\nlanguages—II, pages 331–367.\nLuigi Pietro Cordella, Pasquale Foggia, Carlo Sansone,\nand Mario Vento. 2001. An improved algorithm for\nmatching large graphs. In 3rd IAPR-TC15 workshop\non graph-based representations in pattern recogni-\ntion, pages 149–159.\nBhavana Dalvi, Lifu Huang, Niket Tandon, Wen-tau\nYih, and Peter Clark. 2018. Tracking state changes in\nprocedural text: a challenge dataset and models for\nprocess paragraph comprehension. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 1595–1604, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. 2022. Incoder:\nA generative model for code infilling and synthesis.\narXiv preprint arXiv:2204.05999.\nEmden Gansner, Eleftherios Koutsofios, and Stephen\nNorth. 2006. Drawing graphs with dot.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021. What\nMakes Good In-Context Examples for GPT-$3$?\narXiv:2101.06804 [cs]. ArXiv: 2101.06804.\nKaixin Ma, Filip Ilievski, Jonathan Francis, Eric Ny-\nberg, and Alessandro Oltramari. 2022. Coalescing\nglobal and local information for procedural text un-\nderstanding. arXiv preprint arXiv:2208.12848.\nAman Madaan, Dheeraj Rajagopal, Niket Tandon, Yim-\ning Yang, and Eduard Hovy. 2021a. Could you give\nme a hint ? generating inference graphs for defeasible\nreasoning. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n5138–5147, Online. Association for Computational\nLinguistics.\nAman Madaan, Niket Tandon, Dheeraj Rajagopal, Pe-\nter Clark, Yiming Yang, and Eduard Hovy. 2021b.\nThink about it! improving defeasible reasoning by\nfirst modeling the question scenario. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 6291–6310.\nAman Madaan and Yiming Yang. 2021. Neural lan-\nguage modeling for contextualized temporal graph\ngeneration. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 864–881, Online. Association\nfor Computational Linguistics.\n1393\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A cor-\npus and evaluation framework for deeper under-\nstanding of commonsense stories. arXiv preprint\narXiv:1604.01696.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. A conversational paradigm for program\nsynthesis. arXiv preprint.\nGabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari,\nGustavo Soares, Christopher Meek, and Sumit Gul-\nwani. 2021. Synchromesh: Reliable code generation\nfrom pre-trained language models. In International\nConference on Learning Representations.\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li,\nTingwu Wang, Sanja Fidler, and Antonio Torralba.\n2018. Virtualhome: Simulating household activities\nvia programs. In 2018 IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2018,\nSalt Lake City, UT, USA, June 18-22, 2018 , pages\n8494–8502. IEEE Computer Society.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nDheeraj Rajagopal, Aman Madaan, Niket Tandon, Yim-\ning Yang, Shrimai Prabhumoye, Abhilasha Ravichan-\nder, Peter Clark, and Eduard Hovy. 2021. Curie:\nAn iterative querying approach for reasoning about\nsituations.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nOhad Rubin, Jonathan Herzig, and Jonathan Be-\nrant. 2021. Learning To Retrieve Prompts for In-\nContext Learning. arXiv:2112.08633 [cs]. ArXiv:\n2112.08633.\nSwarnadeep Saha, Prateek Yadav, Lisa Bauer, and Mohit\nBansal. 2021. ExplaGraphs: An Explanation Graph\nGeneration Task for Structured Commonsense Rea-\nsoning. arXiv:2104.07644 [cs]. ArXiv: 2104.07644.\nKeisuke Sakaguchi, Chandra Bhagavatula, Ronan\nLe Bras, Niket Tandon, Peter Clark, and Yejin Choi.\n2021. proScript: Partially Ordered Scripts Genera-\ntion. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 2138–2149,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit\nGoyal, Danfei Xu, Jonathan Tremblay, Dieter\nFox, Jesse Thomason, and Animesh Garg. 2022.\nProgprompt: Generating situated robot task plans\nusing large language models. arXiv preprint\narXiv:2209.11302.\nShao-Hua Sun, Te-Lin Wu, and Joseph J Lim. 2019.\nProgram guided agent. In International Conference\non Learning Representations.\nNiket Tandon, Bhavana Dalvi, Keisuke Sakaguchi, Pe-\nter Clark, and Antoine Bosselut. 2019. WIQA: A\ndataset for “what if...” reasoning over procedural text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 6076–\n6085, Hong Kong, China. Association for Computa-\ntional Linguistics.\nSteven L Tanimoto. 1987. The elements of artificial\nintelligence: an introduction using LISP. Computer\nScience Press, Inc.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. In Advances in Neural Information\nProcessing Systems 32: Annual Conference on Neu-\nral Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada,\npages 3261–3275.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH\nHoi. 2021. Codet5: Identifier-aware unified\npre-trained encoder-decoder models for code un-\nderstanding and generation. arXiv preprint\narXiv:2109.00859.\nLilian D. A. Wanzare, Alessandra Zarcone, Stefan\nThater, and Manfred Pinkal. 2016. A crowdsourced\ndatabase of event sequence descriptions for the acqui-\nsition of high-quality script knowledge. In Proceed-\nings of the Tenth International Conference on Lan-\nguage Resources and Evaluation (LREC’16), pages\n3494–3501, Portorož, Slovenia. European Language\nResources Association (ELRA).\nYuhuai Wu, Albert Q Jiang, Wenda Li, Markus N\nRabe, Charles Staats, Mateja Jamnik, and Christian\nSzegedy. 2022. Autoformalization with large lan-\nguage models. arXiv preprint arXiv:2205.12615.\nFrank F Xu, Uri Alon, Graham Neubig, and Vin-\ncent J Hellendoorn. 2022. A systematic evaluation\nof large language models of code. arXiv preprint\narXiv:2202.13169.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\nating text generation with BERT. In8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\n1394\nShuyan Zhou, Pengcheng Yin, and Graham Neubig.\n2022. Hierarchical control of situated agents through\nnatural language. In Workshop on Structured and\nUnstructured Knowledge Integration (SUKI), Seattle,\nUSA.\n1395\nA Few-shot models size estimates\nAs OpenAI has not released any details of the size\nof their few-shot models, we estimate the relative\nstrengths and weaknesses on code and text gen-\neration by calculating the average loss per token.\nTo calculate the avg. loss of each of these mod-\nels on code, we use the implementation provided\nby Xu et al. (2022). 5 The perplexity on text cor-\npus was evaluated on 30 random wikipedia pages\nfrom Wikiplots6 following a similar procedure The\nstructure and text generation capabilities of the\nmodels are apparent from the results in Table 7;\nDAVINCI outperforms CODEX on text generation\nbut is worse on code-generation and vice-versa.\nCURIE underperforms both DAVINCI and CODEX\nsignificantly. Importantly, these results show that\nCODEX and DAVINCI are of comparable capacities,\nmaking their comparison fair.\nModel CODE TEXT\nCODEX 0.46 2.71\nDAVINCI 0.63 2.25\nCURIE 1.17 3.32\nTable 7: Average loss per token of the three few-shot\nmodels used in this work. TEXT refers to the average\nloss over 30 Wikipedia pages, and CODE is the loss over\nPython scripts in the evaluation split of Polycoder.\nB Dynamic prompt Creation\nAs an alternative to creating prompts, there is now\na growing interest in customizing the in-context\nexamples each example Ttest. Popular techniques\ntypically train a retriever, which is used to fetch\nthe examples in the training set that are closest to\nTtest (Liu et al., 2021; Rubin et al., 2021; Poesia\net al., 2021).\nSpecifically Poesia et al. (2021) train a retriever\nwith a target-similarity tuning (TST) objective over\na corpus of D of (x, y) examples. TST learns an\nembedding function f such that for a pair of exam-\nples (xi, yi)and (xj, yj), if yi ∼yj ⟹ f(xi) ∼\nf(xj). For a new x, f(x) is used to retrieve the\nclosest examples from D.\nWe follow Poesia et al. (2021), and train a\nknowledge-similarity tuner (KST ). We use mpnet-\n5https://github.com/VHellendoorn/\nCode-LMs#evaluation\n6https://github.com/markriedl/\nWikiPlots\nbase7 with SentenceTransformers (Reimers and\nGurevych, 2019) to fine-tune a retrieval function f\nby minimizing the following loss:\nLθ =(cos(fθ(Ti), fθ(Tj))−sim(Gi, Gj))2\n(1)\nwhere fθ is parameterized using a transformer.\nResults on using KST with PROSCRIPT (Table 8)\nand EXPLAGRAPHS (Table 9). While KST is highly\neffective for edge-prediction 6, the results are\nmixed for EXPLAGRAPHS and PROSCRIPT . For\nPROSCRIPT , KST yields marginal gains. However,\nfor EXPLAGRAPHS , a number of training examples\nhave overlapping theme (Table 10), and thus cre-\nating a prompt dynamically reduces the effective\ninformation in the prompt.\nC Human Evaluation\nOut of the four tasks used in this work, PROSCRIPT\nedge prediction and PROPARA have only one possi-\nble correct value. Thus, following prior work, we\nreport the automated, standard metrics for these\ntasks. For EXPLAGRAPHS , we use model-based\nmetrics proposed by Saha et al. (2021), which were\nfound to have a high correlation with human judg-\nments. For PROSCRIPT graph generation, we con-\nducted an exhaustive automated evaluation that sep-\narately scores the correctness of the nodes and the\ncorrectness of the edges.\nHowever, automated metrics are limited in their\nability to evaluate model-generated output. Thus,\nto further investigate the quality of outputs, we\nconduct a human evaluation to compare the out-\nputs generated by COCOGEN and DAVINCI . We\nsampled 20 examples, and three of the authors per-\nformed the evaluation. Annotators were shown two\ngraphs (generated by COCOGEN and DAVINCI )\nand were asked to select one they thought was bet-\nter regarding relevance and correctness. The se-\nlection for each criterion was made independently:\nthe same graph could The annotations were done\nseparately: the same graph could have more rele-\nvant nodes (higher relevance) but may not be cor-\nrect. The identity of the model that generated each\ngraph (COCOGEN or DAVINCI ) was shuffled and\nunknown to the evaluators.\nThe results in Table 11 indicate that human eval-\nuation is closely correlated with the automated\nmetrics: for EXPLAGRAPHS , annotators found the\n7https://huggingface.co/microsoft/\nmpnet-base\n1396\nISO GED Avg(d) Avg( ∣V ∣) Avg( ∣E∣) BLEU ROUGE -L BLEURT\nG 1.0 0.0 1.84 7.41 6.8 - - - -\nCOCOGEN + 002 (15) 0.53 2.1 1.79 7.44 6.7 25.24 38.28 -0.26\nCOCOGEN + 002 (15) + KST 0.52 1.99 1.8 7.45 6.7 25.4 38.4 -0.25\nTable 8: KST on PROSCRIPT generation: Dynamically creating a prompt leads to marginal improvements.\nStCA (↑) SeCA ( ↑) G-BS ( ↑) GED (↓) EA ( ↑)\nCOCOGEN + 002 45.2 23.74 34.68 68.76 23.58\nCOCOGEN + 002 + KST 37.47 18.46 29.41 73.76 19.15\nTable 9: KST on EXPLAGRAPHS : We find that EXPLAGRAPHS contains multiple examples that are similar to each\nother in the training set. Thus, dynamically creating a prompt by selecting examples that are closest to the input\nactually hurts performance.\nbelief : all religions need to be respected, and able to practice. argument: religion is behind many\nwars.\nbelief : every religion needs to be respected and allowed to be practiced. argument: religion is\nbehind most wars.\nbelief : school prayer should not be allowed. argument: many people would prefer to keep\nreligion out of their lives.\nbelief : people should follow whichever religion they choose.argument: this country has freedom\nof religion.\nbelief : people are free to practice the religion they chooseargument: society’s right to be free to\npractice religion should not be limited.\nbelief : the church of scientology should be allowed, because everyone has a right to follow their\nreligion. argument: the church of scientology doesn’t have a religious doctrine.\nbelief : we should avoid discussing religion in schools.argument: some schools are religious in\nnature, and have regular discussions on the topic.\nbelief : freedom of religion is paramount. argument: not all religions are worth it.\nbelief : people don’t follow the same religion. argument: the world has many different religions.\nbelief : people should follow whatever religion they desire. argument: people have the right to\nadhere to the religion of their choice.\nbelief : people should follow whichever religion they choose. argument: some religions are better\nthan others.\nbelief : people should be able to practice whatever religion they choose.argument: some religions\nare not okay to pursue.\nbelief : students have a right to express themselves any way possible, including faith. argument:\nreligion is a personal choice.\nbelief : people should be able to do missionary work if they desire. argument: people should\nhave right to missionary work.\nbelief : students are free to express faith. argument: one should go to church to express their\nreligious beliefs.\nTable 10: The closest examples in the training set corresponding to the test input: belief : religion causes many\nfights. and argument: There would be less fights without religious conflicts.. As the table shows, the examples are\noverlapping which reduces the diversity in the prompt, effectively reducing the number of examples in a prompt\ncreating using nearest neighbors (Section 4.\nDataset C OCOGEN DAVINCI No preference\nRelevance EXPLAGRAPHS 28.3% 16.7% 46.7%\nPROSCRIPT (script generation) 26.7% 18.3% 55%\nCorrectness EXPLAGRAPHS 38.3% 18.3% 31.7%\nPROSCRIPT (script generation) 26.7% 23.3% 50%\nTable 11: Human evaluation of graphs generated by COCOGEN and DAVINCI . The evaluators were shown graphs\ngenerated by COCOGEN and DAVINCI , and were asked to select one that is more relevant to the input and correct.\nIn case of no preference, the evaluators could pick the No preference. The table shows the % of times graphs from\neach model were preferred.\n1397\ngraphs generated by COCOGEN to be more rele-\nvant and correct. We find that DAVINCI often fails\nto recover semantic relations between nodes in the\nargument graphs. For example, consider a belief\n(B) urbanization harms natural habitats for the an-\nimals in the world. We want to generate a graph\nthat can counter this belief with the argument (A)\nurbanization causes increase in jobs.\nFor the same prompt, COCOGEN generated (ur-\nbanization; causes; increase in jobs); (increase\nin jobs; has context; good); (good; not capa-\nble of; harms) whereas DAVINCI generated (jobs;\nnot harms; natural habitats) →(natural habitats;\nnot part of; animals). Note that DAVINCI suc-\ncessfully recovered relevant events (“natural habi-\ntat” “animals”) but arranged them in incorrect rela-\ntions. For PROSCRIPT , the human evaluation shows\nthat COCOGEN and DAVINCI have complementary\nstrengths, while COCOGEN generally produces\nmore relevant and correct outputs.\nD Dataset statistics\nDataset statistics are shown in Table 12. The test\nsplit for EXPLAGRAPHS is not available, so we\nevaluate on the validation split. For PROSCRIPT ,\nwe obtained the test splits from the authors.\nCorpus #Train #Val #Test\nPROSCRIPT 3252 1085 2077\nPROPARA 387 43 54\nEXPLAGRAPHS 2368 398 -\nTable 12: Corpus Statistics for the tasks used in this\nwork.\nE Sample outputs\nSample outputs from COCOGEN for all the\ntasks are located at https://github.com/\nmadaan/CoCoGen/tree/main/outputs.\nRepresentative examples from each task are\npresented in Figure 5. Surprisingly, COCO-\nGEN (CODEX with a Python prompt) generates\nsyntactically valid Python graphs that are similar\nto the task graphs/tables in nearly 100% of the\ncases.\nF Prompts\nThe prompts for each tasks are present at this anony-\nmous URL:\n1. PROSCRIPT script-generation: https:\n//github.com/madaan/CoCoGen/\ntree/main/data/proscript_\nscript_generation/prompt.txt\n2. PROSCRIPT edge-prediction: https:\n//github.com/madaan/CoCoGen/\ntree/main/data/proscript_edge_\nprediction/prompt.txt\n3. PROPARA : https://github.com/\nmadaan/CoCoGen/tree/main/data/\nexplagraphs/prompt.txt\n4. EXPLAGRAPHS : https://github.com/\nmadaan/CoCoGen/tree/main/data/\nexplagraphs/prompt.txt\nThese prompts are also present in the attached\nsupplementary material, and can be found in the\ndatafolder under respective task sub-directories.\nG Designing Python class for a structured\ntask\nFigure 7 shows three different designs for Ex-\nplagraphs. For PROSCRIPT , the various formats\ninclude representing proscript as a Networkx 8\nclass (8), DOT-like class 9, and as a Tree (10).\nH Impact of Model size\nThe CODEX model released by OpenAI is avail-\nable in two versions 9: code-davinci-001\nand code-davinci-002. While the exact\nsizes of the models are unknown because\nof their proprietary nature, OpenAI API\nstates that code-davinci-002 is the Most\ncapable Codex model Tables 16 and ?? com-\npares COCOGEN +code-davinci-001\nwith COCOGEN +code-davinci-002.\nNote that both code-davinci-001 and\ncode-davinci-002 can fit 4000 tokens,\nso the number of in-context examples was\nidentical for the two settings. The results\nshow that for identical prompts, COCOGEN\n+code-davinci-002 vastly outperforms\nCOCOGEN +code-davinci-001, showing\nthe importance of having a better underlying code\ngeneration model.\n8https://networkx.org/\n9as of June 2022\n1398\nFigure 2: A (simpliﬁed) annotated paragraph from\nProPara. Each ﬁlled row shows the existence and lo-\ncation of participants between each step (“?” denotes\n“unknown”, “-” denotes “does not exist”). For example\nin state0, water is located at the soil.\nassumes that a complete and correct model of the\ninitial state is given for each task. However, ap-\nproaches developed using synthetic data often fail\nto handle the inherent complexity in language when\napplied to organic, real-world data (Hermann et al.,\n2015; Winograd, 1972).\nIn this work, we create a new dataset,ProPara\n(Process Paragraphs), containing 488 human-\nauthored paragraphs of procedural text, along with\n81k annotations about the changing states (exis-\ntence and location) of entities in those paragraphs,\nwith an end-task of predicting location and exis-\ntence changes that occur. This is the ﬁrst dataset\ncontaining annotated, natural text for real-world\nprocesses, along with a simple representation of\nentity states during those processes. A simpliﬁed\nexample is shown in Figure 2.\nWhen applying existing state-of-the-art systems,\nsuch as Recurrent Entity Networks (Hena↵ et al.,\n2016) and Query-reduction Networks (Seo et al.,\n2017b), we ﬁnd that they do not perform well on\nProParaand the results are only slightly better than\nthe majority baselines. As a step forward, we pro-\npose two new neural models that use alternative\nmechanisms for state prediction and propagation,\nin particular using LSTM input encoding and span\nprediction. The new models improve accuracy by\nup to 19%.\nOur contributions in this work are twofold: (1)\nwe createProPara, a new dataset for process para-\ngraph comprehension, containing annotated, natu-\nral language paragraphs about real-world processes,\nand (2) we propose two new models that learn to\ninfer and propagate entity states in novel ways, and\noutperform existing methods on this dataset.\n2 Related Work\nDatasets: Large-scale reading comprehension\ndatasets, e.g., SQuAD (Rajpurkar et al., 2016),\nTriviaQA (Joshi et al., 2017), have successfully\ndriven progress in question answering, but largely\ntargeting explicitly stated facts. Often, the result-\ning systems can be fooled (Jia and Liang, 2017),\nprompting e↵orts to create harder datasets where\na deeper understanding of the text appears neces-\nsary (Welbl et al., 2017; Araki et al., 2016).\nProcedural text is a genre that is particularly\nchallenging, because the worlds they describe are\nlargely implicit and changing. While there are\nfew large datasets in this genre, two exceptions are\nbAbI (Weston et al., 2015) and SCoNE (Long et al.,\n2016), described earlier2. bAbI has helped advance\nmethods for reasoning over text, such as memory\nnetwork architectures (Weston et al., 2014), but has\nalso been criticized for using machine-generated\ntext over a simulated domain. SCoNE is closer to\nour goal, but has a di↵erent task (given a perfect\nworld model of the initial state, predict the end\nstate) and di↵erent motivation (handling ellipsis\nand coreference in context). It also used a deter-\nministic, simulated world to generate data.\nModels: For answering questions about procedural\ntext, early systems attempted to extract a process\nstructure (events, arguments, relations) from the\nparagraph, e.g., ProRead (Berant et al., 2014) and\nfor newswire (Caselli et al., 2017). This allowed\nquestions about event ordering to be answered, but\nnot about state changes, unmodelled by these ap-\nproaches.\nMore recently, several neural systems have been\ndeveloped to answer questions about the world state\nafter a process, inspired in part by the bAbI dataset.\nBuilding on the general Memory Network archi-\ntecture (Weston et al., 2014) and gated recurrent\nmodels such as GRU (Cho et al., 2014), Recurrent\nEntity Networks (EntNet) (Hena↵ et al., 2016) is a\nstate-of-the-art method for bAbI. EntNet uses a dy-\nnamic memory of hidden states (memory blocks) to\nmaintain a representation of the world state, with\na gated update at each step. Memory keys can\nbe preset (\"tied\") to particular entities in the text,\nto encourage the memories to record information\nabout those entities. Similarly, Query Reduction\nNetworks (QRN) (Seo et al., 2017b) tracks state in\n2The ProcessBank (Berant et al., 2014) dataset is smaller\nand does not address state change, instead containing 585\nquestions about event ordering and event arguments.\nFigure 5: Example graphs for each of the tasks used for COCOGEN: PROSCRIPT (top-left), EXPLAGRAPHS\n(top-right), and PROPARA (bottom).\nModel Format StCA ( ↑) SeCA ( ↑) G-BS ( ↑) GED (↓) EA ( ↑)\nCODEX -002 Literal 45.2 23.74 34.68 68.76 23.58\nCODEX -002 Tree 39.24 15.95 30.49 73.85 18.24\nCODEX -002 Relation 42.82 23.68 33.38 70.23 21.16\nTable 13: Performance of CODEX on the three different formats present in Figure 7 for EXPLAGRAPHS .\nModel Format F1\nCODEX -001 Literal 15.9\nCODEX -001 Tree 29.7\nCODEX -002 Literal (Figure 9) 52.0\nCODEX -002 Tree (Figure 10) 56.5\nTable 14: Performance of CODEX -001 and CODEX -\n002 on the the different formats present in Figure 10\nand 9 for PROSCRIPT edge prediction. We find that\nthe literal format that combines structure with literally\nFigure output performs the best for CODEX -002.\nModel size vs. sensitivity to the prompt In\nTable 14 shows the performance of CODEX -001\n(smaller) and CODEX -002 (larger, also see Ap-\npendix A) on identical prompts. Our experiments\nshow that as model size increases, the sensitivity\nof the model on the prompt reduces. This indicates\nthat for very large models, prompt design might get\nprogressively easier.\nI Variation in prompts\nWe run each experiment with 3 different random\nseeds, where the random seeds decides the order\nof examples in the prompt. We find minimal vari-\nance between runs using different fixed prompts be-\ntween 3 runs. Further, as shown in the Tables 18,19,\n20, and 21, all improvements of COCOGEN over\nDAVINCI are statistically significant (p-value <\n0.001).\n1399\nTake the pies out to cool Open cabinet drawer\nTake out several plates\nBegin putting\npies on plate\nFill pies onto\nplates evenly\nServe the potpies on a plate\nclass Tree:\ngoal = \"serve the potpies on a plate\"\ndef __init__(self):\n# nodes\nbegin = Node()\ntake_pies_out_to_cool = Node()\ntake_out_several_plates = Node()\nopen_cabinet_drawer = Node()\nfill_pies_onto_plates_evenly = Node()\nbegin_putting_pies_on_plates = Node()\nserve_potpies_on_plate = Node()\n# edges\nbegin.children = [take_pies_out_to_cool, open_cabinet_drawer]\ntake_pies_out_to_cool.children = [take_out_several_plates]\nopen_cabinet_drawer.children = [take_out_several_plates]\ntake_out_several_plates.children = [begin_putting_pies_on_plates,\nfill_pies_onto_plates_evenly]\nbegin_putting_pies_on_plates.children = [serve_potpies_on_plate]\nfill_pies_onto_plates_evenly.children = [serve_potpies_on_plate]\nserve_potpies_on_plate.children = [end]\nFigure 6: A PROSCRIPT plan (top) and the corresponding Python code (bottom).\nBLEU ROUGE -L BLEURT\nDAVINCI 23.1±2.7 36.5 ±2.7 -0.27 ±0.06\nCOCOGEN 25.3±0.1 38.3 ±0.1 -0.25 ±0.01\nTable 18: PROSCRIPT script generation: mean and stan-\ndard deviation across three different random seeds.\nF1\nDAVINCI 48.9 ±2.8\nCOCOGEN 56.2±2.1\nTable 19: PROSCRIPT edge prediction: mean and stan-\ndard deviation across three different random seeds.\nF1\nDAVINCI 56.9 ±2.4\nCOCOGEN 62.8 ±2.4\nTable 21: PROPARA : mean and standard deviation\nacross three different random seeds.\n1400\nModel Format ISO GED Avg(d) Avg( ∣V ∣) Avg( ∣E∣) BLEU ROUGE -L BLEURT\nG - 1.0 0.0 1.84 7.41 6.8 - - -\nCODEX -001 Literal (Figure 9) 0.55 1.8 1.74 7.45 6.5 22.9 36.2 -0.36\nCODEX -001 Tree (Figure 10) 0.35 3 1.79 7.45 6.65 17.8 30.7 -0.45\nCODEX -001 NetworkX (Figure 8) 0.51 1.81 1.69 7.49 6.32 23.7 35.9 -0.37\nCODEX -002 Literal (Figure 9) 0.53 2.1 1.79 7.44 6.7 25.24 38.28 -0.26\nCODEX -002 Tree (Figure 10) 0.35 2.46 1.61 7.46 5.74 18.96 32.92 -0.38\nCODEX -002 NetworkX (Figure 8) 0.5 2.46 1.79 7.38 6.61 23.88 36.89 -0.33\nTable 15: CODEX results on PROSCRIPT generation for various Python source formats.\nclass Relation:\ndef __init__(self):\nbelief = \"Cannabis should be legal.\"\nargument = \"It's not a bad thing to make marijuana more available.\"\nstance = \"support\"\n# create a DAG to support belief using argument\nbegin = [\"cannabis\"]\nadd_edge(\"cannabis\", \"synonym of\", \"marijuana\")\nadd_edge(\"legal\", \"causes\", \"more available\")\nadd_edge(\"marijuana\", \"capable of\", \"good thing\")\nadd_edge(\"good thing\", \"desires\", \"legal\")\nclass Tree:\ndef __init__(self):\nself.belief = \"Cannabis should be legal.\"\nself.argument = \"It's not a bad thing to make marijuana more available.\"\nself.stance = \"support\"\n# tree for support in support of belief\nroot_nodes = cannabis\ncannabis = Node()\ncannabis.add_edge(\"synonym of\", \"marijuana\")\nlegal = Node()\nlegal.add_edge(\"causes\", \"more available\")\nmarijuana = Node()\nmarijuana.add_edge(\"capable of\", \"good thing\")\ngood_thing = Node()\ngood_thing.add_edge(\"desires\", \"legal\")\nclass Literal:\ndef __init__(self):\nself.belief = \"Cannabis should be legal.\"\nself.argument = \"It's not a bad thing to make marijuana more available.\"\nself.stance = \"support\"\nself.graph = \"\"\"\\\n(cannabis; synonym of; marijuana)(legal; causes; more available)\n(marijuana; capable of; good thing)\n(good thing; desires; legal)\"\"\"\nFigure 7: Templates tried for explagraph.\nISO GED Avg(d) Avg( ∣V ∣) Avg( ∣E∣) BLEU ROUGE -L BLEURT\nG 1.0 0.0 0.0 1.84 7.41 6.8 - - -\nCOCOGEN + 001 (15) 0.55 1.8 1.74 7.45 6.5 22.9 36.2 -0.36\nCOCOGEN + 002 (15) 0.53 2.1 1.79 7.44 6.7 25.24 38.28 -0.26\nTable 16: CODEX -001 vs 002 on PROSCRIPT script generation\n1401\nclass Plan:\ngoal = \"create a video game\"\nnum_steps = 7\ndef __init__(self):\ngraph = nx.DiGraph()\n# add nodes\nstep0 = \"decided to create a video game\"\nstep1 = \"Learn the basics of programming\"\nstep2 = \"Learn to use a language that is used in games\"\nstep3 = \"Learn to use an existing game engine\"\nstep4 = \"Program the game\"\nstep5 = \"Test the game\"\nstep6 = \"create a video game\"\ngraph.add_nodes_from([step0, step1, step2, step3, step4, step5, step6])\n# add edges\ngraph.add_edge(step0, step1)\ngraph.add_edge(step1, step2)\ngraph.add_edge(step1, step3)\ngraph.add_edge(step2, step4)\ngraph.add_edge(step3, step4)\ngraph.add_edge(step4, step5)\ngraph.add_edge(step5, step6)\nFigure 8: Proscript as a Networkx class.\nclass CreateAVideoGame:\ntitle = \"create a video game\"\nsteps = 7\ndef step0(self):\nreturn \"decided to create a video game\"\ndef step1(self):\nreturn \"Learn the basics of programming\"\ndef step2(self):\nreturn \"Learn to use a language that is used in games\"\ndef step3(self):\nreturn \"Learn to use an existing game engine\"\ndef step4(self):\nreturn \"Program the game\"\ndef step5(self):\nreturn \"Test the game\"\ndef step6(self):\nreturn \"create a video game\"\ndef get_relations(self):\nreturn [\n\"step0 -> step1\",\n\"step1 -> step2\",\n\"step1 -> step3\",\n\"step2 -> step4\",\n\"step3 -> step4\",\n\"step4 -> step5\",\n\"step5 -> step6\",\n]\nFigure 9: Representing PROSCRIPT graph literally.\nStCA (↑) SeCA ( ↑) G-BS ( ↑) GED (↓) EA ( ↑)\nDAVINCI 25.4 ±2.7 13.7 ±2.8 20 ±2.3 82.5 ±1.9 13.6 ±1.8\nCOCOGEN 44.0 ±1.2 25.1 ±2.5 34.1 ±0.7 69.5 ±0.7 22.0 ±1.3\nTable 20: EXPLAGRAPHS : mean and standard deviation across three different random seeds.\n1402\nclass Tree:\ngoal = \"serve the potpies on a plate\"\ndef __init__(self):\n# nodes\nbegin = Node()\ntake_pies_out_to_cool = Node()\ntake_out_several_plates = Node()\nopen_cabinet_drawer = Node()\nfill_pies_onto_plates_evenly = Node()\nbegin_putting_pies_on_plates = Node()\nserve_potpies_on_plate = Node()\n# edges\nbegin.children = [take_pies_out_to_cool, open_cabinet_drawer]\ntake_pies_out_to_cool.children = [take_out_several_plates]\nopen_cabinet_drawer.children = [take_out_several_plates]\ntake_out_several_plates.children = [begin_putting_pies_on_plates,\nfill_pies_onto_plates_evenly]\nbegin_putting_pies_on_plates.children = [serve_potpies_on_plate]\nfill_pies_onto_plates_evenly.children = [serve_potpies_on_plate]\nserve_potpies_on_plate.children = [end]\nFigure 10: Proscript with a tree-encoding.\n1403",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8719207048416138
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.8208574056625366
    },
    {
      "name": "Serialization",
      "score": 0.6378108263015747
    },
    {
      "name": "Natural language",
      "score": 0.5892700552940369
    },
    {
      "name": "Natural language processing",
      "score": 0.5749180912971497
    },
    {
      "name": "Task (project management)",
      "score": 0.5615519881248474
    },
    {
      "name": "Frame (networking)",
      "score": 0.53116375207901
    },
    {
      "name": "Code (set theory)",
      "score": 0.5302198529243469
    },
    {
      "name": "Graph",
      "score": 0.5064822435379028
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4919383227825165
    },
    {
      "name": "Language model",
      "score": 0.4481017291545868
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.41684114933013916
    },
    {
      "name": "Knowledge graph",
      "score": 0.4152572751045227
    },
    {
      "name": "Programming language",
      "score": 0.3485568165779114
    },
    {
      "name": "Theoretical computer science",
      "score": 0.19366377592086792
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.1389210820198059
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}