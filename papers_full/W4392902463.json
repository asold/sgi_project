{
    "title": "Transformer-Based Named Entity Recognition in Construction Supply Chain Risk Management in Australia",
    "url": "https://openalex.org/W4392902463",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A3204995687",
            "name": "Milad Baghalzadeh Shishehgarkhaneh",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A4364316310",
            "name": "Robert C Moehler",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A2099760064",
            "name": "Yihai Fang",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A2749834456",
            "name": "Amer A. Hijazi",
            "affiliations": [
                "Al-Ahliyya Amman University"
            ]
        },
        {
            "id": "https://openalex.org/A2799591798",
            "name": "Hamed Aboutorab",
            "affiliations": [
                "Charles Sturt University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1663984431",
        "https://openalex.org/W2169818249",
        "https://openalex.org/W3109540321",
        "https://openalex.org/W3196261354",
        "https://openalex.org/W3168428228",
        "https://openalex.org/W2747680751",
        "https://openalex.org/W3094732332",
        "https://openalex.org/W3144293453",
        "https://openalex.org/W2964000578",
        "https://openalex.org/W6792439368",
        "https://openalex.org/W3203241482",
        "https://openalex.org/W2963625095",
        "https://openalex.org/W1993729825",
        "https://openalex.org/W2964167098",
        "https://openalex.org/W2888329843",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W4206192903",
        "https://openalex.org/W2006209644",
        "https://openalex.org/W2588461080",
        "https://openalex.org/W2034334068",
        "https://openalex.org/W4306680578",
        "https://openalex.org/W4386332905",
        "https://openalex.org/W6680532216",
        "https://openalex.org/W6636510571",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W4387304047",
        "https://openalex.org/W6758993734",
        "https://openalex.org/W4319587062",
        "https://openalex.org/W4297094598",
        "https://openalex.org/W4385319047",
        "https://openalex.org/W4281729944",
        "https://openalex.org/W4285041941",
        "https://openalex.org/W4206437118",
        "https://openalex.org/W6838992615",
        "https://openalex.org/W4295763951",
        "https://openalex.org/W3207686987",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4289109804",
        "https://openalex.org/W3088302804",
        "https://openalex.org/W4213249986",
        "https://openalex.org/W2997222178",
        "https://openalex.org/W3169313306",
        "https://openalex.org/W3215078519",
        "https://openalex.org/W4294830236",
        "https://openalex.org/W4281552540",
        "https://openalex.org/W4206572107",
        "https://openalex.org/W4382678493",
        "https://openalex.org/W6790690058",
        "https://openalex.org/W3199638386",
        "https://openalex.org/W4386938179",
        "https://openalex.org/W4243640523",
        "https://openalex.org/W4386465411",
        "https://openalex.org/W2909714293",
        "https://openalex.org/W3118674598",
        "https://openalex.org/W6788864986",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W6768851824",
        "https://openalex.org/W6768021236",
        "https://openalex.org/W6771917389",
        "https://openalex.org/W6769627184",
        "https://openalex.org/W6852016728",
        "https://openalex.org/W4200057797",
        "https://openalex.org/W2240633916",
        "https://openalex.org/W3001654774",
        "https://openalex.org/W2144578941",
        "https://openalex.org/W2983563856",
        "https://openalex.org/W3094054110",
        "https://openalex.org/W4304014782",
        "https://openalex.org/W3115058362",
        "https://openalex.org/W3099740706",
        "https://openalex.org/W3129089344",
        "https://openalex.org/W4379381507",
        "https://openalex.org/W4287889470",
        "https://openalex.org/W4287888483",
        "https://openalex.org/W4287854527",
        "https://openalex.org/W3013836620",
        "https://openalex.org/W6784388832",
        "https://openalex.org/W4287854475",
        "https://openalex.org/W4312278076",
        "https://openalex.org/W4294763143",
        "https://openalex.org/W3175015879",
        "https://openalex.org/W4205349060",
        "https://openalex.org/W3100530666",
        "https://openalex.org/W4308654474",
        "https://openalex.org/W4304755986",
        "https://openalex.org/W3128376221",
        "https://openalex.org/W4210874635",
        "https://openalex.org/W3173587986",
        "https://openalex.org/W2096994080",
        "https://openalex.org/W3121384738",
        "https://openalex.org/W2963433607",
        "https://openalex.org/W6852975210",
        "https://openalex.org/W6780302750",
        "https://openalex.org/W6790475365",
        "https://openalex.org/W6810898628",
        "https://openalex.org/W3185093489",
        "https://openalex.org/W6735896536",
        "https://openalex.org/W6752184718",
        "https://openalex.org/W3204084675",
        "https://openalex.org/W4283524489",
        "https://openalex.org/W4316127362",
        "https://openalex.org/W3209180124",
        "https://openalex.org/W3170936934",
        "https://openalex.org/W6792011604",
        "https://openalex.org/W4291700196",
        "https://openalex.org/W6849835365",
        "https://openalex.org/W3094868181",
        "https://openalex.org/W4287816920",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W4287329000",
        "https://openalex.org/W3037115325",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W3138611044",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W4367858557",
        "https://openalex.org/W4366735603",
        "https://openalex.org/W4377121531",
        "https://openalex.org/W3139580003",
        "https://openalex.org/W4320499618",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W4281738672"
    ],
    "abstract": "In the Australian construction industry, effective supply chain risk management (SCRM) is critical due to its complex networks and susceptibility to various risks. This study explores the application of transformer models like BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA for Named Entity Recognition (NER) in this context. Utilizing these models, we analyzed news articles to identify and classify entities related to supply chain risks, providing insights into the vulnerabilities within this sector. Among the evaluated models, RoBERTa achieved the highest average F1 score of 0.8580, demonstrating its superior balance in precision and recall for NER in the Australian construction supply chain context. Our findings highlight the potential of NLP-driven solutions to revolutionize SCRM, particularly in geo-specific settings.",
    "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\nTransformer-based Named Entity Recognition in\nConstruction Supply Chain Risk Management in\nAustralia\nMILAD BAGHALZADEH SHISHEHGARKHANEH1, ROBERT C. MOEHLER12 ,YIHAI FANG1,AMER\nA. HIJAZI3, and Hamed Aboutorab4\n1Department of Civil Engineering, Faculty of Engineering, Monash University, Clayton, VIC 3800, Australia\n2Department of Infrastructure Engineering, The University of Melbourne, Melbourne, Australia\n3Department of Civil Engineering, Al-Ahliyya Amman University, Amman 19328, Jordan\n4School of Business, UNSW Canberra, Canberra, ACT, Australia\nCorresponding author: Robert C. Moehler (e-mail: Robert.moehler@unimelb.edu.au).\nThis work was supported by the Monash Graduate Scholarship and Monash International Tuition Scholarship provided by Monash\nUniversity, Clayton, Australia.\nABSTRACT In the Australian construction industry, effective supply chain risk management (SCRM) is\ncritical due to its complex networks and susceptibility to various risks. This study explores the application\nof transformer models like BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA for Named Entity\nRecognition (NER) in this context. Utilizing these models, we analyzed news articles to identify and classify\nentities related to supply chain risks, providing insights into the vulnerabilities within this sector. Among\nthe evaluated models, RoBERTa achieved the highest average F1 score of 0.8580, demonstrating its superior\nbalance in precision and recall for NER in the Australian construction supply chain context. Our findings\nhighlight the potential of NLP-driven solutions to revolutionize SCRM, particularly in geo-specific settings.\nINDEX TERMS Construction supply chain risk management, named entity recognition, Transformers,\nNatural Language Processing, BERT.\nI. INTRODUCTION\nN\nATURAL Language Processing (NLP) is a field of com-\nputational techniques that aims to automate the analysis\nand representation of human language. By leveraging both\ntheoretical principles and practical applications, NLP enables\nus to work with natural language data in various ways. From\nparsing and part-of-speech (POS) tagging to machine trans-\nlation, conversation systems, and named entity recognition\n(NER), NLP encompasses a wide range of components and\nlevels. It has proven itself useful in fields such as natural lan-\nguage understanding [1], generation [2], voice/speech recog-\nnition [3], spell correction and grammar check [4], among\nothers. The versatility of NLP allows it to address diverse\nlinguistic tasks effectively [5].\nThe evolution of NLP can be divided into different phases\nthat represent the progress in language generation and other\nlanguage processing aspects. These phases illustrate the cur-\nrent state of the field, as well as ongoing trends and chal-\nlenges. NLP encompasses a wide range of applications and\ncontinues to advance with computational modelling and tech-\nnological innovations [6]. Furthermore, NLP involves study-\ning mathematical and computational models related to var-\nious language aspects. It includes developing systems like\nspoken language interfaces that combine speech and natural\nlanguage, as well as interactive interfaces for databases and\nknowledge bases. This enables modelling of human-human\nand human-machine interactions. Overall, NLP is a multidis-\nciplinary field that intertwines computational, linguistic, and\ncognitive dimensions [7, 8].\nA dedicated series focused on the \"Theory and Applica-\ntions of Natural Language Processing\" explores the latest\nadvancements in computational modelling and processing of\nspeech and text in different languages and domains [9]. This\nhighlights the rapid progress in NLP and Language Technol-\nogy, driven by the increasing volume of natural language data\nand the evolving capabilities of machine learning and deep\nlearning technologies. These references illustrate that NLP is\na dynamic field with a solid theoretical foundation, power-\ning numerous practical applications across various domains.\nNER is a method for identifying, classifying, and separating\nnamed entities into groups according to predetermined cate-\ngories. NER is a crucial component of NLP technology and\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nAbbreviation\nNLP Natural Language Processing\nPOS Part of Speech\nNER Named Entity Recognition\nLSTM Long Short-Term Memory\nRNN Recurrent Neural Network\nBERT Bidirectional Encoder Representations from Transformers\nCSCRM Construction Supply Chain Risk Management\nGNEER Geological News Named Entity Recognition\nELECTRA Efficiently Learning an Encoder that Classifies Token Replacements Accurately\nGPT Generative Pre-trained Transformer\nLr Learning Rate\nW2V Word2Vec\nCBOW Continuous Bag of Words\nGS Grid Search\nNSP Next Sentence Prediction\nSG Skip-Gram\nMFT Multi-feature Fusion Transformer\nforms the foundation for many studies in this field. The recent\nadvancements in deep learning have significantly improved\nthe performance of NER applications, especially in real-\nworld situations where high-quality annotated training data\nis often limited [10].\nHowever, there has been a shift in the paradigm with the\nemergence of deep learning techniques driven by neural net-\nworks. These methods have shown great success in tasks such\nas NER, as confirmed by several studies [11]. One particu-\nlarly acclaimed approach combines Long Short-Term Mem-\nory (LSTM) with CRF. In this combination, LSTM carefully\ncaptures vector representations of each word or token in a\nsentence, which are then fed into the CRF model for accurate\nsequence tagging [12]. In a ground-breaking approach, [13]\ncombined character-level and word-level features in a hybrid\nnetwork architecture. Their model utilized a BiLSTM layer\nfollowed by a log-SoftMax layer to independently decode\neach tag, resulting in improved accuracy. Similarly, [14]\nmerged CRFs with information entropy, effectively identify-\ning abbreviations of financial named entity candidates. This\ndemonstrates the versatility of such models in specialized\ndomains. Figure 1 shows the evolution of probabilistic models\nin machine learning.\nTo contribute to the ongoing discussion, [15] introduced\na novel approach that combined a BiLSTM encoder with an\nincrementally decoded neural network structure. This inno-\nvative method allowed for simultaneous decoding of tags,\npromoting a more nuanced comprehension of textual data.\nAlthough there were various encoding strategies based on\nrecurrent neural network (RNN) architectures, the differences\nin methodology became evident during the decoding phase.\nRecently, advanced language models like ELMo [16], GPT-4\n(Generative Pre-trained Transformer) [17], and BERT (Bidi-\nrectional Encoder Representations from Transformers)[18],\nhave emerged in the field of technology. These models have\nbecome extremely effective across various NLP tasks and\nhave revolutionized the way we approach natural language\nprocessing. Unlike traditional methodologies that heavily re-\nlied on feature engineering, these deep neural networks pos-\nsess the remarkable ability to automatically extract features\nFIGURE 1: Evolution of Probabilistic Models in Machine\nLearning, as illustrated by the authors and inspired by\n[19]\nfrom data. This characteristic has propelled them to achieve\nsuperior performance without the need for manual feature\ncrafting or extensive domain expertise. The adoption of these\nsophisticated models marks a significant milestone in ad-\ndressing NER tasks, facilitating more efficient and automated\napproaches in identifying and categorizing named entities\nacross diverse domains and languages.\nThe integration of NER technologies, such as advanced\nmodels like BERT, into CSCRM frameworks can greatly en-\nhance the automatic extraction of critical information entities\nfrom a vast amount of news data. This, in turn, enables the\ncreation of comprehensive knowledge graphs that encompass\nvarious risk factors and their potential impact on construction\nsupply chains. These knowledge graphs hold immense value\nfor construction firms, regulators, and other stakeholders\nas they foster a more resilient, transparent, and responsive\necosystem within the Australian construction supply chain.\nDespite its wide-ranging applications, there seems to be a\ndearth of research or documentation on the use of NER in\nconstruction supply chain risk management, particularly with\nregards to geological news in Australia [20] and [21]. This\npresents an opportunity for further investigation and explo-\nration into utilizing NER to address risk management chal-\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nlenges within the construction supply chain domain. Specifi-\ncally, it can prove valuable in leveraging geological news for\nmore informed decision-making processes.\nThis research study examined the effectiveness of various\nBERT models in performing NER within the field of Con-\nstruction Supply Chain Risk Management (CSCRM). The\nprimary source of information utilized is news data. This\ninvestigation breaks new ground by exploring NER applica-\ntions in CSCRM specifically through the lens of news data, an\narea that hasn’t been previously studied. The dataset consists\nof information gathered from multiple news outlets, providing\na fertile ground for identifying and analysing numerous risk\nfactors and how they manifest within the construction supply\nchain ecosystem. Through careful examination, this study un-\ncovers several significant contributions. Firstly, it establishes\na practical framework for utilizing NER to dissect real-world\nnews data and extract valuable risk-related entities and their\nrelationships [22]. This contributes to a deeper understanding\nof risk dynamics in construction supply chains. Secondly, this\nresearch provides a comparative analysis of different BERT\nmodels in accurately discerning these entities. This serves as a\nsolid foundation for further advancements in the field. Lastly,\nthe insights obtained through our analysis pave the way for\ndeveloping more resilient and informed risk management\nstrategies in the construction sector. It represents a significant\nstep towards mitigating vulnerabilities within supply chains\nthrough the effective utilization of NER technologies.\nII. LITERATURE REVIEW\nThe use of advanced language models, like BERT and GPT-\n3, in NER has become increasingly prevalent across various\nindustries. From healthcare to finance, legal, and construc-\ntion, businesses are leveraging these sophisticated models\nto accurately identify and categorize named entities within\nlarge volumes of text. These models have the remarkable\nability to autonomously detect complex patterns and relation-\nships between words without the need for labour-intensive\nfeature engineering. This capability allows for a nuanced\nunderstanding of data, enabling critical insights extraction,\nbetter decision-making, regulatory compliance, and improved\ncustomer experiences. Additionally, advancements in transfer\nlearning and the development of domain-specific pre-trained\nmodels have further accelerated the effectiveness and adop-\ntion of NER across diverse industries. In today’s data-driven\necosystem, NER has become an indispensable tool [23, 24].\nWord2Vec (W2V) has revolutionized semantic vector\nspaces in NLP, building on earlier foundations [25]. It intro-\nduces word embeddings through two methods: Continuous\nBag-of-Words (CBOW) and Skip-Gram (SG), both sharing a\nneural network structure but differing in input-output man-\nagement [26, 27]. Evolving beyond basic word embeddings,\nmulti-sense and contextualized embeddings like Elmo, Bert,\nand Xlnet have emerged, focusing on enriched semantic un-\nderstanding [28]. W2V bridges the gap between count-based\nmodels and neural networks, enhancing semantic exploration\nand text analytics in deep learning, thus playing a pivotal role\nin the evolution of pre-trained language models [29].\n[30] created a NER methodology to identify Chinese\nmedicine and disease names in conversations between hu-\nmans and machines. They evaluated various models, and the\ncombination of RoBERTa with biLSTM and CRF performed\nthe best. Using a corpus obtained through web crawling, this\nmodel achieved an impressive Precision, Recall, and F1-score\nof 0.96. These findings highlight its potential for enhancing\nmedication reminders in dialogue systems. [31] developed a\nChinese NER model called BBIEC specifically for analysing\nCOVID-19 epidemiological data. This model effectively pro-\ncesses unlabelled data at the character level, extracting global\nand local features using pre-trained BERT, BiLSTM, and\nIDCNN techniques. The BBIEC model outperforms tradi-\ntional models when it comes to recognizing entities that are\ncrucial for analysing the transmission routes and sources\nof the epidemic. [32] proposed a BERT-Transformer-CRF\nbased service recommendation method (BTC-SR) for en-\nhanced chronic disease management, which initially employs\na BERT-Transformer-CRF model to identify named entities in\ndisease text data, extracts entity relationships, and integrates\nuser implicit representation to deliver personalized service\nrecommendations, demonstrating improved entity recogni-\ntion with an F1 score of 60.15 on the CMeEE dataset and\npaving the way for more precise service recommendations for\nchronic disease patients.\n[33] introduced a deep learning-based Mineral Named En-\ntity Recognition (MNER) model, utilizing BERT for min-\neral text word embeddings and enhancing sequence labelling\naccuracy by integrating the CRF algorithm’s transfer ma-\ntrix. Furthermore, [34] introduced a multi-task model called\nBERT-BiLSTM-AM-CRF. The model utilizes BERT for dy-\nnamic word vector extraction and then refines it through a\nBiLSTM module. After incorporating an attention mecha-\nnism network, the output is passed into a CRF layer for\ndecoding. The authors tested the model on two Chinese\ndatasets and observed significant improvements in F1 score\ncompared to previous single-task models, with increases of\n0.55% in MASR dataset and 3.41% in People’s Daily dataset\nrespectively. [35] explored the NER task in Telugu language\nusing various embeddings such as Word2Vec, Glove, Fast-\nText, Contextual String embedding, and BERT. Remarkably,\nwhen combining BERT embeddings with handcrafted fea-\ntures, the results outperformed other models significantly.\nThe achieved F1-Score was an impressive 96.32%. [36] in-\ntroduced Wojood, a unique corpus specifically designed for\nArabic nested NER. This corpus comprises approximately\n550K tokens of Modern Standard Arabic and dialect, each\nmanually annotated with 21 different entity types. Unlike tra-\nditional flat annotations, Wojood includes around 75K nested\nentities, accounting for about 22.5% of the total annotations.\nThe accuracy and reliability of this corpus are evident in its\nsubstantial interannotator agreement, with a Cohen’s Kappa\nscore of 0.979 and an F1 score of 0.976. Furthermore, to\naddress the limitations of traditional methods for named en-\ntity recognition in the context of agricultural pest information\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nextraction, [37] proposed a PBERT-BiLSTM-CRF model.\nThis model leverages pre-trained BERT to resolve ambiguity,\nBiLSTM to capture long-distance dependencies, and CRF for\noptimal sequence annotation. The results demonstrate signif-\nicant improvements in precision, recall, and an impressive F1\nscore of 90.24% compared to other models.\nA. NAMED ENTITY RECOGNITION IN CONSTRUCTION\nINDUSTRY\nNamed entity recognition in construction has received some\nattention in academic literature, although the available pub-\nlished research in this field is relatively limited. While several\nstudies have been conducted on this topic, the quantity of\npublications compared to other areas of natural language pro-\ncessing and construction is modest. In the realm of CSCRM\nin Australia, the significance of local and international news\ncannot be overstated.\nThe constantly changing geopolitical, environmental, and\neconomic scenarios greatly impact construction supply\nchains. For example, the recent disruptions caused by the\nCOVID-19 pandemic had a profound effect on the China-\nAustralia construction supply chain. This highlighted the ur-\ngent need for timely and accurate information to effectively\nmanage and mitigate risks [38]. The construction sector in\nAustralia is currently facing increased supply chain risks.\nThese risks have been amplified by the growing number\nof suppliers, complex work streams, stringent compliance\nrequirements, and difficulties in finding eligible parties. It is\nimportant to note that disruptions in global supply chains,\nparticularly those originating from regions like China, have\nresulted in project delays. This emphasizes the significance\nof international news for predicting and managing such dis-\nruptions. The lack of transparency in supply chain risk among\nAustralian construction firms emphasizes the need for a well-\ninformed and data-driven approach to risk management. By\nutilizing NER technologies, particularly in the context of\ngeological news texts, automation can play a vital role in\nextracting relevant information from local and international\nnews sources. This enhancement significantly improves the\naccuracy and timeliness of risk assessments and mitigating\nactions within the Australian construction supply chain do-\nmain.\nHowever, the field of geological news texts is rapidly ex-\npanding, offering a wealth of valuable information. Accu-\nrately extracting this information can greatly enhance geo-\nlogical survey efforts. However, traditional manual extraction\nmethods are inefficient and time-consuming, leading to lower\naccuracy. As the volume of geological news text data in-\ncreases, these challenges become even more pronounced. It is\ncrucial to transition towards automated extraction paradigms\nto address this complexity. Automating the extraction of geo-\nlogical news entities goes beyond just a procedural evolution;\nit represents a fundamental leap towards the creation of com-\nprehensive geological knowledge graphs. These knowledge\ngraphs can serve as structured repositories, facilitating the re-\ntrieval and analysis of geological information and propelling\nadvancements in the field of geological surveys.\nBERT, however, is a major breakthrough in the field of deep\nlanguage understanding. Its architecture, which utilizes the\npowerful Transformer model, particularly its encoder com-\nponent, has revolutionized our ability to comprehend natural\nlanguage. BERT’s pre-training phase involves analysing an\nenormous corpus of books and Wikipedia articles, allowing\nit to grasp the complex semantics present in textual data.\nThe core essence of BERT lies within the encoder section\nof the Transformer model—an innovative design introduced\nby [39],—which has received widespread acclaim for its effi-\ncient parallelization of computations, greatly improving com-\nputational efficiency. The recent advancements in machine\nlearning and NLP have significantly improved the challenges\nassociated with manual data extraction. One notable break-\nthrough is the emergence of transformer-based models like\nBERT, which has paved the way for automating the extraction\nprocess. For example, a study introduced a method called\nGeological News Named Entity Recognition (GNNER) that\nutilizes the BERT language model to effectively extract and\nleverage geological data [40]. Moreover, other scholarly en-\ndeavours have demonstrated automated techniques for ex-\ntracting spatiotemporal and semantic information from ge-\nological documents. These techniques are crucial for tasks\nsuch as data mining, knowledge discovery, and constructing\nknowledge graphs [41, 42]. The narrative above explains the\nimportance and modern approaches used in automating the\nextraction of geological news information. This automation\nnot only enhances the efficiency and accuracy of retrieving\ninformation, but it also forms a vital foundation for building\ncomprehensive geological knowledge graphs. Table 1 com-\npares the recent literature on NER in construction industry\nwith current study considering their aims, models and their\ndataset used.\nIII. METHODOLOGY\nA. TRANSFORMERS\nAs shown in Figure 2, the Transformer, distinct from tra-\nditional neural network designs, relies solely on attention\nmechanisms for transferring knowledge. Central to its ef-\nficiency is the attention mechanism, focusing on important\ndata segments. Outputs are computed from weighted sums\nof these segments, optimizing the match between query and\nkey. This design has significantly advanced NLP, computer\nvision, and spatio-temporal modeling [50, 51], enhancing\nmodel performance and training efficiency.\n1) Transformer’s Input and Self-Attention\nThe Transformer processes tokens simultaneously, adding\npositional encodings to each word vector. Self-attention cap-\ntures relationships within sequences, involving Query (Q),\nKey (K), and Value (V) matrices. It computes attention scores,\ndetermining focus levels on different tokens, a key factor in\nNLP applications [52, 53].\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 1: Comparison of Current Study with Literature\nRef Year Aim Model(s) Dataset\n[43] 2019 Extraction of geological hazard Multi-branch BiGRU-\nCRF\nBody of geological hazard literature and\nbuild a geological hazard knowledge graph\n[44] 2019 Improving Chinese construction document BERT A document of Construction Management\nPlanning and daily reports of a hospital con-\nstruction project\n[45] 2021 Compliance checking of the building design GBERT Public construction law texts\n[46] 2022 Extracting defect information from noisy text in con-\nstruction projects\nBERT, KoBERT,\nKoELECTRA\nDefect complaints\n[47] 2022 Sort out the basic information from Chinese text about\nChinese railway construction\nBiLSTM Chinese placenames and numbers\n[48] 2023 Identification of building parts from Chinese con-\nstruction documents\nCRF-based NER model Corpus of building’s parts\n[49] 2023 Compliance checking and resolve referential ambigu-\nities\nBiLSTM-CNN Construction safety regulations\nCurrent Study 2024 Construction Supply Chain Risk Management in Aus-\ntralia\nDifferent transformer\nmodels\nUnique dataset extracted from News web-\nsites\nFIGURE 2: Transformer’s Architecture, as illustrated by the authors and inspired by [39]\n2) Multi-Head Mechanism and Normalization\nThe multi-head mechanism assigns multiple feature expres-\nsions to each input item, enhancing the model’s representa-\ntional capabilities. Residual connections and layer normal-\nization improve feature extraction and model convergence\n[54, 55].\nXatt = X + Xatt (1)\nXatt = LayerNorm(Xatt) (2)\nB. BERT (BIDIRECTIONAL ENCODER REPRESENTATIONS\nFROM TRANSFORMERS)\nBERT, based on the Transformer’s encoder, represents a\nsignificant advancement in deep language comprehension.\nIt pre-trains on vast textual data, gaining intricate semantic\nunderstanding. BERT’s core relies on Transformer’s encoder,\npraised for efficient parallel computations [39]. It processes\ninput as token sequences, incorporating token, segment, and\nposition embeddings for nuanced language understanding\n[56, 57]. Figure 3 illustrates the Transformer model’s archi-\ntecture with a focus on the encoder, vital for BERT.\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nC. KEY COMPONENTS OF BERT\nToken Embedding: Assigns embeddings to tokens, rep-\nresenting them in high-dimensional space. Each token in\nBERT’s vocabulary has a unique embedding learned during\ntraining [56].\nSegment Embedding: Differentiates between sentences in\ntasks like question answering, learning separate embeddings\nfor each sentence [56].\nPosition Embedding: Provides positional information in\nthe absence of a recurrent structure, crucial for understanding\nword order [57].\nHowever, in addition to the BERT model, this paper also\ninvestigates other models like RoBERTa, DistilBERT, AL-\nBERT, ELECTRA, T5, and GPT-3. Each of these models has\nspecific adjustments and enhancements designed for different\nNLP tasks.\nRoBERTa, also known as Robustly Optimized BERT Pre-\ntraining Approach, enhances the performance of BERT by\nmodifying the pre-training process. This includes longer\ntraining periods, the use of larger datasets, and bigger mini-\nbatches compared to BERT [58]. Furthermore, DistilBERT\nis a more compact and efficient version of BERT. It was\ncreated using a process called knowledge distillation, where\nthe DistilBERT model learns from a pre-trained BERT model.\nThis allows DistilBERT to maintain similar performance ca-\npabilities while being faster and more economical in terms of\ncomputational resources [59]. To make BERT more efficient,\nALBERT (A Lite BERT) utilizes techniques like factorised\nembedding parameterisation and cross-layer parameter shar-\ning. These methods reduce the size and increase the speed\nof BERT [60]]. Instead of using the masked language mod-\nelling objective like BERT, ELECTRA (Efficiently Learning\nan Encoder that Classifies Token Replacements Accurately)\ntakes a different approach. It utilizes a pre-training task called\nreplaced token detection, which aims to achieve more ef-\nficient pre-training [61]. T5 (Text-To-Text Transfer Trans-\nformer) takes a distinctive approach by transforming every\nNLP problem into a text-to-text format. This simplifies the\napplication of the model to various NLP tasks [62]. Finally,\nthe GPT model is a ground-breaking technology that has\nrevolutionised NLP. Through unsupervised learning on vast\namounts of text data, it has successfully generated text that\nclosely resembles human writing [63]. GPT-3 is the successor\nto GPT-2 and boasts a significant raise in both parameter\ncount (from 1.5 billion to 175 billion) and data processed\n(from 40 GB to 45 TB), making it the largest language model\never created [64].\nAll models were obtained from the Hugging Face model\nrepository. The selection of these models was predicated on\ntheir proven efficacy in the field of NLP and their suitability\nfor the task at hand, as evidenced by their widespread use\nand robust performance across numerous benchmarks. Each\nmodel was meticulously evaluated to ascertain its compati-\nbility with the specific requirements of recognizing entities\nwithin the context of Australian construction supply chain\nrisk management. This involved an extensive comparative\nanalysis to identify the model that demonstrated the high-\nest accuracy and efficiency when applied to our annotated\ndataset. However, we continuously monitored the Hugging\nFace model hub for the latest developments in GPT models.\nHowever, as of the completion of our research, GPT-4 had not\nbeen released or made publicly accessible in this repository\nor any other known open-source platform.\nD. DATA GATHERING\nThis study conducted a thorough investigation to create a\ndetailed risk categorisation specifically for managing risks in\nthe construction supply chain in Australia. This involved care-\nfully reviewing existing literature and incorporating insights\nfrom the Cambridge Taxonomy of Business Risks [65]. As\nshown in Figure 3, the resulting risk categorisation covers\na wide range of risks commonly found in the Australian\nconstruction supply chain, providing a strong basis for the\nfollowing stages of this study.\nAfter establishing the risk taxonomy, the attention turned\nto collecting a comprehensive dataset for thorough anal-\nysis of the identified risks. A specialised News API was\nutilised to search through approximately 2000 articles from\nrenowned news sources like The Australian, Sky News Aus-\ntralia, Bloomberg, CNN, Reuters, and Google News . The\ncareful selection of news sources is paramount to ensure the\ncomprehensiveness, diversity, and reliability of the dataset.\nGuided by specific criteria aimed at capturing a wide range\nof perspectives and factual reporting, the chosen sources each\nboast a longstanding reputation for journalistic integrity and\nreliability. This approach mitigates biases and inaccuracies in\ndata-driven research, as emphasized by [66]. To ensure a bal-\nanced representation of global news, sources from various ge-\nographical locations and political orientations were selected,\naligning with the recommendations for achieving diversity\nin news recommendation systems highlighted by [67]. These\nsources are known for their comprehensive coverage across\na range of topics, including politics, economics, and interna-\ntional affairs, critical for a well-rounded dataset. Furthermore,\nthe importance of digital accessibility and archival features\nwas a key factor, ensuring ease of data collection and analysis.\nEach selected source, including The Australian and Sky News\nAustralia, offers unique insights into the Asia-Pacific region;\nBloomberg is revered for its financial news; CNN and Reuters\nprovide extensive global coverage; and Google News aggre-\ngates a diverse range of sources, enabling a comprehensive\ntopical analysis. This data collection approach was carefully\ndesigned to adhere to web scraping guidelines and ensure\nethical acquisition of data. The result was a diverse and\nextensive dataset that provided ample material for empirical\ninvestigation of the specified risks within the CSCRM domain\nusing NER.\n1) ANNOTATION OF TEXT CORPUS\nFor dataset annotation, sequence labelling is a critical step\nthat helps organise data for further analysis. Among various\nlabelling methods used in scholarly research, this study uti-\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 3: Construction supply chain risk taxonomy used in this research.\nlizes the \"BIO\" labelling scheme due to its effectiveness and\nwidespread acceptance. This labelling convention, commonly\nemployed in NER, offers a systematic approach to annotate\ntext sequences, allowing for a detailed understanding of the\ntext structure. The \"BIO\" labelling scheme consists of three\nannotations: \"B-X,\" \"I-X,\" and \"O.\" In this scheme, the letter\n\"B\" indicates the beginning of a named entity in the text. The\nletter \"I\" represents the middle and concluding segments of\nthe named entity. Lastly, the letter \"O\" denotes text segments\nthat do not contain a named entity [68].\nAfter carefully applying the \"BIO\" labelling scheme to the\nnews texts, we were able to obtain a substantial data-set with\nlabelled information. Our statistical analysis after labelling\nrevealed an impressive count of 39,500 entities across six\ndifferent categories, as shown in Table 2. Figure 4 shows\nthe methodology of the current research work. Initially, news\narticles are gathered through a web crawler and News API,\nsystematically aggregating them into a news database. This\ncorpus of text is then subject to data cleaning and annota-\ntion processes, during which irrelevant information is purged\nand relevant entities are marked according to predefined\ncategories such as \"Recession\" (RRE) and \"China\" (GPU).\nFollowing this preparatory phase, the text is tokenized and\nmasked in preparation for Named Entity Recognition (NER),\na crucial step for understanding the context and extract-\ning meaningful information. Various Transformer-based NER\nmodels, including BERT, RoBERTa, DistilBERT, ALBERT,\nELECTRA, T5, and GPT-3, are evaluated to determine the\nmost effective framework for the task. The chosen model\nis then trained, tested, and validated using the annotated\ndatasets. The annotated entities in the example news article\nTABLE 2: Annotated Corpus’s Entities Number\nEntity Category Occurrences Examples\nPER Person’s names 560 John Smith, Angela\nHughes\nRRE The most relevant risk events\nfrom risk taxonomy\n3674 Inflation, Labour Strike\nPNR Political, Nationalities, and Re-\nligious groups\n570 Labor Party, Australian\nnationals\nOSC Organisations, Suppliers, and\nCompanies\n1416 BHP Billiton, Sydney\nConstructions Pty Ltd\nGPU Geo Political Units 3606 New South Wales, Vic-\ntoria\nCMS Construction Materials 570 Steel, Concrete\nillustrate the application of the methodology, with entities\nsuch as \"Recession\" and \"Interest rate\" tagged as RRE, indi-\ncating risk events, while \"China\" and \"International Monetary\nFund\" are tagged as GPU and OSC, respectively, highlighting\ngeopolitical units and organizations.\n2) IAA Analysis for Annotated Corpus in CSCRM\nThis section presents the methodology and findings of the\nInter-Annotator Agreement (IAA) analysis conducted to en-\nsure the accuracy and reliability of our NER dataset anno-\ntations. Given the critical role of high-quality annotations in\nNER research, especially within specialized domains such as\nCSCRM, assessing annotation consistency is paramount [69].\nInitially, the paper’s authors annotated the entire dataset,\nensuring a preliminary layer of domain-specific insights\nand linguistic accuracy. Subsequently, a specialized valida-\ntion team—comprising two experts in construction manage-\nment, one in linguistics, and two in computational linguis-\ntics—conducted a thorough review of these annotations. This\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nteam’s primary role was to validate the accuracy, relevance,\nand computational suitability of the initial annotations. Fol-\nlowing this validation phase, statistical analyses, including\nInter-Annotator Agreement metrics, were calculated to quan-\ntify the consistency and reliability of the annotations, ensur-\ning the dataset’s robustness for NER tasks within CSCRM.\nCohen’s Kappa ( κ) measures the agreement between two\nannotators on a categorical scale. It is calculated as:\nκ = po − pe\n1 − pe\n(3)\nwhere po is the observed agreement proportion, and pe is\nthe proportion of agreement expected by chance. Observed\nagreement ( po) is calculated by dividing the number of in-\nstances both annotators agree on by the total number of\ninstances. Expected agreement (pe) is based on the probability\nthat annotators randomly agree, considering the distribution\nof each category.\nFleiss’ Kappa ( κ) assesses agreement among three or more\nannotators and is calculated with:\nκ =\n¯P − ¯Pe\n1 − ¯Pe\n(4)\nwhere ¯P is the average proportion of agreement observed\nbetween all pairs of annotators, and ¯Pe is the expected agree-\nment by chance. Calculating ¯P involves averaging the propor-\ntions of agreement across all annotators for each item, while\n¯Pe considers the chance agreement across all categories. Table\n3 shows the results of IAA in this study.\nTABLE 3: Inter-Annotator Agreement Results for Entity Cat-\negories\nEntity Cohen’s Kappa ( κ) Fleiss’ Kappa ( κ)\nPER 0.90 -\nRRE 0.85 -\nPNR 0.80 -\nOSC 0.88 -\nGPU 0.92 -\nCMS 0.87 -\nOverall Fleiss’ Kappa 0.86\nThe IAA results, featuring both Cohen’s and Fleiss’ Kappa\nscores, indicate a high degree of consistency among annota-\ntors across various entity categories relevant to construction\nsupply chain risk management. Specifically, Cohen’s Kappa\nscores range from 0.80 to 0.92, reflecting substantial to almost\nperfect agreement on entities from personal names to geopo-\nlitical units and construction materials. The overall Fleiss’\nKappa score of 0.86 further underscores a strong consen-\nsus among all annotators, affirming the dataset’s reliability\nand the effectiveness of the multidisciplinary annotation ap-\nproach.\n3) Data Preparation\nIn the division of our dataset into training, validation, and\ntest sets, we implemented a stratified shuffle split approach\nto maintain the distribution of labels consistent across all sub-\nsets. The stratification process ensured that each subset was\nTABLE 4: Experimental Setup of This Study\nType Configuration Features\nSoftware CUDA 11.5\nPython 3.9\nNumpy 1.21.2\nScikit-learn 0.24.2\nPandas 1.3.3\nTensorFlow 2.6\nPyTorch 1.9.0\nHardware Operating System CentOS Version 8.4\nVideo RAM 11 Gigabytes GDDR6\nRAM 32.0 Gigabytes\nProcessor Intel Core i7-12700, 12th Generation\nTABLE 5: Models’ Hyperparameters\nHyperparameters Values\nDrop Rate 0.50\nlr 3e-5\nBatch Size 32\nEpochs 10\nMax len 128\nrepresentative of the full dataset, with all categories of labels\nproportionally reflected in the training, validation, and test\nsets. This method is particularly important in our context to\navoid skewed or biased model training and evaluation, given\nthe uneven distribution of risk-related entities in construc-\ntion supply chain management articles. Before the split, the\ndataset was shuffled to guarantee the randomness of data dis-\ntribution, thereby preventing any potential order effects that\ncould influence the model’s learning pattern. The shuffling\nand stratification were performed using robust functionalities\nprovided by data processing libraries in Python, ensuring\nreproducibility and adherence to standard data preparation\npractices in machine learning.\nIn order to train the transformers models, it requires power-\nful computational resources. Table 4 provides detailed infor-\nmation about the hardware and software used in this exper-\niment, giving a comprehensive understanding of the infras-\ntructure that supported the training of the transformer models\nin this study.\nDuring the model training phase, the choice of hyper-\nparameters greatly affects the outcomes. To ensure consis-\ntency and reduce variability between experiments, this study\nused a fixed set of hyper-parameters for training different\nmodels. The important parameters involved in the training\nprocess are listed in Table 3. In this table, an epoch refers\nto one complete iteration over the entire training dataset, max\nlen indicates the maximum sequence length, batch size deter-\nmines how much data is processed in each training iteration,\nlr controls the rate of learning, and drop rate helps prevent\nover-fitting in the neural network.\nThe training phase commenced with the initialization of\nmodel weights, often starting from pre-trained checkpoints\navailable through the Hugging Face repository, which pro-\nvided a foundation of linguistic knowledge beneficial for\ndownstream tasks. The data was then pre-processed to align\nwith the input requirements of the transformer models, in-\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 4: Methodology of this research.\ncluding tokenization, encoding of the BIO tags, and appli-\ncation of the ’max len’ parameter to standardize sequence\nlengths. Each word in the corpus was converted into tokens,\nand special tokens were added where necessary to signify the\nbeginning, separation, and end of sentences, as required by\nthe model architectures.\nFor each epoch, the model processed the training data in\nbatches, with the ’batch size’ calibrated to ensure efficient\nmemory utilization and gradient updates. The training loop\ninvolved forward propagation of the batch through the model,\ncalculation of the loss function comparing the predicted entity\ntags with the correct tags, and back-propagation to adjust the\nweights of the model using the learning rate (’ lr’) as a factor\nin the weight update rule. The ’drop rate’ was strategically\napplied within the model’s layers to randomly deactivate\ncertain neurons, thereby encouraging the model to learn more\nrobust features that do not rely on any small set of neurons.\nModel validation occurred at the end of each epoch, where\na subset of the data, held out from the training set, was used\nto evaluate the model’s performance. This step was critical\nto monitor for over-fitting and to tune the hyper-parameters\nif necessary. The model with the best validation performance\nwas then selected for final evaluation. In the testing phase, the\nmodel’s generalization capabilities were rigorously assessed\nusing a distinct set of annotated news articles. This phase\nwas designed to simulate the model’s deployment in real-\nworld scenarios, where it would encounter data variations and\ncomplexities. The performance metrics—precision, recall,\nand the F1-score—were meticulously calculated, providing\nquantitative insights into the model’s accuracy, its ability\nto detect relevant entities (true positives), and its precision\nin not misclassifying non-relevant elements as entities (true\nnegatives).\nFor our study, we carefully divided the labelled data-set\ninto three separate subsets: the training set, validation set,\nand test set. We followed a distribution ratio of 8:1:1 to\nallocate the entities. This means that we had 31,600 entities\nfor training, 3,950 entities for validation, and another 3,950\nentities for testing. This division is crucial in order to train\nand evaluate models effectively and ensure their robustness\nand ability to generalise in line with suggestions from [70].\nTo study NER in CSCRM, we trained seven different models\nusing a designated training data-set. After training, we eval-\nuated the performance and effectiveness of these models on\na separate test set for NER tasks. This approach is similar\nto the method used by when evaluating multiple models to\ndetermine the best one for NER tasks in a similar domain [71].\nIV. RESULTS AND DISCUSSION\nThis section discusses the results of different models that were\nused for NER in news articles focusing on construction supply\nchain risk management.\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nA. RESPONSIBLE AI CONSIDERATIONS IN TRANSFORMER\nMODELS FOR NER\nIn the field of NER, it is important to explore the ethical\nimplications and responsible AI considerations that emerge\nwhen using transformer models like BERT, RoBERTa, Dis-\ntilBERT, ALBERT, ELECTRA, T5, and GPT-3. While these\nmodels have good numbers on their side, there are other\nfactors beyond their accuracy scores that reveal some eth-\nical problems associated with them including biases, data\nprivacy, transparency, and interpretability among others [72].\nThe most important thing is to ensure that predictions by the\nmodel are faithful and correspond with what the input pro-\nvided says as well as the world outside. Ensuring adherence\nto fidelity is one of the significant ways to determine whether\na transformer model can correctly identify named entities\nacross varying contexts and subtle language differentiations.\nThis means that thorough validation is needed especially in\nmulti-lingual or domain-specific NER applications to ensure\ndependable outputs that are contextually appropriate. Equally\nimportant for responsible AI discussions is Monotonicity\nanother often overlooked attribute. Trustworthiness requires\nassurance that small changes in input will result in predictable\nsmall changes in output; otherwise, it would not be reliable.\nAssessing how these models maintain monotonicity, espe-\ncially in scenarios where small alterations in input should not\nsignificantly impact NER predictions, serves as a benchmark\nfor their robustness and responsible usage [73].\nOther key factors are identification and mitigation of bi-\nases. The data they are trained on is varied and extensive,\nmeaning that these models are likely to inherit the societal\nbiases present in that data, which then causes distorted pre-\ndictions. Biased systems may have different accuracy rates\nacross different racial groups or reproduce stereotypes. This\nsection looks at how each model deals with bias through ad-\njusting the algorithm or curation of datasets, thereby showing\nits dedication towards ensuring equity in NER tasks. Also,\ntransparentness and interpretability are crucial for ensuring\nethical deployment of AI. Trust building comes from the\ncapability to explain model predictions as well as compre-\nhend why a particular decision-making process is used. It is\nimportant to establish how these transformer models enable\ninterpretation and whether they make clear case for predic-\ntions in order foster responsible use of AI in the NER area.\nB. EXPERIMENT DESIGN AND ASSESSMENT\nWhen evaluating the performance of different models in\nNER, precision (P), recall (R), and F1-score (F1) are com-\nmonly used metrics. These metrics help assess how well the\nmodels perform. The specific computational formulas for\nthese metrics are as follows:\nP = TP\nTP + FP (5)\nR = TP\nTP + FN (6)\nF1 = 2 × P · R\nP + R (7)\nwhere TP represents the number of correctly identified\nentities or true positives. FP denotes the number of incor-\nrectly identified entities or false positives. Lastly, FN signifies\nthe number of missed entities or false negatives. Precision,\nRecall, and F1 scores of the mentioned models are shown in\nFigure 5.\nC. MODEL EVALUATION AND COMPARISON\nIn the provided evaluation metrics (Precision, Recall, and F1\nScore), Table 4 presents a comparative analysis of the models.\nRoBERTa stands out with an impressive average F1 score of\n0.8580, which indicates a well-rounded performance in both\nprecision (0.9341) and recall (0.8023). On the other hand,\nT5 exhibits the highest average precision value of 0.9924 but\nsuffers from a low recall of 0.3645, resulting in a modest F1\nscore of 0.5115. These differences highlight varying capabil-\nities among the models in accurately identifying entities and\nretrieving relevant instances from the news dataset.\nWhen evaluating the performance of models in various\ncategories such as PER, RRE, PNR, OSC, GPU, and CMS,\nit becomes evident that each model has its strengths and\nweaknesses. In terms of precision, almost all models demon-\nstrate high accuracy in the PNR and CMS entities. Some even\nachieve a perfect score of 1.0000. However, the OSC entity\nposes challenges for all models. Though T5 exhibits the high-\nest precision score of 1.0000 in this category, its recall rate\nis notably lower. This suggests that factors like entity char-\nacteristics or variations in training data quality and quantity\nsignificantly impact the overall performance of these models\nacross different categories. The performance of models in\nNER tasks is significantly influenced by their underlying\narchitectures and training data. Transformer-based models\nlike BERT, RoBERTa, and DistilBERT excel in capturing\ncontextual relationships, which are crucial for NER tasks. On\nthe other hand, models like T5 and GPT-3 approach NER\ndifferently as text-to-text and generative models, respectively.\nGPT-3’s performance in NER tasks is generally lower than\nsupervised baselines due to the inherent gap between NER\n(a sequence labelling task) and GPT-3’s nature as a text\ngeneration model. However, adaptations such as GPT-NER\nhave been proposed to bridge this gap by transforming the\nsequence labelling task into a generation task that can be\neasily tailored for large language models like GPT-3 [60].\nMoreover, ELECTRA uses a unique pre-training task where\ntoken detection is replaced with distinguishing \"real\" from\n\"fake\" input data. This can potentially improve its NER per-\nformance by reducing false positives and negatives in entity\nrecognition [74]. When evaluating and selecting models for\nimplementation within the construction supply chain domain,\nit is crucial to consider both the architectural differences of\nthe models and the nature of the NER tasks. This analysis\nhighlights the significance of this dual consideration.\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 5: Statistical Results of Different Models for NER in CSCRM\nD. COMPARISON WITH LITERATURE\nThis section compares the results of this research with re-\ncent research works from literature, considering their perfor-\nmances and F1 scores. The transformative impact of trans-\nformer models on NER is evident across a range of linguis-\ntic tasks, as seen in recent studies. [75] compared BERT,\nRoBERTa, and XLNet to non-transformer models, finding\nthat the transformer models consistently outperformed others\nin terms of the F1 score, regardless of domain. This finding\nis reflected in the current data, which showcases RoBERTa’s\nleading performance with an average F1 score of 0.8580.\nThe work of [76] provides an interesting perspective by\nevaluating DistilBERT’s performance on medical texts. The\nfindings revealed that while DistilBERT achieves F1 scores\ncomparable to those of BERT models on medical texts, its\nefficiency in runtime and resource usage stands out. It is\nsuggested that DistilBERT achieves a balance between per-\nformance and operational efficiency, with an average F1 score\nof 0.8240, which may suggest some trade-offs in general-\nization capacity. [77] further underscore the capability of\nBERT Multilingual Cased and Uncased models, achieving\naverage F1 scores of 85.41 and 83.52, which suggests that\nmultilingual models retain high performance even in the\ncontext of Indonesian health insurance data. An additional\nlayer of insight is provided by [78], who explored the use of\nmachine translation to generate Persian named entity datasets,\nachieving an impressive F1 score of 85.11, highlighting the\npotential of transformer models in low-resource languages.\n[79] reported that the MuRIL (Large) model performs well in\nmultilingual NER tasks for Hindi and Bangla, with F1 scores\nof 0.69 and 0.59, respectively, indicating the adaptability of\ntransformer models across different languages.\nMoreover, the research by [80] on a transformer-based sys-\ntem for English NER achieved a macro F1 score of 72.50 on a\ntest set, reinforcing the effectiveness of these models in com-\nplex NER tasks. [81] demonstrated the utility of the T5 and\ntransformer encoder for multilingual complex NER, showing\nan increased model F1-score by 4% in English when subto-\nken checks were introduced. [82] proposed a cost-sensitive\ncontextualized model for Bangla NER, indicating the need for\nspecialized approaches for low-resource languages, achieving\nan F1 Macro score of 65.96. [83] introduced BioNerFlair,\noutperforming state-of-the-art models with an F1-score of\n90.17, suggesting that alternative architectures could offer\ncompetitive results in domain-specific tasks. [84] demon-\nstrated that ensembles of transformer-based models could\nfurther improve NER results, achieving a 76.36 F1-score.\nThese studies collectively illustrate the broad applicability\nand high performance of transformer models in NER tasks\nacross languages and domains. They provide a strong foun-\ndation for further refinement and application of these models\nin increasingly diverse linguistic environments. The compre-\nhensive analysis across these works underscores the robust-\nness of transformer architectures, particularly RoBERTa, in\nhandling the complexities inherent in NER tasks, as well\nas the nuanced capabilities of models like DistilBERT and\nBERT Multilingual in specific contexts. Continuing the ex-\nploration of transformer model capabilities, [85] introduced\nthe Multi-feature Fusion Transformer (MFT) for Chinese\nNER, significantly outperforming mainstream models with\nan F1 score of 95.77 on the Resume dataset, demonstrating\nthe potential of incorporating character radical information\nin enhancing semantic understanding. This underscores the\nadaptability of transformer models to the intricacies of logo-\ngraphic languages.\n[86] conducted a comparative study of pre-trained lan-\nguage models for NER in clinical trial eligibility criteria, find-\ning that domain-specific transformer models like PubMed-\nBERT outperformed general transformer models, indicating\nthe importance of domain-specific pre-training for achieving\nhigh F1 scores in specialized tasks. [87] demonstrated that\nmulti-task learning using transformer models could improve\nthe F1 score by 2% in biological named entity recognition\ntasks, highlighting the benefits of joint training on related\nNLP tasks. In the medical domain, [88] employed transformer\nencoder and pre-trained language models for Chinese medical\nNER, achieving an F1 score of 82.72, showcasing the effec-\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ntiveness of model ensembles in technical fields. [89] used a\nmulti-embeddings approach coupled with deep learning for\nArabic NER, achieving an F1 score of 77.62 on the AQMAR\ndataset, thus setting a new performance standard for NER in\nArabic. [90] utilized the transformer model XLM-Roberta for\nHindi NER, achieving F1-scores of 0.96 (micro) and 0.80\n(macro), further validating the effectiveness of transformers\nin language-specific NER tasks.\nThese studies not only reinforce the state-of-the-art status\nof transformer models in NER but also illustrate the ongoing\ninnovations aimed at optimizing their performance and ex-\npanding their applicability across languages and specialized\ndomains.\nE. RELATIONSHIP BETWEEN ENTITY CATEGORIES AND\nTHEIR AMOUNTS\nAs shown in Figure 4, the performance of different models\nin identifying entities varies significantly, especially when\ncompared to the frequency of occurrence for each entity\ntype. Entities that occur more frequently, such as RRE (3674)\nand GPU (3606), generally have higher precision and re-\ncall scores across most models. This indicates that having a\nlarger dataset contributes to better model performance. For\nexample, models like BERT, RoBERTa, and ELECTRA show\nnotably high F1 scores for entities like RRE and GPU. In a\nresearch paper, when examining various transformer models,\nincluding BERT, RoBERTa, and ELECTRA using a detailed\nemotion , it was found that the size of the model did not have\na significant impact on the task of emotion recognition. This\nsuggests that while data size may affect performance, the size\nand architecture of the models also play important roles [91].\nHowever, there are some exceptions to this pattern. Despite\nhaving an equal number of occurrences in the data-set (570),\nboth PNR and CMS exhibit fluctuating performance between\ncategories. This variability suggests that the quantity alone\nis not the sole factor determining model performance; the\ncomplexity or uniqueness of the entity type may also play a\nrole. For example, a comparative analysis also examined how\nwell these models recognise emotions from texts, providing\nfurther insight into their performance on entity recognition\ntasks across different categories and data-sets. Additionally,\na separate study focused on domain specific applications\nexplored these models’ ability to extract various clinical con-\ncepts, offering insights into their capacity to handle different\ntypes of entities and understanding how domain and data-set\nsize can impact model performance [92].\nRoBERTa consistently achieves the highest F1 score\namong the models, closely followed by BERT. This indicates\nthat having a large amount of data can improve model perfor-\nmance, but the architecture and training techniques are still\ncrucial factors. When it comes to tasks requiring a balanced\nprecision and recall, RoBERTa or BERT are considered the\nmost suitable options. Furthermore, a study on recognizing\nProtected Health Information (PHI) entities revealed differ-\nences in training times between these models, which could in-\ndirectly impact their performance on entity recognition tasks.\nThese findings suggest that training time and computational\nresources may also influence how well different models per-\nform in entity recognition tasks [93]. Furthermore, RoBERTa\ndiffers from BERT in several key aspects, including removing\nthe Next Sentence Prediction (NSP) task, training with larger\nmini-batches and learning rates, and using longer sequences.\nThese changes enable RoBERTa to better capture context\nand semantics, crucial for tasks like NER that require under-\nstanding of complex sentence structures and subtle language\ncues. Furthermore, RoBERTa is trained on a much larger\ncorpus than BERT, which allows it to develop a more nuanced\nunderstanding of language. These modifications contribute\nto RoBERTa’s superior performance in scenarios demanding\nhigh precision and recall, making it a more suitable option for\nbalanced performance in NER tasks [94].\nWhen using these models, it’s important to consider both\nthe frequency of the entity in the data and its complexity to\nensure the best results. Research on NER using RoBERTa and\nELECTRA models has shown that performance varies de-\npending on the specific model and dataset used. For instance,\nan ELECTRA-based model performed better than BERT-\nbased models when working with a dataset related to drugs,\nas measured by its F1 score. This highlights how the choice\nof model and characteristics of the dataset significantly im-\npact entity recognition performance [95]. It underscores the\nimportance of considering factors such as data availability,\nmodel architecture, and entity complexity in order to achieve\noptimal results in entity recognition tasks.\nF. IMPACT OF HYPER-PARAMETER FINE TUNING ON\nMODELS PERFORMANCE\nGrid search (GS) is a traditional technique used in fine-tuning\nparameters in machine learning and deep learning tasks, in-\ncluding NLP with popular models such as BERT. It system-\natically explores a range of parameter options, typically in a\nmethodical grid-like pattern, to determine the most effective\nparameters for a specific model. The strength of GS lies in\nits simplicity and thoroughness, ensuring that each combina-\ntion of parameters is evaluated to find the optimum setting.\nThis exhaustive approach, while computationally expensive,\nprovides a comprehensive survey of the parameter space,\noften leading to more reliable and robust model performance.\nStudies have shown that GS can be more efficient and produce\nbetter uniformity in parameter selection compared to random\nor heuristic methods, making it a reliable choice for achieving\nhigh accuracy and generalization in machine learning models\n[96]. While GS can be time-consuming and may require con-\nsiderable computational resources, its effectiveness in iden-\ntifying optimal parameters makes it a valuable tool in the\nmachine learning toolbox, especially for tasks requiring high\nprecision.\nFor instance, in a research, the authors used grid search to\nthoroughly refine BERT and other models using the DuoRC\ndataset. They focused on key hyper-parameters such as max-\nimum sequence length, maximum question length, document\nstride, and training batch size, tweaking them prior to train-\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 6: F1 scores of different models for each entity.\ning to enhance model performance [97]. Also, in common\npractice, GS is performed across a range of parameter values\nto figure out the combination that yields the best results for\na given task. This approach plays an especially crucial role\nin the NLP field, where parameters like learning rate, batch\nsize, and optimizer type can greatly affect the performance\nof models like BERT. The GS method works by thoroughly\nassessing models across a particular parameter grid, set up as\nfollows:\nG = {(p1, p2, . . . ,pn) | pi ∈ Pi} (8)\nwhere Pi shows the set of possible values for hyper-parameter\ni.\nThis study involves exploring different sets of hyper-\nparameters, including learning rates ( lr), batch size ( BS),\nepsilon (ε), and two unique optimizers—Adam and AdamW,\nas shown in Table 5. The set Pi represents the set of possible\nvalues for hyper-parameter i. This section specifically looks\nat how these factors affect the overall performance of models\nin NER tasks in construction supply chain risk management.\nThis is distinct from the previous section, which evaluated the\nperformance of models on individual entities.\nTABLE 6: HYPERPARAMETER SETS\nHyperparameter Values\nLearning Rate 1e-6, 5e-6, 1e-5, 3e-5, 5e-5, 1e-4, 5e-4, 1e-3\nEpsilon 1e-7, 1e-8, 1e-9\nBatch Sizes 16, 32, 64\nOptimizers Adam, AdamW\nUsing GS, this study found 144 unique combinations to\nassess how different mixes of hyper-parameters affect model\nperformance in NER tasks. Table VI shows the results of these\nassessments, pointing out the best combination for higher\nprecision, recall, and F1 score, as well as the most efficient\ncombination. It also considers the less successful combina-\ntions, giving a full view of performance across various hyper-\nparameter setups. When conducting hyper-parameter tuning,\nit’s essential to focus on models that are most relevant and\npromising for the task at hand. In this context, we concen-\ntrated on transformer models like BERT, RoBERTa, Distil-\nBERT, ALBERT, and ELECTRA, excluding T5 and GPT-3.\nVOLUME 11, 2023 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 7: Best Hyper-parameter Combinations Based on GS\nModel Hyper-parameters Precision Recall F1-score Efficiency (s)\nlr BS ε Optimizer\nBERT 3e-05 16 1e-9 Adam 0.7882 0.8449 0.8097 161.2068\n1e-4 32 1e-9 Adam 0.8032 0.7797 0.7824 145.9247\n3e-05 16 1e-9 Adam 0.7882 0.8449 0.8097 161.2068\n1e-3 64 1e-9 Adam 0.1372 0.1428 0.1399 134.9384\nRoBERTa 1e-3 64 1e-9 Adam 0.7753 0.8350 0.7944 106.7353\n1e-3 64 1e-9 Adam 0.7753 0.8350 0.7944 106.7353\n3e-05 32 1e-8 AdamW 0.7618 0.8412 0.7903 116.7825\n1e-06 64 1e-8 Adam 0.6850 0.7692 0.7138 106.5951\nDistilBERT 1e-3 16 1e-7 AdamW 0.7898 0.7963 0.7841 84.8277\n5e-05 32 1e-9 AdamW 0.7985 0.7787 0.7725 75.5385\n1e-3 64 1e-8 AdamW 0.7351 0.7998 0.7569 69.8459\n1e-3 64 1e-9 Adam 0.1378 0.1428 0.1403 68.8453\nALBERT 3e-5 32 1e-8 AdamW 0.8100 0.8029 0.8018 155.8440\n1e-3 16 1e-9 Adam 0.8235 0.7424 0.7738 163.4706\n5e-5 32 1e-7 AdamW 0.7900 0.83027 0.7994 155.8783\n1e-3 64 1e-9 Adam 0.1378 0.14285 0.14032 147.5978\nELECTRA 1e-3 32 1e-9 AdamW 0.7910 0.8054 0.7933 149.6477\n1e-3 32 1e-9 AdamW 0.7910 0.8054 0.7933 149.6477\n5e-5 16 1e-8 AdamW 0.7480 0.8201 0.7766 165.5166\n1e-3 64 1e-9 Adam 0.1378 0.1428 0.1403 137.6781\nThis decision was based on several key considerations. First,\ntransformer models have shown exceptional performance in\nunderstanding context and generating language, making them\nideal for a wide range of NLP tasks. Each of these models,\nfrom BERT’s pioneering architecture to ELECTRA’s effi-\nciency in understanding language, has unique strengths that\nmake them suitable for in-depth hyper-parameter optimiza-\ntion. Second, due to their architecture and training methods,\nmodels like T5 and GPT-3 require significantly more com-\nputational resources for training and tuning, which may not\nbe feasible or necessary for the specific objectives of our\nproject [98]. Moreover, GPT-3’s closed-source nature and\nlicensing limitations also posed a constraint [99]. Therefore,\nour focus on the selected transformer models was driven by\na balance of performance, resource availability, and specific\nmodel characteristics that align with our project goals. The\nbest hyperparameter combinations are shown in Table 6.\nIn the conducted grid search for named entity recognition\nwithin the context of Australian construction supply chain\nrisk management, distinct trends and implications have been\nrevealed through the comparative analysis of models such as\nBERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA, in\nrelation to various hyper-parameters and optimizers. It has\nbeen observed that competitive performance is exhibited by\nboth BERT and RoBERTa, with BERT slightly outperforming\nin terms of recall, indicative of its effectiveness in identifying\nrelevant entities. Conversely, RoBERTa is distinguished by\noffering a more balanced trade-off between precision and\nrecall, coupled with higher efficiency, positioning it as a time-\nefficient alternative.\nDistilBERT, characterized by its lighter architecture, has\nbeen noted for its efficiency, achieving this without signifi-\ncant sacrifices in precision and recall, thereby emerging as\na robust option under constraints of computational resources\nor time. In a different vein, ALBERT has been recognized\nfor its precision, especially under specific hyper-parameter\nconfigurations, rendering it particularly suitable for tasks\nwhere precise identification is critical. ELECTRA, while not\noutshining in specific metrics, has been acknowledged for\nproviding a consistent balance across various performance\nmeasures, which can be advantageous in scenarios demand-\ning uniform performance.\nFurther insights have been gained into the effects of hyper-\nparameters and optimizers, where the learning rate has been\nidentified as a critical factor influencing model performance.\nGenerally, lower learning rates have been found to yield better\nrecall and F1-scores, suggesting the benefit of a cautious\napproach in weight updating within this specific domain.\nHowever, it has been noted that excessively low learning\nrates might impair the learning capabilities of the model.\nConsistently, larger batch sizes have been associated with di-\nminished performance, indicating the effectiveness of smaller\nbatch sizes for this particular application.\nRegarding the choice of optimizer, no consistent preference\nhas been discerned between Adam and AdamW. However,\nit has been observed that models employing AdamW, par-\nticularly in the cases of DistilBERT and ALBERT, demon-\nstrate enhanced efficiency. This improved efficiency might be\nattributable to the weight decay strategy of AdamW, which\naids in expediting the convergence process. The findings\nunderscore the necessity for a tailored selection of models and\nhyper-parameters in named entity recognition tasks, with the\naim of aligning them with the specific requirements of the\ntask at hand. BERT and RoBERTa have been noted for their\nproficiency in recall, while DistilBERT and ALBERT excel in\nefficiency and precision, respectively. ELECTRA, as a model,\nstands out for its well-rounded performance. The employment\nof lower learning rates and smaller batch sizes has generally\nbeen found to be more effective, while the choice between\nAdam and AdamW optimizers appears to be more influenced\nby considerations of efficiency than by factors of precision,\nrecall, or F1-score. Figure 5 shows comparative analysis of\n14 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ntransformer models’ performance using Adam and AdamW\noptimizers across various hyper-parameters.\nWhen assessing precision, recall, and F1 scores in relation\nto optimizer choice, it is apparent that AdamW tends to\nenhance model performance across most models, suggesting\nits superiority in handling weight decay and perhaps aiding\nin generalization. However, the degree of this enhancement\nvaries, indicating differing levels of sensitivity among the\nmodels to the optimization method. Considering learning\nrates, a lower learning rate coupled with AdamW optimizer\nseems to consistently benefit models like BERT, ALBERT,\nand to some extent, RoBERTa, in achieving higher F1 scores.\nDistilBERT and Electra, on the other hand, exhibit a less\npronounced preference, indicating a potential robustness to\na wider range of learning rates or an architecture that is less\namenable to the subtle improvements offered by AdamW’s\nweight decay.\nThe impact of batch size on model performance, partic-\nularly with BERT and ALBERT models using the AdamW\noptimizer, reveals nuanced insights into their learning dynam-\nics. Larger batch sizes, when combined with AdamW, have\nshown to offer substantial benefits for models like BERT and\nALBERT, as evidenced by improved F1 scores, indicating\na stronger model performance due to the enhanced stability\nlarger batches provide [100]. This aligns with the observation\nthat these models can better utilize the stability and compu-\ntational efficiency offered by larger batch sizes, optimizing\nthe learning process more effectively. Conversely, models like\nDistilBERT and Electra do not show a marked preference\nfor batch size adjustments, suggesting an intrinsic efficiency\nthat may stem from their design and pre-training strategies.\nThese findings imply that while BERT and ALBERT bene-\nfit significantly from larger batch sizes, the performance of\nDistilBERT and Electra is less contingent on this hyperpa-\nrameter, possibly due to differences in model architecture or\noptimization needs. Interestingly, the role of epsilon values\nin the optimization process does not exhibit a clear pattern\nacross models, hinting that its impact may be overshadowed\nby the more pronounced effects of learning rates and batch\nsizes. This suggests a complex interplay of factors that in-\nfluence model performance beyond simple hyperparameter\nadjustments.\nThe research indicates that AdamW, particularly with\nlarger batch sizes and lower learning rates, is more conducive\nto optimizing BERT and ALBERT models, highlighting the\nimportance of fine-tuning optimization strategies for peak\nperformance. This preference suggests a reliance on precise\noptimization techniques to achieve optimal results. In con-\ntrast, RoBERTa’s performance, while also benefiting from\nAdamW, suggests a degree of inherent robustness within its\narchitecture, possibly making it less sensitive to specific opti-\nmizer configurations. The differential impact of optimization\nstrategies on these models underscores the importance of\ntailored approaches in leveraging the full capabilities of each\nmodel, with specific attention to batch size and optimizer\nselection as key factors in maximizing performance outcomes\n[101].\nIn summary, while AdamW generally provides a perfor-\nmance edge, the extent of its benefits varies by model, with\nBERT and ALBERT showing the greatest improvements,\nRoBERTa demonstrating moderate sensitivity, and Distil-\nBERT and Electra indicating a more optimizer-agnostic be-\nhavior. This reflects the complex interplay between model\narchitecture and optimization techniques, underscoring the\nnecessity for model-specific hyper-parameter tuning.\nG. MODELS’ TRAINING EVALUATION\nFigure 8 provides illustrates the training curves for vari-\nous NER models, with each subplot representing a different\nmodel or a variant of a model under different learning rates.\nThe models showcased are BERT, RoBERTa, DistilBERT,\nALBERT, and ELECTRA, with learning rates ranging from\n1e-5 to 5e-5. These models are evidently being applied to\nthe domain of construction supply chain risk management,\nfocusing on news article datasets.\nUpon closer examination of the training curves, it’s ev-\nident that the increase in learning rate from 1e-5 to 5e-5\nsignificantly impacts the speed at which models like BERT\nand ELECTRA reduce their training loss. This phenomenon,\nmarked by a steeper initial descent in loss curves, suggests\nan enhanced learning efficiency within the specified learning\nrate range. However, the increase to the higher learning rate\nof 5e-5 introduces a notable volatility in the loss reduction\nfor BERT and ELECTRA models. This volatility could imply\nthat these models are either reaching the saturation point\nof their learning capacity for the given dataset or are fac-\ning challenges in efficiently navigating the error landscape\nat such an accelerated rate of learning. This observation is\ncritical, as it highlights the delicate balance between speeding\nup the training process and ensuring stable learning progress\nwithout compromising the model’s ability to generalize from\nthe training data.\nOn the contrary, models like RoBERTa and ALBERT ex-\nhibit a different behavior under the same conditions. They\ndemonstrate smoother and more consistent training loss de-\ncreases across all examined learning rates, suggesting a supe-\nrior ability to manage the complexities of the dataset without\nbeing overly sensitive to changes in the learning rate. This\ndistinction in behavior can be attributed to the inherent ar-\nchitectural and pre-training differences among these models,\nwhich may influence their adaptability and resilience to learn-\ning rate adjustments. For instance, ALBERT’s parameter-\nreduction techniques designed to lower memory consumption\nand increase training speed might contribute to its smoother\nlearning curve. Similarly, RoBERTa’s optimized pre-training\napproach could play a role in its ability to maintain stable loss\nreduction across varying learning rates. These observations\nunderscore the importance of carefully selecting and tuning\nthe learning rate, as it has a profound impact on the train-\ning dynamics and ultimate performance of language models.\nUnderstanding these nuances can guide more efficient and ef-\nVOLUME 11, 2023 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 7: Comparative Analysis of Transformer Models’ Performance Using Adam and AdamW Optimizers Across\nVarious Hyperparameters.\n16 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nfective training strategies, tailoring the learning rate to match\nthe specific characteristics and capacities of different models.\nComparing the models directly, RoBERTa and ALBERT\nappear to achieve lower training losses at the end of 10 epochs\nfor all learning rates when compared to BERT, DistilBERT,\nand ELECTRA. This might indicate their greater efficacy in\ncapturing the nuances of named entity recognition within the\nspecialized context of construction supply chain risk man-\nagement. DistilBERT, while showing a favourable decline in\ntraining loss, does not reach as low a value as RoBERTa or\nALBERT, which may be due to its distilled nature and thus\na reduced capacity to model complex patterns in the data.\nELECTRA, albeit starting with a high training loss especially\nat lower learning rates, demonstrates a significant reduction\nas training progresses, suggesting its potential effectiveness\ngiven sufficient training time.\nH. ANALYSIS OF BERT MODEL ’S PERFORMANCE\nThe analysis of the confusion matrix for the BERT model\nin NER task, as depicted in Figure 9 and trained using the\noptimal combination of hyper-parameters as discussed in\nSection IV-F, reveals crucial insights into the model’s per-\nformance and areas for potential enhancement. Notably, a\nsignificant strength of the model is its ability to accurately\npredict certain labels, particularly RRE and GPU. The high\naccuracy in classifying RRE, which pertains to risks identi-\nfied by the risk taxonomy related to the construction supply\nchain, demonstrates the model’s adeptness in learning and\ndistinguishing the unique characteristics of this specific entity\nclass. This proficiency is indicative of the model’s capacity\nto discern and accurately identify entities that are critical in\nthe context of construction supply chain management. Such\nan ability is instrumental in ensuring the effectiveness of risk\nmanagement processes within this domain, as it facilitates the\naccurate and timely identification of potential risks, thereby\nenabling more informed and strategic decision-making. The\nmodel’s success in precisely classifying RRE entities can be\nattributed to its sophisticated learning algorithms and the fine-\ntuning of hyper-parameters, which collectively contribute to\nits nuanced understanding and recognition of these complex\nentity types.\nAnother positive point is the potential for the model to\nimprove in areas where confusion is currently observed, such\nas the misclassification between GPU and OSC, and the\nmisidentification of PER as RRE. These instances of con-\nfusion, while highlighting current limitations, also present\nopportunities for enhancement. As noted in existing research,\nconfusion in NER tasks often stems from word ambiguity\nand the model’s challenge in context-based disambiguation\n[102]. This suggests that the model has a foundational ability\nto process and classify entities but requires further refinement\nin handling contextual nuances.\nThe application of advanced techniques such as contrastive\nlearning can significantly augment the model’s capability to\ndiscern subtle differences between similar entities. Moreover,\nincorporating knowledge-guided approaches and adopting a\nquestion-answering framework in the training process, as\nfound in research [103], can further enhance the model’s\nperformance. These methods provide a more comprehensive\nlinguistic context, enabling the model to better understand\nand differentiate complex entity structures. By integrating\nthese improvements, the model’s ability to accurately clas-\nsify entities, particularly those currently prone to confusion,\ncan be substantially enhanced, thus leveraging its existing\nstrengths while addressing its limitations.\nFigure 10 not only illustrates the efficacy of the BERT\nmodel in NER tasks but also provides a comparative anal-\nysis highlighting its versatility and precision in identifying\nvarious entity types. The Receiver Operating Characteristic\n(ROC) curves, differentiated by color for each tag, serve as a\nvisual testament to the model’s capability to discern between\nrelevant and irrelevant entities with remarkable accuracy. The\nArea Under the Curve (AUC) scores, which range from 0.79\nto 0.97, underscore a significant degree of model proficiency,\nwith the \"GPU\" tag achieving an AUC of 0.97, nearly perfect\nin its predictive capability. This exceptional performance on\nthe \"GPU\" tag indicates BERT’s adeptness at distinguishing\nentities with high specificity, minimizing false positive rates,\nand effectively handling data imbalances.\nFurthermore, the model’s performance across a variety of\nother tags such as \"CMS\", \"PER\", \"RRE\" , and \"OSC\" with\nAUC scores consistently above 0.8 reveals its broad appli-\ncability and robustness in processing and classifying diverse\ntypes of entities. This versatility is particularly important in\ncomplex NER tasks where the context and subtle nuances of\nlanguage play a critical role in accurate entity recognition.\nBERT’s architecture, leveraging pre-trained language mod-\nels on vast corpora, enables it to capture and utilize these\nnuances, setting a benchmark in the field. The comparative\nanalysis further highlights the model’s strengths and areas of\nexcellence, showcasing its leading-edge performance in the\nlandscape of natural language processing tools. Such insights\naffirm the advanced capabilities of BERT in handling NER\ntasks, illustrating its pivotal role in advancing text analysis\nand processing technologies.\nV. CONCLUSIONS AND FUTURE DIRECTIONS\nThis study has demonstrated the effectiveness of various\ntransformer-based models in NER within the Australian con-\nstruction supply chain risk management context, specifically\nusing news articles. Models like BERT, RoBERTa, Distil-\nBERT, ALBERT, and ELECTRA were evaluated, highlight-\ning their respective strengths in processing and analyzing tex-\ntual data for risk identification and management. A limitation\nof this study is the exclusion of the T5 and GPT-3 models\nfrom the grid search analysis.\nFurthermore, through the integration of advanced NER\ntechnologies, such as BERT, RoBERTa, and ELECTRA, the\nAustralian construction sector enhances its ability to navi-\ngate international market dynamics and geopolitical shifts,\nthereby enabling more resilient and responsive supply chain\noperations through timely risk identification and proactive\nVOLUME 11, 2023 17\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 8: Training curves of different transformer models in NER\nmanagement strategies.\nA. MODEL PERFORMANCE\nThe comparative analysis of different transformer models\nrevealed varying levels of efficacy in NER tasks. Models like\nBERT and RoBERTa showed robust performance, particu-\nlarly in terms of precision and recall, indicating their suitabil-\nity for extracting relevant entities from complex textual data.\nThese insights are crucial for advancing the field of NER and\nits application in construction supply chain risk management.\nB. PROJECT MANAGEMENT PERFORMANCE\nSophisticated transformer models like BERT, RoBERTa, and\nELECTRA have revolutionized project management, particu-\nlarly in the construction industry. Their ability to analyze vast\namounts of text data, including global news trends, allows\nproject managers to detect early warning signs of potential\ndisruptions in the supply chain. This capability is crucial in\nareas like the Australian construction sector, where interna-\ntional market dynamics and geopolitical shifts significantly\nimpact operations. By leveraging NER technologies, project\nmanagers gain a nuanced understanding of the supply chain\nenvironment, enhancing their ability to navigate risk factors\nefficiently, and ensuring projects stay on course amidst global\nuncertainties. These tools not only provide detailed risk pro-\nfiles but also make project planning more agile, helping man-\nagers to stay aligned with timelines and budgets.\nThe adaptability of transformer models to specific do-\n18 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 9: Heat map of confusion matrix of BERT model.\nmains, as shown in various studies, allows for fine-tuning to\nrecognize unique terminologies and nuances in the construc-\ntion industry. Integrating these advanced NER systems into\nanalytical frameworks significantly enhances risk assessment\nand management strategies. Practitioners can sift through\nnews data to extract vital information such as disruptions,\nmarket changes, or regulatory updates. This enhanced entity\nrecognition capability means even subtle references or com-\nplex entity relationships in news articles are accurately iden-\ntified, offering a comprehensive view of potential risks and\nopportunities. Consequently, this leads to more resilient and\nresponsive supply chain operations, empowering decision-\nmakers with timely insights and fostering a proactive ap-\nproach to risk management in the construction supply chain.\nFurthermore, the findings underscore the importance of\nNER in enhancing supply chain resilience and proactive risk\nmanagement in the construction industry. As highlighted by\n[104], recognizing company names from textual data is chal-\nlenging due to the diverse ways a company can be referenced.\nNER systems that can accurately identify these entities are\ncrucial in risk management, especially for non-exchange-\nlisted entities where obtaining timely information is challeng-\ning. The use of NER in constructing company-relationship\ngraphs is particularly beneficial in risk management within\ninstitutions, allowing for better understanding and mitigation\nof risks in the supply chain.\nSecondly, the deployment of NER systems contributes sig-\nnificantly to the proactive management of supply chain risks.\n[105] emphasize the significance of monitoring industry-\nrelevant events for supply chain management. NER facili-\ntates the extraction of specific events and entities from high-\nvolume, heterogeneous text streams, such as traffic reports,\ntweets, and news articles, which are crucial for anticipating\nand managing potential disruptions in the construction supply\nchain. By enabling the extraction of fine-grained entities and\nrelationships, NER systems enhance the responsiveness and\nadaptability of supply chain risk management strategies.\nLastly, the deployment of transformer models with en-\nhanced entity recognition capability is instrumental in driving\nconstruction supply chain resilience and responsiveness, as\nevidenced by recent studies [106, 107, 108, 109]. This techno-\nlogical advancement is pivotal in navigating the complexities\nof supply chain management, enhancing risk identification\nand mitigation strategies, ensuring sustained operational ef-\nficiency amid disruptions.\nVOLUME 11, 2023 19\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 10: Receiver Operating Characteristic (ROC) curve of BERT model.\nC. FUTURE DIRECTIONS\nIn the exploration of alternative transformer models, further\nstudies can consider trying out XLNet and DeBERTa. XL-\nNet’s strength lies in its ability to capture long-range de-\npendencies and handle previously unseen entities, making it\nhighly effective for NER tasks in varied domains, including\ncomplex and technical ones like construction [110, 111, 112].\nThis capability is particularly useful in construction project\nmanagement, where the ability to accurately identify and\nclassify entities from project documentation can enhance risk\nmanagement and sequence planning.\nDeBERTa, on the other hand, improves upon BERT and\nXLNet by incorporating disentangled attention mechanisms,\nenabling more precise and contextually aware entity recog-\nnition [113]. This enhanced attention to contextual detail is\ncrucial in the construction industry, where documents often\ncontain a mix of technical terms, project-specific jargon,\nand standard language. DeBERTa’s advanced feature extrac-\ntion and contextual understanding can lead to more accurate\nidentification of risks and entities, thus contributing to more\nefficient project management.\n• To further improve the performance of NER models in\nconstruction risk management, a more detailed strategy for\ntuning hyperparameters could be employed. This involves\nexpanding the scope of the grid search to consider a wider\nrange of parameters for the Adam and AdamW optimizers.\nFor example, different weight decay rates or learning rate\nschedules could be explored.\n• The integration of NER capabilities into project manage-\nment software could greatly improve the risk identification\nprocess. This integration would provide project managers\nwith real-time alerts and suggestions, leveraging the latest\nnews and market trends. As a result, project management\nbecomes more proactive and adaptive.\n• Sentiment analysis is a valuable tool for risk assessment.\nBy combining NER with sentiment analysis, we can gain\na better understanding of the potential impact of identified\nrisks. By assessing the sentiment of news articles and reports,\nwe can prioritise risks based on their urgency.\n• To enhance the performance of transformer models such\nas BERT, T5, GPT-3, RoBERTa, DistilBERT, and ELEC-\nTRA in NER, future studies can adopt a variety of inno-\nvative approaches. Ensemble methods that combine multi-\nple transformer-based models can demonstrate significant\nimprovements in performance, especially in handling com-\nplex, cross-domain texts. Integrating language model pre-\ntraining with NER fine-tuning would be another effective\nstrategy, particularly in enhancing cross-domain and cross-\nlingual generalization abilities of these models. Addition-\nally, the adoption of multi-task learning approaches, where\nauxiliary tasks like entity boundary prediction and entity\ntype prediction are integrated into the initial layers of the\ntransformer, can show promise in better leveraging the lower\nlayers for more robust character representation. Furthermore,\n20 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nthe development of noise reduction models, particularly those\nutilizing XLNet encoding and CRF decoding, can effectively\naddresse the challenges of noise interference in both the pre-\ntraining and fine-tuning stages.\nREFERENCES\n[1] J. Hirschberg and C. D. Manning, ‘‘Advances in natural\nlanguage processing,’’ Science, vol. 349, no. 6245, pp.\n261–266, 2015.\n[2] P. M. Nadkarni, L. Ohno-Machado, and W. W. Chap-\nman, ‘‘Natural language processing: an introduction,’’\nJournal of the American Medical Informatics Associ-\nation, vol. 18, no. 5, pp. 544–551, 2011.\n[3] M. Zampieri, P. Nakov, and Y . Scherrer, ‘‘Natural lan-\nguage processing for similar languages, varieties, and\ndialects: A survey,’’ Natural Language Engineering ,\nvol. 26, pp. 595 – 612, 2020.\n[4] A. N. M. F. Faisal, M. A. Rahman, and T. Farah,\n‘‘A rule-based bengali grammar checker,’’ 2021 Fifth\nWorld Conference on Smart Trends in Systems Security\nand Sustainability (WorldS4) , pp. 113–117, 2021.\n[5] D. Newman-Griffis, J. F. Lehman, C. Rosé, and\nH. Hochheiser, ‘‘Translational nlp: A new paradigm\nand general principles for natural language processing\nresearch,’’ vol. 2021, p. 4125, 2021.\n[6] D. Khurana, A. Koli, K. Khatter, and S. Singh, ‘‘Natu-\nral language processing: State of the art, current trends\nand challenges,’’ Multimedia tools and applications ,\nvol. 82, no. 3, pp. 3713–3744, 2023.\n[7] E. Ghazizadeh and P. Zhu, ‘‘A systematic literature\nreview of natural language processing: Current state,\nchallenges and risks,’’ in Proceedings of the Future\nTechnologies Conference. Springer, 2020, pp. 634–\n647.\n[8] K. Chowdhary and K. Chowdhary, ‘‘Natural language\nprocessing,’’ Fundamentals of artificial intelligence ,\npp. 603–649, 2020.\n[9] C. Biemann, ‘‘Theory and applications of natural lan-\nguage processing,’’ 2010.\n[10] S. Francis, J. Van Landeghem, and M.-F. Moens,\n‘‘Transfer learning for named entity recognition in\nfinancial and biomedical documents,’’ Information,\nvol. 10, no. 8, p. 248, 2019.\n[11] M. Koroteev, ‘‘Bert: a review of applications in nat-\nural language processing and understanding,’’ arXiv\npreprint arXiv:2103.11943, 2021.\n[12] S. O. Abioye, L. O. Oyedele, L. Akanbi, A. Ajayi,\nJ. M. D. Delgado, M. Bilal, O. O. Akinade, and\nA. Ahmed, ‘‘Artificial intelligence in the construction\nindustry: A review of present status, opportunities and\nfuture challenges,’’ Journal of Building Engineering ,\nvol. 44, p. 103299, 2021.\n[13] J. P. Chiu and E. Nichols, ‘‘Named entity recogni-\ntion with bidirectional lstm-cnns,’’ Transactions of the\nassociation for computational linguistics , vol. 4, pp.\n357–370, 2016.\n[14] S. Wang, R. Xu, B. Liu, L. Gui, and Y . Zhou, ‘‘Fi-\nnancial named entity recognition based on conditional\nrandom fields and information entropy,’’ in 2014 inter-\nnational conference on machine learning and cyber-\nnetics, vol. 2. IEEE, 2014, pp. 838–843.\n[15] M. Miwa and M. Bansal, ‘‘End-to-end relation extrac-\ntion using lstms on sequences and tree structures,’’\narXiv preprint arXiv:1601.00770 , 2016.\n[16] M. E. Peters, M. Neumann, L. Zettlemoyer, and\nW.-t. Yih, ‘‘Dissecting contextual word embeddings:\nArchitecture and representation,’’ arXiv preprint\narXiv:1808.08949, 2018.\n[17] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Ka-\nplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sas-\ntry, A. Askell et al. , ‘‘Language models are few-shot\nlearners,’’ Advances in neural information processing\nsystems, vol. 33, pp. 1877–1901, 2020.\n[18] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,\n‘‘Bert: Pre-training of deep bidirectional transform-\ners for language understanding,’’ arXiv preprint\narXiv:1810.04805, 2018.\n[19] C. Sutton, A. McCallum et al. , ‘‘An introduc-\ntion to conditional random fields,’’ Foundations and\nTrends® in Machine Learning , vol. 4, no. 4, pp. 267–\n373, 2012.\n[20] P. Zou and P. Couani, ‘‘Managing risks in green build-\ning supply chain,’’ Architectural Engineering and De-\nsign Management, vol. 8, pp. 143 – 158, 2012.\n[21] T. Wang, Q. Zhang, H. Chong, and X. Wang, ‘‘In-\ntegrated supplier selection framework in a resilient\nconstruction supply chain: An approach via analytic\nhierarchy process (ahp) and grey relational analysis\n(gra),’’Sustainability, vol. 9, p. 289, 2017.\n[22] P. Zou, D. Mcgeorge, and S. S. Ng, ‘‘Small and\nmedium-sized enterprises’ perspectives towards con-\nstruction supply chain management and e-commerce,’’\nInternational Journal of Construction Management ,\nvol. 5, pp. 1 – 19, 2005.\n[23] C. Berragan, A. Singleton, A. Calafiore, and J. Mor-\nley, ‘‘Transformer based named entity recognition for\nplace name extraction from unstructured text,’’ In-\nternational Journal of Geographical Information Sci-\nence, vol. 37, no. 4, pp. 747–766, 2023.\n[24] C.-M. Tsai, ‘‘Stylometric fake news detection based\non natural language processing using named entity\nrecognition: In-domain and cross-domain analysis,’’\nElectronics, vol. 12, no. 17, p. 3676, 2023.\n[25] Y . Bengio, R. Ducharme, and P. Vincent, ‘‘A neural\nprobabilistic language model,’’ Advances in neural in-\nformation processing systems , vol. 13, 2000.\n[26] T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‘‘Ef-\nficient estimation of word representations in vector\nspace,’’arXiv preprint arXiv:1301.3781 , 2013.\n[27] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado,\nand J. Dean, ‘‘Distributed representations of words\nand phrases and their compositionality,’’ Advances in\nVOLUME 11, 2023 21\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nneural information processing systems , vol. 26, 2013.\n[28] S. J. Johnson, M. R. Murty, and I. Navakanth, ‘‘A\ndetailed review on word embedding techniques with\nemphasis on word2vec,’’ Multimedia Tools and Appli-\ncations, pp. 1–29, 2023.\n[29] F. Almeida and G. Xexéo, ‘‘Word embeddings: A sur-\nvey,’’arXiv preprint arXiv:1901.09069 , 2019.\n[30] T.-H. Yang, M. Pleva, D. Hládek, and M.-H. Su,\n‘‘Bert-based chinese medicine named entity recog-\nnition model applied to medication reminder dia-\nlogue system,’’ in 2022 13th International Symposium\non Chinese Spoken Language Processing (ISCSLP) .\nIEEE, 2022, pp. 374–378.\n[31] C. Yang, L. Sheng, Z. Wei, and W. Wang, ‘‘Chinese\nnamed entity recognition of epidemiological investi-\ngation of information on covid-19 based on bert,’’ Ieee\nAccess, vol. 10, pp. 104 156–104 168, 2022.\n[32] D. Chen, C. Liu, and Z. Zhao, ‘‘Named entity recog-\nnition service of bert-transformer-crf based on multi-\nfeature fusion for chronic disease management,’’ in In-\nternational Conference on Service Science . Springer,\n2023, pp. 166–178.\n[33] Y . Yu, Y . Wang, J. Mu, W. Li, S. Jiao, Z. Wang,\nP. Lv, and Y . Zhu, ‘‘Chinese mineral named entity\nrecognition based on bert model,’’ Expert Systems with\nApplications, vol. 206, p. 117727, 2022.\n[34] X. Tang, Y . Huang, M. Xia, and C. Long, ‘‘A multi-task\nbert-bilstm-am-crf strategy for chinese named entity\nrecognition,’’Neural Processing Letters, vol. 55, no. 2,\npp. 1209–1229, 2023.\n[35] S. Gorla, S. S. Tangeda, L. B. M. Neti, and A. Mala-\npati, ‘‘Telugu named entity recognition using bert,’’\nInternational Journal of Data Science and Analytics ,\nvol. 14, no. 2, pp. 127–140, 2022.\n[36] M. Jarrar, M. Khalilia, and S. Ghanem, ‘‘Wojood:\nNested arabic named entity corpus and recognition\nusing bert,’’ in Proceedings of the Thirteenth Lan-\nguage Resources and Evaluation Conference , 2022,\npp. 3626–3636.\n[37] Z. Lun, Z. Hui et al., ‘‘Research on agricultural named\nentity recognition based on pre train bert,’’ Academic\nJournal of Engineering and Technology Science, vol. 5,\nno. 4, pp. 34–42, 2022.\n[38] C. V . Ndukwe, J. Liu, and T. K. Chan, ‘‘Impact of\ncovid-19 on the china-australia construction supply\nchain,’’ in Proceedings of the 25th International Sym-\nposium on Advancement of Construction Management\nand Real Estate . Springer, 2021, pp. 1275–1291.\n[39] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,\n‘‘Attention is all you need,’’ Advances in neural infor-\nmation processing systems , vol. 30, 2017.\n[40] C. Huang, Y . Wang, Y . Yu, Y . Hao, Y . Liu, and\nX. Zhao, ‘‘Chinese named entity recognition of geo-\nlogical news based on bert model,’’ Applied Sciences ,\nvol. 12, no. 15, p. 7708, 2022.\n[41] Q. Qiu, Z. Xie, L. Wu, and L. Tao, ‘‘Automatic\nspatiotemporal and semantic information extraction\nfrom unstructured geoscience reports using text mining\ntechniques,’’ Earth Science Informatics , vol. 13, pp.\n1393–1410, 2020.\n[42] X. Lv, Z. Xie, D. Xu, X. Jin, K. Ma, L. Tao, Q. Qiu,\nand Y . Pan, ‘‘Chinese named entity recognition in the\ngeoscience domain based on bert,’’ Earth and Space\nScience, vol. 9, no. 3, p. e2021EA002166, 2022.\n[43] R. Fan, L. Wang, J. Yan, W. Song, Y . Zhu, and\nX. Chen, ‘‘Deep learning-based named entity recog-\nnition and knowledge graph construction for geolog-\nical hazards,’’ ISPRS International Journal of Geo-\nInformation, vol. 9, no. 1, p. 15, 2019.\n[44] X. Su, Z. Hong, Q. Zhang, C. Xue, and X. Li,\n‘‘Named entity recognition for chinese construction\ndocuments,’’ in International Symposium on Advance-\nment of Construction Management and Real Estate .\nSpringer, 2019, pp. 839–850.\n[45] P. Schönfelder and M. König, ‘‘Deep learning-based\nentity recognition in construction regulatory docu-\nments,’’ in ISARC. Proceedings of the International\nSymposium on Automation and Robotics in Construc-\ntion, vol. 38. IAARC Publications, 2021, pp. 387–\n394.\n[46] K. Jeon, G. Lee, S. Yang, and H. D. Jeong, ‘‘Named\nentity recognition of building construction defect in-\nformation from text with linguistic noise,’’ Automation\nin Construction, vol. 143, p. 104543, 2022.\n[47] X. Wu, T. Zhang, S. Yuan, and Y . Yan, ‘‘One improved\nmodel of named entity recognition by combining bert\nand bilstm-cnn for domain of chinese railway con-\nstruction,’’ in 2022 7th International Conference on\nIntelligent Computing and Signal Processing (ICSP) .\nIEEE, 2022, pp. 728–732.\n[48] Q. Zhang, C. Xue, X. Su, P. Zhou, X. Wang, and\nJ. Zhang, ‘‘Named entity recognition for chinese con-\nstruction documents based on conditional random\nfield,’’Frontiers of Engineering Management , vol. 10,\nno. 2, pp. 237–249, 2023.\n[49] X. Wang and N. El-Gohary, ‘‘Deep learning–based\nnamed entity recognition and resolution of referential\nambiguities for enhanced information extraction from\nconstruction safety regulations,’’ Journal of Comput-\ning in Civil Engineering , vol. 37, no. 5, p. 04023023,\n2023.\n[50] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang,\n‘‘Transformer in transformer,’’ Advances in Neural\nInformation Processing Systems , vol. 34, pp. 15 908–\n15 919, 2021.\n[51] A. Chernyavskiy, D. Ilvovsky, and P. Nakov, ‘‘Trans-\nformers:“the end of history” for natural language pro-\ncessing?’’ in Machine Learning and Knowledge Dis-\ncovery in Databases. Research Track: European Con-\nference, ECML PKDD 2021, Bilbao, Spain, September\n13–17, 2021, Proceedings, Part III 21 . Springer,\n22 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n2021, pp. 677–693.\n[52] K. Huangliang, X. Li, T. Yin, B. Peng, and H. Zhang,\n‘‘Self-adapted positional encoding in the transformer\nencoder for named entity recognition,’’ in Interna-\ntional Conference on Artificial Neural Networks .\nSpringer, 2023, pp. 538–549.\n[53] B. Ghojogh and A. Ghodsi, ‘‘Attention mechanism,\ntransformers, bert, and gpt: tutorial and survey,’’ 2020.\n[54] N. M. Foumani, C. W. Tan, G. I. Webb, and M. Salehi,\n‘‘Improving position encoding of transformers for\nmultivariate time series classification,’’ arXiv preprint\narXiv:2305.16642, 2023.\n[55] S. N. M. Foumani and A. Nickabadi, ‘‘A probabilistic\ntopic model using deep visual word representation\nfor simultaneous image classification and annotation,’’\nJournal of Visual Communication and Image Repre-\nsentation, vol. 59, pp. 195–203, 2019.\n[56] N. Sabharwal, A. Agrawal, N. Sabharwal, and\nA. Agrawal, ‘‘Bert algorithms explained,’’ Hands-on\nQuestion Answering Systems with BERT: Applications\nin Neural Networks and Natural Language Processing ,\npp. 65–95, 2021.\n[57] B. Wang, L. Shang, C. Lioma, X. Jiang, H. Yang,\nQ. Liu, and J. G. Simonsen, ‘‘On position embeddings\nin bert,’’ in International Conference on Learning Rep-\nresentations, 2020.\n[58] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\nO. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov,\n‘‘Roberta: A robustly optimized bert pretraining ap-\nproach,’’arXiv preprint arXiv:1907.11692 , 2019.\n[59] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, ‘‘Distil-\nbert, a distilled version of bert: smaller, faster, cheaper\nand lighter,’’ arXiv preprint arXiv:1910.01108 , 2019.\n[60] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma,\nand R. Soricut, ‘‘Albert: A lite bert for self-supervised\nlearning of language representations,’’ arXiv preprint\narXiv:1909.11942, 2019.\n[61] K. Clark, M.-T. Luong, Q. V . Le, and C. D. Man-\nning, ‘‘Electra: Pre-training text encoders as dis-\ncriminators rather than generators,’’ arXiv preprint\narXiv:2003.10555, 2020.\n[62] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y . Zhou, W. Li, and P. J. Liu, ‘‘Exploring\nthe limits of transfer learning with a unified text-to-\ntext transformer,’’ The Journal of Machine Learning\nResearch, vol. 21, no. 1, pp. 5485–5551, 2020.\n[63] S. Wang, X. Sun, X. Li, R. Ouyang, F. Wu, T. Zhang,\nJ. Li, and G. Wang, ‘‘Gpt-ner: Named entity recog-\nnition via large language models,’’ arXiv preprint\narXiv:2304.10428, 2023.\n[64] M. Zhang and J. Li, ‘‘A commentary of gpt-3 in\nmit technology review 2021,’’ Fundamental Research,\nvol. 1, no. 6, pp. 831–833, 2021.\n[65] A. Coburn, D. Ralph, M. Tuveson, S. Ruffle, and\nG. Bowman, ‘‘A taxonomy of threats for macro-\ncatastrophe risk management,’’ Centre for Risk Stud-\nies, Cambridge: University of Cambridge, Working\nPaper, July, pp. 20–24, 2013.\n[66] D. G. Ortiz, D. Myers, E. N. Walls, and M.-E. Diaz,\n‘‘Where do we stand with newspaper data,’’ vol. 10,\npp. 397–419, 2005.\n[67] C. Feng, M. Khan, A. U. Rahman, and A. Ahmad,\n‘‘News recommendation systems - accomplishments,\nchallenges future directions,’’ IEEE Access, vol. 8, pp.\n16 702–16 725, 2020.\n[68] E. F. Sang and F. De Meulder, ‘‘Introduction to the\nconll-2003 shared task: Language-independent named\nentity recognition,’’ arXiv preprint cs/0306050 , 2003.\n[69] L. V olkova and V . Bocharov, ‘‘An approach to inter-\nannotator agreement evaluation for the named entities\nannotation task at opencorpora,’’ Communications in\nComputer and Information Science , 2019.\n[70] S. Moon, G. Lee, S. Chi, and H. Oh, ‘‘Automated con-\nstruction specification review with named entity recog-\nnition using natural language processing,’’ Journal of\nConstruction Engineering and Management , vol. 147,\nno. 1, p. 04020147, 2021.\n[71] K. Jeon, G. Lee, S. Yang, and H. D. Jeong, ‘‘Named\nentity recognition of building construction defect in-\nformation from text with linguistic noise,’’ Automation\nin Construction, vol. 143, p. 104543, 2022.\n[72] M. Abdel-Basset et al. , ‘‘Responsible system based\non artificial intelligence for intelligent and climate-\nfriendly energy production,’’ Zagazig University , no.\n202022105963, 2022.\n[73] ——, ‘‘Responsible system based on artificial intelli-\ngence to reduce greenhouse gas emissions in 6g net-\nworks,’’Zagazig University, no. 202022105964, 2022.\n[74] K. Clark and T. Luong, ‘‘More efficient nlp model\npre-training with electra,’’ Preuzeto s from https://ai.\ngoogleblog. com/2020/03/more-efficient-nlp-model-\npre-training. html [4. srpnja 2021.] , 2020.\n[75] C. Lothritz, K. Allix, L. Veiber, T. F. Bissyandé,\nand J. Klein, ‘‘Evaluating pretrained transformer-based\nmodels on the task of fine-grained named entity recog-\nnition,’’ pp. 3750–3760, 2020.\n[76] M. Abadeer, ‘‘Assessment of distilbert performance\non named entity recognition task for the detection of\nprotected health information and medical concepts,’’\npp. 158–167, 2020.\n[77] B. S. Jati, S. Widyawan, and S. M. N. Rizal, ‘‘Mul-\ntilingual named entity recognition model for indone-\nsian health insurance question answering system,’’\n2020 3rd International Conference on Information and\nCommunications Technology (ICOIACT) , pp. 180–\n184, 2020.\n[78] A. Sartipi and A. Fatemi, ‘‘Exploring the potential\nof machine translation for generating named entity\ndatasets: A case study between persian and english,’’\nArXiv, vol. abs/2302.09611, 2023.\n[79] S. Singh, P. Jawale, and U. Tiwary, ‘‘silpa_nlp at\nsemeval-2022 tasks 11: Transformer based ner models\nVOLUME 11, 2023 23\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nfor hindi and bangla languages,’’ in Proceedings of the\n16th International Workshop on Semantic Evaluation\n(SemEval-2022), 2022, pp. 1536–1542.\n[80] N. M. Lai, ‘‘Lmn at semeval-2022 task 11: A\ntransformer-based system for english named entity\nrecognition,’’ pp. 1438–1443, 2022.\n[81] E. Tavan and M. Najafi, ‘‘Marsan at semeval-2022 task\n11: Multilingual complex named entity recognition\nusing t5 and transformer encoder,’’ pp. 1639–1647,\n2022.\n[82] I. Ashrafi, M. Mohammad, A. S. Mauree, G. M. A.\nNijhum, R. Karim, N. Mohammed, and S. Momen,\n‘‘Banner: A cost-sensitive contextualized model for\nbangla named entity recognition,’’ IEEE Access, vol. 8,\npp. 58 206–58 226, 2020.\n[83] H. Patel, ‘‘Bionerflair: biomedical named entity recog-\nnition using flair embedding and sequence tagger,’’\nArXiv, vol. abs/2011.01504, 2020.\n[84] E. Schneider, R. M. R. Zavala, P. Martínez, C. Moro,\nand E. Paraiso, ‘‘Uc3m-pucpr at semeval-2022 task 11:\nAn ensemble method of transformer-based models for\ncomplex named entity recognition,’’ pp. 1448–1456,\n2022.\n[85] X. Han, Q. Yue, J. Chu, Z. Han, Y . Shi, and C. Wang,\n‘‘Multi-feature fusion transformer for chinese named\nentity recognition,’’ 2022 41st Chinese Control Con-\nference (CCC), pp. 4227–4232, 2022.\n[86] J. Li, Q. Wei, O. Ghiasvand, M. Chen, V . Lobanov,\nC. Weng, and H. Xu, ‘‘A comparative study of pre-\ntrained language models for named entity recognition\nin clinical trial eligibility criteria from multiple cor-\npora,’’ BMC Medical Informatics and Decision Mak-\ning, vol. 22, 2021.\n[87] F. Deng, D. Zhang, and J. Peng, ‘‘Biological named\nentity recognition and role labeling via deep multi-\ntask learning,’’ 2021 13th International Conference on\nMachine Learning and Computing , 2021.\n[88] H. Tan, Z. Yang, J. Ning, Z. Ding, and Q. Liu, ‘‘Chinese\nmedical named entity recognition based on chinese\ncharacter radical features and pre-trained language\nmodels,’’ 2021 International Conference on Asian\nLanguage Processing (IALP) , pp. 121–124, 2021.\n[89] A. Youssef, M. Elattar, and S. El-Beltagy, ‘‘A multi-\nembeddings approach coupled with deep learning for\narabic named entity recognition,’’ 2020 2nd Novel In-\ntelligent and Leading Emerging Sciences Conference\n(NILES), pp. 456–460, 2020.\n[90] A. A. Choure, R. B. Adhao, and V . Pachghare, ‘‘Ner in\nhindi language using transformer model:xlm-roberta,’’\n2022 IEEE International Conference on Blockchain\nand Distributed Systems Security (ICBDS) , pp. 1–5,\n2022.\n[91] D. Cortiz, ‘‘Exploring transformers models for emo-\ntion recognition: a comparision of bert, distilbert,\nroberta, xlnet and electra,’’ in Proceedings of the 2022\n3rd International Conference on Control, Robotics and\nIntelligent System, 2022, pp. 230–234.\n[92] A. F. Adoma, N.-M. Henry, and W. Chen, ‘‘Com-\nparative analyses of bert, roberta, distilbert, and xlnet\nfor text-based emotion recognition,’’ in 2020 17th In-\nternational Computer Conference on Wavelet Active\nMedia Technology and Information Processing (IC-\nCWAMTIP). IEEE, 2020, pp. 117–121.\n[93] S. H. Oh, M. Kang, and Y . Lee, ‘‘Protected health\ninformation recognition by fine-tuning a pre-training\ntransformer model,’’Healthcare Informatics Research,\nvol. 28, no. 1, pp. 16–24, 2022.\n[94] D. Cortiz, ‘‘Exploring transformers models for emo-\ntion recognition: a comparision of bert, distilbert,\nroberta, xlnet and electra,’’ Proceedings of the 2022\n3rd International Conference on Control, Robotics and\nIntelligent System, 2022.\n[95] Y . Wu, J. Huang, C. Xu, H. Zheng, L. Zhang, and\nJ. Wan, ‘‘Research on named entity recognition of elec-\ntronic medical records based on roberta and radical-\nlevel feature,’’ Wireless Communications and Mobile\nComputing, vol. 2021, pp. 1–10, 2021.\n[96] C.-M. Huang, Y .-J. Lee, D. K. J. Lin, and S. Huang,\n‘‘Model selection for support vector machines via uni-\nform design,’’ Comput. Stat. Data Anal. , vol. 52, pp.\n335–346, 2007.\n[97] A. J. Quijano, S. Nguyen, and J. Ordonez, ‘‘Grid\nsearch hyperparameter benchmarking of bert, al-\nbert, and longformer on duorc,’’ arXiv preprint\narXiv:2101.06326, 2021.\n[98] L. Bottou, F. E. Curtis, and J. Nocedal, ‘‘Optimization\nmethods for large-scale machine learning,’’ SIAM Rev.,\nvol. 60, pp. 223–311, 2016.\n[99] G. Yenduri, G. Srivastava, P. K. R. Maddikunta, R. H.\nJhaveri, W. Wang, A. V . Vasilakos, T. R. Gadekallu\net al. , ‘‘Generative pre-trained transformer: A com-\nprehensive review on enabling technologies, potential\napplications, emerging challenges, and future direc-\ntions,’’arXiv preprint arXiv:2305.10435 , 2023.\n[100] S. Zheng, H. Lin, S. Zha, and M. Li, ‘‘Accelerated large\nbatch optimization of bert pretraining in 54 minutes,’’\nArXiv, vol. abs/2006.13484, 2020.\n[101] Z. Nado, J. Gilmer, C. J. Shallue, R. Anil, and G. E.\nDahl, ‘‘A large batch optimizer reality check: Tradi-\ntional, generic optimizers suffice across batch sizes,’’\nArXiv, vol. abs/2102.06356, 2021.\n[102] R. Zhou, Q. Hu, J. Wan, J. Zhang, Q. Liu, T. Hu, and\nJ. Li, ‘‘Wcl-bbcd: A contrastive learning and knowl-\nedge graph approach to named entity recognition,’’\nArXiv, vol. abs/2203.06925, 2022.\n[103] P. Banerjee, K. K. Pal, M. Devarakonda, and C. Baral,\n‘‘Biomedical named entity recognition via knowledge\nguidance and question answering,’’ ACM Transactions\non Computing for Healthcare , vol. 2, pp. 1 – 24, 2021.\n[104] M. Loster, Z. Zuo, F. Naumann, O. Maspfuhl, and\nD. Thomas, ‘‘Improving company recognition from\nunstructured text by using dictionaries,’’ pp. 610–619,\n24 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n2017.\n[105] M. Schiersch, V . Mironova, M. Schmitt, P. E. Thomas,\nA. Gabryszak, and L. Hennig, ‘‘A german corpus for\nfine-grained named entity recognition and relation ex-\ntraction of traffic and industry events,’’ ArXiv, vol.\nabs/2004.03283, 2018.\n[106] T. Osunsanmi, C. Aigbavboa, W. Thwala, and R. Mo-\nlusiwa, ‘‘Modelling construction 4.0 as a vaccine for\nensuring construction supply chain resilience amid\ncovid-19 pandemic,’’ Journal of Engineering, Design\nand Technology, 2021.\n[107] M. Gani, T. Yoshi, and M. S. Rahman, ‘‘Optimiz-\ning firm’s supply chain resilience in data-driven busi-\nness environment,’’ Journal of Global Operations and\nStrategic Sourcing, 2022.\n[108] M. Alvarenga, M. P. V . de Oliveira, and T. Oliveira,\n‘‘The impact of using digital technologies on supply\nchain resilience and robustness: the role of memory\nunder the covid-19 outbreak,’’ Supply Chain Manage-\nment: An International Journal , 2023.\n[109] Z.-P. Li, H. Ceong, and S.-J. Lee, ‘‘The effect of\nblockchain operation capabilities on competitive per-\nformance in supply chain management,’’ Sustainabil-\nity, 2021.\n[110] R. Yan, X. Jiang, and D. Dang, ‘‘Named entity recog-\nnition by using xlnet-bilstm-crf,’’ Neural Processing\nLetters, vol. 53, pp. 3339 – 3356, 2021.\n[111] S. Feng, S. Min, and G. Lei, ‘‘Named entity recognition\nmodel of chinese clinical electronic medical record\nbased on xlnet-bilstm,’’ 2021.\n[112] D. Yang, F. Wan, and Y . Zhang, ‘‘Named entity recog-\nnition in xlnet cyberspace security domain based on\ndictionary embedding,’’ 2022 4th International Con-\nference on Advances in Computer Technology, Infor-\nmation Science and Communications (CTISC) , pp. 1–\n5, 2022.\n[113] X.-D. Doan, ‘‘Vtcc-nlp at nl4opt competition\nsubtask 1: An ensemble pre-trained language\nmodels for named entity recognition,’’ ArXiv, vol.\nabs/2212.07219, 2022.\nMILAD BAGHALZADEH SHISHEHGARKHANEH\nis currently pursuing his PhD in Civil Engineering\nat Monash University, Melbourne, Australia. Con-\ncurrently, he holds a lecturer position at Victoria\nUniversity, Melbourne, Australia. His research\nis primarily focused on leveraging advanced Ar-\ntificial Intelligence (AI) methodologies, notably\nTransformer architectures, to address challenges\nin construction supply chain risk management.\nAdditionally, Milad is exploring the integration\nof AI with Blockchain technology to enhance resilience in construction\nprojects. With over 25 published works, including journal articles and book\nchapters, Milad has made significant contributions to the field. His diverse re-\nsearch interests encompass construction supply chain management (CSCM),\nmachine learning, natural language processing (NLP), blockchain technol-\nogy, and Building Information Modeling (BIM). Through his academic\nendeavors, Milad aims to drive innovation in construction management\npractices, ensuring projects are executed efficiently and resiliently amidst\nevolving industry challenges.\nDR. ROBERT C. MOEHLER is a distinguished\nscholar specializing in Engineering Project Man-\nagement. He currently serves as a Senior Lec-\nturer of Engineering Project Management in the\nDepartment of Infrastructure Engineering at The\nUniversity of Melbourne’s Faculty of Engineering\nand Information Technology1. Before this appoint-\nment, he held a position as a Lecturer of Project\nManagement (Civil Engineering) at Monash Uni-\nversity, Melbourne, Australia. Robert’s work sig-\nnificantly revolves around balancing various resource elements in building\nprojects, addressing major concerns such as time, cost, quality, risk, among\nothers. He has made notable contributions to the field of Project Knowledge\nManagement, particularly within the automotive sector. With a rich array of\npublications, including books, articles, and conference papers, Robert has\na substantial impact on the discourse surrounding Project Management in\nEngineering contexts. His work is catalogued extensively in the Structure\nliterature database, reflecting a prolific academic career that continues to\ncontribute to advancing knowledge and practices in his field.\nDR. YIHAI FANGis a distinguished scholar spe-\ncializing in Engineering Project Management. He\ncurrently serves as a Senior Lecturer of Engi-\nneering Project Management in the Department\nof Infrastructure Engineering at The University\nof Melbourne’s Faculty of Engineering and Infor-\nmation Technology1. Before this appointment, he\nheld a position as a Lecturer of Project Manage-\nment (Civil Engineering) at Monash University,\nMelbourne, Australia. Robert’s work significantly\nrevolves around balancing various resource elements in building projects,\naddressing major concerns such as time, cost, quality, risk, among others.\nHe has made notable contributions to the field of Project Knowledge Man-\nagement, particularly within the automotive sector. With a rich array of\npublications, including books, articles, and conference papers, Robert has\na substantial impact on the discourse surrounding Project Management in\nEngineering contexts. His work is catalogued extensively in the Structure\nliterature database, reflecting a prolific academic career that continues to\ncontribute to advancing knowledge and practices in his field.\nVOLUME 11, 2023 25\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nDR. AMER A. HIJAZI has a diverse academic\nand professional background primarily centered\naround construction management and its intersec-\ntion with information technology. He is currently\nserving as an Assistant Professor at Al-Ahliyya\nAmman University in Jordan, contributing to the\nacademia and research in the region. Previously,\nhe embarked on a Ph.D. journey at Western Syd-\nney University, where he was affiliated with the\nCentre for Smart Modern Construction (c4SMC).\nHis Ph.D. research was focused on developing a methodology to integrate\nblockchain technology with Building Information Modeling (BIM) for con-\nstruction supply chains, aiming to enhance the efficiency and transparency\nin the construction sector.\nIn addition to his Ph.D. research, Amer has also held positions as a Casual\nAcademic at Western Sydney University, contributing to the Dean’s Unit of\nthe School of Engineering, Design, and Built Environment. His professional\njourney further extended to Monash University, where he worked as an\nAcademic Research Assistant with Building 4.0 CRC. His role involved\ndominating construction management research with a blend of information\ntechnology and computer science. He focused on building structured datasets\nto transition digital platforms from isolated electronic files to a connected\necosystem of databases, illustrating his keen interest in leveraging technology\nto address construction management challenges.\nDR. HAMED ABOUTORAB has recently com-\npleted his Ph.D. candidacy at the University of\nNew South Wales. His research is centered around\nArtificial Intelligence, with a current focus on\nthe development of Reinforcement Learning-based\nrecommendation systems. Notably, his research\nwork has been published in esteemed international\njournals, including Expert Systems with Applica-\ntions, Journal of Network and Computer Applica-\ntions, and Future Generation Computer Systems.\n26 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3377232\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}