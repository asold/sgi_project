{
  "title": "Long-Short Term Cross-Transformer in Compressed Domain for Few-Shot Video Classification",
  "url": "https://openalex.org/W4285604377",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101181241",
      "name": "Wenyang Luo",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5068621276",
      "name": "Yufan Liu",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5109346446",
      "name": "Bing Li",
      "affiliations": [
        null,
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5114549594",
      "name": "Weiming Hu",
      "affiliations": [
        "Center for Excellence in Brain Science and Intelligence Technology",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5103270427",
      "name": "Yanan Miao",
      "affiliations": [
        "Chinese Academy of Sciences",
        "National Computer Network Emergency Response Technical Team/Coordination Center of Chinar"
      ]
    },
    {
      "id": "https://openalex.org/A5067941891",
      "name": "Yangxi Li",
      "affiliations": [
        "National Computer Network Emergency Response Technical Team/Coordination Center of Chinar"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2974470310",
    "https://openalex.org/W4300860215",
    "https://openalex.org/W2894873912",
    "https://openalex.org/W2963901365",
    "https://openalex.org/W3035374961",
    "https://openalex.org/W2963341924",
    "https://openalex.org/W2625366777",
    "https://openalex.org/W2963370182",
    "https://openalex.org/W2964105864",
    "https://openalex.org/W2507009361",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4234117503",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3093455342",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W3021778166",
    "https://openalex.org/W3209038819",
    "https://openalex.org/W3173271747",
    "https://openalex.org/W4237044863",
    "https://openalex.org/W3092651603",
    "https://openalex.org/W24089286",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3095374178",
    "https://openalex.org/W2604763608"
  ],
  "abstract": "Compared with image few-shot learning, most of the existing few-shot video classification methods perform worse on feature matching, because they fail to sufficiently exploit the temporal information and relation. Specifically, frames are usually evenly sampled, which may miss important frames. On the other hand, the heuristic model simply encodes the equally treated frames in sequence, which results in the lack of both long-term and short-term temporal modeling and interaction. To alleviate these limitations, we take advantage of the compressed domain knowledge and propose a long-short term Cross-Transformer (LSTC) for few-shot video classification. For short terms, the motion vector (MV) contains temporal cues and reflects the importance of each frame. For long terms, a video can be natively divided into a sequence of GOPs (Group Of Picture). Using this compressed domain knowledge helps to obtain a more accurate spatial-temporal feature space. Consequently, we design the long-short term selection module, short-term module, and long-term module to comprise the LSTC. Long-short term selection is performed to select informative compressed domain data. Long/short-term modules are utilized to sufficiently exploit the temporal information so that the query and support can be well-matched by cross-attention. Experimental results show the superiority of our method on various datasets.",
  "full_text": "Long-Short Term Cross-Transformer in Compressed Domain\nfor Few-Shot Video Classification\nWenyang Luo1,2 ∗ , Yufan Liu1,2 ∗ , Bing Li1,4 † , Weiming Hu1,2,3 , Yanan Miao5 and Yangxi Li5\n1National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences\n2School of Artificial Intelligence, University of Chinese Academy of Sciences\n3CAS Center for Excellence in Brain Science and Intelligence Technology\n4PeopleAI, Inc.\n5National Computer Network Emergency Response Technical Team/Coordination Center of China\n{luowenyang2020,yufan.liu}@ia.ac.cn, {bli,wmhu}@nlpr.ia.ac.cn, miaoyn@cert.org.cn,\nliyangxi@outlook.com\nAbstract\nCompared with image few-shot learning, most of\nthe existing few-shot video classification methods\nperform worse on feature matching, because they\nfail to sufficiently exploit the temporal information\nand relation. Specifically, frames are usually evenly\nsampled, which may miss important frames. On\nthe other hand, the heuristic model simply encodes\nthe equally treated frames in sequence, which re-\nsults in the lack of both long-term and short-term\ntemporal modeling and interaction. To alleviate\nthese limitations, we take advantage of the com-\npressed domain knowledge and propose a long-\nshort term Cross-Transformer (LSTC) for few-shot\nvideo classification. For short terms, the motion\nvector (MV) contains temporal cues and reflects\nthe importance of each frame. For long terms, a\nvideo can be natively divided into a sequence of\nGOPs (Group Of Picture). Using this compressed\ndomain knowledge helps to obtain a more accu-\nrate spatial-temporal feature space. Consequently,\nwe design the long-short term selection module,\nshort-term module, and long-term module to com-\nprise the LSTC. Long-short term selection is per-\nformed to select informative compressed domain\ndata. Long/short-term modules are utilized to suffi-\nciently exploit the temporal information so that the\nquery and support can be well-matched by cross-\nattention. Experimental results show the superior-\nity of our method on various datasets.\n1 Introduction\nFew-shot learning (FSL), a fundamental problem in machine\nlearning, aims to learn information about categories from a\nfew training samples. This topic become increasingly pop-\nular because in practice collecting a large amount of la-\nbeled data is often difficult. Recently, FSL has reached\n∗Equal contribution.\n†Corresponding author.\n(a) long term\n(b) short term\nFramesMVsFramesMVs\nGOP1 GOP2 GOP3\nGOP1 GOP2 GOP3\nUniform split GOP split\ninformativeuseless\nFigure 1: Video examples with long-term temporal informa-\ntion (a)-ShavingHead and short-term temporal information (b)-\nPuttingSomethingOntoSomething. The frames with yellow border\nare selected by traditional selection strategy, while the frames with\nred border are selected by our long-short term selection strategy.\na milestone on image classification [Doersch et al., 2020;\nSung et al., 2018]. For video few-shot classification, there\nare still challenges due to the complicated temporal structure.\nFew-shot video classification [Dwivedi et al., 2019; Caoet\nal., 2020; Zhang et al., 2020; Perrett et al., 2021] has been\ntried recently, but most existing methods fail to sufficiently\ncapture the temporal cues. On the one hand, frames are usu-\nally evenly sampled, and each frame is treated as equally im-\nportant. In actuality, the information intensity of video is typ-\nically not uniformly distributed. As a result, some critical\ninformation may be missed. On the other hand, the heuristic\nmodel simply encodes the equally treated frames in sequence,\nwhich results in the lack of both long-term and short-term\ntemporal modeling and interaction. For example, in Figure 1,\nthe evenly sampling strategy often extracts some useless in-\nformation (e.g., the single head in (a) or the background in\n(b)). Besides, some class information reflects in the long term\nwhile others reflect in the short term. Thus, the long-short-\nterm temporal modeling is extremely necessary.\nTo alleviate these problems above, we explore coarse and\nfine-grained temporal information and more accurate tempo-\nral relation, to better match the query and support. On the\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1247\none hand, more effective temporal data is used. We find that\nthe compressed domain natively contains useful information.\nFor example, the motion vectors (MVs) show the short-term\ntemporal and motion information of each frame. The MV in-\ntensity can also indicate the importance of the current frame.\nFor long term, the video is split into different groups of pic-\ntures (GOPs) according to the content, and the frames with\nthe same scene and content are usually included in one GOP.\nBesides the temporal cues, the intra-frame (I-frame) of each\nGOP also provides the appearance information. More impor-\ntantly, the compressed domain data can be acquired at a low\ncost. We only need to entropy-decode the video bitstream to\naccess the compressed domain data, rather than fully decode\nthe RGB frames.\nOn the other hand, a more accurate spatial-temporal feature\nspace and temporal relation are constructed. Taking advan-\ntage of compressed domain knowledge, we propose a Long-\nshort Term Cross-Transformer (LSTC) for few-shot video\nclassification. Firstly, the informative data is adaptively se-\nlected from the multi-modal compressed domain data with\nlong-short term selection. Then for the short-term tempo-\nral module, a multi-modal integration network is designed, in\nwhich the I-frames and MVs sequences from the same GOP\ninteract with each other. Finally, the long-term temporal mod-\nule performs self-attention for long-term modeling and then\ncomputes the cross-attention between tuples of query embed-\ndings and those of support for more sufficient matching. The\nprototype for each support class is generated. The label is\npredicted by the distance between the query and each proto-\ntype.\nOur main contributions are summarized as follows:\n• We proposed a novel framework called long-short term\ncross-transformer (LSTC) for few-shot video classifica-\ntion, which can deeply exploit the long-short term video\ninformation and well match the query-support pair.\n• We take advantage of the compressed domain knowl-\nedge to obtain effective spatial-temporal information at\na low cost and adaptively select the informative data to\nbe processed.\n• Experimental results show that our method is effec-\ntive and outperforms the state-of-the-art (SOTA) on\nboth large-scale and small-scale datasets, including\nSSV2 [Goyal et al., 2017 ], Kinetics [Carreira and\nZisserman, 2017 ], UCF [Soomro et al., 2012 ] and\nHMDB [Kuehne et al., 2011].\n2 Related Work\nFew Shot Video Classification. Few-shot learning (FSL)\naddresses the challenging problem of learning from a few la-\nbeled examples. Videos reside in a higher-dimensional space\nthan images, which increases the difficulty to learn a strong\nclassifier with limited samples. Compound Memory Network\n(CMN) [Zhu and Yang, 2018 ] constructs a two-layer com-\npound memory structure to store video features for match-\ning. Temporal Attentive Relation Network (TARN) [Bishay\net al., 2019] performs segment-wise alignment before match-\ning support and query videos with relation network [Sung\net al., 2018]. EOSVR [Fu et al., 2019] proposes embodied\none-shot video recognition with synthetic data. OTAM [Cao\net al., 2020] aligns videos with differentiable dynamic pro-\ngramming. Temporal Cross-Transformers (TRX) [Perrett et\nal., 2021] adapt Cross-Transformers [Doersch et al., 2020] to\nconstructs query-specific prototypes from tuples of frames.\nThe problem of video embedding is discussed in [Zhu et al.,\n2021]. The previous methods attempt to capture the tempo-\nral structure of videos but achieve limited success. A crit-\nical problem is that their input is RGB frames, where tem-\nporal clues must be inferred indirectly and often implicitly.\nAMeFu-Net[Fu et al., 2020] exploits depth as additional in-\nput. In contrast, our method considers compressed domain\ndata that contains direct temporal information, exploiting\nglobal and local temporal structure with a long-short term\nmodel that better suits the new input modality.\nCompressed Video Classification. Compressed domain\ndata provides simple and fast temporal information, which\ncan improve the performance of video classification meth-\nods with limited overhead. The acquirement of compressed\ndomain information is nearly cost-free compared with con-\nventional methods that fully decode the input videos. More-\nover, MVs contain the movement at the block level so they\ncan serve as coarse motion estimation. CoViAR [Wu et al.,\n2018] pioneers compressed video classification, replacing op-\ntical flow in [Wang et al., 2016] with MVs and residuals.\nDMC-Net [Shou et al., 2019] refines MVs with the super-\nvision of optical flow. Slow-I-Fast-P [Li et al., 2020] estab-\nlishes pseudo optical flow from MVs and residuals for the fast\npath of SlowFast Network[Feichtenhofer et al., 2019]. These\nmethods reveal the potency and efficiency of compressed do-\nmain features for video classification. Nonetheless, they treat\ncompressed domain features as insertion into conventional ar-\nchitecture instead of developing a new befitting compressed\nvideo classification framework.\n3 The Proposed Method\nWe propose a novel Long-Short Term Cross-Transformer\n(LSTC) for few-shot video classification. It takes advan-\ntage of the compressed domain knowledge and performs ac-\ncurate query-support matching with a long-short term struc-\nture. Here, we introduce the proposed method, including the\nframework, formulation, and technical details.\n3.1 Framework and Formulation\nThe overall framework is summarized in Figure 2. We first\nextract the compressed domain data by partially decoding the\nvideo bitstream at a low cost. Specifically, the I-frames and\nthe motion vectors (MVs) from different Groups of Pictures\n(GOPs) are obtained. We denote the frames in thet-th GOP as\n{Gt,l}Lt\nl=0, where Gt,0 is the I-frame. The MVs are denoted\nas Mt,l. The I-frames are essentially RGB images, contain-\ning the appearance information. The MVs denote the motion\nfrom the source positions in the previous frame to the desti-\nnation positions in the current frame, containing the temporal\ninformation. The I-frames and MVs are extracted from the\nvideo stream by entropy decoding.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1248\nMulti-head \nSelf Attention\nWs\nV\nWs\nK\nWs\nQ\nCross attention\nLong-term \nmodule\nWV\nWK\nWQ\nPrototype of\nClass 1\nPrototype\nof\nClass 2\nQuery\n... Spatial-temporal \nfeature space\nLayer norm\nCross attention\nWV\nWK\nWQ\nLayer norm\nCross attention\nWV\nWK\nWQ\nLayer norm Layer norm\nSupport\nClass 1\nSupport\nClass\n 1\nClass 2\nSupportSupport\nClass 2\nSupport\nQuery\nqbqb\nQuery\nqb\nMulti-head \nSelf Attention\nWs\nV\nWs\nK\nWs\nQ\nCross attention\nLong-term \nmodule\nWV\nWK\nWQ\nPrototype of\nClass 1\nPrototype\nof\nClass 2\nQuery\n... Spatial-temporal \nfeature space\nLayer norm\nLayer norm\nSupport\nClass 1\nClass 2\nSupport\nQuery\nqb\nCompressed \ndomain\nBit stream\nLong-short term \nselection\nShort-term \nmodule\nInformative data\nReorder\nMotion \ncompens\nate\nReconstruc\nted\nReorder\nMotion \ncompens\nate\nReconstruc\nted\nI \nfra\nmes\nMVs\n...\nDecoder\nFigure 2: Overall framework of the proposed method, which contains compressed domain information extraction, long-short term selection\nand long-short term modules.\nSecondly, we design a long-short term selection module\nto extract the informative I-frames and MVs from long-term\nGOP-level and short-term frame-level. After that, a short-\nterm module is embedded to exploit the short-term tempo-\nral information and fuse the multiple modals of I-frames and\nMVs at GOP level. Fed with the embeddings from the short-\nterm module, a cross-transformer is constructed to build the\nlong-term temporal information and to explore the relation-\nship between the support and the query.\nIn this paper, we consider the C-way K-shot few-shot video\nclassification problem, in which an episode [Vinyals et al.,\n2016] consists of C classes with K support videos for each\nclass. The target of this problem is to classify the query video\ninto one of the classes c ∈{1, 2, ··· , C}.\n3.2 Long-short Term Selection\nThe input frame selection is an important problem for video\nclassification. Usually frames are randomly sampled either\nevenly or successively with a fixed stride [Wang et al., 2016;\nCao et al., 2020; Perrett et al., 2021]. This selection strat-\negy may miss some keyframes and introduce some irrelevant\nbackground frames. In our method, we utilize the compressed\ndomain to conduct long-short term information selection.\nThere are useful cues to distinguish the informative frames\nin the compressed domain. In the long term, the GOPs with\ndifferent lengths have already divided the contents into sev-\neral parts. In the short term, the MVs reflect the importance\nof the current action. Hence, we define a metric to evaluate\nthe importance of the l-th frame in the t-th GOP:\nI(Mt,l) =\n\n 1\n|Gt,l|\n∑\n(x,y)∈Gt,l\n||Mt,l(x, y)||1\n\n\nα\n, (1)\nwhere |·| is the cardinality of a set, ||·|| 1 is the L1 norm,\n(x, y) is spatial location, and α ≥0. Based on this metric,\nwe obtain the importance score of the t-th entire GOP:\nI(GOPt) = 1\nLt\nLt∑\nl=1\nI(Mt,l), (2)\n+ +\n+ +\nI frame\nMV\nResidual Block\nI frame\nMV1\nInteraction (m=2)\nMV2\n1x1 conv + Sum\nFigure 3: Short-term module (STM) performs short-term modeling\nin a GOP. Left illustrates a modified residual block, which replaces\nthe first block at each stage. Right is the lateral connections in the\nmodified residual block, implemented by grouped conv1 × 1.\nAccording to this importance score, in the long term, we\nextract the informative GOPs with probabilities of {Psel\nt ∝\nI(GOPt)}T\nt=1. Likewise, in the short term, we extract the\ninformative MVs with probabilities of {Psel\nl ∝I(Mt,l)}Lt\nl=1.\nWe sample them with a probability rather than select the fixed\ntop-n frames because the fuzzification in the proposed selec-\ntion scheme can tolerate some noise in these frames. This\nalleviates the influence of some outliers and noise on the per-\nformance. g GOPs are sampled for each video, and the I-\nframe and m additional frames (P-frames) are selected for\neach sampled GOP. We then accumulate the MVs and con-\nduct an alignment between the accumulated MVs and the I-\nframes. The detailed process is described in the Supplemen-\ntary Materials. The selected I-frames and aligned accumu-\nlated MVs are used as input to the Long-short Term Cross-\nTransformer.\n3.3 Long-short Term Cross-Transformer\nShort-Term Module\nWe conduct short-term temporal interaction in each GOP us-\ning the designed short-term module (STM). The embeddings\ngenerated by STM are enhanced with local temporal relation,\nproviding better appearance and motion modeling than fea-\ntures from RGB frames only. Compressed domain natively\ndecouples the original video into appearance part and motion\npart (i.e., I-frames and MVs). Hence, we construct a two-\nbranch network, one for appearance modeling (I-branch, i.e.,\nfI(·)), and the other for motion modeling (MV-branch, i.e.,\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1249\nfMV(·)). Each branch employs a staged convolutional neu-\nral network (CNN) as the backbone and extracts features for\neach frame. Between these two branches, they interact with\neach other at each stage r (i.e., r = 1, 2, ..., R). The interac-\ntion is performed between the I-frame feature maps and the\nMV feature maps from the same GOP.\nIn our experiments, we use ResNet [He et al., 2016] as the\nbackbone. The appearance-motion interaction is performed at\nthe first residual blocks of stages Conv2-Conv5. As depicted\nin Figure 3, the feature maps of each branch are encoded with\nconv1 ×1 and added to the residual path of the other branch.\nOnly a proportion p ∈ [0, 1] of I feature channels interact\nwith MV features so that STM can preserve the original ap-\npearance in the other (1 −p) I-branch channels. All MV fea-\ntures participate in the interaction. The interacting I feature\nchannels are evenly divided intom groups, each of which ex-\nchanges information with the MV features of a P-frame by\nlateral connections. STM is applied independently for each\nGOP. Finally, for each video, the output features of all the\nI-frames at I-branch are stacked as ZI ∈Rg×dI , and the out-\nput features of all the P-frames at MV-branch are stacked as\nZMV ∈Rgm×dMV , where dI and dMV are the output dimen-\nsions of I-branch and MV-branch, respectively.\nLong-Term Module\nAfter obtaining the embeddings, we match the query and\nsupport videos with long-term module (LTM). LTM con-\nsists of two Cross-Transformers, one for the appearance em-\nbeddings from the I-branch and one for the motion embed-\ndings from the MV-branch. For each Cross-Transformer, a\nself-attention layer is first adopted to the embeddings Z ∈\n{ZI, ZMV }of each video. Given query/key/value matrices\nWQ\nS , WK\nS , WV\nS ∈Rd×d, where d ∈{dI, dMV }is the col-\numn dimension of Z, the self-attention is calculated as:\nH = Z + softmax\n(\nZWQ\nS\n(\nZWK\nS\n)T\n√\nd\n)\nZWV\nS , (3)\nPosition encoding [Vaswaniet al., 2017] is applied on both\nZ and H. After self-attention, we construct embeddings for\ntuples of length n from each video’s embeddings H, allow-\ning for fine-grained matching. For the query video, the tuple\nembedding qb is constructed as following:\nqb = [hj1 \u0016 hj2 \u0016, ...,\u0016hjn] ∈Rnd,\ns.t. b = {j1, ..., jn} (4)\nin which \u0016 denotes vector concatenation, {hj1 , ··· , hjn}are\nn different row vectors from H. Similarly, the tuple embed-\ndings of the support videos are obtained in the same way.\nThen for each class c, we stack all possible tuple embeddings\nfrom every shot as row vectors to obtain the class represen-\ntation Sc. Given query matrix WQ ∈Rnd×dk , key matrix\nWK ∈ Rnd×dk and value matrix WV ∈ Rnd×dv , where\ndk, dv are hidden dimensions, the support prototype of class\nc is obtained by cross-attention:\nub,c = softmax\n(\nqbWQ (\nScWK)T\n√dk\n)\nScWV , (5)\nSubsequently, the distance between the query and the c-th\nclass support is computed:\nδb,c = ||qbWV −ub,c||2\n2, (6)\nWe extract all the possible tuples of the query video to\nmatch the support videos. The average distance is calculated:\n¯δc = 1\n|b|\n∑\nb\nδb,c, (7)\nThe predicted result is the class with the smallest distance\nbetween the support and the query.\n3.4 Optimization\nTo train and optimize the proposed Long-short Term Cross-\nTransformer, we minimize the distance between the query\nvideo and the matched support video. In the loss function,\nnegative distances are regraded aslogits to compute the cross-\nentropy loss:\nLoss = −1\nN\nN∑\ni=1\nC∑\nc=1\nhc,ilog\n(\nexp\n(\n−¯δc,i\n)\n∑C\nc′=1 exp\n(\n−¯δc′,i\n)\n)\n. (8)\nwhere hc,i denotes the label of the i-th training sample.\n4 Experiments\n4.1 Settings\nDatasets. Our evaluations are conducted on four datasets,\nincluding Kinetics [Carreira and Zisserman, 2017 ],\nSomething-Something V2 (SSV2) [Goyal et al., 2017 ],\nUCF [Soomro et al., 2012 ] and HMDB [Kuehne et al.,\n2011]. The first few shot video classification dataset is\nconstructed from Kinetics by CMN [Zhu and Yang, 2018 ].\nIn their setup 100 classes are sampled from the total 400\nclasses. Selected classes are then split into train/val/test sets\nof 64/12/24 classes without overlapping, and 100 videos are\nsampled for each class. On SSV2 we evaluate our method\nwith the split proposed by [Cao et al., 2020] which contains\n64/12/24 classes for train/val/test with approximately 1000\nvideos for each class. Following [Zhang et al., 2020], we use\n70/10/21 classes as train/val/test set for UCF and 31/10/10\nfor HMDB, respectively. All reported results are measured\nover 10,000 randomly sampled episodes from testing sets.\nImplementation. MPEG-4 encoded videos are used as in-\nput. We sample g = 4 GOPs from each video and extract\nthe corresponding I-frames and the MVs of m = 2 P-frames.\nWhen the video has no enough GOPs or P-frames, the GOPs\nand P-frames may be sampled several times. The horizontal\nand vertical components of MVs are rescaled respectively by\nthe width and height of the video. For the short-term mod-\nule, we use ResNet-50 [He et al., 2016] as the backbone for\nI-branch and ResNet-18 for MV-branch, both initialized with\nImageNet [Deng et al., 2009 ] pre-trained weights. We set\nα = 0.4 in long-short term selection. For the LSTC, we set\np = 0.5, the latent dimensions are set todk = dv = 1024 and\ndk = dv = 512 for I and MV , respectively. The two Cross-\nTransformers in the long-term module are implemented with\nn = 2 and n = 3, respectively. Their predictions are merged\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1250\nMethod SSV2 HMDB UCF Kinetics\n1-shot 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot\nMatchNet†[Vinyals et al., 2016] - - 56.2 66.9 78.0 87.9 67.8 77.1\nMAML [Finn et al., 2017] - - - - - - 54.2 75.3\nCMN†[Zhu and Yang, 2018] - - 45.6 62.6 71.8 88.7 60.5 78.9\nTARN [Bishay et al., 2019] - - - - - - 64.8 78.5\nARN [Zhang et al., 2020] - - 45.5 60.6 66.3 83.1 63.7 82.4\nOTAM†[Cao et al., 2020] 42.8 52.3 55.0 65.7 80.1 89.8 73.0 85.8\nTRX [Perrett et al., 2021] - 64.6 - 75.6 - 96.1 - 85.9\nOurs 46.7 66.7 60.9 76.8 85.7 96.5 73.4 86.5\nTable 1: Comparison with state-of-the-art models. Table entries are top-1 accuracy (%) on each dataset, with 1-shot and 5-shot setting.\nThe performance margin is larger on SSV2, but lesser on Kinetics and UCF which rely more on appearance and scene inference. Methods\nwith † are evaluated with our implementations when reported results are not available in the literature.\nby averaging. During training, the I-frames and MVs are ran-\ndomly cropped to 224×224. During testing, they are resized\nto height 256 before center cropping. Following [Wu et al.,\n2018], we augment I-frames with random color jittering.\nThe whole network is trained using SGD optimizer [Bot-\ntou, 2010] with the learning rate of 0.025. Cross-entropy loss\nis calculated on a batch of 32 episodes. The training continues\nfor 60,000 episodes on SSV2, and 10,000 episodes on other\ndatasets. We use 4 NVIDIA RTX2060 GPUs for training.\n4.2 Performance Comparison\nTo verify the effectiveness of the proposed method, 7 SO-\nTAs are taken into account for comparison, including Match-\nNet [Vinyals et al., 2016 ], MAML [Finn et al., 2017 ],\nCMN [Zhu and Yang, 2018 ], TARN [Bishay et al., 2019 ],\nOTAM [Cao et al., 2020], ARN [Zhang et al., 2020] and\nTRX [Perrett et al., 2021]. The comparative results are sum-\nmarized in Table 1. The proposed method achieves SOTA\nperformance on all four benchmark datasets. For the 5-shot\nsetting, the proposed method surpasses the previous SOTA\nby 2.1%, 1.2% respectively on SSV2 and HMDB. Note that\nthe gains come with little overhead. Although MVs are in-\ntroduced as an additional stream to I-frames, our elaborate\ndesign of short-term modeling module reduces the convolu-\ntional feature dimension by a factor of m, which greatly de-\ncreases the number of parameters of the Cross-Transformer.\nFurthermore, the acquirement of I-frames and MVs is more\nefficient than traditional fully decoding, because, in the ex-\ntraction of compressed domain data, most of the computation-\nintensive decoding steps can be skipped. Compared with\nprior SOTA methods which typically take 8 RGB frames as\ninput, the proposed method is fed with only 4 I-frames in\na video, which contain much less appearance information\nthan 8 RGB frames. Therefore, the major contribution to\nthe SOTA performance of LSTC is the effective utilization of\ntemporal information contained in MVs and the appearance\ninformation in I-frames.\n4.3 Ablation Study\nIn this section, we conduct extensive experiments to fur-\nther analyze and discuss the effectiveness of the proposed\nmethod. The results of ablation study on constituent parts\nof our method are summarized in Table 2. Baseline model\nconsists of I-frames and MVs streams with traditional selec-\ntion strategy, i.e., GOPs and MVs are uniformly sampled.\nMVs Selection STM LTM HMDB UCF\n✓ 69.2 89.6\n✓ ✓ 69.9 90.4\n✓ ✓ 73.5 94.1\n✓ ✓ ✓ 73.7 94.0\n✓ ✓ ✓ 74.2 94.4\n✓ ✓ ✓ 74.9 95.0\n✓ ✓ ✓ 75.3 95.5\n✓ ✓ ✓ ✓ 76.8 96.5\nTable 2: Ablation study. Selection means long-short term selection,\nSTM means short-term module, LTM means long-term module. Re-\nsults are reported on 5-way 5-shot setting.\nThe baseline model replaces the short-term module with plain\nResNet backbones, without interaction between two streams.\nThe long-term module is removed and the maximal cosine\nsimilarity between frames is used to measure the distance be-\ntween two videos. In addition, we provide analysis on g, m,\nand few-shot settings, and compare compressed domain data\nwith optical flows. See the Supplementary Materials for more\nquantitative and qualitative results.\nThe Effect of Motion Vectors and Frame Selection\nWe argue that densely sampled RGB frames are highly re-\ndundant for video classification. In the traditional setting\n[Perrett et al., 2021; Cao et al., 2020] 8 frames are drawn\nfrom the original video to construct a video representation.\nIn our method, merely 4 I-frames containing RGB informa-\ntion are selected, and MVs provide additional motion infor-\nmation for accurate classification. As demonstrated in Ta-\nble 2, utilizing I-frames and MVs already yields a few-shot\nvideo classifier with considerable accuracy. The performance\nis future improved with the introduction of long-shot term se-\nlection, which utilizes the imbalanced distribution of infor-\nmative frames and the correlation between MVs and the sig-\nnificance of motion. Moreover, after removing the selection,\na 1.5% drop and a 1.0% drop are observed on HMDB and\nUCF, respectively. This proves the effectiveness of the pro-\nposed long-short term selection procedure.\nAlthough a few-shot classifier based on MVs and long-\nshort term selection is a strong baseline, the performance of\nsuch a naive combination is far from satisfactory. The naive\napproach fails to operate on I-frames and MVs jointly and de-\nrive enough information. To achieve SOTA performance, we\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1251\ng = 1 g = 2 g = 4 g = 8\nm = 1 65.4 71.1 74.2 75.8\nm = 2 68.0 72.9 76.8 77.6\nm = 4 68.4 73.3 77.1 78.1\nm = 8 68.2 73.9 77.5 78.4\nTable 3: The effect of number of sampled GOPs and P-frames\nper GOP. g is the number of sampled GOP, m is the number of\nadditionally sampled frames (P-frames) per sampled GOP. Results\nare accuracy(%) on HMDB with 5-way 5-shot setting.\nmust design an elaborate method to fuse them effectively.\nThe Effect of STM and LTM\nIn our method, we propose short-term module (STM) and\nlong-term module (LTM) to facilitate the fusion of appear-\nance encoded by I-frames and motion encoded by MVs. In\nTable 2, results demonstrate the considerable improvement\nwith the introduction of STM and LTM. With STM, the ac-\ncuracy improves by 4.3% on HMDB and 4.0% on UCF com-\npared with the baseline model with MV and long-short term\nselection only. LTM brings an improvement of 5.0% and\n4.6%, respectively. These results prove that our STM and\nLTM design can bridge the gap between two modalities of\nappearance and motion, generating distinctive features from\nthese two streams. Moreover, when MVs in the full method\nare replaced by all-zero input, i.e., no MVs are provided,\nthe performance dropped significantly by 2.9% and 2.5% on\nHMDB and UCF, respectively. The ability of STM and LTM\nto dig and utilize distinctive video features is considerably\nimpaired without MVs.\nVarying the Number of Sampled GOPs and P-frames\nThe numbers of sampled GOPs (g ) and P-frames per GOP\n(m) are both hyperparameters in our method. We choose\ng = 4 and m = 2 in previous experiments. This setting\nallows us to make a fair comparison with previous methods\nwhich typically take 8 RGB frames as input. To demonstrate\nthe scalability of our method, we summarize the performance\non HMDB with different g and m in Table 3. All other set-\ntings are the same as section 4.1. Accuracy is generally im-\nproved with more GOPs and P-frames per GOP, but the com-\nputational cost also increases with more I-frames and MVs to\nprocess. Since the numbers of GOPs and P-frames in each\nGOP have upper bounds for videos in a certain dataset, sam-\npling more GOPs or more P-frames per GOP than those ex-\nisting in the video is pointless. A practical choice for g and\nm depends on the average length of videos, encoding, and\ncomputation ability.\nPerformance with Different Few-Shot Settings\nTable 4 shows the impact of the few-shot setting, namely the\nnumber of ways (C) and shots (K), on performance. In gen-\neral, our method performs better with more shots, i.e. more\n”hints”, and fewer ways, i.e. fewer ”possibilities”. The same\nphenomenon is observed in previous works [Zhu and Yang,\n2018; Bishay et al., 2019; Perrett et al., 2021] and is likely\nubiquitous in few shot video classification. Nevertheless,\nsuch a problem may not be acute in the real world. In prac-\ntice, usually more than a few samples could be retrieved for\n2-way 3-way 4-way 5-way\n1-shot 78.4 68.5 62.1 60.9\n2-shot 83.4 77.0 69.7 66.8\n3-shot 88.0 82.1 75.0 71.2\n4-shot 89.2 83.1 78.3 73.7\n5-shot 90.9 84.5 82.2 76.8\nTable 4: The impact of few shot setting. Rows vary in the number\nof ways, columns vary in the number of shots. Results are accu-\nracy(%) on HMDB.\nMethod Data Time Infer. Time Accuracy\nOptical Flow 25.0 14.6 74.2\nMVs 0.7 14.6 76.8\nTable 5: Comparison with optical flow. Data Time is pre-\nprocessing time and Infer. Time is inference time, both measured\nin ms/frame. Accuracy(%) is reported on HMDB with 5-way 5-shot\nsetting.\na well-defined category, albeit still considered ”few-shot” as\nopposed to the traditional many-shot scenario which requires\nthousands of training samples.\nComparison with Optical Flow\nIn Table 5 we replace MVs in our method with optical flows\nand compare the performance. The optical flows are extracted\nand processed following [Wang et al., 2016]. All other parts\nof our method remain the same. Data time, inference time,\nand accuracy on HMDB are reported. As demonstrated by\nthe results, extracting compressed domain data is by orders\nof magnitude faster than calculating optical flows, saving a\nsignificant amount of time and computing resources. On the\nother hand, the performance of the optical flow-based method\nis worse than the MV-based method. This may be caused by\nthe fact that the optical flows lack the required GOP struc-\nture, which indicates that the compressed domain data is not\na simple replacement of optical flows in our method.\n5 Conclusion\nThis paper proposes a novel framework called Long-short\nTerm Cross-Transformer (LSTC) for few-shot video classi-\nfication. It takes advantage of compressed domain data to\nmatch the query and the support videos. In particular, LSTC\nconsists of long-short term selection, short-term module, and\nlong-term module. The long-short term selection adaptively\nselects and extracts the informative data by analyzing the im-\nplicit cues in the compressed domain. The short-term module\nintegrates the multi-modal compressed domain data (i.e., I-\nframes and MVs) and makes them interact with each other,\nto obtain fine-grained spatial-temporal features. Given these\nshort-term embeddings, the long-term module computes the\nglobal temporal representations and cross-attention between\nthe query and support. Experiments show the effectiveness of\nthe proposed method on various datasets.\nAcknowledgments\nThis work was supported by the National Key Re-\nsearch and Development Program of China (Grant No.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1252\n2020AAA0106800), the Natural Science Foundation of\nChina (Grant No.61902401, No. 62192785, No. 61972071,\nNo. U1936204, No. 62122086, No. 62036011, No.\n62192782, No. 61721004 and No. 61906052), the Beijing\nNatural Science Foundation No. M22005, the CAS Key Re-\nsearch Program of Frontier Sciences (Grant No. QYZDJ-\nSSW-JSC040). The work of Bing Li was also supported by\nthe Youth Innovation Promotion Association, CAS.\nReferences\n[Bishay et al., 2019] Mina Bishay, Georgios Zoumpourlis,\nand Ioannis Patras. TARN: Temporal Attentive Relation\nNetwork for Few-Shot and Zero-Shot Action Recognition.\nIn BMVC, 2019.\n[Bottou, 2010] L´eon Bottou. Large-Scale Machine Learning\nwith Stochastic Gradient Descent. In COMPSTAT, 2010.\n[Cao et al., 2020] Kaidi Cao, Jingwei Ji, Zhangjie Cao,\nChien-Yi Chang, and Juan Carlos Niebles. Few-Shot\nVideo Classification via Temporal Alignment. In CVPR,\n2020.\n[Carreira and Zisserman, 2017] Jo˜ao Carreira and Andrew\nZisserman. Quo Vadis, Action Recognition? A New\nModel and the Kinetics Dataset. In CVPR, 2017.\n[Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher, Li-\nJia Li, Kai Li, and Fei-Fei Li. ImageNet: A large-scale\nhierarchical image database. In CVPR, 2009.\n[Doersch et al., 2020] Carl Doersch, Ankush Gupta, and An-\ndrew Zisserman. CrossTransformers: Spatially-Aware\nFew-Shot Transfer. In NeurIPS, 2020.\n[Dwivedi et al., 2019] Sai Kumar Dwivedi, Vikram Gupta,\nRahul Mitra, Shuaib Ahmed, and Arjun Jain. ProtoGAN:\nTowards Few Shot Learning for Action Recognition. In\nICCV Workshop, 2019.\n[Feichtenhofer et al., 2019] Christoph Feichtenhofer, Haoqi\nFan, Jitendra Malik, and Kaiming He. SlowFast Networks\nfor Video Recognition. In ICCV, 2019.\n[Finn et al., 2017] Chelsea Finn, Pieter Abbeel, and Sergey\nLevine. Model-Agnostic Meta-Learning for Fast Adapta-\ntion of Deep Networks. In ICML, 2017.\n[Fu et al., 2019] Yuqian Fu, Chengrong Wang, Yanwei Fu,\nYu-Xiong Wang, Cong Bai, Xiangyang Xue, and Yu-Gang\nJiang. Embodied one-shot video recognition: Learning\nfrom actions of a virtual embodied agent. In ACM Multi-\nmedia, 2019.\n[Fu et al., 2020] Yuqian Fu, Li Zhang, Junke Wang, Yanwei\nFu, and Yu-Gang Jiang. Depth guided adaptive meta-\nfusion network for few-shot video recognition. In ACM\nMultimedia, 2020.\n[Goyal et al., 2017] Raghav Goyal, Samira Ebrahimi Kahou,\nVincent Michalski, Joanna Materzynska, Susanne West-\nphal, Heuna Kim, Valentin Haenel, Ingo Fr ¨und, Peter\nYianilos, Moritz Mueller-Freitag, Florian Hoppe, Chris-\ntian Thurau, Ingo Bax, and Roland Memisevic. The\n“Something Something” Video Database for Learning and\nEvaluating Visual Common Sense. In ICCV, 2017.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep Residual Learning for Image\nRecognition. In CVPR, 2016.\n[Kuehne et al., 2011] Hilde Kuehne, Hueihan Jhuang,\nEst´ıbaliz Garrote, Tomaso Poggio, and Thomas Serre.\nHMDB: A large video database for human motion\nrecognition. In ICCV, 2011.\n[Li et al., 2020] Jiapeng Li, Ping Wei, Yongchi Zhang, and\nNanning Zheng. A Slow-I-Fast-P Architecture for Com-\npressed Video Action Recognition. In ACM Multimedia,\n2020.\n[Perrett et al., 2021] Toby Perrett, Alessandro Masullo,\nTilo Burghardt, Majid Mirmehdi, and Dima Damen.\nTemporal-Relational CrossTransformers for Few-Shot Ac-\ntion Recognition. In CVPR, 2021.\n[Shou et al., 2019] Zheng Shou, Zhicheng Yan, Yannis\nKalantidis, Laura Sevilla-Lara, Marcus Rohrbach, Xudong\nLin, and Shih-Fu Chang. DMC-Net: Generating Discrim-\ninative Motion Cues for Fast Compressed Video Action\nRecognition. In CVPR, 2019.\n[Soomro et al., 2012] Khurram Soomro, Amir Roshan Za-\nmir, and Mubarak Shah. UCF101: A Dataset of 101 Hu-\nman Actions Classes From Videos in The Wild. ArXiv,\n2012.\n[Sung et al., 2018] Flood Sung, Yongxin Yang, Li Zhang,\nTao Xiang, Philip H. S. Torr, and Timothy M. Hospedales.\nLearning to Compare: Relation Network for Few-Shot\nLearning. In CVPR, 2018.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is All you\nNeed. In NeurIPS, 2017.\n[Vinyals et al., 2016] Oriol Vinyals, Charles Blundell, Tim-\nothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra.\nMatching Networks for One Shot Learning. In NeurIPS,\n2016.\n[Wang et al., 2016] Limin Wang, Yuanjun Xiong, Zhe\nWang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van\nGool. Temporal Segment Networks: Towards Good Prac-\ntices for Deep Action Recognition. In ECCV, 2016.\n[Wu et al., 2018] Chao-Yuan Wu, Manzil Zaheer, Hexiang\nHu, R. Manmatha, Alex Smola, and Philipp Kr ¨ahenb¨uhl.\nCompressed Video Action Recognition. In CVPR, 2018.\n[Zhang et al., 2020] Hongguang Zhang, Li Zhang, Xiaojuan\nQi, Hongdong Li, Philip H. S. Torr, and Piotr Koniusz.\nFew-Shot Action Recognition with Permutation-Invariant\nAttention. In ECCV, 2020.\n[Zhu and Yang, 2018] Linchao Zhu and Yi Yang. Compound\nMemory Networks for Few-Shot Video Classification. In\nECCV, 2018.\n[Zhu et al., 2021] Zhenxi Zhu, Limin Wang, Sheng Guo, and\nGangshan Wu. A Closer Look at Few-Shot Video Classifi-\ncation: A New Baseline and Benchmark. In BMVC, 2021.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1253",
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210097554",
      "name": "Center for Excellence in Brain Science and Intelligence Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210087772",
      "name": "National Computer Network Emergency Response Technical Team/Coordination Center of Chinar",
      "country": "CN"
    }
  ],
  "cited_by": 15
}