{
    "title": "Artificial intelligence language models and the false fantasy of participatory language policies",
    "url": "https://openalex.org/W4214914586",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2116069421",
            "name": "Mandy Lau",
            "affiliations": [
                "York University"
            ]
        },
        {
            "id": "https://openalex.org/A2116069421",
            "name": "Mandy Lau",
            "affiliations": [
                "York University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2980207112",
        "https://openalex.org/W3125815277",
        "https://openalex.org/W3133702157",
        "https://openalex.org/W2483215953",
        "https://openalex.org/W2792307011",
        "https://openalex.org/W2788481061",
        "https://openalex.org/W6812824045",
        "https://openalex.org/W3092684220",
        "https://openalex.org/W3024077542",
        "https://openalex.org/W1148686380",
        "https://openalex.org/W2964222680",
        "https://openalex.org/W1964213721",
        "https://openalex.org/W3135008008",
        "https://openalex.org/W3086249591",
        "https://openalex.org/W2075748225",
        "https://openalex.org/W3099183794",
        "https://openalex.org/W2898970033",
        "https://openalex.org/W3153636560",
        "https://openalex.org/W3087714840",
        "https://openalex.org/W1979542095",
        "https://openalex.org/W3094229442",
        "https://openalex.org/W7071731148",
        "https://openalex.org/W3104758113"
    ],
    "abstract": "Artificial intelligence neural language models learn from a corpus of online language data, often drawn directly from user-generated content through crowdsourcing or the gift economy, bypassing traditional keepers of language policy and planning (such as governments and institutions). Here lies the dream that the languages of the digital world can bend towards individual needs and wants, and not the traditional way around. Through the participatory language work of users, linguistic diversity, accessibility, personalization, and inclusion can be increased. However, the promise of a more participatory, just, and emancipatory language policy as a result of neural language models is a false fantasy. I argue that neural language models represent a covert and oppressive form of language policy that benefits the privileged and harms the marginalized. Here, I examine the ideology underpinning neural language models and investigate the harms that result from these emerging subversive regulatory bodies.",
    "full_text": "Working papers in Applied Linguistics and Linguistics at York 1, 4-15 \n \n© The Authors, 2021. Published by York University Libraries ISSN 2564-2855  4 \nArtificial intelligence language models and the false fantasy of \nparticipatory language policies \nMandy Lau1 \nYork University, Toronto, Canada \nAbstract: Artificial intelligen ce neural language models learn from a \ncorpus of online language data, often drawn directly from user-generated \ncontent through crowdsourcing or the gift economy, bypassing traditional \nkeepers of language policy and planning  (such as governments and \ninstitutions). Here lies the dream that the languages of the digital world \ncan bend towards individual needs and wants, and not the traditional way \naround. Through the participatory language work of users, linguistic \ndiversity, accessibility, personalization, and inclusion can be increased. \nHowever, the promise of a more participatory, just, and emancipatory \nlanguage policy as a result of neural language models is a false fantasy. I \nargue that neural language models represent a covert and oppressive form \nof language policy that benefits the privileged and harms the marginalized. \nHere, I examine the ideology underpinning neural language models and \ninvestigate the harms that result from  these emerging subversive \nregulatory bodies.  \nKeywords: language policy; artificia l intelligence; neural language \nmodels \n1 Introduction \nArtificial intelligence (AI) language models predict the probability of sequences of words \nand sentences, forming the basis of Natural Language Processing (NLP), a branch of computer \nscience. These models are applied to many common NLP interpretation and generation tasks (i.e., \nword prediction for autofill, speech recognition in digital assistants, machine translation, or text \nsummarization). Recently, language models increasingly depend on the use of neural networks in \ntheir machine learning. Also known as deep learning, neural language models enable computers \nto autonomously seek out patterns in a given language dataset. This contrasts with older classical \nmethods, where humans provide computers the lingu istic rules for statistical machine learning. \nNeural networks far outperform classical methods in predictive accuracy, processing vast amounts \nof information quickly and efficiently while seeking new patterns that humans could not possibly \nhave predicted. (For more on ‘what is NLP’, see Crash course computer science, 2017; Brownlee, \n2019.) It is possible that this form of black -box machine learning could mitigate against \nprogrammer bias and manipulation, to the point that programmers themselves could not ex plain \nhow or why computers have learned certain patterns. The learning is wholly dependent on the \nstatistical patterns within the training dataset. \n \n \n1 Corresponding author: laumandy@yorku.ca \nAI & THE FALSE FANTASY OF PARTICIPATORY LANGUAGE POLICIES \nWorking papers in Applied Linguistics and Linguistics at York 1 (2021) 5 \nNeural language models that learn from a corpus of user -generated online content bypass \ntraditional keepers  of language policy and planning  (such as top -down governments and \ninstitutions). The languages offered in digital spaces can be driven by user demand and actual \nusage, increasing the capacity for language personalization. Here lies the dream that the language \nof the digital world can bend towards individual needs and wants. Through the participatory \nlanguage work of users, linguistic diversity, accessibility, and inclusion can be increased. However, \nthe promise of a more participatory, just, and emancipatory language policy resulting from neural \nlanguage models is a false fantasy. In this paper, I argue that neural language models represent a \ncovert and oppressive form of language policy that benefits the privileged and harms the \nmarginalized. I begin by su mmarizing how languages are organized online using Kelly -Holmes’ \n(2019) framework. I will then examine the ideology underpinning English neural language models \nand investigate the negative consequences of this emerging subversive regulatory body. I conclude \nby drawing connections to the neoliberal context in which neural language models reside.  \n2 Online language organization \nThe ways in which languages are organized online are categorized by Kelly-Holmes (2019) \ninto four eras:  \n1. Monolingualism was when English dominated the world wide web at the beginning period \nof the internet. \n2. Multilingualism, described as a “partial and parallel multilingualism” (Kelly-Holmes, 2019, \np. 28), was when the “big” global languages were resourced and available for user selection. \nThis was made possible by the stabilization of non -ASCII-supported alphabets such as \nDevangari and Chinese characters . The typical framing of one language/per user/by \nterritory within a shared web experience resembles multiple monolingualisms.  \n3. Hyperlingualism, which  emerged at the time of Web 2.0, is characterized by dynamic \ninteractivity, collaboration, and crowdsourcing. Both technological and ideological \nchanges contributed to an unlimited number of languages in expanding digital spaces. \n4. Idiolingualism is an “intensified but isolated hyperlingualism” (Kelly-Holmes, 2019, p. 33), \nmarked by increased personalization and linguistic customization.  \nThe development of neural language models coincided with the simultaneous eras of \nhyperlingualism and idiolingualism. The technological and ideological setting of these eras \nenabled the work of language policy and planning to bypass the boundaries of geography, language \nstandardization, official state policy, and language professional competence (Kelly-Holmes, 2019). \nIn the hyperlingualism era, users often provided labour for free through models of crowdsourcing \nand the  so called  “gift economy”  in which labour and data are given without any formal \narrangements for reciprocation. As a result,  many prev iously overlooked and undervalued \nlanguages are now online, particularly benefitting low -resource languages and oral language \nrevitalization efforts. Facebook Translation is one example of how users volunteered to translate \nwords and up-vote translations, with a final moderation by Facebook (Kelly-Holmes, 2019). This \nprocess contains the appearance of democratization as it did not include language standardization \nwork by language professionals, nor did it require users to prove any legitimacy  as language \ntranslators. It was completely community usage based.  \nIdiolingualism builds on the developments of hyperlingualism, adding algorithmic \ncustomization to create a personalized language filter bubble (Kelly -Holmes, 2019). Examples \nMANDY LAU \nWorking papers in Applied Linguistics and Linguistics at York 1 (2021) 6 \ninclude predictive text based on your past language behaviours, or tailored translation according \nto your past language use, time, and geography through apps and mobile devices such as Google \nTranslate or the Translate One2One wearable (Kelly -Holmes, 2019). Common use of these apps \nand devices generates new data that can once again be fed back into neural language models for \nmachine learning refinements without language interventions or regulations. \n3 Machine learning datasets: The case of English online \nParticipatory language knowledge generated by actual users  and leveraged to personalize \nonline language experiences appears to be more linguistically inclusive and socially just. However, \nsuch language data sets are not neutral  when used for machine learning. Hidden within neural \nlanguage modeling are powerful sociopolitical ideologies. As Kelly -Holmes (2019) summarizes, \n“the web is a sociolinguistic machine —fueled by online language practices and choice s and by \nwidespread and common-sense ideologies and beliefs about language” (p. 25).   \nLarge datasets used by the broader general public should be representative of diverse \nworldviews. However, the language practices, beliefs, and ideologies picked up through machine \nlearning are unevenly distributed. For example, the dataset Common Crawl2 derives its language \ndata from the internet over the last eight years. While vast, the internet tends to overrepresent \nyoung users from the global north (Bender & Gebru et al., 2021; Pew, 2021). Datasets of American \nand British English tend to underrepresent the language practices of people of marginalized \nidentities, such as speakers of African American Vernacular English (Martin, 2021), and \noverrepresent the views of white supremacy, misogyny, homophobia, ableism , ageism, etc. \n(Bender and Gebru et al., 2021). This is because the crawling method tends to derive language \ncorpora from user-generated content sites with the most incoming and outgoing links, such as \nReddit, Twitter, or Wikipedia, overlooking non -dominant views from less mainstream sites \n(Bender and Gebru et al., 2021). Reddit, Twitter, and Wikipedia are not as open and accessible as \npresented; these sites enable easy suppression of voices via false flagging for moderation, as well \nas systemic harassment, trolling, and violence upon marginalized communities, restricting and \npushing their voices out (Bender and Gebru et al., 2021; Monteiro, 201 9; Wachter -Boettcher, \n2017). This restriction privileges and further amplifies the voices and worldviews of the dominant \nidentities who do not experience online violence.  \nEven machine learning that does not derive its data from user -generated content reflects \nsocietal bias. The open-source Google algorithm Word2vec combs through Google News articles \nto learn relationships between words, creating word associations or word em beddings. The \nassumption is that Google News is neutral, without consideration of how the content may mirror \nhistorical, societal injustices in content and language, or how market forces may drive media \nreporting coverage. Word2vec w ill then pick up these biases from the dataset, and consequently \nreturn results that perpetuate and reinforce the same biases, such as sexist associations between \nwords (Bolukbasi et al., 2016; Wachter-Boettcher, 2017).  \nCrowdsourcing language work could enable more human agency but is not without bias. \nFor example, the labour market on Amazon’s crowdsourcing platform Mechanical Turk  is \nunevenly distributed across countries, inadvertently biasing the sample (Erlewine & Kotek, 2016). \nIn a demographic survey of 1000 Turk workers, 46% reported to be from the United States, 34% \nfrom India, and the remaining 19% were from 64 other countries (Ipeirotis, 2010). These workers \nare also more likely to be female in the US, but male in India, tend to be born in the 1980s, have a \n \n2 http://commoncrawl.org \nAI & THE FALSE FANTASY OF PARTICIPATORY LANGUAGE POLICIES \nWorking papers in Applied Linguistics and Linguistics at York 1 (2021) 7 \nhigher educat ional level but earn a lower income than the general population, and are single \nwithout kids (Ipeirotis, 2010). This results in a highly skewed sample producing large quantities \nof crowdsourced language data. \nGenerally, the version of a language that holds the highest prestige tends to follow the \nlanguage users with the highest prestige (Milroy, 2001). Knowing which version of online \nlanguage carries the most prestige would require knowing which users carry the most weight: the \nlanguage used by young, straight, American, white males —English. This becomes the default \nlanguage when it comes to language participation or content on the internet. As of March 2021, \nover 60% of online content is in English, followed by Russian at 8.3%, and Turkish, Spanish, and \nPersian at just over 3% (W3Techs, 2021). This is the case despite English-language users being \nestimated in 2020 as comprising 25% of all internet users, followed by Chinese (19%), Spanish \n(7%), and Arabic (5%) ( Internet World Stats, 2021). 3 The default cultural perspective then also \nbecomes that of the young, straight, American, white male.  \nEfforts to make language more inclusive  in online environments  focus on filtering out \nhateful and offensive language. Howe ver, filtering methods generally remove all words on a list \nclassified as offensive without considering the meaning or context of the word in use. For example,  \nthe Colossal Clean Crawled Corpus discards pages that contain 400 “dirty, naughty, obscene or \notherwise bad words,” with most of the words related to sex and some related to white supremacy \n(Bender and Gebru et al., 2021). This effectively wipes out pornography but also inevitably filters \nout the discourse of communities who have reclaimed some of t he words on their list. These \ncommunities include LGBTQ communities or the #metoo movement. (Bender and Gebru et al., \n2021). Filtering algorithms that weed out fake news and conspiracy theories work in similar ways, \nhaving the effect of wiping out the disc ourse of human rights activists or political justice by \nequating shared politically charged words with shared values (Nakov & Da San Martino, 2020). \nSimilar to the aforementioned Facebook translation example, the work of verbal hygiene (Cameron, \n1995) in regulating the appropriacy of online language has also bypassed traditional gatekeepers \nand is now relegated to AI technologists at large media corporations, who are also overwhelmingly \nyoung, straight, American, white, Judeo-Christian, and male (Monteiro, 2016). \n4 Neural language models: A de-facto language policy \nLarge language models, particularly open -sourced ones, are used reflexively in common \napplications found on our mobile devices and computers. Any online activity mediated by \nlanguage is an interaction with neural language models and their schema. In other words, language \nmodels act as a mechanism mediating between ideology and practice, leading to de-facto language \npolicies (Shohamy, 2008). Language models enforce their de -facto policies with stealth  and \neffectiveness, governing our decisions and behaviours in subversive ways.  Public or private \ninstitutions that use applications powered by these language models are in effect surrendering \ncontrol, at least partially, of their language policy to AI tech nologists. Their own carefully \nconstructed language policy need not apply to the AI tools they choose to use; instead, by choosing \nto use the tools, they are inadvertently choosing to subscribe to whatever policy is embedded in \ntheir tools’ language models.  \n \n3 In this estimate, only one dominant language is assigned per internet user, when, in reality, many people are \nmultilingual and speak English as a Global English (Internet World Stats, 2021). This type of framing of language is \ncharacteristic of what Kelly-Holmes (2019) refers to as the “multilingualism era” of language organization. \nMANDY LAU \nWorking papers in Applied Linguistics and Linguistics at York 1 (2021) 8 \nThe human impact is serious; the following sections explain how language models can \nregulate human language production to include an AI audience and manipulate humans through \nfiltering and synthetic language production. The consequences range from chang ing human \nlinguistic behaviour to systemic experiences of identity -based microaggressions, discrimination, \nand violence.   \n4.1 Regulating human language behaviour \nInteractions between artificial intelligence and humans create new forms of data that \ninform future human decisions and behaviours. AI sifts this data through their language models, \nusing language forms to deduce the probability of future events and behavi ours. Without actual \nlanguage understanding, AI uses language proxies to arrive at interpretations, which guide human \ndecisions. For decisions that have important consequences, some humans have learned to adapt \nand produce language tailored to language models to achieve the best possible results. An example \nof this is found in corporate disclosures. CEOs have learned to adjust their speech tone and word-\ncombination choices to improve algorithmic scores and minimize triggering red flags, which affect \nthe share-trading decisions of human analysts and traders (Cao et al., 2020; Wigglesworth, 2020).  \nWhile this may seem innocuous, producing language for a proxy -based machine is an \nopportunity cost. Human energy is redirected to comply with algorithms instead of communicating \nin meaningful ways. Further, knowledge of how opaque algorithms work in specific sectors is \nprivileged and requires additional resources to gain an algorithmic edge, such as professional \ncoaching or access to simulation algorithms. This serves to further widen socioeconomic inequities \nand exacerbate historical, social injustices.  \nA case in point is the AI system Hirevue. Over a million job candidates have been screened \nby Hirevue, which looks for proxies such as word choice, voice tone, and g estures to generate an \nemployability score (Harwell, 2019). These scores determine who gets to the interview stage. \nSince the “look for’s ” are opaque, candidates often seek out recruitment coaches and training \nprograms to learn to optimize their scores (Ha rwell, 2019; O’Neil, 2016). Those who are unable \nto seek this type of support due to financial or time constraints are disadvantaged. The more severe \nconsequence is that employment and human rights laws do not apply to algorithm design (O’Neil, \n2016). Further, some candidates cannot manipulate the scoring algorithms simply because of who \nthey are. Hirevue’s application uses up to 500,000 data points in a 30-minute, 6 question interview \nrecording to generate a score (Harwell, 2019). Like many other automated  recruitment software, \nsome of these data points are linked to personality and mental illness assessments (O’Neil, 2016). \nOther performance proxies are less accurately assessed for certain populations who are not the \ndefault, such as those who speak  with a non-American or non -British Standard English accent \n(Harwell, 2019), or Black wom en with dark skin tone s (Buolamwini & Gebru, 2018). These \nproxies enable illegal employment discrimination by language, disability, race, ethnicity, gender, \nage, and more.  \nCorporate disclosures and employee recruitment are among many examples of how \nlanguage models in AI systems enforce their de -facto language policies upon individuals and \ninstitutions. Other examples of AI language models used for high -stakes decisions inclu de \nalgorithmic recidivism risk prediction systems used in Canadian and American criminal court \n(COMPAS; O’Neil, 2016; Robertson, Khoo & Song, 2020), policing software, and higher \neducation admission systems (O’Neil, 2016). These are only the most explicit and visible ways in \nwhich language is directly used as  a proxy for AI systems to justify decisions and regulations. \nMore implicit ways that neural language models control our language activities and, more \nAI & THE FALSE FANTASY OF PARTICIPATORY LANGUAGE POLICIES \nWorking papers in Applied Linguistics and Linguistics at York 1 (2021) 9 \ngenerally, our lives, include filtering algorithms and synthetic language generation, described in \nthe next section. \n4.2 We see what we want to see \nThe appeal of AI personalization is that it could help us navigate through massive amounts \nof information and increase our human agency in choosing the content we want to see in the \nlanguages we want to see it in.  However, this is only an “illusion of increased choice…we are \nbeing steered through the global, multilingual web in a monolingual bubble” (Kelly-Holmes, 2019, \np. 34). Machine learning determines your choic es based on location data and past language \nbehaviours, reducing your exposure to other languages (Kelly -Holmes, 2019). A narrowing of \nlanguages also means a narrowing of worldviews, with filters that recommend the content you may \nbe interested in based on other users whose language practices are similar to yours. As a result, we \nbecome more socially, linguistically, and ideologically isolated within our own echo chamber.  \nThe exception, however, are the users who are farthest away from the default identity  of \nthe young, straight, American, white  male. Instead, these “edge case” users tend to be \nmisrepresented through the white male gaze (Noble, 2018). Between 2009 -2015, Noble (2018) \ndocumented how Google Search, using its language model BERT, misrepresented and stereotyped \nsocial identities, such as presenting hyper-sexualized Black women and girls as the first results \ngenerated in a broad search. Popular belief may be that search results provide the most relevant or \nuseful information, and that racist and sexist results may be a mere reflection of society. However, \nNoble (2018) debunks this myth, noting that racist and sexist search results are the outcome of \nGoogle prioritizing clicks that generate advertising profits, underrepresenting results from \ncompetitors, less profitable smaller advertisers, and personal blogs. In this way, personalization is \nnot a service to users but a service for advertisers, helping them to find the best match in terms of \nconsumers. Therefore, personalization efforts have not actu ally resulted in as much variation as \nthe public may believe (Noble, 2018, p.  55). Search results that misrepresent, stereotype, and \ndehumanize people lead to multiple harms, such as microaggressions, stereotypes, discrimination, \nto physical violence.  \n4.3 Garbage in, garbage out: Machine generated language \nSince synthetic language, or language generated by language models, reflects the biased \nlanguage and worldviews of the default young, straight, American, white male, it can be inaccurate \nat best and violent at worst. Inaccuracies in machine translation could have terrifying consequences \nif the translation service is leveraged as part of an apparatus for state surveillance as with the case \nof the Israeli occupation of Palestine. In the aforementioned crowdsourced Facebook Translation \napplication, inaccurate machine translation led the Israeli police to the wrongful arrest of a \nPalestinian construction worker in the West Bank (Hern, 2017). Facebook inaccurately translated \nthe man’s caption of a photo of himself next to a bulldozer, “يصبحهمyusbihuhum) meaning  “good \nmorning”, as  “hurt them”  in English or “harm them” in Hebrew (Hern, 2017). The trust placed \non the system was so ingrained that the Israeli police did not even verify the translation with \nArabic-speaking officers before making the arrest. \nTrust in synthetic language makes humans particularly vulnerable to ideological \nmanipulation. Language models can be deployed to generate vast amounts of coherent synthetic \ntext quickly, making it an effective and efficient tool to create oppressive misrepresentation, \npropaganda, conspiracy theories, and fake news. This was brought to public attention in the 2016 \nUnited States election when fake news flooded social media via social bots to sway public opinion \nin Donald Trump’s favour (Bovet & Makse, 2019; Noble, 2018; Wachter-Boettcher, 2017). Other \nMANDY LAU \nWorking papers in Applied Linguistics and Linguistics at York 1 (2021) 10 \nexamples include Google’ s computer vision and language models linking Black people, most \nprominently Michelle Obama, to apes or animals (Noble, 2018), or the LA Times ’ use of \nautomated text generation, leading to tweets that tend to be more racist and misrepresentative than \nhuman-generated tweets, as in the case of their tweet misrepresenting police-shooting victim Keith \nLamont Scott as a criminal (Ascher, 2017; Noble, 2018). \nMcGuffie and Newhouse (2020) demonstrated how one of the largest and most powerful \nlanguage models, OpenAI ’s GPT -3, can be weaponized to create synthetic interactional and \ninformational texts easily and efficiently for far -right extremist radicalization and recruitment \nefforts (Bender & Gebru et al., 2021). While less advanced language models would require hours \nof labour and sophisticated technological resources to create ideologically biased texts, an \nadvanced model like GPT-3 has the capacity to produce realistic and consistent fake text when it \nis fed a  few simple inputs, such as a few tweets, paragraphs, f orum threads, or emails, without \ntechnical know-how (McGuffie & Newhouse, 2020). Accordingly, GPT -3 can be prompted to \nperform language tasks such as “producing polemics reminiscent of Christchurch shooter Brenton \nTarrant, reproducing fake forum threads ca sually discussing genocide and promoting Nazism in \nthe style of the defunct Iron March community, answering questions as if it was a heavily \nradicalized QAnon believer, and producing multilingual extremist texts, such as Russian-language \nanti-Semitic conte nt, even when given English prompts” (McGuffie & Newhouse, 2020). \nCurrently, GPT-3 is not open-sourced, but it is still vulnerable to attack and copying. \nDrawing from this, it is easy to imagine how language models can be weaponized to \ngenerate and distribute synthetic hate-speech, racist memes, conspiracy theories, or pseudoscience \non the internet for many more malicious causes. We are now seeing just how dangerous online \npropaganda is during the current global COVID-19 pandemic (Romer & Jamieson, 2020; van der \nLinden, Roozenbeek & Compton, 2020). What the World Health Organization (2021) terms the \n“infodemic” poses a threat to public trust and public health, from the largest toxic alcohol outbreak \nin Iran following fake news suggesting its preventative use  against COVID -19 (Delirrad & \nMohammadi, 2020), to spikes in hate crimes and xenophobic violence. In Vancouver, anti -Asian \nhate crimes increased 717% from 2019 to 2020 (Manojlovic, 2021). In a September 2020 report, \nBritish Columbia was found to have  the most reported anti -Asian racist incidents per capita in \nNorth America, followed by California, New York, and Ontario (Project 1907, 2020). Women are \nimpacted the most, accounting for 60 -70% of all reported incidents (Project 1907, 2020). In the \nUK, Awan and  Khan-Williams (2020) documented how COVID -19 triggered an increase in \nIslamophobic fake news and theories, such as blaming Muslims for spreading the virus by \nattending mosques and not following social distancing rules.  \nThe extent to which language models , synthetic text generation, and the proliferation of \nfake news online are connected to acts of violence is difficult to prove. However, some examples \nshow a correlation between persistent engagement with online conspiracy theories and hate crimes \n(Noble, 2018). In the 2015 mass shooting at Emanuel African Methodist Episcopal Church, White \nsupremacist Dylann Roof shot at twelve Black church members during worship, killing nine \n(Kaadzi Ghansah, 2017). His manifesto indicated that his racist attitudes were st oked by \nengagement with white supremacist online material, including fake news, conspiracy theories, and \ndehumanizing stereotypes, following a Google search of “Black on White crime” (Kaadzi Ghansah, \n2017; Noble, 2018).  \nTogether, these examples demonstrat e how language policies embedded within language \nmodels can and do manipulate human behaviour, subjecting already marginalized communities to \nAI & THE FALSE FANTASY OF PARTICIPATORY LANGUAGE POLICIES \nWorking papers in Applied Linguistics and Linguistics at York 1 (2021) 11 \nfurther injustices and systemic violence. Those who choose to use tools powered by neural \nlanguage models and those who are most negatively impacted by these tools need to consider their \nethical and legal rights and responsibilities carefully.  \n5 The reality of machine learning \nIf language is “neutral,” then perhaps a participatory approach to language models and \npolicy might work in ways that respect all humans. However, as previous examples demonstrate, \nlanguage is ideologically bound. In thinking through verbal hygiene theory (Cameron, 1995), a \nnaturalist might argue that even if language data is ideologically bound, we should “leave it alone”. \nIt suggests that verbal hygiene practices should not be enacted because it is a natural reflection of \nactual language use in a complex society.  Part of the appeal of machine learning is its ability to \nrelearn according to the mo st current language practices that are fed back into the system and \nimprove when given larger datasets. However, this is not as easy as it seems. Bender et al. (2021) \noutlined the harmful environmental footprint of large datasets, as well as the argument t hat \nincreasing the size of the dataset only increases the many inherent problems previously mentioned. \nThey also note that retraining large language models is expensive and could not possibly reflect \nthe most current social movements or movements that are poorly documented or insufficiently \nreported by the media (Bender  et al., 2021). As a result, large language models tend to be more \nstatic and fixed.  \nFurther, it is unclear who is accountable for the consequences of neural language models. \nStakeholders who might be included could be the company that releases the language model, the \nlanguage model engineers, the software company that uses the language model to create its \napplication, the application user (institution or individual), or state legislators. Cu rrently, the \nstakeholders with the most power in online language policy and the most interest in the opaqueness \nof these systems are the companies that create the language models and leverage them for \napplications. These technology companies thrive on deregulation, a freedom of speech logic, and \nprofit-models built on engagement metrics. Thus, their language policy and planning are based on \nshareholder interest, even if that means protecting this highly profitable driver of engagement: the \n“free” speech of white supremacy and misogyny.  \nAt the same time, technology companies must pacify public resistance and control public \npressure for regulation by installing ethics teams, such as the Ethical AI team at Google. However, \nthese teams tend to hold only symboli c power. Those within who dare to challenge company \npractices are seen as threats. For example, t he co-founder of Google’s Ethical AI team, Timnit \nGebru, one of few Black women in leadership, was fired due to a research paper set for release \n(Schiffler, 2021). Titled “On the dangers of stochastic parrots: Can language models be too big?” \n(Bender & Gebru et al., 2021) and referenced in this paper, the authors consider the harm caused \nby various large language models, implicating Google in a third of the large language models under \nstudy: BERT, ALBERT, GShard, and Switch -C. One month following Gebru’s exit , the other \nGoogle Ethical AI co-founder and co -author of the paper, Margaret Mitchell, was placed on \nadministrative leave and subsequently fired after criticizing the company’s behaviour and actions \ntowards Gebru and their opposition towards the paper (Di ckey, 2021). This reveals Google’s \ncontradictory position: in this case, we see how Google actively engaged in verbal hygiene \npractices by suppressing the voices of marginalized employees (who were fulfilling their job duties \nin critiquing the ethics of AI ) to protect its profit-making model while claiming to be invested in \n“AI for social good…[and] to bring the benefits of AI to everyone” (Google, 2021).  \nMANDY LAU \nWorking papers in Applied Linguistics and Linguistics at York 1 (2021) 12 \n6 Conclusion \nNeural language models act as a de-facto language policy (Shohamy, 2008), enabling what \nNoble (2018) terms technological redlining: how algorithms and big data (such as neural language \nmodels) reinforce social injustices. How languages are organized online in the hyperlingualism \nand idiolingualism eras (Kelly -Holmes, 2019) lead s to language mode ls that overrepresent the \nviews of the default user: English -speaking, young, white males (Bender & Gebru et al., 2021; \nWachter-Boettcher, 2017) and increasing isolation in the languages and worldviews we access \nonline. It can be true that crowdsourcing and the gift economy are able to increase linguistic vitality \nand support the development of under-resourced languages . However, bypassing traditional \nsources of language expertise and state structures can lead to mistakes and misuse. It can also have \nthe unintended effect of maintaining the status quo  in a state’s  official language rights (Kelly-\nHolmes, 2019). Personalization in the forms of devices, such as the Translate One2One wearable, \nonly serves as a linguistic accommodation that keeps you in a monolin gual bubble, rather than \nenabling exposure to a more multilingual society. Rather than bending to our individual needs and \nwants, personalization exposes us to commercial interests and regulates our languag e behaviours \n(Bender & Gebru et al., 2021; Noble, 2018). Democratization, diversity, and inclusion via \nparticipatory language work is only a false fantasy; in reality, participatory language work is \noppressive and exploitative, organized to benefit classes of powerful elites financially and \npolitically (N oble, 2018 ). Just the six big tech companies alone (Apple, Microsoft, Amazon, \nAlphabet/Google, Facebook, and Tesla) are worth over $9 trillion, making up a quarter of the S&P \nindex funds and are bigger than the entire European stock market in 2020 (Klebnik ov, 2020; La \nMonica, 2021). Their sheer power makes it more important than ever to include state governance \nin the work of online language policy and planning. A naturalistic, laissez -faire approach “is \nnothing but a policy for making powerful interests an d strong forces even stronger and more \npowerful” (Kristiansen, 2003, p.  69). Therefore, stronger, more transparent language work and \npolicies involving more diverse stakeholders in the form of democratic regulation on neural \nlanguage models are necessary next steps. \nReferences \nAscher, D. (2017). The new yellow journalism: Examining the algorithmic turn in news \norganizations’ social media information practice through the lens of cultural time \norientation. (Proquest ID: Ascher_ucla_0031D_16033) [Doctoral dissertation, University \nof California, Los Angeles]. eScholarship. \nAwan, I., & Khan-Williams, R. (2020). Research briefing report 2020: Coronavirus, fear and how \nIslamophobia spreads on social media 2020 . Anti-Muslim hatred working group.  \nhttps://antimuslimhatredworkinggrouphome.files.wordpress.com/2020/04/research-\nbriefing-report-7-1.pdf \nBender, E., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the dangers of \nstochastic parrots: Can language models be too big? FAcct ’21: Proceedings of the ACM \nConference on Fairness, Accountability, and Transparency, 610-623. \nhttps://doi.org/10.1145/3442188.3445922 \nBolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., & Kalai, A. (2016). Man is to computer \nprogrammer as woman is to homemaker? Debiasing word embeddings. 30th Conference \non Neural Information Processing Systems, 1–9. \nhttps://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf \nAI & THE FALSE FANTASY OF PARTICIPATORY LANGUAGE POLICIES \nWorking papers in Applied Linguistics and Linguistics at York 1 (2021) 13 \nBovet, A., & Makse, H. A. (2019). Influence of fake news in Twitter during the 2016 US \npresidential election. Nature Communications, 10(7), 1–14. \nhttps://doi.org/10.1038/s41467-018-07761-2 \nBrownlee, J. (2019). What Is Natural Language Processing? Machine learning mastery. \nhttps://machinelearningmastery.com/natural-language-processing/ \nBuolamwini, J., & Gebru, T. (2018). Gender shades: Intersectional accuracy disparities in \ncommercial gender classification. Proceedings of Machine Learning Research, 81, 1–15. \nhttp://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf \nCameron, D. (1995). Verbal Hygiene. Routledge. \nCao, S., Jiang, W., Yang, B., Zhang, A. L., & Robinson, J. M. (2020). How to talk when a \nmachine is listening: Corporate disclosure in the age of AI (NBER Working Paper No. \n27950). National Bureau of Economic Research. https://www.nber.org/papers/w27950 \nCrash course computer science. (2017, November 22). Natural language processing: Crash \ncourse computer science #36 [Video]. YouTube. \nhttps://www.youtube.com/watch?v=fOvTtapxa9c \nDelirrad, M., & Mohammadi, A. B. (2020). New methanol poisoning outbreaks in Iran following \nCOVID-19 pandemic. Alcohol and Alcoholism, 55(4), 347–348. \nhttps://doi.org/10.1093/alcalc/agaa036 \nDickey, M. R. (2021, February 19). Google fires top AI ethics researcher Margaret Mitchell. \nTech Crunch. https://techcrunch.com/2021/02/19/google-fires-top-ai-ethics-researcher-\nmargaret-mitchell/ \nErlewine, M. Y., & Kotek, H. (2016). A streamlined approach to online linguistic surveys. \nNatural Language & Linguistic Theory, 34(2), 481–495. https://doi.org/10.1007/S11049-\n015-9305-9 \nGoogle. (n.d.). AI for Social Good. Google AI. https://ai.google/social-good/ \nHarwell, D. (2019, November 6). HireVue’s AI face-scanning algorithm increasingly decides \nwhether you deserve the job. The Washington Post. \nhttps://www.washingtonpost.com/technology/2019/10/22/ai-hiring-face-scanning-\nalgorithm-increasingly-decides-whether-you-deserve-job/ \nHern, A. (2017, October 24). Facebook translates “good morning” into “attack them”, leading to \narrest. The Guardian. https://www.theguardian.com/technology/2017/oct/24/facebook-\npalestine-israel-translates-good-morning-attack-them-arrest \nInternet World Stats. (2021). Internet world users by language: Top 10 languages. Internet \nWorld Stats: Usage and population statistics. \nhttps://www.internetworldstats.com/stats7.htm \nIpeirotis, P. (2010). The new demographics of Mechanical Turk. A Computer Scientist in a \nBusiness School. https://www.behind-the-enemy-lines.com/2010/03/new-demographics-\nof-mechanical-turk.html \nKaadzi Ghansah, R. (2017, August 21). A most American terrorist: The making of Dylann Roof. \nGQ. https://www.gq.com/story/dylann-roof-making-of-an-american-terrorist \nKelly-Holmes, H. (2019). Multilingualism and technology: A review of developments in digital \ncommunication from monolingualism to idiolingualism. Annual Review of Applied \nLinguistics, 39, 24–39. https://doi.org/10.1017/S0267190519000102 \nKlebnikov, S. (2020, August 28). U.S. tech stocks are now worth more than $9 trillion, eclipsing \nthe entire European stock market. Forbes. \nMANDY LAU \nWorking papers in Applied Linguistics and Linguistics at York 1 (2021) 14 \nhttps://www.forbes.com/sites/sergeiklebnikov/2020/08/28/us-tech-stocks-are-now-worth-\nmore-than-9-trillion-eclipsing-the-entire-european-stock-market/ \nKristiansen, T. (2003). Language attitudes and language politics in Denmark. International \nJournal of the Sociology of Language, 159(2003), 57–71. \nhttps://doi.org/10.1515/ijsl.2003.009 \nLa Monica, P. (2021, January 6). Proof Big Tech is way too big: It’s a quarter of your portfolio. \nCNN. https://www.cnn.com/2021/01/06/investing/stocks-sp-500-tech/index.html \nManojlovic, D. (2021). Report to the Vancouver Police Board: Year-end 2020 Year-to-date key \nperformance indicators report. Vancouver Police Department. \nhttps://vancouverpoliceboard.ca/police/policeboard/agenda/2021/0218/5-1-2102P01-\nYear-end-2020-KPI-Report.pdf \nMartin, J. L. (2021). Spoken corpora data, automatic speech recognition, and bias against \nAfrican American language: The case of habitual ‘be.’ FAcct ’21: Proceedings of the \nACM Conference on Fairness, Accountability, and Transparency, 284. \nhttps://doi.org/10.1145/3442188.3445893 \nMcGuffie, K., & Newhouse, A. (2020). The radicalization risks of GPT-3 and advanced neural \nlanguage models. ArXiv. http://arxiv.org/abs/2009.06807 \nMilroy, J. (2001). Language ideologies and the consequences of standardization. Journal of \nSociolinguistics, 5(4), 530–555. \nMonteiro, M. (2019). Ruined by design: How designers destroyed the world, and what we can do \nto fix it. Mule Books. \nNakov, P., & Da San Martino, G. (2020, November 19). Fact-checking, fake news, propaganda, \nand media bias: Truth seeking in the post-truth era [Conference presentation]. EMNLP \n2020 Conference. https://virtual.2020.emnlp.org/tutorial_T2.html \nNoble, S. (2018). Algorithms of oppression. NYU Press.  \nO’Neil, C. (2016). Weapons of math destruction. Crown Books. \nPew Research Center. (2021). Internet/broadband fact sheet. Pew research center. \nhttps://www.pewresearch.org/internet/fact-sheet/internet-broadband/ \nProject 1907. (2020). Racism incident reporting centre: A community-based reporting tool to \ntrack incidents of racism. Project 1907. https://www.project1907.org/reportingcentre \nRobertson, K., Khoo, C., & Song, Y. (2020). To surveil and predict: A human rights analysis of \nalgorithmic policing in Canada. Citizen Lab and the International Human Rights \nProgram. https://citizenlab.ca/wp-content/uploads/2020/09/To-Surveil-and-Predict.pdf \nRomer, D., & Jamieson, K. H. (2020). Conspiracy theories as barriers to controlling the spread of \nCOVID-19 in the U.S. Social Science and Medicine, 263. \nhttps://doi.org/10.1016/j.socscimed.2020.113356 \nSchiffler, Z. (2021, March 5). Timnit Gebru was fired from Google - then the harassers arrived. \nThe Verge. https://www.theverge.com/22309962/timnit-gebru-google-harassment-\ncampaign-jeff-dean \nShohamy, E. (2008). Language policy and language assessment: The relationship. Current Issues \nin Language Planning, 9(3), 363–373. https://doi.org/10.1080/14664200802139604 \nvan der Linden, S., Roozenbeek, J., & Compton, J. (2020). Inoculating against fake news about \nCOVID-19. Frontiers in Psychology, 11. https://doi.org/10.3389/fpsyg.2020.566790 \nW3Techs. (2021). Historical trends in the usage statistics of content languages for websites. \nWeb technology surveys. \nhttps://w3techs.com/technologies/history_overview/content_language \nAI & THE FALSE FANTASY OF PARTICIPATORY LANGUAGE POLICIES \nWorking papers in Applied Linguistics and Linguistics at York 1 (2021) 15 \nWachter-Boettcher, S. (2017). Technically wrong: Sexist apps, biased algorithms, and other \nthreats of toxic tech. W.W. Norton & Company. \nWigglesworth, R. (2020, December 5). Robo-surveillance shifts tone of CEO earnings calls. \nFinancial Times. https://www.ft.com/content/ca086139-8a0f-4d36-a39d-409339227832 \nWorld Health Organization. (2021). Infodemic. World Health Organization. \nhttps://www.who.int/health-topics/infodemic#tab=tab_1 \n \n "
}