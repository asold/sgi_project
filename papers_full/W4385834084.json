{
  "title": "Unleashing the Potential of PIM: Accelerating Large Batched Inference of Transformer-Based Generative Models",
  "url": "https://openalex.org/W4385834084",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5032839104",
      "name": "Jaewan Choi",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A5100618952",
      "name": "Jaehyun Park",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A5002219153",
      "name": "Kwanhee Kyung",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A5037648751",
      "name": "Nam Sung Kim",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A5078262826",
      "name": "Jung Ho Ahn",
      "affiliations": [
        "Seoul National University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6767997687",
    "https://openalex.org/W2761132374",
    "https://openalex.org/W3189166979",
    "https://openalex.org/W3207399097",
    "https://openalex.org/W4280496502",
    "https://openalex.org/W6781533629",
    "https://openalex.org/W6842393504",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W4308083513",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4292779060"
  ],
  "abstract": "Transformer-based generative models, such as GPT, summarize an input sequence by generating key/value (KV) matrices through attention and generate the corresponding output sequence by utilizing these matrices once per token of the sequence. Both input and output sequences tend to get longer, which improves the understanding of contexts and conversation quality. These models are also typically batched for inference to improve the serving throughput. All these trends enable the models' weights to be reused effectively, increasing the relative importance of sequence generation, especially in processing KV matrices through attention. We identify that the conventional computing platforms (e.g., GPUs) are not efficient at handling this attention part for inference because each request generates different KV matrices, it has a low operation per byte ratio regardless of the batch size, and the aggregate size of the KV matrices can even surpass that of the entire model weights. This motivates us to propose AttAcc, which exploits the fact that the KV matrices are written once during summarization but used many times (proportional to the output sequence length), each multiplied by the embedding vector corresponding to an output token. The volume of data entering/leaving AttAcc could be more than orders of magnitude smaller than what should be read internally for attention. We design AttAcc with multiple processing-in-memory devices, each multiplying the embedding vector with the portion of the KV matrices within the devices, saving external (inter-device) bandwidth and energy consumption. AttAcc improves the performance and energy efficiency for DRAM access of serving GPT-3 by 3.19 and 3.22 times over the DGX A100 640 GB with an input sequence length of 2048 and an output sequence length of 128.",
  "full_text": "SUBMITTED TO IEEE COMPUTER ARCHITECTURE LETTERS 1\nUnleashing the Potential of PIM: Accelerating Large Batched\nInference of Transformer-based Generative Models\nJaewan Choi†, Jaehyun Park†, Kwanhee Kyung†, Nam Sung Kim‡, and Jung Ho Ahn†\n†Seoul National University, ‡University of Illinois at Urbana-Champaign\nAbstract—Transformer-based generative models, such as GPT, summarize an input sequence by generating key/value (KV) matrices\nthrough attention, and generate the corresponding output sequence by utilizing these matrices once per token of the sequence. Both\ninput and output sequences tend to get longer, which improves the understanding of contexts and conversation quality. These models\nare also typically batched for inference to improve the serving throughput. All these trends enable the models’ weights to be reused\neffectively, increasing the relative importance of sequence generation, especially in processing KV matrices through attention.\nWe identify that the conventional computing platforms (e.g., GPUs) are not efficient at handling this attention part for inference because\neach request generates different KV matrices, it has a low operation per byte ratio regardless of the batch size, and the aggregate size\nof the KV matrices can even surpass that of the entire model weights. This motivates us to propose AttAcc, which exploits the fact that\nthe KV matrices are written once during summarization but used many times (proportional to the output sequence length), each\nmultiplied by the embedding vector corresponding to an output token. The volume of data entering/leaving AttAcc could be more than\norders of magnitude smaller than what should be read internally for attention. We design AttAcc with multiple processing-in-memory\ndevices, each multiplying the embedding vector with the portion of the KV matrices within the devices, saving external (inter-device)\nbandwidth and energy consumption. AttAcc improves the performance and energy efficiency for DRAM access of serving GPT -3 by\n3.19 and 3.22 times over the DGX A100 640GB with an input sequence length of 2048 and an output sequence length of 128.\nIndex Terms—Transformer-based generative model, processing-in-memory, attention\n✦\n1 T RANSFORMER -BASED GENERATIVE MODELS\nGenerative artificial intelligence refers to machine learning\nmodels that can generate new content based on input context.\nSince its introduction, the Transformer-based generative model\n(TbGM) has demonstrated remarkable accuracy in various ap-\nplication types, including not only tasks in the field of natural\nlanguage processing but also image and speech processing\ndomains. Among TbGMs, Generative Pre-trained Transformers\n(GPTs) are gaining popularity due to their outstanding text\ngeneration performance and the popularity of ChatGPT service.\nBased on the observation that larger models and longer\nsequences can improve accuracy [7], [10], the size of pretrained\nweights has increased to hundreds of GB (see Table 1) and\nthe recently published GPT-4 [5] has a maximum input se-\nquence length of 32,768. Moreover, considering that the services\nsuch as ChatGPT can accumulate the previous context ( e.g.,\nconversation) of the same user and store multiple examples\nin the context window [1], TbGM is expected to have a long\ninput sequence. The amount of computation and the memory\nworking set size increase proportional to the model size and\nthe sequence length. GPT-3 has up to 175 billion parameters\nand requires 1,425 TFLOPs of computation ( i.e., 365,384 × of\nResNet-50) for inference with an input sequence length (Lin) of\n2,048 and an output sequence length ( Lout) of 2,048. As GPT-\nbased services such as ChatGPT are recently drawing attention\nand the request traffic from their clients has surged rapidly, it is\ncritical to improve the serving throughput of TbGM inference.\nWe focus on the inference of GPT in this paper.\nGPT model: The GPT architecture is based on the decoder of\nTransformer [8] (see Figure 1). In a GPT model, input tokens are\nfirst converted into embedding vectors through Token Embed-\nding, and cascaded through Ndec decoders, each with different\npretrained weights, generating an output embedding vector\nusing these vectors as input. The output vector is converted\nto an output token through a language model (LM) head.\nA decoder consists of a multi-head attention block and\na feedforward block. Both blocks are accompanied by layer\nTable 1: Trends of GPT models\nModel GPT-1 GPT-2 GPT-3 GPT-4\nMax. model size (FP16)0.21GB 2.8GB 326GB -\nMax. input seq. len. 512 1,024 2,048 32,768\nnormalization (LayerNorm), a type of normalization performed\non the embedding vector, and residual, element-wise addition.\nThe multi-head attention block is composed of three layers:\nQKV generation layer, attention layer, and projection layer in\norder. QKV generation and projection layers are fully connected\n(FC) layers for generating query (Q), key (K), and value (V)\ndata which contain the information of each input token and\nprojecting the outputs of the attention layer. The attention\nlayer calculates the contextual representation between tokens\n(Figure 1(left)). It operates as a multi-head; The Q, K, and V\ndata are equally divided into Nhead heads (Qi, Ki, and V i for i\n= 0, 1, ..., Nhead-1). Each head sequentially performs an inner\nproduct (score operation) on Q i and K i, a softmax operation\non the result, and another inner product (context operation)\nwith V i on the softmax result. The outputs of each head are\nconcatenated and used as input for the projection layer. The\nfeedforward block consists of two FC layers and a Gaussian\nerror linear unit (GELU) layer.\nGPT inference process:GPT inference of a request comprises a\nsummarization (Sum) stage that understands the context of an\ninput sequence, followed by multiple generation (Gen) stages\nthat generate new tokens. Each stage proceeds sequentially and\nreuses the same pretrained weights. The primary operations\nin the Sum stage are the General Matrix-Matrix Multiplication\n(GEMM). In the Sum stage, each decoder receives a matrix of\nLin ×demb (dimension of embedding) as an input, processes it\nthrough FC and attention layers, and outputs a matrix of the\nsame size. During the attention layer, the Sum stage generates\nkey and value (KV) matrices with a size of 2×Lin×demb, which\ncontain the context of the input sequence.\nThe Gen stage proceeds similarly to the Sum stage but with\ntwo major differences. First, a Gen stage mainly performs the\ngeneral matrix-vector multiplication (GEMV) operation as it\nThis article has been accepted for publication in IEEE Computer Architecture Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LCA.2023.3305386\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nSUBMITTED TO IEEE COMPUTER ARCHITECTURE LETTERS 2\nLayerNormResidualLayerNormResidualProjectionFCGELUFCScore (QxKT)SoftmaxContext (SxV)QKV generationFeedForwardBlockMulti-head AttentionBlock: Attention layer: FC layer“there”1st Output token\nDecoder0Input token“there”TokenEmbeddingDecoder0Input token“was”TokenEmbeddingDecoder0Input sequence“Once upon a time”Decoder1Decoder1Decoder1TokenEmbeddingSummarization stageDecoder Ndec-1“was”2nd Output tokenLM headDecoder Ndec-1“a”3rdOutput tokenLM headDecoder Ndec-1LM headGeneration stagesAppend K, V\nFigure 1: The architecture and inference process of GPT.\n2 8 32 128 512 2,048\n# Input tokens (Lin)\n2,048\n512\n128\n32\n8\n2\n# Output tokens (Lout) 99.9 99.9 99.9 99.9 99.8 99.2\n99.8 99.8 99.8 99.8 99.2 96.8\n99.2 99.2 99.2 99.1 96.7 88.2\n96.9 96.8 96.7 96.3 87.9 64.5\n87.5 87.4 87.0 85.6 62.1 29.1\n50.0 49.8 48.9 45.8 19.0 5.5\nFigure 2: Generation stage time ratio in total execution time\nfor various numbers of input and output tokens on GPT-3.\ntakes single token generated in the previous stage as input, and\nthe input of each decoder is also a vector. Second, the attention\nlayer operates the Q vector for an input token of the current\nstage with the KV matrices that aggregate the input tokens of\nthe Sum stage with the generated tokens up to the previous\nGen stage. That is, each Gen stage appends newly created KV\nvectors of an input token to the KV matrices transferred from\nthe previous stage. When L is defined as the sum of Lin and\nthe number of tokens generated up to the current stage, the\ndimensions of Q vector and the KV matrix are 1 ×demb and\nL×demb, respectively. The output token of a Gen stage becomes\nthe input of the next Gen stage, being repeated until an end-of-\nsequence token is generated.\nThe Gen stages typically overwhelm the Sum stage in\nexecution time due to their sequential nature of reading the\nentire pretrained weights per token. Figure 2 shows the por-\ntion of time taken by the Gen stages over the total execution\ntime for various Lin and Lout in the GPT-3 175B model. We\nexperimented with our in-house simulator assuming the peak\nFLOPs (2.5 PFLOPS) and maximum memory bandwidth (16\nTB/s) of the NVIDIA DGX A100 640GB (DGX). When both Lin\nand Lout are 32, the portion of Gen stages exceeds 96%. Even\nif Lin is 2048, the longest Sum stage in our configuration, and\nLout is 128, the portion of Gen stages also exceeds 88%. This\ndominance can also be confirmed in the prior works [2], [11]\ndealing with GPT-2. We focus on the Gen stages hereafter.\n2 A NALYSIS OF BATCHING FOR TBGM\nBatching has been widely used to improve the serving through-\nput of machine learning inference, which is a methodology that\nprocesses multiple client requests for the same model in the\nform of grouped tasks. In the case of TbGM, we assume stage-\nlevel scheduling for batching as in [9] considering that the\nnumber of stages (same as Lout) is different for each request.\nThat is, when a request in a batch is completed, a new request\nis added to the batch in the next stage instead of waiting for the\nother requests in a batch to be completed.\nIn this section, we first analyze the benefits of batching\nfor GPT on the DGX system, a representative of serving large\nTbGMs. Then, we identify the limitations of the DGX system\non large batch sizes while processing the attention layer.\n2.1 Benefits of Batching for TbGM\nApplying the batching in GPT improves the reusability of\nweights in computing the FC layer; it improves both the\nthroughput and energy efficiency of conventional serving plat-\nforms, such as DGX. When batched, the FC layer sharing the\nweights across different requests becomes a GEMM operation\ninstead of a GEMV operation. This means that once weights are\nread from the off-chip memory, all requests in the batch can be\nprocessed simultaneously, improving the Op/B (the number of\narithmetic operations per byte accessed from off-chip memory)\nof the FC layer. Consequently, there is little change in the\nexecution time of the FC layer, and the throughput of FC layers\nimproves by a factor equivalent to the batch size. Moreover, off-\nchip memory accesses to the weights decrease proportionally\nto the batch size, reducing the energy consumption of the off-\nchip memory. Figures 3(a) and 3(b) represent the throughput\n(generated tokens per second) and energy consumption for off-\nchip memory accesses. For all three configurations of Lout with\nLin of 2,048, an increase in batch size leads to an improvement\nin throughput and a reduction in energy consumption.\n2.2 Limitations of Conventional Platforms for Batching\nDespite the benefits of batching, new challenges due to the\nattention layer arise as the batch size increases with the trend\nof long input sequences; they impose limitations on conven-\ntional platforms. First, the size of the KV matrices, which\ngrows with batch size, could surpass the memory capacity of\nconventional platforms such as GPU. The attention layer has\ndifferent KV matrices for each request, with their sizes being\nproportional to the sequence length. The number of elements\nof KV matrices is (Lin +Lout)×demb per decoder, resulting in\n2×Ndec×(Lin+Lout)×demb per request. For example, on the GPT-3\n175B model using FP16 with both Lin and Lout of 2048, the size\nof KV matrices is 18 GB per request; thus, the required memory\ncapacity for a batch size of 64 is about 1.5TB, including 324GB\nof FC layer weights and 1,152GB of KV matrices, which is much\nlarger than DGX’s 640GB memory capacity (see Figure 3(a)).\nSecond, the latency of executing the attention layer increases\nwith batching, and with large batch sizes, it takes a large\nportion of the execution time. The latency of executing an FC\nlayer is nearly uniform regardless of batch sizes until the Op/B\nratio in the course of computing the FC layer exceeds that\nprovided by DGX (when the batch size is 146). In contrast, the\nlatency of executing the attention layer increases steadily with\nbatch size, accounting for more than 60 percent of execution\ntime when the batch size is 64 (see Figure 3(c)).\nThe attention layer has a low arithmetic intensity regardless\nof batch size. The primary operation of the attention layer in\nthe Gen stage is GEMV between the Q vector generated by\nthe input token and the KV matrices, exhibiting a low Op/B\n(∼1). Unlike the FC layer, the attention layer still has memory-\nintensive GEMV operations even after batching due to the\nsequential nature of processing the Gen stages, as explained\nin Section 1. Therefore, the computing units of DGX are mostly\nidle for the attention layer, even with batching. As shown in\nThis article has been accepted for publication in IEEE Computer Architecture Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LCA.2023.3305386\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nSUBMITTED TO IEEE COMPUTER ARCHITECTURE LETTERS 3\n10\n100\n1,000\n1 4 1664256\nThroughputRequired memory capacity\nMemory capacity (GB)\nTokens per second Batch size\nMemory access energy per token (uJ)\n(b)(a)\n064012801920\n0300600900\n141664256141664256141664256Batch sizeBatch sizeBatch size\n640 \nDGX-A100\nLatency (ms/batch)\n(c)\nBatch size(Lin: 2048, Lout:128) Batch size(Lin=2048, Lout=512)Batch size(Lin=2048, Lout=2048)\nLin=2048, Lout=128Lin=2048, Lout=512Lin=2048, Lout=2048 FC layerAttention layerEtc\n0%20%40%60%80%100%\n0306090120150\n141664256141664256141664256Batch sizeBatch sizeBatch size\n275 294 370\nBatch size(Lin=2048, Lout=128)Batch size(Lin=2048, Lout=512)Batch size(Lin=2048, Lout=2048)\nComp. Util\nCompute utilization\n2932428429324489303348108\nFigure 3: (a) Throughput, required memory capacity, (b) memory access energy consumption per output token, and (c) the Gen\nstage time breakdown and GPU compute utilization for various batch sizes withLin of 2048 andLout of 128, 512, or 2048 when\nrunning the GPT-3 175B model on DGX assuming unlimited memory capacity. The dotted bars in (a) indicate the throughput\nwith a batch size not available with DGX A100 (640GB) due to memory capacity limitation.\nAttAcc\nInterconnect PIM Controller\nDRAM Controller\nHBM\nPE\nHBM\nPE\nHBM\nPE\nHBM\nPE\nDRAM Controller\nHBM\nPE\nHBM\nPE\nHBM\nPE\nHBM\nPE\nDRAM Controller\nHBM\nPE\nHBM\nPE\nHBM\nPE\nHBM\nPE\nDRAM Controller\nHBM\nPE\nHBM\nPE\nHBM\nPE\nHBM\nPE\nxPU\n(e.g., GPU, TPU)\nxPU\n(e.g., GPU, TPU)\nAttAcc\nFigure 4: Overall heterogenous system with xPUs and AttAcc.\nFigure 3(c), the GPU utilization increases due to the improved\nOp/B of the FC layers. However, even when the batch size\nreaches 256, the utilization remains below 20% because of the\nsubstantial time spent in the attention layer.\nMoreover, the increased latency due to the attention layer\ncould potentially violate a Service Level Objective (SLO). For\ninstance, if the SLO of GPT-3 is set to 50ms per output token,\nwithin a reasonable range, a Gen stage with a batch size of 64\nand Lin/Lout of 2048 would violate the SLO as the latency rises\nto 108ms (see Figure 3(c)), necessitating a limit on the batch size\nto 16 or fewer to meet the SLO of 50ms. Thus, if considering the\nSLO, conventional platforms need to further increase expensive\noff-chip memory bandwidth for attention layer acceleration to\nprocess large batches.\nTo overcome these limitations, scaling out the conventional\nplatforms (by populating more) is an option, but it is not\ncompelling due to low compute utilization. Instead, we propose\nan architecture that cost-effectively addresses the limitations.\n3 T BGM ACCELERATOR ARCHITECTURE\nTo tackle the inefficiency of conventional platforms executing\nTbGM, we propose a distributed heterogeneous computing\nsystem. Specifically, it consists of high-performance compute\nunits (xPUs), such as GPUs or TPUs, and AttAcc, which are con-\nnected by a commercial high-bandwidth interconnect, such as\nPCIe, NVLink, and CXL (see Figure 4). This system accelerates\nthe low Op/B attention layers with AttAcc, while executing\nthe high Op/B FC layers with large batch sizes ( i.e., GEMM\noperations) on xPUs.\nFocusing on AttAcc in this paper, we start the design with\na PIM (processing-in-memory) architecture adopted by HBM-\nPIM [3], which directly couples processing elements (PEs) with\neach DRAM bank at the I/O boundary and activates NPE\nsuch DRAM banks in parallel to expose NPE × higher internal\nbandwidth to the PEs than external bandwidth to the xPUs. We\nidentify the following two reasons that such a PIM architecture\nis compelling for a baseline AttAcc.\nFirst, KV matrices are written once at the Sum stage but\nused many times at the Gen stages. Such a write at the Sum\nstage and the reuses of the KV matrices at the Gen stages re-\nquire the external bandwidth and the internal PIM bandwidth,\nrespectively. Considering that writing KV matrices at the Sum\nstage of a batch can be amortized to reading at the Gen stages\nof the previous batch, PIM’s external bandwidth requirement\nfor KV matrices could be reduced by orders of magnitude.\nSecond, the GEMV operations of the attention layer during\nthe Gen stages have much smaller inputs and outputs than the\nlarge KV matrices to process. In the attention layer, the score\nand context operations, each performing Nhead GEMVs, have\nthe sizes of (input, K or V matrix, output) as ( 1×demb, L×demb,\nNhead×L) and (Nhead×L, L×demb, 1×demb). Thus, for the GEMVs\nof the attention layer, the size ratio of the input and output data\nover the KV matrices corresponds to(demb+Nhead×L)/(L×demb).\nFor GPT-3 with 175B weights, demb and Nhead are 12,288 and\n96, respectively, and assuming a largeL (e.g., > 2048), this ratio\nis about Nhead/demb = 1/128, which implies the ratio of PIM’s\nexternal and internal bandwidth requirements.\nAttAcc based on such a PIM architecture has an ample\ndesign space to explore. It includes what to use for baseline\nmemory type ( e.g., DDR, LPDDR, and HBM DRAM devices),\nwhere to place PEs for GEMV and softmax operations within\nthe DRAM organization hierarchy, whether to use 3D-stacking\nor chiplet integration technology if we place controllers and\nexpensive PEs on different dies, whether to use a logic process\nor a DRAM process for those dies, and how to map the attention\nlayer to the DRAM banks (similar to [6]). While leaving such\ndeep design space explorations as future work, we focus on\nHBM-PIM-based AttAcc and propose to place the PEs for\nGEMV on the DRAM die and place the PEs for expensive\noperations such as exponentiation for softmax operation on the\nHBM buffer die in this paper. We map each attention head to\none HBM. Each bank processes a portion of Ki and Vi divided\ninto column-wise and row-wise, respectively. Because GEMV\noperations in the attention layer have a sufficient degree of\nparallelism across multiple requests, heads, and tokens, bank-\nlevel parallelism can be fully utilized.\nHBM-PIM was the first PIM designed by a major DRAM\nmanufacturer to be compatible with a JEDEC standard DRAM\ninterface. It has reinvigorated development interests in com-\nmercial PIM devices since its introduction. Nonetheless, it still\nneeds to demonstrate its superior efficiency over conventional\nplatforms for highly-demanding commercial applications, such\nas TbGM. A key contribution of this paper is to demonstrate\nthe potential of such a PIM architecture for TbGM.\nAttAcc is generally not appropriate for accelerating TbGM\ntraining because it mostly proceeds with compute-intensive\nSum stages. Instead, AttAcc can be utilized for fine-tuning\nfrom human preferences, such as Reinforcement Learning from\nHuman Feedback, which entails a Gen stage. AttAcc can also\nThis article has been accepted for publication in IEEE Computer Architecture Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LCA.2023.3305386\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nSUBMITTED TO IEEE COMPUTER ARCHITECTURE LETTERS 4\nDGX + AttAcc(1280GB)DGX (1280GB)DGX (640GB)\n01234w/o SLO30ms50ms70msw/o SLO30ms50ms70msw/o SLO30ms50ms70msw/o SLO30ms50ms70msw/o SLO30ms50ms70msSLO(Lin=128,Lout=128)SLO(Lin=512,Lout=512)SLO(Lin= 2048, Lout=128)SLO(Lin=2048, Lout=512)SLO(Lin=2048, Lout=2048)\nNormalized throughputLin=128,Lout=128(3039 tokens/s)Lin=512,Lout=512(1294tokens/s)Lin=2048,Lout=128(530 tokens/s)Lin=2048,Lout=512(469 tokens/s) Lin=2048,Lout=2048(331 tokens/s)\nFigure 5: The normalized throughput of DGX(640GB),\nDGX(1280GB), and DGX+AttAcc(1280GB) for various SLOs.\nThe baseline isDGX(640GB) without the SLO, with absolute\nvalues in parentheses.\nimprove the performance of other memory-intensive applica-\ntions suitable for PIM [6].\n4 E VALUATION\nExperimental setup: We compared a system combining DGX\nand AttAcc ( DGX+AttAcc) with a baseline DGX-only system\n(DGX(640GB)) and the baseline but with twice the memory\ncapacity (DGX(1280GB)). The memory capacity of DGX+AttAcc\nis 1280GB in total where that of DGX is 324GB to store the\nweight of the FC layers and that of AttAcc corresponds to the\nrest. The aggregate internal bandwidth of AttAcc is 64 TB/s,\nfour times higher than the aggregate memory bandwidth of\nDGX [3]. The energy per bit transfer of HBM for PIM operations\nis assumed to be 3.5× less than that of DGX’s HBM [3].\nWe developed an in-house simulator to measure the per-\nformance and energy efficiency of the systems. The simulator\nreceives DGX and AttAcc configurations as well as model\ninformation (dimension and operation type of each layer) as\ninput, and outputs the execution time and energy consumption\nof each system. We used the DRAM access energy as that of\nHBM2 reported in [4]. We validated our in-house simulator by\ncomparing the performance executed on the actual NVIDIA\nA100 GPU for GPT-J 6B, which is known to perform similarly\nto the closed-source GPT-3.\nWe set GPT-3 175B, as the target model with the data type\nof FP16. We used the average sequence length of requests in a\nbatch for various configurations of Lin and Lout assuming that\nenough requests can be batched. The SLO of TbGMs is different\nfor each service, and there is no specific information disclosed.\nTherefore, we inferred approximate values of the latency target\nper token by referring to [9], and explored the scenario without\nthe SLO, and with 30ms, 50ms, and 70ms latency targets.\nPerformance: DGX+AttAcc achieves up to 3.24 × and 2.04 ×\nhigher throughput for DGX(640GB) and DGX(1280GB), re-\nspectively, with greater Lin leading to larger throughput im-\nprovement (see Figure 5). This improvement attributes to\nDGX+AttAcc’s increased memory capacity to support larger\nbatch sizes than DGX(640GB) and its use of the PIM archi-\ntecture that provides higher internal aggregate memory band-\nwidth for the attention layer compared to both DGX(640GB)\nand DGX(1280GB). Considering the SLO, DGX+AttAcc can\nachieve further throughput improvement than DGX(640GB)\nand DGX(1280GB), which have limited batch size due to the\nlatency of the attention layer (e.g., 5.86 × than DGX(1280GB)\nwith SLO of 30ms). Overall, the tighter the SLO, the greater the\nthroughput improvement of DGX+AttAcc.\nEnergy consumption:DGX+AttAcc achieves higher energy ef-\nficiency for off-chip memory access than DGX systems (see Fig-\nure 6). First, due to the larger batch size, the weight reusability\nof FC layers increases, reducing the number of off-chip memory\nAttention layerFC layerEtc\n00.20.40.60.81DGX(640GB)DGX(1280GB)DGX+ AttAccDGX(640GB)DGX(1280GB)DGX+ AttAccDGX(640GB)DGX(1280GB)DGX+ AttAccDGX(640GB)DGX(1280GB)DGX+ AttAccDGX(640GB)DGX(1280GB)DGX+ AttAccLin=128,Lout=128(6.71mJ)Lin=512,Lout=512(17.6mJ)Lin=2048,Lout=128(44.9mJ)Lin=2048,Lout=512(50.8mJ)Lin=2048,Lout=2048(72.4mJ)\nNormalized memory access energy per tokenLin=128,Lout=128(6.71mJ)Lin=512,Lout=512(17.6mJ)Lin=2048,Lout=128(44.9mJ)Lin=2048,Lout=512(50.8mJ)Lin=2048,Lout=2048(72.4mJ)\nFigure 6: Normalized memory access energy per output token\nof DGX(640GB), DGX(1280GB), and DGX+AttAcc for GPT-3\n175B without SLO. The baseline isDGX(640GB), with abso-\nlute values in parentheses.\naccesses. Second, the energy consumption of the attention layer\nis reduced because of the memory access energy reduction of\nthe PIM architecture. Without considering the SLO, memory\naccess energy consumption is reduced by up to 69% and 51%\ncompared to DGX(640GB) and DGX(1280GB), respectively.\n5 C ONCLUSION\nWe have analyzed the impact of batching for Transformer-based\ngenerative models and identified the growing importance of\nthe attention layer in the trend with increasing model sizes and\nsequence length. The conventional serving platforms, such as\nGPUs, are suboptimal for large batch sizes having stringent\nmemory capacity and bandwidth requirements processing the\nattention layer under a tight service-level objective. We pro-\nposed AttAcc, an accelerator for the attention layer, composed\nof cost-effective processing-in-memory devices exploiting the\ncharacteristic that the external bandwidth requirement is far\nless than the internal bandwidth requirement for the attention\nlayer. Compared to the monolithic state-of-the-art GPU sys-\ntem, the heterogeneous system combining GPUs with AttAcc\nachieved significantly higher throughput (up to 3.24 ×) and\nenergy efficiency (up to 3.22×).\nACKNOWLEDGMENTS\nThis work was in part supported by the IITP grant funded\nby the Korea government (MSIT) (No. 2021-0-00863), Samsung\nElectronics (IO230216-05037-01), and PRISM, one of the seven\ncenters in JUMP 2.0, an SRC program sponsored by DARPA.\nREFERENCES\n[1] T. Brown et al., “Language Models are Few-Shot Learners,” in\nNeurIPS, 2020.\n[2] S. Hong et al., “DFX: A Low-latency Multi-FPGA Appliance for Ac-\ncelerating Transformer-based Text Generation,” in MICRO, 2022.\n[3] S. Lee et al., “Hardware Architecture and Software Stack for PIM\nbased on Commercial DRAM Technology: Industrial Product,” in\nISCA, 2021.\n[4] M. O’Connor et al., “Fine-Grained DRAM: Energy-Efficient DRAM\nfor Extreme Bandwidth Systems,” in MICRO, 2017.\n[5] OpenAI, “GPT-4 Technical Report,” 2023.\n[6] J. Park et al., “TRiM: Enhancing Processor-Memory Interfaces with\nScalable Tensor Reduction in Memory,” in MICRO, 2021.\n[7] M. Shoeybi et al., “Megatron-LM: Training Multi-Billion Param-\neter Language Models using Model Parallelism,” arXiv preprint\narXiv:1909.08053, 2019.\n[8] A. Vaswani et al., “Attention is All you Need,” in NeurIPS, 2017.\n[9] G.-I. Yu et al. , “ORCA: A Distributed Serving System for\nTransformer-Based Generative Models,” in OSDI, 2022.\n[10] M. Zaheer et al., “Big Bird: Transformers for Longer Sequences,”\nin NeurIPS, 2020.\n[11] M. Zhou et al., “TransPIM: A Memory-based Acceleration via\nSoftware-Hardware Co-Design for Transformer,” in HPCA, 2022.\nThis article has been accepted for publication in IEEE Computer Architecture Letters. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/LCA.2023.3305386\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7841237783432007
    },
    {
      "name": "Byte",
      "score": 0.5158149600028992
    },
    {
      "name": "Embedding",
      "score": 0.5135793685913086
    },
    {
      "name": "Security token",
      "score": 0.5111854672431946
    },
    {
      "name": "Inference",
      "score": 0.5017287731170654
    },
    {
      "name": "Transformer",
      "score": 0.49411657452583313
    },
    {
      "name": "Dram",
      "score": 0.425618052482605
    },
    {
      "name": "Sequence (biology)",
      "score": 0.41275736689567566
    },
    {
      "name": "Algorithm",
      "score": 0.38782936334609985
    },
    {
      "name": "Parallel computing",
      "score": 0.3536420464515686
    },
    {
      "name": "Computer engineering",
      "score": 0.3503304421901703
    },
    {
      "name": "Computer hardware",
      "score": 0.2506718039512634
    },
    {
      "name": "Voltage",
      "score": 0.21050721406936646
    },
    {
      "name": "Artificial intelligence",
      "score": 0.15711647272109985
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    }
  ]
}