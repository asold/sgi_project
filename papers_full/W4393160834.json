{
  "title": "Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning of Large Language Models",
  "url": "https://openalex.org/W4393160834",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5000300840",
      "name": "Dingzirui Wang",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5039847345",
      "name": "Longxu Dou",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100710814",
      "name": "Wenbin Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5011562560",
      "name": "Junyu Zeng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5019108029",
      "name": "Wanxiang Che",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2252123671",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3199132172",
    "https://openalex.org/W3198243861",
    "https://openalex.org/W3200079259",
    "https://openalex.org/W4385570993",
    "https://openalex.org/W2799151853",
    "https://openalex.org/W4323927271",
    "https://openalex.org/W4378771323",
    "https://openalex.org/W6756315458",
    "https://openalex.org/W6720621119",
    "https://openalex.org/W4385569968",
    "https://openalex.org/W4385894652",
    "https://openalex.org/W6847707732",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W4308609800",
    "https://openalex.org/W6691715656",
    "https://openalex.org/W2524879642",
    "https://openalex.org/W6792087171",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W6779254487"
  ],
  "abstract": "Numerical reasoning is a vital capability for natural language processing models to understand and process numerical information in real-world scenarios. Most current methods first generate the Intermediate Meaning Representations (IMRs) of questions and then generate answers. Current SOTA methods generate programs as IMRs with large language models (LLMs). Intuitively, equations have fewer restrictions and closer semantics to the question than programs, leading to higher generation accuracy. However, current LLMs generate equations worse than programs, where we assume that the equation data is rare in pre-training data compared to programs. So in this paper, we try to use equations as IMRs to solve the numerical reasoning task by addressing two problems: (1) Theoretically, how to prove that the equation is an IMR with higher generation accuracy than programs; (2) Empirically, how to improve the generation accuracy of equations with LLMs. For the first problem, we propose and prove a proposition to theoretically compare the generation accuracy of different IMRs. For the second problem, we present a method called Boosting Numerical ReasonIng by Decomposing the Generation of Equations Bridge, which can improve the accuracy of LLMs in generating equations as IMRs by reducing the tendency of generating constant expressions and programs. Our method improves the performance by 2.2%, 0.9%, and 1.7% on GSM8K, SVAMP, and Algebra datasets compared to the previous state-of-the-art methods under the single reasoning path setting. Our code and prompts are available at https://github.com/zirui-HIT/Bridge_for_Numerical_Reasoning}.",
  "full_text": "Exploring Equation as a Better Intermediate Meaning Representation for\nNumerical Reasoning of Large Language Models\nDingzirui Wang1, Longxu Dou1, Wenbin Zhang2, Junyu Zeng2, Wanxiang Che1,*\n1Harbin Institute of Technology\n2Yunfu Technology (Beijing) Co., Ltd.\n{dzrwang, lxdou, car}@ir.hit.edu.cn, {zhangwenbin, zengjunyu}@yunfutech.com\nAbstract\nNumerical reasoning is a vital capability for natural language\nprocessing models to understand and process numerical in-\nformation in real-world scenarios. Most current methods first\ngenerate the Intermediate Meaning Representations (IMRs)\nof questions and then generate answers. Current SOTA meth-\nods generate programs as IMRs with large language mod-\nels (LLMs). Intuitively, equations have fewer restrictions and\ncloser semantics to the question than programs, leading to\nhigher generation accuracy. However, current LLMs generate\nequations worse than programs, where we assume that the\nequation data is rare in pre-training data compared to pro-\ngrams. So in this paper, we try to use equations as IMRs to\nsolve the numerical reasoning task by addressing two prob-\nlems: (1) Theoretically, how to prove that the equation is an\nIMR with higher generation accuracy than programs;(2) Em-\npirically, how to improve the generation accuracy of equa-\ntions with LLMs. For the first problem, we propose and\nprove a proposition to theoretically compare the generation\naccuracy of different IMRs. For the second problem, we\npresent a method called Boosting Numerical Reasoning by\nDecomposing the Generation of Equations (BRIDGE ), which\ncan improve the accuracy of LLMs in generating equations\nas IMRs by reducing the tendency of generating constant\nexpressions and programs. Our method improves the per-\nformance by 2.2%, 0.9%, and 1.7% on GSM8K, SV AMP,\nand Algebra datasets compared to the previous state-of-the-\nart methods under the single reasoning path setting. Our\ncode and prompts are available at https://github.com/zirui-\nHIT/Bridge\nfor Numerical Reasoning.\nIntroduction\nNumerical reasoning is an essential ability of natural lan-\nguage processing (NLP) models to handle documents fulling\nof numerical information, which is widely used in finance,\nscience, and other fields (Chen et al. 2021b, 2023; Lu et al.\n2023). Generally, numerical reasoning is to generate a value\nresult based on the given question, which describes the val-\nues and relationships of quantities (Zhang et al. 2020).\nNumerical calculations, by their inherent complexity,\nmake it a struggle to produce accurate value results di-\nrectly (Thawani et al. 2021). To overcome this challenge,\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nEquation\nalice_candy = 2 ✕ bob_candy\ndavid_candy + 2 = 3 ✕ bob_candy\ndavid_candy + 2 = 18\nans = alice_candy\nProgram\ndavid_candy = 18 - 2\nbob_candy = (david_candy + 2) / 3\nalice_candy = 2 ✕ bob_candy\nans = alice_candy\nConstant Expression\nans = 2 ✕ (18 - 2 + 2) / 3\nAlice has twice as much candy as Bob. \nDavid has 3 times of Bob if David takes 2.\nHow much candy does alice have?\nIf David takes two candies, he has 18.\nAlice has twice as much candy as Bob. \nDavid has 3 times of Bob if David takes 2.\nHow much candy does alice have?\nIf David takes two candies, he has 18.\nAlice has twice as much candy as Bob. \nDavid has 3 times of Bob if David takes 2.\nHow much candy does alice have?\nIf David takes two candies, he has 18.\nQuestion\nAlice has twice as much candy as Bob. David has 3 times of Bob if David takes 2. If David \ntakes two candies, he has 18. How much candy does alice have?\nFigure 1: Examples of three types of IMRs. The dotted line\nindicates the correspondence between the question and the\ngenerated IMR. The more complex the correspondence is,\nthe more challenging it becomes to generate accurately.\nmost current methods first generate Intermediate Meaning\nRepresentations (IMRs) of questions, then compute the\nvalue results with external tools (e.g., algorithms, inter-\npreters) (Huang et al. 2018; Wang, Zhang, and Wang 2023).\nFor example, the constant expression is a commonly used\nIMR (Roy and Roth 2015; Koncel-Kedziorski et al. 2016).\nThe program is another common IMR used by the current\nstate-of-the-art (SOTA) methods (Chen et al. 2022; Gao\net al. 2022; Xie et al. 2023). Examples of these two types\nof IMRs are shown in Figure 1. Current methods mainly use\nlarge language models (LLMs) to generate IMRs because\nLLMs can use few-shot inference to generate various IMRs\nwithout training (Jin and Lu 2023; Xie et al. 2023).\nIn addition to the IMRs above, previous work has also\nused systems of equations as IMRs (Roy, Upadhyay, and\nRoth 2016; He-Yueya et al. 2023). From an intuitive view,\nusing equations as IMRs should be better than using pro-\ngrams because equations do not need to define variables be-\nfore using them, leading to closer semantics to natural lan-\nguage questions. An intuitive example is shown in Figure 1.\nHowever, the current method using equations as IMRs does\nnot have better performance than that of using programs\n(He-Yueya et al. 2023). So in this paper, we try to use equa-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19116\ntions as IMRs to solve the numerical reasoning task by ad-\ndressing two problems: (1) Theoretically, how to prove that\nthe equation is an IMR with higher generation accuracy\nthan programs;(2) Empirically, how to improve the gener-\nation accuracy of equations with LLMs.\nFor the first problem, we present and prove a proposition\nto compare the generation accuracy of different IMRs:given\ntwo IMRs, IMRA and IMRB, if IMRA is the subset of\nIMRB, then the accuracy in generating IMRB of ques-\ntions theoretically surpasses IMRA. Based on this proposi-\ntion, we can prove that the accuracy of generating equations\nis higher than that of programs. Because programs can be\nseen as equations with the restriction “variables must be de-\nfined before being used”, programs are a subset of equations.\nConsequently, employing equations as IMRs confers theo-\nretically higher generation accuracy than that of programs.\nFor the second problem, current LLMs have poor perfor-\nmance in generating equations (He-Yueya et al. 2023). We\nassume that is because current LLMs are mainly pre-trained\nwith constant expressions and programs for numerical rea-\nsoning (Brown et al. 2020; Chen et al. 2021a), which makes\nLLMs prefer to generate these two types of IMRs rather\nthan other IMRs during few-shot inference. This limits the\nnumerical reasoning ability of LLMs since these two types\nof IMRs may not be the best IMRs for this task. To lower\nthe tendency of LLMs to generate constant expressions and\nprograms, we propose our method called Boosting Numeri-\ncal Reasoning by Decomposing the Generation of Equations\n(BRIDGE ). Our method erases asking parts and decomposes\nquestions into sub-questions, which can improve the ten-\ndency of LLMs to generate equations.\nTo evaluate the effectiveness of BRIDGE , we adopt experi-\nments on GSM8K (Cobbe et al. 2021), SV AMP (Patel, Bhat-\ntamishra, and Goyal 2021), and Algebra (He-Yueya et al.\n2023), which are mainstream datasets of the numerical rea-\nsoning task. B RIDGE improves 1.6% performance over the\nprevious SOTA results on all above datasets on average and\nachieves new SOTA results under the single reasoning path\nsetting. In addition, ablation experiments show that BRIDGE\ncan improve the proportion of equations in generated results,\nwhich shows that our method can indeed improve the ten-\ndency of LLMs to generate equations as IMRs.\nOur contribution can be summarized as follows:\n• To theoretically prove that equations have higher gener-\nation accuracy than the IMRs of the current SOTA meth-\nods, we present and prove a proposition that can theoreti-\ncally compare the generation accuracy of different IMRs.\n• To empirically improve the performance of LLMs in gen-\nerating equations other than constant expressions and\nprograms, we present B RIDGE , which improves the ten-\ndency of LLMs to generate equations as IMRs.\n• To verify the effectiveness of B RIDGE , we conduct ex-\nperiments on multiple mainstream numerical reasoning\ndatasets, where our method achieves new SOTA results\non all datasets under the single reasoning path setting.\nMethodology\nIn this section, we introduce our work in detail. First, we ex-\nplain why we use equations as IMRs as an intuitive explana-\ntion for our proposition. Then, we present and prove a propo-\nsition that can theoretically compare the generation accuracy\nof different IMRs. After that, we introduce the pipeline of\nBRIDGE .\nEquations as Intermediate Meaning\nRepresentation\nIntuitive Principle of IMR DesignThe previous research\nhas shown that even with the same model architecture, gen-\nerating different IMRs may lead to different performances\n(Huang et al. 2018; Li et al. 2022). Therefore, the design\nof IMRs also affects the accuracy of generating. Generally,\nthe more restrictive rules there are on IMRs, the harder it is\nfor the model to generate such IMRs. Because if IMRs have\nmore restrictions, the model needs more reasoning steps to\nmeet these restrictions, resulting in the semantic difference\nwith the question, increasing the difficulty of reasoning.\nComparison of Different IMRs A commonly used IMR\nis the constant expression (Roy and Roth 2015; Koncel-\nKedziorski et al. 2016), which asks that only values, no vari-\nables, appear in one single expression as a result. This leads\nto a great semantic difference between the questions and the\nconstant expressions because questions may describe many\nrelationships between different quantities. To address these\nrestrictions, previous works propose using programs, which\nis the IMR of the current SOTA methods (Chen et al. 2022;\nGao et al. 2022; Xie et al. 2023). First, the program allows to\nuse variables to calculate other variables instead of just con-\nstant values. Second, programs can use multiple statements\nrather than just one statement to represent the answer.\nHowever, programs also have their restrictions, where\neach variable must be defined before use. This also leads\nto a semantic difference between the IMR and the question\nbecause one question may describe the relationships of vari-\nables before giving their values. To solve this restriction, we\npropose to use the equation as IMR, where the equation al-\nlows variables to be used before their definition (Roy, Upad-\nhyay, and Roth 2016; He-Yueya et al. 2023). Therefore, the\nequation is closer to the semantics of questions than pro-\ngrams, leading to higher generation accuracy.\nDiscovery from IMR ComparisonFrom the above anal-\nysis, we can discover that increasing the number of restric-\ntions on IMRs is actually a process of screening a set. For\nexample, programs are equations with the restriction “vari-\nables must be defined before being used ”. Constant expres-\nsions are programs with the restriction “there is only one\nassignment statement with only specific values”. In the fol-\nlowing, we summarize this discovery into a proposition to\nguide the design of IMRs with high generation accuracy.\nGeneration Accuracy of Different Intermediate\nMeaning Representations\nIn the following, we prove that generating equations has\nhigher accuracy than generating programs, for which we\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19117\nDirect + Erase + Decompose\nQuestion\nAlice has twice as much candy as \nBob. David has three times as \nmany as Bob if David takes two \nmore candies, which is 18. How \nmuch candy does alice have?\nEquations\nalice_candy = 2 ✕ bob_candy\ndavid_candy = 3 ✕ bob_candy\ndavid_candy = david_candy + 2\ndavid_candy = 18\nans = alice_candy\nErased Question\nAlice has twice as much candy as \nBob. David has three times as \nmany as Bob if David takes two \nmore candies, which is 18.\nEquations\nalice_candy = 2 ✕ bob_candy\ndavid_candy + 2 = 3 ✕ bob_candy\ndavid_candy + 2 = 18\nans = alice_candy\nErased & Decomposed Question\n1. Alice has twice as much candy as Bob.\n2. David has three times as many as Bob \nif David takes two more candies.\n3. If David takes two candies, he has 18.\nEquations\nalice_candy = 2 ✕ bob_candy\ndavid_candy + 2 = 3 ✕ bob_candy\ndavid_candy + 2 = 18\nans = alice_candy\nEquations of Erased Question\nalice_candy = 2 ✕ bob_candy\ndavid_candy + 2 = 3 ✕ bob_candy\ndavid_candy + 2 = 18\nEquations of Erased Question\nalice_candy = 2 ✕ bob_candy\ndavid_candy + 2 = 3 ✕ bob_candy\ndavid_candy + 2 = 18\n1.Erase\n2.Decompose\n3.Translate\n4.Answer\nFigure 2: The illustration of B RIDGE under different settings of reasoning stages. The incorrect reasoning paths and results\nare annotated with red. The Direct method modifies the value of the constant unknown “david candy” like the program. The\nmethod that only uses Erase misses the equation “david candy + 2 = 3 × bob candy” since it pretends to translate one entire\nsentence into one single equation. The correct one is annotated with green, which decomposes the numerical reasoning of\nLLMs into four stages. (1) Erase: erase the asking part of the question; (2) Decompose: decompose the question into multiple\nsub-questions; (3) Translate: translate the sub-questions to equations; (4) Answer: generate the answer equation.\npresent a proposition that can theoretically compare the gen-\neration accuracy of different IMRs. Before presenting this\nproposition, we first propose an auxiliary proposition:\nProposition 1 Given IMRA and IMRB, let A = {all exam-\nples of IMR A} and B = {all examples of IMR B}, A ⊆ B.\nGiven one natural language question q. Let N(q, x) denote\nthe hop number (Yang et al. 2018) required to generate x\nbased on q. Then ∃b ∈ B, ∀a ∈ A, N(q, b) ≤ N(q, a),\nwhere all a, bare the IMRs of q.\nThe hop number in Proposition 1 can be regarded as a nu-\nmerical quantification of the difficulty of generating differ-\nent IMRs. Intuitively, if IMRA is a subset of IMRB, it means\nthat IMRA has more restrictions than IMR B, so more rea-\nsoning hops are needed to convert a natural language ques-\ntion to the corresponding IMRA. With Proposition 1, we can\npresent the proposition to compare the generation accuracy\nof different IMRs:\nProposition 2 Given IMRA and IMRB, A ⊆ B. Given one\nnatural language questionq. Then generating IMRB of q has\nhigher accuracy than IMRA.\nBased on Proposition 2, we can compare the generation\naccuracy of different IMRs by judging the inclusion rela-\ntionship of IMRs. Considering the discussion above, pro-\ngrams are a subset of equations, so generating equations as\nIMRs has higher accuracy than generating programs in the-\nory. However, Proposition 2 can only compare the genera-\ntion accuracy from the view of different IMRs themselves.\nApart from the type of IMR, the generation accuracy also de-\npends on many other factors, such as the model architecture\nand pre-training data. For example, although using equations\nas IMR has higher accuracy than constant expressions ac-\ncording to Proposition 2, models pre-trained with constant\nexpressions have higher accuracy in generating expressions\nthan generating equations. Therefore, in order to empirically\nenhance the ability of LLMs to generate equations, we need\nto design specific methods that boost the generation capabil-\nities of LLMs for equations outside of constant expressions\nand programs.\nPipeline of BRIDGE\nIn this section, we introduce the pipeline of BRIDGE , which\ndecomposes the numerical reasoning into four stages. The\nillustration of BRIDGE is shown in Figure 2.\nStage1: Erase The previous research has shown that the\ncurrent NLP models mainly learn the mapping between in-\nput and output formats rather than specific NLP capabili-\nties (McCoy, Pavlick, and Linzen 2019; Jawahar, Sagot, and\nSeddah 2019; Bubeck et al. 2023). So during the few-shot\ninference, current LLMs are more inclined to generate con-\nstant expressions and programs since we assume that these\nIMRs are mainly contained in the pre-training data (Brown\net al. 2020; Chen et al. 2021a). However, since the pre-\ntraining data is not available as an open resource, we have\nbeen unable to validate this assumption thus far. To enhance\nthe tendency of current LLMs to generate equations, we\nshould disrupt the input format of numerical reasoning ques-\ntions that LLMs have seen in the pre-training data\nWe observe that even if the asking part is erased, the re-\nmaining part can still be expressed as solvable equations.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19118\nAlgorithm 1: Generate Answer Equation\nInput: Natural language question question, translated\nequations equations, number of retries limit retry.\nOutput: Equation of the answer.\n1: temperature = 0\n2: for i = 1 to retry do\n3: ans = answer(question, equations, temperature)\n4: if is solvable(equations, ans) then\n5: return ans\n6: else\n7: temperature += 0.1\n8: end if\n9: end for\n10: return None\nFor example, about the question “Alice has twice as much\ncandy as Bob. How much candy does Alice have if Bob has\n12 candies”, after erasing the asking part “How much candy\ndoes Alice have”, the rest part can still be listed as equations\n“alice\ncandy = 2×bob candy, bob candy = 12”. To guide\nthe LLMs in generating equations, we erase the asking part\nof each question. This disrupts the input format, as observed\nin the pre-training data, and reduces the tendency to generate\nconstant expressions or programs.\nStage2: Decompose During generating equations based\non questions, LLMs may translate one entire sentence into\none single equation, resulting in missing intermediate infor-\nmation and unsolvable equations. Take the results only with\nErase stage as an example in Figure 2, LLMs directly trans-\nlate “David has three times as many as Bob if David takes\ntwo more candies, which is 18” into “david\ncandy + 2 =\n18” while ignoring the information “David has three times\nas many as Bob if David takes two more candies”. To ad-\ndress this issue, we try to decompose the question into sub-\nequations before generating the equations. With the decom-\nposed sub-questions, LLMs are able to generate equations\nbased on finer-grained information, thus alleviating the phe-\nnomenon of missing information.\nStage3: Translate In this stage, we use both the erased\nquestion and the decomposed sub-questions as the input to\ngenerate the corresponding equations with LLMs, which\ncomplement each other for the generation of the complete\nequations. However, the Translation stage may generate un-\nsolvable equations where there is no solution for the equa-\ntions. This makes it to be unable to get the value of each\nquantity to calculate the answer in the next stage. To address\nthis problem, if the Translation stage generates unsolvable\nequations, we set the result to be empty and let the next stage\ngenerate all equations from scratch.\nStage4: Answer The Translation stage currently com-\nputes the value of each quantity separately, whereas we need\nto collectively compute them together, which is done by\nequation solving and post-processing 1. Concretely, in this\n1For example, if the answer should be present with a percent-\nage, the result value needs to be multiplied by 100.\nstage, we use the original questions and the translated equa-\ntions as input and output the final answer equation. The pro-\ncess of this stage is shown in Algorithm 1. Since the trans-\nlated equations themselves have certain semantic informa-\ntion, LLMs can understand the meaning of each unknown\nand use them to represent the answer equation. If the equa-\ntions are unsolvable, we can not get the answer. So we let\nthe LLMs regenerate the results if the generated equations\nare unsolvable and gradually increase the generation tem-\nperature until the results are solvable.\nExperiments\nIn this section, we adopt experiments to verify the effective-\nness of our method. First, we give the setup information of\nour experiments. Then, we present the main experiment re-\nsults to demonstrate that our method can improve the nu-\nmerical reasoning ability of LLMs. After that, to prove the\neffectiveness of each stage of BRIDGE , we give the ablation\nexperiments of each stage. Finally, we analyze the experi-\nment results of BRIDGE to shed light on future research.\nExperiment Setup\nDataset We adopt B RIDGE to GSM8K (Cobbe et al.\n2021), SV AMP (Patel, Bhattamishra, and Goyal 2021),\nand Algebra (He-Yueya et al. 2023), which are widely\nused numerical reasoning datasets. The question number of\nGSM8K, SV AMP, and Algebra is1319, 1000, and 222, re-\nspectively. GSM8K consists of grade school math questions,\nwhich require 2-8 steps of reasoning and use basic arithmetic\noperations (+−×÷) to get answers. SV AMP contains1, 000\nmath questions selected from the existing numerical reason-\ning datasets, using basic arithmetic operations to solve. Dif-\nferent from the above datasets, Algebra is a dataset contain-\ning more algebra questions, which can reflect the ability of\nthe model to translate the questions into the corresponding\nalgebraic equations.\nMetric We use the exact match (EM) as the evaluation\nmetric of our work. Because of the round-off error, follow-\ning the previous work (Chen et al. 2022; Gao et al. 2022),\nwe consider the prediction is equal to the ground truth if\ntheir relative difference is below 10−3.\nModel We use Codex (Chen et al. 2021a) and GPT3.5 2\nas our experimental LLMs, which belong to the most widely\nused LLMs. Codex is an advanced model that is capable of\ntranslating natural language instructions into code across a\nvariety of programming languages. GPT3.5 is the model im-\nproved on GPT-3 (Brown et al. 2020) and can handle both\nnatural language and code.\nImplement Detail For the equation generation, we use\nthe Azure OpenAI API of code-davinci-002 and\ngpt-3.5-turbo for our experiments 3. We use Codex\nto denote code-davinci-002 and GPT3.5 to denote\ngpt-3.5-turbo for brief in the following. We use 5-8\n2https://platform.openai.com/docs/models/gpt-3-5\n3https://azure.microsoft.com/en-us/products/cognitive-\nservices/openai-service\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19119\nshots for all the experimental datasets with different hard-\nness. For equation solving, we employ sympy (Meurer et al.\n2017) to solve generated equations, which is a Python pack-\nage for symbolic mathematics.\nMain Result\nThe main experiment results are shown in Table 1. We can\nsee that B RIDGE brings robust performance improvement,\nleading to new SOTA results on all datasets and models un-\nder the single reasoning path setting, which proves the effec-\ntiveness of our method.\nGSM8K Compared with the results of the previous SOTA\nmethods, BRIDGE consistently surpasses the previous meth-\nods using programs as IMRs (PoT, PAL), empirically prov-\ning that equations are a better IMR than programs and the\ncorrectness of Proposition 2 to a certain extent.\nSV AMP BRIDGE also achieves the new SOTA result with\n0.9% improvement. Notably, we can observe that the perfor-\nmance boost brought by B RIDGE in SV AMP is not as valid\nas GSM8K. This is because the semantics of bad cases in\nprevious methods are complicated, requiring enhancing the\nunderstanding ability of LLMs to relationships of quantities\nrather than simply changing IMRs.\nAlgebra BRIDGE brings 1.7% improvement compared\nwith the previous SOTA result. However, our method is less\nrobust compared with other datasets, with a fluctuation of\nmore than 1.7%. It is because Algebra is smaller than the\nother two datasets with only222 questions, resulting in more\nobvious performance fluctuations.\nOur method can also apply the self-consistency\nmethod (Wang et al. 2023) to enhance the performance.\nTo prove this point, we evaluate B RIDGE on GSM8K\nwith self-consistency, which is shown in Table 3. The\nresult shows that self-consistency also brings significant\nimprovement to our method.\nAblation Study\nAnswer Generation We adopt the ablation study to verify\nthe stage effectiveness of B RIDGE . The experiment result\nis shown in Table 4, from which we can see that: (1) The\nErase and Decompose stages can bring improvement to all\ndatasets, proving the effectiveness of these two stages; (2)\nCompared with Erase, using Decompose can bring a more\nsignificant improvement on GSM8K and SV AMP since De-\ncompose is also effective for questions that do not require\nequation solving, while Erase is mainly designed for ques-\ntions using equations; (3) Compared with GSM8K, the im-\nprovement in SV AMP is less obvious because the questions\nof SV AMP are much simpler than GSM8K, and most ques-\ntions not solved by previous methods require enhancing abil-\nities other than numerical reasoning, so the improvement\nbrought by our method is not significant.\nEquation Generation To verify whether the Erase stage\ncan improve the tendency of LLMs to generate equations,\nwe count the number of generated equations in the results\nwith and without Erase. The experiment results are shown\n2-shots 4-shots 6-shots 8-shots\n72.3 72 73.7 75\n70.1\n72.9 74.6 75.8\n71.6 71.6\n74.3\n77.6\nFigure 3: The exact match of inference with different shots\non GSM8K run three times using gpt-3.5-turbo. Dif-\nferent color denotes the result of the different run.\nin Table 2. From the results, we can see that: (1) On each\ndataset, the numbers of results using equations on both cor-\nrect and total cases have increased, proving that the Erase\nstage indeed improves the tendency of LLMs to generate\nequations; (2) Compared with GSM8K, the improvement of\nthe Erase stage in Algebra is not obvious because Algebra\nmainly includes algebra questions, which can guide to gen-\nerate algebra equations by questions themselves.\nAnalysis\nSensitivity to Exemplars To study the impact of different\nprompts on BRIDGE , we write a total of15 samples and ran-\ndomly select {2, 4, 6, 8} shots from them as prompts to run\nthree times. The experiment results are shown in Figure 3.\nFrom the results, we can see that as the number of shots in\nthe prompt increases, the performance of our method grad-\nually improves. Besides, as the number of shots increases,\nthe variance of the results is gradually decreasing, indicat-\ning that the robustness is also gradually improving.\nEquation Proportion To evaluate whether B RIDGE en-\nhances the numerical reasoning ability of LLMs by gen-\nerating equations, we select the cases where PoT is incor-\nrect while our method is correct and see if each case gener-\nates equations as IMRs. The proportions of generating equa-\ntions of different methods and datasets are shown in Table 5.\nBased on the results, we can see that: (1) On all models\nand all datasets, the performance improvement has equa-\ntions involved, and it is most significant in Algebra, where\nabout half of the improvement uses equations; (2) The per-\nformance improvement is not all brought by using equations\nas IMRs because the results of LLMs are not robust, and\nmany questions in the part that does not use equations may\nbe solved in the results of other runs of PoT; (3) The im-\nprovement brought by using equations on SV AMP is not\nsignificant because the questions of this dataset are relatively\nsimple, and most of them can be solved without equations.\nReasoning Complex We analyze the effectiveness of our\nmethod of solving the questions with different complexes,\nwhich are categorized by the number of statements or equa-\ntions of answers. The performance improvement of our\nmethod compared with PoT is shown in Table 6, which\nshows that: (1) Our method can bring performance im-\nprovement of questions under different complexes, espe-\ncially on the Algebra dataset, improvement is more than\n30%, which proves the effectiveness of our method under\ndifferent complexes; (2) On the GSM8K dataset, the im-\nprovement of more complex questions is not obvious, show-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19120\nModel Method GSM8K SV AMP Algebra\nCodex\nCoT (Wei et al. 2022) 65.6 74 .8 47 .9\nTab-CoT (Jin and Lu 2023) 61.6 82 .9 −\nDeclarative (He-Yueya et al. 2023) 69.4 − 76.3\nPoT (Chen et al. 2022) 71.6 85 .2 −\nPAL (Gao et al. 2022) 72.0 79 .4 56 .2\nBRIDGE 74.2 ±0.4 86 .1 ±0.5 78 .5 ±1.7\nGPT3.5\nCoT (Our runs) 76.5 80.8 53.6\nPoT (Our runs) 74.8 79.3 64.0\nBRIDGE 77.2 ±0.4 82 .3 ±0.6 82 .0 ±0.9\nTable 1: The exact match under different datasets and models with different prompt methods. The results of our method are\naveraged over five runs. The best results of different datasets are annotated in bold.\nModel Method GSM8K SV AMP Algebra\nC T C T C T\nCodex\nBRIDGE 83 108 49 57 68 80\n- Erase 61 89 44 52 65 77\n∆(%) 36 .1 21 .3 11 .4 9 .6 4 .6 3 .9\nGPT3.5\nBRIDGE 93 139 56 65 77 86\n- Erase 65 96 43 51 75 81\n∆(%) 43 .1 44 .8 30 .2 27 .5 2 .7 6 .2\nTable 2: The equations generated with and without Erase\nstage. C denotes the correct cases, and T denotes the total\ncases. ∆ denotes the ratio of the equations increased after\nusing Erase to that without Erase.\nMethod GSM8K\nCoT w. Self-Consistency 82.0\nPoT w. Self-Consistency 77.4\nBRIDGE 77.2\nw. Self-Consistency 83.6 (+6.4)\nTable 3: The exact match ofgpt-3.5-turbo with differ-\nent prompt methods. The best result is annotated with bold.\ning that B RIDGE brings relatively small performance im-\nprovement when dealing with more complex questions.\nError Type In order to investigate the main sources of er-\nror in B RIDGE , we count the number of bad cases where\nerrors occurred in different stages. We randomly select one\nhundred bad cases of GSM8K and SV AMP, then manually\nclassify them according to the stages where the error oc-\ncurred. The results are shown in Figure 4.\nBased on the results, we can draw the conclusion that:\n(1) There are very few cases of LLMs making mistakes in\nthe Erase and Decompose stages because LLMs in these\ntwo stages mainly rely on understanding the meaning of\nthe question and do not need reasoning, so they have bet-\nter performance; (2) The errors mainly concentrate in the\nTranslate and Answer stages, which proves that the ability of\nModel Method GSM8K SV AMP Algebra\nCodex\nBRIDGE 74.2 86 .1 78 .5\n- Erase 69.0 86 .0 72 .1\n- Decompose 68.2 85 .6 77 .8\nGPT3.5\nBRIDGE 77.0 82 .5 82 .9\n- Erase 76.1 81 .9 81 .5\n- Decompose 74.2 81 .9 79 .7\nTable 4: The ablation experiment results in Erase and De-\ncompose stages on GSM8K and SV AMP of different mod-\nels. The first line of each model denotes using BRIDGE .\nModel GSM8K SV AMP Algebra\nCodex 20.8% 15 .3% 47 .8%\nGPT3.5 41.7% 10 .0% 59 .6%\nTable 5: Among the cases where PoT is incorrect while\nBRIDGE is correct, the proportion of cases using equations.\ncurrent LLMs to translate natural language into equations is\nstill weak because LLMs still confuse the semantics between\nquestions and equations after erasing and decomposing; (3)\nThe main error types of GSM8K and SV AMP are not con-\nsistent because the relationship of quantities in the questions\nof GSM8K is more complicated and hard to be translated\ninto equations, while the main difficulty of SV AMP lies in\ndetermining which quantity is asked by the question.\nCase Study To better understand how B RIDGE improves\nthe numerical reasoning performance of LLMs, we present\na case study in Figure 5. Since the program needs to define\nvariables before use, when dealing with the question in Fig-\nure 5, it is necessary first to understand the relationships of\ndifferent quantities in the question, then adjust the order of\neach sub-question in the result, which is with low genera-\ntion accuracy. While B RIDGE uses equations as IMRs, the\ngenerated results correspond to each sentence of the original\nquestion in sequence, and the difficulty of generation is low,\nleading to the correct result.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19121\n#Steps GSM8K Algebra\nPoT B RIDGE ∆ PoT B RIDGE ∆\n≤ 4 71 .1 78 .6 7 .5 49 .5 80 .5 31 .0\n[5, 6] 71 .6 78 .3 6 .7 20 .0 60 .0 40 .0\n≥ 7 68 .5 70 .0 1 .5 5 .9 58 .8 52 .9\nTable 6: The exact match of questions with different equa-\ntions steps of code-davinci-002. #Steps denotes the\nnumber of answer equations.\n2-shots 4-shots 6-shots 8-shots\n72.3 72\n73.7 75\n70.1\n72.9\n74.6 75.8\n71.6 71.6\n74.3\n77.6\nErase Decompose Translate Answer\n0\n20\n40\n60\n80\n#Cases\nGSM8K\nSVAMP\nFigure 4: The number of bad cases in different stages of\nGSM8K and SV AMP ofcode-davinci-002.\nRelated Work\nNumerical Reasoning with LLMs\nThe ability to perform numerical reasoning is essential for\nNLP models as it enables them to understand and manip-\nulate numerical information embedded in natural language\n(Lu et al. 2023). Utilizing LLMs for numerical reasoning\ntasks has become the mainstream in current research due\nto their brilliant few-shot inference ability without training\n(Wei et al. 2022; Chen et al. 2022; Xie et al. 2023).\nEarly researches guide LLMs to generate answers and the\nreasoning process in one step (Jie and Lu 2023; Liu and\nLow 2023; Imani, Du, and Shrivastava 2023). For example,\nCoT (Wei et al. 2022) asks LLMs to generate the reason-\ning process with natural language to get the answer. As the\npre-training data of LLMs include numerous programs, PoT\n(Chen et al. 2022) and PAL (Gao et al. 2022) ask the model\nto generate programs to solve numerical questions. Follow-\ning the same reason, Tab-CoT (Jin and Lu 2023) asks the\nmodel to generate tables to solve this task, as a large amount\nof tabular data is used during the pre-training of LLMs.\nBesides, many works also try to decompose the process\nof numerical reasoning into multiple steps to reduce the\ndifficulty of model inference (Gaur and Saunshi 2023; Li\net al. 2023; Wang, Zhang, and Wang 2023). For example,\nPHP (Zheng et al. 2023) generates answers through multi-\nturn inferences, and each turn can use the answers of the\nprevious turns as a hint. Decomposition (Xie et al. 2023)\ngenerates multiple candidate results based on the previous\nsteps of each reasoning step (e.g., one program statement)\nand keeps the best k of them like beam-search.\nHowever, due to the high proportion of constant expres-\nsions and programs in the pre-training data, LLMs prefer\nQuestion\nCaroline is three times older than Ben. Ben is two times older than Chris. If Chris is \n4, how old is Caroline?\nCaroline is three times older than Ben. \nBen is two times older than Chris.\nIf Chris is 4, how old is Caroline?\nProgram\nchris_age = 4\nben_age = chris_age / 2\ncaroline_age = ben_age * 3\nans = caroline_age\nEquation\ncaroline_age = 3 * ben_age\nben_age = 2 * chris_age\nchris_age = 4\nans = caroline_age\nCaroline is three times older than Ben. \nBen is two times older than Chris.\nIf Chris is 4, how old is Caroline?\nFigure 5: The results of PoT and B RIDGE of a case in\nGSM8K. The above part is the result of PoT, and the below\npart is BRIDGE . The error steps and results are annotated in\nred, and the correct results are annotated in green.\nto generate these two types of IMRs during few-shot infer-\nence. These may limit the numerical reasoning capability of\nLLMs since these IMRs may not be the best format for solv-\ning the numerical reasoning task. To overcome this problem,\nwe present BRIDGE , which erases asking parts in questions.\nAfter erasing, we disrupt the input structure that LLMs have\nseen in the pre-training data, and LLMs are less likely to\ngenerate constant expressions or programs.\nReasoning with Intermediate Meaning\nRepresentation\nIn the field of NLP, a common way to reduce the difficulty\nof reasoning is to generate answers with an intermediate\nmeaning representation (IMR) (Gan et al. 2021; Nie et al.\n2022; Paul et al. 2023). Such methods first generate IMRs\nof questions and then use external tools (e.g., algorithms,\ninterpreters) to generate the answer results based on IMRs.\nSince generating the answer from IMRs is deterministic, the\nmain bottleneck is how to generate IMRs of questions.\nThe most widely studied IMRs are the semantic repre-\nsentation language (Kamath and Das 2019). Previous works\nhave designed many semantic representation languages to\nrepresent natural language sentences, such as AST (Jones\n2003) and AMR (Banarescu et al. 2013). By converting\ninto these languages, then applying the converted results on\ndownstream tasks, the semantic understanding ability of the\nmodel can be effectively improved (Che et al. 2021).\nMany methods also employ IMRs to solve the numerical\nreasoning task (Wang, Zhang, and Wang 2023; Paul et al.\n2023). A commonly used IMR is constant expressions (Roy\nand Roth 2015; Koncel-Kedziorski et al. 2016). The cur-\nrent SOTA methods use programs as IMRs because pro-\ngrams have closer semantics to questions than constant ex-\npressions, and current LLMs have strong program genera-\ntion capabilities (Chen et al. 2021a, 2022; Gao et al. 2022).\nBesides, there are many other types of IMRs, such as dol-\nphin languages (Huang et al. 2018), domain-specific lan-\nguages (Chen et al. 2021b) and equations (Roy, Upadhyay,\nand Roth 2016; He-Yueya et al. 2023).\nHowever, most methods design IMRs based on expert ex-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19122\nperience or experimental results, affected by factors other\nthan IMRs themselves, such as model structure and train-\ning data. To theoretically compare the generation of differ-\nent IMRs, in this paper, we propose and prove a proposition\nto guide the design of IMRs with high generation accuracy.\nConclusion\nIn this paper, we employ equations as IMRs to solve the\nnumerical reasoning task by addressing two problems: (1)\nTheoretically, how to prove that the equation is an IMR with\nhigher generation accuracy than programs; (2) Empirically,\nhow to improve the generation accuracy of equations with\nLLMs. For the first problem, we present and prove a propo-\nsition to compare the generation accuracy of different IMRs\nin theory. For the second problem, we present B RIDGE to\nenhance the equation generation of LLMs by reducing the\ntendency of generating constant expressions and programs\nand decomposing questions. To evaluate B RIDGE , we con-\nduct experiments across three datasets: GSM8K, SV AMP,\nand Algebra. Compared to the previous SOTA results, our\nmethod has increased average performance by 1.6%, setting\nnew SOTA performance across all the datasets under the sin-\ngle reasoning path setting. Moreover, ablation experiments\nshow that using BRIDGE can enhance the tendency of LLMs\nto generate equations as IMRs, proving that our method can\nimprove the ability of LLMs to generate IMRs outside of\nconstant expressions and programs.\nAcknowledgments\nWe thank all anonymous reviewers for their constructive\ncomments. We gratefully acknowledge the support of the\nNational Natural Science Foundation of China (NSFC) via\ngrant 62236004 and 62206078, and the support of Du Xiao-\nman (Beijing) Science Technology Co., Ltd.\nReferences\nBanarescu, L.; Bonial, C.; Cai, S.; Georgescu, M.; Griffitt,\nK.; Hermjakob, U.; Knight, K.; Koehn, P.; Palmer, M.; and\nSchneider, N. 2013. Abstract Meaning Representation for\nSembanking. In Proceedings of the 7th Linguistic Anno-\ntation Workshop and Interoperability with Discourse, 178–\n186. Sofia, Bulgaria: Association for Computational Lin-\nguistics.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Mod-\nels are Few-Shot Learners. In Larochelle, H.; Ranzato, M.;\nHadsell, R.; Balcan, M.; and Lin, H., eds.,Advances in Neu-\nral Information Processing Systems, volume 33, 1877–1901.\nCurran Associates, Inc.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,\nS.; Nori, H.; Palangi, H.; Ribeiro, M. T.; and Zhang, Y . 2023.\nSparks of Artificial General Intelligence: Early experiments\nwith GPT-4. arXiv:2303.12712.\nChe, W.; Feng, Y .; Qin, L.; and Liu, T. 2021. N-LTP:\nAn Open-source Neural Language Technology Platform for\nChinese. In Proceedings of the 2021 Conference on Em-\npirical Methods in Natural Language Processing: System\nDemonstrations, 42–49. Online and Punta Cana, Dominican\nRepublic: Association for Computational Linguistics.\nChen, M.; Tworek, J.; Jun, H.; Yuan, Q.; de Oliveira Pinto,\nH. P.; Kaplan, J.; Edwards, H.; Burda, Y .; Joseph, N.; Brock-\nman, G.; Ray, A.; Puri, R.; Krueger, G.; Petrov, M.; Khlaaf,\nH.; Sastry, G.; Mishkin, P.; Chan, B.; Gray, S.; Ryder, N.;\nPavlov, M.; Power, A.; Kaiser, L.; Bavarian, M.; Winter, C.;\nTillet, P.; Such, F. P.; Cummings, D.; Plappert, M.; Chantzis,\nF.; Barnes, E.; Herbert-V oss, A.; Guss, W. H.; Nichol, A.;\nPaino, A.; Tezak, N.; Tang, J.; Babuschkin, I.; Balaji, S.;\nJain, S.; Saunders, W.; Hesse, C.; Carr, A. N.; Leike, J.;\nAchiam, J.; Misra, V .; Morikawa, E.; Radford, A.; Knight,\nM.; Brundage, M.; Murati, M.; Mayer, K.; Welinder, P.; Mc-\nGrew, B.; Amodei, D.; McCandlish, S.; Sutskever, I.; and\nZaremba, W. 2021a. Evaluating Large Language Models\nTrained on Code. arXiv:2107.03374.\nChen, W.; Ma, X.; Wang, X.; and Cohen, W. W. 2022.\nProgram of Thoughts Prompting: Disentangling Compu-\ntation from Reasoning for Numerical Reasoning Tasks.\narXiv:2211.12588.\nChen, W.; Yin, M.; Ku, M.; Lu, P.; Wan, Y .; Ma, X.; Xu,\nJ.; Wang, X.; and Xia, T. 2023. TheoremQA: A Theorem-\ndriven Question Answering dataset. arXiv:2305.12524.\nChen, Z.; Chen, W.; Smiley, C.; Shah, S.; Borova, I.; Lang-\ndon, D.; Moussa, R.; Beane, M.; Huang, T.-H.; Routledge,\nB.; and Wang, W. Y . 2021b. FinQA: A Dataset of Nu-\nmerical Reasoning over Financial Data. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natural Lan-\nguage Processing, 3697–3711. Online and Punta Cana, Do-\nminican Republic: Association for Computational Linguis-\ntics.\nCobbe, K.; Kosaraju, V .; Bavarian, M.; Chen, M.; Jun, H.;\nKaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.;\nHesse, C.; and Schulman, J. 2021. Training Verifiers to\nSolve Math Word Problems. arXiv:2110.14168.\nGan, Y .; Chen, X.; Xie, J.; Purver, M.; Woodward, J. R.;\nDrake, J.; and Zhang, Q. 2021. Natural SQL: Making SQL\nEasier to Infer from Natural Language Specifications. In\nFindings of the Association for Computational Linguistics:\nEMNLP 2021, 2030–2042. Punta Cana, Dominican Repub-\nlic: Association for Computational Linguistics.\nGao, L.; Madaan, A.; Zhou, S.; Alon, U.; Liu, P.; Yang, Y .;\nCallan, J.; and Neubig, G. 2022. PAL: Program-aided Lan-\nguage Models. arXiv preprint arXiv:2211.10435.\nGaur, V .; and Saunshi, N. 2023. Reasoning in Large Lan-\nguage Models Through Symbolic Math Word Problems. In\nFindings of the Association for Computational Linguistics:\nACL 2023, 5889–5903. Toronto, Canada: Association for\nComputational Linguistics.\nHe-Yueya, J.; Poesia, G.; Wang, R. E.; and Goodman, N. D.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19123\n2023. Solving Math Word Problems by Combining Lan-\nguage Models With Symbolic Solvers. arXiv:2304.09102.\nHuang, D.; Yao, J.-G.; Lin, C.-Y .; Zhou, Q.; and Yin, J.\n2018. Using Intermediate Representations to Solve Math\nWord Problems. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume 1:\nLong Papers), 419–428. Melbourne, Australia: Association\nfor Computational Linguistics.\nImani, S.; Du, L.; and Shrivastava, H. 2023. MathPrompter:\nMathematical Reasoning using Large Language Models.\nIn Proceedings of the 61st Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 5: Industry\nTrack), 37–42. Toronto, Canada: Association for Computa-\ntional Linguistics.\nJawahar, G.; Sagot, B.; and Seddah, D. 2019. What Does\nBERT Learn about the Structure of Language? In Proceed-\nings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, 3651–3657. Florence, Italy: Associ-\nation for Computational Linguistics.\nJie, Z.; and Lu, W. 2023. Leveraging Training Data in Few-\nShot Prompting for Numerical Reasoning. In Findings of\nthe Association for Computational Linguistics: ACL 2023 ,\n10518–10526. Toronto, Canada: Association for Computa-\ntional Linguistics.\nJin, Z.; and Lu, W. 2023. Tab-CoT: Zero-shot Tabular Chain\nof Thought. arXiv:2305.17812.\nJones, J. 2003. Abstract Syntax Tree Implementation Id-\nioms. Pattern Languages of Program Design. Proceedings\nof the 10th Conference on Pattern Languages of Programs\n(PLoP2003) http://hillside.net/plop/plop2003/papers.html.\nKamath, A.; and Das, R. 2019. A Survey on Semantic Pars-\ning. In Automated Knowledge Base Construction (AKBC).\nKoncel-Kedziorski, R.; Roy, S.; Amini, A.; Kushman, N.;\nand Hajishirzi, H. 2016. MAWPS: A Math Word Problem\nRepository. In Proceedings of the 2016 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, 1152–\n1157. San Diego, California: Association for Computational\nLinguistics.\nLi, Y .; Lin, Z.; Zhang, S.; Fu, Q.; Chen, B.; Lou, J.-G.; and\nChen, W. 2023. Making Language Models Better Reasoners\nwith Step-Aware Verifier. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), 5315–5333. Toronto, Canada:\nAssociation for Computational Linguistics.\nLi, Z.; Guo, J.; Liu, Q.; Lou, J.-G.; and Xie, T. 2022. Ex-\nploring the Secrets Behind the Learning Difficulty of Mean-\ning Representations for Semantic Parsing. In Proceedings\nof the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, 3616–3625. Abu Dhabi, United Arab\nEmirates: Association for Computational Linguistics.\nLiu, T.; and Low, B. K. H. 2023. Goat: Fine-\ntuned LLaMA Outperforms GPT-4 on Arithmetic Tasks.\narXiv:2305.14201.\nLu, P.; Qiu, L.; Yu, W.; Welleck, S.; and Chang, K.-W. 2023.\nA Survey of Deep Learning for Mathematical Reasoning.\nIn Proceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\n14605–14631. Toronto, Canada: Association for Computa-\ntional Linguistics.\nMcCoy, T.; Pavlick, E.; and Linzen, T. 2019. Right for the\nWrong Reasons: Diagnosing Syntactic Heuristics in Natu-\nral Language Inference. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics ,\n3428–3448. Florence, Italy: Association for Computational\nLinguistics.\nMeurer, A.; Smith, C. P.; Paprocki, M.; ˇCert´ık, O.; Kir-\npichev, S. B.; Rocklin, M.; Kumar, A.; Ivanov, S.; Moore,\nJ. K.; Singh, S.; Rathnayake, T.; Vig, S.; Granger, B. E.;\nMuller, R. P.; Bonazzi, F.; Gupta, H.; Vats, S.; Johansson,\nF.; Pedregosa, F.; Curry, M. J.; Terrel, A. R.; Rouˇcka, v.; Sa-\nboo, A.; Fernando, I.; Kulal, S.; Cimrman, R.; and Scopatz,\nA. 2017. SymPy: symbolic computing in Python. PeerJ\nComputer Science, 3: e103.\nNie, L.; Cao, S.; Shi, J.; Sun, J.; Tian, Q.; Hou, L.; Li, J.; and\nZhai, J. 2022. GraphQ IR: Unifying the Semantic Parsing of\nGraph Query Languages with One Intermediate Represen-\ntation. In Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, 5848–5865.\nAbu Dhabi, United Arab Emirates: Association for Compu-\ntational Linguistics.\nPatel, A.; Bhattamishra, S.; and Goyal, N. 2021. Are NLP\nModels really able to Solve Simple Math Word Problems? In\nProceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 2080–2094. Online: Asso-\nciation for Computational Linguistics.\nPaul, D.; Ismayilzada, M.; Peyrard, M.; Borges, B.;\nBosselut, A.; West, R.; and Faltings, B. 2023. RE-\nFINER: Reasoning Feedback on Intermediate Representa-\ntions. arXiv:2304.01904.\nRoy, S.; and Roth, D. 2015. Solving General Arithmetic\nWord Problems. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing, 1743–\n1752. Lisbon, Portugal: Association for Computational Lin-\nguistics.\nRoy, S.; Upadhyay, S.; and Roth, D. 2016. Equation Pars-\ning : Mapping Sentences to Grounded Equations. In Pro-\nceedings of the 2016 Conference on Empirical Methods in\nNatural Language Processing, 1088–1097. Austin, Texas:\nAssociation for Computational Linguistics.\nThawani, A.; Pujara, J.; Ilievski, F.; and Szekely, P. 2021.\nRepresenting Numbers in NLP: a Survey and a Vision. In\nProceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 644–656. Online: Associa-\ntion for Computational Linguistics.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q. V .; Chi, E. H.;\nNarang, S.; Chowdhery, A.; and Zhou, D. 2023. Self-\nConsistency Improves Chain of Thought Reasoning in Lan-\nguage Models. In The Eleventh International Conference on\nLearning Representations.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19124\nWang, Y .; Zhang, Z.; and Wang, R. 2023. Meta-Reasoning:\nSemantics-Symbol Deconstruction For Large Language\nModels. arXiv:2306.17820.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; brian ichter;\nXia, F.; Chi, E. H.; Le, Q. V .; and Zhou, D. 2022. Chain\nof Thought Prompting Elicits Reasoning in Large Language\nModels. In Oh, A. H.; Agarwal, A.; Belgrave, D.; and Cho,\nK., eds., Advances in Neural Information Processing Sys-\ntems.\nXie, Y .; Kawaguchi, K.; Zhao, Y .; Zhao, X.; Kan, M.-Y .; He,\nJ.; and Xie, Q. 2023. Decomposition Enhances Reasoning\nvia Self-Evaluation Guided Decoding. arXiv:2305.00633.\nYang, Z.; Qi, P.; Zhang, S.; Bengio, Y .; Cohen, W.; Salakhut-\ndinov, R.; and Manning, C. D. 2018. HotpotQA: A Dataset\nfor Diverse, Explainable Multi-hop Question Answering. In\nProceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, 2369–2380. Brussels, Bel-\ngium: Association for Computational Linguistics.\nZhang, J.; Wang, L.; Lee, R. K.-W.; Bin, Y .; Wang, Y .; Shao,\nJ.; and Lim, E.-P. 2020. Graph-to-Tree Learning for Solving\nMath Word Problems. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics ,\n3928–3937. Online: Association for Computational Linguis-\ntics.\nZheng, C.; Liu, Z.; Xie, E.; Li, Z.; and Li, Y . 2023.\nProgressive-Hint Prompting Improves Reasoning in Large\nLanguage Models. arXiv:2304.09797.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19125",
  "topic": "Meaning (existential)",
  "concepts": [
    {
      "name": "Meaning (existential)",
      "score": 0.775081992149353
    },
    {
      "name": "Representation (politics)",
      "score": 0.70147705078125
    },
    {
      "name": "Linguistics",
      "score": 0.5076762437820435
    },
    {
      "name": "Computer science",
      "score": 0.4465828537940979
    },
    {
      "name": "Cognitive science",
      "score": 0.39860349893569946
    },
    {
      "name": "Epistemology",
      "score": 0.38193005323410034
    },
    {
      "name": "Psychology",
      "score": 0.37405312061309814
    },
    {
      "name": "Philosophy",
      "score": 0.19551411271095276
    },
    {
      "name": "Political science",
      "score": 0.07249787449836731
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    }
  ]
}