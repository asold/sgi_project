{
  "title": "Backward and Forward Language Modeling for Constrained Sentence Generation",
  "url": "https://openalex.org/W2340329281",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2576690091",
      "name": "Mou, Lili",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108241421",
      "name": "Yan Rui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2031667691",
      "name": "Li, Ge",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1989596241",
      "name": "Zhang Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2016811418",
      "name": "Jin Zhi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2963371736",
    "https://openalex.org/W2951176429",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2184587602",
    "https://openalex.org/W2963546833",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2165560648",
    "https://openalex.org/W1784932861",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2964217331",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1951216520",
    "https://openalex.org/W196214544",
    "https://openalex.org/W1847211030",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2158195707"
  ],
  "abstract": "Recent language models, especially those based on recurrent neural networks (RNNs), make it possible to generate natural language from a learned probability. Language generation has wide applications including machine translation, summarization, question answering, conversation systems, etc. Existing methods typically learn a joint probability of words conditioned on additional information, which is (either statically or dynamically) fed to RNN's hidden layer. In many applications, we are likely to impose hard constraints on the generated texts, i.e., a particular word must appear in the sentence. Unfortunately, existing approaches could not solve this problem. In this paper, we propose a novel backward and forward language model. Provided a specific word, we use RNNs to generate previous words and future words, either simultaneously or asynchronously, resulting in two model variants. In this way, the given word could appear at any position in the sentence. Experimental results show that the generated texts are comparable to sequential LMs in quality.",
  "full_text": "Backward and Forward Language Modeling for Constrained\nSentence Generation\nLili Mou,1 Rui Yan,2 Ge Li,1 Lu zhang,1 Zhi Jin1\n1Software Institute, Peking University\nKey Laboratory of High Conﬁdence Software Technologies (Peking University),\nMinistry of Education, China\n2Baidu Inc.\n{doublepower.mou,rui.yan.peking}@gmail.com\n{lige,zhanglu,zhijin}@sei.pku.edu.cn\nAbstract\nRecent language models, especially those based on recurrent neural networks (RNNs),\nmake it possible to generate natural language from a learned probability. Language gen-\neration has wide applications including machine translation, summarization, question an-\nswering, conversation systems, etc. Existing methods typically learn a joint probability of\nwords conditioned on additional information, which is (either statically or dynamically)\nfed to RNN’s hidden layer. In many applications, we are likely to impose hard constraints\non the generated texts, i.e., a particular word must appear in the sentence. Unfortunately,\nexisting approaches could not solve this problem. In this paper, we propose a novel back-\nward and forward language model. Provided a speciﬁc word, we use RNNs to generate\nprevious words and future words, either simultaneously or asynchronously, resulting in two\nmodel variants. In this way, the given word could appear at any position in the sentence.\nExperimental results show that the generated texts are comparable to sequential LMs in\nquality.\n1 Introduction\nLanguage modeling is aimed at minimizing the joint probability of a corpus. It has long been\nthe core of natural language processing (NLP) [8], and has inspired a variety of other models,\ne.g., the n-gram model, smoothing techniques [4], as well as various neural networks for NLP\n[2, 6, 17]. In particular, the renewed prosperity of neural models has made groundbreaking\nimprovement in many tasks, including language modeling per se [2], part-of-speech tagging,\nnamed entity recognition, semantic role labeling [6], etc.\nThe recurrent neural network (RNN) is a prevailing class of language models; it is suitable\nfor modeling time-series data (e.g., a sequence of words) by its iterative nature. An RNN\nusually keeps one or a few hidden layers; at each time slot, it reads a word, and changes\nits state accordingly. Compared with traditional n-gram models, RNNs are more capable of\nlearning long range features—especially with long short term memory (LSTM) units [7] or\ngated recurrent units (GRU) [5]—and hence are better at capturing the nature of sentences.\nOn such a basis, it is even possible to generate a sentence from an RNN language model, which\nhas wide applications in NLP, including machine translation [15], abstractive summarization\n[13], question answering [19], and conversation systems [18]. The sentence generation process\n1\narXiv:1512.06612v2  [cs.CL]  3 Jan 2016\nis typically accomplished by choosing the most likely word at a time, conditioned on previous\nwords as well as additional information depending on the task (e.g., the vector representation\nof the source sentence in a machine translation system [15]).\nIn many scenarios, however, we are likely to impose constraints on the generated sentences.\nFor example, a question answering system may involve analyzing the question and querying\nan existing knowledge base, to the point of which, a candidate answer is at hand. A natural\nlanguage generator is then supposed to generate a sentence, coherent in semantics, containing\nthe candidate answer. Unfortunately, using existing language models to generate a sentence\nwith a given word is non-trivial: adding additional information [16, 19] about a word does\nnot guarantee that the wanted word will appear; generic probabilistic samplers (e.g., Markov\nchain Monte Carlo methods) hardly applies to RNN language models 1; setting an arbitrary\nword to be the wanted word damages the ﬂuency of a sentence; imposing the constraint on\nthe ﬁrst word restricts the form of generated sentences.\nIn this paper, we propose a novel backward and forward (B/F) language model to tackle\nthe problem of constrained natural language generation. Rather than generate a sentence\nfrom the ﬁrst word to the last in sequence as in traditional models, we use RNNs to generate\nprevious and subsequent words conditioned on the given word. The forward and backward\ngeneration can be accomplished either simultaneously or asynchronously, resulting in two\nvariants, syn-B/F and asyn-B/F. In this way, our model is complete in theory for generating\na sentence with a wanted word, which can appear at an arbitrary position in the sentence.\nThe rest of this paper is organized as follows. Section 2 reviews existing language models\nand natural language generators. Section 3 describes the proposed B/F language models in\ndetail. Section 4 presents experimental results. Finally, we have conclusion in Section 5.\n2 Background and Related Work\n2.1 Language Modeling\nGiven a corpus w = w1,··· ,wm, language modeling aims to minimize the joint distribution\nof w, i.e. p(w). Inspired by the observation that people always say a sentence from the\nbeginning to the end, we would like to decompose the joint probability as 2\np(w) =\nm∏\nt=1\np(wt|wt−1\n1 ) (1)\nParameterizing by multinomial distributions, we need to further simplify the above equation in\norder to estimate the parameters. Imposing a Markov assumption—a word is only dependent\non previous n−1 words and independent of its position—results in the classic n-gram model,\nwhere the joint probability is given by\np(w) ≈\nm∏\nt=1\np\n(\nwt\n⏐⏐wt−1\nt−n+1\n)\n(2)\nTo mitigate the data sparsity problem, a variety of smoothing methods have been proposed.\nWe refer interested readers to textbooks like [8] for n-gram models and their variants.\n1With recent eﬀorts in [3].\n2 w1, w2, · · · , wt is denoted as wt\n1 for short.\n2\nBengio et al. [2] propose to use feed-forward neural networks to estimate the probability\nin Equation 2. In their model, a word is ﬁrst mapped to a small dimensional vector, known\nas an embedding; then a feed-forward neural network propagates information to a softmax\noutput layer, which estimates the probability of the next word.\nA recurrent neural network (RNN) can also be used in language modeling. It keeps a\nhidden state vector (ht at time t), dependent on the its previous state ( ht−1) and the current\ninput vector x, the word embedding of the current word. An output layer estimates the\nprobability that each word occurs at this time slot. Following are listed the formulas for a\nvanilla RNN.3\nht = RNN(xt,ht−1) (3)\n= f(Winxt + Whidht−1) (4)\np(wt|wt−1\n0 ) ≈softmax (Woutht) (5)\nAs is indicated from the equations, an RNN provides a means of direct parametrization of\nEquation 1, and hence has the ability to capture long term dependency, compared with n-\ngram models. In practice, the vanilla RNN is diﬃcult to train due to the gradient vanishing\nor exploding problem; long short term (LSTM) units [7] and gated recurrent units (GRU) [5]\nare proposed to better balance between the previous state and the current input.\n2.2 Language Generation\nUsing RNNs to model the joint probability of language makes it feasible to generate new\nsentences. An early attempt generates texts by a character-level RNN language model [14];\nrecently, RNN-based language generation has made breakthroughs in several real applications.\nThe sequence to sequence machine translation model [15] uses an RNN to encode a source\nsentence (in foreign language) into one or a few ﬁxed-size vectors; another RNN then decodes\nthe vector(s) to the target sentence. Such network can be viewed as a language model,\nconditioned on the source sentence. At each time step, the RNN predicts the most likely\nword as the output; the embedding of the word is fed to the input layer at next step. The\nprocess continues until the RNN generates a special symbol <eos> indicating the end of the\nsequence. Beam search [15] or sampling methods [16] can be used to improve the quality and\ndiversity of generated texts.\nIf the source sentence is too long to ﬁt into one or a few ﬁxed-size vectors, an attention\nmechanism [1] can be used to dynamically focus on diﬀerent parts of the source sentence dur-\ning target generation. In other studies, Wen et al. use an RNN to generate a sentence based\non some abstract representations of semantics; they feed a one-hot vector, as additional infor-\nmation, to the RNN’s hidden layer [16]. In a question answering system, Yin et al. leverage\na soft logistic switcher to either generate a word from the vocabulary or copy the candidate\nanswer [19].\n3 The Proposed B/F Language Model\nIn this part, we introduce our B/F language model in detail. Our intuition is to seek a new\napproach to decompose the joint probability of a sentence (Equation 1). If we know a priori\n3W’s refer to weights; biases are omitted.\n3\nws­1\nws+1\nw0 ...\n... ...\n<eos>\n<eos>\nBackward sequence\n(softmax)\nHidden layer\nForward sequence\n(softmax)\n<eos> <eos>\nwm\nws\nws\nFigure 1: Synchronous foward/backward generation.\nthat a word ws should appear in the sentence ( w = w1,··· ,wm, ws ∈w), it is natural to\ndesign a Bayesian network where ws is the root node, and other words are conditioned on ws.\nFollowing the spirit of “sequence” generation, ws split the sentence into two subsequences:\n•Backward sequence: ws,ws−1,ws−2,··· ,w1\n(s words)\n•Forward sequence: ws,ws+1,ws+2,··· ,wn\n(m−s+ 1 words)\nThe probability that the sentence w with the split word at position s decomposes as\nfollows.4\np\n(\nw1\ns\nwn\ns\n)\n= p(ws)\ns∏\ni=0\np(bw)(ws−i|·)\nm−s+1∏\ni=0\np(fw)(ws+1|·) (6)\nTo parametrize the equation, we propose two model variants. The ﬁrst approach is to\ngenerate previous and backward models simultaneously, and we call this syn-B/F language\nmodel (Figure 1).5 Concretely, Equation 6 takes the form\np\n(\nw1\ns\nwn\ns\n)\n=\nmax{s,m−s+1}−1∏\nt=0\np\n(\nws−t\nws+t\n⏐⏐⏐⏐⏐\nws−t+1\ns\nws+t−1\ns\n)\n(7)\nwhere the factor p(=|=) refers to the conditional probability that current time steptgenerates\nws−t,ws+t in the forward and backward sequences, respectively, given the middle part of the\nsentence, that is, ws−t+1 ···ws ···ws+t−1. If one part has generated <eos>, we pad the special\nsymbol <eos> for this sequence until the other part also terminates.\n4p(\n·\n=\n·\n) denotes the probability of a particular backward/forward sequence.\n5Previously called backbone LM.\n4\nws­1ws w0 ... <eos> <eos>\nBackward sequence (softmax)\nws ws+1w0  ... ... <eos>\nForward sequence (softmax)\nwm ...\nGenerated backward \nsequence in Step I\nStep I\nStep II\nws­1\nFigure 2: Asynchronous forward/backward generation.\nFollowing the studies introduced in Section 2, we also leverage a recurrent neural network\n(RNN) . The factors in Equation 7 are computed by\np\n(\nws−t\nws+t\n⏐⏐⏐⏐⏐\nws−t+1\ns\nws+t−1\ns\n)\n(8)\n=p(bw) (ws−t|ht) ·p(fw) (ws+t|ht) (9)\n= softmax\n(\nW(bw)\nout ht\n)\n·softmax\n(\nW(fw)\nout ht\n)\n(10)\nHere, ht is the hidden layer, which is dependent on the previous state ht−1 and current\ninput word embeddings ˜x =\n[\nx(fw)\nt ; x(bw)\nt\n]\n. We use GRU [5] in our model, given by\nr = σ(Wr ˜x + Urht−1) (11)\nz = σ(Wz ˜x + Uzht−1) (12)\n˜h = tanh\n(\nWx ˜x + Ux(r ◦ht−1)\n)\n(13)\nht = (1 −z) ◦ht−1 + z ◦˜h (14)\nwhere σ(·) = 1\n1+e(−·) ; ◦denotes element-wise product. r and z are known as gates, ˜h the\ncandidate hidden state at the current step.\nIn the syn-B/F model, we design a single RNN to generate both chains in hope that each\nis aware of the other’s state. Besides, we also propose an asynchronous version, denoted as\nasyn-B/F (Figure 2). The idea is to ﬁrst generate the backward sequence, and then feed the\nobtained result to another forward RNN to generate future words. The detailed formulas are\nnot repeated.\nIt is important to notice that asyn-B/F’s RNN for backward sequence generation is dif-\nferent from a generic backward LM. The latter is presumed to model a sentence from the last\nword to the ﬁrst one, whereas our backward RNN is, in fact, a “half” LM, starting from ws.\n5\nTraining Criteria.If we assume ws is always given, the training criterion shall be the cross-\nentropy loss of all words in both chains except ws. We can alternatively penalize the split\nword ws in addition, which will make it possible to generate an entire sentence without ws\nbeing provided. We do not deem the two criteria diﬀer signiﬁcantly, and adopt the latter one\nin our experiments.\nBoth labeled and unlabeled datasets suﬃce to train the B/F language model. If a sentence\nis annotated with a specially interesting word ws, it is natural to use it as the split word. For\nan unlabeled dataset, we can randomly choose a word as ws.\nNotice that Equation 6 gives the joint probability of a sentence w with a particular split\nword ws. To compute the probability of the sentence, we shall marginalize out diﬀerent split\nwords, i.e.,\np(w) =\nm∑\ns=1\np\n(\nw1\ns\nwn\ns\n)\n(15)\nIn our scenarios, however, we always assume that ws is given in practice. Hence, diﬀerent\nfrom language modeling in general, the joint probability of a sentence is not the number one\nconcern in our model.\n4 Evaluation\n4.1 The Dataset and Settings\nTo evaluate our B/F LMs, we prefer a vertical domain corpus with interesting application\nnuggets instead of using generic texts like Wikipedia. In particular, we chose to build a\nlanguage model upon scientiﬁc paper titles on arXiv. 6 Building a language model on paper\ntitles may help researchers when they are preparing their drafts. Provided a topic (desig-\nnated by a given word), constrained natural language generation could also acts as a way of\nbrainstorming.7\nWe crawled computer science-related paper titles from January 2014 to November 2015. 8\nEach word was decapitalized, but no stemming was performed. Rare words (≤10 occurrences)\nwere grouped as a single token, <unk>, (referring to unknown). We removed non-English\ntitles, and those with more than three <unk>’s. We notice that <unk>’s may appear fre-\nquently, but a large number of them refer to acronyms, and thus are mostly consistent in\nsemantics.\nCurrently, we have 25,000 samples for training, 1455 for validation and another 1455 for\ntesting; our vocabulary size is 3380. The asyn-B/F has one hidden layer with 100 units; syn-\nB/F has 200; This makes a fair comparison because syn-B/F should simultaneously learn im-\nplicit forward and backward LMs, which are completely diﬀerent. In our models, embeddings\nare 50 dimensional, initialized randomly. To train the model, we used standard backpropa-\ngation (batch size 50) with element-wise gradient clipping. Following [9], we applied rmsprop\nfor optimization (embeddings excluded), which is more suitable for training RNNs than na¨ ıve\nstochastic gradient descent, and less sensitive to hyperparameters compared with momentum\nmethods. Initial weights were uniformly sampled from [ −0.08,0.08]. Initial learning rate was\n6http://arxiv.org\n7The title of this paper is NOT generated by our model.\n8Crawled from http://http://dblp.uni-trier.de/db/journals/corr/\n6\nMethodOverall PPL First word’s PPL Subsequent words’ PPL\nSequential LM 152.2 416.2 134.8\nInfo-init 148.7 371.5 133.3\nInfo-all 125.4 328.0 121.8\nsep-B/F 192.4 556.1 169.9\nsep-B/F (ws oracle) 99.2 – –\nsyn-B/F 185.4 592.7 162.9\nsyn-B/F (ws oracle) 97.5 – –\nasyn-B/F 177.2 584.5 153.7\nasyn-B/F (ws oracle) 89.8 – –\nTable 1: Perplexity (PPL) of our B/F LMs and baselines.\n0.002, with a multiplicative learning rate decay of 0.97, moving average decay 0.99, and a\ndamping term ϵ = 10−8. As word embeddings are sparse in use [12], they were optimized\nasynchronously by pure stochastic gradient descent with learning rate being divided by √ϵ.9\n4.2 Results\nWe ﬁrst use the perplexity measure to evaluate the learned language models. Perplexity is\ndeﬁned as 2−ℓ, where ℓ is the log-likelihood (with base 2), averaged over each word.\nℓ= 1\nm\nm∑\ni=1\nlog p(wi)\nNote that <eos> is not considered when we compute the perplexity.\nWe compare our models with several baselines:\n•Sequential LM: A pure LM, which is not applicable to constrained sentence generation.\n•Info-all: Built upon sequential LM, Info-all takes the wanted word’s embedding as\nadditional input at each time step during sequence generation, similar to [16].\n•Info-init: The wanted word’s embedding is only added at the ﬁrst step (sequence to\nsequence model [15]).\n•Sep-B/F: We train two separate forward and backward LMs (both starting from the\nsplit word).\nTable 1 summarizes the perplexity of our models and baselines. We further plot the\nperplexity of a word with respect to its position when generation (Figure 3).\nFrom the experimental results, we have the following observations.\n•All B/F variants yield a larger perplexity than a sequantial LM. This makes much sense\nbecause randomly choosing a split word increases uncertainly. It should be also noticed\nthat, in our model, the perplexity reﬂects the probability of a sentence with a speciﬁc\nsplit word, whereas the perplexity of the sequential LM assesses the probability of a\nsentence itself.\n9The implementation was based on [10, 11].\n7\nSheet1\nPage 1\nsyn­B/F Sequential LM Info­all\nt=0 584.58 592.731 556.08 416.198 328.049 371.526\nt=1 171.78 213.357 218.244 173.756 144.133 165.266\nt=2 157.49 167.005 167.73 86.578 77.6477 84.4156\nt=3 165.31 159.472 163.92 123.114 108.86 122.801\nt=4 163.18 166.353 159.052 144.323 121.649 135.437\nt>=5 139.08 136.188 153.168 141.325 132.928 142.817\nasyn­B/F sep­B/F Info­init\nt=0 t=1 t=2 t=3 t=4 t>=5\n0\n100\n200\n300\n400\n500\n600\n700\nasyn­B/F\nsyn­B/F\nsep­B/F\nSequential LM\nInfo­all\nInfo­init\nFigure 3: Perplexity versus word position t, which is the distance between the current word\nand the ﬁrst / split word in sequential, B/F LMs, respectively.\nasyn-B/F syn-B/F Sep-B/F Sequential LM Info-all\ndeep convolu-\ntional neural\nnetworks for unk\n- based image\nsegmentation\nconvolutionalneu-\nral networks for unk\n- unk\ndeep convolu-\ntional neural\nnetworks\nconvolutionalneu-\nral networks for unk\n-based object detec-\ntion\nconvolutionalneu-\nral networks for im-\nage classiﬁcation\nobjecttrackingand\nunk for visual recog-\nnition\nlearning deep convo-\nlutional features for\nobjecttracking\nobjecttracking tracking- unk -\nbased social media\nunk - based unk\ndetection for image\nsegmentation\noptimal control for\nunk systemswith\nunk - type ii : a unk\n- unk approach\nformal veriﬁca-\ntion of unk - unk\nsystems\noptimal control for\nunksystems\nsystems - based\nsynthesis for unk\nbased diagnose\na new approach for\nthe unk of the unk -\nfree problem\nunk: a new method\nfor unk based on -\nline counting on unk\nunk : a new ap-\nproach for unk -\nbased unk\nunk: a survey : a unk - based\napproach to unk\n- based deign of\nunk -based image\nretrieval\nunk: a unk - based\napproach for unk -\nfree grammar\nan approachto unk\nthe edge - preserving\nproblem\nan approachtounk an approachto unk\n- unk\nto unk : a unk\n- eﬃcient and scal-\nable framework for\nthe unk of unk\na unk - based ap-\nproach to unk for\nunk\nTable 2: Generated texts by the B/F and sequential LMs, with the word in bold being\nprovided.\n•Randomly choosing a split word cannot make use of position information in sentences.\nThe titles of scientiﬁc papers, for example, oftentimes follow templates, which may begin\nwith “<unk> : an approach ” or “<unk> - based approach.” Therefore, sequential LM\nyields low perplexity when generating the word at a particular position (t= 2), but such\ninformation is smoothed out in our B/F LMs because the split word is chosen randomly.\n•When tis large (e.g., t≥4), B/F models yield almost the same perplexity as sequential\nLM. The long term behavior is similar to sequential LM, if we rule out the impact of\nchoosing random words. For syn-B/F, in particular, the result indicates that feeding\ntwo words’ embeddings to the hidden layer does not add to confusion.\n8\n•In our applications, ws is always given, which indicates p(ws) = 1 (denoted as ws oracle\nin Table 1). This reduces the perplexity to less than 100, showing that our B/F LMs\ncan well make use of such information that some word should appear in the generated\ntext. Further, our syn-B/F is better than na¨ ıve sep-B/F; asyn-B/F is further capable\nof integrating information in backward and forward sequences.\nWe then generate new paper titles from the learned language model with a speciﬁc word\nbeing given, which can be thought of, in the application, as a particular interest of research\ntopics. Table 2 illustrates examples generated from B/F models and baselines. As we see,\nfor words that are common at the beginning of a paper title—like the adjective convolutional\nand gerund tracking—sequential LM can generate reasonable results. For plural nouns like\nsystems and models, the titles generated by sequential LM are somewhat inﬂuent, but they\nbasically comply with grammar rules. For words that are unlikely to be the initial word,\nsequential LM fails to generate grammatically correct sentences.\nAdding additional information does guide the network to generate sentences relevant to\nthe topic, but the wanted word may not appear. The problem is also addressed in [16].\nBy contrast, B/F LMs have the ability to generate correct sentences. But the sep-B/F\nmodel is too greedy in its each chain. As generating short and general texts is a known issue\nwith neural network-based LMs, sep-B/F can hardly generate a sentence containing much\nsubstance. syn-B/F is better, and asyn-B/F is able to generate sentences whose quality is\ncomparable with sequential LMs.\n5 Conclusion\nIn this paper, we proposed a backward and forward language model (B/F LM) for constrained\nnatural language generation. Given a particular word, our model can generate previous words\nand future words either synchronously or asynchronously. Experiments show a similar per-\nplexity to sequential LM, if we disregard the perplexity introduced by random splitting. Our\ncase study demonstrates that the asynchronous B/F LM can generate sentences that contain\nthe given word and are comparable to sequential LM in quality.\nReferences\n[1] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to\nalign and translate. In International Conference on Learning Representations, 2015.\n[2] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language\nmodel. The Journal of Machine Learning Research , 3:1137–1155, 2003.\n[3] M. Berglund, T. Raiko, M. Honkala, L. K¨ arkk¨ ainen, A. Vetek, and J. T. Karhunen.\nBidirectional recurrent neural networks as generative models. In Advances in Neural\nInformation Processing Systems, pages 856–864, 2015.\n[4] S. F. Chen and J. Goodman. An empirical study of smoothing techniques for language\nmodeling. In Proceedings of the 34th Annual Meeting on Association for Computational\nLinguistics, pages 310–318, 1996.\n9\n[5] K. Cho, B. van Merri¨ enboer, D. Bahdanau, and Y. Bengio. On the properties of neural\nmachine translation: Encoder-decoder approaches. In Proceedings of Eighth Workshop\non Syntax, Semtnatics and Structure in Statistical Translation .\n[6] R. Collobert and J. Weston. A uniﬁed architecture for natural language processing:\nDeep neural networks with multitask learning. In Proceedings of the 25th International\nConference on Machine Learning, pages 160–167, 2008.\n[7] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation ,\n9(8):1735–1780, 1997.\n[8] D. Jurafsky and J. H. Martin. Speech and Language Processing. Pearson, 2014.\n[9] A. Karpathy, J. Johnson, and F.-F. Li. Visualizing and understanding recurrent networks.\narXiv preprint arXiv:1506.02078 , 2015.\n[10] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin. Convolutional neural networks over tree\nstructures for programming language processing. In Proceedings of the Thirtieth AAAI\nConference on Artiﬁcial Intelligence , 2016.\n[11] L. Mou, H. Peng, G. Li, Y. Xu, L. Zhang, and Z. Jin. Discriminative neural sentence\nmodeling by tree-based convolution. In Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing, pages 2315–2325, 2015.\n[12] H. Peng, L. Mou, G. Li, Y. Chen, Y. Lu, and Z. Jin. A comparative study on regu-\nlarization strategies for embedding-based neural networks. In Proceedings of the 2015\nConference on Empirical Methods in Natural Language Processing , pages 2106–2111,\n2015.\n[13] A. M. Rush, S. Chopra, and J. Weston. A neural attention model for abstractive sentence\nsummarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 379–389, 2015.\n[14] I. Sutskever, J. Martens, and G. E. Hinton. Generating text with recurrent neural net-\nworks. In Proceedings of the 28th International Conference on Machine Learning , pages\n1017–1024, 2011.\n[15] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112,\n2014.\n[16] T.-H. Wen, M. Gasic, D. Kim, N. Mrksic, P.-H. Su, D. Vandyke, and S. Young. Stochas-\ntic language generation in dialogue using recurrent neural networks with convolutional\nsentence reranking. In Proceedings of the 16th Annual Meeting of the Special Interest\nGroup on Discourse and Dialogue , pages 275–284, 2015.\n[17] Y. Xu, L. Mou, G. Li, Y. Chen, H. Peng, and Z. Jin. Classifying relations via long short\nterm memory networks along shortest dependency paths. In Proceedings of Conference\non Empirical Methods in Natural Language Processing , 2015.\n[18] K. Yao, G. Zweig, and B. Peng. Attention with intention for a neural network conversation\nmodel. arXiv preprint arXiv:1510.08565 (NIPS Workshop) , 2015.\n10\n[19] J. Yin, X. Jiang, Z. Lu, L. Shang, H. Li, and X. Li. Neural generative question answering.\narXiv preprint arXiv:1512.01337 , 2015.\n11",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8614961504936218
    },
    {
      "name": "Automatic summarization",
      "score": 0.773956298828125
    },
    {
      "name": "Recurrent neural network",
      "score": 0.7252172231674194
    },
    {
      "name": "Machine translation",
      "score": 0.6929125189781189
    },
    {
      "name": "Language model",
      "score": 0.6428419947624207
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6416000127792358
    },
    {
      "name": "Sentence",
      "score": 0.6326665282249451
    },
    {
      "name": "Word (group theory)",
      "score": 0.603664219379425
    },
    {
      "name": "Natural language generation",
      "score": 0.6002675294876099
    },
    {
      "name": "Natural language processing",
      "score": 0.5307742357254028
    },
    {
      "name": "Natural language",
      "score": 0.4566234350204468
    },
    {
      "name": "Conversation",
      "score": 0.41789937019348145
    },
    {
      "name": "Translation (biology)",
      "score": 0.4142310917377472
    },
    {
      "name": "Artificial neural network",
      "score": 0.22783976793289185
    },
    {
      "name": "Linguistics",
      "score": 0.10688632726669312
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    }
  ]
}