{
    "title": "RT-Transformer: retention time prediction for metabolite annotation to assist in metabolite identification",
    "url": "https://openalex.org/W4392143551",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2097907449",
            "name": "Jun Xue",
            "affiliations": [
                "Chinese Academy of Agricultural Sciences",
                "Ministry of Agriculture and Rural Affairs",
                "Yunnan University",
                "Agricultural Genomics Institute at Shenzhen"
            ]
        },
        {
            "id": "https://openalex.org/A2100635762",
            "name": "Bingyi Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097655234",
            "name": "Hongchao Ji",
            "affiliations": [
                "Agricultural Genomics Institute at Shenzhen",
                "Ministry of Agriculture and Rural Affairs",
                "Chinese Academy of Agricultural Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2113229624",
            "name": "Wei-hua Li",
            "affiliations": [
                "Yunnan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2890671425",
        "https://openalex.org/W2289191408",
        "https://openalex.org/W2101394823",
        "https://openalex.org/W2806376870",
        "https://openalex.org/W2892186999",
        "https://openalex.org/W2088363255",
        "https://openalex.org/W3021129126",
        "https://openalex.org/W2914609306",
        "https://openalex.org/W2737322204",
        "https://openalex.org/W2070977314",
        "https://openalex.org/W2803664483",
        "https://openalex.org/W2937662637",
        "https://openalex.org/W2996714860",
        "https://openalex.org/W2179948434",
        "https://openalex.org/W2922522932",
        "https://openalex.org/W2067083286",
        "https://openalex.org/W2519747900",
        "https://openalex.org/W4200240219",
        "https://openalex.org/W3114814976",
        "https://openalex.org/W4281778378",
        "https://openalex.org/W2051814417",
        "https://openalex.org/W6685305002",
        "https://openalex.org/W4233763100",
        "https://openalex.org/W3212001303",
        "https://openalex.org/W2177317049",
        "https://openalex.org/W4206313288",
        "https://openalex.org/W1963989877",
        "https://openalex.org/W4303982455",
        "https://openalex.org/W3196798783",
        "https://openalex.org/W2065104255",
        "https://openalex.org/W2285468881",
        "https://openalex.org/W2954086013",
        "https://openalex.org/W2265106087",
        "https://openalex.org/W2551876238",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2333394028",
        "https://openalex.org/W2751484141",
        "https://openalex.org/W2395579298",
        "https://openalex.org/W2767683865",
        "https://openalex.org/W2175158758",
        "https://openalex.org/W3118642024",
        "https://openalex.org/W4293543509",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2170876339"
    ],
    "abstract": "Abstract Motivation Liquid chromatography retention times prediction can assist in metabolite identification, which is a critical task and challenge in nontargeted metabolomics. However, different chromatographic conditions may result in different retention times for the same metabolite. Current retention time prediction methods lack sufficient scalability to transfer from one specific chromatographic method to another. Results Therefore, we present RT-Transformer, a novel deep neural network model coupled with graph attention network and 1D-Transformer, which can predict retention times under any chromatographic methods. First, we obtain a pre-trained model by training RT-Transformer on the large small molecule retention time dataset containing 80 038 molecules, and then transfer the resulting model to different chromatographic methods based on transfer learning. When tested on the small molecule retention time dataset, as other authors did, the average absolute error reached 27.30 after removing not retained molecules. Still, it reached 33.41 when no samples were removed. The pre-trained RT-Transformer was further transferred to 5 datasets corresponding to different chromatographic conditions and fine-tuned. According to the experimental results, RT-Transformer achieves competitive performance compared to state-of-the-art methods. In addition, RT-Transformer was applied to 41 external molecular retention time datasets. Extensive evaluations indicate that RT-Transformer has excellent scalability in predicting retention times for liquid chromatography and improves the accuracy of metabolite identification. Availability and implementation The source code for the model is available at https://github.com/01dadada/RT-Transformer. The web server is available at https://huggingface.co/spaces/Xue-Jun/RT-Transformer.",
    "full_text": null
}