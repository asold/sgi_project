{
    "title": "AI language models cannot replace human research participants",
    "url": "https://openalex.org/W4385066501",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2132755104",
            "name": "Jacqueline Harding",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A4301470673",
            "name": "William D’Alessandro",
            "affiliations": [
                "Ludwig-Maximilians-Universität München",
                "Munich School of Philosophy"
            ]
        },
        {
            "id": "https://openalex.org/A2334925995",
            "name": "N. G. Laskowski",
            "affiliations": [
                "University of Maryland, College Park"
            ]
        },
        {
            "id": "https://openalex.org/A1993197906",
            "name": "Robert Long",
            "affiliations": [
                "National Patient Safety Foundation"
            ]
        },
        {
            "id": "https://openalex.org/A4301470673",
            "name": "William D’Alessandro",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1993197906",
            "name": "Robert Long",
            "affiliations": [
                "National Patient Safety Foundation"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4376117416",
        "https://openalex.org/W2617322327",
        "https://openalex.org/W2798002542"
    ],
    "abstract": null,
    "full_text": "Vol.:(0123456789)1 3\nAI & SOCIETY (2024) 39:2603–2605 \nhttps://doi.org/10.1007/s00146-023-01725-x\nCURMUDGEON CORNER\nAI language models cannot replace human research participants\nJacqueline Harding1 · William D’Alessandro2 · N. G. Laskowski3  · Robert Long4\nReceived: 15 June 2023 / Accepted: 30 June 2023 / Published online: 21 July 2023 \n© The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2023\nGenerative artificial intelligence has the potential to trans-\nform many of our projects, from art and education to busi-\nness and security. AI’s promise in these areas comes largely \nfrom its ability to perform complex cognitive tasks more \ncheaply, quickly, and reliably than humans.\nGiven such capabilities, AI is also likely to have splashy \neffects on scientific research. Machine learning techniques \nhave already been used to speed up the search for useful \ndrugs, help prove mathematical theorems, refine climate \nmodels, and parse enormous datasets from space telescopes \nand particle accelerators. While large language models \n(LLMs) and other generative AI tools haven’t yet taken \ncenter stage, their research potential will undoubtedly grow \nas context windows expand, reasoning abilities improve, and \nmultimodal proficiencies multiply.\nOther potential scientific uses of generative AI, however, \nare relatively speculative and require more critical scrutiny. \nOur plan here is to wax curmudgeonly about one such pro-\nposal, pertaining to LLMs and moral psychology research.\nThe proposal starts from the observation that, having \nbeen trained on trillions of internet text tokens, contem-\nporary language models are skilled imitators of human \nlinguistic behavior. LLM responses closely align with an \naverage person’s on a variety of prompts; current models \nhave even replicated classical phenomena from psychology, \nlike Hsee’s less-is-better effect, and behavioral economics, \nlike Harsanyi’s ultimatum games. In light of their speed, \nease of recruitment and talents at mimicry, it’s natural to \nwonder what contributions LLMs might make to the human \nsciences.\nDillion et al.’s (2023) recent “Can AI language models \nreplace human participants?” raises a sharp version of this \nquestion. The authors focus on moral psychology, noting \nthat GPT-3.5 (text-davinci-003) produces judgments about \na variety of moral scenarios which correlate strongly with \naverage human ratings. Such correlations are impressive, \nand they deserve further study across a wider range of mod-\nels and prompts. But, as we’ll argue, they don’t underwrite \nany interesting degree of replacement of humans by lan-\nguage models.\nDillion et al. themselves propose three concrete applica-\ntions of LLMs in moral psychology research: (1) helping \ngenerate and refine research hypotheses, (2) piloting test \nitems, and (3) corroborating data gathered from human sub-\njects. These proposals have some plausibility. However, to \nthe extent that they’re plausible, they offer little support for \nthe prospect of human replacement.\nLet us elaborate. The authors’ proposal 1, generating \nand refining hypotheses, concerns a stage of research which \ndoesn’t inherently involve human participants, so there’s no \nquestion of replacement to begin with. As for proposal 2, \nLLMs are unsuitable for important aspects of item pilot-\ning. Consider the need to determine whether participants \nmay misinterpret or struggle with a given test item. Because \nlanguage models are exemplary text processors by design, \nthey won’t accurately model human participants’ difficulties \nwith comprehension, reasoning and the like. So humans will \nstill be needed to assess these factors. Proposal 3 calls for \nlanguage model outputs to serve as comparison points to \nordinary experimental data. While an interesting sugges-\ntion, this presupposes that LLMs haven’t replaced humans \nas primary research subjects. Dillion et al.’s concrete sug-\ngestions therefore seem to involve relatively modest kinds \nof supplementation, not replacement.\nApart from these three proposals, Dillion et al. also hint at \na larger role for LLMs, as reflected in their paper’s title. The \nauthors ask whether AI models might “…become a substi-\ntute for the people—and minds—that [moral psychologists] \nJacqueline Harding, William D’ Alessandro, N. G. Laskowski, \nRobert Long contributed equally to this work.\n * N. G. Laskowski \n ngl.philosophy@gmail.com\n1 Stanford University, Stanford, USA\n2 Munich Center for Mathematical Philosophy, Ludwig \nMaximilian University, Munich, Germany\n3 University of Maryland at College Park, College Park, USA\n4 Center for AI Safety, San Francisco, USA\n2604 AI & SOCIETY (2024) 39:2603–2605\n1 3\nstudy” (p.1). They then claim that “To replace human par -\nticipants, AI must give humanlike responses” (ibid). Finally, \nthey note that “Recent work suggests…[LLMs] can make \nhuman-like judgments” (ibid). This all amounts to an appar-\nently optimistic, or at least open-minded, appraisal of a \nstrong replacement thesis.\nWhat would the motivation for such a thesis look like? \nDillion et al. suggest picturesquely that a successful model \ntrained on large corpus of human text will “indirectly cap-\nture millions of human minds” (p.3). The output of such a \nmodel can then be thought of as an expression of a “modal \nopinion” (p.2) of the captured minds. Of course—as Dil-\nlion et al. are quick to observe—the data on which language \nmodels are trained were produced by a specific subpopula-\ntion of humans, meaning that claims made on the basis of \nthe model’s representativeness must be carefully circum-\nscribed. Current methods for fine-tuning LLM performance, \nsuch as reinforcement learning with human feedback, further \nexacerbate this issue.\nAssume, however, that an LLM can output relatively \naccurate modal opinions for some populations. Suppose we \npresent this model with a novel moral vignette for which we \nhave no human data, and its output is intuitively surprising. \nIs this strong evidence that some populations of humans \nwould form that judgment? Or should we suspect that the \nmodel has given a non-humanlike response, perhaps because \nit’s latched onto some unconsidered aspect of the prompt, or \nbecause the vignette is out of distribution for the model? In \nscenarios like this, the informativeness of the LLM’s output \nis impossible to assess without doing further confirmatory \nwork with human participants.\nOne might try to steer a middle course between the \nreplacement fan’s optimism and our pessimism by suggest-\ning that evidence from LLMs, while useful enough to take \nthe place of some human data, is nevertheless defeasible and \nrelatively weak on its own. Dillion et al. may be espousing a \nversion of this idea when they recommend taking “a broadly \nBayesian perspective, with data from language models pro-\nviding only a small adjustment in the probability of priors” \n(p.3).\nWe have no quarrel with the idea of using LLMs for small \nBayesian updates. If GPT-4 classifies controlled forest burns \nas morally good, say, this gives a bit of reassurance that \n(some) humans would judge similarly. The problem with \nthis suggestion is that many information sources can pro-\nvide small Bayesian updates without thereby qualifying as \nblue-ribbon experimental data; not all useful evidence is \nadmissible scientific evidence. So Dillion et al.’s modesty \nis appropriate, but it doesn’t significantly strengthen the case \nfor replacement.\nHere’s a final possible defense of replacement opti-\nmism. At present, LLMs are relatively immature, and \nwe haven’t yet adequately probed the nature and extent \nof their ability to simulate human moral judgments. But \nthese factors will improve over time. Conceivably, we’ll \nbe so confident about a future GPT-n’s accuracy (for some \npopulations, in some domains of interest) that we can cut \nhumans out of the experimental process.\nBut this response appears to rest on a mistaken assump -\ntion about the stability of our judgments over time. Given \ntheir training data, language models offer a snapshot of \naverage moral opinion over some fixed past period. But \nchanging world events, personal experiences and social \ndevelopments mean that moral views are always in flux—\nand major shifts can happen rapidly, as with American \nattitudes toward LGBT issues in the 2000s. Consequently, \nfrequent and careful work with human participants will \nalways be an integral part of moral psychology research. 1\nWhile LLMs will undoubtedly prove useful to scien-\ntists, it’s unlikely that they can supplant human research \nparticipants in any significant way. We remain optimistic \nabout a broader range of AI research applications in psy -\nchology and elsewhere. In general, the possibility of using \nLLMs to model or simulate human behavior presents rich \npossibilities and questions. The relevant conversations at \nthe intersection of AI, philosophy of science, moral psy -\nchology, and ethics are only beginning.\nOn behalf of all authors, the corresponding author \nstates that there is no conflict of interest.\nCurmudgeon Corner Curmudgeon Corner is a short opinionated col-\numn on trends in technology, arts, science and society, commenting \non issues of concern to the research community and wider society. \nWhilst the drive for super-human intelligence promotes potential \nbenefits to wider society, it also raises deep concerns of existen-\ntial risk, thereby highlighting the need for an ongoing conversation \nbetween technology and society. At the core of Curmudgeon concern \nis the question: What is it to be human in the age of the AI machine? \n-Editor.\nData availability Not applicable.\nReferences\nDillion D et al (2023) Can AI language models replace human par -\nticipants? Trends Cogn Sci 2438:1–4. https:// doi. org/ 10. 1016/j. \ntics. 2023. 04. 008\nKagan S (1998) Normative ethics. Westview Press, Boulder\nLaskowski N (2018) Epistemic modesty in ethics. Philos Stud \n175(7):1577–1596\n1 The idea that human moral development is constantly in flux has \nbeen espoused by several prominent moral philosophers, including \nRawls (1974: 289), Kagan (1998: 16), and Scanlon (1998: 361), the \nlatter of whom writes, “Working out the terms of moral justification \nis an unending task.” See also Laskowski (2018) for precisification \nand extended defense of this idea.\n2605AI & SOCIETY (2024) 39:2603–2605 \n1 3\nRawls J (1974) The independence of moral theory. Proc Addresses Am \nPhilosophical Assoc 48:5–22\nScanlon T (1998) What we owe to each other. Harvard University \nPress, Cambridge\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations."
}