{
  "title": "Patient Centric Summarization of Radiology Findings using Large Language Models",
  "url": "https://openalex.org/W4391528467",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2231377352",
      "name": "Amara Tariq",
      "affiliations": [
        "Mayo Clinic Hospital",
        "Mayo Clinic in Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A2979888116",
      "name": "Sam Fathizadeh",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5113112202",
      "name": "Gokul Ramaswamy",
      "affiliations": [
        "Mayo Clinic in Arizona",
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2933101254",
      "name": "Shubham Trivedi",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2922011578",
      "name": "Aisha Urooj",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2139946794",
      "name": "Nelly Tan",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2259045158",
      "name": "Matthew T. Stib",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2158956040",
      "name": "Bhavik N Patel",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2122079672",
      "name": "Imon Banerjee",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2231377352",
      "name": "Amara Tariq",
      "affiliations": [
        "Mayo Clinic in Arizona",
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2979888116",
      "name": "Sam Fathizadeh",
      "affiliations": [
        "Illinois College"
      ]
    },
    {
      "id": "https://openalex.org/A2933101254",
      "name": "Shubham Trivedi",
      "affiliations": [
        "Mayo Clinic in Arizona",
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2922011578",
      "name": "Aisha Urooj",
      "affiliations": [
        "Mayo Clinic Hospital",
        "Mayo Clinic in Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A2139946794",
      "name": "Nelly Tan",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2259045158",
      "name": "Matthew T. Stib",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2158956040",
      "name": "Bhavik N Patel",
      "affiliations": [
        "Mayo Clinic Hospital",
        "Mayo Clinic in Arizona"
      ]
    },
    {
      "id": "https://openalex.org/A2122079672",
      "name": "Imon Banerjee",
      "affiliations": [
        "Mayo Clinic in Arizona",
        "Mayo Clinic Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2907025287",
    "https://openalex.org/W4220725161",
    "https://openalex.org/W2786718592",
    "https://openalex.org/W2920839350",
    "https://openalex.org/W4287095483",
    "https://openalex.org/W4387356888",
    "https://openalex.org/W4321351832",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W2097177814",
    "https://openalex.org/W3151410070",
    "https://openalex.org/W3170218295",
    "https://openalex.org/W2891022667",
    "https://openalex.org/W3175316843",
    "https://openalex.org/W4205403018",
    "https://openalex.org/W3034863243",
    "https://openalex.org/W4382182493",
    "https://openalex.org/W4366330426",
    "https://openalex.org/W6800875267",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4386065569",
    "https://openalex.org/W4386076140",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W4386117070",
    "https://openalex.org/W4308408786",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4297253404"
  ],
  "abstract": "ABSTRACT Objective Develop automated AI models for patient-sensitive summarization of radiology reports. Level of medical education or socio-economic background of a patient may dictate their level of understanding of medical jargon. Inability to understand primary findings from a radiology report may lead to unnecessary anxiety among patients or result in missed follow up. Materials and Methods Computed tomography exams of chest were selected as a use-case for this study. Approximately 7K chest CT reports were collected from Mayo Clinic Enterprise. Summarization model was built on the T5 large language model (LLM) as its text-to-text transfer architecture is intuitively suited for abstractive text summarization, resulting in a model size of ~0.77B. Noisy groundtruth for model training was collected by prompting LLaMA 13B model. Results We recruited both experts (board-certified radiologists) and laymen to manually evaluate summaries generated by model. Model-generated summaries rarely missed information as marked by majority opinion of radiologists. Laymen indicated 63% improvement in their understanding by reading layman summaries generated by the model. Comparative study with zero-shot performance of LLaMA indicated that LLaMA hallucinated and missed information 3 and 4 times more often, respectively, than the proposed model. Discussion The proposed patient-sensitive summarization model can generate summaries for radiology reports understandable by patients with vastly different levels of medical knowledge. In addition, task-specific training allows for more reliable performance compared to much larger off-the-shelf models. Conclusions The proposed model could improve adherence to follow up treatment suggested by radiology reports by increasing patients’ level of understanding of these reports.",
  "full_text": "Patient Centric Summarization of Radiology Findings using Large Language \nModels \nAmara Tariq, Ph.D.1†, Sam Fathizadeh, B.S.3, Gokul Ramaswamy1, M.S., Shubham Trivedi, B.S.1, Aisha \nUrooj, PhD.1, Nelly Tan, M.D.2, Matthew T. Stib, M.D.2, Bhavik N. Patel, M.D.1,2, Imon Banerjee, \nPh.D.1,2 \n \n1Arizona Advanced AI (A3) Hub, Mayo Clinic Arizona \n2Department of Radiology, Mayo Clinic, Phoenix, AZ, USA \n3University of Illinois College of Medicine \n†Corresponding author \n \n \n \n \nAddress correspondence to: \nAmara Tariq, Ph.D. \nArizona Advanced AI (A3I) Hub \ntariq.amara@mayo.edu \n \nWord count: 3502/4000 words \n \nKeywords: radiology report summarization, user-sensitive summarization, LLM \n \n \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 5, 2024. ; https://doi.org/10.1101/2024.02.01.24302145doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nABSTRACT \n \nObjective. Develop automated AI models for patient-sensitive summarization of radiology reports. Level \nof medical education or socio-economic background of a patient may dictate their level of understanding \nof medical jargon. Inability to understand primary findings from a radiology report may lead to \nunnecessary anxiety among patients or result in missed follow up.  \nMaterials and Methods.  Computed tomography exams of chest were selected as a use-case for this \nstudy. Approximately 7K chest CT reports were collected from Mayo Clinic Enterprise. Summarization \nmodel was built on the T5 large language model (LLM) as its text-to-text transfer architecture is \nintuitively suited for abstractive text summarization, resulting in a model size of ~0.77B. Noisy \ngroundtruth for model training was collected by prompting LLaMA 13B model.  \nResults. We recruited both experts (board-certified radiologists) and laymen to manually evaluate \nsummaries generated by model. Model-generated su mmaries rarely missed information as marked by \nmajority opinion of radiologists. Laymen indicated 63% improvement in their understanding by reading \nlayman summaries generated by the model.  Comparative study with zero-shot performance of LLaMA \nindicated that LLaMA hallucinated and missed information 3 and 4 times more often, respectively, than \nthe proposed model. \nDiscussion. The proposed patient-sensitive summarization model can generate summaries for radiology \nreports understandable by patients with vastly different  levels of medical knowledge. In addition, task-\nspecific training allows for more reliable performance compared to much larger off-the-shelf models. \nConclusions. The proposed model could improve adherence to follow up treatment suggested by \nradiology reports by increasing patients’ level of understanding of these reports.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 5, 2024. ; https://doi.org/10.1101/2024.02.01.24302145doi: medRxiv preprint \nINTRODUCTION \nThe 21st Century Cures Act’s mandate for immediate release of electronic health information (EHR) \nfacilitated patients’ access to their radiology reports, often before referring physicians can explain to \npatients their radiological findings’ significance. These diagnostic and procedural reports are composed of \ncomplex jargon and non-grammatical fragments, with less than 4% of reports at the 8th grade reading \nlevel of the average United States adult [1]. In today’s practice, medical records in the EHR such as \nradiology reports are largely constructed for provider-to-provider communication and for ICD/CPT \ncoding for billing. These medical reports are now being increasingly accessed by patients. This can \ncontribute to significant patient confusion and anxiety, which in turn can lead to confusion about one’s \nmedical care and may contribute to lack of adherence to follow-up or treatment[2], [3]. Moreover, \ncomplex language within the report may contribute to lack of follow up for incidental and other findings \nsuggested within the report.  It has been shown that only 50% of recommended follow up is performed[4].  \nWhile some work has been done to study the effect of patient-level factors on adherence to follow up \nrecommendation [5], negligible research effort has been directed towards making radiology reports \nreadable to patients by taking patient-level factors into account.  \nSeveral studies in the past have established a direct link between patients’ understanding of their \nmedical information with adherence to recommended prevention and treatment processes, better clinical \noutcomes, better patient safety within hospitals, and less health care utilization[10]. Radiology reports are \nan integral part of medical information but are traditionally hard to understand by the patients, even the \nimpression, as they are mainly written for communication between radiologists and clinical specialists. \nLarge amount of Natural Language Processing (NLP) research effort has been directed towards \nautomated generation of radiology reports, often with radiology images as input[11], [12], automated \nwriting of the Impression section[13]–[15] with focus on factual correctness [16]. While previously \nrecurrent neural networks were specifically trained for this task, LLMs are now being tested on this \napplication as zero-shot learners[17], [18]. However, most of these models are not designed with patients \nin mind, and hence, their output is not sensitive to any patient characteristics. We attempted to solve this \nissue by providing the proposed model information about the patient so that the model’s output was \ntailored to the patient. This additional step will ensure that communication between radiologists and \nclinical specialists remains unaffected as the original radiology report writing standards are not altered for \nincreasing patients’ understanding level by potentially compromising information delivery to clinical \nspecialists. Building an automated model to do this job will ensure that radiologists’ workload does not \nincrease in any manner.  \nWe hypothesized that an ‘understandable’ radiology report summary will look different for \nsomeone with a high-school diploma compared to a clinical professional. Hence, any radiology report \nsimplification model must be conditioned on the patient. While general-purpose large languages like GPT \nand LLaMA may be used as zero-shot learners for this task [6], studies have shown that their off-the-shelf \nuse in sensitive fields like medicine is inadvisable given inconsistency in their performance and their \npropensity to “hallucinate” information[7]–[9]. To reliably achieve the goal of enhancing patient \nunderstanding of radiology reports in a safe and sensitive manner, we developed and trained an innovative \nmodel built based on a publicly available and relatively “smaller” LLM whereas the training data for this \nnovel task was curated using the largest publicly available LLM in addition to manual filtering effort. \nThis approach of using larger LLM in the inference model and fine tuning a smaller LLM allowed us to \ncurtail computational costs while achieving high task-based performance.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 5, 2024. ; https://doi.org/10.1101/2024.02.01.24302145doi: medRxiv preprint \nLarge language models (LLM) are huge models w ith 100s of billions of parameters trained on \nextremely large textual datasets like text crawled from the world wide web, often in self-supervised \nmanner with tasks like masked token prediction or next sentence prediction. LLMs have shown \nastounding capabilities in solving many NLP tasks. Their biggest advantage is their performance as zero-\nshot learners promising the availability of a universal language model - one model that can perform any \nlanguage based extraction, retrieval or even reasoning based tasks[19]–[23] . However, this approach may \nnot be suitable for application in every domain, Sensitive domains like medicine require precise use of \nlanguage in a consistent manner. LLMs have displayed trends of inconsistency in performance - different \noutput for the same input - and hallucination where the model seems to “imagine” information that does \nnot exist [7]–[9]. Additional problems arise when the model is used to make recommendations and those \nrecommendations are not concordant with current clinical standards as set by clinical expert bodies like \nASCO and NCCN[24]. In the case of communicating radiologic findings with patients, these trends may \nresult in inconsistent, inaccurate, or misleading information to be disseminated.  \n \nWhile extremely large language models have been able to show astounding generalization \ncapacity, training of such models requires extremely large training datasets and huge amounts of \ncomputational resources. We argue that there is a balance to strike between computational resources and \nperformance, especially when the model is targeted for a specific use-case and not intended to be used for \nunrelated applications. Our use case falls within this scenario. Therefore, we experimented with a \nrelatively smaller LLM and showed that it was able to achieve reliable performance for the given task \nwhile curtailing the need for computational resources. Our novel use case had no available verified \ntraining data - layman summary of the radiology findings. Thus, we decided to make use of the ability of \nmuch larger LLMs to generalize to novel tasks by generating noisy ground truth for training our model.  \nWe later used manual filtering techniques to dropout low-quality data points. This approach limited the \namount of manual effort needed as much larger time and effort would have been needed to generate \ngroundtruth data from scratch. In addition, larger LLMs were only used in inference mode without \nfinetuning or training thus limiting the number of computational resources used. Two-pronged user and \nexpert studies were conducted to evaluate the performance of our model. The results establish the efficacy \nof our model in terms of genera ting understandable and accurate summaries se nsitive to the patients’ \nlevels of medical knowledge.  \n \n  \nMATERIALS AND METHODS \n \nThe proposed problem requires generation of two versions summary for each radiology report - 1) \ntechnical summary meant for patients or referring physicians with a high level of understanding of \nmedical jargon, 2) layman summary meant for people with limited knowledge of medical terminology. In \nFig 1, we presented the overall framework for generating two versions of summary of findings \ndocumented within radiology reports which contains the three primary modules - (i) section \nsegmentation; (ii) noisy data generation for layman summary; (iii) two-step large language model fine-\ntuning; and (iv) user evaluation to evaluate the quality of both technical and layman summaries. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 5, 2024. ; https://doi.org/10.1101/2024.02.01.24302145doi: medRxiv preprint \n  \nFigure 1. Overall pipeline of generating two different versions of summary of radiology reports - \ntechnical summary (similar to “impression” section) and layman summary. \n  \nSection segmentation: We utilize previously developed NLP methods to parse the clinical history, \nimaging protocol, findings, and impression sections of the radiology reports using section segmentation \nbased on the header[25]. To generalize the section segmentation across multiple institutions, we extracted \nall the variations of the headers using a similar word list generated by Word2Vec language model trained \non 3M radiology reports from Emory University Hospital. Such a non-contextual language model \ngenerates a similar word list only by reflecting co-occurrence statistics which is sufficient for capturing \nheader variations between reports given that such words appear in similar context. We computed the \nsimilar word list for each header by intersecting the list of other headers. For example, similar wordlists \nfor the ‘clinical history:’ section include: ‘indications:’, ‘history:’, ‘patient history:’, ‘reason for exam’, \n‘reason for order’. In the later sections, we only utilize the ‘finding’ and ‘impression’ section from the \noriginal report.  \n \nNoisy data generation: The most challenging aspect of radiology finding simplification is the lack of \nrobust groundtruth, i.e., patient understandable summary, for model training while we plan to use the \noriginal ‘impression’ section from the radiology reports. To the best of our knowledge, no dataset of \nradiology reports and their corresponding patient-understandable summaries exists, and generating those \nsummaries manually is extremely expensive and variable based on manual experience. We decided to \nmeet this challenge through the use of weak ground truth generated by publicly available large language \nmodels (LLM) - LLaMA 13B model [26] where we instruct the model to generate patient-understandable \nsummaries given the radiologic findings as input (see Fig 2 for the exact prompt). In our study, this \nsummary generation process served another alternative purpose - for the given radiology report \nsummarization task, study of zero-shot performance of very large state-of-the-art LLMs - LLaMA 13B. \nHowever, we call these generative summaries - ‘weak groundtruth’ since tendencies of foundational \nmodel zero shot summaries are known to have inconsistency and hallucination which puts limits on their \noff-the-shelf use for sensitive applications like communication with patients. Hallucinated information \nmay increase patient anxiety. Inconsistency in model response may increase confusion among patients.  \nTwo-step LLM fine-tuning: For the specialized task of radiology simplification, we opted to fully fine-\ntuned LLM models which require availability of labeled training data. However, we designed a two-step \ntraining process - (step 1) : technical summary generation with <input: finding and output: original \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 5, 2024. ; https://doi.org/10.1101/2024.02.01.24302145doi: medRxiv preprint \nimpression section> as labeled pair; (step 2): user sensitive radiology report summarization with  <input: \nfinding and output: LLaMA generated layman summary> as labeled pair with weak ground truth; Our \nLLM summarization model was based on the architecture and initial weights of the text-to-text transfer \ntransformer (T5) model publicly released by Google [27]. We used their “large” version consisting of 770 \nmillion parameters which is considerably smaller than off-the-shelf open-source popular LLMs like GPT-\n2 (1.5B) and LLaMA (7B, 13B, and 65B parameter version), and thus it should be comparatively easy to \nfine-tune with limited data. The selection was also based on intuitive adaptability of text-to-text transfer \narchitecture for “transferring” report to summaries as displayed by the models original target tasks which \nincluded summarization [27].  \nGiven the fact that the desired model is supposed to be sensitive to the user's level of medical \nknowledge, we designed the modeling framework to utilize instruction-based prompts for providing \ninformation about a user's level of medical knowledge in the form of free-text encoding which does not \nrequire any change in the model architecture. For practical purposes, the current version of the proposed \nmodel expects two prompts for generating two different summaries of the same report, i.e., layman and \nexpert levels of knowledge.  \n \nFigure 2. Sample prompt for generating noisy ground truth for layman summary. \n \nUser-centric evaluation (radiologist and users with non-medical background): As discussed earlier, the \ngroundtruth labeled pair does not exist for the layman summary. Thus, we decided to formulate two-\npronged strategy for true evaluation of the summaries generated by the model to evaluate - i) model \ngenerated summary especially layman level summa ry is understandable by the patients with little-to-\nmoderate level of medical knowledge, and ii) model generated both layman and technical summaries are \nproviding correct and complete information without hallucination.  \nIn evaluation I, 3 users with limited medical knowledge and different education levels were asked \nto grade on Likert scale [28] within range of 1 to 5 with 5 being the perfect score if the layman summary \ngenerated by the model is understandable or not. In evaluation II , the 3 expert board certified radiologists \nwere asked to read both the layman and expert level summaries and fill out a detailed survey based on \nthree aspects; a) missing information:  is there any information missing that should have been \ncommunicated to the patient (evaluated on binary scale: Yes/No), b) hallucination: is there any \n“hallucinated'' information in the model generated summary (evaluated on binary scale: Yes/No), and  c) \nlinguistic accuracy: semantic quality of the summary language (evaluated on Likert scale within range of  \n1 to 5 with 5 being the perfect score).  The results of both studies are reported in Table 4.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 5, 2024. ; https://doi.org/10.1101/2024.02.01.24302145doi: medRxiv preprint \n \nResults \nDataset: With the approval of Mayo Clinic Institutional Review Board (IRB), we chose a commonly \nperformed diagnostic exam of chest CT as the primary use-case for experiments. Approximately 120K \nchest CTs are performed each year in Mayo Clinic for new diagnosis and/or tracking progression of a \nvariety of issues including chest infection. heart and lung problems, blocked arteries, congestive heart \nfailure, lung cancer, pulmonary embolism, damage to lymph nodes, muscle and bone disorders, bone \ntumors and fractures, and internal bleeding. While inability to identify incidental findings may lead to \ndelayed followup, misunderstandings regarding critical i ssues like malignant lesions in lungs may lead to \nincreased anxiety among patients. We collected 6970 chest CT reports from Mayo Clinic for exams \nperformed between 2013 and 2022. The following table described major characteristics for these reports, \nincluding distribution of findings.  \n \nTable 1: Study cohort characteristics. Findings are not mutually exclusive.  \nTotal number of reports 6970 \nYears of study 2013 - 2022 \nMean length of finding \nsection \n150 +/- 65 words \nMean length of impression 33 +/- 26 words \nPatient Characteristics Age: 66.7+/-10.2 year \nGender: 54% male, 46% female \nRace: 91.6% Caucasian, 3.5% African American, 1.4% Asian \nMajor findings Lung nodules: 5276 \nCardiovascular issues: 4617 \nFractures:771 \nMetastatic cancer: 432 \nStable exam: 272 \n \nQuantitative Evaluation: To evaluate the performance quantitatively, we first ran experiments with \nprogressively larger filtered training sets; 2k, 3k, 4k, and 6k after manually filtering out low quality \nground truth summaries generated by LLaMA (e.g., incomplete information, hallucinations, linguistic \nerror).  Model overfitted severely for dataset sizes up to 3K. Larger training set (>3K) sizes resulted in \nbetter learning curves of the training process as depicted in Figure 3 (training curves from all 4 \nexperiments).  \n \nOnce the model training was stabilized, we manually compared the performance of the proposed model \nwith the use of LLaMA as zero-shot learner for layman level summary generation over randomly sampled \nsubset of 100 reports in terms of hallucination and missing information. Table 2 presented the \ncomparative performance where T5 model (less no. of trainable parameters) with task specific fine-tuning \nresulted in less hallucination and missing information and Table 3 shows some actual examples generated \nby LLAMA and our trained model.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 5, 2024. ; https://doi.org/10.1101/2024.02.01.24302145doi: medRxiv preprint \n \n \nFigure 3: Effects of training dataset size on cross entropy- based validation losses during training of the propos ed\nsummarization model. Training datasets sizes (shown in legend) were progressively increased as this proc ess\nrequired manual effort. \n \n \nTable 2 : Comparison between the performance of the proposed model and zero- shot performance  of\nLLaMA for layman summary generation. Optimal performance highlighted in bold. \nModel Model size Hallucination Missing information \nLLaMA (zero-shot) 13 billion 18% 17% \nOur model (task-\nspecific training) \n770 million 6% 4% \n \nTable 3: Sample outputs of LLaMA and the domain-specific fine- tuned proposed model. ‘...’ represents\nadditional text which is not added to preserve readability.  \nOriginal Finding Section LLaMA-generated \nlayman summary \nLayman summary- Our \nmodel \ncomment \nTwo new sub-3 mm \nmicronodules in the right \nmiddle lobe … groundglass \nsuggesting an \ninfectious/inflammatory \netiology.    A 3 mm nodule \nin the central left upper lobe \n…. Coronary artery \ncalcification. Trace \npericardial  fluid. … \n  1. The radiologist found \ntwo small spots in the \nright middle lobe of the \nlung that may be caused \nby an infection or \ninflammation, but it's not \nclear and needs to be \nmonitored. 2. The rest of \nthe chest X-ray\n looks the \nsame as it did in [DATE] \nThe radiologist found two \nsmall, new spots on the \nright side of your lung \nthat are not yet clear what \nthey are. They may be \ncaused by an infection or \ninflammation. The \nradiologist also found \nsome cloudy areas in your \nlungs that could be caused \nby an infection or \ninflammation. \nLLaMA \nhallucinated \nthe report \noriginated from \n“X-ray” exam \na few scattered tiny solid \npulmonary nodules in the \n   The radiologist found a \nsmall increase in the \nThe radiologist found a \nsmall area of thickening \nLLaMA missed\n \nlung nodules, \ned \ness \n of \nnts \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 5, 2024. ; https://doi.org/10.1101/2024.02.01.24302145doi: medRxiv preprint \nupper lobes are unchanged  \nsince  [DATE] and should \nbe benign  …    mild upper \nlung predominant \ncentrilobular emphysema. \nmild biapical scarring.  mild \ndependent atelectasis. new \nslight peripheral \ninterlobular septal \nthickening  in the basal \nlower lobes posteriorly ….     \nthickness of the tissue in \nthe lower lobes of the \nlungs, specifically in the \nback of the lungs. This \ncould be caused by a \nreaction to a medication \nor slight fluid buildup in \nthe lungs.  \n \nin one of the lower lobes \nof the lungs, which could \nbe caused by an infection \nor inflammation. The \nradiologist also found \nsome small, solid nodules \nin the upper lobes of the \nlungs, which are stable \nand should be benign. \n \nOur model \nincluded a \ndescription of \nthese nodules \nin the \nsummary. \nno appreciable change in a \nslightly irregular 4 mm \nlateral left upper lobe  \nnodule on …. sub-solid \nnodules measuring 5 mm or  \nless, …..  stable clustered \nmicronodularity in the left \nlower lobe peripherally   \nand right upper lobe \nmedially . stable tiny \nnodule/lymph  node near \nthe right minor fissure  . \ncalcified granulomas. tiny \nfoci of  endobronchial \nmucus plugging scattered in \nboth lungs.   …     \n  This examination was \nperformed to evaluate the \nlymph nodes in the neck \nand the soft tissues of the \nneck.  \nImpression: \n1. there is a 2.0 cm short \naxis mass in the right \nlevel II lymph node. 2. \nthere is a 1.5 cm short \naxis mass in the left level \nII lymph node. 3. there is \na 1.0 cm short axis mass \nin the right level III \nlymph node. 4. there is a \n0.5 cm short axis mass in \nthe left level III lymph \nnode.\" \nThe radiologist found no \nsigns of cancer spreading \nto the lungs. \n \nLLaMA \nhallucinated\n \nmasses and \ntheir sizes.  \n \nOur model \nmissed\n that \nthere was no \nchange in lung \nnodules. \ncentral airways are patent. \nfat-containing right \nposterior  diaphragmatic \nbochdalek hernia. small \nhiatal  hernia. heart and \npericardium: left atrial \nenlargement; …. changes of \nthe spine. please  see mri \nabdomen report from today \nfor further description of \nabdominal findings  \nincluding known pancreatic \nmass with biliary stent in \nplace and pneumobilia. \n Here is the layman \nsummary of the report: \n \nThe MRI scan of your \nabdomen shows: \n* Your liver is slightly \nlarger than normal and \nthere are no signs of \ncancer in it. * Your spleen \nis also slightly larger than \nnormal and there are no \nsigns of cancer in it. * \nYour pancreas is … \nThe radiologist found no \nsigns of cancer spreading \nto the lungs. \n \nLLaMA \nhallucinated\n \nMRI scan \nreport and \nabdominal \nfindings.  \n \n… focal  sclerosis of the \nanterolateral left sixth rib is \nunchanged …. mildly \nenlarged 11 mm \n   The radiologist found \nsome small spots on the \nlungs that are not \ncancerous. Some of these \nThe radiologist found a \nnew area of hardening on \nthe left side of the sixth \nrib, which could be a sign \nOur model \nhallucinated \npossible cause \nof focal  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 5, 2024. ; https://doi.org/10.1101/2024.02.01.24302145doi: medRxiv preprint \nparaesophageal lymph node \nis  unchanged since  …    \nincreased small partially \nloculated left greater than \nright pleural effusions.  \nthere is mid and upper lung \npredominant interstitial \nabnormality characterized  \nby subpleural reticulation. \n…. many of the  pre-\nexisting bilateral pulmonary \nnodules measuring up to 6 \nmm are unchanged  since \n…      \n \nspots are the same size as \nbefore, while others have \ngrown a bit. The doctor \nwill use these images as a \nbaseline to compare \nfuture scans.\n There is also \nsome fluid buildup around \nthe lungs and an enlarged \nlymph node near the \nesophagus, but these are \nnot cause for concern. \nAdditionally, there is a \nsmall area of scar tissue \non the left side of the \nchest that is likely from a \nprevious injury. \n \nof cancer that has spread \nfrom another part of the \nbody (metastatic disease). \nThere is also fluid buildup \nin the space between the \nlungs and the chest wall \n(pleural effusions) on \nboth sides of the body \n(interstitial lung disease). \nThe radiologist also found \nsome small nodules in the \nlungs that have not \nchanged since the last \nexam on [DATE]. \n \nsclerosis of the \nanterolateral \nleft sixth rib. \n \nLLaMA \nhallucinated \npossible \nfollowup with \nfuture scans. \n \nUser centric evaluation:  \nTable 4 summarizes the expert and laymen evaluation results on 23 radiology reports where we grouped \nthe reports based on the findings and 3 expert radiologists and 3 laymen (people with non-clinical \nbackground) evaluated the model generated layman and expert summaries.  Given the known variability \namong the radiologists, we also present the majority opinion for missing information and hallucination. \nAccording to the majority opinion, the model didn’t generate any hallucinated information for normal \nreports and had missing information only for 1 expert summary, while for the metastatic and lung nodule \ncases model had single hallucinated information and 1 missing information for the layman summary. \nHowever linguistic accuracy scored highly for the generated layman summaries 4.18 average Likert scale \n(high quality). Most hallucinations appeared in “Other Diseases” category which includes findings with \nrelatively smaller representation in the dataset such as fractures. Model sometimes hallucinated the \nassociation between rib fracture and pleural effusion while correctly identifying the presence of both the \nfracture and effusion. On the other hand, the model generated layman summaries obtained an average \n63% improvement over the expert summary in terms of understanding scores as assigned by our laymen \nannotators.   \nTable 4: Expert evaluation; Missing information and hallucination columns show number of reports \nmarked with the corresponding issues; Language quality column show average language quality score \nassigned by each annotator. Non-clinical user evaluation: each column reports average understanding \nscore. Number of reports of each category can be found within parentheses with reports category label \n(left-most column) \n \nReport \nCategory \nEXPERT EVALUATION STUDY \n \nExpert \nExpert Summary Layman Summary \nMissing \ninformatio\nn \nHallucinati\non \nLanguag\ne \nQuality \nMissing \ninformatio\nn \nHallucinati\non \nLangua\nge \nQuality \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 5, 2024. ; https://doi.org/10.1101/2024.02.01.24302145doi: medRxiv preprint \nNormal (5) 1 1 0 5.0 0 1 4.8 \n2 0 0 4.6 0 0 4.2 \n3 1 1 3.8 1 1 4.3 \nMajority\n/avg \n1 0 3.13 0 0 4.43 \nMetastatic \nCancer (6) \n1 0 0 4.8 0 2 4.7 \n2 0 0 3.2 0 0 4.7 \n3 3 0 3.6 2 1 3.6 \nMajority\n/avg \n0 0 3.87 0 1 4.33 \nLung \nNodules (5) \n1 0 0 4.6 1 1 4.2 \n2 0 0 3.9 0 0 3.9 \n3 1 1 4.2 2 3 3.6 \nMajority\n/avg  \n0 0 4.23 1 1 3.9 \nOthers (7) 1 0 0 5 0 3 4.2 \n2 0 0 4.3 0 0 4.8 \n3 3 0 4.2 2 6 3.2 \nMajority\n/avg\n \n0 0 4.5 0 3 4.06 \n LAYMEN EVALUATION STUDY \nAnnotator Expert Summary Layman Summary \nNormal (5) 1 3.8 5.0 \n2 2.5 5.0 \n3 2.8 5.0 \nAverage 3.0+/-0.68 5.0+/-0.0 (67% improvement) \nMetastatic \nCancer (6) \n1 2.7 5.0 \n2 1.5 4.7 \n3 3.3 4.7 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 5, 2024. ; https://doi.org/10.1101/2024.02.01.24302145doi: medRxiv preprint \nAverage 2.5+/-0.9 4.8+/-0.17 (92% improvement) \nLung \nNodules (5) \n1 3.0 5.0 \n2 3.2 4.7 \n3 4.3 4.9 \nAverage  3.5+/-0.7 4.9+/-0.15 (40% improvement) \nOthers (7) 1 2.9 5.0 \n2 2.4 4.7 \n3 3.4 4.9 \nAverage 2.9+/-0.5 4.9+/-0.15 (69% improvement) \nOverall Average 3.0 4.9 (63% improvement) \n \n \nDISCUSSION \n \nIn this work, we presented a domain-specific fine-tuning of a LLM-based model with noisy groundtruth \nto generate a patient-centric summary of radiology report findings with the goal of enhancing patients’ \nunderstanding of these reports.  Patients’ inability to understand their personal medical and clinical status \nis a critical issue limiting adherence to treatment plans and follow-up appointments. However, no \ngroundtruth dataset - a set of reports with multiple versions of summaries for patients with different levels \nof knowledge of medical jargon, exist. Given the recent trend of the use of extremely large language \nmodels (10’s to 100’s of billion parameters) as zero-shot learners, we used LLaMA to generate layman \nsummaries while using actual impression sections as expert level summaries for training. However, ~15% \nof layman summaries generated by LLaMA needed to be filtered out because of obvious mistakes, \nindicating that simple use of LLMs for this task is not sufficient. We fine-tuned a relatively “smaller” \nLLM - T5 (770m) for the specific task of patient sensitive summary generation which can change \nlanguage of the summary based on patients’ clinical knowledge - layman or expert. Even though the \nsummarizer model is based on the T5 model with le ss trainable (~770 million) parameters, the pretrained \nmodel suffered severe overfitting for the finetuning unless manual effort was applied to curate ~4K clean \ntraining samples from the LLaMA generated noisy groundtruth. Larger models may require even more \nmanual effort to curate larger training sets. This experiment provided motivation to use appropriately \nsized models for the given task, especially when task-specific finetuning was being performed.  \n \nThe user evaluation study done by expert radiologists indicates a lack of consensus among experts on \nwhat should be reported to the patient. Annotating radiologists disagree on the quality of several model-\ngenerated reports. Interestingly, radiologists sometimes even disagree with the reporting made in the \noriginal report. For example, the original impression contains “ Prominence of the ascending aorta at 3.5 \ncm” indicating enlarged aorta which was included in the summary generated by the model. One of the \nannotating radiologists pointed out that the threshold for aorta enlargement is 4cm, hence, the aorta was \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 5, 2024. ; https://doi.org/10.1101/2024.02.01.24302145doi: medRxiv preprint \nnot technically enlarged but was reported in an ambiguous manner. In some cases, the model seemed to \nbe adding information about common causes for some findings. For example, the model generated \nsummary included the phrase “ small amount of fluid around the heart, which is likely caused by an \ninfection or inflammation ”. One of the annotating radiologists considered this form of reporting as \n“hallucination” because while likely correct, the reason was never explicitly mentioned in the original \nreport. An example of disagreement in annotating radiologists’ opinion regarding what should be included \nin a summary generated for a patient is the exact size of the nodule. While two annotators considered \nsummaries reporting the growth trend of the nodule (growing or stable) sufficient, one annotator \nconsidered such cases as having “missing information”. When the report only mentioned unchanged \nnodes, the model-generated layman summary only stated that no new issues or problems were detected. \nSome annotators considered this as an incomplete description while others agreed with the model. \n \nGiven these disagreements between annotators, we relied on majority opinion for final assessment of \nmodel-generated summaries. Among 23 expert summaries, only one case was agreed upon as having \nmissing information where lymphangitic spread of carcinoma was considered a possibility by the \nreporting radiologist because of the history of cancer. However, the study is designed in such a way that \nonly the finding section was supplied from the original reports for consistency and the model was not \nprovided with the history section of the report, and consequently, missed this possibility.  One layman \nsummary was annotated as having missing information based on majority opinion. In that summary, the \nmodel failed to describe all indeterminate nodules in the summary.  Relatively larger number (5) of \nlayman summaries were considered to have hallucinated information. Model seems to do relatively poorly \nwhile generating layman summaries for fractures (categorized under Other Disease). This may be the \nresult of relatively small representation (~10%) of such reports in the training set. While stable exams and \nmetastatic disease also have similar representation in the dataset, their summarization is quite \nstraightforward. In both cases, the patient needs to know their disease is unchanged, or their cancer has \nnow spread. In case of fractures, there are many important pieces of information including anatomical \nlocation, severity, and any resulting complications (e.g., joint effusion/hemarthrosis). In another case, the \nmodel hallucinated “new spots on liver” when the report mentioned hepatic metastasis. Note that the \nlayman summary generation was trained on noisy groundtruth generated by LLaMA model. Even after \nmanual filtering, it is possible that the training data contains small amounts of discrepancies resulting in \nlimitations on model’s performance for layman summary generation. Even under these circumstances, our \nnon-expert (layman with no medical training) reported on average 63% improvement in their \nunderstanding reports when presented with model-generated layman summaries compared to expert \nsummaries, thus proving potential for improvement in patients’ understanding of their clinical status \nthrough the use the proposed model.  \n \nComparison between the performance of the proposed model and the zero-shot performance of LLaMA \nfor layman summarization establishes the superiority of our much smaller model both in terms of \ncorrectness and lack of hallucination. LLaMA often hallucinated that the report was for chest X-ray \ninstead of chest CT exams and sometimes even hallucinated several abdominal findings just at the \nmention of a concurrent abdominal MRI in the findings section of the chest CT report. In some cases, \nLLaMA ignored lung nodules. Our model also sometimes hallucinated the reason for abnormal mass or \nnodule, even though the model clearly conveyed the uncertainty of such statements.  \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 5, 2024. ; https://doi.org/10.1101/2024.02.01.24302145doi: medRxiv preprint \n \nCONCLUSION \n \nWe designed a first-of-its-kind radiology report summarization model conditioned upon patient clinical \nknowledge. Our evaluation experiments clearly indicate the limitation of using off-the-shelf LLM as zero-\nshot learners for this task. On the other hand, noisy ground truth curated from off-the-shelf LLM was \nsuccessfully used to train a relatively smaller summarizer model which outperformed much larger LLMs \nfor the given task. Two-pronged user study evaluated model generated summaries from points of view of \nboth laymen and experts. Model generated layman summaries were 63% more understandable to laymen. \nMajority opinion of experts found only a handful of cases with missing or hallucinated information in \nmodel generated summaries. In general, experts evaluated expert-level summaries to be better than \nlayman summaries.  \n \nREFERENCES \n \n[1] T. Martin-Carreras, T. S. Cook, and C. E. Kahn Jr, “Readability of radiology reports: \nimplications for patient-centered care,” Clin. Imaging, vol. 54, pp. 116–120, 2019. \n[2] J. Domingo et al., “Preventing delayed and missed care by applying artificial intelligence to \ntrigger radiology imaging follow-up,” NEJM Catal. Innov. Care Deliv., vol. 3, no. 4, p. CAT-\n21, 2022. \n[3] T. Mabotuwana, C. S. Hall, J. Tieder, and M. L. Gunn, “Improving quality of follow-up \nimaging recommendations in radiology,” presented at the AMIA annual symposium \nproceedings, American Medical Informatics Association, 2017, p. 1196. \n[4] T. Mabotuwana et al., “Automated tracking of follow-up imaging recommendations,” Am. J. \nRoentgenol., vol. 212, no. 6, pp. 1287–1294, 2019. \n[5] A. Á.-G. Calvillo, L. C. Kodaverdian, R. Garcia, D. Y. Lichtensztajn, and M. D. Bucknor, \n“Patient-level factors influencing adherence to follow-up imaging recommendations,” Clin. \nImaging, vol. 90, pp. 5–10, 2022. \n[6] K. Jeblick et al., “ChatGPT makes medicine easy to swallow: an exploratory case study on \nsimplified radiology reports,” Eur. Radiol., pp. 1–9, 2023. \n[7] H. Alkaissi and S. I. McFarlane, “Artificial hallucinations in ChatGPT: implications in \nscientific writing,” Cureus, vol. 15, no. 2, 2023. \n[8] M. Sallam, “ChatGPT utility in healthcare education, research, and practice: systematic \nreview on the promising perspectives and valid concerns,” presented at the Healthcare, \nMDPI, 2023, p. 887. \n[9] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, and D. S. W. \nTing, “Large language models in medicine,” Nat. Med., vol. 29, no. 8, pp. 1930–1940, 2023. \n[10] R. Anhang Price et al., “Examining the role of patient experience surveys in measuring \nhealth care quality,” Med. Care Res. Rev., vol. 71, no. 5, pp. 522–554, 2014. \n[11] O. Alfarghaly, R. Khaled, A. Elkorany, M. Helal, and A. Fahmy, “Automated radiology report \ngeneration using conditioned transformers,” Inform. Med. Unlocked, vol. 24, p. 100557, \n2021. \n[12] S. Dai, Q. Wang, Y. Lyu, and Y. Zhu, “BDKG at MEDIQA 2021: System report for the \nradiology report summarization task,” presented at the Proceedings of the 20th Workshop \non Biomedical Language Processing, 2021, pp. 103–111. \n[13] Y. Zhang, D. Y. Ding, T. Qian, C. D. Manning, and C. P. Langlotz, “Learning to summarize \nradiology findings,” ArXiv Prepr. ArXiv180904698, 2018. \n[14] B. Gundogdu et al., “Customized impression prediction from radiology reports using bert \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 5, 2024. ; https://doi.org/10.1101/2024.02.01.24302145doi: medRxiv preprint \nand lstms,” IEEE Trans. Artif. Intell., 2021. \n[15] X. Cai, S. Liu, J. Han, L. Yang, Z. Liu, and T. Liu, “Chestxraybert: A pretrained language \nmodel for chest radiology report summarization,” IEEE Trans. Multimed., 2021. \n[16] Y. Zhang, D. Merck, E. B. Tsai, C. D. Manning, and C. P. Langlotz, “Optimizing the factual \ncorrectness of a summary: A study of summarizing radiology reports,” ArXiv Prepr. \nArXiv191102541, 2019. \n[17] Z. Sun et al., “Evaluating GPT-4 on impressions generation in radiology reports,” Radiology, \nvol. 307, no. 5, p. e231259, 2023. \n[18] C. Ma et al., “ImpressionGPT: an iterative optimizing framework for radiology report \nsummarization with chatGPT,” ArXiv Prepr. ArXiv230408448, 2023. \n[19] J. Wei et al., “Finetuned Language Models are Zero-Shot Learners,” presented at the \nInternational Conference on Learning Representations, 2021. \n[20] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large language models are \nzero-shot reasoners,” Adv. Neural Inf. Process. Syst., vol. 35, pp. 22199–22213, 2022. \n[21] M. F. Naeem et al., “I2MVFormer: Large Language Model Generated Multi-View Document \nSupervision for Zero-Shot Image Classification,” presented at the Proceedings of the \nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 15169–\n15179. \n[22] J. Guo et al., “From Images to Textual Prompts: Zero-shot Visual Question Answering with \nFrozen Large Language Models,” presented at the Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition, 2023, pp. 10867–10877. \n[23] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language models as zero-shot \nplanners: Extracting actionable knowledge for embodied agents,” presented at the \nInternational Conference on Machine Learning, PMLR, 2022, pp. 9118–9147. \n[24] S. Chen et al., “Use of artificial intelligence chatbots for cancer treatment information,” \nJAMA Oncol., vol. 9, no. 10, pp. 1459–1462, 2023. \n[25] I. Banerjee et al., “Natural Language Processing Model for Identifying Critical Findings—A \nMulti-Institutional Study,” J. Digit. Imaging, vol. 36, no. 1, pp. 105–113, 2023. \n[26] H. Touvron et al., “Llama: Open and efficient foundation language models,” ArXiv Prepr. \nArXiv230213971, 2023. \n[27] C. Raffel et al., “Exploring the limits of transfer learning with a unified text-to-text \ntransformer,” J. Mach. Learn. Res., vol. 21, no. 1, pp. 5485–5551, 2020. \n[28] T. Nemoto and D. Beglar, “Likert-scale questionnaires,” presented at the JALT 2013 \nconference proceedings, 2014, pp. 1–8. \n[29] R. Luo et al., “BioGPT: generative pre-trained transformer for biomedical text generation \nand mining,” Brief. Bioinform., vol. 23, no. 6, p. bbac409, 2022. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 5, 2024. ; https://doi.org/10.1101/2024.02.01.24302145doi: medRxiv preprint ",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9060379862785339
    },
    {
      "name": "Computer science",
      "score": 0.5972535014152527
    },
    {
      "name": "Radiology",
      "score": 0.5608404278755188
    },
    {
      "name": "Reading (process)",
      "score": 0.5078919529914856
    },
    {
      "name": "Jargon",
      "score": 0.4569265842437744
    },
    {
      "name": "Medical physics",
      "score": 0.397490918636322
    },
    {
      "name": "Natural language processing",
      "score": 0.39202335476875305
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3876749575138092
    },
    {
      "name": "Medicine",
      "score": 0.37892425060272217
    },
    {
      "name": "Linguistics",
      "score": 0.15690985321998596
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210146710",
      "name": "Mayo Clinic in Florida",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2802423016",
      "name": "WinnMed",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210127938",
      "name": "Mayo Clinic Hospital",
      "country": "US"
    }
  ],
  "cited_by": 6
}