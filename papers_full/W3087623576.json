{
  "title": "BioALBERT: A Simple and Effective Pre-trained Language Model for Biomedical Named Entity Recognition",
  "url": "https://openalex.org/W3087623576",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5077006200",
      "name": "Usman Naseem",
      "affiliations": [
        "The University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A5030777395",
      "name": "Matloob Khushi",
      "affiliations": [
        "The University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A5073143936",
      "name": "V. Lakshma Reddy",
      "affiliations": [
        "The University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A5037711547",
      "name": "Sakthivel Rajendran",
      "affiliations": [
        "The University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A5033585021",
      "name": "Imran Razzak",
      "affiliations": [
        "The University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A5100614820",
      "name": "Jinman Kim",
      "affiliations": [
        "The University of Sydney"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4248138401",
    "https://openalex.org/W2110279753",
    "https://openalex.org/W2785001576",
    "https://openalex.org/W6608040171",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4237354024",
    "https://openalex.org/W2955483668",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W6600299915",
    "https://openalex.org/W6602670149",
    "https://openalex.org/W2604019706",
    "https://openalex.org/W3105491236",
    "https://openalex.org/W2510847435",
    "https://openalex.org/W2149369282",
    "https://openalex.org/W2912026815",
    "https://openalex.org/W2071879021",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2168905447",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2857028992",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W3104059174",
    "https://openalex.org/W2047782770",
    "https://openalex.org/W2964242047",
    "https://openalex.org/W2911681509",
    "https://openalex.org/W2769387903",
    "https://openalex.org/W2794604371",
    "https://openalex.org/W2114388055",
    "https://openalex.org/W2100627415",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2963709490",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2949176808",
    "https://openalex.org/W2993029738",
    "https://openalex.org/W2887593177",
    "https://openalex.org/W2890830728",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4289366653",
    "https://openalex.org/W2527896214",
    "https://openalex.org/W2925613093",
    "https://openalex.org/W2407776548",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2168782065"
  ],
  "abstract": "<title>Abstract</title> <bold>Background:</bold> In recent years, with the growing amount of biomedical documents, coupled with advancement in natural language processing algorithms, the research on biomedical named entity recognition (BioNER) has increased exponentially. However, BioNER research is challenging as NER in the biomedical domain are: (i) often restricted due to limited amount of training data, (ii) an entity can refer to multiple types and concepts depending on its context and, (iii) heavy reliance on acronyms that are sub-domain specific. Existing BioNER approaches often neglect these issues and directly adopt the state-of-the-art (SOTA) models trained in general corpora which often yields unsatisfactory results. <bold>Results:</bold> We propose biomedical ALBERT (A Lite Bidirectional Encoder Representations from Transformers for Biomedical Text Mining) - bioALBERT - an effective domain-specific pre-trained language model trained on huge biomedical corpus designed to capture biomedical context-dependent NER. We adopted self-supervised loss function used in ALBERT that targets on modelling inter-sentence coherence to better learn context-dependent representations and incorporated parameter reduction strategies to minimise memory usage and enhance the training time in BioNER. In our experiments, BioALBERT outperformed comparative SOTA BioNER models on eight biomedical NER benchmark datasets with four different entity types. The performance is increased for; (i) disease type corpora by 7.47% (NCBI-disease) and 10.63% (BC5CDR-disease); (ii) drug-chem type corpora by 4.61% (BC5CDR-Chem) and 3.89 (BC4CHEMD); (iii) gene-protein type corpora by 12.25% (BC2GM) and 6.42% (JNLPBA); and (iv) Species type corpora by 6.19% (LINNAEUS) and 23.71% (Species-800) is observed which leads to a state-of-the-art results. <bold>Conclusions:</bold> The performance of proposed model on four different biomedical entity types shows that our model is robust and generalizable in recognizing biomedical entities in text. We trained four different variants of BioALBERT models which are available for the research community to be used in future research.",
  "full_text": "BioALBERT: A Simple and Effective Pre-trained\nLanguage Model for Biomedical Named Entity\nRecognition\nUsman Naseem  (  usman.naseem@sydney.edu.au )\nThe University of Sydney https://orcid.org/0000-0003-0191-7171\nMatloob Khushi \nThe University of Sydney\nVinay Reddy \nThe University of Sydney\nSakthivel Rajendran \nThe University of Sydney\nImran Razzak \nDeakin University\nJinman Kim \nThe University of Sydney\nResearch article\nKeywords: Named entity recognition, Biomedical text mining, Contextual word representations, Pre-trained\nLanguage Modes\nPosted Date: October 13th, 2020\nDOI: https://doi.org/10.21203/rs.3.rs-90025/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nNaseem et al.\nRESEARCH ARTICLE\nBioALBERT: A Simple and Eﬀective Pre-trained\nLanguage Model for Biomedical Named Entity\nRecognition\nUsman Naseem 1* , Matloob Khushi 1, Vinay Reddy 1, Sakthivel Rajendran 1, Imran Razzak 2\nand Jinman Kim 1,3\nAbstract\nBackground: In recent years, with the growing amount of biomedical documents , coupled with advancement\nin natural language processing algorithms, the research on biome dical named entity recognition (BioNER) has\nincreased exponentially. However, BioNER research is challe nging as NER in the biomedical domain are: (i)\noften restricted due to limited amount of training data, (ii) an entity can refer to multiple types and concepts\ndepending on its context and, (iii) heavy reliance on acronyms th at are sub-domain speciﬁc. Existing BioNER\napproaches often neglect these issues and directly adopt the sta te-of-the-art (SOTA) models trained in general\ncorpora which often yields unsatisfactory results.\nResults: We propose biomedical ALBERT (A Lite Bidirectional Encoder Repr esentations from Transformers\nfor Biomedical Text Mining) – bioALBERT – an eﬀective domain-spec iﬁc pre-trained language model trained\non huge biomedical corpus designed to capture biomedical context- dependent NER. We adopted\nself-supervised loss function used in ALBERT that targets on mod elling inter-sentence coherence to better\nlearn context-dependent representations and incorporated p arameter reduction strategies to minimise memory\nusage and enhance the training time in BioNER. In our experiment s, BioALBERT outperformed comparative\nSOTA BioNER models on eight biomedical NER benchmark datasets wi th four diﬀerent entity types. The\nperformance is increased for; (i) disease type corpora by 7.47% (NCBI–disease) and 10.63%\n(BC5CDR–disease); (ii) drug–chem type corpora by 4.61% (BC5CDR–Chem ) and 3.89% (BC4CHEMD); (iii)\ngene–protein type corpora by 12.25% (BC2GM) and 6.42% (JNLPBA); and (iv) s pecies type corpora by 6.19%\n(LINNAEUS) and 23.71% (Species–800) is observed which leads to a state -of-the-art results.\nConclusions: The performance of proposed model on four diﬀerent biomedical enti ty types shows that our\nmodel is robust and generalizable in recognizing biomedical enti ties in text. We trained four diﬀerent variants\nof BioALBERT models which are available for the research commun ity to be used in future research.\nKeywords: Named entity recognition; Biomedical text mining; Contextual wor d representations; Pre-trained\nLanguage Modes\nBackground\nThe growing volume of the published biomedical lit-\nerature, such as clinical reports [1] and health liter-\nacy [2], are fuelling the advancements in the devel-\nopment of text mining algorithms. Biomedical named\nentity recognition (BioNER) intends to automatically\nidentify biomedical entities such as diseases, chemicals,\ngenes and proteins, etc., from the biomedical litera-\nture. So, a crucial step towards this aim is to build bet-\n*Correspondence: usman.naseem@sydney.edu.au\n1School of Computer Science, University of Sydney, Australi a\nFull list of author information is available at the end of the article\nter and eﬀective methods which can automatically rec-\nognize and extract biomedical entities. BioNER is an\nintegral component of many downstream information\nextraction tasks e.g., disease–treatment [3] relations\nand extracting drug–to–drug interactions [4]. Tradi-\ntionally, BioNER relies on feature engineering methods\n(e.g., lexicon-based, rules-based and statistics-based).\nHowever, feature engineering is dependent on domain-\nspeciﬁc knowledge which does not perform well on\nBioNER [5].\nDeep learning (DL) with its ability to automati-\ncally extract features have become common in BioNER\nNaseem et al. Page 2 of 8\nrecently [6]. For instance, Long Short-Term Memory\n(LSTM) is usually employed to learn vector represen-\ntations of each word in a sentence, and then as the\ninput to conditional random ﬁelds (CRF) have con-\nsiderably increased the performance in BioNER [6].\nRecently state-of-the-art (SOTA) DL based language\nmodels for instance Embeddings from Language Mod-\nels (ELMo) [7], Bidirectional Encoder Representa-\ntions from Transformers (BERT) [8] and (A Lite Bidi-\nrectional Encoder Representations from Transformers\n(ALBERT) [9] obtained SOTA best performance on\nmany NLP tasks.\nAlthough these models show promising results, but\napplying them on BioNER has multiple challenges and\nlimitations: (i) a limited amount of training data; (ii)\nan entity could represent multiple entity types depend-\ning on its textual context, i.e., a “ BRCA1” can be re-\nferred as gene name as well as a disease entity depend-\ning on its context. Similarly, “ heart attack” and “ my-\nocardial infarction” refer to the same concept and, (iii)\nthe heavy use of acronyms in biomedical texts makes\nit challenging to identify concepts, i.e., the abbrevia-\ntion ” RA” may refer to “ right atrium,” “ rheumatoid\narthritis,” or one of several other concepts, where the\nresolution of the abbreviation is, therefore, context-\ndependent. Therefore, current models in BioNER rely\non various context-independent and transformer-based\ncontext-dependent language models which are trained\non biomedical corpora [10, 11, 12].\nTo overcome the identiﬁed limitations, we present\nbiomedical ALBERT – bioALBERT - a context-\ndependent, fast and eﬀective language model that ad-\ndresses the shortcomings of recently proposed domain-\nspeciﬁc language models. BioALBERT is trained on\nlarge biomedical corpora which address the limitation\nof limited training data. We also innovate in the adop-\ntion of cross-layer parameter sharing by learning pa-\nrameters for the ﬁrst block and reuse the block in the\nremaining layers instead of learning unique parameters\nfor each of the layers and sentence-order-prediction\n(SOP) technique as a measure of coherence loss be-\ntween sentences. SOP takes two consecutive sentences\nfrom training data and creates a random pair from\ndiﬀerent sentences which helps the model to learn bet-\nter representations and ﬁnally, in BERT based models\nthe size of the embedding was linked to the hidden\nlayer sizes of the transformer blocks. These embed-\ndings are projected directly to the hidden space of\nthe hidden layer whereas in our model we use factor-\nized embedding parameterization which decomposes\nembedding matrix in two small matrices which dis-\ntinguish the size of the hidden layers from the size of\nvocabulary embeddings. This allows for increasing the\nhidden size without signiﬁcantly increasing the param-\neter size of the vocabulary embeddings. BioALBERT is\nsimple and eﬃcient when ﬁne-tuned for BioNER task\nas compared to other SOTA models. We evaluate our\nmodel on eight biomedical named entity recognition\nbenchmark datasets.\nIn biomedical text mining research, there is a long\nhistory of using shared language representations to\ncapture the semantics of the text. One established\ntrend is a form of word-embeddings [13] that rep-\nresent syntactic and semantic meaning, and map\nwords into low-dimensional vectors. Similar meth-\nods also have been derived for improving embeddings\nof word sequences by introducing sentence embed-\ndings [14]. These context-independent word embed-\ndings approach such as word2vec [13] were trained\non biomedical corpus containing words along with\nphrases that are not normally available in a general\ndomain corpus [10]. These methods always require\ncomplex neural networks to be eﬀectively used and\nmodel context-independent representations.\nAnother common trend, particularly in recent years,\nis text representation based on context [7, 8, 9]. Un-\nlike traditional word-embeddings, this enables a word\nto change its meaning depending on the context in\nwhich it occurs. Several other works have investigated\nthe beneﬁt of contextual models in biomedical and\nclinical areas. Several researchers trained ELMo on\nbiomedical corpora and presented BioELMo and found\nthat BioELMo beats ELMo on BioNER tasks [11, 15].\nAlong with their work, a pre-trained BioELMo model\nwas published, enabling further clinical research. Belt-\nagy et al. [16] released Scientiﬁc BERT (SciBERT),\nwhere BERT was trained on the scientiﬁc texts. In\nnon-contextual embedding, BERT has been usually su-\nperior and better than ELMo.\nSi et al. [17], trained the BERT on clinical notes cor-\npora, using complex task-speciﬁc models to improve\nboth traditional embedding and ELMo embedding on\nthe i2b2 2010 and 2012 BioNER. Similarly in an-\nother study, a new domain-speciﬁc language model,\nBioBERT [12], trained a BERT model on biomed-\nical documents from PMC abstracts [1] and articles\nfrom PubMed [2] that resulted in improved BioNER\nresults. Peng et al [18] introduced Biomedical Lan-\nguage Understanding Evaluation (BLUE), a collection\nof resources for evaluating and analysing biomedical\nnatural language representation models. Their study\nalso conﬁrmed that pre-trained BERT models with\nPubMed abstracts and clinical notes perform better\nthan many other state-of-the-art models.\nDespite this success, BERT has some limitations\nsuch as BERT has a huge number of parameters which\nis the cause for problems like degraded pre-training\n[1]https://www.ncbi.nlm.nih.gov/pmc/\n[2]https://www.ncbi.nlm.nih.gov/pubmed/\nNaseem et al. Page 3 of 8\ntime, memory management issues and model degra-\ndation etc [9]. These issues are very well addressed in\nALBERT, which is modiﬁed based on the architecture\nof BERT and proposed by Lan et al. [9]. In scaling pre-\ntrained models, ALBERT implements two-parameter\nreduction methods that lift the essential barriers. (i)\nfactorized embedding parameterization - decomposes\nthe large vocabulary embedding matrix into two small\nmatrices, (ii) replaces the NSP loss by SOP loss; and\n(iii) cross-layer parameter sharing- stops the param-\neter from prospering with the network depth. These\nmethods signiﬁcantly lower the number of parameters\nused when compared with BERT without signiﬁcantly\naﬀecting the performance of the model, thus improv-\ning parameter-eﬃciency. An ALBERT conﬁguration is\nsame as BERT (large) has 18 times less parameters and\ncan be trained about 1.7 times faster.\nFine-tuning has been successfully used to transfer\npre-trained weights as initialisation for parameters for\nvarious downstream tasks [19]. This improves the eﬃ-\nciency of the target task when we have limited data\nand similar tasks [20]. Recently, ﬁne-tuning of pre-\ntrained language models have been widely utilized in\nvarious text mining tasks [21]. Liu et al. [22] pre-\ntrained LSTM with a language model and ﬁne-tuned\nit, and this has contributed to improved performance\nfor various text classiﬁcation tasks.\nUniversal Language Model Fine-tuning (ULMFiT)\nused general-domain pre-training and ﬁne-tuning tech-\nniques to avoid over-ﬁtting and achieved SOTA per-\nformance on the datasets with less samples [19]. Simi-\nlarly, authors of BERT, ALBERT, BioBERT and oth-\ners tested the performance of their model for a wide\nrange of tasks with minimum ﬁne-tuning eﬀorts and\nachieved good performance. Recently, multi-task ﬁne-\ntuning has led to improvements even with many tar-\nget tasks [23]. In this paper, we ﬁne-tuned our pre-\ntrained language model BioALBERT which is trained\non biomedical corpora for BioNER. BioALBERT of-\nfers better performance on BioNER as it addresses\nthe shortcomings of BERT used in BioBERT. How-\never, this is not a trivial task as it requires several\noptimisations which are discussed in the next section.\nOur pre-trained models, along with the source code,\nwill be publicly available.\nResults\nIn this section, we present the dataset used, baselines\nand evaluation to demonstrate the eﬀectiveness of our\nmodel.\nDatasets\nOur model is evaluated on eight biomedical NER\nbenchmark datasets which contain four types of en-\ntities and provided by Lee et al. [12]. For (i) Disease\nTable 1 Statistics of the BioNER datasets\nEntitity Type Dataset # of Annotations\nDisease NCBI Disease 6,881\nBC5CDR 12,694\nDrug/Chem BC5CDR 15,411\nBC4CHEMD 79,842\nDrug/Protein BC2GN 20,703\nJNLPBA 35,460\nSPECIES LINNAEUS 4,077\nSpecies-800 3,708\nentity type, we used NCBI-Disease [30] and BC5CDR-\nDisease [31] datasets; (ii) Drug-Chem entity type,\nwe used BC5CDR-Chem [31] and BC4CHEMD [32]\ndatasets; (iii) Drug-Protein entity type, we used\nBC2GM [33] and JNLPBA [34] datasets; and (iv)\nSpecies entity type, we used LINNAEUS [35] and\nSpecies-800 [36] datasets. The statistics of the BioNER\ndatasets used in this study are given in Table 1.\nBaselines\nTo assess the performance of the proposed method,\nan exhaustive comparison is performed with several\nadvanced SOTA methodologies along with their pub-\nlished results [3]. Our model is compared with the fol-\nlowing methods.\nYoon et al. [28] presented CollaboNet which contains\nseveral BiLSTM and CRF models for BioNER. Pre-\nviously proposed models could only handle datasets\nwith a single form of entity whereas CollaboNet lever-\nages diﬀerent datasets and demonstrated the good per-\nformance. CollaboNet is based on several single task\nmodels for named entity recognition that transmit in-\nformation to one another for better predictions.\nLou et al. [26] introduced transition based algorithm\nfor the normalization and disease entity recognition.\nTheir model was built on the transition structured\nprediction framework. In another study, an attention\nbased BiLSTM-CRF, was proposed by Lou et al. [27]\nfor chemical named entity recognition (CNER). This\napproach leverages global document-level information\ngathered through the attention process to ensure con-\ntinuity of labelling across several cases of the similar\ntoken in a document and achieved good results with\nsome feature engineering.\nXu et al. [24] introduced DABLC, a new dictionary-\nbased and document-level attention approach for dis-\nease entity recognition. At document level, DABLC la-\nbels the consistency of several instances in a text and\nincorporates an external dictionary of diseases which\n[3]The reported results were obtained from the original\npublication.\nNaseem et al. Page 4 of 8\nTable 2 Comparison of performance in biomedical named entity recognition (BioNER) task.\nType Datasets M SOTA\nBioBERT v1.0 BioBERT v1.0 BioBERT v1.0 BioBERTv1.1 BioALBERT 1.0 BioALBERT 1.0 BioALBERT 1.1 BioALBERT 1.1\n(PubMed) (PMC) (PubMed+PMC) (PubMed) (PubMed) (PubMed+PMC) (PubMed) (PubMed+PMC)\nBase Base Base Base Base Base Large Large\nDisease\nNCBI Disease\np 88.30 86.76 86.16 89.04 88.22 97.45 96.84 97.18 97.38\nR 89.00 88.02 89.48 89.69 91.25 94.39 94.40 97.18 94.37\nF 88.60 87.38 87.79 89.36 89.71 95.89 95.61 97.18 95.85\nBC5CDR\np 89.61 85.80 84.67 85.86 86.47 99.69 99.11 99.27 99.39\nR 83.09 86.60 85.87 87.27 87.84 95.72 96.17 96.33 95.85\nF 86.23 86.2 85.27 86.56 87.15 97.66 97.62 97.78 97.61\nDrug/Chem\nBC5CDR\np 94.26 92.52 92.46 93.27 93.68 99.99 99.99 99.99 99.99\nR 92.38 92.76 92.63 93.61 93.26 95.89 96.24 95.62 95.68\nF 93.31 92.64 92.54 93.44 93.47 97.9 98.08 97.76 97.79\nBC4CHEMD\np 92.29 91.77 91.65 92.23 92.80 97.76 97.71 97.71 97.88\nR 90.01 90.77 90.30 90.61 91.92 94.22 94.83 94.83 94.63\nF 91.14 91.26 90.97 91.41 92.36 95.96 96.25 96.25 96.23\nDrug/Protein\nBC2GM\np 81.81 81.72 82.86 85.16 84.32 97.86 97.84 98.26 98.02\nR 81.57 83.38 84.21 83.65 85.12 94.87 94.27 95.72 94.70\nF 81.69 82.54 83.53 84.40 84.72 96.34 96.02 96.97 96.33\nJNLPBA\np 74.43 71.11 71.17 72.68 72.24 85.14 85.60 86.23 85.56\nR 83.22 83.11 82.76 83.21 83.56 80.43 80.98 81.90 81.49\nF 78.58 76.65 76.53 77.59 77.49 82.72 83.22 84.01 83.53\nSpecies\nLINNAEUS\np 92.80 91.83 91.62 93.84 90.77 99.95 99.98 99.98 99.92\nR 94.29 84.72 85.48 86.11 85.83 99.47 99.44 99.48 99.55\nF 93.54 88.13 88.45 89.81 88.24 99.71 99.72 99.73 99.73\nSpecies-800\np 74.34 70.60 71.54 72.84 72.80 99.17 98.93 99.10 98.75\nR 75.96 75.75 74.71 77.97 75.36 98.34 98.04 98.95 98.69\nF 74.98 73.08 73.09 75.31 74.06 98.76 98.49 99.02 98.72\nNotes: Metrics (M), Precision (P), Recall (R) and F1 (F) results ar e recorded on each dataset. Bold has the best results, and underlined has the second\nbest results. We report the SOTA model results on various dat asets as follows: NCBI-Disease results from Xu et al. [24], BC2G M from Sachan et al. [25],\nBC5CDR-disease from Lou et al. [26], BC4CHEMD from Luo et al. [27], BC5CDR-chemical and JNLPBA from Yoon et al. [28], and LINNAEUS and\nSpecies-800 from Giorgi and Bader [29].\nis designed with ﬁve disease databases comprising a\nrich set of disease entities.\nLee et al. [12] proposed a biomedical BERT (BioBERT)\nwhich is pre-trained language model trained on biomed-\nical corpus. Authors of BioBERT model demonstrated\nthat training BERT on biomedical corpus improve the\nperformance on BioNER and outperforms previously\npresented models for BioNER. We compare BioAL-\nBERT with both BioBERT (v1.0 and V1.1) models\nand other SOTA models used in BioNER task. We\nchose these methods because they are the SOTA, and\nexhibit the highest performance on the basis of the\nmeta-analysis carried out.\nEvaluation\nTable 2 presents the performance of all the variants of\nBioALBERT and contrasts them to baseline method-\nologies. Our model outperforms all other methods\non all eight datasets. For, (i) Disease-type datasets,\nBioALBERT increased the performance by by 7.47%\n(NCBI–disease) and 10.63% (BC5CDR–disease); (ii)\ndrug–chem type corpora by 4.61% (BC5CDR–Chem)\nand 3.89% (BC4CHEMD); (iii) gene–protein type cor-\npora by 12.25% (BC2GM) and 6.42% (JNLPBA); and\n(iv) species type corpora by 6.19% (LINNAEUS) and\n23.71% (Species–800) is observed which is the SOTA\nperformance on BioNER.\nWe have performed multiple comparisons to anal-\nyse the eﬀectiveness of BioALBERT as compared to\nBioBERT. We measured and compared the training\ntime of BioALBERT with BioBERT. We found that\nall models BioALBERT outperforms BioBERT with a\nFigure 1 Comparison of run-time statistics of BioALBERT v/s\nBioBERT\nconsiderable margin which makes it more faster and\npractical as compared to BioBERT models. The run\ntime statistics of both pre–trained models are given in\nFigure. 1. We also compared the performance com-\nparison (shown in Figure 2 ) of the same versions\n(trained on same corpora and for the same number of\nsteps) of both BioALBERT and BioBERT. We can see\nthat in Figure (2a), we compared BioBERT v1.0 base\nmodel which is trained on PubMed with BioALBERT\n1.0 base model trained on Pubmed and similarly in\nFigure (2b), we compared BioBERT v1.0 base model\ntrained on Pubmed and PMC with BioALBERT 1.0\nbase model trained on PubMed and PMC biomedi-\ncal corpus. In both cases, BioALBERT outperformed\nBioBERT on all of the eight datasets which makes\nour model more robust, generalize and eﬀective than\nBioBERT.\nNaseem et al. Page 5 of 8\nFigure 2 Comparison of a) BioBERT v1.0 (Pubmed)-Base v/s BioALBERT 1 .0 (Pubmed)-Base ; b) BioBERT\nv1.0(Pubmed+PMC)-Base v/s BioALBERT 1.0(Pubmed+PMC)-Ba se\nDiscussions\nBioALBERT gives better performance and addresses\nthe previously mentioned challenges in the biomedical\ndomain. We attribute this to the BioALBERT built\non top of the transformer-based language model that\nlearns contextual relationship between words in the\ncorpus. The transformer encoder learns the whole se-\nquence of tokens at once when compared to directional\nalgorithms which learns the input tokens sequentially\n(i.e., right-to-left or left-to-right). This feature makes\nit possible for the algorithm to learn meaning of a word\nbased on contexts (i.e., left and right of the word) and\nto address the contextual representation problem.\nOur model addresses the shortcomings of BERT\nbased biomedical models. At ﬁrst, BioALBERT uses\ncross-layer parameter sharing and reduces 110 million\nparameters of 12-layer BERT-base model to 31 million\nparameters while keeping the same number of layers\nand hidden units by learning parameters for the ﬁrst\nblock and reuse the block in the remaining 11 layers.\nSecondly, our model uses the SOP, which takes two\nsegments from the training corpus that appear con-\nsecutively and constructs a random pair of segments\nfrom diﬀerent documents. This enables the model to\nlearn about discourse-level coherence characteristics\nfrom a ﬁner-grained distinction and leads to better\nlearning representation in downstream tasks. Thirdly,\nour model uses factorized embedding parameterization\nto decompose the embedding matrix into two small\nmatrices, the smaller size layer vocabulary and hid-\nden layer minimise the parameters number between\nboth the vocabulary and ﬁrst hidden layer whereas\nin BERT based biomedical models embedding size is\nequal to the size of the hidden layer. Finally, our model\nis trained on massive biomedical corpus to be eﬀective\non BioNER to address the issue of the shift of word\ndistribution from general domain corpus to biomedi-\ncal corpus. All these, when combined, address all the\nissues associated with BioNER stated earlier. As our\nproposed model provides a consistent gain in perfor-\nmance for all tested datasets over all other models,\nFigure 3 Comparison of diﬀerent variants BioALBERT models\nwe can infer that our model is a robust solution for\nBioNER task.\nTo extend our analysis, we analysed the performance\nof diﬀerent pre-trained models of BioALBERT. We\nfound out that the performance of all BioALBERT\nmodels are almost equally good, but BioALBERT 1.1\nLarge trained on PubMed works better than others\n(shown in Figure. 3). BioALBERT 1.1 Large model,\ntrained on PubMed with dup-factor as ﬁve gives better\nperformances. This demonstrates that the relevance of\nduplication data in NLP tasks.\nConclusion\nIn this study, we presented BioALBERT, a pre-trained\nlanguage model for biomedical named entity recogni-\ntion. We presented four diﬀerent variants of BioAL-\nBERT models which are trained on huge biomedical\ncorpora for a diﬀerent number of steps. We demon-\nstrated that training ALBERT on biomedical corpus\nis an essential step in applying it to BioNER. As future\nworks, we plan to pre-trained other versions which in-\nclude hybrid of general and biomedical corpora of AL-\nBERT on biomedical corpora with more training steps\nand ﬁne-tune on biomedical text mining task. We also\nplan to ﬁne-tune BioALBERT on other text mining\ntasks to show the eﬀectiveness of our model.\nMethods\nFirst, we initialized BioALBERT with weights from\nALBERT. BioALBERT is trained on biomedical do-\nNaseem et al. Page 6 of 8\nFigure 4 Overview of the pre-training and ﬁne-tuning of BioALBERT on NER\nmain corpus (PubMed and PMC). To demonstrated\nthe eﬀectiveness of our model, BioALBERT is ﬁne-\ntuned on BioNER. We tested various pre-training\nmethods with various combinations and sizes of biomed-\nical corpus. The overview of our methodology is shown\nin Figure. 4. Below we present details of each step in-\nvolved in pre-training and ﬁne-tuning of BioALBERT.\nPre-training of BioALBERT\nIn this section, we present steps involved in the pre-\ntraining of BioALBERT, which has the same archi-\ntecture as ALBERT, which makes it simple. BioAL-\nBERT is trained on abstracts from PubMed and full-\ntext articles of PMC which contains biomedical terms\nand enables to train the ALBERT model, trained on\nthe general text on biomedical corpora. Unstructured\nand raw biomedical corpora (PubMed and PMC) was\nconverted to structured format by converting raw text\nﬁles into a single sentence in which; (i) Within a text,\nall blank lines are removed and transformed into a\nsingle paragraph; (ii) any line with a length of less\nthan 20 characters is excluded and (iii) there will be a\nblank line (when the diﬀerent ﬁles are combined) be-\ntween each document for training the model. Overall,\nPubMed contains approximately 4.5 Billion words, and\nPMC contains about 13.5 Billion words. As an initial\nstep, we have initialized the model weights from AL-\nBERT to create BioALBERT model by pre-training on\nbiomedical corpora and kept the original vocabulary of\nALBERT.\nSentence embeddings are used for tokenization, and\nfor that, we pre-processed the data as a sentence text.\nSo every line in the input text document is a sentence,\nand an empty line separates every document. We set\nthe maximum length of each sentence to 512 words.\nShorter sentences were padded to make 512 whereas\nlonger sentences were truncated. Learning rate (lr =\n0.00176) and 3,125 warm-up steps are used for train-\ning all of our models except for BioALBERT v 1.1\nTable 3 Summary of parameters used in the Pre-training\nSummary of All parameters used: (Pre-Training)\nName BioALBERT 1.0 BioALBERT 1.1\nArchitecture ALBERT Base ALBERT Large\nActivation Function GeLU GeLU\nAttention Heads 12 16\nNo. of Layers 12 24\nSize of Hidden Layer 768 1,024\nSize of Embedding 128 128\nSize of Vocabulary 30k 30k\nOptimizer Used LAMB LAMB\nTraining Batch Size 1024 256\nEvaluation Batch Size 16 16\nMaximum Sentence Length 512 512\nMaximum Predictions per Sentence 20 20\nLearning Rate (lr) 0.00176 0.00062\nTraining Steps for PubMed 200K 200K\nTraining Steps for PMC 270K 270K\nWarm-up Steps 3,125 3,125\nwhich trained on Pubmed and PMC where learning\nrate (lr = 0.00062) is used to stop the issue of explod-\ning loss. Both BioALBERT 1.0 and BioALBERT 1.1\nare trained for 200K and 270K steps on PubMed and\nPMC corpus respectively. We used LAMB optimizer\nduring training process of our models and kept the\nvocabulary size to 30K. GeLU activation is used in all\nvariants of models during training process. For BioAL-\nBERT 1.1 base models training batch size of 1,024 was\nused where as in BioALBERT 1.1 large models train-\ning size was reduced to 256 due to computational re-\nsources limitations. Summary of parameters used in\nthe training process is given in Table 3.\nTable 4 BioALBERT Pre-trained Models\nModel Version Model Trained On # of words Steps\nBioALBERT 1.0 Base PubMed 4.3B 200K\nBioALBERT 1.0 Base PubMed+PMC 18.8B 470K\nBioALBERT 1.1 Large PubMed 4.3B 200K\nBioALBERT 1.1 Large PubMed+PMC 18.8B 470K\nWe experimented with diﬀerent settings and found\nout that both (base and large) pre-trained models were\nsuccessful with bigger batch size on V3-8 TPU. We\nhave used two diﬀerent embedding sizes, i.e., 128 and\nNaseem et al. Page 7 of 8\n256. The 128 embedding size creates a base model\nwith 12 Million parameters, whereas a large model has\n16 Million parameters with 256 embedding size. With\nthese combinations, we have presented a total of four\nmodels, given in Table 4.\nFine-tuning of BioALBERT\nFine-tuning of BioALBERT on BioNER task is pre-\nsented in this section. BioNER involves annotating\ntokens in a sentence as named-entities. The labelled\ndatasets that were used for this task include four cat-\negories representing Disease, Species, Drug/Proteins,\nand Drugs/Chemicals. The objective is to train and\nmake a prediction on the labels, which are the proper\nnouns within the domain area. More formally, given\nan input sentence S = {x1, x2, ..., xz}, where xi is the\ni-th word and z represents the length of the sentence.\nThe goal of BioNER is to classify each word in S and\nassign it to a corresponding label y ∈ Y , where Y is a\npredeﬁned list of all possible label.\nFine-tuning is simple as compared to the pre-\ntraining, and the computational requirements are also\nnot that signiﬁcant. BioALBERT which takes less\nphysical memory and improvised parameter sharing\ntechniques. The BioNER ﬁne-tuning is trained to learn\nthe word embeddings using the sentence piece tok-\nenization while the BioBERT model was based on the\nword piece embeddings. For each of the pre-trained\nmodels, we constructed a ﬁne-tuning task by using the\nspeciﬁc dataset.\nThe model setup uses the weights of the pre-trained\nmodel that are created previously. Adamw opitmzer\nand learning rate (lr= 0.00001) is used. Training batch\nsize is 32 and evaluation batch size of 16 is used in\nour experiments. We restricted sentence length to 512,\nlower-cased all words and saved model checked points\nat every 200 steps. Finally, we ﬁne-tuned our pre-\ntrained models for 5,336 training steps and used 512\nwarm-up steps during ﬁne-tuning process. All of the\nhyper-parameters used are same as default ALBERT\nunless stated otherwise. All the tested datasets con-\ntain a collection of words with a label “B” (”Begin-\nning”), “I” (“Inside”), and “O” (”Outside”) of an en-\ntity. For our experiments, we used these datasets as-is\nand passed to it our pre-trained models for the down-\nstream task. The adamw optimiser was used with eval-\nuation checkpoint so that the model will be evaluated\nat diﬀerent time intervals using the holdout develop-\nment dataset to identify the best model for ﬁnal pre-\ndictions. The predictions were performed on the test\ndatasets, and the performance is compared with base-\nline models that were established previously by calcu-\nlating the F1 Score, Precision and Recall. The sum-\nmary of all parameters used in ﬁne-tuning is given in\nTable 5.\nTable 5 Summary of parameters used in ﬁne–tuning\nSummary of All parameters used: (Fine-Tuning)\nName BioNER\nOptimizer used Adamw\nTraining Batch Size Size 32\nEvaluation Batch Size Size 16\nCheckpoint Saved 200\nMaximum Sentence Length 512\nLearning Rate (lr) 0.00001\nTraining Steps 5,336\nWarm-up Steps 320\nAbbreviations\nBioNER: Biomedical Named Entity Recognition; NLP: Natural lang uage\nprocessing; BiLSTM: Bi-directional long short-term memor y network;\nELMo: Embeddings from Language Model; CRF: Conditional ran dom ﬁelds;\nBERT: Bi-directional encoder representations from transf ormer; ALBERT:\nA Lite Bidirectional Encoder Representations from Transfo rmers; BioBERT:\nBiomedical Bi-directional encoder representations from t ransformer;\nBioALBERT: Biomedical A Lite Bidirectional Encoder Repres entations\nfrom Transformers; CDR: BioCreative V chemical disease rel ation corpus;\nCID: Chemical-induced disease;\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nAvailability of data and materials\nThe code and pre-trained models will be publicly available o n GitHub at\nhttps://github.com/usmaann/BioALBERT (Under Construct ion)\nCompeting interests\nThe authors declare that they have no competing interests.\nFunding\nNot applicable.\nAuthors’ contributions\nUN designed the methodology. UN, VR and SR implemented the mod el,\nperformed experiments and analyses. All authors participa ted in manuscript\npreparation. All authors read and approved the ﬁnal manuscr ipt.\nAcknowledgements\nNot applicable.\nAuthor details\n1School of Computer Science, University of Sydney, Australi a. 2School of\nInformation Technology, Deakin University, Australia. 3Telehealth and\nTechnology Centre, Nepean hospital, Australia.\nReferences\n1. Meystre, S.M., Savova, G.K., Kipper-Schuler, K.C., Hurdl e, J.F.:\nExtracting information from textual documents in the elect ronic health\nrecord: a review of recent research. Yearbook of medical inf ormatics\n17(01), 128–144 (2008)\n2. M˚ artensson, L., Hensing, G.: Health literacy–a heterogen eous\nphenomenon: a literature review. Scandinavian journal of c aring\nsciences 26(1), 151–160 (2012)\n3. Rosario, B., Hearst, M.: Classifying semantic relations i n bioscience\ntexts. In: Proceedings of the 42nd Annual Meeting of the Asso ciation\nfor Computational Linguistics (ACL-04), Barcelona, Spain , pp.\n430–437 (2004). doi:10.3115/1218955.1219010.\nhttps://www.aclweb.org/anthology/P04-1055\n4. Lim, S., Lee, K., Kang, J.: Drug drug interaction extracti on from the\nliterature using a recursive neural network. PLOS ONE 13, 0190926\n(2018). doi:10.1371/journal.pone.0190926\n5. Cohen, A.M., Hersh, W.R.: A survey of current work in biomed ical text\nmining. Brieﬁngs in bioinformatics 6(1), 57–71 (2005)\nNaseem et al. Page 8 of 8\n6. Yadav, V., Bethard, S.: A Survey on Recent Advances in Named Entity\nRecognition from Deep Learning models (2019). 1910.11470\n7. Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C ., Lee, K.,\nZettlemoyer, L.: Deep contextualized word representation s. CoRR\nabs/1802.05365 (2018). 1802.05365\n8. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: Pr e-training\nof deep bidirectional transformers for language understan ding. In:\nProceedings of the 2019 Conference of the North American Chap ter of\nthe Association for Computational Linguistics: Human Langu age\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171–41 86.\nAssociation for Computational Linguistics, Minneapolis, Minnesota\n(2019). doi:10.18653/v1/N19-1423.\nhttps://www.aclweb.org/anthology/N19-1423\n9. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Sor icut, R.:\nALBERT: A Lite BERT for Self-supervised Learning of Languag e\nRepresentations (2019). 1909.11942\n10. Pyysalo, S., Ginter, F., Moen, H., Salakoski, T., Ananiad ou, S.:\nDistributional semantics resources for biomedical text pr ocessing.\n(2013)\n11. Jin, Q., Dhingra, B., Cohen, W.W., Lu, X.: Probing Biomed ical\nEmbeddings from Language Models (2019). 1904.02181\n12. Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.H., Kang, J.:\nBioBERT: a pre-trained biomedical language representatio n model for\nbiomedical text mining (2019). 1901.08746\n13. Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dea n, J.:\nDistributed representations of words and phrases and their\ncompositionality. In: Advances in Neural Information Proce ssing\nSystems, pp. 3111–9 (2013)\n14. Chen, Q., Peng, Y., Lu, Z.: Biosentvec: creating sentenc e embeddings\nfor biomedical texts. 2019 IEEE International Conference o n\nHealthcare Informatics (ICHI), 1–5 (2018)\n15. Zhu, H., Paschalidis, I.C., Tahmasebi, A.M.: Clinical co ncept\nextraction with contextual word embedding. NIPS Machine Lea rning\nfor Health Workshop (2018)\n16. Beltagy, I., Lo, K., Cohan, A.: SciBERT: A Pretrained Lan guage\nModel for Scientiﬁc Text (2019). 1903.10676\n17. Si, Y., Wang, J., Xu, H., Roberts, K.: Enhancing clinical c oncept\nextraction with contextual embeddings. Journal of the Amer ican\nMedical Informatics Association 26(11), 1297–1304 (2019).\ndoi:10.1093/jamia/ocz096\n18. Peng, Y., Yan, S., Lu, Z.: Transfer Learning in Biomedica l Natural\nLanguage Processing: An Evaluation of BERT and ELMo on Ten\nBenchmarking Datasets (2019). 1906.05474\n19. Howard, J., Ruder, S.: Universal language model ﬁne-tuni ng for text\nclassiﬁcation. In: Proceedings of the 56th Annual Meeting o f the\nAssociation for Computational Linguistics (Volume 1: Long Papers),\npp. 328–339. Association for Computational Linguistics, M elbourne,\nAustralia (2018). doi:10.18653/v1/P18-1031.\nhttps://www.aclweb.org/anthology/P18-1031\n20. Yogatama, D., d’Autume, C.d.M., Connor, J., Kocisky, T. ,\nChrzanowski, M., Kong, L., Lazaridou, A., Ling, W., Yu, L., D yer, C.,\net al.: Learning and evaluating general linguistic intelli gence. arXiv\npreprint arXiv:1901.11373 (2019)\n21. Qiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., Huang, X.: Pre-tr ained\nmodels for natural language processing: A survey. arXiv pre print\narXiv:2003.08271 (2020)\n22. Liu, P., Qiu, X., Huang, X.: Recurrent neural network for t ext\nclassiﬁcation with multi-task learning. arXiv preprint ar Xiv:1605.05101\n(2016)\n23. Liu, X., He, P., Chen, W., Gao, J.: Multi-task deep neural n etworks for\nnatural language understanding. arXiv preprint arXiv:190 1.11504\n(2019)\n24. Xu, K., Yang, Z., Kang, P., Wang, Q., Liu, W.: Document-le vel\nattention-based bilstm-crf incorporating disease dictio nary for disease\nnamed entity recognition. Computers in biology and medicin e 108,\n122–132 (2019)\n25. Sachan, D.S., Xie, P., Sachan, M., Xing, E.P.: Eﬀective U se of\nBidirectional Language Modeling for Transfer Learning in B iomedical\nNamed Entity Recognition (2017). 1711.07908\n26. Lou, Y., Zhang, Y., Qian, T., Li, F., Xiong, S., Ji, D.: A\ntransition-based joint model for disease named entity reco gnition and\nnormalization. Bioinformatics (Oxford, England) 33 (2017).\ndoi:10.1093/bioinformatics/btx172\n27. Luo, L., Yang, Z., Yang, P., Zhang, Y., Wang, L., Lin, H., Wa ng, J.:\nAn attention-based bilstm-crf approach to document-level chemical\nnamed entity recognition. Bioinformatics 34, 1381–1388 (2018)\n28. Yoon, W., So, C.H., Lee, J., Kang, J.: Collabonet: collabo ration of\ndeep neural networks for biomedical named entity recogniti on. BMC\nBioinformatics 20(S10) (2019). doi:10.1186/s12859-019-2813-6\n29. Giorgi, J.M., Bader, G.D.: Transfer learning for biomed ical named\nentity recognition with neural networks. Bioinformatics 34(23),\n4087–4094 (2018)\n30. Doundeﬁnedan, R.I., Leaman, R., Lu, Z.: Ncbi disease corp us. J. of\nBiomedical Informatics 47(C), 1–10 (2014)\n31. Li, J., Sun, Y., Johnson, R.J., Sciaky, D., Wei, C.-H., Lea man, R.,\nDavis, A.P., Mattingly, C.J., Wiegers, T.C., Lu, Z.: Biocre ative v cdr\ntask corpus: a resource for chemical disease relation extra ction.\nDatabase : the journal of biological databases and curation 2016\n(2016)\n32. Krallinger, M., Rabal, O., Leitner, F., Vazquez, M., Sal gado, D., lu, Z.,\nLeaman, R., Lu, Y., Ji, D., Lowe, D., Sayle, R., Batista-Navar ro, R.,\nRak, R., Huber, T., Rockt¨ aschel, T., Matos, S., Campos, D., T ang, B.,\nXu, H., Valencia, A.: The chemdner corpus of chemicals and dru gs and\nits annotation principles. Journal of Cheminformatics 7, 2 (2015).\ndoi:10.1186/1758-2946-7-S1-S2\n33. Ando, R.K.: Biocreative ii gene mention tagging system a t ibm\nwatson. (2007)\n34. Kim, J.-D., Ohta, T., Tsuruoka, Y., Tateisi, Y., Collier , N.:\nIntroduction to the bio-entity recognition task at jnlpba. In:\nProceedings of the International Joint Workshop on Natural L anguage\nProcessing in Biomedicine and Its Applications. JNLPBA ’04, pp.\n70–75. Association for Computational Linguistics, USA (20 04)\n35. Gerner, M., Nenadic, G., Bergman, C.M.: Linnaeus: a speci es name\nidentiﬁcation system for biomedical literature. BMC bioin formatics\n11(1), 85 (2010)\n36. Paﬁlis, E., Frankild, S.P., Fanini, L., Faulwetter, S., Pavloudi, C.,\nVasileiadou, A., Arvanitidis, C., Jensen, L.J.: The specie s and\norganisms resources for fast and accurate identiﬁcation of taxonomic\nnames in text. PLOS ONE 8(6), 1–6 (2013).\ndoi:10.1371/journal.pone.0065390\nFigures\nFigure 1\nComparison of run-time statistics of BioALBERT v/s BioBERT\nFigure 2\nComparison of a) BioBERT v1.0 (Pubmed)-Base v/s BioALBERT 1.0 (Pubmed)-Base ; b) BioBERT\nv1.0(Pubmed+PMC)-Base v/s BioALBERT 1.0(Pubmed+PMC)-Base\nFigure 3\nComparison of different variants BioALBERT models\nFigure 4\nOverview of the pre-training and \u0000ne-tuning of BioALBERT on NER",
  "topic": "Simple (philosophy)",
  "concepts": [
    {
      "name": "Simple (philosophy)",
      "score": 0.6994442939758301
    },
    {
      "name": "Computer science",
      "score": 0.6615386009216309
    },
    {
      "name": "Natural language processing",
      "score": 0.6153662204742432
    },
    {
      "name": "Named-entity recognition",
      "score": 0.5976136326789856
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5109183192253113
    },
    {
      "name": "Language model",
      "score": 0.503851592540741
    },
    {
      "name": "Engineering",
      "score": 0.0924902856349945
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ]
}