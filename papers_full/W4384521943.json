{
  "title": "nl2spec: Interactively Translating Unstructured Natural Language to Temporal Logics with Large Language Models",
  "url": "https://openalex.org/W4384521943",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4384542860",
      "name": "Matthias Cosler",
      "affiliations": [
        "Helmholtz Center for Information Security"
      ]
    },
    {
      "id": "https://openalex.org/A1999795560",
      "name": "Christopher Hahn",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2100455838",
      "name": "Daniel Mendoza",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2800523426",
      "name": "Frederik Schmitt",
      "affiliations": [
        "Helmholtz Center for Information Security"
      ]
    },
    {
      "id": "https://openalex.org/A2229729172",
      "name": "Caroline Trippel",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A4384542860",
      "name": "Matthias Cosler",
      "affiliations": [
        "Helmholtz Center for Information Security"
      ]
    },
    {
      "id": "https://openalex.org/A2800523426",
      "name": "Frederik Schmitt",
      "affiliations": [
        "Helmholtz Center for Information Security"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963088785",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3200853289",
    "https://openalex.org/W4242741415",
    "https://openalex.org/W4384521943",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W6949881694",
    "https://openalex.org/W4382463985",
    "https://openalex.org/W3107164050",
    "https://openalex.org/W2807172739",
    "https://openalex.org/W2152863619",
    "https://openalex.org/W6602087570",
    "https://openalex.org/W2088655521",
    "https://openalex.org/W4284699363",
    "https://openalex.org/W4230220072",
    "https://openalex.org/W2159958244",
    "https://openalex.org/W1547304883",
    "https://openalex.org/W1688777878",
    "https://openalex.org/W2023808162",
    "https://openalex.org/W3178940883",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W567973435",
    "https://openalex.org/W6600741150",
    "https://openalex.org/W2979826702"
  ],
  "abstract": "Abstract A rigorous formalization of desired system requirements is indispensable when performing any verification task. This often limits the application of verification techniques, as writing formal specifications is an error-prone and time-consuming manual task. To facilitate this, we present , a framework for applying Large Language Models (LLMs) to derive formal specifications (in temporal logics) from unstructured natural language. In particular, we introduce a new methodology to detect and resolve the inherent ambiguity of system requirements in natural language: we utilize LLMs to map subformulas of the formalization back to the corresponding natural language fragments of the input. Users iteratively add, delete, and edit these sub-translations to amend erroneous formalizations, which is easier than manually redrafting the entire formalization. The framework is agnostic to specific application domains and can be extended to similar specification languages and new neural models. We perform a user study to obtain a challenging dataset, which we use to run experiments on the quality of translations. We provide an open-source implementation, including a web-based frontend.",
  "full_text": "nl2spec: Interactively Translating\nUnstructured Natural Language\nto Temporal Logics with Large Language\nModels\nMatthias Cosler2, Christopher Hahn1(B), Daniel Mendoza1(B),\nFrederik Schmitt2, and Caroline Trippel1\n1 Stanford University, Stanford, CA, USA\nhahn@cs.stanford.edu, {dmendo,trippel}@stanford.edu\n2 CISPA Helmholtz Center for Information Security,\nSaarbr¨ucken, Germany\n{matthias.cosler,frederik.schmitt}@cispa.de\nAbstract. A rigorous formalization of desired system requirements is\nindispensable when performing any veriﬁcation task. This often limits\nthe application of veriﬁcation techniques, as writing formal speciﬁcations\nis an error-prone and time-consuming manual task. To facilitate this,\nwe present nl2spec, a framework for applying Large Language Models\n(LLMs) to derive formal speciﬁcations (in temporal logics) from unstruc-\ntured natural language. In particular, we introduce a new methodology\nto detect and resolve the inherent ambiguity of system requirements in\nnatural language: we utilize LLMs to map subformulas of the formaliza-\ntion back to the corresponding natural language fragments of the input.\nUsers iteratively add, delete, and edit these sub-translations to amend\nerroneous formalizations, which is easier than manually redrafting the\nentire formalization. The framework is agnostic to speciﬁc application\ndomains and can be extended to similar speciﬁcation languages and new\nneural models. We perform a user study to obtain a challenging dataset,\nwhich we use to run experiments on the quality of translations. We pro-\nvide an open-source implementation, including a web-based frontend.\n1 Introduction\nA rigorous formalization of desired system requirements is indispensable when\nperforming any veriﬁcation-related task, such as model checking [7], synthesis [6],\nor runtime veriﬁcation [20]. Writing formal speciﬁcations, however, is an error-\nprone and time-consuming manual task typically reserved for experts in the ﬁeld.\nThis paper presents nl2spec, a framework, accompanied by a web-based tool,\nto facilitate and automate writing formal speciﬁcations (in LTL [34] and similar\ntemporal logics). The core contribution is a new methodology to decompose\nthe natural language input into sub-translations by utilizing Large Language\nModels (LLMs). The nl2spec framework provides an interface to interactively\nc⃝ The Author(s) 2023\nC. Enea and A. Lal (Eds.): CA V 2023, LNCS 13965, pp. 383–396, 2023.\nhttps://doi.org/10.1007/978-3-031-37703-7\n_18\n384 M. Cosler et al.\nFig. 1. A screenshot of the web-interface for nl2spec.\nadd, edit, and delete thesesub-translations instead of attempting to grapple with\nthe entire formalization at once (a feature that is sorely missing in similar work,\ne.g., [13,30]).\nFigure1 shows the web-based frontend of nl2spec. As an example, we con-\nsider the following system requirement given in natural language: “Globally,\ngrant 0 and grant 1 do not hold at the same time until it is allowed”. The tool\nautomatically translates the natural language speciﬁcation correctly into the\nLTL formula G((!((g0 & g1)) U a)) . Additionally, the tool generates sub-\ntranslations, such as the pair (“do not hold at the same time”, !(g0 & g1) ),\nwhich help in verifying the correctness of the translation.\nConsider, however, the following ambiguous example: “a holds until b holds\nor always a holds”. Human supervision is needed to resolve the ambiguity on\nthe operator precedence. This can be easily achieved withnl2spec by adding or\nediting a sub-translation using explicit parenthesis (see Sect. 4 for more details\nand examples). To capture such (and other types of) ambiguity in a benchmark\ndata set, we conducted an expert user study speciﬁcally asking for challenging\ntranslations of natural language sentences to LTL formulas.\nThe key insight in the design of nl2spec is that the process of translation\ncan be decomposed into many sub-translations automatically via LLMs, and\nthe decomposition into sub-translations allows users to easily resolve ambigu-\nous natural language and erroneous translations through interactively modifying\nsub-translations. The central goal of nl2spec is to keep the human supervision\nnl2spec 385\nminimal and eﬃcient. To this end, all translations are accompanied by a con-\nﬁdence score. Alternative suggestions for sub-translations can be chosen via a\ndrop-down menu and misleading sub-translations can be deleted before the next\nloop of the translation. We evaluate the end-to-end translation accuracy of our\nproposed methodology on the benchmark data set obtained from our expert\nuser study. Note that nl2spec can be applied to the user’s respective appli-\ncation domain to increase the quality of translation. As proof of concept, we\nprovide additional examples, including an example for STL [ 31] in the GitHub\nrepository\n1.\nnl2spec is agnostic to machine learning models and speciﬁc application\ndomains. We will discuss possible parameterizations and inputs of the tool in\nSect.3. We discuss our sub-translation methodology in more detail in Sect. 3.2\nand introduce an interactive few-shot prompting scheme for LLMs to generate\nthem. We evaluate the eﬀectiveness of the tool to resolve erroneous formaliza-\ntions in Sect. 4 on a data set obtained from conducting an expert user study.\nWe discuss limitations of the framework and conclude in Sect.5. For additional\ndetails, please refer to the complete version [8].\n2 Background and Related Work\n2.1 Natural Language to Linear-Time Temporal Logic\nLinear-time Temporal Logic (LTL) [34] is a temporal logic that forms the basis\nof many practical speciﬁcation languages, such as the IEEE property speciﬁca-\ntion language (PSL) [22], Signal Temporal Logic (STL) [31], or System Verilog\nAssertions (SVA) [43]. By focusing on the prototype temporal logic LTL, we\nkeep the nl2spec framework extendable to speciﬁcation languages in speciﬁc\napplication domains. LTL extends propositional logic with temporal modalities\nU (until) and X (next). There are several derived operators, such asFϕ ≡ trueUϕ\nand Gϕ ≡¬ F¬ϕ. Fϕ states that ϕ will eventually hold in the future and\nGϕ states that ϕ holds globally. Operators can be nested: GFϕ, for example,\nstates that ϕ has to occur inﬁnitely often. LTL speciﬁcations describe a sys-\ntems behavior and its interaction with an environment over time. For exam-\nple given a process 0 and a process 1 and a shared resource, the formula\nG(r\n0 → Fg0) ∧ G(r1 → Fg1) ∧ G¬(g0 ∧ g1) describes that whenever a process\nrequests (ri) access to a shared resource it will eventually be granted ( gi). The\nsubformula G¬(g0 ∧ g1) ensures that grants given are mutually exclusive.\nEarly work in translating natural language to temporal logics focused on\ngrammar-based approaches that could handle structured natural language [ 17,\n24]. A survey of earlier research before the advent of deep learning is provided\nin [4]. Other approaches include an interactive method using SMT solving and\nsemantic parsing [15], or structured temporal aspects in grounded robotics [ 45]\nand planning [ 32]. Neural networks have only recently been used to translate\n1 The tool is available at GitHub: https://github.com/realChrisHahn2/nl2spec.\n386 M. Cosler et al.\ninto temporal logics, e.g., by training a model for STL from scratch [ 21], ﬁne-\ntuning language models [19], or an approach to apply GPT-3 [ 13,30] in a one-\nshot fashion, where [ 13] output a restricted set of declare templates [ 33] that\ncan be translated to a fragment of LTLf [ 10]. Translating natural langauge to\nLTL has especially been of interest to the robotics community (see [ 16] for an\noverview), where datasets and application domains are, in contrast to our setting,\nbased on structured natural language. Independent of relying on structured data,\nall previous tools lack a detection and interactive resolving of the inerherent\nambiguity of natural language, which is the main contribution of our framework.\nRelated to our approach is recent work [26], where generated code is iteratively\nreﬁned to match desired outcomes based on human feedback.\n2.2 Large Language Models\nLLMs are large neural networks typically consisting of up to 176 billion parame-\nters. They are pre-trained on massive amounts of data, such as “The Pile” [14].\nExamples of LLMs include the GPT [36] and BERT [11] model families, open-\nsource models, such as T5 [ 38]a n dB l o o m[39], or commercial models, such as\nCodex [5]. LLMs are Transformers [42], which is the state of the art neural archi-\ntecture for natural language proccessing. Additionally, Transformers have shown\nremarkable performance when being applied to classical problems in veriﬁcation\n(e.g., [9,18,25,40]), reasoning (e.g., [28,50]), as well as the auto-formalization [35]\nof mathematics and formal speciﬁcations (e.g., [19,21,49]).\nIn language modelling, we model the probability of a sequence of tokens in a\ntext [41]. The joint probability of tokens in a text is generally expressed as [39]:\np(x)= p(x\n1,...,x T )=\nT∏\nt=1\np(xt|x<t) ,\nwhere x is the sequence of tokens, xt represents the t-th token, and x<t is the\nsequence of tokens preceding xt. We refer to this as an autoregressive language\nmodel that iteratively predicts the probability of the next token. Neural network\napproaches to language modelling have superseded classical approaches, such as\nn-grams [41]. Especially Transformers [42] were shown to be the most eﬀective\narchitecture at the time of writing [1,23,36].\nWhile ﬁne-tuning neural models on a speciﬁc translation task remains a valid\napproach showing also initial success in generalizing to unstructured natural lan-\nguage when translating to LTL [19], a common technique to obtain high perfor-\nmance with limited amount of labeled data is so-called “few-shot prompting” [3].\nThe language model is presented a natural language description of the task usu-\nally accompanied with a few examples that demonstrate the input-output behav-\nior. The framework presented in this paper relies on this technique. We describe\nthe proposed few-shot prompting scheme in detail in Sect. 3.2.\nCurrently implemented in the framework and used in the expert-user study\nare Codex and Bloom, which showed the best performance during testing.\nnl2spec 387\nCodex and GPT-3.5-turbo. Codex [5] is a GPT-3 variant that was initially of\nup to 12B parameters in size and ﬁne-tuned on code. The initial version of\nGPT-3 itself was trained on variations of Common Crawl,\n2 Webtext-2 [37], two\ninternet-based book corpora and Wikipedia [3]. The ﬁne-tuning dataset for the\nvanilla version Codex was collected in May 2020 from 54 million public software\nrepositories hosted on GitHub, using 159GB of training data for ﬁne-tuning. For\nour experiments, we used the commercial 2022 version of code-davinci-002,\nwhich is likely larger (in the 176B range\n3) than the vanilla codex models. GPT-\n3.5-turbo is the currently available follow-up model of GPT-3.\nBloom. Bloom [39] is an open-source LLM family available in diﬀerent sizes of\nup to 176B parameters trained on 46 natural languages and 13 programming\nlanguages. It was trained on the ROOTS corpus [27], a collection of 498 hugging-\nface [29,48] datasets consisting of 1 .61 terabytes of text. For our experiments,\nwe used the 176B version running on the huggingface inference API\n4.\n3 The nl2spec Framework\n3.1 Overview\nThe framework follows a standard frontend-backend implementation. Figure 2\nshows an overview of the implementation of nl2spec. Parts of the framework\nthat can be extended for further research or usage in practice are highlighted. The\nframework is implemented in Python 3 and ﬂask [44], a lightweight WSGI web\napplication framework. For the experiments in this paper, we use the OpenAI\nlibrary and huggingface (transformer) library [ 47]. We parse the LTL output\nformulas with a standard LTL parser [ 12]. The tool can either be run as a\ncommand line tool, or with the web-based frontend.\nThe frontend handles the interaction with a human-in-the-loop. The inter-\nface is structured in three views: the “Prompt”, “Sub-translations”, and “Final\nResult” view (see Fig. 1). The tool takes a natural language sentence, optional\nsub-translations, the model temperature, and number of runs as input. It pro-\nvides sub-translations, a conﬁdence score, alternative sub-translations and the\nﬁnal formalization as output. The frontend then allows for interactively select-\ning, editing, deleting, or adding sub-translations. The backend implements the\nhandling of the underlying neural models, the generation of the prompt, and\nthe ambiguity resolving, i.e., computing the conﬁdence score including alter-\nnative sub-translations and the interactive few-shot prompting algorithm (cf.\nSect.3.2). The framework is designed to have an easy interface to implement\nnew models and write domain-speciﬁc prompts. The prompt is a .txt ﬁle that\ncan be adjusted to speciﬁc domains to increase the quality of translations. To\napply the sub-translation reﬁnement methodology, however, the prompt needs to\nfollow our interactive prompting scheme, which we introduce in the next section.\n2 https://commoncrawl.org/.\n3 https://blog.eleuther.ai/gpt3-model-sizes/.\n4 https://huggingface.co/inference-api.\n388 M. Cosler et al.\nFrontend\nSub-translations\nSub-translations\nscores\nPrompts\nNeural Models\nBackend\nAmbiguity\nDetection\nNatural Language\nSub-translations\nSub-translations\nscores\nTemperature\nNumber of runs\nFormal LTL spec\nNatural Language\nTemperature\nNumber of runs\nFormal LTL spec\nFig. 2. Overview of the nl2spec framework with a human-in-the-loop: highlighted\nareas indicate parts of the framework that are eﬀortlessly extendable.\n3.2 Interactive Few-Shot Prompting\nThe core of the methodology is the decomposition of the natural language input\ninto sub-translations. We introduce an interactive prompting scheme that gener-\nates sub-translations using the underlying neural model and leverages the sub-\ntranslations to produce the ﬁnal translation. Algorithm 1 depicts a high-level\noverview of the interactive loop. The main idea is to give a human-in-the-loop\nthe options to add, edit, or delete sub-translations and feed them back into\nthe language models as “Given translations” in the prompt (see Fig. 3). After\nquerying a language model M with this prompt F, model speciﬁc parameters P\nand the interactive prompt that is computed in the loop, the model generates\na natural language explanation, a dictionary of sub-translations, and the ﬁnal\ntranslation. Notably, the model M can be queried multiple times as speciﬁed\nby the number of runs r, thereby generating multiple possible sub-translations.\nThe conﬁdence score of each sub-translation is computed as votes over multiple\nqueries and by default the sub-translation with the highest conﬁdence score is\nselected to be used as a given sub-translation in the next iteration. In the fron-\ntend, the user may view and select alternative generated sub-translations for\neach sub-translation via a drop-down menu (see Fig.1).\nFigure3 shows a generic prompt, that illustrates our methodology. The\nprompting scheme consists of three parts. The speciﬁcation language speciﬁc\npart (lines 1–4), the fewshot examples (lines 5–19), and the interactive prompt\nnl2spec 389\nminimal.txt\n1 Translate the following natural language sentences into an LTL formula and explain your\n2 translation step by step. Remember that X means \"next\", U means \"until\", G means\n3 \"globally\", F means \"finally\", which means GF means \"infinitely often\". The formula\n4 should only contain atomic propositions or operators &, , ->, <->, X, U, G, F.\n5 Natural Language: Globally if a holds then c is true until b. Given translations: {}\n6 Explanation: \"a holds\" from the input translates to the atomic proposition a.\n7 \"c is true until b\" from the input translates to the subformula c U b. \"if x then y\"\n8 translates to an implication x -> y, so \"if a holds then c is true until b\" translates\n9 to an implication a - >cUb . \"Globally\" from the input translates to the temporal\n10 operator G. Explanation dictionary: {\"a holds\" : \"a\", \"c is true until b\" : \"c U b\",\n11 \"if a holds then c is true until b\" : \"a -> c U b\", \"Globally\" : \"G\"} So the final\n12 LTL translation is G a -> c U b.FINISH Natural Language: Every request r is\n13 eventually followed by a grant g. Given translations: {} Explanation: \"Request r\"\n14 from the input translates to the atomic proposition r and \"grant g\" translates to the\n15 atomic proposition g. \"every\" means at every point in time, i.e., globally, \"never\"\n16 means at no point in time, and \"eventually\" translates to the temporal operator F.\n17 \"followed by\" is the natural language representation of an implication. Explanation\n18 dictionary: {\"Request r\" : \"r\", \"grant g\" : \"g\", \"every\" : \"G\", \"eventually\": \"F\",\n19 \"followed by\" : \"->\"} So the final LTL translation is G r -> F g.FINISH\nFig. 3. Prompt with minimal domain knowledge of LTL.\nincluding the natural language and sub-translation inputs (not displayed, given\nas input). The speciﬁcation language speciﬁc part leverages “chain-of-thought”\nprompt-engineering to elicit reasoning from large language models [46]. The key\nof nl2spec, however, is the setup of the few-shot examples. This minimal prompt\nconsists of two few-shot examples (lines 5–12 and 12–19). The end of an exam-\nple is indicated by the “FINISH” token, which is the stop token for the machine\nlearning models. A few-shot example innl2spec consists of the natural language\ninput (line 5), a dictionary of given translations, i.e., the sub-translations (line\n5), an explanation of the translation in natural language (line 6–10), an expla-\nnation dictionary, summarizing the sub-translations, and ﬁnally, the ﬁnal LTL\nformula.\nThis prompting scheme elicits sub-translations from the model, which serve\nas a ﬁne-grained explanation of the formalization. Note that sub-translations\nprovided in the prompt are neither unique nor exhaustive, but provide the con-\ntext for the language model to generate the correct formalization.\n4 Evaluation\nIn this section, we evaluate our framework and prompting methodology on a data\nset obtained by conducting an expert user study. To show the general applica-\nbility of this framework, we use theminimal prompt that includes only minimal\ndomain knowledge of the speciﬁcation language (see Fig. 3). This prompt has\nintentionally been written before conducting the expert user study. We lim-\nited the few-shot examples to two and even provided no few-shot example that\nincludes “given translations”. We use the minimal prompt to focus the evaluation\non the eﬀectiveness of our interactive sub-translation reﬁnement methodology in\n390 M. Cosler et al.\nAlgorithm 1: Interactive Few-shot Prompting Algorithm\n1 Input: Natural language S, Few-shot prompt F, set of given sub-translations\n(s, ϕ), and language model M\n2 Interactions: set of sub-translations (s, ϕ), conﬁdence scores C\n3 Set of Model speciﬁc parameter P: e.g., model-temperature t,n u m b e ro f\nruns r\n4 Output: LTL formulaψ that formalizes S\n1: ψ, (s, ϕ) ,C =e m p t y\n2: while user not approves LTL formulaψ do\n3: interactive prompt = compute prompt( S ,F ,(s, ϕ))\n4: ψ, (s, ϕ) ,C = query(M, P, interactive prompt)\n5: ( s, ϕ)= user interaction((s, ϕ) ,C )\n6: end while\n7: return ψ\nresolving ambiguity and ﬁxing erroneous translations. In practice, one would like\nto replace this minimal prompt with domain-speciﬁc examples that capture the\nunderlying distribution as closely as possible. As a proof of concept, we elaborate\non this in the full version [8].\n4.1 Study Setup\nTo obtain a benchmark dataset of unstructured natural language and their for-\nmalizations into LTL, we asked ﬁve experts in the ﬁeld to provide examples that\nthe experts thought are challenging for a neural translation approach. Unlike\nexisting datasets that follow strict grammatical and syntatical structure, we\nposed no such restrictions on the study participants. Each natural language\nspeciﬁcation was restricted to one sentence and to ﬁve atomic propositions\na, b, c, d, e. Note that nl2spec is not restricted to a speciﬁc set of atomic propo-\nsitions (cf. Fig. 1). Which variable scheme to use can be speciﬁed as an initial\nsub-translation. We elaborate on this in the full version [ 8]. To ensure unique\ninstances, the experts worked in a shared document, resulting in 36 benchmark\ninstances. We provide three randomly drawn examples for the interested reader:\nnatural language S LTL speciﬁcation ψ\nIf b holds then, in the next step, c holds until a holds or always c holds b- >X( ( cUa )| |Gc )\nIf b holds at some point, a has to hold somewhere beforehand (F b) -> (!b U (a & !b))\nOne of the following aps will hold at all instances: a,b,c G (a|b|c )\nThe poor performance of existing methods (cf. Table 1) exemplify the diﬃ-\nculty of this data set.\n4.2 Results\nWe evaluated our approach using theminimal prompt (if not otherwise stated),\nwith number of runs set to three and with a temperature of 0.2.\nnl2spec 391\nQuality of Initial Translation. We analyze the quality ofinitial translations, i.e.,\ntranslations obtained before any human interaction. This experiment demon-\nstrates that the initial translations are of high quality, which is important to\nensure an eﬃcient workﬂow. We compared our approach to ﬁne-tuning language\nmodels on structured data [19] and to an approach using GPT-3 or Rasa [2]t o\ntranslate natural language into a restricted set of declare patterns [ 13] (which\ncould not handle most of the instances in the benchmark data set, even when\nreplacing the atomic propositions with their used entities). The results of eval-\nuating the accuracy of the initial translations on our benchmark expert set is\nshown in Table1.\nAt the time of writing, using Codex in the backend outperforms GPT-3.5-\nturbo and Bloom on this task, by correctly translating 44 .4% of the instances\nusing the minimal prompt. We only count an instance as correctly translated\nif it matches the intended meaning of the expert, no alternative translation\nto ambiguous input was accepted. Additionally to the experiments using the\nminimal prompt, we conducted experiments on an augmented prompt with in-\ndistribution examples after the user study was conducted by randomly drawing\nfour examples from the expert data set (3 of the examples haven’t been solved\nbefore, see the GitHub repository or full version for more details). With this in-\ndistribution prompt (ID), the tool translates 21 instances (with the four drawn\nexamples remaining in the set), i.e., 58.3% correctly.\nThis experiment shows 1) that the initial translation quality is high and\ncan handle unstructured natural language better than previous approaches and\n2) that drawing the few-shot examples in distribution only slightly increased\ntranslation quality for this data set; making the key contributions of nl2spec,\ni.e., ambiguity detection and eﬀortless debugging of erroneous formalizations,\nvaluable. Since nl2spec is agnostic to the underlying machine learning models,\nwe expect an even better performance in the future with more ﬁne-tuned models.\nTeacher-Student Experiment. In this experiment, we generate an initial set of\nsub-translations with Codex as the underlying neural model. We then ran the\ntool with Bloom as a backend, taking these sub-translations as input. There were\n11 instances that Codex could solve initially that Bloom was unable to solve. On\nthese instances, Bloom was able to solve 4 more instances, i.e., 36.4% with sub-\ntranslations provided by Codex. The four instances that Bloom was able to solve\nTable 1. Translation accuracy on the benchmark data set, where B stands for Bloom\nand C stands for Codex and G for GPT-3.5-Turbo.\nnl2ltl [13] T-5 [19] nl2spec+B nl2spec+C nl2spec+C nl2spec+C\nrasa fine-tuned initial initial initial+ID interactive\n1/36 (2.7%) 2/36 (5.5%) 5/36 (13.8%) 16/36 (44.4%) 21/36 (58.3%) 31/36 (86.1%)\n– – – nl2spec+G nl2spec+G nl2spec+G\ninitial initial+ID interactive\n– – – 12/36 (33.3%) 17/36 (47.2%) 21/36 (58.3%)\n392 M. Cosler et al.\nwith the help of Codex were: “It is never the case that a and b hold at the same\ntime.”, “Whenever a is enabled, b is enabled three steps later.”, “If it is the case\nthat every a is eventually followed by a b, then c needs to holds inﬁnitely often.”,\nand “One of the following aps will hold at all instances: a,b,c”. This demonstrates\nthat our sub-translation methodology is a valid appraoch: improving the quality\nof the sub-translations indeed has a positive eﬀect on the quality of the ﬁnal\nformalization. This even holds true when using underperforming neural network\nmodels. Note that no supervision by a human was needed in this experiment to\nimprove the formalization quality.\nAmbiguity Detection. Out of the 36 instances in the benchmark set, at least 9 of\nthe instances contain ambiguous natural language. We especially observed two\nclasses of ambiguity: 1) ambiguity due to the limits of natural language, e.g.,\noperator precedence, and 2) ambiguity in the semantics of natural language;\nnl2spec can help in resolving both types of ambiguity. Details for the following\nexamples can be found in the full version [8].\nAn example for the ﬁrst type of ambiguity from our dataset is the example\nmentioned in the introduction: “a holds until b holds or always a holds”, which\nthe expert translated into ( aUb )|Ga . Running the tool, however, trans-\nlated this example into (a U (b | G(a))) . By editting the sub-translation of\n“a holds until b holds” to(a U b) through adding explicit parenthesis, the tool\ntranslates as intended. An example for the second type of ambiguity is the follow-\ning instance from our data set: “Whenever a holds, b must hold in the next two\nsteps.” The intended meaning of the expert was G( a- >( b|Xb ) ) , whereas\nthe tool translated this sentence into G((a -> X(X(b)))) . After changing the\nsub-translation of “b must hold in the next two steps” to b|Xb , the tool\ntranslates the input as intended.\nFixing Erroneous Translation. With the inherent ambiguity of natural lan-\nguage and the unstructured nature of the input, the tool’s translation cannot\nbe expected to be always correct in the ﬁrst try. Verifying and debugging sub-\ntranslations, however, is signiﬁcantly easier than redrafting the complete for-\nmula from scratch. Twenty instances of the data set were not correctly trans-\nlated in an initial attempt using Codex and the minimal prompt in the backend\n(see Table1). We were able to extract correct translations for 15 instances by\nperforming at most three translation loops (i.e., adding, editing, and removing\nsub-translations), We were able to get correct results by performing 1.86 trans-\nlation loops on average. For example, consider the instance, “whenever a holds,\nb holds as well”, which the tool mistakenly translated to G(a & b) .B yﬁ x i n g\nthe sub-translation “b holds as well” to the formula fragment-> b , the sentence\nis translated as intended. Only the remaining ﬁve instances that contain highly\ncomplex natural language requirements, such as, “once a happened, b won’t\nhappen again” were need to be translated by hand.\nIn total, we correctly translated 31 out of 36 instances, i.e., 86.11% using the\nnl2spec sub-translation methodology by performing only 1.4 translation loops\non average (see Table1).\nnl2spec 393\n5 Conclusion\nWe presented nl2spec, a framework for translating unstructured natural lan-\nguage to temporal logics. A limitation of this approach is its reliance on compu-\ntational resources at inference time. This is a general limitation when applying\ndeep learning techniques. Both, commercial and open-source models, however,\nprovide easily accessible APIs to their models. Additionally, the quality of initial\ntranslations might be inﬂuenced by the amount of training data on logics, code,\nor math that the underlying neural models have seen during pre-training.\nAt the core ofnl2spec lies a methodology to decompose the natural language\ninput into sub-translations, which are mappings of formula fragments to relevant\nparts of the natural language input. We introduced an interactive prompting\nscheme that queries LLMs for sub-translations, and implemented an interface\nfor users to interactively add, edit, and delete the sub-translations, which avoids\nusers from manually redrafting the entire formalization to ﬁx erroneous transla-\ntions. We conducted a user study, showing thatnl2spec can be eﬃciently used\nto interactively formalize unstructured and ambigous natural language.\nAcknowledgements. We thank OpenAI for providing academic access to Codex and\nClark Barrett for helpful feedback on an earlier version of the tool.\nReferences\n1. Al-Rfou, R., Choe, D., Constant, N., Guo, M., Jones, L.: Character-level language\nmodeling with deeper self-attention. In: Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, vol. 33, pp. 3159–3166 (2019)\n2. Bocklisch, T., Faulkner, J., Pawlowski, N., Nichol, A.: Rasa: open source language\nunderstanding and dialogue management. arXiv preprintarXiv:1712.05181 (2017)\n3. Brown, T., et al.: Language models are few-shot learners. Adv. Neural Inf. Process.\nSyst. 33, 1877–1901 (2020)\n4. Brunello, A., Montanari, A., Reynolds, M.: Synthesis of ltl formulas from natural\nlanguage texts: state of the art and research directions. In: 26th International\nSymposium on Temporal Representation and Reasoning (TIME 2019). Schloss\nDagstuhl-Leibniz-Zentrum fuer Informatik (2019)\n5. Chen, M., et al.: Evaluating large language models trained on code. arXiv preprint\narXiv:2107.03374 (2021)\n6. Church, A.: Application of recursive arithmetic to the problem of circuit synthesis.\nJ. Symb. Logic 28(4) (1963)\n7. Clarke, E.M.: Model checking. In: Ramesh, S., Sivakumar, G. (eds.) FSTTCS 1997.\nLNCS, vol. 1346, pp. 54–56. Springer, Heidelberg (1997).https://doi.org/10.1007/\nBFb0058022\n8. Cosler, M., Hahn, C., Mendoza, D., Schmitt, F., Trippel, C.: nl2spec: interactively\ntranslating unstructured natural language to temporal logics with large language\nmodels. arXiv preprint arXiv:2303.04864 (2023)\n9. Cosler, M., Schmitt, F., Hahn, C., Finkbeiner, B.: Iterative circuit repair against\nformal speciﬁcations. In: International Conference on Learning Representations (to\nappear) (2023)\n394 M. Cosler et al.\n10. De Giacomo, G., Vardi, M.Y.: Linear temporal logic and linear dynamic logic\non ﬁnite traces. In: IJCAI 2013 Proceedings of the Twenty-Third international\njoint conference on Artiﬁcial Intelligence, pp. 854–860. Association for Computing\nMachinery (2013)\n11. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n12. Fuggitti, F.: LTLf2DFA. Zenodo (2019). https://doi.org/10.5281/ZENODO.\n3888410, https://zenodo.org/record/3888410\n13. Fuggitti, F., Chakraborti, T.: Nl2ltl-a python package for converting natural lan-\nguage (nl) instructions to linear temporal logic (ltl) formulas (2023)\n14. Gao, L., et al.: The pile: an 800 gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027 (2020)\n15. Gavran, I., Darulova, E., Majumdar, R.: Interactive synthesis of temporal spec-\niﬁcations from examples and natural language. Proc. ACM Program. Lang.\n4(OOPSLA), 1–26 (2020)\n16. Gopalan, N., Arumugam, D., Wong, L.L., Tellex, S.: Sequence-to-sequence lan-\nguage grounding of non-markovian task speciﬁcations. In: Robotics: Science and\nSystems, vol. 2018 (2018)\n17. Grunske, L.: Speciﬁcation patterns for probabilistic quality properties. In: 2008\nACM/IEEE 30th International Conference on Software Engineering, pp. 31–40.\nIEEE (2008)\n18. Hahn, C., Schmitt, F., Kreber, J.U., Rabe, M.N., Finkbeiner, B.: Teaching tempo-\nral logics to neural networks. In: International Conference on Learning Represen-\ntations (2021)\n19. Hahn, C., Schmitt, F., Tillman, J.J., Metzger, N., Siber, J., Finkbeiner, B.: Formal\nspeciﬁcations from natural language. arXiv preprint arXiv:2206.01962 (2022)\n20. Havelund, K., Ro¸su, G.: Monitoring java programs with java pathexplorer. Elec-\ntron. Notes Theor. Comput. Sci. 55(2), 200–217 (2001)\n21. He, J., Bartocci, E., Niˇckovi´c, D., Isakovic, H., Grosu, R.: Deepstl: from english\nrequirements to signal temporal logic. In: Proceedings of the 44th International\nConference on Software Engineering, pp. 610–622 (2022)\n22. IEEE-Commission, et al.: IEEE standard for property speciﬁcation language\n(PSL). IEEE Std 1850–2005 (2005)\n23. Kaplan, J., et al.: Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361 (2020)\n24. Konrad, S., Cheng, B.H.: Real-time speciﬁcation patterns. In: Proceedings of the\n27th International Conference on Software Engineering, pp. 372–381 (2005)\n25. Kreber, J.U., Hahn, C.: Generating symbolic reasoning problems with transformer\ngans. arXiv preprint arXiv:2110.10054 (2021)\n26. Lahiri, S.K., et al.: Interactive code generation via test-driven user-intent formal-\nization. arXiv preprint arXiv:2208.05950 (2022)\n27. Lauren¸con, H., et al.: The bigscience roots corpus: a 1.6 tb composite multilingual\ndataset. In: Thirty-sixth Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track (2022)\n28. Lewkowycz, A., et al.: Solving quantitative reasoning problems with language mod-\nels. arXiv preprint arXiv:2206.14858 (2022)\n29. Lhoest, Q., et al.: Datasets: a community library for natural language processing.\narXiv preprint arXiv:2109.02846 (2021)\nnl2spec 395\n30. Liu, J.X., et al.: Lang2ltl: translating natural language commands to temporal\nspeciﬁcation with large language models. In: Workshop on Language and Robotics\nat CoRL 2022\n31. Maler, O., Nickovic, D.: Monitoring temporal properties of continuous signals. In:\nLakhnech, Y., Yovine, S. (eds.) FORMATS/FTRTFT -2004. LNCS, vol. 3253, pp.\n152–166. Springer, Heidelberg (2004). https://doi.org/10.1007/978-3-540-30206-\n3\n12\n32. Patel, R., Pavlick, R., Tellex, S.: Learning to ground language to temporal logical\nform. In: NAACL (2019)\n33. Pesic, M., van der Aalst, W.M.P.: A declarative approach for ﬂexible business\nprocesses management. In: Eder, J., Dustdar, S. (eds.) BPM 2006. LNCS, vol. 4103,\npp. 169–180. Springer, Heidelberg (2006). https://doi.org/10.1007/11837862 18\n34. Pnueli, A.: The temporal logic of programs. In: 18th Annual Symposium on Foun-\ndations of Computer Science (sfcs 1977), pp. 46–57. IEEE (1977)\n35. Rabe, M.N., Szegedy, C.: Towards the automatic mathematician. In: Platzer, A.,\nSutcliﬀe, G. (eds.) CADE 2021. LNCS (LNAI), vol. 12699, pp. 25–37. Springer,\nCham (2021). https://doi.org/10.1007/978-3-030-79876-5 2\n36. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving lan-\nguage understanding by generative pre-training (2018)\n37. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language\nmodels are unsupervised multitask learners. OpenAI blog 1(8), 9 (2019)\n38. Raﬀel, C., et al.: Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. J. Mach. Learn. Res. 21(140), 1–67 (2020). http://jmlr.org/papers/\nv21/20-074.html\n39. Scao, T.L., et al.: Bloom: a 176b-parameter open-access multilingual language\nmodel. arXiv preprint arXiv:2211.05100 (2022)\n40. Schmitt, F., Hahn, C., Rabe, M.N., Finkbeiner, B.: Neural circuit synthesis from\nspeciﬁcation patterns. Adv. Neural Inf. Process. Syst. 34, 15408–15420 (2021)\n41. Shannon, C.E.: A mathematical theory of communication. Bell Syst. Tech. J.27(3),\n379–423 (1948)\n42. Vaswani, A., et al.: Attention is all you need. Adv. Neural Inf. Process. Syst. 30\n(2017)\n43. Vijayaraghavan, S., Ramanathan, M.: A Practical Guide for SystemVerilog Asser-\ntions. Springer, Heidelberg (2005). https://doi.org/10.1007/b137011\n44. Vyshnavi, V.R., Malik, A.: Eﬃcient way of web development using python and\nﬂask. Int. J. Recent Res. Asp 6(2), 16–19 (2019)\n45. Wang, C., Ross, C., Kuo, Y.L., Katz, B., Barbu, A.: Learning a natural-\nlanguage to ltl executable semantic parser for grounded robotics. arXiv preprint\narXiv:2008.03277 (2020)\n46. Wei, J., et al.: Chain of thought prompting elicits reasoning in large language\nmodels. arXiv preprint arXiv:2201.11903 (2022)\n47. Wolf, T., et al.: Huggingface’s transformers: state-of-the-art natural language pro-\ncessing. arXiv preprint arXiv:1910.03771 (2019)\n48. Wolf, T., et al.: Transformers: State-of-the-art natural language processing. In:\nProceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pp. 38–45 (2020)\n49. Wu, Y., et al.: Autoformalization with large language models. arXiv preprint\narXiv:2205.12615 (2022)\n50. Zelikman, E., Wu, Y., Goodman, N.D.: Star: bootstrapping reasoning with reason-\ning. arXiv preprint arXiv:2203.14465 (2022)\n396 M. Cosler et al.\nOpen Access This chapter is licensed under the terms of the Creative Commons\nAttribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium\nor format, as long as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons license and indicate if changes were\nmade.\nThe images or other third party material in this chapter are included in the\nchapter’s Creative Commons license, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the chapter’s Creative Commons license and\nyour intended use is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright holder.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9106875658035278
    },
    {
      "name": "Natural language",
      "score": 0.6866413950920105
    },
    {
      "name": "Task (project management)",
      "score": 0.6681268215179443
    },
    {
      "name": "Ambiguity",
      "score": 0.6621729731559753
    },
    {
      "name": "Programming language",
      "score": 0.5434882640838623
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48858174681663513
    },
    {
      "name": "Natural language processing",
      "score": 0.4278607666492462
    },
    {
      "name": "Formal specification",
      "score": 0.4196043014526367
    },
    {
      "name": "Formal language",
      "score": 0.4183775782585144
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210128801",
      "name": "Helmholtz Center for Information Security",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ]
}