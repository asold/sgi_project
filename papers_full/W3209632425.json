{
  "title": "A Simple and Effective Positional Encoding for Transformers",
  "url": "https://openalex.org/W3209632425",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5089555603",
      "name": "Pu-Chin Chen",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2135515116",
      "name": "Henry Tsai",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A264277963",
      "name": "Srinadh Bhojanapalli",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2104689355",
      "name": "Hyung Won Chung",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5100894936",
      "name": "Yin-Wen Chang",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5030156442",
      "name": "Chun-Sung Ferng",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3037983807",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3102094970",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2995273672",
    "https://openalex.org/W3102892879",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W2995744795",
    "https://openalex.org/W3066373881",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W4297747548",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2885185669",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W3033188311",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3122515622"
  ],
  "abstract": "Transformer models are permutation equivariant. To supply the order and type information of the input tokens, position and segment embeddings are usually added to the input. Recent works proposed variations of positional encodings with relative position encodings achieving better performance. Our analysis shows that the gain actually comes from moving positional information to attention layer from the input. Motivated by this, we introduce Decoupled Positional Attention for Transformers (DIET), a simple yet effective mechanism to encode position and segment information into the Transformer models. The proposed method has faster training and inference time, while achieving competitive performance on GLUE, XTREME and WMT benchmarks. We further generalize our method to long-range transformers and show performance gain.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2974–2988\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n2974\nA Simple and Effective Positional Encoding for Transformers\nPu-Chin Chen∗, Henry Tsai∗, Srinadh Bhojanapalli∗,\nHyung Won Chung, Yin-Wen Chang, Chun-Sung Ferng\nGoogle Research\nAbstract\nTransformer models are permutation equivari-\nant. To supply the order and type informa-\ntion of the input tokens, position and seg-\nment embeddings are usually added to the in-\nput. Recent works proposed variations of po-\nsitional encodings with relative position en-\ncodings achieving better performance. Our\nanalysis shows that the gain actually comes\nfrom moving positional information to atten-\ntion layer from the input. Motivated by this,\nwe introduce Decoupled posItional attEntion\nfor Transformers (DIET), a simple yet effec-\ntive mechanism to encode position and seg-\nment information into the Transformer mod-\nels. The proposed method has faster training\nand inference time, while achieving compet-\nitive performance on GLUE, XTREME and\nWMT benchmarks. We further generalize our\nmethod to long-range transformers and show\nperformance gain.\n1 Introduction\nTransformers are sequence-to-sequence models\nthat achieve state of the art performance in many\nNatural Language Processing (NLP) tasks, such as\nmachine translation, language modeling and ques-\ntion answering (Vaswani et al., 2017; Devlin et al.,\n2018; Yang et al., 2019; Liu et al., 2020). Trans-\nformers have two major components: self-attention\nand a position-wise feed forward layer. Both are\npermutation equivariant and are not sensitive to\nthe order of input tokens. To make these mod-\nels position-aware, the position information of the\ninput words is typically added as an additional em-\nbedding to the input token embeddings (Vaswani\net al., 2017). For example, input embedding (W)\nof a sentence is added to the position embeddings\n(P), resulting in input W + P to the Transformer.\nThese position embeddings only depend on the lo-\ncation the word appears. For multi-segment tasks,\n∗ The authors contribute equally to this paper. Corre-\nsponding author email: puchin@google.com\nadditional segment embeddings can be added just\nlike the position embeddings (Devlin et al., 2018).\nThere have been multiple works exploring differ-\nent ways to include position information in Trans-\nformers (Shaw et al., 2018; Yang et al., 2019; Raf-\nfel et al., 2020). Many of those note the advan-\ntages of using a relative position encoding scheme\nover absolute position encodings (see also Fig 1).\nHowever what causes this difference is not clear.\nYun et al. (2020) have shown that Transformers\nwith absolute position encodings are universal ap-\nproximators of all sequence to sequence functions,\nproving that absolute position encodings can cap-\nture the position information. Hence what causes\nthe superiority of relative position encodings? A\nsystematic study and understanding of the beneﬁts\nand drawbacks of different position encoding meth-\nods is missing. Ke et al. (2020) hypothesised that\nthe cross correlation between word and position\nembeddings while computing attention could be\nthe cause of poor performance of absolute position\nencodings. However such cross terms are present\nin some of the relative position encoding methods\n(Shaw et al., 2018; Yang et al., 2019), and these\nmethods perform on par or better than the other\nposition encoding schemes (see §4).\nIn this paper we undertake a systematic study\nto understand different position encoding methods.\nWe argue that absolute position embeddings mainly\nsuffer from being added at the input. We show, with\nour experiments on classiﬁcation, question answer-\ning and machine translation tasks, that absolute po-\nsition encodings added to attention matrices with\ndifferent parameters for each head improves sig-\nniﬁcantly over absolute position encodings added\nto the input. This highlights that where the posi-\ntion information is included in the Transformer is\nimportant, providing an explanation for the gap in\nperformance between absolute and relative posi-\ntion encodings. We also compare different position\nencodings and the effect of sharing position encod-\n2975\n(a) English Transfer Learning on\nMultiNLI\n(b) Cross-lingual Transfer on\nXNLI\n(c) Translation on CS-EN\nFigure 1: Performance effect of different positional encoding methods for Transformers (see § 2) on two Natural\nlanguage Inference datasets from GLUE (Wang et al., 2019), XTREME (Hu et al., 2020) and one Neural Machine\nTranslation dataset WMT 18 (Bojar et al., 2018). Absolute positional encoding (D IET-ABS) can achieve better\nperformance than the relative counterpart (D IET-REL), showing the importance of designing the right position\nencoding method.\nings across different heads and layers of a Trans-\nformer. Based on these observations we propose\ndecoupled positional attention and a new segment\nencoding approach (for tasks with multiple seg-\nments), and empirically show its superiority.\nWe summarize our contributions in this paper\nbelow.\n•We theoretically and empirically analyze the\nlimitation of the absolute position embeddings\nadded to the input. For both absolute and\nrelative information, we show that encoding\nposition to attention matrix per-head results\nin superior performance.\n•We propose a simple and efﬁcient way to en-\ncode position and segment information. The\nproposed encoding matches the SoTA meth-\nods on multiple standard NLP tasks while\nhaving a simpler model with lower train-\ning/inference costs.\n•Our proposed method can be easily applied to\nlong sequence models ( DIET-ABSLIN ) and\nimprove all metrics compared with Linformer\n(Wang et al., 2020).\n•We present ablation studies comparing differ-\nent position encoding methods and ways of\nsharing position encoding parameters across\nheads and layers in Transformer.\n2 Position Encoding for Transformers\nIn this section, we brieﬂy review the Transformer\nmodels (Vaswani et al., 2017) and discuss previ-\nous improvement of position encoding and analyze\nthe limitation of the additive position embedding\nproposed in the initial and widely-adopted Trans-\nformer model.\n2.1 Transformer\nA Transformer block consists of two types of layers:\n1) Self-attention layer and 2) Feed forward layers.\nSelf-Attention Module Given input sequence\nlength n, hidden size d, multi-head query-key\ndown-projection size dh, we deﬁne hidden layer\ninput to this attention head asX ∈Rn×d, the query\nprojection matrix as Wi\nQ ∈Rd×dh, the key projec-\ntion matrix as Wi\nK ∈Rd×dh and the value projec-\ntion matrix as Wi\nV ∈Rd×dh, i ∈[h], for h heads.\nUsually, dh < d as we do multi-head attention\nwith a smaller representation per head (dh = d/h).\nWith that we can write dot-product attention score:\nAi = (XWi\nQ)(XWi\nK)⊤\nThis attention score is used to compute the output\nfor each head, after scaling and per row normaliza-\ntion using softmax:\nheadi = Softmax(Ai/\n√\nd) ·(XWi\nV )\nOutput of all attention heads in a layer are concate-\nnated and passed to the next feed-forward layer\napplied token-wise.\n2.2 Position Aware Self Attention\nMany NLP tasks, such as machine translation, lan-\nguage modeling, are sensitive to the ordering of\ninput words. Since Transformers are permutation\nequivariant, we usually additionally include the po-\nsition information in the input. Below we discuss\nsome of the popular position encoding methods.\n2.2.1 Absolute Position Encodings\nAbsolute position encodings are computed in the\ninput layer and are summed with the input token\nembeddings. Vaswani et al. (2017) proposed this\n2976\nfor Transformers and it has been a popular choice\nin the followup works (Radford et al., 2018; Devlin\net al., 2018). There are two common variations of\nthe absolute position encodings - ﬁxed and learned.\n2.2.2 Relative Position Encodings\nOne drawback of absolute position encoding is that\nit requires ﬁxed length of input sequence and does\nnot directly capture relative positions to each word.\nTo solve these problems several relative positions\nschemes have been proposed.\nShaw et al. (2018) proposed using relative posi-\ntion encoding instead of absolute position encoding,\nand add position embeddings to the key and option-\nally value projections instead of the input. They\nshow that this new way of encoding position in-\nformation leads to better performance on machine\ntranslation tasks. Yang et al. (2019) simpliﬁed this\nby removing the position embeddings in value pro-\njections and showed better performance on the lan-\nguage modeling tasks. Both these approaches use\na vector representation to encode position informa-\ntion.\nRaffel et al. (2020) use scalars to encode rela-\ntive position between query and key indices and\nadd directly to the attention scores matrix. They\nfurther use logarithmic binning of position infor-\nmation into a ﬁxed number of buckets. All these\nrelative position methods further share the position\nencoding parameters across layers.\nRecently Ke et al. (2020) hypothesised that the\ncross correlation between position and token em-\nbeddings can result in weaker performance of ad-\nditive absolute position embeddings and instead\nproposed to add both absolute and relative posi-\ntional information based attention directly in each\nhead. However such cross terms are present in the\nmethod proposed by Shaw et al. (2018), which does\ncompetitively with other approaches. We instead\nhypothesise that position encodings at input limit\nthe rank of the position attention matrix leading to\nits poor performance.\n2.3 Limitations of the Input Additive\nPosition Embedding\nIn this section we discuss some limitations of the\nde facto way of adding absolute position encodings\nto the input token embeddings.\nWe ﬁrst compare the representation power in\nterms of the rank of attention matrices achievable\nwith different position encodings.\nFigure 2: Rank of attention matrices: We present a\ncomparison of the rank of the attention score matrices\nof a BERT BASE model with absolute position embed-\ndings at input v.s. absolute position embeddings per-\nhead (DIET-ABS (1)). With additive positional embed-\nding at input, the attention matrices have much lower\nrank, limiting the representative power. This is allevi-\nated by DIET-ABS.\nTheorem 1. Let P ∈Rn×d be the input position\nembedding and ˆP ∈Rn×dp be the layer-wise po-\nsition embeddings. Let WQ, WK ∈ Rd×dh be\nthe query and key projection matrices with head\nprojection size dh, and dh < dp, dand n ≥\ndh + dp. Let Aa = (X + P)WQW⊤\nK(X + P)⊤\nand Ar = XWQW⊤\nKX⊤+ ˆPˆP⊤be the atten-\ntion matrices computed using input and layer-wise\nposition embeddings respectively. Then for any\nX, P, WQ, WK\nrank(Aa) ≤dh.\nThere exists a choice ofX, ˆP, WQ, WK such that\nrank(Ar) =dp + dh > dh.\nRemarks. This theorem shows us that the rank of\nattention matrices is constrained with the absolute\nposition encodings at the input and using per-head\nposition encodings by adding position information\nto attention matrix directly results in allowing for\nhigher rank attention. See § B for the proof.\nAdding the position encodings directly to the in-\nput further places a constraint on training dynamics\nby forcing gradients to be same for both the input\ntoken and position embeddings (see § B). Relative\nposition encodings discussed earlier, while address-\ning some of these concerns, suffer from slower\ntraining/inference times (see Table 1) with com-\nplex implementations (Shaw et al. (2018); Ke et al.\n(2020)). In the next section, we present simple posi-\ntion encoding methods that avoid these limitations.\n2977\n3 Proposed Position and Segment\nEncodings\nIn the previous section, we learned about the limi-\ntations of input additive positional embeddings and\nexisting works. Based on these observations, we\npropose two minimal/efﬁcient ways to incorporate\n(absolute/relative) positional encodings along with\na novel absolute segment encoding approach. By\ndecoupling position and segment from token em-\nbeddings we match the SoTA performance while\nimproving training/inference time (see §3.3).\n3.1 Decoupled Absolute Positional Attention\nWe propose the following simple absolute position\nencoding method that adds position information to\nthe token attention matrix directly in each attention\nhead. We further also add segment information to\nthe token attention instead of the input embeddings.\nThis way we can set the rank of position encodings\nindependently resulting in higher rank attention\nmatrix, addressing the limitations discussed earlier.\nDIET-ABS\nAABS\ni,j = (Xi:WQ)(Xj:WK)⊤/\n√\nd\n+ (PQP⊤\nK)i,j + ES(S(i), S(j)),\n(1)\nwhere PQ, PK ∈Rn×dp are low-rank position em-\nbedding matrices and ES is the absolute segment\nattention to model interactions between segments\ndeﬁned as\nES(S(i), S(j)) =Sˆi,ˆj\nwhere S(i) =ˆi if index i is in segment ˆi.\n(2)\nPlease note that we use the following notation\nin the above equation. Ai,j denotes the (i, j) entry\nof matrix A. Xi: and X:j denote the ith row and\njth column of X respectively. We will follow this\nnotation in the remainder of the paper.\nBy default, we set dp same as dh. This already\nresults in potentially a rankdp+dh attention matrix\nas shown in Theorem 1. To illustrate this, we com-\npare the rank of the attention matrices in the ﬁrst\nlayer of a baseline BERT model and a DIET-ABS\nmodel for a sampled batch in Figure 2. The ﬁgure\nshows that attention matrices of DIET-ABS have\nhigher ranks than the baseline BERT. Our detailed\nexperiment results in § 4 also show that DIET-ABS\nperforms noticeably better. This conﬁrms our ear-\nlier observation in Theorem 1 that additive position\nembeddings at input can constrain the model and\nadding the position embeddings per-head removes\nthis constraint and results in better performance.\nWith the decoupled positional embedding, we\ncan increase dp to any width k to break the low-\nrank bottleneck shown in Theorem 1. We call such\nmodel DIET-ABS-Rank-k. We also address the ef-\nﬁciency issue introduced by one additional matrix\nmultiplication (PQP⊤\nK). As the positional embed-\ndings are independent of the input, we only need\nto compute the matrix multiplication once for each\ntraining batch, and we can cache the computed\nmatrix before running inference. As a result, we\nobserve neglectable training and inference cost in-\ncrease in this model variant.\n3.2 Decoupled Relative Positional Attention\nTo incorporate relative position inductive bias, we\nconsider a simpliﬁed version of the position encod-\ning proposed in T5 (Raffel et al., 2020) without\nlog-binning and per-layer parameter sharing. We\nfurther also incorporate our per-head segment en-\ncoding as in DIET-ABS. The model can be written\nas:\nDIET-REL\nAREL\ni,j = (Xi:WQ)(Xj:WK)⊤/\n√\nd\n+ Ri−j + ES(S(i), S(j)).\n(3)\nWe show an example of this model with two seg-\nments in Figure 3.\n3.3 Training and Inference Costs\nWe next show the proposed models introduce\nlittle computational overhead compared to the\nbaseline model, making our model more practi-\ncal than alternatives. We consider two different\nmodels - BERTBASE model and a smaller model,\nBERTSMALL, that has hidden size 512, 4 layers and\n8 attention heads.\nIn Table 1 we compare the training and inference\ncosts of position encoding methods of Shaw et al.\n(2018), Ke et al. (2020), DIET-ABS and DIET-REL.\nWe notice that the simplicity of the proposed meth-\nods indeed translates to savings in both training and\ninference times compared to other position encod-\ning approaches. The savings in step times are even\nmore signiﬁcant for smaller models (BERTSMALL)\nand during inference.\nNote that the discrepancy between training and\ninference speed is likely because gradient updates\ndominate the cost at training time (Lan et al., 2020).\nAt inference time, we only measure the time of a\n2978\na00 a01 a02 a03\na10 a11 a12 a13\na20 a21 a22 a23\na30 a31 a32 a33\np00 p01\np10 p11\np20 p21\np30 p31\n+\nToken Absolute Position\np00 p01 p02 p03\np10 p11 p12 p13\nseg00 seg01\nseg10 seg11\n+\nSegment\nx\n(a) DIET-ABS\na00 a01 a02 a03\na10 a11 a12 a13\na20 a21 a22 a23\na30 a31 a32 a33\np0 p1 p2 p3\np-1 p0 p1 p2\np-2 p-1 p0 p1\np-3 p-2 p-1 p0\n+\nToken Relative Position\nseg00 seg01\nseg10 seg11\n+\nSegment (b) DIET-REL\nFigure 3: Proposed efﬁcient approach to include position and segment encoding by adding them directly to the\ntoken attention matrix per-head. Left ﬁgure shows how we encode absolute positional attention. Right ﬁgure\nrepresents relative positional attention.\nMode Shaw et al. (2018) Ke et al. (2020) D IET-ABS DIET-REL\nBERTBASE Training +13% +1% +0% +0%\nBERTBASE Inference +33% +19% +0% +0%\nBERTSMALL Training +24% +4% +0% +0%\nBERTSMALL Inference +65% +27% +1% +0%\nTable 1: Pre-training and inference time of Transformers with different position encoding methods in comparison\nto the baseline BERT model on TPU v2. We observe that simplicity of the D IET-REL and D IET-ABS result in\nsubstantial gains in both training and inference time. We notice even more speedup for the smaller BERT SMALL\nmodel compared to BERTBASE.\nforward pass which corresponds to costs of using\nsuch models in real systems.\n3.4 Application to Long-range Transformers\nAnother advantage of our propose approaches is\nthey easily extend to long range Transformer mod-\nels. For long sequence inputs, Transformers suf-\nfer from quadratic dependence of computational\ncomplexity with respect to the sequence length. A\nclass of methods reduce this complexity by using a\nlow rank projection of the input sequence for atten-\ntion computation (Wang et al., 2020; Choromanski\net al., 2021; Dai et al., 2020). However, such meth-\nods use the default input position encodings, and\nthere has not been much work in incorporating po-\nsition information per-head without introducing the\nquadratic computation complexity on the input se-\nquence length. We illustrate the applicability of our\nmethods to such settings by applying DIET-ABS to\nLinformer (Wang et al., 2020), which projects the\nattention key and value matrices to a lower dimen-\nsion k during attention computation.\nDIET-ABSLIN The proposed method can be\nwritten as:\nALIN\ni,j = (Xi:WQ)((EX)j:WK)⊤/\n√\nd\n+ (PQP⊤\nK)i,j,\n(4)\nwhere E ∈Rk×n, PQ ∈Rn×d, PK ∈Rk×d.\n4 Experiments\nIn this section, we present our experimental results\ncomparing different position and segment encod-\ning approaches discussed in earlier sections. We\nconduct experiments in three different settings to\ncover a wide range of use cases. First, we examine\nthe results of a popular transfer learning approach\nfrom masked-LM pretraining to the end tasks in\nGLUE (Devlin et al., 2018). Second, we study\nzero-shot cross-lingual transferability of the mul-\ntilingual pretrained models (Hu et al., 2020) to\nclassiﬁcation and question answering tasks in the\nXTREME benchmark (Hu et al., 2020). Lastly, we\nconsider training Transformer models from scratch\nfor machine translation.\nWe compare the following positional encoding\napproaches - absolute positional embedding (De-\nvlin et al., 2018), relative positional embedding\n(Shaw et al., 2018), combined absolute and rela-\ntive positional encoding (Ke et al., 2020), relative\nscalar approach (Raffel et al., 2020), our proposed\nDIET-ABS and DIET-REL per-head positional en-\ncoding approaches. We denote the methods that\nadd position/segment information directly to input\ntoken embeddings with input, and methods that add\nposition/segment information directly in attention\nlayer with per-head. For complete experimental\nsetup, see Appendix A.\n4.1 English Transfer Learning Results\nDatasets and Model For pre-training, we use\nEnglish Wikipedia and Books datasets (Devlin\net al., 2018). For Finetuning tasks we use the\ndatasets from the GLUE benchmark (Wang et al.,\n2019). We apply sub-word tokenization on raw\ntext data using WordPiece (Wu et al., 2016) with a\n30,000 token vocabulary.\n2979\nModel Position Segment MNLI QQP QNLI SST2 CoLA STS-B Avg393k 364k 105k 67k 8.5k 7k\nDevlin et al. (2018) input input 85.8 / 85.9 91.1 89.9 93.2 58.7 89.0 84.8\nShaw et al. (2018) per-head input 86.3 / 86.0 91.2 90.5 93.2 59.8 89.3 85.2\nRaffel et al. (2020) per-head input 86.4 / 86.2 91.2 90.1 93.0 59.6 90.1 85.2\nKe et al. (2020) per-head input 86.1 / 86.2 91.2 90.3 93.1 59.6 89.6 85.2\nDIET-REL per-head input 86.0 / 86.1 91.0 89.8 92.8 59.6 89.0 84.9\nDIET-REL per-head per-head 86.3 / 86.3 91.0 90.5 92.9 60.3 89.3 85.2\nDIET-ABS (dp=128, share) per-head per-head 86.4 / 86.4 90.8 89.5 93.0 59.8 90.2 85.2\nWang et al. (2020) (dp=32) input input 82.3 / 82.6 90.2 86.3 91.4 53.9 87.6 82.0\nDIET-ABSLIN (dp=32) per-head input 83.0 / 83.1 90.6 86.7 92.0 55.7 87.6 82.7\nTable 2: GLUE: Results on the GLUE dev set of the ﬁnetuned models based on a pre-trained model with 12-\nlayer BERTBASE architecture. We report the median of the maximum accuracy over all checkpoints among ﬁve\nruns. We notice that the shared D IET-ABS with rank 128 performs competitively with existing relative positional\nembedding SoTA models without the inductive bias of the relative positions. The proposed method also improves\nperformance in the low-rank long range transformer setting of (Wang et al., 2020), where relative positional\nembedding approaches are inefﬁcient to use.\nModel Position Segment\nClassiﬁcation Question Answering\nAvgXNLI XQuAD MLQA TyDiQA\n393k 88k 3.7k\nDevlin et al. (2018) input input 67.0 66.0 / 49.9 56.2 / 41.0 59.0 / 47.9 55.3\nShaw et al. (2018) per-head input 67.9 69.5 / 53.9 58.2 / 43.1 64.8 / 49.9 58.2\nRaffel et al. (2020) per-head input 68.5 69.9 / 53.5 59.5 / 44.3 63.8 / 50.6 58.6\nKe et al. (2020) per-head input 67.8 68.6 / 52.0 58.6 / 43.2 63.9 / 48.7 57.5\nDIET-REL per-head input 68.0 68.1 / 52.8 57.7 / 42.7 63.3 / 50.9 57.6\nDIET-REL per-head per-head 68.4 69.4 / 54.4 58.6 / 43.5 62.4 / 49.3 58.0\nDIET-ABS (dp=128, share) per-head per-head 68.5 70.0 / 53.6 59.8 / 44.5 64.6 / 51.5 58.9\nWang et al. (2020) (dp=256) input input 63.6 59.1 / 43.7 48.9 / 34.0 50.5 / 37.9 48.2\nDIET-ABSLIN (dp=256) per-head input 64.4 61.6 / 46.0 52.2 / 37.0 53.6 / 40.9 50.8\nTable 3: XTREME: Fine-tune cross-lingual model on English training set (Cross-lingual Transfer). Performance\nis measured by accuracy for classiﬁcation, and f1 score / exact matchfor question answering. In agreement with\nresults in Table 2 we see in this table that using per-head position encodings is strictly better than absolute position\nencodings at the input. With layer-wise sharing, DIET-ABS with rank 128 outperforms all SoTA models.\nModel EN-DE DE-EN EN-CS CS-EN\nVaswani et al. (2017) 39.00 38.42 18.55 22.93\nShaw et al. (2018) 40.10 38.90 18.74 23.89\nDIET-REL 39.47 38.49 18.68 23.93\nTable 4: Machine Translation: We report results com-\nparing different position encoding methods for Trans-\nformers on machine translation tasks en-de, de-en, en-\ncs and cs-en from the Newstest 2018 dataset. We no-\ntice that all per-head position encoding schemes (all ex-\ncept the ﬁrst row) do better than the absolute position\nembeddings added at the input. Further the proposed\nsimple D IET-REL approach is competitive with other\nposition encoding approaches.\nResults We examine how different ways of en-\ncoding position and segment affect the transfer\nlearning ability of the pre-trained English BERT\nmodels by ﬁne-tuning on the GLUE benchmark\n(Wang et al., 2019), and present the results in Ta-\nble 2. We ﬁrst notice that all the approaches that\nencode position features explicitly at per-head level\nperform better than the baseline additive position\nencodings at the input (Devlin et al., 2018). All\nmodels incorporating relative positions (Shaw et al.,\n2018; Raffel et al., 2020; Ke et al., 2020), despite\ntheir modeling differences, have very similar av-\nerage score. We show further gains (84.9 to 85.2\nfor DIET-REL) by moving segment features to per-\nhead.\nInterestingly we notice that the proposed abso-\nlute position encoding method DIET-ABS, with\nlayer-wise sharing, is on par with all previous\nSoTA relative positional encodings. This shows\nthat even absolute position encodings can perform\nbetter when included per-head instead at the input.\nWe present a detailed ablation study varying the\nrank and sharing methods of absolute positional\nattention (DIET-ABS) in Table 8 and Tables 9 in\n2980\nAppendix C.\nFor long range input, we consider Linformer\n(Wang et al., 2020) with a projection dimension of\n32. Due to down-projection, we see non-trivial per-\nformance drop, when compared to a Transformer.\nEven for this setting we see that our absolute posi-\ntional attention DIET-ABS can be used to improve\nthe model’s performance.\n4.2 Cross-lingual Model Results\nDatasets and Model For our multilingual ex-\nperiments, we pre-train the models on Wikipedia\ncorpus in 100 languages similar to (Lample and\nConneau, 2019) for 125K steps with a sequence\nlength of 512, and then ﬁne-tune on downstream\nXTREME tasks (Hu et al., 2020). We use language-\nindependent tokenizer, Sentence Piece (Kudo and\nRichardson, 2018) model, with 120,000 token vo-\ncabulary to encode input text.\nClassiﬁcation We conduct 5 trials of ﬁne-tuning\nfor each model on the MultiNLI (Williams et al.,\n2018) training data, then perform zero-shot predic-\ntions on XNLI (Conneau et al., 2018), choosing\nmedian accuracy to report.\nQuestion Answering We conduct 5 trials of ﬁne-\ntuning for each model on SQuAD V1.1 dataset,\nfollowing by zero-shot predictions on XQuAD (11\nlanguages), MLQA (7 languages) and TyDiQA-\nGoldP (9 languages), choosing median F1 / EM\nscores to report.\nResults We present our results on the classiﬁ-\ncation and question answering ﬁnetuning tasks in\nXTREME for different position and segment en-\ncoding methods in Table 3. Again all per-head\nposition encoding methods outperform input addi-\ntive position encodings. Interestingly, our simple\nDIET-ABS turns out to be the best model, better\nthan other models using relative position features.\nLayer-wise sharing and per-head segment attention\nallows DIET-ABS to outperform DIET-REL. We\npresent a detailed ablation study in Table 5 to un-\nderstand effect of decoupled positional attention\nvariants. Finally, we notice similar advantages in\nusing DIET-ABS with the Linformer (Wang et al.,\n2020) model in the long range setting.\n4.3 Translation Results\nDatasets and Model For the machine translation\ntask we consider two language pairs (both direc-\ntions) for training - WMT 2018 English-to-German\n(en-de), German-to-English (de-en), English-to-\nCzech (en-cs) and Czech-to-English (cs-en) (Bo-\njar et al., 2018). We test the corresponding mod-\nels on Newstest 2018 datasets respectively and re-\nport the BLEU score output by SacreBLEU (Post,\n2018) with default setting. Our setup follows\nVaswani et al. (2017) closely and use their Ten-\nsor2Tensor framework (Vaswani et al., 2018). Fol-\nlowing Vaswani et al. (2017) we use a 6 layer Trans-\nformer with encoder-decoder architecture. For\nmore details of our experimental setup please see\nAppendix A\nResults We report the BLEU scores of the mod-\nels in Table 4. We observe that moving positional\ninformation from input to per-head attention layer\nimproves BLEU scores. Different variations of\nper-head positional attention do not make much\ndifference with DIET-REL being competitive with\nShaw et al. (2018).\n4.4 Ablation Study\nIn this section, we share our ﬁndings of key factors\nthat affect performance of decoupled positional\nattention.\nSharing the Positional Encoding Previous\nworks (Raffel et al., 2020; Ke et al., 2020; Shaw\net al., 2018) used different sharing methods for the\npositional encodings to reduce the model parame-\nters. We present a detailed study on different forms\nof sharing positional encodings and its effect on\nperformance. In particular, we compare the fol-\nlowing variations in sharing the position encoding\nparameters across different heads and the layers in\nthe Transformer.\n•head-wise - Same parameters are used for all\nheads in a layer, with different layers using\ndifferent parameters (Shaw et al., 2018; Ke\net al., 2020).\n•layer-wise - Sharing of position encoding pa-\nrameters across layers with different parame-\nters for each head (Raffel et al., 2020).\n•none - Every layer and head uses different\nposition encoding parameters.\nWe present results comparing different sharing\nmethods in Table 5 for XTREME tasks. We make\nthe following observations 1) head-wise sharing is\nconsistently worse than layer-wise, 2) sharing hurts\nthe performance of DIET-REL whereas it improves\n2981\nModel Sharing Segment Classiﬁcation Question Answering AvgXNLI XQuAD MLQA TyDiQA-GoldP\nDIET-REL - input 68.0 68.1 / 52.8 57.7 / 42.7 63.3 / 50.9 57.6\nDIET-REL head-wise input 67.7 66.2 / 51.0 56.0 / 41.1 60.1 / 45.9 55.4\nDIET-REL layer-wise input 68.0 68.6 / 53.3 58.1 / 43.1 61.3 / 48.2 57.2\nDIET-REL - per-head 68.4 69.4 / 54.4 58.6 / 43.5 62.4 / 49.3 58.0\nDIET-REL head-wise per-head 67.8 66.0 / 50.5 55.5 / 40.4 59.2 / 44.6 54.7\nDIET-REL layer-wise per-head 68.1 68.7 / 53.8 58.4 / 43.2 61.0 / 48.4 57.3\nDIET-ABS (dp=64) - input 68.0 67.4 / 50.5 57.8 / 42.3 61.3 / 46.8 56.3\nDIET-ABS (dp=64) - per-head 67.9 67.5 / 52.4 57.3 / 42.3 61.6 / 46.8 56.5\nDIET-ABS (dp=128) - per-head 68.1 68.2 / 52.0 57.9 / 42.6 61.5 / 47.6 56.8\nDIET-ABS (dp=512) - per-head 68.5 68.0 / 52.0 57.7 / 42.4 61.6 / 48.4 56.9\nDIET-ABS (dp=64) layer-wise input 68.0 69.3 / 53.1 59.3 / 43.9 63.2 / 48.6 57.9\nDIET-ABS (dp=64) layer-wise per-head 68.4 69.3 / 53.2 59.4 / 44.1 63.3 / 48.6 58.0\nDIET-ABS (dp=128) layer-wise per-head 68.5 70.0 / 53.6 59.8 / 44.5 64.6 / 51.5 58.9\nDIET-ABS (dp=256) layer-wise per-head 68.4 69.9 / 53.8 59.6 / 44.2 62.8 / 49.1 58.3\nDIET-ABS (dp=512) layer-wise per-head 67.8 69.0 / 53.2 58.4 / 43.0 62.5 / 48.8 57.5\nTable 5: Ablation study on XTREME: We run decoupled positional attention ablation study to understand the\neffect of 1) sharing positional attention parameters across layers and heads 2) segment attention added at per-head\n3) performance of relative and absolute 4) absolute positional attention rank dp from 64 to 512.\nEnglish Multilingual\nParameters +∆ GLUE Parameters +∆ XTREME\nDevlin et al. (2018) 110.1M - 84.8 178.9M - 55.3\nShaw et al. (2018) 112.9M +2.5% 85.2 181.7M +1.7% 57.9\nDIET-REL 109.9M +0.0% 85.2 178.7M +0.0% 58.0\nDIET-REL (share) 109.7M +0.0% 85.0 178.5M +0.0% 57.3\nDIET-ABS (dp=128) 128.6M +16.8% 85.3 197.4M +10.0% 56.8\nDIET-ABS (dp=128, share) 111.3M +1.1% 85.2 180.1M +0.6% 58.9\nTable 6: Model Parameters: We list the number of model parameters and performance for different position en-\ncoding approaches. We observe that sharing hurts the performance of D IET-REL with negligible beneﬁt in the\nnumber of parameters. On the contrary, the regularization effect of sharing makes D IET-ABS more stable with\nlesser parameters to achieve competitive performance.\nthe performance of DIET-ABS. We summarize the\nkey settings along with the number of model pa-\nrameters in Table 6. For DIET-REL, sharing brings\nlittle effect on saving parameters, and hurts the per-\nformance. Hence, we recommend no sharing for\nrelative positional encodings (DIET-REL). On the\nother hand, it is necessary to share parameters for\nDIET-ABS in order to keep the number of parame-\nters low. Interestingly, sharing has regularization\neffect on DIET-ABS, making the model perform\nbetter. We choose layer-wise sharing over head-\nwise sharing for its better performance.\nSegment Encoding Our novel segment encod-\ning design further improves the model perfor-\nmance showed in Table 5. Both relative and ab-\nsolute decoupled positional attention models ben-\neﬁt from moving the segment encoding from in-\nput to per-head: DIET-REL (+0.4%), layer-wise\nshared DIET-REL (+0.1%), DIET-ABS (+0.2%),\nlayer-wise shared DIET-ABS (+0.1%). See Ap-\npendix D for the results of GLUE benchmark and\nAppendix C for segment attention visualization.\nRank of Absolute Positional Attention The de-\nsign of DIET-ABS allows to learn higher rank at-\ntention matrices as shown in Theorem 1. To under-\nstand the effect of absolute positional attention rank\n(dp) in practice, we conduct experiments varying\nthe rank from dp = 64to dp = 512. We present\nthe results in Table 5. We notice that the perfor-\nmance improves as we increase the rank from 64 to\n128. However there is a performance saturation in\nfurther increasing it to 512. We present a visualiza-\ntion of the rank of the positional attention matrix\nin Appendix B.\n4.5 Positional Attention Pattern Visualization\nWe next visualize the learned positional attention\npatterns of DIET-ABS in Figure 4. We ﬁrst note\nthat DIET-ABS has learned to capture the relative\npositional relations between inputs. Also note that,\nfor the the index zero (the [CLS] token), decoupled\nabsolute positional attention usually learns a spe-\n2982\nFigure 4: Visualization of learned positional attention\npatterns of D IET-ABS. Note that in addition to captur-\ning the the relative positional relations, the model also\nlearn to attend to [CLS] at index 0, suggesting the ded-\nicated [CLS] untying design in Ke et al. (2020) is not\nnecessary with DIET-ABS.\ncial pattern. This pattern cannot be solely modeled\nby existing relative positional embedding methods,\nand some existing works (Ke et al., 2020) handled\nthis case speciﬁcally by introducing new parame-\nters. This shows the beneﬁt of DIET-ABS in not\nrequiring any carefully designed inductive biases\nas in existing approaches( Shaw et al. (2018); Raf-\nfel et al. (2020)), which may not generalize across\ntasks.\n5 Conclusion\nIn this paper we theoretically and empirically ex-\namined the limitation of additive position embed-\nding at input and showed that having per-head posi-\ntion embeddings results in better performance. We\nargued that the superior performance of some of\nthe relative position encoding methods come from\ntheir per-head addition to attention matrix rather\nthan the position information being relative vs ab-\nsolute. Indeed we show that using absolute position\nencodings per-head results in better performance.\nMotivated by this we propose a simple per-head po-\nsition and segment attention method that achieves\nthe state-of-the-art performance on multiple NLP\ntasks and is more computationally efﬁcient than\nexisting approaches.\nReferences\nOndˇrej Bojar, Christian Federmann, Mark Fishel,\nYvette Graham, Barry Haddow, Philipp Koehn, and\nChristof Monz. 2018. Findings of the 2018 con-\nference on machine translation (WMT18). In Pro-\nceedings of the Third Conference on Machine Trans-\nlation: Shared Task Papers, pages 272–303, Bel-\ngium, Brussels. Association for Computational Lin-\nguistics.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, David Belanger, Lucy Colwell, and\nAdrian Weller. 2021. Rethinking attention with per-\nformers.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nZihang Dai, Guokun Lai, Yiming Yang, and Quoc V .\nLe. 2020. Funnel-transformer: Filtering out sequen-\ntial redundancy for efﬁcient language processing.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional Transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham\nNeubig, Orhan Firat, and Melvin Johnson. 2020.\nXtreme: A massively multilingual multi-task bench-\nmark for evaluating cross-lingual generalisation. In\nProceedings of Machine Learning and Systems 2020,\npages 7449–7459.\nGuolin Ke, Di He, and Tie-Yan Liu. 2020. Rethink-\ning the positional encoding in language pre-training.\narXiv preprint arXiv:2006.15595.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learn-\ning of language representations.\nXiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng\nGao. 2020. Very deep transformers for neural ma-\nchine translation.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nMatt Post. 2018. A call for clarity in reporting bleu\nscores. In WMT.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. Technical Re-\nport, OpenAI.\n2983\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan N Gomez, Stephan Gouws,\nLlion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki\nParmar, et al. 2018. Tensor2tensor for neural ma-\nchine translation. arXiv preprint arXiv:1803.07416.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2019.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-attention\nwith linear complexity.\nYu-An Wang and Yun-Nung Chen. 2020. What do\nposition embeddings learn? an empirical study of\npre-trained language model positional encoding. In\nEMNLP 2020.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus for\nsentence understanding through inference.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G.\nCarbonell, Ruslan Salakhutdinov, and Quoc V . Le.\n2019. XLNet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nChulhee Yun, Srinadh Bhojanapalli, Ankit Singh\nRawat, Sashank J Reddi, and Sanjiv Kumar.\n2020. Are Transformers universal approximators of\nsequence-to-sequence functions? In International\nConference on Learning Representations.\n2984\nA Experimental setup\nIn this section we present more details of our experimental setup.\nPre-training We pre-train the models using a masked LM task (Devlin et al., 2018) and do not use\nthe Next Sentence Prediction (NSP) loss as suggested in RoBERTa (Liu et al., 2019). Each input is\nconstructed with full sentences from documents, and packed up to the maximum sequence length. We use\nthe same architecture as BERTBASE (Devlin et al., 2018) (L = 12, H = 768, A = 12) for our experiments.\nFine-tuning Some downstream tasks have different groups of full sentences provided at inputs. For\nthose tasks (e.g. MNLI, CoLA, XNLI, SQuAQ), we ﬁne-tune models with supplemental segment encoding\ndiscussed in Section §3. We leave models for other tasks unchanged as their pre-training correspondences.\nHyper-parameters Hyper-parameters we use are presented in Table 7.\nEnglish Multilingual\nPretrain Finetune Pretrain Finetune\nMax Steps 500K 5 or 10 epochs 125K 3 epochs\nLearning Rate 0.0018 {1e-5, 2e-5, 3e-5, 4e-5} 0.0018 {1e-5, 2e-5, 3e-5, 4e-5}\nWarmup Proportion 0.025 0.1 0.025 0.1\nSequence Length 128 128 512 512\nBatch Size 4096 32 4096 32\nCheckpoint Interval 20k 3.5k 20k 3.5k\nTable 7: Hyperparameters for all models\nTranslate For our Translate experiments we follow the setup of Vaswani et al. (2017) and use their\nTensor2Tensor framework (Vaswani et al., 2018). We train using WMT18 ((Europarl v7, Common Crawl\ncorpus and News Commentary v13) en-de, de-en, en-cs and cs-en datasets. We report BLUE scores\nprovided by SacreBLEU (Post, 2018) on newstest 2018 dataset. We train a 6 layer Transformer model.\nAny changes to position encoding are applied to all the attention layers both in the encoder and decoder.\nWe use Adam optimizer and train for 250k steps. For decoding we use beam search with beam size 10\nand length penalty 0.6.\nB Proofs\nProof of Theorem 1.The ﬁrst claim follows easily by observing that rank of product of an two matrices is\nupper bounded by the minimum of the individual ranks.\nrank(Aa) =rank((X + P)WQW⊤\nK(X + P)⊤)\n≤min(rank(X + P), rank(WQ), rank(X + P), rank(WK))\n≤dh.\nrank((X + P)WQW⊤\nK(X + P)⊤) ≤dh, where WQ, WK ∈Rd×dh\nThe last inequality follows from rank(WQ) ≤dh as WQ ∈Rd×dh.\nTo prove the second claim we follow a construction approach. Let us ﬁrst take WQ = WK to be same\nmatrices with ﬁrst dh rows being identity matrix and the remaining d −dh rows being all zeros. Then\nWQW⊤\nK =\n( Idh,dh 0dh,d−dh\n0d−dh,dh 0d−dh,d−dh\n)\n.\nHere Idh,dh denotes the identity matrix in Rdh×dh and 0dh,d denotes the all zeros matrix in Rdh,d.\nWe let X be such that the ﬁrst d rows form an identity matrix and rest are zeros - X⊤= [Id,d, 0n−d,d].\nHence XWQW⊤\nKX⊤becomes a similar diagonal matrix with\nXWQW⊤\nKX⊤=\n( Idh,dh 0dh,n−dh\n0n−dh,dh 0n−dh,n−dh\n)\n.\n2985\nChoose dp = n > dh and let ˆP = I. Now chosing ˆP with zeros in the ﬁrst n −dp columns and identity\nin the last dp columns ( ˆP = [0d,n−dp, Idp,dp]) gives\nˆPˆP⊤=\n( 0n−dp,n−dp 0n−dp,dp\n0dp,n−dp Idp,dp\n)\n.\nCombining these two gives us\nrank(Ar) =rank(XWQW⊤\nKX⊤+ ˆPˆP⊤)\n= min(dh + dp, n) > dh.\nLet X ∈Rn×d be the input word embeddings in dimensiond with sequence length n. We have trainable\nposition embeddings P ∈Rn×d, which are added to the input sequence before feeding into the model g.\nFor a given input X and label y, the objective for a loss function ℓ is as follows:\nL = ℓ (g(X + P), y) (5)\nTheorem 2. Let X and P be trainable embedding matrices inRn×d. Then the gradients of the loss\nfunction in equation(5), at any point(X, y), and for any differentiable functionsℓ and g, are same forX\nand P.\nRemarks. This theorem shows us that the gradients are same for the input token embeddings and\nposition embeddings. While in standard NLP tasks the inputs X can be different in each step due to\ndifferent input tokens being present in each mini batch, the result still suggests that additive position\nembedding can limit the model from learning the relative importance of position encodings with respect\nto token embeddings based on the training task at hand.\nProof of Theorem 2.The above theorem follows by just computing the gradients and showing they are\nequal for each step.\nGradients of the above objective w.r.tX and P are as follows.\n∇XL = ∇gL ·∇X+Pg ·∇X(X + P)\n= ∇gL ·∇X+Pg\n∇PL = ∇gL ·∇X+Pg ·∇P(X + P)\n= ∇gL ·∇X+Pg.\nThe above computation of gradient follows from chain rule. This shows that the gradients of L w.r.t. X\nand P are the same.\n2986\nC Attention Visualization\nIn this section, we examine the model internals to understand how the proposed model works. We ﬁrst\nvisualize the model internals of different modeling alternatives to argue our proposed model is sensible.\nWhy We Remove the Input Embedding To understand if it is sensible to remove the input additive\nembedding after adding position scalars per-head, we add additive position embedding to our DIET-ABS\nmodel. Then, we examine the position embedding of the BERT model and our DIET-ABS variant with\nadditive position embedding. Figure 5 shows that, when the model has both absolute scalar and additive\nabsolute position embedding, the position embedding encodes almost no information — all position\nembeddings at input are similar.\nFigure 5: The cosine similarity distribution between all absolute position pairs of the input additive positional\nembedding for the baseline BERT model and the proposed DIET-ABS. We observed that, after the position features\nare added to each head as in DIET-ABS, the input position embedding contains almost no information — all input\nposition pairs are similar.\nThe Effect of Segment Attention We also examine the effect of adding segment attention on top of\nthe position attention. Figure 6 shows some representative patterns. We observe that segment attention\nenables the model to attend more to parts of the sequence that belongs to certain segments.\n(a) Attend to the Second Segment\n (b) Down-weight Relative Position Attention\nFigure 6: We consider input of length 32 with two segments. The second segment starts at index 16. We observe\nthe attention patterns in the DIET-REL model without token-to-token attention.\n2987\nShifting Pattern Learned from Absolute Positional Attention Using relative position encoding gives\ngenerally better results despite smaller improvement scale compared to moving feature encoding per-head.\nTo understand this, we visualize the attention pattern of the absolute positional attention and found two\nrepresentative patterns in DIET-ABS in Figure 7. We observe that even given absolute position features,\nthe model learns a “shifting pattern” for the most part. Different from Wang and Chen (2020) which\nclaimed absolute position only learns local patterns, we show the position attention can actually attend\nto longer context. However, the shifting pattern can be modeled directly by relative position. Thus,\nDIET-REL can be a better model choice with fewer parameters and more accurate inductive bias in some\napplications.\n(a) Attend to Forward Neighbors\n (b) Attend to Previous Tokens\nFigure 7: Sampled position attention score patterns for the D IET-ABS model. We can see a clear shifting patterns\ngenerated by the model. Such patterns can be modeled better by relative positional scalar encodigs.\nRank of Positional Attention Matrices In Figure 8, we present a comparison of rank of position\nattention matrices for a BERTBASE model with absolute position embeddings at input (PQWQW⊤\nKP⊤\nK)\nv.s. absolute position embeddings per-head (DIET-ABS (1), (PQP⊤\nK), where PQ, PK ∈Rn×dp). With\nadditive positional embedding at input, position attention matrices have much lower rank, limiting the\nrepresentative power. This is alleviated by DIET-ABS.\nFigure 8: Rank of positional attention matrices\n2988\nD Additional Ablation Study on GLUE\nEarlier we present an ablation study on XTREME in Table 5 for decoupled positional attention variants.\nWe compare DIET-REL and DIET-ABS against the baseline (Devlin et al., 2018). We now present a\nsimilar study on the GLUE benchmark in Table 8 and observe similar results.\nPositional Encoding In Table 8, moving positional embeddings from input to per-head improves\naverage score for both DIET-REL (+0.1%) and DIET-ABS (+0.2%).\nSegment Encoding In Table 8, moving segment embeddings from input to per-head improves both\nDIET-REL (+0.3%) and DIET-ABS (+0.05%).\nSharing Strategies Sharing plays an important role for DIET-ABS. In Table 9, we ﬁnd that sharing will\ndegrade the performance of DIET-REL (-0.2% layer-wise, -0.3% head-wise). For DIET-ABS, sharing\nmakes the model more stable, and able to compete with DIET-REL.\nModel Position Segment MNLI QQP QNLI SST2 CoLA STS-B Avg393k 364k 105k 67k 8.5k 7k\nDevlin et al. (2018) input input 85.8 / 85.9 91.1 89.9 93.2 58.7 89.0 84.8\nDIET-REL per-head input 86.0 / 86.1 91.0 89.8 92.8 59.6 89.0 84.9\nDIET-REL per-head per-head 86.3 / 86.3 91.0 90.5 92.9 60.3 89.3 85.2\nDIET-ABS (dp=64) per-head input 86.1 / 85.8 91.2 90.0 93.0 58.9 89.9 85.0\nDIET-ABS (dp=64) per-head per-head 86.1 / 86.1 91.2 90.2 93.0 58.9 89.8 85.0\nDIET-ABS (dp=64, share) per-head per-head 86 / 86.8 91.1 90.4 92.9 59.3 89.8 85.2\nDIET-ABS (dp=128, share) per-head per-head 86.4 / 86.4 90.8 89.5 93.0 59.8 90.2 85.2\nTable 8: Position and segment ablation study on GLUE: DIET-REL and DIET-ABS demonstrate the advantages of\nmoving both positional and segment embedding from input to per-head.\nModel Sharing MNLI QQP QNLI SST2 CoLA STS-B Avg393k 364k 105k 67k 8.5k 7k\nDIET-REL - 86.3 / 86.3 91.0 90.5 92.9 60.3 89.3 85.2\nDIET-REL layer-wise 86.5 / 86.3 91.1 90.0 93.0 58.8 89.6 85.0\nDIET-REL head-wise 85.8 / 85.7 91.2 90.2 92.8 59.8 89.1 84.9\nDIET-ABS (dp=64) - 86.1 / 86.1 91.2 90.2 93.0 58.9 89.8 85.0\nDIET-ABS (dp=128) - 86.7 / 86.5 91.2 90.6 92.8 60.1 89.4 85.3\nDIET-ABS (dp=64) layer-wise 86 / 86.8 91.1 90.4 92.9 59.3 89.8 85.2\nDIET-ABS (dp=128) layer-wise 86.4 / 86.4 90.8 89.5 93.0 59.8 90.2 85.2\nTable 9: Sharing ablation study on GLUE: We run ablation study to understand the effect of sharing position\nencoding parameters across layers and heads. We notice that sharing improves the performance of DIET-ABS, but\nhurts the performance of DIET-REL with both layer-wise or head-wise sharing.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7284162044525146
    },
    {
      "name": "Computer science",
      "score": 0.6968197226524353
    },
    {
      "name": "ENCODE",
      "score": 0.6657071113586426
    },
    {
      "name": "Inference",
      "score": 0.5005056858062744
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4746675491333008
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4335017800331116
    },
    {
      "name": "Algorithm",
      "score": 0.3679136633872986
    },
    {
      "name": "Engineering",
      "score": 0.127582848072052
    },
    {
      "name": "Electrical engineering",
      "score": 0.08685502409934998
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}