{
    "title": "Exploring Large Language Models for Classical Philology",
    "url": "https://openalex.org/W4385571809",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5092029597",
            "name": "Frederick Riemenschneider",
            "affiliations": [
                "Heidelberg University"
            ]
        },
        {
            "id": "https://openalex.org/A2113562383",
            "name": "Anette Frank",
            "affiliations": [
                "Heidelberg University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2998696444",
        "https://openalex.org/W3093517588",
        "https://openalex.org/W3156170450",
        "https://openalex.org/W3037109418",
        "https://openalex.org/W3088026279",
        "https://openalex.org/W2981821110",
        "https://openalex.org/W3174520822",
        "https://openalex.org/W3212892226",
        "https://openalex.org/W2898879711",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2077419346",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3104625094",
        "https://openalex.org/W4319915529",
        "https://openalex.org/W2915977242",
        "https://openalex.org/W2736855481",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2969873034",
        "https://openalex.org/W1535269839",
        "https://openalex.org/W3164045210",
        "https://openalex.org/W4385573957",
        "https://openalex.org/W2039696003",
        "https://openalex.org/W2135843243",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W3111372685",
        "https://openalex.org/W4283031539",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W2964198424",
        "https://openalex.org/W2530151296",
        "https://openalex.org/W3098749165",
        "https://openalex.org/W2911323286"
    ],
    "abstract": "Recent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical languages unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study their versatility for tasks of interest for Classical languages: we explore (i) encoder-only and encoder-decoder architectures using RoBERTa and T5 as strong model types, and create for each of them (ii) a monolingual Ancient Greek and a multilingual instance that includes Latin and English. We evaluate all models on morphological and syntactic tasks, including lemmatization, which demonstrates the added value of T5's decoding abilities. We further define two probing tasks to investigate the knowledge acquired by models pre-trained on Classical texts. Our experiments provide the first benchmarking analysis of existing models of Ancient Greek. Results show that our models provide significant improvements over the SoTA. The systematic analysis of model types can inform future research in designing language models for Classical languages, including the development of novel generative tasks. We make all our models available as community resources, along with a large curated pre-training corpus for Ancient Greek, to support the creation of a larger, comparable model zoo for Classical Philology.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 15181–15199\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nExploring Large Language Models for Classical Philology\nFrederick Riemenschneider\nDept. of Computational Linguistics\nHeidelberg University\n69120 Heidelberg\nriemenschneider@cl.uni-heidelberg.de\nAnette Frank\nDept. of Computational Linguistics\nHeidelberg University\n69120 Heidelberg\nfrank@cl.uni-heidelberg.de\nAbstract\nRecent advances in NLP have led to the cre-\nation of powerful language models for many\nlanguages including Ancient Greek and Latin.\nWhile prior work on Classical languages unani-\nmously uses BERT, in this work we create four\nlanguage models for Ancient Greek that vary\nalong two dimensions to study their versatility\nfor tasks of interest for Classical languages: we\nexplore (i) encoder-only and encoder-decoder\narchitectures using ROBERTA and T5 as\nstrong model types, and create for each of them\n(ii) a monolingual Ancient Greek and a multi-\nlingual instance that includes Latin and English.\nWe evaluate all models on morphological and\nsyntactic tasks, including lemmatization, which\ndemonstrates the added value of T5’s decod-\ning abilities. We further define two probing\ntasks to investigate the knowledge acquired by\nmodels pre-trained on Classical texts. Our ex-\nperiments provide the first benchmarking ana-\nlysis of existing models of Ancient Greek. Re-\nsults show that our models provide significant\nimprovements over the SoTA. The systematic\nanalysis of model types can inform future re-\nsearch in designing language models for Clas-\nsical languages, including the development of\nnovel generative tasks. We make all our mod-\nels available as community resources, along\nwith a large curated pre-training corpus for An-\ncient Greek, to support the creation of a larger,\ncomparable model zoo for Classical Philol-\nogy. Our models and resources are available\nat https://github.com/Heidelberg-NLP/\nancient-language-models.\n1 Introduction\nSince the beginning of the creation of the Index\nThomisticus in 1946 (Busa, 1980) and the publica-\ntion of the Concordance to Livy (Packard, 1968),\nClassical Philology has been revitalized by the “dig-\nital revolution” (Berti, 2019). Today, numerous ef-\nforts have been undertaken to make Classical texts\ndigitally available, annotate, and automatically pro-\ncess them. E.g., the Classical Language Toolkit\n(CLTK, Johnson et al., 2021) offers various tools to\nprocess pre-modern languages, in particular Latin\nand pre-modern Greek.1\nRecently, we see a surge of the first pre-trained\ncontextualized language models (PLMs) for Classi-\ncal languages: Latin BERT has been proposed by\nBamman and Burns (2020), Ancient Greek (AG)\nBERT by Singh et al. (2021). Lately, a second\nAG BERT has been proposed by Yamshchikov\net al. (2022). However, both AG BERT models\nhave been pre-trained on a comparatively small pre-\ntraining dataset. Moreover, they have been initial-\nized from Modern Greek BERT (Koutsikakis et al.,\n2020), which limits them to the modern Greek al-\nphabet, ignoring the diacritics of Ancient Greek.\nAlthough numerous richly annotated treebanks\nare available for Latin and AG, systems have, by\nnow, not been evaluated on a shared benchmark.\nGiven that two popular treebanks for AG have been\nintegrated into Universal Dependencies (de Marn-\neffe et al., 2021), it is surprising that researchers\nworking on AG do not compare to benchmarking\nresults of, e.g., Straka (2018). Hence, a thorough\nassessment of the performance of the existing mod-\nels is necessary in order to compare and evaluate\ntheir effectiveness for this underexplored language.\nWhile BERT models are known to achieve high\nperformance on a wide range of tasks, encoder-\ndecoder models or multilingual models may often\nbe a better choice, depending on the task. In this\nwork, we explore a variety of language models for\nClassics in general and Ancient Greek in particular:\nWe introduce GRεTA, GRεBERTA, PHIL BERTA,\nand PHILTA, four PLMs for Classics. GRεBERTA\nand GRεTA are ROBERTA (Liu et al., 2019) and\nT5 (Raffel et al., 2020) models trained on An-\ncient Greek texts, respectively. PHIL BERTA and\n1We use the term “pre-modern” and “Ancient” interchange-\nably following the convention of calling every pre-modern\nlanguage stage “Ancient”. This is in line with, e.g. Singh et al.\n(2021), Yamshchikov et al. (2022), and the ISO code standard.\n15181\nPHILTA are their trilingual counterparts pre-trained\non Greek as well as Latin and English data.\nWe explore the advantages of (i) the two model\narchitectures in (ii) mono- and multilingual pre–\ntraining for the mid-resource language Ancient\nGreek on a variety of morphological, syntactic,\nand semantic tasks, helping to answer questions,\nsuch as: When to choose one architecture over\nthe other? or: How does multilinguality affect a\nlanguage model?\nMoreover, we publish the first wide-ranging\nbenchmark results to compare our models for AG\nand Latin to the relevant prior work, establishing\nnew SoTA results for both languages.\nIn summary, we aim to unify and push forward\nthe current research landscape at the intersection of\nClassics and NLP with the following contributions:\n(i) We introduce four pre-trained language\nmodels for Classics: GRε(BERT |T)A and\nPHIL (BERT |T)A. To our knowledge, we are\nthe first to develop encoder-decoder models\nfor Classics, and multilingual models tailored\nto both Latin and Greek.\n(ii) We evaluate the already existing and our pro-\nposed models on several tasks, making many\nof them comparable for the first time. Fur-\nthermore, we outperform the existing Ancient\nGreek BERT models by a notable margin.\n(iii) Our evaluation sheds light on the differences\nbetween encoders like ROBERTA and en-\ncoders of encoder-decoder models like T5 as\nwell as on the influence of multilinguality on\nthe mid-resource language Ancient Greek. By\noffering novel model types for AG, we aim to\ninspire new research and application tasks.\n(iv) We develop and publish a large-scale, high-\nquality pre-training corpus for AG as a contri-\nbution to the community.\n2 Related Work\nPre-training Data for Ancient Greek. Pre-\ntrained language models require large amounts of\nunlabeled pre-training data. Ancient Greek and\nLatin being historical languages, the number of\navailable texts is inherently limited, which makes\nthe creation of a high-quality pre-training corpus\neven more important. To circumvent this problem,\nSingh et al. (2021) and Yamshchikov et al. (2022)\npre-trained their AG BERT model from a Mod-\nern Greek BERT (Koutsikakis et al., 2020). But\nthis approach has two weaknesses: First, there is\nan important cultural gap between modern and an-\ncient texts that we do not want to introduce into our\nmodels. A Modern Greek BERT is familiar with\ncontemporary concepts like cell phones or commu-\nnism, which are unknown to antiquity, while we in-\ntend to use PLMs as a “window” to ancient cultures.\nAlso the style of modern internet documents is fun-\ndamentally different from the transmitted ancient\ntexts. Second, and more importantly, continuing\npre-training of the Modern Greek BERT prevents\nus from adapting its tokenizer. AG, however, uses\nmore diacritics, which host important information.\nBy contrast, in our work, we build a tokenizer from\nscratch that is optimized for Ancient Greek.\nIn order to boost the data needed to train “pure”\nmodels of Ancient Greek, we put special effort into\nthe curation of a large, but high-quality pre-training\ncorpus for AG, leveraging previously unused tex-\ntual sources. Finally, we evaluate the effect of using\nadditional multilingual pre-training data.\nEvaluating Models for Ancient Languages.\nMorphological and syntactic tasks, such as PoS tag-\nging, dependency parsing, and lemmatization, have\nalways been of interest to researchers of Latin and\nAncient Greek. The standard tool for AG morpho-\nlogical analysis is Morpheus (Crane, 1991), a rule-\nbased system, that has also been integrated into\nmany more recent approaches. PoS Tagging has\nalso been performed by various language-agnostic\nsystems trained on AG data (Celano et al., 2016),\nbut their success depends heavily on the chosen\ndataset: a winning system on one dataset (Celano\net al., 2016) achieves the worst results on another\n(Keersmaekers, 2019). More recently, the CLTK\n(Johnson et al., 2021) provides a variety of taggers\nfor many tasks. Surprisingly, although numerous\nrichly annotated treebanks are available, systems\nhave, by now, not been evaluated on a common\nbenchmark.2 E.g., Singh et al. (2021) test their pro-\nposed AG BERT on random splits from three pop-\nular treebanks, which we cannot compare against.\nThe second AG BERT (Yamshchikov et al., 2022)\nhas only been evaluated on authorship attribution.\nAs for lemmatization, Vatri and McGillivray\n(2020) provide an evaluation of three different lem-\nmatizers. However, one of the evaluated candidates\nwas partly trained on test data, which may have\ninfluenced its performance. It is noteworthy that,\n2Cf. also Johnson et al. (2021): “The CLTK lacks formal\nevaluations of its models’ accuracies. [...] Unfortunately, [out-\nside] benchmarks do not yet exist for pre-modern languages.”\n15182\nForm: τὸν δ᾿ ἄῤ ὑπόδρα ἰδὼν προσέφη πόδας ὠκὺς ᾿ Αχιλλεύς ·\nLemma: ὁ δέ ἄρα ὑπόδρα εἶδον πρόσφημι πούς ὠκύς ᾿ Αχιλλεύς ·\nUPoS: PRON PART PART ADV VERB VERB NOUN ADJ NOUN PUNCT\nXPoS: p-s---ma- g-------- g-------- d-------- v-sapamn- v3siia--- n-p---ma- a-s---mn- n-s---mn- u--------\nGloss: him but then from-below watching spoke-to of-foot swift Achilles .\nROOT\nobj\ncc\nadvmod\nadvmod advcl nmod amod\nnsubj\npunct\nFigure 1: Example sentence (Hom. Il. 1.148) with corresponding dependency, lemma, UPoS, and XPoS labels.\nTranslation: “Then watching him grimly from under his brows, swift-footed Achilles spoke to him.”\ndespite the integration of two popular treebanks for\nAG into Universal Dependencies (UD, de Marneffe\net al., 2021), many groups working on AG systems\nhave not compared their models against the results\nof models benchmarked on UD data, such as Straka\n(2018). We remedy these issues by evaluating our\nsystems and existing AG BERT models on the\ntwo authoritative treebanks covered by UD. The\ntasks we consider – dependency parsing, lemmati-\nzation, coarse, universal (UPoS) PoS tagging and\nfine-grained, language-specific (XPoS) tagging –\nare visualized in Figure 1.\nFor Latin, the issue does not arise thanks to the\nEvaLatin 2022 campaign (Sprugnoli et al., 2022),\nwhich has enabled direct comparison of models\nand has engendered strong models for Latin. Yet,\ndespite the impressive results achieved in EvaLatin,\nour trilingual models outperform the existing sys-\ntems on PoS tagging and lemmatization.\nLanguage Model Architectures. Language\nmodels can be categorized into three classes:\nencoder-only, decoder-only, and encoder-decoder\nmodels. Encoder-only models such as BERT (De-\nvlin et al., 2019) and ROBERTA (Liu et al., 2019)\nare best suited for tasks that aim to analyze com-\nplete sequences by sequence or token classifica-\ntion. Encoder-decoder models, on the other hand,\nare typically employed for conditional generation\ntasks, such as machine translation. Currently, all\nthree models for ancient languages are BERT and\nthus encoder-only architectures.\nWe argue that an encoder-decoder model, such\nas T5 (Raffel et al., 2020), is a useful addition\nto this encoder-only landscape. First, it enlarges\nthe space of possible NLP tasks for AG, enabling\nus, e.g., to cast lemmatization as a sequence-to-\nsequence task and to explore machine translation\nfor ancient languages. Second, it allows us to com-\npare the encoder of an encoder-only model with\nthat of an encoder-decoder architecture, as they are\nboth trained on the same data with a similar pre-\ntraining objective. Finally, commonly used multi-\nlingual encoder-decoder models like MT5 (Xue\net al., 2021) and BYT5 (Xue et al., 2022) are not\npre-trained on Ancient Greek texts.\nAs we aim for optimally trained encoder-only\nmodels, we chose ROBERTA over BERT: its dy-\nnamic masking strategy exploits the pre-training\ndata better, and it has been shown that BERT’s\nNSP objective can be detrimental (Liu et al., 2019).\n3 Pre-trained Language Models for\nAncient Greek and Latin\n3.1 G Rε(BERT |T)A and PHIL (BERT |T)A\nGRεBERTA and PHIL BERTA are ROBERTAbase-,\nGRεTA and PHILTA are T5base-sized models. Both\nmodels are pre-trained using a masked language\nmodeling (MLM) objective. Specifically, in the\ncase of ROBERTA, wordpieces are masked during\nthe pre-training process, while for T5, spans are\nmasked. Although it has been shown that multilin-\ngual pre-training can lead to gains for low-resource\nlanguages through cross-lingual transfer, it remains\nan open question when exactly it is preferable to\nuse a multilingual instead of a monolingual model\n(Doddapaneni et al., 2021). To explore the impli-\ncations of multilinguality for AG language models,\nwe test different capabilities and possible interfer-\nences by comparing the different model types.\n3.2 PLM Fine-tuning for Downstream Tasks3\nPoS Tagging. PoS tagging for Ancient Greek typ-\nically aims for a complete morphological analysis:\n3The following descriptions remain neutral to different\nPLM types by referring to basic transformer components.\nWhere necessary, we will distinguish specific PLM types.\n15183\nNext to the word class, the model has to predict\neight fine-grained morphological attributes.4 We\nframe this sequence labeling task as a multi-task\nclassification problem applied to each token, with\nnine different classification heads per token on top\nof one shared encoder: We denote a sequence of\ntokens S of length nas S = w1,w2,...,w n and\nrefer to the contextualized embedding of each token\nas ei = Encoder(w1:n,i). As Byte Pair Encoding\n(Sennrich et al., 2016) splits words into subword\nunits, we represent each token using its first sub-\nword embedding in the encoder. Each of the nine\nattributes is predicted using a feed-forward network\napplied to the last encoding layer, followed by a\nsoftmax function. The total loss is calculated as:\nLtotal =\n8∑\nm=0\n1\n9Lm\nWe use this approach for the Perseus XPoS dataset.\nFor the other, less-detailed tagsets, we employ a\nsingle classification head.\nDependency Parsing. We follow Zhang et al.\n(2017) who cast dependency parsing as head se-\nlection. The model predicts a unique head for\neach token considered as a dependent. Since the\nmodel makes independent predictions, the obtained\ndependency graph can (in a few cases) be un-\nconnected and is then completed by the Chu-Liu-\nEdmonds algorithm (Chu and Liu, 1965; Edmonds,\n1967) for building non-projective trees – given that\nAG allows free word order. While Zhang et al.’s\n(2017) DENSE parser was based on a bi-directional\nLSTM, we define the model on top of the final hid-\nden states of the transformer encoders.\nFollowing Zhang et al. (2017), we add an artifi-\ncial ROOT token w0 and calculate the probability of\nthe word wj ∈{w0,w1,...,w N }being the head\nof the word wi ∈{w1,w2,...,w n}in Sas:\nphead(wj|wi,S) = exp(f(ej,ei))∑N\nk=0 exp(f(ek,ei))\nwhere f predicts the score of an edge (wj,wi) as\nfollows:\nf(ej,ei) =v⊤·tanh(U ·ej + W ·ei)\nHere, v is a weight vector and U, W weight matri-\nces. Dependency labels are predicted in a similar\n4Person, number, tense, mood, voice, gender, case, and\ndegree. Usually, many of these attributes are left empty. E.g.,\nonly adjectives have the attribute “degree”.\nfashion: Let gbe a single hidden-layer rectifier net-\nwork that takes as input the concatenation [ei; ej].\nThe probability for the label lis then computed as:\nplabel(l|wj,wi,S) = exp(g(ej,l, ei))∑\nl′∈L exp(g(ej,l′,ei))\nWhile Zhang et al. (2017) use the representations\nof their trained DENSE model as input for the label\nclassifier, we resort to the pre-trained embeddings.\nLemmatization. Current systems for lemmati-\nzation of AG, such as UDP IPE (Straka, 2018) or\nGLEM (Bary et al., 2017), are rule-based or use\na classifier to predict editing rules that modify a\ntoken’s pre- and suffixes. However, these complex\nscripts are not well-suited for a language like AG,\nwhich has many irregular forms that involve modi-\nfications of the word stem. An alternative approach\nis to utilize an encoder-decoder model that receives\nthe inflected form, the PoS tag, and (optionally)\nadditional information such as morphological fea-\ntures, as demonstrated for different languages by\nSchmid (2019) or Wróbel and Nowak (2022).\nYet, these earlier encoder-decoder-based lemma-\ntization models are purely word-based and rely on\npre-computed PoS tags or morphological features\nin a pipeline setting. By contrast, we propose a\nnovel T5-based lemmatization model that is (i)\ncontextualized, so that relevant morphological indi-\ncators can be inferred by the model on the fly from\nthe token’s surrounding context. (ii) The model\nworks end-to-end: it receives the surface form of\nthe word to be lemmatized in its full sentence con-\ntext and predicts its lemma without receiving or\npredicting PoS tags or morphological features. 5\nWe mark the t(arget) token to be lemmatized in\nits context using delimiter tokens <t_tok_beg>\nand <t_tok_end>. For instance, for the input sen-\ntence ξύνοιδα <t_tok_beg> ἐμαυτῷ <t_tok_-\nend>οὐδὲν ἐπισταμένῳwith the marked inflected\nt(arget) token ἐμαυτῷ, we expect as output the\nlemma ἐμαυτοῦ. We also experiment with provid-\ning, in addition, the target word as a sequence of in-\ndividual characters, delimited by an additional sep-\narator token <t_tok_sep>: ξύνοιδα <t_tok_-\nbeg> ἐμαυτῷ <t_tok_sep> ἐ μ α υ τ ῷ<t_-\ntok_end>οὐδὲν ἐπισταμένῳ.\nSemantic and World Knowledge Probing Tasks.\nSo far, we considered only morphological and syn-\n5However, multi-task learning for joint morphological ana-\nlysis and lemmatization is an interesting option that we did\nnot pursue here.\n15184\ntactic tasks. However, to evaluate the models more\ncomprehensively, it is necessary to also test their\nsemantic and world knowledge. Since such bench-\nmarks do not exist for AG or Latin, we create\ntwo small datasets to evaluate these aspects. In-\nspired by Talmor et al. (2020), we test whether\nthe language models can distinguish synonyms\nfrom antonyms. For this task, we input a sentence,\ne.g., τὸ χρήσιμον καὶ τὸ ἀγαθόν:<mask>ὁμοῖά\nἐστιν (“the useful and the good: they are <mask>\nsimilar”), and the model has to predict either οὐχ\n(“not”) or πάντως (“very”). Talmor et al. (2020)\ncast a similar task for English as a zero-shot MLM\nprediction problem using BERT and ROBERTA.\nHowever, with our prompt, the models always pre-\ndict οὐχ (“not”), regardless of the provided word\npairs. Experiments with variations of the prompt\nhave led to similar difficulties. Hence, we evalu-\nate this task in a few-shot setting, fine-tuning the\nMLM-head on 10 to 50 shots of synonyms and\nantonyms each, to prepare them for the task.\nSimilarly, we construct a dataset of family re-\nlationships between (mythical) heroes and gods.\nHere, the model is given a phrase, such as Τηλέ-\nμαχος ὁ τοῦ<mask>παῖς (“Telemachus, son of\n<mask>”), and has to predict the correct entity\n(in this case, Odysseus). For this task, we test the\nmodels in a zero-shot setting. However, this task\ncannot be solved by most encoder-only models, as\nthe masked names typically consist of more than a\nsingle wordpiece. Thus, for this task, we evaluate\nonly GRεTA and PHILTA, which can predict full\nentity names. By comparing the mono- and multi-\nlingual variants, we assess the models’ acquired\nworld knowledge as well as potential effects that\nmay be induced by multilingual training: Given\nthat Greek and Roman mythology share many of\nthese gods, yet by different names, the multilingual\nmodel may be able to acquire additional knowledge\nfrom the Latin pre-training data, to solve the task\nformulated in Ancient Greek. We describe both\ndatasets in Appendix B.2.\n3.3 Acquisition of Pre-training Data\nAncient Greek. To cover a wide range of di-\nalects, topics, and time periods of Ancient Greek,\nwe make use of four different data sources: (the\nGreek part of) the Open Greek & Latin Project, 6\nthe CLARIN corpus Greek Medieval Texts,7 the\n6https://opengreekandlatin.org/.\n7https://inventory.clarin.gr/corpus/890.\nPatrologia Graeca,8 and the Internet Archive (IA).9\nWhile the first three sources contain born-digital\ntextual data, the IA online library provides books\nin the public domain along with their OCR tran-\nscriptions.\nHowever, we found the partition of texts labeled\nas Ancient Greek in the IA to be incomplete and\nnoisy: only a small fraction of the books contain-\ning AG text was labeled as such, and only few of\nthem were transcribed with OCR settings support-\ning Greek characters. We hence extracted a novel\ndata partition from the IA that was then fully re-\nOCRed by the Internet Archive to ensure correct\ntranscription. To select a large number of high-qua-\nlity texts, we applied a complex retrieve and filter\nprocedure, focusing not only on (i) text quality, but\nalso on (ii) collecting purely Ancient Greek texts,\navoiding inclusion of texts in different languages,\nsuch as Latin, English, or German that can co-occur\nin the same book, and on (iii) filtering duplicates.\nLatin and English. Acquiring pre-training data\nfor Latin was facilitated by the Corpus Corporum\nproject,10 a meta-repository of Latin corpora that\noffers a comprehensive representation of the Latin\nlanguage. All this data was kindly offered to us.\nFor English, we collect pre-training data from\nvarious sources, aiming for texts that are related to\nantiquity, by being focused on the same topics that\nwe find in ancient texts – as opposed to modern\nthemes. To this end, we utilize English translations\nof Latin and Ancient Greek texts as pre-training\ndata. Furthermore, we ensure that the amount of\nEnglish data is of similar size as the ancient texts,\nto prevent the models from being overwhelmed by\na large number of English texts.\nStatistics of pre-training data in Table 1. More\ndetails on corpus creation and statistics in Ap-\npendix C.\n3.4 Pre-training Process\nEven though our filtering of the IA corpus re-\nsulted in high-quality texts, the corpus is neces-\nsarily noisier than the born-digital texts. We there-\nfore start pre-training on the IA data, and continue\nwith the born-digital texts. Our tokenizers and the\nmultilingual variants are trained on the born-digital\ntexts only. For further pre-training details, see Ap-\npendix A.\n8https://patristica.net/graeca/.\n9https://archive.org/.\n10https://www.mlat.uzh.ch/.\n15185\nLanguage Dataset Number of Tokens\nAncient Greek\nOpen Greek & Latin 30.0million\nGreek Medieval Texts 3.3million\nPatrologia Graeca 28.5million\nInternet Archive 123.3million\nOverall 185.1 million\nLatin Corpus Corporum 167.5 million\nEnglish Overall 212.8 million\nTable 1: Statistics of the pre-training datasets. Only\nOpen Greek & Latin is used by Singh et al. (2021) and\nYamshchikov et al. (2022) for their AGBERT models.\nToken counts determined by UNIX command wc -w.\n4 Experiments\nWe run the experiments outlined in Section 3.2 to\nprovide insight into the performances achieved by\ndifferent model types and in relation to prior SoTA.\n4.1 Datasets\nAncient Greek. For the PoS tagging, dependency\nparsing, and lemmatization tasks, we evaluate the\nPLMs for AG on the data provided by the Perseus\nand the PROIEL datasets, which are both integrated\ninto Universal Dependencies 2.10 (de Marneffe\net al., 2021).\nTo probe our models for semantic and world\nknowledge (see Section 3.2), we use our newly\nconstructed datasets, described in Appendix B.2.\nLatin. For Latin, we resort to the treebank used\nin EvaLatin 2022 (Sprugnoli et al., 2022), which\ncovers three tasks: PoS tagging, lemmatization, and\nfeature identification. Since no data for dependency\nparsing is provided, we restrict our evaluation to\nPoS tagging and lemmatization. In EvaLatin, in-\nstead of constructing test data by drawing samples\nfrom the initial data set, the test data exhibits dif-\nferent degrees of distribution differences in rela-\ntion to the training data. For each task, three test\nsets are provided: The Classical set belongs to the\nsame genre and time period as the training data, but\ncomes from an author not included in the training\ndata. The Cross-genre data includes two works\nthat belong to different genres, yet being written\nduring roughly the same time period. The Cross-\ntime test set is based on text written in the 15th\ncentury, which is significantly later than the texts\nof the training data.\nIn Table 2, we summarize the diverse tasks un-\nder consideration with their corresponding metrics,\nthe used evaluation datasets, the model architec-\ntures, and the pre-trained language models that\nare applicable to the respective task. Further de-\ntails, including dataset statistics, are provided in\nAppendix B.1.\n4.2 Models and Baselines\nAncient Greek. To showcase the capabilities of a\nrecent system tailored to AG, we report the results\nof the taggers provided by the Classical Language\nToolkit (Johnson et al., 2021).11 As a baseline, we\nuse the currently best-performing system, UDP IPE\n(Straka et al., 2019), a transformer-based multi-\ntask architecture that utilizes multilingual BERT,\ntrainable word embeddings, and character embed-\ndings.12 In addition, to directly assess the benefits\nof using our monolingual model, we replace this\nmultilingual BERT with our G RεBERTA model.\nFor PoS tagging and dependency parsing, we fur-\nther compare to both prior encoder models trained\non AG texts. We use the PoS tagger and DENSE\n(Section 3.2) to evaluate both AGBERT models as\nwell as our GRεBERTA and PHIL BERTA models.\nWe apply the same approach to GRETA’s encoder\n(GRETA-ENC) to investigate its behavior.\nFor lemmatization, we compare the performance\nof CLTK and UDP IPE with that of our full-fledged\nT5 models. To predict a lemma during inference,\nwe use beam search with a width of 20.\nLatin. For Latin, we report the results of both\nteams that participated in the EvaLatin 2022 com-\npetition: Team KRAKÓW (Wróbel and Nowak,\n2022) utilizes the XLM-R OBERTAlarge (Conneau\net al., 2020) model for PoS tagging, team KU-\nLEUVEN (Mercelis and Keersmaekers, 2022) em-\nploys an ELECTRA model. For lemmatization,\nWróbel and Nowak (2022) use BYT5small (Xue\net al., 2022), a multilingual encoder-decoder model\nsimilar to MT5 (Xue et al., 2021) that operates\non UTF-8 bytes instead of subwords. Mercelis\nand Keersmaekers (2022) implement a cascaded\napproach that resembles the Greek lemmatizer\nGLEM (Bary et al., 2017): If a rule-based lookup\n11 From the multiple taggers offered by the CLTK we\nchoose the one that achieved best performance on the valida-\ntion set. For the Perseus dataset, this is a TnT tagger (Brants,\n2000), while for PROIEL, it is Stanza (Qi et al., 2020). Note,\nhowever, that it is not possible to directly compare the results\nto those of the other models, as they were trained on differ-\nent data splits and using an older version of the dataset (cf.\nhttps://github.com/cltk/greek_treebank_perseus).\n12We report scores of the most recent, unpublished\nversion of UDP IPE (https://ufal.mff.cuni.cz/udpipe/\n2/models#universal_dependencies_210_models) and the\nscores obtained when training UDP IPE ourselves.\n15186\nPoS Tagging Dependency Parsing Lemmatization\nUPoS XPoS Unlabeled Labeled\nTask DescriptionPoS tagging with univer-\nsally applicable, coarse\nPoS tags\nPoS tagging with\nlanguage-specific, fine-\ngrained tags; complete\nmorphological analysis\nin the case of Perseus\npredicting the head\nof each token in text\npredicting the head\nand relation type of\neach token in text\npredicting the lemma\nof each token in text\nMetric Accuracy Accuracy UAS LAS Accuracy\nDatasets Perseus✓\nPROIEL✓\nEvaLatin✓\nPerseus✓\nPROIEL✓\nEvaLatin✗\nPerseus✓\nPROIEL✓\nEvaLatin✗\nPerseus✓\nPROIEL✓\nEvaLatin✗\nPerseus✓\nPROIEL✓\nEvaLatin✓\nModel ArchitectureEncoder + Classification\nHead\nEncoder + Classification\nHead(s)\nEncoder + DENSE Encoder + DENSE Encoder-decoder\nPLM Instances (GRε|PHIL)BERTA,\n(GRε|PHIL)TA-ENC\n(GRε|PHIL)BERTA,\n(GRε|PHIL)TA-ENC\n(GRε|PHIL)BERTA,\n(GRε|PHIL)TA-ENC\n(GRε|PHIL)BERTA,\n(GRε|PHIL)TA-ENC\n(GRε|PHIL)TA\nTable 2: Summary of the tasks under consideration.\nreturns multiple lemmata, the system tries to disam-\nbiguate between these possibilities by means of the\npredicted PoS tag. To further clarify any remain-\ning ambiguities, a classifier is trained to select the\ncorrect lemma from the available options.\n5 Results\nAncient Greek. We present the results for PoS\ntagging and dependency parsing for Ancient\nGreek on the Perseus dataset in Table 3. The\nPROIEL dataset seems to be easier to solve, as all\nmodels achieve performances that are much closer\nto each other (see Appendix D). Since the overall\ntrends are consistent across both datasets, we focus\nour discussion on the results on the Perseus dataset.\nAs seen in Table 3, the CLTK performs clearly\nbelow all other models on both tasks. While the\nCLTK is not directly comparable to the other mod-\nels (see fn. 11), the evaluation still provides a per-\nspective on the capabilities of the de facto only\navailable framework for processing AG text.\nUDP IPE provides a strong baseline, which AG\nBERT (Yamshchikov et al., 2022) is unable to\nconsistently outperform. By contrast, all other\nPLMs show clear gains over UDP IPE . The mono-\nlingual, encoder-only GRεBERTA model consis-\ntently performs best on all tasks. Interestingly, the\nperformance of GRETA-ENC on PoS tagging is\nslightly worse than that of PHIL BERTA, while\nit achieves better results for dependency parsing.\nThis trend has also been observed in initial exper-\niments. We elaborate on the behavior of GRεTA-\nENC and PHIL BERTA in Section 6.\nResults for Lemmatization are shown in Table 4.\nHere, augmenting UDP IPE with GRεBERTA’s pre-\ntrained embeddings does not lead to better scores.\nWe attribute this to the tokenization process and\nrefer to our discussion in Section 6. GRεTA, on the\n20 40 60 80 100\nTraining Examples\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75Accuracy\nPhilBERT a\nGreBERT a\nSingh et al.\nYamshchikov et al.\nFigure 2: Synonym/antonym disambiguation accuracy\nfor growing few-shot sizes: GRεBERTA and PHIL -\nBERTA vs. AG BERT models. We use equal amounts\nof synonyms and antonyms (a run with 20 samples in-\ncludes 10 synonyms and 10 antonyms). We use k-fold\ncross-validation. Error bars show standard deviation.\nother hand, demonstrates strong encoder-decoder\ncapabilities and significantly outperforms UDP IPE .\nProviding GRεTA with the individidual characters\nof the target word leads to a small gain.\nThe results of the Synonym/antonym disam-\nbiguation taskare visualized in Figure 2. Again,\nGRεBERTA and PHIL BERTA demonstrate higher\nscores compared to the AG BERT models. We\nobserve the same for GRεTA and PHILTA (cf. Fig-\nure 4 in Appendix D). Our monolingual models\nand their multilingual counterparts perform almost\nequally, especially taking into account the overlap-\nping standard deviation bands. We see a minimal\ntrend for PHILTA to gain over GRεTA in Figure 4,\nbut our small datasets do not allow drawing firm\nconclusions on their relative performance.\nFinally, we report zero-shot results for the Fam-\nily relationship taskin Table 5. As the T5-based\nmodels have been pre-trained to predict multiple\nmasked spans at once, they tend to predict, for each\nsample, more than a single entity. We interpret the\noutput as a ranked list and report recall@k, evalu-\n15187\nModel PoS Tagging Dependency Parsing\nUPoS XPoS UAS LAS\nCLTK 68.83 47 .21 59 .21 43 .24\nUDPIPE(official) 92.88 85 .60 80 .32 74 .53\nUDPIPE(ours) 92.36 (0.09) 84.72 (0.06) 78.74 (0.04) 73.14 (0.06)\nUDPIPE+ GRεBERTA 95.74 (0.06) 90.95 (0.07) 86.30 (0.14) 82.15 (0.14)\nAG BERT (Singh et al., 2021) 94.92 (0.18) 88.27 (0.27) 84.03 (0.12) 78.80 (0.37)\nAG BERT (Yamshchikov et al., 2022)92.50 (0.03) 84.56 (0.13) 80.34 (0.11) 74.22 (0.21)\nGRεTA-ENC 94.44 (0.14) 89.03 (0.13) 87.32 (0.04) 83.06 (0.07)\nPHILBERTA 95.60 (0.21) 90.41 (0.18) 86.99 (0.06) 82.69 (0.06)\nGRεBERTA 95.83(0.10) 91.09(0.02) 88.20(0.11) 83.98(0.21)\nTable 3: PoS tagging and dependency parsing results on the Ancient Greek Perseus dataset. The results are averaged\nover three runs with different random seeds, and the standard deviation is indicated in parentheses, except for the\nCLTK and UDP IPE (reported results). Note also that the CLTK is not trained on exactly the same data as the other\nmodels and therefore not strictly comparable.\nModel Accuracy\nCLTK 76.10\nUDPIPE(official) 86.70\nUDPIPE(ours) 84.50 (0.09)\nUDPIPE+ GRεBERTA 85.56 (0.06)\nPHILTA 90.02 (0.02)\nPHILTA+ Chars 90.66 (0.01)\nGRεTA 90.80 (0.10)\nGRεTA+ Chars 91.14(0.10)\nTable 4: Lemmatization results for Ancient Greek on\nthe Perseus dataset. Results are averaged over three\nruns, with standard deviation in parentheses, except for\nthe CLTK and UDP IPE (reported results).\nModel k=1 k=5 k=10 k>10\nGRεTA 4.39 9.65 10.53 10.96\nPHILTA 3.07 8.33 11.40 11.84\nTable 5: Zero-shot family relationships task (recall@k).\nating whether the correct entity is contained in the\nfirst 1, 5, 10, and >10 predictions, restricting the\nmaximum sequence length to 50 wordpieces.\nLatin. The PoS tagging and lemmatization scores\non EvaLatin 2022 are reported in Table 6. While\nthe performance scores of all models are rather\nclose to each other, our trilingual models consis-\ntently outperform the EvaLatin 2022 participant\nsystems across all three subtasks. PHIL BERTA\nreaches even higher scores than KRAKÓW -OPEN\non PoS tagging, which leverages additional an-\nnotated data. On lemmatization, PHILTA simi-\nlarly outperforms KRAKÓW -CLOSED on the Classi-\ncal, Cross-genre, and Cross-time subtasks by 2.25,\n1.78, and 0.23 percentage points, respectively, but\ndoes not outperform KRAKÓW -OPEN on the Cross-\ngenre and the Cross-time subtask.\nModel Classical Cross-genreCross-time\nUPoS\nKRAKÓW-OPEN 97.99 96 .06 92 .97\nKRAKÓW-CLOSED97.61 94 .62 92 .70\nKU-LEUVEN 96.33 92 .31 92 .11\nPHILBERTA 98.23(0.06) 96.59(0.15) 93.25(0.12)\nLemmatiz.\nKRAKÓW-OPEN 97.26 96 .45 92 .15\nKRAKÓW-CLOSED95.08 91 .62 91 .68\nKU-LEUVEN 85.44 86 .48 84 .60\nPHILTA+ Chars 97.33(0.04) 93.40(0.13) 91.91(0.04)\nTable 6: PoS tagging and lemmatization results\n(EvaLatin 2022 dataset). KRAKÓW -OPEN uses addi-\ntional data.\n6 Analyses and Discussion\nTraining Behavior of GRεTA-ENC. While\nGRETA-ENC and GRεBERTA are of similar size\n(Table 7) and pre-trained with comparable objec-\ntives, GRεTA-ENC performs slightly worse than\nGRεBERTA. One reason may be that in a T5\nmodel, some important information is distributed\nacross encoder and decoder. This raises the ques-\ntion of whether encoders in encoder-decoder mod-\nels are trained suboptimally, and whether improve-\nments may be obtained by combining separately\npre-trained encoders and decoders, or by pre-\ntraining the encoder before adding the decoder. An-\nother reason may be that the encoder is not accus-\ntomed to using its classification head. Here again,\nit may be advantageous to pre-train the encoder be-\nfore extending it to encoder-decoder pre-training.\nIn Figure 3 we compare the PoS tagging valida-\ntion accuracy of GRεTA-ENC to that of a randomly\ninitialized T5 encoder (same size). GRεTA-ENC\nperforms much worse than the randomly initial-\nized model after one epoch, reaching only approx-\nimately 6%. However, while the randomly initial-\nized model stagnates, GRεTA-ENC outperforms\nthe randomly initialized model after two epochs,\nsignificantly improving its performance thereafter.\n15188\n2 4 6 8 10\nEpochs\n0.2\n0.4\n0.6\n0.8Validation Accuracy\nGreBERT a\nGreT a-Enc\nGreBERT a-random\nGreT a-Enc-random\nFigure 3: Validation accuracy (AG XPoS Tagging on\nPerseus) for GRεTA-ENC and GRεBERTA and random-\nly initialized counterparts over various training epochs.\nBy contrast, GRεBERTA reaches a high validation\naccuracy already after one epoch. We see the same\ntrend with different random seeds and for depen-\ndency parsing, but it is most apparent in Perseus\nXPoS tagging.\nLemmatization as a Character-based Task.As\nseen in Table 4, augmenting UDP IPE with GRε-\nBERTA does not lead to significant improvement\nfor lemmatization. This we attribute to the tokeniza-\ntion process. GRεBERTA uses wordpieces, which\ncontain little information about individual charac-\nters. We hypothesize that UDP IPE ignores the\nGRεBERTA embeddings for lemmatization and in-\nstead relies on its own additional character embed-\ndings. Accordingly, explicitly providing GRεTA\nwith the individual characters of the inflected word\nform leads to a slight increase in performance.\nThis explanation can also shed light on the suc-\ncess of the UTF-8 bytes-based BYT5 model for\nlemmatization in Latin. This model was chosen by\nWróbel and Nowak (2022), after initial experiments\nwith the wordpiece-based MT5 (Xue et al., 2021)\nhad underperformed. Future work on (AG) lemma-\ntization could therefore investigate whether Byte\nPair Encoding-based models can be augmented\nwith character embeddings as additional input.\nEffect of Multilinguality. Table 3 shows that\nPHIL BERTA consistently performs slightly worse\ncompared to monolingual GRεBERTA on morpho-\nlogical and syntactic tasks. We attribute this to the\ncurse of multilinguality (Conneau et al., 2020): the\ncapacity of the trilingual models is split between\nthree languages. Still, both models achieve strong\nresults on AG and Latin tasks and can be especially\nuseful in tasks that require multilingual knowledge,\nas in MT or glossing tasks. Our small-sized knowl-\nedge probing tasks show very similar performance\nfor both model types. While the size of our data\ndoes not allow for firm conclusions, this is in line\nwith Kassner et al. (2021), who find no improved\nknowledge representation in multilingual PLMs.\n7 Conclusion\nWe introduce four strong language models for Clas-\nsical Philology, including the first encoder-decoder\nPLMs for Ancient Greek and Latin. We rigorous-\nly benchmark our models and prior work on vari-\nous tasks, demonstrating strong improvement over\nthe SoTA. We showcase the versatility of encoder-\ndecoder models, (i) by offering a novel end-to-end\ncontextualized lemmatization model for AG and\nLatin, with a greatly simplified architecture that\nclearly outperforms prior work; (ii) while MLM in\nencoder-only models is restricted to single-token\npredictions, our T5-based models exhibit great flex-\nibility for formulating probing tasks, which help\nexploring what models learn from pre-training data.\nConsidering the two investigated model dimen-\nsions, our work (iii) sheds light on differences be-\ntween the encoders of T5 vs. ROBERTA, where\nthe former tends to exhibit slower learning curves;\n(iv) our monolingual models outperform the multi-\nlingual ones in monolingual morphological and\nsyntactic tasks, without clear trends on small-scale\nsemantic and knowledge probing tasks.\nLimitations\nWhile we aim for a comprehensive analysis of ex-\nisting methods (such as lemmatization) and model\ntypes for Ancient Greek and other Classical lan-\nguages, there are limits to exhaustively exploring\nthe full space of variations and rigorously eval-\nuating their impact on model performance. For\nexample, we could not comprehensively evaluate\nthe effects of (i) the pre-training corpora, as we did\nnot re-train a BERT model for Ancient Greek, to\npin down the exact difference between prior BERT\nmodels (which were trained on smaller data before)\nand our own models, which are based on inherently\nstronger model types; similarly, we did not induce\nLatin ROBERTA and T5 models, to confirm the\ndifferences between mono- and multilingual mod-\nels for language-specific Latin tasks. (ii) In a simi-\nlar vein, we did not compare different model sizes.\nHowever, we studied prior work and scaling laws\nand believe that the base model is appropriate for\nthe size of our training data. Further factors of this\ntype concern (iii) hyperparameter settings and (iv)\nother factors in isolation.\nNot only do we miss sufficient computational\n15189\nresources to perform such manifold ablations and\ncomparative assessments, we also considered the\ncarbon footprint that such experiments cause and\nwhich does not stand up to the insights that could\npossibly be gained from more experiments.\nFor these reasons, we focused on two selected di-\nmensions of variants that we believe to be valuable\nfor a community interested in Classical languages:\n(i) We tried to answer questions as to when\nmultilingual models can be profitably used, and\n(ii) aimed to showcase various potential advantages\nof encoder-decoder models, which by now have not\nbeen considered in studies on Classical languages.\nAnother clear limitation lies in the size of the\ndemonstrated semantic and knowledge probing\ntasks. (i) They are of small size, and we cannot,\ntherefore, draw firm conclusions as to, e.g., the ef-\nfect of multilinguality. Also, the synonym/antonym\ndisambiguation task is presumably the most diffi-\ncult one. As a counter-balance, we used a more\ntangible task for knowledge probing, by choosing\nfamily relationships, which we expect to be fre-\nquently found in the pre-training corpora.\n(ii) A further limitation we find for the knowl-\nedge probing tasks resides in the size of our trained\nmodels and the underlying pretraining data. This\nlimitation could be one that is not easy to over-\ncome. But we still encourage the community to\ncreate similar probing task datasets. Future work\nmay find appropriate ways of data augmentation,\nor transfer learning methods that are applicable to\nhistorical languages so that further progress and\ninsight will be possible.\nEthics Statement\nIt is a computationally demanding task to pre-train\nlarge language models. However, transfer learning\nopens the possibility to fine-tune our pre-trained\nmodels, which showed strong performances, in a\nreasonable amount of time.\nThe texts utilized for pre-training the models\nmay well exhibit biases related to ancient perspec-\ntives of the world. We do not view this as an issue,\nas the proposed language models for historical lan-\nguages are intended for academic use and do not\nhave practical, everyday applications.\nAcknowledgments\nWe are deeply indebted to the Internet Archive team\nfor their continuous support by creating new OCR\ntranscriptions of the misclassified Greek books, and\nto our anonymous reviewers for their comments,\nwhich have helped to significantly improve the\npaper. We thank Nina Stahl and Thomas Kuhn-\nTreichel for their help in creating our semantic and\nknowledge probing tasks, as well as Jan Ctibor and\nPhilipp Roelli for providing us with the invaluable\nCorpus Corporum data. Finally, we acknowledge\nand thank for crucial support from the Google TPU\nResearch Cloud program, for granting us access to\ntheir TPUs.\nReferences\nDavid Bamman and Patrick J Burns. 2020. Latin BERT:\nA contextual language model for classical philology.\narXiv preprint arXiv:2009.10053.\nCorien Bary, Peter Berck, and Iris Hendrickx. 2017. A\nMemory-Based Lemmatizer for Ancient Greek. In\nProceedings of the 2nd International Conference on\nDigital Access to Textual Cultural Heritage , DAT-\neCH2017, page 91–95, New York, NY , USA. Associ-\nation for Computing Machinery.\nM. Berti. 2019. Digital Classical Philology: Ancient\nGreek and Latin in the Digital Revolution . Age of\naccess? Grundfragen der Informationsgesellschaft.\nDe Gruyter.\nThorsten Brants. 2000. TnT – a statistical part-of-\nspeech tagger. In Sixth Applied Natural Language\nProcessing Conference , pages 224–231, Seattle,\nWashington, USA. Association for Computational\nLinguistics.\nR. Busa. 1980. The Annals of Humanities Computing:\nThe Index Thomisticus. Computers and the Humani-\nties, 14(2):83–90.\nGiuseppe G. A. Celano, Gregory Crane, and Saeed Ma-\njidi. 2016. Part of Speech Tagging for Ancient Greek.\nOpen Linguistics, 2(1).\nYoeng-Jin Chu and Tseng-Hong Liu. 1965. On shortest\narborescence of a directed graph. Scientia Sinica,\n14(10):1396.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nGregory Crane. 1991. Generating and Parsing Classical\nGreek. Literary and Linguistic Computing, 6(4):243–\n245.\n15190\nMarie-Catherine de Marneffe, Christopher D. Man-\nning, Joakim Nivre, and Daniel Zeman. 2021. Uni-\nversal Dependencies. Computational Linguistics ,\n47(2):255–308.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nSumanth Doddapaneni, Gowtham Ramesh, Mitesh M\nKhapra, Anoop Kunchukuttan, and Pratyush Kumar.\n2021. A primer on pretrained multilingual language\nmodels. arXiv preprint arXiv:2107.00676.\nJack Edmonds. 1967. Optimum branchings. Journal\nof Research of the National Bureau of Standards B,\n71(4):233–240.\nKyle P. Johnson, Patrick J. Burns, John Stewart, Todd\nCook, Clément Besnier, and William J. B. Mattingly.\n2021. The Classical Language Toolkit: An NLP\nframework for pre-modern languages. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing:\nSystem Demonstrations, pages 20–29, Online. Asso-\nciation for Computational Linguistics.\nNora Kassner, Philipp Dufter, and Hinrich Schütze.\n2021. Multilingual LAMA: Investigating knowledge\nin multilingual pretrained language models. In Pro-\nceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 3250–3258, Online.\nAssociation for Computational Linguistics.\nAlek Keersmaekers. 2019. Creating a richly annotated\ncorpus of papyrological Greek: The possibilities of\nnatural language processing approaches to a highly\ninflected historical language. Digital Scholarship in\nthe Humanities, 35(1):67–82.\nManfred Kern, Alfred Ebenbauer, and Silvia Krämer-\nSeifert. 2003. Lexikon der antiken Gestalten in den\ndeutschen Texten des Mittelalters. Walter de Gruyter.\nJohn Koutsikakis, Ilias Chalkidis, Prodromos Malaka-\nsiotis, and Ion Androutsopoulos. 2020. GREEK-\nBERT: The Greeks Visiting Sesame Street. In 11th\nHellenic Conference on Artificial Intelligence, SETN\n2020, page 110–117, New York, NY , USA. Associa-\ntion for Computing Machinery.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nWouter Mercelis and Alek Keersmaekers. 2022. An\nELECTRA model for Latin token tagging tasks. In\nProceedings of the Second Workshop on Language\nTechnologies for Historical and Ancient Languages,\npages 189–192, Marseille, France. European Lan-\nguage Resources Association.\nD. W. Packard. 1968. A Concordance to Livy. A Con-\ncordance to Livy. Harvard University Press.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and\nChristopher D. Manning. 2020. Stanza: A Python\nnatural language processing toolkit for many human\nlanguages. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nHelmut Schmid. 2019. Deep Learning-Based Morpho-\nlogical Taggers and Lemmatizers for Annotating His-\ntorical Texts. In Proceedings of the 3rd International\nConference on Digital Access to Textual Cultural\nHeritage, DATeCH2019, page 133–137, New York,\nNY , USA. Association for Computing Machinery.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nPranaydeep Singh, Gorik Rutten, and Els Lefever. 2021.\nA pilot study for BERT language modelling and mor-\nphological analysis for ancient and medieval Greek.\nIn Proceedings of the 5th Joint SIGHUM Workshop\non Computational Linguistics for Cultural Heritage,\nSocial Sciences, Humanities and Literature , pages\n128–137, Punta Cana, Dominican Republic (online).\nAssociation for Computational Linguistics.\nRachele Sprugnoli, Marco Passarotti, Flavio Massim-\niliano Cecchini, Margherita Fantoli, and Giovanni\nMoretti. 2022. Overview of the EvaLatin 2022 eval-\nuation campaign. In Proceedings of the Second\nWorkshop on Language Technologies for Historical\nand Ancient Languages, pages 183–188, Marseille,\nFrance. European Language Resources Association.\nMilan Straka. 2018. UDPipe 2.0 prototype at CoNLL\n2018 UD shared task. In Proceedings of the CoNLL\n2018 Shared Task: Multilingual Parsing from Raw\nText to Universal Dependencies , pages 197–207,\nBrussels, Belgium. Association for Computational\nLinguistics.\nMilan Straka, Jana Straková, and Jan Hajiˇc. 2019. Eval-\nuating contextualized embeddings on 54 languages in\n15191\npos tagging, lemmatization and dependency parsing.\narXiv preprint arXiv:1908.07448.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. oLMpics-on what language\nmodel pre-training captures. Transactions of the As-\nsociation for Computational Linguistics, 8:743–758.\nAlessandro Vatri and Barbara McGillivray. 2020.\nLemmatization for Ancient Greek: An experimental\nassessment of the state of the art. Journal of Greek\nLinguistics, 20(2):179 – 196.\nKrzysztof Wróbel and Krzysztof Nowak. 2022.\nTransformer-based part-of-speech tagging and\nlemmatization for Latin. In Proceedings of the\nSecond Workshop on Language Technologies for\nHistorical and Ancient Languages, pages 193–197,\nMarseille, France. European Language Resources\nAssociation.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-\nRfou, Sharan Narang, Mihir Kale, Adam Roberts,\nand Colin Raffel. 2022. ByT5: Towards a token-free\nfuture with pre-trained byte-to-byte models. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:291–306.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nIvan Yamshchikov, Alexey Tikhonov, Yorgos Pantis,\nCharlotte Schubert, and Jürgen Jost. 2022. BERT in\nPlutarch’s Shadows. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 6071–6080, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nXingxing Zhang, Jianpeng Cheng, and Mirella Lapata.\n2017. Dependency parsing as head selection. In\nProceedings of the 15th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Volume 1, Long Papers , pages 665–676,\nValencia, Spain. Association for Computational Lin-\nguistics.\n15192\nHyperparameterGRεBERTA PHILBERTA GRεTA PHILTA\nAdamϵ 1·10−8 1·10−8 1·10−8 1·10−8\nAdamβ1 0.9 0 .9 0 .9 0 .9Adamβ2 0.999 0 .999 0 .999 0 .999Attention Dropout0.1 0 .1 0 .1 0 .1Attention Heads12 12 12 12Batch Size 128 256 512 512dff — — 2048 2048dkv — — 64 64dmodel — — 768 768Hidden Dropout0.1 0 .1 0 .1 0 .1Hidden Size 768 768 — —Learning Rate (LR)5·10−5 5·10−5 5·10−3 5·10−3\nLR Scheduler linear linear linear linearNb. of Layers12 12 2 ·12 2 ·12Nb. of Parameters126mill. 135mill. 223mill.247mill.Train Epochs 50,100 0 ,100 50 ,100 0,100Warmup Steps0 0 10000 10000Weight Decay0 0 0 .01 0 .01\nTable 7: Pre-training hyperparameters.\nA Training Details\nA.1 Pre-training Details\nWe pre-train the monolingual models for 50 epochs\non the Internet Archive corpus and continue pre-\ntraining for 100 epochs on the born-digital texts, the\ntrilingual models were trained for 100 epochs on\nthe born-digital texts. The tokenizers were trained\non the born-digital data only. GRεBERTA and\nPHIL BERTA were trained on an NVIDIA A100-\nPCIE-40GB, GRεTA and PHILTA on a Google\nTPU v2-8. Training took between 3 and 7 days.\nFurther details in Table 7.\nA.2 Fine-tuning Details\nWe train every Greek model for 50 epochs on an\nNVIDIA GeForce GTX 1080 Ti, evaluating the\nmodel after every epoch on the validation set and\nusing early stopping with a stopping patience of\n5. As the EvaLatin dataset does not provide a val-\nidation set, we use 2% of the training data as the\nvalidation set. Furthermore, since the EvaLatin\ndataset is larger than the Greek datasets, we set\nthe maximum number of training epochs to 20 for\nthe Latin models. Depending on the treebank and\nthe task, training the models took approximately 1\nhour (PoS tagging), 5–7 hours (dependency pars-\ning), and 1–3 days (lemmatization). Further details\nin Table 8. We did not experiment with different\nhyperparameter settings, as our main goal was to\nprovide comparable and wide-ranging benchmark-\ning results.\nHyperparameter\nAdamϵ 1·10−8\nAdamβ1 0.9\nAdamβ2 0.999\nBatch Size 32\nEarly Stopping Patience5\nLearning Rate 1·10−4\nLearning Rate Scheduler linear\nRandom Seeds 42,1,2\nTrain Epochs 50\nWeight Decay 1·10−5\nTable 8: Fine-tuning hyperparameters.\nPerseusPROIELEvaLatin\nSentences (train) 11 476 15 014 15 785\nSentences (dev) 1137 1019 —\nSentences (test) 1306 1047 1960\nSentences (total) 13 919 17 080 17 745\nTokens (train) 159 895 187 033 316 573\nTokens (dev) 22 135 13 652—\nTokens (test) 20 959 13 314 45 544\nTokens (total) 202 989 213 999 362 117\nLemmata 13 413 9348 10 357\nForms 41 304 32 591 54 133\nUPoS Tags 14 14 16\nXPoS Tags 847 27 —\nDependency Relations25 33 —\nTable 9: Statistics of the Perseus, PROIEL, and\nEvaLatin datasets.\nB Downstream Task Details\nB.1 Universal Dependencies and EvaLatin\n2022\nFor PoS tagging, UD provides universal PoS tags\n(UPoS) and language-specific PoS tags (XPoS).\nUPoS consists of 17 tags used for all languages cov-\nered by UD.13 XPoS tags, on the other hand, can\nfollow a dataset-specific annotation scheme. While\nthe XPoS tags of the PROIEL dataset are similar\nto the UPoS tags, the Perseus dataset aims for a\ncomplete morphological analysis (cf. Section 3.2).\nSee Table 9 for further details and Table 2 for\nan overview. In line with common convention,\nwe report the accuracy for both PoS tag sets. For\ndependency parsing, we report the unlabeled at-\ntachment score (UAS) and the labeled attachment\nscore (LAS). The UAS indicates the percentage of\ntokens that have been assigned the correct head,\nwhereas for the LAS, both the predicted head and\nthe dependency label have to be correct. All results\nare obtained from the official evaluation script.14\n13In the case of AG, 3 of these 17 tags are not used.\n14https://universaldependencies.org/conll18/\nconll18_ud_eval.py and https://github.com/CIRCSE/\nLT4HALA/blob/master/2022/data_and_doc/conll18_\nud_eval_EvaLatin_2022_rev2.py.\n15193\nB.2 Semantic and World Knowledge\nSemantic Knowledge. We asked a graduate stu-\ndent and a doctoral candidate in the field of Classics\nto gather synonym and antonym pairs. Such word\npairs can be nouns and substantivized adjectives\nor substantivized infinitives. We then utilized a\npredefined template to generate sentences that in-\ncorporate the collected pairs. As this template does\nnot ensure grammaticality, the annotators manually\nedited the sentences. Subsequently, the sentences\nwere independently reviewed by both annotators,\ndeduplicated, and then verified by a professor of\nAncient Greek. All three annotators participated\non a voluntary basis and were not compensated for\ntheir contributions. One of the annotators is also a\nco-author of this paper.\n141 synonym and 146 antonym pairs were col-\nlected. While we publish all 287 examples, we\ndrop 5 randomly selected antonym pairs in our ex-\nperiments to ensure that the number of synonym\nand antonym pairs is equal. We train all language\nmodels for 10 epochs using a batch size of 4 and\nreport the averaged, cross-validated results.\nWorld Knowledge. This dataset was compiled\nby one of the previous annotators who is not a\nco-author of this paper. The annotator gathered\n228 examples with 11 different relations by read-\ning through Hesiod’s Theogony and by drawing\ninspiration from Kern et al. (2003), a lexicon that\ncontains comprehensive information about mythi-\ncal figures.\nC Acquisition of Pre-training Data\nC.1 Ancient Greek Pre-training Data\nOpen Greek & Latin Project.15 The Open\nGreek & Latin Project is an umbrella project cover-\ning various subprojects that aim toward the devel-\nopment of open-access corpus linguistic resources\nfor Latin and Classical Greek. Two of them, the\nPerseus Digital Library and the First Thousand\nYears of Greek project, contain Ancient Greek\ntexts, mostly covering texts that are typically as-\nsociated with classical antiquity, such as Homer,\nPlato, Herodotus, Euripides, and Plutarch. Already\nin this corpus, we find a wide variety of dialects and\nlanguage stages. The Open Greek & Latin Project\ncontains approximately 30 million tokens.\n15https://opengreekandlatin.org/.\nGreek Medieval Texts.16 The Greek Medieval\nTexts corpus offered by CLARIN covers writings\nfrom the fourth to the sixteenth century AD. It\ncontains religious, poetical-literary and political-\nhistorical texts as well as hymns and epigrams.\nStrictly speaking (and as the name suggests) the\ncorpus contains texts of late antiquity, and in par-\nticular, Medieval Greek. We argue, however, that\nAncient Greek and Medieval Greek, although dif-\nferent language stages, are strongly connected to\neach other and that our language models benefit\nfrom seeing more diverse data during pre-training.\nThis corpus contains about 3.3 million tokens and\nis licensed under the CC BY-NC 4.0 license.\nPatrologia Graeca.17 The Patrologia Graeca is a\nlarge collection of important Christian texts written\nin Greek, dating from the first until the fifteenth cen-\ntury AD. Since not all texts are machine-readable\nand available, we are restricted to those out of copy-\nright texts that are made accessible (around 28.5\nmillion tokens).\nInternet Archive.18 The Internet Archive is an\nonline library that provides texts obtained from pub-\nlic domain books via OCR. We found out that only\na small fraction of the books containing Ancient\nGreek text was labeled as such. Moreover, we dis-\ncovered that even less books were transcribed with\nOCR settings that allowed Greek characters. As a\nresult, many high-quality scans of Ancient Greek\ntexts were transcribed into incomprehensible se-\nquences of non-Greek characters. For example,\nthe verse ὦ γύναι ἦ μάλα τοῦτο ἔπος νημερτὲς\nἔειπες19 is transcribed as & yvvai, ff pdXa\ntovto Sttoˆ vrjpepTeˆ e=C/.7r=C9∗.\nEven though transcriptions of this nature may\nseem useless at first glance, they are nevertheless\nhelpful in identifying documents that have been in-\ncorrectly treated as non-Greek texts, for many com-\nmon words are relatively consistently transcribed.\nτοῦτο (“this”), for example, is often transcribed\ninto tovto. By searching for all books that contain\nthe word tovto, we can identify potential Greek\ntexts. This approach allows us to avoid the com-\nputationally intensive task of applying Greek OCR\nto every book in the Internet Archive, and instead\nfocus our efforts on a more targeted search. All\ncandidates are then filtered more aggressively: If\n16https://inventory.clarin.gr/corpus/890.\n17http://patristica.net/graeca/.\n18https://archive.org/.\n19Hom. Il. 3.204.\n15194\na candidate contains the five (presumably) Greek\nstopwords tovto(τοῦτο), kal(καί), tov(τόν), to\n(τό), and yap (γάρ) more than ten times, the candi-\ndate is considered to contain Greek text.\nWe argue that this method effectively mini-\nmizes false positives while retaining a high recall:\nSince Greek stopwords like τοῦτο (“this”) and καί\n(“and”) should be present often enough in every\nbook with a significant amount of text, our ap-\nproach should correctly classify them as Greek.\nNon-Greek texts, on the other hand, should hardly\ncontain all five stopwords more than ten times.\nThis procedure yields 25378 books, on which\nthe Internet Archive applies OCR with Ancient\nGreek as a target language. While our method reli-\nably detects Greek texts, it does not ensure a high\nscan (and therefore also text) quality. In order to\nuse solely high-quality data, we keep only lines in\nwhich more than 90% of tokens are also present in\nthe born-digital vocabulary. A similar approach is\nused by Bamman and Burns (2020), who use Latin\ntexts from the Internet Archive as pre-training data\nfor Latin BERT. They “retain only those books\nwhere at least 40% of tokens are present in a vocab-\nulary derived from born-digital texts”. We argue\nthat it is more sensible to include or disregard indi-\nvidual lines instead of whole books: Almost every\nGreek text contains a Latin or English introduction,\nand many books are equipped with a translation.\nThus, our method not only ensures high-quality\ndata but also removes non-Greek text parts.\nFinally, to ensure that our dataset does not con-\ntain any significant duplications, we remove all\ninstances of repeated text exceeding 300 characters.\nAfter this aggressive filtering, we have approxi-\nmately 123.3 million tokens left. To demonstrate\nits quality, we show 40 samples randomly drawn\nfrom the dataset in Table 10.\nC.2 English Pre-training Data\nBy collecting English translations of ancient texts,\nwe focus on texts that are strongly connected to an-\ntiquity. We gather these texts from various sources:\nThe Perseus Digital Library20 and the Internet Clas-\nsics Archive21 provide born-digital open-access\ntranslations of Classical Greek and Latin texts. Sim-\nilarly, the Documenta Catholica Omnia database22\ncontains a large amount of primarily catholic texts\nin many languages, of which we select the English\n20http://www.perseus.tufts.edu/hopper/.\n21http://classics.mit.edu/.\n22http://www.documentacatholicaomnia.eu/.\nτῆς Μασίστεω γυναικός, ἐούσης καὶ ταύτης ἐν-\nθαῦτα. ὡς\nπίστις ὑμῶν· φοβηθέντες δὲ ἐθαύμαζον, λέγοντες\nπρὸς ἀλλήλους,\nὑποληπτέον’ ἡ γὰρ πέψις τοῖς μὲν ἐν τῷ ἄνθει μᾶλ-\nλον\nἀνέπαυσαν γὰρ τ. ἐμὸν πνεῦμα κ. τὸ ὑμῶν\nεἰ Σατανᾶς ἀνέστη ἐφ᾿ ἑαυτόν\nπρόσωπον ναοῦ Κυρίου, μετὰ τὸ ἀποικίσαι\nΝαβουχοδονόσορ\nἐκείνοις δὲ ὄντος ἀεὶ τοῦ ἐπιχειρεῖν καὶ ἐφ ἡμῖν εἶναι\nδεῖ τὸ προαμύνασθαι.\nἑξακοσίους ὁπλίτας εἰς τὴν πόλιν ἄ ἄγει. ἐν τῷ\nστρατεύματι\nἔχοντι τοῦ Γερμανικοῦ συναγορεύειν μέλλοντος,’\nνοοῦν εἴη, ὅτι ἄλλου τὴν νόησιν λαμβάνον οὐ τὸ\nἐὰν δὲ μὴ τούτοις δύνῃ χρῆσθαι,\nμου- ἐφ᾿ ὑμᾶς’ ὑμεῖς.δὲ καθίσατε ἐν τῇ πόλει Υ ῾Ιερ-\nουσαλὴμ\nκαὶ νοητῆς τελειότητος.\nμένον οὐκ ἐπίστευσαν.\nτίον ἀράτω ᾿Ιησοῦς\nδιδόντα ὑετὸν ἐπὶ τὴν γῆν, ἀποστέλλοντα ὕδωρ\nταρασσέσθω ὑμῶν ἡ καρδία, μηδὲ δευλιάτω.\nἠκούσατε ὅτι ἐγὼ\nτὴν ξημίην ἐπέθηκαν. Ζυώδεκα δέ μοι δοκέουσι\nπόλιας ποιή-\nἐστι’ τὰ δὲ γενητὰ, ἔξωθεν ὄντα, πρόσκειται, ὡς τῇ\nὁ δὲ Κλεομένης τὸν ἱρέα ἐκέλευε τοὺς εἵλωτας ἀπὸ\nτοῦ\nἅπαξ ἀλλὰ πολλάκις.\nἐλθόντος. καὶ αὖθις ἔδοξε τούτου χάριν καὶ\nκερματίξῃς αὐτό, ἐκεῖνοι πολλαπλασιοῦσιν,\nεὐλαβούμενοι\nκαὶ προλάμψαν τὸ ἐραστὸν αὐτοῦ καὶ τὸν κρυπτό-\nμενον\nπεντακοσίας, οὺς πάντας ἡ τοῦ δεσπότου χάρις καὶ\nφιλανθρωπία διατρέφει.\nταύτης ἰδίᾳ προετρέπετο τὸν Σικόπαν κοινωνῆσαι\nοὐδὲ παναρμονίου ἡμῖν δεήσει ἐν ταῖς ὠδαῖς τε καὶ\nσημεῖα τοῦ τοῦτον συχοφαντεῖν ἐγχαλοῦντ᾿ ἀφορ-\nμήν.\nσυμπεριλαμβανομένων καὶ ψυχῆς καὶ τῶν ἐν\nπλὴν ἐξ ὠκυβόλων εἴ ποτε τόξων\nσφι ἄρτισις περὶ τὸ σῶμα ἐστί.\nμὴ πέσῃς κατέναντι ἐνεδρεύοντος\nο Εἰς τοῦτο ἐσυνέργησαν οἱ πρῶτοι τῆς γενεᾶς τῆς,\nχωρίων ἢ οἰκιῶν ὑπῆρχον, πωλοῦντες ἔφερον τὰς\nτιμὰς\nᾧ δὲ περὶ ἑκάστην μεθοδον¨) φιλοσοφοῦντι καὶ μὴ’\nτῶν τῆς. παιδὸς γάμων, Ζεὺς διαλύσας ἐπέτρεψεν\nὑμῶν. πόλεις αἱ πρὸς νότον συνεκλείσθησαν, καὶ\nοὐκ ἦν ὁ ἀνοίγων: ἀπῳκίσθη Ιουδας,\nπειρασμούς. Περὶ ταύτης ἡ Γραφὴ (ά. Κορ,\nἔπεσεν ἐπὶ πρόσωπον αὐτοῦ προσευχόμενος\nζητεῖ’ οἷδεν. γὰρ ὁ-πατὴριὑμῶν ὁ οὐράνιος\nTable 10: 40 randomly drawn lines of the Internet\nArchive pre-training dataset.\npartition for our use. Finally, we utilize Lexun-\ndria,23 Loebulus,24 and the Project Gutenberg to\nadd (often proofread) scans of books in the pub-\nlic domain. While Lexundria and Loebulus are\n23https://lexundria.com/.\n24https://ryanfb.xyz/loebolus/.\n15195\nrestricted to providing translations of Latin and\nGreek texts, the Project Gutenberg offers a more\ndiverse range of literature. Therefore, we use only\nbooks from Project Gutenberg that are tagged with\nthe keyword “Latin”. We report detailed statistics\nin Table 11.\nLanguage Dataset Number of Tokens\nAncient Greek\nOpen Greek & Latin 30.0million\nGreek Medieval Texts 3.3million\nPatrologia Graeca 28.5million\nInternet Archive 123.3million\nOverall 185.1 million\nLatin Corpus Corporum 167.5 million\nEnglish\nPerseus 10.8million\nClassics Archive 4.9million\nLexundria 2.8million\nLoebulus 14.0million\nProject Gutenberg 28.7million\nDocumenta Catholica Omnia151.7million\nOverall 212.8 million\nTable 11: Statistics of the pre-training datasets. Only\nOpen Greek & Latin is used by Singh et al. (2021) and\nYamshchikov et al. (2022) for their AGBERT models.\nToken counts determined by UNIX command wc -w.\nD Further Results\nModel Accuracy\nCLTK 96.51\nUDP IPE (official) 94.71\nUDP IPE (ours) 93.87 (0.05)\nUDP IPE + GRεBERTA 94.17 (0.05)\nGRεTA 97.40 (0.02)\nGRεTA + Chars 97.48 (0.02)\nTable 12: Lemmatization results on the Ancient Greek\nPROIEL dataset. The results are averaged over three\nruns with different random seeds, and the standard devi-\nation is indicated in parentheses, except for the CLTK\nand UDP IPE (reported results).\n20 40 60 80 100\nTraining Examples\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75Accuracy\nPhilT a\nGreT a\nSingh et al.\nYamshchikov et al.\nFigure 4: Synonym/antonym disambiguation accuracy\nscores for different few-shot training set sizes, for\nGRεTA and PHILTA against AG BERT models. The\nmodels are always given equal amounts of synonyms\nand antonyms, e.g., when using 20 training instances,\nthe models are given 10 synonyms and 10 antonyms.\nWe evaluate all models using k-fold cross-validation\nand report standard deviation as error bars.\n15196\nModel PoS Tagging Dependency Parsing\nUPoS XPoS UAS LAS\nCLTK 97.10 97 .47 76 .81 73 .39\nUDPIPE(official) 97.77 98 .05 86 .05 82 .14\nUDPIPE(ours) 97.99 (0.05) 97.68 (0.06) 85.64 (0.17) 81.70 (0.25)\nUDPIPE+ GRεBERTA 98.56 (0.02) 98.70(0.03) 89.75 (0.16) 86.59 (0.15)\nAG BERT (Singh et al., 2021) 97.98 (0.02) 98.14 (0.05) 88.50 (0.09) 84.72 (0.18)\nAG BERT (Yamshchikov et al., 2022)97.19 (0.06) 97.42 (0.08) 86.61 (0.21) 82.12 (0.15)\nGRεTA-ENC 98.16 (0.02) 98.31 (0.03) 89.93 (0.08) 86.48 (0.08)\nPHILBERTA 98.15 (0.16) 98.45 (0.05) 90.32(0.13) 86.43 (0.61)\nGRεBERTA 98.60(0.03) 98.70(0.04) 90.28 (0.03) 86.84(0.12)\nTable 13: PoS tagging and dependency parsing results on the Ancient Greek PROIEL dataset. The results are\naveraged over three runs with different random seeds, and the standard deviation is indicated in parentheses, except\nfor the CLTK and UDP IPE (reported results). Note also that the CLTK is not trained on exactly the same data as\nthe other models and therefore not directly comparable.\n15197\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nWe discuss general limitations in a section a dedicated section titled \"Limitations\". Furthermore,\nwe acknowledge that the experiments on our small probing datasets do not allow to draw ﬁrm\nconclusions in Section 5 and Section 6.\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nThe paper’s claims are summarized in the Abstract and explicitly listed at the end of the Introduction\n(Section 1).\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nWe use pre-trained Language Models for Ancient Greek, introducing them in Section 1, elaborating\non them in Section 2 as Related Work, and using them in our experiments in Section 5 and Section\n6. Furthermore, we pre-train Language Models, elaborating on them in Section 3 and using them in\nour experiments (Section 5 and 6) as well. Finally, we create a pre-training corpus for Ancient Greek,\ndescribed in Section 3 and in Section C. The downstream task datasets that we use are introduced in\nSection 4 and in Section B.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nWe cite the creators of the datasets we used in Sections 1 and 2. We cite relevant prior work and\nlanguage models that we compare to in Sections 1 and 2. We elaborate on the datasets we use in\nSection 4.1 and specify the version.\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nThe licenses for the data are discussed in Section C.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nWe specify the intended use for our pre-training dataset in Section 1 and the intended use for our\nlanguage models in Section 1 and 2.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nGiven that we create our dataset utilizing open-domain texts from antiquity, we do not consider\nanonymization to be a signiﬁcant concern.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nOur pre-training corpus for Ancient Greek is described in Section 3 and in Section C. Our language\nmodels are documented in Section 3 and in Section A.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n15198\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nFor the Universal Dependencies and EvaLatin datasets that we used, we report the statistics in\nAppendix B. We report the creation of the pre-training and probing corpora and their statistics in\nAppendix B and C.\nC □\u0013 Did you run computational experiments?\nWe elaborate on our experiments in Section 4. Pre-training and ﬁne-tuning details can be found in\nAppendix A.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nWe report pre-training and ﬁne-tuning details in Appendix A.\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNo response.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNo response.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nWe use the ofﬁcial evaluation scripts for our dataset, mentioned in Section B.\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nThe collection of our probing task data is described in Section B.\n□\u0017 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nThey were informed orally in a brief introductory session.\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nWe report details about how the annotators were paid and how they were recruited in Appendix B.\n□\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nDetails about consent are reported in Appendix B.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□\u0013 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nWe report the characteristics of the annotator population in Appendix B.\n15199"
}