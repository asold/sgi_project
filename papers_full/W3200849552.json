{
    "title": "Automatic detection of actionable radiology reports using bidirectional encoder representations from transformers",
    "url": "https://openalex.org/W3200849552",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2102227604",
            "name": "Yuta Nakamura",
            "affiliations": [
                "The University of Tokyo",
                "University of Tokyo Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2171612130",
            "name": "Shouhei Hanaoka",
            "affiliations": [
                "University of Tokyo Hospital",
                "The University of Tokyo"
            ]
        },
        {
            "id": "https://openalex.org/A1964696441",
            "name": "Yukihiro Nomura",
            "affiliations": [
                "University of Tokyo Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2234125986",
            "name": "Takahiro Nakao",
            "affiliations": [
                "University of Tokyo Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2116000654",
            "name": "Soichiro Miki",
            "affiliations": [
                "University of Tokyo Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A1411032783",
            "name": "Takeyuki Watadani",
            "affiliations": [
                "The University of Tokyo",
                "University of Tokyo Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2156414996",
            "name": "Takeharu Yoshikawa",
            "affiliations": [
                "University of Tokyo Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2102011361",
            "name": "Naoto Hayashi",
            "affiliations": [
                "University of Tokyo Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A1422477371",
            "name": "Osamu Abe",
            "affiliations": [
                "The University of Tokyo",
                "University of Tokyo Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2102227604",
            "name": "Yuta Nakamura",
            "affiliations": [
                "University of Tokyo Hospital",
                "The University of Tokyo"
            ]
        },
        {
            "id": "https://openalex.org/A2171612130",
            "name": "Shouhei Hanaoka",
            "affiliations": [
                "University of Tokyo Hospital",
                "The University of Tokyo"
            ]
        },
        {
            "id": "https://openalex.org/A1964696441",
            "name": "Yukihiro Nomura",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2234125986",
            "name": "Takahiro Nakao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2116000654",
            "name": "Soichiro Miki",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1411032783",
            "name": "Takeyuki Watadani",
            "affiliations": [
                "University of Tokyo Hospital",
                "The University of Tokyo"
            ]
        },
        {
            "id": "https://openalex.org/A2156414996",
            "name": "Takeharu Yoshikawa",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102011361",
            "name": "Naoto Hayashi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1422477371",
            "name": "Osamu Abe",
            "affiliations": [
                "The University of Tokyo",
                "University of Tokyo Hospital"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2114748604",
        "https://openalex.org/W16115367",
        "https://openalex.org/W2097360283",
        "https://openalex.org/W2907454539",
        "https://openalex.org/W2595967598",
        "https://openalex.org/W2083339896",
        "https://openalex.org/W2338526423",
        "https://openalex.org/W2949795957",
        "https://openalex.org/W2908140981",
        "https://openalex.org/W2139054399",
        "https://openalex.org/W1745514781",
        "https://openalex.org/W2099269739",
        "https://openalex.org/W2786130180",
        "https://openalex.org/W2052601836",
        "https://openalex.org/W1601835763",
        "https://openalex.org/W2971898452",
        "https://openalex.org/W2064088458",
        "https://openalex.org/W1996582212",
        "https://openalex.org/W2517923437",
        "https://openalex.org/W1484393529",
        "https://openalex.org/W1976610063",
        "https://openalex.org/W2461049481",
        "https://openalex.org/W2114568403",
        "https://openalex.org/W2927599034",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2973154008",
        "https://openalex.org/W3101353398",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W1966716734",
        "https://openalex.org/W1976526581",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1678356000",
        "https://openalex.org/W4294541781",
        "https://openalex.org/W2911495555",
        "https://openalex.org/W2076646346",
        "https://openalex.org/W4236135856",
        "https://openalex.org/W2158698691",
        "https://openalex.org/W2767106145",
        "https://openalex.org/W6737563499",
        "https://openalex.org/W2971296908",
        "https://openalex.org/W4247270727",
        "https://openalex.org/W3034328552",
        "https://openalex.org/W2944416885",
        "https://openalex.org/W3041263301",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2990659240",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2400496102",
        "https://openalex.org/W2614183994",
        "https://openalex.org/W2183415660",
        "https://openalex.org/W3032055028",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2964022491"
    ],
    "abstract": null,
    "full_text": "Nakamura et al. BMC Med Inform Decis Mak          (2021) 21:262  \nhttps://doi.org/10.1186/s12911-021-01623-6\nRESEARCH\nAutomatic detection of actionable \nradiology reports using bidirectional encoder \nrepresentations from transformers\nYuta Nakamura1,2*, Shouhei Hanaoka1,2, Yukihiro Nomura3, Takahiro Nakao3, Soichiro Miki3, \nTakeyuki Watadani1,2, Takeharu Yoshikawa3, Naoto Hayashi3 and Osamu Abe1,2 \nAbstract \nBackground: It is essential for radiologists to communicate actionable findings to the referring clinicians reliably. \nNatural language processing (NLP) has been shown to help identify free-text radiology reports including actionable \nfindings. However, the application of recent deep learning techniques to radiology reports, which can improve the \ndetection performance, has not been thoroughly examined. Moreover, free-text that clinicians input in the ordering \nform (order information) has seldom been used to identify actionable reports. This study aims to evaluate the benefits \nof two new approaches: (1) bidirectional encoder representations from transformers (BERT), a recent deep learning \narchitecture in NLP , and (2) using order information in addition to radiology reports.\nMethods: We performed a binary classification to distinguish actionable reports (i.e., radiology reports tagged as \nactionable in actual radiological practice) from non-actionable ones (those without an actionable tag). 90,923 Japa-\nnese radiology reports in our hospital were used, of which 788 (0.87%) were actionable. We evaluated four methods, \nstatistical machine learning with logistic regression (LR) and with gradient boosting decision tree (GBDT), and deep \nlearning with a bidirectional long short-term memory (LSTM) model and a publicly available Japanese BERT model. \nEach method was used with two different inputs, radiology reports alone and pairs of order information and radiology \nreports. Thus, eight experiments were conducted to examine the performance.\nResults: Without order information, BERT achieved the highest area under the precision-recall curve (AUPRC) of \n0.5138, which showed a statistically significant improvement over LR, GBDT, and LSTM, and the highest area under the \nreceiver operating characteristic curve (AUROC) of 0.9516. Simply coupling the order information with the radiology \nreports slightly increased the AUPRC of BERT but did not lead to a statistically significant improvement. This may be \ndue to the complexity of clinical decisions made by radiologists.\nConclusions: BERT was assumed to be useful to detect actionable reports. More sophisticated methods are required \nto use order information effectively.\nKeywords: Radiology reports, Actionable finding, Natural language processing (NLP), Bidirectional encoder \nrepresentations from transformers (BERT), Deep learning\n© The Author(s) 2021. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nBackground\nA radiology report may include an actionable finding that \nis critical if left overlooked by the referring clinician [1]. \nHowever, clinicians can fail to see mentions of action -\nable findings in radiology reports for various reasons, \nOpen Access\n*Correspondence:  yutanakamura-tky@umin.ac.jp\n1 Division of Radiology and Biomedical Engineering, Graduate \nSchool of Medicine, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, \nTokyo 113-8655, Japan\nFull list of author information is available at the end of the article\nPage 2 of 19Nakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \nand such failure in communication can delay further \nprocedures and impact the prognosis of the patient [2]. \nTherefore, fast and reliable communication on actionable \nfindings is essential in clinical practice.\nInformation technologies are helpful in identifying and \ntracking actionable findings in radiology reports [3, 4]. \nHandling such information in radiology reports seems \na difficult task because radiology reports usually remain \nunstructured free texts [5]. However, thanks to recently \ndeveloped natural language processing (NLP) technolo -\ngies, the detection of radiology reports with actionable \nfindings has been achieved, as well as various other tasks \nusing radiology reports [6]. The aim of this study is to \nautomatically detect reports with actionable findings by \nNLP-technology-based methods.\nMany researchers in previous studies have used NLP \ntechnologies to automatically detect specific findings \nor diseases in radiology reports. Some of them stated \nthat their goal is to assist in tracking and surveillance of \nactionable findings, the details of which are summarized \nin Table 1 [7–26]. Some studies in Table  1 have the fol -\nlowing features: (1) Multiple or all types of pathological \nentities are covered [7–15]. (2) The ground truth is based \non clinical decisions, not just on the existence of specific \nexpressions in radiology reports [16–18]. These two fea -\ntures can both lead to comprehensive detection of radiol-\nogy reports with actionable findings. However, there have \nbeen no studies that use both features to the best of our \nknowledge.\nIn our hospital, for better communication and tracking \nof any actionable findings, an actionable tagging function \nwas implemented in our radiological reporting system \nand this function has been in operation since September \n9, 2019. Thus, adopting actionable tags for labeling can \nprovide a dataset based on clinical decisions for all types \nof pathological entities.\nIn addition to the free texts in radiology reports, the \nfree texts that are input in the ordering form by the refer-\nring clinician (hereafter, order information) may also \nbe useful for detecting radiology reports with action -\nable findings. That is, if serious and incidental findings \nare present, some gaps can be found between the order \ninformation and the radiology report.\nSeveral research groups have investigated the auto -\nmatic detection of actionable findings based on statisti -\ncal machine learning [9–11, 16, 18, 22, 25, 26]. However, \nthese methods are mainly based on the frequency of \nwords in each document, and other rich features such \nas word order and context are hardly taken into account. \nRecently, bidirectional encoder representations from \ntransformers (BERT), one of the Transformer networks \n[27, 28], has attracted much attention because it achieves \nstate-of-the-art performance in various NLP tasks. For \nbetter detection of radiology reports with actionable \nfindings, BERT is worth using for two reasons: (1) BERT \ncan use linguistic knowledge not only from an in-house \ndataset but also from a corpus (a set of documents) for \npre-training [29]. (2) BERT is able to capture the relation-\nship between two documents [28], which may enable it to \nperform well for a pair comprising order information and \na radiology report. BERT has been used in several very \nrecent studies of classification tasks in radiology reports \n[30, 31]. To the best of our knowledge, however, there \nhave been no attempt to use BERT for the automated \ndetection of radiology reports with actionable findings.\nIn this study, we investigate the automated detection of \nradiology reports with actionable findings using BERT.\nThe contributions of this study are as follows.\n• Examination of the performance of BERT for the \nautomated detection of actionable reports\n• Investigation of the difference in detection perfor -\nmance upon adding order information to the input \ndata\nMethods\nTask description\nThis study was approved by the institutional review \nboard in our hospital, and was conducted in accordance \nwith the Declaration of Helsinki.\nWe define two collective terms: (1) “report body, 1” \nreferring to the findings and impression in radiology \nreports, and (2) “order information, ” referring to the free \ntexts that are written in the ordering form by the refer -\nring clinician (e.g., the suspected diseases or indications), \nas explained in Introduction. Our task is thus defined as \nthe detection of radiology reports with actionable tags \nusing the report body alone, or both the order informa -\ntion and the report body.\nClinical data\nWe obtained 93,215 confirmed radiology reports for \ncomputed tomography (CT) examinations performed \nat our hospital between September 9, 2019, and April \n30, 2021, all of which were written in Japanese. Next, \nwe removed the following radiology reports that were \nnot applicable for this study: (1) eight radiology reports \nwhose findings and impressions were both registered as \nempty, (2) 254 reports for CT-guided biopsies, and (3) \n2030 reports for CT scans for radiation therapy planning. \n1 For simplicity, we regarded impression as part of the report body, although \nthis is different from the definition of the body of the report by the American \nCollege of Radiology [32].\nPage 3 of 19\nNakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \n \nTable 1 Summary of previous studies of automatic detection of radiology reports with actionable findings, along with this study\nBERT = Bidirectional Encoder Representations from Transformers, CNN = Convolutional Neural Network, GRU = Gated Recurrent Units, LSTM = Long Short-Term \nMemory, and SML = Statistical Machine Learning\nTarget language Multiple diseases Use of labels in \nclinical practice\nCriteria for positive \nclass\nTarget sections in \nradiology reports\nMethods\nMeng et al. [7] English Yes No Expressions sug-\ngesting the need to \npromptly communi-\ncate to the referring \nclinician\nImpression Existing tool\nHelibrun et al. [8] English Yes No Expressions suggest-\ning specific critical \nfindings\nImpression Existing tool\nCarrodeguas et al. [9] English Yes No Follow-up recom-\nmendations\nImpression SML, LSTM\nYetisgen-Yildiz et al. \n[10]\nEnglish Yes No Follow-up recom-\nmendations\nOrder information, \nfindings, impression\nSML\nYetisgen-Yildiz et al. \n[11]\nEnglish Yes No Follow-up recom-\nmendations\nOrder information, \nfindings, impression\nSML\nDutta et al. [12] English Yes No Follow-up recom-\nmendations\nFindings, impression, \nrecommendation\nExisting tool\nLau et al. [13] English Yes No Follow-up recom-\nmendations\n(Not specified) GRU \nDang et al. [14] English Yes No Follow-up recom-\nmendations\n(Not specified) Decision tree\nImai et al. [15] Japanese Yes No Expressions suggest-\ning malignancy\nFindings Syntactic analysis\nLou et al. [16] English No Yes Reports pointing \nat indeterminate or \nsuspicious upper \nabdominal mass\n(Not specified) SML\nDanforth et al. [17] English No Yes ICD-9 codes suggest-\ning lung nodules\n(Not specified) Rule base\nGarla et al. [18] English No Yes Expressions suggest-\ning potentially malig-\nnant liver lesions\n(Not specified) SML\nFarjah et al. [19] English No No Expressions suggest-\ning lung nodules\n(Not specified) Existing tool\nGershanik et al. [20] English No No Expressions suggest-\ning lung nodules\nFindings, impression Existing tool\nOliveira et al. [21] English No No Expressions suggest-\ning incidental lung \nnodules\nOrder information, \nfindings\nRule base\nPham et al. [22] French No No Expressions suggest-\ning incidentalomas\nOrder information, \nfindings, impression\nSML\nMabotuwana et al. \n[23]\nEnglish No No Follow-up recom-\nmendations\n(Not specified) Rule base\nMorioka et al. [24] English No No Expressions suggest-\ning abdominal aorta \naneurysm\n(Not specified) Existing tool\nXu et al. [25] English (Not specified) No Follow-up recom-\nmendations\nOrder information, \nfindings, impression\nSML\nFu et al. [26] English No No Expressions sug-\ngesting silent brain \ninfarction or white \nmatter disease\n(Not specified) Rule base, SML, CNN\nThis study Japanese Yes Yes Reports with an \nactionable tag\nOrder information, \nfindings, impression\nSML, LSTM, BERT\nPage 4 of 19Nakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \nThe remaining 90,923 radiology reports corresponded to \n18,388 brain, head, and neck; 64,522 body; 522 cardiac; \nand 5673 musculoskeletal reports; and 3209 reports of \nother CT examinations whose body parts could not be \ndetermined from the information stored in the Radiology \nInformation System (RIS) server. The total was greater \nthan the number of reports because some reports men -\ntioned more than one part.\nClass labeling and data split\nEach of the 90,923 radiology reports was defined as \nactionable (positive class) if it had been provided with an \nactionable tag by the diagnosing radiologist, and it was \notherwise defined as non-actionable (negative class). In \nother words, the gold standard had already been given to \nall of the reports in the clinical practice, which enabled \na fully supervised document classification without addi -\ntional annotations.\nThe radiologists in our hospital are requested to regard \nimage findings as actionable when the findings were not \nsupposed to be expected by the referring clinician and \nwere potentially critical if left overlooked. Specific cri -\nteria for actionable tagging were not determined clearly \nin advance but left to clinical decisions of individual \nradiologists.\nThe numbers of actionable and non-actionable reports \nwere 788 (0.87%) and 90,135 (99.13%), respectively. Then, \nthese radiology reports were split randomly into a train -\ning set and a test set in the ratio of 7:3, maintaining the \nsame proportions of actionable and non-actionable \nreports in each set, i.e., in the training set, there were \n63,646 reports, where 552 were actionable and 63,094 \nwere non-actionable, and in the test set, there were \n27,277 reports, where 236 were actionable and 27,041 \nwere non-actionable.\nPreprocessing of radiology reports\nTo apply machine learning methods in the following \nsections, the same preprocessing was carried out on all \nradiology reports (Fig.  1). First, the contents in the order \ninformation and report body were respectively concate -\nnated into passages. Then, the passages were individually \ntokenized with the SentencePiece model, whose vocabu -\nlary size is 32,000 [33, 34].\nBERT\nBERT is one of the Transformer networks [27, 28]. In \ngeneral, “Transformer” refers to neural networks using \nmultiple identical encoder or decoder layers with an \nattention mechanism [35]. Transformer networks have \noutperformed previous convolutional and recurrent neu -\nral networks in NLP tasks [27]. BERT has been proposed \nas a versatile Transformer network. BERT takes one \nor two documents as input, passes them into the inner \nstack of multiple Transformer encoder layers, and char -\nacteristically outputs both document-level and token-\nlevel representations. BERT can thus be applied to both \ndocument-level and token-level classification tasks [28]. \nVarious BERT models pre-trained with large corpora are \npublicly available, which has established a new ecosystem \nfor pre-training and fine-tuning of NLP models.\nWe used the Japanese BERT model developed by \nKikuta [34]. This model is equivalent to “BERT-base” \nwith 12 Transformer encoder layers and 768-dimensional \nhidden states. The model has been pre-trained using a \nJapanese Wikipedia corpus tokenized with the Sentence -\nPiece tokenizer [33].\nWe constructed a binary classifier (hereafter, a BERT \nclassifier) by adding a single-layer perceptron with soft -\nmax activation after the pre-trained BERT model. The \nFig. 1 Preprocessing for each radiology report. Note that subwords recognized by SentencePiece are not precisely substrings of words in the \ngrammatical sense, because SentencePiece automatically constructs its vocabulary without a dictionary. For this reason, SentencePiece sometimes \ntreats long phrases as one subword or conversely one character as one subword\nPage 5 of 19\nNakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \n \nperceptron converts a 768-dimensional document-level \nrepresentation vector output by the pre-trained BERT \nmodel into a two-dimensional vector.\nThe procedure is shown in Fig.  2. For the detection \nexperiment without order information, the sequences \ngenerated from the report body were fed to the BERT \nclassifier. For the detection experiment with order \ninformation, each sequence pair generated from the \norder information and report body was fed to the BERT \nclassifier.\nFine-tuning was performed on all embedding and \nTransformer encoder layers of the BERT model, and \nnone of these layers were frozen. The maximum sequence \nlength was set to 512 and the batch size 2 was set to 256. \nWe used Adam optimizer [36] and binary cross-entropy \nloss function.\nAs in Table  2, the learning rate and the number of \ntraining epochs were set as follows. The learning rate \nwas set to 5.0 ×  10−5 for the experiment without order \ninformation and to 4.0 ×  10−5 for the experiment with \norder information. The number of training epochs was \nset to 3 for both experiments. The learning rate and the \nnumber of training epochs were determined by the grid \nsearch and five-fold cross-validation using the training \nset. We tried all of the 25 direct groups of five learning \nrates, 1.0 ×  10−5, 2.0 ×  10−5, 3.0 ×  10−5, 4.0 ×  10−5, and \n5.0 ×  10−5, and the five training epochs, 1 to 5. We calcu -\nlated the averages of the area under the precision-recall \ncurve (AUPRC) [37, 38] for the five folds, and chose the \nlearning rate and the number of training epochs that gave \nthe highest average AUPRC.\nThe learning environment was as follows: AMD EPYC \n7742 64-Core Processor, 2.0 TB memory, Ubuntu 20.04.2 \nLTS, NVIDIA A100-SXM4 graphics processing unit \n(GPU) with 40  GB memory × 6, Python 3.8.10, PyTorch \n1.8.1, Torchtext 0.6.0, AllenNLP 2.5.0, PyTorch-Light -\nning 0.7.6, scikit-learn 0.22.2.post1, Transformers 4.6.1, \nTokenizers 0.10.3, SentencePiece 0.1.95, MLflow 1.17.0, \nand Hydra 0.11.3.\nFig. 2 Detection of actionable reports with BERT (a) without and (b) with order information. Each sequence was fed into the BERT classifier after \nadding special tokens (e.g., [CLS] and [SEP]) and padding with [PAD] tokens that are required by the standard specification of BERT. Generally, BERT \nmodels output a 512 × 768 hidden state matrix (indicated by **), part of which is a 768-dimensional feature vector for classification tasks (indicated \nby *). We used only the feature vector for classification tasks and discarded the rest of the hidden state matrix, as in the standard procedure\n2 The actual batch size was set to 16 owing to the limited computational \nresources. However, an effective batch size of 256 was realized by accumulat -\ning gradients of every 16 batches with the PyTorch-Lightning implementation.\nPage 6 of 19Nakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \nBaselines: LSTM\nAs one of the baselines against BERT, we performed \nautomated detections of actionable reports using a two-\nlayer bidirectional long short-term memory (LSTM) \nmodel followed by a self-attention layer [27, 39]. As in \nBERT, the inputs to the LSTM model were report bodies \nin the experiments without order information and were \nconcatenations of order information and report bodies \nin the experiments with order information. The lengths \nof the input documents in a batch were aligned to the \nlongest one by adding special padding tokens at the end \nof the other documents in the same batch. Next, each \ndocument was tokenized and converted into sequences \nof vocabulary IDs using the SentencePiece tokenizer, and \nwas then passed into a 768-dimensional embedding layer. \nIn short, the preprocessing converted radiology reports \nin a batch into a batch size × length × 768 tensor.\nThe final layer of the LSTM model outputs two batch \nsize × length × 768 tensors corresponding to the forward \nand backward hidden states. We obtained document-\nlevel representations by concatenating the two hidden \nstates. The representations were further passed into a \nsingle-head self-attention layer with the same architec -\nture as proposed by Vaswani et al. [27]. The self-attention \nlayer converts the document-level representations to \na batch size × 1536 matrix by taking the weighted sum \nof the document-level representations along the time \ndimension effectively by considering the importance of \neach token. Then, the matrix was converted into two-\ndimensional vectors using a single-layer perceptron with \nsoftmax activation. The resulting two-dimensional vec -\ntors were used as prediction scores. Hereafter, we collec -\ntively refer to the LSTM model, the self-attention layer, \nand the perceptron as the “LSTM classifier. ”\nWe trained the LSTM classifier from scratch. The same \noptimizer and loss function as those in BERT were used. \nThe batch size was set to 256. As in BERT, the learning \nrate and the number of training epochs were determined \nby grid search and five-fold cross-validation. Table  2 \nshows the hyperparameter candidates on which the grid \nsearch was performed and the hyperparameters that \nwere finally chosen for each experiment.\nBaselines: statistical machine learning\nLogistic regression (LR) [40] and the gradient boost -\ning decision tree (GBDT) [41] were also examined for \ncomparison.\nFigure  3 shows the procedures. The tokenized report \nbody and order information were individually converted \ninto term frequency-inverse document frequency (TF-\nIDF)-transformed count vectors of uni-, bi-, and trigrams \n(one, two, and three consecutive subwords). The two vec-\ntors were concatenated for the detection experiment with \norder information, and only the vector from the report \nbody was used for the detection experiment without \norder information.\nHere, we describe the details of hyperparameters of \nthe LR and GBDT models. For LR, we used Elastic-Net \nregularization [30, 42], which regulates model weights \nwith the mixture of L1- and L2-norm regularizations. \nElastic-Net takes two parameters, C and the L1 ratio. C is \nthe reciprocal strength to regularize the model weights, \nand the L1 ratio is the degree of dominance of L1-norm \nregularization. The C and the L1 ratio were determined \nwith the grid search and five-fold cross-validation, whose \ncandidates and choices are shown in Table  2. For GBDT, \nthe tree depth was set to 6. The number of iterations was \ndetermined by grid search and five-fold cross-validation \nin the same way as LR.\nWe used the scikit-learn 0.22.2post1 implementation \nfor LR and the CatBoost 0.25.1 [43] implementation for \nGBDT.\nTable 2 Details of hyperparameter tuning for each method. xe+y means x ×  10y and xe−y means x ×  10−y\nMethod Hyper-parameter Candidates Used hyperparameters\nOrder information (−) Order information (+)\nOversampling \n(−)\nOversampling \n(+)\nOversampling \n(−)\nOversampling \n(+)\nLR C 1e− 4, 1e −3, 1e −2, 1e −1, 1.0, \n1e+1, 1e+2, 1e+3, 1e+4\n1e+2 1e −2 1.0 1.0\nL1 ratio 0.0, 0.5, 1.0 1.0 0.5 1.0 1.0\nGBDT Iterations 500, 1,000, 1,500 500 1,000 1,500 1,000\nLSTM Learning rate 5e −6, 1e −5, 2e −5, 3e −5 5e −6 1e −5 5e −6 5e −6\nEpochs 5, 10, 15, 20, 25, 30 20 5 20 25\nBERT Learning rate 1e −5, 2e −5, 3e −5, 4e −5, 5e −5 5e −5 5e −5 4e −5 1e −5\nEpochs 1, 2, 3, 4, 5 3 1 3 1\nPage 7 of 19\nNakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \n \nPerformance evaluation\nSince this experiment is under a highly imbalanced set -\nting, the performance of each method was mainly eval -\nuated with the AUPRC [37, 38], along with the average \nprecision score.\nWe statistically compared the AUPRC and average pre-\ncision among LR, GBDT, LSTM, and BERT using Welch’s \nt-test with Bonferroni correction [44]. The bootstrapping \napproach was applied, where 2000 replicates were made, \nand 2000 AUPRCs and average precisions were calcu -\nlated for LR, GBDT, LSTM, and BERT. Using the same \napproach, we also statistically compared the AUPRC and \naverage precision in the experiments without and with \norder information for each method.\nThe area under the receiver operating characteristics \n(ROC) curve (AUROC) was also calculated [45, 46]. The \nrecall, precision, specificity, and F1 score were also calcu-\nlated at the optimal cut-off point of the ROC curve. The \noptimal cut-off point was chosen using the minimum dis-\ntance between the ROC curve and the upper left corner \nof the plot.\nScikit-learn 0.22.2.post1 implementation was used for \ncalculation of the evaluation metrics, bootstrapping, and \nstatistical analysis.\nFor a more detailed analysis, we divided the truly \nactionable reports in the test set into explicit action -\nable reports (those with expressions recommend -\ning follow-up imaging, further clinical investigations, \nor treatments) and implicit ones (those without such \nexpressions) by manual review by one radiologist \n(Y.  Nakamura, four years of experience in diagnos -\ntic radiology). We also calculated recalls for the mass \nand non-mass subsets of the truly actionable reports in \nthe test set since some previous studies have focused \non actionable reports that point out incidental masses \nor nodules [15– 22]. Each of the reports was included \nin the mass subset when its actionable findings were \ndetermined to involve masses or nodules by manual \nreview, otherwise reports were included in the non-\nmass subset.\nOversampling\nWe mainly used the training set mentioned in the pre -\nvious section, but its significant class imbalance may \naffect the performance of the automated detection of \nactionable reports. Oversampling positive data can be \none of the methods to minimize the negative impact of \nthe class imbalance [47].\nTo examine the effectiveness of oversampling, we \nadditionally performed experiments using the over -\nsampled training set. The oversampled training set was \ncreated by resampling each actionable radiology report \nten times and each non-actionable radiology report \nonce from the original training set. Hyperparameters \nfor each method (LR, GBDT, LSTM, and BERT) and for \neach input policy (using and not using order informa -\ntion) were determined using the same strategy as that \nin the experiments without oversampling. The chosen \nhyperparameters are shown in Table 2 .\nNote that we did not oversample the validation data -\nsets during the five-fold cross-validation because we \nintended to search optimal hyperparameters for the \nsame positive class ratio as the test set.\nTo examine the effect of oversampling, we statistically \ncompared the AUPRC and average precision obtained \nwithout and with oversampling in the same way as \naforementioned.\nFig. 3 Detection of actionable reports with statistical machine learning (a) without and (b) with order information\nPage 8 of 19Nakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \nResults\nFigures  4 and 5 show the precision-recall curves and \nthe ROC curves of each method. Table  3 presents the \nperformance of each method calculated from precision-\nrecall curves and optimal cut-off points of ROC curve. \nTable 4 shows the results of statistical analysis to com -\npare the performance characteristics of LR, GBDT, \nLSTM, and BERT. In both of the experiments without \nand with order information, BERT achieved the highest \nAUPRC and average precision among the four methods, \nand it showed a statistically significant improvement over \nthe other methods. In particular, the highest AUPRC of \n0.5153 was achieved using BERT with order information. \nThe F1 score tended to be higher for the methods with \nhigher AUPRCs, average precisions, and AUROCs. The \nhighest precision was 0.0634, considerably lower than \nthat for recall.\nThe advantage of using order information was unclear. \nTables 3 and 5 show that the use of order information \nmarkedly decreased AUPRC except for BERT. Only \nBERT slightly improved AUPRC with the use of order \ninformation, but the improvement was not statistically \nsignificant.\nOversampling showed a limited positive effect on the \nperformance. As in Tables  6 and 7, oversampling posi -\ntive samples in the training dataset ten times resulted \nFig. 4 Precision-recall curves for detection of actionable reports achieved by each method\nPage 9 of 19\nNakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \n \nFig. 5 ROC curves for detection of actionable reports achieved by each method, with optimal cut-off points shown as open circles\nTable 3 Performance in detection of actionable reports with different use of order information for each method\nMaximum values are shown in bold\nMethod Use of order \ninformation\nAUPRC Average precision AUROC F1 score Recall Precision Specificity\nLR (−) 0.4574 0.4224 0.9351 0.1052 0.8729 0.0559 0.8715\n(+) 0.4218 0.4580 0.8992 0.0763 0.8136 0.0400 0.8296\nGBDT (−) 0.4813 0.4816 0.8986 0.0975 0.7881 0.0520 0.8746\n(+) 0.4767 0.4771 0.9133 0.0699 0.8305 0.0365 0.8087\nLSTM (−) 0.4617 0.4620 0.9331 0.0916 0.8644 0.0484 0.8516\n(+) 0.4265 0.4272 0.9277 0.0797 0.8856 0.0417 0.8226\nBERT (−) 0.5138 0.5142 0.9516 0.1030 0.9068 0.0546 0.9271\n(+) 0.5153 0.5157 0.9497 0.1183 0.8729 0.0634 0.9140\nPage 10 of 19Nakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \nin statistically significant improvements of AUPRC and \naverage precision only for GBDT.\nWe analyzed further how predictions were made by \neach method. For LR and GBDT, each of the available \nn-grams (i.e., uni-, bi-, and trigrams) were scored using \ncoefficients assigned by the LR models or feature impor -\ntance assigned by the GBDT models, which reflected the \nn-grams that the LR and GBDT models placed impor -\ntance during prediction. N-grams consisting only of \neither Japanese punctuations or Japanese postpositional \nparticles were excluded because they were assumed to \nbe of little value. The results are shown in Figs.  6 and 7, \nwhich suggest that the LR and GBDT models tended to \npredict radiology reports as actionable if they contained \nsuch expressions as “is actionable, ” “investigation, ” “can-\ncer, ” or “possibility of cancer. ” This suggests that the \nmodels picked up explicit remarks by radiologists rec -\nommending clinical actions or pointing out cancers. In \ncontrast, patterns in keywords used by the LR model \nfor non-actionable radiology reports were less clear, \nalthough some negations such as “is absent” or “not” are \nobserved in Fig.  6b. The word “apparent” , which is fre -\nquently accompanied by negative findings in Japanese \nradiology reporting, is also present in the top negative \nn-grams in Fig.  6b. These imply that the LR model might \ndeduce that radiology reports are non-actionable when \nnegative findings predominate. Order information may \nnot be used much by the LR and GBDT models because \nfew of the n-grams in order information are present in \nFigs. 6 and 7.\nFigure  8 is a visualization of the self-attention of the \nLSTM and BERT classifier, highlighting tokens on which \nlarge importance was placed by each model during pre -\ndiction. For LSTM, tokens attracting more attention than \nothers are shown in red. The attention scores were calcu -\nlated by averaging the row vectors of the attention matrix \ngenerated by the self-attention layer. The attention matrix \nhas the length × length size, whose (i, j) element of the \nattention matrix stands for the degree of the i-th token \nattending the j-th token. Thus, averaging the row vec -\ntors can clarify which token is attracting more attention \noverall than others. For BERT, tokens directing intensive \nattention toward the [CLS] special token are shown in \nred. The attention scores were calculated by averaging all \nof the attention weight matrices in each of the 12 atten -\ntion heads in the last Transformer encoder layer of the \nBERT classifier. In Fig.  8, attention scores tended to be \nhigher in expressions such as recommendations or sus -\npicions than in anatomical, radiological, or pathological \nterms.\nTable  8 shows the recalls of each method for the \nexplicit and implicit actionable reports in the test set. 111 \nTable 4 Results of statistical analysis to examine the performance of each detection method\nUse of order \ninformation\nMetrics Method p values\nversus GBDT versus  LSTM versus  BERT\n(−) AUPRC LR p < 0.0001 p < 0.0001 p < 0.0001\nGBDT – p < 0.0001 p < 0.0001\nLSTM – – p < 0.0001\nAverage precision LR p < 0.0001 p = 0.0002 p < 0.0001\nGBDT – p < 0.0001 p < 0.0001\nLSTM – – p < 0.0001\n( +) AUPRC LR p < 0.0001 p < 0.0001 p < 0.0001\nGBDT – p < 0.0001 p < 0.0001\nLSTM – – p < 0.0001\nAverage precision LR p < 0.0001 p < 0.0001 p < 0.0001\nGBDT – p < 0.0001 p < 0.0001\nLSTM – – p < 0.0001\nTable 5 Results of statistical analysis to examine the impact of \nuse of order information\nMethod Metrics p values\nUse of order \ninformation ( −) \nversus ( +)\nLR AUPRC p < 0.0001\nAverage precision p < 0.0001\nGBDT AUPRC p < 0.0001\nAverage precision p < 0.0001\nLSTM AUPRC p < 0.0001\nAverage precision p < 0.0001\nBERT AUPRC p = 0.0972\nAverage precision p = 0.1143\nPage 11 of 19\nNakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \n \ntruly actionable reports (47%) were implicit in the test \nset. Although Figs.  6, 7 and 8 imply that all four methods \ntended to detect actionable findings mainly on the basis \nof the existence of specific expressions, Table  8 shows \nthat our methods were able to identify actionable reports \neven if they did not explicitly recommend further medi -\ncal procedures.\nFive of the implicit actionable reports were detected \nonly by BERT and not detected by other methods without \norder information. Figure  9 shows the BERT attention \nvisualizations towards three of the reports, all of which \npoint out pneumothorax. Although none of the three \nreports include explicit recommendations or emphatic \nexpressions to highlight actionable findings, BERT suc -\ncessfully predicted them as actionable. Moreover, Fig -\nure 9 shows that BERT has assigned high attention scores \nto a part of the involved disease name “pneumothorax. ”\nIn short, although Figs.  6, 7 and 8 suggest that all four \nmethods mainly relied on whether radiology reports con-\ntain specific expressions of recommendation, suspicion, \nor negation, Fig.  9 implies further the capability of BERT \nto consider characteristics of diseases.\nTable 9 shows the recall for truly actionable reports \nin the test set. The results in Table  9 suggest that our \nmethods detected actionable reports regardless of the \npathological entity of their actionable findings.\nAs in Table  10, actionable reports accounted for \n0.41% of brain, head, and neck; 1.1% of body; and 0.51% \nof musculoskeletal CT radiology reports in the test set. \nTable 10 also shows that the recall scores for the action -\nable musculoskeletal CT reports were greater than \nthose for brain, head, and neck CT reports.\nTable 6 Performance characteristics of methods in detection of actionable reports without and with oversampling of positive \nsamples in the training data\nMaximum values for each method are shown in bold\nMethod Use of order \ninformation\nOversampling AUPRC Average precision AUROC F1 score\nLR ( −) ( −) 0.4574 0.4224 0.9351 0.1052\n( +) 0.3166 0.3167 0.8036 0.0474\n( +) ( −) 0.4218 0.4580 0.8992 0.0763\n( +) 0.4214 0.4221 0.9277 0.1089\nGBDT ( −) ( −) 0.4813 0.4816 0.8986 0.0975\n( +) 0.4854 0.4858 0.9335 0.0841\n( +) ( −) 0.4767 0.4771 0.9133 0.0699\n( +) 0.4874 0.4878 0.9307 0.0920\nLSTM ( −) ( −) 0.4617 0.4620 0.9331 0.0916\n( +) 0.4188 0.4194 0.9262 0.0818\n( +) ( −) 0.4265 0.4272 0.9277 0.0797\n( +) 0.4086 0.4066 0.9255 0.0795\nBERT ( −) ( −) 0.5138 0.5142 0.9516 0.1030\n( +) 0.4256 0.4273 0.9464 0.1190\n( +) ( −) 0.5153 0.5157 0.9497 0.1183\n( +) 0.4549 0.4559 0.9441 0.0953\nTable 7 Results of statistical analysis to examine the impact of \noversampling\nMethod Use of order \ninformation\nMetrics p values\nOversampling \n( −) versus ( +)\nLR ( −) AUPRC p < 0.0001\nAverage precision p < 0.0001\n( +) AUPRC p = 0.7971\nAverage precision p = 0.9280\nGBDT ( −) AUPRC p = 0.0001\nAverage precision p < 0.0001\n( +) AUPRC p < 0.0001\nAverage precision p < 0.0001\nLSTM ( −) AUPRC p < 0.0001\nAverage precision p < 0.0001\n( +) AUPRC p < 0.0001\nAverage precision p < 0.0001\nBERT ( −) AUPRC p < 0.0001\nAverage Precision p < 0.0001\n( +) AUPRC p < 0.0001\nAverage Precision p < 0.0001\nPage 12 of 19Nakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \nDiscussion\nThe results show that our method based on BERT out -\nperformed other deep learning methods and statisti -\ncal machine learning methods in distinguishing various \nactionable radiology reports from non-actionable ones. \nThe statistical machine learning methods used only limited \nfeatures, because the radiology reports were converted into \nthe vectors of the frequency of words as the standard fea-\nture extraction method [40]. In contrast, BERT and LSTM \npresumably captured various features of each radiology \nreport including the word order, lexical and syntactic infor-\nmation, and context [28, 29]. Moreover, the superiority of \nBERT over LSTM was probably brought about by leverag-\ning knowledge from a large amount of pre-training data.\nAs in Tables  8 and 9, our BERT-based approach was \neffective in identifying actionable reports regardless of \nthe explicitness or the targeted abnormality. The proba -\nble reasons were that (1) implicit actionable reports often \nemphasized the abnormality that was considered action -\nable (e.g., “highly suspected to be primary lung cancer” \nfor lung nodules) and that (2) the BERT classifiers were \nalert to such emphatic expressions in addition to explicit \nrecommendations for follow-up, investigations, or treat -\nment. Furthermore, Figure 9 shows that BERT could still \nidentify implicit actionable reports without emphatic \nexpressions for the actionable findings, and it could \nassign high attention scores to the names of the action -\nable findings. This implies that BERT is capable of learn -\ning to distinguish disease names that are likely to be often \nreported as actionable findings.\nAs in Table 10, the detection performance was affected \nby the body part of the radiology reports. This is probably \ncaused by the difference in the proportion of explicit and \nmass actionable reports for each body part. The action -\nable musculoskeletal CT reports were more often explicit \nand targeting mass abnormality than the brain, head, and \nneck CT reports. Tables  8 and 9 suggest that explicit and \nmass actionable reports were comparatively easier to \nidentify than implicit and non-mass ones. This was prob -\nably why all four methods achieved higher recalls scores \nfor musculoskeletal actionable reports than brain, head, \nand neck ones.\nOrder information did not necessarily improve the \nperformance. This may be because the truly action -\nable reports had a too diverse relationship between the \norder information and the report body. We found that \nthe actionable tags were not only used to caution about \nfindings that were irrelevant to the main purpose of \nordering (e.g., lung nodules found in a CT examination \nto diagnose fracture). Rather, the actionable tags were \nalso given to the radiology reports to highlight unusual \nclinical courses (e.g., liver metastases from colon cancer \nfirst appeared five years after the surgery of the primary \nlesion) or to prompt immediate treatments (e.g., hemor -\nrhage in the nasal septum associated with nasal fracture). \nThese complex situations may have not been recognized \nwell from our small dataset, even with the ability of BERT \nto capture the relationship between the report body and \norder information.\nThe low precision (0.0365–0.0634) was another prob -\nlem in this study. It was probably mainly due to the low \npositive case ratio (0.87%). Generally, an imbalance of \noccurrences between positive and negative samples \nstrongly hampers a binary classification task [48]. This \nnegative impact of low positive case ratio was not alle -\nviated by simple oversampling, probably because it did \nnot provide bring new information to learn characteris -\ntics of actionable reports to the models. To overcome this \nlimitation, obtaining a larger amount of positive data by \ncollecting more radiology reports or data augmentation \n[49] may be an effective solution. Other approaches such \nas cost-sensitive learning [50] or the use of dice loss func-\ntion [51] can also be worth trying in future studies.\nAn important advantage of the proposed approach \nin this study is that the radiology reports were labeled \nwith tags provided in actual radiological practice. Gen -\nerally, radiologists determine whether specific findings \nare actionable or not on the basis of not only radiologi -\ncal imaging but also a comparison with a prior series \nof images, order information, and electronic health \nrecords. The actionable tag can consequently reflect \nsuch clinical decisions. Therefore, there is probably \nroom for improvement in the performance of auto -\nmated detection of actionable reports by using the \nimaging data themselves and the information in elec -\ntronic health records. This benefit may not be obtained \nby independent class labeling, referring only to the sen -\ntences in the radiology reports.\nUsing the actionable tag as the label has another \nmerit: to identify implicit actionable reports. The \nresults of this study suggest that the radiologists may \nhave sometimes thought that actionable findings were \nFig. 6 Top n-grams with positive and negative coefficients with the largest absolute values of the LR models (a) without and (b) with order \ninformation. Only the top 25 n-grams are shown when more than 25 n-grams had non-zero coefficients. N-grams in order information are marked \nwith [Order]. The translation is not given for n-grams too short to make sense. Negation appears among n-grams with the smallest negative \ncoefficient\n(See figure on next page.)\nPage 13 of 19\nNakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \n \nFig. 6 (See legend on previous page.)\nPage 14 of 19Nakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \npresent in the radiological images without explicitly \nurging further clinical examinations or treatments in \nthe radiology report. The labeling and detection meth -\nods in this study identified such implicit actionable \nreports, though with lower performance than those for \nexplicit ones.\nAnother advantage of the approach of this study is \nthat actionable findings for any pathological entity were \ndealt with, thereby realizing comprehensive detection. \nSince various diseases appear as actionable findings in \nradiological imaging [1, 7–15], this wide coverage is con -\nsidered essential for better clinical practice.\nThe actionable tagging itself can play a certain role in \nthe clinical management of actionable reports. None -\ntheless, introducing an automated detection system for \nactionable findings can make further contributions by \nproviding decisions complementary to those of the radi -\nologists. This is because different radiologists have been \nshown to act differently to actionable findings [52], and \nthere have been no specific criteria for actionable tagging \nin our hospital thus far.\nFig. 7 Top 25 n-grams with the largest feature importance of GBDT (a) without and (b) with order information. N-grams in order information are \nmarked with [Order]. The translation is not given for n-grams too short to make sense\nFig. 8 Examples of LSTM and BERT predictions for two truly actionable reports with visualization of attention scores. (a) is an explicit actionable \nreport detected without order information, and (b) is an implicit actionable report detected using order information. (a) Points out hydronephrosis \ndue to ureteral calculus in the postoperative CT examination of rectal cancer, and (b) points out a lung nodule pointed out in the CT examination \nmore than four years after the operation of esophageal carcinoma. “ < unk > ” stands for out-of-vocabulary subwords that were not recognized by \nthe LSTM and BERT classifiers. Subwords with relatively high attention scores are colored red. For luminous visualization, Japanese periods are not \ncolored\n(See figure on next page.)\nPage 15 of 19\nNakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \n \nFig. 8 (See legend on previous page.)\nPage 16 of 19Nakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \nThere are several limitations of the approach of this \nstudy. First, the BERT model used in this study was not \nspecialized in the biomedical domain. The BERT model \nfailed to recognize about 1% of the words, most of which \nwere abbreviations or uncommon Chinese characters of \nmedical terms. Kawazoe et  al. have recently provided a \nBERT model pre-trained with Japanese clinical records, \nwhich may improve the performance [53]. The pre-train -\ning of BERT with a large Japanese biomedical corpus is \nworthwhile as future work, although it can be costly from \nthe viewpoint of computational resources. Second, the \nshort period since the launch of actionable tagging in our \nFig. 9 Three implicit actionable radiology reports pointing out incidental pneumothorax, all of which were successfully identified only by BERT. \nBERT attention scores are visualized in the same way as in Fig. 8. Although none of the three radiology reports emphasize urgency or explicitly \nrecommend clinical actions, BERT has given high attention scores to the disease name “pneumothorax.”\nTable 8 Recall scores for explicit and implicit truly actionable reports in the test set\nThe maximum score for each subset is shown in bold\nMethod LR GBDT LSTM BERT\nUse of order information ( −) ( +) ( −) ( +) ( −) ( +) ( −) ( +)\nExplicit actionable reports (n = 125) 0.960 0.848 0.872 0.912 0.920 0.936 0.968 0.928\nImplicit actionable reports (n = 111) 0.766 0.766 0.685 0.730 0.793 0.820 0.829 0.802\nPage 17 of 19\nNakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \n \nhospital meant that the amount of data was limited. Con-\ntinuous actionable tagging operations can lead to larger \ndatasets. Finally, since this study is a single-institution \nstudy, our classifiers may be adapted to the epidemiology, \nthe style of reporting, and the principle on actionable \nfindings unique to our hospital. Expanding this study to \nother institutions with similar systems of reporting and \ncommunication will be valuable future work.\nConclusions\nWe have investigated the automated detection of radi -\nology reports with actionable findings using BERT. The \nresults showed that our method based on BERT is more \nuseful for distinguishing various actionable radiology \nreports from non-actionable ones than models based \non other deep learning methods or  statistical machine \nlearning.\nAbbreviations\nAUPRC: Area under the precision-recall curve; AUROC: Area under the receiver \noperating characteristics curve; BERT: Bidirectional encoder representations \nfrom transformers; CNN: Convolutional neural network; GRU : Gated recurrent \nunits; CT: Computed tomography; GBDT: Gradient boosting decision tree; LR: \nLogistic regression; LSTM: Long short-term memory; NLP: Natural language \nprocessing; RIS: Radiology information system; ROC: Receiver operating char-\nacteristics; SML: Statistical machine learning; TF-IDF: Term frequency-inverse \ndocument frequency.\nAcknowledgements\nThe Department of Computational Radiology and Preventive Medicine, \nThe University of Tokyo Hospital, wishes to thank HIMEDIC Inc. and Siemens \nHealthcare K.K.\nAuthors’ contributions\nY. Nakamura implemented the model and analyzed the data. Y. Nakamura, \nSH, and Y. Nomura wrote the manuscript. TN, SM, TW, TY, NH, and OA helped \nrevise the manuscript. SH and Y. Nomura contributed in providing the dataset \nand in improvement of study design and analysis. All authors have read and \napproved the final manuscript.\nFunding\nThe Department of Computational Radiology and Preventive Medicine, \nThe University of Tokyo Hospital, is sponsored by HIMEDIC Inc. and Siemens \nHealthcare K.K.\nDeclarations\nEthics approval and consent to participate\nThis study was approved by the institutional review board at The University \nof Tokyo Hospital (No.:2561-(18), approval date: 25 May 2009, last renewal \ndate: 22 January 2020). All procedures performed in studies involving human \nparticipants were in accordance with the ethical standards of the institutional \nand/or national research committee and with the 1975 Declaration of Helsinki, \nas revised in 2008(5). The institutional review board above stated that formal \nconsent was not required for this study.\nConsent for publication\nNot applicable.\nAvailability of data and materials\nThe radiology reports and order information involved in this study are not \npublicly available because publishing the dataset is not approved by the insti-\ntutional review board of the University of Tokyo Hospital. For more informa-\ntion, please contact the corresponding authors.\nTable 9 Recall scores for truly actionable reports pointing out mass and non-mass abnormalities in the test set\nThe maximum value for each subset is shown in bold\n† Vascular lesions (hemorrhage, thrombosis, infarction, and others) (n = 46), pneumonia (n = 17), pneumothorax (n = 11), hydronephrosis (n = 8), gastrointestinal \nperforation (n = 4), mediastinal emphysema (n = 3), hydrocephalus (n = 2), and other abnormalities (n = 21)\nMethod LR GBDT LSTM BERT\nUse of order information ( −) ( +) ( −) ( +) ( −) ( +) ( −) ( +)\nMass subset (n = 124) 0.935 0.895 0.839 0.855 0.895 0.927 0.927 0.903\nNon-mass subset (n = 112) † 0.795 0.714 0.723 0.795 0.821 0.830 0.875 0.830\nTable 10 Recall for truly actionable reports in the test set calculated for each body part\nMaximum values for each body part are shown in bold\nBody part #Actionable reports Recall\nTotal Implicit Non-mass Order information (−) Order information (+)\nLR GBDT LSTM BERT LR GBDT LSTM BERT\nBrain, head and neck 23/5584 (0.41%) 10/23 (43.5%) 16/23 (69.6%) 0.739 0.609 0.870 0.870 0.652 0.783 0.826 0.696\nBody 206/19,256 (1.1%) 101/206 \n(49.0%)\n91/206 (44.2%) 0.879 0.801 0.854 0.903 0.835 0.825 0.883 0.888\nCardiac 0/151 (0%) – – – – – – – – – –\nSkeletal 9/1758 (0.51%) 1/9 (11.1%) 6/9 (66.7%) 1.000 0.889 1.000 1.000 0.667 1.000 1.000 0.889\nOther 0/959 (0%) – – – – – – – – – –\nPage 18 of 19Nakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \nCompeting interests\nOn behalf of all authors, the corresponding author states that there is no \nconflict of interest.\nAuthor details\n1 Division of Radiology and Biomedical Engineering, Graduate School of Medi-\ncine, The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8655, Japan. \n2 The Department of Radiology, The University of Tokyo Hospital, 7-3-1 Hongo, \nBunkyo-ku, Tokyo 113-8655, Japan. 3 The Department of Computational Diag-\nnostic Radiology and Preventive Medicine, The University of Tokyo Hospital, \n7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8655, Japan. \nReceived: 17 February 2021   Accepted: 23 August 2021\nReferences\n 1. Larson PA, Berland LL, Griffith B, Kahn CE, Liebscher LA. Actionable find-\nings and the role of IT support: report of the ACR Actionable Reporting \nWork Group. J Am Coll Radiol. 2014;11(6):552–8.\n 2. Sloan CE, Chadalavada SC, Cook TS, Langlotz CP , Schnall MD, Zafar HM. \nAssessment of follow-up completeness and notification preferences for \nimaging findings of possible cancer: what happens after radiologists \nsubmit their reports? Acad Radiol. 2014;21(12):1579–86.\n 3. Baccei SJ, DiRoberto C, Greene J, Rosen MP . Improving communication of \nactionable findings in radiology imaging studies and procedures using \nan EMR-independent system. J Med Syst. 2019;43(2):30.\n 4. Cook TS, Lalevic D, Sloan C, Chadalavada SC, Langlotz CP , Schnall MD, \net al. Implementation of an automated radiology recommendation-\ntracking engine for abdominal imaging findings of possible cancer. J Am \nColl Radiol. 2017;14(5):629–36.\n 5. Langlotz CP . Structured radiology reporting: are we there yet? Radiology. \n2009;253(1):23–5.\n 6. Pons E, Braun LM, Hunink MG, Kors JA. Natural language processing in \nradiology: a systematic review. Radiology. 2016;279(2):329–43.\n 7. Meng X, Heinz MV, Ganoe CH, Sieberg RT, Cheung YY, Hassanpour S. \nUnderstanding urgency in radiology reporting: identifying associa-\ntions between clinical findings in radiology reports and their prompt \ncommunication to referring physicians. Stud Health Technol Inform. \n2019;264:1546–7.\n 8. Heilbrun ME, Chapman BE, Narasimhan E, Patel N, Mowery D. Feasibility \nof natural language processing-assisted auditing of critical findings in \nchest radiology. J Am Coll Radiol. 2019;16(9):1299–304.\n 9. Carrodeguas E, Lacson R, Swanson W, Khorasani R. Use of machine learn-\ning to identify follow-up recommendations in radiology reports. J Am \nColl Radiol. 2019;16(3):336–43.\n 10. Yetisgen-Yildiz M, Gunn ML, Xia F, Payne TH. A text processing pipeline \nto extract recommendations from radiology reports. J Biomed Inform. \n2013;46:354–62.\n 11. Yetisgen-Yildiz M, Gunn ML, Xia F, Payne TH. Automatic identification of \ncritical follow-up recommendation sentences in radiology reports. AMIA \nAnnu Symp Proc. 2011;2011:1593–602.\n 12. Dutta S, Long WJ, Brown DF, Reisner AT. Automated detection using natu-\nral language processing of radiologists recommendations for additional \nimaging of incidental findings. Ann Emerg Med. 2013;62:162–9.\n 13. Lau W, Payne TH, Uzuner O, Yetisgen M. Extraction and analysis of clini-\ncally important follow-up recommendations in a large radiology dataset. \nAMIA Jt Summits Transl Sci Proc. 2020;2020:335–44.\n 14. Dang PA, Kalra MK, Blake MA, Schultz TJ, Halpern EF, Dreyer KJ. Extraction \nof recommendation features in radiology with natural language process-\ning: exploratory study. AJR Am J Roentgenol. 2008;191:313–20.\n 15. Imai T, Aramaki E, Kajino M, Miyo K, Onogi Y, Ohe K. Finding malignant \nfindings from radiological reports using medical attributes and syntactic \ninformation. Stud Health Technol Inform. 2007;129:540–4.\n 16. Lou R, Lalevic D, Chambers C, Zafar HM, Cook TS. Automated detection of \nradiology reports that require follow-up imaging using natural language \nprocessing feature engineering and machine learning classification. J \nDigit Imaging. 2020;33(1):131–6.\n 17. Danforth KN, Early MI, Ngan S, Kosco AE, Zheng C, Gould MK. Automated \nidentification of patients with pulmonary nodules in an integrated health \nsystem using administrative health plan data, radiology reports, and \nnatural language processing. J Thorac Oncol. 2012;7:1257–62.\n 18. Garla V, Taylor C, Brandt C. Semi-supervised clinical text classification with \nLaplacian SVMs: an application to cancer case management. J Biomed \nInform. 2013;46:869–75.\n 19. Farjah F, Halgrim S, Buist DSM, Gould MK, Zeliadt SB, Loggers ET, et al. An \nautomated method for identifying individuals with a lung nodule can be \nfeasibly implemented across health systems. EGEMS. 2016. https:// doi. \norg/ 10. 13063/ 2327- 9214. 1254.\n 20. Gershanik EF, Lacson R, Khorasani R. Critical finding capture in the \nimpression section of radiology reports. AMIA Annu Symp Proc. \n2011;2011:465–9.\n 21. Oliveira L, Tellis R, Qian Y, Trovato K, Mankovich G. Identification of \nincidental pulmonary nodules in free-text radiology reports: an initial \ninvestigation. Stud Health Technol Inform. 2015;216:1027.\n 22. Pham A-D, Névéol A, Lavergne T, Yasunaga D, Clément O, Meyer G, et al. \nNatural language processing of radiology reports for the detection of \nthromboembolic diseases and clinically relevant incidental findings. BMC \nBioinform. 2014;15(1):266.\n 23. Mabotuwana T, Hall CS, Dalal S, Tieder J, Gunn ML. Extracting follow-up \nrecommendations and associated anatomy from radiology reports. Stud \nHealth Technol Inform. 2017;245:1090–4.\n 24. Morioka C, Meng F, Taira R, Sayre J, Zimmerman P , Ishimitsu D, et al. \nAutomatic classification of ultrasound screening examinations of the \nabdominal aorta. J Digit Imaging. 2016;29:742–8.\n 25. Xu Y, Tsujii J, Chang EIC. Named entity recognition of follow-up and \ntime information in 20 000 radiology reports. J Am Med Inform Assoc. \n2012;19(5):792–9.\n 26. Fu S, Leung LY, Wang Y, Raulli A-O, Kallmes DF, Kinsman KA, et al. Natural \nlanguage processing for the identification of silent brain infarcts from \nneuroimaging reports. JMIR Med Inform. 2019;7(2):e12109.\n 27. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. \nAttention is all you need. Adv Neural Inf Process Syst. 2017;30:5998–6008.\n 28. Devlin J, Chang MW, Lee K, Toutanova K. BERT: pre-training of deep \nbidirectional transformers for language understanding. In Proceedings \nof the 2019 conference of the North American chapter of the association \nfor computational linguistics: human language technologies, Volume 1 \n(Long and Short Papers). Minneapolis, Minnesota: Association for Compu-\ntational Linguistics; 2019. p. 4171–86.\n 29. Lin Y, Tan YC, Frank R. Open Sesame: getting inside BERT’s linguistic \nknowledge. In: Proceedings of the 2019 ACL workshop BlackboxNLP: \nanalyzing and interpreting neural networks for NLP . Florence, Italy: Asso-\nciation for Computational Linguistics; 2019. p. 241–53.\n 30. Kuwabara R, Han C, Murao K, Satoh S. BERT-based few-shot learning for \nautomatic anomaly classification from Japanese multi-institutional CT \nscan reports. Int J Comput Assist Radiol Surg. 2020;15(Suppl 1):S148–9.\n 31. Peng Y, Lee S, Elton DC, Shen T, Tang Y-X, Chen Q, et al. Automatic \nrecognition of abdominal lymph nodes from clinical text. In: Proceedings \nof the 3rd clinical natural language processing workshop. Association for \nComputational Linguistics; 2020. pp. 101–10.\n 32. American College of Radiology. ACR practice parameter for communica-\ntion of diagnostic imaging findings revised 2020. 2020. https:// www. acr. \norg/-/ media/ ACR/ Files/ Pract ice- Param eters/ Commu nicat ionDi ag. pdf? \nla= en. Accessed 10 Feb 2021.\n 33. Kudo T, Richardson J. SentencePiece: a simple and language independ-\nent subword tokenizer and detokenizer for neural text processing. In: \nProceedings of the 2018 conference on empirical methods in natural \nlanguage processing: system demonstrations. Brussels, Belgium: Associa-\ntion for Computational Linguistics; 2018. p. 66–71.\n 34. Kikuta Y. BERT pretrained model trained on Japanese Wikipedia articles. \n2019. https:// github. com/ yohei kikuta/ bert- japan ese. Accessed 10 Feb \n2021.\n 35. Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learn-\ning to align and translate. In: 3rd International conference on learning \nrepresentations, ICLR 2015. San Diego, CA, USA: 2015.\n 36. Kingma DP , Ba J. Adam: a method for stochastic optimization. In: 3rd \nInternational conference on learning representations, ICLR 2015. San \nDiego, CA, USA: 2015.\n 37. Saito T, Rehmsmeier M. The precision-recall plot is more informative than \nthe ROC plot when evaluating binary classifiers on imbalanced datasets. \nPLoS ONE. 2015;10(3):e0118432.\nPage 19 of 19\nNakamura et al. BMC Med Inform Decis Mak          (2021) 21:262 \n \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 38. Davis J, Goadrich M. The relationship between precision-recall and ROC \ncurves. In: Proceedings of the 23rd international conference on machine \nlearning. New York, NY, USA: Association for Computing Machinery; \n233–240, 2006. p. 233–40.\n 39. Hochreiter S, Schmidhuber J. Long short-term memory. Neural Comput. \n1997;9:1735–80.\n 40. Dan J, James HM. Speech and language processing, 3rd edition in draft. \n2020. https:// web. stanf ord. edu/ ~juraf sky/ slp3/ ed3bo ok_ dec30 2020. pdf. \nAccessed 10 Feb 2021.\n 41. Friedman JH. Greedy function approximation: a gradient boosting \nmachine. Ann Stat. 2001;29:1189–232.\n 42. Friedman J, Hastie T, Tibshirani R. Regularization paths for generalized \nlinear models via coordinate descent. J Stat Softw. 2010;33:1–22.\n 43. Prokhorenkova L, Gusev G, Vorobev A, Dorogush AV, Gulin A. CatBoost: \nunbiased boosting with categorical features. In: Proceedings of the 32nd \ninternational conference on neural information processing systems. Red \nHook, NY, USA: Curran Associates Inc.; 2018. p. 6639–49.\n 44. Armstrong RA. When to use the Bonferroni correction. Ophthalmic \nPhysiol Opt. 2014;34:502–8.\n 45. Obuchowski NA. ROC analysis. AJR Am J Roentgenol. 2005;184:364–72.\n 46. Fawcett T. An introduction to ROC analysis. Pattern Recognit Lett. \n2006;27:861–74.\n 47. Buda M, Maki A, Mazurowski MA. A systematic study of the class \nimbalance problem in convolutional neural networks. Neural Netw. \n2018;106:249–59.\n 48. Ali A, Shamsuddin SM, Ralescu AL. Classification with class imbalance \nproblem: a review. Int J Adv Soft Comput Appl. 2015;7(3):176–204.\n 49. Wei J, Zou K. EDA: Easy data augmentation techniques for boosting \nperformance on text classification tasks. In: Proceedings of the 2019 \nconference on empirical methods in natural language processing and \nthe 9th international joint conference on natural language process-\ning (EMNLP-IJCNLP). Hong Kong, China: Association for Computational \nLinguistics; 2019. p. 6382–8.\n 50. Madabushi HT, Kochkina E, Castelle M. Cost-sensitive BERT for generalis-\nable sentence classification with imbalanced data. In: Proceedings of the \nsecond workshop on natural language processing for internet freedom: \ncensorship, disinformation, and propaganda. Hong Kong, China: Associa-\ntion for Computational Linguistics; 2019. p. 125–34.\n 51. Li X, Sun X, Meng Y, Liang J, Wu F, Li J. Dice loss for data-imbalanced NLP \ntasks. In: Proceedings of the 58th annual meeting of the association for \ncomputational linguistics. 2020. p. 465–76.\n 52. Cochon LR, Kapoor N, Carrodeguas E, Ip IK, Lacson R, Boland G, \net al. Variation in follow-up imaging recommendations in radiol-\nogy reports: patient, modality, and radiologist predictors. Radiology. \n2019;291(3):700–7.\n 53. Kawazoe Y, Shibata D, Shinohara E, Aramaki E, Ohe K. A clinical specific \nBERT developed with huge size of Japanese clinical narrative. medRxiv. \n2020. https:// doi. org/ 10. 1101/ 2020. 07. 07. 20148 585.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations."
}