{
    "title": "Automated extraction of functional biomarkers of verbal and ambulatory ability from multi-institutional clinical notes using large language models",
    "url": "https://openalex.org/W4409974329",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5095746379",
            "name": "Levi Kaster",
            "affiliations": [
                "Washington University in St. Louis"
            ]
        },
        {
            "id": "https://openalex.org/A5107512231",
            "name": "Ethan Hillis",
            "affiliations": [
                "Washington University in St. Louis"
            ]
        },
        {
            "id": "https://openalex.org/A2551233547",
            "name": "Inez Y Oh",
            "affiliations": [
                "Washington University in St. Louis"
            ]
        },
        {
            "id": "https://openalex.org/A2250684121",
            "name": "Bhooma R. Aravamuthan",
            "affiliations": [
                "Washington University in St. Louis"
            ]
        },
        {
            "id": null,
            "name": "Virginia C. Lanzotti",
            "affiliations": [
                "Washington University in St. Louis"
            ]
        },
        {
            "id": "https://openalex.org/A2398444098",
            "name": "Casey R Vickstrom",
            "affiliations": [
                "Washington University in St. Louis"
            ]
        },
        {
            "id": null,
            "name": "Brain Gene Registry Consortium",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3069490628",
            "name": "M. Wasserstein",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1986340009",
            "name": "M. Chopra",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2108316427",
            "name": "M. Sahin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1868973100",
            "name": "M. Wangler",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1981791744",
            "name": "Schultz B",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096282375",
            "name": "K. Izumi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2149278282",
            "name": "S. Bergner",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4327689505",
            "name": "A Gropman",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4380135031",
            "name": "C. Smith-Hicks",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4207241081",
            "name": "L Abbeduto",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2496628910",
            "name": "H Hazlett",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2101556506",
            "name": "D. Doherty",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2012696188",
            "name": "K. German",
            "affiliations": []
        },
        {
            "id": null,
            "name": "L DaWalt",
            "affiliations": []
        },
        {
            "id": null,
            "name": "J Neul",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A268010357",
            "name": "J. Constantino",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2626908323",
            "name": "Baldridge D",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097931740",
            "name": "S. Srivastava",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3081795364",
            "name": "S. Molholm",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2215497814",
            "name": "S Walkley",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2233787191",
            "name": "E Storch",
            "affiliations": []
        },
        {
            "id": null,
            "name": "R Samaco",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1968489310",
            "name": "J. Cohen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1994607382",
            "name": "S Shankar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2337375057",
            "name": "J. Piven",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2103002141",
            "name": "S Mahida",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5076461290",
            "name": "A. Sveden",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2468650301",
            "name": "K. Dies",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5044391337",
            "name": "ER Riggs",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5117391932",
            "name": "JM Savatt",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2420709797",
            "name": "B. Minor",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1917023127",
            "name": "Christina A. Gurnett",
            "affiliations": [
                "Washington University in St. Louis"
            ]
        },
        {
            "id": "https://openalex.org/A683941801",
            "name": "Philip R O Payne",
            "affiliations": [
                "Washington University in St. Louis"
            ]
        },
        {
            "id": "https://openalex.org/A1967184651",
            "name": "Aditi Gupta",
            "affiliations": [
                "Washington University in St. Louis"
            ]
        },
        {
            "id": "https://openalex.org/A5095746379",
            "name": "Levi Kaster",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5107512231",
            "name": "Ethan Hillis",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2551233547",
            "name": "Inez Y Oh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2250684121",
            "name": "Bhooma R. Aravamuthan",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Virginia C. Lanzotti",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2398444098",
            "name": "Casey R Vickstrom",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1917023127",
            "name": "Christina A. Gurnett",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A683941801",
            "name": "Philip R O Payne",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1967184651",
            "name": "Aditi Gupta",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W91108901",
        "https://openalex.org/W2604825012",
        "https://openalex.org/W2142071150",
        "https://openalex.org/W1779612606",
        "https://openalex.org/W2053972292",
        "https://openalex.org/W160573773",
        "https://openalex.org/W3165305154",
        "https://openalex.org/W2770189838",
        "https://openalex.org/W1959618675",
        "https://openalex.org/W4321783153",
        "https://openalex.org/W2039648273",
        "https://openalex.org/W4388011452",
        "https://openalex.org/W4396553888",
        "https://openalex.org/W4400378221",
        "https://openalex.org/W2925863688",
        "https://openalex.org/W1493182793",
        "https://openalex.org/W4394891439",
        "https://openalex.org/W4389299015",
        "https://openalex.org/W1495905662",
        "https://openalex.org/W2897030725",
        "https://openalex.org/W2951658763",
        "https://openalex.org/W1589750505",
        "https://openalex.org/W2116956067",
        "https://openalex.org/W2022167001",
        "https://openalex.org/W2050351220",
        "https://openalex.org/W2134481472",
        "https://openalex.org/W1938587512",
        "https://openalex.org/W2116887525",
        "https://openalex.org/W2137207936",
        "https://openalex.org/W2127693561",
        "https://openalex.org/W4392906843",
        "https://openalex.org/W4391644915"
    ],
    "abstract": "Abstract Background Functional biomarkers in neurodevelopmental disorders, such as verbal and ambulatory abilities, are essential for clinical care and research activities. Treatment planning, intervention monitoring, and identifying comorbid conditions in individuals with intellectual and developmental disabilities (IDDs) rely on standardized assessments of these abilities. However, traditional assessments impose a burden on patients and providers, often leading to longitudinal inconsistencies and inequities due to evolving guidelines and associated time–cost. Therefore, this study aimed to develop an automated approach to classify verbal and ambulatory abilities from EHR data of IDD and cerebral palsy (CP) patients. Application of large language models (LLMs) to clinical notes, which are rich in longitudinal data, may provide a low-burden pipeline for extracting functional biomarkers efficiently and accurately. Methods Data from the multi-institutional National Brain Gene Registry (BGR) and a CP clinic cohort were utilized, comprising 3,245 notes from 125 individuals and 5,462 clinical notes from 260 individuals, respectively. Employing three LLMs—GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4 Omni—we provided the models with a clinical note and utilized a detailed conversational format to prompt the models to answer: \"Does the individual use any words?\" and \"Can the individual walk without aid?\" These responses were evaluated against ground-truth abilities, which were established using neurobehavioral assessments collected for each dataset. Results LLM pipelines demonstrated high accuracy (weighted-F1 scores &gt; .90) in predicting ambulatory ability for both cohorts, likely due to the consistent use of Gross Motor Functional Classification System (GMFCS) as a consistent ground-truth standard. However, verbal ability predictions were more accurate in the BGR cohort, likely due to higher adherence between the prompt and ground-truth assessment questions. While LLMs can be computationally expensive, analysis of our protocol affirmed the cost effectiveness when applied to select notes from the EHR. Conclusions LLMs are effective at extracting functional biomarkers from EHR data and broadly generalizable across variable note-taking practices and institutions. Individual verbal and ambulatory ability were accurately extracted, supporting the method's ability to streamline workflows by offering automated, efficient data extraction for patient care and research. Future studies are needed to extend this methodology to additional populations and to demonstrate more granular functional data classification.",
    "full_text": "Kaster et al. \nJournal of Neurodevelopmental Disorders           (2025) 17:24  \nhttps://doi.org/10.1186/s11689-025-09612-w\nRESEARCH Open Access\n© The Author(s) 2025. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecom-\nmons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nJournal of\nNeurodevelopmental Disorders\nAutomated extraction of functional \nbiomarkers of verbal and ambulatory ability \nfrom multi-institutional clinical notes using \nlarge language models\nLevi Kaster1, Ethan Hillis1, Inez Y. Oh1, Bhooma R. Aravamuthan2, Virginia C. Lanzotti3, Casey R. Vickstrom2, Brain \nGene Registry Consortium, Christina A. Gurnett2, Philip R. O. Payne1 and Aditi Gupta1* \nAbstract \nBackground Functional biomarkers in neurodevelopmental disorders, such as verbal and ambulatory abilities, are \nessential for clinical care and research activities. Treatment planning, intervention monitoring, and identifying comor-\nbid conditions in individuals with intellectual and developmental disabilities (IDDs) rely on standardized assessments \nof these abilities. However, traditional assessments impose a burden on patients and providers, often leading to lon-\ngitudinal inconsistencies and inequities due to evolving guidelines and associated time–cost. Therefore, this study \naimed to develop an automated approach to classify verbal and ambulatory abilities from EHR data of IDD and cer-\nebral palsy (CP) patients. Application of large language models (LLMs) to clinical notes, which are rich in longitudinal \ndata, may provide a low-burden pipeline for extracting functional biomarkers efficiently and accurately.\nMethods Data from the multi-institutional National Brain Gene Registry (BGR) and a CP clinic cohort were utilized, \ncomprising 3,245 notes from 125 individuals and 5,462 clinical notes from 260 individuals, respectively. Employing \nthree LLMs—GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4 Omni—we provided the models with a clinical note and utilized \na detailed conversational format to prompt the models to answer: \"Does the individual use any words?\" and \"Can \nthe individual walk without aid?\" These responses were evaluated against ground-truth abilities, which were estab-\nlished using neurobehavioral assessments collected for each dataset.\nResults LLM pipelines demonstrated high accuracy (weighted-F1 scores > .90) in predicting ambulatory ability \nfor both cohorts, likely due to the consistent use of Gross Motor Functional Classification System (GMFCS) as a con-\nsistent ground-truth standard. However, verbal ability predictions were more accurate in the BGR cohort, likely due \nto higher adherence between the prompt and ground-truth assessment questions. While LLMs can be computation-\nally expensive, analysis of our protocol affirmed the cost effectiveness when applied to select notes from the EHR.\nConclusions LLMs are effective at extracting functional biomarkers from EHR data and broadly generalizable \nacross variable note-taking practices and institutions. Individual verbal and ambulatory ability were accurately \nextracted, supporting the method’s ability to streamline workflows by offering automated, efficient data extrac-\ntion for patient care and research. Future studies are needed to extend this methodology to additional populations \nand to demonstrate more granular functional data classification.\nKeywords Functional biomarkers, Neurodevelopmental disorders, Electronic health records, Large language models\n*Correspondence:\nAditi Gupta\nagupta24@wustl.edu\nFull list of author information is available at the end of the article\nPage 2 of 17Kaster et al. Journal of Neurodevelopmental Disorders           (2025) 17:24 \nBackground\nVerbal and ambulatory ability are functional and clinically \nmeaningful biomarkers of intellectual and developmental \ndisabilities (IDDs) that meet the National Institutes of \nHealth Biomarkers Definitions Working Group definition \nas characteristics that are “objectively measured and eval-\nuated as an indicator of normal biological processes, or \npharmacological responses to a therapeutic intervention” \n[1]. Treatment planning, intervention monitoring, and \nidentification of comorbid conditions in individuals with \nIDDs, as well as research to understand the underlying \npathology of these conditions, currently relies on assess -\nments of these functional ability biomarkers. However, \nstandardized assessments, which may consist of either \nshort user-defined surveys or well validated instruments \nor screening tools, are typically burdensome and time-\nconsuming as they require dedicated effort from provid -\ners, specialists, affected individuals, and their caregivers \nfor implementation, completion and storage. Further -\nmore, IDD phenotypes change over time, but standard \nassessments often provide only a one-time snapshot of \nan individual’s abilities and are challenging to accrue over \ntime, limiting longitudinal clinical and research charac -\nterization of these phenotypes. In addition, standardized \nassessments may suffer from inconsistencies, including \nmissing data, evolving guidelines/protocols, and patient/\ncaregiver and assessor subjectivity, resulting in a lack of \nreproducibility. On the other hand, the electronic health \nrecord (EHR) constitutes a rich source of longitudinal \nreal-world clinical data accrued obligately during routine \nhealthcare encounters, including demographics, diagno -\nses, medications, procedures, laboratory results, vitals, \nand clinical notes, making the EHR a powerful resource \nfor predicting health outcomes [2–4]. Hence, we propose \nan automated natural language processing (NLP) based \npipeline to extract longitudinal functional phenotypes \nfrom EHR to augment and fill gaps in the information \ncollected through formal assessments or surveys.\nVerbal and ambulatory abilities are not often docu -\nmented in structured tables within the EHR [5 ], despite \nrecommendations by the US Department of Health and \nHuman Services to document functional disabilities \n[6–9]. While deficient in structured data pertaining \nto functional ability, the EHR nevertheless possesses \na wealth of unstructured and non-standardized infor -\nmation that may be helpful for identifying and charac -\nterizing disabilities. A study from 2011 examining the \nfeasibility of manual abstraction of medical records to \nclassify gross motor function of children with cerebral \npalsy (CP) records found that 90% of the study cohort \nhad gross motor skill descriptions in their medical \nrecords that were adequate for Gross Motor Function \nClassification System (GMFCS) classification, with 75% \nagreement between two qualified clinician raters, sug -\ngesting that even without structured documentation, \nthe EHR remains a valuable source of this information \npertaining to functional ability [10].\nDue to the diverse terminology and structures of \nclinical notes, the identification of verbal and ambula -\ntory ability within notes is not trivial, and NLP tech -\nniques for clinical entity extraction are necessary to \nautomatically derive this information from notes [2 –4, \n11]. Rule-based natural language processing methods \nhave been widely utilized for clinical entity extraction \nbecause these methods identify exact pre-specified \nphrases found within the text, resulting in highly effec -\ntive extraction [11– 13]. However, identifying relevant \nphrases can be extremely time consuming, particularly \nwhen applying the same method across varying note \nauthors, types and templates. Recently, Large Language \nModels (LLMs) have emerged as an alternative clinical \nentity extraction technique, with demonstrated suc -\ncess at information extraction from clinical texts [14, \n15]. LLMs are pre-trained transformer-based artifi -\ncial intelligence (AI) language models that have been \nutilized for a variety of NLP tasks [16, 17], and can be \nused out of the box without fine-tuning or intensive \nmanual rule-development. Thus, we hypothesize that \nthe identification of functional abilities from the EHR \ncan be automated using state-of-the-art artificial intel -\nligence methods, which may reduce patient/caregiver/\nprovider burden and be an efficient tool to collect clini -\ncally important functional biomarkers for trending out -\ncomes in natural history studies, assessing therapeutic \nresponses, and identification of patients with functional \ndisabilities who may benefit from targeted management \nstrategies.\nIn this study, we leveraged LLMs to develop an auto -\nmated and generalized pipeline to determine verbal and \nambulatory ability using clinical notes from the EHR. We \nevaluated the performance of our pipeline in two inde -\npendent cohorts: the national Brain Gene Registry (BGR), \nwhich includes multi-institutional participants with rare \nneurogenetic disorders, and a single institution (St. Louis \nChildren’s Hospital Cerebral Palsy Center) cohort of indi-\nviduals with CP , many of whom have functional limita -\ntions in verbal and motor abilities [18]. By implementing \nthis pipeline on EHR data collected across twelve aca -\ndemic medical centers in the BGR and across two differ -\nent clinical cohorts, we assess both the generalizability \nof the pipeline and its efficacy for predicting clinically \nmeaningful functional biomarkers from EHR data. The \ngoal of this work is to create a clinical extraction pipeline \nthat can be generalized for extracting and analyzing other \nclinical phenotypes in IDDs more broadly.\nPage 3 of 17\nKaster et al. Journal of Neurodevelopmental Disorders           (2025) 17:24 \n \nMethods\nStep 1: Data collection and preprocessing\nNational Brain Gene Registry (BGR) dataset\nData from the National BGR were obtained with the \napproval of the Washington University Institutional \nReview Board (# 202,010,013). The BGR aggregates clini -\ncally informative data from participants with rare genetic \nneurodevelopmental disorders, recruited across twelve \nacademic medical centers with the goal of accelerating \nthe curation of gene-disease associations [19, 20]. The \nrepository includes genotypic and phenotypic data col -\nlected from a variety of sources, including patient EHR \ndata, a Rapid Neurobehavioral Assessment Protocol \n(RNAP) that is comprised of a gold-standard battery of \nremotely delivered neurobehavioral assessments, span -\nning a variety of domains (including verbal and motor \nfunction)(Table  1) [20], variant-level genomic data \nobtained from GenomeConnect [21, 22], and additional \nrelevant records such as previous neuropsychological \nreports, and photographs. As of May 8, 2024, the national \nBGR contained data from 564 participants recruited \nacross 12 Eunice Kennedy Shriver Intellectual and Devel-\nopmental Disability Research Centers (IDDRCs) [23].\nFor this study we extracted data from sources in the \nBGR:\n• Clinical Notes from EHR: We focused on progress \nnotes, corresponding note metadata (author name, \nencounter date, note type, etc.), and demograph -\nics (DOB, sex, etc.) originating from the electronic \nhealth record (EHR); and questionaries, surveys and \nother neurobehavioral assessments collected through \nthe BGR’s RNAP . The clinical notes obtained from \nthe BGR were authored between July 2002 to March \n2024. In sum, 43,482 clinical notes were included in \nthe initial data pull from the BGR, which we nar -\nrowed down to 19,546 notes after selecting only \nprogress notes. We further limited the dataset to \nprogress notes most relevant to recent verbal and \nambulatory status based on these criteria: 1) notes \nauthored by providers from 14 specialties determined \nby expert clinicians to be relevant according to note \ntype and frequency, 2) notes of individuals who had \nground-truth labels (i.e. RNAP data), 3) notes written \nwhen the individual was at least 3 years old, and 4) \nnotes of individuals who have at least 5 notes. The 14 \nTable 1 Assessments included in the rapid neurobehavioral assessment protocol\nTable depicting information relevant to the neurobehavioral assessment measures included in the RNAP , such as the measure name, the domain assessed, how the \nassessment is performed, the age range of individuals the measure can be applied to, and the approximate time taken complete the assessment\nRapid Neurobehavioral Assessment Protocol\nDomains Assessed Measure Assessment Type Age Time\nVision, Hearing and Verbal Ability Telehealth Visit Guide Parent Interview all 5–10 min\nCognitive Ability Shipley- 2, Block Patterns Direct Assessment 7–89 yrs 10 min\nDevelopmental Profile- 4, Cognitive Domain Parent Questionnaire 0–21 yrs, 11 mos 5–10 min\nAdaptive Functioning Vineland- 3: Comprehensive Parent/Caregiver Form Parent Questionnaire 0–2 yrs, 11 mos 13–19 min\nVineland- 3: Domain-Level Parent/Caregiver Form Parent Questionnaire 3 yrs and older 12–18 min\nMotor/Sensory Function Repetitive Behavior Scale-Revised Parent Questionnaire 2 yrs to adult 15 min\nSensory Experiences Questionnaire- 3 Parent Questionnaire 2 yrs—12 yrs 15–20 min\nDevelopmental Coordination Disorder Questionnaire Parent Questionnaire 5 yrs—15 yrs 10–15 min\nGross Motor Functioning Classification System Parent Questionnaire 2 yrs—18 yrs < 5 min\nAutistic Features Social Communication Questionnaire-Lifetime Version Parent Questionnaire > 4 yrs < 10 min\nModified Checklist for Autism in Toddlers-Revised Parent Questionnaire 16 mos—30 mos 10 min\nSocial Responsiveness Scale- 2 Parent Questionnaire 2.5 yrs—19 + yrs 15–20 min\nChildhood Autism Rating Scale- 2 Observer (High Function-\ning or Standard)\nDirect Assessment all 15–20 min\nPsychiatric Symptoms Vanderbilt ADHD Parent Questionnaire 6 yrs—12 yrs 10 min\nAchenbach System of Empirically Based Assessment (ASEBA) Parent Questionnaire 1.5 yrs—59 yrs 15–20 min\nAbberant Behavior Checklist-Community Parent Questionnaire 5 yrs—adult 10–15 min\nPhysical Features Dysmorphology Screen Direct Assessment all 10–15 min\nPhotographs Parent Completed all 10–15 min\nNeurological Symptoms Seizure History Parent Questionnaire all 10–15 min\nVirtual Neurological Exam Direct Assessment All 10–15 min\nEdinburgh Handedness Inventory Parent Questionnaire 4 yrs—adult 5 min\nPage 4 of 17Kaster et al. Journal of Neurodevelopmental Disorders           (2025) 17:24 \nspecialties whose notes were determined by expert \nclinicians to be relevant are listed in Supplementary \nTable 1. After applying these criteria, the final dataset \nconsisted of 3,245 notes from 125 individuals.\n• Neurobehavioral Assessments: Every participant \nin the BGR had their RNAP data collected between \nFebruary 2021 and May 2024. The RNAP data was \nutilized as ground-truth labels to evaluate the per -\nformance of the NLP pipeline developed to extract \nfunctional phenotypes from EHR clinical notes. \nTable 1 summarizes all assessments included in the \nR N A P.\nCerebral palsy (CP) dataset\nData for the CP cohort were obtained with the approval \nof the Washington University Institutional Review Board \n(# 202,309,003), and two databases from the St Louis \nChildren’s Hospital CP Center cohort were utilized:\n• Research Database with Standardized Assessments: \nThese data included St. Louis Children’s Hospital \n(SLCH) CP Center provider-populated GMFCS, \nViking Speech Scale (VSS), and Communication \nFunction Classification System (CFCS) classifica -\ntions, which are well validated batteries for classify -\ning self-initiated motor, speech, and communication \nabilities, respectively [24–29]. Since April 2023, pro -\nviders in the St. Louis Children’s Hospital CP Center \nhave routinely assessed and documented these on all \npatients, thus generating a high-quality ground-truth \nlabels for evaluating our pipeline for extracting and \nmapping verbal and motor function data from EHR \ndata to these scales.\n• Clinical Notes from the EHR: Clinical notes authored \nbetween 9/22/2022—8/26/2024 were extracted \nfrom Washington University School of Medicine’s \n(WUSM’s) Research Data Core, a research repository \nof EHR data originating from the WUSM/Barnes \nJewish Hospital/St. Louis Children’s Hospital Epic \nEHR system. In total, 134,177 clinical notes were \nincluded in the initial CP data pull, of which 19,551 \nwere progress notes. We further reduced the data -\nset to only notes relevant to gross motor function \nclassification based on these criteria: 1) notes from \nthe same 14 specialties as the BGR dataset, 2) notes \nwritten within 1.5 years of when the ground-truth \nannotations were created for each patient, 3) notes \ncontaining at least 500 words, 4) notes of individuals \nwho have at least 5 remaining notes. This resulted in \na dataset of 5,462 notes. In the next step, from these \nnotes we removed any references of the GMFCS and \nVSS score in order to mitigate bias and ensure that \nour pipeline predicts functional phenotypes without \nusing the values from these instruments. Using string \nsearch, the following phrases and the 15 characters \nbefore and after were removed to prevent data leak -\nage: ‘gmfcs’ , ‘gross motor function’ , ‘gross motor func-\ntion cs’ , ‘vss’ , ‘viking speech’ , and ‘cfcs’ . The final data-\nset consisted of 5,462 notes from 260 individuals.\nStep 2: Targeted functional biomarkers for extraction\nWe identified two functional biomarkers of interest \nwhich were the ability to use any number of words via \nmotor speech (verbal ability) and ability to walk inde -\npendently without any assistance or walking devices \n(ambulatory ability), for extraction from the clinical \nnotes in our datasets. To extract these, we developed two \nquestions to inform our prompt to the LLM, “Does the \nindividual use any words?” and “Can the individual walk \nwithout aid?” . An initial prompt was developed to elicit \nan LLM response to these questions when provided a \nclinical note. By testing against a small subset of notes, \niterative prompt engineering was performed to maximize \nperformance. For example, the LLM originally mistak -\nenly included those walking with assistance as those able \nto walk, so the clarifying text “someone who walks with a \nwalking aid should not be considered ‘able to walk’” was \nadded to our prompt. The final prompt was reviewed by \nclinicians and informaticians with NLP experience to \nensure accuracy. The iteratively achieved final prompt \nconsisted of 4 components: (1) a system prompt inform -\ning the model of its role to emulate a medical physician \nwith the goal to extract verbal and ambulatory ability \nfrom a clinical note, (2) definitions of verbal and ambula -\ntory ability, (3) an example of the output format, and (4) \nthe clinical note text. This full prompt is shown in Fig. 1.\nStep 3: Identification of ground‑truth functional biomarker \nstatus of individuals\nGround‑truth for the Brain Gene Registry (BGR) dataset\nThe RNAP in the BGR includes questions directly rel -\nevant to verbal and ambulatory ability, allowing us to \nestablish individual-level ground-truth verbal and \nambulatory status without manually annotating clini -\ncal notes. Relevant surveys and assessments included \nthe second edition of the Child Autism Rating Scale \n(CARS- 2) [30], the Gross Motor Function Classifica -\ntion System (GMFCS) [31], a telehealth screener, and \nthe Modified Checklist for Autism in Toddlers, Revised \n(M-CHAT-R) [32, 33]. We mapped responses from \nthese assessments to ground-truth labels for the two \nselected questions “Does the individual use any words?” \nand “Can the individual walk without aid?” , which are \nprovided in Table  2. There were no conflicting mapped \nPage 5 of 17\nKaster et al. Journal of Neurodevelopmental Disorders           (2025) 17:24 \n \nresponses among participants who completed multiple \nassessments that map to a single identified question \n(i.e. CARS- 2 and Telehealth Screener).\nGround‑truth for the Cerebral Palsy (CP) dataset\nThe ground-truth CP annotations are solely based on \nGMFCS, the Viking Speech Scale (VSS), and the Com -\nmunication Function Classification System (CFCS). The \nmappings to the identified questions are provided in \nTable 3. The ambulatory status predictions were evalu -\nated once against the GMFCS; however, the verbal sta -\ntus predictions were evaluated separately against both \nthe VSS and CFCS.\nStep 4: Development of NLP/LLM pipeline to identify \nindividual functional biomarker status\nThree pipelines were built to prompt the Genera -\ntive Pre-trained Transformer (GPT) model to classify \npatient verbal and ambulatory status at the note level, \nutilizing GPT- 3.5 Turbo version 0613 (GPT- 3.5), \nGPT- 4 Turbo model version 0125-preview (GPT- 4 t), \nor GPT- 4 Omni (GPT- 4o). Each pipeline employed \nHIPAA-compliant OpenAI endpoints of GPT accessed \nthrough Washington University’s Azure tenant, allow -\ning clinical notes to be provided to the model without \nprior deidentification. Each clinical progress note was \nprovided to the model, which was prompted to utilize \nonly the information found in the note to answer: “Does \nthe individual use any words?” and “Can the individual \nwalk without aid?”  Full unmodified progress notes were \nprovided to the model when using GPT- 4 t and GPT- \n4o, but the smaller context limit of GPT- 3.5 required \ntruncated note versions (10,000 characters) to be pro -\nvided when using that model, and the rest of the note \nwas discarded. Two separate prompts were developed \nfor each pipeline: a multi-class prompt (MCP) that \nallowed GPT to respond with “yes” , “no” , or “unknown” \nto both questions concurrently, and a binary-class \nprompt (BCP) that restricted GPT responses to “yes” or \nFig. 1 Prompt for large language model analysis. The generative pre-trained transformer (GPT) model was prompted in a conversational format \nin which GPT’s system prompt is first asserted. The system prompt steers the behavior of the model, allowing for it to be more adaptable to the task. \nThe user (researcher) then asks if GPT understands its role, to which GPT confirms. Finally, the user provides detailed walking and using words \ndefinitions and extraction instructions with the desired output format. The clinical note is then included in the prompt at the placeholder symbol “{}”\nPage 6 of 17Kaster et al. Journal of Neurodevelopmental Disorders           (2025) 17:24 \n“no” . To develop these prompts, we selected a subset of \n10 notes from the BGR representing unique specialties \nand applied a baseline prompt to these notes. Then we \niteratively refined the prompts to ensure that note-level \nextraction matched the note content, resulting in the \nfinal prompts that we used. For each version of GPT, \nthe verbal and ambulatory ability were extracted for \nthree unique experiments consisting of different com -\nbinations of prompts and note sets, testing the effects \nof allowing GPT to respond with ‘unknown’ and eval -\nuating whether notes written more closely in time to \nthe creation of the ground-truth labels contain more \nrelevant information. The three unique experiments are \nbelow:\n1) MCP-All: Prompt GPT to perform multi-class \nextraction (“yes” , “no” , and “unknown”) on the entire \nset of identified clinical notes.\n2) BCP-All: Prompt GPT to perform binary-class \nextraction (“yes” or “no”) on the entire set of identi -\nfied clinical notes.\n3) MCP- 1.5Y: Prompt GPT to perform multi-class \nextraction on only the clinical notes written within \n1.5 years of the date the patient was enrolled in the \nrespective registries.\nTable 2 RNAP assessment responses mapped to identified questions\nThis table lists the RNAP assessments utilized to determine the patients’ ground-truth verbal and ambulatory abilities. Columns include the name of the assessment, \nthe specific verbal or ambulatory ability question that is addressed, the pertinent question from the assessment, the recorded response to the assessment question, \nand the mapped ground-truth patient verbal or ambulatory ability\nAssessment Identified Question Assessment Question Response Mapped \nResponse\nCARS- 2 Does the individual use any words? Is the person you are rating using words? Yes Yes\nNo No\nTelehealth Screener Does [participant’s name] use single words? Yes Yes\nNo No\nGMFCS Can the individual walk without aid? Motor function is classified on a 1–5 scale accord-\ning to different criteria for different ages\nClass: 1\nAge: 3 to 4\nYes\nClass: 2,3,4,5\nAge: 3 to 4\nNo\nClass: 1,2\nAge: 4 to 18\nYes\nClass: 3,4,5\nAge: 4 to18\nNo\nM-CHAT-R Does your child walk? Yes Yes\nNo No\nTable 3 Cerebral palsy assessment responses mapped to identified questions\nThis table lists the assessments utilized to determine the patients’ ground-truth verbal and ambulatory abilities for the Cerebral Palsy cohort. Columns include the \nname of the assessment, the specific verbal or ambulatory ability question that is addressed, the pertinent question from the assessment, the recorded response to \nthe assessment question, and the mapped ground-truth patient verbal or ambulatory ability\nAssessment Identified Question Assessment Question Response Mapped \nResponse\nVSS Does the individual use any words? Motor impact on speech is classified on a 1–4 scale Class: 1, 2, 3 Yes\nClass: 4 No\nCFCS Communication ability with familiar and unfamiliar \npeople at levels 1 through 5\nClass: 1, 2, 3, 4 Yes\nClass: 5 No\nGMFCS Can the individual walk without aid? Motor function is classified at levels 1 through 5 using \ndifferent criteria for different ages\nClass: 1 Yes\nClass: 2,3,4,5\nAge: 3 to 4\nNo\nClass: 1,2\nAge: 4 to 18\nYes\nClass: 3,4,5\nAge: 4 to18\nNo\nPage 7 of 17\nKaster et al. Journal of Neurodevelopmental Disorders           (2025) 17:24 \n \nEach of these experiments were evaluated on the BGR \ncohort data, but only the MCP- 1.5Y prompt and note \nset was evaluated on the CP cohort data. Binary class \nprediction was excluded from the CP cohort analysis \nbecause we found the multi-class prediction prompts led \nto better or matching performance in the BGR data, as \nseen in Table  5. Prediction on the entire CP cohort was \nnot performed due to the larger note counts within the \nCP cohort compared to the BGR cohort, as well as cost \nconstraints associated with a larger note cohort (see Cost \nAnalysis).\nPredict functional biomarker status per individual\nFor the BGR and CP datasets, we mapped the note-\nlevel predictions to a single individual-level prediction \nbecause the ground-truth labels were known for each \nindividual. For each question and individual in the BGR, \nthe mapped response was “yes” if the number of notes \nwith a “yes” prediction was greater than the number of \nnotes with a “no” prediction, and the mapped response \nwas “no” otherwise. The content and contextual pat -\nterns of notes from the CP cohort differed from the BGR \ndataset, which necessitated a different methodology for \nmapping note-level ambulatory predictions to individual-\nlevel predictions. This was because many individuals in \nthe CP cohort had weekly physical therapy notes with \nidentical note history sections, which cause an influx \nof “no” ambulatory predictions for these individuals. \nTherefore, for individuals in the CP cohort, the mapped \nambulatory prediction response was “yes” if there was at \nleast one note with a “yes” prediction, and the mapped \nresponse was “no” otherwise. For verbal predictions, the \nsame mapping methodology as BGR was used.\nStep 5: Evaluation of pipeline\nThe above analyses were performed on both the BGR and \nCP datasets except, as indicated earlier, only the MCP- \n1.5Y experiment was performed for the CP dataset. The \nfinal functional biomarker status prediction for each indi-\nvidual in the BGR and CP datasets was evaluated against \nthe RNAP-informed ground-truth labels or the CP \nCenter-curated ground-truth labels, respectively, using \nmetrics including average precision, average recall, \nweighted-average F1 score, and macro-averaged F1 \nscores. The weighted-average F1 score is calculated as \n(F 1 − Score\"Yes\"Class) ∗ \"Yes\" Annotations\nTotal Annotations + (F 1 Score− \"No \"Class) ∗ ( \"No \"Annotations\nTotal Annotations) , \nand was chosen to give more weight to classes with more \ndata points, which we felt represented model perfor -\nmance accurately given the imbalance within the BGR \ndataset between ‘yes’ and ‘no’ annotations. This weighted \nscore is calculated as Fig.  2 summarizes the NLP/LLM \npipelines and evaluations applied to the clinical notes \nfrom the BGR and CP cohorts.\nCost analysis\nUtilizing OpenAI’s GPT endpoint is a significant expense, \nas there is a per token cost associated with providing the \nnote and prompt to GPT, as well as a per token cost to \nGPT’s output [ 34]. In the interest of transparency for \nFig. 2 Illustration of GPT project workflow for both cohorts. The pipelines in the photo are repeated for all versions of GPT utilized: GPT- 3.5, GPT- 4 t, \nand GPT- 4o\nPage 8 of 17Kaster et al. Journal of Neurodevelopmental Disorders           (2025) 17:24 \nthose who may want to use GPT for similar tasks, we \nprovide our anticipated and actual costs of Azure ser -\nvices associated with the project in Table  7. To calculate \nour expected costs, we first found the average number \nof characters in our prompt (which includes the pro -\nvided clinical notes) and in GPT’s expected output. Next, \nwe calculated the average token size of our inputs and \noutputs using OpenAI’s estimation of 4 characters per \ntoken, which we used to calculate the cost of each API \ncall. Finally, we multiplied by the total number of notes \nto get the cost of generating a single round of predictions. \nFor the BGR cohort there were two total rounds of pre -\ndictions per GPT version and dataset, consisting of the \nMCP-All and BCP-All experiments. A separate analysis \nwas not needed for MCP- 1.5Y, as the results from the \nmulti-class predictions on all notes could be restricted \nto only notes within 1.5 years of enrollment. For the CP \ncohort, only one round of predictions was needed since a \nbinary-class prediction experiment was not utilized.\nThe steps described above are represented in the \nexpected total cost equation that we used below, which is \nthe total cost for each model (GPT- 3.5, 4 or 4o) for the \nspecific cohort. The results of the cost analysis are found \nin Table 6 in the results.\nFor the verbal and ambulatory status classification \nof notes from the BGR, the N was 3245, the P was 2 \n(MCP-All and BCP-All), and the TpO was 13. For the \nTotal Cost= P ·\n[\nN ·\n( {Avg Input Chars}\n4 · {Input Cost}\n1000 + {Output Cost}\n1000 ·TpO\n)]\nN = # of Notes\n⏐⏐4 = Chars per Token\n⏐⏐1000 = Convert to Cost per single token\nTpO = Expected Tokens Per Output\n⏐⏐P = # Prompts Used Per Model /Note\n⏐⏐\nclassification of notes from the CP cohort the N was \n5,462, the P was 1 (MCP- 1.5Y), and the TpO was 13.\nResults\nDemographics\nAs shown in Table  4, the BGR cohort included 564 indi -\nviduals, of which subsets of 104 and 105 met the inclu -\nsion criteria based on identified clinical progress notes \nand ground-truth RNAP information for patient ambu -\nlatory and verbal status, respectively. These subsets \nencompassed 125 unique individuals whose notes were \nprovided to GPT. These individuals had an average age of \n11.2 years, were a majority male (67.2%), and had a race \nbreakdown of 4% Asian, 6.4% Black or African American, \n73.6% White, 2.4% more than one race, and 13.6% other/\nunknown/not reported. Overall, the final dataset of indi -\nviduals with ground-truth labels consisted of a greater \npercentage of individuals who are white or male than \nthe broader BGR dataset, which is 54.1% male and 67.6% \nWhite. Supplementary Table 2 provides a more detailed \nbreakdown of the BGR cohort demographics, compared \naccording to the ground-truth labels.\nThe CP cohort consisted of 633 individuals, all of \nwhom had clinical notes and ground-truth GMFCS, VSS, \nand CFCS scale annotations mapped to binary output \n(Table 5). After the note inclusion criteria were applied, \nthe cohort consisted of 260 individuals, with 125 able \nand 135 unable to walk without aid, as shown in Supple -\nmentary Table 3, which provides a more detailed demo -\ngraphic breakdown. These individuals had an average age \nof 7.5 years, were 51.9% male, and had a race breakdown \nTable 4 Demographics of BGR and CP cohorts\nDemographics are provided for entire cohorts and just the subsection that the pipelines were applied to. Data include self-reported sex, ethnicity, race, and age\nBGR: All Individuals in \nCohort\nBGR: Individuals in GPT \nDataset\nCP: All Individuals in \nCohort\nCP: Individuals \nin GPT Dataset\nN 564 125 633 260\nAge (sd) 11.4 (9.8) 11.2 (6.1) 8.5 (5.1) 7.53 (4.9)\nSex Female 259 (45.9%) 41 (32.8%) 292 (46.1%) 124 (47.7%)\nMale 305 (54.1%) 84 (67.2%) 337 (53.2%) 135 (51.9%)\nUnknown/Not Reported 0 (0.0%) 0 (0.00%) 4 (0.6%) 1 (0.4%)\nEthnicity Hispanic or Latine 56 (9.9%) 8 (6.4%) N/A N/A\nNot Hispanic or Latine 406 (72.0%) 94 (75.2%) N/A N/A\nUnknown/Not Reported 102 (18.1%) 23 (18.4%) N/A N/A\nRace Asian 25 (4.4%) 5 (4.0%) 12 (1.9%) 7 (2.7%)\nBlack or African American 27 (4.8%) 8 (6.4%) 113 (17.9%) 62 (23.9%)\nMore Than One Race 23 (4.1%) 3 (2.4%) 31 (4.9%) 20 (7.7%)\nOther 19 (3.4%) 1 (0.8%) 12 (1.9%) 5 (1.9%)\nUnknown/Not Reported 88 (15.6%) 16 (12.8%) 24 (3.8%) 10 (3.9%)\nWhite 381 (67.6%) 92 (73.6%) 492 (77.7%) 192 (73.9%)\nPage 9 of 17\nKaster et al. Journal of Neurodevelopmental Disorders           (2025) 17:24 \n \nTable 5 BGR dataset GPT performance\nThe precision, recall, and weighted-average F1 scores are reported across 3 GPT versions (GPT- 3.5, GPT- 4 t, ad GPT- 4o) and for verbal and ambulatory extraction tasks. Additionally, results are provided for each \nexperiment, including the multi-class prediction experiment (MCP-All), the binary class prediction experiment (BCP-All), and the multi-class prediction experiment on notes written within 1.5 years of patient enrollment \nin the BGR (MCP- 1.5Y). The top scoring GPT model for each experiment in the table is bolded, and the top scoring experiment and GPT model combination for each extraction task is underlined\nExtraction Task Ground Truth Labels Experi‑ment GPT 3.5 GPT‑ 4 t GPT‑ 4o\nAvg Precision Avg Recall Weighted \nAvg F1\nAvg Precision Avg Recall Weighted \nAvg F1\nAvg Precision Avg Recall Weighted \nAvg F1\nVerbal Ability RNAP (CARS- 2 or Telehealth \nScreener)\nMCP-All .80 .89 .88 .88 .84 .91 .87 .88 .92\nBCP- All .78 .83 .87 .84 .84 .91 .82 .84 .89\nMCP- 1.5Y .71 .76 .82 .80 .79 .86 .82 .84 .89\nAmbulatory Ability RNAP (GMFCS or M-CHAT-R MCP-All .60 .69 .75 .89 .91 .95 .75 .87 .89\nBCP- All .52 .54 .45 .67 .81 .81 .63 .76 .74\nMCP- 1.5Y .63 .74 .78 .84 .86 .94 .74 .83 .88\nPage 10 of 17Kaster et al. Journal of Neurodevelopmental Disorders           (2025) 17:24 \nTable 6 CP dataset GPT performance\nThe precision, recall, and weighted-average F1 scores are reported across 3 GPT versions (GPT- 3.5, GPT- 4 t, ad GPT- 4o) and for verbal and ambulatory extraction tasks. All extractions were performed using multi-class \nprediction on notes written within 1.5 years of patient enrollment in the registry. For the verbal ability extraction, the extraction results utilizing ground truth labels from both the VSS and CFCS assessments were \nevaluated. The top scoring GPT model ground truth label set is bolded, and the top scoring label and GPT model combination for each extraction task is underlined\nExtraction Task Ground \nTruth Labels\nExperi‑ment GPT 3.5 GPT‑ 4 t GPT‑ 4o\nAvg Precision Avg Recall Weighted \nAvg F1\nAvg Precision Avg Recall Weighted \nAvg F1\nAvg Precision Avg Recall Weighted \nAvg F1\nVerbal Ability VSS MCP- 1.5Y .69 .63 .51 .72 .71 .66 .74 .73 .68\nCFCS MCP- 1.5Y .69 .63 .52 .68 .67 .63 .72 .71 .66\nAmbulatory Ability GMFCS MCP- 1.5Y .73 .75 .69 .86 0.9 .88 .88 .91 .90\nPage 11 of 17\nKaster et al. Journal of Neurodevelopmental Disorders           (2025) 17:24 \n \nof 2.7% Asian, 23.9% Black or African American, 73.9% \nWhite, 7.7% Mixed race, and 5.8% other/unknown.\nGPT classification performance on BGR dataset\nGPT was prompted to answer the questions ‘Can the \nindividual walk without aid?’ and ‘Does the individual use \nany words?’ for all clinical notes in the three note/prompt \nsets, which included the multi-class classification prompt \nfor all identified notes (MCP-All) and the multi-class \nclassification prompt for notes written within 1.5 years \nof enrollment in the BGR (MCP- 1.5Y). These note level \nresponses were mapped to individual-level predictions, \nwhich were then compared to the ground-truth labels as \ndetermined by the RNAP . Model performances are dis -\nplayed in Table  5, representing ambulatory status and \nverbal ability classification respectively. The GPT model \nwith the highest F1 scores for each experiment is high -\nlighted in each table. The performances with macro-aver-\naged F1 scores are available in Supplementary Table 4.\nGPT models were successfully able to complete both \ntasks, with a maximum weighted F1 score of 0.95 being \nachieved for classifying ambulatory status, and a maxi -\nmum score of 0.92 for identifying verbal ability. The high-\nest performing GPT model was question dependent, with \nGPT- 4 t outperforming GPT- 4o at classifying ambula -\ntory ability, whereas GPT- 4o generally outperformed \nGPT- 4 t at identifying ambulatory status. GPT- 3.5 was \nworst performing model for both tasks and across all \nexperimental combinations, achieving a maximum per -\nformance of 0.78 for ambulatory classification and a \nhigher maximum performance of 0.88 for verbal ability \nclassification. Finally, we highlight that performance was \nslightly increased when the entire set of identified clinical \nnotes was provided to models, as opposed to only notes \nwritten within one and a half years of BGR enrollment; \nhowever, this increase was not found to be significant uti-\nlizing the Wilcoxon signed-rank test.\nGPT classification performance on CP dataset\nGPT was applied to the CP dataset and prompted to \nanswer the questions ‘Can the individual walk without \naid?’ (MCP- 1.5Y, GMFCS) and ‘Does the individual use \nany words?’ (MCP- 1.5Y, VSS) and (MCP- 1.5Y, CFCS) \nfor notes written within 1.5 years of ground truth gen -\neration. The performance of GPT at determining verbal \nability in this dataset was evaluated twice, first using the \nVSS as ground-truth labels, and second using CFCS. \nModel performances are displayed in Table  6, represent-\ning both verbal and ambulatory status. The results with \nmacro-averaged F1 scores are available in Supplementary \nTable 5.\nGPT models achieved a maximum weighted F1 of \n0.90 for classifying ambulatory status, and a maximum \nweighted F1 score of 0.68 for classifying verbal status. \nGPT- 4o was the highest performing model, outperform -\ning both GPT- 4 t and GPT- 3.5. GPT- 3.5 had the lowest \nperformance with maximum weighted F1 scores of 0.69 \nand 0.52 for ambulatory and verbal status respectively.\nBGR vs CP GPT performance comparison\nFor ambulatory function prediction, performance on \nthe 1.5Y note cohort between the BGR and CP cohorts \nwas comparable. All GPT models for the CP cohort had \nhigher recall, sensitivity, and macro averaged F1 scores, \nwhile the BGR cohort had higher weighted F1 scores. The \nCP models having higher macro averaged F1 and lower \nweighted F1 scores indicates that CP models had bet -\nter general performance for both positive and negative \nambulatory classes while the BGR models had better per-\nformance on the majority positive ambulatory class. This \nshowcases the high degree of prompt generalizability for \npredicting ambulatory status.\nHowever, for verbal status prediction, GPT perfor -\nmances on the CP cohort fell short of its performance \non the BGR cohort performances. Across the three \nGPT models, for both VSS and CFCS labels, the verbal \nweighted F1 scores on the BGR cohort ranged from 0.17 \nto 0.31 greater than the weighted F1 scores on the CP \ncohort.\nExtraction performance across note types\nTo evaluate the most informative notes for the prediction \nof each functional biomarker, we compared the note-\nlevel GPT predictions for each note type to the individual \nground-truth values. Figure 3 displays this analysis for the \nBGR cohort, demonstrating the correctness of each note \ntype at extracting verbal and ambulatory status, and the \nproportion of the time that GPT predicts ‘unknown’ . We \nfound that for verbal status prediction, the speech ther -\napy and speech notes output non-unknown (‘yes’ or ‘no) \npredictions at the highest prevalence, suggesting they are \nmost informative for this extraction. For the ambulatory \nstatus extraction, the physical therapy notes led to non-\nunknown predictions at the highest prevalence. There \nwas no obvious general relationship between the propor -\ntion of non-unknown predictions by note-type, and the \ncorrectness of these non-unknown predictions.\nSimilar findings are found in the CP cohort (Fig.  4). \nIn this cohort, the note types leading to the lowest por -\ntion of unknown predictions for verbal ability extrac -\ntion were psychology and speech therapy notes. As \nobserved in the BGR cohort, the physical therapy notes \nwere the most informative note type for ambulatory \nability extraction. Though there was no relationship \nbetween proportion of non-unknown predictions and \nthe correctness of predictions, the correctness of the \nPage 12 of 17Kaster et al. Journal of Neurodevelopmental Disorders           (2025) 17:24 \npredictions varied widely. For example, the verbal sta -\ntus predictions originating from the psychology notes \nwere over 90% correct, whereas the non-unknown pre -\ndictions from speech therapy notes were worse than \nrandom guessing.\nFor each note type, this figure displays the proportion \nof correct note-level predictions of the BGR cohort notes \nin which there was a non-unknown (i.e. “yes” or “no”) \nprediction. The full bar length shows the proportion of \nthat note type that had non-unknown predictions. The \ndark grey bars represent the incorrect note-level predic -\ntions with light grey bars representing the correct note-\nlevel predictions. The note level GPT prediction was \nconsidered correct if the prediction matched the label for \nFig. 3 Proportion and correctness of non-unknown GPT note-level BGR predictions\nFig. 4 Proportion and correctness of non-unknown GPT note-level CP predictions\nPage 13 of 17\nKaster et al. Journal of Neurodevelopmental Disorders           (2025) 17:24 \n \nthe corresponding individual. This figure was generated \nusing the MCP- 1.5Y (notes within 1.5 years enrollment \nand multi-class classification) experiment and GPT- 4o. \nOut of the 14 relevant note types identified, only note \ntypes with a sample size of at least 50 notes of that type in \nour dataset are displayed in this figure.\nFor each note type, this figure displays the proportion \nof correct note-level predictions of the CP cohort notes \nin which there was a non-unknown (i.e. “yes” or “no”) \nprediction. The full bar length shows the proportion of \nthat note type that had non-unknown predictions. The \ndark grey bars represent the incorrect note-level predic -\ntions with light grey bars representing the correct note-\nlevel predictions. The note level GPT prediction was \nconsidered correct if the prediction matched the label for \nthe corresponding individual. This figure was generated \nusing the MCP- 1.5Y (notes within 1.5 years enrollment \nand multi-class classification) experiment and GPT- 4o. \nOnly note types with a sample size of at least 50 predic -\ntions are included in this figure.\nCost analysis\nWe performed a cost estimate for each model and dataset \ncombination using the equation described in the meth -\nods, showing that GPT- 4 t was expected to be the most \nexpensive analysis, followed by GPT- 4o, and then GPT- \n3.5 (Table 7).\nWe calculated that the total cost of performing the \nextraction across all prompts was $252.63 for the BGR \ndataset and $208.10 for the CP cohort, leading to a \ntotal cost of $460.73. This cost encompassed utilization \nof 3 different GPT versions on two different datasets of \n> 3,000 clinical notes, demonstrating that clinical infor -\nmation extraction can be accurately performed for mul -\ntiple iterations on thousands of notes at reasonable price \npoint. However, recall that only a pre-specified subset of \nthe participants EHR deemed most relevant to the topic \narea was used for this analysis.\nDiscussion\nLongitudinal assessment of functional ability is highly \nrelevant to disease diagnosis and management, but the \nadministration of standardized assessments can pose a \nsubstantial burden on patients, caregivers, and provid -\ners. For example, many of the reference standard meas -\nurements, such as the ADOS or Bayley, take too long to \nbe administered in the clinic, and could potentially be \nshortened or replaced by LLM models applied to clini -\ncally acquired data. In this study, we created an AI pipe -\nline to predict functional biomarkers, specifically verbal \nand ambulatory ability, and tested its performance in two \nindependent cohorts, one comprising individuals with \ngenetic causes of IDD (the BGR), and the other compris -\ning individuals with CP . Our results demonstrate that an \nautomated approach for predicting communication and \nambulatory abilities from passively accrued EHR data, \nwhich is acquired during routine clinical care documen -\ntation, may provide a solution for extracting functional \nbiomarkers for use in research studies and diverse clinical \napplications. For example, knowledge of the functional \nstatus of a patient is essential for tracking disease pro -\ncesses in single patients and groups of patients, as well \nas their response to therapy over time. Potential research \nuses for a rigorously evaluated and highly developed \nfunctional biomarker extraction pipeline include natu -\nral history studies for clinical trial readiness, particularly \nfor rare diseases where patients may not all receive care \nat the same institution. Automated data extraction for \nfunctional outcomes may also yield particular benefits in \nresource-limited environments due to the passive nature \nof data collections.\nWhen implementing our analysis pipeline on the BGR \nand CP cohorts, we used consistent criteria for note \nTable 7 GPT cost analysis\nCalculated costs of all GPT extractions performed on both the BGR and CP datasets, broken down by GPT version\nDataset GPT Version Average Note + Prompt \nLength (Chars)\nInput Cost (Per 1,000 \nTokens)\nOutput Cost (Per 1,000 \nTokens)\nExpected \nTotal Cost\nBGR GPT- 3.5 8583 $.0005 $.0015 $7.09\nGPT- 4 t 9933 $.01 $.03 $163.69\nGPT- 4o 9933 $.005 $.015 $81.85\nCalculated Total Cost of GPT- 3.5, GPT- 4 t, and GPT- 4o $252.63\nCP GPT- 3.5 9676 $.0005 $.0015 $6.71\nGPT- 4 t 9676 $.01 $.03 $134.26\nGPT- 4o 9676 $.005 $.015 $67.13\nCalculated Total Cost of GPT- 3.5, GPT- 4 t, and GPT- 4o $208.10\nCalculated Total Cost of All Analyses on Both Datasets $460.73\nPage 14 of 17Kaster et al. Journal of Neurodevelopmental Disorders           (2025) 17:24 \nselection and analysis, such as including only notes of \na sufficient length from specific author specialties, and \nexcluding individuals with too few notes. We also pro -\nvided the LLMs with the same prompts to elicit predic -\ntions on whether the individuals could “walk without \naid” and “use words” . We found that for both cohorts \nthe LLMs were highly successful at predicting the abil -\nity to “walk without aid” , but were more successful at \npredicting the ability to “use words” for the BGR cohort \nthan the CP cohort. The consistent performance on the \nambulatory ability prediction task across both cohorts \nis likely attributed to low ambiguity in the terminology \nused to describe ambulatory ability. Additionally, both \ncohorts utilize the GMFCS as a major component of the \nground truth values, limiting divergence between prompt \nextracted ambulatory ability and our ground truth labels. \nContrasting this, ground truth assessments of communi -\ncation were different in the BGR and CP cohorts which \nmade it difficult to use a single prompt to assess com -\nmunication, leading to the varying performances across \ncohorts. The verbal ability prompt assessing use of words \nis more closely aligned to the ground truth communi -\ncation assessment for the BGR cohort, as the CARS- 2 \nexplicitly asks if the individual is using words (“Is the \nperson you are rating using words?” and the telehealth-\nscreener addresses the amount of words an individual \nuses (single words, sentences, etc.), which are both eas -\nily aligned to our question “does the individual use any \nwords?” Contrasting this, the communication assessment \nin the CP cohort uses the VSS or CFCS. Of note, the VSS \nassesses the degree to which motor impairment affects an \nindividual’s ability to produce oral speech and the CFCS \ndescribes an individual’s ability to both given and receive \ninformation to familiar and unfamiliar individuals. In this \ncontext, the ability to “use words” could be interpreted in \nmultiple different ways including the ability to produce \nsingle words using oral motor speech or using a com -\nmunication device or using signs. Future assessments of \ncommunication will likely require much more nuanced \ndefinitions of verbal ability that are customized to match \nthe ground-truth assessments available for each clinical \ncohort.\nTaken together, our results indicate that the LLM \npipeline we developed is broadly generalizable, but that \ncohort-specific changes such as note inclusion criteria \nto maximize information content and minimize noise, \nand prompt engineering for clinical-relevance, may \nrequire performance optimization. Additionally, extrac -\ntion methodology may need to be modified to account \nfor cohort differences. For example, the CP cohort note \ntemplate included identical note history sections for \nsome patients receiving weekly physical therapy, which \nbiased toward “no” responses to the ambulatory ability \nextraction question. This necessitated modifying the \nguidelines for mapping from note-level predictions to \nindividual level predictions of ambulatory ability. Despite \nthe need for considerations relating to cohort-specific \nchanges, success of our LLM pipelines at extracting ver -\nbal and ambulatory ability from BGR clinical notes indi -\ncates that LLMs have the potential to build generalizable \nclinical extraction pipelines across multiple institutions \nwith variable note-taking practices, and may therefore be \nhugely valuable for rare disease research.\nIn our LLM pipeline we mapped from predictions \nacross various time points to a single individual-level \nprediction, which we recognized could be influenced \nby changes in patient functional abilities over time. To \nreduce the potential for this bias, we only included notes \nwritten when the individual was at least 3 years old (cap -\nturing individuals who have passed certain develop -\nmental stages), and we performed an analysis on notes \nwritten within 1.5 years of ground truth ascertainment \nacross both cohorts. Despite these steps, we acknowledge \nthat we may not have been able to control for all effects of \nindividual development and/or improvement in ambula -\ntory and verbal abilities.\nOne limitation of our study is the rapid evolution of \nLLMs. The LLMs we used were deployed behind the \nHIPAA-compliant firewalls of Washington University’s \nAzure tenant. While OpenAI does not commonly release \ntraining and detailed model information, GPT- 3.5 was \nreleased in November 2022 and is presumed to be the \nsmallest in total parameter size and count of tokens in \nthe training data compared to subsequent models. In \nMarch 2023, GPT- 4 was released and lauded for its nota-\nble increase in capability over GPT- 3.5. This first model \nreleased in the 4-series showed an ability to handle com -\nplex tasks and instructions with much improved perfor -\nmance [35–37]. Later, in November 2023, GPT- 4 T was \nreleased, and GPT- 4o was released in May 2024. Train -\ning of GPT- 4 T and GPT- 4o became more advanced and \nresulted in more capable models, both showing similar \nstate of the art performances on several LLM text genera-\ntion benchmarks, better than GPT- 4 [38].\nWe also note that incomplete representation for demo-\ngraphic groups may introduce unintentional bias to the \npipelines and prompts, and impact the generalizability of \nthe model to diverse populations. Although we included \ncohorts with representation across all demographic \ngroups, the BGR included fewer individuals identifying \nas Black or African American (6.4%), Hispanic or Latine \n(6.4%), and as having mixed ancestry (2.4%) compared to \nthe U.S. census population [39]. The CP cohort had bet -\nter representation of female and Black or African Ameri -\ncan individuals, but the proportion of Hispanic or Latine \nidentifying individuals was not recorded. If the pipelines \nPage 15 of 17\nKaster et al. Journal of Neurodevelopmental Disorders           (2025) 17:24 \n \nin this study are to be utilized for clinical decision mak -\ning, then additional investigation may be needed to \nensure that there is no bias in the performance against \nunderrepresented groups.\nCost analysis showed that the resources required to \nuse GPT for automated biomarker identification are \nreasonable, albeit dependent on the chosen model and \nselective inclusion of notes. The cost of all LLM-based \nexperiments is larger than that of alternative rule-based \nand machine learning methods, which may have little or \nno monetary cost; however, these other methods have \na much higher time–cost associated. Future work will \nfurther examine the scalability of this work by explor -\ning other open-source LLMs such as Llama, Mixtral, and \nFLAN that can be implemented with existing local com -\nputational resources, thus avoiding per-token costs as \nwell as circumventing data privacy concerns.\nA final limitation of our study was the potential bias \nof our two study cohorts, which were derived from \nacademic institutions with likely higher incidences \nof functional deficits. Therefore, they may have more \ninformative notes compared to IDD patients who have \nnot had the same level of access to healthcare. Future \nplans include application of our pipeline to other popu -\nlations, such as a general pediatric population (e.g. from \nprimary care pediatricians), to determine if clinical notes \nfrom non-specialist care are sufficiently informative to \npredict functional ability. We also plan to determine \nif the model can predict complex information, such as \nthe age at which individuals showed progress or decline \nin verbal and ambulatory ability, and more detailed and \ngranular information such as the individual GMFCS lev -\nels rather than binary predictions. Application of these \ntechniques show promise for informing the need for \nstandardized data collection for natural history studies, \nas well as which assessments may best capture clinically \nmeaningful change for clinical trials.\nConclusions\nIn summary, here we demonstrate the successful design \nand application of LLM tools for extracting functional \nbiomarker data from extant EHR data to make clinically \nmeaningful predictions of verbal and ambulatory abil -\nity. We show that the questions ‘Can the individual walk \nwithout aid?’ and ‘Does the individual use any words?’ \ncan be answered with good, though not perfect, fidelity \nfrom EHR data in two separate cohorts of patients with \nIDD or CP . We found that GPT- 4 t and GPT- 4o were \nsuperior to GPT- 3.5, with minor differences in perfor -\nmance between GPT- 4 t and GPT- 4o, and all can be \naccomplished at reasonable cost, which may be fur -\nther reduced by translating these methods to open-\nsource LLMs. The low-cost ability to extract functional \nbiomarkers with LLM tools has extensive clinical and \nresearch applications, including generation of geno -\ntype–phenotype correlations, assessment of therapeutic \ninterventions or harmful exposures, and identification \nof at-risk patients who may benefit from targeted treat -\nments or therapies. Further development and optimiza -\ntion of LLM tools for extracting functional biomarkers \noffers an exciting opportunity to utilize the wealth of \nEHR data to efficiently advance research and clinical care \nfor patients with IDDs as well as the population-at-large.\nAbbreviations\nIDDs  Intellectual and Developmental Disabilities\nCP  Cerebral Palsy\nLLMs  Large Language Models\nBGR  Brain Gene Registry\nEHR  Electronic Health Record\nAI  Artificial Intelligence\nNLP  Natural Language Processing\nRNAP  Rapid Neurobehavioral Assessment Protocol\nGPT  Generative Pre-trained Transformed\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s11689- 025- 09612-w.\nAdditonal file 1.\nAcknowledgements\nMembers of the Brain Gene Registry Consortium:\nPrincipal Investigators: Wasserstein,  M1; Chopra,  M2; Sahin,  M2; Wangler,  M3; \nSchultz,  B4; Izumi,  K4; Bergner  S5, Gropman,  A5,15; Smith-Hicks,  C6; Abbeduto, \n L7; Hazlett,  H8; Doherty  D9; German,  K9; DaWalt,  L10; Neul,  J11; Constantino,  J12; \nPayne, PRO.13\nCo-Investigators: Gurnett,  C13; Baldridge  D13; Srivastava,  S2; Molholm,  S1; Walk-\nley,  S1; Storch,  E3; Samaco,  R3; Cohen,  J6; Shankar,  S7; Piven, J.8\nClinical Coordinating Center: Mahida,  S2; Sveden,  A2; Dies, K.2\nClinGen Genome Connect Team: Riggs,  ER14; Savatt, JM.14\nProgram Management and CIELO Team: Lanzotti, V 13; Oh,  I13; Gupta,  A13; \nMinor,  B13\nAffiliations\n1. Albert Einstein College of Medicine\n2. Boston Children’s Hospital\n3. Baylor College of Medicine\n4. Children’s Hospital of Philadelphia\n5. Children’s National Medical Center\n6. Kennedy Krieger Institute\n7. UC Davis MIND institute\n8. University of North Carolina, Chapel Hill\n9. University of Washington\n10. Waisman Center, University of Wisconsin-Madison\n11. Vanderbilt University Medical Center\n12. Emory University\n13. Washington University in St. Louis\n14. Autism and Developmental Medicine Institute, Geisinger, Danville PA\n15. St. Jude Hospital and Research Center\nAuthors’ contributions\nConceptualization was completed by LK, IO, BA, CG, and AG. The manuscript \nwas drafted by LK, EH, IO, CG, VL, CV, and AG. Manuscript review and editing \nwas completed by LK, EH, IO, BA, VL, CV, BGR, CG, PP , and AG. Data analysis \nand pipeline development performed by LK and EH. Project supervision \nconducted by CA, PP , and AG. Regular discussion surrounding project direc-\ntion completed by LK, EH, IO, VL, CA, and AG. Clinical expertise provided by \nBA, VL, CV, and CG. Review-writing, editing, enrollment, assessment, and data \nPage 16 of 17Kaster et al. Journal of Neurodevelopmental Disorders           (2025) 17:24 \ngeneration performed by BGR consortium. The authors read and approved \nthe final manuscript.\nFunding\nThis research was supported by the IDDRC-CTSA Brain Gene Registry grant, \nU01 TR002764, from the National Center for Advancing Translational Sciences \n(NCATS) and National Institute of Child Health and Human Development \n(NICHD) of the National Institutes of Health (NIH).\nData availability\nThe Brain Gene Registry dataset utilized in this article is available at https:// \nbrain gener egist ry. wustl. edu/. The Cerebral Palsy dataset utilized is not publicly \naccessible.\nDeclarations\nEthics approval and consent to participate\nNo participants were recruited specifically for this study. This work constitutes \nsecondary use of data approved by the Washington University in St. Louis IRB \n(protocols #202010013 [Brain Gene Registry cohort] and #202309003 [cerebral \npalsy cohort]).\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nAuthor details\n1 Institute for Informatics, Data Science and Biostatistics, Washington University \nSchool of Medicine in St. Louis, St. Louis, MO, USA. 2 Department of Neurol-\nogy, Washington University School of Medicine in St. Louis, St. Louis, MO, USA. \n3 Department of Psychiatry, Washington University School of Medicine in St. \nLouis, St. Louis, MO, USA. \nReceived: 8 November 2024   Accepted: 12 April 2025\nReferences\n 1. Biomarkers Definitions Working G. Biomarkers and surrogate endpoints: \npreferred definitions and conceptual framework. Clin Pharmacol Ther. \n2001;69(3):89–95. https:// doi. org/ 10. 1067/ mcp. 2001. 113989. \n 2. Jensen K, Soguero-Ruiz C, Oyvind Mikalsen K, et al. Analysis of free text in \nelectronic health records for identification of cancer patient trajectories. \nSci Rep. 2017;7:46226. https:// doi. org/ 10. 1038/ srep4 6226.\n 3. Kho AN, Pacheco JA, Peissig PL, et al. Electronic medical records for \ngenetic research: results of the eMERGE consortium. Sci Transl Med. \n2011;3(79):79re1. https:// doi. org/ 10. 1126/ scitr anslm ed. 30018 07.\n 4. Wei WQ, Teixeira PL, Mo H, Cronin RM, Warner JL, Denny JC. Combining \nbilling codes, clinical notes, and medications from electronic health \nrecords provides superior phenotyping performance. J Am Med Inform \nAssoc. 2016;23(e1):e20-7. https:// doi. org/ 10. 1093/ jamia/ ocv130.\n 5. Morris MA, Kho AN. Silence in the EHR: infrequent documentation \nof aphonia in the electronic health record. Bmc Health Serv Res. \n2014;14:Artn 425.https:// doi. org/ 10. 1186/ 1472- 6963- 14- 425.\n 6. Balas EA, Boren SA. Managing clinical knowledge for health care improve-\nment. Yearb Med Inform. 2000;1:65–70.\n 7. Morris MA, Hamer MK, Eberle K, Jensen KM, Wong AA. Implementation of \ncollection of patients’ disability status by centralized scheduling. Jt Comm \nJ Qual Patient Saf. 2021;47(10):627–36. https:// doi. org/ 10. 1016/j. jcjq. 2021. \n05. 007.\n 8. Morris MA, Lagu T, Maragh-Bass A, Liesinger J, Griffin JM. Development of \npatient-centered disability status questions to address equity in care. Jt \nComm J Qual Patient Saf. 2017;43(12):642–50. https:// doi. org/ 10. 1016/j. \njcjq. 2017. 06. 011.\n 9. U.S Department of Health and Human Services Office of Minority Health. \nData collection standards for race, ethnicity, sex, primary language, and \ndisability status. 2024. https:// minor ityhe alth. hhs. gov/ omh/ browse. aspx? \nlvl= 3& lvlid= 53. Accessed 5 June 2024.\n 10. Benedict RE, Patz J, Maenner MJ, et al. Feasibility and reliability of \nclassifying gross motor function among children with cerebral palsy \nusing population-based record surveillance. Paediatr Perinat Epidemiol. \n2011;25(1):88–96. https:// doi. org/ 10. 1111/j. 1365- 3016. 2010. 01164.x.\n 11. Oh IY, Schindler SE, Ghoshal N, Lai AM, Payne PRO, Gupta A. Extraction of \nclinical phenotypes for Alzheimer’s disease dementia from clinical notes \nusing natural language processing. JAMIA Open. 2023;6(1):ooad014. \nhttps:// doi. org/ 10. 1093/ jamia open/ ooad0 14.\n 12. Cui L, Sahoo SS, Lhatoo SD, et al. Complex epilepsy phenotype extrac-\ntion from narrative clinical discharge summaries. J Biomed Inform. \n2014;51:272–9. https:// doi. org/ 10. 1016/j. jbi. 2014. 06. 006.\n 13. Gholipour M, Khajouei R, Amiri P , Hajesmaeel Gohari S, Ahmadian L. \nExtracting cancer concepts from clinical notes using natural language \nprocessing: a systematic review. BMC Bioinformatics. 2023;24(1):405. \nhttps:// doi. org/ 10. 1186/ s12859- 023- 05480-0.\n 14. Huang J, Yang DM, Rong R, et al. A critical assessment of using ChatGPT \nfor extracting structured data from clinical notes. NPJ Digit Med. \n2024;7(1):106–106. https:// doi. org/ 10. 1038/ s41746- 024- 01079-8.\n 15. Bhattarai K, Oh IY, Sierra JM, et al. Leveraging GPT-4 for identify-\ning cancer phenotypes in electronic health records: a performance \ncomparison between GPT-4, GPT-3.5-turbo, Flan-T5, Llama-3–8B, and \nspaCy’s rule-based and machine learning-based methods. JAMIA Open. \n2024;7(3):ooae060-ooae060. https:// doi. org/ 10. 1093/ jamia open/ ooae0 \n60.\n 16. Radford A, Narasimhan K, Salimans T, Sutskever I. Improving language \nunderstanding by generative pre-training. 2018.\n 17. Alsentzer E, Murphy JR, Boag W, et al. Publicly available clinical BERT \nembeddings. 2019.https:// doi. org/ 10. 48550/ arxiv. 1904. 03323.\n 18. Rosenbaum P , Paneth N, Leviton A, et al. A report: the definition and \nclassification of cerebral palsy April 2006. Dev Med Child Neurol Suppl. \n2007;109:8–14.\n 19. Baldridge D, Kaster L, Sancimino C, et al. The Brain Gene Registry: a data \nsnapshot. J Neurodev Disord. 2024;16(1):ARTN 17. https:// doi. org/ 10. \n1186/ s11689- 024- 09530-3.\n 20. Chopra M, Savatt JM, Bingaman TI, et al. Clinical variants paired \nwith phenotype: a rich resource for brain gene curation. Genet \nMed.2024;26(3):ARTN 101035. https:// doi. org/ 10. 1016/j. gim. 2023. 101035.\n 21. Kirkpatrick BE, Riggs ER, Azzariti DR, et al. GenomeConnect: matchmak-\ning between patients, clinical laboratories, and researchers to improve \ngenomic knowledge. Hum Mutat. 2015;36(10):974–8. https:// doi. org/ 10. \n1002/ humu. 22838.\n 22. Savatt JM, Azzariti DR, Faucett WA, et al. ClinGen’s GenomeConnect regis-\ntry enables patient-centered data sharing. Hum Mutat. 2018;39(11):1668–\n76. https:// doi. org/ 10. 1002/ humu. 23633.\n 23. Walkley SU, Abbeduto L, Batshaw ML, et al. Intellectual and developmen-\ntal disabilities research centers: fifty years of scientific accomplishments. \nAnn Neurol. 2019;86(3):332–43. https:// doi. org/ 10. 1002/ ana. 25531.\n 24. Hidecker MJ, Paneth N, Rosenbaum PL, et al. Developing and validating \nthe communication function classification system for individuals with \ncerebral palsy. Dev Med Child Neurol. 2011;53(8):704–10. https:// doi. org/ \n10. 1111/j. 1469- 8749. 2011. 03996.x.\n 25. Morris C, Bartlett D. Gross motor function classification system: impact \nand utility. Dev Med Child Neurol. 2004;46(1):60–5. https:// doi. org/ 10. \n1017/ s0012 16220 40001 18.\n 26. Palisano R, Rosenbaum P , Walter S, Russell D, Wood E, Galuppi B. Devel-\nopment and reliability of a system to classify gross motor function in \nchildren with cerebral palsy. Dev Med Child Neurol. 1997;39(4):214–23. \nhttps:// doi. org/ 10. 1111/j. 1469- 8749. 1997. tb074 14.x.\n 27. Rosenbaum PL, Walter SD, Hanna SE, et al. Prognosis for gross motor \nfunction in cerebral palsy: creation of motor development curves. JAMA. \n2002;288(11):1357–63. https:// doi. org/ 10. 1001/ jama. 288. 11. 1357.\n 28. Pennington L, Virella D, Mjoen T, et al. Development of The Viking Speech \nScale to classify the speech of children with cerebral palsy. Res Dev \nDisabil. 2013;34(10):3202–10. https:// doi. org/ 10. 1016/j. ridd. 2013. 06. 035.\n 29. Virella D, Pennington L, Andersen GL, et al. Classification systems of \ncommunication for use in epidemiological surveillance of children with \ncerebral palsy. Dev Med Child Neurol. 2016;58(3):285–91. https:// doi. org/ \n10. 1111/ dmcn. 12866.\nPage 17 of 17\nKaster et al. Journal of Neurodevelopmental Disorders           (2025) 17:24 \n \n 30. Schopler E, Van Bourgondien ME, Wellman GJ, Love SR. Childhood Autism \nRating Scale-Second Edition (CARS-2). Western Psychological Services: \nLos Angeles; 2010.\n 31. Rosenbaum PL, Palisano RJ, Bartlett DJ, Galuppi BE, Russell DJ. Develop-\nment of the gross motor function classification system for cerebral palsy. \nDev Med Child Neurol. 2008;50(4):249–53. https:// doi. org/ 10. 1111/j. 1469- \n8749. 2008. 02045.x.\n 32. Robins DL, Casagrande K, Barton M, Chen CMA, Dumont-Mathieu T, Fein \nD. Validation of the Modified Checklist for Autism in Toddlers, Revised \nwith Follow-up (M-CHAT-R/F). Pediatrics. 2014;133(1):37–45. https:// doi. \norg/ 10. 1542/ peds. 2013- 1813.\n 33. Robins DL, Fein D, Barton ML, Green JA. The Modified Checklist for \nAutism in Toddlers: an initial study investigating the early detection of \nautism and pervasive developmental disorders. J Autism Dev Disord. \n2001;31(2):131–44. https:// doi. org/ 10. 1023/A: 10107 38829 569.\n 34. Microsoft Azure. Azure OpenAI service pricing. 2024. https:// azure. \nmicro soft. com/ en- us/ prici ng/ detai ls/ cogni tive- servi ces/ openai- servi \nce/. Accessed 27 June 2024.\n 35. Bhattarai K, Oh IY, Sierra JM, et al. Leveraging GPT-4 for identify-\ning cancer phenotypes in electronic health records: a performance \ncomparison between GPT-4, GPT-3.5-turbo, Flan-T5, Llama-3–8B, and \nspaCy’s rule-based and machine learning-based methods. JAMIA Open. \n2024;7(3):ooae060. https:// doi. org/ 10. 1093/ jamia open/ ooae0 60.\n 36. Nakajima N, Fujimori T, Furuya M, et al. A comparison between GPT-3.5, \nGPT-4, and GPT-4V: can the large language model (ChatGPT) Pass \nthe Japanese Board of Orthopaedic Surgery Examination? Cureus. \n2024;16(3):e56402. https:// doi. org/ 10. 7759/ cureus. 56402.\n 37. Meyer A, Riese J, Streichert T. Comparison of the Performance of GPT-3.5 \nand GPT-4 With That of Medical Students on the Written German Medi-\ncal Licensing Examination: Observational Study. JMIR Med Educ. Feb 8 \n2024;10:e50965. https:// doi. org/ 10. 2196/ 50965.\n 38. OpenAI. OpenAI Simple-Evals. GitHub repository. 2024.\n 39. Bureau USC. QuickFacts United States. https:// www. census. gov/ quick \nfacts/ fact/ table/ US/ PST04 5222. Accessed 10/31/2024.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations."
}