{
  "title": "Automated Extraction of Molecular Interactions and Pathway Knowledge using Large Language Model, Galactica: Opportunities and Challenges",
  "url": "https://openalex.org/W4385572540",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2337019770",
      "name": "Gilchan Park",
      "affiliations": [
        "Brookhaven National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2217079036",
      "name": "Byung jun Yoon",
      "affiliations": [
        "Brookhaven National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2604570367",
      "name": "Xihaier Luo",
      "affiliations": [
        "Brookhaven National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A5092596373",
      "name": "Vanessa Lpez-Marrero",
      "affiliations": [
        "Brookhaven National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2502314801",
      "name": "Patrick Johnstone",
      "affiliations": [
        "Brookhaven National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2132315744",
      "name": "Shinjae Yoo",
      "affiliations": [
        "Brookhaven National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A3004896594",
      "name": "Francis Alexander",
      "affiliations": [
        "Brookhaven National Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2159482845",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W2601884081",
    "https://openalex.org/W4385573087",
    "https://openalex.org/W4320024282",
    "https://openalex.org/W2120261416",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W3107527779",
    "https://openalex.org/W3116802215",
    "https://openalex.org/W4313527179",
    "https://openalex.org/W1992045522",
    "https://openalex.org/W2897805491",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2964348125",
    "https://openalex.org/W2527801679"
  ],
  "abstract": "Gilchan Park, Byung-Jun Yoon, Xihaier Luo, Vanessa Lpez-Marrero, Patrick Johnstone, Shinjae Yoo, Francis Alexander. The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks. 2023.",
  "full_text": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks, pages 255–264\nJuly 13, 2023 ©2023 Association for Computational Linguistics\nAutomated Extraction of Molecular Interactions and Pathway Knowledge\nusing Large Language Model, Galactica: Opportunities and Challenges\nGilchan Park∗, Byung-Jun Yoon, Xihaier Luo, Vanessa López-Marrero,\nPatrick Johnstone, Shinjae Yoo, Francis J. Alexander\nComputational Science Initiative, Brookhaven National Laboratory, Upton, NY\n{gpark,byoon,xluo,vlopezmar,pjohnston,sjyoo,falexander}@bnl.gov\nAbstract\nUnderstanding protein interactions and path-\nway knowledge is essential for comprehend-\ning living systems and investigating the mech-\nanisms underlying various biological func-\ntions and complex diseases. While numerous\ndatabases curate such biological data obtained\nfrom literature and other sources, they are not\ncomprehensive and require considerable effort\nto maintain. One mitigation strategies can be\nutilizing large language models to automati-\ncally extract biological information and explore\ntheir potential in life science research. This\nstudy presents an initial investigation of the\nefficacy of utilizing a large language model,\nGalactica in life science research by assess-\ning its performance on tasks involving pro-\ntein interactions, pathways, and gene regula-\ntory relation recognition. The paper details\nthe results obtained from the model evalua-\ntion, highlights the findings, and discusses the\nopportunities and challenges. The code and\ndata are available at: https://github.com/\nboxorange/BioIE-LLM\n1 Introduction\nA significant portion of contemporary molecular\nbiology research is dedicated to studying and com-\nprehending the roles and interactions of the count-\nless proteins that form the fundamental building\nblocks of life. The prediction of protein structures\nand functions is essential in addressing crucial chal-\nlenges in life science, including developing thera-\npeutic solutions for various diseases. By speeding\nup drug discovery and development, such advance-\nments could significantly enhance healthcare. The\nmajority of proteins have undefined functions, and\nonly a fraction of them have been unequivocally\nidentified through arduous and intensive labora-\ntory research. These established protein functions\n∗Corresponding author.\nare used as a benchmark to predict functions com-\nputationally by analyzing DNA and amino acid\nsequence homology across the continuously grow-\ning repository of protein sequences obtained from\ngenome sequencing. To deeply understand pro-\ntein functions, protein interaction information can\nbe crucial, and many databases such as STRING,\nKEGG, IntAct, BioGrid, DIP, and HPRD have been\nestablished to gather and maintain pathway anal-\nysis and regulatory results obtained by lab exper-\niments and from the scientific literature. Regret-\ntably, extracting information from the existing lit-\nerature demands significant manual labor and is a\ntime-consuming process. One viable solution to\nremedy this is to leverage efficient machine learn-\ning models that can accurately recognize such in-\nformation in scientific texts.\nIn recent years, large language models (LLMs)\nhave gained significant attention in the natural lan-\nguage processing (NLP) field owing to their ca-\npability to execute complex language tasks, their\nflexibility, and their potential to generate responses\nsimilar to humans (Brown et al., 2020; Zhao et al.,\n2023). Their application in various domains and\ntasks, including knowledge extraction from texts,\nhas yielded promising outcomes (Agrawal et al.,\n2022). Our study aims to investigate the potential\nof LLMs in extracting pathway knowledge, protein\ninteraction, and gene regulatory information. In\nthis study, we have assessed the capability of Galac-\ntica (Taylor et al., 2022), a general-purpose scien-\ntific LLM, to accomplish these biological tasks.\nAlthough Galactica did not yield optimal outcomes\nin our biology-related tasks, it exhibited the capac-\nity to identify specific genes/proteins, pathways,\nand their interactions. Our preliminary findings re-\ngarding this evaluation are presented in this paper.\n2 Related Work\nThe field of biology encompasses challenging tasks\nsuch as identifying protein-protein interactions\n255\n(PPIs) and gene regulatory relations. Additionally,\npathway analysis is a crucial area of study as it\ndocuments the interactions between proteins and\nreflects important molecular biological processes,\nsuch as metabolic, signaling, protein interaction,\nand gene regulation processes. Research such as\nthat for expression-based disease diagnosis (Lee\net al., 2008; Gatza et al., 2010) and the identifi-\ncation of disease markers (Khunlertgit and Yoon,\n2016) suggests that tasks based on pathway activ-\nity can be more stable than tasks based solely on\ngenes. The scientific literature of biological sci-\nences serves as an important repository of essential\nknowledge that has yet to be effectively discovered.\nTo address this, NLP models based on deep neural\nnetworks have been widely adopted for analysis of\nstructural properties of proteins (Vig et al., 2020),\nPPIs (Peng and Lu, 2017; Park et al., 2022), and\npathway analysis (Casaní-Galdón et al., 2020).\nSeveral studies showed that LLMs performed\ncomparably to traditional neural network mod-\nels that necessitate labeled training data and fine-\ntuning processes, resulting in significant time and\neffort savings while providing a universal model\ncapable of managing multiple tasks simultaneously\n(Kojima et al., 2022; Yuan and Liu, 2022). The\nGalactica LLM (Taylor et al., 2022) has been\ntrained on a massive amount of scientific literature\nand has successfully tackled biological understand-\ning task such as sequence validation perplexity,\nfunctional keyword prediction, protein function de-\nscription. Hence, we aim to further examine the\npotential of Galactica in the domain of biological\nscientific knowledge.\n3 Experiments\nWe investigated the potential of Galactica for ad-\ndressing biological tasks related to PPIs, pathway\nknowledge, and gene regulatory relations. To ac-\ncomplish this, STRING, KEGG, INDRA databases\nwere adopted, and the details pertaining to the\nutilized data for these tasks can be found in Ap-\npendix A. In the context of a LLM, the proper\nselection of the number of examples or shots is es-\nsential to ensure efficient engineering. For this pur-\npose, an ablation study was conducted to identify\nthe optimal number of shots for each task. The shot\nnumber associated with the highest performance\nin test samples was selected for implementation,\nas detailed in Appendix B. Additionally, prompt\nconstruction is another critical factor that merits\n1K 10K\nGalactica (6.7B) 0.166 0.161\nTable 1: STRING Task 1 - Precision for the generated\nbinding proteins for 1K/10K protein samples.\n1K 10K 100K\nGalactica (6.7B) 0.552 0.558 0.562\nTable 2: STRING Task 2 - Micro F-scores for randomly\nselected 1K/10K/100K positive and negative protein\npairs (I.e., 1K = 500 pos + 500 neg).\nattention, and the prompts tested for each task are\nlisted in Appendix C. The experimental setup is\ndetailed in Appendix D.\n3.1 Recognizing Protein-Protein Interactions\nWe evaluated the Galactica on protein binding in-\nformation recognition on a human protein network\nfrom the STRING DB. Specifically, we employed\nthe model to produce a list of proteins that bind to\na designated protein, as part of the generative ques-\ntion task (STRING Task 1: generative question).\n<Predicted answer by model>\nQuestion: Which proteins are related to TBC1D9?\nAnswer: TBC1D8, TBC1D14, TBC1D7, TBC1D5,\nTBC1D6, TBC1D\n<Actual answer>\nAnswer: TBC1D8, TBC1D14, TBC1D7, TBC1D5,\nPLK5, MYO16\nTo assess performance, we randomly selected\n1,000 and 10,000 samples from the network for test-\ning. The generated binding proteins were matched\nwith the proteins in the network with an approxi-\nmately 0.16 precision as described in Table 1. The\nresults of the prediction analysis indicated that the\nmodel exhibited a tendency to generate words pri-\nmarily from the initial letters of a given protein.\nConsequently, the accuracy of the predictions was\nconsiderably high for proteins with similar names,\nsuch as IKZF4 and RFC5, while a significant mis-\nmatch between predicted and actual binding pro-\nteins was observed in cases where dissimilar pro-\ntein names were involved, such as DNAJC10 and\nTRIP11. The details of those examples are pro-\nvided in Appendix E.\nSubsequently, we tested the model’s recognition\nof protein binding relationships in a binary setting,\nwhich was formulated as a yes/no question to deter-\nmine if two proteins bind to each other (STRING\nTask 2: yes/no question).\n256\nFigure 1: Confusion matrices for STRING Task 2(The observed disparity between the total number of samples and\nthe sum of values in the confusion matrix can be attributed to the omission of responses other than ’yes’ or ’no’.)\nTask1 pairs† Consistency‡\nGalactica (6.7B) 0.691 0.726\nTable 3: STRING Task 2 - Precision for the protein\npairs used in STRING Task 1. †All positive protein\npairs. ‡Model prediction consistency between Task1\nand Task2.\n<Predicted answer by model>\nQuestion: Are CHEK2 and BRCA2 related to each\nother?\nAnswer: yes\n<Actual answer>\nAnswer: yes\nTo create negative protein binding pairs, we\nutilized unconnected pairs from the human pro-\ntein network. The experiment was conducted on\n1K/10K/100K protein pairs, and the model exhib-\nited F-scores slightly exceeding 0.5 in its predic-\ntions as described in Table 2. In order to evaluate\nthe consistency of the model’s predictions across\nTask1 and Task2, we conducted an assessment of\nSTRING Task 2using the identical protein pairs\nemployed in STRING Task 1. These pairs exclu-\nsively consisted of positive instances. More specif-\nically, our examination focused on whether the\nmodel successfully generated protein A associated\nwith protein B and correctly classified their rela-\ntionship as ’yes’. To this end, we tested STRING\nTask 2using the 1K protein pairs used in STRING\nTask 1, and the model performed STRING Task\n2 on all positive protein pairs and those generated\ncorrectly by the models in STRING Task 1. The\nevaluation results and the confusion matrices are\ndescribed in Table 3 and Figure 1. The model ex-\nhibited variability in its predictive performance be-\ntween STRING Task 1and STRING Task 2(0.73).\nAlthough this precision (0.73) is slightly higher\nthan the precision of all positive protein pairs used\nin STRING Task 1, this level of precision aligns\nclosely with the precision observed for 50K ran-\ndom positive pairs (0.74 = 36,991 true positives /\n50,000 true positive + false positives). This implies\nthat while the model successfully made some accu-\nrate predictions for proteins in generative questions,\nit encountered challenges when identifying protein\nrelations in the binary format prompts.\n3.2 KEGG Pathway Recognition\nGalactica was assessed for its ability to recognize\ngenes related to human pathways associated with\nlow-dose radiation exposure in KEGG. The task\ninvolved generating a list of genes belonging to the\ntop 20 human pathways linked to low-dose radi-\nation exposure (KEGG Task 1: generative ques-\ntion).\n<Predicted answer by model>\nQuestion: Which genes are involved in \"Adherens\njunction\"?\nAnswer: CDH1, CTNNA3, CTNNB1, CTNNA1,\nCTNNA2, CTNNA8, CTNNA15\n<Actual answer>\nAnswer: CDH1, CTNNA3, CTNNB1, CTNNA1,\nCTNNA2, TGF1a, MEKK7\nThe model accurately predicted the genes that\nbelong to the pathways with about precision 0.26\nas presented in Table 4, which outperformed the\nprevious STRING Task 1generative test. Our spec-\nulation is that the model’s superior performance in\nrecognizing low-dose radiation-related pathways\ncompared to proteins might be attributed to the\nfact that pathway names related to low-dose radi-\nation exposure are typically mentioned in specific\nsections or categories, whereas protein names are\n257\nFigure 2: Confusion matrices for KEGG Task 2.\nPathways\nGalactica (6.7B) 0.256\nTable 4: KEGG Task 1 - Precision for the generated\ngenes that belong to the top 20 pathways relevant to\nlow-dose radiation exposure.\n35,174 gene and pathway pairs\nGalactica (6.7B) 0.562\nTable 5: KEGG Task 2 - Micro F-scores for all positive\nand negative pairs (35,174 = 17,587 pos + 17,587 neg).\nTask1 pairs† Consistency‡\nGalactica (6.7B) 0.883 0.917\nTable 6: KEGG Task 2 - Precision for the gene-pathway\npairs used in KEGG Task 1. †All positive gene-pathway\npairs. ‡Model prediction consistency between Task1\nand Task2.\nmore commonly found in a wider range of topic\npapers. This suggests that searching for informa-\ntion in a well-defined collection of data may yield\nmore precise results than searching for information\nderived from ambiguous inputs in heterogeneous\nsources. The outcome of the prediction analysis\nshowed that the genes produced for a particular\npathway exhibited comparable patterns, a finding\nwhich had also been observed in the priorSTRING\nTask 1experiment. Examples of this can be found\nin Appendix F.\nWe performed yes/no questions for pathways\nand genes relation recognition ( KEGG Task 2:\nyes/no question). Similar to the STRING Task\n2, we used member genes in other pathways as neg-\native samples for a given pathway if they do not\nappear in the pathway. The model was evaluated on\nall positive relations (+ randomly chosen negative\nrelations) and the relations used in KEGG Task 1\nto measure the model consistency between Task1\nand Task2.\n<Predicted answer by model>\nQuestion: Are \"DP beta 1\" and \"Type I diabetes\nmellitus\" related to each other?\nAnswer: yes\n<Actual answer>\nAnswer: yes\nThe results and the confusion matrices are dis-\nplayed in Table 5, Table 6, and Figure 2 respec-\ntively. The model achieved an F-score of approxi-\nmately 0.56 when making predictions for both pos-\nitive and negative gene-pathway pairs. The preci-\nsion of 0.92 for the model consistency signifies the\nmodel’s reliable predictive performance. Moreover,\nthe higher score achieved for the generated posi-\ntive relationships (0.92) in contrast to the scores for\nall positive relationships in the pathways (0.78 =\n13,628 / 17,552) and Task 1 pairs (0.88) suggests\nthat the model possesses a greater level of compre-\nhension concerning specific pathways compared to\nothers.\nIn the STRING Task 2and KEGG Task 2, the\nmodel’s responses to yes/no questions utilizing pos-\nitive and negative samples skewed more towards\npositive, as illustrated by the leftmost confusion\nmatrix in Figures 1 and 2. A plausible explana-\ntion for this outcome is the likelihood of erroneous\nnegative relationships in the negative samples. For\ninstance, among the negative samples is the rela-\ntionship between the gene \"HD1\" and the pathway\n\"Adherens junction\" despite the fact that they are\ngenuinely connected.\n3.3 Evaluating Gene Regulatory Relations\nFinally, we examined Galactica’s ability to recog-\nnize human gene regulatory relations using data\nfrom the INDRA DB. Unlike the previous datasets,\n258\n2 3 4 5 6\nGalactica 6.7B 0.704 0.605 0.567 0.585 0.597\nTable 7: INDRA Task - Micro F-scores with 1K samples\nfor each class. See Appendix G for class names.\nINDRA statements not only provide relation enti-\nties but also text snippets from research papers. We\nused these text snippets as contextual information\nabout regulatory relations to generate questions for\nthe model. The task involved asking the model to\nselect the correct relationship between two genes\nfrom multiple relation classes in a given text (IN-\nDRA Task: multiple-choice question). This task\nserves as an evaluation of the model’s reading com-\nprehension skills specifically related to gene regu-\nlatory relation texts.\n<Predicted answer by model>\nUpon binding with Shh, Ptc1 inactivation al-\nlows Smo to initiate signaling XREF_BIBR,\nXREF_BIBR, XREF_BIBR through the Gli fam-\nily of transcription factors.\nQuestion: Given the options: \"Activation\", \"Inhi-\nbition\", \"Phosphorylation\", \"Dephosphorylation\",\n\"Ubiquitination\", \"Deubiquitination\", which one is\nthe relation type between Ptc1 and Smo in the text\nabove?\nAnswer: Activation\n<Actual answer>\nAnswer: Activation\nTo construct multiple-choice questions, we iden-\ntified the six most frequently occurring classes in\nthe dataset and utilized two to six of them for\nchoices. The model was assessed using 1K sam-\nples for each class, and the findings are detailed\nin Table 7 and Figure 3. With the escalation of\nproblem complexity due to the increased number\nof choices, the model initially encountered difficul-\nties in identifying correct answers. Nonetheless,\nit exhibited improved performance in six-choice\nproblems compared to cases involving four or five\nchoices. When examining the results of yes/no\nquestions in STRING Task 2and KEGG Task 2(≈\n0.56) using the two-class F-score (0.70) in INDRA\nTask, it becomes evident that the model possesses a\nmore consistent ability to recognize entity relations\nwithin contexts as compared to extracting infor-\nmation through straightforward questioning. This\nobservation suggests that incorporating contextual\ninformation in questions could potentially enhance\nthe model’s predictive capabilities.\nFigure 3: Confusion matrix of 6 choice questions for\nINDRA Task.\n4 Discussion and Conclusion\nThis study evaluated the performance of Galactica\non various biological tasks using different types of\ndatabase resources, including PPIs, pathway knowl-\nedge, and regulatory relations. Based on our find-\nings, it was observed that the model faced greater\ndifficulty in answering questions that contained\nlimited information, in contrast to questions that\nprovided contextual details and were more specific.\nWhile the Galactica did not produce ideal results\nin our biology-related tasks, we observed that the\nmodel demonstrated the ability to recognize certain\ngenes and proteins and their interactions. Despite\nthis, our approach presents a potential avenue for\nusing the model, and we anticipate that our find-\nings will assist domain scientists and researchers\nin employing the model for their applications and\nobtaining insights into the model’s behavior based\non experimental outcomes.\n5 Work in progress\nOur study suggests that the model can show better\nperformances when contextual text is provided. We\nwill consider forging a chain of queries to gener-\nate context for the datasets not having supporting\ntext (See Appendix H). We found that the model’s\nperformance was largely affected by prompts. This\nneeds to be further investigated. We plan to evalu-\nate other LLMs such as GPT series, LLaMA, and\nAlpaca by comparing with smaller sized domain-\nspecifically trained models such as BioGPT and\nBioMedLM in biological tasks.\n259\nLimitations\nThere are a few limitations for our work. First,\nto assess the model’s ability to recognize negative\nprotein-protein and gene-pathway pairs, we used\nunconnected pairs in the datasets. However, the\nnegative pairs have not been proved, which might\ncontain authentic or potential positive connections.\nWe continuously search for truly negative gene/pro-\ntein pairs established by experiments and research.\nSecond, this work only reports the assessment of\nthe Galactica standard model (6.7B). The evalua-\ntion of the other models (mini: 125 M, base: 1.3 B,\nlarge: 30 B, huge: 120 B) remains as future works.\nIn-depth studies on the model inference will be also\nfollowed in our future works such as clustering of\nPPIs and genes belonging to pathways predicted by\nthe model.\nAcknowledgements\nThis work is supported by the U.S. Department of\nEnergy, Office of Science, RadBio program under\nAward KP1601011/KP1601017/FWP CC121.\nReferences\nMonica Agrawal, Stefan Hegselmann, Hunter Lang,\nYoon Kim, and David Sontag. 2022. Large language\nmodels are few-shot clinical information extractors.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1998–2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSalvador Casaní-Galdón, Cecile Pereira, and Ana\nConesa. 2020. Padhoc: a computational pipeline for\npathway reconstruction on the fly. Bioinformatics,\n36(Supplement_2):i795–i803.\nMichael L Gatza, Joseph E Lucas, William T Barry,\nJong Wook Kim, Quanli Wang, Matthew D Crawford,\nMichael B Datto, Michael Kelley, Bernard Mathey-\nPrevot, Anil Potti, et al. 2010. A pathway-based\nclassification of human breast cancer. Proceedings\nof the National Academy of Sciences, 107(15):6994–\n6999.\nBenjamin M Gyori, John A Bachman, Kartik Subrama-\nnian, Jeremy L Muhlich, Lucian Galescu, and Peter K\nSorger. 2017. From word models to executable mod-\nels of signaling networks using automated assembly.\nMolecular systems biology, 13(11):954.\nMinoru Kanehisa and Susumu Goto. 2000. Kegg: kyoto\nencyclopedia of genes and genomes. Nucleic acids\nresearch, 28(1):27–30.\nMinoru Kanehisa, Yoko Sato, Miho Furumichi, Kanae\nMorishima, and Mao Tanabe. 2019. New approach\nfor understanding genome variations in kegg. Nu-\ncleic acids research, 47(D1):D590–D595.\nNavadon Khunlertgit and Byung-Jun Yoon. 2016. In-\ncorporating topological information for predicting\nrobust cancer subnetwork markers in human protein-\nprotein interaction network. In BMC bioinformatics,\nvolume 17, pages 143–152. Springer.\nHyunwoong Ko. 2021. Parallelformers: An efficient\nmodel parallelization toolkit for deployment. https:\n//github.com/tunib-ai/parallelformers.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nEunjung Lee, Han-Yu Chuang, Jong-Won Kim, Trey\nIdeker, and Doheon Lee. 2008. Inferring pathway\nactivity toward precise disease classification. PLoS\ncomputational biology, 4(11):e1000217.\nXihaier Luo, Sean McCorkle, Gilchan Park, Vanessa\nLópez-Marrero, Shinjae Yoo, Edward R Dougherty,\nXiaoning Qian, Francis J Alexander, and Byung-Jun\nYoon. 2022. Comprehensive analysis of gene expres-\nsion profiles to radiation exposure reveals molecular\nsignatures of low-dose radiation response. In 2022\nIEEE International Conference on Bioinformatics\nand Biomedicine (BIBM), pages 2366–2374. IEEE.\nGilchan Park, Sean McCorkle, Carlos Soto, Ian Blaby,\nand Shinjae Yoo. 2022. Extracting protein-protein\ninteractions (ppis) from biomedical literature using\nattention-based relational context information. In\n2022 IEEE International Conference on Big Data\n(Big Data), pages 2052–2061. IEEE.\nYifan Peng and Zhiyong Lu. 2017. Deep learning for ex-\ntracting protein-protein interactions from biomedical\nliterature. BioNLP 2017, page 29.\nDamian Szklarczyk, Annika L Gable, Katerina C Nas-\ntou, David Lyon, Rebecca Kirsch, Sampo Pyysalo,\nNadezhda T Doncheva, Marc Legeay, Tao Fang, Peer\nBork, et al. 2021. The string database in 2021: cus-\ntomizable protein–protein networks, and functional\ncharacterization of user-uploaded gene/measurement\nsets. Nucleic acids research, 49(D1):D605–D612.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085.\n260\nJesse Vig, Ali Madani, Lav R Varshney, Caiming Xiong,\nNazneen Rajani, et al. 2020. Bertology meets biol-\nogy: Interpreting attention in protein language mod-\nels. In International Conference on Learning Repre-\nsentations.\nBoshi Wang, Xiang Deng, and Huan Sun. 2022. Itera-\ntively prompt pre-trained language models for chain\nof thought. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning, pages 2714–2730.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nWeizhe Yuan and Pengfei Liu. 2022. restructured pre-\ntraining. arXiv preprint arXiv:2206.11147.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223.\nA Data Description\nSTRING DB (Szklarczyk et al., 2021): The\npresent study employed the human (Homo sapiens)\nprotein network for performing a protein-protein in-\nteraction (PPI) recognition task. The network was\nconstructed based on the STRING (Search Tool\nfor the Retrieval of Interacting Genes/Proteins)\ndatabase, which is a comprehensive biological\nrepository and online resource for both predicted\nand confirmed protein interactions. The database\nintegrates data from a range of sources, including\nexperimental studies, computational prediction\nmethods, and publicly available text collections.\nThe human network encompasses 19,566 proteins\nand 5,968,680 protein bindings.\nKEGG DB(Kanehisa and Goto, 2000): The Kyoto\nEncyclopedia of Genes and Genomes (KEGG)\nis a set of databases encompassing a wide range\nof biological information, including genomic\ndata, disease information, chemical compounds,\nand biological pathways. It houses a staggering\ncollection of over 28,000 complete genomes,\nencompassing a diverse range of organisms.\nFurthermore, it hosts an expansive repertoire of\nmore than 500 pathways, meticulously curated\nand annotated to illuminate the intricate web\nof molecular interactions that govern various\nbiological processes. Moreover, the database\nincludes approximately 5 million reference genes,\nproviding researchers with invaluable resources\nfor gene-centric investigations (Kanehisa et al.,\n2019). The KEGG pathways contain molecular\ninteractions and reactions, which are designed to\nlink genes in the genome to gene products (mostly\nproteins) in biological pathways. The focus of\nour investigation pertains to the pathways within\nthe human body that are affected by exposure\nto low-dose ionizing radiation, which remains a\nsignificant threat to human health and is not yet\nfully comprehended. To explore this topic, we\nutilized the KEGG human pathways which have\nbeen identified as being activated in response to\nlow-dose radiation exposure in a recent study (Luo\net al., 2022).\nINDRA DB (Gyori et al., 2017): The Inte-\ngrated Network and Dynamical Reasoning\nAssembler (INDRA) is a tool that facilitates\nthe integration of information regarding causal\nmechanisms into a unified format suitable for\nthe construction of a variety of predictive and\nexplanatory models. In the field of molecular\nbiology, sources of mechanistic information\ninclude pathway databases, textual descriptions of\nmechanisms generated by human curators, and\ninformation extracted from the scientific literature\nthrough text mining. The INDRA platform stream-\nlines this information by removing duplicates,\nstandardizing the data, and organizing it into a set\nof Statements accompanied by associated evidence.\nBy collating information from multiple sources\nin this manner, INDRA enables researchers to\nbuild robust models for exploring the complex\nmolecular mechanisms underlying biological\nsystems. The present study utilized a set of human\ngene regulatory relation statements that represent\nmechanistic interactions between biological agents.\nThe dataset comprises a total of 4,258,718 distinct\nstatements.\nB Ablation study on the number of shots\n1. STRING Task 1: We randomly drew 1K and\n10K samples out of the STRING DB human\nprotein network for testing, and the generated\nbinding proteins corresponded to the proteins\nin the human network the most with 1-shot\nprompting as seen in Table 8\n2. STRING Task 2: We evaluated 1K samples\n(500 true cases + 500 false cases) randomly\ndrawn from the STRING DB human pro-\ntein network with different number of prompt\n261\nFigure 4: Confusion matrices for KEGG Pathway Recognition Task 2 ablation study.\n0-shot 1-shot 2-shot 3-shot\nPrecision 1K 0.127 0.166 0.145 0.135\n10K 0.130 0.144 0.140 0.137\nTable 8: Precision of different shots with 1K/10K sam-\nples for STRING Task 1 using a human protein network\nfrom STRING DB.\nshots. Here, N-shot indicates the combination\nof N number of true samples and N number of\nfalse samples (e.g., 1-shot: 1 true + 1 false (to-\ntal 2 samples)). The result showed that 3-shot\nprompt performed the best in Table 9.\n3. KEGG Pathway Recognition Task 1: We as-\nsessed human pathways associated with low-\ndose radiation exposure in KEGG DB with\ndifferent number of shots, and 1-shot prompt-\ning showed the best performance as described\nin Table 10.\n4. KEGG Pathway Recognition Task 2: We eval-\nuated 1K samples (500 true cases + 500 false\ncases) randomly drawn from human pathways\nassociated with low-dose radiation exposure\nin KEGG DB with different number of prompt\nshots. Here, N-shot indicates the combination\nof N number of true samples and N number of\nfalse samples (e.g., 1-shot: 1 true + 1 false (to-\ntal 2 samples)). The result showed that 1-shot\nprompt performed the best in Table 11.\n5. Evaluating Gene Regulatory Relations Task:\nWe tested different shots with 400 samples for\n4 classes (100 Activation + 100 Inhibition +\n100 Phosphorylation + 100 Dephosphoryla-\ntion) from INDRA DB, and 2-shot prompting\nshowed the best performance on the multiple\nchoice task in Table 12.\n0-shot 1-shot 2-shot 3-shot\nMicro F1 0.515 0.552 0.543 0.590 †\nTable 9: Micro F-scores of different shots with 1K sam-\nples for STRING Task 2 using a human protein network\nfrom STRING DB. †Due to the high false positive rate,\n1-shot prompting was adopted.\n0-shot 1-shot 2-shot 3-shot\nPrecision 0.170 0.259 0.221 0.209\nTable 10: Precision of different shots for KEGG Path-\nway Recognition Task 1.\n0-shot 1-shot 2-shot 3-shot\nMicro F1 0.489 0.564 0.534 0.501\nTable 11: Micro F-scores of different shots with 1K\nsamples for KEGG Pathway Recognition Task 2.\n0-shot 1-shot 2-shot 3-shot\nMicro F1 0.370 0.508 0.610 0.560\nTable 12: Micro F-scores of different shots with 400\nsamples (100 Activation + 100 Inhibition + 100 Phos-\nphorylation + 100 Dephosphorylation) choice for Eval-\nuating Gene Regulatory Relations Task using INDRA\nDB.\n262\nC Tested Prompts\nSTRING Task 1:\n1. \"Which proteins are bound to x?\"\n2. \"What proteins are bound to x?\"\n3. \"What proteins are bound to x?\"\n4. \"What proteins does x bind to?\"\n5. \"To what proteins does x bind?\"\n6. \"Which proteins are related to x?\"\nSTRING Task 2:\n1. \"Do the two proteins \"x\" and \"y\" bind each\nother?\"\n2. \"Do the two proteins x and y bind each other?\nTrue or False\"\n3. \"Do the two proteins x and y bind to each\nother? True or False\"\n4. \"Do x and y bind each other? True or False\"\n5. \"Does x bind to y? True or False\"\n6. \"Do x and y bind to each other? True or False\"\n7. \"Are x and y related to each other? True or\nFalse\"\n8. \"Are x and y related to each other?\"\n9. \"Are x and y related to each other? yes or no\"\n10. \"x and y are related to each other. Is this\nstatement True or False?\"\n11. \"x and y are related to each other.\"\n12. \"Given the options: \"Related\", \"Unrelated\",\nwhich one is the relation type between x and\ny?\"\nKEGG Pathway Recognition Task 1:\n1. \"Which genes are involved in \"x\"?\"\n2. \"Which genes are involved in x?\"\n3. \"Which genes are related to x?\"\n4. \"Which proteins are related to x?\"\n5. \"Which genes or proteins are related to x?\"\n6. \"Which genes/proteins are related to x?\"\nKEGG Pathway Recognition Task 2:\n1. \"Are x and y related to each other?\"\n2. \"Are \"x\" and \"y\" related to each other?\"\n3. \"Is x related to y?\"\n4. \"Is x related to the pathway y?\"\n5. \"Is x involved in y?\"\n6. \"Is \"x\" involved in \"y\"?\"\n7. \"Is x involved in the human pathway y?\"\n8. \"Is x involved in the KEGG pathway y?\"\n9. \"Does \"y\" have \"x\"?\nEvaluating Gene Regulatory Relations Task:\n1. \"Which of the following is the relation type\nbetween x and y in the text above?\"\n2. \"Which of the following is the relation type\nbetween \"x\" and \"y\" in the text above?\"\n3. \"Which of the following is the relation be-\ntween \"x\" and \"y\" in the text above?\"\n4. \"Given the options: z, which one is the re-\nlation type between \"x\" and \"y\" in the text\nabove?\"\n5. \"Given the options: z, which one is the rela-\ntion type between x and y in the text above?\"\nD Experimental Setup\nWe used the Galactica standard model with 6.7\nbillion parameters. The experiments were con-\nducted on 8 ×NVIDIA V100 GPUs, and we ex-\nploited Galactica’s option for model tensor paral-\nlelizm based on Parallelformers (Ko, 2021) when\nthe machine has enough memories, which signifi-\ncantly increases task processing time (about twice\nfaster). The model processed a batch sized input\nfor a task, which is the number of prompts to infer\n(I.e., the number of input texts for model genera-\ntion at once). The batch sizes for the tasks are as\nfollows.\n• STRING Task 1 (generative question): 16, 32\n• STRING Task 2 (yes/no question): 32, 64\n• KEGG Task 1 (generative question): 16, 32\n• KEGG Task 2 (yes/no question): 32, 64\n• INDRA Task (multiple choice question): 4, 8\nE STRING Task 1 examples\nQuestion: Which proteins are related to IKZF4?\nAnswer (pred): IKZF1, IKZF2, IKZF3, IKZF5\nAnswer (true): IKZF1, IKZF2, IKZF3, IKZF5\n263\nQuestion: Which proteins are related to RFC5?\nAnswer (pred): RFC1, RFC2, RFC3, RFC4, RFC5\nAnswer (true): RFC1, RFC2, RFC3, RFC4,\nERCC1\nQuestion: Which proteins are related to USP32?\nAnswer (pred): USP54, USP41, USP42, USP34,\nUSP38, USP50, USP52, USP32, USP55, USP56, ...\nAnswer (true): USP54, USP41, USP42, USP34,\nUSP38, USP50, CACNA1H, ACTC1, DHX32,\nMAGI3, ...\nQuestion: Which proteins are related to\nDNAJC10?\nAnswer (pred): DNAJC10K, DNAJC10O,\nDNAJC10D, DNAJC10Q, DNAJC10E ...\nAnswer (true): ARL5B, MATN3, KPNA4,\nSRSF10, MTHFD2, ...\nQuestion: Which proteins are related to TRIP11?\nAnswer (pred): TRIP32, TRIP31, TRIP20,\nTRIP23, TRIP25, ...\nAnswer (true): RIC1, GOLGA8J, ARL5B,\nTMTC4, KIF5A, ...\nF KEGG Pathway Recognition Task 1\nexamples\nQuestion: Which genes are involved in “Cellular\nsenescence”?\nAnswer (pred): CDKN2A, CDKN2B, CDKN1A,\nCDKN1D, CDKN1D, CDKN1B, CDKN1C\nAnswer (true): CDKN2A, CDKN2B, CDKN1A,\np107, major histocompatibility complex, mitogen-\nactivated protein kinase 14, P44MAPK\nQuestion: Which genes are involved in “Phago-\nsome”?\nAnswer (pred): RAB5A, RAB7A, RAB47,\nRAB41, RAB48, RAB39B, RAB50 ...\nAnswer (true): RAB5A, RAB7A, ATP6G,\nCD51, HEL-S-62p, phospholipase A2 receptor\n1, ATP6EL2, ...\nQuestion: Which genes are involved in “Proteogly-\ncans in cancer”?\nAnswer (pred): CD63, CD284, CD282, CD44,\nCD166, CD276, CD278, CD81, CD55, ...\nAnswer (true): CD63, CD284, CD282, CD44,\nSJS1, G17P1, GAB1, PLCE1, HPSE1, ...\nQuestion: Which genes are involved in “Autoim-\nmune thyroid disease”?\nAnswer (pred): TSHR, TSH\nAnswer (true): TSHR, hTSHR-I\nG INDRA DB class names\nTable 13 displays the name of classes in INDRA\nDB statements used in the Evaluating Gene Regu-\nlatory Relations Task.\n# Choices Classes\n2 class Activation, Inhibition\n3 class Activation, Inhibition, Phosphorylation\n4 class Activation, Inhibition, Phosphorylation,\nDephosphorylation,\n5 class Activation, Inhibition, Phosphorylation,\nDephosphorylation, Ubiquitination,\n6 class Activation, Inhibition, Phosphorylation,\nDephosphorylation, Ubiquitination, Deu-\nbiquitination\nTable 13: The class names used in the multiple choice\nquestion for Evaluating Gene Regulatory Relations Task\nusing INDRA DB.\nH A chain of query example\nTo provide a model with context information about\na query, we plan to apply an iterative prompting for\na chain of thought (CoT) development (Wei et al.,\n2022; Wang et al., 2022). An example is illustrated\nbelow.\nQuestion: what is “Natural killer cell mediated\ncytotoxicity”?\n-> The answer of this query can be a context of the\nfollowing query.\nQuestion: Which genes are involved in “Natural\nkiller cell mediated cytotoxicity”?\nAnswer: V A V3, NFAT5, HCST, CHP1, SH2D1B,\nRAET1E\n264",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7125157117843628
    },
    {
      "name": "Extraction (chemistry)",
      "score": 0.42760422825813293
    },
    {
      "name": "Natural language processing",
      "score": 0.3939511775970459
    },
    {
      "name": "Chemistry",
      "score": 0.07970654964447021
    },
    {
      "name": "Chromatography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200870766",
      "name": "Brookhaven National Laboratory",
      "country": "US"
    }
  ]
}