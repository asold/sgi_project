{
  "title": "InstOptima: Evolutionary Multi-objective Instruction Optimization via Large Language Model-based Instruction Operators",
  "url": "https://openalex.org/W4389518623",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2097531526",
      "name": "Heng Yang",
      "affiliations": [
        "University of Exeter"
      ]
    },
    {
      "id": "https://openalex.org/A1908799446",
      "name": "Ke Li",
      "affiliations": [
        "University of Exeter"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4312522056",
    "https://openalex.org/W2126105956",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4365563505",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4367701241",
    "https://openalex.org/W4378465006",
    "https://openalex.org/W4379548460",
    "https://openalex.org/W4386566520",
    "https://openalex.org/W4281493460",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4378508502",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W2251648804",
    "https://openalex.org/W3198571508",
    "https://openalex.org/W4386729930",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3084992427",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W3176648901",
    "https://openalex.org/W3185341429"
  ],
  "abstract": "Instruction-based language modeling has received significant attention in pretrained language models. However, the efficiency of instruction engineering remains low and hinders the development of instruction studies. Recent studies have focused on automating instruction generation, but they primarily aim to improve performance without considering other crucial objectives that impact instruction quality, such as instruction length and perplexity. Therefore, we propose a novel approach (i.e., InstOptima) that treats instruction generation as an evolutionary multi-objective optimization problem. In contrast to text edition-based methods, our approach utilizes a large language model (LLM) to simulate instruction operators, including mutation and crossover. Furthermore, we introduce an objective-guided mechanism for these operators, allowing the LLM to comprehend the objectives and enhance the quality of the generated instructions. Experimental results demonstrate improved fine-tuning performance and the generation of a diverse set of high-quality instructions.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 13593–13602\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nInstOptima: Evolutionary Multi-objective Instruction Optimization via\nLarge Language Model-based Instruction Operators\nHeng Yang1, Ke Li1\n1Department of Computer Science, University of Exeter, EX4 4QF, Exeter, UK\n{hy345, k.li}@exeter.ac.uk\nAbstract\nInstruction-based language modeling has re-\nceived significant attention in pretrained lan-\nguage models. However, the efficiency of in-\nstruction engineering remains low and hinders\nthe development of instruction studies. Re-\ncent studies have focused on automating in-\nstruction generation, but they primarily aim\nto improve performance without considering\nother crucial objectives that impact instruction\nquality, such as instruction length and perplex-\nity. Therefore, we propose a novel approach\n(i.e., InstOptima) that treats instruction gen-\neration as an evolutionary multi-objective op-\ntimization problem. In contrast to text edition-\nbased methods, our approach utilizes a large\nlanguage model (LLM) to simulate instruction\noperators, including mutation and crossover.\nFurthermore, we introduce an objective-guided\nmechanism for these operators, allowing the\nLLM to comprehend the objectives and en-\nhance the quality of the generated instructions.\nExperimental results demonstrate improved\nfine-tuning performance and the generation of\na diverse set of high-quality instructions.\n1 Introduction\nWith the rapid development of language mod-\nels (Ouyang et al., 2022; Touvron et al., 2023; Ope-\nnAI, 2023), instructions (also known as prompts)\nplay a crucial role in instruction-based language\nmodeling, and different instructions may lead to\nsignificant differences in model outputs (Zhou\net al., 2022; Honovich et al., 2022; Wan et al.,\n2023). For instance, even slightly perturbed in-\nstructions (e.g., synonym substitutions (Wang et al.,\n2021; Zhou et al., 2021) or adversarial attacks (Wan\net al., 2023; Zhu et al., 2023)) can result in un-\nexpectedly low performance. However, there are\nthree problems regarding instruction-based learn-\ning that still need to be addressed in existing works.\nFirstly, existing works (Lester et al., 2021; Gu\net al., 2022; Zhou et al., 2022, 2023; Li et al.,\n2023; Chen et al., 2023) aim to obtain a large num-\nber of instructions through automated instruction\ngeneration to filter high-performance instructions.\nHowever, due to the large and non-differentiable\ntextual search space (Ishibashi et al., 2023; Cho\net al., 2023), the automated instruction generation\nand instruction engineering methods (Brown et al.,\n2020; Liu et al., 2023) are inefficient and strug-\ngle to search for various high-quality instructions.\nSecondly, the objectives of instruction generation\nare not clear. Current research (Lester et al., 2021;\nGu et al., 2022; Pitis et al., 2023) regards perfor-\nmance (i.e., metrics) as the sole criterion for in-\nstruction quality. However, model performance\nalone cannot precisely explain instruction quality.\nWe propose to refine instruction quality by consid-\nering fine-grained objectives, such as length and\nperplexity. Shorter instructions can lower computa-\ntional costs, especially for large-scale models and\ndatasets. Lower perplexity indicates that instruc-\ntions are more easily understood by language mod-\nels. Lastly, the diversity of instructions has been\nneglected in existing studies, while increasing the\ndiversity of instructions can mitigate adversarial\nattacks (Wan et al., 2023; Zhu et al., 2023) and im-\nprove instruction robustness (Yu et al., 2022; Zhu\net al., 2023). We aim to obtain multiple alternative\ninstructions based on multi-objective optimization,\nwhich can facilitate comprehensive evaluation of\ninstructions.\nTo address these three problems, we formulate\nthe task as an evolutionary multi-objective op-\ntimization problem and propose our framework\ncalled InstOptima. We leverage a large lan-\nguage model, specifically ChatGPT (OpenAI,\n2023), to facilitate instruction operations such as\nmutation and crossover. Furthermore, we intro-\nduce an objective-guided mechanism to assist the\nlanguage model in generating high-quality instruc-\ntions. In terms of optimization objectives for in-\nstruction generation, InstOptima incorporates\n13593\nthree objectives: performance (metrics), length,\nand perplexity, enabling the exploration of a di-\nverse and high-quality set of instructions. We adopt\nNSGA-II (Deb et al., 2002) in InstOptima to\nobtain a Pareto front of instruction sets.\nTo validate the efficacy of InstOptima, we\nconducted experiments on three generation-based\nclassification tasks. The experimental results in-\ndicate that InstOptima can concurrently obtain\na diverse set of instructions that outperform the\ncounterparts regarding performance.\nIn summary, our contributions are as follows:\n• We simulate instruction operators based on an\nLLM. We also show that the objective-guided\noperators help the LLM understand optimization\nobjective values and improve instruction quality.\n• We divide the orientation of instruction search\ninto multiple objectives, such as performance,\nlength, and perplexity, facilitating fine-grained\ncontrol over instruction quality.\n• We utilize a multi-objective optimization algo-\nrithm to automatically search for a set of high-\nquality instructions, which could benefit defend-\ning against adversarial attacks and improving in-\nstruction robustness.\nThe codes are available at: https://github.\ncom/yangheng95/InstOptima.\n2 Proposed Method\nIn this section, we first introduce the instruction-\nbased text generation, followed by the details of\nInstOptima.\n2.1 Instruction-based Generation\nIn text generation-based tasks1, instructions are uti-\nlized to facilitate in-context learning (Brown et al.,\n2020) and improve language modeling. An instruc-\ntion (depicted in the right part of Fig. 1) is repre-\nsented as I = Concat(d, e), where d and e are\nthe definition and example of the target task, re-\nspectively. d and e are token sequences similar to\n(x, y) ∼D, where x, y, and Ddenote the input,\noutput, and task dataset, respectively. The mod-\neling of a generation model f(·, ·) is defined as\nfollows:\nˆy = f(x, I) (1)\nwhere ˆy represents the generated output given\nx and I. In InstOptima, we aim to address\n1We validate InstOptima generation-based text classi-\nfication, and InstOptima can be easily applied to other\ninstruction-based modeling tasks.\nthe problem of automated instruction generation\nthrough multi-objective optimization.\n2.2 Evolutionary Instruction Optimization\nThe workflow of InstOptima is illustrated in\nFig. 1. We begin by initializing a parent population\nof instructions to start evolving. The parent popu-\nlation is manipulated by LLM-based operators to\ngenerate offspring. Subsequently, we employ the\nnon-dominated sort algorithm to rank the combined\npopulation and measure the crowdness of instruc-\ntions. At the end of each generation, we randomly\nreplace some Pareto-front instructions with new\ninstructions to enhance the diversity of the popula-\ntion (referred to as genes in NSGA-II). We also\nprovide the pseudo code of the InstOptima in\nAppendix A.4.\n2.2.1 Operators for Instructions\nTo handle the non-differentiable text search space,\nwe formulate these operators as a text genera-\ntion task based on ChatGPT. In other words,\nwe define a set of fixed prompts ˜P, ˜P =\n{˜Pdm, ˜Pdc, ˜Pem, ˜Pec}, to guide ChatGPT in per-\nforming the instructions, where ˜Pdm, ˜Pdc, ˜Pem, ˜Pec\nare the fixed prompts for the four operations:\n• Definition Mutation ( ˜Pdm): This operator mu-\ntates the definition in an instruction. It can in-\nvolve paraphrases and substitution of new defini-\ntions.\n• Definition Crossover ( ˜Pdc): This operator com-\nbines the definitions of two instructions to cre-\nate a new instruction. It can involve merging or\nexchanging parts of the definitions between the\nparent instructions.\n• Example Mutation ( ˜Pem): This operator per-\nturbs the example to introduce diversity. It can\ninvolve modifications such as example substitu-\ntion, addition, or deletion.\n• Example Crossover ( ˜Pec): This operator ran-\ndomly selects examples from two instructions to\ncreate a new instruction.\nFor instance, we formulate the mutation opera-\ntion as follows:\nˆddm = ChatGPT(Concat( ˜Pdm, d)) (2)\nwhere ˆddm is the new definition generated based\non the original instruction I. The new instruction\nis denoted as ˆI, ˆI = Concat(ˆddm, e). The other\noperators follow a similar formulation to mutation.\nFurther details of the fixed prompts are available in\nAppendix A.5.\n13594\nInstruction\nOperations\nInstruction\nIndividual\nParent Population\nInstruction\nIndividual\nInstruction\nIndividual\nNon-\ndominated\nSorting\nOﬀspring\nInstruction\nIndividual\nInstruction\nIndividual\nInstruction\nIndividual\nCrowdness Evaluation\nIndividual\nReplacement\nPareto-front Oﬀspring\n<Definition>: Please read \ncustomer’s reviews and predict \nsentiments:\n<Example>:\n    I really like this movie!\n    Sentiment: positive\nPredict the sentiment:\n<Input>\n<Definition>: Let’s classify the \ncustomers’ sentiments:\n<Example>:\n    I really like this movie!\n    Sentiment: positive\nPredict the sentiment:\n<Input>\nPlease read the online customer’s \nreviews and predict sentiments:\n<Example>:\n    I really like this movie!\n    Sentiment: positive\nPredict the sentiment:\n<Input>\nInitial Instruction\nDeﬁnition Mutation\nExample Mutation\nEvolve\nInitialise Population\nFigure 1: The main framework of InstOptima (left) and instruction operation examples (right). The details of\nthe workflow that is explained in Section 2.2. The population is composed of individuals of instruction examples.\n2.2.2 Optimization Objectives\nWe consider three objectives F = (m, l, r), in\noptimization, i.e., metrics (m), length (l), and per-\nplexity (r) of the instruction.\n• Performance: We use a set of metrics, such\nas accuracy, f1 score, precision, and recall, ob-\ntained by evaluating the instruction to calculate\nthe performance objective. The performance ob-\njective is represented as the reciprocal of the sum\nof these metrics.\n• Length: The length of the instruction is mea-\nsured in terms of the number of characters. This\nmeasurement is fair regardless of the tokenization\nstrategy.\n• Perplexity: The perplexity of the instruction is\nmeasured using the RoBERTa model.\nThe evaluation of objectives Fis shown in the\npseudo-code in Appendix A.4 but not depicted in\nFig. 1 for simplicity.\n2.3 Objective-Guided Instruction Operators\nTo enhance the performance of ChatGPT through\nin-context learning, we propose a simple yet effec-\ntive objective-feedback mechanism. Specifically,\nwe incorporate the fitness valuesF= (m, l, r) into\nthe fixed prompts. For example, we can append\n“Please refer to the objective values: (d1, F1),\n(d2, F2)” to ˜Pdc in instruction examples crossover.\nThese operators2 allow ChatGPT to autonomously\ndecide to emphasize or down-weight an instruction\nbased on the current objectives F.\n3 Experimental Setup\nWe conducted a comprehensive set of experiments3\nto validate the performance of InstOptima. The\ndetailed experiments setups and implementations\nare described in Appendix A.1.\n3.1 Baseline Methods\nWe used random instruction ( RanInstruct)\ngeneration (i.e., request ChatGPT generates\nseveral instructions similar to instructions gen-\nerated by InstOptima) and no-instruction\n(NoInstruct) as comparison baselines.\nThe RanInstruct generates five random\ninstructions using the LLM to evaluate the\nsame three objectives as InstOptima.\nThe NoInstructablates instruction in the\nclassification-oriented fine-tuning of Flan-T5.\n2Please refer to Table 4 for the actual implementations of\nthese objective-guided operators.\n3To improve the reproducibility, we release all experimen-\ntal materials in the supplementary files of the submission,\nincluding source code, experiment logs, and results, optimized\ninstructions.\n13595\nTable 1: The experimental performance of InstOptima. We show the ACCURACY instead of the performance\nobjective for intuitive evaluation. The symbols ‘ ↗’ and ‘ ↘’ indicate ‘larger is better’ and ‘lower is better’,\nrespectively. We repeat each experiment in five rounds and report the average results. The best results are inbold.\nThe ACCURACY is the best accuracy in the Pareto-front, while the LENGTH and PERPLEXITY are correlated with\nthe instruction that achieves the best accuracy.\nMODEL DATASET InstOptima RanInstruct NoInstructACCURACY↗LENGTH↘PERPLEXITY↘ ACCURACY↗LENGTH↘PERPLEXITY↘ ACCURACY↗\nFlanT5-small\nLaptop14 84.9±0.2 622.6±51.5 1.07±0.02 82.5±0.3 740.2±84.6 1.07±0.05 53.8±0.3\nRestaurant1484.9±0.2 421.6±82.4 1.11±0.01 82.3±0.4 328.5±38.5 1.15±0.03 19.2±0.4\nSST2 89.7±0.1 402.7±39.1 1.09±0.01 88.7±0.5 499.7±73.2 1.16±0.02 86.9±0.1\nAGNews 90.2±0.1 452.5±27.7 1.11±0.04 82.9±0.6 560.6±28.7 1.12±0.04 74.3±0.1\nSNLI 69.1±0.2 295.3±74.8 1.14±0.02 50.8±0.5 507.3±98.0 1.09±0.07 37.9±0.2\nMNLI 57.4±0.3 385.8±57.5 1.12±0.03 40.6±1.1 519.7±68.6 1.09±0.05 37.3±0.3\nFlanT5-base\nLaptop14 88.4±0.3 207.2±57.3 1.04±0.04 86.6±0.3 549.7±85.7 1.10±0.03 62.3±0.2\nRestaurant1489.1±0.2 359.4±39.7 1.06±0.03 87.4±0.5 589.3±63.2 1.11±0.03 52.8±0.2\nSST2 94.5±0.1 397.8±69.4 1.08±0.01 93.0±0.4 385.6±55.0 1.12±0.01 92.6±0.1\nAGNews 93.5±0.3 300.1±73.8 1.15±0.01 90.1±0.6 485.4±68.2 1.16±0.02 88.1±0.1\nSNLI 86.6±0.3 430.9±82.2 1.10±0.02 86.4±0.5 399.3±23.8 1.11±0.04 85.9±0.3\nMNLI 80.2±0.4 388.2±58.8 1.11±0.03 77.8±0.7 449.1±70.3 1.20±0.03 74.5±0.4\nChatGPT Laptop14 83.2±2.2 512.9±51.5 1.08±0.02 83.1±0.8 877.6±51.5 1.05±0.03 67.8±5.8\nRestaurant1496.3±1.9 487.3±55.9 1.09±0.02 92.1±1.3 421.6±82.4 1.10±0.02 75.2±6.1\n3.2 Main Results\nThe results in Table 1 show the performance of\nInstOptima. Overall, InstOptima achieves\nsuperior objectives based on various base mod-\nels (e.g., ChatGPT and FlanT5). For exam-\nple, it outperforms all baselines on all datasets in\nterms of ACCURACY . However, for instruction\nLENGTH and PERPLEXITY , the RanInstruct\nsometimes achieves better objective values. On the\nother hand, NoInstruct performs poorly on all\ndatasets in terms of ACCURACY , underscoring the\nimportance of instructions in generation-based fine-\ntuning. Moreover, the ACCURACY objective ex-\nhibits small intervals but relatively large variances,\nmaking it more challenging to optimize. However,\nexisting methods that prioritize performance opti-\nmization struggle to handle the variances in metrics.\nOn the other hand, the LENGTH objective is eas-\nier to optimize due to its significant variations and\ngreater significance. This is because long instruc-\ntions can result in up to twice training times than\nshort instructions. The PERPLEXITY metric ranges\nwithin small intervals, indicating a moderate opti-\nmization challenge, but it significantly impacts the\nunderstanding of instruction engineers. In addition\nto these three objectives, InstOptima can eas-\nily accommodate additional objectives for precise\ncontrol of instruction generation.\nOverall, InstOptima demonstrates impres-\nsive performance in instruction optimization across\nvarious tasks and datasets.\n3.3 Research Questions\nWe further discuss our observations and analysis\nby answering several research questions.\nRQ1: Do the objective-guided operators help\ninstruction optimization?\nTable 2: The experimental performance of\nInstOptima-N on FlanT5-small. The to-\nkens “−” and “+” indicate worse and better objectives\nthan InstOptima.\nDATASET InstOptima-N\nACCURACY↗ LENGTH↘ PERPLEXITY↘\nLaptop14 84.4±0.2− 789.3±86.2− 1.07±0.02\nRestaurant1483.7±0.3− 455.8±79.9− 1.12±0.03−\nSST2 89.6±0.1− 435.2±52.1− 1.12±0.02−\nAGNews 86.7±0.8− 535.8±69.4− 1.26±0.12−\nSNLI 69.8±0.6+ 454.0±77.0− 1.11±0.03+\nMNLI 57.3±0.5− 465.6±98.3− 1.09±0.02+\nTo investigate the impact of objective-guided\noperators on InstOptima, we conducted ab-\nlative experiments to assess the performance of\nInstOptima-N, which eliminates the objective\nguidance in the operators. The experimental re-\nsults on FlanT5-small are presented in Table 2.\nBased on the results in Table 1 and Table 2, it is evi-\ndent that InstOptima-N achieves inferior objec-\ntive values on most datasets, particularly in terms\nof ACCURACY and LENGTH . However, for the\nSNLI dataset, InstOptima-N obtains better re-\nsults in ACCURACY and PERPLEXITY compared to\nInstOptima. These findings demonstrate the ef-\nfectiveness of objective-guided operators. Nonethe-\nless, the concept of objective-guided operators is\nstill in its early stages and warrants further investi-\ngation in future studies.\nIn conclusion, the experimental results indicate\nthat objective-guided operators obtain better per-\nformance across various datasets.\n13596\nRQ2: Does the number of evolution generations\nmatter in InstOptima?\n2 4 6 8 10 12 14 16 18\n0.35\n0.35\n0.35\n0.35\nLaptop14\nPerformance\n2 4 6 8 10 12 14 16 18\n300\n400\n500\n600\n700\nLaptop14\nLength\n2 4 6 8 10 12 14 16 18\n1.06\n1.08\n1.1\n1.12\n1.14\nLaptop14\nPerplexity\n2 4 6 8 10 12 14 16 18\n0.44\n0.45\n0.46\n0.47\n0.48\nSNLI\nPerformance\n2 4 6 8 10 12 14 16 18\n250\n300\n350\n400\n450\nSNLI\nLength\n2 4 6 8 10 12 14 16 18\n1.1\n1.12\n1.14\n1.16\nSNLI\nPerplexity\n2 4 6 8 10 12 14 16 18\n0.28\n0.28\n0.28\nSST2\nPerformance\n2 4 6 8 10 12 14 16 18\n300\n400\n500\n600\n700\nSST2\nLength\n2 4 6 8 10 12 14 16 18\n1.08\n1.1\n1.12\nSST2\nPerplexity\nFigure 2: The trajectory plots of objective values across\ndifferent datasets. We plot the trajectories of 10 addi-\ntional generations using red lines. In these figures, lower\nobjective values indicate better performance.\nGenerally, a larger number of generations tends\nto result in better objective values after optimiza-\ntion. We conducted additional training for 10\ngenerations on the Laptop14, SST2, and SNLI\ndatasets to study the significance of number of\ngenerations. Based on the experimental results\nin Fig. 2., in most cases (e.g., Laptop14 and\nSNLI datasets), we observed a significant trade-off\namong the three objectives. However, due to the\nsmall scale of the evaluation data and population\nsize, there were large variances in the performance\nobjective (see the left column in Fig. 2). These\nvariances in performance interfere with the conver-\ngence of the other two objectives, resulting in the\nabsence of clear descending trends for the length\nand perplexity objectives with an increase in gen-\nerations. However, this issue can be addressed by\nincreasing the population size, number of genera-\ntions, and scale of training data.\nIn conclusion, given the limited evaluation\nresources, the number of evolution generations\nshowed limited improvement. Instead, it is im-\nportant to reconcile different objective values to\nachieve the final instruction population.\nRQ3: Are there trade-offs between different\nobjectives?\nTo analyze the relationship between different objec-\ntives, we plot the Pareto front (refer to Fig. 5) of in-\nstructions into three groups. The two-dimensional\nPareto fronts between pairwise objectives are pre-\nsented in Fig. 3.\nPerformance\nPerplexity\nPerformance\nLength\nPerplexity\nLength\nPerformance\nPerplexity\nPerformance\nLength\nPerplexity\nLength\nPerformance\nPerplexity\nPerformance\nLength\nPerplexity\nLength\nFigure 3: Visualizations of the 2D-Pareto fronts\nsearched by InstOptima on three datasets. The\nthree columns from left to right indicate the results on\nLaptop14, SST2 and SNLI datasets, respectively.\nOverall, there is a clear trade-off between in-\nstruction length and perplexity. However, when\nconsidering the pairs of performance-length and\nperformance-perplexity, there is no clear trade-off\nobserved in Fig. 3. This could be attributed to the\nlack of strict trade-offs and the presence of noise\nfitness points due to the evaluation of metrics on\nsmall datasets during optimization. It is expected\nthat this issue can be mitigated when evaluating\nperformance on larger datasets.\nNevertheless, InstOptima consistently dis-\ncovers high-quality instructions in most scenarios,\nregardless of the loose trade-offs between objective\npairs such as performance-length and performance-\nperplexity. This demonstrates the effectiveness of\nInstOptima in obtaining a diverse set of instruc-\ntions.\n4 Conclusion\nWe propose a multi-objective instruction optimiza-\ntion framework to obtain a diversified set of in-\nstructions. To address the challenges posed by\nthe large and non-differentiable text search space,\nInstOptima utilizes objective-guided instruc-\ntion operators based on LLM, which shows impres-\nsive performance in instruction generation. How-\never, it is important to note that multi-objective\ninstruction optimization is still in the early stages\nand requires further research in the future.\n13597\n5 Limitations\nThe first limitation of InstOptima lies in the po-\ntential crisis of local optima in the multi-objective\noptimization. InstOptima initializes the instruc-\ntion population based on fixed manually crafted\ninstructions, which are then mutated using LLM.\nAlthough InstOptima has been demonstrated\nto search for diversified and high-quality instruc-\ntions in experiments, the essence on fixed initial\ninstructions may lead to traps in local optima dur-\ning the multi-objective process. In the future, the\ngeneration of initial instruction populations, such\nas employing randomized initial instructions, re-\nmains a topic worth exploring.\nThe second limitation of InstOptima is re-\nlated to experimental resources. Due to resource\nconstraints, we only utilized single-round API calls\nto generate new instructions using LLM. This ap-\nproach overlooks the contextual information that\ncould help in understanding objective feedback in\nthe instruction generation. We believe that continu-\nous dialogue with LLM will significantly improve\nthe quality of instruction generated by LLM. Ad-\nditionally, due to the difficulty of accessing LLM,\nwe conducted experiments with smaller population\nsizes and fewer iterations, which may underesti-\nmate the performance of InstOptima.\nAcknowledgments\nThis work was supported in part by the\nUKRI Future Leaders Fellowship under Grant\nMR/S017062/1 and MR/X011135/1; in part by\nNSFC under Grant 62376056 and 62076056;\nin part by the Royal Society under Grant\nIES/R2/212077; in part by the EPSRC under Grant\n2404317; in part by the Kan Tong Po Fellowship\n(KTP\\R1\\231017); and in part by the Amazon Re-\nsearch Award and Alan Turing Fellowship.\nReferences\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large an-\nnotated corpus for learning natural language infer-\nence. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2015, Lisbon, Portugal, September 17-21,\n2015, pages 632–642. The Association for Computa-\ntional Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nNeurIPS’20: Proc. of Annual Conference on Neural\nInformation Processing Systems.\nLichang Chen, Jiuhai Chen, Tom Goldstein, Heng\nHuang, and Tianyi Zhou. 2023. Instructzero: Ef-\nficient instruction optimization for black-box large\nlanguage models. CoRR, abs/2306.03082.\nSukmin Cho, Soyeong Jeong, Jeongyeon Seo, and\nJong C. Park. 2023. Discrete prompt optimization\nvia constrained generation for zero-shot re-ranker.\nCoRR, abs/2305.13729.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\nCoRR, abs/2210.11416.\nKalyanmoy Deb, Samir Agrawal, Amrit Pratap, and\nT. Meyarivan. 2002. A fast and elitist multiobjec-\ntive genetic algorithm: NSGA-II. IEEE Trans. Evol.\nComput., 6(2):182–197.\nYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.\n2022. PPT: pre-trained prompt tuning for few-shot\nlearning. In ACL’22: Proc. of the 60th Annual Meet-\ning of the Association for Computational Linguistics,\npages 8410–8423. Association for Computational\nLinguistics.\nOr Honovich, Uri Shaham, Samuel R. Bowman, and\nOmer Levy. 2022. Instruction induction: From\nfew examples to natural language task descriptions.\nCoRR, abs/2205.10782.\nYoichi Ishibashi, Danushka Bollegala, Katsuhito Su-\ndoh, and Satoshi Nakamura. 2023. Evaluating the\nrobustness of discrete prompts. In EACL’23: Proc.\nof the 17th Conference of the European Chapter of\nthe Association for Computational Linguistics, pages\n2365–2376. Association for Computational Linguis-\ntics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021 , pages 3045–\n3059. Association for Computational Linguistics.\n13598\nMoxin Li, Wenjie Wang, Fuli Feng, Jizhi Zhang, and\nTat-Seng Chua. 2023. Robust instruction optimiza-\ntion for large language models with distribution shifts.\nCoRR, abs/2305.13954.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Comput. Surv., 55(9):195:1–195:35.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nSilviu Pitis, Michael R. Zhang, Andrew Wang, and\nJimmy Ba. 2023. Boosted prompt ensembles for\nlarge language models. CoRR, abs/2304.05970.\nMaria Pontiki, Dimitris Galanis, John Pavlopoulos, Har-\nris Papageorgiou, Ion Androutsopoulos, and Suresh\nManandhar. 2014. Semeval-2014 task 4: Aspect\nbased sentiment analysis. In Proceedings of the 8th\nInternational Workshop on Semantic Evaluation, Se-\nmEval@COLING 2014, Dublin, Ireland, August 23-\n24, 2014, pages 27–35. The Association for Com-\nputer Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y . Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2013, 18-21 October 2013, Grand Hyatt\nSeattle, Seattle, Washington, USA, A meeting of SIG-\nDAT, a Special Interest Group of the ACL , pages\n1631–1642. ACL.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nAlexander Wan, Eric Wallace, Sheng Shen, and Dan\nKlein. 2023. Poisoning language models during in-\nstruction tuning. CoRR, abs/2305.00944.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nXiaosen Wang, Yichen Yang, Yihe Deng, and Kun He.\n2021. Adversarial training with fast gradient pro-\njection method against synonym substitution based\ntext attacks. In AAAI’21: Proc. of Thirty-Fifth AAAI\nConference on Artificial Intelligence, pages 13997–\n14005. AAAI Press.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transformers:\nState-of-the-art natural language processing. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing: System Demon-\nstrations, EMNLP 2020 - Demos, Online, November\n16-20, 2020, pages 38–45. Association for Computa-\ntional Linguistics.\nXiaoyan Yu, Qilei Yin, Zhixin Shi, and Yuru Ma. 2022.\nImproving the semantic consistency of textual ad-\nversarial attacks via prompt. In International Joint\nConference on Neural Networks, IJCNN 2022, Padua,\nItaly, July 18-23, 2022, pages 1–8. IEEE.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems 28: Annual Conference on Neural In-\nformation Processing Systems 2015, December 7-12,\n2015, Montreal, Quebec, Canada, pages 649–657.\nYi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-Wei\nChang, and Xuanjing Huang. 2021. Defense against\nsynonym substitution-based adversarial attacks via\ndirichlet neighborhood ensemble. In ACL/IJC-\nNLP’21: Proc. of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing, pages 5482–5492. Association for\nComputational Linguistics.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2022. Large language models are human-level\nprompt engineers. CoRR, abs/2211.01910.\nYuhang Zhou, Suraj Maharjan, and Beiye Liu. 2023.\nScalable prompt generation for semi-supervised\nlearning with language models. In EACL’23: Find-\nings of the Association for Computational Linguistics,\npages 758–769. Association for Computational Lin-\nguistics.\nKaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang,\nHao Chen, Yidong Wang, Linyi Yang, Wei Ye,\n13599\nNeil Zhenqiang Gong, Yue Zhang, and Xing Xie.\n2023. Promptbench: Towards evaluating the robust-\nness of large language models on adversarial prompts.\nCoRR, abs/2306.04528.\nA Appendix\nA.1 Experiment Setup\nA.1.1 Datasets\nWe selected six datasets for three classification\ntasks. For the aspect-based sentiment analy-\nsis (ABSA) task, we used the Laptop14 and\nRestaurant14 datasets (Pontiki et al., 2014).\nFor text classification (TC) tasks, we chose the\nSST2 (Socher et al., 2013) and AGNews (Zhang\net al., 2015) datasets. We selected the SNLI (Bow-\nman et al., 2015) and MNLI (Wang et al., 2019)\ndatasets for the natural language inference (NLI)\ntask. We trained our models on the first 1000 sam-\nples from the original training, validation and test-\ning datasets, respectively.\nA.1.2 Experimental PLMs\nFor the LLM to operate instructions, we select the\nChatGPT4(OpenAI, 2023) with a temperature of\n1 and a maximum token length of 500.\nTo obtain the objective value of perfor-\nmance, we performed instruction-based classifica-\ntion experiments using the FlanT5-small and\nFlanT5-base models (Chung et al., 2022), as\nwell as ChatGPT, which are the latest and popular\nPLM/LLM for instruction learning. For the calcu-\nlation of semantic complexity, we employed the\nRoBERTa (Liu et al., 2019) model from transform-\ners(Wolf et al., 2020).\nA.1.3 Hyper-parameter Settings\nThe generation size and number of generation for\nNSGA-II is 100 and 10, respectively. In the\nfine-tuning5 of the PLMs (i.e., FlanT5-small\nand FlanT5-base), we set the learning rate and\nbatch size to 5e −5 and 16, respectively. We fine-\ntune the PLMs for 3 epochs with an L2 regulariza-\ntion parameter of 0.01.\nA.1.4 Experimental Environment\nThe experiments are carried out on a computer run-\nning the Cent OS 7 operating system, equipped\nwith an RTX 3090 GPU and a Core i-12900k pro-\ncessor. We use the PyTorch 2.0.0 library and\ntransformers 4.28.0.\n4ChatGPT-turbo-0301 version.\n5We use the Huggingface Trainer for fine-tuning, and the\ncode is available in the supplementary materials.\nA.2 Additional Experiments for\nSummarization\nA.2.1 Generative Text Summarization\nWe conducted experiments for a text generation\ntask. i.e., generative summarization. To evalu-\nate InstOptima, we used three subsets from\nThe GigaWord dataset and the FlanT5-small\nmodel in our experiments. In these subsets, the\ntraining set contains 5k training examples, while\nthe testing set and validation set each have 1k ex-\namples. According to the Rouge 1 metric, it is\nevident that InstOptima performs well on the\nGigaWord dataset, demonstrating that it is a task-\nagnostic method for multi-objective instruction op-\ntimization.\nA.2.2 Experiments based on Different\nBackbone Models\nWe have conducted experiments to demonstrate\nthe relationship between the backbone model and\nperformance. Due to resource limitations, we\nare currently using FlanT5 variants (small, base,\nand large, Llama is not implemented currently)\nas backbones to implement InstOptima. We\nhave generated a box plot to visualize the experi-\nmental results in Fig. 4 The figure illustrates that\nperformance is highly dependent on the scale of\nthe backbone instruction-follow model. In other\nwords, because the FlanT5-small model has\nlimited capability to follow instructions, the accu-\nracy achieved by an instruction is low and exhibits\na larger variance compared to the larger instruction-\nfollow models. In this context, InstOptima\nplays a crucial role in identifying instructions with\noptimized objectives.\nA.3 The Visualization of Pareto-fronts\nIn Fig. 5, we show the visualizations of Pareto-\nfront instructions obtained by InstOptima on\nthe Laptop14, SST2 and SNLI datasets. Due\nto resource limitations, we only present the plots\non the Laptop14, SST2, and SNLI datasets. We\nplot the first three fronts searched by NSGA-II,\nand the first three fronts are indicated by red, green,\nand blue colors, respectively.\nA.4 Multi-objective Optimization Algorithm\nInstOptima is a multi-objective instruction op-\ntimization approach that evolves a population of\ninstructions through a series of steps. We present\nthe pseudo-code of InstOptima in Algorithm 1.\n13600\nTable 3: The experimental performance of InstOptima. We show the ACCURACY instead of the performance\nobjective for intuitive evaluation. The symbols↗and ↘indicate larger is better and lower is better, respectively. We\nrepeat each experiment in five rounds and report the average results. The best results are in bold. The ACCURACY\nis the best accuracy in the Pareto-front, while the LENGTH and PERPLEXITY are correlated with the instruction that\nachieves the best accuracy.\nMODEL DATASET InstOptima RanInstruct NoInstruct\nACCURACY↗LENGTH↘PERPLEXITY↘ ACCURACY↗LENGTH↘PERPLEXITY↘ ACCURACY↗\nFlanT5-smallGigaWord33.7±0.3 586.9±91.5 1.08±0.02 32.9±1.9 891.6±151.5 1.11±0.03 30.8±0.8\nLarge Base Small\n80\n82\n84\n86\n88\n90\n92\nFlan-T5 Variant\nAccuracy (%)\nRanInstruct\nInstOptima\nLarge Base Small\n85\n90\n95\nFlan-T5 Variant\nAccuracy (%)\nRanInstruct\nInstOptima\nLaptop14 Restaurant14\nLarge Base Small\n88\n90\n92\n94\n96\nFlan-T5 Variant\nAccuracy (%)\nRanInstruct\nInstOptima\nLarge Base Small\n85\n90\n95\nFlan-T5 Variant\nAccuracy (%)\nRanInstruct\nInstOptima\nAGNewsSST2\nSNLI MNLI\nLarge Base Small\n40\n50\n60\n70\n80\n90\nFlan-T5 Variant\nAccuracy (%)\nRanInstruct\nInstOptima\nLarge Base Small\n50\n60\n70\n80\n90\nFlan-T5 Variant\nAccuracy (%)\nRanInstruct\nInstOptima\nFigure 4: Box plot visualizations of the performance\nbased on different backbone models.\nFirstly, the algorithm initializes a population of\ninstructions. Then, it iteratively performs the fol-\nlowing steps for a specified number of generations:\nselecting two instructions from the population, eval-\nuating their objectives, applying LLM-based in-\nstruction operators to create new instructions, and\nadding them to a temporary population. After each\ngeneration, the temporary population is combined\nwith the original population, and a selection pro-\ncess is applied to choose the fittest instructions. Fi-\nnally, the algorithm returns the evolved population\nof instructions as the final results.\nA.5 Fixed Prompts for Instruction Operators\nThe prompts in green are the trigger of objective-\nguided instruction generation.\n400\n600\n800\n1.1\n1.15\n0.28\n0.28\n0.28\nLengthPerplexity\nPerformance\nSST2\n4006008001,000\n1.1\n1.2\n0.35\n0.36\nLength\nPerplexity\nPerformance\nLaptop14\n300\n400\n1.1\n1.12\n1.14\n0.45\n0.5\nLengthPerplexity\nPerformance\nSNLI\nFigure 5: Visualizations of the Pareto fronts searched\nby InstOptima on three datasets. The PLM used to\nevaluate performance is FlanT5-small.\n13601\nAlgorithm 1: The pseudo code of InstOptima.\nInput: Task dataset D, Number of generations N, Population size M, Instruction Operators ˜P\nOutput: Evolved population of instructions P∗\n1 P← InitializePopulation(M) ; // Initialize the population\n2 for i ←1 to N do\n3 Q←∅ ; // Initialize the offspring population\n4 for j ←1 to M do\n5 I1 ←Pj ; // Select parent instruction\n6 I2 ←random(P) ; // Select random parent instruction\n7 F1 ←EvaluateObjectives (I1)6 ; // Evaluate objectives for parent 1\n8 F2 ←EvaluateObjectives (I2) ; // Evaluate objectives for parent 2\n9 (d1, e1) ←I1 ; // Extract definition and example from parent 1\n10 (d2, e2) ←I2 ; // Extract definition and example from parent 2\n11 O← random( ˜P) ; // Select a random operator\n12 if O== ˜Pdm then\n13 ˆddm ←ChatGPT(Concat( ˜Pdm, d1, F1)) ; // Generate mutated definition\n14 ˆI ←Concat(ˆddm, e1) ; // Combine mutated definition with example\n15 if O== ˜Pdc then\n16 ˆddc ←ChatGPT(Concat( ˜Pdc, d1, F1, d2, F2)) ; // Generate crossoverd definition\n17 ˆI ←Concat(ˆddc, e1) ; // Combine crossoverd definition with example\n18 if O== ˜Pem then\n19 ˆ eem ←ChatGPT(Concat( ˜Pem, e1, F1)) ; // Generate mutated example\n20 ˆI ←Concat(d1,ˆ eem) ; // Combine original definition with mutated example\n21 if O== ˜Pec then\n22 ˆ eec ←ChatGPT(Concat( ˜Pec, e1, F1, e2, F2)) ; // Generate crossoverd example\n23 ˆI ←Concat(d1,ˆ eec) ; // Combine original definition with crossoverd example\n24 Q←Q∪{ ˆI}; // Add offspring to the population\n25 Q∗ ←CombinePopulations(P, Q) ; // Combine parent and offspring populations\n26 P← SelectPopulation(Q∗, M) ; // Select the best individuals for the next\ngeneration\n27 P∗ = P; // Set the evolved population as the final population\n28 return P∗ ; // Return the evolved population\nTable 4: The fixed prompts used to implement LLM-based instructions. “ <Input>” indicates the input of the\noperators. The green keywords are the triggers of objective-guided instruction generation.\nOPERATORS PROMPTS INPUT\n˜Pdm\nI want you to be a professional prompt engineer. Now I am working on the multi-objective evolutionary\nprompt optimization, and I need your help to design and optimize the template prompt. Here I give\nyou an example template prompt, please understand the meaning of the prompt and modify it. Given\nthe minimization objectives, please be creative and output the paraphrased or mutated prompt. Please\nremove Minimization objectives in the output:<Input>\n(d,F)\n˜Pdc\nI want you to be a professional prompt engineer. Now I am working on the multi-objective evolutionary\nprompt optimization for sentiment analysis, and I need your help to design and optimize the template\nprompt. Here I give you two template prompts, please understand the meaning of the two prompts and\ncrossover them into a new prompt. Given the minimization objectives, please be creative and output\nthe generated new prompt based on the two examples. Please remove Minimization objectives in the\noutput:<Input>\n(d1,F1, d2,F2)\n˜Pem\nI want you to be a professional prompt engineer. Now I am working on the multi-objective evolutionary\nprompt optimization for sentiment analysis, and I need your help to design and optimize the template\nprompt. Here I give you two groups of examples for completing the prompt, please generate new\nexamples to substitute the following examples and there are no more than two examples in the new\nprompt. Given the minimization objectives, please be creative and output the generated example in\nthe same format. Please remove Minimization objectives in the output:<Input>\n(e,F)\n˜Pec\nI want you to be a professional prompt engineer. Now I am working on the multi-objective evolutionary\nprompt optimization for sentiment analysis, and I need your help to design and optimize the template\nprompt. Here I give you two groups of examples for completing the prompt, please read the examples\nof the two groups of examples and crossover the examples into a new example group and there are\nno more than two examples in the new examples. Given the minimization objectives, please be\ncreative and output the crossovered the examples. Please remove Minimization objectives in the\noutput:<Input>\n(e1,F1, e2,F2)\n13602",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8472182750701904
    },
    {
      "name": "Perplexity",
      "score": 0.7469671964645386
    },
    {
      "name": "Crossover",
      "score": 0.6373143196105957
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4766962230205536
    },
    {
      "name": "Language model",
      "score": 0.46297237277030945
    },
    {
      "name": "Instruction set",
      "score": 0.43939870595932007
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4332544803619385
    },
    {
      "name": "Evolutionary algorithm",
      "score": 0.4158729314804077
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3158845603466034
    },
    {
      "name": "Parallel computing",
      "score": 0.24704426527023315
    },
    {
      "name": "Programming language",
      "score": 0.20874449610710144
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I23923803",
      "name": "University of Exeter",
      "country": "GB"
    }
  ]
}