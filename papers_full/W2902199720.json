{
    "title": "Video Action Transformer Network",
    "url": "https://openalex.org/W2902199720",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A5006312307",
            "name": "Rohit Girdhar",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A5057909195",
            "name": "João Carreira",
            "affiliations": [
                "DeepMind (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A5081047759",
            "name": "Carl Doersch",
            "affiliations": [
                "DeepMind (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A5057678172",
            "name": "Andrew Zisserman",
            "affiliations": [
                "University of Oxford"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2962722947",
        "https://openalex.org/W6703281212",
        "https://openalex.org/W6620707391",
        "https://openalex.org/W6744865055",
        "https://openalex.org/W6763509872",
        "https://openalex.org/W6739520758",
        "https://openalex.org/W2583815496",
        "https://openalex.org/W6684983439",
        "https://openalex.org/W4249279051",
        "https://openalex.org/W6738893770",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W2020163092",
        "https://openalex.org/W2962899219",
        "https://openalex.org/W6780226713",
        "https://openalex.org/W2524365899",
        "https://openalex.org/W2557728737",
        "https://openalex.org/W2034014085",
        "https://openalex.org/W2016053056",
        "https://openalex.org/W6737542352",
        "https://openalex.org/W2126579184",
        "https://openalex.org/W6955071965",
        "https://openalex.org/W6630875275",
        "https://openalex.org/W6752703593",
        "https://openalex.org/W2964185410",
        "https://openalex.org/W6743837088",
        "https://openalex.org/W6600983433",
        "https://openalex.org/W2608988379",
        "https://openalex.org/W2146634731",
        "https://openalex.org/W6739622702",
        "https://openalex.org/W2618799552",
        "https://openalex.org/W2963150697",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W6756173984",
        "https://openalex.org/W2962790054",
        "https://openalex.org/W2963901033",
        "https://openalex.org/W6752539834",
        "https://openalex.org/W6754693662",
        "https://openalex.org/W6751741970",
        "https://openalex.org/W2963524571",
        "https://openalex.org/W2963247196",
        "https://openalex.org/W6757010476",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W6724944384",
        "https://openalex.org/W6747225971",
        "https://openalex.org/W6751936687",
        "https://openalex.org/W2963155035",
        "https://openalex.org/W6753598266",
        "https://openalex.org/W2105101328",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2963907629",
        "https://openalex.org/W2809725072",
        "https://openalex.org/W2894087839",
        "https://openalex.org/W2611596598",
        "https://openalex.org/W2034328688",
        "https://openalex.org/W2172806452",
        "https://openalex.org/W2337252826",
        "https://openalex.org/W2619947201",
        "https://openalex.org/W2964233791",
        "https://openalex.org/W2308045930",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W3037932933",
        "https://openalex.org/W24089286",
        "https://openalex.org/W2963820951",
        "https://openalex.org/W2977233818",
        "https://openalex.org/W2883275382",
        "https://openalex.org/W2706729717",
        "https://openalex.org/W2622263826",
        "https://openalex.org/W2010399676",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2507009361",
        "https://openalex.org/W2806331055",
        "https://openalex.org/W2808675313",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W2950739196",
        "https://openalex.org/W2799261915",
        "https://openalex.org/W2964318666",
        "https://openalex.org/W2902281023",
        "https://openalex.org/W2773514261"
    ],
    "abstract": "We introduce the Action Transformer model for recognizing and localizing human actions in video clips. We repurpose a Transformer-style architecture to aggregate features from the spatiotemporal context around the person whose actions we are trying to classify. We show that by using high-resolution, person-specific, class-agnostic queries, the model spontaneously learns to track individual people and to pick up on semantic context from the actions of others. Additionally its attention mechanism learns to emphasize hands and faces, which are often crucial to discriminate an action - all without explicit supervision other than boxes and class labels. We train and test our Action Transformer network on the Atomic Visual Actions (AVA) dataset, outperforming the state-of-the-art by a significant margin using only raw RGB frames as input.",
    "full_text": "Video Action Transformer Network\nRohit Girdhar1*\nJo˜ao Carreira2 Carl Doersch2 Andrew Zisserman2,3\n1Carnegie Mellon University 2DeepMind 3University of Oxford\nhttp://rohitgirdhar.github.io/ActionTransformer\nAbstract\nWe introduce the Action Transformer model for recogniz-\ning and localizing human actions in video clips. We repur-\npose a Transformer-style architecture to aggregate features\nfrom the spatiotemporal context around the person whose\nactions we are trying to classify. We show that by using\nhigh-resolution, person-speciﬁc, class-agnostic queries, the\nmodel spontaneously learns to track individual people and\nto pick up on semantic context from the actions of others.\nAdditionally its attention mechanism learns to emphasize\nhands and faces, which are often crucial to discriminate an\naction – all without explicit supervision other than boxes\nand class labels. We train and test our Action Transformer\nnetwork on the Atomic Visual Actions (AVA) dataset, out-\nperforming the state-of-the-art by a signiﬁcant margin us-\ning only raw RGB frames as input.\n1. Introduction\nIn this paper, our objective is to both localize and rec-\nognize human actions in video clips. One reason that hu-\nman actions remain so difﬁcult to recognize is that inferring\na person’s actions often requires understanding the people\nand objects around them. For instance, recognizing whether\na person is ‘listening to someone’ is predicated on the ex-\nistence of another person in the scene saying something.\nSimilarly, recognizing whether a person is ‘pointing to an\nobject’, or ‘holding an object’, or ‘shaking hands’; all re-\nquire reasoning jointly about the person and the animate and\ninanimate elements of their surroundings. Note that this is\nnot limited to the context at a given point in time: recogniz-\ning the action of ‘watching a person’, after the watched per-\nson has walked out of frame, requires reasoning over time\nto understand that our person of interest is actually looking\nat someone and not just staring into the distance.\nThus we seek a model that can determine and utilize such\ncontextual information (other people, other objects) when\ndetermining the action of a person of interest. The Trans-\nformer architecture from Vaswani et al. [44] is one suitable\n*Work done during an internship at DeepMind\n…\nInput Video Clip\n! = 0 ! = $/2! = −$/2\n…\nContext Embeddings Person-specific self-attention\nkeyframe\naction class\nFigure 1: Action Transformer in action. Our proposed multi-\nhead/layer Action Transformer architecture learns to attend to rel-\nevant regions of the person of interest and their context (other peo-\nple, objects) to recognize the actions they are doing. Each head\ncomputes a clip embedding, which is used to focus on different\nparts like the face, hands and the other people to recognize that the\nperson of interest is ‘holding hands’ and ‘watching a person’.\nmodel for this, since it explicitly builds contextual support\nfor its representations using self-attention. This architecture\nhas been hugely successful for sequence modelling tasks\ncompared to traditional recurrent models. The question,\nhowever, is: how does one build a similar model for human\naction recognition?\nOur answer is a new video action recognition network,\nthe Action Transformer, that uses a modiﬁed Transformer\narchitecture as a ‘head’ to classify the action of a person\nof interest. It brings together two other ideas: (i) a spatio-\ntemporal I3D model that has been successful in previous\napproaches for action recognition in video [7] – this pro-\nvides the base features; and (ii) a region proposal network\n(RPN) [34] – this provides a sampling mechanism for local-\nizing people performing actions. Together the I3D features\nand RPN generate the query that is the input for the Trans-\nformer head that aggregates contextual information from\nother people and objects in the surrounding video. We de-\nscribe this architecture in detail in section 3. We show in\nsection 4 that the trained network is able to learn both to\ntrack individual people and to contextualize their actions in\nterms of the actions of other people in the video. In addition,\nthe transformer attends to hand and face regions, which is\nreassuring because we know they have some of the most rel-\nevant features when discriminating an action. All of this is\n1\narXiv:1812.02707v2  [cs.CV]  17 May 2019\nobtained without explicit supervision, but is instead learned\nduring action classiﬁcation.\nWe train and test our model on the Atomic Visual Ac-\ntions (A V A) [16] dataset. This is an interesting and suitable\ntestbed for this kind of contextual reasoning. It requires de-\ntecting multiple people in videos semi-densely in time, and\nrecognizing multiple basic actions. Many of these actions\noften cannot be determined from the person bounding box\nalone, but instead require inferring relations to other people\nand objects. Unlike previous works [3], our model learns\nto do so without needing explicit object detections. We set\na new record on the A V A dataset, improving performance\nfrom 17.4% [42] to 25.0% mAP. The network only uses\nraw RGB frames, yet it outperforms all previous work, in-\ncluding large ensembles that use additional optical ﬂow and\nsound inputs. At the time of submission, ours was the top\nperforming approach on the ActivityNet leaderboard [6].\nHowever, we note that at 25% mAP, this problem, or\neven this dataset, is far from solved. Hence, we rigorously\nanalyze the failure cases of our model in Section 5. We\ndescribe some common failure modes and analyze the per-\nformance broken down by semantic and spatial labels. In-\nterestingly, we ﬁnd many classes with relatively large train-\ning sets are still hard to recognize. We investigate such tail\ncases to ﬂag potential avenues for future work.\n2. Related Work\nVideo Understanding: Video activity recognition has\nevolved rapidly in recent years. Datasets have become pro-\ngressively larger and harder: from actors performing simple\nactions [14, 36], to short sports and movie clips [27, 41],\nﬁnally to diverse YouTube videos [1, 26]. Models have fol-\nlowed suit, from hand-crafted features [28, 45] to deep end-\nto-end trainable models [7, 25, 46, 47, 49]. However, much\nof this work has focused on trimmed action recognition, i.e.,\nclassifying a short clip into action classes. While useful,\nthis is a rather limited view of action understanding, as most\nvideos involve multiple people performing multiple differ-\nent actions at any given time. Some recent work has looked\nat such ﬁne-grained video understanding [8, 20, 24, 40],\nbut has largely been limited to small datasets like UCF-\n24 [40, 41] or JHMDB [22]. Another thread of work has\nfocused on temporal action detection [38, 39, 50]; however,\nit does not tackle the tasks of person detection or person-\naction attribution.\nA V A dataset and methods: The recently introduced\nA V A [16] dataset has attempted to remedy this by intro-\nducing 15-minute long clips labeled with all people and\ntheir actions at one second intervals. Although fairly new,\nvarious models [16, 23, 42, 52] have already been pro-\nposed for this task. Most models have attempted to ex-\ntend object detection frameworks [17, 21, 34] to operate\non videos [11, 20, 24]. Perhaps the closest to our ap-\nproach is the concurrent work on person-centric relation\nnetworks [42], which learns to relate person features with\nthe video clip akin to relation networks [35]. In contrast,\nwe propose to use person detections as queries to seek out\nregions to aggregate in order to recognize their actions, and\noutperform [42] and other prior works by a large margin.\nAttention for action recognition: There has been a large\nbody of work on incorporating attention in neural networks,\nprimarily focused on language related tasks [44, 51]. Atten-\ntion for videos has been pursued in various forms, includ-\ning gating or second order pooling [12, 30, 31, 49], guided\nby human pose or other primitives [4, 5, 12, 13], region-\ngraph representations [19, 48], recurrent models [37] and\nself-attention [47]. Our model can be thought of as a form\nof self-attention complementary to these approaches. In-\nstead of comparing all pairs of pixels, it reduces one side of\nthe comparison to human regions, and can be applied on top\nof a variety of base architectures, including the previously\nmentioned attentional architectures like [47].\n3. Action Transformer Network\nIn this section we describe the overall design of our new\nAction Transformer model. It is designed to detect all per-\nsons, and classify all the actions they are doing, at a given\ntime point (‘keyframe’). It ingests a short video clip cen-\ntered on the keyframe, and generates a set of human bound-\ning boxes for all the people in the central frame, with each\nbox labelled with all the predicted actions for the person.\nThe model consists of a distinct base and head net-\nworks, similar to the Faster R-CNN object detection frame-\nwork [34]. The base, which we also refer to as trunk, uses\na 3D convolutional architecture to generate features and re-\ngion proposals (RP) for the people present. The head then\nuses the features associated with each proposal to predict\nactions and regresses a tighter bounding box. Note that, im-\nportantly, both the RPN and bounding box regression are\naction agnostic. In more detail, the head uses the feature\nmap generated by the trunk, along with the RPN proposals,\nto generate a feature representation corresponding to each\nRP using RoIPool [21] operations. This feature is then used\nclassify the box into C action classes or background (total\nC + 1), and regress to a 4D vector of offsets to convert the\nRPN proposal into a tight bounding box around the person.\nThe base is described in Section 3.1, and the transformer\nhead in Section 3.2. We also describe an alternative I3D\nHead in Section 3.3, which is a more direct analogue of the\nFaster-RCNN head. It is used in the ablation study. Imple-\nmentation details are given in Section 3.4.\n3.1. Base network architecture\nWe start by extracting aT-frame (typically 64) clip from\nthe original video, encoding about 3 seconds of context\naround a given keyframe. We encode this input using a\nRPN\nConvolutional feature map from I3D\nRPN over center frame features\nN+1 way classification\nInput clip\nTrunk\nRoIPoolTxTx⊙TxTx⊙TxTx⊙Multi-head, multi-layer Tx HeadI3D base\nRoIPool\n✪SoftmaxAttention \n⨁Weighted Sum⍉Dropout\n+LayerNorm\n⍉\n+LayerNorm\nFFN\nDropout\nQPr\nLocationembedding\nTx UnitBounding box regression\nFigure 2: Base Network Architecture. Our model takes a clip as input and generates a spatio-temporal feature representation using a\ntrunk network, typically the initial layers of I3D. The center frame of the feature map is passed through an RPN to generate bounding box\nproposals, and the feature map (padded with location embedding) and each proposal are passed through ‘head’ networks to obtain a feature\nfor the proposal. This feature is then used to regress a tight bounding box and classify into action classes. The head network consists of a\nstack of Action Transformer (Tx) units, which generates the features to be classiﬁed. We also visualize the Tx unit zoomed in, as described\nin Section 3.2. QPr and FFN refer to Query Preprocessor and a Feed-forward Network respectively, also explained Section 3.2.\nset of convolutional layers, and refer to this network as the\ntrunk. In practice, we use the initial layers of an I3D net-\nwork pre-trained on Kinetics-400 [7]. We extract the feature\nmap from the Mixed 4f layer, by which the T ×H ×W\ninput is downsampled to T′ ×H′ ×W′ = T\n4 ×H\n16 ×W\n16 .\nWe slice out the temporally-central frame from this fea-\nture map and pass it through a region proposal network\n(RPN) [34]. The RPN generates multiple potential person\nbounding boxes along with objectness scores. We then se-\nlect R boxes (we use R = 300) with the highest objectness\nscores to be further regressed into a tight bounding box and\nclassiﬁed into the action classes using a ‘head’ network, as\nwe describe next. The trunk and RPN portions of Figure 2\nillustrate the network described so far.\n3.2. Action Transformer Head\nAs outlined in the Introduction, our head architecture\nis inspired and re-purposed from the Transformer architec-\nture [44]. It uses the person box from the RPN as a ‘query’\nto locate regions to attend to, and aggregates the informa-\ntion over the clip to classify their actions. We ﬁrst brieﬂy\nreview the Transformer architecture, and then describe our\nAction Transformer head framework.\nTransformer: This architecture was proposed in [44] for\nseq2seq tasks like language translation, to replace tradi-\ntional recurrent models. The main idea of the original ar-\nchitecture is to compute self-attention by comparing a fea-\nture to all other features in the sequence. This is carried out\nefﬁciently by not using the original features directly. In-\nstead, features are ﬁrst mapped to a query (Q) and memory\n(key and value, K & V ) embedding using linear projec-\ntions, where typically the query and keys are lower dimen-\nsional. The output for the query is computed as an attention\nweighted sum of values V , with the attention weights ob-\ntained from the product of the query Q with keys K. In\npractice, the query here was the word being translated, and\nthe keys and values are linear projections of the input se-\nquence and the output sequence generated so far. A location\nembedding is also added to these representations in order to\nincorporate positional information which is lost in this non-\nconvolutional setup. We refer the readers to [44] and [32]\nfor a more detailed description of the original architecture.\nAction Transformer: We now describe our re-purposed\nTransformer architecture for the task of video understand-\ning. Our transformer unit takes as input the video feature\nrepresentation and the box proposal from RPN and maps\nit into query and memory features. Our problem setup has\na natural choice for the query ( Q), key (K) and value ( V )\ntensors: the person being classiﬁed is the query, and the clip\naround the person is the memory, projected into key and val-\nues. The unit then processes the query and memory to out-\nput an updated query vector. The intuition is that the self-\nattention will add context from other people and objects in\nthe clip to the query vector, to aid with the subsequent clas-\nsiﬁcation. This unit can be stacked in multiple heads and\nlayers similar to the original architecture [44], by concate-\nnating the output from the multiple heads at a given layer,\nand using the concatenated feature as the next query. This\nupdated query is then used to again attend to context fea-\ntures in the following layer. We show this high-level setup\nand how it ﬁts into our base network highlighted in green\nin Figure 2, with each Action Transformer unit denoted as\n‘Tx’. We now explain this unit in detail.\nThe key and value features are simply computed as lin-\near projections of the original feature map from the trunk,\nhence each is of shape T′ ×H′ ×W′ ×D. In practice, we\nextract the RoIPool-ed feature for the person box from the\ncenter clip, and pass it through a query preprocessor (QPr)\nand a linear layer to get the query feature of size1 ×1 ×D.\nThe QPr could directly average the RoIpool feature across\nRPN\nST-RoIPool\nRPN over center frame features\nExtend proposals in time by replicating\nI3D blocksLogits\nFigure 3: I3D Head. Optionally, we can replace the Action Trans-\nformer head with a simpler head that applies the last few I3D\nblocks to the region features, as described in Section 3.3.\nspace, but would lose all spatial layout of the person. In-\nstead, we ﬁrst reduce the dimensionality by a 1 ×1 convo-\nlution, and then concatenate the cells of the resulting 7 ×7\nfeature map into a vector. Finally, we reduce the dimen-\nsionality of this feature map using a linear layer to 128D\n(the same as the query and key feature maps). We refer to\nthis procedure as HighRes query preprocessing. We com-\npare this to a QPr that simply averages the feature spatially,\nor LowRes preprocessing, in Section 4.3.\nThe remaining architecture essentially follows the Trans-\nformer. We use feature Q(r) corresponding to the RPN\nproposal r, for dot-product attention over the K features,\nnormalized by\n√\nD (same as [44]), and use the result for\nweighted averaging ( A(r)) of V features. This operation\ncan be succinctly represented as\na(r)\nxyt = Q(r)KT\nxyt√\nD\n; A(r) =\n∑\nx,y,t\n[\nSoftmax\n(\na(r)\n)]\nxyt\nVxyt\nWe apply a dropout to A(r) and add it to the original query\nfeature. The resulting query is passed through a residual\nbranch consisting of a LayerNorm [2] operation, followed\nby a Feed Forward Network (FFN) implemented as a 2-\nlayer MLP and dropout. The ﬁnal feature is passed through\none more LayerNorm to get the updated query ( Q′′). Fig-\nure 2 (Tx unit) illustrates the unit architecture described\nabove, and can be represented as\nQ(r)′\n= LayerNorm\n(\nQ(r) + Dropout\n(\nA(r)\n))\nQ(r)′′\n= LayerNorm\n(\nQ(r)′\n+ Dropout\n(\nFFN\n(\nQ(r)′ )))\n3.3. I3D Head\nTo measure the importance of the context gathered by\nour Action Transformer head, we also built a simpler head\narchitecture that does not extract context. For this, we\nextract a feature representation corresponding to the RPN\nproposal from the feature map using a Spatio-Temporal\nRoIPool (ST-RoIPool) operation. It’s implemented by ﬁrst\nstretching the RP in time by replicating the box to form a\ntube. Then, we extract a feature representation from feature\nmap at each time point using the corresponding box from\nthe tube using the standard RoIPool operation [21], simi-\nlar to previous works [11]. The resulting features across\ntime are stacked to get a spatio-temporal feature map cor-\nresponding to the tube. It is then passed through the lay-\ners of the I3D network that were dropped from the trunk\n(i.e., Mixed 5a to Mixed 5c). The resulting feature map\nis then passed through linear layers for classiﬁcation and\nbounding box regression. Figure 3 illustrates this architec-\nture.\n3.4. Implementation Details\nWe develop our models in Tensorﬂow, on top of the TF\nobject detection API [21]. We use input spatial resolution\nof 400 ×400px and temporal resolution ( T) of 64. The\nRoIPool used for both the I3D and Action Transformer head\ngenerates a 14 ×14 output, followed by a max pool to get\na 7 ×7 feature map. Hence, the I3D head input ends up\nbeing 16 ×7 ×7 in size, while for Action Transformer we\nuse the 7 ×7 feature as query and the full 16 ×25 ×25\ntrunk feature as the context. As also observed in prior\nwork [32, 44], adding a location embedding in such archi-\ntectures is very beneﬁcial. It allows our model to encode\nspatiotemporal proximity in addition to visual similarity, a\nproperty lost when moving away from traditional convolu-\ntional or memory-based (eg. LSTM) architectures. For each\ncell in the trunk feature map, we add explicit location infor-\nmation by constructing vectors: [h, w] and [t] denoting the\nspatial and temporal location of that feature, computed with\nrespect to the size and relative to the center of the feature\nmap. We pass each through a 2-layer MLP, and concatenate\nthe outputs. We then attach the resulting vector to the trunk\nfeature map along channel dimension. Since K, Vare pro-\njections the trunk feature map, and Q is extracted from that\nfeature via RoIPool, all of these will implicitly contain the\nlocation embedding. Finally, for classiﬁcation loss, we use\nseparate logistic losses for each action class, implemented\nusing sigmoid cross-entropy, since multiple actions can be\nactive for a given person. For regression, we use the stan-\ndard smooth L1 loss. For the Action Transformer heads,\nwe use feature dimensionality of D = 128and dropout of\n0.3. We use a 2-head, 3-layer setup for the Action Trans-\nformer units by default, though we ablate other choices in\nSection 4.5.\n3.5. Training Details\nPre-training: We initialize most of our models by pre-\ntraining the I3D layers separately on the large, well-labeled\naction classiﬁcation dataset Kinetics-400 [26] as described\nin [7]. We initialize the remaining layers of our model (eg.\nRPN, Action Transformer heads etc) from scratch, ﬁx the\nrunning mean and variance statistics of batch norm layers\nto the initialization from the pre-trained model, and then\nﬁnetune the full model end-to-end. Note that the only batch\nnorm layers in our model are in the I3D base and head net-\nworks; hence, no new batch statistics need to be estimated\nwhen ﬁnetuning from the pretrained models.\nData Augmentation: We augment our training data using\nrandom ﬂips and crops. We ﬁnd this was critical, as remov-\ning augmentation lead to severe overﬁtting and a signiﬁcant\ndrop in performance. We evaluate the importance of pre-\ntraining and data augmentation in Section 4.5.\nSGD Parameters: The training is done using synchronized\nSGD over V100 GPUs with an effective batch size of 30\nclips per gradient step. This is typically realized by a per-\nGPU batch of 3 clips, and total of 10 replicas. However,\nsince we keep batch norm ﬁxed for all experiments except\nfor from-scratch experiments, this batch size can be realized\nby splitting the batch over 10, 15 or even 30 replicas for our\nheavier models. Most of our models are trained for 500K\niterations, which takes about a week on 10 GPUs. We use\na learning rate of 0.1 with cosine learning rate annealing\nover the 500K iterations, though with a linear warmup [15]\nfrom 0.01 to 0.1 for the ﬁrst 1000 iterations. For some cases,\nlike models with Action Transformer head and using ground\ntruth boxes (Section 4.2), we stop training early at 300K it-\nerations as it learns much faster. The models are trained\nusing standard loss functions used for object detection [21],\nexcept for sigmoid cross-entropy for the multi-label classi-\nﬁcation loss.\n4. Experiments\nIn this section we experimentally evaluate the model on\nthe A V A benchmark. We start with introducing the dataset\nand evaluation protocol in Section 4.1. Note that the model\nis required to carry out two distinct tasks: action localiza-\ntion and action classiﬁcation. To better understand the chal-\nlenge of each independently, we evaluate each task given\nperfect information for the other. In Section 4.2, we re-\nplace the RPN proposals with the groundtruth (GT) boxes,\nand keep the remaining architecture as is. Then in Sec-\ntion 4.3, we assume perfect classiﬁcation by converting all\nclass labels into a single ‘active’ class label, reducing the\nproblem into a pure ‘active person’ vs background detec-\ntion problem, and evaluate the person localization perfor-\nmance. Finally we put the lessons from the two together in\nSection 4.4. We perform all these ablative comparisons on\nthe A V A validation set, and compare with the state of the art\non the test set in Section 4.6.\n4.1. The A V A Dataset and Evaluation\nThe Atomic Visual Actions (A V A) v2.1 [16] dataset con-\ntains 211K training, 57K validation and 117K testing clips,\ntaken at 1 FPS from 430 15-minute movie clips. The cen-\nter frame in each clip is exhaustively labeled with all the\nperson bounding boxes, along with one or more of the 80\naction classes active for each instance. Following previous\nTrunk Head QPr GT Boxes Params (M) GFlops Val mAP\nI3D I3D - 16.2 6.5 21.3\nI3D I3D - ✓ 16.2 6.5 23.4\nI3D Tx LowRes 13.9 33.2 17.8\nI3D Tx HighRes 19.3 39.6 18.9\nI3D Tx LowRes ✓ 13.9 33.2 29.1\nI3D Tx HighRes ✓ 19.3 39.6 27.6\nTable 1: Action classiﬁcation with GT person boxes. To iso-\nlate classiﬁcation from localization performance, we evaluate our\nmodels when assuming groundtruth box locations are known. It\ncan be seen that the Action Transformer head has far stronger per-\nformance than the I3D head when GT boxes are used. All perfor-\nmance reported with R = 64 proposals. To put the complexity\nnumbers into perspective, a typical video recognition model, 16-\nframe R(2+1)D network on Kinetics, is 41 GFlops [43]. For a\nsense of random variation, we retrain the basic Tx model (line 5)\nthree times, and get a std deviation of 0.45 (on an mAP of 29.1).\nworks [16, 42], we report our performance on the subset of\n60 classes that have at least 25 validation examples. For\ncomparison with other challenge submissions, we also re-\nport the performance of our ﬁnal model on the test set, as\nreported from the challenge server. Unless otherwise spec-\niﬁed, the evaluation is performed using frame-level mean\naverage precision (frame-AP) at IOU threshold of 0.5, as\ndescribed in [16].\n4.2. Action classiﬁcation given GT person boxes\nIn this section we assess how well the head can classify\nthe actions, given the ground truth bounding boxes provided\nwith the A V A dataset. This will give an upper bound on the\naction classiﬁcation performance of the entire network, as\nthe RPN is likely to be less perfect than ground truth. We\nstart by comparing the I3D head with and without GT boxes\nin Table 1. We use a lower value of R = 64for the RPN,\nin order to reduce the computational expense of these ex-\nperiments. It is interesting to note that we only get a small\nimprovement by using groundtruth (GT) boxes, indicating\nthat our model is already capable of learning a good rep-\nresentation for person detection. Next, we replace the I3D\nhead architecture with the Action Transformer, which leads\nto a signiﬁcant 5% boost for the GT boxes case. It is also\nworth noting that our Action Transformer head implementa-\ntion actually has 2.3M fewer parameters than the I3D head\nin the LowRes QPr case, dispelling any concerns that this\nimprovement is simply from additional model capacity. The\nsigniﬁcant drop in performance with and without GT boxes\nfor the Action Transformer is due to only using R = 64\nproposals. As will be seen in subsequent results, this drop\nis eliminated when the full model with R = 300proposals\nis used.\nRoI source QPr Head Val mAP\nIOU@0.5 IOU@0.75\nRPN - I3D 92.9 77.5\nRPN LowRes Tx 77.5 43.5\nRPN HighRes Tx 87.7 63.3\nTable 2: Localization performance (action agnostic). We\nperform classiﬁcation-agnostic evaluation to evaluate the perfor-\nmance of the heads for person detection. We observe that the I3D\nhead is superior to Action Transformer-head model, though using\nthe HighRes query transformation (QPr) improves it signiﬁcantly.\nAll performance reported with R = 64proposals.\nHead QPr #proposals Val mAP\nI3D - 64 21.3\nI3D - 300 20.5\nTx HighRes 64 18.9\nTx HighRes 300 24.4\nTx+I3D HighRes 300 24.9\nTable 3:Overall performance.Putting\nthe Action Transformer head with\nHighRes preprocessing and 300 pro-\nposals leads to a signiﬁcant improve-\nment over the I3D head. Using both\nheads: I3D for regression and Tx for\nclassiﬁcation performs best.\n4.3. Localization performance (action agnostic)\nGiven the strong performance of the Action Transformer\nfor the classiﬁcation task, we look now in detail to the lo-\ncalization task. As described previously, we isolate the lo-\ncalization performance by merging all classes into a single\ntrivial one. We report performance in Table 2, both with\nthe standard 0.5 IOU threshold, and also with a stricter 0.75\nIOU threshold.\nThe I3D head with RPN boxes excels on this task,\nachieving almost 93% mAP at 0.5 IOU. The naive imple-\nmentation of the transformer using a low-resolution query\ndoes quite poorly at 77.5%, but by adopting the high-\nresolution query, the gap in performance is considerably\nreduced (92.9% to 87.7%, for the IOU-0.5 metric). The\ntransformer is less accurate for localization and this can be\nunderstood by its more global nature; additional research on\nthis problem is warranted. However as we will show next,\nusing the HighRes query we can already achieve a positive\ntrade-off in performance and can leverage the classiﬁcation\ngains to obtain a signiﬁcant overall improvement.\n4.4. Putting things together\nNow we put the transformer head together with the RPN\nbase, and apply the entire network to the tasks of detection\nand classiﬁcation. We report our ﬁndings in Table 3. It can\nbe seen that the Action Transformer head is far superior to\nthe I3D head (24.4 compared to 20.5). An additional boost\ncan be obtained (to 24.9) by using the I3D head for regres-\nsion and the Action Transformer head for classiﬁcation –\nreﬂecting their strengths identiﬁed in the previous sections\n– albeit at a slightly higher (0.1GFlops) computational over-\nhead.\nI3D\nhead\nCls-speciﬁc\nbbox-reg\nNo\nData Aug\nFrom\nScratch\nVal mAP 21.3 19.2 16.6 19.1\nTable 4: Augmentation, pre-training and class-agnostic regres-\nsion. We evaluate the importance of certain design choices such\nas class agnostic box regression, data augmentation and Kinetics\npre-training, by reporting the performance when each of those is\nremoved from the model. We use the I3D head model as the base-\nline. Clearly, removing any leads to a signiﬁcant drop in perfor-\nmance. All performance reported with R = 64proposals.\n4.5. Ablation study\nAll our models so far have used class agnostic regression,\ndata augmentation and Kinetics [26] pre-training, tech-\nniques we observed early on to be critical for good per-\nformance on this task. We now validate the importance of\nthose design choices. We compare the performance using\nthe I3D head network as the baseline in Table 4. As evident\nfrom the table, all three are crucial in getting strong per-\nformance. In particular, class agnostic regression is an im-\nportant contribution. While typical object detection frame-\nworks [17, 21] learn a separate regression layers for each\nobject category, it does not make sense in our case as the\n‘object’ is always a human. Sharing those parameters helps\nclasses with few examples to also learn a good person re-\ngressor, leading to an overall boost. Finally, we note the\nimportance of using a sufﬁcient number of proposals in the\nRPN. As can be seen in Table 3, reducing the number from\n300 to 64 decreases performance signiﬁcantly for the Ac-\ntion Transformer model. The I3D head is less affected. It\nis interesting because, even for 64, we are using far more\nproposals than the actual number of people in the frame.\nNumber of heads/layers in Action Transformer: Our\nAction Transformer architecture is designed to be easily\nstacked into multiple heads per layer, and multiple layers,\nsimilar to the original unit [44]. We evaluate the effect of\nchanging the number of heads and layers in Table 5. We\nﬁnd the performance to be largely similar, though tends to\nget slightly better with more layers and fewer heads. Hence,\nwe stick with our default 2-head 3-layer model for all exper-\niments reported in the paper.\nSwapping out the trunk architecture: As we observe in\nTable 6, our model is compatible with different trunk archi-\ntectures. We use I3D for all experiments in the paper given\nits speed and strong performance.\n4.6. Comparison with existing state of the art\nFinally, we compare our models to the previous state of\nthe art on the test set in Table 7. We ﬁnd the Tx+I3D head\nobtains the best performance, and simply adding temporal\ncontext at test time (96 frames compared to 64 frames at\n#layers↓ #heads→ 2 3 6\n2 27.4 28.7 27.6\n3 28.5 28.8 27.7\n6 29.1 28.3 26.5\nTable 5: Ablating the number of heads and layers. We ﬁnd\nfewer heads and more layers tends to give slightly better vali-\ndation mAP. All performance reported with Action Transformer\nhead, when using GT boxes as proposals.\nTrunk Head QPr GT Boxes Params (M) Val mAP\nI3D I3D - 16.2 21.3\nI3D I3D - ✓ 16.2 23.4\nI3D Tx LowRes ✓ 13.9 28.5\nR3D [47] Tx LowRes ✓ 17.7 26.6\nR3D + NL [47] Tx LowRes ✓ 25.1 27.2\nTable 6: Different trunk architectures. Our model is compatible\nwith different trunk architectures, such as R3D or Non-Local net-\nwork proposed in [47]. We observed best performance with I3D,\nso use it for all experiments in the paper.\nMethod Modalities Architecture Val mAP Test mAP\nSingle frame [16] RGB, Flow R-50, FRCNN 14.7 -\nA V A baseline [16] RGB, Flow I3D, FRCNN, R-50 15.6 -\nARCN [42] RGB, Flow S3D-G, RN 17.4 -\nFudan University - - - 17.16\nYH Technologies [52] RGB, Flow P3D, FRCNN - 19.60\nTsinghua/Megvii [23] RGB, Flow I3D, FRCNN, NL, TSN,\nC2D, P3D, C3D, FPN - 21.08\nOurs (Tx-only head) RGB I3D, Tx 24.4 24.30\nOurs (Tx+I3D head) RGB I3D, Tx 24.9 24.60\nOurs (Tx+I3D+96f) RGB I3D, Tx 25.0 24.93\nTable 7: Comparison with previous state of the art and chal-\nlenge submissions. Our model outperforms the previous state of\nthe art by > 7.5% on the validation set, and the CVPR’18 chal-\nlenge winner by > 3.5% on the test set. We do so while only using\na single model (no ensembles), running on raw RGB frames as in-\nput. This is in contrast to the various previous methods listed here,\nwhich use various modalities and ensembles of multiple architec-\ntures. The model abbreviations used here refer to the following. R-\n50: ResNet-50 [18], I3D: Inﬂated 3D convolutions [7], S3D(+G):\nSeparable 3D convolutions (with gating) [49], FRCNN: Faster R-\nCNN [34], NL: Non-local networks [47], P3D: Pseudo-3D convo-\nlutions [33], C2D [43], C3D [43], TSN: Temporal Segment Net-\nworks [46] RN: Relation Nets [35], Tx: Transformer [32, 44] and\nFPN: Feature Pyramid Networks [29]. Some of the submissions\nalso attempted to use other modalities like audio, but got lower\nperformance. Here we compare with their best reported numbers.\ntraining) leads to a further improvement. We outperform the\nprevious state of the art by more than 7.5% absolute points\non validation set, and the CVPR 2018 challenge winner by\nmore than 3.5%. It is also worth noting that our approach\nis much simpler than most previously proposed approaches,\nespecially the challenge submissions that are ensembles of\nmultiple complex models. Moreover, we obtain this per-\nformance only using raw RGB frames as input, while prior\nworks use RGB, ﬂow, and in some cases audio as well.\n5. Analysis\nWe now analyze the Action Transformer model. Apart\nfrom obtaining superior performance, this model is also\nmore interpretable by explicitly encoding bottom up atten-\ntion. We start by visualizing the key/value embeddings and\nattention maps learned by the model. Next we analyze\nthe performance vis-a-vis speciﬁc classes, person sizes and\ncounts; and ﬁnally visualize common failure modes.\nLearned embeddings and attention: We visualize the\n128D ‘key’ embeddings and attention maps in Figure 4. We\nvisualize the embeddings by color-coding a 3D PCA projec-\ntion. We show two heads out of the six in our 2-head 3-layer\nAction Transformer model. For attention maps, we visual-\nize the average softmax attention over the 2 heads in the last\nlayer of our Tx head. It is interesting to note that our model\nlearns to track the people over the clips, as shown from\nthe embeddings where all ‘person’ pixels are same color.\nMoreover, for the ﬁrst head all humans have the same color,\nsuggesting a semantic embedding, while the other has dif-\nferent, suggesting an instance-level embedding. Similarly,\nthe softmax attention maps learn to attend and track faces,\nhands and other parts of the person of interest as well as the\nother people in the scene. It also tends to attend to objects\nthe person interacts with, like the vaccum cleaner and cof-\nfee mugs. This makes sense as many actions in A V A such\nas talking, listening, hold an object etc. require focusing the\nfaces, hands of people and objects to deduce. A video visu-\nalization of the embeddings, attention maps and predictions\nare provided in the supplementary video [10].\nBreaking down the performance: We now break down\nthe performance of our model into certain bins. We start\nby evaluating the performance per class in Figure 5 (a). We\nsort the performance according the increasing amounts of\ntraining data, shown in green. While there is some corre-\nlation between the training data size and performance, we\nnote that there exist many classes with enough data but poor\nperformance, like smoking. We note that we get some of the\nlargest improvement in classes such as sailing boat, watch-\ning TV etc, which would beneﬁt from our Action Trans-\nformer model attending to the context of the person. Next,\nwe evaluate the performance with respect to the size of the\nperson in the clip, deﬁned by the percentage area occupied\nby the GT box, in Figure 5 (b). For this, we split the valida-\ntion set into bins, keeping predictions and GT within certain\nsize limits. We ﬁnd the size thresholds by sorting all the GT\nboxes and splitting into similar sized bins, hence ensuring\nsimilar ‘random’ performance for each bin. We ﬁnd perfor-\nmance generally increases with bigger boxes, presumably\nFrame Tx-A Tx-B Frame Tx-A Tx-B Attention\nFigure 4: Embedding and attention. For two frames, we show their ‘key’ embeddings as color-coded 3D PCA projection for two of the\nsix heads in our 2-head 3-layer Tx head. It is interesting to note that one of these heads learns to track people semantically (Tx-A: all upper\nbodies are similar color – green), while the other is instance speciﬁc (Tx-B: each person is different color – blue, pink and purple). In the\nfollowing columns we show by the average softmax attention corresponding to the person in the red box for all heads in the last Tx layer.\nOur model learns to hone in on faces, hands and objects being interacted with, as these are most discriminative for recognizing actions.\n(a)\npoint to (an object)\nswim\nturn (e.g., a screwdriver)\nhit (an object)\nwork on a computer\ncut\ntake a photo\nenter\nshoot\njump/leap\nclimb (e.g., a mountain)\nthrow\nfall down\ndress/put on clothing\nhand wave\npull (an object)\npush (another person)\nlift (a person)\npush (an object)\ntext on/look at a cellphone\nlift/pick up\nput down\nlisten (e.g., to music)\ntake (an object) from (a person)\nhand shake\nsail boat\nkiss (a person)\nwrite\nclose (e.g., a door, a box)\ngive/serve (an object) to (a person)\nhug (a person)\nget up\ndrive (e.g., a car, a truck)\nhand clap\nopen (e.g., a window, a car door)\nplay musical instrument\nsing to (e.g., self, a person, a group)\ngrab (a person)\nwatch (e.g., TV)\nread\nmartial art\ndrink\ncrouch/kneel\nfight/hit (a person)\nsmoke\neat\ndance\nanswer phone\nrun/jog\nride (e.g., a bike, a car, a horse)\nlie/sleep\nbend/bow (at the waist)\ntouch (an object)\nwalk\ncarry/hold (an object)\nsit\nlisten to (a person)\ntalk to (e.g., self, a person, a group)\nwatch (a person)\nstand\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9mAP\nI3D Head\nTx Head\n101\n102\n103\n104\n105\n106\nTrain set size\n(b)\n0.07 8.11 17.11 29.24 47.20\n% image area covered by box\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25mAP\nI3D Head\nTx Head\n0\n5\n10\n15\n20\nNum GT boxes\n+4.735e4\n(c)\n0 1 2 3 5 7\nNumber of boxes in a clip\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35mAP\nI3D Head\nTx Head\n5000\n6000\n7000\n8000\n9000\n10000\n11000\n12000\n13000\nNum clips\nFigure 5: Performance by (a) class, (b) box area and (c) count. While overall trend suggests a positive correlation of performance with\ntrain-set size (green line), there do exist interesting anomalies such as ‘smoking’, ‘eating’ etc, which are still hard to recognize despite\nsubstantial training data. In (b) and (c) the green line denotes the validation subset size. We observe the performance largely improves as\nthe person box size increases, and as number of boxes decreases. Axis labels best viewed zoomed in on screen.\nbecause it becomes progressively easier to see what the per-\nson is doing up close. Finally, we evaluate the performance\nwith respect to the number of GT boxes labeled in a clip\nin Figure 5 (c). We ﬁnd decreasing performance as we add\nmore people in a scene.\nQualitative Results: We visualize some successes of our\nmodel in Figure 6. Our model is able to exploit the context\nto recognize actions such as ‘watching a person’, which are\ninherently hard when just looking at the actor. Finally, we\nanalyze some common failure modes of our best model in\nFigure 7. The columns show some common failure modes\nlike (a) similar action/interaction, (b) identity and (c) tem-\nporal position. A similar visualization of top predictions on\nthe validation set for each class, sorted by conﬁdence, is\nprovided in [9].\n6. Conclusion\nWe have shown that the Action Transformer network is\nable to learn spatio-temporal context from other human ac-\ntions and objects in a video clip to localize and classify hu-\nman actions. The resulting embeddings and attention maps\n(learned indirectly as part of the supervised action training)\nhave a semantic meaning. The network exceeds the state-\nof-the-art on the A V A dataset by a signiﬁcant margin. It\nis worth noting that previous state-of-the-art networks have\nused a motion/ﬂow stream in addition to RGB [7, 49], so\nadding ﬂow as input is likely to boost performance also\nfor the Action Transformer network. Nevertheless, perfor-\nmance is far from perfect, and we have suggested several\navenues for improvement and investigation.\nAcknowledgements: We would like to thank V . Patraucean, R.\nArandjelovi´c, J.-B. Alayrac, A. Arnab, M. Malinowski and C. Mc-\nCoy for helpful discussions.\nCarry/hold (an object)\nFight/hit (a person)\nWatch (a person)\nFigure 6: Top predictions. Example top predictions for some\nof the classes using our model. Note that context, such as other\npeople or objects being interacted with, is often helpful for the\nclassifying actions like ‘watching a person’, ‘holding an object’\nand so on. Capturing context is a strength of our model.\n(a) (b) (c)\nFigure 7: Misclassiﬁed videos. Videos from the ‘smoking’ class\nthat obtains low performance even with large amount of training\ndata. Failure modes include, (a) Similar action/interaction: In the\nﬁrst clip, the person has his hand on his mouth, similar to a smoker;\nand in the second, the mic looks like a cigarette; (b)Identity: There\nare multiple people (or reﬂections) and the action is not being as-\nsigned to the correct person; (c) Temporal position: The dataset\nexpects the action to be occurring in the key frame, in these exam-\nples the action has either ﬁnished or not started by the key frame.\nReferences\n[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,\nB. Varadarajan, and S. Vijayanarasimhan. Youtube-8m:\nA large-scale video classiﬁcation benchmark. CoRR,\nabs/1609.08675, 2016.\n[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization.\nStat, 2016.\n[3] F. Baradel, N. Neverova, C. Wolf, J. Mille, and G. Mori.\nObject level visual reasoning in videos. In ECCV, 2018.\n[4] F. Baradel, C. Wolf, and J. Mille. Human action recogni-\ntion: Pose-based attention draws focus to hands. In ICCV\nWorkshop, 2017.\n[5] F. Baradel, C. Wolf, and J. Mille. Human activity recognition\nwith pose-driven attention to rgb. In BMVC, 2018.\n[6] F. Caba et al. Activitynet leaderboard. spatio-temporal\naction localization (ava-1. computer vision only).\nhttp://activity-net.org/challenges/2018/\nevaluation.html.\n[7] J. Carreira and A. Zisserman. Quo Vadis, Action Recogni-\ntion? A new model and the kinetics dataset. In CVPR, 2017.\n[8] K. Duarte, Y . S. Rawat, and M. Shah. VideoCapsuleNet: A\nsimpliﬁed network for action detection. In NIPS, 2018.\n[9] R. Girdhar, J. Carreira, C. Doersch, and A. Zis-\nserman. ActionTransformer: Per-class top predic-\ntions. https://rohitgirdhar.github.io/\nActionTransformer/assets/suppl/pred.pdf.\n[10] R. Girdhar, J. Carreira, C. Doersch, and A. Zis-\nserman. ActionTransformer: Supplementary\nvideo. https://rohitgirdhar.github.io/\nActionTransformer/assets/suppl/combined.\nmp4.\n[11] R. Girdhar, G. Gkioxari, L. Torresani, M. Paluri, and D. Tran.\nDetect-and-Track: Efﬁcient Pose Estimation in Videos. In\nCVPR, 2018.\n[12] R. Girdhar and D. Ramanan. Attentional pooling for action\nrecognition. In NIPS, 2017.\n[13] R. Girdhar, D. Ramanan, A. Gupta, J. Sivic, and B. Russell.\nActionVLAD: Learning spatio-temporal aggregation for ac-\ntion classiﬁcation. In CVPR, 2017.\n[14] L. Gorelick, M. Blank, E. Shechtman, M. Irani, and R. Basri.\nActions as space-time shapes. TPAMI, 2007.\n[15] P. Goyal, P. Doll ´ar, R. Girshick, P. Noordhuis,\nL. Wesolowski, A. Kyrola, A. Tulloch, Y . Jia, and K. He.\nAccurate, large minibatch sgd: training imagenet in 1 hour.\narXiv preprint arXiv:1706.02677, 2017.\n[16] C. Gu, C. Sun, D. Ross, C. V ondrick, C. Pantofaru, Y . Li,\nS. Vijayanarasimhan, G. Toderici, S. Ricco, R. Sukthankar,\nC. Schmid, and J. Malik. A V A: A video dataset of spatio-\ntemporally localized atomic visual actions. In CVPR, 2018.\n[17] K. He, G. Gkioxari, P. Doll ´ar, and R. Girshick. Mask R-\nCNN. In ICCV, 2017.\n[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In CVPR, 2016.\n[19] R. Herzig, E. Levi, H. Xu, E. Brosh, A. Globerson, and\nT. Darrell. Classifying collisions with spatio-temporal action\ngraph networks. arXiv preprint arXiv:1812.01233, 2018.\n[20] R. Hou, C. Chen, and M. Shah. Tube convolutional neu-\nral network (t-cnn) for action detection in videos. In ICCV,\n2017.\n[21] J. Huang, V . Rathod, C. Sun, M. Zhu, A. Korattikara,\nA. Fathi, I. Fischer, Z. Wojna, Y . Song, S. Guadarrama, and\nK. Murphy. Speed/accuracy trade-offs for modern convolu-\ntional object detectors. In CVPR, 2017.\n[22] H. Jhuang, J. Gall, S. Zufﬁ, C. Schmid, and M. J. Black.\nTowards understanding action recognition. In ICCV, 2013.\n[23] J. Jiang, Y . Cao, L. Song, S. Zhang, Y . Li, Z. Xu, Q. Wu,\nC. Gan, C. Zhang, and G. Yu. Human centric spatio-temporal\naction localization. Technical report, Tsinghua University,\n2018.\n[24] V . Kalogeiton, P. Weinzaepfel, V . Ferrari, and C. Schmid.\nAction tubelet detector for spatio-temporal action localiza-\ntion. In ICCV, 2017.\n[25] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,\nand L. Fei-Fei. Large-scale video classiﬁcation with convo-\nlutional neural networks. In CVPR, 2014.\n[26] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier,\nS. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev,\nM. Suleyman, and A. Zisserman. The kinetics human action\nvideo dataset. arXiv preprint arXiv:1705.06950, 2017.\n[27] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.\nHMDB: a large video database for human motion recogni-\ntion. In ICCV, 2011.\n[28] I. Laptev. On space-time interest points. IJCV, 2005.\n[29] T.-Y . Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and\nS. Belongie. Feature pyramid networks for object detection.\nIn CVPR, 2017.\n[30] X. Long, C. Gan, G. de Melo, J. Wu, X. Liu, and S. Wen.\nAttention clusters: Purely attention based local feature inte-\ngration for video classiﬁcation. In CVPR, 2018.\n[31] A. Miech, I. Laptev, and J. Sivic. Learnable pooling with\ncontext gating for video classiﬁcation. arXiv:1706.06905,\n2017.\n[32] N. Parmar, A. Vaswani, J. Uszkoreit, Ł. Kaiser, N. Shazeer,\nand A. Ku. Image transformer. In ICML, 2018.\n[33] Z. Qiu, T. Yao, and T. Mei. Learning spatio-temporal repre-\nsentation with pseudo-3d residual networks. In ICCV, 2017.\n[34] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-\nwards real-time object detection with region proposal net-\nworks. In NIPS, 2015.\n[35] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski,\nR. Pascanu, P. Battaglia, and T. Lillicrap. A simple neural\nnetwork module for relational reasoning. In NIPS, 2017.\n[36] C. Schuldt, I. Laptev, and B. Caputo. Recognizing human\nactions: a local svm approach. In ICPR, 2004.\n[37] S. Sharma, R. Kiros, and R. Salakhutdinov. Action recogni-\ntion using visual attention. ICLR-Workshops, 2016.\n[38] G. A. Sigurdsson, S. Divvala, A. Farhadi, and A. Gupta.\nAsynchronous temporal ﬁelds for action recognition. In\nCVPR, 2017.\n[39] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev,\nand A. Gupta. Hollywood in homes: Crowdsourcing data\ncollection for activity understanding. In ECCV, 2016.\n[40] G. Singh, S. Saha, M. Sapienza, P. Torr, and F. Cuzzolin.\nOnline real-time multiple spatiotemporal action localisation\nand prediction. In ICCV, 2017.\n[41] K. Soomro, A. R. Zamir, and M. Shah. UCF101: A dataset\nof 101 human actions classes from videos in the wild.CRCV-\nTR-12-01, 2012.\n[42] C. Sun, A. Shrivastava, C. V ondrick, K. Murphy, R. Suk-\nthankar, and C. Schmid. Actor-centric relation network. In\nECCV, 2018.\n[43] D. Tran, H. Wang, L. Torresani, J. Ray, Y . LeCun, and\nM. Paluri. A closer look at spatiotemporal convolutions for\naction recognition. In CVPR, 2018.\n[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all\nyou need. In NIPS, 2017.\n[45] H. Wang and C. Schmid. Action recognition with improved\ntrajectories. In ICCV, 2013.\n[46] L. Wang, Y . Xiong, Z. Wang, Y . Qiao, D. Lin, X. Tang, and\nL. Van Gool. Temporal segment networks: Towards good\npractices for deep action recognition. In ECCV, 2016.\n[47] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural\nnetworks. In CVPR, 2018.\n[48] X. Wang and A. Gupta. Videos as space-time region graphs.\nIn ECCV, 2018.\n[49] S. Xie, C. Sun, J. Huang, Z. Tu, and K. Murphy. Rethinking\nspatiotemporal feature learning for video understanding. In\nECCV, 2018.\n[50] H. Xu, A. Das, and K. Saenko. R-C3D: Region convolutional\n3D network for temporal activity detection. In ICCV, 2017.\n[51] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudi-\nnov, R. Zemel, and Y . Bengio. Show, attend and tell: Neural\nimage caption generation with visual attention. In ICML,\n2015.\n[52] T. Yao and X. Li. YH Technologies at ActivityNet Challenge\n2018. arXiv preprint arXiv:1807.00686, 2018."
}