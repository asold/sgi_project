{
    "title": "Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization",
    "url": "https://openalex.org/W4411171737",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5116142045",
            "name": "Ajwad Abrar",
            "affiliations": [
                "Islamic University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2303455967",
            "name": "Farzana Tabassum",
            "affiliations": [
                "Islamic University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2148954323",
            "name": "Sabbir Ahmed",
            "affiliations": [
                "Islamic University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2899158660",
        "https://openalex.org/W2345785790",
        "https://openalex.org/W2963929190",
        "https://openalex.org/W4287888679",
        "https://openalex.org/W4386566460",
        "https://openalex.org/W6856933413",
        "https://openalex.org/W4390542472",
        "https://openalex.org/W3035032094",
        "https://openalex.org/W4385572377",
        "https://openalex.org/W4385572482",
        "https://openalex.org/W4402923149",
        "https://openalex.org/W6860710830",
        "https://openalex.org/W4392679398",
        "https://openalex.org/W6871553294",
        "https://openalex.org/W4401306979",
        "https://openalex.org/W4389518188",
        "https://openalex.org/W2964061924",
        "https://openalex.org/W3042185737",
        "https://openalex.org/W3184094838",
        "https://openalex.org/W4223492536",
        "https://openalex.org/W4401943459",
        "https://openalex.org/W4411113139",
        "https://openalex.org/W4391973028",
        "https://openalex.org/W6850627172",
        "https://openalex.org/W6870610294",
        "https://openalex.org/W6682631176",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W4330337479",
        "https://openalex.org/W4387074787",
        "https://openalex.org/W4400024734"
    ],
    "abstract": "Consumer Health Queries (CHQs) in Bengali (Bangla), a low-resource language, often contain extraneous details, complicating efficient medical responses. This study investigates the zero-shot performance of nine advanced large language models (LLMs): GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet, Llama3-70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro, Qwen2-72b-Instruct, Gemma-2-27b, and Athene-70B, in summarizing Bangla CHQs. Using the BanglaCHQ-Summ dataset comprising 2,350 annotated query-summary pairs, we benchmarked these LLMs using ROUGE metrics against Bangla T5, a fine-tuned state-of-the-art model. Mixtral-8x22b-Instruct emerged as the top performing model in ROUGE-1 and ROUGE-L, while Bangla T5 excelled in ROUGE-2. The results demonstrate that zero-shot LLMs can rival fine-tuned models, achieving high-quality summaries even without task-specific training. This work underscores the potential of LLMs in addressing challenges in low-resource languages, providing scalable solutions for healthcare query summarization.",
    "full_text": "2024 27th International Conference on Computer and Information Technology (ICCIT)\n20-22 December 2024, Cox’s Bazar, Bangladesh\nPerformance Evaluation of Large Language\nModels in Bangla Consumer Health Query\nSummarization\nAjwad Abrar, Farzana Tabassum, Sabbir Ahmed\nDepartment of Computer Science and Engineering, Islamic University of Technology, Gazipur, Bangladesh\nEmail: {ajwadabrar, farzana, sabbirahmed}@iut-dhaka.edu\nAbstract—Consumer Health Queries (CHQs) in Ben-\ngali (Bangla), a low-resource language, often contain\nextraneous details, complicating efficient medical re-\nsponses. This study investigates the zero-shot perfor-\nmance of nine advanced large language models (LLMs):\nGPT-3.5-T urbo, GPT-4, Claude-3.5-Sonnet, Llama3-\n70b-Instruct, Mixtral-8x22b-Instruct, Gemini-1.5-Pro,\nQwen2-72b-Instruct, Gemma-2-27b, and Athene-70B, in\nsummarizing Bangla CHQs. Using the BanglaCHQ-\nSumm dataset comprising 2,350 annotated query-\nsummary pairs, we benchmarked these LLMs using\nROUGE metrics against Bangla T5, a fine-tuned state-\nof-the-art model. Mixtral-8x22b-Instruct emerged as\nthe top performing model in ROUGE-1 and ROUGE-\nL, while Bangla T5 excelled in ROUGE-2. The results\ndemonstrate that zero-shot LLMs can rival fine-tuned\nmodels, achieving high-quality summaries even without\ntask-specific training. This work underscores the poten-\ntial of LLMs in addressing challenges in low-resource\nlanguages, providing scalable solutions for healthcare\nquery summarization.\nIndex Terms —Large Language Models (LLMs), Ben-\ngali Consumer Health Queries, Abstractive Summariza-\ntion, Zero-shot Learning, ROUGE Metrics Evaluation\nI. Introduction\nThe rapid expansion of online health consultation plat-\nforms has made Consumer Health Queries (CHQs) a\nprevalent form of medical inquiry. Patients frequently\nutilize these platforms to obtain advice, resulting in a\nconsiderable burden on healthcare professionals, who are\nrequired to respond manually to these inquiries [1]. A\nkey challenge posed by CHQs is the frequent inclusion\nof irrelevant or extraneous details, which complicates the\nidentification of critical information by medical profession-\nals [2]. Abstractive text summarization involves creating\na concise version of a text that preserves its essential\ninformation [3], presents a promising solution to this issue.\nIn recent years, several models tailored for Bengali,\nsuch as BanglaBERT [4] and BanglaT5 [5], have achieved\nimpressive results across various Bengali NLP tasks [6,\n7]. However, these models typically require fine-tuning on\nlarge, domain-specific annotated datasets, which are often\nunavailable for Bengali due to its underrepresentation in\nthe field of NLP [8]. Large language models (LLMs) have\nshown remarkable proficiency in generating high-quality\nabstractive summaries, reducing the need for extensive\nfine-tuning and domain-specific data [9]. This makes them\nparticularly beneficial for under-resourced languages like\nBengali, where the availability of annotated datasets is\nlimited.\nWhile LLMs have demonstrated substantial success\nin English and other Indo-European languages [10, 11],\ntheir capabilities in Bengali, particularly for special-\nized tasks like CHQ summarization, remain underex-\nplored.This study aims to fill this gap by evaluating\nthe zero-shot performance of nine state-of-the-art large\nlanguage models (LLMs): GPT-3.5-Turbo [ 12], GPT-\n4 [13], Claude-3.5-Sonnet [14], Llama3-70B-Instruct [15],\nMixtral-8x22B-Instruct-v0.1 [ 16], Gemini-1.5-Pro [ 17],\nQwen2-72B-Instruct [18], Gemma-2-27B [19], and Athene-\n70B [20] for the task of summarizing Bengali consumer\nhealth queries (CHQs). To the best of our knowledge, this\nis the first comprehensive study evaluating LLMs specif-\nically for summarization tasks in the Bengali language\nwithin the healthcare domain.\nII. Literature Review\nText summarization has garnered significant research\ninterest, leading to notable progress in both extractive\nand abstractive techniques [22, 23]. However, the task\nof summarizing CHQs has been inadequately addressed\nin existing literature. The limited research and resources\navailable in languages other than English underscore a no-\ntable deficiency in the literature, particularly regarding the\nadvantages of CHQ summarization for densely populated\nareas such as Bangladesh, where healthcare professionals\nfrequently face overwhelming demands [24].\nPrevious studies have demonstrated the potential of\nlarge language models (LLMs) in CHQ summarization\ntasks. For instance, Jahanet al. conducted a comparative\nanalysis between BioBART [25] and ChatGPT (GPT-3.5-\nturbo) in a zero-shot setting [ 10]. Their results indi-\ncated that ChatGPT outperformed BioBART, especially\nin cases where BioBART lacked domain-specific training\ndata, underscoring ChatGPT’s efficacy in low-resource\nenvironments. This emphasizes the capability of LLMs\nAccepted in 27th ICCIT, 2024\nT able I: Sample Summaries from the BanglaCHQ-Summ Dataset [21]\nOriginal Question Annotated Summary\nআমারবয়স৩০।অ েনকিদনধের�কামেররব্যথাক ের।আিমডাক্তার�দিখেয়িছ।\nএক্সের করার পর আমােক নাপ্রেক্সন ১০িদন ধের �খেত বেল িদেয়েছ। আর\nসােথক্যালিসয়াম,িনলটন�খেতিদেয়েছ১মাস। িকন্তুএপ য�ন্তব্যথাযায়নাই।\n�কামেরর�প্রসারিদেলব্যথাকের।এখন িককরাযায়।\nবয়স৩০। �কামেরব্যাথা।ডাক্তারএক্স েরকরেলন্যাপ্র�ক্সন\n১০িদন২ �বলা,ক্যাল িসয়ামও িনবসন১মাস �খেত�দয়।\nব্যাথাকেমিন।িককরনীয়?\nআমার আম্মুরিপেঠর ঠিক মােঝ ভীষণ জ্বেল প্রায় এক বসর ধের এমন হয়।\nডাক্তার�দখােনাহেয়েছশুধুগ্যাি�েকরওষুধ �দয়।িকন্তু�কানকাজহ য়না।এখন\nখুবজ্বেল৷মােঝমােঝএকটুকমথা েক।�কানিবেশষজ্ঞ�দখােলভালহয়?আর\nিকসমস্যাহেতপাের?িবেশষজ্ঞেদরপরামশ�চাই।\nিপেঠরঠিকমা েঝভীষণজ্বেল, ১বছরধের।ডাক্তারগ্যা-\nি�েকরওষুধ �দয়, �কানকাজহয়না।\nআমার উচ্চতা ৫ . ৯ \" ( ৬০�ক�জ ) । মাস খােনক আেগ আমার হালকা\nপাইলেসরসমস্যাহ েয়িছেলা।িনয়মতা��কখাওয়াদাওয়াআর �হািমওওষেধ\nকমএখন।আ িমএখনজানেতচা��এমতাবস্থায়কৃিমনাশকট্যাবেলট�খেত\nপারেবািকনাএবংকৃ িমনাশকট্যাবেলটখাওয়ার িনয়মিক? আিমগত৪/৫\nমােসকৃিমনাশকট্যাবেলটখায়িন।\nহালকা পাইলেসর সমস্যািছল । এখন কৃিমনাশক ট্যাবেলট\nখাওয়াযােবিকএবংখাওয়ার িনয়মিক?\nin summarization tasks, particularly in contexts where\nannotated datasets are limited, such as Bengali CHQs.\nFurther study has investigated the efficacy of LLMs\nin medical summarization tasks. Chen et al. [26] per-\nformed a systematic evaluation of open-source models,\nincluding Llama2 and Mistral, with GPT-4 acting as the\nassessor focusing on metrics such as coherence, fluency,\nand relevance. In parallel, Askari et al. [27] investi-\ngated paraphrasing tasks across various LLMs, including\nGPT-3.5-turbo, Llama2-13B, Mistral-7B, and Dolly-v2-\n7B, demonstrating the varying performance of models\nbased on task complexity and size.\nJahan et al. [28] evaluated the performance of four lead-\ning large language models—GPT-3.5, PaLM-2, Claude-\n2, and Llama-2—across six benchmark biomedical tasks,\nincluding summarization, utilizing the MeQSum dataset\nfor English CHQs. Although Claude-2 exhibited better\nperformance relative to other LLMs, it still fell short\nagainst the state-of-the-art BioBART model when ade-\nquate in-domain training data was present.\nDespite these advancements, there remains a lack of\ncomprehensive evaluation of LLMs for Bengali CHQ sum-\nmarization. This study addresses this gap by evaluating\nthe zero-shot performance of nine state-of-the-art LLMs\non Bengali CHQ summarization, offering insights into the\ncapabilities of these models for under-resourced languages\nin the healthcare domain.\nIII. Methodology\nThis study evaluates the effectiveness of LLMs in sum-\nmarizing Bangla consumer health queries in zero-shot\nsettings. To achieve this, we utilized the BanglaCHQ-\nSumm dataset and evaluated the performance of nine\nLLMs for the assessment.\nA. Dataset\nThe BanglaCHQ-Summ dataset, introduced in [21], is\nthe first publicly available dataset specifically designed for\nconsumer health query (CHQ) summarization in Bangla.\nIt comprises 2,350 pairs of questions and their correspond-\ning summaries. The data was sourced from a popular\nhealth forum1 frequented by Bangla speakers, consisting\nof representative health queries collected from a platform\nthat features user-submitted questions and certified med-\nical responses. Given that the data was sourced from a\npublic forum, it is reasonable to assume that the users\npossessed an average level of medical knowledge. The\ndataset covers 32 different health categories, ensuring a\nbroad range of health topics. Sample consumer health\nqueries along with their corresponding summaries are\npresented inTable I.\nB. Models\nOur selected nine LLMs for assessment can be catego-\nrized into proprietary and open-source models:\nProprietary Models:\n• GPT-3.5-T urbo: The model was built on the Trans-\nformer architecture and consists of 175 billion pa-\nrameters. It shows superior performance in large-\nscale NLP tasks. GPT-3.5-Turbo employs autoregres-\nsive decoding to generate coherent and fluent text,\ndemonstrating high efficacy in tasks such as question-\nanswering, summarization, and the comprehension of\ncomplex language patterns [29].\n• GPT-4: It further improves the parameter efficiency\nand fine-tuning, leading to enhanced context un-\nderstanding and refined text generation. It profi-\nciently addresses multi-step reasoning tasks and de-\nlivers accurate outputs in summarization, translation,\nand creative text generation, demonstrating improved\nalignment for superior, human-like interactions [13].\n• Claude-3.5-Sonnet: The emphasis is on producing\nnuanced and contextually aware responses for intri-\ncate inquiries. Utilizing around 52 billion parame-\nters, it employs reinforcement learning from human\nfeedback (RLHF) to enhance the alignment of its\n1https://daktarbhai.com/\nresponses, thereby demonstrating proficiency in gen-\nerating detailed and sensitive responses in complex\nconversational tasks [30].\n• Gemini-1.5-Pro: Developed by Google DeepMind,\nthe model incorporates advanced transformer opti-\nmization methods and multitask learning functional-\nities. It utilizes reinforcement learning from exter-\nnal knowledge sources, enhancing its adaptability for\ntasks including text classification, natural language\ninference, and conversational AI [17].\nF ree and Open-Source Models:\n• Llama3-70b-Instruct: The architecture comprises\n70 billion parameters, enabling it to manage complex\nreasoning, sequence processing, and instruction-based\ntasks, with particular proficiency in multi-turn dia-\nlogue and intricate problem-solving. Llama3-70b en-\nhances its transformer architecture for sophisticated\nNLP tasks, facilitating improved generalization across\ndiverse applications [31].\n• Mixtral-8x22b-Instruct: Utilizing eight groups of\n22 billion parameters each facilitates effective paral-\nlelization. The multitasking capabilities render it ap-\npropriate for instruction-following tasks across diverse\ndomains, while its computational efficiency ensures\nsuitability for scalable tasks without compromising\naccuracy [16].\n• Qwen2-72b-Instruct: It prioritizes the comprehen-\nsion of user intent and the optimization of tasks spe-\ncific to that intent. Featuring 72 billion parameters,\nthis model is engineered to improve performance in\ncontext-rich and task-specific environments. It em-\nphasizes natural language comprehension, enabling it\nto interpret nuanced inquiries and deliver contextually\npertinent responses [18].\n• Gemma-2-27b: Utilizing its 27 billion parameters\nto address complex and nuanced inquiries, its more\ncompact architecture relative to larger models enables\nspecialization in domain-specific sectors. Gemma-2-\n27b demonstrates significant efficacy in sectors includ-\ning law, medicine, and technical disciplines, where\naccuracy and specialized knowledge are essential [19].\n• Athene-70B: Athene-70B, optimized from Meta AI’s\nLlama-3-70B model by Nexusflow, aims to improve\nthe efficacy of the original architecture in practical\napplications. With 70 billion parameters, it has been\noptimized for enhanced generalization across various\ntasks, including conversational AI and advanced ana-\nlytics 2.\nC. Prompt Design and Evaluation\nWe designed a zero-shot learning framework to bench-\nmark the performance of these LLMs effectively. This\napproach allows us to evaluate how well these models\ngeneralize to domain-specific tasks without the benefit of\n2https://nexusflow.ai/blogs/athene\nfine-tuning, providing insights into their capabilities in\nsummarizing Bangla consumer health queries.\nInitially, we asked the LLMs to summarize the CHQs,\nbut some models (e.g., Gemma-2-27b, Gemini 1.5 Pro,\nLlama-3-70B-Instruct) responded in English instead of\nBangla. To correct this, we explicitly instructed them\nto generate responses in Bangla. Additionally, certain\nmodels (e.g., Claude-3.5-Sonnet, Athene-70B) provided\noverly detailed summaries, leading us to further refine the\nprompt to focus solely on the most essential information.\nAfter several iterations, the final optimized prompt was\ndeveloped, ensuring concise summaries in Bangla. Fi-\nnally, we used the following optimized prompt to obtain\nresponses from the LLMs to generate concise summaries\nof the input queries:\nPrompt: “Provide a concise summary of the\nfollowing long Bangla consumer health query.\nFocus on extracting only the critical and essential\ndetails, and ensure the output is a brief Bangla\nparagraph. Please avoid including any unneces-\nsary information beyond the core query. Note:\nKindly provide only the summarized output in\nBangla.\n[INPUT:] ”\nIV. Results and Discussion\nWe present the performance evaluation of various LLMs\nfor Bangla CHQ summarization, comparing their results\nagainst the current SOTA fine-tuned model, Bangla T5,\nas shown inTable II.\nA. Performance Evaluation\nTo evaluate the performance of the models on CHQ\nsummarization, we used ROUGE-1 (R1), ROUGE-2 (R2),\nand ROUGE-L (RL) metrics [ 32]. The evaluation re-\nsults indicate that LLMs like Mixtral-8x22B-Instruct-v0.1,\nGPT-4 and others, despite not being fine-tuned on the\nspecific task of Bangla CHQ summarization, perform\nremarkably well compared to the state-of-the-art model,\nBangla T5, which is a fine-tuned model on CHQs.\nFor instance, Mixtral-8x22B-Instruct-v0.1 outperforms\nBangla T5 achieving the highest scores in ROUGE-1\n(51.36) and ROUGE-L (49.17), while Bangla T5 leads\nin ROUGE-2 (29.11), a bi-gram overlap measure. Other\nmodels like GPT-4 and Llama-3-70B-Instruct also perform\ncompetitively across all metrics. However, various mod-\nels exhibited suboptimal performance in this assessment.\nAthene-70B demonstrates poor performance across all\nmetrics, recording an R1 score of 41.61 and the lowest\nROUGE-L score of 40.14 among the assessed models.\nSimilarly, Qwen2-72B-Instruct exhibits lower performance\nmetrics, notably in ROUGE-1 (44.24) and ROUGE-L\n(42.06), indicating difficulties with content overlap and\nsequence fluency in this summarization task.\nThe relatively lower ROUGE-2 (R2) scores for zero-\nshot LLMs compared to Bangla T5 can be attributed to\nT able II: Performance comparison between zero-shot LLMs and\nthe fine-tuned state-of-the-art model on the BanglaCHQ-Summ\ndataset. Best results are highlighted in bold.\nModel R1 R2 RL\nGPT-3.5 Turbo 45.17 10.56 44.33\nGPT-4 49.74 15.26 47.95\nClaude 3.5 Sonnet 45.72 11.79 43.88\nLlama-3-70B-Instruct 48.96 15.14 47.51\nMixtral-8x22B-Instruct-v0.1 51.36 14.41 49.17\nGemini 1.5 Pro 48.92 12.72 46.62\nQwen2-72B-Instruct 44.24 11.08 42.06\nGemma-2-27B 46.56 12.27 44.17\nAthene-70B 41.61 8.58 40.14\nBangla T5 [21] 50.05 29.11 48.35\nthe nature of bi-gram evaluation. ROUGE-2 emphasizes\nthe assessment of bi-gram overlaps, which are critical for\nevaluating a model’s capacity to produce word sequences\nthat maintain local context and coherence. Bangla T5, as\na fine-tuned model for, has received task-specific training\non the Bengali language, likely improving its capacity to\ngenerate bi-grams that align more closely with the ground\ntruth summaries. In contrast, the LLMs, while possessing\ngeneral language capabilities, do not exhibit fine-tuning\nfor this particular domain. Their performance on R2\ndeclines due to the lack of task-specific linguistic patterns\nand domain knowledge, resulting in less accurate bi-gram\npredictions and diminished local sequence consistency.\nB. Analysis of Summary Length and Quality\nWe further observe that GPT-4 and Qwen2-72B-\nInstruct tend to generate significantly longer summaries\n(37 words on average) compared to the gold reference\nsummaries (26 words on average). Interestingly, the best-\nperforming model, Mixtral-8x22B-Instruct-v0.1, produces\nsummaries with 33 words on average, slightly longer than\nthe reference but still concise (Table III). Another no-\ntable observation is that Llama-3-70B-Instruct generates\nmuch shorter summaries, with an average of 19 words,\nwhile achieving performance nearly on par with GPT-4 in\nterms of ROUGE scores. This suggests that Llama-3-70B-\nInstruct is capable of delivering high-quality summaries\nwith almost half the word count of GPT-4, highlighting\nits efficiency in text generation.\nThe evaluation of these LLMs in a zero-shot setting,\nwithout any task-specific fine-tuning, yet their close com-\npetition with a model fine-tuned specifically for Bangla,\nunderscores their impressive generalization capabilities.\nThis is particularly evident when examining the sum-\nmaries generated by Bangla T5, Mixtral-8x22B-Instruct-\nv0.1, and GPT-4 in Table IV. The outputs reveal that\nthese models produce remarkably similar summaries, ef-\nfectively capturing essential medical information. This\nconvergence in performance between fine-tuned models\nand LLMs, even in specialized domains such as bioinfor-\nmatics, suggests that the gap is narrowing. These findings\nhighlight the potential of large pre-trained models to excel\nT able III: Average word count of summarized text by different\nLLMs on the BanglaCHQ-Summ dataset.\nModel W ord Count\nGPT-3.5 Turbo 24\nGPT-4 37\nClaude 3.5 Sonnet 36\nLlama-3-70B-Instruct 19\nMixtral-8x22B-Instruct-v0.1 33\nGemini 1.5 Pro 31\nQwen2-72B-Instruct 37\nGemma-2-27B 26\nAthene-70B 23\nin specialized tasks like Bangla query summarization, even\nin the absence of task-specific training.\nC. Impact of Pronunciation Variability\nIn analyzing the performance of various LLMs on\nBangla text summarization, we observed that some sum-\nmaries, particularly those generated by LLMs, exhibited\nlower ROUGE scores. Upon further investigation, we\nidentified that pronunciation mismatches in Bangla were a\ncontributing factor. The same word was often represented\nwith different pronunciations in the reference (gold) sum-\nmary and the LLM-generated summaries, leading to score\ndiscrepancies. This issue highlights a potential limitation\nin the models’ handling of phonetic variations, which\ncould explain the reduced scores in certain cases. These\nfindings suggest that pronunciation variability in Bangla\nis a critical factor affecting LLM performance, warranting\nfurther research to better address these linguistic nuances\nand improve summarization accuracy.\nIn contrast, when we analyzed the top-performing mod-\nels—Mixtral-8x22B-Instruct-v0.1, GPT-4, Llama-3-70B-\nInstruct, and Gemini 1.5 Pro—based on the R1 score, we\nobserved a notable correlation between their performance\nand the word count of the generated summaries. Specif-\nically, in instances where these models performed better\ncollectively, the predicted summaries closely matched the\nword count of the gold standard summary. For example, in\none case, the gold summary had a length of 24 words, while\nthe average word count of the model-generated summaries\nwas around 26. This consistency in word count appears to\npositively impact the models’ performance.\nV. Conclusion\nThis study explores the potential of LLMs in the task\nof CHQ summarization, specifically in zero-shot settings.\nThe results reveal that despite the lack of task-specific fine-\ntuning, LLMs demonstrate strong performance, coming\nclose to and, in some cases, matching the performance\nof fine-tuned state-of-the-art models like Bangla T5. This\nunderscores the growing capabilities of modern pre-trained\nmodels to generalize effectively across different tasks and\nlanguages. While this progress is promising, there is\nstill a considerable journey ahead to fully bridge the gap\nbetween fine-tuned models and zero-shot LLMs, especially\nT able IV: Examples of summaries produced by benchmark models for comparison\nSample 1\nOriginal Question আমারবয়স৩০।অ েনকিদনধের�কামেররব্যথাক ের।আিমডাক্তার�দিখেয়িছ।এক্সেরকরারপর\nআমােকনাপ্রেক্সন১০ িদনধের�খেতবেলিদেয়েছ।আরসা েথক্যালিসয়াম,িনলটন�খেতিদেয়েছ১\nমাস।িকন্তুএপ য�ন্তব্যথাযায়নাই। �কামেরর�প্রসারিদেলব্যথাকের।এখন িককরাযায়।\nBanglaT5 Summary [ 21] বয়স৩০। �কামেরব্যথা।ডাক্তারন্যাপ্র �ক্সন১০ িদনদু �বলাকের�দয়।সােথক্যালিসয়াম,িনবলন�খেত\n�দয়১মাস। িকন্তুব্যাথাযায়নাই। �প্রসারিদেলব্যথাকের।িককরনীয়?\nMixtral-8x22B-Instruct-v0.1 Summary আিম৩০বছরব য়সীএবং �কামরব্যথাকর িছ।ডাঃএক্স েরকেরেছনএবংন্যাপ্র �ক্সন,ক্যালিসয়ামও\nিনবলনপ্রদানক েরেছন। িকন্তুব্যথাযা য়নাই। �কামের�প্রশারিদেলব্যাথাকের।এখন িককরেবা?\nGPT-4 Summary আমারবয়স৩০বছর।অ েনকিদনধের�কামেরব্যথাহে�,ডাক্তােররপরামেশ�ন্যাপ্র�ক্সন,ক্যালিসয়াম\nওিনবলন�খেয়িছ,িকন্তুব্যথাক েমিন।�কামেরচাপিদেলব্যথাবােড়।এখনআ িমকীকরেতপাির?\nSample 2\nOriginal Question আমারআম্মুর িপেঠরঠিকমা েঝভীষণজ্বেলপ্রায়একবসরধ েরএমনহয়।ডাক্তার �দখােনাহেয়েছশুধু\nগ্যাি�েকরওষুধ �দয়।িকন্তু�কানকাজহ য়না।এখনখুবজ্ব েল৷মােঝমােঝএকটু কমথােক।�কান\nিবেশষজ্ঞ�দখােলভালহয়? আরিকসমস্যাহেতপাের?িবেশষজ্ঞেদরপরামশ�চাই।\nBanglaT5 Summary [ 21] িপেঠরঠিকমা েঝভীষণজ্বেলপ্রায়একবছরধ ের।ডাক্তারগ্যা ি�েকরওষুধ �দয়।কাজহ য়না।এখন\nখুবজ্বেল।মােঝমােঝএকটু কমথােক।�কানিবেশষজ্ঞ�দখােলভােলাহেব? আরিকসমস্যাহেত\nপাের?\nMixtral-8x22B-Instruct-v0.1 Summary আমারিপেঠজ্বেলএকবসরধ েরআেছ,ডাক্তােরর�দওয়াগ্যাি�কওষুধকাজক েরিন।এখনএটাখুব\n�বেড়�গেছ,�কানিবেশষেজ্ঞরপরামশ�চাই।\nGPT-4 Summary আমার আম্মুরিপেঠর মাঝখােনএক বছর ধেরভীষণ জ্বালা হে�, ডাক্তার গ্যাি�েকর ওষুধিদেলও\nকাজহয়িন।এখনজ্বালাটাআরও �বেড়�গেছ।িবেশষেজ্ঞরপরামশ�প্রেয়াজনএবংঅন্য �কানসমস্যা\nহেতপােরিকনাজানেতচাই।\nfor specialized tasks in low-resource languages like Bangla.\nHowever, the decreasing performance gap suggests that\nthe need for extensive fine-tuning may become less critical\nas LLM architectures continue to evolve. Future research\nwill need to focus on improving the adaptability and effi-\nciency of these models, ensuring even greater applicability\nin resource-constrained settings.\nReferences\n[1] Xiaojuan Ma et al. “Professional Medical Advice\nat your Fingertips: An empirical study of an on-\nline” Ask the Doctor” platform.” In: Proceedings of\nthe ACM on Human-Computer Interaction 2.CSCW\n(2018), pp. 1–22.\n[2] Kirk Roberts and Dina Demner-Fushman. “Interac-\ntive use of online health resources: a comparison of\nconsumer and professional questions.” In:Journal of\nthe American Medical Informatics Association 23.4\n(2016), pp. 802–811.\n[3] Ramesh Nallapati et al. “Abstractive text summa-\nrization using sequence-to-sequence rnns and be-\nyond.” In:arXiv preprint arXiv:1602.06023 (2016).\n[4] Abhik Bhattacharjee et al. “BanglaBERT: Language\nmodel pretraining and benchmarks for low-resource\nlanguage understanding evaluation in Bangla.” In:\narXiv preprint arXiv:2101.00204 (2021).\n[5] Abhik Bhattacharjee et al. “BanglaNLG and\nBanglaT5: Benchmarks and resources for evaluating\nlow-resource natural language generation in Bangla.”\nIn: arXiv preprint arXiv:2205.11081 (2022).\n[6] Mohsinul Kabir et al. “Benllmeval: A comprehensive\nevaluation into the potentials and pitfalls of large\nlanguage models on bengali nlp.” In:arXiv preprint\narXiv:2309.13173 (2023).\n[7] Abdur Rahman Fahad et al. “Answer Agnostic Ques-\ntion Generation in Bangla Language.” In: Interna-\ntional Journal of Networked and Distributed Com-\nputing 12.1 (2024), pp. 82–107.\n[8] Pratik Joshi et al. “The state and fate of linguistic\ndiversity and inclusion in the NLP world.” In:arXiv\npreprint arXiv:2004.09095 (2020).\n[9] Derek Tam et al. “Evaluating the factual consistency\nof large language models through summarization.”\nIn: arXiv preprint arXiv:2211.08412 (2022).\n[10] Israt Jahan et al. “Evaluation of chatgpt on\nbiomedical tasks: A zero-shot comparison with fine-\ntuned generative transformers.” In: arXiv preprint\narXiv:2306.04504 (2023).\n[11] Pengcheng Qiu et al. “Towards building multilingual\nlanguage model for medicine.” In:Nature Communi-\ncations 15.1 (2024), p. 8384.\n[12] OpenAI. GPT-3.5-Turbo. Accessed: 2024-12-04.\n2024. url: https://platform.openai.com/docs/\nmodels/gpt-3-5-turbo.\n[13] Josh Achiam et al. “Gpt-4 technical report.” In:\narXiv preprint arXiv:2303.08774 (2023).\n[14] Anthropic. Claude-3.5-Sonnet. Accessed: 2024-12-\n04. 2024. url: https://www.anthropic.com/news/\nclaude-3-5-sonnet.\n[15] Meta AI. Meta Llama3-70B-Instruct . Accessed:\n2024-12-04. 2024. url: https://huggingface.co/\nmeta-llama/Meta-Llama-3-70B-Instruct.\n[16] Albert Q Jiang et al. “Mixtral of experts.” In:arXiv\npreprint arXiv:2401.04088 (2024).\n[17] Machel Reid et al. “Gemini 1.5: Unlocking mul-\ntimodal understanding across millions of tokens\nof context.” In: arXiv preprint arXiv:2403.05530\n(2024).\n[18] An Yang et al. “Qwen2 technical report.” In:arXiv\npreprint arXiv:2407.10671 (2024).\n[19] Gemma Team et al. “Gemma 2: Improving open lan-\nguage models at a practical size.” In:arXiv preprint\narXiv:2408.00118 (2024).\n[20] NexusFlow AI. Athene-70B. Accessed: 2024-12-04.\n2024. url: https://nexusflow.ai/blogs/athene.\n[21] Alvi Khan et al. “BanglaCHQ-Summ: An Abstrac-\ntive Summarization Dataset for Medical Queries in\nBangla Conversational Speech.” In: Proceedings of\nthe First Workshop on Bangla Language Processing\n(BLP-2023). 2023, pp. 85–93.\n[22] Mehdi Allahyari et al. “Text summarization\ntechniques: a brief survey.” In: arXiv preprint\narXiv:1707.02268 (2017).\n[23] Wafaa S El-Kassas et al. “Automatic text summa-\nrization: A comprehensive survey.” In: Expert sys-\ntems with applications 165 (2021), p. 113679.\n[24] Shaharior Rahman Razu et al. “Challenges faced\nby healthcare professionals during the COVID-19\npandemic: a qualitative inquiry from Bangladesh.”\nIn: Frontiers in public health 9 (2021), p. 647315.\n[25] Hongyi Yuan et al. “BioBART: Pretraining and eval-\nuation of a biomedical generative language model.”\nIn: arXiv preprint arXiv:2204.03905 (2022).\n[26] Yuhao Chen, Zhimu Wang, and Farhana Zulkernine.\n“Comparative analysis of open-source language mod-\nels in summarizing medical text data.” In:2024 IEEE\nInternational Conference on Digital Health (ICDH) .\nIEEE. 2024, pp. 126–128.\n[27] Hadi Askari et al. “Assessing LLMs for Zero-\nshot Abstractive Summarization Through the Lens\nof Relevance Paraphrasing.” In: arXiv preprint\narXiv:2406.03993 (2024).\n[28] Israt Jahan et al. “A comprehensive evaluation of\nlarge language models on benchmark biomedical\ntext processing tasks.” In:Computers in biology and\nmedicine 171 (2024), p. 108189.\n[29] Junjie Ye et al. “A comprehensive capability anal-\nysis of gpt-3 and gpt-3.5 series models.” In: arXiv\npreprint arXiv:2303.10420 (2023).\n[30] Zhen Huang et al. “OlympicArena Medal Ranks:\nWho Is the Most Intelligent AI So Far?” In:arXiv\npreprint arXiv:2406.16772 (2024).\n[31] Abhimanyu Dubey et al. “The llama 3 herd of mod-\nels.” In:arXiv preprint arXiv:2407.21783 (2024).\n[32] Chin-Yew Lin. “Rouge: A package for automatic\nevaluation of summaries.” In: Text summarization\nbranches out. 2004, pp. 74–81."
}