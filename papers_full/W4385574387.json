{
  "title": "Continual Learning for Natural Language Generations with Transformer Calibration",
  "url": "https://openalex.org/W4385574387",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2034102953",
      "name": "Peng Yang",
      "affiliations": [
        "Bellevue Hospital Center",
        "Cognitive Research (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2221424628",
      "name": "Dingcheng Li",
      "affiliations": [
        "Bellevue Hospital Center",
        "Cognitive Research (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2103444047",
      "name": "Ping Li",
      "affiliations": [
        "Bellevue Hospital Center",
        "Cognitive Research (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2554863749",
    "https://openalex.org/W4287889346",
    "https://openalex.org/W3098714526",
    "https://openalex.org/W4301163820",
    "https://openalex.org/W2964189064",
    "https://openalex.org/W4295249402",
    "https://openalex.org/W2473930607",
    "https://openalex.org/W3121275803",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4288336812",
    "https://openalex.org/W2963813679",
    "https://openalex.org/W2964006684",
    "https://openalex.org/W2765101016",
    "https://openalex.org/W4287814827",
    "https://openalex.org/W4384625728",
    "https://openalex.org/W2786446225",
    "https://openalex.org/W3125116114",
    "https://openalex.org/W4306316933",
    "https://openalex.org/W2788388592",
    "https://openalex.org/W2804175194",
    "https://openalex.org/W2902456977",
    "https://openalex.org/W2971176100",
    "https://openalex.org/W4319988532",
    "https://openalex.org/W3211394631",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W3100152912",
    "https://openalex.org/W2743151379",
    "https://openalex.org/W2986349107",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Conventional natural language process (NLP) generation models are trained offline with a given dataset for a particular task, which is referred to as isolated learning. Research on sequence-to-sequence language generation aims to study continual learning model to constantly learning from sequentially encountered tasks. However, continual learning studies often suffer from catastrophic forgetting, a persistent challenge for lifelong learning. In this paper, we present a novel NLP transformer model that attempts to mitigate catastrophic forgetting in online continual learning from a new perspective, i.e., attention calibration. We model the attention in the transformer as a calibrated unit in a general formulation, where the attention calibration could give benefits to balance the stability and plasticity of continual learning algorithms through influencing both their forward inference path and backward optimization path. Our empirical experiments, paraphrase generation and dialog response generation, demonstrate that this work outperforms state-of-the-art models by a considerable margin and effectively mitigate the forgetting.",
  "full_text": "Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL), pages 40 - 49\nDecember 7-8, 2022 ©2022 Association for Computational Linguistics\nContinual Learning for Natural Language Generations with Transformer\nCalibration\nPang Yang, Dingcheng Li, Ping Li\nCognitive Computing Lab\nBaidu Research\n10900 NE 8th St. Bellevue, W A 98004, USA\n{pengyang5612, dingchengl, pingli98}@gmail.com\nAbstract\nConventional natural language process (NLP)\ngeneration models are trained offline with\na given dataset for a particular task, which\nis referred to as isolated learning. Research\non sequence-to-sequence language generation\naims to study continual learning model to con-\nstantly learning from sequentially encountered\ntasks. However, continual learning studies often\nsuffer from catastrophic forgetting, a persistent\nchallenge for lifelong learning. In this paper,\nwe present a novel NLP transformer model that\nattempts to mitigate catastrophic forgetting in\nonline continual learning from a new perspec-\ntive, i.e., attention calibration. We model the at-\ntention in the transformer as a calibrated unit in\na general formulation, where the attention cal-\nibration could give benefits to balance the sta-\nbility and plasticity of continual learning algo-\nrithms through influencing both their forward\ninference path and backward optimization path.\nOur empirical experiments, paraphrase gener-\nation and dialog response generation, demon-\nstrate that this work outperforms state-of-the-\nart models by a considerable margin and effec-\ntively mitigate the forgetting.\n1 Introduction\nSequence-to-sequence (Seq2Seq) generation has\nbeen widely applied in artificial learning (AI) sys-\ntem to deal with various challenging tasks, e.g.,\nparaphrase, dialogue system (Bordes et al., 2016),\nmachine translation, etc. In addition, powerful rep-\nresentation learning (e.g., Transformer) have been\nused in Seq2Seq models, which have taken the\nstate-of-the-art of generation models to a new level.\nGenerally, nature language generation (NLG) mod-\nels leverage an encoder to create a vector repre-\nsentation for source inputs, and then pass this rep-\nresentation into a decoder so as to output a target\nsequence word by word. For example, Bart (Lewis\net al., 2019) is such a transformer-based NLG ar-\nchitecture that is equipped with the BERT-type net-\nwork structure (Devlin et al., 2019) as its encoder\nand with the GPT-type structure as the decoder.\nDespite the remarkable ability on sequence gen-\neration, the conventional paradigm aims to learn\na Seq2Seq model on the whole available dataset,\nwhich limits its ability in accumulating knowledge\nin continual learning scenario. When switching to\na new task from some previously learned ones, the\nfine-tuned model on the new task sometimes faces\na significant performance drop on previous learned\ndata, where such a phenomenon is also referred to\nas catastrophic forgetting (Parisi et al., 2019; Mai\net al., 2021; Yin et al., 2021; Li et al., 2022a,b). In\ncontrast, humans and animals exhibit remarkable\nability to deal with new tasks by effectively adapt-\ning their acquired knowledge without forgetting the\npreviously learned skills. If one desires to build a\nhuman-like NLG model, continual learning ability\nis a necessary skill for achieving this goal.\nThe existing replay-based continual learning\napproaches have taken into account of differ-\nent perspectives of the model training process\nto remedy the catastrophic forgetting dilemma,\nsuch as regularizing the parameter change dur-\ning training (Chaudhry et al., 2018; Parisi et al.,\n2019), selective memory storage or replay (Aljundi\net al., 2019), Bayesian and variational Bayesian\ntraining (Kirkpatrick et al., 2017; Nguyen et al.,\n2018), and task-specific parameterization of the\nmodel (Pham et al., 2021; Singh et al., 2020). In\nthis paper,we tackle the problem from a novel angle\nthat is distinct to all the aforementioned attempts,\ni.e., seeking a better balance between stability and\nplasticity with neuron calibration. Specifically, we\nrefer to neuron calibration as a process of math-\nematically adjusting the transformation functions\nin various layers of transformer-based architecture.\nIn this way, the neuron calibration is able to prior-\nitize both model parameter and feature map that\nare suitable to new tasks. In detail, our proposed\nneuron calibration approach regularizes the param-\n40\neter update against catastrophic forgetting via pos-\ning a trainable soft mask on the attention and fea-\nture maps, which then influences both the model\ninference process and the model training process\nthrough the forward inference path and the back-\nward optimization path.\nThe contributions of our work are three-fold:\n(i) we introduce a general and light-weight feature\ncalibration approach to tackle task-incremental con-\ntinual learning problems where the models are for-\nmulated as feed-forward transformer-based func-\ntion approximations; (ii) we formulate a novel\ntask-incremental learning paradigm to train the\ncalibrated model with an interleaved optimization\nscheme to mitigate the forgetting issue; (iii) we in-\ndicate through extensive empirical experiments that\nthe proposed method could outperform the recent\ncontinual learning algorithms on Seq2Seq language\ngeneration applications.\n2 Related Work\nContinual Learning.Existing continual learning\nmethods can be classified into three categories. The\nregularization approaches (Li and Hoiem, 2017;\nZenke et al., 2017; Schwarz et al., 2018) impose a\nregularization constraint to the objective function to\nmitigate the catastrophic forgetting. The rehearsal\napproaches (Rolnick et al., 2019; Aljundi et al.,\n2019; Buzzega et al., 2020; Wang et al., 2022) al-\nlocate a small memory buffer to store and replay\nthe exemplar from the previous task to consoli-\ndate the historical knowledge. The architectural\napproaches (Rusu et al., 2016; Serra et al., 2018;\nSingh et al., 2020; von Oswald et al., 2020) avoid\ncatastrophic forgetting through approximating the\ntraining of the task-specific network and allowing\nthe expansion of the parameters during continual\nlearning. Nonetheless, all these methods are con-\nfined to supervised classification problem, which\nlimits their application in real-life problems. Life-\nlong GAN (Zhai et al., 2019) tackles the genera-\ntion problem of continual learning and learn task-\nspecific representation on shared parameters. Their\nmethod is restricted to image generation tasks and\nnot directly applicable to NLP benchmark datasets.\nContinual Language Generation.Few work has\nbeen done in continual learning for Seq2seq lan-\nguage generation. The most relevant work is\nfrom Mi et al. (2020), which propose a contin-\nual learning framework that builds a human-like\ndialogue system in an incremental learning man-\nner. Specifically, this method combines the mem-\nory replay with the regularization technique to ad-\ndress the catastrophic forgetting, and empirically\nachieves a promising result on the MultiWoZ-2.0\ndataset. Nonetheless, their system is specifically\ndesigned for the dialogue task and lacks generaliza-\ntion to Seq2Seq tasks. Our method differs from Mi\net al. (2020) in terms of the following three points:\n(i) our method is built upon a neuron calibration\napproach, where such contribution is orthogonal\nto that from all the previous works; (ii) our pro-\nposed method does not engage any task-specific\npart; (iii) we do not store the historical exemplar\nfrom the episodic memories during training. In ad-\ndition, our proposed method could be adapted to\nvarious seq2seq language generation applications,\nsuch as summarization, translation, paraphrases,\ndialog response generation.\n3 Method\n3.1 Preliminary\nWe introduce the setting of online continual learn-\ning. Formally, we denote the sequence of train-\ning tasks in continual learning as {T1,···,TT}.\nThe tasks come and go in an online fashion, and\nthe training data for each task is available only\nat that time slot. When the new task arrives, the\nprevious task’s data is deleted and cannot be used\nany more. For the t-th task, we denote its training\ndataset as Dt. The objective of the task is to learn\na transformer-based generation model. Our work\ntackles the natural language generation (NLG)-\nbased continual learning problems and thus the\nmodel is typically modeled as a feed-forward trans-\nformer with L-blocks (i.e, {li}L\ni=1), with its corre-\nsponding parameters denoted as {θi}L\ni=1.\n3.2 Transformer Calibration\nWe introduce a general calibration mechanism to\ntackle the continue learning problems on Seq2Seq\ngeneration, where the models are parameterized by\nthe transformer-based NLG models. By applying\nneuron calibration, we aim to adapt the transforma-\ntion function in the deep transformer layers. Our\nproposed learning paradigm with neuron calibra-\ntion could perform both model selection and feature\nselection to effectively avoid catastrophic change\non the model parameters while accomplishing a\nstable consolidation of knowledge among tasks. In\nthis framework, the calibration module is indepen-\ndent from the pre-trained base model in order to\n41\nFigure 1: Overview of our proposed transformer calibration for continual learning framework. This method consists\nof two types of calibration modules: attention calibration module (ACM) and feature calibration module (FCM),\nwhich are sequentially applied to the layers in the multi-head attention model (as shown in the figure) to calibrate\nthe attention signals and feature maps, respectively.\npreserve the learned knowledge and avoid catas-\ntrophic forgetting. Figure 1 provides an illustration\nof our neuron calibration process.\nFormally, we introduce two types of general cal-\nibration modules to be applied on the transformer-\nbased NLG models: (i) attention calibration module\n(ACM) and (ii) feature calibration module (FCM).\nThe attention calibration module learns to scale the\nattentions of the transformer function whereas the\nfeature calibration module learns to scale the fea-\nture map output from the transformer block. When\ncalibrating the i-th layer of the transformer block,\nwe use Ai to denote its scaled attention function\nafter applying attention calibration (ACM). Mean-\nwhile, we use hi and ˜hi to denote the output fea-\nture maps before and after applying feature calibra-\ntion (FCM), respectively.\nWe first introduce the formulation for ACM. To\ncalibrate the attention, we first define a learnable\nmatrix Φi ∈RN×N, which presents the importance\nof each pair of words, where N is the maximal\nnumber of words in the sentence and a subset of\nparameters is used according to sentence length.\nThe scale dot-product attention is formulated as:\nAtten = Softmax\n(\nQiK⊤\ni ⊙\n(Φi√\nd\n))\nVi (1)\nwhere ⊙is the element-wise product. As Φi is\nlearned across the sequential tasks, the task-aware\nattention can serve as a task representation instead\nof traditional task embedding. The overall cali-\nbrated attention can be decoupled into two parts:\nthe QK⊤term presents the content-based attention,\nand Φi/\n√\ndterm acts as the soft mask for attention\ncalibration. This united design offers more task\nadaptation by suppressing the unrelated attention\nvalues and highlighting the important ones. With\nthe ACM, the calibrator module plays a crucial role\nduring the model training process: at the forward\ninference path, it scales the value of the attention\nin the attention block to make prediction; at the\nbackward learning path, it serves as a prioritized\nweight to regularize the update on parameters.\nBy applying attention calibration on transformer\nblocks, the attention function at the i-th layer\nAtten(Qi,Ki,Vi,Φi) is parameterized by Φi and\nproduces the output as follows,\nhi = FAi (hi−1),s.t. Ai = Atten(Qi,Ki,Vi,Φi)\n(2)\nThe output hi of the attention function is then pro-\ncessed by a feature calibration module (FCM) to\ngenerate the calibrated feature map for that layer.\nWe use Ωλi (·) to denote the feature transformation\nfunction at the i-th layer, parameterized byλi. With\nFCM, the calibration parameters also interact with\nthe feature map hi with a multiplicative operation.\nSpecifically, the calibrated feature is computed as:\nΩλi (hi) = tile(λi) ⊙hi, λi ∈Rd,hi ∈RN×d\n(3)\ngiven the dimension of feature map d.\nIn the end, the outputs from (2) and (3) get added\nup in an element-wise manner by a residual con-\nnection. This is followed by normalization and acti-\nvation operations to produce a final output for that\nlayer. In summary, the overall calibration process\n42\nfor the i-th layer could be formulated as follows,\n˜hi = σ(LN(Ωλi (FAi (hi−1)) ⊕FAi (hi−1))) ,\n(4)\nwhere LN(·) denotes the layer normalization, ⊕\ndenotes an element-wise addition operator, andσ(·)\nis an activation function. Then ˜hi is sent as input\nto the i+ 1-th layer in the feed-forward network.\nAll the aforementioned calibrator parameters are\ninitialized with a value of 1 at the start of training.\nWe illustrate an example case of applying the cali-\nbration on a transformer-based model in Figure 1.\n3.3 Learning Calibration Parameters\nWe propose an interleaved learning paradigm to\ntrain the calibrated transformer model. In the train-\ning procedure, we aim to exploit the training of the\ncalibrator parameters to mitigate the catastrophic\nforgetting on the continual learning. Since the ‘for-\ngetting’ in the training is often attributed to dra-\nmatic changes in parameter values, we design the\nlearning objective for the calibrator learning as to\nregularize the parameter change after accessing the\nnew knowledge not to be biased too much from the\nmodel values learned from previous ones.\nTo formulate the objective function for the cali-\nbrated model training, we inherit the elastic weight\nconsolidation (EWC) approach proposed in Kirk-\npatrick et al. (2017) . Specifically, EWC approxi-\nmates the true posterior distribution for the contin-\nual learning parameters by a Gaussian distribution\ngiven by the mean from the previous tasks and\na diagonal precision from the Fisher information\nmatrix. In this work, we formulate a weight cali-\nbration process to prevent the catastrophic change\non model parameters. Then we train the calibrator\nparameters with the following loss function,\nLc = vec\n(\nθ−θt)⊤Λtvec\n(\nθ−θt)\n  \nterm (a)\n+ βLt(Ψ,λ,θ )  \nterm (b)\n(5)\nwhere βis a trade-off parameter, and the operator\nvec (·) stacks the tensor into a vector.\nThe matrix Λt in term (a) are the Fisher infor-\nmation matrix, which is obtained from the data\ntraining loss for previous observed tasks, while\nthe Lt(Ψ,λ,θ ) in term (b) is the loss for the cur-\nrent task. The two terms perform the consolidation\nprocess to retain the essential parameters towards\npast knowledge when the base model parameters\nare trained to absorb new tasks. To consolidate the\nknowledge on the calibrated model, the Fisher in-\nformation matrix is computed upon the gradients\non calibrated parameters.\n3.4 Optimization\nWe formulate the optimization process to train the\ncalibrated model under an iterative optimization\nschema, with the parameters from the base model\nand those from the calibration module being op-\ntimized by the loss function (5). During the inter-\nleaved optimization process, we first fixθt and take\ngradient steps with regard to {Ψ,λ}as follows:\nΨt+1 ←Ψt −α▽Ψ Lc((Ψ,λ),θt,Dt) , (6)\nλt+1 ←λt −α▽λ Lc((Ψ,λ),θt,Dt) , (7)\nThen, we go on to optimize the base model param-\neter when the inference takes place with the up-\ndated base model,\nθt+1 ←θt −α▽θ Lc(θ,(ψt+1,λt+1),Dt) (8)\nwhere αis the learning rate. By employing the cal-\nibrated parameterization of the transformer-based\nnetwork, and optimizing it with the iterative learn-\ning scheme, our method achieves the trade-off be-\ntween new data adaptation and past knowledge con-\nsolidation. We present the details in Algorithm 1.\nAlgorithm 1:Transformer Calibration for\nContinual Learning Algorithm (TCCL)\nInput: Base model θ, calibrator (Φ,λ)\nlearning rate α, trade-off parameter\nβ, training data {Dtr\n1 ,..., Dtr\nT }, test\ndata {Dte\n1 ,..., Dte\nT }\nOutput: Base model Fθ, calibrator F(Φ,λ).\nfunction train_and_eval\nRandomly initialize θ, Ψ and λ.\nfor t←1 toT do\nfor b←1 tonbatch do\nObserve a batch of data\nBt = {xi,yi}bs\ni=1 from Dtr\nt .\nΦ′←Φ −α∇ΦLc(Bt; θ,Φ,λ)\nλ′←λ−α∇ψLc(Bt; θ,Φ,λ)\nθ′←θ−α∇θLc(Bt; θ,Φ′,λ′)\nCompute Λt according to ∇θLc\nfor te←1 tot do\nEvaluate testing accuracy for the\ncurrent model on Dte\n1,...,t:\nˆy1,...,t ←F(Dte\n1,...,t; θt,Φt,λt)\n43\n4 Empirical Experiments\nWe evaluated the proposed algorithm on seq2seq\ngeneration tasks. We applied the algorithms on two\ndatasets for seq2seq generation tasks in the contin-\nual learning. We also conducted the ablation study\nwith respect to attention calibration and feature cali-\nbration to evaluate the robustness and effectiveness\nof the proposed calibration techniques.\n4.1 Application: Paraphrase Generation\nDataset. For paraphrase generation, we train the\nmodel over three existing paraphrase datasets,\nQuora1, Twitter 2 and Wiki_data (linked-wiki-\ntext2)3, in a sequential manner, where the model ob-\nserves the three sequential tasks (i.e., datasets) one\nby one. See Table 1 for Statistics of the datasets.\ntrain valid test\nQuora 111,947 8,000 37,316\nTwitter 85,970 1,000 3,000\nWiki_data 78,392 8,154 9,324\ntotal 276,309 17,154 49,640\nTable 1: Statistics of Dataset on Paraphrase Generation\nExperimental Setting.We exploit the SOTA gener-\nation model, BART, as the generation model back-\nbone in the continual learning framework. We com-\npare our approach with the following baselines:\n• Finetune: for each new task, the model is ini-\ntialized with the parameters learned from pre-\nvious observed tasks, and then fine-tuned with\ndata of the current new task.\n• Full: the model is trained with all the available\ninstances from three datasets together, which\nregarded as the up-bounded performance for\nthe continual learning techniques.\n• EWC: the EWC (Kirkpatrick et al., 2017) is\nintroduced in the objective function to train\nthe model over the sequential tasks.\nFor evaluation metrics, we use Bleu4, RougeL\nand Meteor for the Seq2Seq generation tasks. To\nmeasure the forgetting rates of different methods,\nwe basically exploit the model learned on t-th task\nto evaluate its performance on previous tasks, i.e.,\n1https://huggingface.co/datasets/quora\n2https://metatext.io/datasets/paraphrase-and-semantic-\nsimilarity-in-twitter-(pit)\n3https://paperswithcode.com/dataset/wikitext-2\n1,···,t −1 task. We tune the learning rate αfrom\n{10−3,10−2,..., 100}for both model parameter\nand calibrator parameter, and trade-off parameter\nβ from {0.1,0.5,1,5,10}. Meanwhile, the batch\nsize is set to be {128,256,512}on all datasets. All\ntraining and evaluation experiments are performed\nusing Tesla V100S GPUs. The whole learning pro-\ncess takes around 0.5 GPU day.\n4.1.1 Experimental Results\nAccuracy Measurement:Table 2 presents the ac-\ncuracy results in the continual learning setting,\nwhere the model is evaluated after the model has\nbeen trained on sequential tasks one after another.\nIn the table, the first three models are independent\nbaselines trained on either one of three datasets.\nAs expected, model trained on new dataset may\nsuffer the significant performance drop on previous\ninstances, due to the data distribution gap between\nold and new datasets. For example, twitter includes\nthe short casual text while Wiki_data contains for-\nmal academic text.\nFor the fine-tune, the model is trained in a Quora-\nTweeter-Wiki (QTW) order, in which the model is\ninitialized with the model parameters learned on the\nprevious task and then fine tuned over the follow-\ning task. We observe that finetune results on Quora\nand Wiki_data are comparable with those when\nbuilding the model from scratch. In addition, EWC\ncan achieve a better performance than Finetune and\nindependent training over any evaluation metrics\non Quora and most metrics on Twitter and Wiki,\ndemonstrating the effectiveness of EWC in contin-\nual learning. Nonetheless, our calibration model\nconsistently achieves the best performance across\nall sequential tasks, demonstrating that the calibra-\ntion model yields a promising domain adaptation\nin continual learning.\nForgetting Measurement.Table 3 presents the\nresults when the current models are evaluated on\ntesting data from the previous tasks. The purpose\nof this experimental setting is to measure the for-\ngetting rate of the models in the sequential train-\ning. In the order of QTW, the results are evaluated\non Quora after the model is trained on Twitter, as\nwell as on Quora and Twitter after the model is\ntrained on Wiki. Our method is compared with in-\ndependent baseline, finetune and EWC. Table 3 in-\ndicates that our method obtains a less performance\ndrop than Finetune and EWC, with a low forget-\nting rate. Moreover, after the model is trained on\n44\nQuora Test Twitter Test Wiki Test\nModels bleu4∗ rougeL meteor bleu4∗ rougeL meteor bleu4∗ rougeL meteor\nQuora-trained 30.11 55.85 57.17 2.12 6.13 5.49 4.51 11.21 12.13\nTwitter-trained 3.18 11.46 9.01 35.47 57.49 54.57 4.60 9.76 7.50\nWiki_data-trained 22.38 43.44 46.23 9.32 17.93 21.03 42.12 73.86 73.10\nFinetune 30.11 55.85 57.17 35.79 56.32 54.93 42.12 73.86 73.10\nEWC 30.25 56.16 57.98 33.52 54.41 54.21 42.15 73.53 73.59\nOurs 32.14 58.12 59.13 36.81 58.46 55.32 44.47 74.49 73.66\nFull 33.99 59.56 61.67 38.56 58.76 56.01 46.86 76.59 75.91\nTable 2: Results of model evaluations on QTW setting\n(bleu4∗ denotes a more strict scoring version for the baseline evaluation)\nTrain: Twitter→Test: Quora\nModels bleu4∗ rougeL meteor\nQuora-trained 30.11 55.85 57.17\nFinetune 15.80 46.59 47.31\nEWC 15.63 41.53 46.03\nOurs 15.93 46.65 45.81\nTrain: Wiki_data→Test: Quora\nModels bleu4∗ rougeL meteor\nQuora-trained 30.11 55.85 57.17\nFinetune 19.07 51.76 55.95\nEWC 19.63 49.35 53.02\nOurs 21.39 53.62 56.44\nTrain: Wiki_data→Test: Twitter\nModels bleu4∗ rougeL meteor\nTwitter-based 35.79 56.32 54.93\nFinetune 14.09 37.97 45.89\nEWC 14.84 38.65 46.33\nOurs 16.62 40.25 48.44\nTable 3: Results of all the methods when testing new\nmodels on previous domains (from 2nd row to the last).\nWiki, the performance on Quora is even improved\nfrom the one after trained on Twitter. Moreover,\nthis work outperforms EWC on all the evaluation\ndomains with a noticeable margin, which demon-\nstrates that our calibration module is effective to\nboost the performance for continual learning via\nproperly regularizing the parameter update against\ncatastrophic forgetting. Overall, the empirical re-\nsult demonstrates that the calibration mechanism\ncan mitigate the forgetting issue greatly.\nAblation Study.We conduct the ablation study\nwhere several simplified versions of the calibration\nframework are evaluated in order to understand\nthe effects of different components. Specifically,\nwe evaluate the model variants without attention\ncalibration module (i.e., w/o ACM), or feature cal-\nibration module (i.e., w/o FCM), or EWC regu-\nQuora Test Wiki_data Test\nModels bleu4∗ meteor bleu4∗ meteor\nFinetune 30.11 57.17 42.12 73.10\nw/o FCM 33.32 59.32 43.33 73.10\nw/o ACM 32.25 58.91 42.15 72.59\nw/o R 33.77 59.57 43.51 72.93\nOurs 35.44 61.45 44.47 73.66\nTable 4: Ablation studies on the proposed calibration\ncomponents and regularizion terms.\nlarization term (i.e., w/o R), and present the com-\nparison result in Table 4. From the table, we can\nobserve that (i) equipped with ACM or FCM, the\nperformance is apparently better than the original\nbackbone since dropping the calibration module\n(“w/o ACM\" and “w/o FCM\") would degrade the\nperformance; (ii) EWC regularization is also ef-\nfective, indicated by the better result than the one\nwithout EWC regularization term (“w/o R\"). Over-\nall, the results demonstrate that calibrating on latent\nfeature and attention value is a promising direction.\nNext we aim to investigate the effect of the at-\ntention calibration that is performed on three dif-\nferent attentions in the transformer model. Specif-\nically, we equipped the calibration component on\neither one of the self-attention of encoder, the self-\nattention of decoder and the encoder-decoder (ED)\nattention. The comparison results in Table 5 indi-\ncate that (i) the self-attention calibration on encoder\nis more effective to boost the performance; (ii)\nthe calibration on encoder-decoder attention yields\nQuora Test\nModel Variants bleu4∗ rougeL meteor\nSelf-Attention (E) 33.31 59.94 59.56\nSelf-Attention (D) 32.65 58.76 58.34\nED-Attention (D) 34.81 60.55 60.33\nOurs (All) 35.44 61.37 61.45\nTable 5: Ablation studies of the calibration different\nattention blocks in language model.\n45\nSOURCE BART Ours TARGET\nWhat is the best home workout\nto reduce waist fat?\nHow can I reduce my\nwaist fat through a diet?\nWhat is best home remedy for\nreducing belly fats?\nWhat is best home remedy for\nreducing belly fats?\nWhat’s it like to be\nin a relationship with\na married man?\nWhat is it like for\na married man to be\nin a relationship?\nWhat’s it like to be\nin a relationship with\na married man?\nWhat’s it like to be\nin a relationship with\na married man?\nwhich provides a conventional\nsonic underscore to the\nonscreen action\nwhich provides a sonic\nunderscore to the onscreen\naction\nwhich provides a conventional\nsonic underscoring to the\nonscreen action\nwhich provides a conventional\nunderscore to the onscreen\naction\nExample gymnasium scene’s\nfirst encounter with Angela\nExample gymnasium scene,\nAngela ’s first encounter\nwith Angela\nFor example, the gymnasium\nscene, Pfaster ’s first encounter\nwith Angela\nOne example is the gymnasium\nscene, Lester ’s first encounter\nwith Angela.\nTable 6: Examples of the generated paraphrases by BART and Ours on QTW data setting.\nmuch better results than other two self-attentions.\nOverall, the results demonstrate that the attention\ncalibration plays an important role for boosting the\nperformance of the transformer-based generation\nmodel.\nCase Study.In Table 6, we perform the case stud-\nies on paraphrase generation tasks. All examples\nare results generated by the final model, e.g., the\nmodel trained on Wiki_data is used to generate\nsamples on Quora, Twitter, Wiki_data. Among the\nfour examples, the first two is from Quora, and\nthe others from Wiki_data. We compare our gen-\nerated sentence with ones from BART backbone.\nFrom the table, we observe that our method has a\nbetter generation on all four cases. In those gen-\neration samples, the colored parts are key words.\nYet, BART model either fails to generate those key\nwords or creates the examples of false causality. In\ncontrast, our method is able to generate key words\nin all cases with correct word relations.\n4.2 Application: Dialog Response Generation\nDataset. The proposed model is evaluated on the di-\nalog response generation task using the MultiWoZ-\n2.0 dataset (Budzianowski et al., 2018), which\ncontains 6 domains (Attraction, Hotel, Restaurant,\nBooking, Taxi and Train) and 7 DA intents (“In-\nform, Request, Select, Recommend, Book, Offer-\nBooked, No-Offer\"). We follow the setting (Mi\net al., 2020) to generate the train/validation/test\nsplits of MultiWoz. The details of the dataset is\npresent in Table 7.\nExperimental Setting. To evaluate the method\nperformance, we exploit the slot error rate (SER)\nand BLEU4 score as the evaluation metrics. The\nlower value of SER indicates a better performance.\nTo estimate the forgetting rate, the above met-\nDomain and Intents of MultiWoZ-2.0 Data\nDomains #. Total Intents #. Total\nAttraction 8,823 Inform 28,700\nHotel 10,918 Request 7,621\nRestaurant 10,997 Select 865\nBooking 8,154 Book 4,525\nTaxi 3,535 Recommend 3,678\nTrain 13,326 Offer-Booked 2,099\nNo-Offer 1,703\nTable 7: Statistics on the Dialog Response dataset\nrics are reported in two continual learning set-\ntings (Kemker et al., 2018): Ωall = 1\nT\n∑T\ni=1 Ωall,i\nand Ωfirst = 1\nT\n∑T\ni=1 Ωfirst,i, where T is total\nnumber of tasks in the sequential order.Ωall,i is the\ntest performance on all the tasks evaluated by the\nmodel learned with the i-th task, while Ωfirst,i is\nthe test result on the first task after the i-th task has\nbeen learned.\nOur work exploits the well-known seq2seq gen-\neration model, conditional variational encoder\n(CV AE) as the backbone model, and the proposed\nmodel is compared with the following baselines:\na) Finetune: the model trained from previous ob-\nserved tasks is used to be fine-tuned with data of\nthe current new task.\nb) Full: this model is trained with the data from\ncurrent tasks and all historical tasks together.\nc) ARPER (Mi et al., 2020): the model introduces\nmemory replay and adaptive regularization together\nto mitigate the catastrophic forgetting issue.\nd) ER: the model with the chosen exemplars that\nbest approximate the mean DA vector (Rebuffi\net al., 2017).\nFor CV AE, we equipped the feature calibration\nmodule on the backbone, due to no attention on\nthe CV AE. In the following experiment, we follow\n46\nthe setting (Mi et al., 2020) and utilize the selected\nexemplars to compute the Fisher information as in\nthe function (5).\n4.2.1 Comparison Result\nWe conduct comparison experiments with baselines\nwith various number of exemplars. The first one is\nthat all methods do not use any exemplars. The rea-\nson for this comparison is that our proposed method\nis memory-free, i.e., no memory buffer required to\nstore and replay the exemplar for data rehearsal. In\nsuch setting, ARPER reduces to the general reg-\nularization technique. Table 8 gives the evidence\nthat without any exemplars, our method achieves a\nbetter performance than ARPER in both Ωall and\nΩfirst, with a noticeable margin. We observe that\nthe ARPER severely relies on the exemplars. With-\nout the exemplars, the ARPER suffer a significant\nperformance drop in terms of the accuracy, even\npoorer than Finetune.\nWith the increased number of exemplars, our\nmethod can obtain a better performance since the\nfisher matrix in our objective can cumulative the\ninformative data throughout the training process.\nIn addition, ER and APRE are memory-based tech-\nniques and are obviously beneficial from the ex-\nemplars. Nonetheless, our method can consistently\noutperform APRER and ER in both settings of 250\nexemplars and 500 exemplars. That indicates that\nour memory-free calibration technique can effec-\ntively exploit the exemplar knowledge without the\nneed of data storage for the exemplars.\n4.2.2 Dynamic Results in Continual Learning\n1 2 3 4 5 6\n#Domains\n56\n58\n60\n62\n64BLEU-4 (%)\nOurs-A\nOurs-F\nOriginal-A\nOriginal-F\n1 2 3 4 5 6\n#Domains\n1\n2\n3\n4\n5\n6Slot Error Rate (%)\nOurs-A\nOurs-F\nOriginal-A\nOriginal-F\nFigure 2: BLEU-4 and SER on all observed domains\n(solid) and on the first domain (dashed) over the six\ncontinually observed domains using 250 exemplars.\nFigure 2 presents the comparison results along\nthe six continually observed domains of dialog re-\nsponse. We compare the performance of the cal-\nibrated model with the original CV AE backbone.\nWith more tasks continually learned, our method\ngradually performs better performance than the\noriginal backbone. On the first task (dashed lines),\nZero exemplars in total\nΩall Ωfirst\nModels SER BLEU4 SER BLEU4\nFinetune 64.46 0.361 107.27 0.253\nER 67.23 0.360 105.33 0.181\nARPER 63.54 0.360 102.87 0.192\nOurs 56.90 0.395 68.60 0.258\nALL 4.26 0.599 3.60 0.616\n250 exemplars in total\nΩall Ωfirst\nModels SER BLEU4 SER BLEU4\nFinetune 64.46 0.361 107.27 0.253\nER 16.89 0.535 9.89 0.532\nARPER 5.22 0.590 2.99 0.624\nOurs 4.41 0.603 2.33 0.635\nALL 4.26 0.599 3.60 0.616\n500 exemplars in total\nΩall Ωfirst\nModels SER BLEU4 SER BLEU4\nFinetune 64.46 0.361 107.27 0.253\nER 12.25 0.555 4.53 0.568\nARPER 5.12 0.598 2.81 0.627\nOurs 4.33 0.606 2.21 0.638\nALL 4.26 0.599 3.60 0.616\nTable 8: Average Results of all the methods when\nlearning six domains using 0/250/500 exemplars.\n(BLEU4 follows the setting in Mi et al. (2020))\nthe calibrated model outperforms the original one\non both metrics. These results illustrate the advan-\ntage of our calibration components throughout the\nentire continual learning process.\n5 Conclusions\nWe propose an efficient seq2seq generation model\nwith the calibration on the transformer, where a\nfixed architecture network after calibration can dy-\nnamically adjust the function with respect to each\nindividual task. To optimize our method, we fur-\nther propose a reproductive learning equipped with\nan iterative optimization objective that trade-off\nbetween plasticity and stability. Moreover, our cal-\nibration module is very light-weight without in-\ntroducing any task-specific parameters. Extensive\nempirical experiments indicate that our approach\noutperforms the baselines and achieves a promising\nresult. We also indicate that the calibration module\nand interleaved optimization play a vital role to\nboost the performance. Finally, extending the cali-\nbration module to multi-lingual pre-trained model\nis a promising future research direction.\n47\nReferences\nRahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars,\nLaurent Charlin, Massimo Caccia, Min Lin, and Lu-\ncas Page-Caccia. 2019. Online continual learning\nwith maximal interfered retrieval. In Advances in\nNeural Information Processing Systems (NeurIPS),\npages 11849–11860, Vancouver, Canada.\nAntoine Bordes, Y-Lan Boureau, and Jason Weston.\n2016. Learning end-to-end goal-oriented dialog.\narXiv preprint arXiv:1605.07683.\nPawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gasic. 2018. Multiwoz - A large-\nscale multi-domain wizard-of-oz dataset for task-\noriented dialogue modelling. pages 5016–5026.\nPietro Buzzega, Matteo Boschini, Angelo Porrello, Da-\nvide Abati, and Simone Calderara. 2020. Dark expe-\nrience for general continual learning: a strong, simple\nbaseline. In Advances in Neural Information Process-\ning Systems (NeurIPS), virtual.\nArslan Chaudhry, Puneet Kumar Dokania, Tha-\nlaiyasingam Ajanthan, and Philip H. S. Torr. 2018.\nRiemannian walk for incremental learning: Under-\nstanding forgetting and intransigence. In Proceed-\nings of the 15th European Conference on Computer\nVision (ECCV), Part XI, pages 556–572, Munich, Ger-\nmany.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (NAACL-HLT), pages 4171–4186, Min-\nneapolis, MN.\nRonald Kemker, Marc McClure, Angelina Abitino,\nTyler L. Hayes, and Christopher Kanan. 2018. Mea-\nsuring catastrophic forgetting in neural networks. In\nProceedings of the Thirty-Second AAAI Conference\non Artificial Intelligence (AAAI), pages 3390–3398,\nNew Orleans, LN.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, and Ag-\nnieszka Grabska-Barwinska. 2017. Overcoming\ncatastrophic forgetting in neural networks. Pro-\nceedings of the national academy of sciences ,\n114(13):3521–3526.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\narXiv preprint arXiv:1910.13461.\nDingcheng Li, Zheng Chen, Eunah Cho, Jie Hao, Xi-\naohu Liu, Fan Xing, Chenlei Guo, and Yang Liu.\n2022a. Overcoming catastrophic forgetting during\ndomain adaptation of seq2seq language generation.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5441–5454.\nDingcheng Li, Peng Yang, Zhuoyi Wang, and Ping Li.\n2022b. Power norm based lifelong learning for para-\nphrase generations. preprint.\nZhizhong Li and Derek Hoiem. 2017. Learning without\nforgetting. IEEE transactions on pattern analysis\nand machine intelligence, 40(12):2935–2947.\nZheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe,\nHyunwoo Kim, and Scott Sanner. 2021. Online con-\ntinual learning in image classification: An empirical\nsurvey. arXiv preprint arXiv:2101.10423.\nFei Mi, Liangwei Chen, Mengjie Zhao, Minlie Huang,\nand Boi Faltings. 2020. Continual learning for\nnatural language generation in task-oriented dialog\nsystems. In Findings of the Association for Com-\nputational Linguistics (EMNLP Findings), volume\nEMNLP 2020, pages 3461–3474, Online Event.\nCuong V . Nguyen, Yingzhen Li, Thang D. Bui, and\nRichard E. Turner. 2018. Variational continual learn-\ning. In Proceedings of the 6th International Confer-\nence on Learning Representations (ICLR), Vancou-\nver, Canada.\nGerman Ignacio Parisi, Ronald Kemker, Jose L. Part,\nChristopher Kanan, and Stefan Wermter. 2019. Con-\ntinual lifelong learning with neural networks: A re-\nview. Neural Networks, 113:54–71.\nQuang Pham, Chenghao Liu, Doyen Sahoo, and Steven\nC. H. Hoi. 2021. Contextual transformation networks\nfor online continual learning. In Proceedings of the\n9th International Conference on Learning Represen-\ntations (ICLR), Virtual Event.\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\nSperl, and Christoph H. Lampert. 2017. iCaRL: In-\ncremental classifier and representation learning. In\nProceedings of the 2017 IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n5533–5542, Honolulu, HI.\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Timo-\nthy P. Lillicrap, and Gregory Wayne. 2019. Experi-\nence replay for continual learning. In Advances in\nNeural Information Processing Systems (NeurIPS),\npages 348–358, Vancouver, Canada.\nAndrei A Rusu, Neil C Rabinowitz, Guillaume Des-\njardins, Hubert Soyer, James Kirkpatrick, Koray\nKavukcuoglu, Razvan Pascanu, and Raia Hadsell.\n2016. Progressive neural networks. arXiv preprint\narXiv:1606.04671.\nJonathan Schwarz, Wojciech Czarnecki, Jelena\nLuketina, Agnieszka Grabska-Barwinska, Yee Whye\nTeh, Razvan Pascanu, and Raia Hadsell. 2018.\n48\nProgress & compress: A scalable framework for\ncontinual learning. In Proceedings of the 35th\nInternational Conference on Machine Learning\n(ICML), pages 4535–4544, Stockholm, Sweden.\nJoan Serra, Didac Suris, Marius Miron, and Alexandros\nKaratzoglou. 2018. Overcoming catastrophic forget-\nting with hard attention to the task. In International\nConference on Machine Learning, pages 4548–4557.\nPMLR.\nPravendra Singh, Vinay Kumar Verma, Pratik\nMazumder, Lawrence Carin, and Piyush Rai. 2020.\nCalibrating cnns for lifelong learning. In Advances\nin Neural Information Processing Systems (NeurIPS),\nvirtual.\nJohannes von Oswald, Christian Henning, João Sacra-\nmento, and Benjamin F. Grewe. 2020. Continual\nlearning with hypernetworks. In Proceedings of the\n8th International Conference on Learning Represen-\ntations (ICLR), Addis Ababa, Ethiopia.\nZhuoyi Wang, Dingcheng Li, and Ping Li. 2022. Latent\ncoreset sampling based data-free continual learning.\nIn Proceedings of the 31st ACM International Con-\nference on Information and Knowledge Management\n(CIKM), Atlanta, GA.\nHaiyan Yin, Peng Yang, and Ping Li. 2021. Mitigat-\ning forgetting in online continual learning with neu-\nron calibration. In Advances in Neural Information\nProcessing Systems (NeurIPS), pages 10260–10272,\nvirtual.\nFriedemann Zenke, Ben Poole, and Surya Ganguli. 2017.\nContinual learning through synaptic intelligence. In\nProceedings of the 34th International Conference on\nMachine Learning (ICML), pages 3987–3995, Syd-\nney, Australia.\nMengyao Zhai, Lei Chen, Frederick Tung, Jiawei He,\nMegha Nawhal, and Greg Mori. 2019. Lifelong\nGAN: continual learning for conditional image gen-\neration. In Proceedings of the 2019 IEEE/CVF In-\nternational Conference on Computer Vision (ICCV),\npages 2759–2768, Seoul, Korea.\n49",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7816991209983826
    },
    {
      "name": "Forgetting",
      "score": 0.743330717086792
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6786999702453613
    },
    {
      "name": "Machine learning",
      "score": 0.587887167930603
    },
    {
      "name": "Transformer",
      "score": 0.5718448162078857
    },
    {
      "name": "Inference",
      "score": 0.5126158595085144
    },
    {
      "name": "Sequence learning",
      "score": 0.4809959828853607
    },
    {
      "name": "Natural language",
      "score": 0.44424429535865784
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.4100171625614166
    },
    {
      "name": "Natural language processing",
      "score": 0.32854163646698
    },
    {
      "name": "Engineering",
      "score": 0.12149232625961304
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ]
}