{
  "title": "Encoding Musical Style with Transformer Autoencoders",
  "url": "https://openalex.org/W2995416527",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227464085",
      "name": "Choi, Kristy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221726141",
      "name": "Hawthorne, Curtis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226797544",
      "name": "Simon, Ian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288527787",
      "name": "Dinculescu, Monica",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221726144",
      "name": "Engel, Jesse",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2210838531",
    "https://openalex.org/W2952428868",
    "https://openalex.org/W2964669873",
    "https://openalex.org/W2951004968",
    "https://openalex.org/W2958816042",
    "https://openalex.org/W2145094598",
    "https://openalex.org/W189596042",
    "https://openalex.org/W2803963372",
    "https://openalex.org/W2792210438",
    "https://openalex.org/W2969833269",
    "https://openalex.org/W2766669584",
    "https://openalex.org/W2946521317",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2181347294",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W2949382160",
    "https://openalex.org/W2963636093",
    "https://openalex.org/W2785779000",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2910577860",
    "https://openalex.org/W2898827701",
    "https://openalex.org/W2025768430",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W2898148140",
    "https://openalex.org/W2769811909",
    "https://openalex.org/W2606176153",
    "https://openalex.org/W2606712314",
    "https://openalex.org/W2188365844",
    "https://openalex.org/W2963073614",
    "https://openalex.org/W2027518030",
    "https://openalex.org/W2963408210",
    "https://openalex.org/W2100495367",
    "https://openalex.org/W2963135265",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W1531663008"
  ],
  "abstract": "We consider the problem of learning high-level controls over the global structure of generated sequences, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance. We show it is possible to combine this global representation with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and melody. Empirically, we demonstrate the effectiveness of our method on various music generation tasks on the MAESTRO dataset and a YouTube dataset with 10,000+ hours of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to baselines.",
  "full_text": "Encoding Musical Style with Transformer Autoencoders\nKristy Choi 1 * Curtis Hawthorne 2 Ian Simon 2 Monica Dinculescu 2 Jesse Engel 2\nAbstract\nWe consider the problem of learning high-level\ncontrols over the global structure of generated se-\nquences, particularly in the context of symbolic\nmusic generation with complex language models.\nIn this work, we present the Transformer autoen-\ncoder, which aggregates encodings of the input\ndata across time to obtain a global representation\nof style from a given performance. We show it\nis possible to combine this global representation\nwith other temporally distributed embeddings, en-\nabling improved control over the separate aspects\nof performance style and melody. Empirically, we\ndemonstrate the effectiveness of our method on\nvarious music generation tasks on the MAESTRO\ndataset and a YouTube dataset with 10,000+ hours\nof piano performances, where we achieve im-\nprovements in terms of log-likelihood and mean\nlistening scores as compared to baselines.\n1. Introduction\nThere has been signiﬁcant progress in generative modeling,\nparticularly with respect to creative applications such as art\nand music (Oord et al., 2016; Engel et al., 2017b; Ha & Eck,\n2017; Huang et al., 2019a; Payne, 2019). As the number\nof generative applications increase, it becomes increasingly\nimportant to consider how users can interact with such sys-\ntems, particularly when the generative model functions as\na tool in their creative process (Engel et al., 2017a; Gillick\net al., 2019) To this end, we consider how one can learn\nhigh-level controls over the global structure of a generated\nsample. We focus on the domain of symbolic music genera-\ntion, where Music Transformer (Huang et al., 2019b) is the\ncurrent state-of-the-art in generating high-quality samples\nthat span over a minute in length.\nThe challenge in controllable sequence generation is\n1Department of Computer Science, Stanford University *Work\ncompleted during an internship at Google Brain 2Google Brain.\nCorrespondence to: Kristy Choi <kechoi@cs.stanford.edu>.\nProceedings of the 37 th International Conference on Machine\nLearning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by\nthe author(s).\ntwofold. First, Transformers (Vaswani et al., 2017) and\ntheir variants excel as unconditional language models or in\nsequence-to-sequence tasks such as translation, but it is less\nclear as to how they can: (1)learn and (2) incorporate global\nconditioning information at inference time. This contrasts\nwith traditional generative models for images such as the\nvariational autoencoder (V AE) (Kingma & Welling, 2013)\nor generative adversarial network (GAN) (Goodfellow et al.,\n2014), both of which can incorporate global conditioning\ninformation (e.g. one-hot encodings of class labels) as part\nof their training procedure (Sohn et al., 2015; Sønderby\net al., 2016; Isola et al., 2017; Van den Oord et al., 2016).\nSecond, obtaining a ”ground truth” annotation that captures\nall the salient features of a musical performance may be\na prohibitively difﬁcult or expensive task that requires do-\nmain expertise (Bertin-Mahieux et al., 2011). Thus even if\nconditioning was straightforward, the set of performance\nfeatures that will be relevant for synthesizing a desired sam-\nple will remain ambiguous without descriptive tags to guide\ngeneration.\nIn this work, we introduce the Transformer autoencoder,\nwhere we aggregate encodings across time to obtain a holis-\ntic representation of the performance style in an unsuper-\nvised fashion. We demonstrate that this learned global repre-\nsentation can be incorporated with other forms of structural\nconditioning in two ways. First, we show that given a per-\nformance, our model can generate samples that are similar\nin style to the provided input. Then, we explore different\nmethods to combine melody and performance representa-\ntions to harmonize a new melody in the style of the given\nperformance. We validate this notion of ”perceptual simi-\nlarity” through quantitative analyses based on note-based\nfeatures of performances as well as qualitative user listening\nstudies and interpolations. In both cases, we show that com-\nbining both global and ﬁne-scale encodings of the musical\nperformance allows us to gain better control of generation,\nseparately manipulating both the style and melody of the\nresulting sample without the need for explicit labeling.\nEmpirically, we evaluate our model on two datasets: the\npublicly-available MAESTRO (Hawthorne et al., 2019)\ndataset, and a YouTube dataset of piano performances tran-\nscribed from 10,000+ hours of audio (Simon et al., 2019).\nWe ﬁnd that the Transformer autoencoder is able to generate\nnot only performances that sound similar to the input, but\narXiv:1912.05537v2  [cs.SD]  30 Jun 2020\nEncoding Musical Style with Transformer Autoencoders\nFigure 1.A ﬂowchart of the Transformer autoencoder. We ﬁrst transcribe the .wav data ﬁles into MIDI using the Onsets and Frames\nframework, then encode them into performance representations to use as input. The output of the performance encoder is then aggregated\nacross time and (optionally) combined with a melody embedding to produce a representation of the entire performance, which is then\nused by the Transformer decoder at inference time.\nalso accompaniments of melodies that follow a given style.\nIn particular, we demonstrate that our model is capable of\nadapting to a particular musical style at test time even in the\ncase where we have one single input performance.\n2. Preliminaries\n2.1. Data Representation for Music Generation\nThe MAESTRO (Hawthorne et al., 2019) dataset consists\nof over 1,100 classical piano performances, where each\npiece is represented as a MIDI ﬁle. The YouTube perfor-\nmance dataset is comprised of approximately 400K piano\nperformances (over 10,000 hours) transcribed from audio\n(Simon et al., 2019). In both cases, we represent music as\na sequence of discrete tokens, effectively formulating the\ngeneration task as a language modeling problem. The per-\nformances are encoded using the vocabulary as described\nin (Oore et al., 2018), which captures expressive dynamics\nand timing. This performance encoding vocabulary con-\nsists of 128 note on events, 128 note off events, 100\ntime shift events representing time shifts in 10ms incre-\nments from 10ms to 1s, and 32 quantized velocity bins\nrepresenting the velocity at which the 128 note on events\nwere played. We provide additional details of the data rep-\nresentation, encoding mechanism, and melody extraction\nprocedure in the supplementary material.\n2.2. Music Transformer\nWe build our Transformer autoencoder from Music Trans-\nformer, a state-of-the-art generative model that is capable of\ngenerating music with long-term coherence (Huang et al.,\n2019b). While the original Transformer uses self-attention\nto operate over absolute positional encodings of each to-\nken in a given sequence (Vaswani et al., 2017), Music\nTransformer replaces this mechanism with relative attention\n(Shaw et al., 2018), which allows the model to keep better\ntrack of regularity based on event orderings and periodicity\nin the performance. (Huang et al., 2019b) propose a novel\nalgorithm for implementing relative self-attention that is\nsigniﬁcantly more memory-efﬁcient, enabling the model\nto generate musical sequences that span over a minute in\nlength. For more details regarding the self-attention mech-\nanism and Transformers, we refer the reader to (Vaswani\net al., 2017; Parmar et al., 2018).\n3. Generation with Transformer Autoencoder\n3.1. Model Architecture\nWe leverage the standard encoder and decoder stacks of\nthe Transformer as the foundational building block for our\nmodel, with minor modiﬁcations that we outline below.\nTransformer Encoder: For both the performance and\nmelody encoder networks, we use the Transformer’s stack\nof 6 layers which are each comprised of a: (1) multi-head\nrelative attention mechanism; and a (2) position-wise fully-\nconnected feed-forward network. The performance encoder\ntakes as input the event-based performance encoding of an\ninput performance, while the melody encoder learns an en-\ncoding of the melody which has been extracted from the\ninput performance. Depending on the music generation task\n(Section 3.2), the encoder output(s) are fed into the Trans-\nformer decoder. Figure 1 describes the way in which the\nencoder and decoder networks are composed together.\nTransformer Decoder: The decoder shares the same struc-\nture as the encoder network, but with an additional multi-\nhead attention layer over the encoder outputs. At each step\nof the generation process, the decoder takes in the output of\nthe encoder, as well as each new token that was generated\nEncoding Musical Style with Transformer Autoencoders\nin the previous timestep.\n3.2. Conditioning Tasks and Mechanisms\nPerformance Conditioning and Bottleneck: We aim to\ngenerate samples that sound “similar” to a conditioning in-\nput performance. We incorporate a bottleneck in the output\nof the Transformer encoder in order to prevent the model\nfrom simply memorizing the input (Baldi, 2012). As shown\nin Figure 1, we mean-aggregate the performance embedding\nacross the time dimension in order to learn a global repre-\nsentation of style. This mean-performance embedding is\nthen fed into the autoregressive decoder, where the decoder\nattends to this global representation in order to predict the\nappropriate target. Although this bottleneck may be unde-\nsirable in sequence transduction tasks where the input and\noutput sequences differ (e.g. translation), we ﬁnd that it\nworks well in our setting where we require the generated\nsamples to be similar in style to the input sequence.\nMelody & Performance Conditioning: Next, we synthe-\nsize any given melody in the style of adifferent performance.\nAlthough the setup is similar to the melody conditioning\nproblem in (Huang et al., 2019b), we note that we also\nprovide a conditioning performance signal, which makes\nthe generation task more challenging. During training, we\nextract melodies from performances in the training set as out-\nlined in (Waite, 2016), quantize the melody to a 100ms grid,\nand encode it as a sequence of tokens that uses a different\nvocabulary than the performance representation. For more\ndetails regarding the exact melody extraction procedure, we\nrefer the reader to the supplement. We then use two distinct\nTransformer encoders (each with the same architecture) as\nin Section 3.1 to separately encode the melody and perfor-\nmance inputs. The melody and performance embeddings\nare combined to use as input to the decoder.\nWe explore various ways of combining the intermediate rep-\nresentations: (1) sum, where we add the performance and\nmelody embeddings together; (2) concatenate, where\nwe concatenate the two embeddings separated with a stop\ntoken; and (3) tile, where we tile the performance embed-\nding across every dimension of time in the melody encoding.\nIn all three cases, we work with the mean-aggregated repre-\nsentation of the input performance. We ﬁnd that different\napproaches work better than others on some dataets, a point\nwhich we elaborate upon in Section 5.\n3.3. Model Training\nInput Perturbation: In order to encourage the encoded\nperformance representations to generalize across various\nmelodies, keys, and tempos, we draw inspiration from the\ndenoising autoencoder (Vincent et al., 2008) as a means to\nregularize the model. For every target performance from\nwhich we extract the input melody, we provide the model\nwith a perturbed version of the input performance as the\nconditioning signal. We allow this ”noisy” performance to\nvary across two axes of variation: (1) pitch, where we\nartiﬁcially shift the overall pitch either down or up by 6\nsemitones; and (2) time, where we stretch the timing of\nthe performance by at most ±5%. Then for each new data\npoint during training, a single noise injection procedure is\nrandomly sampled from the cross product of all possible\ncombinations of 12 pitch shift values and 4 time stretch\nvalues (evaluated in intervals of 2.5%). At test time, the data\npoints are left unperturbed. In our experiments, we ﬁnd that\nthis augmentation procedure leads to samples that sound\nmore pleasing (Oore et al., 2018).\nFinally, the model is trained end-to-end with maximum\nlikelihood: for a given sequence xof length n, we maximize\nlog pθ(x) =∑n\ni=1 log pθ(xi|x<i) with respect to the model\nparameters θ. We emphasize that training is conducted in\nan autoencoder-like fashion. Speciﬁcally, for performance-\nonly conditioning, the Transformer decoder is tasked with\npredicting the same performance provided to the encoder.\nFor melody & performance conditioning, the Transformer\nautoencoder is trained to predict a new performance using\nthe combined melody + performance embedding, where the\nloss is computed with respect to the input performance.\n4. Performance Similarity Evaluation\nAs the development of a proper metric to quantify both\nthe quality and similarity of musical performances remains\nan open question (Engel et al., 2019), we draw inspiration\nfrom (Yang & Lerch, 2018; Hung et al., 2019) to capture\nthe style of a given performance based on eight features\ncorresponding to its pitch and rhythm:\n1. Note Density (ND): The note density refers to the av-\nerage number of notes per second in a performance: a\nhigher note density often indicates a fast-moving piece,\nwhile a lower note density correlates with softer, slower\npieces. This feature is a good indicator for rhythm.\n2. Pitch Range (PR): The pitch range denotes the differ-\nence between the highest and lowest semitones (MIDI\npitches) in a given phrase.\n3. Mean Pitch (MP) / Variation of Pitch (VP): Similar in\nvein to the pitch range (PR), the average and overall\nvariation of pitch in a musical performance captures\nwhether the piece is played in a higher or lower octave.\n4. Mean Velocity (MV) / Variation of Velocity (VV): The\nvelocity of each note indicates how hard a key is\npressed in a musical performance, and serves as a\nheuristic for overall volume.\nEncoding Musical Style with Transformer Autoencoders\nModel variation MAESTRO YouTube\nUnconditional model with rel. attention (Huang et al., 2019b) 1.840 1.49\nPerformance autoencoder with rel. attention (ours) 1.799 1.384\nTable 1.Note-wise test NLL on the MAESTRO and YouTube datasets. We exclude the performance autoencoder baseline (no aggregation)\nas it memorized the data (NLL = 0). Conditional models outperformed their unconditional counterparts.\nModel variation MAESTRO YouTube\nMelody-only Transformer with rel. attention (Huang et al., 2019b) 1.786 1.302\nMelody & performance autoencoder with rel. attention, sum (ours) 1.706 1.275\nMelody & performance autoencoder with rel. attention, concat (ours) 1.713 1.237\nMelody & performance autoencoder with rel. attention, tile (ours) 1.709 1.248\nTable 2.Note-wise test NLL on the MAESTRO and YouTube datasets with melody conditioning. We note that sum worked best for\nMAESTRO, while concatenate outperformed all other baselines for the YouTube dataset.\n5. Mean Duration (MD) / Variation of Duration (VD) :\nThe duration describes for how long each note is\npressed in a performance, representing articulation,\ndynamics, and phrasing.\n4.1. Overlapping Area (OA) Metric\nTo best capture the salient features within the periodic struc-\nture of a musical performance, we used a sliding window\nof 2s to construct histograms of the desired feature within\neach window. We found that representing each performance\nwith such relative measurements better preserved changing\ndynamics and stylistic motifs across the entire performance\nas opposed to a single scalar value (e.g. average note density\nacross the entire performance).\nSimilar to (Yang & Lerch, 2018; Hung et al., 2019), we\nsmoothed each feature’s histogram by ﬁtting a Gaussian dis-\ntribution – this allowed us to learn a compact representation\nper feature through its mean µand variance σ2. Then to\ncompare two performances, we computed the Overlapping\nArea (OA) between the Gaussian pdfs of each feature to\nquantify their similarity. The OA can be used to pinpoint\nfeature-wise similarities between two performances, while\nthe average OA across all features (OAavg) can be used as\na scalar-value summary to compare two performances to-\ngether. We use both variants to quantify how similar two\nmusical performances are in terms of their relative features.\nConcretely, suppose we compare two performances A and B\nfor the ”pitch range” feature. If we model A∼N(µ1,σ2\n1)\nand B ∼N (µ2,σ2\n2), and let c denote the point of inter-\nsection between the two pdfs (assuming without loss of\ngenerality that µ1 >µ2), the OA between A and B is:\nOA(A,B) = 1−erf\n(c−µ1√\n2σ2\n1\n)\n+ erf\n(c−µ2√\n2σ2\n2\n)\n(1)\nwhere erf(·) denotes the error function erf(x) =\n2√π\n∫x\n0 e−t2\ndt. We found that other divergences such as\nthe Kullback-Leibler (KL) divergence and the symmetrized\nKL were more sensitive to performance-speciﬁc features\n(rather than melody) than the OA. Empirically, we demon-\nstrate that this metric identiﬁes the relevant characteristics\nof interest in our generated performances in Section 5.\nWe note that for the melody & performance conditioning\ncase, we performed similarity evaluations of our samples\nagainst the original performance from which the melody was\nextracted, as opposed to the melody itself. This is because\nthe melody (a monophonic sequence) is represented using\na different encoding and vocabulary than the performance\n(a polyphonic sequence). Speciﬁcally, we average two OA\nterms: (1) OA(source performance of extracted melody,\ngenerated sample) and (2) OA(conditioning performance,\ngenerated sample), as our ﬁnal similarity metric. In this way,\nwe account for the contributions of both the conditioning\nmelody and performance sequence.\n5. Experimental Results\nDatasets: We used both the MAESTRO (Hawthorne\net al., 2019) and YouTube datasets (Simon et al., 2019)\nfor the experimental setup. We used the standard 80/10/10\ntrain/validation/test split from MAESTRO v1.0.0, and aug-\nmented the dataset by 10x using pitch shifts of no more than\na minor third and time stretches of at most 5%. We note\nthat this augmentation is distinct from the noise-injection\nprocedure referenced in Section 3: the data augmentation\nmerely increases the size of the initial dataset, while the per-\nturbation procedure operates only on the input performance\nsignal to regularize the learned model. The YouTube dataset\ndid not require any additional augmentation.\nEncoding Musical Style with Transformer Autoencoders\nMAESTRO ND PR MP VP MV VV MD VD Avg\nPerformance (ours) 0.651 0.696 0.634 0.689 0.693 0.732 0.582 0.692 0.67\nUnconditional 0.370 0.466 0.435 0.485 0.401 0.606 0.385 0.529 0.46\nYouTube Dataset\nPerformance (ours) 0.731 0.837 0.784 0.838 0.778 0.835 0.785 0.827 0.80\nUnconditional 0.466 0.561 0.556 0.578 0.405 0.590 0.521 0.624 0.54\nTable 3.Average overlapping area (OA) similarity metrics comparing performance conditioned models with unconditional models.\nUnconditional and Melody-only baselines are from (Huang et al., 2019b). The metrics are described in detail in Section 4. The samples in\nthis quantitative comparison are used for the listener study shown in the left graph of Figure 4.\nMAESTRO ND PR MP VP MV VV MD VD Avg\nMelody & perf. (ours) 0.650 0.696 0.634 0.689 0.692 0.732 0.582 0.692 0.67\nPerf-only (ours) 0.600 0.695 0.657 0.721 0.664 0.740 0.527 0.648 0.66\nMelody-only 0.609 0.693 0.640 0.693 0.582 0.711 0.569 0.636 0.64\nUnconditional 0.376 0.461 0.423 0.480 0.384 0.588 0.347 0.520 0.48\nYouTube Dataset\nMelody & perf (ours) 0.646 0.708 0.610 0.717 0.590 0.706 0.658 0.743 0.67\nPerf-only (ours) 0.624 0.646 0.624 0.638 0.422 0.595 0.601 0.702 0.61\nMelody-only 0.575 0.707 0.662 0.718 0.583 0.702 0.634 0.707 0.66\nUnconditional 0.476 0.580 0.541 0.594 0.400 0.585 0.522 0.623 0.54\nTable 4.Average overlapping area (OA) similarity metrics comparing models with different conditioning. Unconditional and Melody-only\nbaselines are from (Huang et al., 2019b). The metrics are described in detail in Section 4. The samples in this quantitative comparison are\nused for the listener study shown in the right graph of Figure 4.\nExperimental Setup: We implemented the model in the\nTensor2Tensor framework (Vaswani et al., 2017), and used\nthe default hyperparameters for training: 0.2 learning rate\nwith 8000 warmup steps, rsqrt decay, 0.2 dropout, and\nearly stopping for GPU training. For TPU training, we\nuse AdaFactor with the rsqrt decay and learning rate\nwarmup steps to be 10K. We adopt many of the hyperpa-\nrameter conﬁgurations from (Huang et al., 2019b), where\nwe reduce the query and key hidden size to half the hidden\nsize, use 8 hidden layers, use 384 hidden units, and set the\nmaximum relative distance to consider to half the training\nsequence length for relative global attention. We set the\nmaximum sequence length (length of event-based represen-\ntations) to be 2048 tokens, and a ﬁlter size of 1024. We\nprovide additional details on the model architectures and\nhyperparameter conﬁgurations in the supplement.\n5.1. Log-Likelihood Evaluation\nAs expected, the Transformer autoencoder with the encoder\nbottleneck outperformed other baselines. In Tables 1 and\n2, we see that all conditional model variants outperform\ntheir unconditional counterparts. For the melody & per-\nformance model, different methods of combining the em-\nbeddings work better for different datasets. For example,\nconcatenate led to the lowest NLL for the YouTube\ndataset, while sum outperformed all other variants for MAE-\nSTRO. We report NLL values for both datasets for the\nperturbed-input model variants in the supplement.\n5.2. Similarity Evaluation\nWe use the OA metric from Section 4 to evaluate whether\nusing a conditioning signal in both the (a) performance\nautoencoder (Perf-only) and (b) melody & performance\nautoencoder (Melody & perf) produces samples that are\nmore similar in style to the conditioning inputs from the\nevaluation set relative to other baselines.\nFirst, we sample 500 examples from the test set as con-\nditioning signals to generate one sample per input. Then,\nwe compare each conditioning signal to: (1) the generated\nsample and (2) an unconditional sample from the Music\nTransformer. We compute the similarity metric as in Sec-\ntion 4 pairwise and average over 500 examples. As shown\nin Tables 3 and 4, the performance autoencoder generates\nsamples that have 48% higher similarity to the condition-\ning input as compared to the unconditional baseline for the\nYouTube dataset (45% higher similarity for MAESTRO).\nFor the melody & performance autoencoder, we sample\nEncoding Musical Style with Transformer Autoencoders\n(a) Relative distance to performance A\n (b) Relative distance to melody A\nFigure 2.For the YouTube dataset, relative distance from performance A (α= 1) as αis slowly increased to 1.0 while the conditioned\nmelody is ﬁxed. As in (b), the relative distance to the conditioning melody with respect to a random performance remains ﬁxed while the\ninterpolation is conducted between performances A and B, suggesting that we can control for elements of style and melody separately.\n717*2 distinct performances – we reserve one set of 717\nfor conditioning performance styles, and the other set of\n717 we use to extract melodies in order to synthesize in the\nstyle of a different performance. We compare the melody &\nperformance autoencoder to 3 different baselines: (1) one\nthat is conditioned only on the melody (Melody-only); (2)\nconditioned only on performance (Perf-only); and (3) an\nunconditional language model. We ﬁnd that the Melody &\nperformance autoencoder performs the best overall across\nalmost all features.\n5.3. Latent Space Interpolations\nNext, we analyze whether the Transformer autoencoder\nlearns a semantically meaningful latent space through a\nvariety of interpolation experiments on both model variants.\n5.3.1. P ERFORMANCE AUTOENCODER\nWe test whether the performance autoencoder can suc-\ncessfully interpolate between different input performances.\nFirst, we sample 1000 performances from the YouTube\ntest set (100 for MAESTRO, due to its smaller size), and\nsplit this dataset in half. The ﬁrst half we reserve for\nthe original starting performance, which we call “perfor-\nmance A”, and the other half we reserve for the end per-\nformance, denoted as “performance B.” Then we use the\nperformance encoder to embed performance A into zA, and\ndo the same for performance B to obtain zB. For a range\nα ∈ [0,0.125,..., 0.875,1.0], we sample a new perfor-\nmance perfnew that results from decodingα·zA+(1−α)·zB.\nWe observe how theOAavg (averaged across all features) de-\nﬁned in Section 4 changes between this newly interpolated\nperformance perfnew and performances {A, B}.\nSpeciﬁcally, we compute the similarity metric between each\ninput performance A and interpolated sample perfnew for\nall 500 samples, and compute the same pairwise similar-\nity for each performance B. We then compute the normal-\nized distance between each interpolated sample and the\ncorresponding performance A or B, which we denote as:\nrel distance(perf A) = 1− OA A\nOA A + OA B , where the\nOA is averaged across all features. We average this distance\nacross all elements in the set and ﬁnd in Figure 3 that the\nrelative distance between performance A slowly increases\nas we increase αfrom 0 to 1, as expected. We note that\nit is not possible to conduct this interpolation study with\nnon-aggregated baselines, as we cannot interpolate across\nvariable-length embeddings. We ﬁnd that a similar trend\nholds for MAESTRO as in Figure 2(a).\nFigure 3.For the YouTube dataset, the relative distance from per-\nformance A (α= 1) to the interpolated sample increases as αis\nslowly increased to 1.0.\n5.3.2. M ELODY & PERFORMANCE AUTOENCODER\nWe conduct a similar study as above with the melody &\nperformance autoencoder. We hold out 716 unique melody-\nperformance pairs (melody is not derived from the same per-\nformance) from the YouTube evaluation dataset and 50 ex-\namples from MAESTRO. We then interpolate across the dif-\nferent performances, while keeping the conditioning melody\ninput the same across the interpolations.\nEncoding Musical Style with Transformer Autoencoders\n0           50         100        150        200      250\nPerformance Conditioning Study\nConditioned\nGround truth\nUnconditioned\nNumber of wins\n(a) Performance conditioning study\nMelody & Performance Conditioning Study\nPerformance only\nMelody only\nMelody & Performance\nUnconditioned\nNumber of wins\n 0            50          100         150        200       250 (b) Melody conditioning study\nFigure 4.Results of our listening studies, showing the number of times each source won in a pairwise comparison. Black error bars\nindicate estimated standard deviation of means.\nAs shown in Figure 2(a), we ﬁnd that a similar trend holds\nas in the performance autoencoder: the newly-interpolated\nsamples show that the relative distance between perfor-\nmance A increases as we increase the corresponding value\nof α. We note that the interpolation effect is slightly lower\nthan that of the previous section, particularly because the\ninterpolated sample is also dependent on the melody that\nit is conditioned on. Interestingly, in Figure 2(b), we note\nthat the relative distance between the input performance\nfrom which we derived the original melody remains fairly\nconstant across the interpolation procedure. This suggests\nthat we are able to factorize out the two sources of variation\nand that varying the axis of the input performance keeps the\nvariation in melody constant.\n5.4. Listening Tests\nTo further evaluate the perceived effect of performance and\nmelody conditioning on the generated output, we also con-\nducted qualitative listening tests. Using models trained on\nthe YouTube dataset, we conducted two studies for separate\nmusic generation tasks: one for the performance condition-\ning, and one for melody and performance conditioning.\n5.4.1. P ERFORMANCE CONDITIONING\nFor performance conditioning, we presented participants\nwith a 20s performance clip from the YouTube evaluation\ndataset that we used as a conditioning signal. We then asked\nthem to listen to two additional 20s performance clips and\nto use a Likert scale to rate which one sounded most similar\nin style to the conditioning signal. The sources rated by\nthe participants included “Ground Truth” (a different snip-\npet of the same sample used for the conditioning signal),\n“Conditioned” (output of the Performance Autoencoder),\nand “Unconditioned” (output of unconditional Music Trans-\nformer). We collected a total of 492 ratings, with each\nsource involved in 328 distinct pair-wise comparisons.\n5.4.2. M ELODY AND PERFORMANCE CONDITIONING\nFor melody and performance conditioning, we similarly\npresented participants with a 20s performance clip from\nthe YouTube evaluation dataset and a 20s melody from a\ndifferent piece in the evaluation dataset that we used as our\nconditioning signals. We then asked each participant to\nlisten to two additional 20s performance clips and to use a\nLikert scale to rate which sounded most like the conditioning\nmelody played in the style of the conditioning performance.\nThe sources rated by the participants included “Melody &\nPerformance” (output of the Melody-Performance Autoen-\ncoder), “Melody only” (output of a model conditioned only\non the melody signal), “Performance only” (output of a\nmodel conditioned only on the performance signal), and\n“Unconditioned” (output of an unconditional model). For\nthis study, we collected a total of 714 ratings, with each\nsource involved in 357 distinct pair-wise comparisons.\nFigure 4 shows the number of comparisons in which each\nsource was selected as being most similar in style to the\nconditioning signal. A Kruskal-Wallis H test of the rat-\nings showed that there is at least one statistically signiﬁcant\ndifference between the models: χ2(2) = 332.09, p <\n0.05 (7.72e−73) for melody conditioning and χ2(2) =\n277.74, p<0.05 (6.53e−60) for melody and performance\nconditioning. A post-hoc analysis using the Wilcoxon\nsigned-rank test with Bonferroni correction showed that\nthere were statistically signiﬁcant differences between all\npairs of the performance study with p <0.05/3 and all\npairs of the performance and melody study withp< 0.05/6\nexcept between the “Melody only” and “Melody & Perfor-\nmance” models (p= 0.0894).\nThese results demonstrate that the performance conditioning\nsignal has a clear, robust effect on the generated output: in\nthe 164 comparisons between “Ground Truth” and “Condi-\ntioned”, participants responded that they had a preference\nfor “Conditioned” sample 58 times.\nEncoding Musical Style with Transformer Autoencoders\nAlthough the results between “Melody-only” and “Melody\n& Performance” are close, this study demonstrates that con-\nditioning with both melody and performance outperforms\nconditioning on performance alone, and they are competitive\nwith melody-only conditioning, despite the model having to\ndeal with the complexity of incorporating both conditioning\nsignals. In fact, we ﬁnd quantitative evidence that human\nevaluation is more sensitive to perceptual melodic similarity,\nas the “Performance-only” model performs worst – a slight\ncontrast to the results from the OA metric in Section 5.2.\nOur qualitative ﬁndings from the audio examples and in-\nterpolations, coupled with the quantitative results from\nthe OA similarity metric and the listening test which\ncapture different aspects of the synthesized performance,\nsupport the ﬁnding that the Melody & Performance au-\ntoencoder offers signiﬁcant control over the generated\nsamples. We provide several audio examples demon-\nstrating the effectiveness of these conditioning signals in\nthe online supplement at https://goo.gl/magenta/\nmusic-transformer-autoencoder-examples .\n6. Related Work\nMeasuring music similarity: We note that quantifying\nmusic similarity is a difﬁcult problem. We incorporate and\nextend upon the rich line of work for measuring music sim-\niliarity in symbolic music (Ghias et al., 1995; Berenzweig\net al., 2004; Slaney et al., 2008; Hung et al., 2019; Yang &\nLerch, 2018) for our setting, in which we evaluate similari-\nties between polyphonic piano performances as opposed to\nmonophonic melodies.\nSequential autoencoders: Building on the wealth of\nautoencoding literature (Hinton & Salakhutdinov, 2006;\nSalakhutdinov & Hinton, 2009; Vincent et al., 2010), our\nwork bridges the gap between the traditional sequence-to-\nsequence framework (Sutskever et al., 2014), their recent\nadvances with various attention mechanisms (Vaswani et al.,\n2017; Shaw et al., 2018; Huang et al., 2019b), and sequen-\ntial autoencoders. Though (Wang & Wan, 2019) propose\na Transformer-based conditional V AE for story generation,\nthe self-attention mechanism is shared between the encoder\nand decoder. Most similar to our work is that of (Kaiser &\nBengio, 2018), which uses a Transformer decoder and a dis-\ncrete autoencoding function to map an input sequence into\na discretized, compressed representation. We note that this\napproach is complementary to ours, where a similar idea\nof discretization may be applied to the output of our Trans-\nformer encoder. The MusicV AE (Roberts et al., 2018) is a\nsequential V AE with a hierarchical recurrent decoder, which\nlearns an interpretable latent code for musical sequences\nthat can be used during generation time. This work builds\nupon (Bowman et al., 2015) that uses recurrence and an\nautoregressive decoder for text generation. Our Transformer\nautoencoder can be seen as a deterministic variant of the\nMusicV AE, with a complex self-attention mechanism based\non relative positioning in both the encoder and decoder ar-\nchitectures to capture more expressive features of the data\nat both the local and global scale.\nControllable generations using representation learning:\nThere is also considerable work on controllable generations,\nwhere we focus on the music domain. (Engel et al., 2017a)\nproposes to constrain the latent space of unconditional gen-\nerative models to sample with respect to some predeﬁned\nattributes, whereas we explicitly deﬁne our conditioning\nsignal in the data space and learn a global representation of\nits style during training. The Universal Music Translation\nnetwork aims to translate music across various styles, but is\nnot directly comparable to our approach as they work with\nraw audio waveforms (Mor et al., 2018). Both (Meade et al.,\n2019) and MuseNet (Payne, 2019) generate music based\non user preferences, but adopt a slightly different approach:\nthe models are speciﬁcally trained with labeled tokens (e.g.,\ncomposer and instrumentation) as conditioning input, while\nour Transformer autoencoder’s global style representation is\nlearned in an unsupervised way. We emphasize Transformer\nautoencoder’s advantage of learning unsupervised represen-\ntations of style, as obtaining ground truth annotations for\nmusic data may be prohibitively challenging.\n7. Conclusion\nWe proposed the Transformer autoencoder for conditional\nmusic generation, a sequential autoencoder model which\nutilizes an autoregressive Transformer encoder and decoder\nfor improved modeling of musical sequences with long-term\nstructure. We show that this model allows users to easily\nadapt the outputs of their generative model using even a\nsingle input performance. Through experiments on the\nMAESTRO and YouTube datasets, we demonstrate both\nquantitatively and qualitatively that our model generates\nsamples that sound similar in style to a variety of condition-\ning signals relative to baselines. For future work, it would\nbe interesting to explore other training procedures such\nas variational techniques or few-shot learning approaches\n(Finn et al., 2017; Reed et al., 2017) to account for situations\nin which the input signals are from slightly different data\ndistributions than the training set. We provide open-sourced\nimplementations in Tensorﬂow (Abadi et al., 2016) at\nhttps://goo.gl/magenta/music-transformer-\nautoencoder-code.\nAcknowledgements\nWe are thankful to Anna Huang, Hanoi Hantrakul, Aditya\nGrover, Rui Shu, and the Magenta team for insightful discus-\nsions. KC is supported by the NSF GRFP, QIF, and Stanford\nGraduate Fellowship.\nEncoding Musical Style with Transformer Autoencoders\nReferences\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,\nJ., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.\nTensorﬂow: A system for large-scale machine learning.\nIn 12th {USENIX}Symposium on Operating Systems\nDesign and Implementation ({OSDI}16), pp. 265–283,\n2016.\nBaldi, P. Autoencoders, unsupervised learning, and deep\narchitectures. In Proceedings of ICML workshop on un-\nsupervised and transfer learning, pp. 37–49, 2012.\nBerenzweig, A., Logan, B., Ellis, D. P., and Whitman, B. A\nlarge-scale evaluation of acoustic and subjective music-\nsimilarity measures. Computer Music Journal , 28(2):\n63–76, 2004.\nBertin-Mahieux, T., Eck, D., and Mandel, M. Automatic\ntagging of audio: The state-of-the-art. In Machine audi-\ntion: Principles, algorithms and systems , pp. 334–352.\nIGI Global, 2011.\nBowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Joze-\nfowicz, R., and Bengio, S. Generating sentences from\na continuous space. arXiv preprint arXiv:1511.06349,\n2015.\nEngel, J., Hoffman, M., and Roberts, A. Latent constraints:\nLearning to generate conditionally from unconditional\ngenerative models. arXiv preprint arXiv:1711.05772 ,\n2017a.\nEngel, J., Resnick, C., Roberts, A., Dieleman, S., Norouzi,\nM., Eck, D., and Simonyan, K. Neural audio synthesis\nof musical notes with wavenet autoencoders. In Proceed-\nings of the 34th International Conference on Machine\nLearning-Volume 70, pp. 1068–1077. JMLR. org, 2017b.\nEngel, J., Agrawal, K. K., Chen, S., Gulrajani, I., Donahue,\nC., and Roberts, A. Gansynth: Adversarial neural audio\nsynthesis. arXiv preprint arXiv:1902.08710, 2019.\nFinn, C., Abbeel, P., and Levine, S. Model-agnostic meta-\nlearning for fast adaptation of deep networks. In Proceed-\nings of the 34th International Conference on Machine\nLearning-Volume 70, pp. 1126–1135. JMLR. org, 2017.\nGhias, A., Logan, J., Chamberlin, D., and Smith, B. C.\nQuery by humming: musical information retrieval in\nan audio database. In Proceedings of the third ACM\ninternational conference on Multimedia , pp. 231–236,\n1995.\nGillick, J., Roberts, A., Engel, J., Eck, D., and Bamman, D.\nLearning to groove with inverse sequence transformations.\narXiv preprint arXiv:1905.06118, 2019.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY . Generative adversarial nets. In Advances in neural\ninformation processing systems, pp. 2672–2680, 2014.\nHa, D. and Eck, D. A neural representation of sketch draw-\nings. arXiv preprint arXiv:1704.03477, 2017.\nHawthorne, C., Stasyuk, A., Roberts, A., Simon, I., Huang,\nC.-Z. A., Dieleman, S., Elsen, E., Engel, J., and Eck, D.\nEnabling factorized piano music modeling and generation\nwith the MAESTRO dataset. In International Conference\non Learning Representations, 2019. URL https://\nopenreview.net/forum?id=r1lYRjC9F7.\nHinton, G. E. and Salakhutdinov, R. R. Reducing the di-\nmensionality of data with neural networks. science, 313\n(5786):504–507, 2006.\nHuang, C.-Z. A., Cooijmans, T., Roberts, A., Courville, A.,\nand Eck, D. Counterpoint by convolution. arXiv preprint\narXiv:1903.07227, 2019a.\nHuang, C.-Z. A., Vaswani, A., Uszkoreit, J., Simon, I.,\nHawthorne, C., Shazeer, N., Dai, A. M., Hoffman,\nM. D., Dinculescu, M., and Eck, D. Music trans-\nformer: Generating music with long-term structure. In\nInternational Conference on Learning Representations,\n2019b. URL https://openreview.net/forum?\nid=rJe4ShAcF7.\nHung, H.-T., Wang, C.-Y ., Yang, Y .-H., and Wang, H.-M.\nImproving automatic jazz melody generation by transfer\nlearning techniques. arXiv preprint arXiv:1908.09484,\n2019.\nIsola, P., Zhu, J.-Y ., Zhou, T., and Efros, A. A. Image-to-\nimage translation with conditional adversarial networks.\nIn Proceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pp. 1125–1134, 2017.\nKaiser, Ł. and Bengio, S. Discrete autoencoders for se-\nquence models. arXiv preprint arXiv:1801.09797, 2018.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114, 2013.\nMeade, N., Barreyre, N., Lowe, S. C., and Oore, S.\nExploring conditioning for generative music systems\nwith human-interpretable controls. arXiv preprint\narXiv:1907.04352, 2019.\nMor, N., Wolf, L., Polyak, A., and Taigman, Y . A\nuniversal music translation network. arXiv preprint\narXiv:1805.07848, 2018.\nOord, A. v. d., Dieleman, S., Zen, H., Simonyan, K.,\nVinyals, O., Graves, A., Kalchbrenner, N., Senior, A.,\nEncoding Musical Style with Transformer Autoencoders\nand Kavukcuoglu, K. Wavenet: A generative model for\nraw audio. arXiv preprint arXiv:1609.03499, 2016.\nOore, S., Simon, I., Dieleman, S., Eck, D., and Simonyan,\nK. This time with feeling: learning expressive musical\nperformance. Neural Computing and Applications, pp.\n1–13, 2018.\nParmar, N., Vaswani, A., Uszkoreit, J., Kaiser,Ł., Shazeer,\nN., Ku, A., and Tran, D. Image transformer. arXiv\npreprint arXiv:1802.05751, 2018.\nPayne, C. Musenet, 2019. URL https://openai.\ncom/blog/musenet/.\nReed, S., Chen, Y ., Paine, T., Oord, A. v. d., Eslami, S.,\nRezende, D., Vinyals, O., and de Freitas, N. Few-shot\nautoregressive density estimation: Towards learning to\nlearn distributions. arXiv preprint arXiv:1710.10304 ,\n2017.\nRoberts, A., Engel, J., Raffel, C., Hawthorne, C., and Eck,\nD. A hierarchical latent vector model for learning long-\nterm structure in music. arXiv preprint arXiv:1803.05428,\n2018.\nSalakhutdinov, R. and Hinton, G. Deep boltzmann machines.\nIn Artiﬁcial intelligence and statistics, pp. 448–455, 2009.\nShaw, P., Uszkoreit, J., and Vaswani, A. Self-attention\nwith relative position representations. arXiv preprint\narXiv:1803.02155, 2018.\nSimon, I., Huang, C.-Z. A., Engel, J., Hawthorne, C.,\nand Dinculescu, M. Generating piano music with\ntransformer. 2019. URL https://magenta.\ntensorflow.org/piano-transformer.\nSlaney, M., Weinberger, K., and White, W. Learning a\nmetric for music similarity. 2008.\nSohn, K., Lee, H., and Yan, X. Learning structured output\nrepresentation using deep conditional generative models.\nIn Advances in neural information processing systems ,\npp. 3483–3491, 2015.\nSønderby, C. K., Raiko, T., Maaløe, L., Sønderby, S. K.,\nand Winther, O. Ladder variational autoencoders. In\nAdvances in neural information processing systems, pp.\n3738–3746, 2016.\nSutskever, I., Vinyals, O., and Le, Q. V . Sequence to se-\nquence learning with neural networks. In Advances in\nneural information processing systems, pp. 3104–3112,\n2014.\nVan den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals,\nO., Graves, A., et al. Conditional image generation with\npixelcnn decoders. In Advances in neural information\nprocessing systems, pp. 4790–4798, 2016.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nVincent, P., Larochelle, H., Bengio, Y ., and Manzagol, P.-A.\nExtracting and composing robust features with denoising\nautoencoders. In Proceedings of the 25th international\nconference on Machine learning, pp. 1096–1103. ACM,\n2008.\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y ., and Man-\nzagol, P.-A. Stacked denoising autoencoders: Learning\nuseful representations in a deep network with a local de-\nnoising criterion. Journal of machine learning research,\n11(Dec):3371–3408, 2010.\nWaite, E. Generating long-term structure in\nsongs and stories, 2016. URL https:\n//magenta.tensorflow.org/2016/07/\n15/lookback-rnn-attention-rnn .\nWang, T. and Wan, X. T-cvae: Transformer-based con-\nditioned variational autoencoder for story completion.\nIn Proceedings of the Twenty-Eighth International Joint\nConference on Artiﬁcial Intelligence, IJCAI-19, pp. 5233–\n5239. International Joint Conferences on Artiﬁcial Intel-\nligence Organization, 7 2019. doi: 10.24963/ijcai.2019/\n727. URL https://doi.org/10.24963/ijcai.\n2019/727.\nYang, L.-C. and Lerch, A. On the evaluation of generative\nmodels in music. Neural Computing and Applications,\npp. 1–12, 2018.\nEncoding Musical Style with Transformer Autoencoders\nSupplementary Material\nA. Additional Details on Melody Representation\nFor the melody representation (vocabulary), we fol-\nlowed (Waite, 2016) to encode the melody as a se-\nquence of 92 unique tokens and quantized it to a\n100ms grid. For the extraction procedure, we used\nthe algorithm as outlined in the Magenta codebase\n(https://github.com/tensorflow/magenta/\nblob/master/magenta/music/\nmelody inference.py), where we use a heuristic to\nextract the note with the highest in a given performance.\nThis heuristic is based on the assumption that all melodies\ncoincide with actual notes played in the polyphonic per-\nformance. Speciﬁcally, we construct a transition matrix of\nmelody pitches and use the Viterbi algorithm to infer the\nmost likely sequence of melody events within a given frame.\nB. NLL Evaluation for ”Noisy” Model\nBelow, we provide the note-wise test NLL on the MAE-\nSTRO and YouTube datasets with melody conditioning,\nwhere the conditioning performance is perturbed by the\nprocedure outlined in Section 3.\nC. Model Architecture and Hyperparameter\nConﬁgurations\nWe mostly use the default Transformer architecture as pro-\nvided in the Tensor2Tensor framework, such as 8 self-\nattention heads as listed in the main text, and list the slight\nadjustments we made for each dataset below:\nC.1. MAESTRO\nFor the MAESTRO dataset, we follow the hyperparameter\nsetup of (Huang et al., 2019b):\n1. num hidden layers = 6\n2. hidden units = 384\n3. ﬁlter size = 1024\n4. maximum sequence length = 2048\n5. maximum relative distance = half the hidden size\n6. dropout = 0.1\nC.2. Y OUTUBE DATASET\nFor the YouTube dataset, we modify the number of hidden\nlayers to 8 and slightly increase the level of dropout.\n1. num hidden layers = 8\n2. hidden units = 384\n3. ﬁlter size = 1024\n4. maximum sequence length = 2048\n5. maximum relative distance = half the hidden size\n6. dropout = 0.15\nD. Additional Relative Distance Interpolations\nIn Figure 5, we show the interpolation relative distance re-\nsults for the (a) performance and (b) melody & performance\nTransformer autoencoders for the MAESTRO dataset.\n(a) Relative distance from interpolated sample to the original\nstarting performance.\n(b) Relative distance from the interpolated sample to the origi-\nnal melody, which is kept ﬁxed.\nFigure 5.The distance to the original performance increases as the\nvalue of αincreases in (a), as expected. In (b), we see that there is\na very slight increase in the relative distance to the original melody\nduring the interpolation procedure.\nWe ﬁnd consistent results in these interpolations as provided\nin the main text.\nE. Internal Dataset Performance Interpolations\nIn Figures 6 and 7, we provide piano rolls demon-\nstrating the effects of latent-space interpolation for the\nYouTube dataset, for both the (a) performance and (b)\nmelody & performance Transformer autoencoder respec-\ntively. For similar results in MAESTRO as well as\nadditional listening samples, we refer the reader to\nthe online supplement: https://goo.gl/magenta/\nmusic-transformer-autoencoder-examples .\nEncoding Musical Style with Transformer Autoencoders\nModel variation MAESTRO YouTube Dataset\nNoisy Melody TF autoencoder with relative attention, sum 1.721 1.248\nNoisy Melody TF autoencoder with relative attention, concat 1.719 1.249\nNoisy Melody TF autoencoder with relative attention, tile 1.728 1.253\nTable 5.Note-wise test NLL on the MAESTRO and YouTube piano performance datasets with melody conditioning, with event-based\nrepresentations of lengths L= 2048.\n(a) Original starting performance\n (b) α= 0.125\n(c) α= 0.375\n (d) α= 0.5\n(e) α= 0.625\n (f) α= 0.875\n(g) α= 1.0 (reconstruction)\n (h) Original ﬁnal performance\nFigure 6.Interpolation of a starting performance (a) from the YouTube dataset to a ﬁnal performance (h), with the coefﬁcientαcontrolling\nthe level of interpolation between the latent encodings between the two performances.\nEncoding Musical Style with Transformer Autoencoders\n(a) Original melody\n (b) Original starting performance\n(c) α= 0(reconstruction)\n (d) α= 0.125\n(e) α= 0.375\n (f) α= 0.5\n(g) α= 0.625\n (h) α= 0.875\n(i) α= 1.0 (reconstruction)\n (j) Original ﬁnal performance\nFigure 7.Interpolation of a starting performance (b) from the YouTube dataset to a ﬁnal performance (j), with the coefﬁcientαcontrolling\nthe level of interpolation between the latent encodings between the two performances. The original conditioning melody (a) is kept ﬁxed\nthroughout the interpolation.",
  "topic": "Autoencoder",
  "concepts": [
    {
      "name": "Autoencoder",
      "score": 0.7353249192237854
    },
    {
      "name": "Computer science",
      "score": 0.708998441696167
    },
    {
      "name": "Transformer",
      "score": 0.6791032552719116
    },
    {
      "name": "Active listening",
      "score": 0.6267914175987244
    },
    {
      "name": "Style analysis",
      "score": 0.5727421641349792
    },
    {
      "name": "Style (visual arts)",
      "score": 0.5484942197799683
    },
    {
      "name": "Representation (politics)",
      "score": 0.5286719799041748
    },
    {
      "name": "Piano",
      "score": 0.5163813829421997
    },
    {
      "name": "Speech recognition",
      "score": 0.4991161823272705
    },
    {
      "name": "Feature learning",
      "score": 0.4217962622642517
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4100148677825928
    },
    {
      "name": "Natural language processing",
      "score": 0.3843410611152649
    },
    {
      "name": "Machine learning",
      "score": 0.3217523396015167
    },
    {
      "name": "Deep learning",
      "score": 0.223233163356781
    },
    {
      "name": "Psychology",
      "score": 0.1037207841873169
    },
    {
      "name": "Communication",
      "score": 0.10134771466255188
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Microeconomics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Investment management",
      "score": 0.0
    },
    {
      "name": "Art history",
      "score": 0.0
    },
    {
      "name": "Incentive",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 9
}