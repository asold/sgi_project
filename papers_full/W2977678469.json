{
    "title": "Learning Private Neural Language Modeling with Attentive Aggregation",
    "url": "https://openalex.org/W2977678469",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2890561010",
            "name": "Shaoxiong Ji",
            "affiliations": [
                "University of Queensland"
            ]
        },
        {
            "id": "https://openalex.org/A2278106503",
            "name": "Shirui Pan",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A2140909072",
            "name": "Guodong Long",
            "affiliations": [
                "University of Technology Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A2099141215",
            "name": "Xue Li",
            "affiliations": [
                "University of Queensland"
            ]
        },
        {
            "id": "https://openalex.org/A2007802817",
            "name": "Jing Jiang",
            "affiliations": [
                "University of Technology Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A2149257649",
            "name": "Zi Huang",
            "affiliations": [
                "University of Queensland"
            ]
        },
        {
            "id": "https://openalex.org/A2890561010",
            "name": "Shaoxiong Ji",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2278106503",
            "name": "Shirui Pan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2140909072",
            "name": "Guodong Long",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2099141215",
            "name": "Xue Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2007802817",
            "name": "Jing Jiang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2149257649",
            "name": "Zi Huang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6635446068",
        "https://openalex.org/W2606065148",
        "https://openalex.org/W6738383168",
        "https://openalex.org/W2535838896",
        "https://openalex.org/W6746200960",
        "https://openalex.org/W6738460352",
        "https://openalex.org/W6607333740",
        "https://openalex.org/W6692563993",
        "https://openalex.org/W6729239390",
        "https://openalex.org/W2963347649",
        "https://openalex.org/W6682137061",
        "https://openalex.org/W6636649193",
        "https://openalex.org/W2337795114",
        "https://openalex.org/W2165698076",
        "https://openalex.org/W6748263980",
        "https://openalex.org/W2607247012",
        "https://openalex.org/W6727099177",
        "https://openalex.org/W2539241874",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W6746720608",
        "https://openalex.org/W6748642497",
        "https://openalex.org/W6728757088",
        "https://openalex.org/W6679434410",
        "https://openalex.org/W2211192759",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W6744649695",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W6750254146",
        "https://openalex.org/W6736057607"
    ],
    "abstract": "Mobile keyboard suggestion is typically regarded as a word-level language modeling problem. Centralized machine learning technique requires massive user data collected to train on, which may impose privacy concerns for sensitive personal typing data of users. Federated learning (FL) provides a promising approach to learning private language modeling for intelligent personalized keyboard suggestion by training models in distributed clients rather than training in a central server. To obtain a global model for prediction, existing FL algorithms simply average the client models and ignore the importance of each client during model aggregation. Furthermore, there is no optimization for learning a well-generalized global model on the central server. To solve these problems, we propose a novel model aggregation with the attention mechanism considering the contribution of clients models to the global model, together with an optimization technique during server aggregation. Our proposed attentive aggregation method minimizes the weighted distance between the server model and client models through iterative parameters updating while attends the distance between the server model and client models. Through experiments on two popular language modeling datasets and a social media dataset, our proposed method outperforms its counterparts in terms of perplexity and communication cost in most settings of comparison.",
    "full_text": "Learning Private Neural Language Modeling\nwith Attentive Aggregation\nShaoxiong Ji∗, Shirui Pan †, Guodong Long ‡, Xue Li ∗, Jing Jiang ‡, Zi Huang ∗\n∗School of ITEE, Faculty of EAIT, The University of Queensland, Australia\n†Faculty of Information Technology, Monash University, Australia\n‡Centre for Artiﬁcial Intelligence, Faculty of Engineering and IT, University of Technology Sydney, Australia\nEmail: shaoxiong.ji@uq.edu.au, shirui.pan@monash.edu, guodong.long@uts.edu.au,\nxueli@itee.uq.edu.au, jing.jiang@uts.edu.au, huang@itee.uq.edu.au\nAbstract—Mobile keyboard suggestion is typically regarded as\na word-level language modeling problem. Centralized machine\nlearning techniques require the collection of massive user data\nfor training purposes, which may raise privacy concerns in\nrelation to users’ sensitive data. Federated learning (FL) provides\na promising approach to learning private language modeling\nfor intelligent personalized keyboard suggestions by training\nmodels on distributed clients rather than training them on a\ncentral server. To obtain a global model for prediction, existing\nFL algorithms simply average the client models and ignore the\nimportance of each client during model aggregation. Further-\nmore, there is no optimization for learning a well-generalized\nglobal model on the central server. To solve these problems, we\npropose a novel model aggregation with an attention mechanism\nconsidering the contribution of client models to the global model,\ntogether with an optimization technique during server aggrega-\ntion. Our proposed attentive aggregation method minimizes the\nweighted distance between the server model and client models by\niteratively updating parameters while attending to the distance\nbetween the server model and client models. Experiments on two\npopular language modeling datasets and a social media dataset\nshow that our proposed method outperforms its counterparts in\nterms of perplexity and communication cost in most settings of\ncomparison.\nIndex Terms—federated learning, language modeling, attentive\naggregation\nI. I NTRODUCTION\nWith the advances in mobile technology, smart phones and\nwearable devices like smart watches are becoming increasingly\npopular in modern life. A study conducted in 2017 shows that\nthe majority of participants spent ﬁve hours or more every\nday on their smartphones 1. These mobile devices generate a\nmassive amount of distributed data such as text messages,\ntravel trajectories and health status.\nIn the traditional machine learning approaches, cloud-based\nservers send a data collection request to the clients, collect data\nfrom clients, train a centralized model, and make predictions\nfor clients. However, this centralized way of learning relies\nheavily on the central server. Moreover, there are privacy\nissues, especially in relation to sensitive data, if the central\nserver is hacked into or misused by other third parties.\n1Reported by The Statistics Portal available at https://www.statista.\ncom/statistics/781692/worldwide-daily-time-spent-on-smartphone/, retrieved\nin Dec, 2018\nRecently, a distributed learning technique called federated\nlearning has attracted great interest from the research com-\nmunity [1]–[3] under the umbrella of distributed machine\nlearning. It protects the privacy of data by learning a shared\nmodel by distributed training on local client devices without\ncollecting the data on a central server. Distributed intelligent\nagents take a shared global model from the central server’s\nparameters as initialization to train their own private mod-\nels using personal data, and make predictions on their own\nphysical devices. There are many applications of federated\nlearning in the real world, for example, predicting the most\nlikely photos a mobile user would like to share on the social\nwebsites [4], predicting the next word for mobile keyboards\n[5], retrieving the most important notiﬁcations, and detecting\nthe spam messages [6].\nOf these applications, mobile keyboard suggestion as a\nlanguage modeling problem is one of the most common\ntasks because it involves with user interaction which can\ngive instant labeled data for supervised learning. In practice,\nthe mobile keyboard applications predict the next word with\nseveral options when a user is typing a sentence. Gener-\nally speaking, an individual’s language usage expresses that\nperson’s language preference and patterns. With the recent\nadvances in deep neural networks, a language model combined\nwith neural networks, called neural language modeling, has\nbeen developed. Of these neural network models, recurrent\nneural networks (RNNs) which capture the temporal relations\nin sentences has signiﬁcantly improved the ﬁeld of language\nmodeling. Speciﬁc RNNs include long short-term memory\n(LSTM) [7] and its variants such as gated recurrent unit (GRU)\n[8].\nIn the real-world scenario, users’ language input and pref-\nerences are sensitive and may contain some private content in-\ncluding private personal proﬁles, ﬁnancial records, passwords,\nand social relations. Thus, to protect the user’s privacy, a fed-\nerated learning technique with data protection is a promising\nsolution. In this paper, we take this application as learning\nword-level private neural language modeling for each user.\nFederated learning learns a shared global model by the\naggregation of local models on client devices. But the original\npaper on federated learning [1] only uses a simple average\non client models, taking the number of samples in each\narXiv:1812.07108v2  [cs.CL]  13 Mar 2019\nclient device as the weight of the average. In the mobile\nkeyboard applications, language preferences may vary from\nindividual to individual. The contributions of client language\nmodels to the central server are quite different. To learn\na generalized private language model that can be quickly\nadapted to different people’s language preferences, knowledge\ntransferring between server and client, especially the well-\ntrained clients models, should be considered.\nIn this paper, we introduce an attention mechanism for\nmodel aggregation. It is proposed to automatically attend to the\nweights of the relation between the server model and different\nclient models. The attentive weights are then taken to minimize\nthe expected distance between the server model and client\nmodels. The advantages of our proposed method are: 1) it\nconsiders the relation between the server model and client\nmodels and their weights, and 2) it optimizes the distance\nbetween the server model and client models in parameter space\nto learn a well-generalized server model.\nOur contributions in this paper are as follows:\n• Our work ﬁrst introduces the attention mechanism to\naggregate multiple distributed models. The proposed at-\ntentive aggregation can be further applied to improve\nbroad methods and applications using distributed machine\nlearning.\n• In the server optimization, we propose a novel layer-wise\nsoft attention to capturing the “attention” among many\nlocal models’ parameters.\n• As demonstrated by the experiments on private neural\nlanguage modeling task for mobile keyboard suggestions,\nthe proposed method achieves a comparable performance\nin terms of perplexity and communication rounds.\nThe structure of this paper is organized as follows. In\nSection II, related works including federated learning, atten-\ntion mechanism and neural language modeling are reviewed.\nOur proposed attentive federated aggregation is introduced\nin Section III. Experimental settings and results are given\nin Section IV together with the comparison and analysis. In\nSection V, the conclusion is drawn.\nII. R ELATED WORK\nThis paper relates to federated learning, language modeling,\nand attention mechanism.\nA. Federated Learning\nFederated learning is proposed by McMahan et al. [1]\nto decouple training procedures from data collection by an\niterative averaging model. It can perform distributed training\nand communication-efﬁcient learning from decentralized data\nto achieve the goal of privacy preservation. Geyer et al.\nproposed a differential privacy-preserving technique on the\nclient side to balance the performance and privacy [2]. Popov\net al. proposed the ﬁne-tuning of distributed private data to\nlearn language models [9]. The federated learning technique\nis useful in many ﬁelds. Chen et al. combined federated\nlearning with meta learning for recommendation [3]. Kim et\nal. proposed federated tensor factorization to discover clinical\nconcepts from electronic health records [10]. The federated\nsetting can also be integrated into other machine learning\nsettings. Smith et al. [11] proposed a framework that ﬁts well\nwith multi-task learning and federated setting to tackle the\nstatistical challenge of distributed machine learning.\nCommunication efﬁciency is one of the performance metrics\nfor federated learning techniques. To improve communication\nefﬁciency, Kone ˇcn`y et al. proposed structured updates and\nsketched updates to reduce the uplink communication costs\n[12]. It is also studied under the umbrella of distributed\nmachine learning. Alistarch et al. proposed quantized com-\npression and the encoding of stochastic gradient descent [13]\nto achieve efﬁcient communication. Wen et al. used ternary\ngradients to reduce the communication cost [14].\nB. Neural Language Modeling\nLanguage modeling, as one of the most crucial natural\nlanguage processing tasks, has achieved better performance\nthan classical methods using popular neural networks. Mikolov\net al. used a simple recurrent neural network-based language\nmodel for speech recognition [15]. Recently, LSTM-based\nneural language models were developed to learn context over\nlong sequences in large datasets [16]. To facilitate learning\nin a language model and reduce the number of trainable\nparameters, Inan et al. proposed tying word vectors and\nword classiﬁers [17]. Press and Wolf evaluated the effect of\nweight tying and introduced a new regularization on the output\nembedding [18].\nC. Attention Mechanism\nThe attention mechanism is simply a vector serving to orient\nperception. It ﬁrst became popular in the ﬁeld of computer\nvision. Mnih et al. used it in recurrent neural network models\nfor image classiﬁcation [19]. It was then widely applied\nin sequence-to-sequence natural language processing tasks\nlike neural machine translation [20]. Luong et al. extended\nattention-based RNNs and proposed two new mechanisms,\ni.e., the global attention and local attention [21]. Also, the\nattention mechanism can be used in convolutional models for\nsentence encoding like ABCNN for modeling sentence pairs\n[22]. Yang et al. proposed hierarchical attention networks for\ndocument classiﬁcation [23]. Shen et al. proposed directional\nself-attention for language understanding [24].\nIII. P ROPOSED METHOD\nIn this section, we ﬁrstly introduce the preliminaries of the\nfederated learning framework, and then propose our attentive\nfederated optimization algorithm to improve the generalizabil-\nity for distributed clients by learning the attentive weights of\nselected clients during model aggregation. As for the client\nlearner, we apply the gated recurrent unit (GRU) [8] as the\nclient model for language modeling. Furthermore, we add a\nrandomized mechanism [2] for learning differentially private\nclient model.\nA. Preliminaries of Federated Learning\nFederated learning decouples the model training and data\ncollection [1]. To learn a well generalized model, it uses\nmodel aggregation on the server side, which is similar to the\nworks on meta-learning by learning a good initialization for\nquick adaptation [25], [26] and transfer learning by transfer-\nring knowledge between domains [27]. The basic federated\nlearning framework comprises two main parts, i.e., server\noptimization in Algorithm 1 and local training in Algorithm 2\n[1].\nCentral Model Update. The server ﬁrstly chooses a client\nlearning model and initializes the parameters of the client\nlearner. It sets the fraction of the clients. Then, it waits for\nonline clients for local model training. Once the selected\nnumber of clients ﬁnishes the model update, it receives the\nupdated parameters and performs the server optimization. The\nparameter sending and receiving consists of one round of\ncommunication. Our proposed optimization is conducted in\nLine 9 of Algorithm 1.\nAlgorithm 1 Optimization for Federated Learning on Cen-\ntral Server\n1: K is the total number of clients; C is the client fraction;\nU is a set of all clients.\n2: Input: server parameters θt at t, client parameters\nθ1\nt+1,...,θ m\nt+1 at t+ 1.\n3: Output: aggregated server parameters θt+1.\n4: procedure SERVER EXECUTION ⊿ Run on the server\n5: initialize θ0\n6: for each round t=1, 2, ... do\n7: m←max(C·K,1)\n8: St ←{ui |ui ∈U}m\n1 ⊿ Random set of clients\n9: for each client k ∈ St on local device do\n10: θk\nt+1 ←ClientUpdate(k,θt)\n11: θt+1 ←ServerOptimization(θt,θk\nt+1)\nPrivate Model Update. Each online selected client receives\nthe server model and performs secure local training on their\nown devices. For the neural language modeling, stochastic\ngradient descent is performed to update their GRU-based client\nmodels which is introduced in Section III-C. After several\nepochs of training, the clients send the parameters of their\nmodels to the central server over a secure connection. During\nthis local training, user data can be stored on their own devices.\nB. Attentive Federated Aggregation\nThe most important part of federated learning is the feder-\nated optimization on the server side which aggregates the client\nmodels. In this paper, a novel federated optimization strategy\nis proposed to learn federated learning from decentralized\nclient models. We call this Attentive Federated Aggregation,\nor FedAtt for short. It ﬁrstly introduces the attention mech-\nanism for federated aggregation by aggregating the layer-wise\ncontribution of neural language models of selected clients to\nthe global model in the central server. An illustration of our\nAlgorithm 2 Secure Local Training on Client\n1: B is the local mini-batch size; E is the number of local\nepochs; β is the momentum term; η is the learning rate.\n2: Input: ordinal of user k, user data X.\n3: Output: updated user parameters θt+1 at t+ 1.\n4: procedure CLIENT UPDATE (k, θ) ⊿ Run on the k-th\nclient\n5: B ←(split user data X into batches)\n6: for each local epoch i from 1 to E do\n7: for batch b ∈ B do\n8: zt+1 ←βzt + ∇L(θt)\n9: θt+1 ←θt −ηzt+1\n10: send θt+1 to server\nproposed layer-wise attentive federated aggregation is shown\nin Figure 1 where the lower box represents the distributed\nclient models and the upper box represents the attentive\naggregation in the central server. The distributed client models\nin the lower box contain several neural layers. The notations\nof “ ⊕” and “ ⊖” stand for the layer-wise operation on the\nparameters of neural models. This illustration shows only a\nsingle time step. The federated updating uses our proposed\nattentive aggregation block to update the global model by\niteration.\n… …\n- - - -\n+\n… …\n✓ m\nt +1✓ k\nt +1✓ 2\nt +1✓ 1\nt +1\n✓ t ✓ t +1\nw l\n1\nw 1\n1\nw L\n1 w L\n2\nw l\n2\nw 1\n2\nw 1\nk\nw l\nk\nw L\nk\nw 1\nm\nw l\nm\nw L\nm\n{ ↵ i\n1 } L\n1\n{ ↵ i\n2 } L\n1\n{ ↵ i\nk } L\n1 { ↵ i\nm } L\n1\nw 1\nw l\nw L\nw 1\nw l\nw L\nServer Model\nClient Models\nFig. 1: The illustration of our proposed layer-wise attentive\nfederated aggregation\nThe intuition behind the federated optimization is to ﬁnd\nan optimal global model that can generalize the client models\nwell. In our proposed optimization algorithm, we take it as\nﬁnding an optimal global model that is close to the client\nmodels in parameter space while considering the importance\nof selected client models during aggregation. The optimization\nobjective is deﬁned as\narg min\nθt+1\nm∑\nk=1\n[1\n2αkL(θt,θk\nt+1)2], (1)\nwhere θt is the parameters of the global server model at time\nstamp t, θk\nt+1 is the parameters of the k-th client model at\ntime stamp t+ 1, L(·,·) is deﬁned as the distance between\ntwo sets of neural parameters, and αk is the attentive weight\nto measure the importance of weights for the client models.\nThe objective is to minimize the weighted distance between\nserver model and client models by taking a set of self-adaptive\nscores as the weights.\nTo attend the importance of client models, we propose a\nnovel layer-wise attention mechanism on the parameters of\nthe neural network models. The attention mechanism in this\npaper is quite similar to the soft attention mechanism. Unlike\nthe popular attention mechanism applied to the data ﬂow,\nour proposed attentive aggregation is applied on the learned\nparameters of each layer of the neural language models. We\ntake the server parameters as a query and the client parameters\nas keys, and calculate the attention score in each layer of the\nneural networks.\nGiven the parameters in the l-th layer of the server model\ndenoted as wl and parameters in the l-th layer of the k-th\nclient model denoted as wl\nk, the similarity between the query\nand the key in the l-th layer is calculated as the norm of the\ndifference between two matrices, which is denoted as:\nsl\nk = ∥wl −wl\nk∥p.\nThen, we apply softmax on the similarity to calculate the layer-\nwise attention score for the k-th client in Equation 2.\nαl\nk = softmax(sl\nk) = esl\nk\n∑m\nk=1 esl\nk\n(2)\nOur proposed attention mechanism on the parameters is layer-\nwise. There are attention scores for each layer in the neural\nnetworks. For each model, the attention score is αk =\n{α0\nk,α1\nk,...,α l\nk,... }in a non-parameter way.\nUsing the Euclidean distance for L(·,·) and taking the\nderivative of the objective function in Equation 1, we get the\ngradient in the form of Equation 3.\n∇=\nm∑\nk=1\nαk(θt −θk\nt+1) (3)\nFor the selected group of m clients, we perform gradient\ndescent to update the parameters of the global model in\nEquation 4 as\nθt+1 ←θt −ϵ\nm∑\nk=1\nαk(θt −θk\nt+1), (4)\nwhere ϵ is the step size. The full procedure of our proposed\noptimization algorithm is described in Algorithm 3. It takes\nthe server parameters θt at time stamp tand client parameters\nθ1\nt+1,...,θ m\nt+1 at time stamp t+ 1, and returns the updated\nparameters of the global server.\nAlgorithm 3 Attentive Federated Optimization\n1: kis the ordinal of clients; lis the ordinal of neural layers;\nϵ is the stepsize of server optimization\n2: Input: server parameters θt at t, client parameters\nθ1\nt+1,...,θ m\nt+1 at t+ 1.\n3: Output: aggregated server parameters θt+1.\n4: procedure ATTENTIVE OPTIMIZATION (θt, θk\nt+1)\n5: Initialize α= {α1,...,α k,...,α m} ⊿ attention for\neach clients\n6: for each layer l= 1, 2,... do\n7: for each user k do\n8: sl\nk = ∥wl −wl\nk∥p\n9: αl\nk = softmax(sl\nk) = esl\nk\n∑m\nk=1 esl\nk\n10: αk = {α0\nk,α1\nk,...,α l\nk,... }\n11: θt+1 ←θt −ϵ∑m\nk=1 αk(θt −θk\nt+1)\n12: return θt+1\nThe advantage of our proposed layer-wise attentive feder-\nated aggregation and its optimization is as follows: 1) The\naggregation of client models is ﬁne-grained on each layer\nof the neural models considering the similarity between the\nclient model and the server model in the parameter space.\nThe learned features of each client model can be effectively\nselected to to produce a ﬁne-tuned global server model. 2) By\nminimizing the expected distance between the client model\nand the server model, the learned server model is close to the\nclient models in the parameter space and can well represent\nthe federated clients.\nC. GRU-based Client Model\nThe learning process on the client side is model-agnostic.\nFor different tasks, we can choose appropriate models in\nspeciﬁc situations. In this paper, we use the gated recurrent unit\n(GRU) [8] for learning the language modeling on the client\nside. The GRU is a well-known and simpler variant of the\nLong Short-Term Memory (LSTM) [7], by merging the forget\ngate and the input gate into a single gate as well as the cell\nstate and the hidden state. In the GRU-based neural language\nmodel, words or tokens are ﬁrstly embedded into word vectors\ndenoted as X = {x0,x1,...,x t,... }and then put into the\nrecurrent loops. The calculation inside the recurrent module is\nas follows:\nzt = σ(wz ·[ht−1, xt]),\nrt = σ(wr ·[ht−1, xt]),\n˜ht = tanh(w·[rt ∗ht−1, xt]),\nht = (1 −zt) ∗ht−1 + zt ∗˜ht,\nwhere zt is the update gate, rt is the reset gate, ht is the\nhidden state, and ˜ht is a new hidden state.\nD. Differential Privacy\nTo protect the client’s data from an inverse engineering\nattack, we apply the randomized mechanism into federated\nlearning [2]. This ensures differential privacy on the client\nside without revealing the client’s data [2]. This differentially\nprivate randomization was ﬁrstly proposed to apply on the\nfederated averaging, where a white noise with the mean of\n0 and the standard deviation of σ is added to the client\nparameters in Equation 5.\nθt+1 = θt − 1\nm(\nK∑\nk=1\n∆θk\nt+1 + N(0,σ2)) (5)\nOur proposed attentive federated aggregation can also add\nthis mechanism smoothly using Equation 6. The randomization\nis added in before the clients send the updated parameters to\nthe server, but it is written in the form of server optimization\nfor simplicity.\nθt+1 ←θt −ϵ\nm∑\nk=1\nαk(θt −θk\nt+1 + βN(0,σ2)) (6)\nIn practice, we add a magnitude coefﬁcient β ∈ (0,1] on\nthe randomization of normal noise to control the effect of\nthe randomization mechanism on the performance of federated\naggregation.\nIV. E XPERIMENTS\nThis section describes the experiments conducted to eval-\nuate our proposed method. Two baseline methods are com-\npared and additional exploratory experiments are conducted\nto further the exploration of the performance of our attentive\nmethod. Our code is available online at https://github.com/\nshaoxiongji/fed-att.\nA. Datasets\nWe conduct experiments of neural language modeling ex-\nperiments on three English language datasets for evaluation\nto mimic the real-world scenario of mobile keyboards in\nthe decentralized applications. They are Penn Treebank [28],\nWikiText-2 [29], and the Reddit Comments from Kaggle.\nLanguage modeling is one of the most suitable tasks for\nthe validation of federated learning. It has a large number\nof datasets to test the performance and there is a real-\nworld application, i.e., the input keyboard application in smart\nphones.\nPenn Treebank is an annotated English corpus. We use the\ndata derived from Zaremba et al. 2 [30]. The WikiText-2 is\navailable online 3. May 2015 Reddit Comments dataset is a\nportion of a large scale dataset of Reddit comments 4 from\nthe popular online community – Reddit. It is available in the\nKaggle Datasets5. We sampled 1‰ of the comments from this\ndataset to train our private language model as a representative\n2Penn Treebank is available at https://github.com/wojzaremba/lstm/tree/\nmaster/data\n3WikiText-2 is available at https://s3.amazonaws.com/research.metamind.\nio/wikitext/wikitext-2-v1.zip\n4Avaliable at https://www.reddit.com/r/datasets/comments/3bxlg7/i have\nevery publicly available reddit comment/, retrieved in Dec, 2018\n5Reddit Comments dataset is available at https://www.kaggle.com/reddit/\nreddit-comments-may-2015\nof social networks data. The statistical information, i.e., the\nnumber of tokens in the training, validation, and testing set of\nthese three datasets is shown in Table I.\nTABLE I: Number of tokens in training, validation and testing\nsets of three datasets\nDataset # Train # Valid. # Test\nPenn Treebank 887,521 70,390 78,669\nWikiText-2 2,088,628 217,646 245,569\nReddit Comments 1,784,023 102,862 97,940\nData Partitioning. To mimic the scenario of real-world\nprivate keyboard applications, we perform data partitioning on\nthese three popular language modeling datasets. At ﬁrst, we\nshufﬂe the whole dataset. Then, we perform random sampling\nwithout replacement under the independently identical distri-\nbution. The whole dataset is partitioned into a certain number\nof shards denoted as the number of users or clients. We split\nthese three datasets into 100 subsets as the user groups of 100\nclients to participate in the federated aggregation after local\ntraining.\nB. Baselines and Settings\nWe conducted to several groups experiments for compar-\nison, for example, performance with different model aggre-\ngation methods, the scale of client models, communication\ncost, and so forth. There are two baselines totally in these\ncomparisons, i.e., FedSGD and FedAvg. The basic deﬁnitions\nand settings of baselines and our proposed method are as\nfollows.\n1) FedSGD: Federated stochastic gradient descent takes all\nthe clients for federated aggregation and each client\nperforms one epoch of gradient descent.\n2) FedAvg: Federated averaging samples a fraction of users\nfor each iteration and each client can take several steps\nof gradient descent.\n3) FedAtt: Our proposed FedAtt takes a similar setting\nas FedAvg, but uses an improved attentive aggregation\nalgorithm.\nWe conduct experiments under the setting of federated\nlearning using the GRU-based private neural language model-\ning with Nvidia GTX 1080 Ti GPU acceleration. The GRU-\nbased client model ﬁrstly takes texts as input, then embeds\nthem into word vectors and feeds them to the GRU network.\nThe last fully connected layer takes the output of GRU as\ninput to predict the next word. The small model uses 300\ndimensional word embedding and hidden state of RNN unit.\nWe deploy models of three scales: small, medium and large\nwith word embedding dimensions of 300, 650 and 1500,\nrespectively. Tied embedding is applied to reduce the size\nof the model and decrease the communication cost. Tied\nembedding shares the weights and biases in the embedding\nlayer and output layer and greatly reduces the number of\ntrainable parameters greatly.\nC. Results\nWe conduct experiments on these three datasets and three\nfederated learning methods. Testing perplexity is taken as the\nevaluation metric. Perplexity is a standard measurement for\nprobability distribution. It is one of the most commonly used\nmetrics for word-level language modeling. The perplexity of\na distribution is deﬁned as\nPPL(x) = 2H(p) = 2−∑\nx p(x) log 1\np(x)\nwhere H(p) is the entropy of the distribution p(x). A lower\nperplexity stands for a better prediction performance of the\nlanguage model.\nWe take 50 rounds of communication between server and\nclients and compare the performance on the validation set\nto select the best model, then test the performance on the\ntesting set to get the testing perplexity. The results of testing\nperplexity of all three datasets are shown in Table II. For\nFedAvg and FedAtt, we set the client fraction C to be 0.1\nand 0.5 within these results. According to the deﬁnition of\nFedSGD, the client fraction is always 1. As shown in this\ntable, our proposed FedAtt outperforms FedSGD and FedAvg\nin terms of testing perplexity in all the three datasets. When\nthe client fraction C is 0.1 and 0.5 in the Penn Treebank\nand WikiText-2 respectively, our proposed method gains a\nsigniﬁcant improvement over its counterparts. We also conduct\nexperiments on the ﬁne-grained setting of the client fraction C\n(from 0.1 to 0.9). When the client fraction is 0.7, our proposed\nmethod obtains the best testing perplexity of 67.59 in the\nWikiText-2 dataset.\nTABLE II: Testing perplexity of 50 communication rounds\nfor federated training using small-scaled GRU network as the\nclient model\nFrac. Methods WikiText-2 PTB Reddit\n1 FedSGD 112.45 155.27 128.61\n0.1 FedAvg 95.27 138.13 126.49\nFedAtt 91.82 115.43 120.25\n0.5 FedAvg 79.75 128.24 101.64\nFedAtt 69.38 123.00 99.04\nWe then further our exploration of the four factors in\nthe WikiText-2 dataset to evaluate the performance of our\nproposed method with a comparison of its counterpart FedAvg.\nIn additional exploratory experiments in the following sub-\nsections, we explored the client fraction, the communication\ncosts, the effect of different randomizations, and the scale of\nthe models.\nD. Client Fraction\nIn real-world applications of federated learning, some\nclients may be ofﬂine due to a change in user behavior or\nnetwork issues. Thus, it is necessary to choose only a small\nnumber of clients for federated optimization. To evaluate\nthe effect of the client fraction C on the performance of\nour proposed attentive federated optimization, we explore the\ntesting perplexity with various number of clients. The result is\nillustrated in Figure 2 where the client fraction varies from 0.1\nto 0.9. The small-scaled neural language model is used in this\nevaluation. The testing perplexity ﬂuctuates when the client\nfraction increases. There is no guarantee that more clients\nresults in a better score. Actually, 70% of clients for model\naggregation achieved the lowest perplexity in this experiment.\nThis result indicates that the number of clients participating\nin model aggregation has an impact on the performance. But\nour proposed FedAtt can achieve much quite lower perplexity\nthan FedAvg for all the settings of the client fraction.\n0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9\n50\n60\n70\n80\n90\n100\n110\n95.27\n101.01\n88.67 85.85\n79.75\n93.54 92.09 94.02\n88.12\n91.82\n84.99\n76.97 75.63\n69.38\n82.77\n67.59\n79.57\n76.23\nClient fraction\nTesting perplexity\nFedAvg FedAtt\nFig. 2: Testing perplexity of 50 communication rounds when\na different number of clients is selected for federated aggre-\ngation\nE. Communication Cost\nCommunication cost for parameter uploading and down-\nloading between the clients and server is another important\nissue for decentralized learning. Communication, both wired\nand wireless, depends on Internet bandwidth highly and has\nan impact on the performance of federated optimization.\nTo save the capacity of network communication, decentral-\nized training should be more communication-efﬁcient. Several\napproaches apply compression methods to achieve efﬁcient\ncommunication. Our method accelerates the training through\nthe optimization of the global server as it can converge more\nquickly than its counterparts.\nTo compare the efﬁciency of communication, we take\nthe communication rounds during training as the evaluation\nmetric in this subsection. Three factors are considered, i.e.,\nthe client fraction, epochs and batch size of client training.\nThe results are shown in Figure 3 where the small-scaled\nlanguage model is used as the client model and 10% of\nclients are selected for model aggregation. We set the testing\nperplexity for the termination of federated training to be 90.\nWhen the testing perplexity is lower than that threshold,\nfederated training comes to an end and we take the rounds\nof training as the communication rounds. As shown in Figure\n3(a), the communication round during training ﬂuctuates when\nthe number of client increases. Furthermore, our proposed\nmethod is always better than FedAvg with less communication\ncost. When the client fraction C chosen is 0.2 and 0.4, our\nproposed method saves a half of communication rounds. Then,\nwe evaluate the effect of the local computation of clients on\nthe communication rounds. We take the local training epochs\nto be 1, 5, 10, 15, and 20 and the local batch size to be\nfrom 10 to 50. We proposed FedAtt to achieve a comparable\ncommunication cost in the comparison of different values of\nepoch and the batch size of local training, as shown in Figure\n3(b) and Figure 3(c) respectively.\n0.2 0.4 0.6 0.8 1\n0\n5\n10\n15\n20\n17\n12 11 12\n9\n12 11 11 12 1312\n6 7 6 7 8 9\n6\n10\n8\nClient fraction\nCommunication rounds\nFedAvg FedAtt\n(a) Rounds vs. client fraction\n0 5 10 15 20\n0\n10\n20\n30\n40\n50\n60\n70\n62\n17\n6 4 2\n37\n17\n6 3 2\nTraining epochs of clients\nCommunication rounds\nFedAvg FedAtt\n(b) Rounds vs. epochs\n10 20 30 40 50\n0\n5\n10\n15\n20\n25\n30\n35\n17 17\n26\n31 33\n17\n12\n19\n16\n25\nBatch size of clients\nCommunication rounds\nFedAvg FedAtt (c) Rounds vs. batch size\nFig. 3: Effect of the client fraction, epochs, and batch size\nof clients on communication rounds when the threshold of\ntesting perplexity is set to be 90 and small-scaled GRU-based\nlanguage model is used\nF . Magnitude of Randomization\nThe federated learning framework focuses on the privacy\nof the input data using distributed training on each client\nside to protect the user’s privacy. To further the privacy\npreservation of the decentralized training, we evaluate the\nmagnitude of normal noise in the randomization mechanism\non model parameters. Comparative experiments are conducted\nto analyze the effect of the magnitude on the testing perplexity.\nThe results are shown in Table III with both randomized\nand nonrandomized settings. For the randomized version, four\nvalues of magnitude are chosen, i.e., 0.001, 0.005, 0.01, and\n0.05.\nAs shown in the table, a very small noise on both the\nmethods does not affect the performance. Actually, the testing\nperplexity for the randomized setting is slightly better than\nthe result of nonrandomized setting. With a larger noise, the\nperformance becomes worse. For our proposed method, the\ntesting perplexity is always lower than its counterpart FedAvg,\nshowing that our method can resist a larger noise and can\nbetter preserve privacy to ensure the perplexity of next-word\nprediction.\nTABLE III: Magnitude of randomization vs. testing perplexity\nusing a small-scaled model with tied embedding\nRandomization FedAvg FedAtt\nNonrandomized β = 0 88.21 77.66\nRandomized\nβ = 0.001 88.17 77.76\nβ = 0.005 88.36 78.59\nβ = 0.01 89.74 79.51\nβ = 0.05 103.17 101.82\nG. Scale of Model\nDistributed training depends on communication between the\nserver and clients, and the central server needs to optimize on\nthe model parameters for the aggregation of the clients models.\nThus, the central server will have a higher communication\ncost and computational cost when there are a larger number\nof clients and the local models have millions of parameters.\nThe size of the vocabulary in most language modeling\ncorpus is very large. To save training costs, the embedding\nweights and output weights are tied which can reduce the\nnumber of trainable parameters [17], [18]. We compared three\nscales of client models with the word embedding dimensions\nof 300, 650 and 1500. Two versions of the tied and untied\nmodels are used. In the tied setting, the dimension of the RNN\nhidden state must be the same as the embedding dimension.\nThe results of the model’s scales on the testing perplexity\nare shown in Table IV. The tied large-scale model achieves\nthe best results for both FedAvg and FedAtt and the tied\nmodel is better than the untied model of the same scale.\nOur proposed method achieves lower testing perplexity in\nfour out of the six settings, i.e., tied and untied small model,\ntied medium model, and tied large model. For the other two\nsettings, the testing perplexity of our method is slightly higher\nthan FedAvg. Overall, for real-world keyboard applications in\npractice, the tied embedding can be used to save the number\nof trainable parameters and the communication cost while\nachieving a better performance.\nV. C ONCLUSION\nFederated learning provides a promising and practical ap-\nproach to learning from decentralized data while protecting the\nprivate data with differential privacy. Efﬁcient decentralized\nlearning is signiﬁcant for distributed real-world applications\nsuch as personalized keyboard word suggestion on mobile\nphones, providing a better service and protect user’s private\npersonal data.\nTo optimize the server aggregation by federated averaging,\nwe investigated the model aggregation and optimization on the\ncentral server in this paper. We proposed a novel layer-wise\nTABLE IV: Testing perplexity of 50 communication rounds\nvs. the scale of the model using a tied embedding or untied\nembedding model\nModel FedAvg FedAtt\nSmall tied 88.21 77.66\nuntied 91.25 81.31\nMedium tied 103.07 77.41\nuntied 96.67 96.71\nLarge tied 77.51 76.37\nuntied 82.97 83.40\nattentive federated optimization for private neural language\nmodeling which can measure the importance of selected\nclients and accelerate the learning process. We partitioned\nthree popular datasets, i.e., Penn Treebank and WikiText-\n2 for the prototypical language modeling task, and Reddit\ncomments from a real-world social networking website, to\nmimic the scenario of word-level keyboard suggestions and\nperformed a series of exploratory experiments. Experiments\non these datasets show our proposed method outperforms its\ncounterparts in most settings.\nACKNOWLEDGEMENT\nThis research is funded by the Australian Government\nthrough the Australian Research Council (ARC) under grants\nLP150100671 partnership with Australia Research Alliance\nfor Children and Youth (ARACY) and Global Business Col-\nlege Australia (GBCA).\nREFERENCES\n[1] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and\nBlaise Aguera y Arcas. Communication-efﬁcient learning of deep\nnetworks from decentralized data. In Artiﬁcial Intelligence and Statistics,\npages 1273–1282, 2017.\n[2] Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially pri-\nvate federated learning: A client level perspective. arXiv preprint\narXiv:1712.07557, 2017.\n[3] Fei Chen, Zhenhua Dong, Zhenguo Li, and Xiuqiang He. Federated\nmeta-learning for recommendation. arXiv preprint arXiv:1802.07876 ,\n2018.\n[4] Eunice Kim, Jung-Ah Lee, Yongjun Sung, and Sejung Marina Choi.\nPredicting selﬁe-posting behavior on social networking sites: An exten-\nsion of theory of planned behavior. Computers in Human Behavior ,\n62:116–123, 2016.\n[5] Kenneth C Arnold, Krzysztof Z Gajos, and Adam T Kalai. On\nsuggesting phrases vs. predicting words for mobile text composition. In\nProceedings of the 29th Annual Symposium on User Interface Software\nand Technology, pages 603–608. ACM, 2016.\n[6] Hongmei He, Tim Watson, Carsten Maple, J ¨orn Mehnen, and Ashutosh\nTiwari. A new semantic attribute deep learning with a linguistic attribute\nhierarchy for spam detection. In 2017 International Joint Conference\non Neural Networks (IJCNN) , pages 3862–3869. IEEE, 2017.\n[7] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory.\nNeural computation, 9(8):1735–1780, 1997.\n[8] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bah-\ndanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning\nphrase representations using rnn encoder–decoder for statistical machine\ntranslation. In Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) , pages 1724–1734,\n2014.\n[9] Vadim Popov, Mikhail Kudinov, Irina Piontkovskaya, Petr Vytovtov, and\nAlex Nevidomsky. Distributed ﬁne-tuning of language models on private\ndata. In International Conference on Learning Representation (ICLR) ,\n2018.\n[10] Yejin Kim, Jimeng Sun, Hwanjo Yu, and Xiaoqian Jiang. Federated\ntensor factorization for computational phenotyping. In Proceedings\nof the 23rd ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining , pages 887–895. ACM, 2017.\n[11] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Tal-\nwalkar. Federated multi-task learning. In Advances in Neural Informa-\ntion Processing Systems , pages 4427–4437, 2017.\n[12] Jakub Kone ˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richt ´arik,\nAnanda Theertha Suresh, and Dave Bacon. Federated learning:\nStrategies for improving communication efﬁciency. arXiv preprint\narXiv:1610.05492, 2016.\n[13] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan V o-\njnovic. QSGD: Communication-efﬁcient SGD via gradient quantization\nand encoding. In Advances in Neural Information Processing Systems ,\npages 1709–1720, 2017.\n[14] Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran\nChen, and Hai Li. Terngrad: Ternary gradients to reduce communication\nin distributed deep learning. In Advances in Neural Information\nProcessing Systems, pages 1509–1519, 2017.\n[15] Tom ´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan ˇCernock`y, and\nSanjeev Khudanpur. Recurrent neural network based language model. In\nEleventh Annual Conference of the International Speech Communication\nAssociation, 2010.\n[16] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and\nYonghui Wu. Exploring the limits of language modeling. arXiv preprint\narXiv:1602.02410, 2016.\n[17] Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word\nvectors and word classiﬁers: A loss framework for language modeling.\narXiv preprint arXiv:1611.01462 , 2016.\n[18] Oﬁr Press and Lior Wolf. Using the output embedding to improve\nlanguage models. In Proceedings of the 15th Conference of the European\nChapter of the Association for Computational Linguistics: Volume 2,\nShort Papers, volume 2, pages 157–163, 2017.\n[19] V olodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models\nof visual attention. In Advances in neural information processing\nsystems, pages 2204–2212, 2014.\n[20] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural\nmachine translation by jointly learning to align and translate. arXiv\npreprint arXiv:1409.0473, 2014.\n[21] Thang Luong, Hieu Pham, and Christopher D Manning. Effective\napproaches to attention-based neural machine translation. In Proceedings\nof the 2015 Conference on Empirical Methods in Natural Language\nProcessing, pages 1412–1421, 2015.\n[22] Wenpeng Yin, Hinrich Sch ¨utze, Bing Xiang, and Bowen Zhou. Abcnn:\nAttention-based convolutional neural network for modeling sentence\npairs. Transactions of the Association of Computational Linguistics ,\n4(1):259–272, 2016.\n[23] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Ed-\nuard Hovy. Hierarchical attention networks for document classiﬁcation.\nIn Proceedings of the 2016 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, pages 1480–1489, 2016.\n[24] Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan, and\nChengqi Zhang. Disan: Directional self-attention network for rnn/cnn-\nfree language understanding. arXiv preprint arXiv:1709.04696 , 2017.\n[25] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic\nmeta-learning for fast adaptation of deep networks. arXiv preprint\narXiv:1703.03400, 2017.\n[26] Alex Nichol, Joshua Achiam, and John Schulman. On ﬁrst-order meta-\nlearning algorithms. arXiv preprint arXiv:1803.02999 , 2018.\n[27] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE\nTransactions on Knowledge and Data Engineering , 22(10):1345–1359,\n2010.\n[28] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.\nBuilding a large annotated corpus of english: The Penn Treebank.\nComputational linguistics, 19(2):313–330, 1993.\n[29] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.\nPointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.\n[30] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural\nnetwork regularization. arXiv preprint arXiv:1409.2329 , 2014."
}