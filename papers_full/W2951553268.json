{
    "title": "Unsupervised and Efficient Vocabulary Expansion for Recurrent Neural Network Language Models in ASR",
    "url": "https://openalex.org/W2951553268",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A4221532426",
            "name": "Khassanov, Yerbolat",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1966190765",
            "name": "Chng, Eng Siong",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1614298861",
        "https://openalex.org/W36903255",
        "https://openalex.org/W2611669587",
        "https://openalex.org/W2131462252",
        "https://openalex.org/W2250357346",
        "https://openalex.org/W2117130368",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2158195707",
        "https://openalex.org/W2111305191",
        "https://openalex.org/W1965154800",
        "https://openalex.org/W2963034893",
        "https://openalex.org/W2951559648",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W2402268235",
        "https://openalex.org/W2345190899",
        "https://openalex.org/W1938755728",
        "https://openalex.org/W2171928131",
        "https://openalex.org/W2802422770",
        "https://openalex.org/W1847088711",
        "https://openalex.org/W1520465330",
        "https://openalex.org/W2519314406",
        "https://openalex.org/W1558797106",
        "https://openalex.org/W2963932686",
        "https://openalex.org/W179875071",
        "https://openalex.org/W1631260214",
        "https://openalex.org/W2152808281",
        "https://openalex.org/W101286142",
        "https://openalex.org/W196214544",
        "https://openalex.org/W2141599568",
        "https://openalex.org/W2057653135",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W1524333225",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W2120861206",
        "https://openalex.org/W2038721957"
    ],
    "abstract": "In automatic speech recognition (ASR) systems, recurrent neural network language models (RNNLM) are used to rescore a word lattice or N-best hypotheses list. Due to the expensive training, the RNNLM's vocabulary set accommodates only small shortlist of most frequent words. This leads to suboptimal performance if an input speech contains many out-of-shortlist (OOS) words. An effective solution is to increase the shortlist size and retrain the entire network which is highly inefficient. Therefore, we propose an efficient method to expand the shortlist set of a pretrained RNNLM without incurring expensive retraining and using additional training data. Our method exploits the structure of RNNLM which can be decoupled into three parts: input projection layer, middle layers, and output projection layer. Specifically, our method expands the word embedding matrices in projection layers and keeps the middle layers unchanged. In this approach, the functionality of the pretrained RNNLM will be correctly maintained as long as OOS words are properly modeled in two embedding spaces. We propose to model the OOS words by borrowing linguistic knowledge from appropriate in-shortlist words. Additionally, we propose to generate the list of OOS words to expand vocabulary in unsupervised manner by automatically extracting them from ASR output.",
    "full_text": "Unsupervised and Efﬁcient Vocabulary Expansion for\nRecurrent Neural Network Language Models in ASR\nYerbolat Khassanov, Eng Siong Chng\nRolls-Royce@NTU Corporate Lab, Nanyang Technological University, Singapore\nyerbolat002@edu.ntu.sg, aseschng@ntu.edu.sg\nAbstract\nIn automatic speech recognition (ASR) systems, recurrent neu-\nral network language models (RNNLM) are used to rescore\na word lattice or N-best hypotheses list. Due to the expen-\nsive training, the RNNLM’s vocabulary set accommodates only\nsmall shortlist of most frequent words. This leads to suboptimal\nperformance if an input speech contains many out-of-shortlist\n(OOS) words.\nAn effective solution is to increase the shortlist size and re-\ntrain the entire network which is highly inefﬁcient. Therefore,\nwe propose an efﬁcient method to expand the shortlist set of a\npretrained RNNLM without incurring expensive retraining and\nusing additional training data. Our method exploits the struc-\nture of RNNLM which can be decoupled into three parts: in-\nput projection layer, middle layers, and output projection layer.\nSpeciﬁcally, our method expands the word embedding matri-\nces in projection layers and keeps the middle layers unchanged.\nIn this approach, the functionality of the pretrained RNNLM\nwill be correctly maintained as long as OOS words are properly\nmodeled in two embedding spaces. We propose to model the\nOOS words by borrowing linguistic knowledge from appropri-\nate in-shortlist words. Additionally, we propose to generate the\nlist of OOS words to expand vocabulary in unsupervised man-\nner by automatically extracting them from ASR output.\nIndex Terms: vocabulary expansion, recurrent neural network,\nlanguage model, speech recognition, word embedding\n1. Introduction\nThe language model (LM) plays an important role in automatic\nspeech recognition (ASR) system. It ensures that recognized\noutput hypotheses obey the linguistic regularities of the target\nlanguage. The LMs are employed at two different stages of the\nstate-of-the-art ASR pipeline: decoding and rescoring. At the\ndecoding stage, a simple model such as count-basedn-gram [1]\nis used as a background LM to produce initial word lattice. At\nthe rescoring stage, this word lattice or N-best hypotheses list\nextracted from it is rescored by a more complex model such as\nrecurrent neural network language model (RNNLM) [2, 3, 4].\nDue to the simplicity and efﬁcient implementation, the\ncount-based n-gram LM is trained over a large vocabulary\nset, typically in the order of hundreds of thousands words.\nOn the other hand, computationally expensive RNNLM is\nusually trained with a small subset of most frequent words\nknown as in-shortlist (IS) set, typically in the order of tens\nof thousands words, whereas the remaining words are deemed\nout-of-shortlist (OOS) and jointly modeled by single node\n<unk> [5]. Since the probability mass of <unk> node is\nshared by many words, it will poorly represent properties of the\nindividual words leading to unreliable probability estimates of\nOOS words. Moreover, these estimates tend to be very small\nwhich makes RNNLM biased in favor of hypotheses mostly\ncomprised of IS words. Consequently, if an input speech with\nmany OOS words is supplied to ASR system, the performance\nof RNNLM will be suboptimal.\nAn effective solution is to increase the IS set size and retrain\nthe entire network. However, this approach is highly inefﬁcient\nas training RNNLM might take from several days up to several\nweeks depending on the scale of application [6]. Moreover, ad-\nditional textual data containing training instances of OOS words\nwould be required, which is difﬁcult to ﬁnd for rare domain-\nspeciﬁc words. Therefore, the effective and efﬁcient methods to\nexpand RNNLM’s vocabulary coverage is of great interest [7].\nIn this work, we propose an efﬁcient method to expand the\nvocabulary of pretrained RNNLM without incurring expensive\nretraining and using additional training data. To achieve this,\nwe exploit the structure of RNNLM which can be decoupled\ninto three parts: 1) input projection layer, 2) middle layers and\n3) output projection layer as shown in ﬁgure 1. The input and\noutput projection layers are deﬁned by input and output word\nembedding matrices that perform linear word transformations\nfrom high to low and low to high dimensions, respectively.\nThe middle layers are a non-linear function used to generate\nhigh-level feature representation of contextual information. Our\nmethod expands the vocabulary coverage of RNNLM by insert-\ning new words into input and output word embedding matrices,\nand keeping the parameters of middle layers unchanged. This\nmethod keeps the functionality of pretrained RNNLM intact as\nlong as new words are properly modeled in input and output\nword embedding spaces. We propose to model the new words\nby borrowing linguistic knowledge from other “similar” words\npresent in word embedding matrices.\nFurthermore, the list of OOS words to expand vocabulary\ncan be generated either in supervised or unsupervised manners.\nFor example, in supervised manner, they can be manually col-\nlected by the human expert. Whereas in unsupervised manner,\na subset of most frequent words from OOS set can be selected.\nIn this work, we propose to generate the list of OOS words\nin unsupervised manner by automatically extracting them from\nASR output. The motivation is that the background LM usually\ncovers much larger vocabulary, and hence, during the decoding\nstage it will produce a word lattice which will contain the most\nrelevant OOS words that might be present in test data.\nWe evaluate our method by rescoring N-best list output\nfrom the state-of-the-art TED 1 talks ASR system. The ex-\nperimental results show that vocabulary expanded RNNLM\nachieves 4% relative word error rate (WER) improvement over\nthe conventional RNNLM. Moreover, 7% relative WER im-\nprovement is achieved over the strong Kneser-Ney smoothed\n5-gram model used to rescore the word lattice. Importantly, all\nthese improvements are achieved without using additional train-\ning data and by incurring very little computational cost.\n1https://www.ted.com/\narXiv:1806.10306v1  [cs.CL]  27 Jun 2018\nExpanded matrix S+\n wt…\nst…\nht\n yt…f(st ,ht-1)\n…\nht-1\nExpanded matrix U+\n…\n…\n…\n…\n…\n……\n…\n……\n…\n…\n…\n…\n…\n…\ns1 s2 s|VIS|    s1        s|Vnew| u1 u2 u|VIS|  u1             u|Vnew|\n|VIS| |Vnew| |VIS| |Vnew|\n|VIS|\n|Vnew|\n|VIS|\n|Vnew|<unk> node <unk> node\nInput projection layer Middle layers Output projection layer\nFigure 1: RNNLM architecture after vocabulary expansion.\nThe rest of the paper is organized as follows. The related\nworks on vocabulary coverage expansion of RNNLMs are re-\nviewed in section 2. Section 3 brieﬂy describes the RNNLM\narchitecture. Section 4 presents the proposed methodology to\nincrease the IS set of RNNLM by expanding the word embed-\nding matrices. In section 5, the experiment setup and obtained\nresults are discussed. Lastly, section 6 concludes the paper.\n2. Related works\nThis section brieﬂy describes popular approaches to expand\nthe vocabulary coverages of RNNLMs. These approaches\nmostly focus on intelligently redistributing the probability mass\nof <unk> node among OOS words, optimizing the training\nspeed for large-vocabulary models or training sub-word level\nRNNLM. These approaches can also be used in combination.\nRedistributing probability mass of <unk>: Park et\nal. [5] proposed to expand the vocabulary coverage by gathering\nall OOS words under special node <unk>and explicitly mod-\neling it together with IS words, see ﬁgure 1. This is a standard\nscheme commonly employed in the state-of-the-art RNNLMs.\nThe probability mass of <unk>node is then can be redis-\ntributed among OOS words by using statistics of simpler LMs\nsuch as count-based n-gram model as follows:\n˜PR(wt+1|ht) =\n{\nPR(wt+1|ht) wt+1 ∈ VIS\nβ(wt+1|ht)PR(<unk>|ht) otherwise (1)\nβ(wt+1|ht) = PN(wt+1|ht)∑\nw/∈VIS\nPN(w|ht) (2)\nwhere PR() and PN () are conditional probability estimates ac-\ncording to RNNLM and n-gram LM respectively, for some\nword wt+1 given context ht. The n-gram model is trained\nwith whole vocabulary set V, whereas RNNLM is trained with\nsmaller in-shortlist subset VIS ⊂V. The β() is a normalization\ncoefﬁcient used to ensure the sum-to-one constraint of obtained\nprobability function ˜PR().\nLater, [3] proposed to uniformly redistribute the probability\nmass of <unk>token among OOS words as follows:\n˜PR(wt+1|ht) =\n\n\n\nPR(wt+1|ht) wt+1 ∈VIS\nPR(<unk>|ht)\n|V\\VIS|+ 1 otherwise (3)\nwhere ‘\\’ symbol is the set difference operation. In this way, the\nvocabulary coverage of RNNLM is expanded to the full vocab-\nulary size |V|without relying on the statistics of simpler LMs.\nTraining speed optimization: Rather than expanding vo-\ncabulary of the pretrained model, this group of studies focuses\non speeding-up the training of large-vocabulary RNNLMs.\nOne of the most effective ways to speed up the training of\nRNNLMs is to approximate the softmax function. The softmax\nfunction is used to normalize obtained word scores to form a\nprobability distribution, hence, it requires scores yw′ of every\nword in the vocabulary:\nsoftmax(yw) = exp(yw)∑\nw′∈VIS\nexp(yw′ ) (4)\nConsequently, its computational cost is proportional to the num-\nber of words in the vocabulary and it dominates the training of\nthe whole model which is the network’s main bottleneck [8].\nMany techniques have been proposed to approximate the\nsoftmax computation. The most popular ones include hierar-\nchical softmax [9, 10, 11], importance sampling [12, 13] and\nnoise contrastive estimation [8, 14]. The comparative study of\nthese techniques can be found in [7, 15]. Other techniques, be-\nsides softmax function approximation, to speed up the training\nof large-vocabulary models can be found in [16].\nSub-word level RNNLM: Another effective method to\nexpand the vocabulary coverage is to train a sub-word level\nRNNLM. Different from standard word-level RNNLMs, they\nmodel ﬁner linguistic units such as characters [17] or sylla-\nbles [18], hence, a larger range of words will be covered.\nFurthermore, character-level RNNLM doesn’t suffer from the\nOOS problem, though, it performs worse than word-level mod-\nels2 [18]. Recently, there has been a lot of research effort aim-\ning to train the hybrid of word and sub-word level models where\npromising results are obtained [19, 6, 20].\n3. RNNLM architecture\nThe conventional RNNLM architecture can be decoupled into\nthree parts: 1) input projection layer, 2) middle layers and\n3) output projection layer, as shown in ﬁgure 1. The input\nprojection layer is deﬁned by input word embedding matrix\nS ∈Rds×|VIS|used to transform the one-hot encoding repre-\nsentation of word wt ∈R|VIS|at time tinto lower dimensional\ncontinuous space vector st ∈Rds , where ds is input word em-\nbedding vector dimension:\nst = Swt (5)\nThis vector st and compressed context vector from previous\ntime step ht−1 ∈Rdh are then merged by non-linear middle\n2At least for English.\nlayer, which can be represented as function f(), to produce a\nnew compressed context vector ht ∈Rdh , where dh is context\nvector dimension:\nht = f(st,ht−1) (6)\nThe function f() can be simple activation units such as sig-\nmoid and hyperbolic tangent, or more complex units such as\nLSTM [3] and GRU [21]. The middle layer can also be formed\nby stacking several such functions.\nThe compressed context vector ht is then supplied to out-\nput projection layer where it is transformed into higher dimen-\nsion vector yt ∈ R|VIS| by output word embedding matrix\nU ∈Rdh×|VIS|:\nyt = UT ht (7)\nThe entries of output vector yt represent the scores of words\nto follow the context ht. These scores are then normalized by\nsoftmax function to form probability distribution (eq. (4)).\n4. Vocabulary expansion\nThis section describes our proposed method to expand the vo-\ncabulary coverage of pretrained RNNLM. Our method is based\non the observation that input and output projection layers learn\nthe word embedding matrices, and middle layers learn the map-\nping from the input word embedding vectors to compressed\ncontext vectors. Thus, by modifying the input and output word\nembedding matrices to accommodate new words, we can ex-\npand the vocabulary coverage of RNNLM. Meanwhile, the pa-\nrameters of middle layers are kept unchanged which allows us\nto avoid expensive retraining. This approach will preserve the\nlinguistic regularities encapsulated within pretrained RNNLM\nas long as the new words are properly modeled in input and\noutput embedding spaces. To model the new words, we will use\nword embedding vectors of “similar” words present inVIS set.\nThe proposed method has three main challenges: 1) how to\nﬁnd relevant OOS words for vocabulary expansion, 2) criteria\nto select “similar” candidate words to model a target OOS word\nand 3) how to expand the word embedding matrices. The details\nare discussed in section 4.1, 4.2 and 4.3, respectively.\n4.1. Finding relevant OOS words\nThe ﬁrst step to vocabulary expansion is ﬁnding relevant OOS\nwords. This step is important as expanding vocabulary with ir-\nrelevant words absent in the input test data is ineffective. The\nrelevant OOS words can be found either in supervised or unsu-\npervised manners. For example, in supervised manner, they can\nbe manually collected by human expert. In unsupervised man-\nner, the subset of most frequent OOS words can be selected.\nIn this work, we employed an unsupervised method where\nrelevant OOS words are automatically extracted from the ASR\noutput. The reason is that at the decoding stage a background\nLM covering very large vocabulary set is commonly employed.\nSubsequently, the generated word lattice will contain the most\nrelevant OOS words that might be present in the input test data.\n4.2. Selecting candidate words\nGiven a list of relevant OOS words, let’s call it set Vnew, the\nnext step is to select candidate words that will be used to model\neach of them. The selected candidates must be present in VIS\nset and should be similar to the target OOS word in both se-\nmantic meaning and syntactic behavior. Selecting inadequate\ncandidates might deteriorate the linguistic regularities incorpo-\nrated within pretrained RNNLM, thus, they should be carefully\ninspected. In natural language processing, many effective tech-\nniques exist that can ﬁnd appropriate candidate words satisfying\nconditions mentioned above [22, 23, 24].\n4.3. Expanding word embedding matrices\nThis section describes our proposed approach to expand the\nword embedding matrices Sand U.\nThe matrix S: This matrix holds input word embedding\nvectors S = [s1 ... s|VIS|] and it’s used to transform words from\ndiscrete form into lower dimensional continuous space. In this\nspace, vectors of “similar” words are clustered together [25].\nMoreover, these vectors have been shown to capture meaningful\nsemantic and syntactic features of language [26]. Subsequently,\nif two words have similar semantic and syntactic roles, their\nembedding vectors are expected to belong to the same cluster.\nAs such, words in a cluster can be used to approximate a new\nword that belongs to the same cluster.\nFor example, let’s consider a scenario where we want to add\na new word Astana, the capital of Kazakhstan, to the vocabu-\nlary set of an existing RNNLM. Here, we can select candidate\nwords from VIS with similar semantic and syntactic roles such as\nLondon and Paris. Speciﬁcally, we extract the input word em-\nbedding vectors of selected candidates C = {slondon,sparis}\nand combine them to form a new input word embedding vector\nˆsastana ∈Rds as follows:\nˆsastana =\n∑\ns∈C mss\n|C| (8)\nwhere ms is some normalized metric used to weigh the candi-\ndates. We repeat this procedure for all words in set Vnew.\nObtained new input word embedding vectors are then used\nto form matrix ˆS = [ˆs1 ...ˆs|Vnew|] where ˆS ∈Rds×|Vnew|, which\nis appended to the initial matrix S ∈Rds×|VIS|to form the ex-\npanded matrix:\nS+ = [S ˆS] (9)\nwhere S+ ∈Rds×(|VIS|+|Vnew|).\nThe input word vectorwt ∈R|VIS|should be also expanded\nto accommodate the new words from Vnew, which results in the\nnew input vector ˆwt ∈R|VIS|+|Vnew|. The input vector and input\nword embedding matrix after expansion are depicted in ﬁgure 1.\nThe matrix U: This matrix holds the output word embed-\nding vectors U = [u1 ... u|VIS|] where u∈Rdh . These vectors\nare compared against context vector ht using the dot product to\ndetermine the score of the next possible word wt+1[8]. Intu-\nitively, for a given context ht, the interchangeable words with\nsimilar semantic and syntactic roles should have similar scores\nto follow it. Therefore, in the output word embedding space,\ninterchangeable words should belong to the same cluster. Sub-\nsequently, we can use the same procedure and candidates which\nwere used to expand matrixSto model the new words in output\nword embedding space. However, this time we operate in the\ncolumn space of matrix U. The output vector and output word\nembedding matrix after expansion are depicted in ﬁgure 1.\n5. Experiment\nThis section describes experiments conducted to evaluate the\nefﬁcacy of proposed vocabulary expansion method for pre-\ntrained RNNLMs on ASR task. The ASR system is built by\nKaldi [27] speech recognition toolkit on the ﬁrst release of\nTED-LIUM [28] speech corpus. To highlight the importance\nof vocabulary expansion, we train the LMs on generic-domain\ntext corpus One Billion Word Benchmark (OBWB) [29].\nTable 1: The characteristics of TED-LIUM corpus\nCharacteristics Train Dev Test\nNo. of talks 774 8 11\nNo. of words 1.3M 17.7k 27.5k\nTotal duration 122hr 1.5hr 2.5hr\nAs the baseline LMs, we trained two state-of-the-art\nmodels, namely the modiﬁed Kneser-Ney smoothed 5-gram\n(KN5) [30] and recurrent LSTM network (LSTM) [3]. We call\nour system VE-LSTM which is constructed by expanding the\nvocabulary of the baseline LSTM. The performance of these\nthree models is evaluated using both perplexity and WER.\nExperiment setup: The TED-LIUM corpus is comprised\nof monologue talks given by experts on speciﬁc topics, its char-\nacteristics are given in table 1. Its train set was used to build the\nacoustic model with the ‘nnet3+chain’ setup of Kaldi includ-\ning the latest developments. Its dev set was used to tune hyper-\nparameters such as the number of candidates to use to model the\nnew words, word insertion penalty and LM scale. The test set\nwas used to compare the performance of proposed VE-LSTM\nand two baseline models. Additionally, the TED-LIUM corpus\nhas a predesigned pronunciation lexicon of 150kwords which\nwas also used as a vocabulary set for baseline LMs.\nThe OBWB corpus consists of text collected from various\ndomains including the news and parliamentary speeches. Its\ntrain set contains around 700M words and is used to train both\nbaseline LMs. Its validation set of size 141kwords was used to\nstop the training of LSTM model.\nThe baseline KN5 was trained using SRILM [31] toolkit\nwith 150k vocabulary. It was used to rescore the word lattice\nand 300-best list. Its pruned 3 version KN5 pruned was used as\na background LM during the decoding stage.\nThe baseline LSTM was trained as a four-layer network\nsimilar to [3] using our own implementation in PyTorch [32].\nThe LSTM explicitly models only the10kmost frequent words4\nof 150k vocabulary set. The remaining 140k words are mod-\neled by uniformly distributing the probability mass of <unk>\nnode using equation (3). Thus, the inputwt and output yt vector\nsizes are 10k+ 1which we call as VIS set. Hence, the baseline\nLSTM theoretically models the same 150k vocabulary set as\nKN5. The OOS rate with respect to dev and test sets are 6.8%\nand 5.5%, respectively. The input ds and output dh word em-\nbedding vector dimensions were set to 300 and 1500, respec-\ntively. The parameters of the model are learned by truncated\nbackpropagation through time algorithm (BPTT) [33] unrolled\nfor 10 steps. For regularization, we applied 50% dropout on the\nnon-recurrent connections as suggested by [34].\nThe VE-LSTM model is obtained by expanding the vocab-\nulary of baseline LSTM with OOS words extracted from the\nASR output. For example, to construct the VE-LSTM model\nfor the test set, we collect the list of OOS words Vnew from\nthe recognized hypotheses of the test set. For each OOS word\nin Vnew, we then select the appropriate set of candidate words\nC. The selection criteria will be explained later. Next, se-\nlected candidates are used to model the new input and out-\nput word embedding vectors of target OOS words as in equa-\ntion (8). For simplicity, we didn’t weigh the selected candi-\ndates. Lastly, these generated new vectors are appended to the\ninput and output word embedding matrices of baseline LSTM\nmodel, see ﬁgure 1. Consequently, the obtained VE-LSTM will\n3The pruning coefﬁcient is 10−7.\n4Plus the beginning <s> and end of sentence </s> symbols.\nTable 2: The perplexity and WER results on dev and test sets of\nTED-LIUM corpus\nLM Perplexity WER (%)\nDev Test Rescore Dev Test\nKN5 pruned 384 341 - 13.05 12.82\nKN5 237 218 Lattice 10.85 10.49\n300-best 11.03 10.79\nLSTM 211 185 300-best 10.63 10.18\nVE-LSTM 163 150 300-best 10.44 9.77\nexplicitly model 10k+1+ |Vnew|words, whereas the remaining\n140k−|Vnew|words are modeled by uniformly distributing the\nprobability mass of <unk>node using equation (3).\nTo select candidate words we used the classical skip-gram\nmodel [23]. The skip-gram model is trained with default param-\neters on OBWB corpus covering all unique words. Typically,\nwhen presented with a target OOS word, the skip-gram model\nreturns a list of “similar” words. From this list, we only select\ntop eight5 words which are present in VIS set.\nResults: The experiment results are given in table 2. We\nevaluated LMs on perplexity and WER measure. The perplexity\nwas computed on the reference data and the OOS words for\nvocabulary expansion were extracted from the reference data as\nwell. The perplexity results computed on the test set show that\nVE-LSTM signiﬁcantly outperforms KN5 and LSTM models\nby 31% and 18% relative, respectively.\nFor WER experiment, the KN5 is evaluated on both lat-\ntice and 300-best rescoring tasks. The LSTM and VE-LSTM\nare evaluated only on 300-best rescoring task. We tried to ex-\ntract the OOS words for vocabulary expansion from different\nN-best lists. Interestingly, the best result is achieved when they\nare extracted from the 1-best. The reason is that 1-best hy-\npothesis list contains high conﬁdence score words, hence, OOS\nwords extracted from it will be reliable. Whereas using other\nN-best lists will result in unreliable OOS words which confuse\nthe VE-LSTM model. The VE-LSTM outperforms the baseline\nLSTM model by 4% relative WER. Compared to KN5 used\nto rescore the word lattice, 7% relative WER improvement is\nachieved. Such improvements suggest that the proposed vocab-\nulary expansion method is effective.\n6. Conclusion\nIn this paper, we have proposed an efﬁcient vocabulary expan-\nsion method for pretrained RNNLM. Our method which mod-\niﬁes the input and output projection layers, while keeping the\nparameters of middle layers unchanged was shown to be feasi-\nble. It was found that extracting OOS words for vocabulary ex-\npansion from the ASR output is effective when high conﬁdence\nwords are selected. Our method achieved signiﬁcant perplex-\nity and WER improvements on the state-of-the-art ASR system\nover two strong baseline LMs. Importantly, the expensive re-\ntraining was avoided and no additional training data was used.\nWe believe that our approach of manipulating input and output\nprojection layers is general enough to be applied to other neural\nnetwork models with similar architectures.\n7. Acknowledgements\nThis work was conducted within the Rolls-Royce@NTU Cor-\nporate Lab with support from the National Research Foundation\n(NRF) Singapore under the Corp Lab@University Scheme.\n5This number was tuned on the dev set.\n8. References\n[1] J. T. Goodman, “A bit of progress in language modeling,” Com-\nputer Speech & Language, vol. 15, no. 4, pp. 403–434, 2001.\n[2] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. Cernock `y, and S. Khudan-\npur, “Recurrent neural network based language model.” in Inter-\nspeech, 2010.\n[3] M. Sundermeyer, R. Schl ¨uter, and H. Ney, “Lstm neural networks\nfor language modeling,” in Interspeech, 2012.\n[4] X. Liu, X. Chen, Y . Wang, M. J. Gales, and P. C. Woodland, “Two\nefﬁcient lattice rescoring methods using recurrent neural network\nlanguage models,” IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, vol. 24, no. 8, pp. 1438–1449, 2016.\n[5] J. Park, X. Liu, M. J. Gales, and P. C. Woodland, “Improved neu-\nral network based language modelling and adaptation,” in Inter-\nspeech, 2010.\n[6] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y . Wu,\n“Exploring the limits of language modeling,” arXiv preprint\narXiv:1602.02410, 2016.\n[7] W. Chen, D. Grangier, and M. Auli, “Strategies for training large\nvocabulary neural language models,” in Proceedings of the 54th\nAnnual Meeting of the Association for Computational Linguistics,\n2016, pp. 1975–1985.\n[8] A. Mnih and Y . W. Teh, “A fast and simple algorithm for training\nneural probabilistic language models,” in In Proceedings of the\nInternational Conference on Machine Learning, 2012.\n[9] F. Morin and Y . Bengio, “Hierarchical probabilistic neural net-\nwork language model,” inAISTATS, 2005, pp. 246–252.\n[10] A. Mnih and G. E. Hinton, “A scalable hierarchical distributed\nlanguage model,” in Advances in neural information processing\nsystems, 2009, pp. 1081–1088.\n[11] T. Mikolov, S. Kombrink, L. Burget, J. ernock, and S. Khudanpur,\n“Extensions of recurrent neural network language model,” in2011\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2011, pp. 5528–5531.\n[12] Y . Bengio and J.-S. Sen´ecal, “Quick training of probabilistic neu-\nral nets by importance sampling.” in AISTATS, 2003, pp. 1–9.\n[13] ——, “Adaptive importance sampling to accelerate training of a\nneural probabilistic language model,”IEEE Transactions on Neu-\nral Networks, vol. 19, no. 4, pp. 713–722, 2008.\n[14] X. Chen, X. Liu, M. J. F. Gales, and P. C. Woodland, “Recurrent\nneural network language model training with noise contrastive es-\ntimation for speech recognition,” in2015 IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP),\n2015, pp. 5411–5415.\n[15] E. Grave, A. Joulin, M. Ciss ´e, D. Grangier, and H. J ´egou, “Efﬁ-\ncient softmax approximation for gpus,” in In Proceedings of the\nInternational Conference on Machine Learning, 2017.\n[16] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. ernock,\n“Strategies for training large scale neural network language mod-\nels,” in 2011 IEEE Workshop on Automatic Speech Recognition\nUnderstanding, 2011, pp. 196–201.\n[17] I. Sutskever, J. Martens, and G. E. Hinton, “Generating text with\nrecurrent neural networks,” inIn Proceedings of the International\nConference on Machine Learning, 2011, pp. 1017–1024.\n[18] T. Mikolov, I. Sutskever, A. Deoras, H.-S. Le, S. Kombrink, and\nJ. Cernock´y, “Subword language modeling with neural networks,”\n2011.\n[19] Y . Kim, Y . Jernite, D. Sontag, and A. M. Rush, “Character-aware\nneural language models.” in AAAI, 2016, pp. 2741–2749.\n[20] H. Xu, K. Li, Y . Wang, J. Wang, S. Kang, X. Chen, D. Povey, and\nS. Khudanpur, “Neural network language modeling with letter-\nbased features and importance sampling,” in 2018 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), 2018.\n[21] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio, “Gated feedback\nrecurrent neural networks,” inIn Proceedings of the International\nConference on Machine Learning, 2015, pp. 2067–2075.\n[22] G. Miller and C. Fellbaum, “Wordnet: An electronic lexical\ndatabase,” 1998.\n[23] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient esti-\nmation of word representations in vector space,” arXiv preprint\narXiv:1301.3781, 2013.\n[24] J. Pennington, R. Socher, and C. Manning, “Glove: Global vec-\ntors for word representation,” in Proceedings of the 2014 con-\nference on Empirical Methods in Natural Language Processing\n(EMNLP), 2014, pp. 1532–1543.\n[25] R. Collobert and J. Weston, “A uniﬁed architecture for natural\nlanguage processing: Deep neural networks with multitask learn-\ning,” in In Proceedings of the International Conference on Ma-\nchine Learning, 2008, pp. 160–167.\n[26] T. Mikolov, W.-t. Yih, and G. Zweig, “Linguistic regularities in\ncontinuous space word representations.” in HLT-NAACL, 2013,\npp. 746–751.\n[27] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y . Qian, P. Schwarzet al.,\n“The kaldi speech recognition toolkit,” in IEEE 2011 Workshop\non Automatic Speech Recognition and Understanding, 2011.\n[28] A. Rousseau, P. Del ´eglise, and Y . Esteve, “Ted-lium: an automatic\nspeech recognition dedicated corpus.” in LREC, 2012, pp. 125–\n129.\n[29] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn,\nand T. Robinson, “One billion word benchmark for measuring\nprogress in statistical language modeling,” in Interspeech, 2014.\n[30] S. F. Chen and J. Goodman, “An empirical study of smoothing\ntechniques for language modeling,” Computer Speech & Lan-\nguage, vol. 13, no. 4, pp. 359–394, 1999.\n[31] A. Stolcke, “Srilm - an extensible language modeling toolkit,” in\nInterspeech, 2002.\n[32] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito,\nZ. Lin, A. Desmaison, L. Antiga, and A. Lerer, “Automatic differ-\nentiation in pytorch,” 2017.\n[33] R. J. Williams and J. Peng, “An efﬁcient gradient-based algo-\nrithm for on-line training of recurrent network trajectories,” Neu-\nral computation, vol. 2, no. 4, pp. 490–501, 1990.\n[34] W. Zaremba, I. Sutskever, and O. Vinyals, “Recurrent neural net-\nwork regularization,”CoRR, 2014."
}