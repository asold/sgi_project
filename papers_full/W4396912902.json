{
    "title": "Utilizing Large Language Models to Generate Synthetic Data to Increase the Performance of BERT-Based Neural Networks",
    "url": "https://openalex.org/W4396912902",
    "year": 2024,
    "authors": [
        {
            "id": null,
            "name": "Woolsey, Chancellor R.",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Bisht, Prakash",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Rothman, Joshua",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2981469581",
            "name": "Leroy Gondy",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2055531721",
        "https://openalex.org/W4383346782",
        "https://openalex.org/W4294647381",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2804566321",
        "https://openalex.org/W2167994626",
        "https://openalex.org/W4360599455",
        "https://openalex.org/W4360600181"
    ],
    "abstract": "An important issue impacting healthcare is a lack of available experts. Machine learning (ML) models could resolve this by aiding in diagnosing patients. However, creating datasets large enough to train these models is expensive. We evaluated large language models (LLMs) for data creation. Using Autism Spectrum Disorders (ASD), we prompted ChatGPT and GPT-Premium to generate 4,200 synthetic observations to augment existing medical data. Our goal is to label behaviors corresponding to autism criteria and improve model accuracy with synthetic training data. We used a BERT classifier pre-trained on biomedical literature to assess differences in performance between models. A random sample (N=140) from the LLM-generated data was evaluated by a clinician and found to contain 83% correct example-label pairs. Augmenting data increased recall by 13% but decreased precision by 16%, correlating with higher quality and lower accuracy across pairs. Future work will analyze how different synthetic data traits affect ML outcomes.",
    "full_text": "Utilizing Large Language Models to Generate Synthetic Data to Increase the \nPerformance of BERT-Based Neural Networks \nChancellor R. Woolsey1, Prakash Bisht1, Joshua Rothman2, MD, MS, Gondy Leroy1, PhD, \nMS \n1University of Arizona, Tucson, Arizona, 2University of California San Diego, San Diego, \nCalifornia \nAbstract  \nAn important issue impacting healthcare is a lack of available experts. Machine learning (ML) models could resolve \nthis by aiding in diagnosing patients. However, creating datasets large enough to train these models is expensive. \nWe evaluated large language models (LLMs) for data creation. Using Autism Spectrum Disorders (ASD), we \nprompted ChatGPT and GPT-Premium to generate 4,200 synthetic observations to augment existing medical data. \nOur goal is to label behaviors corresponding to autism criteria and improve model accuracy with synthetic training \ndata. We used a BERT classifier pre-trained on biomedical literature to assess differences in performance between \nmodels. A random sample (N=140) from the LLM-generated data was evaluated by a clinician and found to contain \n83% correct example-label pairs. Augmenting data increased recall by 13% but decreased precision by 16%, \ncorrelating with higher quality and lower accuracy across pairs. Future work will analyze how different synthetic \ndata traits affect ML outcomes. \n \nIntroduction \nTimely screening and diagnosis can be a problem for many medical conditions1,2,3. The usage of machine learning \ncan help. New deep-learning models that exhibit excellent performance in classifying doctors’ notes on medical \nrecords may be especially helpful. For example, Dong et al.4 created a system for classifying Chinese electronic \nmedical records. However, these deep-learning ML models require large amounts of data to perform satisfactorily. \nSuch data is not always available to researchers, or it is sometimes available from data brokers but is very expensive. \nAn alternative approach may be the creation of artificial data. \n \nDifferent types of data are used in machine learning, ranging from structured data to sounds, images, and text. With \nthe increasing availability of generative AI, large language models (LLMs) may be especially suitable for creating \notherwise difficult-to-mimic data such as images or text. We focus on text in our project in the context of improving \nearly screening and diagnosis of autism spectrum disorders (ASD). Autism is a mental health disorder that can be \ndetected early on in an individual’s life and impacts much of the population, with an estimated 1.56% of all children \nreceiving this diagnosis by the age of four5 and nearly 2.78% of all children by the age of eight6. It is demarcated by \nindividuals who struggle in social interactions, communication, and repetitive actions or activities as clinically \ndefined in the Diagnostic and Statistical Manual of Mental Disorders7. An autism diagnosis is based on showing \nobservable, continuous setbacks in “social communication and interaction” (labeled as ‘A’ symptoms in the DSM) \nas well as “restricted, repetitive behaviors” (labeled as ‘B’ symptoms in the DSM). \n \nWe are working on deep learning machine learning models to detect ASD using information in electronic health \nrecords (EHR). We work with free text in EHR records reviewed and labeled by experts. Each sentence in the text \nreceived a label based on DSM diagnostic criteria or no label. However, since such expert review is time-consuming \nand expensive, the datasets available for training and testing are relatively small. We are evaluating LLMs, i.e., \ndifferent versions of ChatGPT, for their potential to supply synthetic data for the ML. We evaluate the effect of \nadding synthetic data on BioBERT, a BERT model pre-trained on thousands of biomedical abstracts8. \n \nRelated Work \n \nGenerating Artificial Data to Improve Machine Learning \nDifferent types of data can be created artificially. For example, for image data such as x-rays or photographs, \nYoshihiro Shima9 applied a series of rotations, translations, skews, and distortions to increase accuracy by over \n10.00% in classifying images of airplanes, birds, cars, cats, deer, dogs, horses, monkeys, ships and trucks. Nagaraju \net al.10 applied addition using scaling, flips, blurs, and color changes to achieve a boost inaccuracy in the \nclassification of maize crop disease by more than 5.00%. \n \nHowever, techniques appropriate for images are not optimal for creating text-based data (i.e., medical observations, \npatient comments, etc.). Ahlbäck and Dougly11 focused on taking their current datasets and introducing spelling \nerrors, word insertions from embeddings, or greyscaling (replacing “strong” words with less extreme synonyms, i.e., \nchanging “extremely” to “quite”) and were able to increase F1 from of 0.945 to 0.966 on fake news detection.  \n \nMore advanced approaches have turned to generative AI technologies to artificially increase the quantity of text data \nthrough LLMs. Many of these studies focus on prompting an LLM with some conditions to get the desired results. \nTo help with this, many of these studies provide example data as a starting point. Sahu et al.12 prompted GPT-3 with \nthe task of sentence completion and provided a set of ten examples to study intent classification. They were able to \nboost accuracy by 5% across four different datasets ranging from banking to multi-purpose data. Dai et al.13 used \nGPT-3 to generate six new sentences for text classification based on a single sentence from a dataset. They found \nthat this increases the accuracy of their BERT-based networks by an average of 0.1315 from raw datasets from \nAmazon, a set of transcribed medical symptoms, and a set containing 20,000 different biomedical abstracts.  \n \nOther models have attempted to prompt LLMs without providing starting examples with promising results. Timo \nSchick and Hinrich Schütze14 used GPT-2 and developed the DINO framework to help users generate datasets from \npre-trained models via prompts to achieve an average semantic textual similarity (STS) score of 75.20 across a \nvariety of STS benchmarks – outperforming second place by almost one point. This pattern recognition capability \nwas then explored further by Li et al.15, who incorporated Schick and Schütze’s DINO framework for text \ngeneration, but this time added the requirement that each response maintain the usage of specific vocabulary terms. \nThis process achieved a boosted 6 points higher ROUGE F1/Precision/Recall of 36.27/16.78/33.83 compared to a \nbaseline of 29.86/10.19/27.08 FlanT5 (finetuned language model) from Chung et al.’s16 previous work. \n \nProblems with Artificial Data \nLLMs are imperfect. Potential bias is a shortcoming that can affect outcomes. Salinas et al.17 found biases against \nindividuals of Mexican nationality based on a series of prompts asking for job recommendations. They found that \nMexicans were recommended roles such as “Engineer; Software; Quality; Assurance” more than six times less often \nthan other nationalities and found that Mexicans were predicted to have the smallest median salary out of any \nnationality. Additionally, Nicole Gross18 found that ChatGPT still shows gender bias in its responses, noting that it \nperpetuates gender-stereotyping roles of mothers and fathers within family units and even ranks the value of \ntechnical skills and communications skills differently in value for CVs depending on if the “author” of the CV was a \nwoman or a man. \n \nIn medicine, if an LLM maintains such biases against individuals of different backgrounds, it can increase the risk of \na wrong diagnosis or not offering the correct treatment based on preconceived notions of gender, nationality19, or \nother potential biases. If these problems persist into data generation, it may not yield reliable results that can be \ntrained on. \n \nMethods \nData Set and Objective \nAs stated above, autism is diagnosed through the identification of problematic behaviors that are labeled as A1-A3 \nand B1-B4 diagnostic criteria as described in DSM57. The descriptions and examples of the criteria can be seen \nbelow (Table 1).  \n \nOur dataset is part of the CDC surveillance data from the state of Arizona, and the free text is labeled by trained \nexperts. This dataset is highly unbalanced, and only 14.30% of sentences within our medical observation dataset \ncontain a diagnostic label. Additionally, out of our entire usable dataset, fewer than 300 examples correspond with \nB3 symptoms, and more than 1,000 correspond to an A1 symptom. This low number of examples leads to lower \nperformance of ML algorithms20,21. \n \nTable 1: A list of autism labels corresponding to symptoms and examples used by the DSM5. \nLabel Symptom Example \nA1 Lack of emotional reflection. \nStruggles in maintaining back-and-forth \ndialogue \nA2 Struggles in using nonverbal communication. \nPoor/abnormal eye contact, lack of \nunderstanding of body language \nA3 \nIssues in creating, keeping, and grasping how to \nhandle relationships. \nMaladjustment of behavior to match differing \nsocial environments \nB1 \nCommon, pattern-like behaviors in relation to \nspeaking, moving, or using items. \nAbnormal repeated usage of certain phrases, \nspecific ordering of toys \nB2 \nObservation of specific behaviors with issues if such \nrituals are not followed. \nDistress if a specific food is not eaten at the \nsame time every day \nB3 \nHyperfixation on interests differing from common \nnorms. \nInterests in the behaviors of fans or curtains as \nthey interact with light \nB4 \nEither major sensitivity or lack-thereof to sensory \ntraits in a given space. \nPain-like reactions to certain sounds but little \nconcern for extreme heat  \n \nModel Used \nWe use a BioBERT multilabel neural network pretrained on biomedical abstracts and articles from PubMed. For our \nimplementation, we used the “TFBertModel” from the HuggingFace transformers library22 with an attention mask to \nequalize the input data23. After the mask and BioBERT layers, we use a 1-dimensional global average pooling and a \ndense layer for a total of 108,315,655 parameters.  \n \nArtificial Data Generation  \nThere have been multiple iterations of OpenAI’s GPT model before its official release as ChatGPT. For our study, \nwe used OpenAI’s GPT-3.5-Turbo and OpenAI’s GPT-4 to generate data. GPT-3.5-Turbo is the most current \nversion of 175-billion-parameter GPT-324, and GPT-4 is only accessible via a “ChatGPT Plus” subscription25. \nHowever, OpenAI claims it is their “most capable model”26. \n \nIdeally, GPT-3.5 would have been studied independently, but we found that GPT-3.5 did not return more than 25 \nexamples at a time, a shortcoming that was later resolved. We show one of our attempts to prompt GPT-3.5 to return \nmore examples in Table 2. \n \nTo avoid these problems, we created data for two studies. First, we generated examples using both GPT-3.5 and \nGPT-4 and changed as few words as possible in each prompt. The viewpoint was kept constant: “an expert \npediatrician with over ten years of experience in the field, and with that, you have become famous for your \nunderstanding of autism in young c,” where ‘c’ represents the subject’s gender (boy, girl, or ‘child’ for gender \nneutral cases).  \n \nHowever, our study changed three different elements in each prompt: \n● The gender the ‘doctor’ was studying: male, female, neutral (child) \n● The DSM5 symptom we wanted more examples of: A1, A2, A3, B1, B2, B3, or B4 \n● The number of children who were being studied: 25 or 150 \nFor example, if we wanted a prompt that returned 25 examples of boys showing behaviors as described by A1, it \nwould be like the following: \n● You are an expert pediatrician with over ten years of experience in the field, and with that, you have \nbecome famous for your understanding of autism in young boys. Today, you have been presented 25 boys \nexperiencing the following known autism symptom: “Deficits in social-emotional reciprocity, ranging, for \nexample, from abnormal social approach and failure of normal back-and-forth conversation; to reduced \nsharing of interests, emotions, or affect; to failure to initiate or respond to social interactions.” I need you to \nuniquely record each of the 25 boys from the doctor’s point of view. If possible, please write this as though \nthis would end up on the boys’ permanent medical record and keep each observation limited to only 1-2 \nsentences each. Please note, these are medical observations, not perfect descriptions for each young boy. \nThus, if short phrases make the observation clear, then do those. Please note: I need to see each observation \nfor each little boy; there cannot be any ellipses to skip over any of the boys. Please number each one from 1 \nto 25. Thank you. \nUsing this setup, we requested 25 observations per symptom across three genders (boy, girl, and child) for a total of \n1050 new lines of text data and 150 new lines per symptom.  \n \nTable 2: An example of a prompt that failed to generate data due to the number of responses asked for \nPrompt Response \nYou are an expert pediatrician with over ten years of \nexperience in the field, and with that, you have become \nfamous for your understanding of autism in young \nboys. Today, you have been presented 30 boys \nexperiencing the following known Autism symptom: \n“Deficits in social-emotional reciprocity, ranging, for \nexample, from abnormal social approach and failure of \nnormal back-and-forth conversation; to reduced \nsharing of interests, emotions, or affect; to failure to \ninitiate or respond to social interactions.” I need you to \nuniquely record each of the 30 boys from the doctor’s \npoint of view. If possible, please write this as though \nthis would end up on the boys’ permanent medical \nrecord and keep each observation limited to only 1-2 \nsentences each. Please note, these are medical \nobservations, not perfect descriptions for each young \nboy. Thus, if short phrases make the observation clear, \nthen do those. Please note: I need to see each \nobservation for each little boy, there cannot be any \nellipses to skip over any of the boys. Please number \neach one from 1 to 30. Thank you. \n \n \n \n \nI'm sorry for any confusion, but it's important to clarify \nthat I don't have access to real-time data or the ability \nto interact with specific individuals. I also cannot \ngenerate 30 unique observations without more specific \ninformation about each case. If you'd like, I can \nprovide you with a general example of how these \nobservations might look based on the information \nyou've provided. Please let me know if you'd like me to \nproceed with that. \n \n \nSecond, we generated examples using GPT-4. We requested 150 observations across the same three genders for a \ntotal of 3,150 responses and 450 new lines per symptom. Table 3 shows our count of baseline examples compared to \nthe total number of examples, including the synthetic data for each label.  \n \nTable 3: A count of all examples in the original dataset, the dataset augmented with GPT-3.5 and GPT-4, and the \ndataset augmented with GPT-4 only.  \nLabel \nCount of \nExamples per \nLabels \nExamples \naugmented with \nGPT-3.5 and GPT-\n4 \nExamples \naugmented with \nGPT-4 Only \nA1 1079 1229 1529 \nA2 580 730 1030 \nA3 667 817 1117 \nB1 652 802 1102 \nB2 443 593 593 \nB3 215 365 665 \nB4 848 998 1298 \n \nEvaluation \nFirst, we run the model using only existing EHR data and no synthetic data. This run serves two purposes: to \nprovide a baseline to compare augmented datasets to (EHR plus synthetic data) and to provide the initial finetuning \nfor our BioBERT models. Here, we also count the total samples of observations for each criterion to see what \nmodels might be impacted the most by our data augmentation. Next, we analyze sentence starts to get a preliminary \nlook at potential differences between our original data and the synthetic data.  \n \nWe also conducted a domain expert evaluation. Our expert is an assistant professor and pediatrician at UC San \nDiego Health with over four years of experience. We randomly selected 140 text observations from the GPT-3.5 and \nGPT-4 data. The clinical expert evaluated each observation on four dimensions using a 1-5 scale: \n- the behavior is common in autistic individuals (1: rare – 5: common). \n- the behavior is common in people in general (1: rare – 5: common)  \n- the behavior is normal for a different mental health ailment other than autism (e.g. ADHD, anxiety, etc.)(1: \nrare – 5: common) \n- if the data could be found in real EHR notes (1: rare – 5: common) \nAdditionally, he also noted if the given label was correct, incorrect, or missing additional labels (e.g. A1 did not \nfully describe the behavior, it would also need an A2 label) \n \nFinally, we train and test our BioBERT model using the original and augmented data sets. In each case, we used the \ndata to fine-tune the BioBERT model, tested on a separate ADDM test set and reported precision, recall, and the F-\nmeasure. Since we have unbalanced data with the majority of sentences not receiving a label, i.e., the majority of \nsentences in an EHR are not descriptions of autistic behaviors, we do not report accuracy. Accuracy would be \nartificially high and does not contribute to understanding the results.  \n \nResults \nData Set Quantification \nFirst, we counted the number of examples with similar sentence starts (first word only). Table 4 shows the top 10 \nsimilar examples, the starting word, and the observation frequency. We also provide the proportion of the dataset \nthese sentences constitute, with the baseline data subset (comprising of only test with DSM5 criteria) consisting of \n57.52% of the data, the GPT-3.5 and GPT-4 subset consisting of 75.33% of the new data, and the GPT-4 only subset \nconsisting of 39.81% of the new data.  \n \nTable 4: Ten most frequent examples based on initial word similarity.  \nBaseline (N=5351) GPT-3.5 and GPT-4 (N=1050) GPT-4 Only (N=3150) \nStart Word Count (%) Start Word Count (%) Start Word Count (%) \nHe 33.04 Patient 14.10 Struggles 5.81 \nShe 8.26 Demonstrate 11.52 Rare 5.24 \nHis 4.07 Display 9.43 Rarely 5.17 \nChild 2.56 Child 8.86 Show 4.57 \nWhen 1.96 exhibit/Exhibit 8.19 Displays 3.52 \nIn 1.87 Struggles 5.71 Does 3.49 \nThe 1.61 engage/Engage 5.52 Avoid 3.46 \nSensory 1.59 Shows 5.33 Repeat 2.95 \nAt 1.40 Observe 4.19 Appear 2.89 \nDuring 1.16 does 2.48 Demonstrates 2.70 \n% of Dataset 57.52 % of Dataset 75.33 % of Dataset 39.81 \n \nThe expert evaluation of the synthetic is shown in Table 5. Overall, the generated observation was highly common \nfor autistic individuals, highly uncommon for individuals without autism, and highly uncommon for people with a \nmental health issue other than autism and such behaviors would almost always be found in clinical notes if such \nbehaviors were to be observed.  \n \nAdditionally, Table 6 shows the percentage of observations that correctly corresponded to a given label.  Out of all \nthe observations, 83.00% of the labels matched the prompted label, 7.00% of the labels were incorrect, and 10.00% \nof the labels required extra labels to be correct. \n \nTable 5: Expert scores for synthetic data  \nChatGPT-3.5 and ChatGPT-4 Scores \nCriterion \nLabel N \n \nTypical for \nAutism  Is Normal Behavior but not \ntypical for autism \nWould also be found \nin EHR clinical notes \nA1 18 5 1 1 5 \nA2 24 5 1.04 1.04 5 \nA3 22 5 1.05 1 5 \nB1 16 4.88 1.19 1.13 5 \nB2 18 4.94 1.06 1.06 5 \nB3 22 5 1.18 1.05 5 \nB4 20 5 1.4 1.2 4.95 \nAverage 20 4.98 1.13 1.06 4.99 \n \nTable 6: Percent of observations that have the correct label according to expert \nChatGPT-3.5 and ChatGPT-4 Percent Correct \nCriterion Label Total Correct (%) Total Incorrect (%) Total Incomplete (%) \nA1 50.00 22.22 27.78 \nA2 91.67 0 8.33 \nA3 68 9.10 22.73 \nB1 100 0 0 \nB2 88.89 5.56 5.56 \nB3 86.36 9.10 4.45 \nB4 100 0 0 \nOverall 83.57 6.43 10.00 \n \nMachine Learning Outcomes \nTable 7 shows the results for ML labeling using a BioBERT classifier fine-tuned on the baseline data, followed up \nby the augmented data (for both GPT-3.5 and GPT-4 and GPT-4 only). For GPT-3.5 and GPT-4, we found a 6% \nincrease in recall from the baseline. For GPT-4 only, we found a 13% increase in the average recall. Between both \nsets, there was an increase in recall, with the only label that did not experience an increase in both being B4. For B4, \nthe recall dropped by 3% in the combined GPT-3.5 and GPT-4 and 0.05 in the GPT-4 only data. However, recall \nalso dropped by 5% in the GPT-3.5 and GPT-4 for the B1 symptom.  \n \nHowever, precision decreased. For GPT-3.5 and GPT-4, we found an average decrease of 4%. For GPT-4 only, we \nfound a decrease of 0.16. The only two labels to experience an increase from the baseline were B1 and B4 from the \ncombined GP-T3.5 and GPT-4. Out of all precision measures, the only time the GPT-4 extended set was not below \nthe baseline was B4.  \n \nDiscussion and Conclusion \n \nTable 7: Performance of the Classification using the Baseline and Augmented Datasets  \n  \nThrough the usage of ChatGPT, we augmented an unbalanced dataset of medical observations to include more \nexamples of each criterion for autism to increase the performance of our classifiers. Based on an expert review of \nour data, ChatGPT was able to generate data that would be seen in someone with autism that, if combined with other \nbehaviors, would more than likely lead to an autism diagnosis. Through the implementation of text data created by \nChatGPT-3.5 and GPT-4, we were able to successfully increase the recall of our classifier. However, these results \ncome at the cost of a drop in the classifier’s precision.  \n \nBased on the results from Table 4, GPT-3.5 and GPT-4 tended to avoid starting the medical observations with \npronouns, prepositions, articles, or adverbs and instead start sentences with active verbs or proper nouns. This might \nhave come as a result of the instructions in the prompt, which our instructions to the GPT models state, “If short \nphrases make the observation clear, then do those.” This “phrase-like” structure can be seen in Table 1, where each \nexample used came from our synthetic data. However, this cannot be claimed with certainty. Table 4 only shows the \nlack of uniformity of starting words in our datasets.  \n \nWhile the medical expert leaves near-perfect reviews on whether a given observation would be considered a sign of \nautism, the labels themselves were only 100% correct (meaning no additional labels needed) 83.57% of the time. On \na sample size of 140 medical observations, a miss-rate (meaning the total times the labels were incorrect) of 16.43% \nmay be considered small. However, our two studies comprised 1,050 and 3,150 observations, respectively. This \n16.43% miss-rate translates to 173 and 518 incorrect observations. Thus, with domain-specific ChatGPT-generated \ntext data, an expert review of the data is needed to remove incorrect pieces.  \n \nNotably, the B1 criteria ranked the lowest for behaviors common in individuals with autism and highest for normal \nbehavior for individuals without autism. This aligns with the lower recall compared to the baseline on the combined \nGPT-3.5 and GPT-4.0 datasets. Additionally, B4 ranks highest in the “behavior but not typical for autism” category, \nmeaning that the behaviors that exist are not normal but also not common among autistic individuals. This also \naligns with a lower recall in the GPT-3.5 and GPT-4.0 datasets. Future studies need to explore the interaction of \nthese expert evaluations with precision and recall. \n \nThrough the usage of data generated from ChatGPT, we were able to increase the recall performance using single-\nshot, example-free prompts. This recall increase makes this tool useful for screening potential individuals who may \nhave autism, but the low precision makes it untrustworthy for making an official diagnosis. Curiously, despite there \nbeing more A1 samples in the initial dataset than any other criterion, it still had among the largest increases in recall \n(13% for GPT-3.5 and GPT-4.0 and 19% for GPT-4.0 only, respectively), even more than the much smaller B3 \ncriterion (11% for GPT-3.5 and GPT-4.0 and 14% for GPT-4.0 only, respectively). However, given the issues it \nfaces with precision, this augmentation method still needs more work before it can be fully depended upon as a \nreliable data augmentation method. \n  Original Data Set Data Set augmented with GPT-\n3.5 and GPT-4 \nData Set Augmented with GPT-4 \nLabel Recall Precision F1 Recall Precision F1 Recall Precision F1 \nA1 0.33 0.73 0.45 0.46 0.63 0.53 0.52 0.62 0.56 \nA2 0.59 0.83 0.69 0.66 0.76 0.70 0.81 0.61 0.7 \nA3 0.50 0.74 0.60 0.58 0.65 0.61 0.70 0.50 0.58 \nB1 0.67 0.64 0.66 0.62 0.68 0.65 0.69 0.47 0.56 \nB2 0.56 0.76 0.65 0.63 0.72 0.67 0.76 0.49 0.60 \nB3 0.35 0.65 0.46 0.46 0.57 0.51 0.49 0.55 0.52 \nB4 0.65 0.64 0.64 0.62 0.65 0.63 0.60 0.64 0.62 \nAverage  0.52 0.71 0.59 0.58 0.67 0.62 0.65 0.55 0.59 \n \nLimitations \nOur study has several limitations. It should be noted that the data gathered from this study was obtained in \nSeptember. There is a high possibility that any limitations to OpenAI’s GPT models may have changed or no longer \nexist. Thus, it could be valuable to repeat this study later to measure changes to the study over time. Additionally, \nthis study focused on creating new medical observations, not complete records. More research needs to be \nconducted to determine if the GPT models could successfully create complete “artificial individuals” that simulate \nshowing those symptoms over a multi-year span such that a medical professional labels the record with the autism \ndiagnosis. \n \nIdeally, the first study should have been completed using only GPT-3.5 data. However, given the limitations of the \nLLM, it was not possible at this time. As OpenAI updates its ChatGPT models, it could be possible that in a future \nupdate, it could handle such a request and may be worth looking into in a future study. \n \nBy manipulating gender, we may have reinforced GPT-3.5 and GPT-4.0’s potential for bias as it was possible it may \ncreate data containing more traditionally masculine or feminine traits despite minimal changes to the prompt. More \nstudies are needed to analyze if such biases exist within the OpenAI’s models.  \n \nAdditionally, while it would have been ideal to compare different Large Language Models, we must also contend \nwith the requirements to run such models. Processing or creating synthetic text data requires massive amounts of \ncomputational power. A huge advantage of using ChatGPT is that it requires minimal local computational resources \nsince OpenAI handles all of ChatGPT’s backend processing. Ultimately, we do not have the computation resources \nto run our own versions of their models locally. Future research needs to focus not only on comparing different \ntypes of models but also on determining ways to lower their computational costs.  \n \nPotential Implications \nThe ability of GPT-3.5 and GPT-4.0 to produce data that resulted in higher recall has value for autism screening \nmodels and any other output-sensitive domains, such as screening for medical issues such as cancer, disease, or \nother ailments. In these instances, while it may occasionally find samples that turn out not to be problematic, these \nmodels will be trustworthy if they do rule out the issue it intends to detect.  \n \nAcknowledgment \nThis project was supported by grant number R01MH124935 from the National Institute of Mental Health. Part of \nthe data presented were collected by the Centers for Disease Control (CDC) and Prevention Autism and \nDevelopmental Disabilities Monitoring (ADDM) Network supported by CDC Cooperative Agreement Number \n5UR3/DD000680 and by the University of Arizona FY23 Research, Innovation & Impact (RII) and the Technology \nResearch Initiative Fund/Improving Health Initiative (BIO5 Rapid Grant). \n \nThe study was reviewed and approved by the Institutional Review Board at the University of Arizona. \n \nReferences \n1. Phillips J, Pond D, Goode S. Timely diagnosis of dementia: can we do better. Canberra: Alzheimer’s Australia. \n2011; \n2. Moul JW. Timely diagnosis of testicular cancer. urologic clinics of north america. 2007;34(2):109–17. \n3. Bradford A, Kunik ME, Schulz P, Williams SP, Singh H. Missed and delayed diagnosis of dementia in primary \ncare: prevalence and contributing factors. Alzheimer disease and associated disorders. 2009;23(4):306. \n4. Dong X, Qian L, Guan Y, Huang L, Yu Q, Yang J. A multiclass classification method based on deep learning \nfor named entity recognition in electronic medical records. In: 2016 New York Scientific Data Summit \n(NYSDS). 2016. p. 1–10. \n5. Shaw KA, Bilder DA, McArthur D, Williams AR, Amoakohene E, Bakian AV, et al. Early identification of \nautism spectrum disorder among children aged 4 years — Autism and Developmental Disabilities Monitoring \nNetwork, 11 Sites, United States, 2020. MMWR Surveill Summ. 2023 Mar 24;72(1):1–15.  \n6. Maenner MJ, Warren Z, Williams AR, Amoakohene E, Bakian AV, Bilder DA, et al. Prevalence and \ncharacteristics of autism spectrum disorder among children aged 8 years — Autism and Developmental \nDisabilities Monitoring Network, 11 Sites, United States, 2020. MMWR Surveill Summ. 2023 Mar 24;72(2):1–\n14.  \n7. Neurodevelopmental disorders. In: Diagnostic and Statistical Manual of Mental Disorders [Internet]. American \nPsychiatric Association Publishing; 2022 [cited 2023 Sep 14]. (DSM Library). Available from: \nhttps://dsm.psychiatryonline.org/doi/full/10.1176/appi.books.9780890425787.x01_Neurodevelopmental_Disord\ners \n8. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre-trained biomedical language \nrepresentation model for biomedical text mining. Bioinformatics. 2020 Feb 15;36(4):1234–40.  \n9. Shima Y. Image augmentation for object image classification based on combination of pre-trained CNN and \nSVM. In: Journal of Physics: Conference Series. IOP Publishing; 2018. p. 012001. \n10. Nagaraju M, Chawla P, Kumar N. Performance improvement of Deep Learning Models using image \naugmentation techniques. Multimed Tools Appl. 2022 Mar 1;81(7):9177–200. \n11. Ahlbäck E, Dougly M. Can large language models enhance fake mews detection? : improving fake news \ndetection with data augmentation [Internet]. 2023 [cited 2023 Sep 4]. Available from: \nhttps://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-330769  \n12. Sahu G, Rodriguez P, Laradji I, Atighehchian P, Vazquez D, Bahdanau D. Data augmentation for intent \nclassification with off-the-shelf large language models. In: Proceedings of the 4th Workshop on NLP for \nConversational AI [Internet]. Dublin, Ireland: Association for Computational Linguistics; 2022 [cited 2023 Sep \n4]. p. 47–57. Available from: https://aclanthology.org/2022.nlp4convai-1.5  \n13. Dai H, Liu Z, Liao W, Huang X, Cao Y, Wu Z, et al. AugGPT: Leveraging ChatGPT for text data augmentation \n[Internet]. arXiv; 2023 [cited 2023 Aug 24]. Available from: http://arxiv.org/abs/2302.13007 \n14. Schick T, Schütze H. Generating Datasets with pretrained language models. In: Proceedings of the 2021 \nConference on Empirical Methods in Natural Language Processing [Internet]. Online and Punta Cana, \nDominican Republic: Association for Computational Linguistics; 2021 [cited 2023 Sep 4]. p. 6943–51. \nAvailable from: https://aclanthology.org/2021.emnlp-main.555  \n15. Li H, Wu Y, Schlegel V, Batista-Navarro R, Nguyen TT, Ramesh Kashyap A, et al. Team:PULSAR at \nProbSum 2023:PULSAR: Pre-training with extracted healthcare terms for summarising patients’ problems and \ndata augmentation with black-box large language models. In: The 22nd Workshop on Biomedical Natural \nLanguage Processing and BioNLP Shared Tasks [Internet]. Toronto, Canada: Association for Computational \nLinguistics; 2023 [cited 2023 Sep 4]. p. 503–9. Available from: https://aclanthology.org/2023.bionlp-1.49 \n16. Chung HW, Hou L, Longpre S, Zoph B, Tay Y, Fedus W, et al. Scaling instruction-finetuned language models \n[Internet]. arXiv; 2022 [cited 2023 Sep 12]. Available from: http://arxiv.org/abs/2210.11416 \n17. Salinas A, Shah PV, Huang Y, McCormack R, Morstatter F. The unequal opportunities of large language \nmodels: revealing demographic bias through job recommendations [Internet]. arXiv; 2023 [cited 2023 Sep 12]. \nAvailable from: http://arxiv.org/abs/2308.02053 \n18. Gross N. What ChatGPT tells us about gender: a cautionary tale about performativity and gender biases in AI. \nSocial Sciences. 2023 Aug;12(8):435. \n19. Meskó B, Topol EJ. The imperative for regulatory oversight of large language models (or generative AI) in \nhealthcare. npj Digit Med. 2023 Jul 6;6(1):1–6. \n20. Narwane SV, Sawarkar SD. Is handling unbalanced datasets for machine learning uplifts system performance?: \nA case of diabetic prediction. Diabetes Metab Syndr. 2022 Sep;16(9):102609. \n21. Zhao Y, Wong ZSY, Tsui KL. A Framework of Rebalancing Imbalanced Healthcare Data for Rare Events’ \nClassification: A Case of Look-Alike Sound-Alike Mix-Up Incident Detection. Journal of Healthcare \nEngineering. 2018 May 22;2018:e6275435. \n22. transformers.modeling_tf_bert — transformers 3.0.2 documentation [Internet]. [cited 2023 Sep 6]. Available \nfrom: https://huggingface.co/transformers/v3.0.2/_modules/transformers/modeling_tf_bert.html#TFBertModel  \n23. Glossary [Internet]. [cited 2023 Sep 6]. Available from: https://huggingface.co/docs/transformers/glossary \n24. Ye J, Chen X, Xu N, Liu S, Cui Y, Zhou Z, et al. A Comprehensive capability analysis of GPT-3 and GPT-3.5 \nSeries Models. \n25. OpenAI platform [Internet]. [cited 2023 Sep 7]. Available from: https://platform.openai.com \n26. Heaven DW. MIT Technology Review. 2023 [cited 2023 Sep 7]. GPT-4 is bigger and better than ChatGPT—\nbut OpenAI won’t say why. Available from: \nhttps://www.technologyreview.com/2023/03/14/1069823/gpt-4-is-bigger-and-better-chatgpt-openai/  "
}