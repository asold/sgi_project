{
  "title": "Towards Online End-to-end Transformer Automatic Speech Recognition",
  "url": "https://openalex.org/W2981696585",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4283684714",
      "name": "Tsunoo, Emiru",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2565490869",
      "name": "Kashiwagi, Yosuke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281023199",
      "name": "Kumakura, Toshiyuki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2575256866",
      "name": "Watanabe, Shinji",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2608712415",
    "https://openalex.org/W2773781902",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2964089206",
    "https://openalex.org/W2913718171",
    "https://openalex.org/W2193413348",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W854541894",
    "https://openalex.org/W2799800213",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W2766219058",
    "https://openalex.org/W2972977747",
    "https://openalex.org/W2911291251",
    "https://openalex.org/W2514969556",
    "https://openalex.org/W2491408735",
    "https://openalex.org/W2102113734",
    "https://openalex.org/W2963240019",
    "https://openalex.org/W2936078256",
    "https://openalex.org/W2962824709",
    "https://openalex.org/W2972439411",
    "https://openalex.org/W2936123380",
    "https://openalex.org/W2980690446",
    "https://openalex.org/W2963242190",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2963414781",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2962780374",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2526425061",
    "https://openalex.org/W2972995428",
    "https://openalex.org/W2143612262"
  ],
  "abstract": "The Transformer self-attention network has recently shown promising performance as an alternative to recurrent neural networks in end-to-end (E2E) automatic speech recognition (ASR) systems. However, Transformer has a drawback in that the entire input sequence is required to compute self-attention. We have proposed a block processing method for the Transformer encoder by introducing a context-aware inheritance mechanism. An additional context embedding vector handed over from the previously processed block helps to encode not only local acoustic information but also global linguistic, channel, and speaker attributes. In this paper, we extend it towards an entire online E2E ASR system by introducing an online decoding process inspired by monotonic chunkwise attention (MoChA) into the Transformer decoder. Our novel MoChA training and inference algorithms exploit the unique properties of Transformer, whose attentions are not always monotonic or peaky, and have multiple heads and residual connections of the decoder layers. Evaluations of the Wall Street Journal (WSJ) and AISHELL-1 show that our proposed online Transformer decoder outperforms conventional chunkwise approaches.",
  "full_text": "arXiv:1910.11871v1  [eess.AS]  25 Oct 2019\nTOW ARDS ONLINE END-TO-END TRANSFORMER AUTOMA TIC SPEECH RECOGNITION\nEmiru Tsunoo1, Y osuke Kashiwagi1, T oshiyuki Kumakura1, Shinji W atanabe2\n1Sony Corporation, Japan\n2Johns Hopkins University, USA\nABSTRACT\nThe Transformer self-attention network has recently shown promis-\ning performance as an alternative to recurrent neural netwo rks in\nend-to-end (E2E) automatic speech recognition (ASR) syste ms.\nHowever, Transformer has a drawback in that the entire input se-\nquence is required to compute self-attention. W e have propo sed a\nblock processing method for the Transformer encoder by intr oduc-\ning a context-aware inheritance mechanism. An additional c ontext\nembedding vector handed over from the previously processed block\nhelps to encode not only local acoustic information but also global\nlinguistic, channel, and speaker attributes. In this paper , we extend\nit towards an entire online E2E ASR system by introducing an o n-\nline decoding process inspired by monotonic chunkwise atte ntion\n(MoChA) into the Transformer decoder. Our novel MoChA train ing\nand inference algorithms exploit the unique properties of T rans-\nformer, whose attentions are not always monotonic or peaky , and\nhave multiple heads and residual connections of the decoder lay-\ners. Evaluations of the W all Street Journal (WSJ) and AISHEL L-1\nshow that our proposed online Transformer decoder outperfo rms\nconventional chunkwise approaches.\nIndex T erms— Speech Recognition, End-to-end, Transformer,\nSelf-attention Network, Monotonic Chunkwise Attention\n1. INTRODUCTION\nEnd-to-end (E2E) automatic speech recognition (ASR) has be en at-\ntracting attention as a method of directly integrating acou stic models\n(AMs) and language models (LMs) because of the simple traini ng\nand efﬁcient decoding procedures. In recent years, various models\nhave been studied, including connectionist temporal class iﬁcation\n(CTC) [1–4], attention-based encoder–decoder models [5–9 ], their\nhybrid models [10, 11], and the RNN-transducer [12–14]. Tra ns-\nformer [15] has been successfully introduced into E2E ASR by re-\nplacing RNNs [16–20], and it outperforms bidirectional RNN mod-\nels in most tasks [21]. Transformer has multihead self-atte ntion net-\nwork (SAN) layers, which can leverage a combination of infor ma-\ntion from completely different positions of the input.\nHowever, similarly to bidirectional RNN models [22], Trans -\nformer has a drawback in that the entire utterance is require d to\ncompute self-attention, making it difﬁcult to utilize in on line recog-\nnition systems. Also, the memory and computational require ments\nof Transformer grow quadratically with the input sequence l ength,\nwhich makes it difﬁcult to apply to longer speech utterances . A sim-\nple solution to these problems is block processing as in [17, 19, 23].\nHowever, it loses global context information and its perfor mance is\ndegraded in general.\nW e have proposed a block processing method for the encoder–\ndecoder Transformer model by introducing a context-aware i nher-\nitance mechanism, where an additional context embedding ve ctor\nhanded over from the previously processed block helps to enc ode not\nonly local acoustic information but also global linguistic , channel,\nand speaker attributes [24]. Although it outperforms naive block-\nwise encoders, the block processing method can only be appli ed to\nthe encoder because it is difﬁcult to apply to the decoder wit hout\nknowing the optimal chunk step, which depends on the token un it\ngranularity and the language.\nFor the attention decoder, various online processes have be en\nproposed. In [5, 25, 26], the chunk window is shifted from an i n-\nput position determined by the median or maximum of the atten -\ntion distribution. Monotonic chunkwise attention (MoChA) uses a\ntrainable monotonic energy function to shift the chunk wind ow [27].\nMoChA has also been extended to make it stable while training [28]\nand to be able to change the chunk size adaptively to the circu m-\nstances [29]. [30] proposed a unique approach that uses a tri gger\nmechanism to notify the timing of the attention computation . How-\never, to the best of our knowledge, such monotonic chunkwise ap-\nproaches have not yet been applied to Transformer.\nIn this paper, we extend our previous context block approach to-\nwards an entire online E2E ASR system by introducing an onlin e\ndecoding process inspired by MoChA into the Transformer dec oder.\nOur contributions are as follows. 1) Triggers for shifting c hunks\nare estimated from the source–target attention (ST A), whic h uses\nqueries and keys, 2) all the past information is utilized acc ording\nto the characteristics of the Transformer attentions that a re not al-\nways monotonic or locally peaky , and 3) a novel training algo rithm\nof MoChA is proposed, which extends to train the trigger func tion\nby dealing with multiple attention heads and residual conne ctions\nof the decoder layers. Evaluations of the W all Street Journa l (WSJ)\nand AISHELL-1 show that our proposed online Transformer dec oder\noutperforms conventional chunkwise approaches.\n2. TRANSFORMER ASR\nThe baseline Transformer ASR follows that in [21], which is b ased\non the encoder–decoder architecture. An encoder transform s a T -\nlength speech feature sequence x = (x1, . . . , x T ) to an L-length\nintermediate representation h = (h1, . . . , h L), where L ≤ T due\nto downsampling. Given h and previously emitted character outputs\nyi− 1 = (y1, . . . , y i− 1), a decoder estimates the next character yi.\nThe encoder consists of two convolutional layers with strid e 2\nfor downsampling, a linear projection layer, positional en coding,\nfollowed by Ne encoder layers and layer normalization. Each en-\ncoder layer has a multihead SAN followed by a position-wise f eed-\nforward network, both of which have residual connections. L ayer\nnormalization is also applied before each module. In the SAN , at-\ntention weights are formed from queries ( Q ∈ Rtq × d) and keys\n(K ∈ Rtk× d), and applied to values ( V ∈ Rtv× d) as\nEncoder\nLayer\nEncoder\nLayer\nEncoder\nLayer\nEncoder\nLayer\nEncoder\nLayer\nEncoder\nLayer\nEncoder\nLayer\nEncoder\nLayer\nEncoder\nLayer\n…\n…\n…\n…\n…\nDownsampled\ninputs\nContext\nembedding\nEncoder\nlayers (\u0001\n\u0000\n\u0002 )\nOutputs (\n\u0003\n)\n\u0004\n\u0005\n\u0006\n\u0007\n\b\n\t\n\n\u000b\n\f\n\r\n\u000e\n\u000f\nFig. 1. Context inheritance mechanism of the encoder.\nAttention(Q, K, V) = softmax\n( QKT\n√\nd\n)\nV, (1)\nwhere typically d = dmodel/M for the number of heads M. W e\nutilized multihead attention denoted, as the MHD(·) function, as\nfollows:\nMHD(Q, K, V) = Concat(head1, . . . , headM )Wn\nO, (2)\nheadm = Attention(QWn\nQ,m , KWn\nK,m , VWn\nV,m ). (3)\nIn (2) and (3), the nth layer is computed with the projection\nmatrices Wn\nQ,m ∈ Rdmodel× d, Wn\nK,m ∈ Rdmodel× d, Wn\nV,m ∈\nRdmodel× d, and Wn\nO ∈ RMd× dmodel . For all the SANs in the en-\ncoder, Q, K, and V are the same matrices, which are the inputs of\nthe SAN. The position-wise feedforward network is a stack of linear\nlayers.\nThe decoder predicts the probability of the following chara cter\nfrom previous output characters yi− 1 and the encoder output h, i.e.,\np(yi|yi− 1, h). The character history sequence is converted to char-\nacter embeddings. Then, Nd decoder layers are applied, followed\nby the linear projection and Softmax function. The decoder l ayer\nconsists of a SAN and a ST A, followed by a position-wise feedf or-\nward network. The ﬁrst SAN in each decoder layer applies atte ntion\nweights to the input character sequence, where the input seq uence of\nthe SAN is set as Q, K, and V. Then, the following ST A attends\nto the entire encoder output sequence by setting K and V to be the\nencoder output h.\nThe SAN can leverage a combination of information from com-\npletely different positions of the input. This is due to the m ultiple\nheads and residual connections of the layers that complemen t each\nother, i.e., some attend monotonically and locally while others at-\ntend globally. Transformer requires the entire speech utterance for\nboth the encoder and decoder; thus, they are processed only a fter the\nend of the utterance, which causes a huge delay . T o realize an online\nASR system, both the encoder and decoder are processed onlin e.\n3. CONTEXTUAL BLOCK PROCESSING OF ENCODER\nA simple way to process the encoder online is blockwise compu -\ntation, as in [17, 19, 23]. However, the global channel, spea ker, and\n(a)\n h\n50 100 150 200 250\n y\n20\n40\n60\n80\n100\n120\n(b)\n h\n50 100 150 200 250\n y\n20\n40\n60\n80\n100\n120\nFig. 2. Examples of attentions in a Transformer decoder layer. (a) is\na head having wider attentions, and (b) is a head attending a c ertain\narea of h.\nlinguistic context are also important for local phoneme cla ssiﬁcation.\nW e have proposed a context inheritance mechanism for block p ro-\ncessing by introducing an additional context embedding vec tor [24].\nAs shown in the tilted arrows in Fig. 1, the context embedding vector\nis computed in each layer of each block and handed over to the u pper\nlayer of the following block. Thus, the SAN in each layer is ap plied\nto the block input sequence using the context embedding vect or.\nThe context embedding vector is introduced into the origina l for-\nmulation in Sec. 2. Denoting the context embedding vector as cn\nb ,\nthe augmented variables satisfy ˜Qn\nb = [Zn− 1\nb cn− 1\nb ] and ˜Kn\nb =\n˜Vn\nb = [Zn− 1\nb cn− 1\nb− 1 ], where the context embedding vector of the\nprevious block (b − 1) of the previous layer (n − 1) is used. Zn\nb is\nthe output of the nth encoder layer of block b, which is computed\nsimultaneously with the context embedding vector cn\nb as\n[Zn\nb cn\nb ] = max(0, ˜Zn\nb, int. Wn\n1 + vn\n1 )Wn\n2 + vn\n2 + ˜Zn\nb, int. (4)\n˜Zn\nb, int. = MHD(˜Qn\nb , ˜Kn\nb , ˜Vn\nb ) + ˜Vn\nb , (5)\nwhere Wn\n1 , Wn\n2 , vn\n1 , and vn\n2 are trainable matrices and biases.\nThe output of the SAN does not only encode input acoustic feat ures\nbut also delivers the context information to the succeeding layer as\nshown by the tilted red arrows in Fig. 1.\n4. ONLINE PROCESS FOR DECODER\n4.1. Online T ransformer Decoder based on MoChA\nThe decoder of Transformer ASR is incremental at test time, e spe-\ncially for the ﬁrst SAN of each decoder layer. However, the se cond\nST A requires the entire sequence of the encoded features h. Block-\nwise attention mechanisms cannot be simply applied with a ﬁx ed\nstep size, because the step size depends on the output token g ranu-\nlarity (grapheme, character, (sub-)word, and so forth) and language.\nIn addition, not all the ST As are monotonic, because the othe r heads\nand layers complement each other. T ypically , in the lower la yer of\nthe Transformer decoder, some heads attend wider areas, and some\nattend a certain area constantly , as shown in Fig. 2. Therefo re, chunk\nshifting and the chunk size should be adaptive.\nFor RNN models, the median or maximum of the attention dis-\ntribution is used as a cue for shifting a ﬁxed-length chunk, w here\nthe parameters of the original batch models are reused [5, 25 , 26].\nMoChA further introduces the probability distribution of c hunking\nto train the monotonic chunking mechanism. In this paper, we pro-\npose a novel online decoding method inspired by MoChA.\nMoChA [27] splits the input sequence into small chunks over\nwhich soft attention is computed. It learns a monotonic alig nment\nbetween the encoder features h and the output sequence y, with w-\nlength chunking. “Soft” attention is efﬁciently utilized w ith back-\nAlgorithm 1 MoChA Inference for n-th Transformer Decoder Layer\nInput: encoder features h, length L, chunk size w\n1: Initialize: y0 = ⟨sos⟩, tm, 0 = 1, i = 1\n2: while yi− 1 ̸= ⟨eos⟩ do\n3: for m = 1to M do\n4: for j = tm,i − 1 to L do\n5: pm,i,j = σ(Energym(zSAN,i , hj ))\n6: if pm,i,j ≥ 0.5 then\n7: tm,i = j\n8: break\n9: end if\n10: end for\n11: if pm,i,j < 0.5, ∀j ∈ { tm,i − 1, . . . , L } then\n12: tm,i = tm,i − 1\n13: end if\n14: r = tm,i − w + 1 // or r = 1\n15: for k = r to ti do\n16: um,i,k = ChunkEnergym(zSAN,i , hk)\n17: end for\n18: headm,i = ∑ ti\nk=r\nexp(ui,k)\n∑ ti\nl=r exp(ui,l)\nvm,k\n19: end for\n20: zSTA,i = STA(yi− 1, head1,i , . . . , headM,i ), i = i + 1\n21: end while\npropagation to train chunking parameters. At the test time, online\n“hard” chunking is used to realize online ASR, which achieve s al-\nmost the same performance as the soft attention model.\nSince Transformer has unique properties, the conventional\nMoChA cannot be simply applied. One property is that the ST A\nis computed using queries and keys, while MoChA is formulate d on\nthe basis of the attention using a hidden vector of the RNN and tanh.\nAnother property is that not all the ST As are monotonic, beca use the\nother heads and layers complement each other, as examples sh own\nin Fig. 2. W e modify the training algorithm of MoChA to deal wi th\nthese characteristics.\n4.2. Inference Algorithm\nThe inference process for decoder layer n is shown in Algorithm 1.\nThe differences from the original MoChA are highlighted in r ed\ncolor. In our case, MoChA decoding is introduced into the sec ond\nST A of each decoder layer; the vector zSAN,i in Algorithm 1 is the\noutput of the ﬁrst SAN in the decoder layer. STA(·) in line 20 con-\ncatenates and computes an output of the ST A network, zSTA,i , in\neach decoder layer, as in (2). MoChA can be applied independe ntly\nto each head; thus, we added line 3. In line 18, the attention w eight is\napplied to the selected values vm,k = hkWV,m to compute headm\nin (3), and the chunk of selection shifts monotonically .\npm,i,j in line 5 is regarded as a trigger function at head m to\nmove the computing chunk, which is estimated from an Energy\nfunction. For the Energy and ChunkEnergy (in line 16) functions,\nthe original MoChA utilizes tanh because it is used as a nonlinear\nfunction in RNNs. However, in Transformer, attentions are c om-\nputed using queries and keys as in (1). Therefore, we modify t hem\nfor the head m as\nEnergym(zSAN,i , hj ) =gm\nqi,m kT\nj,m\n√\nd||qi,m ||\n+ rm, (6)\nChunkEnergym(zSAN,i , hj ) = qi,m kT\nj,m√\nd\n, (7)\nAlgorithm 2 MoChA Training for n-th Transformer Decoder Layer\nInput: encoder features h, length L, chunk size w, Gauss. noise ǫ\n1: Initialize: y0 = ⟨sos⟩, α0, 0 = 1, α0,k = 0(k ̸= 0), i = 1\n2: while yi− 1 ̸= ⟨eos⟩ do\n3: for m = 1to M do\n4: for j = 1to L do\n5: pm,i,j = σ(Energym(zSAN,i , hj ) +ǫ)\n6: qm,i,j = ∏ L\nk=j+1(1 − pm,i,k )\n7: αm,i,j = pm,i,j\n∑ j\nk=1\n(\nαm,i − 1,k\n∏ j− 1\nl=k (1 − pm,i,l )\n)\n+qm,i,j αm,i − 1,j\n8: end for\n9: for j = 1to L do\n10: um,i,k = ChunkEnergym(zSAN,i , hk)\n11: βm,i,j = ∑ j+w− 1\nk=j\nα m,i,j exp(um,i,k)∑ k\nl=k− w+1 exp(um,i,l)\n12: end for\n13: headm,i = ∑ L\nj=1 βm,i,j vj\n14: end for\n15: zSTA,i =\nSTA(yi− 1, head1,i , . . . , headM,i ), i = i + 1\n16: end while\nwhere gm and rm are trainable scalar parameters, qi,m = zSAN,i WQ,m ,\nand kj,m = hj WK,m as in (3).\nNote that, the exception in lines 11–13, where the trigger ne ver\nignites in frame i, sets headm,i as 0 in the original MoChA. How-\never, we compute headm,i using the previous tm,i − 1 (line 12) be-\ncause the exception often occurs in Transformer. Also, for o nline\nprocessing, all the past frames of encoded features h are also avail-\nable without any latency , while the original MoChA computes atten-\ntions within the ﬁxed-length chunk. T aking into account the property\nthat Transformer attentions tend to be distributed widely a nd are not\nalways monotonic, we also consider utilizing the past frame s. W e\noptionally modify line 14 by setting r = 1 and test both cases in\nSec. 5.\n4.3. T raining Algorithm\nMoChA strongly relies on the monotonicity of the attentions , and it\nalso forces attentions to be monotonic, while Transformer h as a ﬂex-\nible attention mechanism that may be able to integrate infor mation\nof various positions without the monotonicity . Further mor e, the\nTransformer decoder has both multihead and residual connec tions.\nTherefore, typically , not all the attentions become monoto nic, as in\nFig. 2.\nThe original MoChA training computes a variable αi,j , which\nis a cumulative probability of computing the local chunk att ention at\nti = j, deﬁned as\nαi,j = pi,j\nj∑\nk=1\n(\nαi− 1,k\nj− 1∏\nl=k\n(1 − pi,l )\n)\n. (8)\nWhen pi,j ≈ 0 for all j, which occurs frequently in Transformer\nbecause the other heads and layers complement each other for this\nframe, αi,j rapidly decays after i. An example is shown in Fig. 3.\nThe top left shows pm,i,j in Algorithm 1, which has monotonicity .\nThe top right is the original αi,j in (8), in which the value decreases\nimmediately after around frame 50 of the target y and does not re-\ncover.\nTherefore, we introduce a probability of the trigger not ign iting\nas qm,i,j into computation of αm,i,j . Thus, the new training algo-\nrithm for Transformer is shown in Algorithm 2, which encoura ges\n p i,j\n h\n50 100 150 200 250\n y\n20\n40\n60\n80\n100\n120\noriginal  α i,j\n h\n50 100 150 200 250\n y\n20\n40\n60\n80\n100\n120\nmodified  α i,j\n h\n50 100 150 200 250\n y\n20\n40\n60\n80\n100\n120\n β i,j\n h\n50 100 150 200 250\n y\n20\n40\n60\n80\n100\n120\nFig. 3. Example of expected attention in the Transformer decoder.\nT op left: pi,j in Algorithm 2; top right: original αi,j in (8); bot-\ntom left: our modiﬁed αi,j in Algorithm 2; bottom right: expected\nattention βi,j . Head index m is omitted for simplicity .\nMoChA to exploit the ﬂexibility of the SAN in Transformer (co lored\nlines are new to the original MoChA). An example of our modiﬁe d\nαm,i,j is shown in the bottom left of Fig. 3, which maintains the\nmonotonicity . The bottom right shows the expected attentio n βm,i,j .\n5. EXPERIMENTS\n5.1. Experimental Setup\nW e carried out experiments using the WSJ English and AISHELL -\n1 Mandarin data [31]. The input acoustic features were 80-\ndimensional ﬁlter banks and the pitch, extracted with a hop s ize\nof 10 ms and a window size of 25 ms, which were normalized with\nthe global mean and variance. For the WSJ English setup, the n um-\nber of output classes was 52, including symbols. W e used 4,23 1\ncharacter classes for the AISHELL-1 Mandarin setup.\nFor the training, we utilized multitask learning with CTC lo ss as\nin [11, 21] with a weight of 0.1. A linear layer was added onto t he\nencoder to project h to the character probability for the CTC. The\nTransformer models were trained over 100 epochs for WSJ and 5 0\nepochs for AISHELL-1, with the Adam optimizer and Noam learn -\ning rate decay as in [15]. The learning rate was set to 5.0 and t he\nminibatch size to 20. SpecAugment [ ?] was applied to only WSJ.\nThe parameters of the last 10 epochs were averaged and used\nfor inference. The encoder had Ne = 12layers with 2048 units and\nthe decoder had Nd = 6layers with 2048 units, with both having a\ndropout rate of 0.1. W e set dmodel = 256and M = 4for the mul-\ntihead attentions. W e trained three types of Transformer, n amely ,\nbaseline Transformer [21], Transformer with the contextua l block\nprocessing encoder (CBP Enc. + Batch Dec.) [24], and the prop osed\nentire online model with the online decoder (CBP Enc. + Propo sed\nDec.). The training was carried out using ESPNet [32] with th e Py-\nT orch backend. The median based chunk shifting [5] with a win dow\nof 16 frames was also applied to the Batch Dec. with and withou t\npast frames for the fair comparison (CBP Enc. + Median Dec.).\nFor the CBP Enc. models, we set the parameters as Lblock = 16\nT able 1. W ord error rates (WERs) in the WSJ and AISHELL-1 eval-\nuation task.\nWSJ (WER) AISHELL-1 (CER)\nBatch processing\nbiLSTM [11] 6.7 9.2\nuniLSTM 8.4 11.8\nTransformer [21] 4.9 6.7\nCBP Enc. + Batch Dec. [24] 6.0 7.6\nOnline processing\nCBP Enc. + median Dec. [5] 9.9 25.0\n— with past frames 7.9 24.2\nCBP Enc. + Proposed Dec. 8.8 18.7\n— with past frames 6.6 9.7\nand Lhop = 8. For the initialization of context embedding, we uti-\nlized the average of the input features to simplify the imple menta-\ntion. The decoder was trained with the proposed MoChA archit ec-\nture using w = 8. The ST A were computed within each chunk, or\nusing all the past frames of encoded features as described in Sec.\n4.2.\nThe decoding was performed alongside the CTC, whose prob-\nabilities were added with weights of 0.3 for WSJ and 0.7 for\nAISHELL-1 to those of Transformer. W e performed decoding\nusing a beam search with a beam size of 10. An external word-le vel\nLM, which was a single-layer LSTM with 1000 units, was used fo r\nrescoring using shallow fusion [33] with a weight of 1.0 for WSJ. A\ncharacter-level LM with the same structure was fused with a w eight\nof 0.5 for AISHELL-1.\nFor comparison, unidirectional and bidirectional LSTM mod els\nwere also trained as in [11]. The models consisted of an encod er\nwith a VGG layer, followed by LSTM layers and a decoder. The\nnumbers of encoder layers were six and three, with 320 and 102 4\nunits for WSJ and AISHELL-1, respectively . The decoders wer e an\nLSTM layer with 300 units for WSJ and two LSTM layers with 1024\nunits for AISHELL-1.\n5.2. Results\nExperimental results are summarized in T able 1. The chunk ho p-\nping using the median of attention worked well in the English task\nbut poorly in the Chinese task. This was because Chinese requ ires\na wider area of the encoded features to emit each character. O n the\nother hand, our proposed decoder prevented the degradation of per-\nformance. In particular, using all the past frames of encode d fea-\ntures, our proposed decoder achieved the highest accuracy a mong\nthe online processing methods. This indicated that the new d ecoding\nalgorithm was able to exploit the wider attentions of Transf ormer.\n6. CONCLUSION\nW e extended our previous Transformer, which adopted a conte xtual\nblock processing encoder, towards an entirely online E2E AS R sys-\ntem by introducing an online decoding process inspired by Mo ChA\ninto the Transformer decoder. The MoChA training and infere nce al-\ngorithms were extended to cope with the unique properties of Trans-\nformer whose attentions are not always monotonic or peaky an d\nhave multiple heads and residual connections of the decoder lay-\ners. Evaluations of WSJ and AISHELL-1 showed that our propos ed\nonline Transformer decoder outperformed conventional chu nkwise\napproaches. Thus, we realize the entire online processing o f Trans-\nformer ASR with reasonable performance.\n7. REFERENCES\n[1] Alex Graves, Santiago Fern´ andez, Faustino Gomez, and J ¨ urgen\nSchmidhuber, “Connectionist temporal classiﬁcation: lab elling unseg-\nmented sequence data with recurrent neural networks, ” in Proc. of the\n23rd International Conference on Machine Learning , 2006, pp. 369–\n376.\n[2] Alex Graves and Navdeep Jaitly , “T owards end-to-end spe ech recog-\nnition with recurrent neural networks, ” in International Conference on\nMachine Learning, 2014, pp. 1764–1772.\n[3] Y ajie Miao, Mohammad Gowayyed, and Florian Metze, “EESE N: End-\nto-end speech recognition using deep RNN models and WFST-ba sed\ndecoding, ” in Proc. of IEEE Automatic Speech Recognition and Un-\nderstanding (ASRU) W orkshop, 2015, pp. 167–174.\n[4] Dario Amodei et al., “Deep Speech 2: End-to-end speech re cognition in\nEnglish and Mandarin, ” in Proc. of the 33rd International Conference\non Machine Learning, 2016, vol. 48, pp. 173–182.\n[5] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Ky unghyun\nCho, and Y oshua Bengio, “ Attention-based models for speech recog-\nnition, ” in Advances in Neural Information Processing Systems, 2015,\npp. 577–585.\n[6] William Chan, Navdeep Jaitly , Quoc Le, and Oriol V inyals , “Listen,\nattend and spell: A neural network for large vocabulary conv ersational\nspeech recognition, ” in Proc. of IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , 2016, pp. 4960–\n4964.\n[7] Liang Lu, Xingxing Zhang, and Steve Renais, “On training the recur-\nrent neural network encoder-decoder for large vocabulary e nd-to-end\nspeech recognition, ” in Proc. of IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , 2016, pp. 5060–\n5064.\n[8] Albert Zeyer, Kazuki Irie, Ralf Schl ¨ uter, and Hermann N ey , “Improved\ntraining of end-to-end attention models for speech recogni tion, ” in\nProc. of Interspeech 2018, 2018, pp. 7–11.\n[9] Chung-Cheng Chiu, T ara N. Sainath, Y onghui Wu, Rohit Pra bhavalkar,\nPatrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J. W eiss, K anishka\nRao, Ekaterina Gonina, et al., “State-of-the-art speech re cognition with\nsequence-to-sequence models, ” in Proc. of IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP) , 2018, pp.\n4774–4778.\n[10] Suyoun Kim, T akaaki Hori, and Shinji W atanabe, “Joint C TC-attention\nbased end-to-end speech recognition using multi-task lear ning, ” in\nProc. of IEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), 2017, pp. 4835–4839.\n[11] Shinji W atanabe, T akaaki Hori, Suyoun Kim, John R. Hers hey , and\nT omoki Hayashi, “Hybrid CTC/attention architecture for en d-to-end\nspeech recognition, ” IEEE Journal of Selected T opics in Signal Pro-\ncessing, vol. 11, no. 8, pp. 1240–1253, 2017.\n[12] Alex Graves, “Sequence transduction with recurrent ne ural networks, ”\nin ICML Representation Learning W orkshop, 2012.\n[13] Alex Graves, Abdel-Rahman Mohamed, and Geoffrey Hinto n, “Speech\nrecognition with deep recurrent neural networks, ” in Proc. of IEEE\nInternational Conference on Acoustics, Speech and Signal P rocessing\n(ICASSP), 2013, pp. 6645–6649.\n[14] Kanishka Rao, Has ¸im Sak, and Rohit Prabhavalkar, “Exp loring ar-\nchitectures, data and units for streaming end-to-end speec h recognition\nwith RNN-transducer, ” in Proc. of IEEE Automatic Speech Recognition\nand Understanding (ASRU) W orkshop, 2017, pp. 193–199.\n[15] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszko reit, Llion\nJones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin, “ Atten-\ntion is all you need, ” in Advances in Neural Information Processing\nSystems, 2017, pp. 5998–6008.\n[16] Linhao Dong, Shuang Xu, and Bo Xu, “Speech-transformer : a no-\nrecurrence sequence-to-sequence model for speech recogni tion, ” in\nProc. of IEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), 2018, pp. 5884–5888.\n[17] Matthias Sperber, Jan Niehues, Graham Neubig, Sebasti an St ¨ uker, and\nAlex W aibel, “Self-attentional acoustic models, ” in Proc. of Inter-\nspeech, 2018, pp. 3723–3727.\n[18] Julian Salazar, Katrin Kirchhoff, and Zhiheng Huang, “ Self-attention\nnetworks for connectionist temporal classiﬁcation in spee ch recogni-\ntion, ” in Proc. of IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), 2019, pp. 7115–7119.\n[19] Linhao Dong, Feng W ang, and Bo Xu, “Self-attention alig ner: A\nlatency-control end-to-end model for ASR using self-atten tion network\nand chunk-hopping, ” in Proc. of IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , 2019, pp. 5656–\n5660.\n[20] Y uanyuan Zhao, Jie Li, Xiaorui W ang, and Y an Li, “The Spe echtrans-\nformer for large-scale Mandarin Chinese speech recognitio n, ” in Proc.\nof IEEE International Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), 2019, pp. 7095–7099.\n[21] Shigeki Karita, Nanxin Chen, T omoki Hayashi, T akaaki H ori, Hiro-\nfumi Inaguma, Ziyan Jiang, Masao Someki, Nelson Enrique Y al ta\nSoplin, Ryuichi Y amamoto, Xiaofei W ang, et al., “ A comparat ive\nstudy on transformer vs RNN in speech applications, ” arXiv preprint\narXiv:1909.06317, 2019.\n[22] Mike Schuster and Kuldip K. Paliwal, “Bidirectional re current neural\nnetworks, ” IEEE Transactions on Signal Processing , vol. 45, no. 11,\npp. 2673–2681, 1997.\n[23] Navdeep Jaitly , David Sussillo, Quoc V . Le, Oriol V inya ls, Ilya\nSutskever, and Samy Bengio, “ A neural transducer, ” arXiv preprint\narXiv:1511.04868, 2015.\n[24] Emiru Tsunoo, Y osuke Kashiwagi, T oshiyuki Kumakura, a nd Shinji\nW atanabe, “Transformer ASR with contextual block processi ng, ” arXiv\npreprint arXiv:1910.07204, 2019.\n[25] William Chan and Ian Lane, “On online attention-based s peech recog-\nnition and joint Mandarin character-Pinyin training, ” in Proc. of Inter-\nspeech, 2016, pp. 3404–3408.\n[26] Andr´ e Merboldt, Albert Zeyer, Ralf Schl ¨ uter, and Her mann Ney , “ An\nanalysis of local monotonic attention variants, ” Proc. of Interspeech\n2019, pp. 1398–1402, 2019.\n[27] Chung-Cheng Chiu and Colin Raffel, “Monotonic chunkwi se atten-\ntion, ” arXiv preprint arXiv:1712.05382, 2017.\n[28] Haoran Miao, Gaofeng Cheng, Pengyuan Zhang, T a Li, and Y onghong\nY an, “Online hybrid CTC/attention architecture for end-to -end speech\nrecognition, ” Proc. of Interspeech 2019, pp. 2623–2627, 2019.\n[29] Ruchao Fan, Pan Zhou, W ei Chen, Jia Jia, and Gang Liu, “ An online\nattention-based model for speech recognition, ” Proc. of Interspeech\n2019, pp. 4390–4394, 2019.\n[30] Niko Moritz, T akaaki Hori, and Jonathan Le Roux, “Trigg ered atten-\ntion for end-to-end speech recognition, ” in Proc. of IEEE International\nConference on Acoustics, Speech and Signal Processing (ICA SSP),\n2019, pp. 5666–5670.\n[31] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng, “AIS hell-\n1: An open-source Mandarin speech corpus and a speech recogn ition\nbaseline, ” in Oriental COCOSDA, 2017, pp. 1–5.\n[32] Shinji W atanabe, T akaaki Hori, Shigeki Karita, T omoki Hayashi, Jiro\nNishitoba, Y uya Unno, Nelson Enrique Y alta Soplin, Jahn Hey mann,\nMatthew Wiesner, Nanxin Chen, et al., “ESPnet: End-to-end s peech\nprocessing toolkit, ” in Proc. of Interspeech, 2019, pp. 2207–2211.\n[33] Anjuli Kannan, Y onghui Wu, Patrick Nguyen, T ara N Saina th, ZhiJeng\nChen, and Rohit Prabhavalkar, “ An analysis of incorporatin g an exter-\nnal language model into a sequence-to-sequence model, ” in Proc. of\nIEEE International Conference on Acoustics, Speech and Sig nal Pro-\ncessing (ICASSP), 2018, pp. 5824–5828.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7185831665992737
    },
    {
      "name": "Transformer",
      "score": 0.6539703607559204
    },
    {
      "name": "Decoding methods",
      "score": 0.5335394144058228
    },
    {
      "name": "Speech recognition",
      "score": 0.5315178632736206
    },
    {
      "name": "Inference",
      "score": 0.5112109184265137
    },
    {
      "name": "End-to-end principle",
      "score": 0.5108518004417419
    },
    {
      "name": "Encoder",
      "score": 0.4986119270324707
    },
    {
      "name": "Exploit",
      "score": 0.41669976711273193
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4030550718307495
    },
    {
      "name": "Algorithm",
      "score": 0.2133464515209198
    },
    {
      "name": "Engineering",
      "score": 0.1645677387714386
    },
    {
      "name": "Electrical engineering",
      "score": 0.09308117628097534
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2800278093",
      "name": "Sony Corporation (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    }
  ]
}