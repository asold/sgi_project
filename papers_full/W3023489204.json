{
  "title": "Scheduled DropHead: A Regularization Method for Transformer Models",
  "url": "https://openalex.org/W3023489204",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4223874739",
      "name": "Zhou, Wangchunshu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099383290",
      "name": "Ge, Tao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1990118311",
      "name": "Xu Ke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2389670735",
      "name": "Wei, Furu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102363648",
      "name": "Zhou, Ming",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W2903810591",
    "https://openalex.org/W2953084091",
    "https://openalex.org/W2740721704",
    "https://openalex.org/W2989499211",
    "https://openalex.org/W2409027918",
    "https://openalex.org/W4919037",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W2890964657",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964081807",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2781596748",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2890166761",
    "https://openalex.org/W2408279554",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2951652751",
    "https://openalex.org/W2963744496",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W2767989436",
    "https://openalex.org/W1936750108",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "In this paper, we introduce DropHead, a structured dropout method specifically designed for regularizing the multi-head attention mechanism, which is a key component of transformer, a state-of-the-art model for various NLP tasks. In contrast to the conventional dropout mechanisms which randomly drop units or connections, the proposed DropHead is a structured dropout method. It drops entire attention-heads during training and It prevents the multi-head attention model from being dominated by a small portion of attention heads while also reduces the risk of overfitting the training data, thus making use of the multi-head attention mechanism more efficiently. Motivated by recent studies about the learning dynamic of the multi-head attention mechanism, we propose a specific dropout rate schedule to adaptively adjust the dropout rate of DropHead and achieve better regularization effect. Experimental results on both machine translation and text classification benchmark datasets demonstrate the effectiveness of the proposed approach.",
  "full_text": "Scheduled DropHead: A Regularization Method for Transformer Models\nWangchunshu Zhou1 ∗ Tao Ge2 Ke Xu1 Furu Wei2 Ming Zhou2\n1Beihang University, Beijing, China\n2Microsoft Research Asia, Beijing, China\nzhouwangchunshu@buaa.edu.cn, kexu@nlsde.buaa.edu.cn\n{tage, fuwei, mingzhou}@microsoft.com\nAbstract\nWe introduce DropHead, a structured dropout\nmethod speciﬁcally designed for regularizing\nthe multi-head attention mechanism which is\na key component of transformer. In contrast\nto the conventional dropout mechanism which\nrandomly drops units or connections, Drop-\nHead drops entire attention heads during train-\ning to prevent the multi-head attention model\nfrom being dominated by a small portion of\nattention heads. It can help reduce the risk\nof overﬁtting and allow the models to better\nbeneﬁt from the multi-head attention. Given\nthe interaction between multi-headedness and\ntraining dynamics, we further propose a novel\ndropout rate scheduler to adjust the dropout\nrate of DropHead throughout training, which\nresults in a better regularization effect. Exper-\nimental results demonstrate that our proposed\napproach can improve transformer models by\n0.9 BLEU score on WMT14 En-De translation\ntask and around 1.0 accuracy for various text\nclassiﬁcation tasks.\n1 Introduction\nThe transformer (Vaswani et al., 2017) archi-\ntecture has shown state-of-the-art performance\nacross a variety of Natural Language Processing\n(NLP) tasks. One key architectural innovation\nof the transformer model is the multi-head atten-\ntion mechanism (MHA) where attention is com-\nputed independently by multiple parallel attention\nheads. Intuitively, the MHA mechanism allows\nthe model to jointly attend to information from\ndifferent representation subspaces at different po-\nsitions. In practice, however, some recent studies\n(V oita et al., 2019; Michel et al., 2019) show that\nin most cases, the MHA mechanism is dominated\nby only a small portion of attention heads which\n∗This work was done during the ﬁrst author’s internship\nat Microsoft Research Asia.\nhead 1 head 2 head 3 head H\n…\n…\nhead 1 head 2 head 3 head H\n(a) Standard Dropout\n(b) DropHead\nFigure 1: Illustration of DropHead : (a): standard\nDropout randomly drops units in each attention heads.\n(b) DropHead drops entire attention heads.\ncontributes most to the output. The other attention\nheads can even be removed at test time without\nsigniﬁcantly impacting the performance, indicat-\ning that the MHA mechanism is far from being\nfully exploited.\nTo address the limitation, we propose Drop-\nHead, a structured dropout mechanism speciﬁ-\ncally designed to regularize the MHA mechanism\nin transformer models. Different from conven-\ntional dropout approaches (Srivastava et al., 2014)\nthat randomly dropout activations or connections,\nDropHead drops an entire attention head with a\npre-deﬁned probability, which is illustrated in Fig-\nure 1. Under the DropHead mechanism, any at-\ntention head is possible to be dropped out dur-\ning training, which requires every attention head\nto work hard so that it can ensure the model to\nfunction well even if other heads are dropped out.\nAs a result, DropHead can prevent the MHA from\nbeing dominated by one or a few attention heads\nand encourage more attention heads to encode\narXiv:2004.13342v2  [cs.CL]  1 Nov 2020\nuseful information. Also, it helps MHA mech-\nanism generalize better by breaking up excessive\nco-adaptation between multiple attention heads.\nMoreover, considering the training dynam-\nics (Michel et al., 2019) of the MHA where\nthe aforementioned head domination phenomenon\ntends to happen at the very beginning of the train-\ning process, we propose a speciﬁc dropout rate\nscheduler for the DropHead mechanism, which\nlooks like a V-shaped curve: it applies a relatively\nhigh dropout rate for the beginning and ending\nepochs during training to prevent the MHA mech-\nanism from being dominated by a few attentions\nheads and reduce overﬁtting, while uses a lower\ndropout rate in the remaining steps to allow the\nmodel to better ﬁt the training data.\nWe conduct experiments on both neural ma-\nchine translation and text classiﬁcation benchmark\ndatasets. Experimental results show that the pro-\nposed Scheduled DropHead yields consistent and\nsubstantial improvements over the standard trans-\nformer model (0.9 BLEU score on WMT2014 En-\nDe translation dataset and around 1.0 accuracy\non various text classiﬁcation datasets) without any\nother modiﬁcation in the model architecture. In\naddition, we also ﬁnd that Scheduled DropHead\nenables a trained transformer model to better re-\ntain its performance after pruning out a relatively\nlarge number of attention heads.\nOur contribution can be summarized as follows:\n• We propose DropHead, a simple and ver-\nsatile regularization method for transformer\nmodels. It helps avoid the head domina-\ntion problem, which enables more attention\nheads to encode useful information and pre-\nvents excessive co-adaptation between atten-\ntion heads, improving the generalization abil-\nity of the model.\n• We introduce a dropout scheduler for\nDropHead, which adjusts the dropout rate\nthroughout the training process to strengthen\nthe effect of DropHead, which further im-\nproves the performance.\n2 Background\nIn this section, we brieﬂy recall the important\nbackground notations including attention mech-\nanism (Bahdanau et al., 2014), multi-head at-\ntention mechanism (Vaswani et al., 2017), and\ndropout (Srivastava et al., 2014).\nAttention Mechanism An attention function\ncan be described as mapping a query and a set\nof key-value pairs to an output, where the query,\nkeys, values, and output are all vectors. The out-\nput is computed as a weighted sum of the val-\nues, where the weight assigned to each value is\ncomputed by a compatibility function of the query\nwith the corresponding key. We focus on the dot-\nproduct attention (Luong et al., 2015) which is em-\nployed in the transformer architecture.\nGiven a sequence of vectors H ∈Rl×d, where\nland drepresent the length of and the dimension\nof the input sequence, the self-attention projects\nH into three different matrices: the query matrix\nQ, the key matrix K and the value matrix vector\nV, and uses scaled dot-product attention to get the\noutput representation:\nQ,K,V = HWQ,HW K,HW V\nAttn(Q,K,V ) = softmax\n(\nQKT\n√dk\n)\nV (1)\nwhere WQ,WK,WV are learnable parameters\nand softmax() is performed row-wise.\nMulti-Head Attention To enhance the ability\nof self-attention, multi-head self-attention is in-\ntroduced as an extension of the single head self-\nattention, which jointly model the multiple inter-\nactions from different representation spaces:\nMHA(H) = [head1; ... ; headH] WO,\nwhere head i = Attn\n(\nHWQ\ni ,HW K\ni ,HW V\ni\n)\n(2)\nwhere WO,WQ\ni ,WK\ni ,WV are learnable param-\neters. The transformer architecture is a stack\nof several multi-head self-attention layers and\nfully-connected layers. The multi-head attention\nmechanism extends the standard attention mech-\nanism (Bahdanau et al., 2014) by computing at-\ntention with H independent attention heads, thus\nallows the model to jointly attend to information\nfrom different representation subspaces at differ-\nent positions.\nDropout Since introduced by Srivastava et al.\n(2014), dropout is extensively used in training\ndeep neural networks. The method randomly sup-\npresses neurons during training by setting it to 0\nwith probability p, which is called the dropout\nrate. The resulting model at test time is often in-\nterpreted as an average of multiple models dur-\ning training, and it is argued that this can reduce\noverﬁtting and improve test performance. Dropout\nachieves great success on fully-connected layers.\nHowever, vanilla dropout fails to improve the per-\nformance of convolutional neural networks or re-\ncurrent neural networks and is only applied in\nthe feed-forward layers in the transformer model,\nwhich motivates our proposed method.\n3 Scheduled DropHead\nIn this section, we begin by introducing the pro-\nposed DropHead mechanism in detail and discuss\nits potential effects on training transformer mod-\nels. Afterwards, we describe the proposed dropout\nrate scheduler speciﬁcally designed for DropHead.\n3.1 DropHead\nDropHead is a simple regularization method simi-\nlar to Dropout (Srivastava et al., 2014) and Drop-\nConnect (Wan et al., 2013). The main difference\nbetween DropHead and conventional Dropout is\nthat we drop an entire attention head with a pre-\ndeﬁned probability in DropHead, instead of drop-\nping individual units in conventional Dropout.\nFormally, during training, for a transformer\nlayer with H attention heads, we randomly sam-\nple a binary mask vector ξ ∈ {0,1}H with\neach element independently sampled from ξi ∼\nBernoulli(p) where p is the dropout rate. After-\nward, we create Hattention head masks M, which\nis of the same shape with the output of one atten-\ntion head, by ﬁlling all its elements with the corre-\nsponding ξi. We then apply the mask vectors Mi to\nthe corresponding output of attention heads head i\nby element-wise multiplication. Finally, we nor-\nmalize the output of MHA to ensure the scale of\nthe output representation matches between train-\ning and inference. The output of MHA mechanism\nequipped with DropHead can be formulated as:\nMHA(H) = ([head1 ⊙ Mi; ... ; headH ⊙ MH] WO)/γ,\nwhere headi = Attn\n(\nHWQ\ni ,HW K\ni ,HW V\ni\n)\n;\nγ = sum(ξ)/H\n(3)\nwhere γ is the rescale factor used for normaliza-\ntion and ⊙denotes element-wise product. The\nprocedure of DropHead is summarized in Algo-\nrithm 1.\nEffect of DropHead Applying DropHead dur-\ning training forces the MHA mechanism to opti-\nmize the same objective with different randomly\nselected subsets of its attention heads. Its potential\neffects can be summarized as follows: It prevents\nAlgorithm 1 DropHead\nRequire: Dropout rate p; outputs from H atten-\ntion heads O= [head1; ... ; headH]; mode\n1: if mode == inference then\n2: return O\n3: end if\n4: Randomly sample ξ: ξi ∼Bernoulli(p).\n5: For each element ξi, create a mask vector M i\nof the same shape as head i with all elements\nequal to ξi.\n6: Apply the mask: O= O×M\n7: Normalization: O= O×H/count ones(ξ)\nthe MHA from being dominated by few atten-\ntion heads and reduces excessive co-adaptation be-\ntween different attention heads, which helps MHA\nexploit its multi-headness more effectively and re-\nduces the risk of overﬁtting, thus improving the\ngeneralization ability of trained models.\n3.2 Dropout rate scheduler\nMotivated by Curriculum Dropout (Morerio\net al., 2017) and other structured dropout ap-\nproaches (Ghiasi et al., 2018; Zoph et al., 2018),\nwe propose to apply a dropout rate scheduler to ad-\njust the effect of DropHead throughtout the train-\ning process. Different from the previous sched-\nuled dropout approaches that generally initialize\nthe training procedure with a dropout rate of 0\nand gradually increase the dropout rate for allevi-\nating the excessive co-adaptation in the late train-\ning steps, we propose a novel V-shaped dropout\nrate scheduler for DropHead, as shown in Fig-\nure 2 which additionally applies high dropout rate\nat the early stage of training. The motivation of\nhigh dropout rate at the beginning of training is\nto prevent the MHA from being dominated by\nfew attention heads, which proves to happen at\nthe very early stage of training, as the previous\nstudy (Michel et al., 2019) shows.\nSpeciﬁcally, we start the training process with a\nrelatively high dropout rate of pstart and linearly\ndecrease it to 0 during the early stage of training,\nwhich is empirically chosen to be the same train-\ning steps for learning rate warmup. Afterwards,\nwe linearly increase the dropout rate to pend in\nthe remaining training steps following existing\ndropout rate schedulers in DropBlock and Sched-\nuledDropPath. To avoid introducing additional hy-\nperparameters, we empirically set pstart = pend\nand ﬁnd it generally works well across different\nFigure 2: Illustration of the proposed dropout rate\nschedule, the curriculum dropout rate schedule, and the\nanti-curriculum dropout rate schedule.\ntasks.\n4 Experiments\nIn this section, we ﬁrst conduct experiments on\nseveral benchmark datasets on machine translation\nand text classiﬁcation to demonstrate the effective-\nness of Scheduled DropHead. Afterward, we con-\nduct experiments on pruning transformer models\ntrained with DropHead to test if it succeeds in pre-\nventing dominating attention heads and improving\nattention head pruning.\n4.1 Machine Translation\nWe ﬁrst conduct experiments on the machine\ntranslation task, which is one of the primary ap-\nplications of the transformer model.\nDataset Following Vaswani et al. (2017), we use\nthe WMT 2014 English-German dataset consist-\ning of about 4.5 million sentence pairs as the train-\ning set. The newstest2013 and newstest2014 are\nused as the development set and the test set. We\napply BPE (Sennrich et al., 2015) with 32K merge\noperations to obtain subword units.\nModels We train the transformer-big (Vaswani\net al., 2017) model with the proposed Scheduled\nDropHead method as well as its counterpart where\nno dropout rate scheduler is employed. To ensure\nfair comparisons, we compare our models against\nthe result reported by Vaswani et al. (2017) and\nour reproduced result. In addition, as our approach\nis expected to be able to reduce excessive co-\nadaptation between multiple attention heads, we\nalso experiment with a variant of the transformer\nmodel where we increase the number of attention\nheads in each layer from 16 to 32. We denote the\nmodiﬁed transformer model as transformer-big\n(more heads). Note that we change the number\nof transformer heads by reducing the hidden di-\nmension size of each attention heads, thus do not\nincrease the number of total parameters in the re-\nsulting model. For reference, we also compare our\nmodel against that trained by applying the vanilla\ndropout on self-attention layers in the transformer\nmodel, as well as several recent techniques im-\nproving the transformer model in Table 1 to futher\ndemonstrate the effectiveness of our approach.\nTraining We generally follow the hyperparame-\nter setting provided in (Vaswani et al., 2017). We\nset the dropout rate for DropHead to 0.2 and de-\ncrease the dropout rate in feed-forward layers from\n0.3 to 0.2 based on dev set performance, which is\nconsistent with the fact that DropHead mechanism\nalready provides some regularization effects.\nEvaluation Following previous work, we aver-\nage the last 10 checkpoints for evaluation. All re-\nsults use beam search with a beam width of 4 and\nlength penalty of 0.6.\nModels BLEU PPL\nWeighted Transformer(Ahmed et al., 2017) 28.9 -\nTied Transformer(Xia et al., 2019) 29.0 -\nLayer-wise Coordination(He et al., 2018) 29.1 -\nTransformer-big 28.4 -\n- ours (reproduced) 28.7 4.32\n- + Attention dropout 28.7 4.29\n- + DropHead 29.2 4.15\n- + Scheduled DropHead 29.4 4.08\nTransformer-big (more heads) 28.4 4.39\n- + Attention dropout 28.5 4.38\n- + DropHead 29.3 4.12\n- + Scheduled DropHead 29.6* 4.02\nTable 1: Machine translation performance on WMT14\nen-de and newstest2014 of compared models. The re-\nsults in the ﬁrst section are the results reported on the\ncorresponding publications. Other results are reported\nas the median of 5 random runs. * denotes statistically\nsigniﬁcant with p< 0.05 compared with all baselines.\nResults Table 1 shows the BLEU score and the\nperplexity of different compared models. We ﬁnd\nthat applying vanilla dropout on self-attention lay-\ners generally fails to improve the model’s perfor-\nmance. In contrast, the DropHead mechanism is\nable to improve the transformer model by around\n0.5 BLEU score and 0.2 perplexity, demonstrat-\ning its ability to improve the model’s generaliza-\ntion ability. In addition, the proposed dropout rate\nscheduler can further improve the performance of\nthe strong baseline of Transformer + DropHead by\n0.2 BLEU score, which conﬁrms its effectiveness.\nWe also ﬁnd that simply increasing the num-\nber of attention heads in each layer yields slightly\nworse result compared with the default model\nconﬁguration. However, training the transformer\nmodel with more attention heads with Scheduled\nDropHead can yield further improvement com-\npared with the original model architecture. This\nsuggests that the proposed method is able to pro-\nvide good regularization effect for the MHA in\ntransformer. Our approach also substantially out-\nperforms several existing techniques on improving\nthe transformer model that do not involve large\nmodiﬁcation of the model architecture and addi-\ntional training data, which further demonstrates its\neffectiveness.\n4.2 Text Classiﬁcation\nWe also conduct experiments on sentence classi-\nﬁcation tasks to further demonstrate the effective-\nness of the proposed methods.\nDatasets Type #Classes #Documents\nIMDB Sentiment 2 50000\nYelp Review Sentiment 2 598000\nAG’s News Topic 4 127,600\nDBPedia Topic 14 630,000\nTREC Question 6 5,952\nYahoo! AnswersQuestion 10 1,146,000\nSNLI Inference 3 550,152\nTable 2: Statistics for text classiﬁcation datasets.\nDatasets We conduct experiments on text clas-\nsiﬁcation datasets ranging from different tasks.\nStatistics of datasets are listed in Table 2. All\ndatasets are split into training, development and\ntesting sets following previous work. For sen-\ntiment analysis, we use the binary ﬁlm review\nIMDB dataset (Maas et al., 2011) and the bi-\nnary version of the Yelp Review dataset built\nby (Zhang et al., 2015). For topic classiﬁcation\ntask, we employ AG’s News and DBPedia cre-\nated by (Zhang et al., 2015). For question classi-\nﬁcation task, we evaluate our method on the six-\nclass version of the TREC dataset (V oorhees and\nTice, 1999) and Yahoo! Answers dataset created\nby (Zhang et al., 2015). We also evaluate on the\nSNLI dataset (Bowman et al., 2015) which is a\ncollection of sentence pairs labeled for entailment,\ncontradiction, and semantic independence.\nModels We employ BERT-base (Devlin et al.,\n2018) as a strong baseline and compare the perfor-\nmance of ﬁne-tuning it without attention dropout,\nwith vanilla attention dropout, and with Drop-\nHead/Scheduled DropHead. However, the domi-\nnant attention heads in the BERT model may al-\nready be formed during pretraining, which may\nhinder the effect of DropHead, and pretraining\nBERT with our approach is beyond the scope of\nour paper. Therefore, we also employ a smaller,\nrandomly initialized (i.e. without pretraining)\ntransformer-based text classiﬁcation model as our\nbaseline model. The model has a hidden size of\n768, 6 transformer blocks and 12 self-attention\nheads. Following (Sun et al., 2019), we train the\nmodels with a batch size of 24, a dropout rate of\n0.1 for both vanilla Dropout and DropHead. We\nemploy the Adam optimizer with a base learning\nrate as 2e-5 or 1e-4 with a warm-up ratio of 0.1 for\nthe models with and without pretraining respec-\ntively. We train the models for 10 epochs and se-\nlect the best model according to the accuracy on\nthe held out dev set.\nResults We report the accuracy of compared\nmodels on the test set in Table 3. We can see that\napplying Scheduled DropHead can signiﬁcantly\n(with p < 0.05) improve the performance of\nvanilla transformer-based text classiﬁcation mod-\nels, as well as that applying vanilla dropout on the\nMHA mechanism of transformer models, on 6 out\nof 7 datasets. This demonstrates that our proposed\napproach works well for natural language under-\nstanding tasks. Scheduled DropHead can also\nconsistently improve the performance of a very\ncompetitive BERT-based model with an arguably\nsigniﬁcance (with p < 0.1) for most datasets.\nThis further conﬁrms the effectiveness of our ap-\nproach. The performance improvement yielded by\nthe proposed approach is much larger when apply-\ning on the vanilla transformer-based text classiﬁ-\ncation model without pretraining. We suspect this\nmay be because the BERT model is already ap-\nproaching the state-of-the-art performance on each\ndatasets and the dominant attention heads have al-\nready been formed in the pretrained BERT model\nduring its pretraining procedure, which reduces\nthe improvement yielded by Scheduled DropHead.\nPretraining BERT models with DropHead may al-\nleviate this problem, we leave this for the fu-\nture work. In addition, we ﬁnd that the Sched-\nuled DropHead outperforms its counterpart where\nModel IMDB Yelp AG DBPedia TREC Yahoo! SNLI\nChar-level CNN(Zhang et al., 2015) - 95.12 90.49 98.45 - 71.20 -\nVDCNN(Conneau et al., 2017) - 95.72 91.33 98.71 - 73.43 -\nDPCNN(Johnson and Zhang, 2017) - 97.36 93.13 99.12 - 76.10 -\nULMFiT(Howard and Ruder, 2018) 95.40 97.84 94.99 99.20 96.40 - -\nBERT-base(Devlin et al., 2018) 94.60 97.61 94.75 99.30 97.20 77.58 90.73\n-Attention Dropout 94.51 97.64 94.65 99.28 97.20 77.64 90.65\n-DropHead 95.22 97.77 94.90 99.35 97.60 78.05 90.85\n-Scheduled DropHead 95.48* 97.92 94.95 99.41 97.80 77.93 90.92\nTransformer (w/o pretraining) 90.85 95.61 91.08 98.69 94.60 73.15 86.52\n-Attention Dropout 90.91 95.56 91.12 98.62 94.40 73.31 86.57\n-DropHead 91.87 96.14 91.68 98.85 95.20 74.38 87.14\n-Scheduled DropHead 91.95* 96.21* 91.74* 98.91 95.60* 74.41* 87.20*\nTable 3: Test set results (accuracy) of compared models on text classiﬁcation benchmarks. The results in the\nﬁrst section are the results reported on the corresponding publications. Other results are reported as the median\nof 5 random runs. ‘–’ means not reported results. * and means statistically signiﬁcant improvement upon the\ncorresponding baseline with p< 0.05 and p< 0.1.\na constant dropout rate is applied for DropHead.\nThis conﬁrms the effectiveness of the proposed\ndropout rate schedule.\n4.3 Analysis\nIn this section, we perform experiments following\nthat in (Michel et al., 2019) to analyze the relative\nimportance of individual attention heads in trans-\nformer model trained on WMT14 en-de dataset\nwith and without the proposed Scheduled Drop-\nHead mechanism, and try to prune attention heads\nto improve its efﬁciency.\nModel Enc-Enc Enc-Dec Dec-Dec\nTransformer -0.47 -1.05 -0.32\n-DropHead -0.31 -0.79 -0.27\n-Scheduled DropHead -0.28 -0.70 -0.25\nTransformer (more heads)-0.41 -0.92 -0.29\n-DropHead -0.25 -0.61 -0.24\n-Scheduled DropHead -0.21 -0.54 -0.20\nTable 4: Average variation of the BLEU score after re-\nmoving the dominant attention head in different mod-\nels.\nDominant head analysis The aforementioned\nexperiments on machine translation and text clas-\nsiﬁcation datasets demonstrates that our proposed\nmethod is able to improve the performance of\ntransformer models. However, as the performance\nimprovement may exclusively come from the bet-\nter regularization effect yielded by DropHead,\nit’s still unclear whether the DropHead mecha-\nnism can prevent the MHA mechanism from being\ndominated by a few attention heads.\nTherefore, we conduct an experiment to ana-\nlyze the inﬂuence of the dominant head in each\ntransformer layer on the ﬁnal performance of the\ntransformer model. Speciﬁcally, we evaluate the\nmodel’s performance by masking one head at each\ntime and the dominant head is then determined\nas the one without which the performance of the\nmodel degrades the most. We measure the aver-\nage BLEU score variation of the model’s perfor-\nmance after removing the dominant attention head\nin each layer. Following Michel et al. (2019), we\nreport the results when removing encoder-encoder\nattention heads, decoder-decoder attention heads,\nand encoder-decoder attention heads separately as\nthey may behave differently.\nThe results are presented in Table 4. We ﬁnd\nthat the performance degradation when removing\nthe dominant head is signiﬁcantly reduced when\ntrained with Scheduled DropHead. This suggests\nthat our proposed method can substantially re-\nduce the inﬂuence of dominant attention heads\nand exploit multiple attention heads more effec-\ntively. This explains the performance improve-\nment yielded by Scheduled DropHead. In addi-\ntion, we ﬁnd that training with Scheduled Drop-\nHead can reduce the inﬂuence of the dominant\nheads in the transformer model with more atten-\ntion heads more effectively, which may explain the\nimproved performance with the transformer model\nwith more attention heads trained with DropHead.\nFigure 3: Evolution of BLEU score when attention\nheads are pruned from different variants of transformer\nmodels trained on WMT14 en-de dataset.\nAttention head pruning Similar to other struc-\ntured dropout methods (Fan et al., 2019; Ghi-\nasi et al., 2018), training with Scheduled Drop-\nHead makes the MHA module more robust to per-\nform the task with missing attention heads. This\nmay make it easier to prune attention heads in\nthe trained transformer model at test time to re-\nduce the number of parameters and improve the\nmodel’s efﬁciency during inference. To validate\nthis assumption, we perform attention head prun-\ning with estimated head importance scoreIn as de-\nscribed in (Michel et al., 2019). Speciﬁcally, we\nprune different portion of the total number of at-\ntention heads in the transformer-big model trained\nwith and without the DropHead mechanism and\nthe proposed dropout rate schedule, and report the\nevolution of BLEU score in Table 3. We can\nsee that both the DropHead mechanism and the\ndropout rate scheduler can increase the model’s\nrobustness to attention head pruning. This allows\nus to prune more attention heads in a transformer\nmodel to reduce the parameters in the model and\nimprove the inference efﬁciency without signiﬁ-\ncantly sacriﬁcing the model’s performance.\n4.4 Ablation study\nWe also conduct an ablation study to better un-\nderstand DropHead and the dropout rate sched-\nule. We analyze the relationship between stan-\ndard Dropout and Scheduled DropHead; the ef-\nfect of Scheduled DropHead on different types of\nattention heads; and the effect of different vari-\nants of dropout rate schedules. The experiments\nare conducted on the machine translation task with\nWMT14 en-de dataset.\nAttention Dropout vs Scheduled DropHead\nTo analyze the relationship between Scheduled\nDropHead and the standard Dropout applied to\nthe self-attention layers in the transformer model,\nwe present the performance of transformer mod-\nels trained by applying standard Dropout, Sched-\nuled DropHead, and the combination of them\non the MHA modules in Table 5. We can see\nthat applying standard dropout on the MHA mod-\nules only marginally improve the model’s perfor-\nmance. In contrast, the proposed Scheduled Drop-\nHead can yield signiﬁcant improvement upon the\nbaseline model and the model trained with stan-\ndard dropout. We also ﬁnd that the combination of\nthe standard dropout and DropHead does not yield\nfurther improvement. This suggests that Sched-\nuled DropHead is more effective than the standard\nDropout for the MHA mechanism.\nModel BLEU\nTransformer 28.7\n-+ Attention Dropout 28.7\n-+ Scheduled DropHead 29.4\n-+ Combination 29.3\nTable 5: BLEU score of transformer-big models trained\nwith different regularization methods.\nInﬂuence on different types of attention heads\nWe also analyze the effect of Scheduled Drop-\nHead on different types of attention heads. Specif-\nically, we apply Scheduled DropHead separately\non encoder-encoder attention, encoder-decoder at-\ntention, and decoder-decoder attention modules.\nThe results are presented in Table 6. We can\nsee that the proposed approach is more effective\nwhen used on encoder-encoder self-attention and\nencoder-decoder attention modules, which is con-\nsistent with the ﬁnding of (Michel et al., 2019).\nComparison between different dropout rate\nschedules We then analyze the inﬂuence of dif-\nferent dropout rate schedules by comparing the\nproposed dropout schedule with two variants. As\nillustrated in Figure 2, the ﬁrst variant (curricu-\nlum dropout schdule) is that used in Scheduled-\nDropPath (Zoph et al., 2018) which linearly in-\ncreases the dropout rate from 0 to the pre-deﬁned\ndropout rate. The second (anti-curriculum dropout\nschdule) is the opposite of the former schedule\nwhich linearly decreases the dropout rate from\nModel BLEU\nTransformer\n-w/o Scheduled DropHead 28.6\n-Enc-enc Scheduled DropHead 28.9\n-Enc-dec Scheduled DropHead 29.1\n-Dec-dec Scheduled DropHead 28.7\n-Scheduled DropHead 29.4\nTable 6: BLEU score of transformer-big models trained\nwith Scheduled DropHead applied on different MHA\nmodules.\nModel BLEU\nTransformer\n-DropHead (constant dropout rate) 29.2\n-Curriculum DropHead 29.0\n-Anti-curriculum DropHead 28.7\n-Scheduled DropHead 29.4\nTable 7: BLEU score of transformer-big models trained\nwith Scheduled DropHead with different dropout rate\nschedules.\nthe pre-deﬁned dropout rate to 0. The result is\nshown in Table 7. We can see that both curricu-\nlum dropout rate scheduler and anti-curriculum\ndropout schedule fails to improve the performance\nof the vanilla DropHead where the dropout rate is\nconstant, which demonstrates the effectiveness of\nthe proposed dropout rate schedule.\n5 Related work\nSince its introduction, dropout (Srivastava et al.,\n2014) has inspired a number of regularization\nmethods for neural networks, including Drop-\nConnect (Wan et al., 2013), Maxout (Goodfel-\nlow et al., 2013), Spatial Dropout (Tompson\net al., 2015), StochasticDepth (Huang et al.,\n2016), DropPath (Larsson et al., 2016), Drop-\nBlock (Ghiasi et al., 2018), LayerDrop (Fan et al.,\n2019), etc. While standard dropout works well\nfor fully-connected neural networks, structured\ndropout (Huang et al., 2016; Larsson et al., 2016;\nGhiasi et al., 2018) that randomly drops an en-\ntire channel or input block appears to work bet-\nter in convolutional neural networks. For RNNs,\nVariational Dropout (Gal and Ghahramani, 2016),\nZoneOut (Krueger et al., 2016), and Word Em-\nbedding Dropout in both word-level (Gal and\nGhahramani, 2016) and embedding-level (Zhou\net al., 2019) are the most widely used methods.\nMore recently, there exists work investigates em-\nploying standard Dropout on attention layers (Ze-\nhui et al., 2019), or applying StochasticDepth\non transformer layers (Fan et al., 2019). How-\never, to the best of our knowledge, regularization\napproaches speciﬁcally designed for the MHA\nmechasim in the transformer architecture are cur-\nrently under-explored.\nDropHead is also closely related to Spatial\nDropout (Tompson et al., 2015), which is a struc-\ntured dropout method for convolutional neural net-\nworks that randomly drops an entire channel from\na feature map. The proposed dropout rate sched-\nuler is related to the Curriculum Dropout and the\ndropout rate scheduler in ScheduledDropPath and\nDropBlock. Finally, our work is also inspired by\nrecent work analyzing the role of one attention\nhead in the MHA mechanism (Michel et al., 2019;\nV oita et al., 2019) which found that most atten-\ntion heads in MHA fail to contribute to the perfor-\nmance of the transformer model and can be effec-\ntively pruned at test time.\n6 Conclusion\nIn this work, we introduce Scheduled DropHead, a\nsimple regularization approach for training trans-\nformer models. DropHead is a form of struc-\ntured dropout that drops an entire attention head\nin the multi-head attention mechanism, which pre-\nvents MHA from being dominated by a few heads,\nreduces excessive co-adaptation between atten-\ntion heads, and facilitates attention head pruning.\nWe also propose a speciﬁc dropout rate sched-\nuler that is motivated by the training dynamic of\nthe MHA mechanism to improve the performance\nof the vanilla DropHead. Our experiments on\nmachine translation and text classiﬁcation bench-\nmarks demonstrate that the DropHead mechanism\nand the dropout rate scheduler can effectively im-\nprove the performance of a competitive trans-\nformer model. We also ﬁnd that the proposed ap-\nproach can reduce the inﬂuence of the dominant\nattention head and improve the model’s robustness\nto attention head pruning.\nFor future work, we plan to apply DropHead on\npretraining language models such as BERT as well\nas other natural language generation tasks such as\ntext summarization and dialogue systems.\nAcknowledgments\nWe thank the anonymous reviewers for their valu-\nable comments.\nReferences\nKarim Ahmed, Nitish Shirish Keskar, and Richard\nSocher. 2017. Weighted transformer net-\nwork for machine translation. arXiv preprint\narXiv:1711.02132.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\narXiv preprint arXiv:1508.05326.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loic\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. arXiv preprint\narXiv:1705.02364.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAngela Fan, Edouard Grave, and Armand Joulin. 2019.\nReducing transformer depth on demand with struc-\ntured dropout. arXiv preprint arXiv:1909.11556.\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Advances in neural information\nprocessing systems, pages 1019–1027.\nGolnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. 2018.\nDropblock: A regularization method for convolu-\ntional networks. In Advances in Neural Information\nProcessing Systems, pages 10727–10737.\nIan J Goodfellow, David Warde-Farley, Mehdi Mirza,\nAaron Courville, and Yoshua Bengio. 2013. Maxout\nnetworks. arXiv preprint arXiv:1302.4389.\nTianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo\nChen, and Tie-Yan Liu. 2018. Layer-wise coordi-\nnation between encoder and decoder for neural ma-\nchine translation. In Advances in Neural Informa-\ntion Processing Systems, pages 7944–7954.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146.\nGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and\nKilian Q Weinberger. 2016. Deep networks with\nstochastic depth. In European conference on com-\nputer vision, pages 646–661. Springer.\nRie Johnson and Tong Zhang. 2017. Deep pyramid\nconvolutional neural networks for text categoriza-\ntion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 562–570.\nDavid Krueger, Tegan Maharaj, J ´anos Kram ´ar, Mo-\nhammad Pezeshki, Nicolas Ballas, Nan Rose-\nmary Ke, Anirudh Goyal, Yoshua Bengio, Aaron\nCourville, and Chris Pal. 2016. Zoneout: Regulariz-\ning rnns by randomly preserving hidden activations.\narXiv preprint arXiv:1606.01305.\nGustav Larsson, Michael Maire, and Gregory\nShakhnarovich. 2016. Fractalnet: Ultra-deep\nneural networks without residuals. arXiv preprint\narXiv:1605.07648.\nMinh-Thang Luong, Hieu Pham, and Christopher D\nManning. 2015. Effective approaches to attention-\nbased neural machine translation. arXiv preprint\narXiv:1508.04025.\nAndrew L Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nProceedings of the 49th annual meeting of the as-\nsociation for computational linguistics: Human lan-\nguage technologies-volume 1, pages 142–150. Asso-\nciation for Computational Linguistics.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? Advances\nin neural information processing systems (To ap-\npear).\nPietro Morerio, Jacopo Cavazza, Riccardo V olpi, Ren´e\nVidal, and Vittorio Murino. 2017. Curriculum\ndropout. In Proceedings of the IEEE International\nConference on Computer Vision, pages 3544–3552.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. The journal of machine learning\nresearch, 15(1):1929–1958.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to ﬁne-tune bert for text classiﬁcation?\nIn China National Conference on Chinese Compu-\ntational Linguistics, pages 194–206. Springer.\nJonathan Tompson, Ross Goroshin, Arjun Jain, Yann\nLeCun, and Christoph Bregler. 2015. Efﬁcient ob-\nject localization using convolutional networks. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 648–656.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics.\nEllen M V oorhees and Dawn M Tice. 1999. The trec-8\nquestion answering track evaluation. In TREC, vol-\nume 1999, page 82. Citeseer.\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun,\nand Rob Fergus. 2013. Regularization of neural net-\nworks using dropconnect. In International confer-\nence on machine learning, pages 1058–1066.\nYingce Xia, Tianyu He, Xu Tan, Fei Tian, Di He, and\nTao Qin. 2019. Tied transformers: Neural machine\ntranslation with shared encoder and decoder. InPro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, volume 33, pages 5466–5473.\nLin Zehui, Pengfei Liu, Luyao Huang, Jie Fu, Junkun\nChen, Xipeng Qiu, and Xuanjing Huang. 2019.\nDropattention: A regularization method for fully-\nconnected self-attention networks. arXiv preprint\narXiv:1907.11065.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Advances in neural information pro-\ncessing systems, pages 649–657.\nWangchunshu Zhou, Tao Ge, Ke Xu, Furu Wei, and\nMing Zhou. 2019. Bert-based lexical substitution.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n3368–3373.\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, and\nQuoc V Le. 2018. Learning transferable architec-\ntures for scalable image recognition. In Proceedings\nof the IEEE conference on computer vision and pat-\ntern recognition, pages 8697–8710.",
  "topic": "Overfitting",
  "concepts": [
    {
      "name": "Overfitting",
      "score": 0.7599989175796509
    },
    {
      "name": "Computer science",
      "score": 0.7339929342269897
    },
    {
      "name": "Dropout (neural networks)",
      "score": 0.7031881213188171
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.6913095116615295
    },
    {
      "name": "Transformer",
      "score": 0.670280396938324
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5787023305892944
    },
    {
      "name": "Machine learning",
      "score": 0.4687851667404175
    },
    {
      "name": "Artificial neural network",
      "score": 0.15903431177139282
    },
    {
      "name": "Voltage",
      "score": 0.13420939445495605
    },
    {
      "name": "Engineering",
      "score": 0.10486534237861633
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 6
}