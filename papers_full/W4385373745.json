{
    "title": "Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data",
    "url": "https://openalex.org/W4385373745",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2104531663",
            "name": "Xu, Xuhai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221335722",
            "name": "Yao, Bingsheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2745698320",
            "name": "Dong, Yuanzhe",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221594741",
            "name": "Gabriel, Saadia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1978159643",
            "name": "Yu Hong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227054199",
            "name": "Hendler, James",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221663935",
            "name": "Ghassemi, Marzyeh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227028013",
            "name": "Dey, Anind K.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221335726",
            "name": "Wang, Dakuo",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1537829113",
        "https://openalex.org/W4285209895",
        "https://openalex.org/W4281483047",
        "https://openalex.org/W4379769651",
        "https://openalex.org/W4360765018",
        "https://openalex.org/W2000594564",
        "https://openalex.org/W2897583329",
        "https://openalex.org/W4366591012",
        "https://openalex.org/W4308610353",
        "https://openalex.org/W3178912721",
        "https://openalex.org/W3148026034",
        "https://openalex.org/W4285211483",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4385442373",
        "https://openalex.org/W4386249236",
        "https://openalex.org/W4377009978",
        "https://openalex.org/W3192247156",
        "https://openalex.org/W4381930847",
        "https://openalex.org/W201361503",
        "https://openalex.org/W4317757464",
        "https://openalex.org/W2160367613",
        "https://openalex.org/W2489334776",
        "https://openalex.org/W4389523718",
        "https://openalex.org/W4311026115",
        "https://openalex.org/W2252128283",
        "https://openalex.org/W3004493409",
        "https://openalex.org/W4220883868",
        "https://openalex.org/W2889391310",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W2739819123",
        "https://openalex.org/W4297328641",
        "https://openalex.org/W4322708560",
        "https://openalex.org/W4389523957",
        "https://openalex.org/W4224326626",
        "https://openalex.org/W4378498682",
        "https://openalex.org/W3135440523",
        "https://openalex.org/W2998535576",
        "https://openalex.org/W3209409148",
        "https://openalex.org/W2402700",
        "https://openalex.org/W2101807845",
        "https://openalex.org/W2985355520",
        "https://openalex.org/W2094553285",
        "https://openalex.org/W2405042511",
        "https://openalex.org/W2900152803",
        "https://openalex.org/W3162081707",
        "https://openalex.org/W4360891289",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4210736086",
        "https://openalex.org/W2087564028",
        "https://openalex.org/W2537052583",
        "https://openalex.org/W2739681832",
        "https://openalex.org/W4388092464",
        "https://openalex.org/W3013908145",
        "https://openalex.org/W2937284856",
        "https://openalex.org/W4384520874",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W3103163889",
        "https://openalex.org/W2597891111",
        "https://openalex.org/W4376133361",
        "https://openalex.org/W4319837253",
        "https://openalex.org/W4363671699",
        "https://openalex.org/W2888487925",
        "https://openalex.org/W2972890723",
        "https://openalex.org/W4361230825",
        "https://openalex.org/W2911378332",
        "https://openalex.org/W2977128309",
        "https://openalex.org/W2914514892",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2109664771",
        "https://openalex.org/W4387075354",
        "https://openalex.org/W2901416577",
        "https://openalex.org/W2252031683",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3154272574",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W3031532441",
        "https://openalex.org/W4312811953",
        "https://openalex.org/W4307309259",
        "https://openalex.org/W2157035150",
        "https://openalex.org/W78677904",
        "https://openalex.org/W2890787190",
        "https://openalex.org/W4295067194",
        "https://openalex.org/W2755222014",
        "https://openalex.org/W2795743556",
        "https://openalex.org/W4226157316",
        "https://openalex.org/W4321524280",
        "https://openalex.org/W3087893815",
        "https://openalex.org/W4386352630",
        "https://openalex.org/W4320854854",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W3199761064",
        "https://openalex.org/W2076151421",
        "https://openalex.org/W2182854643",
        "https://openalex.org/W4385573087",
        "https://openalex.org/W3007384022",
        "https://openalex.org/W2741216199",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W2106686523",
        "https://openalex.org/W4296151718",
        "https://openalex.org/W3184144760",
        "https://openalex.org/W4297686483",
        "https://openalex.org/W4386415037",
        "https://openalex.org/W2104925568",
        "https://openalex.org/W2987392802",
        "https://openalex.org/W4389523706",
        "https://openalex.org/W2252191003",
        "https://openalex.org/W2898390306",
        "https://openalex.org/W2748635631",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2613843855",
        "https://openalex.org/W2927148761",
        "https://openalex.org/W2277876591",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W4323570543",
        "https://openalex.org/W2898410082",
        "https://openalex.org/W1968199606",
        "https://openalex.org/W3202915159",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4367623495",
        "https://openalex.org/W4297630849",
        "https://openalex.org/W3036236864",
        "https://openalex.org/W4360836968"
    ],
    "abstract": "Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.",
    "full_text": "32\nMental-LLM: Leveraging Large Language Models for Mental Health\nPrediction via Online Text Data\nXUHAI XU, Massachusetts Institute of Technology & University of Washington, USA\nBINGSHENG YAO,Rensselaer Polytechnic Institute, USA\nYUANZHE DONG, Stanford University, USA\nSAADIA GABRIEL, Massachusetts Institute of Technology, USA\nHONG YU, University of Massachusetts Lowell, USA\nJAMES HENDLER, Rensselaer Polytechnic Institute, USA\nMARZYEH GHASSEMI, Massachusetts Institute of Technology, USA\nANIND K. DEY, University of Washington, USA\nDAKUO WANG,Northeastern University, USA\nAdvances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant\ngap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this\nwork, we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data,\nincluding Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of experiments, covering zero-shot\nprompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of\nLLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that\ninstruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned\nmodels, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by\n10.9% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8%. They further perform on par with\nthe state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs‚Äô capability on mental\nhealth reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into\na set of action guidelines for potential methods to enhance LLMs‚Äô capability for mental health tasks. Meanwhile, we also\nemphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial\nand gender bias. We highlight the important ethical risks accompanying this line of research.\nCCS Concepts: ‚Ä¢ Human-centered computing ‚ÜíUbiquitous and mobile computing ; ‚Ä¢ Applied computing ‚ÜíLife\nand medical sciences .\nAdditional Key Words and Phrases: Mental Health, Large Language Model, Instruction Finetuning\nACM Reference Format:\nXuhai Xu, Bingsheng Yao, Yuanzhe Dong, Saadia Gabriel, Hong Yu, James Hendler, Marzyeh Ghassemi, Anind K. Dey,\nand Dakuo Wang. 2024. Mental-LLM: Leveraging Large Language Models for Mental Health Prediction via Online Text Data.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol. 8, 1, Article 32 (March 2024), 32 pages. https://doi.org/10.1145/3643540\nAuthors‚Äô addresses: Xuhai Xu, xuhaixu@uw.edu, Massachusetts Institute of Technology & University of Washington, USA; Bingsheng Yao,\nRensselaer Polytechnic Institute, USA; Yuanzhe Dong, Stanford University, USA; Saadia Gabriel, Massachusetts Institute of Technology, USA;\nHong Yu, University of Massachusetts Lowell, USA; James Hendler, Rensselaer Polytechnic Institute, USA; Marzyeh Ghassemi, Massachusetts\nInstitute of Technology, USA; Anind K. Dey, University of Washington, USA; Dakuo Wang, Northeastern University, USA.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that\ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.\nCopyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy\notherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from\npermissions@acm.org.\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n2474-9567/2024/3-ART32 $15.00\nhttps://doi.org/10.1145/3643540\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\narXiv:2307.14385v4  [cs.CL]  28 Jan 2024\n32:2 ‚Ä¢ Xu et al.\n1 INTRODUCTION\nThe recent surge of Large Language Models (LLMs), such as GPT-4 [ 18], PaLM [23], FLAN-T5 [24], and Al-\npaca [115], demonstrates the promising capability of large pre-trained models to solve various tasks in zero-shot\nsettings (i.e., tasks not encountered during training). Example tasks include question answering [87, 100], logic\nreasoning [124, 135], machine translation [15, 45], etc. A number of experiments have revealed that, built on\nhundreds of billions of parameters, these LLMs have started to show the capability to understand the human\ncommon sense beneath the natural language and do proper reasoning and inference accordingly [18, 85].\nAmong different applications, one particular question yet to be answered is how well LLMs can understand\nhuman mental health states through natural language. Mental health problems represent a significant burden for\nindividuals and societies worldwide. A recent report suggested that more than 20% of adults in the U.S. experience\nat least one mental disorder in their lifetime [9] and 5.6% have suffered from a serious psychotic disorder that\nsignificantly impairs functioning [3]. The global economy loses around $1 trillion annually in productivity due to\ndepression and anxiety alone [2].\nIn the past decade, there has been a plethora of research in natural language processing (NLP) and computational\nsocial science on detecting mental health issues via online text data such as social media content (e.g., [26, 32, 33,\n38, 47]). However, most of these studies have focused on building domain-specific machine learning (ML) models\n(i.e., one model for one particular task, such as stress detection [46, 84], depression prediction [38, 113, 127, 128],\nor suicide risk assessment [28, 35]). Even for traditional pre-trained language models such as BERT, they need to\nbe finetuned for specific downstream tasks [37, 72]. Some studies have also explored the multi-task setup [12],\nsuch as predicting depression and anxiety at the same time [ 106]. However, these models are constrained to\npredetermined task sets, offering limited flexibility. From a different aspect, another line of research has been\nexploring the application of chatbots for mental health services [20, 21, 68]. Most chatbots are simply rule-based\nand can benefit from more advanced models that empower the chatbots [4, 68]. Despite the growing research\nefforts of empowering AI for mental health, it‚Äôs important to note that existing techniques can sometimes\nintroduce bias and even provide harmful advice to users [54, 74, 116].\nSince natural language is a major component of mental health assessment and treatment [ 43, 110], LLMs\ncould be a powerful tool for understanding end-users‚Äô mental states through their written language. These\ninstruction-finetuned and general-purpose models can understand a variety of inputs and obviate the need to\ntrain multiple models for different tasks. Thus, we envision employing a single LLM for a variety of mental\nhealth-related tasks, such as multiple question-answering, reasoning, and inference. This vision opens up a wide\nrange of opportunities for UbiComp, Human-Computer Interaction (HCI), and mental health communities, such\nas online public health monitoring systems [44, 90], mental-health-aware personal chatbots [5, 36, 63], intelligent\nassistants for mental health therapists [108], online moderation tools [39], daily mental health counselors and\nsupporters [109], etc. However, there is a lack of investigation into understanding, evaluating, and improving the\ncapability of LLMs for mental-health-related tasks.\nThere are few recent studies on the evaluation of LLMs (e.g., ChatGPT) on mental-health-related tasks, most of\nwhich are in zero-shot settings with simple prompt engineering [10, 67, 132]. Researchers have shown preliminary\nresults that LLMs have the initial capability of predicting mental health disorders using natural language, with\npromising but still limited performance compared to state-of-the-art domain-specific NLP models [67, 132]. This\nremaining gap is expected since existing general-purpose LLMs are not specifically trained on mental health\ntasks. However, to achieve our vision of leveraging LLMs for mental health support and assistance, we need to\naddress the research question: How to improve LLMs‚Äô capability of mental health tasks ?\nWe conducted a series of experiments with six LLMs, including Alpaca [115] and Alpaca-LoRA (LoRA-finetuned\nLLaMA on Alpaca dataset) [51], which are representative open-source models focused on dialogue and other\ntasks; FLAN-T5 [24], a representative open-source model focused on task-solving; LLaMA2 [ 118], one of the\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\nMental-LLM ‚Ä¢ 32:3\nmost advanced open-source model released by Meta; GPT-3.5 [1] and GPT-4 [18], representative close-sourced\nLLMs over 100 billion parameters. Considering the data availability, we leveraged online social media datasets\nwith high-quality human-generated mental health labels. Due to the ethical concerns of existing AI research\nfor mental health, we aim to benchmark LLMs‚Äô performance as an initial step before moving toward real-life\ndeployment. Our experiments contained three stages: (1) zero-shot prompting, where we experimented with\nvarious prompts related to mental health, (2) few-shot prompting, where we inserted examples into prompt\ninputs, and (3) instruction-finetuning, where we finetuned LLMs on multiple mental-health datasets with various\ntasks.\nOur results show that the zero-shot approach yields promising yet limited performance on various mental health\nprediction tasks across all models. Notably, FLAN-T5 and GPT-4 show encouraging performance, approaching\nthe state-of-the-art task-specific model. Meanwhile, providing a few shots in the prompt can improve the model\nperformance to some extent (Œî = 4.1%), but the advantage is limited. Finally and most importantly, we found that\ninstruction-finetuning significantly enhances the model performance across multiple mental-health-related tasks\nand various datasets simultaneously. Our finetuned Alpaca and FLAN-T5, namelyMental-Alpaca and Mental-\nFLAN-T5, significantly outperform the best of GPT-3.5 across zero-shot and few-shot settings (√ó25 and 15 bigger\nthan Alpaca and FLAN-T5) by an average of 10.9% on balance accuracy, as well as the best of GPT-4 by 4.8% (√ó250\nand 150 bigger than Alpaca and FLAN-T5). Meanwhile, Mental-Alpaca and Mental-FLAN-T5 can further perform\non par with the task-specific state-of-the-art Mental-RoBERTa [ 58]. We further conduct an exploratory case\nstudy on LLM‚Äôs capability of mental health reasoning (i.e., explaining the rationale behind their predictions). Our\nresults illustrate the promising future of certain LLMs like GPT-4, while also suggesting critical failure cases that\nneed future research attention. We open-source our code and model at https://github.com/neuhai/Mental-LLM.\nOur experiments present a comprehensive evaluation of various techniques to enhance LLMs‚Äô capability in\nthe mental health domain. However, we also note that our technical resultsdo not imply deployability. There\nare many important limitations of leveraging LLMs in mental health settings, especially along known racial\nand gender gaps [ 6, 42]. We discuss the important ethical risks to be addressed before achieving real-world\ndeployment.\nThe contribution of our paper can be summarized as follows:\n(1) We present a comprehensive evaluation of prompt engineering, few-shot, and finetuning techniques on\nmultiple LLMs in the mental health domain.\n(2) With online social media data, our results reveal that finetuning on a variety of datasets can significantly\nimprove LLM‚Äôs capability on multiple mental-health-specific tasks across different datasets simultaneously.\n(3) We release our modelMental-Alpaca and Mental-FLAN-T5 as open-source LLMs targeted at multiple mental\nhealth prediction tasks.\n(4) We provide a few technical guidelines for future researchers and developers on turning LLMs into experts\nin specific domains. We also highlight the important ethical concerns regarding leveraging LLMs for\nhealth-related tasks.\n2 BACKGROUND\nWe briefly summarize the related work in leveraging online text data for mental health prediction (Sec. 2.1). We\nalso provide an overview of the ongoing research in LLMs and their application in the health domain (Sec. 2.2).\n2.1 Online Text Data and Mental Health\nOnline platforms, especially social media platforms, have been acknowledged as a promising lens that is capable\nof revealing insights into the psychological states, health, and well-being of both individuals and populations [22,\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\n32:4 ‚Ä¢ Xu et al.\n30, 33, 47, 91]. In the past decade, there has been extensive research about leveraging content analysis and social\ninteraction patterns to identify and predict risks associated with mental health issues, such as anxiety [8, 104, 111],\nmajor depressive disorder [32, 34, 89, 119, 128, 129], suicide ideation [19, 29, 35, 101, 114], and others [25, 27,\n77, 103]. The real-time nature of social media, along with its archival capabilities, often helps in mitigating\nretrospective bias. The rich amount of social media data also facilitates the identification, monitoring, and\npotential prediction of risk factors over time. In addition to observation and detection, social media platforms\ncould further serve as effective channels to offer in-time assistance to communities at risk [66, 73, 99].\nFrom the computational technology perspective, early research started with basic methods [27, 32, 77]. For\nexample, pioneering work by Coppersmith et al. [27] employed correlation analysis to reveal the relationship\nbetween social media language data and mental health conditions. Since then, researchers have proposed a wide\nrange of feature engineering methods and built machine-learning models for the prediction [14, 79, 82, 102, 119].\nFor example, De Choudhury et al. [34] extracted a number of linguistic styles and other features to build an\nSVM model to perform depression prediction. Researchers have also explored deep-learning-based models for\nmental health prediction to obviate the need for hand-crafted features [57, 107]. For instance, Tadesse et al. [114]\nemployed an LSTM-CNN model and took word embeddings as the input to detect suicide ideation on Reddit.\nMore recently, pre-trained language models have become a popular method for NLP tasks, including mental\nhealth prediction tasks [48, 58, 83]. For example, Jiang et al. [60] used the contextual representations from BERT\nas input features for mental health issue detection. Otsuka et al. [88] evaluated the performance of BERT-based\npre-trained models in clinical settings. Meanwhile, researchers have also explored the multi-task setup [12] that\naims to predict multiple labels. For example, Sarkar et al. [106] trained a multi-task model to predict depression\nand anxiety at the same time. However, these multi-task models are constrained to a predetermined task set\nand thus have limited flexibility. Our work joins the same goal and aims to achieve a more flexible multi-task\ncapability. We focus on the next-generation technology of instruction-finetuned LLMs, leverage their power in\nnatural language understanding, and explore their capability on mental health tasks with social media data.\n2.2 LLM and Health Applications\nAfter the great success of Transformer-based language models such as BERT [ 37] and GPT [96], researchers\nand practitioners have advanced towards larger and more powerful language models ( e.g., GPT-3 [17] and\nT5 [97]). Meanwhile, researchers have proposed instruction finetuning, a method that utilizes varied prompts\nacross multiple datasets and tasks. This technique guides a model during training and generation phases to\nperform diverse tasks within a single unified framework [123]. These instruction-finetuned LLMs, such as GPT-\n4 [18], PaLM [23], FLAN-T5 [24], LLaMA [117], Alpaca [115], contain tens to hundreds of billions of parameters,\ndemonstrating promising performance across a variety of tasks, such as question answering [ 87, 100], logic\nreasoning [124, 135], machine translation [15, 45], etc.\nResearchers have explored the capability of these LLMs in health fields [59, 70, 71, 75, 85, 112, 125? ]. For example,\nSinghal et al. [112] finetuned PaLM-2 on medical domains and achieved 86.5% on MedQA dataset. Similarly,\nWu et al. [125] finetuned LLaMA on medical papers and showed promising results on multiple biomedical QA\ndatasets. Jo et al. [61] explored the deployment of LLMs for public health scenarios. Jiang et al. [59] trained a\nmedical language model on unstructured clinical notes from the electronic health record and fine-tuned it across\na wide range of clinical and operational predictive tasks. Their evaluation indicates that such a model can be\nused for various clinical tasks.\nThere is relatively less work in the mental health domain. Some work explored the capability of LLMs for\nsentiment analysis and emotion reasoning [64, 95, 134].\nCloser to our study, Lamichhane [67] and Amin et al. [10] tested the performance of ChatGPT (GPT-3.5) on\nmultiple classification tasks (e.g., stress, depression, and suicide detection). The results showed that ChatGPT\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\nMental-LLM ‚Ä¢ 32:5\nTable 1. A Summary of LLM-related Research for Mental Health Applications.\nLLMs Methods Tasks\nLamichhane [67] GPT-3.5 Zero-shot Classification\nAminet al.[10] GPT-3.5 Zero-shot Classification\nYanget al.[132] GPT-3.5 Zero-shot Classification, Reasoning\nMental-LLaMA [133] LLaMA2, Vicuna (LLaMA-based)Zero-shot, Few-shot,\nInstruction FinetuningClassification, Reasoning\nMental-LLM (Our Work)Alpaca, Alapca-LoRA, FLAN-T5,\nLLaMA2, GPT-3.5, GPT-4\nZero-shot, Few-shot,\nInstruction FinetuningClassification, Reasoning\nshows the initial potential for mental health applications, but it still has a great room for improvement, with at\nleast 5-10% performance gaps on accuracy and F1-score. Yanget al. [132] further evaluated the potential reasoning\ncapability of GPT-3.5 for reasoning tasks (e.g., potential stressors). However, most previous studies focused solely\non zero-shot prompting and did not explore other methods to improve the performance of LLMs. Very recently,\nYang et al. [133] released Mental-LLaMA, a set of LLaMA-based models finetuned on mental health datasets\nfor a set of mental health tasks. Table 1 summarizes the recent related work exploring LLMs‚Äô capabilities on\nmental-health-related tasks. None of the existing work explores the capability other than LLaMA or GPT-3.5. In\nthis work, we present a comprehensive and systematic exploration of multiple LLMs‚Äô performance on mental\nhealth tasks, as well as multiple methods to improve their capabilities.\n3 METHODS\nWe introduce our experiment design with LMMs on multiple mental health prediction task setups, including\nzero-shot prompting (Sec. 3.1), few-shot prompting (Sec. 3.2), and instruction finetuning (Sec. 3.3). These setups\nare model-agnostic, and we will present the details of language models and datasets employed for our experiment\nin the next section.\n3.1 Zero-shot Prompting\nThe language understanding and reasoning capability of LLMs have enabled a wide range of applications without\nthe need for any domain-specific data, but only providing appropriate prompts [ 65, 122]. Therefore, we start\nwith prompt design for mental health tasks in a zero-shot setting.\nThe goal of prompt design is to empower a pre-trained general-purpose LLM to achieve good performance on\ntasks in the mental health domain. We propose a general zero-shot prompt template (PromptùëçùëÜ) that consists of\nfour parts:\nPromptùëçùëÜ = TextData +PromptPart1-S +PromptPart2-Q +OutputConstraint (1)\nwhere TextData is the online text data generated by end-users. PromptPart1-S provides specifications for a mental\nhealth prediction target. PromptPart2-Q poses the question for LLMs to answer. And OutputConstraint controls the\noutput of models (e.g., ‚ÄúOnly return yes or no‚Äù for a binary classification task).\nWe propose several design strategies for PromptPart1-S, as shown in the top part of Table 2: (1) Basic, which\nleaves it as blank; (2) Context Enhancement, which provides more social media context about the TextData; (3)\nMental Health Enhancement , which inserts mental health concept by asking the model to act as an expert. (4)\nContext & Mental Health Enhancement , which combines both enhancement strategies by asking the model\nto act as a mental health expert under the social media context.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\n32:6 ‚Ä¢ Xu et al.\nTable 2. Prompt Design for Mental Health Prediction Tasks. PromptPart1-S aims to provide a better specification for LLMs\nand PromptPart2-Q poses the questions for LLMs to answer. For Part 1, we propose three strategies: context enhancement,\nmental health enhancement, and the combination of both. As for Part 2, we design different content for multiple mental\nhealth problem categories and prediction tasks. For each part, we propose two to three versions to improve its variation.\nStrategy PromptPart1-S\nBasic ‚Ä¢{ blank }\nContext Enhancement ‚Ä¢This person wrote this paragraph on social media.\n‚Ä¢Consider this post on social media to answer the question.\nMental Health Enhancement‚Ä¢As a psychologist, read the post on social media and answer the question.\n‚Ä¢If you are a psychologist, read the post on social media and answer the question.\nContext & Mental Health Enhancement\n‚Ä¢This person wrote this paragraph on social media. As a psychologist, read the post on social media\nand answer the question.\n‚Ä¢This person wrote this paragraph on social media. As a psychologist, consider the mental well-being\ncondition expressed in this post, read the post on social media, and answer the question.\nCategory Task PromptPart2-Q\nMental state\n(e.g., stressed,\ndepressed)\nBinary classification\n(e.g., yes or no)\n‚Ä¢Is the poster [stressed]?\n‚Ä¢Is the poster of this post [stressed]?\n‚Ä¢Determine if the poster of this post is [stressed].\nMulti-class classification\n(e.g., multiple levels)\n‚Ä¢Which level is the person [stressed]?\n‚Ä¢How [stressed] is the person?\n‚Ä¢Determine how [stressed] the person is.\nCritical risk action\n(e.g., suicide)\nBinary classification\n(e.g., yes or no)\n‚Ä¢Does the poster want to [suicide]?\n‚Ä¢Is the poster likely to [suicide]?\n‚Ä¢Determine if the poster of this post want to [suicide].\nMulti-class classification\n(e.g., multiple levels)\n‚Ä¢Which level of [suicide] risk does the person have?\n‚Ä¢How [suicidal] is the person?\n‚Ä¢Determine which level of [suicide] risk does the person have.\nAs for PromptPart2-Q, we mainly focus on two categories of mental health prediction targets: (1) predicting\ncritical mental states, such as stress or depression, and (2) predicting high-stake risk actions, such as suicide. We\ntailor the question description for each category. Moreover, for both categories, we explore binary and multi-class\nclassification tasks 1. Thus, we also make small modifications based on specific tasks to ensure appropriate\nquestions (see Sec. 4 for our mental health tasks). The bottom part of Table 2 summarizes the mapping.\nFor both PromptPart1-S and PromptPart2-Q, we propose several versions to improve its variability. We then evaluate\nthese prompts on multiple LLMs on different datasets and compare their performance.\n3.2 Few-shot Prompting\nIn order to provide more domain-specific information, researchers have also explored few-shot prompting with\nLLMs by providing few-shot demonstrations to support in-context learning (e.g., [7, 31]). Note that these few\nexamples are used solely in prompts, and the model parameters remain unchanged. The intuition is to present a\nfew ‚Äúexamples‚Äù for the model to learn domain-specific knowledge in situ . In our setting, we also test this strategy\nby adding additional randomly sampled [PromptùëçùëÜ ‚àílabel] pairs. The design of the few-shot prompt (PromptùêπùëÜ)\n1We also conduct an exploratory case study on mental health reasoning tasks. Please see more details in Sec. 5.4.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\nMental-LLM ‚Ä¢ 32:7\nis straightforward:\nPromptùêπùëÜ = [Sample PromptùëçùëÜ ‚àílabel]ùëÄ +PromptùëçùëÜ (2)\nwhere ùëÄ is the number of prompt-label pairs and is capped by the input length limit of a model. Note that both\nthe Sample PromptùëçùëÜ and PromptùëçùëÜ follow Eq. 1 and employ the same design of PromptPart1-S and PromptPart2-Q\nto ensure consistency.\n3.3 Instruction Finetuning\nIn contrast to the few-shot prompting strategy in Sec. 3.2, the goal of this strategy is closer to the traditional\nfew-shot transfer learning, where we further train the model with a small amount of domain-specific data (e.g.,\n[52, 71, 126]). We experiment with multiple finetuning strategies.\n3.3.1 Single-dataset Finetuning. Following most of the previous work in the mental health field [26, 35, 132], we\nfirst conduct basic finetuning on a single dataset (the training set). This finetuned model can be tested on the\nsame dataset (the test set) to evaluate its performance and different datasets to evaluate its generalizability.\n3.3.2 Multi-dataset Finetuning. From Sec. 3.1 to Sec. 3.3.1, we have been focusing on one single mental health\ndataset ùê∑. More interestingly, we further experiment with finetuning across multiple datasets simultaneously.\nSpecifically, we leverage instruction finetuning to enable LLMs to handle multiple tasks in different datasets [17].\nIt is noteworthy that such an instruction finetuning setup differs from the state-of-the-art mental-health-specific\nmodels (e.g., Mental-RoBERTa [58]). The previous models are finetuned for a specific task, such as depression\nprediction or suicidal ideation prediction. Once trained on task A, the model becomes specific to task A and is\nonly suitable for solving that particular task. In contrast, we finetune LLMs on several mental health datasets,\nemploying diverse instructions for different tasks across these datasets in a single iteration. This enables them to\nhandle multiple tasks without additional task-specific finetuning.\nFor both single- and multi-dataset finetuning, we follow the same two steps:\nStep 1: Finetune with [PromptùëçùëÜ ‚àílabel]ùêº√çùëÅùê∑ùëñ‚àíùë°ùëüùëéùëñùëõ\nStep 2: Test with [PromptùëçùëÜ]ùêº√çùëÅùê∑ùëñ‚àíùë°ùëíùë†ùë°\n(3)\nwhere ùëÅùê∑ùëñ‚àíùë°ùëüùëéùëñùëõ /ùëÅùê∑ùëñ‚àíùë°ùëíùë†ùë° is the total size of the training/test dataset ùê∑ùëñ, ùêº represents the set of datasets used for\nfinetuning, and ùëñ indicates the specific dataset index (ùëñ ‚ààùêº,|ùêº|‚â• 1). Both PromptùëçùëÜ-train and PromptùëçùëÜ-test follow\nEq. 1. Similar to the few-shot setup in Eq. 2, they employ the same design of PromptPart1-S and PromptPart2-Q.\n4 IMPLEMENTATION\nOur method design is agnostic to specific datasets or models. In this section, we introduce the specific datasets\n(Sec. 4.1) and models (Sec. 4.2) involved in our experiments. In particular, we highlight our instructional-finetuned\nopen-source models Mental-Alpaca and Mental-FLAN-T5 (Sec. 4.2.1). We also provide an overview of our\nexperiment setup and evaluation metrics (Sec. 4.3).\n4.1 Datasets and Tasks\nOur experiment is based on four well-established datasets that are commonly employed for mental health analysis.\nThese datasets were collected from Reddit due to their high-quality and availability. It is noteworthy that we\nintentionally avoid using datasets with weak labels based on specific linguistic patterns ( e.g., whether a user\never stated ‚ÄúI was diagnosed with X‚Äù). Instead, we used ones with human expert annotations or supervision. We\ndefine six diverse mental health prediction tasks based on these datasets.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\n32:8 ‚Ä¢ Xu et al.\n‚Ä¢Dreaddit [120]: This dataset collected posts via Reddit PRAW API [94] from Jan 1, 2017 to Nov 19, 2018,\nwhich contains ten subreddits in the five domains (abuse, social, anxiety, PTSD, and financial) and includes\n2929 users‚Äô posts. Multiple human annotators rated whether sentence segments showed the stress of the\nposter, and the annotations were aggregated to generate final labels. We used this dataset for a post-level\nbinary stress prediction (Task 1).\n‚Ä¢DepSeverity [80]: This dataset leveraged the same posts collected in [120], but with a different focus on\ndepression. Two human annotators followed DSM-5 [98] and categorized posts into four levels of depression:\nminimal, mild, moderate, and severe. We employed this dataset for two post-level tasks: binary depression\nprediction (i.e., whether a post showed at least mild depression,Task 2), and four-level depression prediction\n(Task 3).\n‚Ä¢SDCNL [49]: This dataset also collected posts from Python Reddit API, including r/SuicideWatch and\nr/Depressionfrom 1723 users. Through manual annotation, they labeled whether each post showed suicidal\nthoughts. We employed this dataset for the post-level binary suicide ideation prediction (Task 4).\n‚Ä¢CSSRS-Suicide [40]: This dataset contains posts from 15 mental health-related subreddits from 2181\nusers between 2005 and 2016. Four practicing psychiatrists followed Columbia Suicide Severity Rating\nScale (C-SSRS) guidelines [93] to manually annotate 500 users on suicide risks in five levels: supportive,\nindicator, ideation, behavior, and attempt. We leveraged this dataset for two user-level tasks: binary suicide\nrisk prediction (i.e., whether a user showed at least suicide indicator, Task 5), and five-level suicide risk\nprediction (Task 6).\nIn order to test the generalizability of our methods, we also leveraged three other datasets from various\nplatforms. Similarly, all datasets contain human annotations as labels.\n‚Ä¢Red-Sam [62, 105]: This dataset also collected posts with PRAW API [ 94] , involving five subreddits\n(Mental Health, depression, loneliness, stress, anxiety). Two domain experts‚Äô annotations were aggregated\nto generate depression labels. We used this dataset as an external evaluation dataset on binary depression\ndetection (Task 2 ). Although also from Reddit, this dataset was not involved in few-shot learning or\ninstruction finetuning. We cross-checked datasets to ensure there were no overlapping posts.\n‚Ä¢Twt-60Users [56]: This dataset collected twitters from 60 users during 2015 with Twitter API. Two human\nannotators labeled every tweet with depression labels. We used this non-Reddit dataset as an external\nevaluation dataset on depression detection (Task 2). Note that this dataset has imbalanced labels (90.7%\nFalse), as most tweets did not indicate mental distress.\n‚Ä¢SAD [76]: This dataset contains SMS-like text messages with nine types of daily stressor categories (work,\nschool, financial problem, emotional turmoil, social relationships, family issues, health, everyday decision-\nmaking, and other). These messages were written by 3578 humans. We used this non-Reddit dataset as an\nexternal evaluation dataset on binary stress detection (Task 1). Note that human crowd-workers write the\nmessages under certain stressor-triggered instructions. Therefore, this dataset has imbalanced labels on the\nother side (94.0% True).\nTable 3 summarizes the information of the seven datasets and six mental health prediction tasks. For each\ndataset, we conducted an 80%/20% train-test split. Notably, to avoid data leakage, each user‚Äôs data were placed\nexclusively in either the training or test set.\n4.2 Models\nWe experimented with multiple LLMs with different sizes, pre-training targets, and availability.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\nMental-LLM ‚Ä¢ 32:9\nTable 3. Summary of Seven Mental Health Datasets Employed for Our Experiment. The top four datasets are used for both\ntraining and testing, while the bottom three datasets are used for external evaluation. We define six diverse mental health\nprediction tasks on these datasets.\nDataset Task Dataset Size Text Length (Token)\nDreaddit [120]\nSource: Reddit\n#1: Binary Stress Prediction\npost-level\nTrain: 2838 (47.6% False, 52.4% True)\nTest: 715 (48.4% False, 51.6% True)\nTrain: 114¬±41\nTest: 113¬±39\nDepSeverity [80]\nSource: Reddit\n#2: Binary Depression Prediction\npost-level\nTrain: 2842 (72.9% False, 17.1% True)\nTest: 711 (72.3% False, 17.7% True)\nTrain: 114¬±41\nTest: 113¬±37\n#3: Four-level Depression Prediction\npost-level\nTrain: 2842 (72.9% Minimum, 8.4% Mild,\n11.2% Moderate, 7.4% Severe)\nTest: 711 (72.3% Minimum, 7.2% Mild,\n11.5% Moderate, 10.0% Severe)\nTrain: 114¬±41\nTest: 113¬±37\nSDCNL [49]\nSource: Reddit\n#4: Binary Suicide Ideation Prediction\npost-level\nTrain: 1516 (48.1% False, 51.9% True)\nTest: 379 (49.1% False, 50.9% True)\nTrain: 101¬±161\nTest: 92¬±119\nCSSRS-Suicide [40]\nSource: Reddit\n#5: Binary Suicide Risk Prediction\nuser-level\nTrain: 400 (20.8% False, 79.2% True)\nTest: 100 (25.0% False, 75.0% True)\nTrain: 1751¬±2108\nTest: 1909¬±2463\n#6: Five-level Suicide Risk Prediction\nuser-level\nTrain: 400 (20.8% Supportive, 20.8% Indicator,\n34.0% Ideation, 14.8% Behavior, 9.8% Attempt)\nTest: 100 (25.0% Supportive, 16.0% Indicator,\n35.0% Ideation, 18.0% Behavior, 6.0% Attempt)\nTrain: 1751¬±2108\nTest: 1909¬±2463\nRed-Sam [105]\nSource: Reddit\n#2: Binary Depression Prediction\npost-level External Evaluation: 3245 (26.1% False, 73.9% True) External Evaluation: 151¬±139\nTwt-60Users [56]\nSource: Twitter\n#2: Binary Depression Prediction\npost-level External Evaluation: 8135 (90.7% False, 9.3% True) External Evaluation: 15¬±7\nSAD [76]\nSource: SMS-like\n#1: Binary Stress Prediction\npost-level External Evaluation: 6185 (6.0% False, 94.0% True) External Evaluation: 13¬±6\n‚Ä¢Alpaca (7B) [115]: An open-source large model finetuned from another open-sourced LLaMA 7B model [117]\non instruction following demonstrations. Experiments have shown that Alpaca behaves qualitatively\nsimilarly to OpenAI‚Äôs text-davinci-003 on certain task metrics. We choose the relatively small 7B\nversion to facilitate running and finetuning on consumer hardware.\n‚Ä¢Alpaca-LoRA (7B) [51]: Another open-source large model finetuned from LLaMA 7B model using the same\ndataset as Alpaca [115]. This model leverages a different finetuning technique called low-rank adaptation\n(LoRA) [51], with the goal of reducing finetuning cost by freezing the model weights and injecting trainable\nrank decomposition matrices into each layer of the Transformer architecture. Despite the similarity in\nnames, it is important to note that Alpaca-LoRA is entirely distinct from Alpaca. They are trained on the\nsame dataset but with different methods.\n‚Ä¢FLAN-T5 (11B) [24]: An open-source large model T5 [97] finetuned with a variety of task-based datasets\non instructions. Compared to other LLMs, FLAN-T5 focuses more on task solving and is less optimized for\nnatural language or dialogue generation. We picked the largest version of FLAN-T5 (i.e., FLAN-T5-XXL),\nwhich has a comparable size of Alpaca.\n‚Ä¢LLaMA2 (70B) [118]: A recent open-source large model released by Meta. We picked the largest version of\nLLaMA2, whose size is between FLAN-T5 and GPT-3.5.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\n32:10 ‚Ä¢ Xu et al.\n‚Ä¢GPT-3.5 (175B) [1]: This large model is closed-source and available through API provided by OpenAI. We\npicked the gpt-3.5-turbo, one of the most capable and cost-effective models in the GPT-3.5 family.\n‚Ä¢GPT-4 (1700B) [18]: This is the largest closed-source model available through OpenAI API. We picked the\ngpt-4-0613. Due to the limited availability of API, the cost of finetuning GPT-3.5 or GPT-4 is prohibitive.\nIt is worth noting that Alpaca, Alpaca-LoRA, GPT-3.5, LLaMA2 and GPT-4 are all finetuned with natural\ndialogue as one of the optimization goals. In contrast, FLAN-T5 is more focused on task-solving. In our case, the\nuser-written input posts resemble natural dialogue, whereas the mental health prediction tasks are defined as\nspecific classification tasks. It is unclear and thus interesting to explore which LLM fits better with our goal.\n4.2.1 Mental-Alpaca & Mental-FLAN-T5. Our methods of zero-shot prompting (Sec. 3.1) and few-shot prompting\n(Sec. 3.2) do not update model parameters during the experiment. In contrast, instruction finetuning (Sec. 3.3) will\nupdate model parameters and generate new models. To enhance their capability in the mental health domains, we\nupdate Alpaca and FLAN-T5 on six tasks across the four datasets in Sec. 4.1 using the multi-dataset instruction\nfinetuning method (Sec. 3.3.2), which leads to our new model Mental-Alpaca and Mental-FLAN-T5.\n4.3 Experiment Setup and Metrics\nFor zero-shot and few-shot prompting methods, we load open-source models (Alpaca, Alpaca-LoRA, FLAN-\nT5, LLaMA2) with one to eight Nvidia A100 GPUs to do the tasks, depending on the size of the model. For\nclosed-source models (GPT-3.5, and GPT-4), we use OpenAI API to conduct chat completion tasks.\nAs for finetuning Mental-Alpaca and Mental-FLAN-T5, we merge the four datasets together and provide\ninstructions for all six tasks (in the training set). We use eight Nvidia A100 GPUs for instruction finetuning. With\ncross entropy as the loss function, we backpropagate and update model parameters in 3 epochs, with Adam\noptimizer and a learning rate as 2ùëí‚àí5 (cosine scheduler, warmup ratio 0.03).\nWe focus on balanced accuracy as the main evaluation metric, i.e., the mean of sensitivity (true positive rate)\nand specificity (true negative rate). We picked this metric since it is more robust to class imbalance compared to\nthe accuracy or F1 score [16, 129]. It is noteworthy that the sizes of LLMs we compare are vastly different, with\nthe number of parameters ranging from 7B to 1700B. A larger model is usually expected to have a better overall\nperformance than a smaller model. We inspect whether this expectation holds in our experiments.\n5 RESULTS\nWe summarize our experiment results with zero-shot prompting (Sec. 5.1), few-shot prompting (Sec. 5.2), and\ninstruction finetuning (Sec. 5.3). Moreover, although we mainly focus on prediction tasks in this research, we\nalso present the initial results of our exploratory case study on mental health reasoning tasks in Sec. 5.4.\nOverall, our results show that zero-shot and few-shot settings show promising performance of LLMs for\nmental health tasks, although their performance is still limited. Instruction-finetuning on multiple datasets\n(Mental-Alpaca and Mental-FLAN-T5) can significantly boost models‚Äô performance on all tasks simultaneously.\nOur case study also reveals the strong reasoning capability of certain LLMs, especially GPT-4. However, we note\nthat these results do not indicate the deployability. We highlight important ethical concerns and gaps in Sec. 6.\n5.1 Zero-shot Prompting Shows Promising yet Limited Performance\nWe start with the most basic zero-shot prompting with Alpaca, Alpaca-LoRA, FLAN-T5, LLaMA2, GPT-3.5, and\nGPT-4. The balanced accuracy results are summarized in the first sections of Table 4. AlpacaùëçùëÜ and Alpaca-LoRAùëçùëÜ\nachieve better overall performance than the naive majority baseline ( ŒîAlpaca = 5.5%, ŒîAlpaca-LoRA = 5.6%), but\nthey are far from the task-specific baseline models BERT and Mental-RoBERTa (which have 20%-25% advantages).\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\nMental-LLM ‚Ä¢ 32:11\nWith much larger models GPT-3.5ùëçùëÜ, the performance gets more promising ( ŒîGPT-3.5 = 12.4% over baseline),\nwhich is inline with previous work [132]. GPT-3.5‚Äôs advantage over Alpaca and Alpaca-LoRA is expected due to\nits larger size (25√ó).\nSurprisingly, FLAN-T5ùëçùëÜ achieves much better overall results compared to AlpacaùëçùëÜ (ŒîFLAN-T5_ùë£ùë†_Alpaca = 10.9%)\nand Alpaca-LoRAùëçùëÜ (ŒîFLAN-T5_ùë£ùë†_Alpaca-LoRA = 11.0%), and even LLaMA2 (ŒîFLAN-T5_ùë£ùë†_LLaMA2 = 1.0%) and GPT-3.5\n(ŒîFLAN-T5_ùë£ùë†_GPT-3.5 = 4.2%). Note that LLaMA2 is 6 times bigger than FLAN-T5 and GPT-3.5 is 15 times bigger. On\nTask #6 (Five-level Suicide Risk Prediction), FLAN-T5ùëçùëÜ even outperforms the state-of-the-art Mental-RoBERTa\nby 4.5%. Comparing these results, the task-solving-focused model FLAN-T5 appears to be better at the mental\nhealth prediction tasks in a zero-shot setting. We will introduce more interesting findings after finetuning (see\nSec. 5.3.1).\nIn contrast, the advantage of GPT-4 becomes relatively less remarkable considering its gigantic size.GPT-4ùëçùëÜ‚Äôs\naverage performance outperforms FLAN-T5ùëçùëÜ (150√ósize), LLaMA2ùëçùëÜ (25√ósize), and GPT-3.5ùëçùëÜ (10√ósize) by\n6.4%, 7.5%, and 10.6%, respectively. Yet it is still very encouraging to observe that GPT-4 is approaching the\nstate-of-the-art on these tasks (ŒîGPT-4_ùë£ùë†_Mental-RoBERTa = ‚àí7.9%), and it also outperforms Mental-RoBERTa on\nTask #6 by 4.5%. In general, these results indicate the promising capability of LLMs on mental health prediction\ntasks compared to task-specific models, even without any domain-specific information.\n5.1.1 The Effectiveness of Enhancement Strategies. In Sec. 3.1, we propose context enhancement, mental health\nenhancement, and their combination strategies for zero-shot prompt design to provide more information about\nthe domain. Interestingly, our results suggest varied effectiveness on different LLMs and datasets.\nTable 5 provides a zoom-in summary of the zero-shot part in Table 4. For Alpaca, LLaMA2, GPT-3.5, and\nGPT-4, the three strategies improved the performance in general (ŒîAlpaca = 1.0%, 13 out of 18 tasks show positive\nchanges; ŒîLLaMA2 = 0.3%, 12/18 tasks positive; ŒîGPT-3.5 = 2.8%, 12/18 tasks positive; ŒîGPT-4 = 0.2%, 11/18 tasks\npositive). However, for Alpaca-LoRA and FLAN-T5, adding more context or mental health domain information\nwould reduce the model performance (ŒîAlpaca-LoRA = ‚àí2.7%, ŒîFLAN-T5 = ‚àí1.6%). For Alpaca-LoRA, this limitation\nmay stem from being trained with fewer parameters, potentially constraining its ability to understand context\nor domain specifics. For FLAN-T5, this reduced performance might be attributed to its limited capability in\nprocessing additional information, as it is primarily tuned for task-solving.\nThe effectiveness of strategies on different datasets/tasks also varies. We observe that Task#4 from the SDCNL\ndataset and Task#6 from the CSSRS-Suicide dataset benefit the most from the enhancement. In particular, GPT-\n3.5 benefits very significantly from enhancement on Task #4 ( ŒîGPT-3.5‚àíTask#4 = 14.8%). And LLaMA2 benefits\nsignificantly on Task #6 (ŒîGPT-3.5‚àíTask#6 = 6.8%). These could be caused by the different nature of datasets. Our\nresults suggest that these enhancement strategies are generally more effective for critical action prediction (e.g.,\nsuicide, 2/3 tasks positive) than mental state prediction (e.g., stress and depression, 1/3 task positive).\nWe also compare the effectiveness of different strategies on the four models with positive effects: Alpaca,\nLLaMA2, GPT-3.5, and GPT-4. The context enhancement strategy has the most stable improvement across all\nmental health prediction tasks ( ŒîAlpaca‚àíùëêùëúùëõùë°ùëíùë•ùë° = 2.1%, 6/6 tasks positive; ŒîLLaMA2‚àíùëêùëúùëõùë°ùëíùë•ùë° = 1.2%, 4/6 tasks\npositive; ŒîGPT-3.5‚àíùëêùëúùëõùë°ùëíùë•ùë° = 2.5%, 5/6 tasks positive; ŒîGPT-4‚àíùëêùëúùëõùë°ùëíùë•ùë° = 0.4%, 5/6 tasks positive). Comparatively, the\nmental health enhancement strategy is less effective (ŒîAlpaca‚àíùëö‚Ñé = 1.1%, 5/6 tasks positive; ŒîLLaMA2‚àíùëö‚Ñé = ‚àí0.5%,\n4/6 tasks positive; ŒîGPT-3.5‚àíùëö‚Ñé = 2.1%, 3/6 tasks positive; ŒîGPT-4‚àíùëö‚Ñé = 0.2%, 3/6 tasks positive). The combination\nof the two strategies yields diverse results. It has the most significant improvement on GPT-3.5‚Äôs performance,\nbut not on all tasks (ŒîGPT-3.5‚àíùëèùëúùë°‚Ñé = 3.9%, 4/6 tasks positive), followed by LLaMA2 (ŒîLLaMA2‚àíùëèùëúùë°‚Ñé = 0.2%, 4/6 tasks\npositive). However, it has slightly negative impact on the average performance of Alpaca (ŒîAlpaca‚àíùëèùëúùë°‚Ñé = ‚àí0.1%,\n2/6 tasks positive) or GPT-4 (ŒîGPT-4‚àíùëèùëúùë°‚Ñé = ‚àí0.3%, 3/6 tasks positive). This indicates that larger language models\n(LLaMA2, GPT-3.5 vs. Alpaca) have a strong capability to leverage the information embedded in the prompts.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\n32:12 ‚Ä¢ Xu et al.\nTable 4. Balanced Accuracy Performance Summary of Zero-shot, Few-shot and Instruction Finetuning on LLMs. ùëçùëÜùëèùëíùë†ùë°\nhighlights the best performance among zero-shot prompt designs, including context enhancement, mental health enhance-\nment, and their combination (see Table. 2). Detailed results can be found in Table 10 in Appendix. Small numbers represent\nstandard deviation across different designs of PromptPart1-S and PromptPart2-Q. The baselines at the bottom rows do not\nhave standard deviation as the task-specific output is static, and prompt designs do not apply. Due to the maximum token\nsize limit, we only conduct few-shot prompting on a subset of datasets and mark other infeasible datasets as ‚Äú‚Äì‚Äù. For each\ncolumn, the best result is bolded, and the second best is underlined.\nDataset Dreaddit DepSeverity SDCNL CSSRS-Suicide\nCategory Model Task #1 Task #2 Task #3 Task #4 Task #5 Task #6\nZero-shot\nPrompting\nAlpacaùëçùëÜ 0.593¬±0.039 0.522¬±0.022 0.431¬±0.050 0.493¬±0.007 0.518¬±0.037 0.232¬±0.076\nAlpacaùëçùëÜ_ùëèùëíùë†ùë° 0.612¬±0.065 0.577¬±0.028 0.454¬±0.143 0.532¬±0.005 0.532¬±0.033 0.250¬±0.060\nAlpaca-LoRAùëçùëÜ 0.571¬±0.043 0.548¬±0.027 0.437¬±0.044 0.502¬±0.011 0.540¬±0.012 0.187¬±0.053\nAlpaca-LoRAùëçùëÜ_ùëèùëíùë†ùë° 0.571¬±0.043 0.548¬±0.027 0.437¬±0.044 0.502¬±0.011 0.567¬±0.038 0.224¬±0.049\nFLAN-T5ùëçùëÜ 0.659¬±0.086 0.664¬±0.011 0.396¬±0.006 0.643¬±0.021 0.667¬±0.023 0.418¬±0.012\nFLAN-T5ùëçùëÜ_ùëèùëíùë†ùë° 0.663¬±0.079 0.674¬±0.014 0.396¬±0.006 0.653¬±0.011 0.667¬±0.023 0.418¬±0.012\nLLaMA2ùëçùëÜ 0.720¬±0.012 0.693¬±0.034 0.429¬±0.013 0.589¬±0.010 0.691¬±0.014 0.261¬±0.018\nLLaMA2ùëçùëÜ_ùëèùëíùë†ùë° 0.720¬±0.012 0.711¬±0.033 0.444¬±0.021 0.643¬±0.014 0.722¬±0.039 0.367¬±0.043\nGPT-3.5ùëçùëÜ 0.685¬±0.024 0.642¬±0.017 0.603¬±0.017 0.460¬±0.163 0.570¬±0.118 0.233¬±0.009\nGPT-3.5ùëçùëÜ_ùëèùëíùë†ùë° 0.688¬±0.045 0.653¬±0.020 0.642¬±0.034 0.632¬±0.020 0.617¬±0.033 0.310¬±0.015\nGPT-4ùëçùëÜ 0.700¬±0.001 0.719¬±0.013 0.588¬±0.010 0.644¬±0.007 0.760¬±0.009 0.418¬±0.009\nGPT-4ùëçùëÜ_ùëèùëíùë†ùë° 0.725¬±0.009 0.719¬±0.013 0.656¬±0.001 0.647¬±0.014 0.760¬±0.009 0.441¬±0.057\nFew-shot\nPrompting\nAlpacaùêπùëÜ 0.632¬±0.030 0.529¬±0.017 0.628¬±0.005 ‚Äî ‚Äî ‚Äî\nFLAN-T5ùêπùëÜ 0.786¬±0.006 0.678¬±0.009 0.432¬±0.009 ‚Äî ‚Äî ‚Äî\nGPT-3.5ùêπùëÜ 0.721¬±0.010 0.665¬±0.015 0.580¬±0.002 ‚Äî ‚Äî ‚Äî\nGPT-4ùêπùëÜ 0.698¬±0.009 0.724¬±0.005 0.613¬±0.001 ‚Äî ‚Äî ‚Äî\nInstructional\nFinetuning\nMental-Alpaca 0.816 ¬±0.006 0.775¬±0.006 0.746¬±0.005 0.724¬±0.004 0.730¬±0.048 0.403¬±0.029\nMental-FLAN-T5 0.802 ¬±0.002 0.759¬±0.003 0.756¬±0.001 0.677¬±0.005 0.868¬±0.006 0.481¬±0.006\nBaseline\nMajority 0.500 ¬±‚àí‚àí‚àí 0.500¬±‚àí‚àí‚àí 0.250¬±‚àí‚àí‚àí 0.500¬±‚àí‚àí‚àí 0.500¬±‚àí‚àí‚àí 0.200¬±‚àí‚àí‚àí\nBERT 0.783 ¬±‚àí‚àí‚àí 0.763¬±‚àí‚àí‚àí 0.690¬±‚àí‚àí‚àí 0.678¬±‚àí‚àí‚àí 0.500¬±‚àí‚àí‚àí 0.332¬±‚àí‚àí‚àí\nMental-RoBERTa 0.831¬±‚àí‚àí‚àí 0.790¬±‚àí‚àí‚àí 0.736¬±‚àí‚àí‚àí 0.723¬±‚àí‚àí‚àí 0.853¬±‚àí‚àí‚àí 0.373¬±‚àí‚àí‚àí\nBut for the huge GPT-4, adding prompts seems less effective, probably because it already contains similar basic\ninformation in its knowledge space.\nWe summarize our key takeaways from this section:\n‚Ä¢Both small-scale and large-scale LLMs show promising performance on mental health tasks.\nFLAN-T5 and GPT-4‚Äôs performance is approaching task-specific NLP models.\n‚Ä¢The prompt design enhancement strategies are generally effective for dialogue-focused models,\nbut not for task-solving-focused models. These strategies work better for critical action prediction\ntasks such as suicide prediction.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\nMental-LLM ‚Ä¢ 32:13\nTable 5. Balanced Accuracy Performance Change using Enhancement Strategies. The green/red color indicates increased/de-\ncreased accuracy. This table zooms in on the zero-shot section of Table 4. ‚Üë/‚Üìmarks the ones with better/worse performance\nin comparison.\nDataset Dreaddit DepSeverity SDCNL CSSRS-Suicide\nModel Task #1 Task #2 Task #3 Task #4 Task #5 Task #6 Œî‚àíAll Six Tasks\nŒî‚àíAlpacaùëçùëÜ_ùëêùëúùëõùë°ùëíùë•ùë° ‚Üë+0.019 ‚Üë+0.045 ‚Üë+0.023 ‚Üë+0.004 ‚Üë+0.014 ‚Üë+0.018 ‚Üë+0.021\nŒî‚àíAlpacaùëçùëÜ_ùëö‚Ñé ‚Üë+0.000 ‚Üë+0.055 ‚Üë+0.013 ‚Üì‚àí0.011 ‚Üë+0.006 ‚Üë+0.004 ‚Üë+0.011\nŒî‚àíAlpacaùëçùëÜ_ùëèùëúùë°‚Ñé ‚Üì‚àí0.053 ‚Üë+0.037 ‚Üì‚àí0.010 ‚Üë+0.039 ‚Üì‚àí0.007 ‚Üì‚àí0.010 ‚Üì‚àí0.001\nŒî‚àíAlpaca-LoRAùëçùëÜ_ùëêùëúùëõùë°ùëíùë•ùë° ‚Üì‚àí0.035 ‚Üì‚àí0.047 ‚Üì‚àí0.094 ‚Üì‚àí0.030 ‚Üë+0.027 ‚Üë+0.027 ‚Üì‚àí0.025\nŒî‚àíAlpaca-LoRAùëçùëÜ_ùëö‚Ñé ‚Üì‚àí0.071 ‚Üì‚àí0.047 ‚Üì‚àí0.105 ‚Üì‚àí0.005 ‚Üë+0.017 ‚Üë+0.029 ‚Üì‚àí0.031\nŒî‚àíAlpaca-LoRAùëçùëÜ_ùëèùëúùë°‚Ñé ‚Üì‚àí0.071 ‚Üì‚àí0.048 ‚Üì‚àí0.051 ‚Üì‚àí0.003 ‚Üì‚àí0.023 ‚Üë+0.037 ‚Üì‚àí0.027\nŒî‚àíFLAN-T5ùëçùëÜ_ùëêùëúùëõùë°ùëíùë•ùë° ‚Üë+0.004 ‚Üë+0.011 ‚Üì‚àí0.018 ‚Üë+0.010 ‚Üì‚àí0.018 ‚Üì‚àí0.040 ‚Üì‚àí0.009\nŒî‚àíFLAN-T5ùëçùëÜ_ùëö‚Ñé ‚Üì‚àí0.043 ‚Üë+0.003 ‚Üì‚àí0.030 ‚Üë+0.005 ‚Üì‚àí0.013 ‚Üì‚àí0.046 ‚Üì‚àí0.021\nŒî‚àíFLAN-T5ùëçùëÜ_ùëèùëúùë°‚Ñé ‚Üì‚àí0.055 ‚Üì‚àí0.003 ‚Üì‚àí0.007 ‚Üë+0.002 ‚Üì‚àí0.010 ‚Üì‚àí0.036 ‚Üì‚àí0.018\nŒî‚àíLLaMA2ùëçùëÜ_ùëêùëúùëõùë°ùëíùë•ùë° ‚Üì‚àí0.062 ‚Üë+0.014 ‚Üì‚àí0.019 ‚Üë+0.000 ‚Üë+0.031 ‚Üë+0.106 ‚Üë+0.012\nŒî‚àíLLaMA2ùëçùëÜ_ùëö‚Ñé ‚Üì‚àí0.102 ‚Üë+0.018 ‚Üì‚àí0.033 ‚Üë+0.053 ‚Üë+0.004 ‚Üë+0.031 ‚Üì‚àí0.005\nŒî‚àíLLaMA2ùëçùëÜ_ùëèùëúùë°‚Ñé ‚Üì‚àí0.136 ‚Üë+0.011 ‚Üë+0.016 ‚Üë+0.054 ‚Üì‚àí0.002 ‚Üë+0.067 ‚Üë+0.002\nŒî‚àíGPT-3.5ùëçùëÜ_ùëêùëúùëõùë°ùëíùë•ùë° ‚Üë+0.003 ‚Üë+0.011 ‚Üì‚àí0.060 ‚Üë+0.157 ‚Üë+0.007 ‚Üë+0.031 ‚Üë+0.025\nŒî‚àíGPT-3.5ùëçùëÜ_ùëö‚Ñé ‚Üì‚àí0.006 ‚Üì‚àí0.006 ‚Üë+0.039 ‚Üë+0.116 ‚Üì‚àí0.093 ‚Üë+0.077 ‚Üë+0.021\nŒî‚àíGPT-3.5ùëçùëÜ_ùëèùëúùë°‚Ñé ‚Üì‚àí0.005 ‚Üì‚àí0.015 ‚Üë+0.014 ‚Üë+0.172 ‚Üë+0.047 ‚Üë+0.020 ‚Üë+0.039\nŒî‚àíGPT-4ùëçùëÜ_ùëêùëúùëõùë°ùëíùë•ùë° ‚Üë+0.006 ‚Üë+0.000 ‚Üë+0.001 ‚Üë+0.000 ‚Üì‚àí0.007 ‚Üë+0.023 ‚Üë+0.004\nŒî‚àíGPT-4ùëçùëÜ_ùëö‚Ñé ‚Üë+0.025 ‚Üì‚àí0.035 ‚Üë+0.067 ‚Üë+0.002 ‚Üì‚àí0.023 ‚Üì‚àí0.022 ‚Üë+0.002\nŒî‚àíGPT-4ùëçùëÜ_ùëèùëúùë°‚Ñé ‚Üë+0.018 ‚Üì‚àí0.031 ‚Üë+0.061 ‚Üë+0.003 ‚Üì‚àí0.063 ‚Üì‚àí0.006 ‚Üì‚àí0.003\nŒî‚àíAll Six Models ‚Üì‚àí0.031 ‚Üë+0.000 ‚Üì‚àí0.011 ‚Üë+0.032 ‚Üì‚àí0.006 ‚Üë+0.017 ‚Üë+0.000\nŒî‚àíAlpaca, GPT-3.5, GPT-4‚Üë+0.001 ‚Üë+0.007 ‚Üë+0.017 ‚Üë+0.053 ‚Üì‚àí0.013 ‚Üë+0.015 ‚Üë+0.013\n‚Ä¢Providing more contextual information about the task & input can consistently improve perfor-\nmance in most cases.\n‚Ä¢Dialogue-focused models with larger trainable parameters (Alpaca vs. Alpaca-LoRA, as well as\nLLaMA2/GPT-3.5 vs. Alpaca) can better leverage the contextual or domain information in the\nprompts, yet GPT-4 shows less effect in response to different prompts.\n5.2 Few-shot Prompting Improves Performance to Some Extent\nWe then investigate the effectiveness of few-shot prompting. Note that since we observe diverse effects of prompt\ndesign strategies in Table 5, in this section, we only experiment with the prompts with the best performance in\nthe zero-shot setting. Moreover, we exclude Alpaca-LoRA due to its less promising results and LLaMA2 due to its\nhigh computation cost.\nDue to the maximum input token length of models (2048), we focus on Dreaddit and DepSeverity datasets that\nhave a shorter input and experiment with ùëÄ = 2 in Eq. 2 for binary classification and ùëÄ = 4/5 for multi-class\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\n32:14 ‚Ä¢ Xu et al.\nclassification, i.e., one sample per class. We repeat the experiment on each task three times and randomize the\nfew shot samples for each run.\nWe summarize the overall results in the second section of Table 4 and the zoom-in comparison results in Table 6.\nAlthough language models with few-shot prompting still underperform task-specific models, providing examples\nof the task can improve model performance on most tasks compared to zero-shot prompting (ŒîùêπùëÜ_ùë£ùë†_ùëçùëÜ = 4.1%).\nInterestingly, few-shot prompting is more effective for Alpaca ùêπùëÜ and FLAN-T5ùêπùëÜ (ŒîAlpaca = 8.1%, 3/3 tasks\npositive; ŒîFLAN-T5 = 5.9%, 3/3 tasks positive) than GPT-3.5ùêπùëÜ and GPT-4ùêπùëÜ (ŒîGPT-3.5 = 1.2%, 2/3 tasks positive;\nŒîGPT-4 = 0.9%, 2/3 tasks positive). Especially for Task #3, we observe an improved balanced accuracy of 19.7% for\nAlpaca but a decline of 2.3% for GPT-3.5, so that Alpaca outperforms GPT-3.5 on this task. A similar situation is\nobserved for FLAN-T5 (improved by 12.7%) and GPT-4 (declined by 0.2%) on Task #1. This may be attributed to\nthe fact that smaller models such as Alpaca and FLAN-T5 can quickly adapt to complex tasks with only a few\nexamples. In contrast, larger models like GPT-3.5 and GPT-4, with their extensive ‚Äúin memory‚Äù data, find it more\nchallenging to rapidly learn from new examples.\nThis leads to the key message from this experiment: Few-shot prompting can improve the performance\nof LLMs on mental health prediction tasks to some extent, especially for small models.\n5.3 Instruction Finetuning Boost Performance for Multiple Tasks Simultaneously\nOur experiments so far have shown that zero-shot and few-shot prompting can improve LLMs on mental health\ntasks to some extent, but their overall performance is still below state-of-the-art task-specific models. In this\nsection, we explore the effectiveness of instruction finetuning.\nDue to the prohibitive cost and lack of transparency of GPT-3.5 and GPT-4 finetuning, we only experiment\nwith Alpaca and FLAN-T5 that we have full control of. We picked the most informative prompt to provide more\nembedded knowledge during the finetuning. As introduced in Sec. 3.3.2 and Sec. 4.2.1, we build Mental-Alpaca\nand Mental-FLAN-T5 by finetuning Alpaca and FLAN-T5 on all six tasks across four datasets at the same time.\nThe third section of Table 4 summarizes the overall results, and Table 7 highlights the key comparisons.\nWe observe that both Mental-Alpaca and Mental-FLAN-T5 achieve significantly better performance com-\npared to the unfinetuned versions (ŒîAlpaca‚àíùêπùëá_ùë£ùë†_ùëçùëÜ = 23.4%, ŒîAlpaca‚àíùêπùëá_ùë£ùë†_ùêπùëÜ = 18.3%; ŒîFLAN-T5‚àíùêπùëá_ùë£ùë†_ùëçùëÜ = 14.7%,\nŒîFLAN-T5‚àíùêπùëá_ùë£ùë†_ùêπùëÜ = 14.0%). Both finetuned models surpass GPT-3.5‚Äôs best performance among zero-shot and\nfew-shot settings across all six tasks (ŒîMental-Alpaca_ùë£ùë†_GPT-3.5 = 10.1%; ŒîMental-FLAN-T5_ùë£ùë†_GPT-3.5 = 11.6%) and outper-\nform GPT-4‚Äôs best version in most tasks (ŒîMental-Alpaca_ùë£ùë†_GPT-4 = 4.0%, 4/6 tasks positive; ŒîMental-FLAN-T5_ùë£ùë†_GPT-4 =\nTable 6. Balanced Accuracy Performance Change with Few-shot Prompting. This table is calculated between the zero-shot\nand the few-shot sections of Table 4.\nDataset Dreaddit DepSeverity\nModel Task #1 Task #2 Task #3 Œî‚àíAll Three Tasks\nŒî‚àíAlpacaùêπùëÜ_ùë£ùë†_ùëçùëÜ ‚Üë+0.039 ‚Üë+0.007 ‚Üë+0.197 ‚Üë+0.081\nŒî‚àíFLAN-T5ùêπùëÜ_ùë£ùë†_ùëçùëÜ ‚Üë+0.127 ‚Üë+0.014 ‚Üë+0.036 ‚Üë+0.059\nŒî‚àíGPT-3.5ùêπùëÜ_ùë£ùë†_ùëçùëÜ ‚Üë+0.036 ‚Üë+0.023 ‚Üì‚àí0.023 ‚Üë+0.012\nŒî‚àíGPT-4ùêπùëÜ_ùë£ùë†_ùëçùëÜ ‚Üì‚àí0.002 ‚Üë+0.005 ‚Üë+0.025 ‚Üë+0.009\nŒî‚àíAll Four Models ‚Üë+0.051 ‚Üë+0.012 ‚Üë+0.059 ‚Üë+0.041\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\nMental-LLM ‚Ä¢ 32:15\n5.5%, 5/6 tasks positive). Recall that GPT-3.5/GPT-4 are 25/250 times bigger than Mental-Alpaca and 15/150 times\nbigger than Mental-FLAN-T5.\nMore importantly, Mental-Alpaca and Mental-FLAN-T5 perform on par with the state-of-the-art Mental-\nRoBERTa. Mental-Alpaca has the best performance on one task and the second best on three tasks, while\nMental-FLAN-T5 has the best performance on three tasks. It is noteworthy that Mental-RoBERTa is a task-specific\nmodel, which means it is specialized on one task after being trained on it. In contrast, Mental-Alpaca and Mental-\nFLAN-T5 can simultaneously work across all tasks with a single-round finetuning. These results show the strong\neffectiveness of instruction finetuning: By finetuning LLMs on multiple mental health datasets with instructions,\nthe models can obtain better capability to solve a variety of mental health prediction tasks.\n5.3.1 Dialogue-Focused vs. Task-Solving-Focused LLMs. We further compare Mental-Alpaca and Mental-FLAN-T5.\nOverall, their performance is quite close ( ŒîFLAN-T5_ùë£ùë†_Alpaca = 1.4%), with Mental-Alapca better at Task #4 on\nSDCNL and Mental-FLAN-T5 better at Task #5 and #6 on CSSRS-Suicide. In Sec. 5.1, we observe that FLAN-T5ùëçùëÜ\nhas a much better performance than Alpaca ùëçùëÜ (ŒîFLAN-T5_ùë£ùë†_Alpaca = 10.9%, 5/6 tasks positive) in the zero-shot\nsetting. However, after finetuning, FLAN-T5‚Äôs advantage disappears.\nOur comparison result indicates that Alpaca, as a dialogue-focused model, is better at learning from human\nnatural language data compared to FLAN-T5. Although FLAN-T5 is good at task solving and thus has a better\nperformance in the zero-shot setting, its performance improvement after instruction finetuning is relatively\nsmaller than that of Alpaca. This observation has implications for future stakeholders. If the data and computing\nresources for finetuning are not available, using task-solving-focused LLMs could lead to better results. When\nthere are enough data and computing resources, finetuning dialogue-based models can be a better choice. Further-\nmore, models like Alpaca, with their dialogue conversation capabilities, may be more suitable for downstream\napplications, such as mental well-being assistants for end-users.\n5.3.2 Does Finetuning Generalize across Datasets? We further measure the generalizability of LLMs after finetun-\ning. To do this, we instruction-finetune the model on one dataset and evaluate it on all datasets (as introduced\nTable 7. Balanced Accuracy Performance Change with Instruction Finetuning. This table is calculated between the finetuning\nand zero-shot section, as well as the finetuning and few-shot section of Table 4. GPT-3.5ùêµùëíùë†ùë° and GPT-4ùêµùëíùë†ùë° are the best\nresults among zero-shot and few-shot settings.\nDataset Dreaddit DepSeverity SDCNL CSSRS-Suicide\nModel Task #1 Task #2 Task #3 Task #4 Task #5 Task #6\nŒî‚àíAlpacaùêπùëá_ùë£ùë†_ùëçùëÜ ‚Üë+0.223 ‚Üë+0.253 ‚Üë+0.315 ‚Üë+0.231 ‚Üë+0.212 ‚Üë+0.171\nŒî‚àíAlpacaùêπùëá_ùë£ùë†_ùêπùëÜ ‚Üë+0.184 ‚Üë+0.246 ‚Üë+0.118 ‚Äî ‚Äî ‚Äî\nŒî‚àíFLAN-T5ùêπùëá_ùë£ùë†_ùëçùëÜ ‚Üë+0.143 ‚Üë+0.095 ‚Üë+0.360 ‚Üë+0.047 ‚Üë+0.201 ‚Üë+0.034\nŒî‚àíFLAN-T5ùêπùëá_ùë£ùë†_ùêπùëÜ ‚Üë+0.016 ‚Üë+0.081 ‚Üë+0.324 ‚Äî ‚Äî ‚Äî\nResults comparison:\nMental-RoBERTa 0.831 0.790 0.736 0.723 0.853 0.373\nGPT-3.5ùêµùëíùë†ùë° 0.721 0.665 0.642 0.632 0.617 0.310\nGPT-4ùêµùëíùë†ùë° 0.725 0.724 0.656 0.647 0.760 0.441\nMental-Alpaca 0.816 0.775 0.746 0.724 0.730 0.403\nMental-FLAN-T5 0.802 0.759 0.756 0.677 0.868 0.481\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\n32:16 ‚Ä¢ Xu et al.\nin Sec. 3.3.1). As the main purpose of this part is not to compare different models but evaluate the finetuning\nmethod, we only focus on Alpaca. Table 8 summarizes the results.\nWe first find that finetuning and testing on the same dataset lead to good performance, as indicated by the\nboxed entries on the diagonal in Table 8. Some results are even better than Mental-Alpaca (5 out of 6 tasks)\nor Mental-RoBERTa (3 out of 6 tasks), which is not surprising. More interestingly, we investigate cross-dataset\ngeneralization performance (i.e., the ones off the diagonal). Overall, finetuning on a single dataset achieves better\nperformance compared to the zero-shot setting (Œîùêπùëá-Single_ùë£ùë†_ùëçùëÜ = 4.2%). However, the performance changes vary\nacross tasks. For example, finetuning on any dataset is beneficial for Task #3 (Œî = 19.2%) and #5 (Œî = 16.4%), but\ndetrimental for Task #6 (Œî = ‚àí7.6%) and almost futile for Task #4 (Œî = ‚àí0.4%). Generalizing across Dreaddit and\nDepSeverity shows good performance, but this is mainly because they share the language corpus. These results\nindicate that finetuning on a single dataset can provide mental health knowledge with a certain level and thus\nimprove the overall generalization results, but such improvement is not stable across tasks.\nMoreover, we further evaluate the generalizability of our best models instructional-finetuned on multiple\ndatasets, i.e., Mental-Alpaca and Mental-FLAN-T5. We leverage external datasets that are not included in the\nfinetuning. Table 9 highlights the key results. More detailed results can be found in Table 10.\nConsistent with the results in Table 7, the instruction finetuning enhances the model performance on external\ndatasets (Œîùê¥ùëôùëùùëéùëêùëé = 16.3%, Œîùêπùêøùê¥ùëÅ‚àíùëá5 = 5.1%). Both Mental-Alpaca and Mental-FLAN-T5 ranked top 1 or 2 in 2/3\nexternal tasks. It is noteworthy that Twt-60Users and SAD datasets are collected outside Reddit, and their data\nis different from the source of finetuning datasets. These results demonstrate strong evidence that instruction\nfinetuning with diverse tasks, even with data collected from a single social media platform, can significantly\nenhance LLMs‚Äô generalizability across multiple scenarios.\n5.3.3 How Much Data Is Needed? Additionally, we are interested in exploring how the size of the dataset impacts\nthe results of instruction finetuning. To answer this question, we downsample the training set to 50%, 20%, 10%,\n5%, and 1% of the original size and repeat each one three times. We increase the training epoch accordingly to\nmake sure that the model is exposed to a similar amount of data. Similarly, we also focus on Alpaca only. Figure 1\nvisualizes the results. With only 1% of the data, the finetuned model is able to outperform the zero-shot model\non most tasks (5 out of 6). With 5% of the data, the finetuned model has a better performance on all tasks. As\nexpected, the model performance has an increasing trend with more training data. For many tasks, the trend\nTable 8. Balanced Accuracy Cross-Dataset Performance Summary of Mental-Alpaca Finetuning on Single Dataset. Numbers\nindicate the results of the model finetuned and tested on the same dataset. The bottom few rows are related Alpaca versions\nfor reference. ‚Üë/‚Üìmarks the ones with better/worse cross-dataset performance compared to the zero-shot version AlpacaùëçùëÜ.\nTest Dataset Dreaddit DepSeverity SDCNL CSSRS-Suicide\nFinetune Dataset Task #1 Task #2 Task #3 Task #4 Task #5 Task #6\nDreaddit 0.823 ‚Üë0.720 ‚Üë0.623 ‚Üì0.474 ‚Üë0.720 ‚Üì0.156\nDepSeverity ‚Üë0.618 0.733 0.769 |0.493 ‚Üë0.753 ‚Üì0.156\nSDCNL ‚Üì0.468 ‚Üì0.461 ‚Üë0.623 0.730 ‚Üë0.573 ‚Üì0.156\nCSSRS-Suicide ‚Üì0.500 ‚Üì0.500 ‚Üë0.622 ‚Üë0.500 0.753 0.578\nReference:\nAlpacaùëçùëÜ 0.593 0.522 0.431 0.493 0.518 0.232\nMental-Alpaca 0.816 0.775 0.746 0.724 0.730 0.403\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\nMental-LLM ‚Ä¢ 32:17\nTable 9. Balanced Accuracy Performance Summary on Three External Datasets. These datasets come from diverse social\nmedia platforms. For each column, the best result is bolded, and the second best is underlined.\nDataset Red-Sam Twt-60Users SAD\nCategory Model Task #2 Task #2 Task #1\nZero-shot\nPrompting\nAlpacaùëçùëÜ_ùëèùëíùë†ùë° 0.527¬±0.006 0.569¬±0.017 0.557¬±0.041\nAlpaca-LoRAùëçùëÜ_ùëèùëíùë†ùë° 0.577¬±0.004 0.649¬±0.021 0.477¬±0.016\nFLAN-T5ùëçùëÜ_ùëèùëíùë†ùë° 0.563¬±0.029 0.613¬±0.046 0.767¬±0.050\nLLaMA2ùëçùëÜ_ùëèùëíùë†ùë° 0.574¬±0.008 0.736¬±0.019 0.704¬±0.026\nGPT-3.5ùëçùëÜ_ùëèùëíùë†ùë° 0.506¬±0.004 0.571¬±0.000 0.750¬±0.027\nGPT-4ùëçùëÜ_ùëèùëíùë†ùë° 0.511¬±0.000 0.566¬±0.017 0.854¬±0.006\nInstructional\nFinetuning\nMental-Alpaca 0.604¬±0.012 0.718¬±0.011 0.819¬±0.006\nŒî‚àíAlpacaùêπùëá_ùë£ùë†_ùëçùëÜ ‚Üë+0.077 ‚Üë+0.149 ‚Üë+0.262\nMental-FLAN-T5 0.582¬±0.002 0.736¬±0.003 0.779¬±0.002\nŒî‚àíFLAN-T5ùêπùëá_ùë£ùë†_ùëçùëÜ ‚Üë+0.019 ‚Üë+0.123 ‚Üë+0.012\nFig. 1. Balanced Accuracy Performance Summary of Mental-Alpaca Finetuning with Different Sizes of Training Set. The\nfinetuning is conducted across four datasets and six tasks. Each solid line represents the performance of the finetuned model\non each task. The dashed line indicates the AlpacaùëçùëÜ performance baseline. Note that the x-axis is in the log scale.\napproaches a plateau after 10%. The difference between 10% training data (less than 300 samples per dataset) and\n100% training data is not huge (Œî = 5.9%).\n5.3.4 More Data in One Dataset vs. Fewer Data across Multiple Datasets. In Sec. 5.3.2, the finetuning on a single\ndataset can be viewed as training on a smaller set (around 5-25% of the original size) with less variation (i.e., no\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\n32:18 ‚Ä¢ Xu et al.\nfinetuning across datasets). Thus, the results in Sec. 5.3.2 are comparable to those in Sec. 5.3.3. We found that\nthe model‚Äôs overall performance is better when the model is finetuned across multiple datasets when overall\ntraining data sizes are similar (Œîùêπùëá-5%_ùë£ùë†_ùêπùëá-Single = 3.8%, Œîùêπùëá-10%_ùë£ùë†_ùêπùëá-Single = 8.1%, Œîùêπùëá-20%_ùë£ùë†_ùêπùëá-Single = 12.4%).\nThis suggests that increasing data variation can more effectively benefit finetuning outcomes when the training\ndata size is fixed.\nThese results can guide future developers and practitioners in collecting the appropriate data size and sources\nto finetune LLMs for the mental health domain efficiently. We have more discussion in the next section. In\nsummary, we highlight the key takeaways of our finetuning experiments as follows:\n‚Ä¢Instruction finetuning on multiple mental health datasets can significantly boost the perfor-\nmance of LLMs on various mental health prediction tasks. Mental-Alpaca and Mental-FLAN-T5\noutperform GPT-3.5 and GPT-4, and perform on par with the state-of-the-art task-specific model.\n‚Ä¢Although task-solving-focused LLMs may have better performance in the zero-shot setting for\nmental health prediction tasks, dialogue-focused LLMs have a stronger capability of learning\nfrom human natural language and can improve more significantly after finetuning.\n‚Ä¢Finetuning LLMs on a small number of datasets and tasks may improve model generalizable\nknowledge in mental health, but its effect is not robust. Comparatively, finetuning on diverse\ntasks can robustly enhance generalizability across multiple social media platforms.\n‚Ä¢Finetuning LLMs on a small number of samples (a few hundred) across multiple datasets can\nalready achieve favorable performance.\n‚Ä¢When the data size is the same, finetuning LLMs on data with larger variation ( i.e., more datasets\nand tasks) can achieve better performance.\n5.4 Case Study of LLMs‚Äô Capability on Mental Health Reasoning\nIn addition to evaluating LLMs‚Äô performance on classification tasks, we also take an initial step to explore\nLLMs‚Äô capability on mental health reasoning. This is another strong advantage of LLMs since they can generate\nhuman-like natural language based on embedded knowledge. Due to the high cost of a systematic evaluation of\nreasoning outcomes, here we present a few examples as a case study across different LLMs. It is noteworthy that\nwe do not aim to claim that certain LLMs have better/worse reasoning capabilities. Instead, this section aims to\nprovide a general sense of LLMs‚Äô performance on mental health reasoning tasks.\nSpecifically, we modify the prompt design by inserting a Chain-of-Thought (CoT) prompt [ 65] at the end\nof OutputConstraint in Eq. 1: ‚ÄúReturn [set of classification labels]. Provide reasons step by step‚Äù. We compare\nAlpaca, FLAN-T5, GPT-3.5, and GPT-4. Our results indicate the promising reasoning capability of these models,\nespecially GPT-3.5 and GPRT-4. We also experimented with the finetuned Mental-Alpaca and Mental-FLAN-T5.\nUnfortunately, our results show that after finetuning solely on classification tasks, these two models are no\nlonger able to generate reasoning sentences even with the CoT prompt. This suggests a limitation of the current\nfinetuned model.\n5.4.1 Diverse Reasoning Capabilities across LLMs. We present several examples as our case study to illustrate the\nreasoning capability of these LLMs. The first example comes from the binary stress prediction task (Task #1)\non the Dreaddit dataset (see Figure 2). All models give the right classification, but with significantly different\nreasoning capabilities. First, FLAN-T5 generates the shortest reason. Although it is reasonable, it is superficial\nand does not provide enough insights. This is understandable because FLAN-T5 is targeted at task-solving instead\nof reasoning. Compared to FLAN-T5, Alpaca generates better reason. Among the five reasons, two of them\naccurately analyze the user‚Äôs mental state given the stressful situations. Meanwhile, GPT-3.5 and GPT-4 generate\nexpert-level high-quality reasons. The inference from the user‚Äôs statement is accurate and deep, indicating their\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\nMental-LLM ‚Ä¢ 32:19\npowerful capability of understanding human emotion and mental health. Comparing the two models, GPT-3.5‚Äôs\nreason is simpler, following the user‚Äôs statement point by point and adding basic comments, while GPT-4‚Äôs output\nis more organic and insightful, yet more concise.\nFig. 2. A Case Study of Correct Reasoning Examples on Task #1 Binary Stress Prediction on Dreaddit Dataset. Bolded texts\nhighlight the mental-health-related content in the input section, and the answers of LLMs. Underlined texts highlight the\nreasoning content generated by LLMs, and italicized & underlined texts indicate the wrong or unrelated content.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\n32:20 ‚Ä¢ Xu et al.\nFig. 3. A Case Study of Mixed Reasoning Examples on Task #3 Four-level Depression Prediction on DepSeverity Dataset.\nAlpaca wrongly predicted the label, and GPT-3.5 provided a wrong inference on the meaning of ‚Äúrelief‚Äù.\nWe also have a similar observation in the second example from the four-level depression prediction task\n(Task #3) on the DepSeverity dataset (see Figure 3). In this example, although FLAN-T5‚Äôs prediction is correct, it\nsimply repeats the fact stated by the user, without providing further insights. Alpaca makes the wrong prediction,\nbut it provides one sentence of accurate reasoning (although relatively shallow). GPT-3.5 makes an ambiguous\nprediction that includes the correct answer. In contrast, GPT-4 generates the highest quality reasoning with the\nright prediction. With its correct understanding of depressive symptoms, GPT-4 can accurately infer from the\nuser‚Äôs situation, link it to symptoms, and provides insightful analysis.\n5.4.2 Wrong and Dangerous Reasoning from LLMs. However, we also want to emphasize the incorrect reasoning\ncontent, which may lead to negative consequences and risks. In the first example, Alpaca generated two wrong\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\nMental-LLM ‚Ä¢ 32:21\nFig. 4. A Case Study of Incorrect Reasoning Examples on Task #1 Binary Stress Prediction on Dreaddit Dataset. FLAN-T5,\nGPT-3.5, and GPT-4 all make false positive predictions. All four LLMs provide problematic reasons.\nreasons for the hallucinated ‚Äúreliance on others‚Äù and ‚Äúsafety concerns‚Äù, along with an unrelated reason for\nreaders instead of the poster. In the second example, GPT-3.5 misunderstood what the user meant by ‚Äúrelief‚Äù. To\nbetter illustrate this, we further present another example where all four LLMs make problematic reasoning (see\nFigure 4). In this example, the user was asking for others‚Äô opinions on social anxiety, with their own job interview\nexperience as an example. Although the user mentioned situations where they were anxious and stressed, it‚Äôs\nclear that they were calm when writing this post and described their experience in an objective way. However,\nFLAN-T5, GPT-3.5, and GPT-4 all mistakenly take the description of the anxious interview experience as evidence\nto support their wrong prediction. Although Alpaca makes the right prediction, it does not understand the main\ntheme of the post. The false positives reveal that LLMs may overly generalize in a wrong way: Being stressed in\none situation does not indicate that a person is stressed all the time. However, the reasoning content alone reads\nsmoothly and logically. If the original post was not provided, the content could be very misleading, resulting in\na wrong prediction with reasons that ‚Äúappears to be solid‚Äù. These examples clearly illustrate the limitations of\nthe current LLMs for mental health reasons, as well as their risks of introducing dangerous bias and negative\nconsequences to users.\nThe case study suggests that GPT-4 enjoys impressive reasoning capability, followed by GPT-3.5 and Alpaca.\nAlthough FLAN-T5 shows a promising zero-shot performance, it is not good at reasoning. Our results reveal the\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\n32:22 ‚Ä¢ Xu et al.\nencouraging capability of LLMs to understand human mental health and generate meaningful analysis. However,\nwe also present examples where LLMs can make mistakes and offer explanations that appear reasonable but are\nactually flawed. This further suggests the importance of more future research on LLMs‚Äô ethical concerns and\nsafety issues before real-world deployment.\n6 DISCUSSION\nOur experiment results reveal a number of interesting findings. In this section, we discuss potential guidelines\nfor enabling LLMs for mental -health-related tasks (Sec. 6.1). We envision promising future directions (Sec. 6.2),\nwhile highlighting important ethical concerns and limitations with LLMs for mental health (Sec. 6.3). We also\nsummarize the limitations of the current work (Sec. 6.4).\n6.1 Guidelines for Empowering LLMs for Mental Health Prediction Tasks\nWe extract and summarize the takeaways from Sec. 5 into a set of guidelines for future researchers and practitioners\non how to empower LLMs to be better at various mental health prediction tasks.\nWhen computing resources are limited, combine prompt design & few-shot prompting, and pick\nprompts carefully. As the size of large models continues to grow, the requirement for hardware (mainly GPU)\nhas also been increasing, especially when finetuning an LLM. For example, in our experiment, Alpaca was\ntrained on eight 80GB A100s for three hours [115]. With limited computing resources, only running inferences\nor resorting to APIs is feasible. In these cases, zero-shot and few-shot prompt engineering strategies are viable\noptions. Our results indicate that providing few-shot mental health examples with appropriate enhancement\nstrategies can effectively improve prediction performance. Specifically, adding contextual information about\nthe online text data is always helpful. If the available model is large and contains rich knowledge (at least 7B\ntrainable parameters), adding mental health domain information can also be beneficial.\nWith enough computing resources, instruction finetune models on various mental health datasets.\nWhen there are enough computing resources and model training/finetuning is possible, there are more options\nto enhance LLMs for mental health prediction tasks. Our experiments clearly show that instruction finetuning\ncan significantly boost the performance of models, especially for dialogue-based models since they can better\nunderstand and learn from human natural language. When there are multiple datasets available, merging multiple\ndatasets and tasks altogether and finetuning the model in a single round is the most effective approach to enhance\nits generalizability.\nImplement efficient finetuning with hundreds of examples and prioritize data variation when data\nresource is limited. Figure 1 shows that finetuning does not require large datasets. If there is no immediately\navailable dataset, collecting small datasets with a few hundred samples is often good enough. Moreover, when\nthe overall amount of data is limited (e.g., due to resource constraints), it is more advantageous to collect data\nfrom a variety of sources, each with a smaller size, than to collect a single larger dataset. Because instruction\nfinetuning generalizes better when data and tasks have a larger variation.\nMore curated finetuning datasets are needed for mental health reasoning. Our case study suggests\nthat Mental-Alpaca and Mental-FLAN-T5 can only generate classification labels after being finetuned solely on\nclassification tasks, losing their reasoning capability. This is a major limitation of the current models. A potential\nsolution involves incorporating more datasets focused on reasoning or causality into the instruction finetuning\nprocess, so that models can also learn the relationship between mental health outcomes and causal factors.\nLimited Prediction and Reasoning Performance for Complex Contexts. LLMs tend to make more\nmistakes when the conversation contexts are more complex [13, 69]. Our results contextualize this in the mental\nhealth domain. Section 5.4.2 shows an example case where most LLMs not only predict incorrectly but also\nprovide flawed reasoning processes. Further analysis of mispredicted instances indicates a recurring difficulty:\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\nMental-LLM ‚Ä¢ 32:23\nLLMs often err when there‚Äôs a disconnect between the literal context and the underlying real-life scenarios.\nhe example in Figure 4 is a case where LLMs are confused by the hypothetical stressful case described by the\nperson. In another example, all LLMs incorrectly assess a person with severe depression (false negative): ‚ÄúI‚Äôm\njust blown away by this doctor‚Äôs willingness to help. I feel so validated every time I leave his office, like someone\nactually understands what I‚Äôm struggling with, and I don‚Äôt have to convince them of my mental illness. Bottom line?\nResearch docs if you can online, read their reviews and don‚Äôt give up until you find someone who treats you the\nway you deserve. If I can do this, I promise you can! ‚Äù Here, LLMs are misled by the outwardly positive sentiment,\noverlooking the significant cues such as regular doctor visits and explicit mentions of mental illness. These\nobservations underscore a critical shortfall of LLMs: they cannot handle complex mental health-related tasks,\nparticularly those concerning chronic conditions like depression. The variability of human expressions over\ntime and the models‚Äô susceptibility to being swayed by superficial text rather than underlying scenarios present\nsignificant challenges.\nDespite the promising capability of LLMs for mental health tasks, they are still far from being deployable in\nthe real world. Our experiments reveal the encouraging performance of LLMs on mental health prediction and\nreasoning tasks. However, as we note in Sec. 6.3, our current results do not indicate LLMs‚Äô deployability in real-life\nmental health settings. There are many important ethical and safety concerns and gaps before deployment to be\naddressed before achieving robustness and deployability.\n6.2 Beyond Mental Health Prediction Task and Online Text Data\nOur current experiments mainly involve mental health prediction tasks, which are essentially classification\nproblems. There are more types of tasks that our experiments don‚Äôt cover, such as regression (e.g., predicting a\nscore on a mental health scale). In particular, reasoning is an attractive task as it can fully leverage the capability\nof LLMs on language generation [18, 85]. Our initial case study on reasoning is limited but reveals promising\nperformance, especially for large models such as GPT-4. We plan to conduct more experiments on tasks that go\nbeyond classification.\nIn addition, there is another potential extension direction. In this paper, we mainly focus on online text data,\nwhich is one of the important data sources of the ubiquitous computing ecosystem. However, there are more\navailable data streams that contain rich information, such as the multimodal sensor data from mobile phones and\nwearables (e.g., [55, 78, 81, 130, 131]). This leads to another open question on how to leverage LLMs for time-series\nsensor data. More research is needed to explore potential methods to merge the online text information with\nsensor streams. These are another set of exciting research questions to explore in future work.\n6.3 Ethics in LLMs and Deployability Gaps for Mental Health\nAlthough our experiments on LLMs have shown promising capability for mental-health-related tasks, it still\nhas a long way to go before being deployed in real-life systems. Recent research has revealed the potential bias\nor even harmful advice introduced by LLMs [50], especially with the gender [42] and racial [6] gaps. In mental\nhealth, these gaps and disparities between population groups have been long-standing [54]. Meanwhile, our case\nstudy of incorrect prediction, over-generalization, and ‚Äúfalsely reasonable‚Äù explanations further highlight the risk\nof current LLMs. Recent studies are calling for more research emphasis and efforts in assessing and mitigating\nthese biases for mental health [54, 116].\nAlthough with a much stronger capability of understanding natural language (and early signs of mental health\ndomain knowledge in our case), LLMs are no different from other modern AI models that are trained on a large\namount of human-generated content, which exhibit all the biases that humans do [ 53, 86, 121]. Meanwhile,\nalthough we carefully picked datasets with human expert annotations, there exist potential biases in the labels,\nsuch as stereotypes [92], confirmation bias [41], normative vs. descriptive labels [11]. Besides, privacy is another\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\n32:24 ‚Ä¢ Xu et al.\nimportant concern. Although our datasets are based on public social media platforms, it is necessary to carefully\nhandle mental-health-related data and guarantee anonymity in any future efforts. These ethical concerns need\nto receive attention not only at the monitoring and prediction stage, but also in the downstream applications,\nranging from assistants for mental health experts to chatbots for end-users. Careful efforts into safe development,\nauditing, and regulation are very much needed to address these ethical risks.\n6.4 Limitations\nOur paper has a few limitations. First, although we carefully inspect the quality of our dataset and cover different\ncategories of LLM, the the range of datasets and the types of LLMs included are still limited. Our findings are\nbased on the observations of these datasets and models, which may not generalize to other cases. Related, our\nexploration of zero-shot few-shot prompt design is not comprehensive. The limited input window of some\nmodels also limits our exploration of more samples for few-shot prompting. Furthermore, we have not conducted\na systematic evaluation of these models‚Äô performance in mental health reasoning. Future work can design\nlarger-scale experiments to include more datasets, models, prompt designs, and better evaluation.\nSecond, our datasets were mainly from Reddit, which could be limited. Although our analysis in Section 5.3.2\nshows that finetuned models have cross-platform generalizability, the finetuning was only based on Reddit and\ncan introduce bias. Meanwhile, although the labels are not directly accessible on the platforms, it is possible that\nthese text data have been included in the initial training of these large models. We still argue that there is little\ninformation leakage as long as the models haven‚Äôt seen the labels for our tasks, but it is hard to measure how the\ninitial training process may affect the outcomes in our evaluation.\nThird, another important limitation of the current work is the lack of evaluation of model fairness. Our\nanonymous datasets do not include comprehensive demographic information, making it hard to compare the\nperformance across different population groups. As we discussed in the previous section, lots of future work on\nethics and fairness is necessary before deploying such systems in real life.\n7 CONCLUSION\nIn this paper, we present the first comprehensive evaluation of multiple LLMs (Alpaca, Alpaca-LoRA, FLAN-\nT5, LLaMA2, GPT-3.5, and GPT-4) on mental health prediction tasks (binary and multi-class classification) via\nonline text data. Our experiments cover zero-shot prompting, few-shot prompting, and instruction finetuning.\nThe results reveal a number of interesting findings. Our context enhancement strategy can robustly improve\nperformance for all LLMs, and our mental health enhancement strategy can enhance models with a large number\nof trainable parameters. Meanwhile, few-shot prompting can also robustly improve model performance even by\nproviding just one example per class. Most importantly, our experiments show that instruction finetuning across\nmultiple datasets can significantly boost model performance on various mental health prediction tasks at the\nsame time, generalizing across external data sources and platforms.. Our best finetuned models, Mental-Alpaca\nand Mental-FLAN-T5, outperform much larger LLaMA2, GPT-3.5 and GPT-4, and perform on par with the\nstate-of-the-art task-specific model Mental-RoBERTa. We also conduct an exploratory case study on these models‚Äô\nreasoning capability, which further suggests both the promising future and the important limitations of LLMs.\nWe summarize our findings as a set of guidelines for future researchers, developers, and practitioners who want\nto empower LLMs with better knowledge of mental health for downstream tasks. Meanwhile, we emphasize that\nour current efforts of LLMs in mental health are still far from deployability. We highlight the important ethical\nconcerns accompanying this line of research.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\nMental-LLM ‚Ä¢ 32:25\nACKNOWLEDGMENTS\nThis work is supported by VW Foundation, Quanta Computing, and the National Institutes of Health (NIH) under\nGrant No. 1R01MD018424-01.\nREFERENCES\n[1] 2022. Introducing ChatGPT. https://openai.com/blog/chatgpt\n[2] 2023. Mental Health By the Numbers. https://nami.org/mhstats\n[3] 2023. Mental Illness. https://www.nimh.nih.gov/health/statistics/mental-illness\n[4] Alaa A. Abd-alrazaq, Mohannad Alajlani, Ali Abdallah Alalwan, Bridgette M. Bewick, Peter Gardner, and Mowafa Househ. 2019. An\noverview of the features of chatbots in mental health: A scoping review. International Journal of Medical Informatics 132 (Dec. 2019),\n103978. https://doi.org/10.1016/j.ijmedinf.2019.103978\n[5] Alaa A Abd-Alrazaq, Mohannad Alajlani, Nashva Ali, Kerstin Denecke, Bridgette M Bewick, and Mowafa Househ. 2021. Perceptions\nand opinions of patients about mental health chatbots: scoping review. Journal of medical Internet research 23, 1 (2021), e17828.\n[6] Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent anti-muslim bias in large language models. In Proceedings of the 2021\nAAAI/ACM Conference on AI, Ethics, and Society . 298‚Äì306.\n[7] Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag. 2022. Large language models are few-shot clinical\ninformation extractors. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing . 1998‚Äì2022.\n[8] Arfan Ahmed, Sarah Aziz, Carla T Toro, Mahmood Alzubaidi, Sara Irshaidat, Hashem Abu Serhan, Alaa A Abd-Alrazaq, and Mowafa\nHouseh. 2022. Machine learning models to detect anxiety and depression through social media: A scoping review. Computer Methods\nand Programs in Biomedicine Update (2022), 100066.\n[9] Mental Health America. 2022. The state of mental health in America.\n[10] Mostafa M. Amin, Erik Cambria, and Bj√∂rn W. Schuller. 2023. Will Affective Computing Emerge from Foundation Models and General\nAI? A First Evaluation on ChatGPT. http://arxiv.org/abs/2303.03186\n[11] Aparna Balagopalan, David Madras, David H. Yang, Dylan Hadfield-Menell, Gillian K. Hadfield, and Marzyeh Ghassemi. 2023. Judging\nfacts, judging norms: Training machine learning models to judge humans requires a modified approach to labeling data. Science\nAdvances 9, 19 (May 2023), eabq0701. https://doi.org/10.1126/sciadv.abq0701\n[12] Adrian Benton, Margaret Mitchell, and Dirk Hovy. 2017. Multi-task learning for mental health using social media text. arXiv preprint\narXiv:1712.03538 (2017).\n[13] Sourangshu Bhattacharya, Avishek Anand, et al. 2023. In-Context Ability Transfer for Question Decomposition in Complex QA. arXiv\npreprint arXiv:2310.18371 (2023).\n[14] Michael L Birnbaum, Sindhu Kiranmai Ernala, Asra F Rizvi, Munmun De Choudhury, and John M Kane. 2017. A collaborative approach\nto identifying social media markers of schizophrenia by employing machine learning and clinical appraisals. Journal of medical Internet\nresearch 19, 8 (2017), e7956.\n[15] Thorsten Brants, Ashok C Popat, Peng Xu, Franz J Och, and Jeffrey Dean. 2007. Large language models in machine translation. (2007).\n[16] Kay Henning Brodersen, Cheng Soon Ong, Klaas Enno Stephan, and Joachim M Buhmann. 2010. The balanced accuracy and its\nposterior distribution. In 2010 20th international conference on pattern recognition . IEEE, 3121‚Äì3124.\n[17] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot\nLearners. In Advances in Neural Information Processing Systems , Vol. 33. Curran Associates, Inc., 1877‚Äì1901. https://proceedings.\nneurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html\n[18] S√©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li,\nScott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of Artificial General Intelligence: Early\nexperiments with GPT-4. http://arxiv.org/abs/2303.12712\n[19] Pete Burnap, Walter Colombo, and Jonathan Scourfield. 2015. Machine Classification and Analysis of Suicide-Related Communication\non Twitter. In Proceedings of the 26th ACM Conference on Hypertext & Social Media - HT ‚Äô15 . ACM Press, Guzelyurt, Northern Cyprus,\n75‚Äì84. https://doi.org/10.1145/2700171.2791023\n[20] Gillian Cameron, David Cameron, Gavin Megaw, Raymond Bond, Maurice Mulvenna, Siobhan O‚ÄôNeill, Cherie Armour, and Michael\nMcTear. 2017. Towards a chatbot for digital counselling. https://doi.org/10.14236/ewic/HCI2017.24\n[21] Gillian Cameron, David Cameron, Gavin Megaw, Raymond Bond, Maurice Mulvenna, Siobhan O‚ÄôNeill, Cherie Armour, and Michael\nMcTear. 2019. Assessing the Usability of a Chatbot for Mental Health Care. In Internet Science , Svetlana S. Bodrunova, Olessia Koltsova,\nAsbj√∏rn F√∏lstad, Harry Halpin, Polina Kolozaridi, Leonid Yuldashev, Anna Smoliarova, and Heiko Niedermayer (Eds.). Vol. 11551.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\n32:26 ‚Ä¢ Xu et al.\nSpringer International Publishing, Cham, 121‚Äì132. https://doi.org/10.1007/978-3-030-17705-8_11 Series Title: Lecture Notes in\nComputer Science.\n[22] Stevie Chancellor and Munmun De Choudhury. 2020. Methods in predictive techniques for mental health status on social media: a\ncritical review. npj Digital Medicine 3, 1 (March 2020), 43. https://doi.org/10.1038/s41746-020-0233-7\n[23] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won\nChung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker\nBarnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski,\nXavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,\nAlexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\nBrennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and\nNoah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways. http://arxiv.org/abs/2204.02311 arXiv:2204.02311 [cs].\n[24] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani,\nSiddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex\nCastro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang,\nAndrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling Instruction-Finetuned Language Models. http://arxiv.org/abs/2210.11416 arXiv:2210.11416 [cs].\n[25] Glen Coppersmith, Mark Dredze, Craig Harman, and Kristy Hollingshead. 2015. From ADHD to SAD: Analyzing the Language of\nMental Health on Twitter through Self-Reported Diagnoses. In Proceedings of the 2nd Workshop on Computational Linguistics and\nClinical Psychology: From Linguistic Signal to Clinical Reality . Association for Computational Linguistics, Denver, Colorado, 1‚Äì10.\nhttps://doi.org/10.3115/v1/W15-1201\n[26] Glen Coppersmith, Mark Dredze, Craig Harman, Kristy Hollingshead, and Margaret Mitchell. 2015. CLPsych 2015 Shared Task:\nDepression and PTSD on Twitter. InProceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic\nSignal to Clinical Reality . Association for Computational Linguistics, Denver, Colorado, 31‚Äì39. https://doi.org/10.3115/v1/W15-1204\n[27] Glen Coppersmith, Craig Harman, and Mark Dredze. 2014. Measuring Post Traumatic Stress Disorder in Twitter. Proceedings of the\nInternational AAAI Conference on Web and Social Media 8, 1 (May 2014), 579‚Äì582. https://doi.org/10.1609/icwsm.v8i1.14574\n[28] Glen Coppersmith, Ryan Leary, Patrick Crutchley, and Alex Fine. 2018. Natural language processing of social media as screening for\nsuicide risk. Biomedical informatics insights 10 (2018), 1178222618792860.\n[29] Glen Coppersmith, Ryan Leary, Patrick Crutchley, and Alex Fine. 2018. Natural Language Processing of Social Media as Screening for\nSuicide Risk. Biomedical Informatics Insights 10 (Jan. 2018), 117822261879286. https://doi.org/10.1177/1178222618792860\n[30] Aron Culotta. 2014. Estimating county health statistics with twitter. In Proceedings of the SIGCHI Conference on Human Factors in\nComputing Systems . ACM, Toronto Ontario Canada, 1335‚Äì1344. https://doi.org/10.1145/2556288.2557139\n[31] Hai Dang, Lukas Mecke, Florian Lehmann, Sven Goller, and Daniel Buschek. 2022. How to prompt? Opportunities and challenges of\nzero-and few-shot learning for human-AI interaction in creative applications of generative models. arXiv preprint arXiv:2209.01390\n(2022).\n[32] Munmun De Choudhury, Scott Counts, and Eric Horvitz. 2013. Social media as a measurement tool of depression in populations. In\nProceedings of the 5th Annual ACM Web Science Conference . ACM, Paris France, 47‚Äì56. https://doi.org/10.1145/2464464.2464480\n[33] Munmun De Choudhury and Sushovan De. 2014. Mental Health Discourse on reddit: Self-Disclosure, Social Support, and Anonymity.\nProceedings of the International AAAI Conference on Web and Social Media 8, 1 (May 2014), 71‚Äì80. https://doi.org/10.1609/icwsm.v8i1.\n14526\n[34] Munmun De Choudhury, Michael Gamon, Scott Counts, and Eric Horvitz. 2013. Predicting Depression via Social Media. Proceedings of\nthe International AAAI Conference on Web and Social Media 7, 1 (2013), 128‚Äì137. https://doi.org/10.1609/icwsm.v7i1.14432\n[35] Munmun De Choudhury, Emre Kiciman, Mark Dredze, Glen Coppersmith, and Mrinal Kumar. 2016. Discovering Shifts to Suicidal\nIdeation from Mental Health Content in Social Media. InProceedings of the 2016 CHI Conference on Human Factors in Computing Systems .\nACM, San Jose California USA, 2098‚Äì2110. https://doi.org/10.1145/2858036.2858207\n[36] Kerstin Denecke, Sayan Vaaheesan, and Aaganya Arulnathan. 2020. A mental health chatbot for regulating emotions (SERMO)-concept\nand usability test. IEEE Transactions on Emerging Topics in Computing 9, 3 (2020), 1170‚Äì1182.\n[37] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. arXiv:1810.04805 [cs] (May 2019). http://arxiv.org/abs/1810.04805\n[38] Johannes C Eichstaedt, Robert J Smith, Raina M Merchant, Lyle H Ungar, Patrick Crutchley, Daniel Preo≈£iuc-Pietro, David A Asch, and\nH Andrew Schwartz. 2018. Facebook language predicts depression in medical records. Proceedings of the National Academy of Sciences\n115, 44 (2018), 11203‚Äì11208.\n[39] Mirko Franco, Ombretta Gaggi, and Claudio E Palazzi. 2023. Analyzing the use of large language models for content moderation with\nchatgpt examples. In Proceedings of the 3rd International Workshop on Open Challenges in Online Social Networks . 1‚Äì8.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\nMental-LLM ‚Ä¢ 32:27\n[40] Manas Gaur, Amanuel Alambo, Joy Prakash Sain, Ugur Kursuncu, Krishnaprasad Thirunarayan, Ramakanth Kavuluru, Amit Sheth,\nRandy Welton, and Jyotishman Pathak. 2019. Knowledge-aware Assessment of Severity of Suicide Risk for Early Intervention. In The\nWorld Wide Web Conference . ACM, San Francisco CA USA, 514‚Äì525. https://doi.org/10.1145/3308558.3313698\n[41] Meric Altug Gemalmaz and Ming Yin. 2021. Accounting for Confirmation Bias in Crowdsourced Label Aggregation.. In IJCAI.\n1729‚Äì1735.\n[42] Sourojit Ghosh and Aylin Caliskan. 2023. ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered\nPronouns: Findings across Bengali and Five other Low-Resource Languages. arXiv preprint arXiv:2305.10510 (2023).\n[43] George Gkotsis, Anika Oellrich, Tim Hubbard, Richard Dobson, Maria Liakata, Sumithra Velupillai, and Rina Dutta. 2016. The language\nof mental health problems in social media. In Proceedings of the third workshop on computational linguistics and clinical psychology .\n63‚Äì73.\n[44] Sarah Graham, Colin Depp, Ellen E Lee, Camille Nebeker, Xin Tu, Ho-Cheol Kim, and Dilip V Jeste. 2019. Artificial intelligence for\nmental health and mental illnesses: an overview. Current psychiatry reports 21 (2019), 1‚Äì18.\n[45] Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, and Yoshua Bengio. 2017. On integrating a language model into neural\nmachine translation. Computer Speech & Language 45 (2017), 137‚Äì148.\n[46] Sharath Chandra Guntuku, Anneke Buffone, Kokil Jaidka, Johannes C Eichstaedt, and Lyle H Ungar. 2019. Understanding and measuring\npsychological stress using social media. In Proceedings of the international AAAI conference on web and social media , Vol. 13. 214‚Äì225.\n[47] Sharath Chandra Guntuku, David B Yaden, Margaret L Kern, Lyle H Ungar, and Johannes C Eichstaedt. 2017. Detecting depression\nand mental illness on social media: an integrative review. Current Opinion in Behavioral Sciences 18 (Dec. 2017), 43‚Äì49. https:\n//doi.org/10.1016/j.cobeha.2017.07.005\n[48] Sooji Han, Rui Mao, and Erik Cambria. 2022. Hierarchical attention network for explainable depression detection on Twitter aided by\nmetaphor concept mappings. arXiv preprint arXiv:2209.07494 (2022).\n[49] Ayaan Haque, Viraaj Reddi, and Tyler Giallanza. 2021. Deep Learning for Suicide and Depression Identification with Unsupervised\nLabel Correction. http://arxiv.org/abs/2102.09427 arXiv:2102.09427 [cs].\n[50] Amanda Hoover. 2023. An eating disorder chatbot is suspended for giving harmful advice. https://www.wired.com/story/tessa-\nchatbot-suspended/\n[51] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA:\nLow-Rank Adaptation of Large Language Models. http://arxiv.org/abs/2106.09685 arXiv:2106.09685 [cs].\n[52] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can\nself-improve. arXiv preprint arXiv:2210.11610 (2022).\n[53] Irene Y. Chen, Emma Pierson, Sherri Rose, Shalmali Joshi, Kadija Ferryman, and Marzyeh Ghassemi. 2021. Ethical Machine Learning in\nHealthcare. Annual Review of Biomedical Data Science 4, 1 (2021), 123‚Äì144. https://doi.org/10.1146/annurev-biodatasci-092820-114757\n_eprint: https://doi.org/10.1146/annurev-biodatasci-092820-114757.\n[54] Irene Y. Chen, Peter Szolovits, and Marzyeh Ghassemi. 2019. Can AI Help Reduce Disparities in General Medical and Mental Health\nCare? AMA Journal of Ethics 21, 2 (Feb. 2019), E167‚Äì179. https://doi.org/10.1001/amajethics.2019.167\n[55] M. J. N. Bento e Silva J. Abrantes. 2023. External validation of a deep learning model for breast density classification. In ECR 2023 EPOS .\nhttps://epos.myesr.org/poster/esr/ecr2023/C-16014\n[56] Zunaira Jamil, Diana Inkpen, Prasadith Buddhitha, and Kenton White. 2017. Monitoring Tweets for Depression to Detect At-risk\nUsers. In Proceedings of the Fourth Workshop on Computational Linguistics and Clinical Psychology ‚Äî From Linguistic Signal to Clinical\nReality, Kristy Hollingshead, Molly E. Ireland, and Kate Loveys (Eds.). Association for Computational Linguistics, Vancouver, BC, 32‚Äì40.\nhttps://doi.org/10.18653/v1/W17-3104\n[57] Shaoxiong Ji, Celina Ping Yu, Sai-fu Fung, Shirui Pan, and Guodong Long. 2018. Supervised learning for suicidal ideation detection in\nonline user content. Complexity 2018 (2018).\n[58] Shaoxiong Ji, Tianlin Zhang, Luna Ansari, Jie Fu, Prayag Tiwari, and Erik Cambria. 2021. MentalBERT: Publicly Available Pretrained\nLanguage Models for Mental Healthcare. http://arxiv.org/abs/2110.15621\n[59] Lavender Yao Jiang, Xujin Chris Liu, Nima Pour Nejatian, Mustafa Nasir-Moin, Duo Wang, Anas Abidin, Kevin Eaton, Howard Antony\nRiina, Ilya Laufer, Paawan Punjabi, Madeline Miceli, Nora C. Kim, Cordelia Orillac, Zane Schnurman, Christopher Livia, Hannah Weiss,\nDavid Kurland, Sean Neifert, Yosef Dastagirzada, Douglas Kondziolka, Alexander T. M. Cheung, Grace Yang, Ming Cao, Mona Flores,\nAnthony B. Costa, Yindalon Aphinyanaphongs, Kyunghyun Cho, and Eric Karl Oermann. 2023. Health system-scale language models\nare all-purpose prediction engines. Nature (June 2023). https://doi.org/10.1038/s41586-023-06160-y\n[60] Zheng Ping Jiang, Sarah Ita Levitan, Jonathan Zomick, and Julia Hirschberg. 2020. Detection of mental health from reddit via deep\ncontextualized representations. InProceedings of the 11th international workshop on health text mining and information analysis . 147‚Äì156.\n[61] Eunkyung Jo, Daniel A Epstein, Hyunhoon Jung, and Young-Ho Kim. 2023. Understanding the benefits and challenges of deploying\nconversational AI leveraging large language models for public health intervention. In Proceedings of the 2023 CHI Conference on Human\nFactors in Computing Systems . 1‚Äì16.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\n32:28 ‚Ä¢ Xu et al.\n[62] S Kayalvizhi, Thenmozhi Durairaj, Bharathi Raja Chakravarthi, et al. 2022. Findings of the shared task on detecting signs of depression\nfrom social media. In Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion . 331‚Äì338.\n[63] Samuel Kernan Freire, Mina Foosherian, Chaofan Wang, and Evangelos Niforatos. 2023. Harnessing Large Language Models for\nCognitive Assistants in Factories. In Proceedings of the 5th International Conference on Conversational User Interfaces . 1‚Äì6.\n[64] Jan Koco≈Ñ, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szyd≈Ço, Joanna Baran, Julita Bielaniewicz, Marcin Gruza,\nArkadiusz Janz, Kamil Kanclerz, et al. 2023. ChatGPT: Jack of all trades, master of none. Information Fusion (2023), 101861.\n[65] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large Language Models are Zero-Shot\nReasoners. In 36th Conference on Neural Information Processing Systems .\n[66] Kaylee Payne Kruzan, Kofoworola D.A. Williams, Jonah Meyerhoff, Dong Whi Yoo, Linda C. O‚ÄôDwyer, Munmun De Choudhury, and\nDavid C. Mohr. 2022. Social media-based interventions for adolescent and young adult mental health: A scoping review. Internet\nInterventions 30 (Dec. 2022), 100578. https://doi.org/10.1016/j.invent.2022.100578\n[67] Bishal Lamichhane. 2023. Evaluation of ChatGPT for NLP-based Mental Health Applications. http://arxiv.org/abs/2303.15727\n[68] Yi-Chieh Lee, Naomi Yamashita, and Yun Huang. 2020. Designing a Chatbot as a Mediator for Promoting Deep Self-Disclosure to\na Real Mental Health Professional. Proceedings of the ACM on Human-Computer Interaction 4, CSCW1 (May 2020), 1‚Äì27. https:\n//doi.org/10.1145/3392836\n[69] Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. 2023. Compressing Context to Enhance Inference Efficiency of Large Language\nModels. arXiv preprint arXiv:2310.06201 (2023).\n[70] Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang. 2023. ChatDoctor: A Medical Chat Model Fine-Tuned on a\nLarge Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge. http://arxiv.org/abs/2303.14070 arXiv:2303.14070 [cs].\n[71] Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine, Jiening Zhan, Ming-Zher Poh, Shun Liao, Paolo Di Achille,\nand Shwetak Patel. 2023. Large Language Models are Few-Shot Health Learners. In arXiv.\n[72] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin\nStoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. http://arxiv.org/abs/1907.11692 arXiv:1907.11692 [cs].\n[73] James D. Livingston, Michelle Cianfrone, Kimberley Korf-Uzan, and Connie Coniglio. 2014. Another time point, a different story:\none year effects of a social media intervention on the attitudes of young people towards mental health issues. Social Psychiatry and\nPsychiatric Epidemiology 49, 6 (June 2014), 985‚Äì990. https://doi.org/10.1007/s00127-013-0815-7\n[74] Christopher A Lovejoy. 2019. Technology and mental health: the role of artificial intelligence. European Psychiatry 55 (2019), 1‚Äì3.\n[75] Maria Luce Lupetti, Emma Hagens, Willem Van Der Maden, R√©gine Steegers-Theunissen, and Melek Rousian. 2023. Trustworthy\nEmbodied Conversational Agents for Healthcare: A Design Exploration of Embodied Conversational Agents for the periconception\nperiod at Erasmus MC. In Proceedings of the 5th International Conference on Conversational User Interfaces . 1‚Äì14.\n[76] Matthew Louis Mauriello, Thierry Lincoln, Grace Hon, Dorien Simon, Dan Jurafsky, and Pablo Paredes. 2021. SAD: A Stress Annotated\nDataset for Recognizing Everyday Stressors in SMS-like Conversational Systems. In Extended Abstracts of the 2021 CHI Conference on\nHuman Factors in Computing Systems . ACM, Yokohama Japan, 1‚Äì7. https://doi.org/10.1145/3411763.3451799\n[77] Margaret Mitchell, Kristy Hollingshead, and Glen Coppersmith. 2015. Quantifying the Language of Schizophrenia in Social Media.\nIn Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality .\nAssociation for Computational Linguistics, Denver, Colorado, 11‚Äì20. https://doi.org/10.3115/v1/W15-1202\n[78] Margarida Morais, Francisco Maria Calisto, Carlos Santiago, Clara Aleluia, and Jacinto C Nascimento. 2023. Classification of breast\ncancer in Mri with multimodal fusion. In 2023 IEEE 20th international symposium on biomedical imaging (ISBI) . IEEE, 1‚Äì4.\n[79] Megan A Moreno, Lauren A Jelenchick, Katie G Egan, Elizabeth Cox, Henry Young, Kerry E Gannon, and Tara Becker. 2011. Feeling\nbad on Facebook: Depression disclosures by college students on a social networking site. Depression and anxiety 28, 6 (2011), 447‚Äì455.\n[80] Usman Naseem, Adam G. Dunn, Jinman Kim, and Matloob Khushi. 2022. Early Identification of Depression Severity Levels on\nReddit Using Ordinal Classification. In Proceedings of the ACM Web Conference 2022 . ACM, Virtual Event, Lyon France, 2563‚Äì2572.\nhttps://doi.org/10.1145/3485447.3512128\n[81] Subigya Nepal, Gonzalo J. Martinez, Shayan Mirjafari, Koustuv Saha, Vedant Das Swain, Xuhai Xu, Pino G. Audia, Munmun De\nChoudhury, Anind K. Dey, Aaron Striegel, and Andrew T. Campbell. 2022. A Survey of Passive Sensing in the Workplace.\narXiv:2201.03074 [cs.HC]\n[82] Thin Nguyen, Dinh Phung, Bo Dao, Svetha Venkatesh, and Michael Berk. 2014. Affective and content analysis of online depression\ncommunities. IEEE transactions on affective computing 5, 3 (2014), 217‚Äì226.\n[83] Thong Nguyen, Andrew Yates, Ayah Zirikly, Bart Desmet, and Arman Cohan. 2022. Improving the generalizability of depression\ndetection by leveraging clinical questionnaires. arXiv preprint arXiv:2204.10432 (2022).\n[84] Tanya Nijhawan, Girija Attigeri, and T Ananthakrishna. 2022. Stress detection using natural language processing and machine learning\nover social interactions. Journal of Big Data 9, 1 (2022), 1‚Äì24.\n[85] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. Capabilities of GPT-4 on Medical Challenge\nProblems. http://arxiv.org/abs/2303.13375 arXiv:2303.13375 [cs].\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\nMental-LLM ‚Ä¢ 32:29\n[86] Eirini Ntoutsi, Pavlos Fafalios, Ujwal Gadiraju, Vasileios Iosifidis, Wolfgang Nejdl, Maria-Esther Vidal, Salvatore Ruggieri, Franco\nTurini, Symeon Papadopoulos, Emmanouil Krasanakis, et al. 2020. Bias in data-driven artificial intelligence systems‚ÄîAn introductory\nsurvey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 10, 3 (2020), e1356.\n[87] Reham Omar, Omij Mangukiya, Panos Kalnis, and Essam Mansour. 2023. Chatgpt versus traditional question answering for knowledge\ngraphs: Current status and future directions towards knowledge graph chatbots. arXiv preprint arXiv:2302.06466 (2023).\n[88] Norio Otsuka, Yuu Kawanishi, Fumimaro Doi, Tsutomu Takeda, Kazuki Okumura, Takahira Yamauchi, Shuntaro Yada, Shoko Wakamiya,\nEiji Aramaki, and Manabu Makinodan. [n. d.]. Diagnosing Psychiatric Disorders from History of Present Illness Using a Large-Scale\nLinguistic Model. Psychiatry and Clinical Neurosciences ([n. d.]).\n[89] Minsu Park, David McDonald, and Meeyoung Cha. 2021. Perception Differences between the Depressed and Non-Depressed Users in\nTwitter. Proceedings of the International AAAI Conference on Web and Social Media 7, 1 (Aug. 2021), 476‚Äì485. https://doi.org/10.1609/\nicwsm.v7i1.14425\n[90] Vivek Patel, Piyush Mishra, and JC Patni. 2018. PsyHeal: An Approach to Remote Mental Health Monitoring System. In2018 International\nConference on Advances in Computing and Communication Engineering (ICACCE) . IEEE, 384‚Äì393.\n[91] Michael Paul and Mark Dredze. 2011. You Are What You Tweet: Analyzing Twitter for Public Health. Proceedings of the International\nAAAI Conference on Web and Social Media 5, 1 (2011), 265‚Äì272. https://doi.org/10.1609/icwsm.v5i1.14137\n[92] Dana Pessach and Erez Shmueli. 2022. A review on fairness in machine learning. ACM Computing Surveys (CSUR) 55, 3 (2022), 1‚Äì44.\n[93] K Posner, D Brent, C Lucas, M Gould, B Stanley, G Brown, P Fisher, J Zelazny, A Burke, MJNY Oquendo, et al. 2008. Columbia-suicide\nseverity rating scale (C-SSRS). New York, NY: Columbia University Medical Center 10 (2008), 2008.\n[94] Praw-Dev. [n. d.]. Praw-dev/PRAW: PRAW, an acronym for ‚ÄúPython reddit api wrapper‚Äù, is a python package that allows for simple\naccess to Reddit‚Äôs API. https://github.com/praw-dev/praw\n[95] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is ChatGPT a general-purpose\nnatural language processing task solver? arXiv preprint arXiv:2302.06476 (2023).\n[96] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving Language Understanding by Generative\nPre-Training.\n[97] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020.\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research (2020).\n[98] Darrel A Regier, Emily A Kuhl, and David J Kupfer. 2013. The DSM-5: Classification and criteria changes. World psychiatry 12, 2 (2013),\n92‚Äì98.\n[99] Brad Ridout and Andrew Campbell. 2018. The Use of Social Networking Sites in Mental Health Interventions for Young People:\nSystematic Review. Journal of Medical Internet Research 20, 12 (Dec. 2018), e12244. https://doi.org/10.2196/12244\n[100] Joshua Robinson and David Wingate. 2023. Leveraging Large Language Models for Multiple Choice Question Answering. In The\nEleventh International Conference on Learning Representations . https://openreview.net/forum?id=yKbprarjc5B\n[101] Thomas Ruder, Gary Hatch, Garyfalia Ampanozi, Michael Thali, and Nadja Fischer. 2011. Suicide Announcement on Facebook. Crisis\n32 (June 2011), 280‚Äì2. https://doi.org/10.1027/0227-5910/a000086\n[102] Anna Rumshisky, Marzyeh Ghassemi, Tristan Naumann, Peter Szolovits, VM Castro, TH McCoy, and RH Perlis. 2016. Predicting early\npsychiatric readmission with natural language processing of narrative discharge summaries. Translational psychiatry 6, 10 (2016),\ne921‚Äìe921.\n[103] Koustuv Saha, Larry Chan, Kaya De Barbaro, Gregory D. Abowd, and Munmun De Choudhury. 2017. Inferring Mood Instability on\nSocial Media by Leveraging Ecological Momentary Assessments. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 1, 3, Article 95\n(sep 2017), 27 pages. https://doi.org/10.1145/3130960\n[104] Shoffan Saifullah, Yuli Fauziah, and Agus Sasmito Aribowo. 2021. Comparison of machine learning for sentiment analysis in detecting\nanxiety based on social media data. arXiv preprint arXiv:2101.06353 (2021).\n[105] Kayalvizhi Sampath and Thenmozhi Durairaj. 2022. Data Set Creation and Empirical Analysis for Detecting Signs of Depression from\nSocial Media Postings. In Computational Intelligence in Data Science , Lekshmi Kalinathan, Priyadharsini R., Madheswari Kanmani, and\nManisha S. (Eds.). Vol. 654. Springer International Publishing, Cham, 136‚Äì151. https://doi.org/10.1007/978-3-031-16364-7_11 Series\nTitle: IFIP Advances in Information and Communication Technology.\n[106] Shailik Sarkar, Abdulaziz Alhamadani, Lulwah Alkulaib, and Chang-Tien Lu. 2022. Predicting depression and anxiety on reddit: a\nmulti-task learning approach. In2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM) .\nIEEE, 427‚Äì435.\n[107] Ramit Sawhney, Prachi Manchanda, Puneet Mathur, Rajiv Shah, and Raj Singh. 2018. Exploring and learning suicidal ideation\nconnotations on social media with deep learning. In Proceedings of the 9th workshop on computational approaches to subjectivity,\nsentiment and social media analysis . 167‚Äì175.\n[108] Ashish Sharma, Inna W. Lin, Adam S. Miner, David C. Atkins, and Tim Althoff. 2021. Towards Facilitating Empathic Conversations\nin Online Mental Health Support: A Reinforcement Learning Approach. In Proceedings of the Web Conference 2021 . ACM, Ljubljana\nSlovenia, 194‚Äì205. https://doi.org/10.1145/3442381.3450097\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\n32:30 ‚Ä¢ Xu et al.\n[109] Ashish Sharma, Inna W. Lin, Adam S. Miner, David C. Atkins, and Tim Althoff. 2023. Human‚ÄìAI collaboration enables more\nempathic conversations in text-based peer-to-peer mental health support. Nature Machine Intelligence 5, 1 (Jan. 2023), 46‚Äì57. https:\n//doi.org/10.1038/s42256-022-00593-2\n[110] Eva Sharma and Munmun De Choudhury. 2018. Mental health support and its relationship to linguistic accommodation in online\ncommunities. In Proceedings of the 2018 CHI conference on human factors in computing systems . 1‚Äì13.\n[111] Judy Hanwen Shen and Frank Rudzicz. 2017. Detecting Anxiety through Reddit. InProceedings of the Fourth Workshop on Computational\nLinguistics and Clinical Psychology ‚Äî From Linguistic Signal to Clinical Reality . Association for Computational Linguistics, Vancouver,\nBC, 58‚Äì65. https://doi.org/10.18653/v1/W17-3107\n[112] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene\nNeal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa\nDominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale\nWebster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. 2023. Towards Expert-Level\nMedical Question Answering with Large Language Models. http://arxiv.org/abs/2305.09617 arXiv:2305.09617 [cs].\n[113] Michael M Tadesse, Hongfei Lin, Bo Xu, and Liang Yang. 2019. Detection of depression-related posts in reddit social media forum.\nIEEE Access 7 (2019), 44883‚Äì44893.\n[114] Michael Mesfin Tadesse, Hongfei Lin, Bo Xu, and Liang Yang. 2019. Detection of Suicide Ideation in Social Media Forums Using Deep\nLearning. Algorithms 13, 1 (Dec. 2019), 7. https://doi.org/10.3390/a13010007\n[115] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto.\n2023. Stanford alpaca: An instruction-following llama model.\n[116] Adela C Timmons, Jacqueline B Duong, Natalia Simo Fiallo, Theodore Lee, Huong Phuc Quynh Vo, Matthew W Ahle, Jonathan S\nComer, LaPrincess C Brewer, Stacy L Frazier, and Theodora Chaspari. 2022. A Call to Action on Assessing and Mitigating Bias in\nArtificial Intelligence Applications for Mental Health. Perspectives on Psychological Science (2022), 17456916221134490.\n[117] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman\nGoyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and\nEfficient Foundation Language Models. http://arxiv.org/abs/2302.13971 arXiv:2302.13971 [cs].\n[118] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal\nBhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude\nFernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,\nRui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne\nLachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan,\nIliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and\nThomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. http://arxiv.org/abs/2307.09288 arXiv:2307.09288 [cs].\n[119] Sho Tsugawa, Yusuke Kikuchi, Fumio Kishino, Kosuke Nakajima, Yuichi Itoh, and Hiroyuki Ohsaki. 2015. Recognizing Depression\nfrom Twitter Activity. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems . ACM, Seoul Republic\nof Korea, 3187‚Äì3196. https://doi.org/10.1145/2702123.2702280\n[120] Elsbeth Turcan and Kathleen McKeown. 2019. Dreaddit: A Reddit Dataset for Stress Analysis in Social Media. http://arxiv.org/abs/\n1911.00133 arXiv:1911.00133 [cs].\n[121] Dakuo Wang, Elizabeth Churchill, Pattie Maes, Xiangmin Fan, Ben Shneiderman, Yuanchun Shi, and Qianying Wang. 2020. From\nhuman-human collaboration to Human-AI collaboration: Designing AI systems that can work together with people. In Extended\nabstracts of the 2020 CHI conference on human factors in computing systems . 1‚Äì6.\n[122] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021.\nFinetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).\n[123] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022.\nFinetuned Language Models Are Zero-Shot Learners. http://arxiv.org/abs/2109.01652 arXiv:2109.01652 [cs].\n[124] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-\nThought Prompting Elicits Reasoning in Large Language Models. http://arxiv.org/abs/2201.11903 arXiv:2201.11903 [cs].\n[125] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2023. PMC-LLaMA: Further Finetuning LLaMA on Medical\nPapers. http://arxiv.org/abs/2304.14454 arXiv:2304.14454 [cs].\n[126] Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, and Fei Huang. 2021. Raise a child in large\nlanguage model: Towards effective and generalizable fine-tuning. arXiv preprint arXiv:2109.05687 (2021).\n[127] Xuhai Xu, Prerna Chikersal, Afsaneh Doryab, Daniella K. Villalba, Janine M. Dutcher, Michael J. Tumminia, Tim Althoff, Sheldon Cohen,\nKasey G. Creswell, J. David Creswell, Jennifer Mankoff, and Anind K. Dey. 2019. Leveraging Routine Behavior and Contextually-Filtered\nFeatures for Depression Detection among College Students. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\nMental-LLM ‚Ä¢ 32:31\nTechnologies 3, 3 (Sept. 2019), 1‚Äì33. https://doi.org/10.1145/3351274\n[128] Xuhai Xu, Prerna Chikersal, Janine M. Dutcher, Yasaman S. Sefidgar, Woosuk Seo, Michael J. Tumminia, Daniella K. Villalba, Sheldon\nCohen, Kasey G. Creswell, J. David Creswell, Afsaneh Doryab, Paula S. Nurius, Eve Riskin, Anind K. Dey, and Jennifer Mankoff.\n2021. Leveraging Collaborative-Filtering for Personalized Behavior Modeling: A Case Study of Depression Detection among College\nStudents. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 5, 1 (March 2021), 1‚Äì27. https:\n//doi.org/10.1145/3448107\n[129] Xuhai Xu, Xin Liu, Han Zhang, Weichen Wang, Subigya Nepal, Yasaman Sefidgar, Woosuk Seo, Kevin S. Kuehn, Jeremy F. Huckins,\nMargaret E. Morris, Paula S. Nurius, Eve A. Riskin, Shwetak Patel, Tim Althoff, Andrew Campbell, Anind K. Dey, and Jennifer Mankoff.\n2023. GLOBEM: Cross-Dataset Generalization of Longitudinal Human Behavior Modeling. Proceedings of the ACM on Interactive,\nMobile, Wearable and Ubiquitous Technologies 6, 4 (2023), 1‚Äì34. https://doi.org/10.1145/3569485\n[130] Xuhai Xu, Jennifer Mankoff, and Anind K. Dey. 2021. Understanding practices and needs of researchers in human state modeling by\npassive mobile sensing.CCF Transactions on Pervasive Computing and Interaction (July 2021). https://doi.org/10.1007/s42486-021-00072-4\n[131] Xuhai Xu, Han Zhang, Yasaman Sefidgar, Yiyi Ren, Xin Liu, Woosuk Seo, Jennifer Brown, Kevin Kuehn, Mike Merrill, Paula Nurius,\nShwetak Patel, Tim Althoff, Margaret E Morris, Eve Riskin, Jennifer Mankoff, and Anind K Dey. 2022. GLOBEM Dataset: Multi-Year\nDatasets for Longitudinal Human Behavior Modeling Generalization. In Thirty-sixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track . 18.\n[132] Kailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie, and Sophia Ananiadou. 2023. On the Evaluations of ChatGPT and Emotion-\nenhanced Prompting for Mental Health Analysis. http://arxiv.org/abs/2304.03347\n[133] Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Sophia Ananiadou, and Jimin Huang. 2023. MentaLLaMA: Interpretable Mental\nHealth Analysis on Social Media with Large Language Models. http://arxiv.org/abs/2309.13567 arXiv:2309.13567 [cs].\n[134] Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao. 2023. Can chatgpt understand too? a comparative study on chatgpt\nand fine-tuned bert. arXiv preprint arXiv:2302.10198 (2023).\n[135] Denny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc\nLe, and Ed Chi. 2023. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. http://arxiv.org/abs/2205.10625\narXiv:2205.10625 [cs].\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024.\n32:32 ‚Ä¢ Xu et al.\nAPPENDIX: DETAILED RESULTS TABLES\nTable 10. Balanced Accuracy Performance Summary of Zero-shot, Few-shot and Instruction Finetuning on LLMs.ùëêùëúùëõùë°ùëíùë•ùë°, ùëö‚Ñé,\nand ùëèùëúùë°‚Ñé indicate the prompt design strategies of context enhancement, mental health enhancement, and their combination\n(see Table. 2). Small numbers represent standard deviation across different designs of PromptPart1-S and PromptPart2-Q. The\nbaselines at the top rows do not have standard deviation as the task-specific output is static, and prompt designs do not\napply. Due to token limit, computation cost, and resource constraints, some infeasible experiments are marked as ‚Äú‚Äì‚Äù. For\neach column, the best result is bolded, and the second best is underlined.\nDatasetDreaddit DepSeverity SDCNL CSSRS-Suicide Red-Sam Twt-60Users SAD\nCategory Model Task #1 Task #2 Task #3 Task #4 Task #5 Task #6 Task #2 Task #2 Task #1\nZero-shot\nPrompting\nAlpacaùëçùëÜ 0.593¬±0.039 0.522¬±0.022 0.431¬±0.050 0.493¬±0.007 0.518¬±0.037 0.232¬±0.076 0.524¬±0.014 0.521¬±0.022 0.503¬±0.004\nAlpacaùëçùëÜ‚àíùëêùëúùëõùë°ùëíùë•ùë° 0.612¬±0.065 0.567¬±0.077 0.454¬±0.143 0.497¬±0.006 0.532¬±0.033 0.250¬±0.060 0.525¬±0.019 0.559¬±0.064 0.501¬±0.004\nAlpacaùëçùëÜ_ùëö‚Ñé 0.593¬±0.031 0.577¬±0.028 0.444¬±0.090 0.482¬±0.015 0.523¬±0.013 0.235¬±0.033 0.527¬±0.006 0.569¬±0.017 0.522¬±0.027\nAlpacaùëçùëÜ_ùëèùëúùë°‚Ñé 0.540¬±0.029 0.559¬±0.040 0.421¬±0.095 0.532¬±0.005 0.511¬±0.011 0.221¬±0.030 0.495¬±0.016 0.499¬±0.004 0.557¬±0.041\nAlpaca-LoRAùëçùëÜ 0.571¬±0.043 0.548¬±0.027 0.437¬±0.044 0.502¬±0.011 0.540¬±0.012 0.187¬±0.053 0.577¬±0.004 0.607¬±0.046 0.477¬±0.016\nAlpaca-LoRAùëçùëÜùëêùëúùëõùë°ùëíùë•ùë° 0.537¬±0.047 0.501¬±0.001 0.343¬±0.152 0.472¬±0.020 0.567¬±0.038 0.214¬±0.059 0.535¬±0.017 0.649¬±0.021 0.443¬±0.047\nAlpaca-LoRAùëçùëÜ_ùëö‚Ñé 0.500¬±0.000 0.500¬±0.000 0.331¬±0.145 0.497¬±0.025 0.557¬±0.023 0.216¬±0.022 0.541¬±0.016 0.569¬±0.019 0.471¬±0.033\nAlpaca-LoRAùëçùëÜ_ùëèùëúùë°‚Ñé 0.500¬±0.000 0.500¬±0.000 0.386¬±0.059 0.499¬±0.023 0.517¬±0.031 0.224¬±0.049 0.507¬±0.009 0.535¬±0.025 0.420¬±0.019\nFLAN-T5ùëçùëÜ 0.659¬±0.086 0.664¬±0.011 0.396¬±0.006 0.643¬±0.021 0.667¬±0.023 0.418¬±0.012 0.554¬±0.034 0.613¬±0.040 0.692¬±0.093\nFLAN-T5ùëçùëÜùëêùëúùëõùë°ùëíùë•ùë° 0.663¬±0.079 0.674¬±0.014 0.378¬±0.013 0.653¬±0.011 0.649¬±0.026 0.378¬±0.029 0.563¬±0.029 0.613¬±0.046 0.738¬±0.056\nFLAN-T5ùëçùëÜ_ùëö‚Ñé 0.616¬±0.070 0.666¬±0.009 0.366¬±0.012 0.648¬±0.010 0.653¬±0.018 0.372¬±0.033 0.547¬±0.035 0.613¬±0.033 0.739¬±0.039\nFLAN-T5ùëçùëÜ_ùëèùëúùë°‚Ñé 0.604¬±0.074 0.661¬±0.004 0.389¬±0.051 0.645¬±0.005 0.657¬±0.019 0.382¬±0.048 0.536¬±0.027 0.606¬±0.040 0.767¬±0.050\nLLaMA2ùëçùëÜ 0.720¬±0.012 0.693¬±0.034 0.429¬±0.013 0.589¬±0.010 0.691¬±0.014 0.261¬±0.018 0.574¬±0.008 0.735¬±0.017 0.704¬±0.026\nLLaMA2ùëçùëÜùëêùëúùëõùë°ùëíùë•ùë° 0.658¬±0.025 0.707¬±0.056 0.410¬±0.019 0.588¬±0.026 0.722¬±0.039 0.367¬±0.043 0.562¬±0.011 0.736¬±0.019 0.650¬±0.027\nLLaMA2ùëçùëÜ_ùëö‚Ñé 0.617¬±0.012 0.711¬±0.033 0.395¬±0.017 0.642¬±0.008 0.696¬±0.021 0.291¬±0.038 0.572¬±0.012 0.689¬±0.056 0.567¬±0.021\nLLaMA2ùëçùëÜ_ùëèùëúùë°‚Ñé 0.584¬±0.017 0.704¬±0.036 0.444¬±0.021 0.643¬±0.014 0.689¬±0.043 0.328¬±0.058 0.559¬±0.012 0.692¬±0.069 0.560¬±0.009\nGPT-3.5ùëçùëÜ 0.685¬±0.024 0.642¬±0.017 0.603¬±0.017 0.460¬±0.163 0.570¬±0.118 0.233¬±0.009 0.454¬±0.007 0.536¬±0.024 0.717¬±0.017\nGPT-3.5ùëçùëÜùëêùëúùëõùë°ùëíùë•ùë° 0.688¬±0.045 0.653¬±0.020 0.543¬±0.047 0.618¬±0.008 0.577¬±0.090 0.265¬±0.048 0.473¬±0.001 0.560¬±0.002 0.723¬±0.003\nGPT-3.5ùëçùëÜ_ùëö‚Ñé 0.679¬±0.017 0.636¬±0.021 0.642¬±0.034 0.576¬±0.001 0.477¬±0.014 0.310¬±0.015 0.467¬±0.004 0.571¬±0.000 0.664¬±0.061\nGPT-3.5ùëçùëÜ_ùëèùëúùë°‚Ñé 0.681¬±0.010 0.627¬±0.022 0.617¬±0.014 0.632¬±0.020 0.617¬±0.033 0.254¬±0.009 0.506¬±0.004 0.570¬±0.007 0.750¬±0.027\nGPT-4ùëçùëÜ 0.700¬±0.001 0.719¬±0.013 0.588¬±0.010 0.644¬±0.007 0.760¬±0.009 0.418¬±0.009 0.434¬±0.005 0.566¬±0.017 0.854¬±0.006\nGPT-4ùëçùëÜùëêùëúùëõùë°ùëíùë•ùë° 0.706¬±0.009 0.719¬±0.009 0.590¬±0.011 0.644¬±0.011 0.753¬±0.028 0.441¬±0.057 0.465¬±0.010 0.565¬±0.006 0.848¬±0.001\nGPT-4ùëçùëÜ_ùëö‚Ñé 0.725¬±0.009 0.684¬±0.004 0.656¬±0.001 0.645¬±0.012 0.737¬±0.005 0.396¬±0.020 0.496¬±0.005 0.527¬±0.007 0.840¬±0.003\nGPT-4ùëçùëÜ_ùëèùëúùë°‚Ñé 0.719¬±0.021 0.689¬±0.000 0.650¬±0.011 0.647¬±0.014 0.697¬±0.005 0.411¬±0.009 0.511¬±0.000 0.546¬±0.014 0.837¬±0.002\nFew-shot\nPrompting\nAlpacaùêπùëÜ 0.632¬±0.030 0.529¬±0.017 0.628¬±0.005 ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî\nFLAN-T5ùêπùëÜ 0.786¬±0.006 0.678¬±0.009 0.432¬±0.009 ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî\nGPT-3.5ùêπùëÜ 0.721¬±0.010 0.665¬±0.015 0.580¬±0.002 ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî\nGPT-4ùêπùëÜ 0.698¬±0.009 0.724¬±0.005 0.613¬±0.001 ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî ‚Äî\nInstructional\nFinetuning\nMental-Alpaca 0.816¬±0.006 0.775¬±0.006 0.746¬±0.005 0.724¬±0.004 0.730¬±0.048 0.403¬±0.029 0.604¬±0.012 0.718¬±0.011 0.819¬±0.006\nMental-FLAN-T5 0.802¬±0.002 0.759¬±0.003 0.756¬±0.001 0.677¬±0.005 0.868¬±0.006 0.481¬±0.006 0.582¬±0.002 0.736¬±0.003 0.779¬±0.002\nBaseline\nMajority 0.500 ¬±‚àí‚àí‚àí 0.500¬±‚àí‚àí‚àí 0.250¬±‚àí‚àí‚àí 0.500¬±‚àí‚àí‚àí 0.500¬±‚àí‚àí‚àí 0.200¬±‚àí‚àí‚àí ‚Äî ‚Äî ‚Äî\nBERT 0.783 ¬±‚àí‚àí‚àí 0.763¬±‚àí‚àí‚àí 0.690¬±‚àí‚àí‚àí 0.678¬±‚àí‚àí‚àí 0.500¬±‚àí‚àí‚àí 0.332¬±‚àí‚àí‚àí ‚Äî ‚Äî ‚Äî\nMental-RoBERTa0.831¬±‚àí‚àí‚àí0.790¬±‚àí‚àí‚àí0.736¬±‚àí‚àí‚àí 0.723¬±‚àí‚àí‚àí 0.853¬±‚àí‚àí‚àí 0.373¬±‚àí‚àí‚àí ‚Äî ‚Äî ‚Äî\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 32. Publication date: March 2024."
}