{
    "title": "Application of transformers for predicting epilepsy treatment response",
    "url": "https://openalex.org/W3104964482",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A5109512650",
            "name": "Jiun Choong",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5085327074",
            "name": "Haris Hakeem",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100604100",
            "name": "Zhibin Chen",
            "affiliations": [
                "The Alfred Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A5025489238",
            "name": "Martin J. Brodie",
            "affiliations": [
                "University of Glasgow"
            ]
        },
        {
            "id": "https://openalex.org/A5029185952",
            "name": "Nicholas Lawn",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5017469047",
            "name": "Tom Drummond",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5067443324",
            "name": "Patrick Kwan",
            "affiliations": [
                "The Alfred Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A5005014252",
            "name": "Zongyuan Ge",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2563036039",
        "https://openalex.org/W2104550140",
        "https://openalex.org/W2777270043",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2890139949",
        "https://openalex.org/W3003849273",
        "https://openalex.org/W2558902621",
        "https://openalex.org/W3047280228",
        "https://openalex.org/W2979956313",
        "https://openalex.org/W2971893337",
        "https://openalex.org/W2905810301",
        "https://openalex.org/W3044838059",
        "https://openalex.org/W2900286268",
        "https://openalex.org/W2921009461",
        "https://openalex.org/W2951517713",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2944851425",
        "https://openalex.org/W1525783482",
        "https://openalex.org/W3017637887",
        "https://openalex.org/W3008442486",
        "https://openalex.org/W2277855494",
        "https://openalex.org/W3004785010",
        "https://openalex.org/W2593873339",
        "https://openalex.org/W1899504021",
        "https://openalex.org/W3030520226",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2901409340",
        "https://openalex.org/W3001332251"
    ],
    "abstract": "ABSTRACT There is growing interest in machine learning based approaches to assist clinicians in treatment selection. In the treatment of epilepsy, a common neurological disorder that affects 70 million people worldwide, previous research has employed scoring methods generated from traditional machine learning methods based on pre-treatment patient characteristics to classify those with drug-resistant epilepsy (DRE). In this study, we used an attention-based approach in predicting the response to different antiseizure medications (ASMs) in individuals with newly diagnosed epilepsy. By applying a conventional transformer to model the patient‚Äôs response, we can use the predicted probability to determine the success rate of specific ASMs. Applying the transformer allowed the model to place attention on patient information and past treatments to model future drug responses. We trained a conventional transformer model based on one cohort of 1536 patients with newly diagnosed epilepsy, compared its performance with other trained models using RNN and LSTM, and applied it to a validation cohort of 736 patients. In the development cohort, the transformer model showed the highest accuracy (81%) and AUC (0.85), and maintained similar accuracy and AUC (74% and 0.79, respectively) in the validation cohort.",
    "full_text": "Application of transformers for predicting epilepsy treatment\nresponse\nJiun Choonga, Haris Hakeema, Zhibin Chenb, Martin Brodiec, Nicholas Lawnd, Tom\nDrummond a, Patrick Kwanb, and Zongyuan Gea\naMonash University, Melbourne, Australia;\nbAlfred Hospital, Melbourne, Australia;\ncUniversity of Glasgow, Glasgow, Scotland;\ndWA Adult Epilepsy Service, Perth, Australia\nABSTRACT\nThere is growing interest in machine learning based approaches to assist clinicians in treatment selection. In\nthe treatment of epilepsy, a common neurological disorder that aÔ¨Äects 70 million people worldwide, previous\nresearch has employed scoring methods generated from traditional machine learning methods based on pre-\ntreatment patient characteristics to classify those with drug-resistant epilepsy (DRE). In this study, we used an\nattention-based approach in predicting the response to diÔ¨Äerent antiseizure medications (ASMs) in individuals\nwith newly diagnosed epilepsy. By applying a conventional transformer to model the patient‚Äôs response, we can\nuse the predicted probability to determine the success rate of speciÔ¨Åc ASMs. Applying the transformer allowed\nthe model to place attention on patient information and past treatments to model future drug responses. We\ntrained a conventional transformer model based on one cohort of 1536 patients with newly diagnosed epilepsy,\ncompared its performance with other trained models using RNN and LSTM, and applied it to a validation cohort\nof 736 patients. In the development cohort, the transformer model showed the highest accuracy (81%) and AUC\n(0.85), and maintained similar accuracy and AUC (74% and 0.79, respectively) in the validation cohort.\nKeywords: D eep learning, transformer, self-attention mechanism, epilepsy, treatment, machine learning, pre-\ndiction, time-series, longitudinal, prediction\n1. INTRODUCTION\nEpilepsy is one of the common neurological diseases aÔ¨Äecting approximately 0.8% of people during their lifetime.1\nIt is characterized by an increased tendency to have recurrent seizures at unpredictable times which can lead\nto physical injury, impair social functioning, aÔ¨Äect mental health and quality of life or even cause death. 2\nDespite advances in non-pharmacological treatment options over the years in the form of resective surgery,\nneuromodulation and dietary therapies, the mainstay and Ô¨Årst-line treatment remains as drug therapy, for which\nover 20 antiseizure medications (ASMs) are currently available. However, drug selection still relies on a trial-\nand-error approach and 30% of patients have drug-resistant epilepsy (DRE) that does not respond to currently\navailable ASMs.3 Although there are general guidelines based on broad seizure types, there is currently no\nreliable way to predict the optimal drug choice for individual patients.\nThere is growing interest in applying machine learning in assisting healthcare decision making, fuelled by\nadvancements in the deep learning Ô¨Åeld such as BioBERT for natural language processing (NLP) tasks, 4 gen-\nerative adversarial networks (GANs) for medical image generation 5 and convolutional neural networks (CNNs)\nfor detection through image classiÔ¨Åcation. 6 In epilepsy, the application of machine learning has been limited to\ntraditional models such as random forest (RF) algorithms 7 and scoring based systems 8 to predict a patient‚Äôs\nASM treatment outcome. Furthermore, recent advances with transformers which uses attention-mechanisms\nhave proven success in many domains4, 9, 10due to the ability to focus on speciÔ¨Åc parts of the data.\nIn this study, we applied an attention-based model to predict the optimal ASM prescription for individual\npatients based on a broad range of demographic factors and epilepsy information. The performance and robust-\nness were veriÔ¨Åed through a separate cohort that was not used in the training or validation of the transformer\nmodel.\nOur contributions are as follows:\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 13, 2020. ; https://doi.org/10.1101/2020.11.10.20229385doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nAge\nDrug\n‚Ä¶ \nAge\nDrug\n‚Ä¶ \nAge\nDrug\n‚Ä¶ \nAge\nDrug\n‚Ä¶ ‚Ä¶\nClinical Datasets\nTraining Data\nValidation Data\nDrug 1 \nDrug 2\n‚Ä¶ \nDrug 1 \nDrug 2\n‚Ä¶ \n‚Ä¶\nSorting patients\nNormalise variables\nPre-Processing\nDrug 1 \nDrug 2\n‚Ä¶ \nDrug 1 \nDrug 2\n‚Ä¶ \n‚Ä¶\nSorting patients\nNormalise Variables\nPre-Processing\nTransformer Model\nPredicted Outcomes\nDrugs Outcomes\n‚Ä¶ ‚Ä¶Testing Data\na) b) c)\nd)\nCohort 1\nCohort 2\nFigure 1. High level overview of using the transformer model for longitudinal data. a) Each row of patient data corresponds\nto an ASM taken for a duration of time. Individual patients may trial diÔ¨Äerent ASMs. b) The raw data is normalized\nacross each variable to ¬µ= 0 and œÉ= 1 within the training set and applied to the validation and test set. ASM trials are\ngrouped by patient and split into training/validation sets. c) The transformer is trained and tuned with the Ô¨Årst cohort.\nd) The transformer will output a treatment outcome probability for each patient based on the ASM provided. The model\ncan provide a prediction to the ASM treatment response for individual epilepsy patients.\n‚Ä¢We are the Ô¨Årst to utilise and show that an attention-based model can capture the latent structure for\nindividual epilepsy patient response to ASM treatment. The transformer model is able to capture additional\ninformation from the demographic and ASM information due to the multi-headed self-attention mechanism.\n‚Ä¢We show that the transformer model is more suitable for capturing the relevant longitudinal information\nin predicting treatment response compared to other time-series based models.\n‚Ä¢We achieve reasonable performance with an external validation cohort in Section 4.4 without further Ô¨Åne-\ntuning of the model. The transformer model is able to generalise to an extent as seen in Section 4.5.\n2. RELATED WORK\nMachine learning in epilepsy management can potentially assist epileptologists in clinical decision making in\ndiverse domains such as automated analysis of electroencephalography (EEG) and diagnosis though images,\nprediction of responses to ASM, and resective surgery. 11 Past papers that investigated epilepsy treatment\ntypically used traditional ML algorithms7 and there has been a recent shift towards deep learning ML algorithms\ndue to their exceptional performances in diÔ¨Äerent domains. 12 Other cases of modelling within epilepsy is to\npredict seizure recurrence after withdrawal from ASMs by using a scoring system. 8, 13 These models generally\ninclude demographic and clinical risk factors identiÔ¨Åed by standard statistical methods. There has also been\na recent paper that employed a manually tuned mathematical model to determine which ASM to use. 8 A few\nstudies have also looked at developing individualized prediction models for early diagnosis of DRE 14, 15 and for\nselecting the most appropriate ASM. 6, 14, 15\nIn recent advances of deep learning, attention-based models have shown promise in various applications such\nas NLP, image recognition, and even electronic health records (EHR) data. 4, 10, 16Attention-based models is a\nmethod in deep learning that allows the network to break down complex inputs into smaller parts for a sequence\nof data. The backbone of transformers utilises these attention-mechanisms in a modular fashion to focus its\nattention on diÔ¨Äerent parts of the data. 17 Although there have been applications of transformers within the\nmedical domain, the usage of transformers to predict treatment outcomes has not been adapted before.\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 13, 2020. ; https://doi.org/10.1101/2020.11.10.20229385doi: medRxiv preprint \n3. METHODS\nIn this section, we will discuss the methodology applied to train and validate the model. Due to the nature of\nlongitudinal data encompassing a time aspect, time-series models are used for baselines to compare against the\ntransformer model. Recurrent neural networks (RNN) and long-short term memory cells (LSTM) are networks\nthat are used to compare against the transformer. Figure 1 depicts the high level overview of the procedure used\nfor using the transformer to predict ASM treatment response for patients.\n3.1 Baseline models and transformer\nThis section explains the structure of the baseline models and the transformer architecture. Figure 2 highlights\nthe diÔ¨Äerent structures of each model.\nRNN LSTM\nœÜ X œÜ X\nœÉ œÉ\nœÉ\nX\nInput \ngate\nOutput \ngate\nForget \ngate\n‚Ñéùë°\nùë•ùë°\n‚Ñéùë°‚àí1\na) b)\nEncoder\nDecoder\nùë•ùë°\n‚Ñéùë°‚àí1\n‚Ñéùë°\nœÉ\nœÜ\nX Multiply\nSigmoid\nTanh\nùë•ùë°\n+~\nPositional \nencoding\nùëßùë°‚àí1\n+~\nPositional \nencoding\nMLP\nSigmoid\nùëßùë°Transformer\nFigure 2. In both Ô¨Ågures, xt,ht,ht‚àí1,zt represents input, output from current neuron, output from the previous time\nperiod of the current neuron, and output at the Ô¨Ånal layer respectively. a) The top shows the common MLP connection\nfor a RNN and LSTM network. RNN cells have a direct recurrent connection to itself. LSTM cells have several gates to\ncontrol the long and short term memory of the inputs and previous outputs. b) High level overview of the transformer.\n3.1.1 RNN\nRecurrent neural networks (RNN) have a feedback aspect within the neuron to roll back input sequences for\ntime-series data.18 Basic RNNs follow a standard multi-layer perceptron (MLP) except for each neuron in the\nhidden layer, there is a connection that feed back into itself. Hence, this weight can be updated and it allows\nthe neuron to retain temporal information for a sequence of data. However, due to the nature of a RNN having\na simple recurrent loop, the network cannot hold long term memory as the gradient Ô¨Çowing back in time will\neither diminish or explode. 18\nBefore the prevalence of transformers, RNNs were typically used for time series data due to the simplicity\nof the network. However, RNNs are diÔ¨Écult to train and extending the complexity of the network results in\nvanishing and exploding gradients during back propagation of the network. 19\n3.1.2 LSTM\nLSTMs tackles the long term dependency by introducing ‚Äùgates‚Äù into the cell where the gates act as a switch\nto control the reading and writing of inputs within the cell. 18 As the gates control the read and write of past\ninput sequences, it mitigates the vanishing and exploding gradients problem encountered by a RNN. LSTMs are\nmore complex compared to a basic RNN, as it has several gates and additional activation functions within the\nmodel.18\nAlthough LSTMs were created to mitigate the vanishing and exploding gradient problems faced by RNNs,\nLSTMs are unable to model extensively long term dependencies. Furthermore, LSTMs are diÔ¨Écult to train in\nnature compared to a non-recurrent neural network.\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 13, 2020. ; https://doi.org/10.1101/2020.11.10.20229385doi: medRxiv preprint \nEncoder Layer\n+\nMulti-head \nattention\nNormalize\nASM 1\nInput patient \ndata\n+\nPositional \nencoding\nDecoder Layer\n+\nMulti-head \nattention\nNormalize\nTreatment \noutcomes\n+\nPositional \nencoding\n+\nNormalize\nFeedforward\n+\nNormalize\nFeedforward\n+\nMulti-head \nattention\nNormalize\nLinear\nSigmoid & Threshold\nPredicted outcome\nASM 2\nNo outcome\nOutcome 1\nTransformer\nFigure 3. Overview of a single encoder-decoder pair for a transformer network. Note that the modularity of the transformer\narises from stacking encoder and decoders. The transformer model utilises the original structure17 except there is no input\nembedding that needs to be learnt. Each ASM trial is fed into the network sequentially and the time period is encoded\nvia the positional encoder.\n3.2 Transformer model\nThe transformer model is a relatively new deep learning model that uses an attention mechanism which allows\nthe model to pay attention to diÔ¨Äerent parts of an input sequence of data. 17 Transformers are able to focus on\ndiÔ¨Äerent parts of the data by dividing the self-attention mechanism into parts and aggregating the results in the\nlatter part of the model.\nIn this work, we will apply a vanilla transformer 17 to predict ASM treatment responses at the individual\npatient level, using longitudinal registries of new onset epilepsy patients containing comprehensive diagnostic\nand follow-up information. Transformers have demonstrated prominence in modelling of longitudinal electronic\nhealth records (EHR) for personalised diagnosis. 16, 20 We adopt the transformer to predict individual patient\nresponse to ASM treatment with longitudinal data.\nTo adapt the transformer to the longitudinal data, each ASM trial is a separate input to the transformer\nas seen in Figure 3. The inputs are passed on sequentially for each patient with the inputs being reset when it\nreaches the next patient. The inputs are Ô¨Årst passed through a positional encoder to encode the time step for\nthe ASM trial. The encoded data is passed through a multi-head attention layer described as:\nTout = softmax( (TinK)(TinQ)T\n‚àödk\n)(TinV))\nwhere Tin,Tout are input and output data in the mult-head attention layer. K,Q,V are key, query, and value\nvectors that retrieves corresponding values for each of the inputs. 1‚àödk\nrepresents a scaling factor for the queried\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 13, 2020. ; https://doi.org/10.1101/2020.11.10.20229385doi: medRxiv preprint \nvalue. The multi-attention layer analyses the encoding of the current ASM trial and its encoded information such\nas demographic and past ASM trials, and relates it to other ASM trials. There are several multi-head attention\nlayers within the network as the transformer builds its own representation of the data within the feedforward\nlayers. The inputs to the decoder are the treatment responses from previous ASM trial outcomes and current\nASM trial information passed from the encoder.\nWe utilise the transformer in this setting due to its ability to capture hidden latent dependencies between each\npatient ASM trial, and the ability of the attention mechanism to focus on diÔ¨Äerent ASM treatment outcomes.\nThe transformer model also allows a variable length of patient visits, hence it is not restricted to only training\nand validating the model on the Ô¨Årst few regimen. This allows the transformer model to eÔ¨Äectively utilise the\nentire dataset.\n3.3 Model training\nTo utilise the data eÔ¨Äectively, the model was trained on each progressive ASM trial. For example, a patient\ntreated with 2 regimens will be used to train the model 2 times, with the Ô¨Årst time only using the Ô¨Årst regimen\nand the second time utilising both ASM trials. Furthermore, the Ô¨Ånal treatment outcome for the current sequence\nof data is always masked within the training data to avoid the model from seeing the current treatment outcome.\nThe transformer model uses a 5-fold cross validation approach on the Glasgow dataset to obtain optimal hyper-\nparameters. Table 4 in the Appendix shows the Ô¨Ånal hyper parameter values that were used. The model was\nused to validate its performance on the Perth cohort without any further training of the models using the Perth\ndataset as seen in section 4.5.\n4. EXPERIMENTS\n4.1 Dataset\nSex\nGlasgow Perth\n0%\n25%\n50%\n75%\n100%\nFemale\nMale\nBirth trauma\nGlasgow Perth\n0%\n25%\n50%\n75%\n100% No\nY es\nNA\nCerebral infection\nGlasgow Perth\n0%\n25%\n50%\n75%\n100% No\nY es\nNA\nCerebrovascular disease\nGlasgow Perth\n0%\n25%\n50%\n75%\n100%\nNo\nY es\nDrug abuse\nGlasgow Perth\n0%\n25%\n50%\n75%\n100%\nNo\nY es\nFamily history\nGlasgow Perth\n0%\n25%\n50%\n75%\n100% No\nY es\nNA\nFebrile seizure\nGlasgow Perth\n0%\n25%\n50%\n75%\n100% No\nY es\nNA\nFocal epilepsy\nGlasgow Perth\n0%\n25%\n50%\n75%\n100%\nNo\nY es\nHead trauma\nGlasgow Perth\n0%\n25%\n50%\n75%\n100% No\nY es\nNA\nLearning disability\nGlasgow Perth\n0%\n25%\n50%\n75%\n100% No\nY es\nNA\nEpilepsy class\nGlasgowPerth\n0%\n25%\n50%\n75%\n100% Generalised\nSymptomatic\nUnknown \naetiology\n0.00\n0.01\n0.02\n25 50 75\nGlasgow\nPerth\nAge initiated\nEEG findings\nGlasgow Perth\n0%\n25%\n50%\n75%\n100%\nEpileptiform abnormality\nNon‚àíepileptiform abnormality\nNormal\nNA\nBrain MRI\nGlasgow Perth\n0%\n25%\n50%\n75%\n100%\nEpileptiform abnormality\nNon‚àíepileptiform abnormality\nNormal\nNA\nFigure 4. Side-by-side comparison of the Glasgow and Perth cohort patient data. The y-axis represents the % of patients\nwithin that cohort with the coloured label.\nThe data was obtained from two longitudinal registries of patients maintained in UK (Glasgow) and in\nAustralia (Perth). These registries include a total of 2,272 adults with newly diagnosed and treated epilepsy.\nThe Glasgow dataset (n=1536) includes patients seen at the Epilepsy Unit of Western inÔ¨Årmary in Glasgow,\nScotland from 1 July 1982 to 31 October 2012 and followed up prospectively till 30 April 2016 or death. The Perth\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 13, 2020. ; https://doi.org/10.1101/2020.11.10.20229385doi: medRxiv preprint \nTable 1. Discretizing continuous variables. Numbers represent (¬µ ¬± œÉ)\nVariables Accuracy (%) AUC\nContinuous 76.71 ¬±0.45 0.78 ¬±0.01\nDiscretized 80.93 ¬±0.57 0.85 ¬±0.01\ndataset (n=736) has started registering patients seen at First Seizure Clinics in Western Australia since 1999\nand is being actively maintained. The patients‚Äô characteristics, treatment approach, and database structure are\nsimilar across the registries. The registries collect similar patient data including demographics, medical history,\nfamily history, epilepsy risk factors, ASM regimens, pre-treatment seizure number and frequency, EEG and MRI\nresults, and treatment response (including seizure control and tolerability).\nBetween the two cohorts, there are diÔ¨Äerences in the distribution with the demographic information such as\nthe patient history, EEG, age initiated and MRI results as seen in Figure 4. The Perth dataset is more dispersed\nacross the age group whereas the Glasgow dataset has a younger population. There is also a lower proportion of\nepileptogenic abnormalities on brain imaging and higher proportion of patients with unknown aetiology in the\nGlasgow cohort which could be due low Ô¨Åeld MRI scans before the year 2000. The list of variables describing\nthe patients used in this work ‚àócan be found in table 5.\n4.2 Pre-processing\nThe dataset was normalized before being used for training and validation of the model . Before normalization, the\ncategorical variables were binarised into separate columns for each category and continuous variables were split\ninto discrete categories to reduce the complexity for the model. Furthermore, ASMs that were not commonly\nprescribed were removed from both cohorts.\nTo prevent information from leaking between the training and validation cohorts, the normalization step was\ncalculated from the training samples for each input variable ( ¬µi = 0,œÉi = 1,where i is the ith input variable)\nand the normalization values were applied to the validation samples. This allows the transformer model to\npredict treatment outcomes of new patients by applying the same normalization. Moreover, the complexity of\nthe input variables were further reduced by categorising continuous variables into discrete categories based on\nquartiles. This was shown to improve the model performance as shown in Section 4.3.\n4.3 Ablation studies\nIn this section, we perform ablation studies on the techniques described in Section 4.2 to isolate their individual\ncontributions. The studies were performed with a 5-fold cross validation with the Glasgow cohort. Accuracy\nis determined by the predicted treatment outcome by the model based on each ASM trial, with a threshold of\n0.5 to separate successful and unsuccessful treatment outcomes. This predicted treatment outcome is compared\nwith the actual result from the ASM trial, and the accuracy is calculated across each ASM trial in the 5-fold\ncross validation. AUC is derived from the predicted treatment outcome values compared to the ground truth (0\n= unsuccessful, 1 = successful) as the model predicts values between 0 to 1.\nCategorising continuous variablesTo reduce the complexity of the data, continuous variables such as age\nwere categorised into equal segments. The results in Table 1 highlights the eÔ¨Äectiveness of reducing the complexity\nof the input data by discretizing the continuous variables.\nMulti-head attentionHere, we study the eÔ¨Äects of tuning the number of heads for the multi-head attention\nmechanism. The results in Table 2 highlight the use of additional heads in improving the performance of the\nmodel. This shows that the model beneÔ¨Åts from separating the embedding spaces within the network.\n‚àóDetailed explanation of the Glasgow data can be found in 21\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 13, 2020. ; https://doi.org/10.1101/2020.11.10.20229385doi: medRxiv preprint \nTable 2. EÔ¨Äect of multi-head attention. Numbers represent ( ¬µ¬± œÉ)\n# of heads Accuracy (%) AUC\n1 80.54 ¬±0.36 0.84 ¬±0.01\n2 80.93 ¬±0.57 0.85 ¬±0.01\n3 80.53 ¬±0.60 0.83 ¬±0.01\n4.4 Results\nThe model performances were based oÔ¨Ä a 5-fold cross validation split of the Glasgow dataset. Table 3 highlights\nthe capability of the transformer to model the variable lengths between epilepsy treatment outcomes. In contrast,\nRNN and LSTM have trouble modelling the diÔ¨Äerent time lengths across the patient dataset. LSTM converges\nto classifying a single treatment outcome for each ASM treatment while the RNN converges to a performance\nthat is lower than the transformer model. This can also be explained with the sequences of data typically being\naround 2-4 trials long, with the highest number of trials within the dataset being 13 trials for an individual\npatient. Due to the nature of the dataset, the longer term dependencies are not as prevalent compared to the\nshorter term dependencies for each treatment response.\nTable 3. Model testing performance comparison on Glasgow cohort. Numbers represent ( ¬µ¬± œÉ)\nModel Accuracy (%) AUC\nRNN 75.18 ¬±0.85 0.78 ¬±0.02\nLSTM 75.83 ¬±0.73 0.69 ¬±0.03\nTransformer 80.93 ¬±0.57 0.85 ¬±0.01\nOverall, the performance of the transformer can be attributed to the architecture of the model, as it focuses\non relevant embeddings within the network due to the multi-head attention mechanism. It is also able to capture\nthe time aspect eÔ¨Äectively without the direct use of recurrent connections compared to RNNs and LSTMs.\n4.5 Cross-Cohort validation\nTo ensure that the model is generalising well, a cross-cohort validation was used to test the transformer model.\nThe trained transformer model from the Glasgow dataset was applied directly to the Perth dataset without\nfurther Ô¨Åne-tuning of the model. The accuracy and AUC on the Perth dataset are 74.05% and 0.79 respectively.\nFigure 5 shows the similarities of the two cohorts captured by the encoder of the transformer but there are\nstill distinct clusters of each cohort that the transformer cannot capture. Figure 6 shows the misclassiÔ¨Åcations\n(indicated by the blue points) are clustered closely with the correctly classiÔ¨Åed treatment outcomes. The cluster\nof Perth data towards the bottom right of the Figure 5 indicates that the model has not been able to generalise\ninformation from the Glasgow patients as there are clusters of Perth patients resulting in misclassiÔ¨Åcations of the\nPerth cohort towards bottom right of Figure 6. This indicates that the transformer model requires additional\nrelevant variables and data to further separate the treatment outcomes within the latent space of the model.\n5. LIMITATIONS AND DISCUSSION\nWe present a novel approach in adapting a transformer architecture to model epilepsy treatment responses. The\napproach in this work can greatly advance the application of machine learning in predicting treatment outcome\nin epilepsy, by using novel deep learning techniques in this area. While previous attempts at the same were done\nlargely through surrogate markers of treatment response, 14, 15, 22we have the advantage of being able to train\nour predictive model using relevant clinical information with known standard treatment outcome measures in\ntwo large cohorts of newly diagnosed and treated epilepsy patients. 3, 23\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 13, 2020. ; https://doi.org/10.1101/2020.11.10.20229385doi: medRxiv preprint \n20\n 15\n 10\n 5\n 0 5 10 15\nt-SNE projection axis one\n15\n10\n5\n0\n5\n10\n15\nt-SNE projection axis two\nLocation\nGlasgow Data\nPerth Data\nFigure 5. Projection of the Glasgow validation and Perth patients within the last encoder layer of the transformer network\n20\n 15\n 10\n 5\n 0 5 10 15\nt-SNE projection axis one\n15\n10\n5\n0\n5\n10\n15\nt-SNE projection axis two\nCorrect\nCorrect Prediction\nIncorrect Prediction\nOutcome\nSuccessful treatment outcome\nUnsuccessful treatment outcome\nFigure 6. Projection of the diÔ¨Äerent predictions for the treatment outcome of each ASM schedule. There are clusters of\nincorrect predictions which indicates the complexity of separating the treatment outcomes\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 13, 2020. ; https://doi.org/10.1101/2020.11.10.20229385doi: medRxiv preprint \nAlthough the preliminary results show promise of the transformer architecture being able to predict the\noutcome of epilepsy treatment, our study has notable limitations such as limited patient information and data.\nWe plan to improve the prediction models by incorporating more clinically relevant input variables for model\ntraining such as the latest etiological classiÔ¨Åcation of epilepsy proposed by International League Against Epilepsy\n(ILAE),24 electroencephalographic abnormalities, relevant imaging abnormalities, comorbidities considered dur-\ning drug selection and concomitant medication use.\nIn clinical practice, safety of ASM is also factored in while prescribing. An eÔ¨Écacious and tolerable drug\nmay not be the safest to use in a given situation. 21 For instance, although valproate is eÔ¨Écacious for idiopathic\ngeneralised epilepsy, it is not recommended for women of child bearing age due to its teratogenic potential.\nThese sorts of decisions require more nuanced knowledge of balancing pros and cons of selecting a particular\nregimen. As the model does not naturally account for this limitation, a generalized approach needs to be further\ninvestigated to implement these additional considerations.\n6. CONCLUSION AND CLINICAL OUTCOMES\nThe usage of an attention-based model to capture relationships within treatment outcomes for epilepsy patients\nhas yielded promising results. The model was able to generalise well to an external cohort without further Ô¨Åne-\ntuning of the model. In the future, the model can be improved upon with additional variables, which will allow\nthe model to aid clinicians in selecting the most suitable ASM based on its predicted eÔ¨Écacy in an individual.\nREFERENCES\n[1] Fiest, K. M., Sauro, K. M., Wiebe, S., Patten, S. B., Kwon, C. S., Dykeman, J., Pringsheim, T., Lorenzetti,\nD. L., and Jette, N., ‚ÄúPrevalence and incidence of epilepsy: A systematic review and meta-analysis of\ninternational studies,‚Äù Neurology 88(3), 296‚Äì303 (2017).\n[2] Kerr, M. P., ‚ÄúThe impact of epilepsy on patients‚Äô lives,‚Äù Acta Neurol Scand Suppl (194), 1‚Äì9 (2012).\n[3] Chen, Z., Brodie, M. J., Liew, D., and Kwan, P., ‚ÄúTreatment outcomes in patients with newly diagnosed\nepilepsy treated with established and new antiepileptic drugs: A 30-year longitudinal cohort study,‚Äù JAMA\nNeurol 75(3), 279‚Äì286 (2018).\n[4] Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., and Kang, J., ‚ÄúBiobert: a pre-trained biomedical\nlanguage representation model for biomedical text mining,‚Äù Bioinformatics 36(4), 1234‚Äì1240 (2020).\n[5] Yi, X., Walia, E., and Babyn, P., ‚ÄúGenerative adversarial network in medical imaging: A review,‚Äù Med\nImage Anal 58, 101552 (2019).\n[6] Liu, Y., Sivathamboo, S., Goodin, P., Bonnington, P., Kwan, P., Kuhlmann, L., O‚ÄôBrien, T., Perucca,\nP., and Ge, Z., ‚ÄúEpileptic seizure detection using convolutional neural network: A multi-biosignal study,‚Äù\nin [Proceedings of the Australasian Computer Science Week Multiconference 2020, ACSW 2020 ], Forkan,\nA., ed., Association for Computing Machinery (ACM), United States of America (feb 2020). Australasian\nComputer Science Week Multiconference 2020, ACSW 2020 ; Conference date: 03-02-2020 Through 07-02-\n2020.\n[7] Colic, S., Wither, R. G., Lang, M., Zhang, L., Eubanks, J. H., and Bardakjian, B. L., ‚ÄúPrediction of\nantiepileptic drug treatment outcomes using machine learning,‚Äù J Neural Eng 14(1), 016002 (2017).\n[8] Choi, H., Detyniecki, K., Bazil, C., Thornton, S., Crosta, P., Tolba, H., Muneeb, M., Hirsch, L. J., Heinzen,\nE. L., Sen, A., Depondt, C., Perucca, P., Heiman, G. A., and Consortium, E., ‚ÄúDevelopment and validation\nof a predictive model of drug-resistant genetic generalized epilepsy,‚Äù Neurology (2020).\n[9] Xiong, Y., Du, B., and Yan, P., [ Reinforced Transformer for Medical Image Captioning], 673‚Äì680 (10 2019).\n[10] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S., [ End-to-End Object\nDetection with Transformers] (2020).\n[11] Abbasi, B. and Goldenholz, D. M., ‚ÄúMachine learning applications in epilepsy,‚Äù Epilepsia 60(10), 2037‚Äì2047\n(2019).\n[12] Esteva, A., Robicquet, A., Ramsundar, B., Kuleshov, V., DePristo, M., Chou, K., Cui, C., Corrado, G.,\nThrun, S., and Dean, J., ‚ÄúA guide to deep learning in healthcare,‚Äù Nat Med 25(1), 24‚Äì29 (2019).\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 13, 2020. ; https://doi.org/10.1101/2020.11.10.20229385doi: medRxiv preprint \n[13] Asadi-Pooya, A. A., Beniczky, S., Rubboli, G., Sperling, M. R., Rampp, S., and Perucca, E., ‚ÄúA pragmatic\nalgorithm to select appropriate antiseizure medications in patients with epilepsy,‚Äù Epilepsia (2020).\n[14] An, S., Malhotra, K., Dilley, C., Han-Burgess, E., Valdez, J. N., Robertson, J., Clark, C., Westover, M. B.,\nand Sun, J., ‚ÄúPredicting drug-resistant epilepsy - a machine learning approach based on administrative\nclaims data,‚Äù Epilepsy Behav 89, 118‚Äì125 (2018).\n[15] Delen, D., Davazdahemami, B., Eryarsoy, E., Tomak, L., and Valluru, A., ‚ÄúUsing predictive analytics to\nidentify drug-resistant epilepsy patients,‚Äù Health Informatics J 26(1), 449‚Äì460 (2020).\n[16] Choi, E., Xu, Z., Li, Y., Dusenberry, M. W., Flores, G., Xue, Y., and Dai, A. M., ‚ÄúGraph convolutional\ntransformer: Learning the graphical structure of electronic health records,‚Äù ArXiv abs/1906.04716 (2019).\n[17] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,  L., and Polosukhin,\nI., ‚ÄúAttention is all you need,‚Äù in [ Advances in Neural Information Processing Systems ], 5998‚Äì6008 (2017).\n[18] Yu, Y., Si, X., Hu, C., and Zhang, J., ‚ÄúA review of recurrent neural networks: Lstm cells and network\narchitectures,‚Äù Neural Computation 31, 1‚Äì36 (05 2019).\n[19] Informatik, F., Bengio, Y., Frasconi, P., and Schmidhuber, J., ‚ÄúGradient Ô¨Çow in recurrent nets: the diÔ¨Éculty\nof learning long-term dependencies,‚Äù A Field Guide to Dynamical Recurrent Neural Networks (03 2003).\n[20] Li, Y., Rao, S., Solares, J. R. A., Hassaine, A., Ramakrishnan, R., Canoy, D., Zhu, Y., Rahimi, K., and\nSalimi-Khorshidi, G., ‚ÄúBehrt: Transformer for electronic health records,‚Äù ScientiÔ¨Åc Reports 10(1), 7155\n(2020).\n[21] Perucca, E., Brodie, M. J., Kwan, P., and Tomson, T., ‚Äú30 years of second-generation antiseizure medica-\ntions: impact and future perspectives,‚Äù Lancet Neurol 19(6), 544‚Äì556 (2020).\n[22] Devinsky, O., Dilley, C., Ozery-Flato, M., Aharonov, R., Goldschmidt, Y., Rosen-Zvi, M., Clark, C., and\nFritz, P., ‚ÄúChanging the approach to treatment choice in epilepsy using big data,‚Äù Epilepsy Behav 56, 32‚Äì7\n(2016).\n[23] Sharma, S., Chen, Z., Rychkova, M., Dunne, J., Lee, J., Kalilani, L., Lawn, N., and Kwan, P., ‚ÄúTreat-\nment initiation decisions in newly diagnosed epilepsy-a longitudinal cohort study,‚Äù Epilepsia 61(3), 445‚Äì454\n(2020).\n[24] Fisher, R. S., Cross, J. H., D‚ÄôSouza, C., French, J. A., Haut, S. R., Higurashi, N., Hirsch, E., Jansen,\nF. E., Lagae, L., Moshe, S. L., Peltola, J., Roulet Perez, E., ScheÔ¨Äer, I. E., Schulze-Bonhage, A., Somerville,\nE., Sperling, M., Yacubian, E. M., and Zuberi, S. M., ‚ÄúInstruction manual for the ilae 2017 operational\nclassiÔ¨Åcation of seizure types,‚Äù Epilepsia 58(4), 531‚Äì542 (2017).\nAPPENDIX A. HYPER-PARAMETERS\nFigure 4 the list of hyper-parameters that were tuned with with the training/validation split of the Glasgow\ndataset.\nTable 4. Tuned hyper-parameters for the Ô¨Ånal transformer model\nHyper-parameter Final value\nNumber of encoder/decoder pairs 3\nNumber of heads 2\nLearning rate 3e-4\nLearning rate decay factor 7e-1\nEarly stopping epochs 10\nNumber of epochs 50\nAPPENDIX B. LIST OF VARIABLES\nFigure 5 outlines the patient variables that were used to train the model. The ASMs that were used in the\nGlasgow and Perth datasets are not shown in this section. Further information on the variables can be found\nin.21\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 13, 2020. ; https://doi.org/10.1101/2020.11.10.20229385doi: medRxiv preprint \nTable 5. List of variables describing the patient.\nVariable Description\nSex Biological gender of the patient\nAge initiated Age at initiation with the Ô¨Årst antiseizure medication\ntreatment\nEpilepsy class Epilepsy classiÔ¨Åcation\nFocal Dichotomised epilepsy classiÔ¨Åcation, whether it is focal\nepilepsy or not\nFamily history Prior to the treatment initiation, whether the patient has\nfamily history of epilepsy or not\nFebrile Prior to the treatment initiation, whether the patient has\nhistory of febrile seizure or not\nCerebral infection Prior to the treatment initiation, whether the patient has\nhistory of cerebral infection or not\nBirth trauma Prior to the treatment initiation, whether the patient has\nhistory of birth trauma or not\nHead injury Prior to the treatment initiation, whether the patient has\nhistory of head injury or not\nDrug Prior to the treatment initiation, whether the patient has\nhistory of drug abuse or not\nAlcohol Prior to the treatment initiation, whether the patient has\nhistory of alcohol abuse or not\nCerebrovascular disease Prior to the treatment initiation, whether the patient\nhas history of cerebrovascular disease or not\nPsychiatric comorbidities Prior to the treatment initiation, whether the patient has\nhistory of psychiatric comorbidities or not\nLearning disability Prior to the treatment initiation, whether the patient\nhas history of learning disability or not\nBrain MRI Categorized CT/MRI Ô¨Åndings\nEEG category Categorized EEG Ô¨Åndings\nOutcome Terminal treatment outcome, at the end of study period\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted November 13, 2020. ; https://doi.org/10.1101/2020.11.10.20229385doi: medRxiv preprint "
}