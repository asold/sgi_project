{
  "title": "Linguistic Profiling of a Neural Language Model",
  "url": "https://openalex.org/W3092469091",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5062667199",
      "name": "Alessio Miaschi",
      "affiliations": [
        "University of Pisa"
      ]
    },
    {
      "id": "https://openalex.org/A5057659257",
      "name": "Dominique Brunato⋄",
      "affiliations": [
        "National Academies of Sciences, Engineering, and Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5084812833",
      "name": "Felice Dell’Orletta⋄",
      "affiliations": [
        "University of Groningen"
      ]
    },
    {
      "id": "https://openalex.org/A5080111149",
      "name": "Giulia Venturi⋄",
      "affiliations": [
        "Scuola Superiore Sant'Anna"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2908400318",
    "https://openalex.org/W2963651521",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W2962777840",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W2299976354",
    "https://openalex.org/W2132781244",
    "https://openalex.org/W3037722591",
    "https://openalex.org/W3029139904",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2055011070",
    "https://openalex.org/W2579343286",
    "https://openalex.org/W131522978",
    "https://openalex.org/W3102226577",
    "https://openalex.org/W2128961655",
    "https://openalex.org/W2292919134",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1951216520",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2914924671",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W2250496306",
    "https://openalex.org/W2897507397",
    "https://openalex.org/W2971044268",
    "https://openalex.org/W3037458976",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2250263931",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2181262297",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2808681756"
  ],
  "abstract": "In this paper we investigate the linguistic knowledge learned by a Neural Language Model (NLM) before and after a fine-tuning process and how this knowledge affects its predictions during several classification problems. We use a wide set of probing tasks, each of which corresponds to a distinct sentence-level feature extracted from different levels of linguistic annotation. We show that BERT is able to encode a wide range of linguistic characteristics, but it tends to lose this information when trained on specific downstream tasks. We also find that BERT's capacity to encode different kind of linguistic properties has a positive influence on its predictions: the more it stores readable linguistic information of a sentence, the higher will be its capacity of predicting the expected label assigned to that sentence.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 745–756\nBarcelona, Spain (Online), December 8-13, 2020\n745\nLinguistic Proﬁling of a Neural Language Model\nAlessio Miaschi⋆ ⋄, Dominique Brunato⋄, Felice Dell’Orletta⋄, Giulia Venturi⋄\n⋆Department of Computer Science, University of Pisa\n⋄Istituto di Linguistica Computazionale “Antonio Zampolli”, Pisa\nItaliaNLP Lab – www.italianlp.it\nalessio.miaschi@phd.unipi.it, {name.surname}@ilc.cnr.it\nAbstract\nIn this paper we investigate the linguistic knowledge learned by a Neural Language Model (NLM)\nbefore and after a ﬁne-tuning process and how this knowledge affects its predictions during\nseveral classiﬁcation problems. We use a wide set of probing tasks, each of which corresponds to\na distinct sentence-level feature extracted from different levels of linguistic annotation. We show\nthat BERT is able to encode a wide range of linguistic characteristics, but it tends to lose this\ninformation when trained on speciﬁc downstream tasks. We also ﬁnd that BERT’s capacity to\nencode different kind of linguistic properties has a positive inﬂuence on its predictions: the more\nit stores readable linguistic information of a sentence, the higher will be its capacity of predicting\nthe expected label assigned to that sentence.\n1 Introduction\nNeural Language Models (NLMs) have become a central component in NLP systems over the last few\nyears, showing outstanding performance and improving the state-of-the-art on many tasks (Peters et al.,\n2018; Radford et al., 2018; Devlin et al., 2019). However, the introduction of such systems has come\nat the cost of interpretability and, consequently, at the cost of obtaining meaningful explanations when\nautomated decisions take place.\nRecent work has begun to study these models in order to understand whether they encode linguistic\nphenomena even without being explicitly designed to learn such properties (Marvin and Linzen, 2018;\nGoldberg, 2019; Warstadt et al., 2019). Much of this work focused on the analysis and interpretation of\nattention mechanisms (Tang et al., 2018; Jain and Wallace, 2019; Clark et al., 2019) and on the deﬁnition\nof probing models trained to predict simple linguistic properties from unsupervised representations.\nProbing models trained on different contextual representations provided evidences that such models\nare able to capture a wide range of linguistic phenomena (Adi et al., 2016; Perone et al., 2018; Tenney\net al., 2019b) and even to organize this information in a hierarchical manner (Belinkov et al., 2017; Lin\net al., 2019; Jawahar et al., 2019). However, the way in which this knowledge affects the decisions they\nmake when solving speciﬁc downstream tasks has been less studied.\nIn this paper, we extended prior work by studying the linguistic properties encoded by one of the\nmost prominent NLM, BERT (Devlin et al., 2019), and how these properties affect its predictions when\nsolving a speciﬁc downstream task. We deﬁned three research questions aimed at understanding: (i)\nwhat kind of linguistic properties are already encoded in a pre-trained version of BERT and where across\nits 12 layers; (ii) how the knowledge of these properties is modiﬁed after a ﬁne-tuning process; (iii)\nwhether this implicit knowledge affects the ability of the model to solve a speciﬁc downstream task,\ni.e. Native Language Identiﬁcation (NLI). To tackle the ﬁrst two questions, we adopted an approach\ninspired to the ‘linguistic proﬁling’ methodology put forth by van Halteren (2004), which assumes that\nwide counts of linguistic features automatically extracted from parsed corpora allow modeling a speciﬁc\nlanguage variety and detecting how it changes with respect to other varieties, e.g. complex vs simple\nlanguage, female vs male–authored texts, texts written in the same L2 language by authors with different\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://\ncreativecommons.org/licenses/by/4.0/.\n746\nL1 languages. Particularly relevant for our study, is that multi-level linguistic features have been shown\nto have a highly predictive role in tracking the evolution of learners’ linguistic competence across time\nand developmental levels, both in ﬁrst and second language acquisition scenarios (Lubetich and Sagae,\n2014; Miaschi et al., 2020).\nGiven the strong informative power of these features to encode a variety of language phenomena across\nstages of acquisition, we assume that they can be also helpful to dig into the issues of interpretability of\nNLMs. In particular, we would like to investigate whether features successfully exploited to model the\nevolution of language competence can be similarly helpful in proﬁling how the implicit linguistic knowl-\nedge of a NLM changes across layers and before and after tuning on a speciﬁc downstream task. We\nchose the NLI task, i.e. the task of automatically classifying the L1 of a writer based on his/her language\nproduction in a learned language (Malmasi et al., 2017). As shown by Cimino et al. (2018), linguistic\nfeatures play a very important role when NLI is tackled as a sentence–classiﬁcation task rather than as\na traditional document–classiﬁcation task. This is the reason why we considered the sentence-level NLI\nclassiﬁcation as a task particularly suitable for probing the NLM linguistic knowledge. Finally, we in-\nvestigated whether and which linguistic information encoded by BERT is involved in discriminating the\nsentences correctly or incorrectly classiﬁed by the ﬁne-tuned models. To this end, we tried to under-\nstand if the linguistic knowledge that the model has of a sentence affects the ability to solve a speciﬁc\ndownstream task involving that sentence.\nContributions In this paper: (i) we carried out an in-depth linguistic proﬁling of BERT’s internal\nrepresentations (ii) we showed that contextualized representations tend to lose their precision in encod-\ning a wide range of linguistic properties after a ﬁne-tuning process; (iii) we showed that the linguistic\nknowledge stored in the contextualized representations of BERT positively affects its ability to solve NLI\ndownstream tasks: the more BERT stores information about these features, the higher will be its capacity\nof predicting the correct label.\n2 Related Work\nIn the last few years, several methods have been devised to obtain meaningful explanations regarding\nthe linguistic information encoded in NLMs (Belinkov and Glass, 2019). They range from techniques to\nexamine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to\nmore domain speciﬁc approaches, such as interpreting attention mechanisms (Raganato and Tiedemann,\n2018; Kovaleva et al., 2019; Vig and Belinkov, 2019), studying correlations between representations\n(Saphra and Lopez, 2019) or designing speciﬁc probing tasks that a model can solve only if it captures a\nprecise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as\ntraining features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019; Miaschi and\nDell’Orletta, 2020). These latter studies demonstrated that NLMs are able to encode a variety of language\nproperties in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney et al., 2019b)\nand even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). Jawahar et\nal. (2019) investigated the representations learned at different layers of BERT, showing that lower layer\nrepresentations are usually better for capturing surface features, while embeddings from higher layers are\nbetter for syntactic and semantic properties. Using a suite of probing tasks, Tenney et al. (2019a) found\nthat the linguistic knowledge encoded by BERT through its 12/24 layers follows the traditional NLP\npipeline: POS tagging, parsing, NER, semantic roles and then coreference. Liu et al. (2019), instead,\nquantiﬁed differences in the transferability of individual layers between different models, showing that\nhigher layers of RNNs (ELMo) are more task-speciﬁc (less general), while transformer layers (BERT)\ndo not exhibit this increase in task-speciﬁcity.\n3 Our Approach\nTo probe the linguistic knowledge encoded by BERT and understand how it affects its predictions in\nseveral classiﬁcation problems, we relied on a suite of 68 probing tasks, each of which corresponds to a\ndistinct feature capturing lexical, morpho–syntactic and syntactic properties of a sentence. Speciﬁcally,\n747\nLevel of Annotation Linguistic Feature Label\nRaw Text\nRaw Text Properties (RawText)\nSentence Length sent length\nWord Length char per tok\nV ocabulary Vocabulary Richness (Vocabulary)\nType/Token Ratio for words and lemmas ttr form, ttr lemma\nPOS tagging\nMorphosyntactic information (POS)\nDistibution of UD and language–speciﬁc POS upos dist *, xpos dist *\nLexical density lexical density\nInﬂectional morphology (VerbInﬂection)\nInﬂectional morphology of lexical verbs and auxiliaries xpos VB-VBD-VBP-VBZ, aux *\nDependency Parsing\nVerbal Predicate Structure (VerbPredicate)\nDistribution of verbal heads and verbal roots verbal head dist, verbal root perc\nVerb arity and distribution of verbs by arity avg verb edges, verbal arity *\nGlobal and Local Parsed Tree Structures (TreeStructure)\nDepth of the whole syntactic tree parse depth\nAverage length of dependency links and of the longest link avg links len, max links len\nAverage length of prepositional chains and distribution by depth avg prep chain len, prep dist *\nClause length avg token per clause\nOrder of elements (Order)\nRelative order of subject and object subj pre, obj post\nSyntactic Relations (SyntacticDep)\nDistribution of dependency relations dep dist *\nUse of Subordination (Subord)\nDistribution of subordinate and principal clauses principal prop dist, subordinate prop dist\nAverage length of subordination chains and distribution by depth avg subord chain len, subordinate dist 1\nRelative order of subordinate clauses subordinate post\nTable 1: Linguistic Features used in the probing tasks.\nwe deﬁned three sets of experiments. The ﬁrst consisted in probing the linguistic information learned\nby a pre-trained version of BERT (BERT-base, cased) using gold sentences annotated according to the\nUniversal Dependencies (UD) framework (Nivre et al., 2016). In particular, we deﬁned a probing model\nthat uses BERT contextual representations for each sentence of the dataset and predicts the actual value\nof a given linguistic feature across the internal layers. The second set of experiments consisted in inves-\ntigating variations in the encoded linguistic information between the pre-trained model and 10 different\nﬁne-tuned ones obtained training BERT on as many Native Language Identiﬁcation (NLI) binary tasks.\nTo do so, we performed again all probing tasks using the 10 ﬁne-tuned models. For the last set of ex-\nperiments, we investigated how the linguistic competence contained in the models affects the ability of\nBERT to solve the NLI downstream tasks.\n3.1 Data\nWe used two datasets: (i) the UD English treebank (version 2.4) for probing the linguistic information\nlearned before and after a ﬁne-tuning process; (ii) a dataset used for the NLI task, which is exploited\nboth for ﬁne-tuning BERT on the downstream task and for reproducing the probing tasks in the third set\nof experiments.\nUD dataset It includes three UD English treebanks: UD English-ParTUT, a conversion of a multilin-\ngual parallel treebank consisting of a variety of text genres, including talks, legal texts and Wikipedia\narticles (Sanguinetti and Bosco, 2015); the Universal Dependencies version annotation from the GUM\ncorpus (Zeldes, 2017); the English Web Treebank (EWT), a gold standard universal dependencies corpus\nfor English (Silveira et al., 2014). Overall, the ﬁnal dataset consists of 23,943 sentences.\nNLI dataset We used the 2017 NLI shared task dataset, i.e. the TOEFL11 corpus (Blanchard et al.,\n2013). It contains test responses from 13,200 test takers (one essay and one spoken response transcription\nper test taker) and includes 11 native languages (L1s) with 1,200 test takers per L1. We selected only\nwritten essays and we created pairwise subsets of essays written by Italian L1 native speakers and essays\nfor all the other languages. At the end of this process, we obtained 10 datasets of 2,400 documents\n(33,756 sentences in average): 1,200 for the Italian L1 speakers and 1,200 for each of the other L1s\nincluded in the TOEFL11 corpus.\n3.2 Probing Tasks and Linguistic Features\nOur experiments are based on the probing tasks approach deﬁned in Conneau et al. (2018), which aims to\ncapture linguistic information from the representations learned by a NLM. In our study, each probing task\n748\nModels RawText Vocabulary POS VerbInﬂection VerbPredicate TreeStructure Order SyntacticDep Subord All\nBERT 0.68 0.78 0.68 0.72 0.60 0.78 0.72 0.69 0.71 0.69\nBaseline 0.52 0.23 0.27 0.35 0.48 0.70 0.51 0.34 0.48 0.38\nTable 2: BERT ρ scores (average between layers) for all the linguistic features (All) and for the 9 groups\ncorresponding to different linguistic phenomena. Baseline scores are also reported.\nFigure 1: BERT average layerwise ρ scores.\nconsists in predicting the value of a speciﬁc linguistic feature automatically extracted from the parsed\nsentences in the NLI and UD datasets. The set of features is based on the ones described in Brunato\net al. (2020) which are acquired from raw, morpho-syntactic and syntactic levels of annotation and can\nbe categorised in 9 groups corresponding to different linguistic phenomena. As shown in Table 1, these\nfeatures model linguistic phenomena ranging from raw text ones, to morpho–syntactic information and\ninﬂectional properties of verbs, to more complex aspects of sentence structure modeling global and local\nproperties of the whole parsed tree and of speciﬁc subtrees, such as the order of subjects and objects with\nrespect to the verb, the distribution of UD syntactic relations, also including features referring to the use\nof subordination and to the structure of verbal predicates.\n3.3 Models\nNLM We relied on the pre–trained English version of BERT (BERT-base cased, 12 layers, 768 hidden\nunits) for both the extraction of contextual embeddings and the ﬁne-tuning process for the NLI down-\nstream task. To obtain the embeddings representations for our sentence-level tasks we used for each of\nits 12 layers the activation of the ﬁrst input token ([CLS]), which somehow summarizes the information\nfrom the actual tokens, as suggested in Jawahar et al. (2019).\nProbing model As mentioned above, each of our probing tasks consists in predicting the actual value\nof a given linguistic feature given the inner sentence representations learned by a NLM for each of its\nlayers. Therefore, we used a linear Support Vector Regression (LinearSVR) as probing model.\n4 Proﬁling BERT\nOur ﬁrst experiments investigated what kind of linguistic phenomena are encoded in a pre-trained version\nof BERT. To this end, for each of the 12 layers of the model (from input layer -12 to output layer -1),\nwe ﬁrstly represented each sentence in the UD dataset using the corresponding sentence embeddings\naccording to the criterion deﬁned in Sec. 3.3. We then performed for each sentence representation our\nset of 68 probing tasks using the LinearSVR model. Since most of our probing features are strongly\ncorrelated with sentence length, we compared the probing model results with the ones obtained with a\nbaseline computed by measuring the Spearman’s rank correlation coefﬁcient ( ρ) between the length of\nthe UD dataset sentences and the corresponding probing values. The evaluation is performed with a 5-\nfold cross validation and using Spearman correlation (ρ) between predicted and gold labels as evaluation\nmetric.\n749\nFigure 2: Layerwise ρ scores for the 68 linguistic features. Absolute baseline scores are reported in\ncolumn B.\nAs a ﬁrst analysis, we probed BERT’s linguistic competence with respect to the 9 groups of probing\nfeatures. Table 2 reports BERT (average between layers) and baseline scores for all the linguistic features\nand for the 9 groups corresponding to different linguistic phenomena. As a general remark, we can notice\nthat the scores obtained by BERT’s internal representations always outperform the ones obtained with the\ncorrelation baseline. For both BERT and the baseline, the best results are obtained for groups including\nfeatures highly sensitive to sentence length. For instance, this is the case of syntactic features capturing\nglobal aspects of sentence structure (Tree structure). However, differently from the baseline, the abstract\nrepresentations of BERT are also very good at predicting features related to other linguistic information\nsuch as morpho-syntactic (POS, Verb inﬂection) and syntactic one, e.g. the structure of verbal predicate\nand the order of nuclear sentence elements (Order).\nWe then focused on how BERT’s linguistic competence changes across layers. These results are\nreported in Figure 1, where we see that the average layerwise ρ scores are lower in the last layers both\nfor all distinct groups and for all features together. As suggested by Liu et al. (2019), this could be due\nto the fact that the representations that are better-suited for language modeling (output layer) are also\nthose that exhibit worse probing task performance, indicating that Transformer layers trade off between\nencoding general and probed features. However, there are differences between the considered groups:\ncompetences about raw texts features ( RawText) and the distribution of POS are lost in the very ﬁrst\nlayers (by layer -10), while the knowledge about the order of subject/object with respect to the verb, the\nuse of subordination, as well as features related to verbal predicate structure is acquired in the middle\nlayers.\nInterestingly, if we consider how the knowledge of each feature changes across layers (Figure 2), we\nobserve that not all features belonging to the same group have an homogeneous behaviour. This is for\nexample the case of the two features included in theRawText group: word length (char per tok) achieves\nquite lower scores across all layers with respect to thesent length feature. Similarly, the knowledge about\nPOS differs when we consider more granular distinctions. For instance, within the broad categories of\nverbs and nouns, worse predictions are obtained by sub–speciﬁc classes of verbs based on tense, person\nand mood features (see especially past participle, xpos dist VBN), and by inﬂected nouns both singular\nand plural ( NN, NNS). Within the broad set of features extracted from syntactic annotation, we also\nsee that different scores are reported for features referring e.g. to types of dependency relations: those\nlinking a functional POS to its head (e.g. dep dist case, dep dist cc, dep dist conj, dep dist det) are\n750\nFigure 3: Hierarchical clustering of the 68 probing tasks based on layerwise ρ values. Bold numbers\ncorrespond to the ranking of each probing feature based on the correlation with sentence length.\nKOR TEL HIN JPN CHI TUR ARA GER FRE SPA\nBaseline 59.05 51.32 54.09 56.27 55.68 55.66 52.92 59.29 56.03 52.61\nBERT 85.74 85.18 84.75 84.19 82.78 79.29 76.38 72.78 72.50 70.03\nTable 3: NLI classiﬁcation results in terms of accuracy. We used the Zero Rule algorithm as baseline.\nNote that, for each task, sentences of the 10 languages are paired with the Italian ones (e.g. KOR =\nKOR-ITA).\nbetter predicted than others relations, such as dep dist amod, advcl. Besides, within the VerbPredicate\ngroup, lower ρ scores are obtained by features encoding sub-categorization information about verbal\npredicates, such as the distribution of verbs by arity ( verbal arity 2,3,4), which also remains almost\nstable across layers.\nSince we observed these not homogeneous scores within the groups we deﬁned a priori, we investi-\ngated how BERT hierarchically encodes across layers all the features. To this end, we clustered the 68\nlinguistic characteristics according to layerwise probing results: speciﬁcally, we performed hierarchi-\ncal clustering using Euclidean distance as distance metric and Ward variance minimization as clustering\nmethod. Interestingly enough, Figure 3 shows that the traditional division of features with respect to\nthe linguistic annotation levels has not been maintained. On the contrary, BERT puts together features\nfrom all linguistic groups into clusters of different size. In addition, these clusters gather features that\nare differently ranked according to the baseline scores (ranking positions are bolded in the ﬁgure). For\nexample, the ﬁrst cluster includes features with similar ρ scores, and both highly and lower ranked by\nthe baseline. All these features model aspects of global sentence structure, e.g. sent length, functional\nPOSs (e.g. upos dist DET, ADP, CCONJ), parsed tree structures (e.g. parse depth, verbal heads dist,\navg links len), nuclear elements of the sentence such as subjects (dep dist nsubj), verbs ( VERBS), pro-\nnouns ( PRON).\n5 The Impact of Fine–Tuning on Linguistic Knowledge\nOnce we have probed the linguistic knowledge encoded by BERT across its layers, we investigated how\nit changes after a ﬁne-tuning process. To do so, we started with the same pre-trained version of the\nmodel used in the previous experiment and performed a ﬁne-tuning process for each of the 10 subsets\n751\nFigure 4: Layerwise mean ρ scores for the pre-trained and ﬁne-tuned models.\nFigure 5: Differences between BERT–base and ﬁne–tuned modelsρ scores (multiplied by 100) computed\nusing the output layer representations (-1). Statistically signiﬁcant variations (Wilcoxon Rank-sum test)\nare marked (*).\nbuilt from the original NLI corpus (Sec. 3.1). We decided to use 50% of each NLI subset for training\n(40% and 10% for training and development set) and the remaining 50% for testing the accuracy of the\nnewly generated models.\nTable 3 reports the results for the 10 binary NLI tasks. As we can notice, BERT achieves good results\nfor all downstream tasks, meaning that is able to discriminate the L1 of a native speaker on a sentence-\nlevel regardless of the L1 pairs taken into account. The best performance is achieved by the model that\nwas ﬁne-tuned on the Korean and Italian pairwise subset, while the lowest scores are obtained with the\nmodel trained on the subset consisting of essays written by Spanish and Italian L1 speakers (SPA-ITA).\nInterestingly, these results seem to reﬂect typological distances among L1 pairs, with higher scores for\nlanguages that are more distant from Italian (Korean, Telugu or Hindi) and lower scores for L1s belonging\nto the same language family (FRE-ITA or SPA-ITA).\nAfter ﬁne-tuning the model on NLI, we performed again the suite of probing tasks on the UD dataset\nusing the 10 newly generated models and following the same approach discussed in Section 4. Figure\n4 reports layerwise mean ρ correlation values for all probing tasks obtained with BERT-base and the\nother ﬁne-tuned models. It can be noticed that the representations learned by the NLM tend to lose their\n752\nFigure 6: % of probing features for which the MSE of the sentences correctly classiﬁed by BERT-base\n(Pre-train) and the ﬁne-tuned models ( Fine-tune) is lower than that of the incorrectly ones. Results are\nreported for layers -12, -7 and -1.\nprecision in encoding our set of linguistic features after the ﬁne-tuning process. This is particularly no-\nticeable at higher layers and it possibly suggests that the model is storing task–speciﬁc information at\nthe expense of its ability to encode general knowledge about the language. Again, this is particularly\nevident for the models ﬁne–tuned on the classiﬁcation of language pairs belonging to the same family,\nSPA–ITA above all. To study which phenomena are mainly involved in this loss, we computed the dif-\nferences between the probing tasks results obtained before and after the ﬁne-tuning process. We focused\nin particular on the scores obtained on the output layer representations (layer -1), since it is the most\ntask-speciﬁc (Kovaleva et al., 2019). For each subset, Figure 5 reports the difference between the score\nof each linguistic feature obtained with the pre–trained model and the ﬁne–tuned one. Not surprisingly,\nthe loss of linguistic knowledge reﬂects the typological trend observed for overall classiﬁcation perfor-\nmance. In fact, when the task is to distinguish Italian vs German, French and Spanish L1, BERT loses\nmuch of its encoded knowledge for almost all the considered features. This is particularly evident for\nthe morpho-syntactic features (i.e. distribution of upos dist and xpos dist) and for features related to\nlexical variety (i.e. ttr form, ttr lemma). It seems that for typologically similar languages BERT needs\nmore task-speciﬁc knowledge mostly encoded at the level of morpho-syntactic information rather than\nthe structural level. On the contrary, the drop is less pronounced and in most cases not signiﬁcant for\nmodels ﬁne–tuned on the classiﬁcation of more distant languages (e.g. models ﬁne–tuned on KOR-ITA\nor TUR-ITA). In this case, the quite stable performance on the probing tasks may suggest that those fea-\ntures were still useful to perform the downstream task. Interestingly, the class of features that decreases\nsigniﬁcantly in all models are those encoding the knowledge about the tense of verbs. This is particularly\nthe case of the third-person singular verbs in the present tense ( xpos dist VBZ) and of verbs in the past\ntense (xpos dist VBD). A possible explanation could be related to the prompts of essays, which are the\nsame across the NLI dataset. Thus, the textual genre could have favored a quite homogeneous use of\nverbal morphology features by students of all L1s. This makes this class of features less useful for the\nidentiﬁcation of native languages.\n6 Are Linguistic Features useful for BERT’s predictions?\nAs a last research question we investigated whether the implicit linguistic knowledge affects BERT’s\npredictions when solving the NLI downstream task. To answer this question we have split each NLI\nsubset into two groups, i.e. sentences correctly classiﬁed according to the L1 and those incorrectly\nclassiﬁed. For the two groups of each NLI subset, we performed the probing tasks using the pre–trained\nBERT-base and the speciﬁc NLI ﬁne-tuned model. For each sentence of the two groups, we calculated\nthe variation between the actual and predicted feature value obtaining two lists of absolute errors. We\nused the Wilcoxon Rank-sum test to verify whether the two lists were selected from samples with the\nsame distribution. As a general remark, we observed that much more than half of features vary in\na statistically signiﬁcant way between correctly and incorrectly classiﬁed sentences. This suggests that\nBERT’s linguistic competence on the two groups of sentences is very different. To deepen the analysis of\n753\nthis difference, we calculated the accuracy achieved by BERT in terms of Mean Square Error (MSE) only\nfor the set of features varying in a signiﬁcant way. Figure 6 reports the percentage of features for which\nthe MSE of the sentences correctly classiﬁed ( MSE Pos) is lower than that of the incorrectly ones (MSE\nNeg). This percentage is signiﬁcantly higher, thus showing that BERT’s capacity to encode different\nkind of linguistic information could have an inﬂuence on its predictions: the more BERT stores readable\nlinguistic information into the representations it creates, the higher will be its capacity of predicting the\ncorrect L1. Moreover, we noticed that this is true also (and especially) using the pre-trained model. In\nother words, this result suggests that the evaluation of the linguistic knowledge encoded in a pre–trained\nversion of BERT on a speciﬁc input sequence could be an insightful indicator of its ability in analyzing\nthat sentence with respect to a downstream task.\nInterestingly, if we analyze the average length of correct and incorrect classiﬁed sentences, the correct\nones are much more longer than the others for all tasks (from 3 tokens more for SPA-ITA to 9 for\nTEL-ITA). This is quite expected for the NLI task, since a higher number of linguistic events possibly\noccurring in longer sentences are needed to classify the L1 of a sentence (Dell’Orletta et al., 2014). At\nthe same time, longer sentences make more complex the probing tasks because the output space is larger\nfor almost all them. This is an additional evidence that BERT’s linguistic knowledge is not strictly related\nto sentence complexity, but rather to the model’s ability to solve a speciﬁc downstream task. To conﬁrm\nthis hypothesis and verify whether such tendency does not only depend on sentence length, we trained\nanother LinearSVR that takes as input the sentence length and predict our probing tasks according to\ncorrectly or incorrectly classiﬁed NLI sentences. Table 4 reports the average Spearman’s correlation\ncoefﬁcients between gold and predict probing features for the two classes of sentences. Results showed\nthat, for all the considered language pairs, the LinearSVR achieved higher accuracy for the probing tasks\ncomputed with respect to the incorrectly NLI classiﬁed sentences. This is an additional evidence that\ndeeper linguistic knowledge is needed for BERT to correctly classify the L1 of a sentences.\nModel ARA CHI TUR SPA GER FRE JPN KOR TEL HIN\nCorrect 0.226 0.225 0.236 0.223 0.215 0.224 0.276 0.239 0.234 0.229\nIncorrect 0.248 0.251 0.249 0.235 0.244 0.239 0.290 0.255 0.258 0.257\nTable 4: Averageρ scores for sentences correctly and incorrectly classiﬁed using only sentence length as\ninput feature.\n7 Conclusion\nIn this paper we studied what kind of linguistic properties are stored in the internal representations\nlearned by BERT before and after a ﬁne-tuning process and how this implicit knowledge correlates with\nthe model predictions when it is trained on a speciﬁc downstream task. Using a suite of 68 probing\ntasks, we showed that the pre-trained version of BERT encodes a wide range of linguistic phenomena\nacross its 12 layers, but the order in which probing features are stored in the internal representations\ndoes not necessarily reﬂect the traditional division with respect to the linguistic annotation levels. We\nalso found that BERT tends to lose its precision in encoding our set of probing features after the ﬁne-\ntuning process, probably because it is storing more task–related information for solving NLI. Finally, we\nshowed that the implicit linguistic knowledge encoded by BERT positively affects its ability to solve the\ntested downstream tasks.\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. 2016. Fine-grained analysis of\nsentence embeddings using auxiliary prediction tasks. arXiv preprint arXiv:1608.04207.\nYonatan Belinkov and James Glass. 2019. Analysis methods in neural language processing: A survey. Transac-\ntions of the Association for Computational Linguistics, 7:49–72.\nYonatan Belinkov, Llu´ıs M`arquez, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. 2017. Evalu-\nating layers of representation in neural machine translation on part-of-speech and semantic tagging tasks. In\n754\nProceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers), pages 1–10.\nDaniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife Cahill, and Martin Chodorow. 2013. Toeﬂ11: A corpus\nof non-native english. ETS Research Report Series, 2013(2):i–15.\nTerra Blevins, Omer Levy, and Luke Zettlemoyer. 2018. Deep rnns encode soft hierarchical syntax. In Proceed-\nings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) ,\npages 14–19.\nDominique Brunato, Andrea Cimino, Felice Dell’Orletta, Giulia Venturi, and Simonetta Montemagni. 2020.\nProﬁling-ud: a tool for linguistic proﬁling of texts. In Proceedings of The 12th Language Resources and\nEvaluation Conference, pages 7147–7153, Marseille, France, May. European Language Resources Association.\nAndrea Cimino, Felice Dell’Orletta, Dominique Brunato, and Giulia Venturi. 2018. Sentences and documents in\nnative language identiﬁcation. In CLiC-it.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does BERT look at?\nan analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP, pages 276–286, Florence, Italy, August. Association for Computational\nLinguistics.\nAlexis Conneau, Germ ´an Kruszewski, Guillaume Lample, Lo ¨ıc Barrault, and Marco Baroni. 2018. What you\ncan cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties. In Proceedings\nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages\n2126–2136.\nFelice Dell’Orletta, Martijn Wieling, Giulia Venturi, Andrea Cimino, and Simonetta Montemagni. 2014. As-\nsessing the readability of sentences: Which corpora and features? In Proceedings of the Ninth Workshop on\nInnovative Use of NLP for Building Educational Applications , pages 163–173, Baltimore, Maryland, June.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), pages 4171–4186.\nYoav Goldberg. 2019. Assessing bert’s syntactic abilities. arXiv preprint arXiv:1901.05287.\nJohn Hewitt and Percy Liang. 2019. Designing and interpreting probes with control tasks. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pages 2733–2743.\nJohn Hewitt and Christopher D Manning. 2019. A structural probe for ﬁnding syntax in word representations.\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129–4138.\nSarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers) , pages 3543–3556, Minneapolis, Minnesota, June. Association for\nComputational Linguistics.\nGanesh Jawahar, Benoˆıt Sagot, Djam´e Seddah, Samuel Unicomb, Gerardo I ˜niguez, M´arton Karsai, Yannick L´eo,\nM´arton Karsai, Carlos Sarraute, ´Eric Fleury, et al. 2019. What does bert learn about the structure of language?\nIn 57th Annual Meeting of the Association for Computational Linguistics (ACL), Florence, Italy.\nAkos K ´ad´ar, Grzegorz Chrupała, and Afra Alishahi. 2017. Representation of linguistic form and function in\nrecurrent neural networks. Computational Linguistics, 43(4):761–780.\nAndrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015. Visualizing and understanding recurrent networks. arXiv\npreprint arXiv:1506.02078.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the dark secrets of BERT.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 4365–4374, Hong\nKong, China, November. Association for Computational Linguistics.\n755\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. 2016. Visualizing and understanding neural models in nlp.\nIn Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 681–691.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019. Open sesame: Getting inside BERT’s linguistic knowledge.\nIn Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,\npages 241–253, Florence, Italy, August. Association for Computational Linguistics.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019. Linguistic knowl-\nedge and transferability of contextual representations. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers) , pages 1073–1094, Minneapolis, Minnesota, June. Association for Computational\nLinguistics.\nShannon Lubetich and Kenji Sagae. 2014. Data-driven measurement of child language development with simple\nsyntactic templates. In Proceedings of COLING 2014, the 25th International Conference on Computational\nLinguistics: Technical Papers, pages 2151–2160.\nShervin Malmasi, Keelan Evanini, Aoife Cahill, Joel Tetreault, Robert Pugh, Christopher Hamill, Diane Napoli-\ntano, and Yao Qian. 2017. A report on the 2017 native language identiﬁcation shared task. In Proceedings of\nthe 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 62–75, Copenhagen,\nDenmark, September. Association for Computational Linguistics.\nRebecca Marvin and Tal Linzen. 2018. Targeted syntactic evaluation of language models. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing, pages 1192–1202.\nAlessio Miaschi and Felice Dell’Orletta. 2020. Contextual and non-contextual word embeddings: an in-depth\nlinguistic investigation. In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 110–\n119, Online, July. Association for Computational Linguistics.\nAlessio Miaschi, Sam Davidson, Dominique Brunato, Felice Dell’Orletta, Kenji Sagae, Claudia Helena Sanchez-\nGutierrez, and Giulia Venturi. 2020. Tracking the evolution of written language competence in l2 spanish\nlearners. In Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Appli-\ncations. Association for Computational Linguistics, July.\nJoakim Nivre, Marie-Catherine De Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajic, Christopher D Manning,\nRyan McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, et al. 2016. Universal dependencies v1: A\nmultilingual treebank collection. In Proceedings of the Tenth International Conference on Language Resources\nand Evaluation (LREC’16), pages 1659–1666.\nChristian S Perone, Roberto Silveira, and Thomas S Paula. 2018. Evaluation of sentence embeddings in down-\nstream and linguistic probing tasks. arXiv preprint arXiv:1806.06259.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 2227–2237.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding\nby generative pre-training.\nAlessandro Raganato and J ¨org Tiedemann. 2018. An analysis of encoder representations in transformer-based\nmachine translation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP. Association for Computational Linguistics.\nManuela Sanguinetti and Cristina Bosco. 2015. Parttut: The turin university parallel treebank. In Harmonization\nand Development of Resources and Tools for Italian Natural Language Processing within the PARLI Project ,\npages 51–69. Springer.\nNaomi Saphra and Adam Lopez. 2019. Understanding learning dynamics of language models with svcca. In\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3257–3267.\nNatalia Silveira, Timothy Dozat, Marie-Catherine De Marneffe, Samuel R Bowman, Miriam Connor, John Bauer,\nand Christopher D Manning. 2014. A gold standard dependency corpus for english. In LREC, pages 2897–\n2904.\n756\nGongbo Tang, Rico Sennrich, and Joakim Nivre. 2018. An analysis of attention mechanisms: The case of word\nsense disambiguation in neural machine translation. In Proceedings of the Third Conference on Machine Trans-\nlation: Research Papers, pages 26–35, Belgium, Brussels, October. Association for Computational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a. BERT rediscovers the classical NLP pipeline. InProceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593–4601, Florence, Italy,\nJuly. Association for Computational Linguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin\nVan Durme, Samuel R Bowman, Dipanjan Das, et al. 2019b. What do you learn from context? probing for\nsentence structure in contextualized word representations. arXiv preprint arXiv:1905.06316.\nHans van Halteren. 2004. Linguistic proﬁling for author recognition and veriﬁcation. In Proceedings of the\nAssociation for Computational Linguistics, pages 200–207.\nJesse Vig and Yonatan Belinkov. 2019. Analyzing the structure of attention in a transformer language model. In\nProceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP ,\npages 63–76, Florence, Italy, August. Association for Computational Linguistics.\nAlex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Hagen Blix, Yining Nie, Anna Alsop, Shikha Bordia, Haokun\nLiu, Alicia Parrish, et al. 2019. Investigating bert’s knowledge of language: Five analysis methods with npis.\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2870–2880.\nAmir Zeldes. 2017. The GUM corpus: Creating multilayer resources in the classroom. Language Resources and\nEvaluation, 51(3):581–612.\nKelly Zhang and Samuel Bowman. 2018. Language modeling teaches you more than translation does: Lessons\nlearned through auxiliary syntactic task analysis. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP, pages 359–361.",
  "topic": "Sentence",
  "concepts": [
    {
      "name": "Sentence",
      "score": 0.7426067590713501
    },
    {
      "name": "ENCODE",
      "score": 0.7304568290710449
    },
    {
      "name": "Computer science",
      "score": 0.6749985218048096
    },
    {
      "name": "Natural language processing",
      "score": 0.6693825721740723
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5814430713653564
    },
    {
      "name": "Linguistics",
      "score": 0.5316634178161621
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4882638454437256
    },
    {
      "name": "Profiling (computer programming)",
      "score": 0.4852728545665741
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4189555048942566
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I108290504",
      "name": "University of Pisa",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I197604219",
      "name": "National Academies of Sciences, Engineering, and Medicine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I169381384",
      "name": "University of Groningen",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I162290304",
      "name": "Scuola Superiore Sant'Anna",
      "country": "IT"
    }
  ]
}