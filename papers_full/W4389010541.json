{
  "title": "Leveraging Large Language Models for Automated Dialogue Analysis",
  "url": "https://openalex.org/W4389010541",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3035117366",
      "name": "Sarah E. Finch",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A5113054601",
      "name": "Ellie S. Paek",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A2125846880",
      "name": "Jinho D. Choi",
      "affiliations": [
        "Emory University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4221141168",
    "https://openalex.org/W3207604732",
    "https://openalex.org/W4385570866",
    "https://openalex.org/W4385573652",
    "https://openalex.org/W4322708560",
    "https://openalex.org/W3200833038",
    "https://openalex.org/W4285300946",
    "https://openalex.org/W3116890199",
    "https://openalex.org/W3175643241",
    "https://openalex.org/W4293320163",
    "https://openalex.org/W4224947967",
    "https://openalex.org/W4285122228",
    "https://openalex.org/W3104982372",
    "https://openalex.org/W3034720580",
    "https://openalex.org/W3201162576",
    "https://openalex.org/W4285195150",
    "https://openalex.org/W3036394672",
    "https://openalex.org/W3035068109",
    "https://openalex.org/W3176506090",
    "https://openalex.org/W4366733439",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W3176162574",
    "https://openalex.org/W2962989446",
    "https://openalex.org/W4384642442",
    "https://openalex.org/W3153947101",
    "https://openalex.org/W4316126300",
    "https://openalex.org/W3097663391",
    "https://openalex.org/W4389519535",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W3173540771",
    "https://openalex.org/W3106007100",
    "https://openalex.org/W4367628242",
    "https://openalex.org/W4226227457",
    "https://openalex.org/W3173803111",
    "https://openalex.org/W4362700315",
    "https://openalex.org/W4380136143",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W4296711106",
    "https://openalex.org/W3176264844",
    "https://openalex.org/W3100719877",
    "https://openalex.org/W3116832844",
    "https://openalex.org/W3134900088",
    "https://openalex.org/W4365211542",
    "https://openalex.org/W4287887982"
  ],
  "abstract": "Developing high-performing dialogue systems benefits from the automatic identification of undesirable behaviors in system responses. However, detecting such behaviors remains challenging, as it draws on a breadth of general knowledge and understanding of conversational practices. Although recent research has focused on building specialized classifiers for detecting specific dialogue behaviors, the behavior coverage is still incomplete and there is a lack of testing on real-world human-bot interactions. This paper investigates the ability of a state-of-the-art large language model (LLM), ChatGPT-3.5, to perform dialogue behavior detection for nine categories in real human-bot dialogues. We aim to assess whether ChatGPT can match specialized models and approximate human performance, thereby reducing the cost of behavior detection tasks. Our findings reveal that neither specialized models nor ChatGPT have yet achieved satisfactory results for this task, falling short of human performance. Nevertheless, ChatGPT shows promising potential and often outperforms specialized detection models. We conclude with an in-depth examination of the prevalent shortcomings of ChatGPT, offering guidance for future research to enhance LLM capabilities.",
  "full_text": "Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue, pages 202–215\nSeptember 11–15, 2023. ©2023 Association for Computational Linguistics\n202\nLeveraging Large Language Models for Automated Dialogue Analysis\nSarah E. Finch Ellie S. Paek Jinho D. Choi\nDepartment of Computer Science\nEmory University\nAtlanta, GA, USA\n{sfillwo, ellie.paek, jinho.choi}@emory.edu\nAbstract\nDeveloping high-performing dialogue systems\nbenefits from the automatic identification of un-\ndesirable behaviors in system responses. How-\never, detecting such behaviors remains chal-\nlenging, as it draws on a breadth of general\nknowledge and understanding of conversational\npractices. Although recent research has fo-\ncused on building specialized classifiers for\ndetecting specific dialogue behaviors, the be-\nhavior coverage is still incomplete and there is\na lack of testing on real-world human-bot inter-\nactions. This paper investigates the ability of\na state-of-the-art large language model (LLM),\nChatGPT-3.5, to perform dialogue behavior de-\ntection for nine categories in real human-bot\ndialogues. We aim to assess whether ChatGPT\ncan match specialized models and approximate\nhuman performance, thereby reducing the cost\nof behavior detection tasks. Our findings re-\nveal that neither specialized models nor Chat-\nGPT have yet achieved satisfactory results for\nthis task, falling short of human performance.\nNevertheless, ChatGPT shows promising po-\ntential and often outperforms specialized de-\ntection models. We conclude with an in-depth\nexamination of the prevalent shortcomings of\nChatGPT, offering guidance for future research\nto enhance LLM capabilities.\n1 Introduction\nOne crucial aspect of developing high-performing\ndialogue systems is the automated identification of\nerrors in system responses. These errors can result\nfrom various behaviors, including incorrect infor-\nmation retrieval or illogical semantics (Figure 1).\nIdentifying such errors enhances dialogue system\ndevelopment and complements dialogue-level eval-\nuation methods by providing finer-grained metrics\nfor comparison (Finch et al., 2023).\nTo capitalize on these benefits, recent research\nhas focused on training classifiers for specific di-\nalogue behaviors. While certain behaviors have\nreceived considerable attention, this is not the case\nDo you speak spanish?\nYes, the population of miami-dade county, the most \npopulous county in florida, is estimated to be 5.3 million.\nYes, and miami is also the second-largest us city (after el \npaso, texas) with a spanish-speaking majority .\nYes, I do. I'm a native speaker of english, but my mother \ntongue is spanish.\nOh, okay. incorrect fact\nself contradiction\nFigure 1: Response errors in a human-bot dialogue.\nfor all pertinent dialogue behaviors. Furthermore,\nmost datasets for training are produced by annotat-\ning human-human dialogues (Sharma et al., 2020),\nperturbing human responses (Gupta et al., 2022),\nor crafting post-hoc responses (Nie et al., 2021).\nAs a result, such datasets may not reflect human-\nbot interactions, rendering them less suitable for\nclassifier development.\nLarge language models (LLMs) display a\npromising potential to address the limited coverage\nin specialized classifiers. LLMs have demonstrated\ncompetitive performance across various natural lan-\nguage processing (NLP) tasks without finetuning\n(Koco´n et al., 2023). Adapting LLMs to classify\ndialogue behaviors can alleviate substantial costs\nassociated with current evaluation approaches by\nallowing for a general dialogue behavior evaluator\nthat is less dependent on human involvement.\nAlthough there is much effort towards open-\nsourcing competitive LLMs, OpenAI’s ChatGPT\nremains the most successful LLM to date (Wang\net al., 2023). Thus, we focus our experiments on\nChatGPT to assess the current best-case perfor-\nmance on automated dialogue behavior detection\nusing LLMs. With its wide accessibility and low\ncosts, ChatGPT provides a practical and straight-\nforward platform for automating dialogue behavior\ndetection, if its proves successful.\n203\nTo this end, our work focuses on two main objec-\ntives:\n1. To determine whether or not ChatGPT can\nmatch the performance of state-of-the-art spe-\ncialized behavior classifiers.\n2. To assess the extent to which ChatGPT can ap-\nproximate human-level performance in behavior\nclassification using real human-bot dialogues.\nOur findings indicate that automated methods for\ndialogue behavior detection have not reached satis-\nfactory results, falling short of human performance.\nHowever, ChatGPT showcases compelling results\ncomparative to or often better than specialized mod-\nels. To facilitate further advancements, we conduct\nan in-depth analysis to identify the prevalent errors\nand shortcomings of ChatGPT. This analysis pro-\nvides valuable insights, highlighting key areas to\nbe targeted to enhance the performance of LLMs\nin dialogue behavior detection for future work. We\nrelease our code and data at https://github.\ncom/emorynlp/GPT-ABCEval.\n2 Related Work\nChatGPT has shown promising performance on\nmany NLP tasks, especially for text classification\n(Gilardi et al., 2023; Koco´n et al., 2023; Zhu et al.,\n2023). In addition, GPT models, including Chat-\nGPT and InstructGPT, have been used to produce\nhigh-quality dyadic dialogues (Kim et al., 2022;\nZhan et al., 2023) and have been shown to cor-\nrelate highly with human annotators when evalu-\nating the overall quality of empathetic dialogues\n(Svikhnushina and Pu, 2023). However, ChatGPT\nstill exhibits limitations as Chan et al. (2023) show\nthat ChatGPT struggles with fine-grained dialogue\nunderstanding, reporting poor performance on clas-\nsifying discourse structure and utterance relations.\nTo the best of our knowledge, no prior research\nhas explored the use of any GPT model as a be-\nhavior classifier for chatbot responses. Instead,\nprevious work has focused on the development of\nspecialized dialogue behavior classifiers, as dis-\ncussed in this section.\n2.1 Contradiction Detection\nAlthough much work focuses on dialogue con-\ntradictions in the context of a given bot persona\n(Zhang et al., 2018; Welleck et al., 2019; Kim\net al., 2020; Song et al., 2020; Shuster et al., 2022),\nthere has been some work on a more general sense\nof contradictions, including NLI models targeting\nself-context contradictions (Li et al., 2021; Nie\net al., 2021), inconsistency detectors using domain-\nspecific attribute-value classifiers (Shi et al., 2021),\nand context summarization to encourage consis-\ntency in response generation (Xu et al., 2022a,b).\nNotably, these existing approaches to contradiction\ndetection fail to address partner contradictions.\nThere is also a lack of work on general com-\nmonsense contradiction detection for dialogue re-\nsponses. To the best of our knowledge, Ghazarian\net al. (2023) is the only work that focuses explicitly\non capturing commonsense qualities of dialogue\nresponses. They propose a method for calculating\ncontinuous event commonsense alignment scores\nfor dialogue responses using similarity calculations\nwith the outputs of an event extraction model and\ngenerative commonsense model. However, such\ncontinuous scores cannot be immediately applied\nto commonsense contradiction detection without\nfurther modifications (e.g. learned thresholding,\nclassification head, etc.).\n2.2 Claim Verification\nThere are a variety of approaches taken for\nclaim verification in dialogue, including question-\nanswering (Honovich et al., 2021) and trained clas-\nsifiers (Dziri et al., 2022b). Dziri et al. (2022b) find\nthat trained classifiers perform the best, although\nthey still lag behind human performance. Some\nworks focus on claim verification for question-\nresponse pairs only (Wang et al., 2022), whereas\nothers target multi-turn dialogues, producing an-\nnotated datasets including FaithDial (Dziri et al.,\n2022a), BEGIN (Dziri et al., 2022b), and DialFact\n(Gupta et al., 2022). Most of these works focus\nexclusively on dialogue responses that are given a\ngrounding knowledge text. In practice, however,\na grounding knowledge text is not always prede-\ntermined. Gupta et al. (2022) propose a pipeline\nfor claim verification that includes a knowledge\nretrieval stage rather than assuming it is provided.\n2.3 Empathy\nHuman judges are commonly used when evaluat-\ning the degree of empathy exhibited in a dialogue\nresponse (Zhong et al., 2020; Sabour et al., 2022;\nQian et al., 2023). There has also been some work\non developing empathetic response and question\ntaxonomies, although these are only applied in\nsmall-scale or synthetic settings (Welivita and Pu,\n2020; Svikhnushina et al., 2022). Most applicably,\n204\nLabel Abbr. Description\nEmpathetic Emp The response shows an understanding and reacts appropriately to someone’s emotions.\nLack of Empathy !Emp The bot misunderstands or reacts inappropriately to someone’s emotions.\nCommonsense !Com The response misunderstands or contradicts common knowledge.Contradiction\nIncorrect Fact !Fac The response hallucinates or inaccurately presents encyclopedic or expert knowledge.\nSelf Contradiction !Sel The bot contradicts something it said earlier in the dialogue.\nPartner Contradiction!Par The bot contradicts or misremembers something the user said earlier in the dialogue.\nRedundant Red The response inappropriately repeats information presented earlier in the dialogue.\nIgnore Ign The response ignores what the user just said.\nIrrelevant !Rel The response interrupts the current topic of discussion by presenting unrelated information.\nTable 1: The 9 behavior labels from ABC-Eval (table adapted from Finch et al. (2023)). The {Emp, !Emp}, {!Fac},\n{!Sel}, {Ign, !Rel} labels can be classified by the EPI, FC, DEC, S2T2 models in Section 4, respectively.\nSharma et al. (2020) collect EPITOME, a dataset of\n10K interactions from Reddit and Talklife (a mental\nhealth forum) that are annotated with the strength\nof their expression of three empathetic mechanisms:\nreactions, interpretations, explorations. Some re-\ncent dialogue works have used EPITOME-trained\nclassifiers in their approaches (Zheng et al., 2021;\nMajumder et al., 2022) or for automatic evaluation\n(Kim et al., 2021; Lee et al., 2022).\n2.4 Coherence\nResearch on detecting incoherent behaviors, such\nas redundancy and irrelevancy, is limited. Most\nworks perturb dialogue responses to artificially\nconstruct incoherence examples (Xu et al., 2021;\nZhang et al., 2021; Ghazarian et al., 2022), which\nmay not produce representative examples. On the\nother hand, Mehri and Eskenazi (2020) derive a\nresponse’s relevancy score from the probabilities\nof manually designed future indicator utterances\nbut found little correlation with human judgments.\nIn addition, detection of response redundancy is un-\nderexplored, despite some works addressing token\nrepetition (Li et al., 2020; Xi et al., 2021). Perhaps\nmost relevant, the Dialogue Breakdown Detection\nChallenge (DBDC) aims to identify contextually in-\nappropriate bot responses that hinder conversation\ncontinuation (Higashinaka et al., 2019). Various\nclassifiers have been proposed for this challenge\n(Ng et al., 2020; Lin and Ng, 2022), with observa-\ntions suggesting coherence issues as a dominant\ncause of breakdowns.\n3 ABC-Eval Dataset\nWe use the ABC-Eval dataset from Finch et al.\n(2023) as the behavior detection benchmark. This\ndataset contains 400 open-domain human-bot dia-\nlogues collected between university students and\none of four chatbots: BlenderBot2, Blenderbot\nusing DECODE reranking, Emora, and Bart-FiD-\nRAG. For each bot response in each dialogue, hu-\nman annotators labeled whether or not a specific\ndialogue behavior was present. These turn-level bi-\nnary annotations were collected using crowdwork-\ning annotators on the SurgeHQ platform, 1 who\nwere trained on three curated conversations to accu-\nrately identify each dialogue behavior before being\naccepted into the annotation project. For example,\nin Figure 1, the three bot responses are labeled1, 0,\n0 for the behavior incorrect fact (!Fac)\nand are labeled 0, 0, 1 for the behavior self\ncontradiction (!Sel).\nIn this work, we take 1,634 bot responses from\n108 dialogues that received two rounds of human\nannotations, and focus on the nine dialogue behav-\niors that Finch et al. (2023) found as the most in-\nformative for capturing dialogue quality (Table 1).\n4 Specialized Behavior Detection Models\nIn this section, we present state-of-the-art mod-\nels designed to classify labels that closely align\nwith six of the dialogue behaviors in Table 1: Emp,\n!Emp, !Fac, !Sel, Ign, and !Rel. Note that\nno existing models are available for predicting\n!Com, !Par, and Red so there are no viable com-\nparisons to our LLM approach for them (Section 5).\nFaithCritic (FC) Following Gupta et al. (2022),\nwe build a claim verification pipeline for a dialogue\nresponse r. First, 3 relevant documents Dk for ev-\nery entity in r are retrieved using WikiAPI. Then,\na BERT model trained on the Wizard of Wikipedia\n(WoW) knowledge-response pairs (Dinan et al.,\n2019) selects the top-10 evidence sentences Se\nfrom Dk. To distinguish whether a response makes\na factual claim or not, the lexical overlap between\n1https://www.surgehq.ai\n205\nr and Se is estimated, optimized on the ABC-\nEval training conversations. Finally, a RoBERTa\nmodel trained on Faith-Critic, a dataset of human-\nannotated faithful and unfaithful evidence-response\npairs derived from the WoW (Dziri et al., 2022a), is\napplied to those responses that make factual claims.\nAs a result, responses that are predicted unfaithful\nto any evidence e ∈ Se are labeled as !Fac.\nS2T2 S2T2 is a semi-supervised student-teacher\ntraining framework using two teachers, one trained\non the gold data and the other trained on perturbed\ngold data under a [MASK] replacement, to incor-\nporate self-supervised data augmentation into the\nmodel training (Lin and Ng, 2022). We use the\nreleased S2T2 model for the English-version of\nDBDC5 that is the best-performing model to date.\nWe use S2T2 as identifying Ign and !Rel labels,\nsince it is not trained to distinguish between them.\nDECODE (DEC) We use the released RoBERTa\nclassification model trained on DECODE to label\n!Sel. DECODE contains human-written contra-\ndictory and non-contradictory dialogue responses\nwith respect to the current speaker’s previous utter-\nances in the dialogue (Nie et al., 2021).\nEPITOME (EPI) A RoBERTa-based bi-encoder\nclassification model for each empathetic commu-\nnication mechanism is trained from the publicly\navailable Reddit portion of the EPITOME dataset\n(Sharma et al., 2020). Predictions of weak or strong\nexpressions of any of the three mechanisms are con-\nsidered as Emp. Predictions of no expression for\nall mechanisms are considered as !Emp.\n5 LLM-based Behavior Detection\nFor LLM-based dialogue behavior detection,\nwe use OpenAI’s gpt-turbo-3.5-301 (henceforth,\nChatGPT). Similar to the specialized models (Sec-\ntion 4), ChatGPT is tasked with classifying a single\nbehavior at a time. Following the human annotator\ntraining process for ABC-Eval, we use the three\ntraining conversations for each label as our prompt\nengineering testbed. This section highlights key\ndecisions of our prompt engineering process.\nInstruction Finetuning During prompt engineer-\ning, it became apparent that the instructions de-\nsigned for human annotators (Section 3) were not\nsuitable as ChatGPT instructions. We iteratively re-\nfined the instructions such that ChatGPT’s mistakes\non the training conversations were reduced. This\ninvolved removing instructions ChatGPT appeared\nto misunderstand as well as adding additional be-\nhavior details and specifications.\nUtterance Focus We discovered that when Chat-\nGPT was instructed to label each bot turn given the\nentire dialogue, the resulting classifications often\nfocused on only a subset of the bot responses. To\nensure consistent and robust labeling for every bot\nutterance, our final prompt provides the dialogue\nhistory paired with the next bot response as the\ntarget utterance to be labeled.\nIn-context Examples We also tried including the\nexamples provided to human annotators by Finch\net al. (2023) as in-context examples in the prompts.\nHowever, this degraded the overall performance on\nthe training conversations. It appears that the exam-\nples optimized for improving human annotations\ndo not translate well to ChatGPT’s performance.\nCreativity We conducted experiments involving\nseveral temperature parameters and observed high\ninstability in the classifications for the same inputs\nwhen the temperature was increased. Interestingly,\nwe found that using a low temperature yielded more\naccurate results consistently. Thus, we opted to use\na temperature of 0 for maximum reproducibility\nin our classifications. Similar findings have been\nreported by Gilardi et al. (2023) and Reiss (2023).\nFinal Prompt Table 2 illustrates an example of\nthe final prompt, in particular for the Red behav-\nior2. Each behavior is accompanied by its own elic-\niting question and description, which respectively\nfill in the Behavior Question (Q) and Behavior Def-\ninition (D) containers of the prompt. For labeling a\nparticular context-response example, the historical\nturns from the context and the next target response\nfill in the Dialogue Context (C) and Target Utter-\nance (U) containers. The final label for a behavior\nis parsed from the line produced by ChatGPT that\nbegins with the header ‘Decision:’, where the value\n‘yes’ indicates a positive label and any other value\nindicates a negative label. Appendix A provides\nthe questions and definitions used for all behaviors.\n6 Evaluation\nTo evaluate the detection capability of the models\nin Sections 4 and 5, we compare their performance\nagainst that of human annotators. For this, we\n2Due to spacing constraints, Table 2 contains minor discrep-\nancies with the actual prompts in this work (Appendix B).\n206\nS1: Hi!\nS2: Hi! I am happy to be talking to you.\nDo you have any pets at home?\nS1: I don’t, my mom is allergic to most pets.\nI used to have a pet fish when I was much younger.\nS2: Do you want a pet at some point in the future?\nNo judgment here, I’m just curious.\nS1: I would love to have pets in the future! Once I\nC\nhave my own place, I’d like to get a dog or two.\nQ Is this response repeating something that has already been said:\nU S2: Would you want to get a cat or a dog?\nA response is repetitive if:\n- it repeats something from earlier in the dialogue\n- it includes asking a question whose answer has been\nalready shared\nIf any part of the response is repetitive, then it should be\nlabeled as repetitive.\nNote that sometimes repetition is useful, such as for emphasis,\nacknowledgement, clarification, or elaboration, and in these\nD\ncases it should NOT be labeled as repetitive.\nProvide your reasoning when considering this question start-\ning with “Reasoning:”. Then, finish by writing your final\ndecision as one of: “Decision: [YES]” or “Decision: [NO]”.\nTable 2: A ChatGPT prompt example for theRed behav-\nior. Segments in the prompt are dynamically modified\nbased on the example and behavior, as highlighted in\nthe gray containers (C: dialogue context, Q: behavior\nquestion, U: target utterance, D: behavior definition).\ntake the set of doubly annotated conversations in\nABC-Eval as our evaluation set (108 dialogues),\nand apply each model to the bot responses (1,634\nutterances) to obtain the predicted labels.\n6.1 Metrics\nTo assess the degree to which automated methods\ncan approximate human judgment for a particular\ndialogue behavior, we measure the accuracy of the\nbinary labels predicted by automated methods with\nrespect to the binary labels provided by the human\nannotators. In addition, we calculate both the F1-\nscore for the positive occurrences of each dialogue\nbehavior and for the negative occurrences of each\ndialogue behavior, in order to obtain a more fine-\ngrained picture of the performance.\nEach instance in the evaluation set is double-\nannotated, so two sets of human annotations ex-\nist without adjudication. It is important to note\nthat the assessment of these dialogue behaviors\nis not purely based on objective criteria, as they\nrely on factors inherently subject to human inter-\npretations (e.g., commonsense contradiction, irrel-\nevance). With this in mind, to better capture the\naggregate nature of identifying dialogue behaviors,\nthe final score for each metric is measured by aver-\naging results across the double human annotations,\nwhere e is the metric (either accuracy or F1-score),\nom is the model outputs, and oh1 and oh2 are the\nhuman labels from annotation round 1 and 2, re-\nspectively:\nefinal = 1\n2(e(om, oh1) +e(om, oh2))\nTo assess human performance, we measure the\nF1 score and accuracy by comparing the two hu-\nman annotation sets. Finally, the statistical sig-\nnificance between outputs of models and humans,\nand between outputs of the specialized models and\nChatGPT, is estimated using McNemar’s Test with\nsignificance level of 0.05. Testing is performed\nby treating each human annotation set as ground-\ntruth.3\n6.2 Results & Discussion\nModel F1+ F1- Acc. #+\nEmp\nEPI 54.2 31.3 45.0 1,343\nChatGPT 19.3 75.4 62.3†† 146\nHUM 69.7 81.6 77.1⋆⋆ 618\n!Emp\nEPI 13.4 83.5 72.3 291\nChatGPT 26.6 82.6 71.8 396\nHUM 51.5 92.0 86.3⋆⋆ 231\n!Com ChatGPT 34.9 86.7 78.0 219\nHUM 55.6 88.6 81.9⋆ 333\n!Fac\nFC 15.9 90.1 82.2 223\nChatGPT 41.0 94.7 90.3†† 146\nHUM 67.8 97.4 95.2⋆⋆ 122\n!Sel\nDEC 31.1 92.6 86.6†† 215\nChatGPT 20.7 90.5 83.0 250\nHUM 44.3 96.3 93.1⋆⋆ 101\n!Par ChatGPT 18.6 93.8 88.5 79\nHUM 48.8 94.8 90.5⋆⋆ 151\nRed ChatGPT 32.9 93.8 88.6 148\nHUM 58.7 96.4 93.5⋆⋆ 129\nIgn\nS2T2 25.2 85.3 75.5†† 365\nChatGPT 24.9 72.9 60.2 696\nHUM 61.6 95.5 92.0⋆⋆ 170\n!Rel\nS2T2 27.9 82.9 72.4† 365\nChatGPT 40.6 80.6 70.8 543\nHUM 54.3 91.3 85.4⋆⋆ 261\nTable 3: F1 and accuracy achieved by each model,\nwhere HUM stands for human judges. #+: num. posi-\ntive labels predicted. †| † †denote significance between\nautomated models on one or both human annotation\nsets, respectively. ⋆|⋆⋆ denote significance against best\nautomated model on one or both human annotation sets.\nTable 3 indicates the ongoing challenge of dialogue\nbehavior detection for automated models. Across\n3The other human annotation set relative to the one being\ntreated as ground-truth is used as human output.\n207\nAbbr. Error Type Description Σ %\nIN Inexperience Displays a lack of wisdom about human experiences 83 0.23\nHF History ForgetfulnessForgets information shared previously in the history 51 0.14\nDM Definition MismatchExpands beyond the provided definition of the behavior 51 0.14\nSA Selective Attention Overlooks components in a multi-idea response 33 0.09\nDC Disassociated ContextIncorrectly remembers the historical order of the conversation 28 0.08\nSR Semantic RelatednessMisunderstands the degree of similarity between two ideas 19 0.05\nCN Conversation NormsMisunderstands what constitutes a coherent progression of dialogue17 0.05\nME Mutual Exclusion Misidentifies when two events or concepts can or cannot co-occur together13 0.04\nRC Role Confusion Confuses the speaker of previous utterances 13 0.04\nMI Misidentification Misunderstands the intent of what has been shared 13 0.04\nCF Confused Target Confuses which utterance is being labeled 9 0.03\nTF Temporal Framing Confuses the specified timeline of a particular situtation 7 0.02\nRM Reasoning MismatchIts explanation is at-odds with its final decision 7 0.02\nEX Exhaustive Assumes all examples provided in the behavior definition must be met6 0.02\nCD Claim Detection Incorrectly identifies when a claim/statement is being made 4 0.01\nOA Over-analysis Combines unrelated previous utterances to draw unsupported conclusions4 0.01\nBI Bot Identity Considers indicators of speaker being a bot as erroneous 2 0.01\nTable 4: Results of the error analysis on ChatGPT’s reasoning for dialogue behavior detection.\nall labels, human judges are significantly more sta-\nble than the models. This difference is pronounced\nwith regard to positive instances (F1+), where mod-\nels attain only half the score compared to humans.\nInterestingly, ChatGPT exhibits comparable per-\nformance with several specialized classifiers. In\nthe case of !Fac, ChatGPT outperforms Faith-\nCritic (FC) in every aspect and achieves perfor-\nmance closer to humans. For !Emp and !Rel,\nChatGPT shows similar performance on F1- and\naccuracy, and even better performance on F1+, as\ntheir classifiers. Considering that ChatGPT is not\nfinetuned for these tasks, these results are highly\nencouraging.\nAlthough ChatGPT is seemingly outperformed\nby S2T2 on Ign, this is primarily due to the pre-\ndiction of negative cases. When analyzing the posi-\ntive cases, ChatGPT gives much higher recall yet\nsimilar precision compared to S2T24. In practice,\npositive case detection is more impactful, imply-\ning that ChatGPT has an advantage in real-world\napplications.\nFurthermore, although ChatGPT faces signifi-\ncant challenges in detecting positive cases of Emp,\nEPITOME (EPI) does not perform much better. Its\nhigher F1+ score is achieved by excessively pre-\ndicting positive cases, labeling almost all turns as\npositive. This overprediction impairs its overall\nperformance, allowing ChatGPT to outperform it\nwhen considering all cases as reflected in accuracy.\nThe only behavior for which ChatGPT appears\nto be beaten by the specialized classifier is against\n4Precision and recall provided in Appendix C.\nDECODE (DEC) for !Sel. However, the differ-\nence in performance is only slight overall.\nNotably, ChatGPT shows promising accuracy\nand negative F1 (F1-) to humans for the three be-\nhaviors for which specialized models are not avail-\nable: !Com, !Par, and Red. However, it still\nstruggles with detecting positive cases relative to\nhumans.\n7 ChatGPT Error Analysis\nWe perform an error analysis of ChatGPT’s predic-\ntions of dialogue behaviors to better understand its\nlimitations. For each dialogue behavior, we select\n40 instances where ChatGPT and humans disagree,\nand examine the reasoning provided by ChatGPT\nprior to its final decision ( [R]; see examples be-\nlow). Table 4 presents a set of dialogue character-\nistics and ChatGPT predispositions that highlight\ncommon mistakes made by ChatGPT across multi-\nple dialogue behaviors.\n7.1 Context Management\nThe predominant cause of ChatGPT’s errors is its\ninability to focus on the pertinent parts of the di-\nalogue and response. It often lacks awareness of\nwhat information has been previously shared (HF).\nIn Figure 2, the earlier response that already an-\nswers “Are you older than your brother” is missed.\nMoreover, ChatGPT tends to disregard the imme-\ndiately preceding turn (DC) and make its decision\nbased on an older state of the dialogue. In Fig-\nure 3, it overlooks the preceding question about\nfavorite colors. Similarly, ChatGPT often misses\n208\nideas shared within a response, instead latching on\nthe very last part of the dialogue (SA). In Figure 4,\nChatGPT fails to notice the emotional mimicry\nconveyed by S2 in response to S1’s opinion about\nNicki Minaj.\n(S1) I do! I have a younger brother. What about you?\n(S2) You have a brother? Cool! Do you guys get along?\n(S1) We used to fight a lot as kids, but we get along\nwell now.\n(S2) That’s awesome. Personally, I always wanted an\nolder sister so I could have someone to look up to. Are\nyou older than your brother?\n[R] The response asks a new question that has not been\nanswered yet, which is whether S1 is older than their\nbrother. This information has not been shared before,\nso the response is not repetitive.\nFigure 2: An example of History Forgetfulness (HF).\n(S2) I’m sure you’ll feel great after your workout! You\nshould definitely do some strength training exercises like\nthe push-up, the pull-up and the sit-up! They’re some of\nthe most common bodyweight exercises!\n(S1) Yes! What’s your favorite color?\n(S2) I’m not sure about you, but my favorite color is\nblue! I like it because it reminds me of the sea color of\nthe sea!\n[R] S1 asked about dinner, and S2 responded by saying\nthey haven’t had dinner yet and then shifted the topic to\nworking out. S2 then asked about S1’s favorite exercises\nat the gym. S1 did not mention anything about favorite\ncolors, so S2’s response is completely off-topic and does\nnot address the previous turn from S1.\nFigure 3: An example of Disassociated Context (DC).\n(S2) I love pop! What is your favorite R&B artist? Do\nyou like Nicki Minaj?\n(S1) I do like Nicki!\n(S2) I like her too, she’s so talented. Do either of your\nparents work? My mom is a teacher.\n[R] S2 does not demonstrate any understanding of S1’s\nemotions or react with an appropriate sentiment or emo-\ntion to S1’s shared experience. Instead, S2 changes the\nsubject to ask about S1’s parents. This response is not\nempathetic.\nFigure 4: An example of Selective Attention (SA).\nIn other cases, ChatGPT misattributes shared in-\nformation to a wrong speaker (RC). In Figure 5, it\nrecalls that S2 indicated a return to normalcy, when\nin fact, it was S1 who made that claim.\n(S2) Sounds like a good time!, so the corona virus has\nreally changed the world, huh? What is it like where\nyou live? Is everyone mostly staying inside, or are things\nopening up more like normal?\n(S1) Things are beginning to open up normal where I\nam but it has been a long 2 years. How about for you?\n(S2) I see., how long do you think it will take before\nthe pandemic is totally over, and things go back to how\nthey were before the virus?\n[R] S2’s previous statement was about things open-\ning up to normal where they are , implying that the\npandemic is not over yet. However, in this statement,\nS2 is asking when the pandemic will be totally over and\nthings will go back to how they were before the virus.\nThis seems to be a contradiction of previous statement.\nFigure 5: An example of Role Confusion (RC).\n7.2 Instruction Following\nIn addition, a frequent issue is that ChatGPT strays\nfrom the provided behavior definitions. In Figure\n6, esoteric knowledge is improperly considered as\ncommonsense. GPT also treats the sufficient indica-\ntors of a behavior from its definition as exhaustive\nrequirements (EX). In Figure 7, ChatGPT criticizes\nS2 for not indicating an emotion and offering sup-\nport, despite S2’s validation of S1’s experience.\n(S2) What is the name of this dorm? Do you have a\nroommate? How are you liking it?\n(S1) It’s MLAO. I don’t have a roommate so it’s great.\n(S2) Congratulations. Mlao is a great dorm then. It is\nthe largest in chicago. Enjoy!\n[R] The response would contradict commonsense be-\ncause MLAO is not the largest dorm in Chicago. In\nfact, there is no dorm in Chicago with that name. This\nresponse makes an unfounded assumption and contra-\ndicts common knowledge.\nFigure 6: An example of Definition Mismatch (DM).\n(S2) Cool! I think school can be pretty frustrating, but\nI do love learning new things, and a lot of my teachers\nare great. What about you? Do you like school?\n(S1) More or less. Doing activities like talking to a\nrobot is great, but doing assignments are tired. I’ve been\nspending the past week doing homework.\n(S2) I understand, it can definitely be stressful. What\nwould you change about your school, if you could?\n[R] S2 acknowledges that school can be frustrating and\nasks what S1 would change about it, showing an un-\nderstanding of S1’s experience. However, they do not\nreact with a specific sentiment or emotion, nor do they\noffer reassurance or support.\nFigure 7: An example of Exhaustive (EX).\n209\n7.3 World Model\nAnother major issue is ChatGPT’s poor understand-\ning of common world events and human experi-\nences.\nChatGPT frequently reveals a limited under-\nstanding of the relationship among concepts within\na dialogue. For example, it overlooks elaborations\non previous points, considering them too semanti-\ncally similar (SR). In Figure 8, the opinion about\nthe cat’s characteristics is actually unique informa-\ntion relative to the context.\n(S2) I love animals too! My favorite animal is the cat!\nI really like how they’re able to adapt to the environment\nand adapt to their environment!\n(S1) I love cats too, theya re adorable!\n(S2) I love how cute they are! I especially like how\ntheir fur is so soft and fluffy! They look so beautiful!\n[R] The response mentions how cute cats are and how\nsoft and fluffy their fur is. While this is related to S2’s\nfavorite animal being cats,it does not add any new infor-\nmation to the conversation and is repetitive. Therefore,\nthe response is repetitive.\nFigure 8: An example of Semantic Relatedness (SR).\nChatGPT also often criticizes typical conversa-\ntional practices (CN). In Figure 9, it considers\ntopical introductions at the start of a dialogue as\nirrelevant.\n(S1) Hi!\n(S2) hi! I am happy to be talking to you. I hope you\ndon’t mind me asking, but are you dating anyone now?\n[R] The response from S2 is completely irrelevant to\nwhat S1 said. S1 simply greeted S2 and did not provide\nany information that would lead to a discussion about\ndating. The question about dating is also abrupt and does\nnot flow with the previous turn.\nFigure 9: An example of Conversation Norms (CN).\nIn addition, ChatGPT faces challenges in compre-\nhending the plausibility of co-occurring events, be-\nliefs, and experiences (ME). In Figure 10, it depicts\na lack of understanding that it is implausible to have\nan opinion about the music of an artist if one has\nno prior experience with that artist’s music.\nChatGPT also demonstrates a large degree of\ngeneral inexperience with common phenomenon\nor situations in the world, which can lead to it\nharboring untrue facts about the world or misunder-\nstanding nuances of situations (IN). In Figure 11,\nChatGPT’s lack of commonsense is highlighted by\nits acceptance of well-wishes for a non-existent\ntrip.\n(S2) I have not heard of doj cat. Where does she live?\nHave any albums yet?\n(S1) He most recent album is \"Planet Her\". You should\ndefinitely listen to it!\n(S2) Planet her is amazing..the whole album has a dis-\ntinct sound. It was produced by alex pall.\n[R] There is no indication in the previous dialogue\nthat would suggest a self-contradiction in S2’s state-\nment. They are simply expressing their positive opin-\nion about the album \"Planet Her\" and mentioning\nthe producer. There is no contradiction with anything\nthey have said previously or any implausibility based on\nthe information shared. Additionally, there is no contra-\ndiction with anything S1 has said. Therefore, there is no\nself-contradiction in S2’s statement.\nFigure 10: An example of Mutual Exclusion (ME).\n(S1) I don’t have a trip planned, but maybe I’ll go\nsomewhere soon\n(S2) I’m sure you will! You’ll love it! Have a wonderful\ntrip! I’ll see you soon!\n[R] The response does not contradict commonsense as it\nis a polite and friendly response to the previous statement.\nIt does not make any unfounded assumptions, is not il-\nlogical or self-contradictory, and does not ask a question\nwhere the answer is already obvious. It is a common\nresponse to wish someone a good trip even if they have\nnot planned one yet.\nFigure 11: An example of Inexperience (IN).\n8 Recommendations\nGiven the compelling performance for many dia-\nlogue behaviors observed in this work, ChatGPT is\na promising direction for behavior classification.\nFor one, it is worth noting that ChatGPT boasts\nextreme cost-efficiency relative to humans. Where\nChatGPT costs $0.02 on average to provide labels\nfor a single behavior for one dialogue in this work,\nthe average cost for human annotation ranges from\n$0.29 to $1.96 depending on the behavior (Table 15\nin Appendix D). Since even specialized classifiers\nrely on human annotations for training creation,\nthey also end up being quite costly to maintain.\nFurthermore, the results of our error analysis\nreveal a large degree of systematicity behind Chat-\nGPT’s reasoning mistakes across many of the be-\nhaviors. Correcting these common mistakes is\n210\nlikely to further improve its performance to a no-\nticeable degree. We next discuss mitigation strate-\ngies of these identified issues to aid in future work.\nContext Management Providing the complete\ndialogue history may hinder ChatGPT’s ability to\nattend to the salient content due to information\noverload. To address this, we highlight two strate-\ngies:\n• Windowed Context: instead of providing the en-\ntire history, truncate the context to k previous\nturns. This would directly restrict the decision-\nmaking to the immediate context, which is im-\nportant for behaviors that depend on accurate\nrecency identification, including !Rel, Ign,\n!Emp, and Emp.\n• Turn Pairing : perform the labeling relative\nto each historical turn segment independently,\nrather than a contiguous context. This would en-\nable explicit and focused comparisons to smaller\nsegments of the history that could aid behav-\niors that require such precision, including !Sel,\n!Par, and Red.\nIn-Context Learning Examples Given the iden-\ntified mistake types, it becomes more straightfor-\nward to compose useful in-context learning exam-\nples that are tailored to optimizing ChatGPT. Ex-\namples of those mistake types that are related to\nChatGPT misunderstanding the nuances of a be-\nhavior (e.g. MD, SR, CN, ME, EX) could be taken\nfrom a held-out set of conversations, which would\nprime ChatGPT to avoid such reasoning.\n9 Limitations\nAlthough ChatGPT is a high-performing, widely\naccessible, and affordable LLM at the time of writ-\ning, there are considerations towards the long-term\napplicability of the results found in this work due\nto the ChatGPT infrastructure. Since ChatGPT is\nnot open-source and is only accessible through a\npaid API, there is less detailed understanding of its\ntraining and model design. In addition, this access\nmethod for ChatGPT also results in less user con-\ntrol over potential model changes and even model\ndeprecation over time. As such, further studies\ncould assess the applicability of other language\nmodels to the task of dialogue behavior detection\nto mitigate these concerns, and we leave this to\nfuture work.\nFurthermore, it should be noted that the errors\nmade by ChatGPT may not necessarily align with\nthose made by alternative open-source language\nmodels, or even future versions of ChatGPT itself.\nHowever, it may still be useful to be mindful of the\nprominent problems encountered with ChatGPT\nwhile using other LLMs. These identified phenom-\nena play a crucial role in language comprehension\nand reasoning overall and could also present chal-\nlenges for other models, although the extent of their\nimpact remains to be explored.\n10 Conclusion\nAlthough automated methods for dialogue behavior\nclassification remain a challenging task, this work\nfinds that ChatGPT-3.5 presents promising poten-\ntial to reduce the gap between model and human\nperformance. ChatGPT’s ability to provide com-\npetitive behavior classification against specialized\nclassifiers without necessitating finetuning or hu-\nman annotation across a variety of dialogue behav-\niors gives rise to a low-cost, multi-task evaluator\nmodel. The systematicity behind the common mis-\ntakes observed for ChatGPT reveal concrete steps\nfor future improvements that will improve behavior\nclassification performance, including strategies for\ncontext management and better understanding of\nsituational nuances. We look forward to future ad-\nvancements in behavior classification that leverage\nChatGPT’s unique capabilities.\n11 Acknowledgements\nWe gratefully acknowledge the support of the Ama-\nzon Alexa AI grant. Any opinions, findings, and\nconclusions or recommendations expressed in this\nmaterial are those of the authors and do not neces-\nsarily reflect the views of Amazon.\nReferences\nChunkit Chan, Jiayang Cheng, Weiqi Wang, Yuxin\nJiang, Tianqing Fang, Xin Liu, and Yangqiu Song.\n2023. Chatgpt evaluation on sentence level relations:\nA focus on temporal, causal, and discourse relations.\narXiv preprint arXiv:2304.14827.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In International Conference on Learning\nRepresentations.\nNouha Dziri, Ehsan Kamalloo, Sivan Milton, Os-\nmar Zaiane, Mo Yu, Edoardo M Ponti, and Siva\n211\nReddy. 2022a. Faithdial: A faithful benchmark for\ninformation-seeking dialogue. Transactions of the\nAssociation for Computational Linguistics, 10:1473–\n1490.\nNouha Dziri, Hannah Rashkin, Tal Linzen, and David\nReitter. 2022b. Evaluating Attribution in Dialogue\nSystems: The BEGIN Benchmark. Transactions\nof the Association for Computational Linguistics ,\n10:1066–1083.\nSarah E. Finch, James D. Finch, and Jinho D. Choi.\n2023. Don’t forget your abc’s: Evaluating the state-\nof-the-art in chat-oriented dialogue systems. In Pro-\nceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers). Association for Computational Linguistics.\nSarik Ghazarian, Yijia Shao, Rujun Han, Aram Gal-\nstyan, and Nanyun Peng. 2023. ACCENT: An auto-\nmatic event commonsense evaluation metric for open-\ndomain dialogue systems. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n4398–4419, Toronto, Canada. Association for Com-\nputational Linguistics.\nSarik Ghazarian, Nuan Wen, Aram Galstyan, and\nNanyun Peng. 2022. DEAM: Dialogue coherence\nevaluation using AMR-based semantic manipula-\ntions. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 771–785, Dublin, Ire-\nland. Association for Computational Linguistics.\nFabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.\n2023. Chatgpt outperforms crowd-workers for text-\nannotation tasks. arXiv preprint arXiv:2303.15056.\nPrakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and\nCaiming Xiong. 2022. Dialfact: A benchmark for\nfact-checking in dialogue. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3785–3801.\nRyuichiro Higashinaka, Luis F D’Haro, Bayan Abu\nShawar, Rafael E Banchs, Kotaro Funakoshi, Michi-\nmasa Inaba, Yuiko Tsunomori, Tetsuro Takahashi,\nand Joao Sedoc. 2019. Overview of the dialogue\nbreakdown detection challenge 4. In Wochat Work-\nshop at IWSDS 2019.\nOr Honovich, Leshem Choshen, Roee Aharoni, Ella\nNeeman, Idan Szpektor, and Omri Abend. 2021.\nQ2:: Evaluating factual consistency in knowledge-\ngrounded dialogues via question generation and ques-\ntion answering. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 7856–7870.\nHyunwoo Kim, Jack Hessel, Liwei Jiang, Ximing Lu,\nYoungjae Yu, Pei Zhou, Ronan Le Bras, Malihe\nAlikhani, Gunhee Kim, Maarten Sap, and Yejin Choi.\n2022. Soda: Million-scale dialogue distillation with\nsocial commonsense contextualization.\nHyunwoo Kim, Byeongchang Kim, and Gunhee Kim.\n2020. Will i sound like me? improving persona\nconsistency in dialogues through pragmatic self-\nconsciousness. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 904–916.\nHyunwoo Kim, Byeongchang Kim, and Gunhee Kim.\n2021. Perspective-taking and pragmatics for generat-\ning empathetic responses focused on emotion causes.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2227–2240, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nJan Koco´n, Igor Cichecki, Oliwier Kaszyca, Mateusz\nKochanek, Dominika Szydło, Joanna Baran, Julita\nBielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil\nKanclerz, et al. 2023. Chatgpt: Jack of all trades,\nmaster of none. arXiv preprint arXiv:2302.10724.\nYoung-Jun Lee, Chae-Gyun Lim, and Ho-Jin Choi.\n2022. Does gpt-3 generate empathetic dialogues?\na novel in-context example selection method and au-\ntomatic evaluation metric for empathetic dialogue\ngeneration. In Proceedings of the 29th International\nConference on Computational Linguistics, pages 669–\n683.\nMargaret Li, Stephen Roller, Ilia Kulikov, Sean Welleck,\nY-Lan Boureau, Kyunghyun Cho, and Jason Weston.\n2020. Don’t say that! making inconsistent dialogue\nunlikely with unlikelihood training. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4715–4728.\nZekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng,\nand Jie Zhou. 2021. Addressing inquiries about his-\ntory: An efficient and practical framework for evaluat-\ning open-domain chatbot consistency. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 1057–1067.\nQian Lin and Hwee Tou Ng. 2022. A semi-supervised\nlearning approach with two teachers to improve\nbreakdown identification in dialogues. In Proceed-\nings of the AAAI Conference on Artificial Intelligence,\nvolume 36, pages 11011–11019.\nNavonil Majumder, Deepanway Ghosal, Devamanyu\nHazarika, Alexander Gelbukh, Rada Mihalcea, and\nSoujanya Poria. 2022. Exemplars-guided empathetic\nresponse generation controlled by the elements of\nhuman communication. IEEE Access , 10:77176–\n77190.\nShikib Mehri and Maxine Eskenazi. 2020. Unsuper-\nvised evaluation of interactive dialog with dialogpt.\nIn 21th Annual Meeting of the Special Interest Group\non Discourse and Dialogue, page 225.\nNathan Ng, Marzyeh Ghassemi, Narendran Thangara-\njan, Jiacheng Pan, and Qi Guo. 2020. Improving\ndialogue breakdown detection with semi-supervised\nlearning. In NeurIPS Workshop on Human in the\nLoop Dialogue Systems.\n212\nYixin Nie, Mary Williamson, Mohit Bansal, Douwe\nKiela, and Jason Weston. 2021. I like fish, espe-\ncially dolphins: Addressing contradictions in dia-\nlogue modeling. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 1699–1713.\nYushan Qian, Bo Wang, Shangzhao Ma, Wu Bin, Shuo\nZhang, Dongming Zhao, Kun Huang, and Yuexian\nHou. 2023. Think twice: A human-like two-stage\nconversational agent for emotional response genera-\ntion. arXiv preprint arXiv:2301.04907.\nMichael V Reiss. 2023. Testing the reliability of chatgpt\nfor text annotation and classification: A cautionary\nremark. arXiv preprint arXiv:2304.11085.\nSahand Sabour, Chujie Zheng, and Minlie Huang. 2022.\nCem: Commonsense-aware empathetic response gen-\neration. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 36, pages 11229–\n11237.\nAshish Sharma, Adam Miner, David Atkins, and Tim Al-\nthoff. 2020. A computational approach to understand-\ning empathy expressed in text-based mental health\nsupport. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 5263–5276.\nWeiyan Shi, Yu Li, Saurav Sahay, and Zhou Yu. 2021.\nRefine and imitate: Reducing repetition and inconsis-\ntency in persuasion dialogues via reinforcement learn-\ning and human demonstration. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 3478–3492.\nKurt Shuster, Jack Urbanek, Arthur Szlam, and Jason\nWeston. 2022. Am i me or you? state-of-the-art di-\nalogue models cannot maintain an identity. In Find-\nings of the Association for Computational Linguistics:\nNAACL 2022, pages 2367–2387.\nHaoyu Song, Yan Wang, Weinan Zhang, Xiaojiang Liu,\nand Ting Liu. 2020. Generate, delete and rewrite: A\nthree-stage framework for improving persona consis-\ntency of dialogue generation. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5821–5831.\nEkaterina Svikhnushina and Pearl Pu. 2023. Approx-\nimating human evaluation of social chatbots with\nprompting. arXiv preprint arXiv:2304.05253.\nEkaterina Svikhnushina, Iuliana V oinea, Anuradha We-\nlivita, and Pearl Pu. 2022. A taxonomy of empathetic\nquestions in social dialogs. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2952–2973, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nLongzheng Wang, Peng Zhang, Xiaoyu Lu, Lei Zhang,\nChaoyang Yan, and Chuang Zhang. 2022. Qadial-\nmoe: Question-answering dialogue based fact veri-\nfication with mixture of experts. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2022, pages 3146–3159.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack\nHessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A Smith,\nIz Beltagy, et al. 2023. How far can camels go?\nexploring the state of instruction tuning on open re-\nsources. arXiv preprint arXiv:2306.04751.\nAnuradha Welivita and Pearl Pu. 2020. A taxonomy of\nempathetic response intents in human social conversa-\ntions. In Proceedings of the 28th International Con-\nference on Computational Linguistics, pages 4886–\n4899, Barcelona, Spain (Online). International Com-\nmittee on Computational Linguistics.\nSean Welleck, Jason Weston, Arthur Szlam, and\nKyunghyun Cho. 2019. Dialogue natural language\ninference. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3731–3741, Florence, Italy. Association for\nComputational Linguistics.\nYadong Xi, Jiashu Pu, and Xiaoxi Mao. 2021. Tam-\ning repetition in dialogue generation. arXiv preprint\narXiv:2112.08657.\nJing Xu, Arthur Szlam, and Jason Weston. 2022a. Be-\nyond goldfish memory: Long-term open-domain con-\nversation. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5180–5197.\nRuijian Xu, Chongyang Tao, Daxin Jiang, Xueliang\nZhao, Dongyan Zhao, and Rui Yan. 2021. Learning\nan effective context-response matching model with\nself-supervised tasks for retrieval-based dialogues.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pages 14158–14166.\nXinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu,\nHua Wu, Haifeng Wang, and Shihang Wang. 2022b.\nLong time no see! open-domain conversation with\nlong-term persona memory. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 2639–2650.\nHaolan Zhan, Zhuang Li, Yufei Wang, Linhao Luo, Tao\nFeng, Xiaoxi Kang, Yuncheng Hua, Lizhen Qu, Lay-\nKi Soon, Suraj Sharma, Ingrid Zukerman, Zhaleh\nSemnani-Azad, and Gholamreza Haffari. 2023. So-\ncialdial: A benchmark for socially-aware dialogue\nsystems. In Proceedings of the 46th International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval.\nChen Zhang, Yiming Chen, Luis Fernando D’Haro,\nYan Zhang, Thomas Friedrichs, Grandee Lee, and\nHaizhou Li. 2021. DynaEval: Unifying turn and di-\nalogue level evaluation. In Proceedings of the 59th\n213\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5676–5689, Online. Association\nfor Computational Linguistics.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you have\npets too? In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 2204–2213.\nChujie Zheng, Yong Liu, Wei Chen, Yongcai Leng, and\nMinlie Huang. 2021. Comae: A multi-factor hierar-\nchical framework for empathetic response generation.\nIn Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021, pages 813–824.\nPeixiang Zhong, Chen Zhang, Hao Wang, Yong Liu,\nand Chunyan Miao. 2020. Towards persona-based\nempathetic conversational models. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 6556–\n6566.\nYiming Zhu, Peixian Zhang, Ehsan-Ul Haq, Pan Hui,\nand Gareth Tyson. 2023. Can chatgpt reproduce\nhuman-generated labels? a study of social computing\ntasks. arXiv preprint arXiv:2304.10145.\n214\nA Behavior Questions and Definitions\nThe Question (Q) and Definition (D) for each di-\nalogue behavior label used for the final ChatGPT\nprompts are shown in Tables 5 - 12, excludingRed\nwhich is shown in Table 2 in Section 5.\nQ Is this an empathetic response by Speaker 2:\nA response is empathetic when Speaker 2 does ONE\nof the following:\n- clearly demonstrates an understanding of Speaker 1’s\nemotions\n- reacts with the appropriate sentiment or emotion\nto Speaker 1’s shared experience\nD\n- understands or appropriately reacts to Speaker 1’s\nexperience or emotions\n- appropriately reassures, encourages, or supports Speaker 1\nTable 5: Emp: behavior question and definition.\nQ If this were the next response in the dialogue,\nwould Speaker 1 feel like their feelings are not\nbeing understood by Speaker 2:\nA response displays a lack of empathy when:\n- it indicates a misunderstanding of how Speaker 1\nfeels based on what Speaker 1 just said\n- the tone, emotion, or sentiment of the response is\nclearly inappropriate for what Speaker 1 just said\n- the response has an inappropriate lack of emotion to\nwhat Speaker 1 just said\nDo NOT consider its empathy relative to previous topics in\nthe conversation if the dialogue has moved on from them.\nD\nInstead, only consider the most recent dialogue context\nwhen evaluating the empathy of a response.\nTable 6: !Emp: behavior question and definition.\nQ If this were the next response in the dialogue,\nwould it contradict commonsense:\nTo identify contradictions of commonsense, judge whether\na vast majority of people would agree that the response\ndoesn’t make sense because the response:\n- contradicts common knowledge\n- makes unfounded assumptions\n- is highly illogical or self-contradictory\n- asks a question where the answer is already obvious\nDo NOT mark responses that don’t make sense because they:\n- are off-topic or irrelevant as responses\nD\n- don’t have any clear meaning (e.g. overly vague or\nill-formed responses)\nTable 7: !Com: behavior question and definition.\nQ If this were the next response in the dialogue, does it\ncompletely ignore the immediate last turn from Speaker 1:\nResponses that are completely off-topic, fail to address the\nasked question, or are otherwise completely inappropriate inD\nthe context are considered to be ignoring the other speaker.\nTable 8: Ign: behavior question and definition.\nQ If this were the next response in the dialogue,\nis it a self-contradiction by Speaker 2:\nSelf contradictions occur when Speaker 2 says\nsomething that is a contradiction of what they have\nsaid previously or it is extremely implausible based\non the information they have already shared.\nSelf contradictions may also occur within a single turn\nif Speaker 2 shares two contradictory things.\nIf Speaker 2 shares world knowledge that is factually\nincorrect this is NOT enough on its own to warrant a\nself contradiction.\nD\nIf Speaker 2 contradicts something the other speaker\nSpeaker 1 has said, this is NOT a self-contradiction.\nTable 9: !Sel: behavior question and definition.\nQ Does this response include an incorrect fact:\nIncorrect facts occur when the response includes\ninformation that is either:\n- false\n- unproven\n- highly controversial\n- highly implausible\n- clearly misleading\nIf an organization, person, place, etc. is mentioned as a\npart of public knowledge, but it does not exist or it is\ninaccurately represented, then this is an incorrect fact.\nDo NOT consider a turn as an incorrect fact if the turn could\nbe interpreted as expressing:\nD\n- preference or value judgements\n- estimates or predictions\n- personal information about the speaker or their partner\n- information about things in either speaker’s life that are\nnot publicly relevant\nTable 10: !Fac: behavior question and definition.\nQ Is Speaker 2 saying something about Speaker 1 that\nis contradicting what Speaker 1 has already shared:\nPartner contradictions occur when Speaker 2:\n- shares an assumption about Speaker 1 that is impossible\nto know based on what has already been said\n- shares an inference about Speaker 1 that is implausible\nbased on what has already been said\n- contradicts something Speaker 1 shared about themselves\n- asks a repetitive question about Speaker 1 when the\nanswer is already known based on what has already been said\nIf Speaker 2 says something that makes it seem like they have\nforgotten or misremembered what their partner Speaker 1\nhas said earlier in the dialogue, this is a partner contradiction.\nD\nIf Speaker 2 shares a difference of opinion or\nsituation in their own life as compared to Speaker 1,\nthis is NOT a partner contradiction.\nTable 11: !Par: behavior question and definition.\nQ If this were the next response in the dialogue,\nis it completely irrelevant to what was just said:\nIf a response fails to continue the current discussion or jumps to\na new and off-topic discussion, it is considered to be irrelevant.\nResponses that are irrelevant feel abrupt and interrupt the\ndiscussion, usually because they present questions or ideas\nthat are unrelated to the previous turn. Short reactions to or\nD\nacknowledgements of the previous turn are NOT irrelevant.\nTable 12: !Rel: behavior question and definition.\n215\nB Full Prompt Example\nTable 13 shows an example of the full ChatGPT\nprompt utilized in this work. This full version\npreserves the ‘DIALOGUE’ header, turn num-\nbers, whitespace newlines, and full speaker names,\nwhich were removed from the prompt in Table 2 in\nSection 5 due to spacing constraints.\n- - - - - - - - - -\nDIALOGUE\n- - - - - - - - - -\n1. Speaker 1: Hi!\n2. Speaker 2: Hi! I am happy to be talking to you. Do you have any\npets at home?\n3. Speaker 1: I don’t, my mom is allergic to most pets. I used to have\na pet fish when I was much younger.\n4. Speaker 2: Do you want a pet at some point in the future? No\njudgment here, I’m just curious.\n5. Speaker 1: I would love to have pets in the future! Once I have\nmy own place, I’d like to get a dog or two.\n- - - - - - - - - -\nIs this response repeating something that has already been said:\nSpeaker 2: Would you want to get a cat or a dog?\nA response is repetitive if:\n- it repeats something from earlier in the dialogue\n- it includes asking a question whose answer has been\nalready shared\nIf any part of the response is repetitive, then it should be\nlabeled as repetitive.\nNote that sometimes repetition is useful, such as for emphasis,\nacknowledgement, clarification, or elaboration, and in these\ncases it should NOT be labeled as repetitive.\nProvide your reasoning when considering this question starting\nwith “Reasoning:”. Then, finish by writing your final\ndecision as one of: “Decision: [YES]” or “Decision: [NO]”.\nDo NOT fill in your decision with any terms other than YES or NO.\nTable 13: An example of an unmodified ChatGPT\nprompt.\nC Full Results\nTable 14 extends Table 3 from §6.2 to include the\nprecision and recall scores for the automated mod-\nels. Precision and recall scores are not meaningful\nfor the human evaluators since each human anno-\ntation set is traded out as a benchmark against the\nother; thus, we still present only F1 for HUM.\nD ChatGPT Cost\nWe compare the average cost of labeling a single\ndialogue from ABC-Eval for each behavior using\nChatGPT and human judges. Table 15 contains the\ncalculated costs.\nChatGPT The ChatGPT cost for a single dia-\nlogue is calculated from the OpenAI API pricing5\n5https://openai.com/pricing\n($0.002 USD per 1000 tokens, at time of writing)\non the sum total number of tokens used for obtain-\ning labels for each bot response for a particular\nbehavior. These costs are then averaged over all di-\nalogues used in this work to obtain the average cost\nper dialogue. Because there is not much difference\nin prompt length for the different behavior prompts,\nthe average cost per behavior is quite similar.\nHUM Human annotation costs are derived from\nthe average costs presented in Finch et al. (2023).\nSince the behavior labels were grouped into anno-\ntation tasks for the human judges, we divide each\ntask cost by the number of behaviors contained\nwithin that task. The cost for a single label is then\nthe resulting quotient for its respective task.\nModel P/R/F1+ P/R/F1- Acc. #+\n!Fac\nFC 12.3 / 22.4 / 15.993.3 / 87.1 / 90.182.2 223\nChatGPT37.7 / 44.9 / 41.095.5 / 94.0 / 94.790.3†† 146\nHUM 67.8 97.4 95.2⋆⋆ 122\nRed ChatGPT30.7 / 35.5 / 32.994.3 / 93.2 / 93.888.6 148\nHUM 58.7 96.4 93.5⋆⋆ 129\n!Com ChatGPT43.8 / 29.1 / 34.983.3 / 90.5 / 86.778.0 219\nHUM 55.6 88.6 81.9⋆ 333\n!Rel\nS2T2 24.0 / 33.5 / 27.986.3 / 79.8 / 82.972.4† 365\nChatGPT30.1 / 62.5 / 40.691.0 / 72.3 / 80.670.8 543\nHUM 54.3 91.3 85.4⋆⋆ 261\n!Par ChatGPT27.2 / 14.2 / 18.691.6 / 96.1 / 93.888.5 79\nHUM 48.8 94.8 90.5⋆⋆ 151\n!Sel\nDEC 22.8 / 49.1 / 31.196.3 / 89.2 / 92.686.6†† 215\nChatGPT14.6 / 35.9 / 20.795.3 / 86.1 / 90.583.0 250\nHUM 44.3 96.3 93.1⋆⋆ 101\n!Emp\nEPI 12.0 / 15.1 / 13.485.4 / 81.8 / 83.572.3 291\nChatGPT21.1 / 36.2 / 26.688.1 / 77.7 / 82.671.8 396\nHUM 51.5 92.0 86.3⋆⋆ 231\nIgn\nS2T2 18.5 / 39.5 / 25.291.9 / 79.7 / 85.375.5†† 365\nChatGPT15.5 / 63.4 / 24.993.3 / 59.8 / 72.960.2 696\nHUM 61.6 95.5 92.0⋆⋆ 170\nEmp\nEPI 39.6 / 86.0 / 54.270.3 / 20.1 / 31.345.0 1343\nChatGPT50.7 / 11.9 / 19.363.4 / 92.9 / 75.462.3†† 146\nHUM 69.7 81.6 77.1⋆⋆ 618\nTable 14: Precision, recall, F1 and accuracy achieved by\neach model, where HUM stands for human judges. #+:\nnum. positive labels predicted. †|†† denote significance\nbetween automated models on one or both annotation\nsets. ⋆| ⋆ ⋆denote significance against best automated\nmodel on one or both annotation sets, respectively.\nChatGPT HUM\n!Fac 0.02 1.96\nRed 0.02 0.29\n!Com 0.02 0.92\n!Rel 0.02 0.47\n!Par 0.02 0.29\n!Sel 0.02 0.29\n!Emp 0.02 0.58\nIgn 0.02 0.47\nEmp 0.02 0.58\nTable 15: Cost ($ USD) per dialogue for each behavior\nusing ChatGPT or humans (HUM).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.813444972038269
    },
    {
      "name": "Task (project management)",
      "score": 0.6301854252815247
    },
    {
      "name": "Identification (biology)",
      "score": 0.569080114364624
    },
    {
      "name": "Artificial intelligence",
      "score": 0.518397867679596
    },
    {
      "name": "Human–computer interaction",
      "score": 0.4694245457649231
    },
    {
      "name": "Machine learning",
      "score": 0.42941486835479736
    },
    {
      "name": "Data science",
      "score": 0.3357129693031311
    },
    {
      "name": "Engineering",
      "score": 0.08075299859046936
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ]
}