{
  "title": "Operationalizing and Implementing Pretrained, Large Artificial Intelligence Linguistic Models in the US Health Care System: Outlook of Generative Pretrained Transformer 3 (GPT-3) as a Service Model",
  "url": "https://openalex.org/W4205865577",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1971732328",
      "name": "Emre Sezgin",
      "affiliations": [
        "Nationwide Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4207882106",
      "name": "Joseph Sirrianni",
      "affiliations": [
        "Nationwide Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4207882107",
      "name": "Simon L Linwood",
      "affiliations": [
        "University of California, Riverside"
      ]
    },
    {
      "id": "https://openalex.org/A1971732328",
      "name": "Emre Sezgin",
      "affiliations": [
        "Nationwide Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4207882106",
      "name": "Joseph Sirrianni",
      "affiliations": [
        "Nationwide Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A4207882107",
      "name": "Simon L Linwood",
      "affiliations": [
        "University of California, Riverside"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3135012552",
    "https://openalex.org/W3000558625",
    "https://openalex.org/W3015333520",
    "https://openalex.org/W3006913750",
    "https://openalex.org/W3164718925",
    "https://openalex.org/W3168771811",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2594519269",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2133512280",
    "https://openalex.org/W3119071838",
    "https://openalex.org/W4254088331",
    "https://openalex.org/W3197552266"
  ],
  "abstract": "Generative pretrained transformer models have been popular recently due to their enhanced capabilities and performance. In contrast to many existing artificial intelligence models, generative pretrained transformer models can perform with very limited training data. Generative pretrained transformer 3 (GPT-3) is one of the latest releases in this pipeline, demonstrating human-like logical and intellectual responses to prompts. Some examples include writing essays, answering complex questions, matching pronouns to their nouns, and conducting sentiment analyses. However, questions remain with regard to its implementation in health care, specifically in terms of operationalization and its use in clinical practice and research. In this viewpoint paper, we briefly introduce GPT-3 and its capabilities and outline considerations for its implementation and operationalization in clinical practice through a use case. The implementation considerations include (1) processing needs and information systems infrastructure, (2) operating costs, (3) model biases, and (4) evaluation metrics. In addition, we outline the following three major operational factors that drive the adoption of GPT-3 in the US health care system: (1) ensuring Health Insurance Portability and Accountability Act compliance, (2) building trust with health care providers, and (3) establishing broader access to the GPT-3 tools. This viewpoint can inform health care practitioners, developers, clinicians, and decision makers toward understanding the use of the powerful artificial intelligence tools integrated into hospital systems and health care.",
  "full_text": "Viewpoint\nOperationalizing and Implementing Pretrained, Large Artificial\nIntelligence Linguistic Models in the US Health Care System:\nOutlook of Generative Pretrained Transformer 3 (GPT-3) as a\nService Model\nEmre Sezgin1*, PhD; Joseph Sirrianni1*, PhD; Simon L Linwood2, MBA, MD\n1The Abigail Wexner Research Institute, Nationwide Children's Hospital, Columbus, OH, United States\n2School of Medicine, University of California Riverside, Riverside, CA, United States\n*these authors contributed equally\nCorresponding Author:\nEmre Sezgin, PhD\nThe Abigail Wexner Research Institute\nNationwide Children's Hospital\n700 Children's Drive\nColumbus, OH, 43205\nUnited States\nPhone: 1 6143556814\nEmail: esezgin1@gmail.com\nAbstract\nGenerative pretrained transformer models have been popular recently due to their enhanced capabilities and performance. In\ncontrast to many existing artificial intelligence models, generative pretrained transformer models can perform with very limited\ntraining data. Generative pretrained transformer 3 (GPT-3) is one of the latest releases in this pipeline, demonstrating human-like\nlogical and intellectual responses to prompts. Some examples include writing essays, answering complex questions, matching\npronouns to their nouns, and conducting sentiment analyses. However, questions remain with regard to its implementation in\nhealth care, specifically in terms of operationalization and its use in clinical practice and research. In this viewpoint paper, we\nbriefly introduce GPT-3 and its capabilities and outline considerations for its implementation and operationalization in clinical\npractice through a use case. The implementation considerations include (1) processing needs and information systems infrastructure,\n(2) operating costs, (3) model biases, and (4) evaluation metrics. In addition, we outline the following three major operational\nfactors that drive the adoption of GPT-3 in the US health care system: (1) ensuring Health Insurance Portability and Accountability\nAct compliance, (2) building trust with health care providers, and (3) establishing broader access to the GPT-3 tools. This viewpoint\ncan inform health care practitioners, developers, clinicians, and decision makers toward understanding the use of the powerful\nartificial intelligence tools integrated into hospital systems and health care.\n(JMIR Med Inform 2022;10(2):e32875) doi: 10.2196/32875\nKEYWORDS\nnatural language processing; artificial intelligence; generative pretrained transformer; clinical informatics; chatbot\nIntroduction\nIn 2020, OpenAI unveiled their third-generation language\ngeneration model, which is known as the generative pretrained\ntransformer 3 (GPT-3) model [1]. This model was the latest in\na line of large pretrained models designed for understanding\nand producing natural language by using the transformer\narchitecture, which was published only 3 years prior and\nsignificantly improved natural language understanding task\nperformance over that of models built on prior architectures [2].\nHowever, GPT-3’s development was remarkable because it\nresulted in a substantial increase in the model’s size; it increased\nby more than 10-fold in 1 year, reaching 175 billion weights\n[1-3]. GPT-3’s increased model size makes it substantially more\npowerful than prior models; propels its language capabilities to\nnear–human-like levels; and, in some cases, makes it the\nsuperior option for several language understanding tasks [1].\nJMIR Med Inform 2022 | vol. 10 | iss. 2 | e32875 | p. 1https://medinform.jmir.org/2022/2/e32875\n(page number not for citation purposes)\nSezgin et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nOrdinarily, deep learning tasks require large amounts of labeled\ntraining data. This requirement usually limits the tasks to which\ndeep learning can be effectively applied. However, with its\nincreased model size, GPT-3 has an enhanced capability for\nso-called few-shot, one-shot, and zero-shotlearning when\ncompared to prior models [1,4]. These learning methods involve\ntraining a model on significantly smaller amounts of training\ndata. In these methods, the models are given a description of\nthe task and, if applicable, a handful of examples to learn from,\nwith few-shot training on only hundreds to thousands of\ninstances, one-shot training on only 1 example, and zero-shot\ntraining on only the task description.\nGPT-3 was designed as a language generation model, focusing\non producing appropriate text responses to an input. Although\nit can be adapted to address more traditional machine learning\ntasks, such as answering yes-no questions, matching pronouns\nto their nouns, and conducting sentiment analyses [1], GPT-3’s\ntext generation capabilities have attracted much attention as a\npotential solution for a variety of problems, such as creating\nenhanced chatbots [5], answering complex questions, generating\ncomputer code from a design specification [6], and writing news\narticles [7]. As such, there is much research on and commercial\ninterest in using GPT-3 for a variety of text generation\napplications. Textbox 1 illustrates the power of GPT-3 in 3\nquestion and answer examples [8]. In 3 cases, GPT-3 provided\nhuman-like logical and intellectual responses. However,\napplying GPT-3 in health care is still an emerging topic.\nA plethora of research articles have outlined the potential\nopportunities for integrating artificial intelligence (AI) and\nmachine learning models in a variety of hospital and health care\nsettings [9-11]. However, the authors also quickly pointed out\nthe many development and implementation challenges to\ndeploying such solutions, such as the lack of transparency [11],\npotential model biases [9], and financing [10]. A successful\ndeployment of GPT-3 would need to overcome all of these\nstandard barriers to AI implementation alongside the additional\nchallenges that GPT-3 itself introduces [12]. In this viewpoint,\nwe introduce the use of GPT-3 in the health care setting with a\nuse case and discuss implementation and operational\nconsiderations for its use in clinical practice.\nTextbox 1. Generative pretrained transformer 3 (GPT-3) question and answer examples.\nQuestion and answer\n• Human-generated question: “What is human life expectancy in the United States?”\n• GPT-3–generated response: “Human life expectancy in the United States is 78 years.”\nText summary\n• Human-generated prompt: “Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth\nthat of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible\nto the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god\nJupiter. When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows, and is on average the third-brightest\nnatural object in the night sky after the Moon and Venus.”\n• GPT-3–generated response (a second grader’s language): “Jupiter is a huge planet made of gas. It is the fifth planet from the sun. It is very big\nand bright. It is the third brightest object in the night sky after the moon and Venus. Jupiter has many moons. One of the moons is bigger than\nthe planet Mercury.”\nA chatbot (sarcastic)\n• Human: “Why is the sky blue?”\n• GPT-3 chatbot: “I’m not sure. I’ll ask my friend Google.”\nA Use Case: GPT-3 in Health Care\nThe power of meaningful text generation by GPT-3 makes it\nan ideal tool for human-machine conversations, especially those\nconducted via chatbots. In our case, we illustrate the use of\nGPT-3 within a hospital network. In this hypothetical example,\nthe hospital is providing a chatbot triaging mechanism for\nincoming patients to reduce the overhead at clinics and increase\nthe safety and quality of care during the COVID-19 pandemic.\nThe chatbot has to be connected to the hospital network,\ncombined with a triage text summary service that is to be\nreviewed, and stored in the electronic health record (EHR;\nFigure 1). Putting aside the front-end details in this workflow\n(Figure 1), this use case outlines a typical implementation of\nGPT-3 as a service within a health system.\nJMIR Med Inform 2022 | vol. 10 | iss. 2 | e32875 | p. 2https://medinform.jmir.org/2022/2/e32875\n(page number not for citation purposes)\nSezgin et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 1. GPT-3 use case (chatbot triaging and patient note summarization). API: application programming interface; EHR: electronic health record;\nGPT-3: generative pretrained transformer 3.\nIn this example, triage could be initiated by a patient or a\nhospital to conduct a health screening. The front-end application\nis operationalized through a chatbot mechanism over a front-end\napplication, which could be a patient portal app, voice assistant,\nphone call, or SMS text messaging. Once a connection is\nestablished, the hospital system formulates GPT-3 requests by\ngathering patient health information and formatting this\ninformation to be interpretable with the GPT-3 model. Within\nthe secure hospital network, GPT-3 is located outside of the\nEHR and provided as the “GPT-3-as-a-Service” platform. The\napplication programming interface enables interoperability and\nacts as a gatekeeper for the data transfer of requests and\nresponses. Once a request is received, the “GPT-3-as-a-Service”\nplatform preprocesses the data and requests, allocates the tasks\nto be completed, produces outputs in an interpretable format,\nand sends the outputs to users. The type of tasks allocated\ndepends on the requests, which, in our case, are question\nanswering, text generation or culturally appropriate language\ntranslation, and text summarization. The response is sent back\nto the EHR system and then to the front-end application. At the\nend of triage, similar to the after-visit summary, the conversation\ntext is summarized. To reduce the additional clinical burden of\nreading the whole conversation, GPT-3 summarizes the text\n(similar to a digital scriber) and stores it in the patient's health\nrecords. To avoid or address potential biases [12], correct errors,\nand increase the control over patient data use and the model,\nthe human-in-the-loop model [13] can be implemented by using\na report back mechanism at the front end, or the clinical team\ncan be given oversight of GPT-3 integrated process in the\nhospital EHR system at the back end. Furthermore, the error\ncorrections and adjustments in the text can be used to fine-tune\nthe GPT-3 model to increase its accuracy and effectiveness.\nTo be able to execute this use case in a real-world setting, health\ncare practitioners and decision makers should consider and\naddress the following operational and implementation\nchallenges.\nImplementation Considerations\nProcessing Needs and Information Systems\nInfrastructure\nUnlike more traditional AI models, GPT-3 is considerably larger\nin terms of memory requirements and is more computationally\nintensive. Specialized hardware for model training and\nexecution—either graphics processing units or tensor processing\nunits—is required for a scalable implementation. For any\nhospital system, additional investments for infrastructure to\ncompensate for processing needs could be required.\nGiven its size, dependencies, and hardware requirements, a\nGPT-3 solution would likely need to be run as a service. For\nthis service, hospital systems would need to submit a service\nrequest to the GPT-3 solution service, which would process the\nrequest and return its results back to the hospital system. The\nhospital local network in Figure 1 shows a sample workflow\ndiagram for such an implementation. Such a setup would require\ndiligent and significant provisioning, networking, and\nmonitoring to ensure that the services are accessible and provide\nmeaningful value.\nOperating Cost\nGiven the current state of hospital networks and EHR systems,\nthe integration of GPT-3 solutions would require complex\nsystems and high technical knowledge for effective deployment\nand be costly to operationalize. One possible solution to ease\nthe burden of GPT-3 deployments is integration with cloud\ncomputing platforms within hospital systems. Many cloud\ncomputing providers offer the specialized hardware needed to\nrun such models and can easily handle off-the-shelf networking\nand dynamic load balancing. This would ease the burden of the\nmajor components of GPT-3 deployment; however, outsourcing\ncloud computing platforms can potentially increase the operating\ncost.\nJMIR Med Inform 2022 | vol. 10 | iss. 2 | e32875 | p. 3https://medinform.jmir.org/2022/2/e32875\n(page number not for citation purposes)\nSezgin et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nModel Bias\nSeveral sources of bias can manifest themselves in a\nGPT-3–powered solution at different levels. At a model level,\nGPT-3 is trained on a large data set that has many problematic\ncharacteristics related to racial and sexist stereotypes, and as a\nresult, the model learns certain biases against marginalized\nidentities [14,15]. These biases, which are present in GPT-3,\ncan be harmful in clinical settings. Korngiebel and Mooney [12]\nhighlight the risks of using GPT-3 in health care delivery, noting\nspecific examples where GPT-3 parrots extremist language from\nthe internet [16] and affirms suicidal ideation [17].\nAside from the inherent bias of GPT-3’s initial training,\nfine-tuning on medical data could also introduce the\nunintentional biases present in historic medical data. Practical\nbiases, such as the undertesting of marginalized subpopulations,\ncan influence underlying clinical data and introduce bias during\nthe training of predictive models [9]. Additionally, the implicit\nbiases of health care professionals can influence diagnoses and\ntreatments and are reflected in clinical notes [18], which, if used\nto fine-tune GPT-3, would potentially affect the developed\nmodel.\nGiven these biases, it would be unwise to deploy GPT-3 or any\nother sizable language model without active bias testing [15].\nExplicit procedures should be put in place to monitor, report,\nand react to potential biases produced by GPT-3 predictions.\nThese mechanisms would ensure that GPT-3 can be used\neffectively without introducing harm to the patient. In our use\ncase (Figure 1), we also added a human-in-the-loop mechanism,\nwhich can mandate the control, assessment, and training\nprotocols and yield interpretable and manageable results.\nEvaluation Metrics\nAside from physical implementation, there are methodological\nconsiderations for deploying GPT-3. As Watson et al [10] notes\nin their investigation of model deployment in academic medical\ncenters, clinical utility is a major concern for institutions.\nUnderstanding the best way to receive and interpret model\nresults is imperative for a successful deployment, and ideally,\nmodel performance should be tracked and assessed by using\nevaluation methodologies and frameworks.\nThe evaluation of text generation tasks, that is, those that GPT-3\nis designed to address, is notoriously difficult. Standard metrics,\nsuch as prediction sensitivity and positive predictive value, do\nnot cleanly reflect correctness in text generation, as ideas can\nbe expressed in many ways in text. More specialized text\ngeneration metrics, such as BLEU (Bilingual Evaluation\nUnderstudy) [19] and METEOR (Metric for Evaluation of\nTranslation with Explicit Ordering) [20], try to account for text\nvariation but still only examine text at a word level without\ncapturing the fundamental meaning. Methods that do try to\nincorporate the meaning of text in text evaluation rely on other\nblack-box deep learning models to produce a value [21]. Relying\non a black-box evaluation method to evaluate a black-box model\ndoes not increase interpretability. Such a method would only\nresult in lower trust overall and thus decrease the likelihood of\nthe model being deployed.\nHealth care–specific evaluation methods and frameworks for\ntext generation tasks are therefore needed. The development of\nmore robust methodologies for evaluating text generation tasks\nin the health care domain is required before the significant\nadoption of GPT-3 technology can be achieved. It is imperative\nthat data scientists, informaticists, developers, clinicians, and\nhealth care practitioners collaborate in the development of\nevaluation measures to ensure a successful implementation of\nGPT-3.\nOperational Considerations:Compliance,\nTrust, and Access\nIn addition to implementation, there are 3 major operational\nfactors driving the adoption of GPT-3 in health care, as follows:\n(1) GPT-3 needs to work in compliance with the Health\nInsurance Portability and Accountability Act (HIPAA), (2)\ntechnology providers need to earn trust from health care\nproviders, and (3) technology providers should improve access\nto the tool (Figure 2).\nSimilar to GPT-3, there was huge enthusiasm to use the Amazon\nAlexa (Amazon.com Inc) voice assistant in health care delivery\nwhen it was released in 2014. However, at the time, Alexa was\nnot yet legally able to store or transmit private health\ninformation. It took Amazon 5 years to become HIPAA\ncompliant and to be able to sign business associate agreements\nwith health care providers [22]. A limited number of Alexa\nskills was released, and there is still a long list of other Alexa\nskills waiting to become HIPAA compliant. This example shows\nthe slow progress of legislation changes and regulation updates\nfor including new technologies in health care, suggesting that\nefforts should be put forward as early as possible for GPT-3.\nWithout HIPAA compliance, the adoption of GPT-3 in health\ncare can be a false start [23]. However, although HIPAA\ncompliance may not be immediate, it may be gradually\nprogressing. GPT-3 is a black-box model, which complicates\nthe HIPAA compliance process because unlike with other types\nof programmatic solutions, it is harder to decipher how data are\nprocessed internally by the model itself. However, assuming\nthat GPT-3 will be deployable in the future, operations will start\nwith implementing the limited capabilities of GPT-3 (ie, storing\nand transmitting data, running behind the firewalls of specific\nhardware [security rules], and analyzing a specific data set or\npatient cohort [privacy rules]). In parallel, further practices are\nneeded to optimize the payment models for accommodating\nGPT-3 and seek opportunities for satisfying the US Food and\nDrug Administration’s requirements for software as a medical\ndevice [24] with regard to using AI in clinical applications.\nIn addition to legal requirements, trust must be established\namong patients, health care providers, and technology companies\nto adopt GPT-3 [25]. It is common for technology companies\nto claim the right that they can use their customers’ data to\nfurther improve their services or achieve additional commercial\nvalue. Additionally, the culture of skepticism toward AI among\nclinicians can place a heavy burden on model interpretability\nand result in lower trust in clinical care than in other industries\n[10]. Unlike commercial implementations, GPT-3 needs to be\nexplicitly discussed in terms of what it will and will not do with\nJMIR Med Inform 2022 | vol. 10 | iss. 2 | e32875 | p. 4https://medinform.jmir.org/2022/2/e32875\n(page number not for citation purposes)\nSezgin et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\na patient’s data. Health care providers’ data governance\ncommittees need to be aware and comfortable when they sign\nthe service agreement with GPT-3. Given the black-box nature\nof GPT-3, an operational strategic approach will be necessary\nfor interpreting the evaluation reports and outcomes that are\ngenerated through the human-in-the-loop model.\nFigure 2. A model of operational and implementation considerations for generative pretrained transformer 3. IS: information systems.\nAccess also needs to be ensured. Training large language models\nlike GPT-3 can cost tens of millions of dollars. As such, GPT-3\nis innovating the business model of access. Currently, GPT-3\nis privately controlled by OpenAI, and health care providers\ncan remotely run the program and pay for usage per token (1000\ntokens are approximately equivalent to 750 words) [26]. In\nSeptember 2020, Microsoft bought an exclusive license to\nGPT-3, with plans to integrate it into its existing products.\nSimilarly, a number of companies are already integrating GPT-3\nmodel predictions into their products. However, this business\nmodel also limits open-access research and development and\nwill eventually limit improvements, such as advancements in\ntranslation mechanisms and all-inclusive, equity-driven\napproaches in conversational agent development. In these early\nstages, open-source alternatives, such as GPT-J [27], may help\nhealth care developers and institutions assess operational\nviability. In future iterations, once the value of using GPT-3 in\nthe health care setting is assured, the responsibility of\naccessibility could be delegated to health care and government\nagencies. Such agencies may distribute the\n“GPT-3-as-a-Service” platform through secure cloud platforms\nand establish a federated learning mechanism to run\ndecentralized training services while collaboratively contributing\nto the GPT-3 model [28]. This would also reduce the burden\non individual health systems when it comes to building, training,\nand deploying their own GPT-3 platforms and reduce costs.\nThese advantages are especially beneficial for hospitals in\nlow-resource settings.\nConclusion\nIn this viewpoint, we briefly introduce GPT-3 and its capabilities\nand outline considerations for its implementation and\noperationalization in clinical practice through a use case.\nBuilding on top of Korngiebel and Mooney’s [12] remarks\ntoward unrealistic, realistic, feasible, and realistic but\nchallenging use cases, we provide consideration points for\nimplementing and operationalizing GPT-3 in clinical practice.\nWe believe that our work can inform health care practitioners,\ndevelopers, clinicians, and decision makers toward\nunderstanding the use of the powerful AI tools integrated into\nhospital systems and health care.\nAcknowledgments\nThis study is partially supported through a Patient-Centered Outcomes Research Institute (PCORI) award (award number:\nME-2017C1–6413) under the name “Unlocking Clinical Text in EMR by Query Refinement Using Both Knowledge Bases and\nWord Embedding.” All statements in this report, including its findings and conclusions, are solely those of the authors and do\nnot necessarily represent the views of the PCORI, its Board of Governors, or its Methodology Committee.\nJMIR Med Inform 2022 | vol. 10 | iss. 2 | e32875 | p. 5https://medinform.jmir.org/2022/2/e32875\n(page number not for citation purposes)\nSezgin et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nAuthors' Contributions\nES, JS, and SLL conceived the presented ideas. ES and JS drafted the manuscript. SL supervised and critically reviewed the\nmanuscript. All authors approved the final version of the manuscript.\nConflicts of Interest\nNone declared.\nReferences\n1. Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, et al. Language models are few-shot learners. arXiv.\nPreprint posted online on July 22, 2020 [FREE Full text]\n2. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. 2017 Presented at:\n31st Conference on Neural Information Processing Systems (NIPS 2017); December 4-9, 2017; Long Beach, California,\nUSA URL: http://papers.nips.cc/paper/7181-attention-is-all-you-%0Aneed.pdf\n3. Hutson M. Robo-writers: the rise and risks of language-generating AI. Nature 2021 Mar;591(7848):22-25. [doi:\n10.1038/d41586-021-00530-0] [Medline: 33658699]\n4. Liu J, Shen D, Zhang Y, Dolan B, Carin L, Chen W. What makes good in-context examples for GPT-3? arXiv. Preprint\nposted online on January 17, 2021 [FREE Full text]\n5. Emerson. GPT-3 Demo. URL: https://gpt3demo.com/apps/quickchat-emerson [accessed 2021-12-14]\n6. Langston J. From conversation to code: Microsoft introduces its first product features powered by GPT-3. The AI Blog.\n2021 May 25. URL: https://blogs.microsoft.com/ai/\nfrom-conversation-to-code-microsoft-introduces-its-first-product-features-powered-by-gpt-3/ [accessed 2021-12-14]\n7. A robot wrote this entire article. Are you scared yet, human? GPT-3. The Guardian. 2020 Sep 08. URL: https://www.\ntheguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3 [accessed 2022-02-01]\n8. Examples - OpenAI API. OpenAI. URL: https://beta.openai.com/examples/ [accessed 2021-12-14]\n9. Wiens J, Price WN2, Sjoding MW. Diagnosing bias in data-driven algorithms for healthcare. Nat Med 2020 Jan;26(1):25-26.\n[doi: 10.1038/s41591-019-0726-6] [Medline: 31932798]\n10. Watson J, Hutyra CA, Clancy SM, Chandiramani A, Bedoya A, Ilangovan K, et al. Overcoming barriers to the adoption\nand implementation of predictive modeling and machine learning in clinical care: what can we learn from US academic\nmedical centers? JAMIA Open 2020 Jul 10;3(2):167-172 [FREE Full text] [doi: 10.1093/jamiaopen/ooz046] [Medline:\n32734155]\n11. Waring J, Lindvall C, Umeton R. Automated machine learning: Review of the state-of-the-art and opportunities for healthcare.\nArtif Intell Med 2020 Apr;104:101822 [FREE Full text] [doi: 10.1016/j.artmed.2020.101822] [Medline: 32499001]\n12. Korngiebel DM, Mooney SD. Considering the possibilities and pitfalls of Generative Pre-trained Transformer 3 (GPT-3)\nin healthcare delivery. NPJ Digit Med 2021 Jun 03;4(1):93 [FREE Full text] [doi: 10.1038/s41746-021-00464-x] [Medline:\n34083689]\n13. Miller K. When algorithmic fairness fixes fail: The case for keeping humans in the loop. Stanford University. 2020 Nov\n02. URL: https://hai.stanford.edu/news/when-algorithmic-fairness-fixes-fail-case-keeping-humans-loop[accessed 2021-12-14]\n14. Lucy L, Bamman D. Gender and representation bias in GPT-3 generated stories. In: Proceedings of the Third Workshop\non Narrative Understanding. 2021 Jun Presented at: Third Workshop on Narrative Understanding; June 11, 2021; Virtual\np. 48-55 URL: https://aclanthology.org/2021.nuse-1.5.pdf [doi: 10.18653/v1/2021.nuse-1.5]\n15. Bender EM, Gebru T, McMillan-Major A, Shmitchell S. On the dangers of stochastic parrots: Can language models be too\nbig? 2021 Mar Presented at: FAccT '21: 2021 ACM Conference on Fairness, Accountability, and Transparency; March\n3-10, 2021; Canada. [doi: 10.1145/3442188.3445922]\n16. McGuffie K, Newhouse A. The radicalization risks of GPT-3 and advanced neural language models. arXiv. Preprint posted\nonline on September 15, 2020 [FREE Full text]\n17. Daws R. Medical chatbot using OpenAI’s GPT-3 told a fake patient to kill themselves. AI News. 2020 Oct 28. URL: https:/\n/artificialintelligence-news.com/2020/10/28/medical-chatbot-openai-gpt3-patient-kill-themselves/ [accessed 2022-02-01]\n18. FitzGerald C, Hurst S. Implicit bias in healthcare professionals: a systematic review. BMC Med Ethics 2017 Mar 01;18(1):19\n[FREE Full text] [doi: 10.1186/s12910-017-0179-8] [Medline: 28249596]\n19. Papineni K, Roukos S, Ward T, Zhu WJ. Bleu: a method for automatic evaluation of machine translation. 2002 Jul Presented\nat: 40th Annual Meeting of the Association for Computational Linguistics; July 7-12, 2002; Philadelphia, Pennsylvania,\nUSA URL: https://www.aclweb.org/anthology/P02-1040.pdf [doi: 10.3115/1073083.1073135]\n20. Banerjee S, Lavie A. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.\n2005 Jun Presented at: ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or\nSummarization; June 29, 2005; Ann Arbor, Michigan URL: https://www.aclweb.org/anthology/W05-0909.pdf [doi:\n10.3115/1626355.1626389]\n21. Celikyilmaz A, Clark E, Gao J. Evaluation of text generation: A survey. arXiv. Preprint posted online on May 18, 2021\n[FREE Full text]\nJMIR Med Inform 2022 | vol. 10 | iss. 2 | e32875 | p. 6https://medinform.jmir.org/2022/2/e32875\n(page number not for citation purposes)\nSezgin et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n22. Jiang R. Introducing new Alexa healthcare skills. Amazon Alexa. 2019 Apr 04. URL: https://developer.amazon.com/blogs/\nalexa/post/ff33dbc7-6cf5-4db8-b203-99144a251a21/introducing-new-alexa-healthcare-skills [accessed 2022-02-01]\n23. McGraw D, Mandl KD. Privacy protections to encourage use of health-relevant digital data in a learning health system.\nNPJ Digit Med 2021 Jan 04;4(1):2 [FREE Full text] [doi: 10.1038/s41746-020-00362-8] [Medline: 33398052]\n24. Artificial intelligence and machine learning in Software as a Medical Device. U.S. Food & Drug Administration. URL:\nhttps://www.fda.gov/medical-devices/software-medical-device-samd/\nartificial-intelligence-and-machine-learning-software-medical-device [accessed 2021-12-14]\n25. Patient trust must come at the top of researchers' priority list. Nat Med 2020 Mar;26(3):301. [doi: 10.1038/s41591-020-0813-8]\n[Medline: 32161404]\n26. Brockman G, Murati M, Welinder P, OpenAI. OpenAI API. OpenAI. 2020 Jun 11. URL: https://openai.com/blog/openai-api/\n[accessed 2021-12-14]\n27. Romero A. Can’t access GPT-3? Here’s GPT-J — its open-source cousin. Towards Data Science. 2021 Jun 24. URL: https:/\n/towardsdatascience.com/cant-access-gpt-3-here-s-gpt-j-its-open-source-cousin-8af86a638b11 [accessed 2021-12-14]\n28. Sadilek A, Liu L, Nguyen D, Kamruzzaman M, Serghiou S, Rader B, et al. Privacy-first health research with federated\nlearning. NPJ Digit Med 2021 Sep 07;4(1):132 [FREE Full text] [doi: 10.1038/s41746-021-00489-2] [Medline: 34493770]\nAbbreviations\nAI: artificial intelligence\nBLEU: Bilingual Evaluation Understudy\nEHR: electronic health record\nGPT-3: generative pretrained transformer 3\nHIPAA: Health Insurance Portability and Accountability Act\nMETEOR: Metric for Evaluation of Translation With Explicit Ordering\nPCORI: Patient-Centered Outcomes Research Institute\nEdited by C Lovis; submitted 12.08.21; peer-reviewed by A Trojan, T Caze, G Carot-Sans, J Ayre, W Zhang; comments to author\n24.10.21; revised version received 14.12.21; accepted 09.01.22; published 10.02.22\nPlease cite as:\nSezgin E, Sirrianni J, Linwood SL\nOperationalizing and Implementing Pretrained, Large Artificial Intelligence Linguistic Models in the US Health Care System: Outlook\nof Generative Pretrained Transformer 3 (GPT-3) as a Service Model\nJMIR Med Inform 2022;10(2):e32875\nURL: https://medinform.jmir.org/2022/2/e32875\ndoi: 10.2196/32875\nPMID:\n©Emre Sezgin, Joseph Sirrianni, Simon L Linwood. Originally published in JMIR Medical Informatics (https://medinform.jmir.org),\n10.02.2022. This is an open-access article distributed under the terms of the Creative Commons Attribution License\n(https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium,\nprovided the original work, first published in JMIR Medical Informatics, is properly cited. The complete bibliographic information,\na link to the original publication on https://medinform.jmir.org/, as well as this copyright and license information must be included.\nJMIR Med Inform 2022 | vol. 10 | iss. 2 | e32875 | p. 7https://medinform.jmir.org/2022/2/e32875\n(page number not for citation purposes)\nSezgin et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX",
  "topic": "Operationalization",
  "concepts": [
    {
      "name": "Operationalization",
      "score": 0.8912230730056763
    },
    {
      "name": "Computer science",
      "score": 0.6306342482566833
    },
    {
      "name": "Software portability",
      "score": 0.5790215134620667
    },
    {
      "name": "Generative grammar",
      "score": 0.5500739812850952
    },
    {
      "name": "Health Insurance Portability and Accountability Act",
      "score": 0.5321950912475586
    },
    {
      "name": "Health care",
      "score": 0.5312594771385193
    },
    {
      "name": "Transformer",
      "score": 0.5016958713531494
    },
    {
      "name": "Benchmarking",
      "score": 0.4560948610305786
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3924393951892853
    },
    {
      "name": "Knowledge management",
      "score": 0.325384259223938
    },
    {
      "name": "Computer security",
      "score": 0.1524435579776764
    },
    {
      "name": "Engineering",
      "score": 0.11400669813156128
    },
    {
      "name": "Business",
      "score": 0.10832935571670532
    },
    {
      "name": "Confidentiality",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1314135232",
      "name": "Nationwide Children's Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I103635307",
      "name": "University of California, Riverside",
      "country": "US"
    }
  ]
}