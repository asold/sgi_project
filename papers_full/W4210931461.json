{
    "title": "Legal Information Retrieval and Entailment Based on BM25, Transformer and Semantic Thesaurus Methods",
    "url": "https://openalex.org/W4210931461",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2330004701",
            "name": "Kim Mi Young",
            "affiliations": [
                "University of Alberta"
            ]
        },
        {
            "id": "https://openalex.org/A3178048881",
            "name": "Rabelo Juliano",
            "affiliations": [
                "University of Alberta"
            ]
        },
        {
            "id": null,
            "name": "Okeke, Kingsley",
            "affiliations": [
                "University of Alberta"
            ]
        },
        {
            "id": "https://openalex.org/A3180939716",
            "name": "Goebel, Randy",
            "affiliations": [
                "University of Alberta"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963105309",
        "https://openalex.org/W3176798136",
        "https://openalex.org/W2913352150",
        "https://openalex.org/W204338220",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3105439152",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W6747727460",
        "https://openalex.org/W6685158001",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2606964149",
        "https://openalex.org/W2145755360",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W2979574685",
        "https://openalex.org/W2131744502",
        "https://openalex.org/W2911964244",
        "https://openalex.org/W2148143831",
        "https://openalex.org/W2624370469",
        "https://openalex.org/W3174504910",
        "https://openalex.org/W2970438301",
        "https://openalex.org/W2946224300",
        "https://openalex.org/W4206404363",
        "https://openalex.org/W2607892599",
        "https://openalex.org/W131533222",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3099950029",
        "https://openalex.org/W4252076394",
        "https://openalex.org/W2093390569",
        "https://openalex.org/W4206765718",
        "https://openalex.org/W2009593660",
        "https://openalex.org/W1972594981",
        "https://openalex.org/W2033885566",
        "https://openalex.org/W188516331",
        "https://openalex.org/W2413794162",
        "https://openalex.org/W3099449837"
    ],
    "abstract": "Abstract We describe the techniques applied by the University of Alberta (UA) team in the most recent Competition on Legal Information Extraction and Entailment (COLIEE 2021). We participated in retrieval and entailment tasks for both case law and statute law; we applied a transformer-based approach for the case law entailment task, an information retrieval technique based on BM25 for legal information retrieval, and a natural language inference mechanism using semantic knowledge applied to statute law texts. This competition included 25 teams from 14 countries; our case law entailment approach was ranked no. 4 in Task 2, the BM25 technique for legal information retrieval was ranked no. 3 in Task 3, and the natural language inference technique incorporating semantic information was ranked no. 4 in Task 4. The combination of the latter two techniques on Task 5 was ranked no. 2. We also performed error analysis of our system in Task 4, which provides some insight into current state-of-the-art and research priorities for future directions.",
    "full_text": "Vol.:(0123456789)\nThe Review of Socionetwork Strategies (2022) 16:157–174\nhttps://doi.org/10.1007/s12626-022-00103-1\n1 3\nARTICLE\nLegal Information Retrieval and Entailment Based \non BM25, Transformer and Semantic Thesaurus Methods\nMi‑Young Kim1,2 · Juliano Rabelo2 · Kingsley Okeke1 · Randy Goebel2\nReceived: 10 September 2021 / Accepted: 7 January 2022 / Published online: 7 February 2022 \n© The Author(s) 2022\nAbstract\nWe describe the techniques applied by the University of Alberta (UA) team in the \nmost recent Competition on Legal Information Extraction and Entailment (COLIEE \n2021). We participated in retrieval and entailment tasks for both case law and stat-\nute law; we applied a transformer-based approach for the case law entailment task, \nan information retrieval technique based on BM25 for legal information retrieval, \nand a natural language inference mechanism using semantic knowledge applied to \nstatute law texts. This competition included 25 teams from 14 countries; our case \nlaw entailment approach was ranked no. 4 in Task 2, the BM25 technique for legal \ninformation retrieval was ranked no. 3 in Task 3, and the natural language inference \ntechnique incorporating semantic information was ranked no. 4 in Task 4. The com-\nbination of the latter two techniques on Task 5 was ranked no. 2. We also performed \nerror analysis of our system in Task 4, which provides some insight into current \nstate-of-the-art and research priorities for future directions.\nKeywords Legal Information Extraction · Legal Information Entailment · BM25 · \nTransformers\nMathematics Subject Classification 68T50 · 68T07 · 68T05\n * Mi-Young Kim \n miyoung2@ualberta.ca\n Juliano Rabelo \n rabelo@ualberta.ca\n Kingsley Okeke \n nkokeke@ualberta.ca\n Randy Goebel \n rgoebel@ualberta.ca\n1 Department of Science, Augustana Faculty, University of Alberta, Camrose, AB, Canada\n2 Alberta Machine Intelligence Institute, University of Alberta, Edmonton, AB, Canada\n158 The Review of Socionetwork Strategies (2022) 16:157–174\n1 3\n1 Introduction\nTools to help legal professionals manage the increasing volume of legal docu-\nments are now essential. The volume of information produced in the legal sec-\ntor by its many different actors (e.g., law firms, law courts, independent attor -\nneys, legislators, and many other sources) is overwhelming. To help build a \nlegal research community, the Competition on Legal Information Extraction and \nEntailment (COLIEE) was created, to develop a research community that focuses \non four specific challenge problems in the legal domain: case law retrieval, case \nlaw entailment, statute law retrieval and statute law entailment. Here we provide \ndetails of our approaches for the legal information retrieval and legal text entail-\nment tasks.\nThe competition began in 2014, and completed its eighth edition this year. \nOver its history, initial techniques for open-domain textual entailment focused on \nexploiting shallow text features. But after eight years and competition and dis-\ncussion amongst many teams has evolved the choice of methods to include the \nusage of word embeddings, logical models and general machine learning. The \ncurrent state-of-the-art, especially for problems which have access to enough \nlabeled data, relies on deep learning-based approaches (more notably those based \non transformer methods), which have shown very good results in a wide range \nof textual processing benchmarks, including benchmarks specific to entailment \ntasks.\nOur method for the case law entailment task is based on adapting our methods \nfrom the past editions [1, 2], with an increased focus on transformer methods and a \nheuristic post-processing technique based on a priori probabilities. In this year, we \ndecided to drop similarity calculations, as our previous results have shown they did \nnot significantly contribute to improved performance. For the statute law tasks, we \napplied Opkapi Best Matching otherwise called “BM25,” for the retrieval task and a \ncombination of a transformer-based methods and exploitation of semantic informa-\ntion for the entailment tasks. In the future, we intend to further explore techniques to \ncapture semantic similarity and experiment with data augmentation methods.\nThe rest of this paper are organized as follows: in Sect. 2, we briefly review infor-\nmation retrieval (IR) and textual entailment. Section  3 describes our current meth-\nods and presents our results on both case law and statute law entailment tasks in \nCOLIEE 2021. Section 4 concludes the paper and comments on future work.\n2  Related Work\nTextual entailment, which is also called Natural Language Inference (NLI), is a logic \ntask in which the goal is to determine whether one sentence can be inferred from \nanother (more generally, whether one text segment can be inferred from another).\nIn the sentential case, the task consists of classifying an ordered pair of sen-\ntences into one of three categories: “positive entailment” occurs when one can \n159\n1 3The Review of Socionetwork Strategies (2022) 16:157–174 \nuse the first sentence to prove that a second sentence is true. Conversely, “nega-\ntive entailment” occurs when the first sentence can be used to disprove the second \nsentence. Finally, if the two sentences have no correlation, they are considered to \nhave a “neutral entailment.” In COLIEE, teams are challenged with the task of \ndetermining whether two case law textual fragments have a “positive entailment” \nrelationship or not (i.e., either “negative entailment” or “neutral entailment”). \nThe statute law entailment task (Task 4) in COLIEE is similarly designed: the \nparticipants are required to decide if a query is entailed from the texts of relevant \ncivil law statutes.\nIn the following subsections, we will discuss related research on textual entailment \nin general, and the specific techniques we have developed for case law entailment.\n2.1  Open‑Domain Textual Entailment\nTextual entailment can be viewed as an independent task per se or as a component in \nlarger applications. For example, question–answering systems may use textual entail-\nment to identify an answer from previously stored answer databases [3]. Textual entail-\nment may also be used to enhance document summarization (e.g., used to measure \nsentence connectivity or as additional features to summary generation [4]). Because of \ngrowing interest in textual entailment, there has been an increase in publicly available \nbenchmarks to evaluate such systems (e.g., [5, 6]).\nEarly approaches for open-domain textual entailment relied heavily on exploiting \nsurface syntax or lexical relationships, which have subsequently been elaborated with \na broader range of tools, such as word embeddings, logical models, graphical models, \nrule systems and machine learning [7]. A modern research trend for open-domain tex-\ntual entailment is the application of general deep learning models, such as ELMo [8], \nBERT [9] and ULMFit [10].\nThese methods build on the approach introduced by Dai and Le [11], which showed \nhow to improve document classification performance using unsupervised pre-training \nof an LSTM [12], followed by supervised fine-tuning for domain specific downstream \ntasks. The pre-training is typically done on very large datasets, which do not need to be \nlabeled and are intended to capture general language use knowledge like co-occurrence \nof words. This pre-training is usually formulated as a language modeling task. Subse-\nquently, supervised learning can be used as a fine-tuning step, thus requiring a labeled \nbut significantly smaller dataset, which aims to adjust the weights of the final layers of \nthe model suitable for a specific task. These models have achieved impressive results in \na wide range of publicly available benchmarks of different common natural language \ntasks, such as RACE (reading comprehension) [13] , COPA (common sense reasoning) \n[14] and RTE (textual entailment) [15], to name a few.\n2.2  Case Law Textual Entailment\nThe specific task of assessing textual entailment for case law documents is quite \nnew. The first COLIEE edition to include this task was in 2018 [16]. In that competi-\ntion, Chen et al. [17] proposed the application of association rules for the problem. \n160 The Review of Socionetwork Strategies (2022) 16:157–174\n1 3\nThey applied a machine learning-based model using Word2Vec embeddings [18] \nand Doc2Vec [19] as features. This approach faces two main problems: the lack of \nsufficient training data to make the models converge and generalize, and the compu-\ntational cost of training, which increases exponentially on the size of the dataset. To \novercome that issue, they proposed two association rule models: (1) the basic asso-\nciation rule model, which considers only the similarity between the source docu-\nment and the target document, and (2) the co-occurrence association rule model, \nwhich uses a relevance dictionary in addition to the basic model.\nAnother technique [20] worth mentioning approached the task as a binary clas-\nsification problem, and built feature vectors comprised of the measures of similarity \nbetween the candidate paragraph and (1) the entailed fragment of the base case, (2) \nthe base case summary and (3) the base case paragraphs (actually a histogram of \nthe similarities between each candidate paragraph and all paragraphs from the base \ncase). These feature vectors are the used as input to a Random Forest [21] classi-\nfier. To overcome the problem of severe data imbalance in the dataset, the dominant \nclass was under-sampled and the rarer class was over-sampled by SMOTE sample \nsynthesis [22].\nSince COLIEE 2019, techniques based on BERT or other transformer -based \nmodels have dominated the COLIEE case law entailment task. Rabelo et al. [1] pre-\nsent a method for case law entailment combining similarity-based features which \nrely on multi-word tokens instead of single words, and exploited the BERT frame-\nwork [9], fine-tuned to the task of case law entailment on the provided training data-\nset. In 2020, the task 2 winner [23] applied an approach which has a model cap-\nturing the supporting relation of a text pair, based on the BERT base model, then \nfine-tuned for a supporting text-pair classification task. The set of supporting text-\npairs includes the text-pairs from Task 1 candidate cases using designed heuristics, \nand the gold standard data of Task 2 (decision-paragraph). This system also uses \na BERT model fine-tuned on SigmaLaw (a law dataset described in [24]) for the \nmasked language modeling task. Together with scoring by the BERT models, lexi-\ncal matching (BM25) is also considered for predicting decision-paragraph entail-\nment. Other teams have used BERT to generate features that are then input to other \nclassifiers. For example, Alberts et al. [25] applied an Xgboost classifier with the \nfollowing features as input: NLI probability (bert-nli), similarity between entailed \nfragment and paragraphs based on fine-tuned BERT (bert-base-uncased), and BM25 \nsimilarity between entailed fragment and paragraphs. Those authors also submit-\nted runs using other features as input: n-grams, BM25, NLI, and EUR-LEX (81,000 \nsentences from EU legal documents) fine-tuned ROBERTA and BERT (bert-base-\nuncased) derived similarity features.\n2.3  Statute Law Textual Entailment\nNatural language inference (NLI), the task of identifying whether a hypothesis can be \ninferred from, contradicted by, or not related to a premise, has become one of the stand-\nard benchmark tasks for natural language understanding. NLI datasets are typically \nbuilt by asking annotators to compose sentences based on premises extracted from \n161\n1 3The Review of Socionetwork Strategies (2022) 16:157–174 \ncorpora, so that the composed sentences stand in entailment/contradiction/neutral rela-\ntionship to the premise [26]. In COLIEE 2021, we have two relationships that need to \nbe verified: entailment and non-entailment. Yang et al. [27] showed that human-created \nknowledge can further complement the use of pre-training models, to achieve better \nNLI prediction. Based on the results of Yang et al. [27], we have exploited the external \nknowledge of the Kadokawa thesaurus [28], to tackle Tasks 4 and 5.\nFor information retrieval, Shan et  al. [29] claimed that empirical studies showed \nglobal representative features like BM25 can capture term importance with global con-\ntext information. A word with a high BM25 score reveals its uniqueness in the corpus, \nand this method has been widely adopted in traditional learning to rank tasks.\n3  COLIEE 2021—Approaches and Results\nLegal question answering can be considered as a number of intermediate steps. For \ninstance, consider a question such as “The landowner shall have the owner of the adja-\ncent land repair or remove the obstacle if the owner of the adjacent land is damaging \nhis or her land due to the destruction or blockage of the drainage ditch installed in the \nadjacent land?” In this example, a system must first identify and retrieve relevant docu-\nments, typically legal statutes. It must then compare the semantic connections between \nthe question and the relevant legal statutes, and determine whether an entailment rela-\ntion holds.\nCOLIEE includes both retrieval and entailment tasks in two broad areas: case law \nand statute law. The case law retrieval task consists in determining which cases from \na pool should be “noticed” with respect to each base case in a given list. The entail-\nment task for case law consists in determining whether an entailment relationship exists \nbetween one or more paragraphs in a referenced case and a given fragment from a base \ncase. Note that the general idea is to identify these fragments as proxies for an overall \nlegal argument based on noticed cases, not that any single fragment is necessary and \nsufficient for a complete legal case argument.\nFor case law, the competition focuses on two aspects of legal information process-\ning: case law retrieval (Task 1), and case law entailment (Task 2). For statute law, the \ncompetition provides three aspects of legal information processing related to answer-\ning yes/no questions from legal bar exams: legal document retrieval (Task 3), natural \nlanguage inference (NLI) for Yes/No question answering of legal queries (Task 4), and \na combination of document retrieval and natural language inference (Task 5). Figure 1 \nshows the architectures of Tasks 3, 4 and 5.\nIn the next subsections, we present further details on the methods we have devel-\noped and applied in COLIEE 2021.\n162 The Review of Socionetwork Strategies (2022) 16:157–174\n1 3\n3.1  Case Law Entailment—Task 2\n3.1.1  Task Definition\nTask 2 is a legal case entailment task and it requires the identification of a paragraph \nfrom existing cases that can be claimed to entail the decision of a new case. Given a \ndecision Q of a new case and a relevant case R, the challenge is to identify a specific \nparagraph in R that entails the decision Q. The organizers have confirmed that the \nanswer paragraph cannot be identified merely by information retrieval techniques \nusing some examples. Because the case R is a relevant case to Q, and many para-\ngraphs in R could be relevant to Q, regardless of whether any one paragraph sup-\nports the required entailment. This task requires one to identify a paragraph which \nentails the decision of Q, so required is a specific entailment method that compares \nthe meaning of each paragraph in R and the decision in Q. The data are drawn from \nan existing collection of predominantly Federal Court of Canada case law docu-\nments. The evaluation measures are based on information retrieval measures: preci-\nsion, recall and F-measure.\n3.1.2  Approach\nThe main component of our case law entailment method applies BERT [9] by fine-\ntuning on the provided training dataset. BERT is a framework designed to pre-train \ndeep bidirectional representations by jointly conditioning on both left and right \nFig. 1  Architectures of Tasks 3, 4 and 5\n163\n1 3The Review of Socionetwork Strategies (2022) 16:157–174 \ncontexts in all layers. This leads to pre-trained representations which can be fine-\ntuned with only one additional output layer on downstream tasks, such as question \nanswering, language inference and textual entailment, but without requiring task-\nspecific modifications. BERT has been used to achieve very good results on other \nwell-known benchmarks, such as GLUE [6], MultiNLI [30] and MRPC [31].\nWe used a BERT model pre-trained on a large (general purpose) dataset (the goal \nbeing make it acquire general language “knowledge” 1) which can be fine-tuned on \nsmaller, specific datasets (the goal being to make it learn how to combine the previ-\nously acquired knowledge in a specific scenario). This makes BERT a good fit for \nthis task, since we do not have a large dataset available for training the model. Our \nBERT model is based on the HuggingFace uncased-BERT distribution (bert-base-\nuncased), then fine-tuned on the COLIEE training dataset for 3 epochs (remaining \nhyperparameters used as default), using input pairs of entailment fragment and can-\ndidate paragraph, then confirming whether or not there is an entailment relationship.\nWe encode each candidate paragraph and its corresponding entailed fragment. \nIf the tokenization step produces more than the 512 token limit, we apply another \ntransformer-based model [32] to generate a summary of the input text, and then pro-\ncess the pair again. Since the input text often includes text in French, we remove \nthose fragments by applying a simple language detection model 2 based on a naive \nBayes filter.\nThe fine-tuned model is then applied to the test dataset (with the same summari-\nzation model, when needed). The model predicts scores for the entailment and non-\nentailment classes, which are later used in post processing the results. The objective \nof the post-processing step is to add some context to the classification: the classi-\nfier itself only sees pairs of input candidate paragraphs and entailed fragments, so it \ncould easily output a high score for many of those candidates in the same case or not \nproduce any one with a high enough score for a different case. Whether those situ-\nations are potentially feasible, the priors show that usually there are very few actual \nentailing paragraphs in a case (by far, most of the cases only have one entailing para-\ngraph). So in the post-processing step, we establish limits for the maximum number \nof outputs allowed per case. At the same time, we know at least one paragraph is the \n“correct” answer. We also make use of that fact to expect that at least one paragraph \nshould be returned, but in this case, we do use an empirically determined minimum \nscore in an attempt to reduce number the of false positives.\nBecause pre-training influences how transformer-based models “understand” lan-\nguage, we decided to experiment with LegalBERT [33], which is a BERT model \nfine-tuned on legal corpora. Our assumption was that a model trained on a large \nlegal corpus would provide better results in a legal classification task such as the \ncase law entailment in COLIEE. The LegalBERT model was fine-tuned using the \nsame procedure described for the generic BERT model (please see above), but \n1 Calling the kind of representations learned by BERT (or any other transformer-based model) “knowl-\nedge” is a stretch and even some sort of anthropomorphization but seems to be appropriate in the context \nof machine “learning.”\n2 https:// pypi. org/ proje ct/ langd etect/.\n164 The Review of Socionetwork Strategies (2022) 16:157–174\n1 3\nthe final results produced were disappointing, with a very low f1-score in a vali-\ndation dataset. Despite these results, we intend to further explore this option in \nfuture editions of COLIEE as we understand pre-training transformer-based models \nusing same domain text have the potential to provide good results. The pre-trained \nLegalBERT model used in our experiments is available at HuggingFace (model id: \n’nlpaueb/legal-bert-base-uncased’).\nAs previously mentioned, in past editions, we tried to expand the training dataset \nthrough data augmentation techniques based on back translation (English to German \nto English) but that did not produce the expected improvements. We speculate that \nback-translation methods do not generate enough variability in the new examples \nand contribute only some slight perturbation around the existing data points. Nev -\nertheless, we intend to further explore the data augmentation idea in future editions, \nbut experiment with different techniques. One of the potential data augmentation \ntechniques would rely on the (larger) dataset provided for Task 1 (case law retrieval): \nwe intend to increase size of the training dataset by extracting simple examples of \nentailment relationship through hand written heuristic rules and adding those exam-\nples to the training set for Task 2. We hypothesize that simple text augmentation \ntechniques such as synonym replacement will not provide enough variation over the \nexisting data and do not intend to explore those options.\nThe official COLIEE 2021 results for this task are shown in Table 1. Our submis-\nsions were based on the fine-tuned BERT model with summarization enabled for long \nparagraphs and entailed fragments as detailed above. The difference between the sub-\nmissions are in the post-processing parameters: UA_reg_pp.txt applies a post-process-\ning which will keep at most one answer per case given its confidence score is at least \n-1. UA_def_pp.txt is similar but requires the minimum confidence score to be at least \nTable 1  Task 2 official results Team File F1\nNM Run_task2_DebertaT5.txt 0.6912\nNM Run_task2_monoT5.txt 0.6610\nNM Run_task2_Deberta.txt 0.6339\nUA UA_reg_pp.txt 0.6274\nJNLP JNLP.task2.BM25Sup._Den..txt 0.6116\nJNLP JNLP.task2.BM25Sup._Den._F..txt 0.6091\nUA UA_def_pp.txt 0.5875\nJNLP JNLP.task2.NFSP_BM25.txt 0.5868\nsiat siatCLS_result-task2.txt 0.5860\nDSSIR run_test_bm25.txt 0.5806\nsiat siatFGM_result-task2.txt 0.5670\nUA UA_loose_pp.txt 0.5603\nTR task2_TR.txt 0.5438\nDSSIR run_test_bm25_dpr.txt 0.5161\nDSSIR run_test_dpr.txt 0.5161\nMAN01 [MAN01] task2 run1.txt 0.5069\nMAN01 [MAN01] task2 run0.txt 0.2500\n165\n1 3The Review of Socionetwork Strategies (2022) 16:157–174 \n0. UA_loose_pp.txt also established 0 as the minimum score but allows for at most 2 \npredictions to be made for each base case.\n3.2  Statute Law Information Retrieval—Task 3\n3.2.1  Task Definition\nTask 3 requires the retrieval of an appropriate subset ( S1 , S2,..., Sn ) of Japanese Civil \nCode Articles from the Civil Code texts dataset, used for answering a Japanese legal \nbar exam question Q.\nAn appropriate subset means the identification of a subset of statutes for which an \nentailment system can judge whether the statement Q is true Entails(S1 ,S2 , ...,Sn ,Q)  \nor not Entails(S1 ,S2 , ...,Sn ,¬Q) .\n3.2.2  Approach\nThe key component of the probabilistic information retrieval (IR) model is to esti-\nmate the probability of relevance of the documents for a query. This is where most \nprobabilistic models differ from one another. A number of weighting formulae have \nbeen developed and BM25 [34] has, so far, been the most effective. The major dif-\nferences between BM25 and the other commonly used TFIDF models are the slight \nvariants of inverse document frequency (IDF) formulation and the use of the query \nterm frequency. TFIDF is computed as following:\nwhere D is a document, Q is a query, and t is a term in Q. Here, tf(t,  D) is t’s term \nfrequency in the document D, and idf(t) is the inverse document frequency of t.\nThe length normalization factor in BM25 uses the average document length and \na parameter has been introduced to control the relative length effect. A probabilis-\ntic language modeling technique [35, 36], is another effective ranking model that is \nwidely used. Typically, language modeling approaches compute the probability of \ngenerating a query from a document, assuming that the query terms are chosen inde-\npendently. Unlike TF-IDF models, language modeling approaches do not explicitly \nuse document length factor and the IDF component. It seems that the length of the \ndocument is an integral part of this formula and that automatically takes care of \nthe length normalization issue [37]. However, smoothing is crucial and it has very \nsimilar effect as the parameter that controls the length normalization components in \npivoted normalization or the BM25 model. Three major smoothing techniques (Dir -\nichlet, Jelinek-Mercer and Two-stage) are commonly used in this model [38], and \nwe use Dirichlet smoothing in our language model-based IR [36] for COLIEE 2021. \nWe use the following language model-based information retrieval formula:\ntﬁdf(D, Q)=\n�\nt\n[\n√\ntf(t,D)∗( 1 + log(idf(t)))2 ]\n̂p(Q/uni007C.varM d)=\n/uni220F.s1\nw∈Q\np(w/uni007C.varM d)∗\n/uni220F.s1\nw∉Q\n(1.0 − ̂p(w/uni007C.varM d))\n166 The Review of Socionetwork Strategies (2022) 16:157–174\n1 3\nHere Q is a query, d is a document, and M d is a language model of d. We would like \nto estimate ̂p(Q/uni007C.varMd) , the probability of the query Q given the language model of \ndocument d as shown in the equation above. For more details on each probability \nsuch as ̂p(w/uni007C.varMd) , see Ponte and Croft [36].\nBM25 is computed as following:\nwhere f(qi, D)  is q i ’s term frequency in the document D, |D| is the length of the doc-\nument D in words, and avgdl is the average document length in the text collection \nfrom which documents are drawn. K1 and b are free parameters. We used 1.5 for K1 \nand 0.75 for b. IDF (qi) is the IDF (inverse document frequency) weight of the query \nterm q i . It is usually computed as:\nwhere N is the total number of documents in the collection, and n(qi) is the number \nof documents containing q i\n3.\nThe legal IR task that we use to test our system has several sets of queries paired \nwith a subset of Japan civil law articles as documents (724 articles in total). Here \nfollows one example of the query and a corresponding relevant article:\nQuestion: Land owners can cut off the branches of bamboo trees in the neighbor-\ning land when they cross the border.\nRelated Article: Article 233 (1) When a branch of a bamboo tree in the adjacent \nland crosses the boundary line, the owner of the bamboo tree may cut the branch. \n(2) When a branch of a bamboo tree in the adjacent land crosses the boundary line, \nthe owner of the bamboo tree may cut the branch.\nBefore the final test set was released, we received 14 sets of queries for a “dry \nrun” in COLIEE 2021. The 14 sets of data include 806 queries, and 1040 relevant \narticles (average 1.29 articles per query). The metrics for measuring our IR model \nperformance is F2:\nTable 2 shows the results of experiments with our three IR models on the final test \nset in COLIEE 2021: BM25 (BM25.UA), TF-IDF (TFIDF.UA), and language-\nmodel-based IR (LM.UA). BM25 showed the best performance amongst the three \nmodels. The test data size is 81 queries for Task 3. The performance of our system \nwas ranked 3rd among the submitted systems in the Competition on Legal Informa-\ntion Extraction/Entailment (COLIEE) 2021.\nscore(D, Q)=\nn/uni2211.s1\ni=1\nIDF (qi) f(qi,D)∗( k1 + 1)\nf(qi,D)+ k1 ∗( 1 − b + b ∗ /uni007C.varD/uni007C.var\navgdl)\nIDF (qi)=ln\n/parenleft.s3N − n(qi)+0.5\nn(qi)+ 0.5 + 1\n/parenright.s3\nF 2 = 5 ∗ Precision∗ Recall\n4 ∗ Precision+ Recall\n3 https:// en. wikip edia. org/ wiki/ Okapi_ BM25.\n167\n1 3The Review of Socionetwork Strategies (2022) 16:157–174 \n3.3  Answering Yes/No Questions—Tasks 4 and 5\n3.3.1  Tasks Definition\nTask 4 is a task to determine textual entailment relationships between a given prob-\nlem sentence and relevant article sentences. Competitor systems should answer \n“yes” or “no” regarding the given problem sentences and given article sentences. \nTask 5 requires a system to answer “yes” or “no” given a problem sentence(s) only. \nParticipants can use any external data; however, this assumes that they do not use \nthe test dataset.\n3.3.2  Approach\nThe problem of answering a legal yes/no question can be viewed as a binary clas-\nsification problem. Assume a set of questions Q, where each question qi ∈ Q  is asso-\nciated with a list of corresponding article sentences ai1,ai2,…,aim , where yi = 1 if \nthe answer is ‘yes’ and yi = 0 otherwise. We choose the most relevant sentence a ij \nusing the algorithm of Kim et al. [2], and we simply treat each data point as a triple \n(qi, a ij, yi) . Therefore, our task is to learn a classifier over these triples so that it can \npredict the answers of any additional question–article pairs. BERT [9] has shown \ngood performance on the general natural language inference task. However, Jiang \nand Marnaffe [26] insisted that despite high F1 scores, BERT models have system-\natic error patterns, suggesting that they do not capture the full complexity of human \nTable 2  IR (Task3) results on \ntest run data in COLIEE 2021 Team F2 P R MAP R_5 R_10 R_30\nOvGU_run1 0.73 0.67 0.77 0.74 0.75 0.81 0.85\nJNLP.CLMLT 0.72 0.60 0.80 0.79 0.78 0.89 0.95\nBM25.UA 0.70 0.75 0.70 0.75 0.71 0.73 0.81\nJNLP.CLBJP 0.70 0.62 0.77 0.77 0.82 0.84 0.90\nR3.LLNTU 0.70 0.66 0.74 0.78 0.79 0.83 0.91\nR2.LLNTU 0.70 0.67 0.73 0.78 0.78 0.84 0.91\nR1.LLNTU 0.68 0.63 0.73 0.78 0.78 0.84 0.91\nJNLP.CLBJ 0.68 0.55 0.77 0.77 0.81 0.84 0.91\nOvGU_run2 0.67 0.48 0.80 0.75 0.75 0.81 0.90\nTFIDF.UA 0.65 0.67 0.65 0.73 0.72 0.74 0.81\nLM.UA 0.54 0.56 0.54 0.64 0.64 0.68 0.81\nTR_HB 0.52 0.33 0.61 0.66 0.71 0.74 0.84\nHUKB-3 0.52 0.29 0.69 0.61 0.68 0.74 0.87\nHUKB-1 0.47 0.23 0.65 0.61 0.66 0.75 0.87\nTR_AV1 0.35 0.26 0.51 0.46 0.43 0.47 0.56\nTR_AV2 0.33 0.14 0.55 0.43 0.39 0.44 0.49\nHUKB-2 0.32 0.32 0.32 0.41 0.46 0.54 0.61\nOvGU_run3 0.30 0.15 0.70 0.55 0.57 0.61 0.70\n168 The Review of Socionetwork Strategies (2022) 16:157–174\n1 3\npragmatic reasoning. To aid the pragmatic reasoning, our system incorporates the \nsemantic information into the BERT language model for natural language inference.\nThe entailment result based on the syntactic parser, article segmentation and \nnegation detection showed the best performance in COLIEE 2019. We will call this \napproach SYN. However, in COLIEE 2020, BERT showed better performance than \nSYN. In COLIEE 2021, we combine these two approaches to achieve synergy. We \nsee some cases where the prediction output of BERT is different from the output \nof SYN. To resolve the different prediction issue between the two systems, we use \nadditional information, which is semantic closeness. We add this semantic analysis \ncomponent using the syntactic parser of Kim et al. [40].\nTo measure semantic closeness, we use semantic category codes of the Kadokawa \nthesaurus corresponding to the content words in the input, as shown in Fig.  2. In \nCOLIEE 2021, we compare the entailment outputs between BERT and SYN . If they \nare the same, the entailment output is adopted with consensus. Otherwise, we check \nthe semantic codes of the Kadokawa thesaurus of the content words in the query and \nthe relevant article. Then we just apply the following simple heuristic rule:\n- If there are shared semantic codes in both of the following cases: (1) between \nthe conditions of the query and the article, and (2) between the conclusions of the \nquery and the article, then we choose the answer “yes.” Otherwise, the answer is \n“no.”\nFigure 3 shows the architecture of the Task 4 model.\nFollowing is one example:\nQuery: A, who acts(code:754,822) as the agent(code:552) of B, \nconcluded(code:448) a contract(code:448) with C for sale(code:742) of \nland(code:042) owned(code:379) by B. However, A did not possess the \nFig. 2  Kadokawa Thesaurus Hierarchy [39]\n169\n1 3The Review of Socionetwork Strategies (2022) 16:157–174 \nauthority(code:449) to conclude the contract(code:448). If B ratifies(code:444) \nthe contract(code:448) of sales, A is not liable(code:449) to C as an unauthorized \nagency(code:552). \nRelevant Article: A person(code:507) who concludes a contract(code:448) as an \nagent(code:552) of another person is liable(code:449) to the counterparty(code:505) \nfor the performance of the contract or compensation(code:375) for loss(code:744) \nor damage(code:744), as chosen by the counterparty, unless the person \nproves(code:418,817) the authority(code:449) to represent(code:552) or the \nprincipal(code:505) ratifies(code:444) the contract.\nFor the above example, the output of BERT was “yes,” but the output of SYN was \n“no.” So we then check if there are any content words that share the same semantic \ncode in the conditions of the query and the article. We do the same check for the \nconclusions. In this example, there are content words that share the same semantic \ncodes. Therefore, the output of our system will be “yes.”\nTable 3 shows the Task 4 results on test data in COLIEE 2021. In the table, UA_\nparser is the system combining pre-trained BERT fine-tuned on the SNLI 4 dataset \nand semantic code with the syntactic parser. We also used ELMo-based decompos-\nable attention trained on the SNLI dataset [41] (name: UA_Elmo), and RoBERTa \n[42] fine-tuned on the SNLI dataset (name: UA_dl). We did not achieve better per -\nformance when we fine-tuned models on COLIEE training data, so we fine-tuned \nour models on the SNLI dataset.\nUA_parser is the only system that incorporates the semantic information \n(Kadokawa thesaurus concept code), and it was ranked no. 4 in Task 4 of COLIEE \n2021.\nThe difference between Task 4 and Task 5 is whether or not the gold standard \nanswer for the relevant statutes is used. In Task 4, participants use the gold standard \nfor relevant statutes provided by the organizers, while in Task 5, participants use \nthe retrieved statutes using their own results of Task 3. Table  4 shows the results \nof the submitted systems in COLIEE 2021 for Task 5. When we submitted the \nTask 5 results, we did not know that our BM25 technique showed the best perfor -\nmance amongst our three submitted systems in Task 3. So, we chose the output of \nFig. 3  Architecture of Task 4\n4 https:// nlp. stanf ord. edu/ proje cts/ snli/\n170 The Review of Socionetwork Strategies (2022) 16:157–174\n1 3\na traditional TF-IDF technique for IR, and combined the IR output with our NLI \nsystems for Task 5 submission. Our system combining TF-IDF in IR (Task 3) + \nUA_parser (Task 4) was ranked no. 2. As future work, we will combine our NLI \napproach with our BM25 technique and see if it can improve our current Task 5 \nperformance.\nTable 3  NLI (Task 4) results on \ntest data Team Sid Correct Accuracy\nBaseLine 43/All 81 0.5309\nHUKB HUKB-2 57 0.7037\nHUKB HUKB-1 55 0.6790\nHUKB HUKB-3 55 0.6790\nUA UA_parser 54 0.6667\nJNLP JNLP.EC 51 0.6296\nJNLP JNLP.ECS 51 0.6296\nJNLP JNLP.EB 51 0.6296\nOVGU OVGU_run3 48 0.5926\nTR TR-Ensemble 48 0.5926\nTR TR-MTE 48 0.5926\nOVGU OVGU_run2 45 0.5556\nKIS KIS1 44 0.5432\nKIS KIS3 44 0.5432\nUA UA_elmo 44 0.5432\nKIS KIS2 43 0.5309\nUA UA_dl 43 0.5309\nTR TR_Electra 41 0.5062\nOVGU OVGU_run1 36 0.4444\nTable 4  Task 5 (IR+NLI) \nresults on test data in COLIEE \n2021\nTeam Sid Correct Accuracy\nBaseLine 43/All 81 0.5309\nJNLP JNLP.NFSP 49 0.6049\nUA UA_parser 46 0.5679\nJNLP JNLP.NMSP 45 0.5556\nUA UA_dl 45 0.5556\nTR TRDistillRoberta 44 0.5432\nKIS KIS_2 41 0.5062\nKIS KIS_3 41 0.5062\nUA UA_elmo 40 0.4938\nJNLP JNLP.task5.B_M 38 0.4691\nKIS KIS_1 35 0.4321\nTR TRGPT3Ada 35 0.4321\nTR TRGPT3Davinci 35 0.4321\n171\n1 3The Review of Socionetwork Strategies (2022) 16:157–174 \n3.4  Error analysis in Statute Law Entailment\nFrom unsuccessful instances in Task 4, we classified the error types as shown in \nTable 5. The biggest error arises, of course, from the paraphrasing problem. For \nexample, machines were not able to identify that “obligee” = “beneficiary.” One \ninteresting thing is that UA_elmo and UA_dl have many cases of negation-related \nerrors while UA_parser has only one case. We believe this is because the syntac-\ntic analyzer can correctly identify the boundary of the negation through syntactic \ndependency analysis. We can also see that many errors arise from the complex con-\nstraints in condition and conclusion. In addition, there are many cases belonging \nto the reference resolution error. For example, there is a query “A, who acts as the \nagent of B, concluded a contract with C for sale of land owned by B.”. In Task 4, \nmachines should be able to identify what A, B, and C are referring to in the rel-\nevant article. Currently, in analyzing this kind of input including the reference terms, \nsome errors occurred. At the current stage, we do not employ any specific reference \nresolution process that can deal with this kind of complicated input, but just rely on \nBERT and SYN  and get the final prediction outcome. If we do not have an appro-\npriate reference resolution process, we will not be able to consider that the system \nsemantically understands the input sentences. As future work, we need to figure out \nhow these reference terms can be correctly resolved, in order to get the correct pre-\ndiction outcome based on real understanding of the input sentences. The current \ndata samples that have the referring terms such as A, B, and C are hard to be under -\nstood by machines because these terms can be examples of the relevant article case. \nThis is an open challenge that we need to consider in future work.\n4  Conclusion\nWe explained our models for legal entailment and question answering in COLIEE \n2021. For the case law entailment task, our transformers-based system ranked 4th \nplace among all submissions (2nd among all teams). Our future work will include \nexploring combinations of complementary techniques as well as alternatives \nfor appropriate data augmentation for Task 2. We have experimented with data \nTable 5  Task 4 Error types Error type UA_parser UA_dl UA_elmo\nWrong analysis of condition 7 9 6\nWrong analysis of conclusion 1 1 1\nNegation detection error 1 5 9\nParaphrase detection error 15 15 16\nReference resolution error 1 3 3\nWrong analysis of conjunction 1 2 1\netc. 1 3 1\n172 The Review of Socionetwork Strategies (2022) 16:157–174\n1 3\naugmentation in the past but without much success (please see [2] for more details). \nHowever, we believe we can produce better results if we can find alternative data \nsources. For the statute law tasks, our BM25 system was ranked 3rd in Task 3, and \nour NLI system combining BERT and semantic information was ranked 4th in Task \n4 (we were the 2nd best team in that task) and 2nd in Task 5. As future research, we \nwill investigate methods to obtain semantic representation for paragraphs and per -\nform natural language inference between paragraphs.\nDeclarations \nConflict of interest On behalf of all authors, the corresponding author states that there is no conflict of \ninterest.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is \nnot permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission \ndirectly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen \nses/ by/4. 0/.\nReferences\n 1. Rabelo, J., Kim, M-Y., & Goebel, R. (2019). Combining similarity and transformer methods for case \nlaw entailment. In: Proceedings of the seventeenth international conference on artificial intelligence \nand law(Montreal, QC, Canada)(ICAIL ’19). Association for computing machinery, New York, NY, \nUSA, pp. 290–296\n 2. Rabelo, J., Kim, M-Y., & Goebel, R. (2020). Application of text entailment techniques in COLIEE \n2020. In JURISIN\n 3. Abacha, A. B., & Demner-Fushman, D. (2019). A question-entailment approach to question answer-\ning. CoRR abs/1901.08079 (2019). arXiv: 1901. 08079.\n 4. Lloret, E., Ferrández, Ó., Muñoz, R., & Palomar, M. (2008). A text summarization approach under \nthe influence of textual entailment. In: NLPCS -5th international workshop on natural language pro-\ncessing and cognitive science, pp. 22–31\n 5. Bowman, S.R., Angeli, G., Potts, C., Manning, C. D. (2015). A large annotated corpus for learning \nnatural language inference. In Proceedings of the conference on empirical methods in natural lan-\nguage processing. ACL\n 6. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S R. (2018). GLUE: A multi-task \nbenchmark and analysis platform for natural language understanding. CoRRabs/1804.07461. arXiv: \n1804. 07461\n 7. Androutsopoulos, I, & Malakasiotis, P. (2009). A survey of paraphrasing and textual entailment \nmethods. CoRR abs/0912.3747 (2009). arXiv: 0912. 3747\n 8. Matthew, E. (2018). Peters, Mark Neumann, Mohit Iyyer, Matt Gardner. ChristopherClark: Kenton \nLee, and Luke Zettlemoyer. Deep contextualized wordrepresentations. In: Proc. of NAACL\n 9. Devlin, J., Chang, M-W., Lee, K., & Toutanova, K. (2018). BERT:pre-training of deep bidirectional \ntransformers for language understanding. CoRRabs/1810.04805. arXiv: 1810. 04805\n 10. Howard, J., & Ruder, S. (2018). Fine-tuned language models for text classification. CoR -\nRabs/1801.06146. arXiv: 1801. 06146\n 11. Dai, A M., & Le, Q V. (2015). Semi-supervised sequence learning. CoRR. arXiv: 1511. 01432\n173\n1 3The Review of Socionetwork Strategies (2022) 16:157–174 \n 12. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), \n1735–1780. https:// doi. org/ 10. 1162/ neco. 1997.9. 8. 1735.\n 13. Lai, G., Xie, Q., Liu, H., Yang, Y., & Hovy, E. H. (2017). RACE: Large-scale ReAding comprehen-\nsion dataset from examinations. CoRRabs/1704.04683. arXiv: 1704. 04683\n 14. Roemmele, M., Bejan, C., & Andrew G. (2011). Choice of plausible alternatives: An evaluation of \ncommonsense causal reasoning. In: AAAI Spring Symposium Series\n 15. Dagan, I., Glickman, O., & Magnini, B. (2005). The PASCAL recognising textual entailment chal-\nlenge. In ML challenges workshop. Springer, pp. 177–190\n 16. Kano, Y., Kim, M-Y., Yoshioka, Mas., Lu, Y., Rabelo, J., Kiyota, N., Goebel, R., & Satoh, K. \n(2018). COLIEE-2018: evaluation of the competition on legal information extraction and entail-\nment. In: 12th International workshop on juris-informatics\n 17. Chen, Y., Zhou, Y., Zhen, L., Sun, H., & Yang, W. (2018). In Twelfth international workshop on \njuris-informatics: legal in-formation retrieval by association rules\n 18. Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013). CoRR: Distributed representa-\ntions of words and phrases and their compositionality\n 19. Le, Q V., & Mikolov, T. (2014). Distributed representations of sentences and documents. CoR -\nRabs/1405.4053. arXiv: 1405. 4053\n 20. Rabelo, J., Kim, M-Y., Babiker, H., Goebel, R., & Farruque, N. (2018). Legal information extraction \nand entailment for statute lawand case law. In: Twelfth international workshop on juris-informatics \n(JURISIN)\n 21. Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5–32.\n 22. Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). SMOTE: synthetic minor-\nity over-sampling technique. The Journal of Artificial Intelligence Research, 16(1), 321–357.\n 23. Nguyen, H-T., Thi Vuong, H-Y., Nguyen, P M., Dang, B T., Bui, Q M., Vu, S T., Nguyen, C \nM., Tran, V., Satoh, K., Nguyen, M L. (2020). JNLP team: deep learning for legal processing in \nCOLIEE 2020, COLIEE\n 24. Sugathadasa, K., Ayesha, B., de Silva, N., Perera, A S., Jayawardana, V., Lakmal, D., Perera, M. \n(2017). Synergistic Union of Word2Vec and lexicon for domain specific semantic similarity. IEEE \ninternational conference on industrial and information systems (ICIIS)\n 25. Alberts, H., Ipek, A., Lucas, R., Wozny, P. (2020). COLIEE 2020: Legal information retrieval and \nentailment with legal embeddings and boosting, COLIEE\n 26. Jiang, N., de Marneffe, M C. (2019). Evaluating BERT for natural language inference: A case study \non the CommitmentBank. In: Proceedings of the 2019 conference on empirical methods in natu-\nral language processing and the 9th international joint conference on natural language processing \n(EMNLP-IJCNLP). pp. 6088–6093\n 27. Yang, X., Zhu, X., Zhao, H., Zhang, Q., & Feng, Y. (2019). Enhancing unsupervised pretraining \nwith external knowledge for natural language inference. In Proceeding of the Canadian conference \non artificial intelligence. Springer, pp. 413–419\n 28. Ohno, S., & Hamanishi, M. (1981). MNew synonyms dictionary. Tokyo: Kadogawa Shoten.\n 29. Shan, X., Liu, C., Xia, Y., Chen, Q., Zhang, Y., Ding, K., Liang, Y., Luo, A., & Luo, Y. (2020). \nGLOW : global weighted self-attention network for web search. arXiv: 2007. 05186\n 30. Williams, A., Nangia, N., & Bowman, S R. (2017). A broad-coverage challenge corpus for sentence \nunderstanding through inference. CoRRabs/1704.05426. arXiv: 1704. 05426\n 31. Dolan, William B., & Brockett, C. (2005). Automatically constructing a corpus of sentential para-\nphrases. In: Proceeding of the 3rd international workshop on paraphrasing\n 32. Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., & Zet-\ntlemoyer, L. (2019). BART: Denoising sequence-to-sequence pre-training for natural language gen-\neration, translation, and comprehension. arXiv preprint arXiv: 1910. 13461\n 33. Chalkidis, I., Fergadiotis, M., Malakasiotis, P., Aletras, N., Androutsopoulos, I. (2020). LEGAL-\nBERT: the muppets straight out of law school. In Findings of the association for computational \nlinguistics: EMNLP. Association for computational linguistics, Online, 2898–2904. https:// doi. org/ \n10. 18653/ v1/ 2020. findi ngs- emnlp. 261\n 34. Zaragoza, H., & Robertson, S. (2009). The probabilistic relevance framework: BM25and beyond. \nIn: Found. Trends Inf. Retr, pp. 333–389\n 35. Robertson, S., Zaragoza, H., & Hiemstra, D. (2004). A language modeling approach to information \nretrieval. In: Parsimonious language models for information retrieval, pp.178–185.\n174 The Review of Socionetwork Strategies (2022) 16:157–174\n1 3\n 36. Ponte, J M., & Croft, W B. (1998). A language modeling approach to information retrieval. In Pro-\nceedings of the 21st annual international ACM SIGIR conference on Research and development in \ninformation retrieval, pp. 275–281\n 37. Paik, J H. (2013). A novel TF-IDF weighting scheme for effective ranking. In: Proceedings of the \n36th international ACM SIGIR conference on research and development in information retrieval, \npp. 343–352.\n 38. Lafferty, J., & Zhai, C. (2004). A study of smoothing methods for language models applied to infor-\nmation retrieval. In: ACM Transactions on Information and Systems, pp. 179–214\n 39. Kang, S-J., & Lee, J-H. (2001). Semi-automatic practical ontology construction by using a thesau-\nrus. In: Proceedings of the ACL 2001 workshop on human language technology and knowledge \nmanagement, pp. 413–419.\n 40. Kim, M-Y., Kang, S-J., & Lee, J-H. (2001). Resolving ambiguity in inter-chunk dependency pars-\ning. In: Proceedings of 6th natural language processing pacific rim symposium, pp. 263–270\n 41. Parikh, A P., Oscar, T., Dipanjan, D., & Jakob, U. (2016). A decomposable attention model for natu-\nral language inference. arXiv preprint arXiv: 1606. 01933\n 42. Liu, Y., Myle, O., Naman, G., Jingfei, D., Mandar, J., Danqi, C., Omer, L., Mike, L., Luke, Z., & \nVeselin S. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv: \n1907. 11692\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published \nmaps and institutional affiliations."
}