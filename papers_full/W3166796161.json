{
  "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers",
  "url": "https://openalex.org/W3166796161",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2917519234",
      "name": "Irie, Kazuki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222000348",
      "name": "Schlag, Imanol",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222000349",
      "name": "Csordás, Róbert",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221898969",
      "name": "Schmidhuber, Jürgen",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Csord\\'as, R\\'obert",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Schmidhuber, J\\\"urgen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W185620388",
    "https://openalex.org/W2964138017",
    "https://openalex.org/W1662842982",
    "https://openalex.org/W1526844937",
    "https://openalex.org/W2103581399",
    "https://openalex.org/W2414711238",
    "https://openalex.org/W1581407678",
    "https://openalex.org/W3205643961",
    "https://openalex.org/W2981037730",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W2992155277",
    "https://openalex.org/W1757796397",
    "https://openalex.org/W2797278964",
    "https://openalex.org/W3129421149",
    "https://openalex.org/W2016760105",
    "https://openalex.org/W2963393721",
    "https://openalex.org/W3123673616",
    "https://openalex.org/W2964043796",
    "https://openalex.org/W3123651166",
    "https://openalex.org/W2166519220",
    "https://openalex.org/W3134307371",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3038058348",
    "https://openalex.org/W3184258323",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2962911926",
    "https://openalex.org/W2535697732",
    "https://openalex.org/W3035435378",
    "https://openalex.org/W2963403143",
    "https://openalex.org/W2993446282",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W3212935550",
    "https://openalex.org/W2136848157",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W3118210634",
    "https://openalex.org/W2963005248",
    "https://openalex.org/W2885062394",
    "https://openalex.org/W2963985863",
    "https://openalex.org/W2963059228",
    "https://openalex.org/W2089217417",
    "https://openalex.org/W1658008008",
    "https://openalex.org/W3013618273",
    "https://openalex.org/W2962847657",
    "https://openalex.org/W2970909667",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2950527759",
    "https://openalex.org/W2145339207",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2980077985",
    "https://openalex.org/W2963168530",
    "https://openalex.org/W3169291081",
    "https://openalex.org/W3120885796",
    "https://openalex.org/W2906841252",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1982479097",
    "https://openalex.org/W3123386591",
    "https://openalex.org/W2943845043",
    "https://openalex.org/W2952744660",
    "https://openalex.org/W1980658026",
    "https://openalex.org/W2970555085",
    "https://openalex.org/W3156619466",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W2996037775",
    "https://openalex.org/W3120947768",
    "https://openalex.org/W2784570041",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W1535810436",
    "https://openalex.org/W3034956653",
    "https://openalex.org/W3121592593",
    "https://openalex.org/W3166702123"
  ],
  "abstract": "Transformers with linearised attention (''linear Transformers'') have demonstrated the practical scalability and effectiveness of outer product-based Fast Weight Programmers (FWPs) from the '90s. However, the original FWP formulation is more general than the one of linear Transformers: a slow neural network (NN) continually reprograms the weights of a fast NN with arbitrary architecture. In existing linear Transformers, both NNs are feedforward and consist of a single layer. Here we explore new variations by adding recurrence to the slow and fast nets. We evaluate our novel recurrent FWPs (RFWPs) on two synthetic algorithmic tasks (code execution and sequential ListOps), Wikitext-103 language models, and on the Atari 2600 2D game environment. Our models exhibit properties of Transformers and RNNs. In the reinforcement learning setting, we report large improvements over LSTM in several Atari games. Our code is public.",
  "full_text": "Going Beyond Linear Transformers\nwith Recurrent Fast Weight Programmers\nKazuki Irie1∗, Imanol Schlag1∗, Róbert Csordás1, Jürgen Schmidhuber1,2\n1The Swiss AI Lab, IDSIA, University of Lugano (USI) & SUPSI, Lugano, Switzerland\n2King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia\n{kazuki, imanol, robert, juergen}@idsia.ch\nAbstract\nTransformers with linearised attention (“linear Transformers”) have demonstrated\nthe practical scalability and effectiveness of outer product-based Fast Weight\nProgrammers (FWPs) from the ’90s. However, the original FWP formulation is\nmore general than the one of linear Transformers: a slow neural network (NN)\ncontinually reprograms the weights of a fast NN with arbitrary architecture. In\nexisting linear Transformers, both NNs are feedforward and consist of a single\nlayer. Here we explore new variations by adding recurrence to the slow and fast\nnets. We evaluate our novel recurrent FWPs (RFWPs) on two synthetic algorithmic\ntasks (code execution and sequential ListOps), Wikitext-103 language models,\nand on the Atari 2600 2D game environment. Our models exhibit properties of\nTransformers and RNNs. In the reinforcement learning setting, we report large\nimprovements over LSTM in several Atari games. Our code is public.1\n1 Introduction\nThe Transformer [1] has become one of the most popular neural networks (NNs) for processing\nsequential data. Its success on neural machine translation quickly transferred to other problems in\nnatural language processing, such as language modelling [2, 3] or question answering [4]. Recently,\nit has also been applied in other domains, including image processing [5, 6] or mathematical problem\nsolving [7, 8, 9].\nConceptually, the Transformer is a deep feedforward NN that processes all elements of a sequence in\nparallel: unlike in recurrent NNs (RNNs), the computations of a layer for the entire sequence can be\npacked into one big matrix multiplication. This scales well with the number of parallel processors.\nDespite the beneﬁts of parallelisation, a major drawback of Transformers is that their computational\ncomplexity in time and space is quadratic in sequence length. Furthermore, in the auto-regressive\nversion [1, 2] — the focus of our work — the state size increases linearly with sequence length. This\nmakes Transformers infeasible for auto-regressive settings dealing with very long or potentially inﬁ-\nnite sequences, forcing practitioners to truncate temporal contexts and ignore long-term dependen-\ncies beyond ﬁxed-size time windows. Although recent work tries to address this issue [10, 11], this\nlimitation makes some applications of Transformers challenging, e.g., reinforcement learning (RL)\nin partially observable environments [12, 13], which is still dominated by RNNs such as the Long\nShort-Term Memory (LSTM; [14]) trained by policy gradients [15, 16, 17, 18].\nTo scale Transformers to longer sequences, recent works have proposed to linearise the softmax in\nthe self-attention computation and reorganise the latter in a sequential way [19]. Such models include\n∗Equal contribution.\n1https://github.com/IDSIA/recurrent-fwp\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2106.06295v2  [cs.LG]  26 Oct 2021\nKatharopoulos et al.’sLinear Transformer (LT) [19], Choromanski et al.’sPerformer [20] and Peng\net al. [21]’s variant. They enjoy time and space complexities linear in sequence length with states\nof constant size. While their performance on some tasks does not fully match the one of regular\nTransformers [22], several improvements have already been proposed [21, 23] (see our review in\nSec. 2.2) which makes this Transformer family a promising alternative.\nHere we go one step further in advancing linear Transformer variants as powerful auto-regressive\nsequence processing models, adopting the perspective of “Fast Weight Programmers” (FWPs) [24, 25,\n26]. Recent work emphasised that linearised Transformers are essentially equivalent to outer product-\nbased FWPs from the ’90s ([23]; reviewed in Sec. 2). Here we explore this connection further and\ndescribe more powerful FWPs.\nThe original FWP [24] is a two-NN system: a slow and a fast net, each with arbitrary architectures.\nThe slow net learns to generate rapid context-dependent weight modiﬁcations for the fast net. In the\ncase of existing linear Transformer variants, the slow and fast nets are simple one layer feedforward\nNNs. Here we augment them with recurrent connections to obtain recurrent FWPs (RFWPs).\nRecurrence enhances the model’s theoretical power [27] and can help to solve tasks that naturally\nrequire recurrence as a part of the solution.\nOur experiments on the language modelling dataset Wikitext-103 [ 28] show that our RFWPs are\ncompetitive compared to regular Transformers. We then study various properties of the proposed\nmodels on two synthetic algorithmic tasks: code execution [29] and sequential ListOps [30]. Finally,\nit is straightforward to apply our models to RL problems as a drop-in replacement for LSTMs. Here\nour RFWPs obtain large improvements over LSTM baselines across many Atari 2600 2D game\nenvironments [31]. Although LSTM still works better in a few environments, we show that our\nRFWPs generally improve by scaling them up.\nThe main contribution of this work is twofold: (1) from the perspective of FWPs, we study novel\npowerful FWPs for sequence processing, demonstrating that NNs can easily learn to control NNs\nthat are more complex than a single feedforward layer, and (2) from the perspective of Transformer\nmodels, our RFWPs augment linear Transformers with recurrence, addressing general limitations of\nexisting auto-regressive Transformer models.\n2 Background on Fast Weight Programmers (FWPs)\nHere we review the general concept of FWPs, as well as two speciﬁc instances thereof: the linear\nTransformer [19, 20] and the Delta Net [23].\n2.1 General Formulation\nWe refresh the concept of fast weight controllers or FWPs [24, 25] using modern notation in a sequence\nprocessing scenario. An FWP with trainable parameters θslow sequentially transforms an input\nsequence {x(t)}T\nt=1 with x(t) ∈Rdin to an output sequence{y(t)}T\nt=1 with y(t) ∈Rdout of length T as\nθ(t)\nfast,q(t) = SlowNet\n(\n{x(j)}t\nj=1,{y(j)}t−1\nj=0,{θ(j)\nfast}t−1\nj=0,{q(j)}t−1\nj=0; θslow\n)\n(1)\ny(t) = FastNet({q(j)}t\nj=1,{y(j)}t−1\nj=0; θ(t)\nfast) (2)\nwhere y(0), θ(0)\nfast , and q(0) are initial variables. This is a system with two NNs called FastNet and\nSlowNet in which the parametersθ(t)\nfast of FastNet are generated by SlowNet at each time stept. The\nweights of the fast net are fast in the sense that they may rapidly change at every step of the sequence\nwhile the weights of the slow netθslow are slow because they can only change through gradient descent\nduring training, remaining ﬁxed afterwards2. Eq. 1 expresses a slow NN in its general form. The slow\nnet can generate fast weights conditioned on various variables, depending on architectural choices\nfor the slow and fast NNs. In addition to the fast weights θ(t)\nfast, the slow net also generates or invents\nan input q(t) to be fed to the fast net (alternatively q(t) can simply be x(t)). While the architectures\nof slow and fast nets are arbitrary, they are typically chosen to be differentiable such that the entire\nFWP can be trained in an end-to-end manner using gradient descent. By interpreting the weights of\n2The fast net could also contain some additional slow weights; we omit this possibility here.\n2\nan NN as a program [32], the slow net effectively learns to control, or program, the fast NN. Thus,\nthe slow net is a neural programmer of fast weights, and its parameter set θslow embodies compressed\ninformation used to produce potentially inﬁnite variations of context-dependent fast weights.\nIn many settings, it makes sense to generate the fast weightsθ(t)\nfast incrementally in an iterative fashion,\nwhere the SlowNet is further decomposed into two sub-parts:\nz(t),q(t) = SlowSubnet({x(j)}t\nj=1,{y(j)}t−1\nj=0,{θ(j)\nfast}t−1\nj=0,{q(j)}t−1\nj=0,{z(j)}t−1\nj=0; θslow) (3)\nθ(t)\nfast = UpdateRule(θ(t−1)\nfast ,z(t)) (4)\nwhere UpdateRule takes the fast weights θ(t−1)\nfast from the previous iteration to produce the new\nfast weights θ(t)\nfast conditioned on z(t). The update rule is essentially the differentiable elementary\nprogramming instruction used by the FWP. In the next section we review concrete examples of recent\nFWPs.\n2.2 Linear Transformers as Fast Weight Programmers\nIn general, the dimension of the fast weights θ(t)\nfast is too large to be conveniently parameterised by an\nNN. Instead, it was proposed in 1991 [24] to perform a rank-one update via the outer product of two\nvectors generated by the slow net. Two recent models directly correspond to such outer product-based\nFWPs: linear Transformers [19] and the Delta Net [23].\nLinear Transformer. The “linear Transformer” [19] is a class of Transformers where the softmax in\nthe attention is linearised. This is achieved by replacing the softmax with a kernel functionφ—then the\nself-attention can be rewritten as a basic outer product-based FWP [24, 23]. Previous works focused\non different φmaps with properties such as increased capacity [23] or guaranteed approximation of the\nsoftmax in the limit [20, 21]. For our purposes, the particular choice of φis irrelevant and we simply\nassume φ : Rdkey →Rdkey , simplifying our equations below by writing k,qinstead of φ(k),φ(q).\nUsing otherwise the same notation as above, for each new input x(t), the output y(t) is obtained by:\nk(t),v(t),q(t) = Wkx(t),Wvx(t),Wqx(t) (5)\nW(t) = W(t−1) + v(t) ⊗k(t) (6)\ny(t) = W(t)q(t) (7)\nwhere the slow weight matrices Wk ∈Rdkey×din and Wv ∈Rdout×din are used to obtain the key\nk(t) ∈Rdkey and the value v(t) ∈Rdout . The key and value vectors are used to generate new weights\nvia the outer product v(t) ⊗k(t) ∈Rdout×dkey . A further simpliﬁcation in the equations above is the\nomission of attention normalisation which has been experimentally shown to be unnecessary if the φ\nfunction produces normalised key and query vectors [23].\nIn Eq. 6, the previous fast weight matrix W(t−1) ∈Rdout×dkey is updated to yield W(t) by adding the\nupdate term v(t) ⊗k(t). This corresponds to the sum update rule or purely additive programming\ninstruction. Here the fast NN is a simple linear transformation as in Eq. 7 which takes as input\nthe query vector q(t) ∈Rdkey generated by the slow weights Wq ∈Rdkey×din . Hence, in linear\nTransformers, the previous Eq. 3 simpliﬁes to: z(t),q(t) = SlowSubnet(x(t); θslow) with z(t) =\n(k(t),v(t)).\nDelta Net. The Delta Net [23] is obtained by replacing the purely additive programming instruction\n(Eq. 6) in the linear Transformer with the one akin to the delta rule [33]:\nW(t) = W(t−1) + β(t)(v(t) −¯v(t)) ⊗k(t) (8)\nwhere β(t) ∈R is a fast parameter (learning rate) of the update rule generated by the slow net with\nweights Wβ ∈R1×din and the sigmoid function σ:\nβ(t) = σ(Wβx(t)) (9)\nand ¯v(t) ∈Rdout is generated as a function of the previous fast weights W(t−1) and the key k(t)\n¯v(t) = W(t−1)k(t). (10)\n3\nThis update rule was introduced to address a memory capacity problem affecting linear Trans-\nformers with the purely additive update rule [ 23]. The corresponding Eq. 3 is: z(t),q(t) =\nSlowSubnet(x(t),W(t−1); θslow) with z(t) = (k(t),v(t),β(t),¯v(t)). Thus, unlike linear Transform-\ners, the SlowNet in the Delta Net takes the previous fast weights W(t−1) into account to generate\nthe new fast weight updates.\nWe typically use the multi-head version [1] of the computations above. After the projection (Eq. 5),\nthe vectors k(t), v(t), q(t) are split into equally sized H sub-vectors, and the rest of the operations\nare conducted by Hcomputational heads independently. The resulting output vectors from each head\nare concatenated to form the ﬁnal output.\nOther approaches. While our focus here is on outer product-based weight generation, which is\nan efﬁcient method to handle high dimensional NN weights, there are also other approaches. For\nexample, instead of generating a new weight matrix, Hypernetworks [34] scale the rows of a slow\nweight matrix with a generated vector of appropriate size. Weight compression to control fast weights\nin a low dimensional compressed space has been also studied [ 35]. In the broad sense of context-\ndependent weights [36, 37, 38], many concepts relate to FWPs: e.g. dynamic convolution [39, 40, 41],\nLambdaNetworks [42], or dynamic plasticity [43, 44].\n3 Fast Weight Programmers With Slow or Fast RNNs\nThe original formulation of FWPs reviewed in Sec. 2.1 is more general than existing models presented\nin Sec. 2.2. In particular, both fast and slow networks in existing linear Transformers consist of a\nsingle feedforward layer (Eqs. 5 and 7). Here we present FWPs with recurrent fast nets in Sec. 3.1\nand FWPs with recurrent slow nets in Sec. 3.2.\n3.1 Fast Network Extensions\nIn principle, any NN architecture can be made fast. Its fast weight version is obtained by replacing\nthe networks’ weights with fast weights parameterised by an additional slow network. For example,\nconsider a regular RNN layer with two weight matrices W and R:\nh(t) = σ(Wx(t) + Rh(t−1)) (11)\nA fast weight version can be obtained by replacing W and R with W(t) and R(t) which are\ncontrolled as in Eq. 8 with all necessary variables generated by a separate slow net at each time step t.\nWhile this view illustrates the generality of FWPs, the angle under which we approach these models\nis slightly different: we introduce recurrence as a way of augmenting existing linear Transformers.\nDelta RNN. We obtain a fast weight RNN called Delta RNN by adding an additional recurrent\nterm to the feedforward fast net of the linear Transformer (Eq. 7):\ny(t) = W(t)q(t) + R(t)f(y(t−1)) (12)\nwhere R(t) ∈Rdout×dout is an additional fast weight matrix which introduces recurrent connections.\nIt is also generated by the slow net using the delta update rule, similar to W(t) in Eq. 8 but with\nadditional slow weights. We apply an element-wise activation functionf to the previous output of\nthe fast network y(t−1) to obtain the recurrent query. The choice of activation function is crucial\nhere because, to achieve stable model behaviour, the elements in key and query vectors should be\npositive and sum up to one when the delta update rule is used [ 23]. We use the softmax function\n(f = softmax in Eq. 12) to satisfy these conditions. An ablation study on the choice of using Eq. 12\ninstead of the one similar to Eq. 11 can be found in Appendix A.2.\nAnalogous to the Delta RNN, we also construct a Delta LSTM with six fast weight matrices. The\nexact equations can be found in Appendix A.2.\nAlternative Feedforward Fast Nets. While the focus of this work is on RNNs, there are also\ninteresting fast feedforward models to be used in Eq. 7 which might result in stronger feedforward\n4\nbaselines. For example, we can replace the single layer fast net of Eq. 7 by a K-layer deep network:\nh(t)\nk = W(t)\nk f(h(t)\nk−1) for k∈[1..K] with h(t)\n0 = q(t) (13)\ny(t) = h(t)\nK (14)\nwhere the slow network produces all K fast weights {W(t)\nk }K\nk=1 and query q(t) from a single\ninput x(t). In light of the capacity limitation in linear Transformers [ 23], this might introduce\nadditional capacity without the need of larger representations, analogous to the trade-off in a multilayer\nperceptron (MLP) between narrow & deep versus shallow & wide. We refer to this class of models as\nDelta MLPs. Again, for stable model behaviour with the delta rule, we apply the softmax activation\nf to the vectors to be used as a query.\nAnother interesting approach is to use a Delta Net itself as a fast net, i.e., make the slow weights\nin the Delta Net fast (thus obtaining a Delta Delta Net). Such a model could in principle learn to\nadapt the way of generating fast weights depending on the context. While we plan to investigate the\npotential of such hierarchical FWPs in future work, we also include preliminary results of such a\nmodel in our language modelling experiments (Sec. 4.1). A discussion on the dimensionality of such\na model can also be found in Appendix A.3.\nWe experimentally demonstrate that (slow) NNs can learn to control the weights of these rather\ncomplex fast networks (Sec. 4).\n3.2 Slow Network Extensions\nIn linear Transformers, the slow network is purely feedforward (Eq. 5). It can be made recurrent at\ntwo different levels: within the slow network (i.e. the slow network computes weight updates based\non its own previous outputs e.g., key, value, query vectors) or via the fast network by taking the fast\nnet’s previous output as an input. In our preliminary experiments, we found the former to be sub-\noptimal (at least in language modelling experiments). So we focus on the latter approach: we make\nthe slow net in the Delta Net dependent on the previous output of the fast network. We refer to this\nmodel as the Recurrent Delta Net (RDN).\nRecurrent Delta Net. We obtain the RDN by modifying the generation of key, value, and query\nvectors (Eq. 5) as well as the learning rate (Eq. 9) in the Delta Net. We add additional slow weights\n(Rk,Rq ∈Rdkey×dout , Rv ∈Rdout×dout , and Rβ ∈R1×dout ) for recurrent connections which connect\nthe previous output of the fast net y(t−1) (Eq. 7) to the new k(t), v(t), q(t), and β(t) as follows:\nk(t) = Wkx(t) + Rktanh(y(t−1)) (15)\nv(t) = Wvx(t) + Rvtanh(y(t−1)) (16)\nq(t) = Wqx(t) + Rqtanh(y(t−1)) (17)\nβ(t) = σ(Wβx(t) + Rβtanh(y(t−1))) (18)\nWhile the rest of the model remains as in the Delta Net, with these simple extra recurrent connections\nthe model becomes a proper RNN. The corresponding dependencies in Eq. 3 are: z(t),q(t) =\nSlowSubnet(x(t),y(t−1),W(t−1); θslow) with z(t) = (k(t),v(t),β(t),¯v(t)).\n3.3 Related Models\nAll the RFWP models presented in Sec. 3.1 and 3.2 can be seen as a type of memory augmented\nrecurrent neural networks [45, 46] in the sense that they maintain two-dimensional fast weight states\nas a short-term memory, in addition to the standard one-dimensional RNN states.\nThere are also several previously proposed recurrent fast weight models. For example, Schmidhuber’s\nrecurrent FWP from 1993 [26] has been revisited by Ba et al. [47]. There, key and value vectors are\nnot generated within the same time step, unlike in our models or in linear Transformers. The Fast\nWeight Memory (FWM) [48] is also a recurrent FWP: the slow net is an LSTM and the fast net is a\nhigher-order RNN. However, the FWM is a single pair of slow and fast nets, and a multi-layer version,\nas in the linear Transformer family, was not explored. Similarly, the Metalearned Neural Memory\n[49] uses an LSTM as its slow net and a 3-layer MLP as its fast net but again limited to one pair.\n5\nTable 1: WikiText-103 language model perplexity results with thesmall setting [21, 23]. For each\nmodel, its name, corresponding slow and fast networks, and weight update rule (Update) are speciﬁed.\nAll models are trained and evaluated on the span of 256 tokens except for the models in the last two\nrows (+ full context) which are trained and evaluated without context truncation. Parameter count is\nin millions. See Appendix A for further experimental details and results.\nName Slow net Update Fast net Valid Test #Prms\nTransformer - - - 33.0 34.1 44.0\nLinear Transformer Feedforward sum Linear 37.1 38.3 44.0\nDelta Net delta 34.1 35.2 44.0\nDelta MLP Feedforward delta Deep MLP 35.8 36.8 44.3\nDelta Delta Net Delta Net 34.0 35.2 44.6\nDelta RNN RNN 33.8 35.0 44.6\nDelta LSTM LSTM 32.6 33.8 47.3\nRDN Recurrent Linear 34.1 35.2 44.1\nDelta RNN + full context 31.8 32.8 44.6\nRDN 32.5 33.6 44.1\nOthers have investigated variants of RNNs with fast weights for toy synthetic retrieval tasks [50, 51].\nIn particular, Keller et al. [51] augment the LSTM with a fast weight matrix in the cell update. In\ncontrast, we make all weights in the LSTM fast and, importantly, our model speciﬁcations build upon\nthe successful deep Transformer architecture using residual connections [52, 53], layer-norm [54],\nmultiple attention heads and feed-forward blocks [1]. Essentially, we replace the self-attention layers\nin the regular Transformers by the fast weight programmer operations described above.\n4 Experiments\nWe conduct experiments in four different settings. We start by evaluating all models on a language\nmodelling task (Sec. 4.1) to obtain a performance overview and to discuss computational costs.\nLanguage modelling is an excellent task to evaluate sequence models. However, to highlight their\ndifferent capabilities, we evaluate our models also on algorithmic tasks. In fact, it is well-known\nthat the actual capabilities of RNNs differ from one architecture to another [55]. We are interested\nin discussing such differences. With that goal in mind, we conduct experiments on two synthetic\nalgorithmic tasks, code execution (Sec. 4.2) and sequential ListOps (Sec. 4.3), which are designed\nto compare elementary sequence processing abilities of models. Finally, we apply our models to\nreinforcement learning in 2D game environments (Sec. 4.4) as a replacement for LSTMs.\n4.1 Language Modelling\nWe ﬁrst evaluate all discussed models on the generic language modelling task. This allows for\nobtaining a performance overview and reviewing the computational efﬁciency of different models. We\nuse the Wikitext-103 dataset [28] and follow the small model setting similar to what’s used in recent\nworks by Peng et al. [21] and Schlag et al. [23]. This allows for training and evaluating different\nmodels with a reasonable amount of compute on this resource-demanding language modelling task.\nPerplexity results. The results are shown in Table 1 which also serves as a tabular summary\nrecapitulating different models described in Sec. 2 and 3, with various architectures for slow and\nfast nets, and two choices of update rule. The top block of Table 1 shows the performance of the\nbaseline Transformer, Katharopoulos et al. [19]’s Linear Transformer, and Schlag et al. [23]’s Delta\nNet. The performance of models presented in Sec. 3 can be found in the middle block. First of all, the\nDelta MLP performs worse than the baseline Delta Net despite a slight increase in parameter count\n(44.3 vs. 44.0 M). This supports the intuition that it is better to make the slow network aware of the\noutputs of intermediate layers to generate fast weights in a deep network, instead of generating fast\nweights for all layers at a time. In all other models, the performance never degrades with the proposed\narchitectural augmentation. The Delta Delta Net yields limited improvements; we plan to study this\n6\nmodel in depth in future work. With the same amount of parameters (44.6 M), the Delta RNN yields\ngreater improvements. Among the models presented here, the Delta LSTM variant exhibits the best\nperformance. This shows that the slow network successfully controls the rather complex fast LSTM\nnetwork, although it also requires more parameters (47.3 M) than other models. Finally, the beneﬁts\nof recurrent connections added to the baseline Delta Net do not directly translate into practical\nimprovements in language modelling as demonstrated by the performance of RDN compared to the\none of the baseline Delta Net. Importantly, given a constant memory size w.r.t. sequence length, it\nis straight-forward to train and evaluate our RNNs without context truncation (while still limiting\nthe backpropagation span). Corresponding performances of Delta RNN and RDN are shown in the\nbottom part of Table 1: they outperform the regular Transformer with a limited context (256 tokens).\nWhile language modelling is useful as a sanity check (here for example, except for the Delta MLP, all\nmodels achieve reasonable performance), the task is too generic to identify certain important aspects\nof the models, such as real beneﬁts of recurrence. Before we move on to trickier RL applications,\nSec. 4.2 and 4.3 will focus on studying such aspects using synthetic algorithmic tasks.\nComputational efﬁciency. The modiﬁcations we proposed in Sec. 3 introduce additional computa-\ntional costs to linear Transformers/FWPs. First of all, none of them affect the core complexity of\nlinear Transformers: they all have a constant space and linear time complexity w.r.t. sequence length.\nHowever, the per-time-step computational costs differ a lot from one model to another, as quantiﬁed\nhere in terms of training speed using our implementation. All models are implemented using a custom\nCUDA kernel except the baseline Transformer for which we use regular PyTorch code [56]. Training\nspeeds of LT and Delta Net in Table 1 are 66 K and 63 K words per second respectively (vs. 33 K for\nthe baseline Transformer). The most expensive model is the Delta LSTM. This fast weight LSTM\nwith tied input-forget gates has 6 weight matrices, and each of these are manipulated by separate delta\nrules. The corresponding speed is 14 K words per second, too slow for scaling to more experiments.\nIn contrast, the speeds of Delta RNN and RDN remain reasonable: 41 K and 35 K words per second\nrespectively. Therefore, the remaining experiments will focus on these two recurrent architectures\nwhich are promising and practical in terms of both performance and computational costs.\n4.2 Code Execution Task: Learning to Maintain and Update Variable States\nIn code execution tasks [29], models are trained to sequentially read the input code provided as word-\nlevel text, and to predict the results of the corresponding code execution. We adopt the task setting\nfrom Fan et al. [57] with one conditional and three basic statements. We refer the readers to Appendix\nB.1 for a precise description of the task. This code execution task requires models to maintain the\nvalues of multiple variables, which has been shown to be difﬁcult for relatively shallow Transformers\nwith only feedforward connections [57].\nThe left block of Table 2 shows the results. Following again Fan et al. [57], we control the task\ndifﬁculty by modifying the number of variables (3 or 5). The model architectures are ﬁxed: the\nLSTM has only one layer with 256 nodes and all Transformer variants have the same architecture\nwith 4 layers with a hidden size of 256 using 16 heads and an inner feedforward layer size of 1024.\nWe ﬁrst note that the LSTM is the best performer for both difﬁculty levels, with the smallest\nperformance drops through increasing the number of variables. In contrast to prior claims [57], the\nLSTM is clearly capable of storing the values of multiple variables in its hidden and cell state vectors.\nWith three variables, the regular Transformer already largely underperforms other models with a\nmutable memory: Delta Net, Delta RNN, and RDN. Linear Transformers completely fail at this task,\nlikely due to the memory capacity problem pointed out by Schlag et al. [23] (see Appendix B.2 for\nfurther discussion). By increasing the number of variables to ﬁve, the baseline Transformers, Delta\nNet, and RDN become unstable as shown by high standard deviations w.r.t. the seed. The beneﬁts\nof recurrent connections introduced in our RDN compared to the baseline Delta Net become more\napparent (76.3 vs. 61.4%). In contrast, the Delta RNN remains stable and gives the best performance\n(85.1%) among Transformer variants, which shows the beneﬁts of recurrence and in particular the\nregular RNN architecture in the fast net. To match the performance of LSTM on this task, however,\nthese models need more layers (see Appendix B.2 for more results).\n7\nTable 2: Test accuracies(%) with standard deviations oncode execution (Code Exec) andsequential\nListOps (Seq ListOps). The difﬁculty of the task is controlled by the maximum number of possible\nvariables (# variables) for code execution, and the list depth (10 or 15) for ListOps. For code execution\nwith 5 variables, we report means over six seeds. In all other cases, the results are computed with\nthree seeds. For more results, see Appendix B.2 (Code Exec) and B.4 (Seq ListOps).\nCode Exec (# variables) Seq ListOps (depth)\n3 5 10 15\nLSTM 99.0 ±0.1 93.2 ± 6.1 88.5 ±2.9 24.4 ±1.1\nTransformer 71.8 ±2.6 35.4 ±28.2 79.1 ±0.9 75.3 ±0.4\nLinear Transformer 0.0 ±0.0 0.0 ± 0.0 64.0 ±0.3 64.4 ±0.4\nDelta Net 90.7 ±2.7 61.4 ±20.0 85.7 ±1.8 77.6 ±1.4\nDelta RNN 90.8 ±1.7 85.1 ± 1.9 83.6 ±1.2 78.0 ±1.0\nRDN 92.6 ±2.2 76.3 ±17.6 83.2 ±0.9 79.2 ±1.4\n4.3 Sequential ListOps: Learning Hierarchical Structure and Computation\nThe ListOps task [30] is a typical test for hierarchical structure learning, which requires list operation\nexecutions. We use a simple variant of ListOps whose detailed descriptions can be found in Appendix\nB.4. For example, the list [MAX 6 1 [FIRST 2 3 ] 0 [MIN 4 7 1] ] is of depth two and the\nexpected output is 6. While early research comparing self-attention to RNNs [ 58] has shown\nsome advantages of recurrence in hierarchical structure learning, more recent work [ 59] reports\nTransformers outperforming LSTMs on ListOps. According to Tay et al. [22], linear Transformer\nvariants (LT and Performers) underperform other Transformer variants by a large margin on ListOps.\nThe right block of Table 2 shows results for two different depths: 10 and 15. The model architectures\nare identical to those used in the code execution task (Sec. 4.2). At depth 10, we ﬁnd LSTM to perform\nbest, while mutable memory Transformer variants (Delta Net, Delta RNN, and RDN) outperform the\nregular and linear Transformers. At depth 15, the LSTM’s performance drops drastically (to 24.4%),\nwhile the differences between Transformer variants remain almost the same. We note that sequences\nare longer for the depth 15 problem (mean length of 185 tokens) than for the depth 10 version (mean\nlength of 98 tokens). This turns out to be difﬁcult for the small 256-dimensional LSTM; see Appendix\nB.4 for the corresponding ablation study. The performance differences between the baseline Delta\nNet and the proposed Delta RNN and RDN are rather small for this task. Importantly, our models\noutperform both regular and linear Transformers on this task requiring hierarchical structure learning.\n4.4 Reinforcement Learning in 2D Game Environments\nWe ﬁnally evaluate the performance of our models as a direct replacement for the LSTM in reinforce-\nment learning settings. In fact, only a limited number of prior works have investigated Transformers\nfor RL. Parisotto et al. [12] and Rae et al. [11] evaluate them on the DMLab-30 [60, 61]. Parisotto\net al. [12] also evaluate them on Atari but in a multi-task setting [62]. Others [57, 13] use toy maze\nenvironments. In contrast to Parisotto et al. [12]’s work, which presents multi-task Atari as a side ex-\nperiment, we study the Transformer family of models on the standard Atari 2600 setting [31, 63, 64]\nby training game-speciﬁc agents.\nSettings. We train an expert agent on each game separately with the Importance Weighted Actor-\nLearner Training Architecture (IMPALA) using the V-trace actor-critic setup [ 65] and entropy\nregularization [66] implemented in Torchbeast [67]. Our model follows the large architecture of\nEspeholt et al. [65] which consists of a 15-layer residual convolutional NN with one 256-node LSTM\nlayer which we replace by either the RDN (Sec. 3.2) or the Delta RNN (Sec. 3.1). In line with the\nsmall LSTM used for Atari (only 1 layer with 256 hidden nodes) we also conﬁgure a small RDN: 2\nlayers with a hidden size of 128 using 4 heads, and a feedforward dimension of 512. We ﬁnd this\nsmall model to perform already surprisingly well. For the rest, we use the same hyperparameters as\nEspeholt et al. [65] which can be found in Appendix C.\n8\nSeaquest\nAlienPong\nB. HeistBerzerk\nStargunner\nAmidar\nJamesBond\nQ*Bert\nRobotankMsPacman\nZaxxon\nUp'n Down\nGopherKung-fuB. RiderAssaultD. AttackBattlezoneS. Invaders\n0\n2\n4\n6\n8\n10  Relative Improvements from the LT\nFigure 1: Relative improvements in test\nscores obtained by the Recurrent Delta\nNet (RDN) compared to the Linear Trans-\nformer (LT) after 50 M env. steps.\n60\n90\n120\nSeaquestBerzerk\nUp'nDown\nPong\nRobotank\nB.HeistB.Rider\nStargunner\nD.AttackZaxxonAmidarAssault\nMsPacman\nKung-FuGopher\nJamesBond\nAlien\nBattlezoneS.Invaders\nQ*Bert\n1\n0\n1\n2\n3\n4\n5\n                      Relative Improvements from the LT\nFigure 2: Relative improvements in test\nscores obtained by the Recurrent Delta\nNet (RDN) compared to the Linear Trans-\nformer (LT) after 200 M env. steps.\nMain experiments. We evaluate our models in 20 environments. According to Mott et al. [68],\nin about half of them, the LSTM outperforms the feedforward baselines—which we conﬁrm in our\nsetting with 50 M steps (see Appendix C). We report results at 50 M and 200 M environmental steps\nof training. Like Nair et al. [69], we run the trained agent for 30 test episodes. Here we repeat this\nevaluation ﬁve times to report the average score with a standard deviation. The following analysis\nfocuses on the RDN (Sec. 3.2) compared to the regular linear Transformer and the LSTM. A similar\nstudy of the Delta RNN, as well as comparisons to more baselines, and the exact scores achieved by\neach model on each game can be found in Appendix C.\nIn all our experiments above, we have shown that the Linear Transformer, i.e., a Fast Weight\nProgrammer with a purely additive update rule, consistently underperforms other models based on\nthe delta rule. Here we conﬁrm this trend once more. Figures 1 and 2 show the relative improvements\nof scores obtained by Recurrent Delta Net over those achieved by the linear Transformer on each\ngame, respectively after 50 and 200 M interaction steps. The RDN matches or outperforms the Linear\nTransformer on all games except for two out of 20 games at both stages of training.\nFigure 3 shows relative improvements of RDN over LSTM after 50 M interactions. In 12 games,\nthe RDN yields improvements over LSTM, whereas in 3 games, the LSTM performs better. In the\nKung-fuBerzerk\nUp'nDown\nB.Heist\nSeaquest\nJamesBond\nRobotank\nPong\nAssault\nBattlezoneStargunner\nD.AttackB.RiderGopherQ*Bert\nMsPacman\nAmidarZaxxon\nAlien\nS.Invaders\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n  Relative Improvements from the LSTM\nFigure 3: Relative improvements in test\nscores obtained by 2-layer RDN compared to\nLSTM after 50 M env. steps.\n84.0\n84.5\n85.0\nBerzerkSeaquestBattlezone\nGopherZaxxonKung-FuD.Attack\nPong\nB.Heist\nUp'nDownRobotankStargunner\nAssault\nMsPacmanS.InvadersJamesBond\nB.Rider\nAlien\nAmidarQ*Bert\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n                      Relative Improvements from the LSTM\nFigure 4: Relative improvements in test\nscores obtained by 2-layer RDN compared to\nLSTM after 200 M env. steps.\n9\nremaining 5 games, both reach similar scores. Interestingly, this trend does not directly extrapolate\nto the 200 M case, which is presented in Figure 4. With longer training, the LSTM surpasses the\nperformance of the RDN in Battlezone, Gopher, Seaquest and Zaxxon, while the RDN catches up in\nUp’N Downand Kung-Fu Master. Overall, there are 6 games in which LSTM clearly outperforms\nRDN at 200 M steps, whereas in 9 games the result is the opposite.\nOn a side note, some of the scores achieved by the RDN at 200 M step are excellent: a score of over\n170 K and 980 K inSpace Invader and Q*Bert respectively beats the state-of-the-art set by MuZero\n[70] and Agent57 [62]. However, a direct comparison is not fair as we train game-speciﬁc agents.\nExperiments with larger models. Given the results above, a natural question to ask is whether a\nlarger model size improves the RDN in games where the LSTM dominates. We focus on four such\ngames: Battlezone, Berzerk, Gopher, and Seaquest (See Fig. 4). We double the model size to 3.4 M\nparameters by increasing the number of layers to 4 and the hidden size to 256, with 8 heads. As\nshown in Table 3, larger RDN models reduce the gap to the LSTM (except inBerzerk). This indicates\nthat further scaling RDN might be as promising as scaling regular Transformers in other domains.\nTable 3: Performance of a larger RDN in games where the LSTM dominates (200 M steps).\nBattlezone Berzerk Gopher Seaquest\nLSTM 24,873 ± 1,240 1,150 ± 92 124,914 ± 22,422 12,643 ± 1,627\nRDN 10,980 ± 1,104 348 ± 17 86,008 ± 11,815 4,373 ± 504\nRDN larger 28,273 ± 5,333 346 ± 9 118,273 ± 14,872 14,601 ± 712\n5 Conclusion\nInspired by the formal equivalence of linear Transformers and certain traditional Fast Weight Pro-\ngrammers (FWPs) from the early ’90s, we propose various new linear Transformer variants with\nrecurrent connections. Our novel Recurrent FWPs (RFWPs) outperform previous linear and regular\nTransformers on a code execution task and signiﬁcantly improve over Transformers in a sequential\nListOps task. On Wikitext-103 in the “small” model setting, RFWPs compete well with the previous\nbest linear Transformer variants for truncated contexts, and with full contexts, beat regular Trans-\nformers. Our RFWPs can also be used as drop-in replacements for problems where RNNs are still\ndominant. In particular, we evaluate them in reinforcement learning settings on 20 Atari 2600 en-\nvironments. They clearly outperform the regular Linear Transformer on almost all environments.\nThey also outperform the LSTM across many environments with a small model size and demonstrate\npromising scaling properties for larger models. Given the increasing interest in deploying Transform-\ners in RL [71, 72], in particular in the framework of Upside-Down RL [73, 74], our RFWP models\nare particularly relevant: as RNNs, they conveniently handle long contexts with a constant memory\nsize, while being powerful Transformer variants at the same time. Our work highlights the usefulness\nof the FWP framework from the ’90s and its connection to modern architectures, opening promising\navenues for further research into new classes of recurrent Transformers.\nAcknowledgments and Disclosure of Funding\nWe thank Aleksandar Stani´c and Sjoerd van Steenkiste for valuable comments on the ﬁrst version of\nthis paper. This research was partially funded by ERC Advanced grant no: 742870, project AlgoRNN,\nand by Swiss National Science Foundation grant no: 200021_192356, project NEUSYM. This work\nwas partially supported by computational resources at the CSCS Swiss National Supercomputing\nCentre, project d115. We thank NVIDIA Corporation for donating several DGX machines, and IBM\nfor donating a Minsky machine.\nReferences\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. Advances in Neural\n10\nInformation Processing Systems (NIPS), pages 5998–6008, Long Beach, CA, USA, December\n2017.\n[2] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level\nlanguage modeling with deeper self-attention. In Proc. Conference on Artiﬁcial Intelligence\n(AAAI), pages 3159–3166, Honolulu, HI, USA, January 2019.\n[3] Tom B Brown et al. Language models are few-shot learners. In Proc. Advances in Neural\nInformation Processing Systems (NeurIPS), Virtual only, December 2020.\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of\ndeep bidirectional transformers for language understanding. In Proc. North American Chapter\nof the Association for Computational Linguistics on Human Language Technologies (NAACL-\nHLT), pages 4171–4186, Minneapolis, MN, USA, June 2019.\n[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. In Int. Conf. on Learning Representations (ICLR), Virtual only, May 2021.\n[6] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable\nDETR: Deformable transformers for end-to-end object detection. In Int. Conf. on Learning\nRepresentations (ICLR), Virtual only, May 2021.\n[7] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical\nreasoning abilities of neural models. In Int. Conf. on Learning Representations (ICLR), New\nOrleans, LA, USA, May 2019.\n[8] Imanol Schlag, Paul Smolensky, Roland Fernandez, Nebojsa Jojic, Jürgen Schmidhuber, and\nJianfeng Gao. Enhancing the transformer with explicit relational encoding for math problem\nsolving. Preprint arXiv:1910.06611, 2019.\n[9] Francois Charton, Amaury Hayat, and Guillaume Lample. Learning advanced mathematical\ncomputations from examples. In Int. Conf. on Learning Representations (ICLR), Virtual only,\nMay 2021.\n[10] Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and\nRuslan Salakhutdinov. Transformer-XL: Attentive language models beyond a ﬁxed-length\ncontext. In Proc. Association for Computational Linguistics (ACL), pages 2978–2988, Florence,\nItaly, July 2019.\n[11] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap.\nCompressive transformers for long-range sequence modelling. In Int. Conf. on Learning\nRepresentations (ICLR), Virtual only, April 2020.\n[12] Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayaku-\nmar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick,\nNicolas Heess, and Raia Hadsell. Stabilizing Transformers for reinforcement learning. In Proc.\nInt. Conf. on Machine Learning (ICML), pages 7487–7498, Virtual only, July 2020.\n[13] Emilio Parisotto and Ruslan Salakhutdinov. Efﬁcient transformers in reinforcement learning\nusing actor-learner distillation. In Int. Conf. on Learning Representations (ICLR), Virtual only,\nMay 2021.\n[14] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\n[15] Daan Wierstra, Alexander Förster, Jan Peters, and Jürgen Schmidhuber. Recurrent policy\ngradients. Logic Journal of IGPL, 18(2):620–634, 2010.\n[16] Daan Wierstra, Alexander Förster, Jan Peters, and Jürgen Schmidhuber. Solving deep memory\nPOMDPs with recurrent policy gradients. In Proc. Int. Conf. on Artiﬁcial Neural Networks\n(ICANN), pages 697–706, Porto, Portugal, September 2007.\n11\n[17] OpenAI et al. Dota 2 with large scale deep reinforcement learning. Preprint arXiv:1912.06680,\n2019.\n[18] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik,\nJunyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grand-\nmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350–\n354, 2019.\n[19] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers\nare RNNs: Fast autoregressive transformers with linear attention. In Proc. Int. Conf. on Machine\nLearning (ICML), Virtual only, July 2020.\n[20] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,\nTamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking\nattention with performers. In Int. Conf. on Learning Representations (ICLR), Virtual only, 2021.\n[21] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong.\nRandom feature attention. In Int. Conf. on Learning Representations (ICLR), Virtual only, 2021.\n[22] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,\nLiu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efﬁcient\ntransformers. In Int. Conf. on Learning Representations (ICLR), Virtual only, May 2021.\n[23] Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear Transformers are secretly fast\nweight programmers. In Proc. Int. Conf. on Machine Learning (ICML), Virtual only, July 2021.\n[24] Jürgen Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent\nnets. Technical Report FKI-147-91, Institut für Informatik, Technische Universität München,\nMarch 1991.\n[25] Jürgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic\nrecurrent networks. Neural Computation, 4(1):131–139, 1992.\n[26] Jürgen Schmidhuber. Reducing the ratio between learning complexity and number of time\nvarying variables in fully recurrent nets. In International Conference on Artiﬁcial Neural\nNetworks (ICANN), pages 460–463, Amsterdam, Netherlands, September 1993.\n[27] Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions\nof the Association for Computational Linguistics, 8:156–171, 2020.\n[28] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. In Int. Conf. on Learning Representations (ICLR), Toulon, France, April 2017.\n[29] Wojciech Zaremba and Ilya Sutskever. Learning to execute. Preprint arXiv:1410.4615, 2014.\n[30] Nikita Nangia and Samuel Bowman. ListOps: A diagnostic dataset for latent tree learning.\nIn Proc. North American Chapter of the Association for Computational Linguistics (NAACL):\nStudent Research Workshop, pages 92–99, New Orleans, LA, USA, June 2018.\n[31] Marc G. Bellemare, Georg Ostrovski, Arthur Guez, Philip S. Thomas, and Rémi Munos.\nIncreasing the action gap: New operators for reinforcement learning. In Proc. AAAI Conf. on\nArtiﬁcial Intelligence, pages 1476–1483, Phoenix, AZ, USA, February 2016. AAAI Press.\n[32] Jürgen Schmidhuber. Making the world differentiable: On using fully recurrent self-supervised\nneural networks for dynamic reinforcement learning and planning in non-stationary environ-\nments. Technical Report FKI-126-90, Institut für Informatik, Technische Universität München,\n1990.\n[33] Bernard Widrow and Marcian E Hoff. Adaptive switching circuits. In Proc. IRE WESCON\nConvention Record, pages 96–104, Los Angeles, CA, USA, August 1960.\n[34] David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. In Int. Conf. on Learning Representa-\ntions (ICLR), Toulon, France, April 2017.\n12\n[35] Kazuki Irie and Jürgen Schmidhuber. Training and generating neural networks in compressed\nweight space. In Neural Compression: From Information Theory to Applications – Workshop,\nICLR 2021, Virtual only, May 2021.\n[36] Christoph von der Malsburg. The correlation theory of brain function. Internal Report 81-2,\nGoettingen: Department of Neurobiology, Max Planck Intitute for Biophysical Chemistry, 1981.\n[37] Jerome A Feldman. Dynamic connections in neural networks. Biological cybernetics, 46(1):\n27–39, 1982.\n[38] James L McClelland. Putting knowledge in its place: A scheme for programming parallel\nprocessing structures on the ﬂy. Cognitive Science, 9(1):113–146, 1985.\n[39] Benjamin Klein, Lior Wolf, and Yehuda Afek. A dynamic convolutional layer for short\nrangeweather prediction. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition\n(CVPR), pages 4840–4848, Boston, MA, USA, June 2015.\n[40] Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han. Image question answering using\nconvolutional neural network with dynamic parameter prediction. In Proc. IEEE Conf. on\nComputer Vision and Pattern Recognition (CVPR), pages 30–38, Las Vegas, NV, USA, 2016.\n[41] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. Dynamic ﬁlter networks. In\nProc. Advances in Neural Information Processing Systems (NIPS), pages 667–675, Barcelona,\nSpain, 2016.\n[42] Irwan Bello. LambdaNetworks: Modeling long-range interactions without attention. In Int.\nConf. on Learning Representations (ICLR), Virtual only, May 2021.\n[43] Thomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: training plastic\nneural networks with backpropagation. In Proc. Int. Conf. on Machine Learning (ICML), pages\n3559–3568, Stockholm, Sweden, July 2018.\n[44] Thomas Miconi, Aditya Rawal, Jeff Clune, and Kenneth O. Stanley. Backpropamine: training\nself-modifying neural networks with differentiable neuromodulated plasticity. In Int. Conf. on\nLearning Representations (ICLR), New Orleans, LA, USA, May 2019.\n[45] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. Preprint\narXiv:1410.5401, 2014.\n[46] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-\nBarwi´nska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,\net al. Hybrid computing using a neural network with dynamic external memory. Nature, 538\n(7626):471–476, 2016.\n[47] Jimmy Ba, Geoffrey E Hinton, V olodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using\nfast weights to attend to the recent past. In Proc. Advances in Neural Information Processing\nSystems (NIPS), pages 4331–4339, Barcelona, Spain, December 2016.\n[48] Imanol Schlag, Tsendsuren Munkhdalai, and Jürgen Schmidhuber. Learning associative infer-\nence using fast weight memory. In Int. Conf. on Learning Representations (ICLR), Virtual only,\nMay 2021.\n[49] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. Metalearned\nneural memory. In Proc. Advances in Neural Information Processing Systems (NeurIPS), pages\n13310–13321, Vancouver, Canada, December 2019.\n[50] Imanol Schlag and Jürgen Schmidhuber. Gated fast weights for on-the-ﬂy neural program\ngeneration. In NIPS Metalearning Workshop, Long Beach, CA, USA, December 2017.\n[51] T Anderson Keller, Sharath Nittur Sridhar, and Xin Wang. Fast weight long short-term memory.\nPreprint arXiv:1804.06511, 2018.\n[52] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\nnetworks. In Proc. European Conf. on Computer Vision (ECCV), pages 630–645, Amsterdam,\nNetherlands, October 2016.\n13\n[53] Rupesh K Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks. In the Deep\nLearning workshop at Int. Conf. on Machine Learning (ICML), Lille, France, July 2015.\n[54] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. Preprint\narXiv:1607.06450, 2016.\n[55] Gail Weiss, Yoav Goldberg, and Eran Yahav. On the practical computational power of ﬁnite\nprecision rnns for language recognition. In Proc. Association for Computational Linguistics\n(ACL), pages 740–745, Melbourne, Australia, July 2018.\n[56] Adam Paszke et al. PyTorch: An imperative style, high-performance deep learning library.\nIn Proc. Advances in Neural Information Processing Systems (NeurIPS) , pages 8026–8037,\nVancouver, Canada, December 2019.\n[57] Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Address-\ning some limitations of Transformers with feedback memory. Preprint arXiv:2002.09402, 2020.\n[58] Ke Tran, Arianna Bisazza, and Christof Monz. The importance of being recurrent for modeling\nhierarchical structure. In Proc. Conf. on Empirical Methods in Natural Language Processing\n(EMNLP), pages 4731–4736, Brussels, Belgium, October 2018.\n[59] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal\ncomputation engines. Preprint arXiv:2103.05247, 2021.\n[60] Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich\nKüttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab.\nPreprint arXiv:1612.03801, 2016.\n[61] Joel Z Leibo, Cyprien de Masson d’Autume, Daniel Zoran, David Amos, Charles Beattie,\nKeith Anderson, Antonio García Castañeda, Manuel Sanchez, Simon Green, Audrunas Gruslys,\net al. Psychlab: a psychology laboratory for deep reinforcement learning agents. Preprint\narXiv:1801.08116, 2018.\n[62] Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi,\nZhaohan Daniel Guo, and Charles Blundell. Agent57: Outperforming the Atari human bench-\nmark. In Proc. Int. Conf. on Machine Learning (ICML), pages 507–517, Virtual only, July 2020.\n[63] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. In NIPS Deep\nLearning Workshop, Lake Tahoe, NV, USA, December 2013.\n[64] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.\n[65] Lasse Espeholt, Hubert Soyer, Rémi Munos, Karen Simonyan, V olodymyr Mnih, Tom Ward,\nYotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu.\nIMPALA: scalable distributed deep-RL with importance weighted actor-learner architectures.\nIn Proc. Int. Conf. on Machine Learning (ICML), pages 1406–1415, Stockholm, Sweden, July\n2018.\n[66] V olodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap,\nTim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce-\nment learning. In Proc. Int. Conf. on Machine Learning (ICML), pages 1928–1937, New York\nCity, NY, USA, June 2016.\n[67] Heinrich Küttler, Nantas Nardelli, Thibaut Lavril, Marco Selvatici, Viswanath Sivakumar, Tim\nRocktäschel, and Edward Grefenstette. Torchbeast: A PyTorch platform for distributed RL.\nPreprint arXiv:1910.03552, 2019.\n[68] Alexander Mott, Daniel Zoran, Mike Chrzanowski, Daan Wierstra, and Danilo Jimenez Rezende.\nTowards interpretable reinforcement learning using attention augmented agents. In Proc.\nAdvances in Neural Information Processing Systems (NeurIPS), pages 12329–12338, Vancouver,\nCanada, December 2019.\n14\n[69] Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro\nDe Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, et al.\nMassively parallel methods for deep reinforcement learning. In Deep Learning Workshop,\nInternational Conference on Machine Learning (ICML), Lille, France, July 2015.\n[70] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Si-\nmon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering\nAtari, Go, Chess and Shogi by planning with a learned model. Nature, 588(7839):604–609,\n2020.\n[71] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter\nAbbeel, Aravind Srinivas, and Igor Mordatch. Decision Transformer: Reinforcement learning\nvia sequence modeling. Preprint arXiv:2106.01345, 2021.\n[72] Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence\nmodeling problem. Preprint arXiv:2106.02039, 2021.\n[73] Juergen Schmidhuber. Reinforcement learning upside down: Don’t predict rewards–just map\nthem to actions. Preprint arXiv:1912.02875, 2019.\n[74] Rupesh Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Ja´skowski, and Jürgen Schmid-\nhuber. Training agents using upside-down reinforcement learning. Preprint arXiv:1912.02877,\n2019.\n[75] Stephen José Hanson. A stochastic version of the delta rule. Physica D: Nonlinear Phenomena,\n42(1-3):265–272, 1990.\n[76] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from overﬁtting. The Journal of Machine\nLearning Research, 15(1):1929–1958, 2014.\n[77] Noah Frazier-Logue and Stephen José Hanson. Dropout is a special case of the stochastic delta\nrule: Faster and more accurate deep learning. Preprint arXiv:1808.03578, 2018.\n[78] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Preprint\narXiv:1412.6980, 2014.\n[79] Felix A Gers, Jürgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction\nwith LSTM. Neural computation, 12(10):2451–2471, 2000.\n[80] Kazuki Irie, Albert Zeyer, Ralf Schlüter, and Hermann Ney. Language modeling with deep\nTransformers. In Proc. Interspeech, pages 3905–3909, Graz, Austria, September 2019.\n[81] Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. The devil is in the detail: Simple tricks\nimprove systematic generalization of transformers. In Proc. Conf. on Empirical Methods in\nNatural Language Processing (EMNLP), Punta Cana, Dominican Republic, November 2021.\n[82] Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber. The Neural Data Router: Adaptive\ncontrol ﬂow in Transformers improves systematic generalization. Preprint arXiv:2110.07732,\n2021.\n[83] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5- RMSProp: Divide the gradient by a running\naverage of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012.\n[84] Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and\nMichael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open\nproblems for general agents. Journal of Artiﬁcial Intelligence Research, 61:523–562, 2018.\n[85] Marcin Andrychowicz, Anton Raichuk, Piotr Stanczyk, Manu Orsini, Sertan Girgin, Raphaël\nMarinier, Léonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. What\nmatters for on-policy deep actor-critic methods? A large scale study. In Int. Conf. on Learning\nRepresentations (ICLR), Virtual only, May 2021.\n15\nA Experimental Details and Ablation Studies for Language Modelling\nA.1 Experimental Settings\nAll language models in Table 1 have the same Transformer conﬁguration: a 16-layer model with a\nhidden size of 128 with 8 heads, and a feed-forward dimension of 2048. We use a dropout [75, 76, 77]\nrate of 0.1. The batch size is 96 and we train for about 120 epochs with Adam optimiser [ 78] with an\ninitial learning rate of 0.00025 and 2000 learning rate warm-up steps. All models are trained with\na back-propagation span of 256 tokens. During training, these segments are treated independently,\nexcept for the + full context cases in Table 1 where the states (both recurrent states and fast weight\nstates) from a segment are used as initialisation for the subsequent segment. The models in +\nfull context cases are also evaluated in the same way by carrying over the context throughout the\nevaluation text with a batch size of one. For all other cases, the evaluation is done by going through\nthe text with a sliding window of size 256 with a batch size of one. Transformer states are computed\nfor all positions in each window, but only the last position is used to compute perplexity (except in\nthe ﬁrst segment where all positions are used for evaluation) [2]. We trained all models using two\nGPUs (32 GB V100), and the longest training takes up to 10 days (see Sec. 4.1 in the main text for\nspeed comparison between models).\nFor readers interested in any further details, we refer to our code which is publicly available.\nA.2 Ablation Studies\nIn this section, we specify the exact Delta LSTM and Delta MLP models used in Table 1, and provide\na few ablation studies for Delta RNN, Delta LSTM3, and Delta MLP models.\nTable 4: Ablation studies for Delta LSTM, Delta RNN, and Delta MLP models. Language model\nperplexities are shown and the setting is the same as in Table 1.\nVersion Valid Test #Prms\nDelta RNN A 35.6 36.7 44.6\nB 33.8 35.0\nDelta LSTM A 38.5 39.9 47.3\nB 34.2 35.2\nC 33.5 34.7\nD 32.6 33.8\nDelta MLP A 36.8 37.9 44.3\nB 35.8 36.8 44.3\nDelta RNN. In Sec. 3.1, we argue for a version of fast RNN given by Eq. 12 as a natural augmenta-\ntion of the linear Transformer with recurrent connections. Here we empirically support this choice by\ncomparing to another variant of Delta RNN given by:\ny(t) = f(W(t)q(t) + R(t)y(t−1)) (19)\nwhere f is again the softmax activation which makes y(t) a valid query vector (positive components\nwhich sum up to one) for fast weights maintained by the delta update rule. We refer to this version\nas Version A in this ablation study and the one given by Eq. 12 as Version B. As Table 4 shows,\nVersion B performs better, and this is the one we report in Table 1 in the main text.\nDelta LSTM. We evaluate four versions of Delta LSTM for ablation. In all cases, we tie the input\nand forget gates to reduce the total number of fast weights to be controlled by the slow net. All\n3The numbers reported in Table 4 for the Delta LSTM models are better than those we presented in an earlier\nversion. In fact, we found that in our previous code, the slow weights for key and value generation were shared by\nmistake between the forward and recurrent fast weight matrices (while the reported parameter count was that of\nthe correct model with separate slow weight matrices). Fixing this resulted in the corresponding improvements.\n16\nmodels contain six fast weights and each of them is updated according to the delta update rule (Eq. 8).\nThe different versions differ in the location of the activation function and residual connections in the\nLSTM architecture [14, 79], inspired by the Transformer.\nVersion A(analogous to Version A of Delta RNN above) is the one which is the closest to the original\nLSTM with tied input and forget gate. The only architectural difference is the usual tanh on the cell\noutput c(t) which is replaced by a softmax f placed after the ﬁnal output of the layer f(c(t) ⊙o(t)),\nsuch that it can directly be used as a delta rule compatible query for the next time step (we also use a\nsigmoid instead of tanh for the main transformation u(t), but this is not crucial for any models here).\nu(t) = σ(W(t)q(t) + R(t)y(t−1)) (20)\nf(t) = σ(W(t)\nf q(t) + R(t)\nf y(t−1)) (21)\no(t) = σ(W(t)\no q(t) + R(t)\no y(t−1)) (22)\nc(t) = f(t) ⊙c(t−1) + (1 −f(t)) ⊙u(t) (23)\ny(t) = f(c(t) ⊙o(t)) (24)\nVersion B is obtained by delaying the application of the softmax activation f in Version A.\nu(t) = σ(W(t)q(t) + R(t)f(y(t−1))) (25)\nf(t) = σ(W(t)\nf q(t) + R(t)\nf f(y(t−1))) (26)\no(t) = σ(W(t)\no q(t) + R(t)\no f(y(t−1))) (27)\nc(t) = f(t) ⊙c(t−1) + (1 −f(t)) ⊙u(t) (28)\ny(t) = c(t) ⊙o(t) (29)\nVersion C is obtained from Version B by adding a residual connection from the feed-forward part\nz(t)\nu of the main transformation u(t) to the output.\nz(t)\nu = W(t)q(t) (30)\nu(t) = σ(z(t)\nu + R(t)f(y(t−1))) (31)\nf(t) = σ(W(t)\nf q(t) + R(t)\nf f(y(t−1))) (32)\no(t) = σ(W(t)\no q(t) + R(t)\no f(y(t−1))) (33)\nc(t) = f(t) ⊙c(t−1) + (1 −f(t)) ⊙u(t) (34)\ny(t) = c(t) ⊙o(t) + z(t)\nu (35)\nFinally, Version D is obtained from Version B by removing the sigmoid on the main transformation\nu(t) which results in a highway net-like skip connection [53] from u(t) to the output. This version is\nthen analogous to Version B of the Delta RNN as a natural augmentation of the linear Transformer: a\nrecurrent term is added to the main transformation u(t) and gating components are added to make it\nan LSTM architecture:\nu(t) = W(t)q(t) + R(t)f(y(t−1)) (36)\nf(t) = σ(W(t)\nf q(t) + R(t)\nf f(y(t−1))) (37)\no(t) = σ(W(t)\no q(t) + R(t)\no f(y(t−1))) (38)\nc(t) = f(t) ⊙c(t−1) + (1 −f(t)) ⊙u(t) (39)\ny(t) = c(t) ⊙o(t) (40)\nCorresponding performances can be found in Table 4. The best model, Version D, is the one we\nreport in Table 1 in the main text.\nDelta MLP. We also conduct a few ablation studies for the Delta MLP (Sec. 3.1). As MLP archi-\ntecture we used the feedforward block of the regular Transformer which consists of two feedforward\n17\nlayers: one with the size of the inner feedforward layer (2048 here) and another one with the size\nof hidden dimension (128 here). We test two variants which result in a similar number of param-\neters: Version A with 8 overall Transformer layers where each self-attention layer contains 4 fast\nMLP layers (i.e. a total of 48 feedforward layers with 32 fast ones), and Version B with 11 over-\nall Transformer layers where each self-attention layer contains 2 fast MLP layers (i.e. a total of 44\nfeedforward layers with 22 fast ones). As shown in Table 4, Version B which has fewer fast layers\ncontrolled by the same slow net performs better, and, as already mentioned in Sec. 4, they do not\noutperform the baseline Delta Net which has only one fast feedforward layer (Table 1).\nA.3 Dimensionality of Delta-Delta Net vs. Delta Net\nHere we describe how the dimensionality of Delta-Delta Net scales with the size of the Delta Net. We\nassume a Delta Net with a dimensiondfor all query, key, value and input vectors. Then its slow weight\nmatrix (the projection matrix) is of size d×(3d+ 1) as it projects a d-dimensional input to query,\nkey, value vectors (3d) and a scalar beta (+1) which are needed to maintain a d×dfast weight matrix\nusing the delta rule. Now we can express the dimensionality of a Delta-Delta Net in terms ofd, whose\nfast network is a Delta Net with the dimensionality above. The size of its fast weight matrix is thus\nd×(3d+ 1). In order to maintain a fast weight matrix of this dimension using the delta rule, we need\nkey and query vectors of sized, a value vector of size3d+ 1, and a scalar beta (+1). The slow weight\nmatrix has to produce all these variables with a total dimension of (5d+ 2) from the input of size d.\nTherefore, the size of the slow weight matrix in the Delta-Delta Net isd×(5d+2). Such a Delta-Delta\nNet would have to store two fast weight matrices: one of sized×(3d+1) and another one of sized×d.\nB Experimental Details and Additional Results for Algorithmic Tasks\nB.1 Task Details for Code Execution\nIn code execution tasks [ 29], models are trained to sequentially read the input code provided as\nword-level text and to predict the results of the corresponding code execution. We adopt the task\nsetting from Fan et al. [57]. Each example is a sequence consisting of multiple statements — 100\nin our experiments. A statement can be one of the following three basic statements: assign which\nassigns a value to a variable (e.g. x = 2 ;), increment which increments or decrements an already\nassigned variable (e.g. x ++ ;), or print which outputs the value of the variable (e.g. print x ;).\nIn addition to basic statements, there are also conditional comparisons on already deﬁned variables\nfollowed by a basic statement (e.g. if x < 3 : x ++ ;). The model reads the input word-level\ncode sequence from left to right in an auto-regressive manner, and makes a prediction at each position:\nat the end of each print statement, the model has to predict the correct variable value, and for all\nother positions, the no-output token.\nHere is a short example (with N denoting the no-output token):\nIn: x = 3 ; y = 7 ; x ++ ; if y < 6 : print x ; print x ;\nOut: N N N N N N N N N N N N N N N N N N N N N 4\nIn contrast to Fan et al. [57], we hard-code the last statement to be a print statement of a randomly\nchosen variable such that the model always has to make a prediction at the end of the sequence. The\noutput vocabulary of the model is restricted to discrete values within a pre-determined range (here\nbetween -8 and 16), and the code sequences are constructed such that the value to be printed does not\nexceed this range by rejecting any statement which would result in such values. Like Fan et al. [57],\nwe randomly generate 10,000 sequences for training and 1,000 sequences each for validation and test.\nWith 100 statements per sequence, we obtain sequences with lengths varying from about 370 to 550,\nwith an average length of about 450 tokens for both 3 and 5 variable cases, and for train, valid, and test\nsets. This code execution task requires models to maintain the values of multiple variables, which has\nbeen previously shown to be a difﬁcult task for Transformers with only feedforward connections [57].\nB.2 Additional Results for Code Execution\nToken level print accuracy. First of all, as mentioned in the main text, the test accuracies reported\nin Table 2 are on the sequence-level, i.e., an output sequence is counted as correct only if all output\n18\ntokens in the sequence match the ground truth. The sequence level accuracy is a good evaluation\nmeasure here since for most positions in the sequence (except at the end of print statement) the\ncorrect target is the no-output token. This results in 0% accuracy for the Linear Transformer, which\nmight be shocking at ﬁrst glance at Table 2. Thus, we also provide the token accuracies following\nthe print statements. The results can be found in Table 5. There we can see that the accuracies for\nthe Linear Transformer are not zero: above 20% in both 3 and 5 variable cases. Nevertheless, they\nclearly underperform other models.\nTable 5: Token-level validation accuracies (%) for theprint statements on code execution. Means\nand stds are computed with three seeds for 3-variable and six seeds for 5-variable cases.\n# Variables\n3 5\nLSTM 99.9 ±0.0 99.6 ± 0.4\nTransformer 98.6 ±0.2 75.5 ±31.0\nLinear Transformer 24.6 ±0.6 20.7 ± 1.4\nDelta Net 99.5 ±0.1 97.2 ± 2.0\nDelta RNN 99.5 ±0.0 99.3 ± 0.2\nRDN 99.6 ±0.1 98.6 ± 1.4\nModel conﬁgurations. The Transformer architecture in Table 2 is adopted from Fan et al. [57]:\n4 layers with a hidden dimension of 256 (where we use 16 heads instead of 4) and a feedforward\ndimension of 1024, which yields 3.2 M parameters (like for Fan et al.[57]). We use a dropout rate\nof 0.1. The regular Transformer makes use of sinusoidal positional encoding (as is likely the case\nfor Fan et al. [57]) while all other models in Table 2 don’t [80, 23]. All Transformer models use pre-\nactivation residual connections [52] and layer norm [54]. Our LSTM model in Table 2 has one LSTM\nlayer with a dimension of 256 and an input embedding of 128 which results in 405 K parameters.\nWe train all models with a batch size of 64 using the Adam optimiser with a learning rate of 3e-4\nfor Transformer-family models and a learning rate of 3e-3 for the LSTM. We clip the gradients in\nthe LSTM model at 0.1. To train the regular Transformers, gradient accumulation was necessary to\nachieve the same batch size without hitting the GPU memory limit. This was not the case for space\nefﬁcient linear Transformer variants. All models are trained for 200 epochs which takes no more than\n23 hours for any model on a single 16 GB P100 GPU.\nModel architecture ablation. Here we conduct a few additional experiments to understand the\nmodels’ sensitivity to hyper-parameters. We restrict our analysis to the setting with 5 variables in\nwhich the performance gap between models is large (Table 2). We train deeper but thinner models\nwith 8 layers: each with a hidden size of 128 using 8 heads and a feed-forward dimension of 256.\nThis yields a total of 1.1 M parameters for all Transformer models except for the Delta RNN which\nhas 1.3 M parameters. These deeper but thinner models can be trained within 10 hours using a\nsingle 16 GB V100 GPU. We present the results in the bottom part of Table 6. We don’t report the\nperformance of the regular Transformer since the 8-layer variant learns very slowly and does not\nimprove over the initial 0% sequence-level accuracy within 200 epochs of training after which we\nreport the performance for all models4.\nFirst, we observe that the Delta RNN with 8 layers can now match the performance of the baseline\nLSTM with 256 nodes. However, increasing the LSTM hidden size to 512 (which gives a parameter\ncount of 1.3 M; equal to the Delta RNN’s) further improves the LSTM. Second, the Delta Net still\nremains unstable. We tried several tricks to stabilise Transformers on algorithmic tasks [ 81], e.g.\nembedding initialisation and scaling, but with little success. The problem seems intrinsically difﬁcult\nfor Transformer models, though we note that one of six runs achieved a very good performance of\n97.3%. Finally, we observe that the Recurrent Delta Net becomes more stable and performs better\nwith a deep architecture.\n4Extra experiments with this 8-layer regular Transformer show that after 800 epochs with a dropout rate of\n0.3, a test accuracy of 89.1 ± 2.2% is achieved. This is still worse than the performance of Delta RNN trained\nfor 200 epochs, although the comparison is not even fair due to the longer training and extra tuning.\n19\nTable 6: Test accuracies (%) on code execution with 5 variables. Mean, standard deviation (std), the\nlowest (min) and highest (max) accuracies are computed over six runs. The number of parameters\n(Prms.) is given in millions.\nwidth depth mean ±std min max Prms.\nLSTM 256 1 93.2 ± 6.1 84.7 98.5 0.4\n512 97.7 ± 1.1 96.1 98.7 1.3\nDelta Net 256 4 61.4 ±20.0 26.2 85.7 3.2\nDelta RNN 85.1 ± 1.9 83.1 88.6 3.7\nRDN 76.3 ±17.6 40.2 92.5 3.2\nDelta Net 128 8 62.7 ±36.3 0.1 97.3 1.1\nDelta RNN 94.1 ± 2.7 88.0 95.8 1.3\nRDN 85.0 ± 3.8 78.9 89.0 1.1\nB.3 Task Details for Sequential ListOps\nThe ListOps task [ 30] consists of list operation execution which is a typical test for hierarchical\nstructure learning. A list is constructed using elementary list-operations written in preﬁx notation\n(typically one of six operations: maximum, minimum, median followed by ﬂoor operation, sum\nmodulo 10, ﬁrst and last element retrieval) with a random number of random arguments chosen to be\neither a single digit integer or a sub-list which itself has random arguments. While early research\ncomparing self-attention to RNNs [ 58] has shown some advantages of recurrence in hierarchical\nstructure learning, more recent work [59] reports Transformers to also outperform LSTMs on ListOps.\nAlso relevant here, Tay et al. [22] report linear Transformer variants (Linear Transformers and\nPerformers) to underperform other Transformer variants by a large margin on ListOps. It is thus\nnatural to evaluate our models on this task as models at the intersection of recurrent and self-attention\nbased models. We construct a simple variant of ListOps which only makes use of maximum MAX,\nminimum MIN, and ﬁrst element retrieval FIRST operations. This turns out to be hard enough to shed\nlight on the differences between our models. By construction, the targets are single digit integers.\nThe number of arguments in each list or sub-list is random but less than the pre-determined maximum\nnumber (here set to ﬁve, following Nangia and Bowman [30]) and we control the difﬁculty of the\ntask by changing the problem depth. Here is a depth-two example:\nIn: [MAX 6 1 [FIRST 2 3 ] 0 [MIN 4 7 1] ]\nOut: 6\nIn our setting, the task with depth 10 only contains sequences with depth 105. Here, we refer to the\ntask as “sequential ListOps”, as we let the model read the sequence only once from left to right in\nan auto-regressive fashion. As for the code execution experiments, we randomly generate 10,000\nsequences for training and 1,000 sequences each for validation and test. The lengths for the depth 10\ncase vary from 37 to 364 with an average length of 98 tokens. For the depth 15 case, the lengths are\nbetween 61 and 676, with an average of about 190 tokens. All experiments were conducted using a\nsingle 16 GB P100 GPU. We use the same experimental settings as in the code execution task, and\nthe experiments for depth 10 and 15 take less than 4 and 16 hours, respectively.\nB.4 Ablation Study for the LSTM on Sequential ListOps\nWhile the main goal of Table 2 (Sec. 4.3) was to compare different fast weight programmer variants\nunder the same model conﬁgurations, we also pointed out that the performance of the baseline LSTM\ndramatically drops for the sequential ListOps task by increasing the list depth from 10 to 15. In\nSec. 4.3, we hypothesised the reason for the performance drop of the LSTM for the depth-15 case of\nsequential ListOps to be the small hidden size of the LSTM and the increase of sequence lengths in\n5However, here the depth is simply deﬁned as the depth of nested operations. Since the used operations do\nnot always have to evaluate all arguments to obtain the result, the effective computation may be shallower. This\nproblem has been addressed in a better version of ListOps in our more recent work [82].\n20\nthe depth-15 case. Here we provide the corresponding ablation study. Table 7 shows the performance\nof the LSTM with different hidden layer sizes. We ﬁnd that increasing the hidden size effectively\nhelp the LSTM on this task.\nTable 7: Test accuracies (%) with standard deviations over three runs for the LSTM on the depth-15\ncase of Sequential ListOps.\nHidden size Mean accuracy ±std\n256 24.4 ± 1.1\n1,024 24.4 ± 0.7\n2,048 35.9 ±13.0\n4,096 72.2 ± 1.6\nC Experimental Details and Additional Results for RL in Atari 2600\nSettings. We use the polybeast implementation from Torchbeast [67] with modiﬁcations lim-\nited to model architectures. We train all our models using RMSProp [ 83] with a learning rate of\n0.0006, an epsilon of 0.01, and gradient clipping at 40. We use entropy regularisation with a weight of\n0.01. The backpropagation span is 50 and the batch size is 32. The model architecture and evaluation\nmethod is described in the main text. All Transformer variants make use of pre-activation residual\nconnections [52, 12] and layer norm [54]. The number of actors for IMPALA is 48. No action repeat\nis used. No time limit is set for evaluation. Rewards are clipped between -1 and 1. The OpenAI Gym\nimplementation of the Atari learning environment [84] is used. The only source of stochasticity is the\ndefault sticky action. We train expert models using the game speciﬁc action spaces (models for Ami-\ndar and James Bond were trained with an action space size of 6, which is smaller than the full action\nspace but enough to play these games). We train on 2 GPUs (either 16 GB P100 or 32 GB V100). An\nexperiment for one game takes about 1.5 days. Evaluation is done at 50 M and 200 M environmental\nsteps, which are reported in Table 9 and 10. For cases where performance did not improve after 50 M\nand 200 M, we report the performance at 50 M again in Table 10 (we experienced this forBank Heist\nand Robotank; for Pong 50 M steps are enough to consistently achieve the perfect score).\nIn what follows, we provide additional model comparisons.\nFeedforward vs. LSTM. On Atari, models without recurrence are also known to perform well in\nmany environments [68]. Since it is not easy to compare RL systems across different settings [85],\nwe train our own feedforward baseline. The feedforward baseline is simply obtained by removing the\nLSTM layer in the LSTM model, which corresponds to the model of Espeholt et al. [65]. At 50 M\nsteps (Figure 5; orange), there are 8 games in which the feedforward baseline clearly outperforms the\nLSTM, and in 8 other games the trend is reversed. At 200 M steps (Figure 6; orange), the LSTM\nperforms clearly better in 10 games, whereas the feedforward net clearly dominates only in 4 games.\nFeedforward baseline with more parameters. In the comparison above, the LSTM baseline has\n1.6 M parameters, more than the 1.1 M parameters of the feedforward baseline (while we note that the\nRDN has slightly fewer parameters than the LSTM, namely, 1.5 M). To verify that the improvements\nobtained by the LSTM are not due to the increased parameter count, we build a larger feedforward\nbaseline with 1.7 M parameters by replacing the LSTM layer by one feedforward highway-gated\nlayer [53] (to keep it as similar as possible to the LSTM baseline). Here the output from the vision\nstem is ﬁrst projected to a 320-dimensional vector which is followed by a 256-dimensional highway-\ngated layer. We evaluate this model on four environments on which the LSTM outperforms the 1.1\nM-param feedforward baseline. Table 8 shows the corresponding results. The extra parameters yield\nimprovements only on S. Invader, without matching LSTM’s performance. So we can conﬁrm that\nthe dominance of LSTM over feedforward models in these games is not simply due to the higher\nparameter count.\nRecurrent Delta Net vs. Delta Net. We also compare the Recurrent Delta Net to a stronger\nbaseline, the Delta Net. The results are shown in Figures 7 and 8 (sky blue). While the RDN performs\nequally well or better than the baseline Delta Net on 13 games at 200 M steps, there are also 7 games\n21\nTable 8: Performance of feedforward baseline with more parameters.\nParams. Berzerk Gopher Seaquest S. Invader\nLSTM 1.5 M 1,150 ± 92 124,914 ± 22,422 12,643 ± 1,627 137,657 ± 2,276\nFF 1.1 M 343 ± 23 61,350 ± 3,891 667 ± 1 53,455 ± 6,694\nFF gated 1.7 M 320 ± 29 42,851 ± 7,653 660 ± 0 95,629 ± 11,991\nwhere the Delta Net is better. We thus can not guarantee strict beneﬁts of additional recurrence here.\nAgain we note that compared to other models, both the Delta Net and Recurrent Delta Net achieve\noutstanding performance on Q*Bert.\nDelta RNN vs. LSTM. We also evaluate the Delta RNN (Sec. 3.1) in this RL setting. We ﬁrst\ncompare it to the LSTM baseline. As shown in Figures 9 and 10 ( green), the Delta RNN clearly\noutperforms the LSTM on a few games at 50 M steps. However, the performance gaps reduce across\nall games after 200 M steps. Overall, the performance is close in 7 games, in favour of the LSTM in\n8 games, and in favour of the Delta RNN in 5 games.\nRecurrent Delta Net vs. Delta RNN. Finally, we also compare the Recurrent Delta Net to the\nDelta RNN. Figures 11 and 12 (grey) present our results. In 16 games, the relative performance gap\nis within 50%. In one game (Seaquest), the Delta RNN outperforms the RDN. In 3 games, the RDN\nclearly outperforms the Delta RNN at 200 M steps.\nOverall, the Recurrent Delta Net tends to yield decent performance compared to all baselines. While\nthe performance gaps between the Recurrent Delta Net and the Delta RNN are rather close, the\nRecurrent Delta Net performs particularly well in a few games. As mentioned in the main text, trying\ndeeper architectures might be a straight-forward way to obtain better scores.\nD Comments on Nomenclature\nTo simplify references to speciﬁc Fast Weight Programmers, we gave short names to all of them, such\nas Delta RNN or Recurrent Delta Net. We did not cover, however, many other possible combinations\nof slow and fast networks as well as update rules (which are the elementary programming instructions\nof FWPs). This calls for a systematic nomenclature to specify the various FWP types. For a given\nFWP, one could use “slow-net/update-rule\" as a preﬁx and \"fast-net\" architecture as a sufﬁx. For\nexample, the Delta RNN is an FWP with a fast RNN and a feedforward slow net using the delta rule\nas elementary programming instruction. Therefore, using the convention above, the full name of\nthe Delta RNN would be “Feedforward/Delta fast RNN.\" The full name of the Recurrent Delta Net\nwould be “Recurrent/Delta fast Linear Net,\" and so on. This is also compatible with the baseline\nDelta Net, whose full name would be “Feedforward/Delta fast Linear Net.\"\n22\nKung-fuAssault\nSeaquestBerzerkB. RiderGopherAmidar\nRobotank\nJames Bond\nB. Heist\nPong\nMsPacmanStargunner\nD. Attack\nUp'n Down\nQ*BertZaxxon\nBattlezone\nAlien\nS. Invaders\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n  Relative Improvements from the LSTM\nFigure 5: Rel. improvements in test scores\nobtained by the feedforward baseline com-\npared to LSTM after 50 M env. steps.\nSeaquestBerzerk\nS. Invaders\nGopherKung-Fu\nAlien\nMsPacman\nQ*Bert\nUp'n Down\nAssault\nStargunner\nPong\nB. HeistRobotank\nZaxxonD. AttackB. RiderAmidar\nJames Bond\nBattlezone\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n  Relative Improvements from the LSTM\nFigure 6: Rel. improvements in test scores\nobtained by the feedforward baseline com-\npared to LSTM after 200 M env. steps.\nSeaquest\nAlien\nStargunnerJames Bond\nQ*BertB. Heist\nPong\nBerzerk\nRobotankUp'n DownS. Invaders\nAmidar\nBattlezone\nKung-fu\nMsPacman\nZaxxonD. AttackAssaultB. RiderGopher\n1.0\n0.5\n0.0\n0.5\n1.0\n  Relative Improvements from the Delta Net\nFigure 7: Rel. improvements in test scores ob-\ntained by the Recurrent Delta Net compared\nto the Delta Net after 50 M env. steps.\nSeaquestBattlezoneJamesBond\nAlien\nB. RiderD. AttackZaxxon\nMsPacman\nGopher\nPong\nB. HeistBerzerk\nRobotankStargunner\nAssault\nUp'n Down\nKung-Fu\nQ*BertAmidar\nS. Invaders\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n  Relative Improvements from the Delta Net\nFigure 8: Rel. improvements in test scores ob-\ntained by the Recurrent Delta Net compared\nto the Delta Net after 200 M env. steps.\nS. Invaders\nKung-fuBerzerk\nRobotank\nAmidar\nUp'n DownJames Bond\nB. RiderB. HeistAssault\nPong\nBattlezoneStargunner\nD. Attack\nMsPacman\nZaxxonQ*BertGopher\nAlien\nSeaquest\n1\n0\n1\n2\n3\n4\n5\n6\n  Relative Improvements from the LSTM\nFigure 9: Rel. improvements in test scores\nobtained by the Delta RNN compared to the\nLSTM after 50 M env. steps.\nBerzerk\nS. Invaders\nKung-Fu\nUp'n DownBattlezone\nAmidarGopherQ*Bert\nStargunner\nAssaultZaxxon\nRobotank\nPong\nB. Heist\nMsPacmanSeaquestD. AttackB. Rider\nAlien\nJamesBond\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n  Relative Improvements from the LSTM\nFigure 10: Rel. improvements in test scores\nobtained by the Delta RNN compared to the\nLSTM after 200 M env. steps.\n23\n8\n9\n10\nSeaquest\nAlien\nGopher\nUp'n Down\nQ*BertB. Heist\nPong\nJames BondStargunnerBattlezoneRobotank\nAssaultBerzerk\nMsPacman\nD. AttackKung-fuB. RiderZaxxonAmidar\nS. Invaders\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n                      Relative Improvements from the Delta RNN\nFigure 11: Rel. improvements in test scores\nobtained by the Recurrent Delta Net com-\npared to the Delta RNN after 50 M env. steps.\n90\n95\n100\nSeaquest\nJamesBondBattlezone\nD.AttackZaxxonB.RiderGopher\nAlien\nB.Heist\nPong\nBerzerk\nRobotankStargunner\nAssault\nMsPacman\nKung-FuUp'nDownS.Invaders\nAmidarQ*Bert\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n                      Relative Improvements from the Delta RNN\nFigure 12: Rel. improvements in test\nscores obtained by the Recurrent Delta Net\ncompared to the Delta RNN after 200 M\nenv. steps.\nTable 9: Performance after 50 M environmental steps. Reported scores are mean and std of 5 mean-\nscores obtained over 30 episodes (total of 150 different test episodes). We remind the reader that we\ndenote the Linear Transformer [19] as LT, and our Recurrent Delta Network as RDN. The numbers\nof parameters are: 1.1 M for the feedforward model, 1.6 M for the LSTM, 1.5 M for the Linear\nTransformer, the Delta Net, and the Recurrent Delta Net, and ﬁnally 1.6 M for the Delta RNN.\nFeedforward LSTM LT Delta Net RDN Delta RNN\nAlien 1,985 ± 90 846 ± 81 2,135 ± 184 4,704 ± 452 1,754 ± 48 3,420 ± 834\nAmidar 208 ± 11 233 ± 10 320 ± 16 339 ± 28 368 ± 23 216 ± 14\nAssault 4,658 ± 2,147 7,551 ± 1,774 2,764 ± 380 5,710 ± 2,643 8,088 ± 2,851 7,503 ± 2,794\nBattlezone 12,267 ± 620 6,327 ± 380 933 ± 351 6,780 ± 461 7,373 ± 431 7,040 ± 1,098\nBerzerk 326 ± 21 474 ± 17 323 ± 6 331 ± 24 336 ± 27 305 ± 8\nB. Heist 323 ± 13 327 ± 11 309 ± 11 321 ± 8 316 ± 10 324 ± 10\nB. Rider 9,932 ± 1,592 13,638 ± 1,571 6,695 ± 941 9,185 ± 630 18,156 ± 1,522 13,429 ± 884\nD. Attack 36,255 ± 3,566 31,447 ± 1,850 8,939 ± 950 31,359 ± 3,362 41,726 ± 6,308 36,807 ± 3,700\nGopher 10,356 ± 378 13,765 ± 808 8,197 ± 1,720 8,707 ± 2,381 19,775 ± 1,448 25,445 ± 1,963\nJames Bond 2,942 ± 56 3,020 ± 68 2,425 ± 174 3,338 ± 137 2,979 ± 176 2,929.3 ± 408\nKung-fu 5,449 ± 82 15,216 ± 818 3,722 ± 330 8,095 ± 240 9,201 ± 384 7,388 ± 491\nMsPacman 1,737 ± 53 1,676 ± 86 1,647 ± 101 2,116 ± 30 2,584 ± 121 2,287 ± 32\nPong 21 ± 0 21 ± 0 21 ± 0 21 ± 0 21 ± 0 21 ± 0\nQ*Bert 4,967 ± 266 3,905 ± 252 4,693 ± 195 6,248 ± 204 5,897 ± 357 6,626 ± 240\nRobotank 7.1 ± 0.7 7.6 ± 0.7 4.8 ± 0.3 7.2 ± 0.7 7.5 ± 0.8 7.0 ± 0.5\nSeaquest 469 ± 1 708 ± 1 1,812 ± 61 8,853 ± 937 686 ± 1 5,123 ± 335\nS. Invaders 48,150 ± 7,233 12,461 ± 1,624 2,345 ± 74 25,769 ± 10,156 27,213 ± 3,359 2,847 ± 10\nStargunner 9,397 ± 2,193 8,337 ± 1,094 8,915 ± 713 11,599 ± 3,454 9,737 ± 1,396 9,523 ± 2,214\nUp’n down 185,632 ± 16,490 155,847 ± 15,318 57,435 ± 2,283 120,806 ± 16,261 126,140 ± 19,078 148,759 ± 28,492\nZaxxon 4863 ± 872 2,737 ± 121 2,719 ± 701 4,265 ± 263 5,285 ± 504 3,903 ± 648\n24\nTable 10: Performance after 200 M environment steps. Reported scores are mean and std of 5 mean-\nscores obtained over 30 episodes (total of 150 different test episodes). We remind the reader that we\ndenote the Linear Transformer [19] as LT, and our Recurrent Delta Network as RDN. In cases where\nperformance did not improve after 50 M, we report the performance at 50 M.\nFeedforward LSTM LT Delta Net RDN Delta RNN\nAlien 3,816 ± 139 6,184 ± 558 4,751 ± 530 15,133 ± 1,122 11,220 ± 621 12,177 ± 968\nAmidar 433 ± 27 349 ± 22 646 ± 32 432 ± 27 832 ± 11 269 ± 17\nAssault 6,407 ± 3,430 7,977 ± 2,611 6,465 ± 1,437 7,525 ± 1,703 8,647 ± 3,061 7,670 ± 952\nBattlezone 60,527 ± 12,345 24,873 ± 1,240 2,667 ± 386 19,907 ± 1,409 10,980 ± 1,104 17,180 ± 1,493\nBerzerk 343 ± 23 1,150 ± 92 480 ± 38 333 ± 7 348 ± 17 332 ± 17\nB. Heist 331 ± 10 327 ± 11 317 ± 8 321 ± 8 328 ± 10 328 ± 8\nB. Rider 21,873 ± 2,000 18,024 ± 933 22,444 ± 755 28,594 ± 5,508 23,934 ± 2,292 28,973 ± 3,663\nD. Attack 74,904 ± 10,941 69,750 ± 9,593 57,715 ± 5,009 78,601 ± 16,907 67,039 ± 5,714 92,205 ± 17,933\nGopher 61,350 ± 3,891 124,914 ± 22,422 48,261 ± 7,727 86,168 ± 5,069 86,008 ± 11,815 101,974 ± 10,200\nJames Bond 56,459 ± 7,292 25,106 ± 5,889 16,223 ± 1,118 54,336 ± 7,165 32,923 ± 7,968 53,344 ± 4,768\nKung-fu 12,292 ± 613 24,447 ± 407 13,969 ± 803 15,064 ± 929 20,319 ± 363 15,068 ± 513\nMsPacman 2,499 ± 141 3,431 ± 197 3,052 ± 128 4,180 ± 139 4,168 ± 585 3,500 ± 205\nPong 21 ± 0 21 ± 0 21 ± 0 21 ± 0 21 ± 0 21 ± 0\nQ*Bert 8,655 ± 371 11,513 ± 910 8,389 ± 349 521,839 ± 36,192 987,275 ± 0 10,381 ± 1,259\nRobotank 7.8 ± 0.8 7.6 ± 0.7 7.7 ± 0.9 7.5 ± 0.4 7.9 ± 0.6 7.5 ± 0.5\nSeaquest 667 ± 1 12,643 ± 1,627 12,425 ± 1,910 12,790 ± 1,512 4,373 ± 504 13,898 ± 1,674\nS. Invaders 53,455 ± 6,694 137,657 ± 2,276 2,333 ± 110 86,132 ± 5,483 170,871 ± 80 58,181 ± 14,987\nStargunner 11,564 ± 4,598 12,194 ± 7,038 12,035 ± 6,995 11,734 ± 6,827 13,026 ± 6,431 11,635 ± 6,065\nUp’n down 185,632 ± 16,490 231,157 ± 10,603 252,555 ± 16,331 208,563 ± 22,803 240,003 ± 26,849 159,296 ± 25,013\nZaxxon 11,960 ± 538 11,619 ± 663 7,371 ± 932 1,0523 ± 568 9,126 ± 313 11,365 ± 678\n25",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8153243064880371
    },
    {
      "name": "Computer science",
      "score": 0.7035092711448669
    },
    {
      "name": "Artificial neural network",
      "score": 0.5146647095680237
    },
    {
      "name": "Reinforcement learning",
      "score": 0.4802111089229584
    },
    {
      "name": "Feed forward",
      "score": 0.4734964966773987
    },
    {
      "name": "Scalability",
      "score": 0.45811527967453003
    },
    {
      "name": "Architecture",
      "score": 0.45500823855400085
    },
    {
      "name": "Nonlinear system",
      "score": 0.41385698318481445
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3849654197692871
    },
    {
      "name": "Control engineering",
      "score": 0.24484160542488098
    },
    {
      "name": "Electrical engineering",
      "score": 0.13367387652397156
    },
    {
      "name": "Engineering",
      "score": 0.10003751516342163
    },
    {
      "name": "Voltage",
      "score": 0.08194813132286072
    },
    {
      "name": "Operating system",
      "score": 0.07732948660850525
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": []
}