{
  "title": "Data Augmentation using Pre-trained Transformer Models",
  "url": "https://openalex.org/W3010293452",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2031363001",
      "name": "Kumar, Varun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4289000521",
      "name": "Choudhary, Ashutosh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2295629665",
      "name": "Cho Eun-Ah",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970295111",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2803609229",
    "https://openalex.org/W2885185669",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2971252690",
    "https://openalex.org/W2922917409",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2911588830",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W2979308242",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2038721957",
    "https://openalex.org/W2963545917",
    "https://openalex.org/W2952444318",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2905266130"
  ],
  "abstract": "Language model based pre-trained models such as BERT have provided significant gains across different NLP tasks. In this paper, we study different types of transformer based pre-trained models such as auto-regressive models (GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional data augmentation. We show that prepending the class labels to text sequences provides a simple yet effective way to condition the pre-trained models for data augmentation. Additionally, on three classification benchmarks, pre-trained Seq2Seq model outperforms other data augmentation methods in a low-resource setting. Further, we explore how different pre-trained model based data augmentation differs in-terms of data diversity, and how well such methods preserve the class-label information.",
  "full_text": "Data Augmentation Using Pre-trained Transformer Models\nVarun Kumar\nAlexa AI\nkuvrun@amazon.com\nAshutosh Choudhary\nAlexa AI\nashutoch@amazon.com\nEunah Cho\nAlexa AI\neunahch@amazon.com\nAbstract\nLanguage model based pre-trained models\nsuch as BERT have provided signiﬁcant gains\nacross different NLP tasks. In this paper, we\nstudy different types of transformer based pre-\ntrained models such as auto-regressive models\n(GPT-2), auto-encoder models (BERT), and\nseq2seq models (BART) for conditional data\naugmentation. We show that prepending the\nclass labels to text sequences provides a simple\nyet effective way to condition the pre-trained\nmodels for data augmentation. Additionally,\non three classiﬁcation benchmarks, pre-trained\nSeq2Seq model outperforms other data aug-\nmentation methods in a low-resource setting.\nFurther, we explore how different data aug-\nmentation methods using pre-trained model\ndiffer in-terms of data diversity, and how well\nsuch methods preserve the class-label informa-\ntion.\n1 Introduction\nData augmentation (DA) is a widely used technique\nto increase the size of the training data. Increas-\ning training data size is often essential to reduce\noverﬁtting and enhance the robustness of machine\nlearning models in low-data regime tasks.\nIn natural language processing (NLP), several\nword replacement based methods have been ex-\nplored for data augmentation. In particular, Wei\nand Zou (2019) showed that simple word replace-\nment using knowledge bases like WordNet (Miller,\n1998) improves classiﬁcation performance. Further,\nKobayashi (2018) utilized language models (LM)\nto augment training data. However, such methods\nstruggle with preserving class labels. For example,\nnon-conditional DA for an input sentence of senti-\nment classiﬁcation task “a small impact with a big\nmovie” leads to “a small movie with a big impact”.\nUsing such augmented data for training, with the\noriginal input sentence’s label (i.e. negative senti-\nment in this example) would negatively impact the\nperformance of the resulting model.\nTo alleviate this issue, Wu et al. (2019) pro-\nposed conditional BERT (CBERT) model which\nextends BERT (Devlin et al., 2018) masked lan-\nguage modeling (MLM) task, by considering class\nlabels to predict the masked tokens. Since their\nmethod relies on modifying BERT model’s seg-\nment embedding, it cannot be generalized to other\npre-trained LMs without segment embeddings.\nSimilarly, Anaby-Tavor et al. (2019) used\nGPT2 (Radford et al., 2019) for DA where exam-\nples are generated for a given class by providing\nclass as input to a ﬁne-tuned model. In their work,\nGPT2 is used to generate10 times the number of ex-\namples required for augmentation and then the gen-\nerated sentences are selected based on the model\nconﬁdence score. As data selection is applied only\nto GPT2 but not to the other models, the augmen-\ntation methods can not be fairly compared. Due\nto such discrepancies, it is not straightforward to\ncomprehend how the generated data using different\npre-trained models varies from each other and their\nimpact on downstream model performance.\nThis paper proposes a uniﬁed approach to use\nany pre-trained transformer (Vaswani et al., 2017)\nbased models for data augmentation. In particu-\nlar, we explore three different pre-trained model\ntypes for DA, including 1) an autoencoder (AE)\nLM: BERT, 2) an auto-regressive (AR) LM: GPT2,\nand 3) a pre-trained seq2seq model: BART (Lewis\net al., 2019). We apply the data generation for three\ndifferent NLP tasks: sentiment classiﬁcation, intent\nclassiﬁcation, and question classiﬁcation.\nIn order to understand the signiﬁcance of DA,\nwe simulate a low-resource data scenario, where\nwe utilize only 10 training examples per class in a\nclassiﬁcation task. Section 3.2 provides details of\nthe task and corpora.\nWe show that all three types of pre-trained mod-\narXiv:2003.02245v2  [cs.CL]  31 Jan 2021\nels can be effectively used for DA, and using the\ngenerated data leads to improvement in classiﬁ-\ncation performance in the low-data regime setting.\nAmong three types of methods, pre-trained seq2seq\nmodel provides the best performance. Our code is\navailable at 1.\nOur contribution is three-fold: (1) implementa-\ntion of a seq2seq pre-trained model based data aug-\nmentation, (2) experimental comparison of differ-\nent data augmentation methods using conditional\npre-trained model, (3) a uniﬁed data augmentation\napproach with practical guidelines for using differ-\nent types of pre-trained models.\n2 DA using Pre-trained Models\nLM pre-training has been studied extensively (Rad-\nford et al., 2018; Devlin et al., 2018; Liu et al.,\n2019). During pre-training, such models are either\ntrained in an AE setting or in an AR setting. In\nthe AE setting, certain tokens are masked in the\nsentence and the model predicts those tokens. In\nan AR setting, the model predicts the next word\ngiven a context. Recently, pre-training for seq2seq\nmodel has been explored where a seq2seq model is\ntrained for denoising AE tasks (Lewis et al., 2019;\nRaffel et al., 2019). Here, we explore how these\nmodels can be used for DA to potentially improve\ntext classiﬁcation accuracy.\nAlgorithm 1: Data Augmentation ap-\nproach\nInput :Training Dataset Dtrain\nPretrained model G∈{AE,AR,Seq2Seq}\n1 Fine-tune G using Dtrain to obtain Gtuned\n2 Dsynthetic←{}\n3 foreach {xi,yi}∈Dtrain do\n4 Synthesize s examples {ˆxi, ˆyi}1\np using\nGtuned\n5 Dsynthetic←Dsynthetic∪{ˆxi, ˆyi}1\np\n6 end\nDA Problem formulation : Given a training\ndataset Dtrain = {xi, yi}1\nn, where xi = {wj}1\nm\nis a sequence of m words, yi is the associated label,\nand a pre-trained model G, we want to generate\na dataset of Dsynthetic. Algorithm 1 describes the\ndata generation process. For all augmentation meth-\nods, we generate s = 1synthetic example for every\n1https://github.com/varinf/\nTransformersDataAugmentation\nexample in Dtrain. Thus, the augmented data is\nsame size as the size of the original data.\n2.1 Conditional DA using Pre-trained LM\nFor conditional DA, a model G incorporates label\ninformation during ﬁne-tuning for data generation.\nWu et al. (2019) proposed CBERT model where\nthey utilized BERT’s segment embeddings to con-\ndition model on the labels. Similarly, models can\nbe conditioned on labels by prepending labels yi to\nxi (Keskar et al., 2019; Johnson et al., 2017).\nDue to segment embedding reuse, CBERT con-\nditioning is very speciﬁc to BERT architecture thus\ncannot be applied directly to other pre-trained LMs.\nThus, we compare two generic ways to condition a\npre-trained model on class label:\n• prepend : prepending label yi to each se-\nquence xi in the training data without adding\nyi to model vocabulary\n• expand : prepending label yi to each se-\nquence xi in the training data and adding yi\nto model vocabulary.\nNote that in prepend, the model may split yi\ninto multiple subword units (Sennrich et al., 2015b;\nKudo and Richardson, 2018), expand treats a la-\nbel as a single token.\nHere, we discuss the ﬁne-tuning and the data\ngeneration process for both AE and AR LMs. For\ntransformer based LM implementation, we use Py-\ntorch based transformer package (Wolf et al., 2019).\nFor all pre-trained models, during ﬁne-tuning, we\nfurther train the learnable parameters of G using\nits default task and loss function.\n2.1.1 Fine-tuning and generation using AE\nLMs\nWe choose BERT as a representative of AE models.\nFor ﬁne-tuning, we use the default masking param-\neters and MLM objective which randomly masks\nsome of the tokens from the raw sequence, and\nthe objective is to predict the original token of the\nmasked words using the context. Both BERTprepend\nand BERTexpand models are ﬁne-tuned using the\nsame objective.\n2.1.2 Fine-tuning and generation using AR\nLMs\nFor AR LM experiments, we choose GPT2 as a gen-\nerator model and follow the method proposed by\nAnaby-Tavor et al. (2019) to ﬁne-tune and generate\ndata. For ﬁne-tuning GPT2, we create a training\ndataset by concatenating all sequences in Dtrain\nas follows: y1SEPx 1EOSy2...ynSEPx nEOS.\nSEP denotes a separation token between label and\nsentence, and EOS denotes the end of a sentence.\nFor generating data, we provide yiSEP as a\nprompt to G, and we keep generating until the\nmodel produces EOS token. We use GPT2 to re-\nfer to this model. We found that such generation\nstruggles in preserving the label information, and\na simple way to improve the generated data label\nquality is to provide an additional context toG. For-\nmally, we provide yiSEPw 1..wk as prompt where\nw1..wk are the ﬁrst k words of a sequence xi. In\nthis work, we use k = 3. We call this method\nGPT2context.\n2.2 Conditional DA using Pre-trained\nSeq2Seq model\nLike pre-trained LM models, pre-training seq2seq\nmodels such as T5 (Raffel et al., 2019) and\nBART (Lewis et al., 2019) have shown to improve\nperformance across NLP tasks. For DA experi-\nments, we choose BART as a pre-trained seq2seq\nmodel representative for its relatively lower com-\nputational cost.\n2.2.1 Fine-tuning and generation using\nSeq2Seq BART\nSimilar to pre-trained LMs, we condition BART\nby prepending class labels to all examples of a\ngiven class. While BART can be trained with dif-\nferent denoising tasks including insertion, deletion,\nand masking, preliminary experiments showed that\nmasking performs better than others. Note that\nmasking can be applied at either word or subword\nlevel. We explored both ways of masking and\nfound subword masking to be consistently infe-\nrior to the word level masking. Finally, we applied\nword level masking in two ways:\n• BARTword : Replace a word wi with a mask\ntoken < mask >\n• BARTspan: Replace a continuous chunk of\nk words wi, wi+1..wi+k with a single mask\ntoken < mask >.\nMasking was applied to 40% of the words. We\nﬁne-tune BART with a denoising objective where\nthe goal is to decode the original sequence given a\nmasked sequence.\n2.3 Pre-trained Model Implementation\n2.3.1 BERT based DA models\nFor AutoEncoder (AE) experiments, we use “bert-\nbase-uncased” model with the default parameters\nprovided in huggingface’s transformer package. In\nprepend setting we train model for 10 epochs\nand select the best performing model on dev data\npartition keeping initial learning rate at 4e−5. For\nexpand setting, training requires 150 epochs to\nconverge. Moreover, a higher learning rate of\n1.5e−4 was used for all three datasets. The initial\nlearning rate was adjusted for faster convergence.\nThis is needed for expand setting as embeddings\nfor labels are randomly initialized.\n2.3.2 GPT2 model implementation\nFor GPT2 experiments, we use GPT2-Small model\nprovides in huggingface’s transformer package. We\nuse default training parameters to ﬁne-tune the\nGPT2 model. For all experiments, we use SEP\nas a separate token and <| endoftext |> as EOS\ntoken. For text generation, we use the default nu-\ncleus sampling (Holtzman et al., 2019) parameters\nincluding top k = 0, and top p = 0.9.\n2.3.3 BART model implementation\nFor BART model implementation, we use fairseq\ntoolkit (Ott et al., 2019) implementation of BART.\nAdditionally, we used bart large model weights2.\nSince BART model already contains< mask >\ntoken, we use it to replace mask words. For BART\nmodel ﬁne-tuning, we use denoising reconstruc-\ntion task where 40% words are masked and the\ngoal of the decoder is to reconstruct the original se-\nquence. Note that the label yi is prepended to each\nsequence xi, and the decoder also produces the la-\nbel yi as any other token in xi. We use fairseq’sla-\nbel smoothed cross entropy criterion with a label-\nsmoothing of 0.1. We use 1e−5 as learning rate.\nFor generation, beam search with a beam size of 5\nis used.\n2.4 Base classiﬁer implementation\nFor the text classiﬁer, we use “bert-base-uncased”\nmodel. The BERT model has 12 layers, 768 hidden\nstates, and 12 heads. We use the pooled represen-\ntation of the hidden state of the ﬁrst special token\n([CLS]) as the sentence representation. A dropout\nprobability of 0.1 is applied to the sentence rep-\nresentation before passing it to the Softmax layer.\n2https://dl.fbaipublicfiles.com/\nfairseq/models/bart.large.tar.gz\nAdam (Kingma and Ba, 2014) is used for optimiza-\ntion with an initial learning rate of 4e−5. We use\n100 warmup steps for BERT classiﬁer. We train the\nmodel for 8 epochs and select the best performing\nmodel on the dev data.\nAll experiments were conducted using a sin-\ngle GPU instance of Nvidia Tesla v100 type. For\nBART model, we use f16 precision. For all data\naugmentation models, validation set performance\nwas used to select the best model.\n3 Experimental Setup\n3.1 Baseline Approaches for DA\nIn this work, we consider three data augmentation\nmethods as our baselines.\n(1) EDA (Wei and Zou, 2019) is a simple word-\nreplacement based augmentation method, which\nhas been shown to improve text classiﬁcation per-\nformance in the low-data regime.\n(2) Backtranslation (Sennrich et al., 2015a)\nis another widely used text augmentation\nmethod (Shleifer, 2019; Xie et al., 2019; Edunov\net al., 2018). For backtranslation, we use a\npre-trained EN-DE 3, and DE-EN 4 translation\nmodels (Ng et al., 2019).\n(3) CBERT (Wu et al., 2019) language model\nwhich, to the best of our knowledge, is the latest\nmodel-based augmentation that outperforms other\nword-replacement based methods.\n3.2 Data Sets\nWe use three text classiﬁcation data sets.\n(1) SST-2 (Socher et al., 2013): (Stanford Senti-\nment Treebank) is a dataset for sentiment classiﬁ-\ncation on movie reviews, which are annotated with\ntwo labels (Positive and Negative).\n(2) SNIPS (Coucke et al., 2018) dataset contains\n7 intents which are collected from the Snips per-\nsonal voice assistant.\n(3) TREC (Li and Roth, 2002) is a ﬁne-grained\nquestion classiﬁcation dataset sourced from TREC.\nIt contains six question types (whether the question\nis about person, location, etc.).\nFor SST-2 and TREC, we use the dataset ver-\nsions provided by (Wu et al., 2019) 5, and for\n3https://dl.fbaipublicfiles.com/\nfairseq/models/wmt19.en-de.joined-dict.\nsingle_model.tar.gz\n4https://dl.fbaipublicfiles.com/\nfairseq/models/wmt19.de-en.joined-dict.\nsingle_model.tar.gz\n5https://github.com/1024er/cbert_aug\nSNIPS dataset, we use 6. We replace numeric class\nlabels with their text versions. For our experiments,\nwe used the labels provided in Table 1. Note that\npre-trained methods rely on different byte pair en-\ncodings that might split labels into multiple tokens.\nFor all experiments, we use the lowercase version\nof the class labels.\n3.2.1 Low-resourced data scenario\nFollowing previous works to simulate the low-data\nregime setting for text classiﬁcation (Hu et al.,\n2019), we subsample a small training set on each\ntask by randomly selecting an equal number of\nexamples for each class.\nIn our preliminary experiments, we evaluated\nclassiﬁcation performance with various degrees of\nlow-data regime settings, including 10, 50, 100 ex-\namples per class. We observed that state-of-the-art\nclassiﬁers, such as the pre-trained BERT classiﬁer,\nperforms relatively well for these data sets in a\nmoderate low-data regime setting. For example,\nusing 100 training examples per class for SNIPS\ndataset, BERT classiﬁer achieves 94% accuracy,\nwithout any data augmentation. In order to simu-\nlate a realistic low-resourced data setting where we\noften observe poor performance, we focus on ex-\nperiments with 10 and 50 examples per class. Note\nthat using a very small dev set leads the model to\nachieve 100% accuracy in the ﬁrst epoch which\nprevents a fair model selection based on the dev set\nperformance. To avoid this and to have a reliable\ndevelopment set, we select ten validation examples\nper class.\n3.3 Evaluation\nTo evaluate DA, we perform both intrinsic and ex-\ntrinsic evaluation. For extrinsic evaluation, we\nadd the generated examples into low-data regime\ntraining data for each task and evaluate the per-\nformance on the full test set. All experiments are\nrepeated 15 times to account for stochasticity. For\neach experiment, we randomly subsample both\ntraining and dev set to simulate a low-data regime.\nFor intrinsic evaluation, we consider two as-\npects of the generated text. The ﬁrst one is semantic\nﬁdelity, where we measure how well the generated\ntext retains the meaning and the class information\nof the input sentence. In order to measure this, we\ntrain a classiﬁer on each task by ﬁne-tuning a pre-\ntrained English BERT-base uncased model. Section\n6https://github.com/MiuLab/\nSlotGated-SLU/tree/master/data/snips\nData Label Names\nSST-2 Positive, Negative\nTREC Description, Entity, Abbreviation, Human, Location, Numeric\nSNIPS PlayMusic, GetWeather, RateBook, SearchScreeningEvent, SearchCreativeWork, AddTo-\nPlaylist, BookRestaurant\nTable 1: Label Names used for ﬁne-tuning pre-trained models. Label names are lower-cased for all experiments.\nSST-2 SNIPS TREC\nTrain 6,228 13,084 5,406\nDev 692 700 546\nTest 1,821 700 500\nTable 2: Data statistics for three corpora, without any\nsub-sampling. This setup is used to train a classiﬁer for\nintrinsic evaluation, as described in Section 3.3. When\nsimulating low-data regime, we sample 10 or 50 train-\ning examples from each category. For testing, we use\nthe full test data.\n3.3.1 describes corpus and classiﬁer performance\ndetails.\nAnother aspect we consider is text diversity. To\ncompare different models’ ability to generate di-\nverse output, we measured type token ratio (Roem-\nmele et al., 2017). Type token ratio is calculated\nby dividing the number of unique n-grams by the\nnumber of all n-grams in the generated text.\n3.3.1 Classiﬁers for intrinsic evaluation\nIn this work, we measure semantic ﬁdelity by eval-\nuating how well the generated text retains the mean-\ning and the label information of the input sentence.\nTo measure this, we ﬁne-tune the base classiﬁer\ndescribed in Section 2.4.\nTo take full advantage of the labeled data and\nto make our classiﬁer more accurate, we combine\n100% of training and test partitions of the corre-\nsponding dataset, and use the combined data for\ntraining. Then, the best classiﬁer is selected based\non the performance on the dev partition. Classiﬁca-\ntion accuracy of the best classiﬁer on dev partition\nfor each corpus is provided in Table 3.\nSST-2 SNIPS TREC\nDev 91.91 99 94.13\nTable 3: Classiﬁer performance on dev set for each\ncorpus. Classiﬁers are used for intrinsic evaluation.\n4 Results and Discussion\n4.1 Generation by Conditioning on Labels\nAs described in Section 2.1, we choose BERT as\na pre-trained model and explored different ways\nof conditioning BERT on labels: BERT prepend,\nBERTexpand and CBERT.\nTable 4 shows BERT prepend, BERT expand and\nCBERT have similar performance on three datasets.\nNote that BERT is pre-trained on a very huge\ncorpus, but ﬁne-tuning is applied on a limited\ndata. This makes it difﬁcult for the model to learn\nnew, meaningful label representations from scratch\nas in case the BERT expand. While CBERT and\nBERTprepend both converge in less than 8 epochs,\nBERTexpand requires more than 100 epochs to con-\nverge.\nFurther, the class conditioning technique used\nin CBERT is speciﬁc to BERT architecture which\nrelies on modifying BERT’s segment embedding\nand hence cannot be applied to other model archi-\ntectures. Since the labels in most of the datasets are\nwell-associated with the meaning of the class (e.g.\nSearchCreativeWork), prepending tokens allows\nthe model to leverage label information for condi-\ntional word replacement. Given these insights, we\nrecommend prepend as a preferred technique for\npre-trained model based data augmentation.\n4.2 Pre-trained Model Comparison\nClassiﬁcation Performance Table 4 shows that\nseq2seq pre-training based BART outperforms\nother DA approaches on all data sets. We also\nobserve that back translation (shown as BackTrans.\nin table) is a very strong baseline as it consistently\noutperforms several pre-trained data augmentation\ntechniques including CBERT baseline.\nGenerated Data Fidelity As described in Sec-\ntion 3.3.1, we train a classiﬁer for each dataset and\nuse the trained classiﬁer to predict the label of the\ngenerated text.\nTable 5 shows that AE based methods outper-\nform AR models like GPT2, and Seq2seq based\nModel SST-2 SNIPS TREC\nNo Aug 52.93 (5.01) 79.38 (3.20) 48.56 (11.53)\nEDA 53.82 (4.44) 85.78 (2.96) 52.57 (10.49)\nBackTrans. 57.45 (5.56) 86.45 (2.40) 66.16 (8.52)\nCBERT 57.36 (6.72) 85.79 (3.46) 64.33 (10.90)\nBERTexpand 56.34 (6.48) 86.11 (2.70) 65.33 (6.05)\nBERTprepend 56.11 (6.33) 86.77 (1.61) 64.74 (9.61)\nGPT2context 55.40 (6.71) 86.59 (2.73) 54.29 (10.12)\nBARTword 57.97 (6.80) 86.78 (2.59) 63.73 (9.84)\nBARTspan 57.68 (7.06) 87.24 (1.39) 67.30 (6.13)\nTable 4: DA extrinsic evaluation in low-data regime. Results are reported as Mean (STD) accuracy on the full test\nset. Experiments are repeated 15 times on randomly sampled training and dev data. For data augmentation model\nﬁne-tuning, we use 10 examples per class for training.\nModel SST-2 SNIPS TREC\nEDA 95.00 97.14 87.22\nBackTrans. 96.66 97.14 94.88\nCBERT 96.33 97.90 92.22\nBERTexpand 95.00 97.04 91.44\nBERTprepend 96.66 97.80 94.33\nGPT2context 68.33 92.47 60.77\nBARTword 89.33 91.03 79.33\nBARTspan 90.66 93.04 80.22\nTable 5: Semantic ﬁdelity of generated output. We\ntrained a classiﬁer using all labelled data in order to\nperform accuracy test on the generated data. Higher\naccuracy score means that the model retains the class\nlabel of the input sentence more accurately. For data\naugmentation model ﬁne-tuning, we use 10 examples\nper class for training.\nmodel like BART, in terms of semantic ﬁdelity of\nthe generated data. On two datasets, back trans-\nlation approach outperforms all other methods in\nterms of ﬁdelity which underlines the effectiveness\nof the state of the art translation systems in terms\nof preserving the semantics of the language.\nGenerated Data Diversity To further analyze\nthe generated data, we explore type token ratio\nas described in Section 3.3. Table 6 shows that\nEDA generates the most diverse tri-grams and back\ntranslation approach produces the most diverse un-\nigrams. Since EDA method modiﬁes tokens at ran-\ndom, it leads to more diverse n-grams, not neces-\nsarily preserving the semantic of the input sentence.\nAlso, unlike AE and Seq2seq methods that rely on\nword or span replacements, back translation is an\nopen-ended system that often introduces unseen\nunigrams.\nModel SST-2 SNIPS TREC\nn-gram 1 3 1 3 1 3\nEDA 0.66 0.99 0.49 0.97 0.55 0.97\nBackTrans 0.68 0.99 0.51 0.95 0.57 0.96\nCBERT 0.57 0.99 0.48 0.95 0.46 0.95\nBERTexpand 0.59 0.99 0.49 0.96 0.47 0.95\nBERTprepend 0.57 0.99 0.48 0.95 0.46 0.95\nGPT2context 0.62 0.99 0.34 0.88 0.44 0.92\nBARTword 0.53 0.99 0.42 0.95 0.40 0.93\nBARTspan 0.55 0.99 0.41 0.91 0.39 0.89\nTable 6: Type token ratio for generated text using each\nmodel. For data augmentation model ﬁne-tuning, we\nuse 10 examples per class for training.\n4.3 Guidelines For Using Different Types Of\nPre-trained Models For DA\nAE models : We found that simply prepending\nthe label to raw sequences provides competitive\nperformance than modifying the model architec-\nture. As expected, more complex AE models such\nas RoBERTaprepend (Liu et al., 2019) outperforms\nBERTprepend (66.12 vs 64.74 mean acc on TREC).\nAR models : While AR based model such as\nGPT2 produces very coherent text, it does not pre-\nserve the label well. In our experiments, we found\nthat providing a few starting words along with the\nlabel as in GPT2context is crucial to generate mean-\ningful data.\nSeq2Seq models : Seq2Seq models provide an\nopportunity to experiment with various kinds of\ndenoising autoencoder tasks including masking at\nsubword, word or span level, random word inser-\ntion or deletion. We observe that word or span\nmasking performs better than other denoising ob-\njectives, and should be preferred for DA.\nOverall, we found that while AE models are con-\nstrained to produce similar length sequences and\nare good at preserving labels, AR models excel at\nunconstrained generation but might not retain label\ninformation. Seq2Seq models lie between AE and\nAR by providing a good balance between diversity\nand semantic ﬁdelity. Further, in Seq2Seq models,\ndiversity of the generated data can be controlled by\nvarying the masking ratio.\n4.4 Limitations\nOur paper shows that a pre-trained model can be\nused for conditional data augmentation by ﬁne-\ntuning it on the training data where the class labels\nare prepended to the training examples. Such a\nuniﬁed approach allows utilizing different kinds of\npre-trained models for text data augmentation to\nimprove performance in low-resourced tasks. How-\never, as shown in Table 7, improving pre-trained\nclassiﬁer’s model performance in rich-resource set-\nting is still challenging.\nOur results also show that a particular pre-trained\nmodel based augmentation may do well on one task\nor dataset, but may not work well for other scenar-\nios. In our experiments, we use the same set of\nhyperparameters such as masking rate, learning\nrate and warmup schedule for all three datasets\nwhich might not lead to the best performance for\nall considered tasks. While our primary goal in\nthis work is to propose a uniﬁed data augmentation\ntechnique, we believe that further studies on opti-\nmizing performance for a given task or model will\nbe beneﬁcial.\n5 Conclusion And Future Work\nWe show that AE, AR, and Seq2Seq pre-trained\nmodels can be conditioned on labels by prepending\nlabel information and provide an effective way to\naugment training data. These DA methods can be\neasily combined with other advances in text content\nmanipulation such as co-training the data generator\nand the classiﬁer (Hu et al., 2019). Further, the\nproposed data augmentation techniques can also be\ncombined with latent space augmentation (Kumar\net al., 2019). We hope that unifying different DA\nmethods would inspire new approaches for univer-\nsal NLP data augmentation.\nReferences\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,\nAmir Kantor, George Kour, Segev Shlomov, Naama\nTepper, and Naama Zwerdling. 2019. Not enough\ndata? deep learning to the rescue! arXiv preprint\narXiv:1911.03118.\nAlice Coucke, Alaa Saade, Adrien Ball, Th ´eodore\nBluche, Alexandre Caulier, David Leroy, Cl ´ement\nDoumouro, Thibault Gisselbrecht, Francesco Calta-\ngirone, Thibaut Lavril, et al. 2018. Snips voice plat-\nform: an embedded spoken language understanding\nsystem for private-by-design voice interfaces. arXiv\npreprint arXiv:1805.10190.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nSergey Edunov, Myle Ott, M. Auli, and David Grang-\nier. 2018. Understanding back-translation at scale.\nArXiv, abs/1808.09381.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin\nChoi. 2019. The curious case of neural text degener-\nation.\nZhiting Hu, Bowen Tan, Russ R Salakhutdinov, Tom M\nMitchell, and Eric P Xing. 2019. Learning data ma-\nnipulation for augmentation and weighting. In Ad-\nvances in Neural Information Processing Systems,\npages 15738–15749.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Vi´egas, Martin Wattenberg, Greg Corrado,\net al. 2017. Google’s multilingual neural machine\ntranslation system: Enabling zero-shot translation.\nTransactions of the Association for Computational\nLinguistics, 5:339–351.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nSosuke Kobayashi. 2018. Contextual augmentation:\nData augmentation by words with paradigmatic re-\nlations. arXiv preprint arXiv:1805.06201.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226.\nVarun Kumar, Hadrien Glaude, Cyprien de Lichy, and\nWilliam Campbell. 2019. A closer look at feature\nspace data augmentation for few-shot intent classiﬁ-\ncation. arXiv preprint arXiv:1910.04176.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2019. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, trans-\nlation, and comprehension. arXiv preprint\narXiv:1910.13461.\nXin Li and Dan Roth. 2002. Learning question clas-\nsiﬁers. In Proceedings of the 19th international\nconference on Computational linguistics-Volume 1,\npages 1–7. Association for Computational Linguis-\ntics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nGeorge A Miller. 1998. WordNet: An electronic lexical\ndatabase. MIT press.\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,\nMichael Auli, and Sergey Edunov. 2019. Facebook\nfair’s wmt19 news translation task submission. In\nProc. of WMT.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nMelissa Roemmele, Andrew S Gordon, and Reid Swan-\nson. 2017. Evaluating story generation systems us-\ning automated linguistic analyses. In SIGKDD 2017\nWorkshop on Machine Learning for Creativity.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015a. Improving neural machine translation\nmodels with monolingual data. arXiv preprint\narXiv:1511.06709.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015b. Neural machine translation of rare\nwords with subword units. arXiv preprint\narXiv:1508.07909.\nSam Shleifer. 2019. Low resource text classiﬁcation\nwith ulmﬁt and backtranslation.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631–1642.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nJason W Wei and Kai Zou. 2019. Eda: Easy\ndata augmentation techniques for boosting perfor-\nmance on text classiﬁcation tasks. arXiv preprint\narXiv:1901.11196.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nXing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han,\nand Songlin Hu. 2019. Conditional bert contextual\naugmentation. In International Conference on Com-\nputational Science, pages 84–95. Springer.\nQizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Lu-\nong, and Quoc V . Le. 2019. Unsupervised data aug-\nmentation for consistency training.\nA Appendices\nA.1 Classiﬁcation performance on 50\nexamples per class\nModel SST-2 SNIPS TREC\nNum examples 50 50 50\nNo Aug 78.60 (2.81) 90.98 (2.30) 71.65 (11.09)\nEDA 76.41 (4.90) 89.80 (2.99) 61.98 (11.52)\nBackTrans 78.30 (6.30) 89.87 (3.35) 65.52 (11.82)\nCBERT 77.26 (4.56) 90.57 (2.11) 67.77 (13.81)\nBERTexpand 78.15 (4.56) 89.79 (2.56) 68.06 (12.20)\nBERTprepend 77.96 (4.78) 90.41 (2.39) 71.88 (9.91)\nGPT2context 74.91 (5.43) 88.87 (3.33) 51.41 (11.35)\nBARTword 76.35 (5.84) 91.40 (1.60) 71.58 (7.00)\nBARTspan 77.92 (4.96) 90.62 (1.79) 67.28 (12.57)\nTable 7: DA extrinsic evaluation in low-data regime.\nResults are reported as Mean (STD) accuracy on the\nfull test set. Experiments are repeated 15 times on ran-\ndomly sampled training, dev data. 50 training examples\nare subsampled randomly.\nTable 7 shows the classiﬁcation performance\nof different models when we select 50 examples\nper class. Overall, we ﬁnd that data augmentation\ndoes not improve classiﬁcation performance, and\nin many cases, it even hurts the accuracy.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.797636091709137
    },
    {
      "name": "Computer science",
      "score": 0.738076388835907
    },
    {
      "name": "Encoder",
      "score": 0.6521857976913452
    },
    {
      "name": "Language model",
      "score": 0.6378365755081177
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5662630796432495
    },
    {
      "name": "Machine learning",
      "score": 0.4715419113636017
    },
    {
      "name": "Class (philosophy)",
      "score": 0.4695254862308502
    },
    {
      "name": "Training set",
      "score": 0.4608137011528015
    },
    {
      "name": "Natural language processing",
      "score": 0.3831883668899536
    },
    {
      "name": "Engineering",
      "score": 0.09647080302238464
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210089985",
      "name": "Amazon (Germany)",
      "country": "DE"
    }
  ],
  "cited_by": 140
}