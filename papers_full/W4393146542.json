{
  "title": "Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models",
  "url": "https://openalex.org/W4393146542",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2101807021",
      "name": "Jiang Zhang",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2100521699",
      "name": "Qiong Wu",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2096366413",
      "name": "Yiming Xu",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2100972142",
      "name": "Cheng Cao",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2106697286",
      "name": "Zheng Du",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A340493376",
      "name": "Konstantinos Psounis",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2101807021",
      "name": "Jiang Zhang",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2100521699",
      "name": "Qiong Wu",
      "affiliations": [
        "Bellevue Hospital Center",
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2096366413",
      "name": "Yiming Xu",
      "affiliations": [
        "Bellevue Hospital Center",
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2100972142",
      "name": "Cheng Cao",
      "affiliations": [
        "Amazon (United States)",
        "Bellevue Hospital Center"
      ]
    },
    {
      "id": "https://openalex.org/A2106697286",
      "name": "Zheng Du",
      "affiliations": [
        "Amazon (United States)",
        "Bellevue Hospital Center"
      ]
    },
    {
      "id": "https://openalex.org/A340493376",
      "name": "Konstantinos Psounis",
      "affiliations": [
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6838865847",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4308243058",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4378474184",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4302305884",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4385571011",
    "https://openalex.org/W4318621294",
    "https://openalex.org/W4385571260",
    "https://openalex.org/W4389520109",
    "https://openalex.org/W4285210452",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3176580738",
    "https://openalex.org/W4330337579",
    "https://openalex.org/W3034937117",
    "https://openalex.org/W4363675974",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4385572016",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3093699284",
    "https://openalex.org/W4281623759"
  ],
  "abstract": "Toxic content detection is crucial for online services to remove inappropriate content that violates community standards. To automate the detection process, prior works have proposed varieties of machine learning (ML) approaches to train Language Models (LMs) for toxic content detection. However, both their accuracy and transferability across datasets are limited. Recently, Large Language Models (LLMs) have shown promise in toxic content detection due to their superior zero-shot and few-shot in-context learning ability as well as broad transferability on ML tasks. However, efficiently designing prompts for LLMs remains challenging. Moreover, the high run-time cost of LLMs may hinder their deployments in production. To address these challenges, in this work, we propose BD-LLM, a novel and efficient approach to bootstrapping and distilling LLMs for toxic content detection. Specifically, we design a novel prompting method named Decision-Tree-of-Thought (DToT) to bootstrap LLMs' detection performance and extract high-quality rationales. DToT can automatically select more fine-grained context to re-prompt LLMs when their responses lack confidence. Additionally, we use the rationales extracted via DToT to fine-tune student LMs. Our experimental results on various datasets demonstrate that DToT can improve the accuracy of LLMs by up to 4.6%. Furthermore, student LMs fine-tuned with rationales extracted via DToT outperform baselines on all datasets with up to 16.9% accuracy improvement, while being more than 60x smaller than conventional LLMs. Finally, we observe that student LMs fine-tuned with rationales exhibit better cross-dataset transferability.",
  "full_text": "Efficient Toxic Content Detection by Bootstrapping and Distilling\nLarge Language Models\nJiang Zhang1*, Qiong Wu2, Yiming Xu2, Cheng Cao2, Zheng Du2, Konstantinos Psounis1\n1University of Southern California, Los Angeles, CA, USA\n2Amazon.com, Inc., Bellevue, W A, USA\n{jiangzha,kpsounis}@usc.edu, {qionggwu,ymxu,chengcao,zhengdu}@amazon.com\nAbstract\nToxic content detection is crucial for online services to re-\nmove inappropriate content that violates community stan-\ndards. To automate the detection process, prior works have\nproposed varieties of machine learning (ML) approaches\nto train Language Models (LMs) for toxic content de-\ntection. However, both their accuracy and transferability\nacross datasets are limited. Recently, Large Language Models\n(LLMs) have shown promise in toxic content detection due to\ntheir superior zero-shot and few-shot in-context learning abil-\nity as well as broad transferability on ML tasks. However,\nefficiently designing prompts for LLMs remains challeng-\ning. Moreover, the high run-time cost of LLMs may hinder\ntheir deployments in production. To address these challenges,\nin this work, we propose BD-LLM, a novel and efficient\napproach to Bootstrapping and Distilling LLMs for toxic\ncontent detection. Specifically, we design a novel prompting\nmethod named Decision-Tree-of-Thought (DToT) to boot-\nstrap LLMs‚Äô detection performance and extract high-quality\nrationales. DToT can automatically select more fine-grained\ncontext to re-prompt LLMs when their responses lack confi-\ndence. Additionally, we use the rationales extracted via DToT\nto fine-tune student LMs. Our experimental results on various\ndatasets demonstrate that DToT can improve the accuracy of\nLLMs by up to 4.6%. Furthermore, student LMs fine-tuned\nwith rationales extracted via DToT outperform baselines on\nall datasets with up to 16.9% accuracy improvement, while\nbeing more than 60√ó smaller than conventional LLMs. Fi-\nnally, we observe that student LMs fine-tuned with rationales\nexhibit better cross-dataset transferability.\nIntroduction\nToxic content detection is important for online services to\nprotect users from harmful and offensive content, ensuring\na safer and more positive user experience. Common toxic\ncontent categories include hate speech, biased content, sex-\nual content, violent content, bullying content, etc. Due to the\nmassive amount of content on the Internet, it is impractical\nto manually check the toxicity of each content. Hence, ma-\nchine learning (ML) solutions based on supervised learning\nhave been widely applied to automate the toxic content de-\ntection process, where Language Models (LMs) fine-tuned\n*This work was done when the author was an intern at Amazon.\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\non a task-specific dataset achieve the state-of-the-art (SOTA)\nperformance (Caselli et al. 2020; Kim, Park, and Han 2022).\nHowever, existing supervised learning ML solutions face\nthree challenges. First, they require training data with la-\nbels, which are non-trivial to obtain for toxic content detec-\ntion tasks due to the lack of standard definitions, especially\nfor implicit toxic content. Second, the fine-tuned LMs may\noverfit the training dataset, which limits the transferability to\nother datasets. Lastly, they typically can only predict binary\nlabels without detailed reasoning.\nTo handle the above challenges, the recently emerging\nLarge Language Models (LLMs) have been leveraged to\ndetect toxic content (Wang and Chang 2022; Zhang et al.\n2023), due to their superior zero-shot and few-shot in-\ncontext learning performance and transferability. Existing\nworks on LLMs for toxic content detection focus on design-\ning novel prompting approaches to enhance the performance\nof LLMs. However, their performance relies heavily on the\nquality of prompts, which are non-trivial to design. More-\nover, deploying LLMs for toxic content detection in produc-\ntion can incur both high run-time cost and high latency, es-\npecially when the number of tokens in the prompt is large\n(e.g. for in-context few-shot learning).\nMotivated by the above, in this paper, we propose BD-\nLLM, a novel and efficient approach to Bootstrapping and\nDistilling LLMs for toxic content detection (as shown in\nFigure 1). We first design a novel prompting approach called\nDecision-Tree-of-Thought (DToT) to improve the zero-shot\nand few-shot in-context learning performance of LLMs as\nwell as extract better rationales. At a high level, DToT works\nby iteratively selecting more fine-grained context to re-\nprompt LLMs whenever they have low-confident answers. It\nautomatically decides when to select more fine-grained con-\ntext for re-prompting and which type of fine-grained context\nshould be selected. Second, we propose to fine-tune a stu-\ndent LM with smaller model size to predict both labels and\nrationales extracted via DToT.\nWe evaluate the proposed approach on three public\ndatasets and one Amazon internal dataset. Experimental re-\nsults demonstrate that employing DToT prompting consis-\ntently leads to improved zero-shot and few-shot in-context\nlearning performance on all datasets, by up to 4.6% higher\naccuracy. Furthermore, we demonstrate that fine-tuning stu-\ndent LMs with DToT-extracted rationales results in up\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n21779\nLLM\nDToT Prompting\nQuestion: ùëû\nAnswer: ùëé, Rationale: ùëü\nFine-tune Student LMùëû ùëé,ùëü Question: ùëû Prompt Generator\nLLM\nPrompt: ùëù!\nConfidence Checker\nAnswer: ùëé!Rationale: ùëü!\nRationale: ùëü!\nNew context: ùëê!\"#=ùíÑùíï\"ùüèùíã\nAnswer: ùëé!Rationale: ùëü!\nùíÑùíï\nùíÑùíï\"ùüèùüè\nùíÑùíï\"ùüèùíã\nùíÑùíï\"ùüèùëµùíÑùíï\nContext Tree\nContext SelectorLow\nHighInitial context: ùëê'\nDToT Prompting\nFigure 1: Overall workflow of the proposed BD-LLM. Given question q, it first bootstraps the LLM via DToT prompting to\nextract answer a and rationale r with high-confidence. Then, it uses q as input and (a, r) as output to fine-tune the student LM.\nto 16.9% accuracy improvement compared with baselines.\nMeanwhile, these student LMs have model size more than\n60√ó smaller than conventional LLMs. Finally, we observe\nthat the cross-dataset transferability of student LMs fine-\ntuned with rationale is significantly improved.\nOur contributions are summarised as follows:\n‚Ä¢ We propose BD-LLM, an efficient approach for toxic\ncontent detection, which leverages LLMs strength but\nalso reduces their complexity via bootstrapping and dis-\ntillation. This is the first-of-its-kind study on toxic con-\ntent detection.\n‚Ä¢ To bootstrap LLMs, we design a novel prompting method\nnamed DToT, which selectively re-prompts LLMs with\nmore fine-grained context to boost their detection perfor-\nmance and extract high-quality rationales.\n‚Ä¢ To distill LLMs into smaller student LMs, we fine-tune\nthe student LMs to predict both labels and rationales ex-\ntracted via DToT.\n‚Ä¢ We evaluate the proposed solution on four datasets and\ndemonstrate that DToT can improve the toxic content\ndetection accuracy of LLMs by up to 4.6% and student\nLMs fine-tuned with DToT-extracted rationales achieve\nthe SOTA performance with up to 16.9% accuracy im-\nprovements and more than 60√ó smaller size.\nRelated Work\nToxic Content Detection\nPrior works on toxic content detection can be categorized\ninto two types. One type of research works focuses on cre-\nating benchmark datasets for toxic content detection, either\nby crowdsourcing and annotating human-written text (Ye,\nLe, and Lee 2023; Sap et al. 2019; Vidgen et al. 2020), or\nleveraging ML-based approaches to generate high-quality\ntoxic dataset in a scalable way (Hartvigsen et al. 2022). An-\nother type of works proposes novel approaches to fine-tune\nLMs on toxic dataset. Caselli et al. (2020) propose Hate-\nBERT, a pre-trained BERT model for abusive language de-\ntection, which significantly outperforms the original BERT\nmodel. Kim, Park, and Han (2022) propose a novel con-\ntrastive learning method to improve the cross-dataset perfor-\nmance of HateBert. Most recently, researchers have started\nto use LLMs to detect toxic content. Wang and Chang (2022)\ndesign a generative prompt-based inference method to lever-\nage LLMs for toxic content detection. Zhang et al. (2023)\npropose an interpretable, unified, language checking method\n(UniLC) to enhance the few-shot in-context learning capa-\nbilities of LLMs for misinformation, stereotypes, and hate\nspeech detection. Compared with these works, our work not\nonly proposes a novel and orthogonal prompting approach\nthat improves the zero-shot/few-shot performance, but also\nextracts and distills rationales into a smaller but more effec-\ntive student LM for toxic context detection.\nPrompting LLMs\nLLMs such as GhatGPT (OpenAI 2023) and Llama (Tou-\nvron et al. 2023) have demonstrated superior zero-shot/few-\nshot in-context learning capabilities and generalizability on\na variety of language tasks without fine-tuning (Kojima et al.\n2022). However, their performance is heavily related to the\nquality of input prompts (Zhao et al. 2023). Hence, varieties\nof prompting approaches have been proposed to improve the\nquality and robustness of LLMs‚Äô response (Wei et al. 2022;\nYao et al. 2023; Wang et al. 2022b; Luo et al. 2023; Wang,\nZhu, and Wang 2023; Chen et al. 2023). Wei et al.(2022)\nand Wang et al.(2022b) propose Chain-of-Thought (CoT)\nand self-consistent CoT, which prompts LLMs step by step\nto improve LLMs‚Äô performance on reasoning tasks. Yao\net al.(2023) generalizes CoT into Tree-of-Thought (ToT),\nwhich enables LLMs to explore multiple intermediate steps\nfor complex problem solving tasks. Different from CoT and\nToT which are suitable for step-by-step reasoning problems,\nthe proposed DToT in this work is designed for classifica-\ntion problems with a novel confidence checker and context\nselector, which iteratively searches and injects more fine-\ngrained context into prompts to enhance the classification\nconfidence of LLMs. Other works have proposed to lever-\nage the in-context learning capability of LLMs, by augment-\ning the prompts with demonstrations (Wang, Zhu, and Wang\n2023), rationales (Chen et al. 2023) or grounded informa-\ntion (Luo et al. 2023; Zhang et al. 2023). Note that our\nDToT prompting is orthogonal to existing in-context learn-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n21780\ning methods for LLMs (see Section 3.2).\nDistilling LLMs\nSome recent works also tackle the problem by distilling\nLLMs into smaller LMs for domain-specific tasks (Magis-\nter et al. 2022; Hsieh et al. 2023; Wang et al. 2022a, 2023).\nWang et al. (2022a) propose PINTO, which uses LLMs‚Äô\nrationales as context of a small LM to improve its perfor-\nmance on reasoning tasks. Wang et al. (2023) further pro-\npose SCOTT, a faithful knowledge distillation method that\nelicits self-consistent rationales from LLMs to fine-tune a\nsmall while more faithful LM for open-domain question-\nanswering tasks. Both Magister et al. (2022) and Hsieh et al.\n(2023) demonstrate that LLMs can be distilled into smaller\nbut more effective LMs by fine-tuning with both answers\nand rationales on commonsense reasoning and arithmetic\ntasks. Different from these works, our work focus on the\ndomain of toxic content detection. Moreover, we propose\nDToT that extracts better rationales from LLMs and demon-\nstrate that using DToT-extracted rationales further improves\nthe effectiveness of distillation for toxic content detection.\nApproach\nFigure 1 demonstrates the overall workflow of BD-LLM,\nwhich consists of two separate steps. Specifically, in the first\nstep, we design DToT prompting to iteratively extract high-\nconfidence answers a and rationales r from LLM given input\nquestion q. 1 In the second step, we conduct rationale distil-\nlation by fine-tuning a student LM to generate both a and r\ngiven input q. We describe the details of each step below.\nDToT Prompting\nAt a high level, DToT prompting iteratively selects more\nfine-grained context and re-prompts the LLM when they out-\nput responses with low confidence. Two challenges in de-\nsigning DToT prompting are: 1) how to decide whether the\nresponse of LLM is confident or not; 2) how to decide which\ntype of fine-grained context should be selected to re-prompt\nthe LLM2. To address the first challenge, we design a con-\nfidence checker to measure the self-confidence score of the\nLLM‚Äôs answer. To handle the second challenge, we design\na context selector to select appropriate fine-grained context\nfrom a context tree based on the LLM‚Äôs rationale.\nIn total, DToT prompting consists of four modules: 1)\nconfidence checker; 2) context tree; 3) context selector; 4)\nprompt generator, which can be used for bothblack-box and\nwhite-box LLMs. Note that for black-box LLMs, we assume\nthat we only have access to their output responses (e.g. Chat-\nGPT). By contrast, for white-box LLMs, we assume that we\ncan also have access to their model parameters (e.g. open-\nsource LLMs). The detailed design of these modules and the\nend-to-end workflow are presented below.\n1Note that for toxic content detection, an answer is either ‚ÄòYes‚Äô\nor ‚ÄòNo‚Äô, while a rationale consists of one or more sentences.\n2Since there are many different types of toxic content and for\neach of them we need to use the right context (i.e. part of the prompt\nthat specifies the criteria to call certain content as toxic), there is a\nneed to automatically select the appropriate context for prompting.\nConfidence Checker. This module is designed to measure\nthe self-confidence score of LLM‚Äôs response, which is de-\nfined as sconf\nt for iteration step t . Specifically, for theblack-\nbox LLM, the confidence checker uses the toxicity rating\nof LLM (defined as stoxi\nt ‚àà [0, 100]) to calculate the con-\nfidence score. If stoxi\nt is above a maximal threshold sh or\nbelow a minimal threshold sl, we consider the answer as\nconfident, vice versa. Note that to obtainstoxi\nt , we explicitly\nrequire the LLM to output the toxicity rating in the prompt\npt. For thewhite-box LLM (e.g. any open-source LLMs), the\nconfidence checker will leverage the output logits of LLM\nto calculate the probability of generating answers at condi-\ntional on the prompt pt. This probability will be used as the\nconfidence measurement (i.e. sconf\nt = Pr[at|pt]). Formally,\nwe define sconf\nt as:\nsconf\nt =\n\u001a1[stoxi\nt /‚àà (sl, sh)], for black-box LLMs\nPr[at|pt], for white-box LLMs (1)\nwhere 1[¬∑] is an indicator function, and sl and sh are two\nadjustable thresholds (see Appendix A for selecting sl, sh).\nSuppose the confidence checker has measured the self-\nconfidence score sconf\nt of LLM‚Äôs answer at. Then, if sconf\nt\nis higher than a threshold sŒ¥, the checker will consider at as\na confident answer. Otherwise, at is considered to be un-\nconfident. Formally, the output of the confidence checker\nDcheck is defined as:\nDcheck(sconf\nt ) =\n(\nUnconfident, if sconf\nt ‚àà [0, sŒ¥)\nConfident, if sconf\nt ‚àà [sŒ¥, 1] (2)\nwhere sŒ¥ is an adjustable threshold (see Appendix A for se-\nlecting sŒ¥).\nContext Tree. Before introducing the context selector mod-\nule, we first provide the definition of context tree. Suppose\nthe universe of context is represented as setC. We define the\ncontext tree as Tc : C ‚ÜíLIST [C], which is a mapping from\na parent-node context ct ‚àà Cto the list of its child-node con-\ntexts [c1\nt+1, ..., c\nNct\nt+1], where Nct is the total number of child\nnodes of ct, and cj\nt+1 ‚àà Cis the j-th child-node context ofct.\nMoreover, each child-node context‚Äôs category is designed to\nbe a subcategory of its parent-node context‚Äôs category, such\nthat the child-node context is more fine-grained than parent-\nnode context. For instance, suppose thatc0 provides the def-\ninition of toxic content, which includes hate speech, sexual\ncontent, etc. Then, c1\n1 can be a more fine-grained definition\nfor hate speech, and c2\n1 can be a more fine-grained definition\nfor biased content (see Figure 2).\nContext Selector. Now we introduce the context selector\nmodule, which takes as input the rationale rt and the con-\ntext ct in prompt at step t, and a context tree Tc, to select\na more fine-grained context ct+1 for step t + 1. Whenever\nan answer at at iteration step t is considered as unconfident\nby the confidence checker, the context selector will select\nnew context from the context tree for re-prompting LLMs.\nSpecifically, it measures the relevance between rationale rt\nand each child-node context cj\nt+1, and then selects the most\nrelevant one ct+1 = cj‚àó\nt+1. Formally, we define the output of\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n21781\nToxic content includes hate speech, biased content, sexual content, ‚Ä¶\nHate and intolerant content refers to any form of communication, \nmedia, or expression that promotes or incites violence, hostility, ‚Ä¶\nBiased and discriminatory content can disrespect, stereotype, or \npromote bias and discrimination against an individual ‚Ä¶\n‚Ä¶\nùëê!\nùëê\"\n\"\nùëê\"\n#\nFigure 2: An example of context tree.\ncontext selector as:\nDselect(rt)= j‚àó = arg max\n1‚â§j‚â§Nct\nsrele,j\nt (rt, cj\nt+1), (3)\nwhere [c1\nt+1, ..., c\nNct\nt+1] = Tc(ct), and srele,j\nt is the relevance\nscore between rt and cj\nt+1.\nFurthermore, to calculatesrele,j\nt for LLMs which can gen-\nerate relatively high-quality rationales (mostly forblack-box\nLLMs like ChatGPT), we use a classification prompt to ask\nLLM which of these child-node context categories are most\nrelevant to the rationale rt. We denote the answer from the\nLLM as Class(rt). By contrast, for white-box LLMs which\ncannot generate rationales with decent quality, we construct\na set of candidate rationales as rt = [ r1\nt , ..., r\nNct\nt ], where\neach rt[j] = rj\nt is relevant to only one of the child-node con-\ntexts cj\nt+1. Since we have access to the output logits of the\nLLM, we measure the probability of generating each candi-\ndate rationale conditional on the promptpt (i.e., Pr[rt[j]|pt])\nas the relevance score. Formally, we definesrele,j\nt as:\nsrele,j\nt =\n(\n1[j = Class(rt)], for black-box LLMs\nPr[rt[j]|pt], for white-box LLMs (4)\nwhere 1[¬∑] is an indicator function.\nPrompt Generator. The Prompt Generator module P gen-\nerates an input prompt pt for LLM M based on the question\nq, and the contextct at iteration step t. Note that it will mod-\nify the question q with the change of context. For example,\nthe initial question based on c0 is designed to ask whether\na statement contains toxic content. Suppose c1 is selected\nto provide context related to hate speech. Then, the ques-\ntion will be modified to ask whether the statement contains\nhate speech, which is a specific category of toxic content.\nWe provide the prompt templates in Appendix C.\nEnd-to-end Workflow. Algorithm 1 illustrates the end-to-\nend workflow of DToT prompting, where the input contains\nLLM M, question q, initial context c0. At each iteration step\nt (line 2), it first generates prompt pt via prompt generator\nP and gets LLM M‚Äôs output (lines 4-9). Next, it calculates\nthe confidence score of LLMs‚Äôs answer sconf\nt and decides\nwhether it is confident via the confidence checker Dcheck\n(lines 11-12). For unconfident answers, it will get a list of\nnew candidate contexts from context tree Tc (line 14), cal-\nculate the relevance score between LLM‚Äôs rationale and each\none of the new context in the list (lines 15-22), and select the\nmost relevant new context (lines 23-24). Lastly, it terminates\nwhen the maximal iteration step T is reached (line 2) or the\nLLM‚Äôs answer is confident (line 27).\nAlgorithm 1: DToT Prompting\nInput: Question q, initial context c0, LLM M, thresholds sl,\nsh, and sŒ¥, maximal iteration step T.\nOutput: Answer a, rationale r.\n1: Define current step as t = 0.\n2: while t < Tdo\n3: // Generate prompt and get response\n4: Generate input prompt: pt = P(q, ct, M).\n5: if M is black-box then\n6: Get LLM output: (at, stoxi\nt , rt) = M(pt).\n7: else\n8: Get LLM output: (at, rt) = M(pt).\n9: end if\n10: // Check the confidence of answer\n11: Calculate confidence score sconf\nt by Eq. (1).\n12: if Dcheck(sconf\nt ) = Confidence: then\n13: // Select new context from context tree\n14: Get the new context list: [cj\nt+1]\nNct\nj=1 = Tc(ct).\n15: if M is black-box then\n16: Let M get the rationale class: Class(rt).\n17: else\n18: Let rt be the candidate rationale list: [rt,j]\nNct\nj=1.\n19: Calculate relevance score[srele,j\nt ]\nNct\nj=1 by Eq. (4).\n20: end if\n21: Calculate j‚àó = Dselect(rt) by Eq. (3).\n22: Set new context ct+1 as cj‚àó\nt+1 and t = t + 1.\n23: else\n24: Exit.\n25: end if\n26: end while\n27: return a = at, r= rt\nAugmented DToT Prompting\nSo far, we present how DToT prompting works under the\nzero-shot learning setup. Considering that DToT prompt-\ning is orthogonal to few-shot learning, we propose two aug-\nmented versions of DToT prompting below.\nDToT+FS. The approach combines DToT prompting with\nthe vanilla few-shot in-context learning, which adds a few\ndemonstrations in the prompt. Recent work (Wang, Zhu,\nand Wang 2023) has shown that a few demonstrations can\neffectively improve the in-context learning performance of\nLLMs. Moreover, motivated by prior work (Liu et al. 2021),\nwe select K positive statements and K negative statements\nthat are most semantically similar to the input statement as\ndemonstrations from a development set.\nDToT+FS+R. Recent works (Sun et al. 2022; Zhang et al.\n2023) have demonstrated that rationales or facts in prompts\nserve as grounded information to further enhance the few-\nshot in-context learning capability of LLMs. Therefore, on\ntop of DToT+FS approach, we include the rationale for each\ndemonstration in the prompt.\nRationale Distillation\nAfter obtaining the answers and rationales from LLMs via\nDToT, we can distill LLMs into smaller student LMs, which\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n21782\ncan predict both answers and rationales. We describe the de-\ntails of rationale distillation under two scenarios below.\nDistillation without Labels. Assume that we do not have\nground truth labels for training inputs. Hence, we use both\nthe answers and rationales output by LLMs as ground truth.\nIn practice, this approach can be applied when it is chal-\nlenging or costly to obtain ground truth labels. Suppose for\nthe i-th input xi, the predicted label from LLM is ÀÜyi 3 and\nthe associated rationale is ri. Suppose the student model is\n(ys\ni , rs\ni ) = fs\nŒ∏ (xi) parameterized by Œ∏. Then the loss func-\ntion is defined as\nLs(Œ∏) =\nDX\ni=1\n(CE(ys\ni , ÀÜyi) +CE(rs\ni , ri)), (5)\nwhere D is the total number of training data points and CE\nrepresents the cross-entropy averaged at token-level.\nDistillation with Labels. Assume that we have access to\nground truth labels for training inputs. Hence, we can use\nthe ground truth labels for fine-tuning. Specifically, if the\nLLMs can not predict the right answer, we only use the bi-\nnary ground truth labels. Otherwise, we use both the labels\nand rationales. Formally, the loss function is defined as\nLs(Œ∏) =\nDX\ni=1\n(CE(ys\ni , yi) +Œª ‚àó 1yi=ÀÜyi CE(rs\ni , ri)), (6)\nwhere Œª is an adjustable parameter (see Appendix A for se-\nlecting Œª).\nExperimental Setup\nDatasets\nWe evaluate our approach on three public datasets and an\nAmazon private dataset.\nToxigen. This is an GPT3-generated dataset provided by\nHartvigsen et al. (2022), which contains both toxic and be-\nnign statements about 13 minority groups. We use their an-\nnotated dataset with 8,960 training statements and 940 test-\ning statements in our experiments, where 40% of statements\nare toxic. Note that we exclude about 5% ambiguous state-\nments with toxicity level 3 in experiments, where the scores\n(i.e. toxicity level) range from 1 to 5 and 3 denotes ambigu-\nity, thus are removed from our experiments.\nSBIC. This dataset contains 44,671 social media posts from\nReddit, Twitter, and hate sites. Each post was annotated by\nthe Social Bias Frames proposed in (Sap et al. 2019) to spec-\nify whether there exists any social bias and stereotype to-\nwards a target group that is toxic. In our experiments, we\nrandomly and uniformly sample 4,000 statements as train-\ning dataset and 1,000 statements as testing dataset, 50% of\nwhich are toxic.\nDHate. This dataset is generated by a human-and-model-in-\nthe-loop process proposed in (Vidgen et al. 2020). In total,\nit contains about 40,000 labeled statements, of which 54%\ncontains hate content. We randomly and uniformly sample\n4,000 statements as training dataset and 1,000 statements as\n3Note that we assign label 1 for ‚ÄôYes‚Äô answer and label 0 for\n‚ÄôNo‚Äô answer.\ntesting dataset in our experiments.\nAmazon. This is a private dataset from Amazon, which con-\ntains 8,000 benign statements and 2,000 toxic statements an-\nnotated by professional human labelers. This dataset is only\nused for testing purposes in our experiments due to confi-\ndentiality policy.\nModels and Baselines\nDToT prompting. We evaluate the effectiveness of DToT\nprompting on both black-box and white-box models, which\nare defined in Section 3.1. Specifically, we select gpt-3.5-\nturbo (denoted by ChatGPT (OpenAI 2023)) with 175B pa-\nrameters as our black-box model, and we consider FastChat-\nT5 (denoted by FC-T5 (Zheng et al. 2023)) with 3B param-\neters as our white-box model.\nMoreover, we compare DToT prompting with three exist-\ning baselines: (a) RoBerta model fine-tuned on each dataset,\nsince prior work (Hartvigsen et al. 2022) has shown that\nit can achieve the SOTA performance on Toxigen dataset.\n(b) CoT prompting, which can be viewed as a special case\nof DToT prompting without iteratively re-prompting. (c)\nUniLC prompting proposed by Zhang et al. (2023).\nFinally, we compare DToT prompting with its two aug-\nmented versions: DToT+FS and DToT+FS+R (see Section\n3.2). Note that for DToT+FS, we select K = 3 positive\nstatements and K = 3 negative statements that are most\nsemantically similar to the input statement from a develop-\nment set. To measure the similarity, we use sentence trans-\nformer (Reimers and Gurevych 2019) to convert each state-\nment into an embedding, and use the cosine similarity be-\ntween two statements as a measurement of their semantic\nsimilarity. Moreover, for DToT+FS+R, we use the rationales\nassociated with the correct answers generated by ChatGPT\nas augmentations.\nModel Distillation. For our main experiments, we select\nFC-T5 to evaluate the effectiveness of fine-tuning a student\nLM using rationales generated from LLMs, and we consider\ntwo baseline approaches: (a) fine-tuning with labels without\nrationale, and (b) fine-tuning with rationales generated by\nCoT prompting (which we denote by RCoT ). Note that for\nour approach (which we denote by RDToT ), we use the ra-\ntionales generated via DToT+FS+R prompting as the ground\ntruth during fine-tuning. Furthermore, to investigate how the\nnumber of parameters in student LM affects the fine-tuning\nperformance, we use Flan-T5 models with different model\nsize (Chung et al. 2022) (see Appendix A for more details\non why we select these models and hyperparameter setup).\nEvaluation Results\nEvaluation of DToT\nWe start by evaluating DToT prompting to answer the fol-\nlowing question.\nQ1: Can DToT prompting enhance the detection per-\nformance of LLMs? As shown in Table 1, compared with\nCoT prompting, DToT prompting can enhance the zero-shot\nlearning performance of both black-box model and white-\nbox model significantly on all three public datasets. Specif-\nically, for FC-T5, DToT prompting can increase the accu-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n21783\nModel Method Toxigen SBIC DHate\nAcc F1 AUC Acc F1 AUC Acc F1 AUC\nRoBerta FT 82.45 75.35 90.24 82.80 83.96 91.82 70.60 75.38 81.27\nFC-T5\nCoT 79.69 73.49 85.54 63.50 71.15 67.76 63.40 71.09 69.13\nDToT 81.24 75.36 86.67 68.80 74.26 72.47 66.80 73.05 72.66\nDToT+FS 81.90 75.88 86.74 68.00 73.77 72.74 65.40 72.23 73.80\nDToT+FS+R 82.01 75.99 86.94 68.00 73.77 72.56 66.50 72.87 73.37\nChatGPT\nCoT 82.71 81.29 N/A 68.50 72.82 N/A 65.20 72.20 N/A\nUniLC 83.30 82.73 N/A N/A N/A N/A N/A N/A N/A\nDToT 85.03 82.76 N/A 71.60 74.18 N/A 68.20 73.92 N/A\nDToT+FS 86.03 83.51 N/A 71.70 74.62 N/A 69.50 74.33 N/A\nDToT+FS+R 87.03 85.06 N/A 72.00 74.91 N/A 69.20 74.30 N/A\nTable 1: Evaluation results of DToT on Toxigen, SBIC, DHate datasets. In ‚ÄúMethod‚Äù column, ‚ÄúFT‚Äù stands for fine-tuning on\ntraining dataset, ‚ÄúCoT‚Äù refers to CoT prompting, ‚ÄúDToT‚Äù corresponds to DToT prompting, ‚ÄúDToT+FS‚Äù denotes the combi-\nnation of DToT prompting with few-shot demonstrations, and ‚ÄúDToT+FS+R‚Äù presents the combination of DToT prompting\nwith few-shot demonstrations and rationale augmentations. Due to the lack of output logits, the AUC scores of ChatGPT are\npopulated as ‚ÄúN/A‚Äù.\nracy by up to 5.30% and the F1 score by up to 3.11%. It\nis worth noting that DToT prompting can also improve the\nAUC score of FC-T5 on all datasets, indicating its robust\nperformance. For ChatGPT, DToT prompting consistently\noutperforms CoT prompting, and increases the accuracy by\nup to 3.10% and the F1 score by up to 1.72%.\nMoreover, combining DToT with few-shot in-context\nlearning (i.e. DToT+FS and DToT+FS+R) may further\nimprove models‚Äô performance. For instance, on Toxi-\ngen dataset, compared with the vanilla DToT prompting,\nadding demonstrations (i.e. DToT+FS) can improve Chat-\nGPT‚Äôs accuracy by 1.00%. Furthermore, by incorporating\nboth demonstrations and rationales during prompting (i.e.\nDToT+FS+R), ChatGPT‚Äôs accuracy can be improved by\n2.00% respectively, outperforming baselines by up to 4.58%\n(for RoBerta) and at least 3.73% (for UniLC).\nTherefore, we conclude that DToT prompting and its aug-\nmented versions significantly enhance LLMs‚Äô performance\non toxic content detection.\nEvaluation of Rationale Distillation\nNext, we evaluate whether fine-tuning with rationales ex-\ntracted via DToT improves the performance of student LMs.\nWe first answer the following question:\nQ2: Can fine-tuning with rationales extracted via DToT\nderive a student LM with higher accuracy? Table 2 re-\nports the evaluation results of fine-tuning with or without ra-\ntionales. In this table, ‚ÄúLabel = Human‚Äù denotes the utiliza-\ntion of the ground truth labels in training datasets for fine-\ntuning, ‚ÄúLabel = LLM‚Äù means the usage of labels predicted\nby the teacher LLM for fine-tuning, ‚ÄúRationale = N/A‚Äù in-\ndicates that fine-tuning is conducted solely with labels, ‚ÄúRa-\ntionale = RCoT ‚Äù implies that fine-tuning takes place with\nboth labels and CoT-extracted rationale from teacher LLMs,\nand ‚ÄúRationale = RDToT ‚Äù represents that we fine-tune the\nmodel with both labels and DToT-extracted rationales from\nthe teacher LLM (i.e. ChatGPT).\nAs reported in Table 2, FC-T5 fine-tuned with DToT-\nextracted rationales and ground truth labels outperforms all\nbaselines across various public datasets. Notably, this fine-\ntuning approach yields significant improvements in accu-\nracy, F1 score, and AUC score. Specifically, compared with\nfine-tuning with labels only, it can increase the model accu-\nracy by up to 2.54%, the F1 score by up to 2.46%, and the\nAUC score by up to 1.94% respectively. Compared with the\nprior SOTA LM fine-tuned on these datasets (i.e. RoBerta),\nit can significantly increase the model accuracy by up to\n15.80%, the F1 score by up to 11.49%, and the AUC score\nby up to 13.22% respectively. In addition, FC-T5 fine-tuned\nwith labels and rationales outperforms teacher LLM by up\nto 16.90% w.r.t. accuracy, with 60√ósmaller model size.\nMoreover, compared with fine-tuning with RCoT and\nground truth labels, we observe that fine-tuning withRDToT\nand ground truth labels can consistently result in stu-\ndent LMs with better detection performance on all public\ndatasets. This indicates that rationales generated via DToT\nprompting (RDToT ) have higher quality than those gener-\nated via CoT prompting (RCoT ).\nLastly, under the scenario where we use labels predicted\nby teacher LLM as ground truth, we observe that fine-\ntuning with RDToT and labels can still outperform both\nfine-tuning with RCoT and labels, and fine-tuning with only\nlabels. Specifically, on Toxigen dataset, FC-T5 fine-tuned\nwith RDToT and predicted labels from teacher LLM can\neven outperform RoBerta fine-tuned with human labels from\ntraining dataset. However, without using RDToT , the fine-\ntuned FC-T5 cannot outperform Roberta.\nIn summary, we conclude that fine-tuning with both labels\nand rationales can effectively improve the student LMs‚Äô per-\nformance. Moreover, using rationales with better quality (i.e.\nDToT-extracted rationales versus CoT-extracted rationales)\ncan further enhance the performance of fine-tuned LMs.\nQ3: Can fine-tuning with rationales improve the trans-\nferability of student LMs across different toxic datasets?\nTo evaluate whether fine-tuning with both labels and ratio-\nnales can improve the transferability of student LMs, we\ntest the models fine-tuned on the other two public datsaets\n(SBIC and DHate) and one private dataset (Amazon). Ta-\nble 3 reports our transferability evaluation results, where\nwe use AUC score as our metric. First, we observe that\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n21784\nModel Label Rationale Toxigen SBIC DHate\nAcc F1 AUC Acc F1 AUC Acc F1 AUC\nRoBerta Human N/A 82.45 75.35 90.24 82.80 83.96 91.82 70.60 75.38 81.27\nChatGPT N/A N/A 87.03 85.06 N/A 72.00 74.91 N/A 69.50 74.33 N/A\nFC-T5\nLLM\nN/A 81.90 80.19 91.78 64.90 71.99 74.67 63.50 71.15 70.50\nRCoT 81.79 80.10 92.88 67.30 73.35 74.25 64.30 71.60 72.90\nRDToT 84.00 82.08 93.60 69.00 74.42 81.09 68.00 73.77 77.89\nHuman\nN/A 84.99 83.00 92.43 84.00 84.91 92.89 85.00 85.71 93.64\nRCoT 87.31 85.24 94.15 84.20 85.07 93.18 86.20 86.71 93.71\nRDToT 87.53 85.46 94.37 85.10 85.82 93.85 86.40 86.87 94.49\nTable 2: Distillation evaluation results on Toxigen, SBIC, and DHate datasets. In ‚ÄúLabel‚Äù column, ‚ÄúHuman‚Äù indicates that the\nlabels come from the training dataset, ‚ÄúLLM‚Äù indicates that the labels are predicted by LLM. In ‚ÄúRationale‚Äù column, ‚ÄúN/A‚Äù\nmeans no rationales are used in fine-tuning, ‚ÄúRCoT ‚Äù means rationales extracted via CoT are used in fine-tuning, and ‚ÄúR CoT ‚Äù\nmeans rationales extracted via DToT are used in fine-tuning.\nModel Labels Rationale SBIC DHate Amazon\nRoBerta Human N/A 65.46 61.51 X\nChatGPT N/A N/A 72.00 69.50 N/A\nFC-T5\nLLM\nN/A 75.36 74.96 X+2.47\nRCoT 71.48 70.99 X+4.52\nRDToT 75.90 77.31 X+4.54\nHuman\nN/A 75.24 77.75 X+0.05\nRCoT 77.15 78.97 X+4.16\nRDToT 77.29 77.54 X+3.61\nTable 3: Transferability evaluation results. Note that we fine-\ntune these models on Toxigen dataset while testing them\non other datasets, and we report AUC score. For Amazon\ndataset, due to confidentiality policy, we only report the in-\ncreased AUC score compared with RoBerta (whose AUC\nscore is denoted by X).\ncompared with fine-tuning with labels only (Rationale =\nN/A), fine-tuning with both labels and rationales (Rationale\n= RCoT /RDToT ) can improve the AUC score of student\nLMs by up to 2.35% on testing datasets. Second, FC-T5\nfined-tuned with both labels and rationales have significantly\nbetter AUC scores on all datasets compared with RoBerta.\nLastly, while we only fine-tune FC-T5 on Toxigen datset, it\nstill outperforms teacher LLM (i.e. ChatGPT) on both SBIC\nand DHate datasets. Hence, we conclude that fine-tuning\nwith rationale effectively improves the cross-dataset trans-\nferability of student LMs.\nQ4: How does student LMs‚Äô size affect the detection ac-\ncuracy? So far, we use FC-T5 with 3B parameters as our\nstudent LM during rationale distillation experiments. To in-\nvestigate how the model size affects the student LMs‚Äô per-\nformance, we fine-tune Flan-T5 models of different size\nwith rationales on Toxigen dataset. As reported in Table 4,\nfine-tuning with both labels and rationales can consistently\nenhance the student LMs‚Äô performance on Toxigen dataset\nwith varying model size. Moreover, as the model size be-\ncomes smaller, the model performance will decrease, and\nthe performance gain provided by using rationales also be-\ncomes smaller, which indicates that larger student LMs can\nlearn to generate rationales better.\nQ5: Can student LMs fine-tuned with rationales actu-\nally generate high-quality rationales? We conduct a case\nModel Size Rationale Acc F1 AUC\nFC-T5 3B N/A 84.99 83.00 92.43\nRDToT 87.53 85.46 94.37\nF-T5-XL 3B N/A 85.54 83.52 92.51\nRDToT 87.53 85.42 93.17\nF-T5-L 770M N/A 83.44 81.57 92.42\nRDToT 84.22 82.24 93.18\nF-T5-B 220M N/A 81.02 79.43 91.26\nRDToT 81.35 79.66 91.16\nTable 4: Impact of student LMs‚Äô size on rationale distilla-\ntion. Note that we use Toxigen dataset for a case study, and\nF-T5-XL/F-T5-L/F-T5-B is short for Flan-T5-XL/Flan-T5-\nLarge/Flan-T5-Base.\nstudy on the quality of rationales generated by FC-T5 us-\ning different approaches via manual check. We observe that\nafter fine-tuning with both labels and rationales (R DToT ),\nFC-T5 always provides responses with rich rationales. In\ncontrast, fine-tuning with labels only makes FC-T5 overfit\nthe binary labels and hence output ‚ÄòYes/No‚Äô answers only\nwithout rationales. In addition, without fine-tuning, FC-T5\ncannot consistently generate meaningful rationales (see Ta-\nble 5 in Appendix C for detailed examples).\nConclusions and Limitations\nIn this work, we propose an end-to-end approach to boot-\nstrapping and distilling LLMs for toxic content detection,\nwhere a novel prompting method named DToT is designed\nto enhance LLMs‚Äô detection performance and extract bet-\nter rationales, and smaller LMs are fine-tuned with both la-\nbels and DToT-extracted rationales. Our evaluation results\non four datasets consistently demonstrate the effectiveness\nof both the proposed DToT prompting and the proposed fine-\ntuning method for toxic content detection.\nLimitations. First, the context selector of DToT conducts\ngreedy search at each iteration, which may not be globally\noptimal. Second, we use a pre-defined context-tree in our\nexperiments for DToT, which can not dynamically change\nwith LLMs‚Äô response.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n21785\nA Experimental Parameters and Models\nParameters for DToT Prompting. We set sl and sh in\nEq. (1) as 0 and 90 respectively for experiments with Chat-\nGPT. Since we observe that ChatGPT will answer ‚ÄôYes‚Äô as\nlong as the toxicity rating is above zero, which leads to high\nrecall while relatively low precision (i.e. high false positive),\nwe select a small value forsl and large value forsh to reduce\nthe false positive rate of ChatGPT. We setsŒ¥ in Eq. (2) as 0.9\nfor confidence checker in DToT prompting, since we empir-\nically notice that setting the threshold of confidence score as\n0.9 improves the confidence of generated answers and thus\nenhances the overall detection accuracy.\nMoreover, we use a two-level context tree in our experi-\nments (as shown in Figure 2 in Section 3.1) and the maxi-\nmal terminal step T is set as 2. Note that we did not experi-\nment with a deeper context-tree since DToT prompting with\na two-level context tree can already significantly improve\nLLMs‚Äô detection performance with reasonable cost.\nParameters for Rationale Distillation. We set Œª in Eq.\n(6) as 1, in order to make the predicted answers and ratio-\nnales of student LMs equally important in our experiments.\nDuring rationale distillation experiments (see Section 5.2),\nfor each dataset, we fine-tune the student LM on the train-\ning set for 5 epochs, where stochastic gradient descent with\nmini-batch size of 8 and learning rate of 1e-4 are used.\nModels for DToT Prompting. We select ChatGPT as our\nblack-box LLM since it is the most popular model with\nSOTA performance. Specifically, we use gpt-3.5-turbo in-\nstead of gpt-4, since gpt-3.5-turbo has much better trade-\noff between model performance and experimental cost. For\nwhite-box LLMs, we have tried different open-source LLMs\nincluding FastChat-T5 (Zheng et al. 2023), Flan-T5-XXL\n(Chung et al. 2022), and Falcon-40B (Almazrouei et al.\n2023). We finally select FastChat-T5 since it has the best\ndetection accuracy using prompts from Table 6.\nModels for Distillation. We use FastChat-T5 (Zheng et al.\n2023) as the student LM since it has the best performance\nafter fine-tuning, compared with other models from T5 fam-\nily (Chung et al. 2022) (see Table 4 in Section 5.2). We do\nnot consider models larger than 3B during distillation exper-\niments, due to their high fine-tuning cost while low run-time\nefficiency.\nB Rationale Examples\nTable 5 demonstrates the responses generated by FC-T5 us-\ning different approaches, where the input sentence is ‚Äúwhy\ndo you have to come to America, you should stay in your\nown country and fight for your freedom, not come to Amer-\nica‚Äù. We observe that after fine-tuning with both labels and\nrationales (FT+RDToT ), FC-T5 can provide a response with\nrich rationale (see ‚ÄúRationale: The sentence ...‚Äù). By con-\ntrast, fine-tuning with only labels will make FC-T5 output a\n‚ÄòYes/No‚Äô answer without any rationale. Without fine-tuning,\nFC-T5 also fails to output a reasonable rationale that ex-\nplains why this sentence contains toxic content.\nModel Method Response\nFC-T5\nCoT Yes, this sentence contains toxic\ncontent.\nFT Yes.\nFT+RDToT Yes, this sentence contains toxic\ncontent. Rationale: The sentence\ncontains hate speech and biased\ncontent towards immigrants. It\nimplies that immigrants are not\nwelcome in America and should\nstay in their own country to fight\nfor their freedom.\nTable 5: Responses generated by FC-T5 using different ap-\nproaches. Note we require to detect toxic content in sentence\n‚Äúwhy do you have to come to America, you should stay in\nyour own country and fight for your freedom, not come to\nAmerica‚Äù in the prompt.\nModel Prompt template\nChatGPT\nContext: c.\nSentence: s.\nDoes this sentence contain type(c)\ncontent?\nFirst, answer in ‚ÄúYes‚Äù or ‚ÄúNo‚Äù.\nSecond, rate the type(c) level out\nof 100.\nThird, explain your rationale briefly.\nThe output format is given below:\nAnswer: ...\ntype(c) level: .../100.\nRationale: ...\nFC-T5\nc.\n### Human: ‚Äús‚Äù. Does this sentence\ncontain type(c) content?\nAnswer yes or no, and explain your\nanswer.\n### Assistant:\nTable 6: Prompt templates for different LLMs, given ques-\ntion q: ‚ÄúDoes sentence s contain toxic content?‚Äù (see Sec-\ntion 3.1). Note that c is the context, s is the statement, and\ntype(c) is the category of context c (e.g. toxic, hate and\nviolent).\nC Prompt Templates\nTable 6 provides the prompt templates used in our experi-\nments, where the question q in Section 3.1 is given as ‚ÄúDoes\nsentence s contain toxic content?‚Äù. Note that for black-box\nLLM (ChatGPT), we explicitly ask the model to rate the tox-\nicity level during prompting, in order to calculate the con-\nfidence score (see Section 3.1 Confidence Checker for de-\ntails).\nAcknowledgments\nThe authors would like to thank anonymous reviewers and\nmembers from Amazon for providing insightful feedback.\nThis work is supported in part by the National Science Foun-\ndation under grant numbers 1956435 and 1901488.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n21786\nReferences\nAlmazrouei, E.; Alobeidli, H.; Alshamsi, A.; Cappelli, A.;\nCojocaru, R.; Debbah, M.; Goffinet, E.; Heslow, D.; Lau-\nnay, J.; Malartic, Q.; Noune, B.; Pannier, B.; and Penedo,\nG. 2023. Falcon-40B: an open large language model with\nstate-of-the-art performance.\nCaselli, T.; Basile, V .; Mitrovi¬¥c, J.; and Granitzer, M. 2020.\nHatebert: Retraining bert for abusive language detection in\nenglish. arXiv preprint arXiv:2010.12472.\nChen, W.-L.; Yen, A.-Z.; Huang, H.-H.; Wu, C.-K.; and\nChen, H.-H. 2023. ZARA: Improving Few-Shot Self-\nRationalization for Small Language Models. arXiv preprint\narXiv:2305.07355.\nChung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y .; Fe-\ndus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al.\n2022. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416.\nHartvigsen, T.; Gabriel, S.; Palangi, H.; Sap, M.; Ray, D.;\nand Kamar, E. 2022. Toxigen: A large-scale machine-\ngenerated dataset for adversarial and implicit hate speech\ndetection. arXiv preprint arXiv:2203.09509.\nHsieh, C.-Y .; Li, C.-L.; Yeh, C.-K.; Nakhost, H.; Fujii, Y .;\nRatner, A.; Krishna, R.; Lee, C.-Y .; and Pfister, T. 2023.\nDistilling step-by-step! outperforming larger language mod-\nels with less training data and smaller model sizes. arXiv\npreprint arXiv:2305.02301.\nKim, Y .; Park, S.; and Han, Y .-S. 2022. Generalizable im-\nplicit hate speech detection using contrastive learning. In\nProceedings of the 29th International Conference on Com-\nputational Linguistics, 6667‚Äì6679.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large language models are zero-shot reason-\ners. Advances in neural information processing systems, 35:\n22199‚Äì22213.\nLiu, J.; Shen, D.; Zhang, Y .; Dolan, B.; Carin, L.; and Chen,\nW. 2021. What Makes Good In-Context Examples for GPT-\n3? arXiv preprint arXiv:2101.06804.\nLuo, H.; Chuang, Y .-S.; Gong, Y .; Zhang, T.; Kim, Y .;\nWu, X.; Fox, D.; Meng, H.; and Glass, J. 2023. SAIL:\nSearch-Augmented Instruction Learning. arXiv preprint\narXiv:2305.15225.\nMagister, L. C.; Mallinson, J.; Adamek, J.; Malmi, E.; and\nSeveryn, A. 2022. Teaching small language models to rea-\nson. arXiv preprint arXiv:2212.08410.\nOpenAI. 2023. ChatGPT: get instant answers, find creative\ninspiration, and learn something new. https://openai.com/\nchatgpt. Accessed: 2023-08-15.\nReimers, N.; and Gurevych, I. 2019. Sentence-BERT: Sen-\ntence Embeddings using Siamese BERT-Networks. In Pro-\nceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing. Association for Computa-\ntional Linguistics.\nSap, M.; Gabriel, S.; Qin, L.; Jurafsky, D.; Smith, N. A.;\nand Choi, Y . 2019. Social bias frames: Reasoning about\nsocial and power implications of language. arXiv preprint\narXiv:1911.03891.\nSun, Z.; Wang, X.; Tay, Y .; Yang, Y .; and Zhou, D. 2022.\nRecitation-augmented language models. arXiv preprint\narXiv:2210.01296.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nVidgen, B.; Thrush, T.; Waseem, Z.; and Kiela, D.\n2020. Learning from the worst: Dynamically generated\ndatasets to improve online hate detection. arXiv preprint\narXiv:2012.15761.\nWang, P.; Chan, A.; Ilievski, F.; Chen, M.; and Ren, X.\n2022a. Pinto: Faithful language reasoning using prompt-\ngenerated rationales. arXiv preprint arXiv:2211.01562.\nWang, P.; Wang, Z.; Li, Z.; Gao, Y .; Yin, B.; and Ren, X.\n2023. SCOTT: Self-consistent chain-of-thought distillation.\narXiv preprint arXiv:2305.01879.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang,\nS.; Chowdhery, A.; and Zhou, D. 2022b. Self-consistency\nimproves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171.\nWang, X.; Zhu, W.; and Wang, W. Y . 2023. Large language\nmodels are implicitly topic models: Explaining and finding\ngood demonstrations for in-context learning. arXiv preprint\narXiv:2301.11916.\nWang, Y .-S.; and Chang, Y . 2022. Toxicity detection\nwith generative prompt-based inference. arXiv preprint\narXiv:2205.12390.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824‚Äì24837.\nYao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao,\nY .; and Narasimhan, K. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv preprint\narXiv:2305.10601.\nYe, Y .; Le, T.; and Lee, D. 2023. NoisyHate: Bench-\nmarking Content Moderation Machine Learning Models\nwith Human-Written Perturbations Online. arXiv preprint\narXiv:2303.10430.\nZhang, T.; Luo, H.; Chuang, Y .-S.; Fang, W.; Gaitskell,\nL.; Hartvigsen, T.; Wu, X.; Fox, D.; Meng, H.; and Glass,\nJ. 2023. Interpretable unified language checking. arXiv\npreprint arXiv:2304.03728.\nZhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.;\nHou, Y .; Min, Y .; Zhang, B.; Zhang, J.; Dong, Z.; et al.\n2023. A survey of large language models. arXiv preprint\narXiv:2303.18223.\nZheng, L.; Chiang, W.-L.; Sheng, Y .; Zhuang, S.; Wu,\nZ.; Zhuang, Y .; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.;\nZhang, H.; Gonzalez, J. E.; and Stoica, I. 2023. Judg-\ning LLM-as-a-judge with MT-Bench and Chatbot Arena.\narXiv:2306.05685.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n21787",
  "topic": "Bootstrapping (finance)",
  "concepts": [
    {
      "name": "Bootstrapping (finance)",
      "score": 0.8567227125167847
    },
    {
      "name": "Computer science",
      "score": 0.6069574356079102
    },
    {
      "name": "Content (measure theory)",
      "score": 0.48263368010520935
    },
    {
      "name": "Natural language processing",
      "score": 0.42420488595962524
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34510594606399536
    },
    {
      "name": "Econometrics",
      "score": 0.3125511407852173
    },
    {
      "name": "Mathematics",
      "score": 0.14179301261901855
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}