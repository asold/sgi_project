{
  "title": "A systematic evaluation of large language models of code",
  "url": "https://openalex.org/W4281763794",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5038743835",
      "name": "Frank F. Xu",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5107243336",
      "name": "Uri Alon",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5068811427",
      "name": "Graham Neubig",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5009679905",
      "name": "Vincent J. Hellendoorn",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3170092793",
    "https://openalex.org/W2979792666",
    "https://openalex.org/W6680532216",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W6767737316",
    "https://openalex.org/W1655078475",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W2740130862",
    "https://openalex.org/W4200203799",
    "https://openalex.org/W2344444819",
    "https://openalex.org/W3011564318",
    "https://openalex.org/W2143861926",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2978835257",
    "https://openalex.org/W3170572542",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W4286696249",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2914120296"
  ],
  "abstract": "Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area. We have an online appendix at https://arxiv.org/abs/2202.13169.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8490298986434937
    },
    {
      "name": "Programming language",
      "score": 0.7076215744018555
    },
    {
      "name": "Source code",
      "score": 0.6789590120315552
    },
    {
      "name": "Code (set theory)",
      "score": 0.6556635499000549
    },
    {
      "name": "Natural language",
      "score": 0.5028156638145447
    },
    {
      "name": "Open source",
      "score": 0.47580844163894653
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4667425751686096
    },
    {
      "name": "Natural language processing",
      "score": 0.46373167634010315
    },
    {
      "name": "Language model",
      "score": 0.4306923449039459
    },
    {
      "name": "Software",
      "score": 0.1828484833240509
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.07873255014419556
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ]
}