{
  "title": "Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition",
  "url": "https://openalex.org/W3135651738",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227924421",
      "name": "Fang, Shancheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2350172560",
      "name": "Xie Hongtao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1436780140",
      "name": "Wang Yu-xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2711736853",
      "name": "Mao, Zhendong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1897470950",
      "name": "Zhang Yongdong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2402268235",
    "https://openalex.org/W1491389626",
    "https://openalex.org/W2343052201",
    "https://openalex.org/W3082397598",
    "https://openalex.org/W2963517393",
    "https://openalex.org/W70975097",
    "https://openalex.org/W2928555987",
    "https://openalex.org/W2144554289",
    "https://openalex.org/W2965066169",
    "https://openalex.org/W2952285877",
    "https://openalex.org/W2964300754",
    "https://openalex.org/W2740767790",
    "https://openalex.org/W3004846386",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2795619303",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3035160371",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2750938222",
    "https://openalex.org/W2896034938",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2008806374",
    "https://openalex.org/W2194187530",
    "https://openalex.org/W2997864923",
    "https://openalex.org/W2963233387",
    "https://openalex.org/W2997749585",
    "https://openalex.org/W3035106683",
    "https://openalex.org/W2294053032",
    "https://openalex.org/W2555182955",
    "https://openalex.org/W2964312704",
    "https://openalex.org/W2810983211",
    "https://openalex.org/W2532759528",
    "https://openalex.org/W3003642782",
    "https://openalex.org/W2963712589",
    "https://openalex.org/W3110267192",
    "https://openalex.org/W3034447740",
    "https://openalex.org/W2962790387",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1998042868",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1981283549",
    "https://openalex.org/W3035449864",
    "https://openalex.org/W2122585011",
    "https://openalex.org/W1922126009",
    "https://openalex.org/W2146835493",
    "https://openalex.org/W2998382406",
    "https://openalex.org/W1971822075"
  ],
  "abstract": "Linguistic knowledge is of great benefit to scene text recognition. However, how to effectively model linguistic rules in end-to-end deep networks remains a research challenge. In this paper, we argue that the limited capacity of language models comes from: 1) implicitly language modeling; 2) unidirectional feature representation; and 3) language model with noise input. Correspondingly, we propose an autonomous, bidirectional and iterative ABINet for scene text recognition. Firstly, the autonomous suggests to block gradient flow between vision and language models to enforce explicitly language modeling. Secondly, a novel bidirectional cloze network (BCN) as the language model is proposed based on bidirectional feature representation. Thirdly, we propose an execution manner of iterative correction for language model which can effectively alleviate the impact of noise input. Additionally, based on the ensemble of iterative predictions, we propose a self-training method which can learn from unlabeled images effectively. Extensive experiments indicate that ABINet has superiority on low-quality images and achieves state-of-the-art results on several mainstream benchmarks. Besides, the ABINet trained with ensemble self-training shows promising improvement in realizing human-level recognition. Code is available at https://github.com/FangShancheng/ABINet.",
  "full_text": "Read Like Humans: Autonomous, Bidirectional and Iterative Language\nModeling for Scene Text Recognition\nShancheng Fang Hongtao Xie * Yuxin Wang Zhendong Mao Yongdong Zhang\nUniversity of Science and Technology of China\n{fangsc, htxie, zdmao, zhyd73}@ustc.edu.cn, wangyx58@mail.ustc.edu.cn\nAbstract\nLinguistic knowledge is of great beneﬁt to scene text\nrecognition. However, how to effectively model linguistic\nrules in end-to-end deep networks remains a research chal-\nlenge. In this paper, we argue that the limited capacity\nof language models comes from: 1) implicitly language\nmodeling; 2) unidirectional feature representation; and 3)\nlanguage model with noise input. Correspondingly, we pro-\npose an autonomous, bidirectional and iterative ABINet for\nscene text recognition. Firstly, the autonomous suggests to\nblock gradient ﬂow between vision and language models\nto enforce explicitly language modeling. Secondly, a novel\nbidirectional cloze network (BCN) as the language model\nis proposed based on bidirectional feature representation.\nThirdly, we propose an execution manner of iterative correc-\ntion for language model which can effectively alleviate the\nimpact of noise input. Additionally, based on the ensemble\nof iterative predictions, we propose a self-training method\nwhich can learn from unlabeled images effectively. Extensive\nexperiments indicate that ABINet has superiority on low-\nquality images and achieves state-of-the-art results on sev-\neral mainstream benchmarks. Besides, the ABINet trained\nwith ensemble self-training shows promising improvement\nin realizing human-level recognition. Code is available at\nhttps://github.com/FangShancheng/ABINet.\n1. Introduction\nPossessing the capability of reading text from scene im-\nages is indispensable to artiﬁcial intelligence [ 24, 41]. To\nthis end, early attempts regard characters as meaningless\nsymbols and recognize the symbols by classiﬁcation mod-\nels [42, 15]. However, when confronted with challenging\nenvironments such as occlusion, blur, noise, etc., it becomes\nfaint due to out of visual discrimination. Fortunately, as\ntext carries rich linguistic information, characters can be\nreasoned according to the context. Therefore, a bunch of\n*The corresponding author\nmethods [16, 14, 29] turn their attention to language model-\ning and achieve undoubted improvement.\nHowever, how to effectively model the linguistic behavior\nin human reading is still an open problem. From the observa-\ntions of psychology, we can make three assumptions about\nhuman reading that language modeling is autonomous, bidi-\nrectional and iterative: 1) as both deaf-mute and blind people\ncould have fully functional vision and language separately,\nwe use the term autonomous to interpret the independence of\nlearning between vision and language. The autonomous also\nimplies a good interaction between vision and language that\nindependently learned language knowledge could contribute\nto the recognition of characters in vision. 2) The action of\nreasoning character context behaves like cloze task since\nillegible characters can be viewed as blanks. Thus, predic-\ntion can be made using the cues of legible characters on the\nleft side and right side of the illegible characters simultane-\nously, which is corresponding to the bidirectional. 3) The\niterative describes that under the challenging environments,\nhumans adopt a progressive strategy to improve prediction\nconﬁdence by iteratively correcting the recognized results.\nFirstly, applying the autonomous principle to scene\ntext recognition (STR) means that recognition models\nshould be decoupled into vision model (VM) and language\nmodel (LM), and the sub-models could be served as func-\ntional units independently and learned separately. Recent\nattention-based methods typically design LMs based on\nRNNs or Transformer [ 39], where the linguistic rules are\nlearned implicitly within a coupled model [ 19, 36, 33]\n(Fig. 1a). Nevertheless, whether and how well the LMs\nlearn character relationship is unknowable. Besides, this\nkind of methods is infeasible to capture rich prior knowledge\nby directly pre-training LM from large-scale unlabeled text.\nSecondly, compared with the unidirectional LMs [ 38],\nLMs with bidirectional principle capture twice the amount\nof information. A straightforward way to construct a bidirec-\ntional model is to merge a left-to-right model and a right-to-\nleft model [28, 5], either in probability-level [44, 36] or in\nfeature-level [49] (Fig. 1e). However, they are strictly less\npowerful as their language features are unidirectional repre-\n1\narXiv:2103.06495v1  [cs.CV]  11 Mar 2021\nsentation in fact. Also, the ensemble models mean twice as\nexpensive both in computations and parameters. A recent\nstriking work in NLP is BERT [5], which introduces a deep\nbidirectional representation learned by masking text tokens.\nDirectly applying BERT to STR requires masking all the\ncharacters within a text instance, whereas this is extremely\nexpensive since each time only one character can be masked.\nThirdly, LMs executed with iterative principle can re-\nﬁne the prediction from visual and linguistic cues, which\nis not explored in current methods. The canonical way to\nperform an LM is auto-regression [ 44, 3, 45] (Fig. 1d), in\nwhich error recognition is accumulated as noise and taken as\ninput for the following prediction. To adapt the Transformer\narchitectures, [ 25, 49] give up auto-regression and adopt\nparallel-prediction (Fig. 1e) to improve efﬁciency. However,\nnoise input still exists in parallel-prediction where errors\nfrom VM output directly harm the LM accuracy. In addi-\ntion, parallel-prediction in SRN [49] suffers from unaligned-\nlength problem that SRN is tough to infer correct characters\nif text length is wrongly predicted by VM.\nConsidering the deﬁciencies of current methods from the\naspects of internal interaction, feature representation and\nexecution manner, we propose ABINet guided by the princi-\nples of Autonomous, Bidirectional and Iterative. Firstly, we\nexplore a decoupled method (Fig. 1b) by blocking gradient\nﬂow (BGF) between VM and LM, which enforces LM to\nlearn linguistic rules explicitly. Besides, both VM and LM\nare autonomous units and could be pre-trained from images\nand text separately. Secondly, we design a novel bidirec-\ntional cloze network (BCN) as the LM, which eliminates the\ndilemma of combining two unidirectional models (Fig. 1c).\nThe BCN is jointly conditioned on both left and right con-\ntext, by specifying attention masks to control the accessing\nof both side characters. Also, accessing across steps is not\nallowed to prevent leaking information. Thirdly, we propose\nan execution manner of iterative correction for LM (Fig. 1b).\nBy feeding the outputs of ABINet into LM repeatedly, predic-\ntions can be reﬁned progressively and the unaligned-length\nproblem could be alleviated to a certain extent. Additionally,\ntreating the iterative predictions as an ensemble, a semi-\nsupervised method is explored based on self-training, which\nexploits a new solution toward human-level recognition.\nContributions of this paper mainly include: 1) we propose\nautonomous, bidirectional and iterative principles to guide\nthe design of LM in STR. Under these principles the LM is\na functional unit, which is required to extract bidirectional\nrepresentation and correct prediction iteratively. 2) A novel\nBCN is introduced, which estimates the probability distribu-\ntion of characters like cloze tasks using bidirectional repre-\nsentation. 3) The proposed ABINet achieves state-of-the-art\n(SOTA) performance on mainstream benchmarks, and the\nABINet trained with ensemble self-training shows promising\nimprovement in realizing human-level recognition.\ny2 <s>y1\nTrm TrmTrm\ny1 yn<e>  \n \n ` <s> yn\nTrm Trm Trm\n<e>\n \n` `` yn-1\nright-to-left left-to-right\n(e)\n(c)\ny2 yn\nTrm Trm Trm\ny1 y2 yn\n`\n \n \n `y1` yn\ntext\n(b)\nFusion\nVison\nLanguage\nimage\ntext\nfeature\nimage\n(a)\nVison\nLanguage\ntext\ny2 <s>y1\nTrm TrmTrm\ny1 yn<e>  \n \n ` <s> yn\nTrm Trm Trm\n<e>\n \n` `` yn-1\nright-to-left left-to-right\n(e)\n<s> y1 yn\nRNN RNN RNN\ny1 y2 <e> \n \n \n(d)\n(c)\ny2 yn\nTrm Trm Trm\ny1 y2 yn\n`\n \n \n `y1` yn\ntext\n(b)\nFusion\nVison\nLanguage\nimage\ntext\nfeature\nimage\n(a)\nVison\nLanguage\ntext\nFigure 1. (a) Coupled language model. (b) Our autonomous lan-\nguage model with iterative correction. (c) Our bidirectional struc-\nture. (d) Unidirectional RNN in auto-regression. (e) Ensemble of\ntwo unidirectional Transformers in parallel-prediction.\n2. Related Work\n2.1. Language-free Methods\nLanguage-free methods generally utilize visual features\nwithout the consideration of relationship between charac-\nters, such as CTC-based [ 7] and segmentation-based [ 21]\nmethods. The CTC-based methods employ CNN to ex-\ntract visual features and RNN to model features sequence.\nThen the CNN and RNN are trained end-to-end using CTC\nloss [34, 11, 37, 12]. The segmentation-based methods ap-\nply FCN to segment characters in pixel-level. Liao et al.\nrecognize characters by grouping the segmented pixels into\ntext regions. Wan et al. [40] propose an additional order\nsegmentation map which transcripts characters in the correct\norder. Due to lacking of linguistic information, the language-\nfree methods cannot resolve the recognition in low-quality\nimages commendably.\n2.2. Language-based Methods\nInternal interaction between vision and language. In\nsome early works, bags of N-grams of text string are pre-\ndicted by a CNN which acts as an explicit LM [14, 16, 13].\nAfter that the attention-based methods become popular,\nwhich implicitly models language using more powerful\nRNN [19, 36] or Transformer [43, 33]. The attention-based\nmethods follow encoder-decoder architecture, where the en-\ncoder processes images and the decoder generates characters\nby focusing on relevant information from 1D image fea-\ntures [19, 35, 36, 3, 4] or 2D image features [48, 45, 23, 20].\nFor example, R2AM [19] employs recursive CNN as a fea-\nture extractor and LSTM as a learned LM implicitly mod-\neling language in character-level, which avoids the use of\nN-grams. Further, this kind of methods is usually boosted by\nintegrating a rectiﬁcation module [36, 51, 47] for irregular\nimages before feeding the images into networks. Different\nfrom the methods above, our method strives to build a more\npowerful LM by explicitly language modeling. In attempting\n2\nFeature\nProbabilityVison Model\nBackbone\nPosition AttentionVision PredictionS HD VI N G  · · ·Linear & Softmax\nLanguage Model\nMulti-HeadAttention\nFeed Forward\nFusion Prediction @2SH OW ING \n· · ·Fusion Prediction @1\n· · ·SH OVIN G Linear & Softmax\nProbability\nNx\nFeatureLinear & SoftmaxFusion\nIterativeBidirectionalAutonomousIterative\nGround TruthSH OW INGBlocking gradient flow\nParallel time step Current time step Language prediction\nFigure 2. A schematic overview of ABINet.\nto improve the language expression, some works introduce\nmultiple losses where an additional loss comes from seman-\ntics [29, 25, 49, 6]. Among them, SEED [ 29] proposes to\nuse pre-trained FastText model to guide the training of RNN,\nwhich brings extra semantic information. We deviate from\nthis as our method directly pre-trains LM in unlabeled text,\nwhich is more feasible in practice.\nRepresentation of language features. The character se-\nquences in attention-based methods are generally modeled\nin left-to-right way [19, 35, 3, 40]. For instance, Textscan-\nner [40] inherits the unidirectional model of attention-based\nmethods. Differently, they employ an additional position\nbranch to enhance positional information and mitigate mis-\nrecogniton in contextless scenarios. To utilize bidirectional\ninformation, methods like [8, 36, 44, 49] use an ensemble\nmodel of two unidirectional models. Speciﬁcally, to capture\nglobal semantic context, SRN [49] combines features from\na left-to-right and a right-to-left Transformers for further\nprediction. We emphasize that the ensemble bidirectional\nmodel is intrinsically a unidirectional feature representation.\nExecution manner of language models. Currently, the\nnetwork architectures of LMs are mainly based on RNN and\nTransformer [39]. The RNN-based LM is usually executed in\nauto-regression [44, 3, 45], which takes the prediction of last\ncharacter as input. Typical work such as DAN [44] obtains\nthe visual features of each character ﬁrstly using proposed\nconvolutional alignment module. After that GRU predicts\neach character by taking the prediction embedding of the last\ntime step and the character feature of the current time step\nas input. The Transformer-based methods have superiority\nin parallel execution, where the inputs of each time step\nare either visual features [25] or character embedding from\nthe prediction of visual feature [49]. Our method falls into\nparallel execution, but we try to alleviate the issue of noise\ninput existing in parallel language model.\n···\nEncoding\n  V\nKQResNet+TransformerPosition AttentionVision PredictionLinear & SoftmaxS HD VI N G  \nFeatureCurrent time step Parallel time step \nProbability\nF\nFigure 3. Architecture of vision model.\n3. Proposed Method\n3.1. Vision Model\nThe vision model consists of a backbone network and a\nposition attention module (Fig. 3). Following the previous\nmethods, ResNet1 [36, 44] and Transformer units [49, 25]\nare employed as the feature extraction network and the se-\nquence modeling network. For image x we have:\nFb = T(R(x)) ∈R\nH\n4 ×W\n4 ×C, (1)\nwhere H,W are the size of x and Cis feature dimension.\nThe module of position attention transcribes visual fea-\ntures into character probabilities in parallel, which is based\non the query paradigm [39]:\nFv = softmax(QKT\n√\nC\n)V. (2)\nConcretely, Q ∈ RT×C is positional encodings [ 39] of\ncharacter orders and T is the length of character sequence.\n1There are 5 residual blocks in total and down-sampling is performed\nafter the 1st and 3nd blocks.\n3\nPrevent from attendingAllow to attend\n  \n  \nAdd & Normalize\nAdd & Normalize\nFeed Forward\nNxMulti-Head Attention\nFeature\nProbabilityMKQV1234567M\n· · ·\n· · ·\nProbability Mapping\nK & V\nLinear & Softmax\n1 234 5 6 7\nS HD VI N GAttention masksVision/Fusion  PredictionS HOVI N G\nEncoding\n· · ·\n· · ·\nS H D V I N G\nFigure 4. Architecture of language model (BCN).\nK = G(Fb) ∈R\nHW\n16 ×C, where G(·) is implemented by a\nmini U-Net2 [32]. V = H(Fb) ∈R\nHW\n16 ×C, where H(·) is\nidentity mapping.\n3.2. Language Model\n3.2.1 Autonomous Strategy\nAs shown in Fig. 2, the autonomous strategy includes follow-\ning characteristics: 1) the LM is regarded as an independent\nmodel of spelling correction which takes probability vectors\nof characters as input and outputs probability distributions\nof expected characters. 2) The ﬂow of training gradient is\nblocked (BGF) at input vectors. 3) The LM could be trained\nseparately from unlabeled text data.\nFollowing the strategy of autonomous, the ABINet can\nbe divided into interpretable units. By taking the probabil-\nity as input, LM could be replaceable ( i.e., replaced with\nmore powerful model directly) and ﬂexible (e.g., executed\niteratively in Section 3.2.3). Besides, an important point\nis that BGF enforces model to learn linguistic knowledge\ninevitably, which is radically distinguished from implicitly\nmodeling where what the models exactly learn is unknow-\nable. Furthermore, the autonomous strategy allows us to\ndirectly share the advanced progresses in NLP community.\nFor instance, pre-training the LM can be an effective way to\nboost the performance.\n3.2.2 Bidirectional Representation\nGiven a text string y = ( y1,...,y n) with text\nlength n and class number c, the conditional probabil-\nity of yi for bidirectional and unidirectional models are\n2A network with 4-layer encoder, 64 channels, add fusion and interpo-\nlation upsample.\nP(yi|yn,...,y i+1,yi−1,...,y 1) and P(yi|yi−1,...,y 1),\nrespectively. From the perspective of information the-\nory, available entropy of a bidirectional representation\ncan be quantiﬁed as Hy = ( n − 1) logc. How-\never, for a unidirectional representation the information is\n1\nn\n∑n\ni=1 (i−1) logc = 1\n2 Hy. Our insight is that previous\nmethods typically use an ensemble model of two unidirec-\ntional models, which essentially are unidirectional represen-\ntations. The unidirectional representation basically captures\n1\n2 Hy information, resulting in limited capability of feature\nabstraction compared with bidirectional counterpart.\nBeneﬁtting from the autonomous design in Section 3.2.1,\noff-the-shelf NLP models with the ability of spelling cor-\nrection can be transferred. A plausible way is utilizing the\nmasked language model (MLM) in BERT [5] by replacing\nyi with token [MASK]. However, we notice that this is un-\nacceptable as MLM should be separately called ntimes for\neach text instance, causing extreme low efﬁciency. Instead of\nmasking the input characters, we propose BCN by specifying\nthe attention masks.\nOverall, the BCN is a variant of L-layers transformer\ndecoder. Each layer of BCN is a series of multi-head at-\ntention and feed-forward network [39] followed by residual\nconnection [10] and layer normalization [ 1], as shown in\nFig. 4. Different from vanilla Transformer, character vectors\nare fed into the multi-head attention blocks rather than the\nﬁrst layer of network. In addition, attention masks in multi-\nhead attention are designed to prevent from “seeing itself”.\nBesides, no self-attention is applied in BCN to avoid leaking\ninformation across time steps. The attention operation inside\nmulti-head blocks can be formalized as:\nMij =\n{\n0, i ̸= j\n−∞, i = j , (3)\nKi = Vi = P(yi)Wl, (4)\nFmha = softmax(QKT\n√\nC\n+ M)V, (5)\nwhere Q ∈RT×C is the positional encodings of character\norders in the ﬁrst layer and the outputs of the last layer other-\nwise. K,V ∈RT×C are obtained from character probabil-\nity P(yi) ∈Rc, and Wl ∈Rc×C is linear mapping matrix.\nM ∈RT×T is the matrix of attention masks which prevents\nfrom attending current character. After stacking BCN layers\ninto deep architecture, the bidirectional representation Fl for\ntext y is determined.\nBy specifying the attention masks in cloze fashion, BCN\nis able to learn more powerful bidirectional representation\nelegantly than the ensemble of unidirectional representa-\ntion. Besides, beneﬁtting from Transformer-like architecture,\nBCN can perform computation independently and parallelly.\nAlso, it is more efﬁcient than the ensemble models as only\nhalf of the computations and parameters are needed.\n4\n3.2.3 Iterative Correction\nThe parallel-prediction of Transformer takes noise inputs\nwhich are typically approximations from visual predic-\ntion [49] or visual feature [25]. Concretely, as the example\nshown in Fig. 2 under bidirectional representation, the de-\nsired condition for P(“O”) is “SH-WING”. However, due to\nthe blurred and occluded environments, the actual condition\nobtained from VM is “SH-VING”, in which “V” becomes\nnoise and harms the conﬁdence of prediction. It tends to be\nmore hostile for LM with increased error predictions in VM.\nTo cope with the problem of noise inputs, we propose\niterative LM (illustrated in Fig. 2). The LM is executed M\ntimes repeatedly with different assignment fory. For the ﬁrst\niteration, yi=1 is the probability prediction from VM. For the\nsubsequent iterations, yi≥2 is the probability prediction from\nthe fusion model (Section 3.3) in last iteration. By this way\nthe LM is able to correct the vision prediction iteratively.\nAnother observation is that Transformer-based methods\ngenerally suffer from unaligned-length problem [49], which\ndenotes that the Transformer is hard to correct the vision\nprediction if character number is unaligned with ground truth.\nThe unaligned-length problem is caused by the inevitable\nimplementation of padding mask which is ﬁxed for ﬁltering\ncontext outside text length. Our iterative LM can alleviate\nthis problem as the visual feature and linguistic feature are\nfused several times, and thus the predicted text length is also\nreﬁned gradually.\n3.3. Fusion\nConceptually, vision model trained on image and lan-\nguage model trained on text come from different modalities.\nTo align visual feature and linguistic feature, we simply use\nthe gated mechanism [49, 50] for ﬁnal decision:\nG = σ([Fv,Fl]Wf), (6)\nFf = G ⊙Fv + (1−G) ⊙Fl, (7)\nwhere Wf ∈R2C×C and G ∈RT×C.\n3.4. Supervised Training\nABINet is trained end-to-end using the following multi-\ntask objectives:\nL= λvLv + λl\nM\nM∑\ni=1\nLi\nl + 1\nM\nM∑\ni=1\nLi\nf, (8)\nwhere Lv, Ll and Lf are the cross entropy losses from Fv,\nFl and Ff, respectively. Speciﬁcally, Li\nl and Li\nf are the\nlosses at i-th iteration. λv and λl are balanced factors.\n3.5. Semi-supervised Ensemble Self-training\nTo further explore the superiority of our iterative model,\nwe propose a semi-supervised learning method based on\nAlgorithm 1Ensemble Self-training\nRequire: Labeled images Xwith labels Yand unlabeled images U\n1: Train parameters θ0 of ABINet with (X, Y) using Equation 8.\n2: Use θ0 to generate soft pseudo labels Vfor U\n3: Get (U′, V′) by ﬁltering (U, V) with C<Q (Equation 9)\n4: for i= 1,...,N max do\n5: if i== Nupl then\n6: Update Vusing θi\n7: Get (U′, V′) by ﬁltering (U, V) with C<Q (Equation 9)\n8: end if\n9: Sample Bl = (Xb, Yb) ⫋ (X, Y), Bu = (U′\nb, V′\nb) ⫋ (U′, V′)\n10: Update θi with Bl, Bu using Equation 8.\n11: end for\nself-training [46] with the ensemble of iterative predictions.\nThe basic idea of self-training is ﬁrst to generate pseudo\nlabels by model itself, and then re-train the model using\nadditional pseudo labels. Therefore, the key problem lies in\nconstructing high-quality pseudo labels.\nTo ﬁlter the noise pseudo labels we propose the following\nmethods: 1) minimum conﬁdence of characters within a\ntext instance is chosen as the text certainty. 2) Iterative\npredictions of each character are viewed as an ensemble to\nsmooth the impact of noise labels. Therefore, we deﬁne the\nﬁltering function as follows:\n\n\n\nC = min\n1≤t≤T\neE[log P(yt)]\nP(yt) = max\n1≤m≤M\nPm(yt) , (9)\nwhere Cis the minimum certainty of a text instance, Pm(yt)\nis probability distribution of t-th character at m-th iteration.\nThe training procedure is depicted in Algorithm 1, where Q\nis threshold. Bl, Bu are training batches from labeled and\nunlabeled data. Nmax is the maximum number of training\nstep and Nupl is the step number for updating pseudo labels.\n4. Experiment\n4.1. Datasets and Implementation Details\nExperiments are conducted following the setup of [ 49]\nin the purpose of fair comparison. Concretely, the training\ndatasets are two synthetic datasets MJSynth (MJ) [ 13, 15]\nand SynthText (ST) [9]. Six standard benchmarks include\nICDAR 2013 (IC13) [18], ICDAR 2015 (IC15) [17], IIIT 5K-\nWords (IIIT) [27], Street View Text (SVT) [42], Street View\nText-Perspective (SVTP) [30] and CUTE80 (CUTE) [ 31]\nare as the testing datasets. Details of these datasets can be\nfound in the previous works [49]. In addition, Uber-Text [52]\nremoving the labels is used as unlabeled dataset to evaluate\nthe semi-supervised method.\nThe model dimension Cis set to 512 throughout. There\nare 4 layers in BCN with 8 attention heads each layer. Bal-\nanced factors λv, λl are set to 1, 1 respectively. Images are\ndirectly resized to 32 ×128 with data augmentation such as\ngeometry transformation (i.e., rotation, afﬁne and perspec-\ntive), image quality deterioration and color jitter,etc. We use\n5\nTable 1. Ablation study of VM. Attn is the attention method and\nTrm Layer is the layer number of Transformer. SV , MV1, MV2 and\nLV are four VMs in different conﬁgurations.\nModel Attn Trm IC13 SVT IIIT Avg Params Time3\nName Layer IC15 SVTP CUTE ( ×106) (ms)\nSV parallel 2 94.2 89.6 93.7 88.8 19.6 12.5(small) 80.6 82.3 85.1\nMV1 position 2 93.6 89.3 94.2 89.0 20.4 14.9(middle) 80.8 83.1 85.4\nMV2 parallel 3 94.5 89.5 94.3 89.4 22.8 14.8(middle) 81.1 83.7 86.8\nLV position 3 94.9 90.4 94.6 89.8 23.5 16.7(large) 81.7 84.2 86.5\nTable 2. Ablation study of autonomous strategy. PVM is pre-\ntraining VM on MJ and ST in supervised way. PLM in is pre-\ntraining LM using text on MJ and ST in self-supervised way.\nPLMout is pre-training LM on WikiText-103 [26] in self-supervised\nway. AGF means allowing gradient ﬂow between VM and LM.\nPVM PLMin PLMout AGF IC13 SVT IIIT AvgIC15 SVTP CUTE\n- - - - 96.7 93.4 95.7 91.784.5 86.8 86.8\n\u0013 - - - 97.0 93.0 96.3 92.385.0 88.5 89.2\n- \u0013 - - 97.1 93.8 95.5 91.683.6 88.1 86.8\n\u0013 \u0013 - - 97.2 93.5 96.3 92.384.9 89.0 88.5\n\u0013 - \u0013 - 97.0 93.7 96.5 92.585.3 88.5 89.6\n\u0013 - - \u0013 96.7 92.6 95.7 91.483.3 86.5 88.5\n4 NVIDIA 1080Ti GPUs to train our models with batch size\n384. ADAM optimizer is adopted with the initial learning\nrate 1e−4, which is decayed to 1e−5 after 6 epochs.\n4.2. Ablation Study\n4.2.1 Vision Model\nFirstly, we discuss the performance of VM from two aspects:\nfeature extraction and sequence modeling. Experiment re-\nsults are recorded in Tab. 1. The parallel attention is a\npopular attention method [25, 49], and the proposed position\nattention has a more powerful representation of key/value\nvectors. From the statistics we can conclude: 1) simply up-\ngrading the VM will result in great gains in accuracy but\nat the cost of parameter and speed. 2) To upgrade the VM,\nwe can use the position attention in feature extraction and a\ndeeper transformer in sequence modeling.\n4.2.2 Language Model\nAutonomous Strategy. To analyze the autonomous mod-\nels, we adopt the LV and BCN as VM and LM respectively.\nFrom the results in Tab. 2 we can observe: 1) pre-training\nVM is useful which boosts the accuracy about 0.6%-0.7%\non average; 2) the beneﬁt of pre-training LM on the training\n3Inference time is estimated using NVIDIA Tesla V100 by averaging 3\ndifferent trials.\nTable 3. Ablation study of bidirectional representation.\nVision LanguageIC13 SVT IIIT Avg Params Time\nIC15 SVTP CUTE ( ×106) (ms)\nSRN-U 96.0 90.3 94.9 90.2 32.8 19.181.9 86.0 85.4\nSV SRN 96.3 90.9 95.0 90.6 45.4 24.282.6 86.4 87.5\nBCN 96.7 91.7 95.3 91.0 32.8 19.583.1 86.2 88.9\nSRN-U 96.0 91.2 96.2 91.5 36.7 22.184.0 86.8 87.8\nLV SRN 96.8 92.3 96.3 91.9 49.3 26.984.2 87.9 88.2\nBCN 97.0 93.0 96.3 92.3 36.7 2285.0 88.5 89.2\nTable 4. Top-5 accuracy of LMs in text spelling correction.\nLanguage Model Character Accuracy Word Accuracy\nSRN 78.3 27.6\nBCN 82.8 41.9\ndatasets (i.e., MJ and ST) is negligible; 3) while pre-training\nLM from an additional unlabeled dataset ( e.g., WikiText-\n103) is helpful even when the base model is in high accuracy.\nThe above observations suggest that it is useful for STR to\npre-train both VM and LM. Pre-training LM on additional\nunlabeled datasets is more effective than on training datasets\nsince the limited text diversity and biased data distribution\nare unable to facilitate the learning of a well-performed LM.\nAlso, pre-training LM on unlabeled datasets is cheap since\nadditional data is available easily.\nBesides, by allowing gradient ﬂow (AGF) between VM\nand LM, the performance decreases 0.9% on average (Tab. 2.\nWe also notice that the training loss of AGF reduces sharply\nto a lower value. This indicates that overﬁtting occurs in\nLM as the VM helps to cheat in training, which might also\nhappen in implicitly language modeling. Therefore it is\ncrucial to enforce LM to learn independently by BGF. We\nnote that SRN [49] uses argmax operation after VM, which\nis intrinsically a special case of BGF since argmax is non-\ndifferentiable. Another advantage is that the autonomous\nstrategy makes the models a better interpretability, since we\ncan have a deep insight into the performance of LM ( e.g.,\nTab. 4), which is infeasible in implicitly language modeling.\nBidirectional Representation. As the BCN is a variant of\nTransformer, we compare BCN with its counterpart SRN.\nThe Transformer-based SRN [ 49] shows superior perfor-\nmance which is an ensemble of unidirectional representation.\nFor fair comparison, experiments are conducted with the\nsame conditions except the networks. We use SV and LV as\nthe VMs to validate the effectiveness at different accuracy\nlevels. As depicted in Tab. 3, though BCN has similar pa-\nrameters and inference speed as the unidirectional version of\nSRN (SRN-U), it achieves competitive advantage in accuracy\nunder different VMs. Besides, compared with the bidirec-\ntional SRN in ensemble, BCN shows better performance\nespecially on challenging datasets such as IC15 and CUTE.\nAlso, ABINet equipped with BCN is about 20%-25% faster\n6\nFigure 5. Visualization of top-5 probability in BCN.\nTable 5. Ablation study of iterative correction.\nModel Iteration IC13 SVT IIIT Avg Params Time\nNumber IC15 SVTP CUTE ( ×106) (ms)\nSV 1 96.7 91.7 95.3 91.0 32.8 19.583.1 86.2 88.9\n+ 2 97.2 91.8 95.4 91.2 32.8 24.583.3 86.4 89.2\nBCN 3 97.1 93.0 95.4 91.4 32.8 31.683.4 86.7 89.6\nLV 1 97.0 93.0 96.3 92.3 36.7 2285.0 88.5 89.2\n+ 2 97.1 93.4 96.3 92.4 36.7 27.385.2 88.7 89.6\nBCN 3 97.3 94.0 96.4 92.6 36.7 33.985.5 89.1 89.2\nthan SRN, which is practical for large-scale tasks.\nSection 3.2.1 has argued that the LMs can be viewed as\nindependent units to estimate the probability distribution of\nspelling correction, and thus we conduct experiments from\nthis view. The training set is the text from MJ and ST. To\nsimulate spelling errors, the testing set is 20000 items which\nare chosen randomly, where we add or remove a character\nfor 20% text, replace a character for 60% text and keep the\nrest of the text unchangeable. From the results in Tab. 4, we\ncan see BCN outperforms SRN by 4.5% character accuracy\nand 14.3% word accuracy, which indicates that BCN has a\nmore powerful ability in character-level language modeling.\nTo better understand how BCN works inside ABINet, we\nvisualize the top-5 probability in Fig. 5, which takes “today”\nas an example. On the one hand, as “today” is a string\nwith semantic information, taking “-oday” and “tod-y” as\ninputs, BCN can predict “t” and “a” with high conﬁdence and\ncontribute to ﬁnal fusion predictions. On the other hand, as\nerror characters “l” and “o” are noise for the rest predictions,\nBCN becomes less conﬁdent and has little impact to ﬁnal\npredictions. Besides, if there are multiple error characters,\nit is hard for BCN to restore correct text due to lacking of\nenough context.\nIterative Correction. We apply SV and LV again with\nBCN to demonstrate the performance of iterative correction\nfrom different levels. Experiment results are given in Tab. 5,\nwhere the iteration numbers are set to 1, 2 and 3 both in\ntraining and testing. As can be seen from the results, iterating\nthe BCN 3 times can respectively boost the accuracy by\n0.4%, 0.3% on average. Speciﬁcally, there are little gains on\nIIIT which is a relatively easy dataset with clear character\nappearance. However, when it comes to other harder datasets\nFigure 6. Accuracy of iterating BCN in training and testing.\nearning earni g \nearnin  earning\navailable availall  .   \navailabl   available\nbetty betin \nbetiy  betty\nschool sehdol \nscnool school\nbookstore booksstre \nbooksttre bookstore\njeanswear isanswear \niaanswear jeanswear\nbelgium belyjum \nbelyium belgium\nchristmas  chyustmaa \nchrustmaa christmas\nFigure 7. Successful examples using iterative correction. Text\nstrings are ground truth, vision prediction, fusion prediction without\niterative correction and with iterative correction respectively from\nleft to right and top to bottom.\nsuch as IC15, SVT and SVTP, the iterative correction steadily\nincreases the accuracy and achieves up to 1.3% and 1.0%\nimprovement on SVT for SV and LV respectively. It is\nalso noted that the inference time increases linearly with the\niteration number.\nWe further explore the difference of iteration between\ntraining and testing. The ﬂuctuation of average accuracy in\nFig. 6 suggests that: 1) directly applying iterative correction\nin testing also works well; 2) while iterating in training is\nbeneﬁcial since it provides additional training samples for\nLM; 3) the accuracy reaches a saturated state when iterating\nthe model more than 3 times, and therefore a big iteration\nnumber is unnecessary.\nTo have a comprehensive cognition about iterative cor-\nrection, we visualize the intermediate predictions in Fig. 7.\nTypically, the vision predictions can be revised approaching\nto ground truth while remain errors in some cases. After\nmultiple iterations, the predictions can be corrected ﬁnally.\nBesides, we also observe that iterative correction is able to\nalleviate the unaligned-length problem, as shown in the last\ncolumn in Fig. 7.\nFrom the ablation study we can conclude: 1) the bidirec-\ntional BCN is a powerful LM which can effectively improve\nthe performance both in accuracy and speed. 2) By further\nequipping BCN with iterative correction, the noise input\nproblem can be alleviated, which is recommended to deal\nwith challenging examples such as low-quality images at the\nexpense of incremental computations.\n4.3. Comparisons with State-of-the-Arts\nGenerally, it is not an easy job to fairly compare with\nother methods directly using the reported statistics [ 2], as\ndifferences might exist in backbone (i.e., CNN structure and\n7\nTable 6. Accuracy comparison with other methods.\nMethods Labeled Unlabeled Regular Text Irregular Text\nDatasets Datasets IC13 SVT IIIT IC15 SVTP CUTE\nSOTA methods\n2019 Lyuet al. [25] (Parallel) MJ+ST - 92.7 90.1 94.0 76.3 82.3 86.8\n2019 Liaoet al. [22] (SAM) MJ+ST - 95.3 90.6 93.9 77.3 82.2 87.8\n2020 Qiaoet al. [29] (SE-ASTER) MJ+ST - 92.8 89.6 93.8 80.0 81.4 83.6\n2020 Wanet al. [40] (Textscanner) MJ+ST - 92.9 90.1 93.9 79.4 84.3 83.3\n2020 Wanget al. [44] (DAN) MJ+ST - 93.9 89.2 94.3 74.5 80.0 84.4\n2020 Yueet al. [50] (RobustScanner) MJ+ST - 94.8 88.1 95.3 77.1 79.5 90.3\n2020 Yuet al. [49] (SRN) MJ+ST - 95.5 91.5 94.8 82.7 85.1 87.8\nOurs\nSRN-SV (Reproduced) MJ+ST - 96.3 90.9 95.0 82.6 86.4 87.5\nABINet-SV MJ+ST - 96.8 93.2 95.4 84.0 87.0 88.9\nSRN-LV (Reproduced) MJ+ST - 96.8 92.3 96.3 84.2 87.9 88.2\nABINet-LV MJ+ST - 97.4 93.5 96.2 86.0 89.3 89.2\nABINet-LVst MJ+ST Uber-Text 97.3 94.9 96.8 87.4 90.1 93.4\nABINet-LVest MJ+ST Uber-Text 97.7 95.5 97.2 86.9 89.9 94.1\noscar\nanaheim\n museum\ncrush\nspecial\n epidor\nmandarin\nlittle\nFigure 8. Hard examples successfully recognized by ABINet-LVest.\nparameters), data processing (i.e., images rectiﬁcation and\ndata augmentation) and training tricks, etc. To strictly per-\nform fair comparison, we reproduce the SOTA algorithm\nSRN which shares the same experiment conﬁguration with\nABINet, as presented in Tab. 6. The two reimplemented\nSRN-SV and SRN-LV are slightly different from the re-\nported model by replacing VMs, removing the side-effect\nof multi-scales training, applying decayed learning rate, etc.\nNote that SRN-SV performs somewhat better than SRN due\nto the above tricks. As can be seen from the comparison, our\nABINet-SV outperforms SRN-SV with 0.5%, 2.3%, 0.4%,\n1.4%, 0.6%, 1.4% on IC13, SVT, IIIT, IC15, SVTP and\nCUTE datasets respectively. Also, the ABINet-LV with a\nmore strong VM achieve an improvement of 0.6%, 1.2%,\n1.8%, 1.4%, 1.0% on IC13, SVT, IC15, SVTP and CUTE\nbenchmarks over its counterpart.\nCompared with recent SOTA works that are trained on MJ\nand ST, ABINet also shows impressive performance (Tab. 6).\nEspecially, ABINet has prominent superiority on SVT, SVTP\nand IC15 as these datasets contain a large amount of low-\nquality images such as noise and blurred images, which the\nVM is not able to conﬁdently recognize. Besides, we also\nﬁnd that images with unusual-font and irregular text can be\nsuccessfully recognized as the linguistic information acts\nas an important complement to visual feature. Therefore\nABINet can obtain second best result on CUTE even without\nimage rectiﬁcation.\n4.4. Semi-Supervised Training\nTo further push the boundary of accurate reading, we\nexplore a semi-supervised method which utilizes MJ and\nST as the labeled datasets and Uber-Text as the unlabeled\ndataset. The threshold Qin Section 3.5 is set to 0.9, and the\nbatch size of Bl and Bu are 256 and 128 respectively. Exper-\niment results in Tab. 6 show that the proposed self-training\nmethod ABINet-LVst can easily outperform ABINet-LV\non all benchmark datasets. Besides, the ensemble self-\ntraining ABINet-LVest shows a more stable performance\nby improving the efﬁciency of data utilization. Observing\nthe boosted results we ﬁnd that hard examples with scarce\nfonts and blurred appearance can also be recognized fre-\nquently (Fig. 8), which suggests that exploring the semi-\n/unsupervised learning methods is a promising direction for\nscene text recognition.\n5. Conclusion\nIn this paper, we propose ABINet which explores effec-\ntive approaches for utilizing linguistic knowledge in scene\ntext recognition. The ABINet is 1) autonomous that im-\nproves the ability of language model by enforcing learning\nexplicitly; 2) bidirectional that learns text representation\nby jointly conditioning on character context at both sides;\nand 3) iterative that corrects the prediction progressively to\nalleviate the impact of noise input. Based on the ABINet\nwe further propose an ensemble self-training method for\nsemi-supervised learning. Experiment results on standard\nbenchmarks demonstrate the superiority of ABINet espe-\ncially on low-quality images. In addition, we also claim\nthat exploiting unlabeled data is possible and promising for\nachieving human-level recognition.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. arXiv preprint arXiv:1607.06450, 2016.\n4\n[2] Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park,\nDongyoon Han, Sangdoo Yun, Seong Joon Oh, and Hwalsuk\nLee. What is wrong with scene text recognition model com-\nparisons? dataset and model analysis. In Proceedings of the\n8\nIEEE International Conference on Computer Vision, pages\n4715–4723, 2019. 7\n[3] Zhanzhan Cheng, Fan Bai, Yunlu Xu, Gang Zheng, Shiliang\nPu, and Shuigeng Zhou. Focusing attention: Towards accurate\ntext recognition in natural images. In Proceedings of the\nIEEE international conference on computer vision , pages\n5076–5084, 2017. 2, 3\n[4] Zhanzhan Cheng, Yangliu Xu, Fan Bai, Yi Niu, Shiliang\nPu, and Shuigeng Zhou. Aon: Towards arbitrarily-oriented\ntext recognition. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 5571–5579,\n2018. 2\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional transform-\ners for language understanding. In Proceedings of NAACL-\nHLT, pages 4171–4186, 2019. 1, 2, 4\n[6] Shancheng Fang, Hongtao Xie, Zheng-Jun Zha, Nannan Sun,\nJianlong Tan, and Yongdong Zhang. Attention and language\nensemble for scene text recognition with convolutional se-\nquence modeling. In Proceedings of the 26th ACM inter-\nnational conference on Multimedia , pages 248–256, 2018.\n3\n[7] Alex Graves, Santiago Fern ´andez, Faustino Gomez, and\nJ¨urgen Schmidhuber. Connectionist temporal classiﬁcation:\nlabelling unsegmented sequence data with recurrent neural\nnetworks. In Proceedings of the 23rd international conference\non Machine learning, pages 369–376, 2006. 2\n[8] Alex Graves, Marcus Liwicki, Santiago Fern´andez, Roman\nBertolami, Horst Bunke, and J¨urgen Schmidhuber. A novel\nconnectionist system for unconstrained handwriting recog-\nnition. IEEE transactions on pattern analysis and machine\nintelligence, 31(5):855–868, 2008. 3\n[9] Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman. Syn-\nthetic data for text localisation in natural images. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 2315–2324, 2016. 5\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 4\n[11] Pan He, Weilin Huang, Yu Qiao, Chen Change Loy, and\nXiaoou Tang. Reading scene text in deep convolutional se-\nquences. In Thirtieth AAAI conference on artiﬁcial intelli-\ngence, 2016. 2\n[12] Wenyang Hu, Xiaocong Cai, Jun Hou, Shuai Yi, and Zhiping\nLin. Gtc: Guided training of ctc towards efﬁcient and accurate\nscene text recognition. In AAAI, pages 11005–11012, 2020. 2\n[13] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and An-\ndrew Zisserman. Synthetic data and artiﬁcial neural networks\nfor natural scene text recognition. In NIPS Deep Learning\nWorkshop, 2014. 2, 5\n[14] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and An-\ndrew Zisserman. Deep structured output learning for uncon-\nstrained text recognition. In International Conference on\nLearning Representations (ICLR), 2015. 1, 2\n[15] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and An-\ndrew Zisserman. Reading text in the wild with convolutional\nneural networks. International Journal of Computer Vision,\n116(1):1–20, 2016. 1, 5\n[16] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman.\nDeep features for text spotting. In European conference on\ncomputer vision, pages 512–528. Springer, 2014. 1, 2\n[17] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nico-\nlaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwa-\nmura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chan-\ndrasekhar, Shijian Lu, et al. Icdar 2015 competition on robust\nreading. In 13th International Conference on Document Anal-\nysis and Recognition), pages 1156–1160. IEEE, 2015. 5\n[18] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida,\nMasakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles\nMestre, Joan Mas, David Fernandez Mota, Jon Almazan Al-\nmazan, and Lluis Pere De Las Heras. Icdar 2013 robust\nreading competition. In 2013 12th International Conference\non Document Analysis and Recognition , pages 1484–1493.\nIEEE, 2013. 5\n[19] Chen-Yu Lee and Simon Osindero. Recursive recurrent nets\nwith attention modeling for ocr in the wild. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 2231–2239, 2016. 1, 2, 3\n[20] Hui Li, Peng Wang, Chunhua Shen, and Guyu Zhang. Show,\nattend and read: A simple and strong baseline for irregular\ntext recognition. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 33, pages 8610–8617, 2019. 2\n[21] Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, and Yichen Wei.\nFully convolutional instance-aware semantic segmentation.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 2359–2367, 2017. 2\n[22] Minghui Liao, Pengyuan Lyu, Minghang He, Cong Yao,\nWenhao Wu, and Xiang Bai. Mask textspotter: An end-to-\nend trainable neural network for spotting text with arbitrary\nshapes. IEEE transactions on pattern analysis and machine\nintelligence, 2019. 8\n[23] Minghui Liao, Jian Zhang, Zhaoyi Wan, Fengming Xie, Jia-\njun Liang, Pengyuan Lyu, Cong Yao, and Xiang Bai. Scene\ntext recognition from two-dimensional perspective. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence ,\nvolume 33, pages 8714–8721, 2019. 2\n[24] Shangbang Long, Xin He, and Cong Yao. Scene text detection\nand recognition: The deep learning era. International Journal\nof Computer Vision, pages 1–24, 2020. 1\n[25] Pengyuan Lyu, Zhicheng Yang, Xinhang Leng, Xiaojun Wu,\nRuiyu Li, and Xiaoyong Shen. 2d attentional irregular scene\ntext recognizer. arXiv preprint arXiv:1906.05708, 2019. 2, 3,\n5, 6, 8\n[26] Stephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. Pointer sentinel mixture models. arXiv\npreprint arXiv:1609.07843, 2016. 6\n[27] Anand Mishra, Karteek Alahari, and CV Jawahar. Scene text\nrecognition using higher order language priors. In British\nMachine Vision Conference (BMVC), 2012. 5\n[28] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gard-\nner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\nDeep contextualized word representations. In Proceedings of\nNAACL-HLT, pages 2227–2237, 2018. 1\n9\n[29] Zhi Qiao, Yu Zhou, Dongbao Yang, Yucan Zhou, and Weip-\ning Wang. Seed: Semantics enhanced encoder-decoder\nframework for scene text recognition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13528–13537, 2020. 1, 3, 8\n[30] Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan\nTian, and Chew Lim Tan. Recognizing text with perspective\ndistortion in natural scenes. In Proceedings of the IEEE\nInternational Conference on Computer Vision , pages 569–\n576, 2013. 5\n[31] Anhar Risnumawan, Palaiahankote Shivakumara, Chee Seng\nChan, and Chew Lim Tan. A robust arbitrary text detec-\ntion system for natural scene images. Expert Systems with\nApplications, 41(18):8027–8048, 2014. 5\n[32] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn International Conference on Medical image computing and\ncomputer-assisted intervention, pages 234–241, 2015. 4\n[33] Fenfen Sheng, Zhineng Chen, and Bo Xu. Nrtr: A no-\nrecurrence sequence-to-sequence model for scene text recog-\nnition. In 2019 International Conference on Document Anal-\nysis and Recognition (ICDAR), pages 781–786. IEEE, 2019.\n1, 2\n[34] Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end\ntrainable neural network for image-based sequence recog-\nnition and its application to scene text recognition. IEEE\ntransactions on pattern analysis and machine intelligence ,\n39(11):2298–2304, 2016. 2\n[35] Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao,\nand Xiang Bai. Robust scene text recognition with automatic\nrectiﬁcation. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages 4168–4176, 2016.\n2, 3\n[36] Baoguang Shi, Mingkun Yang, Xinggang Wang, Pengyuan\nLyu, Cong Yao, and Xiang Bai. Aster: An attentional scene\ntext recognizer with ﬂexible rectiﬁcation. IEEE transactions\non pattern analysis and machine intelligence , 41(9):2035–\n2048, 2018. 1, 2, 3\n[37] Bolan Su and Shijian Lu. Accurate recognition of words in\nscenes without character segmentation using recurrent neural\nnetwork. Pattern Recognition, 63:397–405, 2017. 2\n[38] Martin Sundermeyer, Ralf Schl¨uter, and Hermann Ney. Lstm\nneural networks for language modeling. In Thirteenth an-\nnual conference of the international speech communication\nassociation, 2012. 1\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017. 1,\n3, 4\n[40] Zhaoyi Wan, Mingling He, Haoran Chen, Xiang Bai, and\nCong Yao. Textscanner: Reading characters in order for\nrobust scene text recognition. 2020. 2, 3, 8\n[41] Zhaoyi Wan, Jielei Zhang, Liang Zhang, Jiebo Luo, and Cong\nYao. On vocabulary reliance in scene text recognition. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 11425–11434, 2020. 1\n[42] Kai Wang, Boris Babenko, and Serge Belongie. End-to-end\nscene text recognition. In 2011 International Conference on\nComputer Vision, pages 1457–1464. IEEE, 2011. 1, 5\n[43] Peng Wang, Lu Yang, Hui Li, Yuyan Deng, Chunhua Shen,\nand Yanning Zhang. A simple and robust convolutional-\nattention network for irregular text recognition.arXiv preprint\narXiv:1904.01375, 6, 2019. 2\n[44] Tianwei Wang, Yuanzhi Zhu, Lianwen Jin, Canjie Luo, Xi-\naoxue Chen, Yaqiang Wu, Qianying Wang, and Mingxiang\nCai. Decoupled attention network for text recognition. In\nAAAI, pages 12216–12224, 2020. 1, 2, 3, 8\n[45] Zbigniew Wojna, Alexander N Gorban, Dar-Shyang Lee,\nKevin Murphy, Qian Yu, Yeqing Li, and Julian Ibarz.\nAttention-based extraction of structured information from\nstreet view imagery. In 2017 14th IAPR International Con-\nference on Document Analysis and Recognition (ICDAR) ,\nvolume 1, pages 844–850. IEEE, 2017. 2, 3\n[46] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le.\nSelf-training with noisy student improves imagenet classiﬁca-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 10687–10698,\n2020. 5\n[47] Mingkun Yang, Yushuo Guan, Minghui Liao, Xin He, Kaigui\nBian, Song Bai, Cong Yao, and Xiang Bai. Symmetry-\nconstrained rectiﬁcation network for scene text recognition.\nIn Proceedings of the IEEE International Conference on Com-\nputer Vision, pages 9147–9156, 2019. 2\n[48] Xiao Yang, Dafang He, Zihan Zhou, Daniel Kifer, and C Lee\nGiles. Learning to read irregular text with attention mecha-\nnisms. In IJCAI, volume 1, page 3, 2017. 2\n[49] Deli Yu, Xuan Li, Chengquan Zhang, Tao Liu, Junyu Han,\nJingtuo Liu, and Errui Ding. Towards accurate scene text\nrecognition with semantic reasoning networks. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 12113–12122, 2020. 1, 2, 3, 5, 6,\n8\n[50] Xiaoyu Yue, Zhanghui Kuang, Chenhao Lin, Hongbin Sun,\nand Wayne Zhang. Robustscanner: Dynamically enhancing\npositional clues for robust text recognition. In Proceedings of\nthe European Conference on Computer Vision (ECCV), 2020.\n5, 8\n[51] Fangneng Zhan and Shijian Lu. Esir: End-to-end scene text\nrecognition via iterative image rectiﬁcation. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 2059–2068, 2019. 2\n[52] Ying Zhang, Lionel Gueguen, Ilya Zharkov, Peter Zhang,\nKeith Seifert, and Ben Kadlec. Uber-text: A large-scale\ndataset for optical character recognition from street-level im-\nagery. In SUNw: Scene Understanding Workshop - CVPR\n2017, 2017. 5\n10",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8419829607009888
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.6513888835906982
    },
    {
      "name": "Language model",
      "score": 0.6498907208442688
    },
    {
      "name": "Representation (politics)",
      "score": 0.5709794759750366
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5629119873046875
    },
    {
      "name": "Block (permutation group theory)",
      "score": 0.5552680492401123
    },
    {
      "name": "Code (set theory)",
      "score": 0.5403493046760559
    },
    {
      "name": "Noise (video)",
      "score": 0.5071697235107422
    },
    {
      "name": "Natural language processing",
      "score": 0.4369230270385742
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3567380905151367
    },
    {
      "name": "Image (mathematics)",
      "score": 0.23090845346450806
    },
    {
      "name": "Linguistics",
      "score": 0.07721278071403503
    },
    {
      "name": "Programming language",
      "score": 0.07172131538391113
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [],
  "cited_by": 21
}