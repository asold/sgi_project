{
  "title": "Music-evoked emotions classification using vision transformer in EEG signals",
  "url": "https://openalex.org/W4393944549",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2057628437",
      "name": "Dong Wang",
      "affiliations": [
        "Shandong Management University"
      ]
    },
    {
      "id": "https://openalex.org/A2108653094",
      "name": "Jian Lian",
      "affiliations": [
        "Shandong Management University"
      ]
    },
    {
      "id": "https://openalex.org/A2225421015",
      "name": "Hebin Cheng",
      "affiliations": [
        "Shandong Management University"
      ]
    },
    {
      "id": "https://openalex.org/A2096754853",
      "name": "Yanan Zhou",
      "affiliations": [
        "Beijing Foreign Studies University"
      ]
    },
    {
      "id": "https://openalex.org/A2057628437",
      "name": "Dong Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108653094",
      "name": "Jian Lian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2225421015",
      "name": "Hebin Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096754853",
      "name": "Yanan Zhou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2625929003",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W2792193826",
    "https://openalex.org/W2166055528",
    "https://openalex.org/W2982161360",
    "https://openalex.org/W2950557962",
    "https://openalex.org/W3176187724",
    "https://openalex.org/W4307370703",
    "https://openalex.org/W4315701836",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2042266440",
    "https://openalex.org/W4293223625",
    "https://openalex.org/W4214614183",
    "https://openalex.org/W2789607334",
    "https://openalex.org/W2782273434",
    "https://openalex.org/W4206908116",
    "https://openalex.org/W3184236853",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3215947932",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W1980553000",
    "https://openalex.org/W2922265930",
    "https://openalex.org/W2146010402",
    "https://openalex.org/W4386441900",
    "https://openalex.org/W4318812261",
    "https://openalex.org/W2002055708",
    "https://openalex.org/W6852753519",
    "https://openalex.org/W2903959724",
    "https://openalex.org/W6677089919",
    "https://openalex.org/W4225279778",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2774325121",
    "https://openalex.org/W4221092124",
    "https://openalex.org/W2942441362",
    "https://openalex.org/W3205820451",
    "https://openalex.org/W4389105052",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W4210767542",
    "https://openalex.org/W6639824700",
    "https://openalex.org/W4207006143",
    "https://openalex.org/W2152772614",
    "https://openalex.org/W2136559866",
    "https://openalex.org/W4310813888",
    "https://openalex.org/W2759680345",
    "https://openalex.org/W2889782437",
    "https://openalex.org/W3086601978",
    "https://openalex.org/W3045665366",
    "https://openalex.org/W4200149041",
    "https://openalex.org/W3201879177",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1819836888",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2880583584",
    "https://openalex.org/W6779248606",
    "https://openalex.org/W2952286992",
    "https://openalex.org/W4321793421",
    "https://openalex.org/W6847098356",
    "https://openalex.org/W2982299617",
    "https://openalex.org/W2896297654",
    "https://openalex.org/W3184613951",
    "https://openalex.org/W3197261948",
    "https://openalex.org/W2963927307",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W4311559518",
    "https://openalex.org/W4377866341",
    "https://openalex.org/W2913768539",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2114134916",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Introduction The field of electroencephalogram (EEG)-based emotion identification has received significant attention and has been widely utilized in both human-computer interaction and therapeutic settings. The process of manually analyzing electroencephalogram signals is characterized by a significant investment of time and work. While machine learning methods have shown promising results in classifying emotions based on EEG data, the task of extracting distinct characteristics from these signals still poses a considerable difficulty. Methods In this study, we provide a unique deep learning model that incorporates an attention mechanism to effectively extract spatial and temporal information from emotion EEG recordings. The purpose of this model is to address the existing gap in the field. The implementation of emotion EEG classification involves the utilization of a global average pooling layer and a fully linked layer, which are employed to leverage the discernible characteristics. In order to assess the effectiveness of the suggested methodology, we initially gathered a dataset of EEG recordings related to music-induced emotions. Experiments Subsequently, we ran comparative tests between the state-of-the-art algorithms and the method given in this study, utilizing this proprietary dataset. Furthermore, a publicly accessible dataset was included in the subsequent comparative trials. Discussion The experimental findings provide evidence that the suggested methodology outperforms existing approaches in the categorization of emotion EEG signals, both in binary (positive and negative) and ternary (positive, negative, and neutral) scenarios.",
  "full_text": "TYPE Original Research\nPUBLISHED /zero.tnum/four.tnum April /two.tnum/zero.tnum/two.tnum/four.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpsyg./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/two.tnum/seven.tnum/five.tnum/one.tnum/four.tnum/two.tnum\nOPEN ACCESS\nEDITED BY\nFederica Marcolin,\nPolytechnic University of Turin, Italy\nREVIEWED BY\nHiroki Tanaka,\nNara Institute of Science and Technology\n(NAIST), Japan\nJiahui Pan,\nSouth China Normal University, China\n*CORRESPONDENCE\nYanan Zhou\nzhouyanan@bfsu.edu.cn\nHebin Cheng\nchenghebin@sdmu.edu.cn\n†These authors have contributed equally to\nthis work\nRECEIVED /zero.tnum/nine.tnum August /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /two.tnum/zero.tnum March /two.tnum/zero.tnum/two.tnum/four.tnum\nPUBLISHED /zero.tnum/four.tnum April /two.tnum/zero.tnum/two.tnum/four.tnum\nCITATION\nWang D, Lian J, Cheng H and Zhou Y (/two.tnum/zero.tnum/two.tnum/four.tnum)\nMusic-evoked emotions classiﬁcation using\nvision transformer in EEG signals.\nFront. Psychol./one.tnum/five.tnum:/one.tnum/two.tnum/seven.tnum/five.tnum/one.tnum/four.tnum/two.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpsyg./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/two.tnum/seven.tnum/five.tnum/one.tnum/four.tnum/two.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/four.tnum Wang, Lian, Cheng and Zhou. This is\nan open-access article distributed under the\nterms of the\nCreative Commons Attribution\nLicense (CC BY) . The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nMusic-evoked emotions\nclassiﬁcation using vision\ntransformer in EEG signals\nDong Wang/one.tnum,/two.tnum†, Jian Lian /two.tnum†, Hebin Cheng /two.tnum* and Yanan Zhou /three.tnum*\n/one.tnumSchool of Information Science and Electrical Engineering, Shandon g Jiaotong University, Jinan,\nChina, /two.tnumSchool of Intelligence Engineering, Shandong Management Universit y, Jinan, China, /three.tnumSchool\nof Arts, Beijing Foreign Studies University, Beijing, China\nIntroduction: The ﬁeld of electroencephalogram (EEG)-based emotion\nidentiﬁcation has received signiﬁcant attention and has been wide ly utilized\nin both human-computer interaction and therapeutic settings. The process\nof manually analyzing electroencephalogram signals is character ized by a\nsigniﬁcant investment of time and work. While machine learning me thods have\nshown promising results in classifying emotions based on EEG da ta, the task of\nextracting distinct characteristics from these signals still po ses a considerable\ndiﬃculty.\nMethods: In this study, we provide a unique deep learning model that\nincorporates an attention mechanism to eﬀectively extract spati al and temporal\ninformation from emotion EEG recordings. The purpose of this model is to\naddress the existing gap in the ﬁeld. The implementation of e motion EEG\nclassiﬁcation involves the utilization of a global average poo ling layer and a fully\nlinked layer, which are employed to leverage the discernible char acteristics. In\norder to assess the eﬀectiveness of the suggested methodolog y, we initially\ngathered a dataset of EEG recordings related to music-induced emotions.\nExperiments: Subsequently, we ran comparative tests between the state-\nof-the-art algorithms and the method given in this study, ut ilizing this\nproprietary dataset. Furthermore, a publicly accessible datas et was included in\nthe subsequent comparative trials.\nDiscussion: The experimental ﬁndings provide evidence that the suggeste d\nmethodology outperforms existing approaches in the categori zation of emotion\nEEG signals, both in binary (positive and negative) and ternar y (positive, negative,\nand neutral) scenarios.\nKEYWORDS\nmusic-evoked emotion, emotion classiﬁcation, electroence phalographic, deep\nlearning, transformer\n/one.tnum Introduction\nEmotion is intricately intertwined with all facets of the human experience and\naction. According to Jerritta et al. (2011), it has an impact on human attitudes and\nperceptions in both human-human contact and human-computer interaction. In the\nrealm of artistic expression, music holds a paramount position as a means to convey\nand articulate human emotions. Music has been widely recognized as a means of\nevoking distinct emotive states, leading to its characterization as the language of emotions\n(\nVuilleumier and Trost, 2015 ). In their investigations, Ekman (1999) and Gilda et al. (2017)\nintroduced six distinct and quantiﬁable emotional states, namely happiness, sadness,\nanger, fear, surprise, and disgust, as the basis for implementing emotion identiﬁcation.\nFrontiers in Psychology /zero.tnum/one.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpsyg./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/two.tnum/seven.tnum/five.tnum/one.tnum/four.tnum/two.tnum\nOver time, other emotional states have been included in this\ncollection, such as neutrality, arousal, and relaxation (\nBong et al.,\n2012; Selvaraj et al., 2013 ; Goshvarpour et al., 2017 ; Minhad\net al., 2017 ; Wei et al., 2018 ; Sheykhivand et al., 2020 ; Liu et al.,\n2022). In the context of machine learning, the establishment of\ndistinct states for emotions serves as a signiﬁcant framework\nfor eﬀectively addressing the challenge of emotion recognition.\nNumerous algorithms for music emotion identiﬁcation based on\nmachine learning have been proposed in the literature, with\napplications spanning composition and psychotherapy (\nEerola and\nVuoskoski, 2012; Cui et al., 2022 ).\nTypically, a conventional music emotion identiﬁcation system\nbased on machine learning encompasses the subsequent stages:\n• The collection of changes in emotions elicited by music is\nfacilitated via the utilization of physiological information\nobtained by specialized sensors.\n• The physiological samples that have been gathered are\nsubjected to a processing procedure in order to remove any\npotential artifacts.\n• The generation of representation pertaining to emotional\nstates is thereafter accomplished by extracting features from\nthe pre-processed data.\n• By utilizing a classiﬁer, it is possible to generate the\ncorresponding category of music emotion for a given sample.\nNumerous instruments utilized in the acquisition of\nphysiological signals have been employed for the purpose of\nemotion recognition. Various physiological signals have been\ninvestigated for the purpose of emotion recognition. These\ninclude, body movement (\nZhang et al., 2021 ), facial expression\n(Song, 2021 ), respiration ( Siddiqui et al., 2021 ), galvanic skin\nresponse ( Kipli et al., 2022 ), blood volume pulse ( Semerci et al.,\n2022), skin temperature ( Semerci et al., 2022 ), electromyography\n(Xu et al., 2023 ), photoplethysmographic ( Cosoli et al., 2021 ),\nelectrocardiogram ( Hasnul et al., 2021 ), and EEG ( Li et al., 2021 ).\nThe non-invasive nature, aﬀordability, and ability to capture data\nin real-time have contributed to the extensive utilization of EEG\nin the ﬁeld of emotion identiﬁcation (\nAlarcao and Fonseca, 2017 ),\nwith a particular emphasis on music emotion categorization ( Lin\net al., 2006 ).\nSeveral studies have introduced diﬀerent approaches for\nemotion categorization utilizing EEG in the context of machine\nlearning. For example, the study conducted by\nSammler et al.\n(2007) examined the impact of valence on human emotions by\nanalyzing EEG data and heart rate concurrently. The present\nstudy aimed to gather data on positive and negative emotions\nelicited by EEG signals during the auditory experience of consonant\nand discordant musical stimuli. Subsequently, the authors of the\nstudy (\nKoelstra et al., 2011 ) made available a publicly accessible\ndataset. The study conducted by Balasubramanian et al. (2018)\nAbbreviations: EEG, Electroencephalographic; fMRI, functional m agnetic\nresonance imaging; LSTM, long short term; CNN, convolutional neural\nnetwork; ECG, electrocardiogram; EOG, electro-oculugram; GAP, globa l\naverage pooling; FC, fully connected; GPU, graphical processing uni t; TP, true\npositive; FN, false negative; TN, true negative; FP, false positiv e.\nexamined the emotional reaction to various types of music using\nEEG data. The experimental ﬁndings have indicated that there is\nan increase in theta band activity in the frontal midline region\nwhen individuals are exposed to their preferred music. Conversely,\nthe beta band would have an increase in activity when exposed to\nmusic that is perceived as undesirable. In their study,\nOzel et al.\n(2019) introduced a methodology for emotion identiﬁcation that\ninvolves the analysis of temporal-spectral EEG signals. Hou and\nChen (2019) derived a set of 27-dimensional EEG characteristics to\nrepresent music-induced emotions, including calmness, pleasure,\nsadness, and rage. Recently,\nQiu et al. (2022) proposed an integrated\nframework of multi-modal EEG and functional near infrared\nspectroscopy to explore the inﬂuence of music on brain activity.\nIn addition, the utilization of deep learning-based architectures\nin music emotion categorization has been widely adopted due to\nthe shown eﬀectiveness of deep learning in diﬀerent domains such\nas machine vision and natural language processing. In their study,\nHan et al. (2022) conducted a comprehensive review of the existing\nliterature pertaining to the assessment metrics, algorithms, datasets,\nand extracted features utilized in the analysis of EEG signals\nin the context of music emotion detection. In their publication,\nNag et al. (2022) introduced the JUMusEmoDB dataset. The\nmusic emotion categorization challenge was addressed by the\nauthors through the utilization of Convolutional Neural Network\n(CNN) based models, namely resnet50, mobilenet, squeezenet, and\ntheir own suggested ODE-Net.\nEskine (2022) conducted a study\nexamining the impact of music listening on creative cognition, a\nphenomenon that has been empirically demonstrated to enhance\ncreative cognitive processes. The experimental ﬁndings provided\nevidence that cognitive function exhibited an increase inside the\ndefault mode. This was supported by the observed augmentation of\nspectral frequency power in the beta range throughout the entire\nbrain, as well as in the theta range within the parietal region,\nand in the gamma range across the entire brain. In their study,\nDaly (2023) investigated the integration of functional magnetic\nresonance imaging (fMRI) and EEG techniques to develop an\nacoustic decoder for the purpose of classifying music emotions.\nThe study employed an EEG-fMRI combined paradigm to capture\nneural responses during music listening among individuals. In\nthis study, a deep learning model known as the long short-term\nmemory (LSTM) was utilized to extract neural information from\nEEG signals during music listening. The objective was to rebuild\nthe matching music clips based on this extracted information.\nBoth machine learning and deep learning techniques have\ndemonstrated promising results in the categorization of music-\nevoked emotions. Nevertheless, there are a number of constraints\nassociated with these approaches that must be addressed prior\nto their practical implementation in contexts such as medical\ndiagnosis, namely in the realm of emotion identiﬁcation. One\naspect to consider is that the eﬃcacy of machine learning\ntechniques is heavily dependent on the selection of appropriate\nfeatures. The task at hand continues to provide an unsolved\nproblem as the extraction and selection of these characteristics from\nEEG data must be done in a manual manner. In addition, it should\nbe noted that manually-designed features possess subjectivity and\nsusceptibility to errors, perhaps rendering them unsuitable for the\nspeciﬁc requirements of music emotion identiﬁcation. In contrast,\ndeep learning models like as CNNs have the ability to automatically\nFrontiers in Psychology /zero.tnum/two.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpsyg./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/two.tnum/seven.tnum/five.tnum/one.tnum/four.tnum/two.tnum\nFIGURE /one.tnum\nThe data collecting process for classifying music-evoked emotions using an EEG equipment based on the /one.tnum/zero.tnum–/two.tnum/zero.tnum system (Homan et al., /one.tnum/nine.tnum/eight.tnum/seven.tnum).\nextract internal representations from EEG inputs. Nevertheless, it\nis expected that the features derived from CNN models prioritize\nthe consideration of the overall connection between distant EEG\nsignals. This is due to the fact that CNN utilizes a local receptive\nﬁeld approach in the process of extracting features.\nThe present work introduces a transformer architecture\nfor music-evoked emotion categorization, using a self-attention\nmechanism. This model incorporates the self-attention mechanism\nand positional embedding to describe the sequence of channels in\nEEG data, drawing inspiration from the vision transformer’s work\n(\nDosovitskiy et al., 2020 ). The suggested transformer model has the\nability to extract both spatial representations, which correspond\nto self-attention modules, and temporal representations, which\ncorrespond to positional embedding. These representations are\nderived from multi-channel EEG data acquired from subjects who\nwere listening to music. Furthermore, the transformer model that\nhas been introduced has the capability to extract the relationships\nthat exist among EEG signals across extended distances. In\norder to assess the eﬃcacy of the suggested methodology, the\nexperiments were conducted using both a publicly accessible\ndataset (\nKoelstra et al., 2011 ) and a privately held dataset.\nFurthermore, comparative tests were conducted to evaluate the\nperformance of the proposed model in comparison to state-of-the-\nart algorithms. The experimental ﬁndings provide evidence that the\nsuggested methodology exhibits superior performance compared\nto existing binary and ternary music emotion categorization\nalgorithms. The suggested model has a positive conclusion,\nindicating its potential value as a tool for classifying music-evoked\nemotions.\nThe main contributions of this work can be summarized as\nfollows:\n• This is an early application of the spatial-temporal transformer\ninto the classiﬁcation of music-evoked emotions.\n• A novel dataset of music-evoked EEG signals was established.\n• The proposed approach considers both the spatial connections\namong a set of EEG channels and the temporal sequence of each\nindividual EEG signal.\nFrontiers in Psychology /zero.tnum/three.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpsyg./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/two.tnum/seven.tnum/five.tnum/one.tnum/four.tnum/two.tnum\nTABLE /one.tnumThe descriptions of the music excerpts used in this study.\nID Type Title Singer Durating (mm:ss)\n1 Positive Honey Xinling Wang 03:33\n2 Negative Advanced animals Wei Dou 04:38\n3 Neutral Reiki meditation Reiki 06:03\n4 Positive Wu Ha Weibo Pan 03:46\n5 Negative In case SHIN 04:24\n6 Neutral Calm dreams Sleep Tech 04:27\n7 Positive In Spring Feng Wang 05:10\n8 Negative Cloudy day Wenwei Mo 04:02\n9 Neutral Let the sun shine Milk & Sugar 07:02\n10 Positive As broad as the sea and sky Beyond 03:59\n11 Negative Negative Black sun empire 05:44\n12 Neutral Illusionary daytime Shirﬁne 04:10\n13 Positive Invisible wings Shaohan Zhang 03:44\n14 Negative Unfortunately, its not you Jingru Liang 04:45\n15 Neutral Song from a secret garden Secret garden 03:33\n• The performance of our approach surpassed the state-of-the-art\ndeep learning algorithms on both public and private datasets.\nThe subsequent sections of this article are structured as\nfollows: The methodology Section 2 contains information on the\nacquisition of EEG signals during music listening as well as the\ndetails of the presented deep learning model. Section 3 presents a\ndetailed account of the experimental procedures conducted in this\ninvestigation, as well as a comprehensive comparison between the\nexisting state-of-the-art methods and the technique proposed in the\ncurrent study. This research concluded at Section 4.\n/two.tnum Methodology\nThis section provides a comprehensive overview of the\ndata gathering process employed in the present investigation.\nFurthermore, the subsequent sections of the article will present a\ncomprehensive analysis of the suggested transformer model.\n/two.tnum./one.tnum Dataset and pre-processing\nThe initial step in this study included the creation of a private\ndataset using multi-channel EEG, which involved the collection\nof three distinct music-evoked emotions: positive, negative, and\nneutral. The complete workﬂow is depicted in\nFigure 1.\nDuring the course of data gathering, a total of 48 individuals\nwere registered, including 24 females and 24 men. The age range of\nthe participants was between 18 and 25 years, with an average age of\n20.6. All individuals involved in the study were enrolled as students\nat the same institution’s campus. Furthermore, it should be noted\nthat the individuals exhibit robust physical and mental well-being.\nDuring the course of the project, the research team received advice\nand supervision from two psychology specialists, one female and\none male, who possessed signiﬁcant expertise in the ﬁeld.\nTo ensure the consistency of the data gathering process, the\nfollowing challenges were proactively addressed. Additionally, all\nparticipants were provided with instructions to thoroughly review\nthe handbook and become acquainted with the workﬂow of\nEEG signal collecting. It should be noted that the manual has\nidentiﬁed and emphasized the entries that are prone to errors,\nwith the intention of facilitating the reader’s attention toward\nthe vital operations. Subsequently, the participants were requested\nto complete a questionnaire pertaining to their personal details.\nSubsequently, the participants were provided with instructions\nand guidance from the specialists in order to properly don the\nEEG electrode caps. Subsequently, the specialists would assess\nthe adequacy of the EEG electrodes’ contact and ensure that\nno detachment has occurred. Furthermore, the participants were\ninstructed by the experts to initiate the signal gathering procedure\nby hitting the designated buttons. In addition, the EEG collection\ndevice utilized in the study was the Biosemi ActiveTwo system. The\nsystem employs the international 10–20 system, consisting of 32\nchannels, notably Fp1, AF3, F3, F7, FC5, FC1, C3, T7, CP5, CP1,\nP3, P7, PO3, O1, Oz, Pz, Fp2, AF4, Fz, F4, F8, FC6, FC, Cz, C4, T8,\nCp6, Cp2, P4, P8, PO4, and O2. Additionally, the sampling rate is\nset at 512Hz.\nDuring the process of data collection, each participant was\nprovided with instructions to listen to a total of 15 music\nclips. These clips were categorized into three distinct emotional\ncategories, namely positive, negative, and neutral, with each\ncategory consisting of ﬁve clips. To note that the categories of\nthese clips were determined by three psychological experts using\na majority voting mechanism. The speciﬁcs about the music may\nbe found in\nTable 1. The initial duration of the music clips varies\namong them. Nevertheless, the participant received a standardized\n1-min audio clip for each piece of music. Each participant\nFrontiers in Psychology /zero.tnum/four.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpsyg./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/two.tnum/seven.tnum/five.tnum/one.tnum/four.tnum/two.tnum\nFIGURE /two.tnum\nThe architectural design of the proposed transformer model. The\nsymbol L is used to describe the number of encoder blocks in the\nproposed model. The proposed model has several variants, with L(.)\nbeing either /one.tnum/two.tnum, /one.tnum/eight.tnum, or /two.tnum/four.tnum. The abbreviation GAP is used to denote\nglobal average pooling, while FC represents fully-connected.\nwas instructed to listen to the music clips in a randomized\nsequence.\nThe subsequent section presents a comprehensive\noverview of the data collecting procedure involved\nin capturing EEG signals related to music-induced\nemotions.\n(1) The participants were provided with instructions to achieve\na state of calmness, following which the experts started the\nmarking process to denote the commencement of each EEG\nrecording. The duration of this process is expected to be 5\nseconds.\n(2) In each 75-second interval, the participants would undergo a\n15-second pause to transition between music clips, followed\nby a 60-second period of actively listening to the music clip.\nSimultaneously, the experts would provide guidance to the\nparticipants on how to minimize superﬂuous bodily motions.\n(3) Following the auditory experience, the individuals were\ndirected by the experimental personnel to assign a value to\nthe musical composition, with positive being denoted as +1,\nnegative as –1, and neutral as 0. The duration of this procedure\nshould not exceed 15 seconds, during which it is utilized for the\npurpose of transitioning the music.\nFIGURE /three.tnum\nThe encoder block utilized in the transformer model under\nconsideration. The acronym MSA refers to multi-head self atten tion.\n(4) The participants proceeded with the auditory experience by\nsequentially engaging with the subsequent musical excerpt\nuntil the entirety of the 12 excerpts had been presented.\nSo as to guarantee the optimal state of the participants, the\ncollection of music-evoked emotion EEG samples was limited to\nthe time periods of 9 a.m. to 11 a.m. and 3 p.m. to 5 p.m. In\norder to mitigate interference from many sources such as heart\nrate, breathing, electrocardiogram (ECG), and electro-scalogram\n(EOG), the participants were given instructions to cover their eyes\nwhile the recording procedures were being conducted.\nThe dataset contains a total of 43,200 (48 × 15 × 60 = 43, 200)\nseconds of EEG signals, with each second including 32 channels.\nFurthermore, the initial samples were partitioned into the epochs\nof 1 second duration, each consisting of 60, 000 data points. To\nnote that there were still overlapping epochs in the samples since\nthe trivial errors are diﬃcult to avoid due to the human reaction\ntimes. Given the absence of any imbalance issue within the dataset,\nit can be observed that each category of music emotion EEG\nsignals is comprised of an equal number of samples, speciﬁcally\n20,000 epochs. Hence, in the context of binary classiﬁcation, namely\ndistinguishing between positive and negative classes, the proposed\nmodel was trained using a dataset including 40,000 epochs as input\nsamples. In contrast, in the context of the ternary classiﬁcation job,\nthe entirety of the 60,000 epochs were utilized as the input. It should\nFrontiers in Psychology /zero.tnum/five.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpsyg./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/two.tnum/seven.tnum/five.tnum/one.tnum/four.tnum/two.tnum\nFIGURE /four.tnum\nThe MLP block used in the proposed transformer model. GELU\ndenotes the activation function (\nLee, /two.tnum/zero.tnum/two.tnum/three.tnum).\nbe noted that the presence of overlapping epochs has the potential\nto somewhat mitigate over-ﬁtting.\nIn the pre-processing phase, the acquired EEG signals were\nsubjected to a Notch ﬁlter (\nSerra et al., 2017 ) in order to\nremove the 50 Hz components originating from the power supply.\nSubsequently, a ﬁrst-order low-pass ﬁlter with a frequency range of\n0.5 to 45 Hz was utilized. Subsequently, the electroencephalography\n(EEG) data underwent a normalization process resulting in a range\nof values between 0 and 1.\n/two.tnum./two.tnum The proposed transformer architecture\nThe transformer model presented in\nFigure 2 draws inspiration\nfrom the architecture of the vision transformer ( Dosovitskiy et al.,\n2020). The suggested transformer model comprises three main\ncomponents: (1) a linear embedding layer, (2) an encoder block,\nand (3) a multiple-layer perception (MLP) block. Initially, the linear\nembedding unit was utilized to turn a sequence of EEG data into\na ﬁxed-length input for the suggested transformer model. The\nﬂattened embedding includes the class token of the music emotion\nfor each series of EEG data. In addition, the linear embedding is\nconstructed by including the positional embedding, which encodes\nthe sequential order of an individual EEG signal inside a sequence\nof EEG signals. It should be noted that every input sequence of\nEEG data pertains to the identical category of emotion elicited by\nmusic. Furthermore, the pivotal self-attention module (\nFan et al.,\n2021; Liu et al., 2021 ; Wang et al., 2021 ), which aims to reveal the\nTABLE /two.tnumThe proposed transformer model exhibits binary and ternary\nclassiﬁcation outcomes (average values and standard deviatio ns).\nNumber of\nclasses\nAccuracy\n(%)\nSensitivity\n(%)\nSpeciﬁcity\n(%)\nBinary 96.85 (1.73) 95.17 (1.68) 95.69 (2.01)\nTernary 95.74 (2.32) 94.32 (1.97) 95.25 (1.69)\nconnections among distant EEG data, is located within the encoder\nblock. In order to create a cohesive encoder module, it is necessary\nfor the encoder block to be iteratively repeated. In addition to the\nself-attention layer included in each encoder block, there are many\nadditional sorts of layers, namely layer normalization, dropout,\nand MLP block. The generation of representations for music\nemotion EEG signals may be achieved by the utilization of stacked\ntransformer encoder blocks. Ultimately, the use of the MLP block\nwas implemented to get the classiﬁcation result by integrating a\nglobal average pooling (GAP) layer and a fully connected (FC) layer,\ncommonly referred to as a linear layer. The transformer model\nunder consideration has the potential to signiﬁcantly expand the\nscope of receptive ﬁelds in comparison to designs based on CNNs.\nAdditionally, the recovered representation from the multi-channel\nEEG data encompasses both local information pertaining to a series\nof signals and the global association between signals that are far\napart.\nIn the proposed transformer model, the input sequences consist\nof individual EEG signals, each spanning a duration of 1 second and\nincluding 30 channels. Subsequently, the EEG signal sequence was\nﬂattened and transformed into a vector. In addition, it should be\nnoted that the encoder block is iterated a varying number of times\n(12, 18, or 24) across diﬀerent versions of the proposed transformer\nmodel. Furthermore, the structural composition of this encoder\nblock is illustrated in\nFigure 3.\nAs seen in Figure 3, the encoder block has many components,\nnamely layer normalization, MSA, dropout, and MLP block. The\nstudy did not include a comprehensive examination of the MSA\nunit due to its extensive coverage in existing studies (\nVaswani\net al., 2017 ; Dosovitskiy et al., 2020 ). The unit consisting of H\nheads was employed to assess the similarity between a query\nand its associated keys based on the assigned weight for each\nvalue (\nVaswani et al., 2017 ). Furthermore, the utilization of the\nLayer normalizing module is employed to calculate the mean and\nvariance required for normalizing from the entirety of the inputs to\nthe neurons within a layer throughout a singular training instance\n(\nBa et al., 2016 ). In this study, the dropout layer ( Choe and\nShim, 2019 ) is utilized as a regularization technique to mitigate\nthe risk of overﬁtting. The architecture of the MLP block is seen\nin\nFigure 4.\nThe proposed technique allows for the formulation of the\nprocess of music emotion categorization in Equation 1–4:\nz0 = [xclass; x1\npE; x2\npE; ...; xN\np ] + Eposition, (1)\nwhere the variable z0 represents the output of the linear embedding\nlayer. In this context, N = 30 represents the number of channels\nFrontiers in Psychology /zero.tnum/six.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpsyg./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/two.tnum/seven.tnum/five.tnum/one.tnum/four.tnum/two.tnum\nused as input. The variables xclass and Eposition refer to the class token\nand positional embedding, respectively.\nz\n′\nl = MSA(LN(zl−1)) + zl−1, (2)\nzl = MLP(LN(z\n′\nl )) + z\n′\nl , (3)\ny = LN(z0\nL), (4)\nwhere the layer normalization unit is denoted\nas LN(.), where zl represents the output of\nlayer l, and y represents the output classiﬁcation\noutcome.\n/three.tnum Experimental results\n/three.tnum./one.tnum Implementation details\nThe transformer model described in this study was constructed\nusing the PyTorch framework ( Paszke et al., 2019 ). The\ncomputational resources employed for the implementation were\nfour NVidia RTX 3080 Graphical Processing Units (GPUs) with a\ntotal of 64 GB RAM. The best parameters of the proposed network\nwere discovered using a trial and error technique. The learning rate\nis conﬁgured to be 0.004, accompanied by a weight decay of 0.05.\nSubsequently, a 10-fold cross-validation procedure was employed\nto assess the resilience of the suggested methodology. Initially, the\ninput EEG data were partitioned into ten equitably sized groups.\nDuring each iteration, one out of the 10 groups was designated as\nthe testing set, while the remaining nine groups were utilized as the\nFIGURE /five.tnum\nThe suggested model’s inaccuracy in (Top) binary classiﬁcation and (Bottom) ternary classiﬁcation.\nFrontiers in Psychology /zero.tnum/seven.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpsyg./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/two.tnum/seven.tnum/five.tnum/one.tnum/four.tnum/two.tnum\nTABLE /three.tnumBinary classiﬁcation comparison between the state-of-the-ar ts\nand ours.\nMethod Accuracy\n(%)\nSensitivity\n(%)\nSpeciﬁcity\n(%)\nU-Net\n(\nRonneberger et al.,\n2015)\n88.56 88.71 89.05\nMask R-CNN ( He\net al., 2017 )\n87.43 86.39 86.56\nExtremeNet (Zhou\net al., 2019 )\n89.49 89.87 88.51\nTensorMask (Chen\net al., 2019 )\n90.56 90.18 91.27\n4D-CRNN (Shen\net al., 2020 )\n92.57 92.32 93.08\nFBCCNN ( Pan and\nZheng, 2021)\n92.53 91.68 91.24\nMTCNN (Rudakov,\n2021)\n93.02 93.55 94.17\nSSGMC (Kan et al.,\n2022)\n94.82 94.18 94.23\nMViT (Fan et al.,\n2021)\n90.42 91.39 90.72\nPVT (Wang et al.,\n2021)\n92.27 91.15 92.01\nPiT (Heo et al.,\n2021)\n93.53 92.85 93.78\nSwin Transformer\n(\nLiu et al., 2021 )\n95.32 94.64 94.37\nGPViT (Yang et al.,\n2022)\n96.38 94.88 95.27\nThe proposed\napproach\n96.85 95.17 95.69\ntraining set. Hence, the mean result of 10 iterations was utilized as\nthe ultimate output.\nFurthermore, the assessment measures utilized in the\nexperiments involved sensitivity, speciﬁcity, and accuracy. The\nmathematical formulation of these metrics is elucidated in in\nEquations 5–7.\nSensitivity = TP\nTP + FN , (5)\nSpeciﬁcity = TN\nTN + FP , (6)\nAccuracy = TP + TN\nTP + FN + TN + FP , (7)\nwhere TP , FN, TN, and FP represent the terms true positive, false\nnegative, true negative, and false positive, respectively.\n/three.tnum./two.tnum Outcome of the proposed approach\nTable 2 presents a summary of the average values and standard\ndeviations (SD) obtained from the proposed method in the\nTABLE /four.tnumTernary classiﬁcation comparison between the state-of-the-a rts\nand ours.\nMethod Accuracy\n(%)\nSensitivity\n(%)\nSpeciﬁcity\n(%)\nU-Net\n(\nRonneberger et al.,\n2015)\n85.52 83.86 84.20\nMask R-CNN ( He\net al., 2017 )\n85.24 84.21 85.41\nExtremeNet (Zhou\net al., 2019 )\n86.28 83.17 84.53\nTensorMask (Chen\net al., 2019 )\n88.32 86.51 87.02\n4D-CRNN (Shen\net al., 2020 )\n91.57 92.24 91.89\nFBCCNN ( Pan and\nZheng, 2021)\n91.27 91.38 92.24\nMTCNN (Rudakov,\n2021)\n92.21 92.19 93.43\nSSGMC (Kan et al.,\n2022)\n92.18 91.57 94.28\nMViT (Fan et al.,\n2021)\n92.15 91.93 92.78\nPVT (Wang et al.,\n2021)\n91.23 90.46 91.37\nPiT (Heo et al.,\n2021)\n92.43 92.14 91.62\nSwin transformer\n(\nLiu et al., 2021 )\n92.57 91.38 93.27\nGPViT (Yang et al.,\n2022)\n93.14 92.25 93.18\nThe proposed\napproach\n95.74 94.32 95.25\nbinary classiﬁcation task, speciﬁcally in terms of average accuracy,\nsensitivity, and speciﬁcity. The average accuracy was found to\nbe 96.85%, while the sensitivity and speciﬁcity were measured\nat 95.17% and 95.69% respectively. Furthermore, in the ternary\ncategorization, the outcome rates were recorded as 95.74%, 94.32%,\nand 95.25%.\nFurthermore, the loss curves of the suggested methodology\nthroughout both the training and validation procedures were\nillustrated in\nFigure 5 It should be noted that the results presented\nin Figure 5 only include the initial 100 iterations of both the training\nand validation processes.\n/three.tnum./three.tnum Comparison experiments between the\nstate-of-the-arts and the proposed\napproach\nTo assess the eﬃcacy of our suggested technique for music-\nevoked emotion categorization, we conducted comparative tests\nbetween our work and the state-of-the-art algorithms.\nTables 2–4\npresent a comparative analysis of the current state-of-the-art\ndeep learning models and our proposed approach. The proposed\nFrontiers in Psychology /zero.tnum/eight.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpsyg./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/two.tnum/seven.tnum/five.tnum/one.tnum/four.tnum/two.tnum\nTABLE /five.tnumComparison between the state-of-the-arts and ours on DEAP\ndataset (Koelstra et al., /two.tnum/zero.tnum/one.tnum/one.tnum).\nMethod Detail Accuracy\nValence Arousal\n3DCNN (Shawky\net al., 2018 )\nCNN 88.52 89.36\nCNN-LSTM (Yang\net al., 2018 )\nLSTM 92.43 89.51\nSAE-LSTM (Xing\net al., 2019 )\nLSTM 86.32 81.27\nMulti-column CNN\n(\nYang et al., 2019 )\nCNN 93.81 94.15\n4D-CRNN (Shen\net al., 2020 )\nCRNN 95.34 93.62\nFGCCNN (Pan and\nZheng, 2021)\nCNN 91.72 90.28\nMTCNN (Rudakov,\n2021)\nCNN 95.34 95.49\nGANSER (Zhang\net al., 2022 )\nGAN 94.18 93.58\nSSGMC (Kan et al.,\n2022)\nContrastive\nlearning\n96.12 94.62\nThe proposed\napproach\nTransformer 97.41 97.02\nmethodology demonstrated superior performance compared to the\ncurrent leading method. To note that we did not take the traditional\nmachine learning models (\nQiu et al., 2022 ) into the comparison since\nthey usually relied on manually-designed features. The comparison\nexperiments included the following models: U-Net ( Ronneberger\net al., 2015 ), Mask R-CNN ( He et al., 2017 ), ExtremeNet ( Zhou\net al., 2019 ), TensorMask ( Chen et al., 2019 ), 4D-CRNN ( Shen\net al., 2020 ), FBCCNN ( Pan and Zheng, 2021 ), MTCNN ( Rudakov,\n2021), SSGMC ( Kan et al., 2022 ) for the CNN-based models, and\nMViT (Fan et al., 2021 ), PVT ( Wang et al., 2021 ), PiT ( Heo et al.,\n2021), Swin Transformer ( Liu et al., 2021 ), and GPViT ( Yang et al.,\n2022) for the transformer-based models.\nIn order to conduct a comprehensive evaluation of the\nproposed approach, we proceeded to assess its performance\nalongside several state-of-the-art algorithms (\nShawky et al., 2018 ;\nYang et al., 2018 ; Xing et al., 2019 ; Yang et al., 2019 ; Shen et al.,\n2020; Pan and Zheng, 2021 ; Rudakov, 2021; Kan et al., 2022 ; Zhang\net al., 2022 ) using the publicly accessible DEAP dataset ( Koelstra\net al., 2011 ). The results of this evaluation are presented in Table 5.\n/four.tnum Discussion\nBased on the empirical ﬁndings, it can be concluded that\nthis approach exhibits greater eﬃcacy compared to the existing\nstate-of-the-art algorithms. It is worth mentioning that the\ncomparative trials encompassed both CNN-based and transformer-\nbased models. In contrast to CNN-based models, the suggested\nmodel has the capability to extract global connections between\nlong-range multi-channels in EEG data, in addition to the local\nTABLE /six.tnumThe impact of H and L on the performance of the proposed\nmodel in binary classiﬁcation.\nModel Number of\nheads (H)\nNumber of\nlayers (L)\nAccuracy\n(%)\nM_4_4 4 4 90.08\nM_4_8 4 8 90.37\nM_8_4 8 4 91.15\nM_8_8 8 8 91.63\nM_8_12 8 12 93.35\nM_12_12 12 12 93.21\nM_16_12 16 12 94.16\nM_8_18 8 18 94.58\nM_12_18 12 18 95.39\nM_16_18 16 18 95.65\nM_8_24 8 24 96.28\nM_12_24 12 24 96.12\nM_16_24 16 24 96.53\nThe bold value represents the best performance of accuracy with 16 heads and 24 layers.\ninformation already present in the EEG signals. In contrast to\ntransformer-based models (\nHe et al., 2017 ; Chen et al., 2019 ; Zhou\net al., 2019 ; Wu et al., 2020 ; Fan et al., 2021 ; Heo et al., 2021 ;\nWang et al., 2021 ), the proposed approach has been speciﬁcally\noptimized to accommodate the unique characteristics of multi-\nchannel EEG signals. For instance, the linear embedding layer of\nthe proposed approach has been tailored to eﬀectively align with the\nstructural properties of multi-channel EEG signals. Furthermore,\nthe outcomes shown in the ablation research also exhibited the\neﬃcacy of self-attention modules and encoder blocks.\n/four.tnum./one.tnum Ablation study\nAs demonstrated in\nTable 6, the optimal conﬁguration of the\nprimary hyper-parameters was determined through comparison\nexperiments. These experiments involved testing diﬀerent\ncombinations of the number of heads ( H) in the MSA module and\nthe number of transformer encoder layers ( L) on a dataset that was\nmanually collected and constituted 50% of the total dataset. The\ntrials solely included binary music emotion categorization in order\nto streamline the ablation study procedure.\nTherefore, the suggested model exhibits an ideal conﬁguration\nwhile utilizing 16 heads ( H = 16) and 24 layers ( L = 24).\n/four.tnum./two.tnum Limitations and future research\nIn addition, this study possesses certain limitations in addition\nto its contributions. The tests solely focused on the binary and\nternary classiﬁcation problems. In order to enhance the evaluation\nof the proposed approach, it is recommended to integrate the\ncategorization of other types of emotions and employ a multi-\nlabel classiﬁcation methodology. Meanwhile, this study adopted an\nFrontiers in Psychology /zero.tnum/nine.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpsyg./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/two.tnum/seven.tnum/five.tnum/one.tnum/four.tnum/two.tnum\noﬄine learning strategy since the vision transformer-based models\nsuﬀering from high resource occupancy. In addition, this study did\nnot take cross-subject emotion recognition (\nHe et al., 2021 ; Pan\net al., 2023 ) into consideration, which may aﬀect the applicability\nand universality of this study.\nIn subsequent investigations, further electroencephalography\n(EEG) data pertaining to the elicitation of emotions through music\nwill be gathered. Furthermore, the suggested methodology holds\npotential for the identiﬁcation of emotions across a wide range of\napplications.\n/five.tnum Conclusion\nThe present work introduces a transformer model as a\nmeans of classifying music-evoked emotions. The model under\nconsideration consists of three distinct phases, namely linear\nembedding, transformer encoder, and MLP layer. The purpose\nof the ﬁrst phase is to generate ﬂattened input features for the\nproposed model. These features are aimed to extract both local\nand global correlations between the multi-channel EEG data.\nAdditionally, the MLP blocks aim to enhance the classiﬁcation\noutcome. This study presents an initial implementation of a\nvision transformer-based model for the purpose of music emotion\nidentiﬁcation.\nData availability statement\nThe raw data supporting the conclusions of this article will be\nmade available by the authors, without undue reservation.\nEthics statement\nThe studies involving humans were approved by the Shandong\nManagement University’s Human Research Ethics Committee.\nThe studies were conducted in accordance with the local\nlegislation and institutional requirements. The participants\nprovided their written informed consent to participate in\nthis study.\nAuthor contributions\nDW: Writing – review & editing, Formal analysis,\nValidation. JL: Writing – original draft, Supervision, Project\nadministration, Methodology, Funding acquisition, Formal\nanalysis, Data curation, Conceptualization. HC: Writing –\noriginal draft, Validation, Investigation, Formal analysis, Data\ncuration. YZ: Writing – original draft, Validation, Investigation,\nData curation.\nFunding\nThe author(s) declare that ﬁnancial support was received\nfor the research, authorship, and/or publication of this article.\nThe present study received support from the Natural Science\nFoundation of Shandong Province (Grant No. ZR2020MF133),\nthe Key Laboratory of Public Safety Management Technology\nof Scientiﬁc Research and Innovation Platform in Shandong\nUniversities during the 13th Five-Year Plan Period, the\nCollaborative Innovation Center of “Internet plus intelligent\nmanufacturing” of Shandong Universities, and the Intelligent\nManufacturing and Data Application Engineering Laboratory of\nShandong Province.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAlarcao, S. M., and Fonseca, M. J. (2017). Emotions recogniti on using EEG signals:\na survey. IEEE Trans. Aﬀect. Comput . 10, 374–393. doi: 10.1109/TAFFC.2017.2714671\nBa, J., Kiros, J. R., and Hinton, G. E. (2016). Layer normalizat ion. ArXiv,\nabs/1607.06450.\nBalasubramanian, G., Kanagasabai, A., Mohan, J., and Seshadr i, N. G. (2018). Music\ninduced emotion using wavelet packet decomposition—an EEG stu dy. Biomed. Sign.\nProc. Control 42, 115–128. doi: 10.1016/j.bspc.2018.01.015\nBong, S. Z., Murugappan, M., and Yaacob, S. B. (2012). “Analysis\nof electrocardiogram (ECG) signals for human emotional stress\nclassiﬁcation, ” in IEEE International Conference on Robotics and Automation.\ndoi: 10.1007/978-3-642-35197-6_22\nChen, X., Girshick, R., He, K., and Dollár, P. (2019). “Tensorma sk: a foundation for\ndense object segmentation, ” inProceedings of the IEEE/CVF international conference on\ncomputer vision, 2061–2069. doi: 10.1109/ICCV.2019.00215\nChoe, J., and Shim, H. (2019). “Attention-based dropout layer for weakly supervised\nobject localization, ” in 2019 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2214–2223. doi: 10.1109/CVPR.2019.00232\nCosoli, G., Poli, A., Scalise, L., and Spinsante, S. (2021). “Hear t rate variability\nanalysis with wearable devices: inﬂuence of artifact correcti on method on\nclassiﬁcation accuracy for emotion recognition, ” in 2021 IEEE International\nInstrumentation and Measurement Technology Conference (I2MTC) , 1–6.\ndoi: 10.1109/I2MTC50364.2021.9459828\nCui, X., Wu, Y., Wu, J., You, Z., Xiahou, J., and Ouyang, M. (20 22). A review: Music-\nemotion recognition and analysis based on EEG signals. Front. Neuroinf . 16:997282.\ndoi: 10.3389/fninf.2022.997282\nDaly, I. (2023). Neural decoding of music from the EEG. Sci. Rep . 13:624.\ndoi: 10.1038/s41598-022-27361-x\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D. , Zhai, X., Unterthiner,\nT., et al. (2020). An image is worth 16x16 words: transformers for image recognition at\nscale. arXiv preprint arXiv:2010.11929 .\nEerola, T., and Vuoskoski, J. K. (2012). A review of music and e motion\nstudies: Approaches, emotion models, and stimuli. Music Percept . 30, 307–340.\ndoi: 10.1525/mp.2012.30.3.307Frontiers in Psychology /one.tnum/zero.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpsyg./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/two.tnum/seven.tnum/five.tnum/one.tnum/four.tnum/two.tnum\nEkman, P. (1999). Basic Emotions, chapter 3 . London: John Wiley Sons, Ltd. 45–60.\ndoi: 10.1002/0470013494.ch3\nEskine, K. (2022). Evaluating the three-network theory of cr eativity:\nEﬀects of music listening on resting state EEG. Psychol. Music 51, 730–749.\ndoi: 10.1177/03057356221116141\nFan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., et a l. (2021). “Multiscale\nvision transformers, ” in Proceedings of the IEEE/CVF International Conference on\nComputer Vision, 6824–6835. doi: 10.1109/ICCV48922.2021.00675\nGilda, S., Zafar, H., Soni, C., and Waghurdekar, K. (2017). “Sm art music\nplayer integrating facial emotion recognition and music mood recommendation, ” in\n2017 International Conference on Wireless Communications, Signal Pr ocessing and\nNetworking (WiSPNET), 154–158. doi: 10.1109/WiSPNET.2017.8299738\nGoshvarpour, A., Abbasi, A., and Goshvarpour, A. (2017). An accu rate emotion\nrecognition system using ecg and gsr signals and matching purs uit method. Biomed.\nJ. 40, 355–368. doi: 10.1016/j.bj.2017.11.001\nHan, D., Kong, Y., Han, J., and Wang, G. (2022). A survey of musi c emotion\nrecognition. Front. Comput. Sci . 16:166335. doi: 10.1007/s11704-021-0569-4\nHasnul, M. A., Aziz, N. A. A., Alelyani, S., Mohana, M., and Aziz, A. A.\n(2021). Electrocardiogram-based emotion recognition syst ems and their applications\nin healthcare–a review. Sensors 21:5015. doi: 10.3390/s21155015\nHe, K., Gkioxari, G., Dollár, P., and Girshick, R. (2017). “Mask r- cnn, ” in\nProceedings of the IEEE International Conference on Computer Vision , 2961–2969.\ndoi: 10.1109/ICCV.2017.322\nHe, Z., Zhong, Y., and Pan, J. (2021). An adversarial discrim inative temporal\nconvolutional network for EEG-based cross-domain emotion re cognition. Comput.\nBiol. Med. 141:105048. doi: 10.1016/j.compbiomed.2021.105048\nHeo, B., Yun, S., Han, D., Chun, S., Choe, J., and Oh, S. J. (202 1). “Rethinking spatial\ndimensions of vision transformers, ” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 11936–11945. doi: 10.1109/ICCV48922.2021.01172\nHoman, R., Herman, J. H., and Purdy, P. (1987). Cerebral locat ion of international\n10–20 system electrode placement. Electroencephalogr Clin. Neurophysiol . 66, 376–382.\ndoi: 10.1016/0013-4694(87)90206-9\nHou, Y., and Chen, S. (2019). Distinguishing diﬀerent emoti ons evoked by\nmusic via electroencephalographic signals. Comput. Intell. Neurosci . 2019:3191903.\ndoi: 10.1155/2019/3191903\nJerritta, S., Murugappan, M., Nagarajan, R., and Wan, K. (2011 ). “Physiological\nsignals based human emotion recognition: a review, ” in 2011 IEEE 7th\nInternational Colloquium on Signal Processing and its Application s, 410–415.\ndoi: 10.1109/CSPA.2011.5759912\nKan, H., Yu, J., Huang, J., Liu, Z., and Zhou, H. (2022). Self-s upervised group\nmeiosis contrastive learning for EEG-based emotion recognit ion. Appl. Intel . 53,\n27207–27225. doi: 10.1007/s10489-023-04971-0\nKipli, K., Latip, A. A. A., Lias, K. B., Bateni, N., Yusoﬀ, S. M., S uud, J. B., et al. (2022).\n“Evaluation of galvanic skin response (GSR) signals features for emotion recognition, ”\nin International Conference on Applied Intelligence and Informatics (Cham: Springer\nNature Switzerland), 260–274. doi: 10.1007/978-3-031-248 01-6_19\nKoelstra, S., Muhl, C., Soleymani, M., Lee, J.-S., Yazdani, A., Ebrahimi, T., et al.\n(2011). Deap: a database for emotion analysis; using physiologi cal signals. IEEE Trans.\nAﬀect. Comput. 3, 18–31. doi: 10.1109/T-AFFC.2011.15\nLee, M. (2023). Gelu activation function in deep learning: a com prehensive\nmathematical analysis and performance. ArXiv, abs/2305.12073.\nLi, Y., Zheng, W., Zong, Y., Cui, Z., Zhang, T., and Zhou, X. (2 021). A bi-hemisphere\ndomain adversarial neural network model for EEG emotion rec ognition. IEEE Trans.\nAﬀect. Comput. 12, 494–504. doi: 10.1109/TAFFC.2018.2885474\nLin, W.-C., Chiu, H.-W., and Hsu, C.-Y. (2006). “Discoverin g EEG signals response\nto musical signal stimuli by time-frequency analysis and indepe ndent component\nanalysis, ” in 2005 IEEE Engineering in Medicine and Biology 27th Annual Conference\n(IEEE), 2765–2768.\nLiu, J., Sun, L., Huang, M., Xu, Y., and Li, R. (2022). Enhanci ng emotion recognition\nusing region-speciﬁc electroencephalogram data and dynamic fu nctional connectivity.\nFront. Neurosci. 16:884475. doi: 10.3389/fnins.2022.884475\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., et al. (202 1). “Swin\ntransformer: hierarchical vision transformer using shift ed windows, ” in Proceedings\nof the IEEE/CVF International Conference on Computer Vision , 10012–10022.\ndoi: 10.1109/ICCV48922.2021.00986\nMinhad, K. N., Ali, S. H. M., and Reaz, M. B. I. (2017). A design f ramework for\nhuman emotion recognition using electrocardiogram and skin conductance response\nsignals. J. Eng. Sci. Technol . 12, 3102–3119. doi: 10.1587/transinf.2017EDP7067\nNag, S., Basu, M., Sanyal, S., Banerjee, A., and Ghosh, D. (202 2). On the application\nof deep learning and multifractal techniques to classify emotio ns and instruments using\nindian classical music. Physica A. 597:127261. doi: 10.1016/j.physa.2022.127261\nOzel, P., Akan, A., and Yilmaz, B. (2019). Synchrosqueezing tra nsform based feature\nextraction from EEG signals for emotional state prediction. Biomed. Sig. Proc. Control\n52, 152–161. doi: 10.1016/j.bspc.2019.04.023\nPan, B., and Zheng, W. (2021). Emotion recognition based on E EG using generative\nadversarial nets and convolutional neural network. Comput. Mathem. Methods Med .\n2021:2520394. doi: 10.1155/2021/2520394\nPan, J., Liang, R., He, Z., Li, J., Liang, Y., Zhou, X., et al. (2 023). St-scgnn: a spatio-\ntemporal self-constructing graph neural network for cross-su bject EEG-based emotion\nrecognition and consciousness detection. IEEE J. Biomed. Health Inf . 28, 777–788.\ndoi: 10.1109/JBHI.2023.3335854\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Cha nan, G., et al. (2019).\n“Pytorch: an imperative style, high-performance deep learning library, ” inAdvances in\nNeural Information Processing Systems, 32.\nQiu, L., Zhong, Y., Xie, Q., He, Z., Wang, X., Chen, Y., et al. (2 022). Multi-modal\nintegration of EEG-fnirs for characterization of brain acti vity evoked by preferred\nmusic. Front. Neurorob. 16:823435. doi: 10.3389/fnbot.2022.823435\nRonneberger, O., Fischer, P., and Brox, T. (2015). “U-net: c onvolutional\nnetworks for biomedical image segmentation, ” in Medical Image Computing\nand Computer-Assisted Intervention-MICCAI 2015: 18th Internation al Conference,\nMunich, Germany, October 5–9, 2015, Proceedings, Part III 18 (Springer), 234–241.\ndoi: 10.1007/978-3-319-24574-4_28\nRudakov, E. (2021). “Multi-task CNN model for emotion recogn ition\nfrom EEG brain maps, ” in 2021 4th International Conference on Bio-\nEngineering for Smart Technologies (BioSMART) , New York, NY USA (IEEE).\ndoi: 10.1109/BioSMART54244.2021.9677807\nSammler, D., Grigutsch, M., Fritz, T., and Koelsch, S. (2007). M usic and emotion:\nelectrophysiological correlates of the processing of pleasant and unpleasant music.\nPsychophysiology 44, 293–304. doi: 10.1111/j.1469-8986.2007.00497.x\nSelvaraj, J., Murugappan, M., Wan, K., and Yaacob, S. B. (2013). Classiﬁcation of\nemotional states from electrocardiogram signals: a non-linea r approach based on hurst.\nBioMed. Eng. OnLine 12:44. doi: 10.1186/1475-925X-12-44\nSemerci, Y. C., Akgün, G., Toprak, E., and Barkana, D. E. (2022) . “A comparative\nanalysis of deep learning methods for emotion recognition usi ng physiological\nsignals for robot-based intervention studies, ” in 2022 Medical Technologies Congress\n(TIPTEKNO), 1–4. doi: 10.1109/TIPTEKNO56568.2022.9960200\nSerra, H., Oliveira, J. P., and Paulino, N. F. (2017). “A 50 hz sc notch ﬁlter for iot\napplications, ” in2017 IEEE International Symposium on Circuits and Systems (ISCAS) ,\n1–4. doi: 10.1109/ISCAS.2017.8050904\nShawky, E., El-Khoribi, R., Shoman, M., and Wahby Shalaby, M. ( 2018). EEG-based\nemotion recognition using 3D convolutional neural networks . Int. J. Adv. Comput. Sci.\nApplic. 9:843. doi: 10.14569/IJACSA.2018.090843\nShen, F., Dai, G., Lin, G., Zhang, J., Kong, W., and Zeng, H. (202 0). EEG-\nbased emotion recognition using 4D convolutional recurrent neural network. Cogn.\nNeurodyna. 14, 1–14. doi: 10.1007/s11571-020-09634-1\nSheykhivand, S., Mousavi, Z., Rezaii, T. Y., and Farzamnia, A. (2020). Recognizing\nemotions evoked by music using CNN-LSTM networks on EEG sign als. IEEE Access 8,\n139332–139345. doi: 10.1109/ACCESS.2020.3011882\nSiddiqui, H. U. R., Shahzad, H. F., Saleem, A. A., Khakwani, A. B . K., Rustam, F.,\nLee, E., et al. (2021). Respiration based non-invasive approach for emotion recognition\nusing impulse radio ultra wide band radar and machine learning. Sensors (Basel,\nSwitzerland) 21:8336. doi: 10.3390/s21248336\nSong, Z. (2021). Facial expression emotion recognition mode l integrating\nphilosophy and machine learning theory. Front. Psychol . 12:759485.\ndoi: 10.3389/fpsyg.2021.759485\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et\nal. (2017). “Attention is all you need, ” in Advances in Neural Information Processing\nSystems, 30.\nVuilleumier, P., and Trost, W. (2015). Music and emotions: fro m enchantment to\nentrainment. Ann. NY Acad. Sci . 1337, 212–222. doi: 10.1111/nyas.12676\nWang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., et a l. (2021). “Pyramid\nvision transformer: a versatile backbone for dense predictio n without convolutions, ” in\nProceedings of the IEEE/CVF International Conference on Computer Vision , 568–578.\ndoi: 10.1109/ICCV48922.2021.00061\nWei, W., Jia, Q., Feng, Y., and Chen, G. (2018). Emotion recogn ition based\non weighted fusion strategy of multichannel physiological sig nals. Comput. Intell.\nNeurosci. 2018:5296523. doi: 10.1155/2018/5296523\nWu, B., Xu, C., Dai, X., Wan, A., Zhang, P., Yan, Z., et al. (2020 ). Visual transformers:\nToken-based image representation and processing for computer vision. arXiv preprint\narXiv:2006.03677.\nXing, X., Li, Z., Xu, T., Shu, L., and Xu, X. (2019). SAE+LSTM: a new\nframework for emotion recognition from multi-channel EEG. Front. Neurorobot .\n13:37. doi: 10.3389/fnbot.2019.00037\nXu, M., Cheng, J., Li, C., Liu, Y., and Chen, X. (2023). Spatio- temporal deep forest\nfor emotion recognition based on facial electromyography sig nals. Comput. Biol. Med .\n156:106689. doi: 10.1016/j.compbiomed.2023.106689\nYang, C., Xu, J., Mello, S. D., Crowley, E. J., and Wang, X. (2022) . Gpvit: a\nhigh resolution non-hierarchical vision transformer with g roup propagation. ArXiv,\nabs/2212.06795.\nFrontiers in Psychology /one.tnum/one.tnum frontiersin.org\nWang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpsyg./two.tnum/zero.tnum/two.tnum/four.tnum./one.tnum/two.tnum/seven.tnum/five.tnum/one.tnum/four.tnum/two.tnum\nYang, H., Han, J., and Min, K. (2019). A multi-column cnn model f or emotion\nrecognition from EEG signals. Sensors 19:4736. doi: 10.3390/s19214736\nYang, Y., Wu, Q., Qiu, M., Wang, Y., and Chen, X. (2018). “Emot ion\nrecognition from multi-channel EEG through parallel convolutio nal recurrent neural\nnetwork, ” in 2018 International Joint Conference on Neural Networks (IJCNN) , 1–7.\ndoi: 10.1109/IJCNN.2018.8489331\nZhang, H., Yi, P., Liu, R., and Zhou, D. (2021). “Emotion reco gnition\nfrom body movements with as-LSTM, ” in 2021 IEEE 7th International\nConference on Virtual Reality (ICVR) , 26–32. doi: 10.1109/ICVR51878.2021.94\n83833\nZhang, Z., Zhong, S. H., and Liu, Y. (2022). Ganser: a self-supe rvised data\naugmentation framework for EEG-based emotion recognition. IEEE Trans. Aﬀect.\nComput. 14, 2048–2063. doi: 10.1109/TAFFC.2022.3170369\nZhou, X., Zhuo, J., and Krahenbuhl, P. (2019). “Bottom-up obj ect detection by\ngrouping extreme and center points, ” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 850–859. doi: 10.1109/CVPR.2019.00094\nFrontiers in Psychology /one.tnum/two.tnum frontiersin.org",
  "topic": "Electroencephalography",
  "concepts": [
    {
      "name": "Electroencephalography",
      "score": 0.7904410362243652
    },
    {
      "name": "Computer science",
      "score": 0.6226693391799927
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6193826794624329
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.4764019548892975
    },
    {
      "name": "Categorization",
      "score": 0.47590455412864685
    },
    {
      "name": "Emotion classification",
      "score": 0.466845840215683
    },
    {
      "name": "Speech recognition",
      "score": 0.45483773946762085
    },
    {
      "name": "Machine learning",
      "score": 0.4295249581336975
    },
    {
      "name": "Binary classification",
      "score": 0.42125001549720764
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.375882089138031
    },
    {
      "name": "Psychology",
      "score": 0.31100717186927795
    },
    {
      "name": "Support vector machine",
      "score": 0.28968459367752075
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    }
  ],
  "cited_by": 8
}