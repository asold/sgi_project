{
  "title": "Revisiting Transformer-based Models for Long Document Classification",
  "url": "https://openalex.org/W4385573298",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2064365103",
      "name": "Xiang Dai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2779539080",
      "name": "Ilias Chalkidis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A907203725",
      "name": "Sune Darkner",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2257973904",
      "name": "Desmond Elliott",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3200993350",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3202026671",
    "https://openalex.org/W3114610051",
    "https://openalex.org/W2971092323",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2939507640",
    "https://openalex.org/W1550668049",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2962785754",
    "https://openalex.org/W2798843374",
    "https://openalex.org/W2889577585",
    "https://openalex.org/W2096152098",
    "https://openalex.org/W2900758626",
    "https://openalex.org/W3106298483",
    "https://openalex.org/W2739515596",
    "https://openalex.org/W2982597744",
    "https://openalex.org/W3134691471",
    "https://openalex.org/W3035294872",
    "https://openalex.org/W3213241618",
    "https://openalex.org/W3176443126",
    "https://openalex.org/W2985173696",
    "https://openalex.org/W2470673105",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W4285261371",
    "https://openalex.org/W3106092787",
    "https://openalex.org/W2964142373",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W4306672396",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W3167829962",
    "https://openalex.org/W1870686808",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2250966211",
    "https://openalex.org/W4221143523",
    "https://openalex.org/W3100452049",
    "https://openalex.org/W3034503829",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W2007666495",
    "https://openalex.org/W2955041501",
    "https://openalex.org/W3209254806",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W3174982164",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3093838622",
    "https://openalex.org/W3132259035",
    "https://openalex.org/W3102232851"
  ],
  "abstract": "The recent literature in text classification is biased towards short text sequences (e.g., sentences or paragraphs). In real-world applications, multi-page multi-paragraph documents are common and they cannot be efficiently encoded by vanilla Transformer-based models. We compare different Transformer-based Long Document Classification (TrLDC) approaches that aim to mitigate the computational overhead of vanilla transformers to encode much longer text, namely sparse attention and hierarchical encoding methods.We examine several aspects of sparse attention (e.g., size of local attention window, use of global attention) and hierarchical (e.g., document splitting strategy) transformers on four document classification datasets covering different domains. We observe a clear benefit from being able to process longer text, and, based on our results, we derive practical advice of applying Transformer-based models on long document classification tasks.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 7212–7230\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nRevisiting Transformer-based Models for Long Document Classification\nXiang Dai ∗\nCSIRO Data61\ndai.dai@csiro.au\nIlias Chalkidis\nUniversity of Copenhagen\nilias.chalkidis@di.ku.dk\nSune Darkner\nUniversity of Copenhagen\ndarkner@di.ku.dk\nDesmond Elliott\nUniversity of Copenhagen\nPioneer Centre for AI\nde@di.ku.dk\nAbstract\nThe recent literature in text classification is bi-\nased towards short text sequences (e.g., sen-\ntences or paragraphs). In real-world applica-\ntions, multi-page multi-paragraph documents\nare common and they cannot be efficiently en-\ncoded by vanilla Transformer-based models.\nWe compare different Transformer-based Long\nDocument Classification (TrLDC) approaches\nthat aim to mitigate the computational over-\nhead of vanilla transformers to encode much\nlonger text, namely sparse attention and hierar-\nchical encoding methods. We examine several\naspects of sparse attention (e.g., size of local\nattention window, use of global attention) and\nhierarchical (e.g., document splitting strategy)\ntransformers on four document classification\ndatasets covering different domains. We ob-\nserve a clear benefit from being able to process\nlonger text, and, based on our results, we derive\npractical advice of applying Transformer-based\nmodels on long document classification tasks.1\n1 Introduction\nNatural language processing has been revolu-\ntionised by the large scale self-supervised pre-\ntraining of language encoders (Devlin et al., 2019;\nLiu et al., 2019), which are fine-tuned in order to\nsolve a wide variety of downstream classification\ntasks. However, the recent literature in text classi-\nfication mostly focuses on short sequences, such\nas sentences or paragraphs (Sun et al., 2019; Ad-\nhikari et al., 2019; Mosbach et al., 2021), which\nare sometimes misleadingly named as documents.2\nThe transition from short-to-long document clas-\nsification is non-trivial. One challenge is that\nBERT and most of its variants are pre-trained on\n∗This work was partially done when Dai was at the Uni-\nversity of Copenhagen.\n1Code is available at https://github.com/\ncoastalcph/trldc\n2For example, many biomedical datasets use ‘documents’\nfrom the PubMed collection of biomedical literature, but these\ndocuments actually consist of titles and abstracts.\n512 1024 2048 4096\nMaximum sequence length\n52\n54\n56\n58\n60\n62\n64Micro F1\nFigure 1: The effectiveness of Longformer, a long-\ndocument Transformer, on the MIMIC-III development\nset. There is a clear benefit from being able to process\nlonger text.\nsequences containing up-to 512 tokens, which is\nnot a long document. A common practice is to\ntruncate actually long documents to the first 512\ntokens, which allows the immediate application\nof these pre-trained models (Adhikari et al., 2019;\nChalkidis et al., 2020). We believe that this is an\ninsufficient approach for long document classifica-\ntion because truncating the text may omit important\ninformation, leading to poor classification perfor-\nmance (Figure 1). Another challenge comes from\nthe computational overhead of vanilla Transformer:\nin the multi-head self-attention operation (Vaswani\net al., 2017), each token in a sequence of ntokens\nattends to all other tokens. This results in a func-\ntion that has O(n2) time and memory complexity,\nwhich makes it challenging to efficiently process\nlong documents.\nIn response to the second challenge, long-\ndocument Transformers have emerged to deal with\nlong sequences (Beltagy et al., 2020; Zaheer et al.,\n2020). However, they experiment and report re-\nsults on non-ideal long document classification\ndatasets, i.e., documents on the IMDB dataset are\nnot really long – fewer than 15% of examples are\nlonger than 512 tokens; while the Hyperpartisan\n7212\ndataset only has very few (645 in total) documents.\nOn datasets with longer documents, such as the\nMIMIC-III dataset (Johnson et al., 2016) with an\naverage length of 2,000 words, it has been shown\nthat multiple variants of BERT perform worse than\na CNN or RNN-based model (Chalkidis et al.,\n2020; Vu et al., 2020; Dong et al., 2021; Ji et al.,\n2021a; Gao et al., 2021; Pascual et al., 2021). We\nbelieve there is a need to understand the perfor-\nmance of Transformer-based models on classifying\ndocuments that are actually long.\nIn this work, we aim to transfer the success of\nthe pre-train–fine-tune paradigm to long document\nclassification. Our main contributions are:\n• We compare different long document classifi-\ncation approaches based on transformer archi-\ntecture: namely, sparse attention, and hierar-\nchical methods. Our results show that process-\ning more tokens can bring drastic improve-\nments comparing to processing up-to 512 to-\nkens.\n• We conduct careful analyses to understand the\nimpact of several design options on both the\neffectiveness and efficiency of different ap-\nproaches. Our results show that some design\nchoices (e.g., size of local attention window in\nsparse attention method) can be adjusted to im-\nprove the efficiency without sacrificing the ef-\nfectiveness, whereas some choices (e.g., docu-\nment splitting strategy in hierarchical method)\nvastly affect effectiveness.\n• Last but not least, our results show that, con-\ntrary to previous claims, Transformer-based\nmodels can outperform former state-of-the-art\nCNN based models on MIMIC-III dataset .\n2 Problem Formulation and Datasets\nWe divide the document classification model into\ntwo components: (1) a document encoder, which\nbuilds a vector representation of a given document;\nand, (2) a classifier that predicts a single or multi-\nple labels given the encoded vector. In this work,\nwe mainly focus on the first component: we use\nTransformer-based encoders to build a document\nrepresentation, and then take the encoded docu-\nment representation as the input to a classifier. For\nthe second component, we use a TANH activated\nhidden layer, followed by the output layer. Output\nprobabilities are obtained by applying a SIGMOID\n102 103 104\nNumber of tokens (log 10 scale)\n0\n100\n200\n300\n400\n500\n600\n700Number of documents (Density)\nDatasets\nMIMIC-III\nECtHR\nHyperpartisan\n20 News\nFigure 2: The distribution of document lengths. A log-\n10 scale is used for the X axis.\n(multi-label) or SOFTMAX (multi-class) function\nto output logits.3\nWe mainly conduct our experiments on the\nMIMIC-III dataset (Johnson et al., 2016), where re-\nsearchers still fail to transfer “the Magic of BERT”\nto medical code assignment tasks (Ji et al., 2021a;\nPascual et al., 2021).\nMIMIC-III contains Intensive Care Unit (ICU)\ndischarge summaries, each of which is anno-\ntated with multiple labels—diagnoses and pro-\ncedures—using the ICD-9 (The International\nClassification of Diseases, Ninth Revision) hi-\nerarchy. Following Mullenbach et al. (2018),\nwe conduct experiments using the top 50 fre-\nquent labels.4\nTo address the generalisation concern, we also\nuse three datasets from other domains: EC-\ntHR (Chalkidis et al., 2022) sourced from legal\ncases, Hyperpartisan (Kiesel et al., 2019) and 20\nNews (Joachims, 1997), both from news articles.\nECtHR contains legal cases from The European\nCourt of Human Rights’ public database.\nThe court hears allegations that a state has\nbreached human rights provisions of the Euro-\npean Convention of Human Rights, and each\ncase is mapped to one or more articles of the\nconvention that were allegedly violated.5\nHyperpartisan contains news articles which are\n3Long document classification datasets are usually anno-\ntated using a large number of labels. Studies that have focused\non the second component investigate methods of utilising label\nhierarchy (Chalkidis et al., 2020; Vu et al., 2020), pre-training\nlabel embeddings (Dong et al., 2021), to name but a few.\n4Details about dataset split and labels can be found at\nhttps://github.com/jamesmullenbach/caml-mimic\n5https://huggingface.co/datasets/ecthr_cases\n7213\nt0 t1 t2 t4 t5 t6t3\nVanilla self-\nattention\nt0 t1 t2 t4 t5 t6t3\nLocal\nattention\nt0 t1 t2 t4 t5 t6t3\nGlobal\nattention\nFigure 3: A comparison of three types of attention op-\nerations. The example sequence contains 7 tokens; we\nset local attention window size as 2, and only the first\ntoken using global attention. Note that these curves are\nbi-directional that tokens can attend to each other.\nmanually labelled as hyperpartisan (taking an\nextreme left or right standpoint) or not.6\n20 News contains newsgroups posts which are cat-\negorised into 20 topics.7\nWe note that documents in MIMIC-III and ECtHR\nare much longer than those in Hyperpartisan and\n20 News (Table 5 in Appendix and Figure 2).\n3 Approaches\nIn the era of Transformer-based models, we iden-\ntify two representative approaches of processing\nlong documents in the literature that either acts as\nan inexpensive drop-in replacement for the vanilla\nself-attention (i.e., sparse attention) or builds a task-\nspecific architecture (i.e., hierarchical Transform-\ners).\n3.1 Sparse-Attention Transformers\nVanilla transformer relies on the multi-head self-\nattention mechanism, which scales poorly with the\nlength of the input sequence, requiring quadratic\ncomputation time and memory to store all scores\nthat are used to compute the gradients during\nback-propagation (Qiu et al., 2020). Several\nTransformer-based models (Kitaev et al., 2020; Tay\net al., 2020; Choromanski et al., 2021) have been\nproposed exploring efficient alternatives that can\nbe used to process long sequences.\nLongformer of Beltagy et al. (2020) consists\nof local (window-based) attention and global at-\ntention that reduces the computational complexity\nof the model and thus can be deployed to process\nup to 4096 tokens. Local attention is computed\n6https://pan.webis.de/semeval19/semeval19-web/; we use\nthe split provided by Beltagy et al. (2020).\n7http://qwone.com/~jason/20Newsgroups/\nt0 ... t127 ... t255 t256 ... t383t128\nRoBERTa RoBERTa RoBERTa\nh0 ... h127 ... h255 h256 ... h383h128\nTokens\np0 p1 p2\n+ + +\nTwo transformer blocks\nSegment\nposition\nembeddings\nContextual\ntoken\nrepresentations\ns0 s1 s2\nContextual\nsegment\nrepresentations\nAggregator (e.g., max pooling)\nDocument vector\n...\n...\n...\n...\n...\n...\nn0 n1 n2 ...\nNon-contextual\nsegment\nrepresentations\nFigure 4: A high-level illustration of hierarchical Trans-\nformers. A shared pre-trained RoBERTa is used to en-\ncode each segment, and a two layer transformer blocks\nis used to capture the interaction between different seg-\nments. Finally, contextual segment representations are\naggregated into a document representation.\nin-between a window of neighbour (consecutive)\ntokens. Global attention relies on the idea of global\ntokens that are able to attend and be attended by any\nother token in the sequence (Figure 3). BigBird\nof Zaheer et al. (2020) is another sparse-attention\nbased Transformer that uses a combination of a\nlocal, global and random attention, i.e., all tokens\nalso attend a number of random tokens on top of\nthose in the same neighbourhood. Both models are\nwarm-started from the public RoBERTa checkpoint\nand are further pre-trained on masked language\nmodelling. They have been reported to outperform\nRoBERTa on a range of tasks that require mod-\nelling long sequences.\nWe choose Longformer (Beltagy et al., 2020) in\nthis study and refer readers to Xiong et al. (2021)\nfor a systematic comparison of recent proposed\nefficient attention variants.\n3.2 Hierarchical Transformers\nInstead of modifying multi-head self-attention\nmechanism to efficiently model long sequences,\nhierarchical Transformers build on top of vanilla\ntransformer architecture.\nA document, D= {t0,t1,··· ,t|D|}, is first split\ninto segments, each of which should have less than\n512 tokens. These segments can be independently\nencoded using any pre-trained Transformer-based\nencoders (e.g., RoBERTa in Figure 4). We sum\nthe contextual representation of the first token from\neach segment up with segment position embed-\ndings as the segment representation (i.e., ni in\nFigure 4). Then the segment encoder—two trans-\nformer blocks (Zhang et al., 2019)—are used to\n7214\ncapture the interaction between segments and out-\nput a list of contextual segment representations (i.e.,\nsi in Figure 4), which are finally aggregated into\na document representation. By default, the aggre-\ngator is the max-pooling operation unless other\nspecified.\n4 Experimental Setup\nBackbone Models We mainly consider two mod-\nels in our experiments: Longformer-base (Beltagy\net al., 2020), and RoBERTa-base (Liu et al., 2019)\nwhich is used in hierarchical Transformers.\nEvaluation metrics For the MIMIC-III (mul-\ntilabel) dataset, we follow previous work (Mul-\nlenbach et al., 2018; Cao et al., 2020) and use\nmicro-averaged AUC (Area Under the receiver\noperating characteristic Curve), macro-averaged\nAUC, micro-averaged F1, macro-averaged F1 and\nPrecision@5—the proportion of the ground truth\nlabels in the top-5 predicted labels—as the metrics.\nWe report micro and macro averaged F1 for the\nECtHR (multilabel) dataset, and accuracy for both\nHyperpartisan (binary) and 20 News (multiclass)\ndatasets.\n5 Experiments\nWe conduct a series of controlled experiments to\nunderstand the impact of design choices in differ-\nent TrLDC models. Bringing these optimal choices\nall together, we compare TrLDC against the state\nof the art, as well as baselines that only process up-\nto 512 tokens. Finally, based on our investigation,\nwe derive practical advice of applying transformer-\nbased models to long document classification re-\ngarding both effectiveness and efficiency.\nTask-adaptive pre-training is a promising first\nstep. Domain-adaptive pre-training (DAPT) – the\ncontinued pre-training a language model on a large\ncorpus of domain-specific text – is known to im-\nprove downstream task performance (Gururangan\net al., 2020; Kær Jørgensen et al., 2021). How-\never, task-adaptive pre-training (TAPT) – contin-\nues unsupervised pre-training on the task’s data –\nis comparatively less studied, mainly because most\nof the benchmarking corpora are small and thus the\nbenefit of TAPT seems less obvious than DAPT.\nWe believe document classification datasets, due\nto their relatively large size, can benefit from\nTAPT. On both MIMIC-III and ECtHR, we con-\ntinue to pre-train Longformer and RoBERTa us-\n = 4.5\nMax sequence length: 4096\n63\n64\n65\n66\n67\n68\n69Micro F1\nVanilla\nTAPT\n(a) Longformer on MIMIC-III\n = 2.9\nMax sequence length: 512\n53\n54\n55\n56\n57Micro F1\nVanilla\nTAPT (b) RoBERTa on MIMIC-III\n = 0.9\nMax sequence length: 4096\n80.5\n81.0\n81.5\n82.0\n82.5\n83.0\n83.5\n84.0Micro F1\nVanilla\nTAPT\n(c) Longformer on ECtHR\n = 0.4\nMax sequence length: 512\n73.5\n74.0\n74.5\n75.0\n75.5\n76.0Micro F1\nVanilla\nTAPT (d) RoBERTa on ECtHR\nFigure 5: Task-adaptive pre-training (right side in each\nplot) can improve the effectiveness (measured on the de-\nvelopment sets) of pre-trained models by a large margin\non MIMIC-III, but small on ECtHR. ∆: the difference\nbetween mean values of compared experiments.\ning the masked language modelling pre-training\nobjective (details about pre-training can be found\nat Appendix 9.3). We find that task-adaptive pre-\ntrained models substantially improve performance\non MIMIC-III (Figure 5 (a) and (b)), but there are\nsmaller improvements on ECtHR (Figure 5 (c) and\n(d)). We suspect this difference is because legal\ncases (i.e., ECtHR) are publicly available and have\nbeen covered in pre-training data used for training\nLongformer and RoBERTa, whereas clinical notes\n(i.e., MIMIC-III) are not (Dodge et al., 2021). See\nAppendix 9.5 for a short analysis on this matter.\nWe also compare our TAPT-RoBERTa against\npublicly available domain-specific RoBERTa,\ntrained from scratch on biomedical articles and clin-\nical notes. Results (Figure 8 in Appendix) show\nthat TAPT-RoBERTa outperforms domain-specific\nbase model, but underperforms the larger model.\n5.1 Longformer\nSmall local attention windows are effective and\nefficient. Beltagy et al. (2020) observe that many\ntasks do not require reasoning over the entire con-\ntext. For example, they find that the distance be-\ntween any two mentions in a coreference resolution\ndataset (i.e., OntoNotes) is small, and it is possible\nto achieve competitive performance by processing\nsmall segments containing these mentions.\nInspired by this observation, we investigate the\nimpact of local context size on document classifi-\ncation, regarding both effectiveness and efficiency.\n7215\nSize Micro F1\nSpeed\nTrain Test\n32 67.9 ± 0.3 9.9 (2.9x) 45.6 (2.8x)\n64 68.1 ± 0.1 8.8 (2.6x) 41.4 (2.5x)\n128 68.3 ± 0.3 7.4 (2.1x) 34.1 (2.1x)\n256 68.4 ± 0.3 5.5 (1.6x) 25.4 (1.6x)\n512 68.5 ± 0.3 3.5 (1.0x) 16.3 (1.0x)\nTable 1: The impact of local attention window size in\nLongformer on MIMIC-III development set. Speed is\nmeasured using ‘processed samples per second’, and\nnumbers in parenthesis are the relative speedup.\n1 8 16 32 64\nNumber of tokens using global attention\n66.0\n66.5\n67.0\n67.5\n68.0\n68.5\n69.0\n69.5\n70.0Micro F1 (MIMIC-III)\nMIMIC-III\nECtHR\n79.0\n79.5\n80.0\n80.5\n81.0\n81.5\n82.0\n82.5\n83.0\nMicro F1 (ECtHR)\nMIMIC-III\nECtHR\nFigure 6: The effect of applying global attention on\nmore tokens, which are evenly chosen based on their\npositions. In the baseline model (first column), only the\n[CLS] token uses global attention.\nWe hypothesise that long document classification,\nwhich is usually paired with a large label space, can\nbe performed by models that only attend over short\nsequences instead of the entire document (Gao\net al., 2021). In this experiment, we vary the local\nattention window around each token.\nTable 1 shows that even using a small window\nsize, the micro F1 score on MIMIC-III develop-\nment set is still close to using a larger window size.\nWe observe the same pattern on ECtHR and 20\nNews (See Table 11 in the Appendix). A major ad-\nvantage of using smaller local attention windows is\nthe faster computation for training and evaluation.\nConsidering a small number of tokens for global\nattention improves the stability of the train-\ning process. Longformer relies heavily on the\n[CLS] token, which is the only token with global\nattention—attending to all other tokens and all\nother tokens attending to it. We investigate whether\nallowing more tokens to use global attention can\nimprove model performance, and if yes, how to\nchoose which tokens to use global attention.\nFigure 6 shows that adding more tokens using\nglobal attention does not improve F1 score, while a\n32\n=0.6\n64\n=0.8\n128\n=0.5\n256\n=0.5\n512\n=0.5\nSegment length\n65.5\n66.0\n66.5\n67.0\n67.5\n68.0\n68.5\n69.0\n69.5Micro F1\nMIMIC-III\nDisjoint\nOverlap\n32\n=0.1\n64\n=0.8\n128\n=0.4\n256\n=0.3\n512\n=-0.2\nSegment length\n78\n79\n80\n81\n82\n83Micro F1\nECtHR\nDisjoint\nOverlap\nFigure 7: The effect of varying the segment length and\nwhether allowing segments to overlap in the hierarchical\nTransformers. ∆: improvement due to overlap.\nsmall number of additional global attention tokens\ncan make the training more stable.\nEqually distributing global tokens across the\nsequence is better than content-based attribu-\ntion. We consider two approaches to choose ad-\nditional tokens that use global attention: position\nbased or content based. In the position-based ap-\nproach, we distribute nadditional tokens at equal\ndistances. For example, if n= 4 and the sequence\nlength is 4096, there are global attention on tokens\nat position 0, 1024, 2048 and 3072. In the content-\nbased approach, we identify informative tokens,\nusing TF-IDF (Term Frequency–Inverse Document\nFrequency) within each document, and we apply\nglobal attention on the top-K informative tokens,\ntogether with the [CLS] token. Results show that\nthe position based approach is more effective than\ncontent based (see Table 13 in the Appendix).\n5.2 Hierarchical Transformers\nThe optimal segment length is dataset depen-\ndent. Ji et al. (2021a) and Gao et al. (2021) re-\nported negative results with a hierarchical Trans-\nformer with a segment length of 512 tokens on the\n7216\nMIMIC-III dataset. Their methods involved split-\nting a document into equally sized segments, which\nwere processed using a shared BERT encoder. In-\nstead of splitting the documents into such large\nsegments, we investigate the impact of segment\nlength and preventing context fragmentation.\nFigure 7 (left side in each violin plot) shows\nthat there is no optimal segment length across\nboth MIMIC-III and ECtHR. Small segment length\nworks well on MIMIC-III, and using segment\nlength greater than 128 starts to decrease the per-\nformance. In contrast, the ECtHR dataset benefits\nfrom a model with larger segment lengths. The\noptimal performing segment length on 20 News\nand Hyperpartisan are 256 and 128, respectively\n(See Table 14 in the Appendix).\nSplitting documents into overlapping seg-\nments can alleviate the context fragmentation\nproblem. Splitting a long document into smaller\nsegments may result in the problem of context frag-\nmentation, where a model lacks the information it\nneeds to make a prediction (Dai et al., 2019; Ding\net al., 2021). Although, the hierarchical model uses\na second-order transformer to fuse and contextu-\nalise information across segments, we investigate\na simple way to alleviate context fragmentation by\nallowing segments to overlap when we split a doc-\nument into segments. That it, except for the first\nsegment, the first 1\n4 ntokens in each segment are\ntaken from the previous segment, where nis the\nsegment length. Figure 7 (right side in each vio-\nlin plot) show that this simple strategy can easily\nimprove the effectiveness of the model.\nSplitting based on document structure.\nChalkidis et al. (2022) argue that we should follow\nthe structure of a document when splitting it into\nsegments (Tang et al., 2015; Yang et al., 2016).\nThey propose a hierarchical Transformer for\nthe ECtHR dataset that splits a document at the\nparagraph level, reading up to 64 paragraphs of\n128 token each (8192 tokens in total).\nWe investigate whether splitting based on doc-\nument structure is better than splitting a long doc-\nument into segments of same length. Similar to\ntheir model, we consider each paragraph as a seg-\nment and all segments are then truncated or padded\nto the same segment length. We follow Chalkidis\net al. (2022) and use segment length (l) of 128 on\nECtHR, and tune l∈{32, 64, 128} on MIMIC-III.8\n8Note that since we need to pad short segments, therefore,\nResults show that splitting by the paragraph-\nlevel document structure does not improve per-\nformance on the ECtHR dataset. On MIMIC-III,\nsplitting based on document structure substantially\nunderperforms evenly splitting the document (Fig-\nure 9 in the Appendix) .\n5.3 Label-wise Attention Network\nRecall from Section 3 that our models form a sin-\ngle document vector which is used for the final\nprediction. That is, in Longformer, we use the\nhidden states of the [CLS] token; in hierarchical\nmodels, we use the max pooling operation to ag-\ngregate a list of contextual segment representations\ninto a document vector. The Label-Wise Atten-\ntion Network (LWAN) (Mullenbach et al., 2018;\nXiao et al., 2019; Chalkidis et al., 2020) is an al-\nternative that allows the model to learn distinct\ndocument representations for each label. Given a\nsequence of hidden representations (e.g., contex-\ntual token representations in Longformer or contex-\ntual segment representations in hierarchical models:\nS= [s0,s1,··· ,sm]), LWAN can allow each la-\nbel to learn to attend to different positions via:\naℓ = SoftMax(S⊤uℓ) (1)\nvℓ =\nm∑\ni=1\naℓ,isi (2)\nˆyℓ = σ(β⊤\nℓ vℓ) (3)\nwhere uℓ and βℓ are vector parameters for label ℓ.\nResults show that adding a LW AN improves per-\nformance on MIMIC-III (Micro F1 score of 1.1\nwith Longformer; 1.8 with hierarchical models),\nwhere on average each document is assigned 6 la-\nbels out of 50 available labels (classes). There is\na smaller improvement on ECtHR (0.4 with Long-\nformer; 0.1 with hierarchical models), where the\naverage number of labels per document is 1.5 out\nof 10 labels (classes) in total (Table 16 in the Ap-\npendix).\n5.4 Comparison with State of the art\nWe compare TrLDC models against recently pub-\nlished results on MIMIC-III, as well as baseline\nmodels that process up to 512 tokens. In addition\nto the common practice of truncating long docu-\nments (i.e., using the first 512 tokens), we consider\ntwo alternatives that either randomly choose 512\na larger maximum sequence length is required to preserve the\nsame information as in evenly splitting.\n7217\nMacro AUC Micro AUC Macro F1 Micro F1 P@5\nCAML (Mullenbach et al., 2018) C 88.4 91.6 57.6 63.3 61.8\nPubMedBERT (Ji et al., 2021a) T 88.6 90.8 63.3 68.1 64.4\nGatedCNN-NCI (Ji et al., 2021b) C 91.5 93.8 62.9 68.6 65.3\nLAAT (Vu et al., 2020) R 92.5 94.6 66.6 71.5 67.5\nMSMN (Yuan et al., 2022) R 92.8 94.7 68.3 72.5 68.0\nBaselines processing up to 512 tokens\nFirst T 83.0 ± 0.1 86.0 ± 0.1 47.0 ± 0.4 56.1 ± 0.2 55.4 ± 0.2\nRandom T 82.5 ± 0.2 85.4 ± 0.1 42.7 ± 0.4 51.1 ± 0.2 52.3 ± 0.2\nInformative T 82.7 ± 0.1 85.8 ± 0.1 46.4 ± 0.5 55.2 ± 0.3 54.8 ± 0.2\nLong document models\nLongformer (4096 + LW AN) T 90.0 ± 0.1 92.6 ± 0.2 60.7 ± 0.6 68.2 ± 0.2 64.8 ± 0.2\nHierarchical (4096 + LW AN) T 91.1 ± 0.1 93.6 ± 0.0 62.9 ± 0.1 69.5 ± 0.1 65.7 ± 0.2\nHierarchical (4096 + LW AN + L*) T 91.7 ± 0.1 94.1 ± 0.0 65.2 ± 0.2 71.0 ± 0.1 66.2 ± 0.1\nHierarchical (8192 + LW AN) T 91.4 ± 0.0 93.7 ± 0.1 63.8 ± 0.3 70.1 ± 0.1 65.9 ± 0.1\nHierarchical (8192 + LW AN + L*) T 91.9 ± 0.2 94.1 ± 0.2 65.5 ± 0.7 71.1 ± 0.4 66.4 ± 0.3\nTable 2: Comparison of TrLDC against state-of-the-art on the MIMIC-III test set. C: CNN-based models; R:\nRNN-based models; and T: Transformer-based models. Models marked with an asterisk (*) is domain-specific\nRoBERTa-Large (Lewis et al., 2020), whereas Longformer and other RoBERTa models are task-adaptive pre-trained\nbase versions.\n.\nECtHR 20 News Hyper\nFirst (512) 73.5 ± 0.2 86.1 ± 0.3 92.9 ± 3.2\nRandom (512) 79.0 ± 0.6 85.3 ± 0.4 88.9 ± 2.5\nInformative (512) 72.4 ± 0.2 86.2 ± 0.3 91.7 ± 3.2\nLongformer (4096) 81.0 ± 0.5 86.3 ± 0.5 97.9 ± 0.7\nHierarchical (4096) 81.1 ± 0.2 86.3 ± 0.2 95.4 ± 1.3\nTable 3: Comparison of TrLDC against baselines pro-\ncessing up to 512 tokens. We report Micro F1 on EC-\ntHR, Accuracy on 20 News and Hyperpartisan datasets.\n.\ntokens from the document as input or take as input\nthe most informative 512 tokens, identified using\nTF-IDF scores.\nResults in Table 2 and 3 show that there is a\nclear benefit from being able to process longer text.\nBoth the Longformer and hierarchical Transform-\ners outperform baselines that process up to 512\ntokens with a large margin on MIMIC-III and EC-\ntHR, whereas relatively small improvements on 20\nNews and Hyperpartisan. It is also worthy noting\nthat, among these baselines, there is no single best\nstrategy to choose which 512 tokens to process. Us-\ning the first 512 tokens works well on MIMIC-III\nand Hyperpartisan datasets, but it performs much\nworse than 512 random tokens on ECtHR.\nFinally, Longformer, which can process up to\n4096 tokens, achieves competitive results with the\nbest performing CNN-based model (Ji et al., 2021b)\non MIMIC-III. By processing longer text and us-\ning the RoBERTa-Large model, the hierarchical\nmodels further improve the performance, leading\nto comparable results of RNN-based models (Vu\net al., 2020; Yuan et al., 2022). We hypothesise\nthat further improvements can be observed when\nTrLDC models are enhanced with better hierarchy-\naware classifier as in Vu et al. (2020) or code syn-\nonyms are used for training as in Yuan et al. (2022).\n5.5 Comparison in terms of of GPU memory\nconsumption\nGPU memory becomes a big constraint when\nTransformer-based models are trained on long text.\nTable 4 shows a comparison between Longformer\nand Hierarchical models regarding the number of\nparameters and their GPU consumption. We use\nbatch size of 2 in these experiments, and measure\nthe impact of attention window size and segment\nlength on the memory footprint. We find that Hi-\nerarchical models require less GPU memory than\nLongformer in general, and it is possible to set\nsmaller local window size in Longformer or seg-\nment length in hierarchical models to fit the model\nusing smaller GPU memory. Recall that small local\n7218\nLongformer Hierarchical\nSize (148.6M) (139.0M)\nMaximum sequence length: 1024\n64 4.8G 3.6G\n128 5.0G 3.8G\n256 5.5G 4.1G\n512 6.6G 4.7G\nMaximum sequence length: 4096\n64 11.8G 7.8G\n128 12.8G 8.4G\n256 14.9G 9.6G\n512 19.4G 12.2G\nTable 4: A comparison between Longformer and Hier-\narchical models regarding their GPU memory consump-\ntion. The number of parameters are listed in the table\nheader. Size refers to the local attention window size\nin Longformer and the segment length in hierarchical\nmethod, respectively.\nattention windows are effective in Longformer, and\nthe optimal segment length in hierarchical models\nis dataset dependent.\n6 Practical Advice\nWe compile several questions that practitioners may\nask regarding long document classification and pro-\nvide answers based on our results:\nQ1 When should I start to consider using long\ndocument classification models?\nA We suggest using TrLDC models if you work\nwith datasets consisting of long documents (e.g.,\n2K tokens on average). We notice that on 20 News\ndataset, the gap between baselines that process 512\ntokens and long document models is negligible.9\nQ2 Which model should I choose? Longformer\nor hierarchical Transformers?\nA We suggest Longformer as the starting point\nif you do not plan on extensively tuning hyperpa-\nrameters. We find the default config of Longformer\nis robust, although it is possible to set a moderate\nsize (64-128) of local attention window to improve\n9Although Hyperpartisan is a widely used benchmark for\nlong document models, we do not recommend drawing practi-\ncal conclusions based on our results because we observe high\nvariance when we run experiments using different GPUs or\nCUDA versions. We attribute this may to the small size (65)\nof its test set and the subjectivity of the task.\nefficiency without sacrificing effectiveness, and a\nsmall number of additional global attention tokens\nto make the training more stable. On the other hand,\nhierarchical Transformers may benefit from care-\nful hyperparameter tuning (e.g., document splitting\nstrategy, using LW AN). We suggest splitting a doc-\nument into small non-structure-derived segments\n(e.g., 128 tokens) which overlap as a starting point\nwhen employing hierarchical Transformers.\nWe also note that the publicly available Long-\nformer models can process sequences up-to 4096\ntokens, whereas hierarchical Transformers can be\neasily extended to process much longer sequence.\n7 Related Work\nLong document classification Document length\nwas not a point of controversy in the pre-neural era\nof NLP, where documents are encoded with Bag-\nof-Word representations, e.g., TF-IDF scores. The\nissue arised with the introduction of deep neural\nnetworks. Tang et al. (2015) use CNN and BiL-\nSTM based hierarchical networks in a bottom-up\nfashion, i.e., first encode sentences into vectors,\nthen combine those vectors in a single document\nvector. Similarly, Yang et al. (2016) incorporate\nthe attention mechanism when constructing the sen-\ntence and document representation. Hierarchical\nvariants of BERT have also been explored for docu-\nment classification (Mulyar et al., 2019; Chalkidis\net al., 2022), abstractive summarization (Zhang\net al., 2019), semantic matching (Yang et al., 2020).\nBoth Zhang et al., and Yang et al. also propose\nspecialised pre-training tasks to explicitly capture\nsentence relations within a document. A very re-\ncent work by Park et al. (2022) shows that TrLDC\ndo not perform consistently well across datasets\nthat consist of 700 tokens on average.\nMethods of modifying transformer architecture\nfor long documents can be categorised into two\napproaches: recurrent Transformers and sparse\nattention Transformers. The recurrent approach\nprocesses segments moving from left-to-right (Dai\net al., 2019). To capture bidirectional context,\nDing et al. (2021) propose a retrospective mecha-\nnism in which segments from a document are fed\ntwice as input. Sparse attention Transformers have\nbeen explored to reduce the complexity of self-\nattention, via using dilated sliding window (Child\net al., 2019), and locality-sensitive hashing atten-\ntion (Kitaev et al., 2020). Recently, the combi-\nnation of local (window) and global attention are\n7219\nproposed by Beltagy et al. (2020) and Zaheer et al.\n(2020), which we have detailed in Section 3.\nICD Coding The task of assigning most rele-\nvant ICD codes to a document, e.g., radiology re-\nport (Pestian et al., 2007), death certificate (Koop-\nman et al., 2015) or discharge summary (Johnson\net al., 2016), as a whole, has a long history of\ndevelopment (Farkas and Szarvas, 2008). Most\nexisting methods simplified this task as a text\nclassification problem and built classifiers using\nCNNs (Karimi et al., 2017) or LSTMs (Xie et al.,\n2018). Since the number of unique ICD codes is\nvery large, methods are proposed to exploit relation\nbetween codes based on label co-occurrence (Dong\net al., 2021), label count (Du et al., 2019), knowl-\nedge graph (Xie et al., 2019; Cao et al., 2020; Lu\net al., 2020), code’s textual descriptions (Mullen-\nbach et al., 2018; Rios and Kavuluru, 2018). More\nrecently, Ji et al. (2021a); Gao et al. (2021) inves-\ntigate various methods of applying BERT on ICD\ncoding. Different from our work, they mainly focus\non comparing domain-specific BERT models that\nare pre-trained on various types of corpora. Ji et al.\nshow that PubMedBERT—pre-trained from scratch\non PubMed abstracts—outperforms other variants\npre-trained on clinical notes or health-related posts;\nGao et al. show that BlueBERT—pre-trained on\nPubMed and clinical notes—performs best. How-\never, both report that Transformers-based models\nperform worse than CNN-based ones.\n8 Conclusions\nTransformers have previously been criticised for\nbeing incapable of long document classification. In\nthis paper, we carefully study the role of different\ncomponents of Transformer-based long document\nclassification models. By conducting experiments\non MIMIC-III and other three datasets (i.e., ECtHR,\n20 News and Hyperpartisan), we observe clear im-\nprovements in performance when a model is able\nto process more text. Firstly, Longformer, a sparse\nattention model, which can process up to 4096 to-\nkens, achieves competitive results with CNN-based\nmodels on MIMIC-III; its performance is relatively\nrobust; a moderate size of local attention window\n(e.g., 128) and a small number (e.g., 16) of evenly\nchosen tokens with global attention can improve\nthe efficiency and stability without sacrificing its\neffectiveness. Secondly, hierarchical Transform-\ners outperform all CNN-based models by a large\nmargin; the key design choice is how to split a\ndocument into segments which can be encoded by\npre-trained models; although the best performing\nsegment length is dataset dependent, we find split-\nting a document into small overlapping segments\n(e.g., 128 tokens) is an effective strategy. Taken\ntogether, these experiments rebut the criticisms of\nTransformers for long document classification.\nAcknowledgments\nThis work is funded by the Innovation Fund Den-\nmark under the AI4Xray project. Xiang Dai is\nfunded by CSIRO Precision Health Future Science\nPlatform. Ilias Chalkidis is funded by the Innova-\ntion Fund Denmark under File No. 0175-00011A.\nThis project was also undertaken with the assis-\ntance of resources and services from the National\nComputational Infrastructure (NCI), which is sup-\nported by the Australian Government.\nReferences\nAshutosh Adhikari, Achyudh Ram, Raphael Tang, and\nJimmy Lin. 2019. DocBERT: BERT for Document\nClassification. arXiv, 1904.08398.\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The Long-Document Trans-\nformer. arXiv, 2004.05150.\nPengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Sheng-\nping Liu, and Weifeng Chong. 2020. HyperCore:\nHyperbolic and Co-graph Representation for Auto-\nmatic ICD Coding. In ACL.\nIlias Chalkidis, Manos Fergadiotis, Sotiris Kotitsas, Pro-\ndromos Malakasiotis, Nikolaos Aletras, and Ion An-\ndroutsopoulos. 2020. An Empirical Study on Large-\nScale Multi-Label Text Classification Including Few\nand Zero-Shot Labels. In EMNLP.\nIlias Chalkidis, Abhik Jana, Dirk Hartung, Michael\nJ Bommarito II, Ion Androutsopoulos, Daniel Mar-\ntin Katz, and Nikolaos Aletras. 2022. LexGLUE: A\nBenchmark Dataset for Legal Language Understand-\ning in English. In ACL.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences with\nsparse transformers. arXiv, 1904.10509.\nKrzysztof Marcin Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamás\nSarlós, Peter Hawkins, Jared Quincy Davis, Afroz\nMohiuddin, Lukasz Kaiser, David Benjamin Be-\nlanger, Lucy J Colwell, and Adrian Weller. 2021.\nRethinking Attention with Performers. In ICLR.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive Language Models beyond\na Fixed-Length Context. In ACL.\n7220\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL.\nSiYu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun,\nHao Tian, Hua Wu, and Haifeng Wang. 2021.\nERNIE-Doc: A Retrospective Long-Document Mod-\neling Transformer. In ACL-IJCNLP.\nJesse Dodge, Maarten Sap, Ana Marasovi ´c, William\nAgnew, Gabriel Ilharco, Dirk Groeneveld, Margaret\nMitchell, and Matt Gardner. 2021. Documenting\nLarge Webtext Corpora: A Case Study on the Colos-\nsal Clean Crawled Corpus. In EMNLP.\nHang Dong, Víctor Suárez-Paniagua, William White-\nley, and Honghan Wu. 2021. Explainable automated\ncoding of clinical notes using hierarchical label-wise\nattention networks and label embedding initialisation.\nJBI, 116.\nJingcheng Du, Qingyu Chen, Yifan Peng, Yang Xiang,\nCui Tao, and Zhiyong Lu. 2019. ML-Net: multi-label\nclassification of biomedical texts with deep neural\nnetworks. JAMIA, 26.\nRichárd Farkas and György Szarvas. 2008. Automatic\nconstruction of rule-based ICD-9-CM coding sys-\ntems. BMC Bioinform., 9.\nShang Gao, Mohammed Alawad, M. Todd Young,\nJohn Gounley, Noah Schaefferkoetter, Hong Jun\nYoon, Xiao-Cheng Wu, Eric B. Durbin, Jennifer Do-\nherty, Antoinette Stroup, Linda Coyle, and Georgia\nTourassi. 2021. Limitations of Transformers on Clin-\nical Text Classification. IEEE J. Biomed. Health\nInform., 25.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t Stop Pretraining:\nAdapt Language Models to Domains and Tasks. In\nACL.\nShaoxiong Ji, Matti Hölttä, and Pekka Marttinen. 2021a.\nDoes the Magic of BERT Apply to Medical Code\nAssignment? A Quantitative Study. Comput. Biol.\nMed., 139.\nShaoxiong Ji, Shirui Pan, and Pekka Marttinen. 2021b.\nMedical Code Assignment with Gated Convolution\nand Note-Code Interaction. In Findings of ACL-\nIJCNLP.\nThorsten Joachims. 1997. A Probabilistic Analysis of\nthe Rocchio Algorithm with TFIDF for Text Catego-\nrization. In ICML.\nAlistair E W Johnson, Tom J Pollard, Lu Shen,\nH Lehman Li-Wei, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits,\nLeo Anthony Celi, and Roger G Mark. 2016.\nMIMIC-III, a freely accessible critical care database.\nSci. Data, 3.\nSarvnaz Karimi, Xiang Dai, Hamed Hassanzadeh, and\nAnthony Nguyen. 2017. Automatic Diagnosis Cod-\ning of Radiology Reports: A Comparison of Deep\nLearning and Conventional Classification Methods.\nIn BioNLP@ACL.\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019. SemEval-\n2019 Task 4: Hyperpartisan News Detection. In\nSemEval@NAACL.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efficient transformer. In ICLR.\nBevan Koopman, Sarvnaz Karimi, Anthony Nguyen,\nRhydwyn McGuire, David Muscatello, Madonna\nKemp, Donna Truran, Ming Zhang, and Sarah Thack-\nway. 2015. Automatic classification of diseases from\nfree-text death certificates for real-time surveillance.\nBMC Medical Inform. Decis. Mak., 15.\nRasmus Kær Jørgensen, Mareike Hartmann, Xiang Dai,\nand Desmond Elliott. 2021. mDAPT: Multilingual\nDomain Adaptive Pretraining in a Single Model. In\nFindings of EMNLP.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin\nStoyanov. 2020. Pretrained Language Models\nfor Biomedical and Clinical Tasks: Understand-\ning and Extending the State-of-the-Art. In Clini-\ncalNLP@EMNLP.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv, 1907.11692.\nJueqing Lu, Lan Du, Ming Liu, and Joanna Dipnall.\n2020. Multi-label Few/Zero-shot Learning with\nKnowledge Aggregated from Multiple Label Graphs.\nIn EMNLP.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2021. On the Stability of Fine-tuning\nBERT: Misconceptions, Explanations, and Strong\nBaselines. In ICLR.\nJames Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng\nSun, and Jacob Eisenstein. 2018. Explainable Pre-\ndiction of Medical Codes from Clinical Text. In\nNAACL.\nAndriy Mulyar, Elliot Schumacher, Masoud\nRouhizadeh, and Mark Dredze. 2019. Pheno-\ntyping of Clinical Notes with Improved Document\nClassification Models Using Contextualized Neural\nLanguage Models. arXiv, 1910.13664.\nHyunji Park, Yogarshi Vyas, and Kashif Shah. 2022.\nEfficient Classification of Long Documents Using\nTransformers. In ACL.\nDamian Pascual, Sandro Luck, and Roger Wattenhofer.\n2021. Towards BERT-based Automatic ICD Coding:\nLimitations and Opportunities. In BioNLP@NAACL.\n7221\nJohn Pestian, Chris Brew, Pawel Matykiewicz, Dj J\nHovermale, Neil Johnson, K Bretonnel Cohen, and\nWlodzislaw Duch. 2007. A shared task involving\nmulti-label classification of clinical free text. In\nBioNLP@ACL.\nJiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih,\nSinong Wang, and Jie Tang. 2020. Blockwise Self-\nAttention for Long Document Understanding. In\nFindings of EMNLP.\nAlan Ramponi and Barbara Plank. 2020. Neural Unsu-\npervised Domain Adaptation in NLP—A Survey. In\nCOLING.\nAnthony Rios and Ramakanth Kavuluru. 2018. Few-\nShot and Zero-Shot Multi-Label Learning for Struc-\ntured Label Spaces. In EMNLP.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to Fine-Tune BERT for Text Classifica-\ntion? In CCL.\nDuyu Tang, Bing Qin, and Ting Liu. 2015. Document\nmodeling with gated recurrent neural network for\nsentiment classification. In EMNLP.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efficient Transformers: A Survey.\narXiv, 2009.06732.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nThanh Vu, Dat Quoc Nguyen, and Anthony Nguyen.\n2020. A label attention model for ICD coding from\nclinical text. In IJCAI.\nLin Xiao, Xin Huang, Boli Chen, and Liping Jing. 2019.\nLabel-Specific Document Representation for Multi-\nLabel Text Classification. In EMNLP-IJCNLP.\nPengtao Xie, Haoran Shi, Ming Zhang, and Eric Xing.\n2018. A Neural Architecture for Automated ICD\nCoding. In ACL.\nXiancheng Xie, Yun Xiong, Philip S Yu, and Yangyong\nZhu. 2019. EHR Coding with Multi-scale Feature\nAttention and Structured Knowledge Graph Propaga-\ntion. In CIKM.\nWenhan Xiong, Barlas O ˘guz, Anchit Gupta, Xilun\nChen, Diana Liskovich, Omer Levy, Wen-tau Yih,\nand Yashar Mehdad. 2021. Simple Local Attentions\nRemain Competitive for Long-Context Tasks. arXiv,\n2112.07210.\nLiu Yang, Mingyang Zhang, Cheng Li, Michael Ben-\ndersky, and Marc Najork. 2020. Beyond 512 Tokens:\nSiamese Multi-depth Transformer-based Hierarchi-\ncal Encoder for Long-Form Document Matching. In\nCIKM.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy. 2016. Hierarchical\nAttention Networks for Document Classification. In\nNAACL.\nZheng Yuan, Chuanqi Tan, and Songfang Huang. 2022.\nCode Synonyms Do Matter: Multiple Synonyms\nMatching Network for Automatic ICD Coding. In\nACL.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, and\nLi Yang. 2020. Big Bird: Transformers for Longer\nSequences. In NeurIPS.\nXingxing Zhang, Furu Wei, and Ming Zhou. 2019. HI-\nBERT: Document Level Pre-training of Hierarchical\nBidirectional Transformers for Document Summa-\nrization. In ACL.\n7222\n9 Appendix\n9.1 Limitations\nLong document classification datasets are usually\nannotated using a large number of labels. For ex-\nample, the complete MIMIC-III dataset contains\n8,692 unique labels. As we mentioned in Section\n2, we focus on building document representation\nand leave the challenge of learning with a large\ntarget labelset for future work. Therefore, in this\npaper, we follow previous work (Mullenbach et al.,\n2018; Chalkidis et al., 2022) and consider a subset\nof frequent labels in MIMIC-III and ECtHR.\n9.2 Dataset statistics\nTable 5 shows the descriptive statistics of four\ndatasets we use.\nTrain Dev Test\nMIMIC-III\nDocuments 8,066 1,573 1,729\nUnique labels 50 50 50\nAvg. tokens 2,260 2,693 2,737\nECtHR\nDocuments 8,866 973 986\nUnique labels 10 10 10\nAvg. tokens 2,140 2,345 2,532\nHyperpartisan\nDocuments 516 64 65\nUnique labels 2 2 2\nAvg. tokens 741 707 845\n20 News\nDocuments 10,183 1,131 7,532\nUnique labels 20 20 20\nAvg. tokens 613 627 551\nTable 5: Statistics of the datasets. The number of tokens\nis calculated using RoBERTa tokenizer.\n9.3 Details of task-adaptive pre-training\nHyperparameters and training time for task-\nadaptive pre-training can be found in Table 6.\n9.4 Details of classification experiments\nPreprocessing We mainly follow Mullenbach\net al. (2018) to preprocess the MIMIC-III dataset.\nThat is, we lowercase the text, remove all punctua-\ntion marks and tokenize text by white spaces. The\nonly change we make is that we normalise numeric\n(e.g., convert ‘2021‘ to ‘0000‘) instead of deleting\nLongformer RoBERTa\nMax sequence 4096 128\nBatch size 8 128\nLearning rate 5e-5 5e-5\nTraining epochs 6 15\nTraining time ≈130 ≈40(GPU-hours)\nTable 6: Hyperparameters and training time (measured\non MIMIC-III dataset) for task-adaptive pre-training\nLongformer and RoBERTa. Batch size = batch size per\nGPU ×num. GPUs ×gradient accumulation steps.\nnumeric-only tokens in Mullenbach et al. (2018).\nWe did not apply additional preprocessing to EC-\ntHR and 20 News. We follow Beltagy et al. (2020)\nto preprocess the Hyperpartisan dataset.10\nTraining We fine-tune the multilabel classifica-\ntion model using a binary cross entropy loss. That\nis, given an training example whose ground truth\nand predicted probability for the i-th label are yi\n(0 or 1) and ˆyi, we calculate its loss, over the C\nunique classification labels, as:\nL=\nC∑\ni=1\n−yi log(ˆyi) −(1 −yi) log(1−ˆyi).\nFor the multiclass and binary classification tasks,\nwe fine-tune using the cross entropy loss, where ˆyg\nis the predicted probability for the gold label:\nL= −log(ˆyg),\nWe use the same effective batch size (16), learn-\ning rate ( 2e-5), maximum number of training\nepochs (30) with early stop patience (5) in all ex-\nperiments. We also follow Longformer (Beltagy\net al., 2020) and set the maximum sequence length\nas 4096 in most of the experiments unless other\nspecified. We fine-tune all classification models on\nQuadro RTX 6000 (24 GB GPU memory) or Tesla\nV100 (32 GB GPU memory). If one batch of data\nis too large to fit into the GPU memory, we use\ngradient accumulation so that the effective batch\nsizes (batch size per GPU ×gradient accumulation\nsteps) are still the same.\nWe repeat all experiments five times with dif-\nferent random seeds. The model which is most\neffective on the development set, measured using\nthe micro F1 score (multilabel) or accuracy (multi-\nclass and binary), is used for the final evaluation.\n10https://github.com/allenai/longformer\n7223\n9.5 A comparison between clinical notes and\nlegal cases\nAlthough we usually use the term domain to indi-\ncate that texts talk about a narrow set of related\nconcepts (e.g., clinical concepts or legal concepts),\ntext can vary along different dimensions (Ramponi\nand Plank, 2020).\nIn addition to the statistics difference between\nMIMIC-III and ECtHR, which we show in Table 5,\nthere is another difference worthy considering: clin-\nical notes are private as they contain protected\nhealth information. Even those clinical notes after\nde-identification are usually not publicly available\n(e.g., downloadable using web crawler). In contrast,\nlegal cases have generally been allowed and encour-\naged to share with the public, and thus become a\nlarge portion of crawled pre-training data (Dodge\net al., 2021). Dodge et al. find that legal docu-\nments, especially U.S. case law, are a significant\npart of the C4 corpus, a cleansed version of Com-\nmonCrawl used to pre-train RoBERTa models. The\nECtHR proceedings are also publicly available via\nHUDOC, the court’s database.\nWe suspect task-adaptive pre-training is more\nuseful on MIMIC-III than on ECtHR (Figure 5)\nmay relate to this difference. Therefore, we evalu-\nate the vanilla RoBERTa on MIMIC-III and ECtHR\nregarding tokenization and language modelling. A\ncomparison of the fragmentation ratio using the\ntokenizer and perplexity using the language model\ncan be found in Table 7.\nMIMIC-III ECtHR\nFragmentation ratio 1.233 1.118\nPerplexity 1.351 1.079\nTable 7: Evaluating vanilla RoBERTa on MIMIC-III\nand ECtHR. Lower fragmentation ratio and perplexity\nindicate that the test data have a higher similarity with\nthe RoBERTa pre-training data.\n9.6 A comparison between TAPT and public\navailable RoBERTa by (Lewis et al., 2020)\nWe compare our TAPT-RoBERTa against publicly\navailable domain-specific RoBERTa (Lewis et al.,\n2020), which are trained from scratch on biomedi-\ncal articles and clinical notes, in hierarchical mod-\nels. In these experiments, we split long documents\ninto overlapping segments of 64 tokens. Results in\nFigure 8 show that TAPT-RoBERTa outperforms\ndomain-specific base model, but underperforms the\nlarger model.\n9.7 Results on ECtHR test set\nResults in Table 8 show that our results are higher\nthan the ones reported in (Chalkidis et al., 2022).\nChalkidis et al. compare different BERT variants\nincluding domain-specific models, whereas we use\ntask-adaptive pre-trained models. Regarding hier-\narchical method, we split a document into overlap-\nping segments, each of which has 512 tokens. We\nuse the default setting for Longformer as in Beltagy\net al. (2020).\nMacro F1 Micro F1\nRoBERTa 68.9 77.3\nCaseLaw-BERT 70.3 78.8\nBigBird 70.9 78.8\nDeBERTa 71.0 78.8\nLongformer 71.7 79.4\nBERT 73.4 79.7\nLegal-BERT 74.7 80.4\nLongformer (4096) 76.0 ± 1.4 80.7 ± 0.3\nHierarchical (4096) 76.6 ± 0.7 81.0 ± 0.3\nTable 8: Comparison of our results against the results\nreported in (Chalkidis et al., 2022) on the ECtHR test\nset. Results are sorted by Micro F1.\n9.8 A comparison between evenly splitting\nand splitting based on document structure\nFigure 9 shows that splitting by the paragraph level\ndocument structure does not improve performance\non the ECtHR dataset. On MIMIC-III, splitting\n4096 6144\nMaximum sequence length\n67.5\n68.0\n68.5\n69.0\n69.5\n70.0MicroF1\nTAPT\nBase\nLarge\n67.5\n68.0\n68.5\n69.0\n69.5\n70.0\nMicroF1\nTAPT\nBase\nLarge\nFigure 8: A comparison of task-adaptive pre-trained\nRoBERTa against public available domain-specific\nRoBERTa. Both Base and Large RoBERTa models are\ntrained from scratch on biomedical articles and clinical\nnotes (Lewis et al., 2020).\n7224\nMIMIC-III ECtHR\n60\n65\n70\n75\n80Micro F1\n68.5\n81.2\n62.9\n80.1\n65.4\n80.5\n66.0\n81.3\nEvenly (4096)\nStructured (4096)\nStructured (6144)\nStructured (8192)\nFigure 9: A comparison between evenly splitting and\nsplitting based on document structure.\nbased on document structure substantially under-\nperforms evenly splitting the document.\n9.9 Detailed results on the development sets\nFor the sake of brevity, we use only microF1 score\nin most of our illustrations, and we detail results of\nother metrics in this section.\n7225\nAUC F1\nMax sequence length Macro Micro Macro Micro P@5\n512 81.4 ± 0.1 85.1 ± 0.2 39.2 ± 0.9 52.2 ± 0.3 53.3 ± 0.3\n1024 83.6 ± 0.2 87.3 ± 0.3 43.2 ± 0.6 56.3 ± 0.5 56.5 ± 0.2\n2048 86.5 ± 0.2 89.8 ± 0.1 48.2 ± 1.1 60.5 ± 0.4 59.4 ± 0.3\n4096 88.4 ± 0.1 91.5 ± 0.1 53.1 ± 0.5 64.0 ± 0.3 62.0 ± 0.4\nTable 9: Detailed results of Figure 1: the effectiveness of Longformer on the MIMIC-III development set.\nAUC F1\nMacro Micro Macro Micro P@5\nLongformer on MIMIC-III\nVanilla 88.4 ± 0.1 91.5 ± 0.1 53.1 ± 0.5 64.0 ± 0.3 62.0 ± 0.4\nTAPT 90.3 ± 0.2 92.7 ± 0.1 60.8 ± 0.4 68.5 ± 0.3 64.8 ± 0.3\nRoBERTa onMIMIC-III\nVanilla 81.6 ± 0.2 85.0 ± 0.3 43.2 ± 1.7 53.9 ± 0.4 54.0 ± 0.2\nTAPT 82.3 ± 0.4 85.5 ± 0.3 48.8 ± 0.4 56.7 ± 0.2 55.3 ± 0.2\nLongformer on ECtHR\nVanilla — — 77.4 ± 2.3 81.3 ± 0.3 —\nTAPT — — 78.5 ± 2.2 82.1 ± 0.6 —\nRoBERTa onECtHR\nVanilla — — 72.2 ± 1.5 74.8 ± 0.4 —\nTAPT — — 72.7 ± 0.7 75.1 ± 0.4 —\nTable 10: Detailed results of Figure 5: the impact of task-adaptive pre-training. Note that we use maximum sequence\nlength 512 for RoBERTa and4096 for Longformer in these experiments.\n7226\nAUC F1\nSize Macro Micro Macro Micro P@5 Accuracy\nMIMIC-III\n32 89.8 ± 0.1 92.3 ± 0.1 59.6 ± 0.6 67.9 ± 0.3 64.2 ± 0.3 —\n64 90.0 ± 0.1 92.5 ± 0.1 60.3 ± 0.3 68.1 ± 0.1 64.5 ± 0.1 —\n128 90.1 ± 0.1 92.6 ± 0.1 60.5 ± 0.7 68.3 ± 0.3 64.7 ± 0.3 —\n256 90.2 ± 0.0 92.6 ± 0.1 60.7 ± 0.6 68.4 ± 0.3 64.6 ± 0.2 —\n512 90.3 ± 0.2 92.7 ± 0.1 60.8 ± 0.4 68.5 ± 0.3 64.8 ± 0.3 —\nECtHR\n32 — — 78.2 ± 1.2 81.2 ± 0.3 — —\n64 — — 78.6 ± 1.7 81.4 ± 0.1 — —\n128 — — 79.9 ± 1.6 82.1 ± 0.5 — —\n256 — — 78.5 ± 2.1 81.8 ± 0.4 — —\n512 — — 78.5 ± 2.2 82.1 ± 0.6 — —\nHyperpartisan\n32 — — – – — 83.9 ± 0.7\n64 — — – – — 83.3 ± 1.9\n128 — — – – — 83.9 ± 0.7\n256 — — – – — 88.0 ± 0.7\n512 — — – – — 85.9 ± 2.2\n20 News\n32 — — – – — 92.8 ± 0.6\n64 — — – – — 94.0 ± 0.5\n128 — — – – — 93.8 ± 0.3\n256 — — – – — 93.5 ± 0.1\n512 — — – – — 94.0 ± 0.1\nTable 11: The impact of local attention window size in Longformer, measured on the development sets.\nAUC F1\n# tokens Macro Micro Macro Micro P@5\nMIMIC-III\n1 90.1 ± 0.2 92.6 ± 0.1 60.5 ± 0.9 68.2 ± 0.3 64.7 ± 0.3\n8 90.0 ± 0.1 92.5 ± 0.1 60.5 ± 0.7 68.2 ± 0.3 64.6 ± 0.2\n16 90.0 ± 0.2 92.5 ± 0.1 60.0 ± 0.2 68.1 ± 0.2 64.3 ± 0.3\n32 90.0 ± 0.2 92.4 ± 0.1 60.1 ± 0.5 67.9 ± 0.1 64.4 ± 0.2\n64 89.9 ± 0.2 92.4 ± 0.1 59.9 ± 1.0 67.9 ± 0.4 64.4 ± 0.3\nECtHR\n1 — — 78.5 ± 1.8 80.8 ± 0.4 —\n8 — — 77.2 ± 2.0 80.8 ± 0.4 —\n16 — — 77.7 ± 0.4 80.7 ± 0.3 —\n32 — — 78.2 ± 1.4 80.6 ± 0.4 —\n64 — — 77.7 ± 2.3 80.7 ± 0.5 —\nTable 12: Detailed results of Figure 6: the effect of applying global attention on more tokens, which are evenly\nchosen based on their positions.\n7227\nAUC F1\n# tokens Macro Micro Macro Micro P@5\nMIMIC-III\n1 90.1 ± 0.2 92.6 ± 0.1 60.5 ± 0.9 68.2 ± 0.3 64.7 ± 0.3\n8 89.7 ± 0.2 92.0 ± 0.1 61.0 ± 1.3 66.9 ± 0.4 64.0 ± 0.4\n16 89.4 ± 0.2 91.9 ± 0.1 60.1 ± 1.2 66.5 ± 0.3 63.9 ± 0.5\n32 89.4 ± 0.4 91.9 ± 0.2 60.3 ± 1.6 66.4 ± 0.6 63.7 ± 0.7\n64 89.1 ± 0.4 91.7 ± 0.2 59.4 ± 2.0 66.2 ± 0.7 63.4 ± 0.7\nECtHR\n1 — — 78.5 ± 1.8 80.8 ± 0.4 —\n8 — — 79.2 ± 0.3 80.9 ± 0.2 —\n16 — — 77.6 ± 1.2 80.4 ± 0.4 —\n32 — — 77.1 ± 0.7 80.0 ± 0.2 —\n64 — — 76.6 ± 1.1 79.9 ± 0.5 —\nTable 13: The effect of applying global attention on more informative tokens, which are identified based on TF-IDF.\n7228\nAUC F1\nSize Macro Micro Macro Micro P@5 Accuracy\nDisjoint segments on MIMIC-III\n64 89.4 ± 0.1 92.0 ± 0.1 60.8 ± 1.1 67.9 ± 0.3 63.5 ± 0.3 —\n128 89.5 ± 0.1 92.1 ± 0.1 61.2 ± 0.6 68.0 ± 0.3 63.5 ± 0.3 —\n256 89.6 ± 0.1 92.1 ± 0.1 61.0 ± 0.4 67.6 ± 0.2 63.6 ± 0.2 —\n512 89.2 ± 0.2 91.8 ± 0.2 59.4 ± 0.5 66.7 ± 0.3 63.4 ± 0.4 —\nOverlapping segments on MIMIC-III\n64 89.7 ± 0.1 92.3 ± 0.1 62.3 ± 0.2 68.7 ± 0.1 64.1 ± 0.1 —\n128 89.7 ± 0.2 92.3 ± 0.1 61.8 ± 0.9 68.5 ± 0.3 64.0 ± 0.2 —\n256 89.5 ± 0.1 92.1 ± 0.1 61.4 ± 0.3 68.1 ± 0.2 63.8 ± 0.1 —\n512 89.4 ± 0.1 92.0 ± 0.0 60.3 ± 0.3 67.2 ± 0.2 63.6 ± 0.3 —\nDisjoint segments on ECtHR\n64 — — 76.6 ± 1.2 79.7 ± 0.2 — —\n128 — — 77.6 ± 2.3 80.8 ± 0.4 — —\n256 — — 77.7 ± 1.4 81.2 ± 0.4 — —\n512 — — 78.3 ± 1.3 81.7 ± 0.3 — —\nOverlapping segments on ECtHR\n64 — — 76.9 ± 1.7 80.5 ± 0.5 — —\n128 — — 77.5 ± 1.7 81.2 ± 0.5 — —\n256 — — 78.1 ± 1.4 81.5 ± 0.2 — —\n512 — — 78.4 ± 1.5 81.4 ± 0.4 — —\nDisjoint segments on Hyperpartisan\n64 — — — — — 88.8 ± 1.8\n128 — — — — — 89.1 ± 1.4\n256 — — — — — 87.8 ± 1.8\n512 — — — — — 86.2 ± 1.8\nOverlapping segments on Hyperpartisan\n64 — — — — — 87.5 ± 1.4\n128 — — — — — 88.4 ± 1.2\n256 — — — — — 88.1 ± 2.1\n512 — — — — — 88.4 ± 0.8\nDisjoint segments on 20 News\n64 — — — — — 93.3 ± 0.2\n128 — — — — — 93.5 ± 0.3\n256 — — — — — 94.4 ± 0.4\n512 — — — — — 94.0 ± 0.3\nOverlapping segments on 20 News\n64 — — — — — 93.8 ± 0.4\n128 — — — — — 93.4 ± 0.3\n256 — — — — — 94.5 ± 0.2\n512 — — — — — 93.9 ± 0.3\nTable 14: The effect of varying the segment length and whether allowing segments to overlap in the hierarchical\ntransformers.\n7229\nAUC F1\nMacro Micro Macro Micro P@5\nMIMIC-III\nE (4096) 89.7 ± 0.2 92.3 ± 0.1 61.8 ± 0.9 68.5 ± 0.3 64.0 ± 0.2\nS (4096) 87.2 ± 0.2 90.1 ± 0.2 55.2 ± 0.4 62.9 ± 0.2 59.9 ± 0.2\nS (6144) 88.2 ± 0.2 91.0 ± 0.2 57.8 ± 0.3 65.4 ± 0.3 61.7 ± 0.3\nS (8192) 88.5 ± 0.3 91.2 ± 0.2 58.8 ± 0.2 66.0 ± 0.4 62.4 ± 0.1\nECtHR\nE (4096) — — 77.5 ± 1.7 81.2 ± 0.5 —\nS (4096) — — 75.3 ± 1.3 80.1 ± 0.4 —\nS (6144) — — 77.1 ± 1.8 80.5 ± 0.5 —\nS (8192) — — 77.7 ± 1.9 81.3 ± 0.5 —\nTable 15: Detailed results of Figure 9: a comparison between evenly splitting and splitting based on document\nstructure. E: evenly splitting; S: splitting based on document structure.\nAUC F1\nMacro Micro Macro Micro P@5\nMIMIC-III\nLongformer 90.0 ± 0.2 92.5 ± 0.1 60.0 ± 0.2 68.1 ± 0.2 64.3 ± 0.3\n+ LW AN 90.5 ± 0.2 92.9 ± 0.2 62.2 ± 0.7 69.2 ± 0.3 65.1 ± 0.1\nHierarchical 89.7 ± 0.2 92.3 ± 0.1 61.8 ± 0.9 68.5 ± 0.3 64.0 ± 0.2\n+ LW AN 91.4 ± 0.1 93.7 ± 0.1 64.2 ± 0.4 70.3 ± 0.1 65.3 ± 0.1\nECtHR\nLongformer — — 77.7 ± 0.4 80.7 ± 0.3 —\n+ LW AN — — 79.5 ± 0.8 81.1 ± 0.3 —\nHierarchical — — 77.5 ± 1.7 81.2 ± 0.5 —\n+ LW AN — — 79.7 ± 0.9 81.3 ± 0.3 —\nTable 16: The effect of label-wise attention network.\n7230",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7906397581100464
    },
    {
      "name": "Paragraph",
      "score": 0.7590581774711609
    },
    {
      "name": "Computer science",
      "score": 0.7349923849105835
    },
    {
      "name": "ENCODE",
      "score": 0.5323646068572998
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4863331913948059
    },
    {
      "name": "Document classification",
      "score": 0.47074657678604126
    },
    {
      "name": "Machine learning",
      "score": 0.3715834617614746
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3714500665664673
    },
    {
      "name": "Natural language processing",
      "score": 0.362455815076828
    },
    {
      "name": "Data mining",
      "score": 0.3370445668697357
    },
    {
      "name": "Voltage",
      "score": 0.10700440406799316
    },
    {
      "name": "Engineering",
      "score": 0.10102075338363647
    },
    {
      "name": "World Wide Web",
      "score": 0.09944391250610352
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1292875679",
      "name": "Commonwealth Scientific and Industrial Research Organisation",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I42894916",
      "name": "Data61",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I124055696",
      "name": "University of Copenhagen",
      "country": "DK"
    },
    {
      "id": "https://openalex.org/I139025015",
      "name": "Pioneer (United States)",
      "country": "US"
    }
  ],
  "cited_by": 52
}