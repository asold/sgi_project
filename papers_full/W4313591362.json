{
  "title": "A CNN-Transformer Hybrid Model Based on CSWin Transformer for UAV Image Object Detection",
  "url": "https://openalex.org/W4313591362",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2890730315",
      "name": "Wanjie Lu",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2466238174",
      "name": "Chaozhen Lan",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2528089419",
      "name": "Chaoyang Niu",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A1973321923",
      "name": "Wei Liu",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2477389580",
      "name": "Liang Lyu",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2344773575",
      "name": "Qunshan Shi",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2206240444",
      "name": "Shiju Wang",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6810913163",
    "https://openalex.org/W2992240579",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W3107867277",
    "https://openalex.org/W3176331203",
    "https://openalex.org/W6947681574",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3128592650",
    "https://openalex.org/W3159705445",
    "https://openalex.org/W6756444276",
    "https://openalex.org/W3006025044",
    "https://openalex.org/W4200338404",
    "https://openalex.org/W4205365435",
    "https://openalex.org/W3116963012",
    "https://openalex.org/W2989611864",
    "https://openalex.org/W3118837087",
    "https://openalex.org/W2982164728",
    "https://openalex.org/W2109255472",
    "https://openalex.org/W2963849369",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2570343428",
    "https://openalex.org/W6750227808",
    "https://openalex.org/W6777046832",
    "https://openalex.org/W6772261133",
    "https://openalex.org/W3210997334",
    "https://openalex.org/W2800388963",
    "https://openalex.org/W3033917167",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2926837263",
    "https://openalex.org/W3112264198",
    "https://openalex.org/W4205932092",
    "https://openalex.org/W2913087080",
    "https://openalex.org/W3088979451",
    "https://openalex.org/W2125215748",
    "https://openalex.org/W2924873663",
    "https://openalex.org/W2993756598",
    "https://openalex.org/W2996735448",
    "https://openalex.org/W3168844623",
    "https://openalex.org/W3016641475",
    "https://openalex.org/W3047731328",
    "https://openalex.org/W6768884085",
    "https://openalex.org/W3119129845",
    "https://openalex.org/W2795812085",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W4205138939",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W4214627427",
    "https://openalex.org/W4312312588",
    "https://openalex.org/W6811230874",
    "https://openalex.org/W6809665764",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W4214520160",
    "https://openalex.org/W6810308439",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W6810521590",
    "https://openalex.org/W3216720075",
    "https://openalex.org/W3217153199",
    "https://openalex.org/W3210586215",
    "https://openalex.org/W4206952097",
    "https://openalex.org/W4293238236",
    "https://openalex.org/W4213200979",
    "https://openalex.org/W4213004929",
    "https://openalex.org/W1508483467",
    "https://openalex.org/W4307411363",
    "https://openalex.org/W3210997132",
    "https://openalex.org/W3211904225",
    "https://openalex.org/W2962777203",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W4312443924",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4293584584",
    "https://openalex.org/W2998802733",
    "https://openalex.org/W4226345037",
    "https://openalex.org/W4225493839",
    "https://openalex.org/W3018757597",
    "https://openalex.org/W4221146106"
  ],
  "abstract": "The object detection of unmanned aerial vehicle (UAV) images has widespread applications in numerous fields; however, the complex background, diverse scales, and uneven distribution of objects in UAV images make object detection a challenging task. This study proposes a convolution neural network transformer hybrid model to achieve efficient object detection in UAV images, which has three advantages that contribute to improving object detection performance. First, the efficient and effective cross-shaped window (CSWin) transformer can be used as a backbone to obtain image features at different levels, and the obtained features can be input into the feature pyramid network to achieve multiscale representation, which will contribute to multiscale object detection. Second, a hybrid patch embedding module is constructed to extract and utilize low-level information such as the edges and corners of the image. Finally, a slicing-based inference method is constructed to fuse the inference results of the original image and sliced images, which will improve the small object detection accuracy without modifying the original network. Experimental results on public datasets illustrate that the proposed method can improve performance more effectively than several popular and state-of-the-art object detection methods.",
  "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023 1211\nA CNN-Transformer Hybrid Model Based on CSWin\nTransformer for UA V Image Object Detection\nWanjie Lu , Chaozhen Lan ,C h a o y a n gN i u, Wei Liu, Liang Lyu, Qunshan Shi , and Shiju Wang\nAbstract—The object detection of unmanned aerial vehicle\n(UA V) images has widespread applications in numerous ﬁelds;\nhowever, the complex background, diverse scales, and uneven\ndistribution of objects in UA V images make object detection a\nchallenging task. This study proposes a convolution neural network\ntransformer hybrid model to achieve efﬁcient object detection\nin UA V images, which has three advantages that contribute to\nimproving object detection performance. First, the efﬁcient and\neffective cross-shaped window (CSWin) transformer can be used\nas a backbone to obtain image features at different levels, and\nthe obtained features can be input into the feature pyramid net-\nwork to achieve multiscale representation, which will contribute\nto multiscale object detection. Second, a hybrid patch embedding\nmodule is constructed to extract and utilize low-level information\nsuch as the edges and corners of the image. Finally, a slicing-based\ninference method is constructed to fuse the inference results of the\noriginal image and sliced images, which will improve the small\nobject detection accuracy without modifying the original network.\nExperimental results on public datasets illustrate that the proposed\nmethod can improve performance more effectively than several\npopular and state-of-the-art object detection methods.\nIndex Terms—Convolutional neural network (CNN), hybrid\nnetwork, object detection, transformer, unmanned aerial vehicle\n(UA V) image.\nI. INTRODUCTION\nW\nITH the development of remote sensing technologies,\nunmanned aerial vehicles (UA Vs) have been widely em-\nployed in various ﬁelds, such as digital cities, smart agriculture\nand forestry, and disaster inspection. As one of the key technolo-\ngies to realize the application of UA V images, object detection\nbased on UA V images has been widely employed in military\nand civilian areas. Traditional object detection methods, such as\nsupport vector machines and AdaBoost, have problems, such\nas cumbersome manual feature design, poor robustness, and\nManuscript received 26 September 2022; revised 1 December 2022; accepted\n1 January 2023. Date of publication 4 January 2023; date of current version 16\nJanuary 2023. This work was supported by the National Natural Science Foun-\ndation of China under Grant 42201472, Grant 41901378, and Grant 42001338.\n(Corresponding author: Chaozhen Lan.)\nWanjie Lu, Chaoyang Niu, Wei Liu, and Shiju Wang are with the\nInstitute of Data and Target Engineering, PLA Strategic Support Force\nInformation Engineering University, Zhengzhou 450001, China (e-mail:\nlwj285149763@163.com; niucy2017@outlook.com; greatliuliu@163.com;\n13733150660@139.com).\nChaozhen Lan, Liang Lyu, and Qunshan Shi are with the Institute of\nGeospatial Information, PLA Strategic Support Force Information Engi-\nneering University, Zhengzhou 450001, China (e-mail: lan_cz@163.com;\nlvliangvip@163.com; hills1@163.com).\nDigital Object Identiﬁer 10.1109/JSTARS.2023.3234161\ncomputational redundancy, which cannot meet the current needs\nof UA V image object detection tasks. Driven by breakthroughs\nin computer vision (CV) and deep learning in recent years, UA V\nimage object detection methods based on deep learning, such\nas convolutional neural networks (CNN) and transformers, are\nbecoming a ﬁeld of rapid development and intense research[1],\n[2].\nCNN-based object detection methods have excellent abilities\nof adaptive learning and feature extraction and have superior\ndetection performance to the traditional object detection meth-\nods [3], [4]. For example, two-stage detection methods, such\nas R-CNN [5],F a s tR - C N N[6],F a s t e rR - C N N[7],M a s k\nR-CNN [8], Cascade R-CNN[9], and Dynamic R-CNN[10],\ncan classify the object regions by extracting features of several\ncandidate regions, and then obtain the object categories and\npositions; one-stage detection methods, such as you only look\nonce (YOLO) series[11], single-shot multibox detector (SSD)\n[12], RetinaNet [13], and fully convolutional one-stage object\ndetection (FCOS)[14], can achieve end-to-end object location\nand category prediction directly through the initial anchor box\nwithout region proposals. However, CNN-based object detection\nmethods mainly use large-scale proposals, anchors, or window\ncenters for category predictions. Furthermore, repeated predic-\ntion boxes, anchor box design, and assignment between objects\nand anchor boxes seriously affect the model’s performance\nin postprocessing, and global features, such as long-distance\ndependencies in the processing, cannot be effectively obtained.\nWith the development of the attention mechanism, CV re-\nsearchers have gradually attached importance to transformer-\nbased object detection methods[15], which have achieved com-\npetitive performance in multiple CV tasks. A transformer is an\nencoder-decoder sequence transformation model that enables\nlong-range interactions between different encoded elements\nin a sequence using the self-attention mechanism. With self-\nattention, long-distance dependency modeling capabilities can\nbe achieved to enable data processing in various downstream\ntasks. Given these advantages, the transformer, which was orig-\ninally mainly used in natural language processing, has been\npromptly introduced into CV tasks, remote sensing, and related\nﬁelds. For example, by applying the encoder in the transformer\ndirectly to the image patches, Dosovitskiy et al.[16] proposed a\nconvolution-free method called vision transformer (ViT), which\nimproves the object detection performance without major modi-\nﬁcations to the original model architecture. Bazi et al.[17] built a\nremote sensing image classiﬁcation method based on ViT, which\nexhibits superior performance on four public datasets.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n1212 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nOne key factor for the success of transformer-based meth-\nods lies in large-scale training datasets and high-performance\ncomputing resources. However, in conditions of small data\nvolume or insufﬁcient training time, CNN-based models with\nthe same number of parameters can achieve better performance,\nwhereas transformer-based models require enough training data\nand time to achieve similar performance. The main reason is\nthat CNN-based models have inductive biases properties, such\nas locality and translation equivariance[18], which limit the\napplication of transformer-based models in areas with limited\ndata or computing resources. In addition, the perception ﬁelds of\nthe CNN and transformer-based models are considerably differ-\nent. Based on the advantages of the self-attention mechanism,\ntransformer-based models can perform better in capturing the\nrelationships among distant pixels in images. However, because\nof the lack of capturing the internal spatial information of image\npatches, the local information, such as texture and corner, is lost\nin transformer-based models, whereas CNN-based models have\nadvantages in capturing relevant information[19].\nTo make up for the deﬁciencies of CNN-based and\ntransformer-based models, CNN-transformer hybrid methods\n[20], [21], [22] are used to effectively integrate CNN and trans-\nformer to improve the overall performance. However, for UA V\nimage object detection, effectively fusing the local and global\nfeatures in the CNNs and transformers requires further research;\nin addition, the CNN-transformer hybrid models still need rel-\natively large data and a long time, otherwise good performance\ncannot be achieved.\nMeanwhile, because of the characteristics of UA V images,\ndeep learning models face challenges in object detection. The\ncandidate objects in UA V images have the characteristics of\ndifferent luminance, complex backgrounds, scale diversity, etc.\nFor example, the diversity in camera angles leads to various\ncharacteristics of the same category[23]; the objects in the UA V\nimages under the large ﬁeld of view show an uneven distribution,\nsuch as a dense aggregation of objects in the city center[24],\nwhereas various objects in the suburbs are sparsely distributed\n[25]. In addition, natural factors, such as clouds, rain, fog, and\nsnow, can lead to the failure of object detection in UA V images\n[26]. The above factors make it difﬁcult for the current image\nobject detection methods in general scenarios to achieve ideal\nperformance for UA V images.\nConsidering the pros and cons of CNNs and transformers,\nand according to the characteristics of UA V images and object\ndetection requirements, a hybrid object detection method for\nUA V images that combines CNNs and transformers is proposed.\nIn summary, the main contributions are as follows.\n1) A method of using an efﬁcient transformer, cross-shaped\nwindow (CSWin) transformer, as the backbone of the\nMask R-CNN is proposed to effectively achieve multiscale\nUA V image object detection. The CSWin Transformer is\nused to establish the dependence of long-distance features\nin the input image and obtain features at different levels.\nCombined with the feature pyramid network (FPN), the\nmultiscale representations of the obtained features at dif-\nferent levels are realized to meet the need for effective\ndetection of multiscale objects in images.\n2) A hybrid patch embedding module (HPEM) to process the\ninput image is constructed. Using convolution to process\nthe input image into low-dimensional features, and then\ngenerating sequence token embeddings, low-level infor-\nmation, such as edges and corners, can be extracted and\nutilized without hardly increasing the number of parame-\nters in the model.\n3) Given the high-resolution characteristics of UA V images, a\nslicing-based inference (SI) method is constructed. While\nusing the trained model to infer the original input UA V\nimage, the input image is sliced, and it is inferred using\nthe trained model after ampliﬁcation and enhancement.\nThe inference results based on slicing are fused with the\noriginal image inference results to realize further detection\nof small objects in the input image, which can improve the\nobject detection accuracy without modifying the original\nmodel.\nThe remainder of this article is organized as follows. Sec-\ntion II describes the related works and SectionIII describes the\nproposed method in detail. The experiments and results of the\nproposed and compared methods are presented in SectionIV.\nFinally, in SectionV, we discuss and conclude this study.\nII. RELATED WORKS\nThis section will discuss related work from three aspects:\nCNN-based object detection for UA V images, ViT, and CNN-\ntransformer hybrid methods.\nA. CNN-Based Object Detection\nCNN-based object detection models can be roughly divided\ninto two categories: two-stage detectors and one-stage detectors.\n1) Two-Stage Detectors:Based on the powerful feature repre-\nsentation ability of CNN, R-CNN[5], a typical two-stage\ndetector, was proposed and has considerably improved\nthe performance of object detection. Based on R-CNN,\nFast R-CNN [6] was proposed by combining the spatial\npyramid pooling network (SPP-Net)[27] and using region\nof interest (ROI) pooling. Ren et al.[7] proposed Faster\nR-CNN, which used the region proposal network (RPN)\ninstead of the selective search algorithm to realize sharing\nof convolutional features, thereby further improving the\ndetection speed. By adding a branch for predicting an ob-\nject mask in parallel with the existing branch for bounding\nbox recognition, He et al.[8] proposed Mask R-CNN, in\nwhich a small overhead is added to Faster R-CNN. In ad-\ndition, two-stage detectors also include Cascade R-CNN\n[9],H T C[28], and Dynamic R-CNN[10].\n2) One-Stage Detector:Redmon et al.[29] proposed YOLO,\nwhich just used a single neural network to complete the\nprocess from image input to output of object location\nand category information. Based on YOLO, a series of\nimproved algorithms, such as YOLO9000[30], YOLOv3\n[31], and YOLOv4[32], have been proposed. Liu et al.\n[12] proposed SSD to extract multiscale features. To solve\nthe problem of object detection in a per-pixel prediction\nfashion, analog to semantic segmentation, FCOS [14],\nLU et al.: CNN-TRANSFORMER HYBRID MODEL BASED ON CSWIN TRANSFORMER 1213\na fully convolutional and anchor box-free one-stage ob-\nject detector, was proposed, which completely avoids the\ncomplicated computation related to anchor boxes such as\ncalculating overlapping during training. Lin et al.[13]\ndiscovered that the extreme foreground-background class\nimbalance encountered during the training of dense detec-\ntors affected the performance of one-stage detectors and\nproposed RetinaNet using focal loss to prevent the vast\nnumber of easy negatives from overwhelming the detector\nduring training.\nBoth one and two-stage detectors can be applied to object\ndetection tasks. However, due to the characteristics of UA V im-\nages, the above algorithms cannot fully exploit the performance\nof UA V image object detection. For UA V image object detection,\nmultiscale detection is a frequent and common scenario, which is\ncharacterized by the simultaneous existence of multiscale object\ninstances. For example, in the Visdrone2021-DET dataset, the\nsize of the persons occupies less than 6% of the image, whereas\nsome vehicles can occupy more than 20% of the image[33].\nCurrently, the most common method to achieve multiscale\ndetection involves constructing multiscale feature maps[34] and\nobtaining the output results by carrying out multilayer ﬁltering\n[35], [36].I n [37], the FPN used a top-down feature fusion\nmethod to achieve multiscale object detection by fusing the\nlow-level features with more details and the top-level features\nwith rich semantic information. Through research focused on\nmultiscale object proposal networks, multiscale object detection\nwas achieved by generating candidate regions with different\nintermediate layer features[35]. Zhang et al.[38] proposed a\ndual multiscale FPN framework and studied multiple training\nand inference strategies of multiscale object detection. Using\na multiscale information preservation module, Han et al.[39]\nconstructed multiscale pyramid images and features for each\nimage to retain as much multiscale information of the input\ndata as possible, which is helpful to achieve better performance\nof object detection. To adaptively combine multiscale feature\ninformation on different channels and spatial positions, FPN\nwas used to obtain more discriminative features and achieve\nan efﬁcient fusion of multiscale features[40]. In addition, the\ndilated/deformable convolution kernel[41], [42] was used to ex-\npand the receptive ﬁeld of algorithms without loss of resolution\nto achieve multiscale object detection.\nOtherwise, for detecting small and dense objects in UA V\nimages, better performance can be achieved by improving fea-\nture maps of small objects, incorporating context information\nof small objects, and using data enhancement methods[43],\n[44]. Based on the above ideas, researchers have constructed a\nseries of small object detection networks including RRNet[45],\nFSSSD [44], Cascade network[46], HRDNet[47],U A V - Y O L O\n[48], MPFPN[49], and GANet[50]. To solve the problems of\nsmall instances, complex backgrounds, and difﬁcult feature ex-\ntractions in UA V images, because FPN can effectively integrate\nmultiscale features to achieve small object detection and context\ninformation can provide more powerful information, DBNet\n[51] and SINet[52] obtained better small object detection results\nby providing global contextual information.\nFig. 1. Standard architecture of the transformer bock.\nB. Transformer-Based Object Detection\nNowadays, transformer-based models, such as DETR[53]\nand ViT[16], have been widely used in CV tasks. The standard\ntransformer block generally includes a multihead self-attention,\na multilayer perceptron, and multiple layer normalizations[54],\nas shown in Fig.1.\nTo simplify the work pipelines of current object detectors,\nCarion et al.[53] optimized the training process by transforming\nthe object detection task into a direct set prediction problem\nand constructed end-to-end object detection with transformers\n(DETR), which models the interactions among different ele-\nments in a sequence using the encoder-decoder structure in the\ntransformer. Subsequently, researchers carried out optimizations\nand improvements around DETR and proposed models such as\nDeformable DETR [55], Conditional DETR [56], DN-DETR\n[57], DAB-DETR [58], and DETR with improved denoising\nanchor boxes (DINO)[59].\nSimilar to CNN-based models, ViT[16], a two-branch trans-\nformer structure, was designed to learn features at different\nscales and demonstrated that multiscale feature representation is\neffective. To solve the problems of requiring a large amount of\ndata and high-performance computing for training in ViT, DeiT\n[60], a data-efﬁcient transformer-based model that is similar to\nViT, was proposed. However, different from ViT, DeiT mainly\nachieves efﬁcient training and better results on small datasets\n(e.g., ImageNet1K) through the self-attention mechanism and\nthrough the knowledge distillation method. Bashmal et al.[61]\nproposed a multilabel classiﬁcation method based on a data-\nefﬁcient transformer, which achieved the multilabel efﬁcient\nclassiﬁcation of high-resolution UA V remote sensing images.\nRanftl et al. [62] constructed a dense prediction transformer\nto perform dense object predictions by upsampling the low-\nresolution images to obtain high-resolution images. Based on\nthe design philosophy of depthwise separable convolution, sep-\narable vision transformer (SepViT)[63] was designed to realize\nthe information interaction within and among windows through\ndepthwise separable attention, which effectively improves the\ncomputational efﬁciency of ViT.\nHowever, the transformer has inherent defects. During the\ntraining process, the transformer produces a quadratic computa-\ntional complexity related to the image resolution or the number\nof tokens, resulting in a huge amount of attention calculation\nwhen dealing with long sequence tokens. Therefore, computa-\ntional complexity is a key consideration when the transformer is\napplied to the ﬁeld of object detection. To effectively improve the\n1214 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\ncomputational performance of the transformer, a typical solution\ninvolves reducing the scope of the attention mechanism from\nglobal to local or a certain window. Swin Transformer[64] is\na hierarchical transformer whose representation was computed\nwith shifted windows. Swin Transformer divides the image\ninto nonoverlapping local windows and uses the shift window\nmechanism to limit the self-attention computation to the seg-\nmented local windows, which effectively improves computing\nefﬁciency. However, the receptive ﬁeld in Swin Transformer\nexpands slowly and requires numerous computational blocks\nto ﬁnally obtain global attention. To solve the problem of the\nlimited token interaction ﬁeld in the local self-attention, and\neffectively reduce the amount of computation while obtaining a\nwide range of attention simultaneously, the CSWin Transformer\n[65], which achieved good performance on common CV tasks\nby computing self-attention in the horizontal and vertical stripes\nin parallel that form a CSWin, was proposed. Beyond those,\nPVT [66] was proposed to build an attention layer with linear\ncomplexity to achieve improved computational efﬁciency using\ndown-sampled keys and values, and PVT v2[67] was presented\nby improving PVT by adding a linear complexity attention\nlayer, overlapping patch embedding, and using a convolutional\nfeed-forward network. Different from the above methods, Tang\net al.[68] designed an efﬁcient visual transformer using quadtree\nattention to divide the input images into multiple patches and\nevaluate the attention score to achieve quadratic complexity\nreduction, which could reduce the quadratic computational com-\nplexity to linear complexity and simultaneously obtain better-\ndetailed information and long-distance dependencies to achieve\nbetter results in CV tasks.\nAlthough a series of transformer-based methods can achieve\ngood performance by combining the hierarchical structure and\ncan be used as a general backbone in CV tasks, such as object\ndetection [69], [70], instance segmentation[72], and other ﬁelds,\ntheir actual performance varies considerably. Among them,\nCSWin Transformer has a relatively simple self-attention mech-\nanism, which makes CSWin Transformer much more effective\nfor general vision tasks compared with the Swin Transformer,\nPVT, and other methods. Therefore, the CSWin Transformer\nwas used as the backbone of this study.\nC. CNN-Transformer Hybrid Methods for Object Detection\nBy constructing CNN-transformer hybrid models, the advan-\ntages of CNNs and transformers can be effectively integrated.\nZheng et al.[68] proposed an adaptive and dynamic one-stage\ndetector based on the feature-pyramid transformer, which en-\nhanced the feature fusion ability of the model by embedding\na transformer in the FPN. Xu et al.[70] constructed a local-\nperception backbone based on Swin Transformer to enhance the\nlocal-perception capability and improve the detection accuracy\nfor small objects. By replacing the original prediction heads\nwith transformer prediction heads (TPH), TPH-YOLOv5[71]\nachieved good performance with impressive interpretability on\ndrone-captured scenarios. Feng et al.[73] used YOLO as the\nbaseline network and used a cross-stage partial (CSP) bottleneck\ntransformer module as the backbone to implement the transi-\ntivity of the global spatial dependencies, which exhibit superior\nperformance in different application environments and have high\ngenerality. Inspired by DETR, Li et al.[74] constructed a novel\ntransformer-based remote sensing object detection framework,\nTRD, in which a modiﬁed transformer was designed to aggregate\nfeatures of global spatial positions on multiple scales and the\ninteractions between pairwise instances were modeled. Using\nthe transformer as a branch network of the one-stage detection\nnetwork, Zhang et al.[75] constructed GANsformer to utilize\nthe feature information in the entire region and improved the\nmodel’s ability to detect objects in aerial images. The above\nstudies indicate that the transformer mainly functions in vari-\nous CNN-based detection frameworks as a feature interaction\nmodule.\nHowever, the characteristics of UA V images make current\nCNN-based, transformer-based, and CNN-transformer hybrid\nmethods face challenges in object detection. First, the object\nscale in UA V images varies signiﬁcantly because of the con-\nsiderable change in ﬂight altitude. Second, the objects in UA V\nimages have different luminance and complex backgrounds.\nThird, the distribution of various objects in UA V images is\nuneven and varies dramatically. These above challenges make\nit difﬁcult for the current methods to achieve the desired detec-\ntion performance. Therefore, inspired by these excellent works,\ngiven the characteristics of UA V images, this study combines\na transformer with CNN to build an object detection model for\nUA V images. Through improvement and optimization of the\nproposed model, the detection performance is improved to better\nmeet the requirements of object detection in UA V images.\nIII. PROPOSED METHOD\nIn this section, the overall architecture of the proposed method\nis introduced ﬁrst, and then, the relevant key components and\nmodules are described in detail.\nA. Overview Framework\nThe pipeline of the proposed CNN-transformer hybrid net-\nwork model for UA V image object detection is shown in Fig.2,\nwhich mainly includes the following modules.\n1) The object detection network, Mask R-CNN, is used as\nthe pipeline network, in which a bottom-up hierarchical\nstructure is used to extract feature maps. By combining\nwith FPN, a top-down hierarchical structure with lateral\nconnections will fuse features, thereby obtaining high-\nlevel semantic information of different scales and realizing\nmultiscale object detection through the RPN and ROI\nhead.\n2) An efﬁcient and effective transformer-based backbone,\nCSWin Transformer, is used in each stage to extract fea-\ntures. The CNN-based backbone cannot effectively ac-\nquire long-distance interactions and dependencies, while\nthe global self-attention mechanism requires a signiﬁcant\namount of computation, and the interaction ﬁeld between\ndifferent tokens in the local self-attention mechanism is\nLU et al.: CNN-TRANSFORMER HYBRID MODEL BASED ON CSWIN TRANSFORMER 1215\nFig. 2. Pipeline of the proposed CNN-transformer hybrid model for UA V image object detection. The left and right parts of the dotted line are the training and\ninference stages, respectively.\nFig. 3. Architecture of the CSWin transformer-based encoder.\nlimited. CSWin Transformer effectively solves the above\nproblems by computing self-attention in the horizontal and\nvertical stripes in parallel.\n3) An HPEM is proposed to process the input image. Through\nconvolution, low-level feature maps are obtained. Then,\nthese feature maps are ﬂattened into a sequence of patches\nusing a patch embedding module, which can effectively\nobtain and utilize low-level information such as edges and\ncorners in the image while barely increasing the training\ndata and iterations.\n4) A SI method is constructed in the inference stage after\ncompleting model training to further improve the de-\ntection performance of small objects in high-resolution\nUA V images. The SI method divides the input image\ninto overlapping sliced images with a ﬁxed size, which\nwill be reidentiﬁed. Subsequently, the reidentiﬁcation re-\nsults are fused with the original image object detection\nresults through nonmaximum suppression (NMS) to fur-\nther improve the object detection performance of UA V\nimages.\nB. CSWin Transformer–Based Backbone\nIn this study, the CSWin Transformer is used as the encoder\nbackbone in the bottom-up hierarchical structure to extract\nfeature maps, and the encoded results obtained are input into\nthe top-down FPN structure for decoding to achieve multiscale\nobject detection.\n1) CSWin Transformer–Based Network Structure:The hier-\narchical network structure constructed with the CSWin Trans-\nformer as the backbone is shown in Fig.3. First, the input image\nwith sizeH ×W ×C (H, W, andC represent the height, width,\nand the number of channels, respectively; for the input image,\nC =3 ) is processed to obtain patch tokens, which are used as the\ninput of the subsequent hierarchical structure. Then, to produce\na hierarchical feature representation, the hierarchical network\nconsists of four stages based on the CSWin Transformer, and\nthe output of each stage is down-sampled using a convolution\nwith ﬁxed kernel size and stride, so thatH and W are reduced\nby half, while the number of channels is doubled. The output\nfeature map of theith stage has(H/2i+1) ×(W/2i+1) tokens,\n1216 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 4. CSWin transformer block.\nwhich is similar to those in the Swin Transformer. However,\ndifferent from Swin Transformer, CSWin Transformer replaces\nthe patchily stem with a convolutional stem, which achieves\nbetter training efﬁciency while ensuring overall stability[76].\nFinally, the output of the hierarchical network is processed by\nfeature fusion and prediction in the RPN layer and ROI layer to\nachieve object detection.\n2) CSWin Transformer Block:The global attention mecha-\nnism is computationally expensive, whereas the local attention\nmechanism limits the interaction between different tokens, and\nrequires more computing blocks to achieve global attention.\nTherefore, unlike the vanilla transformer, CSWin Transformer\nadopts CSWin self-attention to obtain global attention more\neffectively, as shown in Fig.4.\nSpeciﬁcally, CSWin self-attention divides the input data ver-\ntically and horizontally according to a given size to obtain hori-\nzontal and vertical stripes and performs self-attention calculation\nin horizontal and vertical stripes in parallel, as shown in Fig.5.\nThe main processing ﬂow is as follows.\nFirst, the input data X with size H ×W ×C is linearly\nprojected toK heads, which are divided into two parts equally,\nsuch asPart Aand Part Bin Fig.5. Each part hasK/2 heads.\nThen, Part Aand Part Bare segmented vertically and hori-\nzontally, respectively, to obtain horizontal and vertical stripes,\nand self-attention calculation is performed in the horizontal and\nvertical stripes in parallel. We takePart Ain Fig.5 as an example.\nPart A is evenly divided into nonoverlapping horizontal stripes\n[X1,...,X M] with width SW along the vertical direction,\nwhere M = H/SW , and the dimension of each stripXi is\nSW ×W ×C, wherei =1 , 2,...,M .\nAfter that, multihead attention is computed for each stripeXi,\nwhere the attentionY i\nk of thekth head is deﬁned as\nY i\nk = Attention(XiWQ\nk ,X iWK\nk ,X iWV\nk ) (1)\nwhere WQ\nk ∈ RC×dk, WK\nk ∈ RC×dk, WV\nk ∈ RC×dk, and\ndk = C/K. The overall horizontal stripes self-attention forkth\nhead can be deﬁned as\nH−Attentionk(X)=[ Y 1\nk ,Y 2\nk ,...,Y M\nk ]. (2)\nThe overall vertical stripes’ self-attentionV −Attentionk(X)\nfor thekth head can be performed with reference to the horizontal\nstripes self-attention. After concatenating the self-attention in\nhorizontal and vertical stripes, the ﬁnal result can be deﬁned\nas\nCSWin −Attention(X)\n= Concat(head1, head2,..., headK)WO (3)\nwhere WO ∈ RC×C, and\nheadk =\n{H−Attentionk(X) k =1 ,...,K / 2\nV −Attentionk(X) k = K/2+1 ,...,K . (4)\nUsing CSWin −Attention(X), the attention area of each\ntoken within one transformer block can be enlarged. In addition,\nas the stage increases, CSWin Transformer associates more\nregions by increasing the strip width SW.\nFinally, the processing of the CSWin Transformer block can\nbe formally expressed as\nˆXl =C S W i n−Attentation(LN(Xl−1)) +Xl−1 (5)\nXl = MLP(LN( ˆXl)) + ˆXl (6)\nwhere Xl represents the output of thelth CSWin Transformer\nblock or the precedent convolutional layer of each stage.\nC. FPN-Based Decoder\nMultiscale object detection is a basic challenge in UA V im-\nage object detection, and constructing a feature pyramid is an\neffective technique. In CNN-based models, different network\ndepths correspond to different levels of semantic features. The\nlow-level features have high resolution and rich detailed in-\nformation, whereas the high-level features have low resolution\nand rich semantic information. However, in high-level features,\nsmall objects have a higher probability of being ignored. To\nmake full use of the different level features extracted, FPN uses\nthe multiscale and hierarchical structure of deep convolutional\nnetworks to construct feature pyramids, which will be fused\nthrough a top-down and laterally connected structure to obtain\nhigh-level semantic features at different scales, as shown in\nFig. 6. FPN can be used as a general feature extractor and can\nbe combined with different backbones to achieve performance\nimprovement.\nThe bottom-up pathway of FPN is implemented by the feed-\nforward calculation of CNN. By referring to the idea of a\nhierarchical structure, data processing is divided into different\nstages according to the size of the feature map, and the scale\nof the feature map of each stage is half of the previous stage.\nUsing this structure, FPN can obtain more abundant feature\ninformation, and the output of each stage can be used as part\nof the input features of the corresponding level in the top-down\npathway in the FPN.\nThe top-down pathway of FPN provides higher resolution fea-\ntures by up-sampling spatially coarser, but semantically stronger\nfeature maps from higher pyramid levels, which will make it the\nsame size as the feature map output from the corresponding\nstage in the bottom-up pathway. Then, lateral connections are\nemployed to enhance these features using information from the\nbottom-up pathway, and the output features serve as the input of\nthe next layer in the top-down pathway. To combine the semantic\nLU et al.: CNN-TRANSFORMER HYBRID MODEL BASED ON CSWIN TRANSFORMER 1217\nFig. 5. Cross-shaped window self-attention.\nFig. 6. Architecture of the FPN.\ninformation of the high-level features with precise positioning\ninformation of the low-level features, each lateral connection\nadopts a structure similar to a residual network; to correct the\nnumber of channels, the feature map output by the corresponding\nlevel stage in the bottom-up pathway needs to be processed by\na 1 ×1 convolution.\nBased on the above characteristics, FPN utilizes not only high-\nlevel strong semantic features for classiﬁcation but also low-\nlevel high-resolution information for localization.\nD. Hybrid Patch Embedding Module (HPEM)\nTypically, before being input into the transformer-based back-\nbone, the raw image must be processed to generate a sequence of\ntoken embeddings. For example, ViT splits the input image with\na patch size of16 ×16 or 32 ×32, and the Swin Transformer\nor CSWin Transformer splits each image with a patch size\nof H/4 ×W/4 (H and W represent the height and width of\nthe input image), which makes it difﬁcult to capture low-level\ninformation (such as edges and corners), and requires much more\ntraining data or training iterations. Given the above situation,\nfor capturing more detailed low-level information and enriching\nthe features extracted by the backbone, this study proposes an\nHPEM, as shown in Fig.7.\nBefore the original patch embedding module, HPEM adds a\nconvolution layer to generate low-level features; simultaneously,\nto better facilitate the training process, batch normalization is\nadded after the convolution layer. Subsequently, by ﬂattening\nthese low-level features into sequence tokens using the original\npatch embedding module, the low-level information in the image\ncan be exploited to the fullest while hardly increasing the com-\nputational cost. The entire process of HPEM can be expressed\nas follows:\nx′= HPEM(x) = PatchEmbedding(BN(Conv(x))) (7)\nwhere x′∈ R\nH\nS ×W\nS ×C, and S denotes the stride in the input\nimage. HPEM makes full use of the advantages of CNN in\nextracting low-level features and reduces the training difﬁculty\nof implantation by reducing the patch size.\n1218 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 7. Hybrid patch embedding module.\nFig. 8. Basic principles and steps of slicing-based inference.\nE. Slicing-Based Inference\nInﬂuenced by the ﬂying height and the shooting angle of\nUA Vs, the obtained images contain numerous small objects at\nlong distances or oblique angles. These small objects occupy\nfewer pixels in images, and details are insufﬁcient, which affects\nthe object detection performance. Although the current object\ndetection algorithm achieves multiscale object detection by\nconstructing feature pyramids, the performance of small object\ndetection is expected to be improved further.\nBased on the slicing-aided hyper inference mechanism[77],\nthis study constructs the SI to further improve the performance\nof small object detection in UA V images. The mechanism of the\nSI is shown in Fig.8, whose basic principles and steps are as\nfollows:\nStep 1: use the model trained by the proposed method in this\narticle to perform inference on the original UA V images to\nachieve normal inference results, as shown in Fig.8(a);\nStep 2: according to the given size (such as512 ×512), the\noriginal UA V image is divided into overlap slices, which\nwill be proportionally enlarged. For example, as shown in\nFig. 8(b), the original UA V image is divided into overlap slices\nP1, P2, P3, and P4. The enlarged results of slicesP2 and\nP4 by 2 times areP2′and P4′;\nStep 3:use the model trained by the proposed method to perform\ninference on the enlarged slices (such as P2′ and P4′ in\nFig. 8(b)), and obtain the inference results [such asP2′′and\nP4′′in Fig.8(b)];\nStep 4:the original UA V image inference results in Step 1 [see\nFig. 8(a)] and the sliced image inference results in Step 3 [see\nFig. 8(b)] are fused through NMS to obtain the ﬁnal inference\nresults, which are shown in Fig.8(c).\nIV . EXPERIMENTS AND RESULTS\nA. Datasets and Pretrained Models\n1) Datasets: We selected the popular public dataset\nVisDrone2021-DET and UA VDT to train, test, and evaluate the\nproposed model.\nLU et al.: CNN-TRANSFORMER HYBRID MODEL BASED ON CSWIN TRANSFORMER 1219\nFig. 9. PR curve (IoU = 0.5) of different methods without pretrained models. on the visdrone2021-DET dataset. (a) PR curve on the visdrone2021-DET dataset.\n(b) PR curve on the sparse UA VDT dataset.\nTABLE I\nSTATISTICSDATA OFALL OBJECT BOUNDING BOXES INVISDRONE2021-DET TRAIN AND VAL DATASET\nThe VisDrone2021-DET [78] dataset is a mainstream UA V\nimage dataset, which is used for object detection and has a\ntotal of 11 categories including pedestrian, people, bicycle, car,\nvan, truck, tricycle, awning-tricycle, bus, motor, and others. The\nnumbers of images in the training, test, and evaluation sets are\n6471, 548, and 1610, respectively. In the VisDrone2021-DET\ndataset, the shooting angle of the UA V signiﬁcantly changes,\nthe scale of various objects is diverse, and most sizes of the\nobjects are less than 32 pixels. Taking train and test data as\nexamples for analysis, the results are shown in TableI [79], and\nthe ﬁrst row represents the object size range and the second row\ncontains the number of corresponding objects. In addition, the\nobject categories present in the VisDrone2021-DET dataset are\nﬁne-grained and classiﬁed, which makes it very challenging in\nthe ﬁeld of UA V image object detection task. Simultaneously,\nthe number of images in the VisDrone2021-DET dataset is\nmoderate, which can better test the performance of various\nmodels. In addition, the data size of the VisDrone2021-DET\ndataset is resized to an integral multiple of 224.\nThe UA VDT[80] dataset is a UA V benchmark captured by a\nUA V platform at a number of locations in urban areas; it repre-\nsents various common scenes including squares, arterial streets,\ntoll stations, highways, crossings, and T-junctions, and includes\nabout 80 000 representative frames from 100 video sequences.\nUA VDT dataset has a total of 3 categories: car, truck, and bus.\nBecause the UA VDT dataset is taken from video sequences,\nthe images in the same video sequence contain a lot of similar\ncontent, resulting in redundancy, which makes it challenging to\ndistinguish the performance of different models. Therefore, in\nthis study, a sparse UA VDT dataset was constructed based on the\noriginal UA VDT dataset by extracting one in every 20 images. In\nthe sparse UA VDT dataset, the number of images in the training,\ntest, and evaluation sets were 1442, 206, and 412, respectively.\nIn addition, the data size of the sparse UA VDT dataset is scaled\nto 448 ×448.\n2) Pretrained Models: In addition various deep learning\nmodels to directly train on the given dataset, following-up the\nexisting SOTA networks can better improve the performance of\nthe proposed model. When comparing related methods, some\npretrained results given by the existing SOTA networks will\nalso be used in the experiments. These pretrained models are\ntrained on large object detection datasets such as Pascal VOC,\nCOCO, and FAIR1M. Although these models cannot be directly\nused in UA V image object detection, they can help with weight\ninitialization.\nB. Experimental Implementation Details\n1) Evaluation Metrics: We used precision, a common eval-\nuation metric that measures the percentage of the correct\nprediction results, as the basic experimental evaluation criteria\nand obtained the precision results of the bounding boxes, in-\ncluding average precision (AP), AP50 (AP test results when the\nIoU threshold is greater than 0.5), AP75 (AP test results when\nthe IoU threshold is greater than 0.75), APS (AP test results\nwith object frame size less than 32×32 pixels), APM (AP test\nresults with object frame sizes between 32× 32 pixels and 96\n× 96 pixels), and APL (AP test results with object frame sizes\nlarger than 96× 96 pixels). AP is usually computed for each\nclass separately, therefore, the mean AP, which is the average of\n1220 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 10. Comparison of the qualitative inference results of the different methods without pretrained models in the ﬁrst scenario from the visdrone2021-DET dataset.\n(a) Original image. (b) Ground truth. (c) Mask R-CNN (Resnet-101). (d) HTC (ResNeXt-101). (e) Dynamic R-CNN (ResNeXt-101). (f) FCOS (ResNeXt-101).(g)\nMask R-CNN (Swin-S). (h) Mask R-CNN (CSWin-BM). (i) TPH-YOLOv5 (CSPDarknet53). (j) Cascade R-CNN (ConvNeXt-T). (k) Mask R-CNN (PVT2-B3).\n(l) Proposed method.\nthe AP values of all object categories, was adopted as the ﬁnal\nmeasure for evaluating overall accuracy.\n2) Experimental Setup: All experiments in this article were\ncarried out on a workbench equipped with an Intel CPU i7 9700k\nand one NVIDIA GeForce RTX 3090 (24G). The operating\nsystem was Ubuntu 20.04 LTS, and all deep learning models\nwere constructed based on the open-source object detection\ntoolbox MMDetection and PyTorch framework.\n3) Comparative Methods: For a fair comparison, we only\nconsidered methods providing source codes and selected some\nclassic and advanced networks, such as Mask R-CNN, HTC,\nFCOS, Dynamic R-CNN, TPH-YOLOv5, and Cascade R-CNN,\nand the backbone networks mainly included ResNet-101[81],\nResNeXt-101 [82], ConvNeXt-T [83], CSPDarknet53 [32],\nSwin-S [64], and PVT2-B3 [67]. For CSWin Transformer,\nCSWin-BM was constructed based on CSWin-B[65] by chang-\ning the stages to 2, 2, 18, and 2 blocks. In addition, the\nperformance of each backbone network with the corresponding\npretrained models was also presented.\n4) Data Processing: For the training and testing datasets,\nwe applied a series of standard data augmentation strategies.\nHorizontal random ﬂip with a ﬂipping probability of 0.5 was\nused to meet the requirements of image size for data processing;\nthe height and width of the original input data were expanded to\ninteger multiples of 224; band normalization operations using\nregularization with a mean of [123.675, 116.28, 103.53] and\nstandard deviation of [58.395, 57.12, 57.375] was used.\n5) Experimental Hyperparameter: We set the same training\nhyperparameters to maintain the consistency and comparability\nof the training results. For the experiments, we used the adaptive\nmoment estimation with decoupled weight decay (AdamW)\noptimizer, where the initial learning rate and the weight decay\nwere 0.0001 and 0.05, respectively. Considering the hardware\nlimitations, the batch size was set to 1, and except for the\nLU et al.: CNN-TRANSFORMER HYBRID MODEL BASED ON CSWIN TRANSFORMER 1221\nFig. 11. Comparison of the qualitative inference results of the different methods without pretrained models in the second scenario from the visdrone2021-DET\ndataset. (a) Original image. (b) Ground truth. (c) Mask R-CNN (Resnet-101). (d) HTC (ResNeXt-101). (e) Dynamic R-CNN (ResNeXt-101). (f) FCOS (ResNeXt-\n101). (g) Mask R-CNN (Swin-S). (h) Mask R-CNN (CSWin-BM). (i) TPH-YOLOv5 (CSPDarknet53). (j) Cascade R-CNN (ConvNeXt-T). (k) Mask R-CNN\n(PVT2-B3). (l) Proposed method.\nvanilla Mask R-CNN with ResNet-101 whose schedule is set\nto 200 epochs and Dynamic R-CNN with ResNeXt-101 whose\nschedule is set to 50 epochs, the schedule of all other methods\nwas set to 3×, which means the entire training processing had\n36 epochs.\nC. Ablation Study\nIn this section, an ablation study was designed to analyze how\nmuch each component contributed to the accuracy of the overall\nperformance on different datasets, including without (w/o) and\nwith (w) pretrained models, as shown in TablesII and III.M a s k\nR-CNN was set as the baseline method, and ResNet-101 served\nas the backbone network of vanilla Mask R-CNN.\nTable II shows that Mask R-CNN with the CSWin Trans-\nformer as the backbone network achieved APs of 24.0 and\n25.4 without and with the pretrained model, respectively, and\nthe results of APs in TableIII are 12.3 and 26.0 without and\nwith the pretrained model, respectively, which are better than\nthose of vanilla Mask R-CNN with ResNet-101. In addition, the\ndetection performance showed improvements of 0.6 and 0.4 AP\nin TableII and 1.3 and 0.2 AP in TableIII when the HPEM was\nadded. Using SI, the performance of the proposed method can be\nfurther improved, especially in terms of APS, APM, and APL.\nThe ablation study results proved that the various components\nof the proposed method were effective at improving the object\ndetection accuracy in UA V images. The comparison shows that\nthe performance of the proposed method with pretrained models\nis much better than that without pretrained models, which proves\nthe contribution of the pretrained models.\nTo further verify the effectiveness of HPEM, experiments\non the performance of different backbones with HPEM were\nconducted, and the results are shown in TableIV. Notably, to\nobtain more reliable comparison results, the pretrained models\n1222 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 12. Comparison of the qualitative inference results of the different methods without pretrained models in the third scenario from the sparse UA VDT dataset.\n(a) Original image. (b) Ground truth. (c) Mask R-CNN (Resnet-101). (d) HTC (ResNeXt-101). (e) Dynamic R-CNN (ResNeXt-101). (f) FCOS (ResNeXt-101).(g)\nMask R-CNN (Swin-S). (h) Mask R-CNN (CSWin-BM). (i) TPH-YOLOv5 (CSPDarknet53). (j) Cascade R-CNN (ConvNeXt-T). (k) Mask R-CNN (PVT2-B3).\n(l) Proposed method.\nFig. 13. PR curve (IoU=0.5) of different methods with pretrained models. on the visdrone2021-DET dataset. (a) PR curve on the visdrone2021-DET dataset.\n(b) PR curve on the sparse UA VDT dataset.\nLU et al.: CNN-TRANSFORMER HYBRID MODEL BASED ON CSWIN TRANSFORMER 1223\nFig. 14. Comparison of the qualitative inference results of the different methods with pretrained models in the ﬁrst scenario from the visdrone2021-DET dataset.\n(a) Original image. (b) Ground truth. (c) Mask R-CNN (Resnet-101). (d) HTC (ResNeXt-101). (e) Dynamic R-CNN (ResNeXt-101). (f) FCOS (ResNeXt-101).\n(g) Mask R-CNN (Swin-S). (h) Mask R-CNN (CSWin-BM). (i) TPH-YOLOv5 (CSPDarknet53). (j) Cascade R-CNN (ConvNeXt-T). (k) Mask R-CNN (PVT2-B3).\n(l) Proposed method.\nTABLE II\nABLATION STUDY RESULTS OF THEPROPOSED METHOD ON THEVISDRONE2021-DET DATASET\n1224 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 15. Comparison of the qualitative inference results of the different methods with pretrained models in the second scenario from the visdrone2021-DET dataset.\n(a) Original image. (b) Ground truth. (c) Mask R-CNN (Resnet-101). (d) HTC (ResNeXt-101). (e) Dynamic R-CNN (ResNeXt-101). (f) FCOS (ResNeXt-101).\n(g) Mask R-CNN (Swin-S). (h) Mask R-CNN (CSWin-BM). (i) TPH-YOLOv5 (CSPDarknet53). (j) Cascade R-CNN (ConvNeXt-T). (k) Mask R-CNN (PVT2-B3).\n(l) Proposed method.\nTABLE III\nABLATION STUDY RESULTS OF THEPROPOSED METHOD ON THESPARSE UA VDT DATASET\nLU et al.: CNN-TRANSFORMER HYBRID MODEL BASED ON CSWIN TRANSFORMER 1225\nFig. 16. Comparison of the qualitative inference results of the different methods with pretrained models in the second scenario from the sparse UA VDTdataset.\n(a) Original image. (b) Ground truth. (c) Mask R-CNN (Resnet-101). (d) HTC (ResNeXt-101). (e) Dynamic R-CNN (ResNeXt-101). (f) FCOS (ResNeXt-101).\n(g) Mask R-CNN (Swin-S). (h) Mask R-CNN (CSWin-BM). (i) TPH-YOLOv5 (CSPDarknet53). (j) Cascade R-CNN (ConvNeXt-T). (k) Mask R-CNN (PVT2-B3).\n(l) Proposed method.\nwere not used. The comparison results indicate that although\nHPEM is not universal, it can effectively improve performance\nof the proposed methods.\nD. Evaluation and Comparisons\nBased on the above datasets, pretrained models, and exper-\nimental implementation details, we performed an evaluation\nand comparisons from two aspects: without and with pretrained\nmodels.\n1) Without Pretrained Models:The comparison results be-\ntween different methods combined with the corresponding back-\nbones on the Visdrone2021-DET dataset are shown in Ta-\nble V. The comparison results show that Mask R-CNN with\ntransformer as the backbone network has better performance\nthan methods such as vanilla Mask R-CNN, HTC, FCOS, and\nDynamic R-CNN with a similar count of parameters. Between\nthe different transformer backbone networks, the method with\nCSWin Transformer as the backbone network achieves better\nperformance, and the experimental result is 2.7 AP higher than\nthe Swin Transformer. The proposed method proposed showed\na further improvement in object detection accuracy, which was\nhigher than those of vanilla Mask R-CNN (200 epochs) by 7.8\nAP and Mask R-CNN with CSWin Transformer by 1.8 AP.\nNotably, the method proposed achieves a conspicuous improve-\nment for small, medium, and large object detection compared\nto the Mask R-CNN with CSWin Transformer as the backbone\nnetwork by 20.6, 18.7, and 19.8 AP, respectively.\nThe comparison results between different methods combined\nwith the corresponding backbones on the sparse UA VDT dataset\nare shown in Table VI. The proposed method has a further\nimprovement in object detection accuracy, which is higher than\nthat of vanilla Mask R-CNN (200 epochs) by 6.0 AP and that\nof Mask R-CNN with CSWin Transformer by 2.1 AP. Notably,\nthe proposed method achieves a conspicuous improvement for\nsmall, medium, and large object detection compared to the Mask\nR-CNN with CSWin Transformer as the backbone network by\n17.3, 24.2, and 33.6 AP, respectively.\nThe precision-recall curves of the different methods with-\nout pretrained models on the Visdrone2021-DET and sparse\nUA VDT dataset are provided in Fig.9, which intuitively show the\ndetailed relationship between precision and recall. The proposed\n1226 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nTABLE IV\nABLATION STUDY RESULTS OF THEEFFECTIVENESS OF HPEM ON THE VISDRONE2021-DET DATASET ANDSPARSE UA VDT DATASET\nTABLE V\nOBJECT DETECTION PERFORMANCE OF DIFFERENT METHODS WITH PRETRAINED MODELS ON THEVISDRONE2021-DET DATASET\nmethod exhibits better performance on the Visdrone2021-DET\ndataset in Fig. 9(a). However, the performance exhibited in\nFig. 9(b) is not particularly ideal, which may because of the\nlarger amount of training data needed by the transformer.\nIn addition to the quantitative comparisons, the qualitative\ninference results by different methods of the three scenarios\nselected from the Visdrone2021-DET and sparse UA VDT\ndataset are shown in Figs.10–12, respectively, which show that\nthe proposed method exhibits a conspicuous detection capa-\nbility on the Visdrone2021-DET and sparse UA VDT dataset.\nIn particular, it pays more attention to distant small ob-\njects and can achieve better detection performance for small\nobjects.\n2) With Pretrained Models: For downstream tasks, pre-\ntrained models on large-scale image datasets, such as ImageNet\n[84], and COCO 2017 [85], are usually used as the initial\nLU et al.: CNN-TRANSFORMER HYBRID MODEL BASED ON CSWIN TRANSFORMER 1227\nTABLE VI\nOBJECT DETECTION PERFORMANCE OF DIFFERENT METHODS WITH PRETRAINED MODELS ON THESPARSE UA VDT DATASET\nTABLE VII\nOBJECT DETECTION PERFORMANCE OF DIFFERENT METHODS WITH PRETRAINED MODELS ON THEVISDRONE2021-DET DATASET\nweights of the backbone network. Therefore, in this study,\nwe used these pretrained models to initialize the weights of\nthe corresponding backbone networks in comparative models\nand conducted subsequent comparisons. The results on the\nVisdrone2021-DET dataset in Table IV show that with the\npretrained models, the performance of each method is improved.\nThe proposed method has a further improvement in object\ndetection accuracy, which is higher than that of vanilla Mask\nR-CNN (200 epochs) by 2.1 AP and Mask R-CNN with CSWin\nTransformer by 1.1 AP. Furthermore, the proposed method\nachieved a conspicuous improvement in small, medium, and\nlarge objects detection compared to the Mask R-CNN with\nCSWin Transformer as the backbone network by 20.0, 20.5,\nand 15.8 AP, respectively.\nThe results on the sparse UA VDT dataset in TableVIII show\nthat with pretrained models, the performance of each method\nis improved. The proposed method led to a further improve-\nment in object detection accuracy, which is higher than that of\nvanilla Mask R-CNN (200 epochs) by 5.2 AP and Mask R-CNN\nwith CSWin Transformer by 0.3 AP, and the model achieved a\n1228 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nTABLE VIII\nOBJECT DETECTION PERFORMANCE OF DIFFERENT METHODS WITH PRETRAINED MODELS ON THESPARSE UA VDT DATASET\nconspicuous improvement for small, medium, and large object\ndetection compared to the Mask R-CNN with CSWin Trans-\nformer as the backbone network by 27.4, 28.4, and 40.6 AP,\nrespectively.\nThe precision-recall curves of the different methods with pre-\ntrained models on the Visdrone2021-DET and sparse UA VDT\ndataset are provided in Fig.13, and the proposed method still\nexhibits superior performance. The comparison between the PR\ncurve in Figs. 9 and 13 indicate that an adequate pretraining\nmodel can further improve the performance of the transformer.\nThe qualitative inference results by different methods of the\nthree scenarios selected from the Visdrone2021-DET and sparse\nUA VDT dataset with pretrained models are shown in Figs.14–\n16. The proposed method exhibited a conspicuous detection\ncapability on both the Visdrone2021-DET and sparse UA VDT\ndatasets.\nV. DISCUSSION AND CONCLUSION\nThe experimental results in TablesV–VIII and the qualitative\ninference results in Figs.10–16 show that the CNN-transformer\nhybrid model proposed in this study can achieve better re-\nsults compared with the current classic and popular models.\nBy constructing a feature pyramid structure and using CSWin\nTransformer as the backbone, the proposed method can obtain\nbetter high-level semantic features of different scales while\neffectively establishing long-distance dependencies, which is\nhelpful to achieve multiscale object detection, and the ablation\nstudy results in TablesII and III indicate that the AP test results of\nsmall, middle and large objects have a signiﬁcant improvement.\nCombined with HPEM, low-dimensional information such as\nedges and corners in the image can be further extracted and uti-\nlized to enhance and enrich the feature information. In addition,\naccording to the high-resolution characteristics of UA V images,\nSI is used to fuse the normal and SI results, which can improve\nthe accuracy of object detection, especially that of detecting\nsmall objects, without modifying the original model, and the\nresults in TablesII and III indicate that the AP test results of small\nobjects are improved greatly, even doubled, with the SI method.\nThe SI method indicates that the subsequent inference process\nalso has a large impact on the overall performance. Furthermore,\nthe comparison of the PR curves in Figs.9 and 13 indicate that\nthe transformer needs large amounts of training data or a better\nweight initialization to perform better.\nHowever, deﬁciencies are also observed in experimental com-\nparisons. Although pretrained models can effectively improve\nthe performance of networks, the contributions of various pre-\ntrained models are considerably different. In addition, the sce-\nnarios of pretrained models are considerably different from those\nof the training dataset, which leads to inadequate use of the\npretrained models. The training time required by the proposed\nmethod is low, but each epoch is time-consuming, and the\ntraining process has a high hardware requirement. In addition,\nthis study veriﬁes the effectiveness of the CNN-transformer\nhybrid model and the proposed module (such as the HEPM\nand SI), and there is still a certain gap from SOTA methods.\nMeanwhile, notably, in the comparison results, although some\nmethods have demonstrated excellent performance, ideal results\nunder the training dataset in this study are not achieved with the\nofﬁcial codes, which is a problem that requires further in-depth\nanalysis.\nThere is still room for improving the proposed method,\nand follow-up work can be carried out around model transfer,\nlightweight models, and performance improvement, which can\nlikely be solved by transfer learning and the use of efﬁcient\ntransformers.\nLU et al.: CNN-TRANSFORMER HYBRID MODEL BASED ON CSWIN TRANSFORMER 1229\nREFERENCES\n[1] J. Bo, R. Qu, Y . Li, and C. Li, “Object detection in UA V imagery based on\ndeep learning: Review,”Acta Aeronauticaet Astronautica Sinica, vol. 42,\nno. 4, Aug. 2021, Art. no. 524519.\n[2] K. Nguyen et al., “The state of aerial surveillance: A survey,” 2022,\narXiv:2201.03080v2.\n[3] K. Li, G. Wan, G. Cheng, L. Meng, and J. Han, “Object detection in\noptical remote sensing images: A survey and a new benchmark,”ISPRS J.\nPhotogramm. Remote Sens., vol. 159, pp. 296–307, Jan. 2020.\n[4] G. Mao, T. Deng, and N. Yu, “Object detection in UA V images based\non multi-scale split attention,”Acta Aeronautica et Astronautica Sinica,\nvol. 43, pp. 1–12, doi:10.7527/S1000-6893.2021.26738. [Online]. Avail-\nable: http://hkxb.buaa.edu.cn/CN/10.7527/S1000-6893.2021.26738\n[5] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies\nfor accurate object detection and semantic segmentation,” inProc. IEEE\nConf. Comput. Vis. Pattern Recognit., 2013, pp. 580–587.\n[6] R. Girshick, “Fast R-CNN,” inProc. IEEE Int. Conf. Comput. Vis ., 2015,\npp. 1440–1448.\n[7] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time\nobject detection with region proposal networks,”IEEE Trans. Pattern Anal.\nMach. Intell., vol. 39, no. 6, pp. 1137–1149, Jun. 2017.\n[8] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask R-CNN,” inProc.\nIEEE Conf. Comput. Vis. Pattern Recognit ., 2017, pp. 2980–2988.\n[9] Z. Cai and N. Vasconcelos, “Cascade R-CNN: Delving into high quality\nobject detection,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recog-\nnit., 2018, pp. 6154–6162.\n[10] H. Zhang, H. Chang, B. Ma, N. Wang, and X. Chen, “Dynamic R-CNN:\nTowards high quality object detection via dynamic training,” inProc. Eur.\nConf. Comput. Vis., Aug. 2020, pp. 260–275.\n[11] S. Wang, “Research towards YOLO-series algorithms: Comparison and\nanalysis of object detection models for real-time UA V applications,” in\nProc. 2nd Int. Conf. Internet Things, Artif. Intell. Mech. Autom., May 2021,\nPaper 012021.\n[12] W. Liu et al., “SSD: Single shot multibox detector,” inProc. 14th Eur.\nConf., Oct. 2015, pp. 21–37.\n[13] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for dense\nobject detection,”IEEE Trans. Pattern Anal. Mach. Intell., vol. 42, no. 2,\npp. 318–327, Feb. 2020.\n[14] Z. Tian, C. Shen, H. Chen, and T. He, “FCOS: Fully convolutional one-\nstage object detection,” inProc. IEEE/CVF Int. Conf. Comput. Vis., 2019,\npp. 9626–9635.\n[15] A. Vaswani et al., “Attention is all you need,” inProc. 31st Int. Conf.\nNeural Inf. Process. Syst., Dec. 2017, pp. 6000–6010.\n[16] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” inProc. 9th Int. Conf. Learn. Representations,\nMay 2021.\n[17] Y . Bazi, L. Bashmal, M. M. A. Rahhal, R. A. Dayil, and N. A. Ajlan,\n“Vision transformers for remote sensing image classiﬁcation,”Remote\nSens., vol. 13, no. 3, Feb. 2021, Art. no. 516.\n[18] L. Bashmal, Y . Bazi, M. M. Al Rahhal, H. Alhichri, and N. Al Ajlan, “UA V\nimage multi-labeling with data-efﬁcient transformers,”Appl. Sci., vol. 11,\nno. 9, 2021, Art. no. 3974.\n[19] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, and W.\nBrendel, “ImageNet-trained cnns are biased towards texture; increasing\nshape bias improves accuracy and robustness,” inProc. 7th Int. Conf.\nLearn. Representations, May 2019.\n[20] Z. Li, G. Chen, and T. Zhang, “A CNN-transformer hybrid approach\nfor crop classiﬁcation using multitemporal multisensor images,”IEEE\nJ. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 13, pp. 847–858,\n2020.\n[21] W. F. Hendria, Q. T. Phan, F. Adzaka, and C. Jeong, “Combining trans-\nformer and CNN for object detection in UA V imagery,”ICT Express,\n2022, doi: 10.1016/j.icte.2021.12.006. [Online]. Available: https://www.\nsciencedirect.com/science/article/pii/S2405959521001715\n[22] C. Zhang, W. Jiang, Y . Zhang, W. Wang, Q. Zhao, and C. Wang, “Trans-\nformer and CNN hybrid deep neural network for semantic segmentation\nof very-high-resolution remote sensing imagery,”IEEE Trans. Geosci.\nRemote Sens., vol. 60, 2022, Art. no. 4408820.\n[23] S. Deng et al., “A global-local self-adaptive network for drone-view\nobject detection,”IEEE Trans. Image Process., vol. 30, pp. 1556–1569,\n2021.\n[24] F. Yang, H. Fan, P. Chu, E. Blasch, and H. Ling, “Clustered object detection\nin aerial images,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2019,\npp. 8310–8319.\n[25] Y . Wang, Y . Yang, and X. Zhao, “Object detection using clustering al-\ngorithm adaptive searching regions in aerial images,” inProc. Eur. Conf.\nComput. Vis., Aug. 2020, pp. 651–664.\n[26] Z. Wu, K. Suresh, P. Narayanan, H. Xu, H. Kwon, and Z. Wang, “Delving\ninto robust object detection from unmanned aerial vehicles: A deep nui-\nsance disentanglement approach,” inProc. IEEE/CVF Int. Conf. Comput.\nVis., 2019, pp. 1201–1210.\n[27] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep\nconvolutional networks for visual recognition,”IEEE Trans. Pattern Anal.\nMach. Intell., vol. 37, no. 9, pp. 1904–1916, Sep. 2015.\n[28] K. Chen et al., “Hybrid task cascade for instance segmentation,” inProc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 4969–4978.\n[29] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:\nUniﬁed, real-time object detection,” inProc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2016, pp. 779–788.\n[30] J. Redmon and A. Farhadi, “YOLO9000: Better faster stronger,” inProc.\nIEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 6517–6525.\n[31] J. Redmon and A. Farhadi, “YOLOv3: An incremental improvement,”\n2018, arXiv:1804.02767.\n[32] A. Bochkovskiy, C.-Y . Wang, and H.-Y . M. Liao, “YOLOv4: Optimal\nspeed and accuracy of object detection,” 2020,arXiv:2004.10934.\n[33] P. Zhu, L. Wen, D. Du, X. Bian, Q. Hu, and H. Ling, “Vision meets drones:\nPast, present and future,” 2020,arXiv:2001.06303v3.\n[34] X. Wu, W. Li, D. Hong, R. Tao, and Q. Du, “Deep learning for unmanned\naerial vehicle-based object detection and tracking: A survey,”IEEE Geosci.\nRemote Sens. Mag., vol. 10, no. 1, pp. 91–124, Mar. 2022.\n[35] Z. Deng, H. Sun, S. Zhou, J. Zhao, L. Lei, and H. Zou, “Multi-scale object\ndetection in remote sensing imagery with convolutional neural networks,”\nISPRS J. Photogramm. Remote Sens., vol. 145, no. Part A, pp. 3–22,\nNov. 2018.\n[36] W. Zhang, C. Liu, F. Chang, and Y . Song, “Multi-scale and occlusion aware\nnetwork for vehicle detection and segmentation on UA V aerial images,”\nRemote Sens., vol. 12, no. 11, May 2020, Art. no. 1760.\n[37] T.-Y . Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,\n“Feature pyramid networks for object detection,” inProc. IEEE Conf.\nComput. Vis. Pattern Recognit., 2017, pp. 936–944.\n[38] X. Zhang, K. Zhu, G. Chen, X. Tan, and Y . Gong, “Geospatial object\ndetection on high resolution remote sensing imagery based on double\nmulti-scale feature pyramid network,” Remote Sens., vol. 11, no. 7,\nMar. 2019, Art. no. 755.\n[39] W. Han et al., “Improving training instance quality in aerial image ob-\nject detection with a sampling-balance-based multistage network,”IEEE\nTrans. Geosci. Remote Sens., vol. 59, no. 12, pp. 10575–10589, Dec.\n2021.\n[40] Y . Liu, Q. Li, Y . Yuan, Q. Du, and Q. Wang, “ABNet: Adaptive balanced\nnetwork for multiscale object detection in remote sensing imagery,”IEEE\nTrans. Geosci. Remote Sens., vol. 60, 2022, Art. no. 5614914.\n[41] C. Chen, W. Gong, Y . Chen, and W. Li, “Object detection in remote sensing\nimages based on a scene-contextual feature pyramid network,”Remote\nSens., vol. 11, no. 3, Feb. 2019, Art. no. 339.\n[42] R. Zhang, Z. Shao, X. Huang, J. Wang, and D. Li, “Object detection in\nUA V images via global density fused convolutional network,”Remote\nSens., vol. 12, no. 19, Sep. 2020, Art. no. 3140.\n[43] R. Mottaghi et al., “The role of context for object detection and semantic\nsegmentation in the wild,” in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2014, pp. 891–898.\n[44] X. Liang, J. Zhang, L. Zhuo, Y . Li, and Q. Tian, “Small object detection\nin unmanned aerial vehicle images using feature fusion and scaling-based\nsingle shot detector with spatial context analysis,”IEEE Trans. Circuits\nSyst. Video Technol., vol. 30, no. 6, pp. 1758–1770, Jun. 2020.\n[45] C. Chen et al., “RRNet: A hybrid detector for object detection in drone-\ncaptured images,” inProc. IEEE/CVF Int. Conf. Comput. Vis. Workshop,\n2019, pp. 100–108.\n[46] X. Zhang, E. Izquierdo, and K. Chandramouli, “Dense and small object\ndetection in UA V Vision based on cascade network,” inProc. IEEE/CVF\nInt. Conf. Comput. Vis. Workshop, 2019, pp. 118–126.\n[47] Z. Liu, G. Gao, L. Sun, and Z. Fang, “HRDNet: High-resolution detection\nnetwork for small objects,” inProc. IEEE Int. Conf. Multimedia Expo,\n2021, pp. 1–6.\n[48] M. Liu, X. Wang, A. Zhou, X. Fu, and C. Piao, “UA V-YOLO: Small object\ndetection on unmanned aerial vehicle perspective,”Sensors, vol. 20, no. 8,\nApr. 2020, Art. no. 2238.\n[49] Y . Liu, F. Yang, and P. Hu, “Small-object detection in UA V-captured images\nvia multi-branch parallel feature pyramid networks,”IEEE Access,v o l .8 ,\npp. 145740–145750, 2020.\n1230 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\n[50] Y . Q. Cai et al., “Guided attention network for object detection and\ncounting on drones,” inProc. 28th ACM Int. Conf. Multimedia, Oct. 2020,\npp. 709–717.\n[51] D. Du et al., “VisDrone-DET2020: The vision meets drone object detection\nin image challenge results,” inProc. Eur. Conf. Comput. Vis., Aug. 2020,\npp. 692–712.\n[52] X. Hu et al., “SINet: A scale-insensitive convolutional neural network for\nfast vehicle detection,”IEEE Trans. Intell. Transp. Syst., vol. 20, no. 3,\npp. 1010–1019, Mar. 2019.\n[53] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S.\nZagoruyko, “End-to-end object detection with transformers,” inProc. Eur.\nConf. Comput. Vis., 2020, pp. 213–229.\n[54] X. He, Y . Zhou, J. Zhao, D. Zhang, R. Yao, and Y . Xue, “Swin transformer\nembedding UNet for remote sensing image semantic segmentation,”IEEE\nTrans. Geosci. Remote Sens., vol. 60, 2022, Art. no. 4408715.\n[55] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable DETR:\nDeformable transformers for end-to-end object detection,” inProc. 9th Int.\nConf. Learn. Representations, May 2021.\n[56] D. Meng et al., “Conditional DETR for fast training convergence,” inProc.\nIEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 3631–3640.\n[57] F. Li, H. Zhang, S. Liu, J. Guo, L. M. Ni, and L. Zhang, “DN-DETR:\nAccelerate DETR training by introducing query denoising,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2022, pp. 13609–13617.\n[58] S. Liu et al., “DAB-DETR: Dynamic anchor boxes are better queries for\nDETR,” inProc. 10th Int. Conf. Learn. Representations, Apr. 2022.\n[59] H. Zhang et al., “DINO: DETR with improved denoising anchor boxes for\nend-to-end object detection,” 2022,arXiv:2203.03605v4.\n[60] H. Touvron, M. Cord, M. Douze, F. Massa, and H. Jégou, “Training data-\nefﬁcient image transformers & distillation through attention,” inProc. 38th\nInt. Conf. Mach. Learn., 2021, pp. 10347–10357.\n[61] L. Bashmal, Y . Bazi, M. Rahhal, H. Alhichri, and N. A. Ajlan, “UA V image\nmulti-labeling with data-efﬁcient transformers,”Appl. Sci., vol. 11, no. 9,\nApr. 2021, Art. no. 3974.\n[62] R. Ranftl, A. Bochkovskiy, and V . Koltun, “Vision transformers for\ndense prediction,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021,\npp. 12159–12168.\n[63] W. Li et al., “SepViT: Separable vision transformer,” 2022,\narXiv:2203.15380.\n[64] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted windows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021,\npp. 9992–10002.\n[65] X. Dong et al., “CSWin transformer: A general vision transformer back-\nbone with cross-shaped windows,” inProc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit., 2022, pp. 12114–12124.\n[66] W. Wang et al., “Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions,” inProc. IEEE/CVF Int. Conf.\nComput. Vis ., 2021, pp. 548–558.\n[67] W. Wang et al., “PVT v2: Improved baselines with pyramid vision trans-\nformer,”Comp. Visual Media, vol. 8, pp. 415–424, 2022.\n[68] S. Tang, J. Zhang, S. Zhu, and P. Tan, “Quadtree attention for vision\ntransformers,” inProc. 10th Int. Conf. Learn. Representations, Apr. 2022.\n[69] W. Yuan and W. Xu, “MSST-Net: A multi-scale adaptive network for build-\ning extraction from remote sensing images based on swin transformer,”\nRemote Sens., vol. 13, no. 23, Nov. 2021, Art. no. 4743.\n[70] X. Xu et al., “An improved swin transformer-based model for remote\nsensing object detection and instance segmentation,”Remote Sens., vol. 13,\nno. 23, Nov. 2021, Art. no. 4779.\n[71] X. Zhu, S. Lyu, X. Wang, and Q. Zhao, “TPH-YOLOv5: Improved\nYOLOv5 based on transformer prediction head for object detection on\ndrone-captured scenarios,” inProc. IEEE/CVF Int. Conf. Comput. Vis.\nWorkshops, 2021, pp. 2778–2788.\n[72] F. Fan et al., “Efﬁcient instance segmentation paradigm for interpreting\nSAR and optical images,” Remote Sens., vol. 14, no. 43, Jan. 2022,\nArt. no. 531.\n[73] J. Feng and C. Yi, “Lightweight detection network for arbitrary-oriented\nvehicles in UA V imagery via global attentive relation and multi-path\nfusion,” Drones, vol. 6, no. 5, Apr. 2022, Art. no. 108.\n[74] Q. Li, Y . Chen, and Y . Zeng, “Transformer with transfer CNN for remote-\nsensing-image object detection,”Remote Sens., vol. 14, no. 4, Feb. 2022,\nArt. no. 984.\n[75] Y . Zhang, X. Liu, S. Wa, S. Chen, and Q. Ma, “GANsformer: A detection\nnetwork for aerial images with high performance combining convolu-\ntional network and transformer,”Remote Sens., vol. 14, no. 4, Feb. 2022,\nArt. no. 923.\n[76] W. Maciej, P. Hyunsoo, M. Jacek, and K. Kyung-Joong, “Recent ad-\nvances in general game playing,”Sci. World J., vol. 2015, Aug. 2015,\nArt. no. 986262.\n[77] F. C. Akyon, S. O. Altinuc, and A. Temizel, “Slicing aided hyper inference\nand ﬁne-tuning for small object detection,” inProc. IEEE Int. Conf. Image\nProcess., 2022, pp. 966–970.\n[78] Y . Cao et al., “VisDrone-DET2021: The vision meets drone object de-\ntection challenge results,” inProc. IEEE/CVF Int. Conf. Comput. Vis.\nWorkshops, 2021, pp. 2847–2854.\n[79] J. Wan, B. Zhang, Y . Zhao, Y . Du, and Z. Tong, “VistrongerDet: Stronger\nvisual information for object detection in VisDrone images,” inProc.\nIEEE/CVF Int. Conf. Comput. Vis. Workshops, 2021, pp. 2820–2829.\n[80] D. Du, Y . Qi, H. Yu, Y . Yang, and K. Duan, “The unmanned aerial vehicle\nbenchmark: Object detection and tracking,” inProc. Eur. Conf. Comput.\nVis., Sep. 2018, pp. 375–391.\n[81] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016,\npp. 770–778.\n[82] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual\ntransformations for deep neural networks,” inProc. IEEE Conf. Comput.\nVis. Pattern Recognit., 2017, pp. 5987–5995.\n[83] Z. Liu, H. Mao, C.-Y . Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A\nConvNet for the 2020s,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit., 2022, pp. 11966–11976.\n[84] O. Russakovsky et al., “ImageNet large scale visual recognition challenge,”\nInt. J. Comput. Vis., vol. 115, pp. 211–252, 2014.\n[85] T. Y . Lin, M. Maire, S. Belongie, J. Hays, and C. L. Zitnick, “Microsoft\nCOCO: Common objects in context,” inProc. Eur. Conf. Comput. Vis.,\nOct. 2014, pp. 740–755.\nWanjie Lu received the B.S. degree in photogram-\nmetry and remote sensing and the Ph.D. degree in\nsurveying and mapping from the Information Engi-\nneering University, Zhengzhou, China, in 2016 and\n2020, respectively.\nHe is currently a Lecturer with the Data and Tar-\nget Engineering Institute, Information Engineering\nUniversity, Zhengzhou, China. His research interests\ninclude unmanned aerial vehicle remote sensing im-\nage processing, deep learning algorithm, and spatial\ninformation service.\nChaozhen Lan received the B.S. and M.S. degrees\nin photogrammetry and remote sensing and the Ph.D.\ndegree in surveying and mapping from the Zhengzhou\nInstitute of Surveying and Mapping, Zhengzhou,\nChina, in 2002, 2005, and 2009, respectively.\nHe is currently an Associate Professor and a Mas-\nter’s Supervisor with the Information Engineering\nUniversity, Zhengzhou, China. His research interests\ninclude photogrammetry and unmanned aerial vehi-\ncle remote sensing.\nChaoyang Niu received the B.S. and M.S. degrees\nin information engineering from Zhengzhou Informa-\ntion Technology Institute, Zhengzhou, China, in 2003\nand 2006, respectively, and the Ph.D. degree in signal\nand information processing from Zhengzhou Institute\nof Surveying and Mapping, Zhengzhou, China, in\n2011.\nIn 2016, he was an Associate Professor with the\nData and Target Engineering Institute, Information\nEngineering University. His research interests include\npattern recognition, unmanned aerial vehicle remote\nsensing, and optical and radar imagery processing.\nLU et al.: CNN-TRANSFORMER HYBRID MODEL BASED ON CSWIN TRANSFORMER 1231\nWei Liu received the B.S., M.S., and Ph.D. degrees\nin information and communication engineering from\nthe Information Engineering University, Zhengzhou,\nChina, in 2001, 2003, and 2016, respectively.\nHe is currently an Associate Professor with the\nInformation Engineering University. His research in-\nterests include pattern recognition, remote sensing\ninformation processing, and deep learning.\nLiang Lyu received the B.S. degree in measurement\nand control engineering and the M.S. degree in pho-\ntogrammetry and remote sensing from the Zhengzhou\nInstitute of Surveying and Mapping, Zhengzhou,\nChina, in 2011 and 2014, respectively, and the Ph.D.\ndegree in surveying and mapping from the Informa-\ntion Engineering University, Zhengzhou, China, in\n2019.\nHe is currently an Associate Professor with the In-\nformation Engineering University. His research inter-\nests include photogrammetry, remote sensing, digital\nEarth information resources, space situational awareness, and visualization.\nQunshan Shi received the B.S. and M.S. de-\ngrees in photogrammetry and remote sensing from\nZhengzhou Institute of Surveying and Mapping,\nZhengzhou, China, in 2008 and 2011, respectively,\nand the Ph.D. degree in surveying and mapping from\nthe Information Engineering University, Zhengzhou,\nChina, in 2015.\nHe is currently an Associate Professor and a Mas-\nter’s Supervisor with the Information Engineering\nUniversity. His research interests include photogram-\nmetry, remote sensing, and virtual reality.\nShiju Wang received the B.S. degree in electronic\nscience and technology and the M.S. degree in optical\nengineering from Zhengzhou University, Zhengzhou,\nChina, in 2013 and 2016, respectively.\nHe is currently an Assistant Professor with\nthe Information Engineering University, Zhengzhou,\nChina. His research interests include remote sens-\ning, image processing, and unmanned aerial vehicle\ntechnology.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6921244263648987
    },
    {
      "name": "Transformer",
      "score": 0.6428943872451782
    },
    {
      "name": "Computer vision",
      "score": 0.5876577496528625
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5704320669174194
    },
    {
      "name": "Object detection",
      "score": 0.504623293876648
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3171682357788086
    },
    {
      "name": "Electrical engineering",
      "score": 0.16780754923820496
    },
    {
      "name": "Voltage",
      "score": 0.15883541107177734
    },
    {
      "name": "Engineering",
      "score": 0.13670885562896729
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I169689159",
      "name": "PLA Information Engineering University",
      "country": "CN"
    }
  ]
}