{
  "title": "Biases in Predicting the Human Language Model",
  "url": "https://openalex.org/W2250270548",
  "year": 2014,
  "authors": [
    {
      "id": "https://openalex.org/A2142646356",
      "name": "Alex B. Fine",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2126658726",
      "name": "Austin F. Frank",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127437209",
      "name": "T. Florian Jaeger",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113965177",
      "name": "Benjamin Van Durme",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2031452300",
    "https://openalex.org/W1993889244",
    "https://openalex.org/W2014516359",
    "https://openalex.org/W2138640694",
    "https://openalex.org/W136568931",
    "https://openalex.org/W2005435014",
    "https://openalex.org/W2053362255",
    "https://openalex.org/W1525062299",
    "https://openalex.org/W1905522558",
    "https://openalex.org/W2172268343",
    "https://openalex.org/W2044688197",
    "https://openalex.org/W2117278770",
    "https://openalex.org/W265531733",
    "https://openalex.org/W2126929879",
    "https://openalex.org/W1974967573",
    "https://openalex.org/W2106272460",
    "https://openalex.org/W2007780422",
    "https://openalex.org/W2166637769",
    "https://openalex.org/W2151345657",
    "https://openalex.org/W1976115394"
  ],
  "abstract": "We consider the prediction of three human behavioral measures ‐ lexical decision, word naming, and picture naming ‐ through the lens of domain bias in language modeling. Contrasting the predictive ability of statistics derived from 6 different corpora, we find intuitive results showing that, e.g., a British corpus overpredicts the speed with which an American will react to the words ward and duke, and that the Google n-grams overpredicts familiarity with technology terms. This study aims to provoke increased consideration of the human language model by NLP practitioners: biases are not limited to differences between corpora (i.e. “train” vs. “test”); they can exist as well between corpora and the intended user of the resultant technology.",
  "full_text": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–12,\nBaltimore, Maryland, USA, June 23-25 2014.c⃝2014 Association for Computational Linguistics\nBiases in Predicting the Human Language Model\nAlex B. Fine\nUniversity of Illinois at Urbana-Champaign\nabfine@illinois.edu\nAustin F. Frank\nRiot Games\naufrank@riotgames.com\nT. Florian Jaeger\nUniversity of Rochester\nfjaeger@bcs.rochester.edu\nBenjamin Van Durme\nJohns Hopkins University\nvandurme@cs.jhu.edu\nAbstract\nWe consider the prediction of three hu-\nman behavioral measures – lexical deci-\nsion, word naming, and picture naming –\nthrough the lens of domain bias in lan-\nguage modeling. Contrasting the predic-\ntive ability of statistics derived from 6 dif-\nferent corpora, we ﬁnd intuitive results\nshowing that, e.g., a British corpus over-\npredicts the speed with which an Amer-\nican will react to the words ward and\nduke, and that the Google n-grams over-\npredicts familiarity with technology terms.\nThis study aims to provoke increased con-\nsideration of the human language model\nby NLP practitioners: biases are not lim-\nited to differences between corpora (i.e.\n“train” vs. “test”); they can exist as well\nbetween corpora and the intended user of\nthe resultant technology.\n1 Introduction\nComputational linguists build statistical language\nmodels for aiding in natural language processing\n(NLP) tasks. Computational psycholinguists build\nsuch models to aid in their study of human lan-\nguage processing. Errors in NLP are measured\nwith tools like precision and recall, while errors in\npsycholinguistics are deﬁned as failures to model\na target phenomenon.\nIn the current study, we exploit errors of the lat-\nter variety—failure of a language model to predict\nhuman performance—to investigate bias across\nseveral frequently used corpora in computational\nlinguistics. The human data is revealing because\nit trades on the fact that human language process-\ning is probability-sensitive: language processing\nreﬂects implicit knowledge of probabilities com-\nputed over linguistic units (e.g., words). For ex-\nample, the amount of time required to read a word\nvaries as a function of how predictable that word is\n(McDonald and Shillcock, 2003). Thus, failure of\na language model to predict human performance\nreveals a mismatch between the language model\nand the human language model, i.e., bias.\nPsycholinguists have known for some time that\nthe ability of a corpus to explain behavior depends\non properties of the corpus and the subjects (cf.\nBalota et al. (2004)). We extend that line of work\nby directly analyzing and quantifying this bias,\nand by linking the results to methodological con-\ncerns in both NLP and psycholinguistics.\nSpeciﬁcally, we predict human data from\nthree widely used psycholinguistic experimental\nparadigms—lexical decision, word naming, and\npicture naming—using unigram frequency esti-\nmates from Google n-grams (Brants and Franz,\n2006), Switchboard (Godfrey et al., 1992), spoken\nand written English portions of CELEX (Baayen\net al., 1995), and spoken and written portions\nof the British National Corpus (BNC Consor-\ntium, 2007). While we ﬁnd comparable overall\nﬁts of the behavioral data from all corpora un-\nder consideration, our analyses also reveal spe-\nciﬁc domain biases. For example, Google n-\ngrams overestimates the ease with which humans\nwill process words related to the web ( tech, code,\nsearch, site), while the Switchboard corpus—a\ncollection of informal telephone conversations be-\ntween strangers—overestimates how quickly hu-\nmans will react to colloquialisms (heck, darn) and\nbackchannels (wow, right).\n7\nFigure 1: Pairwise correlations between log frequency es-\ntimates from each corpus. Histograms show distribution over\nfrequency values from each corpus. Lower left panels give\nPearson (top) and Spearman (bottom) correlation coefﬁcients\nand associated p-values for each pair. Upper right panels plot\ncorrelations\n2 Fitting Behavioral Data\n2.1 Data\nPairwise Pearson correlation coefﬁcients for log\nfrequency were computed for all corpora under\nconsideration. Signiﬁcant correlations were found\nbetween log frequency estimates for all pairs (Fig-\nure 1). Intuitive biases are apparent in the corre-\nlations, e.g.: BNCw correlates heavily with BNCs\n(0.91), but less with SWBD (0.79), while BNCs\ncorrelates more with SWBD (0.84).1\nCorpus Size (tokens)\nGoogle n-grams (web release) ∼ 1 trillion\nBritish National Corpus (written, BNCw) ∼ 90 million\nBritish National Corpus (spoken, BNCs) ∼ 10 million\nCELEX (written, CELEXw) ∼ 16.6 million\nCELEX (spoken, CELEXs) ∼ 1.3 million\nSwitchboard (Penn Treebank subset 3) ∼ 800,000\nTable 1: Summary of the corpora under consideration.\n2.2 Approach\nWe ask whether domain biases manifest as sys-\ntematic errors in predicting human behavior. Log\nunigram frequency estimates were derived from\neach corpus and used to predict reaction times\n(RTs) from three experiments employing lexical\n1BNCw and BNCs are both British, while BNCs and\nSWBD are both spoken.\ndecision (time required by subjects to correctly\nidentify a string of letters as a word of English\n(Balota et al., 1999)); word naming (time required\nto read aloud a visually presented word (Spieler\nand Balota, 1997); (Balota and Spieler, 1998));\nand picture naming (time required to say a pic-\nture’s name (Bates et al., 2003)). Previous work\nhas shown that more frequent words lead to faster\nRTs. These three measures provide a strong test\nfor the biases present in these corpora, as they\nspan written and spoken lexical comprehension\nand production.\nTo compare the predictive strength of log fre-\nquency estimates from each corpus, we ﬁt mixed\neffects regression models to the data from each\nexperiment. As controls, all models included (1)\nmean log bigram frequency for each word, (2)\nword category (noun, verb, etc.), (3) log mor-\nphological family size (number of inﬂectional and\nderivational morphological family members), (4)\nnumber of synonyms, and (5) the ﬁrst principal\ncomponent of a host of orthographic and phono-\nlogical features capturing neighborhood effects\n(type and token counts of orthographic and phono-\nlogical neighbors as well as forward and backward\ninconsistent words; (Baayen et al., 2006)). Mod-\nels of lexical decision and word naming included\nrandom intercepts of participant age to adjust for\ndifferences in mean RTs between old (mean age\n= 72) vs. young (mean age = 23) subjects, given\ndifferences between younger vs. older adults’ pro-\ncessing speed (cf. (Ramscar et al., 2014)). (All\nparticipants in the picture naming study were col-\nlege students.)\n2.3 Results\nFor each of the six panels corresponding to fre-\nquency estimates from a corpus A, Figure 2 gives\nthe χ2 value resulting from the log-likelihood ra-\ntio of (1) a model containing A and an estimate\nfrom one of the ﬁve remaining corpora (given on\nthe x axis) and (2) a model containing just the cor-\npus indicated on the x axis. Thus, for each panel,\neach bar in Figure 2 shows the explanatory power\nof estimates from the corpus given at the top of the\npanel after controlling for estimates from each of\nthe other corpora.\nModel ﬁts reveal intuitive, previously undocu-\nmented biases in the ability of each corpus to pre-\ndict human data. For example, corpora of British\nEnglish tend to explain relatively little after con-\n8\ntrolling for other British corpora in modeling lexi-\ncal decision RTs (yellow). Similarly, Switchboard\nprovides relatively little explanatory power over\nthe other corpora in predicting picture naming\nRTs (blue bars), possibly because highly image-\nable nouns and verbs frequent in everyday interac-\ntions are underrepresented in telephone conversa-\ntions between people with no common visual ex-\nperience. In other words, idiosyncratic facts about\nthe topics, dialects, etc. represented in each cor-\npus lead to systematic patterns in how well each\ncorpus can predict human data relative to the oth-\ners. In some cases, the predictive value of one\ncorpus after controlling for another—apparently\nfor reasons related to genre, dialect—can be quite\nlarge (cf. the χ2 difference between a model with\nboth Google and Switchboard frequency estimates\ncompared to one with only Switchboard [top right\nyellow bar]).\nIn addition to comparing the overall predictive\npower of the corpora, we examined the words\nfor which behavioral predictions derived from the\ncorpora deviated most from the observed behav-\nior (word frequencies strongly over- or under-\nestimated by each corpora). First, in Table 2 we\ngive the ten words with the greatest relative differ-\nence in frequency for each corpus pair. For exam-\nple, ﬁfe is deemed more frequent according to the\nBNC than to Google.2\nThese results suggest that particular corpora\nmay be genre-biased in systematic ways. For in-\nstance, Google appears to be biased towards termi-\nnology dealing with adult material and technology.\nSimilarly, BNCw is biased, relative to Google, to-\nwards Britishisms. For these words in the BNC\nand Google, we examined errors in predicted lexi-\ncal decision times. Figure 3 plots errors in the lin-\near model’s prediction of RTs for older (top) and\nyounger (bottom) subjects.\nThe ﬁgure shows a positive correlation between\nhow large the difference is between the lexical de-\ncision RT predicted by the model and the actu-\nally observed RT, and how over-estimated the log\nfrequency of that word is in the BNC relative to\nGoogle (left panel) or in Google relative to the\nBNC (right panel). The left panel shows that BNC\nproduces a much greater estimate of the log fre-\n2Surprisingly, ﬁfe was determined to be one of the words\nwith the largest frequency asymmetry between Switchboard\nand the Google n-grams corpus. This was a result of lower-\ncasing all of the words in in the analyses, and the fact that\nBarney Fife was mentioned several times in the BNC.\nquency of the word lee relative to Google, which\nleads the model to predict a lower RT for this word\nthan is observed (i.e., the error is positive; though\nnote that the error is less severe for older relative to\nyounger subjects). By contrast, the asymmetry be-\ntween the two corpora in the estimated frequency\nof sir is less severe, so the observed RT deviates\nless from the predicted RT. In the right panel, we\nsee that Google assigns a much greater estimate\nof log frequency to the word tech than the BNC,\nwhich leads a model predicting RTs from Google-\nderived frequency estimates to predict a far lower\nRT for this word than observed.\n3 Discussion\nResearchers in computational linguistics often as-\nsume that more data is always better than less\ndata (Banko and Brill, 2001). This is true in-\nsofar as larger corpora allow computational lin-\nguists to generate less noisy estimates of the av-\nerage language experience of the users of compu-\ntational linguistics applications. However, corpus\nsize does not necessarily eliminate certain types of\nbiases in estimates of human linguistic experience,\nas demonstrated in Figure 3.\nOur analyses reveal that 6 commonly used cor-\npora fail to reﬂect the human language model in\nvarious ways related to dialect, modality, and other\nproperties of each corpus. Our results point to\na type of bias in commonly used language mod-\nels that has been previously overlooked. This bias\nmay limit the effectiveness of NLP algorithms in-\ntended to generalize to a linguistic domains whose\nstatistical properties are generated by humans.\nFor psycholinguists these results support an im-\nportant methodological point: while each corpus\npresents systematic biases in how well it predicts\nhuman behavior, all six corpora are, on the whole,\nof comparable predictive value and, speciﬁcally,\nthe results suggest that the web performs as well\nas traditional instruments in predicting behavior.\nThis has two implications for psycholinguistic re-\nsearch. First, as argued by researchers such as\nLew (2009), given the size of the Web compared to\nother corpora, research focusing on low-frequency\nlinguistic events—or requiring knowledge of the\ndistributional characteristics of varied contexts—\nis now more tractable. Second, the viability of\nthe web in predicting behavior opens up possibil-\nities for computational psycholinguistic research\nin languages for which no corpora exist (i.e., most\n9\nCELEX written BNC written Google\nCELEX spoken BNC spoken Switchboard\n0\n40\n80\n120\n0\n10\n20\n30\n40\n0\n10\n20\n30\n0\n10\n20\n30\n0\n10\n20\n30\n40\n0\n5\n10\nCELEX written\nBNC written\nGoogle\nCELEX spoken\nBNC spoken\nSwitchboard\nCELEX written\nBNC written\nGoogle\nCELEX spoken\nBNC spoken\nSwitchboard\nCELEX written\nBNC written\nGoogle\nCELEX spoken\nBNC spoken\nSwitchboard\nComparison\nχΛ\n2\ntask\nlexical decision\npicture naming\nword naming\nPairwise model comparisons\nFigure 2: Results of log likelihood ratio model comparisons. Large values indicate that the reference predictor (panel title)\nexplained a large amount of variance over and above the predictor given on the x-axis.\nGoogle-and-BNC-written\nStandardized-difference-score\nError-in-linear-model\ncentcent\ndamedame\ndoledoledukeduke\nfifefife\nglenglen\ngodgod\ngulfgulf\nhallhall\nhankhank\nkingking\nleelee\nlordlord marchmarchmarchmarchmarchmarchmarchmarch\nnicknick\nprimeprime\nprinceprince\nsirsir wardward\ncentcent damedame\ndoledole\ndukeduke\nfifefife\nglenglen\ngodgod\ngulfgulf\nhallhallhankhank\nkingking\nleelee\nlordlord marchmarchmarchmarchmarchmarchmarchmarch\nnicknick\nprimeprimeprinceprince\nsirsir\nwardward\nassass\nbinbin\nbugbug\nbuttbuttbuttbuttbuttbuttbuttbutt cartcartchatchat clickclick\ncodecode\ndarndarn\ndenden\ndialdialdikedikefilefileflipflip gaygayheckheckhophop hunkhunklinklink loglog mailmail\nmapmap pagepage\npeepee\nprepprep\nprintprint\nquotequote\nranchranchscriptscript\nsearchsearch\nselfself\nsexsex\nsitesite\nskipskipslotslotstorestore\nsucksuck\ntagtag\ntechtech\nteensteens\nthreadthread\ntiretire\ntoetoe\ntwaintwain\nwebwebwhizwhiz\nwowwow\nzipzip\nassass\nbinbin\nbugbugbuttbuttbuttbuttbuttbuttbuttbutt\ncartcartchatchat clickclick\ncodecode\ndarndarndenden\ndialdial\ndikedike\nfilefile\nflipflip\ngaygay\nheckheck\nhophop hunkhunk\nlinklink loglog mailmailmapmap pagepage peepeeprepprep\nprintprint quotequoteranchranchscriptscript\nsearchsearch\nselfself\nsexsex\nsitesite\nskipskip\nslotslot\nstorestore\nsucksucktagtag\ntechtech\nteensteens\nthreadthread\ntiretire\ntoetoe\ntwaintwain webweb\nwhizwhiz\nwowwow\nzipzip\n-0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n-0.1\n0.0\n0.1\n0.2\n0.3\n0.4\noldyoung\n-3.5 -3.0 -2.5 2.5 3.0 3.5 4.0 4.5 5.0 5.5\nGoogle-<-BNC-written Google->-BNC-written\ngoog.f\n-4\n-2\n0\n2\nFigure 3: Errors in the linear model predicting lexical decision RTs from log frequency are plotted against the standardized\ndifference in log frequency in the Google n-grams corpus versus the written portion of the BNC. Top and bottom panels show\nerrors for older and younger subjects, respectively. The left panel plots words with much greater frequency in the written\nportion of the BNC relative to Google; the right panel plots words occurring more frequently in Google. Errors in the linear\nmodel are plotted against the standardized difference in log frequency across the corpora, and word color encodes the degree to\nwhich each word is more (red) or less (blue) frequent in Google. That the ﬁt line in each graph is above 0 in the y-axis means\nthat on average these biased words in each domain are being over-predicted, i.e., the corpus frequencies suggest humans will\nreact (sometimes much) faster than they actually did in the lab.\n10\nGreater Lesser Top-10\ngoogle bnc.s web, ass, gay, tire, text, tool, code, woe, site, zip\ngoogle bnc.w ass, teens, tech, gay, bug, suck, site, cart, log, search\ngoogle celex.s teens, cart, gay, zip, mail, bin, tech, click, pee, site\ngoogle celex.w web, full, gay, bin, mail, zip, site, sake, ass, log\ngoogle swbd gay, thread, text, search, site, link, teens, seek, post, sex\nbnc.w google ﬁfe, lord, duke, march, dole, god, cent, nick, dame, draught\nbnc.w bnc.s pact, corps, foe, tract, hike, ridge, dine, crest, aide, whim\nbnc.w celex.s staff, nick, full, waist, ham, lap, knit, sheer, bail, march\nbnc.w celex.w staff, lord, last, nick, fair, glen, low, march, should, west\nbnc.w swbd rose, prince, seek, cent, text, clause, keen, breach, soul, rise\ncelex.s google art, yes, pound, spoke, think, mean, say, thing, go, drove\ncelex.s bnc.s art, hike, pact, howl, ski, corps, peer, spoke, jazz, are\ncelex.s bnc.w art, yes, dike, think, thing, sort, mean, write, pound, lot\ncelex.s celex.w yes, sort, thank, think, jazz, heck, tape, well, ﬁfe, get\ncelex.s swbd art, cell, rose, spoke, aim, seek, shall, seed, text, knight\ncelex.w google art, plod, pound, shake, spoke, dine, howl, sit, say, draught\ncelex.w bnc.s hunch, stare, strife, hike, woe, aide, rout, yell, glaze, ﬂee\ncelex.w bnc.w dike, whiz, dine, shake, grind, jerk, whoop, say, are, cram\ncelex.w celex.s wrist, pill, lawn, clutch, stare, spray, jar, shark, plead, horn\ncelex.w swbd art, rose, seek, aim, rise, burst, seed, cheek, grin, lip\nswbd google mow, kind, lot, think, ﬁfe, corps, right, cook, sort, do\nswbd bnc.s creek, mow, guess, pact, strife, tract, hank, howl, foe, nap\nswbd bnc.w stuff, whiz, tech, lot, kind, creek, darn, dike, bet, kid\nswbd celex.s wow, sauce, mall, deck, full, spray, ﬂute, rib, guy, bunch\nswbd celex.w heck, guess, right, full, stuff, lot, last, well, guy, fair\nTable 2: Examples of words with largest difference in z-transformed log frequencies (e.g., the relative frequencies of ﬁfe,\nlord, and duke, in the BNC are far greater than in Google).\nlanguages). This furthers the arguments of the “the\nweb as corpus” community (Kilgarriff and Grefen-\nstette, 2003) with respect to psycholinguistics.\nFinally, combining multiple sources of fre-\nquency estimates is one way researchers may be\nable to reduce the prediction bias from any sin-\ngle corpus. This relates to work in automatically\nbuilding domain speciﬁc corpora (e.g., Moore and\nLewis (2010), Axelrod et al. (2011), Daum ´e III\nand Jagarlamudi (2011), Wang et al. (2014), Gao\net al. (2002), and Lin et al. (1997)). Those efforts\nfocus on building representative document collec-\ntions for a target domain, usually based on a seed\nset of initial documents. Our results prompt the\nquestion: can one use human behavior as the tar-\nget in the construction of such a corpus? Con-\ncretely, can we build corpora by optimizing an ob-\njective measure that minimizes error in predicting\nhuman reaction times? Prior work in building bal-\nanced corpora used either rough estimates of the\nratio of genre styles a normal human is exposed to\ndaily (e.g., the Brown corpus (Kucera and Fran-\ncis, 1967)), or simply sampled text evenly across\ngenres (e.g., COCA: the Corpus of Contemporary\nAmerican English (Davies, 2009)). Just as lan-\nguage models have been used to predict reading\ngrade-level of documents (Collins-Thompson and\nCallan, 2004), human language models could be\nused to predict the appropriateness of a document\nfor inclusion in an “automatically balanced” cor-\npus.\n4 Conclusion\nWe have shown intuitive, domain-speciﬁc biases\nin the prediction of human behavioral measures\nvia corpora of various genres. While some psy-\ncholinguists have previously acknowledged that\ndifferent corpora carry different predictive power,\nthis is the ﬁrst work to our knowledge to system-\natically document these biases across a range of\ncorpora, and to relate these predictive errors to do-\nmain bias, a pressing issue in the NLP community.\nWith these results in hand, future work may now\nconsider the automatic construction of a “prop-\nerly” balanced text collection, such as originally\ndesired by the creators of the Brown corpus.\nAcknowledgments\nThe authors wish to thank three anonymous ACL\nreviewers for helpful feedback. This research\nwas supported by a DARPA award (FA8750-13-2-\n0017) and NSF grant IIS-0916599 to BVD, NSF\nIIS-1150028 CAREER Award and Alfred P. Sloan\nFellowship to TFJ, and an NSF Graduate Research\nFellowship to ABF.\n11\nReferences\nA. Axelrod, X. He, and J. Gao. 2011. Domain adap-\ntation via pseudo in-domain data selection. In Pro-\nceedings of the Conference on Empirical Methods in\nNatural Language Processing (EMNLP 11).\nR. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.\nThe CELEX Lexical Database (Release 2). Linguis-\ntic Data Consortium, Philadelphia.\nR. H. Baayen, L. F. Feldman, and R. Schreuder.\n2006. Morphological inﬂuences on the recognition\nof monosyllabic monomorphemic words. Journal of\nMemory and Language, 53:496–512.\nD. A. Balota and D. H. Spieler. 1998. The utility of\nitem-level analyses in model evaluation: A reply to\nSeidenberg & Plaut (1998). Psychological Science.\nD. A. Balota, M. J. Cortese, and M. Pilotti. 1999. Item-\nlevel analyses of lexical decision performance: Re-\nsults from a mega-study. InAbstracts of the 40th An-\nnual Meeting of the Psychonomics Society, page 44.\nD. Balota, M. Cortese, S. Sergent-Marshall, D. Spieler,\nand M. Yap. 2004. Visual word recognition for\nsingle-syllable words. Journal of Experimental Psy-\nchology:General, (133):283316.\nM. Banko and E. Brill. 2001. Mitigating the paucity of\ndata problem. Human Language Technology.\nE. Bates, S. D’Amico, T. Jacobsen, A. Szkely, E. An-\ndonova, A. Devescovi, D. Herron, CC Lu, T. Pech-\nmann, C. Plh, N. Wicha, K. Federmeier, I. Gerd-\njikova, G. Gutierrez, D. Hung, J. Hsu, G. Iyer,\nK. Kohnert, T. Mehotcheva, A. Orozco-Figueroa,\nA. Tzeng, and O. Tzeng. 2003. Timed picture nam-\ning in seven languages. Psychonomic Bulletin & Re-\nview, 10(2):344–380.\nBNC Consortium. 2007. The British National Corpus,\nversion 3 (BNC XML Edition). Distributed by Ox-\nford University Computing Services on behalf of the\nBNC Consortium.\nT. Brants and A. Franz. 2006. Web 1T 5-gram Version\n1. Linguistic Data Consortium (LDC).\nKevyn Collins-Thompson and James P. Callan. 2004.\nA language modeling approach to predicting reading\ndifﬁculty. In HLT-NAACL, pages 193–200.\nH. Daum ´e III and J. Jagarlamudi. 2011. Domain\nadaptation for machine translation by mining unseen\nwords. In Proceedings of the 49th Annual Meet-\ning of the Association for Computational Linguis-\ntics: Human Language Technologies (ACL-HLT 11).\nM. Davies. 2009. The 385+ million word corpus of\ncontemporary american english (19902008+): De-\nsign, architecture, and linguistic insights. Inter-\nnational Journal of Corpus Linguistics , 14(2):159–\n190.\nJ. Gao, J. Goodman, M. Li, and K. F. Lee. 2002. To-\nward a uniﬁed approach to statistical language mod-\neling for chinese. In Proceedings of the ACM Trans-\nactions on Asian Language Information Processing\n(TALIP 02).\nJ. Godfrey, E. Holliman, and J. McDaniel. 1992.\nSWITCHBOARD: Telephone Speech Corpus for\nResearch and Development. In Proceedings of\nICASSP-92, pages 517–520.\nA. Kilgarriff and G. Grefenstette. 2003. Introduction\nto the special issue on the web as corpus. Computa-\ntional Linguistics, 29(3):333–348.\nH. Kucera and W.N. Francis. 1967. Computational\nanalysis of present-day american english. provi-\ndence, ri: Brown university press.\nR. Lew, 2009. Contemporary Corpus Linguistics ,\nchapter The Web as corpus versus traditional cor-\npora: Their relative utility for linguists and language\nlearners, pages 289–300. London/New York: Con-\ntinuum.\nS. C. Lin, C. L. Tsai, L. F. Chien, K. J. Chen, and\nL. S. Lee. 1997. Chinese language model adapta-\ntion based on document classiﬁcation and multiple\ndomain-speciﬁc language models. In Proceedings\nof the 5th European Conference on Speech Commu-\nnication and Technology.\nS.A. McDonald and R.C. Shillcock. 2003. Eye\nmovements reveal the on-line computation of lexical\nprobabilities during reading. Psychological science,\n14(6):648–52, November.\nR. C. Moore and W. Lewis. 2010. Intelligent selection\nof language model training data. In Proceedings of\nthe 48th Annual Meeting of the Association for Com-\nputational Linguistics (ACL 10).\nM. Ramscar, P. Hendrix, C. Shaoul, P. Milin, and R. H.\nBaayen. 2014. The myth of cognitive decline: non-\nlinear dynamics of lifelong learning. Topics in Cog-\nnitive Science, 32:5–42.\nD. H. Spieler and D. A. Balota. 1997. Bringing com-\nputational models of word naming down to the item\nlevel. 6:411–416.\nL. Wang, D.F. Wong, L.S. Chao, Y . Lu, and J. Xing.\n2014. A systematic comparison of data selection\ncriteria for smt domain adaptation. The Scientiﬁc\nWorld Journal.\n12",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7378818392753601
    },
    {
      "name": "Language model",
      "score": 0.7271921634674072
    },
    {
      "name": "Natural language processing",
      "score": 0.7121717929840088
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6363328099250793
    },
    {
      "name": "Word (group theory)",
      "score": 0.5721845030784607
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5265790820121765
    },
    {
      "name": "Linguistics",
      "score": 0.30300503969192505
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I5388228",
      "name": "University of Rochester",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    }
  ],
  "cited_by": 10
}