{
  "title": "Autoregressive Structured Prediction with Language Models",
  "url": "https://openalex.org/W4385573907",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2102556518",
      "name": "Tianyu Liu",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5021488526",
      "name": "Yuchen Eleanor Jiang",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2613919196",
      "name": "Nicholas Monath",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2148165152",
      "name": "Ryan Cotterell",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2035754532",
      "name": "Mrinmaya Sachan",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2155069789",
    "https://openalex.org/W3034797437",
    "https://openalex.org/W2970550868",
    "https://openalex.org/W3014096773",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2785442381",
    "https://openalex.org/W3167136668",
    "https://openalex.org/W3120250095",
    "https://openalex.org/W1869752048",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3121525843",
    "https://openalex.org/W4210956481",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2964222246",
    "https://openalex.org/W3106340866",
    "https://openalex.org/W3105063288",
    "https://openalex.org/W4236265809",
    "https://openalex.org/W4320013820",
    "https://openalex.org/W4287888092",
    "https://openalex.org/W4285305471",
    "https://openalex.org/W2158349948",
    "https://openalex.org/W3034862440",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W1850531616",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2962836003",
    "https://openalex.org/W2740765036",
    "https://openalex.org/W3091432621",
    "https://openalex.org/W4224874866",
    "https://openalex.org/W2963087868",
    "https://openalex.org/W2935052563",
    "https://openalex.org/W1598566484",
    "https://openalex.org/W1538169703"
  ],
  "abstract": "Recent years have seen a paradigm shift in NLP towards using pretrained language models (PLM) for a wide range of tasks. However, there are many difficult design decisions to represent structures (e.g. tagged text, coreference chains) in a way such that they can be captured by PLMs. Prior work on structured prediction with PLMs typically flattens the structured output into a sequence, which limits the quality of structural information being learned and leads to inferior performance compared to classic discriminative models. In this work, we describe an approach to model structures as sequences of actions in an autoregressive manner with PLMs, allowing in-structure dependencies to be learned without any loss. Our approach achieves the new state-of-the-art on all the structured prediction tasks we looked at, namely, named entity recognition, end-to-end relation extraction, and coreference resolution.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 993‚Äì1005\nDecember 7-11, 2022 ¬©2022 Association for Computational Linguistics\nAutoregressive Structured Prediction with Language Models\nTianyu LiuŒ∂ Yuchen Eleanor JiangŒ∂\nNicholas MonathŒ≥ Ryan CotterellŒ∂ Mrinmaya SachanŒ∂\nŒ∂ETH Z√ºrich Œ≥Google Research\n{tianyu.liu, yuchen.jiang}@inf.ethz.ch\nnmonath@google.com {ryan.cotterell, mrinmaya.sachan}@inf.ethz.ch\nAbstract\nIn recent years, NLP has moved towards the\napplication of language models to a more\ndiverse set of tasks. However, applying\nlanguage models to structured prediction, e.g.,\npredicting parse trees, taggings, and coref-\nerence chains, is not straightforward. Prior\nwork on language model-based structured\nprediction typically Ô¨Çattens the target structure\ninto a string to easily Ô¨Åt it into the language\nmodeling framework. Such Ô¨Çattening limits\nthe accessibility of structural information and\ncan lead to inferior performance compared to\napproaches that overtly model the structure.\nIn this work, we propose to construct a\nconditional language model over sequences\nof structure-building actions, rather than over\nstrings in a way that makes it easier for the\nmodel to pick up on intra-structure dependen-\ncies. Our method sets the new state of the\nart on named entity recognition, end-to-end\nrelation extraction, and coreference resolution.\nhttps://github.com/lyutyuh/ASP\n1 Introduction\nMany common NLP tasks, e.g., named entity\nrecognition, relation extraction, and coreference\nresolution are naturally taxonomized as structured\nprediction, the supervised machine-learning task of\npredicting a structure from a large1 set. To general-\nize well to held-out data in a structured prediction\nproblem, the received wisdom has been that it\nis necessary to correctly model complex depen-\ndencies between different pieces of the structure.\nHowever, a recent trend in structured prediction\nfor language has been to forgo explicitly modeling\nsuch dependencies (Ma and Hovy, 2016; Lee et al.,\n2017; He et al., 2017, inter alia), and, instead, to\napply an expressive black-box model, e.g., a neural\nnetwork, with the hope that the model picks up on\nthe dependencies without explicit instruction.\n1Typically, large means exponential in the size of the input.\nFraming structured prediction as conditional\nlanguage modeling is an increasingly common\nblack-box technique for building structured predic-\ntors that has led to empirical success (Vinyals et al.,\n2015; Raffel et al., 2020; Athiwaratkun et al., 2020;\nDe Cao et al., 2021; Paolini et al., 2021, inter alia).\nThe idea behind the framework is to encode the tar-\nget structure as a string, Ô¨Çattening out the structure.\nThen, one uses a conditional language model to\npredict the Ô¨Çattened string encoding the structure.\nFor instance, Vinyals et al. (2015) Ô¨Çatten parse\ntrees into strings and predict the strings encoding\nthe Ô¨Çattened trees from the sentence with a machine\ntranslation architecture. The hope is that the au-\ntoregressive nature of the language model allows it\nto learn to model the intra-structure dependencies\nand the necessary hard constraints that ensure the\nmodel even produces well-formed structures. Ad-\nditionally, many modelers make use of pre-trained\nlanguage models (Lewis et al., 2020; Raffel et al.,\n2020) to further improve the language models.\nHowever, despite their empirical success, simply\nhoping that a black-box approach correctly models\nintricate intra-structure dependencies is often\ninsufÔ¨Åcient for highly structured tasks (Paolini\net al., 2021, ¬ß1). Indeed, the act of Ô¨Çattening a\nstructured object into a string makes properly mod-\neling the intra-structure dependencies harder for\nmany tasks, e.g., those that involve nested spans or\nlong-distance dependencies. For instance, in coref-\nerence resolution, a conference link between two\nmentions can stretch across thousands of words,\nand a coreference chain can also contain over a\nhundred mentions (Pradhan et al., 2012). Flatten-\ning such a large amount of structured information\ninto a string makes the task more difÔ¨Åcult to model.\nIn this paper, we propose a simple framework\nthat augments a conditional language model with\nexplicit modeling of structure. Instead of modeling\nstrings that encode a Ô¨Çattened representation of\nthe target structure, we model a constrained set\nof actions that build the target structure step by\nstep; see Fig. 1 for an example of our proposed\n993\n[*US]PresidentJoeBiden] tookofficein2021.Previously, [*he]wasthesenatorof[*Delaware] .\nùú∫ùú∫#1ùú∫ ùú∫ùú∫#1ùú∫ ùú∫ ùú∫ ùú∫ ùú∫ ùú∫ ùú∫ ùú∫ ùú∫ #2ùú∫ ùú∫ ùú∫ ùú∫ùú∫ ùú∫ #3ùú∫\nùú∫ùú∫ùú∫ùú∫LOCùú∫ ùú∫ùú∫ ùú∫ùú∫ùú∫ùú∫PERùú∫ ùú∫ùú∫ ùú∫ùú∫ ùú∫ùú∫ ùú∫ùú∫ ùú∫ùú∫ ùú∫ùú∫ ùú∫ùú∫ ùú∫ùú∫ ùú∫ùú∫ ùú∫ùú∫ ùú∫ùú∫ ùú∫ùú∫ ùú∫ùú∫ ùú∫ùú∫ùú∫ùú∫ ùú∫ùú∫ LOCLive_inùú∫ùú∫\nùú∫ùú∫ùú∫ ùú∫ ùú∫ùú∫ ùú∫ ùú∫ ùú∫ ùú∫ ùú∫ ùú∫ ùú∫ ùú∫ ùú∫ ùú∫ #2ùú∫ ùú∫ ùú∫ ùú∫ùú∫ ùú∫ ùú∫ ùú∫\na\"\nASP:US President Joe Biden took office in 2021. Previously, he was the senator of Delaware.INPUT\nb\"\nz\"(ERE)\n[* US ]PresidentJoe Biden ]took office in 2021. Previously, [* he ]was the senator of [* Delaware ] .\nz\"(COREF)\nFigure 1: Illustration of the target outputs of our framework on coreference resolution ( COREF ) and end-to-end\nrelation extraction ( ERE ). The lower part illustrates the decoding process of our model. The actions yi are\ncolor-coded as ] , [‚àó and copy . The structure random variables zi are presented along with coreference links or\nrelation links. We present words in the copy cells merely as an illustration.\nframework. Training a conditional language model\nto predict structure-building actions exposes the\nstructure in a way that allows the model to pick\nup on the intra-structure dependencies more easily\nwhile still allowing the modeler to leverage pre-\ntrained language models. We conduct experiments\non three structured prediction tasks: named entity\nrecognition, end-to-end relation extraction, and\ncoreference resolution. On each task, we achieve\nstate-of-the-art results without relying on data\naugmentation or task-speciÔ¨Åc feature engineering.\n2 Autoregressive Structured Prediction\nIn this section, we describe our proposed approach,\nwhich we refer to as autoregressive structured\nprediction (ASP). Unlike previous approaches for\nstructured prediction based on conditional language\nmodeling, we represent structures as sequences of\nactions, which build pieces of the target structure\nstep by step. For instance, in the task of coreference\nresolution, the actions build spans as well as the\nrelations between the spans, contiguous sequences\nof tokens. We give an example in Fig. 1.\n2.1 Representing Structures with Actions\nWhile our approach to structured prediction, ASP,\nis quite general, our paper narrowly focuses on\nmodeling structures that are expressible as a set\nof dependent spans, and we couch the technical\nexposition in terms of modeling spans and relation-\nships among spans. Our goal is to predict an action\nsequence y = y1,...,y N, where each action yn is\nchosen from an action space Yn. In this work, we\ntake Yn to be factored, i.e., Yn\ndef\n= A√óBn √óZn,\nwhere Ais a set of structure-building actions, Bn\nis the set of bracket-pairing actions, and Zn is a\nset of span-labeling actions. Thus, each yn may be\nexpressed as a triple, i.e., yn = ‚ü®an,bn,zn‚ü©. We\ndiscuss each set in a separate paragraph below.\nStructure-Building Actions. We Ô¨Årst de-\nÔ¨Åne a set of structure-building actions\nA=\n{\n] , [‚àó, copy\n}\nthat allow us to encode the\nspan structure of a text, e.g., [‚àóDelaware ] in\nFig. 1 encodes that Delaware is a span of interest.\nMore technically, the action ] refers to a right\nbracket that marks the right-most part of a span.\nThe action [‚àó refers to a left bracket that marks\nthe left-most part of a span. The superscript ‚àóon\n[‚àó is inspired by the Kleene star and indicates\nthat it is a placeholder for 0 or more consecutive\nleft brackets2. Finally, copy refers to copying a\nword from the input document. To see how these\nactions come together to form a span, consider the\nsubsequence in Fig. 1, [‚àóDelaware ] , which is\ngenerated from a sequence of structure-building\nactions [‚àó, copy , and ] .\nBracket-Pairing Actions. Next, we develop the\nset of actions that allow the model to match left\nand right brackets; we term these bracket-pairing\nactions. The set of bracket-pairing actions consists\nof all previously constructed left brackets, i.e.,\nBn =\n{\nm|m<n ‚àßam = [‚àó\n}\n(1)\nThus, in general, |Bn|is O(n). However, it is\noften the case that domain-speciÔ¨Åc knowledge can\n2In our preliminary experiments, we observe unsatisfactory\nperformance when the model has to generate consecutive left\nbrackets. We leverage [‚àó as an engineering workaround. We\nhypothesize that this phenomenon is due to the inability of\ntransformers to recognize Dyck languages (Hahn, 2020; Hao\net al., 2022).\n994\nbe used to prune Bn. For instance, coreference\nmentions and named entities rarely cross sentence\nboundaries, which yields a linguistically motivated\npruning strategy (Liu et al., 2022). Thus, in some\ncases, the cardinality of Bn can be signiÔ¨Åcantly\nsmaller. When we decode action sequences y into\na structure, unpaired [‚àó and ] can be removed\nensuring that the output of the model will not\ncontain unpaired brackets.\nSpan-Labeling Actions. Finally, we add addi-\ntional symbols zn associated with each yn that en-\ncode a labeling of a single span or a relationship\nbetween two or more spans. For instance, see ¬ß2.3\nfor an example. We denote the set of all zn as\nZn =\n{\nm|m<n ‚àßam = ]\n}\n√óL (2)\nwhere\n{\nm |m < n‚àßam = ]\n}\nis the set of\nprevious spans, which allows the model to capture\nintra-span relationships, and Ldenotes the set of\npossible labelings of the current span and the re-\nlationship between the adjoined spans. In general,\ndesigning Zn requires some task-speciÔ¨Åc knowl-\nedge in order to specify the label space. However,\nwe contend it requires less effort than designing\na Ô¨Çattened string output where different levels of\nstructures may be intertwined (Paolini et al., 2021).\n2.2 Model Parameterization\nLet D= w1,..., wK be an input document of K\nsentences where wk denotes the kth sentence in D.\nWe Ô¨Årst convert the structure to be built on top of\nDinto an action sequence, which we denote as y\nwhere yn ‚ààYn. Now, we model the sequence of\nactions y as a conditional language model\npŒ∏(y |D) =\nN‚àè\nn=1\npŒ∏(yn |y<n,D) (3)\nThe log-likelihood of the model is then given\nby log pŒ∏(y |D) = ‚àëN\nn=1 log pŒ∏(yn |y<n,D).\nWe model the local conditional probabilities\np(yn |y<n,D) as a softmax over adynamic set Yn\nthat changes as a function of the history y<n, i.e.,\npŒ∏(yn |y<n,D) = exp sŒ∏(yn)‚àë\ny‚Ä≤n‚ààYn exp sŒ∏(y‚Ä≤n) (4)\nwhere sŒ∏ is a parameterized score function; we\ndiscuss several speciÔ¨Åc instantiations of sŒ∏ in\n¬ß2.3. Finally, we note that the use of a dynamic\nvocabulary stands in contrast to most conditional\nlanguage models where the vocabulary is held\nconstant across time steps, e.g., Sutskever et al.‚Äôs\n(2014) approach to machine translation.\nGreedy Decoding. We determine the approxi-\nmate best sequence y‚àó using a greedy decoding\nstrategy. At decoding step n, we compute\ny‚àó\nn = argmax\ny‚Ä≤n\npŒ∏(y‚Ä≤\nn |y<n,D) (5)\nThe chosen y‚àó\nn = ‚ü®a‚àó\nn,b‚àó\nn,z‚àó\nn‚ü©will then be verbal-\nized as a token as follows: If a‚àó\nn = copy , then\nwe copy the next token from the input that is not\npresent in the output. Otherwise, if y‚àó\nn = [‚àó or\ny‚àó\nn = ] , we insert [‚àó or ] into the output se-\nquence, respectively. The verbalized token is then\nfed into the conditional language model at the next\nstep. The decoding process terminates when the\nmodel copies a distinguished symbol EOS symbol\nfrom the input. The end of the procedure yields an\napproximate argmax y‚àó.\nComputational Complexity. Eq. (4) can be\ncomputed quite efÔ¨Åciently using our framework,\nas the cardinalities of Ais O(1), and the size of\nBn and Zn are both O(n). A tighter analysis says\nthe cardinalities of Bn and Zn are roughly linear\nin the number of spans predicted. In practice, we\nhave n‚â™|V|where |V|is the size of vocabulary,\nwhich is the step-wise complexity of (Paolini et al.,\n2021). A quantitative analysis of the number of\nmentions in coreference can be found in App. B.\nGenerality. Despite our exposition‚Äôs focus on\ntasks that involve assigning labels to span or span\npairs, our method is quite general. Indeed, almost\nany structured prediction task can be encoded by a\nseries of structure-building actions. For tasks that\ninvolve labeling tuples of spans, e.g., semantic role\nlabeling makes use of tree-tuples that consist of the\nsubject, predicate, and object, Eq. (2) can be easily\nextended with a new space of categorical variables{\nm|m<n ‚àßam = ]\n}\nto model the extra item.\n2.3 Task-speciÔ¨Åc Parameterizations\nWe now demonstrate how to apply ASP to three\nlanguage structured prediction tasks: named entity\nrecognition, coreference resolution, and end-to-end\nrelation extraction.\nNamed Entity Recognition. Named entity\nrecognition is the task of labeling all mention spans\nE= {en}|E|\nn=1 in a documentDthat refers to named\n995\nentities. Since named entity recognition only re-\nquires labeling spans (and not linking them), we\nonly need our task-speciÔ¨Åc zn to encode the en-\ntity type, which is canonically taken from a set of\npre-deÔ¨Åned categories C. The function sŒ∏(yn) in\nEq. (4) is implemented by a feed-forward network\nsŒ∏(yn = ‚ü®an,bn,zn‚ü©) (6)\ndef\n=\n{\nFFNzn\nan(mn) if an = ]\nFFNan(hn) otherwise\nwhere hn is the decoder hidden state at step n, a\ncolumn vector, and mn = [h‚ä§\nn; h‚ä§\nbn]‚ä§represents\nthe mention that corresponds to yn. Note that\neach FFNzn\nan and FFNan represent independent\nfeed-forward networks with no shared parameters.\nEnd-to-End Relation Extraction. End-to-end\nrelation extraction is the task of jointly extract-\ning a set of entities alongside a set of relations\nbetween pairs of extracted entities. Formally, given\na set of pre-deÔ¨Åned entity categories Cand a set\nof pre-deÔ¨Åned relations R. The goal is (i) to\nidentify all possible entities E = {en}|E|\nn=1 in D\nthat could be associated with one of the entity\ntypes cin Cand (ii) to identify all possible triples\nT = {(en,rn,e‚Ä≤\nn)}|T|\nn=1 in Dwhere en,e‚Ä≤\nn ‚ààE are\nthe head and tail entity and rn ‚ààR is the rela-\ntion between en and e‚Ä≤\nn. Here, the support of zn\ntakes the form of Eq. (2), where Lis instantiated\nas C√óR . And sŒ∏(yn) kept the same as in Eq. (6).\nCoreference Resolution. The task of corefer-\nence resolution involves identifying all mention\nspans E= {en}|E|\nn=1 in Dand then clustering them.\nHowever, in addition to identifying the mention\nspans, the task of coreference resolution requires us\nto assign an antecedent to every possible mention in\nD. To encode coreference resolution in our frame-\nwork, we consider the task-speciÔ¨Åc zn from the set\nZn =\n{\nm|m<n ‚àßam = ]\n}\n‚à™{œµ} (7)\nwhere we follow the convention set in Lee et al.\n(2017) that the antecedent of the Ô¨Årst mention in\neach coreference chain is deÔ¨Åned to be œµ. Again,\nwe deÔ¨Åne sŒ∏(yn = ‚ü®an,bn,zn‚ü©) as in Eq. (6)\nwith the exception that, when zn = œµ, we deÔ¨Åne\nFFNan(mn)œµ = FFNan(mn).\n3 Experiments\nWe experiment on three NLP structured prediction\ntasks: named entity recognition, end-to-end rela-\ntion extraction, and coreference resolution. We are\nPrec. Rec. F1\nMa and Hovy (2016) 91.4 91.1 91.2\nDevlin et al. + BERT L - - 92.8\nYe et al. + ROBERTA L - - 94.0\nAthiwaratkun et al. - - 91.5\nPaolini et al. + T 5 B - - 91.7\nASP + T 5 B 91.4 92.2 91.8\nASP + T 5 L 92.1 93.4 92.8\nASP + T 5 3 B 93.8 94.4 94.1\nTable 1: Test F1 scores of named entity recognition on\nthe CoNLL-03 test set.\nprimarily interested in understanding whether ASP\nprovides advantages over two existing formalisms:\n(i) conditional language models (Athiwaratkun\net al., 2020; Paolini et al., 2021) that Ô¨Çatten\nthe structure into a string (augmented language\nmodels), and (ii) the classic discriminative models\nwhose autoregressivity is bounded. We experiment\nwith three pre-trained language models, T5 (Raffel\net al., 2020), T0 (Sanh et al., 2021), and Flan-T5\n(Chung et al., 2022) for the three tasks under\nconsideration. Additional experimental details are\ngiven in App. A.1 and App. A.2.\n3.1 Named Entity Recognition\nFirst, we evaluate our model on the CoNLL-03\nEnglish NER task. Following previous work, we\nreport the micro precision, recall, and F1 score. As\nshown in Tab. 1, our model using T0-3B backbone\noutperforms all other models without data augmen-\ntation or ensembling.\n3.2 End-to-End Relation Extraction\nWe compare ASP on the CoNLL-04 and ACE-\n05 English end-to-end relation extraction datasets.\nThe results are shown in Tab. 2 and Tab. 3. Our\nproposed approach achieves state-of-the-art results\non both datasets using T5-3B as the backbone. In\nparticular, it outperforms the Ô¨Çattened-string model\nof Paolini et al. (2021) by a large margin ( > 0.9\nF1). We hypothesize that this is due to relations re-\nquiring higher-order dependencies between spans.\n3Ye et al. (2022) counts symmetric relations twice for\nevaluation, which is inconsistent with previous work. We\nreport the re-evaluated scores under the standard metric.\n4On ACE-05, we observe inferior performance using T0-\n3B instead of T5-3B. We suspect this is due to systematic\ndeÔ¨Åciencies in dataset preprocessing, e.g., errors during senten-\ncization and tokenization as well as inconsistent capitalization.\n996\nEnt Rel\nEberts and Ulges (2020) 88.9 71.5\nZhao et al. (2020) 88.9 71.9\nWang and Lu + ALBERT XXL 90.1 73.8\nPaolini et al. + T 5 B 89.4 71.4\nASP + T 5 B 89.5 73.2\nASP + T 0 3 B 90.3 76.3\nTable 2: Micro F1 scores of entity extraction and re-\nlation extraction on the CoNLL-04 joint entity relation\nextraction test set.\nEnt Rel Rel+\nWang and Lu + ALB XXL 89.5 67.6 64.3\nZhong and Chen + ALB XXL 90.9 69.4 67.0\nYe et al. + ALB XXL\n3 91.1 72.4 70.3\nPaolini et al. + T 5 B 88.9 63.7 -\nASP + T 5 B 90.7 71.1 68.6\nASP + T 5 L 91.3 71.9 69.4\nASP + T 5 3 B\n4 91.3 72.7 70.5\nTable 3: Test F1 scores of entity and relation extraction\non the ACE-05 joint entity relation extraction task.\n3.3 Coreference Resolution\nWe then conduct experiments on the standard\nOntoNotes benchmark in the CoNLL-12 English\nshared task dataset (Pradhan et al., 2012). Tab. 4\nreports the results. Again, our model achieves state-\nof-the-art performance among systems without any\ndata augmentation5, outperforming the previous\nstate of the art by 1.5 F1 score. We also observe\nthat our ASP models substantially outperform dis-\ncriminative models that make use of the same PLM.\nFurther analysis is provided in App. B.\n4 Related Work\nMost similar to our approach is the model of\n(Paolini et al., 2021), which also predicts structures\nin an iterative manner using conditional language\nmodels. Similar approaches exist for constituency\nparsing (Vinyals et al., 2015; Dyer et al., 2016),\nentity retrieval (De Cao et al., 2021), semantic\nparsing (Xiao et al., 2016), slot labeling, and\nintent classiÔ¨Åcation (Athiwaratkun et al., 2020).\nEarlier work on search-based (Daum√© et al.,\n5We achieve 82.9 F1 score on the development set, out-\nperforming the result without pretraining on additional data\nreported by Wu et al. (2020, Tab. 5). In addition, training our\nmodel does not require the usage of TPUs.\nMUC B 3 CEAFœÜ4 Avg. F1\nLee et al. (2017) 75.8 65.0 60.8 67.2\nJoshi et al. (2020) 85.3 78.1 75.3 79.6\nJoshi et al.+T5B\n‚Ä† 79.8 70.2 66.8 72.3\nJoshi et al.+T5L\n‚Ä† 81.4 73.1 73.1 74.9\nUrbizu et al. 64.9 66.5 65.3 65.6\nPaolini et al.+T5B 81.0 69.0 68.4 72.8\nDobrovolskii 86.3 79.9 76.6 81.0\nASP+T5B 82.3 75.1 72.5 76.6\nASP+T5L 84.7 77.7 75.2 79.3\nASP+T03B 86.9 81.5 78.4 82.3\nASP+FLAN -T5XXL 87.2 81.7 78.6 82.5\nTable 4: Results on the CoNLL-12 English test\nset. Avg. F1 denotes the average F1 of MUC, B 3,\nand CEAF œÜ4 . Models marked with ‚Ä† are our re-\nimplementation. Other results are taken from their orig-\ninal papers. The full results are in Tab. 5.\n2009; Doppa et al., 2014; Chang et al., 2015) and\ngreedy-based approaches (Swayamdipta et al.,\n2016) applied to structured prediction also predict\nthe structure in a sequential fashion as we do.Other\nwork such as energy-based models (Belanger and\nMcCallum, 2016; Tu and Gimpel, 2018, inter alia)\nand graphical models (Durrett and Klein, 2014;\nGanea and Hofmann, 2017) predict structures\nmore holistically.\n5 Conclusion\nIn this paper, we propose a novel framework for\nstructured prediction that encodes a structure as\na series of structure-building actions that obtains\nstate-of-the-art performance across three tasks. In\ncontrast to past approaches for structured predic-\ntion, our approach is compatible with pre-trained\nlarge language models. This allows us to reduce\nstructured prediction to the problem of Ô¨Åne-tuning\npre-trained language models over an enlarged al-\nphabet. We show empirically that ASP outperforms\nprevious structured prediction models by a large\nmargin. Indeed, we set the new state of the art on\nthree tasks: named entity recognition, end-to-end\nrelation extraction, and coreference resolution.\nAcknowledgements\nWe acknowledge support from an ETH Z√ºrich\nResearch grant (ETH-19 21-1) and a grant from\nthe Swiss National Science Foundation (project\n#201009) for this work. We also thank Zeerak Ta-\nlat and Peter Szemraj for their feedback on this\nmanuscript.\n997\nEthical Considerations\nTo consider the ethical implications of our work,\nwe consider the tasks and models used and our\nproposed approach. The tasks considered, named\nentity recognition, relation extraction, and coref-\nerence resolution are often used in a pipeline of\napproaches (say for automatically building knowl-\nedge bases). Understanding the biases, errors, and\nfailure cases of these tasks and their models and\nhow they affect downstream use cases of the knowl-\nedge base would be important to consider. That\nsaid, to our knowledge the proposed approach does\nnot exacerbate (or lessen) or introduce new con-\nsiderations to the ones known about tasks/models\nmore generally.\nLimitations\nAutoregressive Modeling Assumption. The de-\ncoder model, which is autoregressive, introduces\nan inductive bias on the structured prediction ap-\nproach. SpeciÔ¨Åcally, the left-to-right approach re-\nquires the model to model dependencies in a spe-\nciÔ¨Åc order. This could account for some of the re-\nduction in performance compared to task-speciÔ¨Åc\ndiscriminative models. Understanding the impli-\ncations of the autoregressive decision is indeed an\ninteresting question, but one that we felt was out of\nscope for this short paper.\nEfÔ¨Åciency. In our experiments, we reduce the\nburden of Ô¨Ånding many mention spans in two-stage\napproaches. On sentence-level tasks, e.g., entity\nand relation extraction, the number of decoding\nsteps is relatively small. For instance, the average\nnumber of words in an input sentence is ‚âà20. Our\nsystem has a lighter memory trace as opposed to\ndiscriminative models. This extra time cost can be\npartially compensated with larger batch sizes. How-\never, on document-level tasks, e.g., coreference res-\nolution, the number of decoding steps is too large\nto be compensated with parallelism. More efÔ¨Åcient\nmethods for inference such as non-autoregressive\ndecoding (Gu et al., 2018) remain to be explored\nin future work.\nDecoding Algorithms. In this work, we use\ngreedy decoding in all the experiments. Alterna-\ntive decoding algorithms might further improve\nthe quality of the generated sequences, e.g., beam\nsearch (Zhang and Clark, 2008; Goldberg et al.,\n2013).\nChoice of Pretrained Language Models. In\nthis work, the choice of T5 and its variants as the\nconditional language model backbone of our model\nis largely motivated by their ability to handle arbi-\ntrarily long sequences. Unlike BART and GPT, T5\nuses relative position encoding. On document-level\ntasks such as coreference resolution, the ability\nto process long sequences is extremely important.\nHowever, other pretrained conditional language\nmodels, either with encoder‚Äìdecoder structures or\ndecoder-only structures, can be used as a backbone.\nIt might be interesting to explore techniques that\ngeneralize Ô¨Åxed-length position encoding to longer\nsequences.\nReferences\nBen Athiwaratkun, Cicero Nogueira dos Santos, Jason\nKrone, and Bing Xiang. 2020. Augmented natu-\nral language for generative sequence labeling. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 375‚Äì385, Online. Association for Computa-\ntional Linguistics.\nDavid Belanger and Andrew McCallum. 2016. Struc-\ntured prediction energy networks. In Proceedings of\nthe 33rd International Conference on International\nConference on Machine Learning, page 983‚Äì992.\nKai-Wei Chang, Akshay Krishnamurthy, Alekh Agar-\nwal, Hal Daum√©, and John Langford. 2015. Learn-\ning to search better than your teacher. In Proceed-\nings of the 32nd International Conference on In-\nternational Conference on Machine Learning , vol-\nume 37, pages 2058‚Äì2066.\nHyung Won Chung, Le Hou, S. Longpre, Barret Zoph,\nYi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Wei Yu, Vincent\nZhao, Yanping Huang, Andrew M. Dai, Hongkun\nYu, Slav Petrov, Ed Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc Le, and Jason\nWei. 2022. Scaling instruction-Ô¨Ånetuned language\nmodels. ArXiv, abs/2210.11416.\nHal Daum√©, John Langford, and Daniel Marcu. 2009.\nSearch-based structured prediction. Machine learn-\ning, 75(3):297‚Äì325.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and\nFabio Petroni. 2021. Autoregressive entity retrieval.\nIn International Conference on Learning Represen-\ntations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\n998\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171‚Äì4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nVladimir Dobrovolskii. 2021. Word-level coreference\nresolution. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7670‚Äì7675, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nJanardhan Rao Doppa, Alan Fern, and Prasad Tade-\npalli. 2014. Structured prediction via output space\nsearch. Journal of Machine Learning Research ,\n15(38):1317‚Äì1350.\nGreg Durrett and Dan Klein. 2014. A joint model\nfor entity analysis: Coreference, typing, and linking.\nTransactions of the Association for Computational\nLinguistics, 2.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural network\ngrammars. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 199‚Äì209, San Diego, California.\nAssociation for Computational Linguistics.\nMarkus Eberts and Adrian Ulges. 2020. Span-based\njoint entity and relation extraction with transformer\npre-training. 24th European Conference on ArtiÔ¨Å-\ncial Intelligence.\nOctavian-Eugen Ganea and Thomas Hofmann. 2017.\nDeep joint entity disambiguation with local neural\nattention. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2619‚Äì2629, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nYoav Goldberg, Kai Zhao, and Liang Huang. 2013. Ef-\nÔ¨Åcient implementation of beam-search incremental\nparsers. In Proceedings of the 51st Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 2: Short Papers), pages 628‚Äì633, SoÔ¨Åa,\nBulgaria. Association for Computational Linguis-\ntics.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor O.K. Li, and Richard Socher. 2018. Non-\nautoregressive neural machine translation. In Inter-\nnational Conference on Learning Representations.\nMichael Hahn. 2020. Theoretical limitations of self-\nattention in neural sequence models. Transactions\nof the Association for Computational Linguistics ,\n8:156‚Äì171.\nYiding Hao, Dana Angluin, and Robert Frank. 2022.\nFormal language recognition by hard attention\ntransformers: Perspectives from circuit complexity.\nTransactions of the Association for Computational\nLinguistics, 10:800‚Äì810.\nLuheng He, Kenton Lee, Mike Lewis, and Luke Zettle-\nmoyer. 2017. Deep semantic role labeling: What\nworks and what‚Äôs next. In Proceedings of the 55th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers) , pages\n473‚Äì483, Vancouver, Canada. Association for Com-\nputational Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64‚Äì77.\nMandar Joshi, Omer Levy, Luke Zettlemoyer, and\nDaniel Weld. 2019. BERT for coreference reso-\nlution: Baselines and analysis. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5803‚Äì5808, Hong Kong,\nChina. Association for Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof the 3rd International Conference on Learning\nRepresentations (ICLR 2015).\nSandra K√ºbler, Ryan McDonald, and Joakim Nivre.\n2009. Dependency parsing. Synthesis Lectures on\nHuman Language Technologies, 2(1):1‚Äì127.\nKenton Lee, Luheng He, Mike Lewis, and Luke Zettle-\nmoyer. 2017. End-to-end neural coreference reso-\nlution. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 188‚Äì197, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nKenton Lee, Luheng He, and Luke Zettlemoyer. 2018.\nHigher-order coreference resolution with coarse-to-\nÔ¨Åne inference. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 2 (Short Papers), pages\n687‚Äì692, New Orleans, Louisiana. Association for\nComputational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics.\nTianyu Liu, Yuchen Jiang, Ryan Cotterell, and Mrin-\nmaya Sachan. 2022. A structured span selector. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\n999\npages 2629‚Äì2641, Seattle, United States. Associa-\ntion for Computational Linguistics.\nYi Luan, Dave Wadden, Luheng He, Amy Shah, Mari\nOstendorf, and Hannaneh Hajishirzi. 2019. A gen-\neral framework for information extraction using dy-\nnamic span graphs. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 3036‚Äì3046, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end\nsequence labeling via bi-directional LSTM-CNNs-\nCRF. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1064‚Äì1074, Berlin, Ger-\nmany. Association for Computational Linguistics.\nGiovanni Paolini, Ben Athiwaratkun, Jason Krone,\nJie Ma, Alessandro Achille, Rishita Anubhai, Ci-\ncero Nogueira dos Santos, Bing Xiang, and Stefano\nSoatto. 2021. Structured prediction as translation be-\ntween augmented natural languages. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nOlga Uryupina, and Yuchen Zhang. 2012. CoNLL-\n2012 shared task: Modeling multilingual unre-\nstricted coreference in OntoNotes. In Joint Confer-\nence on EMNLP and CoNLL - Shared Task , pages\n1‚Äì40, Jeju Island, Korea. Association for Computa-\ntional Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniÔ¨Åed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1‚Äì67.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChafÔ¨Ån, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak,\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian\nJiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\nThomas Wang, Trishala Neeraj, Jos Rozen, Ab-\nheesht Sharma, Andrea Santilli, Thibault Fevry, Ja-\nson Alan Fries, Ryan Teehan, Stella Biderman, Leo\nGao, Tali Bers, Thomas Wolf, and Alexander M.\nRush. 2021. Multitask prompted training enables\nzero-shot task generalization.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to sequence learning with neural networks.\nAdvances in neural information processing systems ,\n27.\nSwabha Swayamdipta, Miguel Ballesteros, Chris Dyer,\nand Noah A. Smith. 2016. Greedy, joint syntactic-\nsemantic parsing with stack LSTMs. In Proceedings\nof the 20th SIGNLL Conference on Computational\nNatural Language Learning, pages 187‚Äì197, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natu-\nral Language Learning at HLT-NAACL 2003, pages\n142‚Äì147.\nLifu Tu and Kevin Gimpel. 2018. Learning approx-\nimate inference networks for structured prediction.\nIn International Conference on Learning Represen-\ntations.\nGorka Urbizu, Ander Soraluze, and Olatz Arregi. 2020.\nSequence to sequence coreference resolution. In\nProceedings of the Third Workshop on Computa-\ntional Models of Reference, Anaphora and Corefer-\nence, pages 39‚Äì46, Barcelona, Spain (online). Asso-\nciation for Computational Linguistics.\nOriol Vinyals, ≈Åukasz Kaiser, Terry Koo, Slav Petrov,\nIlya Sutskever, and Geoffrey Hinton. 2015. Gram-\nmar as a foreign language. In Advances in Neural\nInformation Processing Systems, volume 28. Curran\nAssociates, Inc.\nC. Walker and Linguistic Data Consortium. 2005.ACE\n2005 Multilingual Training Corpus . LDC corpora.\nLinguistic Data Consortium.\nJue Wang and Wei Lu. 2020. Two are better than\none: Joint entity and relation extraction with table-\nsequence encoders. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1706‚Äì1721, Online. As-\nsociation for Computational Linguistics.\nShuly Wintner. 2010. Formal Language Theory, chap-\nter 1. John Wiley & Sons, Ltd.\nWei Wu, Fei Wang, Arianna Yuan, Fei Wu, and Ji-\nwei Li. 2020. CorefQA: Coreference resolution as\nquery-based span prediction. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 6953‚Äì6963, Online. As-\nsociation for Computational Linguistics.\nChunyang Xiao, Marc Dymetman, and Claire Gardent.\n2016. Sequence-based structured prediction for se-\nmantic parsing. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1341‚Äì\n1350, Berlin, Germany. Association for Computa-\ntional Linguistics.\nDeming Ye, Yankai Lin, Peng Li, and Maosong Sun.\n2022. Packed levitated marker for entity and relation\nextraction. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n1000\n(Volume 1: Long Papers), pages 4904‚Äì4917, Dublin,\nIreland. Association for Computational Linguistics.\nYue Zhang and Stephen Clark. 2008. A tale of two\nparsers: Investigating and combining graph-based\nand transition-based dependency parsing. In Pro-\nceedings of the 2008 Conference on Empirical Meth-\nods in Natural Language Processing , pages 562‚Äì\n571, Honolulu, Hawaii. Association for Computa-\ntional Linguistics.\nTianyang Zhao, Zhao Yan, Yunbo Cao, and Zhoujun Li.\n2020. Asking effective and diverse questions: A ma-\nchine reading comprehension based framework for\njoint entity-relation extraction. In Proceedings of\nthe Twenty-Ninth International Joint Conference on\nArtiÔ¨Åcial Intelligence, IJCAI-20 , pages 3948‚Äì3954.\nInternational Joint Conferences on ArtiÔ¨Åcial Intelli-\ngence Organization. Main track.\nZexuan Zhong and Danqi Chen. 2021. A frustratingly\neasy approach for entity and relation extraction. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 50‚Äì61, Online. Association for Computational\nLinguistics.\n1001\nA Experimental Details\nA.1 Experimental Settings\nIn our experiments, the {T5,T0,Ô¨Çan-T5}-base,\n{T5,T0,Ô¨Çan-T5}-large, {T5,T0,Ô¨Çan-T5}-{3B,XL},\n{T5,T0,Ô¨Çan-T5}-{11B,XXL} have 220 million,\n770 million, 3 billion, and 11 billion parameters\nrespectively6. The feedforward neural networks\ndescribed in ¬ß2.3 have one hidden layer of size\n150 for ACE-05, 4096 for CoNLL-03, CoNLL-04,\nand CoNLL-12.\nWe follow the same preprocessing procedure\nand train/dev/test split of previous work on all\ndatasets. For all the experiments, we use the\nAdamW optimizer (Kingma and Ba, 2015). We\ntrain 40 epochs on CoNLL-12 for coreference\nresolution with batch size 1. For end-to-end\nrelation extraction on CoNLL-04 and ACE-05,\nwe train 100 epochs with batch size 8. The initial\nlearning rates are set to 5e-5 for {T5,T0,Ô¨Çan-T5}-\nbase and {T5,T0,Ô¨Çan-T5}-large models, 3e-5 for\n{T5,T0,Ô¨Çan-T5}-{3B,XL,11B,XXL} models.\nWe apply bÔ¨Çoat16 training in our experiments.\nOne single A100-40GB GPU is used for train-\ning models that use {T5,T0,Ô¨Çan-T5}-base and\n{T5,T0,Ô¨Çan-T5}-large. Two A100-40GB GPUs\nare required to train models that use 3B or XL. Six\nA100-80GB GPUs are required to train models\nthat use 11B or XXL models. It takes around 0.1\nseconds for base-scale models and 1 second per\nupdating step for {3B,11B,XXL} models.\nA.2 Datasets\nA.2.1 Named Entity Recognition\nCoNLL-03. We use the CoNLL-03 dataset\n(Tjong Kim Sang and De Meulder, 2003) to eval-\nuate our model on named entity recognition. This\ndataset consists of 946 training articles, 216 de-\nvelopment articles, and 231 test sentences. We\nevaluate under the document-level settings, which\nmeans we feed the entire document into the model\ninstead of the individual sentences.\nA.2.2 End-to-End Relation Extraction\nCoNLL-04. The CoNLL-04 dataset contains\nfour types of entities (location, organization, per-\nson, other) and Ô¨Åve types of relations (work for, kill,\norganization based in, live in, located in). We split\nthe dataset as the training (922 sentences), valida-\ntion (231 sentences), and test (288 sentences) as in\n6https://github.com/google-research/\ntext-to-text-transfer-transformer\nprevious work. For the ACE-05 dataset, we follow\nthe train/dev/test split of previous work (Zhong and\nChen, 2021).\nACE-05. The ACE-05 dataset (Walker and Con-\nsortium, 2005) contains 511 documents in total col-\nlected from multiple domains including newswire,\nbroadcast, discussion forums, etc. We follow Luan\net al. (2019)‚Äôs preprocessing script7 and split the\ndataset into train/dev/test set. ACE-05 contains\ninconsistently capitalized data. The newswire por-\ntion collected from CNN are entirely lowercased,\nwhich involves around 20 documents. Previous\nworks (Zhong and Chen, 2021; Ye et al., 2022) that\nuse case-insensitive encoders such as ALBERT are\nnot affected by this deÔ¨Åciency. However, the T5\nmodel and its variants are case-sensitive. We use\nthe python truecase package8 to restore the cor-\nrect capitalization during preprocessing.\nA.2.3 Coreference Resolution\nCoNLL-12. The CoNLL-12 English shared task\ndataset for coreference resolution (Pradhan et al.,\n2012) contains 2802 documents for training, 343\nfor validation, and 348 for testing. During training,\nwe chunk the documents into segments of 2048\nmaximum words. In total, there are 2830 segments\nfor training. During the evaluation, we use the\nentire document as the input to the model.\n0.1 0.2 0.3 0.4 0.5 0.6\n70\n80\n90\n100\nRatio\nRecall (%)\nJoshi(various ratio)\nJoshi(actual ratio)\nASP+ T5L\nFigure 2: Recall rate of gold mentions. The ratio on\nthe x-axis refers to the number of predicted mentions\ndivided by |D|. Joshi refers to the two-stage model of\n(Joshi et al., 2020).\n7https://github.com/luanyi/DyGIE/tree/master/preprocessing\n8https://pypi.org/project/truecase/\n1002\nMUC B 3 CEAF œÜ4\nP R F P R F P R F Avg. F1\nLee et al. (2017) 78.4 73.4 75.8 68.6 61.8 65.0 62.7 59.0 60.8 67.2\nLee et al. (2018) 81.4 79.5 80.4 72.2 69.5 70.8 68.2 67.1 67.6 73.0\nJoshi et al. (2019) 84.7 82.4 83.5 76.5 74.0 75.3 74.1 69.8 71.9 76.9\nJoshi et al. (2020) 85.8 84.8 85.3 78.3 77.9 78.1 76.4 74.2 75.3 79.6\nJoshi et al. +T5B\n‚Ä† 82.4 77.4 79.8 72.3 68.2 70.2 70.5 63.5 66.8 72.3\nJoshi et al. +T5L\n‚Ä† 85.5 77.7 81.4 78.3 68.5 73.1 75.0 65.9 73.1 74.9\nDobrovolskii 84.9 87.9 86.3 77.4 82.6 79.9 76.1 77.1 76.6 81.0\nUrbizu et al. - - 64.9 - - 66.5 - - 65.3 65.6\nPaolini et al. +T5B - - 81.0 - - 69.0 - - 68.4 72.8\nASP +T5B 81.7 82.8 82.3 74.2 76.1 75.1 74.5 70.6 72.5 76.6\nASP +T5L 83.3 86.2 84.7 75.9 79.5 77.7 75.8 74.5 75.2 79.3\nASP +FLAN -T5L 83.5 87.6 85.5 76.3 81.8 79.0 76.0 76.2 76.1 80.2\nASP +T03B 85.8 88.3 86.9 79.6 83.3 81.5 78.3 78.5 78.4 82.3\nASP +FLAN -T5XL 84.9 88.7 86.7 78.5 83.8 81.1 78.4 78.5 78.4 82.2\nASP +FLAN -T5XXL 86.1 88.4 87.2 80.2 83.2 81.7 78.9 78.3 78.6 82.5\nTable 5: Full results on the CoNLL-12 English test set. Avg. F1 denotes the average F1 of MUC, B3, and CEAFœÜ4 .\nModels marked with ‚Ä†are our re-implementation. Other results are taken from their original papers.\nB Coreference Resolution\nIn this section, we analyze the performance of men-\ntion detection for coreference resolution of our\nmodel in Fig. 2. This analysis casts light on how\nour model plans globally in an autoregressive man-\nner. In the task of coreference resolution, only the\nentities that are mentioned more than once in a doc-\nument are annotated as mentions. This is to say,\nan utterance of an entity should only be labeled\nif that entity is referred to again afterward. Thus,\nin previous coreference resolution models, a ded-\nicated mention detection module that enumerates\ncandidate textual spans (e.g., noun phrases and pro-\nnouns) for mentions is indispensable. However,\nour model is able to directly predict the exact set\nof mentions that we require, even if the target se-\nquence is generated from left to right. We conclude\nthat this results from the cross-attention mechanism\nwhich enables the model to look at relevant parts\nin the input document during decoding. Given an\ninput document of |D|words, our model predicts\nonly 0.096 |D|mentions with a 89.6% recall rate\nof gold mentions. This refrained mention detection\nstrategy imposes a limit on the cardinality of Zi\nin Eq. (2). As a result, this relatively small con-\nstant factor (compared to 0.4 used in most previous\nwork) keeps our model tractable without the need\nfor pruning strategies as in the models based on\n(Lee et al., 2017).\nC Modeling More Restricted Structures\nIn this work, we tackled three tasks that are tradi-\ntionally considered structured prediction problems.\nNamed entity recognition and relation extraction\nconsider labeling spans with a set of given types.\nCoreference resolution has long-range dependen-\ncies and has to model relationships between spans.\nHowever, there are structured prediction problems\nthat require more restricted outputs. For instance,\nin dependency parsing, a spanning tree connecting\nevery word in the input sentence is the desired\noutput (K√ºbler et al., 2009). While in constituency\nparsing, a parse tree in Chomsky Normal Form is\nsupposed to be a complete binary tree except for the\nleaf nodes (Wintner, 2010). Modeling such types\nof structures requires a more speciÔ¨Åed deÔ¨Ånition\nof task-speciÔ¨Åc actions. In future work, we aim to\nexplore the abilities and limitations of our method.\nD Experiments with Flan-T5\nWe conduct additional experiments with the latest\npretrained language model Flan-T5 (Chung et al.,\n2022). Flan-T5 is pretrained on more supervised\ntasks and achieves better performance than the orig-\ninal T5 on multiple NLU tasks. The results are\nshown in Tab. 5, Tab. 6, and Tab. 7. We Ô¨Ånd that\nwith the same size of the model, Flan-T5 performs\nbetter than T5 in general.\n1003\nPrec. Rec. F1\nASP + T 5 B 91.4 92.2 91.8\nASP + FLAN - T 5 B 92.7 93.8 93.3\nASP + T 5 L 92.1 93.4 92.8\nASP + FLAN - T 5 L 93.3 94.3 93.8\nASP + T 5 3 B 93.8 94.4 94.1\nTable 6: Test F1 scores of named entity recognition on\nthe CoNLL-03 test set.\nEnt Rel\nASP+T5B 89.5 73.2\nASP+FLAN -T5B 89.4 73.8\nASP+FLAN -T5L 90.5 76.2\nTable 7: Test F1 scores of named entity recognition on\nthe CoNLL-04 test set.\nE Decoding Examples\nWe provide decoding examples from the tasks we\nexperiment on in Tab. 8. The copy actions are\nverbalized into tokens.\n1004\nnamed\nentity\nrecognition\nGUNMEN WOUND TWO[‚àó MANCHESTER UNITED] FANS IN [‚àó AUSTRIA ] . [‚àó VIENNA ] 1996-12-06 Two\n[‚àó Manchester United ] soccer fans were wounded by unidentified gunmen on Friday and taken\nto hospital in the [‚àó Austrian ] capital, police said. \" The four[‚àó Britons ] were shot\nat from a [‚àó Mercedes ] car at around 1 a.m., \" a spokeswoman told[‚àó Reuters ] . The two\nmen were hit in the pelvis and leg. Police said their lives were not in danger. The fans,\nin [‚àó Austria ] to watch their team play [‚àó Rapid Vienna ] last Wednesday, may have been\ninvolved in a pub brawl earlier, the spokeswoman said.[‚àó Manchester United ] won 2-0.</s>\nend-to-end\nrelation\nextraction\nAnd this final story: retired [‚àó Senator ] [‚àó Strom Thurmond ] has never made a secret\nabout [‚àó his ] fondness for young pretty[‚àó women ] .</s>\ncoreference\nresolution\n<speaker> - </speaker>[‚àó Al Gore ] won‚Äôt be the next U.S. President, but[‚àó he ] has a\nslim chance of becoming[‚àó the next President at[‚àó Harvard ] ] . [‚àó Gore ] holds a degree\nfrom [‚àó the university ] , and is one of about 500 people nominated for[‚àó the job ] . [‚àó A\nschool official ] talked about [‚àó the Vice President‚Äôs] chances during an interview with\n\" the Boston Globe. \"[‚àó He ] says it‚Äôs unlikely [‚àó Gore ] will be selected, because [‚àó\nhe ] doesn‚Äôt have enough experience in the academic world.</s>\n<speaker> - </speaker> [‚àó Violence between Israelis and Palestinians] continued in [‚àó\nits ] third month, though at a slightly reduced level overall.[‚àó Israeli and Palestinian\nnegotiators ] met separately at the White House with President Bill Clinton in hopes of\nrestarting direct negotiations between[‚àó them ] for a final settlement.</s>\nTable 8: Predicted sequences from CoNLL-03, ACE-05, and CoNLL-12 dataset.\n1005",
  "topic": "Coreference",
  "concepts": [
    {
      "name": "Coreference",
      "score": 0.9503070116043091
    },
    {
      "name": "Computer science",
      "score": 0.8529242873191833
    },
    {
      "name": "Discriminative model",
      "score": 0.8145155906677246
    },
    {
      "name": "Language model",
      "score": 0.6771513819694519
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6531673073768616
    },
    {
      "name": "Autoregressive model",
      "score": 0.6310169100761414
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5720657110214233
    },
    {
      "name": "Natural language processing",
      "score": 0.5512189269065857
    },
    {
      "name": "Machine learning",
      "score": 0.44729337096214294
    },
    {
      "name": "Structured prediction",
      "score": 0.4149673879146576
    },
    {
      "name": "Resolution (logic)",
      "score": 0.3694397211074829
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I4210100430",
      "name": "Google (Switzerland)",
      "country": "CH"
    }
  ]
}