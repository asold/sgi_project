{
    "title": "Montage: A Neural Network Language Model-Guided JavaScript Engine Fuzzer",
    "url": "https://openalex.org/W3000350072",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A702897416",
            "name": "Lee, Suyoung",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3154888683",
            "name": "Han, HyungSeok",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4302680760",
            "name": "Cha, Sang Kil",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4307010332",
            "name": "Son, Sooel",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1546956568",
        "https://openalex.org/W2701225458",
        "https://openalex.org/W2071952624",
        "https://openalex.org/W1994573369",
        "https://openalex.org/W2964241064",
        "https://openalex.org/W1984762903",
        "https://openalex.org/W2517087431",
        "https://openalex.org/W2583649498",
        "https://openalex.org/W1575384945",
        "https://openalex.org/W2765435026",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2808279976",
        "https://openalex.org/W2065948900",
        "https://openalex.org/W2964788914",
        "https://openalex.org/W2128128820",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2904932877",
        "https://openalex.org/W2165747537",
        "https://openalex.org/W1506510492",
        "https://openalex.org/W2111305191",
        "https://openalex.org/W1988524530",
        "https://openalex.org/W2083878868",
        "https://openalex.org/W2029329147",
        "https://openalex.org/W2143861926",
        "https://openalex.org/W2947109320",
        "https://openalex.org/W1769343819",
        "https://openalex.org/W2519952770",
        "https://openalex.org/W1938755728",
        "https://openalex.org/W2444132761",
        "https://openalex.org/W2962725091",
        "https://openalex.org/W2950186769",
        "https://openalex.org/W2136848157",
        "https://openalex.org/W2142403498",
        "https://openalex.org/W2524561453",
        "https://openalex.org/W2899781671",
        "https://openalex.org/W2532335977",
        "https://openalex.org/W614438062",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W2886081690",
        "https://openalex.org/W1531203382",
        "https://openalex.org/W2107878631"
    ],
    "abstract": "JavaScript (JS) engine vulnerabilities pose significant security threats affecting billions of web browsers. While fuzzing is a prevalent technique for finding such vulnerabilities, there have been few studies that leverage the recent advances in neural network language models (NNLMs). In this paper, we present Montage, the first NNLM-guided fuzzer for finding JS engine vulnerabilities. The key aspect of our technique is to transform a JS abstract syntax tree (AST) into a sequence of AST subtrees that can directly train prevailing NNLMs. We demonstrate that Montage is capable of generating valid JS tests, and show that it outperforms previous studies in terms of finding vulnerabilities. Montage found 37 real-world bugs, including three CVEs, in the latest JS engines, demonstrating its efficacy in finding JS engine bugs.",
    "full_text": "ARTIFACT\nEVALUATED\nPASSED\nMontage: A Neural Network Language Model-Guided\nJavaScript Engine Fuzzer\nSuyoung Lee, HyungSeok Han, Sang Kil Cha, Sooel Son\nSchool of Computing, KAIST\nAbstract\nJavaScript (JS) engine vulnerabilities pose signiﬁcant security\nthreats affecting billions of web browsers. While fuzzing is\na prevalent technique for ﬁnding such vulnerabilities, there\nhave been few studies that leverage the recent advances in\nneural network language models (NNLMs). In this paper, we\npresent Montage, the ﬁrst NNLM-guided fuzzer for ﬁnding JS\nengine vulnerabilities. The key aspect of our technique is to\ntransform a JS abstract syntax tree (AST) into a sequence of\nAST subtrees that can directly train prevailing NNLMs. We\ndemonstrate that Montage is capable of generating valid JS\ntests, and show that it outperforms previous studies in terms\nof ﬁnding vulnerabilities. Montage found 37 real-world bugs,\nincluding three CVEs, in the latest JS engines, demonstrating\nits efﬁcacy in ﬁnding JS engine bugs.\n1 Introduction\nThe memory safety of web browsers has emerged as a critical\nattack vector as they have become an integral part of every-\nday computing. Malicious websites, which conduct drive-\nby download attacks [48], have typically exploited memory\ncorruption vulnerabilities of web browsers. Currently, an ex-\nploitable memory corruption vulnerability for a browser can\ncost 100,000 USD and sell for a million dollars if it is chained\nwith a kernel exploit to remotely jailbreak iOS [59].\nAmong many components of web browsers, a JavaScript\n(JS) engine is of particular interest to attackers as its Turing-\ncomplete nature enables attackers to craft sophisticated ex-\nploits. One can easily allocate a series of heap chunks to\nperform heap spraying [49], write functions in JS to abstract\naway some exploitation logic [26], and even bypass the miti-\ngation used in modern web browsers [35]. According to the\nNational Vulnerability Database (NVD), 43% of the total vul-\nnerabilities reported for Microsoft Edge and Google Chrome\nin 2017 were JS engine vulnerabilities.\nDespite the increasing attention, there has been relatively\nlittle academic research on analyzing JS engine vulnerabilities\ncompared to other studies seeking to ﬁnd them [18, 24, 54].\nLangFuzz [24] combines code fragments extracted from JS\nseed ﬁles to generate JS test inputs. GramFuzz and IFuzzer\nemploy more or less the same approach [18, 54], but IFuzzer\nuses evolutionary guidance to improve the fuzzing effective-\nness with genetic programming based on the feedback ob-\ntained by executing a target JS engine with produced inputs.\nHowever, none of the existing approaches consider the re-\nlationship between code fragments for generating test inputs.\nIn other words, they produce test inputs by simply combin-\ning fragments as long as JS grammars allow it. Thus, they\ndo not determine which combination is likely to reveal vul-\nnerabilities from the target JS engine. Are there any similar\npatterns between JS test inputs that trigger JS engine vulnera-\nbilities? If so, can we leverage such patterns to drive fuzzers\nto ﬁnd security vulnerabilities? These are the key questions\nthat motivated our research.\nWe performed a preliminary study on JS engine vulnera-\nbilities and observed two patterns. We observed that a new\nsecurity problem often arises from JS engine ﬁles that have\nbeen patched for a different bug. We analyzed 50 CVEs as-\nsigned to ChakraCore, a JS engine used by Microsoft Edge.\nWe found that 18% and 14% of the vulnerabilities were related\nto GlobOpt.cpp and JavascriptArray.cpp, respectively.\nThe second observation was that JS test code that triggers\nnew security vulnerabilities is often composed of code frag-\nments that already exist in regression tests.We collected 2,038\nunique JS ﬁles from the ChakraCore regression test suite and\n67 JS ﬁles that invoked the analyzed vulnerabilities. These\ntwo sets of ﬁles were disjoint. We sliced the AST of each\nJS ﬁle into AST subtrees of depth one, called fragments. We\nthen computed the number of overlapping fragments between\nthe two sets; we found that 95.9% of the fragments extracted\nfrom the 67 vulnerability-triggering JS ﬁles overlapped with\nthe fragments extracted from the regression test suite (see §3).\nGiven these two observations, how do we perform fuzz\ntesting to ﬁnd JS engine vulnerabilities? For this research\nquestion, we propose the ﬁrst approach that leverages a neu-\nral network language model (NNLM) to conduct fuzz testing\narXiv:2001.04107v2  [cs.CR]  14 Jan 2020\non a target JS engine. Our key idea is to mutate a given regres-\nsion JS test by replacing its partial code with new code that\nthe NNLM creates. Consider a regression JS test that invokes\na patched functionality. We generate a JS test from this re-\ngression test while expecting to elicit a new potential bug that\nresides in the patched JS engine ﬁles, thus addressing the ﬁrst\nobservation. We also assemble existing code from regression\ntest suites under the guidance of the NNLM when composing\nnew partial code. This captures the second observation.\nTo manifest this idea, we designed and implemented Mon-\ntage, a system for ﬁnding security vulnerabilities in JS en-\ngines. The system starts by transforming the AST of each\nJS test from a given regression test suite into the sequence\nof fragments. These fragment sequences become training\ninstances over which the NNLM is trained. Therefore, the\nNNLM learns the relationships between fragments. Montage\nmutates a given JS test by reconstructing one of its subtrees\nas the trained NNLM guides.\nPrevious research focused on learning the relationships\nbetween PDF objects [16], characters [11, 32], and lexical\ntokens in the source code [22,40,43]. These language models\naddressed completing incorrect or missing tokens [40, 53], or\nassembling PDF objects [16]. Their methods are not directly\napplicable to generating valid JS tests, which requires model-\ning structural control ﬂows and semantic data dependencies\namong JS lexical tokens. Liu et al. [32] stated their limitation\nin extracting general patterns from character-level training\ninstances from C code, thus generating spurious tests.\nUnlike these previous studies [11, 16], Montage uses frag-\nments as building blocks. Each fragment encapsulates the\nstructural relationships among nodes within an AST unit tree.\nThe model is then trained to learn the relationships between\nsuch AST unit trees. Montage uses this model to assemble\nunit subtrees when mutating a given regression JS test. Thus,\neach generated JS test reﬂects the syntactic and semantic\ncommonalities that exist in the regression test suite.\nWe evaluated Montage to ﬁnd bugs in ChakraCore 1.4.1\nand compared the number of found bugs against CodeAl-\nchemist [20], jsfunfuzz [38], and IFuzzer [54]. We performed\nﬁve fuzzing campaigns; each round ran for 72 hours. Mon-\ntage found 133 bugs, including 15 security bugs. Among the\nfound security bugs, Montage reported 9, 12, and 12 bugs that\nCodeAlchemist, jsfunfuzz, and IFuzzer did not ﬁnd, respec-\ntively. This result demonstrates that Montage is able to ﬁnd\nbugs that the state-of-the-art JS fuzzers are unable to ﬁnd.\nWe measured the efﬁcacy of the Montage language model\nagainst the random selection method with no language model,\nMarkov-chain model, and the character/token-level recurrent\nneural network language model. Montage outperformed the\nother approaches in terms of ﬁnding unique bugs.\nWe further tested Montage to fuzz the latest versions of\nChakraCore, JavaScriptCore, SpiderMonkey, and V8. Mon-\ntage found 37 unique bugs, including three security bugs.\n34 bugs were found from ChakraCore. The remaining two\nand one bugs were from JavaScriptCore and V8, respectively.\nOf these three security bugs, Montage discovered one from\nJavaScriptCore and the other two from ChakraCore. These\nresults demonstrate the effectiveness of leveraging NNLMs\nin ﬁnding real-world JS engine bugs.\n2 Background\n2.1 Language Model\nA language model is a probability distribution over sequences\nof words. It is essential for natural language processing (NLP)\ntasks, such as speech recognition, machine translation, and\ntext generation. Traditionally, language models estimate the\nlikelihood of a word sequence given its occurrence history in\na training set.\nAn n-gram language model [8, 30] approximates this prob-\nability based on the occurrence history of the preceding n −1\nwords. Unfortunately, such count-based language models in-\nherently suffer from the data sparsity problem [8], which\ncauses them to yield poor predictions. The problem is mainly\ndue to insufﬁcient representative training instances. NNLMs\naddress the data sparsity problem by representing words as a\ndistributed vector representation, which is often called a word\nembedding, and using it as input into a neural network.\nBengio et al. [3] introduced the ﬁrst NNLM, a feed-forward\nneural network (FNN) model. An FNN predicts the next word\nbased on its preceding n −1 words, which is called a history\nor a context where n is a hyper parameter that represents the\nsize of the word sequence [1, 3, 17]. In this NNLM setting, all\nwords in a training set constitute a vocabulary V . Each word\nin V is mapped onto a feature vector. Therefore, a context, a\nword sequence, becomes the concatenation of each feature\nvector corresponding to its word. The model is then trained\nto output a conditional probability distribution of words in V\nfor the next word from a given context.\nLong short-term memory (LSTM). Unlike FNN language\nmodels, a recurrent neural network (RNN) is capable of pre-\ndicting the next word from a history of preceding words of an\narbitrary length because an RNN is capable of accumulating\ninformation over a long history of words. An LSTM model is\na special kind of RNN; it is designed to capture long-term de-\npendencies between words [14, 23]. Because a standard RNN\nsuffers from the gradient vanishing/exploding problem [4],\nan LSTM model uses neural layers called gates to regulate\ninformation propagation and internal memory to update its\ntraining parameters over multiple time steps.\n2.2 JS Engine Fuzzing\nFuzz testing is a form of dynamic software testing in which\nthe program under test runs repeatedly with test inputs in\norder to discover bugs in the program. Fuzzing can be catego-\nrized into two types based on their input generation method-\nology [50]: mutational fuzzing and generational fuzzing. Mu-\ntational fuzzing [7, 44, 57, 58] alters given seeds to generate\nnew test inputs, whereas generational fuzzing [19, 20, 24, 38]\nproduces tests based on an input model, such as a grammar.\nSince JS code is highly structured, randomly generated test\ninputs are likely to be rejected by JS engines. Therefore, it\nis common for JS engine fuzzers to employ a generational\napproach. One notable example is jsfunfuzz, a seminal JS\nengine fuzzer [38, 45]. It starts with a start symbol deﬁned\nin a JS grammar and selects the next potential production in\na random fashion until there are no remaining non-terminal\nsymbols. CodeAlchemist [20] is another generational fuzzer\nthat resort to the assembly constraints of its building blocks\ncalled code bricks to produce semantically valid JS code.\nMost other JS engine fuzzers use both mutational and gen-\nerational approaches. LangFuzz [24], GramFuzz [18], and\nIFuzzer [54] parse JS seeds with the JS grammar and con-\nstruct a pool of code fragments, where a code fragment is a\nsubtree of an AST. They combine code fragments in the pool\nto produce a new JS test input, but they also mutate given\nseeds to generate test inputs.\nAlthough it does not aim to ﬁnd security vulnerabilities,\nTreeFuzz [41] leverages a probabilistic context-free grammar\n(PCFG) to generate a test suite from given seeds. Similarly,\nSkyﬁre [56] infers a probabilistic context-sensitive grammar\n(PCSG) from given seeds and uses it to generate a well-\ndistributed set of seeds. Both approaches apply probabilistic\nlanguage models to generate JS testing inputs, but their design\nis too generic to ﬁnd security vulnerabilities in JS engines. Un-\nlike previous approaches, Montage is inspired by a systematic\nstudy of CVEs, i.e., previous JS engine vulnerabilities, and\nleverages an NNLM trained to learn syntactic and semantic\ncommonalities between JS regression test suites.\n3 Motivation\nCan we ﬁnd similarities between JS ﬁles that trigger secu-\nrity vulnerabilities? We answer this question by conducting\na quantitative study of analyzing reported CVEs and corre-\nsponding proof of concept (PoC) exploits for ChakraCore [10].\nWe chose ChakraCore because its GitHub repository main-\ntains well-documented commit logs describing whether a\nspeciﬁc CVE is patched by a commit. This helps us identify\nwhich security vulnerability is related to a given PoC exploit\nand which source lines are affected by the vulnerability. Other\nJS engines, in contrast, have not provided an exact mapping\nbetween a code commit and a CVE.\nNote that collecting PoC exploits is not straightforward be-\ncause CVE reports typically do not carry any PoC exploits due\nto the potential risk of being abused. We manually collected\nCVEs as well as their PoC code from exploitDB, vulnera-\nbility blogs, and the ChakraCore GitHub repository. In total,\nwe obtained 67 PoC exploits, each of which corresponds to\na unique CVE. We further identiﬁed 50 of them where the\ncorresponding vulnerabilities are ﬁxed by a single commit.\nThis means that we can map each of the 50 vulnerabilities\nto a set of affected source ﬁles. The earliest and the latest\nvulnerabilities in the collected set were patched in September\n2016 and March 2018, respectively. In total, 77 ﬁles were\npatched owing to these vulnerabilities.\nWe found that nine out of the 50 vulnerabilities (18%) are\nrelated to the GlobOpt.cpp ﬁle, which mainly implements\nthe just-in-time (JIT) compilation step. Seven of them (14%)\nhave also contributed to patching the JavascriptArray.cpp\nﬁle. Note that each ﬁle implements different functionalities of\nChakraCore. In other words, different JS engine vulnerabili-\nties often arise from a common ﬁle that implements the same\nfunctionalities, such as JIT optimization and JS arrays. For\nexample, a patch for CVE-2018-0776 forces a deep copy of an\narray when the array is accessed via the function arguments\nproperty within a callee, thus avoiding a type confusion vul-\nnerability. However, the patch was incomplete, still leaving\nother ways in which a shallow copy of arrays could be caused.\nCVE-2018-0933 and CVE-2018-0934 were assigned to those\nbugs. Note that all the patches revised theBoxStackInstance\nfunction in the JavascriptArray.cpp ﬁle.\nAmong the 77 patched ﬁles, 26 (33.8%) ﬁles are patched\nat least twice due to the reported CVEs. These examples\ndemonstrate that JS engine vulnerabilities often arise from\nﬁles that were patched for other bugs. Considering that these\npatches are often checked with regression tests, mutating an\nexisting JS test may trigger a new vulnerability whose root\ncause lies in the patched ﬁles that this test already covered.\nObservation 1. JS engine vulnerabilities often arise from\nthe same ﬁle patched for different bugs.\nWe also measured the syntactic similarity between JS code\nfrom the PoC exploits and 2,038 JS ﬁles obtained from re-\ngression test suites maintained by ChakraCore. Note that a\nregression test suite consists of JS tests that trigger previously\npatched bugs and check expected outcomes with adversarial\ntest input. In particular, we gathered the regression test ﬁles\nfrom the ChakraCore version released in August 2016, which\nis one month ahead of the patching date of the earliest vulner-\nability. Therefore, the regression test ﬁles were not affected\nby any of the studied vulnerabilities.\n1 var v0 = {};\n2 for ( var v1 = P; v1 < 5; v1 ++) {\n3 v0[v1] = v1 + 5;\n4 }\nFigure 1: Example of a normalized JS ﬁle.\nTo measure the similarity, we normalized the identiﬁers in\nthe regression test ﬁles as well as the PoC exploits. Speciﬁ-\ncally, we renamed each identiﬁer for variables and functions\nto have a sequential number and a common preﬁx as their\nname. We then parsed the normalized JS ﬁles down to ASTs.\nWe extracted a set of unit subtrees with a depth of one\nIdentiﬁer\nMemberExpr\nright\noperator\nobject\nv0\nname\n=\nAssignExpr\nproperty\nv1\nIdentiﬁer\nleft\nname name\nv1\nIdentiﬁer\nrightoperator\n+\nBinaryExpr\nleft\nvalue\n5\nLiteral\na\ncb\nd e f g\ncb\na\ned\nb d e\nf\nc\ng\nf g\n, , , , , ,\nFigure 2: Fragmentizing an AST from the example in Fig-\nure 1.\nfrom each AST. For a given AST, we extracted a unit subtree\nfrom each internal node. Thus, the number of extracted unit\nsubtrees becomes the number of AST internal nodes. We call\nsuch a unit subtree afragment, as formally deﬁned in §5. Note\nthat the root node of each fragment is an internal node of the\nAST. It also corresponds to a leaf node in another fragment,\nexcept the fragment with the root node of the original AST.\nFigure 2 illustrates the fragmentation results for a JS ﬁle\nlisted in Figure 1. The upper side of the ﬁgure shows an AST\nsubtree obtained from the Esprima JS parser [21]. This subtree\ncorresponds to Line 3. The bottom of the ﬁgure presents\nfragments from this subtree.\nWe also divided each PoC that triggers a CVE into frag-\nments and then counted how many fragments existed in the\nregression test suites. Figure 3 depicts the number of PoC ﬁles\nwhose common fragment percentage is over each percentage\nthreshold. We found that all the fragments (100%) from 10\nPoC exploits already existed in the regression test ﬁles. More\nthan 96% of the fragments in the 42 PoC exploits and 90% of\nthe fragments in the 63 PoC exploits existed in the regression\ntest as well. On average, 95.9% of the fragments from the\nPoC exploits were found in the regression test ﬁles.\nObservation 2. More than 95% of the fragments syntac-\ntically overlap between the regression tests and the PoC\nexploits.\nBoth observations imply that it is likely to trigger a new\nsecurity vulnerability by assembling code fragments from\nexisting regression test suites, which is the primary motivation\nfor this study, as we describe in §4.\n4 Overview\nWe present Montage, an NNLM-driven fuzzer, which auto-\nmatically ﬁnds bugs in JS engines. Recall that the overall\ndesign of Montage is driven by two observations: (1) secu-\nrity bugs often arise from ﬁles that were previously patched\nfor different causes, and (2) the JS test code that triggers\n67 66 63\n58\n49\n42\n26\n100\n20\n40\n60\n70 80 90 100\nCommon Fragments Percentage (%)\n# of PoC Files\nFigure 3: The number of all PoC ﬁles whose common frag-\nment percentages are greater than varying percentages.\nsecurity-related bugs heavily reuses AST fragments found in\nthe existing regression test sets.\nWe propose a novel fuzzing technique that captures these\nobservations. We train an NNLM to capture the syntactic and\nsemantic relationships among fragments from the regression\ntest sets. When generating a new JS test, Montage mutates the\nAST of a given JS regression test. It replaces a subtree of the\nAST with a new subtree, using the trained NNLM. Thus, each\ngenerated test stems from a given regression test that checks\npreviously patched or buggy logic, thereby, capturing the ﬁrst\nobservation. At the same time, it invokes functionalities in\ndifferent execution contexts by assembling existing fragments\nunder the guidance of the NNLM, which addresses the second\nobservation.\nFigure 4 shows the overall workﬂow of Montage. Phase\nI prepares the training instances from given regression test\nsuites. Each training instance is a sequence of AST unit sub-\ntrees, called fragments. Phase II trains an NNLM that learns\ncompositional relationships among fragments. These two\nphases are one-time setup procedures. Phase III generates\nJS tests by leveraging the trained model.\nJS’JS\n,\n,\n,\nPhase I\nBuilding training data\nPhase II\nTraining LSTM model\nPhase III\nGenerating JS tests\nFigure 4: Overview of Montage.\nPhase I begins with a given training set of JS regression\ntest ﬁles. It parses each JS ﬁle into an AST and normalizes\nidentiﬁers that appeared in the AST to deduplicate function\nand variable names. Figure 1 shows a normalized JS ﬁle\nexample. Each appeared variable name is changed into a\ncommon name, such asv0 or v1. From a normalized AST tree,\nPhase I then extracts multiple unit subtrees, each of which\nis called a fragment. For each node in the AST, Montage\nrecursively slices a unit subtree of depth one. Each of the\nsliced subtrees becomes a fragment of the AST. It then emits\nthe sequence of these fragments, produced by the pre-order\ntraversal of their root nodes in the normalized AST tree.\nPhase II trains the NNLM given a set of fragment se-\nquences. From a given fragment sequence of an arbitrary\nlength, we design the NNLM to suggest the next fragments,\nwhich are likely to appear after this fragment sequence. This\nframing is a key contribution of this paper. Note that it is not\nstraightforward to model the inherent structural relationships\nof an AST in such a way that a language model can learn.\nBy leveraging the fragments encapsulating the structural re-\nlationships of ASTs, we encode a given AST into fragment\nsequences. Considering that a vast volume of natural language\nNNLMs have been trained upon word sequences, this frag-\nment sequencing eases the application of existing prevailing\nNNLMs for generating JS tests.\nHere, the objective is to train the NNLM to learn com-\npositional relationships among fragments so that the JS test\ncode generated from the trained model reﬂects the syntax and\nsemantics of the given training set, which is the regression\ntesting set of JS engines.\nPhase III generates a new JS test by leveraging the trained\nmodel and the AST of a regression test. Given a set of ASTs\nfrom regression test suites, it randomly picks a seed AST.\nThen, it randomly selects a subtree for Montage to replace.\nWhen generating a new subtree, Montage considers a con-\ntext, the sequence of all fragments that precedes the selected\nsubtree. Montage iteratively appends fragments from the root\nnode of the selected subtree while considering its context.\nBecause the current AST is assembled from fragments, it\nis expected that some variables and function identiﬁers in the\nAST nodes are used without proper declarations. Montage,\nthus, resolves possible reference errors by renaming them\nwith the declared identiﬁers. Finally, Montage checks the\ngenerated test and reports a bug if the code crashes the target\nJS engine.\nOther model guided approaches. Previous studies pre-\nsented language models, which can predict the lexical code\ntokens in source code. Such framing of language models\nhas been vastly studied while addressing code completion\nproblems [40, 53]. However, the generation of an executable\ntest is more challenging than the code completion problem\nthat predicts a limited number of semantically correct lex-\nical tokens. To our knowledge, the PDF fuzzer proposed\nby Singh et al. [16] is the ﬁrst system that employs a\ncharacter-level RNN model to generate PDF tests. We eval-\nuated whether our fragment-based approach performs better\nthan the character-level RNN model approach in ﬁnding JS\nengine bugs (see §7.5).\n5 Design\nThe design goal of Montage is to generate JS test inputs\nthat can trigger security vulnerabilities in JS engines, which\n(1) reﬂect the syntactic and semantic patterns of a given JS\ntraining set, and (2) trigger no reference errors.\nIt is a technical challenge to frame the problem of teach-\ning a language model the semantic and syntactic patterns of\ntraining code. We address this challenge by abstracting the\nhierarchical structure by AST subtrees, which we refer to as\nfragments. We then enable the language model to learn the\ncompositional relationships between fragments.\nWe propose a novel code generation algorithm that lever-\nages a trained language model. We harness an existing JS\ncode that is already designed to trigger JS engine defects.\nMontage alters this existing JS code by replacing one of its\nAST subtrees with a new subtree that the trained language\nmodel generates. Thus, Montage is capable of generating a\nnew JS test, semantically similar to the regression test case\nthat triggers a previously reported bug. We expect that this\nnew JS test triggers a new bug in a different execution context.\n5.1 Phase I: Building Training Data of Frag-\nment Sequences\nPhase I prepares training instances using a given training set.\nIt conducts parsing and fragmentation.\n5.1.1 Parsing and Normalizing\nPhase I builds an AST by parsing each JS ﬁle in a training\nset and normalizes the parsed AST. Because the training set\nincludes a variety of JS ﬁles from various developers, iden-\ntiﬁer naming practices are not necessarily consistent. Thus,\nit is natural that the training ﬁles have diverse variable and\nfunction names across different JS ﬁles. Consider two JS ﬁles\nthat contain a JS statement var b = a + 1 and var c =\nd + 1, respectively. Both have the same AST structure and\nsemantics, but different identiﬁers.\nThis pattern increases the size of unnecessary vocabulary\nfor a language model to learn, rendering the model evaluation\nexpensive as it requires more training instances. To have\nconcise ASTs with consistent identiﬁer names, we rename all\nthe variable and function identiﬁers in the ASTs.\nSpeciﬁcally, for each declared variable identiﬁer, we as-\nsign a sequential number in the order of their appearance in\na given AST. We then replace each variable name with a\nnew name that combines a common preﬁx and its sequential\nnumber, such as v0 and v1. We also apply the same proce-\ndure to function identiﬁers, e.g., f0 and f1. We deliberately\nexclude language-speciﬁc built-in functions and engine ob-\njects from the normalization step as normalizing them affects\nthe semantics of the original AST. For an eval function that\ndynamically evaluates a given string as the JS code, we ﬁrst\nextract the argument string of the eval function and strip it\nout as the JS code when the argument is a constant string.\nSubsequently, we normalize identiﬁers in the JS code stripped\nout from the eval argument.\nAs our training set is derived from regression tests of JS\nengines, JS ﬁles in the set make heavy use of predeﬁned\nfunctions for testing purposes. Therefore, we manually identi-\nﬁed such vendor-provided testing functions and ignore them\nduring the normalization step. That is, we treated common\ntesting functions provided by each JS engine vendor as a\nbuilt-in function and excluded them from normalization.\n5.1.2 Fragmentation\nMontage slices each normalized AST into a set of subtrees\nwhile ensuring that the depth of each subtree is one. We call\nsuch a unit subtree as a fragment.\nWe represent an AST T with a triple (N,E,n0), where N\nis the set of nodes in T , E is the set of edges in T , and n0 is\nthe root node of T . We denote the immediate children of a\ngiven AST node ni by C (ni), where ni is a node in N. Then,\nwe deﬁne a subtree of T where the root node of the subtree is\nni. When there is such a subtree with a depth of one, we call\nit a fragment. We now formally deﬁne it as follows.\nDeﬁnition 1 (Fragment). A fragment of T = (N,E,n0) is a\nsubtree Ti = (Ni,Ei,ni), where\n• ni ∈N s.t. C (ni) ̸= /0.\n• Ni = {ni}⋃C(ni).\n• Ei = {(ni,n′) |n′= C(ni)}.\nIntuitively, a fragment whose root node is ni contains its\nchildren and their tree edges. Note that each fragment in-\nherently captures an exercised production rule of the JS lan-\nguage grammar employed to parse the AST. We also de-\nﬁne the type of a fragment as the non-terminal symbol of\nits root node ni. For instance, the ﬁrst fragment at the bot-\ntom side of Figure 2 corresponds to the assignment expres-\nsion statement in Line 3 of Figure 1. The fragment possesses\nfour nodes whose root node is the non-terminal symbol of\nan AssignmentExpression, which becomes the type of this\nfragment.\nMontage then generates a sequence of fragments by per-\nforming the pre-order traversal on the AST. When visiting\neach node in the AST, it emits the fragment whose root is the\nvisited node. The purpose of the pre-order sequencing is to\nsort fragments by the order of their appearance in the original\nAST. For example, the bottom side of Figure 2 shows the\nsequence of seven fragments obtained from the AST subtree\nin the ﬁgure.\nWe model the compositional relationships between frag-\nments as a pre-order sequencing of fragments so that an\nNNLM can predict the next fragment to use based on the\nfragments appearing syntactically ahead. In summary, Phase\nI outputs the list of fragment sequences from the training set\nof normalized ASTs.\nLSTM\nht\nfr1\nLSTM\nh1\nLSTM\nfr0\nh0\n…\nTt+1\nf(x)\nfrt\nLSTM\nPt+1\nFigure 5: Architecture of Montage LSTM model. ⊕in the\nﬁgure denotes a concatenation.\n5.2 Phase II: Training an LSTM Model\nAll distinct fragments become our vocabulary for the NNLM\nto be trained. Before training, we label the fragments whose\nfrequency is less than ﬁve in the training set as out-of-\nvocabulary (OoV). This is a standard procedure for building\na language model to prune insigniﬁcant words [22, 40, 43].\nEach fragment sequence represents a JS ﬁle in the training\nset. This sequence becomes a training instance. We build\na statistical language model from training instances so that\nthe model can predict the next fragment based on all of its\npreceding fragments, which is considered as a context. This\nway, the model considers each fragment as a lexicon, and\nthereby, suggests the next probable fragments based on the\ncurrent context.\nTraining objectives. The overall objective is to model a\nfunction f : X →Y such that y ∈Y is a probability distribu-\ntion for the next fragment f rt+1, given a fragment sequence\nx = [f r0, f r1, ...,f rt ] ∈X, where f ri denotes each fragment\nat time step i. Given x, the model is trained to (1) predict the\ncorrect next fragment with the largest probability output and\n(2) prioritize fragments that share the same type with the true\nfragment over other types of fragments. Note that this training\nobjective accords with our code generation algorithm in that\nMontage randomly selects the next fragment of a given type\nfrom the top k suggestions (see §5.3).\nLSTM. To implement such a statistical language model, we\ntake advantage of the LSTM model [23]. Figure 5 depicts the\narchitecture of Montage LSTM model. Our model consists\nof one projection, one LSTM, and one output layers. The\nprojection layer is an embedding layer for the vocabulary\nwhere each fragment has a dimension size of 32. When f rt\nis passed into the model, it is converted into a vector, called\nembedding, after passing the projection layer.\nThen, the embedding vector becomes one of the inputs for\nthe LSTM layer with a hidden state size of 32. At each time\nstep, the LSTM cell takes three inputs: (1) a hidden state ht−1\nand (2) a cell state ct−1 from the previous time step; and (3)\nthe embedding of a new input fragment. This architecture\nenables the model to predict the next fragment based on the\ncumulative history of preceding fragments. In other words,\nthe LSTM model is not limited to considering a ﬁxed number\nof preceding fragments, which is an advantage of using an\nRNN model.\nThe output of the LSTM layer ht is then concatenated with\ntwo other vectors: (1) the type embedding Tt+1 of the next\nfragment, and (2) the fragment embedding Pt+1 of the parent\nfragment of the next fragment in its AST. The concatenated\nvector is now fed into the ﬁnal output layer and it outputs a\nvector f (x) of vocabulary size to predict the next fragment.\nLoss function. To address our training objectives, we deﬁned\na new loss function that rewards the model to locate type-\nrelevant fragments in its top suggestions. The LSTM model\nis trained to minimize the following empirical loss over the\ntraining set (x,y) ∈D.\ng(x) =softmax( f (x))\nLD( f ) = 1\n|D| ∑\n(x,y)∈D\nl1(g(x),y)+ l2(g(x),y) (1)\nAs shown in Equation 1, the loss function has two terms: l1\nand l2. Note that these terms are designed to achieve our two\ntraining objectives, respectively.\nl1(g(x),y) =−\nN\n∑\ni=1\nyi logg(x)i\nl2(g(x),y) = ∑\ni∈top(n)\ng(x)i − ∑\nj∈type(y)\ng(x)j,\n(2)\nEquation 2 describes each term in detail. In the equation, n\ndenotes the number of fragments whose types are same as that\nof the true fragment. top(n) and type(y) indicate functions\nthat return the indices of top n fragments and fragments of\nthe true type, respectively.\nl1 is a cross entropy loss function, which has been used for\ncommon natural language models [29, 34]. l2 is employed for\nrewarding the model to prioritize fragments that have the same\ntype as the true fragment. We formally deﬁne l2 as a type\nerror. It is a gap between two values: the sum of the model\noutput probabilities corresponding to (1) top n fragments and\n(2) fragments of the true type.\nBy reducing the sum of l1 and l2 while training, the model\nachieves our training objectives. Intuitively, the LSTM model\nis trained not only to predict the correct fragment for a given\ncontext, but also to locate fragments whose types are same as\nthe correct fragment in its top suggestions.\nThe fundamental difference of Montage from previous ap-\nproaches that use probabilistic language models [41,56] lies in\nthe use of fragments. To generate JS code, TreeFuzz [41] and\nSkyFire [56] use a PCFG and PCSG to choose the next AST\nproduction rule from a given AST node, respectively. SkyFire\ndeﬁnes its context to be sibling and parent nodes from a given\nAST. It picks an AST production rule that is less frequent in\nthe training set. In contrast, Montage selects a fragment based\non the list of fragments, not AST nodes. Therefore, Montage\nis capable of capturing the global composition relationships\namong code fragments to select the next code fragment. Fur-\nthermore, Montage preserves the semantics in the training\nset by slicing the AST nodes into fragments, which is used\nas a lexicon for generating JS code. We frame the problem\nof training a language model to leverage fragments and their\nsequences, which makes Montage compatible with prevalent\nstatistical language models.\n5.3 Phase III: Generating JS Tests\nGiven a set of ASTs from regression tests and the LSTM\nmodel, Phase III ﬁrst mutates a randomly selected seed AST\nby leveraging the LSTM model. Then, it resolves reference\nerrors in the skeleton AST.\nAlgorithm 1 describes our code generation algorithm. The\nMutateAST function takes two conﬁgurable parameters from\nusers.\nfmax The maximum number of fragments to append.\nThis parameter controls the maximum number\nof fragments that a newly composed subtree can\nhave.\nktop The number of candidate fragments. Montage ran-\ndomly selects the next fragment from suggestions\nof the ktop largest probabilities at each iteration.\nAfter several exploratory experiments, we observed that\nbloated ASTs are more likely to have syntactical and seman-\ntic errors. We also observed that the accuracy of the model\ndecreases as the size of an AST increases. That is, as the size\nof AST increases, Montage has a higher chance of failures in\ngenerating valid JS tests. We thus capped the maximum num-\nber of fragment insertions with fmax and empirically chose its\ndefault value to be 100. For ktop , we elaborate on its role and\neffects in detail in §7.3.\n5.3.1 Mutating a Seed AST\nThe MutateAST function takes in a set of ASTs from regres-\nsion tests, the trained LSTM model, and the two parameters. It\nthen begins by randomly selecting a seed AST from the given\nset (Line 2). From the seed AST, it removes one randomly\nselected subtree (Line 3). Note that the pruned AST becomes\na base for the new JS test. Finally, it composes a new subtree\nby leveraging the LSTM model (Lines 4-13) and returns the\nnewly composed AST.\nAfter selecting a seed AST in Line 2, we randomly prune\none subtree from the AST by invoking the RemoveSubtree\nfunction. The function returns a pruned AST and the initial\ncontext for the LSTM model, which is a fragment sequence\nup to the fragment where Montage should start to generate a\nnew subtree. This step makes a room to compose new code.\nIn the while loop in Lines 4-13, the MutateAST function\nnow iteratively appends fragments to the AST at most fmax\ntimes by leveraging the LSTM model. The loop starts by se-\nlecting the next fragment via the PickNextFrag function in\nLine 6. The PickNextFrag function ﬁrst queries the LSTM\nmodel to retrieve the ktop suggestions. From the suggestions,\nAlgorithm 1: Mutating a seed AST\ninput : A set of ASTs from regression tests (T).\nThe LSTM model trained on fragments (model).\nThe max number of fragments to append ( fmax).\nThe number of candidate fragments (ktop ).\noutput :A newly composed AST.\n1 function mutateast(T, model, f max, ktop I\n2 n0 ←pickrandomseed(TI\n3 n0, context ←removesubtree(n0I\n4 count ←0\n5 while count ≤fmax do\n6 next_ f rag←picknextfrag(model, ktop , contextI\n7 if next_ f rag= ∅ then\n8 return\n9 n0 ←appendfrag(n0, next_ f ragI\n10 if not isastbroken(n0I then\n11 break\n12 context.append (next_ f rag)\n13 count ←count +1\n14 return n0\n15 function appendfrag(node, next_ f ragI\n16 C ←node.child() /* Get direct child nodes. */\n17 if isnonterminal(nodeI ∧C = ∅ then\n18 node ←next_ f rag\n19 return\n20 for c ∈C do\n21 appendfrag(c, f rag_seqI\nthe function repeats random selections until the chosen frag-\nment indeed has a correct type required for the next fragment.\nIf all the suggested fragments do not have the required type,\nthe MutateAST function stops here and abandon the AST.\nOtherwise, it continues to append the chosen fragment by\ninvoking the AppendFrag function.\nThe AppendFrag function traverses the AST in the pre-\norder to ﬁnd where to append the fragment. Note that this\nprocess is exactly the opposite process of an AST fragmenta-\ntion in §5.1.2. Because we use a consistent traversal order in\nPhase I and III, we can easily ﬁnd whether the current node\nis where the next fragment should be appended. Lines 16-19\nsummarize how the function determines it. The function tests\nwhether the current node is a non-terminal that does not have\nany children. If the condition meets, it appends the fragment\nto the current node and returns. If not, it iteratively invokes\nitself over the children of the node for the pre-order traversal.\nNote that the presence of a non-terminal node with no\nchildren indicates that the fragment assembly of the AST is\nstill in progress. The IsASTBroken function checks whether\nthe AST still holds such nodes. If so, it keeps appending the\nfragments. Otherwise, the MutateAST function returns the\ncomposed skeleton AST.\nWe emphasize that our code generation technique based\non code fragments greatly simpliﬁes factors that a language\nmodel should learn in order to generate an AST. TreeFuzz [41]\nallows a model to learn ﬁne-grained relationships among\nedges, nodes, and predecessors in an AST. Their approach\nrequires to produce multiple models each of which covers a\nspeciﬁc property that the model should learn. This, though,\nbrings the unfortunate side-effects of managing multiple mod-\nels and deciding priorities in generating an AST when the\npredictions from different models conﬂict with each other.\nOn the other hand, our approach abstracts such relationships\nas fragments, which becomes building blocks for generating\nAST. The model only learns the compositional relationships\nbetween such blocks, which makes training and managing a\nlanguage model simple.\n5.3.2 Resolving Reference Errors\nPhase III resolves the reference errors from the generated\nAST, which appear when there is a reference to an undeclared\nidentiﬁer. It is natural for the generated AST to have reference\nerrors since we assembled fragments that are used in different\ncontexts across various training ﬁles. The reference error\nresolution step is designed to increase the chance of triggering\nbugs by making a target JS engine fully exercise the semantics\nof a generated testing code. The previous approaches [18,\n24, 54] reuse existing AST subtrees and attach them into a\nnew AST, which naturally causes reference errors. However,\nthey overlooked this reference error resolution step without\naddressing a principled solution.\nWe propose a systematic way of resolving reference errors,\nwhich often accompany type errors. Speciﬁcally, we take into\naccount both (1) statically inferred JS types and (2) the scopes\nof declared identiﬁers. Montage harnesses these two factors\nto generate JS test cases with fewer reference errors in the run\ntime.\nThere are three technical challenges that make resolving\nreference errors difﬁcult. (1) In JS, variables and functions\ncan be referenced without their preceding declarations due\nto hoisting [37]. Hoisting places the declarations of identi-\nﬁers at the top of the current scope in its execution context;\n(2) It is difﬁcult to statically infer the precise type of each\nvariable without executing the JS code because of no-strict\ntype checking and dynamically changing types; and (3) Each\nvariable has its own scope so that referencing a live variable\nis essential to resolve reference errors.\nTo address these challenges, Montage prepares a scope for\neach AST node that corresponds to a new block body. Mon-\ntage then starts traversing from these nodes and ﬁlls the scope\nwith declared identiﬁers including hoistable declarations.\nEach declared identiﬁer in its scope holds the undefined\ntype at the beginning.\nWhen Montage encounters an assignment expression in its\ntraversal, it statically infers the type of its right-hand expres-\nsion via its AST node type and assigns the inferred type to its\nleft-hand variable. Montage covers the following statically in-\nferred types: array, boolean, function, null, number,\nobject, regex, string, undefined, and unknown. Each\nscope has an identiﬁer map whose key is a declared identiﬁer\nand value is an inferred type of the declared identiﬁer.\nTo resolve reference errors, Montage identiﬁes an unde-\nclared variable while traversing each AST node and then\ninfers the type of this undeclared variable based on its us-\nage. A property or member method reference of such an un-\ndeclared variable hints to Montage to infer the type of the\nundeclared variable. For instance, the length property ref-\nerence of an undeclared variable assigns the string type to\nthe undeclared variable. From this inferred type, Montage\nreplaces the undeclared identiﬁer with a declared identiﬁer\nwhen its corresponding type in the identiﬁer map is same as\nthe inferred type. If the inferred type of an undeclared variable\nis unknown, it ignores the type and randomly picks one from\nthe list of declared identiﬁers. For all predeﬁned and built-in\nidentiﬁers, Montage treats them as declared identiﬁers.\n6 Implementation\nWe implemented Montage with 3K+ LoC in Python and JS.\nWe used Esprima 4.0 [21] and Escodegen 1.9.1 [51] for pars-\ning and printing JS code, respectively. As both libraries work\nin the Node.js environment, we implemented an inter-process\npipe channel between our fuzzer in Python and the libraries.\nWe implemented the LSTM models with PyTorch\n1.0.0 [52], using the L2 regularization technique with a pa-\nrameter of 0.0001. The stochastic gradient descent with a\nmomentum factor of 0.9 served as an optimizer.\nWe leveraged the Python subprocess module to execute JS\nengines and obtain their termination signals. We only con-\nsidered JS test cases that crash with SIGILL and SIGSEGV\nmeaningful because crashes with other termination signals\nare usually intended ones by developers.\nTo support open science and further research, we publish\nMontage at https://github.com/WSP-LAB/Montage.\n7 Evaluation\nWe evaluated Montage in several experimental settings. The\ngoal is to measure the efﬁcacy of Montage in ﬁnding JS engine\nbugs, as well as to demonstrate the necessity of an NNLM\nin ﬁnding bugs. We ﬁrst describe the dataset that we used\nand the experimental environment. Then, we demonstrate (1)\nhow good a trained Montage NNLM is in predicting correct\nfragments (§7.2), (2) how we set a ktop parameter for efﬁcient\nfuzzing (§7.3), (3) how many different bugs Montage discov-\ners, which other existing fuzzers are unable to ﬁnd (§7.4),\nand (4) how much the model contributes to Montage ﬁnding\nbugs and generating valid JS tests (§7.5). We conclude the\nevaluation with ﬁeld tests on the latest JS engines (§7.6). We\nalso discuss case studies of discovered bugs (§7.7).\n7.1 Experimental Setup\nWe conducted experiments on two machines running 64-bit\nUbuntu 18.04 LTS with two Intel E5-2699 v4 (2.2 GHz) CPUs\n(88 cores), eight GTX Titan XP DDR5X GPUs, and 512 GB\nof main memory.\nTarget JS engine. The ChakraCore GitHub repository has\nmanaged the patches for all the reported CVEs by the commit\nmessages since 2016. That is, we can identify the patched\nversion of ChakraCore for each known CVE and have ground\ntruth that tells whether found crashes correspond to one of\nthe known CVEs [9]. Therefore, we chose an old version\nof ChakraCore as our target JS engine. We speciﬁcally per-\nformed experiments on ChakraCore 1.4.1, which was the ﬁrst\nstable version after January 31, 2017.\nData. Our dataset is based on the regression test sets of\nTest262 [13] and the four major JS engine repositories at\nthe version of January 31, 2017: ChakraCore, JavaScriptCore,\nSpiderMonkey, and V8. We excluded test ﬁles that Chakra-\nCore failed to execute because of their engine-speciﬁc syntax\nand built-in objects. We did not take into account ﬁles larger\nthan 30 KB because large ﬁles considerably increase the\nnumber of unique fragments with low frequency. In total, we\ncollected 1.7M LoC of 33,486 unique JS ﬁles.\nTemporal relationships. Montage only used the regression\ntest ﬁles committed before January 31, 2017, and performed\nfuzz testing campaigns on the ﬁrst stable version after January\n31, 2017. Thus, the bugs that regression tests in the training\ndataset check and the bugs that Montage is able to ﬁnd are\ndisjoint. We further conﬁrmed that all CVEs that Montage\nfound were patched after January 31, 2017.\nFragments. From the dataset, we ﬁrst fragmented ASTs to\ncollect 134,160 unique fragments in total. On average, each\ntraining instance consisted of 118 fragments. After replacing\nless frequent fragments with OoVs, they were reduced to\n14,518 vocabularies. Note that most replaced fragments were\nstring literals, e.g., bug summaries, or log messages.\nBug ground truth. Once Montage found a JS test triggering\na bug, we ran the test against every patched version of Chakra-\nCore to conﬁrm whether the found bug matches one of the\nreported CVEs. This methodology is well-aligned with that of\nKlees et al. [31], which suggests counting distinct bugs using\nground truth. When there is no match, the uniqueness of a\ncrash was determined by its instruction pointer address with-\nout address space layout randomization (ASLR). We chose\nthis conservative setting to avoid overcounting the number of\nfound bugs [36].\n7.2 Evaluation of the LSTM Model\nTo train and evaluate the LSTM model of Montage, we per-\nformed a 10-fold cross-validation on our dataset. We ﬁrst\nrandomly selected JS ﬁles for the test set, which accounted\nfor 10% of the entire dataset. We then randomly split the\n32\n64\n128\n256\n0 25 50 75 100\nEpoch\nPerplexity\nTraining Set\nValidation Set\n(a) Perplexity of the model.\n0.15\n0.20\n0.25\n0.30\n0 25 50 75 100\nEpoch\nType Error\nTraining Set\nValidation Set (b) Type error proportion.\nFigure 6: Perplexity and type error proportion of the LSTM\nmodel measured against the training and validation sets over\nepochs. They are averaged across the 10 cross-validation sets.\nremaining ﬁles into 10 groups. We repeated holding out one\ngroup for the validation set and taking the rest of them for the\ntraining set for 10 times.\nFigure 6 illustrates the perplexity and type error of the\nLSTM model measured on the training and validation sets.\nRecall that the loss function of the model is a sum of the log\nperplexity and type error (§5.2).\nPerplexity. Perplexity measures how well a trained model\npredicts the next word that follows given words without per-\nplexing. It is a common metric for evaluating natural language\nmodels [29, 34]. A model with a lower perplexity performs\nbetter in predicting the next probable fragment. Note from\nFigure 6a that the perplexities for both the training and val-\nidation sets decrease without a major difference as training\ngoes on.\nType error.Type error presents how well our model predicts\nthe correct type of a next fragment (recall §5.2). A model with\na low type error is capable of predicting the fragments with\nthe correct type in its top predictions. Note from Figure 6b\nthat the type errors for both the training and validation sets\ncontinuously decrease and become almost equal as the epoch\nincreases.\nThe small differences of each perplexity and type error\nbetween the training set and validation set demonstrate that\nour LSTM model is capable of learning the compositional\nrelations among fragments without overﬁtting or underﬁtting.\nWe further observed that epoch 70 is the optimal point\nat which both valid perplexity and valid type errors start to\nplateau. We also noticed that the test perplexity and test type\nerrors at epoch 70 are 28.07 and 0.14, respectively. Note\nfrom Figure 6 that these values are close to those from the\nvalidation set. It demonstrates that the model can accurately\npredict fragments from the test set as well. Thus, for the\nremaining evaluations, we selected the model trained up to\nepoch 70, which took 6.6 hours on our machine.\n60\n70\n80\n1 2 4 8 16 32 64\nTop k (k top )\nPass Rate (%)\nFigure 7: The pass rate of generated JS tests over ktop .\n7.3 Effect of the ktop Parameter\nMontage assembles model-suggested fragments when replac-\ning an AST subtree of a given JS code. In this process, Mon-\ntage randomly picks one fragment from the Top-k (ktop ) sug-\ngestions for each insertion. Our key intuition is that selecting\nfragments from the Top-k rather than Top-1 suggestion helps\nMontage generate diverse code, which follows the pattern of\nJS codes in our dataset but slightly differs from them. We\nevaluated the effect of the ktop with seven different values\nvarying from 1 to 64 to verify our intuition.\nWe measured the pass rate of generated JS tests. A pass\nrate is a measurement unit of demonstrating how many tests a\ntarget JS engine executes without errors among generated test\ncases. To measure the pass rate, we ﬁrst generated 100,000 JS\ntests with each ktop value. We only considered ﬁve runtime\nerrors deﬁned by the ECMAScript standard as errors [27].\nWe then ran Montage for 12 hours with each ktop value to\ncount the number of crashes found in ChakraCore 1.4.1.\nFigures 7 and 8 summarize our two experimental results,\nrespectively. As shown in Figure 7, the pass rate of Montage\ndecreases from 79.82% to 58.26% as the ktop increases. This\nfact demonstrates that the suggestion from the model con-\nsiderably affects the generation of executable JS tests. It is\nalso consistent with the experimental results from Figure 8b,\nin that Montage ﬁnds fewer total crashes when considering\nmore fragment suggestions in generating JS tests. Note that\nMichael et al. [41] demonstrated that their TreeFuzz achieved\na 14% pass rate, which is signiﬁcantly lower than that Mon-\ntage achieved.\nHowever, note from Figure 8b that the number of unique\ncrashes increases, asktop increases, unlike that of total crashes.\nThis observation supports our intuition that increasing thektop\nhelps Montage generate diverse JS tests that trigger undesired\ncrashes in the JS engines. Figure 8 also shows that Montage\nfound more crashes from the debug build than the release\nbuild. Moreover, unlike the debug build, the results for the re-\nlease build did not show a consistent pattern. We believe these\nresults are mainly due to the nature of the debug build. It be-\nhaves more conservatively with inserted assertion statements,\nthus producing crashes for every unexpected behavior.\nAs Klees et al. [31] stated, fuzzers should be evaluated\nusing the number of unique crashes, not that of crashing in-\nputs. For both release and debug builds of ChakraCore 1.4.1,\nTable 1: The number of bugs found with four fuzzers and four different approaches: Montage, CodeAlchemist (CA), jsfunfuzz,\nand IFuzzer; random selection, Markov chain, char/token-level RNN, and Montage (ktop = 64) without resolving reference errors.\nWe marked results in bold when the difference between Montage and the other approach is statistically signiﬁcant.\nBuild Metric\n# of Unique Crashes (Known CVEs)\nMontage CA jsfunfuzz IFuzzer random Markov ch-RNN Montage †\nRelease\nMedian 23 (7) 15 (4) 27 (3) 4 (1) 12 (3) 19 (6) 1 (0) 12 (4)\nMax 26 (8) 15 (4) 31 (4) 4 (2) 15 (4) 22 (7) 1 (1) 13 (5)\nMin 20 (6) 14 (3) 25 (3) 0 (0) 10 (3) 16 (5) 0 (0) 11 (4)\nStdev 2.30 0.55 2.19 1.79 2.07 2.39 0.45 0.84\n(0.84) (0.55) (0.45) (0.71) (0.45) (0.84) (0.55) (0.45)\np-value N/A 0.012 0.029 0.012 0.012 0.037 0.012 0.012\n(0.012) (0.012) (0.012) (0.012) (0.144) (0.012) (0.012)\nDebug\nMedian 49 (12) 26 (6) 27 (4) 6 (1) 31 (7) 44 (11) 3 (0) 41 (9)\nMax 52 (15) 30 (6) 29 (5) 8 (3) 34 (7) 50 (12) 4 (1) 43 (10)\nMin 45 (11) 24 (4) 24 (4) 2 (0) 27 (6) 42 (8) 1 (0) 38 (8)\nStdev 2.70 2.61 2.12 2.41 2.88 3.27 1.10 1.82\n(1.64) (0.89) (0.45) (1.10) (0.45) (1.67) (0.5) (0.84)\np-value N/A 0.012 0.012 0.012 0.012 0.144 0.012 0.012\n(0.012) (0.012) (0.012) (0.012) (0.298) (0.012) (0.012)\nBoth Total 133 (15) 65 (7) 57 (4) 22 (3) 72 (9) 109 (14) 10 (2) 74 (10)\nCommon 36 (8) 22 (2) 17 (3) 1 (0) 29 (6) 37 (8) 1 (0) 37 (7)\n† Montage without resolving reference errors.\n6\n8\n10\n12\n90\n120\n150\n180\n1 2 4 8 16 32 64\nTop k (k top )\n# of Unique Crashes\n# of Total Crashes\nUnique\nTotal\n(a) Crashes on the release build.\n20\n24\n28\n3500\n4000\n4500\n5000\n5500\n6000\n1 2 4 8 16 32 64\nTop k (k top )\n# of Unique Crashes\n# of Total Crashes\nUnique\nTotal (b) Crashes on the debug build.\nFigure 8: The number of total and unique crashes found in\nChakraCore 1.4.1 while varying the ktop .\nMontage found the largest number of unique crashes when\nthe ktop was 64. Therefore, we picked the ktop to be 64 for the\nremaining experiments.\n7.4 Comparison to State-of-the-art Fuzzers\nTo verify the ability to ﬁnd bugs against open-source state-\nof-the-art fuzzers, we compared Montage with CodeAl-\nchemist [20], jsfunfuzz [38], and IFuzzer [54]. jsfunfuzz and\nIFuzzer have been used as a controlled group in the compar-\nison studies [20, 24]. Furthermore, CodeAlchemist, which\nassembles its building blocks in a semantics-aware fashion,\nand IFuzzer, which employs an evolutionary approach with\ngenetic programming, have in common with Montage in that\nthey take in a corpus of JS tests. Since Montage, CodeAl-\nchemist, and IFuzzer start from given seed JS ﬁles, we fed\nthem the same dataset collected from the repositories of\nTest262 and the four major JS engines. For fair comparison,\nwe also conﬁgured jsfunfuzz to be the version of January 31,\n2017, on which we collected our dataset (recall §7.1).\nWe ran all four fuzzers on ChakraCore 1.4.1 and counted\nthe number of found unique crashes and known CVEs. Since\nmost fuzzers depend on random factors, which results in a\nhigh variance of fuzzing results [31], we conducted ﬁve trials;\neach trial lasted for 6,336 CPU hours (72 hours ×88 cores).\nWe intentionally chose such a long timeout, because fuzzers\nusing evolutionary algorithms, such as IFuzzer, could improve\ntheir bug-ﬁnding ability as more tests are generated. Note that\nwe expended a total of 31,680 CPU hours on the ﬁve trials\nof each fuzzer. Because Montage took 6.6 hours to train its\nlanguage model and used this model for the ﬁve trials, we set\nthe timeout of other fuzzers 1.3 hours (6.6 hours / 5 trials)\nlonger than that of Montage for fair comparison.\nThe Montage, CA, jsfunfuzz, and IFuzzer columns of Ta-\nble 1 summarize the statistical analysis of the comparison\nexperimental results. For the release build, Montage found the\nlargest number of CVEs, whereas jsfunfuzz still discovered\nmore unique crashes than others. For the debug build, Mon-\ntage outperformed all others in ﬁnding both unique crashes\nand CVEs. We performed two-tailed Mann Whitney U tests\nand reported p-values between Montage and the other fuzzers\nin the table. We veriﬁed that all results are statistically signiﬁ-\ncant with p-values less than 0.05.\nThe last two rows of the table show the number of total and\ncommon bugs found in the ﬁve trials from the release and\ndebug builds, respectively. We counted common bugs when\nMontage found these bugs in every run of the ﬁve campaigns.\nWhen a bug was found during at least one campaign, they are\n105 (8) 44 (1)\n45 (1)\n8 (1)\n17 (4)\n1 (0)\n3 (2)\njsfunfuzz\nMontage CA\n(a) The # of total bugs.\n24 (5) 10 (0)\n12 (0)\n1 (1)\n8 (0)\n1 (0)\n3 (2)\njsfunfuzz\nMontage CA\n(b) The # of common bugs.\nFigure 9: The comparison of unique crashes (known CVEs)\nfound by Montage, CodeAlchemist (CA), and jsfunfuzz.\ncounted in the total bugs. Note that Montage found at least\n2.14×more CVEs compared to others in a total of the ﬁve\ntrials. We believe that these results explain the signiﬁcance\nof Montage in ﬁnding security bugs compared to the other\nstate-of-the-art fuzzers.\nWe also compared the bugs discovered by each fuzzer. Fig-\nure 9 depicts the Venn diagrams of unique bugs found in\nChakraCore 1.4.1. These Venn diagrams present the total and\ncommon bugs that each fuzzer found, corresponding to the\nlast two rows of Table 1. We excluded IFuzzer from the ﬁgure\nbecause all found CVEs were also discovered by Montage.\nNote from Figure 9a that Montage identiﬁed 105 unique\ncrashes in total, including eight CVEs that were not found by\nCodeAlchemist and jsfunfuzz. Furthermore, Montage discov-\nered all CVEs that were commonly found in the ﬁve trials of\nCodeAlchemist and jsfunfuzz, as shown in Figure 9b. How-\never, CodeAlchemist and jsfunfuzz also identiﬁed a total of\n45 and 46 unique bugs that were not found by Montage, re-\nspectively. These results demonstrate that Montage plays a\ncomplementary role against the state-of-the-art fuzzers in\nﬁnding distinctive bugs.\nPerformance over time. Figure 10 shows the number of\nCVEs that Montage found over time. The number increases\nrapidly in the ﬁrst 1,144 CPU hours (13 hours ×88 cores)\nof the fuzzing campaigns; however, Montage ﬁnds additional\nbugs after running for 2,640 CPU hours (30 hours×88 cores),\nthus becoming slow to ﬁnd new vulnerabilities.\n7.5 Effect of Language Models\nMontage generates JS tests by assembling language model-\nsuggested fragments. Especially, it takes advantage of the\nLSTM model to reﬂect the arbitrary length of preceding frag-\nments when predicting the next relevant fragments. However,\nMontage can leverage any other prevailing language models\nby its design, and the language model it employs may substan-\ntially affect its fuzzing performance. Therefore, to analyze\nthe efﬁcacy of the LSTM model in ﬁnding bugs, we ﬁrst con-\nducted a comparison study against two other approaches: (1)\n0\n5\n10\n15\n0 20 40 60\nTime (Hours)\n# of Known CVEs\nMedian\nMax\nMin\nFigure 10: The number of CVEs found by Montage over time.\na random fragment selection, and (2) Markov model-driven\nfragment selection.\nThe former approach is the baseline for Montage where\nfragments are randomly appended instead of querying a\nmodel. The latter approach uses a Markov model that makes\na prediction based on the occurrence history of the preceding\ntwo fragments. Speciﬁcally, we tailored the code from [25] to\nimplement the Markov chain.\nAdditionally, we compared our approach against a\ncharacter/token-level RNN language model-guided selection.\nIt leverages an NNLM to learn the intrinsic patterns from\ntraining instances, which is in common with ours. Recently\nproposed approaches [11,16,32], which resort to an NNLM to\ngenerate highly structured inputs, adopted an approach more\nor less similar to this one.\nNote that there is no publicly available character/token-\nlevel RNN model to generate JS tests. Thus, we referenced the\nwork of Cummins et al. [11] to implement this approach and\ntrained the model from scratch. To generate test cases from\nthe trained model, we referenced the work of Liu et al. [32]\nbecause their approach is based on the seed mutation like our\napproach.\nThe random, Markov, and ch-RNN columns of Table 1 sum-\nmarize the number of crashes found by each approach. We\nconducted ﬁve fuzzing campaigns, each of which lasted 72\nhours; all the underlying experimental settings are identical\nto those in §7.4. Note that we conducted resolving reference\nerrors and fed the same dataset as Montage when evaluating\nthe aforementioned three models. Montage outperformed the\nrandom selection and character/token-level RNN methods in\nthe terms of ﬁnding crashes and security bugs; thus, yield-\ning p-values under 0.05, which suggests the superiority of\nMontage with statistical signiﬁcance.\nWhen comparing the metrics from release and debug build\nbetween Montage and the Markov chain approach, Montage\nperformed better. Montage found more unique bugs in total as\nwell. However, the Mann Whitney U test deems the difference\ninsigniﬁcant. Nevertheless, we emphasize that Montage is ca-\npable of composing sophisticated subtrees that the Markov\nchain easily fails to generate. For instance, Montage gener-\nated a JS test triggering CVE-2017-8729 by appending 54\nfragments, which the Markov chain failed to ﬁnd. We provide\nmore details of this case in §7.7.1.\n0\n25\n50\n75\n100\n0 25 50 75 100\n# of Appended Fragments\nCumulative Percent (%)\nCrashes\nCVEs\nFigure 11: Empirical CDF of the number of appended frag-\nments against JS tests causing crashes in ChakraCore.\nTo evaluate the effectiveness of the LSTM model, we fur-\nther analyzed the number of fragments Montage appended to\ngenerate JS tests that caused ChakraCore 1.4.1 to crash in the\nexperiment from §7.4.\nFigure 11 shows the cumulative distribution function (CDF)\nof the number of inserted fragments against 169,072 and 5,454\nJS tests causing crashes and known CVEs, respectively. For\n90% of JS tests that caused the JS engine to crash, Mon-\ntage only assembled fewer than 15 fragments; however, it\nappended up to 52 fragments to generate 90% of JS tests\nthat found the known CVEs. This demonstrates that Montage\nshould append more fragments suggested by the model to\nﬁnd security bugs rather than non-security bugs. It also de-\nnotes that the random selection approach suffers from ﬁnding\nsecurity bugs. Note that Table 1 also accords with this result.\nFrom the aforementioned studies, we conclude that the\nLSTM model trained on fragments is necessary for ﬁnding\nbugs in the JS engines.\nResolving reference errors. We evaluated the importance of\nthe reference error resolution step (recall §5.3.2) in ﬁnding\nJS engine bugs. Speciﬁcally, we ran Montage with the same\nsettings as other approaches while letting it skip the reference\nerror resolution step but still leverage the same LSTM model.\nThe last column of Table 1 demonstrates that Montage ﬁnds\nfewer bugs if the resolving step is not applied, denoting that\nthe error resolving step improves the bug-ﬁnding capability of\nMontage. However, Montage still found more bugs than the\nother state-of-the-art fuzzers and the random approach even\nwithout the resolving step. Considering the random approach\nalso takes advantages of the error resolution step, the LSTM\nmodel signiﬁcantly contributes to ﬁnding JS engine bugs.\nPass rate. One of the key objectives of Montage is to generate\na valid JS test so that it can trigger deep bugs in JS engines.\nThus, we further measured how the use of language models\naffects the pass rate of generated codes. A pass rate indicates\nwhether generated test cases are indeed executed after passing\nboth syntax and semantic checking.\nFigure 12 illustrates the pass rate of 100,000 JS tests gen-\nerated by the four different approaches: Montage with and\nwithout resolving reference errors, the random selection, and\nthe Markov model. We excluded the character/token-level\nRNN approach because only 0.58% of the generated tests\nMontage †\nMontage\nRandom\nMarkov\n0 20 40 60 80\nPass Rate (%)\nFigure 12: The pass rate measured against four different ap-\nproaches: Montage ( ktop = 64) with and without resolving\nreference errors, random selection, and Markov model. Mon-\ntage without resolving reference errors is denoted by †.\nwere executed without errors. Such a low pass rate could be\none possible reason why this approach failed to ﬁnd many\nbugs, as shown in Table 1. As Liu et al. [32] also stated in\ntheir paper, we believe this result is attributed to the lack of\ntraining instances and the unique characteristics inherent in\nthe regression test suite.\nNote from the ﬁgure that resolving reference errors in-\ncreases the pass rate by 12.2%. As a result, this helped Mon-\ntage to ﬁnd more bugs, as shown in Table 1. On the other\nhand, the pass rates of the random selection and Markov\nmodel-guided approach were 5.2% and 11% greater than that\nof Montage, respectively. We manually inspected the JS tests\ngenerated by the random selection and the Markov model-\nguided approaches. We concluded that these differences stem\nfrom appending a small number of fragments. For instance,\nif a model always replaces one fragment, such as a string lit-\neral, from the seed ﬁle, all generated JS tests will be executed\nwithout errors.\n7.6 Field Tests\nWe evaluated the capability of Montage in ﬁnding real-world\nbugs. We have run Montage for 1.5 months on the latest\nproduction versions of the four major engines: ChakraCore,\nJavaScriptCore, SpiderMonkey, and V8. For this evaluation,\nwe collected datasets from the repository of each JS engine\nat the version of February 3, 2019. We additionally collected\n165 PoCs that triggered known CVEs as our dataset. Then,\nwe trained the LSTM model for each JS engine.\nMontage has found 37 unique bugs from the four major JS\nengines so far. Among the found bugs, 34 bugs were from\nChakraCore. The remaining two and one bugs were from\nJavaScriptCore and V8, respectively. We manually triaged\neach bug and reported all the found bugs to the vendors. In\ntotal, 26 of the reported bugs have been patched so far.\nEspecially, we reported three of the found bugs as security-\nrelated because they caused memory corruptions of the tar-\nget JS engines. The three security bugs were discovered in\nChakraCore 1.11.7, ChakraCore 1.12.0 (beta), and JavaScript-\nCore 2.23.3, respectively. Note that all of them got CVE IDs:\nCVE-2019-0860, CVE-2019-0923, and CVE-2019-8594. Par-\nticularly, we were rewarded for the bugs found in ChakraCore\nwith a bounty of 5,000 USD.\nOur results demonstrate that Montage is capable of ﬁnding\n37 real-world JS engine bugs, including three security bugs.\nWe further describe one of the real-world security bugs that\nMontage found in §7.7.3.\n7.7 Case Study\nTo show how Montage leverages the existing structure of the\nregression test, we introduce three bugs that Montage found.\nWe show two bugs that Montage found in ChakraCore 1.4.1\nfrom the experiment in §7.4. We then describe one of the real-\nworld security bugs found in the latest version of ChakraCore,\nwhich is already patched. Note that we minimized all test\ncases for ease of explanation.\n7.7.1 CVE-2017-8729\n1 ( function () {\n2 for (var v0 in [{\n3 v1 = class {},\n4 v2 = RP1P\n5 }. v2 = RP ]) {\n6 for ([] in {\n7 value : function () {},\n8 writable : false\n9 }){}\n10 }\n11 })();\nFigure 13: A test code that triggers CVE-2017-8729 on\nChakraCore 1.4.1.\nFigure 13 shows the minimized version of a gener-\nated test that triggers CVE-2017-8729 on ChakraCore\n1.4.1. From its seed ﬁle, Montage removed the body of\nFunctionExpression and composed a new subtree corre-\nsponding to Lines 2-10. Particularly, Montage appended 54\nfragments to generate the new test.\nChakraCore is supposed to reject the generated test be-\nfore its execution because it has a syntax error in the\nObjectPattern corresponding to Lines 2-5. However, as-\nsuming the ObjectPattern to be an ObjectExpression,\nChakraCore successfully parses the test and incorrectly infers\nthe type of the property v2 to be a setter. Thus, the engine\ncrashes with a segmentation fault when it calls the setter in\nLine 5. Interestingly, the latest version of ChakraCore still\nexecutes this syntax-broken JS test without errors but does\nnot crash.\nThe original regression test checked the functionalities re-\ngarding a complicated ObjectPattern. Similarly, the gener-\nated test triggered a type confusion vulnerability while parsing\nthe new ObjectPattern. Therefore, we believe that this case\ncaptures the design goal of Montage, which leverages an exist-\ning regression test and puts it in a different execution context\nto ﬁnd a potential bug.\n7.7.2 CVE-2017-8656\n1 var v1 = {\n2 /quotesingle.Vara/quotesingle.Var: function () {}\n3 }\n4 var v2 = /quotesingle.Vara/quotesingle.Var;\n5 ( function () {\n6 try {\n7 } catch ([ v0 = (v1[v2 ]. __proto__ (1, /quotesingle.Varb/quotesingle.Var))]) {\n8 var v0 = T;\n9 }\n10 v0 ++;\n11 })();\nFigure 14: A test code that triggers CVE-2017-8656 on\nChakraCore 1.4.1.\nFigure 14 shows a test case generated by Montage\nthat triggers CVE-2017-8656 on ChakraCore 1.4.1. Its\nseed ﬁle had a different AssignmentExpression as the\nparameter of a CatchClause in Line 7. From the seed\nAST, Montage removed a subtree corresponding to the\nAssignmentExpression in Line 7 and mutated it by append-\ning eight fragments that the LSTM model suggested.\nIn the generated code, the variable v0 is ﬁrst declared as\nthe parameter of the CatchClause (Line 7) and then rede-\nclared in its body (Line 8). At this point, the ChakraCore\nbytecode generator becomes confused with the scope of these\ntwo variables and incorrectly selects which one to initialize.\nConsequently, the variable v0 in Line 8 remains uninitialized.\nAs the JS engine accesses the uninitialized symbol in Line 10,\nit crashes with a segmentation fault.\nWe note that the seed JS test aimed to check possible scope\nconfusions, and the generated code also elicits a new vulnera-\nbility while testing a functionality similar to the one its seed\nJS test checks. Hence, this test case ﬁts the design objective\nof Montage.\n7.7.3 CVE-2019-0860\nFigure 15 describes a JS test triggering CVE-2019-0860 on\nChakraCore 1.12.0 (beta), which we reported to the vendor.\nIts seed ﬁle had a CallExpression instead of the statements\nin Lines 3-4. From the seed JS test, Montage removed a\nsubtree corresponding to the BlockStatement of the func-\ntion f0 and appended 19 fragments to compose a new block\nof statements (Lines 2-4). Notably, Montage revived the\nAssignmentExpression statement in Line 2, which is a re-\nquired precondition to execute the two subsequent statements\nand trigger the security bug.\nThe seed regression test was designed to test whether JS\nengines correctly handle referencing the arguments property\n1 function f0(f, p = {}) {\n2 f. __proto__ = p;\n3 f. arguments = TT ;\n4 f. arguments === TT ;\n5 }\n6\n7 let v1 = new Proxy({} , {});\n8 for ( let v0 = P; v0 < 1PPP ; ++ v0) {\n9 f0( function () { /quotesingle.Varuse strict /quotesingle.Var;}, v1 );\n10 f0( class C {}, v1 );\n11 }\nFigure 15: A test code that triggers CVE-2019-0860 on\nChakraCore 1.12.0 (beta).\nof a function in the strict mode. For usual cases, JS engines do\nnot allow such referencing; however, to place the execution\ncontext in an exceptional case, the seed JS test enables the\naccess by adding a Proxy object to the prototype chain of the\nfunction f (Line 2). As a result, this new test is able to access\nand modify the property value without raising a type error\n(Line 3).\nWhile performing the JIT optimization process initiated\nby the for loop in Lines 8-11, ChakraCore misses a postpro-\ncessing step of the property in Line 3, thus enabling to write\nan arbitrary value on the memory. Consequently, the engine\ncrashes with a segmentation fault as this property is accessed\nin Line 4.\nNote that the generated test case triggers a new vulnera-\nbility while vetting the same functionality that its seed tests.\nMoreover, theGlobOpt.cpp ﬁle, which is the most frequently\npatched ﬁle to ﬁx CVEs assigned to ChakraCore, was patched\nto ﬁx this vulnerability. Therefore, this JS test demonstrates\nthat Montage successfully discovers bugs that it aims to ﬁnd.\n8 Related Work\nFuzzing. There have been a vast volume of research on\ngeneration-based fuzzing. Highly-structured ﬁle fuzzing [15,\n42], protocol fuzzing [46, 47], kernel fuzzing [19, 28, 55], and\ninterpreter fuzzing [2, 6, 12, 38, 41, 56] are representative re-\nsearch examples.\nIMF infers the model of sequential kernel API calls to fuzz\nmacOS kernels [19]. Dewey et al. [12] generated code with\nspeciﬁed combinations of syntactic features and semantic\nbehaviors by constraint logic programming.\nGodefroid et al. [16] trained a language model from a large\nnumber of PDF ﬁles and let the model learn the relations\nbetween objects constituting the PDF ﬁles. Their approach\nof using a language model in generating tests is similar to\nours per se, but their approach is not directly applicable to\ngenerating JS tests, which demands modeling complicated\ncontrol and data dependencies.\nCummins et al. [11] also proposed a similar approach. They\ntrained an LSTM language model from a large corpus of\nOpenCL code. Unlike Montage, they trained the model at a\ncharacter/token-level, which does not consider the composi-\ntional relations among the AST subtrees.\nTreeFuzz [41] is another model-based fuzzer. Its model\nis built on the frequencies of co-occurring nodes and edges\nfrom given AST examples. Their modeling of generating\ntests is not directly applicable to the prevalent state-of-the-art\nlanguage models, tailored to train word sequences, not node\nand edge relations in ASTs.\nAschermann et al. [2] and Blazytko et al. [6] recently pro-\nposed NAUTILUS and GRIMOIRE , respectively. Both fuzzers\ntest programs that take highly structured inputs by leveraging\ncode coverage. Based on a given grammar, NAUTILUS gener-\nates a new JS test and checks whether it hits new code cov-\nerage for further mutation chances. Contrary to NAUTILUS ,\nGRIMOIRE requires no user-provided components, such as\ngrammar speciﬁcation and language models, but synthesizes\ninputs that trigger new code coverage. As they stated, GRI-\nMOIRE has difﬁculties in generating inputs with complex\nstructures, requiring semantic information.\nPrevious studies of mutational fuzzing [7, 18, 24, 32, 44, 54,\n57,58] focus on altering given seeds to leverage functionalities\nthat the seeds already test.\nLangFuzz [24] is a mutational fuzzing tool that substitutes\na non-terminal node in a given AST with code fragments. It\niteratively replaces non-terminal nodes in the step of inserting\nfragments. However, LangFuzz does not consider any context\nregarding picking a promising candidate to cause a target\nJS engine crash. On the other hand, Montage is capable of\nlearning implicit relations between fragments that may be\ninherent in given examples.\nLiu et al. [32] proposed a mutation-based approach to fuzz\nthe target program. Given a large corpus of C code, they\ntrained a sequence-to-sequence model to capture the inherent\npattern of input at character-level. Then, they leveraged the\ntrained model to mutate the seed. Their approach suffers from\nthe limitation that the model generates many malformed tests,\nsuch as unbalanced parenthesis.\nLanguage model for code. Hindle et al. [22] measured the\nnaturalness of software by computing the cross-entropy val-\nues over lexical code tokens in large JA V A and C applications.\nThey also ﬁrst demonstrated even count-based n-gram lan-\nguage models are applicable to code completion. To make\nmore accurate suggestions for code completion, SLAMC [40]\nincorporated semantic information, including type, scope, and\nrole for each lexical token. SLANG [43] lets a model learn\nAPI call sequences from Android applications. It then uses\nsuch a model to improve the precision of code completion.\nGraLan learns the relations between API calls from the graph\nof API call sequences, and ASTLan uses GraLan to ﬁll holes\nin the AST to complete the code [39].\nMaddison et al. [33] studied the generative models of nat-\nural source code based on PCFGs and source code-speciﬁc\nstructures. Bielik et al. [5] suggested a new generative prob-\nabilistic model of code called a probabilistic higher order\ngrammar, which generalizes PCFGs and parameterizes the\nproduction rules on a context.\nThe objective of using a language model in all of the above\nworks is to make better suggestions for code completion. How-\never, Montage focuses on generating JS tests that should be\naccepted by a target JS engine.\n9 Conclusion\nWe present Montage, the ﬁrst fuzzing tool that leverages an\nNNLM in generating JS tests. We propose a novel algorithm\nof modeling the hierarchical structures of a JS test case and\nthe relationships among such structures into a sequence of\nfragments. The encoding of an AST into a fragment sequence\nenables Montage to learn the relationships among the frag-\nments by using an LSTM model. Montage found 37 real-\nworld bugs in the latest JS engines, which demonstrates its\neffectiveness in ﬁnding JS engine bugs.\nAcknowledgements\nWe thank anonymous reviewers for their helpful feedback\nand Yale Song who helped develop some of the ideas used\nin this paper. We are also grateful to Jihoon Kim for kindly\nsharing his ﬁndings, which indeed inspired our project. Fi-\nnally, we thank Sunnyeo Park for collecting JS seeds used\nfor the evaluation. This work was partly supported by (1)\nInstitute for Information & communications Technology Pro-\nmotion (IITP) grant funded by the Korea government (MSIT),\nNo.2018-0-00254, and (2) LIG Nex1.\nReferences\n[1] Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and Bhu-\nvana Ramabhadran. Deep neural network language mod-\nels. In Proceedings of the NAACL-HLT 2012 Workshop,\npages 20–28, 2012.\n[2] Cornelius Aschermann, Patrick Jauernig, Tommaso Fras-\nsetto, Ahmad-Reza Sadeghi, Thorsten Holz, and Daniel\nTeuchert. NAUTILUS: Fishing for deep bugs with gram-\nmars. In Proceedings of the Network and Distributed\nSystem Security Symposium, 2019.\n[3] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and\nChristian Janvin. A neural probabilistic language model.\nThe Journal of Machine Learning Research, 3(1):1137–\n1155, 2003.\n[4] Yoshua Bengio, Patrice Simard, and Paolo Frasconi.\nLearning long-term dependencies with gradient descent\nis difﬁcult. Transactions on Neural Networks, 5(2):157–\n166, 1994.\n[5] Pavol Bielik, Veselin Raychev, and Martin Vechev.\nPHOG: Probabilistic model for code. In Proceedings\nof the International Conference on Machine Learning,\npages 2933–2942, 2016.\n[6] Tim Blazytko, Cornelius Aschermann, Moritz Schlögel,\nAli Abbasi, Sergej Schumilo, Simon Wörner, and\nThorsten Holz. GRIMOIRE: Synthesizing structure\nwhile fuzzing. In Proceedings of the USENIX Security\nSymposium, pages 1985–2002, 2019.\n[7] Sang Kil Cha, Maverick Woo, and David Brumley.\nProgram-adaptive mutational fuzzing. In Proceedings\nof the IEEE Symposium on Security and Privacy, pages\n725–741, 2015.\n[8] Stanley F. Chen and Joshua Goodman. An empirical\nstudy of smoothing techniques for language modeling.\nIn Proceedings of the 34th Annual Meeting on Asso-\nciation for Computational Linguistics, pages 310–318,\n1996.\n[9] Yang Chen, Alex Groce, Chaoqiang Zhang, Weng-Keen\nWong, Xiaoli Fern, Eric Eide, and John Regehr. Taming\ncompiler fuzzers. In Proceedings of the ACM Confer-\nence on Programming Language Design and Implemen-\ntation, pages 197–208, 2013.\n[10] Microsoft Corporation. Microsoft ChakraCore. https:\n//github.com/Microsoft/ChakraCore.\n[11] Chris Cummins and Alastair Murray. Compiler fuzzing\nthrough deep learning. In Proceedings of the ACM Inter-\nnational Symposium on Software Testing and Analysis,\npages 95–105, 2018.\n[12] Kyle Dewey, Jared Roesch, and Ben Hardekopf. Lan-\nguage fuzzing using constraint logic programming. In\nProceedings of the International Conference on Auto-\nmated Software Engineering, pages 725–730, 2014.\n[13] Technical Committee 39 ECMA International. Test262.\nhttps://github.com/tc39/test262.\n[14] Felix A. Gers, Jürgen Schmidhuber, and Fred Cummins.\nLearning to forget: Continual prediction with LSTM.\nNeural Computation, 12:2451–2471, 1999.\n[15] Patrice Godefroid, Adam Kiezun, and Michael Y . Levin.\nGrammar-based whitebox fuzzing. InProceedings of the\nACM Conference on Programming Language Design\nand Implementation, pages 206–215, 2008.\n[16] Patrice Godefroid, Hila Peleg, and Rishabh Singh.\nLearn&Fuzz: Machine learning for input fuzzing. In\nProceedings of the International Conference on Auto-\nmated Software Engineering, pages 50–59, 2017.\n[17] Joshua T. Goodman. A bit of progress in language\nmodeling. Computer Speech & Language, 15(4):403–\n434, 2001.\n[18] Tao Guo, Puhan Zhang, Xin Wang, and Qiang Wei.\nGramFuzz: Fuzzing testing of web browsers based on\ngrammar analysis and structural mutation. In Proceed-\nings of the International Conference on Informatics Ap-\nplications, pages 212–215, 2013.\n[19] HyungSeok Han and Sang Kil Cha. IMF: Inferred\nmodel-based fuzzer. In Proceedings of the ACM Confer-\nence on Computer and Communications Security, pages\n2345–2358, 2017.\n[20] HyungSeok Han, DongHyeon Oh, and Sang Kil Cha.\nCodeAlchemist: Semantics-aware code generation to\nﬁnd vulnerabilities in JavaScript engines. In Proceed-\nings of the Network and Distributed System Security\nSymposium, 2019.\n[21] Ariya Hidayat. ECMAScript parsing infrastructure for\nmultipurpose analysis. https://www.esprima.org.\n[22] Abram Hindle, Earl T. Barr, Zhendon Su, Mark Gabel,\nand Premkumar Devanbu. On the naturalness of soft-\nware. In Proceedings of the International Conference\non Software Engineering, pages 837–847, 2012.\n[23] Sepp Hochreiter and Jürgen Schmidhuber. Long short-\nterm memory. Neural Computation, 9(8):1735–1780,\n1997.\n[24] Christian Holler, Kim Herzig, and Andreas Zeller.\nFuzzing with code fragments. In Proceedings of the\nUSENIX Security Symposium, pages 445–458, 2012.\n[25] BuzzFeed Inc. Markovify. https://github.com/\njsvine/markovify.\n[26] Theori Inc. pwn.js. https://github.com/\ntheori-io/pwnjs, 2017.\n[27] ECMA International. ECMAScript language spec-\niﬁcation. https://www.ecma-international.org/\necma-262/.\n[28] Dave Jones. Trinity. https://github.com/\nkernelslacker/trinity.\n[29] Rafal Józefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. Exploring the limits of lan-\nguage modeling. CoRR, abs/1602.02410, 2016.\n[30] Yoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M. Rush. Character-aware neural language models.\nIn Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, pages 2741–2749, 2016.\n[31] George Klees, Andrew Ruef, Benji Cooper, Shiyi Wei,\nand Michael Hicks. Evaluating fuzz testing. In Proceed-\nings of the ACM Conference on Computer and Commu-\nnications Security, pages 2123–2138, 2018.\n[32] Xiao Liu, Xiaoting Li, Rupesh Prajapati, and Dinghao\nWu. DeepFuzz: Automatic generation of syntax valid c\nprograms for fuzz testing. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, pages 1044–1051,\n2019.\n[33] Chris J. Maddison and Daniel Tarlow. Structured gen-\nerative models of natural source code. In Proceedings\nof the International Conference on Machine Learning,\npages 649–657, 2016.\n[34] Tomáš Mikolov, Martin Karaﬁát, Lukáš Burget, JanˇCer-\nnock`y, and Sanjeev Khudanpur. Recurrent neural net-\nwork based language model. In Proceedings of the 11th\nAnnual Conference of the International Speech Commu-\nnication Association, pages 1045–1048, 2010.\n[35] Matt Molinyawe, Abdul-Aziz Hariri, and Jasiel Spelman.\n$hell on Earth: From browser to system compromise. In\nProceedings of the Black Hat USA, 2016.\n[36] David Molnar, Xue Cong Li, and David A. Wagner. Dy-\nnamic test generation to ﬁnd integer bugs in x86 binary\nlinux programs. In Proceedings of the USENIX Security\nSymposium, pages 67–82, 2009.\n[37] Mozilla. Hoisting. https://developer.mozilla.\norg/en-US/docs/Glossary/Hoisting.\n[38] MozillaSecurity. funfuzz. https://github.com/\nMozillaSecurity/funfuzz.\n[39] Anh Tuan Nguyen and Tien N. Nguyen. Graph-based\nstatistical language model for code. In Proceedings of\nthe International Conference on Software Engineering,\npages 858–868, 2015.\n[40] Tung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh\nNguyen, and Tien N. Nguyen. A statistical semantic\nlanguage model for source code. In Proceedings of the\nInternational Symposium on Foundations of Software\nEngineering, pages 532–542, 2013.\n[41] Jibesh Patra and Michael Pradel. Learning to fuzz:\nApplication-independent fuzz testing with probabilis-\ntic, generative models of input data. Technical Report\nTUD-CS-2016-14664, TU Darmstadt, 2016.\n[42] Van-Thuan Pham, Marcel Böhme, and Abhik Roychoud-\nhury. Model-based whitebox fuzzing for program bina-\nries. In Proceedings of the International Conference on\nAutomated Software Engineering, pages 543–553, 2016.\n[43] Veselin Raychev, Martin Vechev, and Eran Yahav. Code\ncompletion with statistical language models. InProceed-\nings of the ACM Conference on Programming Language\nDesign and Implementation, pages 419–428, 2014.\n[44] Alexandre Rebert, Sang Kil Cha, Thanassis Avgerinos,\nJonathan Foote, David Warren, Gustavo Grieco, and\nDavid Brumley. Optimizing seed selection for fuzzing.\nIn Proceedings of the USENIX Security Symposium ,\npages 861–875, 2014.\n[45] Jesse Ruderman. Releasing jsfunfuzz and dom-\nfuzz. http://www.squarefree.com/2015/07/28/\nreleasing-jsfunfuzz-and-domfuzz/ , 2007.\n[46] Joeri De Ruiter and Erik Poll. Protocol state fuzzing of\nTLS implementations. In Proceedings of the USENIX\nSecurity Symposium, pages 193–206, 2015.\n[47] Juraj Somorovsky. Systematic fuzzing and testing of\nTLS libraries. In Proceedings of the ACM Conference on\nComputer and Communications Security, pages 1492–\n1504, 2016.\n[48] Aditya K. Sood and Sherali Zeadally. Drive-by down-\nload attacks: A comparative study. IT Professional,\n18(5):18–25, 2016.\n[49] Alexander Sotirov. Heap feng shui in JavaScript. In\nProceedings of the Black Hat USA, 2007.\n[50] Michael Sutton, Adam Greene, and Pedram Amini.\nFuzzing: Brute Force Vulnerability Discovery. Addison-\nWesley Professional, 2007.\n[51] Yusuke Suzuki. Escodegen.https://www.npmjs.com/\npackage/escodegen.\n[52] PyTorch Core Team. Pytorch. https://pytorch.\norg/.\n[53] Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu.\nOn the localness of software. In Proceedings of the\nInternational Symposium on Foundations of Software\nEngineering, pages 269–280, 2014.\n[54] Spandan Veggalam, Sanjay Rawat, Istvan Haller, and\nHerbert Bos. IFuzzer: An evolutionary interpreter fuzzer\nusing genetic programming. In Proceedings of the Eu-\nropean Symposium on Research in Computer Security,\npages 581–601, 2016.\n[55] Dmitry Vyukov. syzkaller. https://github.com/\ngoogle/syzkaller.\n[56] Junjie Wang, Bihuan Chen, Lei Wei, and Yang Liu. Sky-\nﬁre: Data-driven seed generation for fuzzing. In Pro-\nceedings of the IEEE Symposium on Security and Pri-\nvacy, pages 579–594, 2017.\n[57] Maverick Woo, Sang Kil Cha, Samantha Gottlieb, and\nDavid Brumley. Scheduling black-box mutational\nfuzzing. In Proceedings of the ACM Conference on\nComputer and Communications Security , pages 511–\n522, 2013.\n[58] Michal Zalewski. American Fuzzy Lop. http://\nlcamtuf.coredump.cx/afl/.\n[59] ZERODIUM. Zerodium payouts. https://zerodium.\ncom/program.html."
}