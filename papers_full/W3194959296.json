{
  "title": "Vision Transformer Pruning",
  "url": "https://openalex.org/W3194959296",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2222811174",
      "name": "Zhu Ming-jian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2357421627",
      "name": "Tang, Yehui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103390482",
      "name": "Han, Kai",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3108995912",
    "https://openalex.org/W2114766824",
    "https://openalex.org/W2945247689",
    "https://openalex.org/W3159833358",
    "https://openalex.org/W2125389748",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2543539599",
    "https://openalex.org/W2997710335",
    "https://openalex.org/W2300242332",
    "https://openalex.org/W3107036272",
    "https://openalex.org/W3102982004",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2964118293",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2998310140",
    "https://openalex.org/W3087150032",
    "https://openalex.org/W2962851801",
    "https://openalex.org/W2995607862",
    "https://openalex.org/W3101345044",
    "https://openalex.org/W2963000224",
    "https://openalex.org/W2586654419",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2894994475",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2963374099",
    "https://openalex.org/W2167215970",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2963225922",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2754084392",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2998183051"
  ],
  "abstract": "Vision transformer has achieved competitive performance on a variety of computer vision applications. However, their storage, run-time memory, and computational demands are hindering the deployment to mobile devices. Here we present a vision transformer pruning approach, which identifies the impacts of dimensions in each layer of transformer and then executes pruning accordingly. By encouraging dimension-wise sparsity in the transformer, important dimensions automatically emerge. A great number of dimensions with small importance scores can be discarded to achieve a high pruning ratio without significantly compromising accuracy. The pipeline for vision transformer pruning is as follows: 1) training with sparsity regularization; 2) pruning dimensions of linear projections; 3) fine-tuning. The reduced parameters and FLOPs ratios of the proposed algorithm are well evaluated and analyzed on ImageNet dataset to demonstrate the effectiveness of our proposed method.",
  "full_text": "Vision Transformer Pruning\nMingjian Zhu\nZhejiang University\nWestlake University\nzhumingjian@zju.edu.cn\nYehui Tang\nPeking University\nyhtang@pku.edu.cn\nKai Han\nNoahâ€™s Ark Lab, Huawei Technologies\nkai.han@huawei.com\nABSTRACT\nVision transformer has achieved competitive performance on a\nvariety of computer vision applications. However, their storage,\nrun-time memory, and computational demands are hindering the\ndeployment to mobile devices. Here we present a vision transformer\npruning approach, which identifies the impacts of dimensions in\neach layer of transformer and then executes pruning accordingly. By\nencouraging dimension-wise sparsity in the transformer, important\ndimensions automatically emerge. A great number of dimensions\nwith small importance scores can be discarded to achieve a high\npruning ratio without significantly compromising accuracy. The\npipeline for vision transformer pruning is as follows: 1) training\nwith sparsity regularization; 2) pruning dimensions of linear pro-\njections; 3) fine-tuning. The reduced parameters and FLOPs ratios\nof the proposed algorithm are well evaluated and analyzed on Im-\nageNet dataset to demonstrate the effectiveness of our proposed\nmethod.\nKEYWORDS\nVision Transformer, Transformer Pruning, Network Pruning\n1 INTRODUCTION\nRecently, transformer [38] has attracted much attention and shed\nlight on various computer vision applications [ 4, 5, 12, 24] such\nas image classification [8, 13, 37], object detection [2, 3, 50], and\nimage segmentation [16, 39, 41]. However, most of the proposed\ntransformer variants highly demand storage, run-time memory, and\ncomputational resource requirements, which impede their wide\ndeployment on edge devices, e.g., robotics and mobile phones. Al-\nthough massive effective techniques have been developed for com-\npressing and accelerating convolutional neural networks (CNNs)\nincluding low-rank decomposition [ 7, 20, 22, 33, 44], quantiza-\ntion [1, 11, 28, 28, 43], network pruning [ 14, 19, 34, 35, 42], and\nknowledge distillation [15, 21, 26, 29, 36], there still exists an ur-\ngency to develop and deploy efficient vision transformer.\nTaking advantage of different designs [17, 31, 46, 47, 49], trans-\nformer can be compressed and accelerated to varying degrees. AL-\nBERT [18] reduces network parameter and speed up training time\nby decomposing embedding parameters into smaller matrices and\nenabling cross-layer parameter sharing. Star-Transformer [10] spar-\nsifies the standard transformer by moving fully-connected structure\nto the star-shaped topology. Based on knowledge distillation tech-\nniques, the student networks in [ 17, 32] learn from the logits in\nthe larger pre-trained teacher networks. Some effective pruning\nalgorithms have been proposed to reduce the attention heads [27]\nor individual weights [ 9]. The previous methods focus on com-\npressing and accelerating the transformer for the natural language\nprocessing tasks. With the emergence of vision transformers such\nas ViT [ 8], PVT [ 40], and TNT [ 13], an efficient transformer is\nurgently need for computer vision applications.\nTo address the aforementioned problems, we propose to prune\nthe vision transformer according to the learnable importance scores.\nInspired by the pruning scheme in network slimming [23], we add\nthe learning importance scores before the components to be prune\nand sparsify them by training the network with L1 regulation.\nThe dimensions with smaller importance scores will be pruned\nand the compact network can be obtained. Experimental results\non the benchmark demonstrate the effectiveness of the proposed\nalgorithm. Our vision transformer pruning (VTP) method largely\ncompresses and accelerates the original ViT (DeiT) models. As the\nfirst pruning method for vision transformers, this work will provide\na solid baseline and experience for future research.\n2 APPROACH\n2.1 Transformer\nThe typical vision transformer architecture [8, 38] consists of Multi-\nHead Self-Attention (MHSA), Multi-Layer Perceptron (MLP), layer\nnormalization, activation function, and shortcut connection. MHSA\nis the characteristic component of transformer to perform informa-\ntion interaction among tokens. In particular, the input ğ‘‹ âˆˆRğ‘›Ã—ğ‘‘ is\ntransformed to queryğ‘„ âˆˆRğ‘›Ã—ğ‘‘ , keyğ¾ âˆˆRğ‘›Ã—ğ‘‘ and valueğ‘‰ âˆˆRğ‘›Ã—ğ‘‘\nvia fully-connected layers, where ğ‘›is the number of patches. ğ‘‘ is\nthe embedding dimension. The self-attention mechanism is utilized\nto model the relationship between patches:\nAttention(ğ‘„,ğ¾,ğ‘‰ )= Softmax\n\u0010\nğ‘„ğ¾T/\nâˆš\nğ‘‘\n\u0011\nğ‘‰. (1)\nFinally, a linear transformation is applied to generate the output of\nMHSA:\nğ‘Œ = ğ‘‹ +FCğ‘œğ‘¢ğ‘¡ (Attention(FCğ‘ (ğ‘‹),FCğ‘˜ (ğ‘‹),FCğ‘£ (ğ‘‹))), (2)\nwhere the layer normalization and activation function are omitted\nfor simplification. As for the two-layer MLP, it can be formulated\nas\nğ‘ = ğ‘Œ +FC2(FC1(ğ‘Œ)). (3)\nIntuitively, the widely-used fully-connected layers in the trans-\nformer lead to computation and storage burden.\n2.2 Vision Transformer Pruning\nTo slim the transformer architecture, we focus on decreasing the\nFLOPs of MHSA and MLP. We propose to prune the dimension\nof the linear projection by learning their associated importance\nscores. For the features ğ‘‹ âˆˆRğ‘›Ã—ğ‘‘ , where ğ‘› denotes the number\nof features that need to be pruned and ğ‘‘ denotes the dimension of\neach feature, we aim to preserve the generated important features\nand remove the useless ones from the corresponding components\nof linear projection. Suppose the optimal importance scores are\narXiv:2104.08500v4  [cs.CV]  14 Aug 2021\nMingjian Zhu, Yehui Tang, and Kai Han\nHuawei Confidential4\n0.95 0.820.320.120.020.880.780.250.660.860.17\nDimension Pruning\nLinear\nDimension Pruning\nScaled Dot-Product Attention\nDimension Pruning\nLinear\nLinear\nDimension Pruning\nLinear\nDimension Pruning\nSoft gate\nHard gate\nFeatures n\nd\nFigure 1: Vision Transformer Pruning.\naâˆ— âˆˆ{0,1}ğ‘‘ , that is, the scores for generated important features\nand their corresponding components are ones while the scores for\nuseless ones are zeros. With the importance scores, we can obtain\nthe pruned features:\nğ‘‹âˆ—= ğ‘‹diag(aâˆ—). (4)\nHowever, itâ€™s hard to optimize aâˆ—in the neural network through\na back-propagation algorithm due to its discrete values. Thus, we\npropose to relax aâˆ— to real values as ^ aâˆˆ Rğ‘‘ . The soft pruned\nfeatures are obtained as\n^ğ‘‹ = ğ‘‹diag(^ a) (5)\nThen, the relaxed importance scores ^ acan be learned together with\nthe transformer network end-to-end. In order to enforce sparsity\nof importance scores, we apply â„“1 regularization on the importance\nscores: ğœ†âˆ¥^ aâˆ¥1 and optimize it by adding on the training objective,\nwhere ğœ†is the sparsity hyper-parameter. After training with sparsity\npenalty, we obtain the transformer with some importance scores\nnear zero. We rank all the values of regularized importance scores in\nthe transformer and obtain a thresholdğœaccording to a pre-defined\npruning rate. With the threshold ğœ, we obtain the discrete aâˆ—by\nsetting the values below the threshold as zero and higher values as\nones:\naâˆ—= ^ aâ‰¥ğœ. (6)\nAfter pruning according to the importance scores aâˆ—, the total\npruned transformer is fine-tuned to diminish the accuracy drop.\nThe above pruning procedure is denoted as\nğ‘‹âˆ—= Prune(ğ‘‹). (7)\nAs shown in Figure 1, we apply the pruning operation on all\nthe MHSA and MLP blocks. The pruning process for them can be\nformulated as\nğ‘„,ğ¾,ğ‘‰ = FCâ€²\nğ‘ (Prune(ğ‘‹)),FCâ€²\nğ‘˜ (Prune(ğ‘‹)),FCâ€²\nğ‘£ (Prune(ğ‘‹)), (8)\nğ‘Œ = ğ‘‹ +FCâ€²\nğ‘œğ‘¢ğ‘¡ (Prune(Attention(ğ‘„,ğ¾,ğ‘‰ ))), (9)\nğ‘ = ğ‘Œ +FCâ€²\n2(Prune(FCâ€²\n1(Prune(ğ‘Œ)))). (10)\nwhere FCâ€²ğ‘,FCâ€²\nğ‘˜,FCâ€²ğ‘£,FCâ€²\nğ‘œğ‘¢ğ‘¡ ,FCâ€²\n1, and FCâ€²\n2 are pruned linear projec-\ntion corresponding to the pruned features and aâˆ—. The proposed\nvision transformer pruning (VTP) method provides a simple yet\neffective way to slim vision transformer models. We hope that this\nwork will serve as a solid baseline for future research and provide\nuseful experience for the practical deployment of vision transform-\ners.\n3 EXPERIMENTS\nIn this section, we verify the effectiveness of the proposed VTP\nmethods to prune vision transformer models on ImageNet dataset.\n3.1 Datasets\nImageNet-1K.. ImageNet ILSVRC2012 dataset [30] is a large-scale\nimage classification dataset including 1.2 million images for train-\ning and 50,000 validation images belonging to 1,000 classes. The\ncommon data augmentation strategy in DeiT [ 37] is adopted for\nmodel development, including Rand-Augment [6], Mixup [48], and\nCutMix [45].\nImageNet-100. ImageNet-100 is collected as a subset of ImageNet-\n1K. We first randomly sampled 100 classes and their corresponding\nimages for training and validation. We adopt the same data aug-\nmentation strategy for ImageNet-100 as ImageNet-1K.\n3.2 Implementation Details\nBaseline. We evaluate our pruning method on a popular vision\ntransformer implementation, i.e., DeiT-base [ 37]. In our experi-\nments, a 12-layer transformer with 12 heads and 768 embedding\ndimensions is evaluated on both ImageNet-1K and Imagenet-100.\nFor a fair comparison, we utilize the official implementation of DeiT\nand do not use techniques like distillation. On the ImageNet-1K, we\ntake the released model of DeiT-base as the baseline. We finetune\nthe model on the ImageNet-1K using batch size 64 for 30 epochs.\nThe initial learning rate is set to 6.25 Ã—10âˆ’7. Following Deit [37],\nVision Transformer Pruning\nTable 1: Ablation Study on ImageNet-100.\nSparse Penalty Pruning Rate Params (M) Params Reduced FLOPs (B) FLOPs Reduced Top1 (%)\n0.0001\n0.6 29.0 â†“66.4% 6.4 â†“63.5% 90.00\n0.5 38.0 â†“56.0% 8.2 â†“53.4% 91.46\n0.4 47.3 â†“45.3% 10.0 â†“43.0% 92.58\n0.2 66.1 â†“23.5% 13.7 â†“22.0% 93.54\n0.00001\n0.6 28.2 â†“67.4% 6.3 â†“64.4% 89.88\n0.5 37.5 â†“56.6% 8.1 â†“54.0% 91.40\n0.4 47.1 â†“45.5% 10.0 â†“43.2% 92.38\n0.2 66.1 â†“23.5% 13.7 â†“22.0% 93.44\nTable 2: Results on ImageNet-100.\nModel Params (M) FLOPs (B) Top1 (%) Top5 (%)\nDeit-B (Baseline) 86.4 17.6 94.50 98.94\nVTP (20% pruned) 66.1 13.7 93.54 98.36\nVTP (40% pruned) 47.3 10.0 92.58 98.04\nTable 3: Results on ImageNet-1K.\nModel Params (M) FLOPs (B) Top1 (%) Top5 (%)\nCNN based\nResNet-152 60.2 11.5 78.3 94.1\nRegNetY-16GF 83.6 15.9 80.4 -\nTransformer based\nViT-B/16 86.4 55.5 77.9 -\nDeiT-B (Baseline) 86.4 17.6 81.8 -\nVTP (20% pruned) 67.3 13.8 81.3 95.3\nVTP (40% pruned) 48.0 10.0 80.7 95.0\nwe use AdamW [25] with cosine learning rate decay strategy to\ntrain and finetune the models.\nTraining with Sparsity Regularization and Pruning. Based on the\nbaseline model, we train the vision transformer with â„“1 regular-\nization using different sparse regularization rates. We select the\noptimal sparse regularization rate (i.e. 0.0001) on Imagenet-100 and\napply it on ImageNet-1K. The learning rate for training with sparsity\nis 6.25 Ã—10âˆ’6 and the number of epochs is 100. The other train-\ning setting follows the baseline model. After sparsity, we prune\nthe transformer by setting different pruning thresholds and the\nthreshold is computed by the predefined pruning rate, e.g., 0.2.\nFinetuning. We finetune the pruned transformer with the same\noptimization setting as in training, except for removing the â„“1 reg-\nularization.\n3.3 Results and Analysis\nImagenet-100 Experiments and Ablation Study. We firstly conduct\nablation studies on Imagenet-100, as shown in Table 1. From the\nresults, the amount of pruning rate matches the ratio of parameters\nsaving and FLOPs saving. For example, when we prune 40% dimen-\nsions of the models trained with 0.0001 sparse rate, the parameter\nsaving is 45.3% and the FLOPs saving is 43.0%. We can see that the\nParameters and FLOPs drop while the accuracy maintains. Besides,\nthe sparse ratio does not highly influence the effectiveness of the\npruning method. In Table 2, we compare the baseline model with\ntwo VTP models, i.e., 20% pruned and 40% pruned models. The\naccuracy drops slightly with large FLOPs decrease. When we prune\n20% dimensions, 22.0% FLOPs are saved and the accuracy drops by\n0.96%. When we prune 40% dimensions, 45.3% FLOPs are saved and\nthe accuracy drops by 1.92%.\nImagenet-1K Experiments. We also evaluate the proposed VTP\nmethod on the large-scale ImageNet-1K benchmark. The results are\nshown in Table 3. Compared to the base model DeiT-B, the accuracy\nof VTP only decreases by 1.1% when 40% dimensions are pruned.\nThe accuracy only drops by 0.5% while 20% dimensions are pruned.\nThe effectiveness of VTP can be generalized to large-scale datasets.\n4 CONCLUSION\nIn this paper, we introduce a simple yet efficient vision transformer\npruning method. L1 regulation is applied to sparse the dimen-\nsions of the transformer and the important dimensions appear\nautomatically. The experiments conducted on Imagenet-100 and\nImageNet-1K demonstrate that the pruning method can largely\nreduce the computation costs and model parameters while main-\ntaining the high accuracy of original vision transformers. In the\nfuture, the important components such as the number of heads and\nthe number of layers can also be reduced with this method, which\nis a promising attempt to further compress vision transformers.\nMingjian Zhu, Yehui Tang, and Kai Han\nREFERENCES\n[1] Zhaowei Cai, Xiaodong He, Jian Sun, and Nuno Vasconcelos. 2017. Deep learning\nwith low precision by half-wave gaussian quantization. InProceedings of the IEEE\nconference on computer vision and pattern recognition . 5918â€“5926.\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexan-\nder Kirillov, and Sergey Zagoruyko. 2020. End-to-End Object Detection with\nTransformers. arXiv preprint arXiv:2005.12872 (2020).\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexan-\nder Kirillov, and Sergey Zagoruyko. 2020. End-to-End Object Detection with\nTransformers. In ECCV.\n[4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua\nLiu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. 2021. Pre-trained image\nprocessing transformer. In CVPR.\n[5] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan,\nand Ilya Sutskever. 2020. Generative pretraining from pixels. In International\nConference on Machine Learning . PMLR, 1691â€“1703.\n[6] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. 2020. Randaugment:\nPractical automated data augmentation with a reduced search space. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshops. 702â€“703.\n[7] Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus.\n2014. Exploiting linear structure within convolutional networks for efficient\nevaluation. In Advances in neural information processing systems . 1269â€“1277.\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al. 2021. An image is worth 16x16 words: Transformers\nfor image recognition at scale. In ICLR.\n[9] Mitchell A Gordon, Kevin Duh, and Nicholas Andrews. 2020. Compressing\nbert: Studying the effects of weight pruning on transfer learning. arXiv preprint\narXiv:2002.08307 (2020).\n[10] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng\nZhang. 2019. Star-transformer. arXiv preprint arXiv:1902.09113 (2019).\n[11] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.\n2015. Deep learning with limited numerical precision. In International Conference\non Machine Learning . 1737â€“1746.\n[12] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua\nLiu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al . 2020. A Survey on\nVisual Transformer. arXiv preprint arXiv:2012.12556 (2020).\n[13] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.\n2021. Transformer in transformer. arXiv preprint arXiv:2103.00112 (2021).\n[14] Babak Hassibi and David G Stork. 1993. Second order derivatives for network\npruning: Optimal brain surgeon . Morgan Kaufmann.\n[15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in\na neural network. arXiv preprint arXiv:1503.02531 (2015).\n[16] Jie Hu, Liujuan Cao, Yao Lu, ShengChuan Zhang, Yan Wang, Ke Li, Feiyue Huang,\nLing Shao, and Rongrong Ji. 2021. ISTR: End-to-End Instance Segmentation with\nTransformers. arXiv preprint arXiv:2105.00637 (2021).\n[17] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,\nand Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding.\narXiv preprint arXiv:1909.10351 (2019).\n[18] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush\nSharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning\nof language representations. arXiv preprint arXiv:1909.11942 (2019).\n[19] Yann LeCun, John S Denker, and Sara A Solla. 1990. Optimal brain damage. In\nAdvances in neural information processing systems . 598â€“605.\n[20] Dongsoo Lee, Se Jung Kwon, Byeongwook Kim, and Gu-Yeon Wei. 2019. Learning\nlow-rank approximation for cnns. arXiv preprint arXiv:1905.10145 (2019).\n[21] Guilin Li, Junlei Zhang, Yunhe Wang, Chuanjian Liu, Matthias Tan, Yunfeng Lin,\nWei Zhang, Jiashi Feng, and Tong Zhang. 2020. Residual distillation: Towards\nportable deep neural networks without shortcuts. In 34th Conference on Neural\nInformation Processing Systems (NeurIPS 2020) . Curran Associates Inc., 8935â€“8946.\n[22] Shaohui Lin, Rongrong Ji, Chao Chen, Dacheng Tao, and Jiebo Luo. 2018. Holistic\ncnn compression via low-rank decomposition with knowledge transfer. IEEE\ntransactions on pattern analysis and machine intelligence 41, 12 (2018), 2889â€“2905.\n[23] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Chang-\nshui Zhang. 2017. Learning efficient convolutional networks through network\nslimming. In Proceedings of the IEEE International Conference on Computer Vision .\n2736â€“2744.\n[24] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using\nshifted windows. arXiv preprint arXiv:2103.14030 (2021).\n[25] Ilya Loshchilov and Frank Hutter. 2018. Fixing weight decay regularization in\nadam. (2018).\n[26] Ping Luo, Zhenyao Zhu, Ziwei Liu, Xiaogang Wang, and Xiaoou Tang. 2016. Face\nmodel compression by distilling knowledge from neurons. In Proceedings of the\nAAAI Conference on Artificial Intelligence , Vol. 30.\n[27] Paul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really\nbetter than one? arXiv preprint arXiv:1905.10650 (2019).\n[28] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016.\nXnor-net: Imagenet classification using binary convolutional neural networks.\nIn European conference on computer vision . Springer, 525â€“542.\n[29] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang,\nCarlo Gatta, and Yoshua Bengio. 2015. Fitnets: Hints for thin deep nets. In\nInternational Conference on Learning Representations .\n[30] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean\nMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.\n2015. Imagenet large scale visual recognition challenge. International journal of\ncomputer vision 115, 3 (2015), 211â€“252.\n[31] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,\nMichael W Mahoney, and Kurt Keutzer. 2020. Q-bert: Hessian based ultra low\nprecision quantization of bert. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 34. 8815â€“8821.\n[32] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient knowledge distilla-\ntion for bert model compression. arXiv preprint arXiv:1908.09355 (2019).\n[33] Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al . 2015. Convolutional\nneural networks with low-rank regularization. arXiv preprint arXiv:1511.06067\n(2015).\n[34] Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing Xu, Chao Xu, and\nChang Xu. 2020. SCOP: Scientific Control for Reliable Neural Network Pruning.\narXiv preprint arXiv:2010.10732 (2020).\n[35] Yehui Tang, Shan You, Chang Xu, Jin Han, Chen Qian, Boxin Shi, Chao Xu, and\nChangshui Zhang. 2020. Reborn filters: Pruning convolutional neural networks\nwith limited data. In Proceedings of the AAAI Conference on Artificial Intelligence ,\nVol. 34. 5972â€“5980.\n[36] Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2019. Contrastive representation\ndistillation. arXiv preprint arXiv:1910.10699 (2019).\n[37] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre\nSablayrolles, and HervÃ© JÃ©gou. 2020. Training data-efficient image transformers\n& distillation through attention. arXiv preprint arXiv:2012.12877 (2020).\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you\nneed. Advances in neural information processing systems 30 (2017), 5998â€“6008.\n[39] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan L. Yuille, and Liang-Chieh Chen.\n2020. MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transform-\ners. arXiv preprint arXiv:2012.00759 (2020).\n[40] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong\nLu, Ping Luo, and Ling Shao. 2021. Pyramid vision transformer: A versatile back-\nbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122\n(2021).\n[41] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng,\nHao Shen, and Huaxia Xia. 2020. End-to-End Video Instance Segmentation with\nTransformers. arXiv preprint arXiv:2011.14503 (2020).\n[42] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. 2016. Learning\nstructured sparsity in deep neural networks. In NeurIPS. 2074â€“2082.\n[43] Zhaohui Yang, Yunhe Wang, Kai Han, Chunjing Xu, Chao Xu, Dacheng Tao, and\nChang Xu. 2020. Searching for low-bit weights in quantized neural networks. In\nNeurIPS.\n[44] Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. 2017. On compress-\ning deep models by low rank and sparse decomposition. In IEEE Conference on\nComputer Vision and Pattern Recognition . 7370â€“7379.\n[45] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and\nYoungjoon Yoo. 2019. Cutmix: Regularization strategy to train strong classifiers\nwith localizable features. In Proceedings of the IEEE/CVF International Conference\non Computer Vision . 6023â€“6032.\n[46] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert:\nQuantized 8bit bert. arXiv preprint arXiv:1910.06188 (2019).\n[47] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti,\nSantiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020.\nBig bird: Transformers for longer sequences. arXiv preprint arXiv:2007.14062\n(2020).\n[48] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. 2017.\nmixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412\n(2017).\n[49] Zihan Zhao, Yuncong Liu, Lu Chen, Qi Liu, Rao Ma, and Kai Yu. 2020. An\nInvestigation on Different Underlying Quantization Schemes for Pre-trained\nLanguage Models. InCCF International Conference on Natural Language Processing\nand Chinese Computing . Springer, 359â€“371.\n[50] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. 2020.\nDeformable DETR: Deformable Transformers for End-to-End Object Detection.\narXiv preprint arXiv:2010.04159 (2020).",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7285730838775635
    },
    {
      "name": "Computer science",
      "score": 0.6608470678329468
    },
    {
      "name": "FLOPS",
      "score": 0.5044256448745728
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49802565574645996
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.47132304310798645
    },
    {
      "name": "Pruning",
      "score": 0.4569753110408783
    },
    {
      "name": "Parallel computing",
      "score": 0.15786826610565186
    },
    {
      "name": "Engineering",
      "score": 0.15134268999099731
    },
    {
      "name": "Electrical engineering",
      "score": 0.0815003514289856
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2250955327",
      "name": "Huawei Technologies (China)",
      "country": "CN"
    }
  ],
  "cited_by": 51
}