{
  "title": "Self-Training Multi-Sequence Learning with Transformer for Weakly Supervised Video Anomaly Detection",
  "url": "https://openalex.org/W4283811196",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2118732852",
      "name": "Shuo Li",
      "affiliations": [
        "Xidian University",
        "Intelligent Health (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2099661209",
      "name": "Fang Liu",
      "affiliations": [
        "Xidian University",
        "Intelligent Health (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2166558591",
      "name": "Licheng Jiao",
      "affiliations": [
        "Intelligent Health (United Kingdom)",
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2118732852",
      "name": "Shuo Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099661209",
      "name": "Fang Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166558591",
      "name": "Licheng Jiao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3177187266",
    "https://openalex.org/W2619082050",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W3137278571",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3143609527",
    "https://openalex.org/W2933801392",
    "https://openalex.org/W3207409917",
    "https://openalex.org/W2766042998",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W3014246171",
    "https://openalex.org/W2016053056",
    "https://openalex.org/W6955071965",
    "https://openalex.org/W6642206748",
    "https://openalex.org/W2969977325",
    "https://openalex.org/W6794906783",
    "https://openalex.org/W2982190919",
    "https://openalex.org/W2198817044",
    "https://openalex.org/W6796750486",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6797737728",
    "https://openalex.org/W2777342313",
    "https://openalex.org/W3012389398",
    "https://openalex.org/W2079057609",
    "https://openalex.org/W2783882151",
    "https://openalex.org/W3131465739",
    "https://openalex.org/W1968817125",
    "https://openalex.org/W2486791387",
    "https://openalex.org/W3136793533",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3035021504",
    "https://openalex.org/W3164956984",
    "https://openalex.org/W6791943378",
    "https://openalex.org/W6793958814",
    "https://openalex.org/W3173206925",
    "https://openalex.org/W2970271202",
    "https://openalex.org/W6795463671",
    "https://openalex.org/W2021659075",
    "https://openalex.org/W2998097997",
    "https://openalex.org/W2921491036",
    "https://openalex.org/W2962716148",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W4297775537",
    "https://openalex.org/W4287241064",
    "https://openalex.org/W2987228832",
    "https://openalex.org/W3035699237",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2963795951",
    "https://openalex.org/W4287326644",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W4214709605",
    "https://openalex.org/W3164208409",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W3172345956",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W1967456674",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W3089682612",
    "https://openalex.org/W3159663321"
  ],
  "abstract": "Weakly supervised Video Anomaly Detection (VAD) using Multi-Instance Learning (MIL) is usually based on the fact that the anomaly score of an abnormal snippet is higher than that of a normal snippet. In the beginning of training, due to the limited accuracy of the model, it is easy to select the wrong abnormal snippet. In order to reduce the probability of selection errors, we first propose a Multi-Sequence Learning (MSL) method and a hinge-based MSL ranking loss that uses a sequence composed of multiple snippets as an optimization unit. We then design a Transformer-based MSL network to learn both video-level anomaly probability and snippet-level anomaly scores. In the inference stage, we propose to use the video-level anomaly probability to suppress the fluctuation of snippet-level anomaly scores. Finally, since VAD needs to predict the snippet-level anomaly scores, by gradually reducing the length of selected sequence, we propose a self-training strategy to gradually refine the anomaly scores. Experimental results show that our method achieves significant improvements on ShanghaiTech, UCF-Crime, and XD-Violence.",
  "full_text": "Self-Training Multi-Sequence Learning with Transformer for Weakly Supervised\nVideo Anomaly Detection\nShuo Li, Fang Liu\u0003, Licheng Jiao\nKey Laboratory of Intelligent Perception and Image Understanding of Ministry of Education,\nInternational Research Center for Intelligent Perception and Computation,\nJoint International Research Laboratory of Intelligent Perception and Computation,\nSchool of ArtiÔ¨Åcial Intelligent, Xidian University, Xi‚Äôan, 710071, P.R. China,\nalisure@stu.xidian.edu.cn, f63liu@163.com, lchjiao@mail.xidian.edu.cn\nAbstract\nWeakly supervised Video Anomaly Detection (V AD) using\nMulti-Instance Learning (MIL) is usually based on the fact\nthat the anomaly score of an abnormal snippet is higher than\nthat of a normal snippet. In the beginning of training, due\nto the limited accuracy of the model, it is easy to select the\nwrong abnormal snippet. In order to reduce the probability of\nselection errors, we Ô¨Årst propose a Multi-Sequence Learning\n(MSL) method and a hinge-based MSL ranking loss that uses\na sequence composed of multiple snippets as an optimization\nunit. We then design a Transformer-based MSL network to\nlearn both video-level anomaly probability and snippet-level\nanomaly scores. In the inference stage, we propose to use the\nvideo-level anomaly probability to suppress the Ô¨Çuctuation\nof snippet-level anomaly scores. Finally, since V AD needs to\npredict the snippet-level anomaly scores, by gradually reduc-\ning the length of selected sequence, we propose a self-training\nstrategy to gradually reÔ¨Åne the anomaly scores. Experimental\nresults show that our method achieves signiÔ¨Åcant improve-\nments on ShanghaiTech, UCF-Crime, and XD-Violence.\nIntroduction\nVideo Anomaly Detection (V AD) aims to detect abnormal\nevents in the video, which has important practical value\n(Zhang, Qing, and Miao 2019; Guo et al. 2021). General-\nly, V AD predicts the anomaly score of each snippet in the\nvideo. There are three main paradigms: unsupervised V AD\n(Gong et al. 2019; Cai et al. 2021), weakly supervised V AD\n(Zhong et al. 2019), and supervised V AD (Liu and Ma 2019;\nWan et al. 2021). Unsupervised V AD only learns on normal\nvideos, assuming that unseen abnormal videos have high re-\nconstruction errors. Due to the lack of prior knowledge of\nabnormality and inability to learn all normal video patterns,\nthe performance of unsupervised V AD is usually poor (Tian\net al. 2021). Because Ô¨Åne-grained anomaly label is time-\nconsuming and laborious, it is difÔ¨Åcult to collect large-scale\ndatasets for supervised paradigm. With whether the video\ncontains anomalies as video-level label, the weakly super-\nvised paradigm predicts the anomaly score of each frame.\nThe weakly supervised paradigm is veriÔ¨Åed to be a feasi-\nble method because of its competitive performance (Feng,\n\u0003Corresponding author\nCopyright c\r 2022, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\nHong, and Zheng 2021). Recently, many researchers have\nfocused on weakly supervised V AD (Zhong et al. 2019).\nMost weakly supervised V ADs are based on Multi-\nInstance Learning (MIL) (Sultani, Chen, and Shah 2018;\nZhu and Newsam 2019; Wan et al. 2020; Tian et al. 2021).\nMIL-based methods treat a video as a bag, which contains\nmultiple instances. Each instance is a snippet. The bag gen-\nerated from an abnormal video is called a positive bag, and\nthe bag generated from a normal video is called a negative\nbag. Since the video-level label indicates whether the video\ncontains anomalies, the positive bag contains at least one\nabnormal snippet and the negative bag contains no abnormal\nsnippet. MIL-based methods learn instance-level anomaly s-\ncores through the bag-level labels (Zhong et al. 2019).\nIn MIL-based methods, at least one instance of the posi-\ntive bag contains the anomaly, and any instance of the neg-\native bag does not contain the anomaly (Sultani, Chen, and\nShah 2018). Generally, MIL-based methods assume that the\ninstance with the highest anomaly score in the positive bag\nshould rank higher than the instance with the highest anoma-\nly score in the negative bag (Zhu and Newsam 2019). There-\nfore, the important thing for MIL-based methods is to cor-\nrectly select anomalous instance in the positive bag. Most\nMIL-based methods regard an instance as an optimization\nunit (Zhang, Qing, and Miao 2019; Feng, Hong, and Zheng\n2021; Tian et al. 2021). However, if the model predicts the\nanomalous instances incorrectly in the positive bag, this er-\nror will be strengthened as the training progresses. That is, if\na normal instance is predicted as an abnormal instance, this\nerror will affect subsequent instance selection. In addition,\nthe abnormal event is usually multiple consecutive snippets,\nbut MIL-based methods do not consider this prior.\nIn order to alleviate the above-mentioned shortcomings,\nwe propose a Multi-Sequence Learning (MSL) method. Our\nMSL no longer uses an instance as the optimization unit, but\na sequence composed of multiple instances as the optimiza-\ntion unit. In other words, instead of choosing the instance\nwith the highest anomaly score, our MSL method choos-\nes the sequence with the highest sum of anomaly scores.\nThis reduces the probability of incorrect selection of anoma-\nlous instances. In order to achieve our MSL, we propose\na Transformer-based Multi-Sequence Learning Network,\nwhich includes a multi-layer Convolutional Transformer En-\ncoder to encode extracted snippet features, a Video ClassiÔ¨Åer\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n1395\nto predict video-level anomaly scores, and a Snippet Regres-\nsor to predict snippet-level anomaly scores. In the inference\nstage, we propose to use video-level anomaly scores to sup-\npress Ô¨Çuctuations in the snippet-level anomaly scores. Since\nthe goal of V AD is to predict Ô¨Åne-grained anomaly scores\n(Tian et al. 2021), a two stage self-training strategy is used\nto gradually reÔ¨Åne the anomaly scores.\nTo demonstrate the performance of our MSL, we use\nVideoSwin (a Transformer-based method) (Liu et al. 2021c)\nas the backbone to extract snippet-level features and con-\nduct experiments on ShanghaiTech (Luo, Liu, and Gao\n2017), UCF-Crime (Sultani, Chen, and Shah 2018), and XD-\nViolence (Wu et al. 2020). For a fair comparison, we also\nuse C3D (Tran et al. 2015) and I3D (Carreira and Zisserman\n2017) as the backbone to extract features. Experiments show\nthat our MSL achieve the state-of-the-art results. In summa-\nry, our main contributions are as follows:\n‚Ä¢We propose a Multi-Sequence Learning method, which\nuses a sequence composed of multiple instances as an\noptimization unit. Based on this, we propose a Multi-\nSequence Learning ranking loss, which selects the se-\nquence with the highest sum of anomaly scores.\n‚Ä¢Based on Multi-Sequence Learning and its ranking loss,\nwe design a Transformer-based Multi-Sequence Learn-\ning network, and propose to use the video-level anoma-\nly classiÔ¨Åcation probability to suppress the Ô¨Çuctuation of\nthe snippet-level anomaly score in the inference stage.\n‚Ä¢By gradually reducing the length of selected sequence,\nwe propose a two stage self-training strategy to gradually\nreÔ¨Åne the anomaly scores, because V AD needs to predict\nÔ¨Åne-grained anomaly scores.\n‚Ä¢Experimental results show that our method achieves the\nstate-of-the-art results on ShanghaiTech, UCF-Crime,\nand XD-Violence. The visualization shows that our\nmethod can realize the detection of abnormal snippets.\nRelated Work\nWeakly Supervised Video Anomaly Detection\nMost existing weakly supervised V AD methods (He, Shao,\nand Sun 2018; Zhang, Qing, and Miao 2019) are based on\nMIL. Since most methods (Li, Mahadevan, and Vasconcelos\n2014; Zhao, Fei-Fei, and Xing 2011) earlier than 2017 on-\nly used normal training videos, He, Shao, and Sun propose\nan anomaly-introduced learning method to detect abnormal\nevents, and propose a graph-based MIL model with both\nnormal and abnormal video data (He, Shao, and Sun 2018).\nSultani, Chen, and Shah propose a deep MIL ranking loss\nto predict anomaly scores (Sultani, Chen, and Shah 2018).\nZhang, Qing, and Miao further introduces inner-bag score\ngap regularization by deÔ¨Åning an inner bag loss (Zhang,\nQing, and Miao 2019). Zhong et al. consider the anomaly\ndetection with weak labels as a supervised learning under\nnoise labels, and design an alternate training procedure to\npromote the discrimination of action classiÔ¨Åers (Zhong et al.\n2019). Zhu and Newsam propose an attention-based tempo-\nral MIL ranking loss, which use temporal context to distin-\nguish between abnormal and normal events better (Zhu and\nNewsam 2019). Wan et al. propose a dynamic MIL loss to\nenlarge the inter-class distance between anomalous and nor-\nmal instances, and a center loss to reduce the intra-class dis-\ntance of normal instances (Wan et al. 2020). Feng, Hong,\nand Zheng propose a MIL-based pseudo label generator and\nadopt a self-training scheme to reÔ¨Åne pseudo-label by op-\ntimizing a self-guided attention encoder and a task-speciÔ¨Åc\nencoder (Feng, Hong, and Zheng 2021). Tian et al. propose\nan robust temporal feature magnitude learning to effectively\nrecognize the anomaly instances (Tian et al. 2021).\nSelf-Training\nSelf-training is widely used in semi-supervised learning\n(Rosenberg, Hebert, and Schneiderman 2005; Tanha, van\nSomeren, and Afsarmanesh 2017; Tao et al. 2018; Li et al.\n2019; Jeong, Lee, and Kwak 2020; Tai, Bailis, and Valiant\n2021). In self-training, the training data usually contain la-\nbeled and unlabeled data (Liu et al. 2011). Self-training\nincludes the following steps (Zheng et al. 2020; Yu et al.\n2021): 1) Train model with labeled data; 2) Use the trained\nmodel to predict unlabeled data to generate pseudo-labels;\n3) Train model with labeled and pseudo-labeled data to-\ngether; 4) Repeat 2) and 3). In V AD, Pang et al. pro-\npose a self-training deep neural network for ordinal regres-\nsion (Pang et al. 2020). Feng, Hong, and Zheng propose\na multi-instance self-training method that assigns snippet-\nlevel pseudo-labels to all snippets in abnormal videos (Feng,\nHong, and Zheng 2021). Unlike them, our focus is on reÔ¨Ån-\ning anomaly scores through self-training.\nTransformer Combined With Convolution\nMore and more studies have shown that Transformer has ex-\ncellent performance (Dosovitskiy et al. 2021; Touvron et al.\n2021; Liu et al. 2021b). Dosovitskiy et al. Ô¨Årst prove that a\npure Transformer architecture can attain state-of-the-art per-\nformance (Dosovitskiy et al. 2021). Touvron et al. further\nexplore the data-efÔ¨Åcient training strategies for the vision\ntransformer (Dosovitskiy et al. 2021; Touvron et al. 2021).\nLiu et al. further introduces the inductive biases of local-\nity, hierarchy and translation invariance for various image\nrecognition tasks (Liu et al. 2021b). Because transformer\nlacks the ability of local perception, many works combine\nconvolution and transformer (d‚ÄôAscoli et al. 2021; Wu et al.\n2021; Li et al. 2021; Xu et al. 2021; Yan et al. 2021; Zhang\nand Yang 2021; Liu et al. 2021a). To introduce local inter-\nframe perception, similar to Wu et al., we turn the linear pro-\njection in the Transformer Block into a Depthwise Separable\n1D Convolution (Chollet 2017; Howard et al. 2017).\nOur Approach\nIn this section, we Ô¨Årst deÔ¨Åne the notations and problem s-\ntatement. We then introduce our Multi-Sequence Learning\n(MSL). Finally, we present the pipeline of our approach.\nNotations and Problem Statement\nIn weakly supervised V AD, training videos are only labeled\nat the video-level. That is, videos containing anomalies are\nlabeled as 1 (positive), and videos without any anomalies\n1396\n(a) Multi-Sequence Learning Architecture\n(b) Self-Training Multi-Sequence Learning Pipeline\nStage 2: \nMSL with \npredictions to \nselect sequences\nùêæ ‚ü∏ ùëá\nùêæ ‚ü∏ ùêæ\n2\n(c) Convolutional Transformer Encoder\nDW Conv1DDW Conv1DDW Conv1D\nMLP\nLayer Norm\n+\nV K Q\nConcat\nLinear Multi-Head \nAttention\nScaled Dot-Product Attention\nLayer Norm\nFeature ùêπ\n+\nBackbone\nMSL \nRanking \nLoss\nFeature ùêπ\nh head\nFeaturesInference\nBCE Loss\nCTE2\nCTE2\nClass Token\nVideo Classifier\nLinear Head\nSnippet Regressor\nLinear Head\nùëù\nùëìùúÉ(ùë£1)\nTransformer-based Multi-Sequence \nLearning Network\nùëìùúÉ(ùë£7)\nùëìùúÉ(ùë£2)\nùëìùúÉ(ùë£3)\nùëìùúÉ(ùë£4)\nùëìùúÉ(ùë£5)\nùëìùúÉ(ùë£6)\nStage 1: \nMSL with \npseudo-labels to \nselect sequences\nMulti-Sequence Learning \nFeatures\n1\n0\npositive bag\n1\n0\nnegative bag\nInitial Snippet-\nLevel Pseudo-\nLabels\nNew Snippet-\nLevel Pseudo-\nLabels\nT1\n0\n1\nbag\nFigure 1: Overall framework. (a) The architecture of our Multi-Sequence Learning (MSL), which includes a Backbone and\na Transformer-based MSL Network (MSLNet). The feature F ‚ààRT √óD extracted by the Backbone is input into MSLNet\nto predict the anomaly scores, where T is the number of snippets and D is the feature dimension of each snippet. MSLNet\ncontains a video classiÔ¨Åer to predict the probability pof the video containing anomalies and a snippet regressor to predict the\nsnippet anomaly score f\u0012(vi) of the i-th snippet. BCE is the Binary Cross Entropy loss. (b) The pipeline of self-training MSL,\nwhere K gradually changes from T to 1 through a self-training mechanism. According to the way of selecting sequences,\nthe optimization of MSL includes two stages: the Ô¨Årst stage uses pseudo-labels to select sequences and the second stage uses\npredictions to select sequences. (c) Convolutional Transformer Encoder (CTE), which is similar to (Dosovitskiy et al. 2021),\nexcept that the linear projection is replaced with DW Conv1D (Depthwise Separable 1D Convolution) (Howard et al. 2017).\nare labeled as 0 (negative). Given a video V = {vi}T\ni=1\nwith T snippets and its video-level label Y ‚àà{0;1}. MIL-\nbased methods treat video V as a bag and each snippet as\nan instance. A positive video is regarded as a positive bag\nBa = (a1;a2;:::;a T), and a negative video is regarded as a\nnegative bag Bn = (n1;n2;:::;n T). The goal of V AD is to\nlearn a function f\u0012 maps snippets to their anomaly scores,\nranging from 0 to 1. Generally, MIL-based V AD assumes\nthat abnormal snippets have higher abnormal scores than\nnormal snippets. Sultani, Chen, and Shah formulate V AD\nas an anomaly score regression problem and propose a MIL\nranking objective function and a MIL ranking loss (Sultani,\nChen, and Shah 2018):\nmax\ni‚ààBa\nf\u0012(ai) >max\ni‚ààBn\nf\u0012(ni): (1)\nL(Ba;Bn) = max(0;max\ni‚ààBn\nf\u0012(ni) ‚àímax\ni‚ààBa\nf\u0012(ai)): (2)\nThe intuition behind Eq.1 and Eq.2 that the snippet with\nhighest anomaly score in the positive bag should rank higher\nthan the snippet with highest anomaly score in the negative\nbag (Zhu and Newsam 2019). In order to keep a large margin\nbetween the positive and negative instances, Sultani, Chen,\nand Shah give a hinge-based ranking loss:\nL(Ba;Bn) = max(0;1 ‚àímax\ni‚ààBa\nf\u0012(ai) + max\ni‚ààBn\nf\u0012(ni)): (3)\nAt the beginning of the optimization, f\u0012 needs to have a\ncertain ability to predict abnormalities. Otherwise, it will be\npossible to select a normal instance as an abnormal instance.\nIf f\u0012 predicts the instances in the positive bag incorrectly,\ne.g. predicting normal instances as abnormal instances, this\nerror will be strengthened as the training progresses. In ad-\ndition, the abnormal event is usually multiple consecutive\nsnippets, but MIL-based methods do not consider this prior.\nMulti-Sequence Learning\nIn order to alleviate the above shortcomings in MIL-based\nmethods, we propose a novel Multi-Sequence Learning (M-\nSL) method. As shown in Figure 2, given a video V =\n{vi}T\ni=1 with T snippets, the anomaly score curve is pre-\ndicted through a mapping function f\u0012. Let us assume that\n1397\n1 ùëá5\nsnippet\nscore\n5\nùëìùúÉ(ùë£7)\n1 ùëáùëñ ùëñ+ùêæ\n(a)\n(b)\n(c)\nùëá1\nFigure 2: Comparison of instance selection method between\nMIL and our MSL. (a) Anomaly score curve of a video con-\ntaining T snippets, assuming that the 5-th snippet has the\nlargest anomaly score f\u0012(v5). (b) Instance selection method\nof MIL, which selects the5-th snippet. (c) Instance selection\nmethod of our MSL, which selects a sequence consisting of\nKconsecutive snippets starting from the i-th snippet.\nthe 5-th snippet v5 has the largest anomaly score f\u0012(v5). In\nMIL-based methods, the 5-th snippet will be selected to op-\ntimize the network (Zhu and Newsam 2019). In our MSL,\ngiven a hyperparameter K, we propose a sequence selection\nmethod, which selects a sequence that contains K consec-\nutive snippets. In detail, we calculate the mean of anomaly\nscores of all possible sequences of Kconsecutive snippets:\nS = {si}T‚àíK\ni=1 ; s i = 1\nK\nK‚àí1X\nk=0\nf\u0012(vi+k); (4)\nwhere si represents the mean of anomaly scores of the se-\nquence of Kconsecutive snippets starting from thei-th snip-\npet. Then, the sequence with the largest mean of abnormal\nscores can be selected by maxsi‚ààSsi.\nBased on the above sequence selection method, we can\nsimply use an MSL ranking objective function as:\nmax\nsa;i‚ààSa\nsa;i > max\nsn;i‚ààSn\nsn;i;\nsa;i = 1\nK\nK‚àí1X\nk=0\nf\u0012(ai+k); s n;i = 1\nK\nK‚àí1X\nk=0\nf\u0012(ni+k):\n(5)\nwhere sa;i and sn;i represent the mean of abnormal scores\nof K consecutive snippets starting from the i-th snippet in\nabnormal video and normal video, respectively. The intu-\nition of our MSL ranking objective function is that the mean\nof abnormal scores of K consecutive snippets in abnormal\nvideos should be greater than the mean of abnormal scores\nof Kconsecutive snippets in normal videos. To keep a large\nmargin between the positive and negative instances, similar\nto Eq. 3, our hinge-based MSL ranking loss is deÔ¨Åned as:\nL(Ba;Bn) = max(0;1 ‚àí max\nsa;i‚ààSa\nsa;i + max\nsn;i‚ààSn\nsn;i): (6)\nIt can be seen that MIL is a case of our MSL. When K = 1,\nMIL and our MSL are equivalent. When K = T, our MSL\ntreats every snippet in the abnormal video as abnormal.\nTransformer-based MSL Network\nConvolutional Transformer Encoder Before introduc-\ning our Transformer-based MSL architecture, we Ô¨Årst intro-\nduce the basic layer. Transformer (Vaswani et al. 2017) uses\nsequence data as input to model long-range relationships,\nand has made great progress in many tasks. We adopt Trans-\nformer as our basic layer. The representation between the\nlocal frames or snippets of the video is also very important.\nHowever, Transformer is not good at learning local represen-\ntations of adjacent frames or snippets (Yan et al. 2021). Mo-\ntivated by this, as shown in Figure 1(c), we replace the lin-\near projection in the original Transformer with a DW Con-\nv1D (Depthwise Separable 1D Convolution) (Howard et al.\n2017) projection. The new Transformer is named Convolu-\ntional Transformer Encoder (CTE). In this way, our CTE\ncan inherit the advantages of Transformer and Convolutional\nNeural Network.\nTransformer-based MSL Network As shown in Figure\n1 (a), our architecture includes a Backbone and a MSLNet.\nAny action recognition method can be used as the Backbone,\nsuch as C3D (Tran et al. 2015), I3D (Carreira and Zisserman\n2017), and VideoSwin (Liu et al. 2021c). Similar to (Tian\net al. 2021), the Backbone uses pre-trained weights on the\naction recognition datasets (Karpathy et al. 2014; Kay et al.\n2017). Through the Backbone, a featureF ‚ààRT√óD extracts\nfrom a video containing T snippets, where Dis the feature\ndimension of each snippet. Our MSLNet will use F as the\ninput to predict anomalies.\nOur MSLNet includes a video classiÔ¨Åer and a snippet re-\ngressor. The video classiÔ¨Åer is used to predict whether the\nvideo contains anomalies. SpeciÔ¨Åcally, the video classiÔ¨Åer\ncontains two layers of CTE and a linear head for predicting\nthe probability of whether the video contains anomalies:\np= \u001b(Wc¬∑Ec[0]); Ec = CTE√ó2 (classtoken||F); (7)\nwhere Wc is the parameter of the linear head, pis the prob-\nability that the video contains anomalies, and class token\nis used to predict the probability by aggregated features in\nCTE. Since whether the video contains anomalies is a bina-\nry classiÔ¨Åcation problem, \u001bchooses the sigmoid function.\nThe snippet regressor is used to predict the anomaly s-\ncore of each snippet. SpeciÔ¨Åcally, the snippet regressor con-\ntains two layers of CTE and a linear head for predicting the\nanomaly score of each snippet:\nf\u0012(vi) =\u001b(Wr ¬∑Er[i]); E r = CTE√ó2 (Ec); (8)\nwhere Wr is the parameter of the linear head, f\u0012(vi) is the\nabnormal score of thei-th snippet, and Er[i] is the feature of\nthe i-th snippet. Since predicting the anomaly score is treat-\ned as a regression problem, \u001bchooses the sigmoid function.\nWe regard the optimization of the video classiÔ¨Åer and s-\nnippet regressor as a multi-task learning problem. The total\nloss to optimize the parameters of MSLNet is the sum of our\nhinge-based MSL ranking loss and the classiÔ¨Åcation loss:\nL= L(Ba;Bn) +BCE(p;Y ); (9)\nwhere L(Ba;Bn) is the Eq. 6, and BCE is the Binary Cross\nEntropy loss between the output pand the target Y.\nTo reduce the Ô¨Çuctuation of the abnormal scores predict-\ned by the snippet regressor, we propose a score correction\nmethod in the inference stage. SpeciÔ¨Åcally, the score cor-\nrection method corrects the abnormal scores by using the\n1398\nprobability of whether the video contains anomalies:\n^f\u0012(vi) =f\u0012(vi) √óp: (10)\nThe intuition of this method is that to keep the anomaly s-\ncores when the video classiÔ¨Åer predicts that the video con-\ntains anomalies with a higher probability, and weaken the\nanomaly scores when the video classiÔ¨Åer predicts that the\nvideo contains anomalies with a lower probability.\nSelf-Training MSL\nAs shown in Figure 1 (b), we propose a self-training mecha-\nnism to achieve the training from coarse to Ô¨Åne. The training\nprocess of our MSLNet includes two training stages. Be-\nfore introducing our self-training mechanism, we Ô¨Årst get\nthe pseudo-labels ^Yof the training videos. By taking the\nknown video-level labels Yin weakly supervised V AD as\nthe anomaly scores of snippets, we can immediately get the\ninitial snippet-level pseudo-labels ^Y. That is, for an abnor-\nmal video, the pseudo label of each snippet is 1, and for a\nnormal video, the pseudo label of each snippet is 0.\nIn the initial stage of training, the function f\u0012 has a poor\nability to predict abnormalities. Therefore, if the sequence\nis selected directly through the prediction of f\u0012, there is a\nprobability of selecting the wrong sequence. Based on this\nmotivation, we propose a transitional stage (stage one): MSL\nwith pseudo-labels to select sequences. SpeciÔ¨Åcally, by re-\nplacing the predicted anomaly score f\u0012(vi) in Eq. 4 with the\npseudo-label ^yi of each snippet vi, we select the sequence\nwith the largest mean of pseudo labels bymaxsi‚ààSsi. Based\non this sequence, we can calculatesa;iand sn;i, and then op-\ntimize MSLNet through the hinge-based MSL ranking loss:\nL(Ba;Bn) = max(0;1 ‚àísa;i + sn;i); (11)\nwhere sa;iand sn;iare the sequence with the largest mean of\npseudo labels starting from the i-th snippet in the abnormal\nand normal video, respectively. AfterE1 epochs training, f\u0012\nhas a preliminary ability to predict the anomaly scores.\nIn stage two, MSLNet is optimized with predictions to\nselect sequences. This stage uses Eq. 5 and Eq. 6 to calculate\nthe ranking loss. After E2 epochs training, the new snippet-\nlevel pseudo-labels ^Yof training videos are inferenced. By\nhalving the sequence length K and repeating the above two\nstages, the predicted anomaly scores are gradually reÔ¨Åned.\nThe role of the transitional stage is to establish a connec-\ntion between MSL and different self-training rounds. By in-\ntroducing a self-training mechanism, we achieve the predic-\ntion of anomaly scores from coarse to Ô¨Åne. For better under-\nstanding, we show our self-training MSL in Algorithm 1.\nExperiments\nDatasets and Evaluation Metrics\nWe conduct sufÔ¨Åcient experiments on the ShanghaiTech,\nUCF-Crime, and XD-Violence datasets.\nShanghaiTech is a medium-scale dataset that contains 437\ncampus surveillance videos with 130 abnormal events in 13\nscenes (Luo, Liu, and Gao 2017). However, all the training\nvideos of this dataset are normal. In line with the weakly\nAlgorithm 1: Our Self-Training Multi-Sequence Learning.\nInput: A set of features Fand its video-level labels Y.\nParameter: The number T of snippets.\nOutput: MSLNet.\n1: Set K ‚Üê T.\n2: Get the initial snippet-level pseudo-labels ^Yby Y.\n3: while K ‚â•1 do\n4: Initialize the parameters of MSLNet.\n5: // Stage one: with pseudo-labels to select sequences.\n6: Optimize MSLNet with Kby F, ^Y, and Eq. 11.\n7: // Stage two: with predictions to select sequences.\n8: Optimize MSLNet with Kby Fand Eq. 6.\n9: Inference the new snippet-level pseudo-labels ^Y.\n10: Set K ‚Üê K\n2 .\n11: end while\n12: return MSLNet.\nsupervised setting, we adopt the split proposed by (Zhong\net al. 2019): 238 training videos and 199 testing videos.\nUCF-Crime is a large-scale dataset that contains 1,900\nuntrimmed real-world street and indoor surveillance videos\nwith 13 classes of anomalous events and a total duration of\n128 hours (Sultani, Chen, and Shah 2018). The training set\ncontains 1,610 videos with video-level labels, and the test\nset contains 290 videos with frame-level labels.\nXD-Violence is a large-scale dataset that contains 4,754\nuntrimmed videos with a total duration of 217 hours and col-\nlect from multiple sources, such as movies, sports, surveil-\nlances, and CCTVs (Wu et al. 2020). The training set con-\ntains 3,954 videos with video-level labels, and the test set\ncontains 800 videos with frame-level labels.\nFollowing previous works (Zhong et al. 2019; Wan et al.\n2020), we use the AUC (Area Under Curve) of frame-level\nROC (Receiver Operating Characteristic) as our metric for\nShanghaiTech and UCF-Crime. Following previous works\n(Wu et al. 2020; Tian et al. 2021), we use the AP (Aver-\nage Precision) as our metric for XD-Violence. Note that the\nlarger the value of AUC and AP, the better the performance.\nImplementation Details\nWe extract the 4,096D features from the fc6 layer of the\npre-trained C3D (Tran et al. 2015) on Sports-1M (Karpa-\nthy et al. 2014), the 1,024D features from the mixed5c lay-\ner of the pre-trained I3D (Carreira and Zisserman 2017) on\nKinetics-400 (Kay et al. 2017), and the 1,024D features from\nthe Stage4 layer of the pre-trained VideoSwin (Liu et al.\n2021c) on Kinetics-400. Following previous works (Tian\net al. 2021), we divide each video into 32 snippets, that is,\nT = 32and K ‚àà{32;16;8;4;2;1}. The length of each s-\nnippet is 16. Our MSLNet is trained using the SGD optimiz-\ner with a learning rate of 0.001, a weight decay of 0.0005\nand a batch size of 64. We setE1 to 100 and E2 to 400. Fol-\nlowing (Tian et al. 2021), each mini-batch is composed of 32\nrandomly selected normal and abnormal videos. In abnormal\nvideos, we randomly select one of the top 10% snippets as\nthe abnormal snippet. In CTE, we set the number of headers\nto 12 and use DW Conv1D with kernel size is 3.\n1399\nMethod Feature Crop AUC(%) ‚Üë\nMIL-Rank‚Ä† I3D RGB one 85.33\nGCN C3D-RGB ten 76.44\nGCN TSN-Flow ten 84.13\nGCN TSN-RGB ten 84.44\nIBL I3D-RGB one 82.50\nAR-Net‚Ä† C3D RGB one 85.01\nAR-Net I3D Flow one 82.32\nAR-Net I3D RGB one 85.38\nAR-Net I3D-RGB+Flow one 91.24\nCLAWS C3D-RGB one 89.67\nMIST C3D-RGB one 93.13\nMIST I3D-RGB one 94.83\nRTFM C3D-RGB ten 91.51\nRTFM I3D-RGB ten 97.21\nRTFM‚àó VideoSwin-RGB ten 96.76\nOurs C3D-RGB one 94.23\nOurs I3D-RGB one 95.45\nOurs VideoSwin-RGB one 96.93\nOurs C3D-RGB ten 94.81\nOurs I3D-RGB ten 96.08\nOurs VideoSwin-RGB ten 97.32\nTable 1: Compared with related methods on ShanghaiTech.\nThe methods with ‚Ä†are reported by (Feng, Hong, and Zheng\n2021) or (Tian et al. 2021). ‚àóindicates we re-train the\nmethod. Under the same feature, the highest result is bolded.\nResults on ShanghaiTech\nWe report the results on ShanghaiTech (Zhong et al. 2019)\nin Table 1. For a fair comparison, we use two features:\none-crop and ten-crop. One-crop means cropping snippet-\ns into the center. Ten-crop means cropping snippets into\nthe center, four corners, and their Ô¨Çipped version (Zhong\net al. 2019). Under the same backbone and crop, compared\nwith the previous weakly supervised methods, our method-\ns achieve the superior performance on AUC. For example,\nwith the one-crop I3D-RGB feature, our model achieves an\nAUC of 95.45% and outperforms all other methods with the\nsame crop, and with the ten-crop VideoSwin-RGB feature,\nour model achieves the best AUC of 97.32%.\nResults on UCF-Crime\nWe report our experimental results on UCF-Crime (Sul-\ntani, Chen, and Shah 2018) in Table 2. Under I3D and\nVideoSwin as the backbone, our method outperforms all pre-\nvious weakly supervised methods on the frame-level AUC\nmetric. Under C3D as the backbone, our method has also\nachieved competitive result. For example, with the one-crop\nI3D-RGB feature, our model achieves an AUC of 85.30%\nand outperforms all other methods, and with the one-crop\nVideoSwin-RGB feature, our model achieves the best AUC\nof 85.62% which is higher than RTFM by 2.31%.\nResults on XD-Violence\nWe report our results on XD-Violence (Wu et al. 2020) in\nTable 3. For a fair comparison, we use the same Ô¨Åve-crop\nMethod Feature Crop AUC(%) ‚Üë\nMIL-Rank C3D RGB one 75.41\nMIL-Rank‚Ä† I3D RGB one 77.92\nMotion-Aware PWC-Flow one 79.00\nGCN C3D-RGB ten 81.08\nGCN TSN-Flow ten 78.08\nGCN TSN-RGB ten 82.12\nIBL C3D-RGB one 78.66\nCLAWS C3D-RGB ten 83.03\nMIST C3D-RGB one 81.40\nMIST I3D-RGB one 82.30\nRTFM C3D-RGB ten 83.28\nRTFM I3D-RGB ten 84.03\nRTFM‚àó VideoSwin-RGB one 83.31\nOurs C3D-RGB one 82.85\nOurs I3D-RGB one 85.30\nOurs VideoSwin-RGB one 85.62\nTable 2: Compared with other methods on UCF-Crime. The\nmethod with ‚Ä†is reported by (Tian et al. 2021). ‚àóindicates\nwe re-train the method. Bold represents the best results.\nMethod Feature Crop AP(%) ‚Üë\nMIL-Rank‚Ä† C3D RGB Ô¨Åve 73.20\nMIL-Rank‚Ä† I3D RGB Ô¨Åve 75.68\nMultimodal-VD I3D-RGB Ô¨Åve 75.41\nRTFM C3D-RGB Ô¨Åve 75.89\nRTFM I3D-RGB Ô¨Åve 77.81\nRTFM‚àó VideoSwin-RGB Ô¨Åve 77.95\nOurs C3D-RGB Ô¨Åve 75.53\nOurs I3D-RGB Ô¨Åve 78.28\nOurs VideoSwin-RGB Ô¨Åve 78.59\nTable 3: Compared with related methods on XD-Violence.\nThe methods with ‚Ä†are reported by (Wu et al. 2020) or (Tian\net al. 2021). ‚àóindicates we re-train the method.\nfeatures with other methods. Five-crop means cropping s-\nnippets into the center and four corners. Under the same\nbackbone, our method outperforms all previous weakly su-\npervised V AD methods on the AP metric. For example, with\nÔ¨Åve-crop I3D-RGB features, our model achieves an AP of\n78.28% and outperforms all other methods, and with Ô¨Åve-\ncrop VideoSwin-RGB features, our model achieves an AP\nof 78.59% which higher than RTFM by 0.64%.\nComplexity Analysis\nGenerally, Transformer has been often computationally ex-\npensive, but our method can achieve real-time surveillance.\nOn an NVIDIA 2080 GPU, with VideoSwin (Liu et al.\n2021c) as the backbone processes 3.6 snippets per second\n(a snippet has 16 frames), which is 57.6 frames per second\n(FPS); with I3D (Carreira and Zisserman 2017) as the back-\nbone processes 6.5 snippets per second, which is 104 FPS.\nOur MSL Network can reach 156.4 forwards per second.\nOverall, the speed with VideoSwin as the backbone is 42\nFPS, and the speed with I3D as the backbone is 63 FPS.\n1400\n(a) Abnormal Video 01 0025\n (b) Abnormal Video 03 0032\n (c) Abnormal Video 01 0051\n (d) Normal Video 08 045\n(e) Explosion 033 x264\n (f) RoadAccidents 012 x264\n (g) Robbery 102 x264\n (h) Normal Video 894 x264\nFigure 3: Visualization of abnormal score curves. The horizontal axis represents the number of frames, and the vertical axis\nrepresents the abnormal scores. Videos of (a), (b), (c), and (d) are from the ShanghaiTech dataset, and videos of (e), (f), (g), and\n(h) are from the UCF-Crime dataset. The curves indicate the abnormal scores of the video frames, pink areas indicate that the\ninterval contains an abnormal event, and the red rectangles indicate the location of abnormal events. Best viewed in color.\nBasic Layer ShanghaiTech UCF-Crime\nTransformer 96.51 85.41\nCTE 96.93 (+0.42) 85.62 (+0.21)\nTable 4: Compared with Transformer (Dosovitskiy et al.\n2021), AUC(%) improvement brought by CTE on the\nShanghaiTech and UCF-Crime datasets.\nQualitative Analysis\nIn order to further demonstrate the effect of our method, as\nshown in Figure 3, we visualize the anomaly score curves.\nThe Ô¨Årst row shows the ground truth and prediction anomaly\nscores of three abnormal videos and one normal video from\nthe ShanghaiTech dataset. From the Ô¨Årst row of Figure 3,\nwe can see that our method can detect abnormal events in\nsurveillance videos. Our method successfully predicts short-\nterm abnormal events (Figure 3 (a)) and long-term abnormal\nevents (Figure 3 (b)). Furthermore, our method can also de-\ntect multiple abnormal events in a video (Figure 3 (c)). The\nsecond row shows the ground truth and predicted anomaly\nscores of three abnormal videos and one normal video from\nthe UCF-Crime dataset. From the second row of Figure 3,\nwe can see that our proposed method can also detect abnor-\nmal events in complex surveillance scenes.\nAblation Analysis\nIn order to further evaluate our method, we perform ablation\nstudies on the ShanghaiTech and UCF-Crime datasets with\none-crop VideoSwin-RGB features.\nImprovement brought by CTE.To evaluate the effect of\nour CTE, we replace CTE with the standard Transformer\n(Dosovitskiy et al. 2021). The dimension of the standard\nTransformer is the same as our CTE. Table 4 reports the re-\nScore correction ShanghaiTech UCF-Crime\n√ó 95.98 84.94\nX 96.93 (+0.95) 85.62 (+0.68)\nTable 5: Performance improvement brought by the score\ncorrection method in the inference stage measured by\nAUC(%) on the ShanghaiTech and UCF-Crime datasets.\nsults of this ablation experiment. Compared with the result\nusing the standard Transformer as the basic layer, the result\nwith CTE as the basic layer increases an AUC by 0.42% and\n0.21% on the ShanghaiTech and UCF-Crime datasets.\nImpact of score correction in the inference stage.As\nshown in Table 5, we conduct an experiment to report the\nperformance improvement brought by the score correction\nmethod in the inference stage. From Table 5 we can ob-\nserve that score correction can bring an AUC improvemen-\nt of 0.95% and 0.68% with the one-crop features on the\nShanghaiTech and UCF-Crime datasets, respectively.\nConclusion\nIn this work, we Ô¨Årst propose an MSL method and a hinge-\nbased MSL ranking loss. We then design a Transformer-\nbased network to learn both video-level anomaly probability\nand snippet-level anomaly scores. In the inference stage, we\npropose to use the video-level anomaly probability to sup-\npress the Ô¨Çuctuation of snippet-level anomaly scores. Final-\nly, since V AD needs to predict the instance-level anomaly s-\ncores, by gradually reducing the length of selected sequence,\nwe propose a self-training strategy to reÔ¨Åne the anomaly s-\ncores. Experimental results show that our method achieves\nsigniÔ¨Åcant improvements on three public datasets.\n1401\nAcknowledgements\nThis work was supported in part by the National Natural\nScience Foundation of China (No.62076192), Key Research\nand Development Program in Shaanxi Province of China\n(No.2019ZDLGY03-06), the State Key Program of National\nNatural Science of China (No.61836009), in part by the Pro-\ngram for Cheung Kong Scholars and Innovative Research\nTeam in University (No.IRT\n15R53), in part by The Fund\nfor Foreign Scholars in University Research and Teaching\nPrograms (the 111 Project) (No.B07048), in part by the\nKey ScientiÔ¨Åc Technological Innovation Research Project\nby Ministry of Education, the National Key Research and\nDevelopment Program of China.\nReferences\nCai, R.; Zhang, H.; Liu, W.; Gao, S.; and Hao, Z.\n2021. Appearance-Motion Memory Consistency Network\nfor Video Anomaly Detection. In AAAI, 938‚Äì946.\nCarreira, J.; and Zisserman, A. 2017. Quo Vadis, Action\nRecognition? A New Model and the Kinetics Dataset. In\nCVPR, 4724‚Äì4733.\nChollet, F. 2017. Xception: Deep Learning with Depthwise\nSeparable Convolutions. In CVPR, 1800‚Äì1807.\nd‚ÄôAscoli, S.; Touvron, H.; Leavitt, M. L.; Morcos, A. S.;\nBiroli, G.; and Sagun, L. 2021. ConViT: Improving Vision\nTransformers with Soft Convolutional Inductive Biases. In\nICML, volume 139, 2286‚Äì2296.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In ICLR.\nFeng, J.-C.; Hong, F.-T.; and Zheng, W.-S. 2021. Mist: Mul-\ntiple instance self-training framework for video anomaly de-\ntection. In CVPR, 14009‚Äì14018.\nGong, D.; Liu, L.; Le, V .; Saha, B.; Mansour, M. R.;\nVenkatesh, S.; and van den Hengel, A. 2019. Memorizing\nNormality to Detect Anomaly: Memory-Augmented Deep\nAutoencoder for Unsupervised Anomaly Detection. In IC-\nCV, 1705‚Äì1714.\nGuo, Z.; Zhao, J.; Jiao, L.; Liu, X.; and Liu, F. 2021. A\nUniversal Quaternion Hypergraph Network for Multimodal\nVideo Question Answering. IEEE Transactions on Multime-\ndia, 1‚Äì1.\nHe, C.; Shao, J.; and Sun, J. 2018. An anomaly-introduced\nlearning method for abnormal event detection. Multimedia\nTools and Applications, 77(22): 29573‚Äì29588.\nHoward, A. G.; Zhu, M.; Chen, B.; Kalenichenko, D.; Wang,\nW.; Weyand, T.; Andreetto, M.; and Adam, H. 2017. Mo-\nbileNets: EfÔ¨Åcient Convolutional Neural Networks for Mo-\nbile Vision Applications. CoRR, abs/1704.04861.\nJeong, J.; Lee, S.; and Kwak, N. 2020. Self-Training us-\ning Selection Network for Semi-supervised Learning. In\nICPRAM, 23‚Äì32.\nKarpathy, A.; Toderici, G.; Shetty, S.; Leung, T.; Sukthankar,\nR.; and Li, F. 2014. Large-Scale Video ClassiÔ¨Åcation with\nConvolutional Neural Networks. In CVPR, 1725‚Äì1732.\nKay, W.; Carreira, J.; Simonyan, K.; Zhang, B.; Hillier, C.;\nVijayanarasimhan, S.; Viola, F.; Green, T.; Back, T.; Natsev,\nP.; Suleyman, M.; and Zisserman, A. 2017. The Kinetics\nHuman Action Video Dataset. CoRR, abs/1705.06950.\nLi, W.; Mahadevan, V .; and Vasconcelos, N. 2014. Anoma-\nly Detection and Localization in Crowded Scenes. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n36(1): 18‚Äì32.\nLi, Y .; Xing, R.; Jiao, L.; Chen, Y .; Chai, Y .; Marturi, N.;\nand Shang, R. 2019. Semi-Supervised PolSAR Image Clas-\nsiÔ¨Åcation Based on Self-Training and Superpixels. Remote.\nSens., 11(16): 1933.\nLi, Y .; Zhang, K.; Cao, J.; Timofte, R.; and Gool, L. V . 2021.\nLocalViT: Bringing Locality to Vision Transformers.CoRR,\nabs/2104.05707.\nLiu, K.; and Ma, H. 2019. Exploring Background-Bias for\nAnomaly Detection in Surveillance Videos. In Proceedings\nof the 27th ACM International Conference on Multimedia,\nMM ‚Äô19, 14901499.\nLiu, X.; Li, K.; Zhou, M.; and Xiong, Z. 2011. Enhancing\nSemantic Role Labeling for Tweets Using Self-Training. In\nAAAI.\nLiu, Y .; Sun, G.; Qiu, Y .; Zhang, L.; Chhatkuli, A.; and Gool,\nL. V . 2021a. Transformer in Convolutional Neural Network-\ns. CoRR, abs/2106.03180.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021b. Swin Transformer: Hierarchical\nVision Transformer using Shifted Windows. CoRR, ab-\ns/2103.14030.\nLiu, Z.; Ning, J.; Cao, Y .; Wei, Y .; Zhang, Z.; Lin, S.; and Hu,\nH. 2021c. Video Swin Transformer.CoRR, abs/2106.13230.\nLuo, W.; Liu, W.; and Gao, S. 2017. A Revisit of Sparse\nCoding Based Anomaly Detection in Stacked RNN Frame-\nwork. In ICCV, 341‚Äì349.\nPang, G.; Yan, C.; Shen, C.; van den Hengel, A.; and Bai, X.\n2020. Self-Trained Deep Ordinal Regression for End-to-End\nVideo Anomaly Detection. In CVPR, 12170‚Äì12179.\nRosenberg, C.; Hebert, M.; and Schneiderman, H. 2005.\nSemi-Supervised Self-Training of Object Detection Models.\nIn WACV/MOTION, 29‚Äì36.\nSultani, W.; Chen, C.; and Shah, M. 2018. Real-World\nAnomaly Detection in Surveillance Videos. InCVPR, 6479‚Äì\n6488.\nTai, K. S.; Bailis, P.; and Valiant, G. 2021. Sinkhorn\nLabel Allocation: Semi-Supervised ClassiÔ¨Åcation via An-\nnealed Self-Training. In ICML, volume 139, 10065‚Äì10075.\nTanha, J.; van Someren, M.; and Afsarmanesh, H. 2017.\nSemi-supervised self-training for decision tree classiÔ¨Åers.\nInt. J. Mach. Learn. Cybern., 8(1): 355‚Äì370.\nTao, Y .; Zhang, D.; Cheng, S.; and Tang, X. 2018. Improv-\ning semi-supervised self-training with embedded manifold\ntransduction. Trans. Inst. Meas. Control, 40(2): 363‚Äì374.\nTian, Y .; Pang, G.; Chen, Y .; Singh, R.; Verjans, J. W.; and\nCarneiro, G. 2021. Weakly-supervised Video Anomaly De-\ntection with Robust Temporal Feature Magnitude Learning.\nCoRR, abs/2101.10030.\n1402\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J¬¥egou, H. 2021. Training data-efÔ¨Åcient image trans-\nformers & distillation through attention. In ICML, volume\n139, 10347‚Äì10357.\nTran, D.; Bourdev, L. D.; Fergus, R.; Torresani, L.; and\nPaluri, M. 2015. Learning Spatiotemporal Features with 3D\nConvolutional Networks. In ICCV, 4489‚Äì4497.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All you Need. In NIPS, 5998‚Äì6008.\nWan, B.; Fang, Y .; Xia, X.; and Mei, J. 2020. Weakly super-\nvised video anomaly detection via center-guided discrimina-\ntive learning. In ICME, 1‚Äì6. IEEE.\nWan, B.; Jiang, W.; Fang, Y .; Luo, Z.; and Ding, G. 2021.\nAnomaly detection in video sequences: A benchmark and\ncomputational model. IET Image Processing.\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.;\nand Zhang, L. 2021. CvT: Introducing Convolutions to Vi-\nsion Transformers. CoRR, abs/2103.15808.\nWu, P.; Liu, j.; Shi, Y .; Sun, Y .; Shao, F.; Wu, Z.; and Yang,\nZ. 2020. Not only Look, but also Listen: Learning Multi-\nmodal Violence Detection under Weak Supervision. In EC-\nCV.\nXu, W.; Xu, Y .; Chang, T. A.; and Tu, Z. 2021. Co-\nScale Conv-Attentional Image Transformers. CoRR, ab-\ns/2104.06399.\nYan, H.; Li, Z.; Li, W.; Wang, C.; Wu, M.; and Zhang, C.\n2021. ConTNet: Why not use convolution and transformer\nat the same time? CoRR, abs/2104.13497.\nYu, F.; Zhang, M.; Dong, H.; Hu, S.; Dong, B.; and Zhang, L.\n2021. DAST: Unsupervised Domain Adaptation in Semantic\nSegmentation Based on Discriminator Attention and Self-\nTraining. In AAAI, 10754‚Äì10762.\nZhang, J.; Qing, L.; and Miao, J. 2019. Temporal Convo-\nlutional Network with Complementary Inner Bag Loss for\nWeakly Supervised Anomaly Detection. In ICIP, 4030‚Äì\n4034.\nZhang, Q.; and Yang, Y . 2021. ResT: An EfÔ¨Åcient Trans-\nformer for Visual Recognition. CoRR, abs/2105.13677.\nZhao, B.; Fei-Fei, L.; and Xing, E. P. 2011. Online detection\nof unusual events in videos via dynamic sparse coding. In\nCVPR, 3313‚Äì3320.\nZheng, H.; Zhang, Y .; Yang, L.; Wang, C.; and Chen, D. Z.\n2020. An Annotation SparsiÔ¨Åcation Strategy for 3D Medical\nImage Segmentation via Representative Selection and Self-\nTraining. In AAAI, 6925‚Äì6932.\nZhong, J.; Li, N.; Kong, W.; Liu, S.; Li, T. H.; and Li, G.\n2019. Graph Convolutional Label Noise Cleaner: Train a\nPlug-And-Play Action ClassiÔ¨Åer for Anomaly Detection. In\nCVPR, 1237‚Äì1246.\nZhu, Y .; and Newsam, S. D. 2019. Motion-Aware Feature\nfor Improved Video Anomaly Detection. In BMVC, 270.\n1403",
  "topic": "Snippet",
  "concepts": [
    {
      "name": "Snippet",
      "score": 0.9237394332885742
    },
    {
      "name": "Anomaly detection",
      "score": 0.7064931392669678
    },
    {
      "name": "Computer science",
      "score": 0.6803640723228455
    },
    {
      "name": "Anomaly (physics)",
      "score": 0.5645425319671631
    },
    {
      "name": "Inference",
      "score": 0.5314121842384338
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47007089853286743
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4650391936302185
    },
    {
      "name": "Transformer",
      "score": 0.43944570422172546
    },
    {
      "name": "Machine learning",
      "score": 0.3449641168117523
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3310394883155823
    },
    {
      "name": "Engineering",
      "score": 0.10443350672721863
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Condensed matter physics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I149594827",
      "name": "Xidian University",
      "country": "CN"
    }
  ]
}