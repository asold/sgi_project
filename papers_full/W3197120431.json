{
  "title": "Frustratingly Simple Pretraining Alternatives to Masked Language Modeling",
  "url": "https://openalex.org/W3197120431",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2787643087",
      "name": "Atsuki Yamaguchi",
      "affiliations": [
        "Hitachi (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A1974642922",
      "name": "George Chrysostomou",
      "affiliations": [
        "University of Sheffield"
      ]
    },
    {
      "id": "https://openalex.org/A3089003676",
      "name": "Katerina Margatina",
      "affiliations": [
        "University of Sheffield"
      ]
    },
    {
      "id": "https://openalex.org/A93365683",
      "name": "Nikolaos Aletras",
      "affiliations": [
        "University of Sheffield"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2995923603",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W3106061119",
    "https://openalex.org/W3104453885",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2963748441"
  ],
  "abstract": "Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocabulary. When pretraining, it is common to use alongside MLM other auxiliary objectives on the token or sequence level to improve downstream performance (e.g. next sentence prediction). However, no previous work so far has attempted in examining whether other simpler linguistically intuitive or not objectives can be used standalone as main pretraining objectives. In this paper, we explore five simple pretraining objectives based on token-level classification tasks as replacements of MLM. Empirical results on GLUE and SQUAD show that our proposed methods achieve comparable or better performance to MLM using a BERT-BASE architecture. We further validate our methods using smaller models, showing that pretraining a model with 41% of the BERT-BASE‚Äôs parameters, BERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3116‚Äì3125\nNovember 7‚Äì11, 2021.c‚Éù2021 Association for Computational Linguistics\n3116\nFrustratingly Simple Pretraining Alternatives\nto Masked Language Modeling\nAtsuki Yamaguchi1‚àó, George Chrysostomou2, Katerina Margatina2 and Nikolaos Aletras2\n1Research and Development Group, Hitachi, Ltd., Japan\n2Department of Computer Science, University of ShefÔ¨Åeld, United Kingdom\n1atsuki.yamaguchi1@gmail.com\n2{gchrysostomou1, k.margatina, n.aletras}@sheffield.ac.uk\nAbstract\nMasked language modeling ( MLM ), a self-\nsupervised pretraining objective, is widely\nused in natural language processing for learn-\ning text representations. MLM trains a model\nto predict a random sample of input tokens that\nhave been replaced by a [MASK] placeholder\nin a multi-class setting over the entire vocab-\nulary. When pretraining, it is common to use\nalongside MLM other auxiliary objectives on\nthe token or sequence level to improve down-\nstream performance (e.g. next sentence predic-\ntion). However, no previous work so far has\nattempted in examining whether other simpler\nlinguistically intuitive or not objectives can be\nused standalone as main pretraining objectives.\nIn this paper, we explore Ô¨Åve simple pretrain-\ning objectives based on token-level classiÔ¨Åca-\ntion tasks as replacements of MLM . Empirical\nresults on G LUE and SQ UAD show that our\nproposed methods achieve comparable or bet-\nter performance to MLM using a B ERT-BASE\narchitecture. We further validate our methods\nusing smaller models, showing that pretrain-\ning a model with 41% of the BERT-BASE ‚Äôs pa-\nrameters, BERT-MEDIUM results in only a 1%\ndrop in GLUE scores with our best objective.1\n1 Introduction\nMasked Language Modeling ( MLM ) pretrain-\ning (Devlin et al., 2019; Liu et al., 2019; Lan et al.,\n2020; Wang et al., 2020) is widely used in natu-\nral language processing (NLP) for self-supervised\nlearning of text representations. MLM trains a\nmodel (typically a neural network) to predict a par-\nticular token that has been replaced with a[MASK]\nplaceholder given its surrounding context. Devlin\net al. (2019) Ô¨Årst proposed MLM with an additional\nnext sentence prediction (NSP ) task (i.e. predicting\nwhether two segments appear consecutively in the\noriginal text) to train BERT.\n‚àóWork was done while at the University of ShefÔ¨Åeld.\n1Our code is publicly available here:https://github.\ncom/gucci-j/light-transformer-emnlp2021\nRecently several studies have extended MLM ,\nby masking a contiguous segment of the input in-\nstead of treating each token independently (Song\net al., 2019; Sun et al., 2020; Joshi et al., 2020).\nYang et al. (2019) reformulated MLM in XLNET , to\nmask out attention weights rather than input tokens,\nsuch that the input sequence is auto-regressively\ngenerated in a random order. ELECTRIC (Clark\net al., 2020a) addressed the expensive softmax issue\nof MLM using a binary classiÔ¨Åcation task, where\nthe task is to distinguish between words sampled\nfrom the original data distribution and a noise dis-\ntribution, using noise-contrastive estimation. In a\ndifferent direction, previous work has also devel-\noped methods to complement MLM for improving\ntext representation learning. Aroca-Ouellette and\nRudzicz (2020) have explored sentence and token-\nlevel auxiliary pretraining objectives, showing im-\nprovements over NSP . ALBERT (Lan et al., 2020)\ncomplemented MLM with a similar task that pre-\ndicts whether two sentences are in correct order\nor swapped. ELECTRA (Clark et al., 2020b) in-\ntroduced a two-stage token-level prediction task;\nusing a MLM generator to replace input tokens\nand subsequently a discriminator trying to predict\nwhether a token has been replaced or not.\nDespite these advances, simpler linguistically\nmotivated or not auxiliary objective tasks acting as\nprimary pre-training objectives substituting com-\npletely MLM have not been explored. Motivated by\nthis, we propose Ô¨Åve frustratingly simple pretrain-\ning tasks, showing that they result into models that\nperform competitively to MLM when pretrained for\nthe same duration (e.g. Ô¨Åve days) and Ô¨Åne-tuned in\ndownstream tasks in GLUE (Wang et al., 2019) and\nSQUAD (Rajpurkar et al., 2016) benchmarks.\nContributions: (1) To the best of our knowledge,\nthis study is the Ô¨Årst to investigate whether linguis-\ntically and non-linguistically intuitive tasks can\neffectively be used for pretraining (¬ß2). (2) We\nempirically demonstrate that our proposed objec-\n3117\nBERT\nOriginal: ‚ÄúI cooked dinner for my family.‚Äù\nShuffle (ùê∂=2)\ncookedIdinnerformyfamily\nshuffledshuffledoriginaloriginaloriginaloriginal\nBERTIplayeddinnerformythe\noriginalrandomoriginaloriginaloriginalrandom\nBERTIdinnermyforthe\noriginalrandomoriginalshuffledshuffledrandom\nplayed\nBERT[MASK]cooked[MASK]formyfamily\nstop wordcontentBERTI[MASK]dinnerformy[MASK]\nc\nf\nBERTIdinner[MASK]myfamilyfor[MASK]cooked\nRandom (ùê∂=2) Shuffle + Random (ùê∂=3)\nToken Type (ùê∂=4) First Char (ùê∂=29) MLM (ùê∂=50265)\nFigure 1: Overview of our Ô¨Åve frustratingly simple pretraining tasks along with a comparison toMLM . |C|denotes\nthe number of classes for each task.\ntives are often computationally cheaper and result\nin better or comparable performance toMLM across\ndifferent sized models (¬ß4).\n2 Pretraining Tasks\nOur methodology is based on two main hypotheses:\n(1) effective pretraining should be possible with\nstandalone token-level prediction methods that are\nlinguistically intuitive (e.g. predicting whether a\ntoken has been shufÔ¨Çed or not should help a model\nto learn semantic and syntactic relations between\nwords in a sequence); and (2) the deep architec-\nture of transformer models should allow them to\nlearn associations between input tokens even if the\npretraining objective is not linguistically intuitive\n(e.g. predicting the Ô¨Årst character of a masked to-\nken should not matter for the model to learn that\n‚Äòcat‚Äô and ‚Äòsat‚Äô usually appear in the same context).\nFigure 1 illustrates our Ô¨Åve linguistically and non-\nlinguistically intuitive pretraining tasks with a com-\nparison to MLM .\nShufÔ¨Çed Word Detection (S HUFFLE ): Moti-\nvated by the success of ELECTRA , our Ô¨Årst pre-\ntraining objective is a token-level binary classiÔ¨Å-\ncation task, consisting of identifying whether a\ntoken in the input sequence has been shufÔ¨Çed or\nnot. For each sample, we randomly shufÔ¨Çe 15% of\nthe tokens. This task is trained with the token-level\nbinary cross-entropy loss averaged over all input\ntokens (i.e. shufÔ¨Çed and original). The major dif-\nference between ours and ELECTRA is that we do\nnot rely on MLM to replace tokens. Our intuition is\nthat a model can acquire both syntactic and seman-\ntic knowledge by distinguishing shufÔ¨Çed tokens in\ncontext.\nRandom Word Detection (RANDOM ): We now\nconsider replacing tokens with out-of-sequence to-\nkens. For this purpose we propose RANDOM , a\npretraining objective which replaces 15% of tokens\nwith random ones from the vocabulary. Similar to\nshufÔ¨Çing tokens in the input, we expect that replac-\ning a token in the input with a random word from\nthe vocabulary ‚Äúforces‚Äù the model to acquire both\nsyntactic and semantic knowledge from the context\nto base its decision on whether it has been replaced\nor not.\nManipulated Word Detection (S HUFFLE +\nRANDOM ): For our third pretraining objective,\nwe seek to increase the task difÔ¨Åculty and subse-\nquently aim to improve the text representations\nlearned by the model. We therefore propose an\nextension of SHUFFLE and RANDOM , which is a\nthree-way token-level classiÔ¨Åcation task for predict-\ning whether a token is a shufÔ¨Çed token, a random\ntoken, or an original token. For each sample, we\nreplace 10% of tokens with shufÔ¨Çed ones from the\nsame sequence and another 10% of tokens with\nrandom ones from the vocabulary. This task can\nbe considered as a more complex one, because\nthe model must recognize the difference between\n3118\ntokens replaced in the same context and tokens\nreplaced outside of the context. For this task we\nuse the cross-entropy loss averaged over all input\ntokens.\nMasked Token Type ClassiÔ¨Åcation (T OKEN\nTYPE ): Our fourth objective is a four-way classi-\nÔ¨Åcation, aiming to predict whether a token is a stop\nword,2 a digit, a punctuation mark, or a content\nword. Therefore, the task can be seen as a simpli-\nÔ¨Åed version of POS tagging. We regard any tokens\nthat are not included in the Ô¨Årst three categories as\ncontent words. We mask 15% of tokens in each\nsample with a special [MASK] token and compute\nthe cross-entropy loss over the masked ones only\nnot to make the task trivial. For example, if we com-\npute the token-level loss over unmasked tokens, a\nmodel can easily recognize the four categories as\nwe only have a small number of non-content words\nin the vocabulary.\nMasked First Character Prediction (F IRST\nCHAR ): Finally to test our second hypothesis,\nwe propose a simpliÔ¨Åed version of the MLM task,\nwhere the model has to predict only the Ô¨Årst char-\nacter of each masked token instead of performing\na softmax over the entire vocabulary. We deÔ¨Åne\na 29-way classiÔ¨Åcation task, where 29 categories\ninclude the English alphabet (0 to 25), a digit (26),\na punctuation mark (27), or any other character\n(28). We mask 15% of tokens in each sample and\ncompute the cross-entropy loss over the masked\ntokens only.3\n3 Experimental Setup\nModels: We use BERT (Devlin et al., 2019)\n(BASE ) as our basis model by replacing the MLM\nand NSP objectives with one of our Ô¨Åve token-\nlevel pretraining tasks in all our experiments. We\nalso consider two smaller models from Turc et al.\n(2019), MEDIUM and SMALL , where we reduce the\nsize of the following components compared to the\nBASE model: (1) hidden layers; (2) hidden size;\n(3) feed-forward layer size; and (4) attention heads.\nMore speciÔ¨Åcally, MEDIUM has eight hidden layers\nand attention heads, while SMALL has four hid-\nden layers and eight attention heads. The size of\nfeed-forward and hidden layers for both models are\n2048 and 512, respectively.\n2We use the Natural Language Toolkit‚Äôs stop word list:\nhttps://www.nltk.org/.\n3For more details on the pretraining tasks, including equa-\ntions, see Appendix A.\nPretraining Data: We pretrain all models on the\nEnglish Wikipedia and BookCorpus (Zhu et al.,\n2015) (WikiBooks) using the datasets library.4\nImplementation Details: We pretrain and Ô¨Åne-\ntune our models with two NVIDIA Tesla V100\n(SXM2 - 32GB) with a batch size of 32 for BASE\nand 64 for MEDIUM and SMALL . We pretrain all\nour models for up to Ô¨Åve days each due to limited\naccess to computational resources and funds for\nrunning experiments. We save a checkpoint of\neach model every 24 hours.5\nEvaluation: We evaluate our approaches on\nGLUE (Wang et al., 2019) and SQUAD (Rajpurkar\net al., 2016) benchmarks. To measure performance\nin downstream tasks, we Ô¨Åne-tune all models for\nÔ¨Åve times each with a different random seed.\nBaseline: For comparison, we also pretrain mod-\nels with MLM . Following BERT and ROBERTA,\nwe mask 15% of tokens in each training in-\nstance, where 80% of the tokens are replaced with\n[MASK], 10% of the tokens are replaced with a\nrandom word and the rest of tokens remain un-\nchanged. We compute the cross-entropy loss aver-\naged over the masked tokens only.\n4 Results\nPerformance Comparison: Table 1 presents re-\nsults on GLUE and SQUAD, for our Ô¨Åve pretrain-\ning tasks compared to MLM across all model con-\nÔ¨Ågurations (¬ß3). We also include for reference\nour replicated downstream performance by Ô¨Åne-\ntuning BERT-BASE (MLM + NSP ) pretrained6 for\n40 epochs (Upper Bound).\nWe Ô¨Årst observe that our best objective, ShufÔ¨Çe\n+ Random, outperforms MLM on GLUE Avg. and\nSQUAD in the majority of model settings (BASE ,\nMEDIUM and SMALL ) with Ô¨Åve days pretraining.\nFor example in GLUE , we obtain an average of\n79.2 using ShufÔ¨Çe + Random with BERT-BASE\ncompared to 77.6 using MLM . This suggests that\nShufÔ¨Çe + Random can be a competitive alternative\nto MLM . Although ShufÔ¨Çe + Random does not out-\nperform MLM in SQUAD only with BERT-BASE , it\nremains competitive (83.5 compared to 84.8). The\n4https://github.com/huggingface/\ndatasets\n5For more details on model setup, implementation, and\ndata preprocessing, see Appendix C.\n6We used an already pretrained model provided by Wolf\net al. (2020).\n3119\nPretraining task MNLI QNLI QQP RTE SST MRPC CoLA STS GLUE Avg. SQuAD v1.1\nBASE - 40 Epochs Pretraining (Upper Bound)\nMLM + NSP 83.8 90.8 87.8 69.9 91.9 85.0 58.9 89.3 82.1 (0.4) 87.4 (0.6)\nOurs BASE - Five Days Pretraining\nMLM 80.1 88.2 85.9 61.4 89.6 81.6 49.6 84.7 77.6 (0.2) 84.8 (0.2)\nShufÔ¨Çe 73.3 81.6 82.1 57.5 82.4 79.1 33.4 79.9 71.2 (0.3) 74.8 (0.2)\nRandom 78.6 87.0 85.5 60.5 87.4 81.6 47.0 84.0 76.4 (0.2) 81.6 (0.4)\nShufÔ¨Çe + Random 78.6 87.7 86.1 65.1 87.8 87.0 54.9 86.7 79.2 (0.3) 83.5 (0.2)\nToken Type 75.1 84.2 83.9 56.8 86.7 75.5 40.3 77.4 72.5 (0.2) 78.6 (0.7)\nFirst Char 78.2 87.1 85.5 60.7 89.5 83.6 43.9 84.6 76.7 (0.5) 82.0 (0.1)\nMEDIUM - Five Days Pretraining\nMLM 78.7 85.3 85.4 61.7 89.9 80.6 43.1 84.5 76.1 (0.4) 81.8 (0.5)\nShufÔ¨Çe 77.3 86.4 85.3 64.0 87.9 83.4 53.8 84.1 77.8 (0.2) 81.3 (0.2)\nRandom 77.7 86.2 85.6 64.3 87.8 81.7 44.3 84.8 76.6 (0.3) 79.5 (0.1)\nShufÔ¨Çe + Random 78.3 87.0 85.7 63.3 87.8 85.9 52.4 85.4 78.2 (0.2) 81.8 (0.2)\nToken Type 76.0 84.7 84.4 59.7 87.6 81.4 45.8 80.7 75.0 (0.4) 79.8 (0.4)\nFirst Char 77.4 85.6 85.1 59.4 88.8 83.9 42.4 83.0 75.7 (0.3) 79.5 (0.2)\nSMALL - Five Days Pretraining\nMLM 76.2 84.2 84.8 57.5 88.6 82.9 36.3 83.0 74.2 (0.4) 77.1 (0.3)\nShufÔ¨Çe 74.9 84.0 84.2 59.8 86.4 80.0 47.1 81.1 74.7 (0.3) 76.1 (0.6)\nRandom 75.6 84.7 84.8 58.3 86.7 80.0 39.6 83.5 74.1 (0.4) 76.7 (0.5)\nShufÔ¨Çe + Random 76.9 85.7 85.3 60.3 87.1 81.8 41.7 84.6 75.4 (0.4) 77.5 (0.3)\nToken Type 73.2 83.0 83.7 58.8 86.4 77.1 37.1 77.8 72.1 (0.4) 74.2 (0.3)\nFirst Char 75.3 84.0 84.9 55.6 87.2 79.8 33.1 83.3 72.9 (0.8) 77.4 (0.2)\nTable 1: Results on GLUE and SQuAD dev sets with standard deviations over Ô¨Åve runs in parentheses. For\nMNLI, we report matched accuracy, for CoLA Matthews correlation, for STS-B Spearman correlation, for MRPC\naccuracy, for QQP and SQuAD F1 scores; accuracy for all other tasks. Bold values denote best performing across\neach dataset and Avg. for each model setting.\nremainder of our proposed tasks perform well, with\nFirst Char and Random being close to MLM across\nall model conÔ¨Ågurations conÔ¨Årming our two hy-\npotheses. Finally, ShufÔ¨Çe with BERT-BASE records\nthe lowest performance on GLUE (71.2 points), but\nit performs best when combined with Random (i.e.\nShufÔ¨Çe + Random).\nComputational EfÔ¨Åciency Comparison: Fig-\nure 2 presents the performance of our proposed\nmethods across (a) epochs and (b) days in GLUE\n(SQUAD results available in Appendix E). Results\nsuggest that our methods are, in general, more com-\nputationally efÔ¨Åcient compared to MLM . ShufÔ¨Çe +\nRandom trains for the largest number of epochs (i.e.\nfaster forward-backward passes) in Ô¨Åve days for the\nSMALL and MEDIUM settings, with Random outper-\nforming the rest in the BASE model setting (Figure\n2 (a)). If we take a closer look, we can also see that\nShufÔ¨Çe + Random obtains higher performance to\nMLM across all model conÔ¨Ågurations when training\nfor a similar number of epochs, suggesting that our\napproach is a more data efÔ¨Åcient task. Finally, we\ncan also assume that ShufÔ¨Çe + Random is more\nchallenging than MLM as in all settings it results in\nlower GLUE scores after the Ô¨Årst day of pretraining\n(Figure 2 (b)). However, with more iterations it is\nclear that it results in learning better text representa-\ntions and quickly outperforms MLM . For example,\nit achieves a performance of 78.2 compared to 76.1\nfor MLM with MEDIUM on the Ô¨Åfth day. Regarding\nthe remainder of our proposed objectives, we can\nsee that they perform comparably and sometimes\nbetter than the MLM under SMALL and MEDIUM\nmodel settings. However, MLM on average outper-\nforms them in the BASE setting where the models\nare more highly parameterized.\nLastly, we observe that for the majority ofGLUE\ntasks, we obtain better or comparable performance\nto MLM with a maximum of approximately three\nepochs of training with a BASE model. This demon-\nstrates that excessively long and computationally\ninefÔ¨Åcient pretraining strategies do not add a lot in\ndownstream performance.\n5 Discussion\nBased on our results, there are mainly two key\nelements that should be considered for designing\n3120\n0 1 2 3 4\n60\n66\n72\n78GLUE score\n82.1\nBase\n0 2 4 6 8\n(a) Number of epochs\n60\n66\n72\n78\nMedium\n0 2 4 6 8 10\n60\n66\n72\n78\nSmall\nPretraining task\nMLM\nShuffle\nRandom\nShuffle + Random\nToken Type\nFirst Char\nMLM + NSP\n(Pretrained for 40 epochs)\n1 2 3 4 5\n60\n66\n72\n78GLUE score\n82.1\n1 2 3 4 5\n(b) Number of pretraining days\n60\n66\n72\n78\n1 2 3 4 5\n60\n66\n72\n78\nFigure 2: Results on G LUE dev sets across (a) epochs and (b) days. Each point is a checkpoint pretrained for\n1 ‚â§n ‚â§5 day(s).\npretraining objectives.\nTask DifÔ¨Åculty: A pretraining task should be\nmoderately difÔ¨Åcult to learn in order to induce rich\ntext representations. For example, we can assume\nfrom the results that Token Type was somewhat\neasy for a model to learn as it is a four-way classi-\nÔ¨Åcation of identifying token properties. Besides, in\nour preliminary experiments, predicting whether a\nmasked token is a stop word or not (Masked Stop\nWord Detection) also did not exhibit competitive\ndownstream performance to MLM as the task is a\nlot simpler than Token Type.\nRobustness: A model should always learn use-\nful representations from ‚Äúevery‚Äù training sample\nto solve a pretraining task, regardless of the task\ndifÔ¨Åculty. For instance, Figures 3 to 5 in Appendix\nD demonstrate that ShufÔ¨Çe needs some time to start\nconverging across all model conÔ¨Ågurations, which\nmeans the model struggled to acquire useful repre-\nsentations at Ô¨Årst. In contrast, the loss for ShufÔ¨Çe +\nRandom consistently decreases. Because ShufÔ¨Çe +\nRandom is a multi-class classiÔ¨Åcation, unlike Shuf-\nÔ¨Çe or Random, we assume that it can convey richer\nsignals to the model and help stabilize pretraining.\nFinally, we can also assume thatMLM satisÔ¨Åes both\nelements as it is a multi-class setting over the entire\nvocabulary and its loss consistently decreases.\n6 Conclusions\nWe have proposed Ô¨Åve simple self-supervised pre-\ntraining objectives and tested their effectiveness\nagainst MLM under various model settings. We\nshow that our best performing, manipulated word\ndetection task, results in comparable performance\nto MLM in GLUE and SQUAD, whilst also being\nsigniÔ¨Åcantly faster in smaller model settings. We\nalso show that our tasks result in higher perfor-\nmance trained for the same number of epochs as\nMLM , suggesting higher data efÔ¨Åciency. For fu-\nture work, we are interested in exploring which\nhas the most impact in pretraining: the data or the\npretraining objective?\nAcknowledgments\nNA is supported by EPSRC grant EP/V055712/1,\npart of the European Commission CHIST-ERA\nprogramme, call 2019 XAI: Explainable Machine\nLearning-based ArtiÔ¨Åcial Intelligence. KM is sup-\nported by Amazon through the Alexa Fellowship\nscheme.\nReferences\nSt√©phane Aroca-Ouellette and Frank Rudzicz. 2020.\nOn Losses for Modern Language Models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4970‚Äì4981, Online. Association for Computa-\ntional Linguistics.\n3121\nKevin Clark, Minh-Thang Luong, Quoc Le, and\nChristopher D. Manning. 2020a. Pre-training trans-\nformers as energy-based cloze models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n285‚Äì294, Online. Association for Computational\nLinguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020b. Electra: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learn-\ning Representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171‚Äì4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64‚Äì77.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learn-\ning library. In Advances in Neural Information Pro-\ncessing Systems 32, pages 8024‚Äì8035. Curran Asso-\nciates, Inc.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383‚Äì2392, Austin,\nTexas. Association for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715‚Äì\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. Mass: Masked sequence to se-\nquence pre-training for language generation. In In-\nternational Conference on Machine Learning, pages\n5926‚Äì5936.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0:\nA continual pre-training framework for language un-\nderstanding. Proceedings of the AAAI Conference\non ArtiÔ¨Åcial Intelligence, 34(05):8968‚Äì8975.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations.\nWei Wang, Bin Bi, Ming Yan, Chen Wu, Jiangnan Xia,\nZuyi Bao, Liwei Peng, and Luo Si. 2020. Structbert:\nIncorporating language structures into pre-training\nfor deep language understanding. In International\nConference on Learning Representations.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38‚Äì45, Online. Asso-\nciation for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems , volume 32, pages\n5753‚Äì5763. Curran Associates, Inc.\nY . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Ur-\ntasun, A. Torralba, and S. Fidler. 2015. Aligning\nbooks and movies: Towards story-like visual expla-\nnations by watching movies and reading books. In\nProceedings of the 2015 IEEE International Confer-\nence on Computer Vision, pages 19‚Äì27.\n3122\nAppendices\nA Task Details\nHere, we detail our frustratingly simple pretraining\nobjectives, which are based on token-level classi-\nÔ¨Åcation tasks and can be used on any unlabeled\ncorpora without laborious preprocessing to obtain\nlabels for self-supervision.\nShufÔ¨Çed Word Detection (SHUFFLE ): Our Ô¨Årst\npretraining task is a token-level binary classiÔ¨Åca-\ntion task, which consists of identifying whether a\ntoken in the input sequence has been shufÔ¨Çed or\nnot. For each sample, we randomly shufÔ¨Çe 15% of\nthe tokens. This task is trained with the token-level\nbinary cross-entropy loss averaged over all input\ntokens:\nLshuÔ¨Ñe = ‚àí1\nN\nN‚àë\ni=1\nyi log p(xi)\n+ (1‚àíyi) log(1‚àíp(xi))\n(1)\nwhere N is the number of tokens in a sample, and\np(xi) represents the probability of the i-th input\ntoken xi predicted as shufÔ¨Çed by a model. yi is the\ncorresponding target label.\nThis task is motivated by the success of ELEC -\nTRA , whose pretraining task is to let a discrimi-\nnator to predict whether a given token is original\nor replaced (replaced word detection) in addition\nto MLM . The major difference between ours and\nELECTRA is that we do not rely on MLM , whereas\nELECTRA utilizes it as its generator. Here, our intu-\nition is that a model should acquire both syntactic\nand semantic knowledge to detect shufÔ¨Çed tokens\nin contexts.\nRandom Word Detection (RANDOM ): We also\nconsider replacing tokens with out-of-sequence to-\nkens. For this purpose we propose RANDOM , a\npretraining objective which replaces 15% of tokens\nwith random ones from the vocabulary. Similar to\nshufÔ¨Çing tokens in the input, we expect that replac-\ning a token in the input with a random word from\nthe vocabulary ‚Äúforces‚Äù the model to acquire both\nsyntactic and semantic knowledge from the context\nto base its decision on whether it has been replaced\nor not. This task is trained with the token-level\nbinary cross-entropy loss averaged over all input\ntokens (Eq. (1)).\nManipulated Word Detection (S HUFFLE +\nRANDOM ): Our third task is a three-way token-\nlevel classiÔ¨Åcation of whether a token is a shufÔ¨Çed\ntoken, a random token, or an original token. For\neach sample, we replace 10% of tokens with shuf-\nÔ¨Çed ones and another 10% of tokens with random\nones. This task is an extension of SHUFFLE and\nRANDOM and can be regarded as a more complex\none because the model must recognize the differ-\nence between a token replaced in the same context\nand a token replaced outside of the context. For\nthis task we employ the cross-entropy loss averaged\nover all input tokens:\nLmanipulated = ‚àí1\nN\nN‚àë\ni=1\n3‚àë\nj=1\nyij log pij(xi) (2)\nwhere pij(xi) represents the probability of the i-\nth input token xi predicted as shufÔ¨Çed (j = 1),\nrandomized (j = 2), or original (j = 3) by a\nmodel. yij is the corresponding target label.\nMasked Token Type ClassiÔ¨Åcation (T OKEN\nTYPE ): Our fourth task is a four-way classiÔ¨Å-\ncation task that identiÔ¨Åes whether a token is a stop\nword7, a digit, a punctuation mark, or a content\nword. We regard any tokens that are not included\nin the Ô¨Årst three categories as content words. We\nmask 15% of tokens in each sample with a special\n[MASK] token and compute the cross-entropy loss\nover the masked ones only not to make the task\ntrivial: if we compute the token-level loss, includ-\ning unmasked tokens, a model can easily recognize\nthe four categories of tokens as we have a small\nnumber of tokens for non-content words. In this\ntask, a model should be able to identify the distinc-\ntion between different types of tokens; therefore,\nthe task can be seen as a simpliÔ¨Åed version of POS\ntagging.\nMasked First Character Prediction (F IRST\nCHAR ): Our last task is a 29-way classiÔ¨Åcation\ntask, where a model needs to predict the Ô¨Årst char-\nacter of a masked token. The 29 categories include\nthe English alphabet (0 to 25), a digit (26), a punc-\ntuation mark (27), or any other character (28). We\nmask 15% of tokens in each sample and compute\nthe cross-entropy loss over the masked ones only.\nThis task can be seen as a simpliÔ¨Åed version of\nMLM as the model just need to predict the Ô¨Årst\n7A stop word category is based on the Natural Language\nToolkit‚Äôs stop word list:https://www.nltk.org/.\n3123\ncharacter of each masked token. Besides, it is also\nsimilar to masked character-level language model-\ning, in that the output of both tasks is in characters.\nB Non-linguistically Intuitive Task\nAs we have described in Section 2, a non-\nlinguistically intuitive task should not be ‚Äúexplic-\nitly‚Äù related to an input sequence to solve, unlike\nlinguistically intuitive tasks, such as ShufÔ¨Çe and\nRandom. For example, predicting the Ô¨Årst charac-\nter of a masked token should not matter for a model\nto learn that ‚Äòcat‚Äô and ‚Äòsat‚Äô usually appear in the\nsame context. However, because accurately predict-\ning the Ô¨Årst character requires the model to guess\nits whole word ‚Äúimplicitly‚Äù given its surrounding\ntokens, the Ô¨Årst character of each masked token\nshould be related to the context. The deep archi-\ntecture of transformer-based models should allow\nthem to learn such ‚Äúimplicit‚Äù associations between\ninput tokens by solving the non-linguistically in-\ntuitive task, which leads to helping them to learn\nsyntactic and semantic relations between tokens.\nC Experimental Setup\nC.1 Model Architecture\nFor all our experiments, we use BERT (Devlin\net al., 2019) as our basis model by replacing the\nMLM and NSP objectives with one of our Ô¨Åve\ntoken-level pretraining tasks. More speciÔ¨Åcally, we\nemploy BERT-BASE (12 hidden layers and atten-\ntion heads, Dimhidden = 768, Dimintermediate =\n3072, Total parameters = 125 M) ( BASE ),\nMEDIUM (eight hidden layers and attention heads,\nDimhidden = 512 , Dimintermediate = 2048 ,\nTotal parameters = 51.5M), and SMALL (four hid-\nden layers and eight attention heads, Dimhidden =\n512, Dimintermediate = 2048Total parameters =\n38.9M).\nC.2 Data\nFollowing Devlin et al. (2019), we use the English\nWikipedia and BookCorpus (Zhu et al., 2015) data\n(WikiBooks) downloaded from the datasets\nlibrary8. We remove headers for the English\nWikipedia and extract training samples with a max-\nimum length of 512. For the BookCorpus, we\nconcatenate sentences such that the total number of\ntokens is less than 512. For the English Wikipedia,\nwe extract one sample from articles whose length\n8https://github.com/huggingface/\ndatasets\nis less than 512. We tokenize text using byte-level\nByte-Pair-Encoding (Sennrich et al., 2016). The\nresulting corpus consists of 8.1 million samples\nand 2.7 billion tokens in total.\nC.3 Implementation Details\nWe implement our models using PyTorch\n(Paszke et al., 2019) and the transformers\nlibrary (Wolf et al., 2020). We pretrain\nour models with two NVIDIA Tesla V100\n(SXM2 - 32GB) and use one for Ô¨Åne-\ntuning. Our code is publicly available on\nGitHub: https://github.com/gucci-j/\nlight-transformer-emnlp2021.\nPretraining: We set the batch size to 32 for the\nBASE models and 64 for the MEDIUM and SMALL\nmodels. We pretrain models for Ô¨Åve days and op-\ntimized them with an Adam optimizer (Kingma\nand Ba, 2014). We apply automatic mixed pre-\ncision and distributed training during pretraining.\nNote that we generate labels dynamically during\npretraining.\nFinetuning: We Ô¨Åne-tune models for up to 10\nand 20 epochs with early stopping for SQUAD and\nGLUE , respectively. To minimize the effect of ran-\ndom seeds, we test Ô¨Åve different random seeds for\neach task. We omitted the problematic WNLI task\nfor GLUE , following Aroca-Ouellette and Rudzicz\n(2020).\nC.4 Hyperparameter Details\nAs explained in Section 3, we entirely followed\nthe BERT architecture and only modiÔ¨Åed its output\nlayer depending on the task employed. Table 2\nshows the hyperparameter settings for pretraining\nand Ô¨Åne-tuning. Note that we utilized neither any\nparameter sharing tricks nor any techniques that\ndid not appear in Devlin et al. (2019).\nD Pretraining Behavior\nFigures 3, 4 and 5 show the loss curves for our\npretraining tasks in each model setting: BASE ,\nMEDIUM and SMALL .\nE Performance in SQ UAD\nFigure 6 demonstrates the performance of our pro-\nposed methods across (a) epochs and (b) days in\nSQUAD.\n3124\nHyperparameter Pretraining Fine-tuning\nMaximum train epochs10 epochs forBASEandMEDIUM Up to 20 epochs for GLUE\n15 epochs forSMALL Up to 10 epochs for SQuAD\nBatch size (per GPU) 16 forBASE 32 for GLUE\n32 forHALFandQUARTER 16 for SQuAD\nAdamœµ 1e-8\nAdamŒ≤1 0.9\nAdamŒ≤2 0.999\nSequence length 512 128 for GLUE\n384 for SQuAD\nPeak learning rate\nBASE: 1e-4 for MLM and Token Type, 1e-5 for ShufÔ¨Çe.\n3e-55e-5 for First Char, Random and ShufÔ¨Çe + Random.\nMEDIUM& SMALL: 1e-4 for MLM, Token Type and First Char.\n5e-5 for ShufÔ¨Çe, Random and ShufÔ¨Çe + Random.\nWarmup steps 10000 First 6% of steps\nWeight decay 0.01 0\nAttention Dropout 0.1\nDropout 0.1\nEarly stopping criterion GLUE: No improvements over 5% of steps.\nSQuAD: No improvements over 2.5% of steps.\nTable 2: Hyperparameters in our experiments. If not shown, the hyperparameters for Ô¨Åne-tuning are the same as\nthe pretraining ones.\n0 1 2 3\nEpochs\n2\n4\n6\n8\n10Loss\nPretraining task\nMLM\nFirst Char\n0 1 2 3 4\nEpochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2Loss\nPretraining task\nShuffle\nRandom\nShuffle + Random\nToken Type\nFigure 3: The loss curves for BERT-BASE models. Each √ódenotes a checkpoint pretrained for 1 ‚â§n ‚â§5 day(s).\n0 1 2 3 4\nEpochs\n2\n4\n6\n8\n10Loss\nPretraining task\nMLM\nFirst Char\n0 2 4 6 8\nEpochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2Loss\nPretraining task\nShuffle\nRandom\nShuffle + Random\nToken Type\nFigure 4: The loss curves for B ERT-MEDIUM models. Each √ódenotes a checkpoint pretrained for 1 ‚â§n ‚â§5\nday(s).\n3125\n0 2 4 6\nEpochs\n2\n4\n6\n8\n10Loss\nPretraining task\nMLM\nFirst Char\n0.0 2.5 5.0 7.5 10.0 12.5\nEpochs\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4Loss\nPretraining task\nShuffle\nRandom\nShuffle + Random\nToken Type\nFigure 5: The loss curves for BERT-SMALL models. Each √ódenotes a checkpoint pretrained for1 ‚â§n ‚â§5 day(s).\n0 1 2 3 4\n50\n60\n70\n80\n90SQuAD score\n87.4\nBase\n0 2 4 6 8\n(a) Number of epochs\n50\n60\n70\n80\n90\nMedium\n0 2 4 6 8 10\n50\n60\n70\n80\n90\nSmall\nPretraining task\nMLM\nShuffle\nRandom\nShuffle + Random\nToken Type\nFirst Char\nMLM + NSP\n(Pretrained for 40 epochs)\n1 2 3 4 5\n50\n60\n70\n80\n90SQuAD score\n87.4\n1 2 3 4 5\n(b) Number of pretraining days\n50\n60\n70\n80\n90\n1 2 3 4 5\n50\n60\n70\n80\n90\nFigure 6: Results on SQ UAD dev sets across (a) epochs and (b) days. Each point is a checkpoint pretrained for\n1 ‚â§n ‚â§5 day(s).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.79227614402771
    },
    {
      "name": "Vocabulary",
      "score": 0.6934569478034973
    },
    {
      "name": "Language model",
      "score": 0.665686309337616
    },
    {
      "name": "Security token",
      "score": 0.6556792855262756
    },
    {
      "name": "Sequence labeling",
      "score": 0.6182135939598083
    },
    {
      "name": "Natural language processing",
      "score": 0.5626766681671143
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5548765659332275
    },
    {
      "name": "Sentence",
      "score": 0.5270535945892334
    },
    {
      "name": "Class (philosophy)",
      "score": 0.42366528511047363
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.41515839099884033
    },
    {
      "name": "Machine learning",
      "score": 0.38873201608657837
    },
    {
      "name": "Task (project management)",
      "score": 0.1539348065853119
    },
    {
      "name": "Linguistics",
      "score": 0.10310325026512146
    },
    {
      "name": "Engineering",
      "score": 0.08300849795341492
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I65143321",
      "name": "Hitachi (Japan)",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I91136226",
      "name": "University of Sheffield",
      "country": "GB"
    }
  ],
  "cited_by": 18
}