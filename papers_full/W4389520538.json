{
  "title": "Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting",
  "url": "https://openalex.org/W4389520538",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2235093108",
      "name": "Haoyang Huang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2133263866",
      "name": "Tianyi Tang",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2131422403",
      "name": "Dongdong Zhang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2098250721",
      "name": "Zhao Xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101052665",
      "name": "Ting Song",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2100287684",
      "name": "Yan Xia",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2171151462",
      "name": "Furu Wei",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4321472057",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3173360659",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3175746962",
    "https://openalex.org/W4205096996",
    "https://openalex.org/W3099771192",
    "https://openalex.org/W3045958725",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4320167623",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4389519817",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W3171975879",
    "https://openalex.org/W4385565879",
    "https://openalex.org/W4364387438",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W4322760437",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4285077564",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W4285242045",
    "https://openalex.org/W4389524534",
    "https://openalex.org/W3156064004",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385572225",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W3118106810"
  ],
  "abstract": "Large language models (LLMs) demonstrate impressive multilingual capability, but their performance varies substantially across different languages. In this work, we introduce a simple yet effective method, called cross-lingual-thought prompting (XLT), to systematically improve the multilingual capability of LLMs. Specifically, XLT is a generic template prompt that stimulates cross-lingual and logical reasoning skills to enhance task performance across languages. We conduct comprehensive evaluations on 7 typical benchmarks related to reasoning, understanding, and generation tasks, covering both high-resource and low-resource languages. Experimental results show that XLT not only remarkably enhances the performance of various multilingual tasks but also significantly reduces the gap between the average performance and the best performance of each task in different languages. Notably, XLT brings over 10 points of average improvement in arithmetic reasoning and open-domain question-answering tasks.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12365–12394\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nNot All Languages Are Created Equal in LLMs:\nImproving Multilingual Capability by Cross-Lingual-Thought Prompting\nHaoyang Huang1∗, Tianyi Tang 2∗, Dongdong Zhang 1†, Wayne Xin Zhao 2\nTing Song1, Yan Xia1, Furu Wei1\n1Microsoft Research Asia, China\n2Gaoling School of Artificial Intelligence, Renmin University of China\nhttps://github.com/microsoft/unilm\n/uni00000044/uni00000027/uni0000005e/uni00000044\n/uni00000079/uni00000012/uni0000004b/uni00000057/uni00000004\n/uni00000079/uni00000045/uni0000003e/uni0000002f\n/uni00000057/uni00000004/uni00000074/uni0000005e/uni00000372/uni00000079/uni00000044/uni0000003c/uni00000059/uni00000004\n/uni00000079/uni0000003e/uni00000372/uni0000005e/uni000001b5/uni00000175/uni0000038e\n/uni00000026/uni0000003e/uni0000004b/uni0000005a/uni0000001c/uni0000005e/uni0000038e\n/uni000003f5\n/uni000003ed/uni000003ef\n/uni000003ed/uni000003f3\n/uni000003ee/uni000003ed\n/uni000003f2/uni000003f2\n/uni000003f2/uni000003f4\n/uni000003f3/uni000003ec\n/uni000003f3/uni000003ee\n/uni000003f0/uni000003f5/uni000003f1/uni000003ef/uni000003f1/uni000003f3/uni000003f2/uni000003ed/uni000003f0/uni000003f3\n/uni000003f1/uni000003ec\n/uni000003f1/uni000003ef\n/uni000003f1/uni000003f2\n/uni000003ee/uni000003f2\n/uni000003ef/uni000003ec\n/uni000003ef/uni000003f0\n/uni000003ef/uni000003f4\n/uni000003ee/uni000003ee/uni000003ee/uni000003ef/uni000003ee/uni000003f0/uni000003ee/uni000003f1\n/uni000003ed/uni000003f0\n/uni000003ed/uni000003f1\n/uni000003ed/uni000003f2\n/uni000003ed/uni000003f3\n(a)\n80.8\n86.6\n84.6\n92.6\n82.1\n50\n60\n70\n80\n90\n100\nMGSM XCOPA XNLI PAWS-X MKQA\n72.4\n81.9 82.6\n92.2\n69.6\n50\n60\n70\n80\n90\n100\nMGSM XCOPA XNLI PAWS-X MKQA\n(%) (b)\nFigure 1: Comparing the effectiveness of the Cross-Lingual-Thought prompt versus the baseline basic prompt\non 7 representative benchmarks covering 27 languages: (a) Enhancing the multilingual capability of\ntext-davinci-003 under the zero-shot learning, and (b) Narrowing the gap between the average performance and\nthe best performance of each task in different languages.\nAbstract\nLarge language models (LLMs) demonstrate\nimpressive multilingual capability, but their per-\nformance varies substantially across different\nlanguages. In this work, we introduce a sim-\nple yet effective method, called cross-lingual-\nthought prompting (XLT), to systematically\nimprove the multilingual capability of LLMs.\nSpecifically, XLT is a generic template prompt\nthat stimulates cross-lingual and logical reason-\ning skills to enhance task performance across\nlanguages. We conduct comprehensive evalu-\nations on 7 typical benchmarks related to rea-\nsoning, understanding, and generation tasks,\ncovering both high-resource and low-resource\nlanguages. Experimental results show that\nXLT not only remarkably enhances the perfor-\nmance of various multilingual tasks but also\nsignificantly reduces the gap between the av-\nerage performance and the best performance\nof each task in different languages. Notably,\nXLT brings over 10 points of average improve-\nment in arithmetic reasoning and open-domain\nquestion-answering tasks.\n∗Equal contribution. †Corresponding author.\n1 Introduction\nLarge language models (LLMs) demonstrate im-\npressive multilingual capability in a wide range\nof natural language processing tasks, including\nlanguage generation, knowledge utilization, and\ncomplex reasoning (Zhao et al., 2023). Their per-\nformance in downstream tasks has been shown\nto reach or even surpass human-level perfor-\nmance (Brown et al., 2020; Chowdhery et al., 2022;\nScao et al., 2022). The capabilities of LLMs stem\nfrom the extensive volume of training data they\nleveraged (Kaplan et al., 2020). The training data\nfor current models is primarily dominated by the\nEnglish language corpus, but it also encompasses\ndata from other languages, as described in GPT-\n3 (Brown et al., 2020), PaLM (Chowdhery et al.,\n2022), and BLOOM (Scao et al., 2022), etc.\nThere are over 7,000 languages worldwide, with\nthe vast majority being low-resource or extremely\nlow-resource languages (Forkel et al., 2022). De-\nspite the latest GPT-4 model (OpenAI, 2023)\ndemonstrating some generalization capabilities in\n12365\nLLM\nInput\nR equest: 詹姆斯决定每\n周跑 3 次 3 段冲刺，每\n段冲刺跑 60 米。他每\n周一共跑多少米?\nR equest: J ames decides t o run 3 sets o f 60-met er sprints thr ee times a w eek . How many met er s does he run in t otal \neach w eek?\nOutput\n1. J ames runs 3 sets o f 60-met er sprints, which means he runs 60 x 3 = 180 met er s per sprint.\n2. J ames runs 3 sets o f 180 met er s per w eek , which means he runs 180 x 3 = 540 met er s per w eek .\nAnsw er : J ames runs a t otal o f met er s per w eek .  540 \nS t ep-by-st ep answ er :\n540\nR equest: 詹姆斯决定每周跑 3 次 3 段冲刺，每段冲刺跑 60 米。\n他每周一共跑多少米?\nXL T\nI want y ou t o act as an arithmetic r easoning exper t for Chinese .\nY ou should r et ell the r equest in English.\nY ou should do st ep-by-st ep answ er t o obtain a number answ er .\nY ou should st ep-by-st ep answ er the r equest.\nY ou should t ell me the answ er in this for mat 'Answ er:'.\nr equest\narithmetic r easoning Chinese\nansw er Answ er\ndo st ep-by-st ep answ er t o obtain a number answ er\nFigure 2: Overview of our method. Given a request, its associated meta information is filled into the placeholders of\nthe XLT template to form the language-independent prompt, which is fed to the LLM to enhance the generation of\nresponses in the desired format.\nmultilingual tasks as evaluated on the MMLU\nbenchmark (Hendrycks et al., 2021), it is still the\ncase that LLMs do not have equal capability to han-\ndle all languages, leading to imbalanced capability\nacross different languages. Furthermore, several\nevaluation results (Bang et al., 2023; Jiao et al.,\n2023; Hendy et al., 2023; Zhu et al., 2023) indicate\nthat large models struggle with understanding and\ngenerating non-English languages, particularly in\nlow-resource or extremely low-resource languages.\nTherefore, to democratize language intelligence\nand minimize performance gaps in different lan-\nguage, it is essential and meaningful to stimulate\nand enhance the multilingual capability of models\nin non-English and low-resource languages.\nIntuitively, LLMs can improve multilingual ca-\npability by augmenting data (Lin et al., 2022) or\nfine-tuning models (Chen et al., 2021, 2022), but\nboth are computationally expensive. Alternatively,\nin-context learning with prompts can also boost per-\nformance (Brown et al., 2020; Ahuja et al., 2023;\nWei et al., 2022c) but is limited to monolingual\ntasks (Sanh et al., 2022).\nThis work explores a universal in-context learn-\ning approach to enhance the multilingual capabil-\nity of LLMs. We introduce a simple yet effective\nmethod, called cross-lingual-thought prompting\n(XLT), to enable models to handle various natu-\nral language processing tasks across different tar-\nget languages. Our method employs a generic and\nlanguage-independent prompt, which eliminates\nthe need to update model parameters. Depend-\ning on the task input type, cross-lingual-thought\nprompting guides the large language model to as-\nsume the role of an expert in a specific language\nfor a particular task. Given its predefined meta\ninformation, XLT directs LLMs to respond logi-\ncally through a process involving problem under-\nstanding, cross-lingual thinking, task analysis, task\nexecution, and output formatting. During this pro-\ncess, our method is designed to stimulate models’\ncross-lingual and logical reasoning skills, enabling\nthem to respond to input requests regardless of the\nlanguage. For enhanced performance, few-shot\nlearning can also be employed with our method by\nproviding an LLM-generated response output as a\ndemonstration using cross-lingual-thought prompt-\ning zero-shot learning.\nWe conduct a comprehensive evaluation to ver-\nify the effectiveness of XLT across seven repre-\nsentative multilingual benchmarks of natural lan-\nguage reasoning, understanding, and generation\ntasks. Each benchmark includes multilingual data\ncovering both high-resource and low-resource lan-\nguages. The experimental results demonstrate that\nour method can significantly improve the perfor-\n12366\nI want you to act as a task_name expert for task_language .\ntask_input\nYou should retell/repeat the input_tag in English.\nYou should task_goal .\nYou should step-by-step answer the request.\nYou should tell me the output_type ( output_constraint ) in this format ' output_type :'.\nFigure 3: Illustration of XLT template. Referring to Figure 2 and Appendix for instantiated examples.\nmance of all benchmarks across languages under\nboth zero-shot and few-shot learning settings. No-\ntably, XLT achieves an average gain of over 10\npoints on the MGSM and MKQA benchmarks. Fur-\nthermore, we observe that our prompting method\nsignificantly reduces the gap between the average\nperformance and the best performance of each task\nin different languages, indicating its potential to\ndemocratize language intelligence.\n2 Cross-Lingual-Thought Prompting\nAlthough LLMs are capable of accepting any input\nand generating responses, users typically structure\ntheir requests in the form of prompts to elicit the de-\nsired output. The design of these prompts is crucial\nfor achieving optimal performance on downstream\ntasks, as LLMs are sensitive to the format of the\nprompts chosen (Zhao et al., 2021). Through a pro-\ncess called instruction tuning (Wei et al., 2022a),\nmodels can develop the ability to follow natural lan-\nguage instructions (Wei et al., 2022b), which can\nreduce their sensitivity to prompt engineering (Wei\net al., 2022a). In accordance with the guidelines of\nthe OpenAI cookbook1, we propose a cross-lingual\nthought prompting template, denoted as the XLT\ntemplate. This generic template allows LLMs to\nrespond to requests with cross-lingual thought and\nsupports a wide range of multilingual tasks.\nFigure 3 displays the XLT template, with the col-\nored sections representing placeholders. Figure 2\nshowcases an example of instantiated prompt for\nthe Chinese request. The following section will\nexplain the details of constructing XLT.\n2.1 Construction of XLT\nThe XLT template is designed to emulate the pro-\ncess humans employ when handling multilingual\ntasks. Our template is written in English, as En-\nglish is the dominant language during LLM pre-\n1https://github.com/openai/openai-cookbook\ntraining, and existing research indicates that En-\nglish prompting is more effective for multilingual\ntasks (Shi et al., 2023). In contrast to the vanilla\nprompt that only includes a task description, our\nXLT template aims to elicit multilingual capabil-\nity through cross-lingual thoughts. This template\ncomprises six logical instructions in sequence. To\ncomplete the template, only seven placeholders\nneed to be filled in based on intrinsic knowledge of\nthe task and the request, as depicted in igure 3.\nRole Assigning . First, the model receives a role\ndefinition that helps establish the model’s behavior.\nThis concept is akin to the system role of Chat-\nGPT2. To achieve this, we simply need to fulfill\nthe task name with a known category (such as\ncommonsense reasoning or paraphrase identifica-\ntion), along with the language of the task in the\ntask language field.\nTask Inputting . Second, we explicitly append\nthe request as the task input . The request is ba-\nsically structured in terms of the task type so as\nto make sure the model can comprehend it. For\nexample, in the natural language inference task, the\ntwo sentence inputs are specified with “premise”\nand “hypothesis”, respectively.\nCross-lingual Thinking . We encourage the\nmodel to engage in cross-lingual thought by\nrephrasing the requested content in English, which\nis the dominant language used as a pivot lan-\nguage by Shi et al. (2023) and Ahuja et al. (2023).\nRephrasing the requested content enclosed in the\ninput tag helps the model better understand the\nrequest in its native language and knowledge. Our\nobservations suggest that using keywords such as\n\"retell\" or \"repeat\" while rephrasing the content\nmay result in better performance in practice.\n2https://platform.openai.com/docs/guides/chat/\nintroduction\n12367\nTask Analyzing . After rephrasing the task input,\nwe need to complete the task in task goal . This\nstep is comparable to the task description used in\nconventional prompting methods. In practice, we\ncan get the task information from the literature or\nseek assistance from ChatGPT to generate effective\nprompts for solving the task (Jiao et al., 2023).\nCoT Task Solving . We then ask the model to\nfollow the instructions and complete the task step\nby step. Since LLMs exhibit a strong ability to\nmaintain a chain-of-thought (Wei et al., 2022c),\nwe carefully design instructions to guide the model,\nwith the hope that it will respond to our instructions\nin a step-by-step manner and utilize the intermedi-\nate outputs to aid in solving the task.\nOutput Formatting . Finally, we should regular-\nize the output format of the model to obtain the ex-\nact answer. LLMs are utilized in a zero- or few-shot\nmanner, and they tend to generate texts that may\nnot conform to the format of the target answer. For-\ntunately, LLMs possess a strong ability to follow\ninstructions, and we can define the output format in\nterms of output type and output constraint . The\noutput type can be a number, index, or text, while\nthe output constraint is optional and determined\nbased on the task requirements. Output constraint\nmay include length limitations, language specifica-\ntions, and other relevant factors.\n2.2 XLT for Few-shot Learning\nThe above construction of XLT can be directly fed\nto LLMs to yield outputs, which is performed in\nthe zero-shot learning setting. In addition, we also\nexplore incorporating demonstrations into XLT to\nenable few-shot learning. Different from previous\nwork that just appends model outputs to the corre-\nsponding request (Shi et al., 2023) or utilizes a ver-\nbalizer to format the output, our method constructs\nthe demonstrations with better formatted model\noutputs from a step-by-step processing-based XLT.\nAs illustrated in Figure 4, we first sample a few ex-\namples from the development set and incorporate\nthe requested parts into XLT. The zero-shot learn-\ning is performed over LLM to collect responses\nthat are further aligned with those of the samples.\nOnly response-aligned requests are assembled with\nthe corresponding model responses to form final\ndemonstrations for few-shot learning. In this way,\nthe demonstrations are constructed with rich log-\nical knowledge via XLT, which will cater to the\nXLT-based generation of new requests. In practice,\nRequest\nXLT LLM ResponseDev \ndata\nDemonstrations\nResponse\nAligned?\nFigure 4: Construction process for few-shot learning.\nwe can also correct or design the demonstrations\nfor better alignment with the instruction logic.\n3 Experiments\nTo comprehensively verify the effectiveness of our\nmethod on language-independent generality, we\nevaluate our XLT template on different LLMs cov-\nering various natural language processing tasks in\nmultiple languages.\n3.1 Experimental Setups\n3.1.1 Tasks and Benchmarks\nWe conduct evaluations on seven typical bench-\nmarks related to reasoning, understanding, and gen-\neration tasks that can represent different capabil-\nities of LLMs, encompassing both high-resource\nand low-resource languages. These benchmarks\ncover 27 different languages, including English\n(en), German (de), Russian (ru), French (fr), Chi-\nnese Simplified (zh), Spanish (es), Japanese (ja),\nItalian (it), Vietnamese (vi), Turkish (tr), Indone-\nsian (id), Swahili (sw), Arabic (ar), Korean (ko),\nGreek (el), Thai (th), Bulgarian (bg), Hindi (hi),\nEstonian (et), Bengali (bn), Tamil (ta), Galician\n(gl), Urdu (ur), Telugu (te), Javanese (jv), Haitian\nCreole (ht), and Southern Quechua (qu). In terms\nof the language distribution statistics in the Com-\nmon Crawl Monthly Archives3 and the language\nperformance of LLMs (Shi et al., 2023; Ahuja\net al., 2023), we have arranged them in the or-\nder of language frequency from high-resource to\nlow-resource. In particular, the frequency of some\nunderrepresented languages is even less than 0.1%\n(e.g., bn, ta, gl, ur, te, jv, ht, qu).\n• Reasoning tasks\n– Arithmetic Reasoning. The MGSM (Shi et al.,\n2023) benchmark contains grade school mathe-\n3https://commoncrawl.github.io/\ncc-crawl-statistics/plots/languages\n12368\nmatical problems and asks the model to calculate\nthe correct answer. It covers 11 languages, and\nwe utilize the accuracy score for evaluation.\n– Commonsense Reasoning. The XCOPA (Ponti\net al., 2020) benchmark contains one premise and\ntwo choices. It asks the model to choose which\none is the result or cause of the premise. It covers\n11 languages from 11 diverse families, and we\nutilize the accuracy score for evaluation.\n• Understanding tasks\n– Natural Language Inference. The XNLI (Con-\nneau et al., 2018) benchmark contains one\npremise and one hypothesis and requires the\nmodel to determine whether the hypothesis is\nentailed, contradicted, or neutral conditioned on\nthe premise. It covers 15 languages, and we uti-\nlize the accuracy score for evaluation.\n– Paraphrase Identification. The PAWS-X (Yang\net al., 2019) benchmark contains two sentences\nand requires the model to judge whether they\nparaphrase each other or not. It covers 7 lan-\nguages, and we utilize the accuracy score for\nevaluation.\n• Generation tasks\n– Question Answering. The MKQA (Longpre\net al., 2021) benchmark contains an open-domain\nquestion and asks the model to predict a short\nanswer. Since it has unanswerable questions or\nlong questions that do not have precise answers,\nwe remove these questions during evaluation. It\ncovers 25 languages, and we choose a subset of\n10 languages, including de, en, es, fr, ja, ru, th, tr,\nvi, and zh. We utilize the token overlap F1 score\nfor evaluation.\n– Summarization. The XL-Sum* (Hasan et al.,\n2021) (250 test samples randomly sampled from\nXL-Sum per language) benchmark contains a\nlong news article and wants the model to summa-\nrize it into a short text. It covers 44 languages,\nand we choose a subset of 6 languages, including\nen, es, fr, tr, vi, and zh. We utilize the ROUGE-1\nscore (Lin, 2004) for evaluation.\n– Machine Translation. The FLORES* (Costa-\njussà et al., 2022) (200 test samples randomly\nsampled from FLORES-200 per language) bench-\nmark contains parallel text from Wikimedia\nprojects for 204 languages, yielding over 40,000\ntranslation directions. We choose a subset of 12\ndirections, including high resource to high re-\nsource translation (i.e., zh↔ ru and de↔vi), high\nresource to low resource translation (i.e., zh↔ th\nand zh↔jv), and low resource to low resource\ntranslation (i.e., th↔gl and jv↔th). We utilize\nthe SacreBLEU score (Papineni et al., 2002; Post,\n2018) for evaluation.\nAmong these benchmarks, MGSM, XCOPA,\nXNLI, PAWS-X, and MKQA are parallel,i.e., the\ninstances are semantics-equivalent across each lan-\nguage. For all benchmarks, we report the results on\nthe test sets using all instances (Table 5), except for\nXL-Sum and FLORES-200, where we only sam-\nple 250 and 200 examples respectively to show\nthe trend of generation performance. In the few-\nshot setting, we randomly choose examples from\nthe development set if they have, otherwise, we\ntranslate the English training set into correspond-\ning languages to construct several examples.\n3.1.2 Baselines\nBasic Prompt are the vanilla in our experiments\nthat were proposed and suggested in previous\nwork. After determining the prompt, we format\neach monolingual instance using the English ba-\nsic prompt. This setting is similar to the mono-\nlingual prompting in MEGA (Ahuja et al., 2023).\nThe basic prompts used for the evaluation of each\nbenchmark are listed in Table 5. Note that, we\ndismiss the baseline using native-language, since\nMEGA (Ahuja et al., 2023) reveals monolingual\nprompting is superior to cross-lingual prompting.\nChain-of-Thought (CoT) prompting invokes\nLLMs to generate a series of intermediate results\nto solve reasoning tasks (Wei et al., 2022c), which\nis still effective under multilingual scenarios (Shi\net al., 2023). In experiments, we append the in-\nstruction “Let’s think step-by-step and tell me the\nanswer in the end”after the input to prompt LLMs.\nTranslate-English leverages the robust capabili-\nties of LLMs in English to tackle multilingual tasks,\nas suggested by both Shi et al. (2023) and Ahuja\net al. (2023). This approach translates instances\nfrom other languages into English beforehand. In\npractice, we utilize the Google Translate API to\ntranslate examples into English and apply the basic\nprompt to format them. Note that, we do not apply\nthis method to generation tasks since they require\nthe output in respective language rather English.\nXLT utilizes the proposed template consisting of\nmultiple instructions introduced in Section 2. The\n12369\ninstantiated XLT templates for each benchmark are\nlisted in Table 6.\nIn few-shot learning scenarios, for basic prompt,\nwe use the same template as an additional input\nto the model. For XLT, we provide the exemplars\nwith XLT template inputs and anticipate desirable\nstep-by-step outputs as outlined in Figure 4. In the\nsubsequent evaluation, we apply the 5-shot setting,\nexcept for the XL-Sum* experiments, which use\nthe 3-shot setting due to input length constraints.\n3.1.3 LLMs\nWe mainly evaluate two LLMs from the GPT-3.5\nseries models:\n• text-davinci-0034 is trained using instruction\ntuning and reinforcement learning from human\nfeedback (Ouyang et al., 2022). It can perform a\nwide range of natural language tasks with satis-\nfactory results.\n• gpt-3.5-turbo4 is optimized for chat based on\ntext-davinci-003 and suitable for traditional\nNLP tasks. It is the most capable GPT-3.5 model.\nTo verify the compatibility of our XLT template,\nwe further incorporate LLaMA-2-Chat (Touvron\net al., 2023) (Llama-2-70b-chat-hf) as our base\nmodels. It is an open-source model that has been\ntrained through supervised fine-tuning and rein-\nforcement learning from human feedback on the\nbase LLaMA 2 model. In addition, we also re-\nfer to the existing results from other LLMs, such\nas code-davinci-0024, when the evaluation is\ncomparable. During inference, we employ greedy\nsearch (i.e., temperature=0) to generate the LLM re-\nsponses. We find LLMs have excellent instruction-\nfollowing abilities to respond to our instructions\nin the given format. Therefore, we just extract the\npart after “Answer format:” as labels.\n3.2 Experimental Results\nMultilingual Capability. We comprehensively\nevaluate XLT’s performance over seven tasks. The\naverage score of text-davinci-003 is summa-\nrized in Figure 1(a) and Table 1, and more details\nare listed in Appendix A. As for the CoT prompting,\nit can enhance reasoning tasks while becomes less\neffective on understanding and generation tasks. In\nterms of the Translate-En prompting, it can boost\nthe performance in the zero-shot settings while\n4https://platform.openai.com/docs/models/\ngpt-3-5\nmay not work well in the few-shot settings. Over-\nall, compared to the three baseline methods, XLT\nachieves significant improvements over two LLMs\nfor all tasks on both zero-shot and few-shot settings\nregardless of the language difference, except for\na slight drop on the PAWS-X benchmark in the\nzero-shot setting. It is noted that XLT achieves\nremarkable gains of nearly 20 points on average in\nthe MGSM benchmark for the arithmetic reason-\ning task and around 10 points on average in the\nMKQA benchmark for the open-domain question\nanswering task. The experiments demonstrates the\neffectiveness of XLT for empowering LLM with\nmultilingual capability.\nAs for the compatibility test, we list the results\nof LLaMA-2-Chat on the MGSM benchmark in\nTable 7. It is notable that LLaMA 2 can also ben-\nefit from our cross-lingual-thought, which further\ndemonstrates the generality of our XLT template.\nHowever, the gains of LLaMA-2-Chat is not as\ngood as GPT-based models. Our analysis reveals\nthis gap can primarily be attributed to LLaMA 2’s\npoorer multi-step instruction-following ability.\nLanguage Democratization. Furthermore, we\ntry to assess the democratization degree of tasks\nbetween languages by defining a “democratization\nscore”, which calculates the average percentage of\nperformance attained by different languages rela-\ntive to the best performance among all languages.\nGiven the evaluation scores of s1, s2, . . . , sl corre-\nsponding to l language on a task, the democratiza-\ntion score is formulated as:\n∑l\ni=1 si\nl / max{si}l\ni=1. (1)\nTable 2 presents the degree of democratization\nfor tasks across languages under both zero-shot\nlearning and few-shot learning, and we further sum-\nmarize it in Figure 1(b) by averaging all scores\nper task regardless of the setting and model differ-\nences. We can observe that XLT leads to higher\ndemocratization scores in general, particularly for\nXCOPA, and MKQA. As for MGSM, XNLI, and\nPAWS-X, our XLT can improve performance in\nmultiple languages, where the overall performance\nof the baseline is consistently lower but the gap be-\ntween languages is smaller as shown in Tables 7, 9,\nand 10. In conclusion, our method can reduce the\nperformance gap between languages and improve\nthe language democratization of LLMs.\n12370\nSettings\nReasoning Understanding Generation\nMGSM XCOPA XNLI PA WS-X MKQA XL-Sum* FLORES*\nZero-shot\ntext-davinci-003\nBasic Prompt 12.5 70.1 53.3 52.0 29.0 23.7 15.4\nCoT 25.7 70.9 53.0 57.8 30.9 23.8 15.8\nTranslate-En 15.7 68.0 54.8 55.0 – – –\nXLT 23.9 73.3 62.4 57.1 40.2 25.2 17.7\ngpt-3.5-turbo\nBasic Prompt 23.3 76.9 52.6 65.5 31.6 24.7 19.1\nCoT 45.5 78.3 54.8 61.0 14.8 25.4 19.7\nTranslate-En 27.1 75.7 52.2 66.8 – – –\nXLT 70.0 80.3 65.5 63.6 42.7 26.1 21.2\nFew-shot\ntext-davinci-003\nBasic Prompt 45.5 75.6 59.1 68.7 39.1 26.8 –\nTranslate-En 46.5 77.4 56.9 68.5 – – –\nXLT 55.4 81.3 67.5 72.2 49.6 27.3 –\ngpt-3.5-turbo\nBasic Prompt 63.0 80.1 61.4 66.4 43.7 25.5 –\nTranslate-En 65.1 81.9 58.3 63.7 – – –\nXLT 72.5 85.9 65.0 69.1 52.5 27.9 –\nTable 1: The average scores in different languages for the seven benchmarks in zero-shot and few-shot settings. We\nomit the results (denoted as “–”) of Translate-En since it is not applicable for generation tasks.\nSettings Reasoning Understanding Generation\nMGSM XCOPA XNLI PA WS-X MKQA\nZero-shot setting\ntext-davinci-003\nBasic Prompt 65.2 77.8 83.8 97.1 60.2\nCoT 65.4 80.1 83.5 89.5 61.4\nTranslate-En 77.2 78.7 86.0 95.3 51.6\nXLT 68.5 82.1 80.7 88.4 78.7\ngpt-3.5-turbo\nBasic Prompt 73.0 83.6 80.5 89.0 61.8\nCoT 66.7 85.7 80.7 88.9 46.4\nTranslate-En 80.4 84.6 79.8 90.7 54.1\nXLT 84.1 89.1 88.0 96.2 75.3\nFew-shot setting\ntext-davinci-003\nBasic Prompt 75.4 82.0 82.5 88.2 74.3\nTranslate-En 77.1 82.6 79.5 87.8 68.5\nXLT 84.5 85.6 85.3 91.6 82.7\ngpt-3.5-turbo\nBasic Prompt 76.1 84.1 83.6 94.4 82.1\nTranslate-En 78.6 86.4 79.2 95.4 71.3\nXLT 86.2 89.7 84.3 94.1 83.1\nTable 2: The democratization degree of tasks against\nlanguages.\n3.3 Further Analysis\nIn this section, we further investigate the factors\nthat affect the performance of XLT and how they\naffect various multilingual benchmarks.\n3.3.1 Ablation of XLT\nFor the XLT variants, we mainly conduct experi-\nments to compare the following strategies:\n• Ablating the instructions. Since our XLT con-\nsists of six logical instructions, we disable the\nRole Assigning, Cross-lingual Thinking, and CoT\nTask Solvinginstructions separately to analyze\nthe contribution per instruction.\n• Reordering the instructions. Considering the\nlogicality of our instructions, we further change\nthe order of the instructions in XLT to explore\nwhether LLMs will handle tasks differently and\nlead to different results.\n• Changing the content word. As prompts are\nusually sensitive to the word choice, we verify the\nrobustness of XLT when alternating the rephras-\ning keyword with “retell”, “repeat”, and “trans-\nlate” in the cross-lingual thinking instruction.\nThe outcomes are presented in Table 3, indi-\ncating that XLT surpasses almost all the variants,\nthereby validating the effectiveness and reasonable-\nness of our proposed XLT method.\nThe effectiveness of each instruction. The re-\nsults from the “Instruction Ablation\" row indicate\nthat: (1) Cross-lingual Thinkingyields more signifi-\ncant gains compared to other instructions. This sug-\ngests that the LLM’s ability of cross-lingual think-\ning is activated, allowing it to utilize its knowledge\nin English to solve tasks effectively; (2) Remov-\ning Role Assigningfrom XLT impedes the model’s\n12371\nSettings MGSM XNLI FLORES*\nde zh hi vi jv→zh zh →jv\nXLT 79.8 72.6 61.3 64.8 19.0 10.5\nInstruction\nAblation\nw/o Role Assigning 76.6 69.2 57.8 63.9 16.2 8.8\nw/o Cross-lingual Thinking 75.6 62.0 56.1 62.2 13.2 8.2\nw/o CoT Task Solving 77.0 68.0 62.9 65.2 16.8 9.2\nInstruction\nOrder\nSwap Role Assigningand Task Inputting 77.2 71.8 54.2 61.5 19.6 11.2\nSwap Role Assigningand Task Analyzing 76.8 70.8 61.0 64.0 15.8 8.8\nSwap Cross-lingual Thinkingand Task Analyzing 79.0 71.2 59.5 63.4 16.5 9.7\nRephrasing\nWord\nw/ retell 79.8 72.6 61.3 64.8 18.2 10.3\nw/ repeat 77.6 68.0 60.7 64.6 19.0 10.5\nw/ translate 76.4 70.0 60.1 64.5 17.5 10.2\nTable 3: Performance comparison across different variants of XLT. All the experiments are conducted using\ngpt-3.5-turbo under the zero-shot setting.\nDemonstration format en de ru fr zh es ja sw th bn te Avg.\nBasic input + Basic output 84.0 79.2 78.8 78.8 70.8 81.2 68.8 70.8 68.8 65.2 44.8 71.9\nBasic input + XLT output 82.4 72.4 71.2 75.2 64.4 78.8 63.2 66.8 53.6 54.8 32.4 65.0\nXLT input + XLT output 84.8 81.4 80.2 79.2 71.8 81.6 72.8 71.2 69.8 64.4 40.8 72.5\nTable 4: Performance comparison across different few-shot variants on the MGSM benchmark. All the experiments\nare conducted with 5 demonstrations using gpt-3.5-turbo.\nunderstanding of the ultimate goal for diverse mul-\ntilingual tasks, highlighting the task transferability\nof XLT; and (3) the better performance of XLT can\nalso be attributed to CoT Task Solving, which re-\nquires the model to respond to complex instructions\nin a step-by-step manner.\nThe order of logical instructions. The perfor-\nmance drop is evident when the order of our de-\nsigned logical instructions is switched. When de-\nsigning XLT, we have taken into account the pro-\ncess by which humans solve multilingual problems,\nand this experiment further confirms the optimum\norder of our XLT template. Placing the Role As-\nsigning instruction later may confuse the model\ninitially. Additionally, conducting Cross-lingual\nThinking before Task Analyzingis crucial since we\nrely on the English task-solving abilities of LLMs\nto handle multilingual tasks.\nThe robustness of word choice for rephrasing\nkeywords. We can find that different words in-\ndeed affect the performance of XLT, but it is less\nsensitive to the other variants. Through experi-\nmentation, we have determined that “repeat\" yields\nbetter results for text summarization and machine\ntranslation, while “retell\" is more suitable for the\nremaining five tasks. Our aim is to provide XLT\nwith a more unified template, while still allowing\nusers to fine-tune specific keywords for optimal\nperformance in their tasks.\n3.3.2 Effectiveness of XLT Few-shot Learning\nAs mentioned in Section 2.2, the construction of\ndemonstrations for XLT few-shot learning differs\nfrom the previous method. We have compared XLT\nand basic prompt. Here, we focus on the construc-\ntion of the demonstration input-output pairs and\ncompare various demonstrations that may be used\nto perform XLT few-shot learning. The illustrations\ncan be found in Figure 5.\n• Basic prompt input + Basic prompt output:\nThis is the normal demonstration format used in\nmost of the previous work.\n• Basic prompt input + XLT output: This abla-\ntion is to separate the effect of input and output\nformats in the demonstration.\n• XLT input + XLT output: This is the method\nthat we used in this work.\nObserving the experimental results presented in\nTable 4, we can conclude that: (1) Our XLT few-\nshot learning outperforms all other variants, thus\nconfirming its effectiveness. (2) The use of normal\ndemonstrations for XLT few-shot learning leads to\na decrease in performance. (3) Merely incorporat-\ning XLT as a demonstration input without its output\ndoes not result in any improvements. (4) Consis-\ntency in the demonstration for few-shot learning\n12372\nis crucial, implying that the demonstration input-\noutput format should align better with its zero-shot\nlearning input-output format.\n4 Related Work\n4.1 LLM Capability Understanding\nDespite the impressive capabilities of LLMs, it is\ncrucial to determine their impact on natural lan-\nguage processing tasks. Liang et al. (2022) con-\nduct a comprehensive evaluation of LLMs from\nvarious perspectives, such as accuracy, calibration,\nrobustness, fairness, bias, toxicity, and efficiency.\nBang et al. (2023) extensively evaluate the Chat-\nGPT model on multiple natural language process-\ning tasks and find that the model performs well in\nhigh-resource languages but exhibits certain lim-\nitations in low-resource and non-Latin script lan-\nguages. Additionally, studies by Jiao et al. (2023)\nand Hendy et al. (2023) compare different GPT\nmodels with supervised models for machine trans-\nlation tasks and find that GPT models have com-\npetitive translation abilities in high-resource lan-\nguages but perform less effectively in low-resource\nlanguages. It is worth noting that achieving multi-\nlingual generative AI capability necessitates cross-\nlingual knowledge to further improve the model’s\nperformance. In this context, Ahuja et al. (2023)\nevaluate the multilingual task understanding abil-\nity of GPT models and attempt to enhance their\ntask processing abilities in other languages using\nEnglish knowledge. Our work also focuses on eval-\nuating the multilingual capabilities of LLMs, in-\ncluding reasoning, understanding, and generative\ncapabilities. Our evaluations indicate that LLMs ex-\nhibit differences in high-resource and low-resource\nabilities, which necessitates additional efforts to\nenhance their multilingual capability.\n4.2 Multilingual Task Processing\nMultilingual knowledge has been shown to be ex-\nploitable and transferable between languages to im-\nprove model performance (Devlin et al., 2019; Con-\nneau et al., 2020; Raffel et al., 2020; Ouyang et al.,\n2021; Chi et al., 2021). While much research has\nbeen devoted to multilingual understanding tasks,\nmultilingual generation tasks are more challeng-\ning, particularly when the target language is low-\nresource or non-English (Ma et al., 2021; Liu et al.,\n2020). Two methods can enable models to support\nmultilingual task processing: one is training a su-\npervised model that covers multiple languages for\nmultilingual processing (Costa-jussà et al., 2022),\nand the other is training a pre-trained model and\nusing fine-tuning to transfer knowledge among lan-\nguages to achieve multilingual capability (Chen\net al., 2021, 2022). However, the emergence of\nLLMs has made it possible to directly process\nmultilingual tasks via in-context learning (Brown\net al., 2020; Ahuja et al., 2023). These LLMs,\nwith hundreds of billions or even trillions of pa-\nrameters, require a significant amount of compu-\ntation resources for training, making traditional\nfine-tuning methods less feasible. To improve the\ngenerative ability of LLMs, researchers explore\nin-context learning methods that do not require up-\ndating model parameters, such as few-shot prompt-\ning (Vilar et al., 2022), automatic prompt learn-\ning (Shin et al., 2020), task-instruction prompt-\ning (Ye et al., 2023), chain-of-thought prompt-\ning (Wei et al., 2022c), etc. Our work builds upon\nthese methods and proposes an optimized, generic,\nand language-independent prompt to enhance the\nmultilingual capability of LLMs.\n5 Conclusion\nThis work investigates the language processing ca-\npabilities of large language models in multilingual\nsettings and expects to develop a universal frame-\nwork for handling diverse multilingual tasks. To\naccomplish this goal, we propose a generic prompt,\nreferred to as XLT, to enhance the multilingual ca-\npability and reduce the performance gaps among\nlanguages in tasks related to language understand-\ning, reasoning, and generation in non-English and\nlow-resource languages. Although our method is\ngenerally applicable across tasks and languages,\nwe discovered that prompting design factors such\nas instruction logic and word choice have explicit\nimpacts on its effectiveness. Cross-language think-\ning in XLT is particularly effective. Finally, we\nhope this work can inspire further research to pri-\noritize the development of generic prompting. By\ndoing so, large language models can encompass a\nwider range of modalities and languages.\nAcknowledgements\nTianyi Tang and Xin Zhao are supported by Na-\ntional Natural Science Foundation of China un-\nder Grant No. 62222215, Beijing Natural Sci-\nence Foundation under Grant No. 4222027 and\nL233008.\n12373\nLimitations\nDue to limitations imposed by the evaluation bench-\nmarks and OpenAI API cost, we conducted tests\non 27 languages, which merely scratch the surface\nof the vast array of languages in the world. Be-\nsides, our XLT template is based on English. It\ndeserves to explore whether the template written\nin task language can lead to better performance\nand how to better construct the instruction in each\nlanguage. Furthermore, we only verify the effec-\ntiveness of our method on two GPT-based mod-\nels (i.e., text-davinci-003 and gpt-3.5-turbo)\nand LLaMA-2-Chat. It is worthwhile to investi-\ngate the generality of our template on more models,\nsuch as BLOOM and PaLM.\nReferences\nKabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi\nJain, Harshita Diddee, Samuel Maina, Tanuja Ganu,\nSameer Segal, Maxamed Axmed, Kalika Bali, et al.\n2023. Mega: Multilingual evaluation of generative\nai. arXiv preprint arXiv:2303.12528.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nGuanhua Chen, Shuming Ma, Yun Chen, Li Dong,\nDongdong Zhang, Jia Pan, Wenping Wang, and Furu\nWei. 2021. Zero-shot cross-lingual transfer of neu-\nral machine translation with multilingual pretrained\nencoders. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 15–26, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nGuanhua Chen, Shuming Ma, Yun Chen, Dongdong\nZhang, Jia Pan, Wenping Wang, and Furu Wei. 2022.\nTowards making the most of cross-lingual transfer\nfor zero-shot neural machine translation. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 142–157, Dublin, Ireland. Association\nfor Computational Linguistics.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham\nSinghal, Wenhui Wang, Xia Song, Xian-Ling Mao,\nHeyan Huang, and Ming Zhou. 2021. InfoXLM: An\ninformation-theoretic framework for cross-lingual\nlanguage model pre-training. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3576–3588, On-\nline. Association for Computational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina\nWilliams, Samuel Bowman, Holger Schwenk, and\nVeselin Stoyanov. 2018. XNLI: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2475–2485, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nMarta R Costa-jussà, James Cross, Onur Çelebi, Maha\nElbayad, Kenneth Heafield, Kevin Heffernan, Elahe\nKalbassi, Janice Lam, Daniel Licht, Jean Maillard,\net al. 2022. No language left behind: Scaling\nhuman-centered machine translation. arXiv preprint\narXiv:2207.04672.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nRobert Forkel et al. 2022. Glottocodes: Identifiers link-\ning families, languages and dialects to comprehen-\nsive reference information. Semantic Web, 13(6):917–\n924.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\n12374\nM. Sohel Rahman, and Rifat Shahriyar. 2021. XL-\nsum: Large-scale multilingual abstractive summariza-\ntion for 44 languages. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4693–4703, Online. Association for Computa-\ntional Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In International Conference on Learning\nRepresentations.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Hassan\nAwadalla. 2023. How good are gpt models at ma-\nchine translation? a comprehensive evaluation. arXiv\npreprint arXiv:2302.09210.\nWenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing\nWang, and Zhaopeng Tu. 2023. Is chatgpt a good\ntranslator? yes with gpt-4 as the engine. arXiv\npreprint arXiv:2301.08745.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nViet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben\nVeyseh, Hieu Man, Franck Dernoncourt, Trung Bui,\nand Thien Huu Nguyen. 2023. Chatgpt beyond en-\nglish: Towards a comprehensive evaluation of large\nlanguage models in multilingual learning. arXiv\npreprint arXiv:2304.05613.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, et al. 2022. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian O’Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-\nanov, and Xian Li. 2022. Few-shot learning with\nmultilingual generative language models. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 9019–9052,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nShayne Longpre, Yi Lu, and Joachim Daiber. 2021.\nMKQA: A linguistically diverse benchmark for mul-\ntilingual open domain question answering. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1389–1406.\nShuming Ma, Li Dong, Shaohan Huang, Dong-\ndong Zhang, Alexandre Muzio, Saksham Singhal,\nHany Hassan Awadalla, Xia Song, and Furu Wei.\n2021. Deltalm: Encoder-decoder pre-training for\nlanguage generation and translation by augmenting\npretrained multilingual encoders. arXiv preprint\narXiv:2106.13736.\nOpenAI. 2023. Gpt-4 technical report. arXiv.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730–27744.\nCurran Associates, Inc.\nXuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun,\nHao Tian, Hua Wu, and Haifeng Wang. 2021.\nERNIE-M: Enhanced multilingual representation by\naligning cross-lingual semantics with monolingual\ncorpora. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 27–38, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nEdoardo Maria Ponti, Goran Glavaš, Olga Majewska,\nQianchu Liu, Ivan Vuli´c, and Anna Korhonen. 2020.\nXCOPA: A multilingual dataset for causal common-\nsense reasoning. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 2362–2376, Online. As-\nsociation for Computational Linguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Belgium, Brussels. Association for Computa-\ntional Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\n12375\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,\nSuraj Srivats, Soroush V osoughi, Hyung Won Chung,\nYi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das,\nand Jason Wei. 2023. Language models are multi-\nlingual chain-of-thought reasoners. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric\nWallace, and Sameer Singh. 2020. AutoPrompt: Elic-\niting Knowledge from Language Models with Auto-\nmatically Generated Prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4222–4235,\nOnline. Association for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nDavid Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,\nViresh Ratnakar, and George Foster. 2022. Prompt-\ning palm for translation: Assessing strategies and\nperformance. arXiv preprint arXiv:2211.09102.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022a. Finetuned language\nmodels are zero-shot learners. In International Con-\nference on Learning Representations.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022b. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research. Survey Certifica-\ntion.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022c. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 24824–24837. Curran Associates,\nInc.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019. PAWS-X: A cross-lingual adversar-\nial dataset for paraphrase identification. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 3687–3692, Hong\nKong, China. Association for Computational Linguis-\ntics.\nSeonghyeon Ye, Hyeonbin Hwang, Sohee Yang,\nHyeongu Yun, Yireun Kim, and Minjoon Seo. 2023.\nIn-context instruction learning. arXiv preprint\narXiv:2302.14691.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the 38th International Conference\non Machine Learning, volume 139 of Proceedings\nof Machine Learning Research, pages 12697–12706.\nPMLR.\nWenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu,\nLingpeng Kong, Jiajun Chen, Lei Li, and Shujian\nHuang. 2023. Multilingual machine translation with\nlarge language models: Empirical results and analy-\nsis. arXiv preprint arXiv:2304.04675.\n12376\nA Additional Experiments\nA.1 Results on Reasoning Tasks\nTable 7 presents the results of the MGSM bench-\nmark. XLT significantly improves the arithmetic\nreasoning capabilities of both models, particularly\nfor gpt-3.5-turbo in the zero-shot setting. We\nhypothesize that gpt-3.5-turbo may have under-\ngone supervised fine-tuning (Ouyang et al., 2022)\nwith arithmetic reasoning samples in the chain-of-\nthought format, which enables XLT to activate its\narithmetic reasoning ability directly. For both low-\nresource languages ( e.g., sw, th, bn, and te) and\nhigh-resource languages, XLT can further enhance\nthe performance. Even under the few-shot setting,\nXLT can still significantly improve the reasoning\nperformance of both models and reduce the perfor-\nmance gap for all languages. Notably, for some\nhigh-resource languages, such as de, ru, fr, and es,\nthe performance is comparable to English.\nThe XCOPA benchmark results are presented\nin Table 8. Our XLT approach significantly en-\nhances the performance of both models in both\nsettings, as compared to basic prompting. In the\nzero-shot setting, XLT demonstrates significant im-\nprovements for relatively low-resource languages\n(e.g., sw, th, et, ta, and ht), but it underperforms\nthe baseline for some high-resource languages such\nas zh and it. In the few-shot setting, XLT brings\nenhancements for both high- and low-resource lan-\nguages. Our findings suggest that XLT is more\neffective for low-resource languages, particularly\nfor gpt-3.5-turbo on sw, th, ta, and ht, where it\nyields improvements of over 10 accuracy points.\nA.2 Results on Understanding Tasks\nTable 9 presents the results of the XNLI benchmark.\nIn the zero-shot setting, our XLT significantly out-\nperforms the basic prompt in all languages. Addi-\ntionally, when using few-shot setups on high- and\nlow-resource languages, both text-davinci-003\nand gpt-3.5-turbo show significant improve-\nments compared to the basic prompt. Specifically,\nfor low-resource languages such as th, bg, hi, and\nur, XLT achieves an average improvement of 9.4\naccuracy scores for text-davinci-003 and 5.3\naccuracy scores for gpt-3.5-turbo. This demon-\nstrates that XLT is effective for both models, but\ntext-davinci-003 has better natural language in-\nference capabilities.\nTable 10 displays the comparisons on the PAWS-\nX task, where XLT outperforms basic prompt in all\nlanguages, particularly for low-resource languages\nunder the few-shot setting. We observe a slight\nperformance drop on average in zero-shot learn-\ning compared to gpt-3.5-turbo for some high-\nresource languages ( e.g., en, de, and fr). Based\non our analysis of intermediate outputs, we infer\nthat the drop in performance may be due to cross-\nlingual thinking that alters the original meaning of\nthe two sentences, leading to difficulties in judg-\nment. Additionally, a comparable pattern is evident\nin a previous study (Ahuja et al., 2023), where\nnon-Latin script languages (ja, zh, and ko) exhibit\nsignificantly poorer performance than English or\nGerman in the few-shot setting. Nevertheless, by\ndemonstrating the construction of XLT, we can\nguide the model on how to think across different\nlanguages and effectively address the aforemen-\ntioned issues.\nA.3 Results on Generation Tasks\nThe MKQA benchmark outcomes are listed in Ta-\nble 11. Across all languages in the zero-shot and\nfew-shot settings, the XLT template shows a sig-\nnificant improvement over the basic prompt. It\nis worth noting that text-davinci-003 performs\nworse than gpt-3.5-turbo in this task, and we\nspeculate that the latter is optimized for open ques-\ntion answering, which is common in daily chat.\nAdditionally, our findings indicate that XLT can no-\ntably enhance the performance of under-resourced\nlanguages. XLT brings over 10 points of improve-\nment for these languages. ( e.g., zh, ja, vi, and tr)\nThis aligns with previous benchmarking studies\nand is particularly noteworthy in this evaluation.\nWe suspect that high-resource and low-resource\nlanguages share the same cross-lingual thinking as\nEnglish to greatly leverage the LLM’s ability to\nsolve English open-domain QA.\nThe results of the XL-Sum* benchmark are pre-\nsented in Table 12. It can be observed that XLT\noutperforms the basic prompt in both zero- and few-\nshot settings across all languages. Additionally, the\nLLM model exhibits a significant improvement in\ngenerating summaries under the few-shot setting\ncompared to the zero-shot setting. This suggests\nthat providing fewer examples can effectively guide\nthe model in summarizing multilingual texts. Fur-\nthermore, the few-shot results revealed an interest-\ning finding that text-davinci-003 performed bet-\nter when gpt-3.5-turbo and text-davinci-003\nuse basic prompt. However, once XLT is enabled,\n12377\nTable 5: The basic prompt of each benchmark. #Test denotes the number of instances in the test set.\nBenchmark #Test Basic Prompt\nMGSM 250 Request: {problem}\nXCOPA 500 Here is a premise: {premise}. What is the {question}? Help me pick the more plausible\noption: -choice1: {choice1}, -choice2: {choice2}\nXNLI 5,010 {premise} Based on previous passage, is it true that {hypothesis}? Yes, No, or Maybe?\nPAWS-X 2,000 Sentence 1: {sentence1} Sentence 2: {sentence2} Question: Does Sentence 1 paraphrase\nSentence 2? Yes or No?\nMKQA 6,758 Answer the question in one or a few words in {target_language}: {question}?\nXL-Sum* 250 Summarize this article: {article}\nFLORES* 200 {source} Translate from {source_language} to {target_language}:\nTable 6: Task meta data consisting of task name, input tag, task goal, output type, and output constraint per\nbenchmark. Detailed examples of the input for each benchmark are listed in the following part.\nBenchmark Task name Input tag Task goal Output type Output constraint\nMGSM arithmetic reason-\ning\nrequest do step-by-step answer to obtain a number answer answer –\nXCOPA commonsense rea-\nsoning\npremise and the op-\ntions\ndo step-by-step answer to pick a choice choice number –\nXNLI natural language in-\nference\nhypothesis and the\npremise\njudge whether the hypothesis is true, false, or undetermined\ngiven the premise. The relationship can be chosen from en-\ntailment, contradiction, and neutral\nrelationship –\nPAWS-X paraphrase identifi-\ncation\nsentence 1 and sen-\ntence 2\nprovide a yes or no answer to the question: Does Sentence 1\nparaphrase Sentence 2?\nanswer choosing either yes or\nno\nMKQA question answering question answer the question in English in one or a few words answer in one or a few words\nin {target_language}\nXL-Sum multilingual sum-\nmarization\nentire text think step-by-step to summarize the entire text in a maximum\nof two sentences\nsummary into one sentence in\n{target_language}\nFLORES machine transla-\ntion\nsource sentence provide the {target_language} translation for the English\nsource sentence\ntarget translation –\ngpt-3.5-turbo outperforms text-davinci-003,\nhighlighting the effectiveness of our approach.\nMachine translation is a special generation task\nwhere the source and target are two different lan-\nguages. The experiment in this part is to verify how\nXLT boosts machine translation tasks. Since En-\nglish has been specified as the pivot language in the\ncross-lingual thinking in XLT, we exclude English-\ncentric tasks to avoid language redundancy and fo-\ncus on 12 non-English translation directions in the\nFLORES* benchmark, which includes both high-\nresource and low-resource languages. As shown\nin Table 13, XLT achieves impressive zero-shot re-\nsults for all languages compared with basic prompt.\nFor example, it significantly improves translation\nquality in Chinese-to-X or X-to-Chinese. The re-\nsult emphasizes that XLT will potentially transfer\nthe knowledge of a high-resource pivot language\nlike English to the target language. While the bene-\nfit of XLT may not be as obvious for high-to-high\ntranslations, it becomes more significant for high-\nto-low, low-to-high, and low-to-low translations.\nFor instance, XLT improves the translation perfor-\nmance of gpt-3.5-turbo by nearly 4.0, 2.8, and\n3.3 BLEU points for th →gl, jv→zh, and zh →th\ntranslations, respectively, demonstrating its effec-\ntiveness regardless of whether the source language\nis high-resource or low-resource. Noticing that\nHendy et al. (2023) have shown that few-shot con-\nfigurations do not yield significant improvements\nover the zero-shot setup for translation tasks, we do\nnot evaluate the few-shot paradigm on FLORES*\nin this work and leave it for future exploration.\n12378\nSettings (high→low) en de ru fr zh es ja sw th bn te Avg.\nZero-shot\ntext-davinci-003\nBasic Prompt 19.2 12.8 15.6 16.4 15.2 13.6 12.8 7.2 8.8 11.6 4.4 12.5\nXLT 30.0 32.4 23.6 34.8 29.2 26.8 26.0 13.6 18.4 14.8 12.8 23.9\ngpt-3.5-turbo\nBasic Prompt 32.0 24.8 28.0 31.6 22.0 29.2 22.4 24.4 16.8 18.0 7.6 23.3\nXLT 84.4 79.8 77.6 75.2 72.6 76.8 71.0 70.8 63.8 56.8 42.0 70.0\nLlama-2-70b-chat-hf\nBasic Prompt 58.8 48.0 47.2 45.6 39.6 50.4 39.2 10.0 13.6 17.2 5.2 34.1\nXLT 60.0 52.8 52.8 48.8 42.4 52.0 39.2 16.4 18.0 17.6 10.4 37.3\nFew-shot\ncode-davinci-002\n(Shi et al., 2023)* 53.6 46.4 48.8 46.4 47.2 51.6 44.8 37.6 41.2 41.2 42.8 45.6\ntext-davinci-003\nBasic Prompt 60.4 45.6 51.6 45.6 38.8 51.6 37.6 48.8 30.4 43.6 46.8 45.5\nXLT 65.6 58.0 57.6 56.8 53.2 58.0 54.4 58.8 42.4 53.2 51.8 55.4\ngpt-3.5-turbo\nBasic Prompt 82.8 69.2 71.6 72.4 46.8 71.2 56.0 60.0 44.0 62.4 56.6 63.0\nXLT 84.8 81.4 80.2 79.2 71.8 81.6 72.8 71.2 69.8 64.4 40.8 72.5\nTable 7: Accuracy scores on the MGSM benchmark. Shi et al. (2023)* utilize 6-shot learning.\nSettings (high→low) zh it vi tr id sw th et ta ht qu Avg.\nZero-shot\ntext-davinci-003\nBasic Prompt 85.4 90.0 69.2 80.6 83.8 56.4 66.6 73.0 53.4 61.6 50.4 70.1\nXLT 85.8 89.2 76.0 81.0 86.4 59.2 67.2 83.4 55.2 72.2 50.2 73.3\ngpt-3.5-turbo\nBasic Prompt 90.4 92.0 83.6 86.6 88.2 77.0 70.2 84.0 57.2 65.2 51.2 76.9\nXLT 87.8 89.8 87.5 90.2 89.5 82.0 78.0 88.4 64.0 74.6 51.8 80.3\nFew-shot\ncode-davinci-002\n(Shi et al., 2023)* 93.4 96.6 86.6 91.2 91.4 67.4 84.2 88.8 55.8 79.6 52.2 80.7\ntext-davinci-003\n(Ahuja et al., 2023)† – 94.6 – 89.8 93.0 82.8 84.8 89.6 87.0 82.8 – –\nBasic Prompt 90.8 92.2 80.2 85.2 90.8 63.6 69.2 81.8 53.6 73.2 51.0 75.6\nXLT 94.0 95.0 87.0 94.0 92.8 68.4 79.4 90.4 59.4 80.8 53.0 81.3\ngpt-3.5-turbo\nBasic Prompt 91.0 95.2 86.2 89.0 88.6 79.2 73.6 92.0 58.6 74.2 53.0 80.1\nXLT 92.8 95.8 90.6 92.2 90.2 92.6 85.2 93.0 70.8 86.0 56.2 85.9\nTable 8: Accuracy scores on the XCOPA benchmark. (Shi et al., 2023)* utilize 6-shot learning. Ahuja et al. (2023)†\nutilize 8-shot learning.\nSettings (high→low) en de ru fr zh es vi tr sw ar el th bg hi ur Avg.\nZero-shot\ntext-davinci-003\nBasic Prompt 63.6 59.4 55.9 60.9 51.6 59.7 49.5 53.9 40.8 51.9 53.2 49.7 54.4 49.8 45.3 53.3\nXLT 77.4 67.7 64.2 68.3 64.8 69.4 62.0 61.5 54.3 58.7 61.1 56.3 62.6 55.1 53.0 62.4\ngpt-3.5-turbo\nBasic Prompt 65.4 55.5 50.6 53.2 48.8 59.8 52.1 54.4 49.6 50.9 54.9 44.8 55.7 49.2 44.8 52.6\nXLT 74.4 68.5 66.0 69.8 64.9 69.4 64.8 65.0 60.1 62.8 68.3 62.1 67.7 61.3 57.3 65.5\nFew-shot\ntext-davinci-003\n(Ahuja et al., 2023)† 79.5 71.7 67.3 71.8 65.8 72.2 66.9 67.6 57.3 65.1 69.3 62.0 70.8 63.3 55.1 67.1\nBasic Prompt 71.6 65.8 62.5 63.4 56.7 64.6 59.4 56.9 48.2 57.3 62.0 55.0 62.6 52.4 48.0 59.1\nXLT 79.1 70.8 70.0 69.5 69.2 71.0 67.3 66.9 59.5 65.7 67.8 63.7 70.4 63.5 58.1 67.5\ngpt-3.5-turbo\nBasic Prompt 73.4 66.3 60.9 67.9 60.2 68.1 60.2 62.6 55.7 58.8 64.7 52.7 64.6 53.8 50.8 61.4\nXLT 77.1 69.3 64.4 69.6 62.9 70.6 63.2 64.4 60.2 63.4 66.6 59.8 66.9 60.0 56.5 65.0\nTable 9: Accuracy scores on the XNLI benchmark. Ahuja et al. (2023)† utilize 8-shot learning.\n12379\nSettings (high→low) en de fr zh es ja ko Avg.\nZero-shot\ntext-davinci-003\nBasic Prompt 53.3 52.9 50.8 53.6 53.0 50.3 50.3 52.0\nXLT 64.6 56.0 55.3 57.4 56.0 54.6 55.9 57.1\ngpt-3.5-turbo\nBasic Prompt 73.6 68.3 68.4 63.4 69.6 59.7 55.7 65.5\nXLT 65.3 66.1 64.8 65.5 63.3 62.4 57.6 63.6\nFew-shot\ntext-davinci-003\n(Ahuja et al., 2023)† 72.5 69.8 71.3 65.2 70.1 65.4 65.8 68.6\nBasic Prompt 77.8 70.6 72.5 65.0 71.7 62.5 60.5 68.7\nXLT 76.5 78.8 77.4 61.1 78.0 65.0 68.7 72.2\ngpt-3.5-turbo\nBasic Prompt 65.9 70.3 66.6 64.1 68.2 65.6 64.0 66.4\nXLT 73.4 69.8 68.5 70.9 67.8 66.4 66.7 69.1\nTable 10: Accuracy scores on the PAWS-X benchmark. Ahuja et al. (2023)† utilize 8-shot learning.\nSettings (high→low) en de ru fr zh es ja vi tr th Avg.\nZero-shot\ntext-davinci-003\nBasic Prompt 48.1 33.8 15.9 34.8 18.2 34.1 27.7 23.6 24.0 29.6 29.0\nXLT 51.1 42.3 27.3 43.0 36.7 43.3 46.8 35.8 37.8 38.1 40.2\ngpt-3.5-turbo\nBasic Prompt 51.1 40.6 28.4 40.1 16.5 39.3 25.9 23.3 26.9 23.7 31.6\nXLT 56.7 46.0 33.9 47.6 33.0 47.9 47.5 36.5 39.1 38.6 42.7\nFew-shot\ntext-davinci-003\nBasic Prompt 52.6 42.3 21.8 42.9 33.1 42.8 45.5 35.5 37.5 36.6 39.1\nXLT 57.6 49.4 42.7 50.9 51.0 50.0 60.0 46.9 46.9 40.5 49.6\ngpt-3.5-turbo\nBasic Prompt 53.2 48.6 31.0 46.1 40.9 47.9 51.4 38.5 40.0 39.3 43.7\nXLT 59.6 52.5 43.8 53.9 51.9 54.0 63.2 49.4 52.1 44.7 52.5\nTable 11: F1 scores on the MKQA benchmark. The average score is the macro average F1 score.\nSettings (high→low) en fr zh es vi tr Avg.\nZero-shot\ntext-davinci-003\nBasic Prompt 22.2 26.2 30.8 25.1 22.0 15.9 23.7\nXLT 24.4 28.2 32.2 26.0 22.3 17.9 25.2\ngpt-3.5-turbo\n(Lai et al., 2023) 19.7 20.8 21.1 17.8 – 14.5 –\nBasic Prompt 25.3 26.2 30.2 26.3 21.1 19.2 24.7\nXLT 26.8 28.1 33.3 26.4 21.3 20.5 26.1\nFew-shot\ntext-davinci-003\nBasic Prompt 29.2 29.6 33.2 28.3 22.5 18.1 26.8\nXLT 28.2 30.3 34.4 29.4 22.7 18.6 27.3\ngpt-3.5-turbo\nBasic Prompt 25.7 27.2 30.8 27.8 21.5 19.7 25.5\nXLT 28.5 29.2 35.0 28.6 23.7 22.3 27.9\nTable 12: ROUGE-1 scores on the XL-Sum* benchmark.\n12380\nSettings\nHigh-High High-Low Low-Low\nzh-ru de-vi zh-th zh-jv th-gl jv-th\n→ ← → ← → ← → ← → ← → ←\nZero-shot\ntext-davinci-003\nBasic Prompt 19.8 24.2 26.5 24.5 10.2 11.8 8.1 14.0 17.9 12.0 10.0 6.2\nXLT 21.6 24.8 27.4 24.8 12.6 16.4 11.1 18.2 20.7 14.2 11.7 9.0\ngpt-3.5-turbo\nBasic Prompt 23.3 25.4 34.1 29.6 16.6 18.6 9.1 16.2 18.1 18.5 13.0 7.2\nXLT 25.3 25.6 33.3 31.3 19.9 19.3 10.5 19.0 22.1 21.9 15.9 10.6\nTable 13: BLEU scores on the FLORES* benchmark.\nI want you to act as an …\nRequest:罗杰有5个网球…\n…\nYou should tell me the answer …\nAnswer:逐步解答：罗杰一开始\n有5 个球… 答案是11。\n5x\nI want you to act as an …\nRequest:罗杰有5个网球…\n…\nYou should tell me the answer …\nRetelling the request in …\nStep-by-step answer:\n…\nAnswer: 11\n5x\n: Basic Prompt : XLT\n(a) Basic prompt input and output (c) XLT input and output(b) Basic prompt input and XLT output\nRequest: 罗杰有5个网球…\n5x Answer:逐步解答：罗杰一开始\n有5 个球… 答案是11。\nFigure 5: Illustrations of different demonstration input-output pairs in the few-shot learning.\n12381\nInput:\nRequest: 制作一件袍子需要 2 匹蓝色纤维布料和这个数量一半的白色纤维布料。它一共\n需要用掉多少匹布料？\ntext-davinci-003 output:\nAnswer: 3 匹布料\ngpt-3.5-turbo output:\n3 匹布料。\nAnswer:\n3\nFigure 6: A Chinese example of the MGSM benchmark using basic prompt and the corresponding outputs under\nthe zero-shot setting.\nInput:\nI want you to act as an arithmetic reasoning expert for Chinese .\nRequest: 制作一件袍子需要 2 匹蓝色纤维布料和这个数量一半的白色纤维布料。它一共\n需要用掉多少匹布料？\nYou should retell the request in English.\nYou should do step-by-step answer to obtain a number answer .\nYou should step-by-step answer the request.\nYou should tell me the answer in this format ' Answer :'.\ntext-davinci-003 output:\nThe request is: To make a robe, you need 2 pieces of blue fiber fabric and half of that amount of white fiber fabric. How many pieces of fabric are needed in\ntotal?\nStep-by-step answer:\n1. Start with the amount of blue fiber fabric: 2 pieces\n2. Half of that amount is 1 piece\n3. Add the 1 piece of white fiber fabric to the 2 pieces of blue fiber fabric\n4. Answer: 3 pieces of fabric are needed in total.\ngpt-3.5-turbo output:\nThe request is asking how many pieces of fabric are needed to make a robe, given that it requires 2 pieces of blue fiber fabric and half that amount of white\nfiber fabric.\nTo solve this problem, we need to first find out how many pieces of white fiber fabric are needed. We can do this by dividing the number of blue fiber fabric\npieces by 2:2 ÷ 2 = 1\nSo we need 1 piece of white fiber fabric.\nTo find the total number of fabric pieces needed, we can add the number of blue and white pieces together:2 + 1 = 3\nTherefore, Answer: 3 pieces of fabric are needed to make the robe.\nAnswer:\n3\nFigure 7: A Chinese example of the MGSM benchmark using XLT and the corresponding outputs under the\nzero-shot setting.\n12382\nInput:\nI want you to act as an arithmetic reasoning expert for Chinese .\nRequest: 罗杰有5个网球。他又买了2罐网球。每罐有3个网球。他现在有多少个网球？\nYou should retell the request in English.\nYou should do step-by-step answer to obtain a number answer .\nYou should step-by-step answer the request.\nYou should tell me the answer in this format ' Answer :'.\nXXX\nXXX\nRetelling the request in English: Roger currently has 5 tennis balls...\nStep-by-step answer:\n1. Roger has 5 tennis balls.\n2. He buys 2 cans of tennis balls.\n3. Each can has 3 tennis balls, so 2 cans will have a total of 6 tennis ...\n4.To find out how many tennis balls Roger has in total, we need to add the ...\n5. Adding 5 (initial tennis balls) and 6 (purchased tennis balls) gives ...\nAnswer: 11.\nXXX\nXXX\n... (the remaining 4 examples)\nXXX\nXXX\nI want you to act as an arithmetic reasoning expert for Chinese .\nRequest: 制作一件袍子需要 2 匹蓝色纤维布料和这个数量一半的白色纤维布料。它一共\n需要用掉多少匹布料？\nYou should retell the request in English.\nYou should do step-by-step answer to obtain a number answer .\nYou should step-by-step answer the request.\nYou should tell me the answer in this format ' Answer :'.\ntext-davinci-003 output:\nRequest: To make a robe, two pieces of blue fabric and half the amount of white fabric are needed. How many pieces of fabric are needed in total?\nStep-by-step answer:\n1. Two pieces of blue fabric are needed.\n2. Half the amount of blue fabric is needed for white fabric, so that is 1 piece of white fabric (2 x 0.5 = 1).\n3. To find out how many pieces of fabric are needed in total, we need to add the number of blue fabric and white fabric.\n4. Adding 2 (blue fabric) and 1 (white fabric) gives us a total of 3 pieces of fabric.\nAnswer: 3 pieces of fabric.\ngpt-3.5-turbo output:\nRequest: Making one robe requires 2 pieces of blue fiber fabric and half that amount of white fiber fabric. How many pieces of fabric are needed in total?\nStep-by-step answer:\n1. To make one robe, we need 2 pieces of blue fiber fabric.\n2. We also need half that amount of white fiber fabric, which is 1 piece (2/2 = 1).\n3. Therefore, to make one robe, we need a total of 2 + 1 = 3 pieces of fiber fabric.\nAnswer: We need 3 pieces of fiber fabric in total to make one robe.\nAnswer:\n3\nFigure 8: A Chinese example of the MGSM benchmark using XLT and the corresponding outputs under the\nfew-shot setting.\n12383\nInput:\nHere is a premise: Ese oli mullikilesse mässitud. What is the cause? Help me\npick the more plausible option: -choice1: See oli õrn, -choice2: See oli väike.\ntext-davinci-003 output:\nChoice 2 : See oli väike.\ngpt-3.5-turbo output:\nAs an AI language model, I cannot determine the cause of the premise without\nadditional context. The options provided do not seem to be directly related\nto the premise. Can you please provide more information or context?\nAnswer:\n1\nFigure 9: An Estonian example of the XCOPA benchmark using basic prompt and the corresponding outputs\nunder the zero-shot setting.\nInput:\nI want you to act as a commonsense reasoning expert for Estonian .\nHere is a premise: Ese oli mullikilesse mässitud. What is the cause? Help me\npick the more plausible option: -choice1: See oli õrn, -choice2: See oli väike.\nYou should retell the premise and the options in English.\nYou should do step-by-step answer to pick a choice .\nYou should step-by-step answer the request.\nYou should tell me the choice number in this format ' Choice number :'.\ntext-davinci-003 output:\nPremise: Ese oli mullikilesse mässitud.\nOptions: Choice 1: See oli õrn, Choice 2: See oli väike.\nStep 1: Consider the context of the premise. What kind of object was mässitud (inserted) into the pocket?\nStep 2: Consider the two options. Choice 1: See oli õrn (It was soft) and Choice 2: See oli väike (It was small).\nStep 3: Based on the context and the two options, the more plausible choice is Choice 2: See oli väike (It was small).\nChoice number: 2\ngpt-3.5-turbo output:\nPremise: \"Ese oli mullikilesse mässitud\" (The object was wrapped in bubble wrap).\nOptions:\n1. See oli õrn (It was fragile).\n2. See oli väike (It was small).\nStep 1: Based on the premise, we know that the object was wrapped in bubble wrap. It does not provide any information about the size or fragility of the item.\nStep 2: However, common sense would suggest that fragile items are more likely to be wrapped in bubble wrap for protection during transportation or storage.\nTherefore, the more plausible option is choice 1: \"See oli õrn\" (It was fragile).\nChoice number: 1 .\nAnswer:\n1\nFigure 10: An Estonian example of the XCOPA benchmark using XLT and the corresponding outputs under the\nzero-shot setting.\n12384\nInput:\nI want you to act as a commonsense reasoning expert for Estonian .\nHere is a premise: Mees jäi vanaks. What is the effect? Help me pick the more plausible option:\n-choice1: Ta juuksed muutusid halliks, -choice2: Ta müüs oma asjad maha.\nYou should retell the premise and the options in English.\nYou should do step-by-step answer to pick a choice .\nYou should step-by-step answer the request.\nYou should tell me the choice number in this format ' Choice number :'.\nXXX\nXXX\nPremise: A man turned on the tap.\nOptions: 1. The toilet filled with water. 2. Water flowed from the tap.\nStep-by-step answer:\n1. The premise states that a man turned on the tap, which implies that water started flowing.\n2. The first option suggests that the toilet filled with water, which is possible if the tap ...\n3. The second option suggests that water flowed from the tap, which is the more plausible ...\nChoice Number: 2. Water flowed from the tap. The answer is choice2.\"\nXXX\nXXX\n... (the remaining 4 examples)\nXXX\nXXX\nI want you to act as a commonsense reasoning expert for Estonian .\nHere is a premise: Ese oli mullikilesse mässitud. What is the cause? Help me pick the more\nplausible option: -choice1: See oli õrn, -choice2: See oli väike.\nYou should retell the premise and the options in English.\nYou should do step-by-step answer to pick a choice .\nYou should step-by-step answer the request.\nYou should tell me the choice number in this format ' Choice Number :'.\ntext-davinci-003 output:\nPremise: The object was wedged into a crack.\nOptions:\n1. It was delicate.\n2. It was small.\nStep-by-step answer:\n1. The premise states that the object was wedged into a crack.\n2. Option 1 suggests that the object was delicate, which is not necessarily related to the object being wedged into a crack.\n3. Option 2 suggests that the object was small, which is more plausible as a small object is more likely to fit into a crack.\nChoice Number: 2 . It was small.The answer is choice2.\ngpt-3.5-turbo output:\nPremise: \"The object was wrapped in bubble wrap.\"\nOptions:\n1. It was fragile.\n2. It was small.\nStep-by-step answer:\n1. The premise states that the object was wrapped in bubble wrap, which is often used to protect fragile items during transport.\n2. Bubble wrap is not typically used to protect small items, so option 1 is more plausible.\n3. Therefore, the more plausible option is Choice 1: It was fragile.\nChoice Number: 1 . It was fragile. The answer is choice1.\nAnswer:\n1\nFigure 11: An Estonian example of the XCOPA benchmark using XLT and the corresponding outputs under the\nfew-shot setting.\n12385\nInput:\nLes options ne sont pas si attrayantes. Based on previous passage, is it true\nthat Les choix ne sont pas trop attrayants? Yes, No, or Maybe?\ntext-davinci-003 output:\nyes\ngpt-3.5-turbo output:\nmaybe\nAnswer:\nentailment\nFigure 12: A French example of the XNLI benchmark using basic prompt and the corresponding outputs under the\nzero-shot setting. We need a verbalizer to map entailment to yes, contradiction to no, and neutral to maybe.\nInput:\nI want you to act as a natural language inference expert for French .\nPremise: Les options ne sont pas si attrayantes.\nHypothesis: Les choix ne sont pas trop attrayants.\nYou should retell the premise and hypothesis in English.\nYou should judge whether the hypothesis is true (entailment), false\n(contradiction), or undetermined (neutral) given the premise. The relationship\ncan be chosen from entailment, contradiction, and neutral .\nYou should step-by-step answer the request.\nYou should tell me the relationship in this format ' Relationship :'.\ntext-davinci-003 output:\nPremise (in English): The options are not very attractive.\nHypothesis (in English): The choices are not too attractive.\nRelationship: Contradiction\ngpt-3.5-turbo output:\nPremise: The options are not so attractive.\nHypothesis: The choices are not too attractive.\nRelationship: Entailment . The hypothesis is true given the premise as the concepts of options and\nchoices are interchangeable and both refer to the same thing. The degree of attractiveness is also\nconveyed in a similar manner in both statements.\nAnswer:\nentailment\nFigure 13: A French example of theXNLI benchmark using XLT and the corresponding outputs under thezero-shot\nsetting.\n12386\nInput:\nI want you to act as a natural language inference expert for French .\nPremise: savons-nous ce que nous allons dire ?\nHypothesis: Je sais que nous n'avons aucune idée de ce que nous allons dire.\nYou should retell the premise and hypothesis in English.\nYou should judge whether the hypothesis is true (entailment), false\n(contradiction), or undetermined (neutral) given the premise. The relationship\ncan be chosen from entailment, contradiction, and neutral .\nYou should step-by-step answer the request.\nYou should tell me the relationship in this format ' Relationship :'.\nXXX\nXXX\nRetelled premise: do we know what we're going to say.\nRetelled hypothesis: I know we don't have a clue what we'll say.\nRelationship: contradiction\nXXX\nXXX\n... (the remaining 4 examples)\nXXX\nXXX\nI want you to act as a natural language inference expert for French .\nPremise: Les options ne sont pas si attrayantes.\nHypothesis: Les choix ne sont pas trop attrayants.\nYou should retell the premise and hypothesis in English.\nYou should judge whether the hypothesis is true (entailment), false\n(contradiction), or undetermined (neutral) given the premise. The relationship\ncan be chosen from entailment, contradiction, and neutral .\nYou should step-by-step answer the request.\nYou should tell me the relationship in this format ' Relationship :'.\ntext-davinci-003 output:\nretelled premise: the options are not so attractive.\nretelled hypothesis: the choices are not too attractive.\nrelationship: entailment\ngpt-3.5-turbo output:\nretelled premise: the options are not very attractive.\nretelled hypothesis: the choices are not too attractive.\nrelationship: entailment\nAnswer:\nentailment\nFigure 14: A French example of the XNLI benchmark using XLT and the corresponding outputs under the few-shot\nsetting.\n12387\nBasic prompt input:\nSentence 1: Lito spielte für den Zingone Club Football. Sentence 2: Zingone spielte für den\nClub Lito Fußball. Does Sentence 1 paraphrase Sentence 2? Yes or No?\ntext-davinci-003 output:\nNo\ngpt-3.5-turbo output:\nYes\nAnswer:\nNo\nFigure 15: A Germany example of the PA WS-Xbenchmark using basic prompt and the corresponding outputs\nunder the zero-shot setting.\nInput:\nI want you to act as a paraphrase identification expert for Germany .\nSentence 1: Lito spielte für den Zingone Club Football\nSentence 2: Zingone spielte für den Club\nQuestion: Does Sentence 1 paraphrase Sentence 2? Yes or No?\nYou should retell the sentence 1 and sentence 2 in English.\nYou should provide a yes or no answer to the question: Does Sentence 1 paraphrase Sentence 2?\nYou should step-by-step answer the request.\nYou should tell me the answer choosing either yes or no in this format ' Answer :'.\ntext-davinci-003 output:\nSentence 1: Lito played football for the Zingone Club.\nSentence 2: Zingone played football for the Lito Club.\nAnswer: Yes\ngpt-3.5-turbo output:\nSentence 1: Lito played football for the Zingone Club.\nSentence 2: Zingone played football for the Club Lito.\nAnswer: No , Sentence 1 does not paraphrase Sentence 2.\nStep-by-step answer:\n1. Retold Sentence 1 and Sentence 2 in English.\n2. Provided a yes or no answer to the question: Does Sentence 1 paraphrase Sentence 2?\n3. Answered the request in the required format.\nAnswer:\nNo\nFigure 16: A Germany example of the PA WS-Xbenchmark using XLT and the corresponding outputs under the\nzero-shot setting.\n12388\nInput:\nI want you to act as a paraphrase identification expert for Germany .\nSentence 1: Der Ponoru ist eine Nebenfluss des Horezu in Rumänien .\nSentence 2: Der Ponoru ist ein Nebenfluss des Horezu in Rumänien .\nQuestion: Does Sentence 1 paraphrase Sentence 2? Yes or No?\nYou should retell the sentence 1 and sentence 2 in English.\nYou should provide a yes or no answer to the question: Does Sentence 1 paraphrase Sentence 2?\nYou should step-by-step answer the request.\nYou should tell me the answer choosing either yes or no in this format ' Answer :'.\nXXX\nXXX\nRetelled Sentence 1: The Ponoru River is a tributary of the Horezu River in Romania .\nRetelled Sentence 2: The Ponoru River is a tributary of the Horezu in Romania .\nAnswer: Yes\nXXX\nXXX\n... (the remaining 4 examples)\nXXX\nXXX\nI want you to act as a paraphrase identification expert for Germany .\nSentence 1: Lito spielte für den Zingone Club Football\nSentence 2: Zingone spielte für den Club\nQuestion: Does Sentence 1 paraphrase Sentence 2? Yes or No?\nYou should retell the sentence 1 and sentence 2 in English.\nYou should provide a yes or no answer to the question: Does Sentence 1 paraphrase Sentence 2?\nYou should step-by-step answer the request.\nYou should tell me the answer choosing either yes or no in this format ' Answer :'.\ntext-davinci-003 output:\nRetelled Sentence 1: Lito played football for the Zingone Club.\nRetelled Sentence 2: Zingone played football for the Lito Club.\nAnswer: Yes\ngpt-3.5-turbo output:\nRetelled Sentence 1: Lito played football for the Zingone Club.\nRetelled Sentence 2: Zingone played football for the Lito Club.\nAnswer: No\nAnswer:\nNo\nFigure 17: A Germany example of the PA WS-Xbenchmark using XLT and the corresponding outputs under the\nfew-shot setting.\n12389\nInput:\nAnswer the question in one or a few words in Japanese : 世界で一番背の高い人類は誰で\nすか？\ntext-davinci-003 output:\n最高身長者。\ngpt-3.5-turbo output:\n誰？\nAnswer:\nロバート・ワドロー\nFigure 18: A Japanese example of the MKQA benchmark using basic prompt and the corresponding outputs under\nthe zero-shot setting.\nInput:\nI want you to act as a question answering expert for Japanese .\nQuestion: 世界で一番背の高い人類は誰ですか？\nYou should retell the question in English.\nYou should answer the question in English in one or a few words.\nYou should step-by-step answer the request.\nYou should tell me the answer in one or a few words in Japanese in this format ' Answer :'.\ntext-davinci-003 output:\nQuestion in English: Who is the tallest human being in the world?\nAnswer: Robert Wadlow (2.72 m).\nAnswer in Japanese: 答え：ロバート・ワドロウ（2.72 m）。\ngpt-3.5-turbo output:\nRetelling: Who is the tallest person in the world?\nAnswer: Robert Wadlow\nStep-by-step response:\n1. The question is asking for the tallest person in the world.\n2. The answer is Robert Wadlow, who was 8 feet 11 inches (2.72 meters) tall.\nAnswer in Japanese: 答え：ロバート・ワドロー\nAnswer:\nロバート・ワドロー\nFigure 19: A Japanese example of the MKQA benchmark using XLT and the corresponding outputs under the\nzero-shot setting.\n12390\nInput:\nI want you to act as a question answering expert for Japanese .\nQuestion: フルハウスでスティーブが出てきたのはどのシーズンですか？\nYou should retell the question in English.\nYou should answer the question in English in one or a few words.\nYou should step-by-step answer the request.\nYou should tell me the answer in one or a few words in Japanese in this\nformat ' Answer :'.\nXXX\nXXX\nQuestion: what season does steve come in full house?\nEnglish Answer: 5.0 seasons.\nJapanese Answer: 5.0 時節\nXXX\nXXX\n... (the remaining 4 examples)\nXXX\nXXX\nI want you to act as a question answering expert for Japanese .\nQuestion: 世界で一番背の高い人類は誰ですか？\nYou should retell the question in English.\nYou should answer the question in English in one or a few words.\nYou should step-by-step answer the request.\nYou should tell me the answer in one or a few words in Japanese in this\nformat ' Answer :'.\ntext-davinci-003 output:\nquestion: who is the tallest human being in the world\nenglish answer: sultan kösen\njapanese answer: スルタン・ケーセン\ngpt-3.5-turbo output:\nquestion: who is the tallest person in the world\nnenglish answer: robert wadlow\njapanese answer: ロバート・ワドロー\nAnswer:\nロバート・ワドロー\nFigure 20: A Japanese example of the MKQA benchmark using XLT and the corresponding outputs under the\nfew-shot setting.\n12391\nInput:\nSummarize this article: Huang Ming es tan apasionado por la energía solar que construyó el llamado \"Solar Valley\"\n(o Ciudad Solar): un conjunto de edificios que funcionan íntegramente a partir del sol. En su diseño también se\nintegran los paneles solares que los alimentan. Pero aunque Huang soñaba con que todo el mundo imitaría sus\nesfuerzos, todavía sigue siendo la única Ciudad Solar en China (y del mundo). Video producido por Álvaro Ricciardelli.\ntext-davinci-003 output:\nHuang Ming es un innovador apasionado por la energía solar que construyó el primer conjunto de edificios\nalimentados íntegramente por el sol, conocido como \"Solar Valley\" o Ciudad Solar. A pesar de que Huang esperaba\nque su proyecto inspirara a otros, todavía es el único de su tipo en China y el mundo.\ngpt-3.5-turbo output:\nHuang Ming built the \"Solar Valley\" in China, a collection of buildings that run entirely on solar power. Despite\nhis hopes that others would follow his lead, it remains the only solar city in China and the world.\nAnswer:\nLo llaman \"El rey sol\", aunque a él no le gusta.\nFigure 21: A Spanish example of the XL-Sum* benchmark using basic prompt and the corresponding outputs\nunder the zero-shot setting.\nInput:\nI want you to act as a multilingual summarization expert for Spanish .\nText: Huang Ming es tan apasionado por la energía solar que construyó el llamado \"Solar Valley\" (o Ciudad Solar):\nun conjunto de edificios que funcionan íntegramente a partir del sol. En su diseño también se integran los\npaneles solares que los alimentan. Pero aunque Huang soñaba con que todo el mundo imitaría sus esfuerzos,\ntodavía sigue siendo la única Ciudad Solar en China (ydel mundo). Video producido por Álvaro Ricciardelli.\nYou should repeat the entire text in English.\nYou should think step-by-step to summarize the entire text in a maximum of two sentences .\nYou should step-by-step answer the request.\nYou should tell me the summary into one sentence in Spanish in this format ' Summary :'.\ntext-davinci-003 output:\nSummary: Huang Ming es un apasionado de la energía solar que construyó la única Ciudad Solar en China y del\nmundo, llamada \"Solar Valley\", que funciona íntegramente con paneles solares.\ngpt-3.5-turbo output:\nThe text is in Spanish and talks about Huang Ming, who built the \"Solar Valley\" in China, a group of\nbuildings that run entirely on solar energy. Despite his hopes that others would follow his lead, it remains the\nonly solar city in China and the world.\nTo summarize in two sentences: Huang Ming built the \"Solar Valley\" in China, a group of buildings that run\nentirely on solar energy, but it remains the only solar city in China and the world.\nSummary: Huang Ming construyó la \"Ciudad Solar\" en China, un conjunto de edificios que funcionan íntegramente\na partir del sol, pero sigue siendo la única ciudad solar en China y en el mundo.\nAnswer:\nLo llaman \"El rey sol\", aunque a él no le gusta.\nFigure 22: A Spanish example of the XL-Sum* benchmark using XLT and the corresponding outputs under the\nzero-shot setting.\n12392\nInput:\nI want you to act as a multilingual summarization expert for Spanish .\nText: Testigos del tiroteo explicaron que el atacante asaltó la mezquita a los pocos minutos de haber empezado\nla oración. La policía de Nueza Zelanda declaró que hay cuatro personas detenidas y que no puede descartarse\nque haya más implicados en los ataque. Ahora puedes recibir notificaciones de BBC Mundo. Descarga la nueva\nversión de nuestra app y actívalas para no perderte nuestro mejor contenido.\nYou should repeat the entire text in English.\nYou should think step-by-step to summarize the entire text in a maximum of two sentences .\nYou should step-by-step answer the request.\nYou should tell me the summary into one sentence in Spanish in this format ' Summary :'.\nXXX\nXXX\nThe text states that a shooter attacked a mosque during prayer time in New Zealand, with four people currently\nin custody and the possibility of more involved. The article also promotes the BBC Mundo app for notifications.\nTo summarize, a mosque in New Zealand was attacked during prayer time with four people currently detained and\nthe potential for more involvement, while also promoting the BBC Mundo app for notifications.\nSummary: Al menos 49 personas murieron en dos tiroteos en mezquitas en Nueva Zelanda.\nXXX\nXXX\n... (the remaining 2 examples)\nXXX\nXXX\nI want you to act as a multilingual summarization expert for Spanish .\nText: Huang Ming es tan apasionado por la energía solar que construyó el llamado \"Solar Valley\" (o Ciudad Solar):\nun conjunto de edificios que funcionan íntegramente a partir del sol. En su diseño también se integran los\npaneles solares que los alimentan. Pero aunque Huang soñaba con que todo el mundo imitaría sus esfuerzos,\ntodavía sigue siendo la única Ciudad Solar en China (y del mundo). Video producido por Álvaro Ricciardelli.\nYou should repeat the entire text in English.\nYou should think step-by-step to summarize the entire text in a maximum of two sentences .\nYou should step-by-step answer the request.\nYou should tell me the summary into one sentence in Spanish in this format ' Summary :'.\ntext-davinci-003 output:\nSummary: Huang Ming construyó la única Ciudad Solar del mundo, que funciona íntegramente con energía solar.\ngpt-3.5-turbo output:\nThe text highlights Huang Ming's passion for solar energy and his creation of the \"Solar Valley\", a group of\nbuildings that run entirely on solar power, but despite his hopes for others to follow suit, it remains the\nonly solar city in China and the world.\nSummary: Huang Ming construyó la única Ciudad Solar en China y del mundo, un conjunto de edificios que\nfuncionan íntegramente a partir del sol.\nAnswer:\nLo llaman \"El rey sol\", aunque a él no le gusta.\nFigure 23: A Spanish example of the XL-Sum* benchmark using XLT and the corresponding outputs under the\nfew-shot setting.\n12393\nInput:\nNalika ana siji vaksin pacoban katon bisa nyuda kematian Ebola, nganti saiki, ora ana obat\nsing kabukten jelas kanggo ngobati infeksi sing ana. Translate from Javanese to Chinese:\ntext-davinci-003 output:\n目前尚无明确的治疗方法可以治愈埃博拉病毒感染，但是研究人员已经开发出一种疫苗来预防埃博拉病毒引起的死亡。\ngpt-3.5-turbo output:\n目前为止，尚无明确的治疗方案可用于治疗埃博拉病毒感染，但如果接种一种疫苗可能会有所帮助，直至现在为止。\nAnswer:\n虽然有一种实验性疫苗看似能够降低埃博拉病毒的死亡率，但迄今为止，还没明确证明任何药物适合治疗现有的感染。\nFigure 24: A jv →zh example of the FLORES* benchmark using basic prompt and the corresponding outputs\nunder the zero-shot setting.\nInput:\nI want you to act as a machine translation expert for Javanese to Chinese .\nSource sentence: Nalika ana siji vaksin pacoban katon bisa nyuda kematian Ebola, nganti saiki,\nora ana obat sing kabukten jelas kanggo ngobati infeksi sing ana.\nYou should repeat the source sentence in English.\nYou should provide the Chinese translation for the English source sentence .\nYou should step-by-step answer the request.\nYou should tell me the target translation in this format ' Target translation :'.\ntext-davinci-003 output:\nSource sentence: When there was a vaccine available to prevent death from Ebola, until now, there is no clear\nevidence of a treatment to cure the infection.\nTarget translation: 当有一种疫苗可以预防埃博拉死亡时，直到现在，还没有清楚的证据表明有治疗感染的方法\ngpt-3.5-turbo output:\nSource sentence: Nalika ana siji vaksin pacoban katon bisa nyuda kematian Ebola, nganti saiki, ora ana obat sing\nkabukten jelas kanggo ngobati infeksi sing ana.\nEnglish source sentence: When there is a vaccine trial that can help prevent Ebola deaths, until now, there is\nno clear proven medicine to treat existing infections.\nChinese translation: 当有一种疫苗试验可以帮助预防埃博拉死亡时，到目前为止，没有明确的已证明的药物\n来治疗现有的感染。\nTarget translation: 当有一种可以帮助预防埃博拉死亡的疫苗试验时，到目前为止，没有明确证明可以治疗现有感染的药\n物。\nAnswer:\n虽然有一种实验性疫苗看似能够降低埃博拉病毒的死亡率，但迄今为止，还没明确证明任何药物适合治疗现有的感染。\nFigure 25: A jv→zh example of the FLORES* benchmark using XLT and the corresponding outputs under the\nzero-shot setting.\n12394",
  "topic": "Task (project management)",
  "concepts": [
    {
      "name": "Task (project management)",
      "score": 0.7497425079345703
    },
    {
      "name": "Computer science",
      "score": 0.7097699642181396
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4804908335208893
    },
    {
      "name": "Natural language processing",
      "score": 0.44720661640167236
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4442400634288788
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39892199635505676
    },
    {
      "name": "Mathematics",
      "score": 0.09310901165008545
    },
    {
      "name": "Engineering",
      "score": 0.07233047485351562
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    }
  ],
  "cited_by": 41
}